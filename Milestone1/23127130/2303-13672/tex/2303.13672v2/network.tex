
\section{Network Architecture}\label{neural-architecture}

\begin{figure}
	\centering
	\setlength{\abovecaptionskip}{-10pt plus 1pt minus 20pt }
	\incfig[1]{Network} 
	\caption{Architecture of the \ac{nn}. A trainable input vector $\boldsymbol{\Theta}$ is fed into the network. The light blue arrow involves a set of operations that include a fully connected layer and the dark blue arrow involves a set of operations that include a convolutional layer. The intermediate data structures are of size $(c_l,w_l,h_l)$ and the final output, after $n$ layers, gives $\mathbf{\varphi}$. %\sbcom{\hl{I would make use of the width of the page, separating things further, so not arrows over boxes, and put the sizes right in the middle of edges.}}}
	}
	\label{fig:Network}
\end{figure}

In this section, we present the \ac{nn} architecture used in the geometry parameterization. This describes the mapping between the parameters $\mathbf{p}$ and the \ac{ls} vector $\mathbf{\varphi}$. \sbcom{The network used here is based on the U-net architecture and is mainly built from convolutional layers}. In contrast to a fully connected network, convolutional networks connect smaller sets of neurons in each layer assuming that neurons in close proximity have a more important relationship. This is a natural relaxation for the processing of spatial data. Furthermore, convolutional networks are made efficient by the assumption that features that are found in one local block are likely to be found in a different local block, i.e. somewhere else in the domain. This is done by sharing parameters amongst local blocks in the form of a convolutional filter. 
The specific architecture used in this work is the one presented in \cite{Hoyer2019}. Because we are simply reparameterizing the pixel values, there is no input into the network in the traditional sense. Most approaches combining \ac{nn}s and \ac{to} use fully connected layers and take the network to provide a map between a spatial input $\mathbf{x}$ and a scalar output $f_\theta (\mathbf{x})$ \cite{Deng2020,Chandrasekhar2020}. In our case, we only need a single evaluation of the network to output a vector that represents the entire discrete \ac{ls} function. \sbcom{ This is because, as in \cite{Hoyer2019}, the dimension of the output of the network matches the number of nodal values of the \ac{ls} function so that we have a 1:1 mapping between the network and the geometry discretization. This follows the typical use of the U-net, where the output image gives a segmented domain \cite{ronneberger2015unet}. In contrast, however, to original applications of the U-net, our input vector is also taken to be a set of trainable parameters $\mathbf{\boldsymbol{\Theta}}$.}

The architecture of the network is illustrated in Figure \ref{fig:Network}. The first arrow indicated with the label Dense in Figure \ref{fig:Network} contains a fully connected layer and a reshape:
\begin{equation}
	\mathbf{x}^{(1)}(\boldsymbol{\Theta}) = \mathrm{reshape}  (  \mathbf{W}  \boldsymbol{\Theta} + \mathbf{b}  ), \quad  \mathbf{x}^1 \in \mathbb{R}^{c_1,w_1,h_1},
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{c_1 w_1 h_1, N_{\boldsymbol{\Theta}}}$ is the dense weight matrix, $\mathbf{b} \in \mathbb{R}^{c_1 w_1 h_1 }$ is the bias and $\mathrm{reshape}: \mathbb{R}^{c_1 w_1 h_1} \rightarrow \mathbb{R}^{c_1,w_1,h_1}$ is a reshaping map. The lengths $N_{\boldsymbol{\Theta}}$, $c_1$, $w_1$ and $h_1$ represent the length of $\boldsymbol{\Theta}$, the initial number of channels, the latent space image width and the latent space image height, respectively. Note here that the first nonlinearity is imposed at the beginning of the next layer.

The upsampling convolutional layers, depicted by the dark blue arrow in Figure \ref{fig:Network}, are defined as: 
\begin{equation}
	\mathbf{x}^{(l+1)}(\mathbf{x}^{(l)}) =  \boldsymbol{\mathcal{P}}^{(l)} (  \boldsymbol{\varrho}  ( \mathbf{\Phi}^{(l)}  (  \text{tanh}  ( \mathbf{x}^{(l)} )))), \quad  \mathbf{x}^{(l)} \in \mathbb{R}^{c_l  ,  w_l  ,  h_l}, \ \  \mathbf{x}^{(l+1)} \in \mathbb{R}^{c_{l+1}  ,  w_{l+1}  ,  h_{l+1}}, 
\end{equation}
where
$\mathbf{\Phi}^{(l)}: \mathbb{R}^{ c_l, w_l, h_l } \rightarrow \mathbb{R}^{ c_l, w_{l+1}, h_{l+1} }$ is a bilinear resize, 
$\boldsymbol{\varrho}$ is a
normalization to a mean of $0$ and variance of $1$ across the channel dimension and 
$\boldsymbol{\mathcal{P}}^{(l)}: \mathbb{R}^{ c_l  ,  w_{l+1}  ,  h_{l+1}} \rightarrow \mathbb{R}^{ c_{l+1}  ,  w_{l+1}  ,  h_{l+1} }$ is a convolutional operator with kernel size $(5,5)$ \sbcom{which is found to allow for the required level of expressivity.}

In this approach, our input to the network $\boldsymbol{\Theta}$ is taken to be trainable. So to define our parameter vector $\mathbf{p}$, we collect the parameters of $\mathbf{W}$, $\mathbf{b}$, $\boldsymbol{\Theta}$ and $\boldsymbol{P}^i$ into a vector $\mathbf{p}\in \mathbb{R}^{N_p} $. Then, by composing the layers, we obtain the function 
$\boldsymbol{\mathcal{N}}: \mathbf{p} \in \mathbb{R}^{N_p} \mapsto  \mathbf{\varphi} \in \mathbb{R}^{N}$:
\begin{equation}	
	{\boldsymbol{\mathcal{N}}} = \mathbf{x}^{(n)}(... ( \mathbf{x}^{(2)}(\mathbf{x}^{(1)} (\boldsymbol{\Theta})))).
\end{equation}
\sbcom{The output image of size $(w_n,h_n)$ obtained by evaluating the network at a set of parameters $\mathbf{\varphi}=\boldsymbol{\mathcal{N}}(\mathbf{p})$ is then used to define the geometry for the problem as described in Section \ref{level-set-function-processing}.
} Importantly, the locality is preserved when defining this function. 
As we traverse the network, we follow the design principle of the upsampling section of the U-Net and trade-off channel depth for spatial resolution. This means that, in general, the widths $w$ and heights $h$ will increase as we move through the network and the number of channels $c$ will decrease as we move through the network. The exact trade-offs here can vary, but we must set the resizes and channel refinement to ensure $h_n w_n = N $ and $ c_n = 1$ so that our output $\mathbf{\varphi}$ makes sense as a vector representing the  \ac{DOF} values for the \ac{ls}. 
We can increase the network size in the dense layer by increasing $N_{\boldsymbol{\Theta}}$, $w_1$ and $h_1$ and in the convolutional layers by increasing the elements in $c$ and the number of layers. 

\sbcom{
One approach to initializing the parameters of the \ac{nn} is to pre-train the network to output the manually selected geometry with holes, as in \cite{Deng2021}. Using this as an initial guess, however, causes the geometry to converge quickly to poor local minima. A more common approach when working with \ac{nn}s and a given objective map is to start with small random weights \cite{RUMELHART1988}. In this case, we have high asymmetry in the weights and little activation function saturation. It turns out that a random initialization of the \ac{nn} with the volume constraint gives an initial guess of a domain with a few holes in random locations. This is in contrast to initializing the \ac{ls} \ac{fe} function \ac{DOF} values with random values which gives a geometry with many small holes and fine features which is not necessarily desirable \cite{Barrera2020}. As is common in \ac{nn} approaches, we can then easily take multiple seeds for the geometry using different random initializations to alleviate initialization dependency. The random initializations simply correspond to different size holes in different locations. Using this method we eliminate the need for manual geometry initialization.

We use Xavier uniform initialization for the weights \cite{Glorot2010UnderstandingTD}, in this case the design variables, of the neural network to facilitate convergence. To minimize the effect of the initialization dependency of the optimized geometry, that is, wherever the holes appear in the initial design based on the particular random initialization, we take 100 random seeds of the parameters, run the optimization and take the final design with the lowest objective function value. 
}


