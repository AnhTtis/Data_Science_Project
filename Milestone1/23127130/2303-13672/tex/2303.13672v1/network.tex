
\section{Network Architecture}\label{neural-architecture}

\begin{figure}
	\centering
	\setlength{\abovecaptionskip}{-10pt plus 1pt minus 20pt }
	\incfig[1]{Network} 
	\caption{Architecture of the \ac{nn}. A trainable input vector $\boldsymbol{\Theta}$ is fed into the network. The light blue arrow involves a set of operations that include a fully connected layer and the dark blue arrow involves a set of operations that include a convolutional layer. The intermediate data structures are of size $(c_l,w_l,h_l)$ and the final output, after $n$ layers, gives $\mathbf{\varphi}$. %\sbcom{\hl{I would make use of the width of the page, separating things further, so not arrows over boxes, and put the sizes right in the middle of edges.}}}
	}
	\label{fig:Network}
\end{figure}

In this section, we present the \ac{nn} architecture used in the geometry parameterization. This describes the mapping between the parameters $\mathbf{p}$ and the \ac{ls} vector $\mathbf{\varphi}$. The network used here is mainly built from convolutional layers. In contrast to a fully connected network, convolutional networks connect smaller sets of neurons in each layer assuming that neurons in close proximity have a more important relationship. This is a natural relaxation for the processing of spatial data. Furthermore, convolutional networks are made efficient by the assumption that features that are found in one local block are likely to be found in a different local block, i.e. somewhere else in the domain. This is done by sharing parameters amongst local blocks in the form of a convolutional filter. 
The specific architecture used in this work is the one presented in \cite{Hoyer2019}. Because we are simply reparameterizing the pixel values, there is no input into the network in the traditional sense. Most approaches combining \ac{nn}s and \ac{to} use fully connected layers and take the network to provide a map between a spatial input $\mathbf{x}$ and a scalar output $f_\theta (\mathbf{x})$ \cite{Deng2020,Chandrasekhar2020}. In our case, we only need a single evaluation of the network to output a vector $f_\theta (\mathbf{\boldsymbol{\Theta}})$ that represents the entire discrete \ac{ls} function. The input vector in our case is just taken to be a set of trainable parameters $\mathbf{\boldsymbol{\Theta}}$. 

The architecture of the network is illustrated in Figure \ref{fig:Network}. The first arrow indicated with the label Dense in Figure \ref{fig:Network} contains a fully connected layer and a reshape:
\begin{equation}
	\mathbf{x}^{(1)}(\boldsymbol{\Theta}) = \mathrm{reshape}  (  \mathbf{W}  \boldsymbol{\Theta} + \mathbf{b}  ), \quad  \mathbf{x}^1 \in \mathbb{R}^{c_1,w_1,h_1},
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{c_1 w_1 h_1, N_{\boldsymbol{\Theta}}}$ is the dense weight matrix, $\mathbf{b} \in \mathbb{R}^{c_1 w_1 h_1 }$ is the bias and $\mathrm{reshape}: \mathbb{R}^{c_1 w_1 h_1} \rightarrow \mathbb{R}^{c_1,w_1,h_1}$ is a reshaping map. The lengths $N_{\boldsymbol{\Theta}}$, $c_1$, $w_1$ and $h_1$ represent the length of $\boldsymbol{\Theta}$, the initial number of channels, the latent space image width and the latent space image height, respectively. Note here that the first nonlinearity is imposed at the beginning of the next layer.

The upsampling convolutional layers, depicted by the dark blue arrow in Figure \ref{fig:Network}, are defined as: 
\begin{equation}
	\mathbf{x}^{(l+1)}(\mathbf{x}^{(l)}) =  \boldsymbol{\mathcal{P}}^{(l)} (  \boldsymbol{\varrho}  ( \mathbf{\Phi}^{(l)}  (  \text{tanh}  ( \mathbf{x}^{(l)} )))), \quad  \mathbf{x}^{(l)} \in \mathbb{R}^{c_l  ,  w_l  ,  h_l}, \ \  \mathbf{x}^{(l+1)} \in \mathbb{R}^{c_{l+1}  ,  w_{l+1}  ,  h_{l+1}}, 
\end{equation}
where
$\mathbf{\Phi}^{(l)}: \mathbb{R}^{ c_l, w_l, h_l } \rightarrow \mathbb{R}^{ c_l, w_{l+1}, h_{l+1} }$ is a bilinear resize, 
$\boldsymbol{\varrho}$ is a
normalization to a mean of $0$ and variance of $1$ across the channel dimension and 
$\boldsymbol{\mathcal{P}}^{(l)}: \mathbb{R}^{ c_l  ,  w_{l+1}  ,  h_{l+1}} \rightarrow \mathbb{R}^{ c_{l+1}  ,  w_{l+1}  ,  h_{l+1} }$ is a convolutional operator with kernel size $(5,5)$.

In this approach, our input to the network $\boldsymbol{\Theta}$ is taken to be trainable. So to define our parameter vector $\mathbf{p}$, we collect the parameters of $\mathbf{W}$, $\mathbf{b}$, $\boldsymbol{\Theta}$ and $\boldsymbol{P}^i$ into a vector $\mathbf{p}\in \mathbb{R}^{N_p} $. Then, by composing the layers, we obtain the function 
$\boldsymbol{\mathcal{N}}: \mathbf{p} \in \mathbb{R}^{N_p} \mapsto  \mathbf{\varphi} \in \mathbb{R}^{N}$:
\begin{equation}	
	{\boldsymbol{\mathcal{N}}} = \mathbf{x}^{(n)}(... ( \mathbf{x}^{(2)}(\mathbf{x}^{(1)} (\boldsymbol{\Theta})))),
\end{equation}
where $N$ is the number of  \acp{DOF} of the \ac{fe} function for the \ac{ls}. The output image of size $w_n,h_n$ is interpreted as a vector $\mathbf{\varphi}$ representing the  \acp{DOF} of the \ac{ls} \ac{fe} function as described in Section \ref{level-set-function-processing}. Importantly, the locality is preserved when defining this function. 
As we traverse the network, we follow the design principle of the upsampling section of the U-Net and trade-off channel depth for spatial resolution. This means that, in general, the widths $w$ and heights $h$ will increase as we move through the network and the number of channels $c$ will decrease as we move through the network. The exact trade-offs here can vary, but we must set the resizes and channel refinement to ensure $h_n w_n = N $ and $ c_n = 1$ so that our output $\mathbf{\varphi}$ makes sense as a vector representing the  \acp{DOF} for the \ac{ls}. 
We can increase the network size in the dense layer by increasing $N_{\boldsymbol{\Theta}}$, $w_1$ and $h_1$ and in the convolutional layers by increasing the elements in $c$ and the number of layers. 

In a typical \ac{ls} method, the user is required to manually initialize the geometry with holes. One approach to initializing the \ac{nn} is to pre-train the network to output the manually selected geometry with holes, as in \cite{Deng2021}. Using this as an initial guess, however, causes the geometry to converge quickly to poor local minima. A more common approach when working with \ac{nn}s and a given objective map is to start with small random weights \cite{RUMELHART1988}. In this case, we have high asymmetry in the weights and little activation function saturation. It turns out that a random initialization of the \ac{nn} with the volume constraint gives an initial guess of a domain with a few holes in random locations. This is in contrast to initializing the \ac{ls} itself with random values which gives a geometry with many small holes and fine features which is not necessirly desirable \cite{Barrera2020}. As is common in \ac{nn} approaches, we can then easily take multiple seeds for the geometry using different random initializations to alleviate initialization dependency. The random initializations simply correspond to different size holes in different locations. Using this method we eliminate the need for manual geometry initialization.

%parameter sharing
%sparsity connections
