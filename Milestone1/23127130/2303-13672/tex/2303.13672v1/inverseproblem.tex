
\section{optimization Problem}\label{optimsation-problem}

%\the\linewidth

%\begin{document}
In this section, we provide a succinct overview of the overall \ac{to} algorithm proposed in this work. 
We aim to solve the problem:
	\begin{equation}
		\begin{aligned}
			\underset{\mathbf{p}}{\text{min}} & \ J(\boldsymbol{u}(\mathbf{p}),\mathbf{p})&  \\
			\text{s.t} & \ \mathscr{R}(\boldsymbol{u}(\mathbf{p}),\mathbf{p}) &= 0 ,\\
				   & \ \quad \quad \  \mathscr{V}(\mathbf{p})     &= 0 ,\\
		\end{aligned}
	\label{eq:opt-problem}
	\end{equation}

where 
$\mathbf{p}$ are the parameters that describe our geometry,
  $\mathscr{V}$  is an equality constraint (e.g. for the volume),
  $\mathscr{R}$  is the PDE residual,
  $\boldsymbol{u}$ is the solution of the PDE and
  $J$  is the objective.
If desired, further equality and inequality constraints can then be imposed by adding penalty terms to the objective function. 


\begin{figure}
	\centering
	\incfig[0.55]{FlowChart}
	\caption{Computational graph of the optimization loop. We start with an input to the system $\mathbf{p}$, and perform the forward pass by descending through the blue boxes on the right side to obtain a performance measure $J$ as explained in Section \ref{forward-pass}. %We evaluate the network to obtain $\mathbf{\varphi}$, apply the constraint to obtain $\phi$ and solve the residual equation to obtain $\boldsymbol{u}$.
 	The convergence criteria are used to decide whether this $J$ represents an acceptable minimum. If not, the backward pass is performed to compute an update for the parameters as explained in Section \ref{backwards-pass} and the loop is continued. %\sbcom{\hl{I would change the second box to $\phi = \mathscr{H}(\mathbf{\varphi})$, where $\mathscr{H}$ is an operator that takes the NN and generates a suitable  function (e.g., after re-distancing and volume correction by \emph{traslation}).}} } 
	}
 	\label{fig:FlowChart}
\end{figure}

\subsection{Optimization Loop}\label{optimization-loop}
To optimize the parameters $\mathbf{p}$, we make use of a gradient-based optimization strategy. To do so, we establish a map between $\mathbf{p}$ and $J$ and a means to compute the gradient $\frac{dJ}{d\mathbf{p}}$ for parameter updates.
For the neural \ac{ls} \ac{to} method, $\mathbf{p}$ represents the parameters of a particular \ac{nn} that outputs a vector $\mathbf{\varphi}$. This vector is processed using the operator $\mathscr{H}$ to obtain the \ac{ls} $\phi$ used in the PDE and objective function. %\sbcom{\hl{As I say in the figure, I would add some notation.}}. 
It is also only through $\phi$ that the PDE and objective depend on the parameter vector $\mathbf{p}$. 
With these definitions, we present the optimization loop for solving (\ref{eq:opt-problem}) in Figure \ref{fig:FlowChart}. 


\subsubsection{Forward Pass}\label{forward-pass}

To solve the forward problem and get a performance measure $J$ for a set of parameters $\mathbf{p}$ we descend through the light blue boxes on the right hand side of Figure \ref{fig:FlowChart} by performing the following steps:
 
\begin{enumerate}	
	\item In the first light blue box, we evaluate the network $\boldsymbol{\mathcal{N}}: \mathbf{p} \in \mathbb{R}^{N_p} \mapsto  \mathbf{\varphi} \in \mathbb{R}^{N}$, where $N_p$ is the number of parameters and $\boldsymbol{\mathcal{N}}$ is as defined in Section \ref{neural-architecture}. 
	\item 
	In the second light blue box, we process the output of the network $\mathbf{\varphi}$ using the operator $\mathscr{H}: \mathbf{\varphi} \in \mathbb{R}^{N} \mapsto \phi \in V_h^1$ to obtain a suitable \ac{ls} description of the geometry $\phi $, where $V_h^1$ is the \ac{fe} space for the \ac{ls} defined in Section \ref{level-set-function-processing}. This involves an interpolation on a \ac{fe} space, smoothing and the inclusion of an equality constraint on the geometry so that the final \ac{ls} function satisfies $\mathscr{V}(\phi)=0$. This step is broken down in Section \ref{level-set-function-processing}. We enforce the equality constraint here to allow for the use of an unconstrained optimization method suitable for \ac{nn}s.	
	\item In the third light blue box, we solve the \ac{fe} problem associated with the weak form of the residual $\mathscr{R} (\boldsymbol{u}_h(\phi),\phi)=0$ on the domain segmentation defined by $\phi$ for an approximate solution $\boldsymbol{u}_h$ obtained using a \ac{fe} discretization. Details of the \ac{fcm} used to solve the problem are given in Section \ref{simulation}. We can then evaluate the objective $J(\boldsymbol{u}_h(\phi),\phi) \in \mathbb{R}$.
		
		
\end{enumerate}

\subsubsection{Backwards Pass}\label{backwards-pass}

To perform the update for the parameters using a steepest descent optimization strategy, we require an evaluation of the gradient $\frac{\partial J}{\partial \mathbf{p}}$. 
To do this efficiently at a cost roughly matching that of the forward pass, we use reverse mode differentiation and define rules to propagate sensitivities through each of the steps in the forward pass. We use an adjoint rule for the PDE and use automatic differentiation for all of the partial derivatives, including the derivative of integrals with respect to the \ac{ls}.
The derivative is then passed onto a chosen optimizer to update the parameters $\mathbf{p}$. The implementation of the gradient computation is discussed in Section \ref{backwards-pass-implementation}.
