

\hypertarget{numerical-experiments}{%
\section{Numerical Experiments}\label{numerical-experiments}}

\hypertarget{benchmark-results}{%
\subsection{Benchmark Results}\label{benchmark-results}}

We first compare the optimized results obtained for benchmark problems against baseline methods using the model problems presented in Section \ref{simulation}.

The method presented in this work, the NN-LS method, is compared against its non-neural counterpart the Pixel \ac{ls} (Pixel-LS) method, that is, the same method without a neural prior where instead the  \acp{DOF} of the \ac{ls} function are taken as the optimization parameters. We also compare the method against the SIMP method of \ac{to} again using both a neural prior (NN-SIMP) and the standard approach (Pixel-SIMP). 
Following a standard SIMP implementation of the heat conduction problem, we use a conductivity based on the power law $k=\alpha_T + (1-\alpha_T)\rho^{\gamma}$ where $\rho$ is a design variable given by a \ac{fe} function constructed on the space $V_h^1$ where the \acp{DOF} are the optimization parameters or output vector of the \ac{nn} in the Pixel-SIMP and NN-SIMP cases, respectively, and $\gamma$ is the penalization parameter, taken to be equal to $3$. 
Similarly, for the SIMP implementation of the structural problem, we use a Youngs Modulus $E=\alpha_d + (1- \alpha_T)\rho^{\gamma}$. 
To maintain a fair comparison between methods, we use the optimizers most widely regarded as suitable for the particular parameterization. Namely, we take the most commonly used MMA optimization strategy \cite{Svanberg1987} for the pixel parameterization and the ADAM strategy \cite{Kingma2014} for the \ac{nn}.

\subsubsection{Benchmark Problems}

\begin{figure}%
    \centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering\captionsetup{width=.9\linewidth}%
		\incfig[1]{HeatSetup}
		\caption{Heat conduction problem setup. The top and left sides $\Gamma_D$ are given a Dirichlet condition and the bottom and left sides $\Gamma_N$ are given a Neumann condition.}
		\label{fig:heat-conduction-problem-setup}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\centering\captionsetup{width=.9\linewidth}%
		\incfig[1]{MBBSetup}
		\caption{The right half of the MBB problem exploiting symmetry. The roller supports provide vertical restraint on the left-hand side and horizontal restraint in the bottom right corner. A downward force $F$ is prescribed on the top left corner.}
		\label{fig:MBB-problem-setup}
	\end{subfigure}
    \caption{Benchmark problems}
    \label{fig:benchmark-problems}
\end{figure}



The first problem studied is the poisson equation to model heat conduction with the setup in Figure \ref{fig:heat-conduction-problem-setup} selected from \cite{GersborgHansen2006}. For this problem, we set $k_0 =1 m^2s^{-1}$, $\alpha_T=0.01$, $f=0.01Ks^{-1}$, use homogenous Dirichlet and Neumann conditions and use a mesh of $ 95 \times 95 $ with a $0.4$ volume fraction. For the \ac{nn}, we set the number of convolutional layers to $5$, $N_{\boldsymbol{\Theta}} = 64$, $w = (12,12,24,46,96,96)$, $l = (12,12,24,46,96,96)$ and $c = (16 , 128, 64, 32, 16, 1 )$.  %\hypertarget{implementation-aspects}{%

The second problem studied is the typical MBB problem described in \cite{Sigmund2001} with the setup as in Figure \ref{fig:MBB-problem-setup}. For this problem, we set $\nu=0.3$, $E=1 Pa$, $\alpha_d = 0.001$ and $F=1 N$ and use a mesh of $ 287 \times 95 $ with a $0.4$ volume fraction.  We use the same network as in the poisson problem but set $w = (36,36,64,128,256,256)$


\subsubsection{Optimized Structures}
To compare the SIMP and \ac{ls} results, the optimized density is converted to a \ac{ls} by taking the 0.5 isosurface of the density and recomputing the objective function value. 

\begin{table*}%[h!]
	\begin{center}
	  %\caption{Method comparison}
	  \label{fig:comparison}
	  \begin{tabular}{c c c c c } %c c c c }
	    & NN-LS & Pixel-LS & NN-SIMP & Pixel-SIMP \\
		    Heat &
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{NN_LS_heat.png}} & 
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{P_LS_heat.png}} &
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{NN_SIMP_heat.png}} &
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{P_SIMP_heat.png}} \\ 
	    
	    &  0.0\% & 20.3\% & 5.8\% & 15.7\% \\
	    MBB  & 
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{NN_LS_MBB.png}} & 
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{P_LS_MBB.png}}&%P_LS_MBB.png}} &
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{NN_SIMP_MBB.png}} &
	    \raisebox{-.5\height}{\includegraphics[width=0.2\textwidth]{P_SIMP_MBB.png}} \\ 
    
	    &  2.6\% & 1.7\% & 0.2\% & 0.0\% \\
    

    
	  \end{tabular}
	\end{center}
	\captionof{figure}{Optimized geometries for the various methods. The percentage under each geometry represents its performance relative to the best performing geometry in the row as measured by the objective function.} \label{table:mul-heat}
	\label{table:mul} 
      \end{table*}

The optimized geometries using the various methods are seen in Figure \ref{table:mul}. In all cases, the neural parameterization results in more regular geometries. The pixel-based methods could be regularized by augmenting the objective function with a penalization term although this would require manual tuning of a penalization parameter and may have an impact on the convergence. Simplistic structures could also be obtained for the pixel-based cases by controlling the filter radius and mesh resolution although this would prevent fine-scale structure. The NN-LS method instead allows for fine-scale features resolved in the final layer of the network but still produces performant regular geometries as the multi-scale influence of the parameters in the neural parameterization encourages globally performant structures to emerge. %occur rather than adding local features to a global structure fixed early in the optimization as seen most clearly in the comparison between the NN-SIMP and Pixel-SIMP results for the heat conduction problem in Figure \ref{fig:comparison} 
The use of the \ac{nn} is also seen to suppress numerical artifacts observed in the pixel-LS solutions. 
For the MBB problem, the compliance measurements for all methods fall within a few percent of each other. For the heat conduction problem, however, the NN-LS method has the best performance by a fair amount followed by the NN-SIMP baseline. The regular geometries produced by the \ac{nn}s in these cases outperform their pixel counterparts by a significant amount in both the \ac{ls} and SIMP cases. The U-Net here seems able to find better minima because of its ability to focus on larger scale structure while the pixel-based parameterization makes improvements locally by adding finer scale branches to the geometry. 

\subsubsection{Convergence Plots}
The convergence of the methods is plotted for the heat conduction and MBB problems in Figure \ref{fig:convergence}. Since the use of the word iteration in the context of optimization is somewhat ambiguous, we plot the compliance against the number of objective function value calls for the process. For the ADAM and MMA optimizers, the ratio of function and gradient calls is 1:1.  
To compare the SIMP and \ac{ls} methods against each other, we bias the SIMP method using the optimized structures converted \ac{ls} once again. 
The bias is computed as the difference between the SIMP method final objective value using the interpolated material and the SIMP method final objective value using the converted \ac{ls}. This bias is applied to all of the series data in Figures \ref{fig:convergence} for the SIMP methods.   

\begin{figure}%
    \centering
    \begin{subfigure}{0.5\textwidth}
    \centering
		\includegraphics[width=0.99\linewidth]{its_heat.tikz} 
        \caption{Heat conduction}
        \label{fig:heat-convergnce}
    \end{subfigure}%
    \begin{subfigure}{0.5\textwidth}
    \centering
		\includegraphics[width=0.99\linewidth]{its_MBB.tikz} 
        \caption{MBB}
        \label{fig:MBB-convergence}
    \end{subfigure}
    \caption{Convergence plot comparison for the benchmark problems. The objective value at each objective function call is plotted. }
    \label{fig:convergence}
\end{figure}

For the heat conduction problem in Figure \ref{fig:heat-convergnce}, the pixel-SIMP method converges the fastest, albeit to an inferior solution. The rest of the methods converge at similar rates.  For the MBB problem in Figure \ref{fig:MBB-convergence}, the SIMP methods converge much faster than the \ac{ls} methods although the NN-SIMP method jumps out of the minimum at later iterations and stabilizes later on.