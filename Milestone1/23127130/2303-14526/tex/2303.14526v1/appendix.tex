\section{Implementation Details}
\noindent In addition to the implementation details introduced in Section 4.2 of the main paper, we provide more information of training our S5 model and LSMCL below.
\subsection{S5 model}
\noindent Following ViS4mer~\cite{islam2022long}, we introduce a MLP layer in each block to reduce the feature dimension by a factor of $2\times$. Each MLP layer is consisted of a linear layer, a GELU activation layer~\cite{hendrycks2016gaussian} and a dropout layer, where the dropout rate is 0.2. For updating the momentum S4 model, we explore different values of momentum coefficient and set it as $0.01$ to produce the best performance. For all of our experiments of S5 model, we use AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $10^{-3} \times \frac{\text{batch size}}{16}$, and with a weight decay of $0.01$. For COIN~\cite{tang2019coin, tang2020comprehensive}, Breakfast~\cite{kuehne2014language} and each task on LVU dataset~\cite{lvu2021}, we train our S5 model for 100 epochs and reduce the learning rate by a factor of $0.2$ when the training loss has stopped reducing in the past $1$ epoch. We train our S5 model by using $8\times$ NVIDIA Tesla V100 $16\text{G}$ GPUs with a batch size of $16$. All the implementations are coded with PyTorch~\cite{paszke2019pytorch}.

\subsection{LSMCL}
\noindent In LSMCL, we sample two clips with different sampling strides from the video sequence, and the clip shape is consistent with the one in finetuning the S5 model. Specifically, we sample input clips of size $60\times 3 \times 224 \times 224$ on LVU dataset~\cite{lvu2021} and $64\times 3 \times 224 \times 224$ on COIN~\cite{tang2019coin, tang2020comprehensive} and Breakfast datasets~\cite{kuehne2014language}. The sampling stride ratio is set to be $\frac{\tau_L}{\tau_S}=1.5$, which is also ablated in the Figure 5(c) in the main paper. Following LSTCL~\cite{wang2022long}, we adopt a query encoder and a key encoder in the LSMCL. The query encoder consists of a S4 model backbone, a MLP projection head and an additional prediction MLP head. The purpose of the prediction layer is to transform the representation of the query clip to match the key. The key encoder consists of a S4 model backbone and a MLP projection head. The momentum coefficient for updating the key encoder is $0.99$. Following~\cite{wang2022long,chen2021empirical}, the MLP projection head has $3$ layers while the MLP prediction head has $2$ layers. The hidden layers of both MLPs are $4096$-D and are with ReLU; the output layers of both MLPs are $256$-D, without ReLU. In LSMCL, all layers in both MLPs have BN~\cite{ioffe2017batch}, which follows~\cite{chen2021empirical,chen2020simple}. In terms of the optimizer, we adopt AdamW~\cite{loshchilov2017decoupled} with a learning rate of $10^{-4} \times \frac{\text{batch size}}{256}$, and with a weight decay of $0.05$. We train our LSMCL for $300$ epochs in total, and adopt learning rate warm-up~\cite{goyal2017accurate} for the first $40$ epochs. We train LSMCL by using $8\times$ NVIDIA Tesla V100 $16\text{G}$ GPUs with a batch size of $64$ and we optimize the model with the loss in Equation 8, where the $\rho=0.2$. 

% \section{Ablation Study}
\begin{figure}[!htb]
\centering
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/MC_S5.pdf}
%   \caption{}
%   \label{MC_S5}
% \end{subfigure}
% \begin{subfigure}{.235\textwidth}
  \includegraphics[width=0.65\linewidth]{figures/Multi_S5.pdf}
    % \caption{}
%   \label{multiS5}
% \end{subfigure}
\caption{Compared to the baseline performance, average improvement performance of our method with different settings on LVU dataset. Unless otherwise noted, the default number of input frame and masking ratio is 60 and $50\%$. We study the effect of leveraging multiple S5 models in our work, where we substitutes more S4 model in orignial ViS4mer~\cite{islam2022long} with our S5 model.}
\label{multiS5}
\end{figure}

% \subsection{Momentum Coefficient in S5 Model}
% \noindent In this paper, we propose the Selective S4 (S5) model, by introducing a selective mask generator. As is shown in the Figure 3(a) in the main paper, the mask generator leverages the S4 feature produced by the momentum S4 model, which is guided by the momentum distillation from the S4 model. Given the parameter of S4 model and momentum S4 model can be represented as $\phi_{\text{S4}}$ and $\phi_{\text{mS4}}$ respectively, the momentum distillation can be written as: $\phi_{\text{mS4}} = m\phi_{\text{mS4}} + (1-m)\phi_{\text{S4}}$, where the $m$ is the momentum coefficient. In Figure~\ref{MC_S5}, we ablate the impact of momentum coefficient used for updating the momentum S4 model. Similar to previous ablation studies, we set the performance of ViS4mer~\cite{islam2022long} as the baseline, and report the averaged impermanent percentage over 9 tasks on LVU dataset~\cite{lvu2021} with different momentum coefficients. From the Figure~\ref{MC_S5}, the optimal value is $m=0.01$.

\section{Effectof Multiple S5 Models}
\noindent In this paper, we improve the previous S4 model by introducing a novel selective module, formulating the Selective S4 (S5) model. For fair comparison, we follow the architecture introduced in the ViS4mer~\cite{islam2022long}, which utilizes three S4 models with pooling and MLP layers in between. As the advantages of our S5 model will naturally be diminished on less redundant sequences, our default setting is to substitute the first S4 model in ViS4mer~\cite{islam2022long} with our proposed S5 model while keep the rest architecture the same with ViS4mer~\cite{islam2022long}. In this section, we study the impact of using more S5 models in the ViS4mer~\cite{islam2022long} architecture. In Figure~\ref{multiS5}, we gradually increase the number of blocks that use S5 model instead of S4 model. We set the performance of ViS4mer as the baseline, and report the averaged improvement percentage over 9 tasks on LVU dataset~\cite{lvu2021}. Compared to the method of using S4 models, our method achieves substantial improvement by including more S5 models. However, less duplicated sequences will definitely result in a decrease in our S5 model's performance gain, which will lessen the advantage of stacking additional S5 blocks. 