\section{Introduction}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/example10.pdf}
    \vspace{-6mm}
    \caption{\textbf{Illustration of long-form videos --} Evenly sampled frames from two long-form videos, that have long duration (more than 1 minute) and distinct categories in the Breakfast~\cite{kuehne2014language} dataset (grayscale frames are shown for better visualization). The video on top shows the activity of making scrambled eggs, while the one on the bottom shows the activity of making cereal. These two videos heavily overlap in terms of objects (\textit{e.g.}, eggs, saucepan and stove), and actions (\textit{e.g.}, picking, whisking and pouring). To effectively distinguish these two videos, it is important to model long-term spatiotemporal dependencies, which is also the key in long-form video understanding.} 
    % \xy{it shows the problem that for long-form video, when it is subsampled, different actions could be mixed up. However, it is better to indicate the long-form concept here, i.e., you draw a time axis, those video frames are all algined there. Then, by subsampling, the current frames are shown, where they are not discriminative. ``Try to add in concept long-form here''.} fixed
    \label{Fig:example}
    \vspace{-2mm}
\end{figure}

% revised by pichao
%Automatic video understanding continues to be an active research area where a variety of different models have been explored including \textit{e.g.}, two-stream networks~\cite{simonyan2014two,feichtenhofer2017temporal,feichtenhofer2017spatiotemporal}, recurrent neural networks~\cite{veeriah2015differential,zhang2017view,baccouche2011sequential} ,and $3$-D convolutional networks~\cite{tran2015learning,tran2018closer, tran2019video}. 
\noindent Video understanding is an active research area where a variety of different models have been explored including \textit{e.g.}, two-stream networks~\cite{simonyan2014two,feichtenhofer2017temporal,feichtenhofer2017spatiotemporal}, recurrent neural networks~\cite{veeriah2015differential,zhang2017view,baccouche2011sequential} and $3$-D convolutional networks~\cite{tran2015learning,tran2018closer, tran2019video}.
%
%Although, these approaches have shown promising results to model short videos that are typically a few seconds in length, they are significantly less effective in modeling complex long-term spatiotemporal dependencies that often require long-form video understanding (see Figure~\ref{Fig:example} for an illustrative example).
% However, most of these methods focus on short-form videos that are typically a few seconds in length, and cannot effectively handle the complex long-term spatial-temporal dependencies in long-form videos (see  Figure~\ref{Fig:example} for example).
However, most of these methods have primarily focused on short-form videos that are typically with a few seconds in length, and are not designed to model the complex long-term spatiotemporal dependencies often found in long-form videos (see Figure~\ref{Fig:example} for an illustrative example).
%
% \xy{conflict to what you mentioned at the end of first paragraph. You can phrase it as: To overcome the above mentioned problem, there are recent vision transformers designed to model the } fixed
The recent vision transformer (ViT)~\cite{dosovitskiy2020image} has shown promising capability in modeling long-range dependencies,  and several variants~\cite{fan2021multiscale,bertasius2021space, arnab2021vivit, neimark2021video, patrick2021keeping, wang2022deformable, liu2021video} have successfully adopted the transformer architecture for video modeling. However, for a video with $\textrm{T}$ frames and $\textrm{S}$ spatial tokens, the complexity of standard video transformer architecture is $\mathcal{O}(\textrm{S}^2\textrm{T}^2)$, which poses prohibitively high computation and memory costs  when modeling long-form videos. Various attempts~\cite{wu2022memvit,sunlong2022} have been proposed to improve this efficiency, 
%of video transformer in long-form video modeling, 
but the ViT pyramid architecture prevents them from developing long-term dependencies on low-level features.

%Unlike the transformer architecture, a linear complexity approach is ViS4mer~\cite{islam2022long} that uses Structured State-Spaces Sequence (S4) model~\cite{gu2021efficiently} to learn long-term spatiotemporal dependencies in videos.  
% A recent ViS4mer~\cite{islam2022long} method, other than the ViT architecture, applies a Structured State-Spaces Sequence (S4) model~\cite{gu2021efficiently} to show a successful study on the long-term dependencies.
In addition to ViT, a recent ViS4mer~\cite{islam2022long} method has tried to apply the Structured State-Spaces Sequence (S4) model~\cite{gu2021efficiently} as an effective way to model the long-term video dependencies.
%
%However, in this paper, we argue that indiscriminately using all image patches as  ViS4mer~\cite{islam2022long} does in its S4 model results in suboptimal utilization of its memory and computational bandwidths. 
% Nevertheless, we found that the S4 model assumes all image patches are equally important which is sub-optimal for the memory and computation consumption: 
%
%By introducing simple masking techniques, we empirically demonstrate that the S4 model can have different temporal reasoning preferences for different downstream tasks, and can benefit differently by making use of image patches from different space-time locations. 
% we introduced a simple masking technique for the S4 model, revealing different temporal preferences across different downstream tasks.
%
% Therefore, it is not satisfactory to apply the same image token selection method for all long-form video understanding tasks, as ViS4mer~\cite{islam2022long} does. 
% Thus, ViS4mer~\cite{islam2022long} like methods applying the same image token selection for all downstream tasks are not ideally satisfying.
%
However, by introducing simple masking techniques we empirically reveal that the S4 model can have different temporal reasoning preferences for different downstream tasks. This makes applying the same image token selection method as done by ViS4mer~\cite{islam2022long} for all long-form video understanding tasks suboptimal.

% \xy{move the following improvement into the last paragraph, where you summarize how your framework is designed.}
%To tackle this problem, we improve the S4 model in ViS4mer~\cite{islam2022long} with a cost-efficient adaptive token selection block, that we call S5 (\textit{i.e.}, selective S4) model. Our S5 model leverages the output of a simulated linear time-invariant (LTI) system to learn an adaptive mask generator, which improves the efficiency and accuracy of the standard S4 model offering better performance on a variety of standard benchmarks. 

%Although recent works on token reduction~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not,wang2021efficient} share our goal of improving the efficiency of ViT~\cite{dosovitskiy2020image}, however, our work is different from such previous approaches. Most of the previous token reduction approaches are fundamentally designed for images, which is why they process each token independently without incorporating its global context. Direct extension of these works from image to video domain may not be practical, as it is difficult for a light-weight decision maker to capture long-term dependencies and generate good policy. On the other side, increasing the capacity of the decision maker would lose the efficiency gain. In contrast, we show how the long-term dependencies encoded in the sequential output of the simulated LTI system (S4) model can be used to select discriminative image tokens by only employing a lightweight mask generator.

To address this challenge, we propose a cost-efficient adaptive token selection module, termed S$5$ (\textit{i.e.}, selective S$4$) model, which adaptively selects informative image tokens for the S4 model, thereby learning discriminative long-form video representations. 
%
% Previous token reduction methods for efficient image transformers~\cite{yin2021adavit,yin2022vit, meng2022adavit,rao2021dynamicvit,liang2022not,wang2021efficient} highly rely on the dense self-attention calculation, which makes these mask-based token reduction methods~\cite{yin2021adavit,yin2022vit} only theoretically effective and hard for practical implementation.
Previous token reduction methods for efficient image transformers~\cite{yin2021adavit,yin2022vit, meng2022adavit,rao2021dynamicvit,liang2022not,wang2021efficient} heavily rely on a dense self-attention calculation, which makes them less effective in practice despite their theoretical guarantees about efficiency gains.
%
In contrast, our S5 model avoids the dense self-attention calculation by leveraging S4 features in a gumble-softmax sampling~\cite{jang2016categorical} based mask generator to adaptively select more informative image tokens. 
%
Our mask generator leverages S4 feature for its global sequence-context information and is further guided by the momentum distillation from the S4 model.

%rewritten by pichao
%Different from recent token reduction methods for efficient image transformers~\cite{yin2021adavit,yin2022vit, meng2022adavit,rao2021dynamicvit,liang2022not,wang2021efficient}, our method is specific for long-form video understanding based on S4. 
%
%Most previous methods depend on dense self-attention calculation, which makes these mask-based token reduction methods~\cite{yin2021adavit,yin2022vit} only theoretically effective, and hard to implement for practical usage.
%
%In contrast, our proposed mask generator avoids the dense self-attention calculation, making it easy to discard the less informative tokens in practical. In addition, our mask generator leverages the global sequence-context embedded in the S4 features and also benefits from the guidance provided by momentum updating of S4 model itself, and it enjoys dynamic to the long-form video data of different tasks.


%However, due to the noise in the long-form video, the image tokens can be removed or selected incorrectly, which affects the final result.  
%

%The S5 model masking is likely to drop the key tokens and result in a sub-optimal representation. 
% \xy{from Raffay: too much context introduced. Only mention all the previous methods for the hard masking is problematic.}
%
To further improve the robustness and the temporal predictability of our S5 model, we introduce a novel long-short mask contrastive learning (LSMCL) to pre-train our model. 
%
In LSMCL, randomly selected image tokens from long and short clips include the scenario that the less informative image tokens are chosen, and the representation of them are learned to match each other. As a result, the LSMCL not only significantly boosts the efficiency compared to the previous video contrastive learning methods~\cite{wang2022long,recasens2021broaden,feichtenhofer2021large}, but also increases the robustness of our S5 model when dealing with the mis-predicted image tokens. 
%
We empirically demonstrate that the S5 model with LSMCL pre-training can employ shorter-length clips to achieve on-par performance with using longer-range clips without incorporating LSMCL pre-training.

\vspace{0.1cm} \noindent We summarize our \textbf{key contributions} as the following: 
% \xy{reformulate a bit as: we leveraged two core technology novelties, mask generator and LSMCL, and find it is seamlessly suitable to deal with long-form videos, the whole pipeline is novel, while we experimentally demonstrated that using equal weights sampling across time axis as S4 did is suboptimal. ``Merge the current bullet 1''. The current bullet 1 is not strong to stand by its own, even in ICML kind of the learning papers.}fixed
% \begin{itemize}

% \item We present an extensive empirical analysis of how long-term spatiotemporal dependencies and video redundancies affect S4 model, and identify that treating all image-tokens equally as done by S4 can adversely affect its efficiency and accuracy particularly when dealing with long-form videos.


\noindent $\bullet$ We propose a Selective S4 (S5) model that leverages the global sequence-context information from S4 features to adaptively choose informative image tokens in a task-specific way.%, achieving up to $\textbf{4.8\%}$ improvement over baselines while reducing GPU memory usage by $\textbf{25\%}$.

% revised by pichao
\noindent $\bullet$ We introduce a novel long-short masked contrastive learning approach (LSMCL) that enables our model to be tolerant to the mis-predicted tokens and exploit longer duration spatiotemporal context by using shorter duration input videos, leading to improved robustness in the S5 model.

%\item We introduce a novel long-short masked contrastive learning approach (LSMCL) that enables our model to predict longer duration spatiotemporal context by using random masked shorter duration input videos, leading to improved efficiency and the robustness.

\noindent $\bullet$  We demonstrate that two proposed novel techniques (S5 model and LSMCL) are seamlessly suitable and effective for long-form video understanding, achieving the state-of-the-art performance on three challenging benchmarks. Notably, our method achieves up to $\textbf{9.6\%}$ improvement on LVU dataset compared to the previous state-of-the-art S4 method, while reducing the memory footprint by $\textbf{23\%}$.
% Through extensive empirical analysis, we demonstrate the effect of long-term spatiotemporal dependencies and video redundancies on S4 model, and identify that treating all image-tokens equally as done by S$4$ model is sub-optimal.  Furthermore,
% \xy{from Raffay: not concluded properly.}

% \end{itemize}

%\noindent $\bullet$