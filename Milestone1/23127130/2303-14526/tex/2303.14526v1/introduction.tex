\section{Introduction}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/example4.jpg}
    \caption{\textbf{Illustration of long-term video dependencies --} Evenly sampled frames from two videos with distinct categories in the Breakfast~\cite{kuehne2014language} dataset are presented (grayscale frames are shown for better visualization). The video on top shows the activity of making scrambled eggs while the one on the bottom shows the activity of making cereal. The two videos heavily overlap in terms of objects (\textit{e.g.}, eggs, saucepan and stove), and actions (\textit{e.g.} picking, whisking and pouring), demonstrating the importance of modeling the long-term spatiotemporal dependencies among objects and actions in videos to be able to distinguish them effectively.}
    \label{Fig:example}
\end{figure}
% Evenly sampled frames from four videos with distinct categories in the Breakfast~\cite{kuehne2014language} dataset. Can you tell which activity is in each video? From 1 to 4, they are: making scrambled egg, fried egg, pancakges, and cereals. These four videos are highly overlapped in terms of objects, such as egg, saucepan and stove, and short-term actions, such as picking, whisking and cooking. To distinguish them, it is important to model long-term dependencies during the video representation learning.

Video understanding is a well-studied topic in the area of computer vision. Compared to the image, the redundant nature of video sequences makes it a challenging topic to be investigated. In the last few years, we have witnessed the evolution of video understanding methods, such as Two-Stream Neural Networks~\cite{simonyan2014two,feichtenhofer2017temporal,feichtenhofer2017spatiotemporal}, 3D Convolutional Neural Networks (CNNs)~\cite{tran2015learning,tran2018closer, tran2019video}, and recent popular Video Transformers~\cite{wang2022deformable, bertasius2021space, wang2022long, liu2021video, patrick2021keeping}. These works demonstrate promising results in understanding short-form video sequences which are typically less than 10 seconds. However, less efforts are observed in modeling long-form videos, and most works mentioned above are still far from being practically useful in long-form video modeling. They were designed to capture the short-term dynamics instead of the complex long-range temporal reasoning in the long-form video, which is yet to be well explored. Figure~\ref{Fig:example} shows examples in long-form video dataset, which requires to model complex long-term dependencies.

% With the huge success in the area of natural language processing~\cite{devlin2018bert}, transformer architectures has absorbed much attention. Different from the traditional Recurrent Neural Networks~\cite{yue2015beyond,dey2017gate} that are easily subject to the long-term gradient vanish, the transformer architecture demonstrates superior performance in modeling long-term dependencies. This is because each token is treated equally in the self-attention mechanism and the model is able to see the entire sequence from the beginning to the end. 
Followed by the first extension work of Vision Transformer (ViT)~\cite{dosovitskiy2020image}, recent works~\cite{fan2021multiscale,bertasius2021space, arnab2021vivit, neimark2021video, patrick2021keeping, wang2022deformable, liu2021video} successfully adopt the transformer architecture in the video modeling, formulating various video transformers. Nevertheless, all transformer architectures including the video transformer generally have the limitation of high computational cost and GPU memory usage. And this phenomenon would become even worse in the long-form video understanding due to the large number of frames in the input. Given a video sequence that has T frames and S spatial tokens in each frame, the computational cost of the regular video transformer is $\mathcal{O}(S^2T^2)$, which is quartic to the input resolution. Thus, in addition to the effectiveness, the efficiency plays an important role in the practicality of modeling long-form video sequences.

% Transformer architectures has been paid much attention due to its huge success in the area of natural language processing~\cite{devlin2018bert}. Based on this, Dosovitskiy et al~\cite{dosovitskiy2020image} successfully extended this work to the computer vision, formulating the Viion Transformer (ViT). Compared to the traditional CNN architecture, the vision transformer is a convolutional free architecture which decomposites the image into several non-overlapping image patches and uses the self-attention mechanism to learn the final representation. In the case of video, the video transformer~\cite{fan2021multiscale,bertasius2021space, arnab2021vivit, neimark2021video, patrick2021keeping, wang2022deformable, liu2021video} demonstrates huge advantage in modeling both local and global dependencies. This is because all image patches from all frames are treated equally in the self-attention and the model is able to see the entire sequence from the beginning to the end. Nevertheless, all transformer architecture including the video transformer are normally accused by its high computational cost. And this phenomenon becomes even worse in the video transformer due to the redundant nature of video sequences. Given a video sequence that has T frames and S image patch in each frame, the computational cost of regular video transformer is $\mathcal{O}(S^2T^2)$, which is quartic to the input resolution. To model long-term video sequences and capture long-range temporal reasoning, the input normally has a large value of T, which generates significant burden for both memory and cost sides. An example of long-term video is shown in the Figure~\ref{Fig:example}.


% Several attempts have tried to improve the efficiency of video transformer by introducing local shifted windows~\cite{liu2021video} and deformable offset~\cite{wang2022deformable} in the self-attention. Unfortunately, they do not fit the long-form video modeling very well, as the cost of local shifted window attention~\cite{liu2021video} is still high due to the large number of frames and the motion embedding in~\cite{wang2022deformable} would be problematic due to the unpredictable motion dynamic in the long temporal span. 
One successful attempt for improving the efficiency of video transformers in long-form video modeling is introduced in the MeMViT~\cite{wu2022memvit} and LF-VILA~\cite{sunlong2022}, which models long-term reasoning by hierarchically building temporal relations. However, much low-level features are discarded during the pooling operation. Until recently, another promising work, ViS4mer, was introduced by Islam et al.~\cite{islam2022long}. It introduced multi-scaled Structured State-Space Sequence (S4) layer to learn long-term dependencies from space-time image tokens, whose computational cost is linear to the input resolution. Unlike the CNN and Transformer architectures, this work simulates the fundamental state space model (SSM): $x'(t)= Ax(t) + Bu(t), y(t) = Cx(t) + Du(t)$, by learning $A, B, C, D$ via gradient descent. As is shown in the~\cite{islam2022long,gu2021efficiently}, this linear time-invariant (LTI) system could mathematically and empirically handle long-range dependencies from the video data by the right choice of $A$. As as result, ViS4mer~\cite{islam2022long} is able to models the long temporal reasoning by using flattened image tokens from all input frames.

However, in this paper, we argue that not all image patches should be equally fed into the S4 layer, thus consuming memory and computational bandwidth. By introducing simple masking methods, we empirically demonstrate that the S4 model in each long-term video modeling task may have unique temporal reasoning preference, favouring image patches in different space-time locations. Thus, it is not appealing to apply the same image token selection method for different long-form video understanding tasks. To tackle this issue, we improve the S4 model with a cost-efficient adaptive token selection block, dubbed as Selective S4 (S5) model. The S5 model leverages the output of simulated LTI system to learn an adaptive mask generator, which improves the efficiency and effectiveness of the vanilla S4 model and results in better performance in benchmarks. To further improve the temporal predictability while not using even more video frames in our model, we also introduce a novel long-short mask contrastive learning (LSMCL) algorithm for the long-form video modeling, which enables encoding temporal context from the longer video into a shorter-range clip representation.

%  Inspired by the recent self-supervised learning techniques~\cite{wang2022long,he2022masked,tong2022videomae,feichtenhofer2022masked}, we also introduce a novel long-short mask contrastive learning (LSMCL) algorithm for long-form video modeling, which further broadens the temporal horizon of S4 model by capturing longer temporal reasoning with lower computational and memory cost.

% believe that feed the fix-sized input tensor to the S4 layer may not be appealing as different long-term video modeling tasks would have different emphases over spatiotemporal image patches. As a result, we proposed a novel masking strategy that learns to pick meaningful image patches for each particular video tasks. Moreover, after dropping a portion of image patches, our model is able to increase the sampling density for taking care of the fine-gained information in long-term video sequences.

To this end, some recent token reduction works in ViT also share similar motivation with ours~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not,wang2021efficient}. However, most of these works are designed in the image domain, which independently process each token without considering global context. Moreover, they normally decouple the network as one decision maker, learning the policy for weighting/picking tokens, and one encoder that encodes the weighted/picked tokens from the decision maker. This not only makes it difficult for the decision maker to comprehend long-term dependencies, but it also faces the trade-off between the network's efficiency gain and their own capability. In this paper, we show that the long-term dependencies are remembered in the sequential output of the simulated LTI system (S4) model, which can be used to choose discriminative image tokens using a lightweight mask generator. Associated with our proposed long-short mask contrastive learning (LSMCL), our S5 model achieves promising result with cheaper cost in several challenging benchmarks.

Before moving on to explaining our scheme in detail, we summarize below our important contributions. 
\begin{enumerate}
\item Through extensive experiments, we empirically explore long-term dependencies and video redundancy affect the performance of S4 model in various long-form video understanding tasks. 
\item We propose a Selective S4 model for long-form video modeling, which leverages the sequential output from the S4 model. The proposed method effectively picks task-favourable image tokens by learning long-term context from the output of simulated LTI system, which achieves $4.3\%$ averaged improvement over the baseline performance while saving GPU memory usage by $40\%$.
\item We introduce an efficient long-short mask contrastive learning (LSMCL) for long-form video modeling, which enables the model to capture longer dependencies with fewer image tokens. By leveraging the LSMCL, the performance of shorter sequence is on par with the one of using longer sequences.
\item We outperform previous state-of-the-art results in three challenging long-form video modeling benchmarks, demonstrating the generalized effectiveness of our scheme.
\end{enumerate}

