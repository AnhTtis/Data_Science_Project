\section{Background: S4 Model}
\label{background}
We first introduce the Structured State-Space Sequence (S4) Model~\cite{gu2021efficiently}, which is the core architecture in ViS4mer~\cite{islam2022long} and this paper. In the the continuous time space, a simple State-Space Model (a linear time invariant (LTI) system) can be written as:
\begin{align}
\begin{split}
\label{SSM}
&x^\prime (t) = Ax(t)+Bu(t) \\
&y(t)=Cx(t)+Du(t)
\end{split}
\end{align}
Under the deep learning set-up, $A$, $B$ and $C$ are learned via gradient descent, and $+Du(t)$ is replaced by a residual connection. This formulation projects an input signal $u(t)$ from one-dimensional space to an N-dimensional latent space $x(t)$ and then $x(t)$ are mapped to a one-dimensional output signal $y(t)$. Similar with the RNNs, it was found in previous work that Equation~\ref{SSM} would also suffer from gradient vanish/exploding issue when modeling longer sequences. To tackle this issue, Gu et al. leveraged HiPPO theory~\cite{gu2020hippo} to initialize the $A$. The HiPPO specifies a certain expression of $A\in \mathbb{R}^{N\times N}$ (see Equation~\ref{HiPPO}), which allows the hidden state to memorize the input $u(t)$ \footnote{Please refer~\cite{gu2020hippo} for more details and proof.}. %by storing $N$ Legendre polynomial coefficients
\begin{equation}
\label{HiPPO}
\text{HiPPO:}\ A_{nk}= -
\begin{cases}
  (2n+1)^{0.5}(2k+1)^{0.5} & \text{if}\ n>k \\
  n+1 & \text{if}\ n=k\\
  0  & \text{if}\ n<k
\end{cases}
\end{equation}
Where $n$ and $k$ indicates the index of row and column in $A$. To implement Equation~\ref{SSM} in discrete input (such as word and image tokens), \cite{gu2021efficiently} leverages the bi-linear method~\cite{tustin1947method} and discretized the Equation~\ref{SSM} by a step size $\Delta$. Then, the Equation~\ref{SSM} can be rewritten as:
\begin{align}
\begin{split}
\label{S4}
&x_k = \Bar{A}x_{k-1}+\Bar{B}u_k \\
&y_k=\Bar{C}x_k
\end{split}
\end{align}
Where $\Bar{A}=\frac{(I+\frac{\Delta\cdot A}{2})}{(I-\frac{\Delta\cdot A}{2})}$, $\Bar{B}=\frac{\Delta\cdot B}{(I-\frac{\Delta\cdot A}{2})}$ and $\Bar{C} = C$.

The Equation~\ref{S4} can be executed as a discrete convolution~\cite{gu2021efficiently}:
\begin{align}
\begin{split}
\label{S4_conv}
y_k &= \Bar{C}\Bar{A}^k\Bar{B}u_0 + \Bar{C}\Bar{A}^{k-1}\Bar{B}u_1 + \dots + \Bar{C}\Bar{A}\Bar{B}u_{k-1} + \Bar{C}\Bar{B}u_k \\
\boldsymbol{y} &= \boldsymbol{\Bar{K}} * \boldsymbol{u}
\end{split}
\end{align}
Where $\boldsymbol{u}=\{u_0,u_1,\dots,u_{k-1},u_k\}$ and $\boldsymbol{\Bar{K}} \in \mathbb{R}^L := \{\Bar{C}\Bar{B}, \Bar{C}\Bar{A}\Bar{B},\dots,\Bar{C}\Bar{A}^{L-1}\Bar{B}\}$ is a structured convolutional kernel and $L$ is the sequence length. The Equation~\ref{S4_conv} is the core formulation of S4 model, whose computational cost is linear to the input length, and it can be fast computed by leveraging the fast Fourier transform (FFT) and inverse FFT. Moreover, to find the correct value of $\Delta$, Gu et al.~\cite{gu2021combining} set $\Delta$ as a learnable parameter, controlling the width of the convolution kernel.


\section{Technical Approach}
% \begin{table*}[!htb]
% \small
% \centering
% \begin{tabular}{|l|l|l|lll|llll|ll|}
% \hline
% \multirow{2}{*}{Model} & \multicolumn{1}{c|}{\multirow{2}{*}{Frames}} & \multirow{2}{*}{Mask} & \multicolumn{3}{c|}{Content ($\uparrow$)}                                       & \multicolumn{4}{c|}{Metadata ($\uparrow$)}                                                                    & \multicolumn{2}{c|}{User ($\downarrow$)}        \\ \cline{4-12} 
%                       & \multicolumn{1}{c|}{}                        &                       & \multicolumn{1}{l|}{Relation} & \multicolumn{1}{l|}{Speak} & Scene & \multicolumn{1}{l|}{Director} & \multicolumn{1}{l|}{Genre} & \multicolumn{1}{l|}{Writer} & Year  & \multicolumn{1}{l|}{Like} & View \\ \hline
% ViS4mer~\cite{islam2022long}                & 60                                           & -                     & \multicolumn{1}{l|}{$57.14$}    & \multicolumn{1}{l|}{$40.79$} & $67.44$ & \multicolumn{1}{l|}{$62.61$}    & \multicolumn{1}{l|}{$54.71$} & \multicolumn{1}{l|}{$48.80$}  & $44.75$ & \multicolumn{1}{l|}{$0.26$} & $3.63$ \\ \hline
% ViS4mer                & 60                                           & 50\%                  & \multicolumn{1}{l|}{$54.81$}     & \multicolumn{1}{l|}{$38.22$} & $67.44$ & \multicolumn{1}{l|}{$63.60$}     & \multicolumn{1}{l|}{$54.97$} & \multicolumn{1}{l|}{$47.00$}  & $42.70$ & \multicolumn{1}{l|}{$0.25$} & $4.00$ \\ \hline
% ViS4mer                & 120                                          & -                     & \multicolumn{1}{l|}{$57.14$}    & \multicolumn{1}{l|}{$36.68$} & $68.61$ & \multicolumn{1}{l|}{$61.68$}    & \multicolumn{1}{l|}{$57.06$} & \multicolumn{1}{l|}{$50.00$}  & $45.50$  & \multicolumn{1}{l|}{$0.26$} & $3.63$ \\ \hline
% ViS4mer                & 120                                          & 50\%                  & \multicolumn{1}{l|}{$59.50$}     & \multicolumn{1}{l|}{$40.79$} & $68.60$  & \multicolumn{1}{l|}{$61.68$}    & \multicolumn{1}{l|}{$58.80$} & \multicolumn{1}{l|}{$48.80$}   & $47.55$ & \multicolumn{1}{l|}{$0.26$} & $4.00$ \\ \hline
% \end{tabular}
% \caption{Performance of ViS4mer~\cite{islam2022long} in LVU~\cite{lvu2021} dataset, where we double and half the default number of frames and image patches.}
% \label{concept_result}
% \end{table*}
\begin{figure}[]
\centering
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/mask_ratio.png}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/num_frame.png}
\end{subfigure}
\caption{Performance improvement of ViS4mer on LVU dataset~\cite{lvu2021}. Top: Compared to the experiment using input of 60 seconds, we show the performance improvement ratio with increasing temporal extent, such as 80, 100 and 120 seconds. Bottom: Compared to the experiment of using 60 seconds' input without random masking, we show the performance improvement ratio of using the same temporal extent with increasing masking ratio, such as $20\%$, $50\%$ and $80\%$.}
\label{concept_result}
\end{figure}



Given a video clip $X \in \mathbb{R}^{H \times W \times 3 \times T}$ consisting of $T$ RGB frames sampled from the video, we convert it into a sequence of $S \cdot T$ image tokens ${\bf x}_s^t \in  \mathbb{R}^D$ for $s=1, \hdots, S$ and $t=1, \hdots, T$. The tokens ${\bf z}_s^t$ are obtained by decomposing each frame into $S$ patches which are then projected to a $D$-dimensional space through a learnable linear transformation. This tokenization can be implemented by linearly mapping the RGB patches of each frame~\cite{bertasius2021space,neimark2021video}. Separate learnable positional encodings ${\bf e}_s$ and ${\bf e}^t$ are then applied to the patch embeddings ${\bf z}_s^t$ for the spatial and the temporal dimension: ${\bf x}_s^t = {\bf z}_s^t + {\bf e}_s + {\bf e}^t$, formulating $\boldsymbol{x_{input}} = \{x_0^0, x_1^0, x_S^0, x_0^1,\dots,x_S^T \} $.

In ViS4mer~\cite{islam2022long}, a multi-scale S4 decoder is introduced for learning the long-term temporal reasoning. As is mentioned in the Section~\ref{background}, S4 model has a linear computation and memory dependency with respect to the input length, which has significantly lower computational cost than the self-attention in transformers. The formulation of S4 decoder can be written as:
\begin{align}
\label{vis4mer}
\begin{split}
&{\bf x}_{s_4} = S_4 \left(\text{LN}\left({\bf x}_{input}\right)\right) \\
&{\bf x}_{mlp} = MLP \left(P\left({\bf x}_{s_4}\right) \right) \\ 
&{\bf x}_{skip} = Linear \left(P\left({\bf x}_{input}\right) \right) \\
&{\bf x}_{out} = {\bf x}_{skip} + {\bf x}_{mlp}
\end{split}
\end{align}
Where $LN(\cdot), MLP(\cdot), Linear(\cdot)$ and $P(\cdot)$ represents the layer normalization~\cite{ba2016layer}, the multi-layer perception, linear layer and pooling layer, and ${\bf x}_{s_4}$ is the $\boldsymbol{y}$ in Equation~\ref{S4_conv}. From the recurrent expression in the Equation~\ref{S4} and~\ref{S4_conv}, it is known that each output signal from S4 model is the result of structured discrete convolution for all previous inputs. Given the ViS4mer feeds image tokens from all space-time locations into the S4 model, we argue that it is not efficient and appealing for S4 model capturing effective long-term dependencies, as not all tokens have the temporal relations and each task may also favour tokens in different space-time locations.  From the observation that short-form video understanding tasks often benefit from longer input clips, we wonder if the performance of different long-form video tasks would also be substantially improved with the increasing number of input frames.
% The $S_4$ represents the structured state-space sequence model which can be written as:
% \begin{align}
% \begin{split}
% &x^\prime (t) = Ax(t)+Bu(t) \\
% &y(t)=Cx(t)+Du(t)
% \end{split}
% \end{align}
% Where A, B, C and D are learnable parameters and updated by using gradient decent. This formulation projects an input signal $u(t)$ from one-dimensional space to an N-dimensional latent space $x(t)$ and then $x(t)$ are mapped to a one-dimensional output signal $y(t)$. As is widely used in other works~\cite{gu2021efficiently}, it also leverages HiPPO theory~\cite{gu2020hippo} to reduce the computational cost and avoid gradient vanish/exploding. As S4 model is not key in this paper, please refer to~\cite{gu2021efficiently} for more details. 
% Although ViS4mer achieves promising result in long-term video modeling tasks, it includes all image patches in the S4 decoder, which we believe is not appealing. 

To better understand the S4 model and long-form video understanding tasks, we re-implement ViS4mer~\cite{islam2022long} with different settings in LVU dataset~\cite{lvu2021} and demonstrate the result in Figure~\ref{concept_result}. In Figure~\ref{concept_result}(a), to reduce the number of tokens, we generate random masks on the 60 seconds' input clips and increase the masking ratio from $20\%$ to $80\%$. Compared to the performance of un-masked input, we report the impact ratio of using random mask with masking ratio of $20\%$, $50\%$ and $80\%$. Similarly, in Figure~\ref{concept_result}(b), we gradually increase the temporal extent from $60$ seconds to $120$ seconds. Compared to the performance of using 60 seconds' input, we report the impact ratio of using $80$, $100$, $120$ seconds' temporal extent in each task.  From Figure~\ref{concept_result}, we realize that the performance of some tasks in the LVU are improved with increasing temporal content, while the performance of rest tasks fluctuate with it. We believe this is due to the conflict between additional temporal content and useless tokens introduced by extra frames. Although random masking generates minor improvement in the task of genre, it degenerates the performance of most tasks, and thus, not appealing for improving the effectiveness of S4 model.  To this end, we are motivated to propose a selective S4 model which adaptively pick discriminative image tokens for the S4 model in different long-form video understanding tasks. We hypothesize that various long-form video comprehension tasks would call for diverse data distributions that are created by different image tokens in the video. In the S4 model, the same group of image tokens that is discriminative in one LTI system (for one task) might be treated as randomly interpolated noise signal in another LTI system (for a different task).


% where we use different length of input video clips with randomly generated masks. Specifically, we set a very high masking ratio as $50\%$, which means we randomly abandon half of image tokens. The LVU dataset and implementation details will be introduced in the Section~\ref{exp}. From the Table~\ref{concept_result}, we surprisingly find that 1) the performance of some LVU tasks are not improved with increasing the number of input frames. 2) after dropping a large portion of image patches from the input clip, the performance in some LVU tasks are even improved. This confirms our hypothesis that various long-term video modeling tasks have different space-time preferences in image tokens. From the perspective of the LTI system, randomly sampled image tokens biased over particular frequencies of the input signal in the frequency domain, and thus augmenting the video feature in some tasks. In addition, the redundancy nature of long video sequences makes it challenging for S4 models to learn robust LTI system for handling the input signal with noise interpolated. 

\begin{figure*}[]
\centering
% \begin{subfigure}{.59\textwidth}
  \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/pipeline4.png}
%   \caption{}
%  \label{Fig:pipeline}
% \end{subfigure}
% \begin{subfigure}{.39\textwidth}
% \centering
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/LSMCL.png}
%   \caption{}
%  \label{LSMCL}
% \end{subfigure}
\caption{(a) A visualization of our proposed S5 model. Compared to the S4 model, we introduce a selective token picking strategy, which leverages the S4 feature from the shadow S4 model. The shadow S4 model is updated by the S4 model in the manner of moving average. Both S4 model and shadow S4 model are consisted of a S4 layer~\cite{islam2022long, gu2021efficiently} and a LN layer~\cite{ba2016layer}. (b) An illustration of proposed LSMCL.}
\label{Fig:pipeline}
\end{figure*}



\noindent\textbf{Adaptive Token in Long-form Videos:} To this end, we extend the concept of adaptive token learning into this problem, which aim at picking out discriminative image tokens from the long-form video in various tasks. Unlike previous adaptive token learning works~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not} that generate independent patch-wised policy, we leverage the sequential output from S4 model to make decisions. This has a few advantages as 1) the feature from S4 model could provide long-term contextual information, which could improve the robustness of the decision maker; 2) the S4 feature is generated through S4 model with forward pass only, so the extra computational cost is negligible; 3) unlike the raw input, this feature is dynamic changed with the updates of network, which continuously feedback the decision maker. Figure~\ref{Fig:pipeline} demonstrates the pipeline of our S5 model, in which we equip the S4 model with an adaptive token selection mechanism. As the S4 feature embed rich temporal relations, we formulate a shadow S4 model to generate the input to our token selection model. This shadow S4 model is the moving-averagely updated by the S4 model. 
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\linewidth]{CVPR2023/latex/figures/pipeline2.png}
%     \caption{A visualization of our proposed S5 model. Compared to the S4 model, we introduce a selective token picking strategy, which leverages the S4 feature from the shadow S4 model. The shadow S4 model is updated by the S4 model in the manner of moving average. Both S4 model and shadow S4 model are consisted of a S4 layer~\cite{islam2022long, gu2021efficiently} and a LN layer~\cite{ba2016layer}.}
%     \label{Fig:pipeline}
% \end{figure*}





%  Left: our improved S4 model with efficient adaptive token masking scheme. Right: our proposed long-short mask contrastive learning scheme, which forces the model to capture long and short term dependencies with fewer randomly sampled image tokens.

% \begin{algorithm}[!ht]
% \caption{Pytorch Like Pseudo Code}\label{algorithm1}

% \textcolor{Brown}{\# max\_depth: the depth of intermediate layer}\\
% \textcolor{Brown}{\# $MG$: mask generator}\\
% \textcolor{Brown}{\# $layers$: stacked S4 models}\\
% \textcolor{Brown}{\# $layer_{shadow}$: shadow S4 model to produce S4 feature}\\
% \textcolor{Brown}{\# $head$: head layer for output}\\
% \textcolor{red}{for} $x$ \textcolor{red}{in} $dataloader$: \textcolor{Brown}{\#load batch data from the data loader}\\
% \hspace*{5mm}$x_{mask}$ = $x$ \\
% \hspace*{5mm} \textcolor{red}{with} $torch.no\_grad()$: \textcolor{Brown}{\#no gradient calculated}\\
% % \hspace*{5mm}\hspace*{5mm} \textcolor{red}{for} $depth, laye$r \textcolor{red}{in} \textcolor{Blue}{enumerate} ($layers$):\\\underline{\ \ \ } , 
% \hspace*{5mm}\hspace*{5mm} $x_{S_4}$ = $layer_{shadow}$($x_{mask}$)\\
% \hspace*{5mm}$mask$ = $MG$ ($x_{S_4}$) \textcolor{Brown}{\#learn the mask}\\
% \hspace*{5mm}$x_{in}$ = $x \otimes mask$ \textcolor{Brown}{\#see Equation~\ref{E_masking} and~\ref{straight-through}}\\
% % \hspace*{5mm}\hspace*{5mm}\hspace*{5mm} \textcolor{red}{if} $depth$ == $max\_depth$:  \textcolor{Brown}{\#return the feature}\\
% % \hspace*{5mm}\hspace*{5mm}\hspace*{5mm}\hspace*{5mm}\textcolor{red}{break} \\
% \hspace*{5mm} \textcolor{red}{for} $depth, layer$ \textcolor{red}{in} \textcolor{Blue}{enumerate} ($layers$):\\
% % \hspace*{5mm} \hspace*{5mm}$mask$ = $MG$ ($x_{S_4}$) \textcolor{Brown}{\#learn the mask}\\
% % \hspace*{5mm} \hspace*{5mm}$x_{in}$ = $x \otimes mask$ \textcolor{Brown}{\#see Equation~\ref{E_masking} and~\ref{straight-through}}\\
% \hspace*{5mm}\hspace*{5mm} $x_{in}$ = $layer$($x_{in}$) \textcolor{Brown}{\#forward pass}\\
% \hspace*{5mm} $x_{out}$ = $head$ $(x_{in})$ \\
% \end{algorithm}

We cast our algorithm as an adaptive mask learning problem. Given a mask generator $MG(\cdot)$ and its input ${\bf x}_{s_4}$, the mask generator is trained for a classification task on predefined category space $\mathbb{C}=\{C_1,\dots,C_{ST}\}$, where $S\cdot T$ is the total number of image tokens in the video. Let's denote $p(c|{\bf x}_{s_4}) \in [0,1]$ be the normalized probabilistic output of $MG({\bf x}_{s_4})$, so that $\sum_{c=C_1}^{c=C_{ST}} p(c|{\bf x}_{s_4}) = 1$. Then, we sample $K$ categories without replacement from the probabilistic outputs of the mask generator. Finally, the $k^{th}$ selected image tokens can be written as:
\begin{align}
\begin{split}
\label{E_masking}
x_{in}^k = X^Tc^k
\end{split}
\end{align}
Where $X \in \mathbb{R}^{ST\times D}$ represents $S\cdot T$ D-dimensional image tokens and $c^k$ is a one-hot vector that select $k^{th}$ token from the $X$. The sampling process is important as it prevents the bias in the training that is potentially caused by the top-K selection. To make this sampling differentiable, we adopt the Gumbel-Softmax with Straight-Through tricks~\cite{jang2016categorical}, which is widely used in~\cite{lin2021vx2text,meng2022adavit}. Specifically, we introduce an additional gumbel noise $g \in \mathbb{R}^{1\times ST}$ into the predicted probability distribution $p \in \mathbb{R}^{1\times ST}$, where $g = -log(-log(u+\epsilon)+\epsilon)$ ($u\sim \text{Uniform(0,1)}$ , and $\epsilon$ is a small value to increase the robustness). Then, we sample the top-K tokens from the re-parameterized distribution $p+g$. During the back-propagation, we estimate the gradient for each selected token $c$ as:
\begin{align}
\begin{split}
\label{straight-through}
G\approx \bigtriangledown_{MG}\frac{exp((log p(c|{\bf x}_{s_4}) + g(c))/\rho)}{\sum_{c'=C_1}^{c'=C_{ST}}exp((log p(c'|{\bf x}_{s_4}) + g(c'))/\rho)}
\end{split}
\end{align}
Where $\rho$ is the temperature hyper-parameter that controls the sharpness.

\noindent\textbf{Long-short Mask Contrastive Learning (LSMCL):} Another direction to improve the temporal predictability of our S5 model is from the observation of recent video contrastive learning works~\cite{recasens2021broaden,wang2022long,Feichtenhofer_large}. Especially, both LSTCL~\cite{wang2022long} and BraVe~\cite{recasens2021broaden} leverage the short and long clips from the same video, formulating positive pairs in the contrastive learning. This essentially forces the model to learning long-term representation from short clips and vice versa. In this paper, we investigate this idea on S4 model and further improve it by introducing masking strategy, which better fits our S5 in the fine-tuning. Specifically, we sample a long clip $(x_L)$ and a short clip$(x_S)$ from each video sequence with largely different sampling stride $\tau_L$ and $\tau_S$, where $\tau_S < \tau_L$. Unlike LSTCL and BraVe that applied independent random sampling, the temporal span of long clips includes the one of short clips in out paper, which prevents dissimilar semantics from two clips in long-form videos. In addition, we independently generate binary random masks with masking ratio of $\eta$ for each clip, which can be written as: $x'=mask(x,\eta), x\in \{x_L,x_S\}$. We set S4 model as the backbone of the query encoder $(f_q)$ and also adopt a momentum key encoder $(f_k)$ in the pipeline, which is widely accepted in MoCo~\cite{he2020momentum}, BYOL~\cite{grill2020bootstrap} and LSTCL~\cite{wang2022long}. Our query encoder and key encoder follow the same design with\cite{he2020momentum,grill2020bootstrap,wang2022long}, that consist of the backbone, projection and prediction heads. Formally, denoting the parameter of $f_q$ is $\theta_q$ and the one of $f_k$ is $\theta_k$, we have: $\theta_k = m\theta_k + (1-m)\theta_q$, where $m \in [0,1]$ is a momentum coefficient. Similarly, the LSMCL is optimized by minimizing the  InfoNCE~\cite{oord2018representation} loss:
\begin{equation}
\label{nce}
    \mathcal{L}_{NCE} = \sum_{i} -log\frac{exp({q^i}^\top k^i/\rho)}{exp({q^i}^\top k^i/\rho)+\sum_{j \neq i}exp({q^i}^\top k^j/\rho)}
\end{equation}
 AWhere $q=f_q(x_S')$, $k=f_k(x_L')$ and $\rho$ is the temperature hyper-parameter. As is commonly done in the~\cite{chen2021empirical, chen2020exploring, grill2020bootstrap,caron2020unsupervised}, we symmetrize the loss function by switching $x_S'$ and $x_L'$ in $f_q$ and $f_k$. In our LSMCL, the S4 model is learned to find correct step size $\Delta$ and SSM parameters to match the representation of long and short clips with randomly sampled sparse image tokens. Given our S5 model takes adaptively learned image tokens in the downstream task, we believe the LSMCL could improve the robustness as well as the temporal modeling ability of S5 model when dealing with partially sampled image tokens. In the Section~\ref{exp}, it is empirically demonstrated that the performance of S5 model with LSMCL is significantly improved in the long-form video understanding. An illustration of our lSMCL algorithm can be found in the supplementary materials. 