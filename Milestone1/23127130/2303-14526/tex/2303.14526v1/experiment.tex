\begin{table*}[!htb]
\small
\centering
\begin{tabular}{|l|lll|llll|ll|l}
\cline{1-10} 
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Mask \\ Generator\end{tabular}} & \multicolumn{3}{c|}{Content ($\uparrow$)}                                       & \multicolumn{4}{c|}{Metadata ($\uparrow$)}                                                                    & \multicolumn{2}{c|}{User ($\downarrow$)}        \\ \cline{2-10} 
                                & \multicolumn{1}{l|}{Relation} & \multicolumn{1}{l|}{Speak} & Scene & \multicolumn{1}{l|}{Director} & \multicolumn{1}{l|}{Genre} & \multicolumn{1}{l|}{Writer} & Year  & \multicolumn{1}{l|}{Like} & View \\ \cline{1-10} 
No Mask (ViS4mer~\cite{islam2022long})                          & \multicolumn{1}{l|}{$57.14$}    & \multicolumn{1}{l|}{$40.79$} & $67.44$ & \multicolumn{1}{l|}{$62.61$}    & \multicolumn{1}{l|}{$54.71$} & \multicolumn{1}{l|}{$48.80$}  & $44.75$ & \multicolumn{1}{l|}{$0.26$} & $3.63$ \\ \cline{1-10} \cline{1-10} 
Random                          & \multicolumn{1}{l|}{$54.81$}    & \multicolumn{1}{l|}{$38.22$} & 67.44 & \multicolumn{1}{l|}{$63.60$}    & \multicolumn{1}{l|}{$54.97$} & \multicolumn{1}{l|}{$47.00$}  & $42.70$ & \multicolumn{1}{l|}{$0.25$} & $4.00$ \\ \cline{1-10} \cline{1-10} 
Single TX             & \multicolumn{1}{l|}{$57.85$}    & \multicolumn{1}{l|}{$40.79$} & $68.66$ & \multicolumn{1}{l|}{$63.98$}    & \multicolumn{1}{l|}{$55.12$} & \multicolumn{1}{l|}{$48.85$}  & $43.46$ & \multicolumn{1}{l|}{$0.26$} & $3.82$ &\tikzmark{e}  \\ 
$\textrm{Single TX}_{\textrm{S4}}$             & \multicolumn{1}{l|}{$\bf 60.54$}    & \multicolumn{1}{l|}{$\bf41.21$} & $\bf69.83$ & \multicolumn{1}{l|}{$\bf66.43$}    & \multicolumn{1}{l|}{$\bf57.55$} & \multicolumn{1}{l|}{$\bf49.47$}  & $\bf44.15$ & \multicolumn{1}{l|}{$\bf0.25$} & $\bf3.51$ &\tikzmark{f}$ \bf{\textcolor{ForestGreen}{+3.4}}$ \\ \cline{1-10} \cline{1-10} 

Stacked TXs         & \multicolumn{1}{l|}{$59.51$}    & \multicolumn{1}{l|}{$41.21$} & $69.83$ & \multicolumn{1}{l|}{$64.91$}    & \multicolumn{1}{l|}{$55.12$} & \multicolumn{1}{l|}{$51.83$}  & $47.55$ & \multicolumn{1}{l|}{$0.25$} & $3.42$ &\tikzmark{c} \\ 
$\textrm{Stacked TXs}_{\textrm{S4}}$          & \multicolumn{1}{l|}{$\bf61.98$}    & \multicolumn{1}{l|}{$\bf41.75$} & $\bf70.94$ & \multicolumn{1}{l|}{$\bf67.34$}    & \multicolumn{1}{l|}{$\bf59.16$} & \multicolumn{1}{l|}{$\bf51.83$}  & $\bf47.55$ & \multicolumn{1}{l|}{$\bf0.24$} & $\bf3.42$ &\tikzmark{d}$\bf\textcolor{ForestGreen}{+2.5}$ \\ \cline{1-10} \cline{1-10} 

Linear                          & \multicolumn{1}{l|}{$54.81$}    & \multicolumn{1}{l|}{$40.28$} & $67.44$ & \multicolumn{1}{l|}{$63.90$}    & \multicolumn{1}{l|}{$54.97$} & \multicolumn{1}{l|}{$48.17$}  & $42.77$ & \multicolumn{1}{l|}{$0.26$} & $3.95$ &\tikzmark{a} \\
$\textrm{Linear}_{\textrm{S4}}$                          & \multicolumn{1}{l|}{$\bf61.98$}    & \multicolumn{1}{l|}{$\bf41.75$} & $\bf69.88$ & \multicolumn{1}{l|}{$\bf66.40$}    & \multicolumn{1}{l|}{$\bf58.80$} & \multicolumn{1}{l|}{$\bf50.60$}  & $\bf47.70$ & \multicolumn{1}{l|}{$\bf0.25$} & $\bf3.51$ &\tikzmark{b}$\bf{\textcolor{ForestGreen}{+6.7}}$\\\cline{1-10} 
\end{tabular}
\begin{tikzpicture}[overlay, remember picture,  thick,  transform canvas={yshift=0.25\baselineskip,xshift=-0.25\baselineskip}]
    \draw [->] ({pic cs:a}) [bend left] to ({pic cs:b});
    \draw [->] ({pic cs:c}) [bend left] to ({pic cs:d});
    \draw [->] ({pic cs:e}) [bend left] to ({pic cs:f});
 \end{tikzpicture}
\caption{Performance of various mask generators in LVU~\cite{lvu2021} dataset, where we adopt 60 frames per clip and $50\%$ masking ratio. The bold results demonstrate the performance of using S4 feature ($x_{S_4}$ in Equation~\ref{vis4mer}). We also provide the average improvement ratio (in green) of nine jobs using S4 features compared to ViT features at the conclusion of each bold row. }
\label{MG_table}

\end{table*}

\section{Experiments}
\label{exp}
\subsection{Dataset}
\noindent\textbf{LVU dataset~\cite{lvu2021}:} is constructed from Movie Clip dataset~\cite{moiveclip}. It contains $\sim30K$ videos from $\sim3K$ movies. Each video lasts one to three minutes. The benchmark contains nine tasks covering a wide range of long-form video understanding tasks, which are further folded into three main categories: (i) content understanding, consisting of (‘relationship’, ‘speaking style’, ‘scene/place’) prediction, (ii) metadata prediction, including (‘director’, ‘genre’, ‘writer’, and ‘movie release year’) classification, and (iii) user engagement, predicting (‘YouTube like ratio’, and ‘YouTube popularity’). For classification and regression tasks, we report accuracy (for content understanding and metadata prediction) and mean-squared error (MSE) (for user engagement) as the evaluation metrics. 

\vspace{+2mm}
\noindent\textbf{COIN~\cite{tang2019coin, tang2020comprehensive}:} consists of 11,827 videos with 180 distinct procedural tasks, which are all collected from YouTube. These videos cover 12 domains, such as nursing $\&$ caring, vehicles, leisure $\&$ performance, gadgets, electric appliances, household items, science $\&$ craft, plants $\&$ fruits, snacks $\&$ drinks dishes, sports, and housework. The average length of a video is 2.36 minutes.

\vspace{+2mm}
\noindent\textbf{Breakfast~\cite{kuehne2014language}:} contains 1,712 videos of 10 complex cooking activities, which are performed by 52 different individuals in 18 different kitchens, resulting in over 77 hours of video footage. The averaged length of video in this dataset is around 2.7 minutes. Ten cooking activities include: making coffee, chocolate milk, juice, tea, cereals, fried egg, pancakes, fruit salad, sandwich and scrambled egg. 




\subsection{Implementation Details}
\noindent Following~\cite{islam2022long, lvu2021}, we stack three structure blocks, which share similar structure to that described in Equation~\ref{vis4mer}, and sample video frames at 1 fps. Unlike previous work, we include an adaptive mask generator to effectively pick image tokens before feeding the input into S4 model. As the advantages of our S5 model will naturally be diminished on less redundant sequences, we follow the same architecture of ViS4mer~\cite{islam2022long} but adopt the S5 model as the first block. 
For data argumentation, we resize each video frame to the spatial resolution of $224\times224$ and use a patch size of $16\times16$. In addition, we use ViT-L~\cite{dosovitskiy2020image} pretrained on ImageNet-21K~\cite{krizhevsky2012imagenet} as the feature extractor in the LVU dataset; Swin-B~\cite{liu2021swin} pretrained on Kinetics-600~\cite{kay2017kinetics} as the feature extractor in COIN and Breakfast datasets. The size of the input in each dataset is also the same as~\cite{islam2022long}: we adopt 60-second input for the LVU dataset and 64-second input for the COIN and Breakfast datasets. In the LSMCL, we adopt the setting from LSTCL~\cite{wang2022long} and apply independent global random masking on long and short clips, which share the same masking ratio with the adaptive mask generator. Unless otherwise noted, we conduct our ablation studies on the LVU dataset due to its diverse tasks in the long-form video understanding. Finally, we report the best performance of our model on all three datasets and compare with the previous state-of-the-art works.
% ablate over different configurations of our mask generator in terms of the structure, the depth, the input feature and the parameter sharing
\subsection{Ablation Study}
\noindent\textbf{a. Our S5 is better than S4 and random masking:} To demonstrate the effectiveness of our proposed S5 model, we compare the performance of S4 models with no mask, random mask, and mask generators of different architectures. Specifically, we utilize one Transformer (TX), two stacked Transformers (TXs), and one linear layer as the mask generator and evaluate on 9 tasks on the LVU dataset (Table~\ref{MG_table}). In addition, we also evaluate the effectiveness of using S4 features from the momentum-updated S4 model. For each architecture, we compare the result of using ViT features and S4 features as the mask generator input. As can be seen from the Table~\ref{MG_table}, the performance of each task substantially increases with the computational complexity of the mask generator. 
%Our selective design achieves significant improvement over the baseline method (ViS4mer~\cite{islam2022long}) and the random masking strategy. 
Results show our design significantly outperforms ViS4mer~\cite{islam2022long} and the random masking strategy, and the performance of each task is further improved by using S4 features. Notably, the mask generator with one linear layer achieves on par performance to one of the more complex transformer architectures. 

\vspace{+2mm}
\noindent\textbf{b. Our S5 reduces up to $25\%$ memory usage:} In Figure~\ref{efficiency}, we also demonstrate the efficiency of our S5 model with the different masking architectures mentioned previously. Compared to ViS4mer (the one without masking strategies) using same number of input frames, our S5 model with linear mask generator reduces the memory footprint by $25\%$ while maintaining the same level of throughput. Memory consumption and throughput are not improved by the intricate transformer mask generators. Since the linear mask generator has a smaller memory footprint and performs tasks more effectively overall, we use it in our S5 model in the following experiments.

% The mask generator plays an important role in our proposed S5 model. Thus, we start the ablation study of our work from the architecture of the proposed mask generator. First of all, as an naive extension of Figure~\ref{concept_result}(a), we simply swap the random masking with our selective mask generator before the S4 model. In Table~\ref{MG_table}, we utilize one Transformer (TX), two stacked Transformers (TXs) and one linear layer as the mask generator respectively and also evaluate the effectiveness of the S4 feature ($x_{S_4}$) in the mask generator. As can be seen from the Table, the performance of each task substantially increases with the number of parameter in the mask generator when using the ViT features. This achieves significant improvement over the baseline method (ViS4mer~\cite{islam2022long}) and the random masking strategy. Moreover, it is found that the performance of each task could be further improved by using the S4 feature. Especially, for the mask generator with one linear layer, it achieves on par performance with transformer architectures. This because the recurrent structure of S4 model embeds the long-term temporal context into its output, the S4 feature. Compared to the complex transformer architecture, the linear mask generator achieves on par performance by leveraging S4 features. On the other side, the improvement of the transformer architecture using S4 feature is not significant due to the saturated long-term dependencies. In the Figure~\ref{efficiency}, we also demonstrate the efficiency of S5 model with different mask generator designs mentioned in the Table~\ref{MG_table}. Compared to ViS4mer (the one without masking strategies), our linear mask generator receives gain in memory efficiency, which reduces $40\%$ memory usage of the ViS4mer. However, as the shadow S4 model essentially double forward passes the input, the throughput of our S5 model with linear mask generator is the same with ViS4mer. Despite having the best performance, the transformer design does not see improvements in memory use or throughput. Thus, we will adopt the linear mask generator in the following experiments.
\begin{figure}[]
\centering
% \begin{subfigure}{.235\textwidth}
  \includegraphics[width=0.7\linewidth]{figures/MG_efficiency4.pdf}
%   \caption{}
%   \label{efficiency}
% \end{subfigure}
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/placeholder.png}
%   \caption{}
%   \label{masking}
% \end{subfigure}
\caption{Efficiency evaluation of each method in Table~\ref{MG_table}, which demonstrates the GPU memory usage as well as throughput. Our proposed S5 model with linear mask generator saves $25\%$ memory cost and achieves on par throughput with ViS4mer~\cite{islam2022long}.}
\label{efficiency}
% \vspace{-2mm}
\end{figure}


\begin{figure*}[!htb]
\centering
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/MG_efficiency2.png}
% \end{subfigure}
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=0.98\linewidth]{figures/MG_masking_ratio5.pdf}
  \caption{}
  \label{MG_masking}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=0.98\linewidth]{figures/number_frames5.pdf}
    \caption{}
  \label{MG_seq_len}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=0.95\linewidth]{figures/LSMCL_stride4.pdf}
    \caption{}
  \label{LSMCL_stride}
\end{subfigure}
\begin{subfigure}{.245\textwidth}
  \includegraphics[width=0.98\linewidth]{figures/LCMCL_mask_ratio3.pdf}
    \caption{}
  \label{LSMCL_masking}
\end{subfigure}
% \vspace{-2mm}
\caption{Compared to the baseline performance, average improvement performance of our method on LVU dataset. Unless otherwise noted, the default number of input frame and masking ratio is 60 and $50\%$. \textbf{(a).} We compared our S5 model and S4 model with random masking with increasing masking ratio; \textbf{(b).} We compare our S5 model and S4 model with increasing number of input frames; \textbf{(c).} We show the effect of LSMCL pretraining with different long-short sampling stride ratio. In addition, we provide the performance of S5 model without LSMCL and S5 model with 100 input frames; \textbf{(d).} We show the impact of the increasing masking ratio in the LSMCL pretraining.}%\xy{from Linda: a bit hard to back-and-forth to check the sub-figures. better to be consistent with Figure 2 as well.}
\label{allinone}
\end{figure*}


% \begin{figure}[]
% \centering
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/MG_masking_ratio.png}

% \end{subfigure}
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/number_frames.png}

% \end{subfigure}
% \caption{(a) Averaged performance improvement ratio on LVU dataset with increasing masking ratio. (b) Averaged performance improvement ratio on LVU dataset with increasing number of input frames.}
% \label{masking_and_seqlen}
% \end{figure}

% \begin{table*}[!htb]
% \small
% \centering
% \begin{tabular}{|l|lll|llll|ll|l|l|}
% \hline
% \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Num of \\ MGs\end{tabular}} & \multicolumn{3}{c|}{Content ($\uparrow$)}                                       & \multicolumn{4}{c|}{Metadata ($\uparrow$)}                                                                    & \multicolumn{2}{c|}{User ($\downarrow$)}  & \multirow{2}{*}{M} & \multirow{2}{*}{T}    \\ \cline{2-10} 
%                                 & \multicolumn{1}{l|}{Relation} & \multicolumn{1}{l|}{Speak} & Scene & \multicolumn{1}{l|}{Director} & \multicolumn{1}{l|}{Genre} & \multicolumn{1}{l|}{Writer} & Year  & \multicolumn{1}{l|}{Like} & View  & & \\ \hline
% 1                          & \multicolumn{1}{l|}{$61.98$}    & \multicolumn{1}{l|}{$41.75$} & $69.88$ & \multicolumn{1}{l|}{$66.40$}    & \multicolumn{1}{l|}{$58.80$} & \multicolumn{1}{l|}{$50.60$}  & $47.70$ & \multicolumn{1}{l|}{$0.25$} & $3.51$  & &  \\ \hline
% 2             & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$}    & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$} & $xx.x_{\textbf{xx.x}}$ & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$}    & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$} & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$}  & $xx.x_{\textbf{xx.x}}$ & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$} & $xx.x_{\textbf{xx.x}}$  & & \\ \hline
% 3         & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$}    & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$} & $xx.x_{\textbf{xx.x}}$ & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$}    & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$} & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$}  & $xx.x_{\textbf{xx.x}}$ & \multicolumn{1}{l|}{$xx.x_{\textbf{xx.x}}$} & $xx.x_{\textbf{xx.x}}$  & & \\ \hline
% \end{tabular}
% \caption{Performance on LVU~\cite{lvu2021} dataset with increasing number of mask generator. The subscript results demonstrate the performance of using independent mask generators without sharing parameters.}
% \label{depth_sharing}
% \end{table*}

% \noindent\textbf{Mask Generator-Depth and Parameter Sharing:} In the previous part, only one mask generator with the S4 feature is good enough to generate superior performance compared to the ViSformer~\cite{islam2022long} with random masking. In this part, we are going to finalize the Algorithm~\ref{algorithm1}, that is to introduce a mask generator before each S4 block. In the Table~\ref{}, we show the performance in LVU dataset with increasing number of mask generators (MG) attached before each S4 block. For fair comparison, we apply the random mask for S4 blocks without mask generator. In addition, we also investigate the effect of sharing parameters between each mask generator. Moreover, the efficiency metrics in terms of the memory usage and throughput (samples per second) are also reported. From the table, we can see that the performance increases with the increasing number of mask generator. As there is only one linear layer in it, the extra cost is relatively low. In addition, it is found that the mask generator with independent parameter without sharing performs .....

\vspace{+2mm}
\noindent\textbf{c. Impact of Masking Ratio and Sequence Length:} In Figure~\ref{MG_masking} and~\ref{MG_seq_len}, we study the effect of masking ratio and sequence length with our S5 model. We set ViS4mer~\cite{islam2022long} (60 frames without mask generator) as baseline and report the average improvement percentage of 9 tasks on LVU dataset by using S5 model with variant masking ratio/sequence length. To demonstrate the effectiveness of our S5 model, we also compare the performance of ViS4mer~\cite{islam2022long} with different settings in these two figures. Figure~\ref{MG_masking} clearly shows that the performance of our S5 model increases initially as the masking ratio increases, which indicates that our selective model effectively picks informative image tokens for the S4 model. However, the performance starts to drop dramatically when the masking ratio is over $50\%$. This is because when the masking ratio increases to be above certain level, the informative tokens are forced to be dropped. As a result, we adopt $50\%$ masking ratio in our following experiments. In Figure~\ref{MG_seq_len}, we observe substantial improvement of S5 model with increasing number of input frames. In contrast to the performance of ViS4mer~\cite{islam2022long}, our proposed S5 model is indeed able to capture longer term dependencies while reducing the spatial-temporal redundancy in the input. 

% \xy{show your mask generator generated masking pattern here, even just some sample-wise is okay. Different from Wentao's visualization, here you will show two subsections, one is spatially selective (this is similar to Wentao's), the other is temporally selective, across time axis, some frames are selected (highlighted) while others are dropped.}
\vspace{+2mm}
\noindent\textbf{d. Effect of Multiple S5 models:} As shown in Figure~\ref{Fig:pipeline}, multiple S5 models can be stacked in the pipeline, similar to what is commonly done in Transformer~\cite{dosovitskiy2020image, bertasius2021space, wu2022memvit} and ViS4mer~\cite{islam2022long}. In the previous setup, we only adopt one S5 model, leaving the remaining blocks as S4 models. By stacking multiple S5 models, we find a further $\bf 0.5\%$ average improvement on the LVU dataset. Less redundant sequences will inevitably reduce the performance gain from our S5 model, decreasing the benefit from stacking additional S5 blocks. As a result, we utilize only one S5 model after the video encoder for maximum memory efficiency gain and throughput.

\vspace{+2mm}
\noindent\textbf{e. Ablation on LSMCL:} In Figure~\ref{LSMCL_stride} and~\ref{LSMCL_masking}, we evaluate the effectiveness of our proposed LSMCL with different sampling strides and random masking ratios. For both figures, we set the performance of ViS4mer~\cite{islam2022long} as the baseline and report the average improvement ratio (in percentage) of 9 tasks from LVU with different settings. From Figure~\ref{LSMCL_stride}, our S5 model with LSMCL can achieve better performance even when $\tau_{L}=\tau_{S}$, which suggests that LSMCL can increase the robustness of our S5 model and help it handle incorrectly picked tokens. When we gradually increase the $\frac{\tau_{L}}{\tau_{S}}$, the performance of S5 model is further improved as the model is able to capture longer temporal context via the proposed LSMCL. Indeed, the performace using LSMCL approaches the performance without LSMCL \textbf{with $\textbf{66\%}$ more input frames} (shown in Figure~\ref{MG_seq_len} both around $6\%$ boost). In Figure~\ref{LSMCL_masking}, we further ablate the random masking ratio used in LSMCL. When the masking ratio of LSMCL is over $50\%$, the benefit from LSMCL is insignificant as the input does not provide sufficient information. Thus, we consider $50\%$ masking ratio in LSMCL for better efficiency in the long-form video contrastive learning.

% To further broaden the temporal horizon of our S5 model, we adopt the LSMCL pretraining to initialize the S4 model(as well as the shadow S4 model) and fine-tune our S5 model using 60 frames and $50\%$ masking ratio in the mask generator. In Figure~\ref{LSMCL_masking}, we ablate the influence of different random masking ratio in the LSMCL. Again, compared to the performance of ViS4mer~\cite{islam2022long}, we report the averaged improvement ratio on 9 tasks of LVU dataset. From the Figure~\ref{LSMCL_masking}, it can be observed that the performance gain from the LSMCL pretraining is on par with the one from using 100 input frames. By leveraging LSMCL, our S5 model with 60 input frames could acheives on par performance with the one of using 100 input frames without LSMCL. When the masking ratio of LSMCL is over $50\%$, the extra benefit from LSMCL will be gone as the number of tokens are too less to learn meaningful representations. We adopt $50\%$ masking ratio in LSMCL for better efficiency.


\begin{table*}[!bpht]
\small
\centering
\begin{tabular}{|l|lll|llll|ll|l|}
\hline
\multirow{2}{*}{Model}   & \multicolumn{3}{c|}{Content ($\uparrow$)}                                                               & \multicolumn{4}{c|}{Metadata ($\uparrow$)}                                                                                                    & \multicolumn{2}{c|}{User ($\downarrow$)}                          & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}GPU Usage\\(GB) ($\downarrow$)\end{tabular}}} \\ \cline{2-10}
                                                       & \multicolumn{1}{l|}{Relation}       & \multicolumn{1}{l|}{Speak}          & Scene          & \multicolumn{1}{l|}{Director}       & \multicolumn{1}{l|}{Genre}          & \multicolumn{1}{l|}{Writer}         & Year           & \multicolumn{1}{l|}{Like}          & View          & \multicolumn{1}{c|}{}                                                                               \\ \hline
Obj. T4mer~\cite{lvu2021}                                                         & \multicolumn{1}{l|}{54.76}          & \multicolumn{1}{l|}{33.17}          & 52.94          & \multicolumn{1}{l|}{47.66}          & \multicolumn{1}{l|}{52.74}          & \multicolumn{1}{l|}{36.30}          & 37.76          & \multicolumn{1}{l|}{0.30}          & 3.68          & N/A                                                                                               \\ \hline
Performer~\cite{choromanski2020rethinking}                                                                   & \multicolumn{1}{l|}{50.00}          & \multicolumn{1}{l|}{38.80}          & 60.46          & \multicolumn{1}{l|}{58.87}          & \multicolumn{1}{l|}{49.45}          & \multicolumn{1}{l|}{48.21}          & 41.25         & \multicolumn{1}{l|}{0.31}          & 3.93         & 5.93                                                                                              \\ \hline
Orthoformer~\cite{patrick2021keeping}                                                                   & \multicolumn{1}{l|}{50.00}          & \multicolumn{1}{l|}{38.30}          & 66.27        & \multicolumn{1}{l|}{55.14}          & \multicolumn{1}{l|}{55.79}          & \multicolumn{1}{l|}{47.02}          & 43.35         & \multicolumn{1}{l|}{0.29}          & 3.86         & 5.56                                                                                                \\ \hline
VideoBERT~\cite{sun2019videobert}                                                                   & \multicolumn{1}{l|}{52.80}          & \multicolumn{1}{l|}{37.90}          & 54.90          & \multicolumn{1}{l|}{47.30}          & \multicolumn{1}{l|}{51.90}          & \multicolumn{1}{l|}{38.50}          & 36.10          & \multicolumn{1}{l|}{0.32}          & 4.46          & N/A                                                                                                 \\ \hline
LST~\cite{islam2022long}                                                                               & \multicolumn{1}{l|}{52.38}          & \multicolumn{1}{l|}{37.31}          & 62.79          & \multicolumn{1}{l|}{56.07}          & \multicolumn{1}{l|}{52.70}          & \multicolumn{1}{l|}{42.26}          & 39.16          & \multicolumn{1}{l|}{0.31}          & 3.83          & 41.38                                                                                                \\ \hline
ViS4mer~\cite{islam2022long}                                                                           & \multicolumn{1}{l|}{57.14}          & \multicolumn{1}{l|}{40.79}          & 67.44          & \multicolumn{1}{l|}{62.61}          & \multicolumn{1}{l|}{54.71}          & \multicolumn{1}{l|}{48.80}          & 44.75          & \multicolumn{1}{l|}{0.26}          & 3.63          & 5.15                                                                                                 \\ \hline\hline
$\textrm{Ours}_{\textrm{60 frames}}$                                                                          & \multicolumn{1}{l|}{\textbf{61.98}}          & \multicolumn{1}{l|}{\textbf{41.75}} & \textbf{69.88} & \multicolumn{1}{l|}{\textbf{66.40}} & \multicolumn{1}{l|}{\textbf{58.80}} & \multicolumn{1}{l|}{\textbf{50.60}} & \textbf{47.70} & \multicolumn{1}{l|}{\textbf{0.25}} & \textbf{3.51} & \textbf{3.85}                                                                          \\ \hline      
$\textrm{Ours}_{\textrm{60 frames+LSMCL}}$                                                                           & \multicolumn{1}{l|}{\textbf{61.98}}          & \multicolumn{1}{l|}{\textbf{41.75}} & \textbf{72.53} & \multicolumn{1}{l|}{\textbf{66.40}} & \multicolumn{1}{l|}{\textbf{61.34}} & \multicolumn{1}{l|}{\textbf{50.60}} & \textbf{47.70} & \multicolumn{1}{l|}{\textbf{0.24}} & \textbf{3.51} & \textbf{3.85}                                                                           \\ \hline 
$\textrm{Ours}_{\textrm{100 frames}}$                                                                            & \multicolumn{1}{l|}{\textbf{66.71}}          & \multicolumn{1}{l|}{\textbf{41.78}} & \textbf{73.28} & \multicolumn{1}{l|}{\textbf{66.64}} & \multicolumn{1}{l|}{\textbf{63.65}} & \multicolumn{1}{l|}{\textbf{50.60}} & \textbf{47.85} & \multicolumn{1}{l|}{\textbf{0.25}} & \textbf{3.51} & \textbf{3.95}                                                                                \\ \hline 
$\textrm{Ours}_{\textrm{100 frames+LSMCL}}$                                                                             & \multicolumn{1}{l|}{\textbf{67.11}}          & \multicolumn{1}{l|}{\textbf{42.12}} & \textbf{73.49} & \multicolumn{1}{l|}{\textbf{67.32}} & \multicolumn{1}{l|}{\textbf{65.41}} & \multicolumn{1}{l|}{\textbf{51.27}} & \textbf{47.95} & \multicolumn{1}{l|}{\textbf{0.24}} & \textbf{3.51} & \textbf{3.95}                                                                                \\ \hline 
\end{tabular}

\caption{Comparison to the state-of-the-art methods on LVU dataset testing set.}
\label{sota_LVU}
% \vspace{-2mm}
\end{table*}

\begin{table}[]
\small
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Method              & P.T. Dataset & P.T. Samples & Accuracy \\ \hline
TSN~\cite{tang2020comprehensive}                 & Kinetics-400        & 306K                & 73.40    \\ \hline
D-Sprv.~\cite{lin2022learning}       & HowTo100M           & 136M                & 90.00    \\ \hline
ViS4mer~\cite{islam2022long}             & Kinetics-600        & 495K                & 88.41    \\ \hline\hline
Ours                & Kinetics-600        & 495K                & \textbf{90.42}    \\ \hline
$\textrm{Ours}_{\textrm{+LSMCL}}$                & Kinetics-600        & 495K                & \textbf{90.81}    \\ \hline
\end{tabular}
\caption{Comparison to the state-of-the-art methods on COIN dataset. P.T. stands for pretraining.}
\label{sota_COIN}
\vspace{-4mm}
\end{table}

\begin{table}[]
\small
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Method              & P.T. Dataset & P.T. Samples & Accuracy \\ \hline
VideoGraph~\cite{hussein2019videograph}                 & Kinetics-400        & 306K                & 69.50    \\ \hline
Timeception~\cite{hussein2019timeception}                 & Kinetics-400        & 306K                & 71.30   \\ \hline
GHRM~\cite{zhou2021graph}                 & Kinetics-400        & 306K                & 75.50   \\ \hline
D-Sprv.~\cite{lin2022learning}       & HowTo100M           & 136M                & 89.90    \\ \hline
ViS4mer~\cite{islam2022long}             & Kinetics-600        & 495K                & $\text{85.10}^{*}$    \\ \hline\hline
Ours                & Kinetics-600        & 495K                & \textbf{90.14}    \\ \hline
$\textrm{Ours}_{\textrm{+LSMCL}}$               & Kinetics-600        & 495K                & \textbf{90.70}    \\ \hline
\end{tabular}
\caption{Comparison to the state-of-the-art methods on Breakfast dataset. P.T. stands for pretraining. $^{*}$We were not able to reproduce the $88.17\%$ baseline result reported in~\cite{islam2022long}, but our proposed S5 model still largely improves from $85.10\%$, and achieves the new state-of-the-art result.}% 
\label{sota_Breakfast}
\vspace{-4mm}
\end{table}

\subsection{Comparison with the State-Of-The-Arts}
\noindent In Table~\ref{sota_LVU}, we compare our method on LVU dataset with previous state-of-the-art methods. Specifically, the LST~\cite{islam2022long} adopt the same architecture with ours, but substitutes the S5/S4 model to the transformer architecture. Whereas the Performer~\cite{choromanski2020rethinking} and Orthoformer~\cite{patrick2021keeping} apply the efficient attention in the transformer architecture, that do not require quadratic complexity \textit{w.r.t.} the input length. When compared to baseline ViS4mer~\cite{islam2022long}, we achieve up to $\mathbf{9.6\%}$ improvement. When compared to other methods, ours outperforms by an even more significant margin. This shows that our method is consistently more effective in understanding the long-form videos.
% \xy{from Linda: a more detailed number to highlight the bigger improement.}

To demonstrate the generalizability of our method, we evaluate our S5 model on COIN~\cite{tang2019coin,tang2020comprehensive} and Breakfast~\cite{kuehne2014language} datasets, which are challenging long-range procedural activity classification datasets. Our proposed method achieves $\textbf{2.4\%}$ and $\textbf{5.5\%}$ over the ViS4mer~\cite{islam2022long} and outperforms the other state-of-the-arts by $\textbf{0.81\%}$ and $\textbf{0.80\%}$ respectively. Notice that D-Sprv.~\cite{lin2022learning} leverages HowTo100M dataset~\cite{miech2019howto100m} for pretraining, which volume is much larger than our pre-training dataset (Kinetics-600~\cite{carreira2018short}). Putting together the aforementioned performance gain and memory efficiency gain, our S5 model successfully demonstrates its efficiency and effectiveness in learning discriminative representation via selecting informative image tokens from long-form video sequences.

% \noindent\textbf{Intermediate feature from different ViS4mer layers:} We follow the algorithm~\ref{algorithm1} to implement our proposed method. To study the effect of intermediate feature, we use features from different layers of ViS4mer to generate masks and report the performance of content prediction task in the Figure~\ref{fig:layer}. In these experiments, we set the masking ratio as $50\%$.  From the Figure~\ref{fig:layer}, the performance is improved with the increasing depth of feature extraction layer. This is because the feature representation capture richer spatiotemporal information in the later stage. However, the performance starts to saturated after the second layer of ViS4mer. To better balance the efficiency and performance, we chose the feature from the second layer of ViS4mer in our experiments.


% \noindent\textbf{Different masking ratio:} In the Table~\ref{concept_result}, it can be seen that the performance of relationship and genre prediction are even improved after randomly dropping half of the image patches. This means redundant video information would hurt the performance and are not necessary to be involved in the computation. In the Figure~\ref{fig:mask}, we ablate different masking ratio in our proposed method. From the Figure~\ref{fig:mask}, it can be seen that the performance is initially improved with dropping a portion of image patches and then start to clearly degenerate when the masking ratio beyond $60\%$. This makes sense as the mask generator first learn to mask redundant information, but is forced to mask out discriminative patches when the masking ratio becomes larger than its saturated value. As a result, we use the $50\%$ masking ratio in our experiments.

% \begin{figure}[!bpht]
% \centering
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/layer.png}
%   \caption{}
%   \label{fig:layer}
% \end{subfigure}
% \begin{subfigure}{.235\textwidth}
%   \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/mask.png}%\vspace{-.2cm}
%   \caption{}
%   \label{fig:mask}
% \end{subfigure}
% \caption{Accuracy in LVU relationship prediction task when varying  (1) the index of layer for extracting feature to generate mask, layer 0 represents input data without passing through the ViS4mer; (b) the masking ratio in the mask generator.}
% \end{figure}



% \noindent\textbf{Comparison with the State-of-the-art:} To produce the best performance of our proposed method and make fair comparison with previous works, we adopt the same setting for both 60 and 120 frames' input. The experimental results as well as the memory usage can be found in the Table~\ref{sota}. From the comparison between Table~\ref{sota} and Table~\ref{concept_result}, we can see that our proposed method adaptively satisfy each task in the LVU dataset by leveraging mask generator with intermediate features, which achieves the new state-of-the-art performance. Moreover, the memory cost of our proposed method is only $60\%$ (60 frames set-up) of the original ViS4mer work. 

