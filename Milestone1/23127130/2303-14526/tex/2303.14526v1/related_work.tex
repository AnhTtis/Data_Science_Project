\section{Related Work}
\noindent We discuss the literature with respect to the three most relevant fields: video understanding with long-form format, efficient token selection for vision transformer training, and self-supervised learning with videos.

\vspace{+2mm}
\noindent\textbf{a. Long-Form Video Modeling:} 
%Transformer architectures demonstrates excellent performance in modeling long-term dependencies, especially in the area of natural language processing (NLP)~\cite{brown2020language,dai2019transformer, devlin2018bert}. However, high computational cost of self-attention mechanism becomes one of its major drawbacks, which is quadratic to the sequence length in NLP and quadratic to the input resolution in Computer Vision (CV). To tackle this issue, several efficient attention works~\cite{wang2022deformable, liu2021swin,choromanski2020rethinking,katharopoulos2020transformers,liu2021video,kitaev2020reformer,pan2021scalable} are introduced in both NLP and CV areas, reducing the computational cost of the transformer. Unfortunately, these works are not well-fitted in modeling long-form videos due to the plethora of spatial and temporal image tokens in the long-form videos. To this end, LF-VILA~\cite{sunlong2022} develops a hierarchical feeding architecture to include more frames in the model, and thus capturing longer temporal information. Similarly, MeMViT~\cite{wu2022memvit} utilize more temporal information by attending the previously cached "memory" of the past. These work leverage the pyramid architecture to efficiently build long-term dependencies on high-level features, which may lost much low-level space-time contextual information.  Unlike CNN and transformer architecture, structured state-space sequence model, dubbed as S4 model, is introduced by Gu et al.~\cite{gu2021efficiently}, which models the long-range dependencies by simulating linear time invariant (LTI) system and each component in LTI is learned via the gradient descent. Subsequently, S4ND~\cite{nguyen2022s4nd} and ViS4mer~\cite{islam2022long} extend S4 model to the video classification task. Specifically, ViS4mer~\cite{islam2022long} stacks multiple S4 layers with different scales in modeling long-form videos and S4ND~\cite{nguyen2022s4nd} substitutes the traditional convolutional layer with the proposed S4ND layer in image and short-form video classification tasks. However, we believe the efficiency of S4 model can be further improved by introducing suitable token selection mechanism, especially when dealing with the long input sequence. In addition, it is unfavorable to simulate LTI system for all image tokens, as there might not be strong temporal relationship among them. As a result, we propose an efficient token selection method for the S4 model that is named as the Selective S4 (S5) model. To demonstrate the effectiveness of our S5 model, we test it in long-form video modeling task.
Transformers have shown excellent performance in modeling long-term dependencies,~\textit{e.g.}, in natural language processing (NLP)~\cite{brown2020language,dai2019transformer,devlin2018bert}. But the high computational cost caused by dense self-attention calculation becomes a bottleneck to apply in not only NLP but also computer vision. Much subsequent work~\cite{wang2022deformable, liu2021swin,choromanski2020rethinking,katharopoulos2020transformers,liu2021video,kitaev2020reformer,pan2021scalable} focuses on improving the transformer efficiency. However, they are not designed for dealing with plethora of spatial and temporal image tokens that are common in long-form video scenarios. LF-VILA~\cite{sunlong2022} develops a hierarchical feeding architecture to include more frames in the model, thus capturing longer temporal information. Similarly, MeMViT~\cite{wu2022memvit} better utilizes temporal information by emerging the previously cached ``memory" from the past. The pyramid structure leveraged by LF-VILA and MeMViT shows efficiency improvements, but may lose low-level spatial-temporal contextual information. 
Gu et al.~\cite{gu2021efficiently} proposed a Structured State-Space Sequence (S4) model, a novel alternative to CNNs or transformers, to model the long-range dependencies by simulating a linear time invariant (LTI) system. Subsequently, S4ND~\cite{nguyen2022s4nd} and ViS4mer~\cite{islam2022long} extend S4 model to the video classification task. ViS4mer~\cite{islam2022long} stacks multiple S4 layers with different scales in modeling long-form videos, and S4ND~\cite{nguyen2022s4nd} substitutes the traditional convolutional layer with the proposed S4ND layer in image and short-form video classification tasks. The equal importance assumption to all the image tokens by ViS4mer and S4ND can be further improved by introducing suitable token selection mechanisms, especially when dealing with the long-form input sequences. Consequently, we propose a token Selection S4 (S5) model to further enhance the efficiency while maintaining the long-form representation power.

\vspace{+2mm}
\noindent\textbf{b. Adaptive Token Selection:} 
%In the recent, the adaptive input selection method is widely used to improve the model efficiency and reduce the cost. In the traditional CNN architecture, Scsampler~\cite{korbar2019scsampler} filter informative clips by using motion and audio embedding. Adaframe~\cite{wu2019adaframe} utilizes a memory-agumented LSTM serves as an agent, predicting where to look in the next time step. Similarly, AR-NET~\cite{meng2020ar} uses LSTM as decision maker to select useful frames and their resolution. When this concept is extended to the transformer architecture, image patches are adaptively selected~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not}. For example, STTS~\cite{wang2021efficient} introduces a token selection method to improve the efficiency of video transformers by selecting top-K frames with the highest scores for computation; AdaViT~\cite{meng2022adavit} extend this idea to produce instance-specific usage policies on which patches, self-attention heads and transformer blocks to keep/activate throughout the network. These works place a light-weighted score generator or decision maker on the top of the raw input to generate policy. This may work well in the image domain, as the volume and the complexity of the input data are relatively low. However, for long videos, it may be challenging to balance the trade-off between the efficiency gain and cost of the decision maker. Specifically, direct extensions of~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not,wang2021efficient} would either not consider the long-range reasoning when making the decision or be computationally expensive due to the self-attention/deep 3D CNNs architectures. To tackle this problem, we leverage S4 feature in our S5 model, which efficiently memorize the long-term dependencies and effectively pick informative tokens in modeling long-form videos.
Adaptive token selection is widely used to improve model efficiency. Traditional CNN methods such as SCsampler~\cite{korbar2019scsampler} filter informative clips by using motion and audio embeddings. Adaframe~\cite{wu2019adaframe} utilizes memory-augmented LSTMs as agents, which predict where to look in the next time step. AR-NET~\cite{meng2020ar} uses LSTM as decision maker to select useful frames and their resolutions. ~\cite{yin2021adavit,wang2021efficient,meng2022adavit,rao2021dynamicvit,liang2022not} apply this selection idea to transformers to adaptively select tokens for increased efficiency. For instance, STTS~\cite{wang2021efficient} leverages a token selection module, the named scorer network, to provide the importance score for each token and select the top-K frames with the highest scores. AdaViT~\cite{meng2022adavit} extends this idea to develop instance-specific policies, guiding the activation of patches, self-attention heads and transformer blocks. All of the above methods demonstrate how a light-weight token selection module can improve inference efficiency. However, these methods are essentially designed for images, and may require non-trivial adaptation to the long-form video scenarios,~\textit{i.e.}, the video-level long-range reasoning and computationally expensive self-attention calculation. To avoid this dense self-attention calculation, our proposed S5 model leverages S4 features to model the long-term dependencies and adaptively pick informative tokens.

\vspace{+2mm}
\noindent\textbf{c. Video Self-Supervised Learning (SSL):} Previous work on token reduction rarely considers the negative impact of mis-dropped tokens. EViT~\cite{liang2022not} simply fuses the  unattended tokens and concatenates with the remaining ones. From the recent successful image SSL works~\cite{he2020momentum,chen2020simple,grill2020bootstrap,he2022masked,chen2020exploring}, many follow-up works~\cite{tong2022videomae, wang2022long,recasens2021broaden,Feichtenhofer_large,feichtenhofer2022masked} learn discriminative video features with great generalization ability in downstream tasks. Specifically, LSTCL~\cite{wang2022long} and BraVe~\cite{recasens2021broaden} utilize long and short clips in the concept of SSL, which enables the model to learn an effective representation by predicting temporal context captured from a longer temporal extent. This essentially broadens the temporal horizon of the model for predicting longer temporal context with fewer from shorter input frames. In this paper, we adopt this idea with an additional random masking strategy to increase the efficiency of contrastive learning in long-form videos, and to further improve the robustness and the temporal predictability of our S5 model in downstream tasks. 
% \xy{still feel the random masking from LSMCL should be discriminated from the mask generator. Now in Figure 3, they visualize in the same way.}