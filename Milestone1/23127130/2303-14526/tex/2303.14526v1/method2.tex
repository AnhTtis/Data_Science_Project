\section{Approach}
\noindent We start by summarizing Structured State-Space Sequence (S4)~\cite{gu2021efficiently} model and ViS4mer~\cite{islam2022long} ($\S$~\ref{ss:preliminaries}), followed by empirical analysis of S4 model in various long-form video understanding tasks ($\S$~\ref{ss:vis4mer_limitations}), and then providing the details of our proposed approach to address these limitations ($\S$~\ref{ss:adaptive_tokens} and $\S$~\ref{ss:LSMCL}).


\subsection{Preliminaries}
\label{ss:preliminaries}
\subsubsection{S4 Model}
\label{sss:preliminaries_s4}
\noindent Recall that a simple State-Space Model \textit{i.e.}, a linear time invariant (LTI) system can be written as:
\begin{align}
\begin{split}
\label{SSM}
\mathbf{x}^\prime (t) &= \mathbf{A}\mathbf{x}(t)+\mathbf{B}\mathbf{u}(t) \\
\mathbf{y}(t) &= \mathbf{C}\mathbf{x}(t) + \mathbf{D}\mathbf{u}(t).
\end{split}
\end{align}
Under deep learning setting, $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ are learned via gradient descent while $+\mathbf{D}\mathbf{u}(t)$ is replaced by a residual connection. This formulation projects an input signal $\mathbf{u}(t)$ from one-dimensional space to an $\textrm{N}$-dimensional latent space $\mathbf{x}(t)$, which is then mapped back to a one-dimensional output signal $\mathbf{y}(t)$. Similar to RNNs, it has been found in previous work that Equation~\ref{SSM} also suffers from gradient vanish or exploding issues when modeling longer sequences. To tackle this issue, the work in~\cite{gu2021efficiently} leveraged HiPPO theory~\cite{gu2020hippo} to initialize the $\mathbf{A}$ matrix. HiPPO specifies a certain expression of $\mathbf{A}\in \mathbb{R}^{\textrm{N}\times \textrm{N}}$ (see Equation~\ref{HiPPO}), which allows the hidden state to memorize the input $\mathbf{u}(t)$ \footnote{Please refer to~\cite{gu2020hippo} for more details and relevant proofs.}. 

\begin{equation}
\label{HiPPO}
\text{HiPPO:}\ \mathbf{A}_{n,k}= -
\begin{cases}
  (2n+1)^{0.5}(2k+1)^{0.5} & \text{if}\ n>k \\
  n+1 & \text{if}\ n=k\\
  0  & \text{if}\ n<k,
\end{cases}
\end{equation}
where $n$ and $k$ indicate the row and column indices of $\mathbf{A}$. To implement Equation~\ref{SSM} using discrete inputs such as word or image tokens, the work in~\cite{gu2021efficiently} leverages the bi-linear discretization method~\cite{tustin1947method} and a discretized version of Equation~\ref{SSM} using a step size $\Delta$ is rewritten as:
\begin{align}
\begin{split}
\label{S4}
&\mathbf{x}_k = \Bar{\mathbf{A}}\mathbf{x}_{k-1}+\Bar{\mathbf{B}}\mathbf{u}_k \\
&\mathbf{y}_k=\Bar{\mathbf{C}}\mathbf{x}_k,
\end{split}
\end{align}
where $\Bar{\mathbf{A}}=(\mathbf{I}+\frac{\Delta\cdot \mathbf{A}}{2})/(\mathbf{I}-\frac{\Delta\cdot \mathbf{A}}{2})$, $\Bar{\mathbf{B}}=\Delta\cdot \mathbf{B}/(I-\frac{\Delta\cdot \mathbf{A}}{2})$ and $\Bar{\mathbf{C}} = \mathbf{C}$. Equation~\ref{S4} can be solved using a discrete convolution~\cite{gu2021efficiently}:
\begin{equation}
\label{S4_conv}
    \mathbf{y} = \Bar{\mathbf{K}} \circledast \mathbf{u},
\end{equation}
where $\mathbf{u}=\{u_0, u_1,\dots, u_{k-1}, u_k\}$ and $\Bar{\mathbf{K}} \in \mathbb{R}^\textrm{L} := \{\Bar{\mathbf{C}}\Bar{\mathbf{B}}, \Bar{\mathbf{C}}\Bar{\mathbf{A}}\Bar{\mathbf{B}},\dots,\Bar{\mathbf{C}}\Bar{\mathbf{A}}^{\textrm{L}-1}\Bar{\mathbf{B}}\}$ is a structured convolutional kernel and $\textrm{L}$ is the sequence length. Equation~\ref{S4_conv} is the core formulation of S$4$ model whose computational cost is linear to the input length and can be efficiently computed using fast Fourier transform (FFT) and inverse FFT. Moreover, to 
control the convolution kernel width, the work in~\cite{gu2021combining} set $\Delta$ as a learnable parameter.

\subsubsection{ViS4mer Model}


By utilizing the S4 model, the ViS4mer~\cite{islam2022long} achieves promising results in the long-form video understanding tasks. We start with defining some notations to help summarize the adaptation of S4 model in computer vision. Given a video clip $\mathbf{X} \in \mathbb{R}^{\textrm{H} \times \textrm{W} \times 3 \times \textrm{T}}$ consisting of $\textrm{T}$ RGB frames sampled from the video, we convert it into a sequence of $\textrm{S} \cdot \textrm{T}$ image tokens ${\mathbf{x}}_s^t \in  \mathbb{R}^\textrm{D}$ for $s=1, \hdots, \textrm{S}$ and $t=1, \hdots, \textrm{T}$. The tokens ${\mathbf{z}}_s^t$ are obtained by decomposing each frame into $\textrm{S}$ patches which are then projected to a $\textrm{D}$-dimensional space through a learnable linear transformation. This tokenization can be implemented by linearly mapping the RGB patches of each frame~\cite{bertasius2021space,neimark2021video}. Separate learnable positional encodings ${\mathbf{e}}_s$ and ${\mathbf{e}}^t$ are then applied to the patch embeddings ${\mathbf{z}}_s^t$ for the spatial and the temporal dimensions: ${\mathbf{x}}_s^t = {\mathbf{z}}_s^t + {\mathbf{e}}_s + {\mathbf{e}}^t$, formulating $\mathbf{x_{input}} = \{x_0^0, x_1^0, x_\text{S}^0, x_0^1,\dots,x_\text{S}^\text{T} \} $.

In ViS4mer~\cite{islam2022long}, a multi-scale S4 decoder is introduced for learning the long-term temporal reasoning. As is mentioned in $\S$~\ref{sss:preliminaries_s4}, S4 model has a linear computation and memory dependency with respect to the input length, which has significantly lower computational cost than the self-attention in transformers. The formulation of S4 decoder can be written as:
\begin{align}
\label{vis4mer}
\begin{split}
&{\bf x}_{s_4} = \text{S}_\text{4} \left(\text{LN}\left({\bf x}_{input}\right)\right) \\
&{\bf x}_{mlp} = \text{MLP} \left(\text{P}\left({\bf x}_{s_4}\right) \right) \\ 
&{\bf x}_{skip} = \text{Linear} \left(\text{P}\left({\bf x}_{input}\right) \right) \\
&{\bf x}_{out} = {\bf x}_{skip} + {\bf x}_{mlp},
\end{split}
\end{align}
Where $\text{LN}(\cdot), \text{MLP}(\cdot), \text{Linear}(\cdot)$ and $\text{P}(\cdot)$ represent the layer normalization~\cite{ba2016layer}, the multi-layer perception, linear layer and pooling layer, and ${\bf x}_{s_4}$ is the $\mathbf{y}$ in Equation~\ref{S4_conv}. 

\begin{figure}[]
\centering
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=1.0\linewidth]{figures/num_frame2.pdf}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \includegraphics[width=1.0\linewidth]{figures/mask_ratio2.pdf}
\end{subfigure}
\caption{Performance gain/loss of ViS4mer on LVU dataset~\cite{lvu2021} with different settings of input frames and random masking ratio, where we conclude: \textbf{(a).} The performance is not substantially improved with increasing number of input frames. \textbf{(b).} Random masking strategy cannot effectively reduce redundant tokens.}
% \textbf{Top:} Compared to the experiment using 60 seconds' input, we show the performance improvement ratio with increasing temporal extent, \textit{i.e.}, 80, 100 and 120 seconds. \textbf{Bottom:} Compared to the experiment using 60 seconds' input without random masking, we show the performance improvement ratio using the same temporal extent with increasing masking ratio, \textit{i.e.}, $20\%$, $50\%$ and $80\%$.
\label{concept_result}
\vspace{-2mm}
\end{figure}


\begin{figure*}[]
\centering
  \includegraphics[width=0.9\linewidth]{figures/pipeline4.pdf}
\caption{(a) A visualization of our proposed S5 model. Compared to the S4 model, we introduce a selective token picking strategy ``mask generator'', leverageing the S4 feature from the momentum S4 model. The momentum S4 model is updated by the S4 model in the moving average manner. Both S4 model and momentum S4 model are consisted of a S4 layer~\cite{islam2022long, gu2021efficiently} and a LN layer~\cite{ba2016layer}. (b) An illustration of the proposed LSMCL pretraining framework, that initializes our S5 model to enrich the robustness.} %\xy{a remind on discriminating ``random mask set'' versus output of ``mask generator''.fixed} \xy{title of a and b in the figure itself, better to change font to ``Times New Roman'', ``momentum updated S4 model'' can be just ``momentum S4 model''.fixed}
\label{Fig:pipeline}
\vspace{-3mm}
\end{figure*}


\subsection{S4 Model in Long-form Video Understanding}
\label{ss:vis4mer_limitations}
% From the recurrent expression in the Equation~\ref{S4} and~\ref{S4_conv}, it is known that each output token from S4 model is the result of structured discrete convolution for all previous inputs. Given the ViS4mer~\cite{islam2022long} feeds image tokens from all space-time locations into the S4 model, we argue that it is not efficient and appealing for S4 model capturing effective long-term dependencies, as not all tokens have the temporal relations and each task may also favor tokens in different space-time locations.  From the observation that short-form video understanding tasks often benefit from longer input clips, we wonder if the performance of different long-form video tasks would also be substantially improved with the increasing number of input frames.

% \xy{actually, this paragraph just shows equally selected frames are sub-optimal. Now it is written just like one pagraph in experimetn section. I suggest to be concise, saving the space for more figures in the experiment, i.e., show up the selection of the tokens.} to be done and discussed
\noindent To better understand the S4 model and long-form video understanding tasks, we re-implement ViS4mer~\cite{islam2022long} with different settings on LVU dataset~\cite{lvu2021} and demonstrate the result in Figure~\ref{concept_result}. From the observation that short-form video understanding tasks often benefit from longer input clips~\cite{bertasius2021space,fan2021multiscale,liu2021video,wang2022long}, we wonder if the performance of S4 model on different long-form video tasks would also be substantially improved with the increasing number of input frames. In Figure~\ref{concept_result} (a), we gradually increase the temporal extent from $60$ seconds to $120$ seconds. Compared to the performance of using 60 second input, we report the impact ratio of using $80$, $100$, $120$ second inputs in each task. From this Figure, we realize that not all long-form video tasks benefit from longer input context, and for those improved tasks, the performance is not necessarily improved with the longer input content. As a result, we raise the hypothesis that capturing long-term relationships is task- and data-dependent, and that additional performance improvements for those temporally-intensive tasks would also be hindered by the redundant spatiotemporal tokens produced by longer input content. Recalling Equation~\ref{S4} and~\ref{S4_conv}, each output token from S4 model is the result of structured discrete convolution for all previous inputs. Thus, we argue that treating all input token equally as ViS4mer~\cite{islam2022long} does is not appealing for S4 model to capture effective long-term dependencies, as not all tokens have the temporal relations and each task may also favor tokens in different space-time locations. To naively reduce the redundant tokens, we generate random masks on the 60 second input clips to drop tokens and increase the masking ratio from $20\%$ to $80\%$. Compared to the performance of un-masked input, we report the impact ratio of using random mask with masking ratio of $20\%$, $50\%$ and $80\%$ in Figure~\ref{concept_result} (b). Despite the minor improvement in some tasks, random masking degenerates the performance of most tasks, so it is not an effective method for reducing the redundancies. To this end, we are motivated to propose a selective S4 model which adaptively pick discriminative image tokens for the S4 model in different long-form video understanding tasks. 

% In Figure~\ref{concept_result}(a), to reduce the number of tokens, we generate random masks on the 60 seconds' input clips and increase the masking ratio from $20\%$ to $80\%$. Compared to the performance of un-masked input, we report the impact ratio of using random mask with masking ratio of $20\%$, $50\%$ and $80\%$. Similarly, in Figure~\ref{concept_result}(b), we gradually increase the temporal extent from $60$ seconds to $120$ seconds. Compared to the performance of using 60 seconds' input, we report the impact ratio of using $80$, $100$, $120$ seconds' temporal extent in each task.  From Figure~\ref{concept_result}, we realize that the performance of some tasks in the LVU are improved with increasing temporal content, while the performance of rest tasks fluctuate with it. We believe this is due to the conflict between additional temporal content and useless tokens introduced by extra frames. Although random masking generates minor improvement in the task of genre, it degenerates the performance of most tasks, and thus, not appealing for improving the effectiveness of S4 model.  To this end, we are motivated to propose a selective S4 model which adaptively pick discriminative image tokens for the S4 model in different long-form video understanding tasks. We hypothesize that various long-form video comprehension tasks would call for diverse data distributions that are created by different image tokens in the video. In the S4 model, the same group of image tokens that is discriminative in one LTI system (for one task) might be treated as randomly interpolated noise signal in another LTI system (for a different task).




\subsection{Adaptive Token in Long-form Videos} 
\label{ss:adaptive_tokens}
\noindent To pick out discriminative image tokens from the long-form videos among various tasks, we extend the concept of adaptive token learning, formulating our Selective S5 (\textit{i.e.}, selective S4) model. Unlike previous image-based adaptive token learning works~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not} that rely on dense self-attention for capturing token-wise relationships, our S5 model avoids the self-attention computation in long-form videos by leveraging S4 features generated from the simulated linear time-invariant (LTI) system. Inherited from the linear complexity of the S4 model, our S5 model can receive long-form video token dependencies with low cost, thus making the adaptive token learning possible in long-form videos. In addition, we propose a momentum updated S4 model to dynamically produce S4 features from the long-form video data in different tasks. Figure~\ref{Fig:pipeline} (a) demonstrates the pipeline of our S5 model, where the momentum updated S4 model is the moving average of the S4 model.

% We extend the concept of adaptive token learning into this problem, which aim at picking out discriminative image tokens from the long-form video in various tasks. Unlike previous adaptive token learning works~\cite{yin2021adavit, meng2022adavit,rao2021dynamicvit,liang2022not} that generate independent patch-wised policy, we leverage the sequential output from S4 model to make decisions. This has a few advantages as 1) feature from S4 model could provide long-term contextual information, which could improve the robustness of the decision maker; 2) the S4 feature is generated through S4 model with forward pass only, so the extra computational cost is negligible; 3) unlike the raw input, this feature is dynamic changed with the updates of network, which continuously feedback the decision maker. Figure~\ref{Fig:pipeline} demonstrates the pipeline of our S5 model, in which we equip the S4 model with an adaptive token selection mechanism. As the S4 feature embed rich temporal relations, we formulate a shadow S4 model to generate the input to our token selection model. This shadow S4 model is the moving-averagely updated by the S4 model. 


%\begin{figure*}[]
%\centering
%\begin{subfigure}{.59\textwidth}
%  \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/pipeline3.png}
%  \caption{}
% \label{Fig:pipeline}
%\end{subfigure}
%\begin{subfigure}{.39\textwidth}
%\centering
%  \includegraphics[width=1.0\linewidth]{CVPR2023/latex/figures/LSMCL.png}
%  \caption{}
% \label{LSMCL}
%\end{subfigure}
%\caption{(a) A visualization of our proposed S5 model. Compared to the S4 model, we introduce a selective token picking strategy, which leverages the S4 feature from the shadow S4 model. The shadow S4 model is updated by the S4 model in the manner of moving average. Both S4 model and shadow S4 model are consisted of a S4 layer~\cite{islam2022long, gu2021efficiently} and a LN layer~\cite{ba2016layer}. (b) An illustration of proposed LSMCL.}
%\label{masking_and_seqlen}
%\end{figure*}


Specifically, we cast our selective module in the S5 model as an adaptive mask learning problem. Given a mask generator $\text{MG}(\cdot)$ and its input ${\bf x}_{s_4}$, the mask generator is a lightweight architecture, which will be ablated in the Section~\ref{exp}. It will be trained for a classification task on predefined category space $\mathbb{C}=\{C_1,\dots,C_{\text{ST}}\}$, where $\text{S}\cdot \text{T}$ is the total number of image tokens in the video. Let's denote $p(c|{\bf x}_{s_4}) \in [0,1]$ be the normalized probabilistic output of $MG({\bf x}_{s_4})$, so that $\sum_{c=C_1}^{c=C_{\text{ST}}} p(c|{\bf x}_{s_4}) = 1$. Then, we sample $K$ categories without replacement from the probabilistic outputs of the mask generator. Finally, the $k^{th}$ selected image tokens can be written as:
\begin{align}
\begin{split}
\label{E_masking}
x_{\text{in}}^k = \mathbf{X^T}c^k
\end{split}
\end{align}
Where $\mathbf{X} \in \mathbb{R}^{\text{ST}\times D}$ represents $\text{S}\cdot \text{T}$ D-dimensional image tokens and $c^k$ is a one-hot vector that select $k^{th}$ token from the $\mathbf{X}$. The sampling process is important as it prevents the bias in the training that is potentially caused by the top-K selection. To make this sampling differentiable, we adopt the Gumbel-Softmax with Straight-Through tricks~\cite{jang2016categorical}, which is widely used in~\cite{lin2021vx2text,meng2022adavit}. Specifically, we introduce an additional gumbel noise $g \in \mathbb{R}^{1\times \text{ST}}$ into the predicted probability distribution $p \in \mathbb{R}^{1\times \text{ST}}$, where $g = -\log(-\log(u+\epsilon)+\epsilon)$ ($u\sim \text{Uniform(0,1)}$ , and $\epsilon$ is a small value for arithmetic robustness consideration). Then, we sample the top-K tokens from the re-parameterized distribution $p+g$. During the back-propagation, we estimate the gradient for each selected token $c$ as:
\begin{align}
\begin{split}
\label{straight-through}
 G\approx \bigtriangledown_{\text{MG}}\frac{\exp((\log p(c|{\bf x}_{s_4}) + g(c))/\rho)}{\sum_{c'=C_1}^{c'=C_{\text{ST}}}\exp((\log p(c'|{\bf x}_{s_4}) + g(c'))/\rho)}
\end{split}
\end{align}
where $\rho$ is the temperature factor controlling the sharpness.

\subsection{Long-Short Mask Contrastive Learning} 
\label{ss:LSMCL}
\noindent Previous token reduction/adaptive learning works rarely take model robustness into consideration. Informative tokens might be incorrectly dropped during training, which could hurt the performance of the model. In this paper, in addition to our proposed S5 model that explicitly picks informative tokens for various long-form video understanding tasks, we also propose Long-Short Mask Contrastive Learning (LSMCL) pretraining, which implicitly learns long-form video representations with better generalizability. Specifically, we equip the recent video contrastive learning framework LSTCL~\cite{wang2022long} with a random masking strategy on both long and short input clips, which mimics all possible scenarios that the selective module could produce in the S5 model. As a result, our S5 model with LSMCL pretraining would be more robust to and tolerant of errors from the selective module. Moreover, the long-short contrastive set-up will further improve the temporal predictability of our S5 model.
% Another direction to improve the temporal predictability of our S5 model is from the observation of recent video contrastive learning works~\cite{recasens2021broaden,wang2022long,Feichtenhofer_large}. Especially, both LSTCL~\cite{wang2022long} and BraVe~\cite{recasens2021broaden} leverage the short and long clips from the same video, formulating positive pairs in the contrastive learning. This essentially forces the model to learning long-term representation from short clips and vice versa. In this paper, we investigate this idea on S4 model and further improve it by introducing masking strategy, which better fits our S5 in the fine-tuning. 

Formally, we sample a long clip $(x_L)$ and a short clip $(x_S)$ from each video sequence with largely different sampling strides $\tau_L$ and $\tau_S$, where $\tau_S < \tau_L$. Unlike LSTCL~\cite{wang2022long} and BraVe~\cite{recasens2021broaden} that apply independent random sampling, in our paper the temporal span of long clips includes the one of short clips, which prevents dissimilar semantics from two clips in long-form videos. Then, we independently generate binary random masks with a masking ratio of $\eta$ for each clip, which can be written as: $\mathcal{R}_\text{mask}(x,\eta), x\in \{x_L,x_S\}$. We set S4 model as the backbone of the query encoder $(f_q)$ and also adopt a momentum key encoder $(f_k)$ in the pipeline, which is widely accepted in MoCo~\cite{he2020momentum}, BYOL~\cite{grill2020bootstrap} and LSTCL~\cite{wang2022long}. Our query encoder and key encoder follow the same design with\cite{he2020momentum,grill2020bootstrap,wang2022long}, that consist of the backbone, projection and prediction heads. Denoting the parameter of $f_q$ as $\theta_q$ and the one of $f_k$ as $\theta_k$, we have: $\theta_k = m\theta_k + (1-m)\theta_q$, where $m \in [0,1]$ is a momentum coefficient. Similarly, the LSMCL adoptes similar objective as the InfoNCE~\cite{oord2018representation}:
% \begin{equation}
% \vspace{-2mm}
% \label{nce}
%     \mathcal{L}_{\text{NCE}} = \sum_{i} -\log\frac{\exp({q^i}^\top k^i/\rho)}{\exp({q^i}^\top k^i/\rho)+\sum_{j \neq i}\exp({q^i}^\top k^j/\rho)}
% \end{equation}
\begin{align}
\begin{split}
&\text{Given:}\ q = f_q(\mathcal{R}_\text{mask}(x_S,\eta)), k = f_k(\mathcal{R}_\text{mask}(x_L,\eta))\\
&\mathcal{L}_{\text{LSMCL}} = \sum_{i} -\log\frac{\exp({q^i}^\top k^i/\rho)}{\exp({q^i}^\top k^i/\rho)+\sum_{j \neq i}\exp({q^i}^\top k^j/\rho)}\\
\end{split}
\end{align}
where $\rho$ is the temperature hyperparameter. As is commonly done in~\cite{chen2021empirical, chen2020exploring, grill2020bootstrap,caron2020unsupervised}, we symmetrize the loss function by switching $x_S$ and $x_L$ in $f_q$ and $f_k$. In our LSMCL, the S4 model is learned to find the correct step size $\Delta$ and SSM parameters to match the representation of random masked long and short clips. Given our S5 model takes adaptively learned image tokens in the downstream task, we believe the LSMCL could improve the robustness as well as the temporal modeling ability of S5 model when dealing with partially sampled image tokens. In Section~\ref{exp}, our S5 model with LSMCL empirically shows significantly improved results in long-form video understanding.
%it is empirically demonstrated that the performance of S5 model with LSMCL is significantly improved in the long-form video understanding.
%  \xy{from Mohamed: masking is not clearly indicated into the design of the loss learning.} where $q=f_q(x_S')$, $k=f_k(x_L')$ and