\begin{abstract}
% Long-form video understanding is an active yet challenging research field, due to the complexity of long-term spatial-temporal dependency and high computational cost. The recent structured state-space sequence (S4) model offers a promising linear complexity solution. However, we demonstrate that treating all image tokens equally as S4 model does adversely affects both its efficiency and accuracy. To address this issue, we propose a Selective S4 (S5) model by adopting a lightweight mask generator to adaptively pick informative image tokens. Different from previous mask-based token reduction methods in transformers, our S5 model avoids the dense self-attention calculation and benefits from the guidance of momentum updated S4 model, making it a practically easy operation to discard less informative tokens and adaptive to different tasks from the long-form videos. Meanwhile, the informative image tokens could also be dropped incorrectly in the early training stage. To improve the robustness, we propose a novel long-short masked contrastive learning (LSMCL) pretraining method, which also broadens the temporal horizon of our model via predicting the longer temporal context using shorter videos. Extensive experiments on three challenging long-form video understanding datasets (LVU, COIN and Breakfast) demonstrate the new state-of-the-art results. Specifically, our method outperforms the previous S4 method by up to $9.6\%$ in the long-form video classification task while reducing the memory footprint by $40\%$.

\noindent Effective modeling of complex spatiotemporal dependencies in long-form videos remains an open problem. The recently proposed Structured State-Space Sequence (S4) model with its linear complexity offers a promising direction in this space. However, we demonstrate that treating all image-tokens equally as done by S4 model can adversely affect its efficiency and accuracy. To address this limitation, we present a novel Selective S4 (\textit{i.e.}, S5) model that employs a lightweight mask generator to adaptively select informative image tokens resulting in more efficient and accurate modeling of long-term spatiotemporal dependencies in videos. Unlike previous mask-based token reduction methods used in transformers, our S5 model avoids the dense self-attention calculation by making use of the guidance of the momentum-updated S4 model. This enables our model to efficiently discard less informative tokens and adapt to various long-form video understanding tasks more effectively. However, as is the case for most token reduction methods, the informative image tokens could be dropped incorrectly. To improve the robustness and the temporal horizon of our model, we propose a novel long-short masked contrastive learning (LSMCL) approach that enables our model to predict longer temporal context using shorter input videos. We present extensive comparative results using three challenging long-form video understanding datasets (LVU, COIN and Breakfast), demonstrating that our approach consistently outperforms the previous state-of-the-art S4 model by up to $9.6\%$ accuracy while reducing its memory footprint by $23\%$.


% \xy{comments from discussions:(1) not within transformer video understanding field, long-form video is not a commonly known concept. (2) Structured State-Spaces is too way technical concept driven, not a good term in title for general people to catch. (3)fixed: ``Noisy'', try to avoid, it is too vague. LSMCL is to enrich robustness, where possibly informative tokens are dropped. With the random masked sets for contrastive learning, the embedding is learned towards robust.}

% % rewritten by pichao
%   Video vision transformers have shown initial success in long-form video understanding by memory bank or hierarchical representation, but the quadratic complexity to the number of image tokens still limits its full usage. The recent structured state-space sequence (S4) model offers a promising linear complexity solution. However, we demonstrate that treating all image-tokens equally as S4 model does adversely affects both its efficiency and accuracy. To address this issue, we propose a Selective S4 (S5) model by adopting a lightweight mask generator to adaptively choose informative image tokens. Different from previous mask-based token reduction methods in image transformers, the proposed token selection method avoids the dense self-attention calculation and benefits from the guidance provided by momentum updating of S4 model itself, making it a practical easy operation to throw out the useless tokens and dynamic to the long-form video data of different tasks. However, the image tokens could be removed or selected incorrectly due to the noisy calculation, and we propose a novel long-short masked contrastive learning (LSMCL) approach to make it tolerant to errors.  LSMCL enables our model to predict longer temporal context using randomly masked shorter videos, mimicking the noisy selected image tokens. Extensive experiments on three challenging long-form video understanding datasets (LVU, COIN and Breakfast) demonstrate new state-of-the-art results. Specifically, our method outperforms the previous S4 method by up to $9.6\%$ in the long-form video classification task while reducing the memory footprint by $40\%$.

% % a trial from Xiang
%   \xy{Long-form video understanding is an active yet challenging field, due to its complexity of long-term spatial-temporal dependency and high computational cost. A recent transformer-based structured state-space sequence (S4) method presents a linear complexity solution under the linear time-invariant system theorem. But the equal importance assumption across all the frames largely impedes the effectiveness and efficiency. In this work, we propose a Selective S4 (S5) pipeline by introducing a gumble-softmax sampling to adaptively select more informative image tokens, avoiding dense self-attention calculation from traditional transformer methods. To mutually distill the spatial-temporal consistency between long and short clips, we further propose a long-short masked contrastive learning (LSMCL) objective, where a random masking is newly applied to enhance the penalization and the efficiency. Extensive experiments on three challenging long-form video understanding datasets (LVU, COIN and Breakfast) demonstrate new state-of-the-art results. Specifically, our method outperforms the previous S4 method by up to $9.6\%$ in the long-form video classification task while reducing the memory footprint by $40\%$.}




%% jue's last version

  %Previous work on video modeling and analysis has primarily focused on short videos that are typically a few seconds duration. Efficient modeling of complex long-term spatiotemporal dependencies in long-form videos therefore remains to be an open problem. The linear complexity offered by the recently proposed structured state-space sequence (S4) model offers a promising solution to address this problem. However, we demonstrate that treating all image-tokens equally as done by the S4 model can adversely affect both its efficiency as well as accuracy. To address this limitation, we present a Selective S4 (S5) model that leverages the global sequence-context embedded in the S4 features to adaptively chooses informative image tokens, which results in more efficient and accurate modeling of long-term spatiotemporal dependencies in videos. To further increase the robustness and temporal horizon of our model, we propose a novel long-short masked contrastive learning (LSMCL) approach that enables our model to predict longer duration temporal context using relatively shorter duration input videos. We present comparative experiments on three challenging long-form video understanding datasets (LVU, COIN and Breakfast), which demonstrates the new state-of-the-art results. Moreover, our method outperforms the previous S4 method by up to $9.6\%$ in the long-form video classification task while reducing the memory footprint by $40\%$.

    % Video understanding has been widely studied in the last few decades. However, most works focus on tasks in the short video format, which is typically a few seconds' long. Thus, how to effectively model long-term videos is yet to be solved, which requires to capture complex long-range temporal reasoning. The recent work of Structured State-Space Sequence (S4) Model provides a promising directions for learning long-term dependencies. The linear complexity of S4 model makes it practically more useful than the transformer architecture in the long-form video understanding. However, treating all image tokens equally in the S4 model is neither effective nor advantageous, as not of them may characterize the underlying dependencies for various long-form video understanding tasks. In this paper, we present the Selective S4 (S5) model to address this issue, which leverages the global context embedded in S4 features to adaptively chooses informative image tokens for long-form video comprehension. To further broaden the temporal horizon of our S5 model, we also propose a long-short mask contrastive learning (LSMCL) method that enables the model to predict longer temporal context from shorter clips. The new state-of-the-art performance on three challenging long-form video benchmarks (LVU, COIN and Breakfast) demonstrates the effectiveness of our proposed method. Moreover, compared to the the S4 model, our S5 model achieves up to $4.3\%$ averaged accuracy improvement and reduce the computational/memory cost by $40\%$.
    

  
%   We present comparative experiments using three challenging long-form video understanding benchmark datasets (LVU, COIN and Breakfast) to demonstrate that our approach consistently outperforms previous state-of-the-art S4 model by up to $4.3\%$ average accuracy while reducing its memory footprint by $40\%$.
  
  
    % Raffay version: Efficient modeling of complex long-term dependencies in videos remains an open problem. The recently proposed ViS4mer model attempts to address this challenge by incorporating self-attention and structured state-space sequence (S4) layer. However, we demonstrate that treating all image-tokens equally as done by ViS4mer can negatively impact model accuracy. To address this challenge, we present a novel approach to leverage the global context embedded in the features of the S4 layer in order to adaptively choose informative image tokens and model long-term dependencies in videos more effectively. To further increase the temporal horizon of our model, we propose a novel approach of long-short mask contrastive learning (LSMCL) that enables our model to predict longer temporal context using shorter duration video inputs. We present experiments using three challenging long-form video understanding benchmarks (LVU, COIN and Breakfast) and demonstrate that our approach consistently offers state-of-the-art results beating the previous state-of-the-art S4 model by up to $xx\%$ accuracy and offering improved computational and memory costs by $xx\%$ and $yy\%$ respectively.
    
    % Video understanding has been widely studied in the last few decades. However, most works focus on tasks in the short video format, which is typically a few seconds' long. Thus, how to effectively model long-term videos is yet to be solved, which requires to capture complex long-range temporal reasoning. The recent work of Structured State-Space Sequence (S4) Model provides a promising directions for learning long-term dependencies. The linear complexity of S4 model makes it practically more useful than the transformer architecture in the long-form video understanding. However, for learning the specific S4 model for various long-form video tasks, treating all image tokens equally as the input signal is neither effective nor advantageous, and each simulated linear time invariant (LTI) system requires distinct combinations of input. To tackle this problem, we extend the concept of adaptive token learning into the area of long-form video modeling and empirically demonstrate our proposed S4 model with adaptive token learning scheme learns discriminative video representations by effectively selecting informative tokens from long-form videos. To improve the temporal predictability and robustness of the S4 model, we also propose a long-short mask contrastive learning (LSMCL) method that forces the model to match the representation of masked long and short clips. The new state-of-the-art performance on three challenging long-form video benchmarks (LVU, COIN and Breakfast) demonstrates the effectiveness of our proposed method. Moreover, compared to the vanilla S4 model, our algorithm achieves up to $xx\%$ accuracy improvement and reduce the computational/memory cost by $xx\%$.
    
    
    % Although recent works of video transformers provide competitive directions for learning long-term dependencies, it is impractical in long-term modeling due to its quartic computational cost. In this paper, we extend the concept of adaptive vision transformer from the image domain to the video domain, demonstrating an efficient and novel long-term video modeling method with dynamic masking strategy. From extensive experimental results, we empirically found that the fixed size input clip was not appealing due to the nature of different video tasks.To reduce the computational cost while learning the discriminative video representations, we generate input-conditioned mask to capture the meaningful spatiotemporal image patches by leveraging self-calibrated feature from the intermediate layer. Our dynamic masking strategy can be applied in any video transformer architectures. Compared to the baseline model, our method achieves competitive and even better results on \textbf{nine} challenging long-term video modeling tasks while saving the cost by up to $40\%$.  \JW{to be rewritten}
\end{abstract}