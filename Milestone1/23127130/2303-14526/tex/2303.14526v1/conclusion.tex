\section{Conclusion}
% In this paper, we propose am efficient model for long-term video modeling by introducing the feedback mask generating system into the ViS4mer. Unlike previous adaptive token learning works, our proposed method is designed for long-term video modeling via utilizing richer spatiotemporal information from the intermediate decoding layer. Compared to the baseline ViS4mer, our model achieves up to $3\%$ improvement with only half of original memory usage. Due to the high efficiency nature of our model, a video clip that includes 120 frames can be easily feed into our model, achieving the state-of-the-art performance in \textbf{9} challenging long-term video modeling tasks. Our proposed mask generator help to generate adaptive spatiotemporal tokens that meet the different feature requirements of each task.
\noindent In this paper, we proposed a selective structured state-space sequence (S5) model for long-form video understanding, where we adopt a lightweight mask generator to adaptively pick informative tokens from long-form videos. Our mask generator avoids dense self-attention computation as what is applied in previous works. It leverages the sequential output of the simulated linear time invariant (LTI) system, and benefits from the momentum distillation of S4 model, enabling our S5 model to dynamically learn from informative tokens for different long-form video tasks. To mitigate the negative impact of picking less informative tokens, we also propose a LSMCL pretraining to improve the robustness and further broaden the temporal horizon of our model. Through extensive experiments, we demonstrate the effectiveness of each proposed component in our S5 model, achieving the new state-of-the-art performance in three challenging long-form video understanding benchmarks.