{
    "arxiv_id": "2303.15621",
    "paper_title": "ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization",
    "authors": [
        "Zheheng Luo",
        "Qianqian Xie",
        "Sophia Ananiadou"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "The performance of abstractive text summarization has been greatly boosted by pre-trained language models recently. The main concern of existing abstractive summarization methods is the factual inconsistency problem of their generated summary. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference and question answering et al. However, they have limitations of high computational complexity and relying on annotated data. Most recently, large language models such as ChatGPT have shown strong ability in not only natural language understanding but also natural language inference. In this paper, we study the factual inconsistency evaluation ability of ChatGPT under the zero-shot setting by evaluating it on the coarse-grained and fine-grained factuality evaluation tasks including binary natural language inference (NLI), summary ranking, and consistency rating. Experimental results show that ChatGPT outperforms previous SOTA evaluation metrics on 6/9 datasets across three tasks, demonstrating its great potential for assessing factual inconsistency in the zero-shot setting. The results also highlight the importance of prompt design and the need for future efforts to address ChatGPT's limitations on evaluation bias, wrong reasoning, and hallucination.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15621v1"
    ],
    "publication_venue": "ongoing work, 12 pages, 4 figures"
}