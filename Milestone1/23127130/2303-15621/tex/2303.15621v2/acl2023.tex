% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{graphicx}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ChatGPT as a Factual Inconsistency Evaluator for Text Summarization}
\author{Zheheng Luo, Qianqian Xie\thanks{\phantom{h}Corresponding author}, Sophia Ananiadou \\
Department of Computer Science, The University of Manchester \\ \{zheheng.luo, qianqian.xie, sophia.ananiadou\}@manchester.ac.uk}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\begin{document}
\maketitle
\begin{abstract}
The performance of text summarization has been greatly boosted by pre-trained language models.
A main concern of existing methods is that most generated summaries are not factually inconsistent with their source documents.
To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference, question answering, and syntactic dependency et al.
However, these approaches are limited by either their high computational complexity or dependence on annotated data.
Most recently, large language models(LLMs) such as ChatGPT have shown excellent performance in not only text generation but also language comprehension.
In this paper, we particularly explore ChatGPT's ability to evaluate factual inconsistency under a zero-shot setting by examining it on both coarse-grained and fine-grained  evaluation tasks including binary entailment inference, summary ranking, and consistency rating.
Experimental results indicate that ChatGPT generally outperforms previous evaluation metrics across the three tasks, indicating its great potential for factual inconsistency evaluation. However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.


%vulnerability to lexical similarity, 

%demonstrating its great potential for assessing factual inconsistency in the zero-shot setting.
%The results also highlight the importance of prompt design and the need for future efforts to address ChatGPT's limitations on evaluation bias, wrong reasoning, and hallucination.
\end{abstract}

\section{Introduction}
Recently, pre-trained language models have greatly improved the performance of automatic text summarization~\cite{liu-lapata-2019-text,lewis-etal-2020-bart,zhang2020pegasus}.
However, a major concern that has limited existing state-of-the-art text summarization methods is factual inconsistency, namely, the generated summaries containing information that is not entailed by input  documents\footnote{the problem is also referred to as unfaithfulness, we use these terms interchangeably in the following }~\cite{Wojciech2020,Maynez2020}. 
To fill the gap, significant efforts have been made in developing automatic evaluation metrics for assessing the factuality of generated summaries, such as semi-supervised method FactCC~\cite{Wojciech2020}, question-answering based approach FEQA~\cite{wang-etal-2020-asking} and QuestEval~\cite{Scialom2021}, and natural language inference (NLI) based method SummaC~\citep{laban2022summac}.
Nevertheless, existing evaluation metrics either have high computational complexity which requires training on a huge amount of data or rely on multi-model combined pipelines, putting in more uncertainties during inferences. Moreover, evaluations based on these metrics exhibit limited agreement with human assessments~\cite{Pagnoni2021}.
Inspired by the ability of pre-trained language models (PLMs) on natural language understanding and generation, a few efforts have been devoted to building data and computational-efficient evaluation metrics based on PLMs like BARTScore~\cite{yuan2021bartscore}.

Most recently, large language models (LLMs), such as GPT-3~\citep{brown2020language}, InstructGPT~\citep{Ouyang2022TrainingLM}, PaLM~\citep{chowdhery2022palm}, and BLOOM~\citep{Scao2022}, have dwarfed small scale fine-tuned models in various natural language processing tasks often requiring only few-shot or zero-shot learning.
These LLMs have demonstrated exceptional performance not only in natural language understanding and generation but also in their ability to perform inference and reasoning tasks." Specifically, equipped with explicitly designed prompts, LLMs can better solve a range of various reasoning tasks in terms of arithmetics, symbolic, and logic~\citep{Kojima2022,Wei2022}. Moreover, the most recent effort ChatGPT~\cite{ChatGPT2022} in particular has been proven to obtain strong natural language inference ability, surpassing fine-tuned pre-trained language models (PLMs) on several datasets~\citep{Zhong2023}. As a result, researchers have paid closer attention to using large language models (LLMs) to evaluate generated text. 
\citet{Kocmi2023LargeLM} investigated the use of rating-based prompts in translation evaluation and achieved better accuracy compared to other metrics across three language pairs. Inspired by this work, \citet{wang2023chatgpt} extends the method into a broader natural language generation field including summarisation, where ChatGPT shows dominating alignment with human rating on four attributes including coherence, relevance, fluency, and consistency. 
However, their experiments only use a single summarisation evaluation dataset and rather focus on exploring ChatGPT to evaluate the overall quality of generated summaries. In addition, they solely framed the evaluation as a marking task and compared the results with only general text generation metrics such as ROUGE~\cite{lin-2004-rouge} and BERTScore~\cite{zhangbertscore}, which have been proven to be not effective in assessing factual consistency~\cite{Maynez2020}.
Metrics proposed specifically for assessing inconsistency such as FactCC, DAE~\citep{Goyal2020}, SummaC have not been examined, leaving a huge gap for a thorough exploration of using ChatGPT to assess the factual consistency in text summarisation.
%Inconsistency, also referred to as unfaithfulness\citep{Maynez2020}, is a problem that occurs when a summary contains information that is not supported by the source document. Recent studies found this problem prevelant in both extractive\citep{Zhang2022ExtractiveIN} and abstractive summarization methods\citep{Pagnoni2021}, and pose a huge challenge for generating high-quality summaries. To tackle this major limitation in text summarisation, various methods have been proposed to detect inconsistency in generated summaries. Among existing attempts, NLI-based methods\citep{Wojciech2020, Laban2021, } achieve overall better performance by not only showing better accuracy but also lead in processing speed. Despite the efficiency of the aforementioned approaches, they still requires supervised training on augmented data or transferring from existing NLI models. The zero-shot inference ability of ChatGPT has not yet been tested in such task.
%Considering the inference ability of LLMs, : Is ChatGPT a better inconsistency detector  for text summarisation?

To fill the gap, in this paper, we conduct a preliminary study of how ChatGPT can perform in both coarse-grained and fine-grained factual inconsistency evaluation from three tasks including inconsistency detection as entailment inference (EI), consistency comparison as summary ranking, and quantitative judgement as consistency rating. We design different prompts on both zero-shot and zero-shot chain-of-thought (CoT)~\cite{Kojima2022} to explore the factuality assessment ability of ChatGPT. We conduct experiments on the benchmark of the EI-based inconsistency detection task including six large standardized datasets, and existing datasets on the other two tasks, and compare the results with SOTA evaluation methods. From experimental results and analysis, we have the following findings:
\begin{enumerate}
    \item  ChatGPT shows great potential for evaluation factuality of text summarisation under the zero-shot setting and outperforms previous SOTA evaluation methods on most datasets across three tested tasks.
    
     \item  Though showing remarkable performance measured by numeric metrics, ChatGPT is found to have the preference to predict a document and a claim is consistent when the lexical similarity is high without considering the semantic entailment between them. Moreover, evidence of ChatGPT conducting false inferences has been observed, revealing the limitation of ChatGPT's language reasoning ability.
    
    \item Despite effectively instructing ChatGPT to detect inconsistency, the tested prompts are not able to keep the output constantly sticking to the given requirements, indicating the insufficient prompting of ChatGPT.
\end{enumerate}
To the best of our knowledge, we are the first to systematically explore ChatGPT's ability in evaluating factual consistency for text summarization. Overall, our results show a comparable if not better performance of ChatGPT than SOTA evaluation metrics, but concerns remain on lexical biases, false reasoning, and inadequate alignment which are expected to be addressed to improve its reliability.

\section{Related Work}
\subsection{Factuality Evaluation in Text Summarization}
Existing factuality evaluation metrics generally can be classified into unsupervised and semi-supervised methods.
Unsupervised evaluation metrics generally include information extraction (IE) based methods, natural language inference (NLI) based methods, and question answering (QA) based methods.
\citet{goodrich2019assessing} proposed the model-based factuality evaluation metric to calculate the overlap of relation tuples (subject, relation, object) that are extracted from generated summaries and the ground truth by the information extraction (IE) model.
\citet{nan-etal-2021-entity} proposed the new evaluation metric assessing the entity-level factuality consistency of generated summaries.

Besides the IE-based methods, natural language inference (NLI) is also explored for factuality evaluation by assessing whether the generated summary is entailed by the input document.
\citet{falke-etal-2019-ranking} found the factuality evaluation methods trained on the NLI datasets have a poor ability to the assessment of text summarization.
\citet{mishra-etal-2021-looking} further found that the poor performance of the evaluation methods training with the NLI datasets is caused by the short length of premises in NLI datasets.
Most recently, \citet{laban2022summac} revisited the use of NLI in inconsistency detection by calculating the factuality score based on sentence pairs, and proposed the novel benchmark SUMMAC (Summary Consistency) with six datasets.
SUMMAC is used in our experiments.

Moreover, there is also question answering-based metrics such as FEQA~\cite{durmus-etal-2020-feqa}, QAGS~\cite{wang-etal-2020-asking}, and QuestEval~\cite{Scialom2021}, by assessing the alignment of the generated answer based on the generated summary and the source, with the given question.
Different from unsupervised NLI-based methods, the semi-supervised methods further utilize the synthetic data from text summarization for weakly supervised learning, such as FactCC~\cite{Wojciech2020}.
However, these methods are usually computationally expensive or rely on annotated data~\cite{huang2021factual}.
Inspired by the effectiveness of PLMs, there are efforts on developing factuality evaluation metrics based on the likelihoods of PLMs, that are computation and data-efficient such as BARTScore~\cite{yuan2021bartscore} and T5Score~\cite{qin2022t5score}.

\subsection{ChatGPT for Natural Language Processing}
Most recently, many efforts have explored the zero-shot ability of ChatGPT on various natural language processing tasks~\cite{jiao2023chatgpt,Zhong2023,qin2023chatgpt,bang2023multitask,yang2023evaluations}.
ChatGPT has been proven to exhibit good performance on machine translation~\cite{jiao2023chatgpt}.
On the GLUE benchmark, \citet{Zhong2023} has found ChatGPT shows significantly better performance on inference tasks, has comparable performance on sentiment analysis and question-answering tasks, and has poor performance on paraphrase and similarity tasks when compared with 4 representative BERT-based fin-tuning methods.
\citet{qin2023chatgpt} further shows ChatGPT has superior performance on reasoning-required tasks including dialogue tasks, natural language inference tasks, and question-answering tasks than GPT-3.5, and has worse performance on the summarization task than GPT-3.5.
ChatGPT and GPT-3.5 have comparable performance on sentiment analysis.
\citet{bang2023multitask} shown ChatGPT outperforms SOTA zero-shot methods in 9/13 NLP datasets and has poor performance on low-resource languages such as Marathi, Sundanese, and Buginese.
\citet{yang2023exploring} and \citet{wang2023cross} explored the query and aspect-based text summarization and cross-lingual summarization with ChatGPT, where it shows comparable performance with the fine-tuning-based methods.
\cite{Soni2023ComparingAS} conducted a human evaluation and found reviewers struggle to distinguish hand-written summaries against generated ones from ChatGPT.
\citet{wang2023chatgpt} examined the ability of ChatGPT on evaluating natural language generation (NLG) tasks such as summarization, story generation, and data-to-text tasks.
ChatGPT shows great potential as the NLG metric, whose evaluation results have a high correlation with human judgment.
However, they only utilized one summarisation dataset and focused on exploring ChatGPT to evaluate the relevance by comparing it with non-factuality evaluation metrics such as ROUGE and BERTScore, leaving a huge gap for a thorough exploration of the ability of ChatGPT to assess the factual consistency in text summarisation.

\section{ChatGPT as a Factual Inconsistency Evaluator}
In this section, we introduce the details of three different tasks for detecting inconsistency with ChatGPT including the prompt designing, evaluation setting, tested datasets, and baseline models.

\subsection{Entailment Inference}
\label{task intro: nli}
\noindent\textbf{Evaluation Setting.} Inconsistency evaluation of the generated summary can be cast as a binary natural language inference classification, in which the evaluation model is solely required to assess if the summary is consistent with the source document rather than rating the consistent levels~\cite{laban2022summac}.
Under this framework, two parameters are needed for the prompts: source document and summary.
We provide ChatGPT with the question including the source document and the corresponding generated summary and ask it to answer yes or no to infer the consistency between the source document and the corresponding generated summary, and then we collect the decisions from the outputs and aggregate the results.

\noindent\textbf{Prompts.} We experiment with two different zero-shot prompts in the NLI setting. The first one is based on \textit{direct assessment} by directly asking ChatGPT to answer yes or no given the question. Another is based on \textit{zero-shot Chain-of-Thought} inspired by previous work~\citep{Kojima2022} of adding "let's think step by step" in prompt to encourage LLMs unfolding a chain-of-thought style reasoning process, which has been proved to be effective on several reasoning tasks. We follow the approach to create the second prompt.
The zero-shot template is shown below:
\begin{quotation}
Decide if the following summary is consistent with the corresponding article. Note that consistency means all information in the summary is supported by the article. 
    
    Article: \textcolor{blue}{[Article]}
    
    Summary: \textcolor{blue}{[Summary]}
    
    Answer (yes or no):
\end{quotation}
The zero-shot CoT template is:
\begin{quotation}
Decide if the following summary is consistent with the corresponding article. Note that consistency means all information in the summary is supported by the article. 
    
Article: \textcolor{blue}{[Article]}
    
Summary: \textcolor{blue}{[Summary]}
    
Explain your reasoning step by step then answer (yes or no) the question:
\end{quotation}

When processing the responses, we only consider solid judgment like "the summary is consistent with the article" as consistency, claims such as "partially consistent" or 'mostly consistent' are all deemed as inconsistent.
We also tried to use few-shot prompts. However, we found the performance unstable when changing the label, order, and amount of examples, so we decide to leave it for further exploration.

\iffalse
\begin{table*}[tb]
\centering
\begin{tabular}{p{0.1\linewidth}p{0.1\linewidth}p{0.7\linewidth}}
\hline
\textbf{Task}&
\textbf{Testing Setting} &  \textbf{Template Prompt}\\
\hline
\multirow{6}{*}{NLI}&Zero shot & Decide if the following claim is consistent with the corresponding article. Note that consistency means all information in the claim is supported by the article. Article:[Article] Summary:[Summary], Answer(yes or no):\\
\cline{2-3}
&Zero shot CoT&Decide if the following claim is consistent with the corresponding article. 
 Note that consistency means all information in the claim is supported by the article. Article:[Article] Summary:[Summary], Explain your reasoning step by step then answer(yes or no) the question:\\
\hline
Sum. rank&Zero-shot& Decide which of the following sentence is more consistent with the article sentence. Article Sentence:  [article] Sentence A: [correct sentence] Sentence B: [incorrect sentence] Answer(A or B):\\
\hline
Consist. Rating&Zero-Shot& Score the consistency of the following 
 summary with respect to
the source article with 1 to 100 points. Note that consistency measures how much information in the summary is supported by the source article. 1 means total inconsistency, and 100 means full consistency.
[Summary]
[Source Article]
Marks:\\
\hline
\end{tabular}
\caption{\label{tab: prompt template}
Template prompt used in NLI setting
}
\end{table*}
\fi

\noindent\textbf{Datasets} We evaluate ChatGPT's performance on the SUMMAC benchmark~\citep{laban2022summac} which includes six largest summary inconsistency detection datasets FactCC~\cite{Wojciech2020}, CoGenSumm~\cite{falke-etal-2019-ranking}, XSumFaith~\cite{Maynez2020}, SummEval~\citep{Fabbri2021}, FRANK~\cite{Pagnoni2021}, and Polytope~\citep{huang-etal-2020-achieved}. Notably, not all the datasets in the SUMMAC benchmark are built for binary consistency classification. For example, in SummEval~\citep{Fabbri2021}, generated summaries are marked on consistency over a range from 1-5 points. SUMMAC standardizes the six datasets into a binary classification format where each instance contains a triplet of (document, summary, label). The label is either consistent or inconsistent. Moreover, they manually created validation and test split for datasets where a such split is not conducted and computed the inter-annotator agreement for data with multiple annotators. The statistics of the benchmark are shown in Table \ref{tab: summac}.

\noindent\textbf{Baseline Models.}
We compare ChatGPT's performance with the following methods:
\begin{itemize}
    \item \textbf{NER Overlap} uses the named entity
recognition (NER) model
to detect inconsistency by examining if an entity in the summary is
in the document~\citep{laban-etal-2021-keep}. The tested model considers only a subset of entity types such as PERSON,
LOCATION, ORGANIZATION, etc.

\item \textbf{MNLI-doc} fine-tunes a Roberta model~\citep{Liu2019RoBERTaAR}
on the MNLI dataset~\citep{williams-etal-2018-broad} and labels the document-summary pair by the predicted probability of entailment.

\item \textbf{FactCC}~\citep{Wojciech2020} is a Roberta model fine-tuned on data synthesized by corrupting sentences in the original documents as inconsistent candidates. 


\item \textbf{DAE}~\citep{Goyal2020} is a parsing-based model evaluating inconsistency by examining the entailment of individual dependency arcs.

\item \textbf{FEQA}~\citep{Durmus2020} first generates question-answer pairs from candidate summaries, then compare the answers extracted from the source documents by asking the same questions. Then the answer sets are compared to determine the consistency.

\item \textbf{QuestEval}~\citep{Scialom2021} extends the methods above by adding an information recall score to a QA-based metric.

\item \textbf{SummaC}~\citep{laban2022summac} builds an NLI matrix by splitting the document and summary into sentence sets, then predicts a score for each sentence pair in the matrix. SummaC zero-shot ($Summac_{zs}$) first obtain the maximum along the columns then average over to get a final consistency score. SummaC convolution ($SummaC_{Conv}$) instead trains a convolution layer to predict a score for each column and then uses the mean output as the summary-level score.
\end{itemize}

Detailed implementations of the above models used to compare can be found in~\cite{laban2022summac}. For scoring models, the threshold is selected using the validation set and allowed to vary over different datasets.

\noindent\textbf{Metric.} Due to the unbalanced distribution of positive and negative samples in the testing sets, we choose balanced accuracy~\citep{brodersen2010balanced} as the main metric since it is more sensitive to predictions difference for data of smaller proportions. Balanced accuracy
is defined as the following:
\begin{equation}
bACC = \frac{1}{2}* (\frac{TP}{TP+FN} + \frac{TN}{TN+FP})
\end{equation}
The first term in the equation is sensitivity, which represents the recall of true positives while the next one is specificity standing for the recall of true negatives. We specifically counted the two sub-metrics to analyze ChatGPT's behavior.
\begin{table}[!tb]
\small
\centering
\begin{tabular}{p{0.25\linewidth}p{0.1\linewidth}p{0.1\linewidth}p{0.15\linewidth}p{0.1\linewidth}}
\hline
\textbf{Dataset} &  
\textbf{Valid. size} &\textbf{Test size}&  \textbf{\%Positive}&\textbf{Source}\\%&\textbf{IAA}\\
\hline
CoGenSumm &1281&  400& 49.8 &C\\
XSumFaith &1250&1250 &10.2 &X\\
Polytope &634& 634 & 6.6&C\\
FactCC &931& 503 &85.0&C \\
SummEval &850&850 &90.6 &C \\
FRANK& 671&1575& 33.2 &C+X\\

\hline
\end{tabular}
\caption{\label{tab: summac}
Statistics of datasets in SUMMAC Benchmark.
}
\end{table}

\subsection{Summary Ranking}

\noindent\textbf{Evaluation Setting} Except binary NLI, a model's awareness of factual inconsistency can also be tested on how whether it can rank a consistent summary over an inconsistent one. In this section, we introduce another evaluation task~\textit{Summary Ranking} which is introduced in~\citet{falke-etal-2019-ranking}
and has been tested in other previous work. Specifically, the model will be asked to choose the consistent one over two candidate summaries (one is faithful, the other one is not) given the source document.

\noindent\textbf{Prompts}
We use a zero-shot prompt which directly asks ChatGPT to answer which sentence out of the two candidates is more consistent with the given article sentence.
\begin{quotation}
    Decide which of the following summary is more consistent with the article sentence. Note that consistency means all information in the summary is supported by the article.
    
    Article Sentence: \textcolor{blue}{[article]} 
    
    Summary A: \textcolor{blue}{[correct summary]}
    
    Summary B: \textcolor{blue}{[incorrect summary] }
    
    Answer (A or B):
\end{quotation}

\noindent\textbf{Dataset}
Here we use the dataset built by~\citet{falke-etal-2019-ranking} which contains
373 samples, each containing an input source
document from CNN/DM~\citep{Nallapati2016} and two summary sentences covering the same content. 
One of
the summary sentences are consistent with the article while
the other is inconsistent. 

\noindent\textbf{Baseline Models}
We compare other evaluation models that reported their performance on this dataset including the aforementioned FactCC~\citep{Wojciech2020}, MNLI-doc, DAE~\citep{Goyal2020} and a human judgement from~\cite{falke-etal-2019-ranking}.

\noindent\textbf{Metric} We report the accuracy of models successfully choosing consistent summary over inconsistent one. Specifically, when collecting responses from ChatGPT, we only deem claims that confirm the correct sentence is consistent as correct. Outputs alleging both candidate sentences are consistent or inconsistent are rendered as failures.

\subsection{Consistency Rating}
\noindent\textbf{Evaluation Setting.} Recently, several studies have found when given accordingly request prompts, LLMs are able to mark the quality of generated text from different aspects~\citep{Kocmi2023, Fu2023GPTScoreEA, Wang2023IsCA}. These scores show high correlations with human assessment, suggesting the potential of ChatGPT in predicting fine-grained consistency levels for summarisation. Moreover, in the experiments of the NLI task in Section \ref{task intro: nli}, we found that part of the output judgments is "partially consistent" or "mostly consistent", indicating ChatGPT's awareness of different inconsistency degrees.
Therefore, we apply the consistency rating task on ChatGPT by asking it to mark the consistency of a summary with the reference to its source document on a scale from 1-10 points, where 1 point stands for total inconsistency, and 10 represents full consistency.

\noindent\textbf{Prompts.} Following~\citet{Kocmi2023}'s approach, we design a prompt that requests ChatGPT to evaluate the consistency of a candidate summary w.r.t the source article in a [1-10] scale:
\begin{quotation}
Score the following summary given the corresponding article with respect to consistency from 1 to 10. Note that consistency measures how much information included in the summary is present in the source article. 10 points indicate the summary contains only statements that are entailed by the source document.

\textcolor{blue}{[Summary]: }

\textcolor{blue}{[Source Article]:}

Marks:

\end{quotation}
The definition of consistency is added for the model to better understand the aspect it is to rate.

\noindent\textbf{Datasets.} The original versions of SummEval and FRANK datasets are used on this task given there are detailed consistency scores in their annotations. In SummEval, 1600 summaries were labeled using a 5-point Likert scale along four categories: coherence, consistency, fluency, and relevance by 3 expert annotators. We average the points in the consistency aspect as the final score. FRANK has a binary consistency score for each sentence in a summary labeled by annotators, then aggregates a summary-level score from 0-1, resulting in 2250 marked summaries in total.

\noindent\textbf{Baseline Models}
We compare other evaluation models that reported their performance on this dataset including the aforementioned FactCC, FEQA, DAE and QAGS~\citep{wang-etal-2020-asking}, which is a QA-based faithfulness evaluation method.

\noindent\textbf{Metrics} To evaluate to what extent the examined models
align with human judgment. Two widely-used
correlation measures are adopted: (1) Spearman
correlation~\citep{zar2005spearman} assesses the monotonic relationships between two variables; (2) Pearman
correlation~\citep{mukaka2012guide} measures the linear relationships between two sets of data; (3) Kendallâ€™s
Tau~\cite{kendall1938new}evaluates the ordinal association between two measured quantities.
\begin{table*}[!tb]
\centering
\begin{tabular}
{p{0.15\linewidth}m{0.1\linewidth}m{0.1\linewidth}m{0.08\linewidth}m{0.08\linewidth}m{0.1\linewidth}m{0.08\linewidth}}\\
\hline
 \multirow{2}{*}{\textbf{Methods}}&  
\multicolumn{5}{c}{\textbf{SUMMAC Benchmark Datasets}}\\
\cline{2-7}
 \textbf{}&  
\textbf{CoGenSum} &\textbf{XsumFaith}&  \textbf{Polytope}&  \textbf{FactCC}& \textbf{SummEval}&  \textbf{FRANK}\\
\hline
NER Overlap&  53.0 &63.3 &52.0 &55.0& 56.8& 60.9\\
MNLI-doc &57.6& 57.5 &61.0 &61.3 &66.6 &63.6\\
FactCC-CLS& 63.1& 57.6& 61.0 &75.9& 60.1 &59.4\\
DAE& 63.4 &50.8& 62.8& 75.9& 70.3 &61.7 \\
FEQA& 61.0 &56.0 &57.8 &53.6 &53.8 &69.9 \\
QuestEval& 62.6 &62.1 &\textbf{70.3}& 66.6& 72.5 &82.1\\
SummaC\textsubscript{ZS}&70.4 &58.4 &62.0& 83.8 &78.7& 79.0\\
SummaC\textsubscript{Conv} &64.7& \textbf{66.4} &62.7 &\textbf{89.5} &81.7 &81.6\\
ChatGPT\textsubscript{ZS} &63.3	&64.7	&56.9	&74.7&	76.5&	80.9\\
ChatGPT\textsubscript{ZS-COT}& \textbf{74.3}&	63.1	&61.4	&79.5	&\textbf{83.3}&	\textbf{82.6}\\
\hline
\end{tabular}
\caption{\label{tab: summac results}
Balanced accuracy results of inconsistency detect models on the test set of SummaC. Results of baselines are referenced from the paper~\cite{laban2022summac}.
}
\end{table*}
\section{Experiment}
We conduct our experiments using the API of ChatGPT (\textit{gpt-3.5-turbo-0301}) which is trained based on InstructGPT~\citep{Ouyang2022TrainingLM} with reinforce learning from human feedback (RLHF).
To avoid the effects of historical dialogues, we sent each request individually to obtain the response.
\subsection{Entailment Inference}
\label{sec entailment}
The full results of the entailment inference task are shown
in Table \ref{tab: summac results}. Overall, ChatGPT is able to achieve comparable performance or even better performance compared to the previous state-of-the-art evaluation models without training on relevant tasks, demonstrating the potential of ChatGPT-like LLMs on detecting inconsistency between two pieces of text in a zero-shot setting.
Specifically, ChatGPT with zero-shot CoT prompt produces the best results and outperforms the previous SOTA method SummaC\textsubscript{ZS} by 3.9\%, 1.6\% and 1.0\% on CoGenSum, SummEval, and FRANK datasets correspondingly.
It remains comparable to the best models on the rest three datasets including XsumFaith (63.1\% compared to SummaC\textsubscript{Conv} with 66.4\%), Polytope (61.4\% compared to QuestEval with 70.3\%), FactCC (79.5\% compared to SummaC\textsubscript{Conv} with 89.5\%). 
In almost all datasets, the ChatGPT\textsubscript{ZS-COT} which guides the ChatGPT with the chain-of-thought prompt has significantly better performance than ChatGPT\textsubscript{ZS}, In detail, ChatGPT\textsubscript{ZS-COT} outperforms ChatGPT\textsubscript{ZS} by 11.0\%, 4.5\%, 4.8\%, 6.8\% and 1.7\% on the CoGenSum, Polytope, FactCC, SummEval, and FRANK datasets correspondingly.
It shows great potential to better explore the factuality evaluation ability of ChatGPT by prompt engineering in the future.
\begin{figure}
    \includegraphics[width=0.5\textwidth]{materials/sensitivity_specificity.png}
    \caption{The results of sensitivity and specificity of ChatGPT\textsubscript{ZS-COT}.}
    \label{fig: SenVSSpec}
\end{figure}

To further investigate ChatGPT's performance in consistent and inconsistent instances, we break the balanced accuracy results of ChatGPT\textsubscript{ZS-COT} into sensitivity (positive recall) and specificity (negative recall), the comparison is in Fig \ref{fig: SenVSSpec}. In five out of the total six datasets, ChatGPT can successfully retrieve more than 95\% consistent summaries (high negative recall namely specificity), while performing rather poorly on identifying all the inconsistent ones (low positive recall namely sensitivity). Based on this observation, we assume that during inference, ChatGPT might still rely more on semantic similarity to make its decision on consistency detection since most of the candidate summaries are lexically close to sentences in the source articles, causing its vulnerability in finding these trivial modifications in inconsistent summaries that changes the meaning of the source document. This could be further demonstrated in ChatGPT's reverse performance on the two types of candidate summaries in the XSumFaith dataset which contains summaries generated by models trained on the XSum dataset in Table \ref{tab: summac results}. Previous works~\citep{durmus-etal-2020-feqa} have shown that the generated summaries are highly affected by training data and models trained on CNN/DM produce nearly extractive summaries while the same model trained on XSum will give significantly more abstractive ones. Abstrativeness brings the decline of lexical similarity between the candidate summary and the source document which might be the main reason why in XSumFaith, ChatGPT tends to predict more cases as inconsistent.
%Also, although the CoT style prompt on ChatGPT (better performance on 5 out of 6 datasets) has been verified in boosting ChatGPT's performance significantly, we further make an error analysis of ChatGPT\textsubscript{ZS-COT}, to discuss the possible reasons why the CoT prompt is ineffective in some cases and provide useful insights for prompt designing in the future work.
%However, in the following case study, we found the reason why the step-by-step guidance is effective might need further investigation.
\subsection{Summary Ranking}
\begin{table}
\centering
\begin{tabular}{p{0.5\linewidth}m{0.3\linewidth}}
\hline
\textbf{Model} &  
\textbf{Ranking Acc.}\\
\hline
FactCC &70.0\\
MNLI-doc &78.3\\
Rule-based dependency &74.8\\
DAE &83.6\\
Human &83.9\\
ChatGPT & \textbf{85.2}\\
\hline
\end{tabular}
\caption{\label{tab: summary ranking}
Performance of models on the
summary ranking task. Results of baselines are reported in~\citet{Goyal2020}.
}
\end{table}
The results of the summary ranking task are shown in Table \ref{tab: summary ranking}. It shows that ChatGPT without any in-context learning can outperform not only existing methods but also a human assessment reported in~\citet{Falke2019}. 
Notably, the ranking dataset is sampled from the output of models trained on CNN/DM. Therefore, the candidate summaries are mostly identical to some sentences in the source document, the inconsistent ones tend to contain minor adjustments corrupting the meaning like deleting modifiers like "half of" as shown in Figure \ref{fig: fail EL got SR}. Though we conclude from Section \ref{sec entailment} that ChatGPT relies heavily on lexical similarity to decide the consistency degree of sentences, in this summary ranking task, we see that ChatGPT can detect the trivial semantic differences even when given two highly similar candidates and pick out the consistent one.
For example, in the second case of Figure \ref{fig: fail EL got SR}, ChatGPT can correctly assess that sentence B is more consistent with the input article, given the highly lexical similarity between sentence B and sentence A.
In our manual inspection, we found that ChatGPT is able to point out the inconsistency in some cases where it failed in the entailment inference when ranking them compared to their consistent counterparts.
As shown in the first case of Figure \ref{fig: fail EL got SR}, ChatGPT failed in detecting the inconsistency of the summary with the input article in the entailment inference task.
While it can correctly pick out the more consistent one when given two summaries with highly lexical similarity in the summary ranking task, as shown in the second case of Figure \ref{fig: fail EL got SR}.
This indicates the importance of prompt engineering with useful contexts in better triggering ChatGPT's capability.

\begin{table*}
\centering
\small
\begin{tabular}{m{0.07\linewidth}m{0.07\linewidth}m{0.07\linewidth}|m{0.07\linewidth}m{0.07\linewidth}|m{0.07\linewidth}m{0.07\linewidth}|m{0.07\linewidth}m{0.07\linewidth}}\\
\hline
&\multicolumn{2}{c}{\textbf{FRANK}}&\multicolumn{2}{c}{\textbf{FRANK(CNN/DM)}}&\multicolumn{2}{c}{\textbf{FRANK(XSum)}}&\multicolumn{2}{c}{\textbf{SummEval}}\\\hline
\multirow{2}{*}{Metrics}& Pear. &Spear.& Pear. &Spear.& Pear. &Spear.&Pear. &Spear.\\
&$\rho$ &$r$&$\rho$ &$r$ &$\rho$ &$r$ &$\rho$ &$r$\\\hline
FEQA &0.00& 0.01&-0.01& -0.01 &0.02&0.07 &-&-\\
QAGS&0.06&0.08&0.13&0.09&-0.02&0.01&-&-\\
DAE&0.16&0.14&0.25&0.24&0.04&\textbf{0.28}&0.20&0.27\\
FactCC&0.20&0.30&0.36&0.33&0.07&0.25&0.32&0.34\\
ChatGPT&\textbf{0.70}&\textbf{0.69}&\textbf{0.50}&\textbf{0.46}&\textbf{0.34}&0.27&\textbf{0.49}&\textbf{0.35}\\\hline
    \end{tabular}
    \caption{
    \label{tab: correlation}
    Pearson correlation, and spearman rank correlation coefficients %and p-values 
    between human judgements and evaluation scores of different methods.}
\end{table*}

% \begin{table*}
% \centering
% \small
% \begin{tabular}{m{0.07\linewidth}m{0.05\linewidth}m{0.05\linewidth}m{0.05\linewidth}|m{0.05\linewidth}m{0.05\linewidth}m{0.05\linewidth}|m{0.05\linewidth}m{0.05\linewidth}m{0.05\linewidth}|m{0.05\linewidth}m{0.05\linewidth}m{0.05\linewidth}}\\
% \hline
% &\multicolumn{3}{c}{FRANK}&\multicolumn{3}{c}{FRANK(CNN/DM)}&\multicolumn{3}{c}{FRANK(XSum)}&\multicolumn{3}{c}{SummEval}\\\hline
% \multirow{2}{*}{Metrics}& Pear. &Spear.&Kend.& Pear. &Spear.&Kend.& Pear. &Spear.&Kend.& Pear. &Spear.&Kend. \\
% &$\rho$ &$r$ &$\tau$&$\rho$ &$r$ &$\tau$&$\rho$ &$r$ &$\tau$&$\rho$ &$r$ &$\tau$\\\hline
% FEQA &0.00& 0.01& - &-0.01& -0.01 &-&0.02&0.07 &-&- &-&-\\
% QAGS&0.06&0.08&-&0.13&0.09&-&-0.02&0.01&-&-&-&-\\
% DAE&0.16&0.14&-&0.25&0.24&-&0.04&0.28&-&0.20&0.27&0.21\\
% FactCC&0.20&0.30&-&0.36&0.33&-&0.07&0.25&-&0.32&0.34&0.27\\
% ChatGPT&\textbf{0.70}&\textbf{0.69}&\textbf{0.58}&\textbf{0.50}&\textbf{0.46}&\textbf{0.38}&\textbf{0.34}&\t0.27&\textbf{0.24}&0.49&0.35&0.30\\\hline
%     \end{tabular}
%     \caption{
%     \label{tab: correlation}
%     Pearson correlation, spearman rank correlation coefficients and p-values between human judgements and evaluation scores of different methods.}
% \end{table*}
\begin{figure}[htb!]
    \includegraphics[scale=0.38]{materials/casestudy1.png}
    \caption{ChatGPT's actions when given the same source document and an inconsistent summary but with and without a consistent one. The red underlined text in the article is content highly related to the candidate summaries.}
    \label{fig: fail EL got SR}
\end{figure}

\subsection{Consistency Rating}
We further show the performance of all methods on the consistency rating task in Table \ref{tab: correlation}, where we compare the correlations of their rating results with human judgement. Still, without in-context training, ChatGPT outperforms other consistency metrics by aligning closer to human assessments. Especially in the whole FRANK dataset, ChatGPT leads other metrics by a large margin, emphasising its superior ability in measuring the consistency degree than the baseline models.

In particular, when splitting the FRANK dataset into summaries from CNN/DM and XSum, the correlations of ChatGPT show a considerable decline from CNN/DM to XSum, which matches our analysis in the previous two parts. The difference might come from the abstractiveness of summaries generated from models trained on XSum, so their lower lexical similarity with the source document affects the model's judgement of consistency, leading to the worse performance in the FRANK XSum dataset. However, though the abstractiveness of XSum summaries lowers the correlations generally, ChatGPT's pearson's correlation is still much higher than the single-digit results of the baselines, suggesting its better language understanding and inference ability.
\begin{figure}
    \centering
    \includegraphics[scale=0.36]{materials/casestudy2.png}
    \caption{An example of ChatGPT fail to stick to the given definition of consistency.}
    \label{fig:rating failure}
\end{figure}
\subsection{Error Analysis}
In this part, we show some example cases of ChatGPT in the three tasks to showcase its limitations and attempt to provide a hint to understand ChatGPT's behavior in the aforementioned tasks. 

In Figure \ref{fig: fail EL got SR}, we show an example from the CoGenSumm dataset where ChatGPT failed in the entailment inference task. The model neglects the disappearance of "half of " in the candidate summary which significantly changes the meaning and decides the summary is consistent with the article.
However, when putting the same summary and article into the summary ranking task combined with a consistent claim, ChatGPT successfully picks the consistent one and gives the right reasoning of why "Summary A" is inconsistent.
The first case of Figure \ref{fig: fail EL got SR} supports our assumption of ChatGPT counting on lexical similarity to determine consistency as the high lexical overlap between inconsistent summary and the red-underlined part in the article cheats ChatGPT. Nevertheless, when another summary is both lexically and semantically closer to the article, ChatGPT detects the difference and manages to answer correctly in the second case of Figure \ref{fig: fail EL got SR}.

With further investigation of failure cases, we found ChatGPT makes false inferences as shown in Figure \ref{fig: wrong reasoning}. The summary claims that "prime minister matteo renzi" won the vote while the red underlined part in the article clearly says the bill has passed the lower house but is held up to be approved by both houses. However, ChatGPT determines this summary is consistent and tries to justify it by using "the bill is approved by the lower house" as evidence. This example, combined with the upper case in the first example, demonstrates that ChatGPT still has a limitation on understanding and inferencing of natural language. Furthermore, a CoT-style prompt is applied in this example to encourage the model to generate a reasoning process to assist its judgment. But ChatGPT directly produces the conclusion first and then unfolds its inference progress afterwards. According to the autoregressive training nature of GPT, the explanation is then conditioned on the "consistent" conclusion and thus cannot guide the decision while following the judgment. In our manual inspection, answers with the conclusion at first are not rare, suggesting zero-shot CoT-style prompts might not be the optimal instruction for ChatGPT to conduct a language inference task with reasoning progress. We suppose fined-engineered few-shot prompts might help to guide ChatGPT's generation and further improve its performance and will investigate it in the future. 

Moreover, there are examples that ChatGPT demonstrates limited comprehension of given prompts. Fig \ref{fig:rating failure} shows a case of the SummEval dataset in the consistency rating task. Though the summary is short, the fact within it is consistent with the article which ChatGPT also admits in the answer. Therefore, all three experts mark 5 out of 5 for the summary. However, ChatGPT then only rates the summary 1 point as it does not cover other facts in the article which is not in the given marking rubric, showing an inadequate understanding of giving prompts. This example demonstrates the insufficient alignment brought by our tested prompt. Prompt engineering including human-in-the-loop alignment optimization and few-shot in-context learning might be helpful to better calibrate ChatGPT's output.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{materials/casestudy3.png}
    \caption{An example of ChatGPT conducts false reasoning.}
    \label{fig: wrong reasoning}
\end{figure}

\section{Conclusion}
In this paper, we comprehensively investigate the factual inconsistency evaluation ability of ChatGPT in the zero-shot setting with three coarse-grained and fine-grained factual inconsistency detection tasks.
Our experimental results empirically show the great potential of ChatGPT as a good factual inconsistency evaluator, where it outperforms SOTA evaluation metrics on six out of nine datasets.
Although its great potential, ChatGPT is also found to have limitations on evaluation bias, false reasoning, and hallucination, which should be further addressed for its reliable use.
The experiments also show that ChatGPT's performance can be significantly boosted by the chain-of-thought prompt.
Lastly, We analyzed the limitation of the chain-of-thought prompt, which highlights the importance of alignment research in future work.
The study in our paper is just the initial step in exploring the factual inconsistency evaluation ability of ChatGPT, which we hope can provide useful insights for future work in this direction.

\section*{Limitations}
Our study has the following limitations: 1) Due to the cost limitation of using the API of ChatGPT, we only investigated the effectiveness of using zero-shot prompts on three tasks. More effective prompts such as the few-shot prompts can be explored in future work; 2) We only evaluated the performance of ChatGPT on the factual inconsistency evaluation. A thorough comparison of different large language models (LLMs) such as GPT-3.5 and GPT-4 can be studied in future work, to help us figure out the superiors and limitations of different LLMs.
%\section*{Ethics Statement}
%Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

%\section*{Acknowledgements}
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

%\appendix

%\section{Example Appendix}
%\label{sec:appendix}


\end{document}
