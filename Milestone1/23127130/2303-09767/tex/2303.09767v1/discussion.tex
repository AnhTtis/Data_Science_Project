\section{Implications and Future Work}
\label{sec:discussion}

\vspace{0.05in}
\noindent
{\bf Empirical Evaluation.}
All but the four papers outlined in Section~\ref{sec:results-domain-specific} study domain-agnostic data properties.
Yet, the majority of the papers we surveyed conduct experimental evaluations on image datasets only.
Applicability of the findings and of the proposed approaches to other domains, with different forms of data,
may need further investigation.
%e.g., binary data for malware detection and sequential data for speech recognition,
For example, for datasets with binary features, which are commonly used in malware detection, one cannot arbitrarily
change feature values to reduce the distance between samples.
This further implies that common distance metrics used to model adversaries in the image domain, such as $L_2$ and $L_\infty$, fail to accurately capture the adversarial threat level in such domains.
Hence, future work applying, adapting, and evaluating the proposed metrics and techniques in other domains and data types is needed.


\vspace{0.05in}
\noindent
{\bf Interdependence of Properties.}
Only a few works in our collected literature consider multiple data properties simultaneously or establish interdependence of data properties.
For example, Wang et al.~\cite{Wang:Jha:Chaudhuri:ICML:2018} and
Rajput et al.~\cite{Rajput:Feng:Charles:Loh:Papailiopoulos:ICML:2019}
find that the number of samples and dimensionality collectively influence the performance of the resulting model.
Sanyal et al.~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021} study the tolerable amount of label noise as a function of the dataset density.
Such works are very valuable as adversarial robustness is indeed a result of compounding properties.
Yet, optimizing for multiple properties simultaneously is not always possible.
A productive direction of future work could be to investigate correlations between different data properties, e.g.,
the effects of feature dimensionality reduction approaches on class density and separation.

%Similarly, other directions of future work include: investigating what kind distributions are less vulnerable as a result of high dimensionality, and how the amount of separation reduces vulnerability of datasets with low density regions?
%\jr{unclear why these topics and why this makes any sense.}

\vspace{0.05in}
\noindent
{\bf Additional Data Properties.}
Existing research on the effects of data on
\emph{standard generalization}~\cite{Lopez:Fernandez:Garcia:Palade:Herrera:InfSci:2013,
Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020,
Santos:Henriques:Pedro:Japkowicz:Fernandez:Soares:Wilk:Santos:AIR:2022} identified several data properties
not discussed in the papers related to robust generalization that we reviewed.
These include the presence of (i) outliers, i.e., samples that drastically differ from most observed samples in a dataset,
(ii) overlapping samples, i.e., different samples of the dataset having the same feature representation, and
%(iii) class imbalance,  i.e., incomparable number of samples for different classes in a dataset, and
(iii) small disjuncts, i.e., training samples from the same class forming small disjoint clusters dispersed throughout the input space
%instead of forming one large connected cluster
(more details are in Section~\ref{sec:relatedwork}).
Investigating the effect of such data properties on the model's adversarial robustness could be yet another direction
for possible future work.

\vspace{0.05in}
\noindent
{\bf Generalization.}
Some works have shown that, under specific assumptions about the data and model, there is an intrinsic
accuracy-robustness trade-off~\cite{Tsipras:Santurkar:Engstrom:Turner:Madry:ICLR:2019, Javanmard:Soltanolkotabi:Hassani:COLT:2020, Mehrabi:Javanmard:Rossi:Rao:Mai:ICML:2021}.
This implies that achieving robust generalization may come at cost of standard generalization.
However, other works have shown that the effect of several of the data properties on standard generalization overlaps with their effect on robust generalization.
For example, %techniques to improve robust generalization discussed in Section~\ref{sec:results}, such as
%increasing the number of samples,
increasing class density and removing label noise also increases standard generalization~\cite{Raudys:Jain:TPAMI:1991, He:Garcia:TKDE:2009, Lopez:Fernandez:Garcia:Palade:Herrera:InfSci:2013, Frenay:Verleysen:TNNLS:2014}.
We believe that more work is needed to map data-related reasons that contribute to the accuracy-robustness trade-off.

%Fact 1: they are at oods
%Fact 2: Some techniques overlap
%Q1: Are they really at odds
%Q2: Which data properties are common
%Q3: which data properties/techniques are at odds


\vspace{0.05in}
\noindent
{\bf Simplified Problem Setup.}
Several studies use a simplified problem setup, e.g., pure Gaussian data distribution, to provide formal proofs related to the studied phenomenon.
While such work helps advance knowledge and our understanding of the effects of data on adversarial robustness,
additional work that investigates the generalizability of the findings on realistic datasets used in practice is needed.
For example, assuming uniform data properties, e.g., same distribution, density, and level of label noise,
for all classes on the training data greatly simplifies the proofs, but is not common in reality.
Likewise, considering only binary classification simplifies calculations of data separation, which can be calculated by measuring the distance between the two classes. Yet, in a multi-class setting, one needs to consider the proximity of data points from multiple classes.

Furthermore, most papers only consider a white-box attack setting, which might not be realistic in many practical scenarios.
Even though a white-box setting makes it possible to model the worst-case adversary and to provide better robustness guarantees, it may result in overly pessimistic findings, i.e., some data transformations may be robust against black-box attacks while still be vulnerable to white-box attacks.
Thus, future works might look into the impact of data properties on the different types of attack scenarios.


\vspace{0.05in}
\noindent
{\bf Quantitative Measure.}
Literature shows that the lower/upper bound of adversarial robustness can be determined by the properties of the underlying data~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019,Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019}.
Modifying certain properties of the data can also change the robustness of the resulting classifier.
Hence, the ability to quantitatively measure such data properties is very valuable.
However, some data properties discussed in this survey, such as, type of distributions and label noise, lack any reliable estimation techniques.
Current work mostly relies on informal comparative analysis, e.g., that the \mnist dataset is closer to a Bernoulli mixture data than a Gaussian mixture because the pixels are concentrated towards black or white.
Quantitatively measuring the degree of similarity between distributions, although difficult, may be necessary in order to make more accurate conclusions.

Interestingly, other data properties have multiple, often inconsistent, measurement techniques, e.g., concentration~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019,Prescott:Zhang:Evans:ICLR:2021,Zhang:Evans:ICLR:2022}, density~\cite{Song:Kim:Nowozin:Ermon:Kushman:ICLR:2017,Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020},
intrinsic dimensionality~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021,Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020}, and inter-class distances~\cite{Ding:Lui:Jin:Wang:Huang:ICLR:2019, Bhagoji:Cullina:Mittal:NeurIPS:2019, Pydi:Jog:ICML:2020}.
%For example, the intrinsic dimensionality of an dataset can be estimated by the average value of the local intrinsic dimensionality (LID)~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021} of individual samples in the training data, which infers the intrinsic dimensionality by the closeness of samples in the local neighborhood. \jr{What?}
%Alternatively, it can also be estimated by calculating the number of PCA components needed to represent 95\% of the variation present in the dataset~\cite{Lorena:Garcia:Lehmann:Souto:Ho:ACMComputSurv:2020}.
%The former determines the intrinsic dimensionality through the local structures, while the later derive it by considering the global distribution.  \jr{jargon. Unclear. Can you give simpler example? }
%They may provide conflicting results as they focus on different aspects of the data.
For example, the inter-class distances can be calculated as the total distance required to move the samples from one class to
another~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019, Pydi:Jog:ICML:2020}.
It can also be calculated as the pairwise distances between a pre-defined portion of samples from different classes,
e.g., 10\% from each class, that are the closest to each other~\cite{Ding:Lui:Jin:Wang:Huang:ICLR:2019}.
While the inter-class distance derived through the first approach is more computationally expensive, the second approach
is more susceptible to outliers as it relies only on a subset of samples close to each other.
Moreover, these metrics might not necessarily correlate with each other.
We believe future research can provide more insights about appropriate application scenarios for each of the proposed metrics.



%\vspace{0.05in}
%\noindent
%{\bf Data Augmentation.}
%Among the works we excluded during our filtering process, we encountered a number of data augmentation techniques that do not explicitly target any specific data property,
%such as Cutout~\cite{Devries:Taylor:ArXiv:2017} which augments by blacking out random patches in images,
%MixUp~\cite{Zhang:Ciss:Dauphin:Lopez:ICLR:2018} which augments by mixing images from two classes, and
%CutMix~\cite{Yun:Han:Chun:Oh:Yoo:Choe:ICCV:2019} which augments by randomly replacing patches from one image with those from another.
%Experiments show they are efficient in improving model's robustness, however, few research investigates on how they improve model's robustness through the lens of underlying data properties.
%The lack of understanding on the low-level impact of data augmentation may pose a challenge on selecting the most appropriate techniques, as the performance differences between techniques may be attributed to experimental biases or limitations.
%For example, Mintun et al.~\cite{Mintun:Kirillov:Xie:NeurIPS:2021} show that the similarity between the type of augmentation used in the training data and adversarial corruption encountered in deployment affects the adversarial performance of the model.
%In addition, Taori et al.~\cite{Taori:Dave:Shankar:Carlini:Recht:Schmidt:NeurIPS:2020} show that existing augmentation strategies for synthetic adversaries are not ideal in the face of natural distribution shifts.
%However, without an understanding of which and how data properties change when data augmentation approaches are applied,
%it is difficult to interpret the robustness gain from such approaches and determine under which scenarios the performance can be transferred.
%We hope future research can systematically investigate the impact of popular data augmentation methods on specific data properties, as well as explaining the significance of data augmentation methods on creating robust models.
%%\mt{Distinguish why we don't include data augmentation papers. And possibly explain what they are doing in terms of data property.}
%\jr{we should not discuss anything not derived from our results.}


\vspace{0.05in}
\noindent
{\bf Sources of Adversarial Vulnerability.}
Even when the training data is optimal for robustness, a sub-optimal training method can lead to adversarial vulnerability~\cite{Richardson:Weiss:JMLR:2021}.
For example, adversarial vulnerability may arise when the complexity of the classifier does not match the complexity of the data, e.g., CNNs may achieve lower robustness due to their complexity than simpler models, such as Kernel-SVMs,
on symmetrical data with well-separated means and similar variances~\cite{Richardson:Weiss:JMLR:2021}.
To alleviate such problems, a few papers propose to select, improve, or optimize classifiers based
on the dimensionality of data~\cite{Wang:Jha:Chaudhuri:ICML:2018,
Yin:Kannan:Bartlett:ICML:2019,
Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}.
%Namely, for input data with excessive dimensions (i.e., the extrinsic dimensionality is much larger than the intrinsic dimensionality), BNNs is better that other neural networks, as they were shown to be more robust against attacks that exploit degeneracy in the data distribution~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}.
%For input data in high dimension, regularize model weights by $L_1$ instead of other $L_p$ constraints can avoid the influence of dimensionality on the robustness performance gap between training and testing data~\cite{Yin:Kannan:Bartlett:ICML:2019}.
%When using $k$-NN classifiers, one may opt for configuring the optimal $k$ based on dimensionality and the available number of samples~\cite{Wang:Jha:Chaudhuri:ICML:2018}.
Similar work that looks at other properties of data, such as separation and density, could be of value.
Future works could also explore strategies for determining whether the input data (vs. the model itself) is the dominant cause \mbox{of adversarial vulnerability}.


%---------------

%\mt{Only the concentration section has different measurement techniques, and they build on each other. So maybe there is nothing to say.}
%\vspace{0.05in}
%\noindent
%{\bf Measuring Data Properties.}
%Most papers studying concentration and density of datasets use their own metrics to measure these quantities.
%\mt{Mention some example metrics here}
%We encourage future works to use standardized metrics and present their findings in comparison to previous work.
%In addition, some works use ``vague'' definitions, like ``data hardness'' or ``data diversity''.
%More formal metrics for such properties are needed.


%(based on the exclusion of ``Augmax: Adversarial Composition of Random Augmentations for Robust Training~\cite{Wang:Xiao:Kossaifi:Yu:Anandkumar:Wang:NIPS:2021}'', as they claim their approach increase the `hardness' of training data by adding adversarial examples, and `diversity' by using more corruption techniques during data augmentation)

%\mt{Talk to Gabby}
%There are a lot of defense techniques based on \textbf{adversarial training}: some work proposed to solve the optimization differently (``Fuzz Testing Based Data Augmentation To Improve Robustness of Deep Neural Networks~\cite{Gao:Saha:Prasad:Roychoudbury:ICSE:2020}''), some proposed to take class-wise parities into consideration and relax the penalty on pair of classes that share higher similarity (``Analysis and Applications of Class-wise Robustness in Adversarial Training~\cite{Tian:Kuang:Jiang:Wu:Wang:KDD:2021}'').

%\mt {To discuss}
%Discuss about Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NIPS:2018} sample complexity lower bounds and Cullina et al.~\cite{Cullina:Bhagoji:Mittal:NIPS:2018} sample complexity upper bounds in a dataset agnostic setting.

%\textbf{Categorization discussion points:}
%
%\begin{itemize}
%	\item Definition of Robustness: (Discussion)
%		\begin{itemize}
%			\item Type of papers that use radius-based robustness evaluation metrics are .... (deal with certification, randomized smoothing?
%		\end{itemize}
%	\item Data properties that explain the origins of adversarial examples.
%		\begin{itemize}
%			\item \gx{We can add the papers the mentioned they intend to explain the origins of adversarial examples / vulnerabilities} \mt{I think that is almost all of the papers, besides the ones that propose techniques.}
%			\item High dimensionality from Gilmer et al.~\cite{Gilmer:Metz:Faghri:Schoenholz:Raghu:Wattenberg:Goodfellow:ICLR:2018}
%			\item High concentration from Dohmatob~\cite{Dohmatob:ICML:2019} and Mahloujifar et al.~\cite{Mahloujifa:Diochnos:Mahmoody:AAAI:2019}
%			\item Label noise / non-optimal representation~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021}
%			\item (Domain specific) CNN learn from different aspects of images than human~\cite{Wang:Wu:Huang:Xing:CVPR:2020,Chen:Peng:Ma:Li:Du:Tian:ICCV:2021}
%			\item How do standard and robust generalizations differ with respect to data properties. So far the related work seems to mention similar things for the case of standard generalization.
%			E.g., label noise on standard generalization.
%			\item \mt{Moved from related work}
%			These works review the literature on the impact of learning from imbalance data on standard generalization, and identify other compounding data factors (e.g., class separation, data density,  noisy data) that complicate the situation.
%			In our work, we find similar data properties associated to model's adversarial robustness, however, only a small amount of work study the compounding influence from multiple factors, and we believe more research are needed in those areas.
%			\item Upsampled \mnist by Shafahi et al.~\cite{Shafahi:Huang:Studer:Feizi:Goldstein:ICLR:2019} is still robust when it has severe degeneracy.
%
%		\end{itemize}
%	\item Approaches to improve adversarial robustness
%	\begin{itemize}
%		\item Label refinement and number of classes: Sanyal et al. hypothesized having refined labels can help improve representation learnt by the classifier and hence improve the resulting robustness, while Fawzi et al. show the vulnerability is negatively impacted by the number of classes.  ==> The trick maybe learning with refine labels and predict with coarse labels
%		\item Some work suggest abstraining prediction on uncertain inputs helps improve robustness.  ==> Shafahi et al, Zhang and Evans mentioned
%	\end{itemize}
%	\item More thorough discussion on Label Noise?
%	Currently, only one work study label noise,
%	while (1) label noise is a true threat in real-world applications, as the scale of required input data grow, use labeling from third party or crowd sourcing become more common in industry.
%	(2) uneven label noise is not studied, in real-life some class may be more prone to mislabel than others.
%	\item Relationship between concentration and high dimensionality: high dimensionality results in high concentration (part of inter-dependence of data properties)
%	
%\end{itemize}

%
%\section{Questions our survey answers}
%\begin{enumerate}
%    \item What properties of data influence the resulting modelâ€™s robustness?
%	\item How to select and represent data to improve robustness?
%	\item How to select and configure a classifier to be robust based on the properties data?
%	\item What are the best and worst case robustness bounds for classifiers on data with certain properties? <--- This is a sub question for question (2)
%\end{enumerate}
