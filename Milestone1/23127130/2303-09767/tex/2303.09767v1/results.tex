\section{Results}
\label{sec:results}

We present the results of our analysis by organizing the papers according to the robustness-related data property they discuss:  % (Sections~\ref{sec:results-number-of-samples}-~\ref{sec:results-domain-specific}).
number of samples,
dimensionality,
type of distribution,
density,
concentration,
separation,
label quality, and
domain specific properties.
Papers that discuss more than one data property are presented in all corresponding sections.
That is, in what follows, a paper can be discussed in more than one section.
Section~\ref{sec:results-summary} summarizes our findings.  
%Furthermore, we define each non-trivial data property under its corresponding section before presenting our findings.

To ease navigation, for each discussed data property,
we also include a map showing how the relevant papers relate to each other via their citation information.
We further annotate each paper with its \emph{applicability} and \emph{explainability} categories.
Specifically,
%we denote with the \roundrect{C} symbol papers that present a correlation between a data property and adversarial robustness;
we annotate with an \roundrect{A} symbol papers that propose an actionable technique to modify or measure a robustness-related property;
we annotate with \roundrect{E} papers that put extra emphasis on explaining the correlation between a data property and robustness rather than establishing such a correlation.
%\ad{In each section, we also categorized papers by their topics, and annotate with \roundrect{\#} in the figures to indicate the grouping.}

\input{results_samples}
\input{results_dimensionality}
\input{results_distribution}
\input{results_density}
\input{results_separation}
\input{results_concentration}
\input{results_labels}
\input{results_domain}


\subsection{Summary of Results}
\label{sec:results-summary}

Overall, the surveyed papers are mostly in agreement on how each of the identified data property influences adversarial robustness. The main findings are given below.

\vspace{0.05in}
\noindent {\bf Number of samples.} More training samples are needed for robust than for standard generalization.
For a variety of training setups (i.e., different types of classifiers and data distributions), the number of training samples required to achieve robust generalization is proportional to the dimensionality of the training data.
Unlabeled samples or generated data can be used to fulfill the need for more samples needed for robust generalization, i.e., to close the sample complexity gap.
Class imbalance, i.e., having an imbalanced number of samples across different classes, hurts robust generalization due to the model bias towards over-represented samples.

\vspace{0.05in}
\noindent
{\bf Dimensionality.} Dimensionality captures the size of the feature set.
Higher dimensionality correlates with
higher adversarial risk, worse standard-to-adversarial risk trade-off, difficulty in robustness certification, and difficulty in applying common defense techniques.
This is because adversarial attacks can exploit the excessive dimensions to construct adversarial examples.

\vspace{0.05in}
\noindent
{\bf Distribution.} Some data distributions are more robust than others, e.g., mixtures of Bernoulli distributions are more robust than mixtures of Gaussian distributions.
%Small variance in data distributions within a class helps robust generalization .
%distributions observed in common image datasets, such as \mnist and \cifarten.
Learning feature representations that resemble robust distributions can improve robustness.

\vspace{0.05in}
\noindent
{\bf Density.} Density reflects the closeness of samples in a particular bounded region (inter-class distance).
Adversarial examples are commonly found in low-density regions of data, where samples are far apart from each other.
This is because models cannot accurately learn decision boundaries near low-density regions due to the small number of samples available. As such, high data density for each class correlates with lower adversarial risk.

\vspace{0.05in}
\noindent
{\bf Separation.} Separation characterizes %the closeness of samples from the same class (density!) and
the distance of samples from different classes to each other (inter-class distance).
Greater separation between classes decreases adversarial risk as it is harder to generate perturbation that will
cross the boundaries between classes.
Most papers that provide techniques for improving separations, e.g., by feature selection, also ensure that it does not come
at the expense of decreasing density, as these two concepts are closely related.

\vspace{0.05in}
\noindent
{\bf Concentration.} Given a function defined over a non-empty set,
concentration (from the phenomenon of concentration of measure~\cite{Talagrand:1996:AnnalsProbability})
is the minimum value of the function after expanding the input set by $\epsilon$ in all dimensions.
%It is used to measure the minimum achievable error considering all epsilon perturbations of sample set.
For example, expanding the set of misclassified samples by a certain $\epsilon$ gives a set of possible samples that can be
misclassified with an $\epsilon$-size perturbation (candidate adversarial examples).
Concentration, in this case, measures the minimal possible size of this set, which provides the upper bound of the achievable model robustness.
As some datasets tend to exhibit inherently high concentration,
e.g., datasets that lie on unit hypersphere~\cite{Mahloujifar:Diochnos:Mahmoody:AAAI:2019},
achieving high robust generalization is harder for these datasets.
The impact of high concentration on adversarial robustness is further magnified for high-dimensional data.



\vspace{0.05in}
\noindent
{\bf Label quality.} High label noise correlates with higher adversarial risk.
More specific labels, e.g., ``cat'' and ``dog'' instead of ``animal'', are more robust than coarse labels, as they allow the model to extract more distinct features.
Learning for different tasks concurrently, e.g., to simultaneously locate and estimate the distance of objects in images,
improves robustness of the learned models, as the model can utilize the information from multiple sources of data.

\vspace{0.05in}
\noindent
{\bf Domain specific.} Image frequency, i.e., the rate of change in pixel value is shown to be correlated with robustness.
Specifically, a diverse distribution of frequencies in training data results in lower adversarial risk.
Most image datasets have a low image frequency, which results in learned models having smaller margins and, thus,
a lower distance of samples to the decision boundary for features corresponding to the low-frequency image components.
This increases the risk for adversarial perturbation that utilize these features.


%We present this information in Fig. \ref{fig:summaryofresults}.
%On the other hand, papers that study dimensionality all note that high dimensionality leads to adversarial vulnerability.
%Some papers that study intrinsic dimensionality and degeneracy also argue that dimensionality is a source of adversarial vulnerability when the dataset is not only high dimensional but can also be represented using a smaller dimension.
%In addition, some works suggest that some distributions such as the Gaussian distribution may be more robust than others.
%Hence, one can increase robustness by using data transformation techniques that make datasets resemble specific type of distributions such as the Gaussian.
%
%%%% These may need to be separated
%Papers that deal with distance related properties note that a larger separation of the datasets, that corresponds to a larger inter-class distance, reduces adversarial vulnerability.
%Similarly, a smaller intra-class distance, that corresponds to a larger class density, also reduces adversarial vulnerability.
%High concentration of samples near the boundary region of the classes increases adversarial vulnerability.
%However, the problem may not be resolved by merely removing such samples from the dataset as these samples are essential to learning an accurate decision boundary by characterizing the uncertain region between classes.
%Adding regularization terms in loss functions and changing the feature representation are among the approaches proposed to promote high inter-class distance and low intra-class distance in feature representations.
%Other reasons for adversarial vulnerability include mislabeled samples and coarse labels.




