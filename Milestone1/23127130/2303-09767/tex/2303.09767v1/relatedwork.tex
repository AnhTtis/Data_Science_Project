\section{Related work}
\label{sec:relatedwork}
We divide related work into three categories:
(1) surveys on adversarial robustness and its relation to data properties,
(2) surveys that discuss the influence of data properties on standard generalization, and
(3) individual papers that study non-data-related reasons for adversarial vulnerability.

\subsection{Surveys on Adversarial Robustness}
Numerous existing surveys focus on attack and defense techniques for adversarial robustness~\cite{Biggio:Roli:PR:2018,
Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,
Li:Li:Ye:Xu:CSUR:2021,
Maiorca:Biggio:Giorgio:CSUR:2019,
Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021,
Liu:Tantithamthavorn:Li:Liu:CSUR:2022,
Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,
Akhtar:Mian:IEEEAccess:2018,
Akhtar:Mian:Kardan:Shah:IEEEAccess:2021,
Serban:Poll:Visser:CSUR:2020,
Machado:Silva:Goldschmidt:CSUR:2021,
Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}.
Only a few of these works mention the relationship between adversarial robustness and properties of the underlying data.
Specifically, Serban et al.~\cite{Serban:Poll:Visser:CSUR:2020} observe that adversarial vulnerability can be caused by an insufficient training sample size %~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}
and high data dimensionality. %~\cite{Gilmer:Metz:Faghri:Schoenholz:Raghu:Wattenberg:Goodfellow:ICLR:2018}.
Similarly, Machado et al.~\cite{Machado:Silva:Goldschmidt:CSUR:2021} mention that the lack of sufficient training data, high-dimensional data, and high concentration contribute to adversarial vulnerability.
Yet, none of these surveys explicitly collect and analyze work that focuses on the effects of data properties
on adversarial robustness.
By explicitly targeting this topic in our survey, we are able to discuss these findings in detail and also identify
additional relevant data properties not mentioned in previous surveys, such as, types of distribution, class density, separation, and label quality. %, and image frequency for the image domain.

\subsection{Influence of Data Properties on Standard Generalization}
A number of surveys investigate the influence of data properties on standard
rather than robust generalization.
One of the earliest is probably the work of Raudys and Jain~\cite{Raudys:Jain:TPAMI:1991},
who review studies related to the influence of sample size on binary classifiers, showing that
a limited sample size usually leads to sub-optimal generalization.
%With the development of deep learning and the ever-increasing need for larger training datasets,
%a variety of data augmentation techniques have been proposed.
Bansal et al.~\cite{Bansal:Sharma:Kathuria:CSUR:2021} and
Bayer et al.~\cite{Bayer:Kaufhold:Reuter:CSUR:2022} also survey papers addressing the data scarcity problem,
focusing in particular on the recent advancements in data augmentation techniques in the fields of computer vision, security, and text classification.
Their results show that augmentation techniques %exist for various application domain and
can help improve a model's generalization by reducing the problem of model overfitting.
%They evaluate the effectiveness of such techniques in improving the accuracy of machine learning models.

%Limited sample size is also one of the culprit behind poor robust generalization~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}, we collected a number of researches characterize the sample complexity for robust generalization or propose data augmentation techniques to fill in the sample complexity gap.

Label noise is another aspect of data that influences both standard and robust generalization.
Most works on this topic find that the presence of noisy labels increases the need for a greater number of training samples and may result in unnecessarily complex decision boundaries~\cite{Frenay:Verleysen:TNNLS:2014,Song:Kim:Park:Shin:Lee:TNNLS:2022}.
For example, Fr\'{e}nay and Verleysen~\cite{Frenay:Verleysen:TNNLS:2014} show
that overfitting to label noise greatly degrades a model's standard generalization;
the same effect has been observed for the case of robust generalization~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021}.
Song et al.~\cite{Song:Kim:Park:Shin:Lee:TNNLS:2022} survey the impact of label noise in deep learning, arguing
that the presence of noisy labels is a more serious concern for deep models as they contain a larger number of parameters which makes them prone to overfitting to the noise in training data.
%They also point out the connection between adversarial poisoning attacks and noisy labels as
%the countermeasures for both share the goal of learning noise-resilient representations.
They mention that adversarial defense techniques, e.g., adversarial training, are effective against label noise~\cite{Zhu:Zhang:Han:Liu:Niu:Yang:Kankanhalli:Sugiyama:ArXiv:2021, Fatras:Damodaran:Lobry:Flamary:Tuia:Courty:TPAMI:2022}
but do not discuss how label noise influences a deep learning model's robustness under attacks.


Lorena et al.~\cite{Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020} identify a collection of 26 quantitative metrics that measure data complexity with respect to
(1) ambiguity of classes, i.e., whether the classes can be clearly distinguished with the given features,
(2) sparsity and dimensionality of data, i.e., whether enough information are provided to learn confident decision boundaries, and
(3) complexity of boundary separating the classes, i.e., whether more intricate functions are required to describe the decision boundaries.
The authors also discuss how these metrics help estimate the difficulty of performing classification on a given dataset.
Similar to our survey, the authors show that high dimensionality and small separation between classes hinder standard generalization.
However, the relationship of some of the metrics reviewed by these authors, e.g.,
%faction of borderline points (i.e., a measure for the complexity of the required decision boundary) and
%the fraction of hyperspheres covering data (i.e.,
the number of non-intersecting spheres needed to enclose all data points of a class,
to robust generalization is not studied, according to our survey.
%Moreover, the effect of XXX on standard generalization needs future investigation as well (that is if we found something they do not have).

%Knowing the characteristics of a dataset according to these perspectives can assist researchers and practitioners to select optimal learning algorithms~\cite{Ho:Basu:TPAMI:2002}.



He and Garcia~\cite{He:Garcia:TKDE:2009} focus on the imbalance learning problem~--
the disproportion in the number of samples belonging to each class in a given dataset.
The authors found that most standard algorithms are designed with the assumption of a balanced class distribution.
These algorithms fail to reliably represent the distributive characteristics of the imbalanced size of samples and result in unfavorable performance across classes.
Furthermore, L\'{o}pez et al.~\cite{Lopez:Fernandez:Garcia:Palade:Herrera:InfSci:2013} present a thorough discussion on six intrinsic data characteristics that potentially complicate learning from imbalanced data:
low density, sample overlap between classes, noisy data, borderline instances,
dataset shift between training and testing distributions, and
small disjuncts, i.e., disperse small clusters of samples from a single class.
Their analysis concludes that while all these ``unfavorable'' data characteristics further complicate the data imbalance
issues, data overlap between classes is probably one of the most harmful.
To follow up on this point, Santos et al.~\cite{Santos:Henriques:Pedro:Japkowicz:Fernandez:Soares:Wilk:Santos:AIR:2022}
focus on the joint effect of data imbalance and class overlap on model generalization.
The negative impact of data imbalance, low separation, and noisy data on robust generalization was also discussed in our survey.
Yet, the compounding effect of these factors, as well as the effect of other properties,
on robust generalization needs future investigation.

Recently, Yang et al.~\cite{Yang:Jiang:Song:Guo:IJCV:2022} summarized relevant studies focusing on
long-tailed distributions in the field of Computer Vision.
% and categorize the main methods for alleviating the issues caused by long-tailed distribution.
%They present quantitative metrics for measuring data imbalance and .
This survey also includes work on the influence of long-tail distributions on a model's adversarial robustness~\cite{Wu:Liu:Huang:Wang:Lin:CVPR:2021}, which is covered in our survey.
%which is included in our survey,
The authors advocate for more research on adapting long-tailed-based approaches for standard generalization to improve robust generalization.

Finally, Moreno-Torres et al.~\cite{MorenoTorres:Raeder:Rodrigues:Chawla:Herrera:PR:2012} present a unifying framework to categorize existing definitions of dataset shift~-- the case where the joint distribution of inputs and outputs differs between training and testing data.
While ML models are normally trained under the premise that testing data has a similar distribution to the training data,
in reality, the observed data distribution may be different from the historical data that the model is trained on.
Such difference can substantially compromise the quality of model predictions.
The authors analyze the possible causes for dataset shift, e.g., malicious software that evolves over time, and
review the techniques dealing with dataset shift.
They characterize adversarial attacks as one form of dataset shift, where adversaries adaptively
change test instances to create a distribution that differs from training data.
%All works discussed in our survey assumed similar distribution on training and testing data, treating adversarial attacks as the only dataset shift in the problem setup.
%However, in real applications, the underlying data distribution itself can be non-stationary, and the characterize the influence of the dataset shift between training and testing data on the adversarial robustness is yet to be investigated.

%More works use the connection between adversarial attacks and distributional shift to analyze the effect of adversaries on generalization performance~\cite{Tu:Zhang:Tao:NeurIPS:2019}.
%However, we do not discuss them in detail, as they focus more on models instead of data.
%\jr{How is that relevant to data properties section?} \gx{This can be removed, as it an individual work we filtered}

\vspace{-0.1in}
\subsection{Non-data Related Reasons for Adversarial Vulnerability}
There has been a variety of hypotheses regarding the reasons behind adversarial vulnerability of ML systems.
In addition to the data used for training,  adversarial robustness could also depend on the choice of the model architecture,
the training procedure, and the interplay between data and the learning algorithm, i.e., correspondence between the complexity of a model to that of the data.
This section summarizes the key hypotheses regarding these aspects.
%The hypotheses reviewed in this section are complementary to the potential influence from the data.

%\jr{there is a lot of undefined terminology and jargon in this section, but let's leave it like this for now.}

\vspace{0.05in}
\noindent
\textbf{Model.}
When Szegedy et al.~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014} first discovered adversarial examples for visual models, they suspected the high non-linearity of DNNs resulted in low probability `pockets' of adversarial examples in the learned representation manifold.
They hypothesize that while these pockets can be found through attack algorithms, the samples residing in these pockets have different distributions compared to normal samples and are thus subsequently harder to find when randomly sampling from the input space.
Instead, Goodfellow et al.~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015} hypothesize that
the linearity from activation functions, like ReLU and sigmoid found in high-dimensional neural networks, induce vulnerability towards adversarial perturbations.
To support their claim, they present the attack method FGSM that exploits the linearity of the target classifier.
Fawzi et al.~\cite{Fawzi:Fawzi:Frossard:ICMLWorkshop:2015} also argue against the hypothesis of high non-linearity as the cause for adversarial examples.
They show that all classifiers are susceptible to adversarial attacks and claim that it is the low flexibility of the classifier compared to the complexity of the classification task that results in vulnerability.
The lack of consensus on primary causes of models' vulnerability invites more studies on this topic.

Singla et al.~\cite{Singla:Ge:Basri:Jacobs:NeurIPS:2021} show that enforcing invariance to circular shifts (e.g., rotation) in neural networks induces decision boundaries with a smaller margin than normal, fully connected networks,
which, in turn, reduces the adversarial robustness of the model.
Moosavi{-}Dezfooli et al.~\cite{Moosavi-Dezfooli:Fawzi:Fawzi:Frossard:Soatto:ICLR:2018} introduce universal,
input-agnostic perturbations to mislead the classifier and hypothesize that the vulnerability of a multi-class classifier to such perturbations is related to the shape of its decision boundaries, e.g.,
linear classifiers with decision boundaries that are parallel to each other and
nonlinear classifier with decision boundaries that are curved in a similar way
tend to be less robust as
perturbations in one direction can change the prediction label for a different class.

Tanay and Griffin~\cite{Tanay:Griffin:ArXiv:2016} conjecture that the decision boundary learned by the classifier being too close to (or `tilted towards') the data manifold instead of being perpendicular to it,
results in small perturbations being sufficient to move samples across the decision boundary for misclassification.
%data manifold refers to the underlying structure that the data exhibit

\vspace{0.05in}
\noindent
\textbf{Computational Resources.}
Bubeck et al.~\cite{Bubeck:Lee:Price:Razenshteyn:ICML:2019} use computational hardness theory to show that the time complexity for learning a robust model is exponential to the size of input data and thus is computationally intractable.
Hence, they attribute adversarial vulnerability to computational limitations of current learning algorithms.
Degwekar et al.~\cite{Degwekar:Nakkiran:Vaikuntanathan:COLT:2019} further extend this work and also show the impossibility of efficiently training robust classifiers.

%\subsubsection{Ineffective Learning Perspective}
\vspace{0.05in}
\noindent
\textbf{Robustness of Features.}
Ilyas et al.~\cite{Ilyas:Santurkar:Tsipras:Engstrom:Tran:Madry:NeurIPS:2019} show that adversarial vulnerability can be a consequence of a model exploiting well-generalizing but non-robust features,
i.e., features that are spurious and sometimes incomprehensible to humans;
when constraining the model to use robust features, the adversarial robustness increases together with the
interpretability of the learned features.
However, Tsipras et al.~\cite{Tsipras:Santurkar:Engstrom:Turner:Madry:ICLR:2019} note that, as the features for achieving high accuracy may be different from the ones for achieving high robustness, robustness may be at odds with standard accuracy.
%
%\jr{why is it called Ineffective learning when it is about features.}\gx{I put it under ineffective learning as in this case, the model learns/decides the features for generalization, and when given the correct objective, the model in fact, can learn more robust features, so I think the underlying reason is objective we gave for the model didn't guide the model to learn the right features}
%
Instead of seeing adversarial vulnerability as a product of classifiers being overly sensitive to changes in spurious features, Jacobsen et al.~\cite{Jacobsen:Behrmann:Zemel:Bethge:ICLR:2019} hypothesize that classifiers can rather be
overly insensitive to relevant semantic information, e.g., images with drastically different content can share similar latent representations.
The authors introduce a new type of adversarial examples that exploit such insensitivity, where the content of images is altered without changing the resulting prediction label.
%As both insensitivity to semantic content and sensitivity to spurious changes can simultaneously exist in models,
%more investigation into how to define proper objectives for models to effectively distinguish the relevant information is needed.
While all these works propose possible reasons for adversarial vulnerabilities, they are orthogonal to our survey, which focuses particularly on the influence of training data.



