\subsection{Domain-Specific}
\label{sec:results-domain-specific}

Papers in this category provide insights into the correlation between domain-specific data properties and adversarial robustness.
Among our collected papers, all the domain-specific studies focused on the same topic: understanding the adversarial vulnerabilities of image classifiers based on image \emph{frequency}~--
how fast the intensity of pixel values changes with respect to space (i.e., images with intensive color changes have high frequency).
As shown in Fig.~\ref{fig:image_freq}, the skin of a zebra has higher image frequency than a horse, because of the black-and-white stripes.
%Frequency information of images is widely used in applications, such as image reconstruction, filtering, and compression.
%
%Discrete Fourier Transform is the technique commonly applied to transform an image from the spatial domain to the frequency domain.
%This transformation outputs the \emph{amplitude} and \emph{phase} of the frequency in the image.
%The amplitude represents the magnitude of the different frequencies in the image and captures the geometrical structure of its features.
%%(i.e. sharp corners in the spatial domain).
%Phase encodes the locations of those features.
%\jr{most definitions in this paragraph are never used, besides in the last paper. I removed it.}
%
Papers studying image frequency are listed in Fig.~\ref{fig:domain}.
They can be roughly divided into:
\roundrect{1} papers discussing the influence of frequency distribution on the model adversarial robustness,
and~\roundrect{2}~papers explaining adversarial vulnerabilities using perceptual differences between human and models. % frequency information.
%Yin et al.~\cite{Yin:Lopes:Shlens:Cubuk:Gilmer:NeurIPS:2019} and Ortiz-Jimenez et al.~\cite{Ortiz-Jimenez:Modas:Moosavi-Dezfooli:Frossard:NeurIPS:2020}
%We found two papers discuss how the distribution of frequency among training images influence resulting visual model's adversarial robustness.

\begin{figure*}[t]
  \centering
    \begin{minipage}{0.5\textwidth}
  \centering
  \vspace{0.05in}
  \includegraphics[valign=t, width =\textwidth]{images/domain_specific_frequency.pdf}
  \caption{Image frequency.}
  \label{fig:image_freq}
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
  \centering
  \includegraphics[valign=t, width =0.9\textwidth]{images/domain_specific_vertical_legend.pdf} %0.56
  \vspace{-0.02in}
  \caption{Papers discussing domain-specific properties. }
  \label{fig:domain}
  \end{minipage}
  \vspace{-0.15in}
\end{figure*}

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Image Frequency Distribution.}
%\jr{Gabby-P23}
Yin et al.~\cite{Yin:Lopes:Shlens:Cubuk:Gilmer:NeurIPS:2019} show that frequency distribution of the inputs generated from data augmentation techniques explains the resulting model sensitivity to adversarial attacks.
%Data augmentation are commonly used to improve image classifier's robust generalization ability, previous work~\cite{Gilmer:Ford:Carlini:Cubuk:ICML:2019} showed training with Gaussian augmentation or adversarial training improves robustness against most types of corruptions (e.g., \emph{blurring}, \emph{Gaussian noise}), but worsens robustness against \emph{fog} and \emph{contrast} corruptions.
%They explain the differences in robustness against different corruption strategies using the distribution of frequency among the data.
In particular, the authors show that Gaussian augmentation,
as well as adversarial training techniques that rely on data augmentation,
generate perturbations with high-frequency components.
Augmenting training data with those augmented inputs makes the resulting model more robust against perturbations in
high-frequency domains while, at the same time, more vulnerable to perturbations concentrated in low-frequency domains.
%
%Augmenting training data with those samples makes resulting models become more robust against noise from high-frequency domains.
%However, corruptions like \emph{fog} and \emph{contrast} generate perturbations that are more concentrated at low-frequency domains, which makes models trained with Gaussian augmentation or adversarial training less robust against these corruptions.
To mitigate this issue, the authors propose to avoid biasing the model towards/against certain frequency ranges by increasing the diversity of frequency distribution in augmentations.

%\jr{All-P48}
Ortiz-Jimenez et al.~\cite{Ortiz-Jimenez:Modas:Moosavi-Dezfooli:Frossard:NeurIPS:2020}
analyze CNN robustness through the classifier margins along particular frequencies, which the authors define
%the margin along a particular frequency range
as the minimal perturbation in that frequency required to change the model prediction.
The authors show that CNN models tend to have smaller margins along low-frequency vs. high-frequency ranges,
likely because, for most image datasets, one can differentiate classes mainly using features from low-frequency ranges.
This finding implies that models are more sensitive to attacks that modify low-frequency components.
The authors thus suggest training more robust models that enlarge margins along low-frequency ranges by augmenting training datasets with perturbations concentrated in those ranges.
%This implies that models are more sensitive to the perturbations on low-frequency components than those on high-frequency components.
%The authors also show that adversarial training can enlarge margins along low-frequency ranges by augmenting training datasets with perturbations more concentrated in the low frequency ranges.
%, and as a by-product of the model's inductive bias, the margins along high-frequency ranges also increase.

%\textbf{``High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks''}: \gx{Data property: frequency of the images, especially high-frequency components}
%Wang et al.~\cite{Wang:Wu:Huang:Xing:CVPR:2020} and Chen et al.~\cite{Chen:Peng:Ma:Li:Du:Tian:ICCV:2021}
%We found another two papers explain the existence of adversarial examples using the differences in what human and CNN models' capture from the images.
\vspace{0.05in}
\noindent
\roundrect{2} {\bf Perceptual Differences.}
%\mt{Add to table}
Wang et al.~\cite{Wang:Wu:Huang:Xing:CVPR:2020} attribute the origin of adversarial examples to the perceptual differences of humans and CNNs in frequency ranges.
In particular, humans classify images based on low-frequency components as high-frequency components are not visible to the human eye.
CNNs, on the other hand, are able to `see' the full frequency spectrum which allows them to exploit high-frequency components for better generalization.
This implies that adversarial examples generated by perturbing high-frequency components can mislead CNNs while being imperceptible to humans.
%In the case of adversarial attacks, this implies that adversarial examples generated by perturbing high-frequency components can mislead CNNs while being imperceptible to humans.
The authors show that adversarially robust models depend less on high-frequency components and propose to use smoother convolutional filters to reduce a model's attention to these components.
%In addition, they demonstrate a connection between the model's reduced attention to high frequency components and the use of smoother Convolutional filters.
%Their experiments show that using such filters can easily improve the robustness of standardly trained models, however, the improvement is only apparent for larger perturbations for adversarially trained models.

%\js{Add to table}
Unlike Wang et al.~\cite{Wang:Wu:Huang:Xing:CVPR:2020},
Chen et al.~\cite{Chen:Peng:Ma:Li:Du:Tian:ICCV:2021} posit that the adversarial vulnerability of CNNs
results from their over-reliance on amplitude information of images~--
the magnitude of the different frequencies in the image.
%Humans, considered as robust classifiers, rely more on phase information to recognize an object.
The authors show that replacing the amplitude information of an image with information from another image can successfully mislead CNNs but not humans, who rather rely on phase information~-- the locations of the features, to recognize objects.
Based on this observation, the authors propose to strengthen CNNs' attention to phase information through a data augmentation technique that fuzzes amplitude while preserving the same phase information.

%\de{replaces the amplitude information of images with ones from multiple other images while maintaining the same labels.} \jr{how does this give more attention to phase information?}
% \ad{creates multiple copies of an image by retaining the phase information while replace the amplitude information with the ones from other images to help strengthening the correlation between phase information and labels.}
%%combines the phase spectrum of the current image with amplitude spectrum of another image and labels it as the current image only.

%\gx{answer the questions in bullet points}
%\section{Questions our survey answers}
%\begin{enumerate}
%     \item What properties of data influence the resulting modelâ€™s robustness?
%	\item How to select and represent data to improve robustness?
%	\item How to select and configure a classifier to be robust based on the properties data?
%\end{enumerate} 