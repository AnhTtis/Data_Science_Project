\subsection{Density}
\label{sec:results-density}

 \begin{figure*}[h]
  \centering
  \vspace{-0.15in}
  \begin{minipage}{0.53\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/density_overview.pdf}
  \vspace{-0.14in}
  \caption{Density illustration.}
  \label{fig:density_guideline}
  \vspace{-0.1in}
  \end{minipage}
  \hspace{0.1in}
  \begin{minipage}{0.42\textwidth}
  %\vspace*{0.1in}
  \centering
  \includegraphics[width =\textwidth]{images/density_vertical_legend.pdf}
  \vspace{-0.25in}
  \caption{Papers discussing density. }
  \label{fig:density}
  \end{minipage}
 % \vspace{-0.1in}
\end{figure*}
%  \begin{minipage}{0.4\textwidth}
%  \centering
%  \vspace{0.18in}
%  \includegraphics[width = 0.92\textwidth]{images/DensityFig.pdf}
%  \vspace{0.05in}
%  \caption{Probability density function.}
%  \label{fig:densityFig}
%  \end{minipage}
%\vspace{-0.15in}
%\end{figure*}

\emph{Density} measures the closeness of samples in a particular bounded region.
For continuous data, it is mathematically described by the probability density function (PDF),
which gives the probability for a variable to take a certain range of values.
For discrete data, it is described as the probability mass function (PMF), which
gives the probability for a variable to take a particular value.
We say that an area is dense when there is a high probability that random samples lie in the same area, i.e.,
close to each other.
For example, the dataset on the right-hand side of Fig.~\ref{fig:density_guideline} contains a larger number of samples in close proximity and, thus, is more dense than the dataset on the left-hand side of the figure.
Furthermore, density can be defined over samples from one class, in which case, it is referred to as \emph{class density}.

Papers that study how density influences adversarial robustness are shown in Fig.~\ref{fig:density}. They can roughly be divided into
\roundrect{1}~papers discussing the effect of class density on robustness and
\roundrect{2}~papers proposing attacks and defenses using density information.


\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effect of Class Density on Robustness}.
%For instance,
Shafahi et al.~\cite{Shafahi:Huang:Studer:Feizi:Goldstein:ICLR:2019} %, discussed in Section~\ref{sec:results-dimensionality},
show that datasets with a higher upper bound of class density lead to better robustness.
In particular, for image datasets, the authors show that images of lower complexity,
e.g., with simple objects on plain backgrounds, have a higher correlation among adjacent pixels.
Datasets comprised of such images have a higher density, as pixel values are more frequently repeated, and, thus, lead to better robustness.
The authors confirm this observation by showing that classifiers trained on \mnist, which has a lower image complexity and thus higher density than \cifarten, are more robust than those trained on \cifarten.
Furthermore, the authors state that class density is a better predictor of robustness than dimensionality:
even after up-scaling \mnist to the same dimensionality as \cifarten,
it still has a higher density and thus results in more robust classifiers than \cifarten.
%\jr{well, that is because up-scaling does not change intrinsic dimensionality, no? Do they have any arguments on that?}
%\gx{They upscale the image by repeating the pixel values, e.g., upscale the image to 4 times of its dimension by repeat each 1 x 1 pixel 4 times (in 2 x 2 pixels).
%They didn't relate their observation to intrinsic dimensionality.}
%However, the authors do not provide an empirical method to determine the density of these datasets.

\vspace{0.05in}
\noindent
\roundrect{2} {\bf Attacks and Defenses Using Density}.
%\jr{All-P49}
%Additional works note that adversarial examples are commonly found in the low-density regions of the training dataset and propose defense or attack strategies leveraging this phenomenon.
Several works note that adversarial examples are commonly found in low-density regions of the training dataset, as models are unable to learn accurate decision boundaries using a small number of samples from these regions.
%\jr{Shubhraneel-P45}
Zhang et al.~\cite{Zhang:Chen:Song:Boning:Dhillon:Hsieh:ICLR:2019}
propose an attack strategy that retrieves candidate samples from low-density regions and
perturbs them to generate adversarial examples.
The authors demonstrate that, even after adversarial training, models will not be robust to adversarial attacks that target these low-density regions.
%\gx{As adversarial training only works well when the testing data share similar distribution as the empirical training data used to train the model, i.e., adversarial training cannot improve the model's robustness in the regions where are not well-supported by data.}
%\jr{All-P40}

A similar finding by Zhu et al.~\cite{Zhu:Sun:Li:ICLR:2022} suggests that adversarial examples from low-density regions have a higher probability of being transferable between different models trained on the same dataset.
Based on this observation, the authors propose an attack that increases the transferability of adversarial examples by
identifying perturbation directions that maximize both the adversarial risk and
the alignment with the direction of density decrease for the underlying data distribution,
i.e., move samples towards regions with lower density.

Departing from the same idea that low-density regions are prone to adversarial attacks,
Song et al.~\cite{Song:Kim:Nowozin:Ermon:Kushman:ICLR:2017} focus on creating a defense mechanism
that uses generative models to detect if a sample comes from a low-density region when making predictions.
If so, the sample is moved towards a more dense region of the training data as a ``purification'' step.

To harden models directly, Pang et al.~\cite{Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020} propose a new loss function for DNNs,
to learn dense latent feature representations.
The authors first show that the commonly used Softmax Cross-Entropy loss function induces sparse representations
(i.e., with low class density),
which lead to vulnerable models. This is because a low number of samples in close proximity to each other prevent a model from learning reliable decision boundaries.
They then propose a loss function that explicitly encourages feature representations to concentrate around class centers;
like in their earlier work~\cite{Pang:Du:Zhu:ICML:2018}, the authors compute the coordinates of the desired
class centers (as a function of the number of classes and the dimensionality of the input data)
to maximize the distances between the centers.
The authors demonstrate that the proposed approach improves robustness under both standard and adversarial training.

%\jr{I still think it is very similar to their previous paper, and both probably relevant in distribution, separation, and density, as they change them all.}
%\gx{Such that it is equally difficult to create adversarial examples for any class. Need to double check.}

%\jr{where this new paper is coming from? I do not follow the description in any case. Please fix grammar issue and simplify to be in plain English, to convey the intuition.}
%\gx{This paper was originally only included in 4.5 Separation, but as it also discuss about density, so we move it here as well.} 