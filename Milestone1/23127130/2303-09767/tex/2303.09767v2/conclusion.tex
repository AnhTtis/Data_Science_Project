\section{Conclusion}
\label{sec:conclusion}
In this survey, we systematically collected, analyzed, and described papers that discuss how data properties affect adversarial robustness in machine learning models.
By analyzing \revreplace{57}{77} research papers from top scientific venues in Machine Learning, Computer Vision, Computational Linguistics, and Security,
we identified seven domain-agnostic data properties and one image-specific data property that are correlated with adversarial robustness. %These data properties can have a fundamental impact on the resulting model's robust generalization, and thereby, altering them can significantly improve adversarial robustness.

While several of the guidelines for constructing high-quality data that we identified
are similar to those recommended for training accurate models,
producing robust models is more sensitive to the characteristics of the data and requires more effort,
e.g., a larger number of samples, better label qualities, etc. There are also additional data properties important for building
robust models that are not extensively discussed in non-adversarial settings, e.g., concentration of measure.
In a sense, robust generalization is a stronger form of standard generalization.

We identified possible next steps towards improving the understanding of how the data affects a model's adversarial robustness.
These include
studying interactions between different properties of data,
considering the effect of additional properties that improve standard generalization on robust model generalization,
devising quantitative metrics for different aspects of the data, and
extending the studies and their empirical evaluation beyond the images domain.
We hope our survey will help researchers and ML practitioners to better understand adversarial vulnerability
and will spark further research to address the identified knowledge gaps.


%\mt{Besides the first few sentences that describe high level findings (which I like), this paragraph also seems like a discussion point (which we already included). I think this entire conclusion section can be much shorter, i.e., just describe what we did plus a high level description of what we found.}
%\jr{I agree.}

%Our survey also showed that high dimensionality and high concentration are frequently associated with inevitability of adversarial examples.
%This is mainly due to the non-intuitive behavior of distance in high dimension, where small perturbations can easily expand any non-empty set to cover almost the whole space.
%Such phenomenon implies that any classifier that have non-zero error rate on normal testing samples are vulnerable to adversarial examples with small perturbations.
%As such, improving model's accuracy on normal samples can potentially improve its adversarial robustness.
%\jr{all this belong to discussion, I think, not to conclusions.}

%For immediate applicability, we identified metrics for estimating and interpreting the intrinsic robustness of empirical datasets. \jr{which metrics? where identified? unclear}.
%Works on dimensionality, concentration, and separation provide such analytic formulas relating the properties of data to the theoretical bounds of adversarial robustness for given datasets. \jr{what about other properties. Unclear, I do not think this belongs to conclusions either. Conclusions need to be more high level and in layman language.}
%We also reviewed the defense techniques that achieve robustness through altering the data or its representations. \jr{where. This, I think belong to summary and with more details.}
%Papers in our collection presented techniques for filling sample complexity gaps, reducing dimensionality, transforming latent representation distributions into optimal distributions, increasing class density, and enlarging inter-class distances.
%\jr{same.}


%\mt{This entire paragraph is a discussion point (future work suggestion), so I think it should be removed from here as we moved our future work to the discussion section.}
%However, we also found that there is a lack of metrics to quantitatively describe the distributional property of the data, or gauge the quality of the input labels in terms of accuracy and granularity.
%We anticipate future research will shed more light on such areas
%by presenting techniques to reliably estimate the type of distribution that an empirical dataset conforms to
%or metrics to determine the coherence and quality of sample labels.
%\gx{More fine finding consider shorten}
%We also identified defense techniques that achieve robustness through altering the data or its representations.
%In the collected literature, techniques were presented for filling sample complexity gaps, reducing dimensionality, transforming latent representation distributions into optimal distributions, increasing class density, and enlarging inter-class distances.
%However, current techniques mostly alter one type of data property. This may be due to the limited understanding of how different data properties impact adversarial robustness.
%Furthermore, we observe that some data properties which have been studied for standard generalization are not explored for robust generalization, such as small disjuncts, overlapping classes, and data complexity.
%There is a need for more systematic research regarding how various aspects of data collectively shape the adversarial robustness of models.
%This will provide a more comprehensive explanation of why adversarial examples exist.

%to fill in the gap in sample complexity for robust generalization, multiple work proposed incorporating certain number of auxiliary unlabeled samples.
%To reduce the impact from high dimensionality, one can reduce the dimensions using dimensionality reduction techniques such as PCA prior to (adversarial) training.
%To transform data into distributions that are more optimal for training robust models, one can choose from loss functions which guide the models to learn latent feature representation with large inter-class distances or to transform the input representation into particular type of distribution that are inherently more robust.

%For non-parametric classifiers, as separation in sample space can effectively guarantee robustness against certain perturbations, strategy for sample pruning to create well-separated data is also proposed.

% regarding classifier
% CNN's case: diversity in the frequency distribution among the training data can improve the resulting model's robustness
% multiple work suggest to abstrain predictions on uncertain samples
% incompatability between the complexity of the classifier and the complexity of the dataset may also result in poor robustness (refering to the paper on symmetry of the data distribution)

%Our work complements existing surveys that study how data properties influence model's standard generalization by identifying data's impacts on model's robust generalization.
%This article also presents a different perspective on adversarial robustness compared to existing surveys by
%focusing on how input data can explain the differences in resulting model's adversarial robustness.



