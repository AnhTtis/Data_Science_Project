\section{Preliminaries}
\label{sec:background}
We now provide a brief overview of the main concepts related to machine learning,
adversarial robustness, and most commonly studied data distributions.
The goal of this section is to introduce terminology used in the rest of the survey rather than provide an extensive overview of the adversarial robustness research area.
For a more detailed overview, please refer to guides on statistics and machine learning~\cite{Shalev-Shwartz:Ben-David:2014, Giuseppe:2017, Hastie:Tibshirani:Friedman:2009} and adversarial robustness~\cite{Chen:Hsieh:2023, Biggio:Roli:PR:2018, Nicolae:Sinn:Tran:2018:ArXiv}. 
\revadd{Please also see Tables~\ref{tbl:symbols} and~\ref{tbl:acronyms} in the Appendix for the list of symbols and acronyms used in this paper.}

\subsection{Machine Learning}
Machine learning refers to the automated detection of meaningful patterns in data~\cite{Shalev-Shwartz:Ben-David:2014}
and can be largely divided into supervised, unsupervised, and reinforcement learning.
In supervised learning, a learning model is provided with input-output pairs of data (a.k.a. labeled training data);
based on this data, the model aims to infer a function that maps the inputs to the outputs.
Supervised learning is typically associated with classification and regression problems, which use categorical and continuous labels, respectively. %\emph{The focus of our survey is on supervised machine learning models}.
In classification, this number of possible labels for an input is also referred to as the number of \emph{classes}.
Datasets with only two classes are called \emph{binary datasets}, on which one can train a \emph{binary classifier}.

Unlike supervised learning, unsupervised learning algorithms are usually concerned with identifying patterns in unlabeled data, e.g.,
grouping similar samples together in the absence of labels (clustering) or
transforming data into a different representation (representation learning).
%Another example is unsupervised , where each data point is transformed into a different representation, e.g., with a smaller number of features (a.k.a., dimensionality reduction).
Reinforcement learning characterizes algorithms that learn from a series of rewards and punishments,
with the goal of maximizing the cumulative reward, e.g., to build robots that learn to take the best sequence of actions according to signals from the environment.

Variations, such as, semi-supervised learning (i.e., learning from partially labeled data) and
self-supervised learning (i.e., learning from labels extracted by the learner itself) have also been proposed for problems where acquiring labeled data may be challenging or expensive.

%In this survey, we focus on the study of adversarial attacks against

%Orthogonal to the learning type,
ML algorithms can also be divided into parametric and non-parametric.
Parametric algorithms have a predetermined, fixed number of parameters defined before the training starts.
For example, for Linear Support Vector Machines (SVMs), these parameters are the coefficients of all features of the
training data and the learned intercept.
For Deep Neural Networks (DNNs), the number of parameters is determined by the architecture of the network.
In non-parametric algorithms, the number of parameters is determined at training time and may vary depending on the number of training samples.
For example, the ``depth'' of Decision Trees can grow (beyond the size of the feature set)
when more decision points are needed to accurately separate training data.
Other commonly used non-parametric models include $k$-Nearest Neighbors ($k$-NN) and Kernel SVMs.

%\vspace{-0.2in}
\subsection{Adversarial Robustness}
Adversarial machine learning studies the arms race between adversarial attacks and defenses.
Attacks aim at degrading model performance while defenses propose algorithms to harden models against the attacks.
Adversarial attacks can be categorized into \emph{evasion} and \emph{poisoning}~\cite{Biggio:Roli:PR:2018,Li:Li:Ye:Xu:CSUR:2021}.
Evasion attacks aim to fool machine learning models by generating inputs that, despite no noticeable difference for a human,
will be incorrectly classified.
Such inputs, known as \emph{adversarial examples} and created by applying non-random perturbations to samples, are carefully designed to change models' predictions~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014,
Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Carlini:Wagner:SP:2017}.
Instead, poisoning attacks tampers with model training data, in order to degrade model performance.
In this survey, we focus on evasion attacks; the terms adversarial and evasion attacks are often used interchangeably in the literature as this is the most popular and commonly studied type of attack.

%Model extraction attacks and inference attacks have similar goals of obtaining sensitive information about the machine learning system, in which model extraction attacks seek to steal the model itself, while inference attacks seek to infer the training data.
%Both of them achieve their goals by feeding in specially designed instances to probe for desired information from the model.

The term \emph{robustness} for machine learning models is often used to refer to different concepts, such as,
stability to distribution shifts,
the ability to identify adversarial examples, and
the ability to make the correct predictions in the face of adversarial examples.
In this survey, we use the latter definition -- the ability to make the correct predictions in the face of adversarial examples.
This is a stronger notion of robustness than merely identifying adversarial examples, as the identification of an adversarial example does not guarantee its correct classification.
The phenomenon of making satisfactory model predictions in the face of adversarial examples is also often referred to as \emph{robust generalization}.
This is different from \emph{standard generalization}, a term used to describe
making satisfactory model predictions for normal, unseen samples.

\vspace{0.05in}
\noindent
{\bf Adversarial (Evasion) Attacks.}
Techniques for generating adversarial examples for evasion attacks can be broadly divided into three categories,
according to the type of information available to the attacker~\cite{Biggio:Roli:PR:2018}.
In \emph{white-box attacks}, the attacker is assumed to be able to leverage all available information about the training data, the model, and the training procedure.
In \emph{grey-box attacks}, the attacker is assumed to have only partial information about the model, such as, the source of training data.
Finally, the most conservative type of attacks are \emph{black-box attacks}, where the attacker has no information about the inner workings of the model except, possibly, for the prediction outcomes.

\emph {Gradient-based attacks} are commonly used in white-box settings. These attacks use the gradient of a differentiable function defined over model weights as a guide when crafting adversarial examples.
The most commonly used differentiable function is the loss function used by the model during training.
A gradient defines the direction of the maximal increase in the local value of a function.
Hence, by using the gradient of the loss function with respect to the input, one can adjust the input to get the maximal increase in the loss of the model, which ultimately leads to a bad prediction.
Fast Gradient Sign Method (FGSM)~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015}, Basic Iterative Method (BIM)~\cite{Kurakin:Goodfellow:Bengio:ICLR:2017}, and Projected Gradient Descent (PGD)~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018} are examples of attack algorithms that utilize the gradient of the loss function used for training.
Instead of the loss function, the FAB attack~\cite{Croce:Hein:ICML:2020:1} uses the gradient of a function defined by the difference of model outputs of the penultimate layer of a neural network -- a layer which outputs the probabilities that a given sample belongs to each of the available classes.
By defining the difference of outputs of the penultimate layer as the differentiable function, the FAB attack maximizes the difference in probabilities between the target class and other classes, to increase the chance of misclassification.
%\gx{Maybe we also want to add "Generative Adversarial Perturbations", the type of work that generate adversarial examples using GANs?}

%	\begin{table}[]
%		\scalebox{0.875}{
%			\begin{tabular}{|p{0.4\textwidth}|p{0.6\textwidth}|}
%				\hline
%				Attack & Description                             \\ \hline
%				Fast Gradient Sign Method (FGSM)~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015}   & Computes the gradient of the loss of a classifier at a given point, and uses its sign to shift a sample by a constant factor towards that direction. \\ \hline
%				Basic Iterative Method (BIM) (Iterative-FGSM)~\cite{Kurakin:Goodfellow:Bengio:ICLR:2017}    &  An iterative version of FGSM where multiple smaller shifts are made instead of one large step. \\ \hline
%				Projected Gradient Descent (PGD)~\cite{Madry:Makelov:Schmidt:Tsipras:Vladu:ICLR:2018}      &  A modification of BIM to use a random initialization of the original sample in an $L_p$ ball and successive random restarts.        \\ \hline
%				Jacobian-based Saliency Map Attack (JSMA)~\cite{Papernot:McDaniel:Jha:Fredrikson:Celik:Swami:EuroSP:2015}    & Constructs saliency map to identify a set of relevant features and perturbs them towards the direction of greatest increase in loss. JSMA is efficient as it does not perturb all available features but it is more expensive. \\ \hline
%				Carlini and Wagner Attack (C\&W)~\cite{Carlini:Wagner:SP:2017}   &  Minimizes the size of a perturbation required to create an adversarial example by using optimization techniques including gradient-based optimization. \\ \hline
%				DeepFool~\cite{Moosavi-Dezfooli:Fawzi:Frossard:CVPR:2016}      &  Finds the minimal perturbation to misclassify an image by successively projecting a sample towards the nearest decision boudary.   \\ \hline
%				Fast Adaptive Boundary Attack (FAB)~\cite{Croce:Hein:ICML:2020:1}  &  An extension of DeepFool that further minimizes the perturbation required by biasing gradient steps towards the original sample. it also adds a backward optimization step and restarts to search for a more optimal perturbation. \\ \hline
%				AutoAttack~\cite{Croce:Hein:ICML:2020:2}       &  An ensemble parameter-free attack comprised of variations of PGD, FAB and a query efficient black box attack designed to comprehensively evaluate the robustness of neural networks. \\ \hline
%				
%			\end{tabular}
%		}
%		\caption{Popular gradient-based attack algorithms}
%		\label{tbl:gradient-based-attacks}
%	\end{table}

\emph{Non-gradient based attacks} are applicable for more diverse types of models that do not use a differentiable functions, e.g., decision trees.
Such attacks can also be used in black-box and grey-box settings, when gradient information is hidden from the attacker.
One example of non-gradient-based attacks is the \emph{mimicry attack}, which involves adding and removing features in the perturbed sample, e.g., based on their popularity in the target class~\cite{Demontis:Melis:Biggio:Maiorca:Arp:Konrad:Rieck:Corona:Giacinto:Roli:TDSC:2019}.
%Constructing a sample using a weighted combination of two different samples can also be considered as a non-gradient based attack.

\vspace{0.05in}
\noindent
{\bf Adversarial Defenses.}
Defense mechanisms against adversarial attacks target various stages of the machine learning pipeline.
Specifically, defenses \emph{on raw data} focus on the training data itself, e.g.,
by selecting a subset of ``robust'' features~\cite{Ilyas:Santurkar:Tsipras:Engstrom:Tran:Madry:NeurIPS:2019} or
using representation learning to transform features into a different representation, making sure
a model trained on the new representation is inherently more robust~\cite{Yang:Guo:Wang:Xu:AAAI:2021}.
%	Examples of such transformations include: using a selected number of useful features to represent the entire dataset (a.k.a. feature selection), complementing the dataset by adding different transformations (a.k.a. data augmentation), and
%	algorithmically determining a different set of features to represent the dataset (a.k.a. representation learning).
%	Representation learning techniques such as Auto-encoders \cite{baldi:PMLR:2012} may also use machine learning models to learn a data transformation function.
%	The goal of such techniques is to represent the data using a set of robust features.

Defenses \emph{during training} alter the standard training procedure to improve model robustness.
The most common such technique is adversarial training~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015}, which involves continually augmenting the training data with adversarial examples generated by an attack algorithm.
By retraining the model while adding correctly labeled malicious samples to the training dataset, the model learns to capture persistent patterns and becomes more robust against these attacks.
Another common method is \emph{regularization}, where model parameters are constrained so that very small perturbations have little effect on the prediction outcome~\cite{Gouk:Frank:Pfahringer:J.Cree:ML:2021}.
%Early stopping~\cite{Wong:Rice:Kolter:ICLR:2020}, which halts the training process before it has fully converged, to reduce the change of overfiting to the training data, is another technique that improves generalization.
%\jr{why would it improve robust generalization.}
	
Defenses \emph{during inference} focus on making existing models more robust when the model is being used on new samples.
For example, randomized smoothing~\cite{Cohen:Rosenfeld:Kolter:ICML:2019} involves creating multiple noisy
instances of a sample and aggregating the model's predictions during inference.
Given that adversarial examples are typically close to genuine samples, averaging the results from close neighbors of an input can potentially reduce the chances of the model being misled.
In addition, different variations of ensemble models~-- using multiple models and aggregating their output~--
have been shown to increase the robustness to adversarial attacks~\cite{Pang:Xu:Du:Chen:Zhu:ICML:2019}.
%For example, Bayesian Neural Networks (BNNs)~\cite{Titterington:StatScience:2004} that first learn probability distributions over the possible values for model parameters and then, during inference,
%randomly sample a number of values for the model parameters from the learned distributions to form concrete neural networks to aggregates their predictions, were shown to be more robust against adversarial examples~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}.


%A related technique is Bayesian Neural Networks (BNN)~\cite{Titterington:StatScience:2004} which learn a probability distribution over the possible neural networks instead of one model with fix weights.
%There are mainly \gx{to fill in} reasons for adversarial vulnerability:
%(1) Model's sensitivity
%(2) Sub-optimal representations: it is empirically showed the representations learned by the network may not be able to reflect the similarity in data space:
%the representations for natural example $x$ and its adversarial example $x'$ can be markedly different even though they are in close proximity in the data space.
%(3) Inherent Inevitability due to the geometry of high dimensionality
%The existing defense techniques fall into the following categories:
%\begin{itemize}
%	\item{Data Augmentation based approaches}
%	\begin{itemize}
%		\item Gaussian augmentation
%		\item Mixup, ...
%	\end{itemize}
%	\item{Adversarial Training} a type of techniques that improve the classifier's robustness by
%	\begin{itemize}
%		\item Augmenting training data with adversarial examples~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014, Goodfellow:Shlens:Szegedy:ICLR:2015, Kurakin:Goodfellow:Bengio:ICLR:2017}
%		\item Updating the training procedure to minimize the worst case adversarial loss by including adversarial examples generated on-the-fly in each training epoch~\cite{Madry:Makelov:Schmidt:Tsipras:Vladu:ICLR:2018, Sinha:Namkoong:Duchi:ICLR:2018}.
%		Depending on the methods used to account for adversarial examples, the optimization approaches can be generally divided into two categories:
%		\begin{itemize}
%			\item Instance-based: generates the worst case adversarial examples by applying powerful attacks (e.g., FSGM, PGD etc.) for each instance.  In this case, the optimal classifier minimizes the expected loss over the set of adversarial examples~\cite{Madry:Makelov:Schmidt:Tsipras:Vladu:ICLR:2018}.
%			\item Distribution-based: assumes the worst case adversarial examples are included in distributions within certain distance away from the training data distribution. In this case, the optimal classifier minimizes the expected loss over the possible set of distributions (also known as \emph{Distributional Robust Optimization}~\cite{Sinha:Namkoong:Duchi:ICLR:2018})
%		\end{itemize}
%	\end{itemize}
%	\item{Early Stopping~\cite{Wong:Rice:Kolter:ICLR:2020}}
%\end{itemize}



%\jr{Properly define robustness, based on excluded ``Analyzing and Improving Representations with the Soft Nearest Neighbor Loss~\cite{Frosst:Papernot:Hinton:ICML:2019}''}
%\begin{itemize}
%	\item Not adversarial example detection
%	\item Standard vs adversarial risk
%\end{itemize}

\vspace{0.05in}
\noindent
{\bf Measures of Robustness.}
The strength of an adversary is mostly measured by the size of the perturbation required to create an adversarial example.
That is, adversaries that introduce more perturbations, e.g., change a larger portion of pixel values in the image, are considered to be stronger.
A typical way of measuring the perturbation size, especially in the image domain,
is by using the $L_p$ distance metric, where $p$ can be a whole number or $\infty$.
Specifically,
$L_0$ counts the total number of changed features, regardless of the changes to individual features.
$L_1$ is the Manhattan distance, i.e., the sum of absolute values representing a change in each feature.
$L_2$ measures the Euclidean distance between the feature values of the original and perturbed samples.
The $L_\infty$ metric measures the largest change in any of the features
(while disregarding changes in all other features).
%These metrics are predominantly used in the image domain.

There are two ways to utilize these distance metrics to evaluate the robustness of a model: error-rate-based and radius-based.
The first calculates a pool of adversarial samples generated from a set of real samples with a fixed allowable perturbation size~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018}.
The robustness is then defined as the error rate of the model on these adversarial samples.
A related concept, \emph{adversarial risk}, is also defined in a similar manner:
the probability of finding, within a certain predefined distance, an adversarial example for a given real sample.

Radius-based evaluation measures the smallest distance required to generate an adversarial sample from a given real sample~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014}.
%, i.e., the the $L_p$ distance from a given sample to its closest adversarial example.
This way is especially useful in robustness certification, which involves learning a classifier that outputs a prediction along with a certified radius within which the prediction is guaranteed to be consistent~\cite{Lee:Yuan:Chang:Jaakkola:NeurIPS:2019}.
%This certified radius for a given sample is precisely the smallest distance to its closest adversarial example, i.e., a large certified radius implies that the model is more confident as it cannot be fooled by any adversarial examples inside this radius.

%\gx{Depending on the techniques used to generate attacks, one approach may be more natural than the other for measuring robustness.
%When attacks are restricted by the amount of perturbation that can be applied, it is more appropriate to measure the average impact of the attacks on a set of samples.
%While the smallest $L_p$ distances to adversarial examples would be more straightforward when considering attacks that generate adversarial examples through minimal perturbations.}

%More formally: $$\forall r' \le r \quad f(x) = f(x') \quad \text{where } x' = x+r'$$
%\begin{itemize}
%	\item $L_p$ norms
%	\item Radius vs misclassification rate
%	\item Certification
%\end{itemize}


%\gx{Better explain the difference between different attacks (e.g., evasion attacks, poisoning attacks / backdoor attacks) and emphasize that our focus is on attack using adversarial perturbation, based on the exclusion of ``Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning~\cite{Zawad:Ali:Chen:Anwar:Zhou:Baracaldo:Tian:Yan:AAAI:2021}''}
%
%\gx{Can discuss how data properties can influence the generalization in standard setting,  ``Measuring Generalization with Optimal Transport~\cite{Chuang:Mroueh:Greenewald:Torralba:Jegelka:NIPS:2021}'' showed the concentration (i.e., features within a class are well-clustered) and separation (i.e., the classes are separable in feature space in Wasserstein sense) are important factors for standard generalization.}

%\begin{itemize}
%	\item{Definition of $L_p$ Norms}
%	\item{Standard and Adversarial risk}
%	\item Measurement of Adversarial Robustness:
%	\begin{itemize}
%		\item{Empirical Robustness}
%		\item{Certified Robustness}: It is the framework ~\cite{Cohen:Rosenfeld:Kolter:ICML:2019, Lecuyer:Atlidakis:Geambasu:Hsu:Jana:SP:2019} that learns a classifier $f$,  such that for each input $x$, it outputs a prediction label $y$, along with a certified radius $r$ that the prediction is guaranteed to be robust against
%		$$\forall r' \le r \quad f(x) = f(x') \quad \text{where } x' = x+r'$$
%	\end{itemize}
%\end{itemize}

\subsection{Data Distributions}
\label{sec:background_distribution}
Numerous works study properties of particular data distributions, which we discuss below.
%{\bf Data Distributions.} There is a number of common distribution types discussed in the literature.
The \emph{uniform distribution} defines a probability distribution in which every possible data point is equally likely.
This implies that for a continuous random variable in the interval $[a, b]$, the probability of seeing a sample from the interval is $\frac{1}{b-a}$ and the probability of seeing a sample from outside of the interval is $0$.
In the discrete case with $n$ possible values, the uniform distribution assigns a probability of $\frac{1}{n}$ to each value.

The \emph{Bernoulli distribution} defines a discrete probability distribution of a random variable with two allowable values, 0 and 1.
Such a random variable takes the value of 1 with probability $p$ and the value of 0 with probability $1-p$.
%$$P(x) =
%\begin{cases}
%	p & \text{for $x = 1$}\\
%	1-p & \text{for $x = 0$}
%\end{cases}
%$$


\begin{wrapfigure}{r}{0.25\textwidth}
\vspace{-0.35in}
	\centering
	\includegraphics[width=0.2\textwidth]{images/gaussian_mixture_intersect.pdf}
	\vspace{-0.1in}
	\caption{A two-dimensional Gaussian mixture data.}
	\label{fig:gaussian_mixture}
\vspace{-0.2in}
\end{wrapfigure}
The \emph{Gaussian (normal) distribution} defines a continuous probability distribution that assigns a probability with its peak at the center of the distribution and decreasing symmetrically outwards.
%Formally defined with the probability density function as follows:
%$$P(x) = \frac{1}{\sigma \sqrt{2\pi}} exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2)$$
For a Gaussian distribution, $\mu$ denotes the mean or center of the distribution, and $\sigma^2$ denotes the variance or the spread of the distribution.
Since the mean and variance fully characterize a Gaussian distribution, it is also commonly denoted as $\mathcal{N}(\mu,\sigma^2)$.

%\begin{figure*}[b]
%	\vspace{-0.2in}
%	\centering
%	\includegraphics[width=0.3\textwidth]{images/gaussian_mixture_intersect.pdf}
%	\vspace{-0.2in}
%	\caption{A two-dimensional Gaussian mixture data.}
%	\label{fig:gaussian_mixture}
%\end{figure*}


One can also imagine a distribution made up of a \emph{mixture} of multiple distributions.
For example, Fig.~\ref{fig:gaussian_mixture} shows a distribution made up of two Gaussians: one centered at $\mu_1$ and another~-- at $\mu_2$.
This mixture also contains labels associated with each independent Gaussian, shown by the two clusters in the figure.
Furthermore, these two clusters have the same variance,
i.e., the same spread of the  distribution surrounding the center of the class.
While the means of the two classes are separated, the distributions intersect with each other.
%This degree of intersection dictates the hardness of binary classification in such datasets.

%\vspace{0.05in}
%\noindent
%{\bf Data Properties.}
%\jr{suggest to move each definition to its appropriate section in the results.}
%In addition to the type of distribution, there are several data properties that can be measured for any dataset.
%This subsection provides high-level explanations; more formal definitions for each data property, such as concentration, are presented in section \ref{sec:results}.
%%\ref{sec:results-concentration}
%
%\emph{Dimensionality} refers to the number of features in a dataset.
%In the Gaussian mixture model above, since any point $P^i = (x_1^i, x_2^i)$ is expressed using two features, the \emph{dimensionality} of the dataset is 2.
%The data model also contains labels associated for each independent Gaussian shown by the blue and red clusters.
%In classification, this number of possible labels for an input is also referred to as the \emph{number of classes}.
%Such a dataset with only two classes is called a \emph{binary} dataset, on which one can train a \emph{binary classifier}.
%
%Properties of datasets may also affect the hardness of classification tasks.
%One property is the \emph{density} of the distribution, i.e., how much further apart are the samples from each other.
%For a Gaussian, the density of the distribution is accurately characterized by its variance.
%In dense datasets, the data points lie in close proximity to each other which allows models to easily learn their similarities.
%
%The \emph{concentration} of a distribution defines how fast the value of a function defined over a data region grows as the region expands.
%%function grows when expanding between clusters of different classes, i.e., between the blue and red clusters in Figure \ref{fig:gaussian_mixture}, also affects the ease of classification.
%%A typical metric to measure this quantity is the \emph{concentration of measure}~\cite{Zhang:Evans:ICLR:2022}.
%Training a classifier on a dataset with a highly concentrated region between the classes is harder as the model's decision boundary will need to accurately separate the dense regions between the different classes.
%A related property is the $separation$ of a distribution which pertains to the distance between data points of different classes.
%A highly separated dataset implies that data points from opposite classes are largely far apart from each other, and makes classification easier.
%
%Some properties are only meaningful when considering specific domains.
%For example, \emph{frequency} is a data property that is meaningful only for signals, such as images and audios.
%The frequency of an image characterizes the rate of change of the pixel values.
%Patches of an image that contain multiple rapid shifts in color correspond to high frequency regions.
%An entire image dataset may also be evaluated on the average frequency that each image in the datasets exhibits.
%Some other properties of datasets include \emph{asymmetry} which occurs when a dataset appears unevenly distributed towards opposite directions, and class imbalance which occurs when a dataset contains many more samples for one class than other classes.

%============================ End of data properties ============================

%For the multi-variate case with $d$ random variables,  the probability distribution function becomes:
%$$\mathcal{N}(\mu,\Sigma) = \frac{1}{(2\pi)^{d/2}} |\Sigma|^{-1/2} exp(-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)^T)$$
%With $\mu$ remain the center of the distribution, while covariance matrix $\Sigma$ encode the covariance values between pair of variables (i.e., how a variable co-variate with another variable), and the covariance between a variable and itself is its variance.
%When variables are independent, $\Sigma$ will only have non-zero values along the diagonal, which correspond to the variance along the dimension of each variable.
%%\gx{Introduce something regarding the cumulative distribution function for Gaussian}

%================================================
%Based on the type of feedback that accompanies the inputs, learning algorithms can be broadly divided into three.
%\begin{enumerate}
%	\item \textbf{Supervised Learning}:
%	In this setting, both input-output pairs of data are provided to an agent which learns an input-output mapping function.
%	These outputs are also referred to as \textbf{labels}, or \textbf{classes} when they are discrete.
%	\item \textbf{Unsupervised Learning}:
%	In this setting, an agent learns patterns from the input data which does not contain any output information. It is used to learn
%	\item \textbf{Reinforcement Learning}:
%	In this setting, an agent learns from a series of rewards and punishments following a set of actions defined over an environment.
%	The agent's goal is to learn the best policy for interacting with the environment and maximize the cumulative reward.
%\end{enumerate}
%
%Since collecting labeled data maybe expensive in terms of time and human labor, two other types of learning were more recently developed to deal with insufficient labels. These approaches fall between Supervised and Unsupervised Learning.
%\begin{enumerate}
%	\item \textbf{Semi-Supervised Learning}:
%	In this setting, a small amount of labeled data and a large amount of unlabeled data is provided to the learning agent.
%	The main objective of the algorithm is to leverage unlabeled data to train a better performing model than what can be obtained using only the labeled portion.
%	
%	\item \textbf{Self-Supervised Learning}:
%	In this setting, only unlabeled data is provided to the learning agent.
%%~\cite{Grandvalet:Bengio:NeurIPS:2004}.
%	The agent then automatically generates supervisory signals from the data structure itself.
%	This is usually done to either learn a different representation of the data (Representation Learning) or generate labels for a downstream supervised task.
%
%\end{enumerate}
%\mt{We might need to add a table about the types of learning with definitions, and some examples for each.}
%
%
%Based on the agent or model being trained, supervised learning algorithms may be Parametric or Non-Parametric.
%%Source:
%%1. https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/
%%2. Book: Artificial Intelligence - A Modern Approach, Russel et al
%\begin{itemize}
%	\item \textbf{Parametric Algorithms}:
%	These types of algorithms summarize information obtained from data into a fixed set of parameters.
%	The model's size or the number of parameters used does not depend on the amount of data.
%	The size and type of parameters should be chosen according to some assumption about the nature of the data at hand.
%%	A the data population can be adequately modeled by a probability distribution that has a fixed set of parameters.
%	\item \textbf{Non-Parametric Algorithms}:
%	These types of algorithms do not make any assumptions about the data and represent the information using the data points themselves.
%	For non-parametric algorithms the number of parameters used to store information grows with the size of the data.
%	This allows them to learn unrestricted structures from the data.
%\end{itemize}
%\mt{Here again, we might need a table with definitions and representative examples.}
%Furthermore, based on whether the output labels are discrete or continuous, supervised learning can be divided into \textbf{Classification} and \textbf{Regression} respectively.
%
%A list of common classification algorithms: \mt{How formally do we want to define this? I suggest using mathematical formulas using with formal notations.}
%\begin{itemize}
%	\item Decision Trees:
%	\item Bayesian Classifiers:
%	\item K-Nearest Neighbors:
%	\item Linear Networks (Example SVMs):
%	\item Deep Learning Algorithms:
%	\item Bayesian Neural Networks:
%\end{itemize}
%
%Notations: data points ($\mathbf{x}, y$) are drawn according to some unknown distribution $\mathcal{D}$ over the feature-label space $\mathcal{X} \times  \mathcal{Y}$ and $\mathcal{X} \subseteq \mathbb{R}^d$. Let $\mathcal{F}$ be a hypothesis class (e.g., a class of Support Vector Machine, or Neural Network with particular architecture), and $\mathit{l}(f(\mathbf{x}), y)$ be the loss associated with $f\in \mathcal{F}$.  Consider the supervised setting where one has access to $n$ i.i.d training samples drawn according to $\mathcal{D}$, denoted by $(\mathbf{x_1}, y_1), (\mathbf{x_2}, y_2), ..., (\mathbf{x_n}, y_n)$.
%
%\subsection{Other Terminologies}
%Distance metric related:
%\begin{itemize}
%	\item{Optimal Transport}
%	\item{Wasserstein Distance}
%\end{itemize}
%
%
%\noindent Classifier related:
%\begin{itemize}
%	\item{\textbf{Empirical risk}}: the risk of a learning algorithm $f$ estimated over the set of training samples:
%	$$R_n(f) := \frac{1}{n}\sum_{i=1}^n \mathit{l}(f(\mathbf{x_i}), y_i)$$
%	\item{\textbf{Population risk}}: the expected risk of a learning algorithm $f$ over the samples from the population $\mathcal{D}$:
%	$$R(f) := \mathbb{E}_{(\mathit{x}, y) \in \mathcal{D}}[\mathit{l}(f(\mathbf{x}), y)]$$
%	\item{\textbf{Bayes Optimal Classifier}}: is a probabilistic classifier that models the data generating distribution, and applies Bayes Theorem to identify the label with highest probability for new instances.
%	Theoretically, Bayes classifier gives the lowest error rate, however, it is hard to train one in real-life, as modeling the data generating distribution accurately can be computationally intractable.
%	\item{\textbf{Sample Complexity}}: the sample complexity of a learning algorithm represents the minimum number of training samples that it requires in order to achieve a desired performance target.
%	\item{\textbf{Radmacher Complexity}}: the Rademacher complexity of a Hypothesis class $\mathcal{F}$ measures the rate that \emph{empirical risk} converges to the \emph{population risk} for all classifiers from that Hypothesis class.
%	It depends on both the classifier's Hypothesis class $\mathcal{F}$ and the distribution of training data.
%	\item{\textbf{Generalization Bounds}}: In the theory of Statistical Machine Learning, a generalization bound is a statement about the predictive performance of learning algorithm or class of algorithm.
%	Given a learning algorithm trained on a set of labeled instances from the some fixed distribution, the generalization bound gives a lower bound of the expected risks on unseen instances from the same distribution.  (from the book ``Encyclopedia of Machine Learning'')
%	\item{\textbf{VC Dimension}}: is a measure of the capacity of a set of functions,
%	formally it is defined as the maximum number of points that can be correctly separated by the learning algorithm, for all possible assignments of labels.
%	For example, the VC dimension for an linear classifier in 2D space is 3, as it is able to separate 3 points given any combination of labels.
%	VC Dimension is commonly used to determine the minimum number of samples for a learning algorithm to achieve a required performance~\cite{Cullina:Bhagoji:Mittal:NIPS:2018} (i.e., sample complexity), without any assumption on the data distribution.
%	\item{\textbf{Kernel Classifier}}: classifier that uses Kernel function for learning, and a Kernel is kind of function that calculate similarities between samples without projecting the them into the feature space.
%	(e.g. ,  binary concentric data in 2D are not linearly separable in 2D, however one can find the linear plane in 3D to separate them.
%	One can find the hyper plane in 3D using kernel function to calculate the similarity directly using the 2D coordinate, and the new feature from the two, instead of projecting the samples to the 3D space, and then calculate the similarity)
%	\item{\textbf{Randomized Smoothing}} is a technique for certifying adversarial robustness, it outputs, for each instance, a radius $r$ with in which the classifier's prediction is guaranteed to remain constant.
%	It does so by aggregating the predictions over a collection of points sampled by adding Gaussian perturbations (under a perturbation radius) to the input instance~\cite{Cohen:Rosenfeld:Kolter:ICML:2019}.
%	\item Linear Discriminant Analysis (LDA) classifier
%\end{itemize}
%
%
%\noindent Distribution related:
%\begin{itemize}
%	\item {Distributional/Data models}
%	\item{$r$-separated Data Distribution}: is a data distribution where any pair of instances that are from different classes has a distance greater than $2r$
%	\item{Intrinsic Dimensionality}
%	\item{Intrinsic Robustness of a distribution: Use definition 2.2 from \cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019}}
%	\item{Dimensionality}
%	\item{Inter-Class Distance}
%	\item{Inter-Class Variation}
%	\item{Inter-Class Compactness}
%	\item{Mahalanobis Distance}
%\end{itemize}
%
%
%\noindent Others:
%\begin{itemize}
%	\item{Concentration of Measure}
%	\item{L\'evy Families}
%	\item{Semantically Lossless Transformations}
%	\item{Lipschitz constants/continuity}
%\end{itemize}
%
