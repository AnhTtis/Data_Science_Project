\subsection{Dimensionality}
\label{sec:results-dimensionality}

\begin{figure*}[h]
\centering
  \begin{minipage}{0.55\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/dimensionality_overview_3d.pdf}
  \vspace{-0.25in}
  \caption{Dimensionality illustration.}
  \label{fig:dimensionality_illustration}
  \vspace{-0.1in}
  \end{minipage}
  \begin{minipage}{0.44\textwidth}
  \vspace{0.1in}
  \centering
    \subcaptionbox{Actual. \label{fig:intrinsic_dimensionality_original}}{
     \includegraphics[width=0.42\linewidth]{images/dimensionality_original_dim.pdf}
   }
   \subcaptionbox{Intrinsic. 	 \label{fig:intrinsic_dimensionality_intrinsic}}{
    \includegraphics[width=0.42\linewidth]{images/dimensionality_intrinsic_dim.pdf}
    }
  \vspace{-0.05in}
  \caption{Actual and intrinsic dimensionality.}
  \label{fig:intrinsic_dimensionality}
  \vspace{-0.1in}
  \end{minipage}
\end{figure*}

%\begin{figure*}[h]
%  \centering
% % \vspace{-0.1in}
%  \includegraphics[width=0.99\linewidth]{images/dimensionality_vertical_legend_wide_updated.pdf}
%  \vspace{-0.1in}
%  \caption{Papers discussing dimensionality. }
%  \label{fig:dimensionality}
%  \vspace{-0.1in}
%\end{figure*}

\begin{figure*}[h]
  \centering
 % \vspace{-0.1in}
  \includegraphics[width=0.99\linewidth]{images/dimensionality_citation_graph.pdf}
  \vspace{-0.1in}
  \caption{Papers discussing dimensionality. }
  \label{fig:dimensionality}
  \vspace{-0.1in}
\end{figure*}

\emph{Dimensionality} refers to the number of features used to represent the data, e.g.,
features $f_1$, $f_2$, $f_3$ in Fig.~\ref{fig:dimensionality_illustration}.  %, \ldots, f_d$
For illustration purposes, we show a dataset with a dimensionality of three on the left-hand side of the figure and
a dataset with a dimensionality of one on the right-hand side.
% as in Figure~\ref{fig:IntrinsicDimensionality}(b) for three-dimensional data.
\emph{Intrinsic dimensionality} refers to the number of features used in a minimal representation of the data.
Fig.~\ref{fig:intrinsic_dimensionality} shows an example of a case where the intrinsic dimensionality
is smaller than the actual dimensionality:
the samples in Fig.~\ref{fig:intrinsic_dimensionality}a are lying on a three-dimensional ``swiss roll''.
``Unwrapping'' the roll into a plain sheet, as shown in Fig.~\ref{fig:intrinsic_dimensionality}b,
makes it possible to distinguish between the samples using only two dimensions.



Papers studying the relationship between data dimensionality and adversarial robustness are shown in Fig.~\ref{fig:dimensionality}.
We divide them into papers
\roundrect{1} characterizing the hardness of robust generalization due to high dimensionality, % under specific assumptions of the dataset or classifier chosen,
\roundrect{2} suggesting robust model types and configurations for high-dimensional data,
\roundrect{3} discussing the impact of high dimensionality on existing defense techniques, and
\roundrect{4} utilizing dimensionality reduction techniques for improving robustness.
%Several works correlate adversarial vulnerability to high dimensionality of the input data.
%To analyze this phenomenon, some works base their study under specific assumptions about the data distribution while others make assumptions about the classifier or the defense mechanism chosen.

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Dimensionality}.
A number of authors show that adversarial examples are inevitable in high-dimensional space.
Specifically, Gilmer et al.~\cite{Gilmer:Metz:Faghri:Schoenholz:Raghu:Wattenberg:Goodfellow:ICLR:2018} prove
this for a synthetic binary dataset composed of two concentric multi-dimensional spheres
(a.k.a., hyperspheres) in high-dimensional space ($>$100), showing that samples are, on average, closer to their nearest adversarial examples than to each other.
They also prove that the adversarial risk
of a model trained on this dataset only depends on its standard accuracy and dimensionality.
A similar result is shown by Diochnos et al.~\cite{Diochnos:Mahloujifar:Mahmoody:2018},
for a uniformly distributed boolean hypercube dataset, and
Shafahi et al.~\cite{Shafahi:Huang:Studer:Feizi:Goldstein:ICLR:2019}, for unit-hypersphere and unit-hypercube datasets.
\revadd{De Palma et al.~\cite{DePalma:Kiani:Lloyd:ICML:2021} prove that irrespective of the model architecture, 
for a dataset with dimensionality $d$, the perturbation required to fool a classifier is inversely proportional 
to $\sqrt{d}$. This means that it gets easier to generate adversarial examples with an increase in dimensionality.}

%%of the data, and not on the learning algorithm.
%%
%%The authors show that, for a fixed standard accuracy, the upper bound of the perturbation required to generate adversarial examples is inversely proportional to $\sqrt{d}$.
%%Specifically, as dimensionality increases, the perturbation required to generate adversarial examples decreases.
%%derive the relationship among the error rate $\mu(E)$ of resulting model, the dimension $d$ of the dataset, and the adversarial robustness of the model, which is estimated by the average $L_2$ distance $d(E)$ between the samples and their closest error (i.e., misclassified when perturbed).
%%Furthermore, they note that adversarial examples are inevitable for high-dimensional data
%%, which they define as $d \geq 100$,
%%consistently successfully attacking models with high standard accuracy.
%%as the samples are, on average, closer to their closest adversarial examples than to each other.
%%the average distance $d(E)$ between the training samples and their closest adversarial examples to be smaller than the average distance between the training samples.
%%
%%
%Diochnos et al.~\cite{Diochnos:Mahloujifar:Mahmoody:2018} investigate a uniformly distributed boolean hypercube dataset and also show that the number of perturbations required to create an adversarial example depends on dimensionality.
%Specifically, for any classifier trained on such a dataset of dimensionality $d$, at most $\mathcal{O}{\sqrt{d}}$ perturbations are enough to significantly increase its adversarial risk.
%Following these works, Shafahi et al.~\cite{Shafahi:Huang:Studer:Feizi:Goldstein:ICLR:2019} investigate continuous unit-hypersphere and unit-hypercube datasets, also proving that adversarial examples are inevitable in high dimension.
%However, they suggest that class density is even a stronger predictor of adversarial robustness.
%Hence, low-dimensional datasets with high class density are favorable for training robust models.
%% They show that dimensionality and class density are both predictors of the resulting model's adversarial robustness.
%%Specifically, dimensionality negatively affects robustness under a constant upper bound for class density, and the upper bound of class density positively affects robustness under a constant dimensionality.
%% The asymtotic relation: accuracy follows $V_c * exp(-d * \epsilon^2)$ --> dimensionality worsen the adversarial robustness at an exponential rate
%%Their analysis show adversarial robustness to be negatively correlated with the dimensionality of the data distribution when the upper bound of class density is constant. %\gx{removed `only' before when, is it necessary?}
%%It is also shown that robustness is positively correlated with the upper bound of class density for the data distribution when dimensionality is constant.
%%Despite pointing out that dimensionality is a weaker predictor of robustness than class density, the authors find low-dimensional datasets with high class density favorable for training robust model.
%%\gx{They pointed this out in the intro, and also expermented with b-MNIST to show}

%\gx{Model specific analysis}

Another line of work analyzes the effect of dimensionality on the robustness of specific types of classifiers.
In particular,
%\jr{Shubhraneel-P38}
Simon-Gabriel et al.~\cite{Simon-Gabriel:Ollivier:Scholkopf:Bottou:Lopez-Paz:ICML:2019} study feedforward neural networks
with ReLU activation functions and He-initialized weights, %~\cite{He:Zhang:Ren:Sun:ICCV:2015}.
% \de{and satisfy the symmetry assumption, i.e., all nodes in the same layer have equal in-degrees and there are no skip-layer connections.}
%%\gx{I updated it because the symmetry assumption is only required to prove some additional results, the weight initialization scheme is the main assumption for the results regarding dimensionality.}
showing that a higher input dimensionality increases the success rate of adversarial attacks, regardless of the topology of the network.
%Specifically, their work suggests that adversarial damage, which measures the attack success rate on correctly classified samples, increases proportionally with $\sqrt{d}$.
The authors, however, demonstrate that regularizing the gradient norms of the network decreases the impact of the input dimension on adversarial vulnerability, thereby improving model robustness on high-dimensional inputs.
Daniely et al.~\cite{Daniely:Schacham:NeurIPS:2020} study the effect of dimensionality on ReLU networks
with random weights and with layers having decreasing dimensions.
Like Simon-Gabriel et al., the authors prove that the robustness of ReLU networks degrades proportionally to dimensionality.

%\jr{Shubhraneel-P37}
Amsaleg et al.~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021}
focus on $k$-NNs and other non-parametric models that base predictions on the proximity of samples.
The authors use the Local Intrinsic Dimensionality metric to represent the intrinsic dimensionality
in the neighborhood of a particular sample $x$.
The authors build up on the observation that when this metric is high, there are more samples in close proximity of $x$
(as, otherwise, a more sparse neighborhood could be encoded in fewer dimensions).
Thus,
%one can change the neighborhood ranking of samples surrounding $x$ with only a small perturbation,
it is possible to arbitrarily change the neighborhood ranking of the nearest neighbor of $x$ using a small perturbation.
As predictions of proximity-based models are based on the nearest neighbor ranking, the adversarial risks increase in this setup.

All the aforementioned works are also in agreement with a number of papers discussed
in Section~\ref{sec:results-number-of-samples},
i.e., ~\cite{Dan:Wei:Ravikumar:ICML:2020,Bhattacharjee:Jha:Chaudhuri:PMLR:2021,Gourdeau:Kanade:Kwiatkowska:Worrell:JMLR:2021},
which show, in their respective settings, that sample complexity for robust generalization is proportional to dimensionality.
%\jr{TODO: why the last two papers are not in Figure 11? They also talk about dimensionality. Same question for separation.}
% $d$, with a rate of $\sqrt{d}$.


%Amsaleg et al.~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021} show that the perturbation required to change the ranks of nearest neighbors for $k$-NN classifiers is inversely proportional to the Local Intrinsic Dimensionality (LID) of the data.
%LID characterizes intrinsic dimensionality in the neighborhood of a reference sample,
%i.e., a high LID indicates that a large number of samples lie near the reference sample.
%Specifically, they prove that, if the LID around a given sample is high, it is possible to arbitrarily change the order of its nearest neighbors using a small perturbation.
%This is relevant since arbitrarily changing the nearest neighbors of a given sample represents an adversarial attack scenario for $k$-NNs and similar non-parametric models.


%Focusing on intrinsic dimensionality,
%Amsaleg et al.~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021} show that the perturbation required to change the ranks of nearest neighbors for $k$-NN classifiers is inversely proportional to the Local Intrinsic Dimensionality (LID) of the data.
%LID characterizes intrinsic dimensionality in the neighborhood of a reference sample,
%i.e., a high LID indicates that a large number of samples lie near the reference sample.
%Specifically, they prove that, if the LID around a given sample is high, it is possible to arbitrarily change the order of its nearest neighbors using a small perturbation.
%This is relevant since arbitrarily changing the nearest neighbors of a given sample represents an adversarial attack scenario for $k$-NNs and similar non-parametric models.

\vspace{0.05in}
\noindent
\roundrect{2} {\bf Model Selection and Configuration}.
%\jr{Jaskeerat-P41}
Wang et al.~\cite{Wang:Jha:Chaudhuri:ICML:2018} prove that the optimal $k$ for producing robust $k$-NN classifiers depends on the dimensionality $d$ and number of samples $n$ of the given dataset ($k = \Omega(\sqrt{dn \text{ log}(n)})$).
However, they note that for high-dimensional data, the optimal $k$ might be too large to use in practice.
The authors thus focus on improving the robustness of 1-NN algorithms through sample selection,
showing the effectiveness of their approach on %that it improves the robustness of adversarially trained 1-NN on
the \halfmoon, \mnistv, and \abalone datasets.

Yin et al.~\cite{Yin:Kannan:Bartlett:ICML:2019} show that transferring a robust solution found on training data
to test data gets more difficult as the dimensionality of data increases.
However, constraining the classifier weights mitigates this problem.
Specifically, the authors prove that constraining the weights by $L_p$ norm, for $p > 1$, leads to
a performance gap between training and test data that has a polynomial dependence on dimensionality;
when the weights are constrained by $L_1$ norm, the performance gap has no dependence on dimensionality.
\revadd{Li et al.~\cite{Li:Jin:Zhong:Hopcroft:Wang:NeurIPS:2022} rather focus on model configuration.
The authors show that robust generalization in networks with ReLU activation requires  
the network size to be exponential in original and intrinsic data dimensionalities, 
even in the simplest case when the underlying distribution is linearly separable.}
%Furthermore, the same exponential relationship also exists for intrinsic data dimensionality.}

%\jr{Michael-P2}
Carbone et al.~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020} study neural networks,
showing that adversarial vulnerability arises due to the gap between the actual and intrinsic dimensionality,
a.k.a., degeneracy.
The authors show that adversarial example generations in high-dimensional degenerate data can be performed by
using gradient information of a neural network, to move the samples in the direction normal to the data manifold.
As such, example generation exploits the additional dimensions without changing the ``semantics'' of the perturbed sample.
The authors then show that Bayesian Neural Networks are more robust than other neural networks to gradient-based attacks:
due to their randomness, they make gradients less effective for crafting attacks.
%The authors validated their findings using the \mnist and \fmnist~\cite{Xiao:Rasul:Vollgraf:ArXiv:FashionMNIST:2017} datasets.

%<Randomized smoothing> is not robust to <high dimensoinality>
%\jr{Michael-P4}
\vspace{0.05in}
\noindent
\roundrect{3} {\bf Effects of Dimensionality on Defense Techniques}.
High dimensionality also poses challenges to defense techniques that aim to improve robustness.
%, such as,
%randomized smoothing, %which results in certifiably robust classifier by first creating multiple noisy instances of the input and aggregating their classification results,
%adversarial training, and %which learns robust classifier by iteratively optimizing with respect to adversarial examples generated on-the-fly,
%data augmentation. % which improves resulting model's robustness by augmenting the training set.
%
Specifically, Blum et al.~\cite{Blum:Dick:Manoj:Zhang:JMLR:2020} focus on randomized smoothing~--
a technique that improves robustness by generating noisy instances of a (possibly perturbed) sample
and then making predictions for the sample based on an aggregation of predictions for its noisy instances.
The authors show that the amount of noise required to defend against $L_{p}$ adversaries, for $p > 2$,
is proportional to dimensionality.
They further demonstrate that, for high-dimensional images, randomized smoothing indeed fails to generate instances that preserve semantic image information.
In a similar line of work, Kumar et al.~\cite{Kumar:Levine:Goldstein:Feizi:ICML:2020} show that the certified radius decreases
as the dimensionality increases when using randomized smoothing for certifying robustness for a given $L_{p}$ radius.
% a similar result:
%for $p > 2$, the largest $L_{p}$ radius that can be certified with randomized smoothing decreases inversely with $d^{(1/2-1/p)}$ as dimensionality $d$ increases.
%% for $L_1$ the largest radius is O(1/d) and for $L_\infty$ the largest radius is O(d^{(1-1/p)}}

Adversarial training~-- a defense technique that improves model robustness by adaptively training a model
against possible adversarial examples~-- often incurs a trade-off between standard and adversarial accuracy~\cite{Tsipras:Santurkar:Engstrom:Turner:Madry:ICLR:2019, Zhang:Yu:Jiao:Xing:Ghaoui:Jordan:ICML:2019}:
optimizing for high robust accuracy results in a drop in standard accuracy and vice versa.
%%\jr{Gabby-P58} %\jr{Gabby-P15}
%For instance,
Mehrabi et al.~\cite{Mehrabi:Javanmard:Rossi:Rao:Mai:ICML:2021} build up on the work of Javanmard et al.~\cite{Javanmard:Soltanolkotabi:Hassani:COLT:2020},
discussed in Section~\ref{sec:results-number-of-samples}, which 
showed that, for a finite number of training samples, % with the number of samples greater than data dimensionality,
the trade-off between adversarial and standard accuracy improves as the number of samples per dimension increases.
Mehrabi et al. further extend this result for unlimited training data and computational power, observing that,
for an unlimited number of training samples, the trade-off between adversarial and standard accuracy improves as the dimension of the data decreases.

%Different from Javanmard et al., they consider distributional adversarial training, which perturbs the entire training data distribution instead of perturbing individual instances.
%Nonetheless, they observe a similar trend as Javanmard et al.~-- the trade-off improves as the dimension of the data decreases.
%\gx{Should we mention their other non-dimensionality related findings here?}
%In addition, they find feature dependency (i.e., correlation between features) to have a more nuanced influence on the trade-off for both Linear Regression and Binary Classification.
%Specifically, they find that increasing feature dependency worsens the trade-off upto a certain value for feature dependency, i.e., if one increases feature dependency beyond this point, the trade-off starts to improve.


%	\item ``Fundamental Tradeoffs in Distributionally Adversarial Training~\cite{Mehrabi:Javanmard:Rossi:Rao:Mai:ICML:2021}''
%	theoretically studied the fundamental trade-off (given unlimited computational power and training data) between standard and adversarial risk for binary classification with adversarial training.
%	Showed the trade-offs depends on adversary's power, feature dimension, and features correlation.
%	\textbf{Include},  as it quantitatively discusses the trade-off in terms of properties of data (e.g., feature dimension, feature correlations).
%	\textbf{Actionable Item}: this paper cited Javanmard et al.~\cite{Javanmard:Soltanolkotabi:Hassani:COLT:2020}, which discussed on the tradeoffs in adversarial learning for linear regression (currently identified as NotSure), check how their conclusions compare to each other.

%\jr{Shubhraneel-P33}
Data augmentation is another common defense technique that aims to improve robustness of a model by creating
perturbed samples at radius $r$ from a certain subset of original samples in training data.
Rajput et al.~\cite{Rajput:Feng:Charles:Loh:Papailiopoulos:ICML:2019}
prove, for linear and certain nonlinear classifiers, that the number of augmentations required for robust generalization depends on the dimensionality of data, % and constraints on the perturbation radius,
i.e., it is at least linearly proportional to dimensionality for any fixed radius $r$.
Thus, data augmentation becomes more expensive for high-dimensional data.

%no constraint on $r$ --> $d+1$
%$r$ is restricted --> at least linearly proportional to $d$
%$r$ is restricted and the perturbation is smaller than the desired margin --> exponential in $d$
%
%They show that, for linear classifiers on a linearly separable $d$-dimensional dataset, $d+1$ augmented data points suffice to achieve the optimal margin given no constraint on the perturbation radius $r$.
%When the radius of perturbation $r$ is restricted, the number of augmentations required is at least linearly proportional to $d$. %and the number of original samples.
%In particular, if the perturbation is smaller than the desired margin, the number of augmentations required is exponential in $d$.
%The authors also extend their results to non-linear classifiers that assign the same label to all data points within small convex hulls (e.g., nearest neighbor classifiers) on both linearly and nonlinearly separable datasets.

\vspace{0.05in}
\noindent
\roundrect{4} {\bf Reducing Dimensionality}.
%\jr{Gabby-P19}
Following the idea that the gap between the actual and intrinsic dimensionality contributes to adversarial vulnerability,
Awasthi et al.~\cite{Awasthi:Jain:Rawat:Vijayaraghavan:NeurIPS:2020} propose to use Principal Component Analysis (PCA)~\cite{Jolliffe:2002}
%a representation learning technique that reduces dimensionality,
to decrease the dimensionality of data before applying randomized smoothing.
As a result, a larger amount of noise can be injected to perturb samples, thus improving robustness without compromising accuracy.
%The large amount of noise leads to classifier that is less sensitive to perturbations, and hence achieves higher certified accuracy across a wide range of radii.
The authors apply the proposed ideas to image data, showing that the combination of PCA and randomized smoothing is more beneficial than using randomized smoothing alone.
%images can be compressed with little information loss using only one fifth of their original dimension.
%the actual dimension required to represent the information on natural images is much smaller than what is currently used.
%
%This is in line with the concept of degeneracy introduced by Carbone et al.~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}.
%They show that one can compress images with little information loss using only one fifth of its original dimension.
%Based on such an observation, they propose to use Principal Component Analysis (PCA), a representation learning technique that reduces dimensionality, to project data from its original dimension $d$ to a lower dimension $r$ prior to applying adversarial training with randomized smoothing.
%As a result, a magnitude of $\sqrt{d/r}$ larger amount of noise can be injected during the adversarial training process, and a larger radius for certified robustness can be achieved.
%
%Decrease <dimensionality> by <Euclidean to Hyperbolic space transformation> For Heirarchical data
%\jr{Jaskeerat-P39}
Weber et al.~\cite{Weber:Zaheer:Rawat:Menon:Kumar:NeurIPS:2020} show, for hierarchical data,
that changing the representation from Euclidean to hyperbolic space reduces the dimensionality
without sacrificing semantic information embedded in the input data.
%\ad{This, in turn, allows for more efficient training of robust \de{large-margin} classifiers.} \jr{how margins are relevant here. Why robust is not enough.} \gx{Even though their main message is supposed to hold for all classifiers, they only validated with large-margin classifiers. }

%They note that this change in representation is practically a geometrical transformation that reduces the dimensionality of hierarchical data without sacrificing its separability or semantic information, which leads to improved robustness.
%They further provide an algorithm to learn a large-margin classifier in the hyperbolic space.
%Their experiments show that their algorithm achieves superior performance in hyperbolic
%space at significantly lower dimensions. 