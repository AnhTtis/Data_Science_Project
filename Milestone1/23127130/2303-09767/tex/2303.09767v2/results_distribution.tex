\subsection{Distribution}
\label{sec:results-distribution}

\begin{figure*}[h]
\centering
%\vspace{-0.1in}
  \begin{minipage}{0.52\textwidth}
  \centering
  \vspace{0.2in}
  \includegraphics[width=\linewidth]{images/distribution_gaus_bern_illustration.pdf}
  \vspace{-0.22in}
  \caption{Distribution illustration.}
  \label{fig:distribution_illustration}
  \vspace{-0.1in}
  \end{minipage}
  \hspace{0.1in}
  \begin{minipage}{0.44\textwidth}
	\centering
	\vspace{-0.1in}
	\includegraphics[width =\textwidth]{images/distribution_citation_graph.pdf}
	\vspace{-0.25in}
    \caption{Papers discussing distribution.}
  \label{fig:distribution}
  \end{minipage}
  \vspace{-0.1in}
\end{figure*}

\emph{Distribution} refers to a function that encodes how samples lie in space, usually by giving the probabilities of their occurrence in particular regions.
Common types of distributions, such as uniform, Bernoulli, and Gaussian
are introduced in Section~\ref{sec:background_distribution}.
Fig.~\ref{fig:distribution_illustration} shows examples of datasets that follow a Gaussian distribution (left) and a Bernoulli distribution (right).
The term \emph{variance} refers to a measure of dispersion that takes into account the spread of all data points in a dataset.
Specifically, the \emph{variance of a distribution} measures the dispersion of samples from the mean;
\emph{feature variance} measures the dispersion of samples over a particular feature only.
We say that a distribution satisfies \emph{symmetry} when distributions on either side of the mean mirror each other.


Papers that discuss how distribution properties, including variance and symmetry, influence models' robustness
are shown in Fig.~\ref{fig:distribution}.
They can be categorized into:
\roundrect{1} papers showing that model robustness depends on the underlying data distribution,
%Fawzi et al.  Ding et al.
\roundrect{2} papers identifying properties of distributions that improve robustness, and
 %Izmailov et al, Lee et al. , Richardson et al.  and
\roundrect{3} papers introducing techniques to transform distributions into ones that are more optimal for robustness.
% Pang et al. and Wan et al.
%Fawzi et al.  show data distribution with some characteristics makes it impossible for training adversarially robust classifiers,
%Ding et al.  show different distributions results in different adversarial robustness

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Types of Distributions}.
%Some works show that some data distributions are more robust than others.
As discussed in Section~\ref{sec:results-number-of-samples},
Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018} prove, for nonlinear classifiers,
that a mixture of Gaussian distributions incurs higher sample complexity
for robust generalization than a mixture of Bernoulli distributions.
%\jr{TODO: Update figure 13 (schmidt et al. citation number)?}
%\jr{Shubhraneel-P35}
Likewise, Ding et al.~\cite{Ding:Lui:Jin:Wang:Huang:ICLR:2019} show that
a distribution shift alone can affect robust accuracy while retaining the same standard accuracy.
Specifically, the authors prove that uniform data lying on a unit cube results in more robust models than
uniform data lying on a unit sphere.
They further experiment with \mnist and \cifarten datasets,
applying existing semantically lossless transformations, namely \emph{smoothing} and \emph{saturation},
to cause the distribution shift.
The results of this experiment show that robustness decreases gradually when transforming \mnist from
a unit-cube-like to a unit-sphere-like distribution and increases for \cifarten when going the opposite way;
in both cases, the models retain \mbox{their standard accuracy}.

Fawzi et al.~\cite{Fawzi:Fawzi:Fawzi:NeurIPS:2018} study the robustness of data distributions modeled by
a smooth generative model~-- a type of generative model that maps samples from input space to output
space while preserving their relative distances, e.g., to compress data.
% satisfying modulus continuity property~\cite{Anastassiou:Gal:2012}. \jr{?}
The authors show that smooth generative models with high-dimensional input space
produce data distributions that make any classifier trained on this data inherently vulnerable.
The authors conclude that non-smoothness and low input space dimensionality
are desirable when modeling data with generative models.
%This is because the robustness radius is directly proportional to the smoothness of the generative model, irrespective of dimensionality of the input space.
%It follows that, for a high-dimensional input space, the
%robustness radius becomes negligible compared to the magnitude of the inputs, and corresponds to adversarial vulnerability}
%\de{This vulnerability is directly proportional to the dimensionality of the input space, the smoothness of the generative model, among others.}
%\jr{why not in the dimensionality section as well?},
%\jr{because a smooth generative model is not a subject to an attack and thus its input space is not in scope for our survey.}

%The authors prove that any classifier trained on a dataset generated by a smooth generative model with high dimensional latent space  %from which input data points are sampled,
%is susceptible to small perturbations.

\vspace{0.05in}
\noindent
\roundrect{2} {\bf Properties of Distributions}.
Izmailov et al.~\cite{Izmaliov:Sugrim:Chadha:McDaniel:Swami:MILCOM:2018} show that, in a binary classification setting,
features with small variance in both classes and means close to each other
cause adversarial vulnerability.
Moreover, a feature with a small variance in one class can still cause vulnerability
even if the means of this feature in both classes are farther separated
but the second class has a larger feature variance.
Intuitively, that is because models tend to assign non-zero weights to such features,
which can be leveraged by attackers to shift the classification into the wrong class.
That is, even small perturbations in such features can shift data points to another class.
To increase robustness, the authors suggest removing such features, either based on domain knowledge
% (e.g., ignoring the pixels in the peripheral regions of images)
or based on feature evaluation metrics, such as, mutual information~\cite{Shannon:1949}.

%study different cases where a feature may contribute to adversarial vulnerability in a binary classification setting.
%In particular, they show that the features that have small variances and close means among classes cause adversarial vulnerability,  as small perturbations on these features suffice to shift data points from one class distribution to another.
%In addition, they demonstrate that vulnerability can arise from features that have separated means among classes if they have significantly different variances.
%In this case, the class with a smaller feature variance becomes more vulnerable, as equivalent perturbations may induce more significant changes in the feature values for that class, resulting in a higher chance of misleading predictions.
%To increase robustness, the authors suggest removing such features, either based on domain knowledge
%% (e.g., ignoring the pixels in the peripheral regions of images)
%or based on feature evaluation metrics, such as, mutual information~\cite{Shannon:1949}.

Similarly,
Lee et al.~\cite{Lee:Lee:Yoon:CVPR:2020} prove that decreasing feature variance in individual classes
can increase robustness for Schmidt's Gaussian mixtures.
These mixtures have equivalent feature variances for all classes and separated means.
In such a setting, low feature variance implies that the feature has a strong correlation with the class and
perturbing this feature will unlikely result in a vulnerability 
(i.e., will likely result in a semantically-meaningful change).
However, even when features have low variance, if these features are
non-robust~\cite{Ilyas:Santurkar:Tsipras:Engstrom:Tran:Madry:NeurIPS:2019}, i.e., hold no semantic information,
and have a smaller variance in the training data than
in the underlying true population,
%adversarially trained models tend to overfit to them,
they will still cause adversarial vulnerability as adversarially trained models tend to overfit to them.
As a countermeasure, the authors propose a label-smoothing-based data augmentation technique which uses
continuous instead of discrete values for labels and acts like a regularization method that prevents the model from overfitting to such features.
%\jr{features with low variance? or also 2 and 3?
%1) low variance
%2) non robust
%3) different from true population


\begin{wrapfigure}{r}{0.30\textwidth}
  \vspace{-0.35in}
  \begin{center}
    \includegraphics[width=0.26\textwidth]{images/gaussianMixFig.pdf}
    %includegraphics[scale = 0.2]{images/gaussianMixFig.pdf}
  \end{center}
    \vspace{-0.15in}
  \caption{Asymmetrical dataset.}
  \label{fig:asymetric_data}
  \vspace{-0.1in}
\end{wrapfigure}
Richardson and Weiss~\cite{Richardson:Weiss:JMLR:2021} claim that adversarial vulnerability can be caused by
sub-optimal data distributions and/or sub-optimal training methods.
The authors define synthetic binary datasets (of images) that use Gaussian distributions with separated means
and say that a dataset is symmetric if and only if classes have the same variance.
They further prove that even an optimal classifier is non-robust when the underlying dataset has strong asymmetry,
as in the
example in Fig.~\ref{fig:asymetric_data}.
If the dataset is symmetric the optimal classifier is provably robust,
even though a sub-optimal training method can still cause vulnerability when trained on this dataset.


\vspace{0.05in}
\noindent
\roundrect{3} {\bf Transforming Distributions}.
%Pang et al. and Wan et al. proposed techniques to transform the input latent representation to particular distributions that are more likely to results in robust classifiers.
%\jr{Gabby-P22}
%As data may not be suitably distributed for robust generalization, some works suggest techniques for learning more optimal distributions in latent feature space.
Both Pang et al.~\cite{Pang:Du:Zhu:ICML:2018} and Wan et al.~\cite{Wan:Chen:Yu:Wu:Zhong:Yang:TPAMI:2022}
change the latent DNN feature representation to be similar to Gaussian mixtures. % \jr{why not Bernoulli?}.
Specifically, Pang et al. show that,
for Linear Discriminant Analysis (LDA) classifiers trained on Gaussian mixtures, the robustness radius of LDA
is proportional to the distance between the Gaussian centers.
The robustness of LDA is further maximized for symmetric Gaussian mixtures.
The authors thus modify the DNN loss function to create a latent feature representation similar to symmetric Gaussian mixtures
and further replace the last layer of DNN from commonly used Softmax Regression~\cite{Cramer:2002} to LDA.
%  (which is good news because we want to estimate and maximize the robustness radius).
To achieve the desired robustness radius, the authors compute the coordinates of the desired
Gaussian centers (as a function of the number of classes and the dimensionality of the input data)
and feed this data to the loss function.
Departing from the assumption that symmetric Gaussian mixtures are advantageous for the underlying model robustness,
Wan et al. modify the DNN loss function to compute the centers of the Gaussians directly while generating symmetric \mbox{Gaussian feature distributions}.
%The authors show that their method improves robustness by promoting symmetry in the resulting distributions.
%\ad{The authors mention that their loss function promotes symmetry in the resulting Gaussian feature distribution, which has been previously shown to improve adversarial robustness. }
%\jr{if they are not using LDA, why are they doing this? why changing to Gaussian mixtures is a good idea? what is their cool idea that makes it work!}
