\section{Introduction}
\label{sec:introduction}
%Machine learning is important
%
%High accuracy is insufficient. We also want robust models.
%
%Properties that influence robustness.
%
%Our focus is on data and why.
%
%Related work, what it does, what it does not do.
%
%Questions the survey aims to answer.
%
%(related work could also come after we pose the questions).
%
%A summary of the study we did to answer these questions: how we search for papers, how many papers reviewed, etc.
%
%Main findings (which answer the survey questions).
%
%Suggestions for future work.
%
%Summary of contributions. Bullet point list of main achievements.
%
%Paper structure.
%


Recent advances in Machine Learning (ML) led to the development of numerous accurate and scalable ML-based techniques,
which are increasingly used in industry and society.
Yet, concerns related to the safety and security of ML-based systems could substantially impede their widespread adoption, especially in the area
of safety-critical systems, such as autonomous cars.
Examples of fooling ML models into making wrong predictions by adding imperceptible-to-the-human noise to the input
are well known~\cite{OpenAI:report:2017}:
adversarial perturbation to a stop sign may cause a machine learning system to recognize it as a ``max speed'' sign instead,
which might lead to wrong and dangerous actions taken by an autonomous car~\cite{Eykholt:Evtimov:Fernandes:Li:Rahmati:Xiao:Prakash:Kohno:Song:CVPR:2018} (see Fig.~\ref{fig:stop_sign_side}).
%~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Carlini:Wagner:SP:2017}
%~\cite{Eykholt:Evtimov:Fernandes:Li:Rahmati:Xiao:Prakash:Kohno:Song:CVPR:2018, Berkeley:Deep:Drive:Blog:2019}
Likewise, malicious software can be perturbed to bypass security models while still retaining its malicious behavior~\cite{Demontis:Melis:Biggio:Maiorca:Arp:Konrad:Rieck:Corona:Giacinto:Roli:TDSC:2019}.

\begin{figure*}[th!]
    \vspace{-0.05in}
	\centering
	\includegraphics[width=0.45\textwidth]{images/stop_sign_adv_example.pdf}
	\vspace{-0.1in}
	\caption{Adversarial examples for traffic signs (picture by Chen and Wu~\cite{Chen:Wu:Altacognita:Blog:2019}).}
	\label{fig:stop_sign_side}
	\vspace{-0.2in}
\end{figure*}

%\mt{Alternative source from an actual paper: https://jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-020-01775-5}

ML models are susceptible to such scenarios, known as \emph{adversarial attacks} or \emph{adversarial examples}~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014, Goodfellow:Shlens:Szegedy:ICLR:2014}.
%, if not specifically trained for~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014, Goodfellow:Shlens:Szegedy:ICLR:2014}.
To address this problem, recent literature investigates mechanisms behind adversarial attacks and proposes defenses against these attacks -- an area commonly referred to as \emph{adversarial robustness}.
The performance of ML models under adversarial attacks, known as \emph{robust accuracy} or \emph{robust generalization}, is often distinguished from the general model accuracy, known as \emph{standard accuracy} or \emph{standard generalization}.

Adversarial attacks that aim to decrease model accuracy can roughly be divided into \emph{evasion} and
\emph{poisoning attacks}~\cite{Biggio:Roli:PR:2018,Li:Li:Ye:Xu:CSUR:2021}.
The stop sign example above is, in fact, an evasion attack, where the attacker carefully modifies
the input to mislead the prediction~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014,Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Carlini:Wagner:SP:2017}.
Instead of changing model inputs, poisoning attacks are carried out by injecting corrupt data into the training dataset, 
to compromise the integrity of the model~\cite{Goldblum:Tsipras:Xie:Chen:Schwarzchild:song:Madry:Li:Goldstein:TPAMI:2022,Shafahi:Huang:Najibi:Suciu:Studer:Dumitras:Goldstein:NeurIPS:2018, Tian:Cui:Liang:Yu:CSUR:2022}.
\emph{The focus of this survey is on evasion attacks}, as they are more common, accessible, and
more frequently discussed in the literature~\cite{Machado:Silva:Goldschmidt:CSUR:2021, Chakraborty:Alam:Dey:Chattopadhyay:Mukhopadhyay:ArXiv:2018, Viso:ai:blog:2022, Li:Li:Ye:Xu:CSUR:2021}.
As literature often uses the term \emph{adversarial attacks} to refer to evasion
attacks~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Yuan:He:Zhu:Li:TNNLS:2019},
in this survey we use these two terms interchangeably.
%we also use \emph{adversarial robustness} to refer to the resilience of ML models against \emph{evasion attacks}.

Most techniques that study adversarial evasion attacks attribute adversarial vulnerability to various
aspects of the learning algorithm and/or properties of the data.
%Due to the recent popularity and advances in work on this topic, there is also a number of
%existing surveys focusing on adversarial robustness.
Numerous existing surveys on adversarial robustness focus on adversarial attacks and defenses~\cite{Zhou:Liu:Ye:Zhu:Zhou:Yu:CSUR:2022, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021, Li:Li:Ye:Xu:CSUR:2021} and sources of adversarial vulnerability
related to learning algorithms~\cite{Machado:Silva:Goldschmidt:CSUR:2021, Serban:Poll:Visser:CSUR:2020}. 
Yet, to the best of our knowledge, there are no surveys that collect and organize literature 
focusing on the influence of \emph{data} on adversarial robustness.
Our work fills this gap. 
Specifically, {\bf we investigate 
(a) what properties of data influence model robustness and 
(b) how to select, represent, and use data to improve model robustness.}
To the best of our knowledge, this is the first survey to analyze adversarial robustness from the perspective of data properties.
% -- an important direction as the quality of data determines what is achievable through any learning algorithm.

To collect literature relevant to our survey, we used popular digital libraries and search engines, selecting papers that investigate the effect of data on ML adversarial robustness.
We identified more than \revreplace{3,089}{4,359} potentially relevant papers published in top scientific venues on Machine Learning, Computer Vision, Computational Linguistics, and Security.
We systematically inspected these papers, identifying \revreplace{57}{77} papers relevant to our survey. We further analyzed, categorized, and described the selected papers in this manuscript.


\vspace{0.02in}
\noindent
{\bf Main findings} \revadd{(see Section~\ref{sec:results} for details)}. The results of our analysis show that producing accurate and robust models requires a larger \emph{number of samples} for training than achieving high accuracy alone.
The required number of samples to learn a robust model also depends on other properties of the data, such as dimensionality and the data distribution itself. Specifically, input data with higher \emph{dimensionality}, i.e., a larger number of features that represent the input dataset, requires a larger number of samples to produce a robust model.
This is consistent with other findings showing that high \emph{dimensionality} is undesirable for robustness.
Moreover, some data \emph{distributions} are inherently more robust than others, e.g., a Gaussian mixture distribution requires more samples to produce a robust model than needed by a Bernoulli mixture distribution.

Another aspect that affects robustness is the \emph{density} of data samples within classes, which measures how far apart samples are from each other.
Papers show that high class density correlates with high robust accuracy and that adversarial examples are commonly found in low-density regions of the data.
This is intuitive as low-density regions imply that there are not enough samples to accurately characterize the region.
A related property, \emph{concentration}, measures how fast the value of a function defined over a data region, e.g., \emph{error rate}, grows as the region expands.
This concept of concentration tightly corresponds to adversarial robustness if we consider the expansion of a data region as the effect of adversarial perturbation, i.e., perturbing samples in all directions causes the region defined by the original samples to expand.
In this case, high concentration implies that the error rate grows as one perturbs all points in a data region and, thus, datasets with high concentration are shown to be inevitably non-robust.
The \emph{separation} between classes of the underlying data distribution also affects robustness, with a large distance between different classes being desirable for adversarial robustness as an attacker would need to use a large perturbation to move samples from one class to another.

Yet another aspect that impacts robust accuracy is the presence of mislabeled samples in a dataset,
referred to as \emph{label noise}.
Furthermore, refining labels to reason about a larger number of classes, e.g., splitting a class ``animal'' into ``cat'' and ``dog'' may improve adversarial robustness as such labels allow learning more compact representations for samples that share stronger similarities.

A number of papers also identify \emph{domain-specific} properties that correlate with adversarial robustness.
For example, image frequency -- the rate of pixel value change -- affects robustness, and it is advisable to use a diverse frequency range in the training dataset to prevent any frequency biases which give rise to adversarial examples.

%Moreover, as there exist correlations between the properties of the training data and the model's adversarial robustness, some papers propose techniques to transform datasets so that they exhibit some desired properties.
%Increasing the number of samples, reducing dimensionality, increasing separation, and transforming datasets to have robust distributions are among the techniques proposed to improve adversarial robustness.
%\jr{I am still not sure what this paragraph wants to say and where does it belong. Why is it stuck at the end} \gx{I rephrased the beginning to mentioned that these techniques acknowledge and exploit the relationship between the properties of training data and resulting model's robustness.}

\vspace{0.02in}
\noindent
\revadd{{\bf Practical Implications} (see Section~\ref{sec:discussion-practical} for details).
Our survey identifies a number of actionable guidelines and techniques that can be used to improve robustness.
These include data manipulation techniques, such as 
techniques for increasing the number of samples with real or generated data,
feature selection and dimensionality reduction techniques, and 
techniques for learning a latent data representation 
with desirable data properties, such as high density, high separation, and low dimensionality. 
While these techniques aim at changing properties of the underlying data, 
improving robustness can also be achieved by manipulating the learning procedures 
based on the properties of the training data, 
e.g., selecting particular models and/or configuring model parameters based on data dimensionality,
adjusting samples at inference time, and more.
%augmenting training procures to use more refined labels, and more. 
% and inference procedures \jr{how, intuitively?}. 
%add fine labels, sample purification (changing the sample, and look at its prediction, before making classification decision)
A number of approaches also propose ways to estimate robustness guarantees for particular data, 
making it possible to reason about inherent robustness limitations in a practical setup.
}

%It is important to note that data pre-processing techniques such as performing sample-selection to optimize a certain data-property are unfruitful for parametric classifiers.
%The goal is to modify the data property for the underlying data distribution, not just your collected sample. 
%For example, it is easy to choose a subset of training data which is well-separated by some distance metric, but the underlying distribution may still be very close and robustness would not improve. 
%Hence, most work proposing techniques either modify loss functions in classifiers to learn a latent representation enforcing a certain property or use a type of data-preprocessing which changes the underlying distribution such as feature selection.}

\vspace{0.02in}
\noindent
{\bf Knowledge Gaps and Future Research Directions} \revadd{(see Section~\ref{sec:discussion-future} for details)}.
Our literature review shows that, even though most works study data properties from a domain-agnostic perspective, they typically conduct an empirical evaluation on image datasets only.
This constrains the types of attacks and robustness measurements considered, 
and thus findings may not generalize to other domains or types of datasets.
Furthermore, most works base their formal derivations on quite simple synthetic data models, such as uniform distributions, a mixture of Gaussian distributions, and a mixture of Bernoulli distributions,
which exhibit unrealistic assumptions compared to real datasets used in practice.
We also observed that while most papers only perform a univariate analysis on a specific data property,
most properties are hard to independently optimize, e.g., to decrease dimensionality without decreasing separation,
as decreasing the dimensionality implies that samples have fewer features to be differentiated from each other.
%\jr{here the summary is much better and clearer than in the discussion section. Please remove this comment and update the discussion section accordingly.}
We also found that some properties, e.g., separation, do not have a standard way of measurement for concrete datasets.
We believe future work should look into these directions.
% provide more insights on how different aspects of data collectively determine adversarial robustness
%and present more practical and standardized approaches for quantitatively describing different data properties.

\vspace{0.02in}
\noindent
{\bf Contributions.} The main contribution of this survey are:
\vspace{-0.1in}
\begin{itemize}
\item A collection of literature on the effects of data on adversarial robustness.
\item \revadd{A categorization and analysis of data properties that affect adversarial robustness}.
\item \revadd{An analysis of practical implications of the finding}, 
knowledge gaps, and \revreplace{suggestions for}{} future \mbox{research directions}.
\end{itemize}

%\vspace{0.02in}
\noindent
\revreplace{
{\bf Structure of the Survey.} The remainder of the survey is structured as follows.
Section~\ref{sec:background} introduces the necessary background and terminology used in this survey.
Section~\ref{sec:methodology} presents our methodology for identifying and categorizing relevant papers.
We describe the collected literature in Section~\ref{sec:results}.
In Section~\ref{sec:discussion}, we discuss our analysis of the literature, possible knowledge gaps, and suggestions for future work.
Section~\ref{sec:relatedwork} summarizes related work and Section~\ref{sec:conclusion} concludes the survey.
}{}

