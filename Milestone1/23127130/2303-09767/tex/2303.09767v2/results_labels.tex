\subsection{Label Quality}
\label{sec:results-label-quality}

%\begin{figure*}[h]
%\vspace{-0.12in}
%	\begin{minipage}[t]{0.55\textwidth}
%	  %\centering
%	  \includegraphics[valign=t, width=0.95\linewidth]{images/label_noise_illustration.pdf}
%	  \vspace{-0.18in}
%	  \caption{Label noise illustration.}
%	  \label{fig:label_quality_guideline}
%	\end{minipage}
%    \begin{minipage}[t]{0.36\textwidth}
%       \centering
%       \includegraphics[valign=t, width=0.72\textwidth]{images/label_vertical_legend.pdf} %0.41
%      \vspace{0.02in}
%      \caption{Papers discussing label quality.}
%      \label{fig:label}
%    \end{minipage}
%\vspace{-0.2in}%
%\end{figure*}

% New figure
\begin{figure*}[h]
	\vspace{-0.12in}
	\begin{minipage}[t]{0.55\textwidth}
		%\centering
		\includegraphics[valign=t, width=0.95\linewidth]{images/label_noise_illustration.pdf}
		\vspace{-0.18in}
		\caption{Label noise illustration.}
		\label{fig:label_quality_guideline}
	\end{minipage}
	\begin{minipage}[t]{0.4\textwidth}
		\centering
		\includegraphics[valign=t, width=0.9\textwidth]{images/label_quality_citation_graph.pdf} %0.41
		\vspace{-0.04 in}
		\caption{Papers discussing label quality.}
		\label{fig:label}
	\end{minipage}
	\vspace{-0.1in}%
\end{figure*}


\emph{Label quality} refers to the correctness and informativeness of the set of labels assigned to a training dataset.
Label correctness or, inversely, the presence of inaccurate labels (shown as the highlighted dots on the left-hand side of Fig.~\ref{fig:label_quality_guideline}) is typically referred to as \emph{label noise}.
The granularity of the labels is typically referred to as \emph{label informativeness}.
Papers that  discuss the relationship between label quality and model robustness are outlined in Fig.~\ref{fig:label}.

%and the quantity of labels for each input to be associated with model's adversarial robustness.
%Assuming an oracle that record the true labels for all samples, label noise refer to having an arbitrary portion of input samples with incorrect labels.

%\js{Add to Table}
Mao et al.~\cite{Mao:Gupta:Nitin:Ray:Song:Yang:Vondrick:ECCV:2020} show that training a model
simultaneously for multiple tasks, e.g., to simultaneously locate and estimate the distance of objects in images
(an approach also referred to as \emph{multi-task learning}),
%such as semantic segmentation, depth estimation, and object detection
improves robustness.
This is because in multi-task learning, a model learns a shared feature representation by training on data
with labels from several tasks.
As a result, perturbations required to attack multiple tasks at the same time, e.g.,
to sabotage an autonomous driving system by misleading the model in both object identification and distance estimation,
cancel each other out.
While the authors prove that the model robustness to adversarial attacks is proportional to the number of tasks
that it is trained on,
the benefits of multi-task learning disappear when the concurrently trained tasks are highly correlated with each other,
as it reduces the chances for the perturbations to cancel each other.
% \jr{I would expect the reasons to relate to feature representation, not perturbations}. \gx{Here we followed the intuition introduced by the paper.
%One hypothesis we had when discussing this paper was that training with multiple task labels reduce the possibility of picking up spurious signals from data, hence resulting in more robust representation. However, this is not mentioned in this paper.}
The authors further show that training with multiple tasks also improves model robustness against single-task attacks.
%The authors suggest to train models with a diverse set of tasks to better reap the benefit of multi-task learning in creating robust models.

%\jr{Gabby-P172}
\begin{wrapfigure}{r}{0.4\textwidth}
  \vspace{-0.25in}
    \begin{minipage}[t]{0.45\textwidth}
	 \centering
   	 \subcaptionbox{Overfit}{
     \includegraphics[width=0.4\linewidth]{images/label_noise_guideline_overfit.pdf}
     \vspace{-0.15in}
   	 }
     \subcaptionbox{Not overfit.}{
     \includegraphics[width=0.4\linewidth]{images/label_noise_guideline_not_overfit.pdf}
     \vspace{-0.15in}
     }
     \vspace{-0.15in}
     \caption{Influence of overfitting to label noise.}
     \label{fig:label_noise_influence}
    \end{minipage}
    \vspace{-0.15in}
\end{wrapfigure}
Sanyal et al.~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021} hypothesize that label noise and coarse labels are the reasons for adversarial vulnerability.
The authors prove that, given a large training set with random label noise, any classifier that overfits to that set is likely to be vulnerable to adversarial attacks.
This is because overfitting leads to overly complex decision boundaries that leave more room for attacks, as illustrated in  Fig.~\ref{fig:label_noise_influence}.

The authors also demonstrate that adversarial risk increases as the level of label noise increases.
Defense mechanisms, such as early stopping and adversarial training, enhance robustness by preventing models from overfitting to noisy samples.
In the absence of label noise, using coarse labels
(e.g., one label for the entire class of dogs rather than labels for each individual dog breed)
results in ``sub-optimal'' latent feature representations and also contributes to the adversarial vulnerability.
\revadd{A similar result was also shown by Shamir et al.~\cite{Shamir:Safran:Ronen:Dunkelman:ArXiv:2019} 
for high-dimensional data and piecewise linear classifiers, such as DNN with ReLU activation: 
the number of perturbations required to generate successful adversarial examples is proportional to the number of classes, 
making models trained on data with fine-grained labels more robust. }

\revadd{Dong et al.~\cite{Dong:Liu:Shang:NeurIPS:2022} show that label noise is an inherent part of adversarial training 
as labels assigned to the adversarial examples may not always match their ``correct'' labels. 
The authors show that the amount of label noise introduced in adversarial training is proportional 
to the perturbation radius and the confidence of the model prediction for an adversarial example. 
They further propose to mitigate this issue by filtering low-confidence examples generated with a high perturbation radius and 
demonstrate that their approach can achieve higher robust accuracy than in standard adversarial training for 
\cifarten, \cifarhundred, and \tinyset. 

Zhang et al.~\cite{Zhang:Jiang:Hou:Wei:Han:Li:Cheng:TIP:2021} utilize soft labels, 
i.e., labels that capture the probability of a data point belonging to a certain class, 
to learn the relationship between classes. This encourages a model 
to learn representations that group similar samples together, thus 
increasing intra-class density and, as a result, increasing robustness. }
%\jr{check if has any connections with papers in either density or separation. If so, move accordingly.}}
%\js{checked, leaving this summary here}

%Such suggestion align with the one from Mao et al. that learning with multi-task can improve the representaiton learned.


