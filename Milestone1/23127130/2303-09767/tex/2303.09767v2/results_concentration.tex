\subsection{Concentration}
\label{sec:results-concentration}

\emph{Concentration} of a dataset refers to the ``concentration of measure'' phenomenon from measure theory~\cite{Talagrand:1996:AnnalsProbability}.
%In measure theory, concentration of measure refers to the minimum value of a measurable set function over all possible epsilon expansions of a set.
In a nutshell, concentration is the minimum value of a measured function over all valid measurable sets,
after an $\epsilon$-expansion.  %measure in our cases mostly refer to the probability measure
%Concentration of measure refers to the fact that the concentration rapidly grow to cover the whole space after certain expansion parameter.
More formally,
for a metric probability space $(\mathcal{X}, \mu, d)$ with instance space $\mathcal{X}$, probability measure $\mu$, and distance metric $d$,
the concentration function $h$ is defined as: $h(\mu, \alpha, \epsilon) = $ inf$_{A \subseteq \mathcal{X}} \{\mu(A_\epsilon) : \mu(A) \geq \alpha\}$ for any $\alpha \in (0,1)$ and $\epsilon \geq 0$~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019}.
Here $A_\epsilon$ refers to the $\epsilon$-expansion of set $A$, defined as $A_\epsilon = \{ x : d(x, A) \leq \epsilon\}$.
%In a nutshell, concentration of measure captures a `closeness' property for a metric probability space of instances.

\begin{figure*}[h]
  \centering
%  \vspace{0.05in}
  \begin{minipage}{0.40\textwidth}
  \centering
  \vspace{-0.08in}
  \includegraphics[width =0.82\textwidth]{images/concentration_guideline.pdf}
  \vspace{-0.12in}
  \caption{Concentration illustration.}
  \label{fig:concentrationFig}
  \end{minipage}
  \begin{minipage}{0.56\textwidth}
  \centering
  %\vspace{0.25in}
  \includegraphics[width =0.9\textwidth]{images/concentration_vertical_legend_wide.pdf} %0.56
  \vspace{-0.1in}
  \caption{Papers discussing concentration. }
  \label{fig:concentration}
  \end{minipage}
  \vspace{-0.1in}
\end{figure*}

Fig.~\ref{fig:concentrationFig} shows how the concentration of measure phenomenon can be used to determine the classification error after adversarial perturbation.
By modeling the classification error set as measurable set $A$
and adversarial errors from perturbation budget $\epsilon$ as $A_{\epsilon}$,
one can relate the concentration of the data to the minimum adversarial risk for any imperfect classifier with error rate $\mu(A) \geq \alpha$.
%By using classification error rate as the measurable set function, one can formulate the concentration of measure principle to directly model the minimum adversarial risk for imperfect classifiers with error rate at least $\alpha$.
%This is possible as the probability density of the standard error set expanded by perturbation bound $\epsilon$ corresponds to the adversarial error set.
Using this formulation, a dataset being highly concentrated implies that, for some non-zero initial error, the minimum adversarial risk from an $\epsilon$-expansion on the error set is very large.
We refer to such datasets as datasets with low \emph{intrinsic robustness}~--
a measure that represents the maximal achievable robustness for any classifier on a dataset.
%As the derived upper bound is independent of the type of classifier and the learning algorithm, it is also referred to as the `intrinsic robustness' of a dataset.
%Therefore, a dataset is said to be highly concentrated or suffer from concentration, when we can successfully prove or calculate a \emph{low} upper bound on the adversarial robustness i.e., for some initial constant error, the minimum adversarial risk is very large.
%Hence, impossibility results are quite common for highly concentrated datasets.

%Similarly to density, some works study adversarial vulnerability with respect to concentration.
Fig.~\ref{fig:concentration} shows the papers that relate data concentration to adversarial robustness.
They can roughly be divided into:
\roundrect{1} papers discussing the effect of concentration on robustness and
\roundrect{2} papers proposing techniques to estimate robustness through calculating concentrations.
 %the concentration of concrete datasets.

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Concentration}.
%\jr {Jaskeerat-P29}
%mt{Need to explain what transportation-cost inequality means.}
A number of papers prove the inevitability of adversarial examples
%\de{for datasets satisfying certain properties}
using the concentration of measure phenomenon. % \ad{on  hypothetical datasets}.
In particular, Dohmatob~\cite{Dohmatob:ICML:2019} investigates datasets
conforming to uniform, Gaussian, and several other distributions
that satisfy $W_2$ transportation-cost inequality~\cite{Talagrand:1996}.
The author proves that data distributions satisfying such inequality have high concentration,
which results in a rapid robustness decrease, beyond a critical perturbation size~--
a value that depends on the standard error of the classifier and the natural noise level of the dataset, which, in turn,
is defined as the largest variance in the case of Gaussian distribution.
Even though \mnist might not satisfy the $W_2$ transportation-cost inequality,
the author experiments with this dataset, observing a sudden drop in robustness
as the perturbation size increases.
As such, the author suggests that the \mnist dataset may also have high concentration and be
governed by the concentration of measure phenomena.


Mahloujifar et al.~\cite{Mahloujifar:Diochnos:Mahmoody:AAAI:2019} focus on a collection of data distributions with high concentration called L\'evy families~\cite{Levy:1951}, which include unit sphere, unit cube, and  isotropic n-Gaussian
(i.e., Gaussian with independent variables with the same variance).
The authors prove that classifiers trained on such highly-concentrated data distributions admit adversarial examples with perturbation
$\mathcal{O}(\sqrt{d})$  for dimensionality $d$.
This implies that a relatively small perturbation can mislead models trained on these data distributions with high dimensional inputs.
%\de{Both papers show the data distributions studied by Gilmer et al.}~\cite{Gilmer:Metz:Faghri:Schoenholz:Raghu:Wattenberg:Goodfellow:ICLR:2018} %\ad{discussed in Section~\ref{sec:results-dimensionality}}
%\de{and Fawzi et al.}~\cite{Fawzi:Fawzi:Fawzi:NeurIPS:2018}
%%\ad{discussed in Section~\ref{sec:results-distribution}}
%\jr{where are these suddenly coming from? Does not flow.}
%\de{satisfy the conditions above, confirming that adversarial examples are inevitable in these datasets.} \jr{Intuition, not summary.}
%\gx{These two papers described here show that distributions satisfying certain mathematical properties have high concentrations, and thus low robustness in the resulting classifiers. It is difficult to convert those mathematical properties into plain English, so instead, we cited the definition of those mathematical properties and tried to give examples of distributions that satisfy such properties.}
%\jr{what is the story of the removed papers? Are they part of our survey or not? Where are they discusses? Why removed here?}
%\mt{This paper mentions the removed papers and says the distributions they study satisfy the condition assumed above. They are discussed in dimensionality and distribution sections respectively. Removed as it does not flow here and just complicates things.}

\vspace{0.05in}
\noindent
\roundrect{2} {\bf Estimating Robustness Through Concentration}.
Several approaches utilize the connection between concentration and adversarial risk to estimate 
the intrinsic robustness of datasets by calculating their concentrations.
%\jr{Shubhraneel-P36}
Mahloujifar et al.~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019} are the first to propose an approach for estimating
dataset concentration using subsets of samples.
% their proposed approach only for $L_{\infty}$ and $L_2$ distance norm \jr{why important?}
Specifically, the authors propose a technique that searches for the minimum expansion set based on a collection of subsets carefully chosen according to the perturbation norm (e.g., a union of balls for $L_2$ norm).
%The estimate is bounded using a complexity penalty that is based on the subsets chosen.
They prove that
the estimated concentration value converges to the true value for the underlying distribution as the sample size and the quality/representativeness of the chosen subsets increase.
%\de{The complexity parameter, e.g., VC dimension or Rademacher complexity}~\cite{Shalev-Shwartz:Ben-David:2014}\de{, measures the degree to which the selected subsets reflect the underlying distribution and helps quantify the convergence of the estimation.}
The authors apply their approach to estimate the maximum achievable robustness for the \mnist and \cifarten datasets,
observing a gap between the derived theoretical values and values observed empirically by the state-of-the-art models.
%They test their approach by identifying the intrinsic robustness for \mnist and \cifarten where they show that both datasets have much higher maximum achievable robustness than the current state-of-the-art for $L_\infty$ adversaries,
%while the maximum achievable robustness is much higher than the state-of-the-art for only the \mnist dataset in the case of $L_2$ adversaries.

% \textbf{``Improved Estimation of Connection Under $l_p$-Norm Distance Metrics Using Half Spaces''}: \gx{Data property: data concentration}
%\mt{Add to table}


In follow-up work, Prescott et al.~\cite{Prescott:Zhang:Evans:ICLR:2021} propose
an alternative approach to estimate concentration based on half space expansion using \emph{Gaussian Isoperimetric Inequality}
for the $L_2$ norm~\cite{Borell:InvMath:1975}.
The authors further generalize their results to $L_p$ norms, where $p \geq 2$.
Compared with Mahloujifar et al.~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019},
their approach yields higher achievable robustness on \mnist and \cifarten, revealing a larger gap between the theoretical robustness and the state-of-the-art.
%They show such gaps also exist on \fmnist, \svhn datasets.
%In particular, they find the intrinsic robustness of \mnist, \cifarten, \fmnist, \svhn to be higher than the numbers reported by Mahloujifar et al., implying that the gap between achievable robustness and the current state of the art on these datasets is even higher than previously reported.
As the theoretically achievable robustness derived from a concentration perspective is shown to be high,
the authors suggest that factors other than concentration may contribute to this gap.
% in adversarial robustness.  % as the gap they obaserved cannot be explained through by concentration, however, they don't provide concrete suggestions regarding the other possible factors

%\js{Add to Table}
Zhang and Evans~\cite{Zhang:Evans:ICLR:2022} assume access to information about label uncertainty,
i.e., a function that assigns the level of label uncertainties for any data point.
Such a function can use, e.g., labeling results from multiple human annotators or confidence scores from an ML classifier.
The authors suggest that considering regions with high label uncertainty can guide the concentration estimation
as these are the regions where a classifier is more likely to make mistakes and be vulnerable to attacks.
They thus propose an approach to estimate concentration by identifying the smallest set after
$\epsilon$-expansion with an average uncertainty level greater than a pre-set value. % $\gamma$.
The evaluation results show that the maximum achievable robustness estimated with their approach
is closer to the robustness values observed for CNN models on the \cifarten dataset than in
any of the aforementioned works, implying that the
room for improvement is smaller than assumed earlier.
%The authors further suggest that abstaining from making prediction on samples from uncertain regions,
%has a potential approach for improving model robustness.
