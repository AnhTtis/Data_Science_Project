\subsection{Number of Samples}
\label{sec:results-number-of-samples}

\emph{Number of samples} simply means the quantity of samples available in the training dataset.
For the example in Fig.~\ref{fig:samples_guideline}, where circles represent training samples for a two-class dataset,
the left dataset has fewer samples than the right dataset.

The term \emph{sample complexity} refers to the number of training samples required to achieve a certain model performance,
e.g., 90\%, in terms of either robust or standard generalization.
Then, \emph{sample complexity gap} refers to the difference in the number of samples required to achieve the same model performance for robust generalization as for standard generalization.

\begin{figure*}[t!]
  \centering
  %\vspace{-0.25in}
  \includegraphics[width=0.5\linewidth]{images/number_of_samples_overview_no_distr_info.pdf}
  \vspace{-0.2in}
  \caption{Number of samples illustration.}
  \label{fig:samples_guideline}
  \vspace{-0.1in}
\end{figure*}


\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.9\linewidth]{images/number_of_samples_citation_graph.pdf} %samples
	\vspace{-0.1in}
	\caption{Papers discussing the number of samples.}
	\label{fig:samples}
	\vspace{-0.15in}
\end{figure*}


Papers studying the relationship between the number of training samples and the robustness of the resulting model
are shown in Fig.~\ref{fig:samples}. They can roughly be divided into
\roundrect{1} papers discussing sample complexity for robust generalization,
\roundrect{2} papers proposing techniques to resolve the sample complexity gap between the number of samples required to achieve the same level of robust and standard generalization, and
%for achieving robust generalization, and
\roundrect{3} papers proposing techniques to deal with data imbalance, i.e., an unequal number of samples in different classes.



%Figure~\ref{fig:samplesTable} \st{ schematically shows how one can identify the \emph{number of samples} and \emph{dimensionality} from the input vector.
%Conventionally, each record in the row of the input vector corresponds to one sample, and each column corresponds to one feature.
%The \emph{number of samples} of a training set is simply the total number of records in the input vector.
%}
%\jr{Each figure in the paper should be references somewhere from the text or removed. Probably true for all sections. Also, why dimensionality  is in this figure (when it belongs to the next section)? What is the message here?
%Why the next section does not address the dimensionality in this figure and shows something completely different? Is there a way to simplify/unify? The easiest would probably be to draw a dataset with dots and say that this is a number of samples. } \mt{Remove this paragraph and the table below, and add the one at the top.}
%
%\begin{figure*}[th!]
%	\centering
%	\includegraphics[scale = 0.6]{images/NumFig.pdf} %samples
%	\caption{\st{The number of samples of a dataset.}}
%\label{fig:samplesTable}
%\vspace{-0.1in}
%\end{figure*}


%\vspace{0.05in}
\noindent
\roundrect{1} {\bf Sample Complexity}.
Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018} observe that the number of training samples required for robust generalization is larger than the number of samples required for the equivalent-level standard generalization, i.e., that there exists a \emph{sample complexity gap} between the standard and robust generalization.
Specifically, for linear classifiers trained on a mixture of Gaussian distributions
(referred to as \emph{Schmidt's Gaussian mixture} in the remainder of this paper),
the authors prove that standard generalization requires a constant number of samples while
equivalent-level robust generalization requires a number of samples proportional to the data dimensionality
($O(\sqrt{d})$).
The gap in sample complexity persists for this data distribution in nonlinear classifiers as well.
Yet, the sample complexity gap disappears for nonlinear classifiers trained on a mixture of Bernoulli distributions;
these distributions also need substantially fewer samples than Gaussian mixtures.
The authors conclude that sample complexity for robust generalization depends on the distribution,
even when the same type of classifiers is considered.
Their experimental validation with the \mnist~\cite{MNST:1998}, \cifarten~\cite{CIFAR:ten:hundred:2009}, and \svhn~\cite{SVHN:dataset:2011} image datasets shows that \mnist, which is closer to a Bernoulli mixture, indeed requires a smaller number of training samples to achieve a reasonable robust generalization %in an adversarially trained setting
than the \cifarten and \svhn datasets, which are \mbox{closer to a Gaussian mixture}.

%\mt{Michael-P12}
In follow-up work, Dan et al.~\cite{Dan:Wei:Ravikumar:ICML:2020} provide reasons for why robust generalization
requires more samples than standard generalization, focusing, again, on Gaussian mixture distributions.
%, including Schmidt's mixture.
%by characterizing the lower bound of the adversarial risk on the Gaussian mixture data studied by Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}.
Departing from the Signal-to-Noise ratio (SNR) metric that is based on the distance
between two Gaussian distributions and is known to capture the hardness of standard classification,
the authors propose a new Adversarial SNR (AdvSNR) metric,
defined as the minimum SNR for standard and adversarially perturbed data,
to capture the hardness of robust classification.
%They then show that two Gaussian mixture distributions with the same SNR can have drastically different AdvSNRs and, thus, different adversarial risks. \jr{This sentence is relevant in the distribution section, but not here. Remove or move.}
They then show that, given a dataset of a particular dimensionality,
the number of samples required to achieve the theoretically optimal, accurate classifier is
inversely proportional to SNR.
Likewise, the number of samples required to achieve the theoretically optimal, robust classifier
is inversely proportional to AdvSNR.
Because AdvSNR is never greater than SNR for a given dataset, it follows that
achieving the same robust generalization as standard generalization requires at least the same amount of samples.

Bhattacharjee et al.~\cite{Bhattacharjee:Jha:Chaudhuri:PMLR:2021} study the sample complexity gap for linear classifiers,
as a factor of data dimensionality (the number of features representing samples ) and 
separation (the distance between samples from different classes).
The authors show that the sample complexity gap is directly proportional to the dimensionality of the data when
the allowed perturbation radius of adversarial samples is similar to the distance between classes.
%That is, in this setup, the higher the dimensionality of the data, the more samples are needed for robust generalization.
However, such a gap no longer exists in well-separated data, when the perturbation radius is much smaller than 
\mbox{the distance between classes}.
%\jr{TODO: update the table and the graphs in dimensionality and separation}

%\gx{Their algorithm is based on the intuition that max-margin classifiers rely on the samples closest to the opposite class to pin-point the decision boundary}
%\gx{studies $l_p$ for $p > 1$ attacks which include $l_\infty$}

%\jr{Shubhraneel-P34}
Similarly, \revadd{Gourdeau et al.~\cite{Gourdeau:Kanade:Kwiatkowska:Worrell:JMLR:2021, Gourdeau:Kanade:Kwiatkowska:Worrell:IJCAI:2022}} show that,
for simple classifiers based on feature conjunctions and $\alpha$-log-Lipschitz distributions laying on a boolean hyper-cube, 
the sample complexity is a function of the data dimensionality $d$ and the adversarial perturbation budget.
Specifically, when the adversarial perturbation size is bounded by $log (d)$, the sample complexity is polynomial to the dimensionality;
when the perturbation size is at least $log (d)$, the sample complexity becomes superpolynomial to dimensionality.
%%%\gx{$O(\cdot)$ denotes upper bound (at most), $\Omega(\cdot)$ denotes lower bound (at least)}.}
Javanmard et al.~\cite{Javanmard:Soltanolkotabi:Hassani:COLT:2020} focus on adversarially-trained linear regression models
for standard Gaussian distributions.
The authors show that, when the number of samples is greater than the data dimensionality,
there exists a trade-off between adversarial and standard risks.
Moreover, this trade-off improves as the number of samples per \mbox{dimension increases}.
%d > n: no tradeoff
%d < n: tradeoff, gets better as you increase n
%%\jr{Jaskeerat-P26}
%Adding on to the analysis, Bhattacharjee et al.~\cite{Bhattacharjee:Jha:Chaudhuri:PMLR:2021} explore the number of training samples required to learn a robust linear classifier for well-separated data, where there exists a perfectly accurate and robust classifier. \jr{how is that established?}
%They show that the sample complexity for such data setting is linearly dependent on the dimensionality of the data.
%They further show that, in this setting, when the maximum perturbation is comparable to the separation between classes \jr{?}, the adversarial risk lower bound is $\mathcal{O}(d/n)$, where $d$ is the dimensionality and $n$ is the number of samples in the dataset.
%However, the adversarial risk lower bound is only dependent on the number of samples as $\mathcal{O}(1/n)$ when the data is very well-separated i.e. the maximum perturbation is much smaller than the separation between the classes.
%\jr{what is the message, in simple language? Also, do they also study the Gaussian mixture?}
%%Finally, the authors prove that when the data is very well-separated i.e. the robustness radius is much smaller than the margin between the classes, the adversarial risk lower bound is only dependent on the number of samples as $\mathcal{O}(1/n)$.
%%\gx{changed `robustness loss' to `adversarial risk' in the following text, can change it back if incorrect}

%%\jr{Shubhraneel-P34}
%Gourdeau et al.~\cite{Gourdeau:Kanade:Kwiatkowska:Worrell:JMLR:2021}, study the feasibility of robust classification with finite number of samples for data on the boolean hypercube \jr{what kind of data is that? how is it related to
%Bernoulli and Gaussian}.
%They first show that in a distribution-agnostic setting, with the number of samples polynomial in the input dimension,
%%i.e., $\mathcal{O}(d)$ where $d$ is the input dimension,
%no classifier can be robust against attacks that modify at least one bit of the input.  \jr{aren't these all attacks?}
%For the uniform distribution in particular, monotone conjuctions, which is a type of classifiers comprising of boolean expressions, cannot learn to be robust against adversary that can perturb $\omega(\text{log}(d))$ bits with $\mathcal{O}(d)$ number of samples.
%However, when the distribution is $\alpha$-log-Lipschitz, a property that constraints the smoothness of the distribution, $\mathcal{O}(d)$ number of samples is sufficient for monotone conjunctions to learn to be robust against adversary that can perturb $\mathcal{O}(\text{log}(d))$ bits.
%\jr{the message is lost.}

Cullina et al.~\cite{Cullina:Bhagoji:Mittal:NeurIPS:2018} give an upper bound on the number of samples needed
for robust generalization for the binary classification problem with linear classifiers in a distribution-agnostic setup,
with $L_p$ norm-bounded adversaries.
% (the setup also used by Schmidt et al.),
The authors derive the upper bound using the classifier VC dimension~\jr{cite}--
a common measure of the capacity and the expressive power of the classifier, shown earlier
to be useful to determine the upper limit of sample complexity for standard generalization~\cite{Shalev-Shwartz:Ben-David:2014}.
They show that the VC dimension for learning adversarially-robust models remains the same as that for learning accurate models, which means that the upper bound of sample complexity is identical for standard and robust generalization
in this setup.
%However, while the upper bound is the same, the actual sample complexity also depends on the learning algorithm used, hence, this doesn't contradict the sample complexity gap shown by Schmidt et al.
However, the authors demonstrate that this conclusion does not generalize to other types of classifiers and types of adversaries.
%\gx{The key purpose of the last sentence is to say that, the conclusion regarding the upper limit of sample complexity remain the same for standard and robust generalization is only valid for linear classifiers with $L_p$ norm bounded adversaries.}

\revadd{Similarly, Montasser et al.~\cite{Montasser:Hanneke:Srebro:NeurIPS:2022} study binary classifiers
constructed using %global one-inclusion graph, which is based on 
the one-inclusion graph algorithm~\cite{Haussler:Littlestone:Warmuth:IC:1994}. 
They show that both the lower and the upper bound of sample complexity is finite, i.e., 
one can achieve robust generalization using a finite number of samples in this setup.}

\revadd{
Xu and Liu~\cite{Xu:Liu:NeurIPS:2022} study sample complexity bounds in a multi-class setup. 
As VC dimension is defined only for the binary case, the authors propose 
Adversarial Graph dimension and Adversarial Natarajan dimension metrics, 
which extend their corresponding counterparts, 
Graph dimension~\cite{BenDavid:CesaBianchi:Haussler:Long:JCSS:1995} and Natarajan dimension~\cite{Natarajan:ML:2004}, 
commonly used in multi-class learning. 
The authors show that sample complexity is upper-bounded by the former and lower-bounded by the latter metric. 
%lower bounded by the Adversarial Natarajan dimension and upper bounded by the Adversarial Graph dimension.
}
 
\vspace{0.05in}
\noindent
\roundrect{2} {\bf Resolving the Sample Complexity Gap}.
As the number of labeled samples required to achieve robust generalization could be large and not readily available,
researchers explore cheaper alternatives, such as, unlabeled data and generated (fake) data.
%several authors propose techniques for using unlabeled data or generated labeled data to bridge the sample complexity gap.
%The former approaches
%are mainly motivated by the observation that adversarial robustness of image classifiers mostly depends on the smoothness of the decision boundary around natural images~\cite{Zhang:Yu:Jiao:Xing:Ghaoui:Jordan:ICML:2019}, which can be estimated through samples in close neighborhood.
%
%\jr{Jaskeerat-P28}
Uesato et al.~\cite{Uesato:Alayrac:Huang:Stanforth:Fawzi:Kohli:NeurIPS:2019} and
Carmon et al.~\cite{Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019}
concurrently proposed to use pseudo-labeling~\cite{Scudder:ITIT:1965}~--
a process of assigning labels to unlabeled samples using a classifier trained on a set of labeled samples,
assessing the effectiveness of their approaches on Schmidt's Gaussian mixture.
The main result of both works is that closing the sample complexity gap requires a number of unlabeled samples
proportional to the dimensionality of the data, albeit with a higher quantity than for the labeled samples,
likely due to the ``noise'' in generating labels.
The main difference between the works is that while Uesato et al. show that in their setup (a specific linear classifier)
the quantity of the required unlabeled samples only depends on data dimensionality,
Carmon et al.~\cite{Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019} use a less restrictive setup and show that the quantity of unlabeled samples also depends on the original sample complexity for standard generalization.
Both of these works empirically evaluate the effectiveness of their proposed approaches on the \cifarten and \svhn datasets showing that unlabeled data could be a much cheaper alternative to labeled data for enhancing the robustness of models.
%
%Carmon et al. prove that, on the Gaussian mixture data model introduced by
%Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}, their proposed approach can close the sample complexity gap using a number of unlabeled samples proportional to the dimensionality of data and the sample complexity needed for standard generalization.
%Carmon et al.~\cite{Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019} formally investigate the Gaussian mixture data model introduced by
%Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}, showing that their proposed approach can indeed close the sample complexity gap for this case and that the number of unlabeled samples
%required to achieve high robust accuracy is
%proportional to both the dimensionality of the dataset and the number of samples needed for standard generalization.
%The authors then empirically test RST on \cifarten and \svhn datasets and show that utilizing data labeled by RST
%makes it possible to achieve high robust accuracy using the same number of labeled samples needed for high standard accuracy.
%Uesato et al. also work on this data model to show that their technique can reduce the sample complexity gap for training linear models.
%They prove that one labeled sample, along with $\mathcal{O}(\sqrt{d})$ unlabeled samples with pesudo-labels assigned by their technique, can have equivalent effect as $\mathcal{O}(\sqrt{d}/\text{log}(d))$ labeled samples.
%
%They further complement their theoretical findings on \cifarten and \svhn datasets where they show that, treating the datasets as largely unlabeled, their method achieves 95\% of the robust generalization possible with the labeled version of the datasets. This shows that unlabeled data could be a much cheaper alternative to labeled data to enhance robustness of models.

%\jr{Gabby-P50}
Najafi et al.~\cite{Najafi:Maeda:Koyama:Miyato:NeurIPS:2019}
%also present an approach to use unlabeled samples to fill the sample complexity gap.
%They first
note that the biggest risk of using a mixture of labeled and unlabeled datasets for learning adversarially robust models is
the uncertainty in sample labels.
Given an estimate of the quality of pseudo-labels,
the authors derive the minimum ratio between labeled and unlabeled samples required to avoid the additional
adversarial risk induced by label uncertainties. % in robust generalization.
%Unlike Carmon et al. and Uesato et al., their theoretical analysis is applicable to all data distributions.

%Given a dataset and a measure of the pseudo-labels quality, they estimate the right ratio between labeled and unlabeled samples, needed for robust generalization.
%This ratio is estimated as a function of the
%is the ratio of labeled to unlabeled samples and  required so that the lower bound of the adversarial risk estimated on a partially labeled training data generalizes to the test data.
%%To limit this risk, they present the concept of \emph{Minimum Supervision Ratio}, a (MSR) and formally prove that when the ratio of labeled to unlabeled samples is greater than that value, the lower bound of the adversarial risk estimated on partially labeled training data generalizes to the test data.
%They note that MSR increases as the desired margin for generalization increases and decreases as the pseudo labels assigned to the unlabeled samples become closer to the ground truth.

%{Michael to decide if this paper is a technique to generate samples (means that we do not include it in the survey) or a first class citizen paper (if they relate their method to a specific data property)}
%\mt{Michael-P17}

Instead of using unlabeled data, which might also be hard to find,
Gowal et al.~\cite{Gowal:Rebuffi:Wiles:Stimberg:Calian:Mann:NeurIPS:2021} suggest
using Generative Adversarial Networks (GANs) to generate labeled data.
The authors show that GANs are more effective than other methods, e.g., image cropping, when
producing additional samples.
This is because such models result in a more diverse dataset, which is beneficial for increasing robust accuracy.
Using images from \cifarten, \cifarhundred~\cite{CIFAR:ten:hundred:2009}, \svhn, and \tinyset~\cite{Torralba:Fergus:Freeman:TPAMI:TinyImageSet:2008}, the authors show that their proposed approach can significantly increase robust accuracy without the need for additional real samples.
%Unlike pseudo-labeling though.
%The authors also show that their method is more complimentary to the training dataset than other data augmentation methods by measuring how different the generated samples are compared to the samples already present in the training dataset.

\revadd{
Xing et al.~\cite{Xing:Song:Cheng:NeurIPS:2022} reason about the effects of real unlabeled and 
generated data on robustness of adversarially trained models. 
As real data is more informative for building decision boundaries, and as 
both real unlabeled and generated data need pseudo-labeling, 
the difference between real unlabeled and generated data boils down to the quality of data generators. 
Following this reasoning, the authors propose a strategy that assigns lower weights to the loss from generated samples compared to real samples during adversarial training, 
with the exact weights (i.e., the representation of the generator quality) being determined through cross-validation.}

\vspace{0.05in}
\noindent
\roundrect{3} {\bf Effects of Data Imbalance}.
Wu et al.~\cite{Wu:Liu:Huang:Wang:Lin:CVPR:2021} analyze the adversarial robustness of DNNs on long-tail distributions: setups where the training data contains a large number of classes with few samples.
They show that robust generalization is harder to achieve on such distributions and compare the performance of
multiple adversarially trained classifiers that use learning algorithms specifically designed for such setups.
%\gx{maybe add more cite info regarding the classifiers they tried}
The comparisons show that scale-invariant classifiers~\cite{Wang:Wang:Zhou:Ji:Gong:Zhou:Li:Liu:CVPR:2018,Pang:Yang:Dong:Xu:Zhu:Su:NeurIPS:2020} result in higher robust accuracy as they
avoid assigning smaller weights to minority classes, which, in turn, promotes robust generalization by reducing bias in the decision boundary.
%\gx{They experiment by integrating adversarial training with classifiers that are specifically design for long-tailed distributions (i.e., long-tailed recognition techniques), and show that scale-invariant classifier is the most promising technique to overcome bias in decision boundaries caused by data imbalance.}

\revadd{
Both Wang et al.~\cite{Wang:Xu:Han:Xiaorui:Yaxin:Thuraisingham:Tang:ArrXiv:2021} and 
Qaraei et al.~\cite{Qaraei:Babbar:ML:2022}
propose to use re-weighted loss functions to improve robustness of under-represented classes.
Specifically, Wang et al. show that, 
for Gaussian mixture distributions, the robustness gap between classes depends on the amount of imbalance and the overall separation of a dataset. %Should we add to separation?
The authors thus propose modifying the loss function to assign weights correlated with data imbalance
while also promoting separation. 
Qaraei et al. focus on extreme multilabel text classification, where the output space is extremely large and the data follows a strongly imbalanced distribution. 
They also recommend using re-weighted loss functions to mitigate the robustness issue of the under-represented classes. 
}

%, in which model weights have similar scale for features from different classes,
%Building on their insights on scale-invariant classifiers, the authors develop a technique to modify training and inference procedures, demonstrating that their approach outperforms other adversarial training methods on long-tailed versions of \cifarten and \cifarhundred datasets with different imbalance ratios.
%The proposed technique is also shown to be effective for cases where labeled samples for only some classes are missing.


%For each generated sample, they first find the real sample that has the most similar representation. Then, they check if the identified real sample is from the training or testing dataset. If the closest real sample to a generated sample is in the testing dataset, this implies that the generated sample is complimentary to the training dataset.

%textbf{``Adversarial Robustness under Long-Tailed Distribution''}: \gx{Data property: imbalance data}
%\mt{Add to table}

% I still think this should be a category different from distribution based properties, as long tail distribution here refer to the distribution of the number of samples, instead of the actual distribution of the data.
% The data can have gaussian distribution for each class, while overall the count of samples from different classes exhibit a long-tail distribution.


%\js{Questions:}
%\begin{itemize}
%	\item  does this paper do any significant research on the gap itself which needs to be part of the summary?
%	\item We need a sentence about the sample complexity gap since there is another paper which cites that portion
%\end{itemize}

%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: Any; Classification Type: Binary;  Targeted Data Distribution: Gaussian mixture and Bernoulli mixture
%	\item \textbf{Problem Setup}: Classifier Type: Linear and non-linear; Training Procedure: N/A ; Definition of Robustness: Expected misclassification rate; Attack: \{Perturbation Bound: $L_{\infty}$;  Attacker's Knowledge: White-box; Technique:any\}
%	\item \textbf{Type of Evidence}: Formal and Empirical
%	\item \textbf{Actionable}: No
%	\item \textbf{Empirical Validation}: \{Datasets: \mnist, \cifarten and \svhn; Attack: PGD; Classifier: DNN-based; Training Procedure: Adversarial; Definition of Robustness: Misclassification based\}
%\end{itemize}
%\gx{if the formula is short, maybe you could direct add the formula here and then describe the terms in the formula?}
%Standard risk lower bound has a similar formula with the only difference being the substitution of AdvSNR with SNR.
%Since AdvSNR is always less than SNR by definition, one has to increase the number of samples to achieve an adversarial risk lower bound equal to some standard risk lower bound.
%This explains the need for more training samples to achieve robust classification.


%They first show that low SNR correlates to a higher number of samples required to achieve optimal accuracy.
%They then prove that adversarial perturbation decreases SNR, thereby increasing the number of samples required for high robust accuracy, compared to standard accuracy which does not have adversarial perturbations.
%Finally, to give robustness bounds that depend on data, the authors also propose a new metric called Adversarial Signal-to-Noise Ratio (AdvSNR) that measures the hardness of robust classification.

%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: any; Classification Type: any;  Targeted Data Distribution: Gaussian mixture
%	\item \textbf{Problem Setup}: Classifier Type: Any; Training Procedure: N/A ; Definition of Robustness: Expected misclassification rate; Attack: \{Perturbation Bound: $L_{\infty}$;  Attacker's Knowledge: White-box; Technique:any\}
%	\item \textbf{Type of Evidence}: Formal
%	\item \textbf{Actionable}: No
%	\item \textbf{Empirical Validation}: None
%\end{itemize}

%\js{Questions:}
%\begin{itemize}
%	\item  The first line says "in a more general setting", how is it a more general setting if all there results are for the same gaussian mixture setting?
%	\item add line near shafahi et all and connect dimensionality correlation
%\end{itemize}	
%	

% as this upper bound only depends on VC dimension.
%However, in general, the adversarial VC dimension can be either smaller or greater than standard VC dimension depending on the learning algorithms, and the adversary.

% while Schmidt et al. show the sample complexity to be at least the same as standard generalization, and in the case of Bernoulli data models the sample complexity is $\sqrt(d)$ greater for robust generalization, where $d$ is the dimension of input}
%\gx{Need to double check with Michael on the differences identified identified above.}

%	\item ``PAC-Learning in the Presence of Adversaries~\cite{Cullina:Bhagoji:Mittal:NeurIPS:2018}"
%	derived an upper bound for sample complexity in a distribution-agnostic manner for binary classifiers.
%	\textbf{Include for now}, as it derive sample complexity for adversarially robust generalization.
%	\textbf{Actionable Item}: the derived bound is a function of VC dimension, which is a complexity measure for the hypothesis class. Better understanding on it to decide how much of this paper is data-dependent.

%\js{Questions:}
%\begin{itemize}
%	\item  PAC-learning framework, corrupted hypothesis class, adversarial VC dimension unclear.
%	\gx{I updated the summary to include a brief introduction on the PAC-learning framework,  corrupted hypothesis class.
%		And the introduction on VC dimension is included in the background section under ``classifier-related'', I was not sure whether we also want to introduce that concept here as I assumed it could also be used in other papers. }
%\end{itemize}

%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: Any; Classification Type: Binary;  Targeted Data Distribution: Any
%	\item \textbf{Problem Setup}: Classifier Type: Linear and non-linear; Training Procedure: N/A ; Definition of Robustness: error rate based; Attack: \{Perturbation Bound: $L_p$;  Attacker's Knowledge: White-box; Technique:any\}
%	\item \textbf{Type of Evidence}: Formal
%	\item \textbf{Actionable}: No
%	\item \textbf{Empirical Validation}: N/A
%\end{itemize}

% This entire sumnary is vague but should fit under number of samples data property
%\mt{Questions:}
%\begin{itemize}
%	\item Up on further reading, I am not sure how accurate the second sentence is as I could not find it in the paper. @Shubhraneel
%	\item Entire summary left unchanged due of lack of understanding.: Difficult paper to read, should think about excluding it.
%	\item ``Non-trivial concept"?: Could you please explain in text or abstract it using other common words?
%	\item ``Montonic conjunctions": Same. (Basic classifiers more like boolean expressions of the features)
%	\item ``..Lipschitz": Same.
%	\item `` ...not possible in polynomial number of samples for a non-trivial concept": Polynomial of what exactly? Dimensionality?: Yes. Dimension.
%\end{itemize}

%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: any; Classification Type: any;  Targeted Data Distribution: any
%	\item \textbf{Problem Setup}: Classifier Type: any; Training Procedure: any; Definition of Robustness: error-rate based; Attack: \{Perturbation Bound: any;  Attacker's Knowledge: white-box; Technique: any\}
%	\item \textbf{Type of Evidence}: Theoretical
%	\item \textbf{Actionable}: No
%	\item \textbf{Empirical Validation}: \emph{N/A}
%\end{itemize}

%\mt{Questions:}
%\begin{itemize}
%	\item Last sentence: ``Completely different" seems a little vague.
%\end{itemize}

%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: any; Classification Type: binary;  Targeted Data Distribution: r-separated
%	\item \textbf{Problem Setup}: Classifier Type: linear; Training Procedure: any; Definition of Robustness: error-rate based; Attack: \{Perturbation Bound: $L_p > 1$;  Attacker's Knowledge: white-box; Technique: any\}
%	\item \textbf{Type of Evidence}: Formal
%	\item \textbf{Actionable}:Yes
%	\item \textbf{Empirical Validation}: \emph{N/A}
%\end{itemize}


%\js{Questions:}
%\begin{itemize}
%	\item  How does no. of unlabeled samples depend on dimensionality and ...
%\end{itemize}

%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: any; Classification Type: any;  Targeted Data Distribution: theory: gaussian; experiment: any
%	\item \textbf{Problem Setup}: Classifier Type: any; Training Procedure: any; Definition of Robustness: radius based; Attack: \{Perturbation Bound: $L_2, L_{\infty}$;  Attacker's Knowledge: white-box; Technique: gradient-based\}
%	\item \textbf{Type of Evidence}: Formal \& Empirical
%	\item \textbf{Actionable}:Yes
%	\item \textbf{Empirical Validation}: \{Dataset: CIFAR10, SVHN; Attack: PGD; Classifier: CNN, ResNet; Training Procedure: adversarial; Definition of Robustness: radius based\}
%\end{itemize}

%\gx{Is it possible to quantitatively define the `modest' amount of unlabeled samples required above?}
%\js{Questions:}
%\begin{itemize}
%	\item  Discuss key differences with the paper above to refine summary
%\end{itemize}

% Michael to update this categorization.
%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: any; Classification Type: any;  Targeted Data Distribution: any
%	\item \textbf{Problem Setup}: Classifier Type: any; Training Procedure: adversarial; Definition of Robustness: error rate based; Attack: \{Perturbation Bound: $L_{\infty}$;  Attacker's Knowledge: white-box; Technique: gradient-based\}
%	\item \textbf{Type of Evidence}: Formal \& Empirical
%	\item \textbf{Actionable}:No
%	\item \textbf{Empirical Validation}: \{Dataset:\cifarten, \svhn ; Attack: PGD, FGSM; Classifier: DNN; Training Procedure: adversarial; Definition of Robustness: error rate based\}
%\end{itemize}


%\begin{itemize}
%	\item \textbf{Problem Space}: Application Domain: any; Classification Type: any;  Targeted Data Distribution: any
%	\item \textbf{Problem Setup}: Classifier Type: any; Training Procedure: adversarial; Definition of Robustness: radius based; Attack: \{Perturbation Bound: $L_2, L_{\infty}$;  Attacker's Knowledge: white-box; Technique: gradient-based\}
%	\item \textbf{Type of Evidence}: Formal \& Empirical
%	\item \textbf{Actionable}:Yes
%	\item \textbf{Empirical Validation}: \{Dataset: \mnist, \cifarten, \svhn ; Attack: PGD; Classifier: DNN; Training Procedure: adversarial; Definition of Robustness: error rate based\}
%\end{itemize}
