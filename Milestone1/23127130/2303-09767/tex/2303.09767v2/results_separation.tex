\subsection{Separation}
\label{sec:results-separation}

Closely related to density, \emph{separation} refers to the distance between classes.
% \de{(a.k.a. inter-class distance)} \ad{that can be quantified using different metrics, e.g., inter-class distance and optimal transport}.\js{Inter-class distance generally refers to a specific way to compute separation,but our section includes other different metrics as well}
Fig.~\ref{fig:separation_guideline} shows examples of not well-separated (top) and well-separated (bottom) datasets.
Intuitively, learning an accurate classifier is easier when data is well-separated as samples from different classes are farther apart and samples from the same class are closer together.
Different metrics to quantify separation include
the \emph{optimal transport distance},
which computes the minimum distance required to transport samples from one class to another, and
\emph{inter-class distance}, which computes the distance between samples in different classes.


%\begin{figure*}[h]
%\centering
%  \vspace{-0.35in}
%  \begin{minipage}{0.43\textwidth}
%  \centering
%  \vspace{0.05in}
%  \includegraphics[width=0.68\linewidth]{images/separation_overview.pdf}
%  \vspace{0.07in}
%  \caption{Separation illustration.}
%  \label{fig:separation_guideline}
%  \vspace{-0.15in}
%  \end{minipage}
%  \begin{minipage}{0.5\textwidth}
%  %\vspace*{0.1in}
%  \centering
%  \includegraphics[width =\textwidth]{images/separation_vertical_legend_updated.pdf}
%  \vspace{-0.3in}
%  \caption{Papers discussing separation.}
%  \label{fig:separation}
%  \end{minipage}
%%  \vspace{-0.1in}
%\end{figure*}

% New figure
\begin{figure*}[h]
	\centering
	\vspace{-0.25in}
	\begin{minipage}{0.43\textwidth}
		\centering
		\vspace{0.25in}
		\includegraphics[width=0.68\linewidth]{images/separation_overview.pdf}
		\vspace{0.07in}
		\caption{Separation illustration.}
		\label{fig:separation_guideline}
		\vspace{-0.3in}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\vspace*{0.10in}
		\centering
 		\includegraphics[width =\textwidth]{images/separation_citation_graph.pdf}
		\vspace{-0.25in}
		\caption{Papers discussing separation.}
		\label{fig:separation}
	\end{minipage}
    \vspace{-0.1in}
\end{figure*}



Papers that discuss data separation in relation to adversarial robustness are shown in Fig.~\ref{fig:separation}.
They can roughly be divided into
\roundrect{1} papers showing the effect of separation on robustness and
\roundrect{2} papers proposing techniques to promote separation and, thus, increase robustness.


\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Separation}.
%Another line of work presents classifier-agnostic bounds on adversarial risk by using inter-class distance.
%For example,
%\jr{Jaskeerat-P27}
Bhagoji et al.~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019} calculate lower bounds for adversarial risk 
in a binary classification setting using the optimal transport distance.
The authors show
%, i.e., the minimum distance used to transport samples from one distribution to another.
that the lower bound decreases as the distance between the two class distributions increases,
i.e., a classifier becomes more robust with better separation.
Based on this result, they estimate the minimum adversarial risks for image datasets, like \mnist and \cifarten,
showing that the theoretically calculated risks are lower than the empirical values achieved by the state-of-the-art defense models.
The authors conclude that there is still room for improving existing techniques.
%\mt{Rephrase as: Their results show that the empirical adversarial risk from state-of-the-art defense models is still higher than what can be achieved.}

%\jr{Gabby-P47 / P8}
Pydi and Jog~\cite{Pydi:Jog:ICML:2020, Pydi:Jog:NeurIPS:2021} arrive at a similar conclusion~-- %as Bhagoji et al.~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019}
%in the same setup but ussing a different proof strategy.
%The authors also
that robustness improves as separation between classes increases.
The authors further focus on datasets with simple univariate distributions, such as Gaussian and uniform.
They propose a technique to construct classifiers that can achieve the optimal,
lowest possible adversarial risk for a given separation between classes.
The main idea behind this technique is to analyze the optimal way to
transport samples from one class to another 
(which represents the smallest perturbation needed to create adversarial examples) and 
further use this information to identify the decision boundary that induces the maximal distance required to transport 
samples between classes. 
% as it maximizes the required perturbation size. .
That is, the approach maximizes the distance between samples of each class and the decision boundary, 
resulting in an optimally robust classifier.
% it should be far from any class to get to the decision boundary
%The authors show that the decision boundaries for optimally robust classifiers are sensitive \jr{?} to the maximum	 perturbation size and use this information to propose a decision boundary
%which makes it harder \jr{vague} to transport samples from one class to another.
%\js{Pydi et al show that as separation increases between classes, adversarial risk decreases i.e. robustness improves same as Bhagoji et al. That is the connection to separation. Then they go a step further and propose a method to get the optimal robust classifier for some given data which would have a fixed separation between classes.
%The intuition behind finding the optimal robust classifier is, analyzing the optimal (shortest) way to transport samples from one class to another i.e. the easiest perturbation which would make adversarial examples, and drawing a decision boundary which makes it harder to transport samples from one class to another}

%\de{They demonstrate that  }\jr{how is this relevant to separation?} \de{and they often differ from the decision boundary optimal for accuracy.}

Bhattacharjee et al.~\cite{Bhattacharjee:Chaudhuri:ICML:2020}
prove that certain non-parametric models, such as k-NNs, % and kernel classifiers with fast-decaying kernel functions,
are inherently robust when trained on a large number of well-separated samples.
This is because these classifiers make predictions based on neighborhoods and well-separated data ensures that
samples in close proximity to each other share the same labels.
In their later work, discussed in Section~\ref{sec:results-number-of-samples}~\cite{Bhattacharjee:Jha:Chaudhuri:PMLR:2021},
the authors
%Bhattacharjee et al.~\cite{Bhattacharjee:Jha:Chaudhuri:PMLR:2021},
%,
show that, in well-separated data, robust accuracy is independent of dimensionality and a robust linear classifier can be learned without the need for a large number of training samples.
This result shows that adversarial vulnerability can be efficiently tamed by increasing separation.
%introduced by high dimensionality can be fought by increasing separation}




\vspace{0.05in}
\noindent
\roundrect{2} {\bf Promoting Separation}.
%\jr{Gabby-P171}
Yang et al.~\cite{Yang:Rashtchian:Wang:Chaudhuri:AISTATS:2020} propose a sample-selection-based technique to improve the adversarial robustness of non-parametric models by increasing the separation among the training data.
In particular,
as non-parametric models tend to learn complex decision boundaries when the training samples from different classes are close to each other,
the authors propose to remove the smallest subset of samples so that all pairs of differently labeled samples
remain separated even when perturbed by the maximum perturbation size. 
Wang et al.~\cite{Wang:Jha:Chaudhuri:ICML:2018}, already discussed in Section~\ref{sec:results-dimensionality}, 
focus on improving robustness of 1-NN classifiers. 
Such classifiers struggle to take advantage of points close together with opposite labels, resulting in worse robustness.
Hence, the authors propose retaining the largest subset of training samples that are
(i) well-separated and (ii) in high agreement on labels with their nearby samples
(a.k.a., highly confident). 
The authors show that their approach outperforms adversarially trained 1-NNs. 

For non-parametric classifiers, a more effective strategy is to enforce separation in the latent representations. 
% to improve the adversarial robustness of DNNs.
%Some papers suggest modifying loss functions to promote separation in latent representation.
Specifically, Mustafa et al.~\cite{Mustafa:Khan:Hayat:Goecke:Shen:Shao:TPAMI:2020}
attribute the cause of adversarial vulnerability to close proximity of classes in latent space.
Hence, they propose a loss function to learn intermediate feature representations that
separate different classes into convex polytopes, i.e., polyhedra in higher dimensions,
that are maximally separated.
%This enforces the model to learn well-separated decision regions for each class and improves robustness.
% \js{Polytopes is the term for polygons (2-dimensional) in higher dimensions. Often times machine learning algorithms separate classes by making planes/hyperplanes. Having classes separated in distant (well separated) polyhedra/polytopes ensures that classifiers can easily learn the decision boundaries.}
\revadd{
Mygdalis et al.~\cite{Mygdalis:Pitas:PR:2022} propose a loss function to separate classes into hyperspheres, 
such that samples in a class have minimum distance from their hypersphere center and 
maximum distance from the remaining hyperspheres. 
The authors demonstrate that their approach outperforms that of
Mustafa et al.~\cite{Mustafa:Khan:Hayat:Goecke:Shen:Shao:TPAMI:2020} and other baselines 
w.r.t. standard and robust accuracy for \cifarten, \cifarhundred and \svhn.}


%\mt{Michael- P173}

Bui et al.~\cite{Bui:Le:Zhao:Montague:deVel:Abraham:Phung:ECCV:2020} observe that the adversarial vulnerability of DNNs arises from a large difference in intermediate layer values between clean and adversarial data.
They thus propose to modify the loss function so that it results in an intermediate latent representation 
that has high similarity between clean and their corresponding adversarial samples,
while promoting large inter-class and small intra-class distances and 
increased margins from class centers to decision boundaries.
%prevents decision boundaries from going through high-density regions, which effectively increases the margin from class centers to decision boundaries in high-density setup.
% \jr{why? it increases in some cases but not others.}
Likewise,
Pang et al.~\cite{Pang:Du:Zhu:ICML:2018} and Wan et al.~\cite{Wan:Chen:Yu:Wu:Zhong:Yang:TPAMI:2022} discussed in Section~\ref{sec:results-distribution},
as well as Pang et al.~\cite{Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020} discussed in Section~\ref{sec:results-density},
improve DNN robustness by separating centers of the produced latent distributions, which, in turn, 
increases the separation \mbox{between classes}.
\revadd{
Similarly, Cheng et al.~\cite{Cheng:Zhu:Zhang:Liu:PR:2023} propose improving separation by 
enforcing equal variance in all directions for all classes (Distribution Normalization) and 
maximizing the minimum margin between any two classes (Margin Balance). 
%The authors compare their approach with standard adversarial training and other baselines on \mnist, \cifarten and \cifarhundred, demonstrating improved robustness.
}

Yang et al.~\cite{Yang:Feng:Du:Du:Xu:ICDM:2021} propose a representation-learning technique
to learn feature representations that
bring samples of class $C$ and adversarial examples generated for $C$ into close proximity
while separating the samples of $C$ from both
(i) adversarial examples generated for other classes and misclassified as class $C$ and
(ii) samples from other classes.
These separations are enforced by the loss function proposed by the authors.
The authors show that their approach improves the resulting model robustness compared with standard DNNs.

%\jr{Michael-P32}
%In the same topic of representation learning,
%Other than learning robust latent representations,
%\ad{Separate from works using loss functions to encourage separation,}
Garg et al.~\cite{Garg:Sharan:Zhang:Hu:NeurIPS:2018} propose an approach to generate well-separated features for a dataset using graph theory.
%In graph theory, the Laplacian matrix of a graph reveals many useful properties, for example, the sparsest cut of a graph can be approximated through the second eigenvector, i.e., the eigenvector corresponds to the second smallest eigenvalue of the graph Laplacian.
%Based on such insight, the authors propose to identify robust features from the graph Laplacian of a dataset.
Specifically, they convert the input dataset into a graph, where vertices correspond to the input data points and
edges represent the similarity between the data points (e.g., calculated using Euclidean distance).
%Afterwards, the Laplacian matrix of the graph which is defined as the difference between the diagonal in-degree matrix and the adjacency matrix is computed.
The authors prove that features extracted using the eigenvectors of the Laplacian matrix capturing the structure of the graph
will have significant variation across the data points,
while being robust to small perturbations. These qualities make them good candidates for robust features.
%\jr{is this in agrement with small/large feature variation discussion in distribution: 
%maybe they are talking here about varience for the entire dataset, which is different from feature varience discussed there.}
%\js{We want features to have variation across data points (so a model can distinguish the points) and we want features to be robust to small perturbations. So the authors are essentially saying, that these qualities make them excellent candidates for robust features}
%The authors then demonstrate that the spectral properties
%(properties of the eigenvectors and eigenvalues of the laplacian matrix)
%of the graph correlated to adversarial robustness. \jr{why? how?}
The authors then demonstrate that a linear model trained on the \mnist dataset with 20 features generated using their approach is more robust to $L_2$-norm-based transfer attacks than a fully connected neural network trained on the full pixel values of the \mnist dataset.

%\jr{why transfer attack?} \js{to simulate a black box setting as white box attacks are unrealistic, I assume, the authors don't say this, they just cite a paper and say that transfer attacks can successfully fool most models} \ad{ }
