\section{Related Work}
\label{sec:relatedwork}

%%%%%%%%%%%%%%%%%%%%%%%%%% Outline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%(1) Evasion Attacks
%(1.1) Surveys on evasion attacks and their relation to data properties - Michael
%(1.2) Individual papers that study non-data related reasons behind evasion attacks - Michael
%(1.3) Techniques related to evasion attacks and defenses (new) - Gabby
%(2) Non-Evasion Attacks (new), and - ???
%(3) Effects of training data on standard generalization - done 
%
%
%
%(1) Evasion Attacks
%(1.1) A number of surveys review literature on evasion attacks. - Michael
%Most of them do not focus specifically on properties of data but also discuss attack and defense mechanisms, non-data-related reasons for adversarial vulnarability, and  more. ~\jr{cite 4}.
%Yet, they these surveys mention data and its relation to evasion attacks. Specifically \jr{what they say about data.}
%The most close to ours is concurrent work by XXX + concrete facts that we have and they don't.
%
%(1.2) individual papers that study non-data related reasons behind evasion attacks, - Michael
%Literature identifies multiple reasons for adversarial vulnerability, in particular, for evasion attacks. 
%These include data-related properties extensively discussed in this survey, as well as reasons related to the models 		   themselves, computations resources, and feature representations. We discuss these below. 
%
%\jr{the rest is from the paper (non-data related reasons for adversarial vulnerability), with sections potentially renamed.}
%
%{\bf Model.}
%
%{\bf Computational Resources.}
%
%{\bf Robustness of Features.}
%
%(1.3) Techniques Related to Evasion Attacks and Defenses (new) - Gabby
%A number of works focus on techniques for generating evasion attacks, countermeasures against these attacks, 
%and defining the notion of the attack itself.   
%
%{\bf Attacks and Defense.}
%Here are the 5 remaining surveys + 1 additional paper for the reviewer.
%
%{\bf Adversarial Examples.}
%2 surveys lines 13 and 14 + 1 additional paper for the reviewer.
%
%(2) Non-Evasion Attacks (new) 
%Need to say that there are other type of attacks, define them, cite surveys (Bo's survey, maybe something else). 
%Only one work explicitly focus on effects of data. 
%
%
%(3) Effects of training data on standard generalization (done)

%%%%%%%%%%%%%%%%%%%%%%%%% Outline %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\revreplace{
We divide related work into three categories:
(1) surveys on adversarial robustness and its relation to data properties,
(2) surveys that discuss the influence of data properties on standard generalization, and
(3) individual papers that study non-data-related reasons for adversarial vulnerability.\\
}
{
This survey investigates properties of training data in the context of model robustness under evasion attacks. 
We start the discussion of related work by reviewing other surveys that focus on evasion attacks and 
include some discussion about data (Section~\ref{sec:relatedwork-surveys-data}).  
We then discuss non-data related reasons behind evasion attacks (Section~\ref{sec:relatedwork-not-data}),
as well as techniques related to evasion attacks and defenses (Section~\ref{sec:relatedwork-attacks}). 
Finally, we discuss data-related concerns for non-evasion attacks (Section~\ref{sec:relatedwork-poisoning}) and
the effects of training data on standard generalization (Section~\ref{sec:relatedwork-standard}).
}

%\vspace{-0.1in}
\subsection{Surveys on Evasion Attacks that Discuss Data}
\label{sec:relatedwork-surveys-data}
Numerous existing surveys 
\revreplace{focus on attack and defense techniques for adversarial robustness. 
%~\cite{Biggio:Roli:PR:2018,
%Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,
%Li:Li:Ye:Xu:CSUR:2021,
%Maiorca:Biggio:Giorgio:CSUR:2019,
%Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021,
%Liu:Tantithamthavorn:Li:Liu:CSUR:2022,
%Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,
%Akhtar:Mian:IEEEAccess:2018,
%Akhtar:Mian:Kardan:Shah:IEEEAccess:2021,
%Serban:Poll:Visser:CSUR:2020,
%Machado:Silva:Goldschmidt:CSUR:2021,
%Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}.
Only a few of these works mention the relationship between adversarial robustness and properties of the underlying data.} 
{review the literature on evasion attacks.
Most of these works do not focus specifically on properties of data but discuss attack and defense mechanisms, non-data-related reasons for adversarial vulnerability, 
and the different threat models. 
Only a few of these works mention data-related reasons for the existence of adversarial examples~\cite{Serban:Poll:Visser:CSUR:2020, Machado:Silva:Goldschmidt:CSUR:2021, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021, Akhtar:Mian:IEEEAccess:2018}.
}
Specifically, Serban et al.~\cite{Serban:Poll:Visser:CSUR:2020} observe that adversarial vulnerability can be caused by an insufficient training sample size %~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}
and high data dimensionality. %~\cite{Gilmer:Metz:Faghri:Schoenholz:Raghu:Wattenberg:Goodfellow:ICLR:2018}.
Similarly, Machado et al.~\cite{Machado:Silva:Goldschmidt:CSUR:2021} mention that the lack of sufficient training data, high dimensionality, 
and high concentration contribute to adversarial vulnerability.
\revadd{
Akhtar et al.~\cite{Akhtar:Mian:IEEEAccess:2018, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021} also mention high dimensionality, along with other non-data-related reasons, 
as a source of adversarial examples.}

\revadd{A concurrent work by Han et al.~\cite{Han:Lin:Shen:Wang:Guan:CSUR:2023} (published at the end of April 2023) 
studies the origins of adversarial vulnerability in deep learning w.r.t. the model, data, and other perspectives.
The authors mention high dimensionality, distributions with high concentration, a small number of output classes, data imbalance, and the perceptual difference in image frequencies as potential sources of adversarial examples.
However, as (a) the focus of that survey is not on data-related properties in particular, 
(b) its paper search was conducted in 2021, and 
(c) it focuses on deep learning models only, 
our work was able to identify more than 50 additional relevant papers which focus on other types of models, 
e.g., non-parametric and linear classifiers, 
and/or discuss additional types of data-related properties, 
such as, types of distribution, class density, separation, and label quality.}
\revreplace{Yet, none of these surveys explicitly collect and analyze work that focuses on the effects of data properties
on adversarial robustness.}
{In summary, by explicitly focusing on the effects of data properties on evasion attacks in our survey, 
we are able to provide a more complete and detailed discussion on this topic, not covered in prior surveys.}

\vspace{-0.05in}
\subsection{Non-data-related Reasons Behind Evasion Attacks}
\label{sec:relatedwork-not-data}

%\vspace{-0.1in}
%\subsection{Non-data Related Reasons for Adversarial Vulnerability}

There has been a variety of hypotheses regarding the reasons behind adversarial vulnerability of ML systems, particularly for evasion attacks.
%\revreplace{
%In addition to the data used for training,  adversarial robustness could also depend on the choice of the model architecture,
%the training procedure, and the interplay between data and the learning algorithm, i.e., correspondence between the complexity of a model to that of the data.
%This section summarizes the key hypotheses regarding these aspects.
%%The hypotheses reviewed in this section are complementary to the potential influence from the data.
%}
These include data-related properties extensively discussed in this survey, as well as reasons related to the models themselves, 
computational resources, and feature learning procedures. We discuss these below.

%\jr{there is a lot of undefined terminology and jargon in this section.}

\vspace{0.02in}
\noindent
\textbf{Model.}
When Szegedy et al.~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014} first discovered adversarial examples for visual models, they suspected that the high non-linearity of DNNs resulted in low probability `pockets' of adversarial examples in the learned representation manifold.
They hypothesize that while these pockets can be found through attack algorithms, the samples residing in these pockets have different distributions compared to normal samples and are thus subsequently harder to find when randomly sampling from the input space.
Instead, Goodfellow et al.~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015} hypothesize that
the linearity from activation functions, like ReLU and sigmoid found in high-dimensional neural networks, induce vulnerability towards adversarial perturbations.
To support their claim, they present the attack method FGSM that exploits the linearity of the target classifier.
Fawzi et al.~\cite{Fawzi:Fawzi:Frossard:ICMLWorkshop:2015} also argue against the hypothesis of high non-linearity as the cause for adversarial examples.
They show that all classifiers are susceptible to adversarial attacks and claim that it is the low flexibility of the classifier compared to the complexity of the classification task that results in vulnerability.
The lack of consensus on the primary causes of model vulnerability invites more studies on this topic.

Singla et al.~\cite{Singla:Ge:Basri:Jacobs:NeurIPS:2021} show that enforcing invariance to circular shifts (e.g., rotation) in neural networks induces decision boundaries with a smaller margin than normal, fully connected networks,
which, in turn, reduces the adversarial robustness of the model.
Moosavi{-}Dezfooli et al.~\cite{Moosavi-Dezfooli:Fawzi:Fawzi:Frossard:Soatto:ICLR:2018} introduce universal,
input-agnostic perturbations to mislead the classifier and hypothesize that the vulnerability of a multi-class classifier to such perturbations is related to the shape of its decision boundaries, e.g.,
linear classifiers with decision boundaries that are parallel to each other and
nonlinear classifier with decision boundaries that are curved in a similar way
tend to be less robust as
perturbations in one direction can change the prediction label for a different class.

Tanay and Griffin~\cite{Tanay:Griffin:ArXiv:2016} conjecture that the decision boundary learned by the classifier being too close to (or `tilted towards') the data manifold instead of being perpendicular to it,
results in small perturbations being sufficient to move samples across the decision boundary for misclassification.
%data manifold refers to the underlying structure that the data exhibit

\vspace{0.02in}
\noindent
\textbf{Computational Resources.}
Bubeck et al.~\cite{Bubeck:Lee:Price:Razenshteyn:ICML:2019} use computational hardness theory to show that the time complexity for learning a robust model is exponential to the size of input data and thus is computationally intractable.
Hence, they attribute adversarial vulnerability to computational limitations of current learning algorithms.
Degwekar et al.~\cite{Degwekar:Nakkiran:Vaikuntanathan:COLT:2019} further extend this work and also show the impossibility of efficiently training robust classifiers.

%\subsubsection{Ineffective Learning Perspective}
\vspace{0.02in}
\noindent
\textbf{Feature Learning.}
Ilyas et al.~\cite{Ilyas:Santurkar:Tsipras:Engstrom:Tran:Madry:NeurIPS:2019} show that adversarial vulnerability can be a consequence of a model exploiting well-generalizing but non-robust features,
i.e., features that are spurious and sometimes incomprehensible to humans;
when constraining the model to use robust features, the adversarial robustness increases together with the
interpretability of the learned features.
However, Tsipras et al.~\cite{Tsipras:Santurkar:Engstrom:Turner:Madry:ICLR:2019} note that, as the features for achieving high accuracy may be different from the ones for achieving high robustness, robustness may be at odds with standard accuracy.
%
%\jr{why is it called Ineffective learning when it is about features.}\gx{I put it under ineffective learning as in this case, the model learns/decides the features for generalization, and when given the correct objective, the model in fact, can learn more robust features, so I think the underlying reason is objective we gave for the model didn't guide the model to learn the right features}
%
Instead of seeing adversarial vulnerability as a product of classifiers being overly sensitive to changes in spurious features, Jacobsen et al.~\cite{Jacobsen:Behrmann:Zemel:Bethge:ICLR:2019} hypothesize that classifiers can rather be
overly insensitive to relevant semantic information, e.g., images with drastically different content can share similar latent representations.
The authors introduce a new type of adversarial examples that exploit such insensitivity, where the content of images is altered without changing the resulting prediction label.
%As both insensitivity to semantic content and sensitivity to spurious changes can simultaneously exist in models,
%more investigation into how to define proper objectives for models to effectively distinguish the relevant information is needed.

While all these works propose possible reasons for adversarial vulnerabilities, they are orthogonal to our survey, which focuses particularly on the influence of training data.

\vspace{-0.05in}
\revadd{
\subsection{Evasion Attacks and Defenses}
\label{sec:relatedwork-attacks}
A number of works focus on techniques for generating evasion attacks, countermeasures against these attacks, 
and defining the notion of the attack itself.

%\jr{need to include~\cite{Biggio:Roli:PR:2018,
%Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,
%Li:Li:Ye:Xu:CSUR:2021,
%Maiorca:Biggio:Giorgio:CSUR:2019,
%Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021,
%Liu:Tantithamthavorn:Li:Liu:CSUR:2022,
%Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,
%Zhang:Sheng:Alhazmi:Li:ACMTIST:2020} x and one more survey.}
%\js{\cite{Biggio:Roli:PR:2018, Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021} moved to Adversarial Examples.
%\cite{Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,
%Li:Li:Ye:Xu:CSUR:2021,
%Maiorca:Biggio:Giorgio:CSUR:2019, Liu:Tantithamthavorn:Li:Liu:CSUR:2022,
%Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,
%Zhang:Sheng:Alhazmi:Li:ACMTIST:2020, Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021} in Attacks and Defense. \cite{Sun:Dou:Yang:Zhang:Wang:Philip:He:Li:TKDE:2022} was the "one more survey" and is also in Attacks and Defenses.}

\vspace{0.02in}
\noindent
{\bf Attacks and Defense.}
Several works~\cite{Liu:Tantithamthavorn:Li:Liu:CSUR:2022,Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,Sun:Dou:Yang:Zhang:Wang:Philip:He:Li:TKDE:2022, Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021} survey adversarial attacks and defenses, observing that most work focuses on computer vision and NLP domains. 
Zhang et al.~\cite{Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}, 
Rosenberg et al.~\cite{Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021},
Li et al.~\cite{Li:Li:Ye:Xu:CSUR:2021}, and 
Maiorca et al.~\cite{Maiorca:Biggio:Giorgio:CSUR:2019}, 
survey attacks and defenses in the NLP domain, cybersecurity domain for networks, Android malware, and PDF malware, respectively. 
These works identify a similar trend of new attacks constantly bypassing defenses, which gives rise to new defenses being proposed, only to be broken again (a.k.a. the `cat and mouse race' or the `arms race'). 
They also observe that research in this field studies attacks / defenses at a feature-level, which restricts 
the practicality of the developed techniques by the feasibility of perturbing the corresponding features in real life. 

%practical attacks are quite difficult and require some basic knowledge about the model or training data such as the feature set or model architecture. 
%Zhang et al.~\cite{Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}, who study adversarial attacks and defenses in the NLP domain,  
%also find that there are obstacles to generating attacks in real-time. 
%For instance, methods that iteratively use gradients to create adversarial examples can be time-consuming, while one-time approaches may fail to produce potent adversarial examples.
%Several works~\cite{Liu:Tantithamthavorn:Li:Liu:CSUR:2022,Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,Sun:Dou:Yang:Zhang:Wang:Philip:He:Li:TKDE:2022, Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021} 
%discuss how most new attacks and defenses are explored in computer vision and NLP, prior to other fields.


%our survey finds the state of the art w.r.t. data properties
%our survey finds that dimensionality is bad ...
%
%%%Here are the 5 remaining surveys + 1 additional paper for the reviewer.
%Numerous surveys have explored the landscape of adversarial evasion attacks and defenses. 
%For instance, Akhtar et al.~\cite{Akhtar:Mian:IEEEAccess:2018, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021} survey the literature on adversarial robustness of deep learning models from Computer Vision field.
%They review popular attacks on visual models, and provided a categorization of existing defense techniques based on the components it modify in the visual model system \gx{Check}.
%
%Rosenberg et al.~\cite{Rosenberg:Shabtai:Elovici:Rokach:ACMComputingSurvey:2021}, Li et al. ~\cite{Li:Li:Ye:Xu:ACMComputingSurvey:2021} and Demetrio et al.~\cite{Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021} review the literature on evasion attacks for cyber-security fields. 
%Li et al. proposed a partial order scheme to compare key attacks and defenses techniques for malware detection in Windows, Android, and PDF domains. 
%
%Zhang et al.~\cite{Zhang:Sheng:Alhazmi:Li:ACMTIST:2020} review the literature on adversarial attacks on deep-learning models for textual classification.
%They pointed out the intrinsic differences between Computer Vision and Natural Language Processing fields that pose challenges to directly apply attacks proposed for Visual models to NLP models and identified the strategies proposed that overcomes the barriers.
%The challenges they identified for creating realistic attacks in NLP fields are from a domain characteristics perspective (e.g., definition of imperceptible perturbations, measurement of the semantic changes),  we differ from them by trying to understand the adversarial robustness of machine learning from the characteristics of underlying data. 
%
%Attack and Defenses for wireless and Mobile systems~\cite{Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022}
%
%

More recent research, not included in the surveys above, has also started investigating the 
susceptibility of newer models to adversarial evasion attacks. 
For example, several studies~\cite{Wang:Pan:Hu:Duan:Pan:IJSWIS:2022,Yin:Lin:Sun:Wei:Chen:TIFS:2023, 
Shi:Han:Tan:Kuang:NeurIPS:2022, Wang:Xie:Microsoft:ChatGPT:ArXiv:2023} proposed attack techniques against contemporary models, 
such as Graph Neural Networks, Generative Pre-training Transformers (GPT), and Vision Transformers. 
These studies showed that adversarial examples persist even for the newer models, some of which are 
trained with large volumes of data. 
As all these works focus on attack and defense mechanisms rather than 
the effects of data on adversarial robustness, our work extends and complements this research.
}

\revadd{
\vspace{0.02in}
\noindent
{\bf Adversarial Examples.}
%2 surveys lines 13 and 14 + 1 additional paper for the reviewer.
Adversarial examples are inputs constructed by perturbing a correctly classified sample in a way that makes the change imperceptible to a human. % but causes the model to misclassify the sample.
However, as `imperceptible to a human' is hard to define, existing research on adversarial examples approximates imperceptibility with a small perturbation measured through $L_p$ norms.
A line of research~\cite{Gilmer:Adams:Goodfellow:Anderson:Dahl:ArXiv:2018,Sharif:Bauer:Reiter:CVPRW:2018,Fezza:Bakhti:Hamidouche:Deforges:QoMEX:2019, Mezher:Deng:Karam:EUVIP:2022} 
investigates the validity of this assumption. 
This work shows that perturbations generated by $L_p$ norms do not entirely align with human perceptions, 
i.e., some changes with a small $L_p$ norm can be apparent to humans. 
In addition, adversarial examples with the minimum $L_p$ perturbation may be less effective and transferable than 
higher perturbation~\cite{Biggio:Roli:PR:2018,Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021}. 
Hence, a number of approaches explore metrics for imperceptibility 
in computer vision and NLP domains~\cite{Fezza:Bakhti:Hamidouche:Deforges:QoMEX:2019,Mezher:Deng:Karam:EUVIP:2022, Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}. 
Yet another issue with $L_p$ norms is that they cannot be used reliably in domains other than images. 
For example, in the case of software/malware, simply generating adversarial examples with $L_p$ norms 
may result in feature representations that are not possible in 
the problem space~\cite{Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,Pierazzi:Pendlebury:Cortellazz:Cavallaro:2020}. 

While all these works focus on the properties of adversarial examples, 
they are orthogonal to the topic of our survey, as we rather focus on how properties of the training data 
affect the success of adversarial examples.
}

%Gilmer et al.~\cite{Gilmer:Adams:Goodfellow:Anderson:Dahl:ArXiv:2018} argue that, while constraining the perturbations by sufficiently small $L_p$ norms can generate indistinguishable samples for most inputs, the actual imperceptibility of the changes depends on the input sample. 
%Several individual studies~\cite{Sharif:Bauer:Reiter:CVPRW:2018,Fezza:Bakhti:Hamidouche:Deforges:QoMEX:2019, Mezher:Deng:Karam:EUVIP:2022} find faults with using $L_p$ norms to generate adversarial examples. They show that the changes measured by $L_p$ norm, does not entirely align with human perceptions, i.e., some changes with a small $L_p$ norm appear apparent to humans. 
%In some domains adversarial examples do not need to be imperceptible but rather semantically preserving. 
%For example, in the case of Android malware~\cite{Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021}, adversarial examples are small perturbations which fool a model while preserving the semantics of the sample, 
%i.e., a malware stays malicious even after the perturbation. 
%This highlights another problem with $L_p$ norm based adversarial examples as Dong et al.~\cite{Dong:Liu:Shang:NeurIPS:2022} show that the semantics of a sample change during adversarial training. 
%Hence, there is a need for metrics to measure the size of perturbations that is imperceptible or semantically preserving.
%Fezza et al.~\cite{Fezza:Bakhti:Hamidouche:Deforges:QoMEX:2019} and Mezher et al.~\cite{Mezher:Deng:Karam:EUVIP:2022} propose to use objective metrics for image quality to approximate the imperceptibility in the computer vision domain.
%Zhang et al.~\cite{Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}, focusing on providing such a metric for Natural Language Processing.
%Vadillo et al.~\cite{Vadillo:Santana:CS:2022} also highlight conducted subject studies to evaluate the noticeability of audio adversarial examples.

%Even in computer vision, adversarial examples are not always imperceptible. For example, Machado et al.~\cite{Machado:Silva:Goldschmidt:CSUR:2021} find that visible perturbations such as adversarial patch~\cite{Brown:Mane:Roy:Abadi:Gilmer:ArXiv:2017}, and graffiti on stop signs~\cite{Eykholt:Evtimov:Fernandes:Li:Rahmati:Xiao:Prakash:Kohno:Song:CVPR:2018} are also considered adversarial examples in research.

%The aforementioned research examines the work on defining and creating adversarial examples, demonstrating the insufficiency of using conventional $L_p$ norms to evaluate the imperceptibility and semantics between clean and adversarial examples. 

\vspace{-0.1in}
\revadd{
\subsection{Non-Evasion Attacks}
\label{sec:relatedwork-poisoning}
Similar to evasion attacks, data poisoning and backdoor attacks aim to compromise model accuracy. 
However, they achieve it by tampering the training data to create deceptive model decision boundaries. 
%Data poisoning attacks involve modifying the training data to create deceptive decision boundaries, either to manipulate the prediction outcomes of a specific input or the entire model.
%Meanwhile, Backdoor attacks are a form of poisoning attacks where the attacker inject tempered training data with triggers 
% and then activates the attack by showing the trigger pattern at inference time.
In addition, backdoor attacks also require perturbing the test instance to result in a misclassification. 
This is achieved by introducing manipulated training data with triggers that can be activated during the testing phase.

Goldblum et al.~\cite{Goldblum:Tsipras:Xie:Chen:Schwarzchild:song:Madry:Li:Goldstein:TPAMI:2022} and Cin√† et al.~\cite{Cina:Grosse:Demontis:Sebastiano:Zellinger:Moser:Oprea:Biggio:Pelillo:Roli:CSUR:2023} 
review recent literature on attack methodologies and countermeasures for both poisoning and backdoor attacks.
Both of these surveys found that existing research made overly-optimistic assumptions when designing / validating attack techniques, e.g., assuming the knowledge of a large portion of training data. 
They advocate for researchers to test proposed methods in more realistic situations to better assess the potential threats. 
Furthermore, they encourage exploration of the relationship between poisoning attacks and evasion attacks. 
This could lead to the creation of attacks that produce less noticeable poisoning examples, 
or defensive strategies that can safeguard models against both backdoor and evasion attacks.
%Their survey catalogs and systematizes the threats in the dataset creation process, and discuss the open problems that benefits the understanding of dataset security. 

In addition to undermining model accuracy, 
adversarial attacks also aim at breaching the privacy and confidentiality of training data. 
In particular, membership inference attacks~\cite{Shokri:Stronati:Song:Shmatikov:SP:2017} attempt to determine whether a specific data point was part of the training set used to train the model.
Hu et al.~\cite{Hu:Salcic:Sun:Dobbie:Yu:Zhang:CSUR:2022} present a comprehensive survey of existing research efforts on membership inference attacks. 
They find that, similar to evasion attacks, the membership inference attack success rate decreases as 
%the training data better represents the whole data distribution, i.e., 
the number of training samples increases.
%and model stealing attacks~\cite{Oliynyk:Mayer:Rauber:CSUR:2023} are designed to breach the privacy of training data and machine learning models. 
However, all these attacks are orthogonal to our survey, as we focus on adversarial evasion attacks.

%Li et al. ~\cite{Li:Jiang:Li:Xia:TNNLS:2022} 
%provide the first survey that focuses on backdoor attacks and identified common scenarios in which backdoor attack happen in real life. 
%Furthermore, they proposed a systematic taxonomy for backdoor attacks and defenses for researchers and practitioners to identify the characteristics and limitations of each method. 

%Wang et al.~\cite{Wang:Ma:Wang:Hu:Qin:Ren:CSUR:2022} and Tian et al.~\cite{Tian:Cui:Liang:Yu:CSUR:2022} argue federated learning~\cite{McMahan:Moore:Ramage:Hampson:Arcas:AISTATS:2017} 
%creates new venue for poisoning attack, and survey recent literature on poisoning attacks for both standard and federated learning scenarios. 
%They present a unified framework to categorize both data poisoning and model poisoning attacks, and compared the defense techniques proposed for each of the learning framework, analyzed their advantages and disadvantages.
}

\vspace{-0.1in}
\subsection{Effects of Training Data on Standard Generalization}
\label{sec:relatedwork-standard}
A number of surveys investigate the influence of data properties on standard
rather than robust generalization.
One of the earliest is probably the work of Raudys and Jain~\cite{Raudys:Jain:TPAMI:1991},
who review studies related to the influence of sample size on binary classifiers, showing that
a limited sample size usually leads to sub-optimal generalization.
%With the development of deep learning and the ever-increasing need for larger training datasets,
%a variety of data augmentation techniques have been proposed.
Bansal et al.~\cite{Bansal:Sharma:Kathuria:CSUR:2021} and
Bayer et al.~\cite{Bayer:Kaufhold:Reuter:CSUR:2022} also survey papers addressing the data scarcity problem,
focusing in particular on the recent advancements in data augmentation techniques in the fields of computer vision, security, and text classification.
Their results show that augmentation techniques %exist for various application domain and
can help improve a model's generalization by reducing the problem of model overfitting.
%They evaluate the effectiveness of such techniques in improving the accuracy of machine learning models.

%Limited sample size is also one of the culprit behind poor robust generalization~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018}, we collected a number of researches characterize the sample complexity for robust generalization or propose data augmentation techniques to fill in the sample complexity gap.

Label noise is another aspect of data that influences both standard and robust generalization.
Most works on this topic find that the presence of noisy labels increases the need for a greater number of training samples and may result in unnecessarily complex decision boundaries~\cite{Frenay:Verleysen:TNNLS:2014,Song:Kim:Park:Shin:Lee:TNNLS:2022}.
For example, Fr\'{e}nay and Verleysen~\cite{Frenay:Verleysen:TNNLS:2014} show
that overfitting to label noise greatly degrades a model's standard generalization;
the same effect has been observed in the case of robust generalization~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021}.
Song et al.~\cite{Song:Kim:Park:Shin:Lee:TNNLS:2022} survey the impact of label noise in deep learning, arguing
that the presence of noisy labels is a more serious concern for deep models as they contain a larger number of parameters which makes them prone to overfitting to the noise in training data.
%They also point out the connection between adversarial poisoning attacks and noisy labels as
%the countermeasures for both share the goal of learning noise-resilient representations.
They mention that adversarial defense techniques, e.g., adversarial training, are effective against label noise~\cite{Zhu:Zhang:Han:Liu:Niu:Yang:Kankanhalli:Sugiyama:ArXiv:2021, Fatras:Damodaran:Lobry:Flamary:Tuia:Courty:TPAMI:2022}
but do not discuss how label noise influences a deep learning model's robustness under attacks.

Lorena et al.~\cite{Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020} identify a collection of 26 quantitative metrics that measure data complexity with respect to
(1) ambiguity of classes, i.e., whether the classes can be clearly distinguished with the given features,
(2) sparsity and dimensionality of data, 
%i.e., whether enough information are provided to learn confident decision boundaries, and
(3) complexity of boundary separating the classes, i.e., whether more intricate functions are required to describe the decision boundaries.
The authors also discuss how these metrics help estimate the difficulty of performing classification on a given dataset.
Similar to our survey, the authors show that high dimensionality and small separation between classes hinder standard generalization.
However, the relationship of some of the metrics reviewed by these authors, e.g.,
%faction of borderline points (i.e., a measure for the complexity of the required decision boundary) and
%the fraction of hyperspheres covering data (i.e.,
the number of non-intersecting spheres needed to enclose all data points of a class,
to robust generalization is not studied, according to our survey.

%Moreover, the effect of XXX on standard generalization needs future investigation as well (that is if we found something they do not have).

%Knowing the characteristics of a dataset according to these perspectives can assist researchers and practitioners to select optimal learning algorithms~\cite{Ho:Basu:TPAMI:2002}.

He and Garcia~\cite{He:Garcia:TKDE:2009} focus on the imbalance learning problem. %~--
%the disproportion in the number of samples belonging to each class in a given dataset.
The authors found that most standard algorithms %are designed with the assumption of a balanced class distribution.
%These algorithms
fail to reliably represent the characteristics of the imbalanced data and result in unfavorable performance across classes.
Furthermore, L\'{o}pez et al.~\cite{Lopez:Fernandez:Garcia:Palade:Herrera:InfSci:2013} discuss six intrinsic data characteristics that potentially complicate learning from imbalanced data:
low density, sample overlap between classes, noisy data, borderline instances,
dataset shift between training and testing distributions, and
small disjuncts, i.e., disperse small clusters of samples from a single class.
Their analysis concludes that while all these ``unfavorable'' data characteristics further complicate the data imbalance
issues, data overlap between classes is probably one of the most harmful.
To follow up on this point, Santos et al.~\cite{Santos:Henriques:Pedro:Japkowicz:Fernandez:Soares:Wilk:Santos:AIR:2022}
focus on the joint effect of data imbalance and class overlap on model generalization.
The negative impact of data imbalance, low separation, and noisy data on robust generalization was also discussed in our survey.
Yet, the compounding effect of these factors, as well as the effect of other properties,
on robust generalization needs future investigation.

Recently, Yang et al.~\cite{Yang:Jiang:Song:Guo:IJCV:2022} summarized relevant studies focusing on
long-tailed distributions in the field of Computer Vision.
% and categorize the main methods for alleviating the issues caused by long-tailed distribution.
%They present quantitative metrics for measuring data imbalance and .
This survey also includes work on the influence of long-tail distributions on a model's adversarial robustness~\cite{Wu:Liu:Huang:Wang:Lin:CVPR:2021}, which is covered in our survey.
%which is included in our survey,
The authors advocate for more research on adapting long-tailed-based approaches for standard generalization to improve robust generalization.

Finally, Moreno-Torres et al.~\cite{MorenoTorres:Raeder:Rodrigues:Chawla:Herrera:PR:2012} present a unifying framework to categorize existing definitions of dataset shift~-- the case where the joint distribution of inputs and outputs differs between training and testing data.
While ML models are normally trained under the premise that testing data has a similar distribution to the training data,
in reality, the observed data distribution may be different from the historical data that the model is trained on.
Such difference can substantially compromise the quality of model predictions.
The authors analyze the possible causes for dataset shift, e.g., malicious software that evolves over time, and
review the techniques dealing with dataset shift.
They characterize adversarial attacks as one form of dataset shift, where adversaries adaptively
change test instances to create a distribution that differs from training data.
%All works discussed in our survey assumed similar distribution on training and testing data, treating adversarial attacks as the only dataset shift in the problem setup.
%However, in real applications, the underlying data distribution itself can be non-stationary, and the characterize the influence of the dataset shift between training and testing data on the adversarial robustness is yet to be investigated.

\revadd{Overall, despite the similarities with our work, literature discussed in this section focuses on standard generalization while our survey discusses 
the effect of data on robust generalization.}

%More works use the connection between adversarial attacks and distributional shift to analyze the effect of adversaries on generalization performance~\cite{Tu:Zhang:Tao:NeurIPS:2019}.
%However, we do not discuss them in detail, as they focus more on models instead of data.
%\jr{How is that relevant to data properties section?} \gx{This can be removed, as it an individual work we filtered}

\vspace{-0.1in}
\subsection{Summary}
\revadd{
Our survey is the first to explicitly focus on properties of training data in the context of model robustness under evasion attacks.
Numerous other surveys on evasion attacks discuss attack and defense mechanisms, non-data-related reasons for adversarial vulnerability, and the different threat models. 
We identified only five surveys that considered data-related reasons for evasion attacks. 
However, as these surveys are older and do not focus on data in particular, our work provides a more extensive
and comprehensive view on this topic. 
By including more than 50 papers not covered in prior work, we were able to 
identify additional relevant properties, practical suggestions, and future research directions in this area. 

Additional work studies non-data-related reasons for evasion attacks, as well as non-evasion attacks, 
such as poisoning and backdoor. 
Yet another body of literature examines how data properties affect standard generalization. These works show that 
some of the properties discussed in our survey, such as 
the number of samples, dimensionality, and label quality, also affect clean accuracy. 
There are also additional data properties that are covered exclusively by these or by our work. 
Studying the interplay between data properties for clean and robust accuracy is an interesting research direction, 
which could be facilitated by our work. 
However, all these current works are orthogonal and complementary to ours.
}

%\ad{
%The related work of our survey can be categorized into four key topics: 
%The first topic examines data for other adversarial attacks, this include the research that investigates the link between the data characteristics and model's resilience against poisoning attacks as well as the studies that explore data poisoning and backdoor attacks and their countermeasures. \jr{same issues as before: this is meta-summary, we need a concrete summary.}
%These studies complement our survey as they highlight the threats directly aimed at data, thus emphasizing the importance of secure data collection. 
%The second topic focuses on the relationship between various properties of training data and model's standard generalization ability. 
%This body of work suggests that data traits such as number of samples, dimensionality, label quality also influence model's ability to generalize in standard classification. \jr{this looks more concrete!}
%
%The third strand of research concerns adversarial evasion attacks. 
%The work in this area encompasses the research frontier in evasion attacks and the countermeasures. 
%Due to the large volume of work in this area, there are numerous surveys that gives more detail on the advancement. 
%\jr{meta-summary again}
%In addition to attacks and defenses, one relevant line of work investigates the alignment of the conventional similarity metrics used for adversarial examples and human perception, showing the need for supplementary metrics. \jr{why important?}
%These studies \jr{which "these studies"?} collectively present an extensive overview of other types of work conducted on adversarial robustness.
%The last category of work proposes alternative explanations for model vulnerability to adversarial examples.
%These studies presented hypothesis showing the characteristics of machine learning models, e.g., nonlinearity, invariance to rotational shift etc, induces susceptibility to attacks, as well as limited computational resources and non-robust feature representations. \jr{all text based on previous related work looks somewhat concrete; the new additions should be at least at the same level, or better.}
%These studies supplement our work, offering a broader perspective of potential factors affecting model's robust generalization ability. }
%

