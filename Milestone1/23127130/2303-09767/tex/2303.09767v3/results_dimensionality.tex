\subsection{Dimensionality}
\label{sec:results-dimensionality}

\begin{figure*}[h]
\centering
  \begin{minipage}{0.55\textwidth}
  \centering
  \includegraphics[width=\linewidth]{images/dimensionality_overview_3d.pdf}
  \vspace{-0.25in}
  \caption{Dimensionality illustration.}
  \label{fig:dimensionality_illustration}
  \vspace{-0.1in}
  \end{minipage}
  \begin{minipage}{0.44\textwidth}
  \vspace{0.1in}
  \centering
    \subcaptionbox{Actual. \label{fig:intrinsic_dimensionality_original}}{
     \includegraphics[width=0.42\linewidth]{images/dimensionality_original_dim.pdf}
   }
   \subcaptionbox{Intrinsic. 	 \label{fig:intrinsic_dimensionality_intrinsic}}{
    \includegraphics[width=0.42\linewidth]{images/dimensionality_intrinsic_dim.pdf}
    }
  \vspace{-0.05in}
  \caption{Actual and intrinsic dimensionality.}
  \label{fig:intrinsic_dimensionality}
  \vspace{-0.1in}
  \end{minipage}
\end{figure*}

\begin{figure*}[h]
  \centering
 % \vspace{-0.1in}
  \includegraphics[width=0.99\linewidth]{images/dimensionality_citation_graph.pdf}
  \vspace{-0.1in}
  \caption{Papers discussing dimensionality. }
  \label{fig:dimensionality}
  \vspace{-0.1in}
\end{figure*}

\emph{Dimensionality} refers to the number of features used to represent the data, e.g.,
features $f_1$, $f_2$, $f_3$ in Fig.~\ref{fig:dimensionality_illustration}.
For illustration purposes, we show a dataset with a dimensionality of three on the left-hand side of the figure and
a dataset with a dimensionality of one on the right-hand side.
\emph{Intrinsic dimensionality} refers to the number of features used in a minimal representation of the data.
Fig.~\ref{fig:intrinsic_dimensionality} shows an example of a case where the intrinsic dimensionality
is smaller than the actual dimensionality:
the samples in Fig.~\ref{fig:intrinsic_dimensionality}a are lying on a three-dimensional ``swiss roll''.
``Unwrapping'' the roll into a plain sheet, as shown in Fig.~\ref{fig:intrinsic_dimensionality}b,
makes it possible to distinguish between the samples using only two dimensions.



Papers studying the relationship between data dimensionality and adversarial robustness are shown in Fig.~\ref{fig:dimensionality}.
We divide them into papers
\roundrect{1} characterizing the hardness of robust generalization due to high dimensionality, 
\roundrect{2} suggesting robust model types and configurations for high-dimensional data,
\roundrect{3} discussing the impact of high dimensionality on existing defense techniques, and
\roundrect{4} utilizing dimensionality reduction techniques for improving robustness.

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Dimensionality}.
A number of authors show that adversarial examples are inevitable in high-dimensional space.
Specifically, Gilmer et al.~\cite{Gilmer:Metz:Faghri:Schoenholz:Raghu:Wattenberg:Goodfellow:ICLR:2018} prove
this for a synthetic binary dataset composed of two concentric multi-dimensional spheres
(a.k.a. hyperspheres) in high-dimensional space ($>$100), showing that samples are, on average, closer to their nearest adversarial examples than to each other.
They also prove that the adversarial risk
of a model trained on this dataset only depends on its standard accuracy and dimensionality.
A similar result is shown by Diochnos et al.~\cite{Diochnos:Mahloujifar:Mahmoody:2018},
for a uniformly distributed boolean hypercube dataset, and
Shafahi et al.~\cite{Shafahi:Huang:Studer:Feizi:Goldstein:ICLR:2019}, for unit-hypersphere and unit-hypercube datasets.
De Palma et al.~\cite{DePalma:Kiani:Lloyd:ICML:2021} prove that irrespective of the model architecture, 
for a dataset with dimensionality $d$, the perturbation required to fool a classifier is inversely proportional 
to $\sqrt{d}$. This means that it gets easier to generate adversarial examples with an increase in dimensionality.


Another line of work analyzes the effect of dimensionality on the robustness of specific types of classifiers.
In particular,
Simon-Gabriel et al.~\cite{Simon-Gabriel:Ollivier:Scholkopf:Bottou:Lopez-Paz:ICML:2019} study feedforward neural networks
with ReLU activation functions and He-initialized weights, 
showing that a higher input dimensionality increases the success rate of adversarial attacks, regardless of the topology of the network.
The authors, however, demonstrate that regularizing the gradient norms of the network decreases the impact of the input dimension on adversarial vulnerability, thereby improving model robustness on high-dimensional inputs.
Daniely et al.~\cite{Daniely:Schacham:NeurIPS:2020} study the effect of dimensionality on ReLU networks
with random weights and with layers having decreasing dimensions.
Like Simon-Gabriel et al., the authors prove that the robustness of ReLU networks degrades proportionally to dimensionality.


Amsaleg et al.~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021}
focus on $k$-NNs and other non-parametric models that base predictions on the proximity of samples.
The authors use the Local Intrinsic Dimensionality metric to represent the intrinsic dimensionality
in the neighborhood of a particular sample $x$.
The authors build up on the observation that when this metric is high, there are more samples in close proximity of $x$
(as, otherwise, a more sparse neighborhood could be encoded in fewer dimensions).
Thus,
it is possible to arbitrarily change the neighborhood ranking of the nearest neighbor of $x$ using a small perturbation.
As predictions of proximity-based models are based on the nearest neighbor ranking, the adversarial risks increase in this setup.

All the aforementioned works are also in agreement with a number of papers discussed
in Section~\ref{sec:results-number-of-samples},
i.e., ~\cite{Dan:Wei:Ravikumar:ICML:2020,Bhattacharjee:Jha:Chaudhuri:PMLR:2021,Gourdeau:Kanade:Kwiatkowska:Worrell:JMLR:2021},
which show, in their respective settings, that sample complexity for robust generalization is proportional to dimensionality.



\vspace{0.05in}
\noindent
\roundrect{2} {\bf Model Selection and Configuration}.
%\jr{Jaskeerat-P41}
Wang et al.~\cite{Wang:Jha:Chaudhuri:ICML:2018} prove that the optimal $k$ for producing robust $k$-NN classifiers depends on the dimensionality $d$ and number of samples $n$ of the given dataset ($k = \Omega(\sqrt{dn \text{ log}(n)})$).
However, they note that for high-dimensional data, the optimal $k$ might be too large to use in practice.
The authors thus focus on improving the robustness of 1-NN algorithms through sample selection,
showing the effectiveness of their approach on 
the \halfmoon, \mnistv, and \abalone datasets.

Yin et al.~\cite{Yin:Kannan:Bartlett:ICML:2019} show that transferring a robust solution found on training data
to test data gets more difficult as the dimensionality of data increases.
However, constraining the classifier weights mitigates this problem.
Specifically, the authors prove that constraining the weights by $L_p$ norm, for $p > 1$, leads to
a performance gap between training and test data that has a polynomial dependence on dimensionality;
when the weights are constrained by $L_1$ norm, the performance gap has no dependence on dimensionality.
Li et al.~\cite{Li:Jin:Zhong:Hopcroft:Wang:NeurIPS:2022} rather focus on model configuration.
The authors show that robust generalization in networks with ReLU activation requires  
the network size to be exponential in original and intrinsic data dimensionalities, 
even in the simplest case when the underlying distribution is linearly separable.


Carbone et al.~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020} study neural networks,
showing that adversarial vulnerability arises due to the gap between the actual and intrinsic dimensionality,
a.k.a. degeneracy.
The authors show that adversarial example generations in high-dimensional degenerate data can be performed by
using gradient information of a neural network, to move the samples in the direction normal to the data manifold.
As such, example generation exploits the additional dimensions without changing the ``semantics'' of the perturbed sample.
The authors then show that Bayesian Neural Networks are more robust than other neural networks to gradient-based attacks:
due to their randomness, they make gradients less effective for crafting attacks.

\vspace{0.05in}
\noindent
\roundrect{3} {\bf Effects of Dimensionality on Defense Techniques}.
High dimensionality also poses challenges to defense techniques that aim to improve robustness.
Specifically, Blum et al.~\cite{Blum:Dick:Manoj:Zhang:JMLR:2020} focus on randomized smoothing~--
a technique that improves robustness by generating noisy instances of a (possibly perturbed) sample
and then making predictions for the sample based on an aggregation of predictions for its noisy instances.
The authors show that the amount of noise required to defend against $L_{p}$ adversaries, for $p > 2$,
is proportional to dimensionality.
They further demonstrate that, for high-dimensional images, randomized smoothing indeed fails to generate instances that preserve semantic image information.
In a similar line of work, Kumar et al.~\cite{Kumar:Levine:Goldstein:Feizi:ICML:2020} show that the certified radius decreases
as the dimensionality increases when using randomized smoothing for certifying robustness for a given $L_{p}$ radius.


Adversarial training~-- a defense technique that improves model robustness by adaptively training a model
against possible adversarial examples~-- often incurs a trade-off between standard and adversarial accuracy~\cite{Tsipras:Santurkar:Engstrom:Turner:Madry:ICLR:2019, Zhang:Yu:Jiao:Xing:Ghaoui:Jordan:ICML:2019}:
optimizing for high robust accuracy results in a drop in standard accuracy and vice versa.
Mehrabi et al.~\cite{Mehrabi:Javanmard:Rossi:Rao:Mai:ICML:2021} build up on the work of Javanmard et al.~\cite{Javanmard:Soltanolkotabi:Hassani:COLT:2020},
discussed in Section~\ref{sec:results-number-of-samples}, which 
showed that, for a finite number of training samples, 
the trade-off between adversarial and standard accuracy improves as the number of samples per dimension increases.
Mehrabi et al. further extend this result for unlimited training data and computational power, observing that,
for an unlimited number of training samples, the trade-off between adversarial and standard accuracy improves as the dimension of the data decreases.


Data augmentation is another common defense technique that aims to improve robustness of a model by creating
perturbed samples at radius $r$ from a certain subset of original samples in training data.
Rajput et al.~\cite{Rajput:Feng:Charles:Loh:Papailiopoulos:ICML:2019}
prove, for linear and certain nonlinear classifiers, that the number of augmentations required for robust generalization depends on the dimensionality of data, 
i.e., it is at least linearly proportional to dimensionality for any fixed radius $r$.
Thus, data augmentation becomes more expensive for high-dimensional data.


\vspace{0.05in}
\noindent
\roundrect{4} {\bf Reducing Dimensionality}.
Following the idea that the gap between the actual and intrinsic dimensionality contributes to adversarial vulnerability,
Awasthi et al.~\cite{Awasthi:Jain:Rawat:Vijayaraghavan:NeurIPS:2020} propose to use Principal Component Analysis (PCA)~\cite{Jolliffe:2002}
to decrease the dimensionality of data before applying randomized smoothing.
As a result, a larger amount of noise can be injected to perturb samples, thus improving robustness without compromising accuracy.
The authors apply the proposed ideas to image data, showing that the combination of PCA and randomized smoothing is more beneficial than using randomized smoothing alone.
Weber et al.~\cite{Weber:Zaheer:Rawat:Menon:Kumar:NeurIPS:2020} show, for hierarchical data,
that changing the representation from Euclidean to hyperbolic space reduces the dimensionality
without sacrificing semantic information embedded in the input data.