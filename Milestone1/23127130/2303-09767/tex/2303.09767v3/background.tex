\section{Preliminaries}
\label{sec:background}
We now provide a brief overview of the main concepts related to machine learning,
adversarial robustness, and most commonly studied data distributions.
The goal of this section is to introduce terminology used in the rest of the survey rather than provide an extensive overview of the adversarial robustness research area.
For a more detailed overview, please refer to guides on statistics and machine learning~\cite{Shalev-Shwartz:Ben-David:2014, Giuseppe:2017, Hastie:Tibshirani:Friedman:2009} and adversarial robustness~\cite{Chen:Hsieh:2023, Biggio:Roli:PR:2018, Nicolae:Sinn:Tran:2018:ArXiv}. 
Please also see our online appendix for the list of symbols and acronyms used in this paper~\cite{appendix}.


\subsection{Machine Learning}
Machine learning refers to the automated detection of meaningful patterns in data~\cite{Shalev-Shwartz:Ben-David:2014}
and can be largely divided into supervised, unsupervised, and reinforcement learning.
In supervised learning, a learning model is provided with input-output pairs of data (a.k.a. labeled training data);
based on this data, the model aims to infer a function that maps the inputs to the outputs.
Supervised learning is typically associated with classification and regression problems, which use categorical and continuous labels, respectively. 
In classification, this number of possible labels for an input is also referred to as the number of \emph{classes}.
Datasets with only two classes are called \emph{binary datasets}, on which one can train a \emph{binary classifier}.

Unlike supervised learning, unsupervised learning algorithms are usually concerned with identifying patterns in unlabeled data, e.g.,
grouping similar samples together in the absence of labels (clustering) or
transforming data into a different representation (representation learning).
Reinforcement learning characterizes algorithms that learn from a series of rewards and punishments,
with the goal of maximizing the cumulative reward, e.g., to build robots that learn to take the best sequence of actions according to signals from the environment.

Variations, such as, semi-supervised learning (i.e., learning from partially labeled data) and
self-supervised learning (i.e., learning from labels extracted by the learner itself) have also been proposed for problems where acquiring labeled data may be challenging or expensive.


ML algorithms can also be divided into parametric and non-parametric.
Parametric algorithms have a predetermined, fixed number of parameters defined before the training starts.
For example, for Linear Support Vector Machines (SVMs), these parameters are the coefficients of all features of the
training data and the learned intercept.
For Deep Neural Networks (DNNs), the number of parameters is determined by the architecture of the network.
In non-parametric algorithms, the number of parameters is determined at training time and may vary depending on the number of training samples.
For example, the ``depth'' of Decision Trees can grow (beyond the size of the feature set)
when more decision points are needed to accurately separate training data.
Other commonly used non-parametric models include $k$-Nearest Neighbors ($k$-NN) and Kernel SVMs.

%\vspace{-0.2in}
\subsection{Adversarial Robustness}
Adversarial machine learning studies the arms race between adversarial attacks and defenses.
Attacks aim at degrading model performance while defenses propose algorithms to harden models against the attacks.
Adversarial attacks can be categorized into \emph{evasion} and \emph{poisoning}~\cite{Biggio:Roli:PR:2018,Li:Li:Ye:Xu:CSUR:2021}.
Evasion attacks aim to fool machine learning models by generating inputs that, despite no noticeable difference for a human,
will be incorrectly classified.
Such inputs, known as \emph{adversarial examples} and created by applying non-random perturbations to samples, are carefully designed to change models' predictions~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014,
Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Carlini:Wagner:SP:2017}.
Instead, poisoning attacks tampers with model training data, in order to degrade model performance.
In this survey, we focus on evasion attacks; the terms adversarial and evasion attacks are often used interchangeably in the literature as this is the most popular and commonly studied type of attack.


The term \emph{robustness} for machine learning models is often used to refer to different concepts, such as,
stability to distribution shifts,
the ability to identify adversarial examples, and
the ability to make the correct predictions in the face of adversarial examples.
In this survey, we use the latter definition -- the ability to make the correct predictions in the face of adversarial examples.
This is a stronger notion of robustness than merely identifying adversarial examples, as the identification of an adversarial example does not guarantee its correct classification.
The phenomenon of making satisfactory model predictions in the face of adversarial examples is also often referred to as \emph{robust generalization}.
This is different from \emph{standard generalization}, a term used to describe
making satisfactory model predictions for normal, unseen samples.

\vspace{0.05in}
\noindent
{\bf Adversarial (Evasion) Attacks.}
Techniques for generating adversarial examples for evasion attacks can be broadly divided into three categories,
according to the type of information available to the attacker~\cite{Biggio:Roli:PR:2018}.
In \emph{white-box attacks}, the attacker is assumed to be able to leverage all available information about the training data, the model, and the training procedure.
In \emph{grey-box attacks}, the attacker is assumed to have only partial information about the model, such as, the source of training data.
Finally, the most conservative type of attacks are \emph{black-box attacks}, where the attacker has no information about the inner workings of the model except, possibly, for the prediction outcomes.

\emph {Gradient-based attacks} are commonly used in white-box settings. These attacks use the gradient of a differentiable function defined over model weights as a guide when crafting adversarial examples.
The most commonly used differentiable function is the loss function used by the model during training.
A gradient defines the direction of the maximal increase in the local value of a function.
Hence, by using the gradient of the loss function with respect to the input, one can adjust the input to get the maximal increase in the loss of the model, which ultimately leads to a bad prediction.
Fast Gradient Sign Method (FGSM)~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015}, Basic Iterative Method (BIM)~\cite{Kurakin:Goodfellow:Bengio:ICLR:2017}, and Projected Gradient Descent (PGD)~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018} are examples of attack algorithms that utilize the gradient of the loss function used for training.
Instead of the loss function, the FAB attack~\cite{Croce:Hein:ICML:2020:1} uses the gradient of a function defined by the difference of model outputs of the penultimate layer of a neural network -- a layer which outputs the probabilities that a given sample belongs to each of the available classes.
By defining the difference of outputs of the penultimate layer as the differentiable function, the FAB attack maximizes the difference in probabilities between the target class and other classes, to increase the chance of misclassification.


\emph{Non-gradient based attacks} are applicable for more diverse types of models that do not use a differentiable functions, e.g., decision trees.
Such attacks can also be used in black-box and grey-box settings, when gradient information is hidden from the attacker.
One example of non-gradient-based attacks is the \emph{mimicry attack}, which involves adding and removing features in the perturbed sample, e.g., based on their popularity in the target class~\cite{Demontis:Melis:Biggio:Maiorca:Arp:Konrad:Rieck:Corona:Giacinto:Roli:TDSC:2019}.

\vspace{0.05in}
\noindent
{\bf Adversarial Defenses.}
Defense mechanisms against adversarial attacks target various stages of the machine learning pipeline.
Specifically, defenses \emph{on raw data} focus on the training data itself, e.g.,
by selecting a subset of ``robust'' features~\cite{Ilyas:Santurkar:Tsipras:Engstrom:Tran:Madry:NeurIPS:2019} or
using representation learning to transform features into a different representation, making sure
a model trained on the new representation is inherently more robust~\cite{Yang:Guo:Wang:Xu:AAAI:2021}.


Defenses \emph{during training} alter the standard training procedure to improve model robustness.
The most common such technique is adversarial training~\cite{Goodfellow:Shlens:Szegedy:ICLR:2015}, which involves continually augmenting the training data with adversarial examples generated by an attack algorithm.
By retraining the model while adding correctly labeled malicious samples to the training dataset, the model learns to capture persistent patterns and becomes more robust against these attacks.
Another common method is \emph{regularization}, where model parameters are constrained so that very small perturbations have little effect on the prediction outcome~\cite{Gouk:Frank:Pfahringer:J.Cree:ML:2021}.

	
Defenses \emph{during inference} focus on making existing models more robust when the model is being used on new samples.
For example, randomized smoothing~\cite{Cohen:Rosenfeld:Kolter:ICML:2019} involves creating multiple noisy
instances of a sample and aggregating the model's predictions during inference.
Given that adversarial examples are typically close to genuine samples, averaging the results from close neighbors of an input can potentially reduce the chances of the model being misled.
In addition, different variations of ensemble models~-- using multiple models and aggregating their output~--
have been shown to increase the robustness to adversarial attacks~\cite{Pang:Xu:Du:Chen:Zhu:ICML:2019}.



\vspace{0.05in}
\noindent
{\bf Measures of Robustness.}
The strength of an adversary is mostly measured by the size of the perturbation required to create an adversarial example.
That is, adversaries that introduce more perturbations, e.g., change a larger portion of pixel values in the image, are considered to be stronger.
A typical way of measuring the perturbation size, especially in the image domain,
is by using the $L_p$ distance metric, where $p$ can be a whole number or $\infty$.
Specifically,
$L_0$ counts the total number of changed features, regardless of the changes to individual features.
$L_1$ is the Manhattan distance, i.e., the sum of absolute values representing a change in each feature.
$L_2$ measures the Euclidean distance between the feature values of the original and perturbed samples.
The $L_\infty$ metric measures the largest change in any of the features
(while disregarding changes in all other features).

There are two ways to utilize these distance metrics to evaluate the robustness of a model: error-rate-based and radius-based.
The first calculates a pool of adversarial samples generated from a set of real samples with a fixed allowable perturbation size~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018}.
The robustness is then defined as the error rate of the model on these adversarial samples.
A related concept, \emph{adversarial risk}, is also defined in a similar manner:
the probability of finding, within a certain predefined distance, an adversarial example for a given real sample.
Radius-based evaluation measures the smallest distance required to generate an adversarial sample from a given real sample~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014}.
This way is especially useful in robustness certification, which involves learning a classifier that outputs a prediction along with a certified radius within which the prediction is guaranteed to be consistent~\cite{Lee:Yuan:Chang:Jaakkola:NeurIPS:2019}.

\subsection{Data Distributions}
\label{sec:background_distribution}
Numerous works study properties of particular data distributions.
The \emph{uniform distribution} defines a probability distribution in which every possible data point is equally likely.
This implies that for a continuous random variable in the interval $[a, b]$, the probability of seeing a sample from the interval is $\frac{1}{b-a}$ and the probability of seeing a sample from outside of the interval is $0$.
In the discrete case with $n$ possible values, the uniform distribution assigns a probability of $\frac{1}{n}$ to each value.
The \emph{Bernoulli distribution} defines a discrete probability distribution of a random variable with two allowable values, 0 and 1.
Such a random variable takes the value of 1 with probability $p$ and the value of 0 with probability $1-p$.
The \emph{Gaussian (normal) distribution} defines a continuous probability distribution that assigns a probability with its peak at the center of the distribution and decreasing symmetrically outwards.
For a Gaussian distribution, $\mu$ denotes the mean or center of the distribution, and $\sigma^2$ denotes the variance or the spread of the distribution.
Since the mean and variance fully characterize a Gaussian distribution, it is also commonly denoted as $\mathcal{N}(\mu,\sigma^2)$.

\begin{wrapfigure}{r}{0.25\textwidth}
\vspace{-0.15in}
	\centering
	\includegraphics[width=0.2\textwidth]{images/gaussian_mixture_intersect.pdf}
	\vspace{-0.1in}
	\caption{A two-dimensional Gaussian mixture data.}
	\label{fig:gaussian_mixture}
\vspace{-0.2in}
\end{wrapfigure}
One can also imagine a distribution made up of a \emph{mixture} of multiple distributions.
For example, Fig.~\ref{fig:gaussian_mixture} shows a distribution made up of two Gaussians: one centered at $\mu_1$ and another~-- at $\mu_2$.
This mixture also contains labels associated with each independent Gaussian, shown by the two clusters in the figure.
Furthermore, these two clusters have the same variance,
i.e., the same spread of the  distribution surrounding the center of the class.
While the means of the two classes are separated, the distributions intersect with each other.
