\subsection{Domain-Specific}
\label{sec:results-domain-specific}

Papers in this category provide insights into the correlation between domain-specific data properties and adversarial robustness.
Among our collected papers, all the domain-specific studies focused on the same topic: understanding the adversarial vulnerabilities of image classifiers based on image \emph{frequency}~--
how fast the intensity of pixel values changes with respect to space (i.e., images with intensive color changes have high-frequency).
As shown in Fig.~\ref{fig:image_freq}, the skin of a zebra has higher image frequency than a horse, because of the black-and-white stripes.
Papers studying image frequency are listed in Fig.~\ref{fig:domain}.
They can be roughly divided into:
\roundrect{1} papers discussing the influence of frequency distribution on the model adversarial robustness,
and~\roundrect{2}~papers explaining adversarial vulnerabilities using perceptual differences between humans and models. 

\begin{figure*}[b]
  \centering
  \vspace{0.05in}
  \begin{minipage}{0.37\textwidth}
	  \centering
	  \vspace{0.05in}
	  \includegraphics[valign=t, width =\textwidth]{images/domain_specific_frequency.pdf}
	  \caption{Image frequency.}
 	  \vspace{-0.13in}
	  \label{fig:image_freq}
  \end{minipage}
  \begin{minipage}{0.62\textwidth}
		\centering
		  \vspace{-0.2in}
		\includegraphics[valign=t, width =\textwidth]{images/domain_specific_citation_graph.pdf} 
		\vspace{-0.07in} 
		\caption{Papers discussing domain-specific properties. }
		\label{fig:domain}
  \end{minipage}
  \vspace{-0.05in}
\end{figure*}

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Image Frequency}.
Wang et al.~\cite{Wang:Wu:Huang:Xing:CVPR:2020} attribute adversarial examples to the perceptual differences between humans and ML models in frequency ranges: 
while high-frequency components are not visible to the human eye and 
humans mostly classify images based on low-frequency components, 
models can utilize both 
ranges, allowing them to create adversarial examples imperceptible to humans in high-frequency ranges.
The authors propose to use smoother convolutional filters to reduce a model's attention to high-frequency components, 
thus improving robust generalization.

Deng and Karam~\cite{Deng:Karam:ECCVWorkshops:2020, Deng:Karam:TIP:2022} also acknowledge and 
experimentally demonstrate the difference between the human and model perspectives. 
The authors focus on universal adversarial perturbations, 
i.e., input-agnostic perturbations that cause label change for most inputs sampled from the data distribution,
noting that such perturbations can be created by exploiting human sensitivity to different frequency ranges.
They use this observation to generate high-success adversarial example by utilizing the Just-Noticeable-Difference~\cite{Albert:Heidi:HVVPD:1997,Hontsch:Karam:TIP:2002} metric, 
which approximates the maximal imperceptible perturbation along different frequency ranges.
Through their experiments, the authors demonstrate that humans are generally more sensitive 
to changes in low-frequency rather than high-frequency components.


Zhang et al.~\cite{Zhang:Benz:Karjauv:Kweon:AAAI:2021} 
observe that successful universal adversarial perturbations are dominated by perturbations in high-frequency components. 
The authors also show that such perturbations cause more ``distinct'' changes in  images with more low-frequency components,
making them more susceptible to universal adversarial perturbations.
The same observation is supported by 
Chen et al.~\cite{Chen:Ren:Yan:NeurIPS:2022}, who use an explainability technique to attribute predictions to data:  
as the adversarial training process is focusing on high-frequency components, 
adversarially trained models rely more heavily on low-frequency components while 
standard models utilize both. 
Yet, the authors show that using high-frequency components is necessary to accurately predict 
certain classes. As such, when the model learns to prioritize low-frequency components through adversarial training, 
the accuracy for high-frequency images is compromised.  



Yin et al.~\cite{Yin:Lopes:Shlens:Cubuk:Gilmer:NeurIPS:2019} outline another disadvantage 
of techniques that generate perturbations with high-frequency components, 
such as adversarial training and Gaussian data augmentation techniques:
they result in models more vulnerable to low-frequency perturbations.
Similarly, Ortiz-Jimenez et al.~\cite{Ortiz-Jimenez:Modas:Moosavi-Dezfooli:Frossard:NeurIPS:2020}
show that CNN models tend to have smaller margins along low-frequency vs. high-frequency ranges and 
that adversarial training results in significantly larger margins against high-frequency perturbations, 
making the model more vulnerable to low-frequency perturbations. 


Sun et al.~\cite{Sun:Mehra:Kailkhura:Chen:Hendrycks:Hamm:Mao:ECCV:2022} also observe the issue 
of low robustness against low-frequency perturbations. Yet, in this case, it is caused by 
using an inference-time technique (randomized smoothing) rather than by adversarial training. 
The aforementioned works propose to mitigate these issues by increasing the diversity of frequency distribution in training data.


\vspace{0.05in}
\noindent
\roundrect{2} {\bf Effects of Other Image Properties}.
Unlike the work above that focuses on image frequency,
Chen et al.~\cite{Chen:Peng:Ma:Li:Du:Tian:ICCV:2021} posit that the adversarial vulnerability of CNNs
results from their over-reliance on amplitude information of images~--
the magnitude of the different frequencies in the image.
The authors show that replacing the amplitude information of an image with information from another image can successfully mislead CNNs but not humans, who rather rely on phase information~-- the locations of the features, to recognize objects.
Based on this observation, the authors propose to strengthen CNNs' attention to phase information through a data augmentation technique that fuzzes amplitude while preserving the same phase information.

Focusing on medical image classification, Ma et al.~\cite{Ma:Niu:Gu:Wang:Zhao:Bailey:Lu:PR:2021} show that image classifiers pre-trained on natural image datasets, such as ImageNet, have lower adversarial robustness when applied to medical images. 
The authors attribute this degradation to the distinct biological textures and relative simplicity of medical images compared to natural ones. 
They argue that models designed for natural images have a high likelihood of over-fitting to 
noise present in non-lesion areas and,
consequently, a higher susceptibility to adversarial perturbations in those regions. 
