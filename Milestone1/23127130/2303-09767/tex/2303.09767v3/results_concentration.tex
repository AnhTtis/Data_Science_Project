\subsection{Concentration}
\label{sec:results-concentration}

\emph{Concentration} of a dataset refers to the ``concentration of measure'' phenomenon from measure theory~\cite{Talagrand:1996:AnnalsProbability}.
In a nutshell, concentration is the minimum value of a measured function over all valid measurable sets,
after an $\epsilon$-expansion.  
More formally,
for a metric probability space $(\mathcal{X}, \mu, d)$ with instance space $\mathcal{X}$, probability measure $\mu$, and distance metric $d$,
the concentration function $h$ is defined as: $h(\mu, \alpha, \epsilon) = $ inf$_{A \subseteq \mathcal{X}} \{\mu(A_\epsilon) : \mu(A) \geq \alpha\}$ for any $\alpha \in (0,1)$ and $\epsilon \geq 0$~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019}.
Here $A_\epsilon$ refers to the $\epsilon$-expansion of set $A$, defined as $A_\epsilon = \{ x : d(x, A) \leq \epsilon\}$.


\begin{figure*}[h]
  \centering
  \begin{minipage}{0.40\textwidth}
  \centering
  \vspace{-0.08in}
  \includegraphics[width =0.82\textwidth]{images/concentration_guideline.pdf}
  \vspace{-0.12in}
  \caption{Concentration illustration.}
  \label{fig:concentrationFig}
  \end{minipage}
  \begin{minipage}{0.56\textwidth}
  \centering
  \includegraphics[width =0.9\textwidth]{images/concentration_vertical_legend_wide.pdf} 
  \vspace{-0.1in}
  \caption{Papers discussing concentration. }
  \label{fig:concentration}
  \end{minipage}
  \vspace{-0.1in}
\end{figure*}

Fig.~\ref{fig:concentrationFig} shows how the concentration of measure phenomenon can be used to determine the classification error after adversarial perturbation.
By modeling the classification error set as measurable set $A$
and adversarial errors from perturbation budget $\epsilon$ as $A_{\epsilon}$,
one can relate the concentration of the data to the minimum adversarial risk for any imperfect classifier with error rate $\mu(A) \geq \alpha$.
Using this formulation, a dataset being highly concentrated implies that, for some non-zero initial error, the minimum adversarial risk from an $\epsilon$-expansion on the error set is very large.
We refer to such datasets as datasets with low \emph{intrinsic robustness}~--
a measure that represents the maximal achievable robustness for any classifier on a dataset.


Fig.~\ref{fig:concentration} shows the papers that relate data concentration to adversarial robustness.
They can roughly be divided into:
\roundrect{1} papers discussing the effect of concentration on robustness and
\roundrect{2} papers proposing techniques to estimate robustness through calculating concentrations.


\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Concentration}.
A number of papers prove the inevitability of adversarial examples
using the concentration of measure phenomenon. 
In particular, Dohmatob~\cite{Dohmatob:ICML:2019} investigates datasets
conforming to uniform, Gaussian, and several other distributions
that satisfy $W_2$ transportation-cost inequality~\cite{Talagrand:1996}.
The author proves that data distributions satisfying such inequality have high concentration,
which results in a rapid robustness decrease, beyond a critical perturbation size~--
a value that depends on the standard error of the classifier and the natural noise level of the dataset, which, in turn,
is defined as the largest variance in the case of Gaussian distribution.
Even though \mnist might not satisfy the $W_2$ transportation-cost inequality,
the author experiments with this dataset, observing a sudden drop in robustness
as the perturbation size increases.
As such, the author suggests that the \mnist dataset may also have high concentration and be
governed by the concentration of measure phenomena.


Mahloujifar et al.~\cite{Mahloujifar:Diochnos:Mahmoody:AAAI:2019} focus on a collection of data distributions with high concentration called L\'evy families~\cite{Levy:1951}, which include unit sphere, unit cube, and  isotropic n-Gaussian
(i.e., Gaussian with independent variables with the same variance).
The authors prove that classifiers trained on such highly-concentrated data distributions admit adversarial examples with perturbation
$\mathcal{O}(\sqrt{d})$  for dimensionality $d$.
This implies that a relatively small perturbation can mislead models trained on these data distributions with high dimensional inputs.


\vspace{0.05in}
\noindent
\roundrect{2} {\bf Estimating Robustness Through Concentration}.
Several approaches utilize the connection between concentration and adversarial risk to estimate 
the intrinsic robustness of datasets by calculating their concentrations.
Mahloujifar et al.~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019} are the first to propose an approach for estimating
dataset concentration using subsets of samples.
Specifically, the authors propose a technique that searches for the minimum expansion set based on a collection of subsets carefully chosen according to the perturbation norm (e.g., a union of balls for $L_2$ norm).
They prove that
the estimated concentration value converges to the true value for the underlying distribution as the sample size and the quality/representativeness of the chosen subsets increase.
The authors apply their approach for estimating the maximum achievable robustness for the \mnist and \cifarten datasets,
observing a gap between the derived theoretical values and values observed empirically by the state-of-the-art models.

In follow-up work, Prescott et al.~\cite{Prescott:Zhang:Evans:ICLR:2021} propose
an alternative approach to estimate concentration based on half space expansion using \emph{Gaussian Isoperimetric Inequality}
for the $L_2$ norm~\cite{Borell:InvMath:1975}.
The authors further generalize their results to $L_p$ norms, where $p \geq 2$.
Compared with Mahloujifar et al.~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019},
their approach yields higher achievable robustness on \mnist and \cifarten, revealing a larger gap between the theoretical robustness and the state-of-the-art.
As the theoretically achievable robustness derived from a concentration perspective is shown to be high,
the authors suggest that factors other than concentration may contribute to this gap.


Zhang and Evans~\cite{Zhang:Evans:ICLR:2022} assume access to information about label uncertainty,
i.e., a function that assigns the level of label uncertainties for any data point.
Such a function can use, e.g., labeling results from multiple human annotators or confidence scores from an ML classifier.
The authors suggest that considering regions with high label uncertainty can guide the concentration estimation
as these are the regions where a classifier is more likely to make mistakes and be vulnerable to attacks.
They thus propose an approach to estimate concentration by identifying the smallest set after
$\epsilon$-expansion with an average uncertainty level greater than a pre-set value. 
The evaluation results show that the maximum achievable robustness estimated with their approach
is closer to the robustness values observed for CNN models on the \cifarten dataset than in
any of the aforementioned works, implying that the
room for improvement is smaller than assumed earlier.
