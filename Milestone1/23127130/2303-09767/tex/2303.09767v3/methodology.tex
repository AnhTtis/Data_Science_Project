\section{Methodology}
\label{sec:methodology}
This section describes our methodology for identifying and categorization relevant papers.

\subsection{Paper Collection}
Papers for this survey were collected in May 2023.
We used the search query schematically described below,
which was designed to identify papers in the area of Machine Learning adversarial robustness that
discuss properties of the underlying data.
We expanded each of these conceptual terms with possible synonyms and specific wording, making sure our query is as comprehensive as possible.

\begin{quote}
  \textbf{Search Query} :=  Machine Learning + Adversarial Robustness + Data + Property  \\
  \textbf{Machine Learning} := classif | ``machine learning'' | ``deep learning'' | ``neural network'' \\
  \textbf{Adversarial Robustness} := ``adversarial robustness''  | ``adversarial vulnerability'' | ``adversarial attack'' | ``adversarial perturbation'' | ``adversarial defense'' | ``evasion attacks''  \\
    \textbf{Data} := data | sample | input\\
  \textbf{Property} := propert | qualit | distribution | characteristic
\end{quote}

In our schematic query representation, the ``+'' and ``|'' signs indicate the AND and OR operators, respectively,
the phases in quotes are matched in full, and
each word is matched with its suffixed versions, e.g., `classif' is matched with both `classifier' and `classification'.
When performing the search, we adapted this schematic query to the requirements and capabilities of each search engine that we used.

An initial search using the query in Google Scholar identified more than 30,000 matches.
To keep the scope of the survey manageable, we thus limited our search to publications from the main track of top-tier conferences and journals in the areas of Machine Learning (ML), Computer Vision (CV), Computational Linguistics (CL), and Security (SEC).
Specifically, we selected all A* conferences from these areas using the most recent, 2021, CORE ranking~\cite{CoreRanking:website:2022};
the top 15 journals according to the Journal Citation Reports (JCR)~\cite{JournalRanking:website:2022}
in the area of Artificial Intelligence (AI),
which includes Machine Learning, Computer Vision, and Computation Linguistics,
and the top five journals in the area of Information System (IS), which includes Security.
Additionally, we included the ACM Computing Surveys Journal~\cite{ACM:Computing:Surveys:Journal} to collect surveys related to our topic.
The first three columns in Table~\ref{tbl:venues} show the final list of publication venues that we selected.

We further identified digital libraries and search engines that host proceedings of our selected venues.
These are shown in the next five columns of Table~\ref{tbl:venues}, for each venue individually.
As some venues are only partially indexed by the digital libraries, 
i.e., the libraries only include proceedings from particular years, we augmented our search for papers in
these venues with a secondary search using Google Scholar.
More specifically, we used Google Scholar's \emph{site} and \emph{source} filtering constraint
to limit our search only to the target venues of interest,
as described in our online appendix~\cite{appendix}.
We also used Google Scholar to search the ArXiv repository and filtered the results using 
\emph{publication venue} information provided by the Semantic Scholar API~\cite{SemanticScholar:api:2022}.

\input{venuesTbl}

The second to last column in Table~\ref{tbl:venues} shows the number of hits for each publication venue identified
by this search. In total, using this procedure, we identified 6,390 papers;
after removing duplicates, we obtained a set of 4,359 potentially relevant papers.

\subsection{Manual Filtering}
Next, we manually classified papers identified in the automated search into relevant and irrelevant for our survey.
To this end, we first randomly selected a set of 40 papers and used them as a pilot for drafting the inclusion and exclusion criteria.
Four authors of this survey independently read the abstract, introduction, and conclusions of each paper and classified it as \emph{relevant}, \emph{non-relevant}, or \emph{unsure}.
Each author also assigned a concise label for each of the \emph{relevant} and \emph{non-relevant} papers, specifying the reasons for inclusion/exclusion.

After completing this phase, all authors of this survey met to cross-validate the decisions,
and to consolidate and refine the inclusion/exclusion labels.
All disagreements between the authors (mostly between \emph{non-relevant} and \emph{unsure} papers) were resolved through a joint discussion.

In the second phase, we randomly selected an additional set of 40 papers, to validate our filtering process.
Four authors of this survey, again, independently read and categorized each of these papers, and all authors met to discuss the results.
While there were disagreements, with a rate of 6.25\%, between the assignment of \emph{non-relevant} and \emph{unsure} papers,
the inclusion/exclusion labels were consistent among the raters.

Specifically, we included papers that:
\vspace{-0.05in}
\begin{enumerate}
\item[(a)] study the correlation between properties of input data and the adversarial robustness of resulting model trained on this data; and/or
\item[(b)] present techniques to improve or disrupt a model's adversarial robustness through explicitly modifying some properties of the input data or its latent representation.
\end{enumerate}

We excluded papers that:
\vspace{-0.05in}
\begin{enumerate}
\item[(a)] discuss adversarial evasion attacks, but focus on features~\cite{Tong:Li:Hajaj:Chen:Xiao:Zhang:Vorobeychik:USENIX:2019}, models~\cite{Ghosh:Losalka:Black:AAAI:2019,  Xu:Chen:Liu:Chen:Weng:Hong:Lin:IJCAI:2019}, and training algorithms~\cite{Guo:Chen:Chen:Zhang:TPAMI:2021, Jeong:Shin:NeurIPS:2020} 
rather than data (44.2\%);
\item[(b)] discuss aspects of ML that are not related to adversarial robustness but rather related to
    accuracy~\cite{Ansuini:Laio:Macke:Zoccolan:NeurIPS:2019,Zhu:Jin:TNNLS:2020,Pope:Zhu:Abdelkader:Goldblum:Goldstein:ICLR:2021},
    robustness to distribution shifts not induced by adversaries~\cite{Mintun:Kirillov:Xie:NeurIPS:2021,Xu:Zhang:Zhang:Wang:Tian:CVPR:2021},
	privacy~\cite{He:Yang:CCS:2021, Hu:Salcic:Sun:Dobbie:Yu:Zhang:CSUR:2022}, and
	interpretability~\cite{Ghorbani:Abid:Zou:AAAI:2019, Guo:Mu:Xu:Su:Wang:Xing:CCS:2018} 
	(41.3\%); 
\item[(c)] propose new robustness evaluation metrics~\cite{Cheng:Deng:Zhao:Cai:Zhang:Feng:ICML:2021}
	or assessment frameworks~\cite{Ling:Ji:Zou:Wang:Wu:Li:Wang:SP:2019} (3.1\%);
\item[(d)] focus on poisoning~\cite{Zhang:Zheng:Gao:Miao:Su:Li:Ren:IJCAI:2019, Liu:Si:Zhu:Li:Hsieh:NeurIPS;2019} rather than evasion attacks (2.7\%); or
\item[(e)] discuss unrelated topics, e.g.,
	literature on blockchain~\cite{Huang:Kong:Zhou:Zheng:Guo:CSUR:2021},
	remote access trojan systems~\cite{Rezaeirad:Farinholt:Dharmdasani:Pearce:Levchenko:Macoy:USENIX:2018},
	or hardware systems~\cite{Tan:Wan:Zhou:Li:SP:2021}.
	These papers appeared in our search results as ``adversarial robustness'' is also desirable for non-ML systems,
	e.g., blockchain systems need to be robust against adversarial selfish miners or Denial-of-Service attacks 
	(5.8\%).
\end{enumerate}

As the inclusion/exclusion labels were consistent among the raters, we decided to proceed to the next phase: distribute the remaining 4,279 papers among the four authors and categorize them using these inclusion and exclusion criteria. In this phase, we instructed each rater to conservatively mark as \emph{unsure} papers with even a slight doubt in categorization.

Following this process, we identified 277 papers as either \emph{relevant} or \emph{unsure}.
We assigned a second reader to papers marked as \emph{unsure}, summarized each such paper in writing, and held a meeting with all the authors of this survey to categorize the paper as either \emph{relevant} or \emph{non-relevant}.
For a select set of papers where we could not confidently reach a decision, we emailed the paper authors to validate our understanding and decide on the relevance of the work (all such papers were excluded in the end).
Our analysis resulted in 71 \emph{relevant} papers. The distribution of these papers by publication venues is shown in the last column of Table~\ref{tbl:venues}.

We further assigned a second reader to each identified relevant paper, to extract and summarize its main findings.
To make sure that we included most of the relevant works on the topic, we also performed backward snowballing 
using the related work sections of the selected papers, which resulted in 6 additional papers, 
bringing our selection to 77 papers in total.
Fig.~\ref{fig:collection} summarizes our paper selection process. A more detailed view is also available online~\cite{appendix}. 

\begin{figure*}[b]
\centering
    \vspace{-0.15in}
    \begin{minipage}{0.6\textwidth}
		\centering
		\includegraphics[scale=0.4]{images/paperCollectionStatsApr2023.pdf}
		\caption{Summary of the collection and selection of papers.}
		\label{fig:collection}
		\vspace{-0.1in}
    \end{minipage}\hfill
    \begin{minipage}{0.38\textwidth}
        \centering
        \includegraphics[width=0.78\textwidth]{images/Selected_paper_by_venue_update.pdf} % first figure itself
%        \vspace{-0.1in}
        \caption{Breakdown by publication venues.}
        \label{fig:selection_by_venue}
    \end{minipage}
    \vspace{-0.05in}
\end{figure*}

Fig.~\ref{fig:selection_by_venue} shows another view on the distribution of publication venues for all 77 papers included in our survey.
The majority of the papers (64, 83\%) are published in Machine Learning venues.
In fact, only the \emph{Advances in Neural Information Processing Systems (NeurIPS)} conference published 25 (around 32\%)
of all papers.
The distribution of selected papers by their publication year can be found in our online appendix~\cite{appendix}. 

\begin{figure*}[b]
	\centering
	\vspace{-0.2in}
	\includegraphics[width=13.6cm, height=3.6cm] {images/paperCategorization.pdf}
	\vspace{-0.3in}
	\caption{Paper categorization dimensions.}
	\label{fig:categorization}
\end{figure*}

\subsection{Categorization of Selected Papers}
To better classify, discuss, and compare the papers, we proposed a categorization schema shown in Fig.~\ref{fig:categorization}.
To construct the schema, we followed an iterative process similar to the one we used for paper selection:
we first sampled ten papers from the final collection and each of the four authors independently proposed categorization attributes to describe these ten papers.
The proposed attributes were discussed by all authors while unifying related attributes, removing redundant ones,
and updating labels.
We then verified the applicability of the constructed schema on another set of ten papers and adjusted it based on a joint discussion.
We continued extending the schema after reading the remaining papers, to ensure its inclusiveness.

The resulting categorization schema, which we further use to analyze and describe the papers,
contains three high-level areas described below.
The detailed categorization of each papers along the attributes of this schema is available online~\cite{appendix}.

\vspace{0.02in}
\noindent
\textbf{1. Problem Setup} defines the scope of the proposed approach and the assumptions authors make in their work. Specifically,
\vspace{-0.05in}
\begin{itemize}
	\item \emph{Target Distribution} describes the type of data distributions the papers focus on.
	The common data distributions studied among the collected papers include Gaussian mixtures, Bernoulli mixtures, and Uniform distributions.
	
	\item \emph{ML Model} focuses on the studied model.
	It captures the \emph{learning task}, e.g., binary classification, multi-class classification, and regression, and
	\emph{classifier type}, e.g., parametric models, such as, neural networks and non-parametric models, such as, $k$-NNs.
	
	\item \emph{Robustness Setting} records the paper's definition of adversarial robustness.
	This includes the \emph{robustness definition} sub-category, which refers to how the authors measure robustness,
	e.g., error-rate-based or 
	radius-based. 
	The second sub-category, the \emph{attacker's knowledge}, reflects the level of information about the target system that the attacker can exploit: white-box, grey-box, or black-box.
	The last sub-category, the \emph{attack}, characterizes the \emph{technique} used to construct the attack,
	e.g., gradient- or non-gradient based, and the \emph{perturbation bound} considered, e.g., the type of $L_p$ norm.
\end{itemize}


\vspace{0.02in}
\noindent \textbf{2. Data Property} dimension includes the eight data properties we identified in the collected papers:
the \emph{number of samples}, data \emph{dimensionality}, \emph{distribution}, \emph{density},
\emph{concentration}, \emph{separation}, \emph{label quality}, and \emph{domain-specific} properties,
relevant in context of particular application domains.
We introduce and structure the discussion of the surveyed papers in Section~\ref{sec:results} around these properties.

\vspace{0.02in}
\noindent \textbf{3. Practicality} specifies how to apply the approach or technique introduced in each paper: 
\begin{itemize}
	\item \emph{Applicability} determines whether specific \emph{quantitative} metrics are provided to measure the data properties discussed in the paper or whether there are any concrete techniques proposed to modify these data properties.
	
	\item \emph{Explainability} determines whether the paper focuses on explaining (rather than establishing) the correlation between data property and robustness.
	
	\item \emph{Type of Evidence} records the type of arguments provided by the paper.
	This may be a \emph{formal proof}, an \emph{empirical evaluation}, or a combination of both.
    For cases where an empirical evaluation is performed, we also collect information about \emph{datasets} and \emph{classifiers} used, the applied \emph{training procedures} (standard vs. adversarial training), and the \emph{attack techniques} employed.
\end{itemize}

In what follows, we present the results of our analysis of the surveyed papers (Section~\ref{sec:results}) and
discuss our observations (Section~\ref{sec:discussion}).
