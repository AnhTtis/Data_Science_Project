\section{Implications and Future Work}
\label{sec:discussion}

We now summarize the practical implications of our findings, 
extracting guidelines and techniques for improving adversarial robustness by 
manipulating data and learning procedures, 
as well as techniques for estimating best-case robustness for particular data (Section~\ref{sec:discussion-practical}). 
We then outline the gaps in research and possible directions for future work (Section~\ref{sec:discussion-future}).

\subsection{Practical Implications}
\label{sec:discussion-practical}

\noindent {\bf Improving Robustness by Manipulating Data.}
We identified two main directions for improving robustness through data manipulation techniques:

\vspace{0.02in} 
\emph{1. Increasing datasets size and diversity.}
Collecting a large number of real labeled samples required to train adversely robust models could be challenging. 
Our survey shows that one could employ cheaper data collection alternatives, such as unlabeled or generated samples, 
and then use semi-supervised learning techniques, such as pseudo-labeling, to learn from such samples~\cite{Uesato:Alayrac:Huang:Stanforth:Fawzi:Kohli:NeurIPS:2019,Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019,
	Najafi:Maeda:Koyama:Miyato:NeurIPS:2019,Gowal:Rebuffi:Wiles:Stimberg:Calian:Mann:NeurIPS:2021}.	
Moreover, data dimensionality can be used to guide the number of augmented/generated samples 
required to improve robustness~\cite{Rajput:Feng:Charles:Loh:Papailiopoulos:ICML:2019}.
Our survey also shows that using fine labels can lead to more robust models~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021, Mao:Gupta:Nitin:Ray:Song:Yang:Vondrick:ECCV:2020}.
	
When collecting data, increasing the diversity of samples, 
e.g., including samples from both low and high frequency ranges, in case of images~\cite{Yin:Lopes:Shlens:Cubuk:Gilmer:NeurIPS:2019,Sun:Mehra:Kailkhura:Chen:Hendrycks:Hamm:Mao:ECCV:2022}, is necessary.  
Furthermore, data properties, such as intrinsic dimensionality and separation, are defined over the 
underlying data manifold, assuming the knowledge of the entire data distribution; 
yet, for practical purposes, these properties are estimated and enforced for concrete datasets. 
This can lead to incorrect expectations / estimations in cases where training samples do not 
correctly approximate the underlying distribution.
For example, producing a well-separated subset of training data by selecting samples can
improve robustness in non-parametric models~\cite{Yang:Rashtchian:Wang:Chaudhuri:AISTATS:2020,Wang:Jha:Chaudhuri:ICML:2018},
but this strategy does not work for more complex classifiers, 
as sample selection does not make the underlying distribution more separated.
Paying close attention to the representativeness of the training dataset can help mitigate this issue.

\vspace{0.02in}
\emph{2. Cleaning and transforming data.}
Besides collecting large and diverse datasets, there are also multiple ways to improve the quality of existing data.
For example, one can use domain knowledge and/or 
automated feature selection techniques, such as mutual information gain~\cite{Shannon:1949}, 
to remove features with low variance, which helps reduce dimensionality and 
improve robustness~\cite{Izmaliov:Sugrim:Chadha:McDaniel:Swami:MILCOM:2018}.
Dimensionality reduction techniques, which project samples onto lower-dimensional spaces, e.g., PCA~\cite{Jolliffe:2002}, 
also reduce adversarial vulnerability~\cite{Awasthi:Jain:Rawat:Vijayaraghavan:NeurIPS:2020}.
Another approach, 
which relies on using graph theory~\cite{Fan:1997} to transform datasets into graphs that use pair-wise 
distances as edges, 
can help generate 
features that maximize the distance between samples from different classes, resulting in 
well-separated feature representations~\cite{Garg:Sharan:Zhang:Hu:NeurIPS:2018}. 

Numerous learning-based techniques use loss functions engineered to optimize for more complex data properties. 
While common loss functions, such as Softmax Cross-Entropy loss~\cite{Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020},
can result in sparse representations (i.e., with low class density), 
more advanced loss functions can utilize inter-class or intra-class distances to learn feature representations that cluster points around class centers and/or promote separation between classes~\cite{Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020,Mustafa:Khan:Hayat:Goecke:Shen:Shao:ICCV:2019,Mygdalis:Pitas:PR:2022,Bui:Le:Zhao:Montague:deVel:Abraham:Phung:ECCV:2020,Pang:Du:Zhu:ICML:2018,Cheng:Zhu:Zhang:Liu:PR:2023,Yang:Feng:Du:Du:Xu:ICDM:2021}, resulting in increased robustness. 
Producing latent representations that resemble symmetric Gaussian distributions, 
which have been shown to increase robustness~\cite{Pang:Du:Zhu:ICML:2018,Wan:Chen:Yu:Wu:Zhong:Yang:TPAMI:2022}, 
could also be beneficial.
 

\vspace{0.02in}
\noindent 
{\bf Improving Robustness by Manipulating Learning Procedures.}
Knowledge about the properties of a dataset can further help improve robustness 
by manipulating the learning procedure:

\vspace{0.02in}
\emph{1. Model selection and configuration.}
When deciding on a model to train for a particular dataset, probabilistic models, 
such as Bayesian Neural Networks~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}, 
are shown to be more robust given ``undesired'' data properties, such as high dimensionality~\cite{Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}.
For datasets suffering from class imbalance, 
scale-invariant classifiers~\cite{Wang:Wang:Zhou:Ji:Gong:Zhou:Li:Liu:CVPR:2018,Pang:Yang:Dong:Xu:Zhu:Su:NeurIPS:2020}
can be used to reduce the robustness bias among the classes~\cite{Wu:Liu:Huang:Wang:Lin:CVPR:2021}.

Data properties are also useful in determining the configuration of models. 
For example, data dimensionality can guide the number of parameters necessary to train 
robust parametric models, such as DNNs, 
and the type of regularization used to constrain their weights (e.g., $L_1$ norm instead of the commonly used $L_2$ or $L_\infty$ norms)~\cite{Li:Jin:Zhong:Hopcroft:Wang:NeurIPS:2022, Yin:Kannan:Bartlett:ICML:2019}.
Dimensionality can also be useful in determining hyper-parameters in non-parametric models, 
e.g., $k$ in $k$-NNs~\cite{Wang:Jha:Chaudhuri:ICML:2018}.

\vspace{0.02in}
\emph{2. Training and inference procedures.}
During training, learning to perform different tasks concurrently can lead to more robust models~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021, Mao:Gupta:Nitin:Ray:Song:Yang:Vondrick:ECCV:2020}.
When faced with imbalanced datasets, loss functions that assign different weights to samples from different classes can be used to mitigate the influence of class imbalance on robustness~\cite{Wu:Liu:Huang:Wang:Lin:CVPR:2021}.

At inference time, sample purification methods, such as randomized smoothing~\cite{Cohen:Rosenfeld:Kolter:ICML:2019}, 
can be used to defend against adversarial attacks by adding noise to cancel the effects of carefully-crafted perturbations.
Purification can also be achieved by using generative models to move samples from low-density to high-density regions, 
i.e., from low-confidence to high-confidence regions, 
before making predictions~\cite{Song:Kim:Nowozin:Ermon:Kushman:ICLR:2017}. 

\vspace{0.02in}
\noindent 
{\bf Estimating Robustness.}
Data-related metrics can be used to evaluate inherent robustness limitations of datasets.
In particular, several approaches~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019,Prescott:Zhang:Evans:ICLR:2021,Zhang:Evans:ICLR:2022} empirically calculate concentration and use it to obtain the minimum adversarial risk.
Similarly, the optimal transport distance among classes can be used to estimate the risk for 
concrete datasets~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019, Pydi:Jog:ICML:2020, Pydi:Jog:NeurIPS:2021}.
Estimating the minimum adversarial risk serves as an indication of achievable robustness, which can 
inspire trying out alternative data collection, processing, and learning strategies, in cases when 
the observed robustness is significantly less than the achievable value.

%==============================================================================

\subsection{Knowledge Gaps and Future Research Directions}
\label{sec:discussion-future}

\vspace{0.02in}
\noindent
{\bf Empirical Evaluation.}
All but the ten papers outlined in Section~\ref{sec:results-domain-specific} study domain-agnostic data properties.
Yet, the majority of the papers we surveyed conduct experimental evaluations on image datasets only.
Applicability of the findings and of the proposed approaches to other domains, with different forms of data,
may need further investigation.
For example, for datasets with binary features, which are commonly used in malware detection, one cannot arbitrarily
change feature values to reduce the distance between samples.
This further implies that common distance metrics used to model adversaries in the image domain, such as $L_2$ and $L_\infty$, fail to accurately capture the adversarial threat level in such domains.
Hence, future work applying, adapting, and evaluating the proposed metrics and techniques in other domains and data types is needed.

\vspace{0.02in}
\noindent
{\bf Simplified Problem Setup.}
Several studies use a simplified problem setup, 
e.g., pure Gaussian data distributions, to provide formal proofs related to the studied phenomenon.
While such work helps advance knowledge and our understanding of the effects of data on adversarial robustness,
additional work that investigates the generalizability of the findings on realistic datasets used in practice is needed.
For example, assuming uniform data properties, e.g., same distribution, density, and level of label noise,
for all classes on the training data greatly simplifies the proofs, but is not common in reality.
Likewise, considering only binary classification simplifies calculations of data separation, which can be calculated by measuring the distance between the two classes. Yet, in a multi-class setting, one needs to consider the proximity of data points from multiple classes.

Furthermore, most papers only consider a white-box attack setting, which might not be realistic in many practical scenarios.
Even though a white-box setting makes it possible to model the worst-case adversary and to provide better robustness guarantees, it may result in overly pessimistic findings, i.e., some data transformations may be robust against black-box attacks while still being vulnerable to white-box attacks.
Thus, future works might look into the impact of data properties on the different types of attack scenarios.


\vspace{0.02in}
\noindent
{\bf Data and Model Interplay.}
Even when the training data is optimal for robustness, a sub-optimal training method can lead to adversarial vulnerability~\cite{Richardson:Weiss:JMLR:2021}.
For example, adversarial vulnerability may arise when the complexity of the classifier does not match the complexity of the data, e.g., CNNs may achieve lower robustness due to their complexity than simpler models, such as Kernel-SVMs,
on symmetrical data with well-separated means and similar variances~\cite{Richardson:Weiss:JMLR:2021}.
To alleviate such problems, a few papers propose to select, improve, or optimize classifiers based
on the dimensionality of data~\cite{Wang:Jha:Chaudhuri:ICML:2018,
Yin:Kannan:Bartlett:ICML:2019,
Carbone:Wicker:Laurenti:Patane:Bortolussi:Sanguinetti:NeurIPS:2020}.
Similar work that looks at other properties of data, such as separation and density, could be of value.
Future works could also explore strategies for determining whether the input data (vs. the model itself) is the dominant cause \mbox{of adversarial vulnerability}.

\vspace{0.02in}
\noindent
{\bf Training Dataset vs. Data Distribution.}
As discussed in Section~\ref{sec:discussion-practical}, representative training data is necessary to truly estimate 
the properties of the entire underlying data manifold. 
Yet, in practice, collecting representative datasets is challenging and data properties, such as separation and density,
are often evaluated on concrete datasets.  
More research on how to assess the gap between the observed and the ``real'' data properties could be valuable. 

\vspace{0.02in}
\noindent
{\bf Efficient Selection of Unlabeled Samples.}
The need for large-scale training data is growing, aided by the increase in compute power and efficient algorithms.
However, manually labeling all samples remains impractical in several domains.
A number of works~\cite{Uesato:Alayrac:Huang:Stanforth:Fawzi:Kohli:NeurIPS:2019, Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019, Najafi:Maeda:Koyama:Miyato:NeurIPS:2019} showed that 
using unlabeled and generated samples can help to fill the sample complexity gap for developing more robust models.
Xing et al.~\cite{Xing:Song:Cheng:NeurIPS:2022} assess the quality of generated samples 
w.r.t. ones from an ideal generator (a.k.a. real unlabeled samples).
However, as the quality of unlabeled samples is undetermined 
(e.g., they can contain a cat and a dog in the same picture when performing a cat vs. dog binary classification),
research on the effects of unlabeled sample quality on model robustness in needed.
Such research can further facilitate the development of techniques that prioritize particular 
unlabeled samples in robust model training.

\vspace{0.02in}
\noindent
{\bf Interdependence of Properties.}
Only a few works in our collected literature consider multiple data properties simultaneously or establish interdependence of data properties.
For example, Wang et al.~\cite{Wang:Jha:Chaudhuri:ICML:2018} and
Rajput et al.~\cite{Rajput:Feng:Charles:Loh:Papailiopoulos:ICML:2019}
find that the number of samples and dimensionality collectively influence the performance of the resulting model.
Sanyal et al.~\cite{Sanyal:Dokania:Kanade:Torr:ICLR:2021} study the tolerable amount of label noise as a function of the dataset density.
Such works are very valuable as adversarial robustness is indeed a result of compounding properties.
Yet, optimizing for multiple properties simultaneously is not always possible.
A productive direction of future work could be to investigate correlations between different data properties, e.g.,
the effects of feature dimensionality reduction approaches on class density and separation.


\vspace{0.02in}
\noindent
{\bf Additional Data Properties.}
Existing research on the effects of data on
\emph{standard generalization}~\cite{Lopez:Fernandez:Garcia:Palade:Herrera:InfSci:2013,
Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020,
Santos:Henriques:Pedro:Japkowicz:Fernandez:Soares:Wilk:Santos:AIR:2022} identified several data properties
not discussed in the papers related to robust generalization that we reviewed.
These include the presence of (i) outliers, i.e., samples that drastically differ from most observed samples in a dataset,
(ii) overlapping samples, i.e., different samples of the dataset having the same feature representation, and
(iii) small disjuncts, i.e., training samples from the same class forming small disjoint clusters dispersed throughout the input space
(more details are in Section~\ref{sec:relatedwork}).
Investigating the effect of such data properties on the model's adversarial robustness could be yet another direction
for possible future work.


\vspace{0.02in}
\noindent
{\bf Quantitative Measure.}
Literature shows that the lower/upper bound of adversarial robustness can be determined by the properties of the underlying data~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019,Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019}.
Modifying certain properties of the data can also change the robustness of the resulting classifier.
Hence, the ability to quantitatively measure such data properties is very valuable.
However, some data properties discussed in this survey, such as, type of distributions and label noise, lack any reliable estimation techniques.
Current work mostly relies on informal comparative analysis, e.g., that the \mnist dataset is closer to a Bernoulli mixture data than a Gaussian mixture because the pixels are concentrated towards black or white.
Quantitatively measuring the degree of similarity between distributions, although difficult, may be necessary in order to make more accurate conclusions.

Interestingly, other data properties have multiple, often inconsistent, measurement techniques, e.g., concentration~\cite{Mahloujifar:Zhang:Mahmoody:Evans:NeurIPS:2019,Prescott:Zhang:Evans:ICLR:2021,Zhang:Evans:ICLR:2022}, density~\cite{Song:Kim:Nowozin:Ermon:Kushman:ICLR:2017,Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020},
intrinsic dimensionality~\cite{Amsaleg:Bailey:Barbe:Erfani:Furon:Houle:Radovanovic:Nguyen:TIFS:2021,Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020}, and inter-class distances~\cite{Ding:Lui:Jin:Wang:Huang:ICLR:2019, Bhagoji:Cullina:Mittal:NeurIPS:2019, Pydi:Jog:ICML:2020}.
For example, the inter-class distances can be calculated as the total distance required to move the samples from one class to
another~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019, Pydi:Jog:ICML:2020}.
It can also be calculated as the pairwise distances between a pre-defined portion of samples from different classes,
e.g., 10\% from each class, that are the closest to each other~\cite{Ding:Lui:Jin:Wang:Huang:ICLR:2019}.
While the inter-class distance derived through the first approach is more computationally expensive, the second approach
is more susceptible to outliers as it relies only on a subset of samples close to each other.
Moreover, these metrics might not necessarily correlate with each other.
We believe future research can provide more insights about appropriate application scenarios for each of the proposed metrics.



\vspace{-0.1in}
\subsection{Summary}
Practical methods for augmenting data and learning procedures to improve robustness include 
increasing dataset size and diversity through the selection of natural or generated samples. 
One can also change the underlying data representation to optimize for certain properties, 
such as reduced dimensionality and better separation, or to project data into a more desirable distribution. 
This can be achieved through statistical and learning-based techniques. 
Selecting appropriate models, as well as configuring models to consider the underlying data properties,
is also shown to be useful. 
Finally, techniques to estimate achievable robustness can be used to gauge 
how much the robustness observed for a particular dataset can be improved.

As for future research directions, assessing the effects of data on robustness in domains beyond images, such as structured text, software, and more, could be valuable. 
Moreover, investigating ways to assess, evaluate, and utilize properties of realistic datasets, 
independently and in conjunction with commonly used types of models, could have large practical significance.
There is also a lack of work on the interplay of different data properties: 
it may be infeasible to modify a data-property without affecting others.
Studying additional properties of data that were shown to benefit standard generalization, 
such as overlapping and outlier samples, could also help better understand both robust generalization and the accuracy-robustness trade-off.
Finally, the lack of consistent metrics for some data properties may also make empirical measurements less reliable. 
