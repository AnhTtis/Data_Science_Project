\section{Introduction}
\label{sec:introduction}

Recent advances in Machine Learning (ML) led to the development of numerous accurate and scalable ML-based techniques,
which are increasingly used in industry and society.
Yet, concerns related to the safety and security of ML-based systems could substantially impede their widespread adoption, especially in the area
of safety-critical systems, such as autonomous cars.
Examples of fooling ML models into making wrong predictions by adding imperceptible-to-the-human noise to the input
are well known~\cite{OpenAI:report:2017}:
adversarial perturbation to a stop sign may cause a machine learning system to recognize it as a ``max speed'' sign instead,
which might lead to wrong and dangerous actions taken by an autonomous car~\cite{Eykholt:Evtimov:Fernandes:Li:Rahmati:Xiao:Prakash:Kohno:Song:CVPR:2018} (see Fig.~\ref{fig:stop_sign_side}).
Likewise, malicious software can be perturbed to bypass security models while still retaining its malicious behavior~\cite{Demontis:Melis:Biggio:Maiorca:Arp:Konrad:Rieck:Corona:Giacinto:Roli:TDSC:2019}.

\begin{figure*}[th!]
    \vspace{-0.05in}
	\centering
	\includegraphics[width=0.45\textwidth]{images/stop_sign_adv_example.pdf}
	\vspace{-0.1in}
	\caption{Adversarial examples for traffic signs (picture by Chen and Wu~\cite{Chen:Wu:Altacognita:Blog:2019}).}
	\label{fig:stop_sign_side}
	\vspace{-0.2in}
\end{figure*}


ML models are susceptible to such scenarios, known as \emph{adversarial attacks} or \emph{adversarial examples}~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014, Goodfellow:Shlens:Szegedy:ICLR:2014}.
To address this problem, recent literature investigates mechanisms behind adversarial attacks and proposes defenses against these attacks -- an area commonly referred to as \emph{adversarial robustness}.
The performance of ML models under adversarial attacks, known as \emph{robust accuracy} or \emph{robust generalization}, is often distinguished from the general model accuracy, known as \emph{standard accuracy} or \emph{standard generalization}.

Adversarial attacks that aim to decrease model accuracy can roughly be divided into \emph{evasion} and
\emph{poisoning attacks}~\cite{Biggio:Roli:PR:2018,Li:Li:Ye:Xu:CSUR:2021}.
The stop sign example above is, in fact, an evasion attack, where the attacker carefully modifies
the input to mislead the prediction~\cite{Szegedy:Zaremba:Sutskever:Bruna:Erhan:Goodfellow:Fergus:ICLR:2014,Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Carlini:Wagner:SP:2017}.
Instead of changing model inputs, poisoning attacks are carried out by injecting corrupt data into the training dataset, 
to compromise the integrity of the model~\cite{Goldblum:Tsipras:Xie:Chen:Schwarzchild:song:Madry:Li:Goldstein:TPAMI:2022,Shafahi:Huang:Najibi:Suciu:Studer:Dumitras:Goldstein:NeurIPS:2018, Tian:Cui:Liang:Yu:CSUR:2022}.
\emph{The focus of this survey is on evasion attacks}, as they are more common, accessible, and
more frequently discussed in the literature~\cite{Machado:Silva:Goldschmidt:CSUR:2021, Chakraborty:Alam:Dey:Chattopadhyay:Mukhopadhyay:ArXiv:2018, Viso:ai:blog:2022, Li:Li:Ye:Xu:CSUR:2021}.
As literature often uses the term \emph{adversarial attacks} to refer to evasion
attacks~\cite{Madry:Makelov:Schmidt:Tspiras:Vladu:ICLR:2018, Kurakin:Goodfellow:Bengio:ICLR:2017, Yuan:He:Zhu:Li:TNNLS:2019},
in this survey we use these two terms interchangeably.

Most techniques that study adversarial evasion attacks attribute adversarial vulnerability to various
aspects of the learning algorithm and/or properties of the data.
Numerous existing surveys on adversarial robustness focus on adversarial attacks and defenses~\cite{Zhou:Liu:Ye:Zhu:Zhou:Yu:CSUR:2022, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021, Li:Li:Ye:Xu:CSUR:2021} and sources of adversarial vulnerability
related to learning algorithms~\cite{Machado:Silva:Goldschmidt:CSUR:2021, Serban:Poll:Visser:CSUR:2020}. 
Yet, to the best of our knowledge, there are no surveys that collect and organize literature 
focusing on the influence of \emph{data} on adversarial robustness.
Our work fills this gap. 
Specifically, {\bf we investigate 
(a) what properties of data influence model robustness and 
(b) how to select, represent, and use data to improve model robustness.}
To the best of our knowledge, this is the first survey to analyze adversarial robustness from the perspective of data properties.

To collect literature relevant to our survey, we used popular digital libraries and search engines, selecting papers that investigate the effect of data on ML adversarial robustness.
We identified more than 4,359 potentially relevant papers published in top scientific venues on Machine Learning, Computer Vision, Computational Linguistics, and Security.
We systematically inspected these papers, identifying 77 papers relevant to our survey. We further analyzed, categorized, and described the selected papers in this manuscript.


\vspace{0.02in}
\noindent
{\bf Main findings} (see Section~\ref{sec:results} for details). The results of our analysis show that producing accurate and robust models requires a larger \emph{number of samples} for training than achieving high accuracy alone.
The required number of samples to learn a robust model also depends on other properties of the data, such as dimensionality and the data distribution itself. Specifically, input data with higher \emph{dimensionality}, i.e., a larger number of features that represent the input dataset, requires a larger number of samples to produce a robust model.
This is consistent with other findings showing that high \emph{dimensionality} is undesirable for robustness.
Moreover, some data \emph{distributions} are inherently more robust than others, e.g., a Gaussian mixture distribution requires more samples to produce a robust model than needed by a Bernoulli mixture distribution.

Another aspect that affects robustness is the \emph{density} of data samples within classes, which measures how far apart samples are from each other.
Papers show that high class density correlates with high robust accuracy and that adversarial examples are commonly found in low-density regions of the data.
This is intuitive as low-density regions imply that there are not enough samples to accurately characterize the region.
A related property, \emph{concentration}, measures how fast the value of a function defined over a data region, e.g., \emph{error rate}, grows as the region expands.
This concept of concentration tightly corresponds to adversarial robustness if we consider the expansion of a data region as the effect of adversarial perturbation, i.e., perturbing samples in all directions causes the region defined by the original samples to expand.
In this case, high concentration implies that the error rate grows as one perturbs all points in a data region and, thus, datasets with high concentration are shown to be inevitably non-robust.
The \emph{separation} between classes of the underlying data distribution also affects robustness, with a large distance between different classes being desirable for adversarial robustness as an attacker would need to use a large perturbation to move samples from one class to another.

Yet another aspect that impacts robust accuracy is the presence of mislabeled samples in a dataset,
referred to as \emph{label noise}.
Furthermore, refining labels to reason about a larger number of classes, e.g., splitting a class ``animal'' into ``cat'' and ``dog'' may improve adversarial robustness as such labels allow learning more compact representations for samples that share stronger similarities.

A number of papers also identify \emph{domain-specific} properties that correlate with adversarial robustness.
For example, image frequency -- the rate of pixel value change -- affects robustness, and it is advisable to use a diverse frequency range in the training dataset to prevent any frequency biases which give rise to adversarial examples.


\vspace{0.02in}
\noindent
{\bf Practical Implications} (see Section~\ref{sec:discussion-practical} for details).
Our survey identifies a number of actionable guidelines and techniques that can be used to improve robustness.
These include data manipulation techniques, such as 
techniques for increasing the number of samples with real or generated data,
feature selection and dimensionality reduction techniques, and 
techniques for learning a latent data representation 
with desirable data properties, such as high density, high separation, and low dimensionality. 
While these techniques aim at changing properties of the underlying data, 
improving robustness can also be achieved by manipulating the learning procedures 
based on the properties of the training data, 
e.g., selecting particular models and/or configuring model parameters based on data dimensionality,
adjusting samples at inference time, and more.
A number of approaches also propose ways to estimate robustness guarantees for particular data, 
making it possible to reason about inherent robustness limitations in a practical setup.


\vspace{0.02in}
\noindent
{\bf Knowledge Gaps and Future Research Directions} (see Section~\ref{sec:discussion-future} for details).
Our literature review shows that, even though most works study data properties from a domain-agnostic perspective, they typically conduct an empirical evaluation on image datasets only.
This constrains the types of attacks and robustness measurements considered, 
and thus the findings may not generalize to other domains or types of datasets.
Furthermore, most works base their formal derivations on quite simple synthetic data models, such as uniform distributions, a mixture of Gaussian distributions, and a mixture of Bernoulli distributions,
which exhibit unrealistic assumptions compared to real datasets used in practice.
We also observed that while most papers only perform a univariate analysis on a specific data property,
most properties are hard to independently optimize, e.g., to decrease dimensionality without decreasing separation,
as decreasing the dimensionality implies that samples have fewer features to be differentiated from each other.
We also found that some properties, e.g., separation, do not have a standard way of measurement for concrete datasets.
We believe future work should look into these directions.


\vspace{0.02in}
\noindent
{\bf Contributions.} The main contributions of this survey are:
\vspace{-0.1in}
\begin{itemize}
\item A collection of literature on the effects of data on adversarial robustness.
\item A categorization and analysis of data properties that affect adversarial robustness.
\item An analysis of practical implications of the finding, 
knowledge gaps, and future \mbox{research directions}.
\end{itemize}

