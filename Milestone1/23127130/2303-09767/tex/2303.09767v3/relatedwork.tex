\section{Related Work}
\label{sec:relatedwork}

To the best of our knowledge, our survey is the first to explicitly focus on properties of training data 
in the context of model robustness under evasion attacks. 
We review other surveys that focus on evasion attacks more broadly, while including some discussion 
on data, in Section~\ref{sec:relatedwork-surveys-data}.
Section~\ref{sec:relatedwork-standard} reviews literature that examines how data properties affect standard generalization. 
Finally, in Section~\ref{sec:relatedwork-attacks}, we review techniques related to evasion attacks and defenses.
Additional work that studies non-data-related reasons for evasion attacks, 
as well as non-evasion attacks, such as poisoning and backdoor, is discussed in our online appendix~\cite{appendix}.
 
\vspace{-0.1in}
\subsection{Surveys on Evasion Attacks that Discuss Data}
\label{sec:relatedwork-surveys-data}
Numerous existing surveys review the literature on evasion attacks.
Most of these works do not focus specifically on properties of data but discuss attack and defense mechanisms, non-data-related reasons for adversarial vulnerability, 
and the different threat models. 
Only a few of these works mention data-related reasons for the existence of adversarial examples~\cite{Serban:Poll:Visser:CSUR:2020, Machado:Silva:Goldschmidt:CSUR:2021, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021, Akhtar:Mian:IEEEAccess:2018}.
Specifically, Serban et al.~\cite{Serban:Poll:Visser:CSUR:2020} observe that adversarial vulnerability can be caused by an insufficient training sample size 
and high data dimensionality. 
Similarly, Machado et al.~\cite{Machado:Silva:Goldschmidt:CSUR:2021} mention that the lack of sufficient training data, high dimensionality, 
and high concentration contribute to adversarial vulnerability.
Akhtar et al.~\cite{Akhtar:Mian:IEEEAccess:2018, Akhtar:Mian:Kardan:Shah:IEEEAccess:2021} also mention high dimensionality, along with other non-data-related reasons, 
as a source of adversarial examples.

A concurrent work by Han et al.~\cite{Han:Lin:Shen:Wang:Guan:CSUR:2023}
studies the origins of adversarial vulnerability in deep learning w.r.t. the model, data, and other perspectives.
The authors mention high dimensionality, distributions with high concentration, a small number of output classes, data imbalance, and the perceptual difference in image frequencies as potential sources of adversarial examples.
However, as (a) the focus of that survey is not on data-related properties in particular, 
(b) its paper search was conducted in 2021, and 
(c) it focuses on deep learning models only, 
our work was able to identify more than 50 additional relevant papers which focus on other types of models, 
e.g., non-parametric and linear classifiers, 
and/or discuss additional types of data-related properties, 
such as, types of distribution, class density, separation, and label quality.

In summary, by explicitly focusing on the effects of data properties on evasion attacks in our survey and 
including more than 50 papers not covered in prior work, we were able to 
identify additional relevant properties, practical suggestions, and future research directions in this area.
 
\vspace{-0.1in}
\subsection{Effects of Training Data on Standard Generalization}
\label{sec:relatedwork-standard}
A number of surveys investigate the influence of data properties on standard
rather than robust generalization.
One of the earliest is probably the work of Raudys and Jain~\cite{Raudys:Jain:TPAMI:1991},
who review studies related to the influence of sample size on binary classifiers, showing that
a limited sample size usually leads to sub-optimal generalization.
Bansal et al.~\cite{Bansal:Sharma:Kathuria:CSUR:2021} and
Bayer et al.~\cite{Bayer:Kaufhold:Reuter:CSUR:2022} also survey papers addressing the data scarcity problem.
Their results show that augmentation techniques
can help improve a model's generalization by reducing the problem of model overfitting.

Label noise is another aspect of data that influences both standard and robust generalization.
Most works on this topic find that noisy labels increase the need for a greater number of training samples and may result in unnecessarily complex decision boundaries~\cite{Frenay:Verleysen:TNNLS:2014,Song:Kim:Park:Shin:Lee:TNNLS:2022}. 
These works also show that adversarial training can improve model clean accuracy in the presence of 
label noise.

Lorena et al.~\cite{Lorena:Garcia:Lehmann:Souto:Ho:CSUR:2020} identify 26 quantitative metrics that can be used to 
estimate the difficulty of performing classification on a given dataset.
While some of these metrics, such as high dimensionality and low class separation, 
were also studied in the context of robust generalization, 
other metrics, such as ambiguity of classes and complexity of their separation boundaries, 
were not yet explored, according to our survey.

A number of authors~\cite{He:Garcia:TKDE:2009,Lopez:Fernandez:Garcia:Palade:Herrera:InfSci:2013,Santos:Henriques:Pedro:Japkowicz:Fernandez:Soares:Wilk:Santos:AIR:2022,Yang:Jiang:Song:Guo:IJCV:2022} focus on the imbalance learning problem. 
They show that several data properties, such as low density, data noise, 
data shifts, and~-- most importantly, data overlap between classes~--
further complicated learning from imbalanced data. 
Similarly, Moreno-Torres et al.~\cite{MorenoTorres:Raeder:Rodrigues:Chawla:Herrera:PR:2012} study 
data shift, showing that it negatively affects clean accuracy. 

The aforementioned works show that 
some of the properties discussed in our survey, such as 
the number of samples, dimensionality, density, and label quality, also affect clean accuracy. 
There are also additional data properties that are covered exclusively by these or by our work, e.g., data shifts and data distributions, respectively. 
Studying these additional properties, as well as the interplay between data properties for clean and robust accuracy, 
is an interesting research direction, which could be facilitated by our work. 


\vspace{-0.05in}
\subsection{Evasion Attacks and Defenses}
\label{sec:relatedwork-attacks}
A number of works focus on techniques for generating evasion attacks, countermeasures against these attacks, 
and defining the notion of the attack itself.

\vspace{0.02in}
\noindent
{\bf Attacks and Defense.}
Several works~\cite{Maiorca:Biggio:Giorgio:CSUR:2019,
Zhang:Sheng:Alhazmi:Li:ACMTIST:2020,
Demetrio:Coull:Biggio:Lagorio:Armando:Roli:ACMTPS:2021,
Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,
Li:Li:Ye:Xu:CSUR:2021,
Liu:Tantithamthavorn:Li:Liu:CSUR:2022,
Liu:Nogueria:Fernandes:Kantarci:IEEECST:2022,
Sun:Dou:Yang:Zhang:Wang:Philip:He:Li:TKDE:2022} 
survey adversarial attacks and defenses, observing that 
new attacks constantly bypass defenses, which gives rise to new defenses being proposed, only to be broken again 
(a.k.a. the `cat and mouse race' or the `arms race'). 
They also observe that research in this field studies attacks / defenses at a feature-level, which restricts 
the practicality of the developed techniques by the feasibility of perturbing the corresponding features in real life. 

More recently, researchers have started to investigate the 
susceptibility of newer models to adversarial evasion attacks. 
For example, several studies~\cite{Wang:Pan:Hu:Duan:Pan:IJSWIS:2022, 
Shi:Han:Tan:Kuang:NeurIPS:2022,Yin:Lin:Sun:Wei:Chen:TIFS:2023,Wang:Xie:Microsoft:ChatGPT:ArXiv:2023} 
propose attack techniques against contemporary models, 
such as Graph Neural Networks, Generative Pre-trained Transformers (GPT), and Vision Transformers. 
These studies showed that adversarial examples persist even in newer models, some of which are 
trained with large volumes of data. 
As all these works focus on attack and defense mechanisms rather than 
the effects of data on adversarial robustness, our work extends and complements this research.


\vspace{0.02in}
\noindent
{\bf Adversarial Examples.}
Adversarial examples are generally defined as inputs constructed by perturbing a correctly classified sample in a way that makes the change imperceptible to a human. 
However, as `imperceptible to a human' is hard to define, existing research on adversarial examples approximates imperceptibility with a small perturbation measured through $L_p$ norms.
A line of research~\cite{Gilmer:Adams:Goodfellow:Anderson:Dahl:ArXiv:2018,Sharif:Bauer:Reiter:CVPRW:2018,Fezza:Bakhti:Hamidouche:Deforges:QoMEX:2019, Mezher:Deng:Karam:EUVIP:2022} 
investigates the validity of this assumption. 
This work shows that perturbations generated by $L_p$ norms do not entirely align with human perceptions, 
i.e., some changes with a small $L_p$ norm can be apparent to humans. 
In addition, adversarial examples with the minimum $L_p$ perturbation may be less effective and transferable than 
higher perturbation~\cite{Biggio:Roli:PR:2018,Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021}. 
Hence, a number of approaches explore metrics for imperceptibility 
in computer vision and NLP domains~\cite{Fezza:Bakhti:Hamidouche:Deforges:QoMEX:2019,Mezher:Deng:Karam:EUVIP:2022, Zhang:Sheng:Alhazmi:Li:ACMTIST:2020}. 
Yet another issue with $L_p$ norms is that they cannot be used reliably in domains other than images. 
For example, in the case of software/malware, simply generating adversarial examples with $L_p$ norms 
may result in feature representations that are not possible in 
the problem space~\cite{Rosenberg:Shabtai:Elovici:Rokach:CSUR:2021,Pierazzi:Pendlebury:Cortellazz:Cavallaro:2020}. 
While all these works focus on the properties of adversarial examples, 
they are orthogonal to the topic of our survey, as we rather focus on how properties of the training data 
affect the success of adversarial examples.

