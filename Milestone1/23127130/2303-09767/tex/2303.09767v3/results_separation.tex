\subsection{Separation}
\label{sec:results-separation}

Closely related to density, \emph{separation} refers to the distance between classes.
Fig.~\ref{fig:separation_guideline} shows examples of not well-separated (top) and well-separated (bottom) datasets.
Intuitively, learning an accurate classifier is easier when data is well-separated as samples from different classes are farther apart and samples from the same class are closer together.
Different metrics to quantify separation include
the \emph{optimal transport distance},
which computes the minimum distance required to transport samples from one class to another, and
\emph{inter-class distance}, which computes the distance between samples in different classes.


\begin{figure*}[h]
	\centering
	\vspace{-0.25in}
	\begin{minipage}{0.43\textwidth}
		\centering
		\vspace{0.25in}
		\includegraphics[width=0.68\linewidth]{images/separation_overview.pdf}
		\vspace{0.07in}
		\caption{Separation illustration.}
		\label{fig:separation_guideline}
		\vspace{-0.3in}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\vspace*{0.10in}
		\centering
 		\includegraphics[width =\textwidth]{images/separation_citation_graph.pdf}
		\vspace{-0.25in}
		\caption{Papers discussing separation.}
		\label{fig:separation}
	\end{minipage}
    \vspace{-0.1in}
\end{figure*}



Papers that discuss data separation in relation to adversarial robustness are shown in Fig.~\ref{fig:separation}.
They can roughly be divided into
\roundrect{1} papers showing the effect of separation on robustness and
\roundrect{2} papers proposing techniques to promote separation and, thus, increase robustness.


\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Separation}.
Bhagoji et al.~\cite{Bhagoji:Cullina:Mittal:NeurIPS:2019} calculate lower bounds for adversarial risk 
in a binary classification setting using the optimal transport distance.
The authors show
that the lower bound decreases as the distance between the two class distributions increases,
i.e., a classifier becomes more robust with better separation.
Based on this result, they estimate the minimum adversarial risks for image datasets, like \mnist and \cifarten,
showing that the theoretically calculated risks are lower than the empirical values achieved by the state-of-the-art defense models.
The authors conclude that there is still room for improving existing techniques.


Pydi and Jog~\cite{Pydi:Jog:ICML:2020, Pydi:Jog:NeurIPS:2021} arrive at a similar conclusion~-- 
that robustness improves as separation between classes increases.
The authors further focus on datasets with simple univariate distributions, such as Gaussian and uniform.
They propose a technique to construct classifiers that can achieve the optimal,
lowest possible adversarial risk for a given separation between classes.
The main idea behind this technique is to analyze the optimal way to
transport samples from one class to another 
(which represents the smallest perturbation needed to create adversarial examples) and 
further use this information to identify the decision boundary that induces the maximal distance required to transport 
samples between classes. 
That is, the approach maximizes the distance between samples of each class and the decision boundary, 
resulting in an optimally robust classifier.


Bhattacharjee et al.~\cite{Bhattacharjee:Chaudhuri:ICML:2020}
prove that certain non-parametric models, such as k-NNs, 
are inherently robust when trained on a large number of well-separated samples.
This is because these classifiers make predictions based on neighborhoods and well-separated data ensures that
samples in close proximity to each other share the same labels.
In their later work, discussed in Section~\ref{sec:results-number-of-samples}~\cite{Bhattacharjee:Jha:Chaudhuri:PMLR:2021},
the authors
show that, in well-separated data, robust accuracy is independent of dimensionality and a robust linear classifier can be learned without the need for a large number of training samples.
This result shows that adversarial vulnerability can be efficiently tamed by increasing separation.



\vspace{0.05in}
\noindent
\roundrect{2} {\bf Promoting Separation}.
Yang et al.~\cite{Yang:Rashtchian:Wang:Chaudhuri:AISTATS:2020} propose a sample-selection-based technique to improve the adversarial robustness of non-parametric models by increasing the separation among the training data.
In particular,
as non-parametric models tend to learn complex decision boundaries when the training samples from different classes are close to each other,
the authors propose to remove the smallest subset of samples so that all pairs of differently labeled samples
remain separated even when perturbed by the maximum perturbation size. 
Wang et al.~\cite{Wang:Jha:Chaudhuri:ICML:2018}, already discussed in Section~\ref{sec:results-dimensionality}, 
focus on improving robustness of 1-NN classifiers. 
Such classifiers struggle to take advantage of points close together with opposite labels, resulting in worse robustness.
Hence, the authors propose retaining the largest subset of training samples that are
(i) well-separated and (ii) in high agreement on labels with their nearby samples
(a.k.a., highly confident). 
The authors show that their approach outperforms adversarially trained 1-NNs. 

For non-parametric classifiers, a more effective strategy is to enforce separation in the latent representations. 
Specifically, Mustafa et al.~\cite{Mustafa:Khan:Hayat:Goecke:Shen:Shao:TPAMI:2020}
attribute the cause of adversarial vulnerability to close proximity of classes in latent space.
Hence, they propose a loss function to learn intermediate feature representations that
separate different classes into convex polytopes, i.e., polyhedra in higher dimensions,
that are maximally separated.
Mygdalis et al.~\cite{Mygdalis:Pitas:PR:2022} propose a loss function to separate classes into hyperspheres, 
such that samples in a class have minimum distance from their hypersphere center and 
maximum distance from the remaining hyperspheres. 
The authors demonstrate that their approach outperforms that of
Mustafa et al.~\cite{Mustafa:Khan:Hayat:Goecke:Shen:Shao:TPAMI:2020} and other baselines 
w.r.t. standard and robust accuracy for \cifarten, \cifarhundred and \svhn.

Bui et al.~\cite{Bui:Le:Zhao:Montague:deVel:Abraham:Phung:ECCV:2020} observe that the adversarial vulnerability of DNNs arises from a large difference in intermediate layer values between clean and adversarial data.
They thus propose to modify the loss function so that it results in an intermediate latent representation 
that has high similarity between clean and their corresponding adversarial samples,
while promoting large inter-class and small intra-class distances and 
increased margins from class centers to decision boundaries.
Likewise,
Pang et al.~\cite{Pang:Du:Zhu:ICML:2018} and Wan et al.~\cite{Wan:Chen:Yu:Wu:Zhong:Yang:TPAMI:2022} discussed in Section~\ref{sec:results-distribution},
as well as Pang et al.~\cite{Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020} discussed in Section~\ref{sec:results-density},
improve DNN robustness by separating centers of the produced latent distributions, which, in turn, 
increases the separation \mbox{between classes}.
Similarly, Cheng et al.~\cite{Cheng:Zhu:Zhang:Liu:PR:2023} propose improving separation by 
enforcing equal variance in all directions for all classes (Distribution Normalization) and 
maximizing the minimum margin between any two classes (Margin Balance). 


Yang et al.~\cite{Yang:Feng:Du:Du:Xu:ICDM:2021} propose a representation-learning technique
to learn feature representations that
bring samples of class $C$ and adversarial examples generated for $C$ into close proximity
while separating the samples of $C$ from both
(i) adversarial examples generated for other classes and misclassified as class $C$ and
(ii) samples from other classes.
These separations are enforced by the loss function proposed by the authors.
The authors show that their approach improves the resulting model robustness compared with standard DNNs.

Garg et al.~\cite{Garg:Sharan:Zhang:Hu:NeurIPS:2018} propose an approach to generate well-separated features for a dataset using graph theory.
Specifically, they convert the input dataset into a graph, where vertices correspond to the input data points and
edges represent the similarity between the data points (e.g., calculated using Euclidean distance).
The authors prove that features extracted using the eigenvectors of the Laplacian matrix capturing the structure of the graph
will have significant variation across the data points,
while being robust to small perturbations. These qualities make them good candidates for robust features.
The authors then demonstrate that a linear model trained on the \mnist dataset with 20 features generated using their approach is more robust to $L_2$-norm-based transfer attacks than a fully connected neural network trained on the full pixel values of the \mnist dataset.

