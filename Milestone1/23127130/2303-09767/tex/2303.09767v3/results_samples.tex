\subsection{Number of Samples}
\label{sec:results-number-of-samples}

\emph{Number of samples} simply means the quantity of samples available in the training dataset.
For the example in Fig.~\ref{fig:samples_guideline}, where circles represent training samples for a two-class dataset,
the left dataset has fewer samples than the right dataset.

The term \emph{sample complexity} refers to the number of training samples required to achieve a certain model performance,
e.g., 90\%, in terms of either robust or standard generalization.
Then, \emph{sample complexity gap} refers to the difference in the number of samples required to achieve the same model performance for robust generalization as for standard generalization.

\begin{figure*}[b!]
  \centering
  \includegraphics[width=0.5\linewidth]{images/number_of_samples_overview_no_distr_info.pdf}
  \vspace{-0.2in}
  \caption{Number of samples illustration.}
  \label{fig:samples_guideline}
  \vspace{-0.1in}
\end{figure*}


\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.9\linewidth]{images/number_of_samples_citation_graph.pdf} 
	\vspace{-0.1in}
	\caption{Papers discussing the number of samples.}
	\label{fig:samples}
	\vspace{-0.15in}
\end{figure*}


Papers studying the relationship between the number of training samples and the robustness of the resulting model
are shown in Fig.~\ref{fig:samples}. They can roughly be divided into
\roundrect{1} papers discussing sample complexity for robust generalization,
\roundrect{2} papers proposing techniques to resolve the sample complexity gap between the number of samples required to achieve the same level of robust and standard generalization, and
\roundrect{3} papers proposing techniques to deal with data imbalance, i.e., an unequal number of samples in different classes.


\noindent
\roundrect{1} {\bf Sample Complexity}.
Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018} observe that the number of training samples required for robust generalization is larger than the number of samples required for the equivalent-level standard generalization, i.e., that there exists a \emph{sample complexity gap} between the standard and robust generalization.
Specifically, for linear classifiers trained on a mixture of Gaussian distributions
(referred to as \emph{Schmidt's Gaussian mixture} in the remainder of this paper),
the authors prove that standard generalization requires a constant number of samples while
equivalent-level robust generalization requires a number of samples proportional to the data dimensionality
($O(\sqrt{d})$).
The gap in sample complexity persists for this data distribution in nonlinear classifiers as well.
Yet, the sample complexity gap disappears for nonlinear classifiers trained on a mixture of Bernoulli distributions;
these distributions also need substantially fewer samples than Gaussian mixtures.
The authors conclude that sample complexity for robust generalization depends on the distribution,
even when the same type of classifiers is considered.
Their experimental validation with the \mnist~\cite{MNST:1998}, \cifarten~\cite{CIFAR:ten:hundred:2009}, and \svhn~\cite{SVHN:dataset:2011} image datasets shows that \mnist, which is closer to a Bernoulli mixture, indeed requires a smaller number of training samples to achieve a reasonable robust generalization 
than the \cifarten and \svhn datasets, which are \mbox{closer to a Gaussian mixture}.


In follow-up work, Dan et al.~\cite{Dan:Wei:Ravikumar:ICML:2020} provide reasons for why robust generalization
requires more samples than standard generalization, focusing, again, on Gaussian mixture distributions.
Departing from the Signal-to-Noise ratio (SNR) metric that is based on the distance
between two Gaussian distributions and is known to capture the hardness of standard classification,
the authors propose a new Adversarial SNR (AdvSNR) metric,
defined as the minimum SNR for standard and adversarially perturbed data,
to capture the hardness of robust classification.
They then show that, given a dataset of a particular dimensionality,
the number of samples required to achieve the theoretically optimal, accurate classifier is
inversely proportional to SNR.
Likewise, the number of samples required to achieve the theoretically optimal, robust classifier
is inversely proportional to AdvSNR.
Because AdvSNR is never greater than SNR for a given dataset, it follows that
achieving the same robust generalization as standard generalization requires at least the same amount of samples.

Bhattacharjee et al.~\cite{Bhattacharjee:Jha:Chaudhuri:PMLR:2021} study the sample complexity gap for linear classifiers,
as a factor of data dimensionality (the number of features representing samples ) and 
separation (the distance between samples from different classes).
The authors show that the sample complexity gap is directly proportional to the dimensionality of the data when
the allowed perturbation radius of adversarial samples is similar to the distance between classes.
%That is, in this setup, the higher the dimensionality of the data, the more samples are needed for robust generalization.
However, such a gap no longer exists in well-separated data, when the perturbation radius is much smaller than 
\mbox{the distance between classes}.


Similarly, Gourdeau et al.~\cite{Gourdeau:Kanade:Kwiatkowska:Worrell:JMLR:2021, Gourdeau:Kanade:Kwiatkowska:Worrell:IJCAI:2022} show that,
for simple classifiers based on feature conjunctions and $\alpha$-log-Lipschitz distributions laying on a boolean hyper-cube, 
the sample complexity is a function of the data dimensionality $d$ and the adversarial perturbation budget.
Specifically, when the adversarial perturbation size is bounded by $log (d)$, the sample complexity is polynomial to the dimensionality;
when the perturbation size is at least $log (d)$, the sample complexity becomes superpolynomial to dimensionality.
Javanmard et al.~\cite{Javanmard:Soltanolkotabi:Hassani:COLT:2020} focus on adversarially-trained linear regression models
for standard Gaussian distributions.
The authors show that, when the number of samples is greater than the data dimensionality,
there exists a trade-off between adversarial and standard risks.
Moreover, this trade-off improves as the number of samples per \mbox{dimension increases}.


Cullina et al.~\cite{Cullina:Bhagoji:Mittal:NeurIPS:2018} give an upper bound on the number of samples needed
for robust generalization for the binary classification problem with linear classifiers in a distribution-agnostic setup,
with $L_p$ norm-bounded adversaries.
The authors derive the upper bound using the classifier VC dimension~--
a common measure of the capacity and the expressive power of the classifier, shown earlier
to be useful to determine the upper limit of sample complexity for standard generalization~\cite{Shalev-Shwartz:Ben-David:2014}.
They show that the VC dimension for learning adversarially-robust models remains the same as that for learning accurate models, which means that the upper bound of sample complexity is identical for standard and robust generalization
in this setup.
However, the authors demonstrate that this conclusion does not generalize to other types of classifiers and types of adversaries.


Similarly, Montasser et al.~\cite{Montasser:Hanneke:Srebro:NeurIPS:2022} study binary classifiers
constructed using 
the one-inclusion graph algorithm~\cite{Haussler:Littlestone:Warmuth:IC:1994}. 
They show that both the lower and the upper bound of sample complexity is finite, i.e., 
one can achieve robust generalization using a finite number of samples in this setup.

Xu and Liu~\cite{Xu:Liu:NeurIPS:2022} study sample complexity bounds in a multi-class setup. 
As VC dimension is defined only for the binary case, the authors propose 
Adversarial Graph dimension and Adversarial Natarajan dimension metrics, 
which extend their corresponding counterparts, 
Graph dimension~\cite{BenDavid:CesaBianchi:Haussler:Long:JCSS:1995} and Natarajan dimension~\cite{Natarajan:ML:2004}, 
commonly used in multi-class learning. 
The authors show that sample complexity is upper-bounded by the former and lower-bounded by the latter metric. 
 
\vspace{0.05in}
\noindent
\roundrect{2} {\bf Resolving the Sample Complexity Gap}.
As the number of labeled samples required to achieve robust generalization could be large and not readily available,
researchers explore cheaper alternatives, such as, unlabeled data and generated (fake) data.
Uesato et al.~\cite{Uesato:Alayrac:Huang:Stanforth:Fawzi:Kohli:NeurIPS:2019} and
Carmon et al.~\cite{Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019}
concurrently proposed to use pseudo-labeling~\cite{Scudder:ITIT:1965}~--
a process of assigning labels to unlabeled samples using a classifier trained on a set of labeled samples,
assessing the effectiveness of their approaches on Schmidt's Gaussian mixture.
The main result of both works is that closing the sample complexity gap requires a number of unlabeled samples
proportional to the dimensionality of the data, albeit with a higher quantity than for the labeled samples,
likely due to the ``noise'' in generating labels.
The main difference between the works is that while Uesato et al. show that in their setup (a specific linear classifier)
the quantity of the required unlabeled samples only depends on data dimensionality,
Carmon et al.~\cite{Carmon:Raghunathan:Schmidt:Duchi:Liang:NeurIPS:2019} use a less restrictive setup and show that the quantity of unlabeled samples also depends on the original sample complexity for standard generalization.
Both of these works empirically evaluate the effectiveness of their proposed approaches on the \cifarten and \svhn datasets showing that unlabeled data could be a much cheaper alternative to labeled data for enhancing the robustness of models.


Najafi et al.~\cite{Najafi:Maeda:Koyama:Miyato:NeurIPS:2019}
note that the biggest risk of using a mixture of labeled and unlabeled datasets for learning adversarially robust models is
the uncertainty in sample labels.
Given an estimate of the quality of pseudo-labels,
the authors derive the minimum ratio between labeled and unlabeled samples required to avoid the additional
adversarial risk induced by label uncertainties. 

Instead of using unlabeled data, which might also be hard to find,
Gowal et al.~\cite{Gowal:Rebuffi:Wiles:Stimberg:Calian:Mann:NeurIPS:2021} suggest
using Generative Adversarial Networks (GANs) to generate labeled data.
The authors show that GANs are more effective than other methods, e.g., image cropping, when
producing additional samples.
This is because such models result in a more diverse dataset, which is beneficial for increasing robust accuracy.
Using images from \cifarten, \cifarhundred~\cite{CIFAR:ten:hundred:2009}, \svhn, and \tinyset~\cite{Torralba:Fergus:Freeman:TPAMI:TinyImageSet:2008}, the authors show that their proposed approach can significantly increase robust accuracy without the need for additional real samples.

Xing et al.~\cite{Xing:Song:Cheng:NeurIPS:2022} reason about the effects of real unlabeled and 
generated data on robustness of adversarially trained models. 
As real data is more informative for building decision boundaries, and as 
both real unlabeled and generated data need pseudo-labeling, 
the difference between real unlabeled and generated data boils down to the quality of data generators. 
Following this reasoning, the authors propose a strategy that assigns lower weights to the loss from generated samples compared to real samples during adversarial training, 
with the exact weights (i.e., the representation of the generator quality) being determined through cross-validation.

\vspace{0.05in}
\noindent
\roundrect{3} {\bf Effects of Data Imbalance}.
Wu et al.~\cite{Wu:Liu:Huang:Wang:Lin:CVPR:2021} analyze the adversarial robustness of DNNs on long-tail distributions: setups where the training data contains a large number of classes with few samples.
They show that robust generalization is harder to achieve on such distributions and compare the performance of
multiple adversarially trained classifiers that use learning algorithms specifically designed for such setups.
The comparisons show that scale-invariant classifiers~\cite{Wang:Wang:Zhou:Ji:Gong:Zhou:Li:Liu:CVPR:2018,Pang:Yang:Dong:Xu:Zhu:Su:NeurIPS:2020} result in higher robust accuracy as they
avoid assigning smaller weights to minority classes, which, in turn, promotes robust generalization by reducing bias in the decision boundary.

Both Wang et al.~\cite{Wang:Xu:Han:Xiaorui:Yaxin:Thuraisingham:Tang:ArrXiv:2021} and 
Qaraei et al.~\cite{Qaraei:Babbar:ML:2022}
propose to use re-weighted loss functions to improve robustness of under-represented classes.
Specifically, Wang et al. show that, 
for Gaussian mixture distributions, the robustness gap between classes depends on the amount of imbalance and the overall separation of a dataset. 
The authors thus propose modifying the loss function to assign weights correlated with data imbalance
while also promoting separation. 
Qaraei et al. focus on extreme multilabel text classification, where the output space is extremely large and the data follows a strongly imbalanced distribution. 
They also recommend using re-weighted loss functions to mitigate the robustness issue of the under-represented classes. 
