\section{Results}
\label{sec:results}
We present the results of our analysis by organizing the papers according to the robustness-related data property they discuss: 
number of samples,
dimensionality,
type of distribution,
density,
concentration,
separation,
label quality, and
domain-specific properties.
Papers that discuss more than one data property are presented in all corresponding sections.
That is, in what follows, a paper can be discussed in more than one section.
Section~\ref{sec:results-summary} summarizes our findings.  
Furthermore, a detailed categorization and comparison of papers collected in this survey is available in our online appendix~\cite{appendix}.

To ease navigation, for each discussed data property,
we also include a map showing how the relevant papers relate to each other via their citation information.
We further annotate each paper with its \emph{applicability} and \emph{explainability} categories.
Specifically,
we annotate with an \roundrect{A} symbol papers that propose an actionable technique to modify or measure a robustness-related property;
we annotate with \roundrect{E} papers that put extra emphasis on explaining the correlation between a data property and robustness rather than establishing such a correlation.

\input{results_samples}
\input{results_dimensionality}
\input{results_distribution}
\input{results_density}
\input{results_separation}
\input{results_concentration}
\input{results_labels}
\input{results_domain}


\subsection{Summary of Results}
\label{sec:results-summary}

Overall, the surveyed papers are mostly in agreement on how each of the identified data properties influences adversarial robustness. The main findings are given below. 

\vspace{0.05in}
\noindent {\bf Number of Samples.} More training samples are needed for robust than for standard generalization.
For a variety of training setups (i.e., different types of classifiers and data distributions), the number of training samples required to achieve robust generalization is proportional to the dimensionality of the training data. Unlabeled samples or generated data can be used to fulfill the need for more samples needed for robust generalization, i.e., to close the sample complexity gap.
Class imbalance, i.e., having an imbalanced number of samples across different classes, hurts robust generalization due to the model bias towards over-represented samples. 
Re-weighted loss functions can be used to mitigate this model bias and thereby improve robustness.


\vspace{0.05in}
\noindent
{\bf Dimensionality.} Dimensionality captures the size of the feature set.
Higher dimensionality correlates with
higher adversarial risk, worse standard-to-adversarial risk trade-off, difficulty in robustness certification, and difficulty in applying common defense techniques.
This is because adversarial attacks can exploit the excess dimensions to construct adversarial examples. 

\vspace{0.05in}
\noindent
{\bf Distribution.} Some data distributions are more robust than others, e.g., mixtures of Bernoulli distributions are more robust than mixtures of Gaussian distributions.
Learning feature representations that resemble robust distributions can improve robustness.


\vspace{0.05in}
\noindent
{\bf Density.} Density reflects the closeness of samples in a particular bounded region (intra-class distance).
Adversarial examples are commonly found in low-density regions of data, where samples are far apart from each other.
This is because models cannot accurately learn decision boundaries near low-density regions due to the small number of samples available. As such, high data density for each class correlates with lower adversarial risk.

\vspace{0.05in}
\noindent
{\bf Separation.} Separation characterizes the distance of samples from different classes to each other (inter-class distance).
Greater separation between classes decreases adversarial risk as it is harder to generate perturbations that 
will cross the decision boundaries between classes.
Most papers that provide techniques for improving separations do so by modifying the loss function to learn a latent data representation with higher separation.
They also ensure that this increase in separation does not come at the expense of decreasing density, as these two concepts are closely related.

\vspace{0.05in}
\noindent
{\bf Concentration.} Given a function defined over a non-empty set,
concentration (from the phenomenon of concentration of measure~\cite{Talagrand:1996:AnnalsProbability})
is the minimum value of the function after expanding the input set by $\epsilon$ in all dimensions.
For example, expanding the set of misclassified samples by a certain $\epsilon$ gives a set of possible samples that can be
misclassified with an $\epsilon$-size perturbation (candidate adversarial examples).
Concentration, in this case, measures the minimal possible size of this set, which provides the upper bound of the achievable model robustness.
As some datasets tend to exhibit inherently high concentration,
e.g., datasets that lie on unit hypersphere~\cite{Mahloujifar:Diochnos:Mahmoody:AAAI:2019},
achieving high robust generalization is harder for these datasets.
The impact of high concentration on adversarial robustness is further magnified for high-dimensional data.


\vspace{0.05in}
\noindent
{\bf Label Quality.} High label noise correlates with higher adversarial risk.
Label noise can be automatically generated as part of adversarial training 
because perturbation can change the semantics of the perturbed data samples. 
Labels thus have to be carefully examined and rectified.
More specific labels, e.g., ``cat'' and ``dog'' instead of ``animal'', are more robust than coarse labels, as they allow the model to extract more distinct features.
Learning for different tasks concurrently, e.g., to simultaneously locate and estimate the distance of objects in images,improves the robustness of the learned models, as the model can utilize the information from multiple sources of data.


\vspace{0.05in}
\noindent
{\bf Domain-Specific.} Image frequency, i.e., the rate of change in pixel value, is shown to be correlated with robustness.
As humans perceive differences in images by focusing on low-frequency components 
whereas models consider both low and high-frequency components,  
high-frequency perturbations can ``fool'' models while being imperceptible to humans. 
Adversarial training tends to focus on high-frequency components and, as a result,
adversarially trained models rely more heavily on low-frequency components. 
While this helps ensure that models focus on low-frequency components as well, it introduces new issues
related to clean and robust accuracy. 
Encouraging a diverse distribution of frequencies in training data results in lower adversarial risk.




