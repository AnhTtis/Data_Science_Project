\subsection{Distribution}
\label{sec:results-distribution}

\begin{figure*}[h]
\centering
  \begin{minipage}{0.52\textwidth}
  \centering
  \vspace{0.2in}
  \includegraphics[width=\linewidth]{images/distribution_gaus_bern_illustration.pdf}
  \vspace{-0.22in}
  \caption{Distribution illustration.}
  \label{fig:distribution_illustration}
  \vspace{-0.1in}
  \end{minipage}
  \hspace{0.1in}
  \begin{minipage}{0.44\textwidth}
	\centering
	\vspace{-0.1in}
	\includegraphics[width =\textwidth]{images/distribution_citation_graph.pdf}
	\vspace{-0.25in}
    \caption{Papers discussing distribution.}
  \label{fig:distribution}
  \end{minipage}
  \vspace{-0.1in}
\end{figure*}

\emph{Distribution} refers to a function that encodes how samples lie in space, usually by giving the probabilities of their occurrence in particular regions.
Common types of distributions, such as uniform, Bernoulli, and Gaussian
are introduced in Section~\ref{sec:background_distribution}.
Fig.~\ref{fig:distribution_illustration} shows examples of datasets that follow a Gaussian distribution (left) and a Bernoulli distribution (right).
The term \emph{variance} refers to a measure of dispersion that takes into account the spread of all data points in a dataset.
Specifically, the \emph{variance of a distribution} measures the dispersion of samples from the mean;
\emph{feature variance} measures the dispersion of samples over a particular feature only.
We say that a distribution satisfies \emph{symmetry} when distributions on either side of the mean mirror each other.


Papers that discuss how distribution properties, including variance and symmetry, influence models' robustness
are shown in Fig.~\ref{fig:distribution}.
They can be categorized into:
\roundrect{1} papers showing that model robustness depends on the underlying data distribution,
\roundrect{2} papers identifying properties of distributions that improve robustness, and
\roundrect{3} papers introducing techniques to transform distributions into ones that are more optimal for robustness.

\vspace{0.05in}
\noindent
\roundrect{1} {\bf Types of Distributions}.
As discussed in Section~\ref{sec:results-number-of-samples},
Schmidt et al.~\cite{Schmidt:Santurkar:Tsipras:Talwar:Madry:NeurIPS:2018} prove, for nonlinear classifiers,
that a mixture of Gaussian distributions incurs higher sample complexity
for robust generalization than a mixture of Bernoulli distributions.
Likewise, Ding et al.~\cite{Ding:Lui:Jin:Wang:Huang:ICLR:2019} show that
a distribution shift alone can affect robust accuracy while retaining the same standard accuracy.
Specifically, the authors prove that uniform data lying on a unit cube results in more robust models than
uniform data lying on a unit sphere.
They further experiment with \mnist and \cifarten datasets,
applying existing semantically lossless transformations, namely \emph{smoothing} and \emph{saturation},
to cause the distribution shift.
The results of this experiment show that robustness decreases gradually when transforming \mnist from
a unit-cube-like to a unit-sphere-like distribution and increases for \cifarten when going the opposite way;
in both cases, the models retain \mbox{their standard accuracy}.

Fawzi et al.~\cite{Fawzi:Fawzi:Fawzi:NeurIPS:2018} study the robustness of data distributions modeled by
a smooth generative model~-- a type of generative model that maps samples from input space to output
space while preserving their relative distances, e.g., to compress data.
The authors show that smooth generative models with high-dimensional input space
produce data distributions that make any classifier trained on this data inherently vulnerable.
The authors conclude that non-smoothness and low input space dimensionality
are desirable when modeling data with generative models.

\vspace{0.05in}
\noindent
\roundrect{2} {\bf Properties of Distributions}.
Izmailov et al.~\cite{Izmaliov:Sugrim:Chadha:McDaniel:Swami:MILCOM:2018} show that, in a binary classification setting,
features with small variance in both classes and means close to each other
cause adversarial vulnerability.
Moreover, a feature with a small variance in one class can still cause vulnerability
even if the means of this feature in both classes are farther separated
but the second class has a larger feature variance.
Intuitively, that is because models tend to assign non-zero weights to such features,
which can be leveraged by attackers to shift the classification into the wrong class.
That is, even small perturbations in such features can shift data points to another class.
To increase robustness, the authors suggest removing such features, either based on domain knowledge
or based on feature evaluation metrics, such as, mutual information~\cite{Shannon:1949}.


Similarly,
Lee et al.~\cite{Lee:Lee:Yoon:CVPR:2020} prove that decreasing feature variance in individual classes
can increase robustness for Schmidt's Gaussian mixtures.
These mixtures have equivalent feature variances for all classes and separated means.
In such a setting, low feature variance implies that the feature has a strong correlation with the class and
perturbing this feature will unlikely result in a vulnerability 
(i.e., will likely result in a semantically-meaningful change).
However, even when features have low variance, if these features are
non-robust~\cite{Ilyas:Santurkar:Tsipras:Engstrom:Tran:Madry:NeurIPS:2019}, i.e., hold no semantic information,
and have a smaller variance in the training data than
in the underlying true population,
they will still cause adversarial vulnerability as adversarially trained models tend to overfit to them.
As a countermeasure, the authors propose a label-smoothing-based data augmentation technique that uses
continuous instead of discrete values for labels and acts like a regularization method that prevents the model from overfitting to such features.


\begin{wrapfigure}{r}{0.30\textwidth}
  \vspace{-0.35in}
  \begin{center}
    \includegraphics[width=0.26\textwidth]{images/gaussianMixFig.pdf}
  \end{center}
    \vspace{-0.15in}
  \caption{Asymmetrical dataset.}
  \label{fig:asymetric_data}
  \vspace{-0.1in}
\end{wrapfigure}
Richardson and Weiss~\cite{Richardson:Weiss:JMLR:2021} claim that adversarial vulnerability can be caused by
sub-optimal data distributions and/or sub-optimal training methods.
The authors define synthetic binary datasets (of images) that use Gaussian distributions with separated means
and say that a dataset is symmetric if and only if classes have the same variance.
They further prove that even an optimal classifier is non-robust when the underlying dataset has strong asymmetry,
as in the
example in Fig.~\ref{fig:asymetric_data}.
If the dataset is symmetric the optimal classifier is provably robust,
even though a sub-optimal training method can still cause vulnerability when trained on this dataset.


\vspace{0.05in}
\noindent
\roundrect{3} {\bf Transforming Distributions}.
Both Pang et al.~\cite{Pang:Du:Zhu:ICML:2018} and Wan et al.~\cite{Wan:Chen:Yu:Wu:Zhong:Yang:TPAMI:2022}
change the latent DNN feature representation to be similar to Gaussian mixtures. 
Specifically, Pang et al. show that,
for Linear Discriminant Analysis (LDA) classifiers trained on Gaussian mixtures, the robustness radius of LDA
is proportional to the distance between the Gaussian centers.
The robustness of LDA is further maximized for symmetric Gaussian mixtures.
The authors thus modify the DNN loss function to create a latent feature representation similar to symmetric Gaussian mixtures
and further replace the last layer of DNN from commonly used Softmax Regression~\cite{Cramer:2002} to LDA.
To achieve the desired robustness radius, the authors compute the coordinates of the desired
Gaussian centers (as a function of the number of classes and the dimensionality of the input data)
and feed this data to the loss function.
Departing from the assumption that symmetric Gaussian mixtures are advantageous for the underlying model robustness,
Wan et al. modify the DNN loss function to compute the centers of the Gaussians directly while generating symmetric \mbox{Gaussian feature distributions}.
