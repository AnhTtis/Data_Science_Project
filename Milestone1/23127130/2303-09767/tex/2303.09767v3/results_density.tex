\subsection{Density}
\label{sec:results-density}

\begin{figure*}[h]
	\centering
	\vspace{-0.15in}
	\begin{minipage}{0.53\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/density_overview.pdf}
		\vspace{-0.14in}
		\caption{Density illustration.}
		\label{fig:density_guideline}
		\vspace{-0.1in}
	\end{minipage}
	\hspace{0.1in}
	\begin{minipage}{0.42\textwidth}
		\centering
		\includegraphics[width =\textwidth]{images/density_citation_graph.pdf}
		\vspace{-0.25in}
		\caption{Papers discussing density. }
		\label{fig:density}
	\end{minipage}
\end{figure*}


\emph{Density} measures the closeness of samples in a particular bounded region.
For continuous data, it is mathematically described by the probability density function,
which gives the probability for a variable to take a certain range of values.
For discrete data, it is described as the probability mass function, which
gives the probability for a variable to take a particular value.
We say that an area is dense when there is a high probability that random samples lie in the same area, i.e.,
close to each other.
For example, the dataset on the right-hand side of Fig.~\ref{fig:density_guideline} contains a larger number of samples in close proximity and, thus, is more dense than the dataset on the left-hand side of the figure.
Furthermore, density can be defined over samples from one class, in which case, it is referred to as \emph{class density}.

Papers that study how density influences adversarial robustness are shown in Fig.~\ref{fig:density}. They can roughly be divided into
\roundrect{1}~papers discussing the effect of class density on robustness and
\roundrect{2}~papers proposing attacks and defenses using density information.


\vspace{0.05in}
\noindent
\roundrect{1} {\bf Effects of Class Density}.
Shafahi et al.~\cite{Shafahi:Huang:Studer:Feizi:Goldstein:ICLR:2019} 
show that datasets with a higher upper bound of class density lead to better robustness.
In particular, for image datasets, the authors show that images of lower complexity,
e.g., with simple objects on plain backgrounds, have a higher correlation among adjacent pixels.
Datasets comprised of such images have a higher density, as pixel values are more frequently repeated, and, thus, lead to better robustness.
The authors confirm this observation by showing that classifiers trained on \mnist, which has a lower image complexity and thus higher density than \cifarten, are more robust than those trained on \cifarten.
Furthermore, the authors state that class density is a better predictor of robustness than dimensionality:
even after up-scaling \mnist to the same dimensionality as \cifarten,
it still has a higher density and thus results in more robust classifiers than \cifarten.

Naseer et al.~\cite{Naseer:Prabakaran:Hasan:Shafique:ML:2023} show that imbalance in class densities 
is a more substantial predictor of robustness bias among classes than the difference in the number of samples.
The authors further propose a two-step strategy to remove this bias through data augmentation.
First, they gradually increase the perturbation size in samples from all classes and identify which classes get misclassified with the smallest perturbation size, treating this as an indication of low density. 
They then generate realistic and diversified samples for these classes, 
to reduce imbalance, which in turn leads to improved robustness.

\vspace{0.05in}
\noindent
\roundrect{2} {\bf Attacks and Defenses Using Density}.
Several works note that adversarial examples are commonly found in low-density regions of the training dataset, as models are unable to learn accurate decision boundaries using a small number of samples from these regions.
Zhang et al.~\cite{Zhang:Chen:Song:Boning:Dhillon:Hsieh:ICLR:2019}
propose an attack strategy that retrieves candidate samples from low-density regions and
perturbs them to generate adversarial examples.
The authors demonstrate that, even after adversarial training, models will not be robust to adversarial attacks that target these low-density regions.


A similar finding by Zhu et al.~\cite{Zhu:Sun:Li:ICLR:2022} suggests that adversarial examples from low-density regions have a higher probability of being transferable between different models trained on the same dataset.
Based on this observation, the authors propose an attack that increases the transferability of adversarial examples by
identifying perturbation directions that maximize both the adversarial risk and
the alignment with the direction of density decrease for the underlying data distribution,
i.e., move samples towards regions with lower density.

Departing from the same idea that low-density regions are prone to adversarial attacks,
Song et al.~\cite{Song:Kim:Nowozin:Ermon:Kushman:ICLR:2017} focus on creating a defense mechanism
that uses generative models to detect if a sample comes from a low-density region when making predictions.
If so, the sample is moved towards a more dense region of the training data as a ``purification'' step.

To harden models directly, Pang et al.~\cite{Pang:Xu:Dong:Du:Chen:Zhu:ICLR:2020} propose a new loss function for DNNs,
to learn dense latent feature representations.
The authors first show that the commonly used Softmax Cross-Entropy loss function induces sparse representations
(i.e., with low class density),
which lead to vulnerable models. This is because a low number of samples in close proximity to each other prevent a model from learning reliable decision boundaries.
They then propose a loss function that explicitly encourages feature representations to concentrate around class centers;
like in their earlier work~\cite{Pang:Du:Zhu:ICML:2018}, the authors compute the coordinates of the desired
class centers (as a function of the number of classes and the dimensionality of the input data)
to maximize the distances between the centers.
The authors demonstrate that the proposed approach improves robustness under both standard and adversarial training.
