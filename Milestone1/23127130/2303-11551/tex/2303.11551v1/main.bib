@inproceedings{NIPS1999_b618c321,
 author = {Hershey, John and Movellan, Javier},
 booktitle = {NeurIPS},
 title = {Audio {V}ision: Using Audio-Visual Synchrony to Locate Sounds},
 year = {1999}
}
@inproceedings{NIPS2000_9f699296,
 author = {Slaney, Malcolm and Covell, Michele},
 booktitle = {NeurIPS},
 title = {{FaceSync}: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks},
 year = {2000}
}
@INPROCEEDINGS{5745053,  
 author={Morishima, Shigeo and Ogata, Shin and Murai, Kazumasa and Nakamura, Satoshi},  
 booktitle={IEEE ICASSP},
 title={Audio-visual speech translation with automatic lip syncqronization and face tracking based on 3-D head model},   
 year={2002}
}

@INPROCEEDINGS{asr_is_all_you_need,
  title={ASR is All You Need: Cross-Modal Distillation for Lip Reading},
  author={Triantafyllos Afouras and Joon Son Chung and Andrew Zisserman},
  booktitle={IEEE ICASSP},
  year={2020}
 }

@article{lewis1991automated,
  title={Automated lip-sync: Background and techniques},
  author={Lewis, John},
  journal={The Journal of Visualization and Computer Animation},
  volume={2},
  number={4},
  pages={118--122},
  year={1991},
  publisher={Wiley Online Library}
}
@InProceedings{VoxCeleb2,
  author       = "Joon~Son Chung. and Arsha Nagrani and Zisserman, A.",
  title        = "VoxCeleb2: Deep Speaker Recognition",
  booktitle    = "INTERSPEECH",
  year         = "2018",
}
@InProceedings{Chung16a,
  author       = "Joon~Son Chung and Andrew Zisserman",
  title        = "Out of time: automated lip sync in the wild",
  booktitle    = "Workshop on Multi-view Lip-reading, ACCV",
  year         = "2016",
}
@INPROCEEDINGS{8682524,  
 author={Chung, Soo-Whan and Chung, Joon Son and Kang, Hong-Goo}, 
 booktitle={IEEE ICASSP},
 title={{Perfect Match}: Improved Cross-modal Embeddings for Audio-visual Synchronisation},
 year={2019},  
}

@inproceedings{10.1145/3394171.3413532,
author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},
year = {2020},
booktitle = {ACM International Conference on Multimedia},
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv:1810.04805},
  year={2018}
}
@inproceedings{NEURIPS2020_1457c0d6,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}
@inproceedings{50650,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{Ma2021EndToEndAS,
  title={End-To-End Audio-Visual Speech Recognition with Conformers},
  author={Pingchuan Ma and Stavros Petridis and Maja Pantic},
  booktitle={IEEE ICASSP},
  year={2021},
}
@InProceedings{Lin_2020_ACCV,
    author    = {Lin, Yan-Bo and Wang, Yu-Chiang Frank},
    title     = {Audiovisual Transformer with Instance Attention for Audio-Visual Event Localization},
    booktitle = {ACCV},
    year      = {2020}
}
@inproceedings{NEURIPS2021_cb3213ad,
 author = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
 booktitle = {NeurIPS},
 title = {{VATT}: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text},
 year = {2021}
}
@InProceedings{Chen21b,
              title        = "Audio-Visual Synchronization in the Wild",
              author       = {Honglie Chen and Weidi Xie and Triantafyllos Afouras and Arsha Nagrani and Andrea Vedaldi and Andrew Zisserman},
              booktitle    = "BMVC",
              year         = "2021"}

@inproceedings{kadandale22_interspeech,
  author={Venkatesh Shenoy Kadandale and Juan F. Montesinos and Gloria Haro},
  title={{VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices}},
  year=2022,
  booktitle={Interspeech},
}
@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv:1807.03748},
  year={2018}
}
@inproceedings{li2022clip,
  title={Clip-event: Connecting text and images with event structures},
  author={Li, Manling and Xu, Ruochen and Wang, Shuohang and Zhou, Luowei and Lin, Xudong and Zhu, Chenguang and Zeng, Michael and Ji, Heng and Chang, Shih-Fu},
  booktitle={IEEE CVPR},
  year={2022}
}
@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  year={2021},
}
@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {NeurIPS},
 title = {Attention is All you Need},
 year = {2017}
}
@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {IEEE CVPR},
year = {2016}
}

@InProceedings{pmlr-v9-gutmann10a,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyv√§rinen, Aapo},
  booktitle = 	 {IEEE ICASSP},
  year = 	 {2010},
}
@inproceedings{zhang2017s3fd,
  title={S3fd: Single shot scale-invariant face detector},
  author={Zhang, Shifeng and Zhu, Xiangyu and Lei, Zhen and Shi, Hailin and Wang, Xiaobo and Li, Stan Z},
  booktitle={IEEE ICCV},
  year={2017}
}
@inproceedings{frosst2019analyzing,
  title={Analyzing and improving representations with the soft nearest neighbor loss},
  author={Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  year={2019},
}
@INPROCEEDINGS{kalantidis2020hard,
  title={Hard negative mixing for contrastive learning},
  author={Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, Noe and Weinzaepfel, Philippe and Larlus, Diane},
 booktitle = {NeurIPS},
  year={2020}
}
@ARTICLE{8585066,  
 author={Afouras, Triantafyllos and Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},  
 journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
 title={Deep Audio-visual Speech Recognition},   
 year={2018}
}

@article{afouras2018lrs3,
  title={LRS3-TED: a large-scale dataset for visual speech recognition},
  author={Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
  journal={arXiv:1809.00496},
  year={2018}
}
@INPROCEEDINGS{8461326,
  author={Petridis, Stavros and Stafylakis, Themos and Ma, Pingehuan and Cai, Feipeng and Tzimiropoulos, Georgios and Pantic, Maja},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={End-to-End Audiovisual Speech Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={6548-6552},
  doi={10.1109/ICASSP.2018.8461326}}