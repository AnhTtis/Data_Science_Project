\section{Related Work}
% In this section, we discuss the recent approaches that generate long videos and the transformer architectures that reduces the complexity of transformers.

\paragraph{Video Generative Models}
\iffalse
Video generative models have shown rapid growth with the advance of generative frameworks such as generative adversarial networks (GANs~\cite{GAN}) and autoregressive models.

GAN-based models extend image GANs by considering as a sequence of frames \cite{DVDGAN,TGAN,TGANv2,MoCoGAN,MoCoGAN-HD,LDVDGAN} or as a function that returns a frame at specific time \cite{StyleGAN-V, DIGAN}.
Although GAN-based video models show fast inference time, they fail to model complex spatiotemporal structures as shown in Section \ref{sec:exp_long_video}.
On the other hand, autoregressive models model the joint likelihood of videos directly on pixel space~\cite{VPN, ARVideo} or on discrete latent space \cite{LVT, videoGPT, TATS, NUWA}.
To be specific, VideoGPT~\cite{videoGPT} models videos by adopting the transformers on latent space provided with 3d VQVAE, and TATS~\cite{TATS} enhances the VideoGPT by proposing a temporal agnostic 3d VQGAN that generalizes to arbitrary video length.
The combination of discrete representation and autoregressive transformers showed surprising performance on video generation, but they are not scalable to train and inference with long videos and are prone to error propagation. Thus, we address the issues of autoregressive transformers while enjoying their powerful performance by combining the proposed MeBT with bidirectional modeling.
\fi
% styleGAN-V?
% Video generative models have shown rapid growth
% Recent works can be categorized into GAN-based and AR based.
% GAN-based shows fast inference speed but fails to model complex spatiotemporal structures.
% On the other hand, AR model showed powerful performance.
% They are good for modeling complex videos but not scalable to train with long videos.

% Various kind of works tried to generate long videos with

Video generative models have shown rapid growth with the advance of generative frameworks.
Various approaches based on Generative Adversarial Networks (GAN)~\cite{DVDGAN,TGAN,TGANv2,MoCoGAN,MoCoGAN-HD,LDVDGAN}, Variational Autoencoders (VAE)~\cite{SVG, SAVP, FitVid, CWVAE, SRVP}, and Implicit Neural Representations (INR)~\cite{StyleGAN-V, DIGAN} are proposed to extend the image generative models to powerful video sequence models.
Despite the impressive success, these methods are mostly displayed in simple and short videos, while progress in modeling complex scenes and motions is still behind ones in the image domains.
% \jh{One line of works models videos with variational inference \cite{SVG, SAVP, FitVid, CWVAE, SRVP}.}
% Another line of works models videos with an extension of image GANs by considering as a sequence of frames \cite{DVDGAN,TGAN,TGANv2,MoCoGAN,MoCoGAN-HD,LDVDGAN} or as a function that returns a frame at a specific time \cite{StyleGAN-V, DIGAN}.
% \jh{Both approaches with GANs or variational inference usually demonstrate powerful performance on modeling simple videos, but their performance on long and complex videos is not explored much.}
% Although GAN-based video models show fast inference time, they fail to model complex spatiotemporal structures as shown in Section \ref{sec:exp_long_video}.
Recently, autoregressive transformers~\cite{LVT, videoGPT, TATS, NUWA, CogVideo} demonstrated promising results in modeling complex videos, closing the gap between the image and video generative models.
However, they are mostly limited in short-term videos due to quadratic computation, and inherently suffer from slow inference time and error propagation due to autoregressive process.
% Although there has been success in generating long-term videos, they are usually with the powerful conditioning variables such as real video frames~\cite{CWVAE, HVP, SRVP} or text~\cite{CogVideo}. 
% Extending the success in Transformers to long-term videos have been studied
This motivates us to seek efficient yet robust transformers for long-term videos.
% Finally, a line of works studied long-term video generation in various context, often conditioned on powerful side information such as real frames~\cite{CWVAE, SRVP, FitVid, LeeKCKR21}, dense pixel labels~\cite{HVP}, action sequences~\cite{Villegas2017, KimNCK19}, or text description~\cite{CogVideo,NUWA}.
Long-term video generation has been studied under various context, often conditioned on powerful side information such as real frames~\cite{CWVAE, SRVP, FitVid, LeeKCKR21, GHVAE, ImprovedVRNN}, dense pixel labels~\cite{HVP, Akan2022, LiangLDX17}, action sequences~\cite{Villegas2017, KimNCK19}, or text description~\cite{CogVideo,NUWA}.
Apart from these, we study the potential of transformers for unconditional long-term video generation.  
% On the other hand, autoregressive transformers model the joint likelihood of videos directly on pixel space~\cite{ARVideo} or on discrete latent space \cite{LVT, videoGPT, TATS, NUWA, CogVideo}.
% To be specific, VideoGPT~\cite{videoGPT} models videos by adopting the transformers on latent space provided with 3d VQVAE, and TATS~\cite{TATS} enhances the VideoGPT by proposing a temporal agnostic 3d VQGAN that generalizes to arbitrary video length.
% The combination of discrete representation and autoregressive transformers showed surprising performance on video generation, but they are not scalable to train and inference with long videos and are prone to error propagation.
% \jh{There have been many approaches that generate long videos with 
%  meaningful conditions. They generate long videos while conditioning on ground truth context frames \cite{CWVAE, HVP, SRVP} or text \cite{CogVideo}. However, generating long videos without reliable conditions is still challenging.}
% \jh{MeBT enables to model the unconditional distribution of long videos by combining linear transformers with bidirectional modeling.}
\cutparagraphup
\vspace{-0.2in}
\paragraph{Efficient Transformers}
\label{sec:effi_transformers}
% Transformers are not scalable.
% Many different works attempt to try this issue.
% Sparse attention reduces the complexity by restricting the number of tokens to attend.
% It may harm the performance (receptive field or predefined structure)
% another works attempt to utilize latent bottleneck.
% Our work adopts the latent bottleneck for bidirectional video transformer. (Our work is the first attempt).
% Transformers are not scalable to long sequences due to their quadratic complexity.
% Large body of works are proposed to address this problem.
To improve the scalability of transformers for long sequences, a large volume of work has been developed to approximate the quadratic computation. 
One promising direction is sparse attention that approximates dense pair-wise attention by restricting the size of the neighborhood that each token can attend to~\cite{MaskViT, NUWA, CogVideo, BigBird, Ho2019, LongFormer}. 
However, the fixed sparsity patterns are often incomprehensive to cover various token-wise dependencies in broad applications, especially when combined with masked bidirectional transformers with high masking ratio.
% However, sparse attention is not suitable for bidirectional transformers as a fixed sparse pattern may hinder the interaction between contexts when the data is highly masked.
% Although many sparse attentions are shown to be universal approximator of dense attention~\cite{BigBird}, in practice the sparse attention may hinder the performance of transformer depending on the given sparsity and network designs.
% Although \cite{BigBird} showed that any transformer can be approximated by stacking sparse attention layers, in practice the sparse attention may hinder the performance of transformer depending on the given sparsity and network designs.
The other line of work proposed latent bottleneck \cite{SetTransformer, SetVAE, LUNA, Perceiver, PerceiverIO} that projects the input to fewer latent embeddings. 
Latent bottleneck can be easily combined with bidirectional transformers as it models dense dependencies between tokens, and this work is the first attempt to demonstrate it in videos. 
% To the best of our knowledge, this is the first attempt to combine the two.
% Contrary to sparse attention methods, latent bottlenecks can approximate the dense dependency between inputs. 
% Several works adopt the sparse attention mechanism to reduce the quadratic complexity of video generative transformers~\cite{NUWA, CogVideo, MaskViT}.
% However, we are the first attempt to adopt latent bottlenecks for it.
% The other line of work proposed latent bottleneck \cite{SetTransformer, SetVAE, LUNA, Perceiver, PerceiverIO} which projects the input to fewer latent embeddings. Contrary to sparse attention methods, latent bottlenecks can approximate the dense dependency between inputs. Several works adopt the sparse attention mechanism to reduce the quadratic complexity of video generative transformers~\cite{NUWA, CogVideo, MaskViT}. However, we are the first attempt to adopt latent bottlenecks for it.
% One line of work proposes sparse attention that approximates
% \shcmt{Finish this section by 8pm}