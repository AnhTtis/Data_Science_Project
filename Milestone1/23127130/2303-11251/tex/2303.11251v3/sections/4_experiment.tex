\input{sections/main_table.tex}
% layout?


\iffalse
\begin{table*}[h]
\caption{Quantitative results on 128-frame video generation. \shcmt{edit caption. mark the best {\bf bold}}}
    \begin{adjustbox}{width=1.0\textwidth}
    \label{tab:longterm}
    \centering
        \begin{tabular}{lccccccccc}
        \toprule
        \\[-1em]& \multicolumn{3}{c}{SkyTimelapse} & \multicolumn{3}{c}{Taichi-HD} & \multicolumn{3}{c}{UCF-101} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Method 
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & Inf.Time & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & Inf.Time
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & Inf.Time\\
        \\[-1em]\Xhline{2\arrayrulewidth}
        \\[-1em]MoCoGAN-HD & 663\tiny{$\pm 12.0$} & 32\tiny{$\pm0.89$} & 0.37s & -\tiny{$\pm$} & -\tiny{$\pm$} & 0.37s & 3060\tiny{$\pm37$} & 225\tiny{$\pm10$} & 0.37s\\
        \\[-1em]DIGAN & 304\tiny{$\pm10$} & 11\tiny{$\pm0.80$} & 0.11s & 1178\tiny{$\pm13$} & 555\tiny{$\pm15$} & 0.11s & 1635\tiny{$\pm70$} & 109\tiny{$\pm9.3$} & 0.11s\\
        \\[-1em]CCVS & N/A & N/A & N/A & N/A & N/A & N/A & 1411\tiny{$\pm45$} & 121\tiny{$\pm11$} & 219s \\
        \\[-1em]TATS-Base & 435\tiny{$\pm12$} & 19\tiny{$\pm1.50$} & 110s & 458\tiny{$\pm21$} & 211\tiny{$\pm18$} & 110s & 1107\tiny{$\pm31$} & 91.8\tiny{$\pm8.5$} & 110s\\
        \\[-1em]TATS-Hierarchical & 455\tiny{$\pm12$} & 21\tiny{$\pm0.99$} & 127s & 803\tiny{$\pm40$} & 371\tiny{$\pm31$} & 127s & 1138\tiny{$\pm53$} & 83.4\tiny{$\pm$11.6} & 127s\\
        \\[-1em]MeBT (Ours) & 239\tiny{$\pm3.6$} & 5.1\tiny{$\pm0.16$} & 6.53s & 399\tiny{$\pm19$} & 139\tiny{$\pm13$} & 6.62s & 968\tiny{$\pm75$} & 75.5\tiny{$\pm12.1$} & 7.53s\\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table*}
\fi




\iffalse
\begin{figure}[h]
    \centering
    % \includegraphics{}
    \vspace{1cm}
    {\color{blue} Short figure on Video Editing (Sky-timelapse)} 
    \vspace{1cm}
    \caption{Caption}
    \label{fig:long_samples}
\end{figure}
\fi

\section{Experiments}
\label{sec:experiments}
% This section presents our results on video synthesis.
% Generated videos are available in the supplementary files.

\subsection{Experimental Setup}
\cutsubsectiondown
\paragraph{Datasets}
We evaluate our method on three popular video benchmarks; SkyTimelapse \cite{stl}, Taichi-HD \cite{taichi}, and UCF-101 \cite{ucf}.
For each dataset, we use 16- and 128-frame-long videos for evaluation of short-term and long-term generation, respectively.
Following prior works, all videos are pre-processed with $128\times128$ spatial resolution.
% \jh{More details are described in the supplementary file.}
More details are described in the Appendix (Section \ref{appx:datasets}).
%{\color{red}We use consecutive frames except for Taichi-HD 16-frame videos where we sample every 4$^{th}$ frame to construct dynamic videos.}
% \shcmt{Discuss details of datasets; how do we choose 128-frame videos? what are the frame-rates? are there any difference in 16 and 128 frame settings?}

\cutparagraphup
\paragraph{Evaluation metrics}
We evaluate the quality of the generated videos using Fréchet Video Distance (FVD) \cite{FVD} and Kernel Video Distance (KVD) \cite{FVD} based on an I3D network trained on Kinetics-600 \cite{kinetics}. 
For 16-frame videos, we follow the evaluation protocol of \cite{TGANv2} and report the average score over 10 runs where each run samples 2048 videos.
For 128-frame videos, we sample 512 videos 5 times and report the average score.
% \shcmt{what was the classifier}
For UCF-101 dataset, we also measure the Inception Score (IS)~\cite{IS} with a pre-trained C3D model~\cite{c3d} following the prior works~\cite{TATS, CCVS, VGAN}. 
% Following the prior works~\cite{CCVS, VGAN}, we additionally compute the Inception Score (IS)~\cite{IS} with a pre-trained C3D model~\cite{c3d} for UCF-101 with 16-frame videos. 
To evaluate the generation quality over time on longer videos (128 frames), we also measure FVD for every 8 frames using a sliding window of 16 frame length.
% In short-term videos, we evaluate the peak training memory and inference time using a batch size 4.
% Furthermore, we evaluate the computational efficiency of the models by measuring the peak training memory and inference time on 16-frame videos with batch size 4.
Furthermore, we evaluate the efficiency of the models by measuring the peak training memory and inference time with batch size 4.
% In short-term videos, we evaluate the peak memory usage in training and inference time using the batch size of one.
%, where the former is measured by the PyTorch \shcmt{discuss what packages you used} with batch size of 1, and the latter is measured by generating a video with batch size of 1.\shcmt{discuss details}. 
% To compare the efficiency, we measured the training peak memory and the time spent in generating a video.

% {\color{red}
% We analyze our model on Sky Time-lapse \cite{stl}, Taichi-HD \cite{taichi}, and UCF-101 \cite{ucf} with 128 x 128 resolution following the prior works~\cite{DIGAN, TATS}. We compute Fréchet Video Distance (FVD) \cite{FVD} and Kernel Video Distance (KVD) \cite{KVD} to measure the quality of generated both 16-frame and 128-frame videos. For UCF-101 with 16-frame videos, we additionally compute the Inception Score (IS)~\cite{IS} with a pre-trained C3D model~\cite{c3d} following~\cite{CCVS, VGAN}. On 128-frame videos, we measured the degradation of FVD over time~\cite{TATS} to quantify the error propagation. We measured FVD of every 16 frames extracted by shifting the window by 8 frames.
% To compare the efficiency, we measured the training peak memory and the time spent in generating a video.
% }
\iffalse
\begin{tabular}{lcccccc}
        \toprule
        \\[-1em]& \multicolumn{2}{c}{SkyTimelapse} & \multicolumn{2}{c}{Taichi-HD} & \multicolumn{2}{c}{UCF-101} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        Method 
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ 
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ \\
        \\[-1em]\Xhline{2\arrayrulewidth}
        \\[-1em]MoCoGAN-HD$_{128}$ & {498\tiny{$\pm 15$}} & {23\tiny{$\pm1.39$}} & {991\tiny{$\pm23$}} & {290\tiny{$\pm12$}} & {1622\tiny{$\pm53$}} & {118\tiny{$\pm6.4$}}\\
        \\[-1em]DIGAN$_{128}$ & 331\tiny{$\pm13$} & 9\tiny{$\pm0.38$} & 764\tiny{$\pm19$} & 230\tiny{$\pm25$} & 1627\tiny{$\pm51$} & 122\tiny{$\pm10$}\\
        % 6048, 5241, 4838
        \\[-1em]MeBT$_{128}$ (Ours) & \bf{239\tiny{$\pm3.6$}} & \bf{5.1\tiny{$\pm0.16$}} & \bf{399\tiny{$\pm19$}} & \bf{139\tiny{$\pm13$}} & \bf{968\tiny{$\pm75$}} & \bf{75.5\tiny{$\pm12.1$}}\\
        \hline
        \\[-1em]CCVS$_{16}$ & N/A & N/A & N/A & N/A & 1411\tiny{$\pm45$} & 121\tiny{$\pm11$} \\
        \\[-1em]TATS-base$_{16}$ & 435\tiny{$\pm12$} & 19\tiny{$\pm1.50$} & 458\tiny{$\pm21$} & 211\tiny{$\pm18$} & 1107\tiny{$\pm31$} & 91.8\tiny{$\pm8.5$}\\
        \\[-1em]TATS-hierarchical$_{128}$ & 455\tiny{$\pm12$} & 21\tiny{$\pm0.99$} & 803\tiny{$\pm40$} & 371\tiny{$\pm31$} & 1138\tiny{$\pm53$} & 83.4\tiny{$\pm$11.6}\\
        \bottomrule
        \end{tabular}
\fi
\begin{figure*}[!ht]
    \begin{minipage}{\textwidth}
    \captionsetup{type=table}
    \centering
    \caption{Quantitative results on 128-frame video generation. The subscripts on methods denote the length of training videos.}
    \vspace{-0.2cm}
    % \shcmt{edit caption. mark the best {\bf bold}}}
    \begin{adjustbox}{width=1.0\textwidth}
    \label{tab:longterm}
    \centering
        \begin{tabular}{lccccccccc}
        \toprule
        \\[-1em]& \multicolumn{3}{c}{SkyTimelapse} & \multicolumn{3}{c}{Taichi-HD} & \multicolumn{3}{c}{UCF-101} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Method 
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & Inf.Time & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & Inf.Time
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & Inf.Time\\
        \\[-1em]\Xhline{2\arrayrulewidth}
        \\[-1em]MoCoGAN-HD$_{128}$ & 498\tiny{$\pm 15$} & 23\tiny{$\pm1.39$} & 0.37s & 991\tiny{$\pm23$} & 290\tiny{$\pm12$} & 0.37s & 1622\tiny{$\pm53$} & 118\tiny{$\pm6.4$} & 0.37s\\
        \\[-1em]DIGAN$_{128}$ & 331\tiny{$\pm13$} & 9\tiny{$\pm0.38$} & {0.12s} & 764\tiny{$\pm19$} & 230\tiny{$\pm25$} & {0.12s} & 1627\tiny{$\pm51$} & 122\tiny{$\pm10$} & {0.12s}\\\hline
        \\[-1em]CCVS$_{16}$ & N/A & N/A & N/A & N/A & N/A & N/A & 1411\tiny{$\pm45$} & 121\tiny{$\pm11$} & 219s \\
        \\[-1em]TATS-base$_{16}$ & 435\tiny{$\pm12$} & 19\tiny{$\pm1.50$} & 110s & 458\tiny{$\pm21$} & 211\tiny{$\pm18$} & 110s & 1107\tiny{$\pm31$} & 91.8\tiny{$\pm8.5$} & 110s\\
        \\[-1em]TATS-hierarchical$_{128}$ & 455\tiny{$\pm12$} & 21\tiny{$\pm0.99$} & 127s & 803\tiny{$\pm40$} & 371\tiny{$\pm31$} & 127s & 1138\tiny{$\pm53$} & 83.4\tiny{$\pm$11.6} & 127s\\%\hline
        \\[-1em]MeBT$_{128}$ (Ours) & \bf{239\tiny{$\pm3.6$}} & \bf{5.1\tiny{$\pm0.16$}} & 6.53s & \bf{399\tiny{$\pm19$}} & \bf{139\tiny{$\pm13$}} & 6.62s & \bf{968\tiny{$\pm75$}} & \bf{75.5\tiny{$\pm12.1$}} & 7.53s\\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
    \end{minipage}
    \vspace{0.1cm}
    
    \begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/comparison.pdf} % change it to the figure
    \vspace{-0.5cm}
    % \vspace{2cm}
    % put figure here
    % \vspace{2cm}
    \captionsetup{type=figure}
    \vspace{-0.2cm}
    \caption{Qualitative results on 128-frame video generation with different models on the UCF-101 dataset: (a) MeBT (Ours), (b) TATS-base, (c) TATS-hierarchical, (d) DIGAN. 
    % Every 16$^{th}$ frame is extracted from the generated videos. 
    We present every 10th frames from the generated videos.
    % More results can be found in the supplementary file.
    More results are in Appendix (Fig.~\ref{fig:supp_stl}-\ref{fig:supp_ucf}).
    }
    \label{fig:longterm_qualitative}
    \end{minipage}
    \vspace{-0.6cm}
\end{figure*}
\vspace{-0.4cm}
\paragraph{Implementation}
For discrete tokenization, we use the official VQGAN checkpoints from \cite{TATS} that have a compression ratio of 4 and 8 in temporal and spatial dimensions, respectively.
For a fair comparison, we match the number of parameters and layers in our method to be roughly the same as other transformer-based baseline~\cite{TATS}.
% We configure the capacity of our transformer following TATS by matching the number of attention layers. 
% We set the size of the latent bottleneck to $N_L=256$ for every experiment, except for the 16-frame SkyTimelapse dataset where $N_L=128$.
We set the size of the latent bottleneck to $N_L=256$ for every experiment.
We train 16-frame MeBT without curriculum while 128-frame MeBT models are trained with the proposed learning schedule in Section~\ref{sec:train_inference}. 
For the interval scheduling function, we use $\beta = 2$ for all datasets while $\alpha = 30K$ for SkyTimelapse and $\alpha = 100K$ for Taichi-HD and UCF-101. 
% More details on network architecture and hyperparameters are described in the supplementary file.
More details on training are described in Appendix (Section \ref{appx:training_inference}).

\subsection{Results on Short-term Video Generation}
\paragraph{Baselines}
We compare our method with state-of-the-art video generative models whose pre-trained models are publicly available.
In all datasets, we employ MoCoGAN-HD~\cite{MoCoGAN-HD} and DIGAN~\cite{DIGAN} as methods based on Generative Adversarial Networks (GAN), and TATS~\cite{TATS} as the state-of-the-art autoregressive transformer that differs only in architecture while sharing the same quantization codebook with ours.
We also present comparisons to other unconditional models in UCF-101 using the reported numbers.
% We choose two baselines based on generative adversarial networks, such as MoCoGAN-HD~\cite{} and DIGAN~\cite{}.
% We also employ the state-of-the-art autoregressive Transformers

% We compare MeBT with the state-of-the-art video generative models on short video (16-frame) modeling. The results are shown in Table \ref{tab:short-quantitative}. 
\cutparagraphup
\paragraph{Results}
Table~\ref{tab:short-quantitative} summarizes the results on short-term video generation (16 frames).
Overall, we observe that our method exhibits competitive performance to the state-of-the-art approaches.
% Our method achieved the best performance in SkyTimelapse which contains nonlinear dynamics of clouds, and second-best in Taichi containing relatively simple foreground motion of a human. 
% Our method achieved the best and the second best performance in SkyTimelapse and Taichi datasets, respectively, and the best frame-level fidelity in UCF-101 dataset.
% Our method outperforms all non-Transformer baselines by a large margin in all datasets, and competitive performance to other Transformer-based autoregressive models (TATS and CCVS); it achieves the best and the second-best performance in SkyTimelapse and Taichi datasets, respectively, and the best frame-level fidelity in UCF-101 dataset.
Our method outperforms all non-transformer baselines by a large margin in all datasets, and competitive performance to other autoregressive transformers (TATS and CCVS); it achieves the best and the second-best performance in SkyTimelapse and Taichi-HD, respectively, and the best Inception score in UCF-101.
Compared to TATS, however, our method exhibits considerable improvement in efficiency in both training and inference; it reduces the peak training memory usage by $33\%$ while improving the inference speed by $10$ times.
Note that the gap increases rapidly in longer videos as the asymptotic complexity is quadratic in TATS but linear in MeBT.
% Note that the complexity of MeBT and TATS are linear and quadratic
Overall, our results show that MeBT provides compelling short-term generation performance while reducing computation cost significantly. 
It allows our model to be trained with longer videos to model interesting long-term dynamics, which is described in the next section.
% It opens up the opportunity to apply MeBT directly to training with longer videos, which is described in the next section.

\iffalse
Our model shows comparable performance with state-of-the-art models. Specifically, our model showed preferable performance on SkyTimelapse dataset which mainly contains global motions. We conjecture that our model has strength in modeling global actions as it can access long-range contexts bidirectionally.

In addition, compared to the autoregressive transformer, our model exhibits improved efficiency in both training and inference. Compared to TATS-base, our model could reduce the training peak memory into $35\%$. Since our model has the same number of attention layers as the TATS-base, we may conclude that MeBT's efficiency directly comes from the latent bottleneck. Moreover, our model showed about 8 times faster inference speed by reducing the number of decoding steps compared to the autoregressive transformer.
\fi
% Not only the iterative decoding of bidirectional modeling but also the latent bottleneck boosts the inference speed by reducing the number of tokens to process. 
% \shcmt{Complete the discussions ASAP}

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.45\textwidth]{figures/ours_2row_dotted.pdf}
    \includegraphics[width=0.47\textwidth]{figures/ours_2row.pdf}
    % \includegraphics[width=0.41\textwidth]{figures/ours_2row.pdf}
    \vspace{-0.17cm}
    \caption{
    % Generated 128-frame videos from MeBT. Every 10$^{th}$ frame is extracted from the generated videos.
    Qualitative results of MeBT on 128-frame videos. We present every 16th frames from the generated videos.
    }
    % Each video is displayed sequentially in 2 rows.}
    \label{fig:longterm_ours}
    \vspace{-0.7cm}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/sample.pdf}
    % \vspace{1cm}
    % {\color{blue}A figure showing delta-FVD over time.} 
    % \vspace{1cm}
    \vspace{-0.3cm}
    \caption{Evaluation of generation quality over time. FVD is measured in 16-frame interval relative to the initial prediction.}
    % \shcmt{axis title. poor visibility. make it fit to the page width}}
    \label{fig:error_propagation}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Results on Long-term Video Generation}
\label{sec:exp_long_video}
% Our method outperforms the other baselines on all three datasets. The results are shown in Table.\ref{table:longterm}.
\paragraph{Baselines}
We evaluate our method with five strong baselines: MoCoGAN-HD~\cite{MoCoGAN-HD}, CCVS~\cite{CCVS}\footnote{For CCVS, we report the results only in UCF-101 dataset where the official pre-trained model is available.}, and TATS~\cite{TATS} as strong autoregressive approaches that recursively generate long videos conditioned on the previous generation, and DIGAN~\cite{DIGAN} as an implicit network that directly generates each frame independently conditioned on latent variable. 
% We also compare with the hierarchical variants of TATS~\cite{TATS} that generates videos in coarse-to-fine manner using two separate transformers; one generates keyframes in coarse temporal scale, and the other interpolates missing frames between two consecutive keyframes.
We also compare with the hierarchical variants of TATS~\cite{TATS} that generates videos in coarse-to-fine manner using two separate transformers; one generates keyframes in coarse temporal scale, and the other interpolates missing frames between two consecutive keyframes.
We trained the baselines on 128-frame videos if their training peak memory stays under the VRAM capacity, and used 16-frames otherwise.
See the Appendix for more details on the baselines (Section \ref{appx:baselines}).

\cutparagraphup
\paragraph{Results}
Table~\ref{tab:longterm} and Figure~\ref{fig:longterm_qualitative} summarize the quantitative and qualitative comparisons on long-term video synthesis, respectively.
The quantitative results show that our method outperforms all baselines by a large margin over all datasets.
More specifically, the qualitative results display that the prediction error accumulates over time in na\"ive autoregressive transformers (Fig.~\ref{fig:longterm_qualitative}(b)), which is partly addressed in the hierarchical method yet suffering from the inconsistency at keyframe boundaries and inaccurate interpolation (Fig.~\ref{fig:longterm_qualitative}(c)). 
The DIGAN based on implicit representation exhibits more robustness to error propagation due to non-autoregressive generation process, yet fails to capture meaningful structures in complex videos (Fig.~\ref{fig:longterm_qualitative}(d)).
Compared to these methods, MeBT generates convincing videos by generating not only high-fidelity frames consistently over time but also coherent semantic structures and dynamics, such as a person leaving and re-entering the scene (Fig.~\ref{fig:longterm_qualitative}(a)), by exploiting long-term dependencies.
Figure~\ref{fig:longterm_ours} illustrates more qualitative results of our method.
It shows that MeBT can capture interesting long-term dynamics, such as global dynamics as the sun goes down over the water (Fig.~\ref{fig:longterm_ours}(a)), complex dynamics of a human (Fig.~\ref{fig:longterm_ours}(b)) and combination of human and camera motion (Fig.~\ref{fig:longterm_ours}(c)).

%##################################################################################################
% overall table of all ablations
\iffalse
\begin{table*}[htbp]
\vspace{-0.1cm}
\caption{Ablation experiments on SkyTimelapse. For 128-frame experiments, we reported the peak training memory with batch size 1.}
\label{tab:ablations} \vspace{-0.7em}
%#################################################
% Decoder design ablation
%#################################################
\subfloat[
Ablation study on decoder design and bottleneck sizes.
\label{tab:16f_ablation}
]{
\begin{minipage}{0.39\linewidth}{\begin{center}
\centering
\footnotesize
\begin{tabular}{cc|ccc}
    \toprule
    Decoder & $N_L$& Memory & FVD$_{16}$ & KVD$_{16}$ \\
    \midrule
    Full & 128 & 18.2 GB & { 92.2\tiny{$\pm2.2$}} & {\bf 2.6\tiny{$\pm.12$}} \\
    \hline
    Ours & 32 & {\bf 6.0} GB & 110.8\tiny{$\pm2.4$} & 4.0\tiny{$\pm.15$} \\
    Ours & 128 & {7.4} GB & 95.2\tiny{$\pm1.5$} & 2.8\tiny{$\pm.11$} \\
    Ours & 256 & {9.5} GB & {\bf 90.4}\tiny{$\pm1.8$} & 2.8\tiny{$\pm.17$} \\
    Ours & 512 & {14.7} GB & 90.8\tiny{$\pm2.1$} & \bf{2.6}\tiny{$\pm.14$} \\
    % MeBT & 3.90 & 95.2\tiny{$\pm1.5$} & 2.8\tiny{$\pm.11$} \\
    % Full & 6.64 & 92.2\tiny{$\pm2.2$} & 2.6\tiny{$\pm.12$} \\
    \bottomrule
    \end{tabular}
\end{center}}\end{minipage}
}
%#################################################
% Sparse attention ablation
%#################################################
\subfloat[
Ablation study on attention structure and interval scheduling strategies.
\label{tab:128f_ablation}
]{
\begin{minipage}{0.6\linewidth}{\begin{center}
    % \vspace{-0.2cm}
    \centering
    \footnotesize
    \begin{tabular}{cc|cccc}
    \toprule
        Model & $p_n(v)$ & FVD$_{128}$ & KVD$_{128}$ & Memory & Time\\
    \midrule
        Window & Gauss. & 458\tiny$\pm21.2$ & 33.2\tiny$\pm3.31$ & 34.9 GB & 13.7s\\
        Axial & Gauss. & 377\tiny$\pm12.2$& 13.6\tiny$\pm0.81$ & 21.2 GB & 12.0s \\
        % Local& & & N/A & $O(T)$\\
        \hline
        Ours & None & 276\tiny{$\pm10.9$} & 7.5\tiny{$\pm.99$} & \textbf{13.3} GB & \textbf{6.53}s\\
        Ours & Unif. & 253\tiny{$\pm4.6$} & 6.7\tiny{$\pm.49$} & \textbf{13.3} GB & \textbf{6.53}s\\
        \hline
        Ours & Gauss. & \textbf{239\tiny${\pm3.6}$} & \textbf{5.1}\tiny$\pm0.16$ & \textbf{13.3} GB & \textbf{6.53}s\\
    \bottomrule
    \end{tabular}
    \label{tab:curriculum_ablation}
\end{center}}\end{minipage}
}
% {
% \begin{minipage}{0.6\linewidth}{\begin{center}
%     % \vspace{-0.2cm}
%     \centering
%     \footnotesize
%     \begin{tabular}{cc|cccc}
%     \toprule
%         Model & Schedule & FVD$_{128}$ & KVD$_{128}$ & Memory & Time\\
%     \midrule
%         Window & Gauss. & 458\tiny$\pm21.2$ & 33.2\tiny$\pm3.31$ & 34.9 GB & 13.70s\\
%         Axial & Gauss. & 377\tiny$\pm12.2$& 13.6\tiny$\pm0.81$ & 21.2 GB & 11.96s \\
%         % Local& & & N/A & $O(T)$\\
%         \hline
%         Ours & None & 276\tiny{$\pm10.9$} & 7.5\tiny{$\pm.99$} & \textbf{13.3} GB & \textbf{6.53}s\\
%         Ours & Unif. & 253\tiny{$\pm4.6$} & 6.7\tiny{$\pm.49$} & \textbf{13.3} GB & \textbf{6.53}s\\
%         \hline
%         Ours & Gauss. & \textbf{239\tiny${\pm3.6}$} & \textbf{5.1}\tiny$\pm0.16$ & \textbf{13.3} GB & \textbf{6.53}s\\
%     \bottomrule
%     \end{tabular}
%     \label{tab:curriculum_ablation}
% \end{center}}\end{minipage}
% }
\vspace{-0.3cm}
\end{table*}
\fi
%####################################################################################

% To better understand the generation quality, we measure the FVD scores over time relative to the initial predictions and summarize the results in Figure~\ref{fig:error_propagation}.
% It shows the similar trend that autoregressive models (TATS, MoCoGAN-HD, and CCVS) suffer from dramatic error propagation especially in complex videos, while the hierarchical model (TATS-hierarchical) suffers from the inconsistency around the keyframe boundaries displayed as ziggy patterns.
% On the other hand, MeBT enjoys the consistent generation quality over time thanks to bidirectional architecture and end-to-end training with long sequences, achieving near-zero quality degradation in all datasets ($\vartriangle \text{FVD}\approx 0$).
% \jh{On the other hand, DIGAN and MeBT enjoys the consistent generation quality over time thanks to non-autoregressive generation and end-to-end training with long sequences achieving near-zero quality degradation in all datasets ($\vartriangle \text{FVD}\approx 0$). However, as discussed before, MeBT is the only model that generates meaningful structures over long sequences.}

% in complex videos -> (taichi-HD, UCF-101 which contains complex dynamics)
To assess the consistency of generation quality, we measure FVD over time relative to the initial predictions and summarize the results in Figure~\ref{fig:error_propagation}.
The autoregressive models (MoCoGAN-HD, CCVS, and TATS-base) suffer from dramatic error propagation. Conversely, the hierarchical model (TATS-hierarchical) is plagued by inconsistencies around keyframe boundaries that manifest as zigzag patterns. In contrast, DIGAN and MeBT achieve near-zero quality degradation in all datasets ($\vartriangle \text{FVD}\approx 0$) due to their non-autoregressive design and end-to-end training with long sequences. Nevertheless, unlike MeBT, DIGAN fails to generate meaningful structures as discussed in previous paragraphs (Table~\ref{tab:longterm}).

% On sky time-lapse, TATS-B shows inconsistent ground modeling, TATS-H shows discontinuous motions.
% On Taichi, TATS-B --> Collapsing, changing color of cloths
% On UCF, 
\subsection{Ablation Studies}
\label{sec:ablation_study}
% In this section, we ablate the components of MeBT. We evaluate our methods on the simple Sky Time-lapse dataset. Except for the ablation study on curriculum learning, we used 16-frame model for simplicity.
This section presents component-wise analysis of MeBT. 
All experiments are conducted with SkyTimelapse videos. Except for the ablation studies about long-range dependency modeling, we used 16-frame model for simplicity.

\cutparagraphup
\paragraph{Impact of latent bottleneck size}
To study the efficiency and performance trade-off introduced by the latent bottleneck, we train MeBT with varying number of latent codes and report the results in Table~\ref{tab:16f_ablation}.
Since the entire context tokens are compressed into much smaller latents by the encoder (Section~\ref{sec:mebt_encoder}), employing small latents generally sacrifices the performance. 
Yet, the performance quickly saturates at a reasonable scale $N_L=256$, which is still considerably smaller than the context size $N_C=1024$.
It shows that MeBT can achieve linear complexity without compromising much performance.

% It shows that MeBT can achieve linear-time complexity without compromising much performance.
%the performance generally increases as we employ more latent codes, yet it saturates in 

\iffalse
Fig.\ref{fig:latent_bottleneck} shows the change of FVD over the training memory cost by controlling the size of the latent bottleneck. 
By increasing the size of the latent bottleneck, the model's performance increases at the cost of efficiency. 
Interestingly, the performance of the model seems to be saturated when $N_L \geq 256$. 
This indicates that the context can be expressed with fewer tokens. 
When modeling a 16-frame video expressed with 1024 tokens, only 256 tokens corresponding to a 4-frame clip was enough to express the context.
\fi

\begin{table}[t]
    \caption{Ablation study on decoder design and bottleneck sizes.}
    \vspace{-0.3cm}
    \label{tab:16f_ablation}
    \centering
    \footnotesize
    \begin{tabular}{cc|ccc}
    \toprule
    Decoder & $N_L$& Memory & FVD$_{16}$ & KVD$_{16}$ \\
    \midrule
    Full & 128 & 18.2 GB & { 92.2\tiny{$\pm2.2$}} & {\bf 2.6\tiny{$\pm.12$}} \\
    \hline
    Ours & 32 & {\bf 6.0} GB & 110.8\tiny{$\pm2.4$} & 4.0\tiny{$\pm.15$} \\
    Ours & 128 & {7.4} GB & 95.2\tiny{$\pm1.5$} & 2.8\tiny{$\pm.11$} \\
    Ours & 256 & {9.5} GB & {\bf 90.4}\tiny{$\pm1.8$} & 2.8\tiny{$\pm.17$} \\
    Ours & 512 & {14.7} GB & 90.8\tiny{$\pm2.1$} & \bf{2.6}\tiny{$\pm.14$} \\
    \bottomrule
    \end{tabular}
    \vspace{-0.3cm}
\end{table}

\cutparagraphup
\paragraph{Impact of the decoder architecture}
\label{sec:dec_ablation}
To evaluate the impact of our decoder (Section~\ref{sec:mebt_decoder}), we compare our method with the bidirectional decoder that leverages the full self-attention among the latent and masked tokens.
% Table~\ref{tab:16f_ablation} summarizes the results.
% It shows that approximating all pair-wise interaction of context and mask tokens through linear-time MeBT decoder merely degrades the performance while considerably reducing the memory footprint.
Comparing the first and third rows in Table~\ref{tab:16f_ablation} shows that approximating all pair-wise interaction of context and mask tokens through linear-time MeBT decoder merely degrades the performance while considerably reducing the peak memory.
% \vspace{-0.5cm}

\begin{table}[t]
\caption{Ablation study on attention and scheduling functions. The training peak memory is measured with batch size 1.}
\label{tab:128f_ablation}
\vspace{-0.3cm}
    \centering
    \footnotesize
    \begin{tabular}{cc|cccc}
    \toprule
        Model & Schedule & FVD$_{128}$ & KVD$_{128}$ & Memory & Time\\
    \midrule
        Window & Gaussian & 458\tiny$\pm21.2$ & 33.2\tiny$\pm3.31$ & 34.9 GB & 13.7s\\
        Axial & Gaussian & 377\tiny$\pm12.2$& 13.6\tiny$\pm0.81$ & 21.2 GB & 12.0s \\
        % Local& & & N/A & $O(T)$\\
        \hline
        Ours & None & 276\tiny{$\pm10.9$} & 7.5\tiny{$\pm.99$} & \textbf{13.3} GB & \textbf{6.53}s\\
        Ours & Uniform & 253\tiny{$\pm4.6$} & 6.7\tiny{$\pm.49$} & \textbf{13.3} GB & \textbf{6.53}s\\
        \hline
        Ours & Gaussian & \textbf{239\tiny${\pm3.6}$} & \textbf{5.1}\tiny$\pm0.16$ & \textbf{13.3} GB & \textbf{6.53}s\\
    \bottomrule
    \end{tabular}
\vspace{-0.5cm}
\end{table}

\cutparagraphup
\paragraph{Impact of attention structure}
We evaluate the impact of attention structure on 128-frame videos, by replacing the latent bottleneck attention in MeBT with sparse attention methods used in video modeling: axial attention~\cite{Ho2019} alternates attention over horizontal, vertical, and temporal axes, and window attention~\cite{MaskViT} alternates attention over spatial and temporal axes.
% \footnote{\color{red}We exclude the local attention~\cite{NUWA} from the comparison as the implementation with sparse operation is unavailable.}.
See Section~\ref{appx:asymptotic_mem} for more details on the baselines.
As shown in Table \ref{tab:128f_ablation}, ours shows clear improvements over the baselines in both efficiency and performance.
% In terms of efficiency, MeBT shows clear improvement over the baselines in both memory and inference time thanks to its linear complexity while the baselines are quadratic to the sequence length.
In terms of efficiency, MeBT outperforms baselines in both memory and inference time thanks to its linear complexity, while baselines have quadratic complexity to the temporal length from self-attention on the temporal axis.
% \jh{MeBT surpasses baselines in memory and inference time due to its linear complexity, while the baselines have quadratic complexity to the temporal length resulting from self-attention over the temporal axis.}
Interestingly, MeBT also exhibits improvement in sample quality since the heuristic sparsity patterns in the baselines are insufficient to capture complex long-range dependency while ours can \emph{learn} them adaptively through the latents.
% In terms of efficiency, our model is more efficient than the baselines because it still has linear complexity with respect to the temporal dimension, whereas the baselines have quadratic complexity. 
% The detailed analysis of efficiency is in the supplementary file. 
% As for performance, the result shows that heuristic sparse patterns in the prior works are insufficient to capture complex long-range dependency, while ours can \emph{learn} them holistically and adaptively through the latents.

% We evaluated the impact of attention structure on MeBT by replacing the latent bottleneck attention with sparse attention methods used in video modeling: axial attention~\cite{Ho2019} alternates attention over horizontal, vertical, and temporal axes, and window attention~\cite{MaskViT} alternates attention over spatial and temporal axes. Our model shows clear improvements over the baselines in both efficiency and performance, as shown in Table \ref{tab:128f_ablation}. In terms of efficiency, our model is more efficient than the baselines because it still has linear complexity with respect to the temporal dimension, whereas the baselines have quadratic complexity. A detailed analysis of efficiency is included in the supplementary file. As for performance, the result shows that heuristic sparse patterns in prior works are insufficient to capture complex long-range dependencies, while ours can holistically and adaptively \emph{learn} them through the latent bottleneck.
% It is because heuristic sparse patterns in the prior works are insufficient to capture complex long-range dependency, while ours can \emph{learn} them holistically and adaptively through the latent bottleneck. Asymptotic comparison to the baselines is in the supplementary file.

\cutparagraphup
\paragraph{Impact of interval scheduling function}
To evaluate the effectiveness of the interval scheduling function in Section~\ref{sec:train_inference}, we train MeBT on 128-frame videos with three different stratagies: \emph{None} refers random sampling over entire video volume ($p_n(v)=\delta(n=t)$), \emph{Uniform} refers sampling the video interval uniformly random ($p_n(v)=\frac{1}{t}$), and the \emph{Gaussian} refers the proposed curriculum sampling in Eq.~\eqref{eq:masking_schedule}.
As summarized in Table~\ref{tab:128f_ablation}, sampling the context and mask tokens randomly from long videos often leads to suboptimal results (\emph{None}), while limiting the interval of the samples makes the training easier (\emph{Uniform}). 
By learning from short to long videos (\emph{Gaussian}), the proposed strategy stabilizes the training and improves the performance.
% Table~\ref{tab:curriculum_ablation} summarizes the results.
\iffalse
\begin{table}[!t]
    \centering
    \small
    \caption{
    Ablation study on different training schedules. 
    %The proposed curriculum (Gaussian) improves the generation performance.
    }
    \vspace{-0.2cm}
    \begin{tabular}{lcc}
    \toprule
    Curriculum & FVD$_{128}$ & KVD$_{128}$ \\
    \midrule
    None & 276\tiny{$\pm10.9$} & 7.5\tiny{$\pm.99$} \\
    Uniform & 253\tiny{$\pm4.6$} & 6.7\tiny{$\pm.49$} \\
    Gaussian & {\bf 239\tiny{$\pm3.6$}} & {\bf 5.1\tiny{$\pm.16$}} \\
    \bottomrule
    \end{tabular}
    \label{tab:curriculum_ablation}
\end{table}
\fi


\iffalse
\begin{table}[h]
    \centering
    \label{tab:dnr}
    \begin{tabular}{lcc}
    \toprule
    Phase & FVD$_{16}$ & KVD$_{16}$ \\
    \midrule
    Draft & 101.1\tiny{$\pm2.98$} & 3.6\tiny{$\pm.26$} \\
    Revise & 95.2\tiny{$\pm1.5$} & 2.8\tiny{$\pm.11$} \\
    \bottomrule
    \end{tabular}
    \caption{Ablation study: gain from revision}
\end{table}

% {\color{red}
% \paragraph{Ablation on the revision phase}
% \shcmt{this may not be necessary. revision is a part of decoding strategy.}
% We ablated the gain from the revision phase. The revisions are obtained from the same drafts that are used to measure the quality of drafts.
% As shown in Table\ref{tab:dnr}, The revision phase could improve the quality of generated videos. 
% }
\fi