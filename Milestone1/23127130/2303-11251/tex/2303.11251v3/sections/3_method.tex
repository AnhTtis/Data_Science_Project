\iffalse
\ifdefined\paratitle {\color{blue}
[Discrete representation with transformers can synthesize high-fidelity videos.] \\
} \fi
\warn{Video generative models synthesize videos $\mathbf{x} \in \mathbb{R}^{T \times H \times W \times 3}$ by sampling videos $\mathbf{x}$ from the generative distribution $p_\theta(\mathbf{x})$.} Various kinds of generative models are used to model the generative distribution $p_\theta(\mathbf{x})$.
Recently, the combination of transformers and discrete representations are often used to generate high-fidelity videos \cite{TATS, videoGPT}.

\ifdefined\paratitle {\color{blue}
[We adopt transformers with discrete representations to generate high-fidelity videos. (we adopt bidirectional modeling.)] \\
} \fi
We adopt the combination of discrete representations and transformers to generate videos as they can generate high-fidelity videos \cite{VideoGPT, TATS}. To be specific, we extended the bidirectional generative model \cite{MaskGIT} introduced for images to videos. We adopt the bidirectional generative model as it supports faster decoding than autoregressive models.
\fi

\section{Background}
% Preliminary에서 long dependency 모델링이 왜 중요한지 잘 드러나 있지 못함. (related work + intro에서 어필해도 충분할 지?)
% formulation of video generative model.
% Recently, VQGAN + Transformers showed good performance on visual synthesis.
% 2 kinds of Transformers used with discrete tokens.
%
% Maybe it has to be placed in the introduction.
% Recently, the combination of transformers and discrete representations are often used to generate high-fidelity videos \cite{TATS, VideoGPT}.
%
% We adopt the combination of discrete representations and transformers to generate videos as they can generate high-fidelity videos \cite{VideoGPT, TATS}. To be specific, we adopt a bidirectional transformer by extending the bidirectional generative model \cite{MaskGIT} introduced for images to videos. The bidirectional generative model can generate videos faster than autoregressive models through iterative decoding and shows less error propagation due to its bidirectional dependency modeling.
%
% transformers are used, bidirectional models are used. To use them, VQGAN is needed.
% transformers: high fidelity
% Extending MaskGIT to videos: fast gen.
% In this section, we introduce autoregressive transformers that utilize discrete representations of videos and  bidirectional transformers that enhance the inference speed of autoregressive transformers.
This section introduces generative transformers for videos that utilize discrete token representation, which can be categorized into autoregressive and bidirectional models.

% \paragraph{\jw{Generative Transformers for Videos}}
Let $\mathbf{x} \in \mathbb{R}^{T \times H \times W \times 3}$ be a video.
To model its generative distribution $p(\mathbf{x})$, prior works on transformers employ discrete latent representation of frames $\mathbf{y} \in \mathbb{R}^{t \times h \times w \times d}$ and model the prior distribution on the latent space $p(\mathbf{y})$.
\cutparagraphup
\paragraph{Vector Quantization}
To map a video $\mathbf{x}$ into discrete tokens $\mathbf{y}$, previous works~\cite{TATS, NUWA, LVT, MMVID, VPVQVAE} utilize vector quantization with an encoder $E$ that maps $\mathbf{x}$ onto a learnable codebook $F=\{e_i\}_{i=1}^U$~\cite{VQVAE}.
Specifically, given a video~$\mathbf{x}$, the encoder produces continuous embeddings $\mathbf{h} = E(\mathbf{x}) \in \mathbb{R}^{t \times h \times w \times d}$ and searches for the nearest code $e_u\in F$.

The encoder $E$ is trained through an autoencoding framework by introducing a decoder $D$ that takes discrete tokens $\mathbf{y}$ and produces reconstruction $\hat{\mathbf{x}}=D(\mathbf{y})$.
The encoder $E$, codebook $F$, and the decoder $D$ are optimized with the following training objective:
\begin{gather}
    \mathcal{L}_q = ||\mathbf{x} - \hat{\mathbf{x}}||_2^2 + ||\texttt{sg}(\mathbf{h}) - \mathbf{y}||_2^2 + \beta ||\texttt{sg}(\mathbf{y}) - \mathbf{h}||^2_2,
\end{gather}
where \texttt{sg} denotes stop-gradient operator.
In practice, to improve the quality of discrete representations, additional perceptual loss and adversarial loss are often introduced~\cite{VQGAN}.

For the choice of the encoder $E$, we utilize 3D convolutional networks that compress a video in both spatial and temporal dimensions following prior works~\cite{videoGPT, TATS}.
%To improve the reconstruction quality, additional perceptual loss and adversarial loss are introduced in VQGAN \cite{VQGAN}.
% In practice, frame-wise 2d VQGANs that compress each frame spatially \cite{NUWA, MaskViT} or 3d VQGANs that compress a video in both spatial and temporal \cite{videoGPT, TATS} are used.

% VQ-VAE~\cite{VQVAE} consists of an encoder $E$, a learnable codebook $F=\{e_i\}_{i=1}^U$, and a decoder $D$.
% The encoder $E$ first projects a video $\mathbf{x}$ into its latent representation $\mathbf{h} = E(\mathbf{x}) \in \mathbb{R}^{t \times h \times w \times d}$. Then,
% the discrete representation $\mathbf{y} \in \mathbb{R}^{t \times h \times w \times d}$ is obtained by replacing $\mathbf{h}$ with the nearest code $e_u \in F$. The discrete representation $\mathbf{y}$ can be reconstructed to the video $\hat{\mathbf{x}}=D(\mathbf{y})$. The model is optimized through the following loss:
% \begin{gather}
%     \mathcal{L} = ||\mathbf{x} - \hat{\mathbf{x}}||_2^2 + ||sg(\mathbf{h}) - \mathbf{y}||_2^2 + \beta ||sg(\mathbf{y}) - \mathbf{h}||^2_2,
% \end{gather}
% where $sg$ denotes the stopping gradient. 
% To improve the reconstruction quality of VQ-VAE, additional perceptual loss and adversarial loss are introduced in VQGAN \cite{VQGAN}.
% In practice, frame-wise 2d VQGANs that compress each frame spatially \cite{NUWA, MaskViT} or 3d VQGANs that compress a video in both spatial and temporal \cite{videoGPT, TATS} are used.
% As introduced in VQGAN \cite{VQGAN}, additional perceptual loss and adversarial loss can be added to improve the reconstruction quality of VQ-VAE.

% \subsection{Autoregressive Generative Transformers}
% we introduce video generative models that utilize discrete representations
% \ifdefined\paratitle {\color{blue}
% [Video transformers model the generative distribution of videos on the discrete representation space.] \\
% } \fi

% Let a video $\mathbf{x} \in \mathbb{R}^{T \times H \times W \times 3}$ be given. The goal of video generative transformers is to model the generative distribution of video $p(\mathbf{x})$. However, due to the quadratic complexity of transformers, it is challenging to model videos on high-dimensional pixel space. Therefore, the transformers models $p(\mathbf{y})$ instead of $p(\mathbf{x})$ where discrete representations $\mathbf{y} \in \mathbb{R}^{t \times h \times w \times d}$ represent videos $\mathbf{x}$ on a discrete and low-dimensional space. To compress videos into discrete representations, previous works \cite{TATS, NUWA, LVT, MMVID, VPVQVAE} utilized VQ-VAE \cite{VQVAE} to map high-dimensional videos into low-dimensional discrete representations.
% \jhcmt{The sentences are not connected well. It would be better if we introduce why latent representations are better at first.}

% \ifdefined\paratitle {\color{blue}
% [On the discrete latent space, AR transformers model videos as a sequence of tokens.] \\
% } \fi
\paragraph{Autoregressive Transformers}
Given the discrete latent representations $\mathbf{y} \in \mathbb{R}^{t \times h \times w \times d}$, generative modeling of videos boils down to modeling the prior $p(\mathbf{y})$.
Prior work based on autoregressive transformers employ a sequential factorization $p(\mathbf{y})=\Pi_{i\leq N}p(y_i|y_{<i})$ where $N = thw$, and use a transformer to model the conditional distribution of each token $p(y_i|y_{<i})$.
The transformer is trained to minimize the following negative log-likelihood of training data:
\begin{equation}\label{eq:autoregressive}
    \mathcal{L}_a = \sum_{i\leq N} -\log{p(y_i|y_{<i})}.
\end{equation}

During inference, the transformer generates a video by sequentially sampling each token $y_i$ from the conditional $p(y_i|y_{<i})$ based on context $y_{<i}$.
The sampled tokens $\mathbf{y}$ are then mapped back to a video using the decoder $D$.

While simple and powerful, autoregressive transformers for videos suffer from critical scaling issues.
First, each conditional $p(y_n|y_{<n})$ involves $\mathcal{O}(n^2)$ computational cost due to the quadratic complexity of self-attention.
This forces the model to only utilize short-term context in both training and inference, making it inappropriate in modeling spatio-temporal long-term coherence.
Furthermore, during inference, the sequential decoding requires $N$ model predictions that recursively depend on the previous one.
% This leads to a slow inference, and more notably, a potential error propagation over space and time since the prediction error can accumulate over $N$ decoding steps.
This leads to a slow inference, and more notably, a potential error propagation over space and time since the prediction error at a certain token accumulates over the remaining decoding steps.
This is particularly problematic for videos since $N$ is often very large as tokenization spans both spatial and temporal dimensions.

%Autoregressive transformers are implemented with self-attention layers which have quadratic complexity with respect to the size of the input.
%Moreover, due to the sequential decoding of autoregressive transformers, the inevitable sampling error propagates through time, and the model should be feed-forwarded N times to generate a video.

%The obtained discrete representations $\mathbf{y} \in \mathbb{R}^{t \times h \times w \times d}$ are used to train autoregressive transformers.
%The transformers consider the discrete representation $\mathbf{y}$ as a sequence of tokens $(y_1, y_2, \cdots, y_{thw})$ and are trained to maximize the likelihood of $\mathbf{y}$ with the following training objective: 
% \begin{equation}
%     \mathcal{L} = \sum_{i=1}^N -\log{p(y_i|y_{<i})},
% \end{equation}
% where $N = thw$ is a size of the discrete representation $\mathbf{y}$.
% During inference, the transformer generates a video by sequentially sampling each token $y_i$. The sampled tokens are then decoded into a video.

\ifdefined\paratitle {\color{blue}
[Limitation1: Quadratic complexity of autoregressive transformers prohibits e2e learning: prior works (sparse attentions) and limitation] \\
} \fi
\iffalse
The autoregressive transformers are not suitable for modeling longer videos due to the quadratic complexity. Autoregressive transformers are implemented with self-attention layers which have quadratic complexity with respect to the size of the input.
To address the quadratic complexity of self-attention, sparse attention methods such as local attention \cite{NUWA, BigBird, LongFormer} or axial attention \cite{Ho2019, GODIVA} are proposed. The local attention reduces the complexity by restricting the query not to attend far-away tokens, and the axial attention reduces the complexity by factorizing the dependency of self-attention over axes. Although the proposed method reduces the complexity, the local attention limits the model to learn the dependency over the receptive field, and the axial attention's complexity is still quadratic to the temporal length of the video.

\ifdefined\paratitle {\color{blue}
[Limitation2: Error Propagation,  Slow decoding. Hierarchical model and its limitation.] \\
} \fi
% \jhcmt{Not connected to the previous paragraph.}
Moreover, due to the sequential decoding of autoregressive transformers, the inevitable sampling error propagates through time, and the model should be feed-forwarded N times to generate a video.
To alleviate the problems, hierarchical modeling is proposed  \cite{CogVid, TATS}. Hierarchical transformers consist of an autoregressive transformer that models sparse videos with a low frame rate, and an interpolation transformer that generates a short clip by interpolating nearby frames in the sparse video. The hierarchical transformer can generate longer videos faster and more consistently by shortening the length of the token sequence that a single transformer should model, and applying the interpolation transformer in parallel. Although hierarchical transformers can learn a long-term dependency, they cannot model the dependency across the clips generated by interpolation directly. This lack of dependency results in inconsistent motions across clips. 
% Moreover, sparse attentions are not easy-to-use.
\fi

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/overview.pdf}
    \vspace{-0.35cm}
    \caption{Overview of our method. Our model learns to predict masked tokens from the context tokens with linear complexity encoder and decoder. The encoder and decoder utilize latent bottlenecks to achieve linear complexity while performing full dependency modeling. 
    %During inference, our model iteratively decodes multiple tokens in parallel.
    }
    \label{fig:overview}
    \vspace{-0.4cm}
\end{figure*}
\cutparagraphup
\paragraph{Bidirectional Transformers}
\label{sec:maskgit}
\ifdefined\paratitle {\color{blue}
[BT address slow sequential decoding of autoregressive transformers.] \\
} \fi
To improve the decoding efficiency of autoregressive transformers,
bidirectional generative transformers have been proposed~\cite{MaskGIT, M6-UFC, MMVID}.
% Contrary to autoregressive transformers that factorize the prior sequentially over time, the bidirectional transformers model the conditional over multiple tokens at once.
% Formally, 
% Contrary to autoregressive models that predict a single consecutive token at a time, the bidirectional transformers formulates the generative task as an iterative masked modeling, and predicts 
%\jw{At each decoding step, a bidirectional transformer models joint distribution $p(\mathbf{y}_M|\mathbf{y}_C, \mathbf{z}_M)$ of \emph{multiple} masked tokens $\mathbf{y}_M= \{y_i | i \in \mathbf{m}\}$ given available context $\mathbf{y}_C = \{y_i | i \notin \mathbf{m}\}$ and masking indices $\mathbf{m}$.}
Contrary to autoregressive models that predict a single consecutive token at each step, a bidirectional transformer learns to predict \emph{multiple} masked tokens at once based on the previously generated context.
Specifically, given the random masking indices $\mathbf{m}\subseteq\{1,...,N\}$, it models the joint distribution over masked tokens $\mathbf{y}_M= \{y_i | i \in \mathbf{m}\}$ conditioned on the visible context $\mathbf{y}_C = \{y_i | i \notin \mathbf{m}\}$, and is trained with the below objective:
%The randomly sampled masking indices $\mathbf{m}$ defines the masked tokens $\mathbf{y}_M= \{y_i | i \in \mathbf{m}\}$ and the context tokens $\mathbf{y}_C = \{y_i | i \notin \mathbf{m}\}$. The model is trained with the below objective:
\begin{equation}
\label{eq:maskgit}
    \mathcal{L}_b=-\log{p(\mathbf{y}_M|\mathbf{y}_C, \mathbf{z}_M)}
\approx -\sum_{i \in \mathbf{m}} \log{p(y_i|\mathbf{y}_C, \mathbf{z}_M)},
\end{equation}
% where mask embeddings $\mathbf{z}_M$ represent masking indices $\mathbf{m}$ on the latent space $\mathbb{R}^d$.
% where mask embeddings $\mathbf{z}_M$ encodes learnable vectors for the mask tokens. 
where mask embeddings $\mathbf{z}_M$ encode positions of the mask tokens with learnable vectors. 
% {\color{red}Computing each conditional in Eq.~\eqref{eq:maskgit} involves self-attention across  context and masked tokens $\mathbf{y}=\mathbf{y}_C\cup \mathbf{y}_M$}.
Each conditional in Eq.~\eqref{eq:maskgit} is modeled by a transformer, but contrary to autoregressive models that apply causal masking, bidirectional transformers operate on entire tokens $\mathbf{y}=\mathbf{y}_C\cup \mathbf{y}_M$.
% During inference, the model is initialized with empty context tokens (\emph{i.e.}, $\mathbf{y}_C=\emptyset$), and performs iterative decoding by predicting the probability over all masked tokens $\mathbf{y}_M$ by Eq.~\eqref{eq:maskgit} and sampling their subset as the context for the next iteration.
During inference, the model is initialized with empty context (\emph{i.e.}, $\mathbf{y}_C=\emptyset$), and performs iterative decoding by predicting the probability over all masked tokens $\mathbf{y}_M$ by Eq.~\eqref{eq:maskgit} and sampling their subset as the context for the next iteration.
% Usually, the number of decoding steps $S$ is substantially smaller than the size of tokens $N$, resulting in a faster sampling speed of bidirectional transformers. 

% {\color{red}As bidirectional transformers decode multiple tokens in parallel, they are faster than autoregressive transformers when generating data.}
% \shcmt{you did not introduce how the bidrectional transformer works yet. you cannot say it here}
% The parallel decoding is learned by predicting masked tokens $\mathbf{y}_M$ based on the unmasked context tokens $\mathbf{y}_C$ and the randomly sampled masking indices $\mathbf{m}$. The model is optimized to minimize the below objective:
% \begin{equation}
% \label{eq:maskgit}
%     \mathcal{L}=-\log{p(\mathbf{y}_M|\mathbf{y}_C, \mathbf{m})}
% \approx \sum_{i \in \mathbf{m}} -\log{p(y_i|\mathbf{y}_C, \mathbf{m})},
% \end{equation}
% where $\mathbf{y}_C = \{y_i | i \notin \mathbf{m}\}$ and $\mathbf{y}_M = \{y_i | i \in \mathbf{m}\}$ are the context tokens and the masked tokens, respectively. During inference, the model starts to sample tokens from the empty context, i.e., $\mathbf{y}_C = \emptyset$. The sampled tokens are iteratively added to context tokens, and the remaining tokens are masked again. The decoding process ends if all tokens are added to the context tokens.

Adopting bidirectional transformers to model videos has two advantages compared to autoregressive transformers.
First, by decoding multiple tokens at once, bidirectional transformers enjoy a better parallelization and smaller decoding steps than autoregressive transformers which results in a faster sampling.
Second, bidirectional transformers are more robust to error propagation than autoregressive counterparts since the decoding of the masked token is independent of the temporal order, allowing consistent prediction quality over time.
However, complexity of the bidirectional transformers is still quadratic to the number of tokens, limiting the length of training videos to only short sequences thus hindering learning long-term dependencies.
% \jh{As bidirectional transformers can attend to the tokens at arbitrary positions, the fidelity of predicted tokens is consistent through time.} However, as the bidirectional transformers' complexity is still quadratic to the input size, the longer dependency cannot be directly modeled with bidirectional transformers by training with longer videos in an end-to-end manner.



\section{Memory-efficient Bidirectional Transformer}
% Goal of MeBT & High level intuition
    % Previous Works' limitation (training memory complexity, testing time complexity)
    % Due to this, cannot training the models in end-to-end manner to learn long-term dependency.
    % Memory efficient architecture. (linear complexity, reducing N.)
    % By adopting MaskGIT, faster inference. lower error prop. over time.
% Technical Overview of the framework

\ifdefined\paratitle {\color{blue}
[To alleviate the problems of prior works, we propose MeBT, a bidirectional transformer with linear complexity that learns to model long-term dependency directly.: slow decoding, error propagation, complexity] \\
} \fi
% To alleviate the aforementioned problems, we propose MeBT, a bidirectional transformer with linear complexity that models long-term dependency directly.
% \jhcmt{How about: Overview - Purpose?}

Our goal is to design a generative transformer for videos that can perform fast inference with a robustness to error propagation, while being able to fully leverage the long-range statistical dependency of video frames in training.

For inference speed and robustness, we adopt the bidirectional approach and parameterize the joint distribution of masked tokens $p(\mathbf{y}_M|\mathbf{y}_C,\mathbf{z}_M)\approx\Pi_{i\in\mathbf{m}}p(y_i|\mathbf{y}_C,\mathbf{z}_M)$ (Eq.~\eqref{eq:maskgit}) with a transformer.
For learning long-range dependency, we take a simple approach of employing an efficient transformer architecture~\cite{EfficientTransformers} of sub-quadratic complexity and directly training it with longer videos.
While sparse attention (\emph{e.g.}, local, axial, strided) is dominant in autoregressive transformers to reduce complexity~\cite{NUWA, MaskViT, CogVideo, TATS}, we find it potentially problematic for bidirectional transformers since the context $\mathbf{y}_C$ can be provided for an arbitrary subset of token positions that often cannot be covered by the fixed sparse attention patterns\footnote{Note that, in autoregressive transformers, the context $\mathbf{y}_{<i}$ (Eq.~\eqref{eq:autoregressive}) is always the entire past, allowing sparse attention to robustly see the context.}.
Thus, unlike prior work, we design an efficient bidirectional transformer based on low-rank latent bottleneck~\cite{SetTransformer, Linformer, SetVAE, Perceiver, PerceiverIO, LUNA} that always enables a \emph{dense} attention dependency over all token positions while guaranteeing \emph{linear} complexity.

%Our goal is to model the long-term dependency in long videos directly. To achieve this goal, we propose a memory-efficient bidirectional transformer (MeBT).
% Goal 1
%MeBT models videos $\mathbf{x}$ using bidirectional modeling that is robust to error propagation over time and can generate videos in a reasonable time. 
% Goal 2
%MeBT reduces the quadratic complexity of bidirectional transformers into linear complexity by adopting latent bottlenecks \cite{Perceiver, Perceiver-IO}.

To this end, we propose Memory-efficient Bidirectional Transformer (MeBT) for generative modeling of videos.
The overall framework of MeBT is illustrated in Fig.~\ref{fig:overview}.
To predict the masked tokens $\mathbf{y}_M$ from the context tokens $\mathbf{y}_C$, MeBT employs an encoder-decoder architecture based on a fixed number of latent bottleneck tokens $\mathbf{z}_L\in\mathbb{R}^{N_L\times d}$ with $N_L \ll N$.
The encoder projects the context tokens $\mathbf{y}_C$ to the latent tokens $\mathbf{z}_L$, and the decoder utilizes the latent tokens $\mathbf{z}_L$ to predict the masked tokens $\mathbf{y}_M$.
Overall, the encoder and decoder exploit the latent bottleneck to achieve a linear complexity $\mathcal{O}(N)$ while performing a full dependency modeling across the token positions.
In the following sections, we describe the details of the encoder, decoder, and the training and inference procedures of MeBT.

%Overview of the network architecture + introduce notations.
\subsection{Encoder Architecture}
\label{sec:mebt_encoder}
\begin{figure}[!th]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/encoder.pdf}
    \vspace{-0.3cm}
    \caption{Encoder architecture. Our encoder compresses the context tokens into fixed number of latent tokens.}
    \label{fig:encoder}
    \vspace{-0.5cm}
\end{figure}
The encoder aims to project all previously generated context tokens $\mathbf{y}_C$ to fixed-size latent bottleneck $\mathbf{z}_L$ with a memory and time cost linear to context size $\mathcal{O}(N_C)$.
Following prior work on transformers with latent bottlenecks~\cite{Perceiver, PerceiverIO, SetTransformer, LUNA, SetVAE}, we construct the encoder as an alternating stack of two types of attention layers that progressively update the latent tokens based on the provided context tokens.
Specifically, as illustrated in Fig.~\ref{fig:encoder}, the first layer updates the latent tokens by cross-attending to context tokens, and the second one updates the latent tokens by performing self-attention within them.
By stacking the layers, the encoder progressively gathers appropriate contextual information into the latent tokens as well as flexibly updating their state through the exchanges of latent information.

% % TODO: convert y into z. (when describing VQGAN)
% % The goal of our encoder is to compress the context tokens $\mathbf{y}_C \in \mathbb{R} ^{N_C \times d}$ into the latent embeddings $\mathbf{z}_L \in \mathbb{R} ^{N_L \times d}$ with linear complexity operations. To achieve this goal, we utilize the encoder design of Perceiver \cite{Perceiver}.
% Our encoder aims to encode all previously generated tokens in linear memory and time complexity to their size. 
% To this end, we employ the design choice of Perceiver~\cite{Perceiver} and project the entire context tokens $\mathbf{y}_C \in \mathbb{R} ^{N_C \times d}$ into latent tokens $\mathbf{z}_L \in \mathbb{R} ^{N_L \times d}$, where $N_L \ll N_C$.

% The encoder is constructed by stacking the encoding block. 
% As illustrated in Figure~\ref{fig:encoder}, each encoding block is a series of two attention layers. 
% % The first attention layer is a cross-attention layer which gets the latent embeddings $\mathbf{z}_L$ as query, and the context tokens $\mathbf{y}_C$ as key and value. 
% % The second attention layer is a self-attention layer among the latent tokens that updates the latent embeddings. 
% The first attention layer projects the context tokens into the latent tokens by applying cross-attention using the former as keys and values and the later as a query.
% Then the second attention layer updates the latent tokens through the self-attention.
% By stacking the encoding blocks, the updated latent embeddings from the previous block are used as a query to project the contexts into the next latents. 
% It improves the {\color{red}integrity}\shcmt{intention?} of the latent embeddings as the model can progressively gather appropriate contextual information based on the current state of latent embeddings.

% Our encoder with the latent bottleneck is effective in modeling long-term dependency in several reasons.
Our encoder with latent bottleneck can benefit bidirectional modeling of videos in several ways.
% First, the computational complexity of the encoder block is linear to the number of context tokens, allowing the model to take longer videos with reduced memory footprint.
First, since we employ the fixed-size latent tokens independent to the video length, the complexity of the encoder scales linearly to the number of context tokens, allowing it to directly learn and model long-range dependency by taking longer videos with a reduced memory footprint.
% More importantly, unlike the sparse attention~\cite{CogVideo, TATS} or hierarchical methods~\cite{MaskViT, MMVID} that are designed to capture partial dependency among the hand-designed neighborhoods, our encoder captures full observations of the previously generated tokens through latent bottleneck, thus can directly learn to harvest long-term dependencies.
More importantly, unlike the sparse attention~\cite{Ho2019, MaskViT} or hierarchical methods~\cite{CogVideo, TATS} that capture partial dependency among the predefined local subset of context tokens, our encoder always captures full observations of any given set of context tokens by exploiting the latent bottleneck.
This property suits well with the bidirectional modeling since the context tokens can be given as an arbitrary subset of a video often beyond the hand-designed patterns of sparse attention. %, whose dependency should be modeled densely by the  
% and the model can directly learn the statistical dependencies of context tokens placed arbitrarily far to each other.}
% As discussed earlier, this property also suits well with the bidirectional decoder since the \shcmt{put similar discussion in overview}.

% {\color{red}
% The encoder achieves linear complexity, as each attention layer in the encoding block has linear or constant complexity with respect to the number of the context tokens. 
% The complexity of the first attention layer in the encoding block is linear, as the query is a fixed number of latent embeddings $\mathbf{z}_L$. 
% Similarly, the complexity of the second attention layer is constant, as it only considers the latent embeddings $\mathbf{z}_L$.
% }
% \shcmt{compared to sparse attention, the encoding with the latent tokens is able to capture the full history of the previous frames (context). we observe that the iterative cross attention from latent to the context tokens allows more effective encoding of the context given the fixed number of latents...}
% \shcmt{it would be interesting if we can visualize which context tokens are encoded more heavily into the latents depending on varying context (both in size and mask pattern) using cross attention. If tokens more relevant to the mask predictions are more heavily encoded than the others, this adaptive encoding looks a strong choice for our task.}

% In the following section, we introduce our decoder that utilizes the latent embeddings $\mathbf{z}_L$ from the encoder as a proxy for the context tokens.
% \begin{gather}
%     \mathbf{z}'_L = EB(\mathbf{z}_L, \mathbf{z}_C), \\
%     EB(\mathbf{z}_L, \mathbf{z}_C) = MAB(\mathbf{h}, \mathbf{h}, \mathbf{h}), \\
%     \label{eq:enc_proj}
%     \text{where, } \mathbf{h} = MAB(\mathbf{z}_L, \mathbf{z}_C, \mathbf{z}_C).
% \end{gather}
% $MAB$ denotes the multihead attention block 

% TMI
% The encoder gets the context tokens $\mathbf{y}_C \in [U]^{N_C}$, and compresses them into latent embeddings $\mathbf{z}_L \in \mathbb{R} ^{N_L \times d}$. Specifically, the model embeds the context tokens $\mathbf{y}_C$ into context embeddings $\mathbf{z}_C \in \mathbb{R}^{N_C \times d}$ as:
% \begin{gather}
%     \mathbf{z}_C = \{z_{y_i} + p_i | i \notin \mathbf{m}\},
% \end{gather}
% where $\mathbf{p} = \{p_1, \cdots, p_N\}$, $\mathbf{z} = \{z_1, \cdots, z_U\}$ denotes the learnable positional embeddings and learnable codebook embeddings, respectively.
% \cite{attention_is_all_you_need, set_transformer}.

% Can encode Partially observable data

% In the following section, we introduce our decoder that utilizes the latent embeddings $\mathbf{z}_L$ from the encoder as a proxy for the context tokens.


\subsection{Decoder Architecture}
\label{sec:mebt_decoder}
Given the latent bottleneck $\mathbf{z}_L$ as a proxy of context\footnote{
Since the latent tokens $\mathbf{z}_L$ are a proxy of context, we use \emph{latent} and \emph{context} interchangeably to refer to them when describing the decoder.}, the decoder aims to predict the masked tokens $\mathbf{y}_M$ by updating the mask embeddings $\mathbf{z}_M$.
% Here, we want the decoder to have a linear complexity to the mask size $\mathcal{O}(N_M)$, as entire video can be masked in the worst case ($N_M=N$).
Since the standard masked modeling~\cite{MaskGIT, BERT, MAE} of applying a bidirectional transformer on mask and context $\{\mathbf{z}_M, \mathbf{z}_L\}$ leads to a complexity of $\mathcal{O}((N_M+N_L)^2)$, we opt into an efficient decoder that reduces the cost to linear to mask size $\mathcal{O}(N_M)$ while preserving as much modeling capacity as possible.

% \ifdefined\paratitle {\color{blue}
% [The decoder gets the latent embeddings and mask embeddings which represents the position of masks.] \\
% } \fi
% \shcmt{say that the complexity should be linear in number of both context and mask tokens.}
% % TODO: introduce mask embedding in MaskGIT.
% The goal of our decoder is to predict the masked tokens $\mathbf{y}_M$ based on the latent embeddings $\mathbf{z}_L$ and the mask embeddings $\mathbf{z}_M$ with linear complexity. During the prediction, to reach the performance of the quadratic decoder, the proposed decoder should consider all the dependencies that are modeled in the quadratic decoder. To this end, We propose a decoder that precisely models the dependencies while having a linear complexity.

We construct our decoder as a stack of two types of attention layers that update the latent tokens $\mathbf{z}_L$ and mask tokens $\mathbf{z}_M$ in an alternating manner.
More specifically, as illustrated in Fig.~\ref{fig:decoder}, the first layer updates latent tokens $\mathbf{z}_L$ by attending to \emph{both} latent and mask $\{\mathbf{z}_L, \mathbf{z}_M\}$, and the second layer updates mask tokens $\mathbf{z}_M$ by attending to the latent tokens $\mathbf{z}_L$.
After being processed by the decoder, the mask tokens $\mathbf{z}_M$ are then used to predict the masked tokens $\mathbf{y}_M$.

% Our decoder is constructed by stacking the decoding block. The overall design of the decoding block is illustrated in Fig.\ref{fig:decoder}.(c). The decoding block consists of two cross-attention layers. The first cross-attention layer gets the latent embeddings $\mathbf{z}_L$ as query, and the union of latent embeddings and mask embeddings $\mathbf{u} = \{\mathbf{z}_L, \mathbf{z}_M\}$ as key and value. The second attention layer gets the mask embeddings $\mathbf{z}_M$ as query, and the latent embeddings $\mathbf{z}_L$ as key and value.
% Formally, the decoding block $Dec$ is designed as follows:
% \begin{gather}
%     \label{eq:dec_mab1}
%     Dec(\mathbf{z}_L, \mathbf{z}_M) = MAB_2(\mathbf{z}_M, \mathbf{z}'_L, \mathbf{z}'_L),\\
%     \label{eq:dec_mab2}
%     \text{where, } \mathbf{z}'_L = MAB_1(\mathbf{z}_L, \mathbf{u}, \mathbf{u})
% \end{gather}
% The processed mask embeddings $\mathbf{z}_M$ are used to predict the masked tokens $\mathbf{y}_M$.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/decoder.pdf}
    \vspace{-0.3cm}
    \caption{Decoder architecture. Our decoder predicts the masked tokens based on the latent tokens encoding the entire context.}
    \label{fig:decoder}
    \vspace{-0.4cm}
\end{figure}


Our decoder achieves linear complexity while retaining a high expressive power, thus being suitable for directly learning and generating long-term dependency.
Since the two attention types have $\mathcal{O}(N_L \times (N_L+N_M)))$ and $\mathcal{O}(N_LN_M))$ complexities respectively, our decoder overall achieves a linear complexity to mask size $N_M$.
Still, we note that this efficient decoder retains much of the modeling capacity of standard masked decoders~\cite{BERT, MAE, MaskGIT} that employ full self-attention on context and mask $\{\mathbf{z}_L, \mathbf{z}_M\}$.
This is seen by decomposing their \emph{all-to-all} dependency $\{\mathbf{z}_L, \mathbf{z}_M\}\to\{\mathbf{z}_L, \mathbf{z}_M\}$ into \emph{all-to-context} $\{\mathbf{z}_L, \mathbf{z}_M\}\to \mathbf{z}_L$, \emph{context-to-mask} $\mathbf{z}_L\to\mathbf{z}_M$, and \emph{mask-to-mask} $\mathbf{z}_M\to\mathbf{z}_M$ dependencies.
In our efficient decoder, the all-to-context and context-to-mask dependencies are precisely modeled by the two types of attention layers, and the mask-to-mask dependency is approximately modeled through the \emph{composition} of the layers by exploiting the latent tokens $\mathbf{z}_L$ as an intermediate bottleneck.
More specifically, to model $\mathbf{z}_M\to\mathbf{z}_M$, our decoder imposes a low-rank approximation through the fixed-size latent representation $\mathbf{z}_M\to\mathbf{z}_L\to\mathbf{z}_M$.
While such approximation appears in a range of prior work on efficient transformers~\cite{SetTransformer, Linformer, LUNA, SetVAE}, to our knowledge, we are the first to employ it for bidirectional video generation.
% Our experiments show that the proposed decoder still maintains much expressiveness in the bidirectional decoders modeling all-to-all relationships (Section~\ref{sec:dec_ablation}).
% {\color{red}Even with such approximation, our experiments show that the proposed decoder still maintains much expressiveness in the bidirectional decoders modeling all-to-all relationships (Section~\ref{sec:dec_ablation}).}

% Our decoder can reach the performance of the quadratic bidirectional decoder, as both decoders model the necessary dependencies. The design of the quadratic bidirectional decoder is illustrated in Fig.\ref{fig:decoder}(a). The bidirectional decoder is constructed by stacking self-attention layers. The self-attention layer gets the union of the latent embeddings and the mask embeddings $\mathbf{u}$. Through the attention layer, three kinds of dependencies are modeled: latent-latent dependency, latent-mask dependency, and mask-mask dependency. The first two dependencies are directly modeled by $MAB_1$ in Eq.\ref{eq:dec_mab1}. The mask-mask dependency is implicitly modeled in our decoder by updating the mask embeddings $\mathbf{z}_m$ by the latent embeddings $\mathbf{z}_L$, which is updated by the previous mask embeddings $\mathbf{z}_M$.

% The proposed decoder achieves linear complexity, as each attention layer in the decoding block has linear complexity with respect to the size of the target. The complexity of $MAB_1$ in the decoding block is linear, as the query is a fixed number of latent embeddings $\mathbf{z}_L$. Similarly, the complexity of $MAB_2$ is linear, as $\mathbf{z}'_L$ follows the constant size of $\mathbf{z}_L$. To summarize, the proposed decoder achieves linear complexity and the expressibility of the quadratic bidirectional decoder.

% We found that the modeling of mask-mask dependency is significant in practice. To check the importance of mask-mask dependency empirically, we design a decoding block that achieves linear complexity, but drops the mask-mask dependency as demonstrated in Fig.\ref{fig:decoder}(b). We conjecture 
% The masking indices $\mathbf{m}$ is embedded to the mask embeddings $\mathbf{z}_M$ as follows:
% \begin{equation}
%     \mathbf{z}_M = \{[\texttt{MASK}] + p_i | i \in \mathbf{m}\},
% \end{equation}
% where $[\texttt{MASK}] \in \mathbb{R}^d$ is a special learnable token that represents a mask, and $p_i \in \mathbf{p}$ is the positional embedding.

% \ifdefined\paratitle {\color{blue}
% [There exist various designs for decoding block, and our decoding block can achieve linear complexity without dropping any dependency.] \\
% } \fi
% The decoder is constructed by stacking the decoding block $DB$. The decoding block $DB$ gets $\mathbf{z}_L$ and $\mathbf{z}_M$ and outputs $\mathbf{z}'_L$ and $\mathbf{z}'_M$.
% \begin{gather}
%     (\mathbf{z}'_L, \mathbf{z}'_M) = DB(\mathbf{z}_L, \mathbf{z}_M).
% \end{gather}
% Various design of the decoding block is illustrated in Fig.\ref{fig:decoder}. Based on the above formulation and considering the latent embeddings as a proxy for the contexts, the decoding block in bidirectional transformers can be interpreted as: 
% \begin{gather}
%     DB_{full}(\mathbf{z}_L, \mathbf{z}_M) = (\mathbf{z}'_L, \mathbf{z}'_M), \\
%     \text{where, } \mathbf{z}'_L = MAB(\mathbf{z}_L, \{\mathbf{z}_L, \mathbf{z}_M\}, \{\mathbf{z}_L, \mathbf{z}_M\}),\\
%     \label{eq:MAB}
%     \mathbf{z}'_M = MAB(\mathbf{z}_M, \{\mathbf{z}_L, \mathbf{z}_M\}, \{\mathbf{z}_L, \mathbf{z}_M\}).
% \end{gather}
% This type of decoding block can model any dependency between latent-mask, mask-mask, and latent-latent, but the attention block in Eq.\ref{eq:MAB} has quadratic complexity.

% Intuitively, to improve the complexity, we may adopt the symmetric structure of the encoder as:
% \begin{gather}
%     DB_{cross}(\mathbf{z}_L, \mathbf{z}_M) = (\mathbf{z}'_L, MAB(\mathbf{z}_M, \mathbf{z}'_L, \mathbf{z}'_L)), \\
%     \text{where, } \mathbf{z}'_L = MAB(\mathbf{z}_L, \mathbf{z}_L, \mathbf{z}_L)
% \end{gather}
% The above decoding block achieves linear complexity by using the fixed number of latent embeddings $\mathbf{z}_L$. However, the block cannot model the dependency between the mask embeddings $\mathbf{z}_M$ as the queries are independently processed in the attention block $MAB$. We found that the dependency dropped by $DB_{cross}$ results in an inconsistency between the target tokens empirically.

% To model the whole dependency of a bidirectional transformer while keeping the linear complexity, we propose the decoding block $DB_{linear}$:
% \begin{gather}
%     \label{eq:dec1}
%     DB_{linear}(\mathbf{z}_L, \mathbf{z}_M) = (\mathbf{h}, MAB(\mathbf{z}_M, \mathbf{h}, \mathbf{h})), \\
%     \label{eq:dec2}
%     \text{where, } \mathbf{h} = MAB(\mathbf{z}_L, \{\mathbf{z}_L, \mathbf{z}_M\}, \{\mathbf{z}_L, \mathbf{z}_M\}).
% \end{gather}
% The $MAB$ in Eq.\ref{eq:dec2} models the mask-latent dependency and the latent-latent dependency. The mask-mask dependency is modeled in $DB_{linear}$ by updating the mask embeddings $\mathbf{z}_M$ with $\mathbf{h}$ which contains the information of mask embeddings $\mathbf{z}_M$.
% As the size of latent embeddings $\mathbf{z}_L$ is constant to the size of mask embeddings $\mathbf{z}_M$, the complexity of $MAB$s in both Eq.\ref{eq:dec1} and Eq.\ref{eq:dec2} are linear.

% One possible design to predict the target tokens $\mathbf{y}_M$ from the fixed number of latent embeddings $\mathbf{z}_L$ is flipping the design of our encoder as demonstrated in Fig.\ref{fig:decoder}(a). This design of the decoding block can predict the target tokens $\mathbf{y}_M$ based on the contextual information encoded in the latent embeddings $\mathbf{z}_L$. However, the current decoding block processes each mask embeddings $\mathbf{z}_M$ independently, so it is challenging to output consistent prediction over masked tokens.

% To let the model simulate the dependency between mask embeddings to predict the tokens consistently, we designed the decoder block $DB$ with two cross-attention layers. The first cross-attention block updates the mask embeddings $\mathbf{z}_M$ based on the latent embeddings $\mathbf{z}_L$, and the second cross-attention block updates the latent embeddings $\mathbf{z}_L$ based on the latent embeddings itself and the mask embeddings $\mathbf{z}_M$. As the mask embeddings $\mathbf{z}_M$ and the latent embeddings $\mathbf{z}_L$ are updated according to each other, the decoder block $DB$ can approximate the dependency between the mask embeddings $\mathbf{z}_M$ through the latent embeddings $\mathbf{z}_L$. By modeling the dependency between the mask embeddings, our decoder can predict tokens consistently.
% Specifically, the decoder block $DB$ is implemented as follows:
% \begin{gather}
%     DB(\mathbf{z}_M, \mathbf{z}_L) = MAB(\mathbf{z}_L, \{\mathbf{z}_L, \mathbf{h}_M\}),\\
%     \text{where } \mathbf{h}_M = MAB(\mathbf{z}_M, \mathbf{z}_L).
% \end{gather}
% The $MAB$ denotes a multi-head attention block. As demonstrated in Fig.\ref{fig:decoder}, the decoder is constructed by stacking several decoder blocks $DB$ and adding the cross-attention layer that decodes the mask embeddings $\mathbf{z}_M$ from the latent embeddings $\mathbf{z}_L$. As the $MAB$s used in the decoder block adopts either query or key as a fixed number of latent embeddings $\mathbf{z}_L$, the decoder has linear complexity with respect to $N_M$, the number of target tokens.



% \subsection{Complexity Analysis}
% \label{sec:complexity}
% \ifdefined\paratitle {\color{blue}
% [Multihead attention block is formulated as follows.] \\
% } \fi
% In this section, we introduce the formulation of multi-head attention block (MAB) \cite{attention_is_all_you_need, set_transformer} and analyze our model's complexity.

% Let's denote the queries, keys, and values of the multi-head attention block (MAB) as $Q \in \mathbb{R}^{N_Q \times d}$ and $K \in \mathbb{R}^{N_K \times d}$ and $V \in \mathbb{R}^{N_V \times d}$, respectively. Then, the MAB computes the followings:
% \begin{gather}
%     \text{MAB(Q, K)} = h + MLP(\mathbf{o}),\\
%     \text{where } \mathbf{o} = Q + MHA(Q, K, K),
% \end{gather}
% where MHA denotes multi-head attention \cite{attention_is_all_you_need}, and we omitted the normalization layers for brevity.

% \ifdefined\paratitle {\color{blue}
% [Multihead attention block's complexity is proportional to the product of the size of keys and the size of queries.] \\
% } \fi
% The multihead attention block's complexity is proportional to the product of the size of keys and the size of queries. To be specific, MHA consists of input and output projection layers with the complexity of $O((N_Q + N_K)d^2)$ and softmax-attention with the complexity of $O(N_Q N_K d)$. Therefore, the total complexity of MAB results in $O(N_QN_Kd + (N_Q+N_K)d^2)$.

\subsection{Training and Inference}
\label{sec:train_inference}
\paragraph{Training} 
% To train bidirectional transformers for masked token prediction objective in Eq.~\eqref{eq:maskgit}, prior works~\cite{MaskGIT,MaskViT} employed random sampling of the masking indices $\mathbf{m}$. 
% Specifically, it choose masking ratio $r \in (0, 1]$ according to a predefined schedule function, and sample $N_M = \left \lceil r N \right \rceil$ unique masking indices randomly by $\mathbf{m}\sim\text{Uniform}(1,N)$.
% Although such strategy works well with images or short videos, we observe that training with random masking directly on moderately long-term videos often leads to suboptimal solution.  
% This is presumably because sampling temporally distant tokens at early stage of training makes mask prediction task too difficult, preventing model to learn important local structures. 
To train bidirectional transformers for masked token prediction objective in Eq.~\eqref{eq:maskgit}, prior works~\cite{MaskGIT,MaskViT} employed random sampling of the masking indices $\mathbf{m}$. 
Specifically, it choose masking ratio $r \in (0, 1]$ according to a predefined schedule function, and sample $N_M = \left \lceil r N \right \rceil$ unique masking indices randomly by $\mathbf{m}\sim\text{Uniform}(1,N)$.
Although such strategy works well with images or short videos, we observe that training with the strategy directly on moderately long videos often leads to suboptimal solution.  
This is presumably because sampling temporally distant tokens at early stage of training makes the task too difficult, preventing model to learn important local structures. 

To resolve the issue, we propose a simple curriculum-based training that guides the model to learn from short- to long-term sequences gradually over training. 
% To this end, we define an \emph{interval} scheduling function by:
To this end, we define an \emph{interval} scheduling function that dictates the lengths (intervals) $v$ of training videos at a particular training iteration $n$ by:
\begin{equation}
    p_n(v) \propto \text{exp}\left( - \frac{(v-1 - \frac{n}{\alpha})^2}{2\beta^2} \right)
    \label{eq:masking_schedule}
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters. 
Eq.~\eqref{eq:masking_schedule} characterizes a Gaussian distribution of training video lengths $v$ whose mean gradually increases by the training iterations $n$ with a speed determined by $\alpha$ (Fig.~\ref{fig:video_schedule}).
At every training iteration $n$, we sample the interval $v\in(1,t]$ by first sampling $\hat{v}\sim p_n(v)$ and truncating it within a valid range by $v=\min(\max(1,\lceil\hat{v}\rceil),t)$. 
% Then we sample a random interval of length $v$ from a training video and the masking indices randomly within the interval as discussed above.
Then we sample $v$ consecutive frames randomly in a video, and sample the masking indices $\mathbf{m}$ and the context tokens randomly within the sampled interval as discussed above.
We found that such curriculum learning is effective in stabilizing training and improving the prediction quality for long videos (Section~\ref{sec:ablation_study}).

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/schedule.pdf}
    \vspace{-0.2cm}
    \caption{Visualization of the interval scheduling function. 
    }
    \label{fig:video_schedule}
    \vspace{-0.7cm}
\end{figure}

% {\color{red}
% Our model is trained to predict randomly masked tokens from the unmasked context tokens. The masking indices $\mathbf{m}$ are sampled as follows. For each training step $i$, we first sample the length $v$ of a video to train. The distribution of $v$ is defined with a video scheduling function $f(v, n)$. Then, we sample the masking ratio $r \in (0, 1]$. With $r$ and $v$ we sample the masking indices $\mathbf{m}$ by randomly collecting disjoint $N_M = \left \lceil r hwv \right \rceil$ indices to mask from the index set $\mathcal{I} = \{1, \cdots, hwv\}$. The model is trained with the objective in Eq.\ref{eq:maskgit}.

% On longer video modeling, we found that adopting a video scheduling function that gradually extends the length of videos to learn enhances the quality of generation. \jh{We conjecture that directly training the model with longer videos encourages the model to encode global context rather than high-frequency details.} The distribution of video lengths obtained from various video scheduling functions are plotted in Fig.\ref{fig:video_schedule}.
% }
%\jh{A detailed definition of video scheduling functions can be founded in the supplementary material.}\\
% \jhcmt{Listing the video scheduling functions may mess up. Is it fine to move the definition of functions to the appendix?}
\vspace{-0.2cm}
\paragraph{Inference} 
% During inference, we iteratively generate videos by applying the mask prediction discussed in Section~\ref{}. 
During inference, MeBT generates videos by iteratively decoding subsets of mask tokens.
% Following the standard pipeline~\cite{}, the iterative decoding process is governed by the mask schedule function $\gamma$, which is a decreasing function on $[0, 1]$, where $\gamma(0) = 1$, and $\gamma(1) = 0$. 
% To this end, we define a mask sampling function $\gamma$, which is a decreasing function on $[0, 1]$, where $\gamma(0) = 1$, and $\gamma(1) = 0$.
To this end, we define a mask sampling function $\gamma$ on $[0, 1]$ that decreases from $\gamma(0) = 1$ to $\gamma(1) = 0$.
At each decoding step $s \in \{0, \cdots, S-1\}$, our method produces predictions on the mask tokens by Eq.~\eqref{eq:maskgit}, and updates the context tokens with top $N_s$ mask tokens according to their sampled probability where $N_s = N - \left \lceil \gamma(\frac{s}{S}) N \right \rceil$.
The process is iteratively applied $S$ times until all tokens are included in the context.
% At each decoding step $s \in \{0, 1, \cdots, S-1\}$, our model produces predictions mask tokens by Eq.~\eqref{eq:maskgit} and choose the top $N_s = N - \left \lceil \gamma(\frac{s}{S}) N \right \rceil$ tokens with most confidence prediction scores.
In addition to the na\"ive decoding, we also adopt the revision phase proposed in \cite{draft_and_revise}. 
In this stage, we randomly divide the indices evenly into $R$ partitions $\Pi = \{\mathbf{m}_1, \cdots, \mathbf{m}_R\}$, and revise the tokens in $\mathbf{m}_i$ based on the contexts. 
We found that the additional revision is helpful to improve the consistency and fidelity of generated videos.
%Through the revision phase, we can improve the consistency and fidelity of generated videos.

% {\color{red}
% During inference, our model generates a video in a particular number of steps. To describe the process, let us denote $\gamma$ and $S$ as a mask scheduling function and the number of decoding steps, respectively. The mask scheduling function $\gamma$ is defined as a decreasing function on $[0, 1]$, where $\gamma(0) = 1$, and $\gamma(1) = 0$. For each decoding step $s \in \{0, 1, \cdots, S-1\}$, $N_s = \left \lceil \gamma(\frac{s}{S}) N \right \rceil$ tokens from the previous decoding step are masked again and predicted.

% To be specific, we followed the confidence score method introduced in MaskGIT \cite{MaskGIT} to select the tokens to mask.
% MaskGIT considers the probability of sampled tokens as a confidence score and masks $N_s$ low-scored tokens. In practice, MaskGIT adds a Gumbel noise with temperature annealing to generate more diverse samples. By controlling the temperature, we can balance the fidelity and diversity of generated samples.

% In addition to the naive decoding strategy, we adopt the revision phase proposed in \cite{draft_and_revise}. In the revision phase, we randomly divide the indices evenly into $R$ partitions $\Pi = \{\mathbf{m}_1, \cdots, \mathbf{m}_R\}$. Then, the model masks and revise the tokens in $\mathbf{m}_i$ based on the contexts. Through the revision phase, we can improve the consistency and fidelity of generated videos.
% }
% \jhcmt{Need more details for confidence score + Draft \& Revise}

