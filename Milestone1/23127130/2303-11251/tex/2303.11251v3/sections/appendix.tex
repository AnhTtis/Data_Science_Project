% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

% \documentclass[10pt,twocolumn,letterpaper]{article}

% %%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% % \usepackage[review]{cvpr}      % To produce the REVIEW version
% % \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
% \usepackage{graphicx}

% % Include other packages here, before hyperref.
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}

% \usepackage{tikz}
% \usepackage{comment}
% \usepackage{color}
% \usepackage{subcaption}
% % \usepackage{titletoc}
% \usepackage{wrapfig}
% \usepackage{pifont}
% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%
% \usepackage{bm}
% \usepackage{amsthm}
% \usepackage{makecell}
% \usepackage{adjustbox}
% \usepackage{multirow}
% \usepackage{wrapfig}

% % It is strongly recommended to use hyperref, especially for the review version.
% % hyperref with option pagebackref eases the reviewers' job.
% % Please disable hyperref *only* if you encounter grave issues, e.g. with the
% % file validation for the camera-ready version.
% %
% % If you comment hyperref and then uncomment it, you should delete
% % ReviewTempalte.aux before re-running LaTeX.
% % (Or just hit 'q' on the first LaTeX run, let it finish, and you
% %  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% \newcommand{\sh}[1]{\textcolor{blue}{#1}}
% \newcommand{\shcmt}[1]{\textcolor{blue}{[SH: {#1}]}}
% \newcommand{\jh}[1]{\textcolor{red}{#1}}
% \newcommand{\warn}[1]{\textcolor{green}{#1}}
% \newcommand{\jhcmt}[1]{\textcolor{red}{[JH: {#1}]}}
% \newcommand{\jw}[1]{\textcolor{teal}{#1}}
% \newcommand{\red}[1]{\textcolor{red}{#1}}

% % magical magic commands for compact draft
% \newcommand{\cutabstractup}{\vspace*{-0.2in}}
% \newcommand{\cutabstractdown}{\vspace*{-0.2in}}
% \newcommand{\cutsectionup}{\vspace*{-0.15in}}
% \newcommand{\cutsectiondown}{\vspace*{-0.12in}}
% \newcommand{\cutsubsectionup}{\vspace*{-0.1in}}
% \newcommand{\cutsubsectiondown}{\vspace*{-0.07in}}
% \newcommand{\cutparagraphup}{\vspace*{-0.1in}}
% \renewcommand{\thetable}{\Alph{table}}  
% \renewcommand{\thefigure}{\Alph{figure}}
% % \def\paratitle{} % display titles for all paragraphs. Uncomment it if you want it disappears.

% % Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}


% %%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{9561} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}


% \begin{document}
% \title{Towards End-to-End Generative Modeling of Long Videos\\with Memory-Efficient Bidirectional Transformers\\ {\emph {\large Appendix}}}
% \author{}
% \maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

% \shcmt{Global comment: use the reference to the corresponding parts of the main text whenever possible.}
\section*{Appendix}
This document provides comprehensive descriptions and results of our method that could not be accommodated in the main paper due to space restriction. More generated samples from MeBT and the baselines~\cite{MoCoGAN-HD, DIGAN, TATS, CCVS} can be found at \href{https://sites.google.com/view/mebt-cvpr2023}{https://sites.google.com/view/mebt-cvpr2023}.

\section{Experimental Details}
\label{appx:experiment}
In this section, we first describe the detailed implementation of training and inference in Section~\ref{sec:train_inference} in the main paper.
Then, we provide additional descriptions of the datasets and baselines used in Section~\ref{sec:experiments} in the main paper.

\subsection{Training and Inference}
\label{appx:training_inference}
\paragraph{Training}
\label{appx:training}
For all experiments, we used the same network architecture except for the size of latent bottleneck $N_L$. Following the configuration of TATS~\cite{TATS}, all models have 24 attention layers, 16 attention heads, 1024 embedding dimensions, and learnable positional embedding for all spatiotemporal positions. For the size of latent bottleneck, we used $N_L=256$ for all experiments.
We use AdamW optimizer~\cite{AdamW} with $\beta_1 = 0.9$ and $\beta_2 = 0.95$. Table~\ref{tab:train_config} summarizes our training configuration for all experiments.

\paragraph{Inference}
\label{appx:inference}
% sequential decoding. (Initialize the context tokens with these, and apply MaskGIT)
% MaskGIT context temperature annealing. (\tau: context temperature)
% Revision options. (R: number of partitions, N_R: number of revisions).
When sampling long videos, we decode $N_d$ tokens by appending a single token to the context tokens $N_d$ times before applying the iterative decoding to hold the consistency of the video. Then, the iterative decoding is applied with the $N_d$ decoded context tokens. For the iterative decoding, we adopt a top-k sampling strategy when sampling the code from logits. We also adopt context temperature annealing when updating the context tokens to increase the diversity of samples following the official implementation of MaskGIT\footnote{https://github.com/google-research/maskgit}. Specifically, let $p_i \in (0, 1)$ be the probability value of sampled token $i$, and $s$, $S$ be the current decoding step and total decoding steps, respectively. Then, we update context tokens with top-$N_s$ decoded tokens with the highest confidence scores $c_i~=~p_i +~\text{Gumbel(0,1)} * (1-\frac{s}{S})\tau$. For the revision, we repeated the revision $N_R$ times with the logits scaled with a certain temperature. The specific configuration for each experiment is summarized in Table \ref{tab:inference_config}. 

\iffalse
\begin{figure*}[!t]
    \begin{minipage}{\textwidth}
    \captionsetup{type=table}
    \centering
    \caption{Quantitative comparison with additional baselines. Subscripts denote the length of videos.}
    \vspace{-0.2cm}
    % \shcmt{edit caption. mark the best {\bf bold}}}
    \begin{adjustbox}{width=0.8\textwidth}
    \label{tab:longterm}
    \centering
        \begin{tabular}{lcccccc}
        \toprule
        \\[-1em]& \multicolumn{2}{c}{SkyTimelapse} & \multicolumn{2}{c}{Taichi-HD} & \multicolumn{2}{c}{UCF-101} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        Method 
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ 
        & FVD$_{128}$  $(\downarrow)$ & KVD$_{128}$  $(\downarrow)$ \\
        \\[-1em]\Xhline{2\arrayrulewidth}
        \\[-1em]MocoGAN-HD$_{16}$ & 663\tiny{$\pm 12.0$} & 32\tiny{$\pm0.89$} & 983\tiny{$\pm35$} & 468\tiny{$\pm31$} & 3060\tiny{$\pm37$} & 225\tiny{$\pm10$}\\
        \\[-1em]MocoGAN-HD$_{128}$ & {498\tiny{$\pm 15$}} & {23\tiny{$\pm1.39$}} & {991\tiny{$\pm23$}} & {290\tiny{$\pm12$}} & {1622\tiny{$\pm53$}} & {118\tiny{$\pm6.4$}}\\
        \\[-1em]DIGAN$_{16}$ & 304\tiny{$\pm10$} & 11\tiny{$\pm0.80$} & 1178\tiny{$\pm13$} & 555\tiny{$\pm15$} & 1635\tiny{$\pm70$} & 109\tiny{$\pm9.3$}\\
        \\[-1em]DIGAN$_{128}$ & 331\tiny{$\pm13$} & 9\tiny{$\pm0.38$} & 764\tiny{$\pm19$} & 230\tiny{$\pm25$} & 1627\tiny{$\pm51$} & 122\tiny{$\pm10$}\\
        % 6048, 5241, 4838
        \\[-1em]MeBT$_{128}$ (Ours) & \bf{239\tiny{$\pm3.6$}} & \bf{5.1\tiny{$\pm0.16$}} & \bf{399\tiny{$\pm19$}} & \bf{139\tiny{$\pm13$}} & \bf{968\tiny{$\pm75$}} & \bf{75.5\tiny{$\pm12.1$}}\\
        \hline
        \\[-1em]CCVS$_{16}$ & N/A & N/A & N/A & N/A & 1411\tiny{$\pm45$} & 121\tiny{$\pm11$} \\
        \\[-1em]TATS-base$_{16}$ & 435\tiny{$\pm12$} & 19\tiny{$\pm1.50$} & 458\tiny{$\pm21$} & 211\tiny{$\pm18$} & 1107\tiny{$\pm31$} & 91.8\tiny{$\pm8.5$}\\
        \\[-1em]TATS-hierarchical$_{128}$ & 455\tiny{$\pm12$} & 21\tiny{$\pm0.99$} & 803\tiny{$\pm40$} & 371\tiny{$\pm31$} & 1138\tiny{$\pm53$} & 83.4\tiny{$\pm$11.6}\\
        \bottomrule
        \end{tabular}
    \end{adjustbox}
    \end{minipage}
    \vspace{0.1cm}
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/supple_degradation.pdf}
        \caption{Evaluation of generation quality over time. Subscripts denote the length of training videos.}
        \label{fig:supp_degradation}
    \end{minipage}
\vspace{-0.3cm}
\end{figure*}
\fi

\subsection{Datasets}
\label{appx:datasets}
\paragraph{SkyTimelapse} SkyTimelapse~\cite{stl} contains time-lapse videos that illustrate the dynamics of the sky such as the growth and motions of clouds. We used the training split to train the model and the validation split to measure the FVD. We train our model with videos that are longer than the training length. We utilized 2,115 videos for training the short-term models and 1,059 videos for training the long-term models.

\paragraph{Taichi-HD} Taichi-HD is a collection of videos of a single actor performing Taichi. Taichi-HD has a total of 2,854 videos including both training and validation split. For Taichi-HD, we utilize both splits and sampled every 4 frames when training on 16-frame videos following the setting of \cite{TATS, DIGAN}. For 128-frame videos, we sampled all frames for training as the 4-frame sampling drops 2,438 videos over 2,854 videos. By sampling every frame, we could utilize 2,760 videos for training the 128-frame model.

\paragraph{UCF-101} UCF-101 is an action recognition dataset that contains 13,320 videos with 101 classes in total. We trained our short-term model on the training split with 9,537 16-frame videos and our long-term model with 6,469 128-frame videos.

\subsection{Long-term Video Generation Baselines}
\label{appx:baselines}
\paragraph{MoCoGAN-HD} MoCoGAN-HD~\cite{MoCoGAN-HD} models videos by training a motion generator on the image latent space provided by a pre-trained image generator. The motion generator of MoCoGAN-HD is implemented with RNNs and thus has sub-quadratic complexity to the video length. Therefore, we trained MoCoGAN-HD with 128-frame videos for long-term comparison.

\paragraph{DIGAN}
DIGAN~\cite{DIGAN} considers a video as a function that outputs the RGB pixel value for the given spatiotemporal coordinate. DIGAN models videos with a motion discriminator that achieves constant complexity by discovering unnatural motions when two frames and the time gap between frames are given. As the discriminator gets two frames and the video is generated by a coordinate-based network, DIGAN has sub-quadratic complexity to the video length. Thus, we trained DIGAN with 128-frame videos for long-term comparison.

\paragraph{CCVS}
CCVS~\cite{CCVS} is a video prediction model that utilizes optical flow and an autoregressive transformer. CCVS can generate longer videos by sliding the attention window, but due to the quadratic complexity of the autoregressive transformer, it cannot be directly trained with long videos.
Hence, for the long-term comparison, we utilized the 16-frame CCVS model to predict 128-frame videos from randomly sampled real frames as an initial frame.

\paragraph{TATS}
TATS~\cite{TATS} models videos by adopting an autoregressive transformer and a time-agnostic 3d VQGAN. The proposed time-agnostic 3d VQGAN can decode longer videos beyond the training length. Therefore, TATS-base can generate longer videos by sliding the attention window. However, due to the quadratic complexity of transformers, TATS-base cannot directly model long videos. To address this issue, the authors proposed TATS-hierarchical that generates long videos sparsely and interpolates the missing frames. We compared TATS-base by applying a sliding attention window, and TATS-hierarchical by training the hierarchical model on 128-frame videos.

\subsection{Comparison to Sparse Attentions}
\label{appx:asymptotic_mem}
\iffalse
To compare the asymptotic memory complexity of sparse attention methods used in Section {\color{red}5.4} with MeBT, let's denote $N(=H\times W)$ and $T$ as the number of tokens in spatial and temporal dimensions, respectively. Then,
\begin{enumerate}
    \item {\bf Axial attention}~\cite{Ho2019} alternates attention over horizontal, vertical, and temporal axes. 
    The queries in each attention layer can only interact with tokens on the same axis. 
    The overall complexity is $O(NT(H+W+T))$.
    \item {\bf Window attention}~\cite{MaskViT} alternates attention over spatial and temporal axes. It performs frame-wise attention over $N$ tokens followed by spatio-temporal attention over $n\times T$ tokens where $n(=16)$ is the size of spatial window. 
    The complexity is $O(N^2T+nNT^2)$.
\end{enumerate}
While these methods are asymptotically more efficient than dense attention of $O(N^2T^2)$, they still have quadratic complexity with respect to the spatial ($N$) or temporal dimension ($T$). It is important to note that MeBT with $n$ latent codes has a linear complexity of $O(nNT)$ for both $N$ and $T$.
\fi

% {\color{red}
% In Section~\ref{sec:ablation_study}, we compared MeBT with sparse attention methods empirically. In this section, we will compare the asymptotic complexity of sparse attention methods with MeBT. To do this, let's denote $N(=H\times W)$ and $T$ as the number of tokens in spatial and temporal dimensions, respectively. Then,
% }
This section presents more in-depth analysis and comparisons with the baselines presented in Section~\ref{sec:ablation_study}.
Let $N(=H\times W)$ and $T$ denote the number of tokens in spatial and temporal dimensions, respectively. Then,
\begin{enumerate}
    \item {\bf Axial attention}~\cite{Ho2019} alternates attention over horizontal, vertical, and temporal axes. 
    The queries in each attention layer can only interact with tokens on the same axis. 
    The overall complexity is $O(NT(H+W+T))$.
    \item {\bf Window attention}~\cite{MaskViT} alternates attention over spatial and temporal axes. It performs frame-wise attention over $N$ tokens followed by spatio-temporal attention over $n\times T$ tokens where $n(=16)$ is the size of spatial window. The complexity is $O(N^2T+nNT^2)$.
    \vspace{-0.4cm}
    \item {\bf Local attention}~\cite{NUWA} computes the attention within a fixed number ($n$) of spatio-temporal neighborhood tokens. The complexity of local attention is $O(nNT)$.
\end{enumerate}
% While these methods are asymptotically more efficient than dense attention of $O(N^2T^2)$, they still have quadratic complexity with respect to the spatial ($N$) or temporal dimension ($T$). It is important to note that MeBT with $n$ latent codes has a linear complexity of $O(nNT)$ for both $N$ and $T$.
% While the axial and window attention are asymptotically more efficient than dense attention of $O(N^2T^2)$, they still have quadratic complexity with respect to the spatial ($N$) or temporal dimension ($T$). In contrast, MeBT with $n$ latent codes and the local attention has a linear complexity of $O(nNT)$ for both N and T.
% Nonetheless, unlike MeBT, local attention may not capture long-range dependencies between tokens that are farther apart. Moreover, without implementing with sparse operations, local attention cannot achieve linear complexity in practice. Since the sparse implementation of local attention is unavailable, we only compared MeBT with axial and window attention in Section~\ref{sec:ablation_study}.

While axial attention and window attention are asymptotically more efficient than dense attention of $O(N^2T^2)$, they still have quadratic complexity with respect to the spatial ($N$) or temporal dimension ($T$). On the other hand, local attention and MeBT with $n$ latent codes have a linear complexity of $O(nNT)$ for both N and T. 
% However, local attention may not capture long-range dependencies between distant tokens, which MeBT can do. 
% Additionally, local attention cannot achieve linear complexity in practice without sparse operations. 
% It is worth noting that the sparse implementation of local attention for videos is currently unavailable. 
However, local attention cannot effectively model the long-term dependency of tokens due to the limited receptive fields, and its benefits are often not fully leveraged due to the lacking support in both hardware and software for the sparse operations\footnote{Most of the currently available implementations of local attention is not leveraging the sparse operators, which makes them have the same complexity as dense attention.}. %is often not GPU-friendly and lacks both hardware and software support.
Therefore, we only compared MeBT with axial and window attention in our ablation study in Section~\ref{sec:ablation_study}.


% Although these methods are asymptotically efficient than dense attention of $O(N^2T^2)$, they are still quadratic to either spatial ($N$) or temporal dimension ($T$); note that MeBT with $n$ latent codes is linear $O(nNT)$ to both $N$ and $T$.

% \iffalse
% \section{Additional Comparison with Baselines}
% In this section, we additionally compare our model with MoCoGAN-HD~\cite{MoCoGAN-HD} and DIGAN~\cite{DIGAN} which can generate longer videos than the videos trained with, and compare our model's empirical memory complexity against the transformer baseline \cite{TATS}.

% \subsection{Comparison to Extrapolatable Baselines}
% We compared MeBT with models that have sub-quadratic complexity. We trained the sub-quadratic baselines (MoCoGAN-HD, DIGAN) on 128-frame videos to compare the performance of long video modeling. We measured the FVD, KVD~\cite{FVD}, and the relative FVD over time following the experimental setup in Section \jh{5.3} in the main paper. The results are shown in Table \ref{tab:longterm}, Figure \ref{fig:supp_degradation}, \ref{fig:supp_stl}, \ref{fig:supp_taichi}, \ref{fig:supp_ucf}.

% As shown in Table \ref{tab:longterm}, the baselines showed improved performance in generating long videos compared to their short-term models. To be specific, MoCoGAN-HD showed improved performance on SkyTimelapse and UCF-101 in terms of FVD and KVD, and on Taichi-HD in terms of KVD. As shown in Figure \ref{fig:supp_degradation}, MoCoGAN-HD could address the collapsing of frames in long-term by training with longer videos. However, as displayed in Figure~\ref{fig:supp_stl}, due to the recurrent module in the motion generator, MoCoGAN-HD still exhibits error propagation even in the training length. On the other hand, DIGAN showed improved performance on Taichi-HD while the performance change on SkyTimelapse and UCF-101 are subtle. This is because DIGAN could address the performance degradation on Taichi-HD by training with long videos (Fig. \ref{fig:supp_taichi}) while still showing weakness in modeling complex structures on UCF-101. As shown in Figure \ref{fig:supp_ucf}, although DIGAN and MoCoGAN-HD are trained with longer videos, they demonstrated inferior performance in modeling complex scenes compared to the transformer models. It shows that training transformers on long videos is necessary for modeling complex long videos.
% \fi

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/empirical_memory.pdf}
    \caption{Empirically measured training peak memory over training video lengths. MeBT showed linear complexity to the video length, while TATS showed quadratic complexity.}
    \label{fig:memory_comparison}
\end{figure}
% MeBT exhibits linear complexity not only in theory but also in practice.
\section{Empirical Analysis on Memory Complexity}
\label{appx:empirical_mem}
We compare MeBT's empirical memory consumption over video lengths with an autoregressive transformer by measuring the training peak memory on a single A100 GPU with 80GB of VRAM with batch size 4. The results are shown in Figure \ref{fig:memory_comparison}. As shown in the figure, the training peak memory of MeBT scaled up linearly while the autoregressive transformer \cite{TATS} showed quadratic growth. In particular, with batch size 4, we couldn't train the autoregressive transformer with videos longer than 36 frames represented with 2304 tokens due to the out-of-memory error. Compared to the autoregressive transformer, our model spends about 40GB when training with 128-frame videos (8,192 tokens). It shows that MeBT is much more efficient than the transformer in long videos and capable of modeling longer videos directly.

\section{Additional Qualitative Results}
\label{appx:qualitative}
In this section, we extend the discussion in Section~\ref{sec:exp_long_video} in the main paper with additional qualitative results.
% Ours SkyTimelapse samples
% Ours Taichi samples
% Ours UCF-101 samples
The generated videos from MeBT and the baselines can be found at \href{https://sites.google.com/view/mebt-cvpr2023}{https://sites.google.com/view/mebt-cvpr2023}.
\subsection{Qualitative Comparison on SkyTimelapse}
The generated videos from the baselines and MeBT are displayed in Figure \ref{fig:supp_stl}. As shown in the figure, our model demonstrates consistent high-fidelity sky videos. To be specific, compared to TATS-base and TATS-hierarchical, our model showed better consistency on modeling the static ground. This is because our model can decode the tokens in a bidirectional manner. As autoregressive transformers follow the raster-scan ordering, the tokens that represent the ground in the previous frame and current frame are far away on the sequence. Compared to DIGAN~\cite{DIGAN} and MoCoGAN-HD~\cite{MoCoGAN-HD}, MeBT exhibits high-fidelity video generation while DIGAN shows unrealistic artifacts and MoCoGAN-HD collapses when generating long-term frames.

\subsection{Qualitative Comparison on Taichi-HD}
The generated videos on Taichi-HD are shown in Figure \ref{fig:supp_taichi}. Unlike other baselines, MeBT could model the non-linear motions in Taichi by connecting the basic actions. Specifically, DIGAN showed difficulties in generating non-linear motions and MoCoGAN-HD could not keep the background and actor consistently. The frames generated by TATS-base become brighter as the video goes longer due to the error propagation, and TATS-hierarchical showed temporally ziggy motions by adopting two separately trained transformers. The ziggy motions of TATS-hierarchical are best viewed on the website.

\subsection{Qualitative Comparison on UCF-101}
The generated videos on UCF-101 are displayed in Figure \ref{fig:supp_ucf}. On the complex UCF-101 dataset, DIGAN and MoCoGAN-HD failed to model complex structures such as human faces. On the other hand, transformer-based models (TATS, MeBT) could model complex structures. However, TATS-base showed unnatural artifacts due to the error propagation, and TATS-hierarchical showed inconsistent quality between the keyframes generated by the hierarchical transformer and the interpolated frames. Compared to the baselines, MeBT generates complex motion that combines both camera motion and horse riding.

% Comparison on Taichi
% Comparison on STL
% Comparison
% More discussion with extra samples.
% Also attach the website.

% Video length distribution. (# of videos dropped.)
% How many videos are in each dataset.
% Where we got the datasets.
% Each datasets characteristic (STL -> global. Taichi -> local, UCF-101 -> local + global, diverse actions.)

\begin{table*}[t]
    \centering
    \caption{Training configuration for all experiments. Subscripts denote the length of training videos.}
    \label{tab:train_config}
    \begin{tabular}{l|cccccc}
    \toprule
    Dataset & SkyTimelapse$_{16}$ & Taichi-HD$_{16}$ & UCF-101$_{16}$ & SkyTimelapse$_{128}$ & Taichi-HD$_{128}$& UCF-101$_{128}$\\
    \midrule
    Batch size & 24 & 128 & 128 & 40 & 48 & 48\\
    Learning rate & 1.08e-5 & 3e-5 & 3e-5 & 1.8e-5 & 3e-5 & 3e-5\\
    Training steps & 400k & 750k & 2.6M & 500k & 3M & 3.55M \\
    Dropout rate & 0.1 & 0 & 0 & 0.1 & 0 & 0 \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Decoding configuration for all experiments. Subscripts denote the length of training videos.}
    \label{tab:inference_config}
    \begin{tabular}{l|cccccc}
    \toprule
    Dataset & SkyTimelapse$_{16}$ & Taichi-HD$_{16}$ & UCF-101$_{16}$ & SkyTimelapse$_{128}$ & Taichi-HD$_{128}$& UCF-101$_{128}$\\
    \midrule
    $N_d$ & 0 & 0 & 0 & 64 & 64 & 64\\
    \hline
    $S$ & 32 & 64 & 128 & 32 & 32 & 32\\
    top-k & - & - & - & 32 & 32 & 32 \\
    $\gamma$ & cosine & cosine & cosine & cosine & cosine & cosine \\
    $\tau$ & 8 & 2 & 6 & 4 & 4 & 2 \\
    \hline
    $R$  & 2 & 2 & 4 & 2 & 2 & 32 \\
    temperature  & 0.7 & 0.3 & 0.7 & 0.7 & 0.1 & 0.1 \\
    $N_R$  & 2 & 8 & 4 & 2 & 4 & 2 \\
    \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/supp_stl_v2.pdf}
    \caption{Generated videos on SkyTimelapse. We displayed every 10th frame in the generated video. The subscript denotes the length of training videos. More samples can be found on the website.}
    \label{fig:supp_stl}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/supp_taichi_v2.pdf}
    \caption{Generated videos on Taichi-HD. We displayed every 10th frame in the generated video. The subscript denotes the length of training videos. More samples can be found on the website.}
    \label{fig:supp_taichi}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/supp_ucf_v2.pdf}
    \caption{Generated videos on UCF-101. We displayed every 10th frame in the generated video. The subscript denotes the length of training videos. More samples can be found on the website.}
    \label{fig:supp_ucf}
\end{figure*}