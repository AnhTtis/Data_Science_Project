% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{graphicx, amsmath, amssymb, caption, subcaption, multirow, overpic, textpos, tabularx}
\usepackage[table]{xcolor}
% Include other packages here, before hyperref.
% \usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{comment}
\usepackage{color}
% \usepackage{subcaption}
\usepackage{titletoc}
\usepackage{wrapfig}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{bm}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{adjustbox}
% \usepackage{multirow}
\usepackage{wrapfig}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand{\sh}[1]{\textcolor{blue}{#1}}
\newcommand{\shcmt}[1]{\textcolor{blue}{[SH: {#1}]}}
\newcommand{\jh}[1]{\textcolor{red}{#1}}
\newcommand{\warn}[1]{\textcolor{green}{#1}}
\newcommand{\jhcmt}[1]{\textcolor{red}{[JH: {#1}]}}
\newcommand{\jw}[1]{\textcolor{teal}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}

% magical magic commands for compact draft
\newcommand{\cutabstractup}{\vspace*{-0.2in}}
\newcommand{\cutabstractdown}{\vspace*{-0.2in}}
\newcommand{\cutsectionup}{\vspace*{-0.15in}}
\newcommand{\cutsectiondown}{\vspace*{-0.12in}}
\newcommand{\cutsubsectionup}{\vspace*{-0.1in}}
\newcommand{\cutsubsectiondown}{\vspace*{-0.07in}}
\newcommand{\cutparagraphup}{\vspace*{-0.1in}}

% commands from MAE
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand\blfootnote[1]{\begingroup\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup}

\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}

\newcommand{\app}{\raise.17ex\hbox{$\scriptstyle\sim$}}
\newcommand{\mypm}[1]{\color{gray}{\tiny{$\pm$#1}}}
\newcommand{\x}{{\times}}
\definecolor{deemph}{gray}{0.6}
\newcommand{\gc}[1]{\textcolor{deemph}{#1}}
\definecolor{baselinecolor}{gray}{.9}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\newcommand{\authorskip}{\hspace{2.5mm}}

% \def\paratitle{} % display titles for all paragraphs. Uncomment it if you want it disappears.

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{9561} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Long-term Dependency Modeling on Videos with Memory Efficient Bidirectional Transformers}
% \title{Towards End-to-End Modeling of Long-Term Dependency in Videos\\with Memory-Efficient Bidirectional Transformers}
\title{Towards End-to-End Generative Modeling of Long Videos\\with Memory-Efficient Bidirectional Transformers}
\author{Jaehoon Yoo\textsuperscript{\rm 1}, Semin Kim\textsuperscript{\rm 1},  Doyup Lee\textsuperscript{\rm 2}, Chiheon Kim\textsuperscript{\rm 2}, Seunghoon Hong\textsuperscript{\rm 1}\\
\textsuperscript{\rm 1}KAIST, 
\textsuperscript{\rm 2}Kakao Brain\\
{\tt\small \{wogns98, kimsm1121, seunghoon.hong\}@kaist.ac.kr, \{damien.re, frost.conv\}@kakaobrain.com}}
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Autoregressive transformers have shown remarkable success in video generation. 
    %   However, due to the quadratic complexity of self-attention, the transformers are prohibited from directly learning the long-term dependency in videos. 
    However, the transformers are prohibited from directly learning the long-term dependency in videos due to the quadratic complexity of self-attention, and inherently suffering from slow inference time and error propagation due to the autoregressive process.  
   In this paper, we propose Memory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of long-term dependency in videos and fast inference. 
   Based on recent advances in bidirectional transformers, our method learns to decode the entire spatio-temporal volume of a video in parallel from partially observed patches. 
   The proposed transformer achieves a linear time complexity in both encoding and decoding, by projecting observable context tokens into a fixed number of latent tokens and conditioning them to decode the masked tokens through the cross-attention. 
%   Empowered by linear complexity and bidirectional modeling, our method can be trained with long videos directly and demonstrates superior performance compared to the models that cannot be trained with long videos due to the limited memory budget.
Empowered by linear complexity and bidirectional modeling, our method demonstrates significant improvement over the autoregressive transformers for generating moderately long videos in both quality and speed. Videos and code are available at \href{https://sites.google.com/view/mebt-cvpr2023}{https://sites.google.com/view/mebt-cvpr2023}.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{sections/1_introduction}
\input{sections/3_method}
\input{sections/2_related_work}
\input{sections/4_experiment}
\input{sections/5_conclusion}

\cutparagraphup
\paragraph{Acknowledgements}
This work was supported in part by the Institute of Information \& communications Technology Planning \& Evaluation (IITP) (No.  and 2019-0-00075) and the National Research Foundation of Korea (NRF) (No. 2021R1C1C1012540 and 2021R1A4A3032834) funded by the Korea government (MSIT), Korea Meteorological Administration under Grant (KMA2021-00121), and Kakao Brain corporations. 
We thank Jinwoo Kim (KAIST) for helpful comments and discussions.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\clearpage
\input{sections/appendix.tex}

\end{document}
