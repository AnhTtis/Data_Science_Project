\section{Introduction}
% \begin{itemize}
%     \item Powerful generation with transformers.
%     \item The problem of Prior Works.
%     \item Our Solution.
%     \item Our Solution's property
%     \item What is shown by experiments?
% \end{itemize}

% \ifdefined\paratitle {\color{blue}
% [we tackle the task of video synthesis. Video synthesis is hard for many reasons, but modeling long-range dependency is important.] \\
% } \fi

Modeling the generative process of videos is an important yet challenging problem.
Compared to images, generating convincing videos requires not only producing high-quality frames but also maintaining their semantic structures and dynamics coherently over long timescale~\cite{DIGAN, TATS, StyleGAN-V, HVP, CogVideo, CWVAE}.
% \jh{The generative modeling of videos} is an important yet challenging problem.
% Compared to images, generating convincing videos requires not only producing high-quality frames but also maintaining \jh{the} semantic structures and dynamics coherently over long timescale~\cite{DIGAN, TATS, StyleGAN-V, HVP, CogVideo, CWVAE}.

% Recently, autoregressive transformers on discrete representation of videos have shown promising generative modeling performances~\cite{TATS, NUWA, videoGPT, LVT}.
% Such methods generally involve two stages, where the video frames are first turned into discrete tokens through vector quantization, and then their sequential dynamics are modeled by an autoregressive transformer.
% Powered by the flexibility of discrete distributions and the expressiveness of transformer architecture, these methods demonstrate impressive results in learning and synthesizing high-fidelity videos.

Recently, autoregressive transformers on discrete representation of videos have shown promising generative modeling performances~\cite{TATS, NUWA, videoGPT, LVT}.
Such methods generally involve two stages, where the video frames are first turned into discrete tokens through vector quantization, and then their sequential dynamics are modeled by an autoregressive transformer.
Powered by the flexibility of discrete distributions and the expressiveness of transformer architecture, these methods demonstrate impressive results in learning and synthesizing high-fidelity videos.

% \ifdefined\paratitle {\color{blue}
% [Limitations in Transformer (computational complexity and inference time); there are alternatives, but they all suffer from some limitations. End-to-end training is the only way to go] \\
% } \fi
% However, exploiting the Transformers for learning long-range dependency in videos still remains an open challenge. First, due to the quadratic complexity of self-attention, training Transformers is limited to short sequences (usually less than dozens of frames), making it hard to learn and leverage long-term dependencies in videos. 
% Second, the autoregressive generative process is inherently prone to error propagation. (i.e., prediction error accumulates over time). 
% Finally, the autoregressive process is suffering from slow inference time. inherently serial processing thus
% % Second, the autoregressive generative process is inherently prone to error propagation (i.e., prediction error accumulates over time) and suffering from slow inference time, both of which hinder synthesizing long-term videos. 

However, autoregressive transformers for videos suffer from critical scaling issues in both training and inference.
During training, due to the quadratic cost of self-attention, the transformers are forced to learn the joint distribution of frames entirely from short videos (\emph{e.g.}, 16 frames~\cite{TATS, videoGPT}) and cannot directly learn the statistical dependencies of frames over long timescales.
During inference, the models are challenged by two issues of autoregressive generative process -- its serial process significantly slows down the inference speed, and perhaps more importantly, autoregressive prediction is prone to error propagation where the prediction error of the frames accumulates over time.

To (partially) address the issues, prior works proposed improved transformers for generative modeling of videos, which are categorized as the following:
\textbf{(a)}~Employing sparse attention to improve scaling during training~\cite{NUWA, MaskViT, CogVideo},
% \textbf{(b)}~Employing a hierarchy of autoregressive transformers with coarse-to-fine temporal sampling rates~\cite{CogVideo, TATS}, and
\textbf{(b)}~Hierarchical approaches that employ separate models in different frame rates to generate long videos with a smaller computation budget~\cite{CogVideo, TATS}, and
% \textbf{(c)}~Removing autoregression by viewing inference as masked generative modeling and training a bidirectional transformer~\cite{MaskViT, MMVID}.
\textbf{(c)}~Removing autoregression by formulating the generative process as masked token prediction and training a bidirectional transformer~\cite{MaskViT, MMVID}.
% While each approach focuses on solving one of the issues of autoregressive transformers, they fail to solve them jointly -- \textbf{(a,~b)}~still suffer the issues of autoregressive inference, and \textbf{(c)}~still cannot directly learn long-range dependency.
While each approach is effective in addressing specific limitations in autoregressive transformers, none of them provides comprehensive solutions to aforementioned problems -- \textbf{(a,~b)}~still inherits the problems in autoregressive inference and cannot leverage the long-term dependency by design due to the local attention window, and \textbf{(c)}~is not appropriate to learn long-range dependency due to the quadratic computation cost.
We believe that an alternative that jointly resolves all the issues would provide a promising approach towards powerful and efficient video generative modeling with transformers.


% Many prior works attempt to address this problem by limiting the attention window to a local spatio-temporal neighborhood~\cite{} or combining multi-stage models each of which operates in different time scales~\cite{}. 
% Yet, these approaches are still not able to capture the long-term dependency by design, which should be directly learned through end-to-end training. 
% However, none of these approaches is able to solve all three problems.

% \ifdefined\paratitle {\color{blue}
% [Our method; overall framework and how we address the problems in the prior work.] \\
% } \fi
% % In this work, we explore efficient Transformer architectures for generative modeling videos. 
% To address the aforementioned problems, we explore efficient Transformer architectures for end-to-end generative modeling of longer videos.
% Our method employs the bidrectional Transformers~\cite{} \jh{that predicts masked tokens from the remaining contexts. Our model projects the contexts into a fixed number of latent tokens and predicts the masked tokens based on the encoded latent tokens. By adopting the latent bottleneck, our model achieves linear complexity which allows learning the long-range dependency with end-to-end training of longer videos.}
% % What bidirectional transformers are doing is missing. (Masked Prediction)
% % observable and masked tokens are not introduced, but used for explanation.
% % Should summarize why it achieves linear complexity.
% % 단순 나열 금지.
% % Our method 
% % Our method is based on bidrectional Transformers~\cite{} that synthesize entire spatio-temporal volume of a video at once.


In this work, we propose an efficient transformer for video synthesis that can fully leverage the long-range dependency of video frames during training, while being able to achieve fast generation and robustness to error propagation.
We achieve the former with a linear complexity architecture that still imposes \emph{dense} dependencies across all timesteps, and the latter by removing autoregressive serialization through masked generation with a bidirectional transformer.
While conceptually simple, we show that efficient dense architecture and masked generation are highly complementary, and when combined together, lead to substantial improvements in modeling longer videos compared to previous works both in training and inference.
The contributions of this paper are as follows:
\begin{itemize}
    \item We propose Memory-efficient Bidirectional Transformer (MeBT) for generative modeling of video.
    Unlike prior methods, MeBT can directly learn long-range dependency from training videos while enjoying fast inference and robustness in error propagation.
    \item To train MeBT for moderately long videos, we propose a simple yet effective curriculum learning that guides the model to learn short- to long-term dependencies gradually over training.
    % \item \jw{As a training strategy for MeBT, we find that a simple curriculum learning scheme that guides the model from short-range to long-range dependency over training consistently improves model performance.}
    \item We evaluate MeBT on three challenging real-world video datasets.
    MeBT achieves a performance competitive to state-of-the-arts for short videos of 16 frames, and outperforms all for long videos of 128 frames while being considerably more efficient in memory and computation during training and inference.
    % Also, MeBT requires less memory for training and less time for generation in all setups.
\end{itemize}

\iffalse
[High-fidelity generation with transformers]
Recent literature shows that transformers applied as either autoregressive or non-autoregressive generative models to discrete latent code tokens are able to generate high-fidelity images and videos \ref{VQGAN, DALL-E, NUWA, MaskGIT, MMVID, CogView, CogView2, CogVid}.

[Heavy memory consumption lacks the modeling of long videos.]
Yet, the high computational requirements of standard transformer architecture prevent such an approach from scaling over moderately long sequences, especially during training.
Thus, current video transformers are enforced to be trained on short videos only, after which sliding windows or hierarchical dependency \ref{TATS, CogVid} must be applied to support the test-time generation of long videos.
This approach comes with an apparent drawback, as it cannot directly model long-term dynamics involving any pair of frames with a time gap longer than the temporal receptive field of the decoder.

[We propose our memory efficient video transformer.]
To overcome such issues and achieve the generation of long videos with better long-term dynamics, a method that can faithfully utilize long-term dynamics in training videos is essential.
Motivated by this, we develop a simple and efficient video transformer architecture that can consume long-term videos during training while being able to model dependency between frames placed arbitrarily far away in time.
Our architecture is based on asymmetric attention involving latent bottlenecks, which offers a high efficiency due to the linear cost of attention and reduced number of tokens.
Furthermore, we discard autoregression and adopt a masked modeling framework, which allows an extremely fast sampling and various applications compared to autoregressive modeling.
% \jhcmt{We also found that training the non-autoregressive model with long videos results in blurry generations and propose to guide the model with short-term dynamics to prevent blurry generations.}
While our design is focused on training and generation efficiency, we find our design choice empirically produces high-fidelity long-term videos through extensive experiments on three standard video benchmarks.
\fi