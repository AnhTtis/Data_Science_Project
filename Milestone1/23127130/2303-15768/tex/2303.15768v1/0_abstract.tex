
\begin{abstract}

Face swapping aims at injecting a source image's identity (i.e., facial features) into a target image, while strictly preserving the target's attributes, which are irrelevant to identity.
However, we observed that previous approaches still suffer from source attribute leakage, where the source image's attributes interfere with the target image's.
In this paper, we analyze the latent space of StyleGAN and find the adequate combination of the latents geared for face swapping task.
Based on the findings, we develop a simple yet robust face swapping model, \textbf{\ourmodel}, which is resistant to the potential source attribute leakage.
Moreover, we exploit the coordination of 3DMM's implicit and explicit information as a guidance to incorporate the structure of the source image and the precise pose of the target image.
Despite our method solely utilizing an image dataset without identity labels for training, our model has the capability to generate high-fidelity and temporally consistent videos.
Through extensive qualitative and quantitative evaluations, we demonstrate that our method shows significant improvements compared with the previous face swapping models in synthesizing both images and videos.

\end{abstract}


% 지금 까지 페아스 스왑하는 애들은 id-att tradeoff의 옵티말을 찾으려고 노력했지만,실패했다. -> 트레이드 옾이 만성적인 문제로 여겨져 왔지만 다들 성공하지 못했다.
% 우리 백본으로도 충분히 잘하지만, 더 디테일한 얼굴의 아이덴티티를 학습하기 위해서 3dmm의 무언가를 썼다.

% Face swapping's ultimate goal is injecting a source image's identity (i.e., facial features) to a target image, while strictly preserving the target's attribute which is irrelevant with identity.

% In face swapping, there is a chronic problem in that the source image's attributes interfere with the target image's, which is defined as source attribute leakage.
% However, previous methods still fail to overcome this problem.

% Based on the findings of the analysis, we develop a source attribute leakage-free face swapping model, \textbf{\ourmodel}, which is a simple yet robust.

% a source attribute leakage-free face swapping model, , which is simple yet robust.

% As opposed to most previous methods which rely on identity-labeled or video datasets, we propose a simple yet robust face swapping model without any those datasets thanks to our subspaces combination geared toward the face swapping task.
%with our simple and fine-grained model design 
% Although our face swapping model is built on the image-to-image scenario, our model shows high-fidelity and robust performances in video face swapping scenario, as well. 

% Our model outperforms previous state-of-the-art methods in both image and video scenarios, as indicated by extensive qualitative and quantitative evaluations.

% Extensive experiments show that our model is comparable or even superior to previous state-of-the-art methods with qualitatively and quantitatively, both in image and video scenarios.
% Through rigorous experimentation, our model has demonstrated comparable or even superior performance to prior state-of-the-art methods in both image and video contexts, as evidenced by qualitative and quantitative evaluations.
%, each from blendshape parameters and rendered mesh's landmark supervision. 
% This powerful information escorts the model to learn detailed source's identity and target's pose.
% The gist of our idea is mixing source's inner facial information from the 3D Morphable Models (3DMMs) and face-recognition models, and injecting these information to a truncated style-based generator (a.k.a., StyleGAN). 
% Specifically, with 3DMMs' UV map our model's back, our model can successfully disentangle source's identity, and target's expression and head-pose. 
% Unlike the previous studies, we exclude the source's contour shape which is the most critical piece that disturbing the stable and robust training. 
% Moreover, for disentangling the source's facial feature and others, we devise a novel 3DMM's rendered mesh-based partial landmarks loss.
% As opposed to previous methods which only utilize 3DMMs' semantic information and define source images' identity as ambiguous, we newly claims that 3DMMs' explicit information effectively assist the face swapping task, in respect of disentanglement for source facial features and target contour information.