\section{Related Work}

\noindent\textbf{Face Swapping.}
% todo 3 : 
There are numerous face swapping methods employing identity-labeled datasets. 
FaceShifter~\cite{faceshifter} designs its occlusion-aware architecture with two stages.
SimSwap~\cite{simswap} devises a robust method via weak feature-matching loss. 
InfoSwap~\cite{infoswap} utilizes the information-bottleneck principle for disentangling identity-attribute information. 
HifiFace~\cite{hififace} firstly exploits 3DMM's semantic information in face swapping.
StyleSwap~\cite{styleswap} uses simple modification of StyleGAN with the identity-labeled datasets for training. % , instead of employing the pre-trained StyleGAN.
% However, the difficulty in obtaining high-quality images with identity labels limits the applicability of these methods. 
However, the usability of these methods is restricted due to the challenge of obtaining high-quality images with identity labels or video datasets.
Moreover, they necessitate careful hyperparameter tuning to determine the appropriate ratio between theÂ same and cross-identity images. 
% On the other hand, \textbf{\ourmodel} is trained on a single and high-quality image dataset~\cite{progan} no need for searching the appropriate ratio.
In contrast, \textbf{\ourmodel} is trained on a high-quality image dataset~\cite{sg1}, eliminating the need for searching for the appropriate ratio.
To generate high-resolution images, recent face swapping approaches, such as MegaFS~\cite{megapixel}, FSLSD~\cite{fslsd}, and MFIM~\cite{mfim}, employ a pre-trained StyleGAN~\cite{sg2} as a strong prior.
% The other methods generate high-resolution images in face swapping by utilizing StyleGAN~\cite{sg2}.
% MegaFS~\cite{megapixel} firstly has leveraged pre-trained StyleGAN.
% FSLSD~\cite{fslsd} leverages the pre-trained StyleGAN, introducing the structure transfer method.
% MFIM replaces StyleGAN's noise maps as the target's spatial maps to preserve the attributes of the target images.
However, we discover that those methods based on the pre-trained StyleGAN fail to prevent \textbf{source attribute leakage} problem.
% However, they have shallow analyses of StyleGAN's conformity and struggle with \textbf{ source attribute leakage} problem.
Different from previous studies, we conduct a depth experiment to seek the face swapping adaptive latent space of StyleGAN and appropriate architecture.


\noindent\textbf{StyleGAN's Latent Space.} % taeu
StyleGANs~\cite{sg1, sg2, sg3} have shown remarkable success in generating realistic images. 
Following the success of the StyleGANs, the latent space of StyleGAN has been the subject of recent studies, with exploring various aspects of its properties and dynamics.
In the previous StyleGAN inversion studies~\cite{image2stylegan, image2stylegan++, psp, e4e}, they expand the $\mathcal{W}$ space to $\mathcal{W+}$ to amplify the StyleGAN's representation capacity.
Moreover, a previous study~\cite{oorinversion} proposes a method that maps images to an alternative latent space $\mathcal{F}/\mathcal{W+}$ in StyleGAN, which allows for more accurate reconstruction and semantic editing of out-of-range images with geometric transformations and local variations.
Also, numerous recent work~\cite{barbershop, styleyourhair, wang2022high, styleheat} utilize the latent feature map space $\mathcal{F}$, which is spatial-aware, to keep spatial information to be maintained while manipulating other traits.
They demonstrate the potential of the latent feature map space $\mathcal{F}$ in StyleGAN for a variety of image manipulation tasks.
Inspired by these findings and applications, we investigate the suitability of $\mathcal{F}/\mathcal{W+}$ for face swapping task and find which combination of the subspaces is proper in respective of face swapping.
To achieve this goal, we conduct a detailed experiment to explore the $\mathcal{F}/\mathcal{W+}$ space of StyleGAN, and analyze the subspaces to design a robust face swapping model. 


% supple
\begin{figure}[t!]
    \centering 
    \includegraphics[width=\linewidth]{figure/sg_an.png}
    \vspace{-0.2cm}
    \caption{\textbf{Analysis process of $\mathcal{F}/\mathcal{W+}$} with pre-trained StyleGAN; we generate \textit{random sampled} images with a fixed feature map $\mathbf{F}^{*}_{h \times w}$ and $\mathbf{w_{m+}}$, and an \textit{anchor} image is obtained from $\mathbf{w_{1+}}$.}
    \vspace{-0.65cm}
    \label{fig:short}
\end{figure} 

\noindent\textbf{3D Morphable Models.} 
A 3D morphable face model (3DMM)~\cite{bfm,flame,ls3dmm} is a strong representation for modeling human faces, including head pose, shape, and expression.
The 3DMM's shape is transformed into a PCA-based vector space, which can fit the human faces into the vector space.
Consequently, their corresponding encoders~\cite{deep3drecon,deca,ringnet} have came out to alleviate the time-consuming optimization. 
We utilize the 3DMM's shape parameter from the state-of-the-art~\cite{deca} 3DMM encoder, and corresponding decoder~\cite{flame} for our partial landmark loss.


% MFIM~\cite{mfim} utilizes the StyleGAN with the layer-wise modulation design in $\mathcal{S}$ space and replaces noise maps with the target image's encoded spatial maps to preserve the target's attributes. 
% However, MFIM's handcrafted architecture design makes the results bring the source image's attributes such as jaw structure, hairstyle, and eye gazing.
% Our method is different from previous methods in two respects. 
% First, as opposed to the previous methods~\cite{infoswap,faceshifter,simswap,hififace,styleswap} employing identity-labeled or video datasets, our model is trained on a single dataset~\cite{progan} with no training dataset ratio tuning. 
% It implies that we do not have to tune the training dataset ratio, which is a heuristic process.

% A 3D morphable face model (3DMM)~\cite{bfm,flame,ls3dmm} is a strong representation for modeling human faces which can describe head pose, expression, shape, lighting condition and texture.
% The 3DMM's shape and texture are transformed in to PCA-based vector space, which can fit the arbitrary human face into the vector space.
