\section{Method}

Given a source identity image $I_{src} \in \mathbb{R}^{H \times W \times 3}$ and target attribute image $I_{tgt} \in \mathbb{R}^{H \times W \times 3}$, our goal is to inject the identity of $I_{src}$ to $I_{tgt}$, while preserving the attribute of $I_{tgt}$ to synthesize the swapped image $\hat{I}$.
$H$ and $W$ indicate the height and width of the image, respectively. 
We explore latent subspaces $\mathcal{F/W+}$ of StyleGAN~\cite{sg2} to analyze the degree of variation in aspects of identity and attributes (Section~\ref{one}).
Through the analysis, we find the appropriate combination of latents, which can preserve the attribute of $I_{tgt}$, while reflecting the identity of $I_{src}$.
We introduce our face swapping model, \textbf{\ourmodel}, which is robust to the \textbf{source attribute leakage} (Section~\ref{two}).
Last but not least, we describe the objective functions for our method, including a novel partial landmark loss, which coordinates with 3DMM's implicit shape information (Section~\ref{three}).



\subsection{Exploring StyleGAN for Face Swapping.}\label{one}

In this section, we analyze the latent space of StyleGAN~\cite{sg2} from the perspective of developing the face swapping model.
Then, we justify the proper combination of latents for a \textbf{source attribute leakage}-free model.

\noindent\textbf{Revisiting Latent Space of StyleGAN.}
StyleGAN is a generative model that produces a high-resolution image $\hat{I} \in \mathbb{R}^{H \times W \times 3}$ using $n$ identical vector $\textbf{w} \in \mathcal{W} \subsetneq \mathbb{R}^{1 \times 512}$.
Recent work on GAN inversion~\cite{oorinversion,wang2022high} split the latent space of StyleGAN into two subspaces: the latent vector space $\mathcal{W+}$ and latent feature map space $\mathcal{F}$.
The extended latent vectors $\{w_1,w_2, \cdots, w_n\} \in \mathcal{W+} \subsetneq \mathbb{R}^{n \times 512}$, used for the different StyleGAN layers, allow StyleGAN to represent the diverse images and fine-grained control over the generated images. 
However, due to the deficient spatial information in $\mathcal{W+}$, it is difficult to reconstruct the structural details of images.
To address this problem, latent spatial feature map $\mathbf{F}_{h \times w} \in \mathbb{R}^{h \times w \times c}$, which is in $\mathcal{F}$, is used to represent the details of spatial information, where $h$, $w$, and $c$ are height, width, and channel dimension of the feature map, respectively.
With $\mathcal{W+}$ and $\mathcal{F}$, StyleGAN is reformulated as:
\begin{equation}
    \hat{I} = G(\mathbf{F}_{h \times w},\textbf{w}_{m+}),
\end{equation}
where $\textbf{w}_{m+}$ = $\{w_{m}, w_{m+1}, \cdots, w_{n}\}$$ \subset \mathcal{W+}.$
Note that the $\mathbf{F}_{h \times w}$ and $\textbf{w}_{m+}$ are complementary to each other.~\footnote{The $h \times w$ block maps ($\{{w_{m-2}, w_{m-1}}\}$, $\mathbf{F}_{h/2 \times w/2}$) to $\mathbf{F}_{h \times w}$. Please refer the Fig.~\ref{fig:short}. and Fig.~\ref{fig:model} (B)} % w_m 은 to rgb 에 쓰임
% More details are in supplementary materials.

\begin{figure}[t!]
    \centering 
    \includegraphics[width=.85\linewidth]{figure/figure-analysis-quantiative-1.png}
    \vspace{-0.3cm}
    \caption{\textbf{Quantitative analysis} between \textit{anchor} and \textit{random sampled}. 
    The larger $\mathbf{F}_{h \times w}^{*}$ results in improved preservation of expression, head pose, and eye gazing, while the identity undergoes less change. 
    Overall score is calculated by (ID sim)$^{3} *$ (HP dis) $*$ (Exp dis) $*$ (EG dis) which is standardized. Details for each score metric are described in the supplementary materials.}
    \vspace{-0.5cm}
    \label{fig:graph}
\end{figure}



Motivated by the advantages of $\mathcal{F}/\mathcal{W+}$, the following question arises: Is it appropriate to map the target spatial attribute to $\mathcal{F}$ while injecting source identity via $\mathcal{W+}$?
However, there is a lack of studies analyzing the suitability of the StyleGAN latent space for the face swapping.
Thus, we explore the latent space $\mathcal{F}/\mathcal{W+}$; the combination of $(\mathbf{F}_{h \times w},\textbf{w}_{m+})$ for building a face swapping model.



\noindent\textbf{Analysis on $\mathcal{F}/\mathcal{W+}$ for Face Swapping.}
% 그러면 target 의 정보를 f 로 map하고, source 의 identity 를 w 로 map 하는데, 어떤 조합이 적절한지 study 했다.
We study the profitable combination of $\mathbf{F}_{h \times w}$ containing spatial attributes of the target and $\textbf{w}_{m+}$ embedded the source identity from the perspective of face swapping task.
To achieve this goal, we conduct the following experiment. 
As shown in Fig.~\ref{fig:short}, we fix the $\mathbf{F}_{h \times w}$ (corresponds to target attributes) at the certain spatial resolution denoted as $\mathbf{F}^{*}_{h \times w}$, and generate images with randomly initialized $\textbf{w}_{m+}$ (corresponds to the identity of source).
Formally, it is denoted as:
\begin{equation}
    \hat{I}_{\textbf{w}_{m+}}=G(\mathbf{F}^{*}_{h \times w}, \textbf{w}_{m+}),
\end{equation}
where $\mathbf{F}^{*}_{h \times w}$ is generated from fixed vectors $\{w_1, \cdots, w_{m-1} \}$.
Then, we examine the generated image as gradually increasing the resolution of $\mathbf{F}^{*}_{h \times w}$ from $4\times 4$ to $512 \times 512$.

Now, we analyze quantitative factors to be considered in the face swapping task to find a suitable combination.
% 어떤 실험을 했냐...중요!
% 왜 identity? 왜 head pose / expression / eye gazing??
% 어떤 걸 분석해야하는지..! (face swapping task에서는 뭐가 중요하다..!)
% 그래서, 우리는 뭘 분석했다..! 뭘 measure함으로서
% Anchor Image = Target / Randomed Sampled vector = Source
As shown in Fig.~\ref{fig:graph}, when the resolution of feature map enlarges, identity similarity increases, and head pose, expression, and eye gazing discrepancy decrease between \textit{anchor} and \textit{random sampled} images.
We assume that the most adequate combination of $\mathbf{F}_{h \times w}$ and $\textbf{w}_{m+}$ for the robust face swapping should show low identity similarity, and head pose, expression, and eye gazing discrepancy between \textit{anchor} and \textit{random sampled}, since that combination can change identity with preserving the pose information. 

\begin{figure}[t!]
    \centering 
    \vspace{-0.4cm}
    \includegraphics[width=\linewidth]{figure/figure-analysis-qualitative-1.png}
    \caption{\textbf{Qualitative analysis.} Examples from the analysis on the latent space for face swapping. An \textit{anchor} image is obtained from the inverted vectors $\textbf{w}_{1+}$ by using GAN inversion method~\cite{psp}. \textit{Random sampled} images of (A) are generated by the fixed feature map $\mathbf{F}^{*}_{16 \times 16}$ and randomly initialized $\mathbf{w}_{6+}$. 
    (B)'s \textit{random sampled} images are produced by $\mathbf{F}^{*}_{32 \times 32}$ and $\mathbf{w}_{8+}$.
    \textit{Random sampled} images of (C) are obtained from $\mathbf{F}^{*}_{64 \times 64}$ and $\mathbf{w}_{10+}$.
    }
    \vspace{-0.6cm}
    \label{fig:toy}
\end{figure}

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{figure/model_final.png}  
    \vspace{-0.7cm}
    \caption{(A) Our \textbf{\ourmodel} architecture; the blurred trapezoidal box is the area of the discarded block of StyleGAN. The target encoder $E_{t}$ encodes $I_{tgt}^{\downarrow}$ to $\mathbf{F}_{32 \times 32}$. The encoded $\textbf{w}_{8+}$ from the two source encoder $E_{i}$ and $E_{s}$ is injected to StyleGAN $G$. (B) illustrates the details of 64 x 64 Block. It produces $\mathbf{F}_{64 \times 64}$ from $\{\textbf{w}_{8}, \textbf{w}_{9}\}$ and $\mathbf{F}_{32 \times 32}$. (C) is the construction pipeline of ground-truth for partial landmark loss. More details are described in our supplementary materials. }
    \vspace{-0.5cm}
\label{fig:model}
\end{figure*}

To observe the three highest overall scored $\mathbf{F}^{*}_{h \times w}$, from 16 to 64, we visualize the \textit{anchor} and \textit{random sampled} in Fig.~\ref{fig:toy}. 
(A) varies a lot of attributes like expression and eye gazing, and (C) does not vary except for lighting conditions, skin, and background colors.
However, (B) varies inner facial parts, while the pose, eye gazing, and expression are similar to the \textit{anchor} image's.
% However, (B) varies inner facial parts, while the pose, eye gazing, and expression are unchanged compared to the anchor image.

Therefore, we select the combination of $(\mathbf{F}_{32 \times 32}, \textbf{w}_{8+})$ since it effectively preserves the pose, eye gazing, and expression while changing identity relevant features such as eyes, nose, lip, and eyebrows.
It implies that as long as we utilize $\mathbf{F}_{32 \times 32}$ and $\textbf{w}_{8+}$, the source identity is well-reflected, minimizing damage to the target attributes.



\subsection{\textbf{\ourmodel}: Simple yet Robust Architecture for Face Swapping.}\label{two}
% \noindent\textbf{
As shown in Fig.~\ref{fig:model}, we utilize pre-trained StyleGAN without any architectural modification since $(\mathbf{F}_{32 \times 32}$, $\textbf{w}_{8+})$ preserve the target attributes while switching the identity. 
% todo : 두개의 문장으로 나눠서 얘기.

\noindent\textbf{Target Attributes Encoder.} % naming 은 다시
Our generation pipeline starts from $\mathbf{F}_{32 \times 32}$, which is encoded from $I_{tgt}$. 
To directly map the spatial information of $I_{tgt}$ to the $\mathbf{F}_{32 \times 32}$, we design a simple convolution target encoder $E_{t}$.
The $4 \times$ down-sampled $I_{tgt}^{\downarrow}$ is fed to $E_{t}$, then encoded features $\mathbf{F}_{32 \times 32}=E_{t}(I_{tgt}^{\downarrow})$ are conveyed to the StyleGAN $G$.

\noindent\textbf{Source Identity Encoder.}
Since $\textbf{w}_{8+}$ has the potential of injecting the source's identity information into the target without damaging the target's attributes, we map $I_{src}$ to the source identity embedding $\textbf{w}_{id}^{+} = E_{i}(I_{src}^{\downarrow})$ by using the source identity encoder $E_{i}$. % \in \mathbb{R}^{11 \times 512}
Here, we utilize pSp encoder~\cite{psp} as the source identity encoder $E_{i}$ to map the overall source's identity attributes to $\mathcal{W+}$ space.
% However, we assume that the only 11 $\textbf{w}$ vectors is insufficient to contain source identity attributes. % 여기에 대한 근거는 ? 다른 베이스라인들은 더 많은 source vector 들이나 s space 등을 이용한다. 근데 굳이 언급할 필요?

\noindent\textbf{Shape-Guided Identity Injection.}
Additionally, we exploit the 3DMM parameter space to focus on the source's structural information. 
To be specific, we leverage the 3DMM's shape parameter extracted from shape encoder $E_{s}$ which is a state-of-the-art 3DMM encoder~\cite{deca}.

Here, we only utilize the shape parameter, since the $G$ already has the capability of preserving the target image's poses by employing the $\textbf{F}_{32 \times 32}$.
Then, a mapping network $M: \mathcal{A} \rightarrow \mathcal{W}+$ produces $\mathbf{w}^{+}_{shape}$ with 3DMM's shape parameter $\alpha \in \mathcal{A}$.
\begin{equation}
    \mathbf{w}^{+}_{shape}=M(\alpha)=M(E_{s}(I_{src})).
\end{equation}
Finally, $\textbf{w}_{8+}$ is constructed with summation of shape embedding $\textbf{w}_{shape}^{+}$ and identity embedding $\textbf{w}_{id}^{+}$. 
Formally, 
\begin{equation}    
    \mathbf{w}_{8+}=\mathbf{w}_{shape}^{+}+\mathbf{w}_{id}^{+},
\end{equation}
where $\textbf{w}_{shape}^{+}$ is broadcast with the same size as $\textbf{w}_{id}^{+}$.

To summarize, our pipeline is described as 
\begin{equation}
    \hat{I}=G(\mathbf{F}_{32 \times 32},\textbf{w}_{8+}).
\label{eq:full}
\end{equation} 
% joining with $\mathbf{F}_{32 \times 32}$ and $\textbf{w}_{8+}$



\input{table/baseline.tex}

\subsection{Objective Functions}\label{three}
% Our training objectives are four parts including our novel partial landmark loss.

\noindent\textbf{Partial Landmark Loss.}
To encourage cooperation of the 3DMM's implicit and explicit information, we propose a partial landmark loss, only focusing on designated 51 landmarks out of 68 which supervise the source's inner facial shape. 
Moreover, such supervision also guides to more precise expression and head pose. 
To construct the ground truth of partial landmarks, we mix the target image's head pose and expression parameters and the source image's shape parameter and then feed the mixed parameters to the 3DMM decoder (\textit{i.e.}, FLAME~\cite{flame}) reconstructing the mesh, $Mesh^{mix}$, which is composed of 5023 vertices. 
More details are described in the supplementary materials. 
% In Fig.~\ref{fig:shape}, the reconstruction pipeline is illustrated.

\begin{equation}
    \mathcal{L}_{pl} =\sum_{(i,j) \in Lmk} \norm{Mesh_{i}^{mix}-Mesh_{j}^{swap}},
    \label{formula:pl}
\end{equation}
where $Lmk$ is the set of inner face landmark pairs, $Mesh^{swap}$ represents swapped image's extracted mesh from $E_{s}$ and 3DMM decoder.


\noindent\textbf{Reconstruction Loss.}
We adopt the reconstruction loss for regularizing $\hat{I}$ attributes with $I_{tgt}$.
This part is composed of two losses, $L_{l2}$ and LPIPS~\cite{lpips} loss.
\begin{equation}
    \mathcal{L}_{recon}=\rVert{I_{tgt}-\hat{I}}_{1}\rVert_{2}+LPIPS(I_{tgt}, \hat{I})
\end{equation}

\noindent\textbf{Identity Loss.}
Identity loss ensures the $\hat{I}$ to have the same identity with $I_{src}$. 
% is essential for the face swapping task. This
\begin{equation}
    \mathcal{L}_{id}=1-\textrm{cossim}(R(I_{src}), R(\hat{I})),
\end{equation}
where $R$ is the pretrained face recognition model, ArcFace~\cite{arcface}. 
The notation cossim$(\cdot,\cdot)$ represents the cosine similarity between the ArcFace's embeddings.

\noindent\textbf{Adversarial Loss.}
Adversarial loss makes the model to generate the realistic $\hat{I}$.
We directly use the StyleGAN~\cite{sg2}'s non-saturating adversarial loss, $\mathcal{L}_{adv}$.
The detailed description is in supplementary materials.


\noindent\textbf{Total Objective.} 
\textbf{\ourmodel} is trained with the following total objective function: 
\begin{equation}
    \mathcal{L}_{total} = \lambda_{pl}\mathcal{L}_{pl} + \lambda_{recon}\mathcal{L}_{recon} + \lambda_{id}\mathcal{L}_{id} + \lambda_{adv}\mathcal{L}_{adv}, % 순서 체크하기
\end{equation}
where $\lambda_{pl}, \lambda_{recon}$, $\lambda_{id}$ and $\lambda_{adv}$ are the hyper-parameters. 
% controlling relative importance between different losses.


% todo : where MLP 어쩌고 저쩌고 써주기
% M(\alpha) 이부분 좀 뺌
% Our source encoder $E_{src}$ is decomposed into two components, a source identity encoder $E_{i}$ and a pretrained 3DMM shape encoder $E_{s}$. 
% With a source image $I_{s}$, $E_{src}$ produces $\textbf{w}_{id}^{+} \in \mathbb{R}^{11 \times 512}$ and $\textbf{w}_{shape}^{+} \in \mathbb{R}^{1 \times 512}$.
% The source identity encoder $E_{i}$ is based on the Feature Pyramid Network (FPN), which can insert style vectors extracted from the source's different spatial information. 
% The shape encoder $E_{s}$ can make the $G_{mod}$ learn the source's PCA-based 3D shape information. 
% % E_s는 어떤 구조이고, M mapper가 따라붙는다. -> 어떤걸 예측한다.
% % 이 역할을 한다??
% That means the source's identity and 3D shape information are added and injected into the $G_{mod}$, combining with the target encoded feature to output the high-fidelity swapped image. 
% First, $E_{id}$ outputs $\textbf{w}_{id}^{+} \in \mathbb{R}^{11 \times 512}$. 
% Second, $E_{shape}$ takes the $x_{src}$, and then pass to the MLP-based mapper $M$ which maps the 3DMM's shape parameter to the latent subspace $\mathcal{W}_{t}^{+} \subsetneq \mathbb{R}^{11 \times 512}$ to produce $\textbf{w}_{shape}^{+} \in \mathbb{R}^{11 \times 512}$. 

%$\hat{I}_{\textbf{w}_{6+}}=G(\mathbf{F}^{*}_{16 \times 16}, \textbf{w}_{6+})$, where $\mathbf{F}^{*}_{16 \times 16}$ comes from the some of the inverted vectors $\{w^{'}_{1},w^{'}_{2},...w^{'}_{5}\}$, (B) is  $\hat{I}_{\textbf{w}_{8+}}$ obtained from  $\mathbf{F}^{*}_{32 \times 32}$ and $ \textbf{w}_{8+}$. 
    %(C) is $\hat{I}_{\textbf{w}_{10+}}$ produced by $\mathbf{F}^{*}_{64 \times 64}$ and $\textbf{w}_{10+}$. 


% Moreover, HifiFace directly injects the texture and 3DMM embeddings with concatenation to their generator in low-dimension.
% We map blendshape parameter to $\mathcal{W+}$ space by the mapping network $M$, similar to the previous studies~\cite{pirenderer,pie,stylerig}.
% Whereas, we choose the injection manner with mapping the blendshape parameter to higher dimension by the mapping network $M$, following previous StyleGAN inversion studies~\cite{pirenderer,pie,stylerig}. 
% Namely, $M: \mathcal{A} \rightarrow \mathcal{W}+$ to produce the $\mathbf{w}^{+}_{shape}$ from the 3DMM's shape parameter $\alpha \in \mathcal{A}$

% 아래는 음.. 굳이 있어야 할까 싶음, 필요하면 supple 로
% =======================
% The 3DMM's 3D face shape $\mathbf{S}$ is parameterized as
% \begin{equation}
%     \mathbf{S}=\bar{\mathbf{S}}+\alpha \mathbf{B}_{shape} + \beta \mathbf{B}_{exp}
% \end{equation}
% where $\bar{\mathbf{S}}$ is the mean face shape, $\mathbf{B}_{shape}$ and $\mathbf{B}_{exp}$ are the bases of shape and expression from Principal Component Analysis (PCA) of scanned human faces, $\alpha$ and $\beta$ are each coefficients. 
% We only leverage the shape coefficient $\alpha$. %
% =======================
% 이 아래 부분은 일단 생략
% The source image $I_{src}$ is fed into two separated encoders, the texture encoder $E_{tex}$ and off-the-shelf 3DMM~\cite{deca} encoder $E_{3D}$ with a mapping network $M$, which transforms the blendshape space to StyleGAN's $\mathcal{W+}$ space.
% \begin{equation}
%     \mathbf{w}^{+}_{tex}=E_{tex}(I_{src})
% \end{equation}
% where $\mathbf{w}^{+}_{tex}$ is encoded source image's texture embedding.
% $E_{tex}$ is a Feature Pyramid Network (FPN)-based network which can embed the spatial-wise texture information of $I_{src}$. 
% ==================== 
% Different from HifiFace~\cite{hififace},

% where the $\lambda$s are loss coefficients.

%============================================
%We set our experiment as shifting the fixed feature map resolution from low to high, freezing specific feature map step-by-step by increasing the resolution of feature map. % 이거를 좀더 잘 풀이해서 쓰고
% By fixing the corresponding vectors to yield the fixed feature map, we can observe that 
%"which properties are preserved in the feature map corresponding to the specific resolution of StyleGAN layer?", while changing other block's vector.
%Specifically, as shown in Fig.~\ref{fig:short}, we select an \textit{anchor} image utilizing a state-of-the-art StyleGAN inversion encoder~\cite{psp,e4e}, then fix the corresponding vectors while changing the other random sampled vectors related with higher resolution.
%Consequently, we compare these random sampled images with the \textit{anchor} image.
% For example, let's assume that we fix the $8 \times 8$ resolution constant.
% \begin{itemize}
%     % we extract W+ vectors from an anchor by using the existing gan inversion method ~\pSp, e4e.
%     \item First, we extract vectors $\textbf{w}^{+}$ from an \textit{anchor} image by using the existing StyleGAN inversion model~\cite{psp,e4e}
%     \item Second, we forward first three vectors to generate an $8 \times 8$ constant. % 8 x 8 constant
%     \item Third, we generate other random images changing the last 15 vectors with the fixed constant. % with the fixed constant, 
%     \item Last, we analyze these images with the \textit{anchor} image with pose, expression discrepancy and identity similarity evaluation.
% \end{itemize}
% sub-figure 추가 이 피규어는 위 프로세스를 설명한다.
%Fig.~\ref{fig:short} also illustrate the above process.
% todo : 실험에 대한 보충 설명 기재

% In conclusion, the swapped image is derived as Eq.~\ref{eq:full}.
% generator??
% E_trt 한번 더 간단히 얘기
% I = G_mod(F_32x32, W+sub)
% G_mod 를 통해 최종적인 결과물이 어떻게 생성된다.
% Target Attribute를 잘 유지하면서, Identity를 잘 바꾼다.
% F_32x32 -> 앞에서 finding에서 기반해서 이런 architecture 디자인했다...
% exploring architecture (3.1) 과 forward(3.2) 하는 부분을 좀 더 구분지어주면 좋을듯 
% Unfortunately : 3.3 에 구체적으로


% As mentioned earlier, we use the pretrained 3DMM encoder $E_{shape}$ for utilizing the 3D shape information. 
% We select this network as DECA~\cite{deca} the state-of-the-art 3DMM encoder.  

% 왜 안쪽만 해야하는가?

% Unfortunately, the shape parameter contains redundant face contour information, which hinders our generator from synthesizing the swapped image that we intended. 
% In Fig.~\ref{fig:shape}, we render the source shape, and target head-pose and expression mixed 3D mesh with the landmarks. Note that the blue points are the swapped image's ground truth inner face landmarks, yellows are the contour landmarks which is out of our interests.
% As can be seen, the rendered mesh follows the source's jaw contour. This conflicts with the designed the modified StyleGAN which preserve the target's contour of face as we intend.



% \begin{figure}
%     \centering 
%     \includegraphics[width=\linewidth]{figure/figure-3d-loss.png}
%     \caption{Rendering process for constructing ground truth of partial landmark loss.}
% \label{fig:shape}
% \end{figure}

% todo : loss 배치는 좀 생각해보기

% Note that our model outputs the well-aligned image following the FFHQ~\cite{sg1}'s image cropping and alignment pipeline. Therefore, we can assume that a single orthographic camera. The partial landmarks loss can cut-off the contour shape of source shape parameter.
% Standing alone with our model \textbf{\ourmodel} can robustly synthesize the swapped image excluding the \textbf{source attribute leakage}. We only use a high-quality image dataset~\cite{sg1} without any id-labeled or video datasets.
