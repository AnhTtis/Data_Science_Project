\vspace{-0.5cm}
\section{Introduction}
\label{sec:intro}

Face swapping has become a prominent task with various applications such as digital resurrection, virtual human avatars, and movie films.
The goal of face swapping is to inject a source's identity (\textit{e.g.}, eyes, nose, lips, and eyebrows) into a target, while strictly preserving the target's attributes (\textit{e.g.}, hair, background, light condition, expression, head pose, and eye gazing), which are irrelevant to identity.
Due to the notorious intractability of protecting the target person's attributes against potential interference by the source person's attributes, previous research has endeavored to overcome this challenge.
Two primary categories of face swapping approaches exist.

In one approach to face swapping, the reconstruction loss between the swapped and target images is employed when the source and target images share the same identity.~\cite{simswap,hififace,faceshifter,styleswap,fslsd,uniswap}.
However, applying the reconstruction loss in certain scenarios necessitates the use of identity-labeled image datasets~\cite{vggface1,vggface2} or video datasets~\cite{voxceleb1,voxceleb2}.
Unfortunately, it is challenging to obtain high-quality images with identity labels, hence limiting the applicability of these methods.
Moreover, these methods require careful hyperparameter tuning to determine the appropriate ratio between the same and cross-identity images.

To synthesize high-resolution images, the other approaches utilize a pre-trained StyleGAN model as a strong prior with layer-wise information injection~\cite{megapixel,mfim,fslsd}.
Despite the power of the pre-trained StyleGAN, MegaFS~\cite{megapixel} and FSLSD~\cite{fslsd} often fail to preserve the target person's attributes.
This issue stems from utilizing solely $\mathcal{W+}$ space for assembling the latent codes in StyleGAN from the source and target images.
To preserve the target person's attributes, MFIM~\cite{mfim} replaces the spatial noise maps of StyleGAN with the spatially-dimensioned feature maps of the target image. 
However, we found that their empirically designed architecture still induces low-fidelity results that are affected by the source person's attributes, such as the source person's hair and eyeglasses.

Although previous studies struggle to balance the information between the source and target images, they are still vulnerable to \textbf{source attribute leakage} problem, defined as \textit{source person's identity irrelevant information leaking to the target person's image}.
For example, as shown in the first row of Fig.~\ref{fig:leakage}, the existing face swapping methods often bring the source image's appearance to the target image, such as hair and skin color, which is defined as \textit{appearance leakage}. 
In the second row of Fig.~\ref{fig:leakage}, the source's pose (\textit{e.g.}, head pose, expression, and eye gazing) interferes with the target's pose, which is defined as \textit{pose leakage}.

To solve these \textbf{source attribute leakages}, we thoughtfully design a simple yet robust face swapping model called \textbf{\ourmodel}, which employs a pre-trained StyleGAN~\cite{sg2}.
Behind our model, we explore StyleGAN's latent space $\mathcal{F}/\mathcal{W+}$ to find the promising combination of latents in the subspaces for preventing \textbf{source attribute leakage}.
In specific, we investigate the suitable latents by assessing the extent to which the target's pose can be changed at each combination of latents in subspaces.
Armed with the investigation, we elaborately design a face swapping model, which is robust to preserving the target image's attributes, while effectively reflecting the source image's identity.
  
\begin{figure}[t!]
    \centering 
    \includegraphics[width=\linewidth]{figure/leakage_namechange.png}
    \caption{\textbf{Examples of source attribute leakage} and our improved results; In the first row, FSLSD~\cite{fslsd} often fails to preserve the skin color and lighting condition of the target image. MFIM~\cite{mfim} brings hairstyle from the source; In the second row, FSLSD~\cite{fslsd} and MFIM~\cite{mfim} hardly preserve the target image's pose such as eye gazing and expression. Besides, our result has no artifacts like those.
    \textcolor{Dandelion}{Yellow} boxes indicate the \textit{appearance leakage}.
    \textcolor{red}{Red} boxes indicate the \textit{pose leakage}.}
    \vspace{-0.5cm}
    \label{fig:leakage}
\end{figure}

To impose the detailed face shape information of the source image, our model takes the source's shape parameter of 3D Morphable Model (3DMM)~\cite{flame,bfm,ls3dmm} as the input.
% we utilize the shape parameter of 3D Morphable Model (3DMM)~\cite{flame,bfm,ls3dmm} as auxiliary identity guidance input. % 모델에 input으로 들어간다~!
% 모델에 condition 주입하는 것에 더해서, 우리는 objective function으로 loss를 제안한다.
In addition to inject the shape parameters into the model, we introduce a novel partial landmark loss, which is effective to retain the head pose and expression of the target image, while injecting the inner facial geometry of the source image.
% Moreover, we propose a rendered mesh's partial landmark loss in that the source image supervises the swapped image's inner facial geometry intimately related to the human face's identity.
% This rendering-based partial landmark loss can assist the model in retaining the target image's pose, such as head pose and expression.
Thanks to our well-designed simple architecture and the coordination of the 3DMM information, \textbf{\ourmodel} is secured from the \textbf{source attribute leakage} and injects the more abundant identity information.
Moreover, \textbf{\ourmodel} is built on megapixels (\textit{e.g.}, 1024 $\times$ 1024), which is practical and applicable in various applications.
% \textbf{\ourmodel} is also built on megapixels which is a common fashion in face swapping~\cite{mfim,fslsd,megapixel}.

In summary, our contributions are three-fold.
% contribution 표현 한 번 같이 다듬기, 좀 더 간결하게, 
\begin{itemize}
    \item Based on the analysis of StyleGAN latent space, we introduce \textbf{\ourmodel}, preserving target attributes while preventing the \textbf{source attribute leakage}. 
    \item For casting detailed source identity information and precise target's pose, we propose a shape-guided identity condition and a partial landmark loss with 3DMM. 
    % This cooperation makes \textbf{\ourmodel} to be escorted only to consider the source's inner facial shape consistently. 
    % \item For casting more detailed source identity information and precise target's pose, we jointly utilize the 3DMM's blendshape parameters and rendered-mesh's partial landmarks.
    % This cooperation makes \textbf{\ourmodel} to be escorted only to consider the source's inner facial shape consistently.
    \item Extensive experiments demonstrate that \textbf{\ourmodel} outperforms previous approaches quantitatively and qualitatively. 
    Moreover, \textbf{\ourmodel} can produce high-quality videos without training on video datasets. % exhibits robustness to \textbf{source attribute leakage} and
    % any temporal coherency modules or
    % todo : Megapixel output 강조, 각 bullet point 별로 3줄 이내, 총 12줄 이내로 compact 하게 작성
\end{itemize}



% simswap: vggface쓰면서 id같을떄만 recon loss를 줌
% faceshifter: vgg+celeb+ffhq 다 쓰면서 이미지 같을떄만
% hififace: vggface, asianceleb, same id일떄
% styleswap: vggface + voxceleb
% FSLSD: ffhq same image
% one shot mega는 아님
% mfim/smooth swap/ 은 이런거 안쓰긴했는데 그래서 많이 바뀜


% video 관련된 metric 측면에서 다른 모델이 가지는 문제를 지적
% video 를 타겟팅한 무언가의 장치가 없이 강조를 하기에는 좀 무리가 있음. -> video 를 처음부터 나오는것은 어색하고, source leakage 를 얘기하고 그다음 video 에서 더 안좋은 결과를 야기할 수 있다. 이런식으로 작성해야 자연스럽다.
% [3DMM] inner face 가 id를 많이 결정짓기 때문에 우리는 거기에만 집중하려고 3dmm shape parameter를 썼다.
% 이 방법론은 앞으로 다른 모델들이 face swap model 을 개발할때 좋은 출발점, insight 를 개선해줄 수 있어야할 것 같다.
% 목표 video dataset 에 robust 한 model 만들기 (다른 모델들이 가지는 단점)
% 많은 face swap model 들 그대로 쓰고
% One related work : video dataset 쓴 애들인데,
% 마지막에, 하지만 여전히 video dataset 에 inference 를 하면, inconsistent 한 결과를 보인다. (flickering, temporal consistent, 등등)
% 한편으로 StyleGAN 을 활용한 연구들이 있는데, 고해상도 이미지를 만들고, identity 를 더 잘 변환시킨 연구들이 있지만, video dataset 에서 inference 를 해보면 여전히 shit 이다.
% 따라서 기존 연구들이 video dataset 에서 어떻게 하면 robust 한 결과를 보일 수 있을지를 잘 다룬 연구들이 부족하다고 생각했다.
% 그래서 우리는 video dataset 에서도 견고한 성능을 낼 수 있을지 고민을 했다.
% 그러면 어떤 부분을 잡아야 video dataset 에서 견고한 성능을 낼 수 있는지를 좀 정의를 하고 (id -irrelevant 한 정보는 최대한 유지, 그러면서도 그 정보에 source id-irrelevant 한 attirbute가 leakage 되지 않는 것이 중요하다는 것. 
% 그러던 와중 StyleGAN 의 잘 알려진 사실들이 눈에 들어왔고,
% 그거에 영감을 받아 트렁케이션 실험을 진행했고, video dataset 에서 robust 함을 살릴 수 있는 방법들이 있을 수 있다는 것을 밝혀냈다. 
% 하지만 이것만으로 좀 부족하다. (ex, eye gazing)
% 그래서 3DMM 파라미터 인풋 및 loss 활용을 통해 더 consistent 한 결과를 낼 수 있게 디자인 했다.
% 마침내 우리는 video dataset 에서 견고한 모델을 만들 수 있었다. 아주 심플하지만 잘 짜여진 아키텍쳐와 비디오 데이터셋 없이도.
% Face Swap Task 정의