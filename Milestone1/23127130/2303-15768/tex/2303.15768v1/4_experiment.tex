\section{Experiments}


\begin{figure*}[h!]
    \centering 
    \includegraphics[width=0.9\linewidth]{figure/figure-ff++.png}
    \vspace{-0.35cm}
    \caption{\textbf{Qualitative results} on $256 \times 256$ resolution FF++; (A) Source, (B) Target, \textbf{(C) \ourmodel}, (D) MFIM, (E) FSLSD, (F) MegaFS, (G) HifiFace, (H) InfoSwap, and (I) SimSwap. More results are in our supplementary materials. } % todo : A, B, C 다 설명
    \vspace{-0.4cm}
\label{fig:FF++}
\end{figure*}


\begin{figure*}[t!]
    \centering 
    \includegraphics[width=0.9\textwidth]{figure/figure-mega.png} % mega_comp_0306
    \vspace{-0.2cm}
    \caption{\textbf{Qualitative results} on $1024 \times 1024$ resolution CelebA-HQ with megapixel baselines.} % source에 안경 낀 sample and Source 앞머리 있는 sample 있으면 좋을듯 (MFIM 까기 위해) -> Source attribute leakage 문제 할때 비교해서 보여줘도 괜찮고
    \vspace{-0.6cm}
\label{fig:celeba}
\end{figure*}

\noindent\textbf{Datasets.} 
We train our model only on the FFHQ~\cite{sg1} without any identity-labeled or video datasets, different from previous methods~\cite{infoswap,styleswap,simswap,hififace}. 
We evaluate our method on CelebA-HQ~\cite{progan} validation set and FaceForensics++ (FF++)~\cite{ff++} dataset.
For CelebA-HQ, we sample 10,000 pairs randomly for the source and target images.
For FF++, we randomly select video pairs for qualitative evaluation. 
Note that the quantitative evaluations are only with CelebA-HQ, a high-resolution image dataset. 
% CelebV-HQ

\noindent\textbf{Baselines.} 
We compare our methods with the following face swapping baselines: SimSwap~\cite{simswap} InfoSwap~\cite{infoswap}, HifiFace~\cite{hififace}, MegaFS~\cite{megapixel}, FSLSD~\cite{fslsd} and MFIM~\cite{mfim}.
% HifiFace and Simswap are based on self-designed architectures; 
% InfoSwap leverage the pre-trained face recognition model~\cite{arcface} with interleaving layers; 
% MegaFS and FSLSD utilize the freezed StyleGAN;
% MFIM leverages the unfreezed StyleGAN generator and pSp encoder~\cite{psp}. 
We utilize an unofficial code for HifiFace and reimplement the MFIM, strictly following the original paper.
% with the authors' advice.


\noindent\textbf{Implementation Details.} 
Our model is trained with 8 batch size on a NVIDIA A100 GPU for megapixels about 5 days. 
We use ADAM optimizer with a learning rate $1 \times 10^{-4}$. 
$\lambda_{id}$ and $\lambda_{rec}$ are set to 1.
$\lambda_{pl}$ is set to 100. $\lambda_{adv}$ is set to $10^{-2}$.


\subsection{Quantitative Evaluation}
% todo : check
\noindent\textbf{Evaluation Metrics.} % Metric detail 은 분량조절시 supple 로 % textbf 쓰니까 너무 좀 조잡해보임
% We leverage CelebA-HQ~\cite{sg1} for quantitative evaluation.
% Our quantitative evaluation is conducted on CelebA-HQ~\cite{sg1}.
For quantitative evaluation, the source and target image pairs are randomly sampled without duplication from CelebA-HQ~\cite{progan}.
% We randomly sampled the source and target image pairs without any duplication.
We measure five metrics widely used for evaluating face swapping methods: Identity, Expression, Head Pose, Head Pose-HN, and Frechet Inception Distance (FID)~\cite{fid}.
In addition to five metrics, we employ two new metrics: \textbf{Masked-L1} and \textbf{Eye gazing}.
% We utilize five common metrics in face swapping and propose two new metrics, \textbf{Masked-L1} and \textbf{Eye gazing}. 
% This metric is devised to measure the \textbf{source attribute leakage} of appearances such as hair, glasses, and skin color.
% Moreover, to evaluate the eye gazing of the $\hat{I}$, we utilize the state-of-the-art eye gazing estimator~\cite{rtgene}.
% These two new metrics' specification is described in supplementary materials.
Identity score is the cosine similarity between the embedding vectors of $I_{src}$ and $\hat{I}$ extracted by a pre-trained face recognition model~\cite{cosface}, where we utilize a different model from the model used for the identity objective function.

Expression and Head Pose scores are calculated by measuring $L1$ distance between expression and head pose blendshape parameters of $I_{tgt}$ and $\hat{I}$ extracted by another pre-trained 3DMM encoder~\cite{ringnet}.
% Expression and Head Pose are evaluated by expression and head pose blendshape parameters' $L1$ distance by using another pre-trained 3DMM encoder's~\cite{ringnet}. 
We measure Head Pose-HopeNet (HN) score by computing $L1$ distance between $I_{tgt}$ and $\hat{I}$ using a pre-trained head pose estimator~\cite{hopeneet}.
% Head Pose-HN is measured by $L1$ distance of a pre-trained head pose estimator~\cite{hopeneet}.
We also measure FID for the 10,000 $\hat{I}$ and real images of CelebA-HQ. 
% The FID score measures how similar the distributions of the synthesized images and the real images are
% todo : FID 측정 법 쓰기, 10,000 pair real sample 10,000 pair
\textbf{Masked-L1} measures the difference of the skin and head area excluding identity attributes between $\hat{I}$ and $I_{tgt}$. % todo : 좀 더 매끄럽게
Specifically, we utilize a pre-trained face parsing map predictor~\cite{bisenet} for \textbf{Masked-L1} to extract only the skin and hair area of $I_{tgt}$ and $\hat{I}$.
Then, we measure the $L1$ distance for the pixels of the designated area. 
We employ \textbf{Masked-L1} to measure the \textbf{source attribute leakage} of appearances such as hair, glasses, and skin color.
Moreover, we utilize the pre-trained eye gazing estimator~\cite{rtgene} to evaluate the eye gazing of the swapped image $\hat{I}$.
In specific, we compute the $L1$ distance between eye gazing angles (\textit{e.g.}, yaw and pitch) of $I_{tgt}$ and $\hat{I}$.
The details for these two metrics are described in supplementary materials.

\input{table/user.tex}

\input{table/resolution_ablation.tex}

% The \textbf{Masked-L1} utilizes a pre-trained face parsing map predictor~\cite{bisenet}, which extracts only the skin and hair area of $I_{tgt}$ and $\hat{I}$. 
% The \textbf{Eye gazing} measures the $L1$ distance between the $I_{tgt}$ and $\hat{I}$'s eye gazing angles such as yaw and pitch.

\noindent\textbf{Comparison with Baselines.} 
% todo : 문장 어색한 거 수정
Table~\ref{table:baselines} reports the quantitative comparison with baselines and \textbf{\ourmodel}.
% In Table.~\ref{table:baselines}, the quantitative comparison with baselines, and \textbf{\ourmodel} is outlined. 
\textbf{\ourmodel} model achieves the state-of-the-art performance compared to other face swapping baselines, except for the Identity score.
% Except for the Identity score, our \textbf{\ourmodel} model outperforms other methods. 
Although MFIM shows the best Identity score, the synthesized images of MFIM show severe \textbf{source attribute leakage} such as vanished hair and incorrect eye gazing and expression as shown in Fig.~\ref{fig:leakage}.
% For Identity, MFIM~\cite{mfim} gets the best score, but their results show the severe \textbf{source attribute leakage} such as vanished hair and incorrect eye gazing and expression, already shown in Fig.~\ref{fig:leakage}.




\noindent\textbf{Ablation Studies.}
As shown in Table~\ref{table:res_able}, we compare the performances of our model across different resolutions of $\mathbf{F}_{h \times w}$ (\textit{i.e.}, from $8 \times 8$ to $64 \times 64$).
Considering \textit{1st} to \textit{4th} row of our methods, there is the same tendency in Sec.~\ref{one} that the larger resolution of $\mathbf{F}_{h \times w}$, the lower expression, head pose errors, and identity score.
% In addition, Ours full with shape-guided identity injection, partial landmark loss, and Ours $32 \times 32$, achieves the lowest expression and head pose errors.
Since shape-guided identity injection and partial landmark loss boost the identity injection of the source image, Ours full achieves a higher identity score than Ours $16 \times 16$.
Lower expression and head pose scores demonstrate that the proposed techniques are also effective to preserve the target attributes.
% Moreover, the identity similarity is the second best beyond the Ours $16 \times 16$. 
% Therefore, the proposed 3DMM guidance method works for boosting identity injection and target attribute preservation.
% 수정한 부분 :
% 강조된 부분 2가지를 나눠서 작성하면 좋다.
% 앞에서는 F/W 분석해서 찾은 거에대한 Finding이 우리 ablation study 에서도 일치된 결과를 보임
% 뒤에서는 3DMM 활용한게 성능을 더 보완해줌
 
\begin{figure*}[!t]
    \centering 
    \includegraphics[width=\textwidth]{figure/figure-ablation.png}
    \vspace{-0.7cm}
    \caption{\textbf{Qualitative ablation studies} of $\mathbf{F}_{h \times w}$ and Ours full version. Ours $8 \times 8$ indicates that we embed target attributes to $\mathbf{F}_{8 \times 8}$ and use $\mathbf{w}_{4+}$ for injecting identity attributes of a source. Please be aware of the \textcolor{Dandelion}{yellow} and \textcolor{red}{red} boxes. Ours full is improved in respects of preserving the target image's pose such as eye gazing and expression, and reflecting the source image's shape compared with Ours $32 \times 32$.}
    \vspace{-0.3cm}
\label{fig:res_ablation}
\end{figure*}


\begin{figure}[t!]
    \centering 
    \includegraphics[width=\linewidth]{figure/figure-vid.png} %vid_0307
    \vspace{-0.5cm}
    \caption{\textbf{Qualitative results} on $512 \times 512$ resolution CelebV-HQ; (A) Target frames, (B) HifiFace, (C) FSLSD, (D) MFIM, \textbf{(E) RobustSwap}. Please be aware of the \textcolor{Dandelion}{yellow} and \textcolor{red}{red} arrows. Baselines suffer from \textbf{source attribute leakage}.}
    \vspace{-0.5cm}
\label{fig:vid}
\end{figure}


\noindent\textbf{User Studies.} % multi-column 으로 video image 구분하기
We further evaluate our model and three recent baselines~\cite{hififace,fslsd,mfim} via a user study on synthesizing the images and the videos. 
% We conduct user studies for subjective evaluations on image and video scenarios for the three recent baselines~\cite{hififace,fslsd,mfim}. 
The participants evaluated 11 swapped image samples from CelebA-HQ and 6 video samples from CelebV-HQ~\cite{celebvhq}.
% The participants to discriminate 11 image samples in CelebA-HQ and 6 video samples on CelebV-HQ~\cite{celebvhq}. 
The users are asked to score the quality of swapped images and videos according to the following criteria: 1) Identity similarity and Attribute preservation (ID sim \& Att pre); 2) Naturalness; and 3) Quality. 
We designate the highest score to be 4 and the lowest score to be 1 for each criterion. Table~\ref{table:user_study} shows that our method achieves the best score in every criterion, demonstrating that our results are the most plausible in human perceptual evaluation.
Notably, our video score is higher than other baselines with large margin even though we do not train any video datasets. 
These results indicate that preventing the \textbf{source attribute leakage} is also crucial for synthesizing temporally consistent videos.
% We recommend referring to our supplementary material videos to view video samples.




\subsection{Qualitative Evaluation}

\noindent\textbf{Comparison of Baselines.}
We compare our \textbf{\ourmodel} and baselines on CelebA-HQ and FF++ datasets.
As shown in Fig.~\ref{fig:FF++}, SimSwap sometimes shows \textbf{source attribute leakage} such as bringing the source's hair lines with low-quality results.
% In Fig.~\ref{fig:FF++}, SimSwap shows good expression and eye gazing preservation of the target, but they sometimes show \textbf{source attribute leakage} such as bringing the source's hair lines with low-quality results.
InfoSwap and HifiFace often fail to retain the target image's expression and eye gazing.
In contrast, our method generates more perceptually convincing swapped images without source \textbf{source attribute leakage}.

In Fig.~\ref{fig:celeba}, we compare our \textbf{\ourmodel} with megapixel models~\cite{mfim,fslsd,megapixel}. 
Although MFIM and FSLSD reflect source identity well, they often produce visual artifacts like \textit{appearance leakage} (\textit{e.g.}, hairstyle and eyeglasses) and \textit{pose leakage} (\textit{e.g.}, incorrect eye gazing and expression).
MegaFS generates inaccurate skin-colored images. %, even producing severe artifacts. 
% It stems from that they utilize the strong post-processing pasting the source's face to target face region. 
On the other hand, our method robustly changes the target face to the source's one almost without \textbf{source attribute leakage}, following the target image's eye gazing and expression, and having no texture leakage from the source image.
Moreover, as shown in Fig.~\ref{fig:vid}, we compare the recent three baselines with our method on generating the videos. 
While the baselines show the \textit{pose} and \textit{appearance leakages}, \textbf{\ourmodel} is robust to \textit{source attribute leakage} even in the video.
Notably, these results show that \textit{source attribute leakage} is also crucial to synthesize the temporally consistent videos in face swapping task.
% Unlike the baselines which show the \textit{pose} and \textit{appearance leakage},  \textbf{\ourmodel} is robust to \textit{source attribute leakage} even in the video scenario. % Please see our supplementary video samples
% todo : 
% 강조된 부분 2가지를 나눠서 작성하면 좋다.
% 앞에서는 F/W 분석해서 찾은 거에대한 Finding
% 뒤에서는 3DMM 붙인거에 대한 finding 을 나눠서 서술
% Video 가 큰 강점이면, 글을 좀 강조해서 쓰고, Megapixel 얘기랑, demo 꼭 보라는 얘기도 보충

\noindent\textbf{Ablation Studies.}
% 이 부분 좀 다시 체크
As shown in Fig.~\ref{fig:res_ablation}, Ours $8 \times 8$ and Ours $16 \times 16$ hardly preserve the hat, hairstyle, skin color, eye gazing, and expression of the target image.  % 16 은 나름 괜찮긴 한데
In contrast, Ours $64 \times 64$ faithfully follows the appearance and pose of the target, but the identity of the result is quite heterogeneous with the source. 
Ours $32 \times 32$ contains the target's attribute and source's identity in balance, which demonstrates that our analysis in Sec.~\ref{one} is effective for searching the proper combination of latent spaces.
While the performance of Ours $32 \times 32$ is commendable, there is a room for improvement in accurately preserving the pose of the inner face region.
Therefore, by leveraging the shape-guided identity injection and partial landmark loss, Ours full can preserve more detailed expression and head pose, and simultaneously reflect source's inner shape than Ours $32 \times 32$. 

% 수정된 부분 :
% 강조된 부분 2가지를 나눠서 작성하면 좋다.
% 앞에서는 F/W 분석해서 찾은 거에대한 Finding이 우리 ablation study 에서도 일치된 결과를 보임
% 뒤에서는 3DMM 활용한게 성능을 더 보완해줌








