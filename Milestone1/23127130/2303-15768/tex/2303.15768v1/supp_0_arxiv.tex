\twocolumn[
\begin{center}
    \vspace*{1.1cm}
    \Large{\bf{Supplementary Material}}
    \vspace*{1.7cm}
\end{center}]


\section*{A. Architecture}
% MFIM supple table 3 참고 해서 decoder 
% source encoder : jsl
% target encoder + decoder : taeu
% decoder + detail of block (B) : taeu
This section outlines the details of architectures for the target attribute encoder, source identity encoder, source shape encoder, mapper, and generator which are described in Table ~\ref{supp_table:architecture}.

\input{supp_table/architecture}
 
\section*{B. Details of StyleGAN's $\mathcal{F/W+}$ Analysis}
As already described in Sec.~3 of our main paper, we conduct the depth analysis to investigate the conformity of StyleGAN~\cite{sg2}'s latent combinations for face swapping task. 
Following subsections describe the more detailed analysis.


\begin{figure}[h!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/supp_stylegan_analysis.png}
    % todo : caption 수정
    \caption{\textbf{Qualitative analysis.} More examples from the StyleGAN latent analysis for face swapping task. 
    An \textit{anchor} image is obtained from the inverted vectors $\textbf{w}_{1+}$ by using GAN inversion method~\cite{psp}. \textit{Random sampled} images of (A) and (D) are generated by the fixed feature map $\mathbf{F}^{*}_{16 \times 16}$ and randomly initialized $\mathbf{w}_{6+}$. 
    (B) and (E)'s \textit{random sampled} images are produced by $\mathbf{F}^{*}_{32 \times 32}$ and $\mathbf{w}_{8+}$.
    \textit{Random sampled} images of (C) and (F) are obtained from $\mathbf{F}^{*}_{64 \times 64}$ and $\mathbf{w}_{10+}$.} 
    \label{fig:quali}
\end{figure}

\subsection*{B.1. Metrics for Quantitative Analysis.}
% 목적 추가
To find the appropriate combination of latents to maintain the identity-irrelevant attributes while modifying identity information, we quantitatively compare the \textit{anchor image} (correspond to a target image in face swapping) and other \textit{random sampled} images (correspond to swapped images in face swapping) with four metrics. 
Please see Table \ref{supp_table:analysis_spec}, which contains the specification of our quantitative analysis factors.

\subsection*{B.2. More Qualitative Results of the analysis.}
Furthermore, we conduct the qualitative analysis on the three highest overall scored combinations ($\mathbf{F}_{16 \times 16}$, $\mathbf{w}_{6}^{+}$), ($\mathbf{F}_{32 \times 32}$, $\mathbf{w}_{8}^{+}$) and ($\mathbf{F}_{64 \times 64}$, $\mathbf{w}_{10}^{+}$).
In this section, we show more qualitative results which are not shown in the main paper due to limited space. 
% Figure1 은 더 많은 sampmle 인데 그 결과는 다음과 같다.
% (A),(B),(C) 는 안경쓴 anchor image 에서 이런걸 바꿧을때 target 의 Attribute 가 
% 마찬가지로 (D),(E),(F) 는 앞머리있는 케이스인데 각각은 이러하다
% 따라서 우리는 이런 정성 결과에 대한 분석을 토대로, F_32x32 와 W_8+ 의 조합을 face swap model 을 만들때 사용할 latent space 로 선정했다.
% The reason why we select the ($\mathbf{F}_{32 \times 32}$, $\mathbf{w}_{8}^{+}$) as the adequate latent combination on face swapping task is grounded from Fig. 4 and Fig. 5 of main manuscript. 
% For the pre-trained StyleGAN~\cite{sg2}, the degree of \textit{pose leakage} can be estimated by the Table~\ref{supp_table:analysis_spec}'s metrics, but \textit{appearance metric} is not easy to measure.  % -> mask-l1 같은 걸로 잴 수 있지 않냐라는 질문이 나올 수도? 그래서 그냥 뺴고, 우리는 정성결과를 더 살펴봤다~ 이런식으로 자연스럽게 넘어가는게 나을 것 같아서 우선 생략하겠음
% It stems from that there is no constraint among \textit{anchor} and \textit{random sampled} images when only StyleGAN stands alone. 
% Therefore, we additionally observe the StyleGAN's resolution-wise dynamics by visualizing the \texit{anchor} image \textit{random sampled} images. We already have shown why the ($\mathbf{F}_{32 \times 32}$, $\mathbf{w}_{8}^{+}$) is most proper combination for the \textbf{source attribute leakage}-robust face swapping. 
In Fig.~\ref{fig:quali} (B) and (E), compared with the \textit{anchor} image, other \textit{random sampled} images' eyeglasses and hair bang are preserved while changing identity. 
On the other hand, in (A) and (D), those appearance attributes such as eyeglasses and hair vary. 
Moreover, in the (C) and (F), except for the light condition and skin color, there are almost no changes. 
Therefore, we choose the combination of ($\mathbf{F}_{32 \times 32}$, $\mathbf{w}_{8}^{+}$) since it properly preserves pose and appearance attributes and can change the capability of identity.

% \subsection{Generating \textit{Anchor} and \textit{Random Sampled} Images.}
% The \textit{anchor} image ($=G(\mathbf{w_{1}^{+}})$) is inverted from a real image drawn from FFHQ~\cite{progan} dataset. We utilize the pSp encoder for inversion. The \textit{random sampled} images are constructed from ($\mathbf{w_{m}^{+}},\mathbf{F}^{*}_{h \times w}$), where $\mathbf{F}^{*}_{h \times w}$ is fixed.
% The $\mathbf{m}$ is represented as $2*log_{2}(h)$. Note that $\mathbf{F}^{(*)}_{h \times w}$ excludes the $4 \times 4$ learnable constant and $\mathbf{w}_{m}^{+} \subset \mathbf{w}_{1}^{+}$. 
% Please refer the Table~\ref{supp_table:architecture} for more details.

\section*{C. New Metrics}
Following two new metrics are proposed in this work for measuring the more precise degree of \textit{source attribute leakage}.

\subsection*{C.1. Eye gazing.}
\textbf{Eye gazing} metric is calculated by RT-GENE~\cite{rtgene}'s yaw and pitch $L1$ error between the target and swapped images.
This metric helps to estimate the part of \textit{pose leakage}. 

\subsection*{C.2. Masked-L1.}
\textbf{Masked-L1} measures the error of the hair and skin region between the target and swapped images.
We utilize the off-the-shelf face parsing map predictor BiseNet~\cite{bisenet} for extracting the region.
Note that as can be seen in Fig.~\ref{fig:mask1l}, for excluding the other region, the error is calculated on the intersection area of the target and swapped images. 
This metric helps to estimate the part of \textit{appearance leakage}.
%-------------------------------------------------------------------------
\input{supp_table/analysis_metric_spec}
\begin{figure}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/maskl1.png}
    \caption{\textbf{Logic of the measurement of Masked-L1 metric.} Since the target and swapped images have different eyes, nose, lip and eyebrows, we exclude those region for calculating the metric score. Moreover, for measuring on the shared region, we designate the region as intersection.}
    \label{fig:mask1l}
\end{figure}

\begin{figure}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/plloss_detail.png}
    \caption{\textbf{Details of Partial Landmark Loss.} We designate the 51 inner facial feature related landmarks from 5023 vertices as the interest of loss. $Mesh_{i}^{mix}$ works as the ground truth of the partial landmarks which supervises the location of $Mesh_{j}^{swap}$.} 
    \label{fig:plloss}
\end{figure}


\section*{D. Details of Objective Functions}
\subsection*{D.1. Partial Landmark Loss.}
We proposed a novel partial landmark loss $L_{pl}$, which helps the swapped image's inner facial shape to resemble the source's one and follow the target's head pose and expression. Please see Fig.~\ref{fig:plloss}.
\subsection*{D.2. Adversarial Loss}
We follow the StyleGAN2~\cite{sg2}'s non-saturating adversarial loss and R1 regularizer. 
\begin{equation}    
    \mathcal{L}_{adv-D} = \text{softplus}(-D(I_{tgt})) + \text{softplus}(D(\hat{I})),
\end{equation}   
\begin{equation}    
    \mathcal{L}_{adv-G} = \text{softplus}(-D(\hat{I})),
\end{equation}   
\begin{equation}
    \mathcal{L}_{R1} = \frac{\gamma}{2} [\lVert \nabla_{\textbf{x}} D(\textbf{x}) \rVert^2_2],
\end{equation}   
\begin{equation}
    \mathcal{L}_{adv} = \mathcal{L}_{adv-D}+\mathcal{L}_{adv-G}+\mathcal{L}_{R1},
\end{equation}  
where $D$ is a pre-trained discriminator of StyleGAN2.


\section*{E. Comparison with StyleSwap}
% Recently, several StyleGAN-based face swapping models~\cite{fslsd,mfim,styleswap} are out. 
% \subsection{MFIM.}
% For comparison with the MFIM~\cite{mfim} which is the state-of-the-art face swapping model, we re-implemented the model with the manuscript and supplementary materials. To check the re-implementation fidelity, in the Fig., we show the MFIM's manuscript result and our version. 


StyleSwap~\cite{styleswap} is also a state-of-the-art baseline that proposes a modified StyleGAN~\cite{sg2}-based architecture with the ToMask branch similar to ToRGB branch of the original StyleGAN. Although their open-source code is not released to the public, to prove the superiority of our \textbf{RobustSwap}, we retrieve the StyleSwap's source and target images and compare with \textbf{RobustSwap}'s result.
As can be seen in Fig.~\ref{fig:retrieve} and~\ref{fig:retrieve2}, StyleSwap fails to synthesize the pupil of the swapped image, while \textbf{RobustSwap} seamlessly reconstructs. 





\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/retrieve.png}
    \caption{\textbf{Comparison with StyleSwap's Fig. 5 in the main manuscript.} Please pay attention to the \textcolor{red}{red} box which indicates the result's pupil. The source and target images are from CelebA-HQ train dataset.}
    \label{fig:retrieve}
\end{figure*}

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/styleswap_comp2.png}
    \caption{\textbf{Comparison with StyleSwap's Fig. 6 in the main manuscript.} Please pay attention to the \textcolor{red}{red} box which indicates the result's pupil. The source and target images are from CelebA-HQ train dataset.}
    \label{fig:retrieve2}
\end{figure*}

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/supp_mega1.png}
    \caption{\textbf{Qualitative comparisons \#1} on 1024 × 1024 resolution same gender (female) CelebA-HQ with megapixel baselines}
    \label{fig:mega1}
\end{figure*}
\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/supp_mega2.png}
    \caption{\textbf{Qualitative comparisons \#2} on 1024 × 1024 resolution same gender (male) CelebA-HQ with megapixel baselines}
    \label{fig:mega2}
\end{figure*}
\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/supp_mega3.png}
    \caption{\textbf{Qualitative comparisons \#3} on 1024 × 1024 resolution cross gender CelebA-HQ with megapixel baselines}
    \label{fig:mega3}
\end{figure*}



\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/FF++1.png}
    \caption{\textbf{Qualitative comparisons \#4} on 256 × 256 resolution FF++ with all baseline}
    \label{fig:FF++1}
\end{figure*}
\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/FF++2.png}
    \caption{\textbf{Qualitative comparisons \#5} on 256 × 256 resolution FF++ with all baseline}
    \label{fig:FF++2}
\end{figure*}

\section*{F. More Comparisons}
As mentioned before, we show more numerous results as extension of Fig. 7 and 8 in the main manuscript, comparison with all baselines and megapixel baselines, respectively (from Fig~\ref{fig:mega1} to~\ref{fig:FF++2}). Note that the video comparisons are in the attached .mp4 file, please watch the video.



\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/celeba_matrix0_suppmat.png}
    \caption{\textbf{Face matrix \#1}  on \textbf{RobustSwap} from 1024 x 1024 CelebA-HQ}
    \label{fig:celeba_matrix1}
\end{figure*}

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/celeba_matrix_suppmat_1.png}
    \caption{\textbf{Face matrix \#2} on \textbf{RobustSwap} from 1024 x 1024 CelebA-HQ}
    \label{fig:celeba_matrix2}
\end{figure*}

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/inthewild1.png}
    \caption{\textbf{In-the-wild result.} Source: Elon Musk \& Target: Sam Smith}
    \label{fig:inthewild1}
\end{figure*}

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/inthewild2.png}
    \caption{\textbf{In-the-wild result.} Source: Elizabeth Olsen \& Target: Timothee Chalamet}
    \label{fig:inthewild2}
\end{figure*}
\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/inthewild3.png}
    \caption{\textbf{In-the-wild result.} Source: Lily-Rose Depp \& Target: Olivia Rodrigo}
    \label{fig:inthewild3}
\end{figure*}
\begin{figure*}[t!]
    \centering 
    \includegraphics[width=\linewidth]{supp_figure/inthewild4.png}
    \caption{\textbf{In-the-wild result.} Source: Jaeyong Lee \& Target: Volodymyr Zelenskyy}
    \label{fig:inthewild4}
\end{figure*}

\section*{G. More Results}
From Fig.~\ref{fig:celeba_matrix1} to~\ref{fig:inthewild4}, we show the face matrices from FaceForensic++ (FF++)~\cite{ff++} dataset and internet-crawled in-the-wild data. Note that the video results are in the attached mp4 file. Please watch the video.