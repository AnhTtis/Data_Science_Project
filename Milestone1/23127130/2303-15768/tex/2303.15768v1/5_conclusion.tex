\section{Conclusion}
In this paper, we propose a robust face swapping model, \textbf{\ourmodel}, which solves \textbf{source attribute leakage} problems. 
We analyze the latent space of StyleGAN for face swapping, ultimately we develop a simple yet robust face swapping model without any architectural modification of StyleGAN, which is easy to train and implement.
On the other hand, we believe that our model can be extended to other combinations of subspaces, not limited to only face swapping tasks. 
We further utilize the explicit and implicit information of 3DMM to provide more detailed source identity information and precise target person's pose. 
Our experiments show that \textbf{\ourmodel} is comparable with previous face swapping models. Additionally, \textbf{\ourmodel} shows high-quality results in video face swapping without video datasets. 
% Overall, our contributions offer an effective and robust solution to face swapping tasks.
% On the other hand, the face swapping models can be misused in human society. 
% We are also aware of this issue and promise to utilize this work as academic purpose. 
We believe that our analysis on StyleGAN for face swapping inspires the future researchers to analyze the latent spaces of the generative model in perspective of face swapping task and utilize it as a strong prior for face swapping.

%This study is anticipated to have significant implications for numerous subsequent investigations.
% todo: Discussion and limitation 추가하는거
% (1) 3dmm paramter space 가 disentangle 이 완벽히 x, 3DMM encoder 더 좋은거찾으면 더 잘할 수 있음
% (2) 지금은 F/W+ 에서 적절한 하나의 조합을 쓰고 있는데, 여러 조합들을 종합적으로 활용할 수 있는 방안도 생각해봐야함 .(HFGI) 처럼 multiple F 와 multiple W 간의 조합하면 target attribute 를 더 풍부하게, 그리고 source attribute 를 더 풍부하게 전달할 수 있음. 하지만 제약을 더 푸는 순간, source attribute leakage 에 취약해질 수 있으나.







