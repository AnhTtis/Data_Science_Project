

\section{Offline Software }

\subsection{Introduction }

The offline software is extensively used to design and optimize a detector, 
to generate large amounts of MC data, to help in developing reconstruction algorithms and analysis procedures, to understand and demonstrate 
that a conceptual detector satisfies the goals of the STCF experiment at the experiment investigation stage as well as to support physics analysis at the experiment operation stage.

According to the design of the STCF,  the luminosity will be above $\stcflum$, 100 times higher than that of BESIII, and the total trigger rate as well as the data rate  are expected to be approximately $\stcftrigrate$ and  $\stcftotdata$ respectively, which are also much larger than those of the BESIII. Therefore, an Offline Software System of Super Tau-Charm Facility~(OSCAR) is designed and developed to provide the unified computing environment and platform to facilitate design of the STCF detector, conduct detector performance study as well as physics potential study. This chapter is going to talk about strategies and possible technologies to be used for development of STCF offline software system.


\subsection{Architecture Design }

Fig.~\ref{fig:oscar} shows the architecture of the whole offline software system, which consists of three components: External Libraries, Core Software and Applications. The  External Libraries include frequently used third-party software and tools, such as DD4hep, PODIO, ROOT, {\sc Geant4} and so on. The Core Software provides the common functionalities for all data processing and MC production, such as event data model, event data management, data processing control, data input and output, stable interfaces between different components, job configuration, user interfaces and the tools for the compiling, building and deployment of the whole software system. The Applications include the components specific to the STCF experiment, including extensions to SNiPER, Generator, Simulation, Calibration, Reconstruction and Analysis.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.6\textwidth]{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_01_Introduction/oscar2.png}
\caption{Overview of the STCF offline software system}
\label{fig:oscar}
\end{center}
\end{figure}

The offline software system is designed to meet the various requirements of data processing.
Based on our experience, Linux OS and GNU compilers are the first choice for the STCF software platform.
However, other popular OSs (such as Ubuntu, MacOS, ...) development environments should be considered for the sake of compatibility.
CMT and CMAKE, which can calculate package dependencies and generate makefiles,
are used for the software configuration and management.
An automated installation tool can ease the deployment of the STCF software and is also helpful for daily compilation and testing.
Users can concentrate on the implementation of application software, free from the interference of different development environments.

Currently, mixed programming with multiple languages is practical,
therefore, we choose different technologies for different parts of our software.
The main features of the application are implemented via C++ to guarantee efficiency.
User interfaces are provided in Python for additional flexibility.
Boost Python, as a widely used library, is a good choice for integrating C++ and Python.
If it is included properly at the beginning of the system design,
most users will be able to enjoy the benefits of mixed programming without knowing the details of Boost Python.


\subsection{Core Software}

\subsubsection{SNiPER Framework}

SNiPER ~\cite{ref_sniper} is the foundation of the STCF software system and determines the performance, flexibility and extensibility of the system. It provides common functionalities for offline data processing and the standard interfaces for developing different applications via \emph{Task, Algorithm, Service and Tool}. An algorithm provides a specific procedure for data processing. A sequence of algorithms forms the whole data processing chain. A service provides certain useful features, such as access to the detector geometry or database, which are necessary during data processing. Both algorithms and services are plugged in and executed dynamically, and they can be flexibly selected and combined to perform certain data processing tasks. All applications in terms of algorithms request event data from a Data Store and push new event data back to the Data Store, finally all event data in the Data Store will be automatically written into files via the File Output System. Meanwhile, The File Input System is responsible for reading the event data from the output files and placing them into the Data Store for downstream processing. SNiPER also provides many frequently used functions, such as the logging mechanism, particle property lookup, system resource loading, database access and histogram booking, etc.
%

 \subsubsection{Event Data Model}
 
 The event data model (EDM) serves as the central part of the whole offline software system; it defines the event information at the different data processing stages and builds the inter-event and intra-event correlations, which are very useful for optimization of reconstruction algorithms, event selection criteria and physics analysis. Two options of EDM are investigated, one is based on ROOT TObject and implemented with an XML Object Description (XOD) toolkit~\cite{XOD}, the other is based on PODIO~\cite{podio}, which is a new merging tool developed by the future collider experiments and supposed to have good supports on the concurrency of EDM. The EDMs for detector simulation and reconstruction have been implemented in both XOD and PODIO, their performances are under investigation by running detector simulation and reconstruction algorithms.
 
 
\subsubsection{Event Data Management }
Event data management system is designed and implemented with the Data Store which provides a common place for data sharing between applications via standard interfaces to get event objects from the Data Store, and to push new event objects into the Data Store.The event objects in the Data Store can be written to the ROOT files and read back from the ROOT files later. The event data management system based on PODIO consists of three components: DataHandle, PodioDataSvc and PodioSvc. PodioDataSvc is used to manage event data objects, while PodioSvc is used to read/write event data from/into ROOT files. User code is not involved with any data management details, except that DataHandle is used to register/retrieve event data to/from the Data Store.

\subsubsection{Parallel Computing}
Based on the estimation above, the STCF is expected to deliver more than 1~$\invab$ of data per year. Assuming a lifetime of 10 years for the STCF, a total of 10~${\invab}$ of data is expected. Therefore, 
it is vital to adopt parallel computing techniques to make use of all possible computing resources and accelerate the offline data processing and physics analysis. At the moment, multi-threaded programming has been supported and implemented in SNiPER by integration of multi-task with the TBB technology,  which can extract the capacity of multi-core CPUs to speed up a single job significantly. Meanwhile, a server-client system will be deployed to run detector simulation and produce massive MC data on both local computing clusters and the distributed computing infrastructure.


\subsection{Detector Simulation}

\subsubsection{Physics Generator}

The goals of the STCF are to study  $\tau$-charm physics, to precisely test the standard model and to search for new physics.
MC simulations are used to study physics potentials, to determine detection efficiencies and to study backgrounds. To meet these challenges and to reach unprecedented precision, a comprehensive MC study is necessary. Event generators with high quality and precision are essential for performing a reliable MC simulation and removing systematic uncertainty as much as possible.
For high-precision measurements, we expect MC generators to simulate the processes under study as realistically as possible.
Hence, the generators with only kinematic information (e.g., a pure phase space) do not meet this requirement.
Recently, high-precision generators for QED processes have been developed based on the Yennie-Frautchi-Suura exponentiation technique to describe the process $e^+e^-\to f\bar f$ ($f$: fermion).
The official precision tags of these generators are approximately 1\% or less, e.g., KKMC and BHLUMI.
Generators with dynamical information for hadron decays have also been developed,
such as EvtGen for BaBar and CLEO collaborations to study B physics.
All these generators have been integrated within OSCAR and provide us with a large room to make a choice among them to simulate $\tau$-charm physics processes.
Furthermore, one generator framework is developed to provide the uniform format and standardize interface for all generators. The application developers or physicists can easily and quickly call generators in a coherent manner without knowing much details of generators.



\subsubsection{Detector Geometry }
\label{section:geo}

Detector simulation requires a precision description of the detector, including materials, geometry and structure, and this description is used not only for detector simulation but also for reconstruction and visualization. The STCF detector is composed of the five sub-detectors and two auxiliary devices. There are more than  200,000 volumes in total constructing the whole spectrometer, so it is a very complicated task to design the STCF detector and requires collaboration between different working groups, thus, a geometry management system (GMS)~\cite{GMS_dete_geo_mana} is designed to provide a consistent detector-geometry description for different offline applications. A new Detector Description Toolkit, {\sc DD4Hep}~\cite{GMS_dd4hep}, is adopted to describe STCF detector geometry for GMS, with all geometric parameters stored in the compact files with eXtensible Markup Language (XML)~\cite{GMS_xml}, which is more human readable and can be edited by any text editor. 

To hold and manage geometric parameters, a customized repository with hierarchical structure is designed in the GMS, which is shown in  Fig.~\ref{fig:detectorrepository}. The feature of XML allows a flexible configuration for the assembly of different sub-detectors. As shown in Fig.~\ref{fig:detctorconstruction}, OSCAR provides DDXMLSvc to transform XML descriptions to the offline applications, including simulation, reconstruction and visualization within two steps. First, specialized C++ code fragments, called detector constructors, read and parse the geometry parameters from the XML-based repository and construct the generic detector description model based on the ROOT geometry modeller (TGeo) in memory. Second, the DDXMLSvc converts TGeo to  geometry format of simulation, reconstruction or visualization. Based on the GMS, the whole STCF detector has been build, as shown in Fig.~\ref{fig:fulldetector}.



\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.2]
{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_05_DetectorDescription/detectorrepository.jpg}
\caption{Structure of the geometry parameters repository. The library of elements and materials is shared
by all sub-detectors. Different sub-detector designs are separately managed in different XML files; if a
sub-detector has more than one designs, the parameters for different designs are stored in different XML files
with version numbers. A mother XML file can include several daughter XML files.}
\label{fig:detectorrepository}
\end{center}
\end{figure}


\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.15]{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_05_DetectorDescription/DDXMLSVC.jpg}
\caption{Workflow of the DDXMLSvc. The geometry parameters in the repository
are parsed by detector constructors and converted to formats for simulation, reconstruction and visualization.}
\label{fig:detctorconstruction}
\end{center}
\end{figure}


\begin{figure}[htbp]
	\centering
	{
		\begin{minipage}{0.4\linewidth}
		\includegraphics[scale=0.25]{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_05_DetectorDescription/fulldetector_a.jpg}
		\end{minipage}
	}
	{
		\begin{minipage}{0.4\linewidth}
		\includegraphics[scale=0.25]{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_05_DetectorDescription/fulldetector_b.jpg}
		\end{minipage}
	}

	\caption{Full detector described by the GMS and printed using the default 3D visualization plugin of DD4Hep. 2D sections are viewed from different directions: (left) Z-R view and (right) X-Y view}
	\label{fig:fulldetector}
\end{figure}


\subsubsection{Standalone Detector Simulation}

First, dedicated standalone simulation packages are implemented based on  {\sc Geant4}~\cite{Geant4_ref} for the purpose of R\&D for each sub-detector. This provides guidance for choosing the best sub-detector option that can fulfill the desired physics goals. Based on these packages, we conduct very detailed studies on the performance of these sub-detectors, including the energy resolution, momentum resolution, tracking efficiency and PID. 



\subsubsection{Full Detector Simulation }

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_06_DetectorSimulation/Simulation.png}
\caption{Overview of the full detector simulation framework for STCF}
\label{fig:sim}
\end{center}
\end{figure}

To study the performance of the whole detector, the full detector simulation framework is developed to serve as a bridge between {\sc Geant4} and OSCAR.
It consists of the integration of {\sc Geant4} with SNiPER, a configurable user interface, geometry management and modularized user actions.
With this extra layer as a middleware, detector simulation becomes a seamless component of OSCAR. The Fig.~\ref{fig:sim} shows the architecture of the full detector simulation framework, the algorithm named {\sc DetSimAlg} is designed to invoke {\sc Geant4} event loop in OSCAR, and it invokes the service named {\sc G4SvcRunManager}, which is derived from {\sc G4RunManager}, to initialize simulation and start tracking.
To decouple run manager and user code, such as detector construction and physics list, the factory class named {\sc DetSimFactory} is designed and implemented to construct user actions and pass them to {\sc G4SvcRunManager}.
Now we have setup the full detector simulation chain which includes detector geometry description, physics processes, hit recording and user interfaces. The detector geometry is described in Section~\ref{section:geo}. At the moment, a uniform magnetic field of 1 Tesla is currently defined in the detector geometry  xml file, and a realistic magnetic field map will be implemented later.
The physics processes include standard electromagnetic, ionization, multiple scattering, bremsstrahlung and optical photon processes.
The hit information and particle history information, including the initial primary particles, energy deposition, direction and position of the hit as well as the relationship between track and hits, for each sub-detector has been defined in the simulation EDM, and all these information can be saved into the ROOT files and used for reconstruction. 

\subsection{Reconstruction}

The reconstruction of track, photon, muon and particle identification plays very important roles for achieving STCF physics goals with unprecedented precision. At the moment, the reconstruction methods and algorithms are under-study with the simulated hit information.The reconstruction chain starts with the track reconstruction, followed by the particle flow interpretation of tracks and calorimeter hits and finally the reconstruction of compound physics objects such as converted photons.



\subsubsection{Track Reconstruction}
Track reconstruction consists of two main procedures, track finding and track fitting.
The purpose of track finding is to find hits produced by the same particle.
The 2D track candidates in r-$\phi$ plane are searched first,
then $z$ direction information is used to infer 3D track.
The commonly used methods are conformal transformation and Hough transformation.
Other methods, such as hypothesis test method and machine learning technique, are also under investigation.
The  track fitting is performed by the Deterministic Annealing Filter (DAF) method, which is an extension of Kalman Filter and can reduce the influence of bad points on reconstruction
by setting weights of measurement points and updating iteratively.


\subsubsection{PID Reconstruction  }
PID systems (including DTOF and RICH detectors) are used to identify charged particles based on the information of  the Cherenkov light  which is generated when tracks pass through the DTOF and RICH detectors. For DTOF, the reconstruction algorithms mainly focus on the reconstruction of the Cherenkov angle, and the flight time of the charged particle can also be used to separate different particles. For RICH, the Cherenkov photons are collected by anode pads of the detector and the log-likelihood values in all pads for a certain particle hypothesis, including $\pi, K, p, e, \mu$, are calculated, respectively. The reconstruction algorithm of RICH calculates the sum of the log-likelihood difference between two particle hypotheses and the one with larger log-likelihood difference is chosen as the optimal candidate.


\subsubsection{EMC Reconstruction}

EMC is used to detect the energy deposition of neutral particles (mainly photons) and charged particles, and also to measure the time information. The reconstruction process is divided into several steps, including cluster searching, seed finding, and cluster splitting into showers. While reconstructing the energy of the shower, the position of photons can be reconstructed by the barycenter method with logarithmic weight. On this basis, the timing algorithm included waveform simulation, leading edge timing or waveform fitting is introduced into the reconstruction framework.


\subsubsection{MUD Reconstruction}
MUD is used to detect and separate muons from other charged particles as well as identify neutral particles. The reconstruction algorithm includes three steps: firstly, the information from MDC, EMC and MUD is combined together to judge whether the gathering hits are generated by a charged track or a neutral particle; secondly, the direction from MDC extrapolation or EMC is used to do a pre-screening of the hits, then these hits are fitted and some hits with large deviations are removed; finally, after reconstruction of a track, BDT is adopted to identify which particle does the track belong to.


\begin{comment}
\subsection{Visualization}
Visualization plays important roles in many activities of a High Energy Physics (HEP)experiment, including early planning of experiments, debugging of simulation and reconstruction algorithms, detector calibration, physics analysis and online monitoring. ROOT EVE~\cite{ROOTEVE} is adopted as the framework to supply basic functions for the design of visualization, as shown in Figure~\ref{fig:detectorvisualization}. Two basic elements to be visualized are detector geometry and event data, which are encapsulated in classes of EVE. The interactive interface between users and the software is implemented with ROOT GUI package.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.6\textwidth,scale=0.5]
{Figures/Figs_06_PredictedDetectorPerformance/Figs_06_10_Visualization/Visualizationworkflow.jpg}
\caption{Design and data flow of the visualization application based on ROOT EVE.}
\label{fig:detectorvisualization}
\end{center}
\end{figure}


\subsection{Validation, Installation and Version Control}

The validation toolkit developed based on Python plays a vital role in the long lifecycle of OSCAR. It includes a unit test system and a data production system, covering the software validation from code quality monitoring to physics validation.

OSCAR is built and deployed with CMake. CMake is an open-source, cross-platform tools for building, testing and packaging software, which controls the software compilation process and generate native makefiles and workspaces that can be used in the user's compiler environment.

To facilitate the version control of the software, we use Gitlab to manage the code repository, which also provides functionalities of the wiki page, the issue tracker and the Continuous integration (CI).

\end{comment}


\subsection{Validation System}

Software validation at different levels is vital in the long lifecycle of OSCAR.To make sure its quality and performance, an automated software validation system is developed based on Python. It includes a unit test system and a data production system, covering the software validation from code quality monitoring to physics validation. The validation system supports defining and profiling unit test cases, results validation based statistical methods, as well as automated data production for high level physical validation.

\subsection{Summary}

OSCAR is developed based on SNiPER framework and several state-of-art tools, including DD4hep, PODIO and TBB, and have been adopted to well meet the requirements and challenges from the large amount of data of the STCF. Currently, the main functions of the core software have been implemented, including the event data model, event data management, IO system and interfaces between different components, and the full detector simulation chain has been set up to optimize the detector options and study the detector performance. Development of event reconstruction methods and algorithms is under going , including reconstruction of the charged tracks, electromagnetic showers and particle identifications for further physics analysis. 

