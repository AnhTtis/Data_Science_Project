\section{Trigger, Clock and Data Acquisition}
\label{sec:tdaq}


The c.m.~energy region of the STCF spans from 2.0 to 7.0~GeV, and the corresponding event rate
varies with respect to different c.m.~energies.
The event rates of different c.m.~energies can be divided into two categories: on-$J/\psi$ peak
and off-$J/\psi$ peak. The total cross-sections and event rates at the
on-$J/\psi$ peak, $\sqrt{s}=3.097$~GeV, is approximately one magnitude larger than that at the off-$J/\psi$ peak due to the
large production cross-section of $J/\psi$.
Table~\ref{eventrate} summarizes the cross-sections and event rates
at $\sqrt{s}=3.097$~GeV and 3.773~GeV, respectively.
The production cross-section of $J/\psi$ is related to the energy spread of the beams. According to the
designed machine parameters of the STCF, which are described in Sec.~\ref{sec:expcon}, the beam energy spread is
$4.0\times10^{-4}$, corresponding to a beam energy spread of $\Delta E_{\rm beam}=$0.6~MeV
and a total energy spread of $\Delta  E=\sqrt{2}\cdot\Delta E_{\rm beam}=0.848$~MeV at $\sqrt{s}=3.097$~GeV.
The event rate is calculated from the product of the cross-section and luminosity, which is supposed to be 75\% of the peaking luminosity at the optimized c.m energy $\sqrt{s}=4.0$~GeV, that is $0.75\times10^{35}$\,cm$^{-2}$s$^{-1}$ at $\sqrt{s}=3.097$~GeV. The total rate of physics events is expected to be approximately $\stcftrigrate$ at the on-$J/\psi$ peak and to be approximately 60~kHz at the off-$J/\psi$ peak.


\begin{table}[htbp]
\caption{Summary of the cross-sections and event rates from physics processes at $\sqrt{s}=3.097$~GeV and
$\sqrt{s}=3.773$~GeV.
}
\label{eventrate}
\footnotesize
\begin{center}
\begin{spacing}{1.3}
\begin{tabular}{ccc}
\hline
\hline
\vspace{0.2cm}
Physics Process~~~~~              &	  ~~~~~~~~Cross-section~(nb)~~~~~~~~          &  ~~~~~~~Rate~(Hz)~~~~~~~~ \\
\hline
\hline
\multicolumn{3}{c}{$\sqrt{s}=3.097$~GeV, ~~$\mathcal{L}=0.75\times10^{35}$~cm$^{-2}$s$^{-1}$,~~$\Delta E=0.848$~MeV }\\
\hline
$J/\psi$            &    4500                   &  337500 \\
\hspace{1.25cm} $\to e^{+}e^{-}$  &  \hspace{0.5cm}  270  &  \hspace{0.5cm}  20000\\
\hspace{1.25cm} $\to \mu^{+}\mu^{-}$ &\hspace{0.5cm}  270  & \hspace{0.5cm}  20000 \\
Bhabha $(\theta\in(20^{\circ}, 160^{\circ})$  &  734    & 55000 \\
$\gamma\gamma$~$(\theta\in(20^{\circ}, 160^{\circ})$ & 36 & 2700 \\
$\mu^{+}\mu^{-}$    &  11.4       & 900  \\
Hadronic from continuum   &  25.6  &  2000  \\
$2\gamma$ process~$(\theta\in(20^{\circ}, 160^{\circ}), E>0.1$~GeV  &  $\sim$23.3 &  $1740$\\
\hline
Total                      &      $\sim$5300  &  $\sim$400000 \\
\hline
\hline
\multicolumn{3}{c}{$\sqrt{s}=3.773$~GeV, ~~$\mathcal{L}=1.0\times10^{35}$cm$^{-2}$s$^{-1}$}\\
\hline
$\psi(3770)$            &    9                   &  900 \\
Bhabha $(\theta\in(20^{\circ}, 160^{\circ})$  &  517    & 51700 \\
$\gamma\gamma$~$(\theta\in(20^{\circ}, 160^{\circ})$ & 24.5 & 2450 \\
$\mu^{+}\mu^{-}$    &  7.9       & 790  \\
Hadronic from continuum   &  18  &  1800  \\
$2\gamma$ process~$(\theta\in(20^{\circ}, 160^{\circ}), E>0.1$~GeV  &  $\sim$25 &  $2500$\\
\hline
Total                      &      $\sim$601  &  $\sim$60100 \\
\hline
\hline
\end{tabular}
\end{spacing}
\end{center}
\end{table}


\subsection{Trigger}

\subsubsection{Trigger System}
\quad\\
The expected peak event rate at the STCF of \stcftrigrate places challenging requirements on the trigger system.
The baseline design of the STCF trigger system is based on a two-level trigger concept: a hardware-based ~(Level 1, L1) trigger and a software-based high-level trigger~(HLT), as shown in Fig.~\ref{fig:trigger_system}.
The L1 trigger mainly utilizes the information from the MDC and EMC subdetectors.
The MDC provides charged track information (momentum, position, charge, multiplicity, and so on).
The EMC gives energy deposit information and alternative cluster information (position, energy, size, multiplicity, and so on).
The trigger signal generated by L1 will be send back to each sub-detector system.
The HLT uses infomation from all sub-detector systems. Typically, the MUD provides the fast reconstructed cluster information (layer number, hit number, and position). EMC also calculates the cluster shape information and gets used in HLT. In this level, the reconstructed tracks and clusters will be marked with a preliminary tab and get identified as various trigger types. The HLT will be implemented as part of the DAQ system.

%%%%%%%%%%%%%%%%%%% Fig %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htb]
    \centering
    {
        \includegraphics[width=100mm]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/triggersystem.png}
    }
    \vspace{0cm}
\caption{The schematic of STCF trigger system.}
    \label{fig:trigger_system}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


It is worth noting that the \stcftrigrate\ event rate only occurs at the $J/\psi$ peak. At other c.m. energies, the event rate would be much lower. For instance, according to Table~\ref{eventrate}, the expected event rate is approximately 60~kHz at $\sqrt{s}=3.773$~GeV ($\psi(3770)$). At the event rate of \stcftrigrate\ expected on the $J/\psi$ peak, the probability of more than one physics event occuring within a time window of 200 (500) ns is approximately 8 (18)\%. This poses major challenges to the STCF detector in terms of separating the physics events close in time. This calls for fast detector response to minimize the event overlapping probability. The MDC is the part of the STCF detector that is most susceptible to the high event rate given its rather long drift time and hence a long integration time window (up to ~1 $\mu$s). However, timing capability of other components of the STCF detector (for example, the CMOS-ITK and EMC) can be exploited to resolve the overlapping events recorded by the MDC as described in Sec.~\ref{sec:mdc_pileup_effect}. It should be noted that the current conceptual design of the STCF detector has not been fully optimized to effectively distinguish close-by events at a \stcftrigrate\ event rate. Further design studies of both detectors and readout electronics are needed to fully meet the challenge posed by the very high event rate.



\subsubsection{Trigger Electronics}
As shown in Fig.~\ref{fig:trig_elec}, the trigger electronics are organized into several hierarchies. The first step of trigger processing is finished within the electronics of each subdetector (i.e., in ``trigger logic'' in each readout unit of Fig.~\ref{fig:trig_elec}), which contributes to the generation of the ``global trigger'' signal. The results (marked as ``InfHIT'' in Fig.~\ref{fig:trig_elec}) are then sent to the subtrigger unit~(STU) for further processing, and the final information is gathered by the global trigger unit~(GTU) for the decision of whether to generate a global trigger (marked as ``TrgG" in Fig.~\ref{fig:trig_elec}). Actually, there may be several layers of STUs, according to the final system scale.
Optical fibers will be used for communication between the RU, STU, and GTU, as this is suitable for signal transmission over long distances and can isolate the ground of different modules over a large area. Based on the wavelength division multiplexing~(WDM) technique, bidirectional signal transmission can be performed over one optical fiber.
As for the downlink distribution of TrgG, TrgG is fanned out through the same optical fiber path for the uplink from the RU to the STU and finally to the GTU, just backward. Once the FEE receive TrgG, the ``trigger match" function is executed to locate the valid data, which are then read out to the DAQ. This ``trigger match" process would be conducted in the RU or in FEE, according to the electronics designs of different subdetectors. The uplink delay from the subdetector electronics to GTU and the downlink delay backwards contribute to the final trigger latency. Besides, a proper trigger match window should be selected to perform the ``trigger match” process. According to the trigger latency and match window size, the times tamp region of valid data can be calculated, and then the trigger match process is accomplished by searching data within the caches of the subdetector electronics.


\begin{figure*}[htb]
	\centering
    \includegraphics[width=0.9\linewidth]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/trig_elec.pdf}
\vspace{0cm}
\caption{Block diagram of the trigger electronics for the STCF.}
    \label{fig:trig_elec}
\end{figure*}

\subsection{Clock System for the Readout Electronics}
\quad\\
The structure of the clock system for the readout electronics in the STCF is shown in Fig.~\ref{fig:trig_clk}.
The readout electronics should be synchronized for time measurement or event building. This is achieved in two steps. First, all the FEE should be synchronized to a system clock which functions as the global reference for the time stamping or precise time measurement in all the subdetector electronics, and this system clock will be imported from the clock system of the accelerator in STCF; second, the time stamp should be started (or cleared) at the same time point.
For the first step, as shown in Fig.~\ref{fig:trig_clk}, the global clock unit (GCU) receives the system clock and then fans it out to multiple subclock units (SCUs). These SCUs further send the synchronized clocks to the Common ReadOut Board based on PXI (CROB-PXI) modules in the DAQ and then finally transmit them to the RUs \& FEE of different subdetector electronics. Considering the large scale of electronics, optical fibers will also be used to guarantee high-quality clock signal transmission over long distances.

For the second step ({\it i.e.}, the synchronous starting of the time stamp), once the DAQ receives a command from the operator, it transmits this command to the GTU, as shown in Fig.~\ref{fig:trig_elec}. The GTU translates this command and generates a ``Start" signal, and then distributes this signal through the same path for the global trigger signal in normal working mode. When the FEE receive this ``Start" signal, the counters within all the FEEs are cleared to zero, and then start counting driven by the aforementioned system clock. The outputs of the counters are used as the time stamps, which are well synchronized among all the FEEs. Besides, a periodic checking process will also be implemented: the GTU sends out global time stamp information which is compared with the local time stamps within the FEEs. Errors detected in this checking process will be reported in time, and the local time stamp will be corrected simultaneously. The time stamp information is added to the data packages from the FEEs, which will be further used in event building.

\begin{figure*}[htb]
	\centering
    \includegraphics[width=0.9\linewidth]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/trig_clk_bl.pdf}
\vspace{0cm}
\caption{Block diagram of the clock system for the readout electronics in the STCF.}
    \label{fig:trig_clk}
\end{figure*}


\FloatBarrier

\subsection{Data Acquisition System}

\subsubsection{Data Size Requirements}
The data size requirements are based on requests from subdetectors, as summarized in Table~\ref{tab:stcf_data_size}.
\input{Chapters/Chapter_04_00_DetectorSubSystems/Chapter_04_08_Trigger_DataAcquisition/event_size}

\subsubsection{Design Goal}
The data acquisition (DAQ) system performs data processing and system control in the STCF experiment. The main functionalities of the STCF DAQ system are as follows:
\begin{itemize}
\item Reads out the detector data after the Level-1~(L1) trigger matching from the front-end electronics (FEE) or the merged L1 triggered data from the FEE readout unit~(RU).
\item Implements several steps of the data processing, including event building, event data compression or information extraction, and high-level trigger~(HLT) computing, before the physics events of interest are ultimately saved to the storage system.
\item Organizes and manages the FEE modules, configures the FEE working parameters, and controls the working flow of the system.
\item Monitors the running status online, including the FEE working status, the status of the link, etc.
\item Decimates and analyzes the event data to assess the working status of the system when needed.
\end{itemize}
Of the points mentioned above, the reading out and processing of FEE data is the most significant functionality of a DAQ system.

In the STCF, as shown in Table~\ref{tab:stcf_data_size}, the total trigger rate is expected to be approximately \stcftrigrate, which produces a total data rate of approximately
\stcftotdata\ when the average event size is \stcfeventsize.
Furthermore, the design of the DAQ architecture should consider other factors, such as the budget, the development time, the difficulty of development, the scalability, and the maintenance cost in hardware, software and human resources.


\subsubsection{Conceptual Hardware Architecture of the DAQ System}
\quad\\
The conceptual DAQ system is composed of two layers. One is the FPGA-based processing layer (FPGA layer, FL), where the FEE data reading, preprocessing and merging are performed in real time with low time latency by the FPGA. The other part is the CPU/GPU-based processing layer (software layer, SL), where the software part of the DAQ, such as event building, data compressing/information extracting, high-level trigger~(HLT) computing, is running,
The FPGA layer is described in Fig.~\ref{fig:daq_fl}:

\begin{figure*}[htb]
	\centering
    \includegraphics[width=0.9\linewidth]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/daq_fl.pdf}
\vspace{0cm}
\caption{Conceptual hardware architecture (FL).}
    \label{fig:daq_fl}
\end{figure*}

As shown in Fig.~\ref{fig:daq_fl}, a data merging stage named readout units (RUs) %Please note that this abbreviation has already been defined as ``readout units". --By J.Yang: All the ``Read Unit" in the figures should be replaced by the ``Readout Unit"
is used to collect the data from the FEE, extract the trigger information, send the trigger information to the trigger system (TS), receive the trigger signals (from the trigger system) and the commands (from the DAQ system), and forward them to the FEE. The RUs are connected to the FEE through cables, such as twisted pairs or coaxial cables, at a typical data rate of 80/160/320~Mbps, through which a typical merging ratio of 1:32 or 1:64 can be provided.

After the RUs, two other data merging stages (CROB-PXI and CROB-PCIe) are used to merge the data from the RUs, transmit the commands, and distribute the clock and trigger signal (optional) to the RUs and FEE. By using GTX/GTH transceivers in the FPGA, each fiber link in CROB-PXI and CROB-PCIe can transfer data at a rate up to 10~Gb/s, while each PCIe Gen2$\times$8 interface can provide a theoretical transmission capability of 32~Gb/s. Additionally, the FPGAs on CROB-PXI and CROB-PCIe can perform real-time data preprocessing and data merging in the pipeline, so the entire FL can provide sufficient transfer bandwidth, processing ability, and scalability for the FEE of different detectors.

The SL is mainly composed of workstations or servers, and the conceptual hardware architecture of the SL is shown in Fig.~\ref{fig:daq_sl}.

\begin{figure*}[htb]
	\centering
    \includegraphics[width=0.9\linewidth]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/daq_sl.pdf}
\vspace{0cm}
\caption{Conceptual hardware architecture (SL).}
    \label{fig:daq_sl}
\end{figure*}

There are 3 main types of server nodes in the SL: the readout servers (ROS), the event builders (EBs), and the NAS nodes. All the nodes are interconnected with 10G/25G/40G Ethernet. The ROSs are equipped with CROB-PCIe boards, acting as the entrance of the uplink streams (such as the data stream, the status stream) and the output port of the downlink streams (such as the command stream). The EBs perform high-performance computing (HPC) in terms of data compression, information extraction, and event building by integrating all kinds of computing resources, including CPUs, GPUs, and FPGA hardware accelerators. The NAS provides the necessary storage capacity and bandwidth for the STCF experiment.

In addition to the server nodes mentioned above, there are some other servers in the SL. The LOG/DB server (LOG) provides the system database for recording the system log information, including the system topology, running status, working parameters, and warning and error messages. The online/control server (OLC) is the command hub of the whole system and can receive, process, and forward the commands sent by the remote control server (RCS). It is also an online information controller that can merge the status stream, extract the online information for each detector system, and decimate the event data. All of this online information is processed in the remote online server (ROL) and the remote event analysis server (REA).


\subsubsection{Conceptual Architecture of the Firmware and Software}
\label{sec:daq_fl_sl}
\quad\\
The firmware and the software of the STCF conceptual DAQ system are based on a generic stream-processing DAQ framework named the D-Matrix. The design philosophies of the D-Matrix are as follows:
\begin{itemize}
\item The workflows of a DAQ system are abstracted into several independent streams, such as the data stream, the command stream, the command feedback stream, the status stream, the online information stream, the log stream, and the emergency message stream, etc.
\item Each stream has its own processing map constructed by the cascading of some standard or custom stream processing nodes named ``logic nodes" (LNs). LNs can be FPGA firmware modules or software processes, work in the application layer of the OSI model, and can be distributed to ``physical nodes (PNs)", which means the hardware entities where the LNs run, such as FPGAs or servers. The distribution of an LN to a PN is based on the factors including the LN type, the resources and the processing ability that PN can provide and the real connection map among PNs.
With a feasible function abstract, most of the LNs can be reused in different streams with different working parameters. With the standard interface and the standard data structure definition, the LNs with matched parameters can be connected freely.
\item For the transport layer interfaces between two PNs, the ``multiple point-to-point" (MPP) model is used, which means that the interfaces based on different mediums and different protocols are encapsulated to a unified model, as shown in Fig.~\ref{fig:daq_mpp}. Under this model, each stream crossing the PNs has a channel ``independent" from other streams, and the interface between PNs seems to be ``transparent" for each stream. In this way, the connection of the LNs for each stream is independent of the transport layer, the link layer and the physical layer of the hardware.
\end{itemize}

\begin{figure*}[htb]
	\centering
    \includegraphics[width=0.7\linewidth]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/daq_mpp.pdf}
\vspace{0cm}
\caption{Multiple point-to-point (MPP) model.}
    \label{fig:daq_mpp}
\end{figure*}

Taking the data stream as an example.


One of the core functionalities of a DAQ system is the event building. In D-Matrix, this functionality is implemented by the cascading of the “Merge” nodes, which can merge the data fragments for some continuous small spatio-temporal ranges into an intact data frame. The process of the event building can be divided into multiply steps, shown as Figure~\ref{fig:daq_proc_map_data}. In the CROB-PXI and CROB-PCIe, a firmware “Merge” node is used severally to aggregate the data with same trigger number from multiple RUs. In the Readout Server, the data from multiply CROB-PCIe boards can be packaged in a software “Merge” node and then be distributed to diverse EBs according to the trigger number by a “Map-T” node (mapping the stream to multiple output according to the “Time index” of the frame). All the data sections with same trigger number will be sent to the same EB, where a sub-system level event building (building an event section with all the data from one detector system) and a system level event building (generating the final event file collected all the data with the same trigger number) are accomplished. Among these event-building steps, two custom LNs may be inserted to perform the possible data compressing or information extracting after the sub-system level event building, or to do the Level-II trigger computing through a “Filter” way when needed. At the same time, optional “Map-T” nodes may be inserted to promote the depth of parallelism. Benefit from the flexible stream-processing architecture, the software LNs can also be replaced by the GPU version or the FPGA version to accelerate the event building computing.

\begin{figure*}[htb]
	\centering
    \includegraphics[width=0.9\linewidth]{Figures/Figs_04_00_DetectorSubSystems/Figs_04_08_TDAQ/daq_proc_map_data.pdf}
	\vspace{0cm}
	\caption{Processing Map for Data stream}
    \label{fig:daq_proc_map_data}
\end{figure*}




Following the D-Matrix framework, the processing function of the STCF DAQ can be adjusted flexibly by reusing the standard LNs and inserting the custom LNs, which greatly reduces the difficulty of development and maintenance.



\subsection{Event Start Time ($T_0$) Determination}
\label{sec:daq_t0}


The time of an $e^{+}e^{-}$ collision corresponding to an event that fired a given trigger, defined as the event start time $T_0$, will be determined offline.
$T_0$ is essential for event reconstruction and particle identification.
For example, the measurement of time of flight~(TOF) of a charged particle by the DTOF detector and that of the drift time by the MDC both require a precise determination of $T_0$.
The precise  $T_0$ is actually provided by the accelerator beam arrival monitor with a time resolution around $100$~fs. The bunch spacing of the STCF accelerator is 4~ns in the preliminary conceptual design of the accelerator, while the the trigger clock is designed to have a period of 24~ns.. Thus there will be 6 or 7 collisions within one trigger clock, and the task of $T_{0}$ determination is to resolve the bunching crossings within one trigger clock, which requires at least 800~ps time resolution.


In the current conceptual design of the STCF detector, no dedicated $T_0$ detectors are considered.
However, a precise determination of $T_{0}$ would still be viable by exploiting the timing capability (including offline timing capability) of the subdetectors.
The MDC is expected to provide a time resolution of approximately 1.0~ns for a charged track traversing most of the wire layers, which give a $T_{0}$ precision better than 1 ns when multiple tracks are present in an event~\cite{MaXiang_2008}. 
The DTOF detector has been demonstrated to be able to distinguish bunch crossings correctly with an efficiency of $> 99$\% (Sec.~\ref{sec:dtof_t0}).
However, the DTOF detector has a momentum threshold for Cherenkov production that prevents it from determining $T_{0}$ with low-momentum charged particles.  
The EMC detector is capable of providing a 300 ps time resolution for each through-going charged track as well as well as a photon with energy above 100 MeV (as discussed in Sec.~\ref{sec:emc_perf}). Thus, the EMC has the potential to cover the whole $T_{0}$ determination task alone. 
More investigation and studies are needed to explore the full potential of the STCF detector in its current design in terms of determining $T_{0}$.



\FloatBarrier
