\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subfigure}
\usepackage{multirow}
\usepackage{url}
\usepackage{booktabs}
\usepackage{arydshln}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\makeatletter
\renewcommand*{\@fnsymbol}[1]{\@arabic{#1}}
\begin{document}

\title{Denoising Diffusion Autoencoders are Unified Self-supervised Learners}

\author{Weilai Xiang\thanks{Beihang University, Beijing, China. Correspondence to: Weilai Xiang \textless xiangweilai@buaa.edu.cn\textgreater. \newline \newline Preprint. Work in progress.} \qquad Hongyu Yang\footnotemark[1] \qquad Di Huang\footnotemark[1] \qquad Yunhong Wang\footnotemark[1]
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training.
This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are \textbf{unified} self-supervised learners:
by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations at its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for self-supervised generative and discriminative learning.
To verify this, we perform linear probe and fine-tuning evaluations on multi-class datasets. Our diffusion-based approach achieves 95.9\% and 50.0\% linear probe accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked autoencoders and contrastive learning for the first time.
Additionally, transfer learning from ImageNet confirms DDAE's suitability for latent-space Vision Transformers, suggesting the potential for scaling DDAEs as unified foundation models.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Understanding data with limited human supervision is a crucial challenge in machine learning. To deal with massive amounts of data with scarce annotations, deep learning paradigms are shifting from supervised to self-supervised pre-training. 
Regarding natural language processing (NLP), self-supervised models such as BERT \cite{bert}, GPTs \cite{gpt, gpt2, gpt3} and T5 \cite{t5} have achieved outstanding performance across diverse tasks, and large language models like ChatGPT \cite{gptHF} are showing a profound impact beyond the machine learning community.
Among these, BERT uses masked language modeling (MLM) as a pretext task to train text encoders that cannot generate full text samples, whereas GPTs and T5 can generate long paragraphs autoregressively (AR). Moreover, they prove that decoder-only or encoder-decoder models can also learn deep language understandings via generative pre-training, without training an encoder alone.
With the advent of AI-Generated Content (AIGC), compared to pure encoders, GPTs and T5 have been receiving more attention, which \textbf{unify} the generative (\eg translation, summarization) and discriminative (\eg classification) tasks \cite{gpt2, t5}.

\begin{figure}[t]
\centering
    \subfigure[Denoising networks in pixel-space and latent-space diffusion models.]{
    \includegraphics[width=\linewidth]{DDAE_1.pdf}
    \label{fig:DDAE_1}
    }
    \subfigure[Evaluating DDAEs as self-supervised representation learners.]{
    \includegraphics[width=\linewidth]{DDAE_2.pdf}
    \label{fig:DDAE_2}
    }
    \caption{\textbf{Denoising Diffusion Autoencoders (DDAE).}
    \textit{Up:} Diffusion networks are essentially equivalent to multi-level denoising autoencoders (DAE), except that the former are trained to predict noise. The networks are named as DDAEs due to this similarity.
    \textit{Down:} By linear probe evaluations, we confirm that DDAEs can produce strong representations at some intermediate layers. Truncating and fine-tuning DDAEs as vision encoders further leads to superior image classification performance.
    }
\end{figure}

In computer vision, self-supervised learners have not yet achieved the same level of success as GPTs in bridging the gap between generation and recognition. While generative adversarial networks (GAN) \cite{gan, stylegan2, biggan} and autoregressive Transformers \cite{transformer, vqgan, dalle} can synthesize high-fidelity images, they do not offer significant benefits for discriminative tasks.
For recognition-only purposes, contrastive learning \cite{moco, simclr, simclr2} builds features through discriminative pretext tasks on augmented images. Masked autoencoders \cite{mae, beit, simmim} introduce BERT-like masked image modeling (MIM) pre-training for Vision Transformers \cite{vit}, but they seem not natural and practical for convolutional networks.
It is noteworthy that although MLM/MIM methods can fit the conditional distribution of masked tokens and recover them, they do not model complete data directly and are thus challenging to generate full text or image samples.
Consequently, they are not referred to as generative pre-training or unified generative-and-discriminative methods. Instead, we denote them as ``semi-'' generative pre-training for their limited similarities to full generative models (Table~\ref{tab:ssl}).

Theoretically, it is more practical to extend generative models for discriminative purposes and gain the benefit of both ways via unified models.
Recently, we have witnessed the flourish of AI-generated visual content due to the emergence of diffusion models \cite{ddpm, sde}, with state-of-the-art results reported in image synthesis \cite{adm, edm, dit}, image editing \cite{prompt2prompt}, text-to-image \cite{dalle2, imagen, ldm} and video synthesis \cite{imagenvideo}.
Considering such a capability, versatility, and scalability in generative modeling, we ask: \textit{whether diffusion models can replicate the success of GPTs and T5, in becoming unified generative-and-discriminative learners?} We are optimistic about this potential, based on the following observations:

\textbf{(\romannumeral1)} It has been demonstrated that discriminative text or image encodings can be learned through end-to-end generative pre-training \cite{t5-enc, t5-sts, igpt}, consistent with the ``analysis-by-synthesis'' theory \cite{ana-by-syn1, ana-by-syn2, ana-by-syn3} in human perception.
Intuitively, a full generation task should contain, and be more challenging than semi-generative masked modeling, suggesting that language (or image) generation is compatible with language (or visual) understanding, but not vice versa.
This \textbf{generative pre-training} paradigm supports diffusion as a meaningful discriminative learning method.

\textbf{(\romannumeral2)} Diffusion networks are essentially trained as multi-level denoising autoencoders (DAE, see Figure~\ref{fig:DDAE_1}). The idea of \textbf{denoising autoencoding} has been widely applied to discriminative visual representation learning \cite{dae1, dae2, seg-decoder-denoise}.
More recently, masked autoencoders (MAE) \cite{mae} have further highlighted the effectiveness of denoising pre-training, which can also be inherited by networks in diffusion models --- resembling MAE's de-masking, recovering images with large and multi-scale noise is a nontrivial task and may also require a high-level understanding of visual concepts.

\textbf{(\romannumeral3)} The benefits of diffusion-based representation learning are evidenced.
\cite{seg-diffusion} confirms that diffusion models can capture pixel-wise semantic information at their intermediate layers, indicating the feasibility.
BigBiGAN \cite{bigbigan} and iGPT \cite{igpt} find that better image generation performance can translate to improved feature quality on other generative models, suggesting that diffusion is even more capable of representation learning as the state-of-the-art generative model.
Besides, utilizing diffusion models as representation learners can be further facilitated by existing large-scale AIGC projects if the learned knowledge can be conveniently transferred from pre-trained models.

\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{llll}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Pre-training} \\ \textbf{target and method}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{(\romannumeral1) Generative} \\ \textbf{pre-training}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{(\romannumeral2) Denoising}\\ \textbf{autoencoding}\end{tabular}} \\
     & & & \\ \hline
    \multicolumn{4}{l}{\textit{Natural   Language Processing}}      \\
    BERT \cite{bert}       & Encoder-only,  MLM & Semi-  & Masked   \\
    GPT \cite{gpt}         & Decoder-only,  AR  & Full   & --       \\
    T5 \cite{t5}           & Enc-dec, MLM+AR    & Full   & Masked   \\ \hline
    \multicolumn{4}{l}{\textit{Computer   Vision}}                  \\
    MAE \cite{mae}         & Encoder-only,  MIM & Semi-  & Masked   \\
    iGPT \cite{igpt}       & Decoder-only,  AR  & Full   & --       \\
    \multirow{2}{*}{\textbf{DDAE (ours)}} & \multirow{2}{*}{Enc-dec, Diffusion} & \multirow{2}{*}{Full} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Multi-level \\ Gaussian\end{tabular}} \\
     & & & \\ \hline \\
    \end{tabular}
}
\caption{\label{tab:ssl} A summary of self-supervised learners. DDAE takes full advantage of generative pre-training and denoising autoencoding.}
\end{table}

Driven by this analysis, we investigate whether diffusion models, which incorporate the best practices of generative pre-training and denoising autoencoding (as summarized in Table~\ref{tab:ssl}), can learn effective representations for classification.
Our approach is straightforward: we evaluate diffusion pre-trained networks, namely denoising diffusion autoencoders (DDAE), as feature extractors by measuring the linear probe and fine-tuning accuracies of intermediate activations (Figure~\ref{fig:DDAE_2}).
For linear probing, we pass noised images with specific scales (or timesteps) to DDAEs and examine the activations at different layers. For fine-tuning, we truncate DDAEs at the best representation layer as image encoders and fine-tune them without additional noising.

We confirm that via end-to-end diffusion pre-training, DDAEs do learn strongly linear-separable features, which lie in the middle of up-sampling and can be extracted when images are perturbed with noise.
Moreover, we confirm the correlation between generative and discriminative performance of DDAEs through ablation studies on noise configurations, training steps, and the mathematical model.
Evaluations on CIFAR-10 \cite{cifar10} and Tiny-ImageNet \cite{tiny} show that our diffusion-based approach is comparable to supervised Wide ResNet \cite{wrn}, contrastive SimCLRs \cite{simclr, simclr2} and MAE \cite{mae} for the first time. The transfer ability is also verified on ImageNet \cite{in1k} pre-trained models, including UNets and latent-space Vision Transformers such as DiT \cite{dit}.

Our study highlights the underlying value of diffusion models as unified vision foundation models. With the insightful elucidation and the proposed approach in this paper, we are optimistic about transferring powerful discriminative knowledge from large-scale pre-trained AIGC models like Stable Diffusion \cite{ldm}. Additionally, the revealed duality of DDAEs as state-of-the-art generative models and competitive recognition models may inspire further improvements to vision pre-training and applications in both domains.


\section{Relate work}
\textbf{Diffusion models} are becoming the most popular generative models due to their high-fidelity generation performance and the ability to synthesize complex visual concepts \cite{ddpm, sde, edm, ldm, dalle2, imagen}, without unstable adversarial training, mode collapse issues or strict architecture constraints.
With improvements to computational efficiency \cite{ldm}, training \cite{edm}, sampling \cite{ddim, iddpm, edm} and conditional guidance \cite{adm, cfg}, diffusion becomes the state-of-the-art on unconditional CIFAR-10 \cite{cifar10}, class-conditional ImageNet \cite{in1k}, and text-to-image on MS-COCO \cite{coco}. Large AIGC projects such as Stable Diffusion \cite{ldm} and ControlNet \cite{ctrlnet} have achieved broad social impacts.
Recent work on diffusion with ViTs \cite{vit, uvit, dit, ascend} further suggests a trend towards scalable diffusion models with modern backbones.

\textbf{Representation learning with generative models} is a long-standing idea since they model the data distribution in an unsupervised manner. While VAEs \cite{vae, vqvae} can learn meaningful representations, they have proven more useful in generation than recognition.
BigBiGAN \cite{bigbigan} learns discriminative features from large-scale GANs \cite{biggan} with jointly trained encoders \cite{bigan1, bigan2}. However, since GANs naturally capture less data diversity, their feature quality may consequently be compromised.
iGPT \cite{igpt} models next pixel prediction with GPT-2 Transformers \cite{gpt2} and achieves competitive results with contrastive methods, but the autoregressive architecture without inductive bias for images makes its downstream applications restricted and inefficient.

\textbf{Representation learning with diffusion models.} Some studies \cite{diff-ae, diff-ae-pretrained} introduce auxiliary encoders to extract representations following GAN Inversion \cite{ganinv}, but they focus on attribute manipulation rather than recognition.
Other methods \cite{DRL-VDRL, DRL-VDRL-time} learn linear-separable features with modified diffusion frameworks, but they underperform contrastive baselines by large margins.
Training diffusion models with classification objectives has been explored \cite{hybrid}, but it fails to rival pure recognition models and hurts generative performance heavily.
Diffusion-based conditional likelihood estimation is also straightforward \cite{score-class}, but its accuracy on CIFAR-10 is still below ResNet baselines.
In contrast, our approach outperforms or matches common self-supervised and supervised models without modifying diffusion frameworks.
Except for \cite{seg-diffusion}, studies on diffusion-based recognition have ignored the essence of denoising autoencoding, which motivates us to the simple approach. While \cite{seg-diffusion} studies DDPM \cite{ddpm} on single super-class datasets for segmentation, we are the first to explore various diffusion models for classification on complex multi-class datasets.


\section{Approach}
\subsection{Background: DDAEs as generative models}
\label{sec:background}
Diffusion models \cite{ddpm} define a series of data corruptions which apply Gaussian noise to data $x_0$. Given timestep $t=1,...,T$ which indicates noise levels and noise scale $\beta_t$, the corruption is defined as $q(x_t | x_0) = \mathcal{N}(\sqrt{\bar \alpha_t} x_0, (1-\bar \alpha_t) \mathbf{I})$, where $\bar \alpha_t$ is hyper-parameter derived from $\beta_t$s.
When $T$ is large enough, any real data will approximately be fully corrupted to $x_T \sim \mathcal{N}(0, \mathbf{I})$.
With the reparameterization trick, we can obtain a noised instance $x_t$ from $x_0$ at an arbitrary level of $t$, by sampling $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ and taking:
\begin{equation}
    x_t = \sqrt{\bar \alpha_t} x_0 + \sqrt{1-\bar \alpha_t} \epsilon
    \label{eq:noising}
\end{equation}

Diffusion models aim to invert the corruption and reconstruct $x_0$ given $x_T$. DDPM \cite{ddpm} proposes a Markov chain to gradually denoise $x_{T...1}$ with transitions: $p_\theta(x_{t-1} | x_t) = \mathcal{N}(\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$. Time-conditioned networks are employed to predict transitions and are optimized with the variational lower bound. Denoising score matching \cite{sde} demonstrates that these networks are estimating gradients on perturbed data distribution $\nabla_{x_t} \log q(x_t)$, which lead to Langevin sampling on the original distribution \cite{ncsn} and enable diffusion to cover modes more faithfully than GANs.

DDPM fixes $\Sigma_\theta(x_t,t)$ to hyper-parameters and reparameterizes $\mu_\theta$ as a noise prediction network $\epsilon_\theta$, which leads to a simple mean squared error loss:
\begin{equation}
    \mathcal{L}_{simple} = \|\epsilon_\theta(x_t, t) - \epsilon\|^2
    \label{eq:simple}
\end{equation}
where $\epsilon$ is the sampled noise component in $x_t$. Note that given the relationship between $x_0$, $x_t$ and $\epsilon$ in Eq.~\ref{eq:noising}, the noise predictor $\epsilon_\theta$ can also be interpreted as a denoising network $D_\theta$, by parameterizing $D_\theta(x_t, t) = \frac{x_t - \sqrt{1-\bar \alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\bar \alpha_t}}$ and re-weighting Eq.~\ref{eq:simple} by $\frac{1-\bar \alpha_t}{\bar \alpha_t}$. In this view, network is trained to minimize the reconstruction error $\mathcal{L}_{denoise} = \|D_\theta(x_t, t) - x_0\|^2$, and can be seen as a multi-level, level-conditioned and re-weighted version of conventional DAEs, specifically \textit{Denoising Diffusion Autoencoders (DDAE)}.

Song \etal \cite{sde} further extend the DDPM with finite timesteps to stochastic differential equations (SDE) with continuous-time transformations. The noising procedure is replaced by a forward SDE and the denoised samples can be obtained along the reverse-time SDE. With an improved DDPM++ network, SDE-based models outperform DDPM and StyleGAN2 \cite{stylegan2} on CIFAR-10.
EDM \cite{edm} further improves training and sampler for SDEs, and yields state-of-the-art results on CIFAR-10 and down-sampled ImageNet.

Apart from the mathematical model, DiT \cite{dit} explores scalable diffusion backbones under the framework of Latent Diffusion Models \cite{ldm}. LDMs employ diffusion models within the VAE \cite{vae} latent space and outperform pixel-space models on high-resolution synthesis. DiT replaces UNets with Vision Transformers and achieves state-of-the-art results on class-conditional ImageNet of $256^2$ or greater.

In this paper, we consider the (\romannumeral1) vanilla DDPM, (\romannumeral2) DDPM++ under the EDM framework, and (\romannumeral3) latent-space DiT as representative DDAE implementations. DDPM and DDPM++ use pixel-space UNets with timestep embeddings to parameterize $\epsilon_\theta(x_t, t)$. DiT uses latent-space ViTs with timestep and label embeddings to learn $\epsilon_\theta(x_t, y, t)$ for conditional generation, and drops $y$ randomly to null during training to achieve classifier-free guidance \cite{cfg} sampling.

\subsection{Evaluating DDAEs as discriminative learners}
\label{sec:grid}
Extracting meaningful representations from diffusion models is not a trivial task. Although deterministic inference process in diffusion models like DDIM \cite{ddim} is able to derive uniquely determined latent variables, the contained information is too coarse for image editing or classification. Some studies employ additional encoders to learn representations for attribute manipulation \cite{diff-ae, diff-ae-pretrained} or classification \cite{DRL-VDRL, DRL-VDRL-time}.
In contrast, we propose to \textit{directly take the intermediate activations in pre-trained DDAEs}. This approach requires no modification to common diffusion frameworks and is compatible with numerous open-source models.

This design is inspired by iGPT \cite{igpt} and \cite{seg-diffusion}, which evaluate learned features in Transformers and diffusion networks for classification, and the latter confirms that intermediate activations in DDAEs do capture high-level information for semantic segmentation.
Drawing from the connections between diffusion networks and denoising autoencoders (Section~\ref{sec:background}), we infer that DDAEs can produce linear-separable representations at some implicit encoder-decoder interfaces, resembling DAEs and MAEs.
Driven by this, we extend previous investigations on GPTs and DDPM \cite{igpt, seg-diffusion} to various diffusion networks (including UNets and DiT) under different frameworks.
Note that although DiT uses encoder-only Transformers \cite{transformer} in an end-to-end manner, our preliminary experiments confirm it shares similar properties to encoder-decoder UNets in feature extraction.

Considering the UNet with long skip connections has been the de-facto design in diffusion, we avoid splitting the encoder-decoder explicitly to prevent diminishing the generation performance. However, the best layer to extract features remains unknown. Additionally, to prevent the gap between pre-training and deploying, images have to be noised by certain scales for linear evaluations. Therefore, we investigate the relationship between feature qualities and layer-noise combinations through grid search, following \cite{seg-diffusion}.

\begin{figure}[t]
\centering
    \subfigure{
    \includegraphics[width=\linewidth]{layer_time_1.pdf}
    }
    \subfigure{
    \includegraphics[width=\linewidth]{layer_time_2.pdf}
    }
    \caption{\textbf{Feature quality depends on layers and noising scales.}
    Best features in DDAE UNets lie in the middle of the up-sampling stage, while in classical DAEs and supervised models they usually aggregate at the bottleneck. Moreover, DDAEs extract best features when images are perturbed with relatively small (but not trivial) noises. \textit{Up:} vanilla DDPM trained on CIFAR-10. \textit{Down:} DDPM++ trained on CIFAR-10 under the EDM framework.
    }
\label{fig:layer_time}
\end{figure}

To apply noise, we randomly sample $\epsilon$ and use Eq.~\ref{eq:noising} to obtain $x_t$, since our preliminary experiments show no differences between random and deterministic noising. Linear probe accuracies on features after global average pooling are examined, as illustrated earlier in Figure~\ref{fig:DDAE_2}.
Figure~\ref{fig:layer_time} shows that layers and noising scales affect feature quality jointly as a concave function, whose global maximum point can be found empirically. For input resolution of $32^2$, the best features lie in the middle of up-sampling, rather than at the bottleneck with lowest resolutions as in common practices.
Furthermore, we find that perturbing images with relatively small noises will improve the linear probe performance, especially on DDPM++ trained with EDM, which achieves {\boldmath$95\%+$} linear probe accuracy and surpasses classical AE or VAEs \cite{DRL-VDRL}. These properties have been verified across different datasets and models, but the optimal layer-noise combination may vary under different settings.

In the remainder of this paper, linear probe accuracies are reported as the highest found in grid search. For fine-tuning, clean images are passed to the partial DDAEs, which are truncated as encoders at the optimal layers. The timestep inputs are also fixed to the optimal values in grid search. We find that though simply removing all time-conditioning layers and fine-tuning the tailored network also works well, the accuracy slightly decreases by $0.1\%$ to $0.5\%$. Therefore, we report the fine-tuning results with fixed timestep inputs.

\begin{figure*}[t]
\centering
    \subfigure[Evaluation metrics of image generation and linear probing. Dots denote checkpoints at 150, 250, 500, 1000, 1500 and 2000 epochs. Images are sampled with the same seed.]{
    \includegraphics[width=0.55\linewidth]{ablation_1.pdf}
    \label{fig:denoising_1}
    }
    \hfill
    \subfigure[Illustration of ablation configurations. Noise schedules are linearly spaced with varying numbers of levels and ranges.]{
    \includegraphics[width=0.42\linewidth]{ablation_2.pdf}
    \label{fig:denoising_2}
    }
    \caption{\textbf{Correlation between generative and discriminative capabilities of DDAEs.}
    We ablate vanilla DDPM on CIFAR-10 from the perspective of denoising autoencoding. We observe that before over-fitting, better generative model learns better representations. Reduction on noise levels (shown in \textcolor[RGB]{1,121,135}{green}), noise ranges (shown in \textcolor[RGB]{125,103,75}{brown}) and training steps (denoted by dots) will weaken both performances.
    }
\label{fig:denoising}
\end{figure*}


\section{Experiments}
\label{sec:exp}
We firstly examine the impact of some core designs in diffusion models on both generative and discriminative performance through ablation studies. We then present our main results on small-sized CIFAR-10 \cite{cifar10} and medium-sized Tiny-ImageNet \cite{tiny} datasets under linear evaluation, supervised fine-tuning and ImageNet \cite{in1k} transfer settings.

All diffusion models are retrieved or trained from official (or equivalent) codebases. For image classification, we do not use regularization methods such as mixup \cite{mixup} or cutmix \cite{cutmix}, and only employ lightweight augmentations (\textit{i.e.} \texttt{RandomHorizontalFlip} and \texttt{RandomCrop}). Implementation details as well as optimal layer-noise settings in grid search are provided in the appendix.

\subsection{Main properties}
Previous research has demonstrated that in autoregressive Transformers, better generative performance links to better representations, as measured by log-likelihood and linear probe accuracy \cite{igpt}. As Fr√©chet Inception Distance (FID) \cite{fid} is a more popular metric for image generation, we propose to investigate the correlation between DDAE's generative and discriminative capabilities by plotting evaluation accuracy as a function of FID calculated on 50k samples. We conduct ablation studies from two perspectives: (\romannumeral1) denoising autoencoding, and (\romannumeral2) generative pre-training, which correspond to the two sides of denoising diffusion autoencoders as discussed in Section~\ref{sec:intro}.

\begin{figure*}[t]
\centering
    \subfigure[DDAEs in DDPM and EDM trained on CIFAR-10.]{
    \includegraphics[width=0.48\linewidth]{ssl_1.pdf}
    }
    \subfigure[DDAEs in DDPM and EDM trained on Tiny-ImageNet.]{
    \includegraphics[width=0.48\linewidth]{ssl_2.pdf}
    }
    \caption{\textbf{Generative and discriminative performance on multi-class datasets.} We evaluate DDAEs in two representative unconditional diffusion models. With improved training practice and network backbones, EDM-based DDAEs show better generative performance and promising linear probe accuracies. After fine-tuning, EDM-based encoders surpass supervised Wide ResNets.
    ``Scratch'' denotes the same truncated encoders trained from scratch, which are incompetent vision backbones and may have encumbered discriminative performance.
    }
\label{fig:generative-math}
\end{figure*}

\subsubsection{Denoising autoencoding}
Diffusion models can be viewed as multi-level denoising autoencoders. Since Figure~\ref{fig:layer_time} already confirms that DDAEs can learn strongly linear-separable features in an unsupervised manner, we wonder what design in the multi-level denoising makes DDAEs stronger representation learners than classical AEs, VAEs \cite{DRL-VDRL} or DAEs. In particular, we consider two key factors in the vanilla DDPM framework that may contribute to improved denoising pre-training: the number of noise levels ($T$) and the range of noise scales ($\beta_{1...T}$).

To investigate the effect of noise levels, we reduce the default $T=1000$ \cite{ddpm} gradually to $T=512$, $256$, and $64$. We opt not to decrease it further as DDPM cannot generate meaningful images when $T=64$. For each configuration, the noise schedule $\beta_{1...T}$ is linearly spaced in the range of $[\beta_{min}, \beta_{max}]$, where $\beta_{min}=10^{-4}$ and $\beta_{max}=0.02$, following DDPM default.
Additionally, we examine the contribution of the noise scales by evaluating performance of the larger half $[\frac{\beta_{min}+\beta_{max}}{2}, \beta_{max}]$ and the smaller half $[\beta_{min}, \frac{\beta_{min}+\beta_{max}}{2}]$ of the default range. We keep $T=1000$ in these two scenarios for fair comparison.
Figure~\ref{fig:denoising_2} provides an illustration of ablated pre-training configurations, but it only indicates relative comparisons for visualization and does not represent actual practices.
Figure~\ref{fig:denoising_1} shows the influence of the noise configurations and training steps. The complete DDPM with the maximum number of noise levels and the broadest noise scale coverage, achieves best performance in both image generation and recognition.

\textbf{Noise levels and range.} Reducing noise levels or narrowing the noise range weakens both generative and discriminative performances. Training handicapped diffusion models with only $T=256$ levels or the larger scales leads to more significant performance declines.
Interestingly, we observe that the recognition ability seems to depend less on a dense and wide noise configuration than the generation ability. As shown in Figure~\ref{fig:denoising_1}, while generation FID may suffer huge decreases, the highest linear probe accuracy drops slightly by less than 3\%. The $T=64$ model even produces stronger features than other handicapped models, despite its inability to generate meaningful images.

We attempt to explain these intriguing properties from a \textit{contrastive learning} angle.
Since the denoising parameterization (Section~\ref{sec:background}) demonstrates that diffusion networks are forced to predict the very same $x_0$ from different versions of $x_t$, where $x_t$ is derived from random sampling of $t$ and $\epsilon$, there exists an underlying constraint of \textit{alignment} on these various noisy versions. In other words, the time levels and random noises serve jointly as data augmentations, and $x_t$s are different views of $x_0$.
In this view, a denser and wider noise configuration can increase the \textit{diversity of positive samples} in contrastive learning and improve the representation quality. The strong linear probe performance in the $T=64$ model suggests that more noise levels could be an overkill for recognition-only purposes, but they still contribute significantly to generation.

Unfortunately, this diversity may require longer training duration --- Figure~\ref{fig:denoising_1} shows that models with half level counts or ranges can be well-trained at around 1000 epochs, while the full model requires 2000 for this simple CIFAR-10. EDM \cite{edm} even relies on a training period of 400 A100 GPU days to reach state-of-the-art results on $64^2$ ImageNet.
Due to the revealed duality of generation and recognition, we hope the best practices in discriminative representation learning will inspire future improvements to training efficiency in diffusion models beyond sampling, so that research can explore the scaling of diffusion models on high-resolution images more efficiently.

\textbf{Training steps.} By tracking checkpoints throughout the training duration, it can be observed that better generative model learns better representations. Moreover, we observe that while linear probe accuracy tends to overfit after 1000 epochs in DDPM training, the generation performance continues to improve, indicating that it has not saturated. Similar observations can be found in other curves (Figure~\ref{fig:denoising_1}), that recognition tends to overfit earlier than generation.

Since noise prediction can be considered as a pixel-to-pixel translation, it is reasonable to assume that DDAEs firstly learn high-level understandings from the noisy input $x_t$ at the deeper layers, and gradually learn to predict the exact pixel-level $\epsilon$ through decoding. Consequently, models may focus on fitting some imperceptible details after learning saturated semantic representations, consistent with the plateaued loss curves observed in our training progress after 1000 epochs. These properties somewhat provide support for the theory of relationship between image generation and understanding, as claimed intuitively in Section~\ref{sec:intro}.

In summary, by evaluating various checkpoints with different denoising pre-training setups, we confirm the positive correlation between the generative and discriminative capabilities of DDAEs, aligning with the findings in iGPT \cite{igpt}.

\subsubsection{Generative pre-training}
To delve deeper into improving generative pre-training for recognition, we compare DDAEs trained with two different diffusion formulations: the vanilla DDPM and the stronger EDM. With the mathematical and network improvements, we consider EDM as a better generative model than DDPM, and we examine whether it performs better in classification as well. Figure~\ref{fig:generative-math} shows their evaluation accuracies with respect to generation FIDs on CIFAR-10 and Tiny-ImageNet.

It is worth noting that the networks and hyper-parameters used for unconditional Tiny-ImageNet training are empirically determined without tuning and are not optimal, since finding the best practice is too expensive. EDM proposes to use the ADM network \cite{adm} to achieve state-of-the-art results on $64^2$ ImageNet, but it may cost 32 A100 GPU days to train on Tiny-ImageNet.
Tuning hyper-parameters and networks for different input resolutions is still an open question. Therefore, our goal is to provide further confirmation of the observations, rather than optimizing performances.

\textbf{Recognition accuracies.} Figure~\ref{fig:generative-math} demonstrates that EDM outperforms DDPM on both generative and discriminative metrics.
On CIFAR-10, while the FID improves from 3.00 to the nearly state-of-the-art 2.02, the linear probe accuracy increases dramatically from 88.5\% to 95.9\%. After supervised fine-tuning, EDM-based DDAE achieves 97.2\% recognition rate, surpassing the strong Wide ResNet-28-10 baseline \cite{wrn} with comparable parameters. This superior classification performance confirms diffusion as a meaningful self-supervised pre-training approach for recognition.
On Tiny-ImageNet, observations are similar: while the FID has limited improvements between DDPM and EDM, the recognition rates increase more significantly by 3.2\% (linear probe) and 5.3\% (fine-tuning). The fine-tuned EDM also slightly surpasses supervised Wide ResNet.
These results suggest that improving diffusion models for better generation performance will naturally lead to better recognition models due to the effective generative pre-training.

\textbf{Network backbone issues.} Although the DDPM++ network in EDM has a larger model size than vanilla DDPM, their truncated versions fail to benefit from scaling when trained from scratch on both datasets (see ``Scratch'' in Figure~\ref{fig:generative-math}). Moreover, even though we tune hyper-parameters and train them longer, these truncated UNets fail to reach comparable accuracies as ResNets and are often unstable to train. These results suggest that truncating diffusion UNets at up-sampling and appending global pooling are not optimal practices for classification. These incompetent backbones for recognition may have encumbered DDAE's performance. Using general-purpose vision backbones without up-sampling (\eg ViTs) or designing novel networks with explicit encoder-decoder split may overcome this issue.

However, current ViT-based diffusion models \cite{uvit, dit} are mainly operating in the latent space to achieve promising performance. We conduct some preliminary experiments to compare pixel-space and latent-space image classification, and we find that the latter performs consistently worse on CIFAR-10 (96.3\% \textit{v.s.}~96.0\%) and Tiny-ImageNet (69.3\% \textit{v.s.}~65.3\%), suggesting latent compression may lose some discriminative information for recognition. Moreover, the latent space may be an obstacle for other pixel-related tasks such as detection and segmentation. We leave the exploration for unified pixel-space backbones to future work.

\subsection{Comparisons with previous methods}

\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{llrr}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Evaluation}} & \multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}\textbf{Generation}\\ \textbf{FID}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}\textbf{Acc.}\\ \textbf{\%}\end{tabular}} \\
     & & & \\ \toprule
    \multicolumn{4}{l}{\textit{on CIFAR-10}} \\
    \textcolor[RGB]{128,128,128}{Wide ResNet-28-10 \cite{wrn}} & \textcolor[RGB]{128,128,128}{Supervised} & \textcolor[RGB]{128,128,128}{N/A} & \textcolor[RGB]{128,128,128}{96.3} \\
    DRL/VDRL \cite{DRL-VDRL, DRL-VDRL-time} & Non-linear & $\sim$3.0 & \textless{}80.0 \\
    HybViT   \cite{hybrid}                  & Supervised & 26.4      & 95.9 \\
    SBGC     \cite{score-class}             & Supervised & *         & 95.0 \\
    \textbf{DDAE   (EDM)}                   & Linear     & 2.0       & 95.9 \\
    \textbf{DDAE   (EDM)}                   & Fine-tune  & N/A       & 97.2 \\ \midrule
    \multicolumn{4}{l}{\textit{on Tiny-ImageNet}} \\
    \textcolor[RGB]{128,128,128}{Wide ResNet-28-10 \cite{wrn}} & \textcolor[RGB]{128,128,128}{Supervised} & \textcolor[RGB]{128,128,128}{N/A} & \textcolor[RGB]{128,128,128}{69.3} \\
    HybViT   \cite{hybrid}                 & Supervised  & 74.8      & 56.7 \\
    \textbf{DDAE   (EDM)}                  & Linear      & 19.5      & 50.0 \\
    \textbf{DDAE   (EDM)}                  & Fine-tune   & N/A       & 69.4 \\                       
    \bottomrule
    \multicolumn{4}{l}{\begin{footnotesize}* Negative log-likelihood (NLL) of 3.11 is reported. Similar model \cite{sde} achieves \end{footnotesize}} \\
    \multicolumn{4}{l}{\begin{footnotesize} \enspace 2.99 NLL and 2.92 FID, for reference.\end{footnotesize}}
    \end{tabular}
}
\caption{\label{tab:diffusion-based-represent}\textbf{Comparisons with other diffusion-based representation learning methods on CIFAR-10 and Tiny-ImageNet.}
We compare DDAEs with unsupervised DRL/VDRL, supervised hybrid model HybViT, and supervised likelihood model SBGC. All results for other methods are retrieved from their original papers.
}
\end{table}

We compare EDM-based and DiT-based DDAEs with other (\romannumeral1) diffusion-based representation learning methods, and (\romannumeral2) self-supervised methods for recognition-only purposes. Since the generative performance is our first priority, we select checkpoints with the lowest FID for diffusion models, despite the recognition performance may overfit.

\textbf{Comparisons with diffusion-based methods.} Table~\ref{tab:diffusion-based-represent} shows that EDM-based DDAE outperforms all previous supervised or unsupervised diffusion-based methods on both generation and recognition. Moreover, our DDAE can be seen as the state-of-the-art hybrid model \cite{hybrid} on CIFAR-10, which can generate and classify (through linear classifier) with a single model.
On Tiny-ImageNet, our self-supervised EDM yields significantly better generation FID than the supervised HybViT, despite a lower linear probe accuracy. After fine-tuning, DDAE catches up with supervised Wide ResNet and surpasses HybViT by large margins.

\textbf{Comparisons with contrastive learning methods.}  Table~\ref{tab:cifar10} presents the evaluation results on CIFAR-10. For linear probing, EDM-based DDAE is comparable with SimCLRs considering model sizes. After fine-tuning, EDM achieves 97.2\% (w/o transfer) and 98.1\% (w/ transfer) accuracies, outperforming SimCLRs with comparable parameters, despite underperforming the scaled 375M SimCLR model by 0.5\%.
Table~\ref{tab:tiny} presents results on Tiny-ImageNet. Our EDM-based model significantly outperforms SimCLR pre-trained ResNet-18 under both linear probing and fine-tuning settings. However, DDAE is not as efficient as SimCLR on this dataset: a slightly larger ResNet-50 can surpass our linear probe result with fewer parameters.

\textbf{Transfer learning with Vision Transformers.}
To verify transfer learning ability on scalable Vision Transformers beyond UNets, we compare ImageNet $256^2$ pre-trained DiT \cite{dit} to MAE pre-trained vanilla ViTs \cite{vit} on CIFAR-10 and Tiny-ImageNet. Since the DiT codebase only provides the largest DiT-XL/2 checkpoint for class-conditional generation, we use it in an unconditional manner by dropping label $y$ to null \cite{cfg}. Although this comparison may not be strictly fair due to the scale and supervision difference, we mainly aim to confirm the scalability of ViT-based DDAEs.

Table~\ref{tab:cifar10} and Table~\ref{tab:tiny} show that the scaled DiT-XL/2 outperforms the smaller MAE ViT-B/16 under all settings by large margins except for linear probing on CIFAR-10. It also catches up with the 375M SimCLR and achieves 98.4\% accuracy on CIFAR-10 after fine-tuning. These results indicate that similar to pixel-space ViTs, latent-space DiTs can also benefit from scaling and pre-training on larger datasets. However, diffusion pre-trained DiTs may not be as efficient as MAE pre-trained ViTs on recognition tasks, since the former is specifically designed for advanced image generation without optimizing its representation learning ability.

\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{llrr}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Evaluation}} & \multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}\textbf{Params}\\ \textbf{(M)}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}\textbf{Acc.}\\ \textbf{\%}\end{tabular}} \\
     & & & \\ \toprule
    \multicolumn{4}{l}{\textit{on CIFAR-10}} \\
    \textcolor[RGB]{128,128,128}{Wide ResNet-28-10 \cite{wrn}} & \textcolor[RGB]{128,128,128}{Supervised} & \textcolor[RGB]{128,128,128}{36} & \textcolor[RGB]{128,128,128}{96.3} \\
    \textbf{DDAE (EDM)}                 & Linear     & 36       & 95.9 \\
    SimCLR   Res-50 \cite{simclr}       & Linear     & 24       & 94.0 \\
    SimCLRv2 Res-101-SK \cite{simclr2}  & Linear     & 65       & 96.4 \\
    \textbf{DDAE (EDM)}                 & Fine-tune  & 36       & 97.2 \\
    SimCLRv2 Res-101-SK \cite{simclr2}  & Fine-tune  & 65       & 97.1 \\ \midrule
    \multicolumn{4}{l}{\textit{on CIFAR-10, with ImageNet transfer}}        \\
    \textbf{DDAE (EDM)}                 & Linear     & 36       & 91.4 \\
    SimCLR   Res-50 \cite{simclr}       & Linear     & 24       & 90.6 \\
    SimCLR   Res-50-4x \cite{simclr}    & Linear     & 375      & 95.3 \\
    \textbf{DDAE (EDM)}                 & Fine-tune  & 36       & 98.1 \\
    SimCLR   Res-50 \cite{simclr}       & Fine-tune  & 24       & 97.7 \\
    SimCLR   Res-50-4x \cite{simclr}    & Fine-tune  & 375      & 98.6 \\ \hdashline
    \textbf{DDAE (DiT-XL/2)\textsuperscript{\textdagger}} & Linear     & 314      & 84.3 \\
    MAE      ViT-B/16 \cite{MAE-report1}& Linear     & 86       & 85.2 \\
    \textbf{DDAE (DiT-XL/2)\textsuperscript{\textdagger}} & Fine-tune  & 314      & 98.4 \\
    MAE      ViT-B/16 \cite{MAE-report1}& Fine-tune  & 86       & 96.5 \\
    \bottomrule
    \multicolumn{4}{l}{\begin{footnotesize} \textsuperscript{\textdagger} Trained as class-conditional model but evaluated in an unconditional manner. \end{footnotesize}} \\
    \multicolumn{4}{l}{\begin{footnotesize} \enspace  Extra VAE encoder is used. \end{footnotesize}}
    \end{tabular}
}
\caption{\label{tab:cifar10} \textbf{Comparisons with other self-supervised methods on CIFAR-10.}
We compare DDAEs with self-supervised contrastive learning (SimCLR and SimCLRv2) and masked autoencoders (MAE). Results for SimCLRs are retrieved from the original papers, and MAE's are reported by \cite{MAE-report1}. For DDAE and MAE, only encoder parameters are taken into account.
}
\end{table}

\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{llrr}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Evaluation}} & \multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}\textbf{Params}\\ \textbf{(M)}\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}r@{}}\textbf{Acc.}\\ \textbf{\%}\end{tabular}} \\
     & & & \\ \toprule
    \multicolumn{4}{l}{\textit{on Tiny-ImageNet}} \\
    \textcolor[RGB]{128,128,128}{Wide ResNet-28-10 \cite{wrn}} & \textcolor[RGB]{128,128,128}{Supervised} & \textcolor[RGB]{128,128,128}{36} & \textcolor[RGB]{128,128,128}{69.3} \\
    \textbf{DDAE   (EDM)}               & Linear     & 40       & 50.0 \\
    SimCLR   Res-18 \cite{report2}      & Linear     & 12       & 48.8 \\
    SimCLR   Res-50 \cite{report3}      & Linear     & 24       & 53.5 \\
    \textbf{DDAE   (EDM)}               & Fine-tune  & 40       & 69.4 \\
    SimCLR   Res-18 \cite{report3}      & Fine-tune  & 12       & 54.8 \\ \midrule
    \multicolumn{4}{l}{\textit{on Tiny-ImageNet, with ImageNet transfer}} \\
    \textbf{DDAE   (DiT-XL/2)\textsuperscript{\textdagger}} & Linear & 338 & 66.3 \\
    MAE   ViT-B/16 \cite{MAE-report1}   & Linear     & 86       & 55.2 \\
    \textbf{DDAE   (DiT-XL/2)\textsuperscript{\textdagger}} & Fine-tune & 338 & 77.8 \\
    MAE   ViT-B/16 \cite{MAE-report1}   & Fine-tune  & 86       & 76.5 \\
    \bottomrule
    \multicolumn{4}{l}{\begin{footnotesize} \textsuperscript{\textdagger} Trained as class-conditional model but evaluated in an unconditional manner. \end{footnotesize}} \\
    \multicolumn{4}{l}{\begin{footnotesize} \enspace  Extra VAE encoder is used. \end{footnotesize}}
    \end{tabular}
}
\caption{\label{tab:tiny} \textbf{Comparisons with other self-supervised methods on Tiny-ImageNet.}
We compare DDAEs with SimCLR and MAE. Results for SimCLR are reported by \cite{report2, report3} which are the highest in literature without cherry picking, and MAE's are from \cite{MAE-report1}.
}
\end{table}

\section{Discussion and conclusion}
We propose diffusion pre-training as a unified approach to simultaneously learn superior generation capability and deep visual understandings, which potentially leads to the development of unified vision foundation models. Moreover, Gaussian-based denoising is compatible with convolutional networks and Vision Transformers. In contrast, masked autoencoders are challenging to apply on convnets. The comparison between DDAE and MAE on transfer learning further suggests that de-masking may not be a necessary, essential, and optimal choice for vision. However, as the first study to investigate diffusion for recognition at scale, there remain some limitations and open questions.

\textbf{Backbone issues.} Truncating DDAEs in the middle is not an elegant and optimal practice for encoders, especially on UNets with up-sampling layers. Even for ViTs without up-sampling, our approach still relies on searching to find the best representation layer.
In constrast, ideal DDAE backbones may have deterministic encoder-decoder separation without regression to generative performance. Moreover, whether latent-based networks can rival pixel-space models on more recognition tasks needs more exploration.

\textbf{Efficiency issues.} Although DDAEs can achieve comparable recognition rates to some typical self-supervised models, sometimes they rely on larger model scales and are not as efficient as pure recognition methods. Besides, diffusion models require longer training duration to achieve optimal generative performance, making them more costly to scale.

\textbf{Discriminative representation learning.} We hypothesize that contrastive learning may implicitly contribute to the discriminative properties of DDAEs through multi-level denoising, which operates similarly to augmented positive samples and encourages DDAEs to learn the alignment between noisy versions. However, the analysis in this paper is presented intuitively without much evidence. The relationship between other representation learning approaches and diffusion models remains unclear and needs more exploration.
We hope that future research will address these problems and unlock the potential of diffusion models for scalable, efficient, and unified vision learning.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\section{Implementation details}
% \thispagestyle{empty}

\textbf{Diffusion pre-training.} We follow official implementations of DDPM, EDM and DiT for generative diffusion pre-training. The networks used in DDPM and EDM are UNets based on Wide ResNet with multiple convolutional down-sampling and up-sampling stages. Single head self-attention layers are used in the residual blocks at some resolutions. For CIFAR-10, we retrieve official checkpoints\footnote{\href{https://github.com/pesser/pytorch_diffusion}{https://github.com/pesser/pytorch\_diffusion}}\footnote{\href{https://github.com/NVlabs/edm}{https://github.com/NVlabs/edm}} from their codebases. For Tiny-ImageNet, we use official (or equivalent) implementations and similar configurations to train unconditional diffusion models by ourselves. The setting is in Table~\ref{tabapp:pretrain}.
Transformer-based DiT-XL/2 pre-trained on $256^2$ ImageNet is retrieved from its official codebase\footnote{\href{https://github.com/facebookresearch/DiT}{https://github.com/facebookresearch/DiT}}, and we do not train a smaller version (\eg DiT-B/2) due to the high computational cost. The used off-the-shelf VAE model for latent compression is retrieved from Stable Diffusion\footnote{\href{https://huggingface.co/docs/diffusers}{Hugging Face/Diffusers}}, which has a down-sample factor of 8.

\textbf{Linear probing and fine-tuning.} We use very simple settings for linear probing and fine-tuning experiments (see Table~\ref{tabapp:linear} and Table~\ref{tabapp:finetune}) and we intentionally do not tune the hyper-parameters such as Adam $\beta_1$/$\beta_2$ or weight decays.
In contrast with common practices in representation learning, we do not use additional normalization layers before linear classifiers since we find it also works well.

To train latent-space DiTs for recognition efficiently, we store the extracted latent codes through the VAE encoder and train DiTs in an offline manner. We encode 10 versions of the training set with data augmentations and randomly sample one version per epoch at the training. This approach may suffer from insufficient augmentation, and increasing augmentation versions or training with online VAE encoder may improve the recognition accuracy.

\textbf{Supervised training from scratch.} In Figure~\ref{fig:generative-math}, we present recognition accuracies of truncated UNet encoders trained from scratch and compare them to supervised Wide ResNets. The setting is in Table~\ref{tabapp:scratch}. We intentionally train these supervised models for long duration (200 epochs) to reach maximum performance for fair comparisons.

\section{Layer-noise combinations in grid search}
In Section~\ref{sec:grid} we have shown that the layer-noise combination affects representation quality heavily. We perform grid searching to find a good enough, if not the best, combination for each model and dataset. For 18-step or 50-step EDM models, we train linear classifiers for 10 epochs with each layer and timestep. For 1000-step DDPM or DiT, we increase the timestep by 5 or 10 to search more efficiently. Table~\ref{tabapp:combi} shows the combinations adopted in Section~\ref{sec:exp}.

\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|cc|cc}
    \hline
    \textbf{dataset}        & \multicolumn{2}{c|}{\textbf{CIFAR-10}} & \multicolumn{2}{c}{\textbf{Tiny-ImageNet}} \\
    \textbf{model}          & \textbf{DDPM}  & \textbf{EDM} & \textbf{DDPM}   & \textbf{EDM}    \\ \hline
    architecture            & DDPM           & DDPM++       & DDPM            & DDPM++          \\
    base   channels         & 128            & 128          & 128             & 128             \\
    channel   multipliers   & 1-2-2-2        & 2-2-2        & 1-2-2-2         & 1-2-2-2         \\
    attention   resolutions & \{16\}         & \{16\}       & \{16\}          & \{16\}          \\
    blocks   per resolution & 2              & 4            & 2               & 4               \\
    full   DDAE params      & 35.7M          & 55.7M        & 35.7M           & 61.8M           \\ \hline
    pre-training   epochs   & 2000           & 4000         & 2000            & 2000            \\ \hline
    \end{tabular}
}
\caption{\label{tabapp:pretrain} \textbf{Network specifications for diffusion pre-training.}}
\end{table}


\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|lll}
    \hline
    \textbf{config}                    & \multicolumn{3}{l}{\textbf{value}}                              \\ \hline
    optimizer                          & \multicolumn{3}{l}{Adam with default momentum  \& weight decay} \\
    base   learning rate               & \multicolumn{3}{l}{1e-3}                                        \\
    learning   rate schedule           & \multicolumn{3}{l}{cosine   decay}                              \\
    batch size per GPU                 & \multicolumn{3}{l}{128}                                         \\
    GPUs                               & \multicolumn{3}{l}{4}                                           \\ \hline
    augmentations                      & \multicolumn{3}{l}{\texttt{RandomHorizontalFlip()} and}         \\
                                       & \multicolumn{3}{l}{\texttt{RandomCrop(32, 4)} for CIFAR-10 or}  \\
                                       & \multicolumn{3}{l}{\texttt{RandomCrop(64, 4)} for Tiny-ImageNet}\\ \hline
    \multirow[t]{4}{*}{training epochs}&                       & CIFAR-10         & Tiny-ImageNet        \\
                                       & DDPM                  & 10               & 20                   \\
                                       & EDM                   & 15               & 30                   \\
                                       & DiT                   & 30               & 30                   \\ \hline
    \end{tabular}
}
\caption{\label{tabapp:linear} \textbf{Linear probing setting.}}
\end{table}


\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|lll}
    \hline
    \textbf{config}                    & \multicolumn{3}{l}{\textbf{value}}                              \\ \hline
    optimizer                          & \multicolumn{3}{l}{Adam with default momentum  \& weight decay} \\
    base   learning rate               & \multicolumn{3}{l}{1e-3 (DDPM and EDM), 8e-5(DiT)}              \\
    learning   rate schedule           & \multicolumn{3}{l}{cosine   decay}                              \\
    batch size per GPU                 & \multicolumn{3}{l}{128 (DDPM and EDM), 8 (DiT)}                 \\
    GPUs                               & \multicolumn{3}{l}{4 (DDPM and EDM), 8 (DiT)}    \\ \hline
    augmentations                      & \multicolumn{3}{l}{\texttt{RandomHorizontalFlip()} and}         \\
                                       & \multicolumn{3}{l}{\texttt{RandomCrop(32, 4)} for CIFAR-10 or}  \\
                                       & \multicolumn{3}{l}{\texttt{RandomCrop(64, 4)} for Tiny-ImageNet}\\ \hline
    \multirow[t]{3}{*}{training epochs}&                       & CIFAR-10         & Tiny-ImageNet        \\
                                       & DDPM                  & 30               & 80                   \\
                                       & EDM                   & 50               & 100                  \\
                                       & DiT                   & 50               & 50                   \\ \hline
    \end{tabular}
}
\caption{\label{tabapp:finetune} \textbf{Fine-tuning setting.}}
\end{table}


\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|l}
    \hline
    \textbf{config}                    & \textbf{value}                              \\ \hline
    optimizer                          & Adam (DDAE encoder), SGD (Wide ResNet)      \\
    base   learning rate               & 5e-4 (DDAE encoder), 0.1 (Wide ResNet)      \\
    learning   rate schedule           & cosine   decay                              \\
    batch size per GPU                 & 128                                         \\
    GPUs                               & 4                                           \\ \hline
    augmentations                      & \texttt{RandomHorizontalFlip()} and         \\
                                       & \texttt{RandomCrop(32, 4)} for CIFAR-10 or  \\
                                       & \texttt{RandomCrop(64, 4)} for Tiny-ImageNet\\ \hline
    training epochs                    & 200                                         \\
    warmup epochs                      & 5                                           \\ \hline
    \end{tabular}
}
\caption{\label{tabapp:scratch} \textbf{Setting for training supervised models from scratch.}}
\end{table}


\begin{table}[]
\resizebox{\linewidth}{!}{
    \begin{tabular}{ll|ll}
    \hline
    \textbf{model} & \textbf{dataset@resolution} & \textbf{layer} & \textbf{timestep} \\ \hline
    DDPM  & CIFAR-10@32        & 7/12   (1st block@16)   & 11/1000  \\
    EDM   & CIFAR-10@32        & 6/15   (1st block@16)   & 4/18     \\
    DiT   & CIFAR-10@256       & 12/28                   & 121/1000 \\ \hline
    DDPM  & Tiny-ImageNet@64   & 2/12   (2nd block@8)    & 45/1000  \\
    EDM   & Tiny-ImageNet@64   & 7/20   (2nd   block@16) & 14/50    \\
    DiT   & Tiny-ImageNet@256  & 13/28                   & 91/1000  \\ \hline
    \end{tabular}
}
\caption{\label{tabapp:combi} \textbf{Adopted layer-noise combinations.} The numbers following ``@'' denote image or feature map resolutions.}
\end{table}

\end{document}