\section{Experiments}

In this section, we introduce the setup (Sec.~\ref{exp:setup}) and implementation details (Sec.~\ref{exp:imp}) of our experiments. Then, we present the main results (Sec.~\ref{exp:main_res}), ablation studies and discussions (Sec.~\ref{exp:abl_dis}) to demonstrate the effectiveness of our proposed methods.

\subsection{Setup}
\label{exp:setup}
We conduct our experiments on KITTI~\cite{geiger2012kitti} datasets. Commonly, KITTI contains 7,481 images for training and 7,518 images for testing. To achieve local evaluation, we follow \cite{chen20153d} to split the training set into 3,712 training and 3,769 validation samples. In our semi-supervise setting, we leavelage the unlabeled raw data to further boost the model performance. When pre-processing the data, we clean up the video clips containing validation samples and remove repeat frames in the training set. As a result, we own 33,507 unlabeled samples for semi-supervise training. When adopt search strategies to optimize the best augmentation factors, we split a small amount of data for simplicity. In our experiments, we utilize the average precision on Pedestrian and Cyclist for 3D and birdâ€™s eye view (BEV) object detection as the metrics. To prove the effectiveness of ABPA, we also calculate the recall as a crucial measurement for pseudo labels. Following \cite{simonelli2019disentangling}, all evaluation results on validation and testing set are based on AP40.



\subsection{Implementation Details}
\label{exp:imp}
We adopt the MonoDLE~\cite{ma2021monodle} and MonoCon~\cite{liu2022monocon} as two baseline detectors. As default, the ABPA augments raw images into $K=9$ different views. While the initial threshold is selected as 0.65,  the predictions with confidence score lower than 0.65 will be replenished in the center aggregation algorithm. During the retraining period, We initialize the student with the previous teacher model. In terms of CRS, we adopt a ResNet-50~\cite{he2016resnet} as the critical module. We extend the input channel and reduce the output dimension of last fully connection layer to 1. Other layers are initialized with the standard pre-trained model~\cite{deng2009imagenet}. During training, we utilize a batch size of 8 and drop out 2 samples with the lowest evaluation value $V$ when adopting CRS. We only conduct one cycle of semi-supervised training for all experiments to investigate the effectiveness of proposed methods. For fair comparison, we reproduce baseline methods based on official codes provided by the authors. While most Mono3D methods are based on single GPU when implementation, We adopt 8 A4000 GPUs for all experiments to facilitate training with a larger amount of data. Without specified, we utilize the MonoDLE~\cite{ma2021monodle} as our baseline in ablation studies since there is no performance degradation when reproducing it with 8 GPUs.


\input{tables/label_unlabel_ratio}
\input{tables/kitti_test}

\subsection{Main Results}
\label{exp:main_res}



\subsubsection{Comparison with Baselines.}

We present a detailed comparison with the fully supervised baseline under different amount of unlabeled data. Different from previous 2D detection works~\cite{xu2021softteacher,zhou2022denseteacher}, we leverage the whole labeled training set since the Mono3D performance is poor and only a small part of labeled training set cannot ensure the training stability. As depicted in Tab.~\ref{tab:label_unlabel_ratio}, our semi-supervise method significantly outperforms the baseline under each ratio settings, which demonstrates the effectiveness of the proposed self-training strategies. When using only 20\% unlabeled data (about 3350 samples), our approach gains around +1.55\% and +1.25 AP$_{3D}$ improvements on moderate level of car and pedestrian, respectively. This indicates our method is able to learn knowledge from unlabeled data, even though the number of unlabeled data is scarce as well. Moreover, when we adopt the all unlabeled data, our method is capable to outperform the baseline with a significant margin. It is possible that Mono3D can benefit from more unlabeled data within our proposed semi-supervise method.






\subsubsection{Results on KITTI Test Set.}

We evaluate our method on KITTI online benchmark with the official test set. To indicate the generalization of our method, we choose two different Mono3D detectors as baselines. As shown in Tab.~\ref{tab:kitti_test}, we present quantitative comparisons of our method with other state-of-the-art counterparts on the KITTI leaderboard. By effectively leverage the unlabeled data, our proposed semi-supervise strategy significantly boost the model performance. For instance, our approach improves MonoDLE AP$_{3D}$ Moderate by +3.32 without any other tricks like training separated models for different classifications. Notably, when based on the SoTA Mono3D method MonoDETR, we further enhance its capability with a remarkable margin of +xxx on AP$_{3D}$ Moderate, which ranks 1$^{st}$ on the KITTI online benchmark.

% \subsubsection{Results of Ped. and Cyc. on KITTI Test Set.}

\subsection{Ablation Studies and Discussions}
\label{exp:abl_dis}

In this section, we present detailed ablation studies and in-deep discussions to investigate the effectiveness of each proposed component.

\subsubsection{Effectiveness of Semi-Supervise Self-Training}

To understand how each component facilitates the Mono3D performance, we evaluate them incrementally on the baseline detector: MonoDLE~\cite{ma2021monodle} and report its performance in Tab.~\ref{tab:warmup}. The $\mathtt{Car~ Moderate~ AP_{3D}}$ starts from 13.66. When we adopt the plainest self-training strategy, the AP score is significantly raised by 2.11\% \textit{without bells and whistles}, indicating the potential of semi-supervise learning for Mono3D. Instead of directly adopting our proposed methods, we enhance the baseline with commonly used oversampling~\cite{} and strong augmentation~\cite{}. The former oversamples the labeled data to the number of unlabeled data, and the later adopt strong augmentation on samples with pseudo labels. We summarize different augmentation utilized in our implementation in Tab.~\ref{}. These two strategies improve the $\mathtt{Car~ Moderate~ AP_{3D}}$ by 0.14\% and 0.41\%, respectively. Subsequently, we adopt the ABPA and the $\mathtt{Car~ Moderate~ AP_{3D}}$ is raised by 1.24\%. Such a significant improvement validates the importance to design a robust strategy for pseudo label generation and the effectiveness of proposed ABPA. Then, we adopt the CRS to filter the harmful samples during retraining. While the improvement on $\mathtt{Car~ Moderate~ AP_{3D}}$ of 0.09\% is not prominent, it boosts the $\mathtt{Ped.~ Moderate~ AP_{3D}}$ with 1.48\%, which is even doubled up the improvement (0.78\%) from plainest self-training to the ensemble of os, sa, and ABPA. These results stand that the CRS is effective to select informative samples during retraining and improve the Mono3D performance especially for the difficult category (\textit{i.e.,} pedestrian).

\subsubsection{Effectiveness of ABPA}
\textbf{Comparison with pseudo label generation strategies.} In this experiment, we adopt different threshold $\tau$ to filter pseudo labels and retrain the model with corresponding generated data pairs. As presented in Fig.~\ref{fig:abpa}, the vanilla generation strategy suffers from a drastic degradation of $\mathtt{Recall}$ with the increasing of threshold and the performance results obtained by retraining is fluctuated. In contrast, our ABAP achieves a much more robust generation with a relatively slight decrease. Moreover, the retraining performance is much more stable than the vanilla strategy, indicating the robustness of our proposed ABPA. We select $\tau=0.65$ in other experiments since the recall drop exacerbates when the threshold surpasses 0.65.

\noindent\textbf{Effectiveness of by-products.} To validate the by-products in ABPA, we test each score independently on the baseline detector as shown in Tab.~\ref{tab:2dscore}. The score is adopted as a weight of each sample for classification loss during the retraining.

\textbf{Discussion.}

\subsubsection{Effectiveness of CRS}
\textbf{Comparison with pseudo label filtering strategies.}

\textbf{Effectiveness of critical module.}

\textbf{Discussion.}

% \input{tables/label_ratio_unlabel}

\input{tables/warmup}
% \input{tables/abpa}

\begin{figure}[t]
    \centering
    \footnotesize
    \includegraphics[width=0.98\linewidth]{images/_vs.png}
    \caption{\textbf{Effectiveness of ABPA.} We evaluate the ABPA from two respects including the improvement on pseudo label quality (recall) and semi-supervise retrained model performance.}
    \label{fig:abpa}
\end{figure}


\input{tables/abl_score2d}
% \input{tables/kitti_test_pedcyc}
\input{tables/crs}