\section{Experiments}

In this section, we first recap the experimental setup in Sec.~\ref{exp:setup}. Then, we respectively present the evaluation results (Sec.~\ref{exp:main_res}), ablation studies and analysis (Sec.~\ref{exp:abl_dis}) to demonstrate the effectiveness of the proposed methods.

\input{tables/kitti_test}
% \input{tables/kitti_test_pedcyc}

\subsection{Experimental Setup}
\label{exp:setup}
\textbf{Dataset.} We mainly conduct our experiments on KITTI~\cite{geiger2012kitti}, which contains 7,481 images for training and 7,518 images for testing. Following \cite{chen20153d}, we split the original training set into 3,712 training and 3,769 validation samples.
We picked 151 unlabeled video sequences from KITTI. After removing the duplicated samples in the training set, we obtained 33,507 unlabeled samples for semi-supervised training purposes, roughly 10 times larger than the annotated training set.
A smaller subset of the training data is held out for the TPE hyper-parameter search. It is observed that previous works on semi-supervised Mono 3D detection only evaluate KITTI~\cite{geiger2012kitti}. To further show the generality of our method, we design a toy experiment on nuScenes (see Sec.~\ref{sec:nus_exp}).  


%To prevent exposing the information of the validation set to TPE search, a smaller sub-set is separated out from the training samples for evaluating the effectiveness of a candidate hyper-parameter group.

% All the details and results about TPE will be released for reproduction. 

\textbf{Metrics.} In our experiments, we utilize the average precision (AP) as the metrics (both in 3D and bird’s eye view) to evaluate and compare different methods. To prove the effectiveness of the proposed APG, we calculate the detection recall to measure the qualities of the generated pseudo labels (see Sec.~\ref{sec:exp:subsec:APG}). Following \cite{simonelli2019disentangling}, all evaluation results on validation and test sets are based on AP@40.




% \subsection{Implementation Details}
% \label{exp:imp}

% to demonstrate the effectiveness and generality of our methodåå

\textbf{Implementation Details.} We integrate the proposed semi-supervised framework to classical Mono3D detectors MonoDLE~\cite{ma2021monodle} and MonoFlex~\cite{zhang2021monoflex}. Unless otherwise specified, the proposed APG augments an input image from the unlabeled dataset to $K=9$ different views. While the initial threshold for filtering detection boxes is set as 0.65, other predictions with confidence scores lower than 0.65 will be used in the center aggregation algorithm (see Sec.~\ref{sec:method:subsec:APG}). For the proposed CRS, we construct the critical module with ResNet-18~\cite{he2016resnet} network and modify the output dimension of the last fully connected layer to 1. Other layers are initialized with the standard pre-trained weights on ImageNet~\cite{deng2009imagenet}. Notably, the critical module is not used during inference. For a batch size of 8, we chop off the 2 samples with lowest evaluation value $v$ in CRS training. For retraining, we initialize the student model with the weights from the teacher model trained on the labeled dataset. 

% As mentioned in Sec.~\ref{Preliminary}, self-training cyclically can iterate the training of the student and teacher model for multiple times. Without loss of generality, we only iterate once. 

For fair comparisons, we reproduce the baseline methods MonoDLE~\cite{ma2021monodle} and MonoFlex~\cite{zhang2021monoflex} based on the official codes provided by the authors. While most Mono3D methods are trained on a single GPU, we adopt 8 A6000 GPUs in all experiments to facilitate training with a larger data volume. 
Ablations are conducted based on MonoDLE unless otherwise specified. Configs will be released. 

% upon publication for reproduction.

%  MonoDLE~\cite{ma2021monodle} is used as the baseline method in ablation experiments since there is no performance degradation when reproducing it with 8 GPUs.



\subsection{Main Results}
\label{exp:main_res}

\input{tables/kitti_test_pedcyc}

% \subsubsection{Results on KITTI Test Set}
We apply the proposed framework to MonoDLE~\cite{ma2021monodle} and MonoFlex~\cite{zhang2021monoflex}, and evaluate our methods on the official test and validation sets of KITTI.  Tab.~\ref{tab:kitti_test} and Tab.~\ref{tab:kitti_ped_cyc} present quantitative comparisons of our method with other state-of-the-art counterparts on the KITTI leaderboard. It shows that by effectively leveraging larger volumes of unlabeled data, our proposed semi-supervised strategy significantly boosts the performance of the baseline methods. In particular, our approach respectively improves the baseline MonoDLE $\mathtt{AP_{3D} (Mod.)}$ and $\mathtt{AP_{BEV} (Mod.)}$ by +3.32\%/+2.89\% on the test set without any tricks (\emph{e.g.,} test-time augmentation). The gains on $\mathtt{AP (Easy)}$ of our 3DSeMo$_{\text{DLE}}$ surprisingly exceeds +5\% on all metrics and data splits. When integrating our method to MonoFlex~\cite{liu2022monocon}, it achieves gains of +3.61/1.36 on $\mathtt{AP_{3D} (Easy/Mod.)}$, respectively, evidencing the generality of our framework. 

% Note that, as shown in Tab.~\ref{tab:kitti_ped_cyc}, 
% challenging categories of `pedestrian' and `cyclist' also benefit from the proposed semi-supervised framework, 
%  shows the potential of applying our method on  


% generality of our method.


% When integrating our method to MonoDETR~\cite{liu2022monocon}, it achieves gains of +xxx on $\mathtt{AP_{3D} (Mod.)}$, evidencing the effectiveness of our framework.


% To prove the generalization of our method, we choose two different Mono3D detectors as baselines, \textit{i.e.,} MonoDLE~\cite{ma2021monodle} and MonoDETR~\cite{zhang2022monodetr}.

% like training separated models for different categories~\cite{x}
% ranking the 1$^{st}$ on the KITTI online benchmark.




% It is possible that Mono3D can benefit from more unlabeled data within our proposed semi-supervise method.








% \subsubsection{Results of Ped. and Cyc. on KITTI Test Set.}

\subsection{Ablation Studies and Analysis}
\label{exp:abl_dis}

This section presents more in-depth analyses to demonstrate the effectiveness of each proposed component.




\subsubsection{Influence of the Unlabeled Data Volume}
We compare the fully supervised baseline with our semi-supervised model trained with different amounts of unlabeled data in Tab.~\ref{tab:label_unlabel_ratio}. It shows that our semi-supervised method outperforms the baseline under different unlabeled data volume, which proves that our method can effectively exploit the useful information in unlabeled data and the generated pseudo labels. Notably, even only using 20\% unlabeled data (about 3350 samples), our approach can obtain +1.55\% $\mathtt{AP_{3D} (Mod.)}$ and +1.25\% $\mathtt{AP_{BEV} (Mod.)}$ improvements on car and pedestrian, respectively. When using all the unlabeled data, our method outperforms the baseline with a favorable margin, demonstrating the profound potentials of semi-supervised learning in boosting Mono3D detection. 




% Besides, collecting more unlabeled images or videos is 

% in practical seniors,  


% Different flirom previous 2D detection works~\cite{xu2021softteacher,zhou2022denseteacher}, we leverage all labeled training samples since Mono3D performances are not satisfactory as 2D detection. A small part of the labeled training set cannot ensure its training stability. 
\input{tables/label_unlabel_ratio}
\subsubsection{Component-wise Analysis}
To understand the effect of each component, we incrementally apply the proposed APG and CRS to the baseline detector MonoDLE~\cite{ma2021monodle} with $\mathtt{Car~ AP_{3D} (Mod.)}$ of 13.66. As shown in Tab.~\ref{tab:warmup}, the vanilla self-training strategy (see Sec.~\ref{Preliminary}) improves the baseline model for 2.11\% on $\mathtt{Car~ AP_{3D} (Mod.)}$ without bells and whistles. Subsequently, we apply the proposed APG and CRS to the model, respectively. It shows that both of them can significantly improve $\mathtt{Car~ AP_{3D} (Mod.)}$ by about 2.5\%, which validates our argument that robust pseudo label generation and finding informative samples are both crucial for semi-supervised Mono3D object detection. Last but not least, while maintaining the superiority on $\mathtt{Car}$, combining the proposed APG and CRS can pre-eminently improve  $\mathtt{Ped. (Mod.)~ AP_{3D}}$ for about 1.4\%. In Mono3D object detection, the pedestrian is more challenging than car because of the much smaller object size, for which a slight prediction shift leads to drastic degradation of IoU. The pseudo labels of pedestrian thus contain much more noise. Therefore, the gains on pedestrian prove CRS's ability in adaptively selecting informative samples from severely noisy pseudo labels. 

% Instead of directly adopting our proposed methods, we first enhance the semi-supervised baseline with commonly used oversampling~\cite{yang2022st++} and strong augmentation~\cite{yang2022st++,xu2021softteacher,zhou2022denseteacher}. The former oversamples the labeled data, and the latter adopts strong augmentation on unlabeled data. These two strategies improve the $\mathtt{Car~ AP_{3D} (Mod.)}$ by 0.14\% and 0.41\%, respectively. 

\input{tables/warmup}

% Subsequently, we adopt the APG and it significantly improves $\mathtt{Car~ AP_{3D} (Mod.)}$ by 1.24\%, which validates our argument that robust pseudo label generation is crucial for semi-supervised Mono3D object detection. 
% \zy{Subsequently, we adopt the APG and CRS, respectively. They can both significantly improve $\mathtt{Car~ AP_{3D} (Mod.)}$ by about 1.2\%, which validates our argument that robust pseudo label generation and critical training are crucial for semi-supervised Mono3D object detection.}

% \zy{Last but not least, while maintaining the superiority on $\mathtt{Car}$, combining the proposed APG and CRS can pre-eminently improve  $\mathtt{Ped. (Mod.)~ AP_{3D}}$ for about 1.4\%. }


% These results stand that the CRS is effective to select informative samples during retraining and improve the Mono3D performance especially for the difficult category (\textit{i.e.,} pedestrian).

\begin{figure}[t]
    \centering
    \footnotesize
    \includegraphics[width=0.98\linewidth]{images/_vs_split_1.png}
    \caption{\textbf{Effectiveness of APG.} We demonstrate the robustness of APG on both pseudo label quality (recall) and 3D object detection performance (AP@40). Results show that our method is less sensitive to threshold change.}
    \label{fig:APG}
\end{figure}


\subsubsection{Analysis of APG}
\label{sec:exp:subsec:APG}
\textbf{Robustness.} Previous semi-supervised methods usually filter detection boxes to generate pseudo labels by applying a threshold $\tau$ on the classification score. However, as presented in Fig.~\ref{fig:APG} (the \textcolor{blue}{blue} solid line), it suffers from a drastic degradation on $\mathtt{Recall}$ when enlarging the threshold. Besides, its detection performance is sensitive to threshold change (the \textcolor{cyan}{
cyan} dotted line). In contrast, the performances of our APG (the \textcolor{red}{red} dotted lines) are more stable, which proves its robustness in generating pseudo labels. We select $\tau=0.65$ in our model based on the observation of this experiment, with which the APG can boost the $\mathtt{Car~(Mod.)~ AP_{3D}}$ for 1.06\%, as shown in Tab.~\ref{tab:2dscore}. Though we need to set an initial threshold in APG, our experiment (the \textcolor{red}{red} solid line) show that it is less sensitive to threshold change. 
% Fig.~\ref{fig:APG} also proves our previous argument that geometry-based augmentation (the \textcolor{red}{red} solid line) is superior to the content-based counterpart (the \textcolor[RGB]{61,145,64}{green} solid line) in improving the quality of the generated pseudo labels. 
Fig.~\ref{fig:APG} also proves that geometry-based augmentation (the \textcolor[RGB]{61,145,64}{green} solid line, \textit{i.e.,} geometry transform) is superior to the content-based counterpart (the \textcolor{red}{red} solid line, \textit{i.e.,} color enhencement) in improving the quality of the generated pseudo labels.
This may attribute to their different mechanism that content-based transformations only marginally modify the context, while geometry-based transformations can significantly migrate the position and scale distribution of objects, which are the common reasons for false negatives in Mono3D object detection (see Fig.~\ref{fig::teaser} again). Notably, the transformations are automatically learned by TPE, which can be effortlessly integrated into other detectors. All details, codes, and results about TPE will be released for reproduction.

 


% \noindent\textbf{[To rewrite] Transformations in APG.} We have proved the effectiveness of APG with meticulous experiments, which reimburses missed predictions with augmentations. Though we need to set a initial threshold for it, our experiments show that APG is highly robust for various thresholds. With a simple TPE algorithm, we can automatically search the optimized augmentation for the specific dataset. Interestingly, as shown in Fig.~\ref{fig:APG}, we observe that commonly utilized content-based transformations like color-jitter~\cite{} are useless, whereas geometry-based transformations like resize and crop show much affirmative effect for improving detection recall. We highlight that missing detections in Mono3D can be generally caused by complicated object geometry distribution. Since geometry-based transformations can significantly migrate the distribution, objects are re-detected with a higher score after the scaling and translation. 


\input{tables/abl_score2d}

% Our strategy is different from previous semi-supervised methods~\cite{xu2021softteacher} which only use classification score as sample weight.

\noindent\textbf{Influence of sample weight.} While APG improves the overall recall of pseudo labels, it inevitably introduces more noise to challenging categories (\textit{e.g.,} pedestrian) as shown in Tab.~\ref{tab:2dscore}. As a result, the $\mathtt{Ped.~(Mod.)~ AP_{3D}}$ drops 0.16\% (\ding{173} $v.s.$ \ding{172}). To alleviate this, we weight the loss of each unlabeled sample in the retraining phase with the by-product clues from the proposed APG (see Sec.~\ref{sec:method:subsec:APG} and Eq.~\ref{eq:weight}). As shown in Tab.~\ref{tab:2dscore}, when adopting classification score to weight samples as in previous works~\cite{xu2021softteacher}, it can improve the performance of hard category ( \ding{174} $v.s.$ \ding{173}). Introducing the clues from APG can further enhance the detection of pedestrian, obtaining 0.89\% improvement on the $\mathtt{Ped.~(Mod.)~ AP_{3D}}$ (\ding{175} $v.s.$ \ding{173}).
Threshold-free approaches, such as DenseTeacher~\cite{zhou2022denseteacher}, however, does not bring benefits to the Mono3D SSL task. The discrepancies between 2D and 3D detections is to blame for such failure.
%We also compare our strategy with DenseTeacher~\cite{zhou2022denseteacher}, which gets rid of using a threshold to filter the predicted 2D detection boxes. However, result shows that it does not benefit Mono3D SSL, which may blamed on the discrepancy between 2D and 3D detection.

% indicating that they can effectively alleviate the harmful influence during the retraining. The performance on $\mathtt{Car~ Moderate~ AP_{3D}}$ is also slightly improved with 0.18\%.




\subsubsection{Analysis of CRS}
\label{sec:exp:subsec:crs}
\textbf{Different strategies for selecting informative samples.} The proposed CRS aims to adaptively separate informative samples from noisy ones. To demonstrate the superiority of CRS, we compare against some alternative strategies which have demonstrated success in other tasks. The compared counterparts include 1) filtering samples with the quality score of pseudo labels introduced in Eq.~\ref{eq:weight}, 2) the bbox jitter proposed for 2D detection in \cite{xu2021softteacher}. We tailor the 2D box jitter strategy for 3D detection, and details are presented in the supplementary material. As shown in Tab.~\ref{tab:crs}, bbox jitter causes performance degradation because of its unreliable quality measurement for pseudo labels (\ding{173}  \textit{v.s.} \ding{172}). \ding{174} throws away unreality samples based on classification and location scores (see Eq.~\ref{eq:weight}). It shows that \ding{174} only slightly improves pedestrian detection performance, however still lagging behind our proposed CRS (\ding{176}). Besides the unreliability of detection scores and box jitter in Mono3D, another underlying reason for the advance of CRS is that \ding{172} and \ding{173} are static strategies where the filtering indicator of a sample holds along the retraining phase. Conversely, the indicator learned by the critical module changes in different retraining timestamps, as shown in Fig.~\ref{fig:reward}. It both intuitively and theoretically makes sense that the importance of a sample should be mutative in training. 

% Similar observations are also presented in recent curriculum learning works~\cite{bengio2009curriculum,jiang2018mentornet}.

% Notably, it is different from the baseline (Tab.~\ref{tab:crs} \ding{172} and Tab.~\ref{tab:2dscore} \ding{174}) that use these scores to reweight the loss of a sample during training.

% A similar motivation derived from curriculum learning is first to divide samples into different difficulty levels and progressively train the network. However, it is tricky for Mono3D to handcraft specific metrics due to the poor detection capability. In this work, we design a learnable module to serve as reliab

\input{tables/crs}
\begin{figure}[t]
    \centering
    \footnotesize
    \includegraphics[width=0.98\linewidth]{images/reward_plot_crop.pdf}
    \caption{\textbf{Adaptive Reward of CRS.} Vanilla filter-based strategies invariably drop out or preserve a samples whereas our proposed critical module predicts adaptive rewards during retraining.}
    \label{fig:reward}
\end{figure}

\noindent\textbf{Learnable or not.} The proposed CRS learns the filtering indicator with a learnable critical module (Eq.~\ref{eq:cr}). Yet intuitively, we can simply determine the contribution of a sample by the training loss before and after the model updating with Eq.~\ref{eq:reward}.  To validate the necessity of the proposed scheme, we prohibit the critical module and directly leverage the reward calculated in Eq.~\ref{eq:reward} as the indicator to select samples during retraining. As shown in Tab.~\ref{tab:crs} \ding{175}, unsurprisingly, this naive strategy degrades the overall performance because of its biased optimization objective. In particular, the strategy of \ding{175} can only access several samples during calculating the reward, lacking the global vision of the evaluation set. In contrast, the learning-based critical module encodes the knowledge of the whole dataset to its weights parameters through cyclically updating the memory bank, which
 can provide stable and effective filtering indicators for model retraining (see \ding{176}). % Tab.~\ref{tab:crs} \ding{175} directly leverages reward from Eq.~\ref{eq:reward} to choose informative samples, which is different from \ding{176} adopting a neural network (Eq.~\ref{eq:cr}) to learn the filtering indicator.
% The results show that it is inferior to use a critical module (Eq.~\ref{eq:cr}) to adaptively learn the filtering indicator (\ding{176} \textit{v.s.} \ding{175}), which evidence the effectiveness of our model. As shown in Tab.~\ref{tab:crs} (\ding{175}),      
% While filtering samples by the classification and location score brings a slight performance gain (\ding{174} \textit{v.s.} \ding{172}), our proposed CRS pre-eminently improves the model with a more prominent margin (\ding{176} \textit{v.s.} \ding{172}). The improvement on difficult category $\mathtt{Ped. (Mod.)~ AP_{3D}}$ of 1.48\% demonstrates that our CRS is capable of adaptively selecting informative samples from severely noisy pseudo labels.

%  \To validate the necessity of it, we prohibit the critical module and directly leverage the reward calculated in Eq.~\ref{eq5} as an indicator to update the detector. As shown in Tab.~\ref{tab:crs} (\ding{175}), this naive strategy cannot bring performance gains but leads to a drastic degradation, which is caused by a biased optimization target. For each step, this naive strategy can only be accessible to a small amount of data to calculate the reward, leading to severe local optima and hindering data usage efficiency. In contrast, the learning-based critical module gets the knowledge of the whole dataset through cyclical and random samples, providing stable and effective reward for model retraining.


% It is worth noting that our approach is essentially different
% from ‘leave-one-out’ (LOO) retraining. For each training sample xi
% , to compute the influence on validation loss, LOO needs
% to retrain the network by removing xi from the training set.
% Hence it needs n times of retraining to investigate all training
% samples, which is not feasible in deep learning.


% \noindent\textbf{Discarded samples in CRS.} It is worth noting that our approach is different from `leave-one-out' (LOO) retraining. For each training sample $x_i^u$, LOO needs to retrain the network by removing $x_i^u$, which is highly time-consuming and not feasible given a large amount of pseudo-labeled data. In CRS, we adopt a critical module to learn the reward for each sample, greatly simplifying the sample selection. Moreover, the reward is highly adaptive and varies in different retraining timestamps, as shown in Fig.~\ref{}. It makes sense that during different training stages, one sample can own mutative influence on the model. A similar motivation derived from curriculum learning is first to divide samples into different difficulty levels and progressively train the network. However, it is tricky for Mono3D to handcraft specific metrics due to the poor detection capability. In this work, we design a learnable module to serve as reliable indicators for retraining sample selection.

% \input{tables/kitti_test_pedcyc}

\input{tables/nus}
\subsubsection{Evaluation on nuScenes}
\label{sec:nus_exp}
It is noticed that recent semi-supervised works only evaluate KITTI~\cite{geiger2012kitti}. To further show the generalization and potential of our method, we conduct a toy experiment on nuScenes~\cite{nuscenes}. Knowing there is no established semi-supervised Mono3D prototype on nuScenes, we roughly divide the official training set of 28,130 images into two subsets: 3,375 labeled ones and 24,755 unlabeled ones. Evaluations are performed on the official validation set consisting of 6,019 images. Following the approach in \cite{kumar2022deviant}, we only consider the frontal view cars and use mean absolute error (MAE) of the depth and average precision (AP) to measure the prediction accuracy. We refer the readers to ~\cite{kumar2022deviant} for more details about the criteria. As shown in Tab.~\ref{tab:nus}, the proposed 3DSeMo shows consistent improvement across different distance ranges to ego car, again demonstrating our method's generation. We hope our attempt can drive more interest in semi-supervised 3D object detection. 

