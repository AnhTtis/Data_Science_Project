\section{Introduction}\label{sec:intro}


Audio analysis is a highly important technology with a wide range of applications, such as speech recognition \cite{gulati2020conformer, kriman2020quartznet}, acoustic scene classification \cite{ford2019deep,piczak2015environmental,mesaros2018multi}, music recognition \cite{snyder2015musan,engel2017neural} and so on. 
%
Encouraged by the great improvement in image analysis \cite{he2016deep,szegedy2015going,tan2019efficientnet}, the mainstream audio analysis methods \cite{kong2020panns,saeed2021contrastive,hershey2017cnn,gong2021psla} generally transform the audio into spectrogram firstly, and then obtain its semantic representation by CNN for further analysis. 
%
Limited to the scale of manually labelled data, recently self-supervised learning \cite{saeed2021contrastive,shor2020towards,tagliasacchi2020pre} is widely applied in audio analysis, to effectively learn better audio representation from the massive unlabelled data.
\emph{However, both CNN and existing self-supervised learning methods have limitations on audio representation learning.} 


\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/intro_1.pdf}
  \caption{\textbf{The sensitivity to positional transformations of audio spectrogram (left) and natural image (right)}. The semantic of ``\textit{tiger}'' image stays consistent before and after it is flipped, whereas the semantic of audio spectrogram changes.}
  \label{fig:intro_1}
\end{figure}

We first introduce the audio spectrogram to discover the key issues involved.
The audio spectrogram is a two-dimensional matrix and can be seen as one-channel image. However, spectrogram should not be treated as a kind of natural image, because it shows some dramatically different characteristics. 
As illustrated in Figure~\ref{fig:intro_1}, the positional transformation of an audio spectrogram usually leads to semantic change, which is different from natural images.
%
Specifically, 
%we have summarized the discrepancies between audio spectrogram and images at two aspects:
(1) the vertical axis of spectrogram represents the frequency of the audio, whose distribution reflects some inherent attributes of the acoustic source, and is the key characteristic in audio analysis. 
%
For example, increasing the voice pitch greatly while keeping the talk content, has a significant influence on speaker identification.
(2) The horizontal axis indicates the temporal information of the audio, where elements are arranged in time order. 
%That means 
If we flip a spectrogram of speech audio along time dimension, the speech content changes thoroughly. 
%
To sum up, the audio spectrogram is sensitive to both frequency and temporal change, which we call \textbf{positional transformation}.
%

However, CNN is insensitive to positional transformation.
%
% For CNN, it shows a certain spatial invariance in vision representation learning,
% which plays a positive role in image analysis but negative in audio analysis. 
In general, a fully convolutional network is a cascade of multiple Conv-Pool units and each unit is a combination of convolution layers and pooling layers. 
%
Such a cascade structure helps CNN progressively learn representation and output high-level features.
%
Inside Conv-Pool unit, CNN recognizes specific local visual patterns with convolution kernels over feature map, and then pools the feature map, overlooking the positional information. 
%
As depicted in \cref{fig:intro_2}, the two operations in Conv-Pool unit cause that the learned representation focuses on whether a specific visual pattern appears or not, while ignores its position. 
% Which means, if we scroll a spectrogram along its frequency dimension, the convolution layer outputs a feature map with equivariant local response, then the pooling layer aggregates the feature map to a scalar value, which dose not consider the specific spatial location of each pattern response. 
%
Since the positional information is essential to the semantics of audio spectrogram, we empirically consider the CNN has disadvantage on audio analysis.


% For self-supervised learning, it is usually expressed as a pre-trained CNN with unlabeled data.
% Contrastive learning \cite{chen2020simple,he2020momentum,xu2020hierarchical} is widely applied in self-supervised learning to optimize a latent space by mapping semantic-relevant pairs (positive pairs) closer and pushing semantic-irrelevant pairs (negative pairs) away.
In addition to this, existing self-supervised learning methods in audio field usually optimize a latent space by mapping semantic-relevant pairs (positive pairs) closer and pushing semantic-irrelevant pairs (negative pairs) away~\cite{saeed2021contrastive,fonseca2020unsupervised,jansen2018unsupervised}.
 % the spectrogram is usually treated as natural image and refer to the practice of image field, where a spectrogram is similar to its augmented view, but dissimilar to the examples extracted from other audios.
%
However, none of them consider about the positional transformations of audio spectrogram.

To address this issue, this paper introduces a new self-supervised learning framework based on Adversarial Sample Generation (ASG) to learn robust audio representation. The contributions can be summarized as follows:


\begin{itemize}[leftmargin=10pt]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item We introduce the \textbf{Adversarial Sample Generation} mechanism for audio contrastive learning to enhance the audio representation learning, based on the discovery of the sensitivity of audio spectrogram to positional transformations, which is different from image analysis but important to audio analysis.
Given a spectrogram, our ASG generates adversarial samples by translating or flipping it along time or frequency axis.
% considering the physical meanings of two dimensions of spectrogram, it generates adversarial samples with two types of positional transformations: (1) \emph{Location transformation on time dimension} is to drive the CNN more sensitive to the time direction;
% (2) \emph{Location transformation on frequency dimension} is to drive the CNN more sensitive to the specific frequency distribution. 
%
The generated adversarial examples share the same local visual patterns with original view, challenging CNN to pay attention to not only its appearance but also its position.
\vspace{3pt}
\item A tailored framework is introduced to learn better audio representation from a large scale unlabeled audio data, then transfer to the downstream tasks.
Following previous work, we evaluate the pre-trained representations on 9 downstream tasks under two different settings: \textit{(1) frozen}: training a single linear layer on the top of the pre-trained feature extractor; \textit{(2) fine-tuned}: end-to-end training the whole framework. 
%
The proposed ASG achieves state-of-the-art performance, which demonstrates its effectiveness.

\end{itemize}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/intro_2.pdf}
  \caption{\textbf{Illustration for the positional insensitivity of CNN}. The two spectrograms which contain same visual patterns at different positions are irrelevant to each other. However, they are encoded as same embedding after pooling operation.}
  \label{fig:intro_2}
\end{figure}