
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{figures/arch.pdf}
  \caption{\textbf{Overall architecture of our pre-training framework}. PDT stands for positive data transformation and ADT stands for adversarial data transformation. Besides the common practice that pushing away spectrogram pairs sampled from different audio recordings, our framework asks the model to push away the adversarial pairs also.
  }
  \label{fig:framework}
  \vspace{-3mm}
\end{figure*}



\section{Approach}
% We apply contrastive learning to learn general-purpose audio representation. 
% %
% It attempts to learn a latent space in which similar examples are assigned closer while dissimilar examples are pushed away. 
%
We incorporate the proposed ASG mechanism into existing contrastive learning framework, \textit{i.e.}, SimCLR \cite{chen2020simple}, to further enhance the discriminative power of the learned audio representation, as shown in \cref{fig:framework}.
%
Unlike the previous methods that only generate positive pairs, we divide the data transformation strategies into two groups: \textit{(1) Positive sample generation}, which modifies the audio stream while retain its semantic. 
\textit{(2) Adversarial sample generation}, which employs positional transformations on spectrogram to generate negative samples.
Then, the NT-Xent Loss proposed in SimCLR is employed to optimize the network. The detailed descriptions of each part are given in the following sections.



\subsection{Positive Sample Generation}
Given an audio recording, we cut out two fix-length segments randomly and perform positive data transformation on one segment. They are considered as a positive pair, denoted as $x$ and $x_{p}$ respectively. 
%Then, they are converted to log-scale spectrograms via short-time Fourier transform for further process. 

\vspace{2pt}
\noindent\textbf{Positive data transformation}. We employs three transformations to generate the positive samples: 
%
\textit{(1) Room Impulse Response}, in which we convolve an audio stream with a set of room impulse responses to simulate the situations of playing audio in different rooms. 
%
% In our experiments we use the BRIRs in ASH IR dataset\footnote{https://github.com/ShanonPearce/ASH-IR-Dataset}, which contains 785 different binaural room impulse responses. The BRIRs were measured in a variety of reverberant rooms, each containing unique acoustical properties. For each room, a set of binaural room impulse responses are provided for a range of source directions around the head on the horizontal plane. The BRIRs are provided as 2 channel WAV files with a sampling rate of 44,100Hz and we pre-process it to a single-channel wav files with a sampling rate of 16,000Hz, to make them consistent with the training audios.
%
\textit{(2) Random Tuning Volume}, which is realised by first randomly selecting a tuning factor in the volume tuning range and then multiplying the audio stream with the selected tuning factor.
%
\textit{(3) Adding White Noise}, which adds a uniform distribution with random intensity to audio stream. 
%
Some hyper-parameters used in positive sample generation are listed in Table \ref{tab:da_params}.

\input{tables/data_aug_param.tex}


% For $x$ and $x_{p}$ , adversarial sample generation further generates adversarial spectrograms with three spatial location transformations, denoted as $x_{n}$ and $x_{pn}$ respectively. In sum, given an audio recording, a quadruple $\left( x, x_{p}, x_{n}, x_{pn}\right)$ is obtained after positive and adversarial sample generation. Among all possible pairs in this quadruple, $\left( x, x_{p} \right) $, $\left( x_{p}, x \right) $, $\left( x_{n}, x_{pn} \right) $, $\left( x_{pn}, x_{n} \right)$ are considered as positive pairs and others are considered as negatives. Finally, a CNN encoder $f$ is employed to map each spectrogram into a latent representation $h$, then a multi-layer perceptron $g$ maps the representation $h$ to a new space, in which the contrastive loss is optimized. The following sections will introduce all modules in detail.





\subsection{Adversarial Sample Generation}
We convert the audio segments into spectrograms of shape $F \times T$ via Short-Time Fourier Transform (STFT), where $F$ stands for the number of frequency bins and $T$ stands for the number of time frames. 
We then transform the spectrograms to log-scale and feed them into adversarial data transformation to produce negative samples $x_{n}$ and $x_{pn}$ used for contrastive learning. 

\vspace{2pt}
\noindent\textbf{Adversarial data transformation}. As shown in \cref{fig:nda}, the positional transformations can be divided into two types according to the central axis: 
%
(1) Positional transformations along time axis. We employ \emph{Flip along Time dimension} (FT) to emphasize the unidirectionality of time dimension of spectrogram. It challenges model to be sensitive to the direction of visual pattern in time dimension.
%
(2) Positional transformations along frequency axis. We employ \emph{Filp along Frequency dimension} (FF) and \emph{Scroll along Frequency dimension} (SF) transformations to emphasize the specificity of frequency distribution.
%
For SF, we introduce a hyper-parameter $\mathcal{L}_{ms}$ as the minimal length of scrolling along frequency dimension, due to that slight jittering is insufficient to change the semantics. 
%
When training, the scrolling length $\mathcal{L}_{\text{SF}}$ is randomly selected from $[\mathcal{L}_{ms}, F-\mathcal{L}_{ms}]$ at a uniform distribution, then we scroll the spectrogram along frequency dimension over $\mathcal{L}_{\text{SF}}$ length.


\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/asg.pdf}
  \caption{Three positional transformations employed in ASG. The generated adversarial sample share same appearance of visual patterns with the input, but at different direction or position.}
  \label{fig:nda}
  \vspace{-5mm}
\end{figure}

% \begin{itemize}[leftmargin=10pt]
% \setlength{\itemsep}{0pt}
% \setlength{\parsep}{0pt}
% \setlength{\parskip}{0pt}
% \item \textbf{positional transformations along time axis}: 

% We propose the \emph{Flip along Time dimension} transformation (FT) to emphasize the unidirectionality of time dimension of spectrogram. The generated adversarial samples share the same visual patterns in spectrogram with its original view, but at an inverse order in time dimension. This transformation challenges CNN to be sensitive to the direction of visual pattern in time dimension.
% \vspace{2pt}
% \item \textbf{positional transformations along frequency axis}: 
% We propose the \emph{Filp along Frequency dimension} (FF) and \emph{Scroll along Frequency dimension} (SF) transformations to emphasize the specificity of frequency distribution. Both transformations do not destroy the local spatial structure, but change the frequency distribution of audio, which challenges CNN to be sensitive to the frequency distribution of spectrogram. 

% \end{itemize}



\subsection{Training}

% When training, given a positive spectrogram pair $\left( x, x_{p} \right)$, we first randomly select one from the all three adversarial location transformations, then we perform the selected transformation on both spectrograms in positive pair and generate two adversarial samples, denoted as $ x_{n}$ and $x_{pn}$ respectively. We consider the pair $\left( x_{n}, x_{pn} \right)$ as a positive pair too. Finally, for a $N$-size batch of audio recording, our ASG module outputs $4N$ samples, forming $4N$ positive pairs and $4N \times (4N-2) $ negative pairs in total.


Given a $N$-size batch of quadruples $(x, x_{p}, x_{n}, x_{pn})$, we reorganize them into $4N$ positive pairs, including $(x, x_{p})$, $(x_{p}, x)$, $(x_{n}, x_{pn})$, $(x_{pn}, x_{n})$. Other combinations are considered negative pairs.
%
We adopt ResNet-50 \cite{he2016deep} network as feature encoder $f$ to extract audio representation $\mathbf{h}=f\left(x\right) \in \mathbb{R}^{d_{h}}$. 
%
% Though previous work prefer to encode audios with a smaller network such as EfficientNet-B0 \cite{tan2019efficientnet}, our experiment results show that the deep CNN performs better on downstream tasks, even though lacking of enough training data. 
%
Then a multi-layer perceptron (MLP) is attached on the top of ResNet-50 as a projection head $g$, that maps $\mathbf{h}$ onto a space $\mathbf{z}=g \left( \mathbf{h} \right) \in \mathbb{R}^{d_{z}}$ for increasing flexibility, as described in SimCLR. The MLP is composed of a linear layer with output size of 4096, a batch normalization \cite{ioffe2015batch} layer, rectified linear units(ReLU) \cite{glorot2011deep} and a linear layer with output size of 256. Cosine similarity $s$ is measured, which is formulated as 

\begin{equation}
    s \left( \mathbf{z},\mathbf{z}^{\prime} \right) = \frac{\mathbf{z} ^\top}{||\mathbf{z}||} \cdot \frac{\mathbf{z}^{\prime}}{|| \mathbf{z}^{\prime}||} 
\end{equation}

The NT-Xent Loss proposed in SimCLR is utilized to optimize the parameters of encoder. For each positive pair index $\left( i, j \right)$ in batch input, the loss is calculated as 

\begin{equation}
    \mathscr{L}_{i,j} = -\log \frac{\exp \left( s \left(\mathbf{z}_{i}, \mathbf{z}_{j}\right) / \tau \right) }{\sum^{4N}_{k=1} \mathds{1}_{\left[ k \neq i \right]} \exp \left( s \left(\mathbf{z}_{i}, \mathbf{z}_{k}\right) / \tau \right) }
\end{equation}

where $\tau$ denotes as the temperature which is used to tune the scale of similarity.