\section{Experiments}



To verify the effectiveness of our ASG approach, we first learn the audio representation from the large-scale dataset (\textit{e.g.}, Audioset \cite{gemmeke2017audio} and VGGSound \cite{chen2020vggsound}) in a self-supervised manner, and then transfer the pre-trained representation to diverse downstream audio classification datasets. 
%
% We assess the performance of pre-trained audio representations by measuring the mean average precision (mAP) on Audioset and the top-1 accuracy on other datasets under two different evaluation settings: frozen evaluation and fine-tuned evaluation. The former only trains the linear classifier on top of a fixed encoder to fit downstream datasets, while the latter fine-tunes the whole framework in an end-to-end manner.
%
Following a generalized experimental setup in self-supervised learning, we measure the mean average precision (mAP) on Audioset and the top-1 accuracy on other datasets (\textit{e.g.}, LibriSpeech, voxCeleb1, VGGSound, \textit{etc.}) under two different evaluation settings: frozen evaluation and fine-tuned evaluation. The former only trains the linear classifier on top of a fixed encoder to fit downstream datasets, while the latter fine-tunes the whole framework in an end-to-end manner.




\subsection{Datasets}

\noindent\textbf{Pre-training dataset}.
%
% Audioset dataset \cite{gemmeke2017audio} is applied to pre-train the audio representation, which is a large-scale audio dataset for audio event recognition. 
% Following \cite{gemmeke2017audio}, each sample is a 10-second audio recording cut from a YouTube video. The official protocol splits the dataset into three parts: \textit{balanced train segments}, \textit{unbalanced train segments}, and \textit{eval segments}. Samples in balanced train segments and unbalanced train segments are used in our pre-training stage.
% Since partial urls have been invalid, finally only 1,793,632 samples are used, which is less than the official number 1,963,807. 
%
Following \cite{saeed2021contrastive}, we use a large-scale audio dataset Audioset \cite{gemmeke2017audio} which is usually used for audio event recognition to pre-train the audio representation. Each sample in Audioset is a 10-second audio recording cut from a YouTube video. The dataset is splited into three parts: \textit{balanced train segments}, \textit{unbalanced train segments}, and \textit{eval segments}. Samples in balanced train segments and unbalanced train segments are used in our pre-training stage.
Since partial urls have been invalid, finally only 1,793,632 samples are used, which is less than the official number 1,963,807. 
%
In order to fully verify the effectiveness of our method, we employ another dataset VGGSound \cite{chen2020vggsound} on the ablation experiment. VGGSound is designed for audio event recognition task and multi-modal audio-visual analysis tasks. It is a large-scale audio dataset that consists of 200k videos for 309 audio classes, with a fixed duration of 10s for each sample.
% In Ablation Study, we also report the performance of models pretrained on VGGSound dataset. VGGSound \cite{chen2020vggsound} is designed for audio event recognition task and multi-modal audio-visual analysis tasks. It is a large-scale audio dataset that consists of 200k videos for 309 audio classes, with a fixed duration of 10s for each sample.

\vspace{2pt}
\noindent\textbf{Downstream datasets}.
9 downstream datasets are employed to assess the performances of the pre-trained audio representation, including Audioset (AS), VGGSound (VS), librispeech (LS), Speech Commands V1 (SCV1), Speech Commands V2 (SCV2), NSynth (NS), voxCeleb1 (VC), MUSAN (MS) and ESC-50 (ESC). \textbf{Librispeech} \cite{panayotov2015librispeech} dataset is a corpus of approximately 1000 hours of English speech. The data is collected from audio of books read by 251 speakers and contains 33862 audio recordings in total. \textbf{Speech Commands V2} \cite{warden2018speech} dataset is a keyword spotting dataset with 35 spoken commands as classes. It contains 105829 examples in total with average duration of 1.0s. \textbf{Speech Commands V1} dataset is the same keyword spotting dataset with SCV2, but under different evaluation setting. It only contains ten basic commands that used in IoT or robotics applications, with the background noises as a new class. \textbf{NSynth} \cite{engel2017neural} dataset is used for musical instrument family classification with 11 family name classes. It contains 305979 samples with average duration of 4.0s. \textbf{voxCeleb1} \cite{nagrani2017voxceleb} dataset is used for speaker identification task, the same as Librispeech. It contains 153516 samples with average duration of 8.2s, collected from 1211 speakers. \textbf{MUSAN} \cite{snyder2015musan} dataset is used to differentiate audio samples across 3 classes (speech, music and noise). It contains only 2016 samples. \textbf{ESC-50} \cite{piczak2015esc} is an audio classifcation dataset consists of 2,000 5-second environmental audio recordings organized into 50 classes. 

\subsection{Implementation Details}

\noindent\textbf{Audio Pre-processing}.
Given an audio recording of arbitrary length, we decode it to a mono-channel digital audio stream, with sample rate of 16,000. Then we cut a fixed-length audio segment from it and convert the audio segment to a log-scaled spectrogram of shape $F \times T$ with window size of 32ms, hop size of 10ms and frequency range 50-8000Hz. $T$ stands for the number of frames in one spectrogram, corresponding to the duration of audio segment. $F$ stands for the number of frequency bins of spectrogram and equals to $L_{\text{FFT}} // 2 + 1$, where $L_{\text{FFT}}$ stands for the length of Fast Fourier Transform. In our experiments, $L_{\text{FFT}}=512$, corresponding to $F=257$.

\vspace{2pt}
\noindent\textbf{Pre-training Details}. 
We pre-train all representations using the LARS \cite{you2017large} optimizer with linear learning-rate scaling (\textit{i.e.}, \textit{LearningRate} $= 0.3 \times$ \textit{BatchSize} $/ 256$ ) 
and weight decay of $10^{-6}$. The number of frames $T$ is set to 300 (i.e., the shape of spectrogram input is $1 \times 257 \times 300$). We set the minimal length of scrolling $\mathcal{L}_{ms}=30$ and the temperature in contrastive loss $\tau=0.1$. We use linear warmup of learning rate and decay the learning rate with the cosine decay schedule without restarts. 
% Table \ref{tab:encoder} exhibits the input and output shape of each module of our framework in pre-training carefully. 
We train the network at batch size 512 for 50 epochs, with a linear warmup of learning rate for the first 2 epochs.
All the experiments are implemented in Python and run on Tesla-V100-32G GPU.

\vspace{2pt}
\noindent\textbf{Downstream Task Details}.
When training on downstream tasks, we split each audio into non-overlap segments and average their prediction scores as the final results. The number of frames $T$ in one segment is determined by the average duration of each dataset, following BYOL-A. We train the network at batch size 64 and a learning rate of $10^{-3}$ in both evaluation settings. For frozen evaluation, we train 100 epochs for each task and decay the learning rate by a factor of $0.1$ after 50 epochs. For fine-tuned evaluation, we train 50 epochs and decay the learning rate by a factor of $0.1$ after 25 epochs. 





\subsection{Comparison with SOTAs} 
\input{tables/sota_comp}
We compare our ASG with state-of-the-art self-supervised pre-trained models. We evaluate transfer learning performance across 8 downstream tasks and report the overall results in Table \ref{tab:sota}. 
%
Specially, for frozen evaluation, we compare our ASG approach with 7 previous self-supervised methods, including Audio2Vec (CBoW and SG) \cite{tagliasacchi2020pre}, TemporalGap \cite{tagliasacchi2020pre}, COLA \cite{saeed2021contrastive}, BYOL-A \cite{niizumi2021byol}, TRILL \cite{shor2020towards}, Bidir-CPC \cite{wang2020contrastive} and Wav2Vec2.0 \cite{baevski2020wav2vec}.
%
We use the results reported on \cite{wang2022towards} for Bidir-CPC, Wav2Vec2.0 and BYOL-A and provide the results reported on \cite{saeed2021contrastive} for other methods. 
%
It is noteworthy that the Wav2Vec2.0 is pre-trained on LibriSpeech because it doesn't perform well when pre-trained with Audioset, as described in \cite{wang2022towards}. 
%
Among these methods, TRILL, Bidir-CPC and Wav2Vec2.0 are proposed to learn representation for speech task while others tend to learn general-purpose audio representation. Bidir-CPC and Wav2Vec2.0 operate on raw audio sequence directly, which preserves the temporal structure of audio so that they are suitable for speech recognition task. 
%
From table \ref{tab:sota} we can see Wav2Vec2.0 show big superiority on speech task, surpassing our ASG by 11.6\% and 2.6\% on SCV2 and VC respectively. However, it perform much worse on other tasks. Especially on NS, Wav2Vec2.0 achieves 35.8\% performance drop compared with our ASG, which indicates that it is not discriminative enough for non-speech audio classification. 
%
Considering only spectrogram-based methods, our ASG achieves best or comparable performance on most benchmarks, demonstrating its effectiveness on audio representation learning. 
%
Specifically, our ASG achieves gains of \textbf{13.9\%} / \textbf{0.9\%} / \textbf{2.4\%} / \textbf{9.2\%} / \textbf{8.2\%} on SCV1, MS, NS, AS and ESC respectively.
%
However, we can observe a 6.5\% performance drop when compared with BYOL-A on SCV2. A reason for this observation is BYOL-A augments audio with random resize cropping, making learned representation robust to flexible temporal scales, which plays an important role on speech recognition task but is insignificant for acoustic scene classification.
%
For fine-tuned evaluation, we compare our ASG method with two state-of-the-art methods, \textit{i.e.}, COLA and SSAST \cite{gong2022ssast}. 
%
Our ASG achieves \textbf{0.6\%} / \textbf{0.6\%} / \textbf{17.6\%} / \textbf{7.9\%} / \textbf{13.4\%} / \textbf{8.5\%} improvements on SCV1, MS, VC, NS, AS and ESC respectively, which demonstrates the superiority of our method. 




\subsection{Comparison with Supervised Pre-trained Models}

\input{tables/sup_comp}
We then show the comparison results of self-supervised and supervised manners to further evaluate the transfer learning performance of our proposed ASG. 
%
For supervised pre-training, we follow PANNs \cite{kong2020panns} to train CNN on Audioset with a multi-label classification loss at batch size 1024 for 52,560 steps. 
%
We use LARS optimizer with a learning rate of 0.4 and decay the learning rate with the cosine decay schedule without restarts. Class-balanced data sampling strategy, mixup and SpecAugement \cite{park2019specaugment} methods are employed to mitigate over-fitting. 
%
For fair compaison, resnet50 is adopted as backbone and log-spectrogram is used as network input.
%
We assess their performances on 8 downstream datasets and report the results in Table \ref{tab:supervise}. 
%
Our ASG model achieves superior or competitive results on all datasets. Especially on voxCeleb1, ASG achieves \textbf{18.6\%} / \textbf{9.5\%} improvements compared with supervised pre-trained resnet50, under frozen and fine-tuned evaluation respectively. These results indicate that our self-supervised pre-trained representation is robust enough and can be used as initialization for various downstream tasks.



\subsection{Ablation Studies}


\input{tables/ablation_asg}
\noindent\textbf{Adversarial data transformation}.
We conduct experiments to clarify the contribution of each proposed adversarial data transformation. The results are shown in Table \ref{tab:asg}. Three adversarial transformations are proposed in this paper, namely Flip along Time dimension (FT), Flip along Frequency dimension (FF), Scroll along Frequency dimension (SF) respectively. 
In this section, the three transformations are separately applied to generate adversarial samples, instead of a random selection from all three transformations in ASG, to evaluate the effectiveness of each adversarial transformation. 
The result of the model pre-trained without using ASG is reported as baseline, denoted as NONE.
Besides, a model that trained with Scroll along Time axis (ST) is also evaluated as a comparison.

We pre-train all representations under the same setup and evaluate them under frozen evaluation on 5 downstream datasets. From Table \ref{tab:asg}, we can observe that:
(1) All three proposed adversarial transformations(\textit{i.e.}, FT, SF and FF), perform better than the baseline(\textit{i.e.}, NONE). Specifically, FT, SF, FF outperforms NONE by \textbf{2.2\%, 1.3\%, 2.1\%}, respectively.
It verifies that audio semantic is really sensitive to positional change, and each proposed adversarial transformation plays a positive role on audio contrastive learning.
%
(2) FT outperforms ST and achieves an \textbf{1.7\%} improvement on average, which suggests it is the direction of visual pattern rather than the absolute position in time dimension plays a decisive role in audio semantics.
%
(3) The improvement of ST is negligible compared with the other three transformations and it even makes negative effect on some tasks, such as SCV1 and NS. So it is not suitable on general-purpose audio representation learning.



\vspace{2pt}
\input{tables/ablation_mel}
\noindent\textbf{Type of spectrogram}.
We explore the effectiveness of ASG with mel-spectrogram as input and report the experimental results in Table \ref{tab:mel}. From the table we can observe that model pre-trained with ASG performs better than model pre-trained without ASG on all experiment settings, achieving \textbf{1.2\%} / \textbf{0.9\%} performance gains on average, under frozen and fine-tuned evaluation respectively. This observation is consistent with experiments using spectrogram as input, and further demonstrates that audio semantic is sensitive to the positional transformation of its spectrogram.

% Specifically, ASG-S-Model performs better than ASG-MS-Model on all tasks, achieving average improvement of 1.21\% under linear evaluation setting and 0.32\% under fine-tuning setting. S-Model performs better than MS-Model on all tasks under fine-tuning evaluation setting, and on all but LibriSpeech and voxCeleb1 tasks under linear evaluation setting, achieving average improvement of 0.99\% and 0.47\% respectively. These results suggest that the spectrogram retains more details of sound, which are critical to the audio semantic than mel-spectrogram. Besides, using ASG mechanism helps improve the performance further on all tasks under both linear and fine-tuning evaluation settings, which demonstrates that our ASG approach enhances the self-supervised audio representation.




\vspace{2pt}
\input{tables/ablation_vggsound}
\noindent\textbf{Pre-training data}.
Finally, to evaluate the generalization ability of our proposed ASG, we compare the performances of supervised pre-trained model (denoted as Sup.), self-supervised pre-trained model without ASG (denoted as w/o ASG) and self-supervised pre-trained model with ASG (denoted as w/ ASG), under two large-scale pre-train dataset, \textit{i.e.}, Audioset and VGGSound. Table \ref{tab:data} shows that models trained with ASG achieves \textbf{3.5\%} / \textbf{1.3\%} improvements on Audioset and \textbf{3.1\%} / \textbf{1.5\%} improvements on VGGSound, under frozen and fine-tuned evaluation respectively. Besides, our ASG outperforms the model trained from scratch by \textbf{2.5\%} / \textbf{2.5\%} when fine-tuning on Audioset and VGGSound respectively, which demonstrates the effectiveness of ASG.

