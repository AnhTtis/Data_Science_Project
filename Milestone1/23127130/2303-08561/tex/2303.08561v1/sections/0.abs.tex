\begin{abstract}

Existing audio analysis methods generally first transform the audio stream to spectrogram, and then feed it into CNN for further analysis. A standard CNN recognizes specific visual patterns over feature map, then pools for high-level representation, which overlooks the positional information of recognized patterns.
%
However, unlike natural image, the semantic of an audio spectrogram is sensitive to positional change, as its vertical and horizontal axes indicate the frequency and temporal information of the audio, instead of naive rectangular coordinates. 
%
Thus, the insensitivity of CNN to positional change plays a negative role on audio spectrogram encoding.
%
% That means some data augmentation strategies which are commonly used in image analysis to help avoid over-fitting and improve generalization, such as flipping and translation, play a negative role on audio representation learning. 
%
To address this issue, this paper proposes a new self-supervised learning mechanism, which enhances the audio representation by first generating adversarial samples (\textit{i.e.}, negative samples), then driving CNN to distinguish the embeddings of negative pairs in the latent space.
%
%It first constructs the negative pairs by generating adversarial examples for each spectrogram with location transformations, such as flipping and scrolling. Then we enforce the instances of negative pair should be pushed away in the latent space, driving the CNN more sensitive to the spatial location transformations. 
%
Extensive experiments show that the proposed approach achieves best or competitive results on 9 downstream datasets compared with previous methods, which verifies its effectiveness on audio representation learning.



\end{abstract}