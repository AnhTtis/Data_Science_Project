
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithm}
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx

\newfloat{algorithm}{tbp}{loa}
\algsetup{linenodelimiter=}
\renewcommand{\algorithmiccomment}[1]{/* #1 */}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicforall}{\textbf{for each}}
\renewcommand{\algorithmicreturn}{\textbf{Return:}}


% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
 \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.

\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{xcolor}
\definecolor{Ren_color}{rgb}{0.858, 0.188, 0.478}
\newcommand{\Ren}[1]{\textcolor{blue}{Ren: #1}}
%\usepackage{wrapfig}
\usepackage{amsfonts}
\usepackage{mathtools}
%\usepackage{caption}
%\usepackage{subcaption}

\newtheorem{myremark}{\bf{Remark}}
\newtheorem{mydef}{\bf{Definition}}

\DeclareMathOperator*{\minimize}{\text{minimize}}
\DeclareMathOperator*{\maximize}{\text{maximize}}

\DeclareMathOperator*{\st}{\text{subject to}}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand{\Def}[0]{\mathrel{\mathop:}=}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\cD}{\mathcal D}
\newcommand{\cZ}{\mathcal Z}
\newcommand{\din}{\mathcal D^{\texttt{ft}}}
\newcommand{\dint}{\tilde{\mathcal D}^{\texttt{ft}}}
\newcommand{\dout}{\mathcal D^{\texttt{val}}}
\newcommand{\doutt}{\tilde{\mathcal D}^{\texttt{val}}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\grad}{\nabla}
\newcommand{\egrad}{\widehat{\nabla}}
\newcommand{\task}{\mathcal{T}}
\newcommand{\rnn}{\text{{\tt RNN}}\xspace}
\newcommand{\EI}{\text{EI}}
\newcommand{\lopt}{\text{LO}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\pphi}[1]{\frac{\partial #1}{\partial \bphi}}

\usepackage{adjustbox}
\usepackage{pifont}

% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


 \newcommand{\SL}[1]{\textcolor{blue}{SL: #1}}



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
%\title{Towards Neural Network General Robustness Via Robust Mode Connectivity-Based Optimization}
\title{Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Ren~Wang,~\IEEEmembership{Member,~IEEE,}
        Yuxuan~Li,~\IEEEmembership{Student Member,~IEEE,}
        and~Sijia~Liu,~\IEEEmembership{Senior~Member,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Ren Wang is with the Department
of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago,
IL, 60616.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: rwang74@iit.edu
\IEEEcompsocthanksitem Yuxuan Li is a graduate research intern at the Trustworthy and Intelligent Machine Learning Research Lab in the Department
of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago,
IL, 60616.\protect\\ Email: lyxzcx@outlook.com.
\IEEEcompsocthanksitem Sijia Liu is with the Department of Computer Science and Engineering,
Michigan State University, East Lansing, MI 48824.\protect\\ Email: liusiji5@msu.edu.}% <-this % stops an unwanted space
\thanks{The first two authors contributed equally to this paper.}
\thanks{Corresponding author: Ren Wang.}
\thanks{This work was supported by the National Science Foundation (NSF) under Grant 2246157.}
\thanks{Under review at IEEE TPAMI.}
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Adversarial robustness is a key concept in measuring the ability of neural networks to defend against adversarial attacks during the inference phase. Recent studies have shown that despite the success of improving adversarial robustness against a single type of attack using robust training techniques, models are still vulnerable to diversified $\ell_p$ attacks. To achieve diversified $\ell_p$ robustness, we propose a novel robust mode connectivity (RMC)-oriented adversarial defense that contains two population-based learning phases. The first phase, RMC, is able to search the model parameter space between two pre-trained models and find a path containing points with high robustness against diversified $\ell_p$ attacks. In light of the effectiveness of RMC, we develop a second phase, RMC-based optimization, with RMC serving as the basic unit for further enhancement of neural network diversified $\ell_p$ robustness. To increase computational efficiency, we incorporate learning with a self-robust mode connectivity (SRMC) module that enables the fast proliferation of the population used for endpoints of RMC. Furthermore, we draw parallels between SRMC and the human immune system. Experimental results on various datasets and model architectures demonstrate that the proposed defense methods can achieve high diversified $\ell_p$ robustness against $\ell_\infty$, $\ell_2$, $\ell_1$, and hybrid attacks. Codes are available
at \url{https://github.com/wangren09/MCGR}.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Robustness, deep learning, neural network, robust mode connectivity, adversarial training, population-based optimization.
\end{IEEEkeywords}}

% make the title area
\maketitle



% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.



The last decade has seen a rapid development of deep learning techniques, which are now widely applied in areas like medical imaging \cite{sarvamangala2022convolutional}, defect detection \cite{jiwei2019bottom}, and power systems \cite{li2023physics} that require high security. As the key component of deep learning, neural networks (NNs) learn the desired mappings from a set of data. Although NNs may accurately identify the underlying relationship in the provided data, their decisions are sensitive to even little changes in the inputs, known as adversarial perturbations \cite{goodfellow2014explaining,wang2022ask}. The perturbed data are called adversarial examples, which show no difference to human eyes compared with their clean counterparts. Such a vulnerability raises questions about how users can trust these models in high-security applications \cite{madry2018towards,carlini2017towards}. 



\begin{figure*}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.85\textwidth]{Figures/RMC.png}
  \caption{{Overview of the Robust Mode Connectivity (RMC)-Oriented Adversarial Defense}. The upper level of the panel shows Phase I of the framework, illustrating that a robust path (robust to adversary types 1 and 2) in the parameter space can be found by connecting one model robust to adversary type 1 and the second model robust to adversary type 2. Selecting optimal points from the path and implementing the RMC process again can further improve robustness, as illustrated in the lower level of the panel. Phase II suggests that more adversary types can be considered by using RMC as the basic unit.} 
  \label{fig: framework}
\end{figure*}

Extensive research has lately been conducted to solve the vulnerability issue of NNs. Among all works, adversarial training and its variants have demonstrated cutting-edge performance \cite{madry2018towards,zhang2019theoretically,shafahi2019adversarial,wang2020fast}. The main idea of adversarial training is to update NNs with continuously generated adversarial examples from the clean training data. Therefore, NNs learn the adversarial distributions and are able to maintain a certain level of robustness in the inference phase. However, most of the existing works consider a single type of $\ell_p$ norm perturbation during adversarial training, resulting in the rapid decline of robustness when facing inputs with perturbations that are different from the one used in training \cite{tramer2019adversarial}. Although a few works have attempted to address the issues by training NNs with multiple $\ell_p$ norm perturbations \cite{croce2019provable,stutz2020confidence,tramer2019adversarial,maini2020adversarial,croce2022adversarial,wang2021adversarial}, they have not yet completely resolved the issue of lack of diversified $\ell_p$ robustness, most likely because traditional neural network learning mechanisms rely on optimization on a single set of parameters, and involving sampling and evaluating individual points in the search space. When dealing with multiple robustness objectives, limited search capabilities may trap conventional learning mechanisms in local minima or saddle points where the gradient is close to zero, but the loss function is not yet optimized. This can prevent the network from achieving the global minimum of our robustness objective. In contrast to conventional neural network learning methods, population-based optimization can explore a broader range of solutions by maintaining a diverse population of candidate solutions, which can lead to better overall results \cite{eiben2015evolutionary,diaz2016review,mirjalili2019evolutionary}. Additionally, population-based optimization can be used to optimize complex problems, such as the problem of achieving diversified $\ell_p$ robustness, that traditional methods cannot handle. Recent works find the mode connectivity property that a low-loss high-accuracy path connected by two well-trained models exists in the parameter space \cite{freeman2016topology,garipov2018loss}. The method of finding the path can be viewed as an accelerated population-based optimization strategy, which finds a large number of candidates on the path. However, simply adopting the method will fail in the adversarial setting.

%\SL{[Thep previous sentence is not clear. "eliminate the vulnerability?" or "has not yet completely resolved the  issue of lack of robustness." it is not clear what you meant by point-based optimization and why it has deficiencies?]}




The major goal of this work is to achieve models' diversified  $\ell_p$ robustness against perturbations generated by attacks constrained on $\ell_p$ norms (In this work we consider $p=1,2,\infty$). Spurred by the mode connectivity property and our hypothesis that the diversified  $\ell_p$ robustness of conventional training methods is limited by insufficient space search, we propose a robust mode connectivity-oriented adversarial defense that utilizes two-phase population-based optimization. In Phase I of the defense, we develop a robust mode connectivity (RMC) method by implementing the idea of mode connectivity and leveraging a multi steepest descent (MSD) algorithm \cite{maini2020adversarial} to find a path of NNs with high robustness against different perturbations, as shown in Figure~\ref{fig: framework} panel of RMC. In Phase II, RMC serves as a basic unit of a bigger regime - an RMC-based optimization, which is also illustrated in Figure~\ref{fig: framework}. RMC-based optimization selects models with high diversified  $\ell_p$ robustness from the population generated by each RMC module. To improve the efficiency of the RMC-based optimization, we further incorporate a self-robust mode connectivity (SRMC) module to RMC-based optimization. We point out that there exists a strong connection between RMC-based optimization and the human immune system.

%The proposed population-based optimization can be viewed as a computational parallel of the human immune process, which provides robustness to the human body against various threats. 



\noindent\textit{Contributions.}
We propose a robust mode connectivity-oriented adversarial defense that includes the following main contributions:

\begin{itemize}
    \item We have developed a robust mode connectivity (RMC) method that can find a path connecting two adversarially trained models with high robustness against diversified $\ell_p$ norm perturbations. (Experimental results are given in Figs~\ref{fig: adv_mc}, \ref{fig: adv_mc_l1_linf}, \ref{fig: adv_mc_l1_l2}, \ref{fig: rmc_vgg16_cifar100}, \ref{fig: adv_mc_l1_linf_2}, \ref{fig: adv_mc_l1_l2_2})
    \item Building on the multi-stage RMC, we have developed an RMC-based optimization method that generates a larger population of candidates. This method further boosts the diversified $\ell_p$ robustness of neural networks by selecting the optimal models. (Experimental results can be viewed in Figs~\ref{fig: adv_mc_opt1}, \ref{fig: adv_mc_opt12}, \ref{fig: adv_mc_opt2})
    \item We propose a self-robust mode connectivity (SRMC) module as an efficient strategy to enhance RMC-based optimization. (Experimental results are shown in Figs~\ref{fig: adv_self_rmc_opt}, \ref{fig: adv_self_rmc}, \ref{fig: adv_self_rmc2}) 
    \item We conduct a comprehensive study of the RMC and RMC-based optimization, showing that the proposed methods can achieve higher diversified $\ell_p$ robustness than the baselines. (see Table~\ref{tab: main})
\end{itemize}

\noindent The rest of this article is organized as follows. Section~\ref{sec: related_work} introduces related works on defenses against diversified $\ell_p$ norm perturbations and population-based neural network learning. In Section~\ref{sec: pre}, we provide the definition of diversified $\ell_p$ robustness, and give introductions to adversarial attack, adversarial training, and mode connectivity. Sections~\ref{sec: rmc} and \ref{sec: opt} introduce the two phases of the proposed mode connectivity-oriented adversarial defense. The RMC method is presented in Section~\ref{sec: rmc}. The RMC-based optimization and SRMC are proposed in Section~\ref{sec: opt}. Section~\ref{sec: exp} shows the experimental results. Section~\ref{sec: conclusion} concludes the article.



\section{Related Work}\label{sec: related_work}


\subsection{Defenses on Diversified $\ell_p$ Norm Perturbations} 

Among all the works, \cite{croce2019provable} is the only work that provides a provable defense, and \cite{stutz2020confidence} considers withholding specific inputs to improve model resistance to stronger attacks. \cite{tramer2019adversarial} designs the inner loss by selecting the type of perturbation that provides the maximum loss or averaging the loss on all types of perturbations. According to the report from a recent work \cite{croce2022adversarial}, \cite{maini2020adversarial} is the existing SOTA method for achieving diversified $\ell_p$ robustness.  \cite{maini2020adversarial} includes the various perturbation models within each step of the projected steepest descent in order to produce an adversary with complete knowledge of the perturbation region. Nevertheless, despite their efforts, all the aforementioned works still depend on optimizing a single set of parameters, and the challenge of addressing the deficiency in diversified $\ell_p$ robustness remains unresolved. This work solves the challenge from a population-based optimization perspective.


\subsection{Population-Based Neural Network Learning} 
Optimizing a population of neural networks instead of a single network can prevent getting stuck at local minimums and lead to improved results. In one approach, \cite{jaderberg2017population} trained multiple instances of a model in parallel and selected the best performing instances to breed new ones. \cite{cui2018evolutionary} proposed an evolutionary stochastic gradient descent method that improved upon existing population-based methods. However, such methods typically have low learning speed and neglect adversarial robustness. Inspired by the human immune system, researchers have mimicked the key principles of the immune system in the inference phase to increase the robustness and not affect the learning speed in the training phase \cite{wang2022rails}. Mode connectivity can be treated as a faster population-based learning with two ancestor models that enhances the learning efficiency in the training phase \cite{garipov2018loss}. Researchers also analyze the mode connectivity when
networks are tampered with backdoor or error-injection attacks or under the attack of a single type of perturbation \cite{zhaobridging}. To defend against multiple adversarial strategies, it would be interesting to explore incorporating different adversarial strategies into the learning of mode connectivity and generalize it for mode connectivity-based optimization. 



\section{Preliminaries}\label{sec: pre}


\subsection{Adversarial Attack with Different Input Perturbation Generators}

%Conventional training methods in deep learning rely heavily on point-based optimization. More specifically, these methods use gradient descent-based algorithms to update a single set of model parameters to learn data from $\mathcal D_0$. However, 
Recent studies show that conventional learning methods suffer from perturbed datasets ($\mathcal D_1,\mathcal D_2,\cdots, \mathcal D_S$) generated by
\begin{align}\label{eq: adv_atk}
    \displaystyle \arg\max {\mathcal L(\boldsymbol \theta; \bf x^\prime, y)}, ~~ s.t.~~ \text{Dist}_i(\bf x^\prime, \bf x) \leq \delta_i,
\end{align}
for $\forall \bf x \in \mathcal D_0$, where $\mathcal D_0$ denotes the benign dataset and $\delta_i$s are sufficient small values. This is usually referred to as an adversarial attack \cite{madry2018towards}. In this paper, we restrict distance measures $\text{Dist}_i$s to be $\ell_p, p=1,2,\infty$ norms. A practical way to solve \eqref{eq: adv_atk} is to apply gradient descent and projection $P_{\delta_i}$ that maps the perturbation $\boldsymbol \epsilon_i=\bf x^\prime - \bf x$ to a feasible set, which is usually referred to as PGD attack. We will use $\ell_p$-PGD to denote the PGD attack with the $\ell_p$ norm.

\subsection{Adversarial Training (AT)} 
The min-max optimization-based adversarial training (AT) is known  as one of the most powerful defense methods to obtain a robust model against adversarial attacks \cite{madry2018towards}. We summarize AT below: 
\begin{align}
\begin{array}{ll}
    \displaystyle \min_{\boldsymbol \theta} \mathbb E_{(\mathbf x, y) \in \mathcal D_0} [ \displaystyle \max_{\text{Dist}_i(\bf x^\prime, \bf x) \leq  \delta_i} 
 \mathcal L( \boldsymbol \theta; \mathbf x^\prime,  y ) ],
 \end{array}
 \label{eq: at}
\end{align}
Although AT can achieve relatively high robustness on $\mathcal D_i$, it does not generalize to other $\mathcal D_j, j\not=i$. Moreover, training on all $\mathcal D_i, i \in [S]$ is not scalable and will not provide robustness to all types of perturbations \cite{tramer2019adversarial}. We will use $\ell_p$-AT to denote the AT with the $\ell_p$ norm.


%Adversarial training aims to strengthen the model's robustness under worst-case scenarios \cite{madry2017towards}. The training can be implemented by using different perturbation generators, which leads to robustness bias toward single-type perturbation.



\subsection{Definition of Diversified $\ell_p$ Robustness}

We hope that models can be robust to every $\ell_p$ adversarial type in the adversarial set of concerns, and we need to give a metric to measure such robustness. Diversified $\ell_p$ Robustness (DLR) is defined as its capacity to sustain the worst type of perturbation confined by a specific level of attack power:
\begin{mydef}\label{def_gr}

For a loss function $\mathcal L$ and a benign dataset $\mathcal D_0$, the Diversified $\ell_p$ Robustness of a set of neural network parameters $\boldsymbol \theta$ is 
\begin{equation}\label{general_robustness_def}
\frac{\min_{i \in [S]} \sum_{(\bf x^\prime, y)\in \mathcal D_i} \boldsymbol{1}_{f(\bf x^\prime, \boldsymbol \theta)= y}}{|\mathcal D_i|},
\end{equation}
\end{mydef}
where $\mathcal D_i$ represents the data generated by \eqref{eq: adv_atk} with $\textup{Dist}_i$ as one of $\ell_p, p=1,2,\infty$ norms. We remark that there are other ways to define DLR. For example, the definition can be based on the worst-case sample-wise instead of worst-case data set-wise. Despite the difference, they essentially measure the same quantity, i.e., how well the model performs under various types of $\ell_p$ perturbations.


\subsection{Nonlinear Mode Connectivity}

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.42\textwidth]{Figures/MC_Path.png}
  \caption{{The path with near-constant loss found by mode connectivity in the parameter space}. The endpoints are two pre-trained models.} 
  \label{fig: mc_path}
\end{figure}

Mode connectivity is a neural network's property that local minimums found by gradient descent methods are connected by simple paths belonging to the parameter space \cite{freeman2016topology,garipov2018loss}. Everywhere on the paths achieves a similar cost as the endpoints. The endpoints are two sets of neural network parameters $\boldsymbol \theta_1, \boldsymbol \theta_2 \in \mathbb{R}^{d}$ with the same structure and trained by minimizing the given loss $\mathcal L$. The smooth parameter curve is represented using $\phi(t; \boldsymbol \theta) \in \mathbb{R}^{d}, t \in [0,1]$, such that $\phi(0; \boldsymbol \theta)=\boldsymbol \theta_1, \phi(1; \boldsymbol \theta)=\boldsymbol \theta_2$. To find a desired low-loss path between $\boldsymbol \theta_1$ and $\boldsymbol \theta_2$, it is proposed to find parameters that minimize the following expectation over a uniform distribution on the curve.
\begin{equation}
\min_{\boldsymbol \theta} \mathbb E_{t \sim q(t; \boldsymbol \theta)}  \displaystyle \mathbb E_{(\bf x,y) \sim \mathcal D_0}  \mathcal L( \phi(t; \boldsymbol \theta); (\bf x,y)),
\label{eq: mc_original}
\end{equation}
where $q(t; \boldsymbol \theta)$ represents the distribution for sampling the parameters on the path. Note that \eqref{eq: mc_original} is generally intractable. A computationally tractable surrogate is proposed as follows 
\begin{equation}
\min_{\boldsymbol \theta} \mathbb E_{t \sim U(0,1)}  \displaystyle \mathbb E_{(\bf x,y) \sim \mathcal D_0}  \mathcal L( \phi(t; \boldsymbol \theta); (\bf x,y)),
\label{eq: mc}
\end{equation}
where $U(0,1)$ denotes the uniform distribution on $[0,1]$. Two common choices of $\phi(t; \boldsymbol \theta)$ in nonlinear mode connectivity are the Bezier curve \cite{farouki2012bernstein} and Polygonal chain \cite{gomes2012computer}. As an example, a quadratic Bezier curve is defined as $\phi(t; \boldsymbol \theta) = (1-t)^2 \boldsymbol \theta_1 + 2t(1-t)\boldsymbol \theta + t^2 \boldsymbol \theta_2$. Training neural networks on these curves provides many similar-performing models on low-loss paths. As shown in Fig.~\ref{fig: mc_path}, a quadratic Bezier curve obtained from \eqref{eq: mc} connects the upper two models along a path of near-constant loss.








\section{Phase I: Robust Path Search Via Robust Mode Connectivity}\label{sec: rmc}



\subsection{A Pilot Exploration}
Mode connectivity and adversarial training seem to be two excellent ideas for achieving high DLR that has been defined in Definition~\ref{def_gr}. If we set $\phi(0; \boldsymbol \theta)$ and $\phi(1; \boldsymbol \theta)$ to be two adversarially-trained neural networks under different types of perturbations, applying \eqref{eq: mc} may result in a path with points having high robustness for all these perturbations. Thus we ask:

\noindent \textit{\textbf{(Q1)} Can simply combining adversarial training with mode connectivity provide high DLR?}


Here we aim to see if implementing vanilla mode connectivity can bring us high DLR. We combine two PreResNet110 models \cite{he2016identity}, one trained with $\ell_\infty$-AT ($\delta=8/255$, 150 epochs) and the other trained with $\ell_2$-AT ($\delta=1$, 150 epochs), to find the desired path using the vanilla mode connectivity \eqref{eq: mc}. $\phi(0; \boldsymbol \theta)$ and $\phi(1; \boldsymbol \theta)$ are models trained by $\ell_\infty$-AT and $\ell_2$-AT, respectively. The mode connectivity curve is obtained with additional 50 epochs of training. The results are shown in Figure~\ref{fig: std_mc}. The left (right) endpoint represents the model trained with $\ell_\infty$-AT ($\ell_2$-AT). One can see that the path has high loss and low robust accuracies on both types of attacks, indicating that vanilla mode connectivity fails to find the path that enjoys high DLR.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.5\textwidth]{Figures/vanilla_mc.png}
  \caption{{The vanilla mode connectivity \eqref{eq: mc} with models trained by $\ell_\infty$-AT and $\ell_2$-AT as two endpoints fails to find the path with high DLR}. $\phi(0; \boldsymbol \theta)$ and $\phi(1; \boldsymbol \theta)$ are $\ell_\infty$-AT ($\delta=8/255$, 150 epochs) and $\ell_2$-AT ($\delta=1$, 150 epochs). } 
  \label{fig: std_mc}
\end{figure}

\subsection{Embedding Adversarial Robustness to Mode Connectivity}

Although the vanilla mode connectivity aims to provide insight into the loss landscape geometry, it searches space following the original data distribution. Therefore it cannot provide high DLR by simply using two adversarially-trained models as two endpoints. Instead, we ask:

\noindent \textit{\textbf{(Q2)} Can we develop a new method to embed adversarial robustness to mode connectivity by searching the adversarial input space?}

To answer (Q2), we connect mode connectivity \eqref{eq: mc} with adversarial training under diversified $\ell_p$ adversarial perturbations. In other words, we modify the objective \eqref{eq: mc} to adjust to our high DLR purpose. An adversarial generator is added as an inner maximization loop. We adopt different types of perturbations in the generator. This is because a single type of perturbation may result in robustness bias. Formally, we obtain a model path $\phi(t; \boldsymbol \theta), t \in [0,1]$ parameterized by $\boldsymbol \theta$.  
%\SL{[it is not clear how to interpret $\btheta$ in the following problem. You assume that only non-linear model connectivity is considered, right?]}
% \begin{equation}
% \min_{\boldsymbol \theta} \mathbb E_{t \sim U(0,1)}  \displaystyle \mathbb E_{(\bf x^\prime,y) \sim \cup_{i \in I}\mathcal D_{i} } \max \mathcal L( \phi(t; \boldsymbol \theta); (\bf x^\prime,y)),
% \label{eq: mc_adv}
% \end{equation}
\begin{equation}
\min_{\boldsymbol \theta} \mathbb E_{t \sim U(0,1)}  \displaystyle \mathbb E_{(\bf x,y) \sim \mathcal D_0 } \sum_{i \in I} \max_{\text{Dist}_i(\bf x^\prime, \bf x) \leq  \delta_i} \mathcal L( \phi(t; \boldsymbol \theta); (\bf x^\prime,y)),
\label{eq: mc_adv}
\end{equation}
where $\phi(0; \boldsymbol \theta)$ and $\phi(1; \boldsymbol \theta)$ are two models trained by \eqref{eq: at}, probably under different types of perturbations. Throughout this paper, we fix the curve as a quadratic Bezier curve. Thus a model at the point $t$ can be represented by $\phi(t; \boldsymbol \theta) = (1-t)^2 \boldsymbol \theta_1 + 2t(1-t)\boldsymbol \theta + t^2 \boldsymbol \theta_2$. Similar to the nonlinear mode connectivity, \eqref{eq: mc_adv} is a computationally tractable relaxation by directly sampling $t$ from the uniform distribution $U(0,1)$ during the optimization. Data points $(\bf x^\prime,y)$ are generated from a union of adversarial strategies $i \in I$, where $I$ is a subset of $\{1,2, 3, \cdots, S\}$. For example, data can be generated by using $\ell_2$ or $\ell_\infty$ norm distance measure, which is commonly used in adversarial attacks and adversarial training. Formulation in \eqref{eq: mc_adv} guarantees that the path we found always adjusts to those concerned adversarial perturbations. 

We call \eqref{eq: mc_adv} the Robust Mode Connectivity (RMC), which serves as the first defense phase for robust path search. We remark that RMC itself is a defense method as we can select the model with the highest DLR in the path. One can see that a group of models (all points in the path) are generated from two initial models. Therefore RMC is a population-based optimization. The next step is to find out how to solve the RMC \eqref{eq: mc_adv}.
%\SL{[If \eqref{eq: mc_adv} has the complexity similar to robust training, then it is a model training problem. I am not sure if RMC is a good name for it. I believe there could be confusion between model training and training a model path parameterized by $\btheta$ for non-linear model connectivity. It will be better to carefully differentiate them.]}


\begin{algorithm}[h]
\caption{Robust Mode Connectivity}
\label{alg: RMC}
\begin{algorithmic}[1]
\REQUIRE $\phi(0; \boldsymbol \theta)$, $\phi(1; \boldsymbol \theta)$ - two selected models with the same structure (potentially trained with different strategies, e.g., AT under different perturbation types); initial model $\boldsymbol \theta^0$; the perturbation types $i \in I$ and the corresponding projections $P_{\delta_i}$; training set $\mathcal D_0$; inner loop iteration number $J$; batch size $B$; initial perturbation $\epsilon^{(0)}=\mathbf 0$.
\STATE{$\boldsymbol \theta = \boldsymbol \theta^0$.}
\STATE{\textbf{For} each data batch $\mathcal D_b \in \mathcal D_0$ in each epoch $e \in E$, \textbf{do}}
\STATE{Uniformly select $t \sim U(0,1)$.}
\STATE{\textbf{For} $\forall \bf x \in \mathcal D_b$, \textbf{do}}
\FOR{$j = 1, \cdots, J$}
\FOR{$i \in I$}
\STATE{$\boldsymbol \epsilon_i^{(j)} \leftarrow P_{\delta_i}\big(\boldsymbol \epsilon^{(j-1)} - {\nabla_{\boldsymbol \epsilon}\mathcal L(\phi(t; \boldsymbol \theta); \bf x + \boldsymbol \epsilon^{(j-1)}, y)}\big)$.}
\ENDFOR
\STATE{$\boldsymbol \epsilon^{(j)} \leftarrow \arg\max_{\boldsymbol \epsilon_i^{(j)}, i \in I} {\mathcal L(\phi(t; \boldsymbol \theta); \bf x + \boldsymbol \epsilon_i^{(j)}, y)}$.}
\ENDFOR
\STATE{\textbf{end For}}
\STATE{$\boldsymbol \theta \leftarrow \boldsymbol \theta - \nabla_{\boldsymbol \theta} \sum_{\bf x \in \mathcal D_b} \mathcal L(\phi(t; \boldsymbol \theta); \bf x + \boldsymbol \epsilon^{(j-1)}, y)$}
\STATE{\textbf{end For}}
\RETURN $\boldsymbol \theta$, $\phi(t; \boldsymbol \theta), \forall t \in [0,1]$
\end{algorithmic}
\end{algorithm}


\begin{figure*}[ht]
  \centering
             \subfloat[Epoch=50]{\includegraphics[trim=90 5 90 100,clip,width=0.32\textwidth]{Figures/pgd-pgd2-50.png}}
             \subfloat[Epoch=100]{\includegraphics[trim=90 5 90 100,clip,width=0.32\textwidth]{Figures/pgd-pgd2-100.png}}
             \subfloat[Epoch=150]{\includegraphics[trim=90 5 90 100,clip,width=0.32\textwidth]{Figures/pgd-pgd2-150.png}}
  \caption{{The RMC \eqref{eq: mc_adv} with models trained by $\ell_\infty$-AT and $\ell_2$-AT as two endpoints can find the path with high DLR}. MSD \cite{maini2020adversarial} with perturbations generated by $\ell_2$ and $\ell_\infty$ norm distance measures is leveraged as the inner solver. Solving \eqref{eq: mc_adv} uses 50/100/150 epochs in panel (a)/(b)/(c).} 
  \label{fig: adv_mc}
\end{figure*}


\subsection{Solving Robust Mode connectivity Via Multi Steepest Descent}

Solving \eqref{eq: mc_adv} is difficult as it contains multi-type perturbations. The simplest ways are using `MAX' or `AVG' strategy proposed in \cite{tramer2019adversarial}, where the inner loss is obtained by selecting the type of perturbation that provides the maximum loss or averaging the loss on all types of perturbations. However, both strategies consider perturbations independently. We leverage a Multi Steepest Descent (MSD) approach that includes the various perturbation models within each step of the projected steepest descent in order to produce a PGD adversary with complete knowledge of the perturbation region \cite{maini2020adversarial}. The key idea is to simultaneously maximize the worst-case loss overall perturbation models at each step. Algorithm~\ref{alg: RMC} shows the details, where all the perturbation types are considered in each iteration. Next we test the effectiveness of the proposed RMC algorithm.

We again use models trained with $\ell_\infty$-AT ($\delta=8/255$, 150 epochs) and $\ell_2$-AT ($\delta=1$, 150 epochs) as two endpoints $\phi(0; \boldsymbol \theta)$ and $\phi(1; \boldsymbol \theta)$. The RMC \eqref{eq: mc_adv} with MSD as the inner solver is applied to obtain the path. Figure~\ref{fig: adv_mc} shows the results of training an additional 50/100/150 epochs with $D_{i}$s generated by $\ell_2$ and $\ell_\infty$ norm distance measures. One can find that unlike Figure~\ref{fig: std_mc}, the paths contain points with high accuracy and high robustness against both $\ell_\infty$-PGD and $\ell_2$-PGD attacks. Although the left endpoint has low $\ell_2$ robustness and the right endpoint has relatively low $\ell_\infty$ robustness, they can achieve high DLR in the connection, where the highest DLR is $48.19\%$ in panel (a). One can also notice that when the epoch number for solving \eqref{eq: mc_adv} increases, the path becomes smoother. Another finding is that the optimal points in panels (a), (b), and (c) have similar DLR. The experiments indicate that RMC can find a path with high DLR. If the goal is to select an optimal model from the path, it is enough to only conduct the training with a small number of epochs.

% \begin{figure*}[ht]
%   \centering
%   \begin{subfigure}{0.32\textwidth}
%              \includegraphics[trim=90 5 90 100,clip,width=1\textwidth]{Figures/pgd-pgd2-50.png}
%              \caption{Epoch=50}
%   \end{subfigure}
%   \begin{subfigure}{0.32\textwidth}
%              \includegraphics[trim=90 5 90 100,clip,width=1\textwidth]{Figures/pgd-pgd2-100.png}
%              \caption{Epoch=100}
%   \end{subfigure}
%     \begin{subfigure}{0.32\textwidth}
%              \includegraphics[trim=90 5 90 100,clip,width=1\textwidth]{Figures/pgd-pgd2-150.png} 
%              \caption{Epoch=150}
%   \end{subfigure}
%   \caption{{\bf The RMC \eqref{eq: mc_adv} with models trained by $\ell_\infty$-AT and $\ell_2$-AT as two endpoints can find the path with high DLR}. MSD \cite{maini2020adversarial} with perturbations generated by $\ell_2$ and $\ell_\infty$ norm distance measures is leveraged as the inner solver. Solving \eqref{eq: mc_adv} uses 50/100/150 epochs in panel (a)/(b)/(c).} 
%   \label{fig: adv_mc}
% \end{figure*}











\section{Phase II: Robust Model Selection Via Robust Mode Connectivity-Based Optimization}\label{sec: opt}

\begin{algorithm}[h]
\caption{Robust Mode Connectivity-Based Optimization ($\ell_1, \ell_2, \ell_\infty$ perturbations)}
\label{alg: RMC_opt}
\begin{algorithmic}[1]
%\REQUIRE .
\STATE{Train three models for $T$ epochs using $\ell_1, \ell_2, \ell_\infty$ perturbations, respectively. (Training can be accelerated using the SRMC module proposed in Section~\ref{sec: srmc})}
\STATE{Apply Algorithm~\ref{alg: RMC} with $\ell_2, \ell_1$-AT trained models ($I$ includes $\ell_2, \ell_1$) and $\ell_\infty, \ell_1$-AT trained models ($I$ includes $\ell_\infty, \ell_1$) as $\phi(0; \boldsymbol \theta)$, $\phi(1; \boldsymbol \theta)$, and return model trajectories $\phi_{\boldsymbol \theta-\ell_\infty}(t), \phi_{\boldsymbol \theta-\ell_2}(t), \forall t \in [0,1]$. (pairs of perturbations can be selected in different ways)}
\STATE{Randomly pick points $t_{-\ell_\infty}$, $t_{-\ell_2}$ from  optimal regions for each model trajectory.}
\STATE{Train models for $T$ epochs using $\ell_\infty, \ell_2$ perturbations starting from $\phi_{\boldsymbol \theta-\ell_\infty}(t_{-\ell_\infty}), \phi_{\boldsymbol \theta-\ell_2}(t_{-\ell_2})$, respectively.}
\STATE{Apply Algorithm~\ref{alg: RMC} with the two models as $\phi(0; \boldsymbol \theta)$, $\phi(1; \boldsymbol \theta)$ with $I$ including $\ell_1, \ell_2, \ell_\infty$ perturbations.}
\STATE{Find the optimal point $t_{\text{opt}}$ from the model trajectory.}
\RETURN $\phi_{\boldsymbol \theta}(t_{\text{opt}})$
\end{algorithmic}
\end{algorithm}

\subsection{From RMC to RMC-Based Optimization}

Suppose we have neural networks that share the same structure but are trained with different settings, e.g., different types of perturbations, perturbation magnitudes, learning rates, batch size, etc. In that case, we can use RMC to search for candidates potentially leading us to better solutions or even global optimums. The intuition behind the claim is that low-loss \& high-DLR paths connect all the minimums, and thus it becomes easier for search algorithms to jump out of the sub-local minimums. We have seen the exciting property of the proposed RMC, which indicates that a larger population of NNs can result in higher DLR. Notice that RMC can serve as a component in a larger population-based optimization to select robust models with higher DLR. A natural question to ask is:

\noindent \textit{\textbf{(Q3)} Can we develop a general population-based optimization method built on RMC modules to further improve the DLR of a single RMC?}



The RMC-based optimization we developed below includes an evolving process of RMC units for multiple generations. As a starting point, we generate an initial population by training neural networks with data points augmented using different $\text{Dist}_i$ in \eqref{eq: at}. 
We use gradient descent to train these networks but pause the training when specific stop criteria have been met, e.g., the number of epochs or accuracy achieving the preset threshold. The initial population then varies, and the system selects candidates with the best performances. The two operations in our approach are unified through the RMC that connects two adversarially-trained neural network models on their loss landscape using a high-accuracy \& high-DLR path characterized by a simple curve. Candidates for the next generation are selected among the high DLR points on the curve. The process can be repeated and an optimal solution that enjoys the highest DLR is selected among the final candidates. 

Algorithm~\ref{alg: RMC_opt} shows the pipeline using an example of three types of perturbations. We first train three models with $\ell_\infty$-AT, $\ell_2$-AT, and $\ell_1$-AT for $T$ epochs. We then connect the $\ell_2$-AT model with the $\ell_1$-PGD model and connect the $\ell_\infty$-AT model with the $\ell_1$-AT model using the RMC for some additional epochs. The two model trajectories are denoted by $\phi_{\boldsymbol \theta-\ell_\infty}(t)$ and $\phi_{\boldsymbol \theta-\ell_2}(t), \forall t \in [0,1]$. Notice that the curves will not be perfectly flat. But there exist some regions containing points with high DLR. We will randomly pick a model from a small optimal region in each curve. In practice, we will find the point with the highest DLR and randomly pick a point around the optimal point. The rationale behind this is to increase diversity during the training. After obtaining the models $\phi_{\boldsymbol \theta-\ell_\infty}(t_{-\ell_\infty})$ and $ \phi_{\boldsymbol \theta-\ell_2}(t_{-\ell_2})$ from both trajectories, the two new endpoints are obtained by training each model $T$ epochs using the $\ell_p$-AT that is different from the types used in the previous RMC. In this specific case, we use $\ell_\infty$-AT and $\ell_2$-AT. Finally, we connect the two new endpoints with RMC for some additional epochs and find the new optimum $\phi_{\boldsymbol \theta}(t_{\text{opt}})$ at $t_{\text{opt}}$. In the case of two types of perturbations, one can start to train two models from a single optimal point or train one model from each of the two optimal points. We refer readers to Section~\ref{subsec: rmc_opt} for more details. In a more general scenario where there are $S$ types of perturbations, the process is the same, except that it contains more RMC units, as illustrated in Figure~\ref{fig: framework}. We learn optimal points from pairs of models trained by AT under different perturbations and finally find an optimal point with the highest DLR.



\subsection{Improving Learning Efficiency of the RMC-Based Optimization With a Self-Robust Mode Connectivity Module}\label{sec: srmc}

One drawback of the previous scheme is that it needs to initially pre-train multiple neural networks ($\geq S$ models), which could be slow when the computational resources are limited. We ask:

\noindent \textit{\textbf{(Q4)} How can we accelerate the learning process of the RMC-based optimization?}


Here we propose to replace RMC with a self-robust mode connectivity (SRMC) module in the learning process. SRMC can accelerate the endpoints training in the path search process and thus speed up RMC-based optimization. We start with one set of neural network parameters $\phi(0; \boldsymbol \theta)=\boldsymbol \theta_1$ that is trained by \eqref{eq: at} with a fixed $\text{Dist}_i$. After the model achieves high robustness on $\mathcal D_i$, we retrain the model for a few epochs using \eqref{eq: at} with $\text{Dist}_j$. The new model we obtained will be placed at the endpoint $\phi(1; \boldsymbol \theta)=\boldsymbol \theta_2$. Now the low-loss high-robustness path can be found by optimizing \eqref{eq: mc_adv}. By leveraging the SRMC module, our proposed framework yields both high DLR and learning efficiency.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/srmc_immune.png}
  \caption{{RMC-based optimization with SRMC modules can be viewed as a computational parallel of the human immune system}. After encountering a pathogen, the immune system initiates the proliferation of B cells through somatic hypermutation, whereby a larger population of B cells is generated. Then B cells with higher matching scores to the pathogen are chosen to create new B cells. Through a process similar to the immune process, RMC-based optimization with SRMC modules generates models from a single model and finds the optimal path by connecting a pair of models. Subsequently, models with higher DLR are selected as candidates.} 
  \label{fig: immune}
\end{figure}


\noindent\textbf{Comparison with the human immune system.} RMC-based optimization with SRMC can also be viewed as a computational parallel of the human immune system \cite{wang2021immuno}, as illustrated in Fig.~\ref{fig: immune}. When the immune system is exposed to a pathogen, B cells undergo somatic hypermutation to proliferate and form a larger population. B cells with higher matching scores to the pathogen are then chosen to produce new B cells, which is analogous to the RMC-based optimization process with SRMC modules. In this optimization process, hypermutation represents the creation of models from a single model and the identification of the optimal path through connecting a pair of models. The selection of candidates with higher DLR follows a similar pattern to the immune system's selection of effective defenses against diverse pathogens. Consequently, like the immune system, RMC-based optimization can produce models that are resistant to a range of attacks.





\section{Experimental Results}\label{sec: exp}

% \begin{figure*}[h]
%   \centering
%   \begin{subfigure}{0.35\textwidth}
%              \includegraphics[trim=00 0 90 10,clip,width=1\textwidth]{Figures/pgd-pgd1-50.png}
%              \caption{Endpoints training epoch=50}
%   \end{subfigure}
%     \begin{subfigure}{0.35\textwidth}
%              \includegraphics[trim=00 0 90 10,clip,width=1\textwidth]{Figures/pgd-pgd1-150-50.png} 
%              \caption{Endpoints training epoch=150}
%   \end{subfigure}
%   \caption{{\bf Curves obtained from two models trained by $\ell_1$-AT and $\ell_\infty$-AT contain points with higher DLR on $\ell_1$ and $\ell_\infty$-PGD attacks}. The paths are obtained by training 50 epochs. The two endpoints are trained for 50 epochs (left panel) and 150 epochs (right panel), respectively.} 
%   \label{fig: adv_mc_l1_linf}
% \end{figure*}

Figures~\ref{fig: std_mc}, \ref{fig: adv_mc} show that using the proposed RMC can find a path with points in it enjoying high robustness on diversified $\ell_p$ perturbations. In this section, we conduct more comprehensive experiments on two phases of the Robust Mode Connectivity-Oriented Adversarial Defense.








\subsection{Settings}

We validate our proposed methods on CIFAR-10 and CIFAR-100 datasets  \cite{krizhevsky2009learning} using PreResNet110 and WideResNet-28-10 architectures. The perturbation types $\textup{Dist}_i$s considered in this work are $\ell_\infty$, $\ell_2$, and $\ell_1$ norms with perturbation constrained by $\delta = 8/255, 1$, and $12$, respectively. We use AT to obtain endpoints' models. We compare our methods with the standard $\ell_\infty$-AT baseline and the state-of-the-art method MSD \cite{maini2020adversarial}. The evaluation methods include basic PGD adversarial attacks under $\ell_\infty$, $\ell_2$, $\ell_1$ norm perturbations and MSD attack. The evaluation metrics are (1) standard accuracy on clean test data; (2) robust accuracies under $\ell_\infty$, $\ell_2$, $\ell_1$-PGD adversarial attacks and MSD; (3) accuracy on worst-case sample-wise (Union) using all three basic PGD adversarial attacks; and (4) DLR on $\ell_\infty$, $\ell_2$, $\ell_1$-PGD adversarial attacks for three types of perturbations and DLR on $\ell_\infty$, $\ell_2$ for two types of perturbations. All the following experiments are conducted on two NVIDIA RTX A100 GPUs.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/pgd-pgd1-150-50.png}
  \caption{{Curves obtained from two models trained by $\ell_1$-AT and $\ell_\infty$-AT contain points with higher DLR on $\ell_1$ and $\ell_\infty$-PGD attacks}. The paths are obtained by training 50 epochs. The two endpoints are trained for 150 epochs.} 
  \label{fig: adv_mc_l1_linf}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/pgd2-pgd1-150-50.png}
  \caption{{Curves obtained from two models trained by $\ell_1$-AT and $\ell_2$-AT contain points with higher DLR on $\ell_1$ and $\ell_2$-PGD attacks}. The paths are obtained by training 50 epochs. The two endpoints are trained for 150 epochs. $\ell_2$-AT trained model can also provide a relatively high level of robustness against the $\ell_1$-PGD attack.} 
  \label{fig: adv_mc_l1_l2}
\end{figure}

\subsection{A More Comprehensive Study of the Robust Mode Connectivity}



In this subsection, we aim to study the effectiveness of the proposed method \eqref{eq: mc_adv} on different models. We will consider models trained under various settings. By default, we train endpoints' models 50/150 epochs and the paths are obtained by training an additional 50 epochs. Experiments on pairwise combinations of $\ell_\infty$, $\ell_2$, $\ell_1$-AT trained models are conducted. We also show results on CIFAR-100 and WideResNet-28-10 CNN architecture.








% \begin{figure*}[h]
%   \centering
%   \begin{subfigure}{0.35\textwidth}
%              \includegraphics[trim=00 0 90 10,clip,width=1\textwidth]{Figures/pgd2-pgd1-50.png}
%              \caption{Endpoints training epoch=50}
%   \end{subfigure}
%     \begin{subfigure}{0.35\textwidth}
%              \includegraphics[trim=00 0 90 10,clip,width=1\textwidth]{Figures/pgd2-pgd1-150-50.png} 
%              \caption{Endpoints training epoch=150}
%   \end{subfigure}
%   \caption{{\bf Curves obtained from two models trained by $\ell_1$-AT and $\ell_2$-AT contain points with higher DLR on $\ell_1$ and $\ell_2$-PGD attacks}. The paths are obtained by training 50 epochs. The two endpoints are trained for 50 epochs (left panel) and 150 epochs (right panel), respectively. $\ell_2$-AT trained model can also provide a relatively high level of robustness against the $\ell_1$-PGD attack.} 
%   \label{fig: adv_mc_l1_l2}
% \end{figure*}

\begin{figure*}[h]
  \centering
\subfloat[CIFAR100]{\includegraphics[trim=0 0 0 0,clip,width=0.35\textwidth]{Figures/cifar100.png}}
\subfloat[WideResNet-28-10]{\includegraphics[trim=0 0 0 0,clip,width=0.35\textwidth]{Figures/wide.png}}
  \caption{{RMC can find paths containing points with high DLR on different datasets and model architectures}. The left (right) figure shows that RMC performs well on CIFAR-100 dataset (WideResNet-28-10 architecture). The paths are obtained by training 50 epochs with two endpoints trained with $\ell_\infty$ and $\ell_2$-AT.}  
  \label{fig: rmc_vgg16_cifar100}
\end{figure*}

\noindent\textbf{$\ell_\infty$-AT trained model with $\ell_1$-AT trained model.}
We consider an additional $\ell_1$-AT trained model and combine it with the $\ell_\infty$-AT trained model. The results are shown in Fig.~\ref{fig: adv_mc_l1_linf}. Two endpoints are trained for 150 epochs and the path is obtained by additional 50 epochs. It can be seen that the right endpoint, i.e., the $\ell_1$-AT trained model, has a high resilience to $\ell_1$ perturbations but suffers from $\ell_\infty$ perturbations. The left endpoint, i.e., the $\ell_\infty$-AT trained model, has a high resilience to $\ell_\infty$ perturbations and also can defend against a certain level of $\ell_1$ perturbations. By applying the RMC, we obtain a path with enhanced DLR against both two attacks. %Additional results obtained by training the two endpoints for 50 epochs are shown in Fig.~\ref{fig: adv_mc_l1_linf_2} in Appendix~\ref{sec: appA}.




\noindent\textbf{$\ell_2$-AT trained model with $\ell_1$-AT trained model.} We then consider the combination between the $\ell_1$-AT trained model and the $\ell_2$-AT trained model. The results are shown in Fig.~\ref{fig: adv_mc_l1_l2}. One can see that the model of $\phi(1; \boldsymbol \theta)$, i.e., the $\ell_1$-AT trained model has high robustness against the $\ell_1$-PGD attack, but has almost no resilience against the $\ell_2$-PGD attack. The model of $\phi(0; \boldsymbol \theta)$, i.e., the $\ell_2$-AT trained model can achieve a slightly higher (almost the same) robustness on $\ell_2$-PGD perturbations than $\ell_2$-PGD perturbations. From the DLR perspective, $\phi(0; \boldsymbol \theta)$ is a more robust model. By connecting the $\phi(0; \boldsymbol \theta)$ with $\phi(1; \boldsymbol \theta)$, a path with a higher robustness region can be achieved. One can also see that the $\ell_2$-AT trained model can also provide a relatively high level of robustness against the $\ell_1$-PGD attack. %Additional results can be found in Fig.~\ref{fig: adv_mc_l1_l2_2} in Appendix~\ref{sec: appA}.




\noindent\textbf{RMC on CIFAR-100 and WideResNet-28-10.} Here we evaluate the effectiveness of RMC on CIFAR-100 and WideResNet-28-10 model architecture. We consider two types of perturbations that are generated from $\ell_\infty$ and $\ell_2$-PGD attacks. Endpoints are trained for 150 epochs and path search costs an additional 50 epochs. It can be seen from Fig.~\ref{fig: rmc_vgg16_cifar100} that paths with high DLR points are obtained when CIFAR-10 is replaced with CIFAR-100 and PreResNet110 is replaced with WideResNet-28-10. The results demonstrate that RMC can be applied on different datasets and architectures.


% \begin{figure*}[h]
%   \centering
%   \begin{subfigure}{0.35\textwidth}
%              \includegraphics[trim=0 0 0 0,clip,width=1\textwidth]{Figures/cifar100.png}
%              \caption{CIFAR100}
%   \end{subfigure}
%     \begin{subfigure}{0.35\textwidth}
%              \includegraphics[trim=0 0 0 0,clip,width=1\textwidth]{Figures/wide.png} 
%              \caption{WideResNet-28-10}
%   \end{subfigure}
%   \caption{{\bf RMC can find paths containing points with high DLR on different datasets and model architectures}. The left (right) figure shows that RMC performs well on CIFAR-100 dataset (WideResNet-28-10 architecture). The paths are obtained by training 50 epochs with two endpoints trained with $\ell_\infty$ and $\ell_2$-AT.}  
%   \label{fig: rmc_vgg16_cifar100}
% \end{figure*}






\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.5\textwidth]{Figures/rmc_2pert_opt_1p.png}
  \caption{{RMC-based optimization considering two types of perturbations (one single mid-optimal point) can result in paths with smoother and higher DLR than the path in Figure~\ref{fig: adv_mc} left panel}. The left (right) endpoint is an $\ell_\infty$-AT ($\ell_2$-AT) trained model starting from a single optimal point of a path connected between two models, which are trained by $\ell_\infty$-AT and $\ell_2$-AT for 50 epochs.} 
  \label{fig: adv_mc_opt1}
\end{figure}

\subsection{Robust Mode Connectivity-Based Optimization}\label{subsec: rmc_opt}

As introduced in Section~\ref{sec: opt}, Phase II of the Robust Mode Connectivity-Oriented Adversarial Defense is an enhanced optimization process based on units of RMC (Phase I). We show the effectiveness of RMC-based optimization (Phase II) below. Training epochs for all the experiments below are 200 (allow parallel computing).



\noindent\textbf{Optimization on two types of perturbations.} We first consider $\ell_\infty$ and $\ell_2$ norm perturbations. We train two models for 50 epochs under these two types of perturbations, then leverage RMC to find a path between the two models. Initializing from a single optimal point (randomly select from $t\in [0.77,0.83]$) on the curve, we train two models parallelly with $\ell_\infty$-AT and $\ell_2$-AT for 50 epochs. Finally, we plot the mode connectivity curve based on the two AT-trained models, as shown in Figure~\ref{fig: adv_mc_opt1}. We obtain a smoother path with higher DLR than the path in Figure~\ref{fig: adv_mc} left panel. The optimal point achieved in this optimization process is  $48.8\%$ at $t=0.72$.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.5\textwidth]{Figures/rmc_2pert_opt.png}
  \caption{{RMC-based optimization considering two types of perturbations with two mid-optimal points is able to achieve higher robustness compared with only selecting a single mid-optimal point}. The left (right) endpoint is an $\ell_\infty$-AT ($\ell_2$-AT) trained model starting from two optimal points of a path connected between two models, which are trained by $\ell_\infty$-AT and $\ell_2$-AT for 50 epochs.} 
  \label{fig: adv_mc_opt12}
\end{figure}

Now instead of selecting a single optimal point, we randomly pick two optimal points in the ranges of $t\in [0.27,0.33]$ and $t\in [0.77,0.83]$, respectively. We train two models with $\ell_\infty$-AT and $\ell_2$-AT for 50 epochs starting from each initial point. We then plot the RMC curve based on the two AT-trained models, as shown in Figure~\ref{fig: adv_mc_opt12}. The optimal point achieved in this optimization process is 48.89\% (DLR) at $t=0.71$, indicating that higher robustness can be improved by using a larger population with higher diversity.



\noindent\textbf{Optimization on three types of perturbations.} We take one more step by considering the $\ell_1$ norm perturbation. The process is shown in Algorithm~\ref{alg: RMC_opt}. $T=50$ and we use 50 additional epochs to learn RMC. The results of the final connection are shown in Fig.~\ref{fig: adv_mc_opt2}. The trend of the $\ell_1$-PGD curve is increasing from left to right and the trend of the $\ell_2$-PGD curve from $t=0.7$ to $t=1$ is decreasing. There exists an optimal point with DLR$=46.21\%$ at $t=0.93$. RMC-based optimization in the case of three types of perturbations can further boost models' DLR against $\ell_\infty$, $\ell_1$, $\ell_2$ adversarial attacks.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/rmc_3pert_opt.png}
  \caption{{RMC-based optimization considering three types of perturbations can further boost models' DLR against more types of attacks}. The two endpoints are trained by $\ell_\infty$-AT and $\ell_2$-AT for 50 epochs starting from the optimal points selected from two RMC curves.} 
  \label{fig: adv_mc_opt2}
\end{figure}




\subsection{RMC-Based Optimization with SRMC Modules}



\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/pgd2_5-pgd1_5.png}
  \caption{{RMC-based optimization with SRMC modules considering three types of perturbations can find points with high DLR}. SRMC modules can accelerate the speed of RMC-based optimization and saves computational cost.} 
  \label{fig: adv_self_rmc_opt}
\end{figure}

We then test the proposed SRMC modules to speed up the process of RMC-based optimization. Starting from a $\ell_\infty$-AT model, an additional 5 epochs are used to train a $\ell_2$-AT model and a $\ell_1$-AT model. Then we connect each of the child models with the original $\ell_\infty$-AT model. Results are shown in Fig.~\ref{fig: adv_self_rmc} and Fig.~\ref{fig: adv_self_rmc2} in Section~\ref{sec: add_result}. One can see that there exist paths with high-robustness regions under two connections. The results indicate that we do not need to train all the models from scratch to achieve desired paths.



Now we conduct the RMC-based optimization with SRMC modules on three types of perturbations. The pipeline is the same as the RMC-based optimization, except that we use self-generated $\ell_2$ and $\ell_1$ models as endpoints. As shown in Fig.~\ref{fig: adv_self_rmc_opt}, there exists an optimal point with DLR$=46.10\%$ at $t=0.82$. Compared with Fig.~\ref{fig: adv_mc_opt2}, optimization with SRMC achieves a similar level of DLR in a more efficient way.




\begin{table*}[!t]
%\vspace*{-5mm}
\caption{Our Methods Can Achieve State-Of-Art DLR Level Under Different Perturbations. For methods using two types of perturbations, we only compare them using DLR under $\ell_\infty$-PGD and $\ell_2$-PGD attacks and mark the DLR (the lowest accuracy) using underline. For methods using three types of perturbations, we compare them on all metrics and mark the DLR (the lowest accuracy) under three basic $\ell_p$ attacks using overline. The baselines ($\ell_\infty$-AT \cite{madry2018towards} and MSD \cite{maini2020adversarial}) are all trained with 200 epochs. Our RMC with $\ell_\infty$ and $\ell_2$ norm perturbations is trained with 150 epochs' endpoints and 50 epochs' path search. All the RMC-based optimizations are trained with two RMC stages with each stage including 50 epochs' endpoints and 50 epochs' path searching. One can also see that our methods achieve the highest accuracy under Union and MSD.
}
\label{tab: main}
\begin{center}
%\small

\resizebox{0.99\textwidth}{!}{
\begin{tabular}{l||c|c|c|c|c|c|c}
\hline
\hline
& Standard Accuracy & $\ell_\infty$-PGD ($\delta=8/255$) & $\ell_2$-PGD ($\delta=1$) & $\ell_1$-PGD ($\delta=12$) & DLR & Union & MSD \\
\hline
\begin{tabular}[c]{@{}c@{}} $\ell_\infty$-AT   \end{tabular}  &   85.00\% & 49.03\% & 29.66\%  & 16.61\% &  / & 21.85\%  & 15.27\% \\
\hline
\begin{tabular}[c]{@{}c@{}} MSD \\ (\textbf{two} types of pert)  \end{tabular}  &  81.61\% & 48.57\% & \underline{45.92\%} & 35.64\% & 45.92\% & 34.37\% & 45.72\%  \\
\hline
\begin{tabular}[c]{@{}c@{}} RMC \\ (ours, \textbf{two} types of pert)  \end{tabular}  &  80.90\%  & \underline{48.19\%} & 48.63\% & 38.05\% & 48.19\%  & 36.3\% & 46.52\%  \\
\hline
\begin{tabular}[c]{@{}c@{}} RMC-based optimization \\ (ours, \textbf{two} types of pert)  \end{tabular}  &  81.36\%  & \underline{48.89\%} & 49.03\% & 38.83\% & 48.89\%  & 36.86\% & \bf{47.18\%}  \\
\hline
\begin{tabular}[c]{@{}c@{}} MSD \\ (\textbf{three} types of pert)  \end{tabular}  &  81.35\% & $\overline{40.14\%}$ & 48.58\% & 47.50\% & 40.14\% & 38.35\% & 38.20\%  \\
\hline
\begin{tabular}[c]{@{}c@{}} RMC-based optimization \\ (ours, \textbf{three} types of pert)  \end{tabular}  &  81.76\% & $\overline{46.21\%}$ & 51.86\% & 46.23\% & 46.21\% & 41.47\% & 44.75\%   \\
\hline
\begin{tabular}[c]{@{}c@{}} RMC-based optimization \\ with SRMC modules \\ (ours, \textbf{three} types of pert)  \end{tabular}  &  80.39\%  & $\overline{46.10\%}$ & 48.92\% & 46.39\% & 46.10\%  & \bf{42.03\%} & 45.07\%   \\
\hline
\hline
\end{tabular}}
\end{center}
\end{table*}


\subsection{A Comprehensive Comparison}



A comprehensive comparison is presented in Table~\ref{tab: main}. We compared our proposed RMC (Phase I) and RMC-based optimization (Phase II) frameworks with a $\ell_\infty$-AT baseline \cite{madry2018towards} and the state-of-the-art method MSD \cite{maini2020adversarial}. All baselines were trained for 200 epochs. For MSD, RMC, and RMC-based optimization considering only two types of perturbations, we compared them under $\ell_\infty$-PGD and $\ell_2$-PGD attacks since we did not consider $\ell_1$-PGD attack during the training. The DLR (the lowest robust accuracy) under these two attacks is marked using underline. For methods using three types of perturbations, we compared them under $\ell_\infty$-PGD, $\ell_2$-PGD, $\ell_\infty$-PGD, MSD attacks, and the union metric. The DLR (the lowest robust accuracy) under $\ell_\infty$-PGD, $\ell_2$-PGD, $\ell_\infty$-PGD attacks are marked using overline. We also highlighted the highest accuracy in the union and MSD columns.


\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/pgd-pgd1-50.png}
  \caption{{Curves obtained from two models trained by $\ell_1$-AT and $\ell_\infty$-AT contain points with higher DLR on $\ell_1$ and $\ell_\infty$-PGD attacks}. The two endpoints are trained for 50 epochs.} 
  \label{fig: adv_mc_l1_linf_2}
\end{figure}

From the table, it can be observed that: (1) RMC with two types of perturbations outperforms MSD with two types of perturbations by $2.27\%$ ($48.19\% - 45.92\%$); (2) RMC-based optimization with two types of perturbations achieves slightly higher DLR than RMC and outperforms RMC on all other metrics; (3) RMC-based optimization with three types of perturbations outperforms MSD with three types of perturbations on DLR by $6.07\%$ and has higher accuracy on all metrics (except for $\ell_1$-PGD, where it is slightly lower than MSD); (4) The RMC-based optimization with SRMC modules can achieve similar (slightly lower) DLR performance compared to the RMC-based optimization with three types of perturbations, and even has slightly higher accuracy on the Union metric.

In conclusion, the two phases of the proposed Robust Mode Connectivity-Oriented Adversarial Defense demonstrate superior performance in various metrics, and RMC-based optimization (Phase II) achieves higher DLR than RMC (Phase I) alone. Overall, Robust Mode Connectivity-Oriented Adversarial Defense provides a new defense regime from a population-based optimization perspective and effectively enhances robustness in NNs.





\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/pgd2-pgd1-50.png}
  \caption{{Curves obtained from two models trained by $\ell_1$-AT and $\ell_2$-AT contain points with higher DLR on $\ell_1$ and $\ell_2$-PGD attacks}. The two endpoints are trained for 50 epochs. } 
  \label{fig: adv_mc_l1_l2_2}
\end{figure}


\subsection{Additional Results on RMC and SRMC}\label{sec: add_result}




In Fig.~\ref{fig: adv_mc_l1_linf_2} and Fig.~\ref{fig: adv_mc_l1_l2_2}, we show additional results of using RMC to find paths connecting a $\ell_1$-AT trained model and a $\ell_\infty$-AT ($\ell_2$-AT) trained model. Different from Fig.~\ref{fig: adv_mc_l1_linf} and Fig.~\ref{fig: adv_mc_l1_l2}, here the two endpoints are trained for 50 epochs. One can find that the DLR is high even the number of training epochs is small. This ensures that under the scenario of using a small number of training epochs, we can still find good candidates to serve as new initial points in RMC-based optimization.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/selfrmc_infty_l2.png}
  \caption{{A single SRMC module can also find paths with high DRL by connecting a $\ell_\infty$ model and a $\ell_2$ model}. } 
  \label{fig: adv_self_rmc}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/selfrmc_infty_l1.png}
  \caption{{A single SRMC module can also find paths with high DRL by connecting a $\ell_\infty$ model and a $\ell_1$ model}. } 
  \label{fig: adv_self_rmc2}
\end{figure}


Fig.~\ref{fig: adv_self_rmc} and Fig.~\ref{fig: adv_self_rmc2} show that a single SRMC module can also find paths with high DRL. The base model is trained by $\ell_\infty$-AT with 50 epochs. It serves as one endpoint. Starting from the base model, two models are trained by $\ell_2$-AT and $\ell_1$-AT with additional 5 epochs. They serve as the other endpoint. Fig.~\ref{fig: adv_self_rmc} shows the path connecting the $\ell_\infty$ model and the $\ell_2$ model. Fig.~\ref{fig: adv_self_rmc2} shows the path between the $\ell_\infty$ model and the $\ell_1$ model.




\section{Conclusion}\label{sec: conclusion}
In this paper, we propose a novel Robust Mode Connectivity-Oriented Adversarial Defense that can defend against diversified $\ell_p$ norm attacks and improve upon existing works using a population-based optimization strategy. We first introduce a new method called Robust Mode Connectivity (RMC), which corresponds to Phase I of the defense. RMC can discover a high-robustness path that connects two adversarially trained models against perturbations constrained on diversified $\ell_p$ norms. This approach enhances the Diversified $\ell_p$ Robustness (DLR) of neural networks and can be implemented as a defense method alone. We build on this method to develop Phase II of the defense, which is a multi-stage RMC-based population-based optimization regime that further boosts the DLR. To improve computational efficiency, we propose incorporating the Self-Robust Mode Connectivity (SRMC) module into the RMC-based optimization.

In addition, we conduct a comprehensive study of the RMC and RMC-based optimization methods and show that they can achieve significantly higher DLR than the baselines. Our findings demonstrate the effectiveness of the proposed methods in achieving high DLR against diversified $\ell_p$ threats, including $\ell_\infty$, $\ell_2$, $\ell_1$, and hybrid attacks. Our results show the potential of these methods for practical applications in real-world scenarios where the safety and security of machine learning systems are critical.












\newpage

% \appendices
% \section{Additional Experimental Results For RMC}\label{sec: appA}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.

%In Fig.~\ref{fig: adv_mc_l1_linf_2} and Fig.~\ref{fig: adv_mc_l1_l2_2}, we show additional results of using RMC to find paths connecting a $\ell_1$-AT trained model and a $\ell_\infty$-AT ($\ell_2$-AT) trained model. Different from Fig.~\ref{fig: adv_mc_l1_linf} and Fig.~\ref{fig: adv_mc_l1_l2}, here the two endpoints are trained for 50 epochs. One can find that the DLR is high even the number of training epochs is small.



% \section{Additional Experimental Results For SRMC}\label{sec: appB}


% Fig.~\ref{fig: adv_self_rmc} and Fig.~\ref{fig: adv_self_rmc2} show that a single SRMC module can also find paths with high DRL. The base model is trained by $\ell_\infty$-AT with 50 epochs. It serves as one endpoint. Starting from the base model, two models are trained by $\ell_2$-AT and $\ell_1$-AT with additional 5 epochs. They serve as the other endpoint. Fig.~\ref{fig: adv_self_rmc} shows the path connecting the $\ell_\infty$ model and the $\ell_2$ model. Fig.~\ref{fig: adv_self_rmc2} shows the path between the $\ell_\infty$ model and the $\ell_1$ model.

% \begin{figure}[h]
%   \centering
%   \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/selfrmc_infty_l2.png}
%   \caption{{A single SRMC module can also find paths with high DRL by connecting a $\ell_\infty$ model and a $\ell_2$ model}. } 
%   \label{fig: adv_self_rmc}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[trim=0 0 0 0,clip,width=.45\textwidth]{Figures/selfrmc_infty_l1.png}
%   \caption{{A single SRMC module can also find paths with high DRL by connecting a $\ell_\infty$ model and a $\ell_1$ model}. } 
%   \label{fig: adv_self_rmc2}
% \end{figure}


% \begin{figure*}[htb]
%   \centering
%   \begin{subfigure}{0.39\textwidth}
%              \includegraphics[trim=0 0 0 0,clip,width=1\textwidth]{Figures/selfrmc_infty_l2.png}
%              \caption{$\ell_\infty$ \& $\ell_2$}
%   \end{subfigure}
%     \begin{subfigure}{0.39\textwidth}
%              \includegraphics[trim=0 0 0 0,clip,width=1\textwidth]{Figures/selfrmc_infty_l1.png} 
%              \caption{$\ell_\infty$ \& $\ell_1$}
%   \end{subfigure}
%   \caption{{\bf A single SRMC module can also find paths with high DRL.} The base model is trained by $\ell_\infty$-AT with 50 epochs. It serves as one endpoint. Starting from the base model, two models are trained by $\ell_2$-AT and $\ell_1$-AT with additional 5 epochs. They serve as the other endpoint. The left panel is the path connecting the $\ell_\infty$ model and the $\ell_2$ model. The right panel shows the path between the $\ell_\infty$ model and the $\ell_1$ model. }  
%   \label{fig: adv_self_rmc}
% \end{figure*}


% use section* for acknowledgment
% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\newpage
\bibliographystyle{ieeetr}
\bibliography{ref_adv,ref_new}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/RenWang}}] {Ren Wang} is an assistant professor in the Department of Electrical and Computer Engineering
at the Illinois Institute of Technology, where he leads the Trustworthy and Intelligent Machine Learning (TIML) Research Lab. He received his bachelors degree and masters degree in Electrical Engineering from Tsinghua University, Beijing, China, in 2013 and 2016. He received his Ph.D. degree in Electrical Engineering from Rensselaer Polytechnic Institute, Troy, NY, USA, in 2020. He was a postdoctoral research fellow in the Department of Electrical Engineering and Computer Science at the University of Michigan. His research interests include Trustworthy Machine Learning, High-Dimensional Data Analysis, Bio-Inspired Machine Learning, and Smart Grids.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/YuxuanLi}}]{Yuxuan Li}
is currently a graduate research intern at the Trustworthy and Intelligent Machine Learning Research Lab in the Department
of Electrical and Computer Engineering, Illinois Institute of Technology. He is also pursuing his master's degree at the Harbin Institute of Technology. He received his bachelors degree in Computer Science and Technology from Harbin Institute of Technology in 2021.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Sijia}}] {Sijia Liu}
received the Ph.D. degree (with the All University Doctoral Prize) in Electrical and Computer Engineering from Syracuse University, Syracuse, NY, USA, in 2016. He was a Postdoctoral Research Fellow at the University of Michigan, Ann Arbor, in 2016-2017, and a Research Staff Member at the MIT-IBM Watson AI Lab in 2018-2020. His research interests include scalable and trustworthy AI, e.g., adversarial deep learning, optimization theory and methods, computer vision, and computational biology. He received the Best Student Paper Award at the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). His work has been published at top-tier AI conferences such as NeurIPS, ICML, ICLR, CVPR, and AAAI.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


