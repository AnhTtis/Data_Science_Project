% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{dsfont}
% \newcommand{\xcc}[1]{\textcolor[rgb]{0.9,0,0}{#1}}
\newcommand{\xcc}[1]{#1}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1741} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Unsupervised Domain Adaption with Pixel-level Discriminator \\ for Image-aware Layout Generation}

\author{Chenchen Xu$^{1,2}$\thanks{Work done during an internship at Alibaba Group}\and 
Min Zhou$^2$\and
Tiezheng Ge$^2$\and 
Yuning Jiang$^2$\and
Weiwei Xu$^1$\thanks{Corresponding author}\\
$^1$State Key Lab of CAD\&CG, Zhejiang University \quad $^2$Alibaba Group\\
{\tt\small
xuchenchen@zju.edu.cn,
\{yunqi.zm, tiezheng.gtz, mengzhu.jyn\}@alibaba-inc.com,
xww@cad.zju.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Layout is essential for graphic design and poster generation. Recently, applying deep learning models to generate layouts has attracted increasing attention. This paper focuses on using the GAN-based model conditioned on image contents to generate advertising poster graphic layouts, which requires an advertising poster layout dataset with paired product images and graphic layouts. However, the paired images and layouts in the existing dataset are collected by inpainting and annotating posters, respectively. There exists a domain gap between inpainted posters (source domain data) and clean product images (target domain data). Therefore, this paper combines unsupervised domain adaption techniques to design a GAN with a novel pixel-level discriminator (PD), called PDA-GAN, to generate graphic layouts according to image contents. The PD is connected to the shallow level feature map and computes the GAN loss for each input-image pixel. Both quantitative and qualitative evaluations demonstrate that PDA-GAN can achieve state-of-the-art performances and generate high-quality image-aware graphic layouts for advertising posters.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
    \begin{figure}
    \centering
    \includegraphics[width=8cm]{pictures/introduction_2.pdf}
    \caption{{\bf Examples of image-conditioned advertising posters graphic layouts generation. } Our model generates graphic layouts (middle) with multiple elements conditioned on product images (left).  The designer or even automatic rendering programs can utilize graphic layouts to render advertising posters (right).}
    \label{fig:introduction}
    \end{figure}
%   Graphic layout is fundamental to the design of posters, magazines, comics, and webpages. Deep learning-based models generate graphic layouts by modeling the geometric relationships of different types of 2D elements, such as text and logo bounding boxes. In addition to unconditional models \cite{DBLP:conf/iclr/LiYHZX19,DBLP:conf/cvpr/ArroyoPT21} with random noise as input, some conditional models \cite{DBLP:journals/tvcg/LiY0LWX21,DBLP:journals/tog/ZhengQCL19} have been proposed to exert control over the layout generation process. These conditions include image contents and element attributes containing category, area, and aspect ratio. Especially, image contents play an important role in generating image-aware graphic layouts of posters and magazines  \cite{DBLP:journals/tog/ZhengQCL19,DBLP:conf/ijcai/ZhouXMGJX22}.
  Graphic layout is essential to the design of posters, magazines, comics, and webpages. Recently, generative adversarial network (GAN) has been applied to synthesize graphic layouts through modeling the geometric relationships of different types of 2D elements, for instance, text and logo bounding boxes  \cite{DBLP:conf/nips/GoodfellowPMXWOCB14,DBLP:conf/iclr/LiYHZX19}. Fine-grained controls over the layout generation process can be realized using Conditional GANs, and the conditions might include image contents and the attributes of graphic elements, e.g. category, area, and aspect ratio \cite{DBLP:journals/tvcg/LiY0LWX21,DBLP:conf/ijcai/ZhouXMGJX22}. Especially, image contents play an important role in generating image-aware graphic layouts of posters and magazines \cite{DBLP:journals/tog/ZhengQCL19,DBLP:conf/ijcai/ZhouXMGJX22}.

 This paper focuses on studying the deep-model based image-aware graphic layout method for advertising poster design, where the graphic layout is defined to be a set of elements with their classes and bounding boxes as in~\cite{DBLP:conf/ijcai/ZhouXMGJX22}. As shown in \cref{fig:introduction}, the graphic layout for advertising poster design in our work refers to arranging four classes of elements, such as logos, texts, underlays, and other elements for embellishment, at the appropriate position according to product images. Therefore, its kernel is to model the relationship between the image contents and layout elements~\cite{DBLP:conf/ijcai/ZhouXMGJX22,DBLP:conf/mm/CaoMZLXGJ22} such that the neural network can learn how to produce the aesthetic arrangement of elements around the product image. It can be defined as the direct set prediction problem in~\cite{DBLP:conf/eccv/CarionMSUKZ20}.
 
 Constructing a high-quality layout dataset for the training of image-ware graphic layout methods is labor intensive, since it requires professional stylists to design the arrangement of elements to form the paired product image and layout data items. For the purpose of reducing workload, zhou et.al.~\cite{DBLP:conf/ijcai/ZhouXMGJX22} propose to collect designed poster images to construct a dataset with required paired data. Hence, the graphic elements imposed on the poster image are removed through image inpainting~\cite{DBLP:conf/wacv/SuvorovLMRASKGP22}, and annotated with their geometric arrangements in the posters, which results in state-of-the-art CGL-Dataset with 54,546 paired data items. While CGL-Dataset is substantially beneficial to the training of image-ware networks, there exists a domain gap between product image and its inpainted version. The CGL-GAN in~\cite{DBLP:conf/ijcai/ZhouXMGJX22} tries to narrow this domain gap by utilizing Gaussian blur such that the network can take a clean product image as input for synthesizing a high-quality graphic layout. However, it is possible that the blurred images lose the delicate color and texture details of products, leading to unpleasing occlusion or placement of graphic elements. 
 
%   \begin{figure*}[ht]
%     \centering
%     \hspace{0cm}\includegraphics[width=17.6cm]{pictures/introduction_1.pdf}
%     \caption{The input image in the lower row is taken from the target domain data. For comparison, we add the layout element content as source domain data. The yellow area marked is the domain gap. The six feature maps in the middle are shallow level, fusion (fusing hierarchical feature maps to deep level), and deep level feature maps generated by our model with source and target data as input. On the right is the layout generated by PDA-GAN and CGL-GAN with the source data or target data as input.}
%     \label{fig:introduction}
% \end{figure*}

%（逻辑：为了消除域差异，我们放弃高斯模糊引入无监督域适应方法解决布局生成问题。除了用传统的DA，我们还设计Patch和pixel DA。通过实验证明，PDA可以很好解决gap，并生成高质量的。）
 This paper proposes to leverage  unsupervised domain adaption technique to bridge the domain gap between \xcc{clean product images and inpainted images} in CGL-Dataset to significantly improve the quality of generated graphic layouts. Treating the inpainted poster images without graphic elements as \xcc{the} source domain, our method aims to seek for the alignment of the feature space of \xcc{source domain and the feature space of clean product images in the target domain}. To this end, we design a GAN with \xcc{a} pixel-level domain adaption discriminator, abbreviated as PDA-GAN, to achieve more fine-grained control over feature space alignment. It is inspired by PatchGAN~\cite{DBLP:conf/cvpr/IsolaZZE17}, but non-trivially adapts to pixel-level in our task. First, the pixel-level discriminator (PD) designed for domain adaption can avoid the Gaussian blurring step in~\cite{DBLP:conf/ijcai/ZhouXMGJX22}, which is helpful for the network to model the details of the product image. Second, the pixel-level discriminator is connected to the shallow level feature map, since the inpainted area is usually small relative to the whole image and will be difficult to discern at deep levels with large receptive field. Finally, the PD is constructed by three convolutional layers only, and its number of network parameters is less than $2\%$ of the discriminator parameters in CGL-GAN. This design reduces the memory and computational cost of the PD.
 
 %In addition to classical GAN-based \cite{DBLP:conf/nips/GoodfellowPMXWOCB14,DBLP:conf/cvpr/BousmalisSDEK17} and PatchGAN-based \cite{DBLP:conf/cvpr/IsolaZZE17} domain adaption methods, we proposed a pixel-level domain adaption discriminator (PDA) trained against the feature generation module in the layout generator. Such an adversarial model is called PDA-GAN. The PDA computes a GAN loss \cite{DBLP:journals/corr/Goodfellow17} for each input-image pixel. Both quantitative and qualitative evaluations demonstrate that the PDA-GAN outperforms other models.

 %The PDA-GAN chooses domain adaption at shallow level feature maps for the following two reasons. One is that the discrepancy between pixels of the domain gap is more evident on high-resolution shallow level feature maps \cite{DBLP:conf/eccv/ZhangZPXS18}. Another is that the earlier eliminating the domain gap, the better for the subsequent network processing. 
%  To encourage the PDA to estimate soft probabilities to improve generalization ability, we utilize label smoothing for data \cite{DBLP:conf/cvpr/SzegedyVISW16,DBLP:journals/corr/Goodfellow17}. Since the inpainting areas occupy a small proportion of the input image, we only do label smoothing for target domain data, called one-target label smoothing.

%In addition, qualitative analyses show that PDA-GAN can eliminate the domain gap and reserve images' color and texture information to generate high-quality graphic layouts
 
 We collect 120,000 target domain images during the training of PDA-GAN. Experimental results show that PDA-GAN achieves state-of-the-art (SOTA) performance according to composition-relevant metrics. It outperforms CGL-GAN on CGL-dataset and achieves relative improvement over background complexity, occlusion subject degree, and occlusion product degree metrics by $6.21\%$, $17.5\%$, and $14.5\%$ relatively, leading to significantly improved visual quality of synthesized graphic layouts in many cases. 
 In summary, this paper comprises the following contributions:
 \begin{itemize}
     \item We design a GAN with a novel pixel-level discriminator working on shallow level features to bridge the domain gap that exists between training images in CGL-Dataset and clean product images.
     \item Both quantitative and qualitative evaluations demonstrate that PDA-GAN can achieve SOTA performance and is able to generate high-quality image-aware graphic layouts for advertising posters.
 \end{itemize}
 
 
 \section{Related Work}
%  Modeling layout is essential first step for graphic design, including posters \cite{DBLP:journals/tvcg/ODonovanAH14}, webpages \cite{DBLP:conf/chi/KumarTAK11}, documents \cite{DBLP:conf/eccv/LeeJELG0Y20}, and other design tasks \cite{DBLP:journals/tog/WangSCR18,DBLP:conf/cvpr/Ritchie0L19}. A high-quality graphic layout can catch the reader's attention and convey information in a visually appealing way. Hence, graphic layout generation task has fueled increased interest in the computer vision community in recent years.
\paragraph{Image-agnostic layout generation.}
Early works~\cite{DBLP:journals/tog/JacobsLSBS03,DBLP:conf/chi/KumarTAK11,DBLP:journals/tog/CaoCL12,DBLP:journals/tvcg/ODonovanAH14} often utilize templates and heuristic rules to generate layouts. LayoutGAN~\cite{DBLP:conf/iclr/LiYHZX19} is the first method to apply generative networks~(in particular GAN) to synthesize layouts and use self-attention to build the element relationship. LayoutVAE~\cite{DBLP:conf/iccv/JyothiDHSM19} and LayoutVTN~\cite{DBLP:conf/cvpr/ArroyoPT21} follow and apply VAE and autoregressive methods. Meanwhile, some conditional methods have been proposed to guide the layout generation process~\cite{DBLP:conf/eccv/LeeJELG0Y20,DBLP:conf/cvpr/YangFYW21,DBLP:conf/mm/KikuchiSOY21,DBLP:journals/tvcg/LiY0LWX21,DBLP:conf/iccv/GuptaLA0MS21}. The constraints are in various forms, such as scene graphs, element attributes, and partial layouts. 
However, in a nutshell, these methods mainly focus on modeling the internal relationship between graphic elements, and rarely consider the relationship between layouts and images.

\begin{figure*}[t]
\centering
\hspace{0cm}\includegraphics[width=17.6cm]{pictures/model_4.pdf}
\caption{{\bf The architecture of our network.} Annotated posters (source domain data) must be inpainted before input to the model. The model has both reconstruction and GAN loss when training with source domain data, while only has a GAN loss is used when training with target domain data. Please refer to Sec. \ref{section.3} for the definition of each loss term: $L_{PD}$, $L_{PD}^G$, and $L_{rec}$. During the discriminator or generator pass, both inpainted and clean images are fed into the discriminator.}
\label{fig:model}
\end{figure*}

\paragraph{Image-aware layout generation.}
In layout generation for magazine pages, ContentGAN~\cite{DBLP:journals/tog/ZhengQCL19} first proposes to model the relationship not only between layout elements but also between layouts and images. However, the high-quality training data is relatively rare, since it requires professional stylists to design layouts for obtaining paired clean images and layouts. ContentGAN uses white patches to mask the graphic elements on magazine pages, and replaces the clean images with the processed pages for training. For the same problem, in poster layout generation, CGL-GAN~\cite{DBLP:conf/ijcai/ZhouXMGJX22} leverages inpainting to erase the graphic elements on posters, and subsequently applies Gaussian blur on the whole poster to eliminate the inpainting artifacts. The blur strategy effectively narrows the domain gap between inpainted images and clean images, but it may damage the delicate color and texture details of images and leads to unpleasing occlusion or element placement. In this paper, we find that a pixel-level discriminator for domain adaption can achieve the same goal and avoid the negative effects of blur.

\paragraph{Unsupervised domain adaptation.}
Unsupervised domain adaptation~\cite{DBLP:journals/corr/abs-2010-03978} aims at aligning the disparity between domains such that a model trained on the source domain
with labels can be generalized into the target domain, which lacks labels. Many related methods~\cite{DBLP:conf/esann/MajumdarN18,DBLP:journals/corr/abs-2205-12923,DBLP:journals/mta/ZhangD21,DBLP:conf/cvpr/Zhang0TL22,DBLP:conf/cvpr/BousmalisSDEK17,DBLP:journals/corr/abs-2010-03978,DBLP:conf/aaai/PeiCLW18,DBLP:journals/tip/RenLZH22} have been applied for object recognition and detection. Among these methods, \cite{DBLP:conf/cvpr/BousmalisSDEK17,DBLP:journals/corr/abs-2010-03978,DBLP:conf/aaai/PeiCLW18,DBLP:journals/tip/RenLZH22} leverage adversarial domain adaptation approach~\cite{DBLP:series/acvpr/GaninUAGLLML17}. A domain discriminator is employed and outputs a probability value indicating the domain of input data. In this way, the generator can extract domain-invariant features and eliminate the semantic or stylistic gap between the two domains. However, it does not work well when applied directly to our problem, since the inpainted area is small compared to the whole image and is difficult to discern at deep levels. Therefore, we design a pixel-level discriminator to effectively solve this.


 \section{Our Model} \label{section.3}
 %As mentioned above, the previous approaches to image-aware layout generation encounter two problems. The first problem is how to eliminate the domain gap in the existing dataset. The second problem is how to make the model capture the subtle interaction between image content including color and texture details and layout elements. Therefore, our goal is to eliminate the domain gap while preserving the image's color and texture details information. We combine the unsupervised domain adaption idea to design a pixel-level discriminator. The model introduces the pixel-level discriminator can eliminate the domain gap and generate high-quality image-aware graphic layouts. In the following, we will describe the generator and discriminator of the model in detail.
 
 Our model is a generative adversarial network to learn domain-invariant features with the pixel-level discriminator to minimize the cross-domain discrepancy. As shown in \cref{fig:model}, our network mainly has two sub-networks: the layout generator network that takes the image and its saliency map as the input to generate graphic layout and the convolutional neural network for pixel-level discriminator.
 
 In this section, we will describe the details of our network architecture and the training loss functions for the pixel-level discriminator and the layout generator network respectively. 

 
 \subsection{Network Architecture}
 The architecture of the layout generator network is the same with the generator network in~\cite{DBLP:conf/ijcai/ZhouXMGJX22}, but  the user-constraints are ignored. Its design follows the principle of DETR~\cite{DBLP:conf/eccv/CarionMSUKZ20}, which has three modules: a multi-scale convolutional neural network (CNN) used to extract image features  \cite{DBLP:conf/cvpr/HeZRS16,DBLP:conf/cvpr/LinDGHHB17}, a transformer encoder-decoder that accepts layout element queries as input to model the relationship among layout elements and the product image \cite{DBLP:conf/nips/VaswaniSPUJGKP17}, and two fully connected layers to predict the element class and its bounding box using the element feature output by the transformer decoder.  
  
 Our pixel-level discriminator network consists of three transposed convolutional layers with filter size $3\times 3$ and stride $2$. Its input is the feature map from the first residual block in multi-scale CNN. The transposed convolutional layers can up-sample the feature map, and we also allow to resize the final result to exactly match the dimension of the input image to facilitate the computation of discriminator training loss, which will be elaborated in the next section. 
  
  
  %Shallow level feature maps with low semantics but high resolution, and depp level feature maps with high semantics but low resolution. Since the target domain data does not have labels, the subsequent module only studies the source domain data with labels, as shown in \cref{fig:model}. The transformer module takes feature maps of source domain data as input to implicitly learn the subtle interaction between image contents and layout elements. Finally, the category and coordinate information of layout elements are predicted by two fully connected layers, respectively. Following \cite{DBLP:conf/eccv/CarionMSUKZ20}, we calculate the reconstruction loss $L_{rec}$ between the label and the generator output.
 
 %Model earlier eliminating the domain gap at the high-resolution feature maps, the better for the subsequent network processing. Therefore, we choose to eliminate the domain gap brought by inpainted pixels at the shallow level feature maps.
 
%The pixel-level discriminator (PD) contains of three convolutional layers. The number of the PD parameters is tiny, less than $2\%$ of the number of the discriminator parameters in CGL-GAN.
 
 \begin{table*}[t!]
    \centering
    \setlength{\tabcolsep}{2.18mm}{
    \scalebox{0.98}{
    \begin{tabular}{lccc|cccc}
    \toprule
         Model    &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$ &$R_{occ}\uparrow$\\
    \midrule
         ContentGAN~\cite{DBLP:journals/tog/ZhengQCL19}      &45.59 &17.08 &1.143 &0.0397 &0.8626 &\bf{0.0071} &93.4\\
         CGL-GAN~\cite{DBLP:conf/ijcai/ZhouXMGJX22} &\underline{35.77} &\underline{15.47} &\underline{0.805} &\bf{0.0233} &\underline{0.9359} &\underline{0.0098} &\underline{99.6}\\
         PDA-GAN(Ours)   &\bf{33.55} &\bf{12.77} &\bf{0.688} &\underline{0.0290} &\bf{0.9481} &0.0105   &\bf{99.7}\\
    \bottomrule
    \end{tabular}
    }
    }
    \caption{{\bf Comparison with content-aware methods.} Bold and underlined numbers denote the best and second best respectively.}
    \label{tab:content-aware}
\end{table*}
 
\subsection{Pixel-level Discriminator Training} 
The design of pixel-level discriminator is based on the observation that the domain gap between inpainted images and clean product images mainly exists at pixels synthesized by inpainting process. Therefore, during the discriminator or generator pass in \cref{fig:model}, both inpainted and clean images are fed into the discriminator. When updating the discriminator, we encourage the discriminator to detect the inpainted pixels for inpainted images in the source domain. In contrast, when updating the generator, we leverage the pixel-level discriminator to encourage the generator to output shallow feature maps that can fool the discriminator, which means that, even for the feature map computed for the inpainted images, the discriminator's ability to detect inpainted pixels should be weakened fast. In this way, when the training converges, the feature space of source and target domain images should be aligned. 

To calculate the loss $L_{PD}$ for each input-image pixel, we utilized the white-patch map to distinguish whether the input-image pixel is inpainted, where the pixel in white patch map is set to 1 if the corresponding pixel in the input image is processed by the inpainting, otherwise 0. Correspondingly, the pixel values of white-patch map for the clean images in target domain are all 0. 

When updating discriminator in the GAN training, the pixel-level discriminator takes shallow level feature maps as input and outputs a map with one channel whose dimension is consistent with the input image. The loss $L_{PD}$ used to train the discriminator is a mean absolute error (MAE) loss or L1 norm between the white-patch map of input images and the output map. We can get  as:
 \begin{equation}
    \begin{aligned}
    {L}_{PD} = 
    \frac{1}{N_{p}} \sum^{N_{p}}_{i=1} (
    {\left\vert {{\mathbf{P}}^{s,w}_i - {\mathbf{P}}^{s,o}_i} \right\vert} * \alpha \\ + {\left\vert {{\mathbf{P}}^{t,w}_i - {\mathbf{P}}^{t,o}_i} \right\vert} * \beta),
    \end{aligned}
\end{equation}
where the $N_{p}$ means the number of white-patch map pixels, and $\mathbf{P}_i$ indicates the predicted or ground-truth map for $i_{\text{th}}$ image. The superscript of $\mathbf{P}_i$ indicates it is from source by $s$  or target by $t$, from prediction by $o$ or ground truth by $w$. The two coefficients, $\alpha$ and $\beta$, are used to balance between the source and target domain white-patch map. Since the area of the inpainted pixels in the white-patch map are usually small, we set the value of $\alpha$ to $2$ and $\beta$ to $1$.

We utilize one-side label smoothing \cite{DBLP:conf/cvpr/SzegedyVISW16,DBLP:journals/corr/Goodfellow17} to improve the generalization ability of the trained model. Since the inpainted areas occupy a small proportion of the input image, we only do label smoothing for pixels not in the inpainted area (those pixels with value 0 in the white patch map), denoted as one-target label smoothing in our experiments. Precisely, we only set $0$ to $0.2$ in the ground truth white patch map.

%\xcc{ We utilize one-side label smoothing to improve the generalization ability of the trained model \cite{DBLP:conf/cvpr/SzegedyVISW16,DBLP:conf/cvpr/LiuAGD22,DBLP:conf/cvpr/AlkanatABW22,DBLP:journals/nn/MaBLWZZGLY22,DBLP:journals/pr/LiuQWTS22}. Since the inpainted areas occupy a small proportion of the input image, we only do label smoothing for target domain data, called one-target label smoothing. After one-target label smoothing, the pixel value of the region without inpainting is smoothed to 0.2.}



\subsection{Layout Generator Network Training}
When updating the generator network in the GAN training, we expect to fool the updated discriminator in the detection of inpainted pixels. Therefore, the loss $L_{PD}$ is modified to penalize the generator network if the discriminator outputs pixels with value $1$. Thus, we have:
  \begin{equation}
    \begin{aligned}
    {L}^{G}_{PD} =
    \frac{1}{N_{p}} \sum^{N_{p}}_{i=1} (
    {\left\vert {\hat{\mathbf{P}}^{s}_i - {\mathbf{P}}^{s,o}_i} \right\vert} * \alpha \\ + {\left\vert {{\mathbf{P}}^{t,w}_i - {\mathbf{P}}^{t,o}_i} \right\vert} * \beta),
    \end{aligned}
\end{equation}
where the values of pixels in $\hat{\mathbf{P}}^{s}_i$ are all set to \xcc{$0.2$}. 
The training loss for the layout generator network is as follows:
\begin{equation} \label{eq3}
    {L}_{G} = L_{rec} + \gamma * L^{G}_{PD},
\end{equation}
where the value of the weight coefficient $\gamma$ is set to 6, and the $L_{rec}$ is the reconstruction loss to penalize the deviation between the graphic layout generated by the network and the annotated ground-truth layout for the inpainted images in the source domain.  We calculate the reconstruction loss $L_{rec}$ as the direct set prediction loss in \cite{DBLP:conf/eccv/CarionMSUKZ20}. 
 
 %We calculate the reconstruction loss $L_{rec}$ as the direct set prediction loss in \cite{DBLP:conf/eccv/CarionMSUKZ20}, which is as follows:
  %loss
% \begin{equation}
%  \begin{aligned}
%    L_{rec}(y,\hat{y}) =
%    &\sum_{i=1}^{N} \big\left[-\log{\hat{p}_{\hat{\sigma}(i)}(c_i)} \\
%    &+ \mathds{1}_{\left\{ c_i \neq \varnothing \right\} }L_{box}(b_i,\hat{b}_{\hat{\sigma}}(i)) \big\right]
%    \end{aligned}
%\end{equation}
%where $y$ and $\hat{y}$ are the predicted and ground-truth layout. The correspondence between $y$ and $\hat{y}$ is recorded by $\sigma$, a vector of permutation computed using the Hungarian algorithm. For the $i_{\text{th}}$ element, the first term in $L_{rec}$ computes the negative log-likelihood of its class $c_i$, and the second term is used to compute the IoU and coordinate difference between the predicted box and GT box. Please refer to \cite{DBLP:conf/eccv/CarionMSUKZ20} for details.
 %bipartie matching
 %By bipartite matching between ground truth and prediction with the lowest cost can obtain:
 %  \begin{equation}
 %   \hat{\sigma} = {\underset{\sigma \in \mathfrak{S}_N}{\arg\min}\sum^{N}_{i}L_{match}(y_{i},\hat{y}_{\sigma(i)})}
%\end{equation}
%The $L_{match}(y_{i},\hat{y}_{\sigma(i)})$ is a pair-wise matching cost between ground truth and prediction with index $\sigma(i)$, which is computed following prior work \cite{DBLP:conf/cvpr/StewartAN16}.
%loss of bounding box
%\cite{DBLP:conf/cvpr/RezatofighiTGS019} to design the %box loss $L_{box}$ as:
%  \begin{equation}
%  \begin{aligned}
%    L_{box}(b_i,\hat{b}_{\sigma(i)}) = 
%    &\lambda_{iou}L_{iou}(b_i,\hat{b}_{\sigma(i)}) \\
%    &+\lambda_{L1}\left \| b_i - \hat{b}_{\sigma(i)} % \right\|_1
%    \end{aligned}
%\end{equation} 
%$\lambda_{iou}$ and $\lambda_{L1}$ are 
%hyperparameters.


\section{Experiments}
In this section, we mainly compare our model with SOTA layout generation methods and its ablation studies. More additional experimental analyses and designed advertising posters using generated layouts can be found in the supplementary materials.

 
  \begin{figure*}[!t]
    \centering
    \hspace{0cm}\includegraphics[width=16.6cm]{pictures/exp1_differentModelsv2.pdf}
    \caption{{\bf Qualitative evaluation for different models.} Layouts in a column are conditioned with the same image. And those in a row are from the same model. This figure qualitatively compares and analyzes different models from three aspects: text element background complexity, overlapping subject and overlapping product attention map.}
    \label{fig:3}
\end{figure*}

\begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{0.6mm}{
    \scalebox{1.0}{
    \begin{tabular}{lccc|cccc}
    \toprule
         Model   &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$\\
    \midrule
         LT     &40.92 &21.08 &1.310 &0.0156 &0.9516 &0.0049\\
         VTN      &41.77 &22.21 &1.323 &\bf{0.0130} &\bf{0.9698} &\bf{0.0047}\\
         Ours     &\bf{33.55} &\bf{12.77} &\bf{0.688} &0.0290 &0.9481 &0.0105\\
    \bottomrule
    \end{tabular}
    }
    }
    \caption{{\bf Comparison with content-agnostic methods.} $LT$ and $VTN$ represent LayoutTransformer and LayoutVTN, repectively.}
    \label{tab:content-unaware}
\end{table}

  \begin{figure*}[!h]
    \centering
    \hspace{0cm}\includegraphics[width=15.6cm]{pictures/CGL-PDA_2v2.pdf}
    \caption{More qualitative comparisons with CGL-GAN.}
    \label{fig:CGL-PDA}
\end{figure*}

\begin{table*}[!t]
    \centering
    \setlength{\tabcolsep}{2.18mm}{
    \scalebox{0.88}{
    \begin{tabular}{lccc|ccc|cccc}
    \toprule
         Model &Data \uppercase\expandafter{\romannumeral1} &Data \uppercase\expandafter{\romannumeral2} &Gaussian Blur  &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$ &$R_{occ}\uparrow$\\
    \midrule
         CGL-GAN &\checkmark &\quad &\quad      &33.85 &13.88 &0.766 &0.0299 &0.9351 &0.0139 &\bf{99.7}\\
         CGL-GAN &\checkmark &\quad &\checkmark      &- &- &- &2.5826 &- &- &-\\
        %  CGL-GAN &\quad &\checkmark &\quad      &33.13 &7.86 &0.524 &0.0427 &0.8937 &0.0193 &95.9\\

         CGL-GAN &\quad &\checkmark &\checkmark  &35.77 &15.47 &0.805 &\bf{0.0233} &0.9359 &\bf{0.0098} &99.6\\
         PDA-GAN~(Ours) &\checkmark &\quad &\quad   &\bf{33.55} &\bf{12.77} &\bf{0.688} &0.0290 &\bf{0.9481} &0.0105 &\bf{99.7}\\
    \bottomrule
    \end{tabular}
    }
    }
    \caption{{\bf Comprehensive comparison between CGL-GAN and PDA-GAN~(Ours).} Data \uppercase\expandafter{\romannumeral1} and Data \uppercase\expandafter{\romannumeral2} contain 8,000 and 54,546 source domain samples, respectively. \checkmark indicates the experiment configuration. The symbol "-" indicates that the model cannot complete the layout generation task since the generated element bounding boxes overlap with each other severely.
    % Data \uppercase\expandafter{\romannumeral1} is composed of 8000 samples randomly selected from Data \uppercase\expandafter{\romannumeral2}, which as CGL-Dataset contains 54546 samples. \checkmark indicates that this configuration is used in the ablation experiment configuration.
    }
    \label{tab:CGL-GAN}
\end{table*}

\subsection{Implementation Details}
We implement our PDA-GAN in PyTorch and use Adam optimizer for training the model. 
%The initial learning rates for the generator and pixel-level discriminator are $10^{-4}$ ($10_^-5$ for the generator backbone). 
\xcc{The initial learning rates are $10^{-5}$ for the generator backbone and $10^{-4}$ for the remaining part of this model.} The model is trained for 300 epochs with a batch size of 128, and all learning rates are reduced by a factor of 10 after 200 epochs. 
To make the fair experimental comparisons, 
%we are consistent with 
\xcc{we follow} CGL-GAN~\cite{DBLP:conf/ijcai/ZhouXMGJX22} to resize the inpainted posters and product images to $240 \times 350$ as inputs of our PDA-GAN. The total training time is about 8 hours using 16 NVIDIA V100 GPUs.

% %From each source and target domain data, 
% We randomly select 8000 samples from source and target domain respectively to compose the training data of PDA-GAN (16000 in total).
%Data1 and Data2
\xcc{We observe that, during training, the network is prone to bias towards source domain data. It might be due to the additional reconstruction loss for the source domain to supervise the generator of the model. Therefore, to balance the influence of the two domains, 8000 samples are randomly selected from CGL-Dataset as the source domain data. In each epoch, the 8000 source domain samples are processed, and another 8000 samples of the target domain images are randomly selected. We refer to this choice of training data as Data I. If all the CGL-Dataset training images are used for a comparison, we refer to it as Data II. In the following, if not clearly mentioned, our model is trained with Data I.}

  
 \subsection{Metrics} 
 \xcc{For} quantitative evaluations, we follow \cite{DBLP:conf/ijcai/ZhouXMGJX22} to divide layout metrics into \xcc{the} composition-relevant metrics and \xcc{the} graphic metrics. \xcc{The} composition-relevant metrics include $R_{com}$, and $R_{shm}$, $R_{sub}$, which measure background complexity, occlusion subject degree, and occlusion product degree respectively; \xcc{while the} graphic metrics include $R_{ove}$, $R_{und}$, and $R_{ali}$, which measure layout overlap, underlay overlap, and layout alignment degree respectively. 
 When the $R_{ove}$ value of a model exceeds 0.05, the generated element bounding boxes will overlap each other severely, resulting in useless layouts. This means that the high value of $R_{ove}$ indicates a failure in the layout generation for most images.
 \xcc{Moreover, we use metric $R_{occ}$ to represent} the ratio of non-empty layouts predicted by models. We will use all the above metrics to compare each group's experiments to verify \xcc{the effectiveness of our model}.
 \xcc{The formal definitions of these metrics and corresponding examples of graphic layouts are shown in the supplementary material.}
 %The supplementary material shows the equation and the interpretation of metrics.

 \subsection{Comparison with State-of-the-art Methods}
 \noindent\textbf{Layout generation with image contents.} 
 %In this line of experiments,
 \xcc{We first conduct experiments to} compare our method with ContentGAN and CGL-GAN that can generate image-aware layouts. The quantitative results can be seen from \cref{tab:content-aware}. Our model achieves the best results in most metrics, especially in the composition-relevant metrics since PDA-GAN preserves the image color and texture details. For instance,
 % PDA-GAN performance of background complexity $R_{com}$ is better than contentGAN and CGL-GAN by $26.4\%$ and $6.21\%$, respectively. 
 \xcc{our PDA-GAN outperforms contentGAN and CGL-GAN by $26.4\%$ and $6.21\%$ respectively, with regard to background complexity $R_{com}$.}
 As shown in the first column in \cref{fig:3}, compared with \xcc{these by} contentGAN and CGL-GAN, bounding boxes of text element generated by PDA-GAN are more likely to appear in simple background areas, which improves the readability of the text information. As shown in the second and third columns, when the background of the text element is complex, PDA-GAN will generate an underlay bounding box to replace the complex background to \xcc{enhance} the readability of text information.

 %Comparing the occlusion subject degree $R_{shm}$ of contentGAN and CGL-GAN, PDA-GAN is reduced by $25.2\%$ and $17.5\%$. 
 \xcc{Comparing contentGAN and CGL-GAN, our PDA-GAN reduces the occlusion subject degree $R_{shm}$ by $25.2\%$ and $17.5\%$ respectively.} From the middle three columns of \cref{fig:3}, 
 \xcc{for contentGAN or CGL-GAN, the presentation of the subject content information are largely affected since the generated layout bounding boxes would inevitably occlude subjects.}
 %when layout bounding boxes generated by contentGAN or CGL-GAN occlude subjects, the presentation of the subject content information will be diminished. 
 In particular, it should be noted that when the layout bounding box occludes the critical regions of the subject, such as the human head or face, the visual effect of the poster will be unpleasing, 
 \xcc{taking the image in row-3-column-6 as an example.}
 %The image in the third row and the sixth column is an example. 
 In contrast, layout bounding boxes generated by PDA-GAN avoid subject regions nicely, \xcc{thus the} generated posters better express the information of subjects and layout elements.
 
 %需要在前面定义说明一下subject 与 product的区别; 需要说明attention map的由来
 Meanwhile, the occlusion product degree $R_{sub}$ of PDA-GAN performance \xcc{surpass} contentGAN and CGL-GAN by $39.8\%$ and $14.5\%$ respectively. The three rightmost columns in \cref{fig:3} are the heat maps of the attention of each pixel to the product in the image. \xcc{We get attention maps of product images (queried by their category tags extracted on product pages) by CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21,DBLP:conf/iccv/CheferGW21}.} Compared with contentGAN and CGL-GAN, PDA-GAN generates layout bounding boxes on the region with lower thermal values to avoid occluding products. For example, in the seventh column, the layout bounding box generated by PDA-GAN effectively avoids the region with high thermal values of the product, which makes the hoodie information of the product can be fully displayed.
 The above quantitative and qualitative comparisons of models demonstrate that PDA-GAN improves the relationship modeling between image contents and graphic layouts.
 
 

 \noindent\textbf{Layout generation without image contents.} We also compare with image-agnostic methods of LayoutTransformer~\cite{DBLP:conf/iccv/GuptaLA0MS21} and LayoutVTN~\cite{DBLP:conf/cvpr/ArroyoPT21}. As shown in \cref{tab:content-unaware}, these image-agnostic methods perform pretty well on graphic metrics. \xcc{However, in term of composition-relevant metrics, our model is much better. In detail, our PDA-GAN beyonds LayoutTransformer and LayoutVTN by $18.0\%$ and $19.7\%$ respectively with regard to $R_{com}$. That is because these image-agnostic methods only care about the relationship between elements while do not account for image contents.} 
 %The reason is that these image-agnostic methods only care about the relationship between elements and do not account for image contents. Therefore, in terms of composition-relevant metrics, our model is much better than these image-agnostic methods. PDA-GAN performance of $R_{com}$ better than LayoutTransformer and LayoutVTN by $18.0\%$ and $19.7\%$ respectively. 
These image-agnostic methods \xcc{are prone to} generate bounding boxes of text elements in the area with complex backgrounds(as shown in the first two rows and \xcc{the leftmost} three columns of \cref{fig:3}), which will reduce the readability of the text information.  \xcc{Furthermore, }compared with LayoutTransformer and LayoutVTN, $R_{shm}$ of PDA-GAN is reduced by $39.4\%$ and $42.5\%$, and $R_{sub}$ is reduced by $47.5\%$ and $48.0\%$. The rightmost six columns in \cref{fig:3} show image-agnostic methods generate layout bounding boxes that randomly occlusion on the subject and product areas. These bounding boxes of layout elements will diminish the content and information presentation of the subject and product.

 \noindent\textbf{More comparisons with CGL-GAN.} As shown in the first and the last row of \cref{tab:CGL-GAN}, PDA-GAN outperforms CGL-GAN in all metrics with the same configuration. %The difference between PDA-GAN and CGL-GAN is that the PD replaces the discriminator in CGL-GAN. 
 \xcc{PDA-GAN differs from CGL-GAN that it uses PD to replace the discriminator in CGL-GAN.}
 The number of the PD parameters is 332,545, less than $2\%$ of the discriminator (22,575,841) in CGL-GAN, which significantly reduces the memory and computation cost of PDA-GAN. 
 %这里或者是在其他地方（supplementary或footnote）要解释一下为什么数据少的时候可以不做高斯模糊
 The second row of \cref{tab:CGL-GAN} shows that the model training on Data \uppercase\expandafter{\romannumeral1} with Gaussian blur performs poorly on $R_{ove}$, which causes most bounding boxes to overlap each other. 
%  As shown in the third row, CGL-GAN training on Data \uppercase\expandafter{\romannumeral2} without Gaussian blur, Which performs unpleasing on graphic metrics and $R_{occ}$ due to the domain gap. 
 \xcc{Intuitively, }Gaussian blur can narrow the domain gap, but it will also cause the image color and texture details lost. From the last two rows of \cref{tab:CGL-GAN}, PDA-GAN without Gaussian blur is far better than CGL-GAN with Gaussian blur on composition-relevant metrics. 
 %Compared with CGL-GAN, from the sixth and eighth columns of the first two rows in \cref{fig:CGL-PDA}, 
 \xcc{As illustrated in the the sixth and eighth columns of the first two rows in \cref{fig:CGL-PDA}, compared with CGL-GAN, }
 PDA-GAN generates text bounding boxes with the simpler background. 
 \xcc{It is interesting to observe from the first two rows of \cref{fig:CGL-PDA} that when PDA-GAN generates box among complexity background, it tends to additionally generate an underlay bounding box which covers the complex background to ensure the readability of the text information.}
 %As can be seen from other generated layouts in the first two rows, when the box with complexity background generated by PDA-GAN, which will generate an underlay bounding box to replace the complex background to ensure the readability of the text information. 
 The last two \xcc{rows} show that layouts generated by PDA-GAN can effectively avoid the subject area, \xcc{and} then \xcc{can generate} posters better express the information of subjects and layout elements.
 
  Both above quantitative and qualitative evaluations demonstrate that PDA-GAN can capture the subtle interaction between image contents and graphic layouts and achieve the SOTA performance. Refer to the supplementary for more details.
 
%  \begin{table*}[!h]
%     \centering
%     \setlength{\tabcolsep}{2.18mm}{
%     \scalebox{0.98}{
%     \begin{tabular}{cccc|ccc|cccc}
%     \toprule
%          Data \uppercase\expandafter{\romannumeral1} &Data \uppercase\expandafter{\romannumeral2} &Gaussian Blur &PDA &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$ &$R_{occ}\uparrow$\\
%     \midrule
%          \checkmark &\quad &\quad &\quad   &34.07 &15.13 &0.800  &0.0350 &0.9259 &0.01079 &99.9\\
%          \checkmark &\quad &\checkmark &\quad    &- &- &- &6.2726 &- &-  &-\\
%          \quad &\checkmark &\quad &\quad      &32.96 &7.68 &0.525 &0.0464 &0.9109 &0.0214 &95.6\\
%          \quad &\checkmark &\checkmark &\quad     &36.41 &14.58 &0.814  &0.0217 &0.9032 &0.0084 &99.5\\
%          \checkmark &\quad  &\quad &\checkmark   &33.55 &12.77 &0.688  &0.0290 &0.9481 &0.0105 &99.7 \\
%     \bottomrule
%     \end{tabular}
%     }
%     }
%     \caption{{\bf Quantitative ablation study on PD.} Data \uppercase\expandafter{\romannumeral1} is composed of 8000 samples randomly selected from Data \uppercase\expandafter{\romannumeral2}, which as CGL-Dataset contains 54546 samples. \checkmark indicates that this configuration is used in the ablation experiment configuration. Taking the last row as an example, the model uses Data \uppercase\expandafter{\romannumeral1} for training, without Gaussian blur for data, and introduces the module of PD.}
%     \label{tab:ablation for PD}
% \end{table*}

 \subsection{Ablations}
 
%  %这块和上面一组有点重复，暂时在正文中删除
%  \noindent\textbf{Effects of the PD.} This set of ablation experiments based on the same generator module is designed to verify the effectiveness of the proposed PD, which derives from the idea of unsupervised domain adaption. Compared with the first and last row in \cref{tab:ablation for PD}, under the same configuration, the model with the PD achieves better results in all metrics except $R_{occ}$. When the model training on little labeled data with Gaussian blur, which may make the model unstable and prone to collapse, as shown in the second row. The third row shows that the model training on Data \uppercase\expandafter{\romannumeral2} and without Gaussian blur performs poorly on the $R_{occ}$ and graphic metrics. 
 
%  The fourth row in \cref{tab:ablation for PD} shows that the model training on Data \uppercase\expandafter{\romannumeral2} with Gaussian blur improves performances of graphic metrics. However, the image loses color and texture details due to Gaussian blur, which leads the model to perform unpleasing on the composition-relevant metrics. On the contrary, the PD introduced in the model can eliminate the domain gap to perform better on the composition-relevant metrics. The detailed analysis and proof of PD eliminating the domain gap can be seen in the supplementary.
 
 
%   %这块和上面一组有点重复，暂时在正文中删除
%  \noindent\textbf{Affects of the Gaussian blur.} In \cref{tab:PDA+blur} and \cref{fig:4}, based on the model of PDA-GAN, we both quantitatively and qualitatively analyze the effect of the Gaussian blur on the layout generation. When PDA-GAN utilizing Gaussian blur, $R_{com}$ is increased from 33.55 to 36.71. Boxes 2, 6, and 10 in \cref{fig:4} show that the background of the text bounding box generated by the model with Gaussian blur is more complex, reducing the readability of the text information. In comparison, boxes 7 and 11 show that the model without Gaussian blur generates the bounding box with simple background or simultaneously generates an underlay bounding box to replace the complex background.
 
%  \cref{tab:PDA+blur} shows that $R_{shm}$ and $R_{sub}$ of the model with Gaussian blur are increased from 12.77 to 20.14 and 0.688 to 1.036, respectively. As shown in \cref{fig:4}, layout bounding boxes generated by the model with Gaussian blur are more likely to occlude regions of subjects or products. These layouts will diminish the presentation of subjects and layout elements information in posters. These quantitative and qualitative analyses demonstrate that the lost image details caused by Gaussian blur will degrade the quality of the generated image-aware layout.

%   \begin{figure*}[!h]
%     \centering
%     \hspace{0cm}\includegraphics[width=17.6cm]{pictures/PDA_blur_2.pdf}
%     \caption{{\bf Qualitative evaluation for Gaussian blur.} Each row layouts are generated by models with the same image as input. Layouts in each column are generated by feeding different inputs to the model. Ours$^*$ means our model with Gaussian blur. The blue number corresponds to the enlargement of the element represented by the yellow number on the figure. The left part of the dotted line is the presentation of layouts on the input image, and the right part is the presentation of layouts on the product attention heat map.}
%     \label{fig:4}
% \end{figure*}
  
%  \begin{table}[!h]
%     \centering
%     \setlength{\tabcolsep}{0.62mm}{
%     \scalebox{1}{
%     \begin{tabular}{lccc|ccc}
%     \toprule
%          Model   &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$\\
%     \midrule
%          Ours^*      &36.71 &20.14 &1.036 &0.0475 &0.9376 &0.0068\\
%          Ours     &33.55 &12.77 &0.688 &0.0290 &0.9481 &0.0105\\
%     \bottomrule
%     \end{tabular}}}
%     \caption{{\bf Quantitative ablation study on Gaussian blur.} Ours$^*$ means the model of PDA-GAN with Gaussian blur for input image.}
%     \label{tab:PDA+blur}
% \end{table}
 

 \noindent\textbf{Effects of pixel-level Discriminator.} We first compare our PD with a global discriminator that only predicts one real or fake probability as in classical GAN. The abbreviation DA in \cref{tab:PDA-DA} indicates the global discriminator strategy. When the weight of DA loss ($\gamma$ in \cref{eq3}) is more than 0.01, the model cannot complete the layout generation task, indicated by the symbol $-$, since the $R_{ove}$ value is too high. From the statistics in \cref{tab:PDA-DA}, our PD outperforms DA on all metrics. 
 
Second, we compare the PD with the strategy in PatchGAN~\cite{DBLP:conf/cvpr/IsolaZZE17}. The scores of quantitative metrics listed in \cref{tab:PDA-PatchDA} also verify the advantage of PD. The patch size in this table means the dimension of the output map, which will be compared with correspondingly resized ground-truth white patch map during training. These experiments show that, since the discrepancy by inpainting exists between pixels, the model might be required to eliminate the domain gap at the pixel level. Moreover, the pixel-level strategy can be considered as the most fine-grained patch level strategy. 

%From \cref{tab:PDA-DA} and \cref{tab:PDA-PatchDA}, when the domain adaption discriminator has a corresponding output value for each input-image pixel, the model can effectively eliminate the domain gap and have better performances in layout generation. 

 \begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{0.58mm}{
    \scalebox{0.96}{
    \begin{tabular}{l|ccc|cccc}
    \toprule
         Model-$W$  &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$\\
    \midrule
         DA-6.0      &- &- &- &9.0000 &- &-\\
         DA-1.0      &- &- &- &8.9995 &- &-\\
         DA-0.01      &- &- &- &4.7764 &- &-\\
         DA-0.001      &34.41 &13.78 &0.749 &0.0327 &0.9299 &0.0110\\
         DA-0.0001     &34.77 &14.62 &0.777 &0.0345 &0.9234 &0.0122\\
         DA-0.0     &34.07 &15.13 &0.800  &0.0350 &0.9259 &0.0108\\

         PDA-6.0   &\bf{33.55} &\bf{12.77} &\bf{0.688} &\bf{0.0290} &\bf{0.9481} &\bf{0.0105}\\
    \bottomrule
    \end{tabular}
    }
    }
    \caption{{\bf Ablation study with discriminator level.} DA indicates the global discriminator strategy in classical GAN-based methods, which outputs one probability value for real or fake for an image. The PDA output is a pixel-level map, and its dimension is same with the input image.  $W$ refers to the weight of DA (or PDA) module loss in the training process. Please refer to ~\cref{tab:CGL-GAN} for the explanation of the symbol "-".}
    \label{tab:PDA-DA}
\end{table}

\begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{0.58mm}{
    \scalebox{0.96}{
    \begin{tabular}{l|ccc|cccc}
    \toprule
         Patch size  &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$\\
    \midrule
         12*8      &- &- &- &0.9288 &- &-\\
         24*16      &33.67 &16.00 &0.844 &0.0438 &0.9407 &\bf{0.0075}\\
         44*30      &34.03 &13.02 &0.752 &\bf{0.0284} &0.9377 &0.0119\\
         88*60      &\bf{32.65} &13.35 &0.735 &0.0325 &0.9173 &0.0094\\

         350*240   &33.55 &\bf{12.77} &\bf{0.688} &0.0290 &\bf{0.9481} &0.0105\\
    \bottomrule
    \end{tabular}
    }
    }
    \caption{{\bf Quantitative ablation study PatchGAN-based methods.} Patch size means the size of the map output by the discriminator. The input image height and width are 320 and 240 respectively. We train these models with $\gamma$ in \cref{eq3} equal to 6. Please refer to ~\cref{tab:CGL-GAN} for the explanation of the symbol "-".}
    \label{tab:PDA-PatchDA}
\end{table}

 %前面还是要说明一下inpainting给图像带来的干扰就是文中的domain gap
 
 \noindent\textbf{Effects of PD with different level feature maps.} In our model, PD is connected to the shallow level feature maps of the first residual block. We now investigate how the PD works if the deep level feature, i.e. the feature from fourth residual block, and the fused feature in multi-scale CNN~\cite{DBLP:conf/ijcai/ZhouXMGJX22}, i.e. the fusion of first to fourth residual block feature map, are used in the PD. As shown in \cref{tab:PDA_layer}, discriminating with shallow feature map in PDA-GAN can achieve better results in both composition-relevant and graphic metrics on average. Again, this experiment verifies the advantage of our design of PD. Intuitively, bridging the domain gap at early stage of the network might be 
 beneficial to the subsequent model processing.

 
 \noindent\textbf{Effects of label smoothing.}  \cref{tab:label_smoothing} shows that the model with one-target label smoothing performs better in all metrics than without label smoothing. In addition, the effects of two-side  or one-source label smoothing are not as good as one-target label smoothing on average. For the ground truth map input to the discriminator, the two-side label smoothing means we set 0 to 0.2 and 1 to 0.8, and one-source label smoothing means we only set 1 to 0.8.
 
\begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{0.54mm}{
    \scalebox{0.98}{
    \begin{tabular}{lccc|ccc}
    \toprule
         Feature map   &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$\\
    \midrule
         deep level      &34.22 &13.97 &0.770 &0.0396 &0.9366 &0.0118\\
         fusion      &35.36 &14.54 &0.817 &0.0310 &\bf{0.9513} &0.0117\\
         shallow level     &\bf{33.55} &\bf{12.77} &\bf{0.688} &\bf{0.0290} &0.9481 &\bf{0.0105}\\
    \bottomrule
    \end{tabular}}
    }
    \caption{{\bf Quantitative ablation study on different level feature maps for pixel-level discriminator.}}
    \label{tab:PDA_layer}
\end{table}

 \begin{table}[!t]
    \centering
    \setlength{\tabcolsep}{0.6mm}{
    \scalebox{0.98}{
    \begin{tabular}{lccc|ccc}
    \toprule
         smoothing   &$R_{com}\downarrow$ &$R_{shm}\downarrow$ &$R_{sub}\downarrow$ &$R_{ove}\downarrow$ &$R_{und}\uparrow$ &$R_{ali}\downarrow$\\
    \midrule
         Without      &33.61 &14.04 &0.718 &0.0346 &0.9188 &0.0106\\
         two-side      &33.66 &14.67 &0.794 &0.0334 &0.9297 &0.0098\\
         one-source    &\bf{32.20} &15.23 &0.799 &0.0431 &0.9234 &\bf{0.0085}\\
         one-target     &33.55 &\bf{12.77} &\bf{0.688} &\bf{0.0290} &\bf{0.9481} &0.0105\\
    \bottomrule
    \end{tabular}}
    }
    \caption{{\bf Ablation study on different label smoothing choice.} The first row is the model without label smoothing. Two-side: set 0 to 0.2 and 1 to 0.8; one-source: set 1 to 0.8; and 0ne-target: set 0 to 0.2. }
    \label{tab:label_smoothing}
 \end{table}

\section{Conclusion}
In this paper, we study the domain gap problem between clean product images and inpainted images in CGL-Dataset for generating poster layouts. To solve this problem, we propose to leverage the unsupervised domain adaptation technique and design a pixel-level discriminator. This design of discriminator can not only finely align image features of these two domains, but also avoid the Gaussian blurring step in the previous work~(CGL-GAN), which brings benefits to modeling the relationship between image details and layouts. Both quantitative and qualitative evaluations demonstrate that our method can achieve SOTA performance and generate high-quality image-aware graphic layouts for posters. In the future, we may investigate how to better interact with user constraints, e.g. categories and coordinates of elements, and enhance the layout generation diversity.

\section*{Acknowledgements}
This work is supported by Information Technology Center and State Key Lab of CAD\&CG, Zhejiang University.

% This paper focuses on eliminating the domain gap to generate an image-aware advertising poster graphic layout. We combine the unsupervised domain adaption idea to design a pixel-level discriminator, which calculates the GAN loss for each input-image pixel. The PDA can eliminate the domain gap and retain the input image's color and texture details. Both quantitative and qualitative evaluations demonstrate that PDA-GAN can achieve the SOTA performance and generate high-quality image-aware graphic layouts for advertising posters. Our end goal is to build a fully automated system that is able to translate the input image into an aesthetic poster. From fully achieving the objective, we believe there are some interesting questions to be answered in the future. 

% %%%%%%%%% ABSTRACT
% \begin{abstract}
%   The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
%   Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
%   The abstract is to be in 10-point, single-spaced type.
%   Leave two blank lines after the Abstract, then begin the main text.
%   Look at previous CVPR abstracts to get a feel for style and length.
% \end{abstract}

% %%%%%%%%% BODY TEXT
% \section{Introduction}
% \label{sec:intro}

% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% %-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of cvpr.sty to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%   In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%   Why the previous paper was accepted without this analysis is beyond me.

%   [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%   We describe a system for zero-g frobnication.
%   This system is new because it handles the following cases:
%   A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%   Ours handles it by including a foo term in the bar integral.

%   ...

%   The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%   It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%   \caption{Example of caption.
%   It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%   \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%   ``Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% % Update the cvpr.cls to do the following automatically.
% % For this citation style, keep multiple citations in numerical (not
% % chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% % \cite{Alpher02,Alpher03,Authors14}.


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

% %------------------------------------------------------------------------
% \section{Formatting your paper}
% \label{sec:formatting}

% All text must be in a two-column format.
% The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
% Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
% The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
% The second and following pages should begin 1 inch (2.54 cm) from the top edge.
% On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
% for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
% page.

% %-------------------------------------------------------------------------
% \subsection{Margins and page numbering}

% All printed material, including text, illustrations, and charts, must be kept
% within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
% high.
% %
% Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
% The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
% The \LaTeX\ template takes care of this when used properly.



% %-------------------------------------------------------------------------
% \subsection{Type style and fonts}

% Wherever Times is specified, Times Roman may also be used.
% If neither is available on your word processor, please use the font closest in
% appearance to Times to which you have access.

% MAIN TITLE.
% Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
% The title should be in Times 14-point, boldface type.
% Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
% do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
% Leave two blank lines after the title.

% AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
% and printed in Times 12-point, non-boldface type.
% This information is to be followed by two blank lines.

% The ABSTRACT and MAIN TEXT are to be in a two-column format.

% MAIN TEXT.
% Type main text in 10-point Times, single-spaced.
% Do NOT use double-spacing.
% All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
% Make sure your text is fully justified---that is, flush left and flush right.
% Please do not place any additional blank lines between paragraphs.

% Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
% Short captions should be centred.

% \noindent Callouts should be 9-point Helvetica, non-boldface type.
% Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

% FIRST-ORDER HEADINGS.
% (For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

% SECOND-ORDER HEADINGS.
% (For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
% If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

% %-------------------------------------------------------------------------
% \subsection{Footnotes}

% Please use footnotes\footnote{This is what a footnote looks like.
% It often distracts the reader from the main flow of the argument.} sparingly.
% Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
% If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
% Use Times 8-point type, single-spaced.


% %-------------------------------------------------------------------------
% \subsection{Cross-references}

% For the benefit of author(s) and readers, please use the
% {\small\begin{verbatim}
%   \cref{...}
% \end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
% This will automatically insert the appropriate label alongside the cross-reference as in this example:
% \begin{quotation}
%   To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
%   It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
%   You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
% \end{quotation}
% If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
% {\small\begin{verbatim}
%   \Cref{...}
% \end{verbatim}}
% command. Here is an example:
% \begin{quotation}
%   \Cref{fig:onecol} is also quite important.
% \end{quotation}

% %-------------------------------------------------------------------------
% \subsection{References}

% List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
% When referenced in the text, enclose the citation number in square brackets, for
% example~\cite{Authors14}.
% Where appropriate, include page numbers and the name(s) of editors of referenced books.
% When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
% If you use the template as advised, this will be taken care of automatically.

% \begin{table}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Method & Frobnability \\
%     \midrule
%     Theirs & Frumpy \\
%     Yours & Frobbly \\
%     Ours & Makes one's heart Frob\\
%     \bottomrule
%   \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}

% %-------------------------------------------------------------------------
% \subsection{Illustrations, graphs, and photographs}

% All graphics should be centered.
% In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
% Instead use
% {\small\begin{verbatim}
%   \centering
% \end{verbatim}}
% at the beginning of your figure.
% Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
% Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
% Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
% You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

% When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
% {\small\begin{verbatim}
%   \usepackage{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.pdf}
% \end{verbatim}
% }


% %-------------------------------------------------------------------------
% \subsection{Color}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

% If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
% Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

% %------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.

% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{CVPR2023}
}

\end{document}
