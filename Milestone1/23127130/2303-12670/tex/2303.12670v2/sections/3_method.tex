% !TEX root = ../main.tex
%--------------------------------------------------------
\section{Approach}
\label{sec:methods}
%--------------------------------------------------------
Our correlational image modeling (\methodname) is a simple yet effective self-supervised pre-training approach. As illustrated in Figure~\ref{fig:teaser_mfm}, we formulate a \textbf{\textit{crop-and-correlate}} pretext task that crops a random image region (\exemplar) from an input image (\context) and predicts the correlation map between the \exemplar and \context. Our \methodname consists of four components: cropping strategy, encoder, decoder, and loss function. In the following, we first introduce correlation operations in Section~\ref{subsec:preliminary}. We subsequently detail each component of \methodname in Section~\ref{subsec:cim}. We finally discuss the relation of our \methodname with the unsupervised visual tracking task in Section~\ref{subsec:comparison}.

\subsection{Preliminary: Correlation Operation}
\label{subsec:preliminary}
Given an \exemplar image $\vz\in\mathbb{R}^{h_z \times w_z \times 3}$ along with a typically larger \context image $\vc \in\mathbb{R}^{
h_c \times w_c \times 3}$, a correlation operation between the \exemplar and \context images is defined as follows:
\begin{equation}\label{eq:corr_ops}
f\left(\vz, \vc\right) = f_\theta(\vz) \star f_\theta(\vc) + \vb \mathds{1},
\end{equation}
where $\star$ denotes a correlation operator and $f_\theta$ is a backbone model to extract corresponding representations. Here, $\vb\mathds{1}$ represents a signal that takes value $\vb$ in every location.  Conceptually, it means that a dense similarity of two sets is measured in a 2D fashion. For instance, in standard Siamese-based trackers~\cite{bertinetto2016fully,valmadre2017end,li2018high,wang2019fast}, $\star$ is normally instantiated as a 2D convolution operator, in which the exemplar feature $f_\theta(\vz)$ takes the role of convolutional kernels, sliding over the spatial region of context feature $f_\theta(\vc)$.
For Transformer-based trackers~\cite{chen2021transformer,xie2022correlation,cui2022mixformer,ma2022unified,song2022transformer}, a cross-attention layer combines information from two images to generate a merged representation, which selectively highlights the hotspots in the \context. 
\begin{figure}[!t]
    \centering
    \includegraphics[page=1,width=1.0\linewidth]{figures/pipeline.pdf}
    \vskip -0.3cm
    \caption{
        The overview of our proposed \methodname pre-training framework. Given an image $\vc$ (\context), we crop a random region $\vz$ (\exemplar) within \context $\vc$. The \context and \exemplar images are separately passed through a target encoder $f_\xi$ and an online encoder $f_\theta$ to obtain latent representations $\vh_c$ and $\vh_z$, which are further fed into a lightweight decoder with a cross-attention layer and a linear predictor to predict the correlation map $\vy$. 
    }
    \label{fig:teaser_mfm}
    % \vspace{-10pt}
\end{figure}

\begin{figure*}[!ht]
    \centering
    \includegraphics[page=1,width=0.9\linewidth]{figures/construction.pdf}
    % \vskip -0.2cm
    \caption{
    The procedure to generate an \textit{exemplar-context} pair for \methodname. We control the scale, shape, and rotation of an \exemplar image by randomly sampling $r_0=\frac{B}{A}$, $r_1=\frac{h}{w}$ and, $\alpha$, where $A$ and $B$ are the areas of cropping region ($h\times w$) and \context image ($m\times m$).
    }
    \label{fig:crop}
    % \vspace{-10pt}
\end{figure*}


\subsection{Correlational Image Modeling}
\label{subsec:cim}

\noindent\textbf{Cropping strategy.} To enable effective correlational image modeling for self-supervised visual pre-training, we propose a random cropping strategy to construct \textit{exemplar-context} image pairs. Specifically, as shown in Figure~\ref{fig:crop}, given an original image $\vx\in\mathbb{R}^{ H\times W \times 3}$, we obtain a \context image $\vc\in\mathbb{R}^{ m\times m \times 3}$ with a square shape by first randomly cropping a sub-region followed by a resizing operation. Then, we repeat the \textit{crop-and-resize} process to generate a square \exemplar image $\vz\in\mathbb{R}^{ n\times n \times 3}$ from the \context in consideration of three aspects: scale, shape, and rotation. To control the scale of an \exemplar, we calculate the areas of both the cropping region and \context and compute the scale ratio $r_0$. The shape of the cropping region is determined by the height and width ratio $r_1$. Also, we measure the rotation degree $\alpha$ between the cropping region and \context image. By random sampling the values of $r_0$, $r_1$, and $\alpha$, we can obtain \exemplars with various scales, shapes, and rotations. The corresponding correlation map $\vy \in \{0, 1\}^{m \times m}$ can be derived from the cropping region easily. We further add different transformations to each \exemplar image to increase the data diversity. We will study the effects of different cropping strategies in the experiment section.

\noindent\textbf{Encoder.} The goal of \methodname is to learn useful representations with a backbone model $f_\theta$ in Equation~\ref{eq:corr_ops}, such that $f_\theta$ can be generalized to downstream tasks. Both ViT and CNN architectures can be applied as the encoder for \methodname.
For reliable pre-training, we employ a bootstrap encoder that consists of an online network $\theta$ and a target network $\xi$, which share the same backbone architecture. Given a pair of \exemplar and \context  ($\vz$ and $\vc$), we obtain the corresponding representations:
\begin{equation}\label{eq:encoder}
\vh_z = f_\theta(\vz);
\vh_c = f_\xi(\vc) 
\end{equation}
via the online network $f_\theta$ and target network $f_\xi$, respectively.
The parameters $\xi$ of the target network are updated from the online network $\theta$ with an exponential moving average policy~\cite{lillicrap2015continuous}:
\begin{equation}\label{eq:ema}
\xi = \tau\xi + (1 - \tau)\theta,
\end{equation}
where $\tau \in [0, 1]$ denotes a target decay rate. As a result, the online network $f_\theta$ is responsible for learning the representations to deal with various scales, shapes, and rotations for the \exemplar images. For efficient training, we consider cropping multiple \exemplars for each \context, and all the cropped \exemplars can be grouped together into one forward-backward processing.


\noindent\textbf{Decoder.} To model the correlation between \exemplar and \context images, we design a lightweight cross-attention decoder,   
which is a general form of multi-head attention layer in Transformers~\cite{vaswani2017attention}. 
To be specific, we first project the representations $\vh_z$ and $\vh_c$ to obtain the query, key, and value by linear mappings: $\vq = f_c(\vh_c)+\texttt{PE}; \vk = f_k(\vh_z); \vv = f_v(\vh_z)$. A positional encoding (\texttt{PE}) is added to the query for better decoding. 
The reason why we use the \context as a query rather than the \exemplar is that the output correlation map is of the shape determined by the \context input, not the \exemplar.
Then we can calculate the weighted representation for the \exemplar and \context pair as follows:
\begin{equation}
    \label{eq:ca}
    \va = \texttt{CrossAttention}(\vh_c, \vh_z) = \texttt{Softmax}\biggl(\frac{\vq\trans{\vk}}{\sqrt{d}}\biggl)\vv.
\end{equation}
After that, we compute the correlational representation with \textit{layernorm} and \textit{multilayer perceptron} modules:
\begin{equation}
    \label{eq:de}
    \vh = \vh_c + \va + \texttt{MLP}(\texttt{LN}(\vh_c + \va)).
\end{equation}
Finally, the output correlation map $\widehat{\vy}$ is obtained via a linear predictor and an \textit{upsampling} operation\footnote{https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html}:
\begin{equation}
    \label{eq:pre}
    \widehat{\vy} = \texttt{Upsample}(f_p(\vh)).
\end{equation}

\noindent\textbf{Loss function.} In practice, to optimize the overall \methodname model, we simply minimize a binary cross-entropy loss:
\begin{equation}
    \label{eq:all}
    \mathcal{L}(\widehat{\vy}, \vy) = - \frac{1}{m\times m} \sum^{m\times m}_{i=i} \vy_{i}\log(\widehat{\vy}_i) + (1 - \vy_{i})\log(1-\widehat{\vy}_i),
\end{equation}
between predicted correlation map $\widehat{\vy}$ and ground-truth $\vy$.

\subsection{Relation to Unsupervised Visual Tracking}
\label{subsec:comparison}

Our \methodname is generally related to studies on unsupervised and self-supervised visual tracking~\cite{wang2019unsupervised,wu2021progressive,shen2022unsupervised,lai2020mast,zheng2021learning}. These works explore effective training cues to bypass the need for extensive annotated data for training deep tracking models.
Typically, temporal consistency or correspondence in videos is leveraged as a cue. Several modeling techniques have been proposed, including forward and backward consistency~\cite{wang2019unsupervised}, progressive training~\cite{wu2021progressive}, multi-task learning~\cite{shen2022unsupervised}, and memory augmenting~\cite{lai2020mast}.
Despite the different strategies, all these unsupervised and self-supervised trackers mainly focus on learning task-specific representations for visual tracking from unlabeled videos. Thus, it is infeasible to apply these trackers with temporal modeling on still images, in which such temporal information does not exist.
On the contrary, the goal of our \methodname is to learn generic representations from unlabeled data with the transferable ability to downstream tasks. 
Therefore, we formulate correlational modeling in a more general form and develop it as a useful pretext task for self-supervised visual pre-training. 