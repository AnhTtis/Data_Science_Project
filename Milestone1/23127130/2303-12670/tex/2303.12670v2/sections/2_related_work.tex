% !TEX root = ../main.tex
\section{Related Work}
\label{sec:related_work}
%--------------------------------------------------------
\noindent\textbf{Unsupervised pretext tasks} play the fundamental role in self-supervised representation learning. Beyond \textit{augment-and-compare} and \textit{mask-and-predict}, a series of different unsupervised pretext tasks have been studied in the literature. For instance, Noroozi~\etal~\cite{noroozi2016unsupervised} train a context-free network without human annotation by solving Jigsaw puzzles, further developed in a very recent work~\cite{zhai2022position} by predicting positions from content images.
Bojanowski~\etal~\cite{bojanowski2017unsupervised} propose to learn discriminative features via predicting noise. Gidaris~\etal~\cite{gidaris2018unsupervised} treat the 2D rotation of an image as a supervisory signal. Zhang~\etal~\cite{zhang2019aet} follow this work to predict general affine transformations.
All these initiatives are proven less effective than the state-of-the-art MIM and MV-SSL approaches in large-scale visual pre-training.

\noindent\textbf{Multi-view self-supervised learning} approaches~\cite{wu2018unsupervised,he2020momentum,misra2020self,chen2020simple,chen2020improved,grill2020bootstrap,chen2021exploring,chen2021empirical,caron2021emerging,xie2022delving} are highly successful in learning representations over the past few years. These methods depend on an \textit{augment-and-compare} pretext task that models similarity and dissimilarity between two or more augmented views 
% via simple distance metrics (e.g., Euclidean or cosine distance) 
in an embedding space. Thus, MV-SSL greatly relies on data augmentations and Siamese networks~\cite{bromley1993signature}. There have been several general strategies for comparing augmented views. Most contrastive approaches, such as SimCLR~\cite{chen2020simple}, MoCo~\cite{he2020momentum,chen2020improved,chen2021empirical} measure both positive and negative pairs via cosine distance. On the contrary, BYOL~\cite{grill2020bootstrap} and SimSiam~\cite{chen2021exploring} rely only on positive pairs. Beyond contrastive learning, SwAV~\cite{caron2020unsupervised} resorts to online clustering and predicts cluster assignments of different views. In addition, there is another line of research in MV-SSL that extends the main focus of \textit{global} representations to \textit{dense} representations~\cite{pinheiro2020unsupervised,wang2021dense,xie2021propagate,selvaraju2021casting,henaff2021efficient,roh2021spatially,yang2021instance,xiao2021region,xie2021detco,xie2021unsupervised,wei2021aligning,li2022univip,bai2022point}.

\noindent\textbf{Masked image modeling} follows a \textit{mask-and-predict} pretext task, which is inspired by the successful masked language modeling (MLM) approaches in the NLP community, such as BERT~\cite{devlin2019bert} and RoBERTa~\cite{liu2019roberta}. Two key steps can be identified in a typical MIM pipeline: i) \textit{how to mask}, ii) \textit{what to predict}.
In terms of \textit{how to mask}, most MIM approaches, such as BEiT~\cite{bao2021beit}, MAE~\cite{he2021masked} and SimMIM~\cite{xie2021simmim}, extend the \textit{mask-word} recipe in MLM to randomly mask image patches in the spatial domain. Recent works consider other corruptions to replace the normal patch-masking process. For example, Xie~\etal~\cite{xie2022masked} investigate corruption operations (downsample, blur, and noise) in low-level image processing tasks and present a unified \textit{mask-frequency} recipe. Similarly, other degradation forms are studied in Tian~\etal~\cite{tian2022beyond}, including zoom, distortion, and decolorization. Besides, Fang~\etal~\cite{fang2022corrupted} employ an auxiliary generator to corrupt 
the input images. As to \textit{what to predict}, beyond default raw pixels~\cite{he2021masked,xie2021simmim}, several other reconstruction targets are proposed, \eg, hand-crafted or deep features~\cite{wei2021masked}, low or high frequencies~\cite{xie2022masked,liu2022devil}, and discrete tokens~\cite{bao2021beit}. 

\noindent\textbf{Correlational modeling} is the crucial process in visual tracking~\cite{yilmaz2006object}, aiming to predict a dense set of matching confidence for a target object. The seminal work of Correlation Filter~\cite{bolme2010visual} and its end-to-end Siamese-based variants~\cite{bertinetto2016fully,valmadre2017end,li2018high,wang2019fast,wang2021multiple} learn to distinguish targets from background images via convolution (\ie, cross-correlation). Recently, Transformer-based trackers~\cite{chen2021transformer,xie2022correlation,cui2022mixformer,ma2022unified,song2022transformer} employ a cross-attention mechanism to model the correlation between target objects and backgrounds. These promising correlation-based trackers motivate us to investigate the effectiveness of correlational modeling in the context of self-supervised visual pre-training. Notably, some unsupervised and self-supervised trackers~\cite{wang2019unsupervised,wu2021progressive,shen2022unsupervised,lai2020mast,zheng2021learning} normally conduct training on synthetic datasets without labeling. While similarly considering unsupervised or self-supervised training, our work significantly differs from these unsupervised and self-supervised trackers. We will discuss the differences in Section~\ref{subsec:comparison}.
