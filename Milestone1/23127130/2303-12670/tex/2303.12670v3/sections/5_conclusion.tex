% !TEX root = ../main.tex
%--------------------------------------------------------
\section{Conclusion}
\label{sec:conclude}
In this work, we present \methodname, a novel pretext task for self-supervised visual pre-training. Unlike existing MV-SSL and MIM approaches, \methodname considers correlation modeling in visual tracking as a useful pre-training paradigm. We build a generic self-supervised correlational modeling framework by proposing three unique designs, including a cropping strategy, bootstrap encoder, and correlation decoder. Extensive experiments on transfer learning and robustness evaluation with visual recognition tasks show that our \methodname can efficiently and effectively learn meaningful representations from unlabeled images.

% \newpage
\noindent\textbf{Acknowledgement.}
This study is supported under the RIE2020 Industry Alignment Fund â€“ Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). It is also supported by Singapore MOE AcRF Tier 2 (MOE-T2EP20120-0001).