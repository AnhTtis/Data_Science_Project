% !TEX root = ../main.tex
%--------------------------------------------------------
\section{Experiments}
\label{sec:exps}
%--------------------------------------------------------
\begin{table}[!h]
\centering
\small
\caption{\textbf{Ablations of cropping strategy} for \methodname with ViT-B/16 on ImageNet-200. 
}
\label{tab:ablation_crop}
% \addtolength{\tabcolsep}{-2pt}
% \vspace{-0.2cm}
\begin{subtable}[htp]{0.33\textwidth}
\centering
\small
\caption{\textbf{Crop scale}. 
% Random sampling of both filters works the best.
}
\label{tab:crop_scale}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
Scale    & Ratio $r_0$& Top-1 acc (\%) \\ 
\midrule
scratch  & -            & 77.79               \\ 
\midrule
fixed    & $r_0=0.16$         & 87.57   \\
% fixed    & 0.21         & 13.30 (collapse)    \\
random   & $r_0<0.16$   &     87.25  \\ 
random   & $r_0>0.16$    &     89.39  \\ 
random   & $r_0 \in (0 , 1.0)$   &     \cellcolor{gray!20}\textbf{89.48} \\ 

\bottomrule
\end{tabular}
}
\end{subtable}
\hfill

% \vspace{0.2cm}
\begin{subtable}[htp]{0.32\textwidth}
% \vspace{+0.15cm}
\centering
\small
\caption{\textbf{Crop shape}. 
% Using a fixed radius is enough.
}
% \vspace{-0.05cm}
\label{tab:crop_shape}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
Shape & Ratio $r_1$ & Top-1 acc (\%) \\ 
\midrule
square    &    1.0   & 89.48               \\
rectangle &$[3/4, 4/3]$  & 89.55 \\
rectangle & $[1/2, 2/1]$  & 89.59 \\
rectangle & $[1/3, 3/1]$  & \cellcolor{gray!20}\textbf{89.70} \\
rectangle & $[1/4, 4/1]$  & 89.66 \\
\bottomrule
\end{tabular}
}
\end{subtable}
\hfill

% \vspace{0.2cm}
\begin{subtable}[htp]{0.3\textwidth}
% \vspace{-0.6cm}
\centering
\small
\caption{\textbf{Rotation}. 
}
\label{tab:rotation}
% \vspace{+0.8cm}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{cc}
\toprule
Rotation $\alpha$ & Top-1 acc (\%) \\ 
\midrule
\ang{0}    & 89.70              \\
$[\ang{-45}, \ang{45}]$  & \cellcolor{gray!20}\textbf{89.97} \\
$[\ang{-90}, \ang{90}]$   & 89.91 \\ 
$[\ang{-135}, \ang{135}]$   & 89.19  \\ 
$[\ang{-180}, \ang{180}]$   & 89.07  \\ 
\bottomrule
\end{tabular}
}
\end{subtable}
\vfill

% \vspace{0.2cm}
\begin{subtable}[htp]{0.3\textwidth}
\centering
\small
\caption{\textbf{Transformation}. 
}
\label{tab:transformation}
\resizebox{0.90\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
\context & \exemplar & Top-1 acc (\%) \\ \midrule
\xmark     &  \xmark   & 89.97       \\
\cmark     &  \xmark   & 90.01       \\
\xmark     &  \cmark   & \cellcolor{gray!20}\textbf{90.12}     \\ 
\cmark     &  \cmark   & 90.12      \\\bottomrule
\end{tabular}
}
\end{subtable}
\hfill
\end{table}

\subsection{Main Properties}
To understand the unique properties of \methodname, we conduct ablation studies  on ImageNet-200~\cite{van2020scan}, a smaller subset of the ImageNet-1K dataset~\cite{deng2009imagenet}. 
For all ablation experiments, we choose ViT-Base (ViT-B/16) as the default backbone and follow a common setting used in existing works~\cite{bao2021beit,he2021masked}: \textit{300-epoch self-supervised pre-training without labels and 100-epoch supervised end-to-end fine-tuning}, to evaluate the quality of learned representations. 
For a fair comparison, we tailor the resolutions of \textit{context} and \textit{exemplar} as $160\times160$ and $64\times64$, respectively. By default, we crop six \exemplars for each input image (\context), in order to match with the standard $224\times224$ input size.\footnote{For ViT-Base (ViT-B/16) with $16\times16$ patch size, our configuration of one \textit{context} ($160\times160$) with six \textit{exemplars} ($64\times64$) contains 196 image patches in total, which is equivalent to an image with the size of $224\times224$.} More detailed pre-training and fine-tuning recipes are described in the supplementary material. We present our observations as follows: 

% ablating
\noindent\textbf{Cropping strategy.} We investigate how different cropping strategies will affect our \methodname in self-supervised representation learning. We consider four aspects of cropping \exemplars, \ie, scale, shape, rotation, and transformation:

\textbf{(\romannumeral1) \texttt{Scale}}: As shown in Table~\ref{tab:crop_scale}, we study the scale factor of \textit{exemplars} while keeping the shape and rotation fixed, \ie, square shape and \ang{0} rotation. We first consider cropping with fixed scale ratio $r_0 = \frac{64\times 64}{160\times 160} = 0.16$ and then study three random scale ratio schemes: small scale ($r_0 < 0.16$), large scale ($r_0 > 0.16$), and both small and large scales $r_0 \in (0, 1.0)$. All these entries perform significantly better than the baseline, \ie, training from scratch without pretraining. The random cropping policy that covers both small and large scales $r_0 \in (0, 1.0)$ performs best. This indicates that adding variation on the scale ratio of cropping \exemplar images can help our \methodname to learn better representations.

\textbf{(\romannumeral2) \texttt{Shape}}: In Table~\ref{tab:crop_shape} we further study the crop shape of \exemplars. Given that deep architectures (CNNs and ViTs) are more easily to process rectangle inputs, we extend the square in previous studies to the rectangle with height/width ratio $r_1$. Other non-rectangle shapes (\eg, triangles and circles) are beyond our study. We find that expanding the sampling range of shape ratio $r_1$ as $[1/3, 3/1]$ can boost the performance upon the square entry. However, if a larger range of $[1/4, 4/1]$ is applied, no further performance gain can be obtained. All these experiments suggest that the shape of \exemplars is also a useful factor in our cropping strategy.

\textbf{(\romannumeral3) \texttt{Rotation}}: Table~\ref{tab:rotation} shows the influence of the rotation degree $\alpha$ between
the cropping \exemplars and \context image. We conduct rotation experiments upon the previous best entry in Table~\ref{tab:crop_shape}. 
The optimal sampling range of $\alpha$ is $[\ang{-45}, \ang{45}]$, which means adding a relatively smaller degree of rotation is helpful for our \methodname, while large rotation degree such as $[\ang{-180}, \ang{180}]$ would bring a negative effect. Our consideration of rotation is strategically different from previous predicting image rotations~\cite{gidaris2018unsupervised} in that we treat rotation as a type of augmentation rather than a supervisory signal.
Therefore, a reasonably small degree of rotation can improve our \methodname pre-training.

\textbf{(\romannumeral4) \texttt{Transformation}}: Table~\ref{tab:transformation} studies the influence of data transformations on our \methodname pre-training. We consider random data transformations including horizontal flipping, Gaussian blurring, color jittering, grayscale, and solarization. We can observe that only adding transformations on \exemplars while keeping \context images unaltered works best for our \methodname. This can be explained by our bootstrap encoder design: the online network encode \exemplars while the offline network processes the \context, as a result, \exemplars are more responsible for affecting model training. Note that our transformation for \exemplar-\context pairs is different from existing MV-SSL methods such as MoCo v3~\cite{chen2021empirical} and DINO~\cite{caron2021emerging}, in which two transformed views are conceptually identical, thus can be swapped during training.


\begin{table}[!t]
\centering
\small
\caption{\textbf{Ablations of encoder designs} for \methodname with ViT-B/16 on ImageNet-200. 
}
\label{tab:bootstrap_enc}
% \addtolength{\tabcolsep}{-2pt}
% \vspace{-0.2cm}

\centering
\small

\begin{tabular}{ccc}
\toprule
Bootstrap    & Update & Top-1 acc (\%) \\ 
\midrule
scratch  & -            & 77.79               \\ 
\midrule
\xmark  & shared            & 89.91          \\ 
\cmark  & $\xi\rightarrow\theta$  & 89.05 

\\ 
\cmark  & $\theta\rightarrow\xi$ & \cellcolor{gray!20}\textbf{90.12}  \\ 
\bottomrule
\end{tabular}

\end{table}


\begin{figure}[!h]
    \centering
    \includegraphics[page=1,width=1.0\linewidth]{figures/decoder_study.pdf}
    % \vskip -0.2cm
    \caption{
    \textbf{Ablations of decoder designs} for \methodname with ViT-B/16 on ImageNet-200. 
    }
    \label{fig:dec_design}
    % \vspace{-10pt}
\end{figure}

\noindent\textbf{Encoder design.} Our \methodname encoder follows a bootstrap design with \exemplar and \context images encoded by the online network $f_\theta$ and target network $f_\xi$, respectively.
As studied in Table~\ref{tab:bootstrap_enc}, we first notice that learning with a shared encoder can achieve significantly better performance than the training-from-scratch baseline. We further evaluate two bootstrapping designs: 1) $\xi\rightarrow\theta$ training update from \context to \exemplar, and 2) $\theta\rightarrow\xi$ training update from \exemplar to \context. Obviously, we can find that $\theta\rightarrow\xi$ entry performs better than both the shared and $\xi\rightarrow\theta$ entries. This validates the rationality of our bootstrap encoder design, which is also consistent with our finding of transformation in Table~\ref{tab:transformation}: our \methodname benefits more from learning with \exemplars than \context images.


\noindent\textbf{Decoder design.} Our \methodname decoder plays a key role in correlational modeling. We study our decoder designs as follows:

\textbf{(\romannumeral1) \texttt{Correlation operation}}:
Figure~\ref{fig:dec_design} (a) compares two correlation operations commonly used in existing deep tracking models. As we introduced in Section~\ref{subsec:preliminary}, when applying convolution as the correlation operation, \exemplars features are served as the kernels and convolve with \context features, in which local correlations are computed in each kernel window. Differently, as formulated in Equation~\ref{eq:ca}, a cross-attention layer models global correlation between \exemplar and \context images. The cross-attention entry can yield up to 1\% improvement over the convolution entry. This suggests that modeling global correlation is better for \methodname.

\textbf{(\romannumeral2) \texttt{Predictor}}: In Figure~\ref{fig:dec_design} (b), we study the network design of the final predictor. A simple linear layer followed by an \texttt{Unsample} operation works well for our \methodname. While a deep predictor with three deconvolution layers cannot 
% \cavan{cannot, shouldn't separate `can' and `not'} 
bring further gain but degrade \methodname training. This suggests a lightweight predictor may force \methodname to better representations in the encoder, while a heavy predictor is more specialized for predicting accurate correlation maps in the decoder but less relevant for representation learning.

\textbf{(\romannumeral3) \texttt{Depth}}: Figure~\ref{fig:dec_design} (c) varies the decoder depth (number of cross-attention layers). Interestingly, a \textit{single} cross-attention layer works best for our \methodname training. Adding more layers brings no training gain for correlation modeling, similar to our observation in previous predictor designs.  

\textbf{(\romannumeral4) \texttt{Width}}: Figure~\ref{fig:dec_design} (d) studies the decoder width (number of heads in each cross-attention layer). We set 512-d by default, which performs well for our \methodname. Increasing or decreasing the layer width does not cause significant accuracy improvement or degradation. The decoder depth is less influential
for improving representation learning for our \methodname.

Overall, our \methodname decoder is lightweight. It has only one cross-attention layer with a width of 512-d and a linear layer for final prediction.  As such, our \methodname is efficient in model pre-training.

\noindent\textbf{Loss function.} We study the influence of different loss functions for our \methodname optimization in Table~\ref{tab:loss_fuc}. Given that our \methodname predicts binary correlation maps, we compare typical loss functions for dense predictions, including cross-entropy (CE), balanced cross-entropy (BCE)~\cite{xie2015holistically}, mean squared error (MSE), and Focal loss~\cite{lin2017focal}. A standard cross-entropy performs best for our correlation modeling. This property is dramatically different from deep visual tracking models~\cite{bertinetto2016fully,valmadre2017end,wang2021multiple} and related unsupervised trackers 
% \cavan{trackers?}
~\cite{wang2019unsupervised,wu2021progressive,shen2022unsupervised,lai2020mast,zheng2021learning}, which clearly benefit from proper dense objectives. This can be explained by the difference in task goals between \methodname and visual tracking: our \methodname focuses on learning transferable representations by correlation modeling, whereas deep visual trackers demand task-specific representations in favor of better dense predictions. 

\begin{table}[!t]
% \vspace{-2pt}
\centering
\small
\caption{\textbf{Ablations of loss fuctions} for \methodname with ViT-B/16 on ImageNet-200. 
% All models are pre-trained with 300 epochs, and we report Top-1 accuracy of 100-epoch fine-tuning.
}
\label{tab:loss_fuc}
\addtolength{\tabcolsep}{-2pt}
\begin{tabular}{l|cccccc|c}
\toprule
Loss Function & CE &  BCE &  MSE & Focal  \\ \midrule

Top-1 acc (\%) & \cellcolor{gray!20}\textbf{90.12}  & 89.28 & 87.68      & 77.35    \\
\bottomrule
\end{tabular}
% \vspace{-15pt}
\end{table}

\begin{table}[!t]
% \vspace{-2pt}
\centering
\small
\caption{\textbf{Comparisons with visual tracking works} with ViT-B/16 on ImageNet-200. 
}
\label{tab:vis_tracking}
\addtolength{\tabcolsep}{-2pt}
\begin{tabular}{l|ccccccc|c}
\toprule
Method  & Scratch &  SiamFC &  SiamRPN & TransTrack & \methodname  \\ \midrule

Top-1 (\%) &77.79  & 89.09  & 89.02  & 89.54 & \cellcolor{gray!20}\textbf{90.12} \\

\bottomrule
\end{tabular}
% \vspace{-15pt}
\end{table}

\begin{table*}[!t]
% \vspace{-2pt}
\centering
% \small
\caption{\textbf{ImageNet-1K top-1 fine-tuning accuracy} of self-supervised models using ViT-S/16 and ViT-B/16 as the encoder. 
All entries are on an image size of $224\times224$. 
We use the actual processed images/views to measure the effective pre-training epochs~\cite{zhou2022image}. 
Scratch indicates the supervised baseline in~\cite{touvron2021training}. 
$^\dag$ denotes results are reproduced using the official code.}
\label{tab:vit_results}
\addtolength{\tabcolsep}{-2pt}
\vspace{-2pt}
\begin{tabular}{lccccccc} 
\toprule
Method & Pre-train Data  & Pretext Task  & Tokenizer  & Epochs & ViT-S & ViT-B \\ \midrule
Scratch~\cite{touvron2021training} & - & -               & -            & -                  & 79.9      & 81.8      \\ \midrule
MP3~\cite{zhai2022position}    & IN-1K &  Jigsaw              & -                      & 100       & -      & 81.9      \\ \midrule
MoCo v3~\cite{chen2021empirical} & IN-1K & MV-SSL               & -                       & 1200       & 81.4      & 83.2      \\
DINO~\cite{caron2021emerging}    & IN-1K & MV-SSL              & -                      & 1600       & 81.5      & 82.8      \\ \midrule
BEiT~\cite{bao2021beit}    & IN-1K+DALL-E  & MIM             & dVAE                      & 300       & 81.3      & 82.9      \\
SimMIM~\cite{xie2021simmim}$^\dag$  & IN-1K   & MIM            & -                     & 300       & 80.9      & 82.9     \\
MAE~\cite{he2021masked}$^\dag$     & IN-1K    & MIM           & -                     & 300       & 80.6      & 82.9     \\ 
\midrule %$^\dag$ 
\methodname     & IN-1K        & CIM       & -                       & 300       & 81.6      &  83.1     \\ \bottomrule
\end{tabular}
% \vspace{-15pt}
\end{table*}

\subsection{Comparisons with Visual Tracking Models}
Our \methodname is inspired by the correlation modeling in supervised visual tracking models. Our proposed cropping strategy can generate useful \exemplar-\context pairs that are also suitable for training supervised visual tracking models. We train three representative trackers using ViT-B/16 as the backbone: SiamFC~\cite{bertinetto2016fully}, SiamRPN~\cite{li2018high}, and TransTrack~\cite{chen2021transformer}, with generated \exemplar-\context pairs on ImageNet-200. Following the same pre-training and fine-tuning setting in previous ablation studies, we evaluate the quality of learned representations, as summarized in Table~\ref{tab:vis_tracking}. We can observe that: (1) Owing to the \exemplar-\context pairs generated by our cropping strategy, all three trackers can learn good representations that perform better than the scratch baseline. (2) Based on SiamFC, SiamRPN introduces an additional detection head for bounding box prediction, which brings no performance gain. (3) TransTrack works better than both SiamFC and SiamRPN. This is due in large part to the beneficial global correlation modeling provided by cross-attention layers, in comparison with local correlations computed by convolution operations in SiamFC and SiamRPN. (4) Our \methodname clearly surpasses these visual tracking works, showing the advantages of our encoder and decoder designs for effective correlational modeling.

\subsection{Comparisons with Previous SSL Methods}
\label{exps:sota}

Our \methodname is a general framework that can learn meaningful representations for both ViT and CNN architectures, unlike state-of-the-art methods such as MAE~\cite{he2021masked}. 

\noindent\textbf{ViT.} In Table~\ref{tab:vit_results} we first compare the fine-tuning results of ViT-S/16 and ViT-B/16 models with self-supervised pre-training on ImageNet-1k. Following previous works~\cite{bao2021beit,he2021masked}, we fine-tune ViT-S/16 for 200 epochs, and ViT-B/16 for 100 epochs.
More detailed pre-training
and fine-tuning configurations are described in the supplementary material. 
Compared with previous MV-SSL works~\cite{chen2021empirical,caron2021emerging}, such as MoCo v3~\cite{chen2021empirical}, our \methodname can achieve highly comparable performances (83.1 vs. 83.2), while enjoying significantly fewer epochs of pre-training (300 vs. 1200). 
Compared with previous MIM works~\cite{bao2021beit,he2021masked,xie2021simmim}, using the same 300 epochs of pre-training, our \methodname can achieve better performances with both ViT-S/16 and ViT-B/16 models.

\begin{table}[ht]
\centering
% \small
\caption{\textbf{ImageNet-1K top-1 fine-tuning accuracy} of self-supervised models using ResNet-50 as the encoder. $^\dag$ denotes results are reproduced using the official code.
}
\label{tab:resnet50}
\addtolength{\tabcolsep}{-2pt}
\begin{subtable}[t]{0.45\textwidth}
% \vspace{+0.15cm}
\centering
% \small
% \vspace{-0.1cm}
\label{tab:resnet50_2}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
Method & Pretext Task   & Epochs & Top-1 acc (\%) \\ \midrule
\multicolumn{4}{l}{\emph{Fine-tuning for 100 epochs}}            \\
RSB A3~\cite{wightman2021resnet}  & -  & -       & 78.1           \\
SimMIM~\cite{xie2021simmim}$^\dag$   & MIM    & 300       & 77.7           \\
% BEiT~\cite{bao2021beit}$^\dag$    & MIM   & 300       & 78.5           \\\midrule
\methodname    & CIM   & 300       & 78.6           \\ \midrule
\multicolumn{4}{l}{\emph{Fine-tuning for 300 epochs}}     \\
RSB A2~\cite{wightman2021resnet}  &-  & -       & 79.8           \\ 
SimSiam~\cite{chen2021exploring}   & MV-SSL     & 400       & 79.1           \\
MoCo v2~\cite{chen2020improved}   & MV-SSL & 400       & 79.6           \\
SimCLR~\cite{chen2020simple}  & MV-SSL & 800       & 79.9           \\
% SimCLR~\citep{chen2020simple}   & 2000       & 80.0           \\
BYOL~\cite{grill2020bootstrap}  & MV-SSL & 400       & 80.0           \\
SwAV~\cite{caron2020unsupervised}  & MV-SSL & 600       & 80.1           \\ 
SimMIM~\cite{xie2021simmim}$^\dag$ & MIM  & 300       & 79.5           \\%\midrule
% BEiT~\cite{bao2021beit}$^\dag$ & MIM  & 300       & 79.9           \\\midrule
\methodname     & CIM  & 300       & 80.1           \\ \bottomrule
\end{tabular}
}
\end{subtable}
% \vspace{-10pt}
\end{table}

\noindent\textbf{ResNet-50.} We further demonstrate that our \methodname can effectively pre-train the classic ResNet architecture. During pre-training, we simply apply the same ViT pre-training configurations for ResNet-50. To evaluate the pre-trained representations, we generally follow the state-of-the-art vanilla ResNet “training-from-scratch” recipe in RSB~\cite{wightman2021resnet}. We present detailed fine-tuning settings in the supplementary material. The evaluation results compared to the state-of-the-art methods are summarized in Table~\ref{tab:resnet50}. Due to the architectural difference between ViT and CNN models, we observe performance degeneration of some MIM and MV-SSL pre-training methods, such as SimMIM~\cite{xie2021simmim}, MoCo v2~\cite{chen2020improved}, and SimSiam~\cite{chen2021exploring}. Compared with the best MV-SSL method, SwAV~\cite{caron2020unsupervised}, our \methodname is faster (300 vs. 600). 

Overall, our \methodname is a simple yet effective approach that can perform on par or better than existing MV-SSL and MIM methods with both ViT and ResNet models.


\begin{figure*}[!h]
    \centering
    \includegraphics[page=1,width=1.0\linewidth]{figures/cim_vis.pdf}
    \vskip -0.3cm
    \caption{
    \textbf{Visualization} of \exemplar-\context images in company with both \textcolor{aqua}{ground-truth} and \textcolor{guppiegreen}{predicted correlation} maps for \methodname.
    % ViT-B/16 on ImageNet-1K validation set.  
    }
    \label{fig:cim_vis}
    % \vspace{-10pt}
\end{figure*}

\subsection{Transfer Learning on Semantic Segmentation}
To evaluate the transferability of the pre-trained representations by our \methodname, we further conduct end-to-end fine-tuning on the ADE20K~\cite{zhou2019semantic} semantic segmentation benchmark.
Following the same setup in BEiT~\cite{bao2021beit}, we fine-tune the pre-trained ViT-B/16 model as the backbone in UperNet~\cite{xiao2018unified} for 160K iterations, with an input resolution of $512\times512$. As summarized in Table~\ref{tab:seg}, our \methodname can achieve highly competitive performance compared with other representative self-supervised learners. This demonstrates the effectiveness of our proposed \textit{crop-and-correlate} pretext task in learning transferable representations.


\begin{table}[!t]
\centering
% \small
\caption{\textbf{ADE20K semantic segmentation} of ViT-B/16 models. 
}
\label{tab:seg}
\addtolength{\tabcolsep}{-3pt}
% \vspace{-8pt}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
Method     & Pre-train Data & Pretext Task & mIoU (\%) \\ \midrule
Supervised~\cite{touvron2021training} & IN-1K w/ labels & -& 45.3 \\ \midrule
MoCo v3~\cite{chen2021empirical}    & IN-1K & MV-SSL & 47.2 \\
DINO~\cite{caron2021emerging}       & IN-1K & MV-SSL & 46.8 \\ \midrule
BEiT~\cite{bao2021beit}       & IN-1K+DALL-E & MIM & 47.7 \\
MAE~\cite{he2021masked}        & IN-1K & MIM & 48.1 \\ \midrule
\methodname        & IN-1K & CIM & 48.1 \\ \bottomrule
\end{tabular}
}
% \vspace{-5pt}
\end{table}

\subsection{Robustness Evaluation}

We further evaluate the robustness of our models on six benchmarks that cover the challenges of adversarial attacks,
common corruption, and out-of-distribution. For adversarial attack, we evaluate the adversarial examples on ImageNet-A~\cite{hendrycks2021natural}, along with generated examples by FGSM~\cite{goodfellow2014explaining} and PGD~\cite{madry2017towards} attackers on ImageNet-1K validation set. For data corruption, we test corrupted images on ImageNet-C~\cite{hendrycks2021many}. In terms of out-of-distribution input, we consider images with 
distribution shifts from ImageNet-R~\cite{hendrycks2021many} and ImageNet-Sketch~\cite{wang2019learning}.  Specifically, we directly evaluate the models fine-tuned on original ImageNet-1K (ViT-B/16 in Table~\ref{tab:vit_results} and ResNet-50 in Table~\ref{tab:resnet50}) without further fine-tuning on each robustness validation set. The results are summarized in Table~\ref{tab:robustness}. For both ViT and ResNet architectures, our \methodname consistently outperforms the state-of-the-art self-supervised learners for model robustness.

\begin{table}[h]
% \vspace{-5pt}
\centering
\small
\caption{\textbf{Robustness evaluation on six robustness benchmarks.} We report top-1 accuracy except for IN-C which uses the mean corruption error (mCE). The original ImageNet top-1 fine-tuning results are also appended for reference. The best results are in \textbf{bold}, and the second best results are \underline{underlined}.}
\label{tab:robustness}
\addtolength{\tabcolsep}{-2pt}
\vspace{-8pt}
% \begin{subtable}[h]{0.49\textwidth}
% \centering
% \small
% \caption{}
% \label{tab:robustness}
\resizebox{.48\textwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{6}{c}{Robustness Benchmarks} & \multirow{2}{*}{Orig.} \\ \cmidrule(lr){2-7}
                        & FGSM  & PGD  & IN-C ($\downarrow$) & IN-A & IN-R & IN-SK &                        \\ \midrule
% \multicolumn{8}{l}{\emph{Methods using ViT-B/16}}                                                                         \\
\multicolumn{8}{l}{\emph{ViT-B/16 model results}} \\  
Scratch~\cite{wightman2021resnet}                 & 46.3      & 21.2     & \textbf{48.5}     & 28.1     & 44.7     & 32.0      & 81.8                       \\ 
MAE~\cite{he2021masked}                     & 38.9      & 11.2     & 52.3     & 31.5    & 48.3     & 33.8      & \underline{82.9}                       \\

\methodname                     & \textbf{47.4}      & \textbf{22.7}     & \underline{49.3}     & \textbf{30.3}     & \textbf{48.6}     & \textbf{35.3}      & \textbf{83.1}                       \\ \midrule 
\multicolumn{8}{l}{\emph{ResNet-50 model results}} \\  
Scratch~\cite{wightman2021resnet}                 & \textbf{20.2}      & \textbf{3.4}     & 77.0     & 6.6     & 36.0     & 25.0      & \underline{78.1}                       \\ 

SimMIM~\cite{xie2021simmim}                 & 16.8      & 2.1     & 77.0     & 5.7     & 34.9     & 24.2      & 77.7                      \\
\methodname                     & \underline{19.4}      & \underline{2.5}     & \textbf{73.5}     & \textbf{8.5}     & \textbf{37.4}     & \textbf{27.2}      & \textbf{78.6}                       \\ \bottomrule
\end{tabular}%
}
% \end{subtable}

% \vspace{-6pt}
\end{table}

\subsection{Visualization}
In Figure~\ref{fig:cim_vis}, we visualize the \exemplar-\context images generated by our proposed cropping strategy on the ImageNet-1K validation set. The predicted correlation maps are obtained via the ViT-B/16 model pre-trained on the ImageNet-1K train set using our \methodname in Section~\ref{exps:sota}. No further pre-training or fine-tuning is conducted.
We can observe that these predicted correlation maps match closely with the corresponding ground-truth correlations, under various scales, shapes, rotations, and transformations.
The results demonstrate the effectiveness of self-supervised correlation modeling in our \methodname for unseen data.  More visualized examples are provided in the supplementary material.