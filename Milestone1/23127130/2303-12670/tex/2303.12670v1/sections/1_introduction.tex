% !TEX root = ../main.tex
%--------------------------------------------------------
\section{Introduction}
\label{sec:intro}
%--------------------------------------------------------
Recent advances in self-supervised visual pre-training have shown great capability in harvesting meaningful representations from hundreds of millions of---often easily accessible---\textit{unlabeled} images. Among existing pre-training paradigms, Multi-View Self-Supervised Learning (MV-SSL)~\cite{he2020momentum,chen2020simple,chen2020improved,grill2020bootstrap,chen2021exploring,chen2021empirical,caron2021emerging} and Masked Image Modeling (MIM)~\cite{bao2021beit,he2021masked,wei2021masked,xie2021simmim} are two leading methods in the self-supervised learning racetrack,
thanks to their nontrivial and meaningful \textit{self-supervisory pretext tasks}.

MV-SSL follows an \textbf{\textit{augment-and-compare}} paradigm (Figure~\ref{fig:teaser}(a)) -- randomly transforming an input image into two augmented views and then comparing two different views in the representation space. Such an instance-wise discriminative task is rooted in \textit{view-invariant} learning~\cite{tian2020makes}, \ie, changing views of data does not affect the conveyed information.
On the contrary, following the success of Masked Language Modeling (MLM)~\cite{devlin2019bert}, MIM conducts a \textbf{\textit{mask-and-predict}} pretext task within a single view (Figure~\ref{fig:teaser}(b)) -- removing a proportion of random image patches and then learning to predict the missing information. This simple patch-wise
 generative recipe enables Transformer-based deep architectures~\cite{dosovitskiy2020image} to learn generalizable representations from unlabeled images.


Beyond \textbf{\textit{augment-and-compare}} or \textbf{\textit{mask-and-predict}} pretext tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for self-supervised visual representation learning. We take inspiration from visual tracking~\cite{yilmaz2006object} in computer vision that defines the task of estimating the motion or trajectory of a target object (\textit{exemplar}) in a sequence of scene images (\textit{contexts}). To cope with challenging factors such as scale variations, deformations, and occlusions, one typical tracking pipeline is formulated as maximizing the correlation between the specific \textit{exemplar} and holistic \textit{contexts}~\cite{bolme2010visual,bertinetto2016fully,valmadre2017end,wang2021multiple}.
Such simple correlational modeling can learn meaningful representations in the capability of both localization and discrimination, thus making it appealing to serve as a promising pretext task for self-supervised learning.

Training a standard correlational tracking model, however, requires access to numerous labeled data, which is unavailable in unsupervised learning. Also, the task goal of visual tracking is intrinsically learning toward one-shot object detection---demanding rich prior knowledge of objectness---while less generic for representation learning. Therefore, it is nontrivial to retrofit supervised correlational modeling for visual tracking into a useful self-supervised pretext task.

Driven by this revelation, we present a novel \textbf{\textit{crop-and-correlate}} paradigm for self-supervised visual representation learning, dubbed as \textbf{C}orrelational \textbf{I}mage \textbf{M}odeling (\methodname).  To enable correlational modeling for effectively self-supervised visual pre-training, we introduce three key designs.
First, as shown in Figure~\ref{fig:teaser}(c), we randomly crop image regions (treated as \textit{exemplars}) with various scales, shapes, rotations, and transformations from an input image (\textit{context}). The corresponding correlation maps can be derived from the exact crop regions directly. 
This simple cropping recipe allows us to easily construct the \textit{exemplar-context} pairs together with ground-truth correlation maps without human labeling cost. Second, we employ a bootstrap learning framework that is comprised of two networks: an online encoder and a target encoder, which, respectively, encode \exemplars and \context into  latent space. This bootstrapping effect works in a way that the model learns to predict the spatial correlation between the updated representation of \exemplars and the slow-moving averaged representation of \context.
Third, to realize correlational learning, we introduce a correlation decoder built with a cross-attention layer and a linear predictor, which computes queries from \context, with keys and values from \exemplars, to predict the corresponding correlation maps.

Our contributions are summarized as follows: \textbf{1)} We present a simple yet effective pretext task for self-supervised
visual pre-training, characterized by a novel unsupervised correlational image modeling framework (\methodname). 
\textbf{2)} We demonstrate the advantages of our \methodname in learning transferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art MIM and MV-SSL learners while improving model robustness and training efficiency.
We hope our work can motivate future research in exploring new useful pretext tasks for self-supervised
visual pre-training.
