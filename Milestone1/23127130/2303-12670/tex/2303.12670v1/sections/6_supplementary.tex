% !TEX root = ../main_supp.tex
%--------------------------------------------------------
\appendix
\section{Appendix}
In the supplementary material, we provide the detailed pre-training and fine-tuning recipes in Section~\ref{sec:supp_impl}. Section~\ref{sec:supp_visual} provides more qualitative visualization for \exemplar-\context images and predicted correlation maps.

\subsection{Implementation Details}
\label{sec:supp_impl}

\noindent\textbf{Pre-training.}
%
Table~\ref{tab:supp_pretrain_setting} summarizes the pre-training settings for vanilla ViT and ResNet-50 models. All experiments are conducted on 8 A100 GPUs for both ViT and ResNet-50 models. 
Our \methodname is \emph{general} across architectures that the configurations are \emph{shared} by different architectures, without specialized tuning. 

\noindent\textbf{Fine-tuning.}
%
Table~\ref{tab:supp_finetune_vit_setting} and Table~\ref{tab:supp_finetune_r50_setting} summarize the fine-tuning settings for vanilla ViT and ResNet-50 models, respectively. The configurations for ViT are \emph{shared} across models. The configurations for ResNet-50 basically follow~\cite{wightman2021resnet}, using the AdamW optimizer following~\cite{fang2022corrupted}.

\noindent\textbf{Semantic segmentation on ADE20K.}
%
Following the configurations in BEiT~\cite{bao2021beit}, we fine-tune UperNet~\cite{xiao2018unified} using AdamW as the optimizer for 160K iterations with a batch size of 16. The input resolution is $512\times512$, and we use single-scale inference.
%
Following the common practice of BERT~\cite{devlin2019bert} fine-tuning in NLP~\cite{pruksachatkun2020intermediate}, we initialize all segmentation models using model weights after supervised fine-tuning on ImageNet-1K as suggested in BEiT~\cite{bao2021beit}.

\begin{table*}[h]
% \addtolength{\tabcolsep}{10pt}
% \setlength\tabcolsep{15pt}
\centering
\small
\caption{\textbf{Pre-training settings for vanilla ViT-S/16, ViT-B/16 and ResNet-50 models on ImageNet-200 and ImageNet-1K.} Note that we adopt the \emph{same} pre-training configurations across different architectures without further parameter tuning.}
\label{tab:supp_pretrain_setting}
\begin{tabular}{ll}
\toprule
Configuration \hspace{20pt}                  & Value \\ \midrule
Optimizer \hspace{20pt}                      & AdamW~\cite{loshchilov2017decoupled}      \\
Pre-training epochs \hspace{20pt}            & 300      \\
Peak learning rate \hspace{20pt}             & 2.4e-3      \\
Batch size \hspace{20pt}                     & 4096      \\
Weight decay \hspace{20pt}                   & 0.05      \\
Optimizer momentum \hspace{20pt}             & $\beta_1,\beta_2=0.9,0.95$~\cite{chen2020generative}      \\
Learning rate schedule \hspace{20pt}         & Cosine decay      \\
Warmup epochs \hspace{20pt}                  & 40      \\
Gradient clipping \hspace{20pt}              & 1.0      \\
Dropout~\cite{srivastava2014dropout} \hspace{20pt}                        & \ding{55}      \\
Stochastic depth~\cite{huang2016deep} \hspace{20pt}               & \ding{55}      \\
LayerScale~\cite{touvron2021going} \hspace{20pt}                     & \ding{55}      \\
Data augmentation \hspace{20pt}              & RandomResizedCrop      \\
Pos. emb. in Transformer layers \hspace{20pt} & 1-D absolute pos. emb.~\cite{dosovitskiy2020image}    \\
Patch size \hspace{20pt}                     & 16      \\
Pre-training resolution of \context  image       & 160      \\ 
Pre-training resolution of \exemplar  image        & 64      \\
Number of \exemplars \hspace{20pt}       & 6      \\ 
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[h]
% \addtolength{\tabcolsep}{10pt}
% \setlength\tabcolsep{15pt}
\centering
\small
\caption{\textbf{Fine-tuning settings for vanilla ViT-S/16 and ViT-B/16 on ImageNet-200 and ImageNet-1K.} We fine-tune ViT-S/16 for 200 epochs, and ViT-B/16 for 100 epochs. All other hyper-parameters are the same.}
\label{tab:supp_finetune_vit_setting}
\begin{tabular}{ll}
\toprule
Configuration \hspace{20pt}                 & Value \\ \midrule
Optimizer \hspace{20pt}                     & AdamW~\cite{loshchilov2017decoupled}      \\
Fine-tuning epochs \hspace{20pt}            & 200 (S), 100 (B)      \\
Peak learning rate \hspace{20pt}            & 9.6e-3      \\
Layer-wise learning rate decay~\cite{bao2021beit} \hspace{20pt} & 0.8~\cite{clark2020electra}      \\
Batch size \hspace{20pt}                    & 2048      \\
Weight decay \hspace{20pt}                  & 0.05      \\
Optimizer momentum \hspace{20pt}            & $\beta_1,\beta_2=0.9,0.999$      \\
Learning rate schedule \hspace{20pt}        & Cosine decay      \\
Warmup epochs \hspace{20pt}                & 5      \\
Loss function \hspace{20pt}                 & Cross-entropy loss \\
Gradient clipping \hspace{20pt}             & \ding{55}      \\
Dropout~\cite{srivastava2014dropout} \hspace{20pt}                        & \ding{55}      \\
Stochastic depth~\cite{huang2016deep} \hspace{20pt}              & 0.1      \\
Mixup~\cite{zhang2017mixup} \hspace{20pt}                         & 0.8      \\
Cutmix~\cite{yun2019cutmix} \hspace{20pt}                        & 1.0      \\
Label smoothing~\cite{szegedy2016rethinking} \hspace{20pt}               & 0.1      \\
Random augmentation~\cite{cubuk2020randaugment} \hspace{20pt}           & 9 / 0.5      \\
Patch size \hspace{20pt}                    & 16      \\
Fine-tuning resolution \hspace{20pt}        & 224      \\
Test resolution \hspace{20pt}               & 224      \\ \bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
% \addtolength{\tabcolsep}{10pt}
% \setlength\tabcolsep{15pt}
\centering
\small
\caption{\textbf{Fine-tuning settings for vanilla ResNet-50 on ImageNet-1K.} The hyper-parameters generally follow~\cite{wightman2021resnet}, except that we adopt the AdamW optimizer following~\cite{fang2022corrupted}.}
\label{tab:supp_finetune_r50_setting}
\begin{tabular}{lcc}
\toprule
Configuration \hspace{20pt}                        & 100 epoch FT \hspace{20pt}          & 300 epoch FT         \\ \midrule
Optimizer \hspace{20pt}                     & \multicolumn{2}{c}{AdamW~\cite{loshchilov2017decoupled}}                     \\
Peak learning rate \hspace{20pt}            & \multicolumn{2}{c}{12e-3}                     \\
Layer-wise learning rate decay~\cite{bao2021beit} \hspace{20pt} & \multicolumn{2}{c}{\ding{55}}                          \\
Batch size \hspace{20pt}                    & \multicolumn{2}{c}{2048}                      \\
Weight decay \hspace{20pt}                  & \multicolumn{2}{c}{0.02}                      \\
Learning rate schedule \hspace{20pt}        & \multicolumn{2}{c}{Cosine decay}              \\
Warmup epochs \hspace{20pt}                 & \multicolumn{2}{c}{5}                         \\
Loss function \hspace{20pt}                 & \multicolumn{2}{c}{Binary cross-entropy loss} \\
Gradient clipping \hspace{20pt}             & \multicolumn{2}{c}{\ding{55}}      \\
Dropout~\cite{srivastava2014dropout} \hspace{20pt}                       & \multicolumn{2}{c}{\ding{55}}                          \\
Stochastic depth~\cite{huang2016deep} \hspace{20pt}              & \multicolumn{2}{c}{\ding{55}}                          \\
Mixup~\cite{zhang2017mixup} \hspace{20pt}                         & \multicolumn{2}{c}{0.1}                       \\
Cutmix~\cite{yun2019cutmix} \hspace{20pt}                        & \multicolumn{2}{c}{1.0}                       \\
Label smoothing~\cite{szegedy2016rethinking} \hspace{20pt}               & 0.1 \hspace{20pt}                   & \ding{55}                     \\
Repeated augmentation~\cite{berman2019multigrain,hoffer2019augment} \hspace{20pt}         & \ding{55} \hspace{20pt}                      & \ding{51}                     \\
Random augmentation~\cite{cubuk2020randaugment} \hspace{20pt}           & 6 / 0.5 \hspace{20pt}               & 7 / 0.5              \\
Fine-tuning resolution \hspace{20pt}        & 160 \hspace{20pt}                   & 224                  \\
Test resolution \hspace{20pt}               & \multicolumn{2}{c}{224}                       \\
Test crop ratio \hspace{20pt}              & \multicolumn{2}{c}{0.95}                      \\ \bottomrule
\end{tabular}
\end{table*}

\begin{figure*}[!h]
    \centering
    \includegraphics[page=1,width=1.0\linewidth]{figures/cim_visualization_more.pdf}
    % \vskip -0.2cm
    \caption{
    \textbf{Visualization} of \exemplar-\context images in company with both \textcolor{aqua}{ground-truth} and \textcolor{guppiegreen}{predicted correlation} maps for \methodname.
    }
    \label{fig:cim_vis_more}
    % \vspace{-10pt}
\end{figure*}

\subsection{More Visualization}
\label{sec:supp_visual}

We provide more qualitative visualization of \exemplar-\context images together with both ground-truth and predicted correlation maps for \methodname in Figure~\ref{fig:cim_vis_more}, using unseen ImageNet-1K \emph{validation} images.