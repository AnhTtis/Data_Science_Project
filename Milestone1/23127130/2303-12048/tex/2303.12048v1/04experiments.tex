
\input{figures/results}
\input{figures/real_scenes}

\section{Experiments}
\label{sec:results}
We show qualitative editing results over diverse 3D objects and various edits in Figures \ref{fig:teaser}, \ref{fig:results}, \ref{fig:realscenes}, \ref{fig:comparison2d}, \ref{fig:comparisons}, \ref{fig:t23dablation}. 
Please refer to the supplementary material for many fly-through visualizations demonstrating that our results are indeed consistent across different views.
In Figure \ref{fig:realscenes}, we demonstrate that our method also succeeds in modeling and editing real scenes using the  $360^\circ$ \emph{Real Scenes} available by Mildenhall et al.~\cite{mildenhall2021nerf}. As illustrated in the figure, our approach can locally edit the foreground (e.g., turning the flowers into sunflowers) or the background (e.g. turning the ground into a pond).

To assess the quality of our object editing approach, we conduct several sets of experiments, evaluating both the quality of the images rendered from our volumetric representation as well as the extent of which these rendered images conform to the target text prompt.  We compare to prior 3D object editing methods in Section \ref{sec:comp3D}. In Section \ref{sec:comp2D}, we compare with state-of-the-art image editing techniques. We show ablations in Section \ref{sec:ablations}. Finally, we discuss limitations in Section \ref{sec:limitations}.

\medskip \noindent \textbf{Synthetic Object Dataset.} 
We assembled a dataset using five freely available meshes found on the internet. %
Each mesh was rendered from 100 views in Blender. 
For a quantitative evaluation, we paired each object in our dataset with a number of both local and global edit prompts including:
\begin{itemize}
    \item ``A $\left<object\right>$ wearing sunglasses''.
    \item ``A $\left<object\right>$ wearing a wizard's hat''. 
    \item ``A $\left<object\right>$ wearing a Christmas sweater''.
    \item ``A  yarn doll of a $\left<object\right>$''.
    \item ``A wood carving of a $\left<object\right>$''.
\end{itemize}
We separately evaluate local and global edits. For instance, the first three prompts above are considered local edits and the last two as edits that should produce global edits. We provide additional details in the supplementary material.


\subsection{Metrics}
\medskip \noindent \textbf{Edit Fidelity.} We evaluate how well the generated results capture the target text prompt using two metrics:  
 \smallskip \newline \emph{CLIP Similarity} ($\text{CLIP}_{Sim}$) measures the semantic similarity between the output objects and the target text prompts. We encode both the prompt and images rendered from our 3D outputs using CLIP's text and image encoders, respectively, and measure the cosine-distance between these encodings. 
 \smallskip \newline \emph{CLIP Direction Similarity} ($\text{CLIP}_{Dir}$) evaluates the quality of the edit in regards to the input by measuring the directional CLIP similarity first introduced by Gal et al.~\cite{gal2021stylegannada}. This metric measures the cosine distance between the direction of the change from the input and output rendered images and the direction of the change from an input prompt (\emph{i.e.} ``a dog") to the one describing the edit (\emph{i.e.} ``a dog wearing a hat").


\medskip \noindent \textbf{Edit Magnitude.}  For ablating components in our model, we use the Frech\'et Inception Distance (FID) \cite{Heusel2017GANsTB,Seitzer2020FID} to measure the difference in visual appearance between: (i) the output and input images ($\text{FID}_{Input}$) and (ii) the output and images generated by the initial reconstruction grid ($\text{FID}_{Rec}$). We show both to demonstrate to what extent the appearance is affected by the edit versus the expressive power of our framework.





\input{figures/comparisons3d}
 \input{tables/comparisons}
 \input{figures/text_to_3d_ablation}
\subsection{3D Object Editing Comparisons}
\label{sec:comp3D}

To the best of our knowledge, there are no existing methods that can directly perform our task of text-guided localized edits for 3D objects given a set of posed input images. 
Thus, we consider Text2Mesh~\cite{michel2022text2mesh} and Latent-Nerf~\cite{metzer2022latent} which can be applied in a similar setting to ours. These experiments highlight the differences between prior works and our proposed object editing technique.

Text2Mesh~\cite{michel2022text2mesh} aims at editing the style of a given input mesh to conform to a target prompt with a style transfer network that predicts color and a displacement along the normal direction. As it only predicts displacements along the normal direction, the geometric edits enabled by Text2Mesh are limited mostly to small changes.
Latent-Paint and SketchShape are two applications introduced in Latent-Nerf~\cite{metzer2022latent} which operate on input meshes. 
SketchShape generates shape and appearance from coarse input geometry, while Latent-Paint only edits the appearance of an existing mesh.
Note that these methods are designed for slightly more constrained inputs than our approach. While our focus is on editing 3D models with arbitrary textures (as depicted from associated imagery), they only operate on uncolored meshes. 

We show a qualitative comparison in Figure \ref{fig:comparisons} over an uncolored mesh (its geometry can be observed on the top row as Latent-Paint keeps the input geometry fixed). As illustrated in the figure, Text2Mesh cannot produce significant geometric edits (\emph{e.g.}, adding a Santa hat to the horse or turning the horse into a donkey). Even SketchShape, which is designed to allow geometric edits, cannot achieve significant localized edits. Furthermore, it fails to preserve the geometry of the input---although, we again note that this method is not intended to preserve the input geometry. 
Our method, on the other hand, succeeds in conforming to the target text prompt, while preserving the input geometry, allowing for semantically meaningful changes to both geometry and appearance.

We perform a quantitative evaluation in Table \ref{tab:baseline-stats} on our dataset. To perform a fair comparison where all methods operate within their training domain, we use meshes without texture maps as input for all baseline methods. As illustrated in the table, our method outperforms all baselines over both local and global edits in terms of CLIP similarity, but Text2Mesh yields slightly higher CLIP direction similarity. We note that Text2Mesh is advantaged in terms of the CLIP metrics as it explicitly optimizes on CLIP similarities and thus its scores are not entirely indicative. 

In Figure \ref{fig:t23dablation} we compare to the unconditional text-to-3D model proposed in Latent-NeRF, to show that such unconditional models are also not guaranteed to generate a consistent object over different prompts. We also note that this result (as well as our edits) would certainly look better if fueled with a proprietary big diffusion model \cite{saharia2022photorealistic}, but nonetheless, these models cannot preserve identity.

\input{figures/comparisons2d}

\subsection{2D Image Editing Comparisons}

An underlying assumption in our work is that editing 3D geometry cannot easily be done by reconstructing edited 2D images depicting the scene. To test this hypothesis, we modified images rendered from various viewpoints using the diffusion-based image editing methods InstructPix2Pix~\cite{brooks2022instructpix2pix} and SDEdit~\cite{meng2022sdedit}. %
As illustrated in Figure~\ref{fig:comparison2d}, 2D methods often struggle to produce meaningful results from less \emph{canonical} views (\emph{e.g.}, adding sunglasses on the dog's back) and also produce highly view-inconsistent results. %

\label{sec:comp2D}


\subsection{Ablations}
\label{sec:ablations}
\input{figures/ablations}
\input{tables/ablations}
We provide an ablation study in Table~\ref{tab:ablations} and Figure~\ref{fig:ablations}. Specifically, we ablate our volumetric regularization ($\mathcal{L}_{reg3D}$) and our 3D cross-attention-based spatial refinement module (SR). When ablating our volumetric regularization, we use a single volumetric grid and regularize the SDS objective with an image-based L2 regularization loss. More details and additional ablations are provided in the supplementary material.

The benefit of using our volumetric regularization is further illustrated in Figure~\ref{fig:ablations}, which shows that image-space regularization leads to very noisy results, and often complete failures (see, for instance, the cat result, where the output is not at all correlated with the input object). Quantitatively, we can also observe that images rendered from these models are of significantly different appearance (as measured using the FID metrics). 

Regarding the SR module, as expected, it increases  similarity to the inputs (reflected in lower FID scores). This is also clearly visible in Figure \ref{fig:ablations}---for example, geometric differences are apparent by looking at the animals' paws. The output textures after refinement also are more similar to the input textures. However, we also see that this module slightly hinders CLIP similarity to the edit and text prompt. This is also somewhat expected as we are further constraining the output to stay similar to the input, sometimes at the expense of the editing signal. 


\subsection{Limitations}
\label{sec:limitations}
\input{figures/limitations}

Our method applies a wide range of edits with high fidelity to 3D objects, however, there are several limitations to consider. As shown in Figure \ref{fig:limitations}, since we optimize over different views, our method attempts to edit the same object in differing spatial locations, thus failing on certain prompts. Moreover, the figure shows that some of our edits fail due to incorrect attribute binding, where the model binds attributes to the wrong subjects, which is a common challenge in large-scale diffusion-based models \cite{chefer2023attend}. 
Finally, we inherit the limitations of our volumetric representation. Thus, the quality of real scenes, for instance, could be significantly improved by borrowing ideas from works such as \cite{barron2022mip} (e.g. scene contraction to model the background).


\ignorethis{
\subsection{Object Editing Alternatives}
As no prior work directly address our task of performing such text-guided edits of 3D objects, we compare against several related works, adapting them to our problem setting and performing compherensive comparisons over various metrics.

\medskip \noindent \textbf{SOTA Text-guided Image Editing.} 

We begin by comparing output frames from our pipeline with SOTA Text-Guided Image editing methods that were given our input frames and text prompts as input. \esc{no need to run relu field on these}
\begin{itemize}
    \item Instructpix2pix + ReLU fields
    \item SDEdit + ReLU fields ?
    \item TEXT2LIVE + ReLU fields ?
    \item Best one + NeRF?
\end{itemize}

\medskip \noindent \textbf{Text-to-3D.} 
Bottom line message: look if we remove the original model guidance we get this noisy thing, of course this result and our edits would look better if fueled with a proprietary big diffusion model, but the point still stands that identity is not preserved.
\begin{itemize}
    \item DreamFusion
    \item LatentNeRF
\end{itemize}

\medskip \noindent \textbf{Neural Field Editing.} 
\begin{itemize}
    \item ?
\end{itemize}

\medskip \noindent \textbf{3D Object Editing} 
Bottom line message: look if we remove the original model guidance we get this noisy thing, of course this result and our edits would look better if fueled with a proprietary big diffusion model, but the point still stands that identity is not preserved.
\begin{itemize}
    \item DreamFusion
    \item LatentNeRF
\end{itemize}




\subsection{Ablations / Baselines} User study in addition to metrics?
\begin{itemize}
    \item no annealing
    \item single ReLU
    \item no attention grid
\end{itemize}
}



