\section{Introduction}

Creating and editing 3D models is a cumbersome task. While template models are readily available from online databases, tailoring one to a specific artistic vision often requires extensive knowledge of specialized 3D editing software.
In recent years, neural field-based representations (e.g., NeRF~\cite{mildenhall2021nerf}) demonstrated expressive power in faithfully capturing fine details, while offering effective optimization schemes through differentiable rendering. Their applicability has recently expanded also for a variety of editing tasks. However, research in this area has mostly focused on either appearance-only manipulations, which change the object's texture~\cite{xiang2021neutex,yang2022neumesh} and style~\cite{zhang2022arf,wang2022nerf}, or geometric editing via correspondences with an explicit mesh representation~\cite{garbin2022voltemorph,yuan2022nerf,xu2022deforming}---linking these representations to the rich literature on mesh deformations~\cite{igarashi2005rigid,sorkine2007rigid}. Unfortunately, these methods still require placing user-defined control points on the explicit mesh representation, and cannot allow for adding new structures or significantly adjusting the geometry of the object.

In this work, we are interested in enabling more flexible and localized object edits, guided only by textual prompts, which can be expressed through \emph{both} appearance and geometry modifications. To do so, we leverage the incredible competence of pretrained 2D diffusion models in editing images to conform with target textual descriptions. We carefully apply a \emph{score distillation} loss, as recently proposed in the unconditional text-driven 3D generation setting~\cite{poole2022dreamfusion}. Our key idea is to regularize the optimization in 3D space. We achieve this by coupling two volumetric fields, providing the system with more freedom to comply with the text guidance, on the one hand, while preserving the input geometry and appearance, on the other hand.

Rather than using neural fields, we base our method on \emph{lighter} ReLU Fields~\cite{karnewar2022relu} which do not require any neural networks and instead model the scene as a voxel grid where each voxel contains learned features. This explicit grid structure not only allows for faster reconstruction and rendering times, but also for achieving a tight \emph{volumetric} coupling between volumetric fields representing the 3D object before and after applying the desired edit using a novel \emph{volumetric correlation loss} over the density features. %
To further refine the spatial extent of the edits, we utilize 2D cross-attention maps which roughly capture regions associated with the target edit, and lift them to volumetric grids. This approach is built on the premise that, while independent 2D internal features of generative models can be noisy, unifying them into a single 3D representation allows for better distilling the semantic knowledge.  %
We then use these 3D cross-attention grids as a signal for a binary volumetric segmentation algorithm that splits the reconstructed volume into edited and non-edited regions, allowing for merging the features of the volumetric grids to better preserve regions that should not be affected by the textual edit.


Our approach, coined \emph{Vox-E}, provides an intuitive voxel editing interface, where the user only provides a simple target text prompt (see Figure~\ref{fig:teaser}). %
We compare our method to existing 3D object editing techniques and 2D image editing diffusion-based techniques, and demonstrate that our approach can facilitate local and global edits involving appearance and geometry changes over a variety of objects and text prompts, which are extremely challenging for current methods.

Explicitly stated, our contributions are:
\begin{itemize}
    \item A coupled volumetric representation tied using 3D regularization, allowing for editing 3D objects using diffusion models as guidance while preserving the appearance and geometry of the input object.
    \item A 3D cross-attention based volumetric segmentation technique that defines the spatial extent of textual edits.
    \item Results that demonstrate that our proposed framework can perform a wide array of editing tasks, which cannot be previously achieved.
\end{itemize}





