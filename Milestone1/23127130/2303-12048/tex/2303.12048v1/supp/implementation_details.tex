\section{Additional Details}
\label{sec:details}

\subsection{Implementation Details}
\label{sec:imp}
Below we provide all the implementation details of our method, detailed in Section 3 in the main paper.

\subsubsection*{Grid-Based Volumetric Representation}
We use 100 images uniformly sampled from upper hemisphere poses along with corresponding camera intrinsic and extrinsic parameters to train our initial grid. We follow the standard ReLU Fields~\cite{karnewar2022relu} training process using their default settings aside from two modifications: 
\begin{enumerate}
    \item We change the result grid size from the standard $128^3$ to $160^3$ to increase the output render quality.
    \item As detailed in the main paper, we limit the order of spherical harmonics to be zero order only to avoid undesirable view-dependent effects (we further illustrate these effects in Section \ref{sec:sh}). 
\end{enumerate}

 

\subsubsection*{Text-guided Object Editing}
We perform 8000 training iterations during the object editing optimization stage. During each iteration, a random pose is uniformly sampled from an upper hemisphere and an image is rendered from our edited grid $G_e$ according to the sampled pose and the rendering process described in ReLU Fields \cite{karnewar2022relu}. Noise is then added to the rendered image according to the time-step sampled from the fitting distribution. 

We use an annealed SDS loss which gradually decreases the maximal time-step we draw $t$ from. Formally, this annealed SDS loss introduces three additional hyper-parameters to our system: a starting iteration $i_{start}$, an annealing frequency $f_a$ and an annealing factor $\gamma_a$. With these hyper-parameters set, we change our time-step distribution to be:
\begin{equation}
    t \sim U[t_0 + \varepsilon, t_{final}*k_i + \varepsilon],
\end{equation}
\begin{equation}
    k_i = 
    \begin{cases}
    1, & \text{if } i < i_{start} \\
    k_{i-1}*\gamma_a, & \text{else if } i\ \% \ f_a = 0 \\
    k_{i-1}, & \text{otherwise}
    \end{cases}
\end{equation}
In all our experiments, the values we use for $\varepsilon$, $i_{start}$, $f_a$ and $\gamma_a$ are 0.02, 4000, 600, and 0.75. Additionally, we stop annealing the time-step when it reaches a value of 0.35. %
The latent diffusion model we use in our experiments is "StableDiffusion 2.1" by Stability AI\href{https://huggingface.co/stabilityai/stable-diffusion-2-1}. %

We use a weight of $200$ to balance the two terms (multiplying $\mathcal{L}_\text{reg3D}$ by this weight value). The volumetric regularization term operates only on the density features of the editing grid. The optimizer we used in this (and all other stages) is the Adam optimizer~\cite{adamoptimizer} with a learning rate of 0.03 and betas 0.9, 0.999. The resolution of the images rendered from our grid is 266$\times$266. We add a "a render of" prefix to all of our editing prompts as we found that this produced more coherent results (and the images the LDM receives are indeed renders).



\subsubsection*{Spatial Refinement via 3D Cross-Attention}
The diffusion model we use for this stage is \href{ https://huggingface.co/CompVis/stable-diffusion-v1-4 }{"StableDiffusion 1.4" by CompVis} and it consists of several %
cross-attention layers at resolutions 32, 16, and 8. To extract a single attention map for each token we interpolate each cross attention map from each layer and attention head to our image resolution (266x266) and take an average per each token. %
The time-step we use to generate the attention maps is 0.2 (the actual step being 0.2 * $N_{steps}$ = 200). 

The cross-attention grids $A_{e}$ and $A_{obj}$ contain a density feature and an additional one-dimensional feature $a$, which represents the cross-attention value at a given voxel and can be interpreted and rendered as a grayscale luma value. We initialize the density features in these grids to the density features of the editing grid's (the former stage's output) and freeze them.
At each refinement iteration we generate two 2D cross-attention maps from the LDM, one for the object and one for the edit. After obtaining the 2D cross-attention maps, we render gray-scale heatmaps %
from $A_{e}$ and $A_{obj}$ and use $L1$ loss to encourage similarity between the rendered attention images and their corresponding attention maps extracted from the diffusion model. We repeat this process for 1500 iterations, sampling a random upper-hemisphere pose each time. As in the former optimization stage, we use the Adam optimizer with a learning rate of 0.03 and betas 0.9 and 0.999 and generate images in 266$\times$266 resolution.

After obtaining the two grids $A_{e}$ and $A_{obj}$, we perform element-wise softmax on their $a$ values to obtain probabilities for each voxel belonging to either the object, denoted by $P_{obj}(v)$, or the edit, denoted by $P_e(v)$. 
We then proceed to calculate the binary refinement volumetric mask. To do this we define a graph in which each non-zero density voxel in our edited grid $G_e$ is a node. We define "edit" and "object" labels as the \emph{source} and \emph{drain} nodes, such that a node connected to the source node is marked as an "edit" node and a node connected to the drain node is marked as an "object" node. We rank the nodes according to their $P_e(v)$ values and connect the top $N_{init-edit}$ nodes to the source node. We then rank the nodes according to their $P_{obj}(v)$ value and connect the top $N_{init-object}$ nodes to the drain node.
We then connect the non-terminal nodes to each-other in a 6-neighborhood with the capacity of each edge being $w_{pq}$ as detailed in the main paper.


We set the hyper-parameters $N_{init-edit}$ and $N_{init-object}$ to be 300 and 200. %
To perform graph-cut~\cite{boykov2001fast}, we used the \href{ https://github.com/pmneila/PyMaxflow }{PyMaxflow} implementation of the max-flow / min-cut algorithm. %

\subsection{Evaluation Protocol}
To evaluate our results quantitatively, we constructed a test set composed of three scenes: Dog, Cat and Kangaroo, and six editing prompts: (1) A $\left<object\right>$ wearing big sunglasses, (2) A $\left<object\right>$ wearing a Christmas sweater, (3) A $\left<object\right>$ wearing a birthday party hat, (4) A yarn doll of a $\left<object\right>$, (5) A wood carving of a $\left<object\right>$, (6) A claymation $\left<object\right>$. This yields 18 edited scenes in total. We render each edited scene from 100 different poses distributed evenly along a $360^{\circ}$ ring. In addition to these 18 scenes we also render 100 images from the same poses on the initial (reconstruction) grid $G_i$ for each input scene. When comparing our result with other 3D textual editing papers we evaluate our results using two CLIP-based metrics. %
The CLIP model we used for both of these metrics is \href{https://github.com/openai/CLIP}{ViT-B/32} and the input image text prompts used to calculate the directional CLIP metric is ``A render of a $\left<object\right>$". $CLIP_{Dir}$ is calculated for each edited image in relation to the corresponding image in the reconstruction scene.
To quantitatively evaluate ablations we use two additional metrics using FID \cite{Seitzer2020FID}. For this we use the \href{ https://github.com/mseitzer/pytorch-fid }{pytorch implementation} given by the authors with the standard settings. 

\subsubsection*{$360^\circ$ \emph{Real Scenes}} For the $360^\circ$ \emph{Real Scenes} edits we follow the same implementation details as outlined previously, with two modifications: 
\begin{enumerate}
    \item Our input poses are  created in a spherical manner and when rendering we sample linearly in inverse depth rather than in depth as seen in the official implementation of NeRF \href{https://github.com/bmild/nerf}.
    \item We perform 5000 training iterations during the object editing optimization stage and the values we use for $\varepsilon$, $i_{start}$, $f_a$ and $\gamma_a$ are 0.02, 3000, 400, and 0.75.
\end{enumerate}

\subsection{3D Object Editing Techniques} 
Below we provide additional details on the alternative 3D object editing techniques we compare against. All of the techniques we compare against use only an un-textured mesh and an editing prompt as input. As such, we used the meshes our inputs were rendered from as input for the editing methods. Additionally, we tested an additional scenario in which we imported the 'horse' mesh from the \href{https://github.com/threedle/text2mesh/blob/main/data/source_meshes/horse.obj}{Text2Mesh GitHub repository} to blender, added a grey-matte material to it and rendered images of it to use as input for our system. This scenario used four prompts - (1) A wood carving of a horse, (2) A horse wearing a Santa hat, (3) A donkey, (4) A carousel horse, and was used for qualitative comparisons only.

\subsubsection*{Text2Mesh} 
When comparing to Text2Mesh we used the \href{https://github.com/threedle/text2mesh}{code provided by the authors} and the input settings given in the "run\_horse.sh" demo file.



\subsubsection*{SketchShape}
 In this comparison we again use the \href{https://github.com/eladrich/latent-nerf}{code provided by the authors}. And the input parameters used are the default parameters in the 'train\_latent\_nerf.py' script  \href{https://github.com/eladrich/latent-nerf/tree/main/scripts}{'train\_latent\_nerf.py' script} with 10,000 training steps (as opposed to the default 5,000).

\subsubsection*{Latent-Paint}
We compared our method to Latent-Paint only qualitatively as this method outputs edits that transform only the appearance of the input mesh, rather than appearance and geometry. As in SketchShape we used the code provided by the authors and used the default input settings provided for latent paint, which are given in the  \href{https://github.com/eladrich/latent-nerf/tree/main/scripts}{'train\_latent\_paint.py' script}.

\subsection{2D Image Editing Techniques}

When comparing to InstructPix2Pix and SDEdit we constructed two image sets for each scene / prompt combination we wanted to test. Both sets were created by rendering one of our inputs in evenly spaced poses along a $360^{\circ}$ ring, one set was rendered over a white background and the other over a 'realistic' image of a brick wall. We used these sets as input for each 2D editing method along with an editing prompt and compared the results to rendered outputs from our result grids. When comparing to InstructPix2Pix we used the standard \href{https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/pix2pix}{InstructPix2Pix pipeline} with 16bit floating point precision, a guidance scale of 1 and 20 inference steps. When giving prompts to InstructPix2Pix we rephrased our prompts as instructions, for example turning "a dog wearing sunglasses" to "put sunglasses on this dog". When comparing to SDEdit we used the \href{https://huggingface.co/docs/diffusers/using-diffusers/img2img}{standard SDEdit pipeline} with guidance scale of 0.75 and a strength of 0.6.



\ignorethis{
\begin{enumerate}
    \item Real-scenes?
    \item cross-attention maps - \textbf{maybe a correction figure?} also, what timesteps are we using, any other important details?
    \item explain ablations and comparisons (all details needed to reproduce experiments)
    \item all hyperparameters
    \item Dataset details - the meshes, all the prompts
\end{enumerate}
}


