\begin{figure} %
\centering %
\jsubfig{\includegraphics[height=1.860cm,trim={6cm 4cm 6cm 8cm},clip] {images/t23dablation/input.png}}{\footnotesize {Input}}\hfill
\jsubfig{\includegraphics[height=1.860cm]{images/t23dablation/latent_nerf_skates_.png}\includegraphics[height=1.860cm,trim={6cm 4cm 6cm 8cm},clip]{images/t23dablation/ours_skates.png}}{\footnotesize {``Kangaroo on rollerskates"}} \hfill
\jsubfig{\includegraphics[height=1.860cm]{images/t23dablation/latent_nerf_skis_.png}\includegraphics[height=1.860cm,trim={0cm 0cm 0cm 2cm},clip]{images/t23dablation/ours_skis.png}}{\footnotesize {``Kangaroo on skis"}}
\vspace{-1pt} 
\caption{\textbf{Comparison to unconditional text-to-3D generation}. We compare to unconditional text-to-3D methods by comparing to Latent-NeRF ~\cite{metzer2022latent}, providing it with the two target prompts displayed above. We display these alongside our results (LatentNeRF on the left, ours on the right). As illustrated above, unconditional methods cannot easily match an input object, and are also not guaranteed to generate a consistent object over different prompts.  }
\label{fig:t23dablation}
\end{figure}
