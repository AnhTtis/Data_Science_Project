\section{Related Work}

\noindent \textbf{Text-driven Object Editing.} 
Computational methods targeting text-driven image generation and manipulation have seen tremendous progress with the emergence of CLIP~\cite{radford2021learning} and diffusion models~\cite{ho2020denoising}, advancing from specific domains~\cite{styleflow, shen2020interpreting,patashnik2021styleclip,gal2021stylegannada} to more generic ones~\cite{nichol2021glide,avrahami2022blended,couairon2022diffedit,kawar2022imagic}. Several recent methods allow for performing convincing localized edits on real images without requiring mask guidance~\cite{bar2022text2live,hertz2022prompt,brooks2022instructpix2pix,pnpDiffusion2022, Parmar2023ZeroshotIT}. 
However, these methods all operate on single images and cannot facilitate a consistent editing of 3D objects.


While less common, methods for manipulating 3D objects are also gaining increasing interests. Although several works aim at learning the relations between 3D shape parts and text directly~\cite{huang2022ladis,achlioptas2022changeit3d}, most manipulation techniques are often guided by their 2D image projections which are matched to the driving text.  Text2Mesh~\cite{michel2022text2mesh} leverage CLIP for stylizing 3D meshes based on textual prompts. Tango~\cite{chen2022tango} also styles meshes with CLIP, enabling additionally stylization of lighting conditions, reflectance properties and local geometric variations.
TEXTure~\cite{richardson2023texture} use a depth-to-image diffusion model for texturing 3D meshes. Unlike our work, these methods focus mostly on texturing meshes, and cannot be used for generating significant geometric modifications, such as adding glasses or other types of accessories.

\input{figures/overview.tex}

\medskip
\noindent \textbf{Neural Field Editing.} 
Neural fields (e.g., NeRF~\cite{mildenhall2021nerf}), which can be effectively learned from multi-view images through differentiable rendering, have recently shown great promise for representing object and scenes. Prior works have demonstrated that these fields can be adapted to express different forms of manipulations. ARF~\cite{zhang2022arf} transfers the style of an exemplar image to a NeRF. NeRF-Art~\cite{wang2022nerf} performs a text-driven style transfer. Several works have shown that neural fields can be edited by editing selected 2D images~\cite{liu2021editing,yang2022neumesh}. NeuTex~\cite{xiang2021neutex} uses 2D texture maps, which can be edited directly, to represent the surface appearance. 
Other works demonstrated geometric editing of shapes represented with neural fields via correspondences with an explicit mesh representation~\cite{garbin2022voltemorph,yuan2022nerf,xu2022deforming}, that can be edited using as-rigid-as-possible deformations~\cite{sorkine2007rigid}. However, these cannot easily allow for modifying the 3D mesh to incorporate additional parts, according to the user's provided description.

Rather than using neural fields, we base our method on ReLU Fields~\cite{karnewar2022relu} which do not require any neural networks and instead model the scene as a voxel grid where each voxel contains learned features. In their work, they demonstrated that using ReLU activations after interpolating the grid values is competitive with the state-of-the-art, while enabling fast reconstruction and rendering times. In addition to these advantages, we show that having an explicit grid structure also provides additional advantages for editing 3D objects.






\medskip
\noindent \textbf{Text-to-3D.} Following the great success of text-to-image generation, we are witnessing increasing interests in unconditional text-driven generation of 3D objects and scenes. CLIP-Forge~\cite{sanghi2022clip} uses CLIP guidance to generate coarse object shapes from text. Dream Fields~\cite{jain2022zero}, DreamFusion~\cite{poole2022dreamfusion}, Score Jacobian Chaining~\cite{haochen2022score} and Latent-NeRF~\cite{metzer2022latent} optimize radiance fields to generate the geometry and color of objects driven by the text. While DreamFields relies on CLIP, the other three methods instead use a score distillation loss, which enables the use of a pretrained 2D diffusion model. Magic3D~\cite{lin2022magic3d} proposes a two-stage optimization technique to overcome DreamFusion's slow optimization. Unlike these works, we focus on the conditional setting. In our case, a 3D object is provided, and the desired edit should preserve the object's geometry and appearance. Still, we compare with Latent-NeRF in the experiments, as it can use rough 3D shapes as guidance.


