
\input{figures/attention}

\section{Method}
In this work, we consider the problem of editing 3D objects given a captured set of posed multiview images describing this object and a text prompt expressing the desired edit. We first represent the input object with a grid-based volumetric representation (Section \ref{sec:rep}). We then optimize a \emph{coupled} voxel grid, such that it resembles the input grid on the one hand while conforming to the target text on the other hand (Section \ref{sec:editing}). To further refine the spatial extent of the edits, we perform an (optional) refinement step (Section \ref{sec:refine}).
Figure \ref{fig:overview} provides an overview of our approach.


\subsection{Grid-Based Volumetric Representation}
\label{sec:rep}
Our volumetric representation is based on the voxel grid model first introduced in DVGO~\cite{sun2022direct} and later simplified in ReLU Fields~\cite{karnewar2022relu}. We use a 3D grid $G$, where each voxel holds an 4D feature vector. We model the object's geometry using a single feature channel which represents spatial density values when passed through a ReLU non-linearity. The three additional feature channels represent the object's appearance, and are mapped to RGB colors when passed through a sigmoid function. Note that in contrast to most recent neural 3D scene representations (including ReLU Fields), we do not model view dependent appearance effects, as we found it leads to undesirable artifacts when guided with 2D diffusion-based models. 

To represent the input object with our grid-based representation, we use images and associated camera poses to perform volumetric rendering as described in NeRF \cite{mildenhall2021nerf}. However, in contrast to NeRF, we do not use any positional encoding and instead sample our grid at each location query to obtain interpolated density and color values, which are then accumulated along each ray. We use a simple L1 loss between our rendered outputs and the input images to learn a grid-based volume $G_i$ that represents the input object.



\subsection{Text-guided Object Editing}
\label{sec:editing}
Equipped with the initial voxel grid $G_i$ described in the previous section, we perform text-guided object editing by optimizing $G_e$, a grid representing the edited object which is initialized from $G_i$. Our optimization scheme combines a generative component, guided by the target text prompt, and a pullback term that encourages the new grid not to deviate too strongly from its initial values. As we later show, our coupled volumetric representation provides added flexibility to our system, allowing for better balancing between the two objectives by regularizing directly in 3D space. Next we describe these two optimization objectives.


\subsubsection*{Generative Text-guided Objective}
To encourage our feature grid to respect the desired edit provided via a textual prompt, we use a Score Distillation Sampling (SDS) loss applied over Latent Diffusion Models (LDMs). SDS was first introduced in DreamFusion \cite{poole2022dreamfusion}, and consists of minimizing the difference between noise injected to a generator's output and noise predicted by a pre-trained Denoising Diffusion Probabilistic Model (DDPM). Formally, at each optimization iteration, noise is added to a generated image $x$ using a random time-step $t$,
\begin{equation}
    x_t = x + \epsilon_t,
\end{equation}
where $\epsilon_t$ is the output of a noising function $Q(t)$ at time-step $t$. The score distillation gradients (computed per pixel) can be expressed as:
\begin{equation}
  \nabla_x\mathcal{L}_{SDS} = w(t)\left( \epsilon_t-\epsilon_\phi(x_t, t, s) \right),
\end{equation}
where $w(t)$ is a weighting function, $s$ is an input guidance text, and $\epsilon_\phi(z_t, t, s)$ is the noise predicted by a pre-trained DDPM with weights $\phi$  given $x_t$, $t$ and $s$.
As suggested by Lin et al.~\cite{lin2022magic3d}, we use an annealed SDS loss which gradually decreases the maximal time-step we draw $t$ from, allowing SDS to focus on high frequency information after the outline of the edit has formed. We empirically found that this often leads to higher quality outputs. 

\ignorethis{
In practice, frameworks that utilize SDS \hec{(such as XX,YY,ZZ)} often select the time-step $t$ randomly at each iteration by drawing from a uniform distribution:
\begin{equation}
    t \sim U[t_0 + \varepsilon, t_{final} + \varepsilon],
\end{equation}
with $\varepsilon$ being some small number, and $t_0$ and $t_{final}$ being the first and final time-steps, respectively. This implies that the noisy image we feed the DDPM with is uniformly distributed between nearly pure noise (at the largest time-step) and and the original un-noised output (at the smallest time-step). 

We observe that as noise levels increase, higher frequency information in the image becomes less and less visible, which in turn means that SDS will struggle in facilitating meaningful changes in the higher frequency information of the generated outputs at higher time-steps. Additionally, at lower noise levels SDS will struggle with driving meaningful low frequency changes, as that is ill-posed in regards to what the model was trained to do at these time-steps. Using this observation, we show that gradually decreasing the maximal time-step we draw $t$ from allows SDS to focus on high frequency information after the outline of the edit has formed, and in turn leads to higher quality outputs. 
}


\subsubsection*{Volumetric Regularization}
\label{sec:volume_reg}
Regularization is key in our problem setting, as we want to avoid over-fitting to specific views and also not to deviate too far from the original 3D representation. Therefore, we propose a volumetric regularization term, which couples our edited grid $G_e$ with the initial grid $G_i$. Specifically, we incorporate a loss term which encourages correlation between the density features of the input grid $f^\sigma_i$ and the density features of the edited grid $f^\sigma_e$:

\begin{equation}
    \mathcal{L}_{reg3D} = 1 - \frac{Cov(f^\sigma_i, f^\sigma_e)}
    {\sqrt{Var(f^\sigma_i)Var(f^\sigma_e)}}
\end{equation}


This volumetric loss has a significant edge over image space losses as it allows for decoupling the appearance of the scene from its structure, thereby connecting the volumetric representations in 3D space rather than treating it as a multiview optimization problem.


\subsection{Spatial Refinement via 3D Cross-Attention}
\label{sec:refine}
While our optimization framework described in the previous section can mostly preserve the shape and the identity of a 3D object, for local edits, it is usually desirable to only change specific \emph{local} regions, while keeping other regions completely fixed.
Therefore, we add an (optional) refinement step which leverages the signal from cross-attention layers to produce a volumetric binary mask $M$ that marks the voxels which should be edited. We then obtain the refined grid $G_r$ by merging the input grid $G_i$ and edited $G_r$ grid as
\begin{equation}
   G_r =  M \cdot G_e + (1 - M) \cdot G_i.
\end{equation}

\ignorethis{
Formally, we define the volumetric binary mask $M$ as follows:
\begin{equation}
    M[i, j, k] = 
    \begin{cases}
           1 &  [i, j, k]\in E\\
           0 &\text{otherwise}, \\ 
         \end{cases}
\end{equation}
where $E$ is the latent set of voxels that should be edited.
}

In the context of 2D image editing with diffusion models, the outputs of the cross-attention layers roughly capture the spatial regions associated with each word (or token) in the text. More concretely, these cross attention maps can be interpreted as probability distributions over tokens for each image patch~\cite{hertz2022prompt,brooks2022instructpix2pix}. 
We elevate these 2D probability maps to a 3D grid by using them as supervision for training a ReLU field. We initialize the density values from the ReLU field trained in Section~\ref{sec:rep} and keep these fixed, while using probability maps in place of color images and optimizing for the probability values in the grid using an L1 loss.
As shown in Figure~\ref{fig:attention}, optimizing for a volumetric representation allows for ultimately refining the 2D probability maps. 

We then convert these 3D probability fields to our binary mask $M$ using a seam-hiding segmentation algorithm based on energy minimization~\cite{agarwala2004interactive}. We define the label probabilities for voxel cell as the element-wise softmax of two cross-attention grids $A_{e}$ and $A_{obj}$, where
\begin{itemize}
\item $A_{e}$ is the cross-attention grid associated with the token describing the edit (e.g.\ \emph{sunglasses}), and
\item $A_{obj}$ is the grid associated with the object, defined as the maximum probability over all other tokens in the prompt.
\end{itemize}
We compute the smoothness term from local color differences in the edited grid $G_e$. That is, we sum
\begin{equation}
    w_{pq}=\text{exp}\left(\frac{-(c_p - c_q)^2}{2\sigma^2}\right)
\end{equation}
for each pair of neighboring voxels $p$ and $q$, where $c_p$ and $c_q$ are RGB colors from $G_e$. In our experiments, we use $\sigma=0.1$ and balance the data and smoothness terms with a parameter $\lambda=5$ (strengthening the smoothness term). Finally, we solve this energy minimization problem via graph cuts~\cite{boykov2001fast}, resulting in the high quality segmentation masks shown in Figure~\ref{fig:attention}.

\ignorethis{
Intuitively, since we're only interested in localizing the edit regions, we can optimize a volumetric cross-attention grid $G^{edit}_{att}$ that is associated with the token describing the edit (e.g. \emph{sunglasses}). We can then define the latent set of voxels in $E$ by taking voxels with high activation in this cross-attention grid. However, we found this generated a highly discontinuous mask, and it was also challenging to automatically set a single threshold over all the objects and prompts. 

Therefore, in addition to optimizing $G^{edit}_{att}$, we optimize an object cross-attention grid $G^{obj}_{att}$. This grid is associated with the object, defined by considering the activations  from all other tokens in the prompt. That is, the 2D cross-attention maps (used for supervising this grid) are set by taking a $\textit{max}$ value over all other tokens in the prompt.
We then compute per-voxel segmentation probabilities by taking a $\textit{softmax}$ over the two cross-attention grid values. 
To encourage piecewise smooth binary segmentations, we formulate an energy minimization problem over all the voxels in our grid. The data term is defined using the segmentation probabilities (as detailed above, using the cross-attention grids). The smoothness term is defined using RGB feature values of the edited grid, denoted by $f^{\text{RGB}}_{e}$, considering a 6-neighborhood voxel connectivity. Specifically, we use the following non-increasing function of $|f^{\text{RGB}}_{e,p}-f^{\text{RGB}}_{e,q}|$~\cite{boykov2001interactive}:
\begin{equation}
    w_{pq}=e^{-\frac{\left(f^{\text{RGB}}_{e,p}-f^{\text{RGB}}_{e,q} \right)^2}{2\sigma^2}},
\end{equation}
where $p,q$ are the indices of two neighboring voxels and $\sigma$ is set to 0.1. We balance the data and smoothness terms with a parameter $\lambda=5$ (strengthening the smoothness term).  
The energy minimization can be efficiently solved via graph cuts~\cite{boykov2001fast}.  
In Figure \hec{XXX}, we demonstrate the high quality of the volumetric binary masks obtained using the method described above. 
}













\ignorethis{It consists of two voxel grids: $V^{\sigma} \in \mathbb{R}^{1 \times {N_x} \times {N_y} \times {N_z}}$ and $V^{c} \in \mathbb{R}^{1 \times {N_x} \times {N_y} \times {N_z}}$. The first grid contains density features on its vertices which represent spatial density values when passed through a ReLU non-linearity. The vertices of the second grid contain appearance features, which are mapped to RGB colors when passed through a sigmoid function. Given a normalized volume sample $x = (x, y, z)$ we use trilinear interpolation to obtain a density and appearance feature sample $f_{\sigma}, f_{c}$  from the grid. The density and appearance features are then passed through an appropriate activation function to produce a density and color sample $
\sigma, c$. Note that in contrast to most recent neural 3D scene representation we do not model view dependent appearance effects as we found it leads to undesirable effects when editing with 2D diffusion model based methods.

After obtaining a density and color grid which depicts our input scene $(V_{recon}^{\sigma}, V_{recon}^{c})$ we then proceed to synthesize an edited version of it $(V_{edit}^{\sigma}, V_{edit}^{c})$ in accordance with the input text prompt. This process is composed of two core components: a generative component which builds upon Score Distillation Sampling and a regularizing component which utilizes our grid based representation \esc{and attention maps - later}.
}



\ignorethis{
When extended to Latent Diffusion Models (LDMs), this objective can be expressed as:
\begin{equation}
\mathbb{E}_{z\sim\xi(x),s,\epsilon\sim Q(t)}[\epsilon_t-\epsilon_\phi(z_t, t, s)],
\end{equation}
with $z$ being the output of a pre-trained auto-encoder $\xi$ given a generated image $x$ as input, $s$ being an input guidance text, $\epsilon_t$ is the output of a noising process $Q(t)$ at time-step $t$, and $\epsilon_\phi(z_t, t, s)$ is the noise predicted by a pre-trained DDPM with weights $\phi$ given $t$, $s$ and $z_t$. The latter being output of the auto-encoder $\xi$ given a generated image with added noise $(x + \epsilon_t)$. \hec{I think it's still not clear, we either need to expand a little further or condense (which is fine as it's not our contribution)} 
\phc{Yeah I don't get this, feels like the paramters were optimizing for are missing from this description. Are we minimizing equation by optimizing for the values of $x$?}}