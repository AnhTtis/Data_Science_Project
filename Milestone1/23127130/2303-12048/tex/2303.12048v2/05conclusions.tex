\section{Conclusion}

In this work, we presented Vox-E, a new framework that leverages the expressive power of diffusion models for text-guided voxel editing of 3D objects.
%capable of achieving local and global edits while preserving the general appearance and structure of the 3D object, particularly for regions that should not be affected by the target text prompt. 
Technically, we demonstrated that by combining a diffusion-based image-space objective with volumetric regularization we can achieve fidelity to the target prompt and to the input 3D object. We also illustrated that 2D cross-attention maps can be elevated for performing localization in 3D space.
We showed that our approach can generate both local and global edits, which are challenging for existing techniques. 
%
Our work makes it easy for non-experts to modify 3D objects using just text prompts as input, bringing us closer to the goal of democratizing 3D content creating and editing.