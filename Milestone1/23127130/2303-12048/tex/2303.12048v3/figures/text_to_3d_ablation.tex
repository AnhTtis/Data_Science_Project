\begin{figure} %
\centering %[trim={left bottom right top},clip]
\jsubfig{\includegraphics[height=1.860cm,trim={1.2cm 0.8cm 1.2cm 1.6cm},clip] {images/t23dablation/input.png}}{\footnotesize {Input}}\hfill
\jsubfig{\includegraphics[height=1.860cm]{images/t23dablation/latent_nerf_skates_.png}\includegraphics[height=1.860cm,trim={1.2cm 0.8cm 1.2cm 1.6cm},clip]{images/t23dablation/ours_skates.png}}{\footnotesize {``Kangaroo on rollerskates"}} \hfill
\jsubfig{\includegraphics[height=1.860cm]{images/t23dablation/latent_nerf_skis_.png}\includegraphics[height=1.860cm,trim={0cm 0cm 0cm 1.75cm},clip]{images/t23dablation/ours_skis.png}}{\footnotesize {``Kangaroo on skis"}}
\vspace{-1pt} 
\caption{\textbf{Comparison to unconditional text-to-3D generation}. We compare to unconditional text-to-3D methods by comparing to Latent-NeRF ~\cite{metzer2022latent}, providing it with the two target prompts displayed above. We display these alongside our results (LatentNeRF on the left, ours on the right). As illustrated above, unconditional methods cannot easily match an input object, and are also not guaranteed to generate a consistent object over different prompts.  }
\label{fig:t23dablation}
\end{figure}
