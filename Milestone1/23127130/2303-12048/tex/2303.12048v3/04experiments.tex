
\input{figures/results}

\section{Experiments}
\label{sec:results}
We show qualitative editing results over diverse 3D objects and various edits in Figures \ref{fig:teaser}, \ref{fig:results},  \ref{fig:comparisons}, 
\ref{fig:supp_realscenes},
%
Please refer to the supplementary material for many fly-through visualizations demonstrating that our results are indeed consistent across different views.
%

%\medskip \noindent \textbf{Real Scenes} 
 %\esc{We found the performance of ReLU-Fields \cite{karnewar2022relu} to be somewhat underwhelming when reconstructing these more complex scenes and so we use DVGO \cite{sun2022direct} as our underlying 3D representation when editing these scenes specifically. This also demonstrates that our method is indeed agnostic to the underlying 3D representation of the scene.}
%
% Our initial grid successfully models these complex scenes in a "plug-and-play" manner with no additional complex modeling, such as the modeling used in ReLU Fields. Our output applies a faithful, consistent edit that even adds semantically correct fine details to the scene (e.g. the shadow of the pineapple, the reflection of the pine-cone on the water). 

To assess the quality of our object editing approach, we conduct several sets of experiments, quantifying the extent of which these rendered images conform to the target text prompt. We provide comparisons with prior 3D object editing methods in Section \ref{sec:comp3D}, and comparisons to 2D editing methods in Section \ref{sec:comp2D}. An additional comparison to an unconditional text-to-3D method is presented in Section \ref{sec:comp_uncond} \ignorethis{Comparisons with prior 3D object editing methods are provided in Section \ref{sec:comp3D}. Comparisons to 2D editing methods are provided in Section \ref{sec:comp2D}.} Results over real scenes are illustrated in Section \ref{sec:realscenes}. We show ablations in Section \ref{sec:ablations}.  Finally, we discuss limitations in Section \ref{sec:limitations}.  Additional results, visualizations, ablations and comparisons can be found in the supplementary material.
%

\medskip \noindent \textbf{Synthetic Object Dataset.} 
We assembled a dataset using freely available meshes found on the internet. %Our dataset includes five objects: \textsc{Dog}, \textsc{Cat}, \textsc{Alien}, \textsc{Kangaroo}, and \textsc{Taxi}.
%
Each mesh was rendered from 100 views in Blender. 
%
For a quantitative evaluation, we paired each object in our dataset with
a number of both local and global edit prompts including:
\begin{itemize}
    \item  ``A $\left<object\right>$ wearing sunglasses''.
    \item ``A $\left<object\right>$ wearing a party hat''. 
    \item ``A $\left<object\right>$ wearing a Christmas sweater''.
    \item ``A  yarn doll of a $\left<object\right>$''.
    \item ``A wood carving of a $\left<object\right>$''.
   % \item ``A claymation $\left<object\right>$''.
\end{itemize}

 \noindent We separately evaluate local and global edits, using our spatial refinement step over local edits only. For instance, the first three prompts above are considered local edits (where regions that are not associated with the text prompt should remain unchanged) and the last two as edits that should produce global edits. We provide additional details in the supplementary material.



\medskip \noindent \textbf{Runtime.} All experiments were performed on a single RTX A5000 GPU (24GB VRAM). The training time for our method is approx. 50 minutes for the editing stage and 15 minutes for the optional refinement stage.

%\esc{The first three prompts are local prompts in which we refine our edited grids using our spatial refinement step. This final stage is not used when editing according to the last three, global edit prompts. We observe that our method has varying capabilities when using or not using the refinement stage. Specifically, using this stage allows us to better preserve the identity of the object. However, this is often at the expense of our ability to capture the edit described in the prompt. As such we evaluate local and global edits separately in our quantitative analysis.  }
\ignorethis{
 We separately evaluate local and global edits. For instance, the first three prompts above are considered local edits and the last two as edits that should produce global edits. We provide additional details in the supplementary material.
}


%while also saving camera extrinsic and intrinsic parameters. The meshes we used mostly contain animals and objects such as cars as more niche objects or characters might be less dominant in the diffusion model's learned space \esc{not sure if that last sentence is necessary}\hec{it weakens us rather than strengthens us ;)} \hec{we can give our dataset a name (ObjEdit? or maybe something with blender?) and we should also provide to release it. We also need to talk about the prompts used for the quantitative evaluation somewhere}.

% \medskip \noindent \textbf{Metrics.} 
\subsection{Metrics}
\medskip \noindent \textbf{Edit Fidelity.} We evaluate how well the generated results capture the target text prompt using two metrics:  
 \smallskip \newline \emph{CLIP Similarity} ($\text{CLIP}_{Sim}$) measures the semantic similarity between the output objects and the target text prompts. We encode both the prompt and images rendered from our 3D outputs using CLIP's text and image encoders, respectively, and measure the cosine-distance between these encodings. 
 \smallskip \newline \emph{CLIP Direction Similarity} ($\text{CLIP}_{Dir}$) evaluates the quality of the edit in regards to the input by measuring the directional CLIP similarity first introduced by Gal et al.~\cite{gal2021stylegannada}. This metric measures the cosine distance between the direction of the change from the input and output rendered images and the direction of the change from an input prompt (\emph{i.e.} ``a dog") to the one describing the edit (\emph{i.e.} ``a dog wearing a hat").

%\medskip \noindent \textbf{Input Object Fidelity.} For ablating components in our model, we use the PSNR metric to evaluate the similarity between rendered images and the corresponding ground truth images. 

\medskip \noindent \textbf{Edit Magnitude.}  For ablating components in our model, we use the Frech\'et Inception Distance (FID) \cite{Heusel2017GANsTB,Seitzer2020FID} to measure the difference in visual appearance between: (i) the output and input images ($\text{FID}_{Input}$) and (ii) the output and images generated by the initial reconstruction grid ($\text{FID}_{Rec}$). We show both to demonstrate to what extent the appearance is affected by the edit versus the expressive power of our framework.


%We use three different metrics to quantitatively evaluate our results. To measure semantic similarity beottween our outputs and the target text prompts we encode \phc{encode with what? CLIP?} both the prompt and images rendered from our 3D outputs and measure the cosine-distance between the encodings. We also use CLIP to evaluate the quality of our edit in regards to our inputs by measuring the directional CLIP similarity first introduced by \cite{gal2021stylegannada}. This metric measures the cosine distance between the direction of the change from the input and output images and the direction of the change from an input prompt (i.e. "a render of a dog") to the one describing the edit (i.e. "a render of a dog wearing a hat"). Finally, we measure the Frech\'et Inception Distance (FID) \esc{(cite here!!!!)} between our outputs and both the input images and images generated by our reconstruction grid. This is done to gauge the drop in image quality caused by the editing process. The inherent gap between FID to the input images and FID to the reconstruction is a measure of our framework's expressive power.

%(1) Faithfulness to our object -  before/after edit (2) Consistency / how much the object changed before + after the edit (3) Faithfulness to the prompt (CLIP-Similarity) (4) FID / Quality of results

%Two metrics from instructpix2pix: (a)  \emph{cosine similarity of CLIP image embeddings (how much the edited
%image agrees with the input image)}---corresponds to (3) above, and (b) \emph{the directional CLIP
%similarity introduced by "Stylegan-nada" (how much the change in text
%captions agrees with the change in the images)}---corresponds to (3) as well.  \emph{These are competing metrics—increasing the degree to which the output images correspond to a desired edit will reduce their similarity (consistency) with the input image.}

\input{figures/comparisons3d}
\input{tables/comparisons}
\input{figures/comparisons2d}
\input{figures/supp_realscenes}
\subsection{3D Object Editing Comparisons}
\label{sec:comp3D}

%As no prior work directly address our task of performing such text-guided edits of 3D objects, we compare against several related works, adapting them to our problem setting and performing compherensive comparisons over various metrics.
To the best of our knowledge, there is no prior work that can directly perform our task of text-guided localized edits for 3D objects given a set of posed input images. 
%
Thus, we consider Distilled Feature Fields~\cite{kobayashi2022decomposing} combined with CLIP-NeRF~\cite{wang2022clip} (DFF+CN), Text2Mesh~\cite{michel2022text2mesh} and Latent-NeRF~\cite{metzer2022latent} which can be applied in a similar setting to ours. These experiments highlight the differences between prior works and our proposed editing technique.

Distilled Feature Fields~\cite{kobayashi2022decomposing} distills 2D image features into a 3D feature field to enable query-based local editing of a 3D scenes. CLIP-NeRF edits a neural radiance field by optimizing the CLIP score of the input query and the rendered image. Combining these two methods allows to edit only the relevant parts of the 3D scene.
Text2Mesh~\cite{michel2022text2mesh} aims at editing the style of a given input mesh to conform to a target prompt with a style transfer network that predicts color and a displacement along the normal direction. As it only predicts displacements along the normal direction, the geometric edits enabled by Text2Mesh are limited mostly to small changes.
Latent-Paint and SketchShape are two applications introduced in Latent-Nerf~\cite{metzer2022latent} which operate on input meshes. 
%
SketchShape generates shape and appearance from coarse input geometry, while Latent-Paint only edits the appearance of an existing mesh.
%The style network is optimized by rendering multiple 2D images and a CLIP-based loss on these images and the prompt.
%
Note that Text2Mesh and Latent-NeRF are designed for slightly more constrained inputs than our approach. While our focus is on editing 3D models with arbitrary textures (as depicted from associated imagery), they only operate on uncolored meshes. 

We show a qualitative comparison in Figure \ref{fig:comparisons} over an uncolored mesh (its geometry can be observed on the second row from the top\ignorethis{top row} as Latent-Paint keeps the input geometry fixed). As illustrated in the figure, Text2Mesh cannot produce significant geometric edits (\emph{e.g.}, adding a Santa hat to the horse or turning the horse into a donkey). Even SketchShape, which is designed to allow geometric edits, cannot achieve significant localized edits. Furthermore, it fails to preserve the geometry of the input---although, we again note that this method is not intended to preserve the input geometry. 
%All three methods struggle to generate localized textural edits, such as adding a hat to the horse. 
DFF+CN seems generally less suitable for our problem setting, particularly for prompts that require geometric modifications (i.e. ``A donkey'').
Our method, in contrast to prior works, succeeds in conforming to the target text prompt, while preserving the input geometry, allowing for semantically meaningful changes to both geometry and appearance.

We perform a quantitative evaluation in Table \ref{tab:baseline-stats} on our dataset. To perform a fair comparison where all methods operate within their training domain, we use meshes without texture maps as input for Text2Mesh and Latent-NeRF. As illustrated in the table, our method outperforms all baselines over both local and global edits in terms of CLIP similarity, but Text2Mesh yields slightly higher CLIP direction similarity. We note that Text2Mesh as well as DFF+CN are advantaged in terms of the CLIP metrics as they explicitly optimize on CLIP similarities and thus their scores are not entirely indicative. 

\subsection{2D Image Editing Comparisons}
\label{sec:comp2D}
An underlying assumption in our work is that editing 3D geometry cannot easily be done by reconstructing edited 2D images depicting the scene. To test this hypothesis, we modified images rendered from various viewpoints using the diffusion-based image editing methods InstructPix2Pix \cite{brooks2022instructpix2pix} and SDEdit \cite{meng2022sdedit}. We show two variants of these methods in Figure \ref{fig:comparison2d}, one with added backgrounds, as we observe that it also affects performance. %more reasonable results can be obtained by adding backgrounds. 
In both cases, as illustrated in the figure, 2D methods often struggle to produce meaningful results from less canonical views (e.g., adding sunglasses on the dog’s back) and also produce highly view-inconsistent results. Concurrently to us, Instruct-NeRf2NeRF~\cite{haque2023instruct} explore how to best use these 2D methods to learn view-consistent 3D representations.
%\esc{We note that occasionally better 2D editing results can be obtained when using different guidance-scales. However, as we demonstrate in the supplementary material these results are still highly inconsistant.}


\input{figures/text_to_3d_ablation}
\subsection{Comparisons to an unconditional text-to-3D model}
\label{sec:comp_uncond}
In Figure \ref{fig:t23dablation} we compare to the unconditional text-to-3D model proposed in Latent-NeRF, to show that such unconditional models are also not guaranteed to generate a consistent object over different prompts. We also note that this result (as well as our edits) would certainly look better if fueled with a proprietary big diffusion model \cite{saharia2022photorealistic}, but nonetheless, these models cannot preserve identity.

\subsection{Real Scenes}
\label{sec:realscenes}
%To test the robustness of our method we apply it to very different data---real scenes.
In Figure \ref{fig:supp_realscenes}, we demonstrate that our method also succeeds in modeling and editing real scenes using the  $360^\circ$ \emph{Real Scenes} available by Mildenhall et al.~\cite{mildenhall2021nerf}. As illustrated in the figure, we can locally edit the foreground (e.g., turning the pinecone into a pineapple) as well as globally edit the scene (e.g. turning the scene into a Van-Gogh painting). For these more complex and computationally demanding scenes, we also experiment with implementing our method on top of DVGO \cite{sun2022direct} (bottom row), in addition to ReLU-Fields which we exclusively focus on in all other experiments (top row), as it offers additional features such as scene contraction, a more expressive color feature space and complex ray sampling. These make this underlying representation better suited for editing and reconstructing these real scenes (as illustrated in the columns labeled as 'Initial'). This experiment also demonstrates that our method is agnostic to the underlying 3D representation of the scene and can readily operate over different grid-based representations.
\ignorethis{
As illustrated in the figure, our approach can locally edit the foreground (e.g., turning the flowers into sunflowers) or the background (e.g. turning the ground into a pond). \rev{For these real scenes, our method is applied on top of DVGO~\cite{sun2022direct}, which is better suited for reconstructing more complex scenes (we compare to results obtained with ReLU-Fields in the supplementary material). This experiment also demonstrates that our method is agnostic to the underlying 3D representation of the scene and can readily operate over different grid-based representations.
}
}




\ignorethis{
\input{figures/comparisons2d}
}

\ignorethis{
\subsection{2D Image Editing Comparisons}

An underlying assumption in our work is that editing 3D geometry cannot easily be done by reconstructing edited 2D images depicting the scene. To test this hypothesis, we modified images rendered from various viewpoints using the diffusion-based image editing methods InstructPix2Pix~\cite{brooks2022instructpix2pix} and SDEdit~\cite{meng2022sdedit}. %Each method was given 100 input images and a single prompt, after modifying the images according to the input text we intended to reconstruct the "edited" scene according to the edited images. 
As illustrated in Figure~\ref{fig:comparison2d}, 2D methods often struggle to produce meaningful results from less \emph{canonical} views (\emph{e.g.}, adding sunglasses on the dog's back) and also produce highly view-inconsistent results. %, making 3D reconstruction meaningless.
}

\subsection{Ablations}
\label{sec:ablations}
\input{figures/ablations}
\input{tables/ablations}
We provide an ablation study in Table~\ref{tab:ablations} and Figure~\ref{fig:ablations}. Specifically, we ablate our volumetric regularization ($\mathcal{L}_{reg3D}$) and our 3D cross-attention-based spatial refinement module (SR). When ablating our volumetric regularization, we use a single volumetric grid and regularize the SDS objective with an image-based L2 regularization loss. More details and additional ablations are provided in the supplementary material, including alternative regularization objectives (such as image-based L1 loss, or volumetric regularization over RGB features) and results using higher order spherical harmonics
coefficients.

The benefit of using our volumetric regularization is further illustrated in Figure~\ref{fig:ablations}, which shows that image-space regularization leads to very noisy results, and often complete failures (see, for instance, the cat result, where the output is not at all correlated with the input object). Quantitatively, we can also observe that images rendered from these models are of significantly different appearance (as measured using the FID metrics). 

Regarding the SR module, as expected, it increases  similarity to the inputs (reflected in lower FID scores). This is also clearly visible in Figure \ref{fig:ablations}---for example, geometric differences are apparent by looking at the animals' paws. The output textures after refinement also are more similar to the input textures. However, we also see that this module slightly hinders CLIP similarity to the edit and text prompt. This is also somewhat expected as we are further constraining the output to stay similar to the input, sometimes at the expense of the editing signal. 

%Our quantitative ablation results also point toward the benefit of using our regularization method ($\mathcal{L}_{reg3D}$) over image space losses \esc{should I explain in detail how we tested this?}\phc{Yes please! Either be short, or point to supplement}, which is even more emphasized for in Figure~\ref{fig:ablations}. As can be seen 

%\hec{Etai quantitative, and explain qualitative, only SDS loss to compare to text-to-3D}
%\input{figures/text_to_3d_ablation}



\subsection{Limitations}
\label{sec:limitations}
\input{figures/limitations}

Our method applies a wide range of edits with high fidelity to 3D objects, however, there are several limitations to consider. As shown in Figure \ref{fig:limitations}, since we optimize over different views, our method attempts to edit the same object in differing spatial locations, thus failing on certain prompts. Moreover, the figure shows that some of our edits fail due to incorrect attribute binding, where the model binds attributes to the wrong subjects, which is a common challenge in large-scale diffusion-based models \cite{chefer2023attend}. 
Finally, we inherit the limitations of our volumetric representation. Thus, the quality of real scenes, for instance, could be significantly improved by borrowing ideas from works such as \cite{barron2022mip} (e.g. scene contraction to model the background).
%Finally, our method produces results with different quality between seeds, as is similar to most works that employ diffusion models.% \hec{discuss quality of real scenes (problem in initial quality)}


\ignorethis{
\subsection{Object Editing Alternatives}
% we don't need all these extra auxillary losses that dream fusion has (a bunch of tricks), our model without guidance looks like this.... which further destroys the identity of the 3d object %score distillation alone is not enough, for the purposes of editing it's better to commit to ...
As no prior work directly address our task of performing such text-guided edits of 3D objects, we compare against several related works, adapting them to our problem setting and performing compherensive comparisons over various metrics.

\medskip \noindent \textbf{SOTA Text-guided Image Editing.} 

We begin by comparing output frames from our pipeline with SOTA Text-Guided Image editing methods that were given our input frames and text prompts as input. \esc{no need to run relu field on these}
\begin{itemize}
    \item Instructpix2pix + ReLU fields
    \item SDEdit + ReLU fields ?
    \item TEXT2LIVE + ReLU fields ?
    \item Best one + NeRF?
\end{itemize}

\medskip \noindent \textbf{Text-to-3D.} 
Bottom line message: look if we remove the original model guidance we get this noisy thing, of course this result and our edits would look better if fueled with a proprietary big diffusion model, but the point still stands that identity is not preserved.
\begin{itemize}
    \item DreamFusion
    \item LatentNeRF
\end{itemize}

\medskip \noindent \textbf{Neural Field Editing.} 
\begin{itemize}
    \item ?
\end{itemize}

\medskip \noindent \textbf{3D Object Editing} 
Bottom line message: look if we remove the original model guidance we get this noisy thing, of course this result and our edits would look better if fueled with a proprietary big diffusion model, but the point still stands that identity is not preserved.
\begin{itemize}
    \item DreamFusion
    \item LatentNeRF
\end{itemize}




\subsection{Ablations / Baselines} User study in addition to metrics?
\begin{itemize}
    \item no annealing
    \item single ReLU
    \item no attention grid
\end{itemize}
}



