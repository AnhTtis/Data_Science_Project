\section{Ablations}
\label{sec:ab}
In this section, we show a more detailed ablation study which evaluates the effect of our volumetric regularization loss (Section \ref{sec:ablation-reg}) and an additional experiment, demonstrating the effect of using high order spherical harmonics coefficients (Section \ref{sec:sh}).

\subsection{Alternative Regularization Objectives}
\label{sec:ablation-reg}
Table \ref{tab:grid_losses} shows a quantitative comparison over different image-space and volumetric regularizations. Only the image-space L1 loss also appears in the main paper. % (Table 2). However, unfortunately we only noticed after the paper deadline that we incorrectly reported that specific experiment---the actual performance of this ablation is significantly lower, as illustrated in this corrected table. 
Below we provide additional details on these ablations.

\input{figures/fcl}

\paragraph{Image-space Regularization}
%We ablate our choice to use our Volumetric Regularization loss $\mathcal{L}_\text{reg3D}$ by setting its weight to zero and replacing it with image space losses. 
In this setting we render images from our editing grid $G_e$ in the poses corresponding to the input images during each iteration of the optimization stage. Rather than using a volumetric regularization, we incur a loss between the images rendered from $G_e$ and the corresponding input image while using the same weight used to balance $\mathcal{L}_\text{reg3D}$ with the annealed SDS loss (this weight is set to $200$, as detailed in Section \ref{sec:imp}). We evaluate this ablation using $L_1$ and $L_2$ image space loss functions. %For each function, we generate all scenes in the test set and compare them against outputs generated by our unaltered method.

\input{tables/grid_losses}

\paragraph{Alternative Volumetric Regularization Functions}
In this setting we replace our correlation-based regularization with
%We ablate our choice of implementation for the Volumetric Regularization loss by replacing our density feature correlation encouraging implementation $\mathcal{L}_\text{reg3D}$ with 
other functions that encourage similarity between the density features of the grids using the same balancing weight. % (\emph{i.e.}, $200$). 
Namely we compare against $L1$ and $L2$ volumetric loss functions, both penalizing the distance between the density features of $G_i$ and those of $G_e$. We additionally compare against an alternative version of $\mathcal{L}_\text{reg3D}$ in which we penalize the miscorrelation between both density and color features, formally:

\begin{equation}
    \mathcal{L}_{reg3D++}  = \mathcal{L}_{reg3D} + (
1 - \frac{Cov(f^{rgb}_i, f^{rgb}_e)}
{\sqrt{Var(f^{rgb}_i)Var(f^{rgb}_e)}}
)
\end{equation}

We find that using this loss yields better reconstruction scores, at the expense of significantly lower CLIP-based scores (e.g., $\text{CLIP}_{Dir}$ scores drop from 0.08 to 0.02). Qualitatively, constraining RGB values as well as density features appears too limiting for our purposes. This can be seen in Figure \ref{fig:supp_fcl}, where we compare results obtained when using $\mathcal{L}_{reg3D++}$ against results obtained when using $\mathcal{L}_{reg3D}$. When observing these results, we can see that the edit integrity is reduced at the expense of the preservation of the origin object's color. This is evident in the duck, for instance, where the brown wooden color of the body is only clearly visible in the $\mathcal{L}_{reg3D}$ example. Furthermore, the colors of the sweater on the dog are significantly faded when regularized with $\mathcal{L}_{reg3D++}$ as the colors of a standard christmas sweater are typically much more vibrant than the white fur of the dog.

%We again generate all scenes in the test set and compare them against outputs generated by our unaltered method for each function.

\ignorethis{
\subsubsection*{Refinement}
We ablate the refinement stage of our pipeline by comparing our system's outputs with and without refining the outputs of the editing stage. Note that this ablation only effects local prompts.
\subsubsection*{Image-space Regularization}
We ablate our choice to use our Volumetric Regularization loss $\mathcal{L}_\text{reg3D}$ by setting its weight to zero and replacing it with image space losses. In this setting we render images from our editing grid $G_e$ in the poses corresponding to the input images at each iteration of the Text-Guided Object Editing stage. We incur a loss between the images rendered from $G_e$ and the corresponding input image while using the same weight used to balance $\mathcal{L}_\text{reg3D}$ with the annealed SDS loss - $200$. We evaluate this ablation using two different image space loss functions - $L_1$ and $L_2$. For each function we generate all scenes in the test set and compare them against outputs generated by our unaltered method.

\subsubsection*{Alternative Volumetric Regularization Functions}
We ablate our choice of implementation for the Volumetric Regularization loss by replacing our density feature correlation encouraging implementation $\mathcal{L}_\text{reg3D}$ with other functions that encourage similarity between the density features of $G_e$ and $G_i$ and using the same balancing weight - $200$. Namely we test two different loss functions - $L1$ and $L2$, both penalizing the distance between the density features of $G_i$ and those of $G_e$. We again generate all scenes in the test set and compare them against outputs generated by our unaltered method for each function.

\subsubsection*{Refinement}
We ablate the refinement stage of our pipeline by comparing our system's outputs with and without refining the outputs of the editing stage. Note that this ablation only effects local prompts.
}

\input{figures/supp_timestamps}

\ignorethis{
\input{figures/text_to_3d_ablation}
}




\subsection{Ablating the Color Representation}
\label{sec:sh}
As mentioned in Section 3.1 of the main paper, we  do not model view dependent effects using higher order spherical harmonics as that leads to undesirable effects. We demonstrate this by observing these effects in examples rendered with 1st and 2nd order spherical harmonic coefficients as color features. These results can be seen in videos available on our project page. % (please refer to  \url{index.html} for these videos). 

When observing these results we can clearly see how view-dependent colors yield undesirable effects such as the feet of the ``yarn kangaroo" varying from green to yellow across views or the head of the dog becoming a birthday party hat when it faces away from the camera. We additionally see the colors become over-saturated, especially when using second-order spherical harmonic coefficients. It is also evident that the added expressive capabilities of the model allow it to over-fit more easily to specific views, creating unrealistic results such as the ``cat wearing glasses" in the first and second order coefficient models, where glasses are scattered along various parts of its body. We note that while this expressive power currently produces undesirable effects it does potentially enable higher quality and more realistic renders, and therefore, we believe that constraining this power is an interesting topic for future research.

\subsection{Cross-attention Grid Supervision}
%\paragraph{Visualizing cross-attention maps from different timestamps}.
As explained in Section \ref{sec:imp}, we use a constant time-stamp of 0.2 when extracting attention maps for training our attention grids $A_e$ and $A_{obj}$. This value was chosen empirically as we found that higher time-steps tend to be noisier and less focused, while lower time-steps varied largely from pose to pose producing inferior attention grids. This can be seen qualitatively in Figure \ref{fig:supp_timestamps}. As illustrated in the figure, the attention values for the edit region get gradually more smeared and unfocused as the time-steps increase. This is evident, for instance, in warmer regions around the kangaroo's tail or the head of the duck. While perhaps less visually distinct, we can also observe that in lower timestamps the warm regions denoting high attention values cover a smaller area of the region which should be edited. We empirically find that this makes it more challenging for separating the object and edit regions.

%While not completely incorrect we find that this %makes separating the edit from the object harder in our implementation.\hec{Explain that we observe this and demonstrate it in Figure...}\ref{fig:supp_timestamps}.
