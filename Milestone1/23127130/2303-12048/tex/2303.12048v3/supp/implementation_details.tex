\section{Additional Details}
\label{sec:details}

\subsection{Implementation Details}
\label{sec:imp}
Below we provide all the implementation details of our method, detailed in Section 3 in the main paper.

\subsubsection*{Grid-Based Volumetric Representation}
%We start our method by reconstructing the scene depicted in the input oriented images our system receives as input. In detail our 
We use 100 images uniformly sampled from upper hemisphere poses along with corresponding camera intrinsic and extrinsic parameters to train our initial grid. We follow the standard ReLU Fields~\cite{karnewar2022relu} training process using their default settings aside from two modifications: 
\begin{enumerate}
    \item We change the result grid size from the standard $128^3$ to $160^3$ to increase the output render quality.
    \item As detailed in the main paper, we limit the order of spherical harmonics to be zero order only to avoid undesirable view-dependent effects (we further illustrate these effects in Section \ref{sec:sh}). 
\end{enumerate}

 

\subsubsection*{Text-guided Object Editing}
%In the object editing optimization stage we use the Annealed SDS Loss described above along with the regularization term $\mathcal{L}_\text{reg3D}$ described in Section 3.2 of the paper in order to edit the reconstruction grid according to an input text prompt. We initialize our editing grid $G_e$ to be the initial reconstruction grid $G_i$ obtained in the reconstruction stage.
We perform 8000 training iterations during the object editing optimization stage. During each iteration, a random pose is uniformly sampled from an upper hemisphere and an image is rendered from our edited grid $G_e$ according to the sampled pose and the rendering process described in ReLU Fields \cite{karnewar2022relu}. Noise is then added to the rendered image according to the time-step sampled from the fitting distribution. 

We use an annealed SDS loss which gradually decreases the maximal time-step we draw $t$ from. Formally, this annealed SDS loss introduces three additional hyper-parameters to our system: a starting iteration $i_{start}$, an annealing frequency $f_a$ and an annealing factor $\gamma_a$. With these hyper-parameters set, we change our time-step distribution to be:
\begin{equation}
    t \sim U[t_0 + \varepsilon, t_{final}*k_i + \varepsilon],
\end{equation}
\begin{equation}
    k_i = 
    \begin{cases}
    1, & \text{if } i < i_{start} \\
    k_{i-1}*\gamma_a, & \text{else if } i\ \% \ f_a = 0 \\
    k_{i-1}, & \text{otherwise}
    \end{cases}
\end{equation}
%We find that tuning these three parameters can lead to significant improvements in output quality. 
In all our experiments, the values we use for $\varepsilon$, $i_{start}$, $f_a$ and $\gamma_a$ are 0.02, 4000, 600, and 0.75. Additionally, we stop annealing the time-step when it reaches a value of 0.35. %The noised output is then passed through a latent diffusion model producing a noise prediction. The noise prediction is subtracted from the noise added to the rendered output to produce the "SDS loss" which is directly back-propagated to the edited grid. 
The latent diffusion model we use in our experiments is "StableDiffusion 2.1" by Stability AI\href{https://huggingface.co/stabilityai/stable-diffusion-2-1}. %\hec{please replace the url with the correct one!}. 

We use a weight of $200$ to balance the two terms (multiplying $\mathcal{L}_\text{reg3D}$ by this weight value). The volumetric regularization term operates only on the density features of the editing grid. The optimizer we used in this (and all other stages) is the Adam optimizer~\cite{adamoptimizer} with a learning rate of 0.03 and betas 0.9, 0.999. The resolution of the images rendered from our grid is 266$\times$266. We add a "a render of" prefix to all of our editing prompts as we found that this produced more coherent results (and the images the LDM receives are indeed renders).

%\hec{We need to say somewhere in this section that we always add ``A render of a" to the beginning of text prompt provided to the SDS loss} \esc{Do we still need to mention this if we have "A render of" at the beginning of every prompt we talk about?} \hec{I would remove it from the later section and mention once here -- the main reason is: consistency with what we write in the main paper}


\subsubsection*{Spatial Refinement via 3D Cross-Attention}
%\paragraph{Extracting Cross-Attention Maps} 
%As prior works have shown \cite{hertz2022prompt}, Cross-Attention maps extracted from the cross-attention layers of a diffusion model convey a spatial layout of the areas in the image that relate to a certain input text token and are most likely to be affected by it during the denoising process. Formally, the cross-attention layers of the U-shaped network within the LDM receive the deep spatial features of the noisy image $\phi(z_t)$ and a textual embedding $\psi(\mathcal{P})$. Three learned linear projections $l_Q, l_K, l_V$ map the textual embedding and the spatial features of the noisy image to keys queries and values: $Q = l_Q(\phi(z_t)), K = l_K(\psi(\mathcal{P})), V = l_V(\psi(\mathcal{P}))$. The attention map $M$ is defined as: 
% \begin{equation}
%     M = Softmax(\frac{QK^T}{\sqrt{d}})
% \end{equation}
% With each cell in $M_{ij}$ representing the weight of the value of the $j$-th token on the pixel $i$. In practice, 
The diffusion model we use for this stage is \href{ https://huggingface.co/CompVis/stable-diffusion-v1-4 }{"StableDiffusion 1.4" by CompVis} and it consists of several %\esc{I counted 11 in the code but I'm not sure where to confirm this} 
cross-attention layers at resolutions 32, 16, and 8. To extract a single attention map for each token we interpolate each cross attention map from each layer and attention head to our image resolution (266x266) and take an average per each token. %The attention map is the attention map corresponding to the token of a user selected "edit word" (for example the word "sunglasses" in the prompt "a dog wearing sunglasses") while the "object" attention map is defined as the maximal attention amongst all other tokens. 
The time-step we use to generate the attention maps is 0.2 (the actual step being 0.2 * $N_{steps}$ = 200). 

%\paragraph{Spatial Refinement}
The cross-attention grids $A_{e}$ and $A_{obj}$ contain a density feature and an additional one-dimensional feature $a$, which represents the cross-attention value at a given voxel and can be interpreted and rendered as a grayscale luma value. We initialize the density features in these grids to the density features of the editing grid's (the former stage's output) and freeze them.
%In the spatial refinement stage we use cross attention maps to learn a binary volumetric mask that separates voxels belonging to the edit with those belonging to the object. To do this we define two ReLU fields - $A_{e}$ and $A_{obj}$ initialize their density values to those of the editing grid's (the former stage's output) and freeze them. As for the RGB feature values we replace each three dimensional feature with a one-dimensional feature $a$ which represents the attention value at a given voxel and can be interpreted and rendered as a grayscale luma value. 
At each refinement iteration we generate two 2D cross-attention maps from the LDM, one for the object and one for the edit. After obtaining the 2D cross-attention maps, we render gray-scale heatmaps %\esc{perhaps it would be better to treat them as heatmaps?}\hec{sure} 
from $A_{e}$ and $A_{obj}$ and use $L1$ loss to encourage similarity between the rendered attention images and their corresponding attention maps extracted from the diffusion model. We repeat this process for 1500 iterations, sampling a random upper-hemisphere pose each time. As in the former optimization stage, we use the Adam optimizer with a learning rate of 0.03 and betas 0.9 and 0.999 and generate images in 266$\times$266 resolution.

After obtaining the two grids $A_{e}$ and $A_{obj}$, we perform element-wise softmax on their $a$ values to obtain probabilities for each voxel belonging to either the object, denoted by $P_{obj}(v)$, or the edit, denoted by $P_e(v)$. 
We then proceed to calculate the binary refinement volumetric mask. To do this we define a graph in which each non-zero density voxel in our edited grid $G_e$ is a node. We define "edit" and "object" labels as the \emph{source} and \emph{drain} nodes, such that a node connected to the source node is marked as an "edit" node and a node connected to the drain node is marked as an "object" node. We rank the nodes according to their $P_e(v)$ values and connect the top $N_{init-edit}$ nodes to the source node. We then rank the nodes according to their $P_{obj}(v)$ value and connect the top $N_{init-object}$ nodes to the drain node.
%We connect the $N_{init-edit}$ nodes with the highest \hec{what does highest mean? using a threshold? K-nearest neighbors connectivity?} edit affiliation probabilities $P_e(v)$ to the source node and connect the $N_{init-object}$ nodes with the highest object affiliation probabilities $P_{obj}(v)$ to the drain node. 
We then connect the non-terminal nodes to each-other in a 6-neighborhood with the capacity of each edge being $w_{pq}$ as detailed in the main paper.

% \begin{equation}
%     w_{pq}=\lambda*\text{exp}\left(\frac{-(c_p - c_q)^2}{2\sigma^2}\right)
% \end{equation}

%With $c_p$ and $c_q$ being the RGB colors (given by $G_e$) of the two connected nodes.
%After constructing the graph we segment each node to be either an "edit" or "object" node via graph-cuts \cite{boykov2001fast} which in turn produces a binary volumetric segmentation mask. 
We set the hyper-parameters $N_{init-edit}$ and $N_{init-object}$ to be 300 and 200. %, and set $\lambda$ and $\sigma$ to be 5.0 and 0.1. 
To perform graph-cut~\cite{boykov2001fast}, we used the \href{ https://github.com/pmneila/PyMaxflow }{PyMaxflow} implementation of the max-flow / min-cut algorithm. %To perform the actual refinement of our edited grid $G_e$ we replace all voxel features for voxels in $G_e$ not marked as "edit" by the segmentation mask with the corresponding voxel features in the initial reconstructed grid $G_i$.

\subsection{Evaluation Protocol}
To evaluate our results quantitatively, we constructed a test set composed of eight scenes: 'White Dog', 'Grey Dog', 'White Cat', 'Ginger Cat', 'Kangaroo', 'Alien', 'Duck' and 'Horse', and six editing prompts: (1) A $\left<object\right>$ wearing big sunglasses, (2) A $\left<object\right>$ wearing a Christmas sweater, (3) A $\left<object\right>$ wearing a birthday party hat, (4) A yarn doll of a $\left<object\right>$, (5) A wood carving of a $\left<object\right>$, (6) A claymation $\left<object\right>$. This yields 18 edited scenes in total. We render each edited scene from 100 different poses distributed evenly along a $360^{\circ}$ ring. In addition to these 18 scenes we also render 100 images from the same poses on the initial (reconstruction) grid $G_i$ for each input scene. When comparing our result with other 3D textual editing papers we evaluate our results using two CLIP-based metrics. % - CLIP similarity to the input textual prompt ($CLIP_{Sim}$), and directional CLIP similarity ($CLIP_{Dir}$) as described in \cite{gal2021stylegannada}. 
The CLIP model we used for both of these metrics is \href{https://github.com/openai/CLIP}{ViT-B/32} and the input image text prompts used to calculate the directional CLIP metric is ``A render of a $\left<object\right>$". $CLIP_{Dir}$ is calculated for each edited image in relation to the corresponding image in the reconstruction scene.
To quantitatively evaluate ablations we use two additional metrics using FID \cite{Seitzer2020FID}. For this we use the \href{ https://github.com/mseitzer/pytorch-fid }{pytorch implementation} given by the authors with the standard settings. 

\subsubsection*{$360^\circ$ \emph{Real Scenes}} For the $360^\circ$ \emph{Real Scenes} edits we follow the same implementation details as outlined previously, with three modifications:

\begin{enumerate}
    \item We alternate between using the DVGO model or the ReLU-Fields model as our 3D representation. Results for both models are presented in Figure 6 of the main paper.
    
    \item Our input poses are  created in a spherical manner and when rendering we sample linearly in inverse depth rather than in depth as seen in the official implementation of NeRF \href{https://github.com/bmild/nerf}.
    
    \item We perform 5000 training iterations during the object editing optimization stage and the values we use for $\varepsilon$, $i_{start}$, $f_a$ and $\gamma_a$ are 0.02, 3000, 400, and 0.75.
\end{enumerate}





\subsection{3D Object Editing Techniques} 
Below we provide additional details on the alternative 3D object editing techniques we compare against. All of the techniques we compare against use only an un-textured mesh and an editing prompt as input. As such, we used the meshes our inputs were rendered from as input for the editing methods. Additionally, we tested an additional scenario in which we imported the 'horse' mesh from the \href{https://github.com/threedle/text2mesh/blob/main/data/source_meshes/horse.obj}{Text2Mesh GitHub repository} to blender, added a grey-matte material to it and rendered images of it to use as input for our system. This scenario used four prompts: (1) A wood carving of a horse, (2) A horse wearing a Santa hat, (3) A donkey, (4) A carousel horse, and was used for qualitative comparisons only.

\subsubsection*{Text2Mesh} 
When comparing to Text2Mesh we used the \href{https://github.com/threedle/text2mesh}{code provided by the authors} and the input settings given in the "run\_horse.sh" demo file.

%We used two different settings when comparing our outputs to those of Text2Mesh. In the first setting, we imported the un-textured horse mesh from the \href{https://github.com/threedle/text2mesh/tree/main/data/source_meshes}{Text2Mesh provided meshes} to blender, applied a grey-matte material to it, rendered 100 images of it from random upper hemispherical poses and used these images and poses as input to our system. Using this input we compared our editing results given four different editing prompts: (1) A render of a wood carving of a horse, (2) A render of a horse wearing a Santa hat, (3) A render of a donkey, (4) A render of a carousel horse. The input for Text2Mesh in this setting was the aforementioned un-textured horse mesh with the settings used being the ones given in the \href{https://github.com/threedle/text2mesh/blob/main/demo/run_horse.sh}{'run\_horse.sh' demo bash script}. This setting was used solely for qualitative comparisons.  

%In the second comparison setting we gave Text2Mesh the original meshes (the ones we used to render our input images in blender) of the test-set scenes as input, again using the 'run\_horse.sh' demo settings. We ran Text2Mesh for all six test-set editing prompts for each scene and compared the outputs to those obtained by our system. %As for quantitative comparisons to 3D object editing techniques we evaluated metrics for global and local prompts separately. \hec{Why does this last line belong here in the Text2Mesh section?}

\subsubsection*{SketchShape}
 In this comparison we again use the \href{https://github.com/eladrich/latent-nerf}{code provided by the authors}. And the input parameters used are the default parameters in the 'train\_latent\_nerf.py' script  \href{https://github.com/eladrich/latent-nerf/tree/main/scripts}{'train\_latent\_nerf.py' script} with 10,000 training steps (as opposed to the default 5,000).

\subsubsection*{Latent-Paint}
We compared our method to Latent-Paint only qualitatively as this method outputs edits that transform only the appearance of the input mesh, rather than appearance and geometry. As in SketchShape we used the code provided by the authors and used the default input settings provided for latent paint, which are given in the  \href{https://github.com/eladrich/latent-nerf/tree/main/scripts}{'train\_latent\_paint.py' script}.

\subsubsection*{DFF + CN}
 In this comparison we use the \href{https://github.com/pfnet-research/distilled-feature-fields} {code provided by the authors} and the default input parameters provided for this method.

\subsection{2D Image Editing Techniques}

When comparing to InstructPix2Pix and SDEdit we constructed two image sets for each scene / prompt combination we wanted to test. Both sets were created by rendering one of our inputs in evenly spaced poses along a $360^{\circ}$ ring, one set was rendered over a white background and the other over a 'realistic' image of a brick wall. We used these sets as input for each 2D editing method along with an editing prompt and compared the results to rendered outputs from our result grids. When comparing to InstructPix2Pix we used the standard \href{https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/pix2pix}{InstructPix2Pix pipeline} with 16bit floating point precision and 20 inference steps. We used the default guidance scale (1.0) for the images rendered over the 'realistic' background and increased the guidance scale to 3.0 for the images rendered over a white background, as we found it to produce higher quality results specifically for these more 'synthetic' images. When giving prompts to InstructPix2Pix we rephrased our prompts as instructions, for example turning "a dog wearing sunglasses" to "put sunglasses on this dog". When comparing to SDEdit we used the \href{https://huggingface.co/docs/diffusers/using-diffusers/img2img}{standard SDEdit pipeline} with guidance scale of 0.75 and a strength of 0.6.

%\hec{seems all the details are shared for both methods so let's remove the subsections, and have a unified section, where we also note that we used all the default settings of both, with links to their code}


\ignorethis{
\begin{enumerate}
    \item Real-scenes?
    \item cross-attention maps - \textbf{maybe a correction figure?} also, what timesteps are we using, any other important details?
    \item explain ablations and comparisons (all details needed to reproduce experiments)
    \item all hyperparameters
    \item Dataset details - the meshes, all the prompts
\end{enumerate}
}


