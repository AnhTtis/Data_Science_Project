\ignorethis{
\input{figures/supp_realscenes}
\input{figures/comparison2d_supp}
}


\section{Additional Visualizations and Results}
\label{sec:res}


\ignorethis{
\rev{
\paragraph{2D Image Editing Comparisons} An underlying assumption in our work is that editing 3D geometry cannot easily be done by reconstructing edited 2D images depicting the scene. To test this hypothesis, we modified images rendered from various viewpoints using the diffusion-based image editing methods InstructPix2Pix~\cite{brooks2022instructpix2pix} and SDEdit~\cite{meng2022sdedit}. %Each method was given 100 input images and a single prompt, after modifying the images according to the input text we intended to reconstruct the "edited" scene according to the edited images. 
As illustrated in Figure~\ref{fig:comparison2d_supp}, 2D methods often struggle to produce meaningful results from less \emph{canonical} views (\emph{e.g.}, adding sunglasses on the dog's back) and also produce highly view-inconsistent results. %, making 3D reconstruction meaningless.
}
}


\ignorethis{
\rev{
\paragraph{Comparisons to an unconditional text-to-3D model}
In Figure \ref{fig:t23dablation} we compare to the unconditional text-to-3D model proposed in Latent-NeRF, to show that such unconditional models are also not guaranteed to generate a consistent object over different prompts. We also note that this result (as well as our edits) would certainly look better if fueled with a proprietary big diffusion model \cite{saharia2022photorealistic}, but nonetheless, these models cannot preserve identity.
}
}


\ignorethis{As previously explained, the underlying 3D representation used throughout our work is the ReLU-Fields ~\cite{karnewar2022relu} representation , a method which unfortunately displays underwhelming performance on real-scenes. While not the prime focus of our work, we did make effort to try and improve our method's performance on these more complex scenes. Leveraging the fact that our method is agnostic to its underlying voxel based 3D representation we experimented with implementing our method on top of DVGO \cite{sun2022direct}, a 3D framework better suited for real world scenes. In figure \ref{fig:supp_realscenes} we show results obtained when using this alternative implementation compared against results obtained when using our "default" pipeline. As illustrated in the figure, edit results obtained in the 'DVGO' setting are of higher visual quality, which is somewhat unsurprising as the initial scene reconstruction is also of higher quality. This improved quality however comes at the expense of running time (around 1.5x in our system setting) and did not translate as distinctively to the synthetic scenes and objects which make up the bulk of our experiments. }


\ignorethis{
\begin{enumerate}
    \item Ablation of our approach with higher order SH coefficients 
    \item Try multiple image-based configurations
    \item Lower priority -- additional metrics? a user study?
\end{enumerate}
}

\ignorethis{
\rev{
\paragraph{Alternative voxel-based 3D representations for real-scenes}
As discussed in Section \ref{sec:results}, we use DVGO \cite{sun2022direct} as our underlying 3D representation when editing real world scenes as we found it to be more suitable for these higher complexity scenes when compared to ReLU-Fields \cite{karnewar2022relu}. In Figure \ref{fig:supp_realscenes} we justify this decision by presenting qualitative results using both DVGO and ReLU-Fields for representing real-world scenes. As illustrated in the figure, DVGO (bottom row) produces both higher quality scene reconstructions (the columns labeled 'Initial') and edits when compared to ReLU-Fields. We note that this improved quality however comes at the expense of running time (around 1.5x in our system setting). % and did not translate as distinctively to the synthetic scenes and objects which make up the bulk of our experiments.
}
}


\paragraph{Visualizing 2D cross-attention maps and images rendered from our 3D cross-attention grids}
While the attention maps used as ground-truth are inherently unfocused (as they are up-sampled from very low resolutions) and are not guaranteed to be view consistent, we show that learning the projection of these attention maps on to our objectâ€™s density produces view-consistent heat maps for object and edit regions (Figure \ref{fig:supp_attn_kangaroo}).

\ignorethis{
\paragraph{Additional 2D image editing results} In Figure \ref{fig:comparison2d_supp} we show additional comparisons to 2D image editing techniques. These results further illustrate that 2D methods struggle to produce view consistent results. 
}


\input{figures/supp_attn_kangaroo}