\section{Introduction}

Creating and editing 3D models is a cumbersome task. While template models are readily available from online databases, tailoring one to a specific artistic vision often requires extensive knowledge of specialized 3D editing software.
%Editing objects to accommodate desired attributes while preserving their underlying geometry and appearance is a longstanding goal in computer vision and graphics. 
In recent years, neural field-based representations (e.g., NeRF~\cite{mildenhall2021nerf}) demonstrated expressive power in faithfully capturing fine details, while offering effective optimization schemes through differentiable rendering. Their applicability has recently expanded also for a variety of editing tasks. However, research in this area has mostly focused on either appearance-only manipulations, which change the object's texture~\cite{xiang2021neutex,yang2022neumesh} and style~\cite{zhang2022arf,wang2022nerf}, or geometric editing via correspondences with an explicit mesh representation~\cite{garbin2022voltemorph,yuan2022nerf,xu2022deforming}---linking these representations to the rich literature on mesh deformations~\cite{igarashi2005rigid,sorkine2007rigid}. Unfortunately, these methods still require placing user-defined control points on the explicit mesh representation, and cannot allow for adding new structures or significantly adjusting the geometry of the object.

In this work, we are interested in enabling more flexible and localized object edits, guided only by textual prompts, which can be expressed through \emph{both} appearance and geometry modifications. To do so, we leverage the incredible competence of pretrained 2D diffusion models in editing images to conform with target textual descriptions. We carefully apply a \emph{score distillation} loss, as recently proposed in the unconditional text-driven 3D generation setting~\cite{poole2022dreamfusion}. Our key idea is to regularize the optimization in 3D space. We achieve this by coupling two volumetric fields, providing the system with more freedom to comply with the text guidance, on the one hand, while preserving the input structure, on the other hand. 

\ignorethis{Rather than using neural fields, we base our method on \emph{lighter} ReLU Fields~\cite{karnewar2022relu} which do not require any neural networks and instead model the scene as a voxel grid where each voxel contains learned features. }


Rather than using neural fields, we base our method on \emph{lighter} voxel-based representations %such as ReLU-Fields \cite{karnewar2022relu} or DVGO \cite{sun2022direct} which do not encode the scene into the weights of a neural network but instead model the scene as a voxel grid where each voxel contains learned features. 
which learn scene features over a sparse voxel grid.
This explicit grid structure not only allows for faster reconstruction and rendering times, but also for achieving a tight \emph{volumetric} coupling between volumetric fields representing the 3D object before and after applying the desired edit using a novel \emph{volumetric correlation loss} over the density features. 
%\esc{While we exclusively use DVGO and ReLU-Fields in our experiments, our method is agnostic to the underlying voxel based 3D representation. This flexibility allows using different representations in different scenes and scenarios, and potentially allows for future voxel based representations to make use of our method.}%, encouraging the edited field to resemble the input field. 
%
%
To further refine the spatial extent of the edits, we utilize 2D cross-attention maps which roughly capture regions associated with the target edit, and lift them to volumetric grids. This approach is built on the premise that, while independent 2D internal features of generative models can be noisy, unifying them into a single 3D representation allows for better distilling the semantic knowledge.  %edited regions
We then use these 3D cross-attention grids as a signal for a binary volumetric segmentation algorithm that splits the reconstructed volume into edited and non-edited regions, allowing for merging the features of the volumetric grids to better preserve regions that should not be affected by the textual edit.


Our approach, coined \emph{Vox-E}, provides an intuitive voxel editing interface, where the user only provides a simple target text prompt (see Figure~\ref{fig:teaser}). % See, for example, the terse text prompts provided in Figure~\ref{fig:teaser} and their associated edits which preserve the object's appearance and geometry. 
We compare our method to existing 3D object editing techniques, and demonstrate that our approach can facilitate local and global edits involving appearance and geometry changes over a variety of objects and text prompts, which are extremely challenging for current methods.

Explicitly stated, our contributions are:
\begin{itemize}
    \item A coupled volumetric representation tied using 3D regularization, allowing for editing 3D objects using diffusion models as guidance while preserving the appearance and geometry of the input object.
    \item A 3D cross-attention based volumetric segmentation technique that defines the spatial extent of textual edits.
    \item Results that demonstrate that our proposed framework can perform a wide array of editing tasks, which cannot be previously achieved.
\end{itemize}


%two competing / conflicting image space losses leading to bad results, the idea of the coupled representation is that we instead tie them together in 3D space rather than trying see it as a multiview problem
%decouple structure and appearance trivial with this formulation, but cannot be achieved with images alone

% that hightlight the regions that are being changed / edited 
% as a signal for a binary segmentation algorithm that splits the reconstructed volume into edited and non-edited segmentation
% combining, merging, "to completely preserve the original model in regions which were not focused on by the diffusion model" identity-preservation

%Using these 3D cross-attention grids, we define an energy minimization problem over all the voxels in the grid, allowing for blending the features of the coupled grids in a piecewise smooth manner which better preserves regions which should not be affected by the textual edit. 

%At their core, we lift cross-attention values to volumetric weights, enabling a tight coupling over some regions,  while freely manipulating other regions according to the textual prompt.
%where a tight coupling is achieved over regions , lifting cross-attention values to volumetric weights