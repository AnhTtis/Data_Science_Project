\section{Related Work}

\noindent \textbf{Text-driven Object Editing.} 
Computational methods targeting text-driven image generation and manipulation have seen tremendous progress with the emergence of CLIP~\cite{radford2021learning} and diffusion models~\cite{ho2020denoising}, advancing from specific domains~\cite{styleflow, shen2020interpreting,patashnik2021styleclip,gal2021stylegannada} to more generic ones~\cite{nichol2021glide,avrahami2022blended,couairon2022diffedit,kawar2022imagic}. Several recent methods allow for performing convincing localized edits on real images without requiring mask guidance~\cite{bar2022text2live,hertz2022prompt,brooks2022instructpix2pix,pnpDiffusion2022, Parmar2023ZeroshotIT}. 
However, these methods all operate on single images and cannot facilitate a consistent editing of 3D objects.
%other papers to add: 1. Region-Aware Diffusion for Zero-shot Text-driven Image Editing

While less common, methods for manipulating 3D objects are also gaining increasing interests.
Methods such as LADIS~\cite{huang2022ladis} and ChangeIt3D~\cite{achlioptas2022changeit3d} aim at learning the relations between 3D shape parts and text directly using datasets composed of edit descriptions and shape pairs. These works allow for geometric edits but fail to generalize to out of distribution shapes and cannot modify appearance. 

Alternatively, several methods have proposed leveraging 2D image projections, matching these to a driving text. Text2Mesh~\cite{michel2022text2mesh} uses CLIP for stylizing 3D meshes based on textual prompts. Tango~\cite{chen2022tango} also styles meshes with CLIP, enabling additionally stylization of lighting conditions, reflectance properties and local geometric variations. TEXTure~\cite{richardson2023texture} use a depth-to-image diffusion model for texturing 3D meshes. Unlike our work, these methods focus mostly on texturing meshes, and cannot be used for generating significant geometric modifications, such as adding glasses or other types of accessories.
%%Although several works aim at learning the relations between 3D shape parts and text directly~\cite{huang2022ladis,achlioptas2022changeit3d}, most manipulation techniques are often guided by their 2D image projections which are matched to the driving text.  Text2Mesh~\cite{michel2022text2mesh} leverage CLIP for stylizing 3D meshes based on textual prompts. Tango~\cite{chen2022tango} also styles meshes with CLIP, enabling additionally stylization of lighting conditions, reflectance properties and local geometric variations.


\input{figures/overview.tex}

\medskip
\noindent \textbf{Neural Field Editing.} 
Neural fields (e.g., NeRF~\cite{mildenhall2021nerf}), which can be effectively learned from multi-view images through differentiable rendering, have recently shown great promise for representing object and scenes. Prior works have demonstrated that these fields can be adapted to express different forms of manipulations. ARF~\cite{zhang2022arf} transfers the style of an exemplar image to a NeRF. NeRF-Art~\cite{wang2022nerf} performs a text-driven style transfer. Distilled Feature Fields~\cite{kobayashi2022decomposing}  distill the knowledge of 2D image feature extractors into a 3D feature field and use this feature field to localize edits performed by CLIP-NeRF~\cite{wang2022clip}, which optimizes a radiance field so that its rendered images match with a text prompt via CLIP.

Several works have shown that neural fields can be edited by editing selected 2D images~\cite{liu2021editing,yang2022neumesh}. NeuTex~\cite{xiang2021neutex} uses 2D texture maps, which can be edited directly, to represent the surface appearance. 
% mention Peter's "Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields"?
%mention Sagie's "Volumetric Disentanglement for 3D Scene Manipulation"?
Other works demonstrated geometric editing of shapes represented with neural fields via correspondences with an explicit mesh representation~\cite{garbin2022voltemorph,yuan2022nerf,xu2022deforming}, that can be edited using as-rigid-as-possible deformations~\cite{sorkine2007rigid}. However, these cannot easily allow for modifying the 3D mesh to incorporate additional parts, according to the user's provided description. Concurrently to our work, Instruct-NeRF2NeRF ~\cite{haque2023instruct} uses an image editing model to iteratively edit multi-pose images from which an edited 3D scene is reconstructed. Unlike our work which optimizes the underlying 3D representation, they optimize the input images directly. \ignorethis{This approach differs greatly from ours, as the driving optimization occurs on the input images as apposed to the weights of the underlying neural 3D representation.}
%
Furthermore, our method is based on grid-based representations rather than neural fields, in particular ReLU Fields~\cite{karnewar2022relu}, which do not require any neural networks and instead model the scene as a voxel grid where each voxel contains learned features. We show that having an explicit grid structure is beneficial for editing 3D objects as it enables fast reconstruction and rendering times as well as powerful volumetric regularization. %In their work, they demonstrated that using ReLU activations after interpolating the grid values is competitive with the state-of-the-art, while enabling fast reconstruction and rendering times. In addition to these advantages, we show that having an explicit grid structure also provides additional advantages for editing 3D objects.






\medskip
\noindent \textbf{Text-to-3D.} Following the great success of text-to-image generation, we are witnessing increasing interests in unconditional text-driven generation of 3D objects and scenes. CLIP-Forge~\cite{sanghi2022clip} uses CLIP guidance to generate coarse object shapes from text. Dream Fields~\cite{jain2022zero}, DreamFusion~\cite{poole2022dreamfusion}, Score Jacobian Chaining~\cite{haochen2022score} and Latent-NeRF~\cite{metzer2022latent} optimize radiance fields to generate the geometry and color of objects driven by the text. While DreamFields relies on CLIP, the other three methods instead use a score distillation loss, which enables the use of a pretrained 2D diffusion model. Magic3D~\cite{lin2022magic3d} proposes a two-stage optimization technique to overcome DreamFusion's slow optimization. Unlike these works, we focus on the conditional setting. In our case, a 3D object is provided, and the desired edit should preserve the object's geometry and appearance. Still, we compare with Latent-NeRF in the experiments, as it can use rough 3D shapes as guidance.
% \phc{Missing Latent-NeRF~\cite{metzer2022latent}}
 % also mention Danny's "Latent-nerf for shape-guided generation of 3d shapes and textures"?


