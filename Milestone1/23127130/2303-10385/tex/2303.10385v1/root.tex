%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
\pdfminorversion=4
\documentclass[letterpaper, 10 pt, conference]{IEEEtran}  % Comment this line out if you need a4paper

\usepackage[colorlinks,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue]{hyperref} % 颜色可调
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

%\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}


\usepackage{float}
\usepackage{subcaption}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}






\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother



\title{\LARGE \bf
Social Occlusion Inference with Vectorized Representation for Autonomous Driving
}


\author{Bochao Huang and Ping Sun% <-this % stops a space
\thanks{Bochao Huang and Ping Sun are with the Department of Software Engineering, Tongji University, Shanghai, China. Email: \tt\small \{2131500, pingsun\}@tongji.edu.cn.}%
}


\begin{document}



\maketitle
%\thispagestyle{empty}
%\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Autonomous vehicles must be capable of handling the occlusion of the environment to ensure safe and efficient driving. In urban environment, occlusion often arises due to other vehicles obscuring the perception of the ego vehicle. Since the occlusion condition can impact the trajectories of vehicles, the behavior of other vehicles is helpful in making inferences about the occlusion as a remedy for perceptual deficiencies. This paper introduces a novel social occlusion inference approach that learns a mapping from agent trajectories and scene context to an occupancy grid map (OGM) representing the view of ego vehicle. Specially, vectorized features are encoded through the polyline encoder to aggregate features of vectors into features of polylines. A transformer module is then utilized to model the high-order interactions of polylines. Importantly, occlusion queries are proposed to fuse polyline features and generate the OGM without the input of visual modality. To verify the performance of vectorized representation, we design a baseline based on a fully transformer encoder-decoder architecture mapping the OGM with occlusion and historical trajectories information to the ground truth OGM. We evaluate our approach on an unsignalized intersection in the INTERACTION dataset, which outperforms the state-of-the-art results.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Perception of the environment is an integral part of ensuring the safety and efficiency of autonomous driving. However, the driving environment of autonomous vehicles can be full of occlusion due to static or dynamic obstacles, especially in urban areas \cite{1}. Humans also suffer from visual limitations when driving. Still, they can usually scene abnormal behaviors of other vehicles around them based on intuition and experience and infer that there may be moving pedestrians or vehicles in occluded areas \cite{2}. In order to be safe and efficient, autonomous vehicles should also be capable of making inferences about occluded regions to make up for the lack of perception \cite{3}.

   
Occlusion inference in autonomous driving assumes occluded regions to be either free or occupied space. Afolabi \textit{et al}. \cite{4} exploited that the actions of other intelligent agents also present useful information. In addition to the sensors equipped by the ego vehicle, they modeled other vehicles as additional sensors to infer the regions of occlusion in a map, coining the term \textit{People as Sensor(PaS)}. They used occupancy grid map (OGM) \cite{5} representations to present the regions of occlusion. Itkina \textit{et al}. \cite{6} extended PaS to incorporate distributional multimodality and to a multi-agent framework. They learned a driver sensor model that maps an observed driver trajectory to an OGM of the space ahead of the driver and fused all the OGMs inferred from driver sensors into the environment map.
Even though they took a fusion-based approach in considering multimodality, they still lack global interaction information due to the division of the environment map. Besides, neither of the above methods considers environment context, which influences the vehicles' trajectory.


   \begin{figure}[t]
      \centering
      \includegraphics[scale=0.33]{environment.png}
      \caption{An illustration of the vectorized information and the OGM. All of our considered features are presented in vectors. We divide the global environment map into small grids, and our goal is to infer whether the grids in occlusion are free or occupied.}
      \label{Fig. 1}
   \end{figure}


We use vectors to present the scene context, dynamic trajectory, and occlusion information inspired by VectorNet \cite{9}. All the static and dynamic information have a structured physical meaning. As illustrated in Fig. \ref{Fig. 1}, the structured features can be presented as points, polygons, or curves in geographic coordinates. Specially, the occlusion in the environment is caused by other vehicles blocking the view of ego vehicle, which leads to the ego vehicle failing to detect vehicles in the invisible areas.

\begin{figure*}[t]
  \centering
  \includegraphics[scale=0.365]{pipeline.png}
  \caption{An overview of our occlusion inference approach. Observed historical trajectories, scene context, and occlusion are represented as sets of vectors. These vectors are processed by polyline encoder to obtain the polyline features. Such features encoded with an interaction-aware transformer. Occlusion queries are proposed to fuse the polyline features through self-attention and cross-attention layers to obtain the final result.}
  \label{Fig. 2}
\end{figure*}

The goal of our approach is to compensate for perceived deficits by inferring about unseen regions, thereby reducing the uncertainties. OGMs are helpful in representing occlusion \cite{10, 11}. The inference task then turns to imply each grid in occlusion is occupied or free in the regions of occlusion, as illustrated in Fig. \ref{Fig. 1}. To solve the proposed social occlusion inference task, we use a learning-based method to make inferences about occluded spaces to be free or occupied. With the vectorized scene context, dynamic trajectory, and occlusion information as input, an attention-based polyline encoder is utilized to combine the features of vector sets into the features of polylines. We also employ an interaction-aware transformer block to model the interactions on the polyline features. Inspired by DETR \cite{DETR}, occlusion queries are proposed to tackle the modality mismatch between input and output. We use stacked cross-attention and self-attention layers for occlusion queries to fuses the encoded polyline features. These outputs can be concatenated together as the predicted result. Our proposed occlusion inference method is illustrated in Fig. \ref{Fig. 2}. In summary, our contributions are:

\begin{itemize}

\item We present a novel hierarchical transformer framework that enables end-to-end training for the social occlusion inference task. It can learn a mapping from sets of vectors to an OGM.

\item We propose occlusion queries to tackle the modality mismatch between input and output. It can fuse the polyline features through cross-attention mechanism.

\item We demonstrate that our vector-based approach yields state-of-the-art results and our image-based baseline on an unsignalized intersection in the INTERACTION dataset \cite{12}.

\end{itemize}


\section{Related Work}

\noindent \textbf{Safety and Uncertainty Awareness.} Autonomous vehicles are necessary to have the ability to ensure safety and be aware of uncertainty. Some prior work focuses on motion planning with the presence of uncertainty \cite{13, 14, 15, 16}. Stiller \textit{et al}. \cite{13} define criteria that measure the available margins to a collision to remain collision-free for the worst-case evolution. Sun \textit{et al}. \cite{14} propose a social perception scheme that learns a cost function from observed vehicles to avoid the collision. Rezaee \textit{et al}. \cite{15} present a reinforcement learning-based solution to manage uncertainty by optimizing for the worst-case outcome. Na \textit{et al}. \cite{16} describe a motion planning pipeline with respect to hypothetical hidden agents. Ren \textit{et al}. \cite{8} study a motion prediction task that considers unseen vehicles. All these approaches consider occlusion in perdition or planning tasks, but we focus on inferring the occlusion as a remedy for perceptual deficiencies. 



\noindent \textbf{Social Occlusion Inference.} At present, we find that there are few approaches to directly model the pedestrians' behaviors or vehicles' trajectories to estimate occlusion. The approaches using Pas proposed by Afolabi \textit{et al}. \cite{4} and Itkina \textit{et al}. \cite{6} are most closely to our work. Afolabi \textit{et al}. \cite{4} introduce and formalize the concept of PaS for imputing maps. To model the driver as a sensor, they define five categories of vehicle actions: moving fast, moving slow, accelerating, decelerating, and stopped. Then, they cluster vehicles' trajectories into different actions and finally learn occupancy probabilities for the OGM for each action. This approach is evaluated on a simulation dataset that has a crosswalk scenario with a single driver sensor and a single occluded pedestrian. This work does not account for the multimodality of the occlusion inference task. Itkina \textit{et al}. [6] extend PaS to incorporate distributional multimodality into a multi-agent framework. They present a two-stage occlusion inference algorithm. In the first stage, a driver sensor model is learned to map a vehicle's trajectory to a discrete set of possibilities for the local OGM ahead of the driver. They use a conditional variational autoencoder (CVAE) to account for multimodality. In the second stage, all the local OGMs are fused into the environment map to access the global OGM, using a multi-agent sensor fusion mechanism based on evidential theory. However, these approaches often neglect the interactions between different agents and the interactions between agent and environment. We propose to use a transformer encoder to model the high-order interactions between different elements. 


\section{Method}

\subsection{Problem Formulation}
We assume the receptive field of the ego vehicle to be a
neighborhood bounding box. Under this circumstance, we present the bounding box as an OGM $M \in [0, 1]^{H \times W}$, where $H$ and $W$ are the length and width respectively. The occupied grid cells have an occupancy probability of 1, while the free grid cells have a probability of 0. We consider grids to be in occlusion if their centers are in the invisible areas. To record the occlusion grids, another OGM $M_{mask} \in [0, 1]^{H \times W}$ is proposed that 1 indicates the occlusion grids while 0 indicates the visible grids. The inference task aims to infer the occupancy probability of each grid in occlusion (where $M_{mask} = 1$).
The social occlusion inference task can be formulated as predicting an output $\hat{M}$ at current time conditioned on the past and current states of visible traffic agents and scene context within the neighborhood.


For vectorized inputs, we define the polylines $\mathcal{P} = \{P_1, P_2, \cdots, P_n\}$, each denotes a set of vectors $P_i = \{v^i_1, v^i_2, \cdots, v^i_m \}$. We define the feature of a vector $v$ as:

$$
v = (x^s, y^s, x^e, y^e, c, a). \eqno{(1)}
$$

\noindent where ($x^s$, $y^s$) and ($x^e$, $y^e$) are coordinates of the start and end points of the vector; $c$ is the class of $v$ to determine whether it belongs to trajectory, scene context or occlusion; $a$ corresponds to attribute features, such as timestamps, object type for trajectories, or road type for scene context.

Suppose a mapping model $f$ with parameters $\theta$, the occlusion inference task is formulated as:

$$
\hat{M} = f(\mathcal{P}|\theta). \eqno{(2)}
$$

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.55]{polyline_encoder.png}
  \caption{An overview of polyline encoder. An extra feature token is concatenated with other tokens. After the operation of MSA, we take the extra feature token as polyline feature. }
  \label{Fig. 3}
\end{figure}

\subsection{Model Framework}
Fig. \ref{Fig. 2} shows the overall structure of our occlusion inference approach. Vectorized inputs $\mathcal{P}$ are encoded with polyline encoder to generate polyline features, which then are concatenated with their embedded types through a MLP layer. We adopt the interaction-aware transformer with residual connections to model the interactions between three types of polylines. The stacked cross-attention and self-attention layers fuse the embedded polyline features and the proposed occlusion queries, with the latter ultimately being transformed and concatenated to obtain the final result $\hat{M}$.


\subsection{Vector Encoder}
The process of encoding vectorized input consists of two primary stages. In the first stage, vector features are aggregated into polyline features. In the second stage, polyline features are encoded considering interaction awareness.

\textbf{1) Polyline Encoder:} All the three types of polylines are encoded by a shared polyline encoder based on attention mechanism. As illustrated in Fig. \ref{Fig. 3}, given a polyline $P = \{v_1, v_2, \cdots, v_m\}$, vector features are firstly embedded through a MLP layer to expand the dimension and then added with position embeddings to get the vector embeddings $\{e_1, e_2, \cdots, e_m\}$. Specially, we introduce a learnable token $c$ and concatenate it with other vector embeddings to obtain the multi-head self-attention (MSA) input $\{c, e_1, e_2, \cdots, e_m\}$. After a MSA layer, the information of all other tokens is aggregated on the learnable token, which is treated as polyline feature $h_P$. 

\textbf{2) Interaction-Aware Transformer:} Social interaction modeling is a standard method to capture the interactions between agents \cite{22, 24}. We go one step further by jointly modeling the interactions between agents, scene context, and occlusion for the inference task. We use a transformer \cite{25} encoder composed of 6 layers to model the high-order interactions on these features. Each transformer layer consists of a MSA block followed by a position-wise fully connected feed-forward network (FFN). A residual connection is added after each block, followed by a LN.

\subsection{Decoder}
The purpose of the occlusion inference task is to predict an OGM while the model takes vectorized input. To address the modality mismatch between input and output, we propose occlusion queries. Considering a zero-initialized OGM $M_{init} = \boldsymbol{0}^{H \times W} $, where $H$ and $W$ are the length and width respectively. We reshape it into a sequence of flattened patches $M_p \in \boldsymbol{0}^{N_p \times p^2}$, where $(p, p)$ is the resolution of each map patch, $N_p = (H \times W) / p^2$ is the number of patches. After position embedding, we get the occlusion queries $q \in \mathbb{R}^{N_p \times p^2}$.

The occlusion queries pass through a self-attention module firstly to learn the position information of each other and to increase their dimension to match that of polyline features. Then, there are stacked cross-attention and self-attention layers in the decoder. Occlusion queries extract polyline features in the cross-attention modules, while they interact with each other in the self-attention modules. Finally, we access the output $\hat{M}$ by reshaping and concatenating the occlusion queries.

\begin{table*}[t]
  \renewcommand\arraystretch{1.5}
  \tabcolsep=0.4cm
  \centering
  \caption{Our results compared with baselines}
    \begin{tabular}{|cc||ccc|ccc|ccc|}
    \hline
    \multicolumn{2}{|c||}{\multirow{1}[4]{*}{Method}} & \multicolumn{3}{c|}{Acc(\%).$\uparrow$} & \multicolumn{3}{c|}{MSE(\%) $\downarrow$} & \multicolumn{3}{c|}{IS$\downarrow$} \\
\cline{3-11}  \multicolumn{2}{|c||}{} & \multicolumn{1}{c}{Occ.} & \multicolumn{1}{c}{Free} & \multicolumn{1}{c|}{Overall} & \multicolumn{1}{c}{Occ.} & \multicolumn{1}{c}{Free} & \multicolumn{1}{c|}{Overall}  & \multicolumn{1}{c}{Occ.} & \multicolumn{1}{c}{Free} & \multicolumn{1}{c|}{Overall} \\
    \hline
    \hline
    \multicolumn{2}{|l||}{Our Baseline} & 0.658 & 0.732 & 0.731 & 0.314 & 0.182 & 0.179 & 1.082 & 0.017 & 1.103   \\
    \multicolumn{2}{|l||}{Multi-Agent PaS Avg. \cite{6}} & 0.660 & 0.722 & 0.722 &  0.303 & 0.171 & 0.173 & 1.336 & 0.017 & 1.353 \\
    \multicolumn{2}{|l||}{Multi-Agent PaS Top 3 \cite{6}} & 0.746 & 0.778 & 0.774 & 0.233 & 0.136 & 0.140 & 1.220 & 0.011 & 1.232 \\
    \multicolumn{2}{|l||}{Ours} & \textbf{0.763} & \textbf{0.827} & \textbf{0.826} & \textbf{0.216} & \textbf{0.099} & \textbf{0.101} & \textbf{0.147} & \textbf{0.006} & \textbf{0.153} \\
    \hline
    \end{tabular}
  \label{table1}
\end{table*}



\begin{table*}[t]
  \renewcommand\arraystretch{1.5}
  \tabcolsep=0.4cm
  \centering
  \caption{Ablation Studies for vector types}
    \begin{tabular}{|cc||ccc|ccc|ccc|}
    \hline
    \multicolumn{2}{|c||}{\multirow{1}[4]{*}{Context}} & \multicolumn{3}{c|}{Acc(\%).$\uparrow$} & \multicolumn{3}{c|}{MSE(\%) $\downarrow$} & \multicolumn{3}{c|}{IS$\downarrow$} \\
\cline{3-11}  \multicolumn{2}{|c||}{} & \multicolumn{1}{c}{Occ.} & \multicolumn{1}{c}{Free} & \multicolumn{1}{c|}{Overall} & \multicolumn{1}{c}{Occ.} & \multicolumn{1}{c}{Free} & \multicolumn{1}{c|}{Overall}  & \multicolumn{1}{c}{Occ.} & \multicolumn{1}{c}{Free} & \multicolumn{1}{c|}{Overall} \\
    \hline
    \hline
    \multicolumn{2}{|l||}{Traj. Only} & 0.742 & 0.813 & 0.812 & 0.250 & 0.112 & 0.114 & 0.186 & 0.008 & 0.196 \\
    \multicolumn{2}{|l||}{Traj. + Occ.} & 0.749 & 0.817 & 0.815 & 0.241 & 0.109 & 0.111 & 0.177 & 0.008 & 0.186 \\
    \multicolumn{2}{|l||}{Traj. + Env.} & 0.758 & 0.824 & 0.822 & 0.224 & 0.101 & 0.103 & 0.155 & 0.006 & 0.162 \\
    \multicolumn{2}{|l||}{Traj. + Env. + Occ.} & \textbf{0.763} & \textbf{0.827} & \textbf{0.826} & \textbf{0.216} & \textbf{0.099} & \textbf{0.101} & \textbf{0.147} & \textbf{0.006} & \textbf{0.153}\\
    \hline
    \end{tabular}
  \label{table2}
\end{table*}

\subsection{Loss function}

The occlusion inference task is equivalent to making a binary classification for each grid in occlusion. We train our model to optimize for the objective using:

$$
\mathcal{L} = \mathcal{L}_{global} + \alpha\mathcal{L}_{mask} + \beta\mathcal{L}_{occ}, \eqno{(3)}
$$

\noindent where $\alpha$ and $\beta$ are two scalars to balance the three loss terms, $\mathcal{L}_{global}$ is a standard grid-wise binary cross-entropy loss between $M_{gt}$ and $\hat{M}$ to learn the global representation, $\mathcal{L}_{mask}$ is another cross-entropy loss between $M_{mask} \odot \hat{M}$ and $M_{mask} \odot M_{gt}$ to make the model focus on the grids in occlusion, where $\odot$ denotes element-wise product. Due to occupancy class imbalance, $\mathcal{L}_{occ}$ is proposed to constrain the values of $M_{pred}$ not to be all zeros:

$$
\mathcal{L}_{occ} = \sum_{(x,y) \in I} (M_{gt}(x,y) - M_{pred}(x,y)), \eqno{(4)}
$$

\noindent where $I = \{(x,y) \vert M_{gt}(x,y) = 1\}$.




\section{Experiments}

This section describes the experimental settings, including dataset, data processing, baseline, and metrics. 


\subsection{Dataset and data processing}

INTERACTION dataset \cite{12} contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios from different countries. To enrich the interaction between vehicles, we pick a location in an unsignalized interaction scenario, which has a total of $10518$ vehicles. The vehicles continuously present for more than $1s$ serve as ego vehicles. We sample the presence time of ego vehicles as the current time (time $0s$) at $10Hz$ and then build ego OGMs around it to present the receptive fields. Each ego OGM has $70 \times 60$ grids of $1m \times 1m$ in size. For scene context and occlusion, we consider the elements within the receptive field to form the vectors. For trajectory, we form the trajectory vectors over $1s$ of past data sampled at $10Hz$ of all visible vehicles.

\subsection{Baselines}

Our baseline is based on a fully transformer encoder-decoder architecture mapping the OGM with occlusion and historical trajectories information to the ground truth OGM. Inspired by ViT \cite{26}, we divide the OGM into small patches to pass to the transformer, which will be stitched together after the transformer decoder to generate the result. A classification token is concatenated to the output features of the encoder for the binary classification task on each grid. We use the patch tokens to form the predicted OGM. For the input OGM $M_{in} \in \{0, 0.1, 0.2, \cdots, 1, 2\}^{H \times W}$, $0$ to $1$ represent the occupancy: $0$ indicates the grid is free while $1$ indicates the grid is occupied; $0.1$ to $0.9$ indicate the grid is occupied in the past second to embed the trajectories (the larger the number is, the closer to the current time), $2$ represents the occlusion grids. 

We also compare the results with \cite{6}. They use a CVAE to train the driver sensor model and fuse all the local OGMs to form the global OGM. We use their average results and the Top $3$ results they proposed. 


\subsection{Metrics}

To evaluate the performance of our model, we consider using three metrics: accuracy, mean squared error (MSE), and image similarity (IS) \cite{27, 4, 6}. For the classification task ($0$ or $1$), we threshold the occupancy probability above 0.5 to be occupied, below 0.5 to be free to compute the accuracy. We also use MSE to measure the predicted probability of occupancy. Due to the stochastic existence of vehicles, we cannot precisely infer the occupancy of each individual grid solely from the observed agent behaviors. Therefore, we use the image similarity metric to evaluate the relative similarity of the predicted OGM and the ground truth OGM. 
Due to the quantitative imbalance between free grids and occupied grids (in fact, the quantities of free grids are far more than the quantities of occupied grids), we use the metrics to evaluate each class besides overall. 



\begin{figure*}[t]
\captionsetup[subfigure]{labelformat=empty}
\centering
\centering
\begin{subfigure}{\textwidth}
\begin{subfigure}{.20\textwidth}
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[scale=0.5]{11.png}
\caption{Mask}
\label{4a}
\end{subfigure}
\begin{subfigure}{.20\textwidth}
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[scale=0.5]{12.png}
\caption{Probability}
\label{4b}
\end{subfigure}
\begin{subfigure}{.20\textwidth}
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[scale=0.5]{13.png}
\caption{Threshold}
\end{subfigure}
\begin{subfigure}{.20\textwidth}
\centering
\setlength{\abovecaptionskip}{0.05cm}
\includegraphics[scale=0.5]{14.png}
\caption{GT}
\end{subfigure}
\centering
\vspace{-0.2cm} %调整图片与上文的垂直距离
\end{subfigure}
\caption{Visualization of the outputs of our occlusion inference approach. We demonstrate mask OGM, occupancy probability OGM, predicted OGM, and ground truth OGM separately. The ego vehicles are in fixed positions near the bottom with upward driving directions. Red bounding boxes are used to annotate the vehicles in occlusion.}
\label{Fig. 4}
\end{figure*}



\section{Results}

\subsection{Baseline comparisons}

We demonstrate our results compared with baselines in Table \ref{table1}. Bold denotes the best result across a metric. To ensure a fair comparison, we use precisely the same train dataset and test dataset for the transformer-based baseline as well as the vectorized representation. By comparing our baseline, we can tell that the vectors can represent the structured elements more efficiently with less information loss than an OGM. Multi-agent PaS \cite{6} has two sets of metrics: the average and the Top 3. They take the best metric across the three most likely modes of the CVAE to generate the Top 3. In all metrics, our approach outperforms the results presented in \cite{6}, even their Top 3. We notice that our results make significant progress in IS metrics (note that IS values are divided by 100). It proves that by modeling the high-order interactions between polyline-level features, our predicted OGM has a more similar structure to the ground truth OGM. 


\subsection{Ablation Studies}


\noindent \textbf{Vector types.} There are three types of vectors for the input: trajectories, scene context, and occlusion. We study whether they are helpful for the occlusion inference task in Table \ref{table2}. "Traj." refers to the vectors of trajectory, "Env." refers to the vectors of scene context, and "Occ." refers to the vectors of occlusion. Three additional experiments are conducted to verify the impact of scene context and occlusion on the model performance. From all four rows, we can see the improvements of our model with two additional kinds of vectors, lacking any one of them hurts the performance. Moreover, the second and third rows indicate that the vectors of scene context have more contributions than the vectors of occlusion. 

\subsection{Occupancy visualization}

We demonstrate the visualization of the outputs of our occlusion inference approach in Fig. \ref{Fig. 4}. We can see that our method successfully infers the existence of vehicles in occlusion and their approximate location through the behaviors of observed vehicles.

\section{Conclusion}
This paper presents our approach to the social occlusion inference task. We propose to use vectors to represent the agent trajectories, scene context, and occlusion. With this representation, we design a hierarchical transformer network to infer the OGM, where the polyline encoder aggregates vector-level features to generate polyline-level features, and an interaction-aware transformer encoder models the higher-order interactions between polylines. Occlusion queries are proposed to fuse the polyline features through decoder and generate the inference result. Our model is trained with a loss function containing three parts to focus on the occlusion. Experiments show that our vector-based approach outperforms the image-based baseline and the state-of-the-art results. In the future, OGMs with a smaller grid cell resolution may be used to represent the occupancy. We also consider introduce multi-modal input to achieve better peroformance. 

\newpage

\bibliographystyle{IEEEtran}
\bibliography{mylib}


\end{document}
