\section{Problem formulation: A regret minimization framework}
\label{sec:formulation}

We consider an online sequential learning setting where at each step $t$ the learner observes a batch of size $B_t$ data points $\bz_t = \{z_{t,k}\}_{k=1}^{B_t}$ drawn independently from a distribution $P_t$. The distributions $P_t$ can vary with time and are defined on $\reals^d$. 
The batch loss incurred at step $t$ is $\frac{1}{B_t}\sum_{k=1}^{B_t} \ell(\theta_t,z_{t,k})$ for a function $\ell:\reals^p\times \reals^d\to \reals_{\ge 0}$. 
The learner then updates its model weights $\theta_t \to \theta_{t+1}\in\reals^{p}$.

Define the expected loss as $\ellbar_t(\theta) := \E_{P_t}[\ell(\theta,z_{t,k})]$.
Letting $(\theta_1,\theta_2,\dotsc)$ denote the sequence of learned models, the total expected loss up to time $T$ is $\sum_{t=1}^{T} \ellbar_t(\theta_t)$.
The goal of the learner is to minimize the above objective.
For each step $t$, we define an \emph{oracle model} with weights
\begin{equation}
\label{eqn:theta_star}
    \theta^*_t := \argmin_{\theta \in \mathbb{R}^{p}} \ellbar_t(\theta).
\end{equation}
Note that since the distributions $P_t$ can vary with time,
the weights $\theta^*_t$ also shift over time.

Instead of minimizing the total loss, we equivalently work with the \emph{regret} of the learner defined
with respect to the comparator sequence $(\theta^*_1, \theta^*_2,\dotsc)$ as below:
\begin{align}
\label{eqn:regret_def}
    \Reg(T)
    :=
    \sum_{t=1}^{T} \reg_{t}
    ,\quad
    \reg_{t} := \ellbar_t(\theta_t) - \ellbar_t(\theta_t^*).
\end{align}
Note that $\Reg(t)$ and $\reg_{t}$ are random variables
that depend on $\theta_t$. This framework can be seen
as a game between nature, who chooses the distributions $P_t$
(and thus the sequence of oracle models~$\theta^*_t$),
and the learner, who must choose
the sequence of models $\theta_t$ for $t\ge 1$.

The learner updates its weights using projected mini-batch stochastic gradient descent (mini-batch SGD) given by
\begin{align}\label{eq:BSGD}
    \theta_{t+1} &= \Pi_{\Theta}\left(\theta_t  - \eta_t \nabla\ell^B_t(\theta_t)\right) \\
    \nabla\ell^B_t(\theta_t) &:=
    \frac{1}{B_t} \sum_{k=1}^{B_t} \nabla\ell(\theta_t,z_{t,k})\,,
\end{align}
where $\nabla\ell(\theta_k,z_k)$ are stochastic gradients, $\Theta$ is a bounded convex set, and $\Pi_{\Theta}$ is the projection onto the admissible weight set $\Theta \subseteq \mathbb{R}^{p}$.
Observe that $\E[\nabla\ell(\theta_k,z_k)] = \nabla \ellbar(\theta_k)$, and therefore the sample average gradient above is an unbiased estimate of the gradient of the expected loss.

Nature is allowed to be \emph{adaptive} in that she can set $\theta^*_t$ after observing the
history of the data
\begin{align}\label{eq:history}
 \bz_{[t-1]} := \{ (z_{k,1}, z_{k,2}, \dots, z_{k,B_k})  : 1 \le k \le t-1\}.
\end{align}
The step sizes $\eta_t$, called the \emph{learning rate schedule},
can also change over time in an adaptive manner, i.e.,
the learning rate $\eta_t$ is a function of $\bz_{[t-1]}$.
Note that by the SGD update, $\theta_t$ is a function of $\bz_{[t-1]}$, and so $\theta^*_t$ can depend on the previously learned models $\theta_s$
for $s < t$.
The learning rate schedule controls how the step size changes across iterations.

\begin{definition}[Distribution shift]
\label{def:shift}
Recall the definition of oracle models $\theta_{t}^*$ in \eqref{eqn:theta_star}.
We quantify the \emph{distribution shift} (variation of $P_t$ over time) in terms of the variation in oracle models, namely
\begin{equation}
\label{eqn:gamma_t_def}
    \gamma_t: = \|\theta^*_t - \theta^*_{t+1}\|_{2}\,.
\end{equation}
\end{definition}

\noindent
This is similar to the notion of path variation introduced in~\citet{yang2016tracking}, except that path variation considers the total variation in the minimizers of the sequence of loss functions, whereas we focus on individual changes after each gradient update.
