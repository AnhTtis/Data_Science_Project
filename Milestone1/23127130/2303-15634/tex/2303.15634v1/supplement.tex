% \pagestyle{empty}
\section{Proof of theorems and technical lemmas for linear regression}\label{sec:proofs}
\subsection{Proof of Proposition~\ref{pro:approx}}\label{proof:approx}
The integration form of the stochastic differential equation~\eqref{eq:SDE} reads as
\begin{align}\label{eq:X-proc}
X(\tau) = X_0+ Y_0- \int_0^\tau \zeta(s)X(s)\de s -Y(\tau)+ \int_0^\tau \frac{\zeta(s)}{\sqrt{\nu(s)}} D_s^{1/2}\de W(s)\,,
\end{align}
where $D_s = ((\|X(s)\|^2+\sigma^2)I + X(s)X(s)^\sT)$.
We start by proving some useful bounds on the solution of $X(\tau)$ process.
\begin{lemma}\label{lem:approx}
Consider the process $X(\tau)$ given by \eqref{eq:X-proc} with initialization $X_0$ satisfying $\|X_0\|\le K$. Under Assumptions {\sf A1-A2}, with probability at least $1-e^{-u^2}$ we have
\begin{align}\label{eq:XtauB}
\sup_{\tau\in[0,T]}\|X(\tau)\| \le C\sqrt{T}(\sqrt{d}+u)\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big]\,.
\end{align}
and 
\begin{align}\label{eq:DXtauB}
\sup_{t\in [0,T/\eps]\cap\mathbb{Z}_{\ge 0}} \sup_{u\in[0,\eps]} \|X(t\eps+u) - X(t\eps)\|\le C'\sqrt{T\eps}(\sqrt{d}+u)^2\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big]\,,
\end{align}
for any fixed $u>0$, and constants $C= C(K,\sigma)$,$C' = C'(K,\sigma,\Gamma)$. 
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:approx}]
Define $V(\tau): = \int_0^\tau \frac{\zeta(s)}{\sqrt{\nu(s)}} D_s^{1/2}\de W(s)$. We have 
$${\rm Cov}(V(\tau)) = \int_0^\tau \frac{\zeta(s)^2}{\nu(s)} D_s \de s\,,$$
and so 
\begin{align}
\|{\rm Cov}(V(\tau))\|_{\rm op}\le K^2 \int_0^\tau\|D_s\|_{\rm op}\de s
\le
A_\tau:=K^2 \int_0^\tau (2\|X_s\|^2+\sigma^2)\de s\,.
\end{align}

Note that $\exp{\alpha \|V(\tau)\|^2}$ is a submartingale, and by virtue of Doob’s martingale inequality, we have
\[
\prob\Big(\sup_{\tau\le T}\|V(\tau)\| \ge \lambda\Big)
\le \E[\exp\{\alpha\|V(T)\|/2\}] \exp\{-\alpha \lambda^2/2\}\le (1-A_T\alpha)^{-d/2} \exp\{-\alpha \lambda^2/2\}\,.
\]
Take $\alpha = 1/(2A_T)$ and $\lambda = 2\sqrt{A_T}(\sqrt{d}+u)$ to obtain
\begin{align}\label{eq:VB}
    \prob\left(\sup_{\tau\le T}\|V(\tau)\| \ge 2\sqrt{A_T}(\sqrt{d}+u) \right)
\le 2^{d/2}\exp(-(\sqrt{d}+u)^2) \le e^{-u^2}\,.
\end{align}
Using \eqref{eq:X-proc} and recalling Assumptions {\sf A1-A2}, we get
\begin{align*}
    \|X(\tau)\| &\le \|X_0\|+ \|Y_0\|+ \|Y_\tau\| + \int_0^{\tau} \zeta(s) \|X(s)\|\de s + \|V(\tau)\|\\
    &\le 3K + \int_0^{\tau} K \|X(s)\|\de s + \|V(\tau)\|\,.
\end{align*}
We next use the inequality $(a+b+c)^2\le 3(a^2+b^2+c^2)$ to get
\begin{align*}
    \|X(\tau)\|^2 &\le 27K^2 + 3K^2\Big(\int_0^{\tau}  \|X(s)\|\de s\Big)^2 + 3\|V(\tau)\|^2\\
    &\le 27K^2 + 3K^2 \tau \int_0^{\tau} \|X(s)\|^2\de s + 3\|V(\tau)\|^2\,,
\end{align*}
where in the second line we used Cauchy--Shwarz inequality. Define $\Delta_T = \sup_{\tau\le T}\|X(\tau)\|^2$.
Taking the supremum over $\tau\le T$ of both sides of the previous inequality and using the bound \eqref{eq:VB}, we arrive at
\begin{align*}
\Delta_T&\le 27K^2 + 3K^2T \int_0^{T} \Delta_s\de s + 12A_T(\sqrt{d}+u)^2\\
&\le 27K^2 + 3K^2T \int_0^{T} \Delta_s\de s + 12A_T(\sqrt{d}+u)^2\\
&\le 27K^2 + 3K^2T \int_0^{T} \Delta_s\de s + 12(2K^2\int_0^T\Delta_s \de s+\sigma^2TK^2)(\sqrt{d}+u)^2\\
&= 27K^2+ 12\sigma^2TK^2(\sqrt{d}+u)^2+
(3T+24(\sqrt{d}+u)^2)K^2 \int_0^T \Delta_s \de s
\end{align*}

Using Gronwall’s inequality, the above relation implies that
\[
\Delta_T\le K^2 (27+ 12\sigma^2T(\sqrt{d}+u)^2)
\exp((3T+24(\sqrt{d}+u)^2)K^2T)
\]
Taking square root of both sides and using $\sqrt{a+b}\le \sqrt{a}+\sqrt{b}$, we get
\[
\sup_{\tau\le T}\|X(\tau)\|\le K (\sqrt{27}+ \sqrt{12T}\sigma(\sqrt{d}+u)) \exp((3T+24(\sqrt{d}+u)^2)K^2T/2)\,,
\]
which completes the proof of~\eqref{eq:XtauB}.

We next proceed with proving~\eqref{eq:DXtauB}.
Define $\tilde{\Delta}(t,\eps) = \sup_{h\in[0,\eps]}\|X(t\eps+h)- X(t\eps)\|$.
Using \eqref{eq:X-proc}, we have
\begin{align}
\tilde{\Delta}(t,\eps) &\le \sup_{h\in[0,\eps]}\left\{ \Big\|\int_{t\eps}^{t\eps+h} \zeta(s) X(s)\de s\Big\|
+ \|Y(t\eps+h)-Y(t\eps)\| + \Big\|\int_{t\eps}^{t\eps+h} \frac{\zeta(s)}{\sqrt{\nu(s)}} D_s^{1/2} \de W(s)\Big\|\right\}\nonumber\\
 &\le K \eps\sup_{s\le T} \|X(s)\| + \sup_{h\in[0,\eps], \tau\in[t\eps, t\eps+h]} \|Y'(\tau)\| h+ \sup_{h\in[0,\eps]} \|V(t,h,\eps)\|\nonumber\\
 &\le K \eps\sup_{s\le T} \|X(s)\| + \Gamma+ \sup_{h\in[0,\eps]} \|V(t,h,\eps)\|
 \,,\label{eq:tD-B}
\end{align}
with $V(t,h,\eps): = \int_{t\eps}^{t\eps+h} \frac{\zeta(s)}{\sqrt{\nu(s)}} D_s^{1/2} \de W(s)$. In the last step, we used Assumption {\sf A2}, by which 
$\|Y'(\tau)\|\le \Gamma/\eps$.
Similar to the derivation of \eqref{eq:VB}, we have
\[
\prob\left(\sup_{t\in[0,T/\eps]\cap \mathbb{Z}_{\ge 0}} \sup_{h\in[0,\eps]}\|V(t,h,\eps)\| \ge 2\sqrt{B_\eps}(\sqrt{d}+u) \right)
\le 2^{d/2}\exp(-(\sqrt{d}+u)^2) \le e^{-u^2}\,,
\]
with $B_\eps:= \sup_{h\le \eps} K^2 \int_{t\eps}^{t\eps+h} (2\|X(s)\|^2+\sigma^2)\de s$. Plugging in~\eqref{eq:tD-B}, we have
\begin{align}
  \tilde{\Delta}(t,\eps) &\le  K \eps\sup_{s\le T} \|X(s)\| + 2 K\sqrt{(2\sup_{s\le T}\|X(s)\|^2+\sigma^2)\eps}\; (\sqrt{d}+u)\nonumber\\
  &= \Gamma+ K\left(\eps+2\sqrt{2\eps}(\sqrt{d}+u)\right) (\sup_{s\le T} \|X(s)\|) + 2K\sigma\sqrt{\eps} (\sqrt{d}+u)\nonumber\\
  &\le C'\sqrt{T\eps}(\sqrt{d}+u)^2\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big]\,.
\end{align}
This concludes the proof of Equation~\eqref{eq:DXtauB}.

We next rewrite the stochastic gradient descent update as follows:
 \begin{align}
     \theta_{t+1} &= \theta_t - \eta_t \frac{1}{B_t}\sum_{k=1}^{B_t}\nabla\ell(\theta_t,z_{k,t})\nonumber\\
     &= \theta_t + \eta_t \frac{1}{B_t}\sum_{k=1}^{B_t} (y_{tk}-\<x_{tk},\theta\>) x_{tk}\nonumber\\
     %&= \theta_t -\eta_t \nabla\ellbar_t(\theta_t)-
     %\eta_t \left(\frac{1}{B_t}\sum_{k=1}^{B_t}\nabla\ell(\theta_t,z_{k,t}) - \nabla\ellbar_t(\theta_t)\right)\nonumber\\
      &= \theta_t +\eta_t (\theta^*_t - \theta_t) +\frac{\eta_t}{B_t}\sum_{k=1}^{B_t} \left((x_{tk}x_{tk}^\sT - I)(\theta^*_t - \theta_t)+\eps_{tk} x_{tk}\right)\nonumber\\
      & =\theta_t +\eta_t (\theta^*_t - \theta_t) -\eta_t\xi_t \,, \label{eq:zeta}
 \end{align}
 where the noise term $\xi_t$ has mean zero, given that the data points $z_{t,k}$ are sampled independently at each step $t$. 

Note that   
 $\xi_t$ in~\eqref{eq:zeta} is the average of $B_t$ zero mean variables and thus can be approximated by a normal distribution with covariance $(1/B_t) D_t$, with
 \begin{align}
 D_t &= \left\{\E\left[(x_{tk} x_{tk}^\sT - I)(\theta^*_t - \theta_t)(\theta^*_t - \theta_t)^\sT (x_{tk} x_{tk}^\sT - I)^\sT\right] +\sigma^2 I\right\}\nonumber\\
 &= \left((\|\theta^*_t-\theta_t\|^2+\sigma^2)I +  (\theta^*_t-\theta_t)(\theta^*_t-\theta_t)^\sT \right)\,,
 \end{align} 
 where the above identity follows from Lemma~\ref{lem:stein}. We let $\xi_t = -D_t^{1/2}g_t$ with $g_t\sim\normal(0,I_d)$. Iterating update \eqref{eq:zeta} recursively, we have
 \begin{align}
     \theta_t - \theta^*_t &= \theta_0 - \theta^*_t  +\sum_{\ell=0}^{t-1}  \eta_\ell(\theta^*_\ell-\theta_\ell)+ \sum_{\ell=0}^{t-1} \frac{\eta_\ell}{\sqrt{B_\ell}} D_{\ell}^{1/2}g_\ell\nonumber\\
     &=\theta_0 -\theta^*_t +\eps\sum_{\ell=0}^{t-1}  \zeta(\ell \eps)(\theta^*_\ell-\theta_\ell)+ \int_0^{t\eps} \frac{s\zeta([s])}{\sqrt{s\nu([s])}} D_{[s]}^{1/2} \frac{\de W(s)}{\sqrt{s}}\nonumber\\
     &=\theta_0  - Y(\eps t)  +\eps\sum_{\ell=0}^{t-1}  \zeta(\ell \eps)(\theta^*_\ell-\theta_\ell)+ \int_0^{t\eps} \frac{\zeta([s])}{\sqrt{\nu([s])}} D_{[s]}^{1/2} \de W(s)\,,\label{eq:SGD-full}
 \end{align}
 where we adopt the notation $[s] = \eps \lfloor s/\eps\rfloor$, and $W(s)$ represents the standard Brownian motion.
 
 We take the difference of \eqref{eq:X-proc} and \eqref{eq:SGD-full}.
 Since $\theta_0 = \theta_0 - \theta^*_0 + \theta^*_0 = X_0 + Y_0$, for $\tau\in \mathbb{Z}_{\ge 0}\eps\cap[0,T]$, we have:
 \begin{align}
     \|(\theta_{\tau/\eps}- \theta^*) - X(\tau)\|
     &\le \left\|\eps\sum_{\ell=0}^{\tau/\eps-1}  \zeta(\ell \eps)(\theta_\ell- \theta^*_\ell) - \int_0^\tau \zeta(s)X(s)\de s\right\| \notag\\
     &\hspace{0.45cm}+ \left\|\int_0^\tau \Big(\frac{\zeta([s])}{\sqrt{\nu([s])}} - \frac{\zeta(s)}{\sqrt{\nu(s)}} \Big)\de W(s) \right\|.\label{eq:app1}
 \end{align}
 We first treat the first term. We have
 \begin{align*}
     &\eps\sum_{\ell=0}^{t-1}  \zeta(\ell \eps)(\theta_\ell- \theta^*_\ell) - \int_0^\tau \zeta(s)X(s)\de s \nonumber\\
     &= \int_0^{\tau} \zeta([s]) (\theta_{\lfloor s/\eps\rfloor} - \theta^*_{\lfloor s/\eps\rfloor}) - \int_0^\tau \zeta(s)X(s)\de s\\
     &=\int_0^{\tau} \zeta([s]) \Big(\theta_{\lfloor s/\eps\rfloor} - \theta^*_{\lfloor s/\eps\rfloor} -X([s])\Big) \de s + \int_0^\tau \zeta([s]) (X([s]) - X(s))\de s
     + \int_0^\tau (\zeta([s])-\zeta(s)) X(s)\de s
 \end{align*}
 We have
 \begin{align}\label{eq:aux1}
    \left\| \int_0^\tau \zeta([s]) (X([s]) - X(s))\de s\right\|
     \le K\tau \sup_{t\in [0,T/\eps]\cap\mathbb{Z}_{\ge 0}} \sup_{h\in[0,\eps]} \|X(t\eps+h) - X(t\eps)\|\,.
 \end{align}
 Also,
 \begin{align}\label{eq:aux2}
   \left\| \int_0^\tau (\zeta([s])-\zeta(s)) X(s)\de s\right\|
   \le K\eps \tau \sup_{\tau\in[0,T]}\|X(\tau)\|\,.
 \end{align}
 Note that the right-hand side of~\eqref{eq:aux1} and \eqref{eq:aux2} are bounded in Lemma~\ref{lem:approx}.
 
 We next bound the second term on the right-hand side of~\eqref{eq:app1}.
 Define
 \[
 E(\tau): = \int_0^\tau \Big(\frac{\zeta([s])}{\sqrt{\nu([s])}} - \frac{\zeta(s)}{\sqrt{\nu(s)}} \Big)\de W(s).
 \]
 Note that $E(\tau)~\sim\normal(0,\alpha^2 I_d)$,
 where
 \[
 \alpha^2 = \int_0^\tau \Big(\frac{\zeta([s])}{\sqrt{\nu([s])}} - \frac{\zeta(s)}{\sqrt{\nu(s)}} \Big)^2\de s\le K ^2\eps \tau\,,
 \]
 using Assumption {\sf A2} by which $\|\zeta/\sqrt{v}\|_{\rm Lip}\le K$. By applying Doob's inequality to the martingale $\exp(\frac{1}{2\tau}\|E(\tau)\|)$, similar to derivation of \eqref{eq:VB}, we obtain
 \begin{align}\label{eq:E}
     \prob\left(\sup_{\tau\le T} \|E(\tau)\|\ge 2K \sqrt{\eps T}(\sqrt{d}+u)\right)\le e^{-u^2/2}\,.
 \end{align}

\noindent 
Now we define 
 $$\Delta(\tau) := \sup_{t\in[0,\tau/\eps]\cap\mathbb{Z}_{\ge 0}}\|X(t\eps) - (\theta_t-\theta^*_t)\|\,.$$
 Using Lemma~\ref{lem:approx} to bound \eqref{eq:aux1} and \eqref{eq:aux2} and then combining that with \eqref{eq:E} into \eqref{eq:app1} we arrive at
 \begin{align}
     \Delta(\tau) &\le K\int_0^\tau \Delta(s)\de s+ K\tau C'\sqrt{T\eps}(\sqrt{d}+u)^2\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big]\nonumber\\
     &\hspace{0.45cm}+ K\eps \tau C\sqrt{T}(\sqrt{d}+u)\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big] + 2K\sqrt{\eps T}(\sqrt{d}+u)\nonumber\\
     &\le K\int_0^\tau \Delta(s)\de s
     +C'' T^{3/2} \sqrt{\eps}(\sqrt{d}+u)^2\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big].
 \end{align}
 Using Gronwall's inequality we obtain
 \begin{align}
     \Delta(T)\le C'' T^{3/2} \sqrt{\eps}(\sqrt{d}+u)^2\exp\Big[C\Big(T^2+(\sqrt{d}+u)^2T\Big)+KT\Big]\,.
 \end{align}
 We derive the final claim by noting that
 \begin{align}
     \sup_{t\in[0,\tau/\eps]\cap\mathbb{Z}_{\ge 0}}\Big|\|X(t\eps)\|^2 - \|\theta_t-\theta^*_t\|^2\Big|
     &\le \Delta(T)^2+ 2\Delta(T) \sup_{t\in[0,\tau/\eps]\cap\mathbb{Z}_{\ge 0}} \|X(t\eps)\|\nonumber\\
     &\le C_1 \sqrt{\eps}(\sqrt{d}+u)^4 T^3 \exp\Big[C_2\Big(T^2+(\sqrt{d}+u)^2T\Big)\Big]\,,
 \end{align}
 for some constants $C_1, C_2$, depending on $K,\sigma, \Gamma$. This completes the proof. 
\end{proof}
 
%===========================================
\subsection{Proof of Theorem~\ref{pro:SDE}}\label{sec:Ito-SDE}

Recall the SDE for process $X(\tau)$ given by
\[
\de X(\tau)=-(\zeta(\tau) X(\tau) +Y'(\tau))\de\tau+ \nonumber\\
     \frac{\zeta(\tau)}{\sqrt{\nu(\tau)}} \left((\|X(\tau)\|^2+\sigma^2)I + X(\tau)X(\tau)^\sT\right)^{1/2}\; \de W(\tau)\,,
\]
Let $m_\tau:=\E[X(\tau)]$. Taking expectation of the above SDE, we obtain
\[
 m_\tau'= - \zeta(\tau) m_\tau - Y'(\tau)\,.
\]


Next we define the stochastic process $Z(\tau) = \|X(\tau)\|^2$. By Ito's lemma (cf. Lemma~\ref{lem:Ito}), we have
\begin{align*}
\de Z(\tau) =& \left(-2\zeta(\tau)\|X(\tau)\|^2 - 2X(\tau)^\sT Y'(\tau) + \frac{\zeta(\tau)^2}{\nu(\tau)} \left((d+1) \|X(\tau)\|^2+d \sigma^2\right)\right) \de\tau \nonumber\\
&+ 2 \frac{\zeta(\tau)}{\sqrt{\nu(\tau)}} X(\tau)^\sT\left(( \|X(\tau)\|^2+\sigma^2)I+ X(\tau)X(\tau)^\sT\right)^{1/2}\; \de W(\tau)\,.
\end{align*}
Taking expectation of both sides, we arrive at the following ODE for $v_\tau = \E[Z(\tau)] = \E[\|X(\tau)\|^2]$:
\begin{align}
    v'_\tau &= -2\zeta(\tau)v_\tau - 2m_\tau^\sT  Y'(\tau)
    +\frac{\zeta(\tau)^2}{\nu(\tau)} ((d+1)v_\tau+ d\sigma^2)\nonumber\\
    &=\left((d+1)\frac{\zeta(\tau)^2}{\nu(\tau)} - 2\zeta(\tau)\right)v_\tau + \frac{\zeta(\tau)^2}{\nu(\tau)}\sigma^2 d- 2m_\tau^\sT Y'(\tau)\,.
\end{align}
%==========================
\subsection{Proof of Theorem \ref{thm:optimal-policy}}

We start by giving a brief overview of the Hamilton--Jacobi--Bellman (HJB) equation~\cite{bellman1956dynamic}. 

Consider the following value function:
\begin{align}
    V(z(\tau_0),\tau_0) = \min_{\zeta:[\tau_0,T]\to \mathcal{A}} \int_{\tau_0}^T C(z(\tau),\zeta(\tau))\de \tau + D(z(T))\,,
\end{align}
where $z(\tau)$ is the vector of the system state, $\zeta(\tau)$, for $\tau\in[\tau_0,T]$ is the control policy we aim to optimize over and takes value in a set $\mathcal{A}$,   $C(\cdot)$ is the scalar cost function and $D(\cdot)$ gives the bequest value at the final state $z(T)$.

Suppose that the system is also subject to the constraint
\begin{align}\label{eq:HJB-con}
\frac{\de}{\de \tau}{z}(\tau) = \Phi(z(\tau),\zeta(\tau))\,, \quad \forall \tau\in[\tau_0,T]\,,
\end{align}
with $\Phi$ describing the evolution of the system state over time. The dynamic programming principle allows us to derive a recursion on the value function $V$, in the form of a partial differential equation (PDE).
Namely, the the Hamilton--Jacobi--Bellman PDE is given by
\begin{align}
\partial_\tau V(z,\tau) + \min_{\zeta\in \mathcal{A}} \left[ \partial_z V(z,\tau)\cdot  \Phi(z,\zeta)  + C(z,\zeta) \right] = 0\,,\label{eq:HJB}\\
\text{subject to}\quad V(z,T) = D(z)\,.\nonumber
\end{align}
The above PDE can be solved backward in time and then the optimal control $\zeta^*(\tau)$ is given by 
\begin{align}\label{eq:zeta*}
\zeta^*(\tau) = \arg\min_{\zeta\in \mathcal{A}} \left[ \partial_z V(z(\tau),\tau) \cdot \Phi(z(\tau),\zeta)  + C(z(\tau),\zeta) \right]\,.
\end{align}

We are now ready to prove the claim of Theorem \ref{thm:optimal-policy}, using the HJB equation.  

Consider $\tilde{v}_\tau$ as the system state at time $\tau$ ( i.e., $z(\tau) = \tilde{v}_\tau$), and the cost function $C(\tilde{v}_\tau, \zeta(\tau)) = \tilde{v}_\tau$. Also set $D(\cdot)$ to be the zero everywhere. The control variable $\zeta(\tau)$ takes values in $\mathcal{A} = [0,1]$.

 
The function $\Phi(\cdot,\cdot)$ in~\eqref{eq:HJB-con} is given by~\eqref{eq:tv}, which we recall here:

\[
\Phi(\tilde{v}_\tau,\zeta): = \Big((d+1)\frac{\zeta^2 }{\nu(\tau)}-2\zeta\Big) \tilde{v}_\tau 
     + \frac{\zeta^2}{\nu(\tau)}\sigma^2 d + 2\|Y'(\tau)\|\sqrt{\tilde{v}_\tau}\,.
\]

Note that in our case, the cost function $C$ does not depend on $\zeta(\tau)$. Also, it is easy to see that $\partial_z V(\tilde{v}_\tau,\tau)>0$ because larger $\tilde{v}_\tau$ means we are further from the sequence of models and so the minimum cost achievable in tracking the sequence of models will be higher. Therefore, \eqref{eq:zeta*} reduces to
\[
\zeta^*(\tau) = \arg\min_{\zeta\in[0,1]} \Phi(\tilde{v}_\tau,\zeta)
\]
Since $\Phi$ is quadratic in $\zeta$, solution to the above optimization has a closed form given by
\[
\zeta^*(\tau) = \min\left\{1, \left(\frac{d+1}{\nu(\tau)}\tilde{v}_\tau + \frac{\sigma^2d}{\nu(\tau)}\right)^{-1} \tilde{v}_\tau\right\}\,,
\]
which completes the proof.

%==============================
\subsection{Proof of Lemma~\ref{lem:no-shift-Lin}}

Substituting for $\zeta(\tau)$ from~\eqref{eq:policy}, it is easy to verify that $\tilde{v}'(\tau)\le0$ and so $\tilde{v}(\tau)$ is decreasing in $\tau$.

Define the shorthand ${\sf a}: = (d+1)/\nu(\tau)$ and ${\sf b}:= \sigma^2d/\nu(\tau)$.
Note that if $\tilde{v}_\tau\ge {\sf b}/(1-{\sf a})$,
% \Big(\frac{d+1}{\nu(\tau)} -1\Big) \tilde{v}(\tau) + \frac{\zeta(\tau)^2}{\nu(\tau)}\sigma^2 d\le 0,
then by \eqref{eq:policy}, $\zeta(\tau) = 1$ and in this case ODE~\eqref{eq:tv} reduces to $\tilde{v}_\tau' = ({\sf a}-2)\tilde{v}_\tau+{\sf b}$, with the solution
\[
\tilde{v}_\tau = \left(\tilde{v}_0 + \frac{{\sf b}}{{\sf a}-2}\right) e^{({\sf a}-2)\tau}-\frac{{\sf b}}{{\sf a}-2}\,.
\]
However, the above solution is valid until $\tilde{v}_\tau\ge {\sf b}/(1-{\sf a})$, which is the assumption we started with, which using the above characterization is equivalent to 
\[
\tau\le \tau_*:= \left[\frac{1}{2-{\sf a}}\log\left((1-{\sf a}) \left(\tilde{v}_0 \frac{2-{\sf a}}{{\sf b}} - 1\right)\right)\right]_+\,.
\]
For $\tau>\tau_*$, we have $\tilde{v}_{\tau}\le {\sf b}/(1-{\sf a})$ and so $\zeta(\tau) = \tilde{v}_\tau/({\sf a}\tilde{v}_\tau + {\sf b})$ by~\eqref{eq:policy}. In this case, ODE~\eqref{eq:tv} reduces to
\[
\tilde{v}'_\tau = -\frac{\tilde{v}_{\tau}^2}{{\sf a}\tilde{v}_{\tau}+{\sf b}}\,.
\]
By rearranging the terms and integrating, the solution to above ODE satisfies
\begin{align}\label{eq:ODE2}
{\sf a} \ln\Big(\frac{1}{\tilde{v}_\tau}\Big) + \frac{{\sf b}}{\tilde{v}_{\tau}} = \tau+ C\,,
\end{align}
where $C$ can be obtained by the continuity condition of $\tilde{v}_{\tau}$ at $\tau_*$, i.e.,
\[
C = {\sf a}\ln\Big(\frac{1-{\sf a}}{{\sf b}}\Big) +1-{\sf a} -\tau_*\,.
\]
From~\eqref{eq:ODE2} we observe that as $\tau\to \infty$, $\tilde{v}_\tau\to 0$ and the term ${\sf b}/\tilde{v}_\tau$ becomes dominant by which we obtain
\[
\lim_{\tau\to \infty} \frac{\tilde{v}_\tau}{\frac{{\sf b}}{\tau+C}} = 1\,.
\]
In addition, invoking definition of optimal policy $\zeta(\tau)$, we obtain
\[
\lim_{\tau\to \infty} \frac{\zeta(\tau)}{\frac{1}{{\sf a}+C+\tau}} = 1\,,
\]
which completes the proof.


%=============================
\section{Proof of theorems and technical lemmas for convex loss}
\subsection{Proof of Theorem~\ref{thm:convex-reg}}
We define the shorthand $D_t^2 = \|\theta^*_t - \theta_t\|^2$ and let $v_t = \theta^*_t - \theta^*_{t+1}$ be shifts in the optimal models. We also define the shorthand 
\[
\nabla\ell^B_t(\theta_t):=
\frac{1}{B_t} \sum_{k=1}^{B_t} \nabla\ell(\theta_t,z_{t,k})\,.
\]
Since projection on a convex set is contraction, we have
\[
\|\Pi_{\Theta}(u) - w\|\le \|u - w\|\,,
\]
for any $w\in \Theta$. Using this property, we have
\begin{align*}
D_{t+1}^2 &= \|\Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t)) - \theta^*_{t+1} \|^2\\
&=\|\Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t+ \theta^*_t - \theta^*_{t+1}\|^2\\
&= \|\Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\|^2+\|v_t\|^2
+2\<v_t, \Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\> \\
&\le \|\theta_t-\eta_t \nabla\ell^B_t(\theta_t)-\theta^*_t\|^2+\|v_t\|^2
+2\<v_t, \Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\\
&= D_t^2 - 2\eta_t\<\nabla \ell^B_t(\theta_t), \theta_t - \theta_t^*\> +\|v_t\|^2
+2\<v_t, \Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\> +\eta_t^2\|\nabla \ell^B_t(\theta_t)\|^2.
\end{align*}


% \begin{align*}
% D_{k+1} &= \|\theta_k -\theta_k^*-\eta_k \nabla\ell(\theta_k,z_k)+\theta^*_k - \theta^*_{k+1}\|^2\\
% &= D_k - 2\eta_k\<\nabla \ell(\theta_k,z_k), \theta_k - \theta_k^*\> +\|v_k\|^2
% +\<v_k, \theta_k -\theta_k^*-\eta_k \nabla\ell(\theta_k,z_k)\> +\eta_k^2\|\nabla \ell(\theta_k,z_k)\|^2
% \end{align*}
Define
\[
\delta_t: = \nabla\ell^B_t(\theta_t) -\nabla \ellbar_t(\theta_t)\,,
\]
as the difference between the gradient of the expected loss (at step $t$) and the gradient of the batch average loss at that step.

Writing the above bound in terms of this notation, we get
\begin{align}
D_{t+1}^2 
&\le D_t^2 - 2\eta_t\<\nabla \ellbar_t(\theta_t)+\delta_t, \theta_t - \theta_t^*\> +\|v_t\|^2
+2\<v_t, \Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\nonumber\\ &\quad +\eta_t^2\Big(\|\nabla \ellbar_t(\theta_t)\|^2+ \|\delta_t\|^2+2\<\delta_t,\nabla\ellbar_t(\theta_t)\> \Big)\,.\label{eqn:common}
\end{align}

By \citet[Lemma 4]{zhou2018fenchel} for any $L$-smooth convex function $f$, we have
\begin{align}\label{eq:conv-prop1}
\frac{1}{L} \|\nabla f(y)-\nabla f(x)\|^2\le \<\nabla f(y)-\nabla f(x), y-x\>\,.
\end{align}


Since the loss function $\ell(\theta,z)$ is convex, the expected loss functions $\ellbar_t(\theta)$ are also convex for $t=1,\dotsc, T$.
Using~\eqref{eq:conv-prop1} together with the fact that $\nabla\ellbar_t(\theta^*_t) = 0$ by optimality of $\theta^*_t$, we get
\begin{equation}\label{eqn:cvx1}
\frac{1}{L}\|\nabla\ellbar_t(\theta_t)\|^2\le \<\nabla \ellbar_t(\theta_t), \theta_t - \theta_t^*\>\,.
\end{equation}

Using the above bound, we obtain
\begin{align*}
D_{t+1}^2 
&\le D_t^2 - (2\eta_t-L\eta_t^2)\<\nabla \ellbar_t(\theta_t), \theta_t - \theta_t^*\> +\|v_t\|^2
+2\<v_t, \Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\\ &\quad +\eta_t^2\|\delta_t\|^2- 2\eta_t \<\delta_t,\theta_t - \theta_t^*-\eta_t\nabla \ellbar_t(\theta_t)\> 
\end{align*}
Recall our assumption $\eta_t\le 2/L$.
Using the convexity of $\ellbar_k$, we have 
\begin{equation}\label{eqn:cvx2}
\ellbar_t(\theta_t)-\ellbar_t(\theta^*_t)\le \<\nabla \ellbar_t(\theta_t), \theta_t - \theta^*_t\>,
\end{equation}
which along with the above bound implies that
\begin{align*}
D_{t+1}^2 
&\le D_t^2 - (2\eta_t-L\eta_t^2)(\ellbar_t(\theta_t)-\ellbar_t(\theta^*_t)) +\|v_t\|^2
+2\<v_t, \Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\\ &\quad +\eta_t^2\|\delta_t\|^2- 2\eta_t \<\delta_t,\theta_t - \theta_t^*-\eta_t\nabla \ellbar_t(\theta_t)\> \,.
\end{align*}

 Note that $\Pi_{\Theta}(\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t = \theta_{t+1}-\theta^*_t$.
% Since $\theta_{t+1}, \theta^*_t\in \Theta$, we have $\|\theta_{t+1}-\theta^*_t\|\le D_{\max}$. Hence,
% \begin{align}\label{eqn:cvx3}
% D_{t+1}^2 
% &\le D_t^2 - (2\eta_t-L\eta_t^2)(\ellbar_t(\theta_t)-\ellbar_t(\theta^*_t)) +\gamma_t^2
% +2D_{\max}\gamma_t \nonumber\\ 
% &\quad +\eta_t^2\|\delta_t\|^2- 2\eta_t \<\delta_t,\theta_t - \theta_t^*-\eta_t\nabla \ellbar_t(\theta_t)\>\,. 
% \end{align}
We let $a_t: = 2\eta_t - L\eta_t^2 > 0$, and by rearranging the terms in the above equation we obtain
\begin{align}
    \ellbar_t(\theta_t)-\ellbar_t(\theta^*_t)
    \le \frac{D_t^2}{a_t} - \frac{D_{t+1}^2}{a_t}+
    \frac{\|v_t\|^2}{a_t}+\frac{2}{a_t}\<v_t, \theta_{t+1}-\theta^*_t\>+\frac{\eta_t^2\|\delta_t\|^2}{a_t} - \frac{2\eta_t}{a_t}\<\delta_t,\theta_t - \theta_t^*-\eta_t\nabla \ellbar_t(\theta_t)\>\,.\label{eq:Di-B}
\end{align}

We next note that $\theta_t, \theta^*_t, \eta_t$ are adapted to the filtration $\bz_{[t-1]}$, and therefore,
\[
\E[\<\delta_t,\theta_t - \theta^*_t-\eta_t\nabla\ellbar_t(\theta_t)\>|\bz_{[t-1]}] = \<\E[\delta_t|\bz_{[t-1]}],\theta_t - \theta^*_t-\eta_t\nabla \ellbar_t(\theta_t)\>= 0\,.
\]
%In addition by our assumption $\E[\|\delta_k\|^2]\le \sigma^2$.
Taking iterated expectations of both sides of \eqref{eq:Di-B} with respect to filtration $\bz_t$ (first conditional on $\bz_{[t-1]}$ and then with respect to $\bz_{[t-1]}$), we get
%
\begin{align}
\E[\reg_t] 
\le \E\left[\frac{D_t^2-D_{t+1}^2}{a_t} 
+ \frac{\sigma^2}{B_t}\frac{\eta_t^2}{a_t} + \frac{\|v_t\|^2}{a_t} + \frac{2}{a_t}\<v_t, \theta_{t+1}-\theta^*_t\>\right]\,,\label{eq:Di-B2}
\end{align}
with $\reg_t = \ellbar_t(\theta_t) - \ellbar_t(\theta^*_t)$.
Summing both sides over $t =1,\dotsc, T$, we obtain the desired result.
% \begin{align}\label{eqn:same-ub}
% &\E[\Reg(T)]= \sum_{t=1}^T \E[\reg_t]\nonumber\\
% &\le \sum_{t=1}^T\E\left[ \left(\frac{D_t^2}{a_t} - \frac{D_{t+1}^2}{a_{t}}\right) + \frac{\sigma^2\eta_t^2}{B_ta_t} + \frac{\gamma_t^2}{a_t} + 2D_{\max}\sum_{t=1}^T\frac{\gamma_t}{a_t}\right]\,.
% \end{align}


% Compare Equation~\ref{eqn:same-ub} above for the upper envelope to that for the lower envelope in Equation~\ref{eqn:same-lb}. However, we can further relax and simplify the upper bound slightly to obtain:
% \begin{align*}
% &\E[\Reg(T)]= \sum_{t=1}^T \E[\reg_t]\\
% &\le \E\left[\sum_{t=1}^T \left(\frac{D_t^2}{a_t} - \frac{D_{t+1}^2}{a_{t}}\right) + \sum_{t=1}^T \frac{\sigma^2\eta_t^2}{B_ta_t} + \sum_{t=1}^T\frac{\gamma_t^2}{a_t} + 2D_{\max}\sum_{t=1}^T\frac{\gamma_t}{a_t}\right]\\
%     &= \sum_{t=2}^T \E\left[D_t^2 \left(\frac{1}{a_t} - \frac{1}{a_{t-1}}\right)\right]
%     +\E\left[\frac{D_1^2}{a_1}-\frac{D_{T+1}^2}{a_T}\right] + \sum_{t=1}^T \E\left[\frac{\sigma^2\eta_t^2}{B_ta_t} + \frac{\gamma_t^2}{a_t} + 2D_{\max}\frac{\gamma_t}{a_t}\right]\\
%     &\le D_{\max}^2\sum_{t=2}^T \E\left[ \left(\frac{1}{a_t} - \frac{1}{a_{t-1}}\right)_+\right]
%     +D_{\max}^2\E\left[\frac{1}{a_1}\right]   +\sum_{t=1}^T \E\left[\frac{\sigma^2\eta_t^2}{B_ta_t} + \frac{\gamma_t^2}{a_t} + 2D_{\max}\frac{\gamma_t}{a_t}\right]\,,
% \end{align*}
% where for a scalar $x$, $x_+ = \max(x,0)$ indicates its positive part.

%===================================
\subsection{Proof of Proposition~\ref{propo:optimal-eta}}
Recall the optimization problem for $\eta^*$ given below:
\begin{align}\label{eq:etat*}
    \eta_t^*:= \arg\min_{0\le \eta\le \frac{1}{L}} D_{\max}^2
    \left(\frac{1}{2\eta-L\eta^2} - \frac{1}{2\eta_{t-1}-L\eta_{t-1}^2}\right)_+
    +\frac{\sigma^2}{B_t}\cdot\frac{\eta^2}{2\eta-L\eta^2}+ \frac{\gamma_t^2+2D_{\max}\gamma_t}{2\eta-L\eta^2}\,.
\end{align}
Note that the functions $1/(2\eta - L\eta^2)$ and $\eta^2/(2\eta - L\eta^2)$ are convex for $\eta\le 1/L$. Also the pointwise maximum of convex functions is convex, which implies that the objective function above is convex. With that, we first derive the stationary points of the objective function and then compare them to the boundary points $0$ and $1/L$.

Setting the subgradient of the objective to zero we arrive at the following equation:
\begin{align}\label{eq:stationary}
    \frac{2\sigma^2}{B_t}\cdot\frac{1}{(2-L\eta)^2}+ 
    2\Big(\gamma_t^2+2D_{\max}\gamma_t+ D_{\max}^2\ind(\eta<\eta_{t-1})\Big) \frac{L\eta-1}{(2\eta-L\eta^2)^2} = 0\,.
\end{align}
We consider the two cases below:
\begin{itemize}
    \item $\eta\ge\eta_{t-1}$: In this case, \eqref{eq:stationary} reduces to
    \[
    \frac{\sigma^2}{B_t}+ 
    \Big(\gamma_t^2+2D_{\max}\gamma_t\Big) \frac{L\eta-1}{\eta^2} = 0\,,
    \]
    which is a quadratic equation in $\eta$. Solving for $\eta$, the positive solution is given by $\tau_1$~\eqref{eq:tau1}. This case happens only when the solution satisfies the condition of the case, namely $\eta_{t-1}\le \tau_{1,t}$.
    \item $\eta\le\eta_{t-1}$. In this case, \eqref{eq:stationary} reduces to
    \[
    \frac{\sigma^2}{B_t}+ 
    \Big(\gamma_t^2+2D_{\max}\gamma_t+ D_{\max}^2\Big) \frac{L\eta-1}{\eta^2} = 0\,,
    \]
    which admits the positive solution $\tau_{2,t}$~\eqref{eq:tau2}. This case happens only when the solution satisfies the condition of the case, namely $\tau_{2,t}\le\eta_{t-1}$.
\end{itemize}
If $\tau_{1,t}<\eta_{t-1}<\tau_{2,t}$, then in both of the above cases, the solution happens at the boundary value $\eta_{t-1}$. This brings us to the following characterization for $\eta^*_t$:
\begin{align}
    \eta^*_t = \begin{cases}
    \tau_{1,t} & \text{if } \eta_{t-1}^* \le \tau_{1,t},\\
    \eta_{t-1}^* & \text{if } \tau_{1,t} \le \eta_{t-1}^* \le \tau_{2,t}\\
    \tau_{2,t} & \text{if } \eta_{t-1}^* \ge \tau_{2,t}\,.
    \end{cases}
\end{align}
Note that the above characterization was based on the stationary points of the objective. we next examine if the above solution satisfies the boundary conditions. Obviously $\eta_t^*>0$. We also claim that $\eta^*_t\le 1/L$. For this, we only need to show that $\tau_{2,t}\le 1/L$ (because $\eta^*_t\le \tau_{2,t}$ for all values of $\eta_{t-1}$). Invoking definition of $\tau_{2,t}$, we have 
\[
\tau_{2,t}:= \frac{B_t}{2\sigma^2}\left(\sqrt{b_{2,t}^2L^2+\frac{4\sigma^2}{B_t} b_{2,t}} - b_{2,t}L\right), \quad b_{2,t} := (\gamma_t+D_{\max})^2\,.
\]
It is easy to see that $\tau_{2,t}\le 1/L$ follows simply from $b_{2,t}^2L^2+4\frac{\sigma^2}{B_t}b_{2,t} < (\frac{2\sigma^2}{LB_t}+ b_{2,t} L)^2$.
%===================================
\subsection{Proof of Theorem~\ref{thm:le-convex-reg}}

% \subsubsection{The lower bound on the total expected regret}

%  The results in Theorem~\ref{thm:convex-reg} and Remark~\ref{rmk:opt-sch} are optimized with respect to the upper bound derived for the total expected regret. We also prove a corresponding lower bound result below for SGD, which is matching to the upper bound and only differs in constants. Therefore, our analysis of the optimal learning rate schedules for SGD is, in a sense, tight up to some constants.

% Before we begin, we need some more assumptions.

% \begin{assumption}\label{as:subexp}
% We assume that the loss function $\ell(\theta,z)$ is $\mu$-strongly convex in $\theta$, for some $\mu>0$, i.e., $\ell(\theta) - \frac{\mu}{2}\|\theta\|^2$ is convex in $\theta$. 
% \end{assumption}

% \begin{thm}\label{thm:le-convex-reg}
% Denote by $D_{\max}$ the diameter of the domain $\Theta$. Let $\bz_{[t]}$ be the history up to time $t$, i.e.,
% \[
% \bz_{[t]} := \{z_{k,\ell},k\in[t], \ell\in[B_k]\}\,.
% \]

% Suppose that the oracle model $\theta^*_t$ and the learning rate $\eta_t$ are adapted to the history $\bz_{t-1}$. 

% Let $D_t^2 := \|\theta^*_t - \theta_t\|^2$, $\gamma_t := \|\theta^*_t - \theta^*_{t+1}\|$, and $a'_t: = 2(\eta_t+\frac{L}{\mu}\eta_t - \eta_t^2L)$. Under Assumptions~\ref{ass:main} and~\ref{as:subexp}, and assuming $\eta_t\le \frac{1}{\mu}$, for all $t\ge 1$, we have the following bound on the total regret of the batch SGD:
% %$a_t': = \frac{c}{D_{max}^\alpha}\left(2\eta_t-\frac{\eta_t^2}{c}\right)$
% \begin{align}\label{eq:Reg-LB}
% &\E[\Reg(T)]= \sum_{t=1}^T \E[\reg_t]\nonumber\\
% &\ge \sum_{t=1}^T \mathbb{E}\left[\left(\left(\frac{D_{t}^2}{a'_{t}} - \frac{D_{t+1}^2}{a'_{t+1}}\right) +  \frac{\sigma^2\eta_t^2}{B_ta'_t} + \frac{\gamma_t^2}{a'_t} - 2\frac{\gamma_t}{a'_t} \sqrt{(1+2L^2\eta_t^2)D_t^2+ \frac{2\sigma^2\eta_t^2}{B_t}}\right)_+\right],
% \end{align}
% where for a scalar $x$, $x_+ = \max(x,0)$ indicates its positive part.
% Note that the expectation is with respect to the randomness in data points observed in the $T$ steps.
% \end{thm}

% See Equations~\eqref{eqn:same-ub},~\eqref{eqn:same-lb}, and Remark~\eqref{rmk:comparison} for the comparison of bounds from the two theorems and an interpretation for our regret formula as a predator-prey problem. See also Remark~\ref{rmk:explicit-lb} for a use case of Equation~\eqref{eq:Reg-LB}.

% We will now present the proof of Theorem~\ref{thm:le-convex-reg}. Note that, while it may be possible to derive a lower envelope of the regret for the optimal learning rate schedule, i.e., SGD is slow to react especially if the distribution shifts rapidly, it is not going to give a similar recurrence as our upper envelope thus making the comparison difficult. Hence, we prove a lower envelope result in a way which is similar to the upper envelope. The complete proof is given below.

% \begin{proof}

Recall that
\[
    \delta_t: = \nabla\ell^B_t(\theta_t) -\nabla \ellbar_t(\theta_t)\,,
\]
as the difference between the gradient of the expected loss (at step $t$) and the gradient of the batch average loss at that step. Writing $D_{t+1}$ in terms of this notation, we get
\begin{align}\label{eqn:common_}
D_{t+1}^2 
&= D_t^2 - 2\eta_t\<\nabla \ellbar_t(\theta_t)+\delta_t, \theta_t - \theta_t^*\> +\|v_t\|^2
+2\<v_t, (\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\nonumber\\ &\quad +\eta_t^2\Big(\|\nabla \ellbar_t(\theta_t)\|^2+ \|\delta_t\|^2+2\<\delta_t,\nabla\ellbar_t(\theta_t)\> \Big)\,.
\end{align}

Since the loss function $\ell(\theta,z)$ is $L$-smooth and $\mu$-strongly convex, the expected loss $\ellbar_t(\theta)$ is also $L$-smooth and $\mu$-strongly convex and by invoking \citet[Lemma 3($iii$)]{zhou2018fenchel},
we have
\[
\<\nabla \ellbar_t(\theta_t), \theta_t - \theta^*_t\>
\le \ellbar_t(\theta_t) - \ellbar_t(\theta^*_t) + \frac{1}{2\mu}\|\nabla \ellbar_t(\theta_t)\|^2\,.
\]
Using this bound in \eqref{eqn:common_}, we obtain
\begin{align}\label{eqn:common_1}
D_{t+1}^2 
&\ge D_t^2 - 2\eta_t(\ellbar_t(\theta_t)- \ellbar_t(\theta^*_t)) +\|v_t\|^2
+2\<v_t, (\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\nonumber\\ &\quad +\Big(\eta_t^2 - \frac{\eta_t}{\mu}\Big)\|\nabla\ellbar_t(\theta_t)\|^2+\eta_t^2\|\delta_t\|^2- 2\eta_t \<\delta_t,\theta_t - \theta_t^*-\eta_t\nabla \ellbar_t(\theta_t)\>
\end{align}
We next use \citet[Lemma 4, item 5]{zhou2018fenchel} and the fact that $\nabla\ellbar_t(\theta^*_t) = 0$ to get
\begin{align}
    \|\nabla\ellbar_t(\theta_t)\|^2\le 2L(\ellbar_t(\theta_t)-\ellbar_t(\theta^*_t))\,.
\end{align}
Using the above bound into \eqref{eqn:common_1}, for $\eta_t\le 1/\mu$,
we obtain
\begin{align}\label{eqn:common_1}
D_{t+1}^2 
&\ge D_t^2 - 2\Big(\eta_t+\frac{L}{\mu}\eta_t - \eta_t^2L\Big)(\ellbar_t(\theta_t)- \ellbar_t(\theta^*_t)) +\|v_t\|^2
+2\<v_t, (\theta_t-\eta_t \nabla\ell^B_t(\theta_t))-\theta^*_t\>\nonumber\\ &\quad +\eta_t^2\|\delta_t\|^2- 2\eta_t \<\delta_t,\theta_t - \theta_t^*-\eta_t\nabla \ellbar_t(\theta_t)\>\,.
\end{align}
We recognize that $\theta_t-\eta_t \nabla\ell^B_t(\theta_t)=\theta_{t+1}$ by the SGD update, and let $a'_t: = 2(\eta_t+\frac{L}{\mu}\eta_t - \eta_t^2L)$, with $\eta_t \le 1/\mu$.

Next we obtain a telescoping series for $\Reg(T)$ as before. Continuing as before (in Theorem~\ref{thm:convex-reg}), we can (1) isolate $\ellbar_t(\theta_t)-\ellbar_t(\theta^*_t)$ on the left-hand side, and (2) take expectations: first conditioned on the filtration $\bz_{[t-1]}$ and then an unconditioned expectation, to get:
\begin{align*}
&\E[\Reg(T)]= \sum_{t=1}^T \E[\reg_t]
\ge \mathbb{E}\left[\sum_{t=1}^T \left(\frac{D_{t}^2}{a'_{t}} - \frac{D_{t+1}^2}{a'_{t+1}}\right) +  \frac{\sigma^2\eta_t^2}{B_ta'_t} + \frac{\|v_t\|^2}{a'_t} + 2\frac{\<v_{t}, \theta_{t+1}-\theta^*_{t}\>}{a'_{t}}\right], 
\end{align*}
which completes the proof of theorem.


\section{Proof of theorems and technical lemmas for non-convex loss}
\subsection{Proof of Theorem~\ref{thm:non-Nconvex-reg}}
% We consider the function 
% \[
% \mathcal{C}_t(\theta) := \eta_t\<\nabla\ell_t(\theta_t),\theta\>+\frac{1}{2}\|\theta - \theta_t\|^2\,. 
% \]
% By the projected SGD update rule, we have $\theta_{t+1} = \arg\min_{\theta\in\Theta} \mathcal{C}_t(\theta)$. Note that $\mathcal{C}_t(\theta)$ is quadratic and so convex in $\theta$ (even if $\nabla \ell_t$ may be non-convex function, here it is evaluated at $\theta_t$.) By convexity of $\mathcal{C}_t(\theta)$ and optimality of $\theta_{t+1}$, we have
% \[
% \<\theta-\theta_{t+1},\nabla\mathcal{C}_t(\theta_{t+1})\>\ge0, \quad \forall \theta\in\Theta\,.
% \]
% Using the above condition at $\theta=\theta_t$, and expanding $\nabla\mathcal{C}_t(\theta_{t+1})$, we obtain
% \begin{align*}
% \<\theta_t-\theta_{t+1},\eta_t\nabla\ell_t(\theta_t)+\theta_{t+1} - \theta_t\>\ge 0\,,
% \end{align*}
% which can be rewritten as
% \begin{align}\label{eq:cond-nabla}
% \<\theta_{t+1}-\theta_{t},\nabla\ell_t(\theta_t)\>
% \le -\frac{1}{\eta_t}\|\theta_t-\theta_{t+1}\|^2\,.
% \end{align}
We note that by Assumption~\ref{ass:main},
\begin{align}\label{eq:L-smooth}
    \Big|\ellbar_{t}(\theta_{t+1}) - \ellbar_{t}(\theta_{t})
    -\<\nabla \ell_t(\theta_t),\theta_{t+1}-\theta_t\>\Big|
\le \frac{L}{2}\|\theta_{t+1}-\theta_t\|^2= \frac{L}{2} \eta_t^2\|\nabla\ell_t^B(\theta_t)\|^2\,.
\end{align}
Therefore,
\begin{align*}
    \ellbar_{t}(\theta_{t+1}) - \ellbar_{t}(\theta_{t}) &\le \<\nabla \ellbar_t(\theta_t),\theta_{t+1}-\theta_t\> + \frac{L}{2}\eta_t^2\|\nabla\ell_t^B(\theta_t)\|^2\\
    &\le -\eta_t\<\nabla \ellbar_t(\theta_t), \nabla \ell^B_t(\theta_t)\>+\frac{L}{2}\eta_t^2\|\nabla\ell_t^B(\theta_t)\|^2
\end{align*}
Recall the notation $\delta_t := \nabla\ell_t^B(\theta_t)- \nabla \ellbar_t(\theta_t)$, by which we get
\begin{align*}
\ellbar_{t}(\theta_{t+1}) - \ellbar_{t}(\theta_{t}) &\le
-\eta_t\|\nabla \ellbar_t(\theta_t)\|^2-\eta_t\<\nabla\ellbar_t(\theta_t),\delta_t\> + \frac{L}{2}\eta_t^2 \left(\|\nabla\ellbar_t(\theta_t)\|^2+2\<\nabla\ellbar_t(\theta_t),\delta_t\> + \|\delta_t\|^2\right)\\
&= -\left(\eta_t - \frac{L}{2}\eta_t^2\right)\|\nabla \ellbar_t(\theta_t)\|^2 - (\eta_t - L\eta_t^2) \<\nabla\ellbar_t(\theta_t),\delta_t\> + \frac{L}{2}\eta_t^2 \|\delta_t\|^2\,.
\end{align*}
By condition $\eta_t\le 1/L$, we have $a_t = \eta_t - L\eta_t^2 >0$. Rearranging the terms in the above inequality, we obtain
\begin{align}\label{eq:nconv2}
    \|\nabla \ellbar_t(\theta_t)\|^2 \le 2\frac{\ellbar_t(\theta_t) - \ellbar_t(\theta_{t+1})}{a_t} -2\<\nabla\ellbar_t(\theta_t),\delta_t\> + \frac{L\eta_t^2}{a_t} \|\delta_t\|^2\,.
\end{align}
Since $\theta_t, \theta^*_t,\eta_t$ are adapted to the filtration $\bz_{[t-1]}$, we have
\[
\E[\<\nabla\ellbar_t(\theta_t),\delta_t\>|\bz_{[t-1]}] = 
\<\nabla\ellbar_t(\theta_t),\E[\delta_t\>|\bz_{[t-1]]} = 0\,.
\]
Therefore, by taking expectation from the both sides of \eqref{eq:nconv2}, first conditional on $\bz_{[t-1]}$ and then with respect to $\bz_{[t-1]}$ we get
\begin{align}\label{eq:nconv2}
    \E[\|\nabla \ellbar_t(\theta_t)\|^2] &\le 2\frac{\ellbar_t(\theta_t) - \ellbar_t(\theta_{t+1})}{a_t}  + \frac{L\eta_t^2}{a_t} \frac{\sigma^2}{B_t}\nonumber\\
    &\le 2\frac{\ellbar_t(\theta_t) - \ellbar_{t+1}(\theta_{t+1}) }{a_t} + \frac{|\ellbar_{t+1}(\theta_{t+1})-\ellbar_t(\theta_{t+1})|}{a_t} + \frac{L\eta_t^2}{a_t} \frac{\sigma^2}{B_t}\nonumber\\
    &= 2\frac{\ellbar_t(\theta_t) - \ellbar_{t+1}(\theta_{t+1}) }{a_t} + \frac{\gamma_t}{a_t} + \frac{L\eta_t^2}{a_t} \frac{\sigma^2}{B_t}\,.
\end{align}
Summing both sides over $t=1,\dotsc, T$, we have
\begin{align*}
\E[\Reg(T)] &=\sum_{t=1}^T \E[\|\nabla \ellbar_t(\theta_t)\|^2]\\ 
&\le \sum_{t=1}^T\E\left[ \left(\frac{2\ellbar_t(\theta_t)}{a_t} - \frac{2\ellbar_{t+1}(\theta_{t+1})}{a_{t}}\right) + L\frac{\sigma^2\eta_t^2}{B_ta_t} + \frac{\gamma_t}{a_t}\right]\\
    &= \sum_{t=2}^T \E\left[2\ellbar_t(\theta_t) \left(\frac{1}{a_t} - \frac{1}{a_{t-1}}\right)\right]
    +\E\left[\frac{2\ellbar_1(\theta_1)}{a_1}-\frac{2\ellbar_{T+1}(\theta_T)}{a_{T+1}}\right] + \sum_{t=1}^T \E\left[L\frac{\sigma^2\eta_t^2}{B_ta_t} + \frac{\gamma_t}{a_t} \right]\,.
\end{align*}
The result follows by noting that $\ellbar_{T+1}(\theta_{T+1})\ge 0$.

% Combining Equations~\eqref{eq:cond-nabla} and \eqref{eq:L-smooth}, we arrive at
% \begin{align}
%     \ell_{t+1}(\theta_{t+1})\le \ell_{t}(\theta_{t})+\Big(\frac{L}{2}-\frac{1}{\eta_t}\Big)\|\theta_t-\theta_{t+1}\|^2
%     \le \ell_{t}(\theta_{t})+\Big(\frac{L}{2}-\frac{1}{\eta_t}\Big) \|\nabla\ell_t(\theta_t)\|^2\,,\label{eq:nconv1}
% \end{align}
% where in the last step we used the fact that projection onto a convex set is contraction and so
% \[
% \|\theta_{t+1}-\theta_t\| = \|\Pi_\Theta(\theta_t -\eta_t\nabla \ell_t^B(\theta_t) - \Pi_\Theta(\theta_t) )\|
% \le \|\theta_t -\eta_t\nabla \ell_t^B(\theta_t) - \theta_t\|
%  = \eta_t\|\nabla \ell_t^B(\theta_t)\|
% \]
\section{Auxiliary lemmas}\label{app:lem}
\begin{lemma}\label{lem:stein}
Let $x\in\reals^d$ such that $x\sim \normal(0,I_d)$.
For any fixed vector $u\in\reals^d$, we have
\[
    \E[(xx^\sT-I) uu^\sT (xx^\sT - I)^\sT] = \|u\|^2 I + uu^\sT\,.
\]
\end{lemma}
\begin{proof}
By Stein's lemma, for any function $g:\reals^d \to \reals$ we have
\[
\E[(xx^\sT - I)g(x)] = \E[\nabla^2g(x)]\,.
\]
Using the above identity with $g(x) = (u^\sT x)^2$ we obtain
\begin{align}
    \E[xx^\sT(u^\sT x)^2] = 2uu^\sT + \|u\|^2 I\,.
\end{align}
Using the above characterization, we get
\begin{align*}
 \E[(xx^\sT-I) uu^\sT (xx^\sT - I)^\sT] &=
 \E[xx^\sT (u^\sT x)^2] - u(u^\sT x) x^\sT - x(x^\sT u) u^\sT + uu^\sT\\
 &=2uu^\sT + \|u\|^2 I - 2 uu^\sT+ uu^\sT\\
 &= uu^\sT+ \|u\|^2 I\,,
\end{align*}
which completes the proof.
\end{proof}

We next present Ito's lemma, which allows to find the differential of a time-dependent function of a stochastic process. 
\begin{lemma}[It\^o's lemma,~\cite{oksendal2013stochastic}]
\label{lem:Ito}
Let $X_t\in\reals^p$ be a vector of  It\^o drift-diffusion process, such that
\[
    \de X_t = f(t,X_t) \de t+ g(t,X_t) \de W_t\,,
\]
with $W_t$ being an $q$-dimensional standard Brownian motion and $f(t,X_t)\in\reals^p$ and $g(t,X_t)\in\reals^{p\times q}$. Consider a scalar process $Y(t)$ defined by $Y(t) = \phi(t, X(t))$, where $\phi(t, X)$ is a scalar function which is continuously differentiable with respect to $t$ and twice continuously differentiable with respect to $X$. We then have
\begin{align*}
\de Y_t &= \tilde{f}(t,X_t) \de t+ \tilde{g}(t,X_t) \de W_t\,,\\
\tilde{f}(t,X_t)&= \phi_t(t,X_t) + \phi_x(t,X_t)^\sT f(t,X_t) +\frac{1}{2}{\rm tr}\left(g(t,X_t)^\sT \phi_{xx}(t,X_t)g(t,X_t)\right)\\
\tilde{g}(t,X_t)&=  \phi_x(t,X_t)^\sT g(t,X_t)\,.
\end{align*}
\end{lemma}