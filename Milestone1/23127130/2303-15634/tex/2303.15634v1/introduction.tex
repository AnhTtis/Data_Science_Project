\section{Introduction}

A fundamental question when training neural networks is how much of the weight space to explore and when to stop exploring.
For stochastic gradient descent (SGD)-based training algorithms, this is primarily governed by the learning rate.
If the learning rate is high, then we explore more of the weight space and vice versa.
Learning rates are typically decreased over time
in order to converge to a local optimum,
and there is now a substantial literature focused on how fast learning rates should decay for fixed data distributions
(see, e.g.,~\citet{pmlr-v75-tripuraneni18a} and \citet{JMLR:v19:17-370}, and the references therein).

However, what should we do if the data distribution is constantly changing?
This is the case in many {large-scale} online learning systems where
(1) the data arrives in a stream,
(2) the model continuously makes predictions and computes the loss, and
(3) it always updates its weights based on the new data it sees~\citep{anil2022factory}.
The goal of such a system is to always keep the loss low.
In this setting, convergence is less of a priority because
the model needs to be able to adapt to distribution shifts.
Intuitively, if the loss landscape is consistently changing
(either gradually or due to infrequent sudden spikes),
then it is sensible for the model to always explore its weight space.
We formalize this idea in our work.

Such an investigation naturally leads to the question of how high the learning rate should be,
and what an optimal learning rate schedule is in this online learning scenario?
These questions are critical because while tuning the learning rate can lead to improved accuracy in many applications,
it can also make the online learner widely inaccurate if
the wrong learning rate is used as the distribution shifts.

\begin{figure*}
    \centering

    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/warmup-stepsize-0.003.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/warmup-stepsize-0.01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/warmup-stepsize-0.03.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/warmup-stepsize-0.1.png}
    \end{subfigure}
    \caption{SGD trajectories for online linear regression with different constant learning rates.
    The discrete blue spirals are the optimal model weights $\theta_{t}^* \in \mathbb{R}^{2}$,
    which start at $(1,0)$ and jump clockwise
    every $100$ steps.
    The orange paths are the learned weights $\theta_t$,
    starting at $\theta_0 = 0$
    for $0 \le t \le 17 \cdot 100$.
    The orange squares depict the position every $100$ steps.
    We use batch size $B_t = 1$
    and step sizes
    $\eta_{t} \in \{0.003, 0.01, 0.03, 0.1\}$ from left to right.
    The rightmost SGD is the most out of control,
    but it incurs the least regret
    because it adapts to changes in $\theta_{t}^*$ the fastest
    without diverging.
    }
    \label{fig:warmup_spirals}
\end{figure*}

Formally, we study the problem of learning rate schedules in the presence of distribution shifts by considering the \emph{dynamic regret}, a well-known notion in online optimization that measures the performance against a dynamic comparator sequence. This regret framework captures the lifetime performance of an online learning system that makes predictions on incoming examples as they arrive, possibly from a time-varying distribution,
before using this data to update its weights. 
    
Our main contributions can be summarized as follows:

\paragraph{Linear regression.} We consider a linear regression setup with time-varying coefficients $\{\theta^*_t\}_{t \ge 1}$, which are chosen upfront by an adversary such that $\|\theta^*_t-\theta^*_{t+1}\|_{2} \le \gamma_t$ for a sequence of positive numbers $\{\gamma_t\}_{t\ge 1}$.  The variation in the model coefficients results in response shift (while the covariates distribution remains the same across time). We consider a learner who updates their model estimates via mini-batch SGD with an adaptive step size sequence $\{\eta_t\}_{t\ge 1}$ chosen in an online manner (i.e., only with access to previous data points). We derive a novel stochastic differential equation (SDE) that approximates the dynamics of SGD under distribution shift, and by analyzing it, we derive the optimal learning rate schedule.
    
\paragraph{Convex loss functions.} We generalize our problem formulation along the following directions: $(i)$ We consider general convex loss functions $\ell(\th,z)$ that measure the loss of a model $\th\in\reals^p$ on the data point $z\in\reals^d$. $(ii)$ At each step the learner observes a batch of data points $\{z_{t,k}\}$ drawn from a time-varying distribution $P_t$, which means it can model both response shift and covariate shift. $(iii)$ An adversary can choose the distributions $P_t$ adaptively at each step by observing the history (i.e., the data and model estimates from previous rounds), in contrast to the linear regression setup where the sequence of models are time-varying but fixed a priori.  For strongly convex loss functions, we give a lower bound for the total expected regret that is of the same form as our upper bound and differs only in the constants, demonstrating that our regret analysis is nearly tight. We then propose a learning rate schedule to minimize the derived upper bound on the regret. This schedule is adaptive, resulting in a time-dependent learning rate that tries to catch up with the amount of distribution shift in the moment. We refer to Section~\ref{sub:lit} for a  detailed comparison to the literature on online convex optimization in dynamic environments.
    
\paragraph{Non-convex loss functions.} For settings with non-convex loss functions, we modify the notion of regret to use the gradient norm of the estimated model. We derive an upper bound for the expected cumulative regret and propose a learning rate schedule that minimizes it.
In our experiments in \Cref{app:cyto},
we use neural networks and dynamic learning rates to
continuously classify cells arriving in a stream of
small condition RNA data \citep{Bastdas-Ponce}.
This work simulates an online and deep learning-based
\emph{flow cytometry} algorithm.
We refer the reader to \citet{LMC19} for more details about this application.
One take-away message from our analysis and experiments
in all three settings is that an optimal learning rate schedule
typically increases in the presence of distribution shift. 

% Such a device tries to continuously classify cells based on their type, as they arrive from a tissue sample, in a continuous stream. More recently, deep learning has been used to optimize such devices (see Section~\ref{app:cyto} and the paper~\cite{LMC19} for details). Some of our results for varying learning rates in a simulated flow cytometry setting are given in Figure~\ref{fig:flow-cyt-summary}. More details regarding the application and results are discussed in Section~\ref{app:cyto}.
% One take-away message from our analysis in all of these settings is that the optimal learning rate schedule typically increases in the presence of distribution shift. 

% \begin{figure*}
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%     \includegraphics[width=\textwidth]{figures/flow-cyt-hq.pdf}
%     \end{subfigure}
%     \hspace{0.5cm}
%     \begin{subfigure}[b]{0.45\textwidth}
%     \includegraphics[width=\textwidth]{figures/flow-cyt-dyn-hq-bs.pdf}
%     \end{subfigure}
%     \caption{Empirical regrets for different learning rate choices in a simulated flow cytometry setting. A flow cytometer classifies and sorts a stream of cells fed based on RNA expressions measured via fluorescence levels. We simulate such a stream cells using data from~\citet{Bastdas-Ponce}. The left figure shows that when the distribution of the cells in the input stream changes over time, higher learning rate constants can be beneficial. Moreover, if we fix the average learning rate, then a schedule that uses higher learning rates only when the input distribution changes can outperform an equivalent constant learning rate schedule. The right figure compares the effect of batch size for the adjusted learning rate schedule. Large batch sizes can help smooth gradients but can also be counter-productive as the neural net sees fewer batches between successive distribution shifts.}
%     \label{fig:flow-cyt-summary}
% \end{figure*}
    
% We provide high-dimensional regression experiments to demonstrate the dynamics of these learning rate schedules,
% and we present an application to flow cytometry in~\Cref{app:cyto}. 

\vspace{0.45cm}
The organization of the paper is as follows.
In \Cref{sub:lit}, we proceed with a literature review.
In \Cref{sub:res}, we present an overview of our tools,
techniques, and informal statements of our theoretical results.
We formally define the problem in \Cref{sec:formulation}.
We present our results for linear regression in \Cref{sec:linear-regression},
convex losses in \Cref{sec:cvx}, and non-convex losses in \Cref{sec:nonconvex}.
In \Cref{sec:experiments},
we present experiments to study the effect
of the proposed learning rate schedules,
including high-dimensional regression and a
medical application to flow cytometry.
We defer the proofs of our technical results to the appendix.

\subsection{Related work}
\label{sub:lit}
With deep neural networks now being used in countless applications and SGD remaining the dominant algorithm for training these models, there has been a surge of effort to understand how learning rates affect the behavior of stochastic optimization methods~\citep{Bengio,Smith}. Most of the existing literature, however, assumes no shift in the underlying distribution across the iterations of SGD. Various trade-offs between learning rate and batch size have been studied~\citep{keskar2016large,smith2018don}. In particular, \citet{smith2018don} proposes that instead of the decaying learning rate, one can increase the batch size during training and empirically show that it results in near-identical model performance with significantly fewer parameter updates.
\citet{shi2020learning} analyze the effect of learning rate on SGD by studying its continuum formulation given by a stochastic differential equation (SDE) and show that for a broad class of losses, this SDE converges to its stationary distribution
at a linear rate, further revealing the dependence of a linear convergence rate on the learning rate. Learning rate schedules for SGD, under fixed distribution, and for the setting of least squares has been studied in~\citep{ge2019step,jain2019making}.
Decaying learning rate via cyclical schedules has also
been proposed for training deep neural models (see, e.g.,~\citet{Smith,loshchilov2016sgdr,li2019exponential}).

The effects of SGD hyperparameters (e.g., batch size and learning rate) have also been studied for the adversarial robustness of the resulting models \cite{yao2018hessian,kamath2020sgd}.
In this setting, a model is trained on unperturbed samples, but at test time the sample features are slightly perturbed. In contrast, this paper considers settings where \emph{the data distribution is constantly changing}---even during the training---and studies the effect of learning rates in presence of such distribution shifts.

\paragraph{Connections to online optimization.}
The notion of dynamic regret has been used in online convex optimization to evaluate the performance of a learner against a dynamic target, as opposed to the classical single best action in hindsight~\cite{zinkevich2003online,yang2016tracking,jadbabaie2015online,besbes2015non}. In this setting, nature chooses a sequence of convex functions $f_1,\dotsc, f_T$ and the learner chooses a model (i.e, action) $\theta_t$ at each round and incurs the loss $f_t(\theta_t)$. Our problem is closer to non-stationary approximation~\cite{besbes2015non}, in which the learner only has noisy access to gradients $\nabla f_t(\theta_t)$. There is often a notion of variation to capture the change in the comparator. For example,~\citet{yang2016tracking} works with ``path variation,'' which measures how fast the minimizers of the sequence of loss
functions change, and \citet{besbes2015non} defines a ``functional variation''
based on the supremum distance between consecutive loss functions.

\citet{yang2016tracking} give an upper bound for the cumulative dynamic regret when a constant step size $\eta\propto\sqrt{\mathcal{V}_T/T}$ is used, where $T$ is the horizon length and $\mathcal{V}_T$ is the variation budget that controls the power that nature has in choosing the sequence of loss functions (see Theorem~7 therein).
\citet{besbes2015non} propose a restarting procedure, which for batch size $\Delta_T$ restarts an online gradient descent algorithm every $\Delta_T$ periods. Their analysis suggests to take $\Delta_T = (T/\mathcal{V}_T)^{2/3}$ and $\eta \propto 1/\sqrt{\Delta_T}$ (see Theorem 3 therein).

While these results also suggest that in a changing environment the learning rate should be in general set higher,
our formulation and analysis for the convex setting departs from these works in the following ways: $(i)$ Instead of constant or a pre-determined learning rate, our framework allows for \emph{adaptive schedules} where the learning rate at every epoch can be set based on the history; $(ii)$ The notion of dynamic regret is often defined with respect to an arbitrary but fixed sequence of loss functions that satisfy a variation budget constraint. In contrast, we allow the data distribution to shift adaptively at each step after observing the history, and so the expected loss changes adaptively over time; $(iii)$ \citet{besbes2015non} and \citet{yang2016tracking} establish lower bounds on the dynamic regret, but these lower bounds are for the worst-case regret over the choice of loss function sequences that satisfy the variation budget constraint. The lower bounds are obtained by carefully crafting a sequence that is hard to optimize in an online manner. However, there is a subtle difference in our setting: the loss function $\ell(\theta,z)$ is fixed and the change in the expected loss across time comes from a shift in the data distribution~$z$. The lower bound we develop for dynamic regret uses the same fixed loss function $\ell(\theta,z)$.

\subsection{Overview of  techniques}
\label{sub:res}

To analyze the behavior of SGD in this linear regression setting,
we derive a novel stochastic differential equation (SDE) that approximates the dynamics of SGD in the presence of distribution shift. Using Gr\"{o}nwall's inequality~\cite{gronwall1919note}, we control the deviation of the SGD trajectory from the continuous process and relate the regret of SGD to the second moment of the continuous process, which we characterize using the celebrated It\^o's lemma from stochastic calculus~\cite{oksendal2013stochastic} (see Lemma~\ref{lem:Ito}).
With this characterization, we derive an
optimal learning rate schedule in a sequential manner.

Our results for general convex loss functions are based on an intricate treatment of the regret terms, taking the expectation with respect to a proper filtration, and using several properties of convex functions and the SGD algorithm itself.

Non-convex loss functions can have a complicated landscape with potentially many local minima and saddle points. Even without distribution shifts, first-order methods like SGD are not guaranteed to converge to a global minimum. To deal with this challenge, we modify the definition of regret to use the norm of the gradient of the loss for the estimated models. Thus, a trajectory that stays close to local minima of the loss functions has low total regret. To upper bound the cumulative regret in this setting, we follow a similar proof technique as in the convex case, but rely only on the SGD update formulation and first-order optimality conditions on the sequence of optimal weights $\{\theta^*_t\}_{t \ge 1}$.

% In this subsection, we give an overview of our theoretical results.
% We start by setting up the context.
% Our input consists of labeled data points that are sampled from a time-varying distribution.
% We assume the learner, e.g., a neural network, sees each example once.
% In particular, data is not shuffled across time windows and we do no revisit examples more than once when updating model weights via SGD. The goal is to minimize the learner's lifetime regret:
% the expected total loss minus the expected optimal loss at each time step.

% We provide an informal view of our results and techniques here and refer to subsequent sections for formal statements of the results.

% \begin{thm}[Online linear regression (see Theorem~\ref{pro:SDE})]
% Consider online least squares linear regression, where the input is described as above and the loss function is a linear sum of squares error. For training using the SGD algorithm, the optimal learning rate is unique, and can be explicitly computed. Further, it increases as the rate of distribution shift is raised.
% \end{thm}

% To analyze the behavior of SGD in the linear regression setting, we derive a novel stochastic differential equation (SDE) which approximates the dynamics of SGD, in the presence of distirbution shift. Using Gr\"{o}nwall's inequality we control the deviation of SGD trajectory from the continuous process and relate the regret of SGD to the second moment of the continuous process, which we characterize using the celebrated It\^o's lemma from stochastic calculus. Using this characterization, we derive optimal learning rate schedules in a sequential manner.   

% \begin{thm}[Upper bound on regret for convex loss functions (see Theorem~\ref{thm:convex-reg})]
% Consider an online setting for data samples, as described above, with a general convex loss function. Then, for training using the SGD algorithm (or projected SGD algorithm), we derive an upper bound on the total expected regret. The optimal learning rate which minimizes this upper bound is expressible as a recurrence, and increases as the rate of distribution shift is raised.
% \end{thm}

% \begin{remark}
% % We do not explicitly characterize the optimal learning rate for convex loss functions, but we come pretty close to that with our lower envelope theorem
% % %~\ref{thm:le-convex-reg}
% % in the supplement, which gives a lower envelope whose form resembles the upper envelope we analyze in Theorem~\ref{thm:convex-reg}.
% For the setting with general convex loss, we also develop a lower bound on the total expected regret which resembles the upper bound established in Theorem~\ref{thm:convex-reg}. 
% The lower bound result is deferred to the supplement due to space constraint. 
% % is given in the supplement and is based on the continuous path properties of SGD and characterizes the lower envelope as the solution of an optimization problem with a similar objective function as Theorem~\ref{thm:convex-reg}.
% \end{remark}

% %\pw{can we have higher quality fig 1,2 they are a bit blurry and also in fig 2b, write "LR dynamic" in legend}
% % \begin{thm}[Lower envelope of convex loss functions (see Theorem~\ref{thm:le-convex-reg})]
% % Consider an online setting for data samples, as described above for the SGD algorithm, with a general strictly convex loss function but not increasing too rapidly (sub-exponentially), and the distribution shift is large. Then, unless the regret is large, the lower envelope of the optimal learning rate is expressible as a similar recurrence on a subset of times. In fact, the terms in the expression here deviate from the terms in the expression for the upper envelope in only constants.
% % \end{thm}

% Our results for the general convex loss is based on an intricate treatment of the regret terms and using several properties of convex functions and the SGD algorithm itself. 
