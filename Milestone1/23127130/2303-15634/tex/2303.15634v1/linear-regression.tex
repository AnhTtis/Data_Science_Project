\section{Linear regression}
\label{sec:linear-regression}

We start by studying the linear regression setting with a time-varying coefficient model~\cite{fan2008statistical,hastie1993varying}.
Each sample $z_{t,k} = (x_{t,k},y_{t,k})$ is a pair of covariates $x_{t,k}\in\reals^d$ and a label $y_{t,k}$, with 
\begin{align}\label{eq:LinModel}
    y_{t,k} = \<x_{t,k},\theta^*_t\>+ \eps_{t,k}\,,
\end{align}
where $\eps_{t,k}\sim\normal(0,\sigma^2)$ is random noise. The covariate distribution is assumed to be the same across time, and for simplicity assumed as $x_{t,k}\sim\normal(0,I)$. The model $\theta^*_t$ change over time, and so we have label distribution shift.
We consider least squares loss $\ell(\theta,z) = \frac{1}{2}(y - \<x,\theta\>)^2$, for $z = (x,y)$. 

% We consider SGD updates~\eqref{eq:BSGD} (with $\Theta = \reals^d$, i.e., no projection):
% \begin{align}
%     \theta_{t+1} &= \theta_t - \eta_t \frac{1}{B_t}\sum_{k=1}^{B_t}\nabla\ell(\theta_t,z_{k,t})\nonumber\\
%     &= \theta_t + \eta_t \frac{1}{B_t}\sum_{k=1}^{B_t} (y_{tk}-\<x_{tk},\theta\>) x_{tk}\nonumber\\
%     %&= \theta_t -\eta_t \nabla\ellbar_t(\theta_t)-
%     %\eta_t \left(\frac{1}{B_t}\sum_{k=1}^{B_t}\nabla\ell(\theta_t,z_{k,t}) - \nabla\ellbar_t(\theta_t)\right)\nonumber\\
%      &= \theta_t +\eta_t (\theta^*_t - \theta_t) +\frac{\eta_t}{B_t}\sum_{k=1}^{B_t} \left((x_{tk}x_{tk}^\sT - I)(\theta^*_t - \theta_t)+\eps_{tk} x_{tk}\right)\nonumber\\
%      & =\theta_t +\eta_t (\theta^*_t - \theta_t) -\eta_t\zeta_t \,, \label{eq:zeta}
% \end{align}
% where the noise term $\zeta_t$ has mean zero, given that the data points $z_{t,k}$ are sampled independently at each step $t$. 

To provide theoretical insight on the dependence of SGD on the learning rate under distribution shift, we follow a recent line of work that studies optimization algorithms via the analysis of their behavior in continuous-time
limits~\cite{krichene2017acceleration,li2017stochastic,chaudhari2018deep,shi2020learning}.
Specifically, for SGD this amounts to studying stochastic differential equations (SDEs) as an approximation for discrete stochastic updates. The construction of this correspondence is based on the Euler--Maruyama method. We assume that the step sizes in SGD are given by $\eta_t = \eps \zeta(\eps t)$, where $\zeta(t)\in [0,1]$ is the adjustment factor and $\eps$ is the maximum allowed learning rate. In addition, the batch sizes are given by $B_t = \eps \nu(\eps t)$, for sufficiently regular functions $\zeta, \nu:\reals_{\ge 0}\to \reals_{\ge0}$.\footnote{More precisely, $B_t = \lceil\eps \nu(\eps t)\rceil$ must be an integer,
however, the rounding effect is negligible in the continuous time analysis.}  

% \fahrbach{For the uninitiated, can we clarify: (1) $\tau$ vs $t$ as time parameter, (2) what the batch size $B_t = \varepsilon \nu(\varepsilon t)$ is since it's no longer an integer (i.e., just a rescaling I guess?)}

We use $t$ to denote the iteration number of SGD and use~$\tau$ as the continuous time variable for the corresponding SDE.
We show that the trajectory of SGD updates can be approximated by the solution of the following SDE:
%
\begin{align}\label{eq:SDE}
    &\de X(\tau) =-(\zeta(\tau) X(\tau) +Y'(\tau))\de\tau
     + \tfrac{\zeta(\tau)}{\sqrt{\nu(\tau)}} \left((\|X(\tau)\|^2+\sigma^2)I + X(\tau)X(\tau)^\sT\right)^{1/2}\; \de W(\tau)\,,
\end{align}
where $X(0) = \theta_0 - \theta^*_0$
and $Y(\tau)$ is a sufficiently smooth curve so that $Y(\eps t) = \theta^*_t$. Further, $W(\tau)$ is $d$-dimensional vector with each entry being a standard Brownian motion, independent from other entries. To make this connection, we posit the following assumptions:
\begin{itemize}
    \item[{\sf A1.}] The functions $\zeta(\tau)$ and $\zeta(\tau)/\sqrt{\nu(\tau)}$ are bounded Lipschitz:
    $\|\zeta\|_\infty$, $\|\zeta\|_{\rm Lip}$,
    $\|\zeta/\sqrt{\nu}\|_\infty$, $\|\zeta/\sqrt{\nu}\|_{\rm Lip}\le K$.
    \item[{\sf A2.}] The function $Y(\tau)$ is bounded Lipschitz: $\|Y(\tau)\|\le K$ and $\|Y'(\tau)\|\le \Gamma/\eps$, for constants $K, \Gamma>0$. Recall that $Y(\tau)$ is the continuous interpolation of the sequence models $\theta^*_t$ and therefore $Y'(\tau)$ controls how fast $\theta^*_t$ are changing and is a measure of distribution shift in the response variable $y_{tk}$ in~\eqref{eq:LinModel}.
\end{itemize}

In {\sf (A1)} we use the notation $\|f\|_{\rm Lip} := \sup_{x\neq y} |f(x)-f(y)|/|x-y|$ to indicate the Lipschitz norm of a function and $\|f\|_\infty:= \sup_x |f(x)|$.  
% Recall that by construction $Y'(t\eps)\approx (\theta^*_{(t+1)\eps}-\theta^*_{t\eps})/\eps$, and so the term $\gamma_{t\eps}$ controls the shifts in the model's $\theta^*_t$ over time.

\begin{propo}\label{pro:approx}
For any fixed $T,u>0$, there exists a constant $C = C(K,\Gamma, d,\sigma, T,u)$, with parameters $K,\Gamma$ given in Assumptions {\sf A1-A2}, such that with probability at least $1-e^{-u^2}$ we have
\[
    \sup_{t\in[0,T/\eps]\cap \mathbb{Z}_{\ge 0}} \Big|\|X_{t\eps}\|^2-\|\theta_t-\theta^*_t\|^2\Big|
    \le C\sqrt{\eps}\,.
\]
\end{propo}
We defer the proof of this proposition and the exact expression
for the constant $C$ to \Cref{proof:approx}.

%For ease of presentation, we consider $d=1$, but a similar derivation can be obtained for $d>1$.
% Consider a continuous time variable $\tau$ and define $\tau_\ell = \sum_{t=1}^\ell \eta_t$. Consider  sufficiently smooth curves $X(\tau), Y(\tau)\in\reals^d$ so that $X(\tau_\ell) = \theta_\ell-\theta^*_\ell$ and $Y(\tau_\ell) = \theta^*_\ell$, for $\ell\ge 1$, and $X(0)= Y(0)= 0$. Note that   
% $\zeta_t$ in~\eqref{eq:zeta} is the average of $B_t$ zero mean variables and thus can be approximated by a normal distribution with covariance 
% \begin{align}
% &\frac{\eta_t}{B_t} \left\{\E\left[(x_{tk} x_{tk}^\sT - I)(\theta^*_t - \theta_t)(\theta^*_t - \theta_t)^\sT (x_{tk} x_{tk}^\sT - I)^\sT\right] +\sigma^2 I\right\}\nonumber\\
% &= \frac{\eta_t}{B_t} \left((\|X(\tau)\|^2+\sigma^2)I + X(\tau) X(\tau)^\sT \right)\,,
% \end{align} 
% where the above identity follows from Lemma~\ref{lem:stein} in Appendix~\ref{app:lem}.  Applying a Taylor expansion, we obtain
% \begin{align*}
%     \nabla X(\tau_\ell) = -X(\tau_\ell) - \nabla Y(\tau_\ell) + \sqrt{\frac{\eta_\ell}{B_\ell}} 
%     \left((\|X(\tau)\|^2+\sigma^2)I + X(\tau)X(\tau)^\sT\right)^{1/2}\; \nabla{\de W(\tau_\ell)} + O(\eta_\ell^{3/2})\,,
% \end{align*}
% where $\nabla$ indicates derivative with respect to $\tau$ and $W(\tau)$ represents a standard Brownian motion.\footnote{Despite a Brownian motion is not differentiable, the notation $\nabla W(\tau)$ can be given a rigorous interpretation \cite{evans2012introduction}.
% } 

% Retaining $O(1)$ and $O(\sqrt{\eta_t})$ terms and ignoring smaller order terms, we obtain the following stochastic differential equation (SDE) that approximates the discrete-time SGD algorithm:

\noindent
The expected regret at time $t$ works out as:
\begin{align*}
    \E[\reg_t] &= \E[\ellbar_t(\theta_t)- \ellbar_t(\theta^*_t)] \\
    &= \frac{1}{2}\E[(\<x_{tk},\theta_t-\theta^*_t\>+\eps_{tk})^2] - \frac{1}{2}\E[\eps_{tk}^2] \\
    &=\frac{1}{2}\E[\|\theta_t - \theta^*_t\|^2]\,.
\end{align*}
 Using Proposition~\ref{pro:approx}, $|\E[\reg_t] - \frac{1}{2} \E[\|X(t\eps)\|^2]|\le C\sqrt{\eps}$. Henceforth, we focus on analyzing the second moment of the process $X$, as $\eps$ can be fixed to an arbitrarily small value.

For $X(\tau)$ the solution of SDE~\eqref{eq:SDE}, we define
\begin{align}\label{eq:moments}
m_\tau := \E[X(\tau)]\in\reals^d, \quad
v_{\tau}: = \E[\|X(\tau)\|^2]\,.
\end{align}
In our next theorem, we derive an ODE for $m_\tau$ and $v_\tau$, using It\^o's lemma from stochastic calculus~\cite{oksendal2013stochastic}.
The proof is deferred to Section~\ref{sec:Ito-SDE}.
%
\begin{thm}\label{pro:SDE}
Consider the SDE problem~\eqref{eq:SDE}, and the moments $m_\tau$ and
$v_{\tau}$ given by~\eqref{eq:moments}. We have
\begin{align}
    m'_\tau &= -\zeta(\tau)m_\tau - Y'(\tau)\,,\label{eq:m-ode} \\
    v'_\tau &=
    \Big((d+1)\frac{\zeta(\tau)^2 }{\nu(\tau)}-2\zeta(\tau)\Big) v_\tau \label{eq:v-ode}
    + \frac{\zeta(\tau)^2}{\nu(\tau)}\sigma^2 d - 2m_\tau^\sT Y'(\tau)\,.
    %-2  e^{-\tau} X(0)^\sT \nabla Y(\tau)\,.
\end{align}
\end{thm}

% The regret $v_\tau$ can be solved in closed form from the differential equations~\eqref{eq:m-ode}--\eqref{eq:v-ode}. 

It is worth noting that from the above ODEs, larger distribution shift (quantified by the $Y'(\tau)$ term) increases the drift in $m_\tau$ as well as the drift in $v_\tau$ via the term $m_\tau^\sT Y'(\tau)$.
In this case, the learner needs to choose a larger step size $\zeta(\tau)$ to reduce the drift in $m_\tau$,
which is consistent with our message that in dynamic environments
the learning rate should often be set higher. 

The problem of finding an optimal learning rate can be
seen as an optimal control problem,
where the state of the system $(m_\tau,v_\tau)$ evolves according to ODEs~\eqref{eq:m-ode}--\eqref{eq:v-ode},
the control variables~$\zeta$ can take values in the set of   Borel-measurable functions from $[0,T]$ to $[0,1]$,
and the goal is to minimize the cost functional $\int_0^{T}v_\tau \de \tau$.
The optimal learning rate schedule can then be solved exactly by dynamic programming, using
the Hamilton--Jacobi--Bellman equation~\cite{bellman1956dynamic}.
However, the optimal learning rate will depends on $Y'(\tau)$, which is a $d$-dimensional time-varying vector.
We next do a simplification to reduce the dependence to $\|Y'(\tau)\|$. 

Note that $|m_\tau^\sT Y'(\tau)|\le \|Y'(\tau)\|\;\|m_\tau\|\le \|Y'(\tau)\| \sqrt{v_\tau}$. The first inequality becomes tight if the shift $Y'(\tau)$ is aligned with the expected error $m_\tau$. The second inequality becomes tighter as the batch size grows, since it reduces the variance in $X(\tau)$, which by~\eqref{eq:moments} is given by $v_\tau-\|m_\tau\|^2$. Therefore, we have
\[
v'_\tau \le
    \Big((d+1)\frac{\zeta(\tau)^2 }{\nu(\tau)}-2\zeta(\tau)\Big) v_\tau 
     + \frac{\zeta(\tau)^2}{\nu(\tau)}\sigma^2 d + 2\|Y'(\tau)\|\sqrt{v_\tau}.
\]
With this observation and the fact that
our objective is to minimize the cost $\int_0^T v_\tau\de\tau$,
we consider the process $\tilde{v}_\tau$ defined using the upper bound on $v_\tau'$, namely
\begin{align}\label{eq:tv}
    \tilde{v}'_\tau =
    &\Big((d+1)\frac{\zeta(\tau)^2 }{\nu(\tau)}-2\zeta(\tau)\Big) \tilde{v}_\tau
     + \frac{\zeta(\tau)^2}{\nu(\tau)}\sigma^2 d + 2\|Y'(\tau)\|\sqrt{\tilde{v}_\tau}\,.
\end{align}
Our next result characterizes an optimal learning rate with respect to process $\tilde{v}_\tau$.
%
\begin{thm}\label{thm:optimal-policy}
Consider the control problem
\[
\underset{\zeta:[0,T]\to [0,1]}{{\rm minimize}}\; \int_0^T \tilde{v}_\tau\de \tau\,,\quad \text{\emph{ subject to constraint}}~\eqref{eq:tv}\,.
\]
The optimal policy $\zeta$ is given by 
\begin{align}\label{eq:policy}
    \zeta^*(\tau) =\min\left\{1, \left(\frac{d+1}{\nu(\tau)}\tilde{v}_\tau + \frac{\sigma^2d}{\nu(\tau)}\right)^{-1} \tilde{v}_\tau\right\}\,.
\end{align}
\end{thm}

Using the policy $\zeta^*(\tau)$ given by~\eqref{eq:policy} and~\eqref{eq:tv},
we get an ODE that can be solved for $v_\tau$
and then plugged back into~\eqref{eq:policy} to obtain an optimal policy $\zeta^*(\tau)$ and hence optimal learning rate.
We formalize this approach in Algorithm~\ref{alg:linear}, where we solve the ODE for $\tilde{v}_\tau$ (after substituting for optimal $\zeta^*(\tau)$) using the (forward) Euler method.
Translating from the continuous domain to the discrete domain,
we use the relations $\eta_t = \eps\zeta(\eps t)$, $B_t = \eps\nu(\eps t)$, and $\|Y'(\eps t)\|\approx \|\theta^*_{t+1}-\theta^*_t\|/\eps = \gamma_t/\eps$.

\begin{remark}
The learning rate schedule proposed in Algorithm~\ref{alg:linear} is an online schedule in the sense that $\eta_t$ is determined based on the history up to time $t$, i.e., it does does not look into future.
%\aj{I changed the $\gamma_t$ to $\gamma_{t-1}$ in the algorithm since this is what we said to do in simulations, but didn't explain it here as it cause confusion.}
\end{remark}

\begin{remark}
The proposed learning rate in Algorithm~\ref{alg:linear} depends on the distribution shifts $\gamma_t$. In settings where $\gamma_t$ is not revealed (even after the learner proceeds to the next round),
we estimate $\gamma_t$ using an exponential moving average of the drifts in the consecutive estimated models $\theta_t$,
namely $\hat{\gamma}_t = \beta \hat{\gamma}_{t-1} + (1-\beta)\|\theta_t- \theta_{t-1}\|$, with a factor $\beta\in(0,1)$.
\end{remark}

% \begin{remark}
% Consider the setting that there is no distribution-shift and the batch sizes at each step are equal ($B_t = B$). In the large batch size limit ($B\to \infty$), our learning rate results in:
% \begin{align*}
%     \eta^*_t = \begin{cases}
%     \eps \quad \text{if } t\le \frac{1}{\eps}(1+o_B(1))\\
%     \frac{1}{t}(1+O_B(1))
%     \end{cases}
% \end{align*}
% \end{remark}
% \begin{algorithm}
% \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
% \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
% \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \Input{maximum step size ($\eps$), discretization scale $\kappa\in (0,1]$}
% \Output{step sizes $\eta^*_t$}
% \BlankLine
% {\bf Initialization:} set $v = 0$ \;
% \For{$t = 1, 2, \dotsc,$}{
% \For{$j\leftarrow 1$ \KwTo $1/\kappa$}{\label{forins}
%     $r\leftarrow \min\Big(\frac{v B_t}{(d+1)v + \sigma^2 d},\eps \Big)$\;
%     $u\leftarrow u+\kappa\Big(\frac{d+1}{B_t}r^2 - 2r\Big)v + \kappa \frac{\sigma^2d}{B_t} r^2+2\kappa \gamma_{t-1}\sqrt{v}$\;
%     $\eta_{t}\leftarrow\min\Big(\frac{v B_{t}}{(d+1)v+\sigma^2 d},\eps\Big)$ 
% }
% }
% \caption{Learning rate schedule for linear regression under distribution shift}\label{alg:linear}
% \end{algorithm}

\begin{algorithm}[tb]
   \caption{Optimal learning rate schedule for linear regression undergoing distribution shift.}
   \label{alg:linear}
\begin{algorithmic}
   \STATE {\bfseries Input:} max step size $\eps$,
                            discretization scale $\kappa\in (0,1]$
   \STATE{\bfseries Output:} step sizes $\eta^*_t$
   \STATE{\bfseries Initialization:} $v \gets 0$
   \FOR{$t = 1, 2, \dotsc,$}
   \FOR{$j = 1, \dotsc, \lceil 1/\kappa \rceil$}
   \STATE $r\leftarrow \min\Big(\frac{v B_t}{(d+1)v + \sigma^2 d},\eps \Big)$\;
   \STATE $v\leftarrow v+\kappa\Big(\frac{d+1}{B_t}r^2 - 2r\Big)v + \kappa \frac{\sigma^2d}{B_t} r^2+2\kappa \gamma_{t-1}\sqrt{v}$\;
   \ENDFOR
   \STATE $\eta^*_{t}\leftarrow\min\Big(\frac{v B_{t}}{(d+1)v+\sigma^2 d},\eps\Big)$
   \ENDFOR
\end{algorithmic}
\end{algorithm}


% To make this observation formal and derive the optimal step size, we consider a learner who sequentially updates their learning rate and an adversary who adaptively chooses the distribution shifts. 

% Going back from the continuous domain to the discrete, we have $m_{(t+1)\eps}\approx (1-\eps \zeta(t\eps))m_{t \eps} - \eps Y'(t \eps)$ from~\eqref{eq:m-ode}. We also have $v_{(t+1)\eps} \approx v_{t\eps} + \eps v'_{t\eps}$, which brings us to the following:  
% \begin{align}\label{eq:vte}
%     v_{(t+1)\eps} \approx 
%     v_{t\eps} + \Big((d+1)\frac{\eta_t^2 }{B_t}-2\eta_t\Big) v_{t\eps} + \frac{\eta_t^2}{B_t}\sigma^2 d 
%     - 2(1-\eta_{t})m_{t\eps}^\sT Y'(t\eps)\eps + 2\eps^2 \|Y'(t\eps)\|^2\,.
%     %&\quad - 2e^{-\tau_{\ell-1}-\eta_\ell} X(0)^\sT \nabla Y(\tau_\ell)\,.
% \end{align}
% Subject to condition ({\sf A2}) on $\|Y'(\tau)\|$, the above regret is maximized for the shift $Y'(t\eps) = -\frac{\gamma_t}{\eps} \frac{m_{t\eps}}{\|m_{t\eps}\|}$. Plugging for this shift the regret function $v_{(t+1)\eps}$ becomes quadratic in $\eta_t$, from which we obtain a closed form for the optimal learning rate:
% \begin{align}\label{opt:eta-Lin}
%     \eta^*_t = \frac{\gamma_t\|m_{t\eps}\|+v_{t\eps}}{(d+1)v_{t\eps}+\sigma^2 d} B_t\,.
% \end{align}
% \fahrbach{The above sentence doesn't make sense.}

% % Note that $v_{\tau_\ell}$ is a function of the history (up to $\tau_\ell$). Similarly the vector $u:= 2e^{-\tau_{\ell-1}}\int_0^{\tau_{\ell-1}} e^s \nabla Y(s)\de s$ is a function of history. Therefore, the choice of $\eta_\ell$ amounts to the following minimax problem:
% % \begin{align}
% %     \min_{\eta_\ell\ge0} \max_{\nabla Y(\tau_\ell)} \quad 
% %     \frac{\eta_{\ell}^2}{B_\ell}((d+1)v_{\tau_\ell}+\sigma^2d) - 2v_{\tau_\ell}\eta_\ell +  \eta_\ell e^{-\eta_\ell} u^\sT \nabla Y(\tau_\ell)+ 2\eta_\ell^2\|\nabla Y(\tau_\ell)\|^2\,.
% %     %- 2e^{-\tau_{\ell-1}} e^{-\eta_\ell} X(0)^\sT \nabla Y(\tau_\ell)\,. 
% % \end{align}

% % We have $\|\nabla Y(\tau_\ell)\|\approx \frac{1}{\eta_\ell} \|\theta^*_{\ell+1}-\theta^*_\ell\| \le \frac{\gamma_\ell}{\eta_\ell}$. The inner maximization can be easily solved, which results in the following optimization for the learner:
% %  \begin{align}\label{eq:opt-eta-linear}
% %      \min_{\eta_\ell\ge0}\quad  \frac{\eta_{\ell}^2}{B_\ell}((d+1)v_{\tau_\ell}+\sigma^2d) - 2v_{\tau_\ell}\eta_\ell +  \gamma_\ell e^{-\eta_\ell}\|u\|+ 2\gamma_\ell^2
% %  \end{align}

% % %\begin{align}\label{eq:fixed}
% % %  2\frac{\eta_\ell}{B_\ell} ((d+1)v_{\tau_\ell}+\sigma^2d) - 2v_{\tau_\ell} - C'\gamma_\ell e^{-\eta_\ell} \left(\frac{1}{\eta_\ell} + \frac{1}{\eta_\ell^2}\right) = 0\,. 
% % %\end{align}

% We conclude this section by making a few observations  based on our analysis above:
% \begin{itemize}
%     \item The optimal learning rate $\eta_t$ is increasing in $\gamma_t$, the shift in model parameters.
%     \item As can be seen from \eqref{eq:vte}, the regret at time $t$ is quadratic in $\eta_t$. Furthermore, by~\eqref{opt:eta-Lin}, $\eta^*_t$ is proportional to the batch size $B_t$.
%     \item If the regret at the previous step is small (so $\|m_{t\eps}\|$ and $v_{t\eps}$ are small), then the step size at the next step will be small.
%     \rev{\item Note that the optimal learning rate $\eta^*$ depends on the shift in the model parameters $\gamma_t$ as well as regret in previous round, via terms $\|m_{t\eps}\|$ and $v_{t\eps}$. While the latter is often available to the learner as feedback on their performance in previous rounds, in some contexts the distribution shift is not revealed even after the learner moves on to next rounds. In these settings we use a moving average of the estimated models $\theta_t$ to approximate the shift $\gamma_t$.}
% \end{itemize}

% \fahrbach{Is $\gamma_t$ defined as the shift in $\theta_t^*$ or $\theta_t$? Both options are conveyed in the bullet points.}

% \fahrbach{Should we give a standalone algorithm box for linear regression to make the choice of $m_{t\varepsilon}$ and $v_{t\varepsilon}$ clear in practice?}

% \begin{tcolorbox}
% {\color{red}{
% Assuming access to $\gamma_t = \|\theta^*_{t+1}-\theta^*_t\|$, construct sequence $u_t$ as follows:
% \begin{enumerate}
%     \item Set $u_1 = \|\theta_1-\theta^*_1\|^2$ (or maybe try other starting points) and fix $\kappa$ (say $\kappa = 1/\max_t B_t$)
%     \item For $t=1,2,\dotsc, \frac{T}{\kappa}-1$
%     \begin{align}
%         u_{t+1} = u_t -\kappa \frac{u_t^2 B_{\lceil t\kappa\rceil}}{(d+1)u_t+\sigma^2 d}+ 2\rev{\kappa}\gamma_{\lceil t\kappa\rceil} \sqrt{u_t}
%     \end{align}
% \end{enumerate}
% Then set step size $\eta_t$ as follows: For $t=1,2,\dotsc, T$
% \[
% \eta_t = \frac{u_{\lceil t/\kappa\rceil}}{(d+1)u_{\lceil t/\kappa\rceil}+\sigma^2d} B_t.
% \]
% }}
% \end{tcolorbox}

% \begin{tcolorbox}
% {\color{blue}{
% \begin{itemize}
% \item Set $u_1 = \|\theta_1-\theta^*_1\|^2$, and some initial step size $\eta_1$
% \item For $t= 1, 2, \dots$
% \begin{itemize}
%     \item For $k = 1, \dotsc, 1/\kappa$
%     $$ u = u - \kappa\left(\frac{d+1}{B_t}\eta_t^2 - 2\eta_t\right)u+\kappa \frac{\sigma^2 d}{B_t}\eta_t^2+2\kappa \gamma_t \sqrt{u}$$
% \end{itemize}
% \item Set $$\eta_{t+1} = \min(\frac{u B_{t+1}}{(d+1)u+\sigma^2 d}, C)$$
% \end{itemize}
% for some constant $C$ (maximum step size).
% }}
% \end{tcolorbox}

% \begin{tcolorbox}
% {\color{teal}{
% \begin{itemize}
% \item Set $u_1 = \|\theta_1-\theta^*_1\|^2$
% \item For $t= 1, 2, \dots$
% \begin{itemize}
%     \item For $k = 1, \dotsc, 1/\kappa$
%     $$r =  \min\left(\frac{u B_{t}}{(d+1)u+\sigma^2 d}, C\right)$$
%     $$ u = u + \kappa\left(\frac{d+1}{B_t}r^2 - 2r\right)u+\kappa \frac{\sigma^2 d}{B_t}r^2+2\kappa \gamma_t \sqrt{u}$$
% \end{itemize}
% \begin{itemize}
% \item Set $$\eta_{t} = \min\left(\frac{u B_{t}}{(d+1)u+\sigma^2 d}, C\right)$$
% \end{itemize}
% \end{itemize}
% for some constant $C$ (maximum step size).
% }}
% \end{tcolorbox}

Figure~\ref{fig:LinReg_shift} exhibits illustrations of the learning rate schedule $\eta_t^*$ given by Algorithm~\ref{alg:linear}:
\begin{itemize}
    \item {\bf Bursty shifts.} Figure~\ref{fig:LinReg_jump} corresponds to the setting where $\gamma_t$ follows a jump process. At the beginning of each episode (40 steps each), $\gamma_t$ jumps to a value $s$ and then becomes zero for the rest of the episode.
    Therefore, the distribution remains the same within an episode but then switches to another distribution in the next episode.
    In this case, we see that the learning rate restarts at the beginning of each episode with a more aggressive learning rate (capped at $\varepsilon = 0.1$)
    but then decreases within the episode as there is no shift.
    
    \item {\bf Smooth shifts.} Figure~\ref{fig:LinReg_smooth} illustrates the setting where $\gamma_t$ changes continuously as $\gamma_t = 1/t^{\alpha}$
    for a constant value $\alpha$.
    We see that a smaller value of $\alpha$
    (i.e., larger distribution shift) induces a larger learning rate.
\end{itemize}

\begin{figure*}
    \centering
    \hspace{-0.75cm}
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/shift_jump.pdf}
    \caption{Bursty distribution shifts}\label{fig:LinReg_jump}
    \end{subfigure}
    \hspace{0.15cm}
    \begin{subfigure}[b]{0.45\textwidth}
    %\raisebox{0.14cm}{
        \includegraphics[width=\textwidth]{figures/shift_power.pdf}
    %}
    \caption{Smooth distribution shifts}\label{fig:LinReg_smooth}
    \end{subfigure}
    
    \caption{Behavior of the learning rate schedules $\eta_t^*$ devised in Algorithm~\ref{alg:linear} for online linear regression.
    The batch size is $B_t = 100$ for all $1 \le t \le 200$,
    dimension $d = 100$, max step size $\eps = 0.1$, and $\sigma = 2$.}
    \label{fig:LinReg_shift}
\end{figure*}

\subsection{Case study: No distribution shift}
To  build further insight about the proposed schedule,
we study the behavior of \Cref{alg:linear}
when there is no shift in the data distribution
and the batch size is the same across SGD iterations.
Note that in this case $Y'(\tau) = 0$ and $\nu(\tau) = B/\eps$.
The behavior of the learning rate schedule $\eta^*_t$ is described in the next lemma.  

\begin{lemma}\label{lem:no-shift-Lin}
Consider the following ODE:
\begin{align}\label{eq:tv-n}
    \tilde{v}'_\tau =
    &\Big({\sf a}\zeta(\tau)^2 -2\zeta(\tau)\Big) \tilde{v}_\tau
     + {\sf b}\zeta(\tau)^2\,,\quad {\sf a}:=
     \eps\frac{d+1}{B}, \quad {\sf b}:=
     \eps\frac{\sigma^2 d}{B},
\end{align}
with optimal $\zeta(\tau)$ given by~\eqref{eq:policy}. Define 
\begin{align*}
\tau_*&:= \left[\frac{1}{2-{\sf a}}\log\left((1-{\sf a}) \left(\tilde{v}_0 \frac{2-{\sf a}}{{\sf b}} - 1\right)\right)\right]_+\,,\\
C &= {\sf a}\ln\Big(\frac{1-{\sf a}}{{\sf b}}\Big) +1-{\sf a} -\tau_*\,.
\end{align*}
We then have the following:
\begin{itemize}
    \item If $\tau\le \tau_*$, then
    \begin{align*}
    \tilde{v}_\tau = \left(\tilde{v}_0 - \frac{{\sf b}}{2-{\sf a}}\right) e^{-(2-{\sf a})\tau}+\frac{{\sf b}}{2-{\sf a}}, \quad
    \zeta(\tau)  = 1\,.
    \end{align*}
    \item As $\tau\to \infty$ we have
    \begin{align*}
    \lim_{\tau\to\infty}\frac{\tilde{v}_\tau}{\frac{b}{\tau+C}} = 1\,,\quad
    \lim_{\tau\to\infty} \frac{\zeta(\tau)}{\frac{1}{{\sf a}+C+\tau}}  = 1.
    \end{align*}
\end{itemize}
\end{lemma}

Recalling the relation $\eta_t = \eps \zeta(\eps t)$ and using Lemma~\ref{lem:no-shift-Lin}, we have
$\eta^*_t = \eps$ for $t\le t_*:=\lceil \tau_*/\eps\rceil$ and 
\[
\lim_{t\to\infty} \frac{\eta^*_t}{\frac{\eps}{{\sf a}+ C+\eps t}}  = 1.
\]
In words, $\eta^*_t$ asymptotically has the rate $1/t$. In Figure~\ref{fig:LinReg_LR}, we plot an example of processes $\tilde{v}_\tau$ and the optimal learning rate $\eta^*_t$ for a linear regression instance without any distribution shift.

\begin{figure*}
    \centering
    %\hspace{-0.75cm}
    \begin{subfigure}[b]{0.434\textwidth}
    \includegraphics[width=\textwidth]{figures/v_til.pdf}
    %\caption*{(a)}
    \end{subfigure}
    \hspace{0.18cm}
    \begin{subfigure}[b]{0.45\textwidth}
    %\raisebox{0.14cm}{
        \includegraphics[width=\textwidth]{figures/eta.pdf}
    %}
    %\caption*{(b)}
    \end{subfigure}
    
    \caption{Behavior of the process $\tilde{v}_\tau$ defined by ODE~\eqref{eq:tv-n} when there is no distribution shift (left).
    Here we have $\eps = 0.1$, ${\sf a}:=\eps(d+1)/B = 0.1$, ${\sf b}:= \eps \sigma^2 d/B = 0.3$, and initialization $\tilde{v}_0 = 1$.
    Behavior of the learning rate schedule $\eta^*_t$ given by Algorithm~\ref{alg:linear}, which asymptotically has the rate $1/t$ (right).}
    \label{fig:LinReg_LR}
\end{figure*}