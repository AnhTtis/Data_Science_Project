\section{Introduction}

A fundamental question when training neural networks is how much of the weight space to explore and when to stop exploring.
For stochastic gradient descent (SGD)-based training algorithms, this is primarily governed by the learning rate.
If the learning rate is high, then we explore more of the weight space and vice versa.
Learning rates are typically decreased over time
in order to converge to a local optimum,
and there is now a substantial literature focused on how fast learning rates should decay for fixed data distributions
(see, e.g.,~\citet{pmlr-v75-tripuraneni18a} and \citet{JMLR:v19:17-370}, and the references therein).

However, what should we do if the data distribution is constantly changing?
This is the case in many {large-scale} online learning systems where
(1) the data arrives in a stream,
(2) the model continuously makes predictions and computes the loss, and
(3) it always updates its weights based on the new data it sees~\citep{anil2022factory}.
The goal of such a system is to always keep the loss low.
In this setting, convergence is less of a priority since
the model needs to be able to adapt to distribution shifts.
Intuitively, if the loss landscape is consistently changing
(either gradually or due to infrequent sudden spikes),
then it is sensible for the model to always explore its weight space.
We formalize this idea in our work.

Such an investigation naturally leads to the question of how high the learning rate should be,
and what an optimal learning rate schedule is in an online learning scenario?
These questions are critical because while tuning the learning rate can lead to improved accuracy in many applications,
it can also make the online learner widely inaccurate if
the wrong learning rate is used as the distribution changes.

\begin{figure*}
    \centering
    \includegraphics[width=0.24\textwidth]{figures/warmup-stepsize-0.003.png}
    \hfill
    \includegraphics[width=0.24\textwidth]{figures/warmup-stepsize-0.01.png}
    \hfill
    \includegraphics[width=0.24\textwidth]{figures/warmup-stepsize-0.03.png}
    \hfill
    \includegraphics[width=0.24\textwidth]{figures/warmup-stepsize-0.1.png}
    \caption{SGD trajectories for online linear regression with different constant learning rates.
    The discrete blue spirals are the optimal model weights $\theta_{t}^* \in \mathbb{R}^{2}$,
    which start at $(1,0)$ and jump clockwise
    every $100$ steps.
    The orange paths are the learned weights $\theta_t$,
    starting at $\theta_0 = 0$
    for $0 \le t \le 17 \cdot 100$.
    The orange squares depict the position every $100$ steps.
    We use batch size $B_t = 1$
    and step sizes
    $\eta_{t} \in \{0.003, 0.01, 0.03, 0.1\}$ from left to right.
    The rightmost SGD is the most out of control,
    but it incurs the least regret
    because it adapts to changes in $\theta_{t}^*$ the fastest
    without diverging.
    }
    \label{fig:warmup_spirals}
    \vspace{-0.1cm}
\end{figure*}

Formally, we study learning rate schedules in the presence of distribution shifts by considering \emph{dynamic regret}, a well-known notion in online optimization that measures the performance against a dynamic comparator sequence.
This regret framework captures the lifetime performance of an online learning system that makes predictions on incoming examples as they arrive (possibly from a time-varying distribution)
before using this data to update its weights. 
    
Our main contributions can be summarized as follows:

%\begin{enumerate}
%\item \textbf{Linear regression.}
\paragraph{Linear regression.}
We consider a linear regression setup with time-varying coefficients $\{\theta^*_t\}_{t \ge 1}$, which are chosen upfront by an adversary such that $\|\theta^*_t-\theta^*_{t+1}\|_{2} \le \gamma_t$ for a sequence of positive numbers $\{\gamma_t\}_{t\ge 1}$.  The variation in the model coefficients results in response shift (while the covariates distribution remains the same across time). We consider a learner who updates their model estimates via mini-batch SGD with an adaptive step size sequence $\{\eta_t\}_{t\ge 1}$ chosen in an online manner (i.e., only with access to previous data points). We derive a novel stochastic differential equation (SDE) that approximates the dynamics of SGD under distribution shift, and by analyzing it, we derive the optimal learning rate schedule.

%\item \textbf{Convex loss functions.}
\paragraph{Convex loss functions.}
We generalize our problem formulation along the following directions: $(i)$ We consider general convex loss functions $\ell(\th,z)$ that measure the loss of a model $\th\in\reals^p$ on the data point $z\in\reals^d$. $(ii)$ At each step the learner observes a batch of data points $\{z_{t,k}\}$ drawn from a time-varying distribution $P_t$, which means it can model both response shift and covariate shift. $(iii)$ An adversary can choose the distributions $P_t$ adaptively at each step by observing the history (i.e., the data and model estimates from previous rounds), in contrast to the linear regression setup where the sequence of models are time-varying but fixed a priori.  For strongly convex loss functions, we give a lower bound for the total expected regret that is of the same form as our upper bound and differs only in the constants, demonstrating that our regret analysis is nearly tight. We then propose a learning rate schedule to minimize the derived upper bound on the regret. This schedule is adaptive, resulting in a time-dependent learning rate that tries to catch up with the amount of distribution shift in the moment. We refer to Section~\ref{sub:lit} for a  detailed comparison to the literature on online convex optimization in dynamic environments.
    
%\item \textbf{Non-convex loss functions.}
\paragraph{Non-convex loss functions.}
For settings with non-convex loss functions, we modify the notion of regret to use the gradient norm of the estimated model. We derive an upper bound for the expected cumulative regret and propose a learning rate schedule that minimizes it.
In our experiments in \Cref{app:cyto},
we use neural networks and dynamic learning rates to
continuously classify cells arriving in a stream of
small condition RNA data \citep{Bastdas-Ponce}.
This work simulates an online and deep learning-based
\emph{flow cytometry} algorithm.
We refer the reader to \citet{LMC19} for more details about this application.
One take-away message from our analysis and experiments
in all three settings is that an optimal learning rate schedule
typically increases in the presence of distribution shift. 

The organization of the paper is as follows.
In \Cref{sub:lit}, we proceed with a literature review.
In \Cref{sub:res}, we present an overview of our tools,
techniques, and informal statements of our theoretical results.
We formally define the problem in \Cref{sec:formulation}.
We present our results for linear regression in \Cref{sec:linear-regression},
convex losses in \Cref{sec:cvx}, and non-convex losses in \Cref{sec:nonconvex}.
In \Cref{sec:experiments},
we present experiments to study the effect
of the proposed learning rate schedules,
including high-dimensional regression and a
medical application to flow cytometry.
We defer the proofs of our technical results to the appendix.

\subsection{Related work}
\label{sub:lit}
With deep neural networks now being used in countless applications and SGD remaining the dominant algorithm for training these models, there has been a surge of effort to understand how learning rates affect the behavior of stochastic optimization methods~\citep{Bengio,Smith}. Most of the existing literature, however, assumes no shift in the underlying distribution across the iterations of SGD. Various trade-offs between learning rate and batch size have been studied~\citep{keskar2016large,smith2018don}. In particular, \citet{smith2018don} propose that instead of the decaying learning rate, one can increase the batch size during training and empirically show that it results in near-identical model performance with significantly fewer parameter updates.
\citet{shi2020learning} analyze the effect of learning rate on SGD by studying its continuum formulation given by a stochastic differential equation (SDE) and show that for a broad class of losses, this SDE converges to its stationary distribution
at a linear rate, further revealing the dependence of a linear convergence rate on the learning rate. Learning rate schedules for SGD, under fixed distribution, and for the setting of least squares has been studied in~\citep{ge2019step,jain2019making}.
Decaying learning rate via cyclical schedules has also
been proposed for training deep neural models (see, e.g.,~\citet{Smith,loshchilov2016sgdr,li2019exponential}).

The effects of SGD hyperparameters (e.g., batch size and learning rate) have also been studied for the adversarial robustness of the resulting models \citep{yao2018hessian,kamath2020sgd}.
In this setting, a model is trained on unperturbed samples, but at test time the sample features are slightly perturbed. In contrast, this paper considers settings where \emph{the data distribution is constantly changing}---even during training---and studies the effect of learning rates in presence of such distribution shifts.

\paragraph{Connections to online optimization.}
The notion of dynamic regret has been used in online convex optimization to evaluate the performance of a learner against a dynamic target, as opposed to the classical single best action in hindsight~\citep{zinkevich2003online,yang2016tracking,jadbabaie2015online,besbes2015non,bedi2018tracking}. In this setting, nature chooses a sequence of convex functions $f_1,f_2,\dots, f_T$ and the learner chooses a model (i.e, action) $\theta_t$ at each step and incurs loss~$f_t(\theta_t)$.
Our problem is closest to the works of~\citet{besbes2015non} and \citet{bedi2018tracking}, in which the learner only has noisy access to gradients $\nabla f_t(\theta_t)$. There is often a notion of variation to capture the change in the comparator.
For example,~\citet{yang2016tracking} consider ``path variation'', which measures how fast the minimizers of the sequence of loss
functions change;
\citet{besbes2015non} defines a ``functional variation''
based on the supremum distance between consecutive loss functions;
and \citet{bedi2018tracking} track the ``path length'' between minimizers
(i.e., what we call \emph{distribution shift} in this work).

\citet{yang2016tracking} bound the cumulative dynamic regret when a constant step size $\eta\propto\sqrt{\mathcal{V}_T/T}$ is used, where $T$ is the horizon length and $\mathcal{V}_T$ is the variation budget that controls the power that nature has in choosing the sequence of loss functions (see Theorem~7 therein).
\citet{besbes2015non} propose a restarting procedure, which for batch size $\Delta_T$ restarts an online gradient descent algorithm every $\Delta_T$ periods. Their analysis suggests to take $\Delta_T = (T/\mathcal{V}_T)^{2/3}$ and $\eta \propto 1/\sqrt{\Delta_T}$ (see Theorem 3 therein).
\citet{bedi2018tracking} design and analyze the
inexact online gradient descent (IOGD) algorithm.

While these works also suggest that in a changing environment the learning rate should in general be set higher,
our formulation and analysis for the convex setting departs from these works in the following ways: $(i)$ Instead of constant or a pre-determined learning rate, our framework allows for \emph{adaptive schedules} where the learning rate at each epoch can be set based on the history; $(ii)$ The notion of dynamic regret is often defined with respect to an arbitrary but fixed sequence of loss functions satisfying a variation budget. In contrast, we allow the data distribution to shift adaptively at each step after observing the history, and so the expected loss changes adaptively over time;
$(iii)$ \citet{besbes2015non} and \citet{yang2016tracking} establish lower bounds on the dynamic regret, but these bounds are for the worst-case regret over the choice of loss function sequences that satisfy the variation budget. These lower bounds are obtained by carefully crafting a sequence that is hard to optimize in an online manner. However, there is a subtle difference in our setting---the loss function $\ell(\theta,z)$ is fixed and the change in expected loss over time comes from a shift in the data distribution~$z$.
The lower bound we derive for dynamic regret assumes
the same \emph{fixed loss function} $\ell(\theta,z)$.

\subsection{Overview of  techniques}
\label{sub:res}

To analyze the behavior of SGD in a linear regression setting,
we derive a novel stochastic differential equation (SDE) that approximates the dynamics of SGD in the presence of distribution shift. Using Gr\"{o}nwall's inequality~\citep{gronwall1919note}, we control the deviation of the SGD trajectory from the continuous process and relate the regret of SGD to the second moment of the continuous process, which we characterize using the celebrated It\^o's lemma from stochastic calculus~\citep{oksendal2013stochastic} (see Lemma~\ref{lem:Ito}).
Using this characterization, we derive an
optimal learning rate schedule in a sequential manner.

Our results for general convex loss functions are based on an intricate treatment of the regret terms, taking the expectation with respect to a proper filtration and applying several properties of convex functions and SGD  itself.

Non-convex loss functions can have a complicated landscape with potentially many local minima and saddle points. Even without distribution shifts, first-order methods like SGD are not guaranteed to converge to a global minimum. To deal with this, we modify the definition of regret to use the norm of the gradient of the loss for the estimated models. Thus, a trajectory that stays close to local minima of the loss functions has low total regret. To upper bound the cumulative regret in this setting, we follow a similar proof technique as in the convex case, but rely only on the SGD update formulation and first-order optimality conditions on the sequence of optimal weights $\{\theta^*_t\}_{t \ge 1}$.