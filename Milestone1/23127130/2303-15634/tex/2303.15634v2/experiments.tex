\section{Experiments}
\label{sec:experiments}

We use TensorFlow \citep{abadi2016tensorflow} and Keras~\citep{chollet2015keras} for the following experiments.\footnote{The source code is  available at \url{https://github.com/fahrbach/learning-rate-schedules}.}
In \Cref{subsec:high_dimensional_regression} we study high-dimensional regression, and
in \Cref{app:cyto} we explore an application of neural networks to flow cytometry.

\subsection{High-dimensional regression}
\label{subsec:high_dimensional_regression}

We use the learning rate schedules
in \Cref{alg:linear} and \Cref{propo:optimal-eta}
for linear and logistic regression, respectively.
We consider paths $\{\theta_{t}^*\}_{t = 1}^{T}$
such that for $\theta_{t}^* \in \mathbb{R}^d, i \in [d]$,
\begin{equation}
\label{eqn:theta_path_def}
    \theta_{t}^*(i) = \begin{cases}
        r_{a,b}(t)^3 \cos(\lceil i/2 \rceil 2k\pi \alpha(t)) & \text{if $i$ odd}, \\
        r_{a,b}(t)^3 \sin(\lceil i/2 \rceil 2k\pi \alpha(t)) & \text{if $i$ even}, \\
    \end{cases}
\end{equation}
where $r_{a,b}(t) = \texttt{linspace}(a,b,T)$ controls the radius,
$\alpha(t) = \texttt{linspace}(0,1,T)$, and $k$ is the base frequency.
These paths have linearly independent components due to their trigonometric
frequencies and phases
(useful for high dimensions),
and move at non-monotonic speeds if $a \ne b$.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
    \includegraphics[width=\textwidth]{figures/linear_continuous-vs-discrete_phase-1_sgd.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.31\textwidth}
    \includegraphics[width=\textwidth]{figures/linear_continuous-vs-discrete_phase-4_sgd.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.31\textwidth}
    \includegraphics[width=\textwidth]{figures/linear_continuous-vs-discrete_phase-16_sgd.png}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.31\textwidth}
    \includegraphics[width=\textwidth]{figures/linear_continuous-vs-discrete_phase-1_step-sizes.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.31\textwidth}
    \includegraphics[width=\textwidth]{figures/linear_continuous-vs-discrete_phase-4_step-sizes.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.31\textwidth}
    \includegraphics[width=\textwidth]{figures/linear_continuous-vs-discrete_phase-16_step-sizes.png}
    \end{subfigure}
    
    \caption{SGD trajectories of \Cref{alg:linear} (top);
    and oscillating learning rates $\eta_t$ as we
    discretize the path defined by $\theta_{t}^*$ where $\eta_{\max} = 0.5$ (bottom).}
    \label{fig:linear_regression_continuous_vs_discrete}
\end{figure*}

\subsubsection{Linear regression}
We start by investigating \Cref{alg:linear} for online least squares.
Setting $\theta_0 = 0$,
at each step $t$ we generate $X \in \mathbb{R}^{B_t \times d}$
for $x_{ij} \sim \normal(0, 1)$
and get back the response
$y = X \theta_{t}^* + \varepsilon$ for $\varepsilon_{i} \sim \normal(0, 0.1)$.

%\paragraph{Continuous vs discrete shifts.}
Consider the 2-dimensional trajectory in~\Cref{fig:linear_regression_continuous_vs_discrete}
defined by $r_{1,-1}(t)$, $k=4$, and $B_t=256$.
For $T=2000$, the path starts at 
$\theta_{1}^* = (1,0)$,
spirals into the origin,
and returns to $\theta_{T}^* = (-1,0)$.
To study the effect of \emph{continuous vs discrete distribution shifts},
we downsample the points by $\ell \in \{1,4,16\}$
to get the discretized paths
\[\hat{\theta}_{t}^{*} = \theta_{\lceil t / \ell \rceil \ell}^*,\]
for $t \in [T]$.
As $\ell$ increases
(i.e., from left to right in \Cref{fig:linear_regression_continuous_vs_discrete}),
the learning rate $\eta_t$ of \Cref{alg:linear} starts to oscillate---decreasing
when $\theta_t$ is near $\theta_t^*$
and returning to $\eta_{\max}=0.5$ when $\theta_{t}^*$ shifts.

Next, we increase the dimension $d$ and
plot the cumulative regret of~\Cref{alg:linear}
in \Cref{fig:regression-high-dimensions}.
We use the same $\ell=8$ discretized paths
and set $\eta_{\max} = 1/\sqrt{d}$.
Note that for all values of $d$,
the total regret increases, levels off,
and then increases again.
This corresponds to $\theta_{t}^*$ spiraling into the origin,
spending time there, and exiting.
The initial spike in regret is due to finding the $\theta_{t}^*$ path,
i.e., the first few steps when
$\theta_t$ moves from the origin to $\theta_{t}^*$.

\subsubsection{Logistic regression}
%\paragraph{Logistic regression.}

% Layout 1: Two rows, large figures
\begin{figure*}
    \centering
    \hspace{-0.75cm}
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/linear-high-dim-regret.png}
    %\caption*{(a)}
    \end{subfigure}
    \hspace{0.15cm}
    \begin{subfigure}[b]{0.445\textwidth}
    \raisebox{0.14cm}{
        \includegraphics[width=\textwidth]{figures/linear-high-dim-sgd-d128.png}
    }
    %\caption*{(b)}
    \end{subfigure}
    \\
    \hspace{-0.75cm}
    \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/logistic-high-dim-regret.png}
    %\caption*{(c)}
    \end{subfigure}
    \hspace{0.15cm}
    \begin{subfigure}[b]{0.445\textwidth}
    \raisebox{0.14cm}{
        \includegraphics[width=\textwidth]{figures/logistic-high-dim-sgd-d128.png}
    }
    %\caption*{(d)}
    \end{subfigure}
    
    \caption{Cumulative regret of \Cref{alg:linear} with $\eta_{\max} = 1/\sqrt{d}$
    for increasing dimensions $d$ (top-left);
    and the first and second coordinates of the SGD
    for $d=128$ and batch size $B_t = 256$ (top-right).
    Cumulative regret of \Cref{propo:optimal-eta} for $d$-dimensional logistic regression (bottom-left);
    and the first and second coordinates of the SGD
    for $d=128$ and batch size $B_t = 256$ (bottom-right).}
    \label{fig:regression-high-dimensions}
\end{figure*}

We also empirically study the learning rate schedule in~\Cref{propo:optimal-eta}
for $d$-dimensional logistic regression with binary cross entropy loss.
Similar to the linear regression experiments,
at each step $t$ we generate the covariates
$X \in \mathbb{R}^{B_t \times d}$,
but now we get back
$y = \text{sigmoid}(X \theta_{t}^* + \varepsilon)$.
We note that the learning rate schedule in \Cref{propo:optimal-eta}
is largely parameter-free for generalized linear models.
For example, setting $\sigma^{2}=d/4$ and $L=1/4$
minimizes the upper bound on the regret in \eqref{eq:Reg-UB}
for logistic regression with log loss,
so the only hyperparameter we set is $D_{\max} = d$.