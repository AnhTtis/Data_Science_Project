\section*{Conclusion}

This work explores learning rate schedules that minimize regret
for online SGD-based learning in the presence of distribution shifts.
We derive a novel stochastic differential equation to approximate
the SGD path for linear regression with model shifts,
and we derive new adaptive schedules for general convex and non-convex losses
that minimize regret upper bounds.
These learning rate schedules can increase in the presence of distribution shifts
and allow for more aggressive optimization.

For future works, we propose extending our SDE framework to develop adaptive adjustment schemes for other hyperparameters in SGD variants such as Polyak averaging \citep{polyak1992acceleration}, SVRG \citep{johnson2013accelerating},
and elastic averaging SGD \citep{zhang2015deep},
as well as deriving effective adaptive momentum parameter adjustment policies.
We also propose studying a ``model hedging'' question
to quantify how neutral a model should remain at a given time
to optimally trade off between
underfitting the current distribution
and
being able to quickly adapt to a (possibly adversarial) future distribution.
We believe this area of designing adaptive learning rate schedules
is a fruitful and exciting area that combines control theory,
online optimization, and large-scale recommender systems~\citep{anil2022factory,coleman2023unified}.