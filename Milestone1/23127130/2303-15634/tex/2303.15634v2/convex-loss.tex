\section{General convex loss}
\label{sec:cvx}

\subsection{Upper bound on the total regret}

Here we derive an upper bound on the total regret for general convex loss functions. We use this bound to study the behavior of optimal learning rates (by minimizing the regret upper bound) with respect to distribution shifts. 
We proceed by making the following assumption.
\begin{assumption}\label{ass:main}
Suppose that
\begin{itemize}
    \item[(i)] We have 
    $
    \E_{P_t}[\|\nabla\ell(\theta_t,z_{t,k}) - \nabla \ellbar_t(\theta_t)\|^2]\le \sigma^2,
    $
    for some parameter $\sigma \ge 0$. Since the data points in each batch are sampled i.i.d., this implies that
\[
\E_{P_t}\Big[\Big\|\frac{1}{B_t} \sum_{k=1}^{B_t} \nabla\ell(\theta_t,z_{t,k}) - \nabla \ellbar_t(\theta_t)\Big\|^2\Big]\le \frac{\sigma^2}{B_t}\,.
\]
\item[(ii)] We have $\nabla^2\ellbar_t(\theta)\preceq L I$ for $\theta\in \Theta$, or a weaker $L$-smooth condition
\[
    \|\nabla \ellbar_t(\theta_1) - \nabla \ellbar_t(\theta_2)\|\le L\|\theta_1 - \theta_2\|,
\]
for $\theta_1,\theta_2\in \Theta$.
\item[(iii)] We assume the oracle models $\theta_t^*$ are in $\Theta$ and that the diameter of $\Theta$ is bounded by $D_{\max}$. Alternatively, we assume that $\theta^*_t\in \Theta'$ for all $t$, and $D_{\max} = \max\{\|\theta - \theta'\|: \theta\in\Theta, \theta'\in\Theta'\}$.
\end{itemize}
\end{assumption}
Note that for all steps $t$, $\nabla\ell(\theta_t,z_{t,k})$ is an unbiased estimator of $\nabla\bar{\ell}_t(\theta_t)$ and Assumption $(i)$ bounds its variance. Assumption $(ii)$ is for technical analysis and is satisfied if the loss function has a continuous Hessian.
Assumption $(iii)$ assumes that the oracle models $\theta^*_t$ remain in a bounded set as $t$ grows. Since in practice the SGD is run for a finite number of iterations,
this is not a restricting assumption, e.g.,
$D_{\max}$ can depend on the horizon length $T$.

\begin{thm}\label{thm:convex-reg}
 Suppose the loss function $\ell(\theta,z)$ is convex in $\theta$, and assume that the oracle model~$\theta^*_t$ and the learning rate $\eta_t$ are adapted to the history $\bz_{t-1}$, defined by~\eqref{eq:history}.
 Let $D_t: = \|\theta_t^*-\theta_t\|$ and $a_t: = 2\eta_t - L\eta_t^2 > 0$ for $t\ge 1$. Under Assumption~\ref{ass:main}, and assuming $\eta_t\le \frac{1}{L}$, for all $t\ge 1$, the 
following bound holds on the total regret of SGD:
\begin{align}\label{eq:Reg-UB0}
\E[\Reg(T)] &\le
 \sum_{t=1}^T\E\Bigg[ \left(\frac{D_t^2}{a_t} - \frac{D_{t+1}^2}{a_{t}}\right) + \frac{\sigma^2\eta_t^2}{B_ta_t}
 + \frac{\|\theta^*_t - \theta^*_{t+1}\|^2}{a_t} + \frac{2}{a_t}\<\theta^*_t - \theta^*_{t+1}, \theta_{t+1}-\theta^*_t\>\Bigg]\,.
\end{align}
Here, the expectation is with respect to the randomness in data points observed in the $T$ steps.
\end{thm}
We next discuss how the regret bound~\eqref{eq:Reg-UB0} can be used to derive optimal learning rate schedules.
We would like to derive optimal rates $\eta^*_t$ by minimizing the bound~\eqref{eq:Reg-UB0} in a sequential manner. However, the bound depends on $D_t$ and $\theta_{t+1}$,
which are not observable. Indeed, $\theta_{t+1}$ is defined at step $t+1$ where $\eta_t$ should have already been determined. To address this issue, we use the fact that the projected SGD updates remain in the set $\Theta$ and by invoking Assumption $(iii)$, we have $D_t\le D_{\max}$ and $\|\theta_{t+1}-\theta^*_t\|\le D_{\max}$. Also recall our notation $\gamma_t = \|\theta^*_t - \theta^*_{t+1}\|$ for the distribution shift. Therefore, by rearranging the terms in~\eqref{eq:Reg-UB0} and telescope summing over $1/a_t$, we have
\begin{align}\label{eq:Reg-UB}
    \E[\Reg(T)] &\le
    D_{\max}^2 \E \left[ \frac{1}{a_1} +
    \sum_{t=2}^T \left(\frac{1}{a_t} - \frac{1}{a_{t-1}}\right)_+ \right]
    +\sum_{t=1}^T\E\left[\frac{1}{a_t} \left(\frac{\sigma^2\eta_t^2}{B_t} + \gamma_t^2 + 2D_{\max}\gamma_t\right)\right],
\end{align}
where $x_+ = \max(x,0)$ indicates the positive part of $x$.

We next discuss the choice of learning rates that minimizes the upper bound \eqref{eq:Reg-UB} in a sequential manner. Conditioned on $\bz_{[t-1]}$, the optimal $\eta_t$ is given by
\begin{align}\label{eq:etat*0}
    \eta_t^*:= \underset{0\le \eta\le \frac{1}{L}}{\text{argmin}} \bigg\{&D_{\max}^2
    \left(\frac{1}{2\eta-L\eta^2} - \frac{1}{2\eta_{t-1}-L\eta_{t-1}^2}\right)_+
    +\frac{\sigma^2}{B_t}\cdot\frac{\eta^2}{2\eta-L\eta^2}+ \frac{\gamma_t^2+2D_{\max}\gamma_t}{2\eta-L\eta^2}\bigg\}\,.
\end{align}
Our next proposition characterizes $\eta_t^*$.
\begin{propo}[Learning rate schedule]
\label{propo:optimal-eta}
Define the thresholds $\tau_{1,t}$ and $\tau_{2,t}$ as follows:
\begin{align}
    \tau_{1,t}&:= \tfrac{B_t}{2\sigma^2}\left(\sqrt{b_{1,t}^2L^2+\tfrac{4\sigma^2}{B_t} b_{1,t}} - b_{1,t}L\right),  \label{eq:tau1}\\
    \tau_{2,t}&:= \tfrac{B_t}{2\sigma^2}\left(\sqrt{b_{2,t}^2L^2+\tfrac{4\sigma^2}{B_t} b_{2,t}} - b_{2,t}L\right), \label{eq:tau2}\\
    b_{1,t} &:= \gamma_t^2+2D_{\max}\gamma_t,\quad  b_{2,t} := (\gamma_t+D_{\max})^2\,.\nonumber 
\end{align}
The optimal learning rate $\eta_t^*$ defined by~\eqref{eq:etat*0} is given by:
\begin{align}
    \eta^*_t = \begin{cases}
    \tau_{1,t} & \text{if } \eta_{t-1}^* \le \tau_{1,t},\\
    \eta_{t-1}^* & \text{if } \tau_{1,t} \le \eta_{t-1}^* \le \tau_{2,t}\\
    \tau_{2,t} & \text{if } \eta_{t-1}^* \ge \tau_{2,t}\,.
    \end{cases}
\end{align}
\end{propo}

\begin{remark}
The proposed learning rate in \eqref{eq:etat*0} depends on $\sigma$, $L$ and shifts $\gamma_t$.
Having access to the loss function $\ell(\theta,z)$, the learner can use sample estimates for $\sigma$, $L$. Also note that we can use any upper bound on $\gamma_t$ in the bound~\eqref{eq:Reg-UB} and obtain a similar schedule.
Of course, if the upper bound is crude, it results in a conservative learning rate schedule. In settings where an upper bound on the shifts $\gamma_t$ is not available, we estimate $\gamma_t$ using an exponential moving average of the drifts in the consecutive estimated models $\theta_t$, namely $\hat{\gamma}_t = \beta \hat{\gamma}_{t-1} + (1-\beta)\|\theta_t- \theta_{t-1}\|$, with a factor $\beta\in(0,1)$.
%$\hat{\gamma}_t  = \frac{1}{W}\sum_{k=t-W+1}^{t}\|\theta_k  - \theta_{k-1}\|$, for a window size $W$.
\end{remark}

\begin{remark}\label{rmk:opt-sch}
The values $b_{1,t}$ and $b_{2,t}$ in~\eqref{eq:tau1} and \eqref{eq:tau2} are increasing in the distribution shift $\gamma_t$ and it is easy to see that the thresholds $\tau_{1,t},\tau_{2,t}$ are also increasing in $\gamma_t$. As a result for every value of $\eta_{t-1}$, higher distribution shift $\gamma_t$ increases the optimal learning rate $\eta^*_t$.
\end{remark}

Note that Theorem~\ref{thm:convex-reg} and Remark~\ref{rmk:opt-sch} are optimized with respect to the upper envelope of the optimal regret. We also prove a corresponding lower envelope result for SGD. 

\subsection{Lower bound on the total regret}

 The learning rate schedule in \ref{propo:optimal-eta} is optimized with respect to the upper bound derived for the cumulative dynamic regret. We next prove a corresponding lower bound result for SGD, which matches the upper bound and only differs by constants.
 Thus, our analysis of the optimal learning rate schedules for SGD is tight up to constants.

Before we begin, we make an additional assumption.

\begin{assumption}\label{as:subexp0}
We assume that the loss function $\ell(\theta,z)$ is $\mu$-strongly convex in $\theta$, for some $\mu>0$, i.e., $\ell(\theta) - \frac{\mu}{2}\|\theta\|^2$ is convex in $\theta$. 
\end{assumption}

\begin{thm}\label{thm:le-convex-reg}
 Suppose the oracle model $\theta^*_t$ and the learning rate $\eta_t$ are adapted to the history $\bz_{t-1}$, defined by~\eqref{eq:history}. Let $D_t := \|\theta^*_t - \theta_t\|$, $\gamma_t := \|\theta^*_t - \theta^*_{t+1}\|$, and $a'_t: = 2(\eta_t+\frac{L}{\mu}\eta_t - \eta_t^2L)$. Under Assumptions~\ref{ass:main} and~\ref{as:subexp0}, and assuming $\eta_t\le \frac{1}{\mu}$, for all $t\ge 1$, we have the following bound on the total regret of the batch SGD:
%$a_t': = \frac{c}{D_{max}^\alpha}\left(2\eta_t-\frac{\eta_t^2}{c}\right)$
\begin{align}\label{eq:Reg-LB}
\E[\Reg(T)]&\ge
 \sum_{t=1}^T\E\bigg[ \left(\frac{D_t^2}{a'_t} - \frac{D_{t+1}^2}{a'_{t}}\right) + \frac{\sigma^2\eta_t^2}{B_ta'_t} +\frac{\|\theta^*_t-\theta^*_{t+1}\|^2}{a'_t}  +\frac{2}{a_t'}\<\theta^*_t-\theta^*_{t+1},\theta_{t+1}-\theta^*_t\>\bigg],
\end{align}
where the expectation is with respect to the randomness in data points observed in the $T$ steps.
\end{thm}

\noindent
Note that Equations~\eqref{eq:Reg-LB} and~\eqref{eq:Reg-UB0} have the same form and thus give upper and lower ``envelopes'' for the cumulative expected dynamic regret under these assumptions.

One possible interpretation of terms in bounds~\eqref{eq:Reg-UB0} and~\eqref{eq:Reg-LB} is a predator-prey setting, as follows. The predator is $\theta_t$ and the prey is $\theta^*_t$.
    The first term in \eqref{eq:Reg-UB0} can be rearranged as 
    \[
    \sum_{t=1}^T \left(\frac{D_t^2}{a_t} - \frac{D_{t+1}^2}{a_t}\right)
    =
    \frac{D_1^2}{a_1}- \frac{D_{T+1}^2}{a_T} +
    \sum_{t=2}^T D_t^2\left(\frac{1}{a_t} -\frac{1}{a_{t-1}}\right)\,.
    \]
    Recall that $D_t = \|\theta_t-\theta^*_t\|$ is the distance between the prey and the predator. If the predator moves closer to the prey, it reduces its regret. 
    The other terms in \eqref{eq:Reg-UB0} involve  $\theta^*_t-\theta^*_{t+1}$ and reflect the movement of $\theta^*_t$ (the prey).
    If the prey moves further, it is harder to follow and the regret increases.