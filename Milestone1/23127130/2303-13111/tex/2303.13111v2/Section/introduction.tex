\section{Introduction}
\label{sec:introduction}
% ------------------------------------------
Computer-aided diagnosis (CAD) systems have gained popularity in the healthcare sector, assisting radiologists in diagnosing and treating patients. Convolution neural networks (CNN) have shown remarkable advancements, improving the performance of CAD, particularly in medical image segmentation~\cite{cao2021swinunet,chen2021transunet}. Over the past decades, significant research efforts have focused on developing efficient and robust segmentation methods for medical images.
One of the most popular architectures for this task is U-Net~\cite{ronneberger2015u}, which employs an encoder-decoder structure and skip connections to capture the contextual information of the input image. Following the success of U-Net, many variants of U-Net have been proposed with various convolution-based blocks and different strategies for the skip connections, including ResUNet~\cite{he2016deep}, Y-Net~\cite{mehta2018net}, and V-Net~\cite{milletari2016v}, etc.

\input{Figure/fig_overview}
Recently, attention mechanism-based Transformers have shown promising superiority in the field of natural language processing~\cite{vaswani2017attention}.
Subsequent studies such as ViT~\cite{dosovitskiy2020an} and DeiT~\cite{touvron2020training} have demonstrated that the Transformer architecture can achieve state-of-the-art performance on versatile computer vision tasks.
Given the remarkable advance of Transformers in natural image recognition tasks, many researchers have investigated the effectiveness of various neural networks for medical image segmentation.
To name a few, TransUNet~\cite{chen2021transunet} proposed to employ a Transformer in the bottleneck of a U-Net architecture for global information communication.
Similarly, UNETR~\cite{hatamizadeh2022unetr} and CoTr~\cite{xie2021cotr} designed a hierarchical fusion of Transformer and CNN architecture.

Despite the recent developments in medical image segmentation through the use of Transformers, their heavy computational burden caused by the self-attention mechanism limits their practical application, particularly for 3D medical images, which require a considerable number of forward and backward passes~\cite{valanarasu2022unext}. In light of this, multi-layer perceptron (MLP)-based methods have regained interest in the research community, as they have demonstrated comparable performance with CNN and Transformers, without requiring the heavy self-attention mechanism~\cite{tolstikhin2021mixer}.
For instance, MLP-Mixer~\cite{tolstikhin2021mixer} enabled information communication by a series of MLPs, capturing long-range dependencies in the input data.
However, the effectiveness of MLP in volumetric medical image segmentation remains understudied.

This paper proposes PHNet, a novel \textbf{P}ermutable \textbf{H}ybrid \textbf{Net}work that combines both CNN and MLP for accurate volumetric medical image segmentation. PHNet employs an encoder-decoder architecture, with the encoder utilizing a 2.5D CNN structure that can capitalize on the inherent isotropy of medical images, while avoiding information loss in shallow layers by capturing the varying information density in different directions of volumetric medical images. The paper further proposes MLPP, a {M}ulti-Layer {P}ermute {P}erceptron module that can maintain the positional information while integrating global interdependence in a computationally-efficient manner. To enhance computational efficiency, token-group operations are introduced, which efficiently aggregate feature maps at a token level, reducing the number of computations required. This is the first attempt to investigate the effectiveness of combining CNN and MLPs in volumetric medical image segmentation. The proposed method is evaluated on two publicly available datasets, COVID-19 Lung CT Lesion Segmentation Challenge-2020 (COVID-19-20)~\cite{roth2022rapid} and Synapse Multi-Organ Segmentation~\cite{landman2015miccai}. Extensive experimental results validate that PHNet achieves state-of-the-art accuracy on both datasets, surpassing the winner in the MICCAI Covid-19-20 challenge. 
