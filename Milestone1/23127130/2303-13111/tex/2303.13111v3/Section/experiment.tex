\section{Experiments}
\subsection{Datasets and Evaluation Metrics}
We conduct experiments on two publicly available datasets: COVID-19-20~\cite{roth2022rapid} and Synapse~\cite{landman2015miccai}. 
COVID-19-20 is comprised of 249 unenhanced chest CT scans, with 199 samples designated for training and 50 samples for testing. All samples are positive for SARS-CoV-2 RT-PCR.
Synapse consists of 30 cases of CT scans, with 14, 4, and 12 cases designated for training, validation, and testing, respectively~\cite{cao2021swinunet}. 
For COVID-19-20, we use the official evaluation metrics from the challenge~\cite{roth2022rapid}, including Dice coefficient (Dice), Intersection over Union (IoU), Surface Dice coefficient (SD), Normalized Volume Difference (NVD), and Hausdorff Distance (HD). 
For Synapse, we follow \cite{chen2021transunet} to adopt Dice and HD as evaluation metrics.
The computational cost is measured with an input size of $192\times192\times48$ and a batch size of 1, in terms of FLOPS (G), Parameters (M), Peak Memory (G) and Throughput (samples/s). For details, please refer to \cite{hou2022vip}. 

\subsection{Implementation Details}
PHNet is implemented using PyTorch and MONAI~\cite{cardoso2022monai} framework and trained on an NVIDIA RTX 3090 GPU. 
For COVID-19-20, all images are interpolated into the voxel spacing of $ 0.74 \times 0.74 \times 5.00 \textup{mm}^3$. 
Three sub-volumes of $ 224 \times 224 \times 28$ are sampled from each scan. 
We train PHNet for a total of 250 epochs on Synapse and 450 epochs on COVID-19-20. For all experiments we adopt the AdamW optimizer~\cite{loshchilov2017decoupled} with an initial learning rate $lr = 10^{-3} \times \frac{\text{batch\_size}}{1024}$, as suggested by~\cite{hou2022vip}. 
The objective function is the summation of Dice loss and cross-entropy loss.
Except for the above, we follow baseline from~\cite{roth2022rapid} and~\cite{isensee2021nnunet} for the COVID-19-20 and Synapse datasets, respectively.


\input{Table/synapse_sota}
\input{Table/covid_sota}
\input{Figure/visualization}

\subsection{Comparisons with State-of-the-Arts} 
\subsubsection{Comparison methods.} 
We compare the proposed PHNet with CNN-based, Transformer-based, MLP-based methods, and foundation model. 
We list the details below. 
\begin{itemize}
\item\textit{CNN-based methods}, including V-Net~\cite{milletari2016v}, UNet~\cite{ronneberger2015u}, nnUNet~\cite{isensee2021nnunet}, and attention-UNet~\cite{schlemper2019attention}. 
These architectures feature a hierarchical contracting path for context aggregation and a symmetric expanding path for resolution recovery and precise localization.
\item\textit{Transformer-based methods}, including ViT~\cite{dosovitskiy2020vit}, UNETR~\cite{hatamizadeh2022unetr}, SwinUNETR~\cite{tang2022swinunetr}, TransUNet~\cite{chen2021transunet}, SwinUNet~\cite{cao2021swinunet}, CoTr~\cite{xie2021cotr}, and CTO-Net~\cite{lin2023rethinking}. 
These architectures can be classified into three categories: 1) classical Transformer (\ie, ViT), 2) encoder-decoder framework with pure Transformer blocks (\ie, SwinUNet), and 3) hybrid architectures with CNN and Transformer (\ie, UNETR, SwinUNETR, TransUNet, CoTr, CTO-Net).
\item\textit{MLP-based methods}, including UNext~\cite{valanarasu2022unext}, CycleMLP~\cite{chen2022cyclemlp}, MLP-Mixer (Mixer)~\cite{tolstikhin2021mixer}, ShiftMLP (Shift)~\cite{Lian_2021_ASMLP}, and WaveMLP (Wave)~\cite{tang2022wave}.
We only replace the MLPP module in PHNet with these alternatives for a fair comparison.
\item\textit{SAMed}~\cite{zhang2023customized} is built upon SAM~\cite{kirillov2023sam} which is a foundation model for semantic segmentation.
SAMed retains the same architecture as SAM, which features a ViT-based encoder, a prompt module, and a mask decoder. 
SAMed applies the LoRA~\cite{hu2021lora} finetuning strategy on target tasks.
\end{itemize}

\subsubsection{Quantitative comparisons.} 
We begin by evaluating the performance of our method in multi-organ segmentation on Synapse~\cite{landman2015miccai} in Table~\ref{Synapse_sota}. 
Result shows that our method achieves the highest average Dice score of 85.62$\%$ and lowest HD of 11.75, outperforming the SOTA methods. 
Specifically, our method achieves 1.64$\%$ and 8.77$\%$ Dice improvements over nnUNet~\cite{isensee2021nnunet} and UNet~\cite{falk2019unet}, whose backbones are built upon 3D CNNs. 
Compared to top-performing competitors of Transformer-based CTO-Net~\cite{lin2023rethinking}, UNETR~\cite{hatamizadeh2022unetr}, MLP-based UNext~\cite{valanarasu2022unext}, Wave~\cite{tang2022wave}, and SAMed~\cite{zhang2023customized}, our method also achieves better performance with 4.52$\%$, 6.05$\%$, 18.55$\%$, 0.53$\%$, 3.74$\%$ Dice gains, respectively. 
These results conform to our argument that PHNet obtains satisfying segmentation performance through effective local-to-global modeling.

We further evaluate the performance on COVID-19-20~\cite{roth2022rapid} and the official evaluation result is presented in Table~\ref{COVID-19-20_sota}. 
Compared to CNN-based methods, our method attains the highest scores in all metrics. This suggests that CNN-based approaches have limitations in long-distance context fusion.
Compared to Transformer-based methods, PHNet also achieves better performance, suggesting that our proposed hybrid structure design facilitates more adept feature learning. 
Compared to MLP-based methods, PHNet outperforms by a remarkable margin. 
This is partly because existing MLP-based methods are designed to segment on 2D slices which lose through-plane features, resulting in severe performance deterioration in challenging volumetric image segmentation tasks. 
The large performance gap between SAMed~\cite{zhang2023customized} and PHNet indicates the constrained generalization capacity of foundation models within specific segmentation tasks.
Additionally, following~\cite{roth2022rapid}, we perform five-fold cross-validation and model ensemble using our proposed method.
The result demonstrates that our method achieves the highest dice score of 77.18\%, surpassing the performance of the top-12 solutions in this challenge\footnote{\url{https://covid-segmentation.grand-challenge.org/evaluation/challenge/leaderboard}}.

Additionally, our method is subjected to further evaluation on two other public datasets, namely the Liver Tumor Segmentation dataset (LiTS)~\cite{bilic2023liver} and the Medical Segmentation Decathlon (MSD) brain tumor segmentation (BraTS)~\cite{antonelli2022medical} dataset.  
The details are presented in the supplementary material.
Results show that our method outperforms other state-of-the-art methods on both datasets, demonstrating the credibility and wide-ranging applicability of our proposed method.


\subsubsection{Qualitative comparisons.}
Visual comparisons on Synapse~\cite{landman2015miccai} are shown in Figure~\ref{fig:visual}. 
PHNet achieves the best visual segmentation results compared to nnUNet~\cite{isensee2021nnunet}, TransUNet~\cite{chen2021transunet}, UNext~\cite{valanarasu2022unext}, and Mixer~\cite{tolstikhin2021mixer}.
As shown in the first row, the comparison indicates that our method achieves a better segmentation where the under-segmentation of pancreas is observed in nnUNet, TransUNet, and UNext. 
Moreover, as shown in the second row, PHNet can accurately delineate boundaries between liver and stomach, while the over-segmentation of liver is observed in both UNeXt and Mixer. 
Similarly, PHNet delineates a more precise boundary between pancreas and stomach as shown in the last row. 
The primary factor could be PHNet helps extract accurate contours by effective local-to-global modeling and aggregation of inter-axis and intra-axis token features. 
Overall, PHNet attains better segmentation results and mitigates the issues associated with under- and over-segmentation contours.


\subsection{Ablation Study} 
We conduct ablation studies on Synapse to validate the effectiveness and efficiency of each component in our method. 
We use the same decoder architecture for all variations.

\input{Table/MLPP_ablation_Synapse}
\input{Figure/visualization_abl_Synapse}
\subsubsection{Effectiveness of each components.}
To verify the effectiveness of core components in our approach, we increase each essential component gradually based on the vanilla MLP network (abbreviated as ``baseline''),
as shown in Table~\ref{tab:ablation_Synapse}. 
Compared with the baseline, the integration of axial decomposition yields a 1.49\% enhancement in Dice and a noteworthy improvement in efficiency. 
This is due to positional information encoding and complexity reduction.
There is an additional improvement of 0.65\% Dice through decomposition into in-plane and through-plane features, which is attributed to the amplified learning capacity of the module as it contains distinct learning parameters for in-plane and through-plane features~\cite{bertasius2021space}. Furthermore, AA-MLP contributes to 0.68\% performance gains with slightly increasing computational cost, and the whole framework achieves an 85.62\% Dice score. 
This is because intra-axis token communication enables better learning representation from larger receptive field, as shown in Figure~\ref{fig:visual_abl_Synapse}.
A similar trend can be observed in the evaluation of HD metric.
These results underscore the effectiveness of each component in our approach.

\input{Figure/fig_ablation}
\subsubsection{Impact of segment length.}
In Figure~\ref{fig:ablation}(a), we investigate the impact of different segment lengths $L$ in PHNet. 
Expressly, the segment length is set to various ratios of the width ($W$), \ie, $1$, $\frac{1}{2}$, $\frac{1}{3}$, and $\frac{1}{4}$, respectively. 
With a larger segment length, long-range dependencies can be more effectively captured in deep layers. 
Conversely, smaller segment length implies fewer adjacent tokens are grouped, emphasizing more on local information.
The best performance is achieved when $L=\frac{1}{2}W$, which provides a good balance between local information and long-range dependencies. 

\subsubsection{Impact of MLP layers.}
In Figure~\ref{fig:ablation}(b), we study the influence of a different number of MLP layers $K$ in our PHNet.
Results show that the best performance is achieved when the number of MLP layers is $2$. 
This observation may vary across datasets.
Our method consistently outperforms the baseline method across different configurations of MLP layers, affirming the robustness and stability of our method.

