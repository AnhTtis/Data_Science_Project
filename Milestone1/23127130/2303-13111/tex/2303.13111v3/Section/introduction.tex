\section{Introduction}
\label{sec:introduction}
% ------------------------------------------
Computer-aided diagnosis (CAD) systems have gained popularity in the healthcare sector, assisting radiologists in diagnosing and treating patients. Convolution neural networks (CNNs) have shown remarkable advancements, improving the performance of CAD, particularly in medical image segmentation (MedSeg)~\cite{cao2021swinunet,zhang2022deep,chen2021transunet}. Over the past decades, substantial research efforts have focused on developing efficient and robust MedSeg methodologies.
One of the most popular architectures for this task is UNet~\cite{ronneberger2015u}, which employs an encoder-decoder structure and skip connections to reserve both contextual and semantic information. Building upon the success of UNet, numerous variants have been proposed with various convolution-based blocks and different skip connections strategies, including ResUNet~\cite{he2016deep}, Y-Net~\cite{mehta2018net}, and V-Net~\cite{milletari2016v}, etc.

\input{Figure/fig_throughput}
Recently, Transformers with attention mechanism have shown promising superiority in the realm of natural language processing~\cite{vaswani2017attention}. 
Subsequent studies, such as ViT~\cite{dosovitskiy2020vit} and DeiT~\cite{touvron2020training}, have demonstrated remarkable capabilities in achieving state-of-the-art performance on versatile computer vision tasks.
Given the notable strides of Transformers in natural image recognition tasks, researchers have investigated the effectiveness of these neural networks for MedSeg.
To name a few, TransUNet~\cite{chen2021transunet} proposed to employ a Transformer in the bottleneck of a UNet architecture for global information communication.
Similarly, UNETR~\cite{hatamizadeh2022unetr}, CoTr~\cite{xie2021cotr}, and SwinUNet~\cite{cao2021swinunet} designed a hierarchical fusion of Transformer and CNNs architecture.

Despite the developments in MedSeg domain via Transformer methods, their heavy computational costs caused by the self-attention limit their practical application, particularly for 3D volumetric medical images, which necessitate a substantial number of forward and backward passes~\cite{valanarasu2022unext}. 
Consequently, multi-layer perceptron (MLP) has regained interest in the community, as it has demonstrated comparable performance with both CNNs and Transformers (in Figure~\ref{fig:throughput}), without requiring the heavy self-attention mechanism~\cite{tolstikhin2021mixer}.
For instance, MLP-Mixer~\cite{tolstikhin2021mixer} enabled information communication by a series of MLPs, capturing long-range dependencies in the input data.
However, the effectiveness of MLP in volumetric MedSeg remains understudied.

In this paper, we propose PHNet, a novel \textbf{P}ermutable \textbf{H}ybrid \textbf{Net}work that integrates the strengths of CNN and MLP for volumetric medical image segmentation. 
As illustrated in Figure~\ref{fig:overview}, PHNet embodies an encoder-decoder paradigm. 
Notably, the encoder utilizes a 2.5D CNN structure that capitalizes on the inherent isotropy of medical images, while avoiding information loss in shallow layers by capturing the varying information density in different directions of volumetric medical images. 
In PHNet, we further proposes MLPP, a {M}ulti-Layer {P}ermute {P}erceptron module that can maintain the positional information with the axial decomposition operation while integrating global interdependence in a computationally-efficient manner. To enhance computational efficiency, token-group operation is introduced, which efficiently aggregates feature maps at a token level, reducing the number of computations required. PHNet is evaluated on two publicly available datasets, COVID-19 Lung CT Lesion Segmentation Challenge-2020~\cite{roth2022rapid} and Synapse Multi-Organ Segmentation~\cite{landman2015miccai}.
Extensive experimental results validate that PHNet achieves state-of-the-art performance on both datasets, surpassing the winner in MICCAI COVID-19-20 challenge.
