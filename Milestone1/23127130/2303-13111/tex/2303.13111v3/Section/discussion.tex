\input{Figure/fig_discussion}
\subsection{Discussion}
\subsubsection{Comparisons with conventional architectures.} 
We conduct experiments to validate our analysis as shown in Fig.~\ref{fig:discussion}(a). 
Specifically, we replace the MLPP module with 3D CNN and Transformer blocks to assess their impact on efficiency and effectiveness.
For the Transformer component, we opt for the advanced Swin Transformer~\cite{tang2022swinunetr} due to the promising performance and notable reduction in computational cost through the use of shift windows.
Compared to CNN, our method achieves 1.36\% improvement in Dice, indicating that our proposed MLPP module more effectively encodes the global information, leading to enhanced segmentation accuracy.
Concerning computational efficiency, our method achieves a profile of 953G FLOPs, 24.37M parameters, 13.57G peak memory, and 1.73 samples/second throughput. 
These efficiency outcomes are comparable to those of 3D CNN, indicating that augment CNN performance while upholding efficiency. 
Although Transformer showcases competitive performance when configured with stacked blocks, our method shows significant superiority in terms of efficiency. This suggests that our MLPP is capable of capturing global information while eliminating the exhaustiveness of token comparison seen in self-attention. We also provide theoretical analysis for the computational cost in the supplementary material.

\subsubsection{Comparisons with architecture combinations.} 
We further conduct a comprehensive experimental analysis of various combinations of CNN, Transformer and MLP in both shallow and deep layers of the encoder. 
As depicted in Figure~\ref{fig:discussion}(b), the combination of CNN in shallow layers and MLP in deep layers achieves the best performance with an 85.62\% Dice score. 
This finding supports our argument that CNN excels in capturing local features, while MLP is more effective in modeling long-range dependencies. 
Interestingly, the inverse configuration, which employs MLP in shallow layers and CNN in deep layers, results in a significant decline in performance. 
This suggests that local information can be effectively extracted in shallow layers, while global information is better captured in deeper layers.
We also observe that the performance of Transformer architectures is underwhelming, which may be due 
to their high model complexity leading to overfitting on small datasets.