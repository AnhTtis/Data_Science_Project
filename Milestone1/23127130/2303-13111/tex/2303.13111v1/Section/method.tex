\input{Figure/MLPP}
\section{Method}
This paper presents PHNet, which follows an encoder-decoder architecture, as illustrated in Fig.~\ref{fig:mlpp}(a). The encoder consists of two primary components: a 2.5D convolution module (\emph{Sec.~\ref{sec2.1}}) and a Multi-Layer Permute Perceptron (MLPP) module (\emph{Sec.~\ref{sec2.2}}). The 2.5D convolutional stage extracts local features, and the output feature maps are passed to the MLPP module to learn global features. The decoder processes the hierarchical features for the final prediction. In the following sections, each component is explained in detail.

\subsection{2.5D Convolution} 
\label{sec2.1}
% ------------------------------------------
Based on the previous work on bias in medical image analysis~\cite{battaglia2018bias} and the anisotropic nature of volumetric medical images, we incorporate convolutional layers in the shallow layers of the encoder to extract local features. Volumetric images, such as CT and MRI scans, are often affected by anisotropic problems due to their thick-slice scanning~\cite{wang20192d5}, resulting in high in-plane (IP) resolution and low through-plane (TP) resolution~\cite{dong2022mnet,liu2018ahnet}. This discrepancy is particularly pronounced in COVID-19-20~\cite{roth2022rapid}, where the IP resolution is 0.74mm on average, whereas the TP resolution is only 5mm. To address this issue, we use 2D conv-blocks to capture the IP information until the feature is reformulated in approximately uniform resolution across all three axes: axial, coronal, and sagittal. Then, we apply 3D conv-blocks to handle the volumetric information. Each encoder layer consists of two residual convolution blocks, with each block comprising two sequential Convolution-Instance Normalization-Rectified Linear Unit (Conv-IN-ReLU) operations~\cite{ulyanov2016instancenorm}. The residual addition takes place before the final ReLU activation.

\subsection{Multi-Layer Permute Perceptron (MLPP)} 
\label{sec2.2}
 Although CNNs are capable of modeling long-range dependencies through deep stacks of convolution layers, studies~\cite{valanarasu2022unext,hou2022vip,tolstikhin2021mixer} have demonstrated that MLP-based networks are more effective in learning global context. 
Therefore, we design the MLPP module (Fig.~\ref{fig:mlpp}(b))to obtain global information in deep layers.
 MLPP decomposes the training of in-plane (IP) feature and through-plane (TP) feature in sequential order. 
 We denote these two blocks as \textbf{IP-MLP} and \textbf{TP-MLP}, respectively.
 To enable the communication of cross-axis tokens, we further propose an auxiliary attention branch in IP-MLP, which denotes as \textbf{AA-MLP}. We provide an example of the code based on the PyTorch in supplementary material.
 
\noindent\textbf{IP-MLP.} 
Given one slice of input feature maps $\mathbf{X}\in \mathbb{R}^{H\times W \times C}$, common MLP-based methods~\cite{liu2021gmlp,tolstikhin2021mixer} typically directly flatten $\mathbf{X}$ into a 1D vector, which loses the spatial information of the original conv-features~\cite{zhang2022morphmlp}.
Contrarily, as shown in Fig.~\ref{fig:mlpp}, we introduce axial decomposition in triplet pathways that separately process $\mathbf{X}$ in horizontal ($W$), vertical ($H$), and channel ($C$) axis, respectively.
This enables the preservation of precise positional information along other axes when encoding information along one axis.
Next, to balance the long-distance interaction and computation cost, and alleviate image resolution sensitivity problem~\cite{liu2022mlpsurvey}, we present a token segmentation operation\footnote{For more discussion, please refer to the supplementary material.} that splits the feature vector into multiple tokens, which can be efficiently processed by the following fully-connected (FC) layers.
We take the horizontal axis as an example. 
Instead of encoding the entire dimension, we split $\mathbf{X}$ into non-overlapping segments along the horizontal direction. 
Then, we set segment length to $L$ and thus obtain $\mathbf{X}_{i}\in \mathbb{R}^{L\times C}$, where $ i \in \{1,...,HW/L\} $. 
Similarly, we split each $\mathbf{X}_{i}$ into multiple non-overlapping groups along channel dimension, where each group has $g = C/L$ channels. 
Thus we obtain split segments, and each single segment is  $\mathbf{X}_{i}^{k}\in \mathbb{R}^{Lg}$, where $ k \in \{1,...,C/g\}$. 
Next, we flatten each segment and map $\mathbb{R}^{Lg} \mapsto \mathbb{R}^{Lg}$ by an FC layer to transform each segment, producing $\mathbf{Y}_{i}^{k}$. 
To recover the original dimension, we permute all segments back to $\mathbf{Y}_{W} \in \mathbb{R}^{H\times W \times C}$. 
Similarly, we conduct the same operations in the vertical pathway as above to permute tokens along the vertical direction, yielding $\mathbf{Y}_{H}$. 
To enable communication among groups along channel dimension, we add a parallel branch that contains an FC layer that maps $\mathbb{R}^{C} \mapsto \mathbb{R}^{C}$ to process each token individually, yielding $\mathbf{Y}_{C}$. 
Finally, we feed the element-wise summation of horizontal, vertical, and channel features into a new FC layer to attain the output, which can be formulated as: 
\begin{equation} \label{eqn:fusion2}
    \mathbf{Y}_{\text{IP}} = (\mathbf{Y}_H + \mathbf{Y}_W + \mathbf{Y}_C)\mathbf{W},
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{C\times C} $ denotes an FC weight matrix. 

\noindent\textbf{AA-MLP.} 
There are two limitations of the MLPP module that could potentially harm the segmentation performance.
First, the axial decomposition cuts off the direct interaction of tokens that are not in the same horizontal or vertical position.
Second, the token segmentation operation would suffer from a small local reception field compared with the vanilla MLP.
To address these limitations, we design an auxiliary branch to enable intra-axis token communication, which serves as an attention function via a lightweight yet effective MLP-like architecture.
Specifically, given an input slice of feature maps $\mathbf{X}\in \mathbb{R}^{H\times W \times C}$, we partition $\mathbf{X}$ into non-overlapping windows. 
We set window size to $L$ and thus obtain $\mathbf{X}_{i}\in \mathbb{R}^{L\times L}$, where $ i \in \{1,...,HWC/L^{2}\}$. 
Then we apply an FC matrix $\mathbf{W}\in \mathbb{R}^{L^{2}\times L^{2} }$ to transform each window and get $\mathbf{Y}_{i} \in \mathbb{R}^{L\times L}$. 
The final attention map $\mathbf{Y}_A \in \mathbb{R}^{H\times W\times C} $ is obtained by permuting all windows back to the original dimension. 
Finally, the feature maps $\mathbf{F}_{\text{IP}}$ of IP-MLP are obtained by performing residual attention~\cite{wang2017residual} of $\mathbf{Y}_{\text{IP}}$ and $\mathbf{Y}_A$.
\begin{equation} \label{eqn:fusion}
    \mathbf{F}_{\text{IP}} = (1+\mathbf{Y}_A) \odot \mathbf{Y}_{\text{IP}},
\end{equation}
where $\odot$ denotes element-wise multiplication.
 
\noindent\textbf{TP-MLP.} 
Once obtained the in-plane information from the IP-MLP, TP-MLP is applied to capture the long-term through-plane features.
Similarly, given input feature maps $\mathbf{F}_{\text{IP}}\in \mathbb{R}^{H\times W\times D \times C}$, we first split $\mathbf{X}=\mathbf{F}_\text{IP}$ along depth dimension into non-overlapping segments with segment length $L$. 
We thus obtain $\mathbf{X}_{i}\in \mathbb{R}^{L\times C}$, where $ i \in \{1,...,HWD/L\} $. 
Next, we split $\mathbf{X}$ into a couple of non-overlapping groups along channel dimension where each group has $g = C/L$ channels and get $\mathbf{X}_{i}^{k}\in \mathbb{R}^{Lg}$, where $ k \in \{1,...,C/g\}$. 
Then we flatten each segment and map $\mathbb{R}^{Lg} \mapsto \mathbb{R}^{Lg}$ by an FC layer, yielding $\mathbf{Y}_{i}^{k}$. 
Finally, we permute all segments $\mathbf{Y}_{i}^{k} \in \mathbb{R}^{Lg} $ back to original dimension and output $\mathbf{F}_{\text{TP}} \in \mathbb{R}^{H\times W\times D \times C}$.

\subsection{Decoder}
The decoder in our proposed method utilizes a pure CNN architecture, employing transpose convolution to progressively upsample the feature maps to match the input image resolution. Following the upsampling process, a residual convolution block is used to refine the feature maps. To further enhance segmentation accuracy, we include skip connections between the encoder and decoder, allowing for the preservation of low-level details. 