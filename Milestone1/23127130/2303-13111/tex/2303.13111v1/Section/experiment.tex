\section{Experiments}
\subsection{Datasets}
We conduct experiments on two publicly available datasets: COVID-19-20~\cite{roth2022rapid} and Synapse ~\cite{landman2015miccai}. 
COVID-19-20 is comprised of 249 unenhanced chest CT scans, with 199 samples designated for training and 50 samples for testing. All samples in this dataset are positive for SARS-CoV-2 RT-PCR.
Synapse consists of 30 cases of CT scans, with 14, 4, and 12 cases designated for training, validation, and testing, respectively~\cite{cao2021swinunet}. 
For COVID-19-20, we utilize the official evaluation metrics from the challenge~\cite{roth2022rapid}, which include Dice coefficient (Dice), Intersection over Union (IoU), Surface Dice coefficient (SD), Normalized Volume Difference (NVD), and Hausdorff Distance (HD). Meanwhile, following~\cite{chen2021transunet}, we adopt Dice and HD as evaluation metrics for Synapse.

\subsection{Implementation Details}
PHNet is implemented using PyTorch and MONAI\footnote{{\url{https://monai.io/}}} framework and trained on an NVIDIA RTX 3090 GPU. 
For COVID-19-20, all images are interpolated into the voxel spacing of $ 0.74 \times 0.74 \times 5.00 \textup{mm}^3$. 
Three sub-volumes of $ 224 \times 224 \times 28$ are sampled from each scan. 
We train PHNet in a total of 250 epochs for Synapse and 450 epochs for COVID-19-20. 
For all experiments we adopt the AdamW optimizer \cite{loshchilov2017decoupled} with an initial learning rate $lr = 10^{-3} \times \frac{\text{batch\_size}}{1024}$ and a weight decay of $5\times 10^{-2}$, as suggested by~\cite{hou2022vip}. Except for the above, we follow baseline from~\cite{roth2022rapid} and~\cite{isensee2021nnunet} for the COVID-19-20 and Synapse datasets, respectively.

\input{Figure/visualization}
\input{Table/covid_sota}
\input{Table/synapse_sota}
\subsection{Comparisons with State-of-the-art Methods}
We compare our method with three types of methods, \ie, CNN-based, Transformer-based, and MLP-based methods.
Quantitative results on COVID-19-20 and Synapse datasets are reported in Table~\ref{COVID-19-20_sota} and Table~\ref{Synapse_sota}, respectively. In both tables, PHNet* stands for PHNet w/o AA-MLP.
Qualitative results on Synapse~\cite{landman2015miccai} are shown in Fig.~\ref{fig:visual}, which demonstrate that the proposed method is able to generate more accurate and detailed segmentation results compared to other methods.

\noindent\textbf{Results on Lung Lesion Segmentation.} 
On COVID-19-20~\cite{roth2022rapid}, 
official evaluation results presented in Table~\ref{COVID-19-20_sota} show that our method outperforms the existing baselines and achieves the best score in all metrics.
Additionally, following~\cite{roth2022rapid}, we perform five-fold cross-validation and model ensemble using our proposed method.
The result demonstrates that our method achieves the highest dice score of 77.18$\%$, outperforming the top-5 solutions in this challenge\footnote{{\url{https://covid-segmentation.grand-challenge.org/evaluation/challenge/leaderboard}}}.

\noindent\textbf{Results on Multi-Organ Segmentation.} 
On Synapse~\cite{landman2015miccai}, Table~\ref{Synapse_sota} shows that our method achieves the highest average dice score of 85.54$\%$ and second lowest HD of 14.62, outperforming the SOTA methods. Notably, as for Dice, the distinct improvements can be markedly observed for organs with blurry boundaries, such as the “Gallbladder” and the “Stomach,”  where our model achieves significant gains over the SOTA methods, \ie, 7.73$\%$ and 2.95$\%$ in Dice, respectively.

\subsection{Ablation Studies}
We further conduct ablation studies on Synapse to validate each component in our method. For all models, we use the same decoder as our proposed method and fix the number of channels in each layer. The results are shown in Fig.~\ref{fig:abl}. 

\noindent\textbf{Comparisons with different architecture combinations.}
We compare the performance of different combinations of Conv, Attention, and MLP in shallow and deep layers.
For Conv and MLP, We adopt the same module as PHNet.
For Attention, we use Swin Transformer block \cite{tang2022swinunetr} and set window size equal to segment length for a fair comparison.
As shown in Fig.~\ref{fig:abl}(a), a combination of Conv in shallow layers and MLP in deep layers achieves the best performance, which conforms to our argument that Conv excels at extracting local features while MLP is more effective in modeling long-range dependencies.

\noindent\textbf{Comparisons with different MLP designs.} 
We compare the performance with related MLP-based variations in Fig.~\ref{fig:abl}(b), including MLP-Mixer (M)~\cite{tolstikhin2021mixer}, ShiftMLP (S)~\cite{valanarasu2022unext}, and WaveMLP (W)~\cite{tang2022wave}.
We only replace the MLPP module in our PHNet (P) with others for a fair comparison.
Results show that our method gains 2.15$\%$, 1.85$\%$, and 0.45$\%$ improvements in Dice, respectively, demonstrating the decent performance of our design.

\noindent\textbf{Impact of segment length.}
In Fig.~\ref{fig:abl}(c), we investigate the impact of different segment lengths $L$ in our PHNet. 
Expressly, the segment length is set to different ratios of the width ($W$), \ie, $1$, $\frac{1}{2}$, $\frac{1}{3}$, and $\frac{1}{4}$, respectively.
It could particularly benefit different sizes of regions of interest (ROI).
Results show that the best performance is achieved when $L=\frac{1}{2}W$.

\noindent\textbf{Impact of MLP layers.}
In Fig.~\ref{fig:abl}(d), we study the influence of a different number of MLP layers in our PHNet.
Results show that the best performance is achieved when the number of MLP layers is $2$. 
\input{Figure/ablation_fig}








