{
    "arxiv_id": "2303.12187",
    "paper_title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
    "authors": [
        "Xiaoming Ren",
        "Chao Li",
        "Shenjian Wang",
        "Biao Li"
    ],
    "submission_date": "2023-02-28",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 1,
    "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
    ],
    "abstract": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12187v1"
    ],
    "publication_venue": null
}