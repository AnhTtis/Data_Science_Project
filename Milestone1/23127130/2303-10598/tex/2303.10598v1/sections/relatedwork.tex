\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\section{Related Work}

\noindent \textbf{Neural scene representations.} 
3D scene representation has been extensively studied in recent years with different ways of representations such as volumes~\cite{ji2017surfacenet,wu20153d,seitz1999photorealistic, qi2016volumetric,kutulakos2000theory}, point clouds~\cite{achlioptas2018learning,qi2017pointnet}, meshes\cite{kanazawa2018learning, wang2018pixel2mesh}, depth maps~\cite{huang2018deepmvs, liu2015learning}, and implicit functions~\cite{chen2019learning,mescheder2019occupancy, niemeyer2020differentiable,yariv2020multiview}. These methods adopt differentiable rendering which enables model optimization by using 2D multi-view images. Among them, Neural Radiance Field (NeRF)~\cite{mildenhall2021nerf} can render a complex 3D scene with high fidelity and accurate geometry. It represents scenes with an implicit coordinate function that maps each 3D coordinate to a density value and a color value, and employs volume rendering to generate images of novel views. However, the implicit coordinate function is represented by a large multilayer perceptron (MLP) that is often hard to optimize and slow to infer. Serval studies adopt a hybrid representation \cite{chan2022efficient,fridovich2022plenoxels,sun2022direct,muller2022instant, chen2022tensorf,peng2020convolutional,martel2021acorn,liu2020neural,devries2021unconstrained,zhan2023general} to speed up the reconstruction and rendering. They employ explicit data structures such as discrete voxel grids \cite{sun2022direct,fridovich2022plenoxels}, decomposed tensors\cite{chen2022tensorf, chan2022efficient,fridovich2023k}, hash maps \cite{muller2022instant}, etc. to store features or spherical harmonics, enabling fast convergence and inference.

Although most existing work extracts features as middle-level representations of scenes, the extracted features are usually an intermediate output of neural networks which have little semantic meanings and are not suitable for the style transfer task. We introduce decomposed tensors~\cite{chen2022tensorf} to store high-level features extracted by pre-trained CNNs, which enables transformations in feature space as well as efficient training and inference. Though~\cite{niemeyer2021giraffe,chan2022efficient,gu2021stylenerf} also render feature maps instead of RGB maps, they are computationally intensive and usually work with low-resolution feature maps. StyleRF can instead render full-resolution feature maps (the same as the output RGB images) efficiently and it uses high-level features largely for transformation only.


\noindent \textbf{Neural style transfer.} 
Neural style transfer aims at rendering a new image that contains the content structure of one image and the style patterns of another. The seminal work in~\cite{gatys2016image} shows that multi-level feature statistics extracted from intermediate layers of pre-trained CNNs could be used as a representation of the style of an artistic image, but it treats style transfer as a slow and iterative optimization task. \cite{johnson2016perceptual,huang2017arbitrary,li2017universal, wu2021styleformer,sheng2018avatar,liu2021adaattn,park2019arbitrary,deng2020arbitrary,li2019learning} utilize feed-forward networks to approximate the optimization procedure to speed up rendering. Among them, \cite{huang2017arbitrary,li2017universal, wu2021styleformer,sheng2018avatar,liu2021adaattn,park2019arbitrary,deng2020arbitrary,li2019learning} can achieve zero-shot style transfer by applying transformations to the high-level features extracted by pre-trained CNNs, where the feature transformations can be achieved by matching second-order statistics \cite{huang2017arbitrary,li2017universal}, linear transformation \cite{li2019learning,wu2021styleformer} , self-attention transformation \cite{liu2021adaattn, park2019arbitrary,deng2020arbitrary}, etc. %Video style transfer extends style transfer to a sequence of video frames requiring the stylized video to remain consistent across adjacent frames. 
Video style transfer extends style transfer to videos for injecting target styles consistently across adjacent video frames. Several studies leverage optical flow~\cite{chen2017coherent, huang2017real,ruder2018artistic, wang2020consistent,ReReVST2020} as temporal constraints to estimate the movement of video contents. They can produce smooth videos, but have little knowledge of the underlying 3D geometry and cannot render consistent frames in arbitrary views \cite{mu20223d,huang2021learning}.

Huang et al. first tackle %the challenge of 
stylizing complex 3D scenes~\cite{huang2021learning}. They construct a 3D scene by back-projecting image features into the 3D space to form a point cloud and then perform style transformation on the features of 3D points. Their method can achieve zero-shot style transfer, but requires %a pre-trained depth estimator to model scene geometry where the estimated 3D geometry is prone to errors and causes artifacts in the final stylization output. 
an error-prone pre-trained depth estimator to model scene geometry. \cite{mu20223d} also constructs a point cloud for stylization but it mainly focuses on monocular images. Instead, \cite{huang2022stylizednerf,chiang2022stylizing,zhang2022arf,nguyen2022snerf, fan2022unified,chen2022upstnerf} use NeRF\cite{mildenhall2021nerf} as the 3D representation which can reconstruct scene geometry more faithfully. 
\cite{chen2022upstnerf} is a photorealistic style transfer method that can only transfer the color tone of style images.
\cite{zhang2022arf,nguyen2022snerf} achieve 3D style transfer via optimization and can produce visually high-quality stylization, but they %are not scalable requiring 
require a time-consuming optimization procedure for every reference style. \cite{huang2022stylizednerf, fan2022unified} employ latent codes to represent a set of pre-defined styles, but cannot generalize to unseen styles. \cite{chiang2022stylizing} can achieve arbitrary style transfer by implicitly instilling the style information into MLP parameters. % via a hyper network. 
However, it can only transfer the color tone of style images but cannot capture detailed style patterns. StyleRF can transfer arbitrary style in a zero-shot manner, and it can capture style details such as strokes and textures as well.

\end{document}


