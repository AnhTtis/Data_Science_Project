\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\begin{figure*}
    \centering
    \includegraphics[scale=.75]{images/comparison.pdf}
    \caption{Comparison of StyleRF with two state-of-the-art zero-shot 3D style transfer methods LSNV \cite{huang2021learning} and Hyper \cite{chiang2022stylizing}. For each of the two sample \textit{Scenes} and reference \textit{Styles}, StyleRF produces clearly better 3D style transfer and depth estimation. Check zoom-in for details.}
    \vspace{-1.2em}
    \label{fig:comparison}
\end{figure*}


\section{Experiments}

We evaluate StyleRF extensively with qualitative experiments in \cref{sec:qualitative}, quantitative experiments in \cref{sec:quantitative} and ablation studies in  \cref{sec:ablation}. We demonstrate two applications of StyleRF in \cref{sec:app}. The implementation details are provided in the appendix.


\subsection{Qualitative Experiments}
\label{sec:qualitative}
We evaluate StyleRF over two public datasets including LLFF \cite{mildenhall2019llff} that contains real scenes with complex geometry structures and Synthetic NeRF \cite{mildenhall2021nerf} that contains \(360^{\circ}\) views of objects. In addition, we benchmark StyleRF with two state-of-the-art zero-shot 3D style transfer methods LSNV\cite{huang2021learning} and Hyper \cite{chiang2022stylizing} with their released codes. We perform comparisons on LLFF dataset \cite{mildenhall2019llff}.

\cref{fig:comparison} shows qualitative comparisons. We can see that StyleRF achieves clearly better stylization with more precise geometry reconstruction. Specifically, StyleRF can generate high-definition stylization with realistic textures and patterns of style images. The superior stylization is largely attributed to our transformation design that allows working in the feature space with full-resolution feature maps. As illustrated in the highlight boxes, StyleRF can successfully restore the intricate geometry of complex scenes thanks to its radiance field representations. In addition, only StyleRF faithfully transfers the squareness texture in the second style image. Furthermore, StyleRF can robustly generalize to new styles in a zero-shot manner and can adapt well to \(360^{\circ}\) dataset as illustrated in \cref{fig:teaser}. As a comparison, LSNV \cite{huang2021learning} fails to capture fine-level geometry like the bones of the T-Rex and the petals of the flower while Hyper \cite{chiang2022stylizing} produces very blurry stylization.

\subsection{Quantitative Results}
\label{sec:quantitative}

\begin{table}

% \definecolor{red}{rgb}{1,0.6,0.6}
% \definecolor{orange}{rgb}{1,0.8,0.6}
% \definecolor{yellow}{rgb}{1,1,0.6}

\centering

\resizebox{\linewidth}{!}{
    \begin{tabular}{*5c}
    \toprule
    Method & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}Short-range \\ Consistency\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}Long-range \\ Consistency\end{tabular}} \\
    \midrule
    {}    & \textbf{LPIPS} & \textbf{RMSE}  & \textbf{LPIPS} & \textbf{RMSE} \\
    
    AdaIN\cite{huang2017arbitrary}  &  0.152  & 0.123  & 0.220  & 0.186   \\
    CCPL\cite{wu2022ccpl}           &  0.110  & 0.106  & 0.191  & 0.174   \\
    ReReVST\cite{ReReVST2020}       &  0.098  & 0.080  & 0.186  & 0.146   \\
    LSNV\cite{huang2021learning}    &  0.093  & 0.092  & 0.181  & 0.155   \\
    Hyper\cite{chiang2022stylizing} &  0.084  & 0.068  & 0.131  & 0.101    \\
    \textbf{Ours}                   &  0.072  & 0.082  & 0.149  & 0.137  \\
    \bottomrule
    \end{tabular}
}
\caption{\textbf{Results on consistency.}  We compare StyleRF with the state-of-the-art on consistency using LPIPS (\(\downarrow\)) and RMSE (\(\downarrow\)).}
\vspace{-1.2em}
\label{tab:compare}
\end{table}



3D style transfer is a very new and under-explored task and there are few metrics for quantitative evaluation of stylization quality. Hence, we manage to evaluate the multi-view consistency only. In our experiments, we warp one view to the other according to the optical flow \cite{teed2020raft} using softmax splatting \cite{niklaus2020softmax}, and then computed the masked RMSE score and LPIPS score \cite{zhang2018unreasonable} to measure the stylization consistency. Following \cite{huang2021learning,fan2022unified,chiang2022stylizing}, we compute the short-range and long-range consistency scores which compare adjacent views and  far-away views respectively. 
We compare StyleRF against two state-of-the-art zero-shot 3D style transfer methods Hyper \cite{chiang2022stylizing} and LSNV \cite{huang2021learning},  one SOTA single-frame-based video style transfer method CCPL\cite{wu2022ccpl}, one SOTA multi-frames-based video style transfer method ReReVST \cite{ReReVST2020}, and one classical image style transfer method AdaIN\cite{huang2017arbitrary}.

It can be seen from \cref{tab:compare} that StyleRF significantly outperforms image style transfer approach~\cite{huang2017arbitrary} and video style transfer approach~\cite{wu2022ccpl, ReReVST2020} which capture little information about the underlying 3D geometry. In addition, StyleRF achieves better consistency than point-cloud-based 3D style transfer \cite{huang2021learning} as well. Note Hyper\cite{chiang2022stylizing} achieves slightly better LPIPS and RMSE scores than our method, largely because it produces over-smooth results and inadequate stylization as shown in \cref{fig:comparison}. 



\end{document}


