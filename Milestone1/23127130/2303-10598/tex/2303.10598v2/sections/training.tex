\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\subsection{Two-stage Model Training}
The training of our model is divided into the \emph{feature grid training stage} and the \emph{stylization training stage}, the former is trained with the target of novel view synthesis, and the latter is trained with the target of style transfer.

\noindent
\textbf{Feature grid training stage (First stage). }
We first learn the feature grid 3D representation for the novel view synthesis task, in preparation for performing feature transformation for style transfer. We train the feature grid and the 2D CNN decoder simultaneously, with the supervision of both RGB images and their bilinearly up-sampled feature maps extracted from \verb+ReLU3_1+ layer of pre-trained VGG\cite{simonyan2014very}. By aligning the VGG features with the feature grid, the reconstructed features acquire semantic information.
We use density grid pre-trained solely on RGB images since the supervising feature maps are not strictly multi-view consistent. The training objective is the mean square error (MSE) between the predicted and ground truth feature maps and RGB images. Following \cite{mu20223d,huang2021learning}, we use perceptual loss\cite{johnson2016perceptual} as additional supervision to increase reconstructed image quality. The overall loss function is:
\begin{multline}
    \mathcal{L}_{grid} =  \sum_{r \in \mathcal{R}}  \left \| \hat{F}(\textbf{r}) - F(\textbf{r})  \right \|_2^2 + \left \| \hat{I}_{\mathcal{R}} - I_{\mathcal{R}}  \right \|_2^2 \\
    + \sum_{l\in l_p} \left \| {\cal{F}}^l( \hat{I}_{\mathcal{R}} ) - {\cal{F}}^l(I_{\mathcal{R}}) \right \|_2^2 ,
\end{multline}
where \(\mathcal{R}\) is the set of rays in each training batch, \( \hat{F}(\textbf{r}), F(\textbf{r})\) are the predicted and ground truth feature of ray \(\textbf{r}\), \(\hat{I}_{\mathcal{R}}, I_{\mathcal{R}}\) are the predicted and ground truth RGB image,  \(l_p\) denotes the set of VGG layers %for computing the 
that compute perceptual loss, ${\cal{F}}^l$ denotes the feature maps %extracted from 
of the \(l\)th layer of pre-trained VGG network.

\noindent
\textbf{Stylization training stage (Second stage). } 
Our model learns to stylize novel views in the second stage. We freeze the feature grid, train the style transfer module, and fine-tune the CNN decoder.
Thanks to the memory-efficient representation of 3D scenes and DST, unlike \cite{schwarz2020graf, chiang2022stylizing, fan2022unified}, our model can be trained directly on \(256 \times 256\) patches, making patch sub-sampling algorithm \cite{huang2022stylizednerf, chiang2022stylizing, fan2022unified,schwarz2020graf} unnecessary. We use the same loss as \cite{huang2017arbitrary} where the content loss $\mathcal{L}_{c}$ is the MSE of the feature maps and the style loss $\mathcal{L}_{s}$ is the MSE of the channel-wise feature mean and standard-deviation: %of the features:
\begin{equation}
    \mathcal{L}_{stylization} = \mathcal{L}_{c} + \lambda\mathcal{L}_{s} ,
\end{equation}
where \(\lambda\) balances the content preservation and the stylization effect.




\end{document}

