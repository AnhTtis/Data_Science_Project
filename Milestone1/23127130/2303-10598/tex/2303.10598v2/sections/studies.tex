\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\subsection{Ablation Studies}
\label{sec:ablation}

\begin{figure}
    \centering
    \includegraphics[scale=.3]{images/ablation.pdf}
    \caption{\textbf{Ablation studies.}  (a) shows the stylization of our full pipeline. (b) shows the stylization without the adaptive bias. (c) shows the stylization when replacing the volume-adaptive instance normalization (IN) with vanilla IN. (d) shows the stylization without any IN.}
    \vspace{-1em}
    \label{fig:ablation}
\end{figure}

We design two innovative techniques to improve the stylization quality and maintain the multi-view consistency. The first is volume-adaptive instance normalization which uses the learned mean and variance of the whole volume during inference, eliminating the dependency on holistic statistics of the sampled point batch. The second is the adaptive bias addition in DST, which improves the stylization quality using bias capturing style information. We evaluate the two designs to examine how they contribute to the overall stylization of our method.


\noindent \textbf{Volume-adaptive instance normalization.} 
We compare our volume-adaptive instance normalization (IN) with vanilla IN and StyleRF without IN. As \cref{fig:ablation} (c) shows, vanilla IN produces severe block-shape artifacts as the transformation of each batch is conditioned on the holistic statistics of itself, thus each batch (i.e. block in the image) produces inconsistent stylization which leads to the artifacts. However, if we discard IN as shown in \cref{fig:ablation} (d), the multi-view consistency can maintain but the stylization quality compromises a lot, failing to capture the correct color tone of the reference style image. This is because IN removes the original style information of the content image which facilitates the transfer of the reference style \cite{huang2017arbitrary}.

\noindent \textbf{Adaptive bias addition.}
As illustrated in \cref{fig:ablation} (b), the stylization quality degrades a lot if we eliminate the adaptive bias addition in DST (\cref{sec:DST}), producing unnatural stylization compared to the stylization of our full pipeline in \cref{fig:ablation} (a). This is because bias usually contains crucial style information such as the overall color tone \cite{wu2021styleformer}. StyleRF employs bias addition that is adaptively modulated by the weight of each ray, improving the stylization quality and keeping multi-view consistency concurrently.

\subsection{Applications}
\label{sec:app}

\begin{figure}
    \centering
    \includegraphics[scale=.35]{images/interpolation.pdf}
    \caption{\textbf{Multi-style interpolation.}  StyleRF can smoothly interpolate between arbitrary styles by interpolating features of the scene.}
    \vspace{-1.2em}
    \label{fig:interpolation}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=.36]{images/composition.pdf}
    \caption{\textbf{Compositional 3D style transfer.}  Given the 3D-consistent segmentation masks, StyleRF can create infinite combinations of styles by spatial composition.}
    \vspace{-1.2em}
    \label{fig:composition}
\end{figure}

StyleRF can be easily extended along different directions with different applications. We provide two possible extensions in the ensuing subsections.    

\noindent \textbf{Multi-style interpolation.}
StyleRF can smoothly interpolate different styles thanks to its high-level feature representation of a 3D scene. As illustrated in \cref{fig:interpolation}, we linearly interpolate the feature maps of a specific view by using four different styles at four corners. Unlike previous NeRF-based 3D style transfer that supports style interpolation by interpolating one-hot latent vectors \cite{fan2022unified}, StyleRF can interpolate arbitrary numbers of unseen new styles by interpolating features of the scene, yielding more smooth and harmonious interpolation. Hence, StyleRF can not only transfer arbitrary styles in a zero-shot manner but also generate non-existent stylization via multi-style interpolation.

\noindent \textbf{Compositional 3D style transfer.}
Thanks to its precise geometry reconstruction, StyleRF can be seamlessly integrated with NeRF-based object segmentation \cite{fan2022nerf,Zhi:etal:ICCV2021,kobayashi2022decomposing} for compositional 3D style transfer. As shown in \cref{fig:composition}, we apply 3D-consistent segmentation masks to the feature maps and apply different styles to stylize the contents inside and outside the masks separately. We can see that the edges of the masks can be blended more softly by applying the segmentation masks to the feature maps instead of RGB images. Due to its zero-shot nature, StyleRF can create infinite combinations of styles without additional training, producing numerous artistic creations and inspirations.

\end{document}


