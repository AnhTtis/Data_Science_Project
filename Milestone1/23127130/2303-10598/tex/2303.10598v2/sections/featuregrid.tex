\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

The overview of StyleRF is shown in \cref{fig:overview}. For a batch of sampled points along a ray \( \textbf{r} \), the corresponding features \(F_i, i \in [1,2,...,N]\) are first extracted from the feature grid described in \cref{sec:grid}, each of which is transformed to \( \bar{F_i} \) independently via \textit{Sampling-Invariant Content Transformation (SICT)} described in \cref{sec:SICT}, regardless of the holistic statistics of the point batch. \( \bar{F_i} \) is then transformed to a feature map \( \bar{F_c} \) via \textit{Volume Rendering}. After that, the \textit{Deferred Style Transformation (DST)} described in \cref{sec:DST} transforms \( \bar{F_c} \) to the feature map \(F_{cs}\) adaptively using the sum weight of the sampled points \( w_{\textbf{r}} \) along the ray \( \textbf{r} \) and the style information \(T,\mu(F_s),\) and \(\sigma(F_s)\). Finally, a stylized novel view is generated via a CNN decoder.

\subsection{Feature Grid 3D Representation}
\label{sec:grid}

To model a 3D scene with deep features, we use a continuous volumetric field of density and radiance. Different from the original NeRF\cite{mildenhall2021nerf}, for every queried 3D position \( x \in \mathbb{R}^3 \), we get a volume density \(\sigma(x)\) and a multi-channel feature \( F(x) \in \mathbb{R}^C \) instead of an RGB color, where \(C\) is the number of the feature channels. Then we can get the feature of any rays \(\textbf{r}\) passing through the volume by integrating sampled points along the ray via approximated volume rendering \cite{mildenhall2021nerf}:
\begin{equation}
    F(\textbf{r}) = \sum_{i=1}^N w_i F_i,
    \label{eq:volumerender}
\end{equation}
\begin{equation}
    \text{where}\quad w_i = \mathrm{exp} \left(-\sum_{j=1}^{i-1} \sigma_j \delta_j    \right) \left( 1-\mathrm{exp}\left( -\sigma_i \delta_i \right) \right),
    \label{eq:weight}
\end{equation}
where %\(w_i\) represents the weight of the feature of sampled point \(i\) in the ray \(\textbf{r}\), \(\sigma_i, F_i\) denotes the  volume density and feature of sampled point \(i\)  , 
\(\sigma_i, F_i\) denotes the volume density and feature of sampled point \(i\), \(w_i\) denotes the weight of \(F_i\) in the ray \(\textbf{r}\),and \(\delta_i\) is the distance between adjacent samples. We disable view-dependency effect for better multi-view consistency.

Then we can generate feature maps capturing high-level features and map them to RGB space using a 2D CNN decoder. However, unlike \cite{chan2022efficient,niemeyer2021giraffe,gu2021stylenerf}, we render full-resolution feature maps which have the same resolution as the final RGB images rather than down-sampled feature maps. Rendering full-resolution feature maps has two unique features: \textbf{1)} it discards up-sampling operations which cause multi-view inconsistency in general \cite{gu2021stylenerf}, \textbf{2)} it removes aliasing when rendering low-resolution feature maps \cite{barron2021mip} which causes severe flickering effects in stylized RGB videos.

Directly using 3D voxel grid to store features is memory-intensive. We thus adopt vector-matrix tensor decomposition\cite{chen2022tensorf} that relaxes the low-rank constraints for two modes of a 3D tensor and factorizes tensors into compact vector and matrix factors, which lowers the space complexity from \(\mathcal{O}(n^3)\) to \(\mathcal{O}(n^2)\), massively reducing the memory footprint. We employ a density grid to store volume density and a feature grid to store multi-channel features respectively.  

\end{document}


