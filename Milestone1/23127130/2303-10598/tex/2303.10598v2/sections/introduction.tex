\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\section{Introduction}

Given a set of multi-view images of a 3D scene and an image capturing a target style, 3D style transfer aims to generate novel views of the 3D scene that have the target style consistently across the generated views (\cref{fig:teaser}). Neural style transfer has been investigated extensively, and state-of-the-art methods allow transferring arbitrary styles in a zero-shot manner. However, most existing work focuses on style transfer across 2D images~\cite{gatys2016image, johnson2016perceptual, huang2017arbitrary} but cannot extend to a 3D scene that has arbitrary new views. Prior studies~\cite{huang2022stylizednerf, nguyen2022snerf, huang2021learning,mu20223d} have shown that naively combining 3D novel view synthesis and 2D style transfer often leads to multi-view inconsistency or poor stylization quality, and 3D style transfer should optimize novel view synthesis and style transfer jointly.

However, the current 3D style transfer is facing a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to new styles. Different approaches have been investigated to resolve the three-way dilemma. For example, multiple style transfer~\cite{huang2022stylizednerf, fan2022unified} requires a set of pre-defined styles but cannot generalize to unseen new styles. Point-cloud-based style transfer~\cite{huang2021learning,mu20223d} requires a pre-trained depth estimation module that is prone to inaccurate geometry reconstruction. Zero-shot style transfer with neural radiance fields (NeRF)~\cite{chiang2022stylizing} cannot capture detailed style patterns and textures as it implicitly injects the style information into neural network parameters. Optimization-based style transfer~\cite{zhang2022arf, hollein2022stylemesh, nguyen2022snerf} suffers from slow optimization and cannot scale with new styles.

In this work, we introduce \textbf{StyleRF} to resolve the three-way dilemma by performing style transformation in the feature space of a radiance field. A radiance field is a continuous volume that can restore more precise geometry than point clouds or meshes. In addition, transforming a radiance field in the feature space is more expressive with better stylization quality than implicit methods \cite{chiang2022stylizing}, and it can also generalize to arbitrary styles. We construct a 3D scene representation with a grid of deep features to enable feature transformation. In addition, multi-view consistent style transformation in the feature space could be achieved by either transforming the whole feature grid or transforming the sampled 3D points. We adopt the latter as the former incurs much more computational cost during training to stylize the whole feature grid in every iteration, whereas the latter can reduce computational cost through decreasing the size of training patch and the number of sampled points.  However, applying off-the-shelf style transformations to a batch of sampled 3D points impairs the multi-view consistency as they are conditioned on the holistic statistics of the batch. Beyond that, transforming every sampled 3D point is memory-intensive since NeRF needs to query hundreds of sampled points along each ray for rendering a single pixel.

We decompose the style transformation into sampling-invariant content transformation (SICT) and deferred style transformation (DST), the former eliminating the dependency on holistic statistics of sampled point batch and the latter deferring style transformation to 2D feature maps for better efficiency. In SICT, we introduce volume-adaptive normalization that learns the mean and variance of the whole volume instead of computing them from a sampled batch. In addition, we apply channel-wise self-attention to transform each 3D point independently to make it conditioned on the feature of that point regardless of the holistic statistics of the sampled batch. In DST, we defer the style transformation to the volume-rendered 2D feature maps based on the observation that the style transformation of each point is the same. By formulating the style transformation by pure matrix multiplication and adaptive bias addition, transforming 2D feature maps is mathematically equivalent to transforming 3D point features but it saves computation and memory greatly. Thanks to the memory-efficient representation of 3D scenes and deferred style transformation, our network can train with \(256 \times 256\) patches directly without requiring sub-sampling like previous NeRF-based 3D style transfer methods\cite{huang2022stylizednerf, chiang2022stylizing, fan2022unified}.

The contributions of this work can be summarized in three aspects. \textit{First}, we introduce StyleRF, an innovative zero-shot 3D style transfer framework that can generate zero-shot high-quality 3D stylization via style transformation within the feature space of a radiance field. \textit{Second}, we design sampling-invariant content transformation and deferred style transformation, the former achieving multi-view consistent transformation by eliminating dependency on holistic statistics of sampled point batch while the latter greatly improves stylization efficiency by deferring style transformation to 2D feature maps. \textit{Third}, extensive experiments show that StyleRF achieves superior 3D style transfer with accurate geometry reconstruction, high-quality stylization, and great generalization to new styles.

\begin{figure*}
    \centering
    \includegraphics[scale=.8]{images/overview.pdf}
    \caption{\textbf{The framework of StyleRF.}  For a batch of sampled points along a ray \( \textbf{r} \), the corresponding features \(F_i, i \in [1,2,...,N]\) are first extracted, each of which is transformed to \( \bar{F_i} \) independently via \textit{Sampling-Invariant Content Transformation}, regardless of the holistic statistics of the point batch. \( \bar{F_i} \) is then transformed to a feature map \( \bar{F_c} \) via \textit{Volume Rendering}. After that, the \textit{Deferred Style Transformation} transforms \( \bar{F_c} \) to the feature map \(F_{cs}\) adaptively using the sum weight of the sampled points \( w_{\textbf{r}} \) along the ray \( \textbf{r} \) and the style information \(T,\mu(F_s)\), and \(\sigma(F_s)\). Finally, a stylized novel view is generated via a CNN decoder.}
    \vspace{-1em}
    \label{fig:overview}
\end{figure*}

\end{document}