\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}
\subsection{Feature Transformation for Style Transfer}
\label{sec:styletransfer}

Once we have the feature grid representation of a scene, we can tackle the task of stylizing 3D scenes. Given a reference style image, our goal is to render stylized novel views of the 3D scene with multi-view consistency. To achieve this, we apply transformations to the features of the grid. 

One plausible solution to this task is to apply style transfer to the feature grid directly. This solution is efficient in evaluations as it can render any stylized views with a single style transfer process only. However, it is impractical to train such transformation as it needs to stylize the whole feature grid in every iteration. Another solution is to apply an off-the-shelf zero-shot style transfer method to the features of the sampled 3D points. While this solution can reduce computational cost through decreasing the size of training patch and the number of sampled points, it has two problems:  \textbf{1)}  vanilla zero-shot style transformation is conditioned on holistic statistics of the sampled point batch \cite{li2019learning, huang2017arbitrary, liu2021adaattn}, which violates multi-view consistency in volume rendering as the feature transformation of a specific 3D point will vary across different sampled points; \textbf{2)} volume rendering requires sampling hundreds of points along a single ray, which makes transformation on the point batch memory-intensive.

Motivated by the observation that style transformation is conditioned on both content information and style information, we decompose the style transformation into sampling-invariant content transformation (SICT) and deferred style transformation (DST). After the decomposition, SICT will be conditioned solely on the content information while DST conditioned solely on the style information, more details to be elaborated in the ensuing subsections.

\vspace{-0.5em}
\subsubsection{Sampling-invariant Content Transformation}
\label{sec:SICT}

\begin{figure}
    \hfill
    \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[scale=.65]{images/IN.pdf}    \caption{Vanilla IN.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[scale=.65]{images/VA_IN.pdf}
    \caption{Volume-adaptive IN.}
    \end{subfigure}
    \hfill
    \caption{Comparison between vanilla instance normalization (IN) in (a) and volume-adaptive %instance normalization (IN) 
    IN in (b). During evaluation, volume-adaptive %instance normalization 
    IN uses learned mean and standard-deviation, discarding dependency over the sampled point batch's holistic statistics (indicated by the \textcolor{red}{red} arrows in the left graph).}
    \vspace{-1.2em}
    \label{fig:IN}
\end{figure}

Given a batch of sampled points, we can get their corresponding features \(F_i \in \mathbb{R}^{C}, i \in [1,2,...,N] \) from the feature grid, where \(N\) is the number of the sampled points along a ray and \(C\) is the number of the feature channels. The goal of SICT is to transform the extracted features \(F_i\) so that they can be better stylized. We formulate SICT as a channel-wise self-attention operation to the features after instance normalization (IN) \cite{ulyanov2016instance}. Specifically, we formulate \(Q\)(query), \(K\)(key), and \(V\)(value) as:
\begin{align}
    Q &= q(Norm(F_i)), \\
    K &= k(Norm(F_i)), \\
    V &= v(Norm(F_i)),
\end{align}
where \(q,k,v\) are \(1\times 1\) convolution layers which reduce the channel number from \(C\) to \(C'\) for computational efficiency, and \(Norm\) denotes the %instance normalization
IN. However, as shown in \cref{fig:IN}, vanilla %instance normalization 
IN calculates per-dimension mean and standard-deviation of the batch of sampled points, which varies with different sampled points and incurs multi-view inconsistency accordingly. Thus we design volume-adaptive %instance normalization 
IN which, during training, keeps running estimates of the computed mean and standard-deviation, and uses them for normalization during evaluations (instead of computing from the sampled point batch). Through volume-adaptive %instance normalization
IN, we can ensure that the content transformation is consistent regardless of the sampled point batch's holistic statistics.

Channel-wise self-attention can thus be implemented by:

\begin{equation}
    \bar{F_i} = V \otimes \mathrm{Softmax}\left( \widetilde{cov}(Q,K)\right),
\end{equation}
where  \(\otimes\) denotes matrix multiplication and \(\widetilde{cov}(Q,K) \in \mathbb{R}^{N\times C' \times C'}\) denotes the covariance matrix in the channel dimension.

\vspace{-0.5em}
\subsubsection{Deferred Style Transformation}
\label{sec:DST}

\begin{figure}
    \centering
    \includegraphics[scale=0.9]{images/dst.pdf}
    \caption{\textbf{Deferred style transformation.}  We apply the style transformation to the volume-rendered feature maps \(\bar{F_c}\) according to the style feature maps \(F_s\). To ensure multi-view  consistency, we modulate the bias (e.g. the mean value of the style feature maps \(\mu(F_s)\))  with the sum weight of sampled points along each ray \(w_{\textbf{r}}\).}
    \vspace{-1.2em}
    \label{fig:DST}
\end{figure}

After applying SICT to the features of each 3D point, we apply DST to the volume-rendered 2D feature maps \(\bar{F_c}\) rather than 3D point features \(\bar{F_i}\). To ensure multi-view consistency, we formulate the transformation as matrix multiplication and adaptive bias addition as illustrated in \cref{fig:DST}.

Specifically, we first extract feature maps \(F_s\) of the reference style \(S\) using a pre-trained VGG\cite{simonyan2014very}, and then generate the style transformation matrix \(T \in \mathbb{R}^{C'\times C'}\) using feature covariance \(cov(F_s)\) following \cite{li2019learning}. Next, we apply matrix multiplication with \(T\) to the feature maps \(\bar{F_c}\) and use a \(1 \times 1\) convolution layer \(conv\) without bias to restore the channel number from \(C'\) to \(C\). Though these operations can partially instill style information, they are not expressive enough without bias addition containing style information \cite{wu2021styleformer}. Thus following \cite{huang2017arbitrary}, we multiply the feature maps with the standard-deviation value \(\sigma(F_s) \) and add the mean value \(\mu(F_s)\). To ensure it is equivalent when applying the transformation to either 3D point features or 2D feature maps, we adaptively modulate the mean value \(\mu(F_s)\) with the sum weight of sampled points along each ray \(w_{\textbf{r}}\). DST can be mathematically formulated by:
\begin{equation}
    F_{cs} = conv \left(  T \otimes \bar{F_c}  \right)  \times \sigma(F_s) + w_{\textbf{r}} \times \mu(F_s),
    \label{eq:styletrans}
\end{equation}
\begin{equation}
    \text{where}\quad \bar{F_c} = \sum_{i=1}^N w_i \bar{F_i}, w_{\textbf{r}} = \sum_{i=1}^N w_i, \textbf{r} \in \mathcal{R}
\end{equation}
where \(w_i\) denotes the weight of sampled point \(i\) (\cref{eq:weight}), \(\bar{F_i}\) denotes the feature of sample \(i\) after SICT, and \(\mathcal{R}\) is the set of rays in each training batch.

Note \(conv\) is a \(1 \times 1\) convolution layer without bias, so it is basically a matrix multiplication operation. And \(\sigma(S), \mu(S)\) are scalars. Together with the adaptive bias modulation \(w_{\textbf{r}}\), \cref{eq:styletrans} can be reformulated by:
\begin{equation}
    F_{cs} = \sum_{i=1}^N w_i \left(  \underbrace{  conv\left( T \otimes \bar{F_i} \right) \times \sigma(F_s)  + \mu(F_s)   }_{\mbox{(i)}} \right),
    \label{eq:styletrans2}
\end{equation}
where part \(\mbox{(i)}\) can be seen as applying style transformation on every 3D point feature independently before volume rendering. This proves that applying DST on 2D feature maps is equivalent to applying the transformation on 3D pointsâ€™ features, maintaining multi-view consistency. The full derivation of \cref{eq:styletrans2} is provided in the appendix.%supplementary material.

Finally, we adopt a 2D CNN decoder to project the stylized feature maps \(F_{cs}\) to RGB space to generate the final stylized novel view images.


\end{document}

