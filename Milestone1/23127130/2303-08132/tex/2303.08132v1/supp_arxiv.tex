\appendix

%%%%%%%%% ABSTRACT
\section*{Appendices}
Here we provide implementation details (Sec.~\ref{supsec:imp}) and extended experimental results (Sec.~\ref{supsec:exp}) omitted from the main paper for brevity.
%%%%%%%%% BODY TEXT
\section{Implementation Details}
\label{supsec:imp}
\noindent\textbf{Training \OURS.} We use Adam optimizer with a learning rate of $5\times 10^{-5}$ during training. For all experiments, the model is training for 10K iterations on 8 V100 GPUs of 32G RAM, with a batch size of 32. We re-scale all the input image masks to $384\times 384$ with padding to preserve the aspect ratios. We set memory length $l=256$ and memory size $c=100$. During training, we randomly select adjacent 3 to 5 frames (the last frame serves as the target frame) to enable the model to handle different input lengths during inference.


\section{Comparison with Optical Flow}
\label{supsec:exp}

Optical flow is used to provide motion information in many previous methods. Since it considers pixel-level motion, it can be used to propagate previous object masks to the current frame through a warp layer.
In this section, we use RAFT to propagate the object masks and provide a quantitative comparison with our method on the OVIS-Sparse dataset.
Specifically, we use flow between frames $t$ and $t-1$ provided by RAFT to propagate the predicted masks $\textbf{m}_{t-1}$ in the frame $t-1$ to frame $t$, and then calculate the mask IoUs between the propagated masks and the predicted masks to get the flow score. As the same with the motion score, the flow score is added to the original matching score of VIS methods.


We compare RAFT and our InstMove on the OVIS-Sparse dataset. Two SOTA VIS methods, \ie MinVIS and IDOL, are used.
The frames and annotations are kept every 1, 3, 5, or 7 frames (\ie Sparse-1/3/5/7) to simulate different FPS.
Note that RAFT is pretrained on a large number of datasets including FlyingChairs~\cite{dosovitskiy2015flownet}, FlyingThings~\cite{mayer2016large}, FlyingThings3D, Sintel~\cite{butler2012naturalistic}, KITTI-2015~\cite{menze2015object}, and HD1K~\cite{kondermann2016hci}, while the VIS datasets are relatively small, we train our motion model on the OVIS-Sparse training set that only contains 485 videos.
As shown in Table~\ref{table:RAFTonVIS}, our method outperforms the optical flow-based method in different FPS, which demonstrates the robustness and effectiveness of \OURS.





\begin{table}
\renewcommand\arraystretch{0.9}
    \small 
    \centering
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{l*5{c}}
    \toprule
       &Sparse-1   &Sparse-3 &Sparse-5  &Sparse-7  \\
    % & mAP ($\uparrow$) \\
    \midrule
    MinVIS~\cite{MinVIS}  &19.2 &18.9 &15.3 &15.1  \\
    MinVIS + RAFT &20.4 &19.6 &18.1 &16.3   \\
    MinVIS + InstMove &\textbf{20.8} &\textbf{20.0} &\textbf{18.2} & \textbf{16.7}    \\
    \midrule
    IDOL~\cite{IDOL}  &24.4 &21.3 &16.5 &14.1  \\
    IDOL + RAFT &25.7 &\textbf{21.5} &17.5 &15.2   \\
    IDOL + InstMove  &\textbf{27.0} &\textbf{21.5} &\textbf{18.8} & \textbf{16.2}    \\
    \bottomrule
    \end{tabular}}
    \caption{
    \textbf{Effects of instance-level motion module (\OURS) and pixel-level motion module (RAFT) on VIS task.}
    We report the mAP on the OVIS-Sparse validation set. \OURS outperforms the optical flow-based method in different FPS, which demonstrates the robustness and effectiveness of \OURS. Note that RAFT is pretrained on a large number of datasets while \OURS is only trained on 485 videos.
    }
    \label{table:RAFTonVIS}
    \vspace{-1ex}
\end{table}

