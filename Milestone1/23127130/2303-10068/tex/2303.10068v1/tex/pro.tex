
\section{Progressive Branch-and-Bound}
Although  Algorithm~\ref{al_bab}~improves the effectiveness of basic greedy by conducting the branch-and-bound framework, it still suffers from a high computational cost due to heavily invoking Algorithm~\ref{al_sbound} for bound estimations. To be more specific, in each greedy search iteration of Algorithm~\ref{al_sbound}, it has to recalculate the marginal gain $(\overline{\object}({\pr} \cup \{ v \}|{\ru}) - \overline{\object}({\pr}|{\ru}))$ for all candidate nodes.

Motivated by this observation, we propose a progressive  sampling-based upper bound estimation method (\psbound). It selects multiple, but not only one, nodes in each greedy search iteration to cut down the total number of iterations required and hence the computation cost. Meanwhile, we will prove that it can achieve an approximation ratio of $(1 - 1/e - \epsilon - \rho)$ for the upper bound estimation, where $\rho$ is a tunable parameter that provides a trade-off between efficiency and accuracy.

The pseudo-code of \psbound~is shown in Algorithm~\ref{al_psbound}. \psbound~first sorts $v \in V$ based on descending order of $\overline{\object}_{v}(P|R)$ and initializes the threshold h to the value of $\max _{v \in V}\overline{\object}_{v}(P|R)$. Then, it iteratively fetches all the nodes with their marginal gains not smaller than $h$ into $P$ and meanwhile lowers the threshold $h$ by a factor of $(1 + \rho)$ for the next iteration (Lines~\ref{pro:start}-\ref{pro:end}). The iteration continues until there are $k$ nodes in $P$. Unlike the basic greedy method that has to check all the potential nodes in candidate node set $V$ in each iteration, it is not necessary for \psbound~as it implements an early termination (Lines~\ref{pro:s1}-\ref{pro:e1}). Since nodes are sorted by $\overline{\object}_{v}(P|R)$ values, if $\overline{\object}_{v}(P|R)$ of the current node is smaller than $h$, all the nodes $v‘$ pending for evaluation will have their $\overline{\object}_{v‘}(P|R)$ values smaller than $h$ and hence could be skipped from evaluation. 

In the following, we first analyze the approximation ratio of Algorithm~\ref{al_psbound}  for upper bound estimation by Lemma 2. Based on Lemma 2, we show the approximation ratio of the branch-and-bound framework invoking Algorithm~\ref{al_psbound} for \prob~by Theorem 3.

\begin{lem}\label{ratio:psbound}
	\psbound~achieves a $(1 - 1/e - \epsilon - \rho)$ approximation ratio for upper bound estimation.
\end{lem}
$Proof.$ We first prove \psbound~achieves a $(1 - 1/e - \rho)$ approximation ratio for maximizing monotone and submodular functions. At this stage, we do not consider estimating the marginal gains based on the sampling results but assume that the true marginal gains can be obtained. Then we show \psbound~achieves a $(1 - 1/e - \epsilon - \rho)$ approximation ratio for upper bound estimation. 

\noindent
\textbf{\psbound~for maximizing monotone and submodular functions.}  For a given rumor set $R$, let $v_i$ be the node selected at a given threshold $h$ and $O$ denote the optimal
local solution to the problem of selecting $k$ nodes that can maximize $\overline{\object}$. Because of the submodularity of $\overline{\object}$, we have
\begin{equation} \label{appro1}
\overline{\object}_{v}(P|R)= \left\{ \begin{array}{l}
\ge h\\
\le h \cdot (1 + \rho ) 
\end{array} \right. \begin{array}{*{20}{l}}
{ \textrm{if $v=v_i$}}\\
{\textrm{if $v\in O\backslash(P\cup v_i)$}},
\end{array}
\end{equation}
where $P$ is the current partial solution. Equation~(\ref{appro1}) implies that $\overline{\object}_{v_i}(P|R)\geq\overline{\object}_{v}(P|R)/(1+\varepsilon)$ for any $v\in O\backslash P$. Thus, we have %$\overline{\object}_{v_i}(P|R) \geq \frac{1}{(1+\rho)|\OPT\backslash P|}\sum\nolimits_{v \in \OPT\backslash P}\overline{\object}_{v}(P|R)\geq \frac{1}{(1+\varepsilon)n}\sum\nolimits_{v \in \OPT\backslash P}\overline{\object}_{v}(P|R)$.
\begin{equation*}
\begin{aligned}
	\overline{\object}_{v_i}(P|R) &\geq \frac{1}{(1+\rho)|\OPT\backslash P|}\sum\nolimits_{v \in \OPT\backslash P}\overline{\object}_{v}(P|R)\\
	&\geq \frac{1}{(1+\varepsilon)n}\sum\nolimits_{v \in \OPT\backslash P}\overline{\object}_{v}(P|R).
\end{aligned}
\end{equation*}
Let $P_i$ denote the partial solution that $v_i$ has been included and $v_{i+1}$ be the node selected at the $(i+1)$th step. Then we have %$\overline{\object}(P_{i+1}|R)-\overline{\object}(P_i|R)=\overline{\object}_{v_i}(P_i|R) \geq \frac{1}{(1+\rho)n}\sum\nolimits_{v \in \OPT\backslash P_i}\overline{\object}_{v}(P_i|R) \geq \frac{1}{(1+\rho)n}(\overline{\object}(\OPT\cup P_{i}|R)-\overline{\object}(P_{i}|R))\geq \frac{1}{(1+\rho)n}(\overline{\object}(\OPT|R)-\overline{\object}(P_{i}|R))$. 
\begin{equation*}
	\begin{aligned}
	\overline{\object}(P_{i+1}|R)&-\overline{\object}(P_i|R)=\overline{\object}_{v_i}(P_i|R)\\
	&\geq \frac{1}{(1+\rho)n}\sum\nolimits_{v \in \OPT\backslash P_i}\overline{\object}_{v}(P_i|R)\\
	&\geq \frac{1}{(1+\rho)n}(\overline{\object}(\OPT\cup P_{i}|R)-\overline{\object}(P_{i}|R))\\
	&\geq \frac{1}{(1+\rho)n}(\overline{\object}(\OPT|R)-\overline{\object}(P_{i}|R)).
	\end{aligned}
\end{equation*}

The solution $P^*$ obtained by Algorithm~4 with $|P^*|= k$. Using the geometric series formula, we have %$\overline{\object}(P^{*}|R)\ge\left( {1 - \left( 1- \frac{{1}}{{(1 + \rho )n}}\right)^n} \right)\overline{\object}\left( \OPT |R\right)\ge\left( {1 - {e^{\frac{{ - n}}{{(1 + \rho )n}}}}} \right)\overline{\object}\left( \OPT |R\right)=\left( {1 - {e^{\frac{{ - 1}}{{(1 + \rho )}}}}} \right)\overline{\object}\left( \OPT |R\right)\ge\left( {(1-1/e-\rho)} \right)\overline{\object}\left( \OPT |R\right)$. 
\begin{equation*}
	\begin{aligned}
	\overline{\object}(P^{*}|R)&\ge\left( {1 - \left( 1- \frac{{1}}{{(1 + \rho )n}}\right)^n} \right)\overline{\object}\left( \OPT |R\right)\\
	&\ge\left( {1 - {e^{\frac{{ - n}}{{(1 + \rho )n}}}}} \right)\overline{\object}\left( \OPT |R\right)\\
	&=\left( {1 - {e^{\frac{{ - 1}}{{(1 + \rho )}}}}} \right)\overline{\object}\left( \OPT |R\right)\\
	&\ge\left( {(1-1/e-\rho)} \right)\overline{\object}\left( \OPT |R\right).
	\end{aligned}
\end{equation*}

Hence, that \psbound~achieves a $(1 - 1/e - \rho)$ approximation ratio for maximizing monotone and submodular functions has been proved.

\noindent
\textbf{\psbound~for upper bound estimation.}  As we analyzed above, Algorithm 4  achieves an approximation factor
of $(1 - 1/e - \rho)$ for maximizing monotone and submodular functions. Based on Lemma~\ref{Hoeffding}, by a similar analysis presented in \cite{DBLP:conf/icde/LiYHC14}, the progressive sampling-based greedy algorithm achieves a ($1-1/e-\epsilon - \rho$) approximation factor through setting an appropriate parameter $X$ with at least ($1-\delta$) probability for upper bound estimation. $\blacksquare$

\begin{thm}\label{proratio}
	The branch and bound framework with sampling-based computeBound achieves an approximation factor of ($1-1/e-\epsilon- \rho$) for the $\prob$ through setting an appropriate parameter $X$.
\end{thm}
$Proof.$ Similar to the proof of Theorem~\ref{ratio}.
Let $P$ denote the solution outputted by Algorithm~4 and $P^*$ denote the optimal solution for progressive sampling-based computeBound.
As we analyzed above, Algorithm~4 achieves a ($1-1/e-\epsilon - \rho$) approximation factor by setting an appropriate parameter $X$ with at least ($1-\delta$) probability. Then we have 
\begin{equation*}
\begin{aligned}
\overline{\object}(P|{\ru}) &\ge (1-1/e-\epsilon -\rho)(\overline{\object}(P^*|{\ru})\\&\ge (1-1/e-\epsilon -\rho)({\object}(P^* |{\ru}).
\end{aligned}
\end{equation*}
Let $P_{out}$ denote the returned solution by Algorithm~2. For any branch that has not been searched, under the termination condition $L<U$. Then we have $\object(P_{out}|{\ru})\ge \overline{\object}(P|{\ru})$. Therefore, Algorithm~2 achieves $\object(P_{out}|{\ru})\ge (1-1/e-\epsilon  - \rho)({\object}(P^* |{\ru})$.
$\blacksquare$




\input{./tex/al_ProGreedyBound}
