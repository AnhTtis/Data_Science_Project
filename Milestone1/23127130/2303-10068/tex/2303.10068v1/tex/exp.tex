\section{Experiments}\label{sec_5}
In this section, we present our experimental results on the effectiveness, efficiency, memory consumption, and scalability of our proposed methods.
\subsection{Experimental settings}
\noindent
\textbf{DataSets.} We use three real-world datasets in the experiments: Gnutella, Email-Enron, and Gowalla. All the datasets are obtained from an open-source website\footnote{http://snap.stanford.edu/data/}, and
their statistics are shown in Table~\ref{tab_datasets}. The Gnutella dataset is a peer-to-peer file-sharing network, the Email-Enron dataset is an email communication network, and the Gowalla dataset is a location-based social networking website where users share their locations by checking in.

\begin{table}[t]	
		\centering
	\caption{Parameter setting.}
	\begin{small}
		\label{exp_param}
		\begin{tabular}[r]{|p{1.8cm}<{\centering}|p{4cm}<{\centering}|}
			\hline
			\multicolumn{1}{|c|}{Parameters}                                  & \multicolumn{1}{c|}{Values} \\ \hline
			$k$                                                       &	50, 100, \textbf{150}, 200, 250\\ \hline
			$|R|$                                                             &	50, 100, \textbf{150}, 200, 250\\ \hline
			$T$                                                               &	3, 6, \textbf{9}, 12, 15    \\ \hline
			$\beta/\alpha$                                                               &	\textbf{3/7}, 3/8, 3/9, 3/10, 3/11    \\ \hline
			$X$                                                               &	500, \textbf{1000}, 1500, 2000, 2500    \\ \hline
			$\rho$                                                               &	0.0001, 0.001, 0.01, \textbf{0.1}, 1    \\ \hline
		\end{tabular}
	\end{small}
\end{table}

\begin{table}[t]
	\caption{Summary of the datasets.}
	\centering
	\begin{small}
		\label{tab_datasets}
		\begin{tabular}[c]{|c|c|c|c|c|}
			\hline
			&$n$			& $m$		     &\#AvgDegree       &\#MaxDegree 	 \\ \hline
			Gnutella    	&8.8k     	 	&63k      	 &7.2    		    &88   		\\ \hline
			Email-Enron    	&37k     		&184k       	 &5.01   		    &1383  		 	 \\ \hline
			Gowalla     	&197k     	&950k          &4.83      	    &14730   	   \\ \hline    	
		\end{tabular}
	\end{small}
\end{table}

\noindent
\textbf{Algorithms.} To the best of our knowledge, this is the first work to study \prob, and thus there exists no previous work for direct comparison. In particular, we compare the four following methods. (1) \degreeTop: It is to select the top-$k$ high block degree nodes in the sampling random walk set as the targeted nodes. (2) \samgreedy: A basic sampling-based greedy algorithm~(Algorithm \ref{al_samgreedy}). (3) \bab~(BAB): The branch-and-bound framework (Algorithm \ref{al_bab}) with Algorithm~\ref{al_sbound} for bound estimations. (4) Progressive \bab~(ProBAB): The branch-and-bound framework (Algorithm \ref{al_bab}) with Algorithm~\ref{al_psbound} for bound estimations.

% \begin{itemize}
% 	\item \degreeTop: It is to select the top-$k$ high block degree nodes in sampling random walk set as the targeted nodes.
% 	\item \samgreedy: A basic sampling-based greedy algorithm~(Algorithm \ref{al_samgreedy}). In each iteration, it selects the node $u$ which maximizes the marginal gain  $(\object({\pr} \cup \{ u \}|{\ru}) - \object({\pr}|{\ru}))$ to a candidate solution set $\pr$, until the budget $k$ is exhausted.
% 	\item \bab~(BAB): The branch-and-bound framework (Algorithm \ref{al_bab}) with Algorithm~\ref{al_sbound} for bound estimations.
% 	\item Progressive \bab~(ProBAB): The branch-and-bound framework (Algorithm \ref{al_bab}) with Algorithm~\ref{al_psbound} for bound estimations.
% \end{itemize}

%\degreeTop, \samgreedy~(Algorithm \ref{al_samgreedy}), and \bab~(BAB for short, Algorithm \ref{al_bab}). 

\noindent
\textbf{Evaluation metrics.} We evaluate the performance of all methods by the runtime and the blocking percentage of the selected nodes. In particular, the percentage is computed by $\object({\pr}|{\ru})/\IS(\ru)$, where $\IS(\ru)$ denote the random walk set influenced by rumor set $\ru$. %Moreover, to accurately measure $\object({\pr}|{\ru})$ for each algorithm, we compute it by running a sufficient number of MC simulations, i.e., $X = 1000$.


\noindent
\textbf{Parameter.} Table \ref{exp_param} shows the settings of all parameters, such as the budget $k$, the size of the rumor set $R$, the (random walk) length threshold $T$, the number of samples $X$, the parameter $\alpha$, the parameter $\beta$ and parameter $\rho$.
Here the default one is highlighted in bold.
To simulate the rumor set $R$, we select nodes uniformly at random from the nodes whose degrees are in the top 10\% of $G$.

% \begin{table}[t]
% 	\caption{Parameter setting.}
% 	\centering
% 	\begin{small}
% 		\label{exp_param}
% 		\begin{tabular}[r]{|p{1.8cm}<{\centering}|p{4cm}<{\centering}|}
% 			\hline
% 			\multicolumn{1}{|c|}{Parameters}                                  & \multicolumn{1}{c|}{Values} \\ \hline
% 			$k$                                                       &	50, 100, \textbf{150}, 200, 250\\ \hline
% 			$|R|$                                                             &	50, 100, \textbf{150}, 200, 250\\ \hline
% 			$T$                                                               &	3, 6, \textbf{9}, 12, 15    \\ \hline
% 			%$\beta/\alpha$                                                               &	\textbf{3/7}, 3/8, 3/9, 3/10, 3/11    \\ \hline
% 			%$X$                                                               &	500, \textbf{1000}, 1500, 2000, 2500    \\ \hline
% 			%$\rho$                                                               &	0.0001, 0.001, 0.01, \textbf{0.1}, 1    \\ \hline
% 		\end{tabular}
% 	\end{small}
% \end{table}

\noindent
\textbf{Setup.} All codes are implemented in Java, and experiments are conducted on a server with 2.1 GHz Intel Xeon 8 Core CPU and 32GB memory running CentOS/6.8 OS.
\input{./tex/Effectiveness}
\input{./tex/Efficiency}
\input{./tex/Memory}
\input{./tex/Scalability}


