\section{Methodology}
\label{sec:method}

In this section, we formalize our objectives, theoretically ground our work, and finally describe our proposed solution.

% Instead of swapping the facial identity from a source image to a target image, we only work with one facial image as input. Given a facial image, we present a framework that changes the facial identity for the purpose of privacy but preserving the other attributes for the purpose of other utilities (\eg, facial expression, gaze, and landmarks). 

\subsection{Problem Formulation}
\label{sec:formalism}

\noindent\textbf{Privacy Utility Dual Optimization.}
Let $\mathcal{X} \subset \mathbb{R}^{3\times H \times W}$ be the image space% with an arbitrary but fixed marginal probability distribution $\mathrm{P}(X)$
, with $x \in \mathcal{X}$ an image depicting an individual.
Let $\left(\mathcal{Z}, d_\mathcal{Z}\right)$ be a metric space, with $\mathcal{Z} \subset \mathbb{R}^{n_\mathcal{Z}}$ space of identity-distilled facial features % with a probability distribution $\mathrm{P}(Z)$
(\ie, facial features that uniquely identify an individual) and $d_\mathcal{Z}: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}$ a distance function attached to space $\mathcal{Z}$.
Let $\left(\mathcal{Y}, d_\mathcal{Y}\right)$ be another metric space, with $\mathcal{Y} \subset \mathbb{R}^{n_\mathcal{Y}}$ containing utility-distilled facial features % with a probability distribution $\mathrm{P}(Y)$
(\ie, features that are useful to downstream tasks) and $d_\mathcal{Y}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ a distance function relating to $\mathcal{Y}$.
We note $f_\mathcal{Z}: \mathcal{X} \rightarrow \mathcal{Z}$ and $f_\mathcal{Y}: \mathcal{X} \rightarrow \mathcal{Y}$ the objective labeling functions respective to each domain. 
% \zikui{(\eg, $f_\mathcal{Z}$ is an ID extractor and $f_\mathcal{Y}$ is a model for downstream task).} \bpnote{$f_\mathcal{Z}$ and $f_\mathcal{Y}$ are the \textit{objective} functions (true labeling functions), which we are trying to approximate via our mixture-of-experts $H_\mathcal{Z}$, $H_\mathcal{Y}$.} 

%(\ie, the functions that, given an image $x \in X$, return the true $z \in Z$ and $y \in Y$).

We define a conditional generative function $G: \mathcal{X} \rightarrow \mathcal{X}$ parameterized by $\theta$, that takes $x \in \mathcal{X}$ as input and returns an edited version $G(x) = \widetilde{x}$. Our goal is to learn a $G$ such that \textit{utility} is maximized (\ie, $f_\mathcal{Y}(x) = f_\mathcal{Y}(\widetilde{x})$) and \textit{privacy} is maximized (\ie, $f_\mathcal{Z}(x)$ is distant from $ f_\mathcal{Z}(\widetilde{x})$). In other terms, the output of $G$ should contain the same utility attributes as the input and contain identity attributes different from the input beyond recognition.
% $\widetilde{x}$ has similar \textit{utility} and overall content but obfuscates the identity of the person beyond recognition.  
% In other terms, $g$ should edit an image of a person in a way that maximizes \textit{utility} ($\widetilde{x}$ should contain the same utility attributes as $x$) and maximizes \textit{privacy} ($\widetilde{x}$ should contain identity attributes distant from the original ones in $x$).
Formally, we want $G$ to achieve Pareto optimality \cite{sener2018multi,momma2022multi} \wrt the aforementioned multiple objectives (\ie, identity obfuscation and utility preservation), accounting for their possible competition (depending on downstream tasks, utility and identity attributes may overlap), thus minimizing the following objective:
\vspace{-3pt}
\begin{equation}
\begin{aligned}
    \min_{\theta} 
        \Bigl(
            -&\mathbb{E}_{x \in \mathcal{X}}\left[
                d_\mathcal{Z}\bigl(f_\mathcal{Z}\left(x\right), f_\mathcal{Z} \circ G_\theta\left(x\right)\bigr)
            \right], \\ 
            &\mathbb{E}_{x \in \mathcal{X}}\left[
                d_\mathcal{Y}\bigl(f_\mathcal{Y}\left(x\right), f_\mathcal{Y} \circ G_\theta\left(x\right)\bigr)
            \right]
        \Bigr)^\intercal
    % \min_{\theta_G} 
    %     \Bigl(
    %         -\underset{x \sim \mathcal{X}}{\mathbb{E}}\left[
    %             d_\mathcal{Z}\bigl(f_\mathcal{Z}\left(x\right), f_\mathcal{Z}\left(\widetilde{x}\right)\bigr)
    %         \right],
    %         \underset{x \sim \mathcal{X}}{\mathbb{E}}\left[
    %             d_\mathcal{Y}\bigl(f_\mathcal{Y}\left(x\right), f_\mathcal{Y}\left(\widetilde{x}\right)\bigr)
    %         \right]
    %     \Bigr)^\intercal
\end{aligned}
\end{equation}
Before tackling the challenges of multi-objective optimization that such a task brings, one has to consider how to model the unknown objective distance and labeling functions $d_\mathcal{Z}, f_\mathcal{Z}$ and $d_\mathcal{Y}, f_\mathcal{Y}$ for the identity and utility space respectively.
We argue that identity and utility are conceptually subjective, \ie, different authoritative entities have different definitions and target features assigned to each concept. 
\eg, given a picture of a person, each human or algorithmic agent will rely on different features (facial landmarks, eye color, \etc) and their own subjective judgment to certify the person's identity, as there is no absolute objective function to perform the ill-posed mapping of a facial picture to an identity. Similarly, the concept of \textit{utility} is conditioned by a set of target tasks or the agents in charge of said tasks. 
\eg, an image with the person's face completely blurred could still be \textit{used} by a person-detection algorithm, but would be \textit{useless} for facial landmark regression.

Therefore, we propose to rely on predefined agents (\textit{experts}) to provide the identity and utility definitions to guide the optimization of our model \cite{gross2005integrating}.
% Instead of directly relying on discrete sets of labels over $\mathcal{Z}$ and $\mathcal{Y}$, 
We thus consider some parameterized models $h_\mathcal{Z}$ and  $h_\mathcal{Y}$ pre-optimized to approximate their respective objective labeling functions $f_\mathcal{Z}$ and $f_\mathcal{Y}$. Note that we make no assumption on the architecture or training of each of these models (we demonstrate with various state-of-the-art identity extraction and recognition models). 
Without loss of generality and to account for individual bias, we define $H_\mathcal{Z} = \left\{h_\mathcal{Z}^i\right\}_{i=1}^{k_\mathcal{Z}}$ and $H_\mathcal{Y} = \left\{h_\mathcal{Y}^i\right\}_{i=1}^{k_\mathcal{Y}}$ as sets of $k_{{\mathcal{Z}}}$ and $k_{{\mathcal{Y}}}$ unique models which differ in terms of architecture and/or training regime, \cf mixture-of-experts theory \cite{miller1996mixture,masoudnia2014mixture,dai2021generalizable}.
% From this definition of subjective experts as identity/utility labeling functions, we can derive an approximation of the distance functions $d_\mathcal{Z}$ and $d_\mathcal{Y}$:
% \begin{equation}
%     \d_{\boldsymbol{\cdot}} = ...
% \end{equation}
We demonstrate in this paper how these identification/utilization experts can be leveraged in an adversarial/collaborative framework to train $g$ towards a satisfying optimum.
% \vspace{-5pt}

\vspace{.5em}
\noindent\textbf{Identity Obfuscation Guarantees.}
To provide formal de-identification guarantees, we ground our work in the extensive theory on $\epsilon$-differential privacy ($\epsilon$-DP) and $\epsilon$-local-differential privacy ($\epsilon$-LDP, relevant when obfuscation should be performed without global knowledge) applied to identity-swapping functions \cite{duchi2013local,dwork2014algorithmic,abadi2016deep,yu2020gan,liu2021dp,croft2021obfuscation,tolle2022content,qiu2022novel}. 
Let $\psi: \mathcal{Z} \rightarrow \mathcal{Z}$ be a function that performs ID obfuscation, 
% via swapping
\ie, taking an identity vector $z$ and returning a new one $\widetilde{z}$ that maximizes $d_\mathcal{Z}(z, \widetilde{z})$.
We consider that an approximate but randomized function $\psi^\epsilon: \mathcal{Z} \rightarrow \mathcal{Z}$ satisfies 
% $\epsilon$-differential privacy
$\epsilon$-LDP
if, for any two adjacent 
% input sets $Z_1$, $Z_2 \subset Z$
inputs $z$, $z' \in \mathcal{Z}$
and for any subset of outputs $Z_s \subseteq \range(\psi^\epsilon)$, it holds that $\mathrm{P}\left(\psi^\epsilon(z)\right) \leq e^\epsilon \mathrm{P}\left(\psi^\epsilon(z')\right)$.
Given $\Delta\psi = \sup_{z, z' \in \mathcal{Z}}\left\|\psi(z) - \psi(z')\right\|_1$ the sensitivity of $\psi$, Laplace noise is commonly leveraged to define an $\epsilon$-DP version of the function: $\psi^\epsilon(z) \triangleq \psi(z) + \left(\Lap\left({\sfrac{\Delta\psi}{\epsilon}}\right)\right)^{n_\mathcal{Z}}$ \cite{duchi2013local,dwork2014algorithmic,abadi2016deep,liu2021dp}.
%
We demonstrate that to ensure $\epsilon$-LDP, the $d_\mathcal{Z}$-maximization property of the identity-obfuscation function has to be relaxed. The manifold of identity vectors generated by an identification function $h_{\mathcal{Z}}$ is bounded by the range of said function. In such a space and for any Euclidean distance $d_\mathcal{Z}$, a non-relaxed version of $\psi$ would be the bijective (and thus non-private) function $\psi_{\text{opp}}$ mapping an ID vector to its opposite. No other function (\eg, $\psi^\epsilon$) could ensure $d_\mathcal{Z}$-maximization.
Therefore, in this work, we consider the inherent trade-off between maximizing swapping-based identity obfuscation and ensuring differential privacy, and we propose a variety of solutions $\psi^\epsilon$ tailored to different needs (as illustrated in Figure~\ref{fig:transform}, and more details in Proposed Solution Section).

\noindent\textbf{Non Re-identifiability.}
Another important aspect to consider in privacy-preserving applications is \textit{non-invertibility}. If the de-identified data can be re-identified with additional information, then the operation is not truly anonymization but \textit{pseudonymization}. For example, with the correct password for Password \cite{gu2020password} and RiDDLE~\cite{li2023riddle}, or using the opposite ID for $\psi_{\text{opp}}$, the original ID is compromized.
% Even if attackers were to %infiltrate the system and 
% acquire a copy of the proposed model $g$, they should not be able to invert its obfuscation process and recover the original images and identity of the depicted individuals (\eg, gradient-descent-based white-box inversion attacks) \cite{fredrikson2015model,wang2015regression,zhang2022visual}.
We empirically demonstrate that the proposed obfuscation solutions achieve varying degrees of robustness to such re-identification efforts. 
% \zikui{this should be rewritten}

In the remaining of the section, we explain how we define and train $g$ to ensure privacy-preserving non-invertible identity swapping in images and utility preservation.


\subsection{Proposed Solution}\label{sec:solution}

The proposed architecture can be defined as the composition of a face-swapping model $g: \mathcal{X} \times \mathcal{Z} \rightarrow \mathcal{X}$, an identity extractor $h_{\mathcal{Z}}: \mathcal{X} \rightarrow \mathcal{Z}$, and an identity obfuscation function $\psi^\epsilon: \mathcal{Z} \rightarrow \mathcal{Z}$, such that $G(x) = g\left(x, \psi^\epsilon \circ h_{\mathcal{Z}}(x)\right)$. 
Given a facial image $x$, $h_{\mathcal{Z}}$ extracts the vector $z$ encoding the identity of the depicted person. This vector $z$ is passed to the privacy-enabling function $\psi^\epsilon$, which returns a synthetic identity $\widetilde{z}$ that maximizes obfuscation. 
Finally, the face-swapper model $g$ edits the original image $x$ to inject the fake identity $\widetilde{z}$, resulting in an image $\widetilde{x}$ where the original visual identifying attributes are replaced by those encoded in $\widetilde{z}$. 
Additionally, during its training, $g$ relies on the feedback of tasks-specific models $h_{\mathcal{Y}}^i: \mathcal{X} \rightarrow \mathcal{Y}$ to ensure that the utility of $\widetilde{x}$ is maintained compared to $x$.
We expand on each block in the following paragraphs.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/transform.pdf}
\caption{Identity transformation. The identity vector is normalized to the surface of a unit n-sphere.}
\label{fig:transform}\vspace{-12pt}
\end{figure}

% \subsection{Identity Obfuscation with Privacy Guarantees}

% \paragraph{
\vspace{.2em}
\noindent\textbf{
Identity Extraction.} 
As mentioned in Section~\ref{sec:formalism}, we propose to extract the identity information from facial images via model ensembling \cite{miller1996mixture,masoudnia2014mixture,dai2021generalizable}, to ensure generalizability 
%of the proposed solution
as well as to limit the impact of models' bias (as we assume no control over the architecture or training regimen of selected identity-expert models).
Therefore, given a set $H_\mathcal{Z} = \left\{h_\mathcal{Z}^i\right\}_{i=1}^{k_\mathcal{Z}}$ of ID extractors, we define $h_{\mathcal{Z}}$ as the ensemble method $h_{\mathcal{Z}}(x) = \MLP_{\theta_z}\big(\concat_{i=1}^{k_\mathcal{Z}}h_\mathcal{Z}^i(x)\big)$, \ie, concatenating (symbol $\Vert$) the $k_\mathcal{Z}$ predicted vectors together and merging them into $z \in \mathcal{Z}$ via a multilayer perceptron (MLP) with parameters $\theta_z$ and $\tanh$ as final activation. 
% Therefore, given a set $H_\mathcal{Z} = \left\{h_\mathcal{Z}^i\right\}_{i=1}^{k_\mathcal{Z}}$ of ID extractors, we define $h_{\mathcal{Z}}$ as the ensemble method $h_{\mathcal{Z}}(x) = \concat_{i=1}^{k_\mathcal{Z}}h_\mathcal{Z}^i(x)$, \ie, concatenating (symbol $\concat$) the $k_\mathcal{Z}$ extracted vectors together. To further merge the concatenated vectors into one, we also propose a multi-layer perceptron $\MLP_{\theta_z}: \mathcal{Z}^{k_\mathcal{Z}} \rightarrow  \mathcal{Z}$, with parameters $\theta_z$ and $\tanh$ as final activation. It is applied after the identity obfuscation, so $\phi$ can be self-supervised during training: $\widetilde{z} = \MLP_{\theta_z} \circ\; \phi \circ h_{\mathcal{Z}}(x)$.



% Facial identity extraction plays an important role in disentangling the identity information and the other attributes of a facial image. We can extract the facial identity vector $v$ from a pretrained face recognition model. However, simply relying on one pretrained model will have bias towards the model because of its training data, network architecture, and training strategy. Furthermore, we do not know which facial recognition model people will use to recognize/attack our output facial images. In this paper, we combine the identity information from two different pretrained face recognition models to improve the robustness and generality. We concatenate the two identity vectors and merge the identity information with a multilayer perceptron (MLP) as
% \begin{align}\label{eq:idenity}
% v = MLP(v_A \oplus v_B).
% \end{align}

% \paragraph{
\vspace{.2em}
\noindent\textbf{
Identity Transformation.}
\label{sec:id_trans}
A variety of techniques can be considered to perform the identity transformation $\psi$, as shown in Figure~\ref{fig:transform}. 
If we were to maximize the distance between the original and obfuscating IDs, the optimal function would be $\psi_{\text{opp}}(z) = -z$ since $\argmax_{z'}d_\mathcal{Z}(z, z') = -z$  in our normalized Euclidean identity space. 
However, such a function is reversible, making it easy to re-identify the original individual by taking the opposite of the pseudonymized ID.
% does not guarantee any privacy \zikui{any privacy? maybe just non-inversion not de-id.}, as attackers can easily invert it and recover the original vector (\cf Section~\ref{sec:formalism}) \zikui{given that the attacker knows the opposite id is used and also the id extractor}.
A more secure solution would be a parametric function, \eg, $\psi_{\text{mlp}}(z) = \MLP_{\theta_{\psi}}(z)$, trained to optimally fool $h_{\mathcal{Z}}$.  
As a non-explicit function, $\psi_{\text{mlp}}$ is more challenging to invert, though not impossible with the access to the model or its parameters $\theta_{\psi}$ (\cf gradient-based attacks \cite{fredrikson2015model,wang2015regression}). 
To increase robustness and ensure $\epsilon$-LDP, we can add dimension-wise noise to the inner operation, \ie, $\psi^\epsilon_{\text{mlp}}(z) = \MLP_{\theta_{\psi}}\left(z + \left(\Lap\left(\beta\right)\right)^{n_\mathcal{Z}}\right)$, with $\beta = {\frac{\Delta\psi_{\text{mlp}}}{\epsilon}}$.
The larger $\beta$ is set (\ie, the smaller $\epsilon$ is), the more noise is applied to the original ID vector before further MLP-based obfuscation. Therefore, larger $\beta$ provides stricter privacy guarantee and robustness but adversarial affects the ability of $\psi^\epsilon_{\text{mlp}}$ to learn how to fool identification experts $H_\mathcal{Z}$.

To better navigate this trade-off and guarantee a more continuous space for the noise application, we leverage the properties inherent to variational autoencoders (VAEs) \cite{kingma2013auto}. We introduce a variational encoder-decoder (VED) to transform the identity vector, \ie,
$\psi^\epsilon_{\text{ved}}(z) = \VED_{\theta_{\psi'}}\left(z\right)$. %, 
% s.t. $\|\psi^\epsilon_{\text{ved}}(z) - z\| > m$
%where $m$ is a hyperparameter fixing the minimum threshold between the original and transformed identity vectors. 
This model's encoder predicts the parameters $(\mu, \sigma)$ of the latent data distribution (assumed to be Gaussian). 
A latent vector $v_z$ is picked as $\mu + \sigma \eta$ with $\eta \sim \left(\mathcal{N}(0, 1)\right)^{n_v}$ (\cf reparameterization \cite{kingma2015variational}) then passed to the decoder. While a VAE decoder would reconstruct the input identity from $v_z$, our VED decoder should generate a new, distant identity.
During inference, we sample $v_z$ as $\mu + \sigma \left(\Lap\left(\alpha\right)\right)^{n_v}$ to meet $\epsilon$-LDP, with ${n_v}$ dimension of latent space and $\alpha = {\frac{\Delta\psi_{\text{ved}}}{\epsilon}}$. To train either of these models, we enforce cosine dissimilarity between the original and generated ID vectors:
% \vspace{-5pt}

\begin{equation}
    \mathcal{L}_{\text{deid}} = 1 + \frac{z \cdot \widetilde{z}}{\|z\|_2\|\widetilde{z}\|_2}.
\end{equation}
For the VED model, we add to this criterion the usual Kullback–Leibler divergence (KLD) loss $\mathcal{L}_{\text{kld}}$ \cite{kingma2015variational,kingma2013auto}.

% For the purpose of privacy, we need to change the facial identity of the input image such that the person cannot be identified with any facial recognition models. Importantly, the identity should also not be able to inverted back from the changed identity (\ie, non-invertibility). 

% Since the facial identity vector is normalized, the most straightforward way to transform the identity vector is choosing its opposite as 
% \begin{align}\label{eq:opp}
% v_{opp} = -v,
% \end{align}
% which has the largest L2 norm distance $d = 2$ between the two identity vectors. However, choosing the opposite identity vector cannot serve for the privacy purpose, since attackers can easily find the original identity vector by choose the opposite of the opposite identity vector as, $v = - v_{opp}$. Furthermore, the largest L2 norm distance does not mean the lowest recognition rate for a facial recognition model. 

% Another way to transform the facial identity is using an MLP model to map the identity vector to an identity vector that has the lowest facial recognition rate as, 
% \begin{align}\label{eq:mlp}
% v_{mlp} = \text{MLP}(v).
% \end{align}
% Since the transformation model is not an explicit function, attackers cannot invert back to the original identity vector arithmetically. However, if attackers have the access to our transformation model, the original facial vector can be recovered through backpropagation. 

% We can resolve the issue of invertibility through backpropagation by adding noise to the identity vector as 
% \begin{align}\label{eq:mlpn}
% v_{mlp+n} & = \text{MLP}((1-\beta)v + \beta \Bar{n}), \\
% \textrm{s.t.} \quad & \|v_{mlp+n} - v\| > T, \nonumber
% \end{align}
% where $n$ is Gaussian noise and $\Bar{n} = \frac{n}{||n||_2}$, $\beta$ is the factor to control the level of noise being added to the identity vector, and $T$ is the minimum threshold between the transformed identity vector and the input identity vector. When we choose a larger $\beta$, it is more difficult to invert back to the original identity vector (\ie, low invertibility) but results in higher recognition rates for attackers. We need to find a trade-off between the invertibility and recognition rate to for the privacy purpose. 
% 
% The identity vector space learned from the MLP transformation model does not guarantee to be continues. Thus, it is difficult to sample the noise and to control the trade-off abovementioned. Inspired by the nice continuity property of the latent space in variational auto encoders (VAEs), we introduce a variational encoder decoder (VED) to transform the identity vector as, 
% \begin{align}\label{eq:ved}
% v_{ved} & = \text{VED}(v), \\
% \textrm{s.t.} \quad & \|v_{ved} - v\| > T, \nonumber
% \end{align}
% where $T$ is the minimum threshold between the transformed identity vector and the input identity vector. Since the latent space of our VED transformation model is close to Gaussian distribution, we can adjust the privacy level by scaling the predicted variance when sampling the latent vector. When we sample a latent vector with a larger variance, it is more difficult to invert back but results in higher recognition rate. Since the latent space has a nice continuity property, it is easier to control the trade-off between the invertibility and recognition rate for the privacy purpose. 

% \paragraph{
\vspace{.2em}
\noindent\textbf{
Face Swapping.}
Once the fake identity vector $\widetilde{z}$ is generated, it is passed to the face-swapping model $g$, along with the original image $x$. 
Similar to existing solutions \cite{chen2020simswap,perov2020deepfacelab,liu2021dp}, $g$ is composed of three modules: (1) an image encoder that extracts identity-unrelated features $\nu$; (2) an ID injector that aggregates $\nu$ and $\widetilde{z}$ into a vector encoding the content of the obfuscated image $\widetilde{x}$; (3) a decoder conditioned on this vector that generates $\widetilde{x}$. 
These existing works also share similar losses that we borrow and adapt:
% \setlist{nolistsep}
% \begin{itemize}[noitemsep, wide=5pt]
%     \item $\mathcal{L}_{\text{mix}} = \|g(x, \widetilde{z}) - g(x, z)\|_1$, a mixing loss to ensure implicit disentanglement of identity features (encoded in $z$ or $\widetilde{z}$) and residual features (\ie, $\nu$); 
%     \item $\mathcal{L}_{\text{gen}} = \sum_{i=1}^{k_d}\log\left(1 - d_i(x, \widetilde{x})\right)$, an adversarial loss pitting the generator against $k_d$ discriminators to ensure realistic results preserving image saliency, \cf recent GAN solutions \cite{wang2018pix2pixHD,hukkelaas2019deepprivacy,chen2020simswap} (from these methods, we also use their weak-feature matching loss, further ensuring the alignment of high-level semantic information between the image pairs);
%     \item $\mathcal{L}_{\text{id}} = \sum_{\hat{z} \in \{z, \widetilde{z}\}}\left(1 - \frac{\hat{z} \cdot \hat{z}_h}{\|\hat{z}\|_2\|\hat{z}_h\|_2}\right)$, with $\hat{z}_h = h_\mathcal{Z}\left(g(x, \hat{z})\right)$, \ie, a loss enforcing cosine similarity between the injected identity $\hat{z}$ and the one observed by the identification model $h_\mathcal{Z}$ in the resulting image.
% \end{itemize}

\vspace{-5pt}
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{\text{mix}} &= \|g(x, \widetilde{z}) - g(x, z)\|_1 ; 
        \\
        \mathcal{L}_{\text{gen}} &= \sum_{i=1}^{k_d}\log\left(1 - D_i(x, \widetilde{x})\right) ;
        \\
        \mathcal{L}_{\text{id}} &= \sum_{\hat{z} \in \{z, \widetilde{z}\}}
            % \left(
            \Big(
            1 - \frac{\hat{z} \cdot \hat{z}_h}{\|\hat{z}\|_2\|\hat{z}_h\|_2}
            % \right)
            \Big)
            ;
    \end{aligned}
\end{equation} 
with $\hat{z}_h = h_\mathcal{Z}\left(g(x, \hat{z})\right)$.
Here, $\mathcal{L}_{\text{mix}}$ is a mixing loss to ensure implicit disentanglement of ID features (encoded in $z$ or $\widetilde{z}$) and residual features (\ie, $\nu$).
$\mathcal{L}_{\text{gen}}$ pits the generator against $k_d$ discriminators $D$ to ensure realistic results preserving image saliency, \cf recent GAN solutions \cite{wang2018pix2pixHD,hukkelaas2019deepprivacy,chen2020simswap} (we also use their weak-feature matching loss, further ensuring the high-level semantic alignment between the image pairs).
Finally, $\mathcal{L}_{\text{id}}$ enforces cosine similarity between the injected identity $\hat{z}$ and the one observed by the identification model $h_\mathcal{Z}$ in the resulting image.
Combined together, along with $\mathcal{L}_{\text{deid}}$ and $\mathcal{L}_{\text{kld}}$ (using weighting hyperparameters), these losses form the overall objective for our privacy-enforcing face-swapping solution $G$. 

\input{figures/fig-qualities}
% \input{figures/fig-pipeline-training}

% \paragraph{
\vspace{.5em}
\noindent\textbf{
Utility Preservation.}
Existing face-swapping methods \cite{hukkelaas2019deepprivacy,chen2020simswap,perov2020deepfacelab,liu2021dp} claim that their adversarial and feature-matching losses ensure the preservation of non-identifying content. However, such supervision is too weak to guarantee that the images will maintain their utility \wrt downstream tasks, especially for tasks relying on small attention regions (\eg, gaze estimation). 
We thus complement the aforementioned objective with a criterion that leverages the implicit expertise of tasks-relevant models $H_\mathcal{Y}$:
% \vspace{-5pt}

\begin{equation}\label{eq:uti_loss}
    \mathcal{L}_{\text{uti}} = \sum_{i=1}^{k_\mathcal{Y}} \lambda_{\text{uti},i} \| h_\mathcal{Y}^{i,l}(x) - h_\mathcal{Y}^{i,l}(\widetilde{x}) \|_1,
\end{equation}
with $h_\mathcal{Y}^{i,l}(\boldsymbol{\cdot})$ the features returned by the last differential non-softmax layer $l$ of model $h_\mathcal{Y}^i$, and $\lambda_{\text{uti}} \in \mathbb{R}^{k_\mathcal{Y}}$ hyperparameters weighting the task/expert contributions. 
Hence, $\mathcal{L}_{\text{uti}}$ imposes that altered images contains the same utility attributes as original images, as expected by tasks-relevant models.

Note that the entire solution $G(x) = g\left(x, \psi^\epsilon \circ h_{\mathcal{Z}}(x)\right)$ is end-to-end differentiable, thus single-pass trainable. 
In practice, we leverage its modularity and train each component separately before jointly fine-tuning.
Scalar hyperparameters weigh the contribution of each loss to the total objective (we fix $\{\lambda_{\text{id}}, \lambda_{\text{deid}}, \lambda_{\text{mix}}, \lambda_{\text{uti,eye}}, \lambda_{\text{uti,emo}}, \lambda_{\text{kld}}\} = \{30, 30, 10, 2, 2, 0.2\}$).
% (see next section and supplementary material for details on  curriculum).

% In practice, we leverage the components' modularity and train each separately before joint fine-tuning (see next section and supplementary material for details on  curriculum).


% Following SimSwap \cite{chen2020simswap}, the weak feature matching loss is used to implicitly constrain the identity-unrelated attribute information. In addition, we explicitly constrain some utilities (\ie, emotion and gaze) using pretrained models. Even though we cannot enforce all the attributes explicitly with pretrained models, considering one or some attributes not only benefits to those considered attributes, but also improves the other attributes. This is because, many attributes are naturally correlated, \eg, facial emotion and facial landmark, gaze and head pose, and so on. At last, since many utility models are trained on datasets with large number of people, they further help our model disentangle the identity information from other attributes.

% \subsection{Loss function design}

% We keep the four loss components used in SimSwap \cite{chen2020simswap}, including identity loss $L_{id}$, reconstruction loss $L_{recon}$, adversarial loss $L_{adv}$, and weak feature matching loss $L_{wfm}$. In addition, we introduce adversary identity loss $L_{aid}$, utility loss $L_{util}$, and Kullback–Leibler divergence (KLD) loss $L_{kld}$. 

% \textbf{Adversary identity loss} In order to enforce the transformed identity to have a low recognition rate, we use the negative cosine similarity to calculate the distance between the input identity vector and the transformed identity vector as,
% \begin{align}\label{eq:loss-adv-id}
% L_{aid} & = 1 + \frac{v_T\cdot v}{\|v_T\|_2\|v\|_2},
% \end{align}
% where $v_T$ can be $v_{mlp}$ from the MLP transformation model or $v_{ved}$ from the VED transformation model. 

% \textbf{Utility loss} We use pretraiend utility models to constrain the network explicitly. The utility loss is formulated differently depending on which utilities are selected. We denote the utility loss as $L_{util}$.

% \textbf{KLD loss} For the VED transformation model, we need to add a KLD term to constrain the latent space of the VED to be close to a normal distribution as, 
% \begin{align}\label{eq:loss-adv-id}
% L_{kld} & = D_{KL}(P\|Q),
% \end{align}
% where $P$ and $Q$ are the distributions of the latent space and a normal distribution, respectively.  

% The overall loss is formulated as
% $\mathcal{L} = & \lambda_{id}L_{id} + \lambda_{gen}\mathcal{L}_{\text{gen}} +  \lambda_{wfm}L_{wfm}
% & + \lambda_{aid}L_{aid} + \lambda_{util}L_{util} + \lambda_{kld}L_{kld} $
% where $\lambda_{id} = \lambda_{aid} = 30$, $\lambda_{recon} = \lambda_{wfm} = 10$, $ \lambda_{util} = 2$, and $\lambda_{kld} = 0.05$ when we use VED transformation model or $\lambda_{kld} = 0$ when we use MLP transformation model.
% \zikui{need an overall loss.} \bpnote{I think this can be kept for the sup-mat.}