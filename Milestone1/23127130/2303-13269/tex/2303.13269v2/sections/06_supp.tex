\newpage
\clearpage
\label{sec:appendix}
% \noindent \begin{center} {\LARGE  \textbf{Supplementary Material}} \end{center}
\begin{strip} 
    \begin{center} 
        {\LARGE  \textbf{Disguise without Disruption: Utility-Preserving Face De-Identification \\ (Supplementary Material)}}
    \end{center} 
\end{strip} 
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}


% % resolve indexing issue
% \makeatletter
% \def\mysequence#1{\expandafter\@mysequence\csname c@#1\endcsname}
% \def\@mysequence#1{%
%   \ifcase#1\or -\or - \or -\or -\or -\or A\or B\or C\or D\or E\or F\or G\or H\else\@ctrerr\fi}
% \makeatother
% \renewcommand\thesection{\mysequence{section}}


\section*{Summary}
In this supplementary material, we augment the main paper in the following aspects: 
\begin{itemize}
    \item We provide comprehensive implementation details of our framework \textit{Disguise};
    \item We conduct additional ablation studies covering various variations of our methods;
    \item We include extra qualitative comparisons with other techniques using images in real-world scenarios, encompassing medical settings and images containing multiple faces.
\end{itemize}


\section{Further Implementation Details}
\label{sec:implementation}

\noindent\textbf{Architecture.} We build our face-swapping model $g$ based on the architecture and training framework proposed in SimSwap \cite{chen2020simswap}. The identity-merging module uses a $\MLP_{\theta_z}$ with two layers and feature sizes of [1024, 1024, 512]. The identity-transforming $\MLP_{\theta_z}$, on the other hand, is a 3-layer network with feature sizes [512, 2048, 1024, 512]. 
The VED encoder consists of two dense layers of sizes [512, 1024, 1024], followed by two parallel layers of size [512] to predict the mean and variance in latent space. The VED decoder has three dense layers of sizes [512, 1024, 1024, 512]. 
We use $\tanh$ as final activation throughout the model.

\noindent\textbf{Training.} We apply the Adam optimizer \cite{kingma2014adam} with $\beta_1=0$ and $\beta_2=0.999$, learning rate $10^{-4}$, and a batch size of $4$. 
% During the training, we alternately set one batch for image pairs with the same identity and another for image pairs with different identities. 
We train our pipeline in two stages: (1) we first pre-train the face-swapper $g$ according to \cite{chen2020simswap} for 1M iterations; (2) then we fine-tune it together with utility module and ID transformer for another 100k iterations. This is illustrated in Figure \ref{fig:training_phases}.

\input{figures/fig-pipeline-training}

% \subsection{Training}
% As mentioned in Subsection \refwithdefault{sec:protocol}{4.1}, while our solution is end-to-end differentiable, we observe better optima when splitting the training regimen into two phases: (1) we first pre-train the face-swapper $g_{\text{fs}}$ according to \cite{chen2020simswap} for 1M iterations; (2) then we fine-tune it together with utility module and ID transformer for another 100k iterations. This is illustrated in Figure \ref{fig:training_phases}.

% \bpnote{opt. present additional SimSwap training losses.}

\noindent\textbf{Evaluation.} For the de-correlation evaluation presented in Figure \ref{fig:hist-4dist}(e), the MLP attacker networks consist of three layers of feature sizes [512, 2048, 1024, 512], tasked to reconstruct the original identity embedding from the obfuscated one extracted from the edited image. 
We train one attacker specific to each obfuscation method (CIAGAN \cite{maximov2020ciagan}, DeepPrivacy \cite{hukkelaas2019deepprivacy}, ours, \etc). 
We use Adam optimizer with a learning rate of $10^{-3}$ and a total epoch of 100 epochs. We trained the decoders on CelebA-HQ and evaluated them on LFW.

% \bpnote{opt. provide training loss detail for  HRNetv2-W18 applied to landmark detection.}

% \zgnote{We train a decoder consisting of three layers of MLP with feature sizes [512, 2048, 1024, 512] to reconstruct the original embedding from the obfuscated embedding, rather than from the ground truth image. This is because the identity embedding lacks facial pose, expression, and background information, making it challenging to reconstruct the ground truth image from the obfuscated embedding alone. We use Adam optimizer with a learning rate of $1e-3$ and a total epoch of 100 epochs. We trained the decoders on CelebA-HQ and evaluated them on LFW.}



\section{More Ablation Studies and Analyses}
\label{sec:quantitative}

Complementing Ablation Study section in the main paper, we delve deeper into assessing how diverse ID extraction methods, ID transformation techniques, and utility experts can collectively influence the overall obfuscation pipeline.

\input{figures/fig-hist-roc-all}
\input{tables/tab-compare-deid-ablation}
\input{tables/tab-compare-utility-both-ablation}

\noindent\textbf{Impact of ID extraction models.}
Existing face swapping solutions \cite{chen2020simswap} also leverage out-of-the-box identification networks (\eg, ArcFace \cite{deng2019arcface} as the most common choice), but they do not provide any analysis on the possible bias that these pretrained methods may have and how such bias may impact the de-identification process, \eg, by improperly disentangling some facial features.

\input{tables/tab-supp-invertibility.tex}
% \input{tables/tab-supp-utility.tex}
\input{tables/tab-supp-direct-noise.tex}

% To address this issue, we provide such analyses in Tables \ref{tab:compare-deid-ablation}, \ref{tab:compare-utility-ablation} and \ref{tab:supp-invertibility}. Since our method can leverage multiple ID extractors, we compare various versions of our solutions, \ie, either relying on ArcFace \cite{deng2019arcface}, AdaFace \cite{kim2022adaface}, SphereFace \cite{Liu_2017_CVPR}, or a combination of these three methods (note that for fairness, we do not use SphereFace to evaluate the methods using SphereFace as ID extractor).
% Table \ref{tab:supp-invertibility} shows that combining multiple ID extractors can achieve better de-identification and non-invertibility. This is especially visible for the pipeline versions relying on AdaFace. By itself, it seems like this solution suffers from some bias or lack of performance (at least compared to the adversarial FaceNet \cite{schroff2015facenet} or SphereFace \cite{Liu_2017_CVPR} used for re-identification) and probably misses some biometric features, causing higher re-identification rate of the obfuscated images compared to versions relying on other ID extractors. But once combined with another identification method, \eg, ArcFace \cite{deng2019arcface}, the re-identification rate drops.
% More significantly even, combining multiple identity extractors makes our solution much more robust to inversion attacks (\cf last two columns in Table \ref{tab:supp-invertibility}), as expected from mixture-of-experts solutions.

% However, we still observe a trade-off between privacy preservation and utility preservation. As shown in Table \ref{tab:compare-utility-ablation}, our solutions relying on multiple ID extractors tend to have a slightly bigger impact on utility features, causing some accuracy drop \wrt the selected downstream tasks. Navigating this trade-off and coming up with a solution that better disentangles identity and utility features (given that they do not overlap) remains an open problem, though we believe \textit{Disguise} to be a significant step forward (\cf comparison to state-of-the-art in the main paper and in this document).

To address this concern, we present our analyses in Tables \ref{tab:compare-deid-ablation}, \ref{tab:compare-utility-ablation}, and \ref{tab:supp-invertibility}. Our method can harness multiple ID extractors, thus we compare distinct versions of our solutions: employing ArcFace \cite{deng2019arcface}, AdaFace \cite{kim2022adaface}, SphereFace \cite{Liu_2017_CVPR}, or a fusion of these methods. Notably, we exclude the assessment on one ID extractor when it is utilized in the de-identification pipeline, \eg AdaFace in Table \ref{tab:compare-deid-ablation} last tow rows and SphereFace in Table \ref{tab:supp-invertibility} last tow rows, ensuring fairness.

Table \ref{tab:supp-invertibility} underscores that combining various ID extractors yields enhanced de-identification and non-invertibility. Particularly with AdaFace-based pipelines, this effect is evident. When solely used, AdaFace exhibits slight bias or performance limitations (vis-à-vis FaceNet \cite{schroff2015facenet} or SphereFace \cite{Liu_2017_CVPR} for re-identification), possibly due to missing biometric features, leading to higher re-identification rates post-obfuscation compared to other ID extractors. However, coupling AdaFace with an alternative ID method like ArcFace \cite{deng2019arcface} mitigates the re-identification rate effectively. Moreover, combining multiple identity extractors notably boosts resilience against inversion attacks (as evident in the last two columns of Table \ref{tab:supp-invertibility}), as anticipated from mixture-of-experts approaches.

Nonetheless, a trade-off between preserving privacy and utility remains observable. As Table \ref{tab:compare-utility-ablation} illustrates, solutions leveraging multiple ID extractors tend to exert a slightly greater impact on utility attributes, resulting in a minor accuracy dip for the designated downstream tasks. Navigating this trade-off and devising a solution that better disentangles identity and utility attributes—given their non-overlapping nature—remains an open challenge. Nevertheless, we believe that \textit{Disguise} represents a substantial stride forward in this regard (as evident from comparisons to state-of-the-art in both the main paper and this document).

% \subsection{Noise-based Tampering of ID Vectors}

% \input{tables/tab-supp-gauslaplac}

% \bpnote{should we add the discussion on $\epsilon$-LDP and the sensitivity evaluation from the rebuttal here, or should we avoid putting too much emphasis on LDP?} \zikui{It is fine to add here if it helps and does not introduce confusion. How does the number $33.917$ in the rebuttal connect to the numbers in our experiments?}
% \bpnote{I've added a paragraph below accordingly, please check. The sensitivity value computed for the rebuttal links $\alpha$ and $\epsilon$ (\cf equations below).} \zikui{looks good!}


\noindent\textbf{Impact of ID transformation models.}
Figure \ref{fig:hist-all} extends the analysis presented in Tables \ref{tab:compare-invertibility} and \ref{tab:both-noise}, highlighting the superiority of our VED-based obfuscation scheme compared to the other MLP-based proposed solution, as well as their superiority compared to prior art. 
The first row in Figure \ref{fig:hist-all} shows that compared to other methods, our VED-based model is further from the positive pairs both on the histogram and ROC curve, demonstrating the best de-identification ability. The second row shows that when we introduce more $\beta$ noise in our MLP model, both the histogram and ROC curve move closer towards the positive pairs. When $\beta=0.5$, our MLP model can de-identify facial images on which recognition model has performance close to random guess. For our VED model, when increasing the $\alpha$ noise, the histogram stays close to the negative pairs and the ROC curve stays close to the diagonal line, as shown in the third row. These results suggest that our VED model achieves the best de-identification while ensuring non re-identifiability. 

As a reminder, we define $\alpha$ and $\beta$ as inversely proportional to $\epsilon$, \cf $\alpha = \frac{\Delta\psi}{\epsilon}$ and $\beta = {\frac{\Delta\psi_{\text{mlp}}}{\epsilon}}$.
As a measure of privacy budget, the higher $\epsilon$ is fixed (\ie, the lower  $\alpha$ or $\beta$), the higher the privacy loss, \cf $\log{\mathrm{P}(\widetilde{z}|z)} - \log{\mathrm{P}(\widetilde{z}|z') \leq \epsilon}$ according to the formal definition in Problem Formulation subsection.
Local differential privacy (LDP) guarantees that an adversary observing $\widetilde{z}$ cannot determine with some degree of confidence if it comes from $z$ or $z'$. \Eg, $\epsilon = 0$ would mean zero confidence in linking a masked ID to a specific input one, as only noise would be transferred (\cf Laplacian noise with $\alpha = \frac{\Delta\psi}{\epsilon} = \infty$). 
To choose $\epsilon$ (and thus $\alpha$) adequately based on privacy budget, one should first estimate the sensitivity $\Delta$ of the processing function. Following standard practice \cite{liu2021dp,wen2022identitydp}, we measure the sensitivity of ours empirically: \eg, over LFW dataset, we obtain $\Delta\psi_{\text{ved}} = \sup_{z, z'}\left\|\psi(z) - \psi(z')\right\|_1 = 33.92$ (\eg, hence fixing $\alpha = 2$ means opting for a relative privacy budget equals to $\epsilon = 67.84$). 

% We also complement the qualitative analysis provided in Table \ref{tab:both-noise} with Table \ref{tab:supp_noise}, showing the de-identification and non-invertibility performance of the ID transformation scheme which consists of applying only $\epsilon$-controlled Laplace noise to the features (without any additional neural functions to further obfuscate the vector). 
% % This table should be compared to similar Table \refwithdefault{tab:both-noise}{3} in the main paper, which showcases our ID transformation schemes (with MLP/VED) on the same metrics.
% Our proposed MLP and VED greatly improve identity obfuscation, compared to only applying noise to the features, as evident in Table \ref{tab:supp_noise}. However, their continuous property makes them more prone to re-identification, for similar privacy budgets (\eg, for small noise values, they may be mapping different inputs to the same fake identity). Nevertheless, we believe that our proposed VED-based solution provides the best trade-off between maximal de-identification and non re-identifiability.

We enhance the analysis presented in Table \ref{tab:both-noise} with additional insights from Table \ref{tab:supp_noise}. This new table illustrates the performance of the ID transformation scheme, which entails applying solely $\epsilon$-controlled Laplace noise to the features without employing additional neural networks for further vector obfuscation.
Comparatively, our proposed Multi-Layer Perceptron (MLP) and Variational Encoder-Decoder (VED) solutions distinctly elevate identity obfuscation beyond the effects of noise-only feature manipulation, as depicted in Table \ref{tab:supp_noise}. However, their continuous nature renders them more susceptible to re-identification risk, especially for similar privacy budgets. In cases of slight noise values, they could inadvertently map distinct inputs to a common fabricated identity. Despite this, we maintain the conviction that our VED-based approach strikes the most optimal balance between maximal de-identification and non-reidentifiability.

\noindent\textbf{Comparison with other noise-based ID tampering methods.}
Some other methods \cite{li2019anonymousnet,liu2021dp,li2021differentially,wen2022identitydp}, have been recently proposed to tackle de-identification of facial images by extracting identity features from the target data, altering it, and decoding it back into a similar but obfuscated image.
While we could not satisfyingly reproduce their results (no implementation has been released), we could approximate their solution using our own framework.
Indeed, most of these methods can be described as a subset of our modular solution, \ie, minus our main contributions. 
This is especially true for DP-Image \cite{liu2021dp} (not peer-reviewed yet) and IdentityDP \cite{wen2022identitydp} (published in August, the \nth{28}, 2022), which use an image encoder-decoder combined with an ID extractor \cite{deng2019arcface} and ID/image feature mixer, similar to ours. 
However, they do not provide our additional guarantees in terms of disentanglement of the facial attributes and preservation of the utility ones by using mixture-of-experts supervision. 
More importantly, they obfuscate the extracted ID vector (before injecting it with the residual image features and decoding it back into an image) only by adding Laplace noise to them. They do not leverage additional transformations in the ID latent space to ensure optimal de-identification, such as our MLP and VED neural functions.

To highlight the impact of our proposed ID transformation functions and indirectly compare to these other solutions, we direct the readers to Figure \ref{fig:sup_qualinoise}.
For each original image, we display the results obtained by transforming the extracted ID features either after only adding Laplace-based noise to them (first row); after applying our proposed $\psi^\epsilon_{\text{mlp}}$, \ie, adding  Laplace noise and then passing the vector to our MLP optimized to ensure de-identification (second row); or after passing the vector to our VED $\psi^\epsilon_{\text{ved}}$, which also applies $\epsilon$-controlled noise to the data in its own latent space (third row).
For each solution, we provide several results with different privacy budgets ($\beta$ parameter, encompassing $\epsilon$).

We observe that applying only $\epsilon$-controlled noise to the ID vector results in images barely obfuscated (\eg, same nose/cheek/eyebrow shapes) compared to additionally using our proposed neural functions, for the same privacy budgets $\beta$. Furthermore, our VED-based solution provides better continuity in the obfuscated results \wrt $\beta$ compared to the other two variants. Such continuity makes choosing an adequate privacy budget much more intuitive and straightforward for users.
\input{figures/fig-qualities-sup3}


\noindent\textbf{Impact of utility experts.}
% We observe that fine-tuning with utility experts will improve the preservation on the down-stream tasks, as show in Tables \ref{tab:compare-deid-ablation}, \ref{tab:compare-utility-ablation}. Specifically, for gaze estimation task, the model tuned with eye has the lowest offset. The same applies to emotion recognition. For facial landmarks, the model without emotion or gaze fine-tuning achieves the highest performance, suggesting that trade-off phenomenon exists between the influence of different utility experts.
Our observations indicate that engaging in fine-tuning alongside utility experts yields notable enhancements in preserving performance for downstream tasks, as evidenced by the findings in Tables \ref{tab:compare-deid-ablation} and \ref{tab:compare-utility-ablation}. To delve into specifics, we note that in the gaze estimation task, the model fine-tuned with the inclusion of eye-related utility experts showcases the most minimal offset, denoting superior alignment. Similarly, the same pattern emerges in the context of emotion recognition, where the model fine-tuned with emotion-centric utility experts achieves optimal results. On the other hand, concerning facial landmarks, intriguing dynamics come to light. The model deprived of fine-tuning with emotion or gaze experts demonstrates the highest performance in this regard. This contrast highlights the existence of a trade-off phenomenon, indicating that distinct utility experts exert varying degrees of influence, necessitating a balanced consideration.

\section{More Qualitative Evaluation and Comparisons}
\label{sec:qualitative}

In this section, we provide additional qualitative results highlighting the quality of privacy and utility preservation provided by the proposed method, compared to the most popular face anonymization methods, such as blurring \cite{frome2009large}, pixelation \cite{zhou2020personal}, Password \cite{gu2020password}, CIAGAN \cite{maximov2020ciagan}, DeepPrivacy \cite{hukkelaas2019deepprivacy}, and Repaint \cite{lugmayr2022repaint}. We also exhibit the ability to directly run on images in the wild without the need of complex post-precessing, in contrast to \cite{li2021differentially,barattin2023attribute}. 

\noindent\textbf{More qualitative comparisons on LFW.} Figure \ref{fig:sup_qualities} complements Figure \ref{fig:qualities} with more comparisons. Simpler methods, such as blurring and pixelation, provide effective anonymization, but the resulting facial images cannot be leveraged for downstream tasks. 
For the other deep-learning-based solutions, the observations are similar to those made \wrt Figure \ref{fig:qualities}. 
Password \cite{gu2020password} fails at removing identifying features; whereas RePaint \cite{lugmayr2022repaint} has difficulties hallucinating entire new faces, resulting in images too distorted to be useful. CIAGAN \cite{maximov2020ciagan} and DeepPrivacy \cite{hukkelaas2019deepprivacy} provide strong anonymization and pseudo-realistic results, but they still suffer from artifacts that can also impair usability (in terms of saliency and utility attributes).
Our method sometimes struggles with out-of-distribution images (\eg, facial images with lighting conditions hiding key features) causing some utility loss, but it overall yields realistic images sharing utility attributes with the original ones while successfully altering identifying traits (nose width, thickness of the eyebrows, shape of cheeks, \etc).



% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures_rebuttal/rebuttal_pipeline_cropped3.pdf}
%     % \vspace{-8pt}
%     \caption{Detailed training pipeline of \textit{Disguise}, in supplement to Figure \refwithdefault{fig:architecure}{2}. The proposed solution is end-to-end differentiable. However, in practice, to guide the optimization process, we train the network in two phases. Firstly, we train the face-swapping network (the branch marked in dark green); then in the second phase, we add the ID obfuscation branch (marked in light green) and the utility-guaranteeing module (the branch on top) to finetune the whole network.}
%     \label{fig:training_phases}
%     % \vspace{-7pt}
% \end{figure*}

\input{figures/fig-qualities-sup}

\noindent\textbf{Sensitive images taken in medical settings.} Closer to the target use-cases discussed in the Introduction, we also share additional results on sensitive images taken in medical settings (the images were taken, edited, and shared with the consent of the depicted volunteers), \cf Figure \ref{fig:sup_qualities_medical}. Once again, we note the superiority of the proposed method in terms of image quality and usability. For example, the gaze and facial expression are better preserved, and so are elements occluding the faces (oxygen mask, glasses). If the obfuscated data were to be used for training algorithms on face-focused tasks for medical environments, preserving such challenging non-facial features would be important to ensure the robustness of these methods after deployment. 

\noindent\textbf{Group photos with multiple faces.} Figure \ref{fig:sup_qualities_group} shows some results when applying the evaluated methods to group images, again highlighting the performance of our method compared to the state-of-the-art. \Eg, while DeepPrivacy is able to generate high-quality \textit{fake} faces, it does not preserve key utility attributes as well as our method (\eg, changing gaze directions or facial expressions, adding glasses, \etc). 

Note that for such group images or images showing more than just a face (\cf Figure \ref{fig:sup_qualities_group}), we first apply InsightFace \cite{noauthor_insightface_2022,deng2019arcface}, a face detection model, to obtain the region for each face present in the image; then we apply the de-identification methods to each corresponding crop separately; before merging everything back into the obfuscated image.








\input{figures/fig-qualities-medical-supp.tex}
\input{figures/fig-qualities-group.tex}