\section{Experiments}\label{sec:exp}

\input{figures/fig-hist-4dist}

We now describe our experimental setup and compare with other methods in terms of privacy robustness and data usability. More details in supplementary material.

\subsection{Experimental Protocol}\label{sec:protocol}

% \subsubsection{Implementation Details}

% \noindent\textbf{Architecture.} We build our face-swapping model $g$ based on the architecture and training framework proposed in SimSwap \cite{chen2020simswap}. The identity-merging module uses a $\MLP_{\theta_z}$ with two layers and feature sizes of [1024, 1024, 512]. The identity-transforming $\MLP_{\theta_z}$, on the other hand, is a 3-layer network with feature sizes [512, 2048, 1024, 512]. 
% The VED encoder consists of two dense layers of sizes [512, 1024, 1024], followed by two parallel layers of size [512] to predict the mean and variance in latent space. The VED decoder has three dense layers of sizes [512, 1024, 1024, 512]. 
% We use $\tanh$ as final activation throughout the model.

% \noindent\textbf{Training.} We apply the Adam optimizer \cite{kingma2014adam} with $\beta_1=0$ and $\beta_2=0.999$, learning rate $10^{-4}$, and a batch size of $4$. 
% % During the training, we alternately set one batch for image pairs with the same identity and another for image pairs with different identities. 
% We train our pipeline in two stages: (1) we first pre-train the face-swapper $g_{\text{fs}}$ according to \cite{chen2020simswap} for 1M iterations; (2) then we fine-tune it together with utility module and ID transformer for another 100k iterations. 

\noindent\textbf{Datasets.} We use multiple datasets for taining and evaluation. We train our models on VGGFace2 dataset \cite{Cao18}, which totals 3.31 million images with 9,131 identities. We use multiple datasets for evaluation, including 
% We apply standard dataset preprocessing \cite{chen2020simswap} and evaluate on 
LFW \cite{huang2008labeled} (13,233 face images and 5,749 identities) for utility and de-identification performance, CelebA-HQ \cite{karras2017progressive} (30,000 face images) for utility evaluation, and WFLW \cite{wayne2018lab} (10,000 face images) for the training usability \wrt the  downstream task of landmark detection.

%, \ie, keeping only images larger than $250 \times 250$px, then centering, cropping, and resizing them to $224 \times 224$. 


\noindent\textbf{Identity and Utility Models.} To demonstrate the genericity of our method, we consider a variety of pretrained face-identification networksand of utility networks over different recognition tasks.
As identity experts, we use ArcFace \cite{deng2019arcface}, AdaFace \cite{kim2022adaface}, FaceNet \cite{schroff2015facenet}, and SphereFace \cite{Liu_2017_CVPR}. 
Either ArcFace ($h_\mathcal{Z}^\text{arc}$), AdaFace ($h_\mathcal{Z}^\text{ada}$), or both ($h_\mathcal{Z}^\text{mix}$) are used to guide $g$ during training (\cf Equation \ref{eq:uti_loss}); FaceNet and SphereFace are used only for evaluation. 
For the downstream tasks, we use ETH-XGaze \cite{Zhang2020ETHXGaze} (noted $h_\mathcal{Y}^\text{eye}$) for gaze estimation, DAN \cite{wen2021distract} ($h_\mathcal{Y}^\text{emo}$) for facial expression recognition, or both ($h_\mathcal{Y}^\text{mix}$) to provide utility feedback during training.
During evaluation, we use L2CS-Net \cite{abdelrahman2022l2cs} for gaze estimation, DeepFace (DF) \cite{serengil2021lightface} for emotion recognition, and
RetinaFace \cite{deng2020retinaface} %(\ie, $h_\mathcal{Y}^\text{kpt-l}$) 
and Dlib \cite{dlib09} for landmark detection. 

\noindent\textbf{%Evaluation 
Metrics.} We employ the commonly-used validation rate and verification accuracy as metrics for evaluating privacy preservability \cite{schroff2015facenet,Liu_2017_CVPR,deng2019arcface,kim2022adaface}. The validation rate is defined as the true positive rate (TPR) at certain false positive rate (FPR), \eg, TPR @ FPR=1e-3. 
Verification accuracy is the percentage of image pairs correctly classified as the same/different person using the best $\ell_2$ distance threshold. The verification accuracy of random guessing is thus 50\%, which is what anonymization aims at. 
To measure utility preservation, we use $\ell_2$ pixel distance and normalized mean error (NME) for facial landmark detection, mean absolute error (MAE) for gaze estimation, and accuracy for emotion recognition. For image quality, we use SER-FIQ \cite{terhorst2020ser}.

\noindent\textbf{Comparison.} We consider various de-identification methods, including DeepPrivacy \cite{hukkelaas2019deepprivacy}, DeepPrivacy2 \cite{hukkelaas2023deepprivacy2}, CIAGAN \cite{maximov2020ciagan}, Password \cite{gu2020password}, and RePaint \cite{lugmayr2022repaint}.
For readibility of the tables, we denote different versions as ``Ours ($a$, $b$, $c$)" where $a$ fixes the identity model(s) $h_\mathcal{Z}^a$ used, $b$ the transformation function $\psi_b^\epsilon$, and $c$ the utility model(s) $h_\mathcal{Y}^c$.
For simplicity, we use ``Ours ($\text{arc}, \text{ved}, \text{eye}$)" as our default method unless otherwise mentioned. We demonstrate the impact of different transformation models and identity/utility experts in ablation studies.


% \zikui{Privacy has 3 parts, Utility has 2 parts
% \subsection{Privacy: Obfuscation Evaluation}
% \subsubsection{De-ID rate}
% \subsubsection{Anti-invertibility}
% \subsubsection{Non re-identification}
% \subsection{Utility: Usability Evaluation}
% \subsubsection{Offset in utilities}
% \subsubsection{Anonymized dataset for training}
% }


% \subsection{Obfuscation and Usability Evaluation}

\subsection{Privacy: Obfuscation Evaluation}
\label{sec:exp-privacy}
% Our privacy evaluation comes in three parts. First, xxx

\noindent\textbf{De-identification performance.}
As shown in Table \ref{tab:compare-deid}, we achieve near perfect de-id rate, \ie, with a validation rate close to $0$ and verification accuracy close to $50\%$, outperforming other methods by a significant margin, and is even more secure than randomly picking replacement images from the dataset. Figure \ref{fig:hist-4dist}(b) presents the $\ell_2$ distance histogram for original positive pairs, original negative pairs, and original-anonymized positive pairs on LFW \cite{huang2008labeled}, and Figure \ref{fig:hist-4dist}(c) shows the ROC curves of validation rate. We observe that \textit{Disguise} creates image pairs that are close to the negative distribution, hence perfect obfuscation. We also achieve the highest facial image quality, see Figures \ref{fig:teaser} and \ref{fig:qualities} for visual reference. Among other comparing methods, it is worth noticing that Password fails to de-identify images, hence the highest validation rate. CIAGAN and RePaint are better than Password in de-identification, however they suffer from low facial image quality due to high artifacts and distortions. 

% outperform other de-identification methods in both data anonymization and image quality by a large margin. 
% and \ref{tab:compare-utility}, we outperform other de-identification methods in both data anonymization, usability, and image quality by a large margin. 
% Our family of solutions achieves the highest facial image quality and near perfect de-id rate, \ie, with a validation rate close to $0$ and verification accuracy close to $50\%$ (except when leveraging AdaFace, see supplementary material), which is even more secure than randomly picking replacement images from the dataset. Password fails to de-identify images, hence the highest validation rate. CIAGAN and RePaint are better than Password in de-identification, however they suffer from low facial image quality due to high artifacts and distortions. Qualitative results are in Figures \ref{fig:teaser} and \ref{fig:qualities}.


\input{tables/tab-compare-deid-simplified}


% \input{figures/fig-hist-roc-only}



% As shown in Fig. \ref{fig:hist}a, the distance histogram of original-anonymized pairs produced by our VED is close to that of negative pairs and the ROC curve is close to the diagonal line (i.e. close to random guess). 
% Compared to other methods, our VED is further from the positive pairs both on the histogram and ROC curve. Fig. \ref{fig:hist}b shows that when we introduce more $\beta$ noise in our MLP model, both the histogram and ROC curve move closer towards the positive pairs. When $\beta=0.5$, our MLP model can de-identify facial images on which recognition model has performance close to random guess. For our VED model, when increasing the $\alpha$ noise, the histogram stays close to the negative pairs and the ROC curve stays close to the diagonal line, as shown in Fig. \ref{fig:hist}c. These results suggest that our VED model is robust to inversion attacks. 

% To counter inversion attackers for the MLP transformation, we add noise to the input of the MLP transformation. As shown in Tab. \ref{tab:mlp-noise}, when we increase the ratio of noise, the recognition rate on swapped faces increases and the recognition rate on inverted faces decreases. We can find a trade-off between these two recognition rates for privacy-preserving.

% \input{tables/tab-vae-noise}
% \input{figures/fig-hist-vae-noise.tex}
% \input{figures/fig-roc-vae-noise.tex}

% For the VED transformation, we can scale the variance of the latent vector to control the invertibility. As shown in Tab. \ref{tab:vae-noise}, when we increase the variance, the recognition rate on inverted faces decreases, In the meantime, VED tranformation can maintain relatively smaller recognition rates for the swapped face compared with the MLP transformation. 

\noindent\textbf{Original and anonymized ID de-correlation.} 
% Mainly describe Figure. \ref{fig:hist-4dist}.
We consider scenarios where malicious attackers attempt to link anonymized IDs with their original IDs, allowing them to perform inversion inference on the anonymized IDs and recover the original ones. We use encoder-decoder networks to learn the correlation on existing original-anonymized image pairs. Figure \ref{fig:hist-4dist}(e) shows the results of using MLPs to decode obfuscated IDs from CelebA-HQ \cite{karras2017progressive} while trained on LFW \cite{huang2008labeled} using original IDs as supervision. % \zikui{maybe Zhongpai can detail the setting here.} 
% If attackers get access to obfuscation models, they may invert the swapping process and recover the original identities in altered images. 
While methods like DeepPrivacy, CIAGAN, and RePaint are inherently robust to inversion attacks since the original face region is entirely erased, and their networks are solely tasked with inpainting the blank region, our method still offers de-correlation on par with these methods, suggesting that our method is also resilient to inversion attacks.
% \zikui{Question: Fig5(e) shows distributions close to positive in (b), means that all methods are invertible? But other methods should not be invertible since the original IDs are completely missing}
% This also means that their models cannot maximize identity obfuscation (as they receive no identity information) and are thus at risk of inpainting a face too similar to the original one, \cf higher re-identification rates compared to ours in Table~\ref{tab:compare-deid}.
% For the comparing methods, such as Deep Privacy, CIAGAN and repaint, they completely remove the original face region, thus there is no notion of face id distance nor invertibility. Password can be inverted given that the password is leaked. Due to these reasons, we do not perform quantitative evaluations on the invertibility of them.


\subsection{Utility: Usability Evaluation}
% We evaluate our utility performance in two parts, xxx
\input{tables/tab-compare-utility-both-simplified}
\input{tables/tab-compare-utility-training}

\noindent\textbf{Utility corruption in anonymized images.}
Our method demonstrates superior utility preservation compared to others across datasets (Table \ref{tab:compare-utility}). We highlight our approach's excellence through qualitative comparison (Figure \ref{fig:teaser} and \ref{fig:qualities}). DeepPrivacy lacks facial attribute preservation, exhibiting bias towards smiles and youth. CIAGAN bears heavy artifacts; Password yields blurry and easily re-identifiable outcomes. RePaint excels with in-distribution faces (RePaint is trained on CelebA-HQ thus has improved performance on the same dataset in Table \ref{tab:compare-utility}), but it fails elsewhere and doesn't retain original attributes. For challenging scenarios, like heavy occlusion (\eg, masks), CIAGAN and DeepPrivacy falter, unlike our effective face-swapping model.

% Our methods demonstrate the least utility corruption in comparison with other methods across various datasets, as shown in Table \ref{tab:compare-utility}. 
% Specifically, ours ($arc,mlp,mix$) preserves utility to the largest extent overall, with lowest corruption of facial landmarks, gaze, and emotion. 
% Additionally, we showcase the superiority of our approach through qualitative comparison in Figure \ref{fig:teaser} and \ref{fig:qualities}. DeepPrivacy fails to preserve facial attributes and exhibits a strong bias towards generating smiling and youthful identities. 
% CIAGAN suffers from heavy artifacts, whereas Password's results are blurry and easily re-identifiable. RePaint can produce high-quality results when faces are in-distribution (note that RePaint is trained on CelebA-HQ thus has improved performance on the same dataset in Table \ref{tab:compare-utility}) but fails otherwise. It also does not retain the original facial attributes. 
% For challenging scenarios with heavy occlusion (\eg, wearing face masks), both CIAGAN and DeepPrivacy fail to preserve the occluding entity, unlike our face-swapping-based model.
% Finally, it is worth noting that methods like RePaint require user input to define the editing area, whereas our method automatically preserves attributes other than identifiable ones.


\noindent\textbf{Usability of anonymized images as training data.%set for downstream tasks.
}
% \subsection{Application to the Training of Utility Models}
\label{sec:util_training}
% So far, we have demonstrated the non-corruption of utility attributes by comparing how pretrained task-specific models perform on the obfuscated data versus the original one.
% We now take a step further towards the initial motivation of anonymizing data to train new solutions, and we evaluate how utility networks trained from scratch on the anonymized data perform afterwards on real, unseen samples. Ideally, these models trained on datasets pre-processed by privacy-preserving methods should perform as well as those trained on the original, non-obfuscated dataset.
% % We take facial landmark detection as exemplary task and consider the WFLW dataset \cite{wayne2018lab} that contains 10,000 faces with 98 manually-annotated landmarks for each. We split the data into training/testing sets (7,500/2,500), and we generate obfuscated versions of the training data using the aforementioned methods (we keep the test data unaltered). 
% We take facial landmark detection as exemplary task and consider the WFLW dataset \cite{wayne2018lab} that provides 98 manually-annotated landmarks for each image. We split the data into training/testing sets (7,500/2,500), and we generate obfuscated versions of the training data using the aforementioned methods (we keep the test data unaltered). 
% To learn and perform the task, we choose a HRNetv2-W18 model \cite{wang2021hrnetv2}, which we train for 60 epochs using an Adam optimizer \cite{kingma2014adam} ($\beta_1=0$, $\beta_2=0.999$) with a learning rate of $10^{-4}$ and batch size of 64.
% As shown in Table \ref{tab:compare-utility-training}, models trained on the obfuscated datasets perform worse than the version trained on the original training data (\cf higher NME of facial landmarks). The model trained on data anonymized by our solution suffers the smallest accuracy drop, confirming that our model maintains higher utility for training downstream utility tasks while preserving privacy.
We have demonstrated utility attribute non-corruption by comparing performance of pretrained task-specific models on obfuscated versus original data. Now, we advance toward the initial motivation of data anonymization for new solutions, evaluating how utility networks trained from scratch on anonymized data perform on real, unseen samples. Ideally, these privacy-preprocessed models should match performance of those trained on original, non-obfuscated data. Taking facial landmark detection as an example on the WFLW dataset \cite{wayne2018lab} (98 landmarks per image), we split data into training/testing sets (7,500/2,500) and generate obfuscated training data using mentioned methods (test data remains unaltered). We use an HRNetv2-W18 model \cite{wang2021hrnetv2} for the task, trained for 60 epochs with Adam optimizer \cite{kingma2014adam} ($\beta_1=0$, $\beta_2=0.999$), learning rate $10^{-4}$, and batch size 64. Table \ref{tab:compare-utility-training} shows models on obfuscated data perform worse (higher NME of facial landmarks) than the one on original data. Our anonymized data model demonstrates the smallest accuracy drop, confirming higher utility preservation for downstream tasks while maintaining privacy.


\subsection{Ablation Study}
Here we demonstrate the impact of different transformation models, identity and utility experts.

\noindent\textbf{Impact of transformation models on re-identifiability.}
% To achieve maximum identity obfuscation, we condition our method on the original ID vectors, and learn ID transformations to minimize the cosine similarity between the original and transformed IDs. This makes re-identification possible, \eg, by trying to recover the original ID using the opposite of transformed ID, which has the lowest cosine similarity. 
As justified in Section~\ref{sec:method} and experimentally measured in Table~\ref{tab:compare-invertibility}, $\psi_{\text{opp}}$ would suffer high re-identification, \ie we can recover the original ID using the opposite of transformed ID.
% On the other hand, methods conditioned on the original identity vectors, such as ours, are thus traditionally weaker against inversion attacks; hence the effort that we put in strengthening the identity-transformation operation $\psi^\epsilon$ (\cf Subsection \ref{sec:id_trans}). 
% Table \ref{tab:compare-invertibility} summarizes the related ablation study, showing the re-identification rates of obfuscated images and their inverted versions. 
% As justified in Section~\ref{sec:metho}, we observe that $\psi_{\text{opp}}$ results in both higher re-identification and invertibility (as attackers can easily recover opposite identities). 
MLP-based transformations outperforms opposite transformation but VED-based transformations yield the best results in terms of de-identification and non-invertibility, confirming the superiority of our proposed solution.

\input{tables/tab-invertibility}

% Table \ref{tab:compare-invertibility} shows the recognition rates of swapped faces and inverted faces using different identity transformation methods. We can see that the opposite has the highest recognition rate on the inverted faces since attackers can easily choose the opposite of the swapped face identity. For the MLP transformation, the model with two identity extractors, \ie, Ours ($mix, mlp, eye$), performs better than that with one identity extractor. At last, the models with VED transformation have lower recognition rates than MLP transformation and the opposite. This is because VED is not a determined function. The latent vector of VED is sampled from a learned distribution of the latent space. 


\input{tables/tab-both-noise}

The introduction of stochastic operations in alignment with $\epsilon$-LDP further strengthen the solution. 
% As shown in \nth{1} half of Table \ref{tab:both-noise}, the higher the amount of $\beta$ noise introduced (\ie, the lower $\epsilon$), the higher the robustness to inversion, but the lower the original de-identification rate. 
% % This trade-off is also discussed in Section~\ref{sec:metho}, and it is used as justification to propose our VED.
% % Table \ref{tab:compare-invertibility} confirms the superiority of such a solution, which yields the best results in terms of de-identification and non-invertibility.
% In \nth{2} half of Table \ref{tab:both-noise}, we also observe increased robustness to attacks with higher ratio $\alpha$ (\ie, lower $\epsilon$); though like $\beta$ for MLP-based versions, this slightly impacts re-identification (the noisier the data, the harder it is to synthesize an ID that maximizes obfuscation). 
As shown in Table \ref{tab:both-noise}, the higher the amount of $\beta$ or $\alpha$ noise introduced (\ie, the lower $\epsilon$), the more robust to attacks the method becomes, but the lower the original de-identification rate (the noisier the data, the harder it is to synthesize an ID that maximizes obfuscation). This negative impact is however better mitigated by the proposed VED.
We provide further insights in supplementary material.

% Figure \ref{fig:hist-4dist}(b) presents the $\ell_2$ distance histogram for original positive pairs, original negative pairs, and original-anonymized positive pairs on LFW \cite{huang2008labeled}, and Figure \ref{fig:hist-4dist}(c) shows the ROC curves of validation rate. We observe that \textit{Disguise} creates image pairs that are close to the negative distribution, hence obfuscation. 
% However, in Figure \ref{fig:hist} we note that our noise-free MLP solutions have their ROC curves much below the diagonal (symbolizing random guess), implying inverse correlations. This issue disappears as noise is introduced, with an optimum for $\beta=\frac{1}{2}$. 
% On the other end, our VED-based solutions almost perfectly map pairs to the negative distribution and have an ROC curve extremely close to the diagonal, regardless of $\alpha$ amplitude, suggesting enormous difficulties for re-identification.


\noindent\textbf{Efeects of using multiple ID extractors.}
As shown in Table~\ref{tab:compare-invertibility}, MLP-based transformations relying on multiple identity extractors, \ie, ``Ours ($\text{mix}, \text{mlp}, \text{eye}$)", perform better than versions with only one ID expert. We attribute the increased robustness to the combined knowledge of the two algorithms which capture more varied ID-related features that are then obfuscated. 