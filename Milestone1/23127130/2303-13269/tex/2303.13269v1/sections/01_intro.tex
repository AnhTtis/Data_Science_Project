\section{Introduction}
\label{sec:intro}

\input{figures/fig-teaser.tex}

% \bpnote{Brainstorming possible names for our method: 
% (1) FauxNet (Face Anonymization with Utility eXpertise) ;
% (2) PurgeNet (Privacy- and Utility Retainment Guided by Experts) ;
% (3) SimuNet (Swap Identity, Maintain Utility) ;
% (4) Disguis: Disentangeled Generative Utility-preserving Identity Swapper;
% (5) UP-Face: Utility-Preserving Face De-identification;
% (6) Disguise: Deep Identity Swapper Guaranteeing Utility with Implicit Supervision from Experts
%   Disguise: Deep Identity Swapper with Guided 
% SA: I like this last one (7)
%
% }

Privacy laws protect personal data in most regions of the globe, such as GDPR \cite{voigt2017eu} in Europe, HIPPA \cite{HIPAA} and CCPA \cite{CCPA} in the US, and PIPL \cite{PIPL} in China. 
These regulations are especially strict for medical information (\eg, medical records, scans), and information collected in medical settings (\eg, smart cameras installed in operating rooms to assist surgeons), imposing strong yet necessary constraints, \eg, on the storage and distribution of patient images to ensure medical confidentiality.
However, such data %(\eg, images of patients undergoing medical procedures) 
could provide precious insight to medical providers if leveraged properly. In this age of data-driven machine learning, such untapped datasets could be the key to unlocking new applications, such as the automation of sensitive medical procedures and deployment of novel AI-driven diagnosis tools.

Eager to tap into such \textit{valuable} datasets, data scientists have explored various techniques to enable the usage of sensitive images without compromising the identity (ID) of depicted individuals. 
Since humans (and thus human-crafted biometric methods) have a strong evolutionary bias towards facial features for peer identification \cite{tsao2008mechanisms,burke2013evolution}, most de-identification methods for images focus on face obfuscation \cite{chen2020simswap,agarwal2021privacy}, such as occlusion \cite{newton2005preserving}, blurring \cite{frome2009large}, pixelation \cite{zhou2020personal}, or warping \cite{korshunov2013using} of depicted faces. While these solutions may provide some anonymity, they have a substantial impact on image saliency. Recently, face-swapping methods \cite{hukkelaas2019deepprivacy,maximov2020ciagan,chen2020simswap,gu2020password,cao2021personalized,proencca2021uu,agarwal2021privacy} have gained relative momentum as a potential solution to this disadvantage.

Popularized through the notion of \textit{deepfakes} \cite{westerlund2019emergence}, these deep-learning models are trained to replace any face in an image or video by another one (user-provided or AI-generated), while trying to preserve the overall saliency or specific facial attributes, such as perceived gender, expression, or hair color. While recent solutions can generate convincing results, they are not suitable for the targeted use cases as they lack formal \textit{privacy} and 
% \zikui{definition of privacy? Is it de-id rate? Or non-invertibility? } 
\textit{utility} guarantees for the resulting images. Face-swapping methods evade the confidentiality of the ID provider since the swapped face leaks the source ID. In addition, they lack proper mechanisms to maximize de-identification and minimize identity leakage of the target ID. 
% , and do not consider the non-invertibility of their operation (\ie, an attacker with access to their parameters could invert the swapping and recover the original identity), thus jeopardizing privacy and confidentiality. 
Furthermore, 
% while these methods try to preserve some image-level or predefined attributes, 
they do not emphasize on maintaining the \textit{utility} of resulting images, \ie, they do not guarantee that the altered images can have the same function as the original ones for various downstream tasks. For example, a dataset would become \textit{useless} for analysis if relevant non-biometric features are corrupted (\eg, facial expressions have changed for sentiment analysis tasks) or for training recognition models if the altered images no longer match their annotations (\eg, facial landmarks, gaze directions, head-pose orientations, \etc). 

In this work, we aim to address the challenge of anonymizing images of individuals while ensuring privacy and maintaining high data utility. 
% \ie, without corrupting other visual features that could be critical for downstream tasks.
To this end, we propose \textit{Disguise} (Deep Identity Swapper Guaranteeing Utility with Implicit Supervision from Experts), a de-identification method built upon face-swapping technology that offers formal guarantees regarding identity obfuscation and utility retention. Our main contributions are as follows:

\squishlist
% \setlist{nolistsep}
% \begin{itemize}[noitemsep, wide=5pt]
    \item We propose a novel method for face de-identification which generates high-quality, photo-realistic faces with distinct identities to replace the original ones, while maintaining non-biometric attributes 
    % relevant to other tasks 
    unchanged.
    \item Unlike existing methods that pre-discard original face IDs, we condition the synthetic faces on the original ID vectors and maximize the distance to the original identities while ensuring differential privacy \cite{dwork2014algorithmic,abadi2016deep}, with randomization to prevent re-identification.
    % robustness to inversion attacks.
    % \zikui{do we tone down the anti-inversion here?}.
    \item We extensively evaluate the image quality, de-identification rate, and utilities of the resulting images over a variety of metrics, demonstrating superior results than state-of-the-art in multiple aspects.
% \end{itemize}
\squishend