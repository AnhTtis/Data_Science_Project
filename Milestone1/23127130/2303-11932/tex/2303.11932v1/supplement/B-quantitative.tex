
\section{Additional Quantitative Results (\vocs and \cocos)}
\label{supp:sec:quanti}


In this section, we provide additional quantitative results from our experiments on the \vocs and \cocos datasets. Specifically, in \cref{supp:sec:quantitative:classvsloc}, we show additional results comparing classification and localization performance. In \cref{supp:sec:quantitative:gradcam} we present results for guiding models via \gradcam attributions. In \cref{supp:sec:quantitative:intermediate}, we show that training at intermediate layers can be a cost-effective way approach to performing model guidance.
% , leading to gains in \epg even at the input layer (\cref{supp:sec:quantitative:tdes}).
In \cref{supp:sec:quantitative:segmentepg}, we evaluate how well the attributions localize to on-object features (as opposed to background features) within the bounding boxes, and find that the \energyloss outperforms other localization losses in this regard. In \cref{supp:sec:quantitative:limited}, we provide additional analyses regarding training with a limited number of annotated images. Finally, in \cref{supp:sec:quantitative:dilation}, we provide additional analyses regarding the usage of coarse, dilated bounding boxes during training.

\subsection{Comparing Classification and Localization Performance}
\label{supp:sec:quantitative:classvsloc}
In this section, we discuss additional quantitative findings with respect to localization and classification performance metrics (\iou, \map) for a selected subset of the experiments; for a full comparison of all layers and metrics, please see \cref{fig:supp:voc:f1_results,,fig:supp:coco:f1_results,,fig:supp:voc:map_results:2,,fig:supp:coco:map_results}. 

\myparagraph{Additional \iou results.} In \cref{fig:sub:iou:voc,fig:sub:iou:coco}, we show the remaining results comparing \iou vs.~F1 scores that were not shown in the main paper for \vocs and \cocos respectively.
Similar to the results in the main paper for the \epg metric (\cref{fig:epg_results}), we find that the results between datasets are highly consistent for the \iou metric.

In particular, as discussed in \cref{sec:results:epg+iou}, we find that the \lone loss yields the largest improvements in \iou when optimized at the final layer, see bottom rows of \cref{fig:sub:iou:voc,,fig:sub:iou:coco}. At the input layer, we find that \vanilla and \xdnn \resnet models are not improving their \iou scores noticeably, whereas the \bcos models show significant improvements. We attribute this to the noisy patterns in the attribution maps of \vanilla and \xdnn models, which might be difficult to optimize.

\input{supplement/tex_figures/sub_iou_voc}
\input{supplement/tex_figures/sub_iou_coco}

\myparagraph{Using \map to evaluate classification performance.} In all results so far, we plotted the localization metrics (\epg, \iou) versus the \fone score as a measure of classification performance. In order to highlight that the observed trends are independent of this particular choice of metric, in \cref{fig:supp:voc:map_results:1}, we show both \epg as well as \iou results plotted against the \map score. 

In general, we find the results obtained for the \map metric to be highly consistent with the previously shown results for the \fone metric. \Eg, across all configurations, we find the \epgloss to yield the highest gains in \epg score, whereas the \lone loss provides the best trade-offs with respect to the \iou metric. In order to easily compare between all results for all datasets and metrics, please see \cref{fig:supp:voc:f1_results,,fig:supp:coco:f1_results,,fig:supp:voc:map_results:2,,fig:supp:coco:map_results}.

\input{supplement/tex_figures/sub_voc_map}



\subsection{Model Guidance via \gradcam}
\label{supp:sec:quantitative:gradcam}
\input{supplement/tex_figures/sub_gradcam_voc}

In \cref{fig:supp:gradcam_epg_voc}, we show the \epg vs.~\fone results of training models with \gradcam applied at the final layer on the \vocs dataset; for \iou results and results on \cocos, please see \cref{fig:supp:gradcam:voc,fig:supp:gradcam:coco}. When comparing between rows (\textbf{top:} main paper results; \textbf{bottom:} \gradcam), it becomes clear that \gradcam performs very similarly to \ixg\ / \intgrad\  / \bcos attributions on \vanilla\ / \xdnn\ / \bcos models. In fact, note that \gradcam is very similar to \ixg and \intgrad (equivalent up to an additional zero-clamping) for the respective models and any differences in the results can be attributed to the non-deterministic training pipeline and the similarity between the results should thus be expected. 


\subsection{Model Guidance at Intermediate Layers}
\label{supp:sec:quantitative:intermediate}


In \cref{sec:results}, we show results for guidance on two `model depths', \ie at the input and the final layer. This corresponds to the two depths at which attributions are typically computed, \eg \ixg and \intgrad are typically computed at the input, while \gradcam is typically computed using final spatial layer activations. Following \citeApp{rao2022towards}, for a fair comparison we optimize using each attribution methods at identical depths. For the final and intermediate layers in the network, this is done by treating the output activations at that layer as effective inputs over which attributions are to be computed. As done with \gradcam \citeApp{selvaraju2017grad}, we then upscale the attribution maps to image dimensions using bilinear interpolation and then use them for model guidance.

In \cref{fig:sub:intermediate:epg}, we show results for performing model guidance at additional intermediate layers: Mid1, Mid2, and Mid3. Specifically, for the \resnet models we use, these layers correspond to the outputs of \verb|conv2_x|, \verb|conv3_x|, and \verb|conv4_x| respectively in the ResNet nomenclature (\citeApp{he2016deep}), while the final layer corresponds to the output of \verb|conv5_x|. We find that the \epg performance at these intermediate layers through the network follows the trends when moving from the input to the final layer. Similar results for \iou can be found in \cref{fig:intermediate:iou}.

\input{supplement/tex_figures/sub_intermediate_epg}

\subsection{Evaluating On-Object Localization}
\label{supp:sec:quantitative:segmentepg}


\input{supplement/tex_figures/segmentation_epg}

The standard \epg metric (\cref{eq:epg}) evaluates the extent to which attributions localize to the bounding boxes. However, since such boxes often include background regions, the \epg score does not distinguish between attributions that focus on the object and attributions that focus on such background regions within the bounding boxes. 

To additionally evaluate for on-object localization, we use a variant of \epg that we call On-object \epg. In contrast to standard \epg, we compute the fraction of positive attributions in pixels contained within the segmentation mask of the object out of positive attributions within the bounding box. This measures how well attributions \textit{within the bounding boxes} localize to the object, and is not influenced by attributions outside the bounding boxes. A visual comparison of the two metrics is shown in \cref{fig:seg_epg}.

We find that the \energyloss localization loss outperforms the \loneloss localization loss both qualitatively (\cref{fig:seg_epg:schema}) and quantitatively (\cref{fig:seg_epg:epg_vs_f1}) on this metric. This is explained by the fact that the \loneloss promotes uniformity in attributions across the bounding box, giving equal importance to on-object and background features within the box. In contrast, the \energyloss loss only optimizes for attributions to lie within the box, without any constraint on \textit{where} in the box they lie. This also corroborates our previous qualitative observations (\eg \cref{fig:loss_comp}).


\subsection{Model Guidance with Limited Annotations}
\label{supp:sec:quantitative:limited}
\input{supplement/tex_figures/limited_annotations_sub}

In \cref{fig:supp:limited:sub}, we show the impact of using limited annotations when training (\cref{sec:results:ablations}) when optimizing with the \energyloss and \loneloss localization losses for \bcos attributions at the input. We find that in addition to \epg, trends in \iou scores also remain consistent even when using bounding boxes for just 1\% of the the training images.

    
\subsection{Model Guidance with Noisy Annotations} 
\label{supp:sec:quantitative:dilation}
\input{supplement/tex_figures/coarse_bboxes}


In \cref{fig:supp:dilation_quanti}, we additionally show the impact of training with coarse, dilated bounding boxes for \ixg attributions on the \vanilla model, and \intgrad attributions on the \xdnn model. Similar to the results seen with \bcos attributions (\cref{fig:coarse_annotations}), we find that the \energyloss localization loss is robust to coarse annotations, while the performance with \loneloss localization loss worsens as the dilations increase.







