\vspace{-.75em}
\begin{figure}[!ht]
    \centering 
    \begin{subfigure}[c]{.9\columnwidth}
    \includegraphics[width=\textwidth]{results/VOC/figures/qualitative/teaserv2_0.pdf}
    \end{subfigure}
    \vspace{.15cm}

    \begin{subfigure}[c]{\columnwidth}
    \includegraphics[width=\textwidth]{results/figures/waterbird_teaser.pdf}
    \end{subfigure}
    \caption{\textbf{(a) Model guidance increases object focus.}
    Models may rely on irrelevant background features or spurious correlations (\eg presence of person provides positive evidence for bicycle, center row, col. 1). Guiding the model via bounding box annotations can mitigate this and consistently increases the focus on object features (bottom row).
    \textbf{(b) Model guidance can improve accuracy.} In the presence of spurious correlations in the training data, non-guided models might focus on the wrong features. In the example image in (b), the waterbird is incorrectly classified to be a landbird due to the background (col.~3). Guiding the model via bounding box annotation (as shown in col.~2), the model can be guided to focus on the bird features for classification (col.~4).
    }
    \label{fig:teaser}
\end{figure}
\section{Introduction}
\label{sec:introduction}


\input{results/tex_figures/double_col_qualitative}
 
Deep neural networks (DNNs) excel at learning predictive features that allow them to correctly classify a set of training images with ease. The features learnt on the training set, however, do not necessarily transfer to unseen images: \eg, instead of learning the actual class-relevant features, DNNs might memorize individual images {(\cf.~\citeMain{feldman2020neural})}
or exploit spurious correlations in the training data (cf.~\citeMain{xiao2021noise}).
For example, if bikes are highly correlated with people in the training data, a model might learn to associate the presence of a person in an image as positive evidence for a bike {(\eg \cref{fig:teaser}{\color{red}a}, col.\ 1, rows 1-2)}, which can limit how well it generalizes. Similarly, a bird classifier might rely on background features from the bird's habitat, and fail to correctly classify in a different habitat (cf.~\cref{fig:teaser}{\color{red}b} cols.\ 1-3 and \citeMain{petryk2022guiding}).

{To detect and thus be able to correct such behaviour, recent advances in model interpretability have provided attribution methods (\eg \citeMain{selvaraju2017grad,sundararajan2017axiomatic,shrikumar2017learning,bohle2022b}) as a means to understand a model's reasoning. These methods typically provide attention maps that highlight regions of importance in an input to explain the model's decisions. Consequently, they can be used to identify incorrect reasoning such as reliance on spurious or irrelevant features such as in \cref{fig:teaser}{\color{red}b}.}

As many attribution methods are in fact themselves differentiable (\eg \citeMain{shrikumar2017learning,sundararajan2017axiomatic,selvaraju2017grad,bohle2022b}), recent work \citeMain{ross2017right,shen2021human,gao2022aligning,gao2022res,teso2019explanatory,teso2019toward} has explored the idea of using them to guide the models to
make them ``right for the right reasons'' \citeMain{ross2017right}. Specifically, models can be guided by jointly optimizing for correct classification as well as for localization of attributions to human annotated masks. This can help the model focus on the relevant features of a class, and correct errors in reasoning (\cref{fig:teaser}{\color{red}b}, col.\ 4). Such guidance has the added benefit of providing well-localized masks that are easier to understand for end users (\eg \cref{fig:teaser2}).
 
While model guidance has shown promising results, it has thus far been limited to simple and/or synthetic datasets, usually for binary classification tasks. Further, their evaluation is typically limited to specific attribution methods and localization optimization strategies, {and they greatly vary in the types of datasets and human annotations used, which makes it difficult to compare them and understand if they translate to more challenging, real-world settings.}

In this work, we perform an in-depth evaluation of model guidance on real-world, large scale datasets, and compare the effectiveness of a variety of attribution methods and localization losses. To reduce annotation cost, we only use coarse bounding box annotation masks over objects of each class in an image,
and show that while existing approaches can localize to the masks, they often force the model to attribute background regions within the mask and worsen attribution granularity (cf.~\cref{fig:loss_comp}). To mitigate this, we propose a novel \energyloss localization loss, and show that it can be used with coarse annotations to guide models to focus only on the object itself (\cref{fig:teaser}{\color{red}a}, row 3).

\myparagraph{Contributions.} \textbf{(1)} We perform the first evaluation of model guidance on challenging large scale multi-label classification datasets (\voc \citeMain{everingham2009pascal}, \coco \citeMain{lin2014microsoft}), and compare across attribution methods, layers, and localization loss functions. Further, we show that, despite being relatively coarse,  using bounding box supervision can provide sufficient guidance to the models while taking advantage of pre-existing annotations.
\textbf{(2)} We propose a novel \energyloss loss for model guidance, and show its efficacy at localizing attributions to object features while avoiding irrelevant background features within the annotation masks. \textbf{(3)} We show that model guidance can be performed cost-effectively by using annotation masks that are noisy or are available for only a small fraction (\eg $1\%$) of the training data. \textbf{(4)} We show through experiments on the \waterbirds dataset \citeMain{sagawa2019distributionally,petryk2022guiding} that model guidance with a small number of annotations can help improve the model's generalization under distribution shifts in the test data.

