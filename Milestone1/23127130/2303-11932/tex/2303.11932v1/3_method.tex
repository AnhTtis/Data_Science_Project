\section{Guiding Models Using Attributions}
\label{sec:method}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{results/ModelGuidingPipeline.png}
    \caption{\textbf{Model guidance overview.} 
    We jointly optimize for classification ($\mathcal L_\text{class}$) and localization of attributions to human-annotated bounding boxes ($\mathcal L_\text{loc}$), to guide the model to focus on object features. Various localization loss functions can be used, see \cref{sec:method:losses}.
    }
    \label{fig:pipeline}
\end{figure}

In this section, we provide an overview of the model guidance approach that jointly optimizes for classification and localization (\cref{sec:method:procedure}). Specifically, we describe the attribution methods (\cref{sec:method:attributions}), metrics (\cref{sec:method:metrics}), and localization loss functions (\cref{sec:method:losses}) that we evaluate in \cref{sec:results}. In \cref{sec:method:energyloss} we then introduce  our proposed \energyloss loss, and in \cref{sec:method:efficient} we discuss our strategy to train for localization in the presence of multiple ground truth classes.

\myparagraph{Notation:} We consider a multi-label classification problem with $K$ classes with $X\myin\mathbb{R}^{C\times H\times W}$ the input image and $y\myin\{0,1\}^K$ the one-hot encoding of the image labels. With $A_k\myin\mathbb{R}^{H\times W}$ we denote an attribution map for a class $k$ for $X$ using a classifier $f$; $A_{k}^+$ denotes the positive component of the attributions, and $\hat{A}_k=\frac{A_k}{\max(A_k)}$ normalized attributions. Finally, $M_k \myin \{0,1\}^{H\times W}$ denotes the binary mask for class $k$, which is the given by the union of bounding boxes of all occurrences of class $k$ in $X$.

\subsection{Model Guidance Procedure}\label{sec:method:procedure}

Following prior work (\eg \citeMain{ross2017right,shen2021human,gao2022aligning,gao2022res}), the model is trained jointly for classification and localization (cf.~\cref{fig:pipeline}):
\begin{equation}\label{eq:overall}
\textstyle
    \mathcal{L}=\mathcal{L}_\text{class} + \lambda_\text{loc}\mathcal{L}_\text{loc}\;.
\end{equation}
\Ie, the loss consists of a classification loss ($\mathcal{L}_\text{class}$), for which we use binary cross-entropy, and a localization loss ($\mathcal{L}_\text{loc}$), which we discuss in \cref{sec:method:losses}; here, the hyperparameter $\lambda_\text{loc}$ controls the weight given to each of the objectives.

\subsection{Attribution Methods}
\label{sec:method:attributions}

In contrast to prior work that typically use \gradcam \citeMain{selvaraju2017grad} attributions, we perform an evaluation over a selection of popularly used differentiable\footnote{Differentiability is necessary for optimizing attributions via gradient descent, so non-differentiable methods (\eg \citeMain{ribeiro2016should,petsiuk2018rise}) are not considered.} attribution methods which have been shown to localize well \citeMain{rao2022towards}: \ixg \citeMain{shrikumar2017learning}, \intgrad \citeMain{sundararajan2017axiomatic}, and \gradcam \citeMain{selvaraju2017grad}. We further evaluate model-inherent explanations of the recently proposed \bcos models \citeMain{bohle2022b}. To ensure comparability across attribution methods \citeMain{rao2022towards}, we evaluate all attribution methods at the input, various intermediate, and the final spatial layer. 

\myparagraph{\gradcam \citeMain{selvaraju2017grad}} is a popular method that weighs feature maps, typically at the final layer (\ie last spatial layer), with pooled gradients with respect to the maps. 

\myparagraph{\ixg \citeMain{shrikumar2017learning}} computes the element-wise product of the input and gradients with respect to the input, and serve as a faithful approximation of the contribution of a local neighbourhood in the input to the output.

\myparagraph{\intgrad \citeMain{sundararajan2017axiomatic}} takes an axiomatic approach and is formulated as the integral of gradients over a straight line path to the input from a baseline. Approximating this integral requires several gradient computations, making it computationally expensive for use in model guidance. To alleviate this, when optimizing with \intgrad, we use the recently proposed \xdnn models \citeMain{hesse2021fast} that efficiently compute \intgrad attributions by removing bias terms in the network.

\myparagraph{\bcos \citeMain{bohle2022b}} attributions are generated using the inherently-interpretable \bcos networks, which promote alignment between the input $\mathbf x$ and a dynamic weight matrix $\mathbf W(\mathbf x)$ during optimization. 
In our experiments, 
we use the contribution maps given by the element-wise product of the dynamic weights with the input ($\mathbf W^T_c(\mathbf x)\odot \mathbf x$), which faithfully represent the contribution of each pixel to class $c$. We also provide a differentiable implementation of \bcos explanations to use them for model guidance (supplement).

\subsection{Evaluation Metrics}
\label{sec:method:metrics}

We evaluate the models' performance on both our training objectives: classification and localization. For classification, we use the F1 
score and mean average precision (\map). We discuss the localization metrics below.

\myparagraph{Intersection over Union (IoU)} is a commonly used metric (cf.~\citeMain{gao2022res}) that thresholds the attributions to predict a binary mask and computes the ratio of the area of intersection with that of union of this mask with the bounding boxes.
The optimal threshold is typically selected \citeMain{fong2017interpretable} via a heldout set. 

\myparagraph{Energy-based Pointing Game (EPG) \citeMain{wang2020score}} measures the concentration of attribution energy within the mask, \ie the fraction of positive attributions inside the bounding boxes:
\begin{equation}\label{eq:epg}
    \text{EPG}_k = \frac{\sum_{h=1}^H\sum_{w=1}^W M_{k,hw} A^+_{k,hw}}{\sum_{h=1}^H\sum_{w=1}^W A^+_{k,hw}}\;.
\end{equation}
In contrast to \iou, this is sensitive to the relative magnitude of attributions within an attribution map, and as a result more faithfully takes into account the importance given to each input region. Like \iou, the scores lie in $[0,1]$, with higher scores indicating better localization. 

\subsection{Localization Losses}\label{sec:method:losses}
We evaluate the most commonly used localization losses ($\mathcal{L}_\text{loc}$ in \cref{eq:overall}) from prior work. We describe these losses as applied on attribution maps of an image for a single class $k$, and propose our novel \energyloss loss (\cref{sec:method:energyloss}).

\myparagraph{\loneloss loss (\citeMain{gao2022aligning,gao2022res}, \cref{eq:lone})}
minimizes the $L_1$ distance between the annotation mask and the normalized positive attributions $A_{k}^+$, and guides the model to attribute uniformly to the existing highest attribution value within the mask, while suppressing attributions outside the mask. 
\begin{equation}\label{eq:lone}
    \textstyle
    \mathcal{L}_{\text{loc},k} = \frac{1}{H\times W}\sum_{h=1}^H\sum_{w=1}^W\lVert M_{k,hw} - \hat{A}_{k,hw}^+ \rVert_1
\end{equation}

\myparagraph{Per-pixel cross entropy (\ppceloss) loss (\citeMain{shen2021human}, \cref{eq:ppce})}
applies a binary cross entropy loss between the mask and the normalized positive annotations within the mask, and guides the model to maximize the attributions inside the mask:
\begin{equation}\label{eq:ppce}
\textstyle
    \mathcal{L}_{\text{loc},k} = -\frac{1}{\lVert M_k \rVert_1}\sum_{h=1}^H\sum_{w=1}^W M_{k,hw}\log(\hat{A}_{k,hw}^+) \;.
\end{equation}
However, since it does not apply any constraint on attributions outside the mask, there is no explicit pressure to the model to avoid spurious features.
    
\myparagraph{\rrrloss loss (\citeMain{ross2017right}, \cref{eq:rrr}).}
In \citeMain{ross2017right}, the authors introduced the RRR loss applied to the models' gradients $\hat{A}_{k,hw}$ as 
\begin{equation}\label{eq:rrr}
    \textstyle \mathcal{L}_{\text{loc},k} = \sum_{h=1}^H\sum_{w=1}^W (1-M_{k,hw}) \hat{A}_{k,hw}^2 \;.
\end{equation}
To extend it to our setting, we generalize this loss and 
take $\hat{A}_{k,hw}$ to be given by an arbitrary attribution method (\eg \intgrad); 
we denote this generalized version by \rrrloss. 

\subsubsection{\energyloss Loss}\label{sec:method:energyloss}
As discussed in \cref{sec:method:losses}, existing localization losses do not constrain attributions across the entire input (\rrrloss, \ppceloss), or force the model to attribute uniformly within the mask even if it includes irrelevant background regions (\loneloss, \ppceloss). To mitigate this, inspired by the \epg metric (\citeMain{wang2020score}, \cref{eq:epg}), we propose a novel \energyloss localization loss to directly optimize for maximizing this metric (\cref{eq:energyloss}):
\begin{equation}\label{eq:energyloss}
\textstyle
    \mathcal{L}_{\text{loc},k} = -\text{EPG}_k
\end{equation}
In contrast to \loneloss and \ppceloss, the \energyloss loss does not guide attributions to be uniform within the mask. Given that we use coarse bounding box annotations, this allows the model to focus on object features while ignoring the background regions within the bounding boxes.
As such, this loss provides effective guidance while allowing the model to learn what to focus on within the bounding boxes, see \cref{sec:results}.

\subsection{Efficient Optimization}\label{sec:method:efficient}
In contrast to prior work \citeMain{ross2017right,shen2021human,gao2022aligning,gao2022res}, we perform model guidance on a multi-label classification setting, and consequently there are multiple ground truth classes whose attribution localization could be optimized. Computing and optimizing for several attributions within an image would add a significant overhead to the computational cost of training (multiple backward passes).
Hence, for efficiency, we sample one ground truth class $k$ per image at random for every batch and only optimize for localization of that class, \ie, $\mathcal{L}_\text{loc}\myeq\mathcal{L}_{\text{loc},k}$. We find that this still provides effective model guidance while keeping the training cost tractable.