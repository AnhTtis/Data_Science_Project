\section{Related Work}
\label{sec:related}

\myparagraph{Attribution Methods} \citeMain{simonyan2013deep,springenberg2014striving,sundararajan2017axiomatic,shrikumar2017learning,selvaraju2017grad,wang2020score,ramaswamy2020ablation,jiang2021layercam,chattopadhyay2018grad,petsiuk2018rise,fong2017interpretable,zeiler2013visualizing,ribeiro2016should,dabkowski2017real,bach2015pixel} are often used to explain black-box models by generating heatmaps that highlight input regions important to the model's decision. However, such methods are often not faithful to the model \citeMain{adebayo2018sanity,rao2022towards,kim2021sanity,zhou2022feature,adebayo2022post} and risk misleading users. Recent work proposes inherently interpretable models \citeMain{boehle2021convolutional,bohle2022b} that address this by providing model-faithful explanations by design. In our work, we use both popular post-hoc and model-inherent attribution methods to guide models and discuss their effectiveness.

\myparagraph{Attribution Priors:} Several approaches have been proposed for training better models by enforcing desirable properties on their attributions. These include enforcing consistency against augmentations \citeMain{pillai2021explainable,pillai2022consistent,guo2019visual}, smoothness \citeMain{erion2021improving,moshe2022improving,kiritoshi2019l1}, separation of classes \citeMain{zhang2022correct,pillai2022consistent,sun2020fixing,nakka2020towards,singh2020don}, or constraining the model's attention \citeMain{fukui2019attention,asgari2022masktune}.
In contrast, in this work, we focus on providing explicit human guidance to the model using bounding box annotations. {This constitutes more explicit guidance but allows fine-grained control over the model's reasoning even with few annotations.}

\myparagraph{Model Guidance:} In contrast to the indirect regularization effect achieved by attribution priors, approaches have also been proposed (cf.~\citeMain{friedrich2022typology,teso2022leveraging}) to actively guide models using attributions as a tool, for a variety of tasks such as classification \citeMain{ross2017right,gao2022aligning,gao2022res,rieger2020interpretations,petryk2022guiding,hagos2022identifying,teney2020learning,mitsuhara2019embedding,teso2019explanatory,teso2019toward,schramowski2020making,shao2021right,linsley2018learning,shen2021human,yang2022improving}, segmentation \citeMain{li2018tell}, VQA \citeMain{selvaraju2019taking,teney2020learning}, and knowledge distillation \citeMain{fernandes2022learning}. The goal of such approaches is not only to improve performance, but also make sure that the model is ``right for the right reasons'' \citeMain{ross2017right}. For classifiers, this typically involves jointly optimizing both for classification performance and localization to object features. However, most prior work evaluate on simple datasets \citeMain{ross2017right,shao2021right,gao2022aligning,gao2022res} or require modifying the network architecture \citeMain{mitsuhara2019embedding,linsley2018learning}. In this work, we conduct an in-depth evaluation and propose a novel loss function for guiding DNNs, and show its utility on much more challenging real-world multi-label classification datasets (\voc, \coco). We also provide a comprehensive comparison against the localization losses introduced and used in the closest related work, \ie \rrr \citeMain{ross2017right}, \haics \citeMain{shen2021human}, and \gradia \citeMain{gao2022aligning}, and discuss their utility. {Model guidance has also been used to mitigate reliance on spurious features using language guidance \citeMain{petryk2022guiding}, and we show that using a small number of coarse bounding box annotations can be similarly effective.}

\myparagraph{Evaluating Model Guidance:} The benefits of model guidance have typically been shown via improvements in classification performance (\eg \citeMain{ross2017right,rieger2020interpretations}) or localization to the object mask (\eg \citeMain{gao2022res,li2018tell}) by computing the \iou with the attribution maps. In addition to evaluating on these metrics as done in prior work, we also evaluate on the \energypg metric \citeMain{wang2020score}, and use it to derive our energy loss.