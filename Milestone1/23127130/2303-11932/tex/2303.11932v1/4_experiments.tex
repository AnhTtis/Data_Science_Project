\section{Experimental Setup}
\label{sec:experiments}
\input{results/tex_figures/pareto_figure}
In this section, we describe our experimental setup
and how we select the best models across metrics. {Full training details can be found in the supplement.} We evaluate across the full sweep of combinations of choices for each category, and discuss our results in \cref{sec:results}. 

\myparagraph{Datasets:} We evaluate on \voc \citeMain{everingham2009pascal} and \coco \citeMain{lin2014microsoft} for multi-label image classification. {In \cref{sec:results:waterbirds}, to understand the effectiveness of model guidance in mitigating spurious correlations, we also evaluate on the synthetically constructed Waterbirds-100 dataset \citeMain{sagawa2019distributionally,petryk2022guiding}, where landbirds are perfectly correlated with land backgrounds on the training and validation sets, but are equally likely to occur on land or water in the test set (similar for waterbirds and water). With this dataset, we evaluate model guidance for suppressing undesired features.}

\myparagraph{Attribution Methods and Architectures:} As described in \cref{sec:method:attributions}, we evaluate with \ixg \citeMain{shrikumar2017learning}, \intgrad \citeMain{sundararajan2017axiomatic}, \bcos \citeMain{bohle2022b}, and \gradcam \citeMain{selvaraju2017grad} using models with a \resnet \citeMain{he2016deep} backbone. For \intgrad, we use an \xdnn \resnet \citeMain{hesse2021fast} to reduce the computational cost, and a \bcos \resnet for the \bcos attributions. We optimize the attributions at the input and final layer\footnote{As typically used in \ixg (input) and \gradcam (final) respectively.}; for intermediate layer results, see supplement. Given the similarity of the results between \gradcam and \ixg, and since \bcos attributions performed better than \gradcam for \bcos models, we show \gradcam results in the supplement. 
All models were pretrained on \imagenet \citeMain{imagenet}, and model guidance was performed starting from a baseline model fine-tuned on the target dataset.

\myparagraph{Localization Losses:} As described in \cref{sec:method:losses}, we compare four localization losses in our evaluation: (i) \energyloss, (ii) \loneloss \citeMain{gao2022aligning,gao2022res}, (iii) \ppceloss \citeMain{shen2021human}, and (iv) \rrrloss (cf.~\cref{sec:method:losses}, \citeMain{ross2017right}).

\myparagraph{Evaluation Metrics:} As discussed in \cref{sec:method:metrics}, we evaluate both for classification and localization performance of the models. For classification, we report the F1 scores, similar results with \map scores can be found in the supplement. For localization, we evaluate using the \epg and \iou scores.

\myparagraph{Selecting the best models:} As we evaluate for two distinct objectives (classification and localization), it is non-trivial to decide which models to select during training. \Eg, a model that provides the best classification performance might provide significantly worse localization performance than a model that provides slightly lower classification performance but much better localization. Finding the right balance and deciding which of those models in fact constitutes the `better' model depends on the preference of the end user. 
Hence, instead of selecting models based on a single metric, we select the set of Pareto-dominant models \citeMain{pareto1894massimo,pareto2008maximum,backhaus1980pareto} across three metrics---F1, \epg, and \iou---for each training configuration, as defined by a combination of attribution method, layer, and loss. Specifically, as shown in \cref{fig:pareto_example}, we train for each configuration using three different choices of $\lambda_\text{loc}$, and select the set of Pareto-dominant models among all checkpoints (epochs and $\lambda_\text{loc}$). This provides a more holistic view of the general trends on the effectiveness of model guidance for each configuration.