\section{Implementation Details}
\label{supp:sec:implementation}

\subsection{Training and Evaluation Details}
\label{supp:sec:implementation:training}

\myparagraph{Implementations:} We implement our code using PyTorch\footnote{\url{https://github.com/pytorch/pytorch}} \citeApp{paszke2019pytorch}. The \voc \citeApp{everingham2009pascal} and \coco \citeApp{lin2014microsoft} datasets and the \vanilla \resnet model were obtained from the Torchvision library\footnote{\url{https://github.com/pytorch/vision}} \citeS{paszke2019pytorch_2,torchvision2016_2}. Official implementations were used for the \bcos\footnote{\label{footnote:bcoscode}\url{https://github.com/B-cos/B-cos-v2}} \citeApp{bohle2023b} and \xdnn\footnote{\url{https://github.com/visinf/fast-axiomatic-attribution}} \citeApp{hesse2021fast} networks. Some of the utilities for data loading and evaluation were derived from NN-Explainer\footnote{\url{https://github.com/stevenstalder/NN-Explainer}} \citeApp{stalder2022wyswyc}, and for visualization from the Captum library\footnote{\url{https://github.com/pytorch/captum}} \citeApp{kokhlikyan2020captum}.

\subsubsection{Experiments with \vocs and \cocos}

\myparagraph{Training baseline models:} We train starting from models pre-trained on \imagenet \citeApp{imagenet}. We fine-tune with fixed learning rates in $\{10^{-3},10^{-4},10^{-5}\}$ using an Adam optimizer \citeApp{kingma2014adam} and select the checkpoint with the best validation F1-score. For \vocs, we train for 300 epochs, and for \cocos, we train for 60 epochs.

\myparagraph{Training guided models:} We train the models jointly optimized for classification and localization (\cref{eq:overall}) by fine-tuning the baseline models. We use a fixed learning rate of $10^{-4}$ and a batch size of $64$. For each configuration (given by a combination of attribution method, localization loss, and layer), we train using three different values of $\lambda_{\text{loc}}$, as detailed in \cref{tab:supp:lambdas}. For \vocs, we train for 50 epochs, and for \cocos, we train for 10 epochs.

\myparagraph{Selecting models to visualize:} As described in \cref{sec:experiments}, we select and evaluate on the set of Pareto-dominant models for each configuration after training. Each model on the Pareto front represents the extent of trade-off made between classification (F1) and localization (\epg) performance. In practice, the `best' model to choose would depend on the requirements of the end user. However, to evaluate the effectiveness of model guidance (\eg \cref{fig:teaser,fig:teaser2,fig:loss_comp}), we select a representative model on the front whose attributions we visualize. This is done by selecting the model with the highest \epg score with an at most 5 p.p. drop in F1-score.

\myparagraph{Efficient Optimization:} As described in \cref{sec:method:efficient}, for each image in a batch, we optimize for localization of a single class selected at random. This approximation allows us to perform model guidance efficiently and keeps the training cost tractable. However, to accurately evaluate the impact of this optimization, we evaluate the localization of all classes in the image at test time.

\myparagraph{Training with Limited Annotations:} As described in \cref{sec:results:ablations}, we show that training with a limited number of annotations can be a cost effective way of performing model guidance. In order to maintain the relative magnitude of $\mathcal{L}_{\text{loc}}$ as compared to $\mathcal{L}_{\text{class}}$ in this setting, we scale up the values of $\lambda_{\text{loc}}$ when training. The values of $\lambda_{\text{loc}}$ we use are shown in \cref{tab:supp:dilation:lambdas}.

\subsubsection{Experiments with \waterbirds}

\myparagraph{Data distributions:} The conventional binary classification task includes classifying \textit{Landbird} from \textit{Waterbird}, irrespective of their backgrounds.  We use the same splits generated and published by \citeApp{petryk2022guiding}. As discussed in \cref{supp:sec:waterbirds}, at training time there are no samples from \textbf{G2} or \textbf{G3}, making the bird type and the background perfectly correlated.  In contrast, both the validation and test sets are balanced across foregrounds and backgrounds, \ie a landbird is equally likely to occur on land or water, and vice-versa. However, as noted by \citeApp{sagawa2019distributionally}, using a validation set with the same distribution as the test set leaks information on the test distribution in the process of hyperparameter and checkpoint selection during training. Therefore, we modify the validation split to avoid such information leakage; in particular, we use a validation set with the same distribution as the training set, and only use examples of groups \textbf{G1} and \textbf{G4}. Note that \cref{tab:waterbirds} refers to \textbf{G3} as the ``Worst Group''. \\

\myparagraph{Training details:} We train starting from models pre-trained on \imagenet \citeApp{imagenet}. We fine-tune with fixed learning rate of $10^{-5}$ with $\lambda_{\text{loc}}$ of $5\times10^{-2}$ ($5\times10^{-4} \times 100$ for using 1\% of annotations) using an Adam optimizer \citeApp{kingma2014adam} . We train for 350 epochs with random cropping and horizontal flipping and select the checkpoint with the highest accuracy on the modified validation set.

\begin{table}[t]
    \def\arraystretch{1.2}
    \centering
    \begin{tabular}{c@{\hskip10pt}|@{\hskip10pt}c}
	    % \hline
	    \bf Localization Loss & \bf Values of $\lambda_{\text{loc}}$ \\
	    % \hhline{|=||=|}
     \hline
        \energyloss & \footnotesize$5\mytimes10^{-4}$, \phantom{0} $1\mytimes10^{-3}$, \phantom{0} $5\mytimes10^{-3}$ \\
        % \hline 
        \loneloss &\footnotesize $1\mytimes10^{-3}$, \phantom{0} $5\mytimes10^{-3}$, \phantom{0} $1\mytimes10^{-2}$ \\
        % \hline 
        \ppceloss &\footnotesize $1\mytimes10^{-4}$, \phantom{0} $5\mytimes10^{-4}$, \phantom{0} $1\mytimes10^{-3}$ \\
        % \hline 
        \rrrloss &\footnotesize $5\mytimes10^{-6}$, \phantom{0} $1\mytimes10^{-5}$, \phantom{0} $5\mytimes10^{-5}$  \\
        % \hline
	\end{tabular}
	\caption{\textbf{Hyperparameter $\lambda_{\text{loc}}$: Default training.} used for when training on \vocs and \cocos with each localization loss. Different values are used for different loss functions since the magnitudes of each loss varies.}
    \label{tab:supp:lambdas}
\end{table}

\begin{table}[t]
    \def\arraystretch{1.2}
    \centering
    \begin{tabular}{c@{\hskip10pt}|@{\hskip10pt}c}
	    % \hline
	    \bf Localization Loss & \bf Values of $\lambda_{\text{loc}}$ \\
	    % \hhline{|=||=|}
     \hline
        \energyloss & \footnotesize$0.05$, \phantom{0} $0.100$, \phantom{0} $0.50$ \\
        % \hline 
        \loneloss &\footnotesize $0.01$, \phantom{0} $0.100$, \phantom{0} $1.00$ \\
	\end{tabular}
	\caption{\textbf{Hyperparameter $\lambda_{\text{loc}}$: Limited annotations.} used for when training on \vocs and \cocos with \textbf{limited data} for each localization loss. Different values are used for different loss functions since the magnitudes of each loss varies. We use larger values of $\lambda_{\text{loc}}$ when training with limited annotations to maintain the relative magnitudes of the classification and localization losses during training.}
    \label{tab:supp:dilation:lambdas}
\end{table}

\subsection{Optimizing \bcos Attributions}
\label{supp:sec:implementation:bcos}

Training for optimizing the localization of attributions (\cref{eq:overall}) requires backpropagating through the attribution maps, which implies that they need to be differentiable. While \bcos attributions \citeApp{bohle2022b} as formulated are mathematically differentiable, the original implementation\footnoteref{footnote:bcoscode} \citeApp{bohle2023b} for computing them involves detaching the dynamic weights from the computational graph, which prevents them from being used for optimization. In this work, to use them for model guidance, we develop a twice-differentiable implementation of \bcos attributions.