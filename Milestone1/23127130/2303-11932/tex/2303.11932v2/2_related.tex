\section{Related Work}
\label{sec:related}

\myparagraph{Attribution Methods} \citeMain{simonyan2013deep,springenberg2014striving,sundararajan2017axiomatic,shrikumar2017learning,selvaraju2017grad,wang2020score,ramaswamy2020ablation,jiang2021layercam,chattopadhyay2018grad,petsiuk2018rise,fong2017interpretable,zeiler2013visualizing,ribeiro2016should,dabkowski2017real,bach2015pixel} are often used to explain black-box models by generating heatmaps that highlight input regions important to the model's decision. However, such methods are often not faithful to the model \citeMain{adebayo2018sanity,rao2022towards,kim2021sanity,zhou2022feature,adebayo2022post} and risk misleading users. Recent work proposes inherently interpretable models \citeMain{boehle2021convolutional,bohle2022b} that address this by providing model-faithful explanations by design. In our work, we use both popular post-hoc and model-inherent attribution methods to guide models and discuss their effectiveness.

\myparagraph{Attribution Priors:} Several approaches have been proposed for training better models by enforcing desirable properties on their attributions. These include enforcing consistency against augmentations \citeMain{pillai2021explainable,pillai2022consistent,guo2019visual}, smoothness \citeMain{erion2021improving,moshe2022improving,kiritoshi2019l1}, separation of classes \citeMain{zhang2022correct,pillai2022consistent,sun2020fixing,nakka2020towards,singh2020don}, or constraining the model's attention \citeMain{fukui2019attention,asgari2022masktune}.
In contrast, in this work, we focus on providing explicit human guidance to the model using bounding box annotations. {This constitutes more explicit guidance but allows fine-grained control over the model's reasoning even with few annotations.}

\myparagraph{Model Guidance:} In contrast to the indirect regularization effect achieved by attribution priors, various approaches have been proposed (cf.~\citeMain{friedrich2022typology,teso2022leveraging}) to actively guide models by regularizing their attributions, for tasks such as classification \citeMain{ross2017right,gao2022aligning,gao2022res,rieger2020interpretations,petryk2022guiding,hagos2022identifying,teney2020learning,mitsuhara2019embedding,teso2019explanatory,teso2019toward,schramowski2020making,shao2021right,linsley2018learning,shen2021human,yang2022improving,fel2022harmonizing}, segmentation \citeMain{li2018tell}, VQA \citeMain{selvaraju2019taking,teney2020learning}, and knowledge distillation \citeMain{fernandes2022learning}. The goal of such approaches is not only to improve performance, but also make sure that the model is ``right for the right reasons'' \citeMain{ross2017right}. For classifiers, this typically involves jointly optimizing both for classification performance and localization to object features.
While various benefits of model guidance have been reported, most prior work evaluate on simple datasets \citeMain{ross2017right,shao2021right,gao2022aligning,gao2022res} and, thus far, no common evaluation setting has emerged. Recently, \citeMain{chefer2022optimizing} has extended model guidance to ImageNet, showing that its benefits can scale to large scale problems. In contrast to \citeMain{chefer2022optimizing}, who investigated one particular attribution method \citeMain{chefer2021beyond}, our focus lies on a better understanding of the impact of the different design choices for model guidance.

To distill the most effective techniques for model guidance, in this work, we conduct an in-depth evaluation on challenging,commonly used real-world multi-label classification datasets (\voc, \coco). 
Specifically, we perform a comprehensive comparison across multiple dimensions of interest: the loss function, the model architecture, the guidance depth, and the attribution method.
For this, we evaluate the localization losses introduced in the closest related work, \ie \rrr \citeMain{ross2017right}, \haics \citeMain{shen2021human}, and \gradia \citeMain{gao2022aligning}; additionally, we propose using the EPG metric \cite{wang2020score} as a loss function and show that it has various desirable properties, in particular when guiding models via bounding box annotations. 

{Finally, model guidance has also been used to mitigate reliance on spurious features using language guidance \citeMain{petryk2022guiding}, and we show that using a small number of coarse bounding box annotations can be similarly effective.}

\myparagraph{Evaluating Model Guidance:} The benefits of model guidance have typically been shown via improvements in classification performance (\eg \citeMain{ross2017right,rieger2020interpretations}) or an increase in \iou between object masks and attribution maps (\eg \citeMain{gao2022res,li2018tell}). In addition to these metrics, we also evaluate on the \energypg metric \citeMain{wang2020score}, which has thus far only been used to evaluate the quality of the attribution methods themselves. We further show that it lends itself well to being used as a guidance loss, as it places only minor constraints on the model, and, in contrast to the IoU metric, it is fully differentiable.