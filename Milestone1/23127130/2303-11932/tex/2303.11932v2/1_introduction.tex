\vspace{-.75em}
\begin{figure}[!ht]
    \centering 
    \begin{subfigure}[c]{.9\columnwidth}
    \includegraphics[width=\textwidth]{results/VOC/figures/qualitative/teaserv2_0.pdf}
    \end{subfigure}
    \vspace{.15cm}

    \begin{subfigure}[c]{\columnwidth}
    \includegraphics[width=\textwidth]{results/figures/waterbird_teaser.pdf}
    \end{subfigure}
    \caption{\textbf{(a) Model guidance increases object focus.}
    Models may rely on irrelevant background features or spurious correlations (\eg presence of person provides positive evidence for bicycle, center row, col. 1). Guiding the model via bounding box annotations can mitigate this and consistently increases the focus on object features (bottom row).
    \textbf{(b) Model guidance can improve accuracy.} In the presence of spurious correlations in the training data, non-guided models might focus on the wrong features. In the example image in (b), the waterbird is incorrectly classified to be a landbird due to the background (col.~3). Guiding the model via bounding box annotation (as shown in col.~2), the model can be guided to focus on the bird features for classification (col.~4).
    }
    \label{fig:teaser}
\end{figure}
\section{Introduction}
\label{sec:introduction}


\input{results/tex_figures/double_col_qualitative}
 
Deep neural networks (DNNs) excel at learning predictive features that allow them to correctly classify a set of training images with ease. The features learnt on the training set, however, do not necessarily transfer to unseen images: \ie, instead of learning the actual class-relevant features, DNNs might memorize individual images {(\cf.~\citeMain{feldman2020neural})}
or exploit spurious correlations in the training data (cf.~\citeMain{xiao2021noise}).
For example, if bikes are highly correlated with people in the training data, a model might learn to associate the presence of a person in an image as positive evidence for a bike {(\eg \cref{fig:teaser}{\color{red}a}, col.\ 1, rows 1-2)}, which can limit how well it generalizes. Similarly, a bird classifier might rely on background features from the bird's habitat, and fail to correctly classify in a different habitat (cf.~\cref{fig:teaser}{\color{red}b} cols.\ 1-3 and \citeMain{petryk2022guiding}).

{To detect 
such behaviour, recent advances in model interpretability have provided attribution methods (\eg \citeMain{selvaraju2017grad,sundararajan2017axiomatic,shrikumar2017learning,bohle2022b})
to understand a model's reasoning. These methods typically provide attention maps that highlight regions of importance in an input to explain the model's decisions and can help identify incorrect reasoning such as reliance on spurious or irrelevant features, see for example \cref{fig:teaser}{\color{red}b}.}

As many attribution methods are in fact themselves differentiable (\eg \citeMain{shrikumar2017learning,sundararajan2017axiomatic,selvaraju2017grad,bohle2022b}), recent work \citeMain{ross2017right,shen2021human,gao2022aligning,gao2022res,teso2019explanatory,teso2019toward} has explored the idea of using them to guide the models to
make them ``right for the right reasons'' \citeMain{ross2017right}. Specifically, models can be guided by jointly optimizing for correct classification as well as for attributing importance to regions deemed relevant by humans.
This can help the model focus on the relevant features of a class, and correct errors in reasoning (\cref{fig:teaser}{\color{red}b}, col.\ 4). Such guidance has the added benefit of providing well-localized 
explanations that are thus easier to understand for end users (\eg \cref{fig:teaser2}).
 
While model guidance has shown promising results, a detailed study of how to do this most \emph{effectively} is crucially missing. In particular, model guidance has so far been studied for a limited set of attribution methods and models and usually on relatively simple and/or synthetic datasets; further, the evaluation settings between approaches can significantly differ, which makes a fair comparison difficult.

Therefore, in this work, we perform an in-depth evaluation of model guidance on large scale, real-world datasets, to better understand the effectiveness of a variety of design choices.  
Specifically, we evaluate model guidance along the following dimensions: the model architecture,
the guidance \emph{depth}\footnote{The layer at which guidance is applied, \eg typically at the last convolutional layer for \gradcam \cite{selvaraju2017grad} or the first layer for \ixg\cite{shrikumar2017learning}.}, the attribution method,
and the loss function. In this context, we propose using the EPG score \cite{wang2020score}---an evaluation metric that has thus far been used to evaluate the quality of attribution methods---as an additional loss function (which we call the \energyloss loss) as it is fully differentiable.

Further, as annotation costs can be a major hurdle for making model guidance practical, we place a particular focus on \emph{efficient} guidance. Specifically, we use bounding boxes instead of semantic segmentation masks, and evaluate the robustness of guidance techniques under limited or overly coarse annotations to reduce data collection costs.

We find that our \energyloss loss lends itself well to those settings. On the one hand, it exhibits a high degree of robustness to limited or noisy bounding box annotations (cf.~\cref{fig:coarse_annotations,fig:lim_local_input}). On the other hand, despite the coarseness of bounding box guidance, it maintains a clear focus on object-specific features inside the bounding boxes, see \cref{fig:teaser}{\color{red}a}, row 3. In contrast, prior approaches often regularize for a uniform distribution of the attribution values inside the annotation masks, and thus
tend to exhibit much lower attribution granularity (cf.~\cref{fig:loss_comp}).

\myparagraph{Contributions.} \textbf{(1)} We perform an in-depth evaluation of model guidance on challenging large scale, multi-label classification datasets (\voc \citeMain{everingham2009pascal}, \coco \citeMain{lin2014microsoft}), assessing the impact of attribution methods, model architectures, guidance depths, and loss functions. Further, we show that, despite being relatively coarse,  bounding box supervision can provide sufficient guidance to the models whilst being much cheaper to obtain than semantic segmentation masks. 
\textbf{(2)} We propose using the Energy Pointing Game (EPG) score \citeMain{wang2020score} as an alternative to the IoU metric for evaluating the effectiveness of such guidance and show that the EPG score constitutes a good loss function for model guidance, particularly when using bounding boxes. \textbf{(3)} We show that model guidance can be performed cost-effectively by using annotation masks that are noisy or are available for only a small fraction (\eg $1\%$) of the training data. \textbf{(4)} We show through experiments on the \waterbirds dataset \citeMain{sagawa2019distributionally,petryk2022guiding} that model guidance with a small number of annotations suffices to improve the model's generalization under distribution shifts at test time.