\section{Guiding Models Using Attributions}
\label{sec:method}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{results/ModelGuidingPipeline.png}
    \caption{\textbf{Model guidance overview.} 
    We jointly optimize for classification ($\mathcal L_\text{class}$) and localization of attributions to human-annotated bounding boxes ($\mathcal L_\text{loc}$), to guide the model to focus on object features. Various localization loss functions can be used, see \cref{sec:method:losses}.
    }
    \label{fig:pipeline}
\end{figure}

In this section, we provide an overview of the model guidance approach that jointly optimizes for classification and localization (\cref{sec:method:procedure}). Specifically, we describe the attribution methods (\cref{sec:method:attributions}), metrics (\cref{sec:method:metrics}), and localization loss functions (\cref{sec:method:losses}) that we evaluate in \cref{sec:results}.
In \cref{sec:method:efficient} we discuss our strategy to train for localization in the presence of multiple ground truth classes.

\myparagraph{Notation:} We consider a multi-label classification problem with $K$ classes with $X\myin\mathbb{R}^{C\times H\times W}$ the input image and $y\myin\{0,1\}^K$ the one-hot encoding of the image labels. With $A_k\myin\mathbb{R}^{H\times W}$ we denote an attribution map for a class $k$ for $X$ using a classifier $f$; $A_{k}^+$ denotes the positive component of the attributions, $\hat{A}_k=\frac{A_k}{\max(\text{abs}(A_k))}$ normalized attributions, and $\hat{A}_{k}^+=\frac{A_{k}^+}{\max(A_{k}^+)}$ normalized positive attributions. Finally, $M_k \myin \{0,1\}^{H\times W}$ denotes the binary mask for class $k$, which is given by the union of bounding boxes of all occurrences of class $k$ in $X$.

\subsection{Model Guidance Procedure}\label{sec:method:procedure}

Following prior work (\eg \citeMain{ross2017right,shen2021human,gao2022aligning,gao2022res}), the model is trained jointly for classification and localization (cf.~\cref{fig:pipeline}):
\begin{equation}\label{eq:overall}
\textstyle
    \mathcal{L}=\mathcal{L}_\text{class} + \lambda_\text{loc}\mathcal{L}_\text{loc}\;.
\end{equation}
\Ie, the loss consists of a classification loss ($\mathcal{L}_\text{class}$), for which we use binary cross-entropy, and a localization loss ($\mathcal{L}_\text{loc}$), which we discuss in \cref{sec:method:losses}; here, the hyperparameter $\lambda_\text{loc}$ controls the weight given to each of the objectives.

\subsection{Attribution Methods}
\label{sec:method:attributions}

In contrast to prior work that typically use \gradcam \citeMain{selvaraju2017grad} attributions, we perform an evaluation over a selection of popularly used differentiable\footnote{Differentiability is necessary for optimizing attributions via gradient descent, so non-differentiable methods (\eg \citeMain{ribeiro2016should,petsiuk2018rise}) are not considered.} attribution methods which have been shown to localize well \citeMain{rao2022towards}: \ixg \citeMain{shrikumar2017learning}, \intgrad \citeMain{sundararajan2017axiomatic}, and \gradcam \citeMain{selvaraju2017grad}. We further evaluate model-inherent explanations of the recently proposed \bcos models \citeMain{bohle2022b}. To ensure comparability across attribution methods \citeMain{rao2022towards}, we evaluate all attribution methods at the input, various intermediate, and the final spatial layer. 

\myparagraph{\ixg \citeMain{shrikumar2017learning}} computes the element-wise product $\odot$ of the input and the gradients of the $k$-th output w.r.t.~the input, \ie $X\odot\nabla_X f_k(X)$. For piece-wise linear models such as DNNs with ReLU activations \citeMain{nair2010rectified}, this faithfully computes the linear contributions of a given input pixel to the model output.

\myparagraph{\gradcam \citeMain{selvaraju2017grad}} computes importance attributions as a ReLU-thresholded, gradient-weighted sum of activation maps. In detail, it is given by
$\text{ReLU}(\sum_c \alpha_c^k \odot U_c)$ with $c$ denoting the channel dimension, and $\alpha^k$ the average-pooled gradients of the output for class $k$ with respect to the activations $U$ of the last convolutional layer in the model.

\myparagraph{\intgrad \citeMain{sundararajan2017axiomatic}} takes an axiomatic approach and is formulated as the integral of gradients over a straight line path from a baseline input to the given input $X$. Approximating this integral requires several gradient computations, making it computationally expensive for use in model guidance. To alleviate this, when optimizing with \intgrad, we use the recently proposed \xdnn models \citeMain{hesse2021fast} that 
allow for an exact computation of \intgrad in a single backward pass.

\myparagraph{\bcos \citeMain{bohle2022b}} attributions are generated using the inherently-interpretable \bcos networks, which promote alignment between the input $\mathbf x$ and a dynamic weight matrix $\mathbf W(\mathbf x)$ during optimization. 
In our experiments, 
we use the contribution maps given by the element-wise product of the dynamic weights with the input ($\mathbf W^T_k(\mathbf x)\odot \mathbf x$), which faithfully represent the contribution of each pixel to class $k$. To be able to guide \bcos models, we developed a differentiable implementation of \bcos explanations, see supplement.

\subsection{Evaluation Metrics}
\label{sec:method:metrics}

We evaluate the models' performance on both our training objectives: classification and localization. For classification, we use the F1 
score and mean average precision (\map). We discuss the localization metrics below.

\myparagraph{Intersection over Union (IoU)} is a commonly used metric (cf.~\citeMain{gao2022res}) that computes the intersection between the ground truth annotation masks and the binarized attribution maps, normalized by their union; for binarization, a threshold parameter needs to be chosen. 
In this work, the ground truth masks are taken to be the union of
all bounding boxes of a class in the image and, following prior work \citeMain{fong2017interpretable}, the threshold parameter is selected via a heldout set. 

\myparagraph{Energy-based Pointing Game (EPG) \citeMain{wang2020score}} measures the concentration of attribution energy within the mask, \ie the fraction of positive attributions inside the bounding boxes:
\begin{equation}\label{eq:epg}
    \text{EPG}_k = \frac{\sum_{h=1}^H\sum_{w=1}^W M_{k,hw} A^+_{k,hw}}{\sum_{h=1}^H\sum_{w=1}^W A^+_{k,hw}}\;.
\end{equation}
In contrast to \iou,
\epg more faithfully takes into account the relative importance given to each input region,
since it does not binarize the attributions. Like \iou, the scores lie in $[0,1]$, with higher scores indicating better localization. 

\subsection{Localization Losses}\label{sec:method:losses}
We evaluate the most commonly used localization losses ($\mathcal{L}_\text{loc}$ in \cref{eq:overall}) from prior work. We describe these losses as applied on attribution maps of an image for a single class $k$, as well as the proposed EPG-derived \energyloss loss.

\myparagraph{\loneloss loss (\citeMain{gao2022aligning,gao2022res}, \cref{eq:lone})}
minimizes the $L_1$ distance between annotation masks and normalized positive attributions $\hat A_{k}^+$, guiding the model towards uniform attributions inside the mask and suppressing attributions outside of it. 
\begin{equation}\label{eq:lone}
    \textstyle
    \mathcal{L}_{\text{loc},k} = \frac{1}{H\times W}\sum_{h=1}^H\sum_{w=1}^W\lVert M_{k,hw} - \hat{A}_{k,hw}^+ \rVert_1
\end{equation}

\myparagraph{Per-pixel cross entropy (\ppceloss) loss (\citeMain{shen2021human}, \cref{eq:ppce})}
applies a binary cross entropy loss between the mask and the normalized positive annotations $\hat A_{k}^+$, thus guiding the model to maximize the attributions inside the mask:
\begin{equation}\label{eq:ppce}
\textstyle
    \mathcal{L}_{\text{loc},k} = -\frac{1}{\lVert M_k \rVert_1}\sum_{h=1}^H\sum_{w=1}^W M_{k,hw}\log(\hat{A}_{k,hw}^+) \;.
\end{equation}
As \ppceloss does not constrain attributions outside the mask, there is no explicit pressure to avoid spurious features.
    
\myparagraph{\rrrloss loss (\citeMain{ross2017right}, \cref{eq:rrr}).}
\citeMain{ross2017right} introduced the RRR loss to regularize the normalized input gradients $\hat{A}_{k,hw}$ as 
\begin{equation}\label{eq:rrr}
    \textstyle \mathcal{L}_{\text{loc},k} = \sum_{h=1}^H\sum_{w=1}^W (1-M_{k,hw}) \hat{A}_{k,hw}^2 \;.
\end{equation}
To extend it to our setting, we 
take $\hat{A}_{k,hw}$ to be given by an arbitrary attribution method (\eg \intgrad); 
we denote this generalized version by \rrrloss.
In contrast to the \ppceloss loss, \rrrloss only regularizes attributions \emph{outside} the ground truth masks. While it thus does not introduce a uniformity prior similar to the \loneloss loss, it also does not explicitly promote high importance attributions inside the masks.

\myparagraph{\energyloss Loss.}\label{sec:method:energyloss}
In addition to the losses described in prior work, we propose to also evaluate using the \epg score (\citeMain{wang2020score}, \cref{eq:epg}) as a loss function for model guidance, as it is fully differentiable. In particular, we simply define it as
\begin{equation}\label{eq:energyloss}
\textstyle
    \mathcal{L}_{\text{loc},k} = -\text{EPG}_k.
\end{equation}
Unlike existing localization losses that either (i) do not constrain attributions across the entire input (\rrrloss, \ppceloss), or (ii) force the model to attribute uniformly within the mask even if it includes irrelevant background regions (\loneloss, \ppceloss), maximizing the \epg score jointly optimizes for higher attribution energy within the mask and lower attribution energy outside the mask. 
By not enforcing a uniformity prior, we find that the \energyloss loss is able to provide effective guidance while allowing the model to learn freely what to focus on within the bounding boxes (\cref{sec:results}).

\subsection{Efficient Optimization}\label{sec:method:efficient}
In contrast to prior work \citeMain{ross2017right,shen2021human,gao2022aligning,gao2022res}, we perform model guidance on a multi-label classification setting, and consequently there are multiple ground truth classes whose attribution localization could be optimized. Computing and optimizing for several attributions within an image would add a significant overhead to the computational cost of training (multiple backward passes).
Hence, for efficiency, we sample one ground truth class $k$ per image at random for every batch and only optimize for localization of that class, \ie, $\mathcal{L}_\text{loc}\myeq\mathcal{L}_{\text{loc},k}$. We find that this still provides effective model guidance while keeping the training cost tractable.