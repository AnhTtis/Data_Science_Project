\begin{table*}[htb]
\centering
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dimension Reduction & Index Info & Memory (GB) & $R \cap k$%~\eqref{eq:eval r int k} 
& IN Zero-shot Top-1 \\
\hline
Raw ViT-B-32 & - &  239 & 100 &  56.1 \\
\hline
$\mathcal{L}_{\textsc{SNIP}}$, $d=128$ & IVFPQ, M=4 &  2.2 & 25.4 &  31.4 \\
\hline
$\mathcal{L}_{\textsc{SNIP}}$, $d=128$ & IVFPQ, M=16 &  4.2 & 35.5 &  34.5 \\
\hline
$\mathcal{L}_{\textsc{MSE}}$, $d=128$ & IVFPQ, M=4 &  2.2 & 25.1 &  20.7 \\
\hline
$\mathcal{L}_{\textsc{MSE}}$, $d=128$ & IVFPQ, M=16 &  4.2 & 35.4 &  30.4 \\
\hline
OPQ, $d=56$ & AutoFaiss &  2.9 & .05 &  1.0 \\
\hline
OPQ, $d=168$ & AutoFaiss &  5.6 & 21.4 &  23.1 \\
\hline
\end{tabular}
\vspace*{1mm}
\caption{Comparisons of the k-NN accuracy (with k=5) of different networks and losses on ViT-B-32 \cite{radford2021learning} for queries outside of the database vectors, against an exhaustive search. The compressed CLIP features have a dimensionality of $d$, and the number of chunks for product quantization is denoted by $M$.}
\label{tab:vitb32 full analysis}
%\medskip
\end{table*}
