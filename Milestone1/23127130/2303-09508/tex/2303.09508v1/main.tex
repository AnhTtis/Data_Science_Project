\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{conf}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% pasted from previous project
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{booktabs}
% \usepackage{caption,soul}
\usepackage{soul}
\usepackage[caption=false]{subfig}
\usepackage{capt-of,etoolbox}
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithm,algorithmic}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\newcommand\norm[1]{\left\lVert#1\right\rVert}


\finalcopy


\begin{document}





%%%%%%%%% TITLE
\title{LDMVFI: Video Frame Interpolation with Latent Diffusion Models}

\author{Duolikun Danier \qquad\qquad Fan Zhang \qquad\qquad David Bull\\
University of Bristol\\
{\tt\small \{duolikun.danier, fan.zhang, dave.bull\}@bristol.ac.uk}
}

\twocolumn[{%
% \maketitle
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \includegraphics[width=\linewidth]{figures/visual.pdf}
    \vspace{-3em}
    \captionof{figure}{Visual examples of frames interpolated by the state-of-the-art methods and the proposed LDMVFI. Under large and complex motions, our method preserves the most high-frequency details, delivering superior perceptual quality.}\label{fig:visual}
\end{center}
}]


\ifcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Existing works on video frame interpolation (VFI) mostly employ deep neural networks trained to minimize the L1 or L2 distance between their outputs and ground-truth frames. Despite recent advances, existing VFI methods tend to produce perceptually inferior results, particularly for challenging scenarios including large motions and dynamic textures. Towards developing perceptually-oriented VFI  methods, we propose latent diffusion model-based VFI, LDMVFI. This approaches the VFI problem from a generative perspective by formulating it as a conditional generation problem. As the first effort to address VFI using latent diffusion models, we rigorously benchmark our method following the common evaluation protocol adopted in the existing VFI literature. Our quantitative experiments and user study indicate that LDMVFI is able to interpolate video content with superior perceptual quality compared to the state of the art, even in the high-resolution regime. Our source code will be made available \href{https://github.com/danielism97/ldmvfi}{here}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
The purpose of video frame interpolation (VFI) is to generate intermediate frames between two existing consecutive frames in a video sequence. It is commonly used  to synthetically increase frame rate, for example to generate jitter-free slow-motion content~\cite{jiang2018super}. VFI has also been used in video compression~\cite{wu2018video}, novel view synthesis~\cite{flynn2016deepstereo}, medical imaging~\cite{karargyris2010three} and animation production~\cite{siyao2021deep}.

Existing VFI methods~\cite{jiang2018super, xue2019video, bao2019depth, park2020bmbc, lee2020adacof, cheng2020video, ding2021cdfi, niklaus2020softmax, kalluri2023flavr, park2021asymmetric, kong2022ifrnet, lu2022video, shi2022video} are mostly based on deep neural networks. These deep models differ in architectural designs and motion modeling approaches, but are mainly PSNR-oriented in the sense that they are trained to minimize the L1 or L2 distance between their outputs and the ground-truth intermediate frames. Although some previous works~\cite{niklaus2017video,niklaus2018context, danier2022st} have used feature distortion-based~\cite{Simonyan15vgg} or GAN-based~\cite{goodfellow2020generative} losses to fine-tune the PSNR-oriented models, the major contributor to the optimization objective (and hence the final performance of the model) is still the L1/L2-based distortion loss. As a result, it has been reported ~\cite{danier2022subjective} that existing methods, while achieving high PSNR values, tend to under-perform perceptually, especially under challenging scenarios including large motion and dynamic textures.

In contrast, diffusion models~\cite{ho2020denoising, song2021scorebased, ho2022video, rombach2022high} have recently shown remarkable performance in generating realistic, perceptually-optimized images and videos. Diffusion models have reportedly outperformed other generative models including GANs and VAEs~\cite{dhariwal2021diffusion, ho2022cascaded}. However, despite their strong ability to synthesize high-fidelity visual content, the application of diffusion models for VFI and their potential advantages over current PSNR-oriented VFI approaches have not been fully investigated.

In this work, we develop a latent diffusion model for video frame interpolation (LDMVFI), where we formulate VFI as a conditional image generation problem. Specifically, we adopt the recently proposed \textit{latent diffusion models}~\cite{rombach2022high} (LDMs, a.k.a. Stable Diffusion) within a framework comprising an autoencoding model that projects images into a latent space, and a denoising U-Net which performs reverse diffusion process in that latent space. To better adapt LDMs to VFI, we devise VFI-specific components, notably a novel vector quantization-based VFI-autoencoding model, VQ-FIGAN, with which our method shows superior performance over vanilla LDMs. %We also perform experiments to study the effect of different factors in LDMs on VFI performance.

Despite the paradigmatic shift from mainstream PSNR-oriented VFI methods, we adhere to the commonly adopted VFI benchmarking protocol and evaluate the proposed method on various VFI test sets, covering both low and high resolution contents (up to 1920$\times$1080). Our results indicate that LDMVFI performs favorably against the state of the art in terms of three perceptual metrics~\cite{zhang2018unreasonable, hou2022perceptual, danier2022flolpips}. We also conducted a user study to collect subjective judgments on the quality of full HD videos interpolated by our method and several competitive counterparts to further confirm the superior perceptual performance of LDMVFI.

To the best of our knowledge, this work is the first attempt to address VFI as a conditional generation problem using latent diffusion models, as well as the first to demonstrate the perceptual superiority of diffusion-based VFI over the state-of-the-art PSNR-based approaches. Our contributions are summarized below.

\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item We present LDMVFI, a novel latent diffusion model for VFI that leverages the high-fidelity image synthesis ability of diffusion models to perform conditional generation. 
    \item We design novel VFI-specific components in LDMs, including a vector-quantized autoencoding model, VQ-FIGAN, which further enhances VFI performance.
    \item We demonstrate through quantitative and subjective experiments that the proposed method produces perceptually superior interpolation results over the state of the art.
\end{itemize}

\section{Relate Work}
\textbf{Video Frame Interpolation.} Existing VFI approaches are mostly based on deep learning, and can be generally categorized as flow-based or kernel-based. Flow-based methods rely on optical flow estimation to generate interpolated frames. To obtain the optical flow from input frames to the non-existent middle frame (or the other way around), some methods~\cite{jiang2018super, xu2019quadratic, niklaus2018context, niklaus2020softmax, sim2021xvfi}  assume certain motion types to infer the intermediate optical flows using the flows between two input frames, while others~\cite{liu2017video,xue2019video,park2020bmbc,park2021asymmetric,lu2022video,kong2022ifrnet,reda2022film,huang2022real} directly estimate the intermediate flows. On the other hand, kernel-based methods argue that optical flows can be unreliable in dynamic texture scenes, so they predict locally adaptive convolution kernels to synthesize output pixels, allowing more flexible many-to-one mapping. While earlier methods~\cite{adaconv, niklaus2017video} in this class predict fixed-size kernels, more recent ones~\cite{lee2020adacof, shi2020video, ding2021cdfi, cheng2020video, cheng2021multiple, gui2020featureflow, danier2022enhancing, shi2022video} tend to adopt deformable convolution~\cite{dai2017deformable} kernels. Other than these two classes, there are also attempts to combine flows and kernels~\cite{bao2019depth,bao2019memc,danier2022st}, and to perform end-to-end frame synthesis~\cite{choi2020channel,kalluri2023flavr}. 

It is noted that the above methods are trained to optimize PSNR-oriented loss functions, i.e., the L1/L2 distance between the model outputs and the ground-truth frames. Although this results in reasonably good PSNR performance, it has been previously reported~\cite{danier2022subjective} that PSNR does not fully reflect the perceptual quality of interpolated videos, exhibiting poor correlation performance with subjective ground truth. To improve perceptual performance, some existing methods~\cite{niklaus2018context,niklaus2020softmax} use the VGG~\cite{Simonyan15vgg} feature-based loss in combination with the L1 loss. However, we observe two limitations of this approach. Firstly, it was shown in previous works~\cite{danier2022subjective} that such deep feature-based distances also fail to predict the perceived quality of VFI-generated videos. Secondly, in these models,  the L1 loss still plays a more important role (compared to the VGG loss) during the optimization process, constraining the perceptual performance. An alternative approach uses GANs~\cite{lee2020adacof,danier2022st} to enhance perceptual quality of interpolated videos. However, due to the instability of GAN training~\cite{goodfellow2020generative}, these models are typically pre-trained using L1 loss, then fine-tuned with an adversarial loss. There is thus only limited impact of the additional adversarial fine-tuning on the final perceptual quality.

\textbf{Diffusion Models.} Recently, diffusion models (DMs)~\cite{ho2020denoising,song2021scorebased,rombach2022high, li2022srdiff, ho2022video} have demonstrated remarkable performance in synthesizing high-fidelity images and videos. In their original form~\cite{ho2020denoising}, DMs generate new images by progressively denoising a Gaussian noise image;  the process corresponds to the reverse of a Markov chain that gradually adds noise to a clean image. DMs have been reported to offer superior performance~\cite{dhariwal2021diffusion, ho2022cascaded} compared to GANs~\cite{goodfellow2020generative} and VAEs~\cite{Kingma2014vae} in image generation tasks. The only previous application of DMs on VFI is \cite{voleti2022mcvd}, but this work focused on low-resolution images and the model lacked any VFI-specific innovations, showing limited interpolation performance. The recently proposed latent diffusion models (LDMs) have demonstrated strong ability to synthesize high-resolution images by performing diffusion processes in latent space. However, we observe that LDMs have not previously been exploited for VFI. 



\section{Proposed Method: LDMVFI}
Given two consecutive frames $I^0,I^1$ from a video, VFI aims to generate the non-existent intermediate frame $I^n$ where $n=0.5$ for $\times$2 upsampling. Approaching VFI from a generative perspective, our goal is to learn a parametric approximation of the conditional distribution $p(I^n|I^0,I^1)$ using a dataset $\mathcal{D}=\{I^0_s, I^n_s, I^1_s\}_{s=1}^S$. To achieve this, we adopt the latent diffusion models~\cite{rombach2022high} to perform conditional generation for VFI. The proposed LDMVFI contains two main components: (i) a VFI-specific \textbf{autoencoding model}, VQ-FIGAN, that projects frames into a latent space, and reconstructs the target frame from the latent encoding; (ii) a \textbf{denoising U-Net} that performs reverse diffusion process in the latent space for conditional image generation. Figure~\ref{fig:overall} shows the overview of the LDMVFI. 

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/overall.pdf}
\caption{Overview of the diffusion processes in LDMVFI. The encoder and decoder enable projection between image and latent spaces, and the diffusion processes take place in the latent space.}
\vspace{-1em}
\label{fig:overall}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/vqfinet.pdf}
\vspace{-0.5em}
\caption{The architecture of the VFI autoencoding model, VQ-FIGAN. It differs from the original VQGAN~\cite{esser2021taming} in three aspects: (i) the use of features extracted by the encoder from neighboring frames during the decoding via MaxViT-based cross attention; (ii) use of more efficient MaxViT block instead of the vanilla self-attention; (iii) frame reconstruction via locally adaptive deformable convolution. The kernel, offset, visibility and residual heads contain 3$\times$\texttt{\{conv3x3, ReLU\}}.}
\vspace{-0.5em}
\label{fig:vqfinet}
\end{figure*}

\subsection{Latent Diffusion Models}
Before presenting LDMVFI, we briefly describe the latent diffusion model (LDMs)~\cite{rombach2022high}. LDMs are built upon the denoising diffusion probabilistic models~\cite{ho2020denoising} (referred to as diffusion models in this paper), which are a class of generative models that approximates a data distribution $p(x)$ by learning the reverse conditional probability distributions of a pre-defined Markov chain. DMs comprise two processes: \textbf{forward diffusion} and \textbf{reverse diffusion}.

Formally, starting from a ``clean'' image $x_0$, we can define a Markov chain $q$ of length $T$ which gradually adds Gaussian noise to $x_0$ according to a noise schedule $\{\beta_t\}_{t=1}^T$:
\begin{equation}
    q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t \mathbf{I}).
    %\Rightarrow \quad q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}).
\end{equation}
Here the noise schedule $\{\beta_t\}_{t=1}^T$ is designed such that $q(x_T|x_0)\approx \mathcal{N}(x_T; \mathbf{0}, \mathbf{I})$, i.e. as the forward diffusion process finishes, the last state of the image becomes close to a pure Gaussian noise.

Since the reverse process $q(x_{t-1}|x_t)$ of the Markov chain above is intractable, to sample (generate) images, we can use a $\theta$-parameterized Gaussian distribution $p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2\mathbf{I})$ to approximate these reverse conditional probabilities (provided that $\beta_t$ is sufficiently small in each forward step~\cite{sohl2015deep}), where $\sigma_t^2$ can be set to a value based on $\beta_t$~\cite{ho2020denoising,li2022srdiff,rombach2022high}, and $\mu_\theta$ is realized through a neural network. Then, to perform unconditional generation, we can start from a Gaussian noise $x_T \sim \mathcal{N}(x_T;\mathbf{0},\mathbf{I})$ and sample the less noisy versions of the image from $p_\theta(x_{t-1}|x_t)$ until $x_0$. Using the parameterization in \cite{ho2020denoising}, instead of learning to predict the mean $\mu_\theta(x_t,t)$, we can train a neural network to predict the noise $\epsilon_\theta(x_t,t)$ using the objective
\begin{equation}
    \mathcal{L}_{\mathrm{DM}} = \mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T)} \big[ \norm{\epsilon - \epsilon_\theta(x_t,t)}^2 \big],
\end{equation}
where $x_t$ is sampled from the forward diffusion process and $\mathcal{U}(1,T)$ denotes uniform distribution over $\{1,2,\dots,T\}$. This corresponds to a reweighted variant of the variational lower bound on the log-likelihood $\log p_\theta(x_0)$. We provide the derivation of the loss function in Appendix~\ref{appendix:loss}.

Under the conditional generation setting, we can approximate the conditional reverse diffusion process $p_\theta(x_{t-1}|x_t,y)$, where the condition $y$
can denote the two input frames in the context of VFI. Accordingly, the noise-prediction network, $\epsilon_\theta(x_t,t,y)$, can be trained for sampling from the reverse diffusion process. 

Latent diffusion models contain an image encoder $E:x\mapsto z$ that encodes an image $x$ into a lower-dimensional latent representation $z$, and a decoder $D:z\mapsto x$ that reconstructs the image $x$. The forward and reverse diffusion processes then happen in the latent space, and the training objective for learning the reverse diffusion process becomes
\begin{equation}\small
\setlength{\abovedisplayskip}{-2pt}
    \mathcal{L}_{\mathrm{LDM}} = \mathbb{E}_{E(x_0),\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T)} \big[ \norm{\epsilon - \epsilon_\theta(z_t,t)}^2 \big].
\end{equation}

By projecting images into a compact latent space, LDMs allow the diffusion process to concentrate on the semantically important portions of the data and enable more computationally efficient sampling process.

\subsection{Autoencoding with VQ-FIGAN}\label{sec:vqfigan}
In the original form of LDMs, the autoencoding model $\{E,D\}$ is considered as a perceptual image codec. Its design purpose is to project images into efficient latent representations where high-frequency details are removed during encoding and recovered in the decoding. However, in the context of VFI, such information is likely to affect the perceived quality of the interpolated videos, so the limited reconstruction ability of the decoder can negatively impact the VFI performance. To enhance high-frequency detail recovering, we propose a VFI-specific autoencoding model: VQ-FIGAN, which is illustrated in Figure~\ref{fig:vqfinet}. While the backbone model is similar to the original VQGAN~\cite{esser2021taming} used in \cite{rombach2022high}, there are three major differences.

Firstly, we take advantage of a property of VFI problems - that the neighboring frames are available during inference, in order to design a frame-aided decoder. Specifically, given the ground-truth target frame $I^n\in\mathbb{R}^{H\times W\times 3}$, the encoder $E$ produces the latent encoding $z^n = E(I^n)$, where $z^n\in\mathbb{R}^{\frac{H}{f}\times \frac{W}{f}\times 3}$, and $f$ is a hyper-parameter (Figure~\ref{fig:vqfinet} shows the case for $f=8$). Then, the decoder $D$ outputs a reconstructed frame, $\hat{I}^n$, taking input $z^n$, as well as the feature pyramids, $\phi^0, \phi^1$, extracted by $E$ from two neighboring frames, $I^0,I^1$. During decoding, these feature pyramids are fused with the decoded features from $z^n$ at multiple layers using the MaxCA blocks, which are newly designed MaxViT~\cite{tu2022maxvit}-based Cross Attention blocks, where the query embeddings for the attention mechanism are generated using decoded features from $z^n$, and the key and value embeddings are obtained from $\phi^0,\phi^1$.

Secondly, we notice that the original VQGAN makes use of the full self-attention~\cite{vaswani2017attention} as in ViT\cite{dosovitskiy2021an}, which can be computationally heavy (quadratic complexity) and memory-intensive, especially when the image resolution is high. For more efficient inference on high-resolution (e.g. full HD) videos, we employ the recently proposed MaxViT block~\cite{tu2022maxvit} to perform self-attention. The multi-axis self-attention layer in the MaxViT block combines windowed attention and dilated grid attention to perform both local and global operations, while achieving linear complexity with respect to the input size.


\begin{figure*}[t]
\begin{minipage}[t]{0.490\textwidth}
\begin{algorithm}[H]
\caption{Training} \label{alg:train}
\small
\begin{algorithmic}[1]
    \STATE\textbf{Input}: dataset $\mathcal{D}=\{I^0_s, I^n_s, I^1_s\}_{s=1}^S$ of consecutive frame triplets, maximum diffusion step $T$, noise schedule $\{\beta_t\}_{t=1}^T$
    \STATE\textbf{Load}: pre-trained VQ-FIGAN encoder $E$
    \STATE\textbf{Initialize}: denoising U-Net $\epsilon_\theta$
    \REPEAT
      \STATE Sample $(I^0, I^n, I^1) \sim \mathcal{D}$
      \STATE Encode $z^0=E(I^0), z^n=E(I^n), z^1=E(I^1)$
      \STATE Sample $t\sim\mathcal{U}(1,T)$
      \STATE Sample $\epsilon\sim\mathcal{N}(\mathbf{0}, \mathbf{I})$
      \STATE Sample $z^n_t$ from forward diffusion step $t$ using $\epsilon,t,\beta_t$
      \STATE Take a gradient descent step on \\ \quad\quad\quad\quad\quad\quad $\nabla_\theta\norm{\epsilon - \epsilon_\theta(z^n_t,t,z^0,z^1)}^2$
    \UNTIL{converged}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.490\textwidth}
\begin{algorithm}[H]
  \caption{Inference} \label{alg:inference}
  \small
  \begin{algorithmic}[1]
    \STATE\textbf{Input}: Original frames $I^0,I^1$, noise schedule $\{\beta_t\}_{t=1}^T$, maximum diffusion step $T$
    \STATE\textbf{Load}: pre-trained denoising U-Net $\epsilon_\theta$, VQ-FIGAN encoder $E$ and decoder $D$
    \STATE Sample $z^n_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
    \STATE Encode $z^0=E(I^0), z^1=E(I^1)$ and store features $\phi^0, \phi^1$
    \FOR{$t=T,\dots, 1$}
      \STATE Predict noise $\hat{\epsilon} = \epsilon_\theta(z^n_t,t,z^0,z^1)$
      \STATE Compute the mean $\mu_\theta$ from $\hat{\epsilon}$
      \STATE Compute the standard deviation $\sigma_t$ from $\beta_t$
      \STATE Sample $z^n_{t-1}$ from $p_\theta(z^n_{t-1}|z^n_t)=\mathcal{N}(\mu_\theta, \sigma_t^2\mathbf{I})$
    \ENDFOR
    \STATE \textbf{return} $\hat{I}^n=D(z^n_0,\phi^0,\phi^1)$ as the interpolated frame
    \vspace{.01in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure*}


Thirdly, instead of having the decoder directly output the reconstructed image $\hat{I}^n$, the proposed VQ-FIGAN outputs the deformable convolution-based interpolation kernels~\cite{dai2017deformable, lee2020adacof, cheng2020video} to enhance VFI performance. Specifically, given that $H,W$ are the frame height and width, the output of the decoder network contains parameters of the locally adaptive deformable convolution kernels of size $K\times K$: $\{W^\tau, \alpha^\tau, \beta^\tau\}$ where $\tau=0,1$ indexes the input frames. Here $W\in [0,1]^{H\times W\times K\times K}$ contains the weights of the kernels, and $\alpha,\beta\in\mathbb{R}^{H\times W\times K\times K}$ are their spatial offsets (horizontal and vertical respectively). The decoder also outputs a visibility map $v\in[0,1]^{H\times W}$ to account for occlusion~\cite{jiang2018super}, and a residual map $\delta\in\mathbb{R}^{H\times W}$ to further enhance VFI performance~\cite{cheng2021multiple}. To generate the interpolated frame, firstly locally adaptive deformable convolutions are performed for each input frame $\{I^\tau\}_{\tau=0,1}$:
\begin{gather}
\footnotesize
    I^{n\tau}(h,w) = \sum_{i=1}^K\sum_{j=1}^K W^\tau_{h,w}(i,j)\cdot P^\tau_{h,w}(i,j), \label{eqn:defconv1} \\
    P^\tau_{h,w}(i,j) = I^\tau(h+\alpha^\tau_{h,w}(i,j), w+\beta^\tau_{h,w}(i,j)), \label{eqn:defconv2}
\end{gather}
in which $I^{n\tau}$ denotes the result obtained from $I^\tau$, and $P^\tau_{h,w}$ is the patch sampled from $I^\tau$ for output location $(h,w)$. These intermediate results are then combined using the visibility and residual maps:
\begin{gather}
\footnotesize
    \hat{I}^n = v\cdot I^{n0} + (1-v)\cdot I^{n1} + \delta. \label{eqn:defconv3}
\end{gather}
We adopt the separable deformable convolution implementation in \cite{cheng2020video} which exploits separability properties of kernels~\cite{rigamonti2013learning} to reduce memory requirements while maintaining VFI performance.

\textbf{Training VQ-FIGAN}. We follow the original training settings of VQGAN in \cite{esser2021taming,rombach2022high}, where the loss function consists of an LPIPS-based~\cite{zhang2018unreasonable} perceptual loss, a patch-based adversarial loss~\cite{isola2017image} and a latent regularization term based on a vector quantization (VQ) layer~\cite{van2017neural}. We refer the readers to \cite{esser2021taming} for details on training VQGANs.



\subsection{Conditional Generation with LDM}
The trained VQ-FIGAN allows us to access a compact latent space in which we perform forward diffusion by gradually adding Gaussian noise to the latent $z^n$ of the target frame $I^n$ according to a pre-defined noise schedule, and learn the reverse (denoising) process to perform conditional generation. To this end, we adopt the noise-prediction parameterization~\cite{ho2020denoising} of DMs and train a denoising U-Net by minimizing the re-weighted variational lower bound on the conditional log-likelihood $\log p_\theta(z^n|z^0,z^1)$ where $z^0,z^1$ are the latent encodings of the two input frames. 

\textbf{Training.} Specifically, the denoising U-Net $\epsilon_\theta$ takes as input the ``noisy'' latent encoding $z^n_t$ for the target frame $I^n$ (sampled from the $t$-th step in the forward diffusion process), the diffusion step $t$, as well as the conditioning latents $z^0,z^1$ for the input frames $I^0,I^1$. It is trained to predict the noise added to $z^n$ at each time step $t$ by minimizing
\begin{equation}
    \mathcal{L} = \mathbb{E}_{z^n,z^0,z^1,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t} \big[ \norm{\epsilon - \epsilon_\theta(z^n_t,t,z^0,z^1)}^2 \big],
\end{equation}
where $t\sim\mathcal{U}(1,T)$. The training procedure of $\epsilon_\theta$ is summarized in Algorithm~\ref{alg:train}, and we provide the derivation and full details of this process in Appendix~\ref{appendix:ddpm}. Intuitively, the training is performed by alternately adding a random noise to $z^n$ using the forward diffusion Markov chain (lines 7-9), and having the network $\epsilon_\theta$ predict the noise added given the step $t$, conditioning on $z^0,z^1$ (line 10).


\textbf{Inference}. To interpolate $\hat{I}^n$ from $I^0, I^1$, as shown in Algorithm~\ref{alg:inference}, we start by sampling a Gaussian noise $z^n_T$ in the latent space (line 3), and perform $T$ steps of denoising until we obtain $z^n_0$ (lines 5-10). Within each step, firstly the network $\epsilon_\theta$ predicts the noise $\hat{\epsilon}$ (line 6) which is used to calculate the mean $\mu_\theta$ of the approximated reverse conditional $p_\theta(z^n_{t-1}|z^n_t)$ (line 7), and the variance $\sigma_t^2$ is obtained based on $\beta_t$ (line 8). Then $z^n_{t-1}$ is sampled from this distribution (line 9). Finally, the decoder $D$ produces the interpolated frame (line 11) from the denoised latent $z^n_0$, with the help of feature pyramids $\phi^0,\phi^1$ extracted by the encoder $E$ from $I^0,I^1$ (line 4). See full details in Appendix~\ref{appendix:ddpm}.

\textbf{Network Architecture.} We employ the time-conditioned U-Net~\cite{ronneberger2015u} as in \cite{rombach2022high} for $\epsilon_\theta$, but with one modification: all the vanilla self-attention blocks~\cite{vaswani2017attention} are replaced with the aforementioned MaxViT blocks~\cite{tu2022maxvit} for computational efficiency. The conditioning mechanism for in U-Net is a simple concatenation of $z^n_t$ and $z^0,z^1$ at the input. See the full architecture in Appendix~\ref{appendix:unet}.



\section{Experimental setup}


\begin{table*}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccccc}
    \toprule
    & \multicolumn{4}{c}{Middlebury} & \multicolumn{4}{c}{UCF-101}& \multicolumn{4}{c}{DAVIS}& \multirow{2}[2]{*}{\makecell{RT \\ (sec)}} & \multirow{2}[2]{*}{\makecell{\#P \\ (M)}}\\
    \cmidrule(l{5pt}r{5pt}){2-5}\cmidrule(l{5pt}r{5pt}){6-9}\cmidrule(l{5pt}r{5pt}){10-13}
    &PSNR$\uparrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &PSNR$\uparrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &PSNR$\uparrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ \\
    \midrule
    BMBC & \textcolor{blue}{36.368} & 0.023 & 90.280 & 0.046 & 32.576 & 0.034 & 92.800 & 0.045 & 26.835 & 0.125 & \textcolor{blue}{75.674} & 0.185 & 0.51 & 11.0\\
    AdaCoF & 35.256 & 0.031 & 89.989 & 0.052 & 32.488 & 0.034 & 92.621 & 0.046 & 26.234 & 0.148 & 71.086 & 0.198 & 0.01 & 21.8\\
    CDFI & 36.205 & 0.022 & \textcolor{blue}{90.439} & 0.048 & 32.541 & 0.036 & 92.866 & 0.049 & 26.471 & 0.157 & 73.339 & 0.211 & 0.02 & 5.0 \\
    XVFI & 34.724 & 0.036 & 89.450 & 0.070 & 32.224 & 0.038 & 92.573 & 0.050 & 26.475 & 0.129 & 75.601 & 0.185 & 0.08 & 5.6 \\
    ABME & \textcolor{red}{37.639} & 0.027 & 90.293 & \textcolor{red}{0.040} & 32.055 & 0.058 & 91.305 & 0.069 & 26.861 & 0.151 & 74.832 & 0.209 & 0.27 & 18.1 \\
    IFRNet & \textcolor{blue}{36.368} & \textcolor{blue}{0.020} & 90.215 & 0.045 & 32.716 & \textcolor{blue}{0.032} & 92.762 & \textcolor{blue}{0.044} & \textcolor{blue}{27.313} & \textcolor{blue}{0.114} & 75.017 & \textcolor{blue}{0.170} & 0.02 & 5.0 \\
    VFIformer & 35.566 & 0.031 & 89.738 & 0.065 & 32.745 & 0.039 & 92.716 & 0.051 & 26.241 & 0.191 & 66.441 & 0.242 & 1.74 & 5.0 \\
    ST-MFNet & N/A & N/A & N/A & N/A & \textcolor{red}{33.384} & 0.036 & 92.854 & 0.049 & \textcolor{red}{28.287} & 0.125 & 75.446 & 0.181 & 0.14 & 21.0 \\
    FLAVR & N/A & N/A & N/A & N/A & \textcolor{blue}{33.224} & 0.035 & \textcolor{blue}{93.022} & 0.046 & 27.104 & 0.209 & 69.831 & 0.248 & 0.02 & 42.1 \\
    MCVD & 20.539 & 0.123 & 54.397 & 0.138 & 18.775 & 0.155 & 52.994 & 0.169 & 18.946 & 0.247 & 46.117 & 0.293 & 52.55 & 27.3 \\
    \midrule
    LDMVFI & 34.033 & \textcolor{red}{0.019} & \textcolor{red}{90.472} & \textcolor{blue}{0.044} & 32.186 & \textcolor{red}{0.026} & \textcolor{red}{93.360} & \textcolor{red}{0.035} & 25.541 & \textcolor{red}{0.107} & \textcolor{red}{75.777} & \textcolor{red}{0.153} & 8.48 & 439.0 \\
    \bottomrule
\end{tabular}
}
\vspace{0.2em}
\caption{Quantitative comparison results of LDMVFI ($f=32$) and 10 tested methods on Middlebury, UCF-101, DAVIS. Note ST-MFNet and FLAVR require four input frames cannot be evaluated on Middlebury dataset which contains frame triplets. For each column, we highlight the best result \textcolor{red}{red} and the second best in \textcolor{blue}{blue}. The last two columns show the average run time (RT) needed to to interpolate one 480p frame, and the number of parameters (\#P) in each model.}
\label{tab:quant1}
\vspace{-4mm}
\end{center}
\end{table*}

\begin{table*}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
    \toprule
    & \multicolumn{3}{c}{SNU-FILM-Easy} & \multicolumn{3}{c}{SNU-FILM-Medium}& \multicolumn{3}{c}{SNU-FILM-Hard}& \multicolumn{3}{c}{SNU-FILM-Extreme}\\
    \cmidrule(l{5pt}r{5pt}){2-4}\cmidrule(l{5pt}r{5pt}){5-7}\cmidrule(l{5pt}r{5pt}){8-10}\cmidrule(l{5pt}r{5pt}){11-13}
    &LPIPS$\downarrow$&VFIPS$\uparrow$&FloLPIPS$\downarrow$&LPIPS$\downarrow$&VFIPS$\uparrow$&FloLPIPS$\downarrow$&LPIPS$\downarrow$&VFIPS$\uparrow$&FloLPIPS$\downarrow$&LPIPS$\downarrow$&VFIPS$\uparrow$&FloLPIPS$\downarrow$ \\
    \midrule
    BMBC & 0.020 & 93.522 & 0.031 & 0.034 & 91.109 & 0.059 & 0.068 & 83.837 & \textcolor{blue}{0.118} & 0.145& 70.643 & 0.237 \\
    AdaCoF & 0.021 & 93.487 & 0.033 & 0.039 & 90.707 & 0.066 & 0.080 & 82.574 & 0.131 & 0.152 & 69.321 & 0.234\\
    CDFI & 0.020&93.615&0.031 &0.036&90.988&0.066&0.081&83.098&0.141&0.163&69.876&0.255 \\
    XVFI &0.022&93.310&0.037	&0.039&90.630&0.072	&0.075&83.051&0.138	&0.142&70.267&0.233  \\
    ABME &0.022&93.505&0.034	&0.042&90.838&0.076	&0.092&83.638&0.168	&0.182&70.008&0.300  \\
    IFRNet &\textcolor{blue}{0.019}&93.534&\textcolor{blue}{0.030}	&\textcolor{blue}{0.033}&91.220&\textcolor{blue}{0.058}	&\textcolor{blue}{0.065}&83.763&0.122	&\textcolor{blue}{0.136}&\textcolor{blue}{71.233}&\textcolor{blue}{0.229} \\
    % VFIformer &  &  & & &  &  & & & &  & & \\
    ST-MFNet &\textcolor{blue}{0.019}&\textcolor{blue}{93.813}&0.031	&0.036&\textcolor{blue}{91.148}&0.061	&0.073&\textcolor{red}{84.155}&0.123	&0.148&71.225&0.238 \\
    FLAVR &0.022&93.691&0.034	&0.049&90.371&0.077	&0.112&82.831&0.169	&0.217&69.729&0.303 \\
    MCVD & 0.199 & 59.793 & 0.230 & 0.213 & 58.156 & 0.243 & 0.250 & 54.916 & 0.292 & 0.320 & 47.006 & 0.385 \\
    \midrule
    LDMVFI &\textcolor{red}{0.014}&\textcolor{red}{94.175}&\textcolor{red}{0.024}	&\textcolor{red}{0.028}&\textcolor{red}{91.255}&\textcolor{red}{0.053}	&\textcolor{red}{0.060}&\textcolor{blue}{84.058}&\textcolor{red}{0.114}	&\textcolor{red}{0.123}&\textcolor{red}{71.730}&\textcolor{red}{0.204} \\
    \bottomrule
\end{tabular}
}
\vspace{0.2em}
\caption{Quantitative comparison results on SNU-FILM (note VFIformer is not included because the GPU goes out of memory).}
\label{tab:quant2}
\vspace{-6mm}
\end{center}
\end{table*}

\textbf{Implementation Details.} We set the downsampling factor of VQ-FIGAN to $f=32$, by repeating the \texttt{ResNetBlock+Conv3x3} layer in Figure~\ref{fig:vqfinet} twice. The size of the kernels output by the decoder is $K=5$. Regarding the diffusion processes, following~\cite{rombach2022high}, we adopt a linear noise schedule and a codebook size of 8192 for vector quantization in VQ-FIGAN. We sample from all diffusion models with the DDIM~\cite{song2021denoising} sampler for 200 steps (details provided in Appendix~\ref{appendix:ddim}. We also follow \cite{rombach2022high} to train the VQ-FIGAN using the ADAM~\cite{kingma2014adam} optimizer and the denoising U-Net using the Adam-W optimizer~\cite{loshchilov2018decoupled}, with the initial learning rates set to $10^{-5}$ and $10^{-6}$ respectively. All models were trained until convergence, which corresponds to around 70 epochs for VQ-FIGAN, and around 60 epochs for the U-Net. NVIDIA RTX 3090 GPUs were used for all training and evaluation.

\textbf{Training Dataset.} Considering the limited motion diversity and magnitudes in the commonly adopted Vimeo90k dataset~\cite{xue2019video}, we follow \cite{danier2022st} to additionally incorporate the BVI-DVC~\cite{ma2020bvi} quintuplets. The final training set consists of 64612 frame septuplets from Vimeo90k and 17600 frame quintuplets from BVI-DVC provided by \cite{danier2022st}. Since we need only a frame triplet ($I^0,I^n,I^1$) to train the model, for each septuplet or quintuplet, we only use the three frames in the center. For data augmentation, we randomly crop $256\times 256$ patches from the triplets, and perform random flipping and temporal order reversing.



\textbf{Test Datasets.} We evaluate models on the most commonly used VFI benchmarks, including Middlebury~\cite{baker2011database}, UCF-101~\cite{soomro2012ucf101}, DAVIS~\cite{perazzi2016benchmark}, and SNU-FILM~\cite{choi2020channel} datasets. These test sets cover spatial resolutions from $225\times 225$ up to $1280\times 720$, and various levels of VFI difficulties. To further assess the perceptual performance on full HD ($1920\times 1080$) content, we make use of the BVI-HFR~\cite{mackin2018study} dataset which covers various texture and motion types. This dataset is chosen because it contains relatively long videos which facilitate a rigorous user study, while other full HD video datasets~\cite{sim2021xvfi,xiph} used for VFI evaluation contain much shorter clips.

\textbf{Evaluation Methods.} As the main focus of this work is on improving the perceptual quality of interpolated content, we 
adopt a perceptual image quality metric LPIPS~\cite{zhang2018unreasonable}, and two bespoke VFI metrics: VFIPS~\cite{hou2022perceptual} and FloLPIPS~\cite{danier2022flolpips} for performance evaluation. These metrics have shown superior correlation with human judgments of VFI quality compared to commonly used quality measurements, PSNR and SSIM~\cite{wang2004image}. For completeness, we still provide benchmark results based on PSNR and SSIM, but they are limited in reflecting the perceptual quality of interpolated content~\cite{danier2022subjective} and are therefore not the focus of the paper. To measure the true perceptual performance of the VFI methods, we also conducted a psychophysical subjective experiment, in which the proposed method was compared against the state of the art (see Sec.~\ref{sec:userstudy}).



\section{Experiments}
\subsection{Quantitative Evaluation}
The proposed LDMVFI was compared against 10 recent state-of-the-art VFI methods, including BMBC~\cite{park2020bmbc}, AdaCoF~\cite{lee2020adacof}, CDFI~\cite{ding2021cdfi}, XVFI~\cite{sim2021xvfi}, ABME~\cite{park2021asymmetric}, IFRNet~\cite{kong2022ifrnet}, VFIformer~\cite{lu2022video}, ST-MFNet~\cite{danier2022st}, FLAVR~\cite{kalluri2023flavr}, and MCVD~\cite{voleti2022mcvd}. It it noted that MCVD is the only existing diffusion-based VFI method. All these models were re-trained on our training dataset for fair comparison.

\textbf{Performance.} Table~\ref{tab:quant1} shows the performance of the evaluated methods on the Middlebury, UCF-101 and DAVIS test sets in terms of three perceptual metrics and PSNR (SSIM results are included in Appendix~\ref{appendix:quantitativefull}). It is observed from the table that although the PSNR performance of LDMVFI is not comparable to other methods, LDMVFI outperforms all the other VFI methods on the perceptual metrics. The model performance on the four splits of the SNU-FILM dataset measured by the perceptual metrics are summarized in Table~\ref{tab:quant2}, which again demonstrates the superior perceptual quality of videos interpolated by LDMVFI. It is also noticed that the other diffusion-based VFI method, MCVD, does not offer satisfactory overall performance, which implies that directly applying the original formulation of diffusion models to VFI is not sufficient to get enhanced performance. This further demonstrates the effectiveness of the LDMVFI design. The full evaluation results in terms of all five metrics for all test sets are presented in Appendix~\ref{appendix:quantitativefull}.


\textbf{Complexity.} The average time taken to interpolate a 480p frame on an RTX 3090 GPU and the number of parameters of each model are presented in Table~\ref{tab:quant1}. It is observed that the inference speed of LDMVFI is much lower compared to other methods, and this is mainly due to the iterative denoising operation performed during sampling. This is a common drawback of existing diffusion models. Various methods have been proposed to speed up sampling process of diffusion models~\cite{salimans2022progressive,karras2022elucidating}, which can also be applied to LDMVFI. The number of parameters in LDMVFI is also large, and this is because we adopted (with some modifications, see Sec.~\ref{sec:vqfigan}) the existing denoising U-Net~\cite{rombach2022high} designed for generic image generation. We leave the design of a more efficient denoising U-Net and the improvement of LDMVFI sampling speed as future work. More discussion on the limitations is included in Appendix~\ref{appendix:limitations}.




\subsection{Subjective Experiment}\label{sec:userstudy}
\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/userstudy.pdf}
\end{center}
\vspace{-1.5em}
\caption{Results of the user study in terms of preference ratio. Error bar reflects the standard error over test sequences.}
\vspace{-2.5mm}
\label{fig:userstudy}
\end{figure}

To further confirm the superior perceptual quality of the videos interpolated by LDMVFI compared to the state of the art, and also to measure the temporal consistency of LDMVFI, we conducted a subjective experiment where human participants were hired to rate the quality of videos interpolated by ours and competing methods. 

\textbf{Test Videos.} We use the 22 high-quality full HD 30fps videos from the BVI-HFR~\cite{mackin2018study} dataset as source content. There are two reasons for selecting this dataset over the commonly used high resolution datasets in VFI~\cite{sim2021xvfi,xiph}. Firstly, the length of the BVI-HFR videos facilitates the user study where the participants should ideally watch longer clips rather than short segments~\cite{moss2015optimal}. Secondly, these videos cover an excellent range of video features related to motion and texture as shown by the analysis in the original paper~\cite{mackin2018study}, allowing for a more thorough benchmarking of VFI methods. To generate the test content, the 22 videos were first truncated to 5 seconds (150 frames) following \cite{moss2015optimal}. Then we used four different VFI methods to interpolate all videos to 60fps. Other than LDMVFI, the tested methods include ST-MFNet, IFRNet and BMBC, which showed the most competitive performance on the more challenging test sets (e.g. DAVIS, SNU-FILM-extreme) in the quantitative experiment. As a result, we obtain 88 test videos generated by four VFI methods.

\textbf{Test Methodology.} Following previous works~\cite{liu2017video,niklaus2018context,kalluri2023flavr}, the 2AFC approach is adopted for the subjective experiment, where the participant is asked to choose the one with better perceptual quality from a pair of two videos. Specifically, in each test session, the participant was displayed 66 pairs of videos where one video in each pair is interpolated by LDMVFI and the other is interpolated by ST-MFNet, IFRNet or BMBC. The display order of the 66 pairs, and the order of test videos within each pair are both randomized. The user is unaware of the methods used for generating the videos. Each pair is presented twice to the participant, who is then asked to provide an answer to the question ``which of the two videos is of higher quality?''. Twenty participants were hired in total. See Appendix~\ref{appendix:userstudy} for more details of the user study.


\begin{table*}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccccccccc}
    \toprule
    & \multirow{2}[2]{*}{$f$} & \multirow{2}[2]{*}{$c$} & \multirow{2}[2]{*}{\makecell{AE\\ Model}} & \multirow{2}[2]{*}{\makecell{Cond.\\ Mode}} & \multicolumn{3}{c}{Middlebury} & \multicolumn{3}{c}{UCF-101}& \multicolumn{3}{c}{DAVIS}\\
    \cmidrule(l{5pt}r{5pt}){6-8}\cmidrule(l{5pt}r{5pt}){9-11}\cmidrule(l{5pt}r{5pt}){12-14}
    & & & & &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ \\
    \midrule
    V1 & 32 & 256 & frame & concat & 0.077 & 82.233 & 0.085 & 0.063 & 87.198 & 0.067 & 0.168 & 62.876 & 0.200\\
    V2 & 32 & 256 & MaxCA+frame & concat & 0.028 & 87.579 & 0.045 & 0.032 & 91.826 & 0.041 & 0.135 & 73.697 & 0.176 \\
    \midrule
    V3 & 8 & 256 & MaxCA+kernel & concat & 0.018 & 90.330 & 0.046 & 0.028 & 92.588 & 0.036 & 0.125 & 75.372 & 0.168\\
    V4 & 16 & 256 & MaxCA+kernel & concat & 0.017 & 90.496 & 0.037 & 0.026 & 93.312 & 0.035 & 0.107 & 75.757 & 0.154 \\
    V5 & 64 & 256 & MaxCA+kernel & concat & 0.026 & 88.325 & 0.049 & 0.033 & 91.763 & 0.039 & 0.131 & 74.288 & 0.172 \\
    \midrule
    V6 & 32 & 64 & MaxCA+kernel & concat & 0.020 & 89.786 & 0.048 & 0.029 & 92.569 & 0.037 & 0.133 & 75.467 & 0.179\\
    V7 & 32 & 128 & MaxCA+kernel & concat & 0.019 & 89.968 & 0.048 & 0.028 & 92.861 & 0.036 & 0.131 & 75.586 & 0.175 \\
    \midrule
    V8 & 32 & 256 & MaxCA+kernel & MaxCA & 0.019 & 90.432 & 0.036 & 0.027 & 92.914 & 0.036 & 0.108 & 77.992 & 0.158 \\
    \midrule
    Ours & 32 & 256 & MaxCA+kernel & concat & 0.019 & 90.472 & 0.044 & 0.026 & 93.360 & 0.035 & 0.107 & 75.777 & 0.153\\ 
    \bottomrule
\end{tabular}
}
\vspace{0.2em}
\caption{Ablation experiment results, showing performance of variants of the proposed LDMVFI.}
\label{tab:ablation}
\vspace{-6mm}
\end{center}
\end{table*}


\textbf{Results.} After collecting all the user data, for each of the 22 source sequences, the ratio of users that preferred LDMVFI is calculated. Figure~\ref{fig:userstudy} reports the average preference ratios for LDMVFI and the standard error over the sequences. It can be seen that in all comparisons, LDMVFI achieved higher preference ratios. T-test analysis on the sequence-wise preference ratios shows that the advantage of LDMVFI over the other three tested methods is statistically significant at 95\% confidence level. These results further confirm the superior perceptual performance of LDMVFI.

\textbf{Visual Examples.} Figure~\ref{fig:visual} shows example blocks interpolated by LDMVFI and the competing methods, clearing demonstrating that the LDMVFI results have the best visual quality. More sample frames and videos are provided in Appendix~\ref{appendix:visual}.



\subsection{Ablation Study}
In this section we experimentally validate and study different components and hyper-parameters in LDMVFI. The ablation study results are summarized in Table~\ref{tab:ablation}, which shows evaluation results of 8 variants of the proposed model on three test sets (see the full ablation study results in Appendix~\ref{appendix:ablationfull}). These variants differ in four aspects: downsampling factor $f$, denoising U-Net's base channel size $c$, the autoencoding model's architecture and the conditioning mechanism for denoising. 

\textbf{Effectiveness of VQ-FIGAN.} To validate the effectiveness of the proposed VQ-FIGAN design, we tested two variants of the model: V1 and V2. V2 outputs the frame $\hat{I}^n$ directly instead of predicting the deformable kernels, i.e. the convolutional heads after the last \texttt{Conv3x3} layer are removed (see Figure~\ref{fig:vqfinet}). For V1, a further change is made by removing the feature-aided reconstruction process that involves $\phi^0,\phi^1$ and replacing \texttt{MaxCABlock}s in the decoder with \texttt{ResNetBlock}s. As such, there is no information from the neighbor frames during decoding. Note that V1 is similar to the original VQGAN~\cite{esser2021taming}. Table~\ref{tab:ablation} shows that without the deformable convolution-based synthesis, the performance of V2 sees an evident decrease. Furthermore, V1 shows a more severe drop in performance indicating the effectiveness of using features of neighbor frames during reconstruction.

\textbf{Downsampling Factor $f$.} Here we study how the dimension of the latent space affects the VFI performance. Specifically, to obtain V4 ($f=16$), we remove one \texttt{ResNetBlock+Conv3x3} layer and one \texttt{ResNetBlock+MaxCABlock+Upsample+Conv3x3} layer from the encoder and decoder of VQ-FIGAN respectively (see Figure~\ref{fig:vqfinet}). We repeat this process once to obtain V3 ($f=8$). To create V5 ($f=64$) we add these layers instead of removing them. Table~\ref{tab:ablation} shows that as $f$ increases from $8$ to $32$, there is generally an increasing trend in model performance in terms of the VFI-specific metrics VFIPS and FloLPIPS (except that V4 outperformed LDMVFI on Middlebury). Such improvement is more obvious on DAVIS which mainly contains challenging large motion content. However, looking at $f=64$, the general performance deteriorates significantly. The reason for this can be that to a reasonable extent, increasing $f$ allows the VQ-FIGAN decoder to make use of more information from the neighboring frames which can benefit frame interpolation, while preserving sufficient information for conditional generation in the latent space. However, if the downsampling is too aggressive (e.g. $f=64$), the information needed to perform reverse latent diffusion can be insufficient, resulting in degraded quality of latent generation. A similar trade-off between downsampling factor and model performance was also observed in \cite{rombach2022high}.


\textbf{Effect of Model Size.} We also study how the model size of the denoising U-Net affects the performance. The denoising U-Net contains multiple layers of ResNet and MaxViT blocks, where the number of feature channels is a multiple of a base channel number $c$ (detailed in Appendix~\ref{appendix:unet}). In the default LDMVFI, $c=256$. This corresponds to a total of 439.0M parameters. We experiment with $c=128, 64$ (V7,V6), which reduce the model parameters to 126.8M and 48.7M respectively. Table~\ref{tab:ablation} reflects a decreasing trend in model performance as $c$ is decreased, implying that the choice of $c=256$ is effective. However, we did not experiment with larger $c$ for memory issues.

\textbf{Conditioning Mechanism.} In LDMVFI, the mechanism for conditioning the denoising U-Net on the latents $z^0,z^1$ is concatenation, i.e., we concatenate $z^n_t, z^0, z^1$ to form the U-Net input. In \cite{rombach2022high}, an alternative cross attention-based conditioning mechanism is introduced. Based on this, We create a variant of LDMVFI (V8) where the conditioning is done using MaxCA blocks at different layers of the U-Net. As shown in Table~\ref{tab:ablation}, V8 occasionally outperformed LDMVFI, e.g. in FloLPIPS on Middlebury, in VFIPS on DAVIS. Given the limited improvement, we adopted the simpler concatenation-based conditioning for LDMVFI.



\section{Conclusion}
In this work we propose LDMVFI, the first generative modeling approach that addresses video frame interpolation as a conditional generation problem using latent diffusion models. It contains two major components: an autoencoding model that provides access to a low-dimensional latent space, and a denoising U-Net that performs reverse diffusion on latent representations. To leverage latent diffusion models for VFI, we present several innovative designs including a VFI-specific autoencoding network VQ-FIGAN that employs efficient self-attention modules and deformable kernel-based frame synthesis techniques to deliver enhanced VFI performance. LDMVFI was comprehensively evaluated on a wide range of test sets (including full HD content) using both quantitative metrics and subjective experiments. The results demonstrate its superior perceptual performance over the state of the art. 



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\onecolumn


\section{Details of LDMVFI Loss Function (Diffusion Part), Training, and Inference}\label{appendix:loss}

In this section we derive the training objective of the denoising U-Net in LDMVFI which is responsible for performing conditional generation. As mentioned in the main paper, diffusion models consist of a forward diffusion and a reverse denoising process. The forward diffusion process is defined by a Markov chain that gradually adds noise to a ``clean'' image $x_0$ using a pre-defined noise schedule $\{\beta_t\}_{t=1}^T$ in $T$ steps, with conditional probabilities
\begin{gather}
    q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t \mathbf{I}), \\
    \Rightarrow \quad q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}).
\end{gather}
Let $\alpha_t=1-\beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t\alpha_i$, according to the conditional independence property of Markov chain, one can sample from the forward diffusion process at an arbitrary time step $t$ with
\begin{equation}
    q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)\mathbf{I}).
\end{equation}
In order to sample $x_t$ from this distribution in practice, we can use a reparameterization
\begin{equation}
    x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}). \label{eqn:xt}
\end{equation}
Here the noise schedule $\{\beta_t\}_{t=1}^T$ is designed such that $\bar{\alpha}_T \approx 0$ and $q(x_T|x_0)\approx \mathcal{N}(x_T; \mathbf{0}, \mathbf{I})$. That is, as the forward diffusion process comes to an end, the last state of the image becomes close to a pure Gaussian noise.

Given the forward diffusion process, one could generate new samples by starting from pure Gaussian noise and sampling from the reverse conditionals $q(x_{t-1}|x_t)$. However, $q(x_{t-1}|x_t)$ is intractable, so one can use a $\theta$-parameterized Gaussian distribution 
\begin{gather}
    p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2\mathbf{I}), \\
    \Rightarrow \quad p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t),
\end{gather} 
to approximate the reverse Markov chain (provided that $\beta_t$ is sufficiently small in each forward step~\cite{sohl2015deep}). Here $\mu_\theta$ corresponds to a neural network, and $\sigma_t^2$ can be set to a value based on $\beta_t$~\cite{ho2020denoising,li2022srdiff,rombach2022high}. One can then derive (\cite{yang2022diffusion,ho2020denoising}) the variational lower bound on the data log-likelihood:
\begin{equation}
    \mathbb{E}_{q(x_0)}[-\log p_\theta(x_0)] \leq \mathbb{E}_{q(x_0)q(x_{1:T}|x_0)} \Big[ -\log \frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)} \Big] =: L,
\end{equation}
and training can be done by minimizing $L$. It can be shown \cite{sohl2015deep,ho2020denoising} that $L$ can be decomposed as
\begin{equation}
    L=\mathbb{E}_{q} \Big[ D_\mathrm{KL}\big(q(x_T|x_0)||p_\theta(x_T)\big) + \sum_{t=2}^T D_\mathrm{KL}\big(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)\big) - \log p_\theta(x_0|x_1) \Big]. \label{eqn:vlb}
\end{equation}
The first term in (\ref{eqn:vlb}) can be ignored because the prior $p_\theta(x_T)$ can be set to a standard normal distribution, and the last term was handled by a separate decoder in \cite{ho2020denoising}, leaving the second term as the main focus for learning the reverse diffusion process. The $q(x_{t-1}|x_t,x_0)$ in the second term is tractable and it can be derived that
\begin{equation}
    q(x_{t-1}|x_t,x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t,x_0), \tilde{\beta}_t\mathbf{I}),
\end{equation}
where 
\begin{gather}
    \tilde{\mu}_t(x_t,x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t} x_0 +\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t,\\
    \tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t.
\end{gather}
Since both $q(x_{t-1}|x_t,x_0)$ and $p_\theta(x_{t-1}|x_t)$ are Gaussian distributions, the KL-divergence in the second term in (\ref{eqn:vlb}) takes the form
\begin{equation}
    L_{t-1}=:D_\mathrm{KL}\big( q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t) \big) = \mathbb{E}_q \Big[ \frac{1}{2\sigma_t^2}\norm{\tilde{\mu}(x_t,x_0) - \mu_\theta(x_t,t)}^2 \Big]. \label{eqn:lt-1}
\end{equation}
In \cite{ho2020denoising}, it is noted that plugging (\ref{eqn:xt}) into $\tilde{\mu}(x_t,x_0)$, the latter can be written as
\begin{equation}
    \tilde{\mu}(x_t,x_0) = \frac{1}{\sqrt{\alpha_t}}\big( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon \big) \label{eqn:mutilde}.
\end{equation}
Therefore, it is proposed that instead of predicting $\mu_\theta(x_t,t)$ directly, one can predict the noise $\epsilon_\theta(x_t,t)$ then infer $\mu_\theta(x_t,t)$ with
\begin{equation}
    \mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}}\big( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t) \big). \label{eqn:mutheta}
\end{equation}
Plugging in (\ref{eqn:mutilde}) and (\ref{eqn:mutheta}) into (\ref{eqn:lt-1}), the loss then reads
\begin{align}
        L_{t-1}  & = \mathbb{E}_{x_0\sim q(x_0),\epsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})} \Big[ \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)} \norm{\epsilon - \epsilon_\theta(x_t,t)}^2\Big] + C \\
         & = \mathbb{E}_{x_0\sim q(x_0),\epsilon\sim \mathcal{N}(\mathbf{0},\mathbf{I})} \Big[ \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)} \norm{\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon,t)}^2\Big] + C, \label{eqn:ddpmloss}
\end{align}
where $C$ absorbs terms independent of $\theta$. It was observed in \cite{ho2020denoising} that setting the multiplicative term before the norm in (\ref{eqn:ddpmloss}) to 1 provides improved performance, i.e.
\begin{equation}
    L_\mathrm{DM} = \mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T)} \big[ \norm{\epsilon - \epsilon_\theta(x_t,t)}^2 \big],
\end{equation}
which corresponds to a re-weighted version of the variational lower bound. Latent diffusion models (LDMs) adopt the similar overall framework derived above, but performs the diffusion processes in a lower-dimensional latent space provided by an autoencoding model that contains an encoder $E:x\mapsto z$ and a decoder $D:z\mapsto x$. Accordingly, the noise predictor $\epsilon_\theta$ in LDMs are trained to optimize
\begin{equation}
    L_\mathrm{LDM} = \mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T)} \big[ \norm{\epsilon - \epsilon_\theta(z_t,t)}^2 \big].
\end{equation}
In LDMVFI, the noise prediction also conditions on the latent encodings $z^0,z^1$ of the two input frames $I^0,I^1$, and the loss we use becomes
\begin{align}
    \mathcal{L}  & = \mathbb{E}_{z^n,z^0,z^1,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T)} \big[ \norm{\epsilon - \epsilon_\theta(z^n_t,t,z^0,z^1)}^2 \big] \\
     & = \mathbb{E}_{z^n,z^0,z^1,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T)} \big[ \norm{\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}z^n_0 + \sqrt{1-\bar{\alpha}_t}\epsilon,t,z^0,z^1)}^2 \big].
\end{align}


\section{Details of LDMVFI Training and Inference}\label{appendix:ddpm}
The full training and inference algorithms are summarized in Algorithm~\ref{alg:trainfull} and \ref{alg:inferencefull}. In addition to  the algorithms presented in the main paper, here we provide detailed equations, which refer to those derived in Section~\ref{appendix:loss} above.

% \algrenewcommand\algorithmicindent{0.5em}%
\begin{figure*}[t]
\begin{minipage}[t]{0.490\textwidth}
\begin{algorithm}[H]
\caption{Training} \label{alg:trainfull}
\small
\begin{algorithmic}[1]
    \STATE\textbf{Input}: dataset $\mathcal{D}=\{I^0_s, I^n_s, I^1_s\}_{s=1}^S$ of consecutive frame triplets, maximum diffusion step $T$, noise schedule $\{\beta_t\}_{t=1}^T$
    \STATE\textbf{Load}: pre-trained VQ-FIGAN encoder $E$
    \STATE\textbf{Initialize}: denoising U-Net $\epsilon_\theta$
    \REPEAT
      \STATE Sample $(I^0, I^n, I^1) \sim \mathcal{D}$
      \STATE Encode $z^0=E(I^0), z^n=E(I^n), z^1=E(I^1)$
      \STATE Sample $t\sim\mathcal{U}(1,T)$
      \STATE Sample $\epsilon\sim\mathcal{N}(\mathbf{0}, \mathbf{I})$
      \STATE $z^n_t = \sqrt{\bar{\alpha}_t}z^n + \sqrt{1-\bar{\alpha}_t}\epsilon$ 
      \STATE Take a gradient descent step on \\ \quad\quad\quad\quad\quad\quad $\nabla_\theta\norm{\epsilon - \epsilon_\theta(z^n_t,t,z^0,z^1)}^2$
    \UNTIL{converged}
    \vspace{.01in}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.490\textwidth}
\begin{algorithm}[H]
  \caption{Inference} \label{alg:inferencefull}
  \small
  \begin{algorithmic}[1]
    \STATE\textbf{Input}: original frames $I^0,I^1$, noise schedule $\{\beta_t\}_{t=1}^T$, maximum diffusion step $T$
    \STATE\textbf{Load}: pre-trained denoising U-Net $\epsilon_\theta$, VQ-FIGAN encoder $E$ and decoder $D$
    \STATE Sample $z^n_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
    \STATE Encode $z^0=E(I^0), z^1=E(I^1)$ and store features $\phi^0, \phi^1$
    \FOR{$t=T,\dots, 1$}
      \STATE Predict noise $\hat{\epsilon} = \epsilon_\theta(z^n_t,t,z^0,z^1)$
      \STATE $\mu_\theta = \frac{1}{\sqrt{\alpha_t}}\big( z^n_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\epsilon} \big),\quad \sigma^2_t = \tilde{\beta}_t$
      \STATE Sample $\zeta \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
      \STATE $z^n_{t-1}=\mu_\theta + \sigma_t \zeta$
    \ENDFOR
    \STATE \textbf{return} $\hat{I}^n=D(z^n_0,\phi^0,\phi^1)$ as the interpolated frame
    % \vspace{.01in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure*}


\section{Details of DDIM Sampling Process}\label{appendix:ddim}
As stated in the main paper, in order  to sample from LDMVFI and other diffusion models, we use the DDIM~\cite{song2021denoising} sampler, which has been shown to achieve sampling quality on par with the full original sampling method (Algorithm~\ref{alg:inferencefull}), but with fewer steps. We refer the reader to the original paper for details on the derivation and design of DDIM, and present the DDIM sampling procedure for LDMVFI in Algorithm~\ref{alg:ddim}.

\begin{figure}[t]
\centering
\vspace{-1em}
\begin{minipage}[t]{\textwidth}
\begin{algorithm}[H]
  \caption{DDIM Sampling for LDMVFI} \label{alg:ddim}
  \small
  \begin{algorithmic}[1]
    \STATE\textbf{Input}: original frames $I^0,I^1$, noise schedule $\{\beta_t\}_{t=1}^T$, maximum DDIM step $\mathcal{T}$
    \STATE\textbf{Load}: pre-trained denoising U-Net $\epsilon_\theta$, VQ-FIGAN encoder $E$ and decoder $D$
    \STATE Sample $z^n_\mathcal{T} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
    \STATE Encode $z^0=E(I^0), z^1=E(I^1)$ and store features $\phi^0, \phi^1$
    \FOR{$t=\mathcal{T},\dots, 1$}
      \STATE Predict noise $\hat{\epsilon} = \epsilon_\theta(z^n_t,t,z^0,z^1)$
      \STATE $\hat{z}^n_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(z^n_t - \sqrt{1-\bar{\alpha}_t}\hat{\epsilon})$
      \STATE $ z^n_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\hat{z}^n_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \hat{\epsilon}$
    \ENDFOR
    \STATE \textbf{return} $\hat{I}^n=D(z^n_0,\phi^0,\phi^1)$ as the interpolated frame
    % \vspace{.01in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure}

\section{Denoising U-Net Architecture}\label{appendix:unet}
The high-level architecture of the denoising U-Net used in LDMVFI is shown in Figure~\ref{fig:unet}. As stated in the main paper, one modification of its original version in \cite{rombach2022high} is to replace all vanilla self-attention layers with the MaxViT blocks~\cite{tu2022maxvit}. Figure~\ref{fig:unet} demonstrates how the hyper-parameter $c$ (a base channel size mentioned in the paper) affects the overall model size. We refer the reader to the original papers~\cite{ho2020denoising,dhariwal2021diffusion,rombach2022high} and to our code for full details of the U-Net. 

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/unet.pdf}
\caption{The architecture of the denoising U-Net. The hyper-parameter $c$ is a base channel size. In each block, the $(\cdot \rightarrow \cdot)$ indicates the input and output channels of the block.}
\label{fig:unet}
\end{figure}

\section{Full Quantitative Evaluation Results}\label{appendix:quantitativefull}
The full evaluation results of LDMVFI and the compared VFI methods on all test sets (Middlebury~\cite{baker2011database}, UCF-101~\cite{soomro2012ucf101}, DAVIS~\cite{perazzi2016benchmark} and SNU-FILM~\cite{choi2020channel}) in terms of all metrics (PSNR, SSIM~\cite{wang2004image}, LPIPS~\cite{zhang2018unreasonable}, VFIPS~\cite{hou2022perceptual} and FloLPIPS~\cite{danier2022flolpips}) are summarized in Tables~\ref{tab:quant1full}-\ref{tab:quant2full2}.


\section{Full Ablation Study Results}\label{appendix:ablationfull}
In the main paper, due to space limit we presented the ablation experiment on three test sets. Here we also include the evaluation results of all 8 variants (mentioned in the main paper) on the SNU-FILM dataset. These are summarized in Tables~\ref{tab:ablationfull}.

\section{Details of Subjective Experiment}\label{appendix:userstudy}
The user study and the use of human data have undergone an internal ethics review and have been approved by the Institutional Review Board.

The subjective experiment was conducted in a lab-based environment. The monitor used to display videos was a BENQ XL2720Z (59.8$\times$33.6cm screen size). The spatial resolution of the display was set to $1920\times 1080$ and the frame rate was set to $60$Hz. The viewing distance of the participants was 1m (approximately three times screen height~\cite{itu2002500}).

In the main paper, we state that t-tests are performed between the sequence-wise preference ratios of the proposed method and the three tested baselines. Here we report the $p$ values for the t-tests in Table~\ref{tab:ttest}.

\begin{table}[H]
\footnotesize
\begin{center}
\begin{tabular}{l|c}
    \toprule
    Comparison & $p$ value \\
    \midrule
    Ours vs BMBC & $3.8\times10^{-9}$ \\
    Ours vs IFRNet & $1.6\times10^{-3}$ \\
    Ours vs ST-MFNet & $1.7\times10^{-2}$\\
    \bottomrule
\end{tabular}
\vspace{0.6em}
\caption{The results of t-test analysis on the user study results.}
\label{tab:ttest}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccccccccccc}
    \toprule
    & \multicolumn{5}{c}{Middlebury} & \multicolumn{5}{c}{UCF-101} &  \multicolumn{5}{c}{DAVIS} \\
    \cmidrule(l{5pt}r{5pt}){2-6}\cmidrule(l{5pt}r{5pt}){7-11}\cmidrule(l{5pt}r{5pt}){12-16}
     & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  & VFIPS$\uparrow$  & FloLPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  & VFIPS$\uparrow$  & FloLPIPS$\downarrow$  & PSNR$\uparrow$  & SSIM$\uparrow$  & LPIPS$\downarrow$  & VFIPS$\uparrow$  & FloLPIPS$\downarrow$ \\
    \midrule
    BMBC & \textcolor{blue}{36.368} & 0.982 & 0.023 & 90.280 & 0.046 & 32.576 & 0.968 & 0.034 & 92.800 & 0.045 & 26.835 & 0.869 & 0.125 & \textcolor{blue}{75.674} & 0.185 \\
    AdaCoF & 35.256 & 0.975 & 0.031 & 89.989 & 0.052 & 32.488 & 0.968 & 0.034 & 92.621 & 0.046 & 26.234 & 0.850 & 0.148 & 71.086 & 0.198 \\
    CDFI & 36.205 & 0.981 & 0.022 & \textcolor{blue}{90.439} & 0.048 & 32.541 & 0.968 & 0.036 & 92.866 & 0.049 & 26.471 & 0.857 & 0.157 & 73.339 & 0.211  \\
    XVFI & 34.724 & 0.975 & 0.036 & 89.450 & 0.070 & 32.224 & 0.966 & 0.038 & 92.573 & 0.050 & 26.475 & 0.861 & 0.129 & 75.601 & 0.185  \\
    ABME & \textcolor{red}{37.639} & \textcolor{red}{0.986} & 0.027 & 90.293 & \textcolor{red}{0.040} & 32.055 & 0.967 & 0.058 & 91.305 & 0.069 & 26.861 & 0.865 & 0.151 & 74.832 & 0.209 \\
    IFRNet & \textcolor{blue}{36.368} & \textcolor{blue}{0.983} & \textcolor{blue}{0.020} & 90.215 & 0.045 & 32.716 & \textcolor{blue}{0.969} & \textcolor{blue}{0.032} & 92.762 & \textcolor{blue}{0.044} & \textcolor{blue}{27.313} & \textcolor{blue}{0.877} & \textcolor{blue}{0.114} & 75.017 & \textcolor{blue}{0.170} \\
    VFIformer & 35.566 & 0.977 & 0.031 & 89.738 & 0.065 & 32.745 & 0.968 & 0.039 & 92.716 & 0.051 & 26.241 & 0.850 & 0.191 & 66.441 & 0.242 \\
    ST-MFNet & N/A & N/A & N/A & N/A & N/A & \textcolor{red}{33.384} & \textcolor{red}{0.970} & 0.036 & 92.854 & 0.049 & \textcolor{red}{28.287} & \textcolor{red}{0.895} & 0.125 & 75.446 & 0.181 \\
    FLAVR & N/A & N/A & N/A & N/A & N/A & \textcolor{blue}{33.224} & \textcolor{blue}{0.969} & 0.035 & \textcolor{blue}{93.022} & 0.046 & 27.104 & 0.862 & 0.209 & 69.831 & 0.248 \\
    MCVD & 20.539 & 0.820 & 0.123 & 54.397 & 0.138 & 18.775 & 0.710 & 0.155 & 52.994 & 0.169 & 18.946 & 0.705 & 0.247 & 46.117 & 0.293 \\
    \midrule
    LDMVFI & 34.033 & 0.971 & \textcolor{red}{0.019} & \textcolor{red}{90.472} & \textcolor{blue}{0.044} & 32.186 & 0.963 & \textcolor{red}{0.026} & \textcolor{red}{93.360} & \textcolor{red}{0.035} & 25.541 & 0.833 & \textcolor{red}{0.107} & \textcolor{red}{75.777} & \textcolor{red}{0.153} \\
    \bottomrule
\end{tabular}
}
\vspace{0.1em}
\caption{Quantitative comparison results of LDMVFI ($f=32$) and 10 tested methods. Note ST-MFNet and FLAVR require four input frames so cannot be evaluated on Middlebury that contains frame triplets. In each column we color the \textcolor{red}{best} result and the \textcolor{blue}{second best}.}
\vspace{-1em}
\label{tab:quant1full}
\end{center}
\end{table}


\begin{table}[t]
\begin{center}\footnotesize
\begin{tabular}{lcccccccccc}
    \toprule
    & \multicolumn{5}{c}{SNU-FILM-Easy} & \multicolumn{5}{c}{SNU-FILM-Medium}\\
    \cmidrule(l{5pt}r{5pt}){2-6}\cmidrule(l{5pt}r{5pt}){7-11}
     & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & VFIPS$\uparrow$ & FloLPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & VFIPS$\uparrow$ & FloLPIPS$\downarrow$  \\
    \midrule
    BMBC & 39.809 & \textcolor{blue}{0.990} & 0.020 & 93.522 & 0.031 & 35.437 & 0.978 & 0.034 & 91.109 & 0.059 \\
    AdaCoF & 39.632 & \textcolor{blue}{0.990} & 0.021 & 93.487 & 0.033 & 34.919 & 0.975 & 0.039 & 90.707 & 0.066 \\
    CDFI & 39.881 & \textcolor{blue}{0.990} & 0.020 & 93.615 & 0.031 & 35.224 & 0.977 & 0.036 & 90.988 & 0.066 \\
    XVFI  & 38.903 & 0.989 & 0.022 & 93.310 & 0.037	& 34.552 & 0.975 & 0.039 & 90.630 & 0.072 \\
    ABME  & 39.697 & \textcolor{blue}{0.990} & 0.022 & 93.505 & 0.034	& 35.280 & 0.977 & 0.042 & 90.838 & 0.076 \\
    IFRNet  & 39.881 & \textcolor{blue}{0.990} & \textcolor{blue}{0.019} & 93.534 & \textcolor{blue}{0.030} & 35.668 & 0.979 &\textcolor{blue}{0.033} & 91.220 & \textcolor{blue}{0.058} \\
    % VFIformer &  &  &  &  &  &  &  &  &  &   &  &  \\
    ST-MFNet & \textcolor{red}{40.775} & \textcolor{red}{0.992} & \textcolor{blue}{0.019} & \textcolor{blue}{93.813} & 0.031 & \textcolor{red}{37.111} & \textcolor{red}{0.985} & 0.036 & \textcolor{blue}{91.148} & 0.061	 \\
    FLAVR  & \textcolor{blue}{40.161} & \textcolor{blue}{0.990} & 0.022 & 93.691 & 0.034 & \textcolor{blue}{36.020} & \textcolor{blue}{0.979} & 0.049 & 90.371 & 0.077 \\
    MCVD & 22.201 & 0.828 & 0.199 & 59.793 & 0.230 & 21.488 & 0.812 & 0.213 & 58.156 & 0.243 \\
    \midrule
    LDMVFI  & 38.674 & 0.987 &\textcolor{red}{0.014} & \textcolor{red}{94.175} & \textcolor{red}{0.024} & 33.996 & 0.970 & \textcolor{red}{0.028} & \textcolor{red}{91.255} & \textcolor{red}{0.053}	 \\
    \bottomrule
\end{tabular}
\begin{tabular}{lcccccccccc}
    \toprule
    & \multicolumn{5}{c}{SNU-FILM-Hard} & \multicolumn{5}{c}{SNU-FILM-Extreme}\\
    \cmidrule(l{5pt}r{5pt}){2-6}\cmidrule(l{5pt}r{5pt}){7-11}
     & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & VFIPS$\uparrow$ & FloLPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & VFIPS$\uparrow$ & FloLPIPS$\downarrow$  \\
    \midrule
    BMBC & 29.942 & 0.933 & 0.068 & 83.837 & \textcolor{blue}{0.118} & 24.715 & 0.856 & 0.145 &  70.643 & 0.237 \\
    AdaCoF & 29.477 & 0.925 & 0.080 & 82.574 & 0.131 & 24.650 & 0.851 & 0.152 & 69.321 & 0.234\\
    CDFI & 29.660 & 0.929 & 0.081 & 83.098 & 0.141 & 24.645 & 0.854 & 0.163 & 69.876 & 0.255 \\
    XVFI & 29.364 & 0.928 & 0.075 & 83.051 & 0.138 & 24.545 & 0.853 & 0.142 & 70.267 & 0.233  \\
    ABME & 29.643 & 0.929 & 0.092 & 83.638 & 0.168 & 24.541 & 0.853 & 0.182 & 70.008 & 0.300  \\
    IFRNet & 30.143 & 0.935 & \textcolor{blue}{0.065} & 83.763 & 0.122 & 24.954 & 0.859 & \textcolor{blue}{0.136} & \textcolor{blue}{71.233} & \textcolor{blue}{0.229} \\
    % VFIformer &  &  &  &  &  &  &  &  &  &   &  &  \\
    ST-MFNet & \textcolor{red}{31.698} & \textcolor{red}{0.951} & 0.073 & \textcolor{red}{84.155} & 0.123	& \textcolor{red}{25.810} & \textcolor{red}{0.874} & 0.148 & 71.225 & 0.238 \\
    FLAVR & \textcolor{blue}{30.577} & \textcolor{blue}{0.938} & 0.112 & 82.831 & 0.169 & \textcolor{blue}{25.206} & \textcolor{blue}{0.861} & 0.217 & 69.729 & 0.303 \\
    MCVD & 20.314 & 0.766 & 0.250 & 54.916 & 0.292 & 18.464 & 0.694 & 0.320 & 47.006 & 0.385 \\
    \midrule
    LDMVFI & 28.547 & 0.917 & \textcolor{red}{0.060} & \textcolor{blue}{84.058} & \textcolor{red}{0.114} & 23.934 & 0.837 & \textcolor{red}{0.123} & \textcolor{red}{71.730} & \textcolor{red}{0.204} \\
    \bottomrule
\end{tabular}
\vspace{0.6em}
\caption{Quantitative comparison results on SNU-FILM (note VFIformer is not included because the GPU goes OOM).}
\vspace{-1em}
\label{tab:quant2full2}
\end{center}
\end{table}


\begin{table}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
    \toprule
    & \multicolumn{3}{c}{SNU-FILM-Easy} & \multicolumn{3}{c}{SNU-FILM-Medium} & \multicolumn{3}{c}{SNU-FILM-Hard}& \multicolumn{3}{c}{SNU-FILM-Extreme}\\ \cmidrule(l{5pt}r{5pt}){2-4}\cmidrule(l{5pt}r{5pt}){5-7}\cmidrule(l{5pt}r{5pt}){8-10}\cmidrule(l{5pt}r{5pt}){11-13}
    & LPIPS$\downarrow$ & VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ &LPIPS$\downarrow$ &VFIPS$\uparrow$ &FloLPIPS$\downarrow$ \\
    \midrule
    V1 & 0.054 & 85.830 & 0.056	& 0.068 & 83.659 & 0.082 & 0.104 & 77.620 & 0.142 & 0.172 & 66.182 & 0.243 \\
    V2 & 0.020 & 92.370 & 0.028	& 0.035 & 89.628 & 0.058 & 0.076 & 82.295 & 0.126 & 0.150 & 69.595 & 0.233 \\
    \midrule
    V3 & 0.016 & 93.430 & 0.026	& 0.030 & 90.841 & 0.052 & 0.068 & 83.474 & 0.119 & 0.144 & 70.296 & 0.227 \\
    V4 & 0.014 & 93.883 & 0.024 & 0.027 & 91.089 & 0.052 & 0.059 & 84.012 & 0.112 & 0.122 & 71.606 & 0.204 \\
    V5 & 0.019 & 93.023 & 0.029 & 0.034 & 89.135 & 0.059 & 0.072 & 83.012 & 0.123 & 0.149 & 69.523 & 0.234 \\
    \midrule
    V6 & 0.016 & 93.261 & 0.026	& 0.032 & 90.338 & 0.060 & 0.076 & 82.277 & 0.132 & 0.157 & 68.539 & 0.243 \\
    V7 & 0.016 & 93.398 & 0.024 & 0.031 & 90.642 & 0.054 & 0.073 & 82.921 & 0.127 & 0.155 & 69.181 & 0.241 \\
    \midrule
    V8 & 0.015 & 93.577 & 0.024 & 0.030 & 90.783 & 0.052 & 0.063 & 83.624 & 0.115 & 0.125 & 70.983 & 0.216 \\
    \midrule
    Ours & 0.014 & 94.175 & 0.024 & 0.028 & 91.255 & 0.053 & 0.060 & 84.058 & 0.114 & 0.123 & 71.730 & 0.204\\ 
    \bottomrule
\end{tabular}
}
\vspace{1mm}
\caption{Ablation experiment results on SNU-FILM. Full details of the variants (V1-8) can be found in the main paper.}
\label{tab:ablationfull}
\vspace{-1em}
\end{center}
\end{table}





\section{Limitations and Societal Impact}\label{appendix:limitations}
\textbf{Limitations.} Firstly, as discussed in the main paper, the proposed LDMVFI shows a much slower inference speed compared to the other state-of-the-art methods. This is a common drawback of diffusion models~\cite{ho2020denoising,rombach2022high} mainly due to the iterative reverse process during generation. Various  techniques~\cite{salimans2022progressive,karras2022elucidating} have been proposed to speed up the sampling process and these can also be applied to LDMVFI. Secondly, the number of model parameters in LDMVFI is also larger than other methods. The main component of LDMVFI that accounts for the large model size is the denoising U-Net, which was developed in previous work~\cite{ho2020denoising,dhariwal2021diffusion,rombach2022high} and modified in here by replacing the self-attention layers with MaxViT blocks~\cite{tu2022maxvit}. To reduce the model size, techniques such as knowledge distillation~\cite{hinton2015distilling} and model compression~\cite{modelcompression} can be used. The exploration of these techniques, as well as the accelerated diffusion sampling methods in the context of VFI, remain for future work.

\textbf{Potential Negative Societal Impact.} Firstly, in general, generative models for images and videos can be used to generate inappropriately manipulated content or in unethical ways (e.g. ``deep fake'' generation)~\cite{denton2021ethical}. Secondly, the two-stage training strategy, large model size and slow inference speed of LDMVFI mean that large-scale training and evaluation processes can consume significant amounts of energy, leading to increased carbon footprint~\cite{lacoste2019quantifying}. We refer the readers to \cite{denton2021ethical} and \cite{lacoste2019quantifying} for more detailed discussions on these matters.

\section{Attribution of Assets: Code and Data}\label{appendix:attribution}
In this section, we summarize the sources and licenses of all the datasets and code used in the work. The attribution of datasets and code are shown in Tables~\ref{tab:license1} and \ref{tab:license2} respectively. For MCVD~\cite{voleti2022mcvd}, we used the \texttt{smmnist\_DDPM\_big5.yml} configuration\footnote{\url{https://github.com/voletiv/mcvd-pytorch/blob/master/configs/smmnist_DDPM_big5.yml}}, setting both the numbers of previous and future frames to 1, and the number of interpolated frames also to 1.
% \vfill

\section{Additional Qualitative Examples}\label{appendix:visual}
We include more visual examples of frames interpolated by LDMVFI and other competing methods in Figures~\ref{fig:qualitative2} and \ref{fig:qualitative}.

\vfill

% ===============================================================
\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{\small\begin{tabular}{m{3cm}|m{9cm}|m{4cm}}
\toprule
\textbf{Dataset}  & \textbf{Dataset URL} &\textbf{License / Terms of Use} \\ 
\midrule
Vimeo-90k~\cite{xue2019video} & \url{http://toflow.csail.mit.edu} &MIT license. \\
\midrule
BVI-DVC~\cite{ma2020bvi} & \url{https://fan-aaron-zhang.github.io/BVI-DVC/} (original videos); \url{https://github.com/danielism97/ST-MFNet} (quintuplets) & All sequences are allowed for academic research.\\
\midrule
UCF101~\cite{soomro2012ucf101} & \url{https://www.crcv.ucf.edu/research/data-sets/ucf101/} & No explicit license terms, but compiled and made available for research use by the University of Central Florida. \\
\midrule
DAVIS~\cite{perazzi2016benchmark} & \url{https://davischallenge.org} & BSD license. \\
\midrule
SNU-FILM~\cite{choi2020channel} & \url{https://myungsub.github.io/CAIN/} & MIT license .\\
\midrule
Middlebury~\cite{baker2011database} & \url{https://vision.middlebury.edu/flow/data/} & All sequences are available for research use.\\
\midrule
BVI-HFR~\cite{mackin2018study} & \url{https://fan-aaron-zhang.github.io/BVI-HFR/} & Non-Commercial Government Licence for public sector information. \\
\bottomrule
\end{tabular}}
\vspace{0.6em}
\caption{License information for the datasets used in this work.}
% \vspace{5em}
\label{tab:license1}
\end{table}
% ==============================================================

% ===============================================================
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{\small\begin{tabular}{l|p{9cm}|p{4cm}}
\toprule
\textbf{Method}  & \textbf{Source code URL} & \textbf{License / Teams of Use} \\ 
\midrule
BMBC~\cite{park2020bmbc} & \url{https://github.com/JunHeum/BMBC} & MIT license. \\
\midrule
AdaCoF~\cite{lee2020adacof} & \url{https://github.com/HyeongminLEE/AdaCoF-pytorch} & MIT license. \\
\midrule
CDFI~\cite{ding2021cdfi} & \url{https://github.com/tding1/CDFI} & Available for research use. \\
\midrule
XVFI~\cite{sim2021xvfi} & \url{https://github.com/JihyongOh/XVFI} & Research and education only. \\
\midrule
ABME~\cite{park2021asymmetric} & \url{https://github.com/JunHeum/ABME} & MIT license. \\
\midrule
IFRNet~\cite{kong2022ifrnet} & \url{https://github.com/ltkong218/IFRNet} & MIT license. \\
\midrule
VFIformer~\cite{lu2022video} & \url{https://github.com/dvlab-research/VFIformer} & Available for research use. \\
\midrule
ST-MFNet~\cite{danier2022st} & \url{https://github.com/danielism97/ST-MFNet} & MIT license. \\
\midrule
FLAVR~\cite{kalluri2023flavr} & \url{https://github.com/tarun005/FLAVR} & Apache-2.0 license. \\
\bottomrule
MCVD~\cite{voleti2022mcvd} & \url{https://github.com/voletiv/mcvd-pytorch} & MIT license. \\
\bottomrule
\end{tabular}}
\vspace{0.6em}

\caption{License information for the code assets used in this work.}
\label{tab:license2}
\end{table}

% ==============================================================


\begin{figure}[ht]
    \centering
    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_8.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_8.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_8.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_8.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_8.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_8.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_8.pdf}}\\

    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_10.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_10.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_10.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_10.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_10.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_10.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_10.pdf}}\\

    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_11.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_11.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_11.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_11.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_11.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_11.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_11.pdf}}\\

    \setcounter{subfigure}{0}
    \subfloat[Overlaid inputs] {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_12.pdf}}\;\!\!
	\subfloat[GT] {\includegraphics[width=0.130\linewidth]{figures/crops/gt_12.pdf}}\;\!\!
	\subfloat[BMBC] {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_12.pdf}}\;\!\!
    \subfloat[VFIformer] {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_12.pdf}}\;\!\!
    \subfloat[IFRNet] {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_12.pdf}}\;\!\!
    \subfloat[ST-MFNet] {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_12.pdf}}\;\!\!
    \subfloat[LDMVFI (ours)] {\includegraphics[width=0.130\linewidth]{figures/crops/ours_12.pdf}}\\
    \vspace{1em}
    \caption{More visual interpolation examples.}
	\label{fig:qualitative2}
\end{figure}


\begin{figure}[t]
    \centering
    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_2.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_2.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_2.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_2.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_2.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_2.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_2.pdf}}\\
    
    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_3.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_3.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_3.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_3.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_3.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_3.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_3.pdf}}\\

    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_4.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_4.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_4.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_4.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_4.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_4.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_4.pdf}}\\

    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_5.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_5.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_5.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_5.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_5.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_5.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_5.pdf}}\\

    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_6.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_6.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_6.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_6.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_6.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_6.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_6.pdf}}\\

    \subfloat {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_7.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/gt_7.pdf}}\;\!\!
	\subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_7.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_7.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_7.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_7.pdf}}\;\!\!
    \subfloat {\includegraphics[width=0.130\linewidth]{figures/crops/ours_7.pdf}}\\

    \setcounter{subfigure}{0}
    \subfloat[Overlaid inputs] {\includegraphics[width=0.195\linewidth]{figures/crops/overlay_9.pdf}}\;\!\!
	\subfloat[GT] {\includegraphics[width=0.130\linewidth]{figures/crops/gt_9.pdf}}\;\!\!
	\subfloat[BMBC] {\includegraphics[width=0.130\linewidth]{figures/crops/bmbc_9.pdf}}\;\!\!
    \subfloat[VFIformer] {\includegraphics[width=0.130\linewidth]{figures/crops/vfiformer_9.pdf}}\;\!\!
    \subfloat[IFRNet] {\includegraphics[width=0.130\linewidth]{figures/crops/ifrnet_9.pdf}}\;\!\!
    \subfloat[ST-MFNet] {\includegraphics[width=0.130\linewidth]{figures/crops/stmfnet_9.pdf}}\;\!\!
    \subfloat[LDMVFI (ours)] {\includegraphics[width=0.130\linewidth]{figures/crops/ours_9.pdf}}\\
    \vspace{1em}
    \caption{Additional visual interpolation examples.}
	\label{fig:qualitative}
\end{figure}


\end{document}