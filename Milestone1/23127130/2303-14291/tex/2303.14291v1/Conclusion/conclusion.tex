\chapter{Conclusion}

\ifpdf
    \graphicspath{{Conclusion/Figs/Raster/}{Conclusion/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Conclusion/Figs/Vector/}{Conclusion/Figs/}}
\fi


\section{Summary of Contributions}

The goals of this thesis were first, to examine new use-cases for the existing \textsc{gp} framework in modelling scientific data and second, to extend current \textsc{gp}
methodology and software implementations to tackle a broader range of scientific modelling problems. Below, the chapter-by-chapter contributions are summarised with particular attention given to derivative works that have used or built on ideas and results introduced in the papers authored as part of the thesis.


\begin{itemize}
    \item In Chapter 3, \textsc{gp}s are used to infer the latent lightcurves of the Seyfert galaxy Mrk 335. Bayesian model selection is used to quantitatively compare choices of the \textsc{gp} kernel and the efficacy of the \textsc{gp} model is assessed via simulation. \textsc{gp} modelling of the observational data from Mrk 335 together with cross-correlation analysis provides weak evidence for a lag feature at high frequency with potential implications for the development of future accretion disk theories. Bayesian model selection over kernels is also employed in \cite{2022_Covino}, where the authors use \textsc{gp}s to detect periodicities in the quasar SDSS J025214.67-002813.7. The authors suggest that the rational quadratic kernel may outperform the squared exponential in their application due to its ability to pick up meaningful correlations for very long lags. 
    
    In \cite{2022_Lewin} the authors again use Bayesian model selection over kernels for X-ray reverberation mapping of the Seyfert galaxy Ark-564. The authors observe, similarly to \cite{2021_Mrk}, that as a component of time lag modelling, the rational quadratic and Matérn kernels outperform the squared exponential kernel in the marginal likelihood metric. Through their analysis, the authors constrain the fundamental black hole parameters and make a case for the use of \textsc{gp} models in the development of future theoretical reverberation models. 
    
    Lastly, in \cite{2022_Cackett} the authors conduct a frequently-resolved lag analysis of the AGN lightcurves across the full UV/optical range, obtaining results that are consistent with \cite{2021_Mrk} for the Seyfert galaxy NGC 5548. This work highlights that \textsc{gp} approaches are already being used as a point of comparison for alternative statistical inference methods.
    
    \item In Chapter 4, the software library GAUCHE  is introduced which extends the \textsc{gp} framework to operate on molecular and chemical reaction representations (\href{https://leojklarner.github.io/gauche/}{https://leojklarner.github.io/gauche/}). Specifically, the library provides support for graph, string and bit vector representations of molecules and reactions. GAUCHE subsumes an earlier TensorFlow version of the library, FlowMO by the same authors, \citep{2020_flowmo}, available at \href{https://github.com/Ryan-Rhys/FlowMO}{https://github.com/Ryan-Rhys/FlowMO}, which contains the first open-source implementation of the \textsc{gp}-Tanimoto model combination used in Chapter 5 as well as a wrapper around the GraKel library \citep{2020_GraKel} which, for the first time, makes a wide range of graph kernels available to be used in conjunction with \textsc{gp}s in a modern machine learning framework supporting GPU acceleration.
    
    \cite{2021_Broccard} performs an extensive analysis of the introduced \textsc{gp}-Tanimoto model in the context of \textsc{gp} regression and \textsc{bo}. The author proves the positive-definiteness of the generalised Tanimoto kernel and conducts experiments on the photoswitch dataset introduced in Chapter 5, showing that the \textsc{gp}-Tanimoto model outperforms popular kernels such as the squared exponential and Matérn kernels, likely due to the fact that it possesses only a single hyperparameter (the signal amplitude) and hence optimisation is more stable.
    
    In \cite{2021_Deshwal}, the authors leverage molecular kernels and data processing functionality in \cite{2020_flowmo} to enhance molecule generation architectures featuring deep generative models.
    
    In \cite{2022_Rankovic}, the authors use GAUCHE to perform \textsc{bo} for reaction screening, paying particular attention to optimisation over the space of reaction additives.
    
    \item In Chapter 5, the tools made available in GAUCHE, namely the \textsc{gp}-Tanimoto model used as a component of a \textsc{mogp}, are put to use in discovering novel photoswitch molecules. The model is trained on a curated dataset of photoswitch molecules and is subsequently used to screen a set of candidates satisfying a pre-specified set of performance criteria related to photoswitch use in light-emitting diodes (LEDs). 
    
    In \cite{2021_Mukadum}, the authors introduce an approach for active learning to prioritise molecules for comparatively expensive DFT calculations. The authors make use of similar visualisation techniques for photoswitch space and arrive at similar conclusions regarding the appropriateness of different molecular representations for wavelength prediction.
    
    \item In Chapter 6, the \textsc{bo} framework is extended to incorporate penalisation of experimental measurement noise (heteroscedastic noise). The experiments show that the methodology requires a larger initialisation set relative to standard \textsc{bo} in order to fully enable the desired noise penalisation.
    
    In \cite{2021_Makarova}, the authors introduce a complementary model to \cite{2021_Griffiths} which operates by repeating measurements at the same input location in order to obtain noise estimates. The model of \cite{2021_Makarova} is likely to be useful in scenarios where the cost of repeating noisy measurements is cheap relative to querying a new input location whereas the model introduced in \cite{2021_Griffiths} is likely to be useful if repeat measurements are as expensive as measurements at a different location.
\end{itemize}

\section{Future Work}



There is a long history of \textsc{gp}s being used to model scientific data with the first recorded instance being astronomer T.N. Thiele using \textsc{gp}s for time series analysis in 1880 \citep{1981_Lauritzen}. However, unlike deep learning, some of the more recent advances in \textsc{gp} and \textsc{bo} machinery \citep{2022_Garnett} have yet to be ported to the natural sciences. As an example, although the strengths of \textsc{gp} modelling for astrophysical time lag analysis were realised as early as 1992, there was no knowledge of an automated mechanism for learning the kernel hyperparameters through marginal likelihood optimisation and so it was necessary to specify hyperparameters by hand, significantly complicating the fitting procedure \citep{1992_Rybicki}. Similarly, there is anecdotal evidence that cheminformaticians have been known to abandon \textsc{gp} models due to Cholesky decomposition errors when training \textsc{gp}s on molecular representations with standard kernels defined over continuous input spaces. Although the range of potential applications for \textsc{gp}s in the sciences is vast, ranging from genetics  \citep{2011_Kalaitzis, 2021_Bintayyash} to protein modelling \citep{2022_Hie}, some avenues of future work relevant for astrophysics and chemistry, the applications considered in this thesis, are given below together with suggestions for research into adaptations of \textsc{gp} and \textsc{bo} machinery that may be particularly relevant for scientists.

\subsection{GPs in Astrophysics}

Unlike other areas of the natural sciences, there is an abundance of astrophysical data consisting of one-dimensional time series where the noise process is well understood. Much of the knowledge of the physical process can hence be incorporated into the \textsc{gp} model in order to improve the resulting fit, for example, by specifying known observation noise prior to optimisation of the kernel hyperparameters. As such, performing inference over latent functions for which only irregularly-sampled observations are available, is a task very well-suited to \textsc{gp} models. In the future, given the level of structure present in the data, it may be appropriate to use more sophisticated \textsc{gp} models to identity and capture new properites of the data. Example applications include modelling non-stationarity with deep \textsc{gp}s \citep{2013_Damianou} or transformed \textsc{gp}s \citep{2020_Maronas}, as well as the use of spectral mixture kernels \citep{2013_Wilson} to detect periodicities.

\subsection{GPs in Chemistry}

There is growing excitement in the chemistry community about machine learning approaches for predicting molecular properties. The 1981 Nobel Laureate in Chemistry, Roald Hoffmann, even went so far as to speculate the following about the future of molecular machine learning and quantum chemistry:

\begin{displayquote}
In view of the progress of machine learning and neural networks, it is likely that these two tools will compete efficiently - in quality, in cost - with the best quantum chemistry tools in the near future. Then the community of number-oriented quantum chemists will face a dramatic problem. Will their function be relegated to providing reliable training data sets for the production of improved neural networks? Or will they follow the destiny of super-market cashiers these days and that of taxi drivers tomorrow?
\end{displayquote}

The following Twitter counter-commentary by Max Welling, however, sheds light on some of the limitations and opportunities for machine learning approaches in molecular property prediction:

\begin{displayquote}
Interesting statement by Roald Hoffmann. While this is what happened to CV and NLP, it may not happen like that to chemistry. ML will become a very powerful tool for the computational chemist, but data is expensive and the microscopic equations are known! So inductive bias will remain key! \footnote{Abbreviations used in the original tweet due to the Twitter character limit have been expanded for clarity.}
\end{displayquote}

\noindent In particular, Welling's point about the expense of real-world data remains a valid concern. From a modelling standpoint, the ability to fit small data is a point in favour of the use of \textsc{gp}s in place of deep learning architectures. In terms of generalising beyond the domain of the training data, however, inductive bias does remain a key concern for both deep learning and \textsc{gp} models. While \textsc{gp}s have lagged behind deep learning in terms of incorporating inductive biases, there has been recent progress in learning invariances through the marginal likelihood \citep{2018_Wilk, 2021_Verma} as well as considering causal mechanisms \citep{2020_Aglietti, 2021_Aglietti}. While it remains to be seen how successful deep learning or \textsc{gp} approaches will be at encoding the inductive biases present in the microscopic equations of chemistry, the incorporation of inductive biases and causal mechanisms into machine learning models will no doubt be useful across many scientific applications \citep{2022_Kalinin}.

\subsection{GPs in Scientific Experiments}

Perhaps one of the most challenging obstacles to the adoption of machine learning in the laboratory is convincing an experimentalist to use it. In particular, when focussing on new areas of chemical or materials space for example, it can be challenging to specify an appropriate \textsc{bo} scheme upfront. The ability to do so would entail some knowledge of the underlying black-box function and noise processes for the properties of interest. While some experimental groups have been very successful in applying \textsc{bo} methodology for laboratory experiments \citep{2022_Jorayev}, this has often been paired with an understanding of the design space. As such, for unexplored domains it will become important to develop high-fidelity offline simulators to benchmark \textsc{bo} schemes and/or to develop means for counterfactual evaluation.










