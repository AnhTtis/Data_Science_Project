\chapter{Modelling Molecules with Gaussian Processes}
\chapterimage[height=130pt]{Chapter4/Figs/gauche_logo.png}


\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi


\textbf{Status:} Accepted as Griffiths, RR., Klarner, L., Moss, HB., Ravuri, A., Truong, S., Rankovic, B., Du, Y., Jamasb, A., Schwartz, J., Tripp, A., Kell, G. Bourached, A., Chan, A., Moss, J., Chengzhi, G, Lee AA, Schwaller P., Tang, J.  GAUCHE: A library for Gaussian processes in chemistry. \textit{ICML Workshop on AI4Science}, 2022.

\section{Background on Molecular Machine Learning}
\label{background_gauche}

The chapter begins with a self-contained background on molecular machine learning required to contextualise the findings of this chapter.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.75\textwidth]{Background/Figs/design_make2.png}
    \end{center}
    \caption{The design-make-test cycle of molecular discovery.}
    \label{dmt}
\end{figure}

Although application domains of machine learning in the molecular sciences are constantly expanding \citep{2020_Struble}, an important subset of applications can be taxonimised according to the role they play in enhancing the design-make-test cycle \citep{2012_Plowright} of molecular discovery campaigns, illustrated in \autoref{dmt}. Molecule generation \citep{2018_Design, 2020_Griffiths, 2017_Grammar, 2020_Jin, 2022_Du, 2022_Gao} is concerned with designing novel molecules using generative models such as variational autoencoders (VAEs) \citep{2014_Kingma} and generative adversarial networks (GANs) \citep{2014_Goodfellow}. Chemical reaction prediction \citep{2019_Schwaller}, reaction planning \citep{2018_Coley_new}, and synthesis design \citep{2022_Schwaller}, illustrated in \autoref{reaction}, are focussed on improving the throughput of the \say{make} stage of the design-make-test cycle by using machine learning to suggest synthetic pathways to target molecules.

The molecular machine learning tasks considered in this thesis, are molecular property prediction and chemical reaction optimisation. Molecular property prediction is concerned with the \say{design} phase of the design-make-test cycle in so far as it allows molecules to be prioritised for laboratory synthesis. Chemical reaction optimisation, on the other hand, is concerned with the \say{make} phase as it is a means of improving the yield of chemical reactions. Both applications will be described in detail next.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.75\textwidth]{Background/Figs/reaction.png}
    \end{center}
    \caption{A, B and C are starting materials and P is the product of the chemical reaction. Reaction planning involves finding a set of reagents to transform a starting material into a product. Reaction prediction involves predicting the product given a set of reactants. Synthesis design involves working backwards from the product towards a set of reactants and reagents. Machine learning-based solutions to all of these tasks would yield a blueprint for a chemist to follow in synthesising a novel molecule.}
    \label{reaction}
\end{figure}

\subsection{Molecular Property Prediction}

The laboratory synthesis of a novel molecule is a highly time-intensive process. As such, there has been a great deal of interest in computational approaches to prioritising molecules drawn from vast molecular databases in a process known as high-throughput virtual screening \citep{2015_Knapp}. In this fashion, theoretical techniques can be used to winnow the database down to a few promising candidates for experimental chemists to follow up on. Before the advent of machine learning, the dominant approach was to use first principles quantum chemical calculations to compute molecular properties. Below, one such first principles method is reviewed, density functional theory (DFT), which is compared against machine learning model performance in Chapter 5, before discussing machine learning approaches to molecular property prediction. 

\subsubsection{Density Functional Theory}

DFT is a method of modelling the electronic structure of many-body systems \citep{brazdova_atomistic_2013}, and has been applied across problems in physics, chemistry, biology, and materials science \citep{becke_perspective_2014}. DFT is an \emph{ab initio}, or first principles computational method because physical constants are the only inputs to calculations based on the postulates of quantum mechanics \citep{leach_molecular_2001}. Since the inception of DFT in 1964-1965, Kohn-Sham DFT (KS-DFT) has been one of the most frequently applied electronic structure methods \citep{becke_perspective_2014}. 

KS-DFT makes use of the Hohenberg-Kohn theorems \citep{hohenberg_inhomogeneous_1964}, a trial electron density, and a self-consistency scheme. KS-DFT executes a computational loop by starting with a trial density, solving the Kohn-Sham equations, and obtaining the single electron wavefunctions for the trial density; in the next step, an electron density may be computed. If the computed density is consistent i.e. within a tolerance threshold of the trial density, the theoretical ground state density has been identified. If the densities are not consistent, however, the computed density is taken as the new trial density, and the iterative loop continues to be executed until the tolerance threshold is met. The accuracy of DFT calculations, with exchange and correlation functionals, can be very high, yet may exhibit significant fluctuations with the choice of functional, pseudopotential, basis sets and cutoff energy \citep{howard_assessing_2015}. Furthermore, these quantities are not always trivial to optimise.

\subsubsection{Time-Dependent Density Functional Theory}

Time-Dependent Density Functional Theory (TD-DFT) is a time-dependent analogue of DFT based on the Runge-Gross (RG) theorem in place of the Hohenberg-Kohn theorems \citep{runge_density-functional_1984}. The RG theorem states that a unique delineation exists between the time-dependent electron density and the time-dependent external potential. As such, a computational, time-dependent Kohn-Sham system may be implemented \citep{van_leeuwen_causality_1998} in a similar fashion to KS-DFT. When TD-DFT has been used together with a linear response theory \citep{ullrich_time-dependent_2012}, it has enjoyed success in the calculation of electromagnetic spectra of medium and large molecules \citep{casida_progress_2012, burke_time-dependent_2005}. A relevant application of TD-DFT in this thesis is the computation of the $\pi-\pi{^*}/\emph{n}-\pi{^*}$ electronic transitions wavelengths for photoswitch molecules in Chapter 5.

\subsubsection{Machine Learning Approaches}

In contrast to first principles methods such as DFT, machine learning approaches seek to carry out data-driven prediction. A key issue in data-driven molecular property prediction is how best to featurise molecules. This problem is commonly referred to as choosing a molecular representation. While a great many base representations of molecules exist \citep{2022_Wigh}, some of the most popular featurisations include graph-based, string-based and fingerprint representations. The field of molecular representation learning is concerned with learning representations on top of these base representations e.g. \citep{2015_Duvenaud} typically via deep learning. Below, a brief review is provided of the molecular representations used in this thesis.

\paragraph{Graphs:} Molecules may be represented as an undirected, labeled graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ where vertices, $\mathcal{V}=\{v_1, \ldots, v_N\}$, represent the atoms of an $N$-atom molecule and edges, $\mathcal{E}\subset\mathcal{V}\times\mathcal{V}$, represent covalent bonds between these atoms. Additional information may be incorporated in the form of vertex and edge labels $\mathcal{L}:\mathcal{V}\times\mathcal{E}\to\Sigma_V\times\Sigma_E$. Common label spaces including attributes such as atom types (i.e. hydrogen, carbon) as vertex labels and bond orders (i.e. single, double) as edge labels.

\paragraph{Strings:} The Simplified Molecular-Input Line-Entry System (SMILES) is a text-based representation of molecules \citep{1987_Anderson, 1988_Weininger}, examples of which are given in \autoref{smiles_figure}. Self-Referencing Embedded Strings (SELFIES) \citep{2020_Krenn} is an alternative string representation to SMILES such that a bijective mapping exists between a SELFIES string and a molecule.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.39\textwidth]{Chapter4/Figs/smiles_image.png}
    \caption{SMILES strings for structurally similar molecules. Similarity is encoded in the string through common contiguous subsequences (black). Local differences are highlighted in red. Note the molecules are chosen solely for the purposes of illustrating the SMILES syntax.}
    \label{smiles_figure}
\end{figure}

\paragraph{Fingerprints:} Molecular fingerprints were first introduced for chemical database substructure searching \citep{1993_Christie}, but were later repurposed for similarity searching \citep{1990_Johnson}, clustering \citep{1997_McGregor} and classification \citep{2017_Breiman}. Extended Connectivity FingerPrints (ECFP) \citep{2010_Rogers} were introduced as part of the Pipeline project \citep{2006_Hassan} with the explicit goal of capturing features relevant for molecular property prediction \citep{2004_Xia}. ECFP fingerprints operate by assigning initial numeric identifiers to each atom in a molecule. These identifiers are subsequently updated in an iterative fashion based on the identifiers of their neighbours. The number of iterations corresponds to half the \textit{diameter} of the fingerprint and the naming convention reflects this. For example, ECFP6 fingerprints have a diameter of 6, meaning that 3 iterations of atom identifier reassignment are performed. Each level of iteration appends substructural features of increasing non-locality to an array and the array is then hashed to a bit vector reflecting the presence of absence of those substructures in the molecule.

For property prediction applications a radius of 3 or 4 is recommended. A radius of 3 is used for all experiments in the thesis. Fragment descriptors are also used, which are count vectors, each component of which indicates the number of a certain functional group present in a molecule. For example row 1 of the count vector could be an integer representing the number of aliphatic hydroxl groups present in the molecule. Both fingerprint and fragment features computed using RDKit are made use of \citep{rdkit}, as well as the concatenation of the fingerprint and fragment feature vectors, a representation termed fragprints which has shown strong empirical performance. Example representations $\mathbf{x_f}$, for fingerprints and $\mathbf{x_{fr}}$, for fragments might be

\begin{align*}
 \mathbf{x_{f}} &= \begin{bmatrix}
       1 \\
       0 \\
       \vdots \\
       1
     \end{bmatrix}, \:\:\:
  \mathbf{x_{fr}} = \begin{bmatrix}
       3 \\
       0 \\
       \vdots \\
       2
     \end{bmatrix}.
\end{align*}

\subsubsection{Bayesian Optimisation and Active Learning for Molecules}

\textsc{bo} and active learning hold particular promise for accelerating high-throughput virtual screening efforts for molecules \citep{2017_Knapp, 2020_Pyzer}. The idea in this instance is that data-efficient molecular property optimisation can be performed in a \textsc{bo}/active learning loop featuring laboratory synthesis of the suggested molecules.

\subsection{Chemical Reaction Optimisation}

The \say{yield} of a chemical reaction is an important consideration for large-scale production of a desired molecule. The percentage yield, of a chemical reaction may be defined as

\begin{equation}
     \text{Percentage Yield} = \frac{\text{Actual Yield}}{\text{Theoretical Yield}}.
\end{equation}

\noindent The Theoretical Yield assumes a flawless chemical reaction in which all starting materials are converted to the desired product. In practice, chemical reactions are not perfectly efficient due to factors such as reverse reactions, in which the reactants and products exist in a state of chemical equilibrium, as well as competing chemical reactions that form unwanted side products. Factors such as the temperature of the reaction, the concentration of reactant species, the choice of solvent as well as the presence of reagents, molecular species that enhance the reaction without contributing atoms to the product, are all determinants of the efficiency of the reaction and hence the Actual Yield. Optimising such reaction parameters has recently been tackled by machine learning approaches such as \textsc{bo} \citep{2021_Shields}. Common reaction representations are detailed next. 

\subsubsection{Chemical Reaction Representations}
A chemical reaction comprises reactants and reagents that transform into one or more products together with reaction parameters such as temperature and concentration. The reactants and reagents are instances of molecular species which play different roles in the reaction. By means of illustration, the high-throughput experiments by \cite{ahneman2018predicting} on Buchwald-Hartwig reactions feature a reaction design space consisting of 15 aryl and heteroaryl halides, 4 Buchwald ligands, 3 bases, and 23 isoxazole additives. Below, various means of featurising the reactant and reagent components of a chemical reaction are introduced.

\paragraph{Concatenated molecular representations:} If the number of reactant and reagent categories is constant, the molecular representations discussed above can be used to encode reactants and reagents. The vectors for the individual reaction components may then be concatenated to build the reaction representation \citep{ahneman2018predicting, sandfort2020structure}. An additional concatenated representation, is the one-hot-encoding (OHE) of the reaction categories where bits indicate the presence or absence of a particular reactant/reagent. In the Buchwald-Hartwig example above, the OHE would describe which of the aryl halides, Buchwald ligands, bases, and additives are used in the reaction, resulting in a 44-dimensional bit vector \citep{chuang2018comment}. 

\paragraph{Differential reaction fingerprints:} Inspired by the hand-engineered difference reaction fingerprints by \citet{schneider2015development}, \citet{probst2022reaction} recently introduced the differential reaction fingerprint (DRFP). This reaction fingerprint is constructed by taking the symmetric difference of the sets containing the molecular substructures on both sides of the reaction arrow. The size of the reaction bit vector generated by DRFP is independent of the number of reaction components. 

\paragraph{Data-driven reaction fingerprints:} \citet{schwaller2021mapping} described data-driven reaction fingerprints using Transformer models such as BERT \citep{devlin2018bert}, trained in a supervised or an unsupervised fashion on reaction SMILES. The Transformer models can then be fine-tuned on the task of interest to learn more specific reaction representations \citep{schwaller2021prediction}. These representations are designated using the acronym RXNFP. As with the DRFP, the size of data-driven reaction fingerprints is also independent of the number of reaction components.

In the next section, the focus of this chapter is introduced.

\section{Preface}

This chapter introduces GAUCHE, a library for GAUssian processes in CHEmistry. \textsc{gp}s have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification (UQ) and \textsc{bo}. Extending \textsc{gp}s to chemical representations however is nontrivial, necessitating kernels defined over structured inputs such as graphs, strings and bit vectors. By defining such kernels in GAUCHE, the door is opened to powerful tools for UQ and \textsc{bo} in chemistry. Motivated by scenarios frequently encountered in experimental chemistry, applications for GAUCHE are showcased in molecule discovery and chemical reaction optimisation.

\section{Introduction}
\label{intro}

Early-stage scientific discovery is typically characterised by the small data regime due to the limited availability of high-quality experimental data \citep{2018_Zhang}. Much of the novelty of discovery relies on the fact that there is a lot of knowledge to gain in the small data regime. By contrast, in the big data regime, discovery offers diminishing returns as much of the knowledge about the space of interest has already been acquired. As such, machine learning methodologies that facilitate search in small data regimes such as \textsc{bo} \citep{2018_Bombarelli, 2020_Griffiths, 2021_Shields} and active learning (AL) \citep{2019_Zhang, 2021_Jablonka} have great potential to expedite the rate at which performant molecules, molecular materials, chemical reactions and proteins are discovered.

To date in molecular machine learning, BNNs have been the surrogate of choice to produce the uncertainty estimates that underpin \textsc{bo} and AL \citep{2019_Ryu, 2019_Zhang, 2020_Hwang, 2020_Scalia}. For small datasets, however, DNNs are often not the model of choice. Notably, certain deep learning experts have voiced a preference for \textsc{gp}s in the small data regime \cite{2011_Bengio}. Furthermore, for \textsc{bo}, \textsc{gp}s possess particularly advantageous properties; first, they admit exact as opposed to approximate Bayesian inference and second, few of their parameters need to be determined by hand. In the words of Sir David MacKay \cite{2003_MacKay}, 

\begin{displayquote}
"Gaussian processes are useful tools for automated tasks where fine tuning for each problem is not possible. We do not appear to sacrifice any performance for this simplicity.''
\end{displayquote}

The iterative model refitting required in \textsc{bo} makes it a prime example of such an automated task. Although BNN surrogates have been trialled for \textsc{bo} \citep{2015_Snoek, 2016_Springenberg}, \textsc{gp}s remain the model of choice as evidenced by the results of the recent NeurIPS Black-Box Optimisation Competition \cite{2021_Turner}.


Training \textsc{gp}s on molecular inputs is non-trivial however. Canonical applications of \textsc{gp}s assume continuous input spaces of low and fixed dimensionality. The most popular molecular input representations are SMILES/SELFIES strings \citep{1987_Anderson, 1988_Weininger, 2020_Krenn}, fingerprints \citep{2010_Rogers, probst2018probabilistic, capecchi2020one} and graphs \citep{2015_Duvenaud,2016_Kearnes}. Each of these input representations poses problems for \textsc{gp}s. SMILES strings have variable length, fingerprints are high-dimensional and sparse bit vectors, while graphs are also a form of non-continuous input. To construct a \textsc{gp} framework over molecules, GAUCHE provides GPU-based implementations of kernels that operate on molecular inputs, including string, fingerprint and graph kernels. Furthermore, GAUCHE includes support for protein and chemical reaction representations and interfaces with the GPyTorch \citep{2018_Gardner} and BoTorch \citep{2019_Balandat} libraries to facilitate usage for advanced probabilistic modelling and \textsc{bo}. The detailed contributions of this chapter may be summarised as:

\newpage

\begin{enumerate}
\setlength\itemsep{0.3em}
    \item The introduction of a \textsc{gp} framework for molecules and chemical reactions.
    \item The provision of an open-source, GPU-enabled library building on GPyTorch \citep{2018_Gardner}, BoTorch \citep{2019_Balandat}, and RDKit \cite{rdkit}.
    \item The use of black box graph kernels, from GraKel, \citep{2020_GraKel}, is extended to \textsc{gp} regression via a GPyTorch interface, along with a limited set of graph kernels implemented in native GPyTorch to enable optimisation of the graph kernel hyperparameters under the marginal likelihood.
    \item Benchmark experiments are conducted, evaluating the utility of the \textsc{gp} framework on regression, UQ and \textsc{bo} tasks.
\end{enumerate}

    
GAUCHE is made available at \href{https://github.com/leojklarner/gauche}{https://github.com/leojklarner/gauche} and includes tutorials to guide users through the tasks considered in this paper. 

\section{Related Work}
General-purpose \textsc{gp} and \textsc{bo} libraries do not specifically cater for molecular representations. Likewise, general-purpose molecular machine learning libraries do not specifically consider \textsc{gp}s and \textsc{bo}. Here, existing libraries are reviewed, highlighting the niche GAUCHE fills in bridging the \textsc{gp} and molecular machine learning communities. To date, there has been little work on Gaussian processes applied to discrete molecular representations, some notable exceptions being \citep{2020_Gardiner_gp, 2022_Gosnell, 2023_Jablonka}.

GAUCHE is a PyTorch extension of FlowMO \citep{2020_flowmo}, which introduces a molecular \textsc{gp} library in the GPflow framework. It is upon FlowMO which GAUCHE builds, extending the scope of the library to a broader class of molecular representations (graphs), problem settings (\textsc{bo}), and applications (reaction optimisation).

\paragraph{Gaussian Process Libraries:} \textsc{gp} libraries include GPy (Python) \citep{2014_gpy}, GPflow (TensorFlow) \citep{2017_Matthews, 2020_Wilk}, and GPyTorch (PyTorch) \citep{2018_Gardner}, while examples of recent Bayesian optimisation libraries include BoTorch (PyTorch) \citep{2019_Balandat}, Dragonfly (Python) \citep{2019_dragonfly}, and HEBO (PyTorch) \citep{2020_Rivers}. The aforementioned libraries do not explicitly support molecular representations. Extension to cover molecular representations, however, is nontrivial, requiring implementations of bespoke \textsc{gp} kernels for bit vector, string and graph inputs together with modifications to Bayesian optimisation schemes to consider acquisition function evaluations over a discrete set of heldout molecules, a setting commonly encountered in virtual screening campaigns \citep{2020_Pyzer, 2022_Graff}.

\paragraph{Molecular Machine Learning Libraries:} Molecular machine learning libraries include DeepChem \citep{2019_Ramsundar}, DGL-LifeSci \citep{2020_Li} and TorchDrug \citep{2022_Zhu}. DeepChem features a broad range of model implementations and tasks, while DGL-LifeSci focuses on graph neural networks. TorchDrug caters for applications including property prediction, representation learning, retrosynthesis, biomedical knowledge graph reasoning and molecule generation.

\textsc{gp} implementations are not included, however, in the aforementioned libraries. In terms of atomistic systems, DScribe \citep{2020_Himanen} features, amongst other methods, the Smooth Overlap of Atomic Positions (SOAP) representation \citep{2013_Bartok} which is typically used in conjunction with a \textsc{gp} model to learn atomistic properties. Automatic Selection And Prediction (ASAP) \citep{2020_Cheng} also principally focusses on atomistic properties as well as dimensionality reduction and visualisation techniques for materials and molecules. Lastly, the Graphein library focusses on graph representations of proteins \citep{2020_Jamasb}.

\paragraph{Graph Kernel Libraries:} Graph kernel libraries include GraKel \citep{2020_GraKel}, graphkit-learn \citep{2021_Linlin}, graphkernels \citep{2018_Sugiyama}, graph-kernels \citep{2015_Sugiyama}, pykernels (\href{https://github.com/gmum/pykernels}{https://github.com/gmum/pykernels}) and ChemoKernel \citep{2012_Gauzere}. The aforementioned libraries focus on CPU implementations in Python. Extending graph kernel computation to GPUs has been noted as an important direction for future research \citep{2018_Ghosh}. In our work, the GraKel library is built upon by interfacing it with GPyTorch, facilitating \textsc{gp} regression with GPU computation. Furthermore, this enables the graph kernel hyperparameters to be learned through the marginal likelihood objective as opposed to being pre-specified and fixed upfront. 

\paragraph{Molecular Bayesian Optimisation:} \textsc{bo} over molecular space can be split into two classes of methods. In the first class, molecules are encoded into the latent space of a VAE \citep{2018_Bombarelli}. \textsc{bo} is then performed over the continuous latent space and queried molecules are decoded back to the original space. Much work on VAE-BO has focussed on improving the synergy between the surrogate model and the VAE \citep{2018_Griffiths, 2020_Griffiths, 2020_Tripp, 2021_Deshwal, 2021_Grosnit, 2021_Verma, 2022_Maus, 2022_Stanton}. One of the defining characteristics of VAE-BO is that it enables the generation of new molecular structures.\\

In the second class of methods, \textsc{bo} is performed directly over the original discrete space of molecules \citep{2022_Tom} In this setting it is not possible to generate new structures and so a candidate set of queryable molecules is defined. The inability to generate new structures however, is not necessarily a bottleneck to molecule discovery in many cases, as the principle concern is how best to explore existing candidate sets. These candidate sets are also known as molecular libraries in the virtual screening literature \citep{2015_Pyzer}. 

To date, there has been little work on \textsc{bo} directly over discrete molecular spaces. In \citet{2020_Moss}, the authors use a string kernel \textsc{gp} trained on SMILES to perform \textsc{bo} to select from a candidate set of molecules. In \citet{2020_Korovina}, an optimal transport kernel \textsc{gp} is used for \textsc{bo} over molecular graphs. In \citet{2021_Hase} a surrogate based on the Nadarya-Watson estimator is defined such that the kernel density estimates are inferred using BNNs. The model is then trained on molecular descriptors. Lastly, in \citet{2017_Knapp} and \citet{2021_Moss} a BNN and a sparse \textsc{gp} respectively are trained on fingerprint representations of molecules. In the case of the sparse \textsc{gp} the authors select an ArcCosine kernel. It is a long term aim of the GAUCHE Project to compare the efficacy of VAE-BO against vanilla \textsc{bo} on real-world molecule discovery tasks.


\paragraph{Chemical Reaction Optimisation:} Chemical reactions describe how reactant molecules transform into product molecules. Reagents (catalysts, solvents, and additives) and reaction conditions heavily impact the outcome of chemical reactions. Typically the objective is to maximise the reaction yield (the amount of product compared to the theoretical maximum) \citep{ahneman2018predicting}, to maximise the enantiomeric excess in asymmetric synthesis, where the reactions could result in different enantiomers \citep{zahrt2019prediction}, or to minimise the E-factor, which is the ratio between waste materials and the desired product \citep{schweidtmann2018machine}. 

A range of studies have evaluated the optimisation of chemical reactions in single and  multi-objective settings \citep{schweidtmann2018machine,muller2022automated}. \citet{felton2021summit} and \citet{hase2021olympus} benchmarked reaction optimisation algorithms in low-dimensional settings including reaction conditions, such as time, temperature, and concentrations. \citet{2021_Shields} suggested \textsc{bo} as a general tool for chemical reaction optimisation and benchmarked their approach against human experts. \citet{haywood2021kernel} compared the yield prediction performance of different kernels and \citet{pomberger2022}, the impact of various molecular representations. 

In all reaction optimisation studies above, the representations of the different categories of reactants and reagents are concatenated to generate the reaction input vector, which could lead to limitations if another type of reagent is suddenly considered. Moreover, most studies concluded that simple one-hot encodings (OHE) perform at least on par with more elaborate molecular representations in the low-data regime \citep{2021_Shields, pomberger2022, hickman2022}. In GAUCHE, reaction fingerprint kernels are introduced, based on existing reaction fingerprints \citep{schwaller2021mapping, probst2022reaction} and work independently of the number of reactant and reagent categories. The molecular kernels, constituting the backbone of the GAUCHE library, are described next.



\section{Molecular Kernels}
\label{mol_gauss}

Here, examples are given of the classes of GAUCHE kernel designed to operate on the molecular representations introduced in \autoref{background_gauche}.

\subsection{Fingerprint Kernels}

\paragraph{Scalar Product Kernel:} The simplest kernel to operate on fingerprints is the scalar product or linear kernel defined for vectors $\mathbf{x}, \mathbf{x'} \in \mathbb{R}^d$ as

\begin{equation}
    k_{\text{Scalar Product}}(\mathbf{x}, \mathbf{x'}) \coloneqq \sigma_{f}^2 \cdot \langle\mathbf{x}, \mathbf{x'}\rangle,
\end{equation}

\noindent where $\sigma_{f}$ is a scalar signal variance hyperparameter and $\langle\cdot, \cdot\rangle$ is the Euclidean inner product.

\paragraph{Tanimoto Kernel:} Introduced as a general similarity metric for binary attributes \citep{gower1971general}, the Tanimoto kernel was first used in chemoinformatics in conjunction with non-\textsc{gp}-based kernel methods \citep{2005_Ralaivola}. It is defined for binary vectors $\mathbf{x}, \mathbf{x'} \in \{0, 1\}^d$ for $d \geq 1$ as

\begin{equation}
\label{equation: tanimoto}
    k_{\text{Tanimoto}}(\mathbf{x}, \mathbf{x'}) \coloneqq \sigma_{f}^2 \cdot \frac{\langle\mathbf{x}, \mathbf{x'}\rangle}{\norm{\mathbf{x}}^2 + \norm{\mathbf{x'}}^2 - \langle\mathbf{x}, \mathbf{x'}\rangle},
\end{equation}

\noindent where $||\cdot||$ is the Euclidean norm.

\subsection{String Kernels}
String kernels \citep{lodhi2002text, cancedda2003word} measure the similarity between strings by examining the degree to which their sub-strings differ. In GAUCHE, the SMILES string kernel \citep{cao2012silico} is implemented, which calculates an inner product between the occurrences of sub-strings, considering all contiguous sub-strings made from at most $n$ characters ($n=5$ was chosen in all experiments). Therefore, for the sub-string count featurisation $\phi : \mathcal{S} \rightarrow \mathbb{R}^p$ (also known as a bag-of-characters representation \citep{jurasfky2000introduction}), where $p$ is the number of unique n-grams from the alphabet, the SMILES string kernel between two strings $\mathcal{S}$ and $\mathcal{S}'$ is given by

\begin{equation}
    k_{\textrm{String}}(\mathcal{S},\mathcal{S}')\coloneqq \sigma^2\cdot \langle \phi(\mathcal{S}), \phi(\mathcal{S}') \rangle.
\end{equation}

Although more complicated string kernels do exist in the literature, for example those that allow non-contiguous matches \citep{2020_Moss}, it was found that the significant extra computational overhead of these methods did not provide improved performance over the more simple SMILES string kernel in the context of molecular data. Note that although named the SMILES string kernel, this kernel can also be applied to any other string representation of molecules e.g. SELFIES.

\subsection{Graph Kernels}

\paragraph{Graph Kernels:} 
Graph kernel methods $\phi_\lambda:\mathcal{G}\to\mathcal{H}$, map elements from a graph domain $\mathcal{G}$ to a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$, in which an inner product between a pair of graphs $g,g'\in\mathcal{G}$ is derived as a measure of similarity,

\begin{equation}
    k_{\text{Graph}}(g, g') \coloneqq \sigma^2 \cdot \langle\phi_\lambda(g),\phi_\lambda(g')\rangle_\mathcal{H},
\end{equation}

\noindent where $\lambda$ denotes kernel-specific hyperparameters and $\sigma^2$ is a scale factor.
Depending on how $\phi_\lambda$ is defined \citep{Nikolentzos_2021}, the kernel considers different substructural motifs and is characterised by different hyperparameters.

Frequently-employed approaches include the random walk kernel \citep{2010_Viswanathan}, given by a geometric series over the count of matching random walks of increasing length with coefficient $\lambda$,
and the Weisfeiler-Lehman (WL) kernel \citep{shervashidze2011weisfeiler}, given by the inner products of label count vectors over $\lambda$ iterations of the Weisfeiler-Lehman algorithm.


\paragraph{Graph Embedding:}
Pretrained graph neural networks (GNNs)~\cite{hu2019} may also be used to embed molecular graphs in a vector space. Since the GNN is trained on a large amount of data, the representation it produces has the potential to be a more expressive method to encode a molecule (Note: this assumes access to a large pool of in-domain data). Given a vector representation from a pretrained GNN model, any \textsc{gp} kernel for continuous input spaces may be applied, such as the RBF kernel.

\section{Experiments}

GAUCHE (available at \href{https://github.com/leojklarner/gauche}{https://github.com/leojklarner/gauche}) is evaluated on regression, UQ and \textsc{bo} tasks. The principle goal in conducting regression and UQ benchmarks is to gauge whether performance on these tasks may be used as a proxy for \textsc{bo} performance. \textsc{bo} is a powerful tool for automated scientific discovery but one would prefer to avoid model misspecification in the surrogate when deploying a scheme in the real world. The following datasets were chosen:

\paragraph{The Photoswitch Dataset:} The labels, $y$ are the experimentally-determined values of the \textit{E} isomer $\pi-\pi^*$ transition wavelength (nm) for 392 photoswitch molecules.

\paragraph{ESOL:} \citep{2004_Delaney}: The labels $y$ are the experimentally-determined logarithmic aqueous solubility values (mols/litre) for 1128 organic small molecules.

\paragraph{FreeSolv:} \citep{2014_Mobley}: The labels $y$ are the experimentally-determined hydration free energies (kcal/mol) for 642 molecules.

\paragraph{Lipophilicity:} The labels $y$ are the experimentally-determined octanol/water distribution coefficient (log D at pH 7.4) of 4200 compounds curated from the ChEMBL database \citep{2012_Gaulton, 2014_Bento}.

\paragraph{Buchwald-Hartwig reactions:} \citep{ahneman2018predicting}: The labels $y$ are the experimentally-determined yields for 3955 Pd-catalysed Buchwald–Hartwig C–N cross-couplings. 

\paragraph{Suzuki-Miyaura reactions:} \citep{perera2018platform}: The labels $y$ are the experimentally-determined yields for 5760 Pd-catalysed Suzuki-Miyaura C-C cross-couplings.

\subsection{Regression}

The regression results for molecular property prediction are reported in Table ~\ref{table: regression} and for reaction yield prediction in Table~\ref{table: reaction} of \autoref{app_b}. The datasets are split in a train/test ratio of 80/20 (note that validation sets are not required for the \textsc{gp} models since training uses the marginal likelihood objective). Errorbars represent the standard error across 20 random initialisations. All \textsc{gp} models are trained using the L-BFGS-B optimiser \citep{1989_Liu}. If not mentioned, default settings in the GPyTorch and BoTorch libraries apply. For the SELFIES representation, some molecules could not be featurised and corresponding entries are left blank. The results of Table~\ref{table: reaction} indicate that the best choice of representation (and hence the choice of kernel) is task-dependent.

\begin{table*}[h]
\caption{Molecular property prediction regression benchmark. RMSE values for 80/20 train/test split across 20 random trials. All WL kernel entries computed by Aditya Ravuri.}
\adjustbox{width=\textwidth}{%
\centering
\begin{tabular}{l l | c c c c}
    \toprule
    \multicolumn{2}{c|}{{\bf GP Model}} & \multicolumn{4}{c}{{\bf Dataset}}  \\
    Kernel & Representation & Photoswitch & ESOL & FreeSolv & Lipophilicity \\
    \hline
    
    Tanimoto & fragprints & ${\bf 20.9} \pm {\bf 0.7}$ & $0.71 \pm 0.01$ & $1.31 \pm 0.06 $ & $\textbf{0.67} \pm \textbf{0.01}$ \\
    & fingerprints & $23.4 \pm 0.8$ & $1.01 \pm 0.01$ & $1.93 \pm 0.09$ & $0.76 \pm 0.01$ \\ 
    & fragments & $26.3 \pm 0.8$ & $0.91 \pm 0.01$ & $1.49 \pm 0.05$ & $0.80 \pm 0.01$ \\ 
    \hdashline
    
    Scalar Product & fragprints & $22.5 \pm 0.7$ & $0.88 \pm 0.01$ & $\textbf{1.27} \pm \textbf{0.02} $ & $0.77 \pm 0.01$ \\
    & fingerprints & $24.8 \pm 0.8$ & $1.17 \pm 0.01$ & $1.93 \pm 0.07$  & $0.84 \pm 0.01$ \\
    & fragments & $36.6 \pm 1.0$ & $1.15 \pm 0.01$ &  $1.63 \pm 0.03$ & $0.97. \pm 0.01$ \\
    \hdashline
    
    String & SELFIES & $24.9 \pm 0.6$ & - & - & - \\
    & SMILES & $24.8 \pm 0.7$ & $\textbf{0.66} \pm \textbf{0.01}$  & $1.31 \pm 0.01$ & $\textbf{0.68} \pm \textbf{0.01}$  \\
    
    \hdashline
    WL Kernel (GraKel) & graph & $22.4 \pm 1.4$ & $1.04 \pm 0.02$ & $1.47 \pm 0.06$ & $0.74 \pm 0.05$ \\
    
    
    
    \bottomrule
\end{tabular}}
\label{table: regression}
\end{table*}

\subsection{Uncertainty Quantification (UQ)}

To quantify the quality of the uncertainty estimates three metrics were used, the negative log predictive density (NLPD), the mean standardised log loss (MSLL) and the quantile coverage error (QCE). The NLPD results are provided in Table~\ref{table: nlpd} and the MSLL and QCE results in Table~\ref{table: MSLL} and Table~\ref{table: QCE} respectively. One trend to note is that uncertainty estimate quality is roughly correlated with regression performance. Numerical errors were encountered with the WL kernel on the large lipophilicity dataset which invalidated the results and so the corresponding entry is left blank. The native random walk kernel was discontinued (for the time being) due to poor performance.

\begin{table*}[h]
\caption{UQ benchmark. NLPD values for 80/20 train/test split across 20 random trials.}
\adjustbox{width=\textwidth}{%
\centering
\begin{tabular}{l l | c c c c}
    \toprule
    \multicolumn{2}{c|}{{\bf GP Model}} & \multicolumn{4}{c}{{\bf Dataset}}  \\
    Kernel & Representation & Photoswitch & ESOL & FreeSolv & Lipophilicity \\
    \hline

    Tanimoto & fragprints & $\textbf{0.22} \pm \textbf{0.03}$ & $0.33 \pm 0.01$ & $0.28 \pm 0.02$ & $\textbf{0.71} \pm \textbf{0.01}$ \\
    & fingerprints & $0.33 \pm 0.03$ & $0.71 \pm 0.01$ & $0.58 \pm 0.03$ & $0.85 \pm 0.01$ \\ 
    & fragments & $0.50 \pm 0.04$ & $0.57 \pm 0.01$ & $0.44 \pm 0.03$ & $0.94 \pm 0.02$\\ 
    \hdashline
    
    Scalar Product & fragprints & $\textbf{0.23} \pm \textbf{0.03}$ & $0.53 \pm 0.01$ & $0.25 \pm 0.02$ & $0.92 \pm 0.01$ \\
    & fingerprints & $0.33 \pm 0.03$ & $0.84 \pm 0.01$ & $0.64 \pm 0.03$ & $1.03 \pm 0.01$\\
    & fragments & $0.80 \pm 0.03$ & $0.82 \pm 0.01$ & $0.54 \pm 0.02$ & $0.88 \pm 0.10$  \\
    \hdashline
    
    String & SELFIES & $0.37 \pm 0.04$ & - & - & - \\
    & SMILES & $0.30 \pm 0.04$ & $\textbf{0.29} \pm \textbf{0.03}$  & $\textbf{0.16} \pm \textbf{0.02}$  & $\textbf{0.72} \pm \textbf{0.01}$ \\
    \hdashline
    
    WL Kernel (GraKel) & graph & $0.39 \pm 0.11$ & $0.76 \pm 0.001$ & $0.47 \pm 0.02$ & - \\
    
    \bottomrule
    
\end{tabular}}
\label{table: nlpd}
\end{table*}




\begin{table}[h]
\caption{UQ Benchmark. MSLL Values ($\downarrow$) for 80/20 Train/Test Split.}
\adjustbox{width=\textwidth}{%
\centering
\begin{tabular}{l l | c c c c}
    \toprule
    \multicolumn{2}{c|}{{\bf GP Model}} & \multicolumn{4}{c}{{\bf Dataset}}  \\
    Kernel & Representation & Photoswitch & ESOL & FreeSolv & Lipophilicity \\
    \hline
    
    Tanimoto & fragprints & $\textbf{0.06} \pm \textbf{0.01}$ & $0.17 \pm 0.04$ & $0.16 \pm 0.02$ & $\textbf{0.50} \pm \textbf{0.006}$ \\
    & fingerprints & $0.16 \pm 0.01$ & $0.55 \pm 0.01$ & $0.42 \pm 0.02$ & $0.63 \pm 0.004$ \\ 
    & fragments & $0.27 \pm 0.01$ & $0.34 \pm 0.04$ & $0.24 \pm 0.02$& $0.72 \pm 0.003$\\ 
    \hdashline
    
    Scalar Product & fragprints & $0.03 \pm 0.01$ & $0.32 \pm 0.004$ & $0.06 \pm 0.01$ & $0.67 \pm 0.003$ \\
    & fingerprints & $0.11 \pm 0.01$ & $0.64 \pm 0.006$ & $0.41 \pm 0.02$ & $0.79 \pm 0.003$\\
    & fragments & $0.56 \pm 0.01$ & $0.58 \pm 0.005$ & $0.29 \pm 0.01$ & $0.94 \pm 0.003$  \\
    \hdashline
    
    String & SELFIES & $0.13 \pm 0.01$ & - & - & - \\
    & SMILES & $\textbf{0.08} \pm \textbf{0.02}$ & $\textbf{0.03} \pm \textbf{0.005}$  & $\textbf{0.03} \pm \textbf{0.02}$  & $\textbf{0.52} \pm \textbf{0.002}$ \\
    \hdashline
    
    WL Kernel (GraKel) & graph & $0.14 \pm 0.03$ & $0.54 \pm 0.01$ & $0.26 \pm 0.01$ & - \\

    \bottomrule
    
\end{tabular}}
\label{table: MSLL}
\end{table}

\begin{table}[h]
\caption{UQ benchmark. QCE values ($\downarrow$) for 80/20 train/test split across 20 random trials.}
\adjustbox{width=\textwidth}{%
\centering
\begin{tabular}{l l | c c c c}
    \toprule
    \multicolumn{2}{c|}{{\bf GP Model}} & \multicolumn{4}{c}{{\bf Dataset}}  \\
    Kernel & Representation & Photoswitch & ESOL & FreeSolv & Lipophilicity \\
    \hline
    
    Tanimoto & fragprints & $\textbf{0.019} \pm \textbf{0.003}$ & $0.023 \pm 0.002$ & $0.023 \pm 0.002$ & $0.006 \pm 0.002$ \\
    & fingerprints & $0.023 \pm 0.003$ & $0.022 \pm 0.002$ & $0.018 \pm 0.003$ & $0.006 \pm 0.001$ \\ 
    & fragments & $0.025 \pm 0.005$ & $0.012 \pm 0.002$ & $0.014 \pm 0.002$& $0.009 \pm 0.002$\\ 
    \hdashline
    
    Scalar Product & fragprints & $0.033 \pm 0.006$ & $0.010 \pm 0.002$ & $0.017 \pm 0.003$ & $0.010 \pm 0.001$ \\
    & fingerprints & $0.036 \pm 0.006$ & $0.014 \pm 0.002$& $0.016 \pm 0.002$ & $0.009 \pm 0.001$ \\
    & fragments & $0.027 \pm 0.004$ & $0.012 \pm 0.003$ & $0.021 \pm 0.003$ & $0.010 \pm 0.001$ \\
    \hdashline
    
    String & SELFIES & $0.031 \pm 0.006$ & -  & - & - \\
    & SMILES & $0.024 \pm 0.003$ & $0.016 \pm 0.002$  & $0.019 \pm 0.003$  & $0.005 \pm 0.001$ \\
    \hdashline
    
    WL Kernel (GraKel) & graph & $0.025 \pm 0.007$ & $0.011 \pm 0.004$ & $0.019 \pm 0.009$ & $0.066 \pm 0.014$ \\
    \bottomrule
\end{tabular}}
\label{table: QCE}
\end{table}

\subsection{Chemical Reaction Yield Prediction}
\label{app_b}

Further regression and UQ experiments are presented in Table~\ref{table: reaction}. The differential reaction fingerprint in conjunction with the Tanimoto kernel is the best-performing reaction representation.

\begin{table*}[ht]
\caption{Chemical reaction regression benchmark. 80/20 train/test split across 20 random trials. Experiments performed by Bojana Rankovic. Kernel code written by Ryan-Rhys Griffiths.}
\adjustbox{width=\textwidth}{%
\centering
\begin{tabular}{l l | c c c c}
    \toprule
    \multicolumn{2}{c|}{\bf GP Model} & \multicolumn{4}{c}{\bf Buchwald-Hartwig}  \\
    Kernel & Representation & RMSE $\downarrow$ & $R^{2}$ score $\uparrow$ & MSLL $\downarrow$ & QCE $\downarrow$ \\ \hline
    
    Tanimoto & OHE & $7.94 \pm 0.05$ & $0.91 \pm 0.001$ & $-0.06 \pm 0.002$& $0.011 \pm 0.001$\\
    & DRFP & $\textbf{6.48} \pm \textbf{0.45}$ & $\textbf{0.94} \pm \textbf{0.015}$ & $\textbf{-0.15} \pm \textbf{0.07} $& $0.027 \pm 0.002$ \\ 
    \hdashline
    
    Scalar Product & OHE & $15.23 \pm 0.052$ & $0.69 \pm 0.002$ & $0.57 \pm 0.002$ & $0.008 \pm 0.001$\\ 
    & DRFP & $14.63 \pm 0.050$ & $0.71 \pm 0.002$ &$0.55 \pm 0.002$ & $0.010 \pm 0.001$ \\
    \hdashline
    
    RBF & RXNFP & $10.79 \pm 0.049$ &$0.84 \pm 0.001 $ & $0.37 \pm 0.005$ & $0.024 \pm 0.001$ \\
    \hline
    
    \multicolumn{2}{c|}{} & \multicolumn{4}{c}{\bf Suzuki-Miyaura}  \\ \hline
    
    Tanimoto & OHE & $11.18 \pm 0.036$ & $0.83 \pm 0.001$ & $0.23 \pm 0.001 $ & $0.007 \pm 0.001 $  \\
    & DRFP & $11.46 \pm 0.038 $ & $0.83 \pm 0.001 $ &$0.25 \pm 0.006$ &$0.019 \pm 0.000$  \\
    \hdashline
    
    Scalar Product & OHE & $19.91 \pm 0.042$ & $0.47 \pm 0.003$ & $0.82 \pm 0.001 $  & $0.012 \pm 0.001$ \\
    & DRFP & $19.66 \pm 0.042$ & $0.52 \pm 0.003$ & $0.81 \pm 0.001$  & $0.014 \pm 0.001$  \\
    \hdashline
    
    RBF & RXNFP & $13.83 \pm 0.048 $ & $0.75 \pm 0.002$ & $0.50 \pm 0.001$ & $0.007 \pm 0.001$ \\
    \bottomrule
\end{tabular}}
\label{table: reaction}
\end{table*}

\subsection{Bayesian Optimisation}

Two of the best-performing kernels were taken forward, the Tanimoto-fragprint kernel and the bag of SMILES kernel to undertake \textsc{bo} over the photoswitch and ESOL datasets. Random search is used as a baseline. \textsc{bo} is run for 20 iterations of sequential candidate selection (EI acquisition) where candidates are drawn from 95\% of the dataset. The results are provided in \autoref{bayesopt}. The models are initialised with 5\% of the dataset. In the case of the photoswitch dataset this corresponds to just 19 molecules. In this ultra-low data setting, common to many areas of synthetic chemistry, both models outperform random search, highlighting the real-world use-case for such models in supporting human chemists prioritise candidates for synthesis. Furthermore, one may observe that \textsc{bo} performance is tightly coupled to regression and UQ performance. In the case of the photoswitch dataset, the better-performing Tanimoto model on regression and UQ also achieves relatively better BO performance. Additionally, results are reported on the Buchwald-Hartwig reaction dataset.

\begin{figure*}[]
\centering
\subfigure[Photoswitch]{\label{fig:4}\includegraphics[width=0.3\textwidth]{Chapter4/Figs/photoswitch_bo.png}}
\subfigure[ESOL]{\label{fig:3}\includegraphics[width=0.3\textwidth]{Chapter4/Figs/esol_bo.png}}
\subfigure[Buchwald-Hartwig]{\label{fig:5}\includegraphics[width=0.3\textwidth]{Chapter4/Figs/reaction_bo.png}}
\caption{\textsc{bo} performance. Standard error confidence interval from 50 random initialisations, 20 for Buchwald-Hartwig reactions. Marginal density plots for the trace shown on the right axis. Data for Buchwald-Hartwig plot produced by Bojana Rankovic.}
\label{bayesopt}
\end{figure*}

\section{Conclusions}

This chapter introduces GAUCHE, a library for GAUssian Processes in CHEmistry with the aim of providing tools for UQ and \textsc{bo} that may hopefully be deployed for screening in laboratory settings. Future work, will seek to:

\begin{enumerate}
\setlength\itemsep{0.3em}
    \item Expand the range of \textsc{gp} kernels considered, most notably to include \textit{deep kernels} based on GNN embeddings.
    \item Perform more extensive benchmarking for UQ and active learning against models such as BNNs.
    \item Exploit the benefits of the Autodiff framework to facilitate the learning of graph kernel hyperparameters through the \textsc{gp} marginal likelihood.
    \item Broaden the application domains considered by GAUCHE to include examples in protein engineering.
    \item Investigate more sophisticated \textsc{gp}-based optimization and active learning loops in chemistry applications \citep{eyke2020iterative}, featuring ideas from batch \citep{gonzalez2016batch}, multi-task \citep{swersky2013multi}, multi-fidelity \citep{moss2020mumbo}, multi-objective \citep{daulton2020differentiable}, controllable experimental noise \citep{moss2020bosh}, or quantile \citep{torossian2020bayesian} optimisation.
\end{enumerate}

\nomenclature[Z-GAN]{GAN}{Generative Adversarial Networks}
\nomenclature[Z-DFT]{DFT}{Density Functional Theory}
\nomenclature[Z-KS-DFT]{KS-DFT}{Kohn-Sham Density Functional Theory}
\nomenclature[Z-TD-DFT]{TD-DFT}{Time-Dependent Density Functional Theory}
\nomenclature[Z-SMILES]{SMILES}{Simplified Molecular-Input Line-Entry System}
\nomenclature[Z-SELFIES]{SELFIES}{Self-Referencing Embedded Strings}
\nomenclature[Z-ECFP]{ECFP}{Extended Connectivity Fingerprints}
\nomenclature[Z-OHE]{OHE}{One-Hot Encoding}
\nomenclature[Z-DRFP]{DRFP}{Differential Reaction Fingerprint}
\nomenclature[Z-AL]{AL}{Active Learning}
\nomenclature[Z-BNN]{BNN}{Bayesian Neural Networks}
\nomenclature[Z-ASAP]{ASAP}{Automatic Selection and Prediction}
\nomenclature[Z-RKHS]{RKHS}{Reproducing Kernel Hilbert Space}
\nomenclature[Z-UQ]{UQ}{Uncertainty Quantification}
\nomenclature[Z-RMSE]{RMSE}{Root Mean Square Error}
\nomenclature[Z-NLPD]{NLPD}{Negative Log Predictive Density}
\nomenclature[Z-MSLL]{MSLL}{Mean Standardised Log Loss}
\nomenclature[Z-QCE]{QCE}{Quantile Coverage Error}



