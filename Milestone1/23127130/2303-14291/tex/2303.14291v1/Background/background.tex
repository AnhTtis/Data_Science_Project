


\chapter{Background}
\chapterimage[height=160pt]{Background/Figs/sqexp_small_length.png}

\ifpdf
    \graphicspath{{Background/Figs/Raster/}{Background/Figs/PDF/}{Background/Figs/}}
\else
    \graphicspath{{Background/Figs/Vector/}{Background/Figs/}}
\fi



In this chapter the requisite background is provided on Gaussian processes (Chapters 3, 4, 5 and 6) and Bayesian optimisation (Chapters 4 and 6).

\section{Gaussian Processes}
\label{intro_to_gps}



In the context of machine learning, a Gaussian process (\textsc{gp}) is a Bayesian nonparametric model for functions. \textsc{gp}s are attractive models when limited data is available, a setting common to many areas of the natural sciences, with even notable deep learning experts voicing a preference for \textsc{gp}s in the small data regime \citep{2011_Bengio}. Furthermore, \textsc{gp}s possess several important properties for the applications in this thesis:

\begin{enumerate}
    \item \textbf{Bayesian optimisation:} \textsc{gp}s have few hyperparameters that need to be determined by hand which lends itself well to the repeated surrogate model hyperparameter optimisation required by Bayesian optimisation.
    \item \textbf{Astronomical time series:} For astronomical time series, where noise processes are often well understood, it is possible to incorporate this knowledge into the design of the \textsc{gp} model.
    \item \textbf{Molecules:} \textsc{gp}s maintain uncertainty estimates over molecular property values through exact Bayesian inference. Uncertainty estimates are particularly important when prioritising molecules for screening experiments.
\end{enumerate}







\noindent A Gaussian process (\textsc{gp}) may be defined as a collection of random variables, any finite subset of which have a joint Gaussian distribution \citep{2006_Rasmussen}. In the cases considered in this thesis, the random variables represent the value of the function $f(\mathbf{x})$ at location $\mathbf{x}$. A stochastic process $f$ that follows a \textsc{gp} is written as

\begin{equation}
f(\mathbf{x}) \sim \mathcal{GP}\big(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x'})\big).
\end{equation}


\noindent The inputs to the \textsc{gp} may be scalars (e.g. time points in Chapter 3) or vectors (e.g. molecular representations in Chapters 4 and 5). In the current presentation we assume vector inputs $\mathbf{x} \in \mathbb{R}^d$ and we seek to perform Bayesian inference over the latent function $f$ that represents the mapping between the inputs $\{\mathbf{x_1}, \dotsc , \mathbf{x_N}\}$ and their function values $\{f(\mathbf{x_1}), \dotsc , f(\mathbf{x_N})\}$. The \textsc{gp} is characterised by a mean function,



\begin{equation}
m(\mathbf{x}) = \mathbb{E}[f(\mathbf{x})],
\end{equation}

\noindent and a covariance function

\begin{equation}
k(\mathbf{x}, \mathbf{x'}) = \mathbb{E}[(f(\mathbf{x}) - m(\mathbf{x}))(f(\mathbf{x'}) - m(\mathbf{x'}))].
\end{equation}

\noindent In the absence of prior information on trends in the data, the mean function is typically set to zero following standardisation of the outputs. Standardisation, in this case refers to the common practice of subtracting the mean and dividing by the standard deviation of the data when fitting the \textsc{gp} in order to facilitate the identification of appropriate hyperparameters \citep{2008_Murray}. The standardisation is reversed once the fitting procedure is complete in order to obtain predictions on the original scale of the data. $m(\mathbf{x}) \equiv \mathbf{0}$ will be assumed henceforth for the sake of the current presentation. The covariance function computes the pairwise covariance between two random variables (function values). In the \textsc{gp} literature, the covariance function is commonly referred to as the kernel. Informally, the kernel is responsible for determining the properties of the functions which the \textsc{gp} is capable of fitting e.g. smoothness and periodicity. The inductive bias created by the choice of kernel is an important consideration in \textsc{gp} modelling.

\begin{figure*}[h]
\centering
\subfigure[SQE small lengthscale ]{\label{fig:homo}\includegraphics[width=0.48\textwidth]{Background/Figs/sqexp_small_length.png}}
\subfigure[SQE large lengthscale]{\label{fig:het}\includegraphics[width=0.48\textwidth]{Background/Figs/sqexp_long_length.png}} 
\caption{\textsc{gp}s with small and large lengthscales.}
\label{fig:gp_kern}
\end{figure*}

\subsection{Kernels}

\noindent The most widely-known kernel is the squared exponential (SQE) or radial basis function (RBF) kernel,

\begin{equation}
\label{equation:sqe_}
k_{\text{SQE}}(\mathbf{x}, \mathbf{x'}) = \sigma_{f}^2 \exp\bigg(\frac{-\norm{\mathbf{x} - \mathbf{x'}}^{2}}{2\ell^2}\bigg),
\end{equation}

\noindent where $\norm{\cdot}$ is the Euclidean norm, $\sigma_{f}^2$ is the signal amplitude hyperparameter (vertical lengthscale) and $\ell$ is the (horizontal) lengthscale hyperparameter. Although \autoref{equation:sqe_} is written with a single lengthscale shared across dimensions, for multidimensional input spaces it is possible to optimise a lengthscale per dimension. We will adopt the notation of $\theta$ to represent the set of kernel hyperparameters. An illustration of \textsc{gp}s with different lengthscales is given in \autoref{fig:gp_kern}. It has been argued by \cite{2012_Stein} that the smoothness assumptions of the SQE kernel are unrealistic for many physical processes. As such, kernels such as the Matérn,


\begin{equation}
k_{\text{Matérn}}(\mathbf{x}, \mathbf{x'}) = \frac{2^{1-\nu}}{\Gamma(\nu)}\bigg(\frac{\sqrt{2\nu}-\norm{\mathbf{x} - \mathbf{x'}}}{\ell}\bigg)^\nu K_{\nu}\bigg(\frac{\sqrt{2\nu}-\norm{\mathbf{x} - \mathbf{x'}}}{\ell}\bigg),
\end{equation}

\noindent are more commonly seen in the machine learning literature. Here $K_{\nu}$ is a modified Bessel function of the second kind, $\Gamma$ is the gamma function and $\nu$ is a non-negative hyperparameter of the kernel which is typically taken to be either $\frac{3}{2}$ or $\frac{5}{2}$ \citep{2006_Rasmussen}. The lengthscale hyperparameter $\ell$ can be thought of loosely as a decay coefficient for the covariance between inputs as they become increasingly far apart in the input space; the further apart the inputs are, the less correlated they will be. The rational quadratic (RQ) kernel is defined as

\begin{equation}
k_{\text{RQ}}(\mathbf{x}, \mathbf{x'}) = \bigg(1 + \frac{\norm{\mathbf{x} - \mathbf{x'}}^{2}}{2\alpha\ell^2}\bigg)^{-\alpha},
\end{equation}

\noindent where $\alpha, \ell > 0$. The RQ kernel can be viewed as a scale mixture of SQE kernels with different characteristic lengthscales. A comparison of the functions drawn from \textsc{gp}s with SQE and Matérn $\frac{5}{2}$ kernels is given in \autoref{fig:gp_kern2}. The aforementioned kernels are defined over continuous input spaces and are used in Chapters 3 and 6. For discrete input spaces such as molecular representations it is necessary to define bespoke kernels which will be introduced in Chapters 4 and 5.



\begin{figure*}[h]
\centering
\subfigure[SQE]{\label{fig:homo}\includegraphics[width=0.48\textwidth]{Background/Figs/sqexp_small_length.png}}
\subfigure[Matérn $5/2$]{\label{fig:het}\includegraphics[width=0.48\textwidth]{Background/Figs/matern52_small_length.png}} 
\caption{A comparison of the SQE and Matérn $\frac{5}{2}$ kernels.}
\label{fig:gp_kern2}
\end{figure*}

\subsection{Predictions}

To obtain the predictive equations of \textsc{gp} regression, a mean function $m(\mathbf{x}) \equiv \mathbf{0}$ and kernel $k$ are specified and a \textsc{gp} prior $p$ is placed over $f$,

\begin{equation}
p(f(\mathbf{x})| \theta) = \mathcal{GP}\big(\mathbf{0}, K_{\theta}(X, X')\big).
\end{equation} 

\noindent The notation $K_{\theta}(X, X')$ denotes a kernel matrix with entries $[K]_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$ and the subscript notation is chosen to indicate the dependence on the set of hyperparameters $\theta$ (e.g. the signal variance $\sigma_f$ and lengthscale $\ell$ in \autoref{equation:sqe_}). We suppress the explicit dependence on $\theta$ in the subsequent notation. It is also necessary to specify a likelihood function 

\begin{equation}
p(y_i | f(\mathbf{x}_i)),
\end{equation}

\noindent which depends on $f(\mathbf{x}_i)$ only and is typically taken to be Gaussian i.e. $p(y_i | f(\mathbf{x}_i)) = \mathcal{N}(y_i | f(\mathbf{x}_i), \sigma_y^2)$. The noise level $\sigma_y^2$ is most frequently assumed to be homoscedastic, i.e. constant across the input domain. In Chapter 6, heteroscedastic (input-dependent) noise is considered by introducing a dependence $\sigma_y^2(\mathbf{x})$. The interpretation of $\mathbf{y}_i$ is a noise-corrupted observation of the latent function $f(\mathbf{x}_i)$. Once data $\{X, \mathbf{y}\}$ has been observed, where $X = \{\mathbf{x}_i\}_{i=1}^N$ and $\mathbf{y} = \{y_i\}_{i=1}^N$, the joint prior distribution over the observations $\mathbf{y}$ and the predicted function values $\mathbf{f}_*$ at test locations $X_*$ may be written

\begin{equation}
\label{eq:joint_prior}
    \begin{bmatrix} 
        \mathbf{y} \\
        \mathbf{f_*} \\
    \end{bmatrix}
    \sim
    \mathcal{N}
    \bigg(0,
    \begin{bmatrix}
        K(X, X) + \sigma_{y}^2 I & K(X, X_*)\: \\
        K(X_*, X)\phantom{+ \: \: \sigma{y}^2} & K(X_*, X_*)
    \end{bmatrix}
    \bigg),
\end{equation}

\noindent where $\mathcal{N}$ is the multivariate Gaussian probability density function and $I\sigma_{y}^2$ represents the variance of iid Gaussian noise on the observation vector $\mathbf{y}$. The joint prior in \autoref{eq:joint_prior} may be conditioned on the observations through

\begin{equation}
p(\mathbf{f_*}| \mathbf{y}) = \frac{p(\mathbf{f_*}, \mathbf{y})}{p(\mathbf{y})},
\end{equation}

\noindent which enforces that the joint prior agrees with the observations $\mathbf{y}$. The posterior predictive distribution is then

\begin{equation}
p(\mathbf{f_*}| X, \mathbf{y}, X_*) = \mathcal{N}\big(\mathbf{\bar{f}_*}, \text{cov}(\mathbf{f_*})\big),
\end{equation}

\noindent with predictive mean at test locations $X_*$,

\begin{equation}
\mathbf{\bar{f_*}} = K(X_*, X)[K(X, X) + \sigma_{y}^2 I]^{-1} \mathbf{y},
\end{equation}

\noindent and predictive uncertainty

\begin{equation}
\text{cov}(\mathbf{f_*}) = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_{y}^2 I]^{-1} K(X, X_*).
\end{equation}

\noindent Analysing the form of this expression one may notice that the first term $K(X_*, X_*)$ in the expression for the predictive uncertainty $\text{cov}(\mathbf{f}*)$ may be viewed as the prior uncertainty and the second term $K(X_*, X)[K(X, X) + \sigma_{y}^2 I]^{-1} K(X, X_*)$  can be thought of as a subtractive factor that accounts for the reduction in uncertainty when observing the data points $\mathbf{y}$. An illustration is given in \autoref{fig:gp_post} of the posterior predictive distribution updates following data observation. 


\begin{figure*}[h]
\centering
\fbox{\subfigure{\label{fig:post1}\includegraphics[width=0.415\textwidth]{Background/Figs/post1.png}}}
\fbox{\subfigure{\label{fig:post2}\includegraphics[width=0.486\textwidth]{Background/Figs/post2.png}}}

\vspace{0.75cm}

\fbox{\subfigure{\label{fig:post4}\includegraphics[width=0.5\textwidth]{Background/Figs/post4.png}}}
\caption{An illustration of the \textsc{gp} posterior update on fitting 1, 2 and 4 data points (blue). The posterior distribution encodes the distribution over possible functions that may explain the data.}
\label{fig:gp_post}
\end{figure*}

\subsection{Training}
\label{gp_training}

An important objective function for training \textsc{gp}s is the log marginal likelihood or evidence \citep{1992_MacKay},

\begin{align}
\label{equation: log_lik_}
\log p(\mathbf{y}| X, \theta) =&  \underbrace{-\frac{1}{2} \mathbf{y}^{\top}(K_{\theta}(X, X) + \sigma_{y}^2I)^{-1} \mathbf{y}}_\text{encourages fit with data} \\ 
&\underbrace{-\frac{1}{2} \log | K_{\theta}(X, X) + \sigma_{y}^2 I |}_\text{controls model capacity} -\frac{N}{2} \log(2\pi) \nonumber.
\end{align}


\noindent $N$ is the number of observations and $\theta$ again represents the set of kernel hyperparameters to be optimised under the objective. The two terms in the expression for the log marginal likelihood embody Occam's Razor \citep{2001_Rasmussen} in their preference for selecting the simplest models that explain the data well as illustrated in \autoref{occam}. The first term in \autoref{equation: log_lik_} penalises functions that do not fit the data adequately whereas the second term acts as a regulariser, disfavouring overly complex models. The negative log marginal likelihood (NLML) is the \textsc{gp} training objective for all experiments performed in this thesis.

\begin{figure}[!htbp]
    \begin{center}
        \includegraphics[width=0.7\textwidth]{Background/Figs/occam.png}
    \end{center}
    \caption{An illustration of the Bayesian Occam’s razor effect introduced by \cite{2003_MacKay}. If models are interpreted as probability distributions over observations $y$, the x-axis may be viewed as the space of possible datasets. The simple model (blue) explains datasets inside $S$ well, but not outside. The complex model (red) explains datasets outside $S$ better, but worse inside $S$. The probability density in $S$ must be lower so as to explain the datasets outside $S$. Thus, if the dataset one wishes to model lies inside $S$, Occam's razor assigns preference to the simpler model.}
    \label{occam}
\end{figure}

\subsection{Bayesian Model Selection}

One desirable property of \textsc{gp}s, and Bayesian models in general, is the ability to carry out hierarchical modelling \citep{1992_MacKay_Hierarchical}. The three tiers of the modelling hierarchy are:

\begin{enumerate}
    \item Model Parameters
    \item Model Hyperparameters
    \item Model Structures
\end{enumerate}

\noindent In the case of the nonparametric \textsc{gp} framework, model parameters do not have the same meaning as in parametric Bayesian models and are instead obtained from the posterior distribution over functions. Model hyperparameters consist of parameters of the kernel function such as signal amplitudes and lengthscales as well as the likelihood noise. At the level of model structures, the fit achieved by different kernels can be quantitatively assessed by comparing the values of the optimised NLML objective permitting Bayesian model selection, a procedure that is used in Chapter 3. The next discussion point to be considered is an important application of \textsc{gp}s in mathematical optimisation.

\section{Bayesian Optimisation}

Bayesian optimisation (\textsc{bo})~\citep{1962_Kushner, 1964_Kushner, 1975_Mockus, 1975_Zhilinskas, 1978_Mockus} is a data-efficient methodology for solving black-box optimisation problems.

\subsection{Black-Box Optimisation}

In many problems in science and engineering we are interested in solving global optimisation problems of the form

\begin{equation}
    \label{Eq:ProbOne_}
    \mathbf{x}^{\star} = \arg\max_{\mathbf{x} \in \mathcal{X}} f(\mathbf{x}),
\end{equation}


\noindent where $f:\mathcal{X} \rightarrow\mathbb{R}$ is a function over an input domain $\mathcal{X}$ which is typically a compact subset of $\mathbb{R}^d$ (Chapter 6) but may also be non-numeric in the case of molecular representations such as graphs and strings (Chapter 4). \autoref{Eq:ProbOne_} is also a black-box optimisation problem in the sense that it possesses the following properties:

\begin{enumerate}
    \item Black-Box Objective: We do not have the analytic form of $f$ nor do we have access to its gradients. We can, however, evaluate $f$ pointwise anywhere in the input domain $\mathcal{X}$.
    \item Expensive Evaluations: Choosing an input $\mathbf{x}$ and evaluating $f(\mathbf{x})$ takes a very long time or incurs a large financial cost.
    \item Noise: The evaluation of a given $\mathbf{x}$ is a noisy process. In addition, this noise may vary across $\mathcal{X}$, making the underlying process heteroscedastic.
\end{enumerate}

A motivating example is molecular property optimisation where the input domain $\mathcal{X}$ is a set of molecular graphs $\{ \mathbf{x} : \mathbf{x} \in \mathcal{X}\}$ and the black-box function $f(\mathbf{x})$ is the property of the molecule to be optimised. $f$ maps a molecule to its property, but its analytic form is unknown and so instead $f$ must be queried through experiment by synthesising a molecule and measuring the value of its property under $f$. This is a time-consuming and financially expensive process. In addition, the measurement process using laboratory equipment is typically noisy.


\subsection{Solution Methods}

In the absence of an analytic form for the function to be optimised, strategies for solving black-box optimisation problems tend to proceed by sequentially evaluating the black-box function until the global optimum is found or the evaluation budget is exhausted. Such strategies may be represented by the abstract blueprint of sequential optimisation outlined in Algorithm~\ref{alg:sequential}.

\begin{algorithm}
\caption{Sequential Optimisation}
\begin{algorithmic}
      \State \textbf{input}: initial dataset $\mathcal{D}$ \algorithmiccomment{may be empty}
      \State \textbf{repeat}
            \State \hspace{\algorithmicindent} $\mathbf{x} \gets \text{Policy}(\mathcal{D})$ \algorithmiccomment{select the next input}
            \State \hspace{\algorithmicindent} $y \gets \text{Evaluate}(\mathbf{x})$ \algorithmiccomment{evaluate the black-box at the chosen input}
            \State \hspace{\algorithmicindent $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{x}, y)\}$} \algorithmiccomment{update the dataset}
            
      \State \textbf{until} termination condition reached \algorithmiccomment{e.g. evaluation budget exhausted}
      \State \textbf{return} $\mathcal{D}$

\end{algorithmic}
\label{alg:sequential}
\end{algorithm}

Sequential optimisation algorithms differ in their choice of policy, or in other words, how they make use of the dataset of evaluations $\mathcal{D}$. Strategies may be non-adaptive in the sense that they ignore $\mathcal{D}$ completely, or they may be adaptive in the sense that they use the information about the black-box function stored within $\mathcal{D}$ to inform the selection of the next input $\mathbf{x}$ \citep{2022_Garnett}. Some of the most relevant solution methods for black-box optimisation include:



\noindent \textbf{Grid Search:} Perhaps the most well-known strategy for black-box optimisation problems, such as machine learning hyperparameter tuning, is grid search. Grid search is a deterministic, non-adaptive strategy where the policy consists of an exhaustive search through the input domain $\mathcal{X}$ by manually specifying a subset of inputs to query. Typically the manually-specified inputs are evenly spaced throughout the input domain and hence assume the form of a \say{grid}. Grid search suffers from the curse of dimensionality \citep{1957_Bellman} since the number of inputs to evaluate grows exponentially as a function of the dimensionality of $\textbf{x}$. Grid search is still a popular strategy in practice, however, due to its ease of implementation and the fact that it is \say{embarrassingly parallel} in so far as evaluations tend to be independent of each other.

\noindent \textbf{Random Search:} This stochastic, non-adaptive strategy consists of draws from a uniform density over the input domain $\mathcal{X}$. It has been demonstrated empirically that in high dimensions, random search can often outperform grid search due to its robustness to non-informative dimensions of the input space \citep{2012_Bergstra}. Random search is used as a baseline strategy in Chapters 4 and 6. 









\noindent \textbf{Bayesian Optimisation:} A third solution method is an adaptive strategy where the policy is derived from Bayesian decision theory \citep{2005_De_Groot, 1985_Berger, 2007_Robert} and formalises the approach to decision-making under uncertainty with respect to the unknown objective function. \textsc{bo}, which is the principal subject of Chapter 6 and plays a major role in Chapter 4, has recently achieved notable and widely-publicised success as a component of AlphaGo \citep{2018_Yutian} as well as across applications including chemical reaction optimisation \citep{2021_Shields}, robotics \citep{2016_Calandra}, and machine learning hyperparameter optimisation \citep{2021_Turner, 2020_Rivers}. \textsc{bo} will be the focus from hereon in.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=1\textwidth]{Background/Figs/bayesopt_illustration5.png}
    \end{center}
    \caption{An illustration of the Bayesian optimisation algorithm.}
    \label{bayesopt_illustration}
\end{figure}

\subsection{The Bayesian Optimisation Algorithm}

The \textsc{bo} algorithm, illustrated in Algorithm~\ref{alg:bayesian}, implements the policy from Algorithm~\ref{alg:sequential} through the use of two components:

\begin{algorithm}
\caption{Bayesian Optimisation}
\begin{algorithmic}
      \State \textbf{input}: initial dataset $\mathcal{D}$ \algorithmiccomment{may be empty}
      \State \textbf{repeat}
      
            \State choose $\mathbf{x}$ by optimising $\alpha$, the acquisition function
      
            \State \hspace{\algorithmicindent} \begin{equation*}\mathbf{x} \gets          \operatorname*{arg\,max}_{\mathbf{x} \in \mathcal{X}} \alpha(\mathbf{x}; \mathcal{D})\end{equation*}
         
            \State \hspace{\algorithmicindent} $y \gets \text{Evaluate}(\mathbf{x})$ \algorithmiccomment{evaluate the black-box at the chosen input}
            \State \hspace{\algorithmicindent $\mathcal{D} \gets \mathcal{D} \cup \{(\mathbf{x}, y)\}$} \algorithmiccomment{update the dataset and surrogate model}
            
      \State \textbf{until} termination condition reached \algorithmiccomment{e.g. evaluation budget exhausted}
      \State \textbf{return} $\mathcal{D}$

\end{algorithmic}
\label{alg:bayesian}
\end{algorithm}



\noindent \textbf{Surrogate Model:} A flexible probabilistic model that captures the prior belief about the behaviour of the black-box objective $f(\mathbf{x})$. A probabilistic model is necessary to ensure that uncertainty in the values of the black-box objective is maintained across the design space. This uncertainty measure is then used to inform the data collection policy known as the acquisition function. When a new data point is collected, the surrogate model is updated by means of re-training.

\noindent \textbf{Acquisition Function:} The acquisition function $\alpha(\mathbf{x}, \mathcal{D})$ determines the next input on a given iteration of \textsc{bo} by leveraging the uncertainty estimates of the surrogate model to trade off exploration and exploitation. It is beneficial to explore regions of the design space where the value of the objective is unknown, yet with a finite budget of function evaluations it is desirable to exploit the knowledge acquired to locate an input close to the global optimum of the function. From a computational standpoint, the acquisition function should be cheaper to evaluate relative to the black-box function. It should also be easy to optimise \citep{2018_Wilson, 2020_Grosnit, 2020_Schweidtmann}.

The pseudocode for \textsc{bo} in Algorithm~\ref{alg:bayesian} does not represent a single instantiation of an algorithm but rather a class of algorithms reflecting the broad range of choices available for both the surrogate model and the acquisition function. The set of criteria for choosing the surrogate model and the acquisition function will now be discussed.









\subsection{The Surrogate Model}

The desiderata for the surrogate model in \textsc{bo} are often related to the quality of the posterior distribution and the scalability of the model. In an idealised scenario, viewing Bayesian inference as an optimal calculus for dealing with incomplete information \citep{2010_Turner, 1961_Cox, 2003_Jaynes, 2003_MacKay}, one would obtain uncertainty estimates using full Bayesian inference over the surrogate model posterior. Full Bayesian inference is computationally demanding however and can be infeasible if the \textsc{bo} problem features a large dataset or a long horizon of function evaluations. To date, \textsc{gp}s have been the model of choice for \textsc{bo} on small datasets due to the ability to perform full Bayesian inference. \textsc{gp} surrogates have the following strengths and weaknesses from the point of view of \textsc{bo}:


\paragraph{Strengths}

\begin{enumerate}
    \item Full Bayesian inference, admits a closed-form posterior predictive distribution via exact inference. In contrast, approximate inference methods run the risk of degrading the quality of the uncertainty estimates \citep{2020_Foong}. The importance of uncertainty estimate quality in obtaining strong empirical performance is regularly emphasised in the \textsc{bo} literature \citep{2015_Shahriari, 2022_Garnett}.
    \item We can perform Bayesian model selection at the hyperparameter level meaning that we are more robust to overfitting. This is facilitated by an analytic form for the marginal likelihood.
    \item Few of the \textsc{gp}s hyperparameters needs to be determined by hand for example through hyperparameter search routines. This makes \textsc{gp}s well-suited to problems such as \textsc{bo} in which running hyperparameter search per iteration of the \textsc{bo} loop is not practically feasible \citep{2003_MacKay}. 
\end{enumerate}

\newpage

\paragraph{Weaknesses}

\begin{enumerate}
    \item Common choices of \textsc{gp} kernels are stationary kernels, meaning they cannot accurately model situations in which the complexity of the objective function varies in different regions of the input space. While non-stationary kernels, warping functions \citep{2020_Rivers, 2019_Balandat}, deep \textsc{gp}s \citep{2013_Damianou, 2021_Hebbal}, and normalising flows \citep{2020_Maronas} are potential solutions, they introduce additional complexity into the \textsc{bo} algorithm.

    \item The \textsc{gp} marginal distribution is not heavy-tailed. If outlier detection is a concern for example, one may wish to employ a heavier-tailed distribution such as the student T-process of \cite{2014_Shah} which has shown some success as a surrogate for \textsc{bo} \citep{2017_Cantin}.
    
    \item The observation model assumes homoscedastic Gaussian noise. While modifications to the standard \textsc{gp} framework exist to capture more complex noise distributions \citep{2021_Griffiths, 2021_Makarova}, they likely require more data in order to operate effectively.

    \item The most frequently cited downside of the \textsc{gp} framework for \textsc{bo} is the computational complexity of performing full Bayesian inference. Computing the inverse of the covariance matrix $[K(\mathbf{X}, \mathbf{X}) + I\sigma^2_y]^{-1}$ is $O(N^3)$ in the number of data points $N$. This covariance matrix appears in the expression for the marginal likelihood in addition to the predictive mean and covariance. A mitigating factor is that, for a fixed set of kernel hyperparameters, the Cholesky decomposition of this matrix may be computed once and stored, yielding a complexity of $O(N^2)$ for future predictions. In \textsc{bo}, however, the kernel hyperparameters are recomputed each time a new data point is collected. The $O(N^3)$ complexity cannot be avoided in this instance. Scalable surrogate model alternatives such as deep neural networks (DNNs) \citep{2015_Snoek, 2016_Springenberg, 2018_Perrone, 2021_White}, sparse \textsc{gp}s \citep{2018_Design, 2020_Griffiths}, and transformers \citep{2022_Maraval} have been trialled but face challenges in terms of the quality of the model uncertainty estimates.
    
    \item \textsc{gp}s often struggle to model functions in high-dimensional, continuous input spaces. In as little as $10$ input dimensions, the predictive capabilities of \textsc{gp}s can be impaired because the covariance function stipulates that inputs separated by more than a few lengthscales are negligibly correlated \citep{2014_Garnett}. As such, the majority of the input domain $\mathcal{X}$ may be uncorrelated with the observed data making prediction challenging. Some popular approaches in high-dimensional spaces include embedding methods such as variational autoencoders (VAEs) which seek to learn a low-dimensional embedding of the input data \citep{2018_Design, 2020_Griffiths, 2021_Grosnit, 2021_Verma, 2022_Hie, 2022_Maus}.
    
    
\end{enumerate}

In the \textsc{bo} problems considered in this thesis, however, many of the aforementioned limitations of \textsc{gp}s do not apply. The scientific datasets lie in the small data regime due to factors such as the expense of collecting laboratory measurements of synthesised molecules or the limited observational history of celestial objects and so the scalability of the surrogate model is not an issue. Similarly, the only high-dimensional input space considered is that of molecular fingerprints in which each input dimension is binary and so the problem of extrapolation in high-dimensional, continuous input spaces is avoided. The only exception is the case of the attempt to model heteroscedastic noise distributions in Chapter 6. In this case a bespoke heteroscedastic \textsc{gp} surrogate and acquisition function is devised.


\subsection{The Acquisition Function}

A sequential optimisation algorithm such as that defined in Algorithm~\ref{alg:sequential} requires a policy or acquisition function $\alpha:\mathcal{X} \to \mathbb{R}$ to provide a score for each potential observation.

\paragraph{Evaluation of Policies:}

The ideal performance metric for a \textsc{bo} scheme would quantify how close the set of queried inputs were to the global optimum of the black-box function. Regret is one such metric for quantifying optimisation performance that is commonly used in the analysis of optimisation algorithms. While there are many formulations of regret in different contexts, the central idea is to compare the values of the objective function visited during optimisation with the value of the global optimum. The larger the gap between these values is, the more regret is retrospectively incurred. The instantaneous regret is defined as $r_n(\mathbf{x}) = f^* - f(\mathbf{x}_n)$, where $f^*$ is the global optimum of the black-box and $f(\mathbf{x}_n)$ is the value of the function queried at the input $\mathbf{x}_n$ at iteration $n$. Two derived forms of regret are typically used in theoretical analysis of optimisation algorithm performance:

\begin{enumerate}
    \item \textbf{Simple Regret} - gives the instantaneous regret at the final iteration of \textsc{bo} as $r_\tau = f^* - \max f(\mathbf{x}_\tau)$, where $\mathbf{x}_\tau$ represent the set of inputs queried at the terminal iteration $\tau$. This metric has the advantage of not punishing the algorithm for explorative queries early in the search procedure.
    \item \textbf{Cumulative Regret} - is defined as $R_N = \frac{1}{N} \sum_{n=0}^N r_n$, where $r_n$ is the instantaneous regret at iteration $n$. Thus the cumulative regret is an average over all queries. The simple regret can be obtained by taking the last term, $r_N$, in the expression for the cumulative regret as the performance metric and setting $N = \tau$, where $\tau$ is the terminal iteration.
\end{enumerate}

\paragraph{Designing an Optimal Policy:}

In terms of designing an optimal policy, however, the regret metric cannot be used directly since the global optimum $f^*$ is unknown. In this instance a concept from Bayesian decision theory known as the utility $u(a, \psi, \mathcal{D})$ can be applied, where $a$ represents the action i.e. a choice of query location $\mathbf{x}$, $\psi$ represents the uncertain elements in the optimisation problem e.g. the objective function values, and $\mathcal{D}$ is the dataset of input/observation pairs collected so far. Maximising the expected utility of the data returned by the optimisation algorithm at a given iteration, however, requires consideration of the entire remainder of the optimisation query budget. Under such long time horizons the optimal policy becomes prohibitive to compute. Some attempts have been made to approximate an idealised look-ahead policy \citep{2009_Garnett, 2010_Ginsbourger, 2016_Osborne} but in practice most \textsc{bo} policies take the form of acquisition functions; myopic heuristics that attempt to trade off exploration and exploitation. When linked with the probabilistic surrogate model, this translates to greedily selecting queries which have high (for maximisation problems) predictive mean (exploitation) and high predictive variance (exploration).

\newpage

\paragraph{Classes of Acquisition Functions}

While there exist a broad range of acquisition functions \citep{2015_Shahriari, 2020_Grosnit}, a large subset of commonly-used acquisitions can be divided into three classes:

\begin{enumerate}
    \item \textbf{Optimistic Acquisition Functions} - An example of this type of acquisition function is the Upper Confidence Bound (UCB) \citep{2010_Srinivas}. In the bandits literature these methods are described by the term \say{optimism in the face of uncertainty} because they assign higher values to actions with high uncertainty.
    \item \textbf{Improvement-Based Acquisition Functions} - Are defined relative to some incumbent target, typically taken to be the best queried value found so far in the optimisation. Examples of this class include Probability of Improvement (PI) \citep{1964_Kushner} and Expected Improvement (EI) \citep{1971_Saltenis, 1978_Mockus, 1998_Jones}. The EI acquisition will be used and extended in Chapter 6.
    \item \textbf{Information-Based Acquisition Functions} - In these methods, the posterior over the unknown optimiser $\mathbf{x}_*$, induced implicitly by the posterior distribution over objective functions, is used as a means of selecting queries. Instances of this class of acquisition function include Thompson Sampling (TS) \citep{1933_Thompson}, Entropy Search (ES) \citep{2012_Hennig}, Predictive Entropy Search (PES) \citep{2014_Lobato}, General-Purpose Information-Based Bayesian Optimisation (GIBBON) \citep{moss2021gibbon}, and the Informational Approach to Global Optimization (IAGO) \citep{2009_Villemonteix}.
\end{enumerate}

Additionally, ensembles of acquisition functions known as portfolios are popular in practice and may perform better than any individual acquisition function \citep{2011_Hoffman, 2013_Hoffman, 2014_Shahriari, 2020_Rivers}.






\nomenclature[Z-SQE]{SQE}{Squared Exponential}
\nomenclature[Z-RBF]{RBF}{Radial Basis Function}
\nomenclature[Z-RQ]{RQ}{Rational Quadratic}
\nomenclature[Z-NLML]{NLML}{Negative Log Marginal Likelihood}
\nomenclature[Z-DNN]{DNN}{Deep Neural Network}
\nomenclature[Z-VAE]{VAE}{Variational AutoEncoder}
\nomenclature[Z-UCB]{UCB}{Upper Confidence Bound}
\nomenclature[Z-PI]{PI}{Probability of Improvement}
\nomenclature[Z-EI]{EI}{Expected Improvement}
\nomenclature[Z-TS]{TS}{Thompson Sampling}
\nomenclature[Z-ES]{ES}{Entropy Search}
\nomenclature[Z-PES]{PES}{Predictive Entropy Search}
\nomenclature[Z-GIBBON]{GIBBON}{General-Purpose Information-Based Bayesian Optimisation}
\nomenclature[Z-IAGO]{IAGO}{Informational Approach to Global Optimisation}





