
\ifpdf
    \graphicspath{{Appendix3/Figs/}{Appendix3/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Appendix3/Figs/}{Appendix3/Figs/}}
\fi

\chapter{Molecular Discovery with Gaussian Processes}

\section{Sources of Experimental Data}
\label{exp_source}

Properties were collated from the photoswitch literature originally by Aditya Raymond Thawani. Emphasis was placed on obtaining a broad range of functional groups attached to the photoswitch scaffold. The set of literature articles consulted included \citet{d1,data1,data2,data3,data4,data5,data6,data8,2011_Jacquemin,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19}.

\section{Dataset Visualisations}

The choice of molecular representation is known to be a key factor in the performance of machine learning algorithms on molecules \citep{2017_Faber, 2018_Wu, 2020_Faber}. Commonly-used representations such as fingerprint and fragment-based descriptors are high dimensional and as such, it can be challenging to interpret the inductive bias induced by the representation. To visualise the high-dimensional representation space of the Photoswitch Dataset the data matrix was projected to two dimensions using the UMAP algorithm. \citep{2018_UMAP}. The manifolds were compared under the Morgan fingerprint representation and a fragment-based representation computed using RDKit \citep{rdkit}. 512-bit Morgan fingerprints were generated with a bond radius of 2, setting the nearest neighbours parameter in the UMAP algorithm to a value of 50. The resulting visualisation was produced using the ASAP package (available at \href{https://github.com/BingqingCheng/ASAP}{https://github.com/BingqingCheng/ASAP}) and is shown in \autoref{umap}.


\begin{figure}[!htbp]
    \begin{center}
        \includegraphics[width=1.1\textwidth]{Chapter1/Figs/projmaps.png}
    \end{center}
    \caption{a) UMAP and k-PCA projections of the dataset, using Morgan fingerprints, correctly identify clusters of chemically similar molecules. The regions demarcated by dashed black lines are composed of miscellaneous azoheteroarenes; no grouping was noted here due to the limited ($\leq$	10) examples per class. b) Similar projections using RDKit Fragment descriptors fails to identify any such clusters.}
    \label{umap}
\end{figure}

The structure of the manifold located under the Morgan fingerprint representation identifies meaningful subgroups of azophotoswitches when compared to the fragment-based representation. To demonstrate that the finding is due to the representation and not the dimensionality reduction algorithm the manifolds identified by k-PCA using a cosine kernel are included. Both algorithms identify the same manifold structure in the Morgan fingerprint representation. 

\section{Further Experiments}

The subsections below detail further experiments carried out during the design of the machine learning prediction pipeline.

\subsection{Property Prediction}
\label{benchmark_ml}

For representations, 2048-bit Morgan fingerprints with a bond radius of 3, implemented in RDKit, were used \citep{rdkit}. 85-dimensional fragment features computed using the RDKit descriptors module were used. The Dscribe library \citep{2020_Himanen} was used to compute (Smooth Overlap of Atomic Positions) (SOAP) descriptors using a \texttt{rcut} parameter of 3.0, a \texttt{sigma} value of 0.2, a \texttt{nmax} parameter of 12, and a \texttt{lmax} parameter of 8. An REMatch kernel was used with polynomial base kernel of degree 3.0, \texttt{gamma} = 1.0, \texttt{coef0} = 0, \texttt{alpha} = 0.5, and \texttt{threshold} = $1e^{-6}$.

Performance was evaluated on 20 random train/test splits in a ratio of 80/20 using the root mean square error (RMSE), mean absolute error (MAE) and coefficient of determination ($R^2$) as performance metrics, reporting the mean and standard error for each metric (Table \ref{property_pred1}). The following models were evaluated: Random Forest (RF), Gaussian Processes (\textsc{gp}), Attentive Neural Processes (ANP), \citep{2018_Kim} Graph Convolutional Networks (GCNs) \citep{2017_Kipf}, Graph Attention Networks (GATs) \citep{2018_Velickovic}, Directed Message-Passing Neural Networks (DMPNNs) \citep{2019_Yang}, and the following representations: Morgan fingerprints \citep{2010_Rogers}, RDKit fragments \citep{rdkit}, SOAP \citep{2013_Bartok}, the simplified molecular-input line-Entry system (SMILES) \citep{1988_Weininger}, and self-referencing embedded strings (SELFIES) \citep{2020_Krenn}. In addition, a new hybrid representation was introduced and termed \say{fragprints}. Fragprints are formed by concatenating the fragment and fingerprint vectors. For the purpose of the benchmark, hyperparameter selection for \textsc{gp}-based approaches was performed by optimising the marginal likelihood on the train set whereas for other methods cross-validation was performed using the Hyperopt-Sklearn library \citep{2019_Komer} for Sklearn models such as RF, and 1000 randomly sampled configurations for other models.
 
The RF model was trained using scikit-learn \citep{scikit-learn} with 1000 estimators and a maximum depth of 300. A \textsc{gp} was implemented in GPflow \citep{GPflow} using a Tanimoto kernel \citep{2005_Ralaivola, 2020_flowmo} for fingerprint, fragment and fragprint representations, and the subset string kernel of \citet{2020_Moss} (following the exact experimental setup in \citet{2020_flowmo}) for the character-based SMILES and SELFIES representations. Additionally, a multioutput Gaussian process (\textsc{mogp}) was trained based on the intrinsic coregionalisation model (ICM) \citep{2007_Williams} to leverage information in the multitask setting. For all \textsc{gp} models, the mean function was set to be the empirical mean of the data and the kernel variance and likelihood variance treated as hyperparameters, optimising their values under the marginal likelihood. For the ANP, 2 hidden layers of dimension 32 were used for each of the decoder, latent decoder and the deterministic encoder respectively. 8-dimensional latent variables $r$ and $z$ were used and optimisation was run for 500 iterations with the Adam optimiser \citep{2014_Adam} using a learning rate of 0.001. 

For the ANP, principal components regression was performed by reducing the representation dimension to 50. GCNs and GATs were implemented in the DGL-LifeSci library \citep{2020_Li}. Node features included one-hot representations of atom-type, atom degree, the number of implicit hydrogen atoms attached to each atom, the total number of hydrogen atoms per atom, atom hybridisation, the formal charge and the number of radical electrons on the atom. Edge features contained one-hot encodings of bond-type and Booleans indicating the stereogenic configuration of the bond and whether the bond was conjugated or in a ring. For the GCN, two hidden layers with 32 hidden units and ReLU activations were used, applying BatchNorm \citep{2015_Ioffe} to both layers. For the GAT, two hidden layers with 32 units each, 4 attention heads, and an alpha value of 0.2 were used in both layers with ELU activations. A single DMPNN model was trained for 50 epochs with additional normalised 2D RDKit features. All remaining parameters were set to the default values in \citet{2019_Yang}. SchNet \citep{2017_Schutt} was not benchmarked because it is designed for the prediction of molecular energies and atomic forces. All experiments were performed on the CPU of a MacBook Pro using a 2.3 GHz 8-Core Intel Core i9 processor.

Standardisation was applied (by subtracting the mean and dividing by the standard deviation) to the property values in all experiments. The results of the aforementioned models and representations are given in \autoref{property_pred1}. Additional results including Message-passing neural networks (MPNN) \citep{2017_Gilmer}, a black-box alpha divergence minimisation Bayesian neural network (BNN) \citep{2016_Lobato}, and an LSTM with augmented SMILES, SMILES-X \citep{2020_Lambard}, are presented in \autoref{property_pred2}. It should be noted that featurisations using standard molecular descriptors are more than competitive with neural representations for this dataset. The best-performing representation/model pair on the most data-rich \emph{E} isomer $\pi-\pi^{*}$ task was the \textsc{mogp}$^*$-Tanimoto kernel and the introduced hybrid descriptor set \say{fragprints}. Importantly, there is weak evidence that the \textsc{mogp}$^*$ is able to leverage multitask learning in learning correlations between the transition wavelengths of the isomers, a modelling feature that may be particularly useful in the low-data regimes characteristic of experimental datasets. A Wilcoxon signed-rank test \citep{1945_Wilcoxon} is carried out in order to determine whether the performance differential between the \textsc{gp}/fragprints combination and the \textsc{mogp}$^*$/fragprints combination is statistically significant. In this instance, the \textsc{mogp}$^*$ is provided with auxiliary task labels for test molecules where available (i.e. labels for tasks that are not being predicted). The null hypothesis is that there is no significant difference arising from multitask learning. In the case of the \emph{E} isomer $\pi-\pi^*$ transition, the resultant p-value is $0.33$, meaning that the null hypothesis cannot be rejected at the 95\% confidence level. In the case of the \emph{Z} isomer $\pi-\pi^*$ transition, the resultant p-value is $0.06$, meaning also that the null hypothesis cannot be rejected at the 95\% confidence level. In this latter case, however, rejection of the null hypothesis depends on the confidence level threshold specified. As such, it is concluded that only weak evidence is available to support the benefits of multitask learning over single task learning.  

\begin{table}[!htbp]
\caption{Test set performance in predicting the transition wavelengths of the \emph{E} and \emph{Z} isomers. Best-performing models are highlighted in bold. MOGP$^*$ denotes a multioutput \textsc{gp} such that auxiliary task labels (i.e. not the task being predicted) for test molecules are provided to the model where available.}
\label{tab1_photo}
\setlength{\extrarowheight}{2pt}
\begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
\begin{tabular}{l|cccc}
\toprule
& \emph{E} isomer $\pi-\pi{^*}$ (nm) & \emph{E} isomer \emph{n}$-\pi{^*}$ (nm) & \emph{Z} isomer $\pi-\pi{^*}$ (nm) & \emph{Z} isomer \emph{n}$-\pi{^*}$ (nm) \\ \hline
\multicolumn{1}{c|}{\underline{\textbf{RMSE}}} & & & & \\
RF + Morgan & $25.3 \pm \: 0.9$ & $\textbf{10.2} \pm \: \textbf{0.4}$ & $14.0 \pm \: 0.6$ & $11.1 \pm \: 0.4$ \\ 
RF + Fragments & $26.4 \pm \: 1.1$ & $11.4 \pm \: 0.5$ & $17.0 \pm \: 0.8$& $14.2 \pm \: 0.6$ \\
RF + Fragprints & $23.4 \pm \: 0.9$ & $11.0 \pm \: 0.4$ & $14.2 \pm \: 0.6$ &  $11.3 \pm \: 0.6$ \\ 
GP + Morgan & $23.4 \pm \: 0.8$ & $11.4 \pm \: 0.5$ & $13.2 \pm \: 0.7$ & $\textbf{11.0} \pm \: \textbf{0.7}$ \\
GP + Fragments & $26.3 \pm \: 0.8$ & $11.6 \pm \: 0.5$ & $15.5 \pm \: 0.8$ & $12.6 \pm \: 0.5$ \\
GP + Fragprints & $20.9 \pm \: 0.7$ & $11.1 \pm \: 0.5$ & $13.1 \pm \: 0.6$ & $11.4 \pm \: 0.7$ \\
GP + SOAP & $21.0 \pm \: 0.6$ & $22.7 \pm \: 0.6$ & $17.8 \pm \: 0.8$ & $15.0 \pm \: 0.5$ \\ 
GP + SMILES & $26.0 \pm \: 0.8$ & $12.3 \pm \: 0.4$& $12.5 \pm \: 0.5$ &  $11.8 \pm \: 0.6$\\ 
GP + SELFIES & $23.5 \pm \: 0.7$ &$12.9 \pm \: 0.5$  &$14.4 \pm \: 0.5$ & $12.2 \pm \: 0.5$\\ 

MOGP + Morgan & $23.6 \pm \: 0.8$ & $11.7 \pm \: 0.5$ & $15.5 \pm \: 0.6$ & $11.1 \pm \: 0.7$ \\
MOGP + Fragments & $27.0 \pm \: 0.9$ & $11.9 \pm \: 0.6$ & $16.4 \pm \: 0.9$ & $13.1 \pm \: 0.6$ \\
MOGP + Fragprints & $21.2 \pm \: 0.7$ & $11.3 \pm \: 0.5$ & $13.5 \pm \: 0.6$ & $11.4 \pm \: 0.7$ \\

MOGP$^*$ + Morgan & $22.6 \pm \: 0.8$ & $11.6 \pm \: 0.4$ & $12.3 \pm \: 0.7$ & $10.9 \pm \: 0.7$ \\
MOGP$^*$ + Fragments & $26.9 \pm \: 0.8$ & $12.1 \pm \: 0.6$ & $16.2 \pm \: 0.8$ & $13.8 \pm \: 0.6$ \\
MOGP$^*$ + Fragprints & $\textbf{20.4} \pm \: \textbf{0.7}$ & $11.2 \pm \: 0.5$ & $\textbf{11.3} \pm \: \textbf{0.4}$ & $11.4 \pm \: 0.7$ \\

ANP + Morgan & $28.1 \pm \: 1.3$ & $13.6 \pm \: 0.5$ & $13.5 \pm \: 0.6$ & $\textbf{11.0} \pm \: \textbf{0.6}$ \\
ANP + Fragments & $27.9 \pm \: 1.1$ & $13.8 \pm \: 0.9$ & $17.2 \pm \: 0.8$ & $14.1 \pm \: 0.7$ \\
ANP + Fragprints & $27.0 \pm \: 0.8$ & $11.6 \pm \: 0.5$ & $14.5 \pm \: 0.8$ & $11.3 \pm \: 0.7$ \\ 
GCN & $22.0 \pm \: 0.8$  & $12.8 \pm \: 0.8$ & $16.3 \pm \: 0.8$ & $13.1 \pm \: 0.8$\\
GAT & $26.4 \pm \: 1.1$ & $16.9 \pm \: 1.9$ & $19.6 \pm \: 1.0$ & $14.5 \pm \: 0.8$\\
DMPNN & $27.1 \pm \: 1.4$ & $13.9 \pm \: 0.6$ & $17.5 \pm \: 0.7$ & $13.8 \pm \: 0.4$ \\[5pt]\hline \multicolumn{1}{c|}{\underline{\textbf{MAE}}} & & & & \\
RF + Morgan & $15.5 \pm \: 0.5$ & $\textbf{7.3} \pm \: \textbf{0.3}$ & $10.1 \pm \: 0.4$ & $\textbf{6.6} \pm \: \textbf{0.3}$ \\ 
RF + Fragments & $16.4 \pm \: 0.5$ & $8.5 \pm \: 0.3$ & $12.2 \pm \: 0.6$ & $9.0 \pm \: 0.4$ \\ 
RF + Fragprints & $13.9 \pm \: 0.4$ & $7.7 \pm \: 0.3$ & $10.0 \pm \: 0.4$ & $6.8 \pm \: 0.3$ \\ 
GP + Morgan & $15.2 \pm \: 0.4$ & $8.4 \pm \: 0.3$ & $9.8 \pm \: 0.4$ & $6.9 \pm \: 0.3$ \\
GP + Fragments & $17.3 \pm \: 0.4$ & $8.6 \pm \: 0.3$ & $11.5 \pm \: 0.5$ & $8.2 \pm \: 0.3$ \\
GP + Fragprints & $13.3 \pm \: 0.3$ & $8.2 \pm \: 0.3$ & $9.8 \pm \: 0.4$ & $7.1 \pm \: 0.3$ \\
GP + SOAP & $14.3 \pm \: 0.3$ & $19.3 \pm \: 0.5$ & $12.9 \pm \: 0.6$ & $11.4 \pm \: 0.4$ \\
GP + SMILES &$16.6 \pm \: 0.5$ &$8.6 \pm \: 0.3$ & $9.4 \pm \: 0.4$ & $7.4 \pm \: 0.3$ \\ 
GP + SELFIES & $14.7 \pm \: 0.7$ & $8.8 \pm \: 0.3$&  $11.1 \pm \: 0.3$& $8.1 \pm \: 0.2$\\ 

MOGP + Morgan & $15.3 \pm \: 0.4$ & $8.6 \pm \: 0.3$ & $11.9 \pm \: 0.5$ & $7.0 \pm \: 0.3$ \\
MOGP + Fragments & $17.6 \pm \: 0.5$ & $8.8 \pm \: 0.4$ & $12.1 \pm \: 0.6$ & $8.3 \pm \: 0.3$ \\
MOGP + Fragprints & $13.5 \pm \: 0.3$ & $8.3 \pm \: 0.3$ & $10.2 \pm \: 0.5$ & $7.1 \pm \: 0.3$ \\

MOGP$^*$ + Morgan & $14.4 \pm \: 0.4$ & $8.5 \pm \: 0.3$ & $9.6 \pm \: 0.4$ & $6.9 \pm \: 0.4$ \\
MOGP$^*$ + Fragments & $17.2 \pm \: 0.4$ & $8.9 \pm \: 0.3$ & $11.9 \pm \: 0.5$ & $8.5 \pm \: 0.4$ \\
MOGP$^*$ + Fragprints & $\textbf{13.1} \pm \: \textbf{0.3}$ & $8.3 \pm \: 0.3$ & $\textbf{8.8} \pm \: \textbf{0.3}$ & $7.1 \pm \: 0.4$ \\

ANP + Morgan & $17.9 \pm \: 0.7$ & $10.1 \pm \: 0.4$ & $10.0 \pm \: 0.4$ & $7.2 \pm \: 0.3$ \\
ANP + Fragments & $17.4 \pm \: 0.6$ & $9.4 \pm \: 0.4$ & $12.3 \pm \: 0.6$ & $8.9 \pm \: 0.4$ \\
ANP + Fragprints & $18.1 \pm \: 0.5$ & $8.6 \pm \: 0.3$ & $10.4 \pm \: 0.5$ & $7.0 \pm \: 0.3$ \\ 
GCN & $13.9 \pm \: 0.3$ & $8.6 \pm \: 0.3$ & $11.6 \pm \: 0.5$ & $8.6 \pm \: 0.5$ \\
GAT & $18.1 \pm \: 0.7$ & $10.7 \pm \: 0.6$ & $14.4 \pm \: 0.8$ & $10.8 \pm \: 0.7$\\
DMPNN & $17.1 \pm \: 0.8$ & $10.6 \pm \: 0.4$ & $12.8 \pm \: 0.6$ & $9.8 \pm \: 0.3$\\[5pt] \hline \multicolumn{1}{c|}{\emph{\underline{\textbf{R$^{2}$}}}} & & & & \\
RF + Morgan & $0.85 \pm \: 0.01$ & $\textbf{0.80} \pm \: \textbf{0.01}$ & $0.25 \pm \: 0.06$ & $0.36 \pm \: 0.06$ \\ 
RF + Fragments & $0.83 \pm \: 0.01$ & $0.75 \pm \: 0.02$ & $-0.15 \pm \: 0.11$ & $-0.05 \pm \: 0.07$ \\
RF + Fragprints & $0.87 \pm \: 0.01$ & $0.77 \pm \: 0.02$ & $0.23 \pm \: 0.07$ & $0.33 \pm \: 0.06$ \\ 
GP + Morgan & $0.87 \pm \: 0.01$ & $0.76 \pm \: 0.01$ & $0.34 \pm \: 0.05$ &  $\textbf{0.38} \pm \: \textbf{0.05}$ \\
GP + Fragments & $0.84 \pm \: 0.01$ & $0.74 \pm \: 0.02$ & $0.07 \pm \: 0.08$ & $0.19 \pm \: 0.05$ \\
GP + Fragprints & $\textbf{0.90} \pm \: \textbf{0.01}$ & $0.77 \pm \: 0.02$ & $0.35 \pm \: 0.05$ & $0.33 \pm \: 0.05$\\
GP + SOAP & $0.89 \pm \: 0.01$ & $-0.08 \pm \: 0.03$ & $-0.05 \pm \: 0.02$ & $-0.07 \pm \: 0.02$\\
GP + SMILES & $0.84 \pm \: 0.02$& $0.72 \pm \: 0.02$&  $0.39 \pm \: 0.05$&  $0.29 \pm \: 0.04$\\ GP + SELFIES & $0.86 \pm \: 0.01$ & $0.68 \pm \: 0.02$ &$0.20 \pm \: 0.05$ & $0.23 \pm \: 0.04$\\ 

MOGP + Morgan & $0.87 \pm \: 0.01$ & $0.75 \pm \: 0.01$ & $0.06 \pm \: 0.08$ & $0.37 \pm \: 0.05$ \\
MOGP + Fragments & $0.83 \pm \: 0.01$ & $0.73 \pm \: 0.02$ & $-0.05 \pm \: 0.10$ & $0.11 \pm \: 0.06$ \\
MOGP + Fragprints & $0.89 \pm \: 0.01$ & $0.76 \pm \: 0.02$ & $0.30 \pm \: 0.06$ & $0.33 \pm \: 0.05$ \\

MOGP$^*$ + Morgan & $0.88 \pm \: 0.01$ & $0.75 \pm \: 0.01$ & $0.34 \pm \: 0.12$ & $0.39 \pm \: 0.05$ \\
MOGP$^*$ + Fragments & $0.83 \pm \: 0.01$ & $0.72 \pm \: 0.02$ & $-0.06 \pm \: 0.12$ & $0.00 \pm \: 0.08$ \\
MOGP$^*$ + Fragprints & $\textbf{0.90} \pm \: \textbf{0.01}$ & $0.76 \pm \: 0.01$ & $\textbf{0.49} \pm \: \textbf{0.05}$ & $0.33 \pm \: 0.06$ \\

ANP + Morgan & $0.70 \pm \: 0.02$ & $0.66 \pm \: 0.02$ & $0.30 \pm \: 0.06$ & $\textbf{0.38} \pm \: \textbf{0.05}$ \\ 
ANP + Fragments & $0.81 \pm \: 0.01$ & $0.62 \pm \: 0.05$ & $-0.16 \pm \: 0.11$ & $-0.06 \pm \: 0.10$ \\
ANP + Fragprints & $0.83 \pm \: 0.01$ & $0.75 \pm \: 0.01$ & $0.18 \pm \: 0.08$ & $0.35 \pm \: 0.05$ \\ 
GCN & $0.87 \pm \: 0.01$ & $0.66 \pm \: 0.03$ & $-0.41 \pm \: 0.22$ & $-0.92 \pm \: 0.3$\\
GAT & $0.81 \pm \: 0.02$ & $0.57 \pm \: 0.04$ & $0.39 \pm \: 0.17$ & $-1.07 \pm \: 0.4$\\
DMPNN & $0.82 \pm \: 0.02$ & $0.63 \pm \: 0.02$ & $-0.05 \pm \: 0.07$ & $0.11 \pm \: 0.04$\\[5 pt]
\bottomrule
\end{tabular}
\end{adjustbox}
\label{property_pred1}
\end{table}






Results with additional models on the property prediction benchmark for which extensive hyperparameter tuning was not undertaken, are presented in \autoref{property_pred2}. The black-box alpha divergence minimisation BNN was implemented in the Theano library \citep{2016_Theano} and is based on the implementation of \citep{2016_Lobato}. The network has 2 hidden layers of size 25 with ReLU activations. The alpha parameter was set to 0.5, the prior variance for the variational distribution q was set to 1, and 100 samples were taken to approximate the expectation over the variational distribution. For all tasks the network was trained using 8 iterations of the Adam optimiser \citep{2014_Adam} with a batch size of 32 and a learning rate of 0.05. The MPNN was trained for 100 epochs in the case of the \emph{E} isomer $\pi-\pi{^*}$ task and 200 epochs in the case of the other tasks with a learning rate of 0.001 and a batch size of 32. The model architecture was taken to be the library default with the same node and edge features used for the GCN and GAT models in the main paper. The SMILES-X implementation remained the same as that of \citet{2020_Lambard} save for the difference that the network was trained for 40 epochs without \textsc{bo} over model architectures. In the case of SMILES-X 3 random train/test splits were used instead of 20 for the \emph{Z} isomer tasks whereas 2 splits were used for the \emph{E} isomer \emph{n}$-\pi{^*}$ task. For the \emph{E} isomer $\pi-\pi{^*}$ prediction task results are missing due to insufficient RAM on the machine used to run the experiments. 

\begin{table}[!htbp]
\caption{Test set performance in predicting the transition wavelengths of the \emph{E} and \emph{Z} isomers.}
\label{tab2_photo}
\setlength{\extrarowheight}{2pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccc}
\toprule
& \emph{E} isomer $\pi-\pi{^*}$ (nm) & \emph{E} isomer \emph{n}$-\pi{^*}$ (nm) & \emph{Z} isomer $\pi-\pi{^*}$ (nm) & \emph{Z} isomer \emph{n}$-\pi{^*}$ (nm) \\ \hline
\multicolumn{1}{c|}{\underline{\textbf{RMSE}}} & & & & \\
BNN + Morgan & $27.0 \pm \: 0.9$ & $12.9 \pm \: 0.6$ & $13.9 \pm \: 0.6$ & $12.7 \pm \: 0.4$ \\ 
BNN + Fragments & $31.2 \pm \: 1.1$ & $14.8 \pm \: 0.8$ & $16.9 \pm \: 0.8$ & $12.7 \pm \: 0.4$ \\
BNN + Fragprints & $26.7 \pm \: 0.8$ & $13.1 \pm \: 0.5$ & $14.9 \pm \: 0.5$ &  $13.0 \pm \: 0.6$ \\ 
MPNN & $24.8 \pm \: 0.8$ & $12.5 \pm \: 0.6$ & $16.7 \pm \: 0.8$ & $12.8 \pm \: 0.7$ \\
SMILES-X &  & $25.1 \pm \: 4.2$ & $17.8 \pm \: 0.6$ & $14.8 \pm \: 0.9$
\\[5pt]\hline \multicolumn{1}{c|}{\underline{\textbf{MAE}}} & & & & \\
BNN + Morgan & $19.0 \pm \: 0.6$ & $9.9 \pm \: 0.4$ & $10.2 \pm \: 0.5$ & $8.6 \pm \: 0.3$ \\ 
BNN + Fragments & $22.4 \pm \: 0.8$ & $10.6 \pm \: 0.4$ & $12.9 \pm \: 0.6$ & $8.6 \pm \: 0.3$ \\ 
BNN + Fragprints & $19.1 \pm \: 0.6$ & $10.1 \pm \: 0.5$ & $10.8 \pm \: 0.4$ & $9.3 \pm \: 0.5$ \\ 
MPNN & $15.4 \pm \: 0.8$ & $8.6 \pm \: 0.3$ & $11.6 \pm \: 0.6$ & $8.4 \pm \: 0.4$ \\
SMILES-X &  & $20.6 \pm \: 3.1$ & $11.6 \pm \: 1.0$ & $11.2 \pm \: 1.0$ \\ \hline
\multicolumn{1}{c|}{\emph{\underline{\textbf{R$^{2}$}}}} & & & & \\
BNN + Morgan & $0.83 \pm \: 0.01$ & $0.69 \pm \: 0.02$ & $0.23 \pm \: 0.08$ & $0.18 \pm \: 0.05$ \\ 
BNN + Fragments & $0.77 \pm \: 0.01$ & $0.58 \pm \: 0.04$ & $-0.15 \pm \: 0.14$ & $0.18 \pm \: 0.05$ \\
BNN + Fragprints & $0.83 \pm \: 0.01$ & $0.68 \pm \: 0.02$ & $0.14 \pm \: 0.06$ & $0.11 \pm \: 0.08$ \\ 
MPNN & $0.83 \pm \: 0.01$ & $0.63 \pm \: 0.06$ & $-0.70 \pm \: 0.34$ &  $-0.68 \pm \: 0.27$ \\
SMILES-X &  & $-0.44 \pm \: 0.30$ & $-0.08 \pm \: 0.06$ & $-0.09 \pm \: 0.04$ \\
\bottomrule
\end{tabular}}
\label{property_pred2}
\end{table}

\subsection{Prediction Error as a Guide to Representation Selection}
\label{representation}

On the \emph{E} isomer $\pi-\pi{^*}$ transition wavelength prediction task, occasionally marked discrepancies were noted in the predictions made under the Morgan fingerprint and fragment representations. The resultant analysis motivated the expansion of the molecular feature set to include both representations as \say{fragprints}.


\subsection{Impact of Dataset Choice}
\label{sec:big_data}

In this section, the generalisation performance was evaluated for a model trained on the \emph{E} isomer $\pi-\pi{^*}$ values of a large dataset of 6142 out-of-domain molecules (including non-azoarene photoswitches) from \citet{2019_Beard}, with experimentally-determined labels. A RF regressor was (due to scalability issues with the \textsc{mogp} on 6000+ data points) implemented in the scikit-learn library with 1000 estimators and a max depth of 300 on the fragprint representation of the molecules. In \autoref{tab_merge2} results are presented for the case when the train set consists of the large dataset of 6142 molecules and the test set consists of the entire photoswitch dataset. Results are also presented on the original \emph{E} isomer $\pi-\pi{^*}$ transition wavelength prediction task where the train set of each random 80/20 train/test split was augmented with the molecules from the large dataset. The results indicate that the data for out-of-domain molecules provides no benefit for the prediction task and even degrades performance, when amalgamated, relative to training on in-domain data only.

\begin{table}[]\caption{Performance comparison of curated dataset against large non-curated dataset.}\label{tab_merge2}
\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{lllcl}
\toprule
Dataset & Size & RMSE ($\downarrow$) & MAE ($\downarrow$) & $R^2$ ($\uparrow$) \\ \hline
Large Non-Curated & 6142 & $85.2$ & $72.5$ & $-0.66$ \\
Large Non-Curated + Curated & 6469 & $36.9 \pm 1.2$ &  $22.7 \pm 0.7$ & $0.67 \pm 0.02$ \\ 
Curated & 314 & $\textbf{23.4} \pm \textbf{0.9}$ & $\textbf{13.9} \pm \textbf{0.4}$ & $\textbf{0.87} \pm \textbf{0.01}$ \\
\bottomrule
\end{tabular}
\end{table}

Based on these results the importance of designing synthetic molecular machine learning benchmarks with a real-world application in mind is emphasised, as well as the importance of involving synthetic chemists in the curation process. By targeted data collation on a narrow and well-defined region of chemical space where the molecules are in-domain relative to the task, it becomes possible to mitigate generalisation error.

\subsection{Human Performance Benchmark}\label{sec:human_app}

Below in \autoref{tab_new} the full results breakdown of the human performance benchmark study is provided.


\begin{table}[]\caption{Results breakdown for the human expert performance benchmark predicting the transition wavelength (nm) of the \emph{E} isomer $\pi-\pi^*$ transition for 5 molecules. Closest prediction for each molecule is underlined and highlighted in bold. \textsc{mogp} achieves the lowest MAE relative to all individual human participants.}\label{tab_new}
\centering
\setlength{\extrarowheight}{2pt}
\begin{tabular}{l|lllll|l}
\toprule
 & Mol 1 & Mol 2 & Mol 3 & Mol 4 & Mol 5 & MAE ($\downarrow$) \\
True Value & \textbf{329} & \textbf{407} & \textbf{333} & \textbf{540} & \textbf{565} & \\ \midrule
Postdoc 1 & 325 & 360 & 410 & 490 & 490 & 54.7 \\
PhD 1 & 350 & 400 & 530 & 410 & 425 & 93.3 \\
PhD 2 &  380 & 280 & 530 & 600 & 250 & 177.5 \\
Postdoc 2 & \underline{\textbf{330}} & 350 & 500 & 475 & 500 & 66.7 \\
PhD 3 & 325 & 350 & 350 & \underline{\textbf{540}} & 550 & 16.3 \\
Postdoc 3 & 350 & 370 & 520 & 600 & 500 & 97.5 \\
PhD 4 & \underline{\textbf{330}} & 380 & 390 & 520 & 580 & 34.2 \\
Undergraduate 1 & 340 & 420 & 400 & \underline{\textbf{540}} & 570 & 41.8 \\
Postdoc 4 & 321 & 345 & \underline{\textbf{340}} & 500 & 520 & 28.7 \\
PhD 5 & \underline{\textbf{330}} & 360 & \underline{\textbf{340}} & 500 & 520 & 24.2 \\
PhD 6 & 303 & 367 & 435 & 411 & 450 & 78.7 \\
PhD 7 &  280 & 350 & 450 & 430 & 460 & 85.5 \\
PhD 8 &  270 & 390 & 420 & 420 & 440 & 73.8 \\
PhD 9 & \underline{\textbf{330}} & 310 & 462 & 512 & 512 & 55.3 \\ \midrule
\textsc{mogp} & 321 & \underline{\textbf{413}} & 354 & 518 & \underline{\textbf{569}} & \underline{\textbf{11.9}} \\

\bottomrule
 \end{tabular}
 \end{table}

\subsection{Confidence-Error Curves}
\label{conf_error}

An advantage of Bayesian models for the real-world prediction task is the ability to produce calibrated uncertainty estimates. If correlated with prediction error, a model's uncertainty may act as an additional decision-making criterion for the selection of candidates for lab synthesis. To investigate the benefits afforded by uncertainty estimates, confidence-error curves were produced using the \textsc{gp}-Tanimoto model in conjunction with the fingerprints representation. The confidence-error curves for the RMSE and MAE metrics are shown in \autoref{uc_rmse_plots} and \autoref{uc_mae_plots} respectively. The x-axis, confidence percentile, may be obtained simply by ranking each model prediction of the test set in terms of the predictive variance at the location of that test input. As an example, molecules that lie in the 80th confidence percentile will be the 20\% of test set molecules with the lowest model uncertainty. The prediction error is then measured at each confidence percentile across 200 random train/test splits to see whether the model's confidence is correlated with the prediction error. It is observed that across all tasks, the \textsc{gp}-Tanimoto model's uncertainty estimates are positively correlated with prediction error, offering a proof of concept that model uncertainty can be incorporated into the decision process for candidate selection.

\begin{figure*}[!htbp]
\centering
\subfigure[\emph{E} Isomer $\pi-\pi{^*}$]{\label{fig:4ppa}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_rmse_e_iso_pi.png}}  
\subfigure[\emph{E} Isomer \emph{n}$-\pi{^*}$]{\label{fig:4ppb}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_rmse_e_iso_n.png}}
\subfigure[\emph{Z} Isomer $\pi-\pi{^*}$]{\label{fig:4ppc}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_rmse_z_iso_pi.png}}  
\subfigure[\emph{Z} Isomer \emph{n}$-\pi{^*}$]{\label{fig:4ppd}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_rmse_z_iso_n.png}}  
\caption{RMSE confidence-error curves for property prediction using \textsc{gp} regression.}
\label{uc_rmse_plots}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\subfigure[\emph{E} Isomer $\pi-\pi{^*}$]{\label{fig:4pa}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_mae_e_iso_pi.png}}  
\subfigure[\emph{E} Isomer \emph{n}$-\pi{^*}$]{\label{fig:4pb}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_mae_e_iso_n.png}}
\subfigure[\emph{Z} Isomer $\pi-\pi{^*}$]{\label{fig:4pc}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_mae_z_iso_pi.png}}  
\subfigure[\emph{Z} Isomer \emph{n}$-\pi{^*}$]{\label{fig:4pd}\includegraphics[width=0.49\textwidth]{Chapter1/Figs/fingerprints_confidence_curve_mae_z_iso_n.png}}  
\caption{MAE confidence-error curves for property prediction using \textsc{gp} regression.}
\label{uc_mae_plots}
\end{figure*}

\subsection{TD-DFT Benchmark}
\label{spearman_section}

Below, in \autoref{correlation} and \autoref{signed_error} further plots are included analysing the performance of the methods on the TD-DFT performance comparison benchmark. These plots motivated the use of the Lasso-correction to the TD-DFT predictions.

\begin{figure}[p]
  \centering
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/2correlation_MOGP_CAM-B3LYP.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/2correlation_MOGP_PBE0.png}

  \vspace{1cm}

  \includegraphics[width=.45\textwidth]{Chapter1/Figs/correlation_TD-DFT_CAM-B3LYP.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/correlation_TD-DFT_CAM-B3LYP_+_Linear.png}

  \vspace{1cm}

  \includegraphics[width=.45\textwidth]{Chapter1/Figs/correlation_TD-DFT_PBE0.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/correlation_TD-DFT_PBE0_+_Linear.png}

  \caption{Regression plots for each method on the TD-DFT performance comparison benchmark with the Spearman rank-order correlation coefficient given as $\rho$. One may observe that the correlation between predictions and ground truth experimental values increases with the linear Lasso correction to the TD-DFT methods.}
  \label{correlation}
\end{figure}

\begin{figure}[p]
  \centering
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/2error_distribution_MOGP_CAM-B3LYP.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/2error_distribution_MOGP_PBE0.png}

  \vspace{1cm}

  \includegraphics[width=.45\textwidth]{Chapter1/Figs/error_distribution_TD-DFT_CAM-B3LYP.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/error_distribution_TD-DFT_CAM-B3LYP_+_Linear.png}

  \vspace{1cm}

  \includegraphics[width=.45\textwidth]{Chapter1/Figs/error_distribution_TD-DFT_PBE0.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/error_distribution_TD-DFT_PBE0_+_Linear.png}

  \caption{Signed error distributions for each method on the TD-DFT performance comparison benchmark. Signed error is recorded for each heldout molecule in leave-one-out-validation. Gaussian kernel density estimates are overlaid on the histograms. One may observe that the linear Lasso correction for the TD-DFT methods has a centering effect on the error distribution.}
  \label{signed_error}
\end{figure}

\section{Further Screening Details}
\label{exp_app}

Reagents and solvents were obtained from commercial sources (MolPort) and used as supplied. Experimental measurements were performed by Jake L. Greenfield at Imperial College London and below is included his account of the experimental procedure.

\subsection{UV-Vis Absorption Spectroscopy}

UV-Vis absorption spectra were obtained on an Agilent 8453 UV-Visible Spectrophotometer G1103A. A sampler holder with four open faces was used to enable in-situ irradiation (90° to the measurement beam). Samples were prepared in a UV Quartz cuvette with a path length of 10 mm. Solutions of the compounds were prepared in HPLC grade DMSO at a concentration of 25 $\mu\text{M}$.

\subsection{Photoswitching}

Samples were irradiated with a custom-built irradiation set up using 365 nm (3 × 800 mW Nichia NCSU276A LEDs, FWHM 9 nm), 405 nm (3 × 770 mW Nichia NCSU119C LEDs, FWHM 11 nm), 450 nm (3 × 900 mW Nichia NCSC219B-V1 LEDs, FWHM 18 nm), 495 nm (3 × 750 mW Nichia NCSE219B-V1 LEDs, FWHM 32 nm), 525 nm (3 × 450 mW NCSG219B-V1 LEDs, FWHM 38 nm) and 630 nm (3 × 780 mW Nichia NCSR219B-V1 LEDs, FWHM 16 nm) light sources. Samples were irradiated until no further change in the UV-vis absorption spectra was observed, indicating that the Photostationary State (PSS) was reached. The PSS, and the \say{predicted pure Z} spectra was determined using UV-vis following the procedure reported by \citet{Fischer1967}.



\section{Novelty of Screened Candidates relative to The Photoswitch Dataset}

In \autoref{discovered}, for each of the 6 candidates satisfying both performance criteria, some indication as to the novelty of the discovered photoswitch candidates is provided by giving the 3 closest molecules by Tanimoto similarity from the Photoswitch Dataset.

\begin{figure}[p]
  \centering
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/mol5_proxim.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/mol6_proxim.png}

  \vspace{1cm}

  \includegraphics[width=.45\textwidth]{Chapter1/Figs/mol8_proxim.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/mol9_proxim.png}

  \vspace{1cm}

  \includegraphics[width=.45\textwidth]{Chapter1/Figs/mol10_proxim.png}
  \hspace{1cm}
  \includegraphics[width=.45\textwidth]{Chapter1/Figs/mol11_proxim.png}

  \caption{All 6 experimentally-tested candidates satisfying both performance criteria together with the 3 closest molecules by Tanimoto similarity in the Photoswitch Dataset.}
  \label{discovered}
\end{figure}
