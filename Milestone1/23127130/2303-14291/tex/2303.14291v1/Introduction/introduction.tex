
\chapter{Introduction}

\ifpdf
    \graphicspath{{Introduction/Figs/Raster/}{Introduction/Figs/PDF/}{Introduction/Figs/}}
\else
    \graphicspath{{Introduction/Figs/Vector/}{Introduction/Figs/}}
\fi





\section{Motivation}

The past decade has seen deep learning models achieve breakthroughs in computer vision \citep{2012_Krizhevsky}, speech recognition \citep{2013_Graves}, and natural language processing \citep{2017_Vaswani}. In fact, progress on developing deep learning architectures has proceeded so rapidly that, as of 2021, machine learning pioneer Andrew Ng has voiced the opinion that research into improving deep architectures has plateaued, at least in the traditional domains of vision, speech and language. Ng is now calling for a shift in focus towards data-centric AI, arguing that the dataset, as opposed to the model, is now the performance bottleneck in many real-world problems \citep{2021_Ng}.


In the natural sciences, however, model development is by no means a solved problem. Large scientific datasets have existed for some time, such as those generated by the Large Hadron Collider at CERN \citep{2013_Cern}, or the Chemical Universe Database, GDB-17 \citep{2012_Ruddigkeit}, which enumerates 166 billion small molecules. Developing effective models for scientific data is still an active and fast-moving field of research however \citep{2022_Kalinin}. In contrast to artificial data such as images, speech, and text, scientific data can often be inexorably tied to causal paradigms, entailing challenges for purely data-driven approaches seeking to achieve strong out-of-distribution (OOD) performance. Lines of inquiry in this direction include incorporating invariances due to symmetries into deep learning models for proteins and molecules \citep{2021_Jumper, 2020_Hermann}, as well as causal mechanisms for problems in physics \citep{2021_Scholkopf}.  


A further challenge for building performant machine learning models for scientific applications stems from the availability of data. While large datasets in the sciences have undoubtedly been a key driver of research, there are also areas of scientific discovery which will always be limited to small data. Examples include molecular design, where one wishes to predict the properties of a new class of molecule for which few experimental measurements exist, as well as high-energy astrophysics, where one wishes to draw inferences from astronomical time series with short observation periods. In the past years researchers have achieved success in porting breakthroughs in deep learning to large scientific datasets \citep{2019_Bolgar, 2020_Chithrananda, 2022_White}. Deep learning models, however, are known to struggle in small data regimes to the extent that leading deep learning expert Yoshua Bengio previously voiced a preference for a model called a Gaussian process (\textsc{gp}) for small datasets \citep{2011_Bengio}. As such, leveraging them directly for small data scientific discovery could prove to be difficult.  

\textsc{gp}s have received comparatively less attention relative to deep learning over the past decade due to a variety of factors including a higher barrier to entry in terms of the mathematical background required to use them, fewer open-source software implementations, and perhaps most importantly, concerns over the ability of \textsc{gp}s to carry out representation learning, a stance summed up in the following prescient quote from \cite{2003_MacKay} which foreshadows some of the challenges currently encountered in supervised deep learning for the sciences.  

\begin{displayquote}
"According to the hype of 1987, neural networks were meant to be intelligent models that discovered features and patterns in data. Gaussian processes in contrast are simply smoothing devices. How can Gaussian processes possibly replace neural networks? Were neural networks over-hyped, or have we underestimated the power of smoothing methods? I think both these propositions are true. The success of Gaussian processes shows that many real-world data modelling problems are perfectly well-solved by sensible smoothing methods. The most interesting problems, the task of feature discovery for example are not ones that Gaussian processes will solve. But maybe multilayer perceptrons can't solve them either. Perhaps a fresh start is needed, approaching the problem of machine learning from a paradigm different from the supervised feedforward mapping."
\end{displayquote}

One of the motivations for focussing on \textsc{gp}s in this thesis, is the proposition that many scientific discovery problems are instances of the real-world problems described by MacKay. Furthermore, \textsc{gp}s are more than just smoothing devices. In addition to admitting exact Bayesian inference which can be used to perform plausible reasoning \citep{2003_Jaynes} over scientific hypotheses, \textsc{gp}s are also a longstanding workhorse of Bayesian optimisation (\textsc{bo}) and active learning \citep{2012_Settles}, two methodologies that have already shown promise in accelerating scientific discovery \citep{2020_Pyzer, 2021_Shields}. The goals of this thesis are twofold: First, to showcase some of the use-cases for \textsc{gp}s in modelling scientific data and second, to extend current \textsc{gp} methodology and software implementations to enable their application to scientific problems. Specifically, the problem domains considered are:

\begin{enumerate}
    \item \textbf{High-Energy Astrophysics} - It is challenging to test theories in high-energy astrophysics due to the inability to perform physical experiments at the far reaches of the universe. As such, the analysis of observational data is important to guide the development of theory. It is shown how \textsc{gp} modelling can play a role in performing inference over the structure of black hole accretion discs and hence inform the development of future accretion disk theories.
    \item \textbf{Photoswitch Chemistry} - In synthetic chemistry, new areas of chemical space are constantly being explored and often little experimental data exists to guide exploration. It is shown how \textsc{gp} modelling can be used for molecular property prediction to prioritise the synthesis of novel molecules. We validate the modelling approach with laboratory experiments, discovering new and performant photoswitch molecules.
    \item \textbf{Methodology/Software} - From a methodological standpoint a novel \textsc{bo} algorithm is introduced that identifies and penalises input-dependent (heteroscedasatic) measurement noise, an important consideration for the discovery of robust materials suitable for industrial scale manufacturing. From a software standpoint, an open-source \textsc{gp} library for chemistry is introduced, providing implementations of bespoke kernels designed for common molecular and chemical reaction representations.
\end{enumerate}








\begin{figure}[h]
\centering
{\includegraphics[width=\textwidth]{Introduction/Figs/alt_intro_pic.png}}
\caption{A pictorial overview of the thesis.}
\label{fig:overview_thesis}
\end{figure}

\section{Overview and Contributions}

A pictorial overview of the chapters of this thesis is available in \autoref{fig:overview_thesis}. The detailed summary and contributions of each chapter are as follows:

\paragraph{Chapter 2} The requisite background is provided on \textsc{gp}s and \textsc{bo}, the machine learning methodologies used across chapters of this thesis.

\paragraph{Chapter 3} A self-contained background is provided on the elements of high-energy astrophysics required to understand our findings. The gapped lightcurves of the Seyfert galaxy Markarian 335 (Mrk 335) are interpolated using \textsc{gp} modelling with the intention of inferring the structure of the black hole accretion disk through cross-correlation analysis. In a simulation study, Bayesian model selection through the marginal likelihood is investigated as a means of evaluating the most appropriate choice of \textsc{gp} kernel. Following \textsc{gp} modelling of the observational data, it is found that the distance between the UV and X-ray emission regions of Mrk 335 predicted by the Shakura-Sunyaev accretion disk model is shorter than the light travel time measured using \textsc{gp}-based inference. Tentative evidence is obtained for a short lag feature in the coherence and lag spectra which could indicate the presence of an extended UV
emission region on the accretion disk where reverberation happens.

\paragraph{Chapter 4} A self-contained background is provided on the elements of molecular machine learning required to understand the findings presented. GAUCHE is introduced, a software library for Gaussian processes in chemistry, tackling the problem of extending the \textsc{gp} framework to molecular representations such as graphs, strings and bit vectors. By designing bespoke molecular kernels, the door is opened to uncertainty quantification and \textsc{bo} directly on molecules and chemical reactions.

\paragraph{Chapter 5} A small dataset of experimentally-determined properties for $405$ photoswitch molecules is used in conjunction with the machinery made available in GAUCHE to train a multioutput \textsc{gp} with a Tanimoto kernel to screen a large virtual library of $7,265$ photoswitches, identifying 11 performant candidates validated through laboratory experiment. Additionally, a predictive performance comparison is conducted between the multioutput \textsc{gp} model and a cohort of trained human photoswitch chemists with the \textsc{gp} model outperforming the human experts. From a benchmark comparison against other machine learning models, it is concluded that the curated dataset, as opposed to the choice of model, is the key determinant of performance.

\paragraph{Chapter 6} A novel method for performing \textsc{bo} is introduced that is robust to experimental measurement noise featuring a heteroscedastic \textsc{gp} surrogate model. From an extensive empirical study, it is concluded that a moderately-sized initialisation set is required for the model to be able to distinguish heteroscedastic noise from intrinsic function variability. The chapter concludes with recommendations on how future research might enable the approach to be scaled to high-dimensional datasets.

\paragraph{Chapter 7} The thesis contributions are reviewed and discussed in the broader context of identifying and enabling further applications of \textsc{gp}s in the natural sciences.

\section{List of Publications}

What follows is the full list of publications co-authored during the PhD process starting in October 2018. J1 \citep{2021_Griffiths}, J2 \citep{2021_Mrk}, J3 \citep{2020_Griffiths} and W7 \citep{2022_Gauche} comprise the thesis. In J1 and J2, all coauthors acted in advisory roles, fine-tuning ideas and the final manuscripts. I conducted all experiments, mathematical derivations and implemented all code contributions. In J3, Aditya Raymond Thawani curated the training dataset, designed and recruited participants for the human performance comparison study and specified the set of performance criteria. Jake Greenfield performed the spectral characterisation of the discovered molecules in the Fuchter group laboratory at Imperial College London. I conducted all machine learning experiments and implemented all code contributions with the exception of the results in \autoref{tab1_photo} and \autoref{tab2_photo} of Appendix~\ref{benchmark_ml}, where Penelope Jones, William McCorkindale, Arian Jamasb and Henry Moss obtained results for the attentive neural process (ANP), smooth overlap of atomic positions (SOAP) kernel, graph neural network (GNN) and string kernel models respectively. 

In W7, I ran all experiments excluding the Buchwald-Hartwig reaction optimisation experiments which were run by Bojana Rankovic and the Weisfehler-Lehman (WL) graph kernel table entries which were run by Aditya Ravuri. The remaining co-authored articles are not included in the thesis for ease of exposition.

J4 \citep{2020_Griffiths} is a paper resulting from the continuation of my work from the MPhil in Machine learning at the University of Cambridge in 2017. J5 \citep{2020_Cheng} was work principally led by Dr. Bingqing Cheng. J6 \citep{2020_Rivers} and J7 \citep{2020_Grosnit} were articles written during an internship at the Huawei Noah's Ark Lab. J8 \citep{2020_Zagar} is a continuation of my work during an MSci at Imperial College London in 2016. J9 \citep{2022_Bourached} was principally led by Anthony Bourached. C1 \citep{2019_Grant} is a continuation of work undertaken whilst a machine learning researcher at Secondmind Labs prior to commencement of the PhD. C2-C5 \citep{2022_Kell, 2021_Stork, 2021_Cann, 2021_Bourached_art} are articles published in a domain unrelated to the topic of the thesis. The (unpublished) workshop contributions, W1-W3 \citep{2020_flowmo} are early versions of J3 and W7. W4 \citep{2018_Griffiths} is unrelated to the topic of the thesis although a figure from this paper is used as \autoref{reaction}. W5 \citep{2021_Aziz} was principally led by Ajmal Aziz and so is not included in the thesis. W6 is a condensed version of J8. P1 \citep{2021_Grosnit} resulted from work undertaken whilst at Huawei Noah's Ark Lab and so is not included in the thesis though the subject matter is related. P2 \citep{bourached2021hierarchical} was principally led by Anthony Bourached and so is not included in the thesis. P3 \citep{2023_Frieder} and P4 \citep{2022_Rankovic} was work led by Simon Frieder and Bojana Rankovic respectively and is not included in the thesis.\\

\noindent \textbf{Refereed Journal Papers} \\

\begin{enumerate}[label={[J\arabic*]}]
      \item \textbf{Griffiths RR}, Aldrick A, Garcia-Ortegon M, Lalchand V, Lee, AA. \href{https://iopscience.iop.org/article/10.1088/2632-2153/ac298c}{Achieving Robustness to Aleatoric Uncertainty with Heteroscedastic Bayesian Optimisation}. \textit{Machine Learning: Science and Technology}. 2021.
      \item \textbf{Griffiths RR}, Jiang J, Buisson D, Wilkins D, Gallo L, Ingram, A, Lee AA, Grupe D, Kara M, Parker ML, Alston W, Bourached A, Cann G, Young A, Komossa S. \href{https://iopscience.iop.org/article/10.3847/1538-4357/abfa9f/meta}{Modelling the Multiwavelength Variability of Mrk-335 using Gaussian Processes}. \textit{The Astrophysical Journal}. 2021. %
              \item \textbf{Griffiths RR}, Greenfield JL, Thawani AR, Jamasb A, Moss HB, Bourached A, Jones P, McCorkindale W, Aldrick AA, Fuchter, MJ, Lee AA. \href{https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h}{Data-Driven Discovery of Molecular Photoswitches with Multioutput Gaussian Processes}. \textit{Chemical Science}. 2022.
      \item \textbf{Griffiths RR}, Hern\'andez-Lobato JM. \href{https://pubs.rsc.org/en/content/articlelanding/2020/sc/c9sc04026a#!divAbstract}{Constrained Bayesian Optimization for Automatic Chemical Design using Variational Autoencoders}. \textit{Chemical Science}. 2020. %
      \item Cheng B, \textbf{Griffiths RR}, Wengert S, Kunkel C, Stenczel T, Zhu B, Deringer VL, Bernstein N, Margraf JT, Reuter K, Csanyi G. \href{https://pubs.acs.org/doi/abs/10.1021/acs.accounts.0c00403}{Mapping Datasets of Molecules and Materials}. \textit{Accounts of Chemical Research}. 2020.
      \item Cowen-Rivers A, Lyu W, Tutunov R, Wang Z, Grosnit A, \textbf{Griffiths RR}, Hao J, Wang J, Bou-Ammar H. \href{https://jair.org/index.php/jair/article/view/13643}{HEBO: Pushing the Limits of Sample-Efficient Hyper-parameter Optimisation}. \textit{Journal of Artificial Intelligence Research}, 2022.
      \item Grosnit A, Cowen-Rivers A, Tutunov R, \textbf{Griffiths RR}, Wang J, Bou-Ammar H. \href{https://www.jmlr.org/papers/v22/20-1422.html?ref=https://githubhelp.com}{Are We Forgetting About Compositional Optimisers in Bayesian Optimisation}. \textit{Journal of Machine Learning Research}. 2021. %
      \item Zagar C, \textbf{Griffiths RR}, Podgornik R, Kornyshev AA. \href{https://www.sciencedirect.com/science/article/pii/S1572665720305038}{On the Voltage-Controlled Self-Assembly of NP Arrays at Electrochemical Solid/Liquid Interfaces}. \textit{Journal of Electroanalytical Chemistry}. 2020. %
      \item Bourached A, \textbf{Griffiths RR}, Gray R, Jha A, Nachev P. \href{https://onlinelibrary.wiley.com/doi/full/10.1002/ail2.63}{Generative Model-Enhanced Human Motion Prediction}. \textit{Applied AI Letters}. 2021.
\end{enumerate}

\noindent \textbf{Refereed Conference Papers} \\

\begin{enumerate}[label={[C\arabic*]}]
      \item Grant J, Boukouvalas A, \textbf{Griffiths RR}, Leslie D, Vaikili S, Munoz de Cote E. \href{http://proceedings.mlr.press/v97/grant19a.html}{Adaptive Sensor Placement for Continuous Spaces}. \textit{International Conference on Machine Learning}. 2019. %
      \item Kell G, \textbf{Griffiths RR}, Bourached A, Stork D. \href{https://arxiv.org/abs/2203.07026}{Extracting Associations and Meanings of Objects Depicted in Artworks through Bi-Modal Deep Networks}, Electronic Imaging 2022.
      \item Stork D, Bourached A, Cann G, \textbf{Griffiths RR}. \href{https://arxiv.org/abs/2102.02732}{Computational Identification of Significant Actors in Paintings through Symbols and Attributes}, Electronic Imaging, 2021.
      \item Cann G, Bourached A, \textbf{Griffiths RR}, Stork D. \href{https://arxiv.org/abs/2102.00209}{Resolution Enhancement in the Recovery of Underdrawings Via Style Transfer by Generative Adversarial Deep Neural Networks}, Electronic Imaging, 2021.
      \item Bourached A, Cann G, \textbf{Griffiths RR}, Stork D. \href{https://arxiv.org/abs/2101.10807}{Recovery of Underdrawings and Ghost-Paintings via Style Transfer by Deep Convolutional Neural Networks: A Digital Tool for Art Scholars}, Electronic Imaging, 2021.
\end{enumerate}

\noindent \textbf{Refereed Workshop Papers} \\

\begin{enumerate}[label={[W\arabic*]}]
      \item \textbf{Griffiths RR*}, Moss H*. \href{https://arxiv.org/abs/2010.01118}{Gaussian Process Molecular Machine Learning with FlowMO}. \textit{NeurIPS Workshop on Machine Learning for Molecules}. 2020 (Contributed Talk - top 5\%, * joint first authorship). %
      \item \textbf{Griffiths RR}, Jones P, McCorkindale W, Aldrick AA, Jamasb A, Day B. Benchmarking Scalable Active Learning Strategies on Molecules. \textit{ICLR Workshop on Fundamental Science in the Era of AI}. 2020. %
      \item \textbf{Griffiths RR}, Thawani AR, Elijosius R. \textit{Enhancing the Diversity of Molecular Machine Learning Benchmarks: An Open-Source Dataset for Molecular Photoswitches}. \textit{ICLR Workshop on Fundamental Science in the Era of AI}. 2020. %
      \item \textbf{Griffiths RR}, Schwaller P, Lee AA. \href{https://chemrxiv.org/articles/Dataset_Bias_in_the_Natural_Sciences_A_Case_Study_in_Chemical_Reaction_Prediction_and_Synthesis_Design/7366973}{Dataset Bias in the Natural Sciences: A Case Study in Chemical Reaction Prediction and Synthesis Design}. \textit{NeurIPS Workshop on Critiquing and Correcting Trends in Machine Learning.} 2018. %
      \item Aziz A, Kosasih EE, \textbf{Griffiths RR}, Brintrup A. \href{https://arxiv.org/abs/2107.10609}{Data Considerations in Graph Representation Learning for Supply Chain Networks}. \textit{ICML Workshop on Machine Learning for Data: Automated Creation, Privacy, Bias}. 2021 %
      \item Bourached A, \textbf{Griffiths RR}, Gray R, Jha A, Nachev P. \href{https://arxiv.org/abs/2010.11699}{Generative Model-Enhanced Human Motion Prediction}. \textit{NeurIPS Workshop on Interpretable Inductive Biases and Physically-Structured Learning}. 2020. %
      \item \textbf{Griffiths RR}, Klarner L, Moss Henry B., Ravuri A, Rankovic B, Truong S, Du Y, Jamasb A, Schwartz J, Tripp A, Kell G, Bourached A, Chan A, Moss J, Guo C, Lee AA, Schwaller P, Tang J, \href{https://arxiv.org/abs/2010.11699}{GAUCHE: A Library for Gaussian Processes in Chemistry}. \textit{ICML Workshop on AI4Science}. 2022. %
      
\end{enumerate}

\noindent \textbf{Preprints} \\

\begin{enumerate}[label={[P\arabic*]}]
      \item \textbf{Griffiths RR*}, Grosnit A*, Tutunov R*, Maraval AM*, Cowen-Rivers A, Yang L, Lin Z, Lyu W, Chen Z, Wang J, Peters J, Bou-Ammar H. \href{https://arxiv.org/abs/2106.03609}{High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning}. \textit{arXiv}. 2021. (* joint first authorship)
      \item Bourached A, Gray R, \textbf{Griffiths RR}, Jha A, Nachev P. \href{https://arxiv.org/abs/2111.12602}{Hierarchical Graph-Convolutional Variational Autoencoding for Generative Modelling of Human Motion}. \textit{arXiv}. 2021.
      \item Frieder S, Pinchetti, L, \textbf{Griffiths RR}, Salvatori, T, Lukasiewicz, T, Petersen, PC, Chevalier, A and Berner, J, 2023. \href{https://arxiv.org/abs/2301.13867}{Mathematical capabilities of ChatGPT.}. \textit{arXiv}. 2023.
      \item Ranković, B, \textbf{Griffiths, RR}, Moss, HB and Schwaller, P. \href{https://chemrxiv.org/engage/chemrxiv/article-details/638e196ae6f9a162aa2ce493}{Bayesian optimisation for additive screening and yield improvements in chemical reactions–beyond one-hot encodings}. \textit{ChemRxiv}, 2022.
\end{enumerate}

\noindent \textbf{PhD Thesis} \\
\begin{enumerate}[label={[T\arabic*]}]
       \item \textbf{Griffiths RR}, \href{https://www.repository.cam.ac.uk/handle/1810/346223}{Applications of Gaussian Processes at Extreme Lengthscales: From Molecules to Black Holes}. \textit{University of Cambridge}. 2022.
\end{enumerate}

\section{List of Software}

The following list details the open-source software contributed to over the duration of the PhD process:

\begin{enumerate}[label={[S\arabic*]}]
    \item Constrained Bayesian optimisation for automatic chemical design: \textbf{Ryan-Rhys Griffiths} (2018). Code to reproduce the experiments from \cite{2020_Griffiths}.\\
    
    Available at: \href{https://github.com/Ryan-Rhys/Constrained-Bayesian-Optimisation-for-Automatic-Chemical-Design}{https://github.com/Ryan-Rhys/Constrained-Bayesian-Optimisation-for-Automatic-Chemical-Design}
    
    \item Mapping materials and molecules: Bingqing Cheng, \textbf{Ryan-Rhys Griffiths}, Tamas Stenczel, Bonan Zhu, Felix Faber (2020). A software library containing automatic selection tools for materials and molecules \citep{2020_Cheng}.\\
    
    Available at: \href{https://github.com/BingqingCheng/ASAP}{https://github.com/BingqingCheng/ASAP}
    
    \item Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation: \textbf{Ryan-Rhys Griffiths} (2019). Code to reproduce the experiments from \cite{2021_Griffiths}.\\
    
    Available at: \href{https://github.com/Ryan-Rhys/Heteroscedastic-BO}{https://github.com/Ryan-Rhys/Heteroscedastic-BO}
    
    \item The photoswitch dataset: \textbf{Ryan-Rhys Griffiths}, Aditya Raymond Thawani, Arian Jamasb, William McCorkindale, Penelope Jones (2020). Code to reproduce the experiments from Chapter 5.\\
    
    Available at: \href{https://github.com/Ryan-Rhys/The-Photoswitch-Dataset}{https://github.com/Ryan-Rhys/The-Photoswitch-Dataset}
    
    \item Modelling the multiwavelength variability of Mrk-335: \textbf{Ryan-Rhys Griffiths} (2021). Code to reproduce the experiments from \cite{2021_Mrk}.\\
    
    Available at: \href{https://github.com/Ryan-Rhys/Mrk_335}{https://github.com/Ryan-Rhys/Mrk\_335}
    
    \item An empirical study of assumptions in Bayesian optimisation: Alexander I. Cowen-Rivers, Wenlong Lyu, Rasul Tutunov, Zhi Wang, Antoine Grosnit, \textbf{Ryan-Rhys Griffiths}, Alexandre Max Maraval, Hao Jianye, Jun Wang, Jan Peters, Haitham Bou-Ammar (2021). Code to reproduce the experiments from \cite{2020_Rivers}.\\
    
    Available at: \href{https://github.com/huawei-noah/HEBO/tree/master/HEBO}{https://github.com/huawei-noah/HEBO/tree/master/HEBO}
    
    \item High-dimensional Bayesian optimisation with variational autoencoders and deep metric learning: Antoine Grosnit, Rasul Tutunov, Alexandre Max Maraval, \textbf{Ryan-Rhys Griffiths}, Alexander I. Cowen-Rivers, Lin Yang, Lin Zhu, Wenlong Lyu, Zhitang Chen, Jun Wang, Jan Peters, Haitham Bou-Ammar. Code to reproduce the experiments from \cite{2021_Grosnit}.\\
    
    Available at: \href{https://github.com/huawei-noah/HEBO/tree/master/T-LBO}{https://github.com/huawei-noah/HEBO/tree/master/T-LBO}
        
    \item Are we forgetting about compositional optimisers in Bayesian optimisation?: Antoine Grosnit, Alexander I. Cowen-Rivers, Rasul Tutunov, \textbf{Ryan-Rhys Griffiths}, Jun Wang, Haitham Bou-Ammar. Code to reproduce the experiments from \cite{2020_Grosnit}.\\
    
    Available at: \href{https://github.com/huawei-noah/HEBO/tree/master/T-LBO}{https://github.com/huawei-noah/HEBO/tree/master/T-LBO}
    
    \item FlowMO: \textbf{Ryan-Rhys Griffiths} and Henry Moss (2020). A GPflow library for training Gaussian processes on molecular data \citep{2020_Moss}.\\
    
    Available at: \href{https://github.com/Ryan-Rhys/FlowMO}{https://github.com/Ryan-Rhys/FlowMO}
    
    \item GAUCHE: \textbf{Ryan-Rhys Griffiths}, Leo Klarner, Henry Moss, Aditya Ravuri, Sang Truong, Arian Jamasb, Austin Tripp, Bojana Rankovic, Philippe Schwaller (2022). A software library for Gaussian processes in chemistry.\\
    
    Available at \href{https://github.com/leojklarner/gauche}{https://github.com/leojklarner/gauche}
    
    \item Extracting associations and meanings of objects depicted in artworks through bi-modal deep networks: Gregory Kell, \textbf{Ryan-Rhys Griffiths} (2021). Code to reproduce the experiments from \cite{2022_Kell}.\\
    
    Available at: \href{https://github.com/gck25/fine_art_asssociations_meanings}{https://github.com/gck25/fine\_art\_asssociations\_meanings}
\end{enumerate}


\nomenclature[Z-OOD]{OOD}{Out-Of-Distribution}
\nomenclature[Z-GP]{GP}{Gaussian Process}
\nomenclature[Z-BO]{BO}{Bayesian Optimisation}
\nomenclature[Z-Mrk 335]{Mrk 335}{Markarian 335}
\nomenclature[Z-GAUCHE]{GAUCHE}{GAUssian Processes in CHEmistry}
\nomenclature[Z-ANP]{ANP}{Attentive Neural Process}
\nomenclature[Z-SOAP]{SOAP}{Smooth Overlap of Atomic Positions}
\nomenclature[Z-GNN]{GNN}{Graph Neural Network}
\nomenclature[Z-WL]{WL}{Weisfehler-Lehman}


