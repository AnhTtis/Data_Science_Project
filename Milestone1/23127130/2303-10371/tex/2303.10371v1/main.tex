%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{multirow} 
\usepackage{makecell}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{arydshln}




% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\usepackage{silence}
\WarningFilter{latex}{Text page}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\ourmodel:Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification}

\begin{document}

\newcommand{\ourmodel}{UNREAL}

\twocolumn[
\icmltitle{\ourmodel:Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Liang Yan}{fdu,equal}
\icmlauthor{Shengzhong Zhang}{fdu,equal}
\icmlauthor{Bisheng Li}{fdu}
\icmlauthor{Min Zhou}{hua}
\icmlauthor{Zengfeng Huang}{fdu}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{fdu}{Fudan University, Shanghai, China}
\icmlaffiliation{hua}{Huawei Technologies Co., Ltd Shengzhen, China}

\icmlcorrespondingauthor{Zengfeng Huang}{huangzf@fudan.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% Extremely skewed label distributions are common in real-world node classification tasks. If not dealt with appropriately, it significantly hurts the performance of GNNs in minority classes. Due to its practical importance, there have been a series of recent research devoted to this challenge. Existing over-sampling techniques smooth the label distribution by generating ``fake'' minority nodes and synthesizing their features and local topology, which largely ignore the rich information of unlabeled nodes on graphs. On the other hand, methods based on loss function modification re-weight different samples or change classification margins. Representative methods in this category need to use label information to estimate the distance of each node to its class center, which is unavailable on unlabeled nodes. Motivated by the observation that abundant unlabeled nodes are available in a node classification scenario and the performance degradation problem of pseudo-labeling methods in imbalance learning, we propose UNREAL in this work, an iterative over-sampling method. The first key difference is that we only add unlabeled nodes instead of synthetic nodes, which eliminates the challenge of feature and neighborhood generation. To select which unlabeled nodes to add for imbalanced training sets, we propose geometric ranking to rank unlabeled nodes. Geometric ranking exploits unsupervised learning in the node embedding space to effectively calibrates pseudo-label assignment. Finally, we identify the issue of geometric imbalance in the embedding space and provide a simple metric to filter out geometrically imbalanced nodes. Extensive experiments on real-world benchmark datasets are conducted, and the empirical results show that our method significantly outperforms current state-of-the-art methods across datasets and imbalance ratios.


Extremely skewed label distributions are common in real-world node classification tasks. If not dealt with appropriately, it significantly hurts the performance of GNNs in minority classes. Due to its practical importance, there have been a series of recent research devoted to this challenge. Existing over-sampling techniques smooth the label distribution by generating ``fake'' minority nodes and synthesizing their features and local topology, which largely ignore the rich information of unlabeled nodes on graphs. In this paper, we propose \ourmodel, an iterative over-sampling method. The first key difference is that we only add unlabeled nodes instead of synthetic nodes, which eliminates the challenge of feature and neighborhood generation. To select which unlabeled nodes to add, we propose geometric ranking to rank unlabeled nodes. Geometric ranking exploits unsupervised learning in the node embedding space to effectively calibrates pseudo-label assignment. Finally, we identify the issue of geometric imbalance in the embedding space and provide a simple metric to filter out geometrically imbalanced nodes. Extensive experiments on real-world benchmark datasets are conducted, and the empirical results show that our method significantly outperforms current state-of-the-art methods consistent on different datasets with different imbalance ratios.

\end{abstract}



\section{Introduction}
\label{Introduction}
% Heavily  Imbalanced Node Learning
%Node classification is widely used in real life, such as malicious account detection \citep{mohammadrezaei2018identifying} and fake news detection \citep{monti2019fake}. 
Node classification is ubiquitous in real-world scenarios e.g., social network and commercial graph analysis. %\citep{mohammadrezaei2018identifying, ying2018graph, hamilton2017inductive}. 
%\citep{ ying2018graph, hamilton2017inductive}.
Generally, real-world data come with an imbalanced class distribution~\citep{mohammadrezaei2018identifying, wang2020extending}. A classifier trained using an imbalanced dataset is prone to have biased accuracy on under-represented classes. While GNNs have achieved superior performance on node classification, training a fair GNN model for handling highly-imbalanced class distributions remains a challenging task \citep{liu2018heterogeneous, zhao2021graphsmote}. 


% The message passing scheme of GNN models makes the problem even more complex, as here the samples cannot be treated as i.i.d.\ samples. Moreover, quantity imbalance is often coupled with topology imbalance~\citep{chen2021topology}, and thus it is difficult to extend existing techniques for handling i.i.d.\ data to relational data. 



% Disadvantages of other models
% Given its importance and unique characteristics, a 
Several recent studies have been devoted to solving the imbalanced node classification problem~\citep{zhao2021graphsmote, shi2020multi, chen2021topology, park2021graphens, song2022tam}. One strategy is to adapt over-sampling, a widely used technique in other domains, to graph data. However, it is a non-trivial task, since one needs to additionally generate topological information for newly synthesized nodes. Different approaches to synthesize new nodes together with their feature and relational information have been proposed, e.g., \citep{zhao2021graphsmote, shi2020multi, park2021graphens}. Adding synthetic data introduces additional noise (which harms classification accuracy) and extra computation burden, especially in graph learning. On the other hand, abundant unlabeled nodes are often available in large-scale graph learning tasks. \emph{Can we just add unlabeled nodes instead of synthesizing a large amount of ``fake'' nodes?} Thus far, the value of unlabeled nodes in imbalanced semi-supervised classification has not been explored.


%It is empirically observed in \citep{song2022tam} that the performance of existing over-sampling approaches is easily affected by minority nodes with high connectivity to other classes. To alleviate this issue, \cite{song2022tam, chen2021topology} modify the classification margin based on various statistics of the true label distribution and the graph topology. Such methods rely on ground truth label information, which is not available on most nodes in the graph. Thus, \cite{song2022tam} and \citet{chen2021topology} first approximately infer these information from the training set. However, the training set is highly skewed in the first place, so information derived from them is less reliable, and the bias could spread to later building blocks, which hurts the overall performance. 

 In this work, we propose a novel imbalanced node classification method: \textbf{u}nlabeled \textbf{n}ode \textbf{re}trieval \textbf{a}nd \textbf{l}abeling (\ourmodel). At a high level, \ourmodel~is an oversampling approach similar to self-training (ST)~\cite{yarowsky1995unsupervised,lee2013pseudo}, which adds unlabeled nodes together with their pseudo-labels (predictions made by a previous-learned model) to the training set. Since there is no need for synthesizing node features and topology, it overcomes critical shortcomings of existing oversampling approaches.

It is noteworthy that ST has shown to be effective in dealing with label sparsity in node classification~\citep{li2018deeper, zhou2019effective, sun2020multi, wang2021confident}. Extensive experiments in this work show that ST is effective for imbalanced learning as well but fails to achieve satisfactory performance in heavily-imbalanced scenarios. This is mainly because the bias in the original training set results in unreliable predictions, which makes the pseudo-labels used in ST highly noisy. In this paper, we introduce effective techniques to overcome the initial bias in ST to further boost the performance on heavily-imbalanced node classification. The key idea is to explore the geometric structure in the embedding space to calibrate the bias in pseudo-labels.

This is partially inspired by the work of \cite{kang2019decoupling} from computer vision, where they hypothesize and verify empirically that the classifier is the only under-performed component in the model when trained on an imbalanced training set. In \ourmodel, after the preliminary training step in ST, we retrieve node embeddings from the output layer (before the classification layer) and use unsupervised clustering methods for label prediction, which compensate for the prediction made by the supervised model. As in \cite{chen2021topology}, we wish to add nodes that are closer to their class centers, and unsupervised learning in the embedding space also provides a natural way to rank the closeness of nodes to their (predicted) class centers. As in standard ST, multiple rounds of pseudo-label fitting will be applied. Extensive experimental studies show that our framework outperforms existing imbalanced node classification methods by a large margin in most settings.


% In the end, we propose a set of techniques to address this issue, and our experimental results show that these techniques are highly effective and outperform previous state-of-the-art methods by a large margin.










%In this work, powerful theory and rich experiments prove that ST is effective for imbalanced learning totally but fails to achieve satisfactory performance in heavily-imbalanced scenarios due to bias in the original training set: predictions from a classifier trained on the imbalanced training set may be highly biased and contain a large proportion of incorrect pseudo-labels. We empirically demonstrate this in Section \ref{unlabeled data}. As a result, we propose a set of techniques to address this issue, and our experimental results show that these techniques are highly effective and outperform previous state-of-the-art methods by a large margin.






We summarize our contribution as follows: 1) As far as we know, this is the first work that systematically studies the performance of ST as an over-sampling technique for imbalanced node classification; 2) from our empirical results, we identify deficiencies of ST in heavily-imbalanced scenarios, and propose to apply unsupervised methods in the embedding space to overcome the drawbacks; 3) we introduce several simple yet effective techniques in the implementation to optimize the performance, e.g., we identify the Geometric Imbalance (GI) issue in the embedding space and propose a metric to measure GI and discard imbalanced nodes; 4) we conduct comprehensive experiments on multiple benchmarks which demonstrates the superiority of our framework.





%We conduct comprehensive experiments on multiple benchmarks, including citation networks \citep{sen2008collective}, an Amazon product co-purchasing network \citep{sen2008collective}, and Flickr~\citep{zeng2019graphsaint}. We also test the performance of \ourmodel~on several mainstream GNN architectures namely GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, and GraphSAGE \citep{hamilton2017inductive}. Experimental results demonstrate the superiority of the proposal as \ourmodel~consistently outperforms existing state-of-the-art approaches by a large margin.



\section{Preliminaries}

\subsection{Notations and Definitions}
In this work, we mainly focus on semi-supervised node classification on an undirected and unweighted graph $\mathcal{G}=(\mathcal{V},  \mathcal{E},  \mathcal{L})$. Here, $\mathcal{V}$ is the node set, $\mathcal{E}$ is the edge set, and $\mathcal{L} \subset \mathcal{V}$ denotes the set of labeled nodes. So the set of unlabeled nodes is $\mathcal{U}=\mathcal{V}-\mathcal{L}$. Let $\mathcal{X} \in \mathbb{R}^{n \times f}$ be the feature matrix (where $n=|\mathcal{V}|$ and $f$ is the feature dimension). We use ${A} \in \{0,1\}^{n \times n}$ to denote the adjacency matrix and $\mathcal{N}(v)$ the set of $1$-hop neighbors of node $v$.  The labeled sets for all classes are denoted by $\left(\mathcal{C}_{1},  \mathcal{C}_{2},  \cdots,  \mathcal{C}_{k}\right)$, where $k$ is the number of different classes. We use imbalance ratio, defined as $\rho := {\frac{\max _{i}\left(\left|\mathcal{C}_{i}\right|\right)}{\min _{i}\left(\left|\mathcal{C}_{i}\right|\right)}}$, to measure the level of imbalance in a dataset. We summarize the notation in Appendix \ref{Notation Table}.



\subsection{Message Passing Neural Networks for Node Classification}
A standard MPNN consists of three components, a message function $m_{l}$, an information aggregation function $\theta_{l}$, and a node feature update function $\psi_{l}$. The feature of each node is updated iteratively. Let $h_v^{l}$ be the feature of node $v$ in the $l$-th layer, then in the $(l+1)$-th layer the feature is:
\begin{equation}
\centering
{h_{v}^{(l+1)}=\psi_{l}\left(h_{v}^{(l)},  \theta_{l}\left(\left\{m_{l}\left(h_{v}^{(l)},  h_{u}^{(l)},  e_{v,  u}\right) \mid u \in \mathcal{N}(v)\right\}\right)\right)}
\end{equation}
where $e_{v,  u}$ is the edge weight between $v$ and $u$. For the classic GCN model \citep{kipf2016semi}, $h_{v}^{(l+1)}$ is computed as: ${h_{v}^{(l+1)}=\Phi^l \sum_{u \in \mathcal{N}(v) \cup\{v\}}} {\frac{e_{v,  u}}{\sqrt{\hat{d}_{u} \hat{d}_{v}}} h_{u}^{(l)}}$, where $\Phi^l$ is the parameter matrix of the $l\text {-th }$ layer and ${\hat{d}_{v}=1+\sum_{u \in \mathcal{N}(v)} e_{v,  u}}$. For node classification, a classification layer is concatenated after the last layer of a GNN.


\section{Imbalanced Learning with Unlabeled Data}
\label{Imbalanced Learning with Unlabeled Data}

% As motivated, in this section, we explore the positive value of unlabeled nodes for imbalance learning. In order to confirm the effectiveness of this direction in boosting imbalanced learning on, we conduct extensive experiments to verify the performance of ST in the imbalanced leaning scenarios.
In this work, we try to harness the positive value of unlabeled nodes for imbalance learning. We first conduct extensive experiments to confirm the effectiveness of ST in boosting imbalanced learning on graph-structured data. Our experimental results show that as the imbalance ratio increase, standard ST is more likely to add nodes with wrong pseudo-labels, which leads to performance degradation.

\subsection{ST Improves the Performance of GNN in Imbalanced Learning}
\label{ST Improves the Performance of GNN in Imbalanced Learning}

\citet{yang2020rethinking} considered a binary classiﬁcation problem with the data generating distribution is a mixture of two Gaussians and assumed that a base classifier is trained on imbalanced data. Their analysis of this simple model shows: (1) ST boosts imbalanced learning and more unlabeled data is always helpful. (2) data imbalance affects the probability of obtaining a good estimation. Graph learning is more complicated and such a simple model could miss some critical aspects of graph learning. 


We first conduct extensive experiments to confirm the effectiveness of ST in boosting imbalanced learning on graph-structured data. We repeat each experiment five times and report the average experiment results on Cora under different imbalance ratios in Figure \ref{figure_cora_performance}.\footnote{More results on different datasets and models can be found in Appendix \ref{More Results About Imbalanced Learning with Unlabled Data}.}  It can be observed that across different ratios, ST consistently outperforms vanilla model by a large margin, which verifies the positive value of the unlabeled samples of graph-structured data. However, as imbalance ratio increases, it is observed that the performance of ST degrades rapidly, which renders that ST is insufficient for high imbalance ratios. It can be observed that \ourmodel~consistently outperforms ST by a large margin, and as imbalance ratio increases, the gap of the F1 scores between ST and our method becomes larger. We believe the main reason is that, due to imbalance in the original training, ST adds low-quality nodes into the training set in the early stages. We conduct experiments to verify this in Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}.
% \input{figure-compile/figure_coraciteseer_performance.tex}


\subsection{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}
\label{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}
As we mentioned, since ST adds pseudo-labels to the training set and trains the model iteratively, misjudgements in the early stages will cause the method to fail badly. Here, we hypothesize that as the imbalance ratio of the dataset becomes larger, the pseudo-labels obtained by ST-based methods are less credible. %At the same time, the prediction confidence of unlabeled nodes is no longer reliable. 



\paragraph{Experimental Setup.} We first conduct experiments to test the accuracy of pseudo-label for unlabeled nodes on class-imbalanced graphs. ST based on GCN are trained on Cora. We process the dataset to make it imbalanced following \citet{zhao2021graphsmote, park2021graphens, song2022tam}. The imbalance ratio $\rho$ is set as 1, 5, 10, 20, 50, 100. We test the accuracy of pseudo-labels for unlabeled nodes which are newly added to the training set. More specifically, we examine 100 nodes that join the majority class and 100 nodes that join the minority class. We repeat each experiment five times and report the average experiment results.


\paragraph{Pseudo-label Misjudgment Augmentation Problem.} The accuracy of pseudo-labels for unlabeled nodes which are selected into the minority class and the majority class are reported in Figure \ref{figure_cora_node}.\footnote{More results on different datasets and models can be found in Appendix \ref{More Results About Imbalanced Learning with Unlabled Data}.} We can observe that as $\rho$ becomes larger, the accuracy of pseudo labels for unlabeled nodes selected into the minority class decreases quickly. For unlabeled nodes selected into the majority class, the accuracy of pseudo-labels is stable at a high level, which confirms the heavy bias of the base classifier trained on highly imbalanced data. As a reference, for both majority class or minority class, UNREAL consistently outperforms ST in pseudo-label accuracy.


\begin{figure}[htbp]
\centering
\subfigure[Cora-GCN]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.098]{cora_gcn_performance.pdf}
%\caption{fig1}
\label{figure_cora_performance}
\end{minipage}%
}%
\subfigure[Cora-GCN]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.11]{citeseer_gcn_node.pdf}
%\caption{fig1}
\label{figure_cora_node}
\end{minipage}%
}%
% \subfigure[Cora-SAGE]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1.14in]{figure/cora_gcn_node.pdf}
% %\caption{fig2}
% \end{minipage}
% }%

\centering
\caption{(a) The experimental results on Cora under different imbalance scenarios ($\rho$ = 10, 20, 50, 100). We compare the F1-score ($\%$) with the standard errors of ST and UNREAL. (b) The experimental results on Cora under various imbalance scenarios are presented. We choose 100 unlabeled nodes that have recently been added to the training set using ST \& UNREAL, and we evaluate their performance using GCN by comparing their accuracy ($\%$) with the standard errors of these nodes' pseudo labels. Minor means that we only test unlabeled nodes from the minority classes, and Major means that we only test unlabeled nodes from the majority classes.}
\label{figure_cora_nodeperformance}
\end{figure}



\subsection{More Elaboration and Experiments.} In Appendix \ref{More Results About Imbalanced Learning with Unlabled Data}, we provide more experimental details and more experimental results on different benchmarks and base models to strengthen our finds in Section \ref{ST Improves the Performance of GNN in Imbalanced Learning} and Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}.  











% \paragraph{The Specific Performance of ST in imbalanced learning.} ST is a classic technique in semi-supervised learning to enhance performance and robustness, e.g., \cite{lee2013pseudo}. However, as we have argued above, for highly imbalanced data, ST is unlikely to achieve optimal performance as biased and untrustworthy predictions may bring low-quality nodes into the training set in the early stage. 


% In this section, we empirically verify the informativeness of geometric structures by comparing \ourmodel~with pure self-training schemes. The size of added nodes in each round for each class is a hyperparameter, and we tune the hyperparameter based on the accuracy on the validation set. We repeat each experiment five times and report the average experiment results on the node classification benchmark datasets under different imbalance ratios in Figure \ref{figure_cora_performance_gcngat}. It can be observed that across different ratios, \ourmodel~consistently outperforms self-training by a large margin, and as imbalance ratio increases, the gap of the F1 scores between ST and our method becomes larger. This shows that as the data imbalance issue become more severe, the performance of ST degrades more rapidly, which is likely due to noise introduced in early rounds.
% \input{figure-compile/figure_cora_performance.tex}


% Inspire by previous work \citep{yang2020rethinking} in vision domain, which first developed intuitions about how different ingredients in the imbalanced data and the unlabeled data affect the overall learning process through a simple theoretical model. 

% \paragraph{Theoretical Motivation.} \citet{yang2020rethinking} considered a binary classiﬁcation problem with the data generating distribution being a mixture of two Gaussians and supposed that a base classifier trained on imbalanced training data. Consider the case where extra unlabeled data (potentially also imbalanced) from the data generating distribution are available, this work study how this affects the classifier's performance with pseudo-labeling method. The Theorem proposed by \citep{yang2020rethinking} illustrates several interesting aspects: (1) Training data imbalance affects
% the accuracy of our estimation and more unlabeled data is always helpful. (2) Unlabeled data imbalance affects the probability of obtaining such a good estimation.


% Consider a binary classiﬁcation problem with the data generating distribution ${P_{X Y}}$ being a mixture of two Gaussians. In particular, the label ${Y}$ is either positive (+1) or negative (-1) with equal probability (i.e., 0.5). Condition on $Y=+1, X \mid Y=+1 \sim \mathcal{N}\left(\mu_{1}, \sigma^{2}\right)$ and similarly, $X \mid Y=-1 \sim \mathcal{N}\left(\mu_{2}, \sigma^{2}\right)$. Without loss of generality, let $\mu_{1}>\mu_{2}$. It is straightforward to verify that the optimal Bayes’s classiﬁer is $f(x)=\operatorname{sign}\left(x-\frac{\mu_{1}+\mu_{2}}{2}\right)$, i.e., classify ${x}$ as +1 if $x>\left(\mu_{1}+\mu_{2}\right) / 2$. Therefore, in the following, we measure our ability to learn $\left(\mu_{1}+\mu_{2}\right) / 2$ as a proxy for performance.

% , is given. We consider the case where extra unlabeled data $\left\{\tilde{X}_{i}\right\}_{i}^{\tilde{n}}$ (potentially also imbalanced) from $P_{X Y}$ are available, and study how this affects our performance with the label information from $f_{B}$. Precisely, we create pseudo-label for $\left\{\tilde{X}_{i}\right\}_{i}^{\tilde{n}}$ using $f_{B}$. Let $\left\{\tilde{X}_{i}^{+}\right\}_{i=1}^{\tilde{n}_{+}}$ be the set of unlabeled data whose pseudo-label is +1; similarly let $\left\{\tilde{X}_{i}^{-}\right\}_{i=1}^{\tilde{n}_{-}}$ be the negative set. Naturally, when the training data is imbalanced, $f_{B}$ is likely to exhibit different accuracy for different class. We model this as follows. Consider the case where pseudo-label is +1 and let {$\left\{I_{i}^{+}\right\}_{i=1}^{\tilde{n}_{+}}$ be the indicator that the $i$-th pseudo-label is correct, i.e., if $I_{i}^{+}=1$, then $\tilde{X}_{i}^{+} \sim \mathcal{N}\left(\mu_{1}, \sigma^{2}\right)$ and otherwise $\tilde{X}_{i}^{+} \sim \mathcal{N}\left(\mu_{2}, \sigma^{2}\right)$. We assume $I_{i}^{+} \sim \operatorname{Bernoulli}(p)$, which means $f_{B}$ has an accuracy of p for the positive class. Analogously, we deﬁne $\left\{I_{i}^{-}\right\}_{i=1}^{\tilde{n}_{-}}$ , i.e., if $I_{i}^{-}=1$, then $\tilde{X}_{i}^{-} \sim \mathcal{N}\left(\mu_{2}, \sigma^{2}\right)$ and otherwise $\tilde{X}_{i}^{-} \sim \mathcal{N}\left(\mu_{1}, \sigma^{2}\right)$. Let $I_{i}^{-} \sim \operatorname{Bernoulli}(q)$, which means $f_{B}$ has an accuracy of ${q}$ for the negative class. Denote by $\Delta \triangleq p-q$ the imbalance in accuracy. As mentioned, we aim to learn $\left(\mu_{1}+\mu_{2}\right) / 2$ with the above setup, via the extra unlabeled data. It is natural to construct our estimate as $\hat{\theta}=\frac{1}{2}\left(\sum_{i=1}^{\tilde{n}_{+}} \tilde{X}_{i}^{+} / \tilde{n}_{+}+\sum_{i=1}^{\tilde{n}_{-}} \tilde{X}_{i}^{-} / \tilde{n}_{-}\right)$. Then, we have:


% \begin{theorem}
% \label{theorem1}
% Consider the above setup. For any $\delta>0$, with probability at least $1-2e^{-\frac{2 \delta^{2}}{9 \sigma^{2}} \cdot \frac{\tilde{n}+\tilde{n}-}{\tilde{n}-+\tilde{n}+}}-2 e^{-\frac{8 \tilde{n}_{+} \delta^{2}}{9\left(\mu_{1}-\mu_{2}\right)^{2}}}-2 e^{-\frac{8 \tilde{n}^{-} \delta^{2}}{9\left(\mu_{1}-\mu_{2}\right)^{2}}}$ our estimates $\hat{\theta}$ satisfies 
% \begin{equation}
% \centering
% \left|\hat{\theta}-\left(\mu_{1}+\mu_{2}\right) / 2-\Delta\left(\mu_{1}-\mu_{2}\right) / 2\right| \leq \delta.
% \label{Equation 2}
% \end{equation}
% \end{theorem}



% \paragraph{Interpretation.} The result illustrates several interesting aspects. (1) Training data imbalance affects the accuracy of our estimation and the accuracy of pseudo labels. For heavily imbalanced training data, we expect the base classiﬁer to have a large difference in accuracy between major and minor classes. That is, the more imbalanced the data is, the larger the gap $\Delta$ would be, which inﬂuences the closeness between our estimate and desired value $\left(\mu_{1}+\mu_{2}\right) / 2$. (2) Unlabeled data imbalance affects the probability of obtaining such a good estimation. For a reasonably good base classiﬁer, we can roughly view $\tilde{n}_{+}$ and $\tilde{n}_{-}$ as approximations for the number of actually positive and negative data in unlabeled set. For term $2\exp\left(-\frac{2 \delta^{2}}{9 \sigma^{2}} \cdot \frac{\tilde{n}_{+} \tilde{n}_{-}}{\tilde{n}_{-}+\tilde{n}_{+}}\right)$, note that $\frac{\tilde{n}+\tilde{n}-}{\tilde{n}-+\tilde{n}_{+}}$ is maximized when $\tilde{n}_{+}=\tilde{n}_{-}$ , i.e., balanced unlabeled data. For terms $2\exp\left(-\frac{8 \tilde{n}_{+} \delta^{2}}{9\left(\mu_{1}-\mu_{2}\right)^{2}}\right)$ and $2\exp\left(-\frac{8 \tilde{n}_{-} \delta^{2}}{9\left(\mu_{1}-\mu_{2}\right)^{2}}\right)$, if the unlabeled data is heavily imbalanced, then the term corresponding to the minor class dominates and can be moderately large. Our probability of success would be higher with balanced data, but in any case, more unlabeled data is always helpful.




\section{\ourmodel}

In this section, we provide the details of the proposed method. \ourmodel~iteratively adds unlabeled nodes (with predicted labels) to the training set and retrains the model. 
We propose three complementary techniques to enhance the unlabeled node selection and labeling. More specifically, in Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering}, we describe Dual Pseudo-tag Alignment Mechanism (DPAM) for effective node filtering, the key idea of which is to use unsupervised clustering in the embedding space to obtain a node ranking, namely \emph{geometric ranking}. In Section \ref{reorder}, we show how to combine geometric rank from unsupervised learning and confidence ranking from supervised learning to reorder unlabeled nodes according to their closeness to the class centers (Node-Reordering). Finally, in Section \ref{Geometric Imbalance}, we identify the issue of geometric node imbalance (GI) and define a new metric to measure GI, which is then used to filter out nodes with high GI. The overall pipeline of UNREAL is illustrated in Figure \ref{pipeline}. The pseudo-code of the full algorithm is provided in Appendix \ref{PseudoCode} (Algorithm \ref{Algorithm}).

% We note that self-training techniques also apply a similar iterative process, which have been applied on GNNs to improve performance when node labels are sparse \citep{li2018deeper,zhou2019effective}. However, standard self-training is insufficient for dealing with label imbalance since the original imbalanced training set is likely to produce unreliable initial predictions, and the prediction noise would have an undesirable cascade effect on future rounds. The inadequacy of self-training is also clearly verified through our empirical studies in Section \ref{Explanation}. 

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{pipeline.pdf}
\caption{Overall pipeline of our UNREAL. Colored nodes denote labeled nodes. Parameters in the GNN Model and the classifier are trained together using the current training set.}
\label{pipeline}
\end{figure}




\subsection{Dual Pseudo-tag Alignment Mechanism for Node Filtering}
\label{Dual Pseudo-tag Alignment Mechanism for Node filtering}

\paragraph{Motivating Example.} As we verified above, in heavily-imbalanced scenarios, the pseudo-labels given by the classifier are much less reliable. \citet{kang2019decoupling} hypothesize and verify empirically that the classifier is the only under-performed component in the model when trained on an imbalanced training set. We conduct experiments to verify this on imbalanced node classification.

Let $d$ be the embedding dimension. We use $H^{L} \in \mathbb{R}^{|\mathcal{L}| \times d}$ and $H^{U} \in \mathbb{R}^{|\mathcal{U}| \times d}$ to denote the embedding matrix of labeled and unlabeled nodes respectively. Each row of the embedding matrix is the embedding of a node $u$ (denoted as $h_u^{L}$ and $h_u^{U}$), which is considered as a point in the $d$-dimension Euclidean space. 
We apply an unsupervised clustering algorithm, $f_{\text{cluster}}$, which partitions the embeddings of unlabeled nodes into $k'$ clusters and produces $k'$ corresponding cluster centers, where $k'$ is usually larger than $k$, the number of classes.
\begin{equation}
\centering
f_{\text {cluster}}({{H}}^{U})\Longrightarrow \{ \mathcal{K}_{1}, c_1, \mathcal{K}_{2}, c_2,  \cdots,  \mathcal{K}_{k'}, c_{k'}\}
\end{equation}
where $\mathcal{K}_{i}$ is the $i \text {-th }$ cluster and $c_i$ is the $i$-th cluster center. We use vanilla k-means in our implementation.
We also compute the embedding center of each class in the training set
\begin{align}\label{eqn:classcenter}
    c_i^{\text{train}} = M(\{h_u^{L} ~|~ y_u \in \mathcal{C}_i\}).
\end{align}
Since we use k-means in our experiments, $M(\cdot)$ is simply the mean function. We next assign a pseudo-label $\tilde{y}_m$ to each cluster $\mathcal{K}_m$:
\begin{center}
\begin{equation}
	\tilde{y}_i = \mathop{\arg\min}_{j} \mathop{\mathsf{distance}} (c_j^{\text{train}}, c_i).
\end{equation}
\end{center}
We then combine clusters with the same pseudo-label $m$ as $\mathcal{\tilde{U}}_{m}$, and $\mathcal{U}=\bigcup_{m=1}^{k}\ \mathcal{\tilde{U}}_{m}$.
On the other hand, the supervised GNN model gives each node $u$ in $\mathcal{U}$ a prediction $\hat{y}_u$, and we put unlabeled nodes whose prediction is $m$ into the set $\mathcal{U}_{m}$, and again we have $\mathcal{U}=\bigcup_{m=1}^{k} \mathcal{U}_{m}$. 

\paragraph{Analysis.} Thus far, we obtain two pseudo-labels for each unlabeled node, which come from unsupervised and supervised methods respectively. We conduct experiments on Cora to compare the accuracy of the two pseudo-labels. The experiment settings are the same as Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}. As shown in Figure \ref{figure_coragcn_select} and Figure \ref{figure_coragat_select}, it can be found that the embeddings learned by the GNN encoder are still of high quality to get an effective prediction based on unsupervised algorithms. Especially in heavily-imbalanced scenarios, the reliability of unsupervised algorithms is even higher than the supervised classifier. Based on this, we proposed DPAM. \footnote{We elaborate on the experimental details and conduct more experiments to validate our idea and the effectiveness of DPAM in Appendix \ref{More Experiments of the Motivating Example} and Appendix \ref{Additional analysis for DPAM}.} 

\begin{figure}[htbp]
\centering
\subfigure[Cora-GCN]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.23]{cora_gcn_test.pdf}
%\caption{fig1}
\label{figure_coragcn_select}
\end{minipage}%
}%
\subfigure[Cora-GAT]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.23]{cora_gat_test.pdf}
%\caption{fig1}
\label{figure_coragat_select}
\end{minipage}%
}%

\subfigure[Cora-GCN]{
\begin{minipage}[t]{0.5\linewidth}
\centering
% \includegraphics[width=2\textwidth,height=1.7\textwidth]{figure/Section5Node-reorderingRBO/cora_gcn_10_rbo.pdf}
\includegraphics[width=1.0\textwidth]{rbocora10gcn.pdf}
%\caption{fig2}
\label{figure_coragcn_rbo}
\end{minipage}
}%
\subfigure[Cora-GAT]{
\begin{minipage}[t]{0.5\linewidth}
\centering
% \includegraphics[width=2\textwidth,height=1.7\textwidth]{figure/Section5Node-reorderingRBO/cora_gcn_10_rbo.pdf}
\includegraphics[width=1.0\textwidth]{rbocora10gat.pdf}
%\caption{fig2}
\label{figure_coragat_rbo}
\end{minipage}
}%
\centering
\caption{(a, b) The partial experimental results on Cora under different imbalance scenarios($\rho$ = 1, 5, 10, 20, 50, 100). We compare the accuracy of the two pseudo-labels(predictions) from unsupervised algorithms and supervised classifiers respectively for all unlabeled nodes. (c, d) Fluctuation of RBO values (similarity) of two rankings as iterations progress.}

\end{figure}

\paragraph{Dual Pseudo-tag Alignment Mechanism (DPAM).} The pseudo-labels produced by applying an unsupervised algorithm on the embeddings provide an alternative and potentially less biased prediction, which may compensate for the bias introduced by the imbalanced training set. At the same time, the overall accuracy of the unsupervised algorithm is inferior to supervised methods when the training set gradually becomes balanced, and thus it is sub-optimal to rely solely on the pseudo-labels from clustering. As a result, DPAM only keeps unlabeled nodes whose two labels align, i.e., those belong to the intersection of $\mathcal{\tilde{U}}_{m}$ and $\mathcal{U}_{m}$ for each ${m \in\{1, 2,  \cdots,  k\}}$; and each node in $\mathcal{\tilde{U}}_{m} \cap \mathcal{U}_{m}$ gets a pseudo-label $m$.









\subsection{Node-Reordering}
\label{reorder}
Now DPAM has selected a pool of candidate nodes: $\mathcal{Z}=\bigcup\limits_{i=m}^{k}\ (\mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m})$. In this section, we present Node-Reordering, a method that re-orders nodes in $\mathcal{Z}$ according to the closeness of each node to its class center. Node-Reordering combines the geometric rankings from the unsupervised method and confidence rankings from model prediction. We first give the definitions of two rankings.

% \input{figure-compile/figure_cora_rbo.tex}

\begin{definition}
\textbf{(Geometric Rankings.)} Suppose $u\in \mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m}$, and let $h_u^U$ be the embedding of $u$. We measure the distance between node $u$ and its class center by
\begin{equation}
\label{equation1}
\centering
\delta_u = \mathop{\mathsf{distance}} \ (h^U_u, c^{\text{train}}_m)
\end{equation}
where $c^{\text{train}}_m$ is the class center of class $m$ (see \eqref{eqn:classcenter}). For each class $m$, we sort nodes in $\mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m}$ in the increasing order of their distance to the class center, so we obtain $k$ sorted lists $\{\mathcal{S}_{1}, \mathcal{S}_{2},  \cdots,  \mathcal{S}_{k} \}$, which we call \textit{geometric rankings}. 
\end{definition}


\begin{definition}
\textbf{(Confidence Rankings.)} For each node $u\in \mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m}$, we can get a classification \textit{confidence} for the node from the output of the classifier as follow:
\begin{equation}
\centering
\mathrm{confidence} = \mathop{\mathsf{max}} \ (\mathop{\mathsf{softmax}} \ (logits)),
\end{equation}
Here, $logits$ is the output of the neural network, usually a $k$ (number of classes) dimensional vector. The pseudo-labels of $u$ from the classifier are the index of the class with the highest prediction probability and the corresponding probability is its confidence. We sort nodes in $\mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m}$ in the decreasing order of their confidence, and obtain another $k$ sorted lists $\{\mathcal{T}_{1}, \mathcal{T}_{2},  \cdots,  \mathcal{T}_{k} \}$, which we call \textit{confidence rankings}.
\end{definition}



\paragraph{Rank Biased Overlap.} In the fields of information retrieval and recommendation systems, a fundamental task is to measure the similarity between two rankings. Rank Biased Overlap (RBO) \citep{webber2010similarity} compares two ordered lists and returns a numeric value between zero and one to quantify their similarity. An RBO value of zero indicates the lists are completely different, and an RBO of one means completely identical.



\paragraph{Node-Reordering.}
Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning} has verified that the prediction results of unlabeled nodes given by the bias classifier are not reliable, and Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering} has verified the high-quality of the embeddings. This means that the confidence rankings in early rounds are less unreliable than the geometric rankings.

For each class $m$, we calculate the RBO value between $\mathcal{S}_m$ and $\mathcal{T}_m$ and then use the RBO score as a weight and get the weighted combination of the two rankings. More specifically, we first compute $r_m = \text{RBO}(\mathcal{S}_m, \mathcal{T}_m)$, and then compute
\begin{equation}
\mathcal{N}_{m}^{New}= \max\{r_{m}, 1-r_m\} \cdot\mathcal{S}_{m}+\min \{r_m, 1-r_m\}\cdot\mathcal{T}_{m},
\end{equation}
We then select nodes according to the new ranking based on values in $\mathcal{N}_{m}^{New}$. Note that we always make the geometric rankings have the dominating influence in this step. 

A natural question is why not filter nodes based solely on geometric rankings. UNREAL selects new nodes iteratively for multiple rounds, and as iterations progress, the training set becomes more and more balanced. So in the later stages, the confidence rankings given by the classifier are valuable. The effectiveness of Node-Reordering is empirically verified in Appendix \ref{Additional analysis for Node-Reordering and DGIN}. At the same time, our experiments also show that as iterations progress, the RBO value (similarity) of the two rankings increases (Figure \ref{figure_coragcn_rbo} and Figure \ref{figure_coragat_rbo}).













\subsection{Geometric Imbalance}
\label{Geometric Imbalance}

In this section, we consider the issue of Geometric Imbalance (GI) in the embedding space and define a simple and effective metric to measure GI.


% \paragraph{Motivating Example.} 

\paragraph{Geometric Imbalance.} 
%In highly imbalanced scenarios, minority nodes often suffer from topology imbalance \citep{song2022tam, chen2021topology}, which means the labeled node stays near the boundary between a minority class and a majority class. \ourmodel iteratively assigns pseudo-labels to a portion of unlabeled nodes (Section \ref{Selecting New Nodes Iteratively}), which means that some suboptimal nodes (not so close to class centers) are gradually considered in later stages. Too many nodes from the classification boundaries will affect the decision boundaries finally. Also, such nodes are often close to another class center, which will lead to the conflict problem of pseudo-labeling. We refer to this issue as geometric imbalance in the embedding space.


% We present a visualization to illustrate geometric imbalance, which is in Figure \ref{TSNE} due to space constraints. At the same time, we also want to check whether the nodes that encounter geometric imbalance are assigned the correct labels. We conduct experiments on Cora to validate this hypothesis.


In highly imbalanced scenarios, minority nodes often suffer from topology imbalance \citep{song2022tam, chen2021topology}, which means the labeled node stays near the boundary between a minority class and a majority class. The geometric ranking and DPAM introduced above partially alleviate this issue. However, when the class centers of two classes are very close in the embedding space, the problem may still exist. Consider two nodes $u$ and $v$ which belong to class $1$ and $2$ respectively. When the centers of class $1$ and $2$ are very close and $v$ is on their boundary in the embedding space, $u,v$ are likely both assigned with pseudo-label $1$ and $v$ has a higher geometric ranking than $u$ w.r.t.\ class $1$. We refer to this issue as the geometric imbalance in the embedding space. 
\begin{figure}[!ht]
\centering
\includegraphics[width=6cm]{new100.pdf}
\caption{The diagram of GI and DGIN.}
\end{figure}
\paragraph{Discarding Geometrically Imbalanced Nodes (DGIN).} Intuitively, if a node is very close to the centers of more than one class simultaneously, it should not be selected as there is high uncertainty in the pseudo-label. Therefore, we define a simple and natural metric to measure the degree of GI. According to \eqref{equation1}, $\delta_u$ refers to the distance between the embedding of $u$ and the center of the class to which $u$ is assigned (i.e., the closest class center among all classes). Similarly, we define $\beta_u$ as the distance between the embedding of $u$ and the second closest center to $u$. We have $\delta_u\le \beta_u$ for all $u$, and intuitively, if $\delta_u\approx \beta_u$, then $u$ is likely to have high degree of GI. We thus define the metric for measuring GI as
\begin{equation}
\centering
\text{GI}_{u}=\frac{\beta_u-\delta_u}{\delta_u}.
\end{equation}
We refer to the metric as the GI index. The GI issue is more serious on the node with a smaller GI index. So we set a threshold and discard all nodes with GI index below the threshold. 
% We empirically verify the effectiveness of DGI, and the results and analysis are provided in Appendix \ref{DGI more}.


The effectiveness of DGIN is empirically verified in Appendix \ref{Additional analysis for DI and DGIN}.

\subsection{Selecting New Nodes Iteratively}
\label{Selecting New Nodes Iteratively}
As in standard ST, we select nodes to join the training set in several rounds, and in each round, we retrain the model using the newly formed training set. In highly-imbalanced cases, we only add nodes from minority classes. In this way, the label distribution of the training set is gradually smoothed, and the imbalance issue is alleviated.




\section{Experiment} 
\label{Experiment}

\subsection{Experimental Setups}
% \input{iclr2023/tables/ratio10.tex}
% \input{iclr2023/tables/ratio20.tex}

\paragraph{Datasets.} We validate the advantages of our method on five benchmark datasets %(i.e. Cora, CiteSeer, PubMed, Amazon-Computers, and Flickr) 
under different imbalance scenarios, in which the step imbalance scheme given in  \citep{zhao2021graphsmote, park2021graphens, song2022tam} is adopted to construct class imbalanced datasets. More specifically, we choose half of the classes as minority classes and convert randomly picked labeled nodes into unlabeled ones until the imbalance ratio of the training set reaches $\rho$. For Flickr, in the public split, the training set is already imbalanced, and thus we directly use this split and do not make any changes. For the three citation networks (Cora, CiteSeer, Pubmed), we use the standard splits from \citet{yang2016revisiting} as our initial splits when the imbalance ratio is 10, 20. To create a larger imbalance ratio, 20 labeled nodes per class is not enough, and we use a random split as the initial split for creating an imbalance ratio of  50 and 100. The detailed experimental settings such as evaluation protocol and implementation details of our algorithm are described in Appendix \ref{Details of the experimental setup}.


\paragraph{Baselines.} We compare UNREAL with several classic techniques (cross-entropy loss with re-weighting \citep{japkowicz2002class}, PC Softmax \citep{hong2021disentangling} and Balanced Softmax \citep{ren2020balanced}) and state-of-the-art methods for imbalanced node classification, including GraphSMOTE \citep{zhao2021graphsmote}, GraphENS \citep{park2021graphens},  ReNode \citep{chen2021topology}, and TAM \citep{song2022tam}. Among them, GraphSMOTE and GraphENS are representative over-sampling methods for node classification, and ReNode and TAM are loss function modification approaches. For TAM, we test its performances when combined with different base models, including GraphENS, ReNode, and Balanced Softmax, following \cite{song2022tam}. The implementation details of baselines are described in Appendix \ref{baselines}. 



\subsection{Main Results}
\paragraph{Experimental Results Under Different Imbalance Ratios.} In Table \ref{ratio10gcn} and Table \ref{ratio20}, we report the averaged balanced accuracy (bAcc.) and F1 score with standard errors for the baselines and UNREAL on four class-imbalanced node classification benchmark datasets under different imbalance ratios (${\rho}=10, 20$). The results demonstrate the advantage of \ourmodel. Our method consistently outperforms existing state-of-the-art approaches across four datasets, three base models, and two imbalance ratios (except for GraphSAGE on Amazon-Computers with imbalance ratio 10). In many cases the margin is significant. To evaluate the performance on very skewed label distribution, we also test in more imbalanced settings (${\rho} =50, 100$) and present the results in Table \ref{ratio50} and Table \ref{ratio100} of Appendix \ref{More results in heavily-imbalanced scenarios}. Similarly, our method outperforms all other methods consistently and often by a notable margin. We remark that since GraphSMOTE \citep{zhao2021graphsmote} synthesizes nodes within the minority class, it is not applicable when there is only one node in some classes, which is the case when ${\rho} =20, 50, 100$ in our setup.


\begin{table*}[!ht]
    \centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Experimental results of our method UNREAL and other baselines on four class-imbalanced node classification benchmark datasets with ${\rho=10}$. We report averaged balanced accuracy (bAcc.,$\%$) and F1-score ($\%$) with the standard errors over 5 repetitions on the GCN architecture.}
        \label{ratio10gcn}
 \scalebox{0.73}{\begin{tabular}{lllllllllll}
     \toprule[1.5 pt]
     \multirow{43}{*}{\rotatebox{90}{SAGE \qquad\qquad\quad\qquad\qquad\quad \qquad\qquad GAT \qquad\qquad\quad\qquad\qquad\quad \qquad\qquad GCN}} & \multicolumn{1}{c}{\bf{Dataset}} & \multicolumn{2}{c}{Cora} & \multicolumn{2}{c}{CiteSeer} &  \multicolumn{2}{c}{PubMed} & \multicolumn{2}{c}{Amazon-Computers} &\\


     

     \cmidrule(lr){2-10}
     
     & \bf{Imbalance Ratio $({\rho=10})$} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}  \\

     \cmidrule(lr){2-10}
     
     & Vanilla & 62.82 $\pm$ 1.43 & 61.67 $\pm$ 1.59 & 38.72 $\pm$ 1.88 &28.74 $\pm$ 3.21 & 65.64 $\pm$ 1.72 & 56.97 $\pm$  3.17 & 80.01 $\pm$ 0.71 & 71.56 $\pm$ 0.81\\


     & Re-Weight & 65.36 $\pm$ 1.15 & 64.97 $\pm$ 1.39 & 44.69 $\pm$ 1.78 &38.61 $\pm$ 2.37 & 69.06 $\pm$ 1.84 & 64.08 $\pm$  2.97 &80.93 $\pm$ 1.30 & 73.99 $\pm$ 2.20\\
     
     & PC Softmax & 68.04 $\pm$ 0.82 & 67.84 $\pm$ 0.81 & 50.18 $\pm$ 0.55 &46.14 $\pm$ 0.14 & 72.46 $\pm$ 0.80 & 70.27 $\pm$  0.94&81.54 $\pm$ 0.76 & 73.30 $\pm$ 0.51\\

     & BalancedSoftmax & 69.98 $\pm$ 0.58 & 68.68 $\pm$ 0.55 & 55.52 $\pm$ 0.97 &53.74 $\pm$ 1.42 & 73.73 $\pm$ 0.89 & 71.53 $\pm$  1.06 &81.46 $\pm$ 0.74 &  \underline{74.31 $\pm$ 0.51}\\

     & GraphSMOTE & 66.39 $\pm$ 0.56 & 65.49 $\pm$ 0.93 & 44.87 $\pm$ 1.12 &39.20 $\pm$ 1.62 & 67.91 $\pm$ 0.64  & 62.68 $\pm$  1.92 &79.48 $\pm$ 0.47 &  72.63 $\pm$ 0.76\\
     
     & Renode & 67.03 $\pm$ 1.41 & 67.16 $\pm$ 1.67 & 43.47 $\pm$ 2.22 &37.52 $\pm$ 3.10 & 71.40 $\pm$ 1.42  & 67.27 $\pm$ 2.96 &81.89 $\pm$ 0.77 & 73.13 $\pm$ 1.60\\


     & GraphENS & 70.89 $\pm$ 0.71 & 70.90 $\pm$ 0.81 & 56.57 $\pm$ 0.98 &55.29 $\pm$ 1.33 & 72.13 $\pm$ 1.04  & 70.72 $\pm$ 1.07 &\underline{82.40 $\pm$ 0.39} & 74.26 $\pm$ 1.05 \\

     & BalancedSoftmax+TAM & 69.94 $\pm$ 0.45 & 69.54 $\pm$ 0.47 & 56.73 $\pm$ 0.71 &56.15 $\pm$ 0.78 & 74.62 $\pm$ 0.97  & 72.25 $\pm$ 1.30 &82.36 $\pm$ 0.67 & 72.94 $\pm$ 1.43 \\

     & Renode+TAM & 68.26 $\pm$ 1.84 & 68.11 $\pm$ 1.97 & 46.20 $\pm$ 1.17 &39.96 $\pm$ 2.76 & 72.63 $\pm$ 2.03  & 68.28 $\pm$ 3.30 &80.36 $\pm$ 1.19 & 72.51 $\pm$ 0.68\\

     & GraphENS+TAM & \underline{71.69 $\pm$ 0.36} & \underline{72.14 $\pm$ 0.51} & \underline{58.01 $\pm$ 0.68} &\underline{56.32 $\pm$ 1.03} &  \underline{74.14 $\pm$ 1.42}  & \underline{72.42 $\pm$ 1.39} &81.02 $\pm$ 0.99 & 70.78 $\pm$ 1.72 \\
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{78.33 $\pm$ 1.04}   & \textbf{76.44  $\pm$ 1.06}  & \textbf{65.63  $\pm$ 1.38}   & \textbf{64.94 $\pm$ 1.38} & \textbf{75.35 $\pm$ 1.41}   & \textbf{73.65 $\pm$ 1.43} &\textbf{85.08 $\pm$ 0.38} & \textbf{75.27 $\pm$ 0.23}\\
    
     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+6.64}} & \multicolumn{1}{c}{\textbf{+4.30}} & \multicolumn{1}{c}{\textbf{+7.62}} & \multicolumn{1}{c}{\textbf{+8.62}} & \multicolumn{1}{c}{\textbf{+1.21}} & \multicolumn{1}{c}{\textbf{+1.23}} & \multicolumn{1}{c}{\textbf{+2.68}} & \multicolumn{1}{c}{\textbf{+0.96}}  \\
     
 
     % \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     % &Vanilla & 62.33 $\pm$ 1.56 & 61.82 $\pm$ 1.84 & 38.84 $\pm$ 1.13 &31.25 $\pm$ 1.64 &  64.60 $\pm$ 1.64  & 55.24 $\pm$ 2.80 & 79.04 $\pm$ 1.60 & 70.00 $\pm$ 2.50  \\

     % &Re-Weight & 66.87 $\pm$ 0.97 & 66.62 $\pm$ 1.13 & 45.47 $\pm$ 2.35 &40.60 $\pm$ 2.98 &  68.10 $\pm$ 2.85   & 63.76 $\pm$ 3.54  &80.38 $\pm$ 0.66 & 69.99 $\pm$ 0.76\\

     % &PC Softmax & 66.69 $\pm$ 0.79 & 66.04 $\pm$ 1.10 & 50.78 $\pm$ 1.66 &48.56 $\pm$ 2.08 &  72.88 $\pm$ 0.83   & 71.09 $\pm$ 0.89  &79.43 $\pm$ 0.94 & 71.33 $\pm$ 0.86\\

     % &BalancedSoftmax & 67.89 $\pm$ 0.36 & 67.96 $\pm$ 0.41 & 54.78 $\pm$ 1.25 &51.83 $\pm$ 2.11 & 72.30 $\pm$ 1.20   & 69.30 $\pm$ 1.79  &\underline{82.02 $\pm$ 1.19} & \underline{72.94 $\pm$ 1.54}\\

     % &GraphSMOTE & 66.71 $\pm$ 0.32 & 65.01 $\pm$ 1.21 & 45.68 $\pm$ 0.93 &38.96 $\pm$ 0.97 & 67.43 $\pm$ 1.23   & 61.97 $\pm$ 2.54  &79.38 $\pm$ 1.97 & 69.76 $\pm$ 2.31\\

     % &Renode & 67.33 $\pm$ 0.79 & 68.08 $\pm$ 1.16& 44.48 $\pm$ 2.06 &37.93 $\pm$ 2.87 & 69.93 $\pm$ 2.10   & 65.27 $\pm$ 2.90 &76.01 $\pm$ 1.08 & 66.72 $\pm$ 1.42\\
     
     % &GraphENS & \underline{70.45 $\pm$ 1.25} & 69.87 $\pm$ 1.32 & 51.45 $\pm$ 1.28 &47.98 $\pm$ 2.08 & 73.15 $\pm$ 1.24   & 71.90 $\pm$ 1.03 &81.23 $\pm$ 0.74 & 71.23 $\pm$ 0.42\\ 

     % &BalancedSoftmax+TAM & 69.16 $\pm$ 0.27 & 69.39 $\pm$ 0.37 & 56.30 $\pm$ 1.25 &53.87 $\pm$ 1.14 & 73.50 $\pm$ 1.24   & 71.36 $\pm$ 1.99 &75.54 $\pm$ 2.09 & 66.69 $\pm$ 1.44\\
     % &Renode+TAM & 67.50 $\pm$ 0.67 & 68.06 $\pm$ 0.96 & 45.12 $\pm$ 1.41 &39.29 $\pm$ 1.79 & 70.66 $\pm$ 2.13   & 66.94 $\pm$ 3.54 &74.30 $\pm$ 1.13 & 66.13 $\pm$ 1.75\\

     % &GraphENS+TAM & 70.15 $\pm$ 0.18 & \underline{70.00 $\pm$ 0.40} & \underline{56.15 $\pm$ 1.13} &\underline{54.31 $\pm$ 1.68} & \underline{73.45 $\pm$ 1.07}   & \underline{72.10 $\pm$ 0.36} &81.07 $\pm$ 1.03 & 71.27 $\pm$ 1.98  \\
     
     
     % \cmidrule(lr){2-10}
     
     % & \textbf{UNREAL} &\textbf{78.91 $\pm$ 0.59}  & \textbf{75.99 $\pm$ 0.47}   & \textbf{64.10 $\pm$ 1.49} &\textbf{63.44 $\pm$ 1.47} & \textbf{74.68 $\pm$ 1.43} &  \textbf{72.78 $\pm$ 0.89} &\textbf{85.62 $\pm$ 0.44} & \textbf{75.34 $\pm$ 0.99}\\
     
     % \cmidrule(lr){2-10}
     
     % & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+8.46}} & \multicolumn{1}{c}{\textbf{+5.99}} & \multicolumn{1}{c}{\textbf{+7.80}} & \multicolumn{1}{c}{\textbf{+9.13}} & \multicolumn{1}{c}{\textbf{+1.23}} & \multicolumn{1}{c}{\textbf{+0.68}} &\multicolumn{1}{c}{\textbf{+3.60}} & \multicolumn{1}{c}{\textbf{+2.40}}\\
     
     % \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     % &Vanilla & 61.82 $\pm$ 0.97 & 60.97 $\pm$ 1.07 & 43.18 $\pm$ 0.52 &36.66 $\pm$ 1.25 &  68.68 $\pm$ 1.51  & 64.16 $\pm$ 2.38 & 72.36 $\pm$ 2.39 & 64.32 $\pm$ 2.21\\

     % &Re-Weight & 63.94 $\pm$ 1.07 & 63.82 $\pm$ 1.30 & 46.17 $\pm$ 1.32 &40.13 $\pm$ 1.68 &  69.89 $\pm$ 1.60   & 65.71 $\pm$ 2.31 &76.08 $\pm$ 1.14 & 65.76 $\pm$ 1.40\\

     % &PC Softmax & 65.79 $\pm$ 0.70 & 66.04 $\pm$ 0.92 & 50.66 $\pm$ 0.99 &47.48 $\pm$ 1.66 &  71.49 $\pm$ 0.94  & 70.23 $\pm$ 0.67 &74.63 $\pm$ 3.01 & 66.44 $\pm$ 4.04\\

     % &BalancedSoftmax & 67.43 $\pm$ 0.61 & 67.66 $\pm$ 0.69 & 51.74 $\pm$ 2.32 &49.01 $\pm$ 3.16 & 71.36 $\pm$ 1.37   & 69.66 $\pm$ 1.81 &73.67 $\pm$ 1.11 & 65.23 $\pm$ 2.44\\

     % &GraphSMOTE & 61.65 $\pm$ 0.34 & 60.97 $\pm$ 0.98 & 42.73 $\pm$ 2.87 &35.18 $\pm$ 1.75 & 66.63 $\pm$ 0.65   & 61.97 $\pm$ 2.54 &71.85 $\pm$ 0.98 & 68.92 $\pm$ 0.73\\

     % &Renode & 66.84 $\pm$ 1.78 & 67.08 $\pm$ 1.75& 48.65 $\pm$ 1.37 &44.25 $\pm$ 2.20 & 71.37 $\pm$ 1.33   & 67.78 $\pm$ 1.38 &77.37 $\pm$ 0.74 & 68.42 $\pm$ 1.81\\
     
     % &GraphENS & 68.74 $\pm$ 0.46 & 68.34 $\pm$ 0.33 & 53.51 $\pm$ 0.78 &51.42 $\pm$ 1.19 & 70.97 $\pm$ 0.78   & 70.00 $\pm$ 1.22 & \underline{82.57 $\pm$ 0.50} & 71.95 $\pm$ 0.51\\ 

     % &BalancedSoftmax+TAM & 69.03 $\pm$ 0.92 & 69.03 $\pm$ 0.97 & 51.93 $\pm$ 2.19 &48.67 $\pm$ 3.25 & 72.28 $\pm$ 1.47   & 71.02 $\pm$ 1.31 &77.00 $\pm$ 2.93 & 70.85 $\pm$ 2.28 \\
     % &Renode+TAM & 67.28 $\pm$ 1.11 & 67.15 $\pm$ 1.11 & 48.39 $\pm$ 1.76&43.56 $\pm$ 2.31 & 71.25 $\pm$ 1.07   & 68.69 $\pm$ 0.98 &74.87 $\pm$ 2.25 & 66.87 $\pm$ 2.52 \\

     % &GraphENS+TAM & \underline{70.45 $\pm$ 0.74} & \underline{70.40 $\pm$ 0.75} & \underline{54.69 $\pm$ 1.12} &\underline{53.56 $\pm$ 1.86} & \underline{73.61 $\pm$ 1.35}   & \underline{72.50 $\pm$ 1.58} &82.17 $\pm$ 0.93 & \textbf{72.46 $\pm$ 1.00}\\
     
     %      \cmidrule(lr){2-10}
     
     % & \textbf{UNREAL} &\textbf{75.99 $\pm$ 0.98}  & \textbf{73.63 $\pm$ 1.23}   &\textbf{66.45  $\pm$ 0.39} &\textbf{65.83 $\pm$ 0.30} & \textbf{74.78 $\pm$ 1.30}  & \textbf{72.80 $\pm$ 0.54} & \textbf{83.21 $\pm$ 1.50} & \underline{70.81 $\pm$ 1.70}\\
     %      \cmidrule(lr){2-10}
     
     % & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+5.44}} & \multicolumn{1}{c}{\textbf{+3.23}} & \multicolumn{1}{c}{\textbf{+11.76}} & \multicolumn{1}{c}{\textbf{+12.77}} & \multicolumn{1}{c}{\textbf{+1.07}} & \multicolumn{1}{c}{\textbf{+0.30}} &\multicolumn{1}{c}{\textbf{+0.64}} & \multicolumn{1}{c}{\textbf{-1.65}}\\

     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
      &Vanilla & 62.33 $\pm$ 1.56 & 61.82 $\pm$ 1.84 & 38.84 $\pm$ 1.13 &31.25 $\pm$ 1.64 &  64.60 $\pm$ 1.64  & 55.24 $\pm$ 2.80 & 79.04 $\pm$ 1.60 & 70.00 $\pm$ 2.50  \\

     &Re-Weight & 66.87 $\pm$ 0.97 & 66.62 $\pm$ 1.13 & 45.47 $\pm$ 2.35 &40.60 $\pm$ 2.98 &  68.10 $\pm$ 2.85   & 63.76 $\pm$ 3.54  &80.38 $\pm$ 0.66 & 69.99 $\pm$ 0.76\\

     &PC Softmax & 66.69 $\pm$ 0.79 & 66.04 $\pm$ 1.10 & 50.78 $\pm$ 1.66 &48.56 $\pm$ 2.08 &  72.88 $\pm$ 0.83   & 71.09 $\pm$ 0.89  &79.43 $\pm$ 0.94 & 71.33 $\pm$ 0.86\\

     &BalancedSoftmax & 67.89 $\pm$ 0.36 & 67.96 $\pm$ 0.41 & 54.78 $\pm$ 1.25 &51.83 $\pm$ 2.11 & 72.30 $\pm$ 1.20   & 69.30 $\pm$ 1.79  &\underline{82.02 $\pm$ 1.19} & \underline{72.94 $\pm$ 1.54}\\

     &GraphSMOTE & 66.71 $\pm$ 0.32 & 65.01 $\pm$ 1.21 & 45.68 $\pm$ 0.93 &38.96 $\pm$ 0.97 & 67.43 $\pm$ 1.23   & 61.97 $\pm$ 2.54  &79.38 $\pm$ 1.97 & 69.76 $\pm$ 2.31\\

     &Renode & 67.33 $\pm$ 0.79 & 68.08 $\pm$ 1.16& 44.48 $\pm$ 2.06 &37.93 $\pm$ 2.87 & 69.93 $\pm$ 2.10   & 65.27 $\pm$ 2.90 &76.01 $\pm$ 1.08 & 66.72 $\pm$ 1.42\\
     
     &GraphENS & \underline{70.45 $\pm$ 1.25} & 69.87 $\pm$ 1.32 & 51.45 $\pm$ 1.28 &47.98 $\pm$ 2.08 & 73.15 $\pm$ 1.24   & 71.90 $\pm$ 1.03 &81.23 $\pm$ 0.74 & 71.23 $\pm$ 0.42\\ 

     &BalancedSoftmax+TAM & 69.16 $\pm$ 0.27 & 69.39 $\pm$ 0.37 & 56.30 $\pm$ 1.25 &53.87 $\pm$ 1.14 & 73.50 $\pm$ 1.24   & 71.36 $\pm$ 1.99 &75.54 $\pm$ 2.09 & 66.69 $\pm$ 1.44\\
     &Renode+TAM & 67.50 $\pm$ 0.67 & 68.06 $\pm$ 0.96 & 45.12 $\pm$ 1.41 &39.29 $\pm$ 1.79 & 70.66 $\pm$ 2.13   & 66.94 $\pm$ 3.54 &74.30 $\pm$ 1.13 & 66.13 $\pm$ 1.75\\

     &GraphENS+TAM & 70.15 $\pm$ 0.18 & \underline{70.00 $\pm$ 0.40} & \underline{56.15 $\pm$ 1.13} &\underline{54.31 $\pm$ 1.68} & \underline{73.45 $\pm$ 1.07}   & \underline{72.10 $\pm$ 0.36} &81.07 $\pm$ 1.03 & 71.27 $\pm$ 1.98  \\
     
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} &\textbf{78.91 $\pm$ 0.59}  & \textbf{75.99 $\pm$ 0.47}   & \textbf{64.10 $\pm$ 1.49} &\textbf{63.44 $\pm$ 1.47} & \textbf{74.68 $\pm$ 1.43} &  \textbf{72.78 $\pm$ 0.89} &\textbf{85.62 $\pm$ 0.44} & \textbf{75.34 $\pm$ 0.99}\\
     
     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+8.46}} & \multicolumn{1}{c}{\textbf{+5.99}} & \multicolumn{1}{c}{\textbf{+7.80}} & \multicolumn{1}{c}{\textbf{+9.13}} & \multicolumn{1}{c}{\textbf{+1.23}} & \multicolumn{1}{c}{\textbf{+0.68}} &\multicolumn{1}{c}{\textbf{+3.60}} & \multicolumn{1}{c}{\textbf{+2.40}}\\
     
     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 61.82 $\pm$ 0.97 & 60.97 $\pm$ 1.07 & 43.18 $\pm$ 0.52 &36.66 $\pm$ 1.25 &  68.68 $\pm$ 1.51  & 64.16 $\pm$ 2.38 & 72.36 $\pm$ 2.39 & 64.32 $\pm$ 2.21\\

     &Re-Weight & 63.94 $\pm$ 1.07 & 63.82 $\pm$ 1.30 & 46.17 $\pm$ 1.32 &40.13 $\pm$ 1.68 &  69.89 $\pm$ 1.60   & 65.71 $\pm$ 2.31 &76.08 $\pm$ 1.14 & 65.76 $\pm$ 1.40\\

     &PC Softmax & 65.79 $\pm$ 0.70 & 66.04 $\pm$ 0.92 & 50.66 $\pm$ 0.99 &47.48 $\pm$ 1.66 &  71.49 $\pm$ 0.94  & 70.23 $\pm$ 0.67 &74.63 $\pm$ 3.01 & 66.44 $\pm$ 4.04\\

     &BalancedSoftmax & 67.43 $\pm$ 0.61 & 67.66 $\pm$ 0.69 & 51.74 $\pm$ 2.32 &49.01 $\pm$ 3.16 & 71.36 $\pm$ 1.37   & 69.66 $\pm$ 1.81 &73.67 $\pm$ 1.11 & 65.23 $\pm$ 2.44\\

     &GraphSMOTE & 61.65 $\pm$ 0.34 & 60.97 $\pm$ 0.98 & 42.73 $\pm$ 2.87 &35.18 $\pm$ 1.75 & 66.63 $\pm$ 0.65   & 61.97 $\pm$ 2.54 &71.85 $\pm$ 0.98 & 68.92 $\pm$ 0.73\\

     &Renode & 66.84 $\pm$ 1.78 & 67.08 $\pm$ 1.75& 48.65 $\pm$ 1.37 &44.25 $\pm$ 2.20 & 71.37 $\pm$ 1.33   & 67.78 $\pm$ 1.38 &77.37 $\pm$ 0.74 & 68.42 $\pm$ 1.81\\
     
     &GraphENS & 68.74 $\pm$ 0.46 & 68.34 $\pm$ 0.33 & 53.51 $\pm$ 0.78 &51.42 $\pm$ 1.19 & 70.97 $\pm$ 0.78   & 70.00 $\pm$ 1.22 & \underline{82.57 $\pm$ 0.50} & 71.95 $\pm$ 0.51\\ 

     &BalancedSoftmax+TAM & 69.03 $\pm$ 0.92 & 69.03 $\pm$ 0.97 & 51.93 $\pm$ 2.19 &48.67 $\pm$ 3.25 & 72.28 $\pm$ 1.47   & 71.02 $\pm$ 1.31 &77.00 $\pm$ 2.93 & 70.85 $\pm$ 2.28 \\
     &Renode+TAM & 67.28 $\pm$ 1.11 & 67.15 $\pm$ 1.11 & 48.39 $\pm$ 1.76&43.56 $\pm$ 2.31 & 71.25 $\pm$ 1.07   & 68.69 $\pm$ 0.98 &74.87 $\pm$ 2.25 & 66.87 $\pm$ 2.52 \\

     &GraphENS+TAM & \underline{70.45 $\pm$ 0.74} & \underline{70.40 $\pm$ 0.75} & \underline{54.69 $\pm$ 1.12} &\underline{53.56 $\pm$ 1.86} & \underline{73.61 $\pm$ 1.35}   & \underline{72.50 $\pm$ 1.58} &82.17 $\pm$ 0.93 & \textbf{72.46 $\pm$ 1.00}\\
     
          \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} &\textbf{75.99 $\pm$ 0.98}  & \textbf{73.63 $\pm$ 1.23}   &\textbf{66.45  $\pm$ 0.39} &\textbf{65.83 $\pm$ 0.30} & \textbf{74.78 $\pm$ 1.30}  & \textbf{72.80 $\pm$ 0.54} & \textbf{83.21 $\pm$ 1.50} & \underline{70.81 $\pm$ 1.70}\\
          \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+5.44}} & \multicolumn{1}{c}{\textbf{+3.23}} & \multicolumn{1}{c}{\textbf{+11.76}} & \multicolumn{1}{c}{\textbf{+12.77}} & \multicolumn{1}{c}{\textbf{+1.07}} & \multicolumn{1}{c}{\textbf{+0.30}} &\multicolumn{1}{c}{\textbf{+0.64}} & \multicolumn{1}{c}{\textbf{-1.65}}\\

     \bottomrule[1.5 pt]
  \end{tabular}}
\end{table*}

% \input{Table/ratio20gcn.tex}
% \input{Table/flickr.tex}
\begin{table*}[!ht]
\centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
\caption{Experimental results of our method UNREAL and other baselines on Computers-Random. We report averaged balanced accuracy (bAcc.,$\%$) and F1-score ($\%$) with the standard errors over 5 repetitions on three representative GNN architectures.}

 \scalebox{0.89}{\begin{tabular}{lllllllll}
     \toprule[1.5pt]
     \multirow{43}{*}{} & \multicolumn{1}{c}{\bf{Dataset (Computers-Random)}} & \multicolumn{2}{c}{GCN}  & \multicolumn{2}{c}{GAT}   &\multicolumn{2}{c}{SAGE}  &\\

     \cmidrule(lr){2-8}
     
     & \bf{Imbalance Ratio$\bf ({\rho=25.50})$} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}& \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}\\

     \cmidrule(lr){2-8}
     
     & Vanilla & 78.43 $\pm$ 0.41 & 77.14 $\pm$ 0.39 &71.35 $\pm$1.18 & 69.60 $\pm$ 1.11 & 65.30 $\pm$ 1.07 & 64.77 $\pm$ 1.19\\

     & Re-Weight & 80.49 $\pm$ 0.44 & 75.07 $\pm$ 0.58 & 71.95 $\pm$ 0.80 & 70.67 $\pm$ 0.51 & 66.50 $\pm$ 1.47 & 66.10 $\pm$ 1.46   \\
     
     & PC Softmax & 81.34  $\pm$ 0.55 & 75.17  $\pm$ 0.57 &70.56 $\pm$ 1.46 & 67.26 $\pm$ 1.48 & 69.73 $\pm$ 0.53 & 67.03 $\pm$ 0.6  \\

     & BalancedSoftmax & 81.39 $\pm$ 0.25 & 74.54 $\pm$ 0.64  & 72.09  $\pm$ 0.31 & 68.38 $\pm$ 0.69 & 73.80 $\pm$ 1.06 & 69.74 $\pm$ 0.60 \\

     & GraphSMOTE & 80.50 $\pm$ 1.11 & 73.79 $\pm$ 0.14 & 71.98 $\pm$ 0.21 & 67.98 $\pm$ 0.31 & 72.69 $\pm$ 0.82 & 68.73 $\pm$ 1.01 \\

     & Renode & 81.64 $\pm$ 0.34 & \underline{76.87 $\pm$ 0.32} & 72.80 $\pm$ 0.94 & 71.40 $\pm$ 0.97 & 70.94 $\pm$ 1.50 & 70.04 $\pm$ 1.16 \\

     & GraphENS & 82.66 $\pm$ 0.61 & 76.55 $\pm$ 0.17 & 75.25 $\pm$ 0.85 & 71.49 $\pm$ 0.54 & \underline{77.64 $\pm$ 0.52} & 72.65 $\pm$ 0.53 \\

     & BalancedSoftmax+TAM & 81.64 $\pm$ 0.48 & 75.59 $\pm$ 0.83 & 74.00 $\pm$ 0.77 & 70.72 $\pm$ 0.50 & 73.77 $\pm$ 1.26 & 71.03 $\pm$ 0.69 \\

     & Renode+TAM & 80.50 $\pm$ 1.11 & 75.79 $\pm$ 0.14 & 71.98 $\pm$ 0.21 & 70.98 $\pm$ 0.31 & 72.69 $\pm$ 0.82 & 70.73 $\pm$ 1.01 \\

     & GraphENS+TAM & \underline{82.83 $\pm$ 0.68} & 76.76 $\pm$ 0.39 & \underline{75.81 $\pm$ 0.72} & \underline{72.62 $\pm$ 0.57} & \textbf{78.98 $\pm$ 0.60} & \textbf{73.59 $\pm$ 0.55} \\
     
     \cmidrule(lr){2-8}
     
     & \textbf{UNREAL} & \textbf{85.32 $\pm$ 0.22} & \textbf{80.43 $\pm$ 0.56}  & \textbf{82.52 $\pm$ 0.35 } & \textbf{78.90 $\pm$ 0.38} & 75.81 $\pm$ 1.86  & \underline{71.86  $\pm$ 1.86}\\
     
     \cmidrule(lr){2-8}

     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+2.49}} & \multicolumn{1}{c}{\textbf{+3.97}} & \multicolumn{1}{c}{\textbf{+6.71}} & \multicolumn{1}{c}{\textbf{+6.28}}  & \multicolumn{1}{c}{\textbf{-3.17}} & \multicolumn{1}{c}{\textbf{-1.73}} \\
      
     \bottomrule[1.5pt]
     \label{ComputersRandom}
\end{tabular}}
\end{table*}

% \paragraph{Experimental results for naturally imbalanced datasets}

\paragraph{Experimental Results When Unlabeled Data is Imbalanced.}

We also validate our model on two naturally imbalanced dataset, Flickr (${\rho}\approx 10.8$) and Computers-Random (${\rho}\approx 17.7$), whose unlabeled data is also imbalanced (See Table \ref{full_graph_label_distribution}). The construction of the training set, validation set, and testing set is elaborated on Table \ref{Details of the experimental setup}. We found that existing over-sampling methods use too much memory due to synthetic node generation, and cannot handle Flickr on a 3090 GPU with 24GB memory. This include GraphENS \citep{park2021graphens}, GraphSMOTE \citep{zhao2021graphsmote} and ReNode \citep{chen2021topology}. We present the experimental results in Table \ref{ComputersRandom} and Table \ref{flickr}. More importantly, on these two datasets, UNREAL consistently outperforms other approaches.


\subsection{More Experiments and Further Analysis}
We conduct more experiments on Cora and Amazon-Computers to validate our motivations for DPAM in Appendix \ref{More Experiments of the Motivating Example}.  Besides, more novel experiments are targeted at verifying the effectiveness of DPAM in Appendix \ref{Additional analysis for DPAM}). More experiments are conducted to investigate the variation of the RBO value (similarity) of the two rankings as iterations progress in Appendix \ref{More Results and Analysis about Fluctuation of RBO Values}. More ablation studies on Node-Reordering are provided in Appendix \ref{Additional analysis for Node-Reordering and DGIN}. The effectiveness of DGIN is empirically verified in Appendix \ref{Additional analysis for DI and DGIN}). We provide sensitivity analysis on the hyperparameter $k'$, which is the number of classes in the K-Means algorithm, and on the threshold $\gamma$ of DGIN in Appendix \ref{Hyperparameter Sensitivity Analysis of UNREAL}. We conduct additional ablation studies to analyze the benefit of each component in our method Appendix \ref{Ablation analysis}. 



\section{Related work}
\label{Related work}
\paragraph{Imbalanced Learning} Most real-world data is naturally imbalanced. The major challenge in imbalanced scenarios is how to train a fair model which does not biased toward the majority classes. There are several commonly used approaches for alleviating this problem. Ensemble learning \citep{freund1997decision, liu2008exploratory, zhou2020bbn, wang2020long, liu2020mesa, cai2021ace} combines the results of multiple weak classifiers. Data re-sampling methods \citep{chawla2002smote, han2005borderline, smith2014instance, saez2015smote, kang2019decoupling, wang2021rsg} smooth the label distribution in the training set by synthesizing or duplicating minority class samples. A third class of approaches alleviates the imbalance problem by modifying the loss function, which gives larger weights to minority classes or changes the margins of different classes \citep{zhou2005training, tang2008svms, cao2019learning, tang2020long, xu2020class, ren2020balanced, wang2021adaptive}. Methods based on post-hoc correction compensate minority classes during the inference step, after model training is complete \citep{kang2019decoupling, tian2020posterior, menon2020long, hong2021disentangling}. Although these techniques have been widely applied on the i.i.d.\ data, it is not a trivial task to extend them to graph-structured data.




\paragraph{Imbalanced Learning in Node Classification} Recently, a series of research \citep{shi2020multi, wang2020network, zhao2021graphsmote, liu2021pick, qu2021imgagn, chen2021topology, park2021graphens, song2022tam} explicitly tackle the challenges brought by the topological structures of graph data when handling imbalanced node classification. GraphSMOTE \citep{zhao2021graphsmote} synthesizes minority nodes in embedding space by interpolating two minority nodes using the SMOTE \citep{chawla2002smote} algorithm and infers the neighborhoods of new nodes with link prediction algorithms. ImGAGN \citep{qu2021imgagn} generates the features of minority nodes with all of the minority nodes according to the learned weight matrix and synthesizes the neighborhoods of new nodes based on weights. \cite{qu2021imgagn} only consider binary classification, and it is computationally expensive to build a generator for each class on multi-classification tasks. GraphENS \citep{park2021graphens} works for multi-class node classification, which synthesizes the whole ego network for minority nodes by interpolating the ego networks of two nodes based on their similarity. \cite{chen2021topology} identifies topology imbalance as a main source of difficulty when handling imbalance on node classification tasks; they propose ReNode, which mitigates topology imbalance by adjusting the weights of nodes according to their distance to class boundaries. TAM \citep{song2022tam} adjusts the scores of different classes in the Softmax function based on local topology and label statistics. To obtain label information of unlabeled nodes, TAM trains the model using the original imbalanced training set and takes the model predictions as proxies for ground-truth labels. 


\paragraph{Pseudo-labeling Methods in GNNs} Recent studies have just focused on leveraging pseudo-labeling techniques to train GNNs given limited labeled information. Co-training \citep{li2018deeper}, in particular, uses Parwalks \citep{wu2012learning} to provide confident pseudolabels to help train GNNs, whereas self-training \citep{li2018deeper} expands the label set by obtaining pseudo-labels provided by previously trained GNNs. Furthermore, M3S \citep{sun2020multi} employs a clustering technique to filter out pseudo-labels that do not match the clustering assignments, thereby improving pseudo-labeling accuracy.

\section{Conclusion}
In this work, we observe that selecting unlabeled nodes instead of generating synthetic nodes in oversampling-based methods for imbalanced node classification is much simpler and more effective. We propose a novel iterative unlabeled node selection and retraining framework, which effectively selects high-quality new samples from the unlabeled sets to smooth the label distribution of the training set. Moreover, we propose to exploit the geometric structure in the node embedding space to compensate for the bias in the model predictions. Extensive experimental results show that UNREAL consistently outperforms existing state-of-the-art approaches by large margins.


% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite


\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



\begin{center}
\textbf{\LARGE Supplementary Material}
\end{center}



\section{More Experiments About Imbalanced Learning with Unlabled Data (Section \ref{Imbalanced Learning with Unlabeled Data})}
\label{More Results About Imbalanced Learning with Unlabled Data}

We provide complete evaluation results on more benchmark datasets for Section \ref{ST Improves the Performance of GNN in Imbalanced Learning} and Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}, where more basic models are included in addition to the reported results in the main paper.

\subsection{More Experiments for ST Improves the Performance of GNN in Imbalanced Learning (Section \ref{ST Improves the Performance of GNN in Imbalanced Learning})} 
\paragraph{Details of Experimental Setup.} We chose the three citation datasets, Cora, CiteSeer, and PubMed, to build scenarios with varying degrees of imbalance. To be more specific, we select half of the classes as minority classes and convert randomly selected labeled nodes into unlabeled nodes until the training set's imbalance ratio reaches $\rho$. We fix architecture as the 2-layer GNN (i.e. GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, GraphSAGE \citep{hamilton2017inductive})  having 128 hidden dimensions and train models for 2000 epochs. For the ST method, the size of added nodes for each class is a hyperparameter that we tune based on the validation set's accuracy. We repeat each experiment five times and report the average experiment results under different imbalance ratios in Figure \ref{figure_alldataset_performance}.

\paragraph{Analysis.} More results are presented to confirm the effectiveness of ST in boosting imbalanced learning on Cora, CiteSeer, and PubMed using three GNN architectures in Figure \ref{figure_alldataset_performance}. It can be observed that, across different ratios and data sets, ST consistently outperforms the vanilla model by a wide margin, confirming the positive value of unlabeled nodes. More notably, we can discover that ST performs gradually poorly in heavily imbalanced scenarios, especially for Cora and CiteSeer. In this work, we argue that for highly imbalanced data, ST is unlikely to achieve optimal performance because classifiers' biased and untrustworthy predictions may introduce low-quality nodes into the training set early on.

\begin{figure*}[!ht]
\centering
\subfigure[Cora-GAT]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{cora_gat_performance.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[Cora-SAGE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{cora_sage_performance.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[CiteSeer-GCN]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{citeseer_gcn_performance.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[CiteSeer-GAT]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{citeseer_gat_performance.pdf}
%\caption{fig1}
\end{minipage}%
}%


\subfigure[CiteSeer-SAGE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{citeseer_sage_performance.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[PubMed-GCN]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{pubmed_gcn_performance.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[PubMed-GAT]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{pubmed_gat_performance.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[PubMed-SAGE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.11]{pubmed_sage_performance.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The experimental results on the three citation datasets under different imbalance scenarios ($\rho$ = 10, 20, 50, 100). We report the F1-score ($\%$) with the standard errors of Vanilla, ST, and UNREAL.}
\label{figure_alldataset_performance}
\end{figure*}
\begin{figure*}[!ht]
\centering
\subfigure[Cora-GAT]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{cora_gat_node.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[Cora-SAGE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{cora_sage_node.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[CiteSeer-GCN]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{citeseer_gcn_node.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[CiteSeer-GAT]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{citeseer_gat_node.pdf}
%\caption{fig1}
\end{minipage}%
}%



\subfigure[CiteSeer-SAGE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{citeseer_sage_node.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[PubMed-GCN]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{pubmed_gcn_node.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[PubMed-GAT]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{pubmed_gat_node.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[PubMed-SAGE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[scale=0.13]{pubmed_sage_node.pdf}
%\caption{fig2}
\end{minipage}
}%


\subfigure[Computers-GCN]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[scale=0.13]{computers_gcn_node.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[Computers-GAT]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[scale=0.13]{computers_gat_node.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[Computers-SAGE]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[scale=0.13]{computers_sage_node.pdf}
%\caption{fig2}
\end{minipage}
}%


\centering
\caption{Here, we present the experimental results from four benchmark datasets under various imbalance scenarios. We select top 100 unlabeled nodes newly added to the training set via ST \& UNREAL, and evaluate the performance of ST \& UNREAL based on three GNN architectures by testing the accuracy with the standard errors of these nodes' pseudo labels. Minor means that we only test unlabeled nodes which are selected into the minority classes, and Major means that we only test unlabeled nodes which are selected into the majority classes.}
\label{figure_alldataset_node}
\end{figure*}



\subsection{More Experiments for Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning (Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning})}

\paragraph{Details of Experimental Setup.} Since true labels for all benchmark nodes are provided, we first conduct experiments to test the accuracy of pseudo labels for unlabeled nodes on class-imbalanced graphs inventively. We select top 100 unlabeled nodes newly added to the training set through ST \& UNREAL, and evaluate the performance of ST \& UNREAL by testing the accuracy ($\%$) with the standard errors of these nodes' pseudo labels.  We test unlabeled nodes that are selected into the minority classes and unlabeled nodes that are selected into the majority classes separately. We evaluate the performance of each method on Cora, CiteSeer, PubMed, Amazon-Computers under different imbalance scenarios. We process the datasets with a traditional imbalanced distribution following \citet{zhao2021graphsmote, park2021graphens, song2022tam}. The imbalance ratio $\rho$ between the numbers of the most frequent class and the least frequent class is set as 1 (balanced), 5, 10, 20, 50, 100. We fix architecture as the 2-layer GNN (i.e. GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, GraphSAGE \citep{hamilton2017inductive})  having 128 hidden dimensions and train models for 2000 epochs. The validation accuracy is used to select the model. Each experiment is repeated five times, and the average experiment results are reported in Figure \ref{figure_alldataset_node}.

 


 \paragraph{Analysis.} As shown in Figure \ref{figure_alldataset_node}, in different imbalanced scenarios for ST, the accuracy of the pseudo labels for the unlabeled nodes selected into the minority classes and the majority classes of the training set are reported. We can see that as $\rho$ increases, the accuracy of pseudo labels for unlabeled nodes selected into minority classes decreases, implying that the influence of the classifier's bias increases. The end results demonstrate a number of intriguing aspects. (1) This means that the classifier's pseudo-labels are not credible in a highly imbalanced scenario. More seriously, we want to add unlabeled nodes whose pseudo-labels are minority classes to the training set, which will cause ST to add excessive noise during the training process. (2) It's noteworthy that to evaluate the performance of ST, we only focus on the top 100 nodes which are pick out based on Confidence Rankings (Section \ref{reorder}) from the classifier. So, we believe that even if a node's pseudo-label is correct, the classifier's confidence is skewed, which means that we may include low-quality unlabeled nodes in the training set while ignoring high-quality unlabeled nodes. This, we believe, is the primary factor of the ST's poor performance in imbalanced scenarios. (3) More importantly, regardless of selecting majority class nodes or minority class nodes, UNREAL consistently outperforms ST. Looking back at Figure \ref{figure_alldataset_performance}, it is clear that UNREAL consistently outperforms ST by a wide margin, and as the imbalance ratio increases, so does the gap in F1 scores between ST and our method.

\newpage
\section{More Results of Main Experiments (Section \ref{Experiment})}

\subsection{More Results in Heavily-imbalanced Scenarios}
\label{More results in heavily-imbalanced  scenarios}
Due to space limitations, we present the complete experimental results here. In Table \ref{ratio20}, Table \ref{ratio50}, and Table \ref{ratio100}, we report the results in heavily imbalanced scenarios ($\rho$ = 20, 50, 100).

\begin{table}[!h]
    \centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Experimental results of our method UNREAL and other baselines on four class-imbalanced node classification benchmark datasets with ${\rho=20}$. We report averaged balanced accuracy (bAcc.,$\%$) and F1-score ($\%$) with the standard errors over 5 repetitions on three representative GNN architectures.}
        \label{ratio20}
 \scalebox{0.73}{\begin{tabular}{lllllllllll}
     \toprule[1.5 pt]
     \multirow{40}{*}{\rotatebox{90}{SAGE \qquad\qquad\quad\qquad\qquad\quad \qquad\quad GAT \quad\qquad\qquad\quad\qquad\qquad\quad\quad\quad GCN}} & \multicolumn{1}{c}{\bf{Dataset}} & \multicolumn{2}{c}{Cora} & \multicolumn{2}{c}{CiteSeer} &  \multicolumn{2}{c}{PubMed} & \multicolumn{2}{c}{Amazon-Computers} &\\

     \cmidrule(lr){2-10}
     
     & \bf{Imbalance Ratio $({\rho=20})$} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}  \\

     \cmidrule(lr){2-10}
     
     & Vanilla & 53.20 $\pm$ 0.88 & 47.81 $\pm$ 1.23 & 35.32 $\pm$ 0.15 &21.81 $\pm$ 0.12 & 61.13 $\pm$ 0.35 & 46.85 $\pm$  0.76 & 72.34 $\pm$ 2.92 & 65.42 $\pm$ 3.00 \\

     & Re-Weight & 57.51 $\pm$ 1.05 & 54.63 $\pm$ 1.08 & 36.99 $\pm$ 1.79 &27.33 $\pm$ 2.32 & 66.52 $\pm$ 2.42 & 58.22 $\pm$  3.65 &72.45 $\pm$ 2.06 & 65.85 $\pm$ 1.46 \\
     
     & PC Softmax & 61.74 $\pm$ 1.50 & 60.55 $\pm$ 1.97 & 42.53 $\pm$ 1.53 &36.54 $\pm$ 1.13 & 68.26 $\pm$ 1.99 & 66.54 $\pm$  1.87 &73.84 $\pm$ 2.64 & 66.32 $\pm$ 2.97 \\

     & BalancedSoftmax & 64.06 $\pm$ 0.74 & 62.88 $\pm$ 0.86 & 47.29 $\pm$ 1.29 &44.08 $\pm$ 1.71 & 69.71 $\pm$ 1.74 & 68.31 $\pm$  1.71 &76.92 $\pm$ 2.01 & 69.86 $\pm$ 1.99 \\

     
     & Renode & 59.40 $\pm$ 1.00 & 56.88 $\pm$ 1.52 & 38.25 $\pm$ 1.60 &27.61 $\pm$ 2.25 & 67.45 $\pm$ 3.34  & 60.40 $\pm$ 5.74 &74.15 $\pm$ 1.72 & 67.27 $\pm$ 0.92 \\


     & GraphENS & \underline{67.30 $\pm$ 1.45} & \underline{66.82 $\pm$ 1.40}& 46.39 $\pm$ 3.48 &42.38 $\pm$ 4.14 & 71.37 $\pm$ 1.77  & \underline{69.37 $\pm$ 1.69} &75.41 $\pm$ 1.75 & 69.32 $\pm$ 1.58 \\

     & BalancedSoftmax+TAM & 64.75 $\pm$ 0.54 & 63.46 $\pm$ 0.72 & 48.52 $\pm$ 1.62 &\underline{46.38 $\pm$ 1.79} & 69.95 $\pm$ 2.09  & 68.90 $\pm$ 1.86 & \underline{77.09 $\pm$ 2.02} & \underline{69.86 $\pm$ 1.76}\\

     & Renode+TAM & 59.88 $\pm$ 1.16 & 58.05 $\pm$ 1.66 & 41.11 $\pm$ 2.45 &31.58 $\pm$ 2.62 & 68.53 $\pm$ 3.53  & 64.82 $\pm$ 4.32 &73.46 $\pm$ 1.77 & 67.50 $\pm$ 1.18 \\

     & GraphENS+TAM & 66.94 $\pm$ 1.38 & 66.67 $\pm$ 1.42 & \underline{48.80 $\pm$ 2.98} &45.06 $\pm$ 4.16 &  \underline{71.92 $\pm$ 1.58}  & 69.35 $\pm$ 1.88 &75.78 $\pm$ 1.57 & 68.58 $\pm$ 1.78 \\
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{77.02 $\pm$ 0.75} & \textbf{74.15 $\pm$ 0.87} & \textbf{55.81 $\pm$ 6.11} &\textbf{55.19 $\pm$ 6.23} &  \textbf{73.06 $\pm$ 1.87}  &\textbf{70.77 $\pm$ 1.96} &\textbf{85.69 $\pm$ 0.11} & \textbf{74.81 $\pm$ 0.68} \\

     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+9.72}} & \multicolumn{1}{c}{\textbf{+7.33}} & \multicolumn{1}{c}{\textbf{+7.01}} & \multicolumn{1}{c}{\textbf{+8.81}} & \multicolumn{1}{c}{\textbf{+1.14}} & \multicolumn{1}{c}{\textbf{+1.40}} &\multicolumn{1}{c}{\textbf{+8.60}} & \multicolumn{1}{c}{\textbf{+4.95}} \\

     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 51.51 $\pm$ 0.53 & 46.59 $\pm$ 0.61 & 34.74 $\pm$ 0.16 &22.00 $\pm$ 0.15 &  60.22 $\pm$ 0.47  & 46.03 $\pm$ 0.70 &68.09 $\pm$ 2.96 & 60.08 $\pm$ 2.76 \\

     &Re-Weight & 58.68 $\pm$ 3.44 & 55.98 $\pm$ 3.97 & 36.78 $\pm$ 0.94 &26.63 $\pm$ 1.61 &  63.47 $\pm$ 1.73   & 54.63 $\pm$ 3.25  &71.44 $\pm$ 2.42 & 62.86 $\pm$ 1.94\\

     &PC Softmax & 59.62 $\pm$ 1.41 & 58.77 $\pm$ 1.95 & 43.38 $\pm$ 2.01 &37.76 $\pm$ 2.12 &  70.81 $\pm$ 1.41   & 70.25 $\pm$ 1.30 &71.16 $\pm$ 1.15 & 62.26 $\pm$ 0.87\\

     &BalancedSoftmax & 62.05 $\pm$ 1.62 & 61.14 $\pm$ 1.71 & 47.89 $\pm$ 1.25 &44.84 $\pm$ 1.35 & 69.91 $\pm$ 1.68   & 67.43 $\pm$ 1.73 &72.91 $\pm$ 1.93 & 62.79 $\pm$ 0.98\\

     &Renode & 59.52 $\pm$ 2.28 & 57.16 $\pm$ 2.47& 37.21 $\pm$ 2.01 &27.09 $\pm$ 3.17 & 64.56 $\pm$ 1.65   & 55.87 $\pm$ 2.83 &69.34 $\pm$ 2.35 & 59.02 $\pm$ 1.67\\
     
     &GraphENS & 64.52 $\pm$ 2.05 & 62.52 $\pm$ 1.84 & 43.74 $\pm$ 3.81 &37.47 $\pm$ 4.21 & 69.00 $\pm$ 2.67   & 65.54 $\pm$ 3.54 & 71.78 $\pm$ 2.30 & 61.83 $\pm$ 1.75\\
     
     &BalancedSoftmax+TAM & 63.30 $\pm$ 0.99 & 62.81 $\pm$ 1.18 & \underline{49.34 $\pm$ 1.29} &\underline{46.92 $\pm$ 1.39} & \underline{71.17 $\pm$ 2.09}   & \underline{68.85 $\pm$ 2.90} &65.59 $\pm$ 2.86 & 58.12 $\pm$ 1.22\\
     
     &Renode+TAM & 61.32 $\pm$ 2.18 & 59.19 $\pm$ 2.64 & 39.85 $\pm$ 2.20 & 30.63 $\pm$ 2.63 & 66.28 $\pm$ 3.24   & 58.99 $\pm$ 3.04 &65.81 $\pm$ 2.57 & 56.73 $\pm$ 1.62\\

     &GraphENS+TAM & \underline{65.78 $\pm$ 1.62} & \underline{63.80 $\pm$ 1.79} & 44.81 $\pm$ 2.66 &39.47 $\pm$ 3.54 & 70.33 $\pm$ 2.33   & 67.00 $\pm$ 3.25 &\underline{73.55 $\pm$ 2.04} & \underline{64.03 $\pm$ 1.32}\\

     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{79.10 $\pm$ 0.71} & \textbf{76.21 $\pm$ 0.58} & \textbf{55.11 $\pm$ 5.00}  & \textbf{53.67 $\pm$ 5.51} & \textbf{72.54 $\pm$ 1.52}   & \textbf{70.54 $\pm$ 1.91} &\textbf{83.19 $\pm$ 0.66} & \textbf{74.39 $\pm$ 0.89}\\

     
     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+13.22}} & \multicolumn{1}{c}{\textbf{+12.41}} & \multicolumn{1}{c}{\textbf{+6.75}} & \multicolumn{1}{c}{\textbf{+8.81}} & \multicolumn{1}{c}{\textbf{+1.37}} & \multicolumn{1}{c}{\textbf{+1.69}} &\multicolumn{1}{c}{\textbf{+9.64}} & \multicolumn{1}{c}{\textbf{+10.36}}\\

     
     
     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 54.61 $\pm$ 1.21 & 50.95 $\pm$ 1.90 & 37.36 $\pm$ 1.03 &27.49 $\pm$ 1.41 &  62.04 $\pm$ 1.34  & 54.18 $\pm$ 1.73 &62.70 $\pm$ 2.87 & 55.39 $\pm$ 2.69\\

     &Re-Weight & 57.37 $\pm$ 0.61 & 55.30 $\pm$ 0.72 & 37.69 $\pm$ 1.20  &27.92 $\pm$ 2.01 &  65.01  $\pm$ 2.69   &58.34  $\pm$ 2.19 &68.31 $\pm$ 2.06 & 60.45 $\pm$ 2.40 \\

     &PC Softmax & 59.25 $\pm$ 0.74 & 58.55 $\pm$ 0.81 & 42.77 $\pm$ 1.82 &40.08 $\pm$ 1.82 &  70.55 $\pm$ 1.19  & 67.60 $\pm$ 1.59 &70.57 $\pm$ 2.86 & 62.73 $\pm$ 2.69\\

     &BalancedSoftmax& 61.93 $\pm$ 1.26 & 60.89 $\pm$ 1.36 & 43.64 $\pm$ 1.33 &38.31 $\pm$ 1.13 &  69.89 $\pm$ 1.40  & 68.12 $\pm$ 0.78 &68.45 $\pm$ 2.92 & 62.12 $\pm$ 3.10\\

     &Renode & 58.48 $\pm$ 0.97 & 55.39 $\pm$ 0.94& 40.65 $\pm$ 2.36 &31.78 $\pm$ 3.24 & 66.50 $\pm$ 2.63   & 58.72 $\pm$ 4.16 &68.36 $\pm$ 1.54 & 61.60 $\pm$ 2.00\\
     
     &GraphENS & 63.54 $\pm$ 0.91 & 62.20 $\pm$ 0.87 & 44.89 $\pm$ 2.51 &40.48 $\pm$ 2.94 & \underline{71.37 $\pm$ 1.77}   & 69.37 $\pm$ 1.69 &75.47 $\pm$ 2.20 & 67.49 $\pm$ 1.65\\ 

     &BalancedSoftmax+TAM & \underline{64.16 $\pm$ 0.94} & \underline{63.63 $\pm$ 1.10} & 44.32 $\pm$ 2.36 &40.17 $\pm$ 2.06 & 70.06 $\pm$ 1.46   & \underline{69.54 $\pm$ 1.35} &66.10 $\pm$ 2.37 & 59.22 $\pm$ 2.48\\
     
     &Renode+TAM & 59.77 $\pm$ 2.20 & 57.98 $\pm$ 2.79 & 42.50 $\pm$ 0.93&35.11 $\pm$ 1.84 & 67.31 $\pm$ 2.73   & 60.63 $\pm$ 3.49 &66.42 $\pm$ 2.32 & 58.62 $\pm$ 1.95\\

     &GraphENS+TAM & 63.39 $\pm$ 1.36 & 61.66 $\pm$ 1.53 & \underline{45.92 $\pm$ 1.96} &\underline{41.97 $\pm$ 2.50} & 69.62 $\pm$ 2.57   & 66.85 $\pm$ 3.00 &\underline{75.75 $\pm$ 2.30} & \underline{68.86 $\pm$ 1.29}\\
     
          \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{73.10 $\pm$ 1.60} & \textbf{69.92 $\pm$ 1.43} & \textbf{58.35 $\pm$ 4.58}  & \textbf{57.51 $\pm$ 4.92} & \textbf{73.67 $\pm$ 0.58}   & \textbf{71.15 $\pm$ 0.67} &\textbf{78.88 $\pm$ 2.16} & \textbf{69.00 $\pm$ 1.42}\\
          \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+8.94}} & \multicolumn{1}{c}{\textbf{+5.69}} & \multicolumn{1}{c}{\textbf{+12.43}} & \multicolumn{1}{c}{\textbf{+15.54}} & \multicolumn{1}{c}{\textbf{+2.30}} & \multicolumn{1}{c}{\textbf{+1.61}} &\multicolumn{1}{c}{\textbf{+3.13}} & \multicolumn{1}{c}{\textbf{+0.14}}  \\
     
     \bottomrule[1.5 pt]
     
  \end{tabular}}

\end{table}
\paragraph{Analysis.} In Table \ref{ratio10gcn}, When $\rho$ is 10, UNREAL outperforms other baselines by a wide margin, especially for the datasets Cora and CiteSeer. For the datasets PubMed and Amazon-Computers, UNREAL can still achieve cutting-edge results in the majority of cases. However, the improvement is not substantial. This, we believe, is due to the following factors: (1) For PubMed, because it only has three types of nodes, there is only one minor category in our settings, which means that the imbalanced scenario is not typical. So in such a mild imbalance setting, each method can still achieve good results. Nonetheless, UNREAL can still achieve the most advanced performance. (2) For Amazon-Computers, based on prior experience, we discovered that various models can achieve a good classification effect on this dataset, even if the label setting is sparse, which is related to the nature of the dataset itself. 

In Table \ref{ratio20}, Table \ref{ratio50}, and Table \ref{ratio100}, the results demonstrate several intriguing aspects: (1) The performance gap between other methods and UNREAL is growing, which means that current methods perform poorly in heavily imbalanced scenarios, whereas UNREAL can still achieve stable performance. We believe this is because high-quality unlabeled nodes are compensated into the training set. (2) Existing oversampling methods, such as GraphENS \citep{park2021graphens}, which achieves state-of-the-art experimental performance in a slightly imbalanced setting, perform extremely poorly in heavily imbalanced scenarios. We contend that the paradigm of synthesizing virtual nodes and local topologies always introduces a significant amount of redundancy into model training. (3) It is noteworthy that BalanceSoftmax \citep{ren2020balanced} achieves superior results in highly imbalanced scenes. BalancedSoftmax avoids estimation bias caused by label distribution migration in general.
\begin{table}[!h]
    \centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Experimental results of our method UNREAL and other baselines on four class-imbalanced node classification benchmark datasets with ${\rho=50}$. We report averaged balanced accuracy (bAcc.,$\%$) and F1-score ($\%$) with the standard errors over 5 repetitions on three representative GNN architectures.}
    \label{ratio50}
 \scalebox{0.73}{\begin{tabular}{lllllllllll}
     \toprule[1.5 pt]
     \multirow{40}{*}{\rotatebox{90}{SAGE \qquad\qquad\quad\qquad\qquad\quad \qquad\quad GAT \quad\qquad\qquad\quad\qquad\qquad\quad\quad\quad GCN}} & \multicolumn{2}{c}{Cora} & \multicolumn{2}{c}{CiteSeer} &  \multicolumn{2}{c}{PubMed} & \multicolumn{2}{c}{Amazon-Computers} &\\

     \cmidrule(lr){2-10}
     
     & \bf{Imbalance Ratio $({\rho=50})$} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}  \\

     \cmidrule(lr){2-10}
     
     & Vanilla & 51.81 $\pm$ 0.62 & 43.98 $\pm$ 1.00 & 37.59 $\pm$ 0.17 &23.54 $\pm$ 0.13 & 61.65 $\pm$ 0.34 & 47.95 $\pm$  0.58 &77.36 $\pm$ 3.41 & 69.68 $\pm$ 3.12\\

     & Re-Weight & 58.54 $\pm$ 2.39 & 54.13 $\pm$ 3.20 & 38.19 $\pm$ 1.28 &27.43 $\pm$ 2.34 & 65.70 $\pm$ 1.59 & 56.35 $\pm$  4.26 &79.10 $\pm$ 2.44 & 71.40 $\pm$ 2.86\\
     
     & PC Softmax & 64.87 $\pm$ 2.23 & 62.01 $\pm$ 3.14 & 42.42 $\pm$ 2.19 &38.83 $\pm$ 2.70 & 69.21 $\pm$ 0.59 & 69.40 $\pm$  0.87 &81.90 $\pm$ 1.63 & 74.34 $\pm$ 2.13\\

     & BalancedSoftmax & 65.94 $\pm$ 1.55 & 64.00 $\pm$ 2.05 & 47.62 $\pm$ 1.11 &46.55 $\pm$ 1.46 & 70.40 $\pm$ 1.00 & 69.04 $\pm$  0.66 &\underline{82.97 $\pm$ 0.83} & 73.74 $\pm$ 1.27 \\

     
     & Renode & 62.22 $\pm$ 1.76   & 61.18 $\pm$ 2.24 & 41.23 $\pm$ 1.66 &33.66 $\pm$ 2.69 & 68.67 $\pm$ 1.21  & 63.05 $\pm$ 1.47 &81.71 $\pm$ 0.99 & 72.55 $\pm$ 1.61\\


     & GraphENS & 63.47 $\pm$ 0.98 & 62.21 $\pm$ 1.65 & 48.17 $\pm$ 1.58 &41.07 $\pm$ 2.34 & 69.63 $\pm$ 2.55 & 64.30 $\pm$ 3.51&81.63 $\pm$ 2.35 & 72.57 $\pm$ 2.33 \\

     & BalancedSoftmax+TAM & \underline{68.57 $\pm$ 1.58}  & \underline{67.25 $\pm$ 1.27}  & \underline{53.43 $\pm$ 2.42}  & \underline{51.74 $\pm$ 2.80} & \underline{77.20 $\pm$ 1.45}  & \underline{74.86 $\pm$ 0.99}  &81.74 $\pm$ 2.30 & \underline{73.85 $\pm$ 2.68}\\

     & Renode+TAM & 63.93 $\pm$ 1.96 &61.64 $\pm$ 2.71  & 48.17 $\pm$ 1.58 & 41.07 $\pm$ 2.34 & 69.63 $\pm$ 2.55   & 64.30 $\pm$ 3.51  &80.55 $\pm$ 1.75 & 72.33 $\pm$ 1.63\\

     & GraphENS+TAM & 65.05 $\pm$ 1.11 & 62.11 $\pm$ 1.98 & 45.03 $\pm$ 1.34  & 42.65 $\pm$ 1.94  &  69.74 $\pm$ 0.78  & 70.82 $\pm$ 0.63 &81.69 $\pm$ 2.22 & 72.09 $\pm$ 1.75\\
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{75.62 $\pm$ 2.02} & \textbf{72.59 $\pm$ 2.13} & \textbf{59.97 $\pm$ 4.59}  & \textbf{58.66 $\pm$ 5.20}  &  \textbf{78.55 $\pm$ 0.84}   & \textbf{75.91 $\pm$ 0.81} &\textbf{85.54 $\pm$ 0.26} & \textbf{75.76 $\pm$ 0.13}\\
     
     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+7.05}} & \multicolumn{1}{c}{\textbf{+5.34}} & \multicolumn{1}{c}{\textbf{+6.54}} & \multicolumn{1}{c}{\textbf{+6.92}} & \multicolumn{1}{c}{\textbf{+1.35}} & \multicolumn{1}{c}{\textbf{+1.06}} &\multicolumn{1}{c}{\textbf{+2.57}} & \multicolumn{1}{c}{\textbf{+1.91}}\\
     
     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 53.90 $\pm$ 0.63 & 45.53 $\pm$ 0.89 & 36.48 $\pm$ 0.08 &23.68 $\pm$ 0.16 &  60.16 $\pm$ 0.47  & 46.99 $\pm$ 0.58 &72.42 $\pm$ 2.17 & 64.41 $\pm$ 2.68 \\

     &Re-Weight & 59.78 $\pm$ 1.92 & 56.69 $\pm$ 2.21 & 38.70 $\pm$ 2.23 &29.38 $\pm$ 3.06 &  66.27 $\pm$ 0.68   & 57.34 $\pm$ 1.41 &73.46 $\pm$ 3.07 & 67.00 $\pm$ 2.60\\

     &PC Softmax & 59.44 $\pm$ 2.62 & 58.06 $\pm$ 2.69 & 43.13 $\pm$ 1.56 &37.04 $\pm$ 2.07 &  70.86 $\pm$ 0.44   & 70.96 $\pm$ 0.54 &77.21 $\pm$ 2.90 & 69.17 $\pm$ 2.89\\

     &BalancedSoftmax & 64.71 $\pm$ 2.28 & 62.55 $\pm$ 2.61 & 51.89 $\pm$ 1.15 &49.36 $\pm$ 1.52 & 70.94 $\pm$ 1.09   & 70.33 $\pm$ 0.99 &77.49 $\pm$ 1.58 & 70.44 $\pm$ 2.33\\

     &Renode & 63.81 $\pm$ 1.72  &  60.63 $\pm$ 2.26  & 41.60 $\pm$ 2.30   & 33.94 $\pm$ 4.60  & 70.35 $\pm$ 1.26 & 67.43 $\pm$ 0.01&72.39 $\pm$ 2.75 & 65.23 $\pm$ 3.35\\
     
     &GraphENS & 64.52 $\pm$ 2.51 & 61.41 $\pm$ 3.15 & 45.23 $\pm$ 2.97 &41.12 $\pm$ 4.23 & 69.66 $\pm$ 1.01   &66.83  $\pm$ 0.94 &78.36 $\pm$ 2.74 & 70.44 $\pm$ 2.51\\
     
     &BalancedSoftmax+TAM & \underline{68.05 $\pm$ 1.03}  & \underline{66.07 $\pm$ 1.14} & \underline{54.28 $\pm$ 0.79} & \underline{52.77 $\pm$ 0.97} & \underline{75.65 $\pm$ 1.11}  & \underline{74.02 $\pm$ 1.44}&78.86 $\pm$ 1.53 & 70.71 $\pm$ 2.04 \\
     
     &Renode+TAM & 64.40 $\pm$ 1.83 &63.48 $\pm$ 2.83  & 43.54 $\pm$ 1.54 & 35.80 $\pm$ 2.43 & 71.23 $\pm$ 2.04   & 66.61 $\pm$ 4.31  & 76.07 $\pm$ 2.70 & 68.43 $\pm$ 2.68\\

     &GraphENS+TAM &65.33 $\pm$ 2.67&65.34 $\pm$ 2.53  & 48.00 $\pm$ 1.46  & 48.14 $\pm$ 1.43 & 71.50 $\pm$ 1.26   & 72.58 $\pm$ 1.07&\underline{80.02 $\pm$ 2.32} & \underline{72.38 $\pm$ 2.47}\\
     
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} &\textbf{77.07 $\pm$ 0.83}&\textbf{73.44 $\pm$ 1.05}  & \textbf{57.70 $\pm$ 4.35 }  & \textbf{56.81 $\pm$ 4.67} & \textbf{79.41 $\pm$ 0.29}   & \textbf{77.38 $\pm$ 0.39}& \textbf{86.06 $\pm$ 0.45} & \textbf{77.55 $\pm$ 0.71}\\

     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+9.02}} & \multicolumn{1}{c}{\textbf{+7.37}} & \multicolumn{1}{c}{\textbf{+3.42}} & \multicolumn{1}{c}{\textbf{+4.04}} & \multicolumn{1}{c}{\textbf{+3.76}} & \multicolumn{1}{c}{\textbf{+3.36}} &\multicolumn{1}{c}{\textbf{+6.04}} & \multicolumn{1}{c}{\textbf{+5.17}}\\

     
     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 53.02 $\pm$ 0.83 & 45.58 $\pm$ 1.30 & 38.81 $\pm$ 0.89 &25.28 $\pm$ 0.51 &  61.41 $\pm$ 1.01  & 50.46 $\pm$ 2.47&56.53 $\pm$ 2.12 & 48.52 $\pm$ 2.75 \\

     &Re-Weight & 58.03 $\pm$ 0.81 & 54.32 $\pm$ 0.99 & 38.49 $\pm$ 1.34  &30.41 $\pm$ 1.82 &  62.41  $\pm$ 0.90   &51.37  $\pm$ 2.62 &70.36 $\pm$ 2.21 & 61.52 $\pm$ 2.73\\

     &PC Softmax & 62.33 $\pm$ 1.62 & 59.97 $\pm$ 1.98 & 41.79 $\pm$ 1.19 &36.90 $\pm$ 0.84 &  69.58 $\pm$ 1.09  & 67.13 $\pm$ 0.95&73.53 $\pm$ 2.02 & 66.12 $\pm$ 3.19 \\

     &BalancedSoftmax& 64.57 $\pm$ 0.77 & 62.22 $\pm$ 0.82 & 41.84 $\pm$ 1.72 &40.09 $\pm$ 1.04 &  70.43 $\pm$ 0.38  & 68.99 $\pm$ 0.99& 73.27 $\pm$ 2.30 & 68.30 $\pm$ 1.97\\

     &Renode &61.35 $\pm$ 1.86 & 58.88 $\pm$ 2.53 & 40.37 $\pm$ 2.33& 32.57 $\pm$ 3.62 & 67.54 $\pm$ 3.05   & 59.77 $\pm$ 5.30 &70.46 $\pm$ 3.45 & 62.30 $\pm$ 4.40\\
     
     &GraphENS& 63.95 $\pm$ 0.96 & 62.63 $\pm$ 2.12 & 41.99 $\pm$ 1.54 &37.44 $\pm$ 2.43 & 66.07 $\pm$ 1.12   & 61.63 $\pm$ 1.82 &76.21 $\pm$ 2.84 & 68.10 $\pm$ 2.56\\

     &BalancedSoftmax+TAM & 65.97 $\pm$ 0.71 &\underline{65.53 $\pm$ 0.88} & \underline{52.89 $\pm$ 1.65} &\underline{49.92 $\pm$ 1.83} &   71.11 $\pm$ 0.75 & 71.73 $\pm$ 0.79 &73.12 $\pm$ 1.41 & 66.45 $\pm$ 1.04\\
     
     &Renode+TAM & 62.79 $\pm$ 0.47 &61.05 $\pm$ 0.82  & 43.04 $\pm$ 1.30 & 36.97 $\pm$ 1.92 & 71.79 $\pm$ 1.33   & 67.80 $\pm$ 2.45  & 74.55 $\pm$ 2.95 & 66.06 $\pm$ 2.16\\

     &GraphENS+TAM & \underline{65.98 $\pm$ 1.37} & 64.84 $\pm$ 1.13 & 49.54 $\pm$ 1.79 &49.48 $\pm$ 1.70 & \underline{73.24 $\pm$ 1.32}   & \underline{73.73 $\pm$ 1.14} &\underline{80.75 $\pm$ 1.22} & \underline{72.31 $\pm$ 0.95}\\
     
          \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{76.04 $\pm$ 1.30} & \textbf{72.99 $\pm$ 1.25} & \textbf{58.70 $\pm$ 4.10} &\textbf{57.53 $\pm$ 4.59} & \textbf{75.27 $\pm$ 1.26}   & \textbf{72.16 $\pm$ 1.50}& \textbf{82.03 $\pm$ 0.77} & \textbf{72.98 $\pm$ 0.52}\\
          \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+10.06}} & \multicolumn{1}{c}{\textbf{+7.46}} & \multicolumn{1}{c}{\textbf{+5.81}} & \multicolumn{1}{c}{\textbf{+7.61}} & \multicolumn{1}{c}{\textbf{+2.03}} & \multicolumn{1}{c}{\textbf{-1.57}} &\multicolumn{1}{c}{\textbf{+1.28}} & \multicolumn{1}{c}{\textbf{+0.67}}\\
     \bottomrule[1.5 pt]
  \end{tabular}}
\end{table}
% 

%\input{Table/ratio10gatsage.tex}
% \newpage
% \vspace*{\fill}
%     \begin{center}
%     \input{Table/ratio20.tex}
%     \end{center}
% \vspace*{\fill}
% \newpage
% \vspace*{\fill}
%     \begin{center}
%     \input{Table/ratio50.tex}
%     \end{center}
% \vspace*{\fill}
\newpage
\vspace*{\fill}
    \begin{center}
\begin{table}[!h]
    \centering
        \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Experimental results of our method UNREAL and other baselines on four class-imbalanced node classification benchmark datasets with ${\rho=100}$. We report averaged balanced accuracy (bAcc.,$\%$) and F1-score ($\%$) with the standard errors over 5 repetitions on three representative GNN architectures.}
    \label{ratio100}
 \scalebox{0.73}{\begin{tabular}{lllllllllll}
     \toprule[1.5 pt]
     \multirow{40}{*}{\rotatebox{90}{SAGE \qquad\qquad\quad\qquad\qquad\quad \qquad\quad GAT \quad\qquad\qquad\quad\qquad\qquad\quad\quad\quad GCN}} & \multicolumn{2}{c}{Cora} & \multicolumn{2}{c}{CiteSeer} &  \multicolumn{2}{c}{PubMed} & \multicolumn{2}{c}{Amazon-Computers} &\\

     \cmidrule(lr){2-10}
     
     & \bf{Imbalance Ratio $({\rho=100})$} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}  \\

     \cmidrule(lr){2-10}
     
     & Vanilla & 51.62 $\pm$ 0.20 & 43.91 $\pm$ 0.25 & 38.83 $\pm$ 0.26 &24.71 $\pm$ 0.25 & 61.28 $\pm$ 0.12 & 47.55 $\pm$  0.16 &76.09 $\pm$ 3.79 & 69.32 $\pm$ 3.49\\

     & Re-Weight & 59.11 $\pm$ 1.06 & 54.04 $\pm$ 1.36 & 42.67 $\pm$ 2.06 &33.17 $\pm$ 3.40 & 67.14 $\pm$ 2.71 & 55.24 $\pm$  5.36 &81.53 $\pm$ 2.20 & 71.45 $\pm$ 2.05 \\
     
     & PC Softmax & 63.75 $\pm$ 1.02 & 61.19 $\pm$ 1.43 & 38.34 $\pm$ 0.71 &33.65 $\pm$ 1.42 & 70.85 $\pm$ 0.44 & 70.26 $\pm$  0.63 & 82.22 $\pm$ 1.99 & 72.38 $\pm$ 2.52\\

     & BalancedSoftmax & 63.03 $\pm$ 1.57 & 61.28 $\pm$ 1.77 & 48.49 $\pm$ 1.20 &46.59 $\pm$ 1.34 & 70.77 $\pm$ 1.88 & 68.88 $\pm$  1.74  &83.33 $\pm$ 3.35 & 74.34 $\pm$ 2.74\\

     
     & Renode & 60.76 $\pm$ 2.53   & 58.09 $\pm$ 3.00 & 43.41 $\pm$ 2.07 & 33.69 $\pm$ 2.76 & 67.63 $\pm$ 2.77   & 61.70 $\pm$  4.84  &82.13 $\pm$ 1.73 & 71.79 $\pm$ 1.85\\


     & GraphENS & 63.00 $\pm$ 1.30 & 62.33 $\pm$ 1.67 & 45.99 $\pm$ 2.06 &37.23 $\pm$ 3.40 & 68.65 $\pm$ 1.00 & 62.17 $\pm$ 1.60  &83.37 $\pm$ 2.17 & 73.96 $\pm$ 1.98\\

     & BalancedSoftmax+TAM & \underline{69.44 $\pm$ 0.59}  & \underline{67.10 $\pm$ 0.88} & \underline{52.60 $\pm$ 0.69}  & \underline{51.21 $\pm$ 0.84} & \underline{73.73 $\pm$ 1.10}  & \underline{73.72 $\pm$ 0.83}  & \underline{83.70 $\pm$ 2.17} & \underline{75.39 $\pm$ 1.43}\\

     & Renode+TAM & 64.19 $\pm$ 1.46  &60.90  $\pm$  1.56 & 44.78 $\pm$ 1.51  & 35.90 $\pm$ 2.61 & 70.53 $\pm$ 0.75  & 64.35 $\pm$ 1.79  &82.32 $\pm$ 2.19 & 73.09 $\pm$ 1.75 \\

     & GraphENS+TAM &  60.40 $\pm$ 4.42 &  57.77 $\pm$ 4.02 & 42.72 $\pm$ 2.54  &  39.40 $\pm$ 2.57   &  70.73 $\pm$ 1.96  & 72.50 $\pm$ 1.87 & 81.29 $\pm$ 1.52 & 71.66 $\pm$ 1.75 \\
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} &  \textbf{72.82 $\pm$ 3.55} &  \textbf{69.12 $\pm$ 3.45} & \textbf{57.66 $\pm$ 1.96}  &  \textbf{56.50 $\pm$ 1.12}  &  \textbf{78.73 $\pm$ 0.88}  & \textbf{76.03 $\pm$ 1.08} &\textbf{84.30 $\pm$ 0.30} & \textbf{76.06 $\pm$ 0.32}\\

     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+3.38}} & \multicolumn{1}{c}{\textbf{+2.02}} & \multicolumn{1}{c}{\textbf{+5.06}} & \multicolumn{1}{c}{\textbf{+5.29}} & \multicolumn{1}{c}{\textbf{+5.00}} & \multicolumn{1}{c}{\textbf{+2.31}}&\multicolumn{1}{c}{\textbf{+0.60}} & \multicolumn{1}{c}{\textbf{+0.67}}  \\

     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 51.58 $\pm$ 0.32 & 43.37 $\pm$ 0.21 & 37.91 $\pm$ 0.28 &23.49 $\pm$ 0.21 &  62.07 $\pm$ 0.17  & 47.39 $\pm$ 0.20 &72.66 $\pm$ 2.97 & 64.87 $\pm$ 3.46\\

     &Re-Weight & 58.28 $\pm$ 1.88 & 54.47 $\pm$ 2.35 & 38.13 $\pm$ 1.55 &29.60 $\pm$ 3.02 &  67.41 $\pm$ 2.69   & 58.06 $\pm$ 5.07&77.10 $\pm$ 3.26 & 68.35 $\pm$ 2.71 \\

     &PC Softmax & 63.74 $\pm$ 2.01 & 59.76 $\pm$ 2.19 & 45.07 $\pm$ 1.13 &39.21 $\pm$ 2.29 &  69.68 $\pm$ 1.29   & 69.44 $\pm$ 1.29& 79.72 $\pm$ 1.52 & 70.78 $\pm$ 1.45\\

     &BalancedSoftmax & 63.19 $\pm$ 1.35 & 61.03 $\pm$ 1.46 & 46.03 $\pm$ 2.11 &43.38 $\pm$ 2.24 & 71.45 $\pm$ 1.23   & 69.10 $\pm$ 1.20 &79.15 $\pm$ 2.08 & 69.68 $\pm$ 2.13\\

     &Renode &  60.04 $\pm$ 2.21 & 58.04 $\pm$ 2.66 & 42.40 $\pm$ 2.97  & 34.09 $\pm$ 0.04 & 68.54 $\pm$ 2.11   & 65.63 $\pm$ 3.15&75.34 $\pm$ 1.65 & 69.99 $\pm$ 1.60 \\
     
     &GraphENS & 63.93 $\pm$ 2.70 & 61.77 $\pm$ 3.38 & 44.43 $\pm$ 1.90 &39.26 $\pm$ 2.55 & 68.50 $\pm$ 1.81   &64.14  $\pm$ 3.28 &81.63 $\pm$ 2.08 & 71.20 $\pm$ 2.75\\
     
     &BalancedSoftmax+TAM & \underline{64.96 $\pm$ 3.23}  & \underline{62.91 $\pm$ 3.96} & \underline{52.75 $\pm$ 1.29} & \underline{50.69 $\pm$ 1.83} & \underline{73.38 $\pm$ 0.77}  & \underline{72.45 $\pm$ 0.88}&80.86 $\pm$ 2.52 & 72.93 $\pm$ 2.95 \\
     
     &Renode+TAM & 63.45 $\pm$ 1.41& 61.51 $\pm$ 1.95  & 41.55 $\pm$ 1.39 & 36.13 $\pm$ 2.87  & 71.53 $\pm$ 2.35   & 68.11 $\pm$ 4.28&78.60 $\pm$ 1.90 & 70.35 $\pm$ 2.80\\

     &GraphENS+TAM & 62.52 $\pm$ 0.95& 61.65 $\pm$ 1.19  & 45.79 $\pm$ 1.31 & 44.80 $\pm$ 1.14  & 69.09 $\pm$ 1.11   & 70.64 $\pm$ 1.10&\underline{83.33 $\pm$ 0.83} & \underline{72.81 $\pm$ 1.22}\\
     
     
     \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} &  \textbf{75.42} $\pm$ \textbf{0.91}& \textbf{71.50  $\pm$ 0.89}  & \textbf{60.35 $\pm$ 1.87}  & \textbf{59.63 $\pm$ 1.86}  & \textbf{77.88 $\pm$ 1.31}   & \textbf{74.98 $\pm$ 1.35}&\textbf{85.33 $\pm$ 0.19} & \textbf{75.83 $\pm$ 0.74}\\
     
     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+10.46}} & \multicolumn{1}{c}{\textbf{+8.59}} & \multicolumn{1}{c}{\textbf{+7.60}} & \multicolumn{1}{c}{\textbf{+8.94}} & \multicolumn{1}{c}{\textbf{+4.50}} & \multicolumn{1}{c}{\textbf{+2.53}}&\multicolumn{1}{c}{\textbf{+2.00}} & \multicolumn{1}{c}{\textbf{+3.02}} \\
     
     \cmidrule(lr){2-10}\morecmidrules\cmidrule(lr){2-10}
     
     &Vanilla & 52.65 $\pm$ 0.24 & 43.79 $\pm$ 0.47 & 36.63 $\pm$ 0.09 &24.12 $\pm$ 0.09 &  62.29 $\pm$ 0.25  & 47.02 $\pm$ 0.38 & 55.94 $\pm$ 2.37 & 47.21 $\pm$ 2.73\\

     &Re-Weight & 59.42 $\pm$ 2.88 & 55.26 $\pm$ 4.40 & 36.24 $\pm$ 1.30  &27.07 $\pm$ 2.88 &  63.33  $\pm$ 0.75   &55.11  $\pm$ 1.62 &70.76 $\pm$ 3.35 & 62.09 $\pm$ 3.30\\

     &PC Softmax & 64.01 $\pm$ 1.15 & 60.74 $\pm$ 1.68 & 44.74 $\pm$ 1.41 &37.61 $\pm$ 1.69 &  72.62 $\pm$ 1.42  & 70.95 $\pm$ 1.70&75.96 $\pm$ 2.44 & 69.12 $\pm$ 2.90 \\

     &BalancedSoftmax& 63.43 $\pm$ 2.12 & 62.30 $\pm$ 2.27 & 49.33 $\pm$ 1.12 &44.58 $\pm$ 1.64 &  70.68 $\pm$ 0.92  & 69.15 $\pm$ 0.84 &74.66 $\pm$ 0.86 & 66.28 $\pm$ 1.92\\

     &Renode & 62.42 $\pm$ 0.90 & 60.08 $\pm$ 1.19 & 39.61 $\pm$ 2.66 & 30.13 $\pm$ 3.86 &  67.11 $\pm$ 1.12 & 61.09 $\pm$ 3.50 &73.73 $\pm$ 2.26 & 64.47 $\pm$ 2.39\\
     
     &GraphENS& 63.09 $\pm$ 0.97 & 61.20 $\pm$ 1.74 & 42.03 $\pm$ 1.88 &36.71 $\pm$ 2.99 & 69.71 $\pm$ 1.87   & 63.47 $\pm$ 3.87&81.33 $\pm$ 1.66 & \underline{72.83 $\pm$ 1.76} \\

     &BalancedSoftmax+TAM & \underline{66.58 $\pm$ 1.53} & \underline{64.56 $\pm$ 2.49} &\underline{53.33 $\pm$ 1.06} &50.15 $\pm$ 1.45 &   72.59 $\pm$ 2.06 & 72.22 $\pm$ 2.08 &78.01 $\pm$ 1.06 & 71.02 $\pm$ 1.08\\
     
     &Renode+TAM & 62.06 $\pm$ 2.08 & 60.72 $\pm$ 3.32 & 42.08 $\pm$ 1.88 &33.19 $\pm$ 3.45 & 69.95 $\pm$ 1.01   & 65.99 $\pm$ 2.28 &74.81 $\pm$ 3.29 & 67.48  $\pm$ 3.32 \\

     &GraphENS+TAM & 65.95 $\pm$ 2.25 & 63.88 $\pm$ 1.78 & 51.03 $\pm$ 1.51 &\underline{50.49 $\pm$ 1.88} & \underline{73.58 $\pm$ 2.01}   & \underline{72.44 $\pm$ 1.77}&\underline{81.72 $\pm$ 1.08} & 72.31 $\pm$ 1.98 \\
     
          \cmidrule(lr){2-10}
     
     & \textbf{UNREAL} & \textbf{73.47 $\pm$ 2.31} & \textbf{68.30 $\pm$ 2.11} & \textbf{59.77 $\pm$ 2.98} &\textbf{58.92 $\pm$ 3.07} & \textbf{77.11 $\pm$ 0.59}   & \textbf{74.03 $\pm$ 0.81} &\textbf{82.92 $\pm$ 2.94} & \textbf{73.11 $\pm$ 2.57}\\
     
     \cmidrule(lr){2-10}
     
     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+6.89}} & \multicolumn{1}{c}{\textbf{+3.74}} & \multicolumn{1}{c}{\textbf{+6.44}} & \multicolumn{1}{c}{\textbf{+8.43}} & \multicolumn{1}{c}{\textbf{+3.53}} & \multicolumn{1}{c}{\textbf{+1.59}} &\multicolumn{1}{c}{\textbf{+1.20}} & \multicolumn{1}{c}{\textbf{+0.28}}\\
     

     \bottomrule[1.5 pt]
  \end{tabular}}
\end{table}
    \end{center}
\vspace*{\fill}



\newpage
\subsection{More Results When Unlabeled Data is Imbalanced}
In Table \ref{flickr},  we found that existing over-sampling methods use too much memory due to synthetic node generation, and cannot handle Flickr on a 3090 GPU with 24GB memory. This include GraphENS \citep{park2021graphens}, GraphSMOTE \citep{zhao2021graphsmote} and ReNode \citep{chen2021topology}. More importantly, on Flickr, UNREAL consistently outperforms other approaches.
\begin{table*}[!h]
\centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
\caption{Experimental results of our method UNREAL and other baselines on Flickr. We report averaged balanced accuracy (bAcc.,$\%$) and F1-score ($\%$) with the standard errors over 5 repetitions on three representative GNN architectures.}

 \scalebox{0.89}{\begin{tabular}{lllllllll}
     \toprule[1.5pt]
     \multirow{43}{*}{} & \multicolumn{1}{c}{\bf{Dataset (Flickr)}} & \multicolumn{2}{c}{GCN}  & \multicolumn{2}{c}{GAT}   &\multicolumn{2}{c}{SAGE}  &\\

     \cmidrule(lr){2-8}
     
     & \bf{Imbalance Ratio$\bf ({\rho\approx10.80})$} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}& \multicolumn{1}{c}{bAcc.} & \multicolumn{1}{c}{F1}\\

     \cmidrule(lr){2-8}
     
     & Vanilla & 24.62 $\pm$ 0.07 & 24.53 $\pm$ 0.11 &25.87 $\pm$0.30 & 25.32 $\pm$ 0.44 & 25.29 $\pm$ 0.18 & 24.16 $\pm$ 0.27\\

     & Re-Weight & 28.31 $\pm$ 1.64 & 24.06 $\pm$ 1.16 & \textbf{30.66 $\pm$ 0.76} & 27.12 $\pm$ 0.34 & 27.39 $\pm$ 1.84 & 22.62 $\pm$ 1.04   \\
     
     & PC Softmax &\underline{29.21  $\pm$ 2.16}  &\underline{25.81  $\pm$ 1.75} &30.20 $\pm$ 0.46 & \underline{27.24 $\pm$ 0.37} & 25.40 $\pm$ 2.49 & 21.08 $\pm$ 1.73  \\

     & BalancedSoftmax & 27.61 $\pm$ 0.61& 23.70 $\pm$ 0.77 &26.01 $\pm$ 2.81 & 23.50 $\pm$ 3.07 & 28.24 $\pm$ 2.10 & 24.98 $\pm$ 1.59\\

     & GraphSMOTE & \multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}  & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM} \\

     & Renode &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}  & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM} \\

     & GraphENS &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}  & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM}  & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM}\\

     & BalancedSoftmax+TAM & 27.06 $\pm$ 1.03 & 23.97 $\pm$ 0.60 &28.24 $\pm$ 0.99 & 25.52 $\pm$ 0.89  &\underline{29.79 $\pm$ 0.37} & \underline{27.56 $\pm$ 0.25}  \\

     & Renode+TAM & \multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}  & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM}\\

     & GraphENS+TAM &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}  & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM} & \multicolumn{1}{c}{OOM}   &\multicolumn{1}{c}{OOM}\\
     
     \cmidrule(lr){2-8}
     
     & \textbf{UNREAL} & \textbf{30.76 $\pm$ 0.27} & \textbf{30.60 $\pm$ 0.29}  & \underline{29.45 $\pm$ 0.72 } & \textbf{28.21 $\pm$ 0.76} & \textbf{50.68 $\pm$ 0.63}  & \textbf{51.01  $\pm$ 1.34}\\
     
     \cmidrule(lr){2-8}

     & \pmb{$\Delta$} & \multicolumn{1}{c}{\textbf{+1.55}} & \multicolumn{1}{c}{\textbf{+4.79}} & \multicolumn{1}{c}{\textbf{-1.21}} & \multicolumn{1}{c}{\textbf{+0.97}}  & \multicolumn{1}{c}{\textbf{+20.89}} & \multicolumn{1}{c}{\textbf{+23.45}} \\
      
     \bottomrule[1.5pt]
     \label{flickr}
\end{tabular}}
\end{table*}
\newpage
\section{More Analysis}
\label{More Analysis}


\subsection{More Experiments of the Motivating Example in Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering}.}
\label{More Experiments of the Motivating Example}
We conduct richer experiments on Cora and Amazon-Computers based on three different GNN architectures to verify the motivating example in Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering}. We hypothesize that even if the GNN encoder is trained on skewed data, the embeddings it learns are of high quality.

\paragraph{Details of Experimental Setup.} As explained in Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering}, we can obtain two pseudo-labels for all unlabeled nodes, one from unsupervised algorithms and the other from supervised classifiers. Experiments on more datasets are conducted to compare the accuracy of the two pseudo-labels for all unlabeled nodes. We chose the two benchmark datasets, Cora and Amazon-Computers, to build scenarios with varying degrees of imbalance ($\rho$ = 1, 5, 10, 20, 50, 100). To be more specific, half of the classes are designated as minority classes and randomly selected labeled nodes are converted into unlabeled nodes until the training set's imbalance ratio reaches $rho$. The GNN architecture is fixed as the 2-layer GNN (i.e. GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, GraphSAGE \citep{hamilton2017inductive}) having 128 hidden dimensions and train models for 2000 epochs. We set the K-Means algorithm's cluster size $k'$ to 200. Each experiment is repeated five times, and the average experiment results under different imbalance ratios are shown in Figure \ref{figure_cora_select_gcngat}.


\begin{figure*}[!ht]
\centering
% \subfigure[Cora-GAT]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[scale=0.30]{cora_gat_test.pdf}
% %\caption{fig1}
% \end{minipage}%
% }%

\subfigure[Cora-SAGE]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.30]{cora_sage_test.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[Computers-GCN]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.30]{computers_gcn_test.pdf}
%\caption{fig2}
\end{minipage}%
}%

\subfigure[Computers-GAT]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.30]{computers_gat_test.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[Computers-SAGE]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.30]{computers_sage_test.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The partial experimental results on Cora and Amazon-Computers under different imbalance scenarios ($\rho$ = 1, 5, 10, 20, 50, 100). We compare the accuracy of the two pseudo-labels (predictions) from unsupervised algorithms and supervised classifiers respectively for all unlabeled nodes.}
\label{figure_cora_select_gcngat}
\end{figure*}

\paragraph{Analysis.} As illustrated by Figure \ref{figure_cora_select_gcngat}, Figure \ref{figure_coragcn_select} and Figure \ref{figure_coragat_select}, the predictions given by unsupervised algorithms still have a high accuracy rate even in imbalanced scenarios. The final results reveal several intriguing aspects: (1) In imbalanced scenarios, the performance of both supervised and unsupervised algorithms degrades, especially in extreme cases ($\rho$ = 50,100). (2) The predictions given by the embedding space are superior to the biased classifier, which meaningfully indicates that the classifier is the more under-performed component in the model when trained on an imbalanced training set. (3) A large number of experimental results demonstrate that the predictions from unsupervised algorithms and classifiers are of reference significance, implying that relying on a single component will not result in a good performance. Motivating by this, we propose DPAM (Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering}) and Node-Reordering (Section \ref{reorder}), the two fundamental components of our algorithm.




















% DPAM conducts an unsupervised algorithm to obtain pseudo-labels for each unlabeled node in the embedding space, and finally only unlabeled nodes whose pseudo-labels and classifier's predictions are aligned are put into the candidate pool, which effectively circumvents the bias problem of the classifier, such as pseudo-label misjudgment of unlabeled nodes, selecting low-quality nodes into training set based on the skewed condidence rankings. 

\subsection{Additional Analysis for DPAM (Section \ref{Dual Pseudo-tag Alignment Mechanism for Node filtering})}
\label{Additional analysis for DPAM}



DPAM uses an unsupervised algorithm to obtain pseudo-labels for each unlabeled node in the embedding space, and only unlabeled nodes with aligned pseudo-labels and classifier predictions are put into the candidate pool, effectively circumventing the classifier's bias problem, such as selecting low-quality nodes into the training set based on the skewed confidence rankings. We conduct the novel experiments listed below to see through its essence. 

\paragraph{Details of Experimental Setup.} 
We use DPAM to filter the unlabeled nodes of the whole graph, and test the accuracy of pseudo-labels (prediction of the classifier) of the aligned node set $\mathcal{U}_{in}$ and the discarded node set $\mathcal{U}_{out}$ respectively. DPAM based on different GNN structures are trained on two node classification benchmark datasets, Cora, and Amazon-Computers. We process the two datasets with a traditional imbalanced distribution following \citet{zhao2021graphsmote, park2021graphens, song2022tam}. The imbalance ratio $\rho$ between the numbers of the most frequent class and the least frequent class is set as 1, 5, 10, 20, 50, and 100. We fix architecture as the 2-layer GNN (i.e. GCN\citep{kipf2016semi}, GAT\citep{velivckovic2017graph}, GraphSAGE\citep{hamilton2017inductive})  having 128 hidden dimensions and train models for 2000 epochs. We select the model by the validation accuracy. We observe the accuracy of pseudo labels for unlabeled nodes which are filtered out and absorbed into by DPAM respectively. We repeat each experiment five times and present the average experiment results in Table \ref{DPAM-cora} and Table \ref{DPAM-computers}.


\paragraph{Analysis.}
DPAM divides the unlabeled nodes of the whole graph into two parts, $\mathcal{U}_{in}$, $\mathcal{U}_{out}$. We verify the effect of DPAM by testing the accuracy of pseudo-labels for these two parts of nodes. We can observe that the accuracy of pseudo-labels for $\mathcal{U}_{in}$ and $\mathcal{U}_{out}$ differ greatly in different imbalanced scenarios. Usually the pseudo-label accuracy of $\mathcal{U}_{in}$ is high and the pseudo-label accuracy of $\mathcal{U}_{out}$ is lower, which means the effectiveness of DPAM. We can also observe that as $\rho$ increases, the accuracy of both decreases, which also reflects the model bias caused by the imbalanced label distribution.

\begin{table}[!h]
    \centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Experimental results of DPAM effectiveness on \textbf{Cora} with ${\rho=1, 5, 10, 20, 50, 100}$. We observe the accuracy ($\%$) of the pseudo-label (prediction of the classifier) of the aligned node set $\mathcal{U}_{in}$ and the discarded node set $\mathcal{U}_{out}$ respectively. We report averaged results  with the standard errors over 5 repetitions on three representative GNN architectures. \textbf{All}, \textbf{Labeled}, \textbf{Unlabeled} represent the size of whole nodes, labeled nodes, and unlabeled nodes on the graph. \textbf{Align}, \textbf{Out}, \textbf{Align-True}, \textbf{Out-Ture} represent the size of $\mathcal{U}_{in}$,  $\mathcal{U}_{out}$, nodes with accurate pseudo-labels of $\mathcal{U}_{in}$,  $\mathcal{U}_{out}$ respectively. }
    \label{DPAM-cora}
 \scalebox{0.78}{\begin{tabular}{cccccccccccc}
     \toprule[1.5 pt]
     \multirow{22.5}{*}{\rotatebox{90}{SAGE \qquad\qquad\qquad GAT  \qquad\qquad\qquad GCN}} & \multicolumn{1}{l}{\bf{Dataset}} & \multicolumn{1}{c}{All} & \multicolumn{1}{c}{Labled} &  \multicolumn{1}{c}{Unlabled} & \multicolumn{1}{c}{Align} & \multicolumn{1}{c}{Align-True} & \multicolumn{1}{c}{Accuracy(\%)} &  \multicolumn{1}{c}{Out} & \multicolumn{1}{c}{Out-True}& \multicolumn{1}{c}{Accuracy(\%)}\\

     % \cmidrule(lr){2-11}
     
     % & \multicolumn{1}{l}{Method} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{UNREAL} \\

     \cmidrule(lr){2-11}
     & \multicolumn{1}{l}{${\rho=1}$} & 2708     & 140       & 2568       &2072.00 $\pm$ 10.29       & 1391.00 $\pm$ 22.56      & \textbf{67.11 $\pm$  1.17}      & 496.00 $\pm$ 10.29        &233.80 $\pm$ 16.66         & \textbf{47.17 $\pm$ 3.74}\\
     & \multicolumn{1}{l}{${\rho=5}$} & 2708      & 92       & 2616       &2122.80 $\pm$ 18.93       & 1392.00 $\pm$ 34.21       & \textbf{65.58 $\pm$  1.57}      & 493.20 $\pm$ 18.73        &186.80 $\pm$ 13.08          & \textbf{37.86 $\pm$ 1.75}\\
     & \multicolumn{1}{l}{${\rho=10}$} & 2708      & 86       & 2622       &2134.60 $\pm$ 23.42       & 1326.40 $\pm$ 24.23       & \textbf{62.14 $\pm$  1.67}      & 487.40 $\pm$ 23.43        &181.60 $\pm$ 18.24         & \textbf{37.32 $\pm$ 3.13}\\
     & \multicolumn{1}{l}{${\rho=20}$} & 2708     & 83       & 2625       &2149.60 $\pm$ 17.67       & 1310.20 $\pm$ 86.72     & \textbf{60.97 $\pm$  3.50}      & 475.40 $\pm$ 17.67        &169.80 $\pm$ 21.47         & \textbf{35.64 $\pm$ 3.44}\\
     & \multicolumn{1}{l}{${\rho=50}$} & 2708      & 203       & 2505       &1860.80 $\pm$ 31.15       & 1059.40 $\pm$ 58.77      & \textbf{56.90 $\pm$  2.62}     & 644.20 $\pm$ 31.14        &225.80 $\pm$ 10.70         & \textbf{35.05 $\pm$ 3.79}\\
     & \multicolumn{1}{l}{${\rho=100}$} & 2708      & 403       & 2305      &1820.40 $\pm$ 12.42       & 1001.60 $\pm$ 21.60      & \textbf{55.02 $\pm$  3.99}      & 484.60 $\pm$ 23.99        &151.40 $\pm$ 20.74         & \textbf{31.78 $\pm$ 2.37}\\
     


          
     \cmidrule(lr){2-11}\morecmidrules\cmidrule(lr){2-11}
     & \multicolumn{1}{l}{${\rho=1}$} & 2708     & 140       & 2568       &2072.00 $\pm$ 37.18       & 1412.40 $\pm$ 37.31      & \textbf{68.16 $\pm$  1.41}      & 496.00 $\pm$ 20.89        &239.40 $\pm$ 11.37         & \textbf{48.29 $\pm$ 2.15}\\
     & \multicolumn{1}{l}{${\rho=5}$} & 2708      & 92       & 2616       &2141.40 $\pm$ 26.36       & 1433.00 $\pm$ 59.82       & \textbf{66.90 $\pm$  2.09}      & 474.60 $\pm$ 26.36        &195.20 $\pm$ 24.68          & \textbf{41.02 $\pm$ 3.27}\\
     & \multicolumn{1}{l}{${\rho=10}$} & 2708      & 86       & 2622       &2132.60 $\pm$ 29.94       & 1377.40 $\pm$ 49.61       & \textbf{64.58 $\pm$  1.60}      & 489.40 $\pm$ 29.95        &185.80 $\pm$ 12.28         & \textbf{37.97 $\pm$ 1.13}\\
     & \multicolumn{1}{l}{${\rho=20}$} & 2708     & 83       & 2625       &2150.60 $\pm$ 37.35       & 1344.60 $\pm$ 54.17     & \textbf{62.16 $\pm$  1.64}      & 462.40 $\pm$ 33.28        &178.00 $\pm$ 5.05         & \textbf{38.60 $\pm$ 2.12}\\
     & \multicolumn{1}{l}{${\rho=50}$} & 2708     & 140       & 2568       &1892.40 $\pm$ 37.18       & 1080.80 $\pm$ 31.86      & \textbf{57.52 $\pm$  1.52}      & 612.60 $\pm$ 37.17        &271.20 $\pm$ 6.30         & \textbf{44.35 $\pm$ 1.86}\\
     & \multicolumn{1}{l}{${\rho=100}$} & 2708     & 403       & 2305       &1934.60 $\pm$ 19.65       & 1038.20 $\pm$ 21.08      & \textbf{53.66 $\pm$  0.83}      & 370.40 $\pm$ 37.17        &147.53 $\pm$ 3.20         & \textbf{39.83 $\pm$ 1.36}\\



     \cmidrule(lr){2-11}\morecmidrules\cmidrule(lr){2-11}
     
     & \multicolumn{1}{l}{${\rho=1}$} & 2708     & 140       & 2568       &1944.00 $\pm$ 25.77       & 973.40 $\pm$ 32.26      & \textbf{51.27 $\pm$  3.36}      & 624.00 $\pm$ 25.77        &237.00 $\pm$ 13.28         & \textbf{36.11 $\pm$ 4.07}\\
     & \multicolumn{1}{l}{${\rho=5}$} & 2708      & 92       & 2616       &2004.40 $\pm$ 35.50       & 1038.20 $\pm$ 22.53       & \textbf{51.80 $\pm$  3.73}      & 611.60 $\pm$ 35.50        &203.80 $\pm$ 7.15          & \textbf{33.40 $\pm$ 1.85}\\
     & \multicolumn{1}{l}{${\rho=10}$} & 2708      & 86       & 2622       &2041.60 $\pm$ 32.48       & 1039.00 $\pm$ 41.32       & \textbf{50.89 $\pm$  1.88}      & 580.40 $\pm$ 32.48       &189.20 $\pm$ 2.35         & \textbf{32.56 $\pm$ 4.25}\\
     & \multicolumn{1}{l}{${\rho=20}$} & 2708     & 83       & 2625       &2040.20 $\pm$ 30.94       & 1002.20 $\pm$ 66.97     & \textbf{48.95 $\pm$  2.66}      & 578.80 $\pm$ 30.95        &186.60 $\pm$ 18.00         & \textbf{32.18 $\pm$ 1.57}\\
     & \multicolumn{1}{l}{${\rho=50}$} & 2708      & 203       & 2505       &1789.40 $\pm$ 30.56      & 870.20 $\pm$ 24.33      & \textbf{48.63 $\pm$  1.03}     & 715.60 $\pm$ 30.56        &242.40 $\pm$ 16.77         & \textbf{33.87 $\pm$ 1.18}\\
     & \multicolumn{1}{l}{${\rho=100}$} & 2708      & 403       & 2305      &1859.00 $\pm$ 192.42       & 914.41 $\pm$ 23.65      & \textbf{49.26 $\pm$  2.59}      & 446.00 $\pm$ 21.24        &138.87 $\pm$ 6.32         & \textbf{31.15 $\pm$ 2.43}\\


     \bottomrule[1.5 pt]
     
\end{tabular}}
\end{table}




\begin{table}[!ht]
    \centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Experimental results of DPAM effectiveness on \textbf{Amazon-Computers} with ${\rho=1, 5, 10, 20, 50, 100}$. }
% We observe the accuracy ($\%$) of the pseudo-label (prediction of the classifier) of the aligned node set $\mathcal{U}_{in}$ and the discarded node set $\mathcal{U}_{out}$ respectively. We report averaged results  with the standard errors over 5 repetitions on three representative GNN architectures. \textbf{All}, \textbf{Labeled}, \textbf{Unlabeled} represent the size of whole nodes, labeled nodes, and unlabeled nodes on the graph. \textbf{Align}, \textbf{Out}, \textbf{Align-True}, \textbf{Out-Ture} represent the size of $\mathcal{U}_{in}$,  $\mathcal{U}_{out}$, nodes with accurate pseudo-labels of $\mathcal{U}_{in}$,  $\mathcal{U}_{out}$ respectively.
    \label{DPAM-computers}
 \scalebox{0.74}{\begin{tabular}{cccccccccccc}
     \toprule[1.5 pt]
     \multirow{22.5}{*}{\rotatebox{90}{SAGE \qquad\qquad\qquad GAT  \qquad\qquad\qquad GCN}} & \multicolumn{1}{l}{\bf{Dataset}} & \multicolumn{1}{c}{All} & \multicolumn{1}{c}{Labled} &  \multicolumn{1}{c}{Unlabled} & \multicolumn{1}{c}{Align} & \multicolumn{1}{c}{Align-True} & \multicolumn{1}{c}{Accuracy(\%)} &  \multicolumn{1}{c}{Out} & \multicolumn{1}{c}{Out-True}& \multicolumn{1}{c}{Accuracy(\%)}\\

     % \cmidrule(lr){2-11}
     
     % & \multicolumn{1}{l}{Method} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{Self-Training} & \multicolumn{1}{c}{UNREAL} & \multicolumn{1}{c}{UNREAL} \\

     \cmidrule(lr){2-11}
     & \multicolumn{1}{l}{${\rho=1}$} & 13752      & 200       & 13552       &11977.60 $\pm$ 108.09       & 9603.80 $\pm$ 93.34      & \textbf{80.08 $\pm$  3.07}      & 1554.40 $\pm$ 08.23        &676.60 $\pm$ 141.11         & \textbf{43.58 $\pm$ 2.83}\\
     & \multicolumn{1}{l}{${\rho=5}$} & 13752      & 120       & 13632       &11593.60 $\pm$ 73.16       & 9172.80 $\pm$ 87.32       & \textbf{79.06 $\pm$  1.17}      & 2308.40 $\pm$ 173.54        &544.40 $\pm$ 66.26          & \textbf{30.74 $\pm$ 9.09}\\
     & \multicolumn{1}{l}{${\rho=10}$} & 13752      & 110       & 13642       &11822.40 $\pm$ 13.43       & 8786.60 $\pm$ 55.48       & \textbf{74.24 $\pm$  0.83}      & 1807.60 $\pm$ 109.34        &495.00 $\pm$ 100.37         & \textbf{27.24 $\pm$ 4.30}\\
     & \multicolumn{1}{l}{${\rho=20}$} & 13752     & 105       & 13647       &11866.60 $\pm$ 17.34       & 8698.20 $\pm$ 188.13     & \textbf{73.40 $\pm$  1.39}      & 1780.40 $\pm$ 67.36        &521.00 $\pm$ 60.76         & \textbf{29.20 $\pm$ 2.41}\\
     & \multicolumn{1}{l}{${\rho=50}$} & 13752      & 255       & 13497       &11843.20 $\pm$ 168.20       & 8994.40 $\pm$ 175.24      & \textbf{75.94 $\pm$  0.75}     & 1653.80 $\pm$ 138.11        &474.20 $\pm$ 50.72         & \textbf{28.68 $\pm$ 2.16}\\
     & \multicolumn{1}{l}{${\rho=100}$} & 13752      & 505       & 13247      &9159.00 $\pm$ 192.42       & 7352.90 $\pm$ 61.23      & \textbf{81.41 $\pm$  4.59}      & 4088.00 $\pm$ 93.99        &1129.60 $\pm$ 75.74         & \textbf{28.67 $\pm$ 4.77}\\
     


          
     \cmidrule(lr){2-11}\morecmidrules\cmidrule(lr){2-11}
     & \multicolumn{1}{l}{${\rho=1}$} & 13752      & 200       & 13552       &12008.00 $\pm$ 101.93       & 9984.20 $\pm$ 308.03      & \textbf{83.44 $\pm$  4.13}     & 1544.80 $\pm$ 101.94        &580.40 $\pm$ 190.49         & \textbf{43.33 $\pm$ 1.32}\\
     & \multicolumn{1}{l}{${\rho=5}$} & 13752      & 120       & 13632       &11570.80 $\pm$ 136.11       & 8715.00 $\pm$ 86.33      & \textbf{75.33 $\pm$  0.54}      & 2061.20 $\pm$ 136.13        &477.00 $\pm$ 97.07         & \textbf{25.39 $\pm$ 1.33}\\
     & \multicolumn{1}{l}{${\rho=10}$} & 13752      & 110       & 13642       &8947.60 $\pm$ 13.40      & 6680.40 $\pm$ 177.54      & \textbf{75.85 $\pm$  6.07}      & 4694.40 $\pm$ 134.74        &591.80 $\pm$ 13.74         & \textbf{15.94 $\pm$ 2.97}\\
     & \multicolumn{1}{l}{${\rho=20}$} & 13752      & 105       & 13647       &10245.80 $\pm$ 68.00       & 7300.80 $\pm$ 64.89      & \textbf{71.42 $\pm$  1.80}      & 3401.20 $\pm$ 69.76        &370.60 $\pm$ 43.87         & \textbf{18.52 $\pm$ 0.09}\\
     & \multicolumn{1}{l}{${\rho=50}$} & 13752      & 255       & 13497       &10133.60 $\pm$ 31.56       & 7772.00 $\pm$ 155.87      & \textbf{77.17 $\pm$  2.85}      & 3363.40 $\pm$ 10.42        &457.20 $\pm$ 108.19         & \textbf{19.28 $\pm$ 1.43}\\
     & \multicolumn{1}{l}{${\rho=100}$} & 13752      & 505       & 13247       &11377.00 $\pm$ 63.32       & 9122.20 $\pm$ 96.70      & \textbf{80.46 $\pm$  1.01}      & 1910.00 $\pm$ 63.32        &458.20 $\pm$ 41.04         & \textbf{24.78 $\pm$ 2.04}\\



     \cmidrule(lr){2-11}\morecmidrules\cmidrule(lr){2-11}
     
     & \multicolumn{1}{l}{${\rho=1}$} & 13752      & 200       & 13552       &10815.20 $\pm$ 86.50      & 7131.40 $\pm$ 72.83      & \textbf{65.94 $\pm$ 0.28}     & 2736.80 $\pm$ 86.50        &965.40 $\pm$ 56.42         & \textbf{35.26 $\pm$ 1.31}\\
     & \multicolumn{1}{l}{${\rho=5}$} & 13752      & 120       & 13632       &10627.80 $\pm$ 78.33       & 6728.00 $\pm$ 53.24      & \textbf{63.25 $\pm$  0.36}      & 3004.20 $\pm$ 78.03        &978.20 $\pm$ 59.93         & \textbf{32.55 $\pm$ 1.49}\\
     & \multicolumn{1}{l}{${\rho=10}$} & 13752      & 110       & 13642       &10475.00 $\pm$ 118.41       & 6015.00 $\pm$ 41.14      & \textbf{57.43 $\pm$  4.01}      & 3167.00 $\pm$ 18.41        &1064.40 $\pm$ 52.71         & \textbf{33.59 $\pm$ 6.23}\\
     & \multicolumn{1}{l}{${\rho=20}$} & 13752      & 105       & 13647       &10653.20 $\pm$ 87.35       & 5998.40 $\pm$ 69.35      & \textbf{56.30 $\pm$  4.01}      & 2993.80 $\pm$ 87.35        &886.20 $\pm$ 73.25         & \textbf{29.57 $\pm$ 1.77}\\
     & \multicolumn{1}{l}{${\rho=50}$} & 13752      & 255       & 13497       &11044.80 $\pm$ 129.14       & 6760.80 $\pm$ 50.26      & \textbf{61.22 $\pm$ 3.42}      & 2442.20 $\pm$ 28.48        &879.00 $\pm$ 91.45         & \textbf{35.71 $\pm$ 1.78}\\
     & \multicolumn{1}{l}{${\rho=100}$} & 13752     & 505       & 13247       &9175.20 $\pm$ 32.53       & 6475.60 $\pm$ 80.88      & \textbf{72.07 $\pm$ 1.96}     & 4071.80 $\pm$ 32.63        &1218.60 $\pm$ 14.70         &\textbf{34.43 $\pm$ 1.08}\\


     \bottomrule[1.5 pt]
     
\end{tabular}}
\end{table}







\subsection{More Results and Analysis about Fluctuation of RBO Values (Section \ref{reorder})}
\label{More Results and Analysis about Fluctuation of RBO Values}

In Section \ref{reorder}, we argue that as the iteration progresses, the confidence given by the classifier becomes increasingly valuable (the training set is gradually balanced), whereas the geometric rankings are calculated in the embedding space and are unaffected by the classifier. As a result, it is trustworthy that as the credibility of confidence grows in the iterative process, the similarities between the Confidence Rankings and the Geometric Rankings will gradually increase. Notably, the unsupervised algorithm performs worse overall than supervised methods, especially for a balanced training set. As a result, by combining the features of the two rankings, we can greatly improve the performance of our algorithm. Experiments are used to put the aforementioned hypothesis to the test.


\begin{figure}[!ht]
\centering
% \subfigure[Cora-GAT]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[scale=0.20]{figure/Section5Node-reorderingRBO/rbocora10gat.pdf}
% %\caption{fig1}
% \end{minipage}%
% }%
\subfigure[Cora-SAGE]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.20]{rbocora10sage.pdf}
%\caption{fig1}
\end{minipage}%
}%
\centering
\caption{Fluctuation of RBO values ($\rho$ = 10) of two rankings as iterations progress.}
\label{figure_coragatsage_rbo}
\end{figure}


\paragraph{Details of Experimental Setup.} We conduct more experiments on Cora ($\rho$ = 10) to observe the similarities between the Geometric Rankings and Confidence Rankings. The architecture is fixed as the 2-layer GNN (i.e. GCN\citep{kipf2016semi}, GAT\citep{velivckovic2017graph}, GraphSAGE\citep{hamilton2017inductive})  having 128 hidden dimensions and train models for 2000 epochs. The UNREAL model's hyperparameter settings can be found in Appendix \ref{Implementation details}. We choose a majority and a minority class at random to compare the similarities of their respective two rankings (our setting is the first class and the last class of Cora), and we limit the number of iterations to eight. Each experiment is repeated five times, and the average experiment results are reported in Figure \ref{figure_coragcn_rbo}, Figure \ref{figure_coragat_rbo}, and Figure \ref{figure_coragatsage_rbo}.


\paragraph{Analysis.} As illustrated by Figure \ref{figure_coragcn_rbo}, Figure \ref{figure_coragat_rbo} and Figure \ref{figure_coragatsage_rbo}, it can be observed that the similarities between the Confidence Rankings and the Geometric Rankings will gradually increase as the iteration progresses in the early stages. As a result, our hypothesis is confirmed. It is worth noting that the similarity of the two rankings of the minority class is higher than the majority class when the training set is gradually balanced, which also reflects the compensation benefit of UNREAL for the minority class.









\subsection{Additional Analysis for Node-Reordering (Section \ref{reorder})}
\label{Additional analysis for Node-Reordering and DGIN}


% \input{iclr2023/figures/result-tsne1.tex}
In this section, we analyze why Node-Reordering works. With DPAM, we filter out a large part of untrustworthy nodes and get a pool of candidate nodes. We try to carefully hunt for a part of high-quality nodes in the pool to add to the training set, which involves a priority issue. As we mentioned before, we have already verified in Section \ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning} that the prediction and confidence given by the classifier are biased, resulting in low accuracy of the pseudo-labels for nodes selected by ST in highly imbalanced scenarios. We can get the geometric ranking according to the distance between the unlabeled nodes and the class centers in the embedding space. Considering the influence of classifier bias on confidence ranking, we believe that geometric ranking is more credible in the early rounds. At the same time, we take into account the suboptimal nature of the unsupervised algorithm. We believe that with the rounds of UNREAL increases, the label distribution of the training set is gradually balanced, and the confidence given by the classifier is more reliable. Node-reordering considers both geometric ranking and confidence ranking, specifically, obtaining the similarity between them to get a weight to reorder the priority of the nodes. To quantify the performance of Node-Reordering, we conduct the novel experiments below.


\paragraph{Details of Experimental Setup.} We conduct experiments to test the accuracy of pseudo labels for unlabeled nodes on class-imbalanced graphs. All model combinations based on different GNN structures are trained on two node classification benchmark datasets, Cora, and Amaon-Computers. We process the two datasets with a traditional imbalanced distribution following \citet{zhao2021graphsmote, park2021graphens, song2022tam}. The imbalance ratio $\rho$ between the numbers of the most frequent class and the least frequent class is set as 1, 5, 10, 20, 50, and 100.  We fix architecture as the 2-layer GNN (i.e. GCN\citep{kipf2016semi}, GAT\citep{velivckovic2017graph}, GraphSAGE\citep{hamilton2017inductive})  having 128 hidden dimensions and train models for 2000 epochs. We select the model by the validation accuracy.  We observe the accuracy of pseudo labels for unlabeled nodes which are newly added to the minority class of the training set. We repeat each experiment five times and present the average experiment results in Table \ref{DGIN-cora} and Table \ref{DGIN-computers}.

\paragraph{Analysis.} As shown in Table \ref{DGIN-cora} and Table \ref{DGIN-computers}, we verify the effectiveness of each component of UNREAL by testing the accuracy of the nodes' pseudo-labels selected by different model combinations, DPAM+Confidence ranking(with or without DGIN), DPAM+Geometric ranking(with or without DGIN), DPAM+Node-Reordering(with or without DGIN). It can be observed that in different imbalanced scenarios, each component of UNREAL (Node-reordering \& DGIN) plays an important role, and the performance outperforms the other model combinations significantly.


\begin{table}[!ht]
    \centering
        \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Analyzed experimental results of Node-Reordering and DGIN on \textbf{Cora} with ${\rho=1, 5, 10, 20, 50, 100}$. We select 100 unlabeled nodes newly added to the minority class of training set through different method combinations, and evaluate the validity of Node-Reordering \& DGIN by testing the accuracy ($\%$) with the standard errors of the pseudo labels for these nodes. We report averaged results over 5 repetitions on three representative GNN architectures.}
    \label{DGIN-cora}
 \scalebox{0.79}{\begin{tabular}{lllllllll}
     \toprule[1.5 pt]
     \multirow{25}{*}{\rotatebox{90}{SAGE \qquad\qquad\qquad GAT  \qquad\qquad\qquad  GCN}} & \multicolumn{1}{c}{\bf{Dataset}} & \multicolumn{6}{c}{Cora}  &\\

     \cmidrule(lr){2-8}
     & \bf{Imbalance Ratio $({\rho})$} & \multicolumn{1}{c}{${\rho=1}$} & \multicolumn{1}{c}{${\rho=5}$} & \multicolumn{1}{c}{${\rho=10}$} & \multicolumn{1}{c}{${\rho=20}$} & \multicolumn{1}{c}{${\rho=50}$} & \multicolumn{1}{c}{${\rho=100}$}  \\

     \cmidrule(lr){2-8}
     
     & DPAM+Confidence Ranking     & 61.40 $\pm$ 2.73   & 62.40 $\pm$ 2.59   & 60.20 $\pm$ 1.02   &58.40 $\pm$ 1.05   & 57.60 $\pm$ 1.86   & 58.40 $\pm$  2.15 \\
     & DPAM+Geometric Ranking      & 64.00 $\pm$ 3.67   & 61.20 $\pm$ 2.89   & 61.20 $\pm$ 2.54   &63.60 $\pm$ 1.31   & 55.60 $\pm$ 2.31   & 47.80 $\pm$  2.87 \\
     & DPAM+Node-Reordering        & 89.65 $\pm$ 3.23   & 86.98 $\pm$ 0.21   & 88.32 $\pm$ 0.83   &85.32 $\pm$ 2.98   & 90.87 $\pm$ 2.31   & 71.60 $\pm$  2.91 \\
     \cdashline{2-8}
     & DPAM+Confidence Ranking+DGIN & 71.00 $\pm$ 5.47   & 75.40 $\pm$ 2.15   & 68.20 $\pm$ 1.25   &69.40 $\pm$ 1.28   & 67.80 $\pm$ 2.75   & 66.60 $\pm$  0.16 \\
     & DPAM+Geometric Ranking+DGIN  & 69.60 $\pm$ 3.78   & 73.80 $\pm$ 0.45   & 64.80 $\pm$ 1.26   &64.20 $\pm$ 1.91   & 57.00 $\pm$ 1.57   & 69.00 $\pm$  1.71 \\
     
     
     
     & \textbf{DPAM+Node-Reordering+DGIN(UNREAL)} &  \textbf{92.80 $\pm$ 1.30} &  \textbf{96.40 $\pm$ 4.27} & \textbf{92.20 $\pm$ 0.85}  &  \textbf{89.40 $\pm$ 1.37}  &  \textbf{93.00 $\pm$ 0.82}  & \textbf{77.80 $\pm$ 2.50} \\



     \cmidrule(lr){2-8}\morecmidrules\cmidrule(lr){2-8}
     
     & DPAM+Confidence Ranking     & 61.60 $\pm$ 4.26    &64.00 $\pm$ 2.07    & 62.60 $\pm$ 3.47   &57.80 $\pm$ 1.65   & 58.20 $\pm$ 1.07   & 60.60 $\pm$  0.79 \\
     & DPAM+Geometric Ranking      & 64.00 $\pm$ 2.78    & 67.80 $\pm$ 3.76   & 65.00 $\pm$ 4.30   &52.00 $\pm$ 1.02   & 65.20 $\pm$ 2.58   & 40.80 $\pm$  2.63 \\
     & DPAM+Node-Reordering        & 91.79 $\pm$ 0.23    & 90.45 $\pm$ 5.78   & 84.32 $\pm$ 3.45   &88.34 $\pm$ 0.23   & 90.32 $\pm$ 0.43   & 75.34 $\pm$  1.54 \\
     \cdashline{2-8}
     & DPAM+Confidence Ranking+DGIN & 69.80 $\pm$ 2.77    & 72.80 $\pm$ 3.94   & 72.40 $\pm$ 1.13   &67.60 $\pm$ 1.59   & 71.60 $\pm$ 9.12   & 64.00 $\pm$  1.74 \\
     & DPAM+Geometric Ranking+DGIN  & 73.60 $\pm$ 4.82    & 74.00 $\pm$ 5.47   & 68.40 $\pm$ 1.62   &57.20 $\pm$ 2.17   & 68.00 $\pm$ 1.17   & 62.00 $\pm$  1.53 \\

     

 
     
     & \textbf{DPAM+Node-Reordering+DGIN(UNREAL)} &  \textbf{93.80 $\pm$ 1.92} &  \textbf{91.20 $\pm$ 4.60} & \textbf{90.40 $\pm$ 1.69}  &  \textbf{90.00 $\pm$ 9.92}  &  \textbf{94.60 $\pm$ 4.92}  & \textbf{78.20 $\pm$ 2.47} \\



     
     \cmidrule(lr){2-8}\morecmidrules\cmidrule(lr){2-8}
     & DPAM+Confidence Ranking     & 54.80 $\pm$ 4.96    & 53.00 $\pm$ 2.46   & 51.80 $\pm$ 1.97   &43.60 $\pm$ 2.57   & 46.20 $\pm$ 0.53   & 41.60 $\pm$  1.14 \\
     & DPAM+Geometric Ranking      & 53.60 $\pm$ 2.78    & 45.40 $\pm$ 1.75   & 40.60 $\pm$ 0.26   &52.60 $\pm$ 2.47   & 47.40 $\pm$ 4.27   & 44.80 $\pm$  2.84 \\
     & DPAM+Node-Reordering        & 90.69 $\pm$ 0.21    & 86.90 $\pm$ 0.56   & 86.45 $\pm$ 3.21   &88.34 $\pm$ 2.43   & 75.34 $\pm$ 4.20   & 76.43 $\pm$  1.43 \\
     \cdashline{2-8}
     & DPAM+Confidence Ranking+DGIN & 66.20 $\pm$ 5.78    & 59.00 $\pm$ 3.04   & 63.80 $\pm$ 1.52   &54.60 $\pm$ 1.64   & 60.60 $\pm$ 1.37   & 57.40 $\pm$  2.26 \\
     

     & DPAM+Geometric Ranking+DGIN  & 61.60 $\pm$ 3.71    & 61.80 $\pm$ 5.21   & 54.00 $\pm$ 7.31   &53.60 $\pm$ 1.38   & 63.00 $\pm$ 1.23   & 45.20 $\pm$  1.96 \\

     

    
     
     & \textbf{DPAM+Node-Reordering+DGIN(UNREAL)} &  \textbf{97.80 $\pm$ 1.78} &  \textbf{92.20 $\pm$ 1.32} & \textbf{90.80 $\pm$ 1.82}  &  \textbf{89.20 $\pm$ 1.39}  &  \textbf{94.20 $\pm$ 8.04}  & \textbf{85.40 $\pm$ 1.02} \\



     

     \bottomrule[1.5 pt]
  \end{tabular}}
\end{table}



\begin{table}[!ht]
    \centering
        \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
    \caption{Analyzed experimental results of Node-Reordering and DGIN on \textbf{Amazon-Computers} with ${\rho=1, 5, 10, 20, 50, 100}$. We select 100 unlabeled nodes newly added to the minority class of training set through different method combinations, and evaluate the validity of Node-Reordering \& DGIN by testing the accuracy ($\%$) with the standard errors of the pseudo labels for these nodes. We report averaged results over 5 repetitions on three representative GNN architectures.}
    \label{DGIN-computers}
 \scalebox{0.79}{\begin{tabular}{lllllllll}
     \toprule[1.5 pt]
     \multirow{25}{*}{\rotatebox{90}{SAGE \qquad\qquad\qquad GAT  \qquad\qquad\qquad  GCN}} & \multicolumn{1}{c}{\bf{Dataset}} & \multicolumn{6}{c}{Amazon-Computers}  &\\

     \cmidrule(lr){2-8}
     & \bf{Imbalance Ratio $({\rho})$} & \multicolumn{1}{c}{${\rho=1}$} & \multicolumn{1}{c}{${\rho=5}$} & \multicolumn{1}{c}{${\rho=10}$} & \multicolumn{1}{c}{${\rho=20}$} & \multicolumn{1}{c}{${\rho=50}$} & \multicolumn{1}{c}{${\rho=100}$}  \\

     \cmidrule(lr){2-8}
     
     & DPAM+Confidence Ranking     & 75.40 $\pm$ 2.50   & 70.20 $\pm$ 3.03   & 74.88 $\pm$ 3.11   &68.20 $\pm$ 4.20   & 63.60 $\pm$ 2.30   & 61.40 $\pm$  1.51 \\
     & DPAM+Geometric Ranking      & 76.00 $\pm$ 1.41   & 74.80 $\pm$ 4.71   & 76.80 $\pm$ 2.28   &65.80 $\pm$ 3.27   & 64.80 $\pm$ 3.70   & 65.60 $\pm$  3.98 \\
     & DPAM+Node-Reordering        & 82.80 $\pm$ 2.38   & 79.60 $\pm$ 3.64   & 78.20 $\pm$ 0.26   &74.00 $\pm$ 3.28   & 65.20 $\pm$ 1.87   & 66.00 $\pm$  2.82 \\
     \cdashline{2-8}
     & DPAM+Confidence Ranking+DGIN & 76.40 $\pm$ 2.07   & 67.20 $\pm$ 4.32   & 75.80 $\pm$ 2.38   &66.20 $\pm$ 3.70   & 62.80 $\pm$ 0.12   & 59.20 $\pm$  1.30 \\
     & DPAM+Geometric Ranking+DGIN  & 78.20 $\pm$ 0.83   & 80.00 $\pm$ 1.22   & 76.40 $\pm$ 1.67   &66.00 $\pm$ 2.44   & 64.20 $\pm$ 3.83   & 66.20 $\pm$ 2.38 \\


     & \textbf{DPAM+Node-Reordering+DGIN(UNREAL)} &  \textbf{84.40 $\pm$ 3.60} &  \textbf{82.20 $\pm$ 2.16} & \textbf{80.40 $\pm$ 3.46}  &  \textbf{80.60 $\pm$ 1.51}  &  \textbf{69.60 $\pm$ 3.04}  & \textbf{66.40 $\pm$ 3.20} \\









     \cmidrule(lr){2-8}\morecmidrules\cmidrule(lr){2-8}
     
     & DPAM+Confidence Ranking     & 84.60 $\pm$ 2.40    & 79.20 $\pm$ 1.78   & 73.00 $\pm$ 2.12   &74.80 $\pm$ 2.16   & 65.00 $\pm$ 1.73   & 68.60 $\pm$  1.40 \\
     & DPAM+Geometric Ranking      & 86.00 $\pm$ 3.80    & 79.80 $\pm$ 2.94   & 74.80 $\pm$ 3.42   &75.00 $\pm$ 2.91   & 70.80 $\pm$ 2.16   & 69.40 $\pm$  1.10 \\
     & DPAM+Node-Reordering        & 87.40 $\pm$ 2.30    & 80.60 $\pm$ 3.04   & 80.40 $\pm$ 2.19   &79.00 $\pm$ 3.67   & 75.00 $\pm$ 1.22   & 73.40 $\pm$  2.52 \\
     \cdashline{2-8}
     & DPAM+Confidence Ranking+DGIN & 84.20 $\pm$ 1.64    & 79.40 $\pm$ 2.07   & 76.40 $\pm$ 6.50   &76.00 $\pm$ 2.34   & 66.00 $\pm$ 0.12   & 72.00 $\pm$  1.84 \\
     & DPAM+Geometric Ranking+DGIN  & 83.80 $\pm$ 1.09    & 80.20 $\pm$ 1.09   & 76.20 $\pm$ 2.28   &77.80 $\pm$ 2.58   & 71.60 $\pm$ 0.89   & 69.00 $\pm$  1.16 \\

     
     & \textbf{DPAM+Node-Reordering+DGIN(UNREAL)} &  \textbf{89.00 $\pm$ 2.54} &  \textbf{86.60 $\pm$ 2.50} & \textbf{85.60 $\pm$ 4.44}  &  \textbf{83.40 $\pm$ 3.31}  &  \textbf{78.00 $\pm$ 3.39}  & \textbf{79.80 $\pm$ 3.03} \\







     
     \cmidrule(lr){2-8}\morecmidrules\cmidrule(lr){2-8}
     & DPAM+Confidence Ranking     & 85.20 $\pm$ 3.38    & 80.20 $\pm$ 6.26   & 84.8 $\pm$ 0.83   &77.60 $\pm$ 0.89   & 61.00 $\pm$ 0.70   & 65.40 $\pm$  2.65 \\
     & DPAM+Geometric Ranking      & 86.00 $\pm$ 0.70    & 81.20 $\pm$ 2.16   & 83.40 $\pm$ 1.14   &78.00 $\pm$ 1.22   & 61.40 $\pm$ 0.54   & 65.00 $\pm$  1.72 \\
     & DPAM+Node-Reordering        & 86.00 $\pm$ 1.58    & 83.20 $\pm$ 3.27   & 84.60 $\pm$ 0.54   &79.20 $\pm$ 1.92   & 61.80 $\pm$ 0.44   & 67.80 $\pm$  1.03 \\
     \cdashline{2-8}
     & DPAM+Confidence Ranking+DGIN & 86.40 $\pm$ 2.07    & 81.60 $\pm$ 3.20   & 83.40 $\pm$ 1.14   &\textbf{79.20 $\pm$ 0.44}   & 61.20 $\pm$ 0.44   & 70.40 $\pm$  3.59 \\
     & DPAM+Geometric Ranking+DGIN  & 87.00 $\pm$ 2.12    & 80.80 $\pm$ 2.48   & 84.20 $\pm$ 1.30   &78.20 $\pm$ 1.48   & 61.20 $\pm$ 0.47   & 68.20 $\pm$  1.72 \\


     
     & \textbf{DPAM+Node-Reordering+DGIN(UNREAL)} &  \textbf{88.20 $\pm$ 2.16} &  \textbf{87.60 $\pm$ 1.14} & \textbf{85.40 $\pm$ 4.72}  &  78.00 $\pm$ 1.55  &  \textbf{66.20 $\pm$ 2.86}  & \textbf{72.20 $\pm$ 0.83} \\


     

     \bottomrule[1.5 pt]
  \end{tabular}}
\end{table}





\begin{figure}[!ht]
\centering
\subfigure[Sensitivity performance of $k'$ of K-Means]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.20]{Num_K.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[Sensitivity  performance of the threshold $\gamma$ of DGI]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.20]{DGI.pdf}
%\caption{fig2}
\end{minipage}%
}%
\caption{Sensitivity analysis on Cora based on GCN. The two images show the performance change as clusters' size $k'$ of K-Means and the threshold $\gamma$ of DGI increases respectively.}
\centering
\label{ana_result}
\end{figure}


\begin{table}[!ht]
\centering
\setlength{\abovecaptionskip}{0pt}%    
\setlength{\belowcaptionskip}{10pt}%
\caption{\centering Ablation analysis on different components}
\label{ablation}
\scalebox{1}{\begin{tabular}{c|cccc|c}
   \toprule[1.5pt]
    \textbf{Modules} & \textbf{Confidence ranking} & \textbf{Geometric ranking} &\textbf{Node-reordering}  & \textbf{DGI} & \textbf{F1} \\
   \midrule
   \multirow{6}*{Cora+GCN ($\rho=10$)} & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 73.93 $\pm$ 0.95\\
   &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$& 72.74 $\pm$ 0.63\\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 75.85 $\pm$ 0.82 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & 75.34 $\pm$ 0.63\\
   &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 75.00 $\pm$ 0.97\\
   &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & \textbf{76.44 $\pm$ 1.06}\\
   \cmidrule(lr){1-6}
   \multirow{6}*{CiteSeer+SAGE ($\rho=20$)} & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 46.09 $\pm$ 4.08\\
   &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$& 47.76 $\pm$ 1.06 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 50.32 $\pm$ 3.75\\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & 53.32 $\pm$ 3.75\\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & \textbf{58.71$\pm$ 3.21} \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$& $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & 57.51 $\pm$ 4.92 \\
   \cmidrule(lr){1-6}
   \multirow{6}*{PubMed+GAT ($\rho=50$)} & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 76.34 $\pm$ 0.39\\
   &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$& 75.42 $\pm$ 0.39 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$&$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 77.32 $\pm$ 0.21 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & 76.89 $\pm$ 1.43 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 76.12 $\pm$ 2.63 \\
   &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & \textbf{77.38 $\pm$ 0.39}\\
   \cmidrule(lr){1-6}
   \multirow{6}*{Computers+GAT ($\rho=100$)} & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$& 70.86 $\pm$ 1.73\\
   &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & 68.86 $\pm$ 1.42 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 72.32 $\pm$ 2.43 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & 73.65 $\pm$ 0.67\\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & 74.03 $\pm$ 2.53 \\
   & $\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ &$\textcolor[rgb]{0.46667,0.53333,0.6}{\usym{1F5F4}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & $\textcolor[rgb]{0.98039,0.50196,0.44706}{\usym{2714}}$ & \textbf{75.83 $\pm$ 0.74}\\
   \bottomrule[1.5pt]
\end{tabular}}
\end{table}










 

\subsection{Additional Analysis for GI and DGIN (Section \ref{Geometric Imbalance})}
\label{Additional analysis for DI and DGIN}
In this section, We elaborate on the issue, Geometric Imbalance and verify the effectiveness of DGIN. 

\paragraph{Geometric Imbalance.} As the prior work \citet{song2022tam, chen2021topology} argued, too many labeled nodes remaining near the boundary between two classes will have a significant impact on GNN classification performance. In our work, we try to select some unlabeled nodes to join the training set, which means that nodes near the center of the classes (far from any class boundaries) should be prioritized. Node-Reordering has made some efforts in this part (Section \ref{reorder}). Considering that  UNREAL iteratively selects nodes to join the training set (Section \ref{Selecting New Nodes Iteratively}), some suboptimal nodes (not so close to class centers) are gradually considered in later stages, which is referred to as Geometric Imbalance (GI). We generalize the novel Geometric Imbalance to have the following properties: (1) Only unlabeled nodes suffer from GI, and this is the first work to investigate the local topology of unlabeled nodes in the imbalanced node classification problem. (2) Geometrically imbalanced nodes are on class boundaries and are frequently indistinguishable.  (3) For unlabeled nodes that encounter GI, DPAM often encounters the conflict of pseudo-labeling. This is because these nodes in the embedding space are roughly the same distance apart from the two class centers.


% We conduct experiments to test the accuracy of pseudo labels for unlabeled nodes on class-imbalanced graphs.

\paragraph{Motivating Example For GI.} Here, we conduct a novel experiment on Cora ($\rho$ = 10) to verify the necessity of the GI issue. We choose two adjacent classes (our setting is the first and last classes of Cora) at random in the embedding space and examine only the nodes with pseudo-labels that belong to these two classes (these nodes were previously filtered by DPAM), and we divide the category boundaries of the two classes into five regions, S1, S2, S3, S4, S5. We independently validate the pseudo-label accuracy of unlabeled nodes in each region. The architecture is fixed as the 2-layer GCN\citep{kipf2016semi} having 128 hidden dimensions and train models for 2000 epochs. Each experiment is repeated five times, and the average experiment results are reported in Figure \ref{Pseudo-label Accuracy for Each Region}. The diagram of the motivating example is Figure \ref{Diagram of the motivating example}.

\begin{figure}[!ht]
\centering
\subfigure[The diagram of the Motivating Example]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[scale=0.310]{newDGIN.pdf}
%\caption{fig1}
\label{Diagram of the motivating example}
\end{minipage}%
}%
\subfigure[Accuracy of pseudo-labels for unlabeled nodes per region.]{
\begin{minipage}[t]{0.5\linewidth}
\centering
% \includegraphics[width=2\textwidth,height=1.7\textwidth]{figure/Section5Node-reorderingRBO/cora_gcn_10_rbo.pdf}
\includegraphics[scale=0.20]{Different-Regions.pdf}
%\caption{fig2}
\label{Pseudo-label Accuracy for Each Region}
\end{minipage}
}%
\centering
\caption{(a) We visualize the motivating example's intent. The setting is to divide the distance between the class centers into five equal parts and then evenly divide the class boundaries. (b) Accuracy of pseudo-labels for unlabeled nodes per region..}
% (a) The partial experimental results on Cora under different imbalance scenarios($\rho$ = 1, 5, 10, 20, 50, 100). We compare the accuracy of the two pseudo-labels(predictions) from unsupervised algorithms and supervised classifiers respectively for all unlabeled nodes. (b) Fluctuation of RBO values (similarity) of two rankings as iterations progress.
\label{figure_cora_dgin}
\end{figure}


\paragraph{Analysis.} In Figure \ref{Pseudo-label Accuracy for Each Region}, the result reveals several intriguing aspects: (1) Unlabeled nodes near the classification boundary are vulnerable to GI issues, and the specific performance is that the correctness of pseudo-labels is extremely low. (2) Geometric Ranking has a high reference significance for selecting high-quality nodes. The closer the unlabeled node is to the center of the class, the higher its pseudo-label accuracy. (3) Including these geometrically imbalanced nodes in the training set will introduce a lot of noise into the model training, so a simple and effective method to deal with this issue is desperately needed. Based on this, we propose DGIN.  Looking back at the rich analysis experiments, Table \ref{DGIN-cora} and Table \ref{DGIN-computers} in Appendix \ref{Additional analysis for Node-Reordering and DGIN}, it can be observed that DGIN plays an important role in various imbalanced scenarios, and its performance significantly outperforms the other model combinations, which validates DGIN's effectiveness in dealing with GI issues and contributes to the model's overall performance.






\subsection{Hyperparameter Sensitivity Analysis of UNREAL}
\label{Hyperparameter Sensitivity Analysis of UNREAL}

We investigate the sensitivity of performance to clusters' size $k'$ of the K-Means algorithm and the threshold $\gamma$ of DGIN in Figure \ref{ana_result}. We observe the performance gradually stabilizes when $k'$ has extremely high values, on the other hand, when $k'$ is extremely low values, the performance of UNREAL drops largely. We believe that when $k'$ is too small, the pseudo-labels given by unsupervised algorithms will have more errors. Also, we observe the performance gradually stabilizes when $\gamma$ has extremely low values. We believe this is because the DGIN screening is too strict, which will lead to the loss of some high-quality nodes. On the other hand, extremely large  $\gamma$ will introduce much noise into the training set.





\subsection{Ablation Analysis} 
\label{Ablation analysis}
In this section, we conduct ablation studies to analyze the benefit of each component in our method. From the results in Section~\ref{Pseudo-label Misjudgment Augmentation Problem in Imbalanced Learning}, the necessity of unsupervised learning in the embedding space has been verified. Thus, in this section, DPAM is applied in all comparing methods. Here, we test the performance of three different ranking methods, namely confidence ranking, geometric ranking, and Node-reordering (which combines the former two rankings with information retrieval techniques). Moreover, we test the effect of DGIN, which aims to eliminate geometrically imbalanced nodes. As shown in Table \ref{ablation}, each component of our method can bring performance improvements. In particular, in three out of four settings in the table, Node-Reordering+DGIN achieves the best F1 scores. In all cases, geometric ranking outperforms confidence ranking, proving our hypothesis that prediction confidence scores may contain bias and be less reliable.



\newpage
\section{Details of the experimental setup}
\label{Details of the experimental setup}
Here, we introduce the method of imbalanced datasets construction, evaluation protocol, and the details of our algorithm and baseline methods.

\subsection{Imbalanced datasets construction}
\label{Label distribution}

\begin{table}[!h]
	%\setlength{\abovecaptionskip}{0.2cm}
	%\setlength{\belowcaptionskip}{-0.2cm}
	\scriptsize
	\caption{\centering Summary of the datasets used in our experiments.}	
	\begin{tabular}{lcccc}\toprule \centering
		\textbf{Dataset}&\textbf{Nodes}&\textbf{Edges}&\textbf{Features}&\textbf{Classes}\\
		\midrule
		Cora&2,708&5,429 &1,433&7\\
		Citeseer&3,327&4,732&3,703&6 \\
		Pubmed&19,717&44,338&500&3 \\
		Amazon-Computers&13,752&491,722&767&10\\
		Flickr&89,250 &899,756&500&7\\
		\bottomrule
		%\bottomrule
	\end{tabular}
	\label{tab:datasets}
	\centering
\end{table}
The detailed descriptions of the datasets are shown in Table \ref{tab:datasets}. For each citation dataset, for $\rho=10, 20$, we follow the “public” split, and randomly convert minority class nodes to unlabeled nodes until the dataset reaches an imbalanced ratio $\rho$.  For $\rho=50, 100$, since there are not enough nodes per class in the public split training set, we choose randomly selected nodes as training samples, and for validation and test sets we still follow the public split. For the co-purchased networks Amazon-Computers, we randomly select nodes as training set in each replicated experiment, construct a random validation set with 30 nodes in each class and treat the remaining nodes as the testing set. For Flickr, we follow the dataset split from \citet{zeng2019graphsaint}. For Computers-Random, we build a training set of equal proportions based on the label distribution of the entire graph (Amazon-Computers). The label distribution in the training set for Computers-Random is summarized in Table \ref{training_set_label_distribution}. The details of label distribution in the training set of the five imbalanced benchmark datasets are in Table \ref{training_set_label_distribution}, and the label distribution of the full graph is provided in Table \ref{full_graph_label_distribution}.



\begin{table}[p]
\centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
\caption{\centering Label distributions in the training sets}
\label{training_set_label_distribution}
\scalebox{0.80}{\begin{tabular}{l|cccccccccc}
    \toprule[2.0pt]
    \textbf{Dataset} & $\mathcal{C}_{0}$ & $\mathcal{C}_{1}$ & $\mathcal{C}_{2}$ & $\mathcal{C}_{3}$ &$\mathcal{C}_{4}$ &$\mathcal{C}_{5}$ & $\mathcal{C}_{6}$ & $\mathcal{C}_{7}$ & $\mathcal{C}_{8}$ & $\mathcal{C}_{9}$ \\
    \hline
    Cora ($\rho$ = 10)& \thead{\textbf{20}\\($23.26\%$)} & \thead{\textbf{20}\\($23.26\%$)}  & \thead{\textbf{20}\\($23.26\%$)} & \thead{\textbf{20}\\($23.26\%$)} & \thead{\textbf{2}\\($23.26\%$)} & \thead{\textbf{2}\\($23.26\%$)} & \thead{\textbf{2}\\($23.26\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    Cora ($\rho$ = 20)& \thead{\textbf{20}\\($24.10\%$)} & \thead{\textbf{20}\\($24.10\%$)}  & \thead{\textbf{20}\\($24.10\%$)} & \thead{\textbf{20}\\($24.10\%$)} & \thead{\textbf{1}\\($1.19\%$)} & \thead{\textbf{1}\\($1.19\%$)} & \thead{\textbf{1}\\($1.19\%$)}& $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    Cora ($\rho$ = 50)& \thead{\textbf{50}\\($24.63\%$)} & \thead{\textbf{50}\\($24.63\%$)}  & \thead{\textbf{50}\\($24.63\%$)} & \thead{\textbf{50}\\($24.63\%$)} & \thead{\textbf{1}\\($0.49\%$)} & \thead{\textbf{1}\\($0.49\%$)}& \thead{\textbf{1}\\($0.49\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    Cora ($\rho$ = 100)& \thead{\textbf{100}\\($24.81\%$)} & \thead{\textbf{100}\\($24.81\%$)}  & \thead{\textbf{100}\\($24.81\%$)} & \thead{\textbf{100}\\($24.81\%$)} & \thead{\textbf{1}\\($0.25\%$)} &\thead{\textbf{1}\\($0.25\%$)} & \thead{\textbf{1}\\($0.25\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    \hline
    CiteSeer ($\rho$ = 10)& \thead{\textbf{20}\\($30.30\%$)} & \thead{\textbf{20}\\($30.30\%$)}  &\thead{\textbf{20}\\($30.30\%$)} &\thead{\textbf{2}\\($30.30\%$)} & \thead{\textbf{2}\\($3.03\%$)} & \thead{\textbf{2}\\($3.03\%$)} &$\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    CiteSeer ($\rho$ = 20)& \thead{\textbf{20}\\($31.75\%$)} & \thead{\textbf{20}\\($31.75\%$)}  & \thead{\textbf{20}\\($31.75\%$)} & \thead{\textbf{1}\\($1.59\%$)} & \thead{\textbf{1}\\($1.59\%$)}& \thead{\textbf{1}\\($1.59\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    CiteSeer ($\rho$ = 50)&\thead{\textbf{50}\\($32.68\%$)} &\thead{\textbf{50}\\($32.68\%$)}  & \thead{\textbf{50}\\($32.68\%$)} & \thead{\textbf{1}\\($0.65\%$)} & \thead{\textbf{1}\\($0.65\%$)} & \thead{\textbf{1}\\($0.65\%$)}& $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    CiteSeer ($\rho$  = 100)&\thead{\textbf{100}\\($33.00\%$)} & \thead{\textbf{100}\\($33.00\%$)} & \thead{\textbf{100}\\($33.00\%$)}  & \thead{\textbf{1}\\($0.33\%$)} & \thead{\textbf{1}\\($0.33\%$)} &\thead{\textbf{1}\\($0.33\%$)} &$\textbf{\text{-}}$ &$\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  \\
    \hline
    PubMed ($\rho$ = 10)& \thead{\textbf{20}\\($47.62\%$)} & \thead{\textbf{20}\\($47.62\%$)}  & \thead{\textbf{2}\\($4.76\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    PubMed ($\rho$ = 20)& \thead{\textbf{20}\\($48.78\%$)} & \thead{\textbf{20}\\($48.78\%$)}  & \thead{\textbf{1}\\($2.44\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    PubMed ($\rho$ = 50)& \thead{\textbf{50}\\($49.50\%$)} & \thead{\textbf{50}\\($49.50\%$)}  & \thead{\textbf{1}\\($0.99\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    PubMed ($\rho$ = 100)& \thead{\textbf{100}\\($49.75\%$)} & \thead{\textbf{100}\\($49.75\%$)}  & \thead{\textbf{1}\\($0.50\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ &$\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    \hline
    Amazon-Computers ($\rho$ = 10)& \thead{\textbf{20}\\($18.18\%$)} & \thead{\textbf{20}\\($18.18\%$)} & \thead{\textbf{20}\\($18.18\%$)} &\thead{\textbf{20}\\($18.18\%$)} & \thead{\textbf{20}\\($18.18\%$)} &\thead{\textbf{2}\\($1.82\%$)} & \thead{\textbf{2}\\($1.82\%$)} & \thead{\textbf{2}\\($1.82\%$)} & \thead{\textbf{2}\\($1.82\%$)} & \thead{\textbf{2}\\($1.82\%$)} \\
    Amzon-Computers ($\rho$ = 20)&  \thead{\textbf{20}\\($19.05\%$)} &  \thead{\textbf{20}\\($19.05\%$)}  &  \thead{\textbf{20}\\($19.05\%$)} &  \thead{\textbf{20}\\($19.05\%$)} &  \thead{\textbf{20}\\($19.05\%$)} &  \thead{\textbf{1}\\($0.95\%$)} &  \thead{\textbf{1}\\($0.95\%$)} & \thead{\textbf{1}\\($0.95\%$)} &  \thead{\textbf{1}\\($0.95\%$)} &  \thead{\textbf{1}\\($0.95\%$)} \\
    Amzon-Computers ($\rho$ = 50)& \thead{\textbf{50}\\($19.61\%$)} & \thead{\textbf{50}\\($19.61\%$)}  & \thead{\textbf{50}\\($19.61\%$)} & \thead{\textbf{50}\\($19.61\%$)} & \thead{\textbf{50}\\($19.61\%$)} &\thead{\textbf{1}\\($0.39\%$)} & \thead{\textbf{1}\\($0.39\%$)} & \thead{\textbf{1}\\($0.39\%$)} & \thead{\textbf{1}\\($0.39\%$)} & \thead{\textbf{1}\\($0.39\%$)} \\
    Amzon-Computers ($\rho$ = 100)& \thead{\textbf{100}\\($19.80\%$)} & \thead{\textbf{100}\\($19.80\%$)}  &\thead{\textbf{100}\\($19.80\%$)} & \thead{\textbf{100}\\($19.80\%$)} & \thead{\textbf{100}\\($19.80\%$)} &\thead{\textbf{1}\\($0.20\%$)} & \thead{\textbf{1}\\($0.20\%$)} & \thead{\textbf{1}\\($0.20\%$)} & \thead{\textbf{1}\\($0.20\%$)} & \thead{\textbf{1}\\($0.20\%$)} \\
    \hline
    Computers-Random ($\rho$ = 25.50)& \thead{\textbf{4}\\($3.01\%$)} & \thead{\textbf{21}\\($15.79\%$)}  &\thead{\textbf{14}\\($10.53\%$)} & \thead{\textbf{5}\\($3.76\%$)} & \thead{\textbf{51}\\($38.35\%$)} &\thead{\textbf{3}\\($2.26\%$)} & \thead{\textbf{4}\\($3.01\%$)} & \thead{\textbf{8}\\($6.02\%$)} & \thead{\textbf{21}\\($15.79\%$)} & \thead{\textbf{2}\\($1.50\%$)} \\    
    \hline
    Flickr ($\rho\approx10.80$)& \thead{\textbf{2628}\\($5.89\%$)} & \thead{\textbf{4321}\\($9.68\%$)}  &\thead{\textbf{3164}\\($7.09\%$)} & \thead{\textbf{2431}\\($5.45\%$)} & \thead{\textbf{11525}\\($25.83\%$)} &\thead{\textbf{1742}\\($3.90\%$)} & \thead{\textbf{18814}\\($42.16\%$)} & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$ \\
    
   \bottomrule[2.0pt]
\end{tabular}}
\end{table}


\begin{table}[p]
\centering
    \setlength{\abovecaptionskip}{0pt}%    
    \setlength{\belowcaptionskip}{10pt}%
\caption{\centering Label distributions on the whole graphs}
\label{full_graph_label_distribution}
\scalebox{1.05}{\begin{tabular}{l|cccccccccc}
   \toprule[1.5pt]
    \textbf{Dataset} & $\mathcal{C}_{0}$ & $\mathcal{C}_{1}$ & $\mathcal{C}_{2}$ & $\mathcal{C}_{3}$ &$\mathcal{C}_{4}$ &$\mathcal{C}_{5}$ & $\mathcal{C}_{6}$ & $\mathcal{C}_{7}$ & $\mathcal{C}_{8}$ & $\mathcal{C}_{9}$ \\
   \hline
    Cora ($\rho\approx4.54$) &  \textbf{351}  &  \textbf{217} &  \textbf{418} &  \textbf{818} &  \textbf{426} &  \textbf{298} & \textbf{180} &$\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    \hline
    CiteSeer ($\rho\approx2.66$)& \textbf{264} & \textbf{590}  & \textbf{668} &\textbf{701} &\textbf{696} & \textbf{508} &$\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    \hline
    PubMed ($\rho\approx1.91$)&  \textbf{4103} & \textbf{7739}  & \textbf{7835}& $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$& $\textbf{\text{-}}$ & $\textbf{\text{-}}$ & $\textbf{\text{-}}$  &$\textbf{\text{-}}$ \\
    \hline
    Amazon-Computers ($\rho\approx17.73$) & \textbf{436} &\textbf{2142}  &\textbf{1414} &\textbf{542} & \textbf{5158} &\textbf{308} & \textbf{487} &  \textbf{818} & \textbf{2156}  & \textbf{291} \\
    \hline
    Flickr ($\rho\approx10.84$)&\textbf{5264} & \textbf{8506} &\textbf{6413} & \textbf{4903} &\textbf{22966} &\textbf{3479} &\textbf{37719} &$\textbf{\text{-}}$ & $\textbf{\text{-}}$ & \textbf{\text{-}}   \\
   \bottomrule[1.5pt]
\end{tabular}}
\end{table}


\subsection{Details of GNNs}
\label{The Architecture of UNREAL}

We evaluate our method with three classic GNN architectures, namely GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, and GraphSAGE \citep{hamilton2017inductive}. GNN consists of $L=1, 2, 3$ layers, and each GNN layer is followed by a BatchNorm layer (momentum=0.99) and a PRelu activation \citep{he2015delving}. For GAT, we adopt multi-head attention with 8 heads. We search for the best model on the validation set. The choices of the hidden unit size for each layer are 64, 128, and 256.

\subsection{Evaluation Protocol}
We adopt Adam \citep{kingma2014adam} optimizer with an initial learning rate of 0.01 or 0.005. We follow \citep{song2022tam} to devise a scheduler, which cuts the learning rate by half if there is no decrease in validation loss for 100 consecutive epochs. All learnable parameters in the model adopt weight decay with a rate of 0.0005. For the first training iteration, we train the model for 200 epochs using the original training set for Cora, CiteSeer, PubMed, or Amazon-Computers. For Flickr, we train for 2000 epochs in the first iteration. We train models for 2000 epochs in the rest of the iteration with the above optimizer and scheduler. The best models are selected based on validation accuracy. Early stopping is used with patience set to 300.


\subsection{Implementation details}
\label{Implementation details}
In \ourmodel, we employ the vanilla K-means algorithm as the unsupervised clustering method. The number of clusters $K$ is chosen from $\left\{100,300,500,700,900\right\}$ for Cora, CiteSeer, PubMed, and Amaon-Computers. For Flickr, $K$ is selected among $\left\{1000,2000,3000,5000\right\}$. For Cora, CiteSeer, PubMed, and Amazon-Computers, the number of training round $T$ are tuned among $\left\{40,60,80,100\right\}$. For Flickr, $T$ is tuned among $\left\{40,50,60,70\right\}$. We also introduce a hyperparameter $\alpha$, which is the upper bound on the number of nodes being added per class per round. The tuning range of $\alpha$ is $\left\{4,6,8,10\right\}$ for Cora, CiteSeer, Amazon-Computers and $\left\{64,72,80\right\}$ for PubMed. For Flickr the value of $\alpha$ is selected among $\left\{30,40,50,60\right\}$. The weight parameters $p$ in RBO is selected among $\left\{0.5,0.75,0.98\right\}$, and the threshold in DGIN is tuned among $\left\{0.25,0.5,0.75,1.00\right\}$. For Flickr, we only add minority nodes to the training set in all iterations, which means that we set $\alpha$ = 0 for majority classes in Flickr.


\subsection{Baselines}
\label{baselines}
For GraphSMOTE \citep{zhao2021graphsmote}, we use the branched algorithms whose edge predictions are discrete-valued, which have achieved superior performance over other variants in most experiments. For the ReNode  method \citep{chen2021topology}, we search hyperparameters among lower bound of cosine annealing $w_{\min } \in\{0.25,0.5,0.75\}$ and upper bound of the cosine annealing $w_{\max } \in\{1.25,1.5,1.75\}$ following \citet{chen2021topology}. PageRank teleport probability is fixed as $\alpha = 0.15$, which is the default setting in the released codes. For TAM \citep{song2022tam}, we search the best hyperparameters among the coefficient of ACM term $\alpha\in\{1.25,1.5,1.75\}$, the coefficient of ADM term  $\beta\in\{0.125, 0.25, 0.5\}$, and the minimum temperature of class-wise temperature  $\phi\in\{ 0.8, 1.2\}$ following 
\citet{song2022tam}. The sensitivity to imbalance ratio of class-wise temperature $\delta$ is fixed as 0.4 for all main experiments. Following \citep{song2022tam}, we adopt a warmup for 5 iterations since we utilize model prediction for unlabeled nodes.


\subsection{Configuration}
\label{configuration}
All the algorithms and models are implemented in Python and PyTorch Geometric. Experiments are conducted on a server with an NVIDIA 3090 GPU (24 GB memory) and an Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz.
\newpage
\section{Algorithm}
\label{PseudoCode}
\begin{algorithm}
	%\textsl{}\setstretch{1.8}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{$\textit{UNREAL}$}
	\label{alg1}
	\begin{algorithmic}[1]
            \REQUIRE Imbalanced dataset $(\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{L}_{0} ), y)$, feature matrix $\mathcal{X}$, adjacency matrix $\mathcal{A}$, unlabeled set $\mathcal{U} = \mathcal{V}- \mathcal{L}_{0}$, rounds $T$ to select nodes, the size threshold $\alpha$ of nodes being added in each class per round, weight hyperparameter $p$ of RBO, threshold $\gamma$ of DGIN, learning rate $\eta$, the size $k'$ of the clusters, GNN model $f_{\text {g}}$, clustering algorithm $f_{\text {cluster}}$, and the mean function $M(\cdot)$.

            %\ENSURE 

                      
            
            \FOR{$i=0$ to round $T$}
            \STATE  Train $f_{\text {g}}$ based on the current training set $\mathcal{L}_{i} :=\{\mathcal{C}_1,\cdots,\mathcal{C}_k \}$.
            \STATE Obtain node embedding matrix of the labeled node set and unlabeled node set $H^{L} \in \mathbb{R}^{|\mathcal{L}| \times d}$,$H^{U} \in \mathbb{R}^{|\mathcal{U}| \times d}$, prediction $\hat{y}$ and confidence $r$ from the classifier. 
        
            \STATE  $\textbf{\% Step 1: Dual Pseudo-tag Alignment Mechanism(DPAM)}$
            \STATE  $f_{\text {cluster}}({{H}}^{U})\Longrightarrow \{ \mathcal{K}_{1}, c_1, \mathcal{K}_{2}, c_2,  \cdots,  \mathcal{K}_{k'}, c_{k'}\}$
            \STATE  $c_i^{\text{train}} = M(\{h_u^{L} ~|~ y_u \in \mathcal{C}_i\})$
            \STATE  Assign a label $\tilde{y}_m$ to each cluster $\mathcal{K}_m$: $\tilde{y}_m = \mathop{\arg\min}_{j} \mathop{\mathsf{distance}} (c_j^{\text{train}}, c_m)$.
            \STATE  Combine clusters with the same pseudo-label $m$ as $\mathcal{\tilde{U}}_{m}$, and $\mathcal{U}=\bigcup_{m=1}^{k}\ \mathcal{\tilde{U}}_{m}$.
            \STATE  Put unlabeled nodes whose prediction in $\hat{y}$ is $m$ into the set $\mathcal{U}_{m}$, and $\mathcal{U}=\bigcup_{m=1}^{k} \mathcal{U}_{m}$.
          %  \STATE  Keeps unlabeled nodes whose two labels aligns: $\mathcal{Z}=\bigcup\limits_{i=m}^{k}\ (\mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m})$
            \STATE  $\textbf{\% Step 2: Node-Reordering}$
            \STATE  For each $u\in \mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m}$: $\delta_u = \mathop{\mathsf{distance}} \ (h^L_u, c^{\text{train}}_m)$.
            \STATE  Obtain geometric rankings $\{\mathcal{S}_{1}, \mathcal{S}_{2},  \cdots,  \mathcal{S}_{k} \}$ based on $\delta$; and confidence rankings $\{\mathcal{T}_{1}, \mathcal{T}_{2},  \cdots,  \mathcal{T}_{k} \}$ based on $r$.
            \STATE  For each $m$, $\mathcal{N}_{m}^{New}= \max\{r_{m}, 1-r_m\} \cdot\mathcal{S}_{m}+\min \{r_m, 1-r_m\}\cdot\mathcal{T}_{m}$.
            \STATE  Select nodes based on the rank of their values in $\mathcal{N}_{m}^{New}$.
            \STATE  $\textbf{\% Step 3:Discarding Geometrically Imbalanced Nodes (DGIN)}$
            \STATE  Obtain the distance between the embedding of $u$ and the second closest center to $u$ as $\beta_u$, compute GI index of node $u$ as $\frac{\beta_u-\delta_u}{\delta_u}$.
            \IF{$\frac{\beta_u-\delta_u}{\delta_u} \text{\textless} \gamma$}
            \STATE Discard node $u$.
            \ELSE
            \STATE Select it to the training set.
            \ENDIF

            
            \STATE  Update the training set $\mathcal{L}_{i}$.
            \ENDFOR 
	\end{algorithmic}  
        \label{Algorithm}
\end{algorithm}
\newpage


\section{Notation}
\label{Notation Table}

\begin{table}[!h]
\caption{Elaborated notation table of this paper.}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\scalebox{1.1}{\begin{tabular}{ll}
\toprule	
\underline{\textbf{\emph{Indices}}} \\
$n$ & The number of nodes,$|\mathcal{V}|$.  \\ 
$f$ & The node feature dimension. \\
$k$ & The number of different classes. \\
$k'$ & The number of cluster centers in the embedding space. \\
$d$ & The dimension of the embedding space, or the dimension of the last layer of GNNs. \\
$T$ & Rounds  to select nodes \\

\underline{\textbf{\emph{Parameters}}} \\  
$\mathcal{G}$    & An undirected and unweighted graph. \\
$\mathcal{V}$      & The node set of $\mathcal{G}$. \\
$\mathcal{E}$    & The edge set of  $\mathcal{G}$.\\
$\mathcal{X}$     & The feature matrix of $\mathcal{G}$, $\mathcal{X} \in \mathbb{R}^{n \times f}$.  \\
$\mathcal{L}$     & The set of labeled nodes of $\mathcal{G}$.  \\
$A$     & The adjacency matrix of $\mathcal{G}$, ${A} \in \{0,1\}^{n \times n}$.   \\
$\mathcal{N}(v)$   & The set of $1$-hop neighbors for node $v$.   \\
$\mathcal{U}$   & The set of unlabeled nodes, $\mathcal{U}=\mathcal{V}-\mathcal{L}$.\\
$\mathcal{C}_{i}$   & The $i$ class of the labeled sets.\\
$\rho$  & Imbalance ratio of a dataset, $\rho := {\frac{\max _{i}\left(\left|\mathcal{C}_{i}\right|\right)}{\min _{i}\left(\left|\mathcal{C}_{i}\right|\right)}}$.\\
$h_v^{l}$   & The feature of node $v$ in the $l$-th layer.\\
$e_{v,  u}$  & The edge weight between $v$ and $u$. \\
$\Phi^l$   & The parameter matrix of the $l\text {-th }$ layer.\\
$H^{L}$   & The embedding matrix of labeled nodes, $H^{L} \in \mathbb{R}^{|\mathcal{L}| \times d}$.\\
$H^{U}$   & The embedding matrix of unlabeled nodes, $H^{U} \in \mathbb{R}^{|\mathcal{U}| \times d}$.\\
$h_u^{L}$  &  The embedding of a node $u$, if $u \in \mathcal{L}$.\\
$h_u^{U}$  &  The embedding of a node $u$, if $u \in \mathcal{U}$.\\
$\mathcal{K}_{i}$  &  The $i \text {-th }$ cluster.\\
$c_i$   &  The $i$-th cluster center,the center of cluster $i \text {-th }$.\\
$\tilde{y}_i$   &  The pseudo-label  of the cluster $\mathcal{K}_i$.\\
$\mathcal{\tilde{U}}_{m}$   & The combination of clusters with the same pseudo-label $m$.\\
$\hat{y}_u$   &  The prediction of node $u$ in $\mathcal{U}$ given by GNN model.\\
$\mathcal{U}_{m}$   & The combination of unlabeled nodes whose prediction given by the GNN model is $m$.\\
$\mathcal{Z}$  &  The pool of candidate nodes after DPAM, $\mathcal{Z}=\bigcup\limits_{i=m}^{k}\ (\mathcal{\tilde{U}}_{m}\cap\mathcal{{U}}_{m})$.\\
$c^{\text{train}}_m$   &  The class center of  class $m$ in the embedding space.\\
$\mathcal{S}_{i}$   &  The sorted lists of geometric rankings.\\
$\mathcal{T}_{i}$   &  The sorted lists of confidence rankings.\\
$r_m$   &  The similarity between two rankings, $r_m = \text{RBO}(\mathcal{S}_m, \mathcal{T}_m)$.\\
$\delta_u$   &  The distance between the embedding of $u$ and the closest class center to $u$.\\
$\beta_u$    &  The distance between the embedding of $u$ and the second closest class center to $u$.\\
$\gamma$  &  Threshold  of DGI.\\
$p$  &  Weight hyperparameter of RBO.\\
$\alpha$  &  The size threshold of nodes being added in each class per round.\\
$\eta$ & Learning rate of GNN model.\\

\underline{\textbf{\emph{Functions}}}. \\ 
$m_{l}$  & The message function of MPNNs.\\
$\theta_{l}$    & The information aggregation function.\\
$\psi_{l}$   & The node feature update function.\\
$f_{\text{cluster}}$ & An unsupervised clustering algorithm for the embedding space.\\
$M(\cdot)$  & The mean function.\\
$f_{\text {g}}$  & GNN model.\\

\bottomrule
\end{tabular}}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \input{appendices/5Notation Table.tex}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
