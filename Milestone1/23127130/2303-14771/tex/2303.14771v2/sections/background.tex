\section{Related Work}\label{sec:background}
The primary goal of many CL methods is to mitigate the catastrophic forgetting phenomenon while optimizing the forward and backward knowledge transfer between tasks is seen as a secondary objective. One branch of algorithms addresses the issue of catastrophic forgetting by modifying and growing the model architecture as new tasks are observed~\cite{rusu2016progressive, aljundi2016expert, li2019learn, rosenfeld2018incremental}. Under the fixed architecture constraints, the algorithms can be divided into two categories. 
The first and more popular branch is the rehearsal methods. These methods store and re-use samples of the past tasks while observing the new ones~\cite{lopez2017gradient,chaudhry2019continual}. The second family of approaches is the regularization-based methods. These methods preserve the previously learned information by imposing penalty terms on the objective of the new tasks, including popular methods such as LwF~\cite{li2017learning} and EWC~\cite{kirkpatrick2016overcoming,chaudhry2018riemannian}, where the former imposes a knowledge distillation penalty~\cite{hinton2015distilling} on the objective of the newly observed task and the latter a quadratic penalty based on Fisher information matrix~\cite{myung2003tutorial}.

Recently, several works have considered the use of SupCon loss~\cite{khosla2020supervised} in continual learning
\cite{caccia2022new,asadi2022tackling,cha2021co2l}.
These studies have been largely focused on the combination of SupCon loss~\cite{khosla2020supervised} with replay buffers in the online setting, and do not consider the notion of class prototypes in the replay-free setting. \cite{davari2022probing} demonstrated that the use of the SupCon loss in the offline setting yields more effective ``representation forgetting" (forgetting as measured by an oracle training of a linear probe). However, a direct application  of this observation of the SupCon loss was not proposed in this prior work.

\cite{de2020continual} proposed a prototype-based evolution strategy that continually updated prototypes using a momentum update combined with taking the mean of stored exemplars. Contrary to the present work this method focused on the online setting and leveraging stored data. Furthermore, it did not exploit the efficient and stable representation properties of contrastive supervised learning. 
%This method inherently relies on storage and does not leverage 

Knowledge distillation~\cite{hinton2015distilling}, where a student network attempts to mimic the behavior of a teacher network is a popular technique in deep representation learning\cite{hinton2015distilling,tian2019contrastive,zhu2021complementary}, often used to reduce model size.
%\reza{Add a paragraph discussing similarity and differences of our work vs. \cite{de2021continual} and 
Classical distillation techniques have been applied in various contexts in CL. \cite{rebuffi2017icarl,javed2018revisiting} utilized a distillation loss alongside replayed examples, constraining the current model to give similar outputs. 
\cite{barletti2022contrastive} recently proposed to use a triplet loss alongside a contrastive distillation term. Here samples are constrained to have similar distances under current and previous models. By contrast, we apply a relation distillation that constrains the relative distances of old class prototypes to samples to be preserved.  

Another application of classical distillation techniques closely related to our work \cite{wu2021striking} proposed a method that does not require storage of prior task data. This approach relies on constraining the distance between embeddings of the old and new models combined with a cross-entropy term. The constraint here can be analogous to a traditional distillation term, while our approach focuses on relation distillation to prototypes. \cite{wu2021striking} also utilizes a self-supervised learning objective based on the rotation of images to enhance the representation learning similar to \cite{zhu2021prototype}. 

Relation distillation has recently been used in teacher-student methods \cite{park2019relational}. Unlike conventional knowledge distillation techniques that attempt to make student network representations similar to teacher networks, relation distillation maintains relative distances between a set of points. In the present work, we apply a related idea in the context of continual learning, maintaining relative relationships between prototypes and current task samples. 


