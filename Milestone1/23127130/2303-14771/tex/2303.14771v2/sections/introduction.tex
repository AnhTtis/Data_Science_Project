\section{Introduction}\label{sec:intro}
%Parag 1
%Introduction to the continual learning problem
%Challenges of offline setting.
%Limitations of approaches whihc require past task data
\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{assets/SupCon_Distill_V2.pdf}
    \caption{Illustration of our Prototype-Sample Relation Distillation (PRD). For each prior task prototype, we preserve the relative ordering of samples in the mini-batch. This allows the  representation to adapt to new tasks while maintaining relevant positions of past prototypes. Illustrated for the prototype in orange 4 samples in the minibatch are ranked 1 through 4 based on similarity. PRD attempts to preserve this ranking while learning the new task.\vspace{-20pt}}
    \label{fig:teaser}
\end{figure}
%Can rewrite this intro
Continual Learning (CL) aims to continuously acquire knowledge from an ever-changing stream of data. The goal of the learner is to continuously incorporate new information from the data stream while retaining previously acquired knowledge. Performance decay of the system on the older samples due to the loss of previously acquired knowledge is referred to as catastrophic forgetting~\cite{mccloskey1989catastrophic}, which represents a great challenge in CL. Thus CL algorithms are typically designed to control for catastrophic forgetting while observing additional restrictions such as memory and computation constraints.

% Continual Learning (CL) aims to develop methods for learning under a changing and continous stream of data and tasks. Continual learning systems are typically designed to allow both knowledge transfer from past tasks to future tasks, without forgetting previous tasks.  The latter leads to a commonly observed problem in deep neural networks: catastrophic forgetting \cite{mccloskey1989catastrophic}.   

%Introduction to continual learning and 

%Parag 2
Some of the early work in modern continual learning such as LwF~\cite{li2017learning} and EWC~\cite{kirkpatrick2016overcoming} propose solutions that do not require storage of past data. However, in complex settings with long task sequences, these techniques tend to under-perform by a wide margin compared to the idealized joint training \cite{chaudhry2019continual}. Many recent high-performing approaches in this area maintain existing samples in some form of buffer, allowing them to be reused for distillation\cite{rebuffi2017icarl}, replay \cite{chaudhry2019continual,caccia2020online}, or as part of gradient alignment constraints~\cite{lopez2017gradient,chaudhry2018efficient}.
These approaches have been shown to be more efficient and have become a predominant approach for many state-of-the-art continual learning systems~\cite{SODA}. On the other hand in many cases when training on a new task it may be prohibited to store prior data. For example, prior task data may be sensitive (e.g. medical data) or it may consist of proprietary data that is not aimed for release. Moreover, methods relying on prior task data tend to grow the storage with the number of tasks~\cite{chaudhry2018efficient,caccia2020online}, which can be prohibitive under severe storage constraints. Thus developing methods that can match or exceed the efficiency of data-storage-based methods is of great importance. 

Recently~\cite{davari2022probing} observed that for many continual learning tasks, the representational power of deep networks trained with naive fine-tuning can remain remarkably efficient for representing both new and old task data. In particular, it was observed that when performing continual learning with the Supervised Contrastive Loss~\cite{khosla2020supervised} and no CL constraints, the efficiency of representations on old task data tends to match that of complex CL data-storage methods. These observations relied on an oracle measure of the deep representations and did not provide a practical solution. In order to link the powerful representation learning to the effective prediction of prior class data we can consider alternatives for making the final prediction. An approach previously taken in the continual learning literature is to use the notion of class prototypes \cite{de2021continual}, vector representations whose similarity to new sample representation can give predictions of the target class. If we take the observation that representations of old classes are already well separated \cite{davari2022probing} then an efficient continual learner can be obtained by simply maintaining correct estimates of past class prototypes.

In this work, we propose an effective mechanism to not only maintain relevant class prototypes but also leverage the knowledge embedded in these prototypes to further reduce representation forgetting. We combine contrastive representation learning with a prototype-based classifier. The new class prototypes are learned such that no direct negative influence is incurred on previous prototypes. Then, a novel loss formulation based on the relative similarity of new task data to old class prototypes is deployed to maintain the relevancy of old class prototypes while encouraging the learned representation to remain effective for old tasks.
% In this work we propose an effective way to maintain relevant class prototype positions based on avoiding changes in the relative similarity of new task data and old class prototypes. This allows us to leverage the powerful supervised contrastive loss for learning representations while maintaining a clear link to prototypes capable of providing predictions for past tasks. 
Our approach is illustrated in Fig.~\ref{fig:teaser}.
Our proposed method, Prototype-Sample Relation Distillation (PRD), maintains the relative relation of each prototype, by minimizing changes  in the softmax distribution over samples. This effectively allows representations to adapt to new classes while keeping prototypes from old classes relevant. 
% 
We now summarize our \textit{overall contributions} in this work:
\begin{itemize}
\itemsep0em 
    \item We propose a \textit{novel} CL method, PRD, that \textit{does not rely} on prior data storage during training or inference. 
    \item In a variety of challenging settings (task and class-incremental), datasets (SplitMiniImagenet~\cite{vinyals2016matching}, SplitCIFAR100~\cite{krizhevsky2009learning}, Imagenet-32~\cite{davari2022probing}), and task sequence lengths (20  to 200), we demonstrate that PRD leads to  large improvements over both replay-based and replay-free methods.
    \item Throughout several experiments, we demonstrate that our method not only achieves strong control of forgetting of previously observed tasks but also leads to improved plasticity in learning new tasks.
    
\end{itemize}
% We proposea  novel method that does not rely on any having any data from prior tasks at training or inference time
% In a variety of challenging settings (task, class, domain incremental), datasets (X,Y,Z), and task sequences (20  to 200) we demonstrate that this approach leads to  large improvements over both methods that store data and ones that do not
% We demonstrate that our method not only achieves strong control of forgetting it leads to better better transfer learning and plasticity.
In the following section, Sec.~\ref{sec:background}, we summarize the related work and then describe the essence of our proposed solution, Sec.~\ref{sec:method}. We demonstrate the effectiveness of our approach in Sec.~\ref{sec:experiments}, and conclude our work in Sec.~\ref{sec:conclusion}. 
\footnote{The code for our experiments is available at \href{https://github.com/naderAsadi/CLHive}{https://github.com/naderAsadi/CLHive}.}
