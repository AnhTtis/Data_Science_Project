
\begin{figure*}[t!]
  % \centering
    \begin{minipage}{0.49\textwidth}
    \includegraphics[width=\textwidth]{assets/task_inc/multihead_cifar100_T20.pdf}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
    \includegraphics[width=1.0\textwidth]{assets/task_inc/multihead_miniimagenet_T20.pdf}
    \end{minipage}\hfill
  % \caption{Minimagenet Single Head Evaluation. M=100, N_iters=1, Runs=15}
  \caption{\textit{Task-incremental} accuracy on 20-Task Split-CIFAR100(left) and Split-MiniImageNet(right). We observe that PRD widely outperforms other baselines without storing any previous task data, and as well exceeds the performance of ER with a very large buffer.\vspace{-10pt}}
    \label{fig:task_inc}
\end{figure*}

\section{Experiments}\label{sec:experiments}
In this section, we evaluate our proposed method on a wide range of challenging CL settings. In Sec.~\ref{sec:exp-task}, we focus on the \textit{task-incremental} (multi-head) setting, where we compare our method with other replay-free methods. Sec.~\ref{sec:class-inc} is dedicated to \textit{class-incremental} setting, where the shared output layer poses an enormous challenge that drives most work to employ a replay buffer, unlike our replay-free solution. The evaluations are based on \textit{average observed accuracy}. Specifically, we measure observed accuracy, $A_{ij}$, as the accuracy of the model after step $i$ on the test data of task $j$. Similarly, the average observed accuracy at the end of the sequence is $\frac{1}{T}\sum_{t\in T}A_{T,t}$ as used in ~\cite{li2017learning}. 
%Benchmarks are evaluated in both multi-head (corresponding to \textit{task-incremental})  and single-head (corresponding to \textit{class-incremental}) settings. 
%\textit{i.e.} 
%task descriptors are provided to the model at test time in the former setting and not provided in the later. 

\paragraph{Datasets}
In our experiments, we use Split-CIFAR100~\cite{krizhevsky2009learning}, Split-MiniImageNet, and ImageNet32~\cite{chrabaszcz2017downsampled,davari2022probing} as the benchmarks for both multi-head and single-head settings.
Split-CIFAR100~\cite{krizhevsky2009learning} comprises 20 tasks, each containing a disjoint set of 5 labels. The classes splits are constructed as in \cite{chaudhry2019continual}. All CIFAR experiments process $32 \times32$ images.
Split-MiniImageNet divides the MiniImagenet dataset into 20 disjoint tasks of 5 labels each. Images are $84 \times 84$.
ImageNet32~\cite{chrabaszcz2017downsampled} is a downsampled ($32 \times32$ ) version of the entire ImageNet~\cite{deng2009imagenet} dataset split into 200 tasks of 5 classes each. We use ImageNet32  in order to compare methods performance in very long  sequences scenario.

\paragraph{Baselines}
Although our proposed method does not use any replay buffer, we consider in our evaluation both \textit{replay-free} and \textit{replay-based} methods, as replay-based have been shown to outperform other approaches in the continual learning setting~\cite{chaudhry2019continual, aljundi2019online, ji2020automatic, rebuffi2017icarl}. We consider the following replay-free methods in our evaluations:

\textbf{LwF}~\cite{li2017learning}:  knowledge distillation based on current task data is used to limit forgetting.\\
%  to It aims to match the softmax output of the network of previous models on current data.
\textbf{EWC}~\cite{huszar2017quadratic}: estimates an importance value for each parameter in the network and penalizes changes on parameters deemed important for previous tasks.\\% Keeps the network parameters close to the optimal parameters for the previous task while training the current task.
\textbf{SPB\cite{wu2021striking}}: A recent method that also utilizes contrastive learning and does not rely on replay data. We were unable to effectively reproduce their results  since the code is not provided. However, we compare our approach directly to the reported results in the setting studied in the original work~\cite{wu2021striking} in Sec.~\ref{sec:class-inc}.\\
\textbf{iid}: The learner is trained on the whole data, in a single task containing all the classes.

\noindent The incorporated replay-based baselines are as follows:

\textbf{ER}~\cite{chaudhry2019continual}:  Experience Replay with a buffer of a fixed size. In our experiments, we used buffer sizes of 5, 20, and 50 samples per class based on the evaluation setting. Note this is a very strong baseline that exceeds most methods, particularly with large buffers (50 samples) \cite{davari2022probing}.\\
\textbf{iCaRL}~\cite{rebuffi2017icarl}: A distillation loss alongside binary cross-entropy loss is used during training. Samples are classified based on the closest class prototypes.\\
\textbf{ER-AML}~\cite{caccia2022new}: Utilizes SupCon loss, alongside a replay buffer, to reduce the representation drift of previously observed classes. \\
\textbf{ER-ACE}~\cite{caccia2022new}: Similar to ER-AML, however, ER-ACE  introduces a modified version of the standard softmax-crossentropy. 


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{assets/task_inc/multihead_imagenet32_T200.pdf}
    \caption{\textit{Task-incremental} accuracy on 200-Task ImageNet32. On this long sequence, PRD matches a baseline with a large replay buffer. Other methods degrade over time, but the average accuracy of PRD improves due to the cumulative effect of maintaining better plasticity.}
    \label{fig:task_inc_imagenet32}
\end{figure*}



\begin{figure*}[t!]
  % \centering
    \begin{minipage}{0.49\textwidth}
    \includegraphics[width=\textwidth]{assets/class_inc/singlehead_cifar100_T20.pdf}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
    \includegraphics[width=1.0\textwidth]{assets/class_inc/singlehead_miniimagenet_T20.pdf}
    \end{minipage}\hfill
  \caption{\textit{Class-incremental} accuracy on 20-Task Split-CIFAR100(left) and Split-MiniImageNet(right). We observe that PRD outperforms not only other \textit{replay-free} baselines but also ER, M=5, and is on par with ER, M=20, without storing any data. We also observe that with additional replay samples, PRD M=50 outperforms ER M=50 with the same number of replay samples.\vspace{-10pt}}
    \label{fig:class_inc}
\end{figure*}

\paragraph{Hyperparameter selection}
For each method, optimal hyperparameters were selected via a grid search performed on the validation set. The selection process was done on a per-dataset basis, that is we picked the configuration which maximized the accuracy averaged over different settings. 
We found that for our method, the same hyperparameter configuration worked well across all settings and datasets. All necessary details to reproduce our experiments can be found in the supplementary materials. 

\subsection{Evaluations on Task-Incremental Setting}\label{sec:exp-task}
We evaluate Split-CIFAR100, Split-MiniImagenet, and ImageNet32 using the protocol from \cite{aljundi2019online} with 100 training epochs training per task. We report the mean and standard error over 3 runs.

\paragraph{Split-CIFAR100 and Split-MiniImageNet}
We consider  Split-CIFAR100 and Split-MiniImageNet with 20 tasks of 5 classes each. 
The results can be found in \cref{fig:task_inc}, for Split-CIFAR100 and Split-MiniImageNet, using different buffer sizes for ER. In this setting, we can observe that our proposed method \textit{consistently outperforms} other methods by a significant margin.
Even though our method does not utilize previous tasks' data in any form, it still outperforms ER with $50$ replay samples per class, nearly closing the gap with the oracle \textit{iid} setting. 
From \cref{fig:task_inc}, we can observe that during the whole continual sequence of tasks, the average accuracy of our method on the observed classes remains relatively similar and even increases at several points during the sequence, \textit{e.g.} $12^{th}$ task, suggesting a good trade-off between stability and plasticity of the model. 

Although our method, in terms of average observed accuracy, outperforms other baselines with a considerable margin, a little higher forgetting rate can be observed compared to the strong replay-based baseline, \textit{i.e.} ER with 50 replay samples.
In \cref{sec:forward_transfer}, we  show that our proposed method, in terms of plasticity, \textit{i.e.} ability to learn new tasks, is comparable to naive fine-tuning, which is the upper bound among existing CL methods due to the absence of constraints on preserving previous tasks ( i.e., lacking stability).
This suggests that PRD is able to preserve previous tasks' information without losing the ability to learn new tasks.

% \setlength{\tabcolsep}{2pt}
\begin{table*}[t!]
\small
  \centering
  \begin{tabular}{ll}
  \begin{tabular}{l cccc}
    \toprule
    \multirow{2}{*}{Method} &   \multicolumn{4}{c}{Split-CIFAR100, K = 20 } \\
    
    & $M = 0$ & $M = 5$ & $M = 20$ & $M = 50$\\
    \midrule

    iid & 65.3 & 65.3 & 65.3 & 65.3 \\
    Fine-tuning & 4.6 & 4.6 & 4.6 & 4.6 \\
    
    \midrule
    
    ER~\cite{chaudhry2019continual} & - & 15.2\tiny{$\pm$0.7} & 29.6\tiny{$\pm$0.8} & 38.3\tiny{$\pm$0.9} \\
    iCaRL~\cite{rebuffi2017icarl} &  - &  19.8\tiny{$\pm$0.5} &  28.6\tiny{$\pm$0.7} & 32.9\tiny{$\pm$0.5}\\
    ER-AML~\cite{caccia2022new} &  - & 21.4\tiny{$\pm$0.8}  & 35.3\tiny{$\pm$0.6}  & 42.4\tiny{$\pm$0.8}\\
    ER-ACE~\cite{caccia2022new} & -  &  22.8\tiny{$\pm$0.5} &  35.7\tiny{$\pm$0.2} & 43.3\tiny{$\pm$0.2}\\
    
    PRD(Ours) & \textbf{27.8}\tiny{$\pm$0.2} &  \textbf{32.0}\tiny{$\pm$0.4} &  \textbf{39.5}\tiny{$\pm$0.4} &  \textbf{45.1}\tiny{$\pm$0.5} \\

   \bottomrule
  \end{tabular}
  
  \begin{tabular}{cccc}
  
    \toprule
    \multicolumn{4}{c}{Split-MiniImageNet, K = 20}\\
    
    $M = 0$ & $M = 5$ & $M = 20$ & $M = 50$\\
    \midrule

     54.5  & 54.5 & 54.5 & 54.5 \\
     4.4 & 4.4 & 4.4 &  4.4 \\
      
    \midrule
      
     - &  13.4\tiny{$\pm$0.2} &  21.7\tiny{$\pm$0.8} &  28.7\tiny{$\pm$0.5} \\
     - &  16.2\tiny{$\pm$0.1} &  22.8\tiny{$\pm$0.3} &  26.1\tiny{$\pm$0.2} \\
     - &  17.1\tiny{$\pm$0.3} &  26.3\tiny{$\pm$0.7} &  32.3\tiny{$\pm$0.2} \\
     - &  18.8\tiny{$\pm$0.1} &  27.1\tiny{$\pm$0.5} &  34.2\tiny{$\pm$0.5} \\
     
     \textbf{20.0}\tiny{$\pm$0.1} &  \textbf{25.7}\tiny{$\pm$0.3} &  \textbf{31.3}\tiny{$\pm$0.5} &  \textbf{35.8}\tiny{$\pm$0.4} \\

   \bottomrule
  \end{tabular}
  
  \end{tabular}
    \caption{\textit{Class-incremental} results on 20-Task Split-CIFAR100 and Split-MiniImageNet datasets using different buffer sizes. We observe that even with no replay samples (M=0) PRD outperforms all of the replay-based baselines with 5 replay samples. With a small number of replay samples, \textit{e.g.} M=5, PRD widely outperforms other replay-based methods, suggesting the ability of our method to utilize replay samples while maintaining good performance with no access to prior data.\vspace{-10pt}}
  \label{tab:class_inc_replay}
\end{table*}


\paragraph{ImageNet32 - Long Task Sequence}
We now consider a longer sequence than typically studied which allows us to observe whether the trends we have seen so far continue to hold. Using Imagenet32 we construct  200 tasks of 5 classes each. \cref{fig:task_inc_imagenet32} shows the average observed accuracy throughout the whole 200 tasks sequence. We see that in a very long sequence of tasks, the previously established observations about our method hold. Specifically, we see that as the model reaches the later stages of the sequence, our method outperforms the competitive baseline of ER with 50 replay samples \textit{without} utilizing previous tasks' data in any form. Note that the number of stored data points leveraged by ER increases as we proceed in the sequence. Furthermore, we observe as in the previous section that the average observed accuracy of our proposed method not only stays relatively the same during the beginning but also starts increasing as the model reaches the middle of the sequence, \textit{i.e.} the  $90^{th}$ task.
This observation suggests that our method is able to efficiently learn new tasks' features while preserving the previous tasks' information. 



\subsection{Class-Incremental Setting}\label{sec:class-inc}
In addition to the experiments in the \textit{task-incremental} setting, to further verify the effectiveness of our method in mitigating representation forgetting  with no access to prior task data, we also evaluate on the more challenging \textit{class-incremental} setting where we examine the ability to incrementally learn  a shared classifier.  Here as well we report  the mean and standard error over 3 runs.
%Anytime plots per task for bunch of baselines we implement
% LWF, EWC , E versions as well
% 1 replay baseline 
%CIFAR-100
%MiniImagenet
%imagenet32
\paragraph{Split-CIFAR100 and Split-MiniImageNet}
\cref{fig:class_inc} shows the average observed \textit{class-incremental} accuracy of the model over the 20 task sequence of Split-CIFAR100 and Split-MiniImageNet.
Note that the \textit{replay-based} methods are plotted in dashed lines.
We can see that our method, with no access to previous tasks data, not only outperforms other \textit{replay-free} methods but also beats ER with 5 replay samples and is on par with ER with 20 replay samples per class.
From \cref{fig:class_inc}, we can observe that the average accuracy of our method drops initially, probably due to the drift of the old tasks' prototypical features, but stays relatively the same from the middle of the sequence. This observation also suggests that in longer tasks sequences the learned prototypical features of old classes remain useful, even in the absence of any replay data.\\
In the following section, \cref{sec:class_inc_replay}, we perform a thorough experiment on the effect of different replay buffer sizes, showing that our method beats the state-of-the-art replay-based methods with fewer stored samples.

%~\cite{li2017learning, yu2020semantic}
%~\cite{huszar2017quadratic, yu2020semantic}
%~\cite{aljundi2018memory, yu2020semantic}
% \setlength{\tabcolsep}{2pt},~\cite{wu2021striking}
\begin{table}[t!]
\small
  \centering
  \begin{tabular}{l cc cc}
    \toprule
    \multirow{2}{*}{Method} &   \multicolumn{2}{c}{Split-CIFAR100} &  \multicolumn{2}{c}{ImageNet-Sub}\\
    
    &   K=6 & K=11 & K=6 & K=11\\
    \midrule
    iid & 73.4 & 73.2 & 82.0 & 82.7\\
    Fine-tuning & 22.3 & 12.6 & 23.6 & 13.2\\
    \midrule
 %  LwF-E~\cite{li2017learning, yu2020semantic} & 57.0 & 56.8 & 65.5 & 65.6\\
  %  EWC-E~\cite{huszar2017quadratic, yu2020semantic} & 56.3 & 55.4 & 65.2 & 64.1\\
   % MAS-E~\cite{aljundi2018memory, yu2020semantic} & 56.9 & 56.6 & 65.8 & 65.8\\
    %SDC~\cite{yu2020semantic} & 57.1 & 56.8 & 65.6 & 65.7\\
    \midrule
    %SPB~\cite{wu2021striking} & 60.9 & 60.4 & 68.7 & 67.2\\
        LwF-E~\cite{yu2020semantic} & 57.0 & 56.8 & 65.5 & 65.6\\
    EWC-E~\cite{yu2020semantic} & 56.3 & 55.4 & 65.2 & 64.1\\
    MAS-E~\cite{ yu2020semantic} & 56.9 & 56.6 & 65.8 & 65.8\\
    SDC~\cite{yu2020semantic} & 57.1 & 56.8 & 65.6 & 65.7\\
    \midrule
    SPB~\cite{wu2021striking} & 60.9 & 60.4 & 68.7 & 67.2\\
    % SPB-I~\cite{wu2021striking} & 62.6 & 62.7 & 70.1 & 69.8 \\
    % SPB-M~\cite{wu2021striking} & 65.5 & 65.2 & 71.7 & 70.6\\

    PRD (Ours) & \textbf{64.3} & \textbf{63.7} & \textbf{71.8} & \textbf{70.3}\\
   \bottomrule
  \end{tabular}
  \caption{Pre-trained Initialization. We report average \textit{cumulative} incremental accuracies over all tasks. PRD exceeds recent proposals in this challenging setting.\vspace{-10pt}}
  \label{tab:half_iid}
\end{table}


% Experiments with Replay Buffer
\paragraph{Leveraging stored samples}\label{sec:class_inc_replay}%\EUGENE{Can you help with some initial writing for this section?}
Our method targets  incremental learning in scenarios where no stored samples are allowed. However, here, we investigate if our  method can benefit from the availability of few stored samples. \\
%These could be some selected samples on which privacy issues are removed (e.g., modified images, or virtually generated samples). 
When utilizing replay data we follow the standard approach of ER-based methods, sampling half the training data of the mini-batch from the previous data. The subsequent optimization problem is kept the same. Note that now the relation distillation will also see data from past tasks that directly correspond to the prototypes.  
\cref{tab:class_inc_replay} compares our method with different buffer sizes to other replay-based methods. It can be seen that our method can successfully leverage the available data and further improve the performance achieving high gains over state of art in low buffer regime. This suggests our method is highly effective in both limited and no replay data settings.% solution on scenarios where access to stored samples is limited or restricted. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ablations/imagenet32_probe.pdf}
    \caption{Task 1 LP accuracy over 200-Task ImageNet32. We compare Linear probe accuracy for Tasks 1 data over the whole sequence. We can observe that during a long sequence, the performance of our method, \textit{i.e.} PRD, not only stays relatively flat, but also increases at some points in the later stages of  the sequence, suggesting its ability to preserve the information of observed tasks.\vspace{-10pt}}
    \label{fig:imagenet32_probe}
\end{figure*}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/ablations/forward_transfer.pdf}
    \caption{\textit{Task-incremental} Split-CIFAR100. Accuracy on the current task(left) and average accuracy on previous tasks(right).  PRD performs well on the current task while having low forgetting.\vspace{-10pt}}
    \label{fig:forward_transfer}
\end{figure}

\paragraph{Pre-trained Initialization}
To further measure the class-incremental performance of our method and allow direct comparison to \cite{wu2021striking}, we also evaluate our method on Split-CIFAR100~\cite{krizhevsky2009learning} and ImageNet-Subset~\cite{rebuffi2017icarl, krizhevsky2012imagenet} using the protocol and constraints from~\cite{wu2021striking}. In these settings half the classes are used for an initial pre-training phase.
ImageNet-Subset contains 100 classes randomly sampled from ImageNet. Following~\cite{wu2021striking}, we randomly select the first 50 classes as the 1-\textit{st} phase and evenly split the remaining 50 classes for \textit{K}-1 phases. 
Similar to \cite{wu2021striking}, for this experiment, we evaluate our models with \textit{K}=6 and 11 phases on both Split-CIFAR100 and ImageNet-Subset datasets, \textit{i.e.}, after the $1^{st}$ phase, we incrementally add 5 or 10 new classes at each phase. 
Following \cite{wu2021striking}, we report the average \textit{cumulative} incremental accuracy over all phases. All results are averaged over three runs.

\cref{tab:half_iid} shows average \textit{cumulative} incremental accuracies (as used in \cite{wu2021striking}) over all phases on Split-CIFAR100 and ImageNet-Subset. We observe that our method exceeds the recent proposal of \cite{wu2021striking} in this setting as well as beating strong baselines such as SDC. Note that \cite{wu2021striking} also applied a self-supervised objective which we do not include as we were unable to obtain source code for these experiments, and this was a complementary approach that can enhance our method as well.  

\subsection{Analysis and Ablations}

% Anytime plots for 3 different tasks to show positive forward transfer effect
\paragraph{PRD Balances Plasticity and Stability}\label{sec:forward_transfer} % High Plasticity
A continual learner should be able to easily integrate new knowledge from new tasks (plasticity) while benefiting from prior knowledge to improve performance on the current task (forward transfer). Continual learning methods are often characterized by a trade-off in plasticity and stability. Stability refers to the ability to retain the knowledge of prior tasks\cite{mermillod2013stability}, often measured by the forgetting metric. We have thus far shown that PRD has relatively low forgetting, for example in Fig.~\ref{fig:task_inc}, for CIFAR-100 it has the lowest forgetting, only slightly improved on by the ER-M50 a baseline with large replay buffer. Both ER-M50 and PRD have good stability, but their plasticity can be difficult to gauge in long task sequences directly from observed accuracy. For example, a task can be very poorly learned during a session but learned later on thanks to the replay buffer. We thus directly compare the current task performance separately from the old task performance corresponding to PRD, ER M=50, and EWC on task-incremental CIFAR100, corresponding to the results in Fig.~\ref{fig:task_inc}. The results are shown for tasks 5,10, and 15 in Fig.~\ref{fig:forward_transfer}.  

We first observe that all methods progressively degrade in current task accuracy. Since tasks are sampled uniformly from the set of possible tasks we can assume this corresponds to a gradual reduction in plasticity. This is consistent with many previous observations of continual learning systems\cite{dohare2021continual,mermillod2013stability}. On the other hand, we observe that compared to other strong baselines like EWC and ER, M=50, the current task accuracy of PRD is substantially higher, while the old task accuracy is as well, being largely maintained as training progresses. Thus PRD provides a strong tradeoff in plasticity and stability. If we observe the behavior of ER, M=50, we see that its old task accuracy is sometimes increasing (for example at task 10). Overall, we can state that models trained by our method, PRD, exhibit  a  plasticity  close to the constraint free fine-tuning while showing the best stability. 
%Since ER-M50 maintains a replay buffer its possible that it can perform poorly on the current task, while still obtaining a good average accuracy. 
%\EUGENE{Once the final figure is made this will need modification}
%\EUGENE{If we dont have random init baseline remove all mention of Forward Transfer from the above}
% Probe Acc. on ImageNet32 200T sequence

\paragraph{Representation Forgetting}\label{sec:probe}
Following~\cite{davari2022probing}, we evaluate the representation forgetting of our method against other baselines with a \textit{Linear Probe}, denoted as LP.
Similar to the definition of observed accuracy in \cref{sec:experiments}, we can measure the LP accuracy for each step $i$ and task $j$ as well as the average LP accuracy.   

Similar to \cite{davari2022probing}, we construct 200 tasks of 5 classes using ImageNet32 dataset in the task-incremental setting.
\cref{fig:imagenet32_probe} shows the performance of the model on the first task throughout the whole continual sequence. We observe that although the naive SupCon exceeds replay with M=5 in terms of representation forgetting on the first task, PRD provides a very substantial improvement. This suggests that we not only benefit from stabilizing the prototypes but the representation itself greatly benefits from PRD, avoiding forgetting at the representation level.