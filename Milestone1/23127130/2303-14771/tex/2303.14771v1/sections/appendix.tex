\onecolumn
\section*{APPENDIX}

\section{Experimental Setup}
In this section, we provide additional details regarding the baselines and hyperparameters. In all experiments, we leave the \textbf{batch size} and the \textbf{number of epochs} fixed at \textbf{128} and \textbf{100}. The model architecture ($\theta$) is also kept constant, which is a regular ResNet-18 model, where the dimensions of the last linear layer change depending on the input height and width.

The augmentation pipeline is consistent across all experiments, consisting of random crop, random horizontal flip, color jitter of 0.4, and random grayscale.

\paragraph{Hyperparameter Selection}
All results in the paper have been either implemented by us or adapted from \cite{caccia2022new}, with the exception of SPB~\cite{wu2021striking}, where results were taken from the original paper since there was no public codebase for that baseline at the time of submission. For each method, a grid search was run on the possible hparams, which we detail below. In the following, we list the hyperparameters that we included in our grid search. The best values for each parameter are underlined. 

\noindent\textbf{PRD(ours)}
\begin{itemize}
    \setlength\itemsep{0.5pt}
    \item LR: [\underline{0.01}, 0.005, 0.001]
    \item SupCon Temperature: [\underline{0.1}, 0.2, 0.3]
    \item Relation Distillation Coefficient($\beta$): [1., 2., \underline{4.}, 8., \underline{16.}]
    \item Prototypes Coefficient($\alpha$): [1., \underline{2.}, 4.]
\end{itemize}

\noindent\textbf{EWC}~\cite{huszar2017quadratic}:
\begin{itemize}
    \setlength\itemsep{0.5pt}
    \item LR: [0.01, \underline{0.005}, 0.001]
    \item Lambda Coefficient: [20, 50, 100, 200, 500, 1000]
\end{itemize}

\noindent\textbf{LwF}~\cite{li2017learning}, \textbf{ER}~\cite{chaudhry2019continual}, and \textbf{iCaRL}~\cite{rebuffi2017icarl}:
\begin{itemize}
    \setlength\itemsep{0.5pt}
    \item LR: [0.01, \underline{0.005}, 0.001]
\end{itemize}
Similar to \cite{caccia2022new}, for ER, rehearsal begins as soon as the buffer is not empty. Also when samples are being fetched from the buffer, we do not exclude classes from the current task.

\noindent\textbf{ER-ACE}~\cite{caccia2022new}:
\begin{itemize}
    \setlength\itemsep{0.5pt}
    \item LR: [\underline{0.01}, 0.005, 0.001]
\end{itemize}
Following \cite{caccia2022new} implementation, for the masking loss, we simply use \texttt{logits.maskedfill(mask, -1e9)} to filter out classes which should not receive gradient. 


\noindent\textbf{ER-AML}~\cite{caccia2022new}:
\begin{itemize}
    \setlength\itemsep{0.5pt}
    \item LR: [\underline{0.01}, 0.005, 0.001]
    \item SupCon Temperature: [\underline{0.1}, 0.2, \underline{0.3}]
\end{itemize}



\section{Ablation on Prototypes-Samples Similarity Distillation}

As discussed in \cref{sec:forward_transfer}, continual learning methods are characterized by a trade-off in plasticity and stability. Stability refers to the ability to retain the knowledge of prior tasks\cite{mermillod2013stability}, often measured by the forgetting metric. 
According to our observations from \cref{sec:forward_transfer}, one can observe the PRD has relatively low forgetting while maintaining high plasticity in learning new tasks.
PRD controls the stability-plasticity trade-off mostly using the coefficient for \textit{prototype-sample relation distillation} loss.
Here we do an ablation on the effect of our prototype-sample relation distillation loss in three datasets, Split-CIFAR100, Split-MiniImageNet, and ImageNet32.
\cref{tab:prd_ablation} presents the performance of our method with different \textit{coefficient} values($\beta$) for our prototype-sample relation distillation loss. 

We can observe that using a coefficient value of 0~($\beta=0$), \textit{i.e.} having no relation distillation loss, Eq.~\eqref{eq:overall_loss}, results in very low average accuracy for all of the three datasets. This observation shows the importance of relation distillation loss in remembering old tasks' information.
Further, we can observe using different values of $\beta$, with shorter task sequences like 20 task Split-CIFAR100, does not affect the overall average performance of the model across all tasks.
On the other hand, with long task sequences, \textit{e.g.} 200 task ImageNet32, a higher coefficient value for the distillation loss results in less forgetting and better overall average accuracy.

\begin{table}[h!]
\small
  \centering
  \begin{tabular}{l ccc}
    \toprule
     &   \multicolumn{3}{c}{Dataset}\\
    
    Distillation &   Split-CIFAR100 & Split-MiniImageNet & ImageNet32 \\
    
    Coefficient($\beta$) & (K=20) & (K=20) & (K=200) \\\midrule
    
    $\beta = 0.$ & 39.4\tiny{$\pm1.5$} & 31.2\tiny{$\pm0.9$} &  21.3\tiny{$\pm0.8$}\\
    
    $\beta = 1.$ & 80.0\tiny{$\pm0.5$} &  59.3\tiny{$\pm0.3$}&  55.4\tiny{$\pm0.4$}\\
    
    $\beta = 2.$ & 82.1\tiny{$\pm0.3$} &  63.7\tiny{$\pm0.3$}&  59.2\tiny{$\pm0.4$}\\
    
    $\beta = 4.$ & \textbf{83.5}\tiny{$\pm0.4$} &  \underline{68.3}\tiny{$\pm0.4$}&  62.7\tiny{$\pm0.2$}\\
    
    $\beta = 8.$ & \underline{83.1}\tiny{$\pm0.4$} & \textbf{70.9}\tiny{$\pm0.5$} &  \underline{65.1}\tiny{$\pm0.3$}\\
    
    $\beta = 16.$ & 82.7\tiny{$\pm0.2$} & 67.2\tiny{$\pm0.5$} &  \textbf{67.5}\tiny{$\pm0.2$}\\
    
   \bottomrule
  \end{tabular}
  \caption{Ablation study on the effect of relation distillation coefficient, $\beta$ in Eq. \eqref{eq:overall_loss}. The reported numbers are \textit{Task-incremental} average observed accuracy. We can observe that $\beta=0$, having no distillation, results in very low average accuracy over all tasks. On the other hand, when the sequence is very long, \textit{e.g.} 200 task ImageNet32, a higher coefficient value for the distillation loss results in better overall average accuracy.}
  \label{tab:prd_ablation}
\end{table}



\section{Ablation on Prototype Learning without Contrasts}

PRD uses class prototypes to score a sample's representation with respect to each class. \cref{tab:prd_ablation_alpha} presents the performance of our method with different \textit{coefficient values($\alpha$)} for prototypes learning loss ($\mathcal{L}_{p}$). When the corresponding coefficient value is set to 0 ($\alpha=0$), which means no optimization for class prototypes in Eq.~\eqref{eq:overall_loss}, the average accuracy across all three datasets is basically random. 
When different values of $\alpha$ are used, there is no significant effect on the overall average performance of the model for both shorter and longer task sequences.

\begin{table}[h!]
\small
  \centering
  \begin{tabular}{l cc}
    \toprule
     &   \multicolumn{2}{c}{Dataset}\\
    
    Prototypes &   Split-CIFAR100 & Split-MiniImageNet \\
    
    Coefficient($\alpha$) & (K=20) & (K=20) \\\midrule
    
    $\alpha = 0.$ & 20.6\tiny{$\pm0.2$} & 20.2\tiny{$\pm0.2$} \\
    
    $\alpha = 1.$ & 81.8\tiny{$\pm0.5$} &  68.0\tiny{$\pm0.4$} \\
    
    $\alpha = 2.$ & 82.2\tiny{$\pm0.4$} &  \textbf{69.8}\tiny{$\pm0.3$} \\
    
    $\alpha = 4.$ & \textbf{83.5}\tiny{$\pm0.4$} &  \underline{68.3}\tiny{$\pm0.5$} \\
    
    $\alpha = 8.$ & \underline{82.7}\tiny{$\pm0.5$} & 67.9\tiny{$\pm0.3$} \\
    
    $\alpha = 16.$ & 82.3\tiny{$\pm0.5$} & 67.4\tiny{$\pm0.4$} \\
    
   \bottomrule
  \end{tabular}
  \caption{Ablation study on the effect of prototype learning coefficient, $\alpha$ in Eq. \eqref{eq:overall_loss}. The reported numbers are \textit{Task-incremental} average observed accuracy.}
  \label{tab:prd_ablation_alpha}
\end{table}


\section{Domain-Incremental Experiment}

We evaluate our approach using the CLAD-C dataset~\cite{SODA}, under the conditions laid out in~\cite{SODA}. The dataset contains images recorded via dashcams over 3 days. The shift between night and day constitutes the task boundaries, hence overall we have 6 tasks. The objective of each task is to correctly classify an image into one of 6 possible classes of objects: 
\begin{enumerate*}
    \item pedestrian
    \item cyclist
    \item car
    \item truck
    \item bus
    \item tricycle.
\end{enumerate*}
% 
The dataset reflects the real-world distribution of these objects. Hence, certain classes are rarely observed (e.g. the tricycle class) and others are seen more frequently (e.g. the car class). As the night and day changes in the data stream and we are introduced to new tasks, the image distribution changes, sometimes so drastic that leads to the absence of a few classes. This fact, along with the in-task class imbalance of the data makes the CLAD-C dataset~\cite{SODA} a challenging, yet realistic, benchmark.

The training data contains overall 22,249 objects distributed over 6 tasks. We report our results using the final Average Mean Class Accuracy (AMCA) on the test data which contains 69,881 objects spanning both day and night. The AMCA for $T$ tasks each containing $C$ classes is given by:
% 
\begin{equation}
    \mathrm{AMCA} =  \frac{1}{|T||C|}\sum_{t\in T}\sum_{c \in C} A_{c}^{t}
\end{equation}
where $A_{c}^{t}$ is the accuracy of the class $c$ for the task $t$. The results are given in Table~\ref{tab:domain-incremental-soda10}. All methods in Table~\ref{tab:domain-incremental-soda10} use a ResNet-50~\cite{he2016deep} architecture, pretrained on ImageNet~\cite{deng2009imagenet}, and use a batch size of 32.
Our results highlight the versatility of our method and its applicability to real-life continual learning scenarios. Moreover, it suggests that our method is cable of performing under severe class imbalance and drastic distribution shifts, without having access to past data.

%dont forget the  caption yes I will fill it up! 
\begin{table}[hb]
  \centering
  \begin{tabular}{lcccr}
    \toprule
    & Finetune & EWC~\cite{kirkpatrick2016overcoming} & LwF~\cite{li2017learning} & PRD (ours)\\
    \midrule
    AMCA & 40.5 & 62.5 & 63.7 & \textbf{65.1} \\
   \bottomrule
  \end{tabular}

  \caption{\small Domain-incremental setting using the CLAD-C dataset~\cite{SODA}. All methods use a ResNet-50~\cite{he2016deep} architecture, pre-trained on ImageNet~\cite{deng2009imagenet}, and use a batch size of 32.
Our results highlight the versatility of our method and its applicability to real-life continual learning scenarios. Moreover, it suggests that our method is cable of performing under severe class imbalance and drastic distribution shifts, without having access to past data.}
  \label{tab:domain-incremental-soda10}
  
\end{table}


