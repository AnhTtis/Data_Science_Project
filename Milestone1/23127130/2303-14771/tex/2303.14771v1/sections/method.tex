\section{Methodology}\label{sec:method}
We consider a general continual learning setting where a learner is faced with a possibly never-ending stream of data divided into separate training sessions. At each session $S_t$, a set of data $\mathbf{X}^t$ and their respective labels $\mathbf{Y}^t$ are drawn from a distribution $D_t$ characterized by $P(\textbf{X},\textbf{Y}|T=t)$. When learning new sessions, it is assumed that access to the samples from previous sessions is restricted.
% 
% 
This definition covers both task-incremental settings where $(\mathbf{X}^{t}, \mathbf{Y}^{t})$ represent a separate task, class-incremental scenario where changes in $P(\textbf{X})$ induces a shift on $P(\textbf{Y})$, and the domain-incremental learning where changes in $P(\textbf{X})$ does not affect $P(\textbf{Y})$.
% 
We consider a neural network composed of an  encoder $f$ that maps an input sample $x$ to its features representation $ f(x) \in \mathbb{R}^d$ and a projection head $g$ that projects the features onto another latent space $ g \circ f(x) \in \mathbb{R}^k$ where $k<d$. 
Our goal is to minimize the objective loss $\mathcal{L}_t$ on the new session data while not increasing the objective loss of the previously learned sessions $\mathcal{L}_{i 
 \;  \forall i<t}$.
%
%

A common approach to control the loss of the previously encountered sessions is to use a buffer of stored samples and reuse them upon encountering new sessions. 
In our approach we do not require access to past data, instead, we approximate the behavior of the previously seen classes of objects via a set of prototypes. In our approach upon visiting a new session, we employ a novel distillation term to approximate the now-inaccessible loss of the previous sessions and set to restrict this surrogate loss in order to control the loss of the previously seen sessions.  In the following sections, we introduce the different parts of the objective function used to optimize the model at each step.

% Our approach does not require access to any stored exemplar and instead employs prototypes as proxies to previous classes with a novel distillation term in place of previous classes classification loss. We rely on a modification of the standard classified training objective. In the following we introduce the different parts of the objective function used to optimize the model at each step
%\subsection{Supervised Contrastive Relation Distillation}

\paragraph{Supervised Contrastive Learning}
Supervised Contrastive Learning~\cite{khosla2020supervised} is a powerful representation learning method observed to be useful in many downstream tasks.
\cite{davari2022probing} employed Supervised Contrastive training for continual learning and showed that the learned representations are less prone to forgetting compared to that learned with Cross Entropy loss (CE). In this work, we build on this observation and propose a solution to jointly train the representation and classification head in an incremental fashion. In order to optimize the representation for the task being learned,  we apply a supervised contrastive loss on the incoming data.

\begin{equation}\label{eq:supcon}
\mathcal{L}_{SC}(\mathbf{X}) = -\sum_{\mathbf{x}_i \in \mathbf{X}}\frac{1}{|A(i)|}\mathcal{L}_{SC}(\mathbf{x}_i)
\end{equation}
Where each sample's loss is given:
\begin{align}
\mathcal{L}_{SC}(\mathbf{x}_i)=& \nonumber\\ \sum_{\mathbf{x}_p \in A(i)}\log&\frac{h\big(g\circ f(\mathbf{x}_p),  g\circ f(\mathbf{x}_i)\big)}{\sum_{\mathbf{x}_a \in \mathbf{X}/x_i}
h \big(g\circ f(\mathbf{x}_a), g\circ f(\mathbf{x}_i)\big)}
\end{align}
Where $h(a,b)=\exp(\simm(a,b)/\tau)$ and  $\simm(a,b)=\frac{a^Tb}{\|a\|\|b\|}$. Here $A(i)$ represents the set of samples that form positive pairs with $x_i$ i.e. augmented views of $\mathbf{x}_i$ and  other samples of the same class $\{\mathbf{x}_j | y_j =y_i\}$.
Note that this loss is composed of tightness terms between positive pairs and contrast terms with negative pairs \cite{boudiaf2020unifying}.
\paragraph{Prototype Learning without Contrasts}
In order to easily link the discriminative representations learned by optimizing $\mathcal{L}_{SC}(\mathbf{X})$ to a final class level prediction we consider the notion of class prototypes \cite{caccia2022new,de2021continual}, which allow us to score a sample's representation with respect to each class. A simple solution for learning the class prototypes is to apply the Softmax in combination with the Cross-Entropy loss, for a given sample yielding 
\begin{equation}
   - \simm(\mathbf{p},f_{\theta}(\mathbf{x}_i)) + \log\Bigr(\sum_{\mathbf{p}_k\in \mathbf{P}}h(\mathbf{p}_k,f_{\theta}(\mathbf{x}_i))\Bigl)
\end{equation}

However, it has been shown that in the class-incremental setting, the softmax combined with cross-entropy produces a large interference with previously learned classes due to terms that suppress previous classes logits~\cite{caccia2022new,Ahn_2021_ICCV}.
%These prototypes can be used to make predictions, but also allow us to control the forgetting without interferring with the models representation learning as will be discussed in the next sub  
%Softmax Cross Entropy loss was shown to cause a large interference with previously learned classes due to the contrast term suppressing previous classes logits~\cite{caccia2022new}. 
Here, we propose instead to learn class prototypes that are representatives of each class samples using only the first term in this loss, referred to as the ``tightness" term \cite{boudiaf2020unifying}. For each class $c$ we initialize a random  prototype $\mathbf{p}_c \in \mathbb{R}^d$.  We want to optimize these prototypes to be representatives of current classes samples without introducing any suppression to prototypes of previous classes.  To achieve this we use a loss term considering only positive pairs of class samples and their corresponding prototypes where we aim to maximizing the similarity 
of these pairs:
%We consider the  which aims at maximizing the similarity of each prototypes to its class features.
\begin{equation}\label{eq:tt}
%\resizebox{.99\hsize}{!}{$
    \mathcal{L}_{p}(\mathbf{X}) = -\frac{1}{|\mathbf{X}|} \sum_{\mathbf{x}_{i},y_i \in \mathbf{X,Y}} \simm\bigl(\mathbf{p}_{y_i}, \texttt{sg}\bigl[\ff(\mathbf{x}_i)\bigr]\bigr)%$}
\end{equation}
Here $sg$ denotes the stopgradient operations. %and $\mathbf{p}_{c(x_j)}$ the prototype for the class corresponding to $x_j$. 
The suggested loss contains only  a tightness term, \textit{i.e.}, contrast-free, which doesn't have a direct effect on previous classes prototypes. From this loss term, we aim to only optimize the prototypes and not to change the samples representations as this is taken care of by~\eqref{eq:supcon}. Note that contrary to \cite{caccia2022new} which also uses prototype-based learning, we do not include any contrastive terms for the prototype learning, the learning of class separations being left to $\mathcal{L}_{SC}$. Note that we utilize the stop-gradient operation so that the learning of the prototypes does not interfere with the representation learning or previous prototypes. 

Once prototypes are obtained we can now directly perform predictions at test time by using the similarity of the sample representation and the set of prototypes to decide on the nearest class prototype.
%Further, we aim for an interference, with previous prototypes,  free classification head  hence we adding the stopgrad operation.
\paragraph{Prototypes-Samples Similarity Distillation}
Our prototypes are learned in isolation for each task. However, as we update our feature extractor using the supervised contrastive loss Eq.~\eqref{eq:supcon} prototypes of previous task classes will become outdated leading to the forgetting of previously learned classes. As shown in \cite{davari2022probing} this forgetting may correspond simply to movement in the decision boundary, despite classes still being well separated. To update old prototypes as we update our representation, we propose a similarity distillation term  using new class data as a proxy for old data. Before the start of a new training session, we  compute the prototypes' similarities to each sample of new classes.  During the new training session, we propose to  minimize the KL divergence between the similarities distribution of prototypes to minibatch samples, enforcing current similarities to be similar to previous similarities. % before and  during the current session as follows:

%Specifically we consider the softmax distribution $\mathcal{P}_t(\mathbf{X}) 

Consider the current model and set of prototypes for previous classes $f_{\theta_t}, \mathbf{P}_{o}^t$ along with their corresponding model and prototype from the end of the previous task $f_{\theta_{t-1}},\mathbf{P}_{o}^{t\mathrm{-}1}$. For an incoming mini-batch $\mathbf{X}$ and a corresponding prototype we can consider the softmax output $\mathcal{P}_t(\mathbf{p}_k^t,\mathbf{X})$, where the $i^{th}$ entry is given:
\begin{equation}
    \mathcal{P}_t(\mathbf{p}_k^t,\mathbf{X})_i = \frac{h(\mathbf{p}_k^t, f_{\theta_t}(\mathbf{x}_i))}{\sum_{\mathbf{x_j} \in \mathbf{X}} h(\mathbf{p}_k^t, f_{\theta_t}(\mathbf{x}_j))}
\end{equation}
%We first apply the softmax operation on  each estimated prototype-sample similarity.
Denoting for shorthand $\mathcal{P}_t(\mathbf{p}_k^t,\mathbf{X})$ as $\mathcal{P}_t(k)$ we can now construct a relation distillation term as the KL-divergence between prototype-samples similarity distribution estimated with the model at session $t-1$ and during the current session $t$.
\begin{equation}\label{eq:RD_KL}
    \mathcal{L}_{d}(\mathbf{P}) = \sum_{\mathbf{p}_k \in \mathbf{P}_o} KL \Bigl(\mathcal{P}_{t}(k) \, || \, \mathcal{P}_{t\mathrm{-}1}(k)\Bigr)
\end{equation}
Note that this is distinct from distillation approaches where we compute the similarities for each sample over existing classes. As illustrated in Fig.\ref{fig:teaser} the relative positions of samples to the prototypes are encouraged to remain the same by our loss. This results in flexibility in the representations in order to adapt to new classes while keeping the relative distances of many samples to the prototype as similar as possible.  
% 
%we estimate it for each prototype over current session samples to obtain a diverse set of relations and allow the sample representations to more easily adapt to the new data.  
%Note that here the prototypes are considered anchors in the formed pairs and hence similarities distribution is estimated over samples for each prototype. 
% 
% 
Our overall training objective is thus given as a combination of these three terms:
\begin{equation*}\label{eq:overall_loss}
    \mathcal{L}(\mathbf{X}) = \mathcal{L}_{sc}(\mathbf{X}) + \alpha  \mathcal{L}_p(\mathbf{X},P_{c}) + \beta \mathcal{L}_{d}(\mathbf{X},P_{o})
\end{equation*}