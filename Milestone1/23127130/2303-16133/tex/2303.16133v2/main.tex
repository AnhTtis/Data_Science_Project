
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{xspace}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{wrapfig}
\setlength\intextsep{0pt}

\title{Exposing and Addressing Cross-Task Inconsistency\\ in Unified Vision-Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Adyasha Maharana \email adyasha@cs.unc.edu \\
      \addr University of North Carolina, Chapel Hill
      \AND
      \name Amita Kamath \email kamatha@cs.ucla.edu \\
      \addr University of California, Los Angeles
      \AND
      \name Christopher Clark \email chrisc@allenai.org \\
      \addr Allen Institute for AI
        \AND
      \name Mohit Bansal \email mbansal@cs.unc.edu \\
      \addr University of North Carolina, Chapel Hill
        \AND
      \name Aniruddha Kembhavi \email anik@allenai.org \\
      \addr Allen Institute for AI
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\dataset}{\textsc{CocoCon}\xspace}
\newcommand{\uio}{Unified-IO\xspace}
\newcommand{\ofa}{OFA\xspace}



\def\month{02}  % Insert correct month for camera-ready version
\def\year{2024} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=ue9igTDLN2}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, \dataset{}, where we create contrast sets by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art vision-language models suffer from a surprisingly high degree of inconsistent behavior across tasks, especially for more heterogeneous tasks. To alleviate this issue, we propose a rank correlation-based auxiliary training objective, computed over large automatically created cross-task contrast sets, that improves the multi-task consistency of large unified models while retaining their original accuracy on downstream tasks. Data and code are available at \url{https://adymaharana.github.io/cococon/}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure}[h]
  \centering
   \includegraphics[width=0.9\linewidth]{figures/Fig_1_TMLR.pdf}
   \vspace{-10pt}
   \caption{Examples of consistent and inconsistent predictions from \uio{}$_{XL}$~\citep{lu2022unified}.
}
   \label{fig:intro_example}
   \vspace{-10pt}
\end{figure}





General Purpose Vision (GPV) models~\citep{gupta2022towards,kamath2022webly,cho2021unifying,lu2022unified,wang2022ofa} are trained to perform many diverse multimodal tasks ranging from visual question answering (VQA) and referring expression grounding to semantic segmentation and image generation. A fundamental requirement and intuitive expectation of such systems is that they provide consistent results across the tasks they support. For example, if a system produces the caption \emph{two jaguars are sitting on a tree branch} then one would expect it to answer the question \emph{What animals are these?} with \emph{jaguars} and to return two bounding boxes if asked to locate the \emph{jaguars}.


While the latest GPV models~\citep{lu2022unified,wang2022ofa,huang2023language} perform impressively on multi-task benchmarks~\citep{gupta2022grit}, we find that these models can provide surprisingly inconsistent answers for simple images and tasks. Fig.~\ref{fig:intro_example} shows an example where \uio{}$_{XL}$~\citep{lu2022unified} prefers the caption: \emph{A cat eating a rabbit it has caught}, but then answers \emph{bird} when asked \emph{What is the cat eating?} Solving multiple tasks for one image may require some degree of specialized reasoning, but they necessitate a semantic interpretation of the input image which should be common across tasks. When models demonstrate such trivial inconsistencies, it is hard for end users to trust them, particularly in important applications, because it is harder to understand and predict their behavior. From a practical standpoint, it is challenging to incorporate such models into larger systems, because it's hard to calibrate for them. Finally, from a philosophical view, having different interpretations of an image depending on the target task defies how we intuitively think unified models should behave.

In computer vision, cross-task consistency has been of some interest for classical tasks \citep{zamir2020robust}, while in natural language processing past work has studied consistency for tasks like question-answering~\citep{kassner2021beliefbank}. However, in vision-and-language research, much work has focused on within-task consistency for visual question answering \citep{shah2019cycle, ribeiro2019red, dharur2021sort, ray2019sunny, bitton2021automatic}. Semantic consistency of multi-modal models \textit{across tasks} has remained unexplored, partly due to the absence of models (until recently) that can perform various tasks simultaneously and effectively.



With recent advances in GPV research, we can now probe models for cross-task consistency. A simple and straightforward method is to compute the semantic overlap between a model's predictions for the same image across tasks. While possible for related tasks like captioning and VQA, measuring semantic overlap between outputs from different modalities can be ill-defined (e.g. it is unclear how to quantify the overlap between bounding boxes for localization and an answer for VQA). Additionally, models may perform well by producing simple outputs for tasks. For example, if a model generates short captions about a single subject, this method can only probe consistency for that narrow set of visual elements. Instead, we choose to utilize human-defined outputs for tasks that cover a wide range of semantic elements for a complete evaluation of consistency. For a given pair of tasks, we perturb the test instances in similar but meaningful ways that change the gold label, in order to create contrast sets \citep{gardner-etal-2020-evaluating}. More likely perturbations (e.g. \textit{keyboard} $\rightarrow$ \textit{laptop} in Fig.~\ref{fig:main_figure}(b)) lead to harder contrast sets whereas less likely perturbations (e.g. \textit{keyboard} $\rightarrow$ \textit{earbuds}) lead to easier contrast sets. Then, we measure a model's likelihood of predicting the ground truths as well as their contrast counterparts for both tasks. If a model is more likely to predict the contrast output for one task and the ground truth output for the other task
or vice-versa, it implies that the model has contradicting interpretations of the same input for the two tasks. In the example shown in Fig.~\ref{fig:main_figure}, the model favors the caption with \emph{computer keyboard}, but is more likely to answer \emph{laptop} in response to the question: \emph{What is the object in the lower right-hand corner?}, leading to cross-task inconsistency. Operating with likelihoods also allows us to overcome the challenges of comparing outputs from two different modalities.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.85\linewidth]{figures/main_figure_arxiv.pdf}
   \vspace{-10pt}
   \caption{\textbf{Illustration of our method for probing inconsistencies across tasks.} We build candidate answers for multiple tasks that correspond to different semantic understandings of an image (e.g., keyboard vs. laptop), and check if the model's preferred answers across tasks match the same semantic understanding.
}

   \label{fig:main_figure}
   \vspace{-10pt}
\end{figure*}


For this purpose, we present \dataset{}, a benchmark dataset with contrast sets for four commonly used multimodal tasks. Each sample in \dataset{} contains up to five contrast sets of varying difficulty for each of the tasks. We use image captioning as an \textit{anchor task} because captions contain semantic elements used by most other tasks and evaluate it against VQA which has textual outputs, localization which has bounding box outputs, and image generation with image outputs. This covers task pairs with outputs in the same output modalities as well as different output modalities. We measure consistency \% as well as Spearman's rank correlation coefficient between the ranking of contrast sets.

We evaluate two recent GPV models, \uio \citep{lu2022unified} and \ofa \citep{wang2022ofa}, both of which support all four tasks in \dataset{}. Additionally, we evaluate Kosmos-2 \citep{peng2024kosmos} and GILL \citep{koh2023generating} which support three out of four tasks in \dataset{}. We show that cross-task inconsistency is a surprisingly significant phenomenon in these models across all tasks in \dataset{} and various model sizes. Inconsistency increases with the heterogeneity between output modalities within a pair of tasks as well as with the complexity of the tasks themselves. Moreover, consistency improves with easier contrast sets, yet remains significantly less than 100\% for all tasks. We also find that larger models are more consistent by virtue of being more accurate at the tasks. Finally, our evaluation suggests that multi-task models capable of performing a larger set of tasks are more inconsistent.

Cross-task inconsistency is undesirable in a unified model, and it is paramount that we work toward mitigating it. To this end, we propose using a consistency objective utilizing large automatically generated cross-task contrast sets and a rank correlation loss objective via soft ranking \citep{blondel2020fast}. Our experiments show that continued training of models using this auxiliary consistency-based objective can lead to consistency improvements when evaluated on \dataset{} while preserving or improving the accuracy of the model on the original test sets. In summary, our contributions include:

\begin{enumerate}[label=(\alph*),topsep=0pt,itemsep=0.3ex,partopsep=0.1ex,parsep=0.1ex]
\item highlighting the issue of cross-task inconsistency in multi-modal models,
\item introducing the use of contrast sets and a benchmark dataset, \dataset{}, to measure cross-task inconsistency amongst four popular multimodal tasks,
\item demonstrating the inconsistent behavior of state-of-the-art vision-language models, and
\item a consistency-based training objective to improve consistency without compromising accuracy.
\end{enumerate}

\section{Related Work}
\label{sec:related_work}

To our knowledge, no existing work evaluates cross-task consistency for multi-modal models. In this section, we discuss studies that evaluate and enforce consistency for individual or multiple tasks within one modality.

\textbf{Consistency for VQA.}
\cite{shah2019cycle} revealed that VQA models are inconsistent across linguistic variations of a visual question, then improved consistency using automatic data augmentation; an approach which was further improved in \cite{Kant2021ContrastAC} using an additional contrastive loss. \cite{ribeiro2019red, ray2019sunny} evaluated consistency across the original QA data and automatically generated QA pairs implied by this data. \cite{Selvaraju2020SQuINTingAV} collected human-annotated sub-questions to evaluate model reasoning capabilities through the lens of consistency. \cite{dharur2021sort} trained models to rank the sub-questions proposed by SQUiNT \citep{Selvaraju2020SQuINTingAV} higher than unrelated questions from the same image, making models more consistent across both sub-questions and rephrasings of the question. Contrast sets have also been used to measure and improve consistency for VQA~\citep{ribeiro2019red, bitton2021automatic}. Unlike these works, our approach evaluates and improves consistency across multiple tasks.






\textbf{Consistency for NLP.}
Consistency has also been discussed in NLP, primarily in the single-task setting. \cite{elazar-etal-2021-measuring} evaluated and improved factual consistency of pre-trained LMs across paraphrasings of factual statements. \cite{kassner2021beliefbank} considered the responses of a pre-trained LM to a stream of questions, and evaluated and improved the consistency and accuracy of its answers over time. \cite{kaushik2019learning} collected counterfactual instances to evaluate the overreliance of NLP models on spurious attributes. \cite{gardner-etal-2020-evaluating} manually created contrast sets for 10 individual NLP tasks to evaluate single-task consistent responses across meaning-altering perturbations. In comparison to these works, we evaluate consistency across multiple tasks, without the need for human annotations as used in \cite{gardner-etal-2020-evaluating}. \cite{nishino-etal-2019-keeping} used multi-task learning with a hierarchical consistency objective to predict the headlines, key phrases, and categories of articles; however, the model uses separate decoders per task. Our work studies cross-task consistency of General Purpose Vision (GPV) models with unified output decoders.

\textbf{Cross-task Consistency for Vision.}
Cross-task relationships among classic vision tasks have been studied by \cite{Zamir2018TaskonomyDT}. \cite{lu2021taskology} used geometry and physics to identify consistency constraints between such tasks, and use them to improve performance in low data regimes. \cite{zamir2020robust} enforced cross-task consistency for vision tasks using inference-path invariance and demonstrate their method for tasks in the pixel space (like depth and surface normals). It is not straightforward to extend this approach to vision and language tasks which are often conditioned not just on an image but also on a language input and where one task's output may not easily be transformed into another's output.

\section{Contrast Sets for Cross-Task Consistency}
\label{sec:contrast_sets}

In this section, we describe the problem of inconsistency across tasks in unified models, motivate the use of contrast sets to evaluate consistency, and outline our framework for measuring cross-task consistency.

\noindent \textbf{The Problem.}
In the pursuit of developing task- and modality-agnostic unified systems, models like \uio \citep{lu2022unified} are trained on a variety of tasks geared towards learning robust semantic representations of the input. Each task is designed to strengthen the model's understanding of a distinct perspective of the ground truth. For instance, a visuo-linguistic model is simultaneously trained to generate a caption for the entire image as well as answer questions about subjects in the image. The popular and effective training paradigm for such models is to learn a probability distribution over the space of possible outputs and maximize the likelihood of the target output. This leads to an inherent ranking of possible outputs based on their probabilities, which can be used to rank outputs that reflect distinct semantic understandings of the input. For a reliable and truly unified model, the ranked space of such probable outputs should also be aligned \emph{across tasks}. However, (see Fig.~\ref{fig:intro_example}), we find that unified models can interpret inputs differently for different tasks, leading to misalignment between these spaces and inconsistency in predictions. We measure this inconsistency with the help of contrast sets.

\noindent \textbf{Contrast Sets.} Model performances on the i.i.d. test data are often treated as an absolute measurement of its abilities. However, when the test data has systematic gaps like annotation artifacts \citep{gururangan2018annotation}, the model can learn simple decision boundaries to solve the dataset and result in misleading high performances. \cite{gardner-etal-2020-evaluating} introduce contrast sets to close such systematic gaps in evaluation. Contrast sets are created by perturbing test instances in meaningful ways that change the gold label. This allows for the evaluation of a model's local decision boundary around a \textit{pivot} test instance and measurement of how well it \textit{aligns with the correct decision boundary}. Models with simple decision boundaries fail to perform well on contrast sets. Using the same intuition, we can create equivalent perturbations on a test instance for a pair of tasks and evaluate whether the unified model performs similarly on the contrast set for either task. In this manner, we leverage the framework of contrast sets to measure how well a model's decision boundaries for two distinct tasks \textit{align with each other}.

Consider a model with parameters $\theta$ and two tasks $t_0$, $t_1$ that can be performed by the model. To construct a contrast set, we first pick a test instance and the respective ground truth annotations for each task i.e. $(x_{t_0}, y_{t_0})$, $(x_{t_1}, y_{t_1})$, termed as the \textbf{pivot} instances. We define the space of contrast outputs for an instance $x$ as the set of outputs $\tilde{y}$ that are within some distance $\epsilon$ of $y$. That is, $C(x) = \{(\tilde{y}\,|\,d(y, \tilde{y})\,< \,\epsilon\}$, where $d(.)$ is some distance function. Let $f_{\theta}(y|x)$ be the likelihood of model $\theta$ for predicting the output $y$ in response to input $x$. Now, we define the model $\theta$ to be consistent across tasks $t_0, t_1$ with respect to the pivots $x_{t_0}, x_{t_1}$ if the model is more likely to predict the gold outputs $y_{t_0}, y_{t_1}$ in both tasks, as compared to their respective contrast outputs $\tilde{y}_{t_0}, \tilde{y}_{t_1}$. The model is also considered consistent if it assigns a larger likelihood to the contrast outputs than the gold outputs of both tasks because, even if the model answers wrongly for both tasks, as long as it reflects a common understanding of the input, the model is consistent. Mathematically,
\begin{equation}
% \vspace{-0.1in}
    \mathcal{C} =
    \begin{cases}
      1 & \text{if  $f_{\theta}(y_{t_0}|x_{t_0}) > f_{\theta}(\tilde{y}_{t_0}|x_{t_0})\,\,\land\,\,f_{\theta}(y_{t_1}|x_{t_1})> f_{\theta}(\tilde{y}_{t_1}|x_{t_1})$}\\
        1 & \text{if  $f_{\theta}(y_{t_0}|x_{t_0}) < f_{\theta}(\tilde{y}_{t_0}|x_{t_0})\,\,\land\,\,f_{\theta}(y_{t_1}|x_{t_1}) < f_{\theta}(\tilde{y}_{t_1}|x_{t_1})$}\\
      0 & \text{otherwise}
    \end{cases}
    \label{eqn:consistency_eval}
% \vspace{-0.1in}
\end{equation}
where $\tilde{y}_{t_0}\in C(x_{t_0})$, $\tilde{y}_{t_1}\in C(x_{t_1})$ and $\mathcal{C}$ is the consistency score. This framework can be easily extended to more than two tasks. For the scenario of $>2$ tasks, we define an \textbf{anchor task} $t_0$, that contains semantic elements common to each of the remaining tasks. Then, we compute pairwise consistency scores for the anchor and the rest of the tasks $\{t_1, t_2,\ldots,t_{T}\}$ i.e. we have $T$ pairwise scores for $T$ tasks.

\noindent \textbf{Difficulty ($k$).} Contrast sets can be of varying difficulty, depending on the likelihood of the perturbations used to create the contrast sets. For example, \textit{basketball} is a likelier substitute for the semantic concept \textit{football} whereas \textit{kite} is much less likely. Hence, the contrast set containing \textit{basketball} is a \textbf{hard} contrast set and the one containing \textit{kite} is an \textbf{easy} contrast set. We rank all contrast sets for a given instance and use the rank $k$ to indicate the difficulty i.e. lower $k$ implies harder contrast sets.

\noindent \textbf{Evaluation Metrics.} We introduce two metrics for calculating the consistency of a model over a dataset of $N$ samples, containing $K$ contrast sets each, for $T$ tasks. Each sample consists of pivot instances for the $T$ tasks and the corresponding sets of up to $K$ contrast outputs. We first rank the $K$ contrast sets by difficulty according to the model's likelihoods for the anchor task, $\{\tilde{y}_{t_0}^1,\ldots \tilde{y}_{t_0}^K\}$. For each task $t_{i}$ and at each $k$, we compute \textbf{\% consistency @ $k$ ($\mathcal{C}_k)$} as the \% of samples for which the model is consistent i.e.,
 \begin{equation}
     \mathcal{C}_{k} = \frac{1}{N}{\sum_{i=1}^{N}{\mathcal{C}(y_{t_0}, y_{t_i}, \tilde{y}_{t_0}^k, \tilde{y}_{t_i}^k)}}
 \end{equation}
where consistency $\mathcal{C}(.)$ is computed as per Eqn.~\ref{eqn:consistency_eval}. Higher values for $\mathcal{C}_{k}$ suggest that the model is more consistent across $t_0$ and $t_{i}$.  This metric measures consistency with respect to the ground truth annotations, which are used as pivots in our setup. We also compute \textbf{\texttt{spearmanr} $(\rho_{rank})$}, the Spearman's rank correlation coefficient over the ranked contrast outputs for both tasks, in order to measure the global alignment between the two output spaces. We observe these metrics in tandem with task-specific accuracies to avoid overestimating a model with degenerate but consistent solutions.



\section{The \dataset\ Benchmark}
\label{sec:dataset}

In this section, we detail the construction and composition of our benchmark dataset \dataset{}, developed as per the framework in Sec.~\ref{sec:contrast_sets}. Then, we discuss the statistics and evaluation framework of \dataset{}.

\subsection{Dataset Construction}

The COCO dataset \citep{lin2014microsoft} contains annotations for many tasks in vision and language, which makes it very suitable for evaluating cross-task consistency in a multimodal model. \dataset\ is created from the validation splits of the four tasks i.e. image captioning (\textbf{anchor task}), VQA \citep{antol2015vqa, goyal2017making}, localization, and text-to-image generation. The dataset creation pipeline is as follows.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/Dataset_Construction_Figure_v3.pdf}

   \caption{\textbf{Step-by-step demonstration of the automated pipeline for generating contrast sets.} Contrast sets generated from this pipeline are manually filtered to prepare the \dataset{} benchmark.}
   \label{fig:construction}
   \vspace{-10pt}
\end{figure*}

\noindent \textbf{(Step 1) Selection of Pivot Instances.} First, we select \textbf{pivot instances} for the captioning and VQA tasks since it is easy to compute semantic overlap between the outputs of these tasks. Existing captioning annotations for the COCO dataset were filtered to retain ones that had semantic overlap with at least one question-answer pair from VQAv2 annotations. For instance, the caption: \emph{The woman is jumping in the air to catch the frisbee.} from COCO overlaps with the VQA sample: \emph{What is she playing? frisbee} (see Fig.~\ref{fig:construction}, Step 1) and was retained in our method. The semantic overlap was computed using a series of text-processing steps including lemmatization and word overlap. 

\noindent \textbf{(Step 2) Contrast Set Candidates.} Next, we need to substitute the overlapping semantic concept with other likely concepts to create contrast sets. There are many ways to perform this step. For instance, these perturbations can be written by human annotators, which might result in undesirable systematic biases in the contrast sets \citep{gururangan2018annotation}. Adversarial methods advocate gradient-based methods to get hard negatives for such perturbations \citep{alzantot2018generating}, however, we want to avoid integrating the biases of the models we are evaluating into a benchmark dataset. 

In contrast, we choose to use probable answers to the VQA questions from an off-the-shelf VQA model, GPV-2 \citep{kamath2022webly}, to create a large set of perturbations (see Fig.~\ref{fig:construction}, Step 2). GPV-2 is trained on the Web10K dataset \citep{kamath2022webly} that contains semantic concepts beyond COCO. This makes the contrast sets in \dataset{} diverse and additionally challenging for unified models. Note that we do not evaluate GPV-2 on \dataset{} since it can perform only a subset of the tasks present in it (see Fig.~\ref{fig:main_figure}).

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figures/cococon_v2.pdf}
   \caption{\textbf{Examples of contrast sets used in \dataset{}.} For each example, we show the relevant image (left), the ground truth caption, VQA question, or image generation prompt for the image with the perturbed concept in green (middle), the set of perturbations used to generate alternative answers and predictions from \uio$_{XL}$ for VQA (V), image generation (G) and localization (L) (right columns). \textcolor{green}{\checkmark} and \textcolor{red}{$\times$} indicate scenarios where the model predictions for captioning and the corresponding task for that particular contrast set are consistent and inconsistent respectively. `\textbf{-}' denotes a lack of localization annotations for the sample.}
   \label{fig:examples}
   \vspace{-10pt}
\end{figure*}

\noindent \textbf{(Step 3) Filtering.} The perturbations obtained from the previous step are filtered to retain high-quality candidates only, by creating contrast captions and retaining captions (and the corresponding contrast VQA samples) with high scores from the T5 language model i.e., the ungrammatical and nonsensical captions are filtered out. For instance, in Fig.~\ref{fig:construction} (see Step 3), the GPV-2 answer \emph{hide-and-seek} is filtered out using T5 score, because \emph{catch the hide-and-seek} is an unlikely phrase.

\noindent \textbf{(Step 4) Heterogeneous Evaluation Tasks.} The next step is to add evaluation tasks with heterogeneous output modalities i.e., localization and text-to-image generation (see Fig.~\ref{fig:construction}, Step 4). For the localization task, the automatically generated dataset from the last step is merged with the COCO localization annotations. Annotations for localization in COCO images pertain to the narrow set of pre-defined COCO objects, which may or may not appear in the caption. Only those objects which appear in the caption and VQA answer are retained in \dataset\ for the localization task. The contrast outputs created for the VQA task are used as contrast inputs for the localization task. For instance, in Fig.~\ref{fig:construction}, the contrast outputs \textit{frisbee} and \textit{football} selected in Step 3 for the VQA task are used as localization queries (contrast inputs) in Step 4. 


Finally, since image captioning is the task of generating a natural language description from an image, and text-to-image generation is the reverse process, one can reuse the ground truth annotations and contrasting annotations of captions for the task of image generation by simply reversing them. Similar to localization, the contrast outputs created for the image captioning task are used as contrast inputs for this task, and we measure the models' likelihood of generating the ground truth image in response to the contrast inputs.

\noindent \textbf{(Step 5) Manual Filtering.} This generated dataset was then subject to manual filtering and editing to ensure the high quality of the contrast sets. In this step, contrast sets that were synonyms, holonyms, hypernyms, or meronyms were removed from the dataset, in addition to other invalid perturbations. We conducted a study for inter-annotator agreement between two expert annotators on 200 samples and found an agreement for 98\% of the data, indicating the high quality of the dataset. We prioritized the collection of clean, expert annotations over size for this probing dataset. Note that the contrast sets were manually filtered to ensure high quality at test, but at training time we only use automatically generated data.


\textbf{Note}: Our contrast set generation pipeline precedes recent advances in multimodal large language models \citep{liu2023improved, liu2023visual} that can be used to significantly ease the generation of contrast sets (see Appendix~\ref{ssec:mllms-appendix}).


\subsection{Dataset Categories \& Statistics}
Each sample in the \dataset{} dataset contains a set of ground truth annotations and a semantic concept within the original caption is replaced with multiple contrast sets. The ground truth annotations comprise those for image captioning, VQA, and text-to-image generation, and 30\% of the \dataset{} samples also contain annotations for localization.\footnote{Localization annotations are present when a COCO object appears in the gold caption and VQA answer.} In total, the \dataset\ dataset contains 4789 contrast sets for 1,500 samples from the COCO validation split, with an average of 3.2 contrast sets per sample. The semantic concepts used for perturbing the pivot instances in this dataset range from a large variety of semantic, syntactic, and grounding phenomena. We labeled each sample from \dataset\ for these phenomena, see examples in Fig.~\ref{fig:examples} and a breakdown of the categories as well as additional examples in Appendix~\ref{sec:dataset-appendix}. Attributes (color, height, material, etc.), inanimate objects, and food are the most frequent semantic concept categories in \dataset{}, followed by animals, roles, actions, and location.



\subsection{Evaluation}
\label{sec:evaluation}
We measure the consistency between the captioning task (anchor) and each of the evaluation tasks independently. To evaluate consistency between captioning and VQA tasks, we compare the models' likelihoods of generating the caption and the VQA answer for both, pivot and contrast instances. For the localization and text-to-image generation tasks, the outputs are common to both, pivot and contrast instances, whereas the inputs contain semantic perturbations (see Fig.~\ref{fig:construction}, Step 4). Hence, we compare the models' likelihood of generating the output in response to the input from the pivot instance ($x_t, y_t$) vs. the input from the contrast instance ($\tilde{x}_t, y_t$) i.e., we replace $f_{\theta}(\tilde{y}_{t_0}|x_{t_0}),f_{\theta}(\tilde{y}_{t_1}|x_{t_1})$ in Eqn.~\ref{eqn:consistency_eval} with $f_{\theta}(y_{t_0}|\tilde{x}_{t_0}),f_{\theta}(y_{t_1}|\tilde{x}_{t_1})$ respectively. For example, we compare models' likelihood of generating the ground truth image in response to the gold caption and the contrast caption (e.g. caption containing \emph{frisbee} vs. \emph{football} in Fig.~\ref{fig:construction}) for the text-to-image generation task.



\section{Consistency-based Training}
\label{sec:training_obj}



\begin{wrapfigure}{l}{0.52\textwidth}
    \begin{minipage}{0.52\textwidth}
\begin{algorithm}[H]
\footnotesize
	\caption{Cross-Task Consistency-based Training}
	\begin{algorithmic}[1]
         \State $\gamma \leftarrow$ ratio of consistency-based updates to total updates
        \State $\lambda \leftarrow$ weight co-efficient for consistency-based loss
        \State $t_0, [t_1, t_2, t_3]  \leftarrow$ anchor task (e.g. captioning), evaluation tasks
        % \State $ \leftarrow$ other tasks (e.g. VQA, text2img, localization)
        \State $(x_{t_{i}}, y_{t_{i}}, \{\tilde{y}_{t_{i}}\}) \leftarrow$ input, gold output, contrast outputs for $t_i$
		\For {$epoch=1,2,\ldots,N$}
			\For {$step=1,2,\ldots,M$}
                \State $r \leftarrow \texttt{random(0, 1)}$
                \If {$r \le \gamma$}
                    \State $i \leftarrow \texttt{random(1, 2, 3)}$
		          \State Anchor task: $(X_{t_0}, Y_{t_0}, \{\tilde{Y}_{t_0}\}) \leftarrow (x_{t_0}, y_{t_0}, \{\tilde{y}_{t_0}\})$
            		\State Evaluation task: $(X_{t_i}, Y_{t_i}, \{\tilde{Y}_{t_i}\}) \leftarrow (x_{t_i}, y_{t_i}, \{\tilde{y}_{t_i}\})$
                    \State Cross-entropy losses: $\{L_{ce}^0\}, \{L_{ce}^i\}$
                    \State Ranks: $R_{0}, R_{i} \leftarrow \texttt{rank}(\{L_{ce}^0\}), \texttt{rank}(\{L_{ce}^i\})$
                    \State $L_{const} \leftarrow \texttt{spearmanr}(R_{0}, R_{i})$
                    \State $L \leftarrow \lambda\,*\,L_{const}\,+\,L_{ce}$
                \Else {}
                    \State Standard pretraining data: $(X, Y) \leftarrow \{x, y\}$
                    \State Cross-entropy loss: $\{L_{ce}\}$
		      \EndIf
				\State Compute backward pass
			\EndFor
            \State Evaluate updated model for cross-task consistency
		\EndFor
	\end{algorithmic}
  \label{alg:the_alg}
\end{algorithm}
    \end{minipage}
  \end{wrapfigure}



   A unified model exhibiting inconsistent predictions suggests that the model has learned weak semantic representations that are sensitive to task variations. It is undesirable to work with a model that is susceptible to such frailties. Moreover, consistency constraints can provide useful information for learning well-rounded semantic representations \citep{lu2021taskology} and reduce the need for training data \citep{zamir2020robust}. Hence, we propose to train unified models in a way that preserves consistency across their predictions (see Algorithm~\ref{alg:the_alg}). Given a pair of train instances $x_{t_0}, x_{t_1}$ for the tasks $t_0, t_1$, let $\{y_{t_0}\}, \{y_{t_1}\}$ be the spaces of $K$ probable and semantically equivalent outputs. $f_{\theta}(.)$ is the scoring function for a model with parameters $\theta$ and $\mathcal{R}(.)$ is some ranking function. Since ranking is a non-differentiable operation, we use soft ranking via regularization \citep{blondel2020fast} as the differentiable ranking function $\mathcal{R(.)}$. We formulate the consistency-based loss objective using Spearman's correlation as follows:
\begin{equation}
    \mathcal{L}_{const} = \frac{1}{2}{||\mathcal{R}(f_{\theta}(\{y_{t_0}\})) - \mathcal{R}(f_{\theta}(\{y_{t_1}\}))||}^2
\end{equation}
 Within a space of $k$ probable outputs for either task, if an output for task $t_0$ is ranked at $k-2$ while the equivalent output for task $t_1$ is ranked at $k+2$, the gradients from this objective are designed to push the two misaligned outputs towards a common rank $k$, which increases consistency as per the definition of $\mathcal{C}_{k}$ in Sec.~\ref{sec:contrast_sets}. This can affect the task-specific accuracy of an inconsistent model, especially when the more probable output is the gold label. Hence, we minimize our proposed consistency objective in addition to the standard cross-entropy loss during training i.e.
\begin{equation}
        \mathcal{L} = \lambda*\mathcal{L}_{const} + \mathcal{L}_{ce}
\end{equation}
where $\mathcal{L}_{ce}$ is the cross-entropy loss and $\lambda$ is the weighting factor for the consistency objective. See Algorithm~\ref{alg:the_alg}.



\section{Experimental Setup}
\noindent \textbf{Vision-Language Models.} \uio \citep{lu2022unified} and OFA \citep{wang2022ofa} are two recent publicly released models that perform a wide variety of tasks, including all tasks supported in the \dataset{} benchmark. \uio is pre-trained on all tasks in \dataset{}, as well as multiple other vision-only, language-only and vision-language tasks. OFA models are pretrained on image captioning, VQA, image-infilling, and language-only tasks. Hence, we finetune the pretrained OFA models on the tasks supported in \dataset{} for two epochs to support text-to-image generation.\footnote{The FID score of our finetuned OFA models on the text-to-image generation task is higher (worse performance) than that reported in \cite{wang2022ofa} because the latter model is finetuned on the text-to-image generation task only.} We evaluate all size variations of both models. Additionally, we evaluate Kosmos-2 \citep{peng2024kosmos} and GILL \citep{koh2023generating} which support localization and text-to-image generation tasks respectively. Besides, both models support \emph{zero-shot} image captioning and VQA tasks. See a summary of these models' capabilities in Tab.~\ref{tab:models}.

\begin{table*}
  \caption{Summary of the \dataset{} tasks supported by the various models used in our experiments.}
\vspace{6pt}
  \label{tab:models}
\footnotesize
% \small
  \centering
  \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Image Captioning} & \textbf{Visual QA (VQA)} & \textbf{Localization} & \textbf{Text-to-Image Gen.} \\
    \hline
    \uio{} \citep{lu2022unified} & \ding{51} & \ding{51} & \ding{51} & \checkmark \\
 OFA \citep{wang2022ofa} & \ding{51} & \ding{51} & \ding{51} & finetune \\
Kosmos-2 \citep{peng2024kosmos} & zero-shot &  zero-shot & \ding{51} & \ding{55} \\
GILL \citep{koh2023generating} & zero-shot & zero-shot & \ding{55} & \checkmark \\
    \hline
  \end{tabular}
\end{table*}

\noindent \textbf{Evaluation Metrics.} As outlined in Sec.~\ref{sec:contrast_sets}, we compute consistency \% ($\mathcal{C}_k$) and \texttt{spearmanr} $(\rho_{rank})$ for evaluating cross-task consistency. Additionally, we measure the following task-specific metrics: CIDEr score \citep{vedantam2015cider} for image captioning, accuracy for VQA \citep{goyal2017making}, IOU score \citep{padillaCITE2020} for localization, and FID score \citep{heusel2017gans} for text-to-image generation.

\noindent \textbf{Consistency-based Training.} We begin with the finetuned checkpoint for the OFA$_{LARGE}$ model and continue training with the objective proposed in Sec.~\ref{sec:training_obj}. We adapt the automated pipeline introduced in Sec.~\ref{sec:dataset} to generate nearly 84K contrast sets from the training split of COCO Captioning, VQA, and localization datasets. We performed a manual analysis of this dataset and found that nearly 85\% of the contrast sets are valid, which is of sufficient quality for large-scale training purposes. We use the cross-entropy loss as the score $f_{\theta}(.)$ function for each sample. The models are subjected to continued pretraining for one epoch and trained on the combination of contrast sets and original datasets for the four tasks in \dataset{}. We set $\lambda = 0.25$ and use a learning rate of 1e-6. Additional hyperparameters can be found in Appendix~\ref{sec:hyperparams-appendix}. This finetuned model is referred to as OFA$_{CON}$ in the rest of the paper.




\section{Results}
In this section, we discuss our findings from the evaluation of pretrained vision-language models, calibration of model likelihoods, common failure modes across models, and consistency-based training (see Sec.~\ref{sec:training_obj}).

\subsection{Evaluation of Pretrained Models}
\noindent \textbf{Models are more inconsistent across tasks of diverse modalities.} We wish to study how the semantic understanding of a unified model varies with tasks. We evaluate the best (and largest) \ofa and \uio models on \dataset{} and compare \% consistency across the 3 tasks i.e., VQA, localization, and text-to-image generation, with respect to the anchor task, i.e. image captioning. Results are shown in Fig.~\ref{fig:results}(a). For VQA (blue plots), \ofa$_{HUGE}$ and \uio$_{XL}$ models exhibit 78\% and 68\% top-1 consistency respectively. This number changes to 68\% and 65\% top-1 consistencies for localization (red plots), respectively, suggesting that unified models are especially prone to variation in semantic understanding when the outputs belong to different modalities. This is further supported by results for image generation (green plots) with 48\% and 50\% top-1 consistencies. Text-to-image generation is more complex than localization, because of the high dimensional output and rigorous semantic understanding required for the task. These results also suggest that cross-task inconsistency increases with the complexity of the task as well.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{figures/main_tmlr_rebuttal.pdf}

   \caption{\textbf{Results from evaluation on the \dataset{} benchmark.} (a) \% Consistency of \uio$_{XL}$, OFA$_{HUGE}$, Kosmos-2 and GILL models for varying difficulty ($k$) and all tasks in \dataset{}, (b) comparison of \% accuracy with \% consistency ($k$=1) values for all models evaluated in this paper and our OFA$_{Con}$ model (see Sec.~\ref{sec:training_obj}), and (c) \% consistency ($k$=1) values for different sizes of \uio models.}
   \label{fig:results}
   \vspace{-15pt}
\end{figure*}

\noindent \textbf{Models are inconsistent at hard as well as easy contrast sets.} The contrast sets used for evaluating top-1 \% consistency are \textit{hard negatives} and we observe low consistency for these samples (see Fig.~\ref{fig:results}(a)). For easier contrast sets i.e. in $k>1$ scenarios, the \% consistency increases significantly (yet remains < 100\%) for tasks with outputs of the same modality as the anchor task, as seen for VQA in Fig.~\ref{fig:results}(a). The \% consistency for VQA increases from $k$=1 to $k$=2 by 12\%, 8\%, and 4\% for Unified-IO, OFA, and Kosmos-2 respectively, and then by smaller margins with increasing k. Similarly, the \% consistency for localization increases from $k$=1 to $k$=2 by 12\%, 1\%, and 5\% for Unified-IO, OFA, and Kosmos-2 respectively, and then by smaller (or negative) margins. However, we see almost no improvement in consistency for text-to-image generation with increasing $k$, implying that the unification of modalities within a model is a non-trivial challenge.


\noindent \textbf{Models are more accurate than consistent.} We compare the top-1 \% consistency scores with the task-specific accuracies of models on the \dataset\ dataset in Fig.~\ref{fig:results}(b), and observe that consistency and accuracy are positively correlated. Unified-IO models feature well below the $x=y$ line while other models, i.e., OFA and Kosmos-2, lie close to the $x=y$ line. Unified-IO models are more accurate at the VQA and the localization tasks than the other models, without demonstrating a similar gain in consistency. This suggests that when the Unified-IO models make mistakes for one task, they rarely make the same kind of mistakes on the other tasks, which is what would allow a model to achieve high consistency independently of accuracy. On the other hand, OFA and Kosmos-2 models show a tight correlation between accuracy and consistency. Additionally, we observe that the models gradually transition over the $x=y$ line with increasing $k$ i.e., easier contrast sets (see Fig.~\ref{fig:scatter}). Notably, OFA$_{CON}$ consistently ranks the highest in terms of consistency at all $k$. Ideally, we want models to be highly consistent across tasks despite being inaccurate i.e., lie well above the x=y line, and theoretically, it is possible with a unified semantic backbone in the model (see discussion in Appendix~\ref{sec:discussion-appendix}). Instead, existing models appear to be consistent mostly by virtue of being accurate. This has the worrying implication that harder or more ambiguous tasks will lead to severe inconsistencies, and high consistency on easy tasks does not necessarily mean models are parsing inputs in a unified way across tasks. It also highlights the importance of studying hard tasks like image generation when evaluating consistency.


\begin{table*}
  \caption{\textbf{Results from evaluation on the \dataset{} benchmark.} Metrics are task-specific accuracies, \% consistency ($k=1$) and Spearman's rank correlation coefficient ($\rho_{rank}$). Higher is better, except for FID.}
  \vspace{6pt}
  \label{tab:results}
\footnotesize
% \small
  \centering
  \begin{tabular}{|p{3.3cm}|p{1.0cm}|p{1.2cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Param}} & \textbf{Caption} & \multicolumn{3}{c|}{\textbf{\multirow{1}{*}{\textbf{VQA}}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Localization}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Text-to-Image Gen.}}} \\

    % \hline
    & &  \textbf{CIDEr} & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{FID} $\downarrow$} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ \\
    \hline
    \textbf{A}~~~\uio{}$_{Small}$ & 71M & 111.8 & 75.3 & 28.1 & -0.06 & 50.6 & 36.3 & -0.09 & 93.45 & 49.5 & 0.05 \\
\textbf{B}~~~\uio{}$_{Base}$ & 241M & 140.5 & 87.8 & 36.2 & 0.12 & 61.59 & 41.6 & 0.13 & 91.56 & 50.4 & 0.02  \\
\textbf{C}~~~\uio{}$_{Large}$ & 776M & 227.1 & 90.0 & 55.1 & 0.36 & 68.8 & 56.2 & 0.03 & 85.04 & 48.5 & -0.01  \\
\textbf{D}~~~\uio{}$_{XL}$ & 2.9B & \textbf{269.9} & \textbf{92.3} & 68.7 & 0.48 & 72.1 & 65.9 & 0.20 & 70.23 & 50.8 & -0.0 \\
\hline
 \textbf{E}~~~OFA$_{Medium}$ & 93M & 83.4 & 72.7 & 72.1 & \textbf{0.67} & 55.6 & 52.3 & 0.21 & 110.3 & 49.1 & -0.02 \\
\textbf{F}~~~OFA$_{Base}$ & 182M & 100.7 & 77.8 & 77.0 & 0.65 & 62.3 & 59.4  & 0.19 & 105.7 & 50.1 & 0.04 \\
\textbf{G}~~~OFA$_{Large}$ & 472M & 113.5 & 82.6 & \textbf{80.7} & 0.64 & \textbf{71.3} & 64.7 & 0.28 & 103.4 & 52.3 & 0.02 \\
\textbf{H}~~~OFA$_{Huge}$ & 930M & 110.3 & 82.7 & 78.8 & 0.62 & 70.1 & 68.5 & 0.33 & 107.3 & 48.3 & -0.01 \\
\hline
\textbf{I}~~~Kosmos-2 & 1.6B & 65.8 & 44.8 & 70.9 & 0.62 & 70.8 & \textbf{70.1} & \textbf{0.60} & - & - & - \\
\textbf{J}~~~GILL & 8B & 45.6 & 35.7 & 51.9 & 0.41 & - & - & - & \textbf{25.4} & \textbf{57.6} & \textbf{0.10} \\
\hline
\multicolumn{12}{|c|}{\textbf{\textbf{Consistency-based Training}}} \\
\hline
\textbf{G}~~~OFA$_{Large}$ & 472M & 113.5 & 82.6 & 80.7 & 0.64 & 71.3 & 64.7 & 0.28 & 103.4 & 52.3 & 0.02 \\
\textbf{K}~~~~~ + Cont. Pretrain & 472M & 118.8 & 82.7 & 81.1 & 0.63 & 73.5 & 65.9 & 0.27 & \textbf{98.5} & 51.7 & 0.04 \\
\textbf{L}~~~~~ + Hinge Loss & 472M & 117.5 & \textbf{83.0} & 82.9 & 0.64 & 73.8 & 67.7 & 0.29 & 99.5 & 53.2 & 0.05 \\
\textbf{M}~~~OFA$_{CON}$ (ours) & 472M & 119.4 & 82.4 & \textbf{83.8} & \textbf{0.67} & \textbf{74.1} & \textbf{69.5} & \textbf{0.35} & 99.1 & \textbf{53.8} & \textbf{0.09} \\
    \hline
  \end{tabular}
\end{table*}




\noindent \textbf{Models capable of performing more tasks are more inconsistent.} \uio{} models are trained on 90 diverse datasets from vision and language domains and can perform all 7 tasks on the GRIT benchmark \citep{gupta2022towards}. In contrast, OFA models are pretrained on a subset of the tasks that can be performed by \uio{}. Interestingly, we observe that OFA models are more consistent than \uio{} across all three tasks in the \dataset{} benchmark. Additionally, Kosmos-2 and GILL (see rows I,J in Tab.~\ref{tab:results}) are more consistent than any Unified-IO or OFA models at their specialized tasks i.e., localization and text-to-image generation respectively. This suggests that massive multi-tasking can lead to larger misalignment between models' semantic understanding across tasks, especially those with heterogeneous output modalities.



\noindent \textbf{Larger multi-task models that are more accurate are more consistent.} We evaluate various sized \uio and OFA models (see  Fig.~\ref{fig:results}(c) and Tab.~\ref{tab:results}). We see that the top-1 \% consistency values increase generously with the scale of the model for VQA and localization, up to 20\% increase from Unified-IO$_{SMALL}$ to Unified-IO$_{XL}$ on VQA. Improvements are modest for image generation with model size. We see similar trends in OFA models barring a small drop in accuracy as well as consistency in the largest model.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/tmlr_rebuttal_reliability.pdf}

   \caption{\textbf{Reliability maps \citep{guo2017calibration} for likelihood scores from Unified-IO.} A positive correlation between likelihoods and output accuracy i.e. CLIPScore \citep{hessel2021clipscore} indicates that the likelihood scores can be reliable stand-ins for model confidence in our evaluation of the \dataset{} benchmark.}
   \label{fig:reliability}
   \vspace{-0.1in}
\end{figure*}

\subsection{Calibration of Model Likelihoods for Text-to-image Generation}
\label{sec:t2i_calibration}

In our evaluation of multimodal models on the \dataset{} benchmark, we compare the likelihood of a model for predicting one output vs. another and observe meaningful, intuitive trends for the VQA and localization tasks. However, the results are less sensitive to variations in model size and contrast set difficulty for the text-to-image generation task. Hence, in this section, we examine the calibration of the likelihoods obtained from Unified-IO and OFA models for text-to-image generation outputs, to find out whether they can be used reliably for evaluation on the \dataset{} benchmark. \cite{guo2017calibration} state that the class probabilities predicted by a well-calibrated classifier should reflect the ground truth correctness likelihood of the model. They demonstrate this behavior through reliability maps, where first, the range of class probabilities predicted by a model is divided into equally spaced bins. In each bin, the average predicted probability of the samples belonging to the bin is compared to the average accuracy of the samples. This plot should align with the $x=y$ line for a well-calibrated model, indicating that the model confidence is an exact estimate of how likely it is for the prediction to be correct. We adapt this principle for the text-to-image generation task in multimodal models and plot reliability maps for all model sizes of Unified-IO by comparing the likelihood of predicted images with their average CLIPScore \citep{hessel2021clipscore}. See results in Fig.~\ref{fig:reliability}.


The ideal scenario of $x=y$ for classification tasks is ill-defined for the text-to-image generation task since likelihood scores do not have a finite lower bound. Hence, we only seek a positive correlation between the likelihood scores and CLIPScore values as an indication that the model is well-calibrated. We find that Unified-IO models' likelihoods are indeed positively correlated with CLIPScore values for all model sizes (see linear regression fit over data points in Fig.~\ref{fig:reliability}). The correlation sometimes falters towards the upper bound of the distribution which can be attributed to the small sample size in those bins, as seen in the distribution of the cumulative fraction of the dataset over the range of likelihood scores (see Fig.~\ref{fig:reliability}; bottom). A linear regression fit over 95\% of the samples yields an even stronger positive correlation, especially for the smaller Unified-IO models. This analysis suggests that the low values of cross-task consistency as seen for the text-to-image generation task in Fig.~\ref{fig:results} primarily stems from the complexity of the task itself as well as the gross misalignment between text and image output modalities in multimodal models. Further, we perform temperature calibration \citep{guo2017calibration} on the likelihoods from Unified-IO$_{SMALL}$ and observe no significant improvement in correlation (see discussion and supporting figures in Appendix~\ref{ssec:calibration-appendix}). We also perform this analysis on likelihoods of text-to-image generation outputs from OFA models and find that they are similarly reliable for evaluation on the \dataset{} benchmark.



\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-0.3in}
   \includegraphics[width=0.9\linewidth]{figures/error.png}
   \caption{Comparison of categorical distribution in the \dataset{} benchmark with that of errors from evaluation of \uio{}$_{XL}$, OFA$_{Huge}$, Kosmos-2 models.}
   \label{fig:distribution}
   % \vspace{-0.3in}
\end{wrapfigure}

\subsection{Common Failure Modes}
% \noindent \textbf{Common Failure Modes.} 
We analyze the contrast sets in \dataset{} for which Unified-IO$_{XL}$, OFA$_{Huge}$, Kosmos-2 models are \textit{inconsistent for all tasks} and categorize the errors into the tags defined in Tab.~\ref{tab:dataset}. We find that all three of the models perform worst at recognizing \emph{attributes} correctly, i.e., 39.7\%, 34.2\%, 25.5\% of errors from Unified-IO$_{XL}$, OFA$_{Huge}$, Kosmos-2 respectively pertain to attributes, which are significantly higher than the category's 20.9\% distribution in the dataset. The other prevalent error categories are commensurate with the distribution in \dataset{} i.e., \emph{object}, \emph{food}, \emph{animal}, and \emph{location}. See examples of errors from Unified-IO$_{XL}$ in Fig.~\ref{fig:examples}.


\subsection{Consistency-based Training}

As outlined in Sec.~\ref{sec:training_obj}, we continue training OFA via the use of a cross-task consistency-based loss objective. Results for the finetuned model, OFA$_{Con}$, are shown in Tab.~\ref{tab:results} (see rows K-M in Consistency-based Training). Since OFA$_{Con}$ (row M) is finetuned for an additional epoch, we also provide a baseline where OFA is finetuned for an additional epoch with just the original cross entropy objective (row K). We find that our proposed loss objective improves consistency along both metrics i.e. top-1 \% consistency and rank correlation. The top-1 \% consistency improves by 2\% for VQA and text-to-image generation, and a larger margin i.e. 4\%, for localization. Importantly, we see that this preserves the accuracy for VQA, tipping the model over the $x=y$ line in Fig.~\ref{fig:main_figure}(b), improves localization by +0.6 points, and preserves the FID for text-to-image generation. These results show the benefits of incorporating consistency-based objectives while training GPV models.




\section{Conclusion}
We present a benchmark dataset, \dataset{}, to probe cross-task inconsistency in unified multimodal models and a loss objective to improve the same. Our results demonstrate that cross-task inconsistency is a significant issue in such models and can be mitigated with our proposed loss objective. We hope that \dataset{} serves as a useful resource for probing the reliability of unified multimodal models in the future.


\section*{Limitations \& Future Work}
\paragraph{Out-of-domain evaluation.} The \dataset{} benchmark is the first step (to the best of our knowledge) towards measuring consistency across tasks in multimodal models. It is derived from the COCO dataset \citep{lin2014microsoft} which has been widely used for pretraining multimodal models, hence, the data samples in \dataset{} are in-domain for these models. Despite \dataset{} being sampled from the same distribution as the training data of such models, we are seeing significant inconsistencies across tasks, especially those of different output modalities. We expect that these inconsistencies will aggravate when the samples are drawn from an out-of-domain distribution. Our work does not conduct out-of-domain evaluation, nevertheless, our automated dataset creation pipeline and evaluation framework provide the groundwork for preparing benchmarks similar to \dataset{} for out-of-domain-evaluation in future research.

\paragraph{Additional tasks.} Our evaluation and analysis are carried out on three tasks that do not represent the full spectrum of tasks in the multimodal space. However, as shown in Fig.~\ref{fig:main_figure}, this evaluation framework can be easily extended to more vision-and-language tasks. For instance, the consistency between image captioning and referring expression comprehension \citep{kazemzadeh2014referitgame} can be computed in the same way as done for localization. In this case, the semantic concepts appearing in the referring expression should appear in the caption, and one or more of those concepts should be replaced with contrast sets. The ground truth bounding box of the referring expression can be used to estimate likelihoods (as outlined for localization in Sec.~\ref{sec:evaluation}). Similarly, to compute cross-task consistency between image captioning and visual entailment \citep{xie2019visual}, the hypotheses that contain one or more semantic concepts overlapping with the caption should be perturbed with contrast sets. Further, our work capitalized on creating contrast sets based on text perturbations since they are easy to generate at scale and can be implemented for several vision-and-language tasks \citep{hu2022hand, gupta2022grit}. However, contrast sets based on image perturbations can provide important, additional insights into cross-task consistency of unified vision-language models. Advances in text-guided image-editing \citep{zhang2023sine, choi2023custom} can be leveraged to create such contrast sets at scale in future work.

\paragraph{Availability of cross-task annotations.} We chose the COCO dataset as a starting point for our benchmark because it is a popular dataset that has been widely used for the training and evaluation of vision-language models. The availability of annotations for multiple tasks served as a convenient starting point as it allowed us to create annotations for multiple tasks with the same pivot instance. However, cross-task consistency is defined for a \textit{pair of tasks} and is evaluated independently of the other tasks (see Sec.~\ref{sec:contrast_sets}). Hence, to evaluate cross-task consistency, we do not require multiple task annotations for the same pivot instance. Instead, we can have different sets of pivot instances with annotations for a different pair of tasks, which can be easily scaled to multiple tasks. Further, many contemporary models super-specialize in one or more tasks, such as Segment Anything for image segmentation \citep{kirillov2023segment}, Kosmos-2 for localization \citep{peng2024kosmos}, BLIP-2 for captioning and VQA \citep{li2023blip}. These models can be used to rapidly create annotations for a pair of tasks, and coupled with manual filtering, can be used for scaling to other datasets as well as tasks.

\paragraph{Aggregation of likelihood over semantically similar outputs.} Ideally, when computing likelihood for a textual output such as the caption \textit{A woman playing with her hair while sitting on a bed}, we should consider likelihoods for semantically similar captions as well e.g., those with invariant edits like \textit{her hair}$\rightarrow$\textit{hair} and \textit{a bed}$\rightarrow$\textit{bed}, paraphrases etc. This issue of considering highly similar outputs for likelihood estimation is a subset of a broader, open challenge of aggregating over semantically equivalent outputs during the evaluation of free-form natural language generations. We were mindful of this issue when creating the \dataset{} benchmark. To maintain uniformity in the evaluation setting, all contrast sets in \dataset{} are made of noun, verb, adjective, or preposition words only. Additional pronouns like \textit{her} in \textit{her hair} are neither replaced in the original caption nor added in the contrast sets if they are not significant semantic concepts. Some recent works propose methods for query or output expansion via mining, paraphrasing \citep{kuhn2022semantic}, multiple generations \citep{jiang2020can}, etc., and then ensemble over a pool of similar outputs to get an aggregate score estimate. Since this is an active area of research, we leave their integration with cross-task consistency evaluation to future work.


\section*{Broader Impact Statement}
The \dataset{} benchmark is designed to test the cross-task consistency of unified multimodal models. Our evaluation exposes inconsistencies in such models, indicating that the model outputs are not sufficiently reliable for real-world deployment. We anticipate that our work will influence further research on the important topic of stress testing of unified vision-language models in the community.


\bibliography{main}
\bibliographystyle{tmlr}

\appendix

\section*{Overview}
\noindent The Appendix is organized as follows:\\
\textbf{Section A}: Discussion on the relationship between cross-task consistency and task-specific accuracies, including results from simulation experiments.\\
\textbf{Section B}: Definitions of the categories in \dataset{}, additional examples from \dataset{} and examples of contrast sets generated using Multimodal Large Language Models (MLLMs).\\
\textbf{Section C}: Hyperparameters for training OFA$_{CON}$ model.\\
\textbf{Section D}: Additional results on model calibration, ablations from training of OFA$_{CON}$ and supporting figures.


\section{Relationship between Consistency and Accuracy}
\label{sec:discussion-appendix}

To understand the relationship between cross-task consistency and task-specific accuracies, we run a simulation experiment where we generate predictions from a hypothetical model for an anchor task and a target task under three different scenarios where the model makes: (A) independent errors on both tasks, (B) same errors on both tasks and (C) different errors on both tasks. See results in Fig.~\ref{fig:simulation} and a discussion below.


When a model has 100\% accuracy on the anchor task (see Sec.~\ref{sec:contrast_sets}), the cross-task consistency of the model (with respect to the anchor task and a target task) is equal to the model's accuracy on the target task, irrespective of the model's semantic alignment across tasks (see Fig.~\ref{fig:simulation}). However, when anchor task accuracy is less than 100\%, the outcome of a model's cross-task consistency ($C_{1}$) is closely tied with the nature of errors made by the model across tasks. If the model makes independent errors on anchor and target task, the cross-task consistency tends to stay close to 50\% (see Fig.~\ref{fig:simulation} A). If the model makes the same errors in both tasks, then the model's consistency generally remains high (see Fig.~\ref{fig:simulation} B) and it features above the $x=y$ line in Fig.~\ref{fig:results}(b). If the model makes different errors for both tasks, the model's consistency remains low (see Fig.~\ref{fig:simulation} C) and it features below the $x=y$ line in Fig.~\ref{fig:results}(b). The exceptions to these scenarios are when the anchor and target accuracies are either 0\% or 100\%. In such cases, the cross-task consistency is 100\% if the accuracies of both tasks are the same, and 0\% otherwise.




\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/simulation.pdf}

   \caption{\textbf{Simulation of the relationship between pivot task accuracy and target task accuracy} under the scenarios where a model makes (a) independent errors in both tasks, (b) same errors in both tasks, and (c) different errors in both tasks.}
   \label{fig:simulation}
   \vspace{-0.1in}
\end{figure*}

\section{The \dataset{} Benchmark}
\label{sec:dataset-appendix}

See a selection of additional examples from the \dataset{} benchmark in Fig.~\ref{fig:more_examples} and an explanation as well as a breakdown of the various contrast set categories in \dataset{} in Tab.~\ref{tab:dataset}.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/cococon_supp_v2_tmlr.pdf}
   \caption{\textbf{Additional examples of contrast sets in \dataset{}.} For each example, we show the relevant image (left), the ground truth caption, VQA question, or image generation prompt for the image with the perturbed concept in green (middle) and the set of perturbations used to generate alternative answers.}
   \label{fig:more_examples}
\end{figure*}

\begin{table*}
  \caption{Definition of \dataset{} categories and dataset statistics.}
  \vspace{6pt}
  \label{tab:dataset}
\footnotesize
% \small
  \centering
  %{|p{3.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|}
\begin{tabular}{|p{1.5cm}|p{8.5cm}|p{1.8cm}|p{1.9cm}|}
    \hline
    \multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Description}} & \multirow{2}{*}{\textbf{\# Samples}} & \textbf{\# Unique contrast sets}  \\
    \hline
    Object & All inanimate objects excluding food items. & 388 & 648 \\
    Attribute & Adjectives used as modifiers to a noun e.g., color (\textit{red} chair), height (\textit{tall} building), size (\textit{small}), material (\textit{tiled} wall), etc. & 314 & 221 \\
    Food & Food items including fruits, vegetables, and other cooked items. & 231 & 409 \\
    Animal & Includes all mentions of animals, predominantly those featured in COCO objects. & 139 & 177 \\
    Location & Includes broadly defined areas (e.g., \textit{bathroom}, \textit{hotel}, \textit{library}), finer visual elements (e.g.,
\textit{floor}, \textit{sidewalk}), and spatial references (e.g., \textit{inside}, \textit{outside}, \textit{on table}). & 111 & 143 \\
    Role & Includes professional roles such as \textit{chef}, \textit{baseball player}, etc. & 109 & 74 \\
    Action & Comprises transitive (e.g. \textit{flying kite}) as well as intransitive actions (e.g. \textit{sitting}, \textit{standing})
performed by persons and animals. & 63 & 117 \\
    Person & Concepts from one of the following: \textit{man}/\textit{male}/\textit{guy}, \textit{woman}/\textit{female}/\textit{lady}, \textit{boy}, \textit{girl}. & 47 & 16 \\
    OCR & Texts present in the image e.g., writing on a cake, numbers on a digital clock, billboard, etc. & 43 & 132 \\
    Misc. & All other minor sub-categories e.g., weather, direction, etc.   & 55 & 116 \\
    \hline
    Overall & - & 1500 & 1820 \\
    \hline
  \end{tabular}
\end{table*}

\subsection{Generating Contrast Sets using Multimodal Large Language Models}
\label{ssec:mllms-appendix}

As we note in Sec.~\ref{sec:dataset}, our work on the automated pipeline for generating contrast sets at scale precedes recent advances in open-source multimodal large language models (MLLMs) like LLaVA1.5 \citep{liu2023improved}. Fortunately, the multi-step process outlined in Sec.~\ref{sec:dataset} can now be replaced with a single inference step using a state-of-art MLLM as we show below. To compare the diversity of contrast sets generated using our method to those generated using MLLMs, we perform a small experiment using LLaVA 1.5 \citep{liu2023improved} where we generate contrast set candidates for the examples demonstrated in Fig.~\ref{fig:examples} by prompting the model to replace semantic concepts within captions directly. We use the following prompt and provide the corresponding image as input:

\texttt{Based on the image, write 5 different answers for filling in the blank in the following image caption: [caption with blank placeholder ]. For each answer, write your confidence in how well the answer fits into the caption while staying true to the image.}


The results are presented in Tab.~\ref{tab:llava-outputs}. Overall, we observe similar diversity in the contrast sets generated using VQA vs. image caption. This suggests that the multi-step contrast set generation pipeline in Sec.~\ref{sec:dataset} can be significantly simplified with the use of MLLMs and suitable prompt engineering, and can be potentially used for extending the \dataset{} benchmark to other vision-language tasks in a scalable manner.

\begin{table*}
  \caption{\textbf{Examples of contrast sets generated using LLaVA1.5 \citep{liu2023improved}}. Comparison of contrast sets generated using our method in Sec.~\ref{sec:dataset} to those generated using a single-step inference from the LLaVA1.5 model. Concepts replaced in the caption are \textit{emphasized}. The comparable diversity of both sets of contrast sets suggests that our multi-step pipeline can be significantly simplified with the use of MLLMs.}
  \vspace{6pt}
  \label{tab:llava-outputs}
\footnotesize
% \small
  \centering
  %{|p{3.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|}
\begin{tabular}{|p{6cm}|p{4.5cm}|p{4.5cm}|}
% \begin{tabular}{|c|c|c|}
    \hline
    \multirow{2}{*}{\textbf{Original Caption}} & \textbf{\dataset{} contrast sets} &\textbf{LLaVA 1.5 contrast sets} \\
    & (using VQA) & (using caption)\\
    \hline
    A child in a bed with a striped sweater and a colorful \textit{blanket}. & stuffed animal, pillow, teddy bear & stuffed animal, blanket, pillow, teddy bear  \\
    A brown and \textit{white} cat lying on the bed. & yellow, black, grey, orange & black, orange, tan, cream \\
    \textit{Apples} are hanging from a tree that has hardly any leaves on it. & pear, olive, orange, cantaloupe & oranges, pears, peaches, plums \\
    Office space with \textit{cat} on the television and work. & cartoon, squirrel, baseball, butterfly, bowling & - \\
    A man holding out to catch a \textit{baseball}. & tee ball, basket ball, softball, football & frisbee, soccerball, tennis ball, golf ball \\
    A giant \textit{colgate} clock sits on the shore next to water. & walgreens, billboard, state farm, wrigleys, yves saint lauren & coca-cola, clocktower, clockface \\
    \hline
  \end{tabular}
\end{table*}







\section{Training Hyperparameters}
\label{sec:hyperparams-appendix}
The complete hyperparameters for training OFA$_{Con}$ using the rank correlation-based loss objective are available in Tab.~\ref{tab:hyperparams}.

\begin{table}
\small
  \caption{Hyperparameters for training OFA$_{Con}$.}
  \label{tab:hyperparams}
  \vspace{2pt}
  \centering
  \begin{tabular}{|p{5.5cm}|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Value}\\
\hline
 Proportion of ranking updates $(\gamma)$ & 0.5 \\
 Weight co-efficient of ranking loss $(\lambda)$ & 0.25 \\
 Regularization strength of soft ranking & 1.0 \\
 Learning rate & 1e-6 \\
 Max. train epochs & 1 \\
 Batch Size & 2 \\
 Warmup ratio & 0.1\\
 Label smoothing & 0.0 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Additional Results}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/tmlr_rebuttal_calibration.pdf}

   \caption{\textbf{Temperature calibration \citep{guo2017calibration} of the likelihood scores from Unified-IO$_{SMALL}$ model for the text-to-image generation task.} We measure the slope ($m$) and mean squared error (MSE) of the best linear regression fit to the temperature-calibrated likelihoods, and compare for different values of temperature ($t$). We do not observe significant differences in correlation strength ($m$) or linear fit (MSE) at values of $t$ other than 1, which is the default in our evaluation of the \dataset{} benchmark.}
   \label{fig:calibration}
\end{figure*}

\subsection{Calibration of Model Likelihoods for Text-to-image Generation}
\label{ssec:calibration-appendix}
In Sec.~\ref{sec:t2i_calibration}, we note that we perform temperature calibration \citep{guo2017calibration} of Unified-IO$_{SMALL}$ to examine whether it improves the calibration of the model likelihoods for outputs from the text-to-image generation task. See results in Fig.~\ref{fig:calibration}. We observe that the correlation does not improve significantly with different temperature values. We also compare the distribution of output likelihoods for various tasks and all model sizes of Unified-IO in Fig.~\ref{fig:likelihoods}. While we see different distributions for outputs from pivot instances and contrast instances for the VQA and localization tasks, we observe nearly complete overlap in the distributions of outputs for the text-to-image generation task, further supporting our conclusion that unified vision-language models do not have a unified semantic backbone across all supported tasks.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/tmlr_rebuttal_dsitribution.pdf}

   \caption{\textbf{Comparison of distribution of Unified-IO model likelihoods} for \textit{pivot} and \textit{contrast} instances, for various tasks in the \dataset{} benchmark and all model sizes.}
   \label{fig:likelihoods}
\end{figure*}





\subsection{Ablation Results \& Examples}
We present results from the ablation of the weight co-efficient ($\lambda$) hyperparameter for the consistency-based loss objective in Tab.~\ref{tab:ablation_results}. We observe that a higher $\lambda$ hurts accuracy while a lower $\lambda$ does not improve consistency. We also present examples where OFA$_{CON}$ is more consistent than the pretrained OFA$_{LARGE}$ in Fig.~\ref{fig:supp_example}.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.94\textwidth]{figures/cococon_ofacon_supp.pdf}

   \caption{\textbf{Examples from the \dataset{} benchmark where OFA$_{CON}$ is more consistent than pretrained OFA$_{LARGE}$.} For each example, we show the relevant image (left), the ground truth caption, VQA question, or image generation prompt for the image with the perturbed concept in green (middle), the set of perturbations used to generate alternative answers and predictions from OFA$_{LARGE}$ and OFA$_{CON}$ for VQA (V), image generation (G) and localization (L) (right columns). \textcolor{green}{\checkmark} and \textcolor{red}{$\times$} indicate scenarios where the model predictions for captioning and the corresponding task for that particular contrast set are consistent and inconsistent respectively. `\textbf{-}' denotes a lack of localization annotations for the given sample.
}
   \label{fig:supp_example}
\end{figure*}

\begin{table*}
\footnotesize
% \small
  \caption{\textbf{Results from ablation of the weight co-efficient ($\lambda$) for training of OFA$_{CON}$.} Metrics are task-specific accuracies, \% consistency ($k=1$) and Spearman's rank correlation coefficient ($\rho_{rank}$). Higher is better for all metrics except FID.}
  \label{tab:ablation_results}
  \vspace{2pt}
  \centering
  \begin{tabular}{|p{3.0cm}|p{1.0cm}|p{1.5cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Params}} & \textbf{Captioning} & \multicolumn{3}{c|}{\textbf{\multirow{1}{*}{\textbf{VQA}}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Localization}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Text-to-Image Gen.}}} \\
    % \hline
    & &  \textbf{CIDEr} & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{FID}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ \\
\hline
\multicolumn{12}{|c|}{\textbf{\textbf{Consistency-based Training}}} \\
\hline
OFA$_{CON}$ ($\lambda=0.0$) & 472M & 118.8 & \textbf{82.7} & 81.1 & 0.63 & 73.5 & 65.9 & 0.27 & \textbf{98.5} & 51.7 & 0.04 \\
OFA$_{CON}$ ($\lambda=0.25$) & 472M & \textbf{119.4} & 82.4 & 83.8 & 0.67 & \textbf{74.1} & 69.5 & 0.35 & 99.1 & 53.8 & \textbf{0.09} \\
OFA$_{CON}$ ($\lambda=0.50$) & 472M & 117.8 & 81.8 & \textbf{84.2} & \textbf{0.70} & 73.1 & \textbf{69.9} & 0.35 & 99.3 & \textbf{54.1} & 0.08 \\
    \hline
  \end{tabular}
\end{table*}


\subsection{Consistency vs. Accuracy at Various Difficulty (k)}
We compare the target task accuracies of all models evaluated in our experiments with the cross-task consistency at difficulty $k=1$ ($C_{1}$) in Fig.~\ref{fig:results}(b) in the main text. To further support our analysis, we also present similar plots at decreasing difficulty levels i.e. $k=2,3,4,5$, in Fig.~\ref{fig:scatter}. With decreasing difficulty or for easier contrast sets, models gradually transition over the $x=y$ line which implies that models are generally more consistent than accurate at easier contrast sets.

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/tmlr_rebuttal_scatter_k.pdf}

   \caption{\textbf{Comparison of \% accuracy with \% consistency values} for all models evaluated in this paper and our OFA$_{CON}$ model (see Sec.~\ref{sec:training_obj}) for decreasing difficulty of contrast sets ($k$=2,3,4,5). See Fig.~\ref{fig:results}(b) for performance on the hardest contrast sets i.e., $k=1$.}
   \label{fig:scatter}
   \vspace{-0.1in}
\end{figure*}



\end{document}
