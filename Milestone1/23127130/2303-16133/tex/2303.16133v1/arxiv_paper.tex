\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage[export]{adjustbox}
\usepackage{array}
\usepackage{multirow}
\usepackage{xspace}

\newcommand{\dataset}{\textsc{CocoCon}\xspace}
\newcommand{\uio}{Unified-IO\xspace}
\newcommand{\ofa}{OFA\xspace}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% \usepackage{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Exposing and Addressing Cross-Task Inconsistency\\ in Unified Vision-Language Models}

\author{ Adyasha Maharana\textsuperscript{1} \;\; Amita Kamath\textsuperscript{2} \;\; Christopher Clark\textsuperscript{2} \;\; Mohit Bansal\textsuperscript{1} \;\; Aniruddha Kembhavi\textsuperscript{2} \\
\textsuperscript{1}UNC Chapel Hill \,\,\,\, \textsuperscript{2}Allen Institute for AI\\
{\tt \normalsize \href{https://adymaharana.github.io/cococon/}{adymaharana.github.io/cococon/}}}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs.
Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, \dataset{}, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially for more heterogeneous tasks. Finally, we propose using a rank correlation-based auxiliary objective computed over large automatically created cross-task contrast sets to improve the multi-task consistency of large unified models, while retaining their original accuracy on downstream tasks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
  \centering
   \includegraphics[width=0.47\textwidth]{figures/Figure1_arxiv.pdf}

   \caption{Examples of (a) consistent and (b) inconsistent predictions from \uio{}$_{XL}$~\cite{lu2022unified}.
}
\vspace{-0.2in}
   \label{fig:intro_example}
\end{figure}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.95\linewidth]{figures/main_figure_arxiv.pdf}

   \caption{Illustration of our method for probing inconsistencies across tasks. We build candidate answers for multiple tasks which correspond to different semantic understandings of an image (e.g., if the object is a keyboard or laptop), and check whether the model's preferred answers across tasks match the same semantic understanding.
}
\vspace{-0.2in}
   \label{fig:main_figure}
\end{figure*}

General Purpose Vision (GPV) models~\cite{gupta2021towards,kamath2022webly,cho2021unifying,lu2022unified,wang2022ofa} are trained to perform many diverse multimodal tasks ranging from visual question answering (VQA) and referring expression grounding to semantic segmentation and image generation. A fundamental requirement and intuitive expectation of such systems is that they provide consistent results across the tasks they support. For example, if a system produces the caption \emph{two jaguars are sitting on a tree branch} then one would expect it to answer the question \emph{What animals are these?} with \emph{jaguars} and to return two bounding boxes if asked to locate the \emph{jaguars}.


While the latest GPV models~\cite{lu2022unified,wang2022ofa} perform impressively on multi-task benchmarks~\cite{grit}, we find that these models can provide surprisingly inconsistent answers for simple images and tasks. Fig~\ref{fig:intro_example}(a) shows an example where \uio{}$_{XL}$~\cite{lu2022unified} produces the caption: \emph{A woman riding skis down a snow covered slope.} but then answers \emph{snowboarding} when asked: \emph{What is the person doing?} Solving multiple tasks for one image may require some degree of specialized reasoning, but they necessitate a semantic interpretation of the input image which should be common across tasks. When models demonstrate such trivial inconsistencies, it is hard for end users to trust them, particularly in important applications, because its harder to understand and predict their behavior. From a practical standpoint, it is challenging to incorporate such models into larger systems, because it's hard to depend on and calibrate for them. Finally, from a philosophical view, having different interpretations of an image depending on the target task defies how we intuitively think unified models should behave.

In computer vision, cross-task consistency has been of some interest for classical tasks \cite{zamir2020robust}, while in natural language processing past work has studied consistency between tasks like question-answering~\cite{kassner2021beliefbank}. However, in vision-and-language research, much work has focused on within-task consistency for visual question answering \cite{shah2019cycle, ribeiro2019red, dharur2020sort, ray2019sunny, bitton2021automatic}. Semantic consistency of multi-modal models \textit{across tasks} has remained unexplored, partly due to the absence of models (until recently) that can perform various tasks simultaneously and effectively.



With recent advances in GPV research, we can now probe models for cross-task consistency. A simple and straightforward method is to compute the semantic overlap between a model's predictions for the same image across tasks. While possible for related tasks like captioning and VQA, measuring semantic overlap between outputs from different modalities can be ill-defined (e.g. it is unclear how to quantify the overlap between bounding boxes for localization and an answer for VQA). Additionally, models may perform well by producing simple outputs for tasks. For example, if a model generates short captions about a single subject, this method can only probe consistency for that narrow set of visual elements. Instead, we choose to utilize human-defined outputs for tasks that cover a wide range of semantic elements for a complete evaluation of consistency. For a given pair of tasks, we perturb the test instances in similar but meaningful ways that change the gold label, in order to create contrast sets \cite{gardner-etal-2020-evaluating}. More likely perturbations (e.g. \textit{keyboard} $\rightarrow$ \textit{laptop} in Fig.~\ref{fig:intro_example}(b)) lead to harder contrast sets whereas less likely perturbations (e.g. \textit{keyboard} $\rightarrow$ \textit{earbuds}) lead to easier contrast sets. Then, we measure a model's likelihood of predicting the ground truths as well as their contrast counterparts for both tasks. If a model is more likely to predict the contrast output for one task and the ground truth output for the other task
or vice-versa, it implies that the model has contradicting interpretations of the same input for the two tasks. In the example shown in Fig.~\ref{fig:main_figure}, the model favors the caption with \emph{computer keyboard}, but is more likely to answer \emph{laptop} in response to the question: \emph{What is the object in the lower right-hand corner?}, leading to cross-task inconsistency. Operating with likelihoods also allows us to overcome the challenges of comparing outputs from two different modalities.



For this purpose, we present \dataset, a benchmark dataset with contrast sets for four commonly used multimodal tasks. Each sample in \dataset{} contains up to five contrast sets of varying difficulty for each of the tasks. We use image captioning as an \textit{anchor task} because captions contain semantic elements used by most other tasks and evaluate it against VQA which has textual outputs, localization which has bounding box outputs and image generation with image outputs. This covers task pairs with outputs in same output modalities as well as different modalities. We measure consistency \% as well as Spearman's rank correlation coefficient between the ranking of contrast sets.

We evaluate two recent GPV models, \uio \cite{lu2022unified} and \ofa \cite{wang2022ofa}, both of which support all four tasks in \dataset. We show that cross-task inconsistency is a surprisingly significant phenomenon in these models across all tasks in \dataset{} and various model sizes. Inconsistency increases with the heterogeneity between output modalities within a pair of tasks as well as with the complexity of the tasks themselves. Moreover, consistency improves with easier contrast sets, yet remains significantly less than 100\% for all tasks. We also find that larger models are more consistent by virtue of being more accurate at the tasks. Finally, our evaluation suggests that multi-task models capable of performing a larger set of tasks are more inconsistent.


Cross-task inconsistency is undesirable in a unified model, and it is paramount that we work toward mitigating it. To this end, we propose using a consistency objective utilizing large automatically generated cross-task contrast sets and a rank correlation loss objective via soft ranking \cite{blondel2020fast}. Our experiments show that continued training of models using this auxiliary consistency-based objective can lead to consistency improvements when evaluated on \dataset, while preserving or improving the accuracy of the model on the original test sets.

In summary, our contributions include: (a) highlighting the issue of cross-task inconsistency in multi-modal models, (b) introducing the use of contrast sets and a benchmark dataset, \dataset\ to measure this amongst four popular multimodal tasks, (c) demonstrating the inconsistent behavior of state-of-the-art GPV models, and (d) a consistency-based training objective to improve consistency without compromising accuracy.


\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/Dataset_Construction_Figure_v3.pdf}

   \caption{Step-by-step demonstration of the automated pipeline for generating contrast sets. Contrast sets generated from this pipeline for the validation split of COCO are subjected to manual filtering and then used to prepare the \dataset{} benchmark.}
   \label{fig:construction}
   \vspace{-0.2in}
\end{figure*}

\section{Related Work}
To our knowledge, no existing work evaluates cross-task consistency for multi-modal models. In this section, we discuss studies that evaluate and enforce consistency for individual tasks, or multiple tasks within one modality.

\textbf{Consistency for VQA.}
\cite{shah2019cycle} revealed that VQA models are inconsistent across linguistic variations of a visual question, then improved consistency using automatic data augmentation; an approach which was further improved in \cite{Kant2021ContrastAC} using an additional contrastive loss. \cite{ribeiro2019red, ray2019sunny} evaluated consistency across the original QA data and automatically generated QA pairs implied by this data. \cite{Selvaraju2020SQuINTingAV} collected human-annotated sub-questions to evaluate model reasoning capabilities through the lens of consistency. \cite{dharur2020sort} train models to rank the sub-questions proposed by SQUiNT \cite{Selvaraju2020SQuINTingAV} higher than unrelated questions from the same image, making models more consistent across both sub-questions and rephrasings of the question. Contrastive sets have also been used to measure and improve consistency for VQA~\cite{ribeiro2019red, bitton2021automatic}. Unlike these works, our approach evaluates and improves consistency across multiple tasks.






\textbf{Consistency for NLP.}
Consistency has also been discussed in NLP, primarily in the single-task setting. \cite{elazar-etal-2021-measuring} evaluate and improve factual consistency of pre-trained LMs across paraphrasings of factual statements. \cite{kassner2021beliefbank} consider the responses of a pre-trained LM to a stream of questions, and evaluate and improve the consistency and accuracy of its answers over time. \cite{kaushik2019learning} collect counterfactual instances to evaluate the overreliance of NLP models on spurious attributes. \cite{gardner-etal-2020-evaluating} manually create contrast sets for 10 individual NLP tasks to evaluate single-task consistent responses across meaning-altering perturbations. In comparison to these works, we evaluate consistency across multiple tasks, without the need for human annotations as used in \cite{gardner-etal-2020-evaluating}. \cite{nishino-etal-2019-keeping} use multi-task learning with a hierarchical consistency objective to predict the headlines, key phrases, and categories of articles; however, the model uses separate decoders per task. Our work studies cross-task consistency of General Purpose Vision (GPV) models with unified output decoders.

\textbf{Cross-task Consistency for Vision.}
Cross-task relationships among classic vision tasks have been studied by \cite{Zamir2018TaskonomyDT}. \cite{lu2021taskology} use geometry and physics to identify consistency constraints between such tasks, and use them to improve performance in low data regimes. \cite{zamir2020robust} enforce cross-task consistency for vision tasks using inference-path invariance and demonstrate their method for tasks in the pixel space (like depth and surface normals). It is not straightforward to extend this approach to vision and language tasks which are often conditioned not just on an image but also on a language input and where one task's output may not easily be transformed into another's output.


\section{Contrast Sets for Cross-Task Consistency}
\label{sec:contrast_sets}
In this section, we describe the problem of inconsistency across tasks in unified models, motivate the use of contrast sets to evaluate consistency, and outline our framework for measuring cross-task consistency using contrast sets.

\noindent \textbf{The Problem.}
In the pursuit of developing task- and modality-agnostic unified systems, models like \uio \cite{lu2022unified} are trained on a variety of tasks geared towards learning robust semantic representations of the input. Each task is designed to strengthen the model's understanding of a distinct perspective of the ground truth. For instance, a visuo-linguistic model is simultaneously trained to generate a caption for the entire image as well as answer questions about subjects in the image. The popular and effective training paradigm for such models is to learn a probability distribution over the space of possible outputs and maximize the likelihood of the target output. This leads to an inherent ranking of possible outputs based on their probabilities, which can be used to rank outputs that reflect distinct semantic understandings of the input. For a reliable and truly unified model, the ranked space of such probable outputs should also be aligned \emph{across tasks}. However, (see Figure~\ref{fig:intro_example}), we find that unified models can interpret inputs differently for different tasks, leading to misalignment between these spaces and inconsistency in predictions. We measure this inconsistency with the help of contrast sets.

\noindent \textbf{Contrast Sets.} Model performances on the i.i.d. test data are often treated as an absolute measurement of its abilities. However, when the test data has systematic gaps like annotation artifacts \cite{gururangan2018annotation}, the model can learn simple decision boundaries to solve the dataset and result in misleading high performances. Gardner et al. \cite{gardner-etal-2020-evaluating} introduce contrast sets to close such systematic gaps in evaluation. Contrast sets are created by perturbing test instances in meaningful ways that change the gold label. This allows for the evaluation of a model's local decision boundary around a \textit{pivot} test instance and measurement of how well it \textit{aligns with the correct decision boundary}. Models with simple decision boundaries fail to perform well on contrast sets. Using the same intuition, we can create equivalent perturbations on a test instance for two tasks and evaluate whether the unified model performs similarly on the contrast set for either task. In this manner, we leverage the framework of contrast sets to measure how well a model's decision boundaries for two distinct tasks \textit{align with each other}.

Consider a model with parameters $\theta$ and two tasks $t_0$, $t_1$ that can be performed by the model. In order to construct a contrast set, we first pick a test instance and the respective ground truth annotations for each task i.e. $(x_{t_0}, y_{t_0})$, $(x_{t_1}, y_{t_1})$, termed as the \textbf{pivot} instances. We define the space of contrast outputs for an instance $x$ as the set of outputs $\tilde{y}$ that are within some distance $\epsilon$ of $y$. That is, $C(x) = \{(\tilde{y}\,|\,d(y, \tilde{y})\,< \,\epsilon\}$, where $d(.)$ is some distance function. Let $f_{\theta}(y)$ be the likelihood of model $\theta$ for predicting the output $y$ in response to input $x$. Now, we define the model $\theta$ to be consistent across tasks $t_0, t_1$ with respect to the pivots $x_{t_0}, x_{t_1}$ if the model is more likely to predict the gold outputs $y_{t_0}, y_{t_1}$ in both tasks, as compared to their respective contrast outputs $\tilde{y}_{t_0}, \tilde{y}_{t_1}$. The model is also considered consistent if it assigns a larger likelihood to the contrast outputs than the gold outputs of both tasks because, even if the model answers wrongly for both tasks, as long as it reflects a common understanding of the input, the model is consistent by definition. Mathematically,
\begin{equation}
% \vspace{-0.1in}
    \mathcal{C} =
    \begin{cases}
      1 & \text{if  $f_{\theta}(y_{t_0}) > f_{\theta}(\tilde{y}_{t_0})\,\,\land\,\,f_{\theta}(y_{t_1})> f_{\theta}(\tilde{y}_{t_1})$}\\
        1 & \text{if  $f_{\theta}(y_{t_0}) < f_{\theta}(\tilde{y}_{t_0})\,\,\land\,\,f_{\theta}(y_{t_1}) < f_{\theta}(\tilde{y}_{t_1})$}\\
      0 & \text{otherwise}
    \end{cases}
    \label{eqn:consistency_eval}
% \vspace{-0.1in}
\end{equation}
where $\tilde{y}_{t_0}\in C(x_{t_0})$, $\tilde{y}_{t_1}\in C(x_{t_1})$ and $\mathcal{C}$ is the consistency score. This framework can be easily extended to more than two tasks. For the scenario of $>2$ tasks, we define an \textbf{anchor task} $t_0$, that contains semantic elements common to each of the remaining tasks. Then, we compute pairwise consistency scores for the anchor and the rest of the tasks $\{t_1, t_2,\ldots,t_{T}\}$ i.e. we have $T$ pairwise scores for $T$ tasks.

\noindent \textbf{Difficulty ($k$).} Contrast sets can be of varying difficulty, depending on the likelihood of the perturbations used to create the contrast sets. For example, \textit{basketball} is a likelier substitute for the semantic concept \textit{football} whereas \textit{kite} is much less likely. Hence, the contrast set containing \textit{basketball} is a \textbf{hard} contrast set and the one containing \textit{kite} is an \textbf{easy} contrast set. We indicate the difficulty with $k$ i.e. lower $k$ implies harder contrast sets.

\noindent \textbf{Evaluation Metrics.} We introduce two metrics for calculating the consistency of a model over a dataset of $N$ contrast sets for $T$ tasks. Each sample consists of pivot instances for the $T$ tasks and the corresponding sets of up to $K$ contrast outputs. For each task $t_i$, we first rank the $K$ contrast sets by difficulty according to the model's likelihoods for the anchor task, $\{\tilde{y}_{t_0}^1,\ldots \tilde{y}_{t_0}^K\}$. At each $k$, we compute \textbf{\% consistency@$k$ ($\mathcal{C}_k)$} as the \% of samples for which the model is inconsistent i.e. ,
 \begin{equation}
     \mathcal{C}_{k} = \frac{1}{N}{\sum_{i=1}^{N}{\mathcal{C}(y_{t_0}, y_{t_i}, \tilde{y}_{t_0}^k, \tilde{y}_{t_i}^k)}}
 \end{equation}
where consistency $\mathcal{C}(.)$ is computed as per Eqn.~\ref{eqn:consistency_eval}. Higher values for $\mathcal{C}_{k}$ suggest that the model is more consistent.  This metric measures consistency with respect to the ground truth annotations, which are used as pivots in our setup. We also compute \textbf{\texttt{spearmanr} $(\rho_{rank})$}, the Spearman's rank correlation coefficient over the ranked contrast outputs for both tasks, in order to measure the global alignment between the two output spaces. We observe these metrics in tandem with task-specific accuracies to avoid overestimating a model with degenerate but consistent solutions.


\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.97\linewidth]{figures/cococon_supp_v2.pdf}
   \caption{Examples of contrastive sets used in \dataset{}. For each example, we show the relevant image (left), the ground truth caption, VQA question, or image generation prompt for the image with the perturbed concept in green (middle), the set of perturbations used to generate alternative answers and predictions from \uio$_{XL}$ for VQA (V), image generation (G) and localization (L) (right columns). \textcolor{green}{\checkmark} and \textcolor{red}{$\times$} indicate scenarios where the model predictions for captioning and the corresponding task for that particular contrast set are consistent and inconsistent respectively. `\textbf{-}' denotes a lack of localization annotations for the given sample.}
   \label{fig:examples}
   \vspace{-0.2in}
\end{figure*}

\section{The \dataset\ Benchmark}
\label{sec:dataset}
We now detail the construction and composition of our benchmark dataset \dataset{}, which has been developed as per the framework outlined in Sec.~\ref{sec:contrast_sets}.

\noindent \textbf{Dataset Construction.} The COCO dataset \cite{lin2014microsoft} contains annotations for many tasks in vision and language, which makes it very suitable for the purpose of evaluating cross-task consistency in a multimodal model. \dataset\ was created from the validation splits of each of the four tasks i.e. image captioning (which serves as the \textbf{anchor task}), VQA \cite{goyal2017making}, localization, and text-to-image generation. First, we select \textbf{pivot instances} for the captioning and VQA tasks.

Existing captioning annotations for the COCO dataset were filtered to retain ones that had semantic overlap with at least one question-answer pair from VQAv2 annotations. For instance, the caption: \emph{The woman is jumping in the air to catch the frisbee.} from COCO overlaps with the VQA sample: \emph{What is she playing? frisbee} (see Fig.~\ref{fig:construction}, Step 1) and was retained in our method. The semantic overlap was computed using a series of text-processing steps including lemmatization and word overlap. Next, we need to substitute the overlapping semantic concept with other likely concepts to create contrast sets. To this end, we prepare perturbations for the overlapping semantic concepts in the next step. There are many ways to perform this step. For instance, these perturbations can be written by human annotators, which might result in undesirable systematic biases in the contrast sets \cite{gururangan2018annotation}. Language models like GPT3 \cite{brown2020language} can be trained to generate suitable perturbations using in-context learning, but it can be expensive to secure such data at scale. Adversarial methods advocate gradient-based methods to get hard negatives for such perturbations \cite{alzantot2018generating}, however, we want to avoid integrating the biases of the models we are evaluating into a benchmark dataset.

In contrast, we choose to use probable answers to the VQA questions from an off the shelf VQA model, GPV-2, \cite{kamath2022webly} to create a large set of perturbations (see Fig.~\ref{fig:construction}, Step 2). Note that we do not evaluate GPV-2 for consistency since it can perform only a subset of the tasks covered in \dataset{} (see Fig.~\ref{fig:main_figure}). These perturbations are filtered to retain high-quality candidates only by creating contrast captions and retaining only those captions (and the corresponding contrast VQA samples) with high scores from the T5 language model i.e., the ungrammatical and nonsensical captions are filtered out. 

For instance, in Fig.~\ref{fig:construction} (see Step 3), the GPV2 answer \emph{hide-and-seek} is filtered out using T5 score, because \emph{catch the hide-and-seek} is an unlikely phrase. The next step is to add more evaluation tasks i.e., localization and text-to-image generation (see Fig.~\ref{fig:construction}, Step 4).

For the localization task, the automatically generated dataset in the first step was merged with the COCO localization annotations. Annotations for localization in COCO images pertain to the narrow set of pre-defined COCO objects, which may or may not appear in the caption. Only those objects which appeared in the caption were retained in \dataset\ for the localization task.  

Finally, since image captioning is the task of generating a natural language description from an image, and text-to-image generation is the reverse process, one can reuse the ground truth annotations and contrasting annotations of captions for the task of image generation by simply reversing them. This generated dataset was then subject to manual filtering and editing to ensure the high quality of the contrast sets. In this step, contrast sets that were synonyms, holonyms, hypernyms, or meronyms were removed from the dataset, in addition to other invalid perturbations. We prioritized the collection of clean, expert annotations over size for this probing dataset. Note that the contrast sets were manually filtered to ensure high quality at test, but at training time we only use automatically generated data.

The \dataset\ dataset comprises contrast sets for 1500 instances from the COCO validation split, with an average of 3.2 contrast sets per sample and 4789 annotations in total. Each sample contains annotations for image captioning, VQA, and text-to-image generation, and 30\% of these samples also contain annotations for localization.\footnote{Localization annotations are present when a COCO object appears in the gold caption.} The semantic concepts used for perturbing the pivot instances in this dataset range from a large variety of semantic, syntactic, and grounding phenomena. We labeled each sample from \dataset\ for these phenomena, see examples in Figure~\ref{fig:examples} and a breakdown of the distribution in Figure~\ref{fig:distribution}. We conducted a study for inter-annotator agreement between two expert annotators on 200 samples and found agreement for 98\% of the data, indicating the high quality of the dataset.

\paragraph{Evaluation.} We measure the consistency between the captioning task (anchor) and each of the evaluation tasks independently. To evaluate consistency between captioning and VQA tasks, we compare the models' likelihoods of generating the caption and the VQA answer for both, pivot and contrast instances. For the localization and text-to-image generation tasks, the outputs are common to both, pivot and contrast instances, whereas the inputs contain semantic perturbations (see Fig.~\ref{fig:construction}, Step 4). Hence, we compare the models' likelihood of generating the output in response to the input from the pivot instance vs. the input from the contrast instance. For example, we compare models' likelihood of generating the ground truth image in response to the gold caption and the contrast caption (e.g. caption containing \emph{frisbee} vs. \emph{football} in Fig.~\ref{fig:construction}) for the text-to-image generation task.\\


\begin{algorithm}
\footnotesize
	\caption{Cross-Task Consistency-based Training}
	\begin{algorithmic}[1]
         \State $\gamma \leftarrow$ ratio of consistency-based updates to total updates
        \State $\lambda \leftarrow$ weight co-efficient for consistency-based loss
        \State $t_0, [t_1, t_2, t_3]  \leftarrow$ anchor task (e.g. captioning), evaluation tasks
        % \State $ \leftarrow$ other tasks (e.g. VQA, text2img, localization)
        \State $(x_{t_{i}}, y_{t_{i}}, \{\tilde{y}_{t_{i}}\}) \leftarrow$ input, gold output and contrast outputs for for task $t_i$
		\For {$epoch=1,2,\ldots,N$}
			\For {$step=1,2,\ldots,M$}
                \State $r \leftarrow \texttt{random(0, 1)}$
                \If {$r \le \gamma$}
                    \State $i \leftarrow \texttt{random(1, 2, 3)}$
		          \State Anchor task: $(X_{t_0}, Y_{t_0}, \{\tilde{Y}_{t_0}\}) \leftarrow (x_{t_0}, y_{t_0}, \{\tilde{y}_{t_0}\})$
            		\State Evaluation task: $(X_{t_i}, Y_{t_i}, \{\tilde{Y}_{t_i}\}) \leftarrow (x_{t_i}, y_{t_i}, \{\tilde{y}_{t_i}\})$
                    \State Cross-entropy losses: $\{L_{ce}^0\}, \{L_{ce}^i\}$
                    \State Ranks: $R_{0}, R_{i} \leftarrow \texttt{rank}(\{L_{ce}^0\}), \texttt{rank}(\{L_{ce}^i\})$
                    \State $L_{rank} \leftarrow \texttt{spearmanr}(R_{0}, R_{i})$
                    \State $L \leftarrow \lambda\,*\,L_{rank}\,+\,L_{ce}$
                \Else {}
                    \State Standard pretraining data: $(X, Y) \leftarrow \{x, y\}$
                    \State Cross-entropy loss: $\{L\}$
		      \EndIf
				\State Compute backward pass
			\EndFor
            \State Evaluate updated model for cross-task consistency
		\EndFor
	\end{algorithmic}
  \label{alg:the_alg}
\end{algorithm}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.98\linewidth]{figures/main_table_iccv_arxiv.pdf}

   \caption{Results from the evaluation of various models on the \dataset{} benchmark. (a) \% Consistency of \uio$_{XL}$ and OFA$_{HUGE}$ models for varying difficulty ($k$) and all tasks in \dataset{}, (b) \% consistency ($k$=1) values for different sizes of \uio models and (c) comparison of \% accuracy with \% consistency ($k$=1) values for all sizes of OFA models and our OFA$_{Con}$ model (see Sec.~\ref{sec:training_obj}).}
   \label{fig:results}
   \vspace{-0.1in}
\end{figure*}

\section{Consistency-based Training}
\label{sec:training_obj}
A unified model exhibiting inconsistent predictions suggests that the model has learned weak semantic representations that are sensitive to task variations. It is undesirable to work with a model that is susceptible to such frailties. Moreover, consistency constraints can provide useful information for learning well-rounded semantic representations \cite{lu2021taskology} and reduce the need for training data \cite{zamir2020robust}. Hence, we propose to train unified models in a way that preserves consistency across their predictions (see Algorithm~\ref{alg:the_alg}). Given a pair of train instances $x_{t_0}, x_{t_1}$ for the tasks $t_0, t_1$, let $\{y_{t_0}\}, \{y_{t_1}\}$ be the spaces of $K$ probable and semantically equivalent outputs. $f_{\theta}(.)$ is the scoring function for model with parameters $\theta$ and $\mathcal{R}(.)$ is some ranking function. We formulate the consistency-based loss objective using spearman's correlation as follows:
\begin{equation}
    \mathcal{L}_{const} = \frac{1}{2}{||\mathcal{R}(f_{\theta}(\{y_{t_0}\})) - \mathcal{R}(f_{\theta}(\{y_{t_1}\}))||}^2
\end{equation}
Since ranking is a non-differentiable operation, we use soft ranking via regularization \cite{blondel2020fast} as the differentiable ranking function $\mathcal{R(.)}$. Within a space of $k$ probable outputs for either task, if an output for task $t_0$ is ranked at $k-2$ while the equivalent output for task $t_1$ is ranked at $k+2$, the gradients from this objective are designed to push the two misaligned outputs towards a common rank $k$, which increases consistency as per the definition of $\mathcal{C}_{k}$ in Sec.~\ref{sec:contrast_sets}. This can affect the task-specific accuracy of an inconsistent model, especially when the $y_{t_0}$ or $y_{t_1}$ is the gold label. Hence, we minimize our proposed consistency objective in addition to the standard cross-entropy loss during training i.e.
\begin{equation}
        \mathcal{L} = \mathcal{L}_{ce} + \lambda\mathcal{L}_{const}
\end{equation}
where $\mathcal{L}_{ce}$ is the cross-entropy loss and $\lambda$ is the weighting factor for the consistency objective. See Algorithm~\ref{alg:the_alg}.


\section{Experimental Setup}
\noindent \textbf{Evaluation.} \uio \cite{lu2022unified} and OFA \cite{wang2022ofa} are two recent publicly released models that perform a wide variety of tasks, including all tasks supported in \dataset{} benchmark.\footnote{OFA models are pretrained on \dataset{} tasks for another two epochs to support text-to-image generation.} We evaluate all size variations of both models.


\noindent \textbf{Training.} We begin with the pretrained checkpoint for the OFA$_{LARGE}$ model and continue training with the objective proposed in Sec.~\ref{sec:training_obj}. We adapt the automated pipeline introduced in Sec~\ref{sec:dataset} to generate nearly 84K contrast sets from the training split of COCO Captioning, VQA, and localization datasets. We performed a manual analysis of this dataset and found that nearly 85\% of the contrast sets are valid, which is of sufficient quality for large-scale training purposes. We use the cross-entropy loss as the score $f_{\theta}(.)$ function for each sample. The models are subjected to continued pretraining for one epoch and trained on the combination of contrast sets and original datasets for the four tasks in \dataset{}. We set $\lambda = 0.25$ and use a learning rate of 1e-6. Additional hyperparameters can be found in the supplementary. This finetuned model is referred to as OFA$_{CON}$ in the rest of the paper.


\section{Results}

\begin{table*}
\footnotesize
% \small
  \centering
  \begin{tabular}{|p{3.5cm}|p{1.0cm}|p{1.5cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Params}} & \textbf{Captioning} & \multicolumn{3}{c|}{\textbf{\multirow{1}{*}{\textbf{VQA}}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Localization}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Text-to-Image Generation}}} \\
    % \hline
    & &  \textbf{CIDEr} & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{FID}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ \\
    \hline
    \textbf{A}~~~\uio{}$_{Small}$ & 71M & 111.8 & 75.3 & 28.1 & -0.06 & 50.6 & 36.3 & -0.09 & 93.45 & 49.5 & 0.05 \\
\textbf{B}~~~\uio{}$_{Base}$ & 241M & 140.5 & 87.8 & 36.2 & 0.12 & 61.59 & 41.6 & 0.13 & 91.56 & 50.4 & 0.02  \\
\textbf{C}~~~\uio{}$_{Large}$ & 776M & 227.1 & 90.0 & 55.1 & 0.36 & 68.8 & 56.2 & 0.03 & 85.04 & 48.5 & -0.01  \\
\textbf{D}~~~\uio{}$_{XL}$ & 2925M & \textbf{269.9} & \textbf{92.3} & 68.7 & 0.48 & \textbf{72.1} & 65.9 & 0.20 & \textbf{70.23} & 50.8 & -0.0 \\
\hline
 \textbf{E}~~~OFA$_{Medium}$ & 93M & 83.4 & 72.7 & 72.1 & \textbf{0.67} & 55.6 & 52.3 & 0.21 & 110.3 & 49.1 & -0.02 \\
\textbf{F}~~~OFA$_{Base}$ & 182M & 100.7 & 77.8 & 77.0 & 0.65 & 62.3 & 59.4  & 0.19 & 105.7 & 50.1 & \textbf{0.04} \\
\textbf{G}~~~OFA$_{Large}$ & 472M & 113.5 & 82.6 & \textbf{80.7} & 0.64 & 71.3 & 64.7 & \textbf{0.28} & 103.4 & \textbf{52.3} & 0.02 \\
\textbf{H}~~~OFA$_{Huge}$ & 930M & 110.3 & 82.7 & 78.8 & 0.62 & 70.1 & \textbf{68.5} & 0.33 & 107.3 & 48.3 & -0.01 \\
\hline
\multicolumn{12}{|c|}{\textbf{\textbf{Consistency-based Training}}} \\
\hline
\textbf{G}~~~OFA$_{Large}$ & 472M & 113.5 & 82.6 & 80.7 & 0.64 & 71.3 & 64.7 & 0.28 & 103.4 & 52.3 & 0.02 \\
\textbf{J}~~~OFA$_{Large}$ + Cont. Pretrain & 472M & 118.8 & 82.7 & 81.1 & 0.63 & 73.5 & 65.9 & 0.27 & \textbf{98.5} & 51.7 & 0.04 \\
\textbf{K}~~~OFA$_{Large}$ + Hinge Loss & 472M & 117.5 & \textbf{83.0} & 82.9 & 0.64 & 73.8 & 67.7 & 0.29 & 99.5 & 53.2 & 0.05 \\
\textbf{L}~~~OFA$_{CON}$ (ours) & 472M & 119.4 & 82.4 & \textbf{83.8} & \textbf{0.67} & \textbf{74.1} & \textbf{69.5} & \textbf{0.35} & 99.1 & \textbf{53.8} & \textbf{0.09} \\
    \hline
  \end{tabular}
  \vspace{0.05in}
  \caption{Results from evaluation of \uio{} and \ofa models on the \dataset{} benchmark. Metrics are task-specific accuracies, \% consistency ($k=1$) and Spearman's rank correlation coefficient ($\rho_{rank}$). Higher is better for all metrics except FID.}
  \label{tab:results}
  \vspace{-0.2in}
\end{table*}

\subsection{Evaluation of Pretrained Models}
\noindent \textbf{Models are more inconsistent across tasks of diverse modalities.} We wish to study how the semantic understanding of a unified model varies with tasks. We evaluate the best (and largest) \ofa and \uio models on \dataset{} and compare \% consistency across the 3 tasks i.e., VQA, localization, and text-to-image generation, with respect to the anchor task, i.e. image captioning. Results are shown in Fig.~\ref{fig:results}(a). For VQA (blue plots), \ofa$_{HUGE}$ and \uio$_{XL}$ models exhibit 78\% and 68\% top-1 consistency respectively. This number changes to 68\% and 65\% top-1 consistencies for localization (red plots), respectively, suggesting that unified models are especially prone to variation in semantic understanding when the outputs belong to different modalities. This is further supported by results for image generation (green plots) with 48\% and 50\% top-1 consistencies. Text-to-image generation is more complex than localization, because of the high dimensional output and rigorous semantic understanding required for the task. These results also suggest that cross-task inconsistency increases with the complexity of the task as well.\\


\noindent \textbf{Models are inconsistent at hard as well as easy contrast sets.} The contrast sets used for evaluating top-1 \% consistency are \textit{hard negatives} and we observe low consistency for these samples (see Fig.~\ref{fig:results}(a)). For easier contrast sets i.e. in $k>1$ scenarios, the \% consistency increases steeply (yet remains $<100\%$) for tasks with outputs of the same modality as the anchor task, as seen for VQA in Fig.~\ref{fig:results}(a). However, we do not observe similar trends for the other tasks (different modalities), implying that the unification of modalities within a model is a non-trivial challenge.\\


\noindent \textbf{Larger multi-task models are more accurate as well as consistent.} We evaluate various sized \uio and OFA models (see  Fig.~\ref{fig:results}(b)). We see that top-1 \% consistency values increase generously with the scale of the model for VQA and localization, up to 20\% increase from \uio$_{SMALL}$ to \uio$_{XL}$ on VQA. Improvements are modest for image generation with model size. We see similar trends for OFA.\\

\noindent \textbf{Models are more accurate than consistent.} We compare the top-1 \% consistency scores with the task-specific accuracies of the OFA models on the \dataset\ dataset in Fig.~\ref{fig:results}(c), and observe that consistency and accuracy are tightly correlated. However, these models feature below the $x=y$ line suggesting that they are more accurate than consistent. This suggests that when models make mistakes for one task they rarely make the same kind of mistakes on the other tasks, which is what would allow a model to achieve high consistency independently of accuracy. Instead, existing models appear to be consistent mostly by virtue of being accurate. This has the worrying implication that harder or more ambiguous tasks will lead to severe inconsistencies, and that high consistency on easy tasks does not necessarily mean models are parsing inputs in a unified way across tasks. It also highlights the importance of studying hard tasks like image generation when evaluating consistency.\\


\noindent \textbf{Models capable of performing more tasks are more inconsistent.} \uio{} models are trained on 90 diverse datasets from vision and language domains and can perform all 7 tasks on the GRIT benchmark \cite{gupta2021towards}. In contrast, OFA models are pretrained on a subset of the tasks that can be performed by \uio{}. Interestingly, we observe that OFA models are more consistent than \uio{} across all three tasks in the \dataset{} benchmark. This shows that massive multi-tasking can lead to larger misalignment between models' semantic understanding across tasks.

\begin{figure}[t]
  \centering
   \includegraphics[width=0.95\linewidth]{figures/error.png}
   \caption{Comparison of categorical distribution in the \dataset{} benchmark with the errors from evaluation of \uio{}$_{XL}$.}
   \label{fig:distribution}
   \vspace{-0.2in}
\end{figure}

\subsection{Common Failure Modes}
% \noindent \textbf{Common Failure Modes.} 
We analyze the contrast sets in \dataset{} for which Unified-IO$_{XL}$ is \textit{inconsistent for all three tasks} and categorize the errors into the tags defined in Figure~\ref{fig:distribution}. We find that 34.8\% of the errors are \emph{Attribute} errors, which is significantly higher than the category's 24\% distribution in the dataset. Most of the errors deal with the understanding of colors in various objects in the image. \emph{Object}, \emph{Food} and \emph{Location} cover 15.7\%, 13.5\% and 10\% of the errors respectively. See examples of errors in Figure~\ref{fig:examples}.


\subsection{Consistency-based Training}

As outlined in Sec.~\ref{sec:training_obj}, we continue training OFA via the use of a cross-task consistency-based loss objective. Results for the finetuned model, OFA$_{Con}$, are shown in Table~\ref{tab:results} (see rows I-L in Consistency-based Training). Since OFA$_{Con}$ (row L) is finetuned for an additional epoch, we also provide a baseline where OFA is finetuned for an additional epoch with just the original cross entropy objective (row J). We find that our proposed loss objective improves consistency along both metrics i.e. top-1 \% consistency and rank correlation. The top-1 \% consistency improves by 2\% for VQA and and text-to-image generation, and a larger margin i.e. 4\%, for localization. Importantly, we see that this preserves the accuracy for VQA, tipping the model over the $x=y$ line in Fig.~\ref{fig:main_figure}(c). It also provides an improvement of +0.6 for localization, and preserves the FID for Text-to-Image generation. These results show the benefits of incorporating consistency-based objective while training GPV models. 


\section{Conclusion}
We present a benchmark dataset, \dataset{}, to probe cross-task inconsistency in unified multimodal models and a loss objective to improve the same. Our results demonstrate that cross-task inconsistency is a significant issue in such models and can be mitigated with our proposed loss.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix



\section{\dataset{} Categories \& Examples}
Each sample in the \dataset{} dataset contains a set of ground truth annotations and a semantic concept within the original  caption is replaced with a contrast concept. Based on the type of semantic concept being replaced, we define the following categories for the \dataset{} benchmark:
\begin{itemize}
    \item \textbf{Object.} Inanimate objects (excluding food and sports-related) such as vehicles, tools, furniture, etc.
    \item \textbf{Attribute.} Adjectives used as modifiers to a noun in the caption, such as color (\textit{red} chair), height (\textit{tall} building), size (\textit{small}), and material (\textit{tiled} wall).
    \item \textbf{Food.} Food-related concepts including fruits, vegetables, and cooked items.
    \item \textbf{Sport.} Sport-related objects and references to the sport itself are included in this category.
    \item \textbf{Animal.} Includes all mentions of animals. Since the \dataset{} is created from COCO Captions \cite{lin2014microsoft}, the original captions predominantly contain animals defined in COCO objects.
    \item \textbf{Location.} Includes broadly defined areas (e.g., \textit{bathroom}, \textit{hotel}, \textit{library}), finer visual elements (e.g., \textit{floor}, \textit{sidewalk}), and spatial references (e.g., \textit{inside}, \textit{outside}, \textit{on table})
    \item \textbf{Action.} Comprises transitive (e.g. flying kite) as well as intransitive actions (e.g. sitting, standing) performed by persons and animals.
    \item \textbf{Person.} Concepts from one of the categories: \textit{man/male/guy}, \textit{woman/female/lady}, \textit{boy}, \textit{girl}.
    \item \textbf{Number.} Numerical references involving counting.
    \item \textbf{Misc.} Concepts that don't fall under any of the previous categories; primarily consists of references to text within images.
\end{itemize}

\begin{table}
\small
  \centering
  \begin{tabular}{|p{5.5cm}|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Value}\\
\hline
 Proportion of ranking updates $(\gamma)$ & 0.5 \\
 Weight co-efficient of ranking loss $(\lambda)$ & 0.25 \\
 Regularization strength of soft ranking & 1.0 \\
 Learning rate & 1e-6 \\
 Max. train epochs & 1 \\
 Batch Size & 2 \\
 Warmup ratio & 0.1\\
 Label smoothing & 0.0 \\
 
    \bottomrule
  \end{tabular}
  \caption{Hyperparameters for training OFA$_{Con}$.}
  \label{tab:hyperparams}
\end{table}

\begin{table*}
\footnotesize
% \small
  \centering
  \begin{tabular}{|p{3.5cm}|p{1.0cm}|p{1.5cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.7cm}|}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Params}} & \textbf{Captioning} & \multicolumn{3}{c|}{\textbf{\multirow{1}{*}{\textbf{VQA}}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Localization}}} & \multicolumn{3}{c|}{\multirow{1}{*}{\textbf{Text-to-Image Generation}}} \\
    % \hline
    & &  \textbf{CIDEr} & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{Acc.}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ & \multicolumn{1}{c}{\textbf{FID}} & \multicolumn{1}{c}{$\mathcal{C}_{1}$} & $\rho_{rank}$ \\
\hline
\multicolumn{12}{|c|}{\textbf{\textbf{Consistency-based Training}}} \\
\hline
OFA$_{CON}$ ($\lambda=0.0$) & 472M & 118.8 & \textbf{82.7} & 81.1 & 0.63 & 73.5 & 65.9 & 0.27 & \textbf{98.5} & 51.7 & 0.04 \\
OFA$_{CON}$ ($\lambda=0.25$) & 472M & \textbf{119.4} & 82.4 & 83.8 & 0.67 & \textbf{74.1} & 69.5 & 0.35 & 99.1 & 53.8 & \textbf{0.09} \\
OFA$_{CON}$ ($\lambda=0.50$) & 472M & 117.8 & 81.8 & \textbf{84.2} & \textbf{0.70} & 73.1 & \textbf{69.9} & 0.35 & 99.3 & \textbf{54.1} & 0.08 \\
    \hline
  \end{tabular}
  \vspace{0.05in}
  \caption{Results from ablation of the weight co-efficient ($\lambda$) for training of OFA$_{CON}$. Metrics are task-specific accuracies, \% consistency ($k=1$) and Spearman's rank correlation coefficient ($\rho_{rank}$). Higher is better for all metrics except FID.}
  \label{tab:ablation_results}
\end{table*}

See representative examples from \dataset{} for each of the categories outlined above in Fig.4 in main text. These categories are defined based on the semantic concept within the original caption. However, the contrast concepts for the corresponding sample are not restricted to the same category. For example, consider the category \textit{Animal} in Fig.4 in main text. The original concept is \textit{cat} while the contrast concepts can be \textit{baseball}. This diversity in contrast concepts can be attributed to the question in the VQA sample that is used to generate answers from GPV2 \cite{kamath2022webly}. Similarly, if the original concept is a COCO object, the contrast concept may or may not be a COCO object. GPV2 is trained on Web10K \cite{kamath2022webly} which contains semantic concepts beyond COCO. This makes the contrast sets in \dataset{} diverse and additionally challenging for unified models. 

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.94\textwidth]{figures/cococon_ofacon_supp.pdf}

   \caption{Examples from the \dataset{} benchmark where OFA$_{CON}$ is more consistent than pretrained OFA$_{LARGE}$. For each example, we show the relevant image (left), the ground truth caption, VQA question, or image generation prompt for the image with the perturbed concept in green (middle), the set of perturbations used to generate alternative answers and predictions from OFA$_{LARGE}$ and OFA$_{CON}$ for VQA (V), image generation (G) and localization (L) (right columns). \textcolor{green}{\checkmark} and \textcolor{red}{$\times$} indicate scenarios where the model predictions for captioning and the corresponding task for that particular contrast set are consistent and inconsistent respectively. `\textbf{-}' denotes a lack of localization annotations for the given sample.
}
   \label{fig:supp_example}
\end{figure*}



\section{Experimental Details}
The complete hyperparameters for training OFA$_{Con}$ using the rank correlation-based loss objective in available in Table~\ref{tab:hyperparams}.


\section{Ablation Results \& Examples}
In this section, we first present results from ablation of the weight co-efficient ($\lambda$) hyperparameter for the consistency-based loss objective in Table~\ref{tab:results}. We observe that a higher $\lambda$ hurts accuracy while a lower $\lambda$ does not improve consistency. We also present examples where OFA$_{CON}$ is more consistent than the pretrained OFA$_{LARGE}$. 

\end{document}
