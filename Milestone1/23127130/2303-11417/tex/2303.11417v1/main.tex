\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % 
\IEEEoverridecommandlockouts                              % This 
\overrideIEEEmargins                                      % Needed to meet printer requirements.
% \usepackage{mathpix}
\usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cite}
\usepackage[colorlinks,bookmarks=false]{hyperref}
\hypersetup{citecolor=[RGB]{0,0,255}, urlcolor=[RGB]{0,0,255}}

\usepackage[font=footnotesize]{caption}
\usepackage{mwe}
\usepackage{mathptmx}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem*{design*}{Controller Design}
\newcommand{\todo}[1]{\textcolor{red}{{\bf}  #1}}
\newcommand{\revise}[1]{\textcolor{black}{#1}}
\newcommand{\trace}{\mathrm{Tr}}
\newcommand{\dist}{\mathrm{dist}}
%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\newcommand{\marginJC}[1]{\marginpar{\color{red}\tiny\ttfamily#1}}

\renewcommand{\baselinestretch}{.98}
\allowdisplaybreaks

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% Comment boxes
\usepackage{xcolor}
\newcommand{\colorNoteBox}[3]{
	\begin{center}
		\vspace{-2ex}\small
		\fcolorbox[rgb]{#1}{#2}{\parbox[t]{\linewidth}{\setlength{\parskip}{1.5ex}%
				#3}}
\end{center}}
\newcommand{\wc}[1]{\colorNoteBox{1,0,0}{0.94, 0.97, 1.0}{WC:~#1}}
\newcommand{\e}[1]{\textcolor{red}{#1}}

\title{\LARGE \bf
Bridging Transient and Steady-State Performance in Voltage Control: A Reinforcement Learning Approach with Safe Gradient Flow}


\author{Jie Feng$^{1}$ \quad Wenqi Cui$^{2}$ \quad Jorge Cort\'es$^{3}$ \quad Yuanyuan Shi$^{1}$ % <-this % stops a space
\thanks{*The authors are supported by NSF ECCS-2200692, ECCS-2153937, and  ECCS-1947050 awards and the Jacobs School Early Career Faculty Development Award.}% <-this % stops a space
\thanks{$^{1}$Jie Feng and Yuanyuan Shi are with the Department of Electrical and Computer Engineering, University of California San Diego, \tt jif005@ucsd.edu, yyshi@eng.ucsd.edu.}
\thanks{$^{2}$Wenqi Cui is with the Department of Electrical and Computer Engineering, University of Washington, Seattle, \tt wenqicui@uw.edu.}
\thanks{$^{3}$Jorge Cort\'es is with the Department of Mechanical and Aerospace Engineering, University of California San Diego, \tt cortes@ucsd.edu.}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{abstract}
% Deep reinforcement learning approaches are becoming popular in designing nonlinear controllers for voltage control problems, while the absence of a stability guarantee hinders them from being deployed in practice. In this paper, we construct a decentralized RL-based controller incorporating the safe gradient flow, which can be trained jointly to minimize both transient and steady-state costs, i.e., voltage recovery time and long-term operating cost, while ensuring reactive power safety. We show that if the output of the transient controller is monotonically decreasing and bounded by a tighter bound than the joined controller, then the closed-loop system is asymptotically stable with respect to the optimal steady-state solution. We demonstrate the effectiveness of our method by conducting experiments with IEEE 13-bus test feeders and IEEE 123-bus distribution system test feeders.
% \end{abstract}
\begin{abstract}
Deep reinforcement learning approaches are becoming appealing for the design of nonlinear controllers for voltage control problems, but the lack of stability guarantees hinders their deployment in real-world scenarios. This paper constructs a decentralized RL-based controller featuring two components: a transient control policy and a steady-state performance optimizer. The transient policy is parameterized as a neural network, 
and the steady-state optimizer represents the gradient of the long-term operating cost function. The two parts are synthesized through a safe gradient flow framework, which prevents the violation of reactive power capacity constraints.
We prove that if the output of the transient controller is bounded and monotonically decreasing with respect to its input, then the closed-loop system is asymptotically stable and converges to the optimal steady-state solution. We demonstrate the effectiveness of our method by conducting experiments with IEEE 13-bus and 123-bus distribution system test feeders.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Voltage safety is one of the primary concerns of power system operation, which requires the voltage magnitude to stay in an acceptable range under all working conditions~\cite{Baran1989a}. In recent years, the integration of distributed energy resources (DERs) such as roof-top solar and electric vehicles has led to rapid and unpredictable fluctuations in the load and generation profiles of the distribution systems, thus leading to challenges in voltage control for distribution grids. 
%Conversely, the proliferation of DERs gives rise to an unprecedented capacity for real-time voltage regulation via inverter-based reactive power compensation.

%Tremendous efforts have been made to overcome this challenge with inverter-based controllers. 
There have been tremendous efforts made to overcome this challenge. Much of the attention has focused on optimizing the steady-state cost for voltage control~\cite{6760555,7436388,7028508,zhu2016fast,qu2019optimal,ZY-GC-MKS-JC:22-tps}, which refers to the operation cost 
after the system voltage has settled to equilibrium after a disturbance. However, as the system is subject to more frequent disturbances from load and generation fluctuations, optimizing the transient performance (i.e., how to optimally stabilize voltage after disturbances) becomes of equal importance.
%Depending on whether they utilize online measurements, Steady-state methods can be classified as either feedforward optimization or feedback controllers. With full system knowledge available centrally, the feedforward optimization models the voltage control problem as an Optimal Power Flow (OPF) problem with constraints~\cite{Farivar-2012-VVC-PES}. On the contrary, feedback controllers can take real-time local measurements (like voltage magnitude) to adjust reactive power in a local or distributed way.  The reactive power injection can be calculated by linear droop controller \cite{6760555} or gradient-based methods (including (sub)gradient, primal-dual and dual gradient) \cite{7436388,7028508,zhu2016fast,qu2019optimal}. Nevertheless, 
%these algorithms do not effectively optimize the \textit{transient performance} of voltage control, which is of equal importance.

The transient performance for the voltage control problem involves minimizing the voltage recovery time, at the minimum control effort. 
However, optimizing the transient cost for voltage control is a challenging task~\cite{https://doi.org/10.48550/arxiv.2209.07669}, as both the cost functions and system dynamics can be nonlinear. This is made even more challenging by limited communications and lack of exact model knowledge in the distribution grid. Recently, reinforcement learning (RL) has emerged as a powerful approach for addressing model-free nonlinear control problems. There has been considerable interest in developing RL-based controllers for optimizing the transient performance of voltage control problems. We refer readers to a recent survey \cite{9721402}.

%Despite the promise of optimizing transient performance, a major limitation for RL algorithms is the lack of provable stability guarantees, i.e. the voltage will be stabilized after any disturbance. 
Recent research has revealed that RL with a monotone policy network can ensure transient stability for voltage control~\cite{shi2022stability, feng2022stability, cui2022decentralized}. However, these works do not offer guarantees regarding the optimality of the steady state. Steady-state requirements are difficult to enforce in RL since training can only occur over a finite horizon. 
%and it is even harder to achieve steady-state optimality. 
%Recent research has revealed that the training of a monotone policy network can effectively optimize transient performance while simultaneously ensuring voltage stability~\cite{shi2022stability}. 
%However, this guarantee does not offer any elucidation regarding the optimality of the steady state.
% However, the guarantee is based on a specifically designed Lyapunov function, and it may not remain valid if there are reactive power limitations. 
%There are some attempts to bridge the transient and steady-state performance by integral controller \cite{7171085} and a combination with neural network \cite{https://doi.org/10.48550/arxiv.2206.00261} in frequency regulation. 
Motivated by the challenges, the question we want to address in this paper is,
\begin{center}
    \emph{Can RL be structured to optimize both transient and steady-state performance for voltage control?} 
\end{center}

The key idea underlying our approach is the synthesis of a 
neural-network-based transient control policy and a steady-state optimizer (represented by the gradient of the cost function) in a safe gradient flow framework~\cite{allibhoy2022control}. This enables us to 
coordinate these two sub-controllers to optimize both transient and steady-state performance while respecting the reactive power constraint and guaranteeing closed-loop stability. 
% Assuming that the transient controller meets certain conditions and the optimal solution of the convex objective function falls within the acceptable voltage range, we show that the closed-loop system is stable with respect to this optimal solution (Theorem \ref{thm:optimality(stability)}). 
We summarize our main contributions as follows:
\begin{itemize}
    \item We design a decentralized RL-based controller that optimizes both transient and steady-state performance for the distribution system voltage control; %It is also suitable for different convex objective functions and transient .
    \item %For controllers satisfying the conditions in Theorem \ref{thm:optimality(stability)}, 
    We prove that the proposed controller design guarantees both transient stability and steady-state optimality, for strictly convex objective functions (in Theorem~\ref{thm:optimality(stability)});  % We state that if certain conditions of the transient controller are met, the closed-loop system will converge to the optimal solution (Theorem \ref{thm:optimality(stability)}), and rigorous proof is provided. 
    \item We demonstrate the effectiveness of the proposed method with extensive numerical experiments. Our method can reduce over $30\%$ transient cost compared to controllers that only optimize the steady-state or transient performance, and guarantee optimal steady-state cost.
    % \todo{how much \% than the pure transient controllers?}.
    % \item We are the first to use the safe gradient flow \cite{allibhoy2022control} as a steady-state controller for voltage control, which is a continuous-time gradient-based method to solve constrained optimization problems. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Model \&  Preliminaries}
In this section, we review the distribution system power flow model and introduce the voltage control problem. 
%formulation with one motivating algorithm.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Branch Flow Model}
% \subsubsection{Branch Flow Model}
We consider the linearized branch flow model~\cite{7028508} in a tree-structured distribution network. The system is defined as $\mathcal{G} = (\mathcal{N}_0,\mathcal{E})$, consisting of a set of  nodes $\mathcal{N}_0 = \{0,1,\ldots,n\}$ and an edge set $\mathcal{E}$. Node $0$ is known as the substation, and $\mathcal{N} = \mathcal{N}_0/\{0\}$ denotes the set of nodes excluding the substation node. 
Each node $i\in\mathcal{N}$ is associated with an active power injection $p_i$ and a reactive power injection $q_i$. Let $v_i$ be the squared voltage magnitude, and let $p, q$ and $v$ denote $\{p_i,q_i,v_i\}_{i \in \mathcal{N}}$ stacked into a vector. %$\forall j \in\mathcal{N}, i=\textrm{parent}(j)$, 
The variables satisfy the following equations, $\forall i \in\mathcal{N}$, %j=\textrm{parent}(i)$,

\vspace{-0.4cm}
 \small
\begin{subequations}\label{eq:linear_distflow}
{
\begin{align}
    p_i &= -P_{ji}  + \sum_{k: (i, k) \in \mathcal{E}} P_{ik}\,, \quad
    q_i = -Q_{ji} + \sum_{k: (i, k) \in \mathcal{E}} Q_{ik}\,, \label{eq:conserv_law}\\
    v_i &= v_j - 2(r_{ji}P_{ji} + x_{ji} Q_{ji})\,, \label{eq:bfm_vl} %(j, i) \in \mathcal{E} 
\end{align}}
\end{subequations}
\normalsize
where $j$ is the parent node of $i$ in the distribution network, $P_{ji}$ and $Q_{ji}$ represent the active power and reactive power flow on line $(j,i)$, and $r_{ji}$ and $x_{ji}$ are the line resistance and reactance. %We can rearrange 
\eqref{eq:linear_distflow} can be written in the vector form,
%\todo{R and X are not defined. how are they relate to $\mathbf{r}$ and $\mathbf{x}$?}
\small
\begin{equation}\label{eq:power_flow}
v = R p + X q + v_0 \mathbf{1} = Xq + v^{env} ,
\end{equation}
\normalsize
where $v^{env} = R p +v_0 \mathbf{1}$ is the non-controllable part. $R={[R_{ij}]}_{n \times n}, X = {[X_{ij}]}_{n \times n}$ are defined as
$R_{ij}:= 2 \sum_{(h, k) \in \mathcal{P}_i \cap \mathcal{P}_j} r_{hk}$, $X_{ij}:= 2 \sum_{(h, k) \in \mathcal{P}_i \cap \mathcal{P}_j} x_{hk}$. Here, $\mathcal{P}_i$ is the set of lines on the unique path from bus $0$ to bus $i$, and $v_0$ is the squared
voltage magnitude at the substation bus. $R$ and $X$ are positive definite matrices and all elements are positive \cite{shi2022stability}.
% \wc{check if all elements are positive}
% $R=2Fdiag(\mathbf{r})F^T$, $X=2Fdiag(\mathbf{x})F^T$ with 
% $F :=-A^{-1}$. 
% Here we follow ~\cite{li2014real} to separate the voltage magnitude $\mathbf{v}$ into two parts: the controllable part $X\mathbf{q}$ that can be adjusted via adjusting reactive power injection $\mathbf{q}$ through the inverter-based control devices, and the non-controllable part $\mathbf{v}^{env} = R\mathbf{p}+v_0 \mathbf{1}$ that is decided by the load and PV active power $\mathbf{p}$.  
%$R_{ij} = 2 \sum_{(h, k) \in P_i \cap P_j} r_{hk}\,, X_{ij} = 2 \sum_{(h, k) \in P_i \cap P_j} x_{hk}$, where $P_i$ is the set of lines on the unique path from the substation to bus $i$. 
% Matrix $X$ and $R$ satisfy the following property, which is crucial for the stable control design. %in our ability 
% \begin{proposition}[\cite{li2014real} Lemma 1]\label{prop:RX_pd}
% Suppose $x_{ij}, r_{ij}>0$ for all $(i,j)$. Then, $X$ and $R$ are positive definite matrices. 
% \end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Voltage Control Problem}
%\subsubsection{Steady State Optimization Problem}
% Because the safety set $S_v$ is defined as an acceptable range of system voltages, there are infinitely many solutions to drive the system to stability. However, different solutions may have different maintenance costs. In terms of the grid system, it will stay in the steady state much longer compared to the transient period. As a result, it is very important to optimize the steady state cost. 
The optimal voltage control problem at the steady state is defined as,
\vspace{-0.4cm}
\small
\begin{subequations}
\label{opt:steady1}
\begin{align}
\min_{q} \quad & F(q)=C(q)+\frac{1}{2}q^\top Xq+q^\top \Delta\Tilde{v} \label{eq_ss:obj}\\
\text{s.t.}\quad & \underline{q} \leq q \leq \bar{q}
\end{align}
\end{subequations}
\normalsize
where $C(q)$ is the control cost and $\Delta\Tilde{v}=v^{env}-v^{nom}$. We define the reactive power safety set as $\mathcal{S}_q=\{q\in \mathbb{R}^n|\underline{q} \leq q \leq \bar{q}\}$. For the per-unit system, we define $v^{nom}=1$ p.u. %Following~\cite{}, 
Using \eqref{eq:power_flow}, the objective function can be rewritten as $F(q) = C(q) + \frac{1}{2}(v-v^{nom})^\top X^{-1}(v-v^{nom}) - \frac{1}{2} \Delta \tilde{v}^\top X^{-1} \Delta \tilde{v}$.
% the objective function can be rewritten as the following equation
% \begin{align*}
%     F(q)&=C(q)+\frac{1}{2}(v-v^{env})^\top X^{-1}(v-v^{env})\\&\quad +(v-v^{env})^\top X^{-1}(v^{env}-v^{nom})\\
%     &=C(q)+\frac{1}{2}(v-v^{nom}-\Delta\Tilde{v} )^\top X^{-1}(v-v^{nom}-\Delta\Tilde{v} )\\&\quad+(v-v^{env})^\top X^{-1}\Delta\Tilde{v} \\
%     &=C(q) + \frac{1}{2}(v-v^{nom})^\top X^{-1}(v-v^{nom}) \\&\quad- \frac{1}{2} \Delta \tilde{v}^\top X^{-1} \Delta \tilde{v}
% \end{align*}
Since the last term is a constant, the objective function finds an optimal trade-off between minimizing the control cost $C(q)$ and the voltage deviation $\frac{1}{2}(v-v^{nom})^\top X^{-1}(v-v^{nom})$. Following \cite{qu2019optimal}, we consider $C(q)=\sum_i^n C_i(q_i)$, $C_i(q_i)=\frac{\eta_i}{2s_i} q_i^2$, where $\eta_i, s_i > 0$ represent the cost of reactive power of bus $i$ and its apparent power capacity. Compactly, $C(q)=\frac{1}{2}q^\top C_q q$, where $C_q$ is the diagonal matrix $diag\{\frac{\eta_i}{s_i}\}_{i\in\mathcal{N}}$. %The value of $v^{env}$ equals the initial voltage magnitude given that reactive power injection is zero before deployment.

%$F(q):\Omega\to \mathbb{R}$ as a convex function. In addition, consider the reactive power constraint 

%We formulate the optimization problem as below:


% We make the following assumption about its solution.

% \begin{assumption}\label{asp2} 
%     The steady-state optimization problem \eqref{opt:steady1} is feasible and the optimal solution $(v^{*}, q^{*})$ satisfies $v^{*} \in S_v=\{ v\in\mathbb{R}^n: \underline{v}_i\leq v_i \leq\bar{v}_i \}$.
% \end{assumption}

% \todo{$R_q$ is a bad notation since it confuses readers with $R$, which is the network reactance matrix. Change $R_q$ to $C_q$ perhaps?}
% \wc{It is not very obvious why the objective function can be rewritten as $F(q) = C(q) + \frac{1}{2}(v-v^{nom})^\top X^{-1}(v-v^{nom}) - \frac{1}{2} \Delta \tilde{v}^\top X^{-1} \Delta \tilde{v}$. I suggest to write a bit more on the middle steps. A similar problem exists with Equation (4), it might be deleted since (5) can be derively derived from (3a). }

% Eq~\eqref{opt:steady1} can be equivalently written in a decentralized way,
% \begin{align*}
%     F(q)&=q^\top C_q q+\frac{1}{2}q^\top (Xq+\Delta \tilde{v})\\
%     &=q^\top C_q q+\frac{1}{2}q^\top (v - v^{nom})\\
%     &=\sum_i^n \left \{C_i(q_i)+\frac{1}{2}[q_i(v_i+v_i^{env}-2v_i^{nom})]\label{eq:decentralized_F}\right \}
% \end{align*}
Note that the objective function~\eqref{eq_ss:obj} can be equivalently written as the sum of cost at all nodes by \eqref{eq:power_flow},

\vspace{-0.4cm}
\small
\begin{align*}
    F(q)
    %&=C(q)+\frac{1}{2}q^\top (Xq + 2 \Delta \tilde{v})
    %\\
    % &=C(q)+\frac{1}{2}q^\top (v +v^{env}- 2v^{nom})\\
    =\sum_i^n \Big( C_i(q_i)+\frac{1}{2} \big( q_i(v_i+v_i^{env}-2v_i^{nom}) \big) \label{eq:decentralized_F} \Big) .
\end{align*}
\normalsize
%
% \begin{equation} 
%     F(q)=\sum_i^n \left \{C_i(q_i)+\frac{1}{2}[q_i(v_i+v_i^{env}-2v_i^{nom})]\label{eq:decentralized_F}\right \} .
% \end{equation}
The gradient of the objective function $\nabla F$ is,
\vspace{-0.1cm}
\small
\begin{equation}
\label{eq:gradient}
 \nabla F=C_q q + Xq+\Delta\Tilde{v}=C_q q+ v - v^{nom}.
\end{equation}
\normalsize
% I keep (5) because a direct gradient of (4) is \frac{\eta_i}{s_i}q_i + v_i+v_i^{env}-2v_i^{nom}, however, because v_i is also a function of q_i, so that result is not correct, it might confuse people if I directly write \nabla F_i=\frac{\eta_i}{s_i}q_i+v_i-v_i^{nom}
We write $\nabla F_i=\frac{\eta_i}{s_i}q_i+v_i-v_i^{nom}.$ The decomposable structure of the objective function and the gradient enables decentralized training and deployment of a controller.
%Notice that by choosing a proper $C_q$, we can have the optimal solution of (\ref{opt:steady1}) also recover the voltage of the system. 
We make the assumption that the optimal solution of \eqref{opt:steady1} is unique and the corresponding voltage $v^*$ lies in the safe voltage range.

\begin{assumption}\label{asp2} 
    The steady-state optimization problem \eqref{opt:steady1} is strictly convex and the optimal solution $(v^{*}, q^{*})$ satisfies $v^{*} \in \mathcal{S}_v=\{ v\in\mathbb{R}^n: \underline{v}_i\leq v_i \leq\bar{v}_i \}$, where $\underline{v}_i, \bar{v}_i$ are upper and lower bounds of desired system voltage magnitudes.  
\end{assumption}


% This assumption holds if the following inequalities hold
% \small
% \begin{subequations}
%     \begin{align}
%         &C_q \underline{q}+\bar{v}-v^{nom}\geq 0\label{ineq:high_voltage}\\
%         &C_q \bar{q}+\underline{v}-v^{nom}\leq 0\label{ineq:low_voltage}
%     \end{align}
% \end{subequations}
% \normalsize
% Inequality \eqref{ineq:high_voltage} refers to a worst case such that all the DERs are generating a large amount of power and have voltages greater than $\bar{v}$. In this case, consider we make the largest effort $q^*=\underline{q}$, $\nabla F=C_q\underline{q}+v^*-v^{nom}=0$, and we want $v^*\in\mathcal{S}_v$, which yields \eqref{ineq:high_voltage}. A similar reasoning holds for \eqref{ineq:low_voltage}.


%There are multiple established methods for developing controls with optimality guarantees for the optimization problem (\ref{opt:steady1}). 
% In one approach, a discrete-time non-incremental local control scheme was created by \cite{6760555} through reverse-engineering. This involves defining the control function $f:\mathbb{R}^n \to \Omega$ as a set of non-increasing linear droop controllers $f_i:\mathbb{R}\to\Omega_i$, where $q_i(t+1)=f_i(v_i(t)-v_i^{nom})$ for all $i \in \mathcal{N}$. Reverse-engineering is used to carefully choose $C(q)$ as $C(q)=\sum_{i\in\mathcal{N}}-\int_0^{q_i}f_i^{-1}(q)dq$, which results in a unique solution for the optimization problem (\ref{opt:steady1}) provided by $f(v(t)-v^{nom})$. 
To solve \eqref{opt:steady1}, \cite{7436388} introduces a projected gradient method 
\small
\begin{equation}
    \label{ctl:steven'follow}
    q_i(t+1)=[q_i(t)-\gamma \nabla F_i]_{\underline{q}_i}^{\bar{q}_i} ,
\end{equation}
\normalsize
where $[\cdot]^a_b$ denotes the projection onto $[a,b]$, and $\underline{q}_i$ and $\bar{q}_i$ are the lower and upper bound of reactive power capacity. If the stepsize $\gamma$ satisfies $\gamma<\frac{2}{\lambda_{max}(\nabla^2C(q)+X)}$, where $\lambda_{max}$ denotes the largest eigenvalue, $v(t)$ and $q(t)$ under the controller (\ref{ctl:steven'follow}) converge to $(v^*, q^*)$ -- the optimal solution of (\ref{opt:steady1}).  However, this approach does not account for optimizing transient performance, which is critical when the system is subject to rapid voltage deviations due to renewable integration and EV charging (reflected in changes in $v^{env}$). This limitation motivates our design of a controller that jointly optimizes both steady-state and transient performance.
% Moreover, because the gradient $\nabla F$ is decentralized, (\ref{ctl:steven'follow}) can also be considered a local controller. 
% To maintain consistency, the controller (\ref{ctl:steven'follow}) can be extended to continuous time as follows
% \begin{subequations}
% \label{ctl:projected-gradient-flow}
% \begin{align}
% \dot{q}(t)=&\argmin_{\xi \in\mathbb{R}^n}\quad \frac{1}{2}\lVert \xi +\nabla F(q(t))\rVert^2\\
% \text{s.t.}\quad 
%     & \frac{\partial g_{I_0}(q)}{\partial q}\xi\leq 0 
% \end{align}
% \end{subequations}
% where $I_0$ denotes the active constraint set i.e. $I_0(q)=\{1\leq i\leq n|g_i(q)=0\}$. 

% While this technique provides a systematic way to develop local controllers that comply with the objective function (\ref{opt:steady1}), it does not account for optimizing transient performance, which is critical for ensuring grid system safety. This limitation has inspired us to create a local voltage control law that jointly optimizes both steady-state and transient performance.


% \todo{For this equation (4), change it to the controller representation. 1) Follow Steven's paper and the followup paper first talk about how to solve \eqref{opt:steady1}; 2) Comment on this controller is decentralized}

% \todo{then comment on the problem with this controller - does not optimize the transient performance --> thus link to the next section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Joint Transient and Steady State Performance Optimization}
% \todo{Insert a plot showing what is the transient cost and also showing the $v*, q*$ at time $t_f$}
In this section, we first introduce the joint transient and steady-state optimization problem. Then, we propose a \textit{Transient and Steady-state Reinforcement Learning} (TASRL) framework to solve it. We provide transient stability and steady-state optimality guarantees in the next section. %which combines the RL controller with the safe gradient flow. 
% \wc{I feel it might be good to clarify here that we consider the controller law $\dot{q}(t)=f_{\theta}(q(t)):=\nabla F(q(t))-\pi_\theta(v(t))$ to regulate the reactive power, and gives some intuition why this form is utilized. Then it would be easier for people to understand that  Controller Design 1 finds the closed solution in the safety set.}
%We start with the definition of voltage stability.

% \begin{definition}[Reactive power safety]
% Define the reactive power safety set as $\mathcal{S}_q=\{q\in \mathbb{R}^n|\underline{q} \leq q \leq \bar{q}\}$.
% % \begin{equation}\label{eq:def_g(q)}
% %     g(q)=\begin{bmatrix}
% %     I\\-I
% % \end{bmatrix}q+\begin{bmatrix}
% %     -\bar{q}\\\underline{q}
% % \end{bmatrix}.
% % \end{equation}
% \end{definition}
% \todo{Define $S_{v} = \{ \mathbf{v}\in\mathbb{R}^n: \underline{v}_i\leq v_i \leq\bar{v}_i \}$}
%
% \marginJC{We don't seem to use this definition anywhere. To save space, we could just introduce the set $S_q$ right after equation (5) (where we already talk about reactive power limits). The function $g$ could be introduced later, around equations (8) or (9). That would help us put the length closer to 6 pages} 
%

% \begin{definition}[Asymptotic voltage stability]
% \label{def:voltage_stability}
% The \e{controlled} system is asymptotically stable if for any $v^{env}$ and $v(0)$,  $v(t)$ converges to the set $\mathcal{S}_v=\{ v\in\mathbb{R}^n: \underline{v}_i\leq v_i \leq\bar{v}_i \}$ in the sense that $\lim_{t \rightarrow \infty}\dist(v(t), \mathcal{S}_v) = 0$ and the distance is defined as $\dist(v(t),\mathcal{S}_v) = \min_{v' \in \mathcal{S}_v}||v(t)-v'||$.
% \end{definition}
%
% \marginJC{Where do we use this def of asymptotic voltage stability? I couldn't see it mentioned explicitly. Also, the def refers to "the closed-loop system", but we have not introduced any closed-loop system yet, so it's a bit confusing for the reader.}
%


From the system operator perspective, we wish to achieve two main objectives: 1) \emph{transient stability and performance:} fast convergence of system voltage to the desired operating range $\mathcal{S}_v$ (e.g., $\pm 5$\% around $v^{nom}$) after a disturbance; 2) \emph{steady-state performance:} maintaining the system operation at the most economical point. Thus, the joint transient and steady-state optimization problem is formulated as,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-0.4cm}
\small
\begin{subequations}
\label{opt:trans_ctrl}
\begin{align}
\min_{\mathbf{\theta}} \quad & J(\theta)= \int_{t=0}^{t_f} \gamma^t \sum_{i=1}^n c_i({v}_i(t), q_i(t))  \label{eq_rl:obj}\\
\text{s.t.}\quad 
& v(t) = Xq(t) + v^{env} \,,\\
    &\dot{q}(t)=f_{\theta}(q(t), v(t))\,,\\
% &\text{Voltage stability holds.}\label{eq_rl:constr}\\
& v^* = \lim_{t \rightarrow t_f} v(t), q^* =\lim_{t \rightarrow t_f} q(t)\,, \label{eq:converge1}\\
& q^* \text{is the optimal solution for (3)}\,, \label{eq:converge2}\\   
&q(t)\in \mathcal{S}_q\,, \forall t \text{\ and\ } v^* \in \mathcal{S}_v. \label{eq:reactive_safety}
% & \label{eq:converge3}
\end{align}
\end{subequations}
\normalsize
\vspace{-0.1cm}
where $\gamma$ is the discount factor and $c_i$ is the cost function at node $i$, for which we choose
$c_i(v_i,q_i)=C_i(q_i)+\frac{1}{2}q_i(v_i+v_i^{env}-2v_i^{nom})$. $f_{\theta}(\cdot,\cdot)$ is the controller to be optimized. 
%with respect to voltage deviation and control effort. 
% \todo{Give the example cost function we used}
% $\pi_{\theta}$ is the policy, which we will define later.
Here, 
$t_f \in \mathbb{R}_{>0}$ is the (possibly unbounded) stabilization time.
During the transient period $[0, t_f]$, the goal is to recover the voltage quickly under limited reactive power resources while minimizing the control effort. In addition,  %to optimizing the transient performance, 
we want the system to converge to the steady-state optima $\lim_{t \rightarrow t_f} v(t) \rightarrow v^*, \lim_{t \rightarrow t_f} q(t) \rightarrow q^*$, and such that the equilibrium point $(v^*, q^*)$ solves the steady-state optimization~\eqref{opt:steady1}. 

% A diagram of transient and steady state is available as figure \ref{fig:steady&transient}.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.49\textwidth]{figures/transient+steady.png}
%     \small
%     \caption{This diagram illustrates a control trajectory solving (\ref{opt:steady1}). The left plot is the voltage trajectory, and the right plot is the reactive power usage. 
%     % \todo{move $t_f$ in the right plot to x-axis.. move transient and steady below the line, and $q^*$ above the line. In general, make two plots having same style otherwise it looks messay...}
%     }
%     \label{fig:steady&transient}
% \end{figure}
%Formally, the transient optimization problem is defined as follows:
\paragraph*{Controller synthesis inspired by safe gradient flow}%\label{cpt:CBF}
To jointly optimize the transient and steady-state performance, let's start by considering a direct combination of a transient policy $\pi_{\theta}(v)$, parameterized by a neural network, and the gradient of the long-term operation cost $\nabla F(q)$, cf.~\eqref{eq:gradient},

\vspace{-1mm}
\small
\begin{equation}\label{eq:unconstraied_flow}
    \dot{q} = f_{\theta}(q, v) := -\nabla F(q) + \pi_{\theta}(v) , 
\end{equation}
\normalsize
%where the transient performance optimizer $\pi_\theta(v(t))$ is parameterized as a neural network, and 
with $q(0)=0$.
% We first show how to enforce reactive power safety. The transient stability is further enforced by the transient controller $\pi_\theta(v(t))$, and we will elaborate on it later. 
% \begin{design*}\label{controller}
% We propose to solve the joint optimization problem in~\eqref{opt:trans_ctrl} with the following controller
% \begin{subequations}
% \label{ctl:controller}
% \begin{align}
% f_{\theta}(q(t))=&\argmin_{\xi\in\mathbb{R}^n} \frac{1}{2}||\xi+\nabla F(q(t))-\pi_\theta(v(t))||^2\\
% \text{s.t.}\quad 
%     % &\alpha (\underline{q}-q(t))\leq \xi \leq \alpha (\bar{q}-q(t))\label{ctl:condition}
%     & %\frac{\partial g(q)}{\partial q}\xi\leq -\alpha g(q) 
%     \alpha (\underline{q}-q(t)) \leq \xi \leq \alpha (\bar{q}-q(t))
%     \label{ctl:condition}
%     % &c\alpha (\underline{\mathbf{q}}-\mathbf{q}(t))\leq \pi_\theta(\mathbf{v}(t)) \leq c\alpha (\bar{\mathbf{q}}-\mathbf{q}(t))\label{eq:constrantforpi}
% \end{align}
% \end{subequations}
%where $\pi_\theta(v(t))$ is parameterized as a monotone neural network with bounded output for transient performance optimization, $\nabla F(q(t)) = \nabla C(q) + $\todo{write out the concrete form for $\nabla F$} denotes the gradient of the steady-state cost function, and
% where $\alpha \in (0, 1]$ is a nonnegative hyperparameter, that represents how conservative the controller is about reactive power constraint, where a smaller $\alpha$ means more conservative. 
% \end{design*}
%Based on previous discussions, both transient performance for voltage stability and steady state for long-term cost is critical for grid system operation. To solve this question as a whole, we combine the safe gradient flow and Stable-DDPG for joint optimization with both stability and safety guarantee. 
%\subsection{Voltage control with reactive power constraint}
% \begin{subequations}
% \label{opt:steady}
% \begin{align}
% \min_{\mathbf{q}} \quad & F(\mathbf{q})=C(\mathbf{q})+\frac{1}{2}\mathbf{q}^TX\mathbf{q}+\mathbf{q}^T\Delta\Tilde{\mathbf{v}} \label{eq_ss:obj}\\
% \text{s.t.}\quad 
%     &g(\mathbf{q})\leq 0
% \end{align}
% \end{subequations}
% %As a result, the steady state optimization can be achieved with only local observations. To align the transient and steady state performance, we define $c_i(v_i(t),q_i(t))$ in (\ref{eq_rl:obj}) as $c_i(v_i(t),q_i(t))=C_i(q_i(t))+\frac{1}{2}[q_i(t)(v_i(t)+v_i^{env}-2v_i^{nom})]$ to be the transient cost.
% To solve the voltage control problem with constraint~\eqref{opt:steady}, we use a continuous method that solves constrained nonlinear optimization while constructing a forward invariant feasible region. In this paper, we focus on the class of methods called safe gradient flow~\cite{https://doi.org/10.48550/arxiv.2204.01930}.
% where \eqref{ctl:condition} can be further expressed as 
% \begin{equation}
% \alpha (\underline{q}-q(t)) \leq \xi \leq \alpha (\bar{q}-q(t)). 
% \label{eq:bound}
% \end{equation}
% In this case, the transient and steady-state controllers are synthesized through a safe gradient flow framework~\cite{allibhoy2022control} in \eqref{ctl:controller}, which solves a smoothed projection problem to ensure reactive power safety. %The actual reactive safety set $C_q$ is given by $ (\underline{q}-q(t))\leq \xi \leq (\bar{q}-q(t))$. 
% The hyperparameter $\alpha>0$ indicates the degree of conservatism of the controller towards reactive power constraints. Notably, as $\alpha$ tends towards infinity, the controller reduces to a direct projection of $-\nabla F(q(t))+\pi_\theta(v(t))$ onto $\mathcal{S}_q$.
% The safe gradient flow method is first introduced in~\cite{allibhoy2022control} %is a continuous-time feedback controller that 
% to solve constrained optimization problems using gradient flow while guaranteeing constraint satisfaction along the trajectory. Following the safe gradient flow method, we combine the transient policy network $\pi_{\theta}(v)$ and the gradient term $\nabla F(q(t))$, while ensuring the reactive power safety constraint along the trajectory.
%It can be derived either as a continuous approximation of the projected gradient flow like (\ref{ctl:controller}) or by augmenting the primal gradient flow with a drift term, which is the solution of a CBF-based quadratic program (QP) to ensure safety. 
%of the feasible set. The presence of the drift term ensures that the safety guarantee remains valid even if a transient controller, as previously presented, is added to the system.
% The main results of~\cite{allibhoy2022control} characterize the stability attributes of the minimizers under safe gradient flow.  Additionally, these stability properties remain valid when a transient controller, as mentioned earlier, is incorporated into the system.
% \todo{we can first use about 1 paragraph to provide a review of the safe gradient flow paper. See this paper https://dl.acm.org/doi/pdf/10.1145/3538637.3538853 Section 4 as a reference on how to summarize a previous work while still highlighting the contribution of one's own work}
% \todo{we need to introduce how to derive \eqref{ctl:safe-gradient-flow} in a much more complete way. 1) What is the barrier function; 2) how to derive (9b); }
% Now, we consider to combine the stable transient policy network $\pi_{\theta}(v)$ and the steady-state gradient term $\nabla F(q)$, while ensuring the reactive power safety constraint along the trajectory.
% To start with, let's consider a direct combination of the transient policy network $\pi_{\theta}(v)$ and $\nabla F(q)$ for jointly optimizing the transient and steady-state performance, i.e.,
%
The main issue with the controller~\eqref{eq:unconstraied_flow} is that the resulting trajectory of $q(t)$ may not satisfy the reactive power constraint $\underline{q} \leq q(t) \leq \overline{q}$ at all times. To enforce it, we build on the safe gradient flow framework introduced in~\cite{allibhoy2022control}. This design employs a control barrier function $g(q)$ to ensure that a given  dynamics never leaves a safe set $\mathcal{S}_q=\{q\in \mathbb{R}^n|g(q)\leq 0\}$, where $g(q)$ is defined by  

\vspace{-4mm}
\small
\begin{align}\label{eq:g}
g(q)=\begin{bmatrix}
    I\\-I
\end{bmatrix}q+\begin{bmatrix}
    -\bar{q}\\\underline{q}
\end{bmatrix} .
\end{align}
\normalsize
% where  $$g(q)=\begin{bmatrix}
%     I\\-I
% \end{bmatrix}q+\begin{bmatrix}
%     -\bar{q}\\\underline{q} \end{bmatrix} < 0.$$
%Specially, we consider $g(q)$ as the control barrier function, which satisfies 
%\begin{subequations}\label{eq:cbf_constr1}
%\begin{align}
%    g(q) &\leq 0, \forall q \in \mathcal{S}_q\,,\\
%    g(q) &> 0\,, \forall q \notin \mathcal{S}_q   
%\end{align}
%\end{subequations}
If the dynamics $\dot{q} = f_{\theta}(q, v)$ satisfies
\small
\begin{equation}\label{eq:cbf_constr2}
    {\frac{\partial g}{\partial q}} f_{\theta}(q, v) \leq -\alpha g(q) ,
\end{equation}
\normalsize
then, by Nagumoâ€™s theorem \cite{FB-SM:07},
$q(t)$ must stay inside the safe region $\mathcal{S}_q$ for all $t$. Here, the hyperparameter $\alpha>0$ indicates the degree of consciousness regarding the reactive power constraints, where the larger $\alpha$ is, the more flexibility is allowed when $q(t)$ is not reaching the constraints. %For our purposes, note that $q\in \mathcal{S}_q$ if and only if $g(q)\leq 0$, 
To ensure that the controller in \eqref{eq:unconstraied_flow} satisfies the safety constraints, the safe gradient flow framework~\cite{allibhoy2022control} prescribes modifying it minimally according to the following control
barrier function quadratic program (CBF-QP) safety filter~\cite{gurriet2018towards},

\vspace{-0.4cm}
\small 
\begin{subequations}\label{eq:controller_equiv}
\begin{align}
    f_{\theta}(q, v) = \argmin_{\xi \in \mathbb{R}^n} &\frac{1}{2} \|\xi - \left(-\nabla F(q) + \pi_{\theta}(v)\right)\|_2^2 \label{eq:cbf_obj}\\
    s.t. \quad & {\frac{\partial g}{\partial q}} \xi \leq -\alpha g(q) \label{eq:cbf_constraint}
\end{align}
\end{subequations}
\normalsize 
Equation \eqref{eq:cbf_constraint} is the barrier condition that guarantees the reactive power remains within $ \mathcal{S}_q$.
Plugging in the definition of $g(q)$ in~\eqref{eq:g} gives
\vspace{-4mm}

\small 
\begin{subequations}
\label{ctl:controller}
\begin{align}
f_{\theta}(q, v)=&\argmin_{\xi\in\mathbb{R}^n} \frac{1}{2}||\xi+\nabla F(q)-\pi_\theta(v)||^2\\
\text{s.t.}\quad 
    % &\alpha (\underline{q}-q(t))\leq \xi \leq \alpha (\bar{q}-q(t))\label{ctl:condition}
    & %\frac{\partial g(q)}{\partial q}\xi\leq -\alpha g(q) 
    \alpha (\underline{q}-q) \leq \xi \leq \alpha (\bar{q}-q)
    \label{ctl:condition}
    % &c\alpha (\underline{\mathbf{q}}-\mathbf{q}(t))\leq \pi_\theta(\mathbf{v}(t)) \leq c\alpha (\bar{\mathbf{q}}-\mathbf{q}(t))\label{eq:constrantforpi}
\end{align}
\end{subequations}
\normalsize
%When $\alpha$ decreases, the feasible set will become smaller, leading to a more conservative controller.
The controller \eqref{ctl:controller} is our proposed controller to solve the joint optimization problem in \eqref{opt:trans_ctrl}.
We envision that (\ref{ctl:controller}) finds the safe control action closest to $-\nabla F(q)+\pi_\theta(v)$ while ensuring the reactive power constraints are never violated. If sufficient reactive power capacity exists, $f_{\theta}(q, v) = -\nabla F(q)+\pi_\theta(v)$, otherwise, the action is projected to ensure reactive power capacity constraints are met. Notably, as $\alpha$ tends towards infinity, the smoothed projection reduces to a direct projection of $-\nabla F(q(t))+\pi_\theta(v(t))$ onto $[\underline{q}, \overline{q}]$. We present the closed-form solution to \eqref{ctl:controller} in the following result.
% where the transient controller $\pi_{\theta}(v)$ and the steady-state feedback law $\nabla F$ are synthesized through the safe gradient flow framework~\cite{allibhoy2022control}. 
%This controller can be interpreted as a smoothed projection of the combination of  transient and steady-state policies to ensure reactive power safety. %The actual reactive safety set $C_q$ is given by $ (\underline{q}-q(t))\leq \xi \leq (\bar{q}-q(t))$. 
% 

\begin{proposition}
The optimal solution to \eqref{ctl:controller} is given by, 
\small
\begin{equation}
\label{ctl:solution}
    f_\theta(q, v)=[\pi_{\theta}(v)-\nabla F(q)]_{\alpha(\underline{q}-q)}^{\alpha(\bar{q}-q)}
\end{equation}
\normalsize 
where $[\cdot]^a_b$ denotes the projection onto $[a,b]$. 
\end{proposition}

% \subsection{TASRL Algorithm}
We summarize the proposed controller in Algorithm \ref{alg:satrl}. As observed in Algorithm \ref{alg:satrl}, the controller computation and execution are decentralized. The training process of TASRL follows the same flow as standard policy optimization RL algorithms, observing a state, computing control actions, transiting to the next state and gathering rewards, and improving the policy with the samples. Each local transient policy $\pi_{i, \theta_i}(v_i)$ can be parameterized as neural networks (with requirements specified in Section \ref{sec:theory} Def. \ref{def:stable_ctl}) and trained together with $\nabla F$ via a RL framework to optimize the transient performance. The proposed TASRL framework is general and can be integrated with most policy optimization methods, including DDPG~\cite{lillicrap2015continuous}, PPO~\cite{schulman2017proximal}, and TRPO~\cite{schulman2015trust}.


%with the flexibility of utilizing any appropriate updating rules for RL.
\setlength{\textfloatsep}{4pt}
\begin{algorithm}[t]
	\caption{Transient and Steady-state Reinforcement Learning (TASRL) for Distribution Grid Voltage Control}
	\label{alg:satrl}
	\begin{algorithmic}[1]
	\Ensure policy networks $\pi_{i,\theta_i}(v_i)$ with parameters $\theta_i$; hyperparameter $\alpha$; sampling time $h$; replay buffers $\mathcal{D}_i, \forall i \in \mathcal{N}$. 
	%for every bus $i$. 
% 	Policy and Q-function network learning rate $l_p$ and $l_q$, discount factor $\gamma$. 
	%target network update ratio $\rho$. 
% 	\todo{you define these parameters but they are never used. one way is to explicitly present the update rule for $\theta_i$ and $\phi_i$ such that we can see where these parameters are used}
	\For {$j = 0$ to $N_{ep}$}
	  \State{Randomly generate initial states $v(0)$} 
   %with voltage violation for all nodes}
	  \For {$t = 0$ to $N_{step}$}
	    \State{Observe state $v_i(t)$, $\forall i$}
            \State{Compute transient policy output $\pi_{i,\theta_i}(v_i(t))$, $\forall i$}
            \State{Compute $\nabla F_i=\nabla C_i(q_i(t))+v_i(t)-v_i^{nom}$, $\forall i$}
            \State{Compute the control action (reactive power adjustment)  $f_{i,\theta_i}(q_i(t), v_i(t))=[\pi_{i,\theta_i}(v_i(t))-\nabla F_i]_{\alpha(\underline{q}_i-q_i(t))}^{\alpha(\bar{q}_i-q_i(t))}$}
	    \State {Execute $q_i(t+1) = q_i(t) + hf_{i,\theta_i}(q_i(t), v_i(t)), \forall i$}
            \State {Transit to next state $v(t+1)$, receive cost $c(t)$}
            %= C_i(q_i)+\frac{1}{2}q_i(t)(v_i(t)+v_i(0)-2v^{nom})$}
	    \State{Store $\{v_i(t),q_i(t), f_{i,\theta_i}(q_i(t), v_i(t)), -c_i(t),v_i(t+1)\}$ in $\mathcal{D}_i$, $\forall i$ }
     \State{Update policy network $\theta_i$ for all agent $i$}
	  \EndFor
	%   \If{len($\mathcal{D}_i$) $> $ batch size}
	%   \For {$i=1,...,n$}
	% \State{Update policy networks $\theta_i$ for all agent $i$}
	%    \EndFor
	%   \EndIf
	\EndFor
	\end{algorithmic} 
\end{algorithm}


% As a continuous approximation of the projected gradient flow (\ref{ctl:projected-gradient-flow}), the safe gradient flow for $F(q)$ (\ref{opt:steady1}) is defined as follows: 
% \begin{subequations}
% \label{ctl:safe-gradient-flow}
% \begin{align}
% \Pi_\alpha(q(t))=&\argmin_{\xi \in\mathbb{R}^n}\quad \frac{1}{2}\lVert \xi +\nabla F(q(t))\rVert^2\\
% \text{s.t.}\quad 
%     & \frac{\partial g(q)}{\partial q}\xi\leq -\alpha g(q) \label{eq:barrier_constr}
% \end{align}
% \end{subequations}
% where $\alpha>0$. The safe gradient flow and the projected gradient flow have a similar structure as expressed in equation (\ref{ctl:projected-gradient-flow}) and possess a unique solution if one exists. However, the safe gradient flow is Lipschitz and well-defined outside $S_q$, whereas the projected gradient flow is discontinuous. We refer readers  to \cite{allibhoy2022control} for a rigorous discussion of these properties.


% This method can also be interpreted from a control point of view. 


% \todo{explain how this part connect to (9)} 
% Consider the following control-affine system
% \begin{subequations}
% \label{ctl:cbf_view}
% \begin{align}
% \dot{q}= f(q) &=\Pi(q,v)-\frac{\partial g(q)}{\partial q}^T \omega \\
% \mathbf{\omega}(q)&\in \argmin_{\mathbf{\omega}\in K(q)}\{\lVert\frac{\partial g(q)}{\partial q}^T\mathbf{\omega}\rVert^2\}\\
% K(q)&=\{\mathbf{\omega}\in\mathbb{R}^{2n}_{\geq 0}|L_f g(q)+\alpha g(q)\leq 0\}
% \end{align}
% \end{subequations}
% where function $\Pi(q,v)$ is a particular controller, $\omega$ denotes the minimum drift required to enforce the feasibility constraints. Notably, the original safe gradient flow is represented by (\ref{ctl:cbf_view}) when $\Pi(q,v)=-\nabla F(q)$. According to \cite{allibhoy2022control}, if we modify the control action as $\Pi(q,v)=-\nabla F(q)+\pi_\theta(v)$, then the resulting closed-loop system becomes equivalent to that described in (\ref{ctl:controller}).

% By Lemma 4.1 of \cite{allibhoy2022control}, $g(q)$ is a valid CBF for (\ref{ctl:cbf_view}). Use $L_f$ to denote the Lie derivative along $f(\mathbf{q})$, the admissible control set can be defined as $K(q)$. Then if $\omega \in K(q)$, it follows that $L_f g(q)+\alpha g(q)\leq 0$, which implies safety. When $\alpha$ decreases, the feasible set will become smaller, leading to a more conservative controller.




% The overall controller $\Pi_\alpha(\mathbf{q}(t),\mathbf{v}(t))$ is defined as:
% \begin{subequations}
% \label{ctl:controller}
% \begin{align}
% \Pi_\alpha(\mathbf{q}(t))=&\argmin_{\mathbf{\xi}\in\mathbb{R}^n} \frac{1}{2}||\mathbf{\xi}+\nabla F(\mathbf{q}(t))-\pi_\theta(\mathbf{v}(t))||^2\\
% \text{s.t.}\quad 
%     &\alpha (\underline{\mathbf{q}}-\mathbf{q}(t))\leq \mathbf{\xi} \leq \alpha (\bar{\mathbf{q}}-\mathbf{q}(t))\label{ctl:condition}
%     % &c\alpha (\underline{\mathbf{q}}-\mathbf{q}(t))\leq \pi_\theta(\mathbf{v}(t)) \leq c\alpha (\bar{\mathbf{q}}-\mathbf{q}(t))\label{eq:constrantforpi}
% \end{align}
% \end{subequations}
% where $\alpha$ is a nonnegative hyperparameter, $\pi_\theta(\mathbf{v}(t))$ is parameterized as neural network, which will be defined in the following subsection. The value of $\alpha$ represents how conservative the controller is about reactive power usage, where smaller means more conservative. 
%\todo{explain each part clearly}
% With the proposed controller in the loop, the close-loop dynamics is formulated as follows
% \begin{subequations}\label{eq_continuous}
% \begin{align}
%     \mathbf{v}(t) &= X\mathbf{q}(t) + \mathbf{v}^{env} \,,\label{eq:voltage_model_f}\\
%     \dot{\mathbf{q}}(t)&=f_{\theta}(\mathbf{q}(t)) \label{eq_controller_f}
% \end{align}
% \end{subequations}

% \subsection{Controller implementation}
% We implement (\ref{ctl:controller}) as the following differentiable controller,


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transient Stability and Steady-State Optimality Guarantees} 
\label{sec:theory}
In this section, we establish the closed-loop stability and optimal steady-state performance properties of Algorithm \ref{alg:satrl}, in which the controller is given by \eqref{ctl:controller} (equivalent forms 
 \eqref{eq:controller_equiv} and \eqref{ctl:solution}). The guarantees rely on certain structural constraints of the transient policy in the next definition.
 % that we formalized next.

% We first define the stable transient controller.
%
% \marginJC{How about calling it "stable transient policy"? That way we only use controller for (11).}
%
\begin{definition}[Stable decentralized transient policy]    \label{def:stable_ctl}
A set of decentralized policy $\{\pi_{i, \theta_i}, \forall i \in \mathcal{N}\}$ is a stable transient policy if it satisfies the following conditions for all bus $i \in \mathcal{N}$:
\begin{enumerate}
\item $\pi_{i,\theta_i}(v_i)$ is a continuously differentiable function satisfying $\pi_{i,\theta_i}(v_i) = 0$ for $v_i \in [\underline{v}_i,\bar{v}_i]$;
\item $\pi_{i,\theta_i}(v_i)$ is monotonically decreasing for $v_i \in (-\infty,\underline{v}_i) \cup (\bar{v}_i,\infty)$;
\item $\pi_{i,\theta_i}(v_i)$ is bounded i.e. $c\alpha (\underline{q}_i-q_i(t))\leq \pi_{i,\theta_i}(v_i(t)) \leq c\alpha (\bar{q}_i-q_i(t))$, where $c\in [0,1)$.
%the controller is defined by (\ref{ctl:controller})
\end{enumerate}
\end{definition}

% Given that $g(q)=\begin{bmatrix}
%     I\\-I
% \end{bmatrix}q+\begin{bmatrix}
%     -\bar{q}\\\underline{q}
% \end{bmatrix}$, we have \eqref{ctl:condition} is equivalent to 
% \begin{equation}
% \alpha (\underline{q}-q(t)) \leq \xi \leq \alpha (\bar{q}-q(t)). 
% \end{equation}
% When $\alpha$ gets smaller, the bound of $\xi$ is tightened. As a consequence, the controller becomes more responsive to the deviation of the current control action from the reactive power safety set boundary, leading to a more conservative approach.
 %the constraints corresponding to inactive nodes where $j \notin I_0$ are essentially eliminated. As a result, 
 We write $\pi_{i,\theta_i}(v_i)=\pi_{i,\theta_i}(v_i)-\pi_{i,\theta_i}(v_i^*)$ as $\pi_{i,\theta_i}(v_i^*)=0$.  For $v_i\neq v_i^*$, we define $K_{ii}(v_i)=\frac{\pi_{i,\theta_i}(v_i)-\pi_{i,\theta_i}(v_i^*)}{v_i-v_i^*}$%\todo{$K_{ii}$ is a function of $v_i$. Shall we write as $K(v)$ so that readers do not confuse $\pi_\theta(v)= - K(v-v^*)$ as a linear policy, it's better written as $K(v) (v-v^*)$. The first time we define $K$, specify the dependence on $v$, later on, we can say for simplification, we will write as K...}
 and $K(v)=-\text{diag}\left(K_{11}(v_1), K_{22}(v_2),\cdots,K_{nn}(v_n)\right)$.
% \small 
% \begin{equation*}
%     K(v)= -\begin{bmatrix}
%         K_{11}(v_1)&&0\\&\ddots&\\0&&K_{nn}(v_n)
%     \end{bmatrix}
% \end{equation*}
% \normalsize
By the monotonically decreasing condition in Definition~\ref{def:stable_ctl}, when $v_i \in (-\infty,\underline{v}_i) \cup (\bar{v}_i,\infty)$, $K_{ii}(v_i)<0$. When $v_i \in [\underline{v}_i,\bar{v}_i] \text{ and } v_i\neq v_i^*$, $K_{ii}(v_i)=0$. We define $K_{ii}(v_i) =0$ if $v_i=v_i^*$. As a result, for every $v$, we can write $\pi_\theta(v)=-K(v) (v-v^*)$. 
% For simplicity, we drop the explicit dependent of $v$ when writing $K$.
%but keep in mind that $K$ is not universal but $v$-dependent.
Define $\sigma_{\max}(\cdot)$ and $\sigma_{\min}(\cdot)$ as the largest and smallest singular value of a matrix. The following result establishes the theoretical guarantees of the proposed controller~\eqref{ctl:controller}.
% \todo{extract the the $\pi_{\theta}$ conditions as a definition ``Stable transient controller''; add $F(q)$ convex or strongly convex to the theorem condition? add voltage stability, reactive power safety}

\begin{theorem}[Transient Stability and Steady-State Optimality]\label{thm:optimality(stability)}
%Consider the optimization problem (\ref{opt:steady1}). 
Suppose Assumption \ref{asp2} holds, $\pi_\theta$ is a stable decentralized transient policy as in Definition \ref{def:stable_ctl}, and $2\sigma_{\max} (K(v)) \leq \sigma_{\min}(C_qX^{-1}+I)\,, \forall v \in \mathbb{R}^n$,
%
%\marginJC{All $v \in \mathbb R^n$ or all $v \in \mathcal{S}_v$?}
%
then the closed-loop system is asymptotically stable with controller \eqref{ctl:controller}. In addition, $\lim\limits_{t \rightarrow t_f} v(t) = v^*, \lim\limits_{t \rightarrow t_f} q(t) = q^*$, $q^*$ is the global minimizer of optimization problem \eqref{opt:steady1}, $v^*\in \mathcal{S}_v$, and $q(t)\in \mathcal{S}_q\,, \forall t\geq 0$. 
% . If for all bus $i$, $\pi_{i,\theta_i}(\cdot)$ is a continuously differentiable function satisfying $\pi_{i,\theta_i}(v_i) = 0$ for $v_i \in [\underline{v}_i,\bar{v}_i]$; $\pi_{i,\theta_i}(\cdot)$ is monotonically decreasing on $(-\infty,\underline{v}_i]$ and $[\bar{v}_i,\infty)$; and $\pi_{i,\theta_i}(v_i(t))$ satisfies a state-dependent output bound $c\alpha (\underline{q}_i-q_i(t))\leq \pi_{i,\theta_i}(v_i(t)) \leq c\alpha (\bar{q}_i-q_i(t))$, where $c\in [0,1)$.
% The closed loop system is asymptotically stable, i.e., $\lim_{t \rightarrow \infty} v(t) = v^*, \lim_{t \rightarrow \infty} q(t) = q^*$, and $q^*$ is the global minimizer of \eqref{opt:steady1}.
%If $\mathbf{q}^*$ is a local minimizer, then $\mathbf{q}^*$ is asymptotically stable relative to $\mathcal{C}_q$. Suppose there are enough reactive power resources, then $\mathbf{q}^*$ is the global minimizer of $F(\mathbf{q})$.
\end{theorem}
Theorem \ref{thm:optimality(stability)} shows that with a Lipschitz-like bound on the transient policy $\pi_{i, \theta_i}, \forall i \in \mathcal{N}$, the proposed controller in \eqref{ctl:controller} obtains both transient stability and steady-state optimality while respecting the reactive power capacity constraint at all times. Below, we present the theoretical analysis of Theorem~\ref{thm:optimality(stability)}.
% By employing a stable transient controller, the closed-loop system defined by (\ref{ctl:controller}) is capable of optimizing both transient and steady-state performance with respect to the optimization problem (\ref{opt:steady1}).
% \todo{In words, decribe what Theorem \ref{thm:optimality(stability)} states, and refer to the proofs in Section 4}
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Theorem~\ref{thm:optimality(stability)}}
\label{sec:proof}
% In this section, we prove regarding the stability and steady-state optimality of the proposed controller in \eqref{ctl:controller}.
%\begin{proof}
\begin{lemma}\label{lemma:1}
%For $K(v)$ and $-\nabla F(q)$ defined as above. 
Suppose $2\sigma_{\max} (K(v)) \leq \sigma_{\min}(C_qX^{-1}+I), \forall v \in \mathbb{R}^n$, $\lVert -\nabla F(q)\rVert^2+2\langle \pi_\theta(v),-\nabla F(q)\rangle\geq 0\,, \forall v \in \mathbb{R}^n, q \in \mathcal{S}_q$.
\end{lemma}
\begin{proof}
Following \eqref{eq:gradient} and \eqref{eq:power_flow}, $\nabla F(q)= \nabla F(q)-\nabla F(q^*)=(C_q X^{-1} + I)(v-v^*)$. Denote $A=C_q X^{-1} + I$, we have 

\vspace{-4mm}
\small
\begin{align*}
    &\quad \lVert -\nabla F(q)\rVert^2+2\langle \pi_\theta(v),-\nabla F(q)\rangle\\
    &=(v-v^*)^\top\left [A^\top A+K(v)A+A^\top K(v)\right](v-v^*).
    % &=(v-v^*)^\top\left [(A+K(v))^\top(A+K(v))-K(v)^TK(v)\right](v-v^*).
\end{align*}
\normalsize
To ensure $\lVert -\nabla F(q)\rVert^2+2\langle \pi_\theta(v),-\nabla F(q)\rangle\geq 0$, it suffices to have $A^\top A+K(v)A+A^\top K(v) \succeq 0$.
\vspace{-4mm}

\small
\begin{subequations}
    \begin{align}
    &\quad A^\top A+K(v)A+A^\top K(v) \succeq 0\\
    &\iff [A+K(v)]^\top[A+K(v)] \succeq K(v)^TK(v)\\
    &\iff \sigma_{\min}(A+K(v))\geq \sigma_{\max}(K(v)) \label{ineq:singular}
\end{align}
\end{subequations}
\normalsize

By \cite[Proposition 9.6.8]{10.2307/j.ctt7t833.15}, $\sigma_{\min}(A+K(v))\geq \sigma_{\min}(A)-\sigma_{\max}(K(v))$. Thus, $\sigma_{\min}(A)\geq 2\sigma_{\max} (K(v))$ is a sufficient condition for $\lVert -\nabla F(q)\rVert^2+2\langle \pi_\theta(v),-\nabla F(q)\rangle\geq 0$.
%we have $\sigma_{\min}(A)-\sigma_{\max}(K(v))\geq \sigma_{\max} (K(v))$, which implies that \eqref{ineq:singular} is true. Thus 
\end{proof}
\begin{proof}[Proof of Theorem~\ref{thm:optimality(stability)}]
By design, the proposed controller~\eqref{ctl:controller} guarantees $q(t)\in \mathcal{S}_q$, $\forall t\geq 0$ by the CBF-QP safety filter. We will work with the equivalent controller form in \eqref{eq:controller_equiv} throughout the proof since the inequality constraints are in a more compact form. The Lagrangian of \eqref{eq:controller_equiv} is,
% as Let $L:\mathbb{R}^n\times\mathbb{R}^{2n}_{\geq 0}\times \mathbb{R}^n\to\mathbb{R}$, 

\vspace{-0.4cm}
\small
\begin{align*}
L(\mathbf{\xi},\mathbf{\omega};q)=&\frac{1}{2}\lVert\mathbf{\xi}+\nabla F(q)-\pi_\theta(v)\rVert_2^2+\mathbf{\omega}^\top \Big(\frac{\partial g(q)}{\partial q}\xi +\alpha g(q) \Big) ,
\end{align*}
\normalsize 
where $\omega$ is the dual variable for \eqref{eq:cbf_constraint}.
The \textit{Karash-Kuhn-Tucker} (KKT) conditions of (\ref{eq:controller_equiv}) are:


\vspace{-4mm}
\small
\begin{subequations}
\label{opt:KKT}
\begin{align}
\mathbf{\xi}+\nabla F(q) -\pi_\theta(v)+ \frac{\partial g(q)}{\partial q}^\top\mathbf{\omega}&=0 \label{eq:lagrang}\\
\mathbf{\omega}\geq 0\,, \frac{\partial g(q)}{\partial q}\xi +\alpha g(q)\leq 0\\
\mathbf{\omega}^\top \left(\frac{\partial g(q)}{\partial q}\xi +\alpha g(q)\right)=0\label{eq:lagrang-d}
\end{align}
\end{subequations}
\normalsize
Because \eqref{eq:controller_equiv} is strongly 
% \todo{this should be strictly convex not strongly convex?} 
convex with respect to $\xi$, the existence of a $(\mathbf{\xi},\mathbf{\omega})$ satisfying (\ref{opt:KKT}) is sufficient for $\mathbf{\xi}=f_\theta(q)$. To verify the feasibility of the KKT conditions, we apply the Mangasarian-Fromovitz Constraint Qualification (MFCQ) condition, which requires the existence of $\mathbf{\xi}\in\mathbb{R}^n$, s.t.

\vspace{-0.4cm}
\small
\begin{equation*}
    \nabla g_i(q)^T\xi<0 \quad \forall i\in I_0(q)=\{1\leq i\leq 2n|g_i(q)<0\}
\end{equation*}
\normalsize
where $I_0(q)$ is the active constraint set. Given the specific structure of $g(q)$, $\forall q\in\mathcal{S}_q$, there always exists an $\xi$ such that the MFCQ is satisfied. By Lemma 4.5 of \cite{allibhoy2022control}, the existence of a solution $(\mathbf{\xi},\mathbf{\omega})$ satisfying \eqref{opt:KKT} is guaranteed. We next characterize the stability properties of our proposed algorithm with the solution $(\mathbf{\xi},\mathbf{\omega})$ of the KKT conditions \eqref{opt:KKT}.

% By Assumption \ref{thm:optimality(stability)}, 
% $F(q)$ is strictly convex function with respect to $q$, thus $-\nabla F_i(q_i)$ is monotonically decreasing. 
% % Given that $v^*\in S_v$ and the dynamics (\ref{eq:power_flow}), we have 
% \begin{align*}
% -\nabla F_i(q_i)&> 0 \text{ if } q_i<q^*_i\\
% -\nabla F_i(q_i)&< 0 \text{ if } q_i>q^*_i
% \end{align*}



% \begin{subequations}
%     \begin{align*}
%         &\quad\langle -\nabla F(q),-\nabla F(q)\rangle+2\langle \pi_\theta(v),-\nabla F(q)\rangle\\ 
%         &=\langle -\nabla F(q)+2\pi_\theta(v),-\nabla F(q)\rangle\\
%         &=\langle -\nabla F(q)+2K(v-v^*),-\nabla F(q)\rangle\\
%         &=\sum_{i} \left [\nabla F_i(q)\nabla F_i(q) -2(v_i-v^*_i)K_{ii}\nabla F_i(q)\right ]\\
%         &=\sum_{i} \left [\nabla F_i(q)\nabla F_i(q)\right ] + \sum_{\{i:K_{ii}<0\}}\left [ -2(v_i-v^*_i)K_{ii}\nabla F_i(q)\right ]
%     \end{align*}
% \end{subequations}

% \begin{subequations}
%     \label{eq:inner_product}
%     \begin{align*}
%         &\langle \pi_\theta(v),-\nabla F(q)\rangle\\ &=\langle K(v-v^*),-C_q(q-q^*)-(v-v^*)\rangle\\
%         &=\langle K(v-v^*),-(C_q X^{-1}+I)(v-v^*)\rangle \\%\label{eq:intro_dymcs}
%         &=-(v-v^*)^\top K(C_qX^{-1}+I)(v-v^*)\\%\label{eq:pd_condition}
%         &=-\frac{1}{2}(v-v^*)^\top\left [K(C_qX^{-1}+I)+(X^{-1}C_q+I) K\right ](v-v^*)
%     \end{align*}
% \end{subequations}
% where the second equation holds because $q=X^{-1}(v-v^{env})$. Following Proposition 1 of \cite{qu2019optimal},
% \begin{equation}
%   [X^{-1}]_{ij} =
%     \begin{cases}
%       \sum_{k:k\sim i} x_{ik}^{-1} & \text{if $i=j$}\\
%       -x_{ij}^{-1} & \text{if $i\sim j$}\\
%       0 & \text{otherwise}
%     \end{cases}       
% \end{equation}
% where $i\sim j$ means $i\neq j$ and i is a neighbor of j in the network, $x_{ij}>0$ for all $(i,j)\in \mathcal{E}$. Clearly, $X^{-1}$ is diagonally dominant by rows. Given that $C_q$ is a diagonal matrix with positive entries, $C_qX^{-1}+I$ is still diagonally dominant. Similarly, $K$ is a negative semi-definite diagonal matrix. \todo{We conclude that $K(C_qX^{-1}+I)$ is negative semi-definite, which indicts $\langle \pi_\theta(v),-\nabla F(q)\rangle\geq 0$.}


An immediate result of Lemma \ref{lemma:1} is 

\vspace{-0.4cm}
\small
\begin{align*}
    &\lVert \pi_\theta(v)-\nabla F(q)\rVert^2=\langle\pi_\theta(v)-\nabla F(q),\pi_\theta(v)-\nabla F(q) \rangle\\
    &=\lVert \pi_\theta(v)\rVert^2+\lVert -\nabla F(q)\rVert^2+2\langle \pi_\theta(v),-\nabla F(q)\rangle \geq \lVert \pi_\theta(v)\rVert^2
\end{align*}
\normalsize 
% \e{$\pi_{i,\theta_i}(v_i)>0$ when $v_i<\underline{v}_i$. We also have that $-\nabla F(q_i)$ is monotonically decreasing with an unique root $q_i^*$, which corresponds to an $v_i^*\in S_v$ by assumption~\ref{asp2}. As a result, if $v_i<\underline{v}_i$, we have $-\nabla F(q_i)>0$ and $\langle\pi_{i,\theta_i}(v_i), -\nabla F(q_i)\rangle > 0$. The same argument holds when $v_i>\bar{v}_i$, if $v_i\in S_v$, then $\pi_{i,\theta_i}(v_i)=0$. Thus they are in the same direction.}
% \wc{Add the proof for $\left \langle \pi_\theta, -\Delta F \right \rangle>0$. I think it can be expressed as a lemma}
% Given that $c\alpha (\underline{q}_i-q_i)\leq \pi_{i,\theta_i}(v_i) \leq c\alpha (\bar{q}_i-q_i)$, where $c\in[0,1)$, by Eq (\ref{ctl:solution}), we have
Thus, we have  $\lVert f_\theta(q, v) \rVert = \lVert [\pi_{\theta}(v)-\nabla F(q)]_{\alpha(\underline{q}-q)}^{\alpha(\bar{q}-q)} \rVert \geq \lVert \pi_{\theta}(v) \rVert.$  
Using $F(q)$ as a Lyapunov-like function, from Eq \eqref{eq:lagrang}, we have  $\nabla F(q) =-\frac{\partial g(q)}{\partial q}^\top\mathbf{\omega}  -f_\theta(q, v)+\pi_\theta(v)$, thus
% \begin{equation*}
%   \nabla F(q) =-\frac{\partial g(q)}{\partial q}^\top\mathbf{\omega}  -f_\theta(q)+\pi_\theta(v)
% \end{equation*}
\small
% \begin{subequations}
\begin{align}
\label{opt:growth}
&L_{f_\theta}F(q(t))=f_\theta(q(t), v(t))^\top\nabla F(q(t)) \nonumber\\
&=f_\theta(q(t), v(t))^\top \left(-\frac{\partial g(q(t))^\top}{\partial q(t)}\mathbf{\omega}-f_\theta(q(t), v(t))+\pi_\theta(v(t))\right) \nonumber\\
&=-\lVert f_\theta(q(t), v(t))\rVert^2+f_\theta^\top(q(t), v(t))\pi_\theta(v(t))+\alpha \mathbf{\omega}^\top g(q(t)) \nonumber\\
% &=-\lVert f_\theta(q(t))\rVert^2+\langle f_\theta(q(t)), \pi_\theta(v(t))\rangle+\alpha \mathbf{\omega}^\top g(q(t))\\
&\leq -\lVert f_\theta(q(t), v(t))\rVert^2+||f_\theta(q(t), v(t))\rVert\lVert \pi_\theta(v(t))\rVert+\alpha \mathbf{\omega}^\top g(q(t))\nonumber\\
&\leq 0
\end{align}
%Jie: anyway there will be an equation index, and if keep the \leq 0 at that line, there will be an outflow.
% \end{subequations}
% \vspace{-1mm}
\normalsize
where the second equality follows Eq \eqref{eq:lagrang-d}. Given that $g(q(t))\leq 0$, $\omega$ is a nonnegative dual variable, $\alpha>0$, $\alpha\mathbf{\omega}^\top g(q(t))\leq 0$ holds, which leads to the final inequality. 
% \wc{It might be good to explicit mention why $\alpha \mathbf{\omega}^\top g(q(t))>0$}

%Note that for $q(t)\in \mathcal{S}_q$, $g(q(t))\leq 0$ and $\mathbf{\omega}\geq 0$. Therefore $L_{f_\theta}F(q)\leq 0$ along the trajectory. 
Furthermore, $L_{f_\theta}F(q)=0$ if and only if $f_\theta(q^*,v^*)=0$. Then there exists $(0,\omega^*)$, which is the solution of \eqref{opt:KKT}. Plug $(0,\omega^*)$ into \eqref{opt:KKT}, it is reduced to

\vspace{-0.3cm}
\small
\begin{subequations}
\label{opt:KKT_eq}
\begin{align}
\nabla F(q^*) -\pi_\theta(v^*)+ \frac{\partial g(q^*)}{\partial q}^\top\omega^*&=0 \label{eq:lagrang2}\\
\omega^*\geq 0\,, \alpha g(q^*)\leq 0\\
(\omega^*)^\top \left(\alpha g(q^*)\right )=0\label{eq:lagrang-d2}
\end{align}
\end{subequations}
\normalsize
By Assumption \ref{asp2} where $v^*\in \mathcal{S}_v$, we have $\pi_\theta(v^*)=0$. Given that $\alpha>0$, it follows immediately $(v^*,q^*)$ is the optimal solution of \eqref{opt:steady1}. 
Due to the strict convexity of $F(q)$, $q^*$ is the unique global minimizer. By Lyapunov relative stability \cite{doi:10.1137/S0363012902407119}, we conclude that the closed loop system is asymptotically stable with respect to the global minimizer $(v^*,q^*)$. 
\end{proof}
% \begin{remark}
% The existence of a stricter bound in the Definition of \ref{def:stable_ctl} can reduce the magnitude of $K(v)$. Although it may fail to enforce $\sigma_{\min}(A)\geq 2\sigma_{\max} (K(v))$, it can empirically further prevent the transient policy from using too much reactive power than only using the safe gradient flow.
% \end{remark}
%\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stable transient policy design}
% As stated in Theorem \ref{thm:optimality(stability)}, the transient performance optimizer $\pi_\theta(v(t))$ needs to satisfy the following three conditions for all bus $i$.
We now present the neural network design that meets the stable decentralized transient policy $\pi_{i,\theta_i}(v_i)$ in Definition~\ref{def:stable_ctl}.
% a monotone neural network structure with state-dependent output bound to construct the stable transient controller. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Blank_diagram.png}
    \caption{A policy neural network design meets Definition \ref{def:stable_ctl}. 
    % \todo{Is this plot confusing? Yes...I think we should just say if $v_i \geq 0$ and if $v_i <0$ rather than the two blue blocks $v_i$ and $-v_i$. The equations in the $c\alpha tanh$ blocks are quite small and hard to see...}
    }
    \label{fig:mono_net}
\end{figure}

\textbf{Conditions 1) and 2):}  %The neural network policy must be monotonically decreasing on $(-\infty,\underline{v}_i]$ and $[\bar{v}_i,\infty)$ with bounded output, and $\pi_{i,\theta_i}(v_i) = 0$ for $v_i \in [\underline{v}_i,\bar{v}_i]$.  
To ensure conditions 1) and 2), we adopt the structure in~\cite{cui2022structured} for each bus $i$. The stacked ReLU function constructed by Eq~\eqref{eq:relu_pos} is monotonically decreasing for $v_i-\bar{v} > 0$ and zero when $v_i-\bar{v} \leq 0$. 

\vspace{-0.3cm}
\begin{small}
\begin{subequations}\label{eq:relu_pos}
    \begin{align}
    &\xi^{+}(v_i-\bar{v}; w^+, b^+) = {(w^+)^\top} \text{ReLU}(\mathbf{1} (v_i-\bar{v}) + b^+),\\
    &\sum_{l=1}^{d'} w^{+}_l < 0, \forall d' = 1, \!\cdots\!, d\,, b^{+}_1 = 0, b^{+}_l \leq b^{+}_{l\!-\!1}, \forall l =2, \cdots, d. 
    \end{align}
\end{subequations}
\end{small}
The stacked ReLU function constructed by Eq~\eqref{eq:relu_neg} is monotonically decreasing for $v_i-\underline{v} < 0$ and zero otherwise.

\vspace{-0.4cm}
%when $v_i-\underline{v} \geq 0$.
\begin{small}
\begin{subequations}\label{eq:relu_neg}
    \begin{align}
    &\xi^{-}(v_i-\underline{v}; w^{-}, b^{-}) = (w^{-})^\top \text{ReLU}(-\mathbf{1} (v_i-\underline{v}) + b^{-}),\\
    & \sum_{l=1}^{d'} w^{-}_l > 0, \forall d' = 1, \!\cdots\!, d\,, b^{-}_1 = 0, b^{-}_l \leq b^{-}_{l-1}, \forall l =2,\!\cdots\!, d. 
    \end{align}
\end{subequations}
\end{small}
% Finally, the local policy network of bus i is defined as $\pi_{i,\theta_i}(x)=\xi^{+}(x; w^+, b^+)+\xi^{-}(x; w^{-}, b^{-})$, where $\theta_i$ is the collection of local parameters.
%For situations where the voltage $v_i$ is greater than the upper threshold $\bar{v}$, we utilize $\xi^{+}(x; w^+, b^+)$, and for scenarios where the voltage is lower than the lower threshold $\underline{v}$, we employ $\xi^{-}(x; w^{-}, b^{-})$.
%By Theorem \ref{thm:optimality(stability)}, In this paper, we follow the monotonic neural network design of Stable-DDPG with modification. The network can first be discomposed into positive and negative parts, which are defined by (\ref{eq:relu_pos}) and (\ref{eq:relu_neg}), respectively. 


\textbf{Condition 3)} This condition requires the output of $\pi_{i,\theta_i}(v_i)$ to be bounded. To satisfy this requirement, the $\tanh$ activation function is used to scale the output as a percentage of the largest output while preserving its sign. Then the percentage is multiplied by the absolute value of the bounds. The local policy network of bus $i$ is defined as,

\vspace{-0.4cm}
\small
\begin{multline}
     \pi_{i,\theta_i}(v_i)=c\alpha(q_i-\underline{q}_i)\tanh(\xi^{+}(v_i-\bar{v}; w^+, b^+))\\
     +c\alpha(\bar{q}-q_i)\tanh(\xi^{-}(v_i-\underline{v}; w^{-}, b^{-})).
\end{multline}
\normalsize
% where $\theta_i$ denotes the set of parameters inclu. 
It is noteworthy that the monotonicity of the $\tanh$ function ensures that conditions 1) and 2) are still satisfied. An illustration of the stable transient policy is given in Figure \ref{fig:mono_net}.

%Following Theorem \ref{thm:optimality(stability)}, with the stable transient $\pi_\theta$, 
% \todo{expand this part to discuss what are the conditions need to be satisfied, and how you design the $\tanh$ part to satisfy this condition} 

% \subsection{Gradient of the steady-state cost function $\nabla F(q(t))$}


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=8cm]{figures/system diagram.png}
%     \caption{Controller implementation. \todo{In the title, we say "RL with safe gradient flow", but so far the RL part is not written clearly. I'll suggest changing this graph to RL algorithm with the policy computing equations in the algorithm, especially for the projection layer, we shall use \eqref{ctl:solution} rather than solving the optimization. The overall RL algorithm part can be stylized, but it needs to clearly show how the learning proceeds, e.g., get state -> use policy to compute action -> transient to next state and get reward -> how to use the samples to update policy, etc}}
%     \label{fig:policy-diagram}
% \end{figure}
% When $\alpha=1$, it reduces to a hard constraint on $\mathbf{q}$. 
% $c$ determines the potential portion of the reactive power allocation by $\pi_\theta(\mathbf{v}(t))$. If $c=1$, suppose $\pi_\theta(\mathbf{v}(t))=\alpha(\bar{\mathbf{q}}-q)$, then $\Pi_\alpha(\mathbf{q}(t),\mathbf{v}(t))=\pi_\theta(\mathbf{v}(t))$. 

%The proposed algorithm works as follows. At every time t, we measure the voltage magnitude of all the controlled buses. With the current observation, Stable-DDPG  generates the transient control action $\pi_\theta(\mathbf{v}(t))$. By (\ref{eq:gradient}), the gradient of F is calculated in a separate way. We further input $-\nabla F(\mathbf{q}(t))+\pi_\theta(\mathbf{v}(t))$ into the solver (\ref{ctl:solution}) to get the current change rate of reactive power. Because (\ref{ctl:solution}) is differentiable, we can train the optimization-in-the-loop RL algorithm in an end-to-end manner. The policy diagram is provided as Figure (\ref{fig:policy-diagram}).


% The voltage control problem can be formulated as a control problem on a network system with state $\mathbf{v}$ and controller $\mathbf{q}$. Given local voltage measurement $v_i(t)$, the controller determines the reactive power injection $q_i(t)$ to mitigate voltage deviation. With fast-operating inverters, we define the change rate of reactive power as $\dot{q}_i(t)=u_i(t)$. The dynamics of the closed-loop system reads as 
% \begin{subequations}\label{eq_continuous}
% \begin{align}
%     \mathbf{v}(t) &= X\mathbf{q}(t) + \mathbf{v}^{env} \,,\label{eq:voltage_model}\\
%     \dot{\mathbf{q}}(t)&=\mathbf{u}(t) \label{eq_controller}
% \end{align}
% \end{subequations}
% where $\mathbf{u}(t)=(u_1(t),u_2(t),...,u_n(t))$ is the decentralized controllers. The goal of voltage control is to recover the system voltage to an acceptable range after an arbitrary disturbance. Formally, we define voltage stability and reactive power safety as below.



%With the stability and safety requirement, we further consider the optimality of the controller in both transient and steady-state. A system is said to be in a \textit{steady state} if the states are close to an equilibrium $x^*$, i.e. $\forall t\geq T, ||x(t)-x^*||\leq \epsilon$, where $\epsilon>0$ is the tolerance. Then the time from $0$ to $T$ is called a \textit{transient period}. We first discuss the transient performance, which is evaluated during the \textit{transient period}.


% \subsubsection{Stable-DDPG}
% We now introduce the Stable-DDPG algorithm \cite{https://doi.org/10.48550/arxiv.2209.07669} for optimizing the transient performance with stability guarantee. Stable-DDPG is a reinforcement learning method that inherits all the components of standard DDPG with the exception of a redesigned policy network to accommodate the stability condition. With the explicitly constructed Lyapunov function, asymptotical voltage stability can be ensured with monotonically decreasing local policies. These local policy networks are parameterized as monotone controllers~\cite{cui2020reinforcement} as follows.




%We now introduce the main framework for our stability-guaranteed reinforcement learning method which optimizes both transient and steady state performance under limited reactive power. To bridge the gap, we have two assumptions:
% \begin{assumption}\label{asp1} For all bus $i$, $\pi_{i,\theta_i}(\cdot)$ is a continuously differentiable function satisfying $\pi_{i,\theta_i}(v_i) = 0$ for $v_i \in [\underline{v}_i,\bar{v}_i]$. Further, each $\pi_{i,\theta_i}$ is monotonically decreasing on $(-\infty,\underline{v}_i]$ and $[\bar{v}_i,\infty)$.
% \end{assumption}
% \begin{assumption}
%     \label{asp2} The optimal solution $(v^{*}, q^{*})$ of $F(q)$ lies inside the safe set, i.e., $v^{*} \in S_v$.
% \end{assumption}


% \begin{corollary}(Stacked ReLU Monotone Network~\cite[Lemma 3]{cui2020reinforcement})
% \label{corollary:stacked_relu}
%  The stacked ReLU function constructed by Eq~\eqref{eq:relu_pos} is monotonic increasing for $x > 0$ and zero when $x \leq 0$.
% \begin{small}
% \begin{subequations}\label{eq:relu_pos}
%     \begin{align}
%     &\xi^{+}(x; w^+, b^+) = {(w^+)^\top} \text{ReLU}(\mathbf{1} x + b^+)\\
%     &\sum_{l=1}^{d'} w^{+}_l < 0, \forall d' = 1, ..., d\,, b^{+}_1 = 0, b^{+}_l \leq b^{+}_{l-1}, \forall l =2, ..., d
%     \end{align}
% \end{subequations}
% \end{small}
% The stacked ReLU function constructed by Eq~\eqref{eq:relu_neg} is monotonic increasing for $x < 0$ and zero when $x \geq 0$.
% \begin{small}
% \begin{subequations}\label{eq:relu_neg}
%     \begin{align}
%     &\xi^{-}(x; w^{-}, b^{-}) = (w^{-})^\top \text{ReLU}(-\mathbf{1} x + b^{-})\\
%     & \sum_{l=1}^{d'} w^{-}_l > 0, \forall d' = 1, ..., d\,, b^{-}_1 = 0, b^{-}_l \leq b^{-}_{l-1}, \forall l =2, ..., d
%     \end{align}
% \end{subequations}
% \end{small}
% \end{corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
In this section, we demonstrate the effectiveness of the proposed method in two IEEE distribution test systems.
\subsection{Experiment Setup}
We evaluate our approach on the IEEE 13-bus and 123-bus test feeders~\cite{8063903}. Though our theoretical analysis is based on the linearized system dynamics \eqref{eq:power_flow}, all experiments are run using Pandapower\cite{pandapower} as the nonlinear power flow simulator to evaluate the algorithm performance.  %Simulation environments are built upon pandapower . 
We simulate two different voltage disturbance scenarios: 1) \emph{High voltages:} with abundant sunshine during daylight, the PV generators generate excessive power that can lead to high voltage issues. 2) \emph{Low voltages:} the system is serving peak loads without enough generation. It represents late afternoon or night when there is less solar generation but significant loads. 
For each scenario, we vary $v^{env}$ to obtain different degrees of initial voltage disturbance, i.e., $5\%$ to $15\%$ of $v^{nom}$. 
We test our method against two baselines:
\begin{enumerate}
    \item Stable-DDPG~\cite{shi2022stability} with  Safety Filter: The Stable-DDPG~\cite{shi2022stability} optimizes the transient performance with a stability guarantee. To enforce reactive power safety, we incorporate the CBF-QP safety filter in \eqref{eq:controller_equiv} by replacing the obj \eqref{eq:cbf_obj} with $\argmin_{\xi \in \mathbb{R}^n} \frac{1}{2} \|\xi - \pi_{\theta}(v)\|_2^2 $.
    \item Safe gradient flow~\cite{allibhoy2022control}: Safe gradient flow optimizes the steady-state performance with reactive power safety.
\end{enumerate}
We use  the DDPG framework~\cite{lillicrap2015continuous} to train the policy network update (line 11 in Algorithm~\ref{alg:satrl}) in our TASRL algorithm.
% \todo{we should at least mention what RL algorithm we used for TASRL in our experiments}
 % for method 1) and $\argmin_{\xi \in \mathbb{R^n}} \frac{1}{2} \|\xi + \nabla F(q))\|_2^2 $ for method 2). 
% \e{
% \begin{remark}
% We did not enforce the relative strict condition $\sigma_{\min}(C_qX^{-1}+I)\geq 2\sigma_{\max} (K(v))$ during TASRL training. Instead, we ensured that $[A+K(v)]^\top[A+K(v)]- K(v)^TK(v)$ is a PSD matrix using trained models, which held true for all 1000 sampled trajectories. Another approach is to set $\pi_{i,\theta_i}(v_i)=0$ whenever $\langle \pi_{i,\theta_i}(v_i),-\nabla F_i\rangle <0$, ensuring that F(q) decreases along the trajectory regardless of the transient controller used. Notably, when we applied this trick to resolve conflicts in our experiments, we achieved the same performance as shown in the tables without using this method. This suggests that conflicts hardly occur when stable-transient policies are well-trained.
% \end{remark}
% }
% For all experiments, we use $T_f=100$, $\gamma=0.99$, and $\alpha = 0.5$. 
While the theoretical analysis is done in continuous time, in the numerical simulation the control is executed in a discrete manner with high-frequency sampling. With $h$ denoting the sampling period, the discrete-time update law of reactive power is defined as $q_i(t+1)=q_i(t)+hf_{i, \theta_i}(q_i(t), v_i(t))$.
% \begin{subequations}
% \label{opt:discrete}
% \begin{align}
% \min_{\mathbf{\theta}} \quad & J_d(\theta)= \sum_{t=0}^{T_f} \gamma^t \sum_{i=1}^n c_i({v}_i(t), q_i(t))  \label{eq_rl:obj}\\
% \text{s.t.}\quad 
%  &v(t) = Xq(t) + v^{env} \,,\\
%     &q(t+1)=q(t)+hf_{\theta}(q(t))\,,\\
%     &\eqref{eq:reactive_safety}\,,\eqref{eq:converge1} \text{ and }\eqref{eq:converge2}
% \end{align}
% \end{subequations}
% where $c_i(v_i(t),q_i(t))=C_i(q_i)+\frac{1}{2}q_i(v_i+v_i^{env}-2v_i^{nom})$. We denote $J_d(\theta)$ as the trajectory cost and, 
%Because $f_{\theta}(q(t), v(t))$ is bounded by \eqref{ctl:condition}, 
We define $\alpha_h = \alpha h$ 
%have $\alpha h (\underline{q}-q(t))\leq q(t+1)-q(t) \leq \alpha h (\bar{q}-q(t))$. We use $\alpha_h$ to represent $\alpha h$ for simplicity, 
where $\alpha_h\in (0,1]$. 
For all experiments, we use $t_f=100$, $\gamma=0.99$, and $\alpha_h = 0.5$. The range of reactive power generation is given by $|q|\leq \sqrt{s^2-p^2}\equiv
\bar{q}$. Following \cite{5768094}, it is reasonable to have $\bar{s}\approx 1.1\bar{p}$, where $\bar{s}$ is the apparent power capacity, $\bar{p}$ is the real power capacity. Under these conditions, we set $\bar{q}=\sqrt{\bar{s}^2-\bar{p}^2}\approx 0.45\bar{p}$, $\underline{q}=-\bar{q}$. 
Instead of enforcing $2\sigma_{\max} (K(v)) \leq \sigma_{\min}(C_qX^{-1}+I)$ as a constraint during TASRL training, we verified that $[C_q X^{-1} + I +K(v)]^\top[C_q X^{-1} + I+K(v)]- K(v)^TK(v) \succeq 0$ for the trained models, which guarantees the Lie derivative $L_{f_\theta}F(q(t))$ is strictly decreasing at all steps except at $q^*$. %which held true for all 1000 sampled trajectories.

%In this case, if $\alpha=1$, the safe gradient flow method reduces to the projected gradient flow method.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}
% \todo{Let's first present the simulation results, then demonstrate the effect of $\alpha$. I exchanged the order of B and C}

% \todo{At the beginning of this subsection, properly describe all the baselines that we compare against.}
%In the following discussions, all controllers are tested with fixed $\alpha_h=0.5$. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.22\textwidth]{figures/13busai.png}
%     \caption{Schematic diagram of the 13 bus system with three PV generators and voltage controllers located at nodes 2, 7, and 9.}
%     \label{fig:13bus}
% \end{figure}
% \begin{figure}[tb]
%     \centering
    \includegraphics[width=0.22\textwidth]{figures/123busai.png}
    \caption{Left: Schematic diagram of the 13 bus system with three PV generators and voltage controllers located at nodes 2, 7, and 9. Right: Schematic diagram of 123 bus system, with 14 PV generators and voltage controllers located at nodes 10, 11, 16, 20, 33, 36, 48, 59, 61, 66, 75, 83, 92, and 104.}
    \label{fig:13_123bus}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{IEEE 13-bus}
IEEE 13-bus system is a standard radial distribution system depicted in Figure \ref{fig:13_123bus} (Left), where three PV generators and voltage controllers are located at buses 2, 7, and 9. The nominal voltage magnitude at each bus except the substation is 4.16 kV. The safe operation range $\mathcal{S}_v$ is defined as $\pm 5\%$ of the nominal value, that is $[3.952\text{kV}, 4.368\text{kV}]$. 
% \todo{how about the reactive power safety set?}

Table \ref{table:13_bus_result} compares the transient and steady-state performance of 500 different voltage violation scenarios. Clearly, TASRL achieves the best performance for both transient and steady states. 
%For transient performance, given that the $v^*\in S_v$, the gradient can help the Stable-DDPG method and achieve the best transient performance by both minimizing $F(q)$ and leading the voltage to $S_v$.
For this test case, the magnitude of the gradient is relatively large. %Because of the reactive power cost, the reactive power injection of the Stable-DDPG is restrained. 
As a result, the transient performance of the safe gradient flow is close to TASRL, and the Stable-DDPG is underperforming. 
%Notably, this result also suggests that the gradient may provide valuable information that is difficult for the RL controller to learn on its own. Incorporating this information could potentially accelerate voltage recovery. 
In terms of steady-state performance, both the safe gradient flow and the TASRL achieve the best result as $v(t)$ and $q(t)$ converge to the steady-state optima~$(v^*, q^*)$. 
% \todo{comment on the results. E.g., which algorithm performs the best, and why? Why do other methods not achieve the best transient or steady-state cost? What is the takeaway from this table \ref{table:13_bus_result}?}
\begin{table}[tb]
% \vspace{-3mm}
\centering
\caption{Performance of 500 scenarios for 13-bus system.}%The maximum time step is 100.
 \label{table:13_bus_result}
 \begin{tabular}{lccc}
    \toprule
     & \multicolumn{2}{c}{Transient Performance}  & \multicolumn{1}{c}{Steady State}  \\
    \cmidrule(r){2-4}
     Method& \shortstack{Recovery\\Time}  & \shortstack{Transient\\Cost} & \shortstack{Objective\\ $F(\mathbf{q})$ } \\
    \midrule
    %Linear & 10.25 & 11.82 & N/A &  -0.091  & -0.10\\
   Stable-DDPG \cite{shi2022stability}& 10.18 %& 11.88 
    & -5.61 %&  -0.091 
    & -0.09\\
    %Projected Gradient ($\alpha = 1$) & &       &        \\
    Safe gradient flow \cite{allibhoy2022control} &3.04  & -6.74 & 
    %-0.97  & -0.095& 
    \textbf{-0.11}  \\
    %Stable-DDPG +Proj Gradient & 4.43 & &-0.11\\
    TASRL & \textbf{2.60} & \textbf{-6.76} & \textbf{-0.11}\\
    % %Linear+grad &4.44 &6.84 & N/A &-0.095&-0.11\\
    % Stable-DDPG +Safe Gradient &4.43 &6.84 & N/A &-0.096&-0.11\\
    \bottomrule
\end{tabular}
\\Note: Smaller value means better performance for all three columns.
% \vspace{-4mm}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{IEEE 123-bus}
Figure \ref{fig:13_123bus} (Right) demonstrates the IEEE 123-bus distribution test feeder, which has 14 PV generators and controllers randomly placed in the network. %elected to be placed at Buses 10, 11, 16, 20, 33, 36, 48, 59, 61, 66, 75, 83, 92, and 104. 
The nominal voltage magnitude at each bus except substation is 4.16 kV, and the acceptable range of operation is $\pm 5\%$ of the nominal value which is $[3.952\text{kV}, 4.368\text{kV}]$. 
We summarize the performance of our method and the baselines in Table \ref{table:123_bus_result}. The average response time for TASRL is 12.08 steps, which saved $77\%$ of time compared to the safe gradient flow, and $30\%$ compared to the Stable-DDPG. Both the TASRL and safe gradient flow obtain optimal steady-state cost, while the Stable-DDPG has suboptimal steady-state performance. 
Interestingly, compared to the results of IEEE 13-bus, the gap between the steady-state performance of Stable-DDPG and the other two methods is larger. This indicates that optimizing the steady-state performance becomes increasingly crucial as the system complexity increases.

\begin{table}[tb]
% \vspace{-3mm}
\centering
\caption{Performance of 500 scenarios for 123-bus system.}%The maximum time step is 100.
 \label{table:123_bus_result}
 \begin{tabular}{lccc}
    \toprule
     & \multicolumn{2}{c}{Transient Performance}  & \multicolumn{1}{c}{Steady State}  \\
    \cmidrule(r){2-4}
    Method & \shortstack{Recovery\\Time}  & \shortstack{Transient\\Cost} & \shortstack{Objective\\ $F(\mathbf{q})$}\\
    \midrule
    %Linear & 10.25 & 11.82 & N/A &  -0.091  & -0.10\\
    Stable-DDPG~\cite{shi2022stability} & 17.36 %& 11.88 
    &  -303.44%&  -0.091 
    & -4.90\\
    %Projected Gradient ($\alpha = 1$) & &       &        \\
    Safe gradient flow\cite{allibhoy2022control} &52.43  & -254.72 & 
    %-0.97  & -0.095& 
    \textbf{-5.95}  \\
    %Stable-DDPG +Proj Gradient & 4.43 & &-0.11\\
    TASRL & \textbf{12.08} & \textbf{-333.03} & \textbf{-5.95}\\
    % %Linear+grad &4.44 &6.84 & N/A &-0.095&-0.11\\
    % Stable-DDPG +Safe Gradient &4.43 &6.84 & N/A &-0.096&-0.11\\
    \bottomrule
\end{tabular}
\\Note: Smaller value means better performance for all three columns.
% \vspace{-3mm}
\end{table}
% \begin{table}[H]
% \centering
% \caption{Performance of 500 voltage violation scenarios for 123 bus system.}%The maximum time step is 100.
%  \label{table:123 bus with safe}
%  \begin{tabular}{lccccc}
%     \toprule
%     & \multicolumn{4}{c}{Transient}  & \multicolumn{1}{c}{Steady}  \\
%     \cmidrule(r){2-6}
%     Method     & step     & Q cost & Traj Cost & F value& Steady \\
%     \midrule
%     Linear & 28.58 & 1467.16 & N/A &  -4.52  & -4.79\\
%     Stable-DDPG & 23.64 & 1287.67 & N/A &  -4.54 & -4.80\\
%     Gradient &69.02  & 2522.12 & -259.75  & -4.44&-4.76   \\
%     Linear+grad &22.72 &1222.54 & N/A &-4.58&-5.02\\
%     Stable-DDPG +grad &19.95 &1126.79 & N/A &-4.58&-5.02\\
%     \bottomrule
% \end{tabular}
% \end{table}
% \todo{does that make sense to only record C(q)}

% \todo{Again, comment on the results. E.g., which algorithm performs the best, and why? Why do other methods not achieve the best transient or steady-state cost? What is the takeaway from this table \ref{table:123_bus_result}?}
Figure~\ref{fig:123_traj} shows an example control trajectory of the proposed approach and the baselines at bus 20 and bus 66. %It should be noted that bus 16 has a greater capacity for reactive power than bus 66, resulting in a reduced cost per MVar used by bus 16. 
Although both the Stable-DDPG and the proposed method restore voltage quickly, Stable-DDPG uses more reactive power at bus 66 and less at bus 20, leading to a suboptimal solution for $F(q)$. On the other hand, the safe gradient flow and the proposed method converge to the same steady state, while the safe gradient flow's convergence is slower.
\vspace{-2mm}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=8.5cm]{figures/3in1.png}
    \caption{Control trajectory of IEEE 123-bus system. The voltage trajectory is shown in the left plot, the middle plot displays the reactive power usage, and the right plot shows the objective function's trajectory.
    }
    \label{fig:123_traj}
\end{figure}
% Finally, Figure \ref{fig:13bus_F} demonstrates the trajectory of the objective function with different controllers. Our method can achieve the optimal steady state faster than the safe gradient flow. While the Stable-DDPG will dwell at suboptimal solutions.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=5cm]{figures/123F.png}
%     \caption{The trajectory of the objective function $F(q)$. Note: the value of $F(q)$'s minima depends on the initialization. \todo{combine Fig 3 and 4 to save some space - ensure readability}}
%     \label{fig:13bus_F}
% \end{figure}
% % \todo{revise plot to compare 3 methods}
\vspace{-1em}
\subsection{Effect of design parameter of safe gradient flow}
We illustrate the effect of hyperparameter $\alpha_h$ in our proposed method's conservatism in Figure \ref{fig:alpha}. Smaller $\alpha_h$ results in a smoother voltage trajectory, as the controller becomes more conservative for the reactive power capacity constraints. The right plot shows the corresponding reactive power injection, which is not significantly affected by $\alpha_h$ when it is far from the capacity limit due to the presence of a transient performance optimizer (bus 16). However, when approaching the capacity limit, smaller $\alpha_h$ will slow down the rate of change of reactive power injection (bus 66).
\vspace{-3mm}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=8.4cm]{figures/effect_of_alpha.png}
    \caption{Control trajectory of IEEE 123-bus system with our proposed method using different $\alpha_h$. For the sake of simplicity, we did not plot $\underline{q}_{16}=-21.6$.}
    \label{fig:alpha}
\end{figure}
\vspace{-1em}

% \subsection{Effect of $\alpha$ in safe gradient flow}

% \todo{I think we'd better demonstrate the affect of $\alpha$ in our algorithm rather than in linear policy?}

% Following safe gradient flow (\ref{ctl:safe-gradient-flow}), replace $\nabla F(\mathbf{q}(t))$ by some controller $\mu(\mathbf{q}(t),\mathbf{v}(t))$, which is formulated as follows:
% \begin{subequations}
% \label{ctl:safe-flow}
% \begin{align}
% \Pi_\alpha(\mathbf{q}(t),\mathbf{v}(t))=&\argmin_{\mathbf{\xi}\in\mathbb{R}^n} \frac{1}{2}\lVert\mathbf{\xi}+\nabla \mu(\mathbf{q}(t),\mathbf{v}(t))\rVert^2\\
% \text{s.t.}\quad 
%     & \alpha (\underline{\mathbf{q}}-\mathbf{q}(t))\leq \mathbf{\xi} \leq \alpha (\bar{\mathbf{q}}-\mathbf{q}(t))\label{ctl:safe-flow-cdt}
% \end{align}
% \end{subequations}
% We define this class of controller (\ref{ctl:safe-flow}) as safe flow methods. In this session, we show that safe flow method can improve the ability to stabilize the system by choosing a proper value of hyperparameter $\alpha$. The sanity check is conducted with a linear controller $\mu_i(v_i) = -\epsilon_i ([v_i - \overline{v}_i]^{+} - [\underline{v}_i - v_i]^{+})$ (where $[x]^+= \max(x,0)$, $\epsilon_i$ is a constant slope chosen by experience) under extremely limited reactive power. 

% We choose IEEE-13-bus system with massive solar penetration as the test environment, where \emph{all buses} are associated with PV and voltage controllers. The grid system is depicted by figure \ref{fig:13bus}. We set the reactive power capacity of a bus as $10\sim 20\%$ of the largest real power generation. (Generally speaking, $|q_i|<0.45|p_{max}|$, where $p_{max}$ is the maximal real power generation~\cite{5768094}.) 



% We first show that when the safe flow is getting more conservative with smaller $\alpha$, the controller is more capable to stabilize the system. And possibly the transient performance can also be improved. The result is given in table \ref{table:change_alpha}. When $\alpha=1$, $\mathbf{q}(t)$ is truncated at the reactive power limitation. If $\alpha<1$, the soft bound will drift $\mathbf{q}(t)$ before it hits the boundary. Although smaller $\alpha$ can slow down the control process, the safe flow method can have better result by taking reactive power capacity into consideration instead of making decisions only based on voltage deviation.

% \begin{table}[tb]
% \centering
% \caption{Performance of linear controller with different $\alpha$ under limited reactive power.}%The maximum time step is 100.
%  \label{table:change_alpha}
%  \begin{tabular}{lccccc}
%     \toprule
%     & \multicolumn{4}{c}{$\alpha$}  &  \\
%     \cmidrule(r){2-6}
%     Metric     & 1     & 0.8 & 0.5 & 0.3& 0.1 \\
%     \midrule

%     Success &349  & 351 & 369  & 379&395   \\
%     Recovery step &32.98 &32.34 & 29.05 & 28.08&31.16\\
%     \bottomrule
% \end{tabular}
% \end{table}
% To further investigate how the safe flow method improves the performance of the linear controller, we plot figure \ref{fig:safe-flow-alpha}. Given that Bus 5 has more reactive power capacity than Bus 11, without safe flow, linear controller tends to use more reactive power at bus 11 than bus 5. Due to the unreasonable allocation, when $\alpha=1$, the linear system failed to stabilize the system. In the other hand, when $\alpha$ gets smaller, more reactive power is allocated to Bus 5 and finally achieved voltage stability of the system. This experiment shows that by tuning the hyperparameter $\alpha$, even the simplest methods like linear controller can be aware of the reactive power capacity and adjust the control actions accordingly, which leads to a better stabilization ability.
% \begin{figure}[tb]
% \centering
% \includegraphics[width=8cm]{figures/output.png}
% \caption{Linear Controller with different $\alpha$ values}
% \label{fig:safe-flow-alpha}
% \end{figure}



% Fix $\alpha=0.7$, we summarize the following table
% \begin{table}[H]
% \centering
% \caption{Performance of linear, gradient, and linear+gradient policies on 500 voltage violation scenarios for 13 bus system.}%The maximum time step is 100.
%  \label{table:13 bus with safe}
%  \begin{tabular}{lccccc}
%     \toprule
%     & \multicolumn{4}{c}{Transient}  & \multicolumn{1}{c}{Steady}  \\
%     \cmidrule(r){2-6}
%     Method     & step     & Q cost & Traj Cost & F value& Steady \\
%     \midrule
%     Linear & 32.882 & 38.18 & -4.72 &  -0.152  & -0.155\\
%     Grad &3.042  & 4.82 & -0.61  & -0.159&-0.171   \\
%     Linear+grad &1.88 &3.88 & -0.46 &-0.164&-0.171\\
%     \bottomrule
% \end{tabular}
% \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
We proposed the TASRL framework to optimize transient and steady-state performance simultaneously for voltage control and established formal guarantees for it.
%regarding transient stability and steady-state optimality.
The main insight underlying our approach is that, by synthesizing a stable transient policy and a steady-state optimizer within a safe gradient flow framework, the performance of different time scales can be optimized end-to-end. Our proposed method was tested on both IEEE 13-bus and 123-bus systems. The results demonstrate that TASRL not only converges to the steady-state optimal solution but also exhibits superior transient performance compared to existing methods.
\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.


%
% \marginJC{Would be nice to make a pass and ensure reference format is consistent (I improve several things, but more remain): e.g., we use both "IEEE Transactions on Power Systems" and "IEEE Trans. Power Syst."; Conference names should have first letter capitalized, like "International Conference on Machine Learning", etc. Plus anything you can think of that is inconsistent}
%

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ref}
\end{document}
