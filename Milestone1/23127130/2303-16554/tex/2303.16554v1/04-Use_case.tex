\section{Unconventional Visual Communication} \label{sec:use_case}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{images/incremental-samples.png}
  \caption{Small dots denote individual CNN predictions of the transmitter LED state (12 per bit), before averaging. Vertical shaded areas denote the bit clock. Colors denote 2 start, 8 payload and 2 stop bits.}
  \label{fig:incremental-samples}
\end{figure}

\textbf{CNN training.} 
To decode the message transmitted by OpenTitan of a compromised drone, we use a lightweight CNN with the field-proven architecture of PULP-Frontnet~\cite{frontnet}.
The model input is a $160\times96$ pixels grayscale image; the output estimates the state of the LEDs of a Crazyflie nano-drone that is assumed to be visible in the image (either all on or all off), regardless of the drone position in the frame.
Datasets are acquired with a monochrome QVGA camera from an \textit{observer} drone facing a \textit{transmitter} drone, which toggles its four LEDs every $\sim\SI{0.4}{\second}$.

The dataset is collected in a room equipped with a 18-camera Optitrack motion capture system, which tracks both drones.
The transmitter drone flies in the central part of the room, while the observer records images while following a circular path around the transmitter.  
In this way, we maximize the variability of the images' backgrounds.  
The transmitter drone is automatically controlled in order to: \textit{i}) always stay within the field of view of the observer; \textit{ii}) move to random positions uniformly distributed on the observer's image plane; and \textit{iii}) always lie at a distance of \SIrange{0.2}{1.8}{\meter} from the observer.  
The dataset is composed of 36 flights, where frames are split to build a training set (the first 72\%), a testing set (the intermediate 19\%), and a validation set (the last 9\%).
A 10-frame gap is ignored between different sets in the same flight, to ensure that no similar frames appear in different sets.  
This results in \SI{37}{\kilo\nothing} frames for training, \SI{10}{\kilo\nothing} frames for testing and \SI{5}{\kilo\nothing} frames for validation; each frame is labeled with the corresponding ground-truth LED state; the two states are equally represented in all sets.

\textbf{Message encoding and decoding.}
Messages with an 8-bit payload are encoded to a simple self-clocking binary line protocol~\cite{halsall1995data} that produces 12-bit packets, including 2 start bits and 2 stop bits.
On our prototype, we employ the 8-cores GAP8 SoC manufactured in TSMC \SI{55}{\nano\meter} technology capable of \SI{22.65}{\giga Op/\second} at \SI{4.24}{\milli\watt/\giga Op}.
With this SoC, the bit stream is transmitted by modulating the LED state at a rate of 2.5 bits per second; each 12-bit packet is therefore transmitted in \SI{4.8}{\second}.
The observer drone acquires images at 30 frames per second (FPS).
Each image is fed to the CNN, which estimates the LED's state in each frame.
Each bit appears in 12 consecutive frames.
First, the \textit{bit clock} is determined from this sequence, then each bit in the bit stream is estimated by averaging the corresponding 12 CNN outputs and thresholding the result.  
Messages are decoded from the bit stream starting with a reserved start flag. 
If necessary, error detection and correction codes~\cite{hamming1950error} can be implemented on top of this approach.