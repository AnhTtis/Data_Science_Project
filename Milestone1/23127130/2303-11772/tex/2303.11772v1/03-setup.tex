\section{Methodology}\label{sc:method}



The idea behind our measurements is to evaluate the route that packets traverse to a destination prefix in two scenarios: when the target prefix is hijacked and conflicts with the origin AS in the ROA covering that prefix, and when that prefix is legitimate and matches the ROA. ASes that fall victim to the hijack are classified as non-enforcing, while ASes that adapt their routing according to ROV validation results are classified as enforcing ROV.

To run ethical experiments, we hijack the prefixes on self-managed/owned resources and probe the reaction of ASes both on the control-plane and on the data-plane. We minimize any interference between Internet Routing Registry (IRR) based filtering and our experiment by creating and maintaining proper IRR records (route, route6, aut-num, etc. objects) for all announcements. This ensures that IRR-based filters allow all prefix-origin pairs announced by us to the Default Free Zone.

Our study focuses on ROV enforcement in IPv4, as it is still the predominant technology in today's Internet and differences in routing between IPv4 and IPv6 are beyond the scope of this work. However, the presented methodology is directly applicable to IPv6 measurements.


\subsection{Methodology Derivation}
We explain our methodology by first introducing approaches of the most relevant previous work: Hlavacek et al \cite{hlavacek2018practical} and Rodday et al \cite{rodday2021revisiting}. We illustrate their core ideas and identify their shortcomings. We then derive our methodology by using strengths of the presented approaches while improving on identified shortcomings with new techniques.
\begin{figure}[t!]
\resizebox{0.4\textwidth}{!}{%
\begin{minipage}[t]{0.34\textwidth}
        \begin{tikzpicture}
            \definecolor{c_red}{RGB}{220, 68, 51}
            \definecolor{c_green}{RGB}{11, 218, 81}
            \definecolor{node_red}{RGB}{255, 200, 200}
            \definecolor{node_green}{RGB}{200, 255, 200}
    
    
    
            \tikzset{vertex/.style = {shape=circle,draw,minimum size=2.2em}}
            \tikzset{edge/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, green, transform canvas={yshift=4pt}, > = latex'}}
                    \tikzset{edge_f/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, transform canvas={yshift=-4pt}, > = latex'}}
        
            % vertices
            \node[vertex, fill=node_red] (b) at  (1.7,0) {1};
            \node[vertex, fill=node_red] (c) at  (3.4,0) {2};
            \node[vertex, fill=node_green] (d) at  (4.9,1) {$t_1$};
            \node[vertex, fill=node_red] (g) at  (4.9,-1) {3};
            \node[vertex, fill=node_red] (f) at (6.6,-1) {$t_2$};
            
            %edges
            \draw (b) to (c);
            \draw (c) to (d);
            \draw (c) to (g);
            \draw (g) to (f);
            
            \draw[edge_f] (b) to (c);
            \draw[edge_f] (c) to (d);
        
            \draw[edge, c_green] (c) to (b);
            \draw[edge, c_green] (d) to (c);
            \draw[edge, c_red, transform canvas={yshift=-8pt}] (f) to (g);
            \draw[edge, c_red, transform canvas={yshift=-8pt}] (g) to (c);
        
            
            \end{tikzpicture}
            \captionsetup{labelformat=empty}
            \captionof*{figure}{(a) Propagation Prefix 1}
     \end{minipage}\vspace{50mm}
     \begin{minipage}[t]{0.3\textwidth}
        \begin{tikzpicture}
            \definecolor{c_red}{RGB}{220, 68, 51}
            \definecolor{c_green}{RGB}{11, 218, 81}
            \definecolor{node_red}{RGB}{255, 200, 200}
            \definecolor{node_green}{RGB}{200, 255, 200}
    
    
    
            \tikzset{vertex/.style = {shape=circle,draw,minimum size=2.2em}}
            \tikzset{edge/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, green, transform canvas={yshift=4pt}, > = latex'}}
                    \tikzset{edge_f/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, transform canvas={yshift=-4pt}, > = latex'}}
        
            % vertices
            \node[vertex, fill=node_red] (b) at  (1.7,0) {1};
            \node[vertex, fill=node_red] (c) at  (3.4,0) {2};
            \node[vertex, fill=node_red] (d) at  (4.9,1) {$t_1$};
            \node[vertex, fill=node_red] (g) at  (4.9,-1) {3};
            \node[vertex, fill=node_green] (f) at (6.6,-1) {$t_2$};
            
            %edges
            \draw (b) to (c);
            \draw (c) to (d);
            \draw (c) to (g);
            \draw (g) to (f);
            
            \draw[edge_f] (b) to (c);
            \draw[edge_f] (c) to (d);
        
            \draw[edge, c_red] (c) to (b);
            \draw[edge, c_red] (d) to (c);
            \draw[edge, c_green, transform canvas={yshift=-8pt}] (f) to (g);
            \draw[edge, c_green, transform canvas={yshift=-8pt}] (g) to (c);
        
            
            \end{tikzpicture}
            \captionsetup{labelformat=empty}
            \captionof*{figure}{(b) Propagation Prefix 2}
            \hspace{60mm}
     \end{minipage}
     }

     \resizebox{0.45\textwidth}{!}{%
\begin{minipage}[t]{0.34\textwidth}
        \begin{tikzpicture}
            \definecolor{c_red}{RGB}{220, 68, 51}
            \definecolor{c_green}{RGB}{11, 218, 81}
            \definecolor{node_red}{RGB}{255, 200, 200}
            \definecolor{node_green}{RGB}{200, 255, 200}
    
    
    
            \tikzset{vertex/.style = {shape=circle,draw,minimum size=2.2em}}
            \tikzset{edge/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, green, transform canvas={yshift=4pt}, > = latex'}}
                    \tikzset{edge_f/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, transform canvas={yshift=-4pt}, > = latex'}}
        
            % vertices
            \node[vertex, fill=node_red] (b) at  (1.7,0) {1};
            \node[vertex, fill=node_green] (c) at  (3.4,0) {2};
            \node[vertex, fill=node_green] (d) at  (4.9,1) {$t_1$};
            \node[vertex, fill=node_red] (g) at  (4.9,-1) {3};
            \node[vertex, fill=node_red] (f) at (6.6,-1) {$t_2$};
            
            %edges
            \draw (b) to (c);
            \draw (c) to (d);
            \draw (c) to (g);
            \draw (g) to (f);
            
            \draw[edge_f] (b) to (c);
            \draw[edge_f] (c) to (d);
        
            \draw[edge, c_green] (c) to (b);
            \draw[edge, c_green] (d) to (c);
            \draw[edge, c_red, transform canvas={yshift=-8pt}] (f) to (g);
            \draw[edge, c_red, transform canvas={yshift=-8pt}] (g) to (c);
        
            
            \end{tikzpicture}
            \captionsetup{labelformat=empty}
            \captionof*{figure}{(c) Propagation Prefix 1}
     \end{minipage}\vspace{50mm}
     \begin{minipage}[t]{0.3\textwidth}
        \begin{tikzpicture}
            \definecolor{c_red}{RGB}{220, 68, 51}
            \definecolor{c_green}{RGB}{11, 218, 81}
            \definecolor{node_red}{RGB}{255, 200, 200}
            \definecolor{node_green}{RGB}{200, 255, 200}
    
    
    
            \tikzset{vertex/.style = {shape=circle,draw,minimum size=2.2em}}
            \tikzset{edge/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, green, transform canvas={yshift=4pt}, > = latex'}}
                    \tikzset{edge_f/.style = {->, very thick, shorten >=0.1cm,shorten <=0.1cm, transform canvas={yshift=-4pt}, > = latex'}}
        
            % vertices
            \node[vertex, fill=node_red] (b) at  (1.7,0) {1};
            \node[vertex, fill=node_green] (c) at  (3.4,0) {2};
            \node[vertex, fill=node_red] (d) at  (4.9,1) {$t_1$};
            \node[vertex, fill=node_red] (g) at  (4.9,-1) {3};
            \node[vertex, fill=node_green] (f) at (6.6,-1) {$t_2$};
            
            %edges
            \draw (b) to (c);
            \draw (c) to (d);
            \draw (c) to (g);
            \draw (g) to (f);
            
            \draw[edge_f] (b) to (c);
            \draw[edge_f, transform canvas={yshift=8pt}] (c) to (g);
            \draw[edge_f, transform canvas={yshift=8pt}] (g) to (f);
        
            \draw[edge, c_green] (c) to (b);
            \draw[edge, c_red] (d) to (c);
            \draw[edge, c_green, transform canvas={yshift=-8pt}] (f) to (g);
            \draw[edge, c_green, transform canvas={yshift=-8pt}] (g) to (c);
        
            
            \end{tikzpicture}
            \captionsetup{labelformat=empty}
            \captionof*{figure}{(d) Propagation Prefix 2}
     \end{minipage}
     }
     \vspace{6pt}
     \caption{ Propagation of BGP routes. Green marks valid and enforcing components, red marks invalid and non enforcing components, black arrows indicate data-plane paths of traffic.}
     \label{fig:update_prop}
\end{figure}     

{\bf Hlavacek et al \cite{hlavacek2018practical}}.
The approach of Hlavacek et al. first introduced the concept of measuring ROV enforcement on the data-plane additionally to control-plane measurements. For the data-plane measurements, RIPE Atlas is used, a collection of small devices distributed in different ASes of the Internet. Researchers can obtain access to those devices to run traceroute measurements from many observation points to a predetermined target. The traceroutes allow the reconstruction of AS paths that BGP routes from a specific AS take through the Internet. The process is illustrated in Figure \ref{fig:setup}.
Two different origin ASes, AS 1 and AS 2, both announce the same two prefixes p1 and p2. Additionally, ROA objects are created that make the announcement of one prefix valid from the first AS, and the announcement of the second prefix valid for the second AS resulting in two valid announcements (p1-AS1 and p2-AS2) and two attempted hijacks (p1-AS2 and p2-AS1). The routing paths of valid and invalid announcements are then compared. If any traceroute to a prefix is routed to a ROA-invalid AS, i.e., falls victim to the hijack, the path to that AS is considered invalid and all ASes on the path are marked as not enforcing ROV.

We visualize the core idea of how ROV enforcement is measured with the setup of Hlavacek et al. in Figure \ref{fig:update_prop}. Subfigures (a) and (b) show the propagation of the updates announced by the two target ASes $t_1$ and $t_2$ in a scenario where no on-path AS enforces ROV. In case (a) the announcement of $t_1$ is valid, while in case (b) the announcement of $t_2$ is valid. The valid origin AS of an announcement and ROV-enforcing ASes are marked in green, while red indicates invalid announcements, propagation, and non-enforcement. 

In (a) no AS enforces ROV and thus other routing mechanisms influence the propagation of updates. In this case, AS 2 prefers the valid announcement over the invalid announcement because it has a shorter AS path to the target (1 hop to $t_1$ vs. 2 hops to $t_2$), by chance following ROA validity. For the second prefix in (b) preferring the shorter AS path conflicts with the ROA; ASes 1 and 2 thus fall victim to the prefix hijack of $t_1$. ASes in this scenario are correctly classified as not enforcing ROV.

Figures (c) and (d) illustrate a scenario where one on-path AS enforces ROV (AS 2). In this scenario both prefixes (c) and (d) are routed to the correct target; the prefix hijack is unsuccessful. AS 2 discards the hijack of $t_2$ in (c) and $t_1$ in (d). In this configuration the classification scheme of \cite{hlavacek2018practical} would classify ASes 1, 2, and 3 as ROV-enforcing, as they all do not fall victim to the hijack.

This example illustrates a shortcoming in the methodology; the classification is susceptible to false positives. In this example, AS 1 and AS 3 are wrongfully classified as ROV-enforcing. While the false positives might be reduced by using multiple origins, a lack of identification which on-path AS enforces ROV still leads to faulty classifications. Further, the methodology does not distinguish between ASes that use ROV in a non-strict mode, and ASes that do not enforce ROV.

{\bf Rodday et al \cite{rodday2021revisiting}.}
Improving on previous work, Rodday et al. develop a methodology that emphasizes mitigating false positives in their results. They use a single ASN to announce updates to the Internet and, similarly to \cite{hlavacek2018practical}, probe the paths that updates take over a large number of distributed RIPE Atlas probes. However, the methodology does not simply look at the number of valid paths over different ASes. Instead, they apply a strict classification scheme that limits false positives. They distinguish between ASes one hop away from their target and ASes 2+ hops away. ASes in a distance of one hop do not, by definition, have any AS between them and are thus not susceptible to false positives induced by other on-path ASes enforcing ROV. In the 2+ hop case, intermediate ASes may enforce ROV. The methodology thus proposes strict rules that prevent a false positive from ROV-enforcing ASes on the path. The introduced rules require that every on-path AS hosts a measurement probe to conclude enforcement, and that no other AS on the path enforces ROV. Mandating a probe in every on-path AS allows the classification to pinpoint which AS enforced ROV and which AS is only passively protected.


While this methodology likely achieves the goal of reducing false positives, it trades the reduction in false positive with an increase in false negatives. Consider again the examples in Figure \ref{fig:update_prop}. In the example (a,b) in Figure \ref{fig:update_prop}, the methodology of \cite{rodday2021revisiting} would correctly assert that ASes 1, 2 and 3 do not enforce ROV, as no strict rules are applied for non-enforcement. However, the second scenario (c,d) in Figure \ref{fig:update_prop} illustrates the limitation of the methodology regarding false negatives. The methodology requires all on-path ASes to host a probe to conclude ROV enforcement. Consider that in the scenario (c,d) in Figure \ref{fig:update_prop}, AS 1 does not host an Atlas probe. In this case, the paths observed in (c) and (d) need to be discarded, not counting the ROV enforcement in AS 2. On the other hand, the paths of (a) and (b) would be counted towards non-enforcement. Due to the unbalanced burden of proof between enforcement and non-enforcement, the methodology favors counting paths that only contain non-enforcing ASes. In contrast, paths with ROV-enforcing ASes often need to be discarded. In the presented scenario, the methodology would conclude 0\% enforcement despite actual enforcement of 33\%.

This tendency of many false negatives is also indicated in the results presented in \cite{rodday2021revisiting}; in the set of ASes that are one hop away from the observation point, 82\% of ASes exhibit some signs of direct or indirect ROV enforcement. This rate drops to 1.6\% for ASes 2+ hops away, indicating that the methodology favors the classification of non-enforcing ASes over enforcing ASes in the case of 2+ hops of distance.

Our methodology uses the insights gained by \cite{hlavacek2018practical} and \cite{rodday2021revisiting}. We use the approach of \cite{hlavacek2018practical}, i.e., announcing two prefixes from two ASes, as the basis for our measurement. However, we extend the methodology differently than \cite{rodday2021revisiting} to reduce false positives. Instead of strict rules, we pinpoint which AS actually enforces ROV and which AS is only passively protected with a new metric that we developed, referred to as divergence points.

A divergence point refers to the AS where the path to the two prefixes diverges, following the ROA validity of the prefix announcements. Again consider the example in Figure \ref{fig:update_prop}. In this case, the path to prefix 1 (c) and 2 (d) is identical in the first hop AS 1. The paths diverge after the second hop, and AS 2 would thus be considered the divergence point of paths. AS 2 is the most likely point of ROV enforcement. Our methodology classifies ASes repeatedly appearing as divergence points in different configurations as likely ROV-enforcing. In contrast, ASes that only appear on valid paths but lack identification as divergence points are considered either upstream protected or lacking sufficient evidence for either enforcement or non-enforcement. In the presented example, AS 1 would be classified as upstream protected, AS 2 as enforcing ROV, and AS 3 as lacking sufficient evidence for either classification.

\subsection{Data Acquisition}
{\bf Control-plane.} On the control plane, we use valid and invalid (hijacked) BGP announcement propagation as a metric to identify ASes with ROV enforcement. The hijack is executed from two ASes with two neighboring prefixes, as shown in Figure \ref{fig:setup}. Each AS announces the same two prefixes p1 and p2. We also create two ROAs that authorize prefix p1 from AS 1 and p2 from AS 2. Therefore, when AS 1 announces p2 (resp. AS 2 announces p1), it effectively appears as an attempted hijack of p2 by AS 1 (resp. p1 by AS 2). ASes that do not react to the hijack and route traffic for p1 to AS 2 or traffic for p2 to AS 1, despite the conflicting ROA, are classified by us as non-ROV-enforcing ASes. The robustness of the measurement is enhanced by changing the configuration between the experiments, i.e., authorizing p1 for AS 2 and p2 for AS 1 with corresponding ROAs, reducing the noise from routing events unrelated to ROV. We apply the classification scheme of \cite{hlavacek2018practical} to our results.

{\bf Data-plane.} To find which ASes route traffic for p1 to AS 2 and for p2 to AS 1, we send out traceroute probes to our target ASes. We process the resulting paths to analyze which ASes enforce ROV and which fall victim to the hijacks. 

\subsection{Data Analysis}\label{sc:classification}
{\bf Divergence points.} Our methodology provides new key aspects which result in a more accurate approximation of real-world ROV enforcement on the Internet. Our classification of the data-plane measurements incorporates information about path divergence points, which was not considered in previous studies. 
Divergence points indicate that an AS reached a different conclusion for route propagation between the two prefixes, which provides strong evidence on ROV enforcement. This additional metric improves the accuracy of the classification as it approximates the location of the ROV-enforcing AS. To remove false positives caused by ROV in an invisible IXP in front of an AS, we additionally map collected IP addresses to the routing LANs of IXPs.

{\bf AS classification scheme}. We use information about divergence points and path structure to derive a more fine-grained classification scheme that includes conclusions about invalid-route depreferencing. ASes that show evidence of divergence points but are also traversed by invalid paths likely apply ROV, but the implementation is either non-strict (AS drops invalid announcements in certain scenarios), or the decision process depreferences invalid routes but propagates them if no other routes are available. 
Further, we use the relative position of ASes on paths to conclude about the passive upstream protection of ASes without their own enforcement. The categories are defined next.

$\bullet$ \textit{Non-enforcing C1:}
These are all ASes on invalid paths, i.e. ASes that fall victim to hijacks, which indicates that they are not enforcing ROV. The ASes in this category do not have any hints for partial ROV deployment or invalid-route depreference.
    
$\bullet$ \textit{Weak depreference C2:} ASes that show some sign of ROV enforcement but were on at least one invalid path. ROV enforcement is indicated by twice as many valid than invalid paths and at least one divergence point.
    
$\bullet$ \textit{Strong depreference C3:} ASes that are most likely enforcing ROV since they have at least three times more valid than invalid paths and are a divergence point at least once in each configuration, but their enforcement is non-strict.

$\bullet$ \textit{No negative evidence C4:} All ASes that do not conflict with the ROA, but also do not show any positive evidence for ROV enforcement. It is thus unclear if they enforce ROV or are protected by ROV in other ASes.    

$\bullet$ \textit{Passive positive evidence C5:} Similar to category 4 but with an additional requirement that the paths over an AS indicate that all upstreams enforce ROV. The protection is present in all ASes that appear behind enforcing ASes.  

$\bullet$ \textit{Direct positive evidence C6:} These ASes show signs of ROV enforcement, but the evidence is not comprehensive. The AS has been on at least one path in each configuration and a divergence point at least once.   

$\bullet$ \textit{Strong positive evidence C7:} ASes with strong evidence that they enforce ROV. The AS was on a path to each prefix in each configuration and on a divergence point at least once in each configuration.

\subsection{Correlation Control- and Data-Plane}
Our methodology uses additional control-plane measurements to allow for validation of data-plane results. For this validation, we look at the overlap between the two measurements, first in the location of the vantage points and then in the overlap in classification. We discuss the overlap of measurement locations in Section \ref{subsec:cd}. We expect that both approaches should lead to a similar result on the enforcement status for each AS in the intersection set. This
hypothesis is validated using a similarity measure that correlates
control-plane categories and data-plane categories according
to their logical similarity. The mapping uses the control-plane
category as the first integer in the tuple and the data-plane
category as the second integer. This allows to calculate similarity for each AS in the intersection. For example, an AS that
is categorized into category 1 in the control-plane and category 1 in the data-plane, which results in the correlation tuple (1,1)
and that AS is thus rated as having a high similarity between
measurements. In contrast, an AS that is rated as ROV-enforcing in the control-plane as category 3 and non-enforcing in
the data-plane as category 1 results in a tuple (3,1) which is
mapped to a low similarity between results for this AS.

{\scriptsize
\begin{verbatim}
// High similarity
H = {(1,1),(1,2),(2,3),(2,4),(2,5),(3,6),(3,7),(4,5),(4,6),(4,7)}
// Medium similarity
M = {(1,3),(2,6),(2,7),(3,3),(3,4),(3,5),(4,3),(4,4)}
// Low similarity
L = {(1,4),(1,5),(1,6),(1,7),(2,1),(2,2),(3,1),(3,2),(4,1),(4,2)}
\end{verbatim}
}

High similarity refers to ASes that are mapped identical
or almost identical in both approaches, e.g., both ASes are classified as strictly ROV-enforcing. A medium similarity
between ASes does not require identical results, but the classification must still be coherent, i.e., both classification can
result from the same behavior. For example, consider an AS that allows invalid routes in certain scenarios, such as in cases when the announcement comes from a child. Then the AS would be classified as non-enforcing in the control-plane, as
it allowed an invalid route to pass. On the other hand, the data-plane has the additional measure of divergence points, it can
thus identify that the AS generally enforces ROV. Therefore the AS will be classified as strongly depreferencing invalid
routes (category-3). The classification of control-plane and data-plane is not identical and would thus refer to this result as
medium-similarity. The classification result is still coherent.
