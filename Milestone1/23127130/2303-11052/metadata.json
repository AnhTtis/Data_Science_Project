{
    "arxiv_id": "2303.11052",
    "paper_title": "ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real Novel View Synthesis via Contrastive Learning",
    "authors": [
        "Hao Yang",
        "Lanqing Hong",
        "Aoxue Li",
        "Tianyang Hu",
        "Zhenguo Li",
        "Gim Hee Lee",
        "Liwei Wang"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Although many recent works have investigated generalizable NeRF-based novel view synthesis for unseen scenes, they seldom consider the synthetic-to-real generalization, which is desired in many practical applications. In this work, we first investigate the effects of synthetic data in synthetic-to-real novel view synthesis and surprisingly observe that models trained with synthetic data tend to produce sharper but less accurate volume densities. For pixels where the volume densities are correct, fine-grained details will be obtained. Otherwise, severe artifacts will be produced. To maintain the advantages of using synthetic data while avoiding its negative effects, we propose to introduce geometry-aware contrastive learning to learn multi-view consistent features with geometric constraints. Meanwhile, we adopt cross-view attention to further enhance the geometry perception of features by querying features across input views. Experiments demonstrate that under the synthetic-to-real setting, our method can render images with higher quality and better fine-grained details, outperforming existing generalizable novel view synthesis methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our method also achieves state-of-the-art results.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11052v1",
        "http://arxiv.org/pdf/2303.11052v2"
    ],
    "publication_venue": null
}