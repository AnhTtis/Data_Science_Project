\section{Related Works}
Our proposed region conditioned adaptation is relevant foundation models, abductive reasoning, parameter-efficient fine-tuning, and fine-grained visual representation learning. 
We will  discuss related works according to the areas below.

\input{fig/four_compares}

\textbf{Foundation Models.}
%`\textit{More is different}'' \cite{anderson1972more}. 
Scaling up models' complexities and training data improves the attention-based \cite{galassi2020attention, otter2020survey,liu2023survey,hao2022attention} foundation models' \cite{brown2020language, devlin2018bert, radford2021learning, jia2021scaling, yu2022coca, yuan2021florence,cheng2024emotion} perception capacity, making it proficient in many tasks including zero or few-shot learning. 
Specifically, Large Language Models (LLM), such as BERT \cite{devlin2018bert}, and GPT \cite{brown2020language} are trained on large-scale datasets and they generalize to many downstream NLP tasks.
Following this trend, several vision-language foundational models are also developed e.g. CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling} and BLIP \cite{li2022blip}.
The main idea behind the vision foundation models is to learn transferable visual representation with noisy text supervision through a two-tower structure. 
We follow the current baseline of visual abductive reasoning and adopt the CLIP model as the backbone for visual inference. 

%Recent multimodal generative models~\cite{} are a product of powerful vision models and a large language model. 
%They are fine-tuned on instructional data, showing strong perception and some level of causal reasoning abilities. However, visual abductive reasoning requires the model to make plausible human-like inferences, which they are not trained to do.


\textbf{Abductive Reasoning Tasks.} Humans make plausible inferences or hypotheses from incomplete observations every day \cite{cohen1933collected}. 
To teach AI models to attain the same capability, researchers proposed several new tasks, like \dataset{}~\cite{bhagavatula2020abductive} for NLP, \textit{Sherlock} \cite{hessel2022abduction} for vision, and \textit{VideoVAR} \cite{liang2022visual}, \textit{VideoABC} \cite{zhao2022videoabc} for video. 
Specifically, the \dataset{}~\cite{bhagavatula2020abductive} generates the most likely hypothesis (text) to explain what has happened between the two observations (texts). 
For Sherlock, VideoVAR, and VideoABC, the observations are represented by regional or whole images, while inference is text or middle frames. 
There are similar tasks, like Visual Commonsense Reasoning (VCR) \cite{zellers2019vcr} and Visual7W \cite{zhu2016visual7w}. 
Abductive reasoning differs from them in having non-definitely correct, plausible inferences as humans do.


\textbf{Parameter-Efficient Fine-Tuning} (PEFT). Transferring foundational models to downstream tasks promotes the development of PEFTs \cite{sabry2023peft}. 
Representative PEFTs include Prompt, Adapter, and LoRA tuning. 
Specifically, prompt tuning \cite{brown2020language, lester2021power, zhong2021factual, tu2022prompt, liu2022p,ma2024fedhpl,Wang2024RevisitingTP} enhances the distinctiveness of inputs by prepending additional tokens, which may be either trainable or fixed. 
The vision-language prompt tuning can further be divided into textual \cite{zhou2022coop,zhou2022cocoop,feng2022promptdet,du2022learning,fang2024pros} or visual prompt \cite{jia2022visual,bahng2022exploring,oh2024robust} tuning, depending on the placement of prompt tokens in visual or textual encoders. 
Certain special visual prompts, such as the Merlot \cite{zellersluhessel2021merlot}, CPT \cite{yao2021cpt}, and CiP \cite{shtedritski2023does}, guide the model to focus on specified areas by overlaying these regions with translucent colours or red circles. 
In adapter tuning, trainable Multi-Layer Perceptron (mini MLP) \cite{yang2023aim, sung2022vl,houlsby2019parameter} or Tiny Attention modules \cite{zhao2022tiny} are usually inserted into the foundational model, with only the new additions being fine-tuned. 
LoRA \cite{hu2021lora} update parameters using low-rank projections. 
Our RCA is a hybrid ``\textit{prompt}+\textit{adapter}'' tuning to equip the vision foundational models with local reasoning ability, an approach that has not been studied before.


\textbf{Fine-Grained Visual Representation.} Our work is also relevant to learning fine-grained visual representation \cite{zhong2022regionclip, zhang2018fine, zhang2022glipv2, wang2022internimage, aberdam2023clipter,shao2022region} and object detection \cite{he2017mask,zhao2019object, jiao2021new,huang2022sfa,bi2022iemask}. 
Specifically, GLIP \cite{zhang2022glipv2} and RegionCLIP \cite{zhong2022regionclip} pre-train foundation models for object detection, supervised by region-text pairs. 
The former and latter mimic the process of R-CNN\cite{girshick2014rich} and Faster-RCNN \cite{ren2015faster}, generating an object's vector by either encoding the cropped image or RoI pooling. 
Similarly, UNITER \cite{chen2020uniter} and LXMERT \cite{tan2019lxmert} also rely on RoI pooling to generate regional vectors for vanilla vision-language tasks. 
Besides, the InternImage \cite{wang2022internimage} learns the foundation model with Deformable-CNN for object detection. 
Other works, such as Object-VLAD \cite{zhang2018fine} for event detection and CLIPTER \cite{aberdam2023clipter} for scene text recognition, also studied fine-grained modeling. 
Specifically, the Object-VLAD aggregates densely collected local features with VLAD to generate video representation. 
The CLIPTER introduces extra cross-attention and the gated fusion module to combine local and global features. 
In contrast, our RCA only adjusts the frozen CLIP with an add-on Adapter to tackle new inputs.

