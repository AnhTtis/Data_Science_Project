\section{Introduction}

Visual reasoning refers to the ability to understand, interpret, and rationalize predictions derived from visual inputs.
This ability is essential for creating AI systems capable of interacting with the environment~\cite{liang2022visual,zhao2022videoabc,cao2021knowledge,malkinski2022multi, guo2024benchmarking,hao2022group}.
In this regard, abductive reasoning~\cite{josephson1996abductive,bhagavatula2020abductive,hessel2022abduction} has been a topic of interest in AI for a long time due to its various applications in detecting faults in systems, automated medical diagnosis, and legal reasoning. 

Recently, a novel multimodal visual reasoning problem known as Visual Abductive Reasoning (VAR) has been introduced~\cite{hessel2022abduction}, highlighting the significance of integrating both visual and textual modalities to infer logical conclusions from observed image data. 
Visual abductive reasoning refers to making inferences based on visual information (usually an incomplete set of observations) to arrive at the most plausible (often simplest) explanation or hypothesis for a given observation. 
For example, in VAR, as shown in \ref{Fig:task_define}, the model is expected to make the inference ``\textit{the woman recently ordered the drink}'' from the given regional visual hints, which show only the ``glass bottle'' and surrounding contexts ( ``restaurant scene'' and ``waitress'').
Visual abductive reasoning is challenging because it requires a deep understanding of the observed image and the domain (or the context) of the scene depicted in the image.
Furthermore, VAR demands the ability to generate hypotheses consistent with the observed visual data and the domain rules. 
It involves not only recognizing patterns in the images but also applying domain knowledge, learning to reason about unseen/unknown aspects of the context, and learning the causal relationship between inferences and observations.

Current vision and multimodal foundational models have superior capability in visual understanding and language-based reasoning.
However, they are not explicitly modeled to tackle visual abductive reasoning.  
Interestingly, most of the existing vision-language models are trained in a data-driven manner using image-text contrastive learning~\cite{radford2021learning,li2023blip}, image-to-text matching~\cite{li2023blip} and image-based language generation~\cite{li2023blip,liu2024visual}. 
However, in visual abduction, there is only a causal association between visual observations and inferences and current models are not trained to tackle this aspect.
%For example, the visual observation of ``\textit{wet road}'' may likely be caused by ``\textit{rain}'' or ``\textit{a sprinkler cleaning the road}''.
%If a model learns the rules of the domain such that if it rains, roads get wet and if a sprinkler cleans the road, also the road can get wet, then the challenge is to use backward reasoning to infer that the road is wet because of the above-mentioned possibilities.

Authors in~\cite{hessel2022abduction} adapted vision foundation models such as CLIP~\cite{radford2021learning} with visual prompts and vision-to-inference based contrastive learning for visual abductive reasoning.
Their idea is that if one fine-tunes the CLIP model with vision-to-inference contrastive learning, then the model can learn the domain knowledge explicitly as well as the backward reasoning, i.e., the inference can be made using the observations.
However, fine-tuning the entire foundational model is not ideal as that may change the learned representations of the foundational model. 
Furthermore, direct optimisation of the contrastive loss either using vision-to-inference or vision-to-text-evidence may not allow the model to learn the association between inferences and the observations more effectively.

We also leverage the CLIP model for the VAR task as shown in \Cref{fig:fine_grained}. 
However, we resort to parameter-efficient tuning of the CLIP model as a solution.
Specifically, we train a few newly added adaptor parameters of vision and text Transformers of the CLIP model using both vision-to-evidence \textbf{\emph{and}} vision-to-inference contrastive losses \textbf{\emph{jointly}}.  
Our novel adaptor learns new attention maps using low-rank projection matrices, allowing us to learn the semantic associations between the hypothesis and the observations without destroying the semantic knowledge encapsulated in CLIP's vision and text Transformer modules.
The optimization of both losses allows us to learn the cause-and-effect relation between the hypothesis (i.e., inference) and observations (i.e., visual and textual evidence).
While vision-to-evidence contrastive loss helps to reduce the semantic gap between vision and text modalities using few adaptor parameters, the joint optimization of vision-to-inference and vision-to-evidence contrastive losses helps to learn the causal association between hypothesis and observations (i.e., observations are a result of hypothesis). Furthermore, using newly designed regional prompts, our model attends to the relevant visual cues for hypothesis generation. 
It helps the CLIP vision Transformer to attend to subtle visual cues without modifying the CLIP vision model and the parameters (--see \Cref{fig:fine_grained}).
These regional prompt tokens are further appended to image context tokens to capture context information.
This allows the model to learn context-based domain-level rules and knowledge. For example, during learning our model may learn rules such as ``\textit{if it rains, the road can get wet}'' or ``\textit{in restaurants, there are people, and they order drinks}''.
This provides the foundational model with relevant visual hints to align textual evidence with vision, associate the hypothesis with multimodal evidence during learning, and learn domain/context-specific knowledge.


Experiments on the Sherlock VAR benchmark show that our model surpasses previous state-of-the-art results, ranking the \nth{1} on the leaderboard\footnote{\url{https://leaderboard.allenai.org/sherlock/submissions/public}}. Our contributions are summarised below.

\noindent
\textbf{Region Conditioned Adaptation} (RCA). Our RCA is the first hybrid Parameter-Efficient Fine-Tuning (PEFT) method within the ``\textit{prompts} + \textit{adapter}'' paradigm. It guides frozen vision foundation models to make inferences based on visual observations.

\noindent    
\textbf{Fine-Grained Region Prompts}. We have designed a new visual prompt that encodes regional hints at a fine-grained level within the CLIP model. Our tests confirm that emphasizing local evidence improves visual abductive reasoning.

\noindent    
\textbf{Enhanced Adapter$^\textbf{+}$ Tuning}. We present a new Map Adapter that adjusts the attention map using extra query/key projection weights. Our new MAP adapter is orthogonal to the original adapter~\cite{yang2023aim}, and they are jointly used to form the Adapter$^\textbf{+}$.

\noindent    
\textbf{Dual-Contrastive Loss}. We show that joint optimization of vision-to-inference and vision-to-evidence contrastive losses helps to learn the causal association between hypothesis and observations, which aids visual abductive reasoning.
    


