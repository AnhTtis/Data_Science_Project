\section{Experiments}
We comprehensively study the RCA~and Dual-Contrastive Loss on the Sherlock benchmark \cite{hessel2022abduction}. 
We also tested the RCA's adaptability on the RefCOCO \cite{yu2016modeling}, which focuses on grounding expression to regions. 
We present details below.
%The details of our experiments are provided below.


\subsection{Datasets}
The \textbf{Sherlock} dataset \cite{hessel2022abduction} contains 103K images collected from the Visual Genome \cite{krishnavisualgenome} and Visual Common Sense Reasoning \cite{zellers2019vcr} datasets. 
These images are split into 90K training, 6.6K validation, and 6.6K testing sets. 
Each image is re-annotated with an average of 3.5 observation-inference pairs, forming 363K samples. 
Particularly, a sample includes a bounding box $\boldsymbol{r}$ and two texts (i.e., clue $\boldsymbol{c}$ + inference $\boldsymbol{h}$). 
Notably, the validation set can be evaluated offline with officially provided scripts, while the testing set needs to be submitted to the evaluation server of leaderboard. 

Three types of evaluation metrics, from \textit{retrieval}, \textit{localization}, and \textit{comparision} aspects, are adopted for this benchmark. 
Specifically, retrieval metrics include $img\leftrightarrows text$ mean rank, P@1$_{i\rightarrow t}$. 
For localization, accuracies of grounding candidate regions to the inferences are adopted. 
Comparison metric calculates the accordance between machine and human predictions.
%Specifically, retrieval metrics, such as $img\leftrightarrows text$ mean rank, P@1$_{i\rightarrow t}$, are used to measure correspondence between visual fact and inference text. 
%For localization, accuracies of grounding candidate regions to the inferences are adopted. 
%The candidates are generated from ground-truth or auto-detected boxes. 
%Comparison metric calculates the accordance between machine and human predictions.

The \textbf{RefCOCO} dataset \cite{yu2016modeling} origins from the MSCOCO dataset \cite{lin2014microsoft}. 
We test the generalization of the RCA~on the Referring Expression Comprehension (REC task) using Accuracy@0.5. 
This task aims to link a distinctive sentence to a specific object box when multiple similar objects are present. 
%A box aligned by a grounding sentence is considered correct if it has an Intersection over Union (IoU) of 0.5 with the ground-truth bounding box. 
This dataset contains 3 splits: RefCOCO, RefCOCO+, and RefCOCOg. 
The RefCOCO and RefCOCOg allow for relational expressions of position (left/right), while RefCOCO+ has only expression on appearance. 
Specifically, RefCOCO/+/g contains 19.9/19.9/26.7K images, respectively, covering 50.0/49.8/54.8K object instances with corresponding 142/141/85K referring expressions. 
Since REC requires bounding box proposals for the ``\textit{text-to-region}'' grounding, we adopt the YoloV8 to generate candidate proposals as inputs for our RCA.


\input{table/sota_table_v2.tex}

\subsection{Implementations}
We implement the RCA and Dual Contrastive Loss on top of the OpenCLIP \cite{radford2021learning, cherti2022reproducible} PyTorch toolkit\footnote{\url{https://github.com/mlfoundations/open_clip}}, and fix the training \& testing recipe to be the same for all ablations unless otherwise stated.

\textbf{Training}. We resize $\boldsymbol{r}$ and $\boldsymbol{i}$ into 224$\times$224 (336 for high resolution) square images and then concatenate them into combo-image $\boldsymbol{I}$ of size 448$\times$224. 
We initialize CLIP from OpenAI pre-trained weight and tuning for 10 epochs with a cosine learning lr schedule. 
%Thanks to mixed-precision and gradient-checkpointing \cite{chen2016training}, 
We train with a global batch size=3200, lr=2e-4 using ViT-B-16 backbone (batch=400, lr=2e-5 for ViT-L-14-336) on 2$\times$80 GB A100 GPUs. 

\textbf{Testing}. We apply the same preprocess for region $\boldsymbol{r}$ and full image $\boldsymbol{i}$ to prepare combo-image $\boldsymbol{I}$ as the training phase. 
Given a set of visuals $\{\boldsymbol{r}$, $\boldsymbol{i}\}\times K$ and inferences $\{\boldsymbol{h}\}\times K$, we first calculate the $K\times K$ matrix of \textit{vision-inference} similarity and report retrieval, localization and comparison metrics based on the matrix.

%\input{table/val_cpt_vs_rpa}
\subsection{Comparision with the State-of-the-Art}
We compare RCA with the SOTAs on the Sherlock test set. 
These results are evaluated and published on the official leaderboards. 
%We didn't test RCA with CLIP ResNet50$\times$64 backbone, as Adapter$^{+}$ is presently designed for Transformers.

As shown in \Cref{tab:sota}, our \textit{\textbf{RCA ranks the \nth{1}}} on the Sherlock Leaderboard regarding most of the evaluation metrics. 
It significantly outperforms SOTA competitors. 
For example, our model achieves a ``Human Acc'' score of \textbf{31.74}, compared to 29.58, 22.90, and 21.10 for CPT-CLIP, UNITER, and LXMERT models. 
We note that models built on the CLIP model, including ours and CPT-CLIP, perform much better than traditional models like UNITER and LXMERT. 
This suggests that large-scale pre-trained knowledge is beneficial for tasks requiring abductive reasoning. 
We further validate that our RCA performs well with fine-grained regional evidence as a prompt for visual reasoning tasks. 
Our model achieves a Human Acc score of 26.39/31.74 ({\color{YellowGreen}$\uparrow$\textbf{3.30}/\textbf{2.16}}), compared to 23.09/29.58 for CPT-CLIP when using different backbones. 
Lastly, our new ``Dual-Contrastive Loss'' feature further enhances the performance of the RCA. 
%This performance improvement is consistent across other test data (val-set in Table \ref{tab:cpt_vs_rpa}) and backbones (i.e., ViT-B-16/ViT-L-14), indicating its robustness. 
In summary, our model with Dual-Contrastive Loss outperforms current state-of-the-art methods.


\input{table/different_adapter_v2}
\input{table/different_prompt_v2.tex}
\subsection{Ablation Study}
\label{sec:ab_study}
This section comprehensively studies various factors that influence the performance of RCA on the validation set. 
%We test different combinations of Adapters and Regional Prompts, various losses, the impact of the dimension $d$ in Adapters, and the effect of using different backbones and resolutions. 
%For these tests, 
We use Mixed Prompts, Dual-Contrastive Loss and ViT-B-16 as default settings, except in the comparison of different prompts and losses. More ablations are in supplementary.% and backbones.

\textbf{Impacts of Integrating Adapters}. We analyze how our model performs when we remove certain components, specifically the vanilla (A\&M) and Map Adapters, one at a time. 
The results in \Cref{tab:three_adapt} show that performance decreases with fewer adapters. 
Specifically, using all three types of adapters produces the best results under most evaluation metrics. ``Adapter (M)'' is the best choice when limited to using just one type of adapter. 
If we can use two types, the best combination is ``Adapter (M) + Map Adapter''. This suggests that the Map Adapter works well with the standard adapte

\textbf{Effects of Fine-Grained Regional Prompts}. We also explore how adding fine-grained regional prompts influences the performance of existing prompting techniques, such as colorful (CPT in \cite{yao2021cpt}) and circle prompts (CiP in \cite{shtedritski2023does}). 
In \Cref{tab:combo_prompts}, the terms ``Region Only'' and ``Context'' refer to feeding either just the regional box part or the entire image into the CLIP vision tower, respectively. 

We observe that adding fine-grained tokens based on regional cues significantly improves the performance of all coarse-grained prompts, including ``Context'', ``CPT'', and ``CiP'' across all metrics. 
This basically verifies that ``global context + local cues'' complement each other well for abductive reasoning. 
Moreover, we test the Mixed Prompt mode described in \S \ref{sec:rpg} and observe a stable performance for most metrics. 

\textbf{Dual-Contrastive Loss} \textit{vs} \textbf{Single/Triple counterparts}. 
We test different types of contrastive losses using our RCA model. 
In \Cref{tab:different_loss}, the Dual-Contrastive loss performs better than the Multi-Task Learning and the other single \& triple counterparts under most metrics. 
In terms of localization, the Dual-Contrastive loss is slightly lower than its MTL counterparts but still shows a very competing performance.
   
\input{table/different_loss_v2}

\input{fig/pair_dist_loss}

We further look into the individual contrastive loss value between modality pairs on the validation set to understand how modalities mutually influence each other. 
Specifically, we first report the loss value between each pair before the training phase (i.e., No Train or Zero-Shot reasoning), then re-calculate them after the model is trained with different losses (Fig \ref{fig:dist_all}).



We observe that the gaps of \textit{vision-clue} and \textit{vision-inference} are positively correlated. 
Specifically, when we minimize one of the gaps in training, the other one will also become smaller (e.g., Fig. \ref{fig:dist_p3_inf_only}-\ref{fig:dist_p2_clue_only}). 
Whereas, the gap of \textit{inference-clue} seems not to correlate to gaps of \textit{vision-clue} and \textit{-inference}, as the former is slightly closer or even larger after minimizing either of the latter gaps (e.g., {\color{red}red}/black value in Fig \ref{fig:dist_p3_inf_only}-\ref{fig:dist_p4_mkl}, and \ref{fig:dist_p6_dual}). 
If we enforce the model to close the \textit{inference-clue} gap during training, the \textit{vision-clue} and \textit{-inference} gap would become larger (Triple \textit{vs} Dual, Fig. \ref{fig:dist_p5_trip} \textit{vs} \ref{fig:dist_p6_dual}). 
The reason is that the clue and inference sentences are not literally equivalent and better to be bridged by an extra rational process.

\textbf{Influence of Bottleneck Dimension $d$ in Adapters.} We study the influence of different bottleneck dimensions in the RCA, ranging in $d=\{\frac{D}{32}, \frac{D}{16}, \frac{D}{8}, \frac{D}{4}, \frac{D}{2}, D\}$. 
Notably, a higher $d$ basically introduced more tuned parameters and larger FLOPs, as shown in Figure \ref{fig:plot_flops_para_bubble}. 
For the retrieval metrics, such as mean $img\leftrightarrows txt$ rank, a lower value indicates better performance, whereas the rest are the opposite. 
We observe from Fig. \ref{fig:plot_flops_para_bubble}-\ref{fig:plot_human_acc} that an optimal choice is $d=\frac{D}{4}$, indicating that adjusts the frozen foundational model with either a very heavy $D$ or lightweight $\frac{D}{32}$ would result in sub-optimal performance. 
Notably, human accordance is influenced by the human's subjective judgement and has a different trend. 
Overall, we fix $d=\frac{D}{4}$ for all following experiments.

\input{fig/dim_d_plot}

%\input{table/different_adapter_rate.tex}

%\textbf{Comparison with tiny attention counterpart} \cite{zhao2022tiny}. As in Table \ref{tab:tinyatt}, we compare the performances between different integrations of ``TinyAtten + Adapter$\_$M / Adapters$\_$(A \& M)'' (Figure \ref{fig:tiny_atten}) against RCA (Figure \ref{fig:rpa_adapter}) on frozen CLIP ViT-B-16. 
%We note that ``TinyAtten + Adapters$\_$(A\&M)'' performs worse than the RCA with more tuned parameters and FLOPs. 
%The reason might be Map Adapter only re-weights the attention map and does not change the ``value'' bases. 
%Overall, the RCA is a more effective and efficient adapter than its counterparts.
%\input{fig/tiny_atten}
%\input{table/tiny_adapter}

\textbf{Influence of Adapting CLIP Vision/Text Tower.} 
The CLIP follows a two-tower design, each tower separately for visual/textual embedding; thereby, we can independently insert adapters into visual and textual towers to assess their contributions. 
We test inserting Adapter$^\textbf{+}$ into the ``\textit{Only Text}'' tower, ``\textit{Only Vision}'' tower, and both towers. 
As shown in Table \ref{tab:towers}, adapting both CLIP vision and text towers performs the best at the cost of the most tuned parameters among the three options. 
Notably, both ``\textit{Only Vision}'' and ``\textit{Only Text}'' has a large margin in performance compared with the ``\textit{Vision + Text}'', indicating the adaptions on two towers are complementary.
\input{table/tower_test_v2}


%\textbf{Influence of Image Resolutions.} We test combo images with different resolutions to study whether the RCA can benefit from more tokens. 
%As in Table \ref{tab:img_resolution}, it is natural to see an increment of computations when resolutions become larger. 
%However, the boost of performance is not linear to the resolutions, reaching a saturate performance at the resolution of 448$\times$224. 
%This might lie in that the CLIP is pre-trained at 224$\times$224 resolution on upstream dataset, thereby, downstream tuning is better to process images (\textit{one 448$\times$224 combo image = two 224$\times$224 images}) at similar settings. Considering the trade-off of FLOPs and performance, we pick the resolution of 448$\times$224 for the CLIP ViT-B-16 backbone.

%\input{table/resolution_v2}

%\input{table/different_backbone}

%\textbf{Effects of Backbones}. We test the RPA~with different backbones, namely CLIP ViT-B16 and CLIP ViT-L14 (336). 
%We observe that a larger backbone contains more encoders, thereby increasing both tuned parameters and FLOPs but resulting in a much better performance (see Table \ref{tab:adapt_ab1}).

\input{fig/vis}

\section{Qualitative Results of RCA}
We present two qualitative examples obtained by the RCA in Figure \ref{fig:vis_example} and more examples in supplementary. 
Specifically, a human expert gives a possible inference from given a regional cue specified by the box, and the machine retrieves the top-5 most likely inferences. 
The {\color{red}} sentence indicates the correct match with the human's performance. 
We observe from Example 1 \& 2 that the machine manages to deduct human-like inference such as ``\textit{He is trying to on board}'' from an observation of ``\textit{a man under an airplane}'' and ``\textit{prevent the sunshine}'' from ``\textit{a wearing hat}''.


%\newpage
%\input{fig/vis_more}


\section{Generalization on RefCOCO}
We also tested the generalization of the RCA on the RefCOCO dataset using a two-stage pipeline in Table \ref{tab:res_rc}. 
Specifically, we employed YoloV8 as the object detector to propose candidate object boxes. 
Then, we utilized the RCA to align textual sentences with the object box with the highest matching score. 
We evaluated the RCA using a single-prompt mode, such as ``R-CTX'', ``R-CPT'', to observe their respective effects.



\input{table/refcoco}

The RCA performs better than two-stage models like MattNet on the RefCOCO+/g sets, emphasizing appearance descriptions (e.g., ``\textit{a person with a yellow tie}''). 
However, it lags behind MattNet on RefCOCO, which focuses on positional descriptions (e.g., ``\textit{left person}''). 
This discrepancy arises because MattNet explicitly encodes \textit{appearance}, \textit{location}, and \textit{relation} information, while RCA only encodes appearance. 
Although RCA is adaptable to the Referring Comprehension task, it falls behind one-stage end-to-end models like MDETR. 
The advantage of MDETR has comes from its design to simultaneously regress box coordinates and establish visual-linguistic alignment, especially for visual grounding. 
In contrast, our RCA have to rely on third-party proposals from YoloV8.
