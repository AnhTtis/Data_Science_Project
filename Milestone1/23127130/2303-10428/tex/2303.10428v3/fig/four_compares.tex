 \begin{figure*}[ht!]
\centering
\subfloat[CPT\label{fig:clip_base}]{
\includegraphics[height=0.37\linewidth]{eps/v3_base.eps}
} 
\subfloat[\rpa\label{fig:rpa}]{
\includegraphics[height=0.37\linewidth]{eps/v3_rpa.eps}
} 
\hspace{-0.35cm}
\subfloat[\rpa~(Adapter$^\textbf{+}$)\label{fig:rpa_adapter}]{
\includegraphics[height=0.37\linewidth]{eps/adapter.eps}
} 
\subfloat[Dual-Contrastive Loss\label{fig:v2_dual_loss_fig}]{
\includegraphics[height=0.37\linewidth]{eps/v2_dc_loss.eps}
} 
%	\begin{subfigure}[t]{0.18\textwidth}
%		\includegraphics[width=\textwidth]{eps/v3_base.eps}
%  \caption{\textbf{Color Prompt} (CPT)}	\label{fig:clip_base}
%	\end{subfigure} 
% 	\begin{subfigure}[t]{0.18\textwidth}
%		\includegraphics[width=\textwidth]{eps/v3_base_extend.eps}
%  \caption{{CPT: ($\times 2$ \texttt{Tokens})}}	\label{fig:clip_base_extend}
%	\end{subfigure} 
%  	\begin{subfigure}[t]{0.18\textwidth}
%		\includegraphics[width=\textwidth]{eps/v3_rgp.eps}
%  \caption{\textbf{Regional Prompt}}	\label{fig:rgp_clip}
%	\end{subfigure} 
%   	\begin{subfigure}[t]{0.18\textwidth}
%		\includegraphics[width=\textwidth]{eps/v3_rgp_s.eps}
%  \caption{\textbf{Simplified RGP$_s$}}	\label{fig:rgp_clip_s}
%	\end{subfigure} 
%    	\begin{subfigure}[t]{0.23\textwidth}
%		\includegraphics[width=\textwidth]{eps/v2_dc_loss.eps}
%  \caption{\textbf{Dual-Contrastive Loss}}	\label{fig:v2_dual_loss_fig}
%	\end{subfigure} 
  	%\begin{subfigure}[t]{0.3\textheight}
 	%\begin{subfigure}[t]{0.48\textwidth}
	%	\includegraphics[width=\textwidth]{iccv2023AuthorKit/img/RGP.png}
	%\caption{\textbf{Regional Prompt Tuning}}
        %\label{fig:rgp_clip}
	%\end{subfigure} 
	\caption{\textbf{Comparing Colorful Prompt Tuning and Region-Prompted Adapter Tuning}. (a) \textit{Colorful Prompt Tuning} (CPT) from \cite{yao2021cpt} uses a semi-transparent {\color{mypink}pink} mask to highlight a designated area, marked as $\boldsymbol{r}$; (b) Our method, termed \textit{Region-Prompted Adapter Tuning} (\rpa), simultaneously generates region prompt and contextual tokens by intaking a combo-image $\boldsymbol{J}$, then tunes the frozen CLIP with Adapter$^\textbf{+}$ on top of reasoning dataset; (c) Adapter$^\textbf{+}$ includes two standard adapters and a novel Map Adapter, which separately adjust token features and the attention map; (d) Dual-Contrastive Loss simultaneously guide the visual content minimize semantic (to clue) and causal (to inference) gaps. (Note: Best viewed in color.)}%\label{fig:clip_base}
\end{figure*}