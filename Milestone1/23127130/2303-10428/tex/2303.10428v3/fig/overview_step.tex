\begin{figure}[ht!]
\centering
\subfloat[Visual Abductive Reasoning Task\label{fig:task_define}]{
\includegraphics[width=0.96\linewidth]{eps/var_task.eps}
}\\
\subfloat[Region-Prompted Adapter (\rpa)\label{fig:fine_grained}]{
\includegraphics[width=0.96\linewidth]{eps/fine_grained_v5.eps}
}
\caption{\textbf{Task of Visual Abductive Reasoning (VAR) and Methodology of Region-Prompted Adapter Tuning}. (a). Given a pre-specified region of a picture (Example 1), human beings could easily reckon inference (e.g., ``\textit{the woman recently ordered the drink}'') from a local observation (e.g., ``\textit{glass bottle}''). To imitate humans, the VAR requires the AI to retrieve the most likely inferences for the same region; (b). The \rpa~first patchify regional observation at a more fine-grained level than its colorful counterpart (CPT), and then adjust the frozen CLIP with plugged Adapter$^\textbf{+}$ for downstream tasks. The new Dual-Contrasive Loss simultaneously shrinks gaps between ``\textit{Vision-Inference/-Clue}'' modalities. (This figure is best viewed in color)}
%\caption{\textbf{Task of Visual Abductive Reasoning (VAR) and Methodology of Region-Prompted Adapter Tuning}. (a). Given a pre-specified region of a picture (Example 1), human beings could easily reckon inference (e.g., ``\textit{the woman recently ordered the drink}'') from a local observation (e.g., ``\textit{glass bottle}''). To imitate humans, the VAR requires the AI to retrieve the most likely inferences for the same region; (b). The \rpa~first patchify regional observation at a more fine-grained level than its colorful counterpart (CPT), and then adjust the frozen CLIP with plugged Adapter$^\textbf{+}$ for downstream tasks. The new Dual-Contrasive Loss simultaneously shrinks gaps between ``\textit{Vision-Inference/-Clue}'' modalities. (This figure is best viewed in color)}
\vspace{-0.5cm}
\end{figure}