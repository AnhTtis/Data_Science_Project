\section{Problem Definition \& Baseline}
%We first revisit the definition and baseline with the \textit{Sherlock} benchmark \cite{hessel2022abduction} for the visual abductive reasoning task.

%\subsection{Problem Definition \& Baseline}
\label{sec:problem}
\textbf{Problem}: Hessel et al.\cite{hessel2022abduction} defines a Visual Abductive Reasoning benchmark named ``\emph{Sherlock}'' that requires a model to predict the hypothesis from observations in  ``\textbf{\textit{Observation}} $\boldsymbol{\rightarrow}$ \textbf{\textit{Hypothesis}}'' form. Specifically, visual observation refers to a pre-specified region $\boldsymbol{r}$ of an image $\boldsymbol{i}$ and is accompanied by a clue sentence $\boldsymbol{c}$. Notably, the clue is a straightforward description of real visual content and is only available during training. On the other hand, the hypothesis is defined by an inference sentence $\boldsymbol{f}$. With this, a VAR model calculates a score $\boldsymbol{s}$, which reflects the probability of deducing inference $\boldsymbol{f}$ from the region $\boldsymbol{r}$. Equation \ref{eq:similarity} shows this scoring function $\mathcal F$ and the parameters $\theta$; we call $\mathcal F$ the VAR model.



\begin{align}
    s&=\mathcal{F}(\boldsymbol{f}, \boldsymbol{i}, \boldsymbol{r}| \theta)\label{eq:similarity}
\end{align}
A good VAR model should generate a larger matching score when an inference $\boldsymbol{f}$ and observation $\{\boldsymbol{i}$, $\boldsymbol{r}\}$) are causally related, and a smaller value for wrong or non-related inferences.

\input{fig/four_compares}

%\bff{Perhaps instead of saying baseline. We can say colorful prompt tuning}
%\bff{Now we briefly describe the prior method colorful prompt tuning and then explains our model in secion XXX..
%Before explaining our model in section 4, we present the prior model called colorful prompt tuning.
%}

\textbf{Baseline}: As presented in \cite{hessel2022abduction}, a CLIP \cite{radford2021learning} with colorful prompt tuning (CPT) \cite{yao2021cpt} is adopted as the baseline for the Sherlock benchmark (Fig. \ref{fig:clip_base}). 
Specifically, the CPT highlights the regional observation $\boldsymbol{r}$ by a translucent {\color{mypink}pink} overlay (or mask). Then the altered image $\boldsymbol{i}$ is split into left \& right parts (i.e., $\boldsymbol{i}_l$ \& $\boldsymbol{i}_r$) for feature extraction (Eq. \ref{eq:ir_enc}), and the global feature $\mathcal{I}_{vis}$ is averaged from both parts (Eq. \ref{eq:i_mean}). 
Meanwhile, the inference text $\boldsymbol{f}$ is encoded into a textual feature vector $\mathcal{I}_{text}$ by a text encoder $\mathcal{F}_{\text{T}}$ (Eq. \ref{eq:text_enc}). 
To reduce overfitting, the inference text $\boldsymbol{f}$ is randomly replaced by the clue $\boldsymbol{c}$ with a probability of 0.5 during training. 
This strategy is named \textit{Multi-Task Learning} as now the model has to predict inference text 50\% of the time and clue text 50\% of the time during training. Finally, a contrastive loss $\mathcal L_{ctr}$ bridges the gap between visual and textual features as shown in Eq. \ref{eq:clip_base_loss}. 
\input{equations/clip_base_eq.tex}
\input{fig/rpg}
