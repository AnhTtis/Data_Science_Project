\section{Introduction}
\input{fig/overview_step.tex}

``\textit{Can machines think?}'' Dr. Alan Turing proposed this question and gave the famous thought test, namely the ``\textbf{Imitation Game}'', to help find an answer  \cite{turing2009computing}. This game defines that we could consider ``a machine thinks'' if it fully imitates humans' feedback and tricks us (humans) into believing its replies. Recent progress of foundation models, including GPT \cite{brown2020language}, and CLIP \cite{radford2021learning}, shows that machines could imitate humans better than before, showing evolving thinking and reasoning abilities.
\IEEEpubidadjcol

To promote machines' reasoning ability, researchers devoted more and more efforts to reckon various modalities \cite{bhagavatula2019abductive,hessel2022abduction,liang2022visual,zhao2022videoabc,cao2021knowledge,malkinski2022multi}. 
%This paper studies one of these emerging tasks named \textbf{Visual Abductive Reasoning} (VAR)\cite{hessel2022abduction}. 
One of the most challenging multimodal reasoning tasks is the \textbf{Visual Abductive Reasoning} (VAR)\cite{hessel2022abduction}.
Specifically, the VAR aims to reveal a hidden inference or hypothesis from 
visual observations based on prior knowledge. As shown in Figure \ref{fig:task_define}, the machine is expected to draw an inference ``\textit{the woman recently ordered the drink}'' from regional visual hints (``\textit{glass bottle}'') and surrounding contexts ( ``\textit{restaurant scene}'' + ``\textit{waitress}''), just as humans do. 

However, the VAR task is more challenging than standard vision-language (VL) tasks (e.g., retrieval and captioning) \cite{pan2022video,peng2022relation,yan2023image,zhang2022adaptive,li2023image,hao2021learning,yu2023cgt,hao2016stochastic,zhu2019r2gan}. The reasons lie in two aspects. Firstly, for the VAR, inferences are usually not readily apparent in images (e.g., ``\textit{recently ordered drink}'' $\neq$ ``\textit{glass bottle}'', ``\textit{restaurant}'' or ``\textit{waitress}''), whereas, for vanilla VL tasks, entities in captions appear in images (e.g., ``{\color{black}\textit{glass bottle}}'' in clue sentence ``\textit{A glass bottle on a table}''). Secondly, inferences have causal relations with the local visual hints. The presence or absence of evidential hints will result in different inferences (it's hard to draw the inference ``\textit{recently ordered drink}'' without seeing a glass bottle plus restaurant decoration). 

The typical VAR pipeline contains two steps to reckon an inference from regional hints: Perception and Reasoning Tuning. The former step relies on pre-trained foundation models (e.g., CLIP) to provide prior knowledge and commonsense, transforming visual observations to semantical representation. The latter turns to specific prompt tuning to re-direct the model to align the transformed representation with the inference. Specifically, Colorful Prompt Tuning (CPT in Fig. \ref{fig:fine_grained} is adopted to preprocess images by covering the evidential visual part with a translucent {\color{mypink}pink} box overlay. The altered images are then fed into the CLIP model to minimize the ``\textit{vision}-\textit{inference}'' gap through full fine-tuning. 


Although effective, the pipeline has three limitations. 
First, CPT does not effectively extract fine details from the regional evidence.
As shown in Figure \ref{fig:fine_grained}, the CPT uniformly patchify regional hints and global contexts at the same granularity. Even though the regional part is smaller than the global context in size, it has a more significant influence on the hypothesis. Thereby, uniformly patchifying on regional hints and global context may lead to losing fine-grained details in the regional evidence. Second,  to steer the VLF for deduction, the existing pipeline fully fine-tunes all parameters using a limited amount of downstream data. However, this strategy can cause overfitting of the large VLF model, which could hamper its generalizability. The last is ``\textit{How to utilize Triangular Relationships between Vision (visual evidence), Inference (hypothesis), and Clue (textual clue) for the VAR}''. We argue that there is a weak causal relationship between ``\textit{vision-inference}'' and a semantical relationship between ``\textit{vision-clue}'' modalities. 
Current methods do not attempt to bridge these relations in a symmetric manner.
We propose explicitly finding a correlation law among the three modalities and utilizing it to address the VAR task effectively.

\input{fig/effects}

To address the three limitations, we propose a new approach dubbed \textbf{Region-Prompted Adapter Tuning (\RPA)} combined with \textbf{Dual-Contrastive Loss} learning, which improves the current VAR pipeline (see Figure \ref{fig:clip_base}). Within the \rpa, there are two main components: fine-grained \textit{Regional Prompt Generator} (RPG) and an enhanced \textit{Adapter$^\textbf{+}$}. Specifically, the RPG alleviates ``loss in details'' by gathering visual prompts from regional hints and global context at two levels of granularity (see Fig \ref{fig:fine_grained}). We pluck more tokens from regional hints using smaller patch sizes than those employed by CPT. We also sample surrounding tokens with the normal patch size to retain essential in-context information for inference reasoning. For implementation, We do not tweak the patch size in the embedding layer so as to reuse all the parameters in the CLIP. Alternatively, we turn to enlarge the area of the regional hint and then patchify it with the original embedding layer. For the second concern, we replace the full fine-tuning with Adapter$^\textbf{+}$ tuning. The adapter tuning is a parameter-efficient fine-tuning (PEFT) initially designed for language models (LM) but recently expanded to VLFs. Our Adapter$^\textbf{+}$ improves upon the basic adapter by introducing a new \textit{Map Adapter}, which includes extra attention maps re-weighting using a low-rank query/key projection. For the last concern, we experimentally study different combinations of three contrastive losses (see Fig \ref{fig:loss_all}) and validate that the Dual-Contrasitive Loss, involving the loss of vanilla visual retrieval, benefits reasoning results. This also indicates that the VAR and visual retrieval tasks are positively correlated.



Experiments on the Sherlock VAR benchmark show that our model consistently surpasses previous SOTAs (Fig. \ref{fig:improve}), ranking the \nth{1} on the leaderboard among all submissions \footnote{\url{https://leaderboard.allenai.org/sherlock/submissions/public}}. Our contributions are summarized below.
\begin{itemize}
    \item \textbf{Region-Prompted Adapter Tuning} (\rpa). We introduce the \rpa, the first hybrid Parameter-Efficient Fine-Tuning (PEFT) method within the ``\textit{prompts} + \textit{adapter}'' paradigm. It guides frozen Visual Language (VL) foundation models to make decisions based on local cues.
    
    \item \textbf{Fine-Grained Region Prompts}. We have designed a new visual prompt that encodes regional hints at a fine-grained level within the CLIP model. Our tests confirm that emphasizing local evidence improves visual reasoning.
    
    \item \textbf{Enhanced Adapter$^\textbf{+}$ Tuning}. We present a new Map Adapter that adjusts the attention map using extra query/key projection. This adapter is orthogonal to the vanilla adapter, and they are jointly used to form the Adapter$^\textbf{+}$.
    
    \item \textbf{Dual-Contrastive Loss}. We link the visual abductive reasoning and standard visual retrieval tasks using Dual-Contrastive Loss. Even though these two tasks have different ``\textit{Vision}-\textit{Text}'' relationships (causal \textit{vs} semantical), we find that they are positively correlated and can be fine-tuned together.
    
    \item \textbf{New State-of-the art for VAR.} We evaluate \rpa~with Dual-Contrastive Loss on the standard \textit{Sherlock} benchmark. Experiments show that our model robustly surpasses previous SOTAs, achieving a new record (e.g., Human Acc: {\textbf{\RPA~31.74} \textit{vs} CPT-CLIP 29.58}).

\end{itemize}

