\section{Experiments}
We comprehensively study the \rpa~and Dual-Contrastive Loss on the Sherlock benchmark \cite{hessel2022abduction}. We also tested the \rpa's adaptability on the RefCOCO \cite{yu2016modeling}, which focuses on grounding expression to regions. The details of our experiments are provided below.
%We comprehensively study the \rpa~and Dual-Contrastive Loss on the Sherlock benchmark \cite{hessel2022abduction} and further verify the generalizabilty of \rpa~on RefCOCO \cite{yu2016modeling}, a region-language grounding task. We present experimental details below. 

\subsection{Datasets}
The \textbf{Sherlock} dataset \cite{hessel2022abduction} contains 103K images collected from the Visual Genome \cite{krishnavisualgenome} and Visual Common Sense Reasoning \cite{zellers2019vcr} datasets. These images are split into 90K training, 6.6K validation, and 6.6K testing sets. Each image is re-annotated with an average of 3.5 observation-inference pairs, forming 363K samples. Particularly, a sample includes a bounding box $\boldsymbol{r}$ and two texts (i.e., clue $\boldsymbol{c}$ + inference $\boldsymbol{f}$). Notably, the validation set can be evaluated offline with officially provided scripts, while the testing set needs to be assessed online with the leaderboard. 

Three types of evaluation metrics, from \textit{retrieval}, \textit{localization}, and \textit{comparision} aspects, are adopted for the Sherlock benchmark. Specifically, retrieval metrics, such as $img\leftrightarrows text$ mean rank, P@1$_{i\rightarrow t}$, are used to measure correspondence between visual fact and inference text. For localization, accuracies of grounding candidate regions to the inferences are adopted. The candidates can be collected from ground-truth or auto-detected boxes. Comparison metric calculates the accordance between machine and human predictions.

The \textbf{RefCOCO} dataset \cite{yu2016modeling} originates from the MSCOCO dataset \cite{lin2014microsoft}. We test the generalization of the \rpa~on the Referring Expression Comprehension (REC task) using Accuracy@0.5. This task aims to link a distinctive sentence to a specific object box when multiple similar objects are present. A box aligned by a grounding sentence is considered correct if it has an Intersection over Union (IoU) of 0.5 with the ground-truth bounding box. This dataset contains 3 splits: RefCOCO, RefCOCO+, and RefCOCOg. The RefCOCO and RefCOCOg allow for relational expressions of position (left/right), while RefCOCO+ has only expression on appearance. Specifically, RefCOCO/+/g contains 19.9/19.9/26.7K images, respectively, covering 50.0/49.8/54.8K object instances with corresponding 142/141/85K referring expressions. Since REC requires bounding box proposals for the ``\textit{text-to-region}'' grounding, we adopt the YoloV8 to generate candidate proposals as inputs for our \rpa.



%The \textbf{RefCOCO} dataset \cite{yu2016modeling} originates from the MSCOCO dataset \cite{lin2014microsoft} and aims to link a distinctive sentence to a specific object instance when multiple similar objects are present. This dataset contains 3 splits: RefCOCO, RefCOCO+, and RefCOCOg. The RefCOCO and RefCOCOg allow for relational expressions of position (left/right), while RefCOCO+ has only expression on appearance. Specifically, RefCOCO/+/g contains 19.9/19.9/26.7K images, respectively, covering 50.0/49.8/54.8K object instances with corresponding 142/141/85K referring expressions. We report the Referring Expression Comprehension task using Accuracy@0.5. Specifically,  a region aligned by a grounding sentence is considered correct if it has an Intersection over Union (IoU) of 0.5 with the ground-truth bounding box.

%1. Image from MSCOCO \cite{lin2014microsoft}
%2. 2/4 objects for the same object category
%3. RefCOCOg, 85,474 referring expressions for 54.822 objects, in 26,711 images.
%4. RefCOCO, free-form expression
%142,209 expressiions for 50,000 objets, in 19,994 images
%5. RefCOCO+, location words are disallowed.
%141,564 expressions for 49.856 objects in 19,992 images


\subsection{Implementations}
We develop the \rpa~and Dual Contrastive Loss on top of the OpenCLIP \cite{radford2021learning, cherti2022reproducible} PyTorch toolkit\footnote{\url{https://github.com/mlfoundations/open_clip}}, and fix the training \& testing recipe to be the same for all ablations 
unless otherwise stated.

\textbf{Training}. We resize $\boldsymbol{r}$ and $\boldsymbol{i}$ into 224$\times$224 (336 for high resolution) square images and further concatenate them into combo-image $\boldsymbol{J}$ of size 448$\times$224. We initialize CLIP from OpenAI pre-trained weight and tuning for 10 epochs with a cosine learning rate schedule. Thanks to mixed-precision and gradient-checkpointing \cite{chen2016training}, we train with a global batch size=3200, lr=2e-4 using CLIP ViT-B-16 backbone (batch=400, lr=2e-5 for ViT-L-14-336) on 2$\times$80 GB A100 GPUs. %We give more details in Appendix.

%2$\times$80 GB A100 GPUs and set the global batch size to be 3200 / 400, with lr=2e-4 or 2e-5 for ViT-B-16 or ViT-L-14-336 CLIP backbones. We give more details in Appendix {\color{red}\S \ref{}}.

%For preparing the regional prompt, we extract and resize region $\boldsymbol{r}$ into 224$\times$224 (336 for high resolution) square image, while we are also resizing the full image $\boldsymbol{i}$ into the same-sized square for preparing contextual tokens. We initialize all models from the CLIP pre-trained weight and fully fine-tune them for 10 epochs with a cosine learning rate schedule. Thanks to mixed-precision and gradient-checkpointing \cite{chen2016training}, we train with 2$\times$80 GB A100 GPUs and set the global batch size to be 3200 / 400, with lr=2e-4 or 2e-5 for ViT-B-16 or ViT-L-14-336 CLIP backbones. We give more details in Appendix {\color{red}\S \ref{}}.

\textbf{Testing}. We apply the same preprocess for region $\boldsymbol{r}$ and full image $\boldsymbol{i}$ to prepare regional prompt and contextual token as the training phase. Given a set of visuals $\{\boldsymbol{r}$, $\boldsymbol{i}\}\times K$ and inferences $\{f\}\times K$, we first calculate the $K\times K$ matrix of \textit{vision-inference} similarity and report retrieval, localization and comparison metrics based on the matrix.

\input{table/sota_table_v2.tex}
\input{table/val_cpt_vs_rpa}
\subsection{Comparision with the State-of-the-Art}
We compare \rpa~with the current state-of-the-art on the Sherlock test set. All results are submitted, evaluated, and published on the official leaderboards. We didn't test \rpa~with CLIP ResNet50$\times$64 backbone, as Adapter$^{+}$ is presently designed for Transformers.

As shown in Table \ref{tab:sota}, our top-performing \textit{\textbf{\RPA~ranks the \nth{1}}} on the Sherlock Leaderboard across most of the evaluation metrics. It significantly outperforms competitors like CPT-CLIP, UNITER, and LXMERT. For example, our model achieves a ``Human Acc'' score of 31.74, compared to 29.58, 22.90, and 21.10 for the other models. We note that models built on the CLIP framework, including ours, perform much better than traditional models like UNITER and LXMERT. This suggests that large-scale pre-trained knowledge is beneficial for tasks requiring abductive reasoning. We further validated that our \rpa~performs well with fine-grained regional evidence as a prompt for visual reasoning tasks. Our model achieves a Human Acc score of 26.39/31.74 ({\color{YellowGreen}$\uparrow$3.30/2.16}), compared to 23.09/29.58 for CPT-CLIP when using different backbones. Lastly, our new ``Dual-Contrastive Loss'' feature further enhances the performance of the \rpa. This performance improvement is consistent across other test data (val-set in Table \ref{tab:cpt_vs_rpa}) and backbones (i.e., ViT-B-16/ViT-L-14), indicating its robustness. In summary, our model with Dual-Contrastive Loss outperforms current state-of-the-art methods.



%As shown in Table \ref{tab:sota}, our best-performed \RPA~\textbf{\textit{ranks the \nth{1}}} on the Sherlock Leaderboard under most evaluation metrics, surpassing CPT-CLIP (with ViT-L-14-336, ResNet50$\times$64, and ViT-B-16), UNITER and LXMERT by a large margin (e.g., {Human Acc: \textbf{31.74} \textit{vs} {\color{red}XX.XX} \textit{vs} 27.12 \textit{vs} 23.09 \textit{vs} 22.90 \textit{vs} 21.10). We observe that frameworks on top of the CLIP, including \rpa~and CPT-CLIP, perform much better than conventional models (UNITER and LXMERT), even though the latter two are also pre-trained on up-stream datasets such as Visual Genome \cite{krishnavisualgenome}, MSCOCO \cite{lin2014microsoft}, Conceptual Captions \cite{sharma2018conceptual} and etc. This indicates that large-scale pre-trained knowledge is beneficial for abductive reasoning. Besides, we also validate by \rpa~that the fine-grained regional evidence fits well as a prompt for visual abductive reasoning tasks given the observation that \rpa~\textbf{26.39/XX.XX} \textit{vs} CPT-CLIP 23.09/XX.XX using ViT-B-16/ViT-L-14 backbones. Finally, our new Dual-Contrastive Loss boosts performances of both CPT and \rpa~with single contrastive losses. Notably, we observe the same phenomenon in our internal evaluation on the validation set as the test set in Table \ref{tab:cpt_vs_rpa}, indicating the improvements of the \rpa~and Dual-Contrastie loss are robust to different testing data and different backbones. In short, \rpa~with Dual Contrastive loss performs better than current SOTA methods.

%Particularly, compared to the CPT-CLIP baseline, we observe the same phenomenon on the testing set as our internal test on the validation set in Ablation \S \ref{sec:ab_study}, indicating the improvements of the new methods are robust to different testing data and different backbones (e.g., ViT-B-16 and ViT-L-14-336). In short, \rpa~performs better than colorful prompt tuning.

%; The simplified RGP$_s$ and Dual-Contrastive Loss robustly improve the original RGP. 

\input{table/different_adapter_v2}
\input{table/different_prompt_v2.tex}
\subsection{Ablation Study}
\label{sec:ab_study}
This section comprehensively studies various factors that influence the performance of our model on the validation set. We examine different integrations of Adapters and Regional Prompts, various contrastive losses, the impact of the bottleneck dimension $d$ in Adapters, and the effect of using different backbones and resolutions. For these tests, we use Mixed Prompts, Dual-Contrastive Loss and ViT-B-16 as our default settings, except in the ablations where we compare the performance of different prompts, losses or backbones.

\textbf{Impacts of Integrating Adapters}. We analyze how our model performs when we remove certain components, specifically the vanilla (A\&M) and Map Adapters, one at a time. The results in Table 3 show that performance decreases with fewer adapters. Specifically, using all three types of adapters produces the best results under most evaluation metrics. ``Adapter (M)'' is the best choice when limited to using just one type of adapter. If we can use two types, the best combination is ``Adapter (M) + Map Adapter'', suggesting that the Map Adapter complements the vanilla adapter well.

\textbf{Effects of Fine-Grained Regional Prompts}. We also explore how adding fine-grained regional prompts influences the performance of existing prompting techniques, such as colorful (CPT in \cite{yao2021cpt}) and circle prompts (CiP in \cite{shtedritski2023does}). In Table \ref{tab:combo_prompts}, the terms ``Region Only'' and ``Context'' refer to feeding either just the regional box part or the entire image into the CLIP vision tower, respectively. 

We observe that adding fine-grained tokens based on regional cues significantly improves the performance of all coarse-grained prompts, including ``Context'', ``CPT'', and ``CiP'' across all metrics. This basically verifies that ``global context + local cues'' complement each other well for abductive reasoning. Moreover, we test the Mixed Prompt mode described in \S \ref{sec:rpg} and observe a stable performance for most metrics. 

\textbf{Dual-Contrastive Loss} \textit{vs} \textbf{Single/Triple counterparts}. We test different types of contrastive losses using our \rpa~model. In Table \ref{tab:different_loss}, the Dual-Contrastive loss performs better than the Multi-Task Learning and the other single \& triple counterparts under most metrics. In terms of localization, the Dual-Contrastive loss is slightly lower than its MTL counterparts but still shows a very competing performance.
   
\input{table/different_loss}

\input{fig/pair_dist_loss}

We further look into the individual contrastive loss value between modality pairs on the validation set to understand how modalities mutually influence each other. Specifically, we first report the loss value between each pair before the training phase (i.e., No Train or Zero-Shot reasoning), then re-calculate them after the model is trained with different losses (Fig \ref{fig:dist_all}).



We observe that the gaps of \textit{vision-clue} and \textit{vision-inference} are positively correlated. Specifically, when we minimize one of the gaps in training, the other one will also become smaller (e.g., Fig. \ref{fig:dist_p3_inf_only}-\ref{fig:dist_p2_clue_only}). Whereas, the gap of \textit{inference-clue} seems not to correlate to gaps of \textit{vision-clue} and \textit{-inference}, as the former is slightly closer or even larger after minimizing either of the latter gaps (e.g., {\color{red}red}/black value in Fig \ref{fig:dist_p3_inf_only}-\ref{fig:dist_p4_mkl}, and \ref{fig:dist_p6_dual}). If we enforce the model to close the \textit{inference-clue} gap during training, the \textit{vision-clue} and \textit{-inference} gap would become larger (Triple \textit{vs} Dual, Fig. \ref{fig:dist_p5_trip} \textit{vs} \ref{fig:dist_p6_dual}). The reason is that the clue and inference sentences are not literally equivalent and better to be bridged by an extra rational process.

\textbf{Influence of Bottleneck Dimension $d$ in Adapters.} We study the influence of different bottleneck dimensions in the \rpa, ranging in $d=\{\frac{D}{32}, \frac{D}{16}, \frac{D}{8}, \frac{D}{4}, \frac{D}{2}, D\}$. Notably, a higher $d$ basically introduced more tuned parameters and larger FLOPs, as shown in Figure \ref{fig:plot_flops_para_bubble}. For the retrieval metrics, such as mean $img\leftrightarrows txt$ rank, a lower value indicates better performance, whereas the rest are the opposite. We observe from Figure \ref{fig:plot_meanrank_img_txt}-\ref{fig:plot_localization} that an optimal choice is $d=\frac{D}{4}$, indicating that adjusts the frozen foundational model with either a very heavy $D$ or lightweight $\frac{D}{32}$ would result in sub-optimal performance. Notably, human accordance is influenced by the human's subjective judgment and has a different trend. Overall, we fix $d=\frac{D}{4}$ for all following experiments.

\input{fig/dim_d_plot}

%\input{table/different_adapter_rate.tex}

\textbf{Comparison with tiny attention counterpart} \cite{zhao2022tiny}. As in Table \ref{tab:tinyatt}, we compare the performances between different integrations of ``TinyAtten + Adapter$\_$M / Adapters$\_$(A \& M)'' (Figure \ref{fig:tiny_atten}) against \rpa~(Figure \ref{fig:rpa_adapter}) on frozen CLIP ViT-B-16. 
We note that ``TinyAtten + Adapters$\_$(A\&M)'' performs worse than the \rpa~with more tuned parameters and FLOPs. The reason might be Map Adapter only re-weights the attention map and does not change the ``value'' bases. Overall, the \rpa~is a more effective and efficient adapter than its counterparts.
\input{fig/tiny_atten}
\input{table/tiny_adapter}

\textbf{Influence of Adapting CLIP Vision/Text Tower.} 
The CLIP follows a two-tower design, each tower separately for visual/textual embedding; thereby, we can independently insert adapters into visual and textual towers to assess their contributions. We test inserting Adapter$^\textbf{+}$ into the ``\textit{Only Text}'' tower, ``\textit{Only Vision}'' tower, and both towers. As shown in Table \ref{tab:towers}, adapting both CLIP vision and text towers performs the best at the cost of the most tuned parameters among the three options. Notably, both ``\textit{Only Vision}'' and ``\textit{Only Text}'' has a large margin in performance compared with the ``\textit{Vision + Text}'', indicating the adaptions on two towers are complementary.
\input{table/tower_test}


\textbf{Influence of Image Resolutions.} We test combo images with different resolutions to study whether the \rpa~can benefit from more tokens. As in Table \ref{tab:img_resolution}, it is natural to see an increment of computations when resolutions become larger. However, the boost of performance is not linear to the resolutions, reaching a saturate performance at the resolution of 448$\times$224. This might lie in that the CLIP is pre-trained at 224$\times$224 resolution on upstream dataset, thereby, downstream tuning is better to process images (\textit{one 448$\times$224 combo image = two 224$\times$224 images}) at similar settings. Considering the trade-off of FLOPs and performance, we pick the resolution of 448$\times$224 for the CLIP ViT-B-16 backbone.

\input{table/resolution}

\input{table/different_backbone}

\textbf{Effects of Backbones}. We test the \rpa~with different backbones, namely CLIP ViT-B16 and CLIP ViT-L14 (336). We observe that a larger backbone contains more encoders, thereby increasing both tuned parameters and FLOPs but resulting in a much better performance (see Table \ref{tab:adapt_ab1}).

\input{fig/vis}

\section{Qualitative Results of \rpa}
We present two qualitative examples obtained by the \rpa~in Figure \ref{fig:vis_example} and more examples in Figure \ref{fig:more_vis_example}. Specifically, a human expert gives a possible inference from given a regional cue specified by the box, and the machine retrieves the top-5 most likely inferences. The {\color{red}} sentence indicates the correct match with the human's performance. We observe from Example 1 \& 2 that the machine manages to deduct human-like inference such as ``\textit{He is trying to on board}'' from an observation of ``\textit{a man under an airplane}'' and ``\textit{prevent the sunshine}'' from ``\textit{a wearing hat}''.


%\newpage
\input{fig/vis_more}


\section{Generalization of \rpa~on RefCOCO}
We also tested the generalization of the \rpa~on the RefCOCO dataset using a two-stage pipeline. Specifically, we employed YoloV8 as the object detector to propose candidate object boxes. Then, we utilized the \rpa~to align textual sentences with the object box with the highest matching score. We evaluated the \rpa~using a single-prompt mode, such as ``R-CTX'', ``R-CPT'', to observe their respective effects.



\input{table/refcoco}

The \rpa~performs better than two-stage models like MattNet on the RefCOCO+/g sets, emphasizing appearance descriptions (e.g., ``\textit{a person with a yellow tie}''). However, it lags behind MattNet on RefCOCO, which focuses on positional descriptions (e.g., ``\textit{left person}''). This discrepancy arises because MattNet explicitly encodes \textit{appearance}, \textit{location}, and \textit{relation} information, while \rpa~only encodes appearance. Although \rpa~is adaptable to the Referring Comprehension task, it falls behind one-stage end-to-end models like MDETR. The advantage of MDETR has comes from its design to simultaneously regress box coordinates and establish visual-linguistic alignment, especially for visual grounding. In contrast, our \rpa~have to rely on third-party proposals from YoloV8.



%while lagging behind end-to-end trained one-stage models, such as MDTER. 
%the end-to-end trained single stage MDETR performs the best under most conditions,which meet our expectations. As the MDETR is particularly to do referring task and simultaneously optimize box coordinates and visual-lingusitic alignment.  

%In this section, we conduct experiments comparing Region Prompt and Colorful Prompt Tuning (baseline), studying independent effects and integrations of parts (e.g., region, context), and comparing different contrastive losses, on the Sherlock validation set with the CLIP ViT-B-16 backbone. To be consistent with the baseline method, we adopt the default Multi-Task Learning Loss \cite{hessel2022abduction} in all ablations except for the subsection comparing different losses.

%\textbf{Regional Prompt Tuning} \textit{vs} \textbf{Baseline (CLIP-CPT)}. We compare Regional Prompt (RGP), and its simplified version (RGP$_s$) with the Sherlock baseline \cite{hessel2022abduction}. For a fair comparison, we reproduce the baseline under the same training recipe as the RGP series. 

%\input{table/ablation_tokens_amount.tex}

%We draw three takeaways from Table \ref{tab:rgp_vs_baseline}: (1) Regional Prompt Tuning, regardless of RGP or RGP$_s$, significantly outperforms the baseline using colorful prompt tuning under all evaluation metrics. (2). The gain of performance does not come from a longer token sequence for the CLIP Transformer but from the fine-grained details in the region prompt, as we also extend the baseline in a double token length manner (i.e., CPT-CLIP ($\times2$ \texttt{Tokens} in Fig \ref{fig:clip_base_extend})) and observe a non-significant performance difference. The doubling is achieved by replacing region/context inputs in Figure \ref{fig:rgp_clip} with left/right crops. (3). Simplification of regional prompt tuning introduces further improvement (RGP$_s$ $>$ RGP) under most metrics. This implies that applying a uniform positional encoding (interpolated) for the combo of regional prompts and contextual tokens is better than independent encodings.


%\textbf{Impacts of Each Part}. We study the contributions of each individual part (i.e., regional and contextual tokens in Fig \ref{fig:rgp_clip_s}) by keeping only a part at a time during training and testing. We also conduct a plain \textit{context + region} setting, which independently processes regional and contextual tokens (imitate process in Figure \ref{fig:clip_base}) and sums up features of the two parts.

%\input{table/ablation_part.tex}

%As shown in Table \ref{tab:impacts_of_parts}, both the RGP$_s$ and plain \textit{context+region} outperforms the model using a single part, indicating that the region prompt and context are complementary. The RGP$_s$ further surpasses the plain addition (e.g., P@1$_{i\rightarrow t}$: \textbf{32.55} \textit{vs} 31.72). Moreover, the \textit{region only} outperforms the \textit{context only} setting by a large margin (e.g., P@1$_{i\rightarrow t}$: \textbf{28.72} \textit{vs} 17.87), further validating that fine-grained details in region prompt is critical for precise reasoning. Notably, localization metrics are not reported for \textit{context only}, as the model only sees full images without regional hints and thus can not do the regional matching.


%\input{fig/wrap_combo}
%\textbf{Combinations~of~Region~and~Context~(Combo)}. For the RGP$_s$ model, we test the influence of using different concatenations in generating the combo-image $\boldsymbol{J}$ (Eq. \ref{eq:combo_img}). Specifically, we could concatenate region $\boldsymbol{r}$ and context $\boldsymbol{i}$ images either along the \textit{vertical} or \textit{horizontal} axis (Fig. \ref{fig:combo}). This would affect the distances between token pairs. For example, in a vertical combo, a prompt token ({\color{purple}purple}) is spatially nearer to prompt tokens than contextual tokens ({\color{pigment}dark blue}) due to spatial flattening, whereas this is not true in the horizontal combo situation.

%\input{table/ablation_combo_w_or_h.tex}

%Table \ref{tab:vertcial_horizontal} lists the reasoning performances with vertical and horizontal combo inputs. The vertical combo is better in most metrics than its horizontal counterpart, indicating that keeping independent spatial relations within prompt and contextual tokens is beneficial.


%\textbf{Dual-Contrastive Loss} \textit{vs} \textbf{Single/Triple counterparts}. We test different types of contrastive losses using our RGP$_s$ model.  

%\input{table/dual_contrast.tex}



%Before and after the training phase, 




%\input{table/dual_contrast_adv_merge.tex}

%\textbf{Identical} \textit{vs} \textbf{Separate} text encoders for clue and inference. As mentioned in \S  \ref{sec:dual_contrast}, we test the impacts of using one identical or two separate text encoders for the clue and inference input. We adopt Dual-Contrastive Loss learning and two backbones, including CLIP ViT-B-16 and ViT-L-14-336.


%Table \ref{tab:separate_vitb} shows the results. We observe that with the ViT-B-16, the absolute improvements brought by separated text encoders are larger (e.g., $img\leftrightarrows text$ 15.78/17.88 \textit{vs} 16.05/18.19), whereas, with the ViT-L-14-336, the improvements become small (e.g., $img\leftrightarrows text$ 11.43/13.07 \textit{vs} 11.44/13.03). A possible reason is that with the increased complexity of the CLIP backbone, the text encoders also become deeper and could handle both clue and inference feature learning. 



