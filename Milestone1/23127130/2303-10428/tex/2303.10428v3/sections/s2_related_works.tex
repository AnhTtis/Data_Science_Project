\section{Related Works}
Our Region-Prompted Adapter Tuning~is relevant to research areas, including Foundation Models, Abductive Reasoning Tasks, Parameter-Efficient Fine-Tuning, and Fine-Grained Visual Representation. We will separately illustrate related works according to the areas below.

\textbf{Foundation Models.} ``\textit{More is different}'' \cite{anderson1972more}. Scaling up models' complexities and training data scales significantly improves the attention-based \cite{galassi2020attention, otter2020survey,liu2023survey,hao2022attention} foundation models' \cite{brown2020language, devlin2018bert, radford2021learning, jia2021scaling, yu2022coca, yuan2021florence} perception capacity, making it proficient in humanoid tasks such as zero or few-shot learning. Specifically, Large Language Models (LLM), such as BERT \cite{devlin2018bert}, and GPT \cite{brown2020language} pre-trained on a large-scale corpus, are generalizable to downstream NLP tasks with few training examples. This trend quickly swept to the Vision-Language areas, with representative works such as CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling}, BLIP \cite{li2022blip} etc. The main idea behind the VL foundation models is to learn transferable visual representation with noisy text supervision through a two-tower structure. We follow the current VAR pipeline and adopt the foundation model CLIP to generate initial visual \& textual features. 


\textbf{Abductive Reasoning Tasks.} Humans reckon plausible inferences or hypotheses from incomplete observations every day \cite{cohen1933collected}. To teach the machine with this ability, researchers proposed several new Tasks, like \dataset{}~\cite{bhagavatula2020abductive} for NLP, \textit{Sherlock} \cite{hessel2022abduction} for vision, and \textit{VideoVAR} \cite{liang2022visual}, \textit{VideoABC} \cite{zhao2022videoabc} for video. Specifically, the \dataset{}~\cite{bhagavatula2020abductive} generates the most likely hypothesis (text) to explain what has happened between the two observations (texts). For Sherlock, VideoVAR, and VideoABC, the observations are represented by regional or whole images, while inference could be text or middle frames. There are several similar tasks, such as Visual Commonsense Reasoning (VCR) \cite{zellers2019vcr} and Visual7W \cite{zhu2016visual7w}. Abductive reasoning differs from them in having non-definitely correct, plausible inferences as human beings.


\textbf{Parameter-Efficient Fine-Tuning} (PEFT). Transferring foundational models to downstream tasks promotes the development of PEFTs \cite{sabry2023peft}. Representative PEFTs include Prompt, Adapter, and LoRA tuning. Specifically, prompt tuning \cite{brown2020language, lester2021power, zhong2021factual, tu2022prompt, liu2022p} enhances the distinctiveness of inputs by prepending additional tokens, which may be either trainable or fixed. The VL prompt tuning can further be divided into textual \cite{zhou2022coop,zhou2022cocoop,feng2022promptdet,du2022learning} or visual prompt \cite{jia2022visual,bahng2022exploring} tuning, depending on the placement of prompt tokens in visual or textual encoders. Certain special visual prompts, such as the Merlot \cite{zellersluhessel2021merlot}, CPT \cite{yao2021cpt}, and CiP \cite{shtedritski2023does}, guide the model to focus on specified areas by overlaying these regions with translucent colors or red circles. In adapter tuning, trainable Multi-Layer Perceptron (mini MLP) \cite{yang2023aim, sung2022vl,houlsby2019parameter} or Tiny Attention modules \cite{zhao2022tiny} are usually inserted into the foundational model, with only the new additions being fine-tuned. In the case of LoRA \cite{hu2021lora}, model parameters are adjusted using low-rank projections. Our \rpa~belongs to a hybrid ``\textit{prompt}+\textit{adapter}'' tuning to equip the VLF with local reasoning ability, an approach that has not been studied before.




\textbf{Fine-Grained Visual Representation.} Our work is also relevant to learning fine-grained visual representation \cite{zhong2022regionclip, zhang2018fine, zhang2022glipv2, wang2022internimage, aberdam2023clipter,shao2022region} and object detection \cite{he2017mask,zhao2019object, jiao2021new,huang2022sfa,bi2022iemask}. Specifically, GLIP \cite{zhang2022glipv2} and RegionCLIP \cite{zhong2022regionclip} pre-train foundation models for object detection, supervised by region-text pairs. The former and latter mimic the process of R-CNN\cite{girshick2014rich} and Faster-RCNN \cite{ren2015faster}, generating an object's vector by either encoding the cropped image or RoI pooling. Similarly, UNITER \cite{chen2020uniter} and LXMERT \cite{tan2019lxmert} also rely on RoI pooling to generate regional vectors for vanilla VL tasks. Besides, the InternImage \cite{wang2022internimage} learns the foundation model with Deformable-CNN and achieves a new SOTA for object detection. Other works, such as Object-VLAD \cite{zhang2018fine} for event detection, and CLIPTER \cite{aberdam2023clipter} for scene text recognition, also studied fine-grained modeling. Specifically, the Object-VLAD aggregates densely collected local features with VLAD for generating video representation. The CLIPTER introduces extra cross-attention and the gated fusion module to combine local and global features. In contrast, our \rpa~doesn't tweak the CLIP structure and only modifies the input space.


%\textbf{Parameter-Efficient Fine-Tuning} (PEFT). Transferring Large Language Models to downstream tasks promotes the development of PEFTs \cite{sabry2023peft}. Representative PEFTs include Prompt tuning \cite{brown2020language, lester2021power, zhong2021factual, tu2022prompt, liu2022p}, Adapter tuning \cite{houlsby2019parameter}, and LoRA tuning \cite{hu2021lora}. Specifically, prompt tuning primarily enhances the distinctiveness of inputs by prepending additional tokens, which can be either trainable or fixed. Adapter tuning, on the other hand, inserts trainable mini MLP (Multi-Layer Perceptron) or Tiny Attention modules \cite{zhao2022tiny} into the foundational model, while only fine-tuning these new add-ons. In the case of LoRA \cite{hu2021lora}, model parameters are adjusted using low-rank projections. The rise of VL foundation models further extends PEFT into VL-PEFT for visual downstream tasks. Specifically, VL-Prompt tuning can be divided into textual or visual prompt tuning according to whether to place prompt tokens in visual or textual encoders. For example, the CoOp/CoCoOp \cite{zhou2022coop,zhou2022cocoop} and PromptDet/DetPro \cite{feng2022promptdet,du2022learning} padded extra learnable textual prompts along with the textual category for zero/few-shot classification and open-vocabulary object detection tasks. For the visual prompt tuning, VPT \cite{jia2022visual} proposes to prepend trainable tokens with visual tokens, while the VP \cite{bahng2022exploring} moves the prompt to pixel space by learning learnable paddings that surround the image. For segmentation tasks, the OVSeg \cite{liang2022open} replaces tokens of the non-mask area with learnable visual prompts. The Merlot \cite{zellersluhessel2021merlot},  concurrent CPT \cite{yao2021cpt} and CiP \cite{shtedritski2023does} propose hard visual prompts that cover core regions either with translucent color overlays or red circles to guide the model to focus on these specific areas. For VL-Adapter, 


%Our \rpa~belongs to a hybird ``\textit{prompt}+\textit{addapter}'' tuning, that explicityly enlarges regional hints to generated fine-grained visual prompts, which have not been studied before.


%These techniques  Specifically, the CoOp/CoCoOp \cite{zhou2022coop,zhou2022cocoop} and PromptDet/DetPro \cite{feng2022promptdet,du2022learning} padded extra learnable textual prompts along with the textual category for zero/few-shot classification and open-vocabulary object detection tasks. For the visual prompt tuning, VPT \cite{jia2022visual} proposes to prepend trainable tokens with visual tokens, while the VP \cite{bahng2022exploring} moves the prompt to pixel space by learning learnable paddings that surround the image. For segmentation tasks, the OVSeg \cite{liang2022open} replaces tokens of the non-mask area with learnable visual prompts. The Merlot \cite{zellersluhessel2021merlot} and concurrent CPT \cite{yao2021cpt} propose hard visual prompts that cover core regions with translucent color overlays to guide the model to focus on these specific areas. Our regional prompt tuning belongs to the hard visual prompt tuning as the Merlot/CPT. Our \rpa~belongs to a hybird ``\textit{prompt}+\textit{addapter}'' tuning, that explicityly enlarges regional hints to generated fine-grained visual prompts, which have not been studied before.


%The RGP explicitly enlarges regional hints to generate fine-grained visual prompts, which have not been studied before.

%The foundational Language Models have popularized the PEFT techniques \cite{sabry2023peft} to reduce trainable parameters. Representive PEFTs includes Prompts \cite{brown2020language, lester2021power, zhong2021factual, tu2022prompt, liu2022p}, Adapters\cite{houlsby2019parameter} and LoRA\cite{hu2021lora} tuning. Specifically, prompt tuning mainly improves the distinctiveness of inputs by prepending additional tokens, which can be either trainable or fixed. Adapter tuning, on the other hand, inserts trainable mini MLP (Multi-Layer Perceptron) or Tiny Attention modules \cite{zhao2022tiny} into the foundational model while only fine-tuning these new add-ons. In the case of LoRA \cite{hu2021lora}, model parameters are adjusted with low-rank projections. 

%Prompt technique incorporating additional parameters into input embedding layer, while Adatper uses lightweight bottleneck networks and tiny-attention adapter \cite{zhao2022tiny} that reduces the attention head's dimensionality seems more potent than LoRA\cite{hu2021lora} which inserts trainable low-rank projection matrices. For VL tasks, there are a few notable visual prompts and adapters, such as CPT\cite{yao2021cpt}, CiP\cite{shtedritski2023does}, RGPs\cite{zhang2023fine}, and AIM\cite{yang2023aim}. Some studies show that either adding the bottleneck Adapter \cite{yang2023aim} or even replacing the MLP layer \cite{chen2022adaptformer} in the freezing pre-trained ViT models deliver superior results. Inspired by these findings, we further study the potential of combining certain visual prompts and adapter techniques on top of frozen VLF models for the VAR problem.

%\textbf{Prompt Tuning} comes from adapting the LLM to downstream NLP tasks by adding a few ``virtual tokens'' as the prefix or suffix to original sentences. According to whether using trainable continuous or handcrafted discrete ``virtual tokens'', we could classify prompts into soft \cite{lester2021power,li2021prefix, qin2021learning} or hard \cite{jiang2020can, brown2020language, liu2021gpt} prompts. 

%The rise of VL foundation models further extends prompt tuning for visual downstream tasks. 




%textual tuning contains the CoOp \cite{zhou2022coop}, CoCoOP \cite{zhou2022cocoop}, PromptDet \cite{feng2022promptdet} and DetPro \cite{du2022learning}; whereas the latter includes VPT \cite{jia2022visual,bahng2022exploring}, CPT \cite{zellersluhessel2021merlot, yao2021cpt} and OVSeg \cite{liang2022open}. 

%is original  CoCoOP \cite{zhou2022cocoop}, CoOp \cite{zhou2022coop}, Visual Prompt Tuning \cite{jia2022visual,bahng2022exploring}, Colorful Prompt \cite{zellersluhessel2021merlot}, Multi-Task Prompt \cite{shen2022multitask}, OVSeg \cite{liang2022open} Mask Prompt



%\textbf{Foundation Models.} More is different. Scaling up the complexity of model and training data-scale significantly change Firstly gain success in NLP then Swept over NLP, Vision-Language CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling}, CoCa \cite{yu2022coca} GPT\cite{brown2020language},BERT \cite{devlin2018bert}, Florence \cite{yuan2021florence}, Flamingo \cite{alayrac2022flamingo}



%\textbf{Abductive Reasoning Tasks.} Humans reckon plausible inferences or hypotheses from incomplete observations every day \cite{cohen1933collected}. To teach the machine with this ability, researchers proposed several new Tasks, like \dataset{} \cite{bhagavatula2020abductive} for NLP, \textit{Sherlock} \cite{hessel2022abduction} for Vision, and \textit{VideoVAR} \cite{liang2022visual}, \textit{VideoABC} \cite{zhao2022videoabc} for video. Specifically, given two chronological observations in the forms of texts, the \dataset{} \cite{bhagavatula2020abductive} match/generate the most plausible hypothesis (text) to explain what has probably happened between the two observations. Whereas, for Visual Abductive Reasoning tasks (VAR), like Sherlock, VideoVAR, and VideoABC, the observations are visual, represented by either a single image or two video frames; and the inference or hypothesis from the observation(s) is a sentence or a middle frame in between. Similar tasks, such as Visual Commonsense Reasoning (VCR) \cite{zellers2019vcr} and Visual7W \cite{zhu2016visual7w}, also conduct rational reasoning but with definitely correct inferences. In contrast, the VAR provides the most reasonable hypotheses, as humans do.%For example, drawing a \textit{hypothesis} ``\emph{it might have rained before}'' from an \textit{observation} ``\emph{a wet road}''. 

%Abdutive Reasoning Sherlock \cite{hessel2022abduction}, VideoABC \cite{zhao2022videoabc}, VAR \cite{liang2022visual}, different from VisualGenome \cite{krishnavisualgenome}, VCR \cite{zellers2019vcr}




%\textbf{Object Detection.} Faster-RCNN, DeTER, DINO, Object-VLAD, RegionCLIP \cite{zhong2022regionclip}, CLIPER \cite{aberdam2023clipter}, GLIP \cite{zhang2022glipv2}, InterImage \cite{wang2022internimage} ,