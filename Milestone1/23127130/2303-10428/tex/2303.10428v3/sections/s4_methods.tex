
\section{Method} 

In this section, we introduce our \textbf{Region-Prompted Adapter Tuning} (\rpa~in Fig. \ref{fig:rpa}), which enhances \textit{the frozen VL foundation models to focus on specific visual cues for deducing inference}. The \rpa~consists of two main parts: a Regional Prompt Generator (RPG in \S \ref{sec:rpg}) for targeting specific visual areas and an Adapter$^\textbf{+}$ module (\S \ref{sec:adapter}) to transfer the frozen CLIP model for reasoning tasks. Finally, we replace Multi-Task Learning \cite{hessel2022abduction} with a new \textbf{Dual-Contrastive Loss} (\S \ref{sec:dual_contrast}) to bring the visual features closer to both literal description (``clue'') and hypothesis (``Inference''). We will elaborate on each section below.

\subsection{Regional Prompt Generator}
\label{sec:rpg}

We propose treating pre-specified observation $\boldsymbol{r}$ as a ``prompt'' to directly guide the visual reasoning process. Our Regional Prompt Generator (RPG) can create three detailed prompts focusing on specific areas. These prompts harness local features (i.e., region), surrounding context, and existing visual prompts as shown in Figure \ref{fig:prompt1}-\ref{fig:prompt3}. All three types of prompts go through the same process, with the only difference in being whether colors or circles are drawn on the input images. To explain how it works, we'll use the ``Region+Context'' (R-CTX) as an example.

To prepare prompt and contextual tokens, we pop out patch-embedding layer $\mathcal F_{proj}$ and positional encoding \texttt{PE} from the vision tower $\mathcal F_{\text{V}}$. We further resize region $\boldsymbol{r}$ and full image $\boldsymbol{i}$ into squares of the same size, then concatenate them vertically (or horizontally) into a \textbf{combo-image} $\boldsymbol{J}$ (Eq. \ref{eq:combo_img}). We apply patch-embedding on the combo and add it with upsampled \texttt{PE}$_{inter}$ to get visual tokens $\boldsymbol{z}_0$ (Eq. \ref{eq:ir_enc3}). As the \texttt{PE}$_{inter}$ is twice the size of \texttt{PE}, we initialize it by inflating $\texttt{PE}$ using bilinear interpolation. Notably, $\boldsymbol{z}_0$, generated from the combo-image, already includes both regional prompt and global contextual tokens. The $\boldsymbol{z}_0$ is further fed into the remaining attention and MLP layers (denoted by $\hat{\mathcal F}_{\text{V}}$) to get visual representation $\mathcal{I}_{vis}$ (Eq. \ref{eq:vis_combo}). We unfreeze patch-embedding $\mathcal F_{proj}$ and positional encoding \texttt{PE}$_{inter}$ to generate learnable soft prompts. 

\input{equations/rgp_s_eq.tex}

For the ``Region + Colorful Prompt'' and ``Region + Circle Prompt'' (R-CPT \& R-CiR), we create the combo-image $\boldsymbol{J}$ from the squared region $\boldsymbol{r}$ and a modified image $\boldsymbol{i}'$. In this image $\boldsymbol{i}'$, we either color the pixels inside the region with a translucent {\color{mypink}pink} rectangle or outline them with a {\color{red}red} circle.

\textbf{Mixed Prompts}: During training, we randomly choose from the three types of prompts (R-CTX, R-CPT, and R-CiR) with equal chance. For testing, we take the average of the visual representations created by these three prompts. For the text-based representation $\mathcal{I}_{text}$ and the loss function, we replace the equations \ref{eq:text_enc}-\ref{eq:clip_base_loss} by the new equations \ref{eq:dc_enc}-\ref{eq:dc_comp} under the Dual-Contrastive Loss learning scheme. We'll go into more detail about this loss in \S \ref{sec:dual_contrast}.

\subsection{Adapter$^\textbf{+}$ Tuning}
\label{sec:adapter}
Adapter tuning adjusts a ``\textit{locked-in}'' foundational model for downstream tasks by fine-tuning a few newly implanted modules. This method has successively swept over NLP \cite{yang2023aim} and computer vision \cite{radford2021learning} areas. Current adapter structures, like mini MLP and tiny attention modules, focus mainly on refining visual features. However, they don't consider the need to adjust the original attention maps of the base models. To tackle this, we augment the vanilla adapter with a new, orthogonal \textbf{\textit{Map Adapter}}, which precisely adapts attention maps in Transformers. This results in the improved Adapter$^\textbf{+}$.

The Adapter$^\textbf{+}$ pipeline is illustrated in Figure \ref{fig:rpa_adapter}. We first include two basic adapters, referred to as Adapter (A\&M). They are placed after the MSHA  module and parallel to the MLP module in the $l$-th encoder of a CLIP tower (e.g., $\mathcal{F}_{\text{V}}$ or $\mathcal{F}_{\text{T}}$). These adapters are shallow and contain only two fully-connected layers to downgrade and upgrade feature dimension ($\mathbb{R}^D\leftrightarrows \mathbb{R}^d$, $d<D$) with a GELU activation in between (Equation \ref{eq:adapt_1}-\ref{eq:adapt_3}). The {\color{purple}light red} font indicates the tunable status of each layer.
\input{equations/base_adapter}

The {\textbf{Map Adapter}} further refines the MSHA module by adding a small, modified attention map, labeled as {\color{purple}$\widehat{\boldsymbol{Q}}\widehat{\boldsymbol{K}}^T$} (refer to Equation \ref{eq:map_adapt_1}). This additive map helps to adjust the original attention map dynamically, improving the model's ability to focus on relevant information. To ensure that the original attention map isn't altered too much, we use simpler $D\rightarrow d$ projections for generating the query and key (see Equation 3). Here, $d$ is smaller than $D$.
\input{equations/map_adapter}

We compared the enhanced Adapter$^\textbf{+}$ with vanilla and Tiny Adapter counterparts (see \S \ref{sec:ab_study}) and found that our tuning method consistently performs better.
%generates an additive attention map {\color{pink}$\widehat{\boldsymbol{Q}}\widehat{\boldsymbol{K}}^T$} for the MSHA module (see Equation \ref{eq:map_adapt_1}). To avoid excessive modification of the initial attention map, we utilize low-rank $D\rightarrow d$ projections for query/key generation (see Equation \ref{eq:map_adapt_3}), where $d < D$ . 
%\input{equations/map_adapter}

%The pipeline of Adapter$^\textbf{+}$ is shown in Figure \ref{fig:rpa_adapter}. Following existing pipeline, two vanilla adapters are placed after the MSHA  module (i.e., Adapter$\_$A) and parallel to the MLP modules (i.e., Adapter$\_$M) in the $l$-th Transformer encoder of a CLIP tower (e.g., $\mathcal{F}_{\text{CLIP\_V}}$ or $\mathcal{F}_{\text{CLIP\_T}}$). These adapters are shallow and contain only two fully-connected layers to downgrade and upgrade feature dimension ($\mathbb{R}^D\leftrightarrows \mathbb{R}^d$, $d<D$) with a GELU activation in between.


%\textbf{Vision Adapter}, inspired by the Language Adapter \cite{houlsby2019parameter}, adjusts pre-trained foundation models to suit downstream tasks. It achieves this by inserting trainable modules into the foundation model and only fine-tuning these new parameters. For example, in AIM \cite{yang2023aim}, the adapters are placed after the MHSA module (i.e., Adapter$\_$A) and parallel to the MLP modules (i.e., Adapter$\_$M) in the $l$-th Transformer encoder of a CLIP tower (e.g., $\mathcal{F}_{\text{CLIP\_V}}$ or $\mathcal{F}_{\text{CLIP\_T}}$). These adapters are shallow and contain only two fully-connected layers to downgrade and upgrade feature dimension ($\mathbb{R}^D\leftrightarrows \mathbb{R}^d$, $d<D$) with a GELU activation in between. Formula \ref{eq:adapt_1}-\ref{eq:adapt_3} shows this process, where {\color{brightcerulean}blue}/{\color{mypink1}red} denotes parameters are frozen or tuned.


%Adapter tuning origins from adjusting pre-trained large language models to suits down-stream tasks and recently exapnd to vision areas.

\subsection{Dual-Contrastive Loss}
\label{sec:dual_contrast}
As an observation contains three modalities, such as visual $\boldsymbol{J}$, clue sentence $\boldsymbol{c}$, and inference sentence $\boldsymbol{f}$, we comprehensively study their mutual influences by deploying contrastive loss between different modalities pairs. Specifically, as shown in Figure \ref{fig:loss_all}, we deploy dual, triple, and single contrastive loss in the training phase and screen out that the \textit{Dual-Contrastive Loss works best} (Fig. \ref{fig:v2_dual_loss_fig}). We first elaborate on the Dual-Contrastive Loss and then compare it with the other counterparts.
\input{fig/loss_variants.tex}

\textbf{Dual-Contrastive Loss}: Both the clue $\boldsymbol{c}$ and inference $\boldsymbol{f}$ are positively relevant to visual $\boldsymbol{J}$. More specifically, the former is literally equivalent, while the latter is causally related to the visual hints. Although their relations are in different forms, we can still deploy a Dual-Contrastive Loss, including one for ``\textit{vision-clue}'' pair and the other for ``\textit{vision-inference}'' pair (Fig. \ref{fig:loss_dual}), to regress visual features toward two textual targets. The mathematical process is present in Equation \ref{eq:dc_enc}-\ref{eq:dc_comp}. %The $\mathcal{F}_{T1}$ and $\mathcal{F}_{T2}$ separately denotes textual encoders for clue and inference. Notably, we could use an identical (i.e., Dual-Contrast: $\mathcal{F}_{T1}=\mathcal{F}_{T2}$) or different encoders (i.e., Dual-Contrast*: $\mathcal{F}_{T1}\neq\mathcal{F}_{T2}$) to adjust model's parameters scale.
\input{equations/dual_contrast_eq.tex}

\textbf{Other Loss Variants}. The rest loss functions include the \textbf{Triple} and \textbf{Single} contrastive loss. Particularly, compared with dual contrastive loss, the triple one newly adds the ``\textit{inference-clue}'' pair (e.g., Fig. \ref{fig:loss_trip} and Eq. \ref{eq:trip}).
\input{equations/trip_contrast_eq.tex}

We later observed that: additional \textit{inference-clue} loss in triple contrastive hurts overall performance, as the two texts (i.e., clue and inference) are not literally equivalent. For example, the clue sentence ``\textit{the road is wet}''$\neq$ inference sentence ``it has rained before''. Therefore, we can only let the two texts learn toward the same third-party feature (e.g., the visual) instead of artificially forcing them to be equivalent.

For the single contrastive loss, we have three options, namely \textit{vision-inference} (Fig. \ref{fig:loss_inf_vis}), \textit{vision-clue} (Fig. \ref{fig:loss_clue_vis}), \textit{multi-task} learning (MTL in Fig.\ref{fig:loss_mt}). Notably, we use an identical textual encoder for clue and inference during testing, since we only learn a single contrastive loss between a pair of modalities during training.

These three options can be expressed in one unified form (Eq. \ref{eq:unf}), by thresholding a random probability $p$ with different values $\overline{\mathrm{T}}$. Specifically, when $\overline{\mathrm{T}}=1.0~\mathrm{or}~0~\mathrm{or}~0.5$, the single contrastive loss degenerates into the \textit{vision-clue}, \textit{vision-inference} and \textit{multi-task} learning loss.
\input{equations/s_contrast_3.tex}

With the single contrastive loss, we find that only minimizing the gap between a pair, such as \textit{vision-clue} (or \textit{vision-inference}) will also shorten the gap between the other pair \textit{vision-inference} (or \textit{vision-clue}), indicating retrieval and abductive reasoning tasks have positive rational correlations. We give detailed analysis in \S \ref{sec:ab_study}.


