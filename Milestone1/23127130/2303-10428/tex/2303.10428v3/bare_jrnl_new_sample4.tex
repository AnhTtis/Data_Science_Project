\documentclass[lettersize,journal]{IEEEtran}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url} 
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage[super]{nth}
\usepackage{booktabs}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{amssymb}
%\usepackage{subcaption}
\usepackage{epsfig}
\usepackage[table]{xcolor}
\usepackage{tabulary,multirow,overpic}
\usepackage{makecell}

\usepackage{fontenc}    % use 8-bit T1 fonts

%\definecolor{mypink}{RGB}{156,77,108}
\definecolor{mypink}{rgb}{0.96, 0.76, 0.76}

\definecolor{tunecolor}{RGB}{236, 186, 179}
\definecolor{citecolor}{RGB}{34,139,34}
\definecolor{citecolor2}{HTML}{0071bc}
\definecolor{Graylight}{gray}{0.9}
\definecolor{lightred}{RGB}{241,140,142}
\definecolor{pigment}{rgb}{0.2, 0.2, 0.6}
\definecolor{YellowGreen}{RGB}{91, 165, 133}

\definecolor{baselinecolor}{gray}{.92}
\newcommand{\default}[1]{\cellcolor{baselinecolor}{#1}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\newcommand{\xmark}{\ding{55}}%
\newcommand{\dataset}{{\usefont{T1}{pzc}{m}{n} ART}}
\newcommand{\RPA}{{\usefont{T1}{cinzeldecorativebold}{m}{n} RPA}}
\newcommand{\rpa}{{\usefont{T1}{cinzeldecorative}{m}{n} RPA}}


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

%\title{Fine-Grained Regional Prompt Tuning for Visual Abductive Reasoning}
\title{A Region-Prompted Adapter Tuning for \\Visual Abductive Reasoning}

\author{Hao Zhang, Yeo Keat Ee, Basura Fernando
%,~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\markboth{Submit to the IEEE TNNLS Special Issue on Deep Learning for Intelligent Media Computing and Applications}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Visual Abductive Reasoning (VAR) is an emerging vision-language (VL) topic where the model needs to retrieve/generate a likely textual hypothesis from a visual input (image or part of an image) using backward reasoning based on prior knowledge or commonsense.
Unlike in conventional VL retrieval or captioning tasks, where entities of texts appear in the image, in abductive inferences, the relevant facts about inferences are not readily apparent in the input images. 
Besides, these inferences are causally linked to specific regional visual cues and would change as cues change. 
Existing works highlight visual cues from a global background utilizing a specific prompt (e.g., colorful prompt). Then, a full fine-tuning of a VL foundation model (VLF) is launched to tweak its function from {perception} to {deduction}. However, the colorful prompt uniformly patchify ``regional hints'' and ``global context'' at the same granularity level and may lose fine-grained visual details crucial for abductive reasoning. Meanwhile, full fine-tuning of VLF on limited data would easily be overfitted and incur generalizability. 

To tackle this, we propose a simple yet effective \textit{Region-Prompted Adapter} (\RPA), a hybrid parameter-efficient fine-tuning method that leverages the strengths of detailed cues and efficient training for the VAR task. Specifically, our \RPA~consists of two novel modules: Regional Prompt Generator (RPG) and Adapter$^\textbf{+}$. The prior encodes ``regional visual hints'' and ``global contexts'' into visual prompts separately at fine and coarse-grained levels. The latter extends the vanilla adapters with a newly designed Map Adapter, which modifies the attention map using a trainable low-dim query/key projection. Additionally, we train the \RPA~with a new Dual-Contrastive Loss to regress the visual feature simultaneously toward features of factual description (a.k.a. clue text) and plausible hypothesis (abductive inference text) during training. Extensive experiments on the Sherlock dataset demonstrate that our \RPA~with Dual-Contrastive Loss significantly outperforms previous SOTAs, achieving {the \nth{1} rank} on abductive reasoning leaderboards among all submissions (e.g., Comparison to Human Accuracy: \RPA~31.74 \textit{vs} CPT-CLIP 29.58, higher=better). We would open-source our codes at \textit{\color{magenta}{\url{https://github.com/LUNAProject22/RPA}}}.


\end{abstract}

\begin{IEEEkeywords}
Visual Abductive Reasoning, Regional Prompt, Adapter Tuning, Vision-Language Models, Cross-Media Retrival.
\end{IEEEkeywords}

%%%---------
% Sections
%%%---------

\input{sections/s1_intro}
\input{sections/s2_related_works}
\input{sections/s3_problem}
\input{sections/s4_methods}
\input{sections/s5_experiment}





\section{Conclusion}
We propose a new Region-Prompted Adapter Tuning (\rpa) with a Dual-Contrastive Loss for Visual Abductive Reasoning. Specifically, our method validates that curating fine-grained regional prompts is feasible for CLIP tuning, getting back local details, and benefiting abductive reasoning. We also reveal the positive relationships between the VAR and Vanilla Visual Retrieval tasks, unifying their training processes with the Dual-Contrastive Loss. Extensive experiments show that the \rpa~and the new loss are robust and effective for abductive reasoning and surpass previous SOTAs. The success of the two factors also paves future ways for exploring Multi-Grained, Chain-of-Thoughts Prompts, Visual Referring Prompt, and other multiple relationships modeling on the VAR. 


\section*{Acknowledgments}
This research / project is supported by the National Research Foundation, Singapore, under its NRF Fellowship (Award NRF-NRFF14-2022-0001). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.


%{\appendix[Proof of the Zonklar Equations]
%\input{sections/appendix}

%Use $\backslash${\tt{appendix}} if you have a single appendix:
%Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
%If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
%You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
% starts a section numbered zero.)}

%\appendix
%\section{Training Recipe}
%\label{appendix:train_recipe}
%We present detailed experimental settings for replication purposes in Table \ref{appendix:train_recipe}. All experiments are conducted with 4$\times$ 80GB A100 GPUs, PyTorch-v2.0, and fixed seed 42. All ablations in \S \ref{sec:s4_4} follow the same settings with $448\times224$ resolutions and CLIP ViT-B16, batch size to 1200$\times$8 and learning rate, lr $=3\cdot$10e-4, unless particularly specified. For large backbone or large resolution, we reduce the batch size and learning rate simultaneously according to the linear-scaling rule in \cite{goyal2017accurate}. For full fine-tune settings in Table \ref{tab:three_adapt}, we need to reduce the learning rate by 30 times lower, as we experimentally verify that updating all parameters with a large lr = 3$\cdot$10e-4 easily overfits to downstream training data and incurs generalization.

%\appendix[Training Recipe]
%\section{Training Recipe}
%\label{appendix:train_recipe}
%We present detailed experimental settings for replication purposes in Table \ref{tab:ab_settings}. All experiments are conducted with 2$\times$ 80GB A100 GPUs, PyTorch-v2.0, and fixed seed 42. All ablations in \S \ref{sec:ab_study} follow the same settings with $448\times224$ resolutions and CLIP ViT-B16, batch size to 1600$\times$2 and learning rate, lr $=2\cdot$10e-4, unless particularly specified. For large backbone or large resolution, we reduce the batch size and learning rate simultaneously. For full fine-tune settings in Table \ref{tab:three_adapt}, we need to reduce the learning rate lower, as we experimentally verify that updating all parameters with a large lr easily overfits to downstream training data and incurs generalization.
%\input{table/appendix_train_recipe}

\bibliographystyle{IEEEtran}
\bibliography{egbib}

%\begin{comment}
%\appendices
%\section*{Appendix: Training Recipe}



%\section*{Appendix: Influence of Bottleneck Dimension $d$ in Adapters.}
%\section{Influence of Bottleneck Dimension $d$ in Adapters.}

%\input{table/different_adapter_rate.tex}

%Appendix two text goes here.}
%\section*{Appendix: .}
%\input{fig/vis_more}
%\input{table/different_adapter_rate.tex}
%\end{comment}

%\section{References Section}
%You can use a bibliography generated by BibTeX as a .bbl file.
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
%\section{Simple References}
%You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
% (used to reserve space for the reference number labels box).

%\begin{thebibliography}{1}
%\bibliographystyle{IEEEtran}

%\bibitem{ref1}
%{\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf

%\bibitem{ref2}
%T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

%\bibitem{ref3}
%F. Mittelbach and M. Goossens, {\it{The \LaTeX Companion}}, 2nd ed. Boston, MA, USA: Pearson, 2004.

%\bibitem{ref4}
%G. Gr\"atzer, {\it{More Math Into LaTeX}}, New York, NY, USA: Springer, 2007.

%\bibitem{ref5}M. Letourneau and J. W. Sharp, {\it{AMS-StyleGuide-online.pdf,}} American Mathematical Society, Providence, RI, USA, [Online]. Available: http://www.ams.org/arc/styleguide/index.html

%\bibitem{ref6}
%H. Sira-Ramirez, ``On the sliding mode control of nonlinear systems,'' \textit{Syst. Control Lett.}, vol. 19, pp. 303--312, 1992.

%\bibitem{ref7}
%A. Levant, ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proc. 45th IEEE Conf. Decis.
%Control}, San Diego, CA, USA, 2006, pp. 5585--5590. DOI: 10.1109/CDC.2006.377165.

%\bibitem{ref8}
%M. Fliess, C. Join, and H. Sira-Ramirez, ``Non-linear estimation is easy,'' \textit{Int. J. Model., Ident. Control}, vol. 4, no. 1, pp. 12--27, 2008.

%\bibitem{ref9}
%R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez, ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proc. Amer. Control Conf.}, Chicago, IL, USA,
%2000, pp. 2245--2249.

%\end{thebibliography}


%\newpage

%\section{Biography Section}
%If you have an EPS/PDF photo (graphicx package needed), extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
% your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
% simpler here.)
 
%\vspace{11pt}

%\bf{If you include a photo:}\vspace{-33pt}
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{eps/zhanghao.eps}}]{Dr. Hao Zhang} received his B.Sc. degree from Nanjing University in China in 2012. He then received his M.Sc. degree from the Chinese University of Hong Kong in 2013, followed by a Ph.D. in computer science from City University of Hong Kong. He is a Research Scientist at the Institute of High Performance Computing (IHPC) under the Agency for Science, Technology and Research (A*STAR) in Singapore. His research interests include multimedia content analysis and reasoning, specifically in Multimedia Event Detection, Action Classification and Detection, and Visual Abductive Reasoning.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{eps/yeokeat.eps}}]{Yeo Keat Ee} received his B.Sc. degree in Instrumentation Science in 2017, followed by a M.Sc. degree in Intelligent Systems from Universiti Putra Malaysia in 2022. He is a Research Engineer at Centre for Frontier AI Research (CFAR) of the Agency for Science, Technology and Research (A*STAR) in Singapore. His research interests are including multi-modal learning, computer vision and reinforcement learning.
%\end{IEEEbiography}

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{eps/basura.eps}}]{Dr. Basura Fernando} is a Principal Scientist at the IHPC (Institute of High-Performance Computing) of the Agency for Science, Technology and Research (A*STAR) Singapore. He is also an NRF (National Research Foundation) Singapore Fellowship recipient and a principal investigator for CFAR (Centre for Frontier AI Research). He was an honorary lecturer at the Australian National University (ANU). Before that, he was a research fellow at the Australian Centre for Robotic Vision (ACRV), at the Australian National University. He obtained his PhD from the VISICS group of KU Leuven, Belgium, in March 2015 under the supervision of Professor Tinne Tuytelaars. He is interested in Computer Vision and Machine Learning research.

%\end{IEEEbiography}

%\vspace{11pt}

%\bf{If you will not include a photo:}\vspace{-33pt}
%\begin{IEEEbiographynophoto}{John Doe}
%Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
%\end{IEEEbiographynophoto}



\vfill

\end{document}


