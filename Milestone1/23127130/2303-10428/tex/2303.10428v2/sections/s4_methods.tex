\section{Method} 
In this section, we introduce our \textbf{Regional Prompt Tuning} (RGP-CLIP in Fig. \ref{fig:rgp_clip}), which explicitly guides the reasoning process with an observable regional  ``{\color{pigment}prompt}''. We further simplify RGP-CLIP by reusing parameters when generating regional prompt and global contextual tokens, as shown in Fig. \ref{fig:rgp_clip_s}. Finally, we replace Multi-Task Learning \cite{hessel2022abduction} with a new \textbf{Dual-Contrastive Loss} to simultaneously regress visual features towards literal description (``clue'') and hypothesis (``Inference''). We will elaborate on each section below.


\subsection{Regional Prompt Tuning (RGP)}

We propose treating pre-specified observation $\boldsymbol{r}$ as a ``prompt'' to guide the visual reasoning process directly. 
An overview of RGP-CLIP is shown in Figure \ref{fig:rgp_clip}. To prepare prompt $\boldsymbol{R}$ and contextual tokens $\boldsymbol{G}$, we pop out patch-embedding layer $\mathcal F_{proj}$ and positional encoding \texttt{PE} from the image encoder $\mathcal F_{V}$. Specifically, we resize region $\boldsymbol{r}$ and full image $\boldsymbol{i}$ into squares of fixed resolution and then patchify them with $\mathcal F_{proj}$ and \texttt{PE}. Notably, we separately initialize and optimize positional encodings (i.e., \texttt{PE}$_1$ and \texttt{PE}$_2$) for prompt $\boldsymbol{R}$ and global tokens $\boldsymbol{G}$. These $\boldsymbol{R}$ and $\boldsymbol{G}$ tokens are further concatenated and fed through the remaining attention and MLP layers (denoted by $\hat{\mathcal F}_{V}$) for visual feature generation $\boldsymbol{I}_{vis}$ (Eq. \ref{eq:r_g_enc}-\ref{eq:rgp}). 
This simple yet effective strategy consistently surpasses the CLIP baseline with colorful prompt tuning (CPT-CLIP) under different experimental settings.

\input{equation/rgp_eq.tex}

For the textual representation $\boldsymbol{I}_{text}$ and loss function, we follow the Equations \ref{eq:text_enc}-\ref{eq:clip_base_loss} in \S \ref{sec:problem}. 

\subsection{Simplified Regional Prompt (RGP$_s$)}
We further simplify creations of prompt $\boldsymbol{R}$ and global context $\boldsymbol{G}$ by sharing as many model parameters as possible (Fig. \ref{fig:rgp_clip_s}). Surprisingly, this simplification (RGP$_s$) not only reduces the process's complexity but also improves the performance.

Revisiting Equations \ref{eq:r_enc} and \ref{eq:g_enc},  the only difference in producing regional prompt $\boldsymbol{R}$ and global context $\boldsymbol{G}$ lies in differet Positional Encondings: \texttt{PE}$_1$ \textit{vs} \texttt{PE}$_2$. To unify two positional encodings, we create a \texttt{PE}$_{inter}$, which is twice the size of \texttt{PE}$_1$ or \texttt{PE}$_2$. Consequently, we could simultaneously add a positional encoding \texttt{PE}$_{inter}$ to the concatenation of regional prompt and contextual tokens (Eq. \ref{eq:ir_enc3}). To initialize $\texttt{PE}_{inter}$, we inflate $\texttt{PE}$ from pre-trained CLIP via bilinear interpolation.

For implementation, we resize region $\boldsymbol{r}$ and full image $\boldsymbol{i}$ into squares of the same size, then concatenate them vertically (or horizontally) into a \textbf{combo-image} $\boldsymbol{J}$ (Eq. \ref{eq:combo_img}). We further apply patch-embedding on the combo and add it with upsampled \texttt{PE}$_{inter}$ to get visual tokens $\hat{\boldsymbol{I}}_{vis}$ (Eq. \ref{eq:ir_enc3}). Notably, $\hat{\boldsymbol{I}}_{vis}$, generated from the combo-image, already includes both regional prompt and global contextual tokens. The $\hat{\boldsymbol{I}}_{vis}$ is further fed into multiple attentions and MLPs to get visual representation $\boldsymbol{I}_{vis}$ (Eq. \ref{eq:vis_combo}).

\input{equation/rgp_s_eq.tex}

\subsubsection{Dual-Contrastive Loss}
\label{sec:dual_contrast}
As an observation contains three modalities, such as visual $\boldsymbol{J}$, clue sentence $\boldsymbol{c}$, and inference sentence $\boldsymbol{f}$, we comprehensively study their mutual influences by deploying contrastive loss between different modalities pairs. Specifically, as shown in Figure \ref{fig:loss_all}, we deploy dual, triple, and single contrastive loss in the training phase and screen out that the \textit{Dual-Contrastive Loss works best} (Fig. \ref{fig:v2_dual_loss_fig}). We first elaborate on the Dual-Contrastive Loss and then compare it with the other counterparts.
\input{fig/loss_variants.tex}

\textbf{Dual-Contrastive Loss}: Both the clue $\boldsymbol{c}$ and inference $\boldsymbol{f}$ are positively relevant to visual $\boldsymbol{J}$. More specifically, the former is literally equivalent, while the latter is causally related to the visual hints. Although their relations are in different forms, we can still deploy a Dual-Contrastive Loss, including one for ``\textit{vision-clue}'' pair and the other for ``\textit{vision-inference}'' pair (Fig. \ref{fig:loss_dual}), to regress visual features toward two textual targets. 

The mathematical process is present in Equation \ref{eq:dc_enc}-\ref{eq:dc_comp}. The $\mathcal{F}_{T1}$ and $\mathcal{F}_{T2}$ separately denotes textual encoders for clue and inference. Notably, we could use an identical (i.e., Dual-Contrast: $\mathcal{F}_{T1}=\mathcal{F}_{T2}$) or different encoders (i.e., Dual-Contrast*: $\mathcal{F}_{T1}\neq\mathcal{F}_{T2}$) to adjust model's parameters scale.
\input{equation/dual_contrast_eq.tex}

\textbf{Other Loss Variants}. The rest loss functions include the \textbf{Triple} and \textbf{Single} contrastive loss. Particularly, compared with dual contrastive loss, the triple one newly adds the ``\textit{inference-clue}'' pair (e.g., Fig. \ref{fig:loss_trip} and Eq. \ref{eq:trip}).
\input{equation/trip_contrast_eq.tex}

We later observed that: additional \textit{inference-clue} loss in triple contrastive hurts overall performance, as the two texts (i.e., clue and inference) are not literally equivalent. For example, the clue sentence ``\textit{the road is wet}''$\neq$ inference sentence ``it has rained before''. Therefore, we can only let the two texts learn toward the same third-party feature (e.g., the visual) instead of artificially forcing them to be equivalent.


For the single contrastive loss, we have three options, namely \textit{vision-inference} (Fig. \ref{fig:loss_inf_vis}), \textit{vision-clue} (Fig. \ref{fig:loss_clue_vis}), \textit{multi-task} learning (MTL in Fig.\ref{fig:loss_mt}). Notably, we use an identical textual encoder for clue and inference during testing, since we only learn a single contrastive loss between a pair of modalities during training.

These three options can be expressed in one unified form (Eq. \ref{eq:unf}), by thresholding a random probability $p$ with different values $\overline{\mathrm{T}}$. Specifically, when $\overline{\mathrm{T}}=1.0~\mathrm{or}~0~\mathrm{or}~0.5$, the single contrastive loss degenerates into the \textit{vision-clue}, \textit{vision-inference} and \textit{multi-task} learning loss.
\input{equation/s_contrast_3.tex}

With the single contrastive loss, we find that only minimizing the gap between a pair, such as \textit{vision-clue} (or \textit{vision-inference}) will also shorten the gap between the other pair \textit{vision-inference} (or \textit{vision-clue}), indicating retrieval and abductive reasoning tasks have positive rational correlations. We give detailed analysis in \S \ref{sec:ab_study}.

