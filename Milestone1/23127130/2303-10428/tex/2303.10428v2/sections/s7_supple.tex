
This Appendix presents qualitative visualization of RGP$_s$ and baseline, additional ablation studies, and PyTorch-style algorithm implementation.

\section*{A. Qualitative Results of RGP$_s$ and CPT-Baseline}
\input{fig/one_example}

We visualize qualitative results by our best-performed RGP$_s$ and the previous SOTA CPT-baseline \cite{hessel2022abduction} on the Sherlock validation set. Figure \ref{fig:one_example} shows one good case. We highlight regional visual hints with a translucent pink overlay and present the ground-truth clue \& inference in a yellow rectangle for viewing convenience. Our method successfully retrieves the GT-inference ``\textit{The pic was taken in Japan or UK}'' in the top-five retrieved candidates. More examples are given in Figure \ref{fig:example_vis1} and \ref{fig:example_vis2}.

\section*{B. Additional Ablations on Different Resolutions and Weighting in Loss}
\textbf{Influences of Input Resolutions.} We conduct an experiment studying ``\textit{How many tokens are needed for regional prompt and contextual tokens?}''. Denoting the length of regional prompt/contextual tokens as $L_R$/$L_G$, this length is proportional to the resolution of the region-image/full image, i.e., $L_{R~\text{or}~ G}=\frac{H\cdot W}{P^2}$. Here, [$H$, $W$], $P$ separately represent input resolution and patch-size. To study the influence of using fewer regional prompt tokens, we gradually reduce the resolution of the region-image in the range $R=[224^2, 112^2, 48^2, 16^2]$, while fixing the resolution of the full image to be $C=224^2$. We also conduct the same study for the contextual tokens ($C=[224^2, 112^2, 48^2, 16^2]$, while fixing $R=224^2$).

\input{table/different_size}
\input{fig/res_comp}

We test different resolutions' combinations on the Sherlock Val-Set with our \textit{general RGP} model using CLIP ViT-B-16 backbone. As shown in Table \ref{tab:res_rc}, we find that: (1). Reducing input resolutions results in shorter tokens' length ($L_R$ or $L_G$) and then causes most metrics to become worse than before: \textit{\textbf{the smaller resolution (shorter length), the worse performance}}; (2). \textit{\textbf{The regional prompt plays a more important role than contextual tokens}}, as reducing the length of the former causes more performance drop than the latter. For a more intuitive display, we draw two curves of $P@1_{img\rightarrow text}$ with respect to the resolution $HW$=$X^2$ in Figure \ref{fig:res_comp}: \{$R=X^2$, $C=224^2$\} \textit{vs} \{$C=X^2$, $R=224^2$\}. The $P@1_{i\rightarrow t}$ drops significantly when the $R$ (resolution of the region) reduces.

\textbf{Weighting in Loss.} Since the Dual-Contrastive Loss contains two parts: ``\textit{vision-clue}''  and ``\textit{vision-inference}'' losses, we further study the balance of two parts by introducing two weights (i.e., $\alpha$, $\beta$) into the loss function (see Eq. \ref{eq:dc_comp_weight}).
\input{equation/dual_contrast_eq_weight}
\input{table/different_weight}
\input{fig/weight_loss}
We test different weights' combinations $\{\alpha, \beta\}$ on the Sherlock Val-Set with our \textit{Simplified RGP$_s$ + Dual-Contrastive Loss} using CLIP ViT-B-16 backbone. The results are shown in Table \ref{tab:weights_loss}. We find that: (1). The best performance is achieved by setting $\{\alpha=0.3, \beta=0.7\}$, which slightly bias to ``\textit{vision-inference}'' loss under the Dual-Contrastive Loss scheme. (2). Most evaluation metrics arrive at saturated states when the $\alpha \geq 0.5$, indicating our standard Dual-Contrastive Loss (Eq. \ref{eq:dc_comp}), that treats each part equally, is also an optimal choice. This trend is reflected by drawing the curves of  \textit{Retrieval} metrics with respect to the weight scalar $\alpha$ (see Figure \ref{fig:weight_loss}).

\section*{C. Algorithm of RGP$_s$ and Dual-Contrastive Loss}

We present the Algorithm of RGP$_s$ and the Dual-Contrastive Loss in PyTorch-style as below. 
\begin{algorithm}[]
\small
\caption{\small Codes for RGP$_s$ and Dual-Contrastive Loss (PyTorch-like)}
\vspace{-0.05in}
\begin{PythonA}[frame=none]
# Inputs:
#   img: input tensor of shape (H, W, C)
#   bbox: Coordinates in format [x1, y1, w, h]
#   clue: literal sentence
#   inference: hypthesis sentence
# Model Parameters:
#   Image_Enc: Init from CLIP Image Encoder
#   Text_Enc1: Init from CLIP Text Encoder
#   Text_Enc2: Init from CLIP Text Encoder

# 1. Upsampling Pos-Enc to handle (2H, W) input
Image_Enc = Resize_Pos_Embed(Image_Enc)

# 2. Create Combo Image (H, W, C)->(2H, W, C)
#  Vertically concatenate Image and Box-Image.
bbox_img  = Crop_Resize(img, bbox) # (H, W, C) 
combo_img = Vertical_Concatenate(bbox_img, img)

# 3. Dual-Contrastive Loss
I_vis  = Image_Enc(combo_img)
I_inf  = Text_Enc1(inference)
I_clue = Text_Enc2(clue)
loss_1 = Contrastive_Loss(I_vis, I_inf)
loss_2 = Contrastive_Loss(I_vis, I_clue)
total_loss = loss_1 + loss_2
total_loss.backward()

\end{PythonA}
\label{algo}
\vspace{-0.05in}
\end{algorithm}

\input{fig/example_fig}
\input{fig/example_fig2}