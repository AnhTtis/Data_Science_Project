\section{Experiments}
We comprehensively study the Regional Prompt Tuning, and Dual-Contrastive Loss on the Sherlock benchmark \cite{hessel2022abduction} and present experimental details below. 

%We report the standard retrieval, localization, and comparison metrics that the benchmark defines. We will present experimental details below.

\subsection{Datasets}
The \textbf{Sherlock} dataset \cite{hessel2022abduction} contains 103K images collected from the Visual Genome \cite{krishnavisualgenome} and Visual Common Sense Reasoning \cite{zellers2019vcr} datasets. These images are split into 90K training, 6.6K validation, and 6.6K testing sets. Each image is re-annotated with an average of 3.5 observation-inference pairs, forming 363K samples. Particularly, a sample includes a bounding box $\boldsymbol{r}$ and two texts (i.e., clue $\boldsymbol{c}$ + inference $\boldsymbol{f}$). Notably, the validation set can be evaluated offline with officially provided scripts, while the testing set needs to be assessed online with the leaderboard. %\footnote{\url{https://leaderboard.allenai.org/sherlock/submissions/public}}.

Three types of evaluation metrics, from \textbf{retrieval}, \textbf{localization}, and \textbf{comparision} aspects, are adopted for the Sherlock benchmark. Specifically, retrieval metrics, such as $img\leftrightarrows text$ mean rank, P@1$_{i\rightarrow t}$, are used to measure correspondence between visual fact and inference text. For localization, accuracies of grounding candidate regions to the inferences are adopted. The candidates can be collected from ground-truth or auto-detected boxes. Comparison metric calculates the accordance between machine and human predictions.



%Three types of \textbf{Evaluation Metrics}, separately reflecting visual-text \textbf{retrieval}, \textbf{localization} accuracy of the observable region, and \textbf{comparison} of performance between machine and human beings, are presented together with the benchmark. Specifically, off-the-shelf vision-language retrieval metrics, such as $img\rightarrow text$ mean rank, $text\rightarrow img$ mean rank, P@1$_{i\rightarrow t}$, are adoptted to measure the correspondence between visual contents and inference text. 

%363K regions
%81K Visual Genoeme, 22K Visual Common Sense Reasoning,
%103K images, train/val/test = 90K/6.6K/6.6K
%traing, validation and testing set
\subsection{Implementations}
We develop the Regional Prompt Tuning, including RGP, RGP$_s$, and Dual Contrastive Loss on top of the OpenCLIP \cite{radford2021learning, cherti2022reproducible} PyTorch toolkit \footnote{\url{https://github.com/mlfoundations/open_clip}}, and fix the training \& testing recipe to be the same for all ablations 
unless otherwise stated.

\textbf{Training}. For preparing the regional prompt, we extract and resize region $\boldsymbol{r}$ into 224$\times$224 (337 for high resolution) square image, while we are also resizing the full image $\boldsymbol{i}$ into the same-sized square for preparing contextual tokens. We initialize all models from the CLIP pre-trained weight and fully fine-tune them for 10 epochs with a cosine learning rate schedule. The init lr=10e-5. Thanks to mixed-precision and gradient-checkpointing \cite{chen2016training}, we train with 4$\times$43 GB A40 GPUs and set the global batch size to be 2560 / 256 for ViT-B-16 or ViT-L-14-338 CLIP backbones.

\textbf{Testing}. We apply the same preprocess for region $\boldsymbol{r}$ and full image $\boldsymbol{i}$ to prepare regional prompt and contextual token as the training phase. Given a set of visuals $\{\boldsymbol{r}$, $\boldsymbol{i}\}\times K$ and inferences $\{f\}\times K$, we first calculate the $K\times K$ matrix of \textit{vision-inference} similarity and report retrieval, localization and comparison metrics based on the matrix.

\subsection{Ablation Study}
\label{sec:ab_study}
In this section, we conduct experiments comparing Region Prompt and Colorful Prompt Tuning (baseline), studying independent effects and integrations of parts (e.g., region, context), and comparing different contrastive losses, on the Sherlock validation set with the CLIP ViT-B-16 backbone. To be consistent with the baseline method, we adopt the default Multi-Task Learning Loss \cite{hessel2022abduction} in all ablations except for the subsection comparing different losses.

\textbf{Regional Prompt Tuning} \textit{vs} \textbf{Baseline (CLIP-CPT)}. We compare Regional Prompt (RGP), and its simplified version (RGP$_s$) with the Sherlock baseline \cite{hessel2022abduction}. For a fair comparison, we reproduce the baseline under the same training recipe as the RGP series. 

\input{table/ablation_tokens_amount.tex}

We draw three takeaways from Table \ref{tab:rgp_vs_baseline}: (1) Regional Prompt Tuning, regardless of RGP or RGP$_s$, significantly outperforms the baseline using colorful prompt tuning under all evaluation metrics. (2). The gain of performance does not come from a longer token sequence for the CLIP Transformer but from the fine-grained details in the region prompt, as we also extend the baseline in a double token length manner (i.e., CPT-CLIP ($\times2$ \texttt{Tokens} in Fig \ref{fig:clip_base_extend})) and observe a non-significant performance difference. The doubling is achieved by replacing region/context inputs in Figure \ref{fig:rgp_clip} with left/right crops. (3). Simplification of regional prompt tuning introduces further improvement (RGP$_s$ $>$ RGP) under most metrics. This implies that applying a uniform positional encoding (interpolated) for the combo of regional prompts and contextual tokens is better than independent encodings.


%RGP and RGP$_s$ significantly outperform the baseline for all retrieval, localization, and comparison metrics. Moreover, to exclude the factor that gain of performance comes from a longer token sequence introduced by extra prompt tokens, we double the length of tokens for baseline, by replacing region/context inputs in Figure \ref{fig:rgp_clip} with left/right crops, forming the CLIP-CPT ($\times2$ \texttt{Tokens}). Results show this simple extension achieves similar performance as the original baseline.


%to exclude the f
%an explantation that the improvment is caused by a longer token length caused by extra regional prompt tokens, 

%1. Region Prompt Significantly outperforms baseline
%2. We use colorful input replace R and I, make the attention process the same amount of tokens, but get similar performance as the original baseline.
%3. RGPs simplies process , and brings further improvment, this can be credit to that \textbf{the distance between prompt and contextual tokens are more accurate by an identical interpolated postional encoding, than indepently apply positional encodings on prompt and contextual tokens}. 
%the reason lies in that prompt tokens and contextual tokens are unified encoded by an identical amplfied interpolated positional encoding, 

%We implement both regular Region Prompt Turning and Simplified Region Prompt Tuning and compare them with our re-produced Sherlock baseline (CLIP-CPT). 
%As 
%reproduce the Sherlock baseline (i.e., \S \ref{}) and 
%Colorful Prompt Tuning
%x2 tokens

\textbf{Impacts of Each Part}. We study the contributions of each individual part (i.e., regional and contextual tokens in Fig \ref{fig:rgp_clip_s}) by keeping only a part at a time during training and testing. We also conduct a plain \textit{context + region} setting, which independently processes regional and contextual tokens (imitate process in Figure \ref{fig:clip_base}) and sums up features of the two parts.

\input{table/ablation_part.tex}

As shown in Table \ref{tab:impacts_of_parts}, both the RGP$_s$ and plain \textit{context+region} outperforms the model using a single part, indicating that the region prompt and context are complementary. The RGP$_s$ further surpasses the plain addition (e.g., P@1$_{i\rightarrow t}$: \textbf{32.55} \textit{vs} 31.72). Moreover, the \textit{region only} outperforms the \textit{context only} setting by a large margin (e.g., P@1$_{i\rightarrow t}$: \textbf{28.72} \textit{vs} 17.87), further validating that fine-grained details in region prompt is critical for precise reasoning. Notably, localization metrics are not reported for \textit{context only}, as the model only sees full images without regional hints and thus can not do the regional matching.

%using only region prompt or context performs worse than the original RGP$_s$. 
%reduces overall performances compared with the RGP$_s$ 



\input{fig/wrap_combo}
\textbf{Combinations~of~Region~and~Context~(Combo)}. For the RGP$_s$ model, we test the influence of using different concatenations in generating the combo-image $\boldsymbol{J}$ (Eq. \ref{eq:combo_img}). Specifically, we could concatenate region $\boldsymbol{r}$ and context $\boldsymbol{i}$ images either along the \textit{vertical} or \textit{horizontal} axis (Fig. \ref{fig:combo}). This would affect the distances between token pairs. For example, in a vertical combo, a prompt token ({\color{purple}purple}) is spatially nearer to prompt tokens than contextual tokens ({\color{pigment}dark blue}) due to spatial flattening, whereas this is not true in the horizontal combo situation.

\input{table/ablation_combo_w_or_h.tex}

Table \ref{tab:vertcial_horizontal} lists the reasoning performances with vertical and horizontal combo inputs. The vertical combo is better in most metrics than its horizontal counterpart, indicating that keeping independent spatial relations within prompt and contextual tokens is beneficial.


%Concatenating region and context (full) image along \textit{vertical} axis is better than along horizontal axis, because of the order of latter tokens (after pachification) would be messed up when flatten tokens from 2D spatial grid to 1D seqeunce. 

\textbf{Dual-Contrastive Loss} \textit{vs} \textbf{Single/Triple counterparts}. We test different types of contrastive losses using our RGP$_s$ model.  %Since in the setting of single contrastive loss, text encoder $\mathcal F_{T}$ is shared between the clue $\boldsymbol{c}$ and inference $\boldsymbol{f}$, we keep this setting for the Dual/Triple contrastive loss learning ($\mathcal{F}_{T1}$=$\mathcal{F}_{T2}$ in Eq. \ref{eq:dc_comp}) for fairness. 

\input{table/dual_contrast.tex}

In Table \ref{tab:comp_loss}, the Dual-Contrastive loss performs better than the Multi-Task Learning and the other single \& triple counterparts under most metrics. In terms of localization (GT-Box: Dual 86.52 \textit{vs} Triple 86.70) and comparison (Human Accordance: Dual 25.06 \textit{vs} Infer-Image 25.56),  the Dual-Contrastive loss is slightly lower than its counterparts but still shows a very competing performance.
\input{table/sota_table.tex}


%Before and after the training phase, 
We further look into the individual contrastive loss value between modality pairs on the validation set to understand how modalities mutually influence each other. Specifically, we first report the loss value between each pair before the training phase (i.e., No Train or Zero-Shot reasoning), then re-calculate them after the model is trained with different losses (See Fig \ref{fig:dist_all}).

\input{fig/pair_dist_loss}

We observe that the gaps of \textit{vision-clue} and \textit{vision-inference} are positively correlated. Specifically, when we minimize one of the gaps in training, the other one will also become smaller (e.g., Fig. \ref{fig:dist_p3_inf_only}-\ref{fig:dist_p2_clue_only}). Whereas, the gap of \textit{inference-clue} seems not to correlate to gaps of \textit{vision-clue} and \textit{-inference}, as the former becomes even larger after minimizing either of the latter gaps (e.g., {\color{red}red} value in Fig \ref{fig:dist_p3_inf_only}-\ref{fig:dist_p4_mkl}, and \ref{fig:dist_p6_dual}). If we enforce the model to close the \textit{inference-clue} gap during training, the \textit{vision-clue} and \textit{-inference} gap would become larger (Triple \textit{vs} Dual, Fig. \ref{fig:dist_p5_trip} \textit{vs} \ref{fig:dist_p6_dual}). The reason is that the clue and inference sentences are not literally equivalent and better to be bridged by an extra rational process.


%We further look into how the gap between modalities changes before and after the model is trained to provide insights into different losses. Specifically, we first calculated the contrastive loss values between each pair before training (i.e., zero-shot reasoning), then compared them with the values after training. The changes in values according to different losses are shown in Figure \ref{}.

\input{table/dual_contrast_adv_merge.tex}

\textbf{Identical} \textit{vs} \textbf{Separate} text encoders for clue and inference. As mentioned in \S  \ref{sec:dual_contrast}, we test the impacts of using one identical or two separate text encoders for the clue and inference input. We adopt Dual-Contrastive Loss learning and two backbones, including CLIP ViT-B-16 and ViT-L-14-336.


Table \ref{tab:separate_vitb} shows the results. We observe that with the ViT-B-16, the absolute improvements brought by separated text encoders are larger (e.g., $img\leftrightarrows text$ 15.78/17.88 \textit{vs} 16.05/18.19), whereas, with the ViT-L-14-336, the improvements become small (e.g., $img\leftrightarrows text$ 11.43/13.07 \textit{vs} 11.44/13.03). A possible reason is that with the increased complexity of the CLIP backbone, the text encoders also become deeper and could handle both clue and inference feature learning. 

\subsection{Comparision with the State-of-the-Art}
We compare Regional Prompt Tuning (RGP and RGP$_s$) with the current state-of-the-art on the Sherlock test set. All results are submitted, evaluated, and published on the official leaderboards. Notably, we didn't test RGP$_s$ with CLIP ResNet50$\times$64 backbone, as we could not set an adequate batch size for contrastive learning.

As shown in Table \ref{tab:sota}, our best-performed RGP$_s$ \textbf{ranks the \nth{1}} on the Sherlock Leaderboard under all evaluation metrics, surpassing CPT-CLIP (ViT-L-14-336, ResNet50$\times$64, and ViT-B-16), UNITER and LXMERT by a large margin (e.g., {P@1$_{i\rightarrow t}$: \textbf{38.78} \textit{vs} 36.85 \textit{vs} 33.44 \textit{vs} 30.56 \textit{vs} 19.80 \textit{vs} 14.90). We observe that frameworks on top of foundation models, including RGP/RGP$_s$ and CPT-CLIP, perform much better than conventional models (UNITER and LXMERT), even though the latter two are also pre-trained on up-stream datasets such as Visual Genome \cite{krishnavisualgenome}, MSCOCO \cite{lin2014microsoft}, Conceptual Captions \cite{sharma2018conceptual} and etc. This indicates that the foundation models are beneficial for abductive reasoning. Besides, we also validate by RGP/RGP$_s$ that the fine-grained regional evidence fits well as a prompt for visual abductive reasoning tasks.

Particularly, compared to the CPT-CLIP baseline, we observe the same phenomenon on the testing set as our internal evaluation on the validation set in \S \ref{sec:ab_study} Ablation Section, indicating the improvements of the new methods are generalizable across different testing data and is robust to different backbones (e.g., ViT-B-16 and ViT-L-14-336). In short, regional prompt tuning performs better than colorful prompt tuning; The simplified RGP$_s$ and Dual-Contrastive Loss robustly improve the original RGP. 

