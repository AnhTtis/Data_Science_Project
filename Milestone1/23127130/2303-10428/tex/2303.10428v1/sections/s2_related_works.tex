\section{Related Works}
Our Regional Prompt Tuning is relevant to research areas, including Foundation Models, Abductive Reasoning Tasks, Prompt Tuning, and Fine-Grained Visual Representation. We will separately illustrate related works according to the areas below.

\textbf{Foundation Models.} ``\textit{More is different}'' \cite{anderson1972more}. Scaling up models' complexities and training data scales significantly improves the foundation models' \cite{brown2020language, devlin2018bert, radford2021learning, jia2021scaling, yu2022coca, yuan2021florence} perception capacity, making it proficient in humanoid tasks such as zero or few-shot learning. Specifically, Large Language Models (LLM), such as BERT \cite{devlin2018bert}, and GPT \cite{brown2020language} pre-trained on a large-scale corpus, are generalizable to downstream NLP tasks with few training examples. This trend quickly swept to the Vision-Language areas, with representative works such as CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling}, BLIP \cite{li2022blip} etc. The main idea behind the VL foundation models is to learn transferable visual representation with noisy text supervision through a two-tower structure. We follow the current VAR pipeline and adopt the foundation model CLIP to generate initial visual \& textual features. 


\textbf{Abductive Reasoning Tasks.} Humans reckon plausible inferences or hypotheses from incomplete observations every day \cite{cohen1933collected}. To teach the machine with this ability, researchers proposed several new Tasks, like \dataset{} \cite{bhagavatula2020abductive} for NLP, \textit{Sherlock} \cite{hessel2022abduction} for vision, and \textit{VideoVAR} \cite{liang2022visual}, \textit{VideoABC} \cite{zhao2022videoabc} for video. Specifically, the \dataset{} \cite{bhagavatula2020abductive} generates the most likely hypothesis (text) to explain what has happened between the two observations (texts). For Sherlock, VideoVAR, and VideoABC, the observations are represented by regional or whole images, while inference could be either text or middle frames. There are several similar tasks, such as Visual Commonsense Reasoning (VCR) \cite{zellers2019vcr} and Visual7W \cite{zhu2016visual7w}. Abductive reasoning differs from them in having non-definitely correct, plausible inferences as human beings.

\textbf{Prompt Tuning} comes from adapting the LLM to downstream NLP tasks by adding a few ``virtual tokens'' as the prefix or suffix to original sentences. According to whether using trainable continuous or handcrafted discrete ``virtual tokens'', we could classify prompts into soft \cite{lester2021power,li2021prefix, qin2021learning} or hard \cite{jiang2020can, brown2020language, liu2021gpt} prompts. The rise of VL foundation models further extends prompt tuning for visual downstream tasks. These techniques can be divided into textual or visual prompt tuning according to whether to place prompt tokens in visual or textual encoders. Specifically, the CoOp/CoCoOp \cite{zhou2022coop,zhou2022cocoop} and PromptDet/DetPro \cite{feng2022promptdet,du2022learning} padded extra learnable textual prompts along with the textual category for zero/few-shot classification and open-vocabulary object detection tasks. For the visual prompt tuning, VPT \cite{jia2022visual} proposes to prepend trainable tokens with visual tokens, while the VP \cite{bahng2022exploring} moves the prompt to pixel space by learning learnable paddings that surround the image. For segmentation tasks, the OVSeg \cite{liang2022open} replaces tokens of the non-mask area with learnable visual prompts. The Merlot \cite{zellersluhessel2021merlot} and concurrent CPT \cite{yao2021cpt} propose hard visual prompts that cover core regions with translucent color overlays to guide the model to focus on these specific areas. Our regional prompt tuning belongs to the hard visual prompt tuning as the Merlot/CPT. The RGP explicitly enlarges regional hints to generate fine-grained visual prompts, which have not been studied before.

\textbf{Fine-Grained Visual Representation.} Our work is also relevant to learning fine-grained visual representation \cite{zhong2022regionclip, zhang2018fine, zhang2022glipv2, wang2022internimage, aberdam2023clipter}. Specifically, GLIP \cite{zhang2022glipv2} and RegionCLIP \cite{zhong2022regionclip} pre-train foundation models for object detection, supervised by region-text pairs. The former and latter mimic the process of R-CNN\cite{girshick2014rich} and Faster-RCNN \cite{ren2015faster}, generating an object's vector by either encoding the cropped image or RoI pooling. Similarly, UNITER \cite{chen2020uniter} and LXMERT \cite{tan2019lxmert} also rely on RoI pooling to generate regional vectors for vanilla VL tasks. Besides, the InternImage \cite{wang2022internimage} learns the foundation model with Deformable-CNN and achieves a new SOTA for object detection. Other works, such as Object-VLAD \cite{zhang2018fine} for event detection, and CLIPTER \cite{aberdam2023clipter} for scene text recognition, also studied fine-grained modeling. Specifically, the Object-VLAD aggregates densely collected local features with VLAD for generating video representation. The CLIPTER introduces extra cross-attention and the gated fusion module to combine local and global features. In contrast, RGP doesn't tweak the CLIP structure and only modifies the input space.

