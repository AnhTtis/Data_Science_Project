\section{Introduction}
\input{fig/overview_step.tex}

``\textit{Can machines think?}'' Dr. Alan Turing proposed this question and gave the famous thought test, namely the ``\textbf{Imitation Game}'', to help find an answer  \cite{turing2009computing}. This game defines that we could consider ``a machine thinks'' if it fully imitates humans' feedback and tricks us (humans) into believing its replies. Recent progress of foundation models, including GPT \cite{brown2020language}, and CLIP \cite{radford2021learning}, shows that machines could imitate humans better than before, showing evolving thinking and reasoning abilities.

To promote machines' reasoning ability, researchers devoted more and more efforts to reckon various modalities \cite{bhagavatula2019abductive,hessel2022abduction,liang2022visual,zhao2022videoabc}. This paper studies one of these emerging tasks named \textbf{Visual Abductive Reasoning} (VAR) \cite{hessel2022abduction}. Specifically, the VAR aims to reveal a hidden inference or hypothesis from visual observations based on prior knowledge. As shown in Figure \ref{fig:fine_grained}, the machine is expected to draw an inference ``\textit{This is a formal area}'' from regional visual hints (``\textit{fancy vase}'') and surrounding contexts ( ``\textit{neat marble floor}'' + ``\textit{elegant wallpaper}''), just as humans do. 

However, the VAR task is more challenging than vanilla vision-language (VL) tasks (e.g., retrieval and captioning). The reasons lie in two aspects. Firstly, for the VAR, inferences are usually not directly visible in pictures (e.g., ``\textit{formal area}'' $\neq$ ``\textit{vase}'', ``\textit{marble floor}'' or ``\textit{wallpaper}''), whereas, for vanilla VL tasks, entities in captions appear in images (e.g., ``{\color{black}\textit{vase}}'' in clue sentence ``\textit{A fancy vase on the floor}''). Secondly, inferences have causal relations with the local visual hints. The presence or absence of evidential hints will result in different inferences (it's hard to draw the inference ``\textit{formal area}'' without seeing solemn decoration). 

The typical VAR pipeline contains two steps to reckon an inference from regional hints: Perception and Reasoning Tuning. The former step relies on pre-trained foundation models (e.g., CLIP) to provide prior knowledge and commonsense, transforming visual observations to semantical representation. The latter turns to specific prompt tuning to refine the model to map the transformed representation to the inference. Specifically, Colorful Prompt Tuning (CPT in Fig. \ref{fig:fine_grained} is adopted to preprocess images by covering the evidential visual part with a translucent {\color{pink}pink} overlay. The altered images are then fed into the CLIP model to minimize the ``\textit{vision}-\textit{inference}'' gap. 


Although effective, the pipeline has two limitations. 
First, CPT does not effectively extract fine details from the regional evidence.
As shown in Figure \ref{fig:fine_grained}, the CPT uniformly patchify regional hints and global contexts at the same granularity. Even though the regional part is smaller than the global context in size, it has a more significant influence on the hypothesis. Thereby, uniformly patchifying on regional hints and global context may lead to losing fine-grained details in the regional evidence. 
The second is ``\textit{How to utilize Triangular Relationships between Vision (visual evidence), Inference (hypothesis), and Clue (textual clue) for the VAR}''. We argue that there is a weak causal relationship between ``\textit{vision-inference}'' and a semantical relationship between ``\textit{vision-clue}'' modalities. 
Current methods do not attempt to bridge these relations in a symmetric manner.
We propose explicitly finding a correlation law among the three modalities and utilizing it to address the VAR task effectively.

To tackle the two limitations, we introduce a new paradigm of \textbf{Regional Prompt Tuning} (RGP/RGP$_s$) and \textbf{Dual-Contrastive Loss} learning to the current VAR pipeline. Specifically, the RGP separately patchify regional hints and global context at two granularity levels (see Fig \ref{fig:fine_grained}). To alleviate ``loss in details'', we pluck more tokens from regional hints using a smaller patch size than before. We also sample surrounding tokens with the original patch size since in-context information is necessary for inference reasoning. For implementation, We do not tweak the patch size in the embedding layer to reuse all the parameters in the CLIP. Alternatively, we turn to enlarge the area of the regional hint and then patchify it with the original embedding layer. For the second concern, we experimentally study different combinations of three contrastive losses (see Fig \ref{fig:loss_all}) and validate that the Dual-Contrasitive Loss, involving the loss of vanilla visual retrieval, benefits reasoning results. This also indicates that the VAR and visual retrieval tasks are positively correlated.

Experiments on the Sherlock VAR benchmark show that our model consistently surpasses previous SOTAs (Fig. \ref{fig:improve}), ranking the \nth{1} on the leaderboard among all submissions \footnote{\url{https://leaderboard.allenai.org/sherlock/submissions/public}}. Our contributions are summarized below.
\begin{itemize}
    \item \textbf{Regional Prompt Tuning.} We proposed this new prompt tuning that specially encodes regional hints at a fine-grained level on top of the CLIP foundation model. We verify that amplifying local evidence is beneficial for visual reasoning.
    \item \textbf{Dual-Contrastive Loss.} We connect the Visual Abductive Reasoning and vanilla visual retrieval task with the Dual-Contrastive Loss. Although the two tasks have different ``\textit{Vision-Text}'' relationships (causal \textit{vs} semantical), they are positively correlated and can be jointly fine-tuned.
    \item \textbf{New State-of-the art for VAR.} We test RGP with Dual-Contrastive Loss on the standard \textit{Sherlock} benchmark. Experiments show that our model robustly surpasses previous SOTAs, achieving a new record (e.g., P@1$_{i\rightarrow t}$: \textbf{RGP$_s$ 38.78} \textit{vs} CPT-CLIP 33.44).

\end{itemize}