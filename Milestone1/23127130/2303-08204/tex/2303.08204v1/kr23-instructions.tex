%%%% kr-instructions.tex -- version 1.3 (11-Jan-2021)

\typeout{KR2023 Instructions for Authors}

% These are the instructions for authors for KR-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{kr}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[export]{adjustbox}
\urlstyle{same}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\let\oldReturn\Return
\renewcommand{\Return}{\State\oldReturn}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

\usepackage{stfloats}

\usepackage{array}
\usepackage{multirow}

\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.
%PDF Info Is REQUIRED.
\pdfinfo{
/TemplateVersion (KR.2022.0, KR.2023.0)
}



% \title{A sub-symbolic skill to perform symbolic anchoring from perceptual inside a cognitive architecture}
\title{SAILOR: Perceptual Anchoring For Robotic Cognitive Architectures}

% Single author syntax
\iffalse % (remove the multiple-author syntax below and \iffalse ... \fi here)
\author{%
    Author name
    \affiliations
    Affiliation
    \emails
    email@example.com    % email
}
\fi
% Multiple author syntax

% % % % % % % Blinded % % % % % % % 
\author{%
Miguel Á. González-Santamarta$^1$\and
Francisco J. Rodríguez-Lera$^1$\and
Vicente Matellán-Olivera$^1$\\
\affiliations
$^1$Robotics Group, University of León\\
\emails
\{mgons, fjrodl, vmato\}@unileon.es
}
% % % % % % % Blinded % % % % % % % 
\begin{document}

\maketitle

\begin{abstract}
% Maintaining knowledge about the physical world to generate proper robot behaviors is still an open problem. 
%The use of cognitive architectures for managing this scenario is a cornerstone. 

%In a cognitive-based robot, employing perceptual information to obtain symbolic knowledge by processing sub-symbolic data from real-world sensors is still an open problem. This paper presents SAILOR, a framework for providing symbolic anchoring in ROS 2 scenarios. SAILOR aims to maintain the link between symbolic data and perceptual data in real robots over time. It provides a semantic world modeling approach based on a bottom-up anchoring solution supporting two sub-symbolic skills: object recognition and re-acquire function. 

Symbolic anchoring is a crucial problem in the field of robotics, as it enables robots to obtain symbolic knowledge from the perceptual information acquired through their sensors. In cognitive-based robots, this process of processing sub-symbolic data from real-world sensors to obtain symbolic knowledge is still an open problem. To address this issue, this paper presents SAILOR, a framework for providing symbolic anchoring in ROS 2 ecosystem. SAILOR aims to maintain the link between symbolic data and perceptual data in real robots over time. It provides a semantic world modeling approach using two deep learning-based sub-symbolic robotic skills: object recognition and matching function. The object recognition skill allows the robot to recognize and identify objects in its environment, while the matching function enables the robot to decide if new perceptual data corresponds to existing symbolic data. This paper provides a description of the framework, the pipeline and development as well as its integration in MERLIN2, a hybrid cognitive architecture fully functional in robots running ROS 2.


% The framework uses bottom-up anchoring to establish and maintain the link between symbolic and perceptual data.

\end{abstract}

\section{Introduction}

The use of cognitive architectures~\cite{ye2018survey,kotseruba202040,kotseruba2016review} as a mechanism for generating robot behaviors is broadly accepted. There are three types of cognitive architectures: symbolic, emergent and hybrid. Although there is no well-defined degree of hybridization between symbolic and emergent concepts, usually they work with symbolic information, where the STRIPS provides the semantics and the PDDL the syntax~\cite{fox2003pddl2}.

% Reviews of cognitive architectures (\cite{ye2018survey}, \cite{kotseruba202040}, \cite{kotseruba2016review}) present three types of cognitive architectures: symbolic, emergent and hybrid. 
% Hybrid cognitive architectures are the most common type and combine elements of both symbolic and emergent approaches. 
% There is no well-defined degree of hybridization between symbolic and emergent concepts. However, hybrid architectures usually have a symbolic core, that is a knowledge base and a planner, and several sub-symbolic skills, such as perception, navigation or manipulation.


% In hybrid cognitive architectures (\cite{gonzalez2020merlin}, \cite{martin2020evolution}, \cite{benjamin2004adapt}, \cite{nunez2016towards}), internal representation corresponds with symbolic knowledge. 


PDDL is highly accepted by the current ROS developer community. There are several PDDL-based tools used in practical cognitive architectures, however, two of them are well known: ROSPlan~\cite{cashmore2015rosplan} and PlanSys2~\cite{martin2021plansys2}. Both apply PDDL to represent the real world of the robot by creating the objects and the attributes of the world. Afterward, it is used by planners such as POPF \cite{popf} to generate plans to achieve the goals of the robots. 

% , \cite{cashmore2016compilation}, \cite{younes2003vhpop}) 

How the world model served the architecture and how is updated over time is the problem here. 
Obtaining PDDL knowledge from raw perceptual data is not straightforward. 
Thus, perception is the process of converting raw sensory data into cognitive architectures' internal representation, particularly symbolic knowledge and  anchoring~\cite{coradeschi2003introduction} is the process of creating and maintaining the correspondence between symbols and sensor data that refer to the same physical objects.
Not only knowledge creation is needed, but also knowledge maintenance. This is an aspect of the Symbolic Knowledge Grounding~\cite{harnad1990symbol} that is the problem of how to ground the meanings of symbols used by the robot. 
The process of grounding symbols to real-world objects by a physical agent interacting in the real is known as Physical Symbol Grounding~\cite{vogt2002physical}.

% The knowledge maintenance process is related to symbolic anchoring~\cite{coradeschi2003introduction}, which is the process of creating and maintaining the correspondence between symbols and sensor data that refer to the same physical objects.

This paper presents SAILOR (Symbolic AnchorIng from perceptuaL for rOs2-based Robots). It is intended to be used inside a hybrid cognitive architecture with the aim of creating and maintaining the real-world knowledge that the robot can use. SAILOR is based on two sub-symbolic skills: one for object recognition and the other one to decide if new percepts correspond with the previous knowledge.

SAILOR can be integrated into a ROS 2-based hybrid cognitive architecture improving the learning capability of the robot due to the fact that the robot can learn new symbolic knowledge. 
As a result, knowledge about the real world is obtained in real time as the robot interacts with the world rather than only using innate knowledge manually created before running the cognitive architectures. Besides, this knowledge can be obtained from the effects of well-known actions, such as navigation, whose final state can be inferred using the planner. 


\subsection{Contributions and article overview}

The main contribution of this paper consists of a framework to perform perceptual anchoring in ROS 2. In more detail, this research provides the following contributions:

\begin{enumerate}
    \item An updated anchoring pipeline based on state-of-the-art works. 
    %Literatura PCL+clasificación
    %Nosotros dectection+PCL (extración de características)
    
    
    \item A matching function based on deep learning for an anchoring system.
    
    \item Evaluation of state-of-the-art indoor and outdoor datasets for anchoring. 
% Generalization of datasets and models for anchoring based on state-of-the-art indoor and outdoor datasets.
    % \item Development of ROS 2 components for its integration in a robotic platform. 
     \item Implementation and validation of SAILOR solution inside a cognitive architecture for ROS 2.
    % \item A formal  solution is open source and replicable.
\end{enumerate}

The remainder of this paper is organized as follows. Section \ref{sec:related_works} presents the background of the paper, focusing on the anchoring problem~\cite{coradeschi2003introduction} and solutions. Section \ref{sec:materials_methods} explains the solution of this paper. Section \ref{sec:evaluation} presents the evaluation process carried out. Section \ref{sec:conclusions} summarizes the conclusions obtained.


\section{Related Works} % https://www.sciencedirect.com/science/article/pii/S0004370221001880
\label{sec:related_works}


Anchoring is the process of creating and maintaining the link between symbolic data and sensor data. Symbolic anchoring systems are based on extracting features from physical objects. Then, they are used to check if new perceived objects correspond with known objects. This mechanism is commonly known matching function. As a result, the anchoring problem~\cite{coradeschi2003introduction} is related to how to perform anchoring in an artificial intelligence system. In fact, anchoring is a special case of Physical Symbol Grounding~\cite{vogt2002physical} where symbolic data is maintained and updated in time. The anchoring problem involves different areas, such as psychology, cognition, linguistics and computer science. This makes anchoring a complex process.

There are initial works in symbolic anchoring that use fuzzy logic to implement the matching function and the grounding, such as \cite{coradeschi1999anchoring}. In later works, we can find one of the basic anchoring pipelines presented in \cite{daoutis2009grounding}. It shows a basic system for perceptual anchoring using only visual features to implement the matching function used to check if new objects have to be stored in the knowledge base. This system was composed of a perceptual layer, where sensory data is generated and processed to extract features; a perceptual anchoring layer, where the grounding and anchoring processes take place; and a knowledge representation layer, which contains a knowledge base. This approach uses SIFT~\cite{lowe2004distinctive} to produce visual features to check if new objects are obtained. The grounding is based on describing the physical objects using color, class, semantic localization and spatial relations.

The work~\cite{elfring2013semantic} presents a method for modeling the semantic environment of a robot using probabilistic multiple hypothesis anchoring (PMHA), which includes the matching function. The method uses probabilistic reasoning to update the robot's understanding of the environment as it receives new sensory information, however, there is not a clear anchoring architecture of the complete system. The features used as input for its algorithm are color images and object shapes. It also presents a grounding skill that describes the objects using their size and their color. 

Another alternative is presented in \cite{persson2017learning}. This work proposed a method for improving the anchoring of objects in the real world through the use of learned actions. The feature used in the matching function is the euclidean distance between two objects and the classification coefficient. Both features are used in the matching function that is based on a formula with a threshold for each input. Besides, the presented framework is based on integrating actions and perception to improve the accuracy of object anchoring in situations where visual perception alone is unreliable. The framework is provided with machine learning models that learn actions that can be used to disambiguate objects thus improving the perceptual anchoring of objects in the real world. Another case of using object poses is presented in \cite{gunther2018context}. In this work, the features used are again the euclidean distance and classification coefficient. These two features are fed into the matching function that is based on a Support Vector Machine (SVM) plus the use of the Hungarian Method~\cite{kuhn1955hungarian} to assign each perception to an existing anchor. The SVM is trained with a dataset created by the authors using a mobile robot.

More complex matching functions can be found in symbolic anchoring systems. \cite{persson2019semantic} shows a matching function based on machine learning techniques. The data used to train the model is composed of five similarities: class, color histogram, distance, size and time. The dataset was collected indoors using a fixed camera that was in front of the used robot manipulator. Then, several models were trained that are SVM, MLP, Bayes and KNN.

It is possible to extract from the literature that the use of symbolic anchoring in robots is still a problem to be faced. There are different pipelines but all of then have in common the feature extraction from sensory data and the implementation of a matching function that allows knowing if new perceptions correspond with known objects. In this work, we present a new symbolic anchoring from perceptual, based on ROS 2, that uses deep learning to carry out the matching function.


\section{Materials and methods}
\label{sec:materials_methods}

The Materials and Methods section of this paper aims to detail the components and processes used to develop a symbolic anchoring from perception system for robots. This system leverages the principles of cognitive architecture to allow robots to anchor their perceptions to symbolic representations, enabling them to process and understand the environment in a more human-like manner. The section is divided into six main subsections: Formalization of Perceptual Anchoring,  SAILOR pipeline,  integration in ROS 2, datasets, the cognitive architecture MERLIN 2 and the hardware setup. 
These subsections will provide a comprehensive understanding of the methodology and procedures used to build the symbolic anchoring system, as well as the datasets used.


\subsection{Formalization of Perceptual Anchoring}

Anchoring is the task of creating and maintaining in time the correspondences between symbolic data and sensor data, also called percepts. Following (\cite{coradeschi2003introduction}, \cite{daoutis2009grounding}, \cite{coradeschi2000anchoring}, \cite{coradeschi2001fuzzy}), the perceptual anchoring is composed of three systems:

\begin{itemize}
    \item \textit{Symbolic system}: This system is in charge of maintaining symbolic knowledge and using it to reason about the actions needed to achieve certain goals. Symbolic knowledge is stored in a knowledge base composed of four sets:
    \begin{itemize}
        \item Set of types $T = \{t_1, t_1,...\}$ that describes the types of objects that can appear in the problem.
        \item Set of objects $O = \{o_1, o_1,...\}$ that contains the objects of the problem.
        \item Set of predicates $P = \{p_1, p_1,...\}$ that contains the attributes of the world.
        \item Set of facts $F = \{f_1, f_1,...\}$ that described the the world.
    
    \end{itemize}
    
    \item \textit{Perceptual system}: This system is in charge of generating percepts from the data obtained from the real world. It includes a set of percepts, $\Pi = \{\pi_1, \pi_1,...\}$ A percept is a data structure, $\pi_i$, that defines a physical object. Each percept has a set of measurable features, $\phi_i$ with values in the domain $D(\phi_1)$. As a result, the perceptual system includes a set of features, $\Phi = \{\phi_1, \phi_1,...\}$, used to describe each percept.

    \item \textit{Anchoring system}: This is the system in charge of updating the symbolic knowledge using percepts. This correspondence is represented by the data structure called anchor, $\alpha_i$. Thus, this system has a set of percepts, $A = \{\alpha_1, \alpha_1,...\}$.

\end{itemize}


Perceptual anchoring also has a \textit{predicate grounding relation} $G \subseteq P \times \phi \times D(\phi)$. This relation is in charge of encoding features $\phi_i$ from each $\pi_i$ using the properly predicates $p_i$ to create the facts $f_i$. Percepts can be described using visual features, such as color histograms, descriptors and semantic object categories. On the other hand, \cite{gunther2018context} uses physical features, such as 3D position, 3D size and orientation. Finally, a combination of both, visual and physical features, can be used, as in the case of \cite{persson2019semantic}.


\subsubsection{The functionalities of Anchoring}

In the anchoring process, anchors $\alpha_i$ can be created both top-down and bottom-up. Bottom-up approaches are based on events from the perceptual system (e.g. new percepts $\pi_i$ from object recognition) whose data can be linked to existing anchors. On the other hand, top-down takes place when symbolic data needs to be linked to a percept.

The maintenance of anchors takes place at each cycle of the perceptual system when new percepts are created. The new percepts are compared with the existing anchors. Those percepts that match an anchor are used to update the anchor and the symbolic data. To check if a percept matches an anchor, a matching function $M$~\ref{matching_function} is used. This function takes as input a percept $\pi_i$ and an anchor $\alpha_i$ and returns the degree of matching.

\begin{equation}
    M : \pi \times \alpha \rightarrow [0, 1]
    \label{matching_function}
\end{equation}


There are four main functionalities defined in the literature (\cite{persson2019semantic}, \cite{loutfi2005maintaining}):

\begin{itemize}
    \item \textit{Acquire}: This functionality initiates new anchors whenever new percepts are received and do not match any existing anchor. It takes each new percept $\pi_i$ and each existing anchor $\alpha_i$ and computed the matching degree using the matching function $M$. For each percept that does not match an anchor, symbolic data (objects and facts) is created using the \textit{predicate grounding relation} $G$.

    \item \textit{Find}: This functionality takes an object $o_i$ and the facts $f_i$ that describe that object and returns and anchor $\alpha_i$. Then, that anchor is compared against the existing anchors and current percepts. If there is an anchor that matches, that anchor is selected. A new anchor is created if there is no match with an anchor.

    \item \textit{Re-acquire}: This functionality is intended to extend the definition of a matching anchor $\alpha_i$ from $t - k$ to $t$. It is based on taking the matching percepts and updating the anchors and the symbolic data.

    \item \textit{Track}: This functionality is based on taking an anchor $\alpha_i$ defined in $t - k$ and extends its definition to $t$. This can be performed by using the re-acquire functionality if a match takes place or by predicting the future state of the anchor after some elapsed time from the last observation. 
\end{itemize}


\begin{figure}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{images/SAILOR-Anchoring_Architecture.drawio.pdf}
\caption{\label{fig:sailor_architecture} SAILOR Architecture.}
\end{figure}




\subsection{SAILOR Pipeline}

SAILOR (Symbolic AnchorIng from perceptuaL for rOs2-based Robots) is the symbolic anchoring from perceptual integrated into the cognitive architecture. SAILOR's architecture, which is a bottom-up anchoring architecture, is presented in Figure~\ref{fig:sailor_architecture}. It is divided into three layers as described in \cite{daoutis2009grounding}.



\begin{figure*}[ht!]
\centering
\includegraphics[width=1.0\textwidth]{images/SAILOR-page-8.drawio.pdf}
\caption{\label{fig:sailor_example} Example of using SAILOR in two consecutive times. Original images show the real image captured by the camera of the robot. YOLO images show the object detected using YOLOv8. The drawn bounding boxes represent the percepts that are obtained in that frame obtained from the RGB-D camera of the robot. 3D Boxes images show the 3D representation of the detected objects. Anchors Images show the current percepts acquired or reacquired in the Anchoring Layer.}
\end{figure*}


\begin{itemize}
    \item \textbf{\textit{Symbolic Layer}}: this layer is composed of the knowledge base where the symbolic knowledge is stored. It is implemented using the knowledge base presented in \cite{kant}. As a result, the knowledge base can be ROS 2 node or MongoDB. The former means storing the knowledge in memory while the latter provides persistence to the anchoring system.

    \item \textbf{\textit{Anchoring Layer}}: this is the layer in charge of managing the anchors. This means receiving the percepts from the perceptual layer and applying the matching function. When new percepts for a given frame of the camera arrives at this layer, the matching function is applied. Then, the percepts that do not match any existing anchor will produce the creation of new anchors. The percepts that match an anchor are used to update that anchor and the symbolic knowledge linked. This implies applying the acquire and reacquire functionalities. The anchoring procedure is presented in Algorithm~\ref{alg:anchoring}. In the initial case, when there are no anchors, all received percepts are acquired. In the following cases, the new percepts are used to create a matching table. The procedure to create this table is shown in Algorithm~\ref{alg:table}. It is based on applying the matching to each candidate, that is each new percept and to the existing anchors. Each pair of percept-anchor is compared using their features. As a result, the table obtained is a matrix $N\times M$, where $N$ is the number of new percepts and $M$ is the number of existing anchors. Each cell contains the matching value for each pair of percept-anchor which is a value in the range [0, 1]. The Hungarian Method~\cite{kuhn1955hungarian} is used to associate each percept with its correct anchor. The matching table is the input and the output got is the rows and columns that correspond to the associations. Finally, the pairs of percept-anchor of the associations with a matching value greater than a threshold, that is 0.5, are the reacquire cases. On the other hand, the pairs with a value below the threshold are acquiring cases. There are also acquiring cases when new percepts are not the result of the Hungarian Method. This can happen if the number of new percepts is greater than the number of existing anchors.

\begin{algorithm}[ht!]
\caption{SAILOR's anchoring algorithm.}\label{alg:anchoring}
\begin{algorithmic}[1]
\Require $new\_percepts$
\If{$anchors == \emptyset$}
    \ForEach {$p \in new\_percepts$}
        \State $acquire(p)$
    \EndFor
\Else
    \State $M \gets create\_matching\_table(new\_percepts)$
    \State $H \gets hungarian\_method(M)$

    \ForEach {$i \in M$}
        \If{$i \in H$}
            \If{$H(i) > threshold$}
                \State $reacquire(p)$
            \Else
                \State $acquire(p)$
            \EndIf
        \Else
            \State $acquire(p)$ %\Comment{This happens if the number of percepts is greater than the number of anchors}
        \EndIf
    \EndFor
\EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht!]
\caption{Create matching table algorithm.}\label{alg:table}
\begin{algorithmic}[1]
\Require $new\_percepts$
\Ensure $m\_table$
\State $m\_table \gets \emptyset$
\ForEach {$p \in new\_percepts$}
    \State $m\_row \gets \emptyset$
    \ForEach {$a \in anchors$}
        \State $m\_value \gets matching\_function(p, a)$
        \State $m\_row \gets m\_row \cup m\_value$
    \EndFor
    \State  $m\_table \gets m\_table \cup m\_row$
\EndFor
\Return $m\_table$
\end{algorithmic}
\end{algorithm}




    \item \textbf{\textit{Perceptual Layer}}: this is the layer that generates the percepts. To do this, an RGB-D camera is used. The color images obtained from the camera are processed in an object detection system, which is a sub-symbolic skill of the robot. Then, the obtained bounding boxes are used along with the PointCloud from the camera and the color image to produce the features of each percept. Following the features described in the related works section that are used in the matching function, we have selected five features to describe each percept. As a result, a percept is composed of the following features: 
    \begin{itemize}
        \item Class of the object: this feature is obtained from the object detection skill. In this work, YOLO~\cite{jiang2022review} (YOLOv8), was used to process color images obtained from the robot to detect objects in the environment.
        \item Cropped image: this feature is the image obtained after cropping the original frame from the camera using the bounding box from the object detection skill. Then, the image is fed into a ResNet-18 whose last fully connected layer is cut. The resulting feature vector is used to represent the cropped image.
        \item Position of the object: this feature is the center position of the detected object. It is obtained using the bounding box and the PointCloud. Then, this position is transformed from the local coordinate system of the camera to the global coordinate system of the robot.
        \item Size of the object: this feature is the size of the object representing it as a box. The measurements are obtained using the bounding box and the PointCloud data.
        \item Timestamp: this feature is the timestamp when the percept is created.
    \end{itemize}

\end{itemize}


%\begin{figure*}[bp] %Fuerza el bottom gracias al package \usepackage{stfloats}
 \begin{figure*}[!h]
\centering
\includegraphics[width=1.0\textwidth]{images/SAILOR-FullModel.drawio.pdf}
\caption{\label{fig:sailor_nn} Neural Network used to implement the Matching Function of SAILOR. It is divided into three components: ResNet Siamese, which produces the similarity between two images as a feature vector; PerceptAnchor Network, which encodes each pair of percept-anchor; and Binary Classifier, which classifies each encoded pair as reacquire or acquire.}
\end{figure*}



An example of applying SAILOR is presented in Figure~\ref{fig:sailor_example}. In this example, two samples are taken at different times. The first column represents the original images. Then, YOLOv8 is applied, whose output is presented in the second column. The result of calculating the 3D boxes for each detection of YOLOv8 is presented in the third column. Finally, the last column shows the current percepts that have been acquired and reacquired. In this case, there are no new percepts to acquire so all previous percepts are reacquired.








\subsubsection{Matching Function}

The matching function implemented in the presented solution, which is a symbolic skill for robots, is based on a neural network. These features are used to compare new percepts with existing anchors. The comparison of each pair of percept-anchor gives the input features of the neural network which are the following:
\begin{itemize}
    \item Same class: this feature is a boolean that indicates if the classes of the percept and the anchor are the same.
    \item Cropped images: these images correspond to the cropped images of the percept and the anchor. They are used to measure visual similarity.
    \item Distance: this feature is the $L^2$-distance between the positions of the percept and the anchor.
    \item Scale factor: this feature is the scale factor between the sizes of the percept and the anchor.
    \item Time: this feature is the time, in seconds, between the timestamps of the percept and the anchor.
\end{itemize}



The neural network used in this work is presented in Figure~\ref{fig:sailor_nn}. This network is composed of three modules:
\begin{itemize}
    \item ResNet Siamese: following the siamese networks used to measure the similarity of two images \cite{melekhov2016siamese}, we have used two frozen ResNet-18~\cite{targ2016resnet} networks to extract features of the cropped images of each pair of percept-anchor. Then, the $L^1$-distance is applied to the outputs of the ResNet-18. This result is fed into a fully connected layer. As a result, the comparison of the two images from the pair percept-anchor is obtained.
    \item PerceptAnchor Network: this network is in charge of encoding and concatenating the five features that describe each pair percept-anchor. There is a fully connected layer for each input feature (class, ResNet Siamese output, distance, size, timestamps). Then, the outputs are concatenated and fed into another fully connected layer.
    \item Binary Classifier: this network is a Multi-Layer Perceptron (MLP) responsible for classifying the pair percept-anchor as a reacquire. It uses the output of the previous network as its input and returns a value between 0 and 1, thanks to the sigmoid function, that indicates the matching degree between the percept and the anchor. 
\end{itemize}

In the process of training a neural network, choosing the appropriate optimizer and learning rate is crucial to achieving optimal performance. In this study, the training of this neural network was carried out using Adam optimizer and a learning rate of 0.001. Adam is a popular optimizer that combines the benefits of two other optimization techniques, namely, Adagrad and RMSprop. This optimizer has been shown to work well in a wide range of deep learning tasks and is known for its efficient convergence rate. The learning rate, on the other hand, determines the step size taken in each iteration of the optimization process. A low learning rate can result in slow convergence, while a high learning rate can lead to overshooting the optimal solution. In this study, a learning rate of 0.001 was chosen based on empirical observations and experimentation.



\begin{figure*}[bp] %Fuerza el bottom gracias al package \usepackage{stfloats}
% \begin{figure*}[!h] 
\centering
\includegraphics[width=1.0\textwidth, frame]{images/rosgraph.pdf}
\caption{\label{fig:rosgraph} Rosgraph of SAILOR.}
\end{figure*}

\subsection{SAILOR and ROS 2 }

The previous pipeline has been implemented in ROS 2 for its integration into a real robot called TIAGo. Figure~\ref{fig:rosgraph} shows the rosgraph of SAILOR. It is composed of camera nodes to produce RGB images and PointCloud data, the YOLOv8 node, the percept generator node and the anchoring node. The percept generator node is subscribed to RGB images, PointCloud data and YOLOv8 detections to produce percepts. Then, the anchoring node is subscribed to these percepts and applies the anchoring procedures.


\subsection{Datasets}

The matching function presented in this work is based on a deep learning solution, that is a neural network. As a result, preparing the dataset is a necessary process. In previous machine learning-based works, custom datasets have been created. For instance, in \cite{persson2019semantic} a new dataset is created using a custom labeling tool. This dataset describes each pair of percept-anchor using five similarities (classification, histogram, distance, size, time). However, all data is obtained in scenarios where the robot, a robotic arm, is fixed to a table. Besides, in \cite{gunther2018context} a custom dataset is created using a mobile robot. In this case, pairs of percept-anchor are described using the classification values and distances.

There are several existing datasets that can be used to train the neural network. For instance, KITTI dataset~\cite{Geiger:2013}, is a dataset intended to be used in several tasks such as autonomous driving and object detection. It comprises traffic scenarios recorded with diverse sensor modalities. Another similar dataset is nuScenes~\cite{Caesar:2020} which is a public large dataset for autonomous driving developed. It contains scenes of images, LIDAR data and ground truth that can be used in several tasks. It includes a Python library to access the data and apply transforms to the positions and sizes of detected objects.

One recent indoor dataset that can be used is MOTFront~\cite{schmauser20223d}. It provides photo-realistic RGB-D images with their corresponding instance segmentation masks, class labels, 3D bounding boxes and 3D poses. The scenes were captured in indoor scenarios with furniture.

\begin{algorithm}[ht!]
\caption{Create dataset algorithm.}\label{alg:create_dataset}
\begin{algorithmic}[1]
\Require $scenes$
\Ensure $pairs$
\State $pairs \gets \emptyset$
\ForEach {$S \in scenes$}
    \State $objects \gets \emptyset$
    \ForEach {$sample \in S$}
        \ForEach {$object \in sample$}
            \ForEach {$o \in objects$}
                \State $pair \gets create\_pair(object, o)$
            \EndFor            
            \State $objects \gets objects \cup object$
        \EndFor
    \EndFor
\EndFor
\Return $pairs$
\end{algorithmic}
\end{algorithm}

With all of this, we have created three new datasets using nuScenes and MOTFront. To do this, the procedure presented in Algorithm~\ref{alg:create_dataset} is applied to each scene of the datasets. This way, each object of each sample from the scenes is used to create the pairs of percept-anchor. These pairs are created by calculating the five input features of the neural network presented previously.


The previous algorithm is applied to the full MOTFront dataset. In the case of nuScenes, scenes 1, 2, 4, 5, 6, 7, 8, 41, 42  and 43 have been used for training; scenes 3, 12, 13, 14 and 15 for validation; and scenes 1069, 1070, 1071, 1072 and 1073 for testing. Then, both resulting datasets were merged to create the third dataset. The resulting datasets are summed up in Table~\ref{table:datasets}.

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{12pt} % increase column spacing
\renewcommand{\arraystretch}{1.25} % Increase row spacing
\setlength{\arrayrulewidth}{1pt} % increase border width
\caption{Datasets created using nuScenes and MOTFront data.}
\label{table:datasets}
\begin{tabular}{|c|c|c|c|} 
 \hline
 \textbf{Dataset} & \textbf{Train} & \textbf{Val} & \textbf{Test} \\[0.5ex]\hline
 \textbf{nuScenes} & 429615 & 33172 & 52806 \\\hline
 \textbf{MOTFront} & 469928 & 104274 & 116655 \\\hline
 \textbf{Mix} & 899543 & 137446 & 169461 \\
 
 \hline
\end{tabular}
\end{table}

% \begin{itemize}
%     \item Suecos\cite{persson2019semantic}: 
%     \begin{itemize}
%         \item Interiores con camar fija sobre mesa
        
%         \item Al estar en superficies más cortas (en distancia), tiene limitaciones espaciales (es más fácil acotarlo todo). Cámara-Mesa.
%         \item Con el tiempo nos perdemos
%         \item Con el color tenemos unos márgenes de 70-90 es imposible que a ellos la bounding box 
%         \item Utilizar los colores están mu asociados a la luz del entorno (manolo)
%         \item HAcer doble check a las normalizaciones realizadas (valores minimos y máximo) (manolo)
%     \end{itemize}

%     \item Alemanes\cite{gunther2018context}: mobile robot, interior
    
%     \item KITTI dataset (Karlsruhe Institute of Technology and Toyota Technological Institute): it is a dataset used in mobile robotics and autonomous driving. It comprises traffic scenarios recorded with diverse sensor modalities. 
%     ~\cite{Geiger:2013}. 
%     \begin{itemize}
%         \item No viene bien documentado lo de el cambio ded la coordenadas a global. 
%     \end{itemize}

%     \item nuScenes\cite{Caesar:2020} %\url{https://www.nuscenes.org/}
%     \begin{itemize}
%         \item Arreglado los problemas asociados que los cubos utilizando solo el ancho. 
%         \item 10 clases reales (hay que justificar ya que en la web pone 20 )       
%     \end{itemize}
   
%     \item MOTFront\cite{schmauser20223d}:
%     %\url{https://paperswithcode.com/dataset/motfront}   
%     \begin{itemize}
%         \item natural en interiores
%         \item mueven ligeramente los objetos de interior
%     \end{itemize}    
% \end{itemize}



\subsection{MERLIN2}


% Cognitive architectures employed in robotics need a way to dynamically obtain knowledge from the environment, which is facing the anchoring problem. An example of this is presented in \cite{rodriguez2018generating}. It proposes a method for generating symbolic representations of the world from sensory data inside a cognitive architecture. The symbolic representations are stored in the Knowledge Base (KB), nested in a Symbolic Layer in this publication for its correspondence with other publications, using PDDL~\cite{fox2003pddl2}. Then, the architecture uses that knowledge in the Deliberative and Behavioral Layers. Figure~\ref{fig:MERLIN+SAILOR} illustrates how Sailor components are integrated int the Behavioral Layer as any other skill available in the robot such as NAV2, Text2Speech, etc. 
% , that is ROSPlan~\cite{ROSPlan}, to produce plans that solve the goals of the robot.

Cognitive architectures employed in robotics need a way to dynamically obtain knowledge from the environment, which is facing the anchoring problem. An example of this is presented in \cite{rodriguez2018generating}. It proposes a method for generating symbolic representations of the world from sensory data inside a cognitive architecture. The symbolic representations are stored in the Knowledge Base, nested in a Symbolic Layer, using PDDL~\cite{fox2003pddl2}. Then, that knowledge is used to produce plans that solve the goals of the robot.

In this publication, MERLIN2 architecture~\cite{GONZALEZSANTMARTA2023100477} is used. The architecture is composed of two main systems, Deliberative and Behavioural, that are divided into two layers each. The Deliberative is composed of the Mission Layer and the Planning Layer, where the Knowledge Base can be found. The Behavioural is composed of the Executive Layer and the Reactive Layer, where robot skills can be found.  

Figure~\ref{fig:MERLIN+SAILOR} illustrates how Sailor components are integrated into the MERLIN2 architecture. The sub-symbolic skills, Perceptual Layer (PL) and Anchoring Layer (AL), of SAILOR are integrated into the Reactive Layer from the Behavioral as any other skill available in the robot such as NAV2, Text2Speech, etc. The Symbolic Layer of SAILOR corresponds with the Knowledge Base (KB) of the Planning Layer.



\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\textwidth]{images/MerlinArchitecture.pdf}
\caption{\label{fig:MERLIN+SAILOR} MERLIN 2 architecture showing Sailor as a robot skill.}
\end{figure}


\subsection{Hardware Setup}


All anchored data were acquired with the use of a TIAGo mobile Robot which is equipped with an Asus Xtion Pro live RGB-D sensor. TIAGo Robot runs ROS Melodic and bridges for interfacing with ROS 2 humble of an external laptop with an Intel(R) i7-8750H CPU, 8 GB RAM and a GTX 1060 Nvidia. On the other hand, the training and test were performed using a remote machine with an AMD EPYC 7302P CPU, 256 GB RAM and a Quadro RTX 8000 Nvidia.


\section{Evaluation}
\label{sec:evaluation}

% The use of deep learning in robotics has become an increasingly popular research area, and symbolic anchoring is a crucial component of this domain. In recent years, neural networks have shown great promise in improving the performance of robots in various tasks. This paper aims to use deep learning techniques for symbolic anchoring in robotics.
This evaluation section will discuss the results of the trained models. Specifically, a neural network has been trained on three different datasets to test its ability to anchor symbols to sensory data. The authors examined the performance of the same algorithm, the Matching Function, under the three datasets. The evaluation of the quality of the trained models was carried out by such criteria as a confusion matrix. 


%\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{center}
    \begin{table}[!h]
     \centering
    \caption{Generatic structure of Confusion Matrix. \label{tab:genericConfusion}}
      \resizebox{0.3\textwidth}{!}{%
        
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
          & p$'$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} & P$'$ \\[2.4em]
          & n$'$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} & N$'$ \\
          & total & P & N &
        \end{tabular}
    }
    \end{table}
\end{center}

Table~\ref{tab:genericConfusion} illustrates the template of the confusion matrix. It compares the predicted labels of a model with the true labels of the data it was trained on. The matrix contains four different values: true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN). 
Afterward, these values are then used to calculate different evaluation metrics, such as  accuracy, precision, recall and F1-score. Accuracy measures the proportion of correct predictions, while precision measures the proportion of true positives among all positive predictions. Recall measures the proportion of true positives among all actual positives, while F1-score is the harmonic mean of precision.

% A confusion matrix can provide valuable insights into the strengths and weaknesses of a model, helping to identify where it may need improvement. It can also be useful in selecting an appropriate threshold value for classification, depending on the specific needs of the application.




Tables~\ref{fig:NUSCENEconfusion}, \ref{fig:MOTFRONTconfusion}, \ref{tab:mixedconfusion} and  \ref{tab:MIXEDconfusion} give the confusion matrix of anchoring accuracy in times and percentages for the testing data of the three approaches: using nuScenes, MOTFront and Mix datasets.


Table~\ref{fig:NUSCENEconfusion} shows the confusion matrices associated with our matching function trained with nuScenes and tested with MOTFront and nuScenes groups. The findings, in this case, present a non-common behavior. nuScenes behavior is better with MOTFront test data than with nuScenes itself in True Positive values. It achieves 97,94\% against 89,77\%. True Negative values present a rate of 96,18\% in MOTFront and a rate of 99,86\% in nuScenes. 
 
%\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{center}
    \begin{table}[ht]
    \caption{Confusion matrix using nuScenes dataset.\label{fig:NUSCENEconfusion}}
     
      \resizebox{0.22\textwidth}{!}{%
        
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
          
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
         & p$'$ & \MyBox{33397}{97,94\%} & \MyBox{3152}{3,82\%} & 36549 \\[2.4em]
          & n$'$ & \MyBox{702}{2,06\%} & \MyBox{79404}{96,18\%} & 80106 \\
          & total & 34099 & 82556 &\\ 
             & \multicolumn{3}{c}{\bfseries MOTFront Model} 
        \end{tabular}
    }
      \resizebox{0.22\textwidth}{!}{%
     
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          % \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
           \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering ~\\ 
          ~}}} &
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
          & p$'$ & \MyBox{1685}{89,77\%} & \MyBox{104}{0,20\%} & 1789 \\[2.4em]
          & n$'$ & \MyBox{192}{10,23\%} & \MyBox{50825}{99,80\%} & 51017 \\
          & total & 1877 & 50929 & \\
            & \multicolumn{4}{c}{\bfseries nuScenes Model} 
        \end{tabular}
    }
     \end{table}
    
\end{center}

Table \ref{fig:MOTFRONTconfusion} shows the confusion matrices associated with our matching function trained with MOTFront and tested with MOTFront and nuScenes groups. The results present a number of similar results in True Positive and MOTFront achieving 99,78\%, this is 0,92 \% better than using nuScenes. True Negative values present a rate of 99,78\% in MOTFront and achieve a rate of 98,86\% in nuScenes. 

   
%\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{center}
    \begin{table}[h!]
    \caption{MOTFront dataset.\label{fig:MOTFRONTconfusion}}
     
      \resizebox{0.23\textwidth}{!}{%
        
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
           & p$'$ & \MyBox{36370}{99,81\%} & \MyBox{179}{0,22\%} & 36549 \\[2.4em]
          & n$'$ & \MyBox{69}{0,19\%} & \MyBox{80037}{99,78\%} & 80106 \\
          & total & 36439 & 80216 &\\
             & \multicolumn{3}{c}{\bfseries MOTFront Model} 
        \end{tabular}
    }
      \resizebox{0.23\textwidth}{!}{%
     
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          % \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
          \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering ~\\ 
          ~}}} &
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
           & p$'$ & \MyBox{1201}{99,81\%} & \MyBox{588}{1,14\%} & 1789 \\[2.4em]
          & n$'$ & \MyBox{0}{0,00\%} & \MyBox{51017}{98,86\%} & 51017 \\
          & total & 1201 & 51605 &\\
            & \multicolumn{4}{c}{\bfseries nuScenes Model} 
        \end{tabular}
    }
     \end{table}
    
\end{center}



Table \ref{tab:mixedconfusion} shows the confusion matrices associated with our matching function trained with the Mix dataset and tested with MOTFront and nuScenes groups. The trained model presents better True Positive values in the MOTFront test data than with nuScenes. It achieves 99,76\% against 95,00\%. True Negative values present a rate of 99,03\% in MOTFront and 99,83\% in nuScenes. 
 
%\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{center}
    \begin{table}[!ht]
    \caption{Confusion matrix using Mix dataset.\label{tab:mixedconfusion}}
     
      \resizebox{0.22\textwidth}{!}{%
        
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
          
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
          & p$'$ & \MyBox{1292}{95,00} & \MyBox{497}{0,97} & 1789 \\[2.4em]
          & n$'$ & \MyBox{68}{5,00} & \MyBox{50949}{99,03} & 51017 \\
          & total & 1360 & 51446 &\\ 
             & \multicolumn{3}{c}{\bfseries MOTFront Model} 
        \end{tabular}
    }
      \resizebox{0.22\textwidth}{!}{%
     
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          % \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
           \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering ~\\ 
          ~}}} &
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
           & p$'$ & \MyBox{36411}{99,76} & \MyBox{138}{0,17} & 36549 \\[2.4em]
          & n$'$ & \MyBox{88}{0,24} & \MyBox{80018}{99,83} & 80106 \\
          & total & 36499 & N 80156 \\
            & \multicolumn{4}{c}{\bfseries nuScenes Model} 
        \end{tabular}
    }
     \end{table}
    
\end{center}


Finally, table \ref{tab:MIXEDconfusion} shows the confusion matrix associated with our dataset, with data from both datasets (nuScenes and MotFront). True Positive values present a rate of 99,59\% and True Negative values present a rate of 99,52\%. 
 


\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{center}
    \begin{table}[ht]
    \centering
    \caption{Confusion Matrix Model trained and tested with a Mix dataset.\label{tab:MIXEDconfusion}}
      \resizebox{0.3\textwidth}{!}{%
        
       \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}   
          \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering actual\\ value}}} & 
            & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
          & & \bfseries p & \bfseries n & \bfseries total \\
           & p$'$ & \MyBox{37703}{99,59\%} & \MyBox{635}{0,48\%} & 38338 \\[2.4em]
          & n$'$ & \MyBox{156}{0,41\%} & \MyBox{130967}{99,52\%} & 131123 \\
          & total & 37859 & 131602 &
        \end{tabular}
    }
    \end{table}
\end{center}

As a result of previous findings, Table \ref{tab:Accuracyetal} presents average anchoring classification accuracy, precision, recall and F1-score. The accuracy metric defines the total number of correctly classified data over the total number of data. However, this metric is not enough for non-balanced datasets. Precision defines how accurate is the model out of those predicted positives and measures how many of them are actually positives. This metric is a good measure to determine when the cost of a False Positive is high, this means measuring how many elements are we detecting wrongly. Recall defines how many of the Actual Positives our model captures by labeling it as Positive (True Positive). This metric helps us to select the best model when there is a high cost associated with False Negative. In this case, when we do not acquire an anchor that is present in the scene. F1-Score is a metric to be applied if it is needed to seek a balance between Precision and Recall. In these large datasets would be an uneven class distribution (a considerable number of certain Negatives).

Results present high accuracy rates and the enhancements are close to insignificant to previous results. 
Taking a look to recall we can note differences in nuScenes behavior, increasing MOTFront performance recognizing
nuScenes elements and this improvement is also presented in F1-score. 

Comparing these findings it seems that MotFront has a high influence in the Mix Dataset and the classification of our matching function is highly influenced to better classify the candidate anchors from this dataset. 



\begin{table}[]
 \centering
 \caption{Resulting average classification accuracy together with precision, Recall and F1-score for
    each model tested in our approach to perform the anchoring functionalities.\label{tab:Accuracyetal}}
    \resizebox{0.4\textwidth}{!}{%
    \centering
    \begin{tabular}{|P{2.5cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
        \hline
         &Accuracy & Precision  & Recall  & F1-score  \\ \hline

          \multicolumn{5}{|c|}{\bfseries nuScenes Dataset}  \\ \hline
        MOTFront & 0,967 & 0,979 & 0,914 & 0,945 \\ \hline
        nuScenes & 0,994 & 0,898 & 0,942 & 0,919 \\ \hline \hline
          
          \multicolumn{5}{|c|}{\bfseries MOTFront Dataset} \\ \hline
        MOTFront & 0,998 & 0,998 & 0,995 & 0,997 \\ \hline
        nuScenes & 0,989  & 1,000 & 0,671 & 0,803 \\ \hline \hline
        
        
          \multicolumn{5}{|c|}{\bfseries Mix Dataset}  \\ \hline       
        MOTFront & 0,998 & 0,998 & 0,996 & 0,997 \\ \hline
        nuScenes & 0,989 & 0,950 & 0,722 & 0,821 \\ \hline
        Mix Test & 0,995 & 0,996 & 0,983 & 0,990 \\ \hline
    \end{tabular}
}
\end{table}


\subsection{Contribution evaluation}

This research proposed three main contributions that have been validated: 
\begin{enumerate}
    \item It presented an updated anchoring pipeline based on state-of-the-art works. The presented pipeline is based on object detection followed by percept generation, which implies using the bounding boxes to get physical features from the PointCloud of the camera.
    \item The matching function based on deep learning for an anchoring system achieves more than 96\% on accuracy in all cases tested.
    \item The evaluation of datasets and models for anchoring is being validated by mixing nuScenes (outdoors) and MOTFront (indoors) datasets. The resulting dataset is available at Google Drive\footnote{\url{https://drive.google.com/file/d/1Yklle5-rSuKN-OZ-ZZ3bvCa5eFXusK5T/view?usp=share_link}}.
    \item A set of ROS 2 components have been validated and tested in a real robotic Platform TIAGo. These components are publicly available in a GitHub Repository\footnote{\url{https://github.com/MERLIN2-ARCH/sailor}}.
\end{enumerate}



\section{Conclusions}
\label{sec:conclusions}


In this work, we have developed SAILOR, a set of software components for ROS 2 to provide an anchoring process in a cognitive architecture of a robotic system. We have further introduced an anchoring pipeline that performs an object detection process and afterward performs a pointCloud-based physical feature extraction process, rather than the reviewed state-of-the-art works that perform the process all the way around. 


The developed matching function is based on a neural network. It defines a ResNet Siamese followed by a PerceptAnchor Network and finishing in a Binary Classifier. The use of state-of-the-art datasets allowed us to measure its performance. The classifier performance for the nuScenes dataset (outdoor environment) shows that nuScenes test had better performance in accuracy and  recall than MOTFront test appears to perform better overall on the outdoor environment and MOTFront test has a higher precision and F1-score than the nuScenes test model.
For the MOTFront dataset (indoor environment), the MOTFront test appears to have a better precision score, as expected, while the Mix dataset has a better recall.
Overall, it's difficult to determine which model is definitively better for either indoor or outdoor environments, as the performance can depend on many factors such as the specific task, dataset and evaluation metrics. However, based on the provided table, we can say that the Mix dataset appears to perform well on both indoor and outdoor environments, while the MOTFront dataset has a high precision score for indoor environments.

% Results  have shown that the proposed anchoring procedure is able to accurately anchoring objects
%  with a minimal accuracy of 96.7\% and better using nuScene or MOTFront datasets. 
% Besides, it was created and evaluated a dataset mixing nuScenes (outdoors) and MOTFront (indoor) datasets, obtaining slighlty better results that those cases of when evaluating 

% It is presented and released a set of ROS 2 components for its integration into our Cognitive Architecture. 


Finally, it was initially tested and deployed in the TIAGo robot. However, further, development is needed. The reason is that the robot works with a noisy sensor and the SAILOR performance is directly linked to the slower component in the pipeline. Initial tests with noisy network connections reduce the process performance.



\section*{Acknowledgments}

This work has been partially funded by an FPU fellowship provided by the Spanish Ministry of Universities (FPU21/01438) and the Grant PID2021-126592OB-C21 funded by MCIN/AEI/10.13039/5011000 11033. % and by ERDF A way of making Europe.





% \appendix

% \section{\LaTeX{} and Word Style Files}
% \label{stylefiles}

% The \LaTeX{} style file is \texttt{kr.sty} and the Bib\TeX{} style file to use
% is \texttt{kr.bst}.  The \LaTeX{} file \texttt{kr-instructions.tex}, containing
% the source of the present document, and the Bib\TeX{} file
% \texttt{kr-sample.bib}, containing some example Bib\TeX{} entries, may serve as
% a formatting sample (these two files are not needed for typesetting your
% paper).  The \LaTeX{} style file is for version 2e of \LaTeX{}, and the
% Bib\TeX{} style file is for version~0.99c (\emph{not} version~0.98i) of
% Bib\TeX{}.  Note that the \texttt{kr.sty} style file differs from the
% \texttt{kr.sty} file used for KR2020 but remains unchanged since then.

% The Microsoft Word style file consists of a single file, \texttt{kr21.docx},
% which may serve as a formatting sample for Microsoft Word users.  Please make
% use of the ad-hoc styles that have been defined for the different parts of the
% document, and that are listed in the Styles Pane.  Note that this template
% differs from the one used for KR2020, but remains essentially unchanged since then.

% Further information on using these styles for the preparation of papers for
% KR2023 can be obtained by contacting
% \href{mailto:kr.proceedings@confdna.com}{kr.proceedings@confdna.com}.


%% The file kr.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{kr}
\bibliography{kr-sample}

\end{document}

