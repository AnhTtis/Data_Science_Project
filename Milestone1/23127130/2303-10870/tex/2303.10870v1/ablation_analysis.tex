\begin{table}[ht]
\arrayrulecolor{black}
\begin{adjustbox}{width=1.0\columnwidth,center}
\begin{tabular}{lllllll} 
\toprule
\multirow{1}{*}{Model}  & \multicolumn{1}{c}{CoNLL2003} & \multicolumn{1}{c}{OntoNotes 5.0}            \\ 
\hline
Baseline      & \multicolumn{1}{c}{92.82}
              & \multicolumn{1}{c}{90.02} \\
% + ERP      & \multicolumn{1}{c}{93.39\tiny{(+0.57)}}
%            & \multicolumn{1}{c}{90.80\tiny{(+0.78)}} \\                      
+ RP \& ERA      & \multicolumn{1}{c}{93.01\small{\textbf{\textcolor{blue}{$\uparrow$0.19}}}}      
                  & \multicolumn{1}{c}{90.17\small{\textbf{\textcolor{blue}{$\uparrow$0.15}}}} \\
+ RP \& ETA      & \multicolumn{1}{c}{93.35\small{\textbf{\textcolor{blue}{$\uparrow$0.53}}}}  
                  & \multicolumn{1}{c}{90.07\small{\textbf{\textcolor{blue}{$\uparrow$0.05}}}}  \\ 
+ RP \& ERA \& ETA  &\multicolumn{1}{c}{93.44\small{\textbf{\textcolor{blue}{$\uparrow$0.62}}}}   
                     & \multicolumn{1}{c}{90.63\small{\textbf{\textcolor{blue}{$\uparrow$0.61}}}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The F1 score of flat NER datsets based on different methods}
\label{t4.flat_ablation}
\end{table}
\section{Analysis and Discussion}
\subsection{Ablation Study}
% 该部分分析模型整体提升情况
% 首先比较生成模型在不同数据集的F1 score均值情况，如图\ref{Fig.avg_f}所示可以看出，我们的生成模型效果提升明显优于其他生成模型。
% 然后分析模型分别融入实体关系、实体类型、实体关系+实体类型对于实体识别的影响。如表 \ref{t4.flat_ablation} to \ref{t4.dis_alation}。
To understand the effectiveness of different components in our model, we conduct a set of ablation studies by removing each component individually, i.e., entity relation attention (ERA) and entity type attention (ETA).
Specifically, 
%In this section, we analyze the improvement of our model and the current unified generation NER models through the average F1 score of these different datasets as shown Figure \ref{Fig.avg_f}. 
%We can see that the effect of our model has been significantly improved. Then we mainly conduct experiments to verify the effectiveness of each component in our model and understand its impact on performance. 
%To be precise, the effects of our seq2seq models with only entity type attention, and only entity relation attention respectively are studied.
% To distinguish models that incorporate different methods, we use ERP, ERA, and ETA to represent entity token pair relation prediction task, entity relation attention, and entity type attention, respectively.
we take the seq2seq model without relation prediction task (RP), entity type attention (ETA) and  entity relation attention (ERA) as ``Baseline''. 
%The generative model only with entity relation prediction task is called ``+ ERP''. 
The seq2seq model with entity relation prediction task and entity relation attention as ``+ RP \& ERA''. Similarly for ``+ RP \& ETA'' and ``+ RP \& ERA \& ETA''.
%We view the generative model with entity token pair relation prediction task and entity type attention but without entity relation attention as ``+ ERP \& ETA'', and mutil-task generative method with entity type and token pairs relations introduced into the attention mechanism is as ``+ ERP \& ERA \& ETA''. 
Note that the RP task is a necessary component for applying ERA as it guides the representation learning for the token embeddings.

The results on different NER tasks are shown in Table \ref{t4.flat_ablation}, \ref{t4.nested_alation} and \ref{t4.dis_alation}. 
% We can see that the model has some improvement in different datasets.
From these results, it is clear that the mutil-task model with either entity relation attention (+ RP \& ERA) or entity type attention (+ RP \& ETA) substantially improves over the baseline model, which validates the effectiveness of both the entity relation attention and the entity type attention.
%and incorporating entity relation attention obviously with entity type attention overall. 
Nevertheless, combining the multi-task with both components achieves the best performance.
\begin{figure*}[ht]
    \centering
        \subfigure{
            \begin{minipage}[t]{0.235\textwidth}
                \centering
                \includegraphics[width=4cm]{figures/CoNLL2003_ablation.jpg}
                \label{Fig.conll2003_ablation}
            \end{minipage}
        }
        \subfigure{
            \begin{minipage}[t]{0.235\textwidth}
                \centering
                \includegraphics[width=4cm]{figures/ACE2004_ablation.jpg}
                \label{Fig.ace04_ablation}
            \end{minipage}        
        }
        \subfigure{
            \begin{minipage}[t]{0.235\textwidth}
                \centering
                \includegraphics[width=4cm]{figures/ACE2005_ablation.jpg}
                \label{Fig.ace05_ablation}
            \end{minipage}        
        }
        \subfigure{
            \begin{minipage}[t]{0.235\textwidth}
                \centering
                \includegraphics[width=4cm]{figures/CADEC_ablation.jpg}
                \label{Fig.cadec_ablation}
            \end{minipage}
        }
        \caption{The F1 scores of entity boundary detection on three NER tasks.}
        \label{Fig.ablation} 
\end{figure*}
\begin{figure*}[ht]
    \centering
        \subfigure{
            \begin{minipage}[t]{0.32\textwidth}
                \centering
                \includegraphics[width=4.2cm]{figures/w_ablation_CoNLL2003.jpg}
                \label{Fig.w_ablation_CoNLL2003}
            \end{minipage}
        }
        \subfigure{
            \begin{minipage}[t]{0.32\textwidth}
                \centering
                \includegraphics[width=4.2cm]{figures/w_ablation_ACE2005.jpg}
                \label{Fig.w_ablation_ACE2005}
            \end{minipage}        
        }
        \subfigure{
            \begin{minipage}[t]{0.32\textwidth}
                \centering
                \includegraphics[width=4.2cm]{figures/w_ablation_CADEC.jpg}
                \label{Fig.w_ablation_CADEC}
            \end{minipage}        
        }
        \caption{The impact of multi-task learning.}
        \label{Fig.w_ablation} 
\end{figure*}
\begin{table}[ht]
\arrayrulecolor{black}
\begin{adjustbox}{width=1.0\columnwidth,center}
\begin{tabular}{llll} 
\toprule
\multirow{1}{*}{Model} & \multicolumn{1}{c}{ACE2004}  & \multicolumn{1}{c}{ACE2005}  & \multicolumn{1}{c}{GENIA} \\ 
\hline
Baseline      & \multicolumn{1}{c}{86.43}   
& \multicolumn{1}{c}{85.51} 
& \multicolumn{1}{c}{78.83}     \\
% + ERP         & \multicolumn{1}{c}{87.10\tiny{(+0.67)}}   
% & \multicolumn{1}{c}{85.84\tiny{(+0.33)}} 
% & \multicolumn{1}{c}{79.34\tiny{(+0.51)}} \\
+ RP \& ERA    & \multicolumn{1}{c}{86.87\small{\textbf{\textcolor{blue}{$\uparrow$0.44}}}}   
& \multicolumn{1}{c}{86.24\small{\textbf{\textcolor{blue}{$\uparrow$0.73}}}}  
& \multicolumn{1}{c}{79.61\small{\textbf{\textcolor{blue}{$\uparrow$0.78}}}}     \\
+ RP \& ETA        & \multicolumn{1}{c}{87.11\small{\textbf{\textcolor{blue}{$\uparrow$0.68}}}}   
& \multicolumn{1}{c}{85.92\small{\textbf{\textcolor{blue}{$\uparrow$0.41}}}}  
& \multicolumn{1}{c}{79.19\small{\textbf{\textcolor{blue}{$\uparrow$0.36}}}}     \\
+ RP \& ERA \& ETA  & \multicolumn{1}{c}{87.36\small{\textbf{\textcolor{blue}{$\uparrow$0.93}}}}  
& \multicolumn{1}{c}{86.14\small{\textbf{\textcolor{blue}{$\uparrow$0.63}}}}  
& \multicolumn{1}{c}{79.77\small{\textbf{\textcolor{blue}{$\uparrow$0.94}}}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The F1 score of Nested NER datsets based on different methods} 
\label{t4.nested_alation}
\end{table}
\begin{table}[ht]
\arrayrulecolor{black}
\begin{adjustbox}{width=1.0\columnwidth,center}
\begin{tabular}{llll} 
\toprule
\multirow{1}{*}{Model} & \multicolumn{1}{c}{CADEC}   &\multicolumn{1}{c}{ShARe13}  & \multicolumn{1}{c}{ShARe14}  \\ 
\hline
Baseline      & \multicolumn{1}{c}{70.36}             
& \multicolumn{1}{c}{79.63}  
& \multicolumn{1}{c}{79.58}\\
% + ERP         & \multicolumn{1}{c}{71.55\tiny{(+1.19)}}             
% & \multicolumn{1}{c}{79.96\tiny{(+0.33)}}  
% & \multicolumn{1}{c}{80.77\tiny{(+1.19)}}\\
+ RP \& ERA     & \multicolumn{1}{c}{71.10\small{\textbf{\textcolor{blue}{$\uparrow$0.74}}}}      
& \multicolumn{1}{c}{79.87\small{\textbf{\textcolor{blue}{$\uparrow$0.24}}}}  
& \multicolumn{1}{c}{81.13\small{\textbf{\textcolor{blue}{$\uparrow$1.55}}}}   \\
+ RP \& ETA       & \multicolumn{1}{c}{70.59\small{\textbf{\textcolor{blue}{$\uparrow$0.23}}}}      
& \multicolumn{1}{c}{80.10\small{\textbf{\textcolor{blue}{$\uparrow$0.47}}}} 
& \multicolumn{1}{c}{80.85\small{\textbf{\textcolor{blue}{$\uparrow$1.27}}}}    \\ 
+ RP \& ERA \& ETA    & \multicolumn{1}{c}{71.96\small{\textbf{\textcolor{blue}{$\uparrow$1.60}}}}    
& \multicolumn{1}{c}{80.19\small{\textbf{\textcolor{blue}{$\uparrow$0.56}}}}    
& \multicolumn{1}{c}{81.33\small{\textbf{\textcolor{blue}{$\uparrow$1.75}}}}         \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{The F1 score of Discontinuous NER datsets based on different methods} 
\label{t4.dis_alation}
\end{table}
\subsection{Entity Boundary Recognition}
% 该部分分析模型对于实体边界识别的影响，评价指标是F1 score.
% 计算时我们不考虑实体类型是否预测正确，仅仅考虑实体边界的识别。例如，图一中第1，假如预测“Delphis Hanover”为“LOC”，但是这个实体预测对了，那么我们便考虑实体边界预测正确。如图一中第2个例子，在预测长实体“an ignorant interloper who stole the White House”时，如果预测出“interloper”或者其他，我们便任务实体边界预测错误。
% 我们分别在三种类型实体数据集CoNLL2003,ACE2004,ACE2005,CADEC做了实验，实验如图示
% 从图中可以看出实体关系信息和实体类型信息的融入对于模型识别实体都能起到一定作用。
In this section, we conduct additional experiments to further analyze the effectiveness of our model on entity boundary recognition. 
In this set of experiments, we only focus on entity boundary detection regardless of entity type prediction.
For example, even though the generated entity type of ``Delphis Hanover'' is ``LOC'', which is incorrect, the entity boundary is correct generated.
% if the prediction is ``Delphis TAG'' or ``Hanover TAG'' $\left( \right .$TAG is in entity types set$\left . \right)$, we consider the entity is incorrect.  In the second example in Figure 1, when predicting the long entity "an ignorant interloper who stole the White House", if "interloper" or others are predicted, we will predict the boundary of the task entity incorrectly.
If the prediction is ``Delphis TAG'' or ``Hanover TAG'' (TAG is an entity type), we consider the entity boundary generation is incorrect. For another example in Figure \ref{Fig.entity}, any subset generation of the long entity ``an ignorant interloper who stole the White House'' will be viewed as wrong prediction, e.g., ``interloper''.
The F1 scores of different model ablations on four selected datasets are shown in Figure \ref{Fig.ablation}. 
It can be seen from the results that both entity relation attention and entity type attention have a positive impact on entity boundary detection, where the entity relation attention is more important compared to entity type attention. This is consistent with our expectation as entity relations are more relevant to the entity boundary generation. Similar observations have been found on the other datasets.
%Jointing entity relation and entity type attention in generative NER model has stronger powerful effect.
\subsection{Impact of Multi-task Learning}
% 该部分是分析多任务学习损失函数超参对模型的影响。
% 为了分析多任务损失函数超参多模型的影响，我们分别针对三种实体类型数据集CoNLL2003,ACE2005,CADEC做了分析实验. $W$的取值范围是[1.0,0.9,0.8,07,0.6,0.5,0.4,0.3,0.2,0.1].
% 实验结果如图所示。从途中可以看出，实体关系预测的损失函数。。。
% \begin{figure}[t] 
% 	\centering 
% 	\includegraphics[width=0.35\textwidth]{figures/w_ablation.jpg} 
% 	\caption{The impact of the hyperparameter $w$ on datasets} 
% 	\label{Fig.w_ablation} 
% \end{figure}
%We further conduct a set of parameter sensitivity experiments, with respect to $\alpha$ and $\beta$ on both data splits of SQuAD, to evaluate the robustness of the proposed approach. In each experiment, we tune only one parameter from \{0, 0.01, 0.05, 0.1, 0.2, 0.4, 0.8\}, while fixing the other parameter to the value as described in our implementation details. We find that the performance of QG+AQ+UG is relatively stable with respect to $\alpha$ and $\beta$. We also observe similar results of the proposed method in terms of the other two metrics.


In order to analyze the impact of the multi-task learning, we conduct a set of experiments on CoNLL2003, ACE2005 and CADEC datasets. 
In each experiment, we modify the weight parameter $w$ from \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}, while fixing the other hyperparameters to the values as described in our implementation details. Essentially, the value of $w$ controls the importance of the two tasks.
The model performances with different task weights are shown in Figure \ref{Fig.w_ablation}. 
We find that our model achieves the best performance when $w$ is around 0.3 on all datasets. It can also been seen that the model performance drops with the increasing of $w$. Our hypothesis is that the entity generation task plays a more important role compared to the entity relation prediction task.
Therefore, it is crucial to identify a good balance between the two tasks.
% \subsection{Case Study}
% % 为了分析多任务生成模型with relation and type attention, 我们通过三个例子，这三个例子分别来自flat 数据集conll2003, nested 数据集ACE2005 和discontinous 数据集CADEC。在第一个例子中，model首先预测“XXX” as "",然而。。。。
% Figure \ref{Fig.entity} show an example of generated entity base model \citet{yan2021unified} proposed and our model. 
% It can be seen that our model generates accurate entity entities because of entity relationship attention, and entity type attention make the model aware of entity boundary information and entity type information. 

% Figure X shows an example of the predicted entities using the methods in section 6.1. 
% This example more clearly shows the role of entity relationship attention and entity type attention for entity recognition. 
% The entity-relationship attention in +Relation can perceive the boundary, so the entity recognition is correct. Due to the less perception of entity type information, the entity type recognition error is caused. 
% +Label is just the opposite of +Relation. 
% Entity type attention makes the model possess the information between word and entity type so that the entity type recognition is correct, and the entity boundary recognition is mistaken for missing entity boundary information. 
% +Relation \& Label absorb both simultaneously to generate the correct entity.