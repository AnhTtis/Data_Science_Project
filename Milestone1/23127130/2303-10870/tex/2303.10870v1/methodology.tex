\section{Methodology}
% We formally define the problem of seq2seq named entity recognition in this section.
% Given a sentence of $n$ tokens $X = \{x_{1},x_{2},\dots,x_{n}\}$, the goal of seq2seq NER is to generate a target sequence $Y = \{s_{i1}, e_{i1}, s_{i2}, e_{i2},\dots, s_{ik}, e_{ik}, t_{i},\dots\}$. Here $s$ and $e$ are the start and end indices of a span. $t \in \{g_1,\dots,g_l\}$ is the entity type, where $l$ is the total number of entity types.  
% Note that an entity may contain multiple spans (for discontinuous NER). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t] 
	\centering 
	\includegraphics[width=0.9\columnwidth]{figures/full_2.png} 
	\caption{The architecture of our multi-task Transformer model, which contains relation prediction and entity decoding tasks. We incorporate the entity token relation attention into the cross-attention layer of the decoder. The entity type attention is adopted to both the encoder and decoder.} 
	\label{Fig.frame} 
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approach Overview}
% 本文的多任务包括实体识别和实体关系预测。
% 实体识别任务即输入句子序列，通过生成模型编码解码后最后生成实体和实体类型。
% 在这篇文章中，我们提出了多任务生成模型。实体生成是主要任务即输入句子序列，通过生成模型编码解码后最后生成实体和实体类型。为了让模型能学习到更多信息比如实体边界信息，我们增加了一个实体关系预测任务。为此我们将实体边界识别转化为实体token pair关系的预测，这种可以表达实体之间是否是属于同一实体，同时也可以体现出实体token是否是实体的首尾。
% 这个任务中，我们定义了三种关系，分别如下示
% Begin-End: token pair 属于同一实体，且分别是实体的第一个word和最后一个word。
% *-Middle:token pair 属于同一实体，其中一个token是实体的中间word，另外一个token可以是实体的第一个word，也可以是最后一个word，当然也可以是实体的中间word。
% None: token pair 的关系构不成同一实体，即token pair不属于同一个实体。
% 举例说明上述三种关系
% 阐述通过CLN方法获取token pair关系表示，以及预测实体token pair关系的loss公式
% In this work, we propose a multi-task Transformer model that incorporates the entity token relation and entity type into the modeling.
% Figure \ref{Fig.frame} shows the overall model architecture.
% Our model consists of an encoder that encodes the input sequence to its contextual embedding, and a decoder that generates the output sequence with entity annotations. 
% In addition to the entity generation task, we design a relation prediction task over the entity token pairs to better capture the correlations among the entities. 
% The entity token relation attention and entity type attention are introduced and fused into the encoder and decoder.
% We present the detail of each component separately in the following subsections.
%In this work, we propose a multi-task Transformer model that incorporates the entity token relation and entity type into the modeling.
Figure \ref{Fig.frame} shows the overall model architecture.
Our model consists of an encoder that encodes the input sequence to its contextual embedding and a decoder that generates the output sequence with entity annotations. 
In addition to the entity generation task, we design a relation prediction task over the entity token pairs to better capture the correlations among the entities. 
The entity token relation attention and entity type attention are introduced and fused into the encoder and decoder.
We present the detail of each component separately in the following subsections.
\subsection{Relation Prediction Task}
% The purpose of the relation prediction task is to learn better entity token representations for improving entity boundary detection.
The relation prediction task aims to learn better entity token representations for improving entity boundary detection.
Specifically, the relationship between a pair of tokens can be divided into three categories:
%which can express whether entities belong to the same entity, and can also reflect whether the entity token is the beginning and end of the entity.
%In entity token pair relation prediction task, we define three relationships to enhance the representation understanding of the model as follows:
% \begin{itemize}
% \item \textbf{Begin-End}, which indicates that the token pair belongs to the same entity. 
% % One token is the begin/start token, with the other one the end token.
% One token is the begin token and the other one the end token.
% \item \textbf{*-Inside}, which represents that the token pair belongs to one entity, and the two tokens are the * and inside of the entity respectively (* means begin, inside or end). 
% \item \textbf{None}, the token pair does not have any correlation.
% \end{itemize}

\noindent $\bullet$ \textbf{Begin-End}, which means that the token pair belongs to the same entity. 
% One token is the begin/start token, with the other one the end token.
One is the begin token, the other is the end token.

\noindent $\bullet$ \textbf{*-Inside}, which represents that the token pair belongs to one entity, and the two tokens are the * and inside of the entity respectively (* means begin, inside or end). 

\noindent $\bullet$ \textbf{None}, the token pair does not have any correlation.

\noindent For example, ``aching in shoulders'' is a discontinuous entity in ``I am having aching in the legs and shoulders''.
%detecting the discontinuous entity ``aching in shoulders'' from the sequence ``I am having aching in legs and shoulders'' effectively capturing the relations between the adjacent words segments of ``aching in'' and ``shoulders'' are indispensable. 
% We consider the token pair of (``aching'', ``shoulders'') to be the begin and end of one entity, i.e., Begin-End. 
We consider the token pair (``aching'', ``shoulders'') to be the begin and end of one entity, i.e., Begin-End. 
The token pairs (``aching'', ``in'') and (``in'', ``shoulders'') are of the *-Inside relation. 
% These relations among the entity tokens are useful knowledge, which enable the model to understand the latent information of entity token pairs for identifying entity boundaries.  
These relations among the entity tokens are useful to enable the model to understand the latent information of entity boundaries in entity token pairs.

We adopt the CLN mechanism \cite{Yu2021} to model the relations between token pairs, which could be seen as $\{r_{ij}|(i,j\in[1,N])\} \in \Bbb{R}^{N\times N\times d_h}$, where $r_{ij}$ denotes the relation representation of the token pair $(x_i,x_j)$. 
% Specifically, a conditional vector is introduced as the extra contextual information to generate the gain parameter $\gamma$ and bias $\lambda$ of the layer normalization mechanism \cite{Ba2016} as follows:
Specifically, a conditional vector is introduced as the extra contextual information to generate the gain parameter $\gamma$ and bias $\lambda$ of the layer normalization mechanism as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
% r_{ij} &= CLN(h_i,h_j) \\ &= \gamma_{ij} \odot (\frac{h_j-\mu }{\sigma}) + \lambda_{ij}
r_{ij} &= CLN(h_i,h_j) &= \gamma_{i} \odot (\frac{h_j-\mu }{\sigma}) + \lambda_{i}
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
\gamma _{ij} = W_\alpha h_i + b_\alpha, \lambda_{ij} = W_\beta h_i + b_\beta
% \gamma _{i} = W_\alpha h_i + b_\alpha, \lambda_{i} = W_\beta h_i + b_\beta
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
\mu =\frac{1}{d_h}\sum_{k=1}^{d_h}h_{jk},
\sigma = \sqrt{\frac{1}{d_h}\sum_{k=1}^{d_h}(h_{jk}-\mu)^2},
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% where $h_i$ is represetation of the token $x_i$ by the encoder and the condition to gain the parameter $\gamma_{ij}$ and $\lambda _{ij}$. $\mu$ and $\sigma$ are the mean and standard devilation taken across the elements of $h_{j}$ respectively. $h_{jk}$ means the k-th dimension of  $h_{j}$ and $N$ is the sequence length.
where $h_i$ is represetation of the token $x_i$ by the encoder. $\mu$ and $\sigma$ are the mean and standard devilation taken across the elements of $h_{j}$ respectively. $h_{jk}$ means the k-th dimension vector of $h_{j}$ and $N$ is the sequence length.

% The prediction of token pair relation is calculated as follows:
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{equation}
% \begin{aligned}
% p_{r_{ij}} = Softmax(MLP(r_{ij}))
% \end{aligned}
% \end{equation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In order to alleviate the unbalanced distribution of token pair relations, we use the focal loss \cite{lin2017focal} as the objective in the relation prediction task:
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{equation}
% \begin{aligned}
% L_{relation}=-\alpha(1-p_{r_{ij}})^{\tau}log(p_{r_{ij}})
% \end{aligned}
% \end{equation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% where $\alpha,\tau $ are the hyperparameters. By minimizing the above loss, we can extract the entity relation features of all token pairs in the input sequence. 
The prediction of token pair relation is calculated by MLP \cite{Liu_MLP}, And we use focal loss in order to alleviate the unbalanced distribution of token pair relations. Formulas as follows:
% MLP calculates the prediction of token pair relation, And we use focal loss to alleviate the unbalanced distribution of token pair relations. Related formulas as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
p_{r_{ij}} = Softmax(MLP(r_{ij}))
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
L_{relation}=-\alpha(1-p_{r_{ij}})^{\tau}log(p_{r_{ij}}),
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $\alpha,\tau $ are the hyperparameters. 
% By minimizing the above loss, we can extract the entity relation features of all token pairs in the input sequence. 
% \subsection{Entity Relation Attention}
\subsection{Token Relation Attention}
% 需要扩展，篇幅太少
% 为了让模型感觉和学习到更多的实体边界信息，我们将实体边界信息转化为实体token pair关系，这些关系不仅能够表现出两个word是否是否是实体begin-end,也能体现出实体内部token之间的潜在联系。这种关系的识别显然对于实体在边界感知上的识别能够起到一定作用。在生成NER模型中融入token pair关系表示时，我们想到了通过attention机制受 \cite{WangYu2020}启发。关系表示以attention的方式结合到原有生成模型中，一方面不会改变生成模型框架结构，另一方面这种方式可以在模型解码时抽取更加准确的边界信息增强实体是被效果。
% 为了尽可能保证预训练模型结构不发生改变，我们模仿编码器将hidden state送入到encoder端的cross attention方式，将关系表示以类似的方式融入到生成模型中综合利用实体边界相关信息增强实体识别能力。
% 
%We convert the entity boundary into entity token pair relationships to make the model perceive and learn more entity boundary information, which plays a role in the identification of entities. 
%These relationships can not only show explicit boundary tokens but also indicate the internal connection between tokens within the boundary. 
%For example, the second sentence in Figure \ref{Fig.entity} has complex entities. 
%Two different types of entities are nested in the long entity ``an ignorant interloper who stole the White House''. The entity relation we defined above can clearly show that ``an-House'', ``an-interloper'' and ``the-House'' are the begin-end of these three entities. If we incorporate these information which the original model cannot directly feel into the existing generative model, the model can use these entity relationships to identify entity boundaries exactly.
%\sout{When incorporating the token pair relation representation in the generative NER model, we think of the attention mechanism} 
With the guidance of the relation prediction task, the boundary information is embedded into the entity relation representations $r_{ij}$. 
These representations are valuable knowledge for the model to perceive and learn more accurate entity boundary,  which plays an important role in identifying entities.
Inspired by HIT \cite{WangYu2020}, we incorporate the relationship of the token pair into cross-attention layers in the decoder. 
% In this way, the structure of the pre-training model are preserved, and entity recognition can be enhanced by comprehensively leveraging the explicit entity relation information. 
This way, the pre-training model's structure is preserved, and entity recognition can be enhanced by comprehensively leveraging the explicit entity relation information. 
%We use ERA for abbreviation this method.

% Formally, given the entity relation representations $r_{ij}$, we extract and transform the feature to a vector form using a Convolutional Neural Network \cite{chiu2016}. 
% This feature extraction operation can filter out useless information for entity boundary recognition. 
Given the relation representations $r_{ij}$, we extract and transform the feature to a vector form using a Convolutional Neural Network \cite{chiu2016} to filter out useless information for entity boundary recognition. 
Finally, the obtained entity token relation key $K_{R}$ and value $V_{R}$ matrices are concatenated with the original key $K$ and value $V$ respectively in the cross-attention layers, to enhance the model generation on entity boundaries. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The entity relation attention is defined as:
\begin{equation}
\begin{aligned}
R_{ij}^*=Convolution(r_{ij})
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
head^l&=Attn(Q^l,cat(K_{R}^l,K_{T}^l,K^l), cat(V_{R}^l,V_{T}^l,V^l)),
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $K_{R}^l$, $V_{R}^l,$ are from $R_{ij}^*$, $head^l$ is the $l-$th layer's head vector. $K_{T}^l,V_{T}^l,$ are calculated from the entity type embeddings described in the following section.

\subsection{Entity Type Attention}
% 该部分引入实体类型信息主要是想通过attention机制来让token和实体类型交互，这种交互可以使得模型增强实体类型的识别。
% 我们首先通过外部知识库获取到实体类型下的实体，将实体类型下的实体embedding相加得到一个综合的实体类型embedding。
% 举例说明是实体类型embedding具体实现。
% 获取到实体类型embedding后，受\cite{prefixtuning}启发，我们通过MLP方法对实体类型embedding变形获取$P_{T-k}$ ,$P_{T-V}$,将$P_{T-k}$ ,$P_{T-V}$于encoder、decoder每层self attention中的key,value拼接。拼接后的key,value最后self attention的最终key,value. 
Heterogeneous factors such as entity boundaries and types \cite{Yu2020,fu2021nested,aly2021type} have impact on entity recognition.
In this section, we discuss the modeling of entity types in the our seq2seq NER model, allowing interactions with the inputs and guiding the model to learn more effective token representation and entity recognition. These entity type representations are incorporated into both the encoder and decoder of our model through entity type attention. 

The representation of an entity type $t$ is the weighted sum of entity representations obtained from an external entity base. 
For example, assuming the entity type set $T=\{ person,location,organization,other \}$, entity type $t$ = $location$ contains entities $C_{t} = \{Beijing, Athens, London, \\ New York\}$, the initial entity type representation could be obtained as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
E_t=\theta_1 E_{C_t^{1}} + ......+ \theta_iE_{C_t^{i}},
\end{aligned}
\label{fun.ET}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% where $E_{C_t^{i}}$ is the embedding of $C_t^{i}$, $i$ is the index of the entity in the entity set, $\theta_i$ is the weight of the entity computed from its frequency.
where $E_{C_t^{i}}$ is the embedding of $C_t^{i}$ ,and $C_t^{i}$ is $i-$th entity in $C_{t}$, $\theta_i$ is the entity's weight computed from its frequency.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[t] 
% 	\centering 
% 	\includegraphics[width=0.9\columnwidth]{figures/entity-type.png} 
% 	\caption{Entity type embedding with external entity knowledge. } 
% 	\label{Fig.entity-type}
%  %\vspace{-2mm}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% To leverage the entity type information in our Transformer model, we design a entity type attention in both the encoder and decoder and integrate it into the all corresponding self-attention layers. 
To leverage the entity type information in our model, we design a entity type attention in both the encoder and decoder and integrate it into the all corresponding self-attention layers. 
%, and we use ETA for abbreviation this method. 
Two sets of the vectors $K_{T}$ and $V_{T}$ are introduced to represent the key and value of the entity type, which are concatenated with the original key $K$ and value $V$ vectors as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
head^l &=Attn(Q^l,cat(K_{T}^l,K^l),cat(V_{T}^l,V^l)),
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% where $head^l$ is the $l-$th layer's head vector. $K_{T},V_{T}$ are obtained from the embedings $E_T$ by an MLP layer. 
where $head^l$ is the $l-$th layer's head vector. $K_{T},V_{T}$ are obtained from the embedings $E_T$, and $E_T$ is obtained by an MLP layer after formula \ref{fun.ET}. 
% $K_{T}$, $V_{T}$ are split into $N_h$ head vectors respectively $K_{T}^l$, $V_{T}^l$, We apply this to all self-attention layers. 
$K_{T}$, $V_{T}$ are split into $N_h$ head vectors respectively $K_{T}^l$, $V_{T}^l$. 
\subsection{Entity Generation}
%我们采用seq2seq生成模型生成实体及实体类型。
%输入句子至编码器，获取hidden state。
% The decoder decodes the token embeddings from the encoder to generate the entities. 
% At each step $t$, the decoder obtains the token embedding $h^d_t$ based on the encoder output and all the previous decoded tokens as:
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{equation}
% \begin{aligned}
% h_t^d =Decoder(H^e,Y_{<t}^\wedge)
% \end{aligned}
% \end{equation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% where $Y_{<t}^\wedge =\left [ y_1^\wedge ,...,y_{t-1}^\wedge  \right ]$ is the generated indexes before $t$. 

% Finally, the output token index distribution $P_t$ is obtained by the dot product of $h_t^d$ between $H^e$ and $E_T$ respectively.

% \begin{equation}
% \begin{aligned}
% P_t=Softmax(\left [ H^{e}\otimes h_t^d;E_{T}\otimes h_t^d\right ] )
% \end{aligned}
% \end{equation}
% where $E_{T}$ is the embedding of the entity types. The embeddings are shared between the encoder and decoder. $\otimes$ denotes the dot product and $[.;.]$ means concatenation.
The decoder decodes the token embeddings from the encoder to generate the entities. 
At each step $t$, the decoder obtains the token embedding $h^d_t$ based on the encoder output. Finally, the output token index distribution $P_t$ is obtained by the dot product of $h_t^d$ between $H^e$ and $E_T$ respectively.
All the previous decoded tokens $h^d_t$ and $P_t$ as:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
h_t^d =Decoder(H^e,Y_{<t}^\wedge)
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
P_t=Softmax(\left [ H^{e}\otimes h_t^d;E_{T}\otimes h_t^d\right ] ),
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $Y_{<t}^\wedge =\left [ y_1^\wedge ,...,y_{t-1}^\wedge  \right ]$ is the generated indexes before $t$. $E_{T}$ is the embedding of the entity types. The embeddings are shared between the encoder and decoder. $\otimes$ denotes the dot product and $[.;.]$ means concatenation.

% For a given sequence $X = \{x_{1},x_{2},...,x_{n}\}$, we learn to minimize the negative log-likelihood with respect to the corresponding ground-truth labels, which can be written as:
% We learn to minimize the negative log-likelihood with respect to the corresponding ground-truth labels, which can be written as:
% We learn to minimize the negative log-likelihood with respect to the corresponding ground-truth labels, which can be written as:
% \begin{equation}
% \begin{aligned}
% % L_{entity}= -\log{p(Y|X)}
% L_{entity}= -\log{p(Y|X)}
% \end{aligned}
% \end{equation}
% By combining the above entity generation loss and the entity relation loss, our final loss is defined as follows:
% \begin{equation}
% \begin{aligned}
% L = L_{entity}+ wL_{relation}
% \end{aligned}
% \end{equation}
% where $w$ is the hyperparameter to balance the two terms.
We learn to minimize the negative log-likelihood with respect to the corresponding ground-truth labels, and combine the entity generation loss and the above entity token relation loss as our final loss. The formulas are as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
% L_{entity}= -\log{p(Y|X)}
L_{entity}= -\log{p(Y|X)}
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\begin{aligned}
L = L_{entity}+ wL_{relation},
\end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $w$ is the hyperparameter to balance the two terms.
