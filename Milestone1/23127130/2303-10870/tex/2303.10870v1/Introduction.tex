\section{Introduction}
\label{sec:intro}
% Named entity recognition (NER) is a fundamental research problem in natural language processing, which has been widely adopted in information retrieval and question answering systems ~\cite{molla2006-nerqa,li2014-incrementaljoint,wang2020-pyramid,chen2021-IE-frustratingly,li2020b}.
Named entity recognition (NER) is a fundamental research problem in natural language processing, which has been widely adopted in information retrieval and question answering systems ~\cite{molla2006-nerqa,li2014-incrementaljoint,chen2021-IE-frustratingly,li2020b}.
The NER task can be categorized into three categories, including flat NER, nested NER and discontinuous NER.
% as shown in Figure \ref{Fig.entity}.
% Previous works ~\cite{li2020b,Lample2016,strubell2017fast,2018bidirectional,wang-lu-2019,Yu2021} address NER tasks with the task-specific token-level sequential labeling or span-level classification methods.
Previous works ~\cite{li2020b,Lample2016,strubell2017fast,2018bidirectional,wang-lu-2019} address NER tasks with the task-specific token-level sequential labeling or span-level classification methods.
In token-level sequential labeling methods, each token is assigned a label to represent its entity type.
On the other hand, span-level classification methods enumerate all possible spans in the sentences and classify them into pre-defined entity types.
One main drawback of these methods is that they are not able to tackle all three NER tasks concurrently, but build separate models for different categories.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[t] 
% 	\centering 
% 	\includegraphics[width=1.0\linewidth]{figures/entity.png} 
% 	\caption{Examples of three types of NER.} 
% 	\label{Fig.entity}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, sequence-to-sequence (seq2seq) generative approaches ~\cite{yan2021unified,zhang-debias2022} have been proposed to jointly model all NER tasks in a unified framework, which attracts a lot of attention in the NER community. 
Although achieving promising results on all three NER categories, there are two major limitations of these generative models. 
First, they are not effective at detecting entity boundaries. Generative models achieve the entity recognition task through an auto-regressive decoding process, where the token-relations (e.g., whether two tokens belong to a same entity or not) are not captured in seq2seq models. 
% For example, in the discontinuous NER in Figure \ref{Fig.entity}, existing seq2seq models generate coarse-grained entities while our model obtains more fine-grained entities. 
% Second, entity-type relations are not explicitly considered in the seq2seq framework. The entity type generation is based on its compounding tokens, and thus could be misguided if the compounding tokens are not generated correctly from the decoder.
% Therefore, it is an important problem to incorporate the entity boundaries and entityt ype mapping information into the seq2seq NER models.
Second, entity-type relations are not explicitly considered in the seq2seq framework. The entity type generation is based on its compounding tokens and thus could be misguided if the compounding tokens are not generated correctly from the decoder.
Therefore, it is a critical problem to incorporate the entity boundaries and entity type mapping information into the seq2seq NER models.

In this paper, we propose a multi-task Transformer with relation attention and type attention, which introduces an additional entity boundary detection task into the seq2seq NER model.
More concretely, we achieve the purpose of entity boundary recognition by detecting the head, middle and tail tokens of entities.
To enhance the mapping capability between the entities and their corresponding types, we further incorporate entity-type information by leveraging an external knowledge base (i.e., Wikipedia).
The entity-type distributions could be integrated into the encoder and decoder conveniently with self-attention and cross-attention mechanisms.
Our contributions can be summarized as follows: (1) We propose a multi-task Transformer model for seq2seq NER, which incorporates the entity boundary recognition task into the named entity recognition task, and can unify the three different NER categories.
(2) We introduce two novel attention mechanisms, including entity token relation attention and entity type attention, which improve the performance of the seq2seq NER model.
% \item We conduct extensive experiments to demonstrate our model's effective entity boundary detection and entity type classification capabilities compared with several state-of-the-art baselines. 
(3) We conduct extensive experiments to demonstrate our model's effective capabilities compared with several state-of-the-art baselines.
% \begin{itemize}
% \item We propose a multi-task Transformer model for seq2seq NER, which incorporates the entity boundary recognition task into the named entity recognition task, and can unify the three different NER categories.
% \item We introduce two novel attention mechanisms, including entity token relation attention and entity type attention, which improve the performance of the seq2seq NER model.
% % \item We conduct extensive experiments to demonstrate our model's effective entity boundary detection and entity type classification capabilities compared with several state-of-the-art baselines. 
% \item We conduct extensive experiments to demonstrate our model's effective capabilities compared with several state-of-the-art baselines.
% \end{itemize}