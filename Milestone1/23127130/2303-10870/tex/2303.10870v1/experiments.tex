\section{Experiments}
\subsection{Datasets}
% We evaluate \textbf{flat NER datasets} on the CoNLL-2003 \cite{SangM03} following the same settings in previous works \cite{Lample2016,Yu2020} and OntoNotes 5.0 \cite{Sameer2013b} which is adopted the splits as used in \cite{yan2021unified,Yu2020}.
% And we experiment on \textbf{nested NER datasets} GENIA \cite{Kim2003} following \cite{WangYu2020,Shibuya2020}, ACE 2004 \cite{Doddington2004}, ACE 2005 \cite{walker2005ace} in english following \cite{yan2021unified,Yu2020}. We conduct experiment on \textbf{discontinuous NER datasets} CADEC \cite{KARIMI2015}, ShARe13 \cite{pradhan2013task} and ShARe14 \cite{mowery2014task} corpus and use \cite{Dai2020-effective} to process the data.
We evaluate \textbf{flat NER datasets} on the CoNLL-2003 following the same settings in previous works \cite{Lample2016,Yu2020} and OntoNotes 5.0 which is adopted the splits as used in \cite{yan2021unified,Yu2020}.
And we experiment on \textbf{nested NER datasets} GENIA following \cite{yan2021unified,WangYu2020,zhang-debias2022}, ACE 2004, ACE 2005 in english following \cite{yan2021unified,Yu2020}. We conduct experiment on \textbf{discontinuous NER datasets} CADEC, ShARe13 and ShARe14 corpus and use \cite{Dai2020-effective} to process the data.
\subsection{Implementation Details}
% We adopt BART-Large as the backbone network which has 12 layers with 1024 dimensional embeddings for all experiments. 
We adopt BART-Large as the backbone network for all experiments.
We use AdamW \cite{Adaw2017} optimizer and the learning rate is $1e^{-5}$ for the BART-Large model and $5e^{-5}$ for other components. 
Batch size is 32 for Ontonotes 5.0 and 16 for the other datasets. 
The hyperparameters $w$ is within this range $[0.1,0.9]$. 
When the key $K_{T}$ and value $V_{T}$ of self-attention is obtained by entity type embedding, we use a MLP similar to the proj\_down-proj\_up architecture \cite{prefixtuning} with down dim 512 and up dim 1024. 
After obtaining the entity token relation representation, we reduce the dimension to 256 and use a convolution layer to convert it to the key and value required by the attention mechanism. 
$\alpha$ and $\tau$ in the focal loss of the entity relation are set to 5 and 1 respectively. 
%\subsection{Evaluation Metrics}
%An entity is correctly predicted if the entity label and boundary match the ground truth. 
% Following the prior work \cite{lu2015,yan2021unified,zhang-debias2022}, we compute the precision (P), recall (R) and F1 (F) scores on each dataset. 
% Our models are tuned according to the F1 scores on the development set. 
% We repeat each experiment 10 times and report the metrics based on the averages.
% Following the prior work \cite{yan2021unified,zhang-debias2022,lu2015}, we compute the precision (P), recall (R) and F1 (F) scores on each dataset and tune model according to the F1 score on the development set. 
% Following the prior work \cite{yan2021unified,zhang-debias2022,lu2015}, we compute the precision (P), recall (R) and F1 (F) scores and tune model according to the F1 on the development set.
Following the prior work \cite{yan2021unified,zhang-debias2022}, we compute the precision (P), recall (R) and F1 (F) scores and tune model according to the F1 on the development set. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[t] 
% 	\centering 
% 	\includegraphics[width=0.3\textwidth]{figures/avg_f.jpg} 
% 	\caption{The average F1 scores over all datasets.} 
% 	\label{Fig.avg_f} 
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\arrayrulecolor{black}
\begin{adjustbox}{width=0.85\linewidth,center}
\begin{tabular}{lllllll} 
\toprule
\multirow{2}{*}{Model}  & \multicolumn{3}{c}{CoNLL2003} & \multicolumn{3}{c}{OntoNotes 5.0}            \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
                        &\multicolumn{1}{c}{P}     & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}            
                        & \multicolumn{1}{c}{P}    & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}      \\ 
\hline
Lample et al. (2016) \cite{Lample2016}      & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & 90.94        
                        & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & \multicolumn{1}{c}{-}      \\
Strubell et al. (2017) \cite{strubell2017fast} & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & 90.65         
                         & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & 86.84  \\
% \cite{wang-lu-2019}    & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}    & 90.50          
%                         & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & \multicolumn{1}{c}{-}      \\
Strakova et al. (2019) \cite{strakova2019}    & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & 92.98         
                        & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}     & \multicolumn{1}{c}{-}      \\
Yu et al. (2021) \cite{Yu2021}          & \underline{92.91}     & 92.13    & 92.52         
                        & \underline{90.01}     & 89.77    & 89.89  \\ 
Yan et al. (2021) \cite{yan2021unified}  & 92.61   & \textbf{93.87}  & \underline{93.24}        
                        & 89.99   & 90.77           & 90.38\\
Zhang et al. (2022) \cite{zhang-debias2022}& 92.78   & 93.51           & 93.14   
                        & 89.77   & \underline{91.07}   & \underline{90.42}   \\ 
\arrayrulecolor{black}\hline
Ours                    &\textbf{93.11}  &\underline{93.77}  &\textbf{93.44} 
                        &\textbf{90.18}  &\textbf{91.08}  &\textbf{90.63}   \\
\bottomrule
\end{tabular}
\end{adjustbox}
% \caption{Comparison on Flat NER datasets. Results are statistically significant with p-value $<$ 0.005.}
\caption{Results for Flat NER datasets.}
\label{t1}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\arrayrulecolor{black}
\begin{adjustbox}{width=1.0\linewidth,center}
\begin{tabular}{llllllllll} 
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{ACE2004}   & \multicolumn{3}{c}{ACE2005}  & \multicolumn{3}{c}{GENIA} \\ 
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
                       & \multicolumn{1}{c}{P}       & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}       
                       & \multicolumn{1}{c}{P}       & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}     
                       & \multicolumn{1}{c}{P}       & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}       \\ 
\hline
Ju et al. (2018)\cite{ju2018neural}   & \multicolumn{1}{c}{-}    & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-} 
                       & 74.20  & 70.30  & 72.20   
                       & 78.50  & 71.30  & 74.70   \\
% \cite{wang-lu-2019}   & 78.00  & 72.40  & 75.10  
%                        & 76.80  & 72.30  & 74.50   
%                        & 77.00  & 73.30  & 75.10   
%                        \\
Strakova et al. (2019) \cite{strakova2019}   & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}     & 84.33  
                       & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}     & 83.42   
                       & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}     & 78.20   
                       \\
Wang et al. (2020b) \cite{WangYu2020}     & 86.08  & \underline{86.48}  & 86.28  
                       & 83.95  & 85.39  & 84.66   
                       & 79.45  & \underline{78.94}  & \underline{79.19}   
                       \\
Yu et al. (2021) \cite{Yu2021}         & 85.42  & 85.92  & 85.67  
                       & \underline{84.50}  & 84.72  & 84.61   
                       & 79.43  & 78.32  & 78.87   
                       \\ 
Yan et al. (2021) \cite{yan2021unified} & \textbf{87.27}  & 86.41  & \underline{86.84}  
                       & 83.16  & 86.38  & 84.74   
                       & 78.57  & \textbf{79.30}  & 78.93
                       \\
Zhang et al. (2022) \cite{zhang-debias2022} & 86.36  & 84.54  & 85.44   
                         & 82.92  & \underline{87.05}  & \underline{84.93}   
                         & \underline{81.04}  & 77.21  & 79.08  
                         \\ 
\arrayrulecolor{black}\hline
Ours                   & \underline{86.80}           & \textbf{87.94}  & \textbf{87.36}
                       &\textbf{84.85}   & \textbf{87.46}   & \textbf{86.14}   
                       &\textbf{81.27}   &78.33            & \textbf{79.77}  
                       \\
\bottomrule
\end{tabular}
\end{adjustbox}
% \caption{Comparison on Nested NER datasets. Results are statistically significant with p-value $<$ 0.005.}
\caption{Results For Nested NER datasets.}
\vspace{-2mm}
\label{t2}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht]
\arrayrulecolor{black}
\begin{adjustbox}{width=1.0\linewidth,center}
\begin{tabular}{llllllllll} 
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{CADEC}   &\multicolumn{3}{c}{ShARe13}  & \multicolumn{3}{c}{ShARe14}  \\ 
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
                       & \multicolumn{1}{c}{P}       & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}       
                       & \multicolumn{1}{c}{P}       & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}     
                       & \multicolumn{1}{c}{P}       & \multicolumn{1}{c}{R}      & \multicolumn{1}{c}{F}       \\ 
\hline
Metke-Jimenez et al.  \cite{metke2016concept}      & 64.40  & 56.50  & 60.20      
                       & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}                  
                       & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}\\
Tang et al. (2018) \cite{tang2018}       & 67.80  & 64.99  & 66.36      
                       & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}                  
                       & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-} \\
Wang and Lu (2019) \cite{wang-lu-2019}   & \textbf{72.10}  & 48.40  & 58.00      
                       & \underline{83.80}  & 60.40  & 70.30      
                       & \textbf{79.10}  & 70.70  & 74.70    \\ 
Dai et al. (2020) \cite{Dai2020-effective} & 68.90  & 69.00  & 69.00 
                          & 80.50  & 75.00  & 77.70      
                          & 78.10   & 81.20  & 79.60       \\
Wang et al. (2021) \cite{wang2021clique} & 70.50  & \textbf{72.50}  & 71.50      
                       & \textbf{84.30}  & \underline{78.20}  & \textbf{81.20}      
                       & 78.20 & \textbf{84.70} & \underline{81.30}    \\
Yan et al. (2021) \cite{yan2021unified} & 70.08  & 71.21  & 70.64      
                       & 82.09  & 77.42  & 79.69      
                       & 77.20  & 83.75  & 80.34     \\
Zhang et al. (2022) \cite{zhang-debias2022}   & 71.35  & 71.86  & \underline{71.60}  
                           & 81.09  & 78.13  &79.58       
                           & 77.88  & 83.77  & 80.72    \\   
\arrayrulecolor{black}\hline
Ours                   & \underline{71.99}          &\underline{71.92}     &\textbf{71.96}
                       & 82.09          & \textbf{78.38}    & \underline{80.19}
                       & \underline{78.96}          & \underline{83.84}    & \textbf{81.33}         \\
\bottomrule
\end{tabular}
\end{adjustbox}
% \caption{Comparison on Discontinuous NER datasets. Results are statistically significant with p-value $<$ 0.005.}
\caption{Results for Discontinuous NER datasets.}
\label{t3}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}
% We compare our model with three seq2seq models \cite{strakova2019,yan2021unified,zhang-debias2022} which can be applied to all NER datasets, and several other baselines that are specifically designed for individual NER substask, including sequence labeling \cite{ju2018neural}, span-based methods \cite{WangYu2020,Yu2020} and hypergraph model \cite{wang-lu-2019}, etc. 
% We compare our model with three seq2seq models \cite{strakova2019,yan2021unified,zhang-debias2022}, and several other baselines that are specifically designed for individual NER substask, including sequence labeling \cite{ju2018neural}, span-based methods \cite{WangYu2020,Yu2020} and hypergraph model \cite{wang-lu-2019}, etc. 
We compare our model with three seq2seq models \cite{strakova2019,yan2021unified,zhang-debias2022}, and several other baselines that are specifically designed for individual NER substask, including sequence labeling \cite{ju2018neural}, span-based methods \cite{WangYu2020} and hypergraph model \cite{wang-lu-2019}, etc. 
% The performance comparisons on flat, nested and discontinuous datasets are reported in Table \ref{t1}, \ref{t2} and \ref{t3} respectively. 
The performance comparisons on three type datasets are reported in Table \ref{t1}, \ref{t2} and \ref{t3} respectively. 
There are several key observations from the comparison results. 
First, it can be seen that our model achieves the best performance among all compared methods on almost all datasets. 
E.g., on GENIA, our model outperforms the sequence labeling \cite{ju2018neural} and span-based methods \cite{WangYu2020} by $5.07\%$ and $4.67\%$ in terms of F1 score, and increases over $1.57\%$, $0.54\%$ and $0.69\%$ compared with the seq2seq models \cite{strakova2019,yan2021unified,zhang-debias2022}. 
% The hypothesis is that our approach effectively models the entity relation, which improves the entity boundary detection for better entity generation. 
The hypothesis is that our approach effectively models the entity relation, improving entity boundary detection for better entity generation. 
Moreover, the entity type information incorporated in our model further boosted the model performance. 
Second, we find that the improvement of our model over the baselines on discontinuous NER datasets is marginal, especially compared to \cite{wang2021clique}. 
The reason is that \cite{wang2021clique} is designed explicitly for discontinuous NER with complex multi-stage modeling, which is not generally applicable for nested NER. 
Nevertheless, our model can still achieve similar or even better performance. 
% The average F1 scores of the best three models over all datasets are further shown in Figure \ref{Fig.avg_f}.
\subsection{Ablation Study}
% 该部分分析模型整体提升情况
% 首先比较生成模型在不同数据集的F1 score均值情况，如图\ref{Fig.avg_f}所示可以看出，我们的生成模型效果提升明显优于其他生成模型。
% 然后分析模型分别融入实体关系、实体类型、实体关系+实体类型对于实体识别的影响。如表 \ref{t4.flat_ablation} to \ref{t4.dis_alation}。
To understand the effectiveness of different components in our model, we conduct a set of ablation studies by removing each component individually, i.e., entity token relation attention (TRA) and entity type attention (ETA).
Specifically, we take the seq2seq model without relation prediction task (RP), entity type attention (ETA), and entity token relation attention (TRA) as ``Baseline''. 
The results on different types of NER tasks are shown in Table \ref{t4.alation}. 
From these results, it is clear that the multi-task model with either entity relation attention (+ RP \& TRA) or entity type attention (+ RP \& ETA) substantially improves over the baseline model, which validates the effectiveness of both the token relation attention and the entity type attention.
%and incorporating entity relation attention obviously with entity type attention overall. 
Nevertheless, combining the multi-task with both components achieves the best performance.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\arrayrulecolor{black}
\begin{adjustbox}{width=0.75\columnwidth,center}
\begin{tabular}{llll} 
\toprule
\multirow{1}{*}{Model} & \multicolumn{1}{c}{CoNLL2003} & \multicolumn{1}{c}{ACE2004}  & \multicolumn{1}{c}{CADEC} \\ 
\hline
Baseline      
& \multicolumn{1}{c}{92.82}
& \multicolumn{1}{c}{86.43}   
& \multicolumn{1}{c}{70.36}      \\
% + ERP         & \multicolumn{1}{c}{87.10\tiny{(+0.67)}}   
% & \multicolumn{1}{c}{85.84\tiny{(+0.33)}} 
% & \multicolumn{1}{c}{79.34\tiny{(+0.51)}} \\
+ RP \& TRA    
& \multicolumn{1}{c}{93.01\small{\textbf{\textcolor{blue}{$\uparrow$0.19}}}}
& \multicolumn{1}{c}{86.87\small{\textbf{\textcolor{blue}{$\uparrow$0.44}}}}   
& \multicolumn{1}{c}{71.10\small{\textbf{\textcolor{blue}{$\uparrow$0.74}}}}     \\
+ RP \& ETA        
& \multicolumn{1}{c}{93.35\small{\textbf{\textcolor{blue}{$\uparrow$0.53}}}}
& \multicolumn{1}{c}{87.11\small{\textbf{\textcolor{blue}{$\uparrow$0.68}}}}   
& \multicolumn{1}{c}{70.59\small{\textbf{\textcolor{blue}{$\uparrow$0.23}}}}     \\
+ RP \& TRA \& ETA  
&\multicolumn{1}{c}{93.44\small{\textbf{\textcolor{blue}{$\uparrow$0.62}}}}
& \multicolumn{1}{c}{87.36\small{\textbf{\textcolor{blue}{$\uparrow$0.93}}}}  
& \multicolumn{1}{c}{71.96\small{\textbf{\textcolor{blue}{$\uparrow$1.60}}}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{F1 of three NER datsets based on different methods}
\vspace{-3mm}
\label{t4.alation}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Impact of Multi-task Learning}
% 该部分是分析多任务学习损失函数超参对模型的影响。
% 为了分析多任务损失函数超参多模型的影响，我们分别针对三种实体类型数据集CoNLL2003,ACE2005,CADEC做了分析实验. $W$的取值范围是[1.0,0.9,0.8,07,0.6,0.5,0.4,0.3,0.2,0.1].
% 实验结果如图所示。从途中可以看出，实体关系预测的损失函数。。。
% \begin{figure}[t] 
% 	\centering 
% 	\includegraphics[width=0.35\textwidth]{figures/w_ablation.jpg} 
% 	\caption{The impact of the hyperparameter $w$ on datasets} 
% 	\label{Fig.w_ablation} 
% \end{figure}
%We further conduct a set of parameter sensitivity experiments, with respect to $\alpha$ and $\beta$ on both data splits of SQuAD, to evaluate the robustness of the proposed approach. In each experiment, we tune only one parameter from \{0, 0.01, 0.05, 0.1, 0.2, 0.4, 0.8\}, while fixing the other parameter to the value as described in our implementation details. We find that the performance of QG+AQ+UG is relatively stable with respect to $\alpha$ and $\beta$. We also observe similar results of the proposed method in terms of the other two metrics.
% \begin{figure}
%     \centering
%         \subfigure{
%             \begin{minipage}[t]{0.32\columnwidth}
%                 \centering
%                 \includegraphics[width=4.2cm]{figures/w_ablation_CoNLL2003.jpg}
%                 \label{Fig.w_ablation_CoNLL2003}
%             \end{minipage}
%         }
%         \subfigure{
%             \begin{minipage}[t]{0.32\columnwidth}
%                 \centering
%                 \includegraphics[width=4.2cm]{figures/w_ablation_ACE2005.jpg}
%                 \label{Fig.w_ablation_ACE2005}
%             \end{minipage}        
%         }
%         \subfigure{
%             \begin{minipage}[t]{0.32\columnwidth}
%                 \centering
%                 \includegraphics[width=4.2cm]{figures/w_ablation_CADEC.jpg}
%                 \label{Fig.w_ablation_CADEC}
%             \end{minipage}        
%         }
%         \caption{The impact of multi-task learning.}
%         \label{Fig.w_ablation} 
% \end{figure}
\begin{figure}[h] 
	\centering 
	\includegraphics[width=1\columnwidth]{figures/multitask.png} 
	\caption{The impact of multi-task learning.}
	\label{Fig.w_ablation}
\end{figure}

In order to analyze the impact of the multi-task learning, we conduct a set of experiments on CoNLL2003, ACE2005 and CADEC datasets. 
In each experiment, we modify the weight parameter $w$ from \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}, while fixing the other hyperparameters to the values as described in our implementation details. Essentially, the value of $w$ controls the importance of the two tasks.
The model performances with different task weights are shown in Figure \ref{Fig.w_ablation}. 
We find that our model achieves the best performance when $w$ is around 0.3 on all datasets. It can also been seen that the model performance drops with the increasing of $w$. Our hypothesis is that the entity generation task plays a more important role compared to the entity relation prediction task.
Therefore, it is crucial to identify a good balance between the two tasks.