\vspace{-3mm}
\section{Conclusion}
% 我们提出了一个多任务统一生成NER模型。我们将实体边界信息转化为实体关系，并构建实体关系预测任务提升实体识别效果，同时以attention方式学习实体类型与word的关系增强模型识别能力。实验表明我们提出的方法是有效的。在未来我们将继续关注统一生成NER模型或更大规模的预训练模型在小样本NER的研究。
% In this paper, we propose a multi-mask Transformer model that integrates the relation prediction task and the entity recognition task into a unified sequence-to-sequence framework. 
In this paper, we propose a multi-mask Transformer model that integrates the relation prediction and entity recognition task into a unified seq2seq framework. 
% We further introduce two attention mechanisms, entity token relation attention and entity type attention, into the encoder and decoder to improve the entity generation performance.
We further introduce two attention mechanisms, entity token relation attention and entity type attention, into the encoder and decoder to improve the performance.
% Experimental results on three NER tasks demonstrate the superior performance of our model compared with several state-of-the-art methods. 
Experimental results on three NER tasks demonstrate the superior performance of our model compared with several methods. 
There are several possible research directions. We plan to continue to study the seq2seq NER with large-scale pre-trained models. We also plan to conduct seq2seq NER in few-shot or zero-shot scenarios.
