\section{Related Work}
Early NER works mainly focus on flat NER~\cite{sang2003,Lample2016,strubell2017fast,2018bidirectional}, and later gradually transit to nested and discontinuous NER \cite{KARIMI2015,strakova2019,Yu2020,shen2021locate,Dai2020-effective,FeiLi2021}, which are studied separately.
NER is initially viewed as a token-level classification task, which assigns each token a tag and employs a sequence model \cite{ratinov2009,Collobert2011,metke2016,2018bidirectional,chiu2016,Lample2016,Muis2016,strakova2019,Dai2020-effective} to predict labels of the sequences (such as BIO and BILOU). These methods assign a label to each token representing its entity type. However, they are not suitable to solve nested and discontinuous NER. 
% Then sequence generation methods are applied to the decoding part for predicting the tag sequences \cite{zhang2018,strakova2019,wang-lu-2019}. Yet, they need to design tagging schema for different NER. 

Span-based methods are applied to complex NER, such as nested NER and discontinuous NER \cite{strakova2019,metke2016,ju2018neural,fisher2019,Shibuya2020}. The key problem in span-based methods is how to obtain reasonable spans. For example, \citet{finkel2009nested} regard the parsing nodes as a span, while \citet{xu2017local,luan2019general,Yu2020,WangYu2020,li2020b} try to enumerate all spans in the sentence. 
However, it is impossible to enumerate all possible spans in real-world applications due to the dense computational cost. 
\citet{katiyar2018nested,wang-lu-2019,muis-lu-2017-labeling} propose hyper-graph methods that effectively represent many possible nested mentions in a sentence, which, however, suffer from both spurious structure and structural ambiguity issues during inference. 

Sequence-to-sequence methods generate entity label sequences \cite{strubell2017fast}. 
\citet{strakova2019} propose a seq2seq method for nested NER, which directly outputs the label of each token, only associating the relationship between words and labels with hard attention. 
\citet{yan2021unified} design a seq2seq model with the pointer mechanism to directly generate entity sequences on the decoder side. \citet{zhang-debias2022} build a unified generative model with data augmentation based on causal inference, which simultaneously deals with these three categories.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t] 
	\centering 
	\includegraphics[width=0.9\textwidth]{figures/full_2.png} 
	\caption{The architecture of our multi-task Transformer model, which contains relation prediction and entity decoding tasks. We incorporate the entity relation attention into the cross-attention layer of the decoder. The entity type attention is adopted to both the encoder and decoder.} 
	\label{Fig.frame} 
 \vspace{-3mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
