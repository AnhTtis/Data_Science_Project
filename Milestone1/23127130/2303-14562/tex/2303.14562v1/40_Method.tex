\newcommand{\depgraph}[1]{{\tt DepGraph(#1)}}
\newcommand{\placement}[1]{{\tt SamplePlacement(#1)}}
\newcommand{\addHiddenEdges}[1]{{\tt AddHiddenEdges(#1)}}

%\subsection{Pipeline}

The proposed pipeline is detailed in
%\autoref{alg:subtasks} and 
\autoref{alg:pipeline}. First a voxelized representation of the scene and a dependency graph are computed (lines 3,4,5), which are detailed in \autoref{subsec:voxel_dep}. The dependency graph contains a belief state of the current scene based on visibility and reachability constraints. All the \textit{sinks} of this directed graph represent likely pickable objects.
The \textit{ranks} are described later (see \autoref{subsec:target-prediction}).
If the target object is pickable then it is retrieved and the pipeline is terminated line 6.
Otherwise, a placement is planned (see \autoref{subsec:placement_sampling}) one at a time for each object in a shuffled order biased by the \textit{ranks} (\textit{TryMoveOne} line 7). The first successful plan is executed.

If no placement is found for any of the pickable objects, then a fallback procedure is called
%\edited{(redundant to say "on the same set of objects?")} on the same set of objects
to try and pick one object and move it temporarily out of view of the camera; the first successful plan is executed, the scene is re-sensed, and a new placement is sampled for the object or the object is simply put back at the same spot (\textit{MoveOrPlaceback} line 10). At this stage, the pipeline could restart if a new object was discovered (lines 12-15). Otherwise the same set of pickable objects 
%\edited{(redundent to say "the same set of pickable objects"?)} 
are tested for new placements in the scene (line 16). If these two operations of ``moving an object to look behind it''\textit{(MoveOrPlaceback)} followed by retrying to ``move one of the pickable objects to a new spot''\textit{(TryMoveOne)} fail for all objects, then the pipeline can return and report failure for the current resolution (line 22). If the sequence of operations succeeds, then the pipeline can restart (line $19\rightarrow2$).

%\newcommand{\exec}[1]{{\tt Execute(#1)}}
%\newcommand{\revx}[1]{{\tt Reverse(#1)}}
\newcommand{\trymoveone}[1]{{\tt TryMoveOne(#1)}}
\newcommand{\move}[1]{{\tt MoveOrPlaceback(#1)}}
%\begin{algorithm}
%\caption{SubTask Routines}\label{alg:subtasks}
%\begin{algorithmic}[thpb]
%
%\Function{$\tt TryMoveOne$}{$\tt objects$}
%    \For{$\tt obj \in WeightedShuffle(objects)$}
%        \Comment{a.k.a pick without replacement with probabilities adjusted by rank}
%        \State $\tt pickTraj \gets \pick{obj}$
%        \If{$\tt pickTraj \neq \nil$} 
%            \State $\tt p \gets \placement{obj}$
%            \State $\tt placeTraj \gets \place{obj,p}$
%            \If{$\tt placeTraj \neq \nil$}
%                \State $\tt \exec{pickTraj}$
%                \State $\tt SenseEnvironment()$
%                \State $\tt \exec{placeTraj}$
%                \State \Return $\tt true$
%            \EndIf
%        \EndIf
%    \EndFor
%    \State \Return $\tt false$
%\EndFunction
%
%\Function{$\tt MoveOrPlaceback$}{$\tt object$}
%    \State $\tt pickTraj \gets \pick{object}$
%    \If{$\tt pickTraj \neq \nil$} 
%        \State $\tt \exec{pickTraj}$
%        \State $\tt SenseEnvironment()$
%        \State $\tt p \gets \placement{object}$
%        \State $\tt placeTraj \gets \place{object,p}$
%        \If{$\tt placeTraj \neq \nil$}
%            \State $\tt \exec{placeTraj}$
%        \Else
%            \State $\tt \revx{pickTraj}$
%        \EndIf
%        \State \Return $\tt true$
%    \EndIf
%    \State \Return $\tt false$
%\EndFunction
%
%\end{algorithmic}
%\end{algorithm}

\begin{algorithm}[thpb]
\caption{$\tt RC\_Pipeline$($\tt target$)}\label{alg:pipeline}
\begin{algorithmic}[1]
    \State $\tt failure \gets false$
    \While{$\tt failure = false$}
        \State $\tt space \gets UpdateVoxelsFromImage()$
        \State $\tt dg \gets \depgraph{space}$
        \State $\tt sinks,ranks \gets RankSinks(target,dg)$ 
        \If{$\tt target \in sinks$} $\tt break$\EndIf
        \If{$\tt \trymoveone{sinks,ranks} = false$}
            \State $\tt failure \gets true$
            \For{$\tt sink \in sinks$}
                \If{$\tt \move{sink} = false$}
                    \State $\tt continue$
                \EndIf
                \State $\tt space \gets UpdateVoxelsFromImage()$
                \If{$\tt DidDiscoverObject(space)$}
                    \State $\tt failure \gets false$
                    \State $\tt break$
                \EndIf
                \If{$\tt \trymoveone{sinks,\emptyset} = false$}
                    \State $\tt continue$
                \EndIf
                \State $\tt failure \gets false$
                \State $\tt break$
            \EndFor
        \EndIf
    \EndWhile
    \If{$\tt not~failure$} 
        \State $\tt Retrieve(target)$
    \EndIf
    \State $\tt return~failure$
    
\end{algorithmic}
\end{algorithm}

\begin{figure*}[thpb]
%digraph dep_graph {
%	rankdir=BT
%	node [
%			shape=circle,
%			fixedsize=true,
%			width=0.34,
%           color=black,
%			fillcolor="#eeeeee",
%			style="filled,solid",
%			fontsize=22
%		]
%	T [shape = doublecircle, fillcolor=red]
%	2 [fillcolor=purple]
%	3 [fillcolor=green3]
%	4 [fillcolor=cyan3]
%	5 [fillcolor=greenyellow]
%	6 [fillcolor=pink1]
%	3 -> 4 [ label = "below\n100%" ]
%    2 -> 6 [ label = "grasp blocked by\n100%" ]
%    T -> 2 [ label = "hidden_by\n75.13%" ]
%    T -> 3 [ label = "hidden_by\n8.84%" ]
%    T -> 5 [ label = "hidden_by\n8.69%" ]
%    T -> 6 [ label = "hidden_by\n7.34%" ]
%}

    %\includegraphics[width=.33\linewidth,trim={0 140 40 22},clip]{figures/sim_exp/step1_ws.png}
    \begin{overpic}[width=0.336\linewidth,trim={0 140 40 22},clip]
    {figures/sim_exp/step1_ws.png}
        \put (3,10) {\huge$\displaystyle 1)$}
    \end{overpic}
    %\includegraphics[width=.33\linewidth,trim={0 140 40 22},clip]
    \begin{overpic}[width=0.336\linewidth,trim={0 140 40 22},clip]
    {figures/sim_exp/step2_ws.png}
        \put (3,10) {\huge$\displaystyle 2)$}
    \end{overpic}
    %\includegraphics[width=.33\linewidth,trim={0 140 40 22},clip]
    \begin{overpic}[width=0.336\linewidth,trim={0 140 40 22},clip]
    {figures/sim_exp/step3_ws.png}
        \put (3,10) {\huge$\displaystyle 3)$}
    \end{overpic}
    %\includegraphics[width=.33\linewidth,trim={0 140 40 22},clip]
    \begin{overpic}[width=0.336\linewidth,trim={0 140 40 22},clip]
    {figures/sim_exp/step4_ws.png}
        \put (3,10) {\huge$\displaystyle 4)$}
    \end{overpic}
    %\includegraphics[width=.33\linewidth,trim={0 140 40 22},clip]
    \begin{overpic}[width=0.336\linewidth,trim={0 140 40 22},clip]
    {figures/sim_exp/step5_ws.png}
        \put (3,10) {\huge$\displaystyle 5)$}
    \end{overpic}
    %\includegraphics[width=.33\linewidth,trim={0 140 40 22},clip]
    \begin{overpic}[width=0.336\linewidth,trim={0 140 40 22},clip]
    {figures/sim_exp/step6_ws.png}
        \put (3,10) {\huge$\displaystyle 6)$}
    \end{overpic}
    %\includegraphics[width=0.245\linewidth]
    \begin{overpic}[width=0.245\linewidth]
    {figures/sim_exp/step1_dg_c.png}
        \put (-1,5) {\huge a)}
    \end{overpic}
    %\includegraphics[width=0.245\linewidth]
    \begin{overpic}[width=0.245\linewidth]
    {figures/sim_exp/step2_dg.png}
        \put (-1,5) {\huge b)}
    \end{overpic}
    %\includegraphics[width=0.245\linewidth]
    \begin{overpic}[width=0.245\linewidth]
    {figures/sim_exp/step3_dg.png}
        \put (-1,5) {\huge c)}
    \end{overpic}
    %\includegraphics[width=0.245\linewidth]
    \begin{overpic}[width=0.245\linewidth]
    {figures/sim_exp/step4_dg.png}
        \put (-1,5) {\huge d)}
    \end{overpic}
    %\includegraphics[width=0.2\linewidth]{figures/sim_exp/step5_dg.png}
  %\framebox{\includegraphics[width=\linewidth]{figures/example_scene_labeled.png}}
  %\framebox{\includegraphics[width=\linewidth]{figures/example_scene_both_views.png}}
  %\framebox{\includegraphics[width=\linewidth]{figures/dep_graph.png}}
  %\framebox{\includegraphics[width=\linewidth]{figures/example_scene_camera_view.png}}
  %\vspace{0.1in}
  %\includegraphics[width=.196\linewidth]{figures/sim_exp/step1_ws_I.png}
  %\includegraphics[width=.196\linewidth]{figures/sim_exp/step2_ws_I.png}
  %\includegraphics[width=.196\linewidth]{figures/sim_exp/step3_ws_I.png}
  %\includegraphics[width=.196\linewidth]{figures/sim_exp/step4_ws_I.png}
  %\includegraphics[width=.196\linewidth]{figures/sim_exp/step5_ws_I.png}
  \caption{
  Images (1)-(6) show a simulated experiment from initial configuration to final one action at a time with corresponding camera views in the top left. The corresponding generated dependency graphs transitioning between the images (1)-(2), (2)-(3), (3)-(4), and (4)-(5) are shown in images (a)-(d). 
  The colors of nodes corresponds with the objects in the scene and the red object (labeled `T') is the target object for the trial.
  %The final pick is shown in \autoref{fig:object_pick}.
  The last graph between images 5-6 is not shown since it is trivial having no dependencies between any objects.
  }
  \label{fig:dep_graph}
  \vspace{-0.28in}
\end{figure*}


\subsection{Voxel Map and Dependency Graph}
\label{subsec:voxel_dep}

A 3D occlusion voxel grid within the workspace is constructed from RGB-D images.
First the point cloud (in world frame) is generated using the RGB-D image and the inverse of the camera projection. These points are down-sampled into a voxelization of the scene.
Given segmentation image information, each object is associated with a portion of the voxel grid. Object geometry is used to label voxels as occupied and remaining associated voxels as occluded.
The occluded regions of objects may intersect when jointly occluded.

The dependency graph is a directed graph where each node represents a visible (or target) object and a labeled edge $(\obe_i, \obe_j, r)$ represents a relation $r$ from object $\obe_i$ to $\obe_j$ that necessitates $\obe_j$ to be picked and placed before $\obe_i$ could be picked.
%, where $p \in (0,1]$ denotes a heuristic estimate of the probability for that relation $\obe_i \overset{r}{\longrightarrow} \obe_j$ to be true. For the non-heuristic variant of the pipeline, $p$ does not need to be included.
Valid relations in this work include ``below'', ``grasp blocked by'', and -- for the prediction of the target object -- ``hidden by''. See \autoref{fig:dep_graph} for a sequence of such dependency graphs generated during an example experiment. 

Object x is defined to be ``below'' object y ($x \xrightarrow{below} y$) if object x touches object y and the z-coordinate of the center of mass of x is less than that of y. Note that this isn't guaranteed to capture all intuitive cases of one object being below another for non-convex objects. This relation is computed using object models and poses given by the perception system.
%\edited{Question: what if multiple objects are stacked. Does the edge go from the first object to the third object? Or is it only for objects that are immediately stacked?}

Object x has its ``grasp blocked by'' y ($x \xrightarrow{blocked} y$) if there are no collision free grasp poses for object x and the arm is in collision with y for one or more valid grasp poses.
(Grasp poses are sampled and tested by inverse kinematics (IK) for discovered objects)
Grasp poses are sampled using inverse kinematics (IK) to discovered objects. 
(or, if the grasping pose collides with objects, an edge to each collided object is added)
If there exists a collision free grasp for an object, no blocking edges are added; otherwise, an edge to each object that has a collision with the arm is added. Note that although this relation guarantees that the source object blocks the target, it doesn't capture all such reachability dependencies. This however is not an issue for completeness as \autoref{alg:pipeline} will eventually try to grasp all objects in the case of motion planning failure.
%with a heuristic weight equal to the fraction of total sampled grasps for which that object is in collision with the arm. Note, the weights of grasp blocking edges coming out of any object need not add to one since the arm could be colliding with multiple objects for any single grasp.

The target object t is possibly ``hidden by'' x ($t \xrightarrow{hidden} x$) if the target isn't sensed in the scene and object x is touching the table. This relation is used to keep track of the belief state of where the target is. Each edge is assigned a probability based on the volume of the occluded space behind object x (see \autoref{subsec:target-prediction}).

% WRAPFIGURE IF SPACE
% \begin{figure}[thpb]
%     \centering
%     \includegraphics[width=\linewidth,trim={0 140 40 22},clip]{figures/sim_exp/step6_ws.png}
%     \caption{The final pick action for the experiment shown in \autoref{fig:dep_graph}.}
%     \label{fig:object_pick}
% \end{figure}

%%%%%%%%%%%%%%%
%\begin{algorithm}
%\caption{$\tt \depgraph{}$}\label{alg:dep_graph}
%\begin{algorithmic}[thpb]
%    \State $todo$
%\end{algorithmic}
%\end{algorithm}

\vspace{-0.11in}
\subsection{Placement Sampling}
\label{subsec:placement_sampling}
A valid placement is one that doesn't collide with another object or any undiscovered area of the workspace.
%\edited{(remove the following two sentences if needed space?)}
%This is computed exhaustively up to a resolution.
%The voxelized representation of the space naturally lends itself to sampling objects placements in a grid of the same resolotion.
Instead of randomly sampling x,y coordinates for an object and checking for collision at that point, we create a grid matching the horizontal extents of the workspace and add a collision mask which is the shadow of the object occupancy and occlusion voxel grid looking from a birds eye view. This mask is then convolved with the shadow of the object that is to be placed. 
The occupied pixel indices indicate collision-free placements and are converted to world coordinates.
%The indices of the `1' pixels of this last image are converted to world coordinates and thus used to enumerate all object placements that don't collide with discovered objects or hidden areas of the workspace. 
Object orientations can be enumerated by rotating the object shadow.
%To sample multiple object orientations, the process can be repeated, rotating the object shadow first, as desired.

%heuristic kernels:
%    1) definite occlusion (cone with object as base)
%    2) likely occlusion (cone with object as point)
%        - normalize weight to generate 2D heat map

%\begin{algorithm}
%\caption{$\placement{object}$}\label{alg:placement}
%\begin{algorithmic}[thpb]
%    \State $todo$
%\end{algorithmic}
%\end{algorithm}

\subsection{Target Object Prediction}
\label{subsec:target-prediction}
%\begin{algorithm}
%\caption{$\tt \addHiddenEdges{target,G}$}\label{alg:obj_predict}
%\begin{algorithmic}[thpb]
%    \State $\tt H \gets \emptyset$
%    \State $\tt total = 0$
%    \For{$\tt o \in Nodes(G) \setminus \{target\}$}
%        \State $\Tilde{O_{o}} = {\tt DirectOcclusionSpace(o)}$
%        \State ${\tt h = Heuristic}(\Tilde{O_{o}})$
%        \Comment{See \autoref{subsec:heuristics}}
%        \If{$\tt h \neq 0$}
%            \State $\tt H \gets H \cup \{(o,h)\}$
%            \State $\tt total \gets total + h$
%        \EndIf
%    \EndFor
%    \For{$\tt o,h \in H$}
%        \State $\tt AddEdge((target,o,\text{``hidden by''},\frac{h}{total}), G)$
%    \EndFor
%    %\While{$\tt target \notin \obd$}
%\end{algorithmic}
%\end{algorithm}
Intelligent object location prediction is achieved by applying a heuristic which ranks the pickable objects determined by the dependency graph (line 4 in \autoref{alg:pipeline}).
This is done by augmenting the dependency graph edges with a weight $p \in (0,1]$ estimating the probability for the relation $\obe_i \overset{r}{\longrightarrow} \obe_j$ to be true.
%This is done by augmenting the dependency graph edges to include a weight $p \in (0,1]$ which denotes an estimate of the probability for the given relation $\obe_i \overset{r}{\longrightarrow} \obe_j$ to be true.
%\edited{(What does the following sentence mean? What do you do product on?)}
To rank a pickable object, the sum of products of edge weights of all simple paths between the target and the object is computed. When sampling from the list of pickable objects, this rank is used as a probability weight.

For the ``below'' relation $p=1$ since object segmentation is assumed to be reliable. For the ``grasp blocked by'' relation p is equal to the fraction of total sampled grasps for which that object is in collision with the arm. Note, the weights of grasp blocking edges coming out of any object need not add to one (hence don't truly represent probabilities) since the arm could be colliding with multiple objects for any single grasp.
%A few heuristics are proposed next, but the general intuition is to bias towards larger direct occlusion regions so that the robot gains the most knowledge of the scene after every pick and place action.
For the ``hidden by'' relation, the goal is to encourage knowledge gain of the environment. This is done by normalizing the volume of the direct occlusion region of each stack of objects and assigning the inverse as the probability estimate that the target is hidden behind each stack. This heuristic biases the pipeline towards discovering large volumes of occluded workspace.

From an algorithmic point of view, there is technically no reason to normalize the output of the heuristic, however, representing the heuristics as probabilities is insightful since - without prior knowledge - the probability the hidden object to be in a larger volume is larger than the probability of it being in a smaller volume. Furthermore, modeling the dependency graph edges with probabilities as opposed to non-normalized weights is conducive for exploring future work which might seek to combine the probabilities based on the proposed volumetric-heuristics with priors based on the semantics of the objects involved or from an additional human instruction (see \autoref{sec:conclusion} work).

%\setcounter{subsecnumdepth}{0}
%\subsubsection*{\textbf{"Hidden by" Heuristics:}}
%\label{subsec:heuristics}
%\begin{enumerate}
%    \item The simplest uniform heuristic function is to return the volume of the given object's direct occlusion space. This seems like a reasonable heuristic, however, if a direct occlusion space happens to have large volume but is very narrow then it might not even be feasible for the target object to exist in that occlusion space.
%    \item A heuristic that solves the above problem is to sample target object placements within each occlusion region. That is, for $n$ trials, select a random stable object orientation and coordinate interior to the total occlusion space for all objects; check if the object placement puts it completely inside the total occlusion volume and increment a counter success counter $c$. If the object placement also intersect with the direct occlusion space for the object of interest then increment another counter $d$. The heuristic would then return $d/c$ the fraction of occluded samples that are at least partially occluded by the object of interest. This is also a simple heuristic but its non-deterministic.

%   {\color{red}
%   I assume we don't know what the target object is, and thus need to recognize it from the scene. If this is true, maybe this method isn't a good one?
%   Is the heuristics used in the code?
%   }
    
%   \item A deterministic heuristic function which is also robust to volume ambiguity relies on a similar technique to placement sampling described in the next section, \autoref{subsec:placement_sampling}. In brief, iterate through stable object orientations up to some resolution. For each stable orientation, generate a 2D kernel of the object as if looking from a birds eye view and perform a convolution with it on the negated, flattened occlusion space (projecting the voxels down to pixels and inverting the boolean values). The resulting array has 0 values corresponding to valid placements of the object interior to the occlusion space for the given orientation. Counting the zeros effectively returns the area of the 2D regions where the object would be completely hidden within the occlusion region for a given orientation. This heuristic would then return the total area for all stable object orientations.
%\end{enumerate}


