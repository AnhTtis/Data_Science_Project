%{\bf Motivation}
%Robots have widely been used in factories and warehouses to carry out repetitive tasks with high precision. As robot hardware becomes cheaper it will become possible for people to afford robots that help automate chores at home. In order for such robots to be useful for multiple tasks, people without programming and robotic experience will need a way to instruct robots to perform everyday tasks through a natural interface. In this work, a task specification method is proposed that is able to ground natural language to the domain of pick and place tasks.



{\bf Problem Statement}

The state of a rigid 3D object is describable as a point (or configuration) in $SE(3)$, which is a 6D space expressing both the translation and rotation of the object. For an environment with $n$ objects with configurations $s_0,s_1,...,s_n \in SE(3)$ the problem of object rearrangement involves finding a sequence of motions $\pi_0,\pi_1,...,\pi_m : [0,1] \to \Cspace$ for a robot arm to move the objects from their starting configurations to some given goal configurations $g_0,g_1,...,g_n \in SE(3)$. More generally, we can consider a collection of valid goal configurations per object $G_0,...,G_n \subset SE(3)$. This work, however, considers the case where such sets of goal configurations are not explicitly given but are rather implicitly described through natural language. Thus, before solving the task and motion planning problem of rearranging objects we need to translate a natural language description into a set of valid goal configurations per object.

As a first step in this direction and in order to focus on the task of natural language grounding, this work first considers the simpler task of reorienting objects on a tabletop. In this setting, the translation of the objects is the same in both the start and goal configurations. Furthermore, the objects are far enough apart so that they will not collide for any orientation. Instead of full poses in $SE(3)$ the focus is on transforming the initial object orientations $d_0,...d_n \in SO(3)$ into the respective goal orientation set $H_0,...,H_n \subset SO(3)$. The motion planning challenge of grasping and placing an object is simplified in this task environment as each object has sufficient collision free space around it. The challenge of interpreting and grounding the language instructions describing object rotations is still interesting and has not been explicitly studied to the best of our knowledge.

{\bf Outline of Online Steps to Tackle this Problem}
\begin{enumerate}
    \item Generate a set of stable orientations $st_0, \ldots, st_n \subset SO(3)$ for each object given the supporting surface.
    \item Given a natural language instruction: 
    \begin{itemize}
        \item Identify first the object $o_i$ that the instruction refers to (we will provide a vocabulary to human instructors on how to refer to objects, e.g., "blue cylinder" or the "black cube"). 
        \item Then, the core challenge is to map the natural language instruction to a valid subset $v_i \subset SO(3)$. There are multiple methods to consider here on how to learn this mapping offline - see section below on Related Work.
    \end{itemize}
    \item Rank stable object poses $st_i$ by distance to the valid subsets $v_i$ of $SO(3)$ according to the instruction.
    \item Prune poses in $st_i$ with a  distance greater than some threshold $\delta$ from the valid subsets $v_i$.
    \item Iterate through the remaining poses in $st_i$ according to their ranking and generate motion plans to reorient them. 
    \item Execute the first successful motion plan.
\end{enumerate}

{\bf Related Work on Grounding Language Instructions}
Initial approaches to grounding natural language robot task descriptions approached the problem as a translation problem. They would use sentence parsers to transform sentences in natural language into a more formal language, such as LTL \cite{raman2013sorry} or semspec \cite{pomarlan2018robot}, and then apply cleverly constructed grammars to parse the intermediate languages into an executable program for a given robot system. Such approaches have the benefit of handling multiple failure modes in task specification and can be used to produce explanations of why a given task was found to be invalid. Drawbacks of such approaches are their complexity in adapting to new robot systems and ability to handle ambiguity in user descriptions.

In order to address ambiguity in language input some approached the problem of grounding natural language descriptions through Bayesian inference by constructing factor graphs from a given sentence parsing. Such approaches inferred either robot actions directly \cite{tellex2011understanding}, constraints on the workspace \cite{howard2014natural, paul2018efficient}, which were passed to a conventional motion planner, or formal clauses \cite{misra2016tell} which are evaluated into an executable plan. These techniques are more adaptable to a larger variety of language input but have trouble scaling with workspace complexity.% due to an explosion of factors.

More recently data driven approaches have tackled natural language grounding. Some use demonstrations to learn reward functions for Object Oriented Markov Decision Problems (OO-MDPs) via inverse reinforcement learning (IRL) \cite{macglashan2015grounding, arumugam2017accurately, arumugam2019grounding}. Others have also proposed end-to-end deep neural networks that directly learn and tie together perception and language models \cite{shridhar2018interactive, hatori2018interactively, mees2021composing}.

{\bf Experimental Setup}
First, multiple goal scene categories consisting of objects with basic shapes, such as blocks and cylinders, will be generated where the objects are assigned locations on a tabletop. For each scene category, a set of instances will be generated by scrambling the orientations of the objects but guaranteeing that the objects are lying stable on the tabletop. From each scene category, a start and a goal scene instance will be randomly selected and presented to a human. The human will be asked to write instructions for a robot to manipulate the objects so as to take them from the start scene configuration to the goal configuration. These instructions will then be presented to the robot for the above steps to be executed. The final poses of each object in the manipulated scene and the goal scene will be compared via inverse cosine distance. 
%( or something else \cite{huynh2009metrics}?)

Such an experiment setup may be flawed, however, as humans might consider multiple goal scenes with similar features to be considered equivalent in value and thus give similar or equivalent instructions for such scenes. Thus, in addition to measuring distance between the given goal scene and the produced goal scene, we can also survey participants on how successful the robot executed their instructions. In addition, we could query participants to reorient the objects in the scene to produce multiple configurations that they consider satisfactory, and then take the average $SO(3)$ distance between the robot-produced scene and the human-produced scenes.