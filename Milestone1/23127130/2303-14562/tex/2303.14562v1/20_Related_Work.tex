%Object retrieval (sometimes called mechanical search) has had many recent works focus on cluttered and occluded scenes. \cite{bejjani2021occlusion,huang2020mechanical,kurenkov2020visuomotor,danielczuk2019mechanical} that use various models
% TODO: state explicitly which models
%to reason over the scene to determine manipulations that will make progress towards the discovery of a target object. This work uses a voxelization of the workspace to reason about object occupancy and occlusion.

% {
% \color{red}
% \textbf{General Related Work: Object manipulation under occlusion, Mechanical Search, and Object retrieval problem.}
% Related work about different settings, completeness analysis, and heuristic methods, and their limitations.
% }


%Past work has focused on cases where objects are extracted from the workspace \cite{dogar2014object, nam2021fast}, or uses machine learning methods to guide the decision making \cite{bejjani2021occlusion, huang2022mechanical}. In-place rearrangement is considered in \cite{ahn2021integrated}. Although the method is efficient, it is not complete as it uses the largest object in the scene to analyze the traversability of objects \cite{nam2021fast}. Object stacking has limited visibility and most work applies machine learning methods for reasoning \cite{huang2022mechanicalstack, zhu2021hierarchical}.

Some works on object retrieval rely on geometric analysis of object occlusion \cite{dogar2014object,nam2021fast}. They provide theoretical insights but frequently do not limit actions to in-place rearrangement of blocking objects. Specifically, one method constructs a dependency graph taking into account objects that jointly occlude a region and objects that block others \cite{dogar2014object}. The occlusion volume is used to estimate belief regarding the target object position and helps to construct an optimal A* algorithm. An alternative constructs a Traversability graph (T-graph) \cite{nam2021fast}, where the edges encode if the largest object in the scene can be moved between two poses. It then constructs an  algorithm to extract the target object, but is limited as the traversability edges are too constraining. The POMDP formulation is popular for the task \cite{zhao2021hierarchical, xiao2019online}, which allows the application of general POMDP solvers. The POMDP formulation was also adopted by the work that formalizes object retrieval in unstructured scenes as "mechanical search" \cite{danielczuk2019mechanical}.

Alternatives rely on learning-based methods to solve such challenges, such as reinforcement learning \cite{bejjani2021occlusion} or target belief prediction \cite{huang2022mechanical, huang2022mechanicalstack, huang2021mechanical}. They report good performance but do not provide theoretical guarantees given the black-box nature of the solutions. In particular, a reinforcement learning solution \cite{bejjani2021occlusion} uses the rendered top-down projections of the scene to predict the target poses. A recent follow-up effort \cite{huang2022mechanical} on previous work \cite{huang2021mechanical} estimates the 1D position belief of the target object on the shelf via machine learning. It then constructs a policy based on the distribution change after applying pushing and suction actions. It incorporates stacking and unstacking actions, where object stacking is represented by a tree structure. Other works such as \cite{zeng2022robotic} utilize learning for planning grasps to greedily empty bins of complex and novel objects.

A related work \cite{miao2022safe} proposes a complete framework to safely reconstruct all objects in the scene amidst object occlusions. Nevertheless, object retrieval may not require reconstructing all objects and requires a search procedure that is more task-driven for efficiency. There are also previous works \cite{gupta2013interactive, miao2022safe} that construct a voxelization of the environment to model object occlusions, similar to the current work. This representation is used to compute an object's occlusion volume, which provides heuristic guidance. Object spatial relationships are often represented by scene graphs \cite{zhu2021hierarchical, kumar2022graph}, or implicitly in machine learning solutions \cite{danielczuk2019mechanical, poon2019probabilistic, novkovic2020object}.

What stands out in this work is that it proposes a general template for a $\tt RC$ or $\tt PC$ approach to task retrieval in occluded environments that only relies on basic motion and perception primitives. This modular nature allows for quick sim-to-real transfer and passive performance improvement as the primitives are improved over time.
Furthermore, an efficient implementation is demonstrated utilizing a
voxelized representation of the environment for quick collision filtering of object placements as well as providing an effective heuristic to rank object manipulations. Thus, the framework enables effective in-workspace manipulation.
%The combination of theoretical guarantees and implementation enable effective in-workspace manipulation without removing objects from the table or shelf.

%However, \cite{dogar2014object,nam2021fast} assume extra placements are available outside the workspace, hence do not apply to in-place rearrangement actions.

%Beyond that, most cases with completeness guarantees assume extra placements outside the confined space are available, which may not be the case in general. Meanwhile, past works do not consider complicated spatial relations of objects, such as object stacking on each other.

% {\color{red}
% \textbf{Specific methods for handling different complexities, such as occlusion modeling, and action modeling (rearrangement). Keep or Remove?}}










%Regarding occlusion reasoning, \cite{huang2022mechanical} utilizes object footprint area to reason about possible positions of the hidden object during training. This work uses similar modeling but using the volume of objects to consider possible object locations during runtime.




%{
%\color{red}
%\textbf{How NLP has developed and helped with similar domains, and more related lines.}
%}

%Initial approaches integrating natural language descriptions would would use sentence parsers to ``translate'' sentences in natural language into a more formal language, such as LTL \cite{raman2013sorry} or semspec \cite{pomarlan2018robot}, and then apply cleverly constructed grammars to parse the intermediate languages into an executable program for a given robot system. Such approaches have the benefit of handling multiple failure modes in task specification and can be used to produce explanations of why a given task was found to be invalid. Drawbacks of such approaches are their complexity in adapting to new robot systems and ability to handle ambiguity in user descriptions.
%The focus of this work is similar in terms of reasoning about feasibility of language instructions but also integrates a conversation agent that could resolve contradictory information or ambiguity.

%Other related efforts have used natural language to learn new task primitives \cite{suddrey_teaching_2017} or as input into a hierarchical search process \cite{kurenkov_semantic_2021}. One of the motivations for exploring how a human instruction can complement the partial scene observation is to speed up scene reconstruction and object discovery. To that end, Human-in-the-loop systems \cite{papallas2020non} have been implemented but rely on more direct user interface elements rather than natural language, and don't deal with object occlusion. The proposed work differs by using the language-based task description to inform the planner about the unobserved space.
%A recent work \cite{zheng2021spatial} has a similar motivation to integrate language reasoning to speed up task planning for a city-scale navigation environment.
%Work on Hierarchical Mechanical Search \cite{kurenkov_semantic_2021} does appear similar to the proposed setup, but handles scene representation and language processing separately, while the current work aims to integrate these two sources of information. 


%At least one work does integrate language and task reasoning \cite{nguyen_robot_2020} in order to retrieve objects based on their utility. This work focus not on the object selection but on object discovery within an occluded scene.

%Camera on gripper:
%\url{https://arxiv.org/pdf/2011.03334.pdf}