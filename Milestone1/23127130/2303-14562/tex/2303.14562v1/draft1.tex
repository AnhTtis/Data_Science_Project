{\bf Introduction}

Robots have widely been used in factories and warehouses to carry out repetitive tasks with high precision. As robot hardware becomes cheaper it will become possible for people to afford robots that help automate chores at home. In order for such robots to be useful for multiple tasks, people without programming and robotic experience will need a way to instruct robots to perform everyday tasks through a natural interface. In this work, a task specification method is proposed that is able to ground natural language to the domain of pick and place tasks.

{\bf Related Work}

Initial approaches to task specification involved translating natural language directly into logical statements about system state \cite{raman2013sorry}. These approaches were effective at determining viability of tasks and explaining when and how a given task specification was infeasible. Such approaches, however, did not handle possible ambiguity in the user defined specifications. Thus, the next set of approaches developed multiple probabilistic models to account for task ambiguity by maximizing likelihoods \cite{paul2018efficient} or minimizing ``energy'' \cite{misra2016tell}. Recently, data driven learning approaches have been proposed for various interactive pick and place tasks \cite{hatori2018interactively,shridhar2018interactive,mees2021composing}. These approaches deal with ambiguity by requesting clarification when the task is under-specified. While the learned approaches are better suited at dealing with ambiguity of task specification, they abandon explicit reasoning over task feasibility. Is there a method that can handle both task ambiguity and feasibility?

{\bf Method}

The proposed method will take as input text in natural language and parse it into dependency trees \footnote{\url{https://en.wikipedia.org/wiki/Dependency_grammar}}.
Next object relations will be extracted into a graph with nodes as objects and directed edges describing geometric relations (possibly ambiguous) between the source object and target object. Note, this can be done dynamically, by inserting edges phrase by phrase. Figure~\ref{fig:relation_graph} shows an example of the extracted graph structure for a table setting task. 

To deal with ambiguity of object positions with relation to one another, a set of constraints is enforced based on the following principles.
\begin{enumerate}
    \item Feasibility - objects remain on the table (consider stability for more complicated scenarios)
    \item Symmetry - An object between two other objects should be equally spaced between them unless otherwise specified. This can be applied recursively. For example if I ask the box A is to be put between box B and C but closer to B, then this symmetry would collapse the ambiguity of the position of box A to half way between the half way point between B and C.
    \item Repulsion - unless constrained by above or objects are supposed to touch, enforce a buffer around an object of a quarter of its ``effective diameter''.
    \item Attraction - after all other constraints are satisfied, prefer objects that share a direct positional relation to be as close to one another as possible.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/graph.png}
    \caption{An example geometric relation graph for a table setting task. In this example, the plate is the object of reference.}
    \label{fig:relation_graph}
\end{figure}

In addition to ambiguity in geometric relations between objects there may also be ambiguity of intrinsic object orientation. To address these types of ambiguity we enforce the principle of axis alignment...  

{\bf Task Environment}

The task environment will feature one human agent and one robot agent (with multiple arms) surrounding a table. Within this environment the mobility and reachability of the robot can be varied. For simplicity of modeling physical human-robot interaction, it will be assumed that the robot and human are restricted to opposite sides of the table but that both can reach across the table to touch objects placed in front of one or the other. This environment allows for easy verbal and gesture based interaction while including a basic safety barrier (the table) to minimize human-robot collision.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/task_environment.png}
    \caption{A simulated scene of the task environment. In this example, Baxter observes a human pointing at a cup.}
    \label{fig:task_environment}
\end{figure}


{\bf Evaluation}
First, multiple goal scenes consisting of basic objects such as blocks, puzzle pieces, and tableware, will be generated both manually and randomly. For each unique scene, a human will be tasked to write instructions for another human to produce the given scene starting from an empty table. These instruction will then be given to the robot and the final poses of each object in the manipulated scene and the goal scene will be compared. Furthermore, the same instructions will be given to different human than the one who wrote the instruction and the persons final scene will also be compared to that of the robot's and to the original goal.