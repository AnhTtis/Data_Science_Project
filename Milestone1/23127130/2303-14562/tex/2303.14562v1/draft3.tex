%{\bf Motivation}
%Robots have widely been used in factories and warehouses to carry out repetitive tasks with high precision. As robot hardware becomes cheaper it will become possible for people to afford robots that help automate chores at home. In order for such robots to be useful for multiple tasks, people without programming and robotic experience will need a way to instruct robots to perform everyday tasks through a natural interface. In this work, a task specification method is proposed that is able to ground natural language to the domain of object retrieval.


{\bf Problem Statement}
The state of a rigid 3D object is describable as a point (or configuration) in $SE(3)$, which is a 6D space expressing both the translation and rotation of the object. For an environment with $n$ objects with configurations $s_0,s_1,...,s_n \in SE(3)$ the problem of object rearrangement involves finding a sequence of motions $\pi_0,\pi_1,...,\pi_m : [0,1] \to \Cspace$ for a robot arm to move the objects from their starting configurations to some given goal configurations $g_0,g_1,...,g_n \in SE(3)$. More generally, we can consider a collection of valid goal configurations per object $G_0,...,G_n \subset SE(3)$.

For the specific task of object retrieval, some of the object start configurations may be unknown due to object occlusions. Furthermore, the goal configurations aren't final as the task of object retrieval may require multiple arrangement before the target object is discovered. In order to bypass a brute force search for object discovery, this work proposes using information from natural language descriptions of target objects to influence the planning process. For example, if a human requests, ``pick up the green apple behind the cereal box'', then the planner should be able to deduce to move the cereal box out of the way in order to find the target object (green apple).

In addition to using information only from the original instruction, the planner integrates a full conversational agent to occasionally query the user for more information. More queries could be useful when either a human gives contradictory (or seemingly contradictory) information, the initial human description was vague, or the object discovery is taking too long (too many wrong guesses).

Assumptions:
 - 

TODO: define dependency graph + add figure

{\bf Outline of Online Steps to Tackle this Problem}
\begin{enumerate}
    \item Query the user for a target object.
    \item Parse the user's description and deduce any hints to the objects location in the scene.
    \item \label{item:loop} Given current sensing of the environment + the parsing of user hint (if any). Construct a dependency graph where each node represents an object and each directed edge represents a relation that necessitates moving the target object before the source object.
    \item Perform a DFS from the target object node (or another source node if no hint was provided) to retrieve an ordering of object manipulations.
    \item For each object in the ordering, pick it up, re-sense, and place it somewhere that minimizes further object occlusion (TODO). 
    \item Repeat from step \ref{item:loop} until object is visible. (optionally query the user for additional information at various iterations depending on preference of saving human time vs robot time)
\end{enumerate}

%{\bf Related Work on Grounding Language Instructions}
%Initial approaches to grounding natural language robot task descriptions approached the problem as a translation problem. They would use sentence parsers to transform sentences in natural language into a more formal language, such as LTL \cite{raman2013sorry} or semspec \cite{pomarlan2018robot}, and then apply cleverly constructed grammars to parse the intermediate languages into an executable program for a given robot system. Such approaches have the benefit of handling multiple failure modes in task specification and can be used to produce explanations of why a given task was found to be invalid. Drawbacks of such approaches are their complexity in adapting to new robot systems and ability to handle ambiguity in user descriptions.

%In order to address ambiguity in language input some approached the problem of grounding natural language descriptions through Bayesian inference by constructing factor graphs from a given sentence parsing. Such approaches inferred either robot actions directly \cite{tellex2011understanding}, constraints on the workspace \cite{howard2014natural, paul2018efficient}, which were passed to a conventional motion planner, or formal clauses \cite{misra2016tell} which are evaluated into an executable plan. These techniques are more adaptable to a larger variety of language input but have trouble scaling with workspace complexity.% due to an explosion of factors.

%More recently data driven approaches have tackled natural language grounding. Some use demonstrations to learn reward functions for Object Oriented Markov Decision Problems (OO-MDPs) via inverse reinforcement learning (IRL) \cite{macglashan2015grounding, arumugam2017accurately, arumugam2019grounding}. Others have also proposed end-to-end deep neural networks that directly learn and tie together perception and language models \cite{shridhar2018interactive, hatori2018interactively, mees2021composing}.

{\bf Related Work}
Object retrieval (sometimes called mechanical search) has had many recent works focus on cluttered and occluded scenes. \cite{bejjani2021occlusion,huang2020mechanical,kurenkov2020visuomotor,danielczuk2019mechanical} that use various models
% TODO: state explicitly which models
to reason over the scene to determine manipulations that will make progress towards the discovery of a target object. This work uses a voxelization of the workspace to reason about object occupancy and occlusion.

Initial approaches integrating natural language descriptions would would use sentence parsers to ``translate'' sentences in natural language into a more formal language, such as LTL \cite{raman2013sorry} or semspec \cite{pomarlan2018robot}, and then apply cleverly constructed grammars to parse the intermediate languages into an executable program for a given robot system. Such approaches have the benefit of handling multiple failure modes in task specification and can be used to produce explanations of why a given task was found to be invalid. Drawbacks of such approaches are their complexity in adapting to new robot systems and ability to handle ambiguity in user descriptions.
The focus of this work is similar in terms of reasoning about feasibility of language instructions but also integrates a conversation agent that could resolve contradictory information or ambiguity.

Other related efforts have used natural language to learn new task primitives \cite{suddrey_teaching_2017} or as input into a hierarchical search process \cite{kurenkov_semantic_2021}. One of the motivations for exploring how a human instruction can complement the partial scene observation is to speed up scene reconstruction and object discovery. To that end, Human-in-the-loop systems \cite{papallas2020non} have been implemented but rely on more direct user interface elements rather than natural language, and don't deal with object occlusion. The proposed work differs by using the language-based task description to inform the planner about the unobserved space.
A recent work \cite{zheng2021spatial} has a similar motivation to integrate language reasoning to speed up task planning for a city-scale navigation environment.
Work on Hierarchical Mechanical Search \cite{kurenkov_semantic_2021} does appear similar to the proposed setup, but handles scene representation and language processing separately, while the current work aims to integrate these two sources of information. 

At least one work does integrate language and task reasoning \cite{nguyen_robot_2020} in order to retrieve objects based on their utility. This work focus not on the object selection but on object discovery within an occluded scene.



{\bf Experimental Setup}
First, multiple objects with basic shapes, such as blocks and cylinders are generated on a tabletop. Some objects are stacked on top of others and a few objects placed purposefully to occlude others. The Baxter robot is put behind the table armed with two parallel grippers big enough to grasp any of the objects on the table. A camera to sense the scene is positioned on the `waist' of the robot with side view of the table (as opposed to from above).

TODO: figure of the scene