\section{Appendix}

In this appendix, we present more formally the matching functions used as baselines for our study in (Section~\ref{sec:supp_matching}), as well as extra experiments that study the impact of different hyper-parameters and present results of different task setups (Section~\ref{sec:supp_ablations}). 


\input{figure_wrappers/supplementary_matching_methods}
\subsection{Baseline matching functions}
\label{sec:supp_matching}

In this section, we describe the different matching functions used as our study's baseline\alert{, and depicted in Figure~\ref{fig:matching}}.
Some matching functions use temporal information by leveraging the absolute or relative position of the pairwise similarities $m_{ij}$. The matching functions that use temporal information are called \emph{temporal} whereas the others are called \emph{non-temporal}.


\subsubsection{Temporal matching functions.}

We provide a list of the temporal matching functions implemented in this study as baselines. Some of them were already introduced in prior work.

\noindent
\myparagraph{Diagonal (Diag)} is used as a baseline in prior work \cite{otam}. It is given by $s(M)= \displaystyle \nicefrac{\sum_{ij} m_{ii}}{n}$. It assumes temporally aligned video pairs.

\noindent
\myparagraph{OTAM}~\cite{otam} uses and extends Dynamic Time Warping~\cite{Muller2007DTW} to find an alignment path on $M$ over which similarities are averaged to produce the video-to-video similarity. A differentiable variant is used for training.

\noindent
\myparagraph{Flatten+FC (Linear)} is a simple baseline we use to learn temporal matching by flattening $M$ and feeding it to a Fully Connected (FC) layer without bias and with a single scalar output. Video-to-video similarity is therefore given by $s(M) = \sum_{ij} w_{ij} m_{ij}$, where $w_{ij}$ are learnable parameters which are $n^2$ in total.

\noindent
\myparagraph{ViSiL}~\cite{visil} is an approach originally introduced for the task of video retrieval. We apply it to few-shot action recognition for the first time. A small Fully Convolutional Network (FCN) is applied on $M$. Its output is a filtered temporal similarity matrix, and the Chamfer similarity is applied on it. The filtering is performed according to the small temporal context captured by the small receptive field of this network.

\subsubsection{Non-temporal matching functions.} 
We provide a list of the non-temporal matching functions that were implemented in this study as baselines. 

\noindent
\myparagraph{Mean} is used as a baseline in prior work \cite{otam}. It is given by $s(M)= \displaystyle \nicefrac{\sum_{ij} m_{ij}}{n^2}$. It supposes all the clip pairs should contribute equally to the similarity score.

\noindent
\myparagraph{Max} is used as a baseline in prior work \cite{otam}. It is given by $s(M)= \max_{ij} m_{ij}$.
It supposes that selecting the best matching clip pair is enough to recognize the action. 




\subsection{Additional ablations and impact of hyper-parameters}
\label{sec:supp_ablations}
In this section, we present additional ablations to evaluate the impact of the feature projection head, the ordering of the tuples, and the number of examples per class used in the support set. We also report the impact of using the different variants for the \kinetics and \ucf datasets.


\input{table_wrappers/supplementary_feature_embedding}

\input{table_wrappers/supplementary_projection_dimension}


\subsubsection{The impact of the projection layer} for matching methods is validated in Table~\ref{tab:feature_embeddings_dataloader}.
The performance is consistently improved on all setups and methods by including and learning a projection layer.
Although the backbone is trained with TSL on the same meta-train set, the projection layer allows features and values in the temporal similarity matrix to better align with each matching process.

\subsubsection{Projection head dimension.}
To match the work from \cite{trx}, we set the projection dimension to $D=1152$. This section evaluates the effect of using different values for $D$. The results are reported in Table~\ref{tab:feature_projection_dimension}. A minimum value of $D=1024$ seems enough and could be used for future experiments.


\subsubsection{Impact of using different variants}

\label{sec:supplementary_ablation}
\input{table_wrappers/supplementary_ablation}

We report the accuracy for the different variants of Chamfer++ for the \kinetics and \ucf datasets in Table~\ref{tab:chamfer_ablation_all}.  
As for \ssvtwo, both variants improve performances compared to the vanilla approach.


\subsubsection{Impact of ordering the clip-tuples}

In this section, we evaluate the impact of using ordered clip feature tuples $\vt^{l}$ versus using all the clip feature tuples $\vt_{all}^{l}$.
The comparison between ordered tuples and all tuples is presented in Table~\ref{tab:tuple_order}. On the \ssvtwo dataset, using ordered clip feature tuples boosts the accuracy. On the \kinetics and the \ucf datasets, using ordered tuples doesn't provide a boost and can even slightly harm the performance. 
Since the number of tuples is significantly lower when they are in order, using the ordered clip feature tuples is preferable.


\input{table_wrappers/supplementary_tuples}

\subsubsection{Impact of the number of examples per class}

The impact of $k$ is shown in Figure~\ref{fig:multi-shots} by measuring performance for an increasing number of support examples per class while keeping the number of classes fixed, $C_{t}=C_{f}=5$. We observe that TSL and TRX have inferior performances for the low-shot regime, while their performance increases faster with the number of shots. In the low-regime, Chamfer-QS++ outperforms the other methods and still keeps some benefits while the number of shots increases.
\input{figure_wrappers/supplementary_multi_shots.tex}


