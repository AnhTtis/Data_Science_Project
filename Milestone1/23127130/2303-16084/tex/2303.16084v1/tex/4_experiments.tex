\section{Experiments}
\label{sec:experiments}

We report results on the three most commonly used benchmarks for few-shot action recognition, \ie \kinetics~\cite{zhu2018cmn}, Something-Something V2 (\ssvtwo) \cite{ssv2}, and \ucf \cite{ucf101}. 
\kinetics and \ucf contain videos collected from YouTube. Each video is trimmed to include only one coarse-grained human action such as ``playing trumpet'' or ``reading book''. \ssvtwo  contains egocentric videos where humans were instructed to perform predefined actions such as ``pushing something from left to right'' or ``dropping something into something''. 

We use the train/val/test splits from~\cite{zhu2018cmn} for all three datasets, containing 64/12/24 classes, respectively. 
We learn the parameters of the R(2+1)D backbone using the train split, similar to~\cite{tsl}. 
For matching-based approaches, we learn the projection $\phi$ together with any learnable parameters in the matching function $f$ using episodic training also on the train split. We use the val split for hyper-parameter tuning and early stopping.

\looseness=-1
We evaluate on the common 1-shot and 5-shot setups.
Unless otherwise stated, we use 5-way classification tasks.
Training episodes are randomly sampled from the train set.
We use the same fixed, predefined set of 10k test episodes sampled from the test set for all methods 
% (prior work randomly samples them each time~\cite{otam,trx,Zhu2021CloserLook}). 
(prior work samples them randomly). 
We always evaluate \textit{three} trained models and report mean and standard deviation.



\subsection{Implementation details}
\label{sec:implementationdetails}
We use the publicly available code for TSL\footnote{\url{https://github.com/xianyongqin/few-shot-video-classification}} for 
training the backbone as well as for reproducing the TSL method. We adapt the public TRX\footnote{\url{https://github.com/tobyperrett/trx}} codebase for learning parameters of the matching function and testing,
as well as for reproducing the TRX and OTAM methods. 

\input{table_wrappers/main_results}

\noindent\textbf{Learning the backbone parameters.}
We start from the publicly available 34-layer R(2+1)D backbone provided by the TSL codebase. This model is pre-trained on the large Sports-1M dataset~\cite{karpathy2014large}. We follow~\cite{tsl} and use a SGD optimizer with a constant learning rate of 0.001 for the backbone and 0.1 for the 64-class linear layer. We perform early-stopping using the 64-class validation dataset. 
Then, we use the backbone as a feature extractor to extract features from $n=8$ uniformly sampled clips\footnote{Note that although TSL uses randomly-sampled clips, we found that its performance is usually better when switching to uniformly-sampled clips.}. The input video clips are composed of 16 consecutive RGB frames with a spatial resolution of 112x112, and the dimensionality of the resulting feature vector is $d=512$.

\noindent\textbf{Training matching-based methods.}
We train the matching-based methods on the training episodes with an SGD optimizer and a constant learning rate of 0.001 for every method except for TRX where we use a learning rate of 0.01. 
Similar to prior work~\cite{otam,trx}, we select the best model using early stopping by measuring performance on the
validation set. We learn the projection $\phi$ jointly with any matching parameters. We set the projection dimension to $D=1152$ if not stated otherwise. This is equivalent to the dimensionality that TRX\cite{trx} uses for its attention layer. We train all the matching methods with the cross-entropy loss that uses softmax with a learnable temperature $\tau$.

\noindent\textbf{Learning classifiers for TSL.}
Instead of pairwise matching, TSL~\cite{tsl} learns classifiers at every test episode, using all available support examples. To reproduce TSL we
follow~\cite{tsl} and use the Adam optimizer with a constant learning rate of 0.01 for 10 epochs. 
The original TSL approach uses $n=10$ clips at test-time, but we set this number to $n=8$ to keep it the same with all matching methods for a fair comparison. Preliminary experiments show that this choice doesn't affect the performance of TSL at all.

\noindent\textbf{Data augmentation.} During 
training, videos are augmented with random cropping. 
We uniformly sample 8 clips from each video with temporal jittering, \ie  randomly perturb the starting point of each clip.
Additionally, for the \kinetics dataset, we also use random horizontal flipping as data augmentation. 
Since it is important for \ssvtwo to distinguish between
left-to-right and right-to-left, we do not use horizontal flipping for that dataset.
We only apply a center crop for videos during validation and testing.





\subsection{Results}
\label{sec:results}
\input{figure_wrappers/sota_figures}

In this section, we report and analyse our results. We first discuss the gains from using temporal representations and the comparison between matching-based and classifier approaches. We then discuss the use of temporal information during matching and present results when varying the number of classes per classification task (test episode). Finally, we compare Chamfer++ to other recently published methods and show that our approach achieves state-of-the-art performance.

\myparagraph{Frame or clip-based features?} 
We start by evaluating a number of recent matching-based methods over temporal features. This is an important comparison that is missing from the current few-shot action recognition literature.
In Figure~\ref{fig:teasers}, we report one-shot performance for a number of matching methods under a common evaluation setup and using both frame-based (ResNet, blue points) and clip-based (R(2+1)D, orange points) features. 


We clearly see that using a spatio-temporal backbone significantly boosts accuracy by more than 10\% on all datasets. 
Interestingly, this is also true for the \kinetics and the \ucf datasets that are known to be more biased towards spatial context~\cite{huang2018makes}. Even for this case where context is important, we see that temporal dynamics
remain a valuable cue for few-shot action recognition. 
It is worth noting that the performance on \ucf and \kinetics appears to be saturated when using spatio-temporal representations.


\myparagraph{Pairwise matching or classifiers?}
Matching-based methods and 
classifiers are compared in Table~\ref{tab:main} and Figure~\ref{fig:teasers}.
All results in the table are computed under a common framework, \ie all methods share representations from an R(2+1)D backbone and are tested on the same set of episodes. We see that for
all setups and datasets, several matching approaches outperform TSL. In the 1-shot regime, \textit{most} matching-based methods outperform TSL.

\input{table_wrappers/sota}

\myparagraph{How useful is temporal matching?}
No significant difference in performance is observed between temporal and non-temporal matching approaches, as highlighted in Table~\ref{tab:main} and Figure~\ref{fig:teasers}.
On the \kinetics and \ucf datasets, where action classes are generally coarser and highly dependent on context~\cite{huang2018makes}, most methods we tested perform similarly well. Simply using $f(M)= \max_{ij} m_{ij}$ is enough to achieve top performance for \ucf, while the proposed Chamfer++ outperforms all other methods on \kinetics.
When it comes to the finer-grained \ssvtwo dataset, we see that, although most non-temporal matching methods lag behind the temporal ones for the 1-shot case, the proposed Chamfer++ method achieves the highest performance without temporal matching. For 5-shot action recognition, TRX, ViSiL, and Chamfer++ perform similarly well, with the last being parameter-free, more intuitive, and faster. 


\input{figure_wrappers/multi_classes}
\myparagraph{Varying the number of classes in an episode.}
Figure~\ref{fig:number_ways} shows the performance when extending the case of $C_{f}=5$ classes to the maximum number of classes, $C_{f}=24$, in the 1-shot and 5-shot regime. 
We see that for 1-shot the proposed non-temporal Chamfer++ method highly outperforms TRX and the classifier-based TSL method in all datasets.

\input{table_wrappers/ablation_ssv2}

\myparagraph{Comparison to the state-of-the-art.}
In Table~\ref{tab:sota-papers}, we compare the performance of the proposed Chamfer++ with the corresponding numbers reported in many recent few-shot action recognition papers. Although there is no direct comparison between all these methods, \ie, no common setup or backbones, we present all results jointly to show the overall progress in the task.



\subsection{Chamfer matching ablation and interpretability}
\label{sec:ablation}

In Table~\ref{tab:chamfer_ablation}, we present an ablative study with the 1-shot performance of all the different variants of Chamfer++ we discuss in Section~\ref{sec:newchamfer} on the \ssvtwo dataset. Combining support-based and query-based Chamfer matching helps learn a better projection $\phi$, while both joint matching and the use of clip tuples improve performance compared to the vanilla variant. Although using ordered clip tuples improves for 1-shot on \ssvtwo, overall, we see in Table~\ref{tab:main} that using all or ordered tuples results in more or less similar performance. 

\wrapfill

\looseness=-1
\myparagraph{Which are the most informative clips?}
As we presented in Section~\ref{sec:preliminaries}, we sample and encode a set of clips to represent each video. Not all clips are, however, equally valuable for the matching process.
To provide some insight and illustrate the proposed method's interpretability, we study a qualitative example from \ssvtwo in Figure \ref{fig:example}.
When matching the query video to the two support videos belonging to the same class (top), we see many clip-to-clip correspondences with high magnitudes (shown as thicker lines). This is not the case when matching to the support videos belonging to a negative class (bottom) and we can only see a single strong correspondence. Upon better inspection, we saw that the motion of picking up the object in the fourth clip of the query video does exhibit a left-to-right motion locally that matches the negative class. Nevertheless, the other correspondences are weaker.
Leveraging more than one clip helps Chamfer matching to disambiguate while, at the same time, its inherent selectivity makes it more robust against noisy correspondences.
% 
This example illustrates that not all the clip correspondences are equally valuable to compute a video-to-video similarity metric. The matching step is a dynamic way to select the most informative clip correspondences that highlight the similarity between two videos.

\input{figure_wrappers/visualization}

