\section{Introduction}
\label{sec:introduction}

Recognizing actions within videos is essential for analyzing trends, enhancing broadcasting experience, or filtering out inappropriate content. 
However, collecting and annotating enough video examples to train supervised models can be prohibitively time-consuming. 
It is therefore desirable to recognize new action classes with as few labeled examples as possible. 
This is the premise behind the task of few-shot learning, where models learn to adapt to a set of unseen classes for which only a few examples are available. 
In video action recognition, additional challenges arise from the temporal dimension. 
Recognition methods need to capture the scene's temporal context and temporal dynamics.

\input{figure_wrappers/method_figure1.tex}

One family of approaches is formed by \emph{matching-based} methods \cite{otam,trx,Zhu2021pal,thatipelli2021spatio}
where each
test example or ``query'' is compared against all support examples of a class to infer a class confidence score. 
Most existing matching-based methods use frame-level representations, \ie a 2D convolutional backbone that takes a frame as input, and a feature set is formed by encoding multiple frames. Feature extraction is followed by matching the query feature set $Q$ to the support example set $X$, and a similarity $s_{Q,X}$ between the two is computed.
This process is depicted in Figure~\ref{fig:first}. 
Although each feature represents an individual frame and cannot capture temporal information, the feature sets are usually temporal sequences, and the matching process can exploit such information.

Another family of approaches is formed by methods that learn a conventional linear \emph{classifier} at test time, \ie using the handful of examples available. In this case, any temporal context has to be incorporated in the representation, as shown in Figure~\ref{fig:second}.
As a representative example, 
Xian~\etal~\cite{tsl} adopt the spatio-temporal R(2+1)D architecture~\cite{r2plus1d}, where
the input is a video \textit{clip}, \ie a sequence of consecutive frames, and convolutions across the temporal dimension enable the features to encode temporal information.
Following findings in few-shot learning~\cite{wang2019simpleshot,chen2020closer}, Xian~\etal further abandon episodic training and instead fine-tune a pre-trained backbone using all training examples of the base classes. 
Using strong temporal features and by simply learning a linear classifier at test time, they report state-of-the-art results for few-shot action recognition.

Motivated by the two families of approaches presented above, we introduce a new setup depicted in Figure~\ref{fig:third} that aims at answering the following questions:

\noindent\emph{1. Do matching-based methods still have something to offer for few-shot action recognition given strong temporal representations?} 
We level the playing field with respect to representations and evaluate a number of recent matching-based approaches using strong temporal representations. We find that such approaches perform better than training a classifier at test time.

\noindent\emph{2. Is temporal matching necessary when the features capture temporal information?} We show that matching-based methods invariant to the temporal order in the feature sequence (\emph{non-temporal matching}) are performing as good as the ones that do use it (\emph{temporal matching}) on many common benchmarks. 

Inspired by the findings above, we further introduce \textbf{Chamfer++}, a novel, parameter-free and interpretable matching approach that employs Chamfer matching and is able to achieve a new state-of-the-art for one-shot action recognition on three common benchmarks.

