\section{Method}
% ------------------------------------------------
\input{figure_wrappers/method_figure2.tex}

% ------------------------------------------------
We first present the video representation and the experimental protocol used in this work in Section~\ref{sec:preliminaries}. Then, we describe a framework to fairly compare matching-based and classifier-based approaches in Section~\ref{sec:framework} and propose a new matching function in Section~\ref{sec:chamferqs}.
In the following, we refer to videos as examples and actions as classes.
\label{sec:method}

% ------------------------------------------------

\subsection{Preliminaries}
\label{sec:preliminaries}
We describe the video representation that we use and the episodic protocol as formulated in the recent literature of few-shot video action recognition, which we follow in this work.

\subsubsection{Video representation}
\label{sec:videorepresentation}

A clip $c_i$ is a sequence of $L$ consecutive RGB frames in the form of a $L \times H \times W \times 3$ tensor. A deep video backbone $b$ takes a clip $c_i$ as input and maps it to a $d$-dimensional vector $\vq_i = b(c_i) \in \real^d$, named feature.
We use the R(2+1)D backbone architecture~\cite{r2plus1d} as in the work of Xian \etal~\cite{tsl}, which uses efficient and effective separated spatio-temporal convolutions.
A video $Q$ is represented by a set of clip features $Q=\{\vq_i\}$. The clips are uniformly sampled over the temporal dimension, with possible overlap. 



\subsubsection{Episodic protocol}
\label{sec:episodic}

We adopt the commonly used setup~\cite{trx,otam}. 
The classes of the train and test sets are non-overlapping and usually named base and novel classes, respectively. 
To simulate the limited annotated data, test episodes are randomly sampled from the test set. Each episode corresponds to a different classification task and comprises query and support examples for a fixed set of classes, where labels of support examples are known, while labels of query examples are unknown. Only $k$ labeled examples per class, also named shots, are available in the support set, with $k$ typically ranging from 1 to 5. The performance is evaluated via classification accuracy on the query examples averaged over all test episodes.


\subsection{A common setup for classifier and matching-based approaches}
We identify two dominant families of approaches proposed in the recent few-shot action recognition literature: the classifier-based and the matching-based methods \footnote{Prototypical networks can be seen as an extension of matching based methods \cite{trx,Zhu2021pal,thatipelli2021spatio}. Hence we group them with the matching-based family.}. Unfortunately, discrepancies in setup and architecture between existing approaches make it difficult to compare them fairly. 
We propose to follow the same representation learning strategy and start from a common frozen backbone.


\subsubsection{Classifier-based approaches}
\label{sec:classifier}
The classifier-based approaches ~\cite{tsl,Zhu2021CloserLook} train the video representation using a classifier in the form of a linear layer. This is similar to the corresponding work on few-shot learning in the image domain~\cite{GK18}.

Classifiers are trained using the whole dataset, but episodic training cannot guarantee to see all the examples because it is intractable to sample all the possible episodes.
Xian \etal~\cite{tsl} depart from episodic training and propose a \textit{Two-stage Learning} (\textit{TSL}) process. 
During the first stage, a R(2+1)D video backbone and a classifier are learned jointly using all the labeled examples of the train set. During the second stage, the backbone remains fixed to avoid overfitting and  a newly initialized classifier needs to be trained per test episode using  the support examples. 

In both stages, a linear classifier with a soft-max function denoted by $h: \real^d \rightarrow \real^C$, where C is the number of classes, is added to the output of the backbone. During the training stage, $C=C_{t}$ is equal to the number of all classes in the train set. During the second stage, $C=C_{f}$ is equal to the number of classes per test episode, usually $C_{f}=5$.
Training is performed by optimizing the class probabilities $h(\vq_i)$ for each $\vq_i \in Q$ with the cross-entropy loss ($\mathcal{L}_{cls}$), while inference is performed by sum-pooling of the classifier output across clips, \ie $\sum_{\vq_i \in X} h(\vq_i)$.



\subsubsection{Matching-based approaches}
\label{sec:matching}
The matching-based approaches estimate the similarity between the query and all the support examples of each class to obtain class probabilities. 
Training is performed on episodes sampled from the train set, which are meant to imitate the episodes of the test set.


Let $Q=\{\vq_i\}$ and $X=\{\vx_i\}$ be two videos with $|Q|=|X|=n$ and assume that $n$ is constant across videos.
We form the \emph{temporal similarity matrix} for the ordered video pair $(Q,X)$ denoted by $M_{Q,X}\in \real^{n\times n}$, or just $M$ for brevity, with elements $m_{ij} = \phi(\vq_i)^\top \phi(\vx_j)$, where $\phi: \real^d \rightarrow \real^D$ is a learnable projection head.
The function $\phi$ consists of a linear layer, a layer normalization, and a $\ensuremath{\ell_2}$  normalization to guarantee bounded similarity values $m_{ij}$. When no linear projection is used, $\phi$ is equivalent to the identity mapping with an \ensuremath{\ell_2} normalization.
Each element $m_{ij}$ of the matrix $M$ can be seen as a temporal correspondence between clip $i$ of video $Q$ and clip $j$ of video $X$. 

We consider the family of matching approaches that infer a video-to-video similarity $S_{Q, X}$, between video $Q$ and $X$, solely based on the matrix $M$, \ie $S_{Q,X} = f(M)$, with $f:\real^{n \times n} \rightarrow \real$, named the \emph{matching function}.
The scalar $S_{Q,X}$ should be high if the two videos depict the same action. 
By definition, the result of function $f$ only depends on the strength and position of the pairwise similarities $m_{ij}$ and does not directly depend on the features themselves.
The function $f$ can either be hand-crafted or include learnable parts.
%
A graphical overview of different matching approaches is given in the 
appendix as long as a more detailed description of them.
Some matching functions use temporal information by leveraging the position, either absolute or relative, of the pairwise similarities $m_{ij}$. The matching functions that use temporal information are called \emph{temporal}, whereas the others are called \emph{non temporal}.

The pairwise video-to-video similarities between query and support examples are averaged per class to obtain class probabilities
\footnote{In prototypical networks  \cite{trx,Zhu2021pal,thatipelli2021spatio}, the pairwise clip-to-clip similarities are used as weights to compute a class prototype specific to the query example. The class probabilities are computed as the distance between a query and its prototype.}.
During training, the class probabilities are optimized with the cross-entropy loss ($\mathcal{L}_{cls}$). Inference is also performed by estimating the similarity between query and all support examples. This is a form of a k-nearest-neighbor classifier. 



\subsubsection{A common starting point}
\label{sec:framework}

We follow the training stage of TSL~\cite{tsl} to learn the R(2+1)D backbone parameters using all the annotated examples. We freeze this backbone, and treat the resulting model as a feature extractor. This is our starting point for both classifier-based and matching-based approaches which enable us to fairly compare the two families of approaches. 
Specifically, matching-based methods only learn the feature projection function $\phi$ and the matching parameters when needed in a test-agnostic way. Unlike classifier-based methods, which need to train a classifier for every testing episode, matching-based approaches require no learning or adaptation at test time. They only need the pairwise matching between the query and each one of the support examples. 


\subsection{Chamfer++}
\label{sec:newchamfer}
We propose a new matching function, Chamfer++, which is non-temporal and achieves top performance while being parameter-free and intuitive. 
It is an extension of Chamfer similarity (Figure~\ref{fig:chamfer_matching}) with joint-matching over multiple shots  (Figure~\ref{fig:joint_matching}) applied in conjunction with clip-feature-tuples instead of clip features (Figure~\ref{fig:clip_tuples}). This section details its main components.

\subsubsection{Chamfer}
\label{sec:chamferqs}
The Chamfer matching function $f_{Q}$ is given by
\begin{equation}
f_{Q}(M) := \frac{1}{n}\sum_i \max_j m_{ij}. 
\end{equation}
Each clip of the query example is matched with its most similar one within the clips of a support example to produce the score between this specific query clip and this support example. The final video-to-video similarity is the average of all the query clip scores.
The Chamfer matching function implies that each clip sampled from the query example contributes to the similarity score by matching its closest clip in the support example. 
One can transpose the temporal similarity matrix and derive the symmetric process where each clip from the support example needs to match a query clip. Then the matching function becomes
\begin{equation}
f_{S}(M) := \frac{1}{n}\sum_j \max_i m_{ij}.
\end{equation}
Chamfer-S is equivalent to the Chamfer on the transposed matrix $M^\top$. 
Summing the two gives a symmetric Chamfer variant, where all clips from both the query and the support example are required to match: 
\begin{equation}
f_{QS}(M) := f_{Q}(M) + f_{S}(M).
\label{eq:chamferqs}
\end{equation}
We refer to this symmetric variant as simply \emph{Chamfer matching} in the context of few-shot action recognition.
In the following, we also refer to Chamfer-Q as query-based and Chamfer-S as support-based.


\subsubsection{Joint-matching}
\label{sec:jointmatching}

As discussed in Section~\ref{sec:matching}, the standard option to compute the query-to-class probabilities is by averaging the pairwise similarity score between the query example and all the support examples belonging to the class.
Instead, we propose to match all support examples jointly.
We concatenate the temporal similarity matrices between the query and all support examples. It creates the joint temporal similarity matrix 
$M^{+}$, with $M^{+}\in \real^{n \times kn}$.
Then, we compute the matching function on top of 
$M^{+}$
to obtain video-to-class similarity 
$S_{Q,c} := s(M^{+})$. 
We evaluate the impact of this newly proposed joint-matching versus the standard  single-matching in Section~\ref{sec:ablation}. We refer to this variant as Chamfer+ in the rest of the manuscript.

\subsubsection{Clip tuples}
\label{sec:cliptuples}

Besides the case where each clip feature is matched independently, Perret \etal \cite{trx} additionally propose matching feature \emph{tuples}. Inspired by this, we extend Chamfer to enable matching of \emph{clip} feature tuples formed by any clip subset of fixed length $l$.
A clip feature tuple $\vt^{l}$ contains $l$ clip features, non-necessarily consecutive, but with the same relative order as in $Q = \{\vq_i\}$. For example, $\vt^{2} = \{ (q_i, q_{j \neq i)}\}$. 
Each clip feature tuple is concatenated and fed to the learnable projection head $\phi: \real^{ld} \rightarrow \real^D$. 
The resulting temporal similarity matrix is $M^{++}\in \real^{n'\times kn'}$, with $n'={n \choose l}$. 
The clip tuples are sub-sequence representations on top of single-clip representations.  By definition, clip tuples add additional temporal information to the representation. But the matching function remains non-temporal.

We also define non-temporally ordered clip tuples $\vt_{all}^{l}$ as the permutations of $l$ clip features. For example, $\vt_{all}^{2} = \{ (q_i, q_{j \neq i)}\}$. The resulting temporal similarity matrix is $M^{++}\in \real^{n'\times kn'}$ with $n'=n!$. 
The comparison between ordered and all tuples is presented in Table~\ref{tab:chamfer_ablation} and the appendix.
In the following, the extension of Chamfer matching that jointly matches multiple shots and uses clip tuples is referred to as \textbf{Chamfer++}. Unless otherwise stated, we use ordered clip tuples.
