\section{Related Work}
\label{sec:related}

In the image domain, several methods for few-shot classification are metric-learning-based~\cite{vinyals2016matching,Snell2017Proto,Sung2018Compare,Doersh2020CrossTransformers,cao21comet} and use a k-nearest-neighbor classifier at test time. 
Despite the dominance of meta-learning-based methods in the area~\cite{finn2017model,Finn2018,Grant2018,Rusu2019LatentEmbedding,lifchitz2019dense}, several recent studies highlight the importance of starting from strong visual representations. It is shown~\cite{wang2019simpleshot,chen2020closer} that if one learns representations without meta-learning but using instead all available data from all base classes, simple nearest-neighbors~\cite{wang2019simpleshot} or parametric classifier~\cite{chen2020closer} baselines work on-par or better than most methods on common benchmarks. Similar observations are made for few-shot learning in the video domain~\cite{tsl,Zhu2021CloserLook,Zhu2021pal}.

As with the image domain, most methods leverage meta-learning and are grouped into initialization-based~\cite{tsl,Zhu2021CloserLook},  metric-learning~\cite{zhu2018cmn,bishay2019tarn,otam,Zhang2020FewShotAR,trx,Zhu2021pal} and generative-based~\cite{Dwivedi2019Protogan,tsl} methods. 
However, unlike the image domain, most methods for few-shot action recognition try to take into account the \textit{temporal} dimension in the visual representations and/or during the matching process. In fact, most methods incorporate temporal matching~\cite{zhu2018cmn,otam,bishay2019tarn,visil,Zhang2020FewShotAR,trx,li2021ta2n}.
The features are either extracted from single frames~\cite{zhu2018cmn,otam,trx} or clips~\cite{bishay2019tarn,tsl,Zhang2020FewShotAR} where temporal information is already captured in the features.

Some matching methods explicitly aim for temporal alignment and estimate video-to-video similarity via an ordered temporal alignment score.
OTAM~\cite{otam} finds the optimal path on the temporal similarity matrix via a differentiable approximation of the Dynamic Time Warping (DTW)~\cite{Muller2007DTW} algorithm, a method also used for alignment in other temporal tasks~\cite{chang2019d3tw,dvornik2021drop}. 
Other methods like ARN~\cite{Zhang2020FewShotAR} use spatio-temporal attention, while TA$^2$N~\cite{li2021ta2n} proposes two-stage spatial-temporal alignment.
TARN~\cite{bishay2019tarn} uses temporal attention over clip sequences for alignment. During the training process, the attention parameters, together with the parameters from a subsequent recurrent network, are learned such that features are aligned between the query and a support video from the correct class.
 
Other methods like TRX \cite{trx} or PAL \cite{Zhu2021pal} do not explicitly seek alignment but rely on \textit{cross-attention} mechanisms over the query and support features to perform temporal matching. In both cases, class-wise representations are constructed and matching is performed directly in a \textit{video-to-class} manner. For TRX, class representations are adapted on-the-fly in a query-conditioned manner, while for PAL the query features are instead adapted to match pre-computed class-wise representations. 
In our study, we show how such direct video-to-class approaches are highly competitive when more than one videos per class are provided, but that video-to-video methods are superior and more efficient for the one-shot case. 

Recently, a number of methods suggest that the meta-learning framework is not optimal for the representation learning phase. Following similar observations in the image classification domain~\cite{wang2019simpleshot,chen2020closer}, some few-shot action recognition methods~\cite{tsl,Zhu2021CloserLook,Zhu2021pal} learn their representation on the full train set and report higher performance on multiple benchmarks. As discussed in Section~\ref{sec:framework}, we follow such methods, and use a strong representation learned on the entire train set as the starting point for our study on temporal matching.