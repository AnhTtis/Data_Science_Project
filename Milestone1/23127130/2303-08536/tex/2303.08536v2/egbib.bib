@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{hong2022vcafe,
  title={Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition},
  author={Hong, Joanna and Kim, Minsu and Yoo, Daehun and Ro, Yong Man},
  journal={arXiv preprint arXiv:2207.06020},
  year={2022}
}

@inproceedings{thiemann2013demand,
  title={The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings},
  author={Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
  booktitle={Proceedings of Meetings on Acoustics ICA2013},
  volume={19},
  number={1},
  pages={035081},
  year={2013},
  organization={Acoustical Society of America}
}

@article{varga1993noisex92,
  title={Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems},
  author={Varga, Andrew and Steeneken, Herman JM},
  journal={Speech communication},
  volume={12},
  number={3},
  pages={247--251},
  year={1993},
  publisher={Elsevier}
}

@inproceedings{ko2015audio,
  title={Audio augmentation for speech recognition},
  author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={Sixteenth annual conference of the international speech communication association},
  year={2015}
}

@article{afouras2018deep,
  title={Deep audio-visual speech recognition},
  author={Afouras, Triantafyllos and Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2018},
  publisher={IEEE}
}

@article{wang2019multigrained,
  title={Multi-grained spatio-temporal modeling for lip-reading},
  author={Wang, Chenhao},
  journal={arXiv preprint arXiv:1908.11618},
  year={2019}
}

@article{xiao2020deformation,
  title={Deformation Flow Based Two-Stream Network for Lip Reading},
  author={Xiao, Jingyun and Yang, Shuang and Zhang, Yuanhang and Shan, Shiguang and Chen, Xilin},
  journal={arXiv preprint arXiv:2003.05709},
  year={2020}
}

@INPROCEEDINGS {zhao2020mi,
author = {X. Zhao and S. Yang and S. Shan and X. Chen},
booktitle = {2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020) (FG)},
title = {Mutual Information Maximization for Effective Lip Reading},
year = {2020},
volume = {},
issn = {},
pages = {843-850},
keywords = {lip reading;mutual information;deep learning},
doi = {10.1109/FG47880.2020.00133},
url = {https://doi.ieeecomputersociety.org/10.1109/FG47880.2020.00133},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@article{zhang2020cutout,
  title={Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition},
  author={Zhang, Yuanhang and Yang, Shuang and Xiao, Jingyun and Shan, Shiguang and Chen, Xilin},
  journal={arXiv preprint arXiv:2003.03206},
  year={2020}
}

@inproceedings{chung2016lrw,
  title={Lip reading in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Asian Conference on Computer Vision},
  pages={87--103},
  year={2016},
  organization={Springer}
}

@article{stafylakis2017reslstm,
  title={Combining residual networks with LSTMs for lipreading},
  author={Stafylakis, Themos and Tzimiropoulos, Georgios},
  journal={arXiv preprint arXiv:1703.04105},
  year={2017}
}

@inproceedings{martinez2020mstcn,
  title={Lipreading Using Temporal Convolutional Networks},
  author={Martinez, Brais and Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6319--6323},
  year={2020},
  organization={IEEE}
}

@ARTICLE{kim2021cromm,
  author={Kim, Minsu and Hong, Joanna and Park, Se Jin and Ro, Yong Man},
  journal={IEEE Transactions on Multimedia}, 
  title={CroMM-VSR: Cross-Modal Memory Augmented Visual Speech Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMM.2021.3115626}}

@article{assael2016lipnet,
  title={Lipnet: End-to-end sentence-level lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and De Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01599},
  year={2016}
}

@inproceedings{graves2006ctc,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={369--376},
  year={2006}
}

@inproceedings{chung2017lrs2,
  title={Lip reading sentences in the wild},
  author={Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3444--3453},
  year={2017},
  organization={IEEE}
}

@article{afouras2018lrs3,
  title={LRS3-TED: a large-scale dataset for visual speech recognition},
  author={Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1809.00496},
  year={2018}
}

@InProceedings{Kim_2021_ICCV,
    author    = {Kim, Minsu and Hong, Joanna and Park, Se Jin and Ro, Yong Man},
    title     = {Multi-Modality Associative Bridging Through Memory: Speech Sound Recollected From Face Video},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {296-306}
}
@article{noda2015audio,
  title={Audio-visual speech recognition using deep learning},
  author={Noda, Kuniaki and Yamaguchi, Yuki and Nakadai, Kazuhiro and Okuno, Hiroshi G and Ogata, Tetsuya},
  journal={Applied Intelligence},
  volume={42},
  number={4},
  pages={722--737},
  year={2015},
  publisher={Springer}
}

@inproceedings{xu2020discriminative,
  title={Discriminative multi-modality speech recognition},
  author={Xu, Bo and Lu, Cheng and Guo, Yandong and Wang, Jacob},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14433--14442},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{petridis2018end,
  title={End-to-end audiovisual speech recognition},
  author={Petridis, Stavros and Stafylakis, Themos and Ma, Pingehuan and Cai, Feipeng and Tzimiropoulos, Georgios and Pantic, Maja},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={6548--6552},
  year={2018},
  organization={IEEE}
}

@inproceedings{petridis2018audio,
  title={Audio-visual speech recognition with a hybrid ctc/attention architecture},
  author={Petridis, Stavros and Stafylakis, Themos and Ma, Pingchuan and Tzimiropoulos, Georgios and Pantic, Maja},
  booktitle={2018 IEEE Spoken Language Technology Workshop (SLT)},
  pages={513--520},
  year={2018},
  organization={IEEE}
}

@inproceedings{ma2021end,
  title={End-to-end audio-visual speech recognition with conformers},
  author={Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7613--7617},
  year={2021},
  organization={IEEE}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@inproceedings{mroueh2015deep,
  title={Deep multimodal learning for audio-visual speech recognition},
  author={Mroueh, Youssef and Marcheret, Etienne and Goel, Vaibhava},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2130--2134},
  year={2015},
  organization={IEEE}
}

@inproceedings{huang2013audio,
  title={Audio-visual deep learning for noise robust speech recognition},
  author={Huang, Jing and Kingsbury, Brian},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={7596--7599},
  year={2013},
  organization={IEEE}
}

@inproceedings{weninger2015speech,
  title={Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR},
  author={Weninger, Felix and Erdogan, Hakan and Watanabe, Shinji and Vincent, Emmanuel and Roux, Jonathan Le and Hershey, John R and Schuller, Bj{\"o}rn},
  booktitle={International conference on latent variable analysis and signal separation},
  pages={91--99},
  year={2015},
  organization={Springer}
}

@article{tan2019learning,
  title={Learning complex spectral mapping with gated convolutional recurrent networks for monaural speech enhancement},
  author={Tan, Ke and Wang, DeLiang},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={380--390},
  year={2019},
  publisher={IEEE}
}

@article{wang2020complex,
  title={Complex spectral mapping for single-and multi-channel speech enhancement and robust ASR},
  author={Wang, Zhong-Qiu and Wang, Peidong and Wang, DeLiang},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={28},
  pages={1778--1787},
  year={2020},
  publisher={IEEE}
}

@inproceedings{braun2021towards,
  title={Towards efficient models for real-time deep noise suppression},
  author={Braun, Sebastian and Gamper, Hannes and Reddy, Chandan KA and Tashev, Ivan},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={656--660},
  year={2021},
  organization={IEEE}
}

@inproceedings{voo2022delving,
  title={Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets},
  author={Voo, Kenny TR and Jiang, Liming and Loy, Chen Change},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4711--4720},
  year={2022}
}

@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={European conference on computer vision},
  pages={104--120},
  year={2020},
  organization={Springer}
}

@inproceedings{lee2021parameter,
  title={Parameter Efficient Multimodal Transformers for Video Representation Learning},
  author={Lee, Sangho and Yu, Youngjae and Kim, Gunhee and Breuel, Thomas M and Kautz, Jan and Song, Yale},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{kim2017joint,
  title={Joint CTC-attention based end-to-end speech recognition using multi-task learning},
  author={Kim, Suyoun and Hori, Takaaki and Watanabe, Shinji},
  booktitle={2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={4835--4839},
  year={2017},
  organization={IEEE}
}

@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

@inproceedings{hernandez2018ted,
  title={TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation},
  author={Hernandez, Fran{\c{c}}ois and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Esteve, Yannick},
  booktitle={International conference on speech and computer},
  pages={198--208},
  year={2018},
  organization={Springer}
}

@article{ardila2019common,
  title={Common voice: A massively-multilingual speech corpus},
  author={Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},
  journal={arXiv preprint arXiv:1912.06670},
  year={2019}
}

@article{ma2022visual,
  title={Visual Speech Recognition for Multiple Languages in the Wild},
  author={Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  journal={arXiv preprint arXiv:2202.13084},
  year={2022}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{mohamed2011acoustic,
  title={Acoustic modeling using deep belief networks},
  author={Mohamed, Abdel-rahman and Dahl, George E and Hinton, Geoffrey},
  journal={IEEE transactions on audio, speech, and language processing},
  volume={20},
  number={1},
  pages={14--22},
  year={2011},
  publisher={IEEE}
}

@ARTICLE{hinton2012speech,
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
  year={2012},
  volume={29},
  number={6},
  pages={82-97},
  doi={10.1109/MSP.2012.2205597}}

@article{dahl2011context,
  title={Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition},
  author={Dahl, George E and Yu, Dong and Deng, Li and Acero, Alex},
  journal={IEEE Transactions on audio, speech, and language processing},
  volume={20},
  number={1},
  pages={30--42},
  year={2011},
  publisher={IEEE}}

@inproceedings{seide2011conversational,
  title={Conversational speech transcription using context-dependent deep neural networks},
  author={Seide, Frank and Li, Gang and Yu, Dong},
  booktitle={Twelfth annual conference of the international speech communication association},
  year={2011}
}

@article{hannun2014deep,
  title={Deep speech: Scaling up end-to-end speech recognition},
  author={Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and others},
  journal={arXiv preprint arXiv:1412.5567},
  year={2014}
}
  
@inproceedings{abdel2012applying,
  title={Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Penn, Gerald},
  booktitle={2012 IEEE international conference on Acoustics, speech and signal processing (ICASSP)},
  pages={4277--4280},
  year={2012},
  organization={IEEE}
}

@inproceedings{sainath2013improvements,
  title={Improvements to deep convolutional neural networks for LVCSR},
  author={Sainath, Tara N and Kingsbury, Brian and Mohamed, Abdel-rahman and Dahl, George E and Saon, George and Soltau, Hagen and Beran, Tomas and Aravkin, Aleksandr Y and Ramabhadran, Bhuvana},
  booktitle={2013 IEEE workshop on automatic speech recognition and understanding},
  pages={315--320},
  year={2013},
  organization={IEEE}
}

@article{abdel2014convolutional,
  title={Convolutional neural networks for speech recognition},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={IEEE/ACM Transactions on audio, speech, and language processing},
  volume={22},
  number={10},
  pages={1533--1545},
  year={2014},
  publisher={IEEE}
}

@inproceedings{graves2013speech,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6645--6649},
  year={2013},
  organization={Ieee}
}

@article{sak2014long,
  title={Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition},
  author={Sak, Ha{\c{s}}im and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:1402.1128},
  year={2014}
}

@inproceedings{graves2014towards,
  title={Towards end-to-end speech recognition with recurrent neural networks},
  author={Graves, Alex and Jaitly, Navdeep},
  booktitle={International conference on machine learning},
  pages={1764--1772},
  year={2014},
  organization={PMLR}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{amodei2016deep2,
  title={Deep speech 2: End-to-end speech recognition in english and mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
  booktitle={International conference on machine learning},
  pages={173--182},
  year={2016},
  organization={PMLR}
}

@article{baevski2020wav2vec2,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{schneider2019wav2vec,
  title={wav2vec: Unsupervised pre-training for speech recognition},
  author={Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  journal={arXiv preprint arXiv:1904.05862},
  year={2019}
}

@article{baevski2019vq,
  title={vq-wav2vec: Self-supervised learning of discrete speech representations},
  author={Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  journal={arXiv preprint arXiv:1910.05453},
  year={2019}
}

@inproceedings{kim2022distinguishing,
  title={Distinguishing Homophenes using Multi-head Visual-audio Memory for Lip Reading},
  author={Kim, Minsu and Yeo, Jeong Hun and Ro, Yong Man},
  booktitle={Proceedings of the 36th AAAI Conference on Artificial Intelligence, Vancouver, BC, Canada},
  volume={22},
  year={2022}
}

@inproceedings{martinez2020lipreading,
  title={Lipreading using temporal convolutional networks},
  author={Martinez, Brais and Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6319--6323},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhang2019spatio,
  title={Spatio-temporal fusion based convolutional sequence learning for lip reading},
  author={Zhang, Xingxuan and Cheng, Feng and Wang, Shilin},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={713--722},
  year={2019}
}

@inproceedings{zhao2020hearing,
  title={Hearing lips: Improving lip reading by distilling speech recognizers},
  author={Zhao, Ya and Xu, Rui and Wang, Xinchao and Hou, Peng and Tang, Haihong and Song, Mingli},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={6917--6924},
  year={2020}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{yu2020audio,
  title={Audio-visual recognition of overlapped speech for the lrs2 dataset},
  author={Yu, Jianwei and Zhang, Shi-Xiong and Wu, Jian and Ghorbani, Shahram and Wu, Bo and Kang, Shiyin and Liu, Shansong and Liu, Xunying and Meng, Helen and Yu, Dong},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6984--6988},
  year={2020},
  organization={IEEE}
}


@article{stewart2013robust,
  title={Robust audio-visual speech recognition under noisy audio-video conditions},
  author={Stewart, Darryl and Seymour, Rowan and Pass, Adrian and Ming, Ji},
  journal={IEEE transactions on cybernetics},
  volume={44},
  number={2},
  pages={175--184},
  year={2013},
  publisher={IEEE}
}

@article{cen2020classification,
  title={Classification of Occluded Images for Large-Scale Datasets With Numerous Occlusion Patterns},
  author={Cen, Feng and Zhao, Xiaoyu and Li, Wuzhuang and Zhu, Fanglai},
  journal={IEEE Access},
  volume={8},
  pages={170883--170897},
  year={2020},
  publisher={IEEE}
}

@inproceedings{li2018patch,
  title={Patch-gated CNN for occlusion-aware facial expression recognition},
  author={Li, Yong and Zeng, Jiabei and Shan, Shiguang and Chen, Xilin},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)},
  pages={2209--2214},
  year={2018},
  organization={IEEE}
}

@inproceedings{wang2020robust,
  title={Robust object detection under occlusion with context-aware compositionalnets},
  author={Wang, Angtian and Sun, Yihong and Kortylewski, Adam and Yuille, Alan L},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12645--12654},
  year={2020}
}

@article{cen2021deep,
  title={Deep feature augmentation for occluded image classification},
  author={Cen, Feng and Zhao, Xiaoyu and Li, Wuzhuang and Wang, Guanghui},
  journal={Pattern Recognition},
  volume={111},
  pages={107737},
  year={2021},
  publisher={Elsevier}
}

@article{li2020yolo,
  title={YOLO-ACN: Focusing on small target and occluded object detection},
  author={Li, Yongjun and Li, Shasha and Du, Haohao and Chen, Lijia and Zhang, Dongming and Li, Yao},
  journal={IEEE Access},
  volume={8},
  pages={227288--227303},
  year={2020},
  publisher={IEEE}
}

@article{li2018occlusion,
  title={Occlusion aware facial expression recognition using CNN with attention mechanism},
  author={Li, Yong and Zeng, Jiabei and Shan, Shiguang and Chen, Xilin},
  journal={IEEE Transactions on Image Processing},
  volume={28},
  number={5},
  pages={2439--2450},
  year={2018},
  publisher={IEEE}
}

@inproceedings{makino2019RNNt,
  title={Recurrent neural network transducer for audio-visual speech recognition},
  author={Makino, Takaki and Liao, Hank and Assael, Yannis and Shillingford, Brendan and Garcia, Basilio and Braga, Otavio and Siohan, Olivier},
  booktitle={2019 IEEE automatic speech recognition and understanding workshop (ASRU)},
  pages={905--912},
  year={2019},
  organization={IEEE}
}

@inproceedings{petridis2018avsrhybrid,
  title={Audio-visual speech recognition with a hybrid ctc/attention architecture},
  author={Petridis, Stavros and Stafylakis, Themos and Ma, Pingchuan and Tzimiropoulos, Georgios and Pantic, Maja},
  booktitle={2018 IEEE Spoken Language Technology Workshop (SLT)},
  pages={513--520},
  year={2018},
  organization={IEEE}
}

@inproceedings{yu2020overlaplrs2,
  title={Audio-visual recognition of overlapped speech for the lrs2 dataset},
  author={Yu, Jianwei and Zhang, Shi-Xiong and Wu, Jian and Ghorbani, Shahram and Wu, Bo and Kang, Shiyin and Liu, Shansong and Liu, Xunying and Meng, Helen and Yu, Dong},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6984--6988},
  year={2020},
  organization={IEEE}
}

@article{shi2022learning,
  title={Learning audio-visual speech representation by masked multimodal cluster prediction},
  author={Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  journal={arXiv preprint arXiv:2201.02184},
  year={2022}
}

@article{kim2021lip,
  title={Lip to speech synthesis with visual context attentional GAN},
  author={Kim, Minsu and Hong, Joanna and Ro, Yong Man},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2758--2770},
  year={2021}
}

@article{kim2023lip,
  title={Lip-to-Speech Synthesis in the Wild with Multi-task Learning},
  author={Kim, Minsu and Hong, Joanna and Ro, Yong Man},
  journal={arXiv preprint arXiv:2302.08841},
  year={2023}
}

@article{hong2021speech,
  title={Speech reconstruction with reminiscent sound via visual voice memory},
  author={Hong, Joanna and Kim, Minsu and Park, Se Jin and Ro, Yong Man},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3654--3667},
  year={2021},
  publisher={IEEE}
}

@inproceedings{hong2022visagesyntalk,
  title={VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection},
  author={Hong, Joanna and Kim, Minsu and Ro, Yong Man},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXVI},
  pages={452--468},
  year={2022},
  organization={Springer}
}

@inproceedings{kim2022speaker,
  title={Speaker-adaptive Lip Reading with User-dependent Padding},
  author={Kim, Minsu and Kim, Hyunjun and Ro, Yong Man},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXVI},
  pages={576--593},
  year={2022},
  organization={Springer}
}

@article{kim2023prompt,
  title={Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition},
  author={Kim, Minsu and Kim, Hyung-Il and Ro, Yong Man},
  journal={arXiv preprint arXiv:2302.08102},
  year={2023}
}

@inproceedings{prajwal2022sub,
  title={Sub-word level lip reading with visual attention},
  author={Prajwal, KR and Afouras, Triantafyllos and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5162--5172},
  year={2022}
}