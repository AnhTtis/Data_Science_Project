Khan2022khan@Article{Nti2020,
author={Nti, Isaac Kofi
and Adekoya, Adebayo Felix
and Weyori, Benjamin Asubam},
title={A systematic review of fundamental and technical analysis of stock market predictions},
journal={Artificial Intelligence Review},
year={2020},
month={Apr},
day={01},
volume={53},
number={4},
pages={3007-3057},
issn={1573-7462},
doi={10.1007/s10462-019-09754-z},
url={https://doi.org/10.1007/s10462-019-09754-z}
}

@article{ZHANG2022117239,
title = {Transformer-based attention network for stock movement prediction},
journal = {Expert Systems with Applications},
volume = {202},
pages = {117239},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117239},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422006170},
author = {Qiuyue Zhang and Chao Qin and Yunfeng Zhang and Fangxun Bao and Caiming Zhang and Peide Liu},
keywords = {Stock movement prediction, Deep learning, Transformer, Attention},
}

@Article{Li2022,
author={Li, Yawei
and Lv, Shuqi
and Liu, Xinghua
and Zhang, Qiuyue},
title={Incorporating Transformers and Attention Networks for Stock Movement Prediction},
journal={Complexity},
year={2022},
month={Feb},
day={27},
publisher={Hindawi},
volume={2022},
pages={7739087},
issn={1076-2787},
doi={10.1155/2022/7739087},
url={https://doi.org/10.1155/2022/7739087}
}

@misc{https://doi.org/10.48550/arxiv.2005.02527,
  doi = {10.48550/ARXIV.2005.02527},
  
  url = {https://arxiv.org/abs/2005.02527},
  
  author = {Guo, Tian and Jamet, Nicolas and Betrix, Valentin and Piquet, Louis-Alexandre and Hauptmann, Emmanuel},
  
  keywords = {Computational Finance (q-fin.CP), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Economics and business, FOS: Economics and business, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ESG2Risk: A Deep Learning Framework from ESG News to Stock Volatility Prediction},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{10.1145/3159652.3159690,
author = {Hu, Ziniu and Liu, Weiqing and Bian, Jiang and Liu, Xuanzhe and Liu, Tie-Yan},
title = {Listening to Chaotic Whispers: A Deep Learning Framework for News-Oriented Stock Trend Prediction},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159690},
doi = {10.1145/3159652.3159690},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {261–269},
numpages = {9},
keywords = {deep learning, text mining, stock trend prediction},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@misc{https://doi.org/10.48550/arxiv.2208.07248,
  doi = {10.48550/ARXIV.2208.07248},
  
  url = {https://arxiv.org/abs/2208.07248},
  
  author = {Budennyy, Semen and Kazakov, Alexey and Kovtun, Elizaveta and Zhukov, Leonid},
  
  keywords = {Statistical Finance (q-fin.ST), Machine Learning (cs.LG), Trading and Market Microstructure (q-fin.TR), FOS: Economics and business, FOS: Economics and business, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {New drugs and stock market: how to predict pharma market reaction to clinical trial announcements},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@Article{Khan2022,
author={Khan, Wasiat
and Ghazanfar, Mustansar Ali
and Azam, Muhammad Awais
and Karami, Amin
and Alyoubi, Khaled H.
and Alfakeeh, Ahmed S.},
title={Stock market prediction using machine learning classifiers and social media, news},
journal={Journal of Ambient Intelligence and Humanized Computing},
year={2022},
month={Jul},
day={01},
volume={13},
number={7},
pages={3433-3456},
abstract={Accurate stock market prediction is of great interest to investors; however, stock markets are driven by volatile factors such as microblogs and news that make it hard to predict stock market index based on merely the historical data. The enormous stock market volatility emphasizes the need to effectively assess the role of external factors in stock prediction. Stock markets can be predicted using machine learning algorithms on information contained in social media and financial news, as this data can change investors' behavior. In this paper, we use algorithms on social media and financial news data to discover the impact of this data on stock market prediction accuracy for ten subsequent days. For improving performance and quality of predictions, feature selection and spam tweets reduction are performed on the data sets. Moreover, we perform experiments to find such stock markets that are difficult to predict and those that are more influenced by social media and financial news. We compare results of different algorithms to find a consistent classifier. Finally, for achieving maximum prediction accuracy, deep learning is used and some classifiers are ensembled. Our experimental results show that highest prediction accuracies of 80.53{\%} and 75.16{\%} are achieved using social media and financial news, respectively. We also show that New York and Red Hat stock markets are hard to predict, New York and IBM stocks are more influenced by social media, while London and Microsoft stocks by financial news. Random forest classifier is found to be consistent and highest accuracy of 83.22{\%} is achieved by its ensemble.},
issn={1868-5145},
doi={10.1007/s12652-020-01839-w},
url={https://doi.org/10.1007/s12652-020-01839-w}
}

@article{AUDRINO2020334,
title = {The impact of sentiment and attention measures on stock market volatility},
journal = {International Journal of Forecasting},
volume = {36},
number = {2},
pages = {334-357},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301645},
author = {Francesco Audrino and Fabio Sigrist and Daniele Ballinari},
keywords = {Investor sentiment, Investor attention, Volatility prediction, Realized volatility, High-dimensional regression},
abstract = {We analyze the impact of sentiment and attention variables on the stock market volatility by using a novel and extensive dataset that combines social media, news articles, information consumption, and search engine data. We apply a state-of-the-art sentiment classification technique in order to investigate the question of whether sentiment and attention measures contain additional predictive power for realized volatility when controlling for a wide range of economic and financial predictors. Using a penalized regression framework, we identify the most relevant variables to be investors’ attention, as measured by the number of Google searches on financial keywords (e.g. “financial market” and “stock market”), and the daily volume of company-specific short messages posted on StockTwits. In addition, our study shows that attention and sentiment variables are able to improve volatility forecasts significantly, although the magnitudes of the improvements are relatively small from an economic point of view.}
}

@Article{math10122001,
AUTHOR = {Kamal, Saurabh and Sharma, Sahil and Kumar, Vijay and Alshazly, Hammam and Hussein, Hany S. and Martinetz, Thomas},
TITLE = {Trading Stocks Based on Financial News Using Attention Mechanism},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {12},
ARTICLE-NUMBER = {2001},
URL = {https://www.mdpi.com/2227-7390/10/12/2001},
ISSN = {2227-7390},
DOI = {10.3390/math10122001}
}

@article{AYALA2021107119,
title = {Technical analysis strategy optimization using a machine learning approach in stock market indices},
journal = {Knowledge-Based Systems},
volume = {225},
pages = {107119},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107119},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121003828},
author = {Jordan Ayala and Miguel García-Torres and José Luis Vázquez Noguera and Francisco Gómez-Vela and Federico Divina},
keywords = {Stock market prediction, Machine learning, Technical analysis},
}

@article{PENG2021100060,
title = {Feature selection and deep neural networks for stock price direction forecasting using technical analysis indicators},
journal = {Machine Learning with Applications},
volume = {5},
pages = {100060},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100060},
url = {https://www.sciencedirect.com/science/article/pii/S266682702100030X},
author = {Yaohao Peng and Pedro Henrique Melo Albuquerque and Herbert Kimura and Cayan Atreio Portela Bárcena Saavedra},
keywords = {Deep learning, Technical analysis indicators, Time-series forecasting, Market efficiency, Trading profitability},
}

@inproceedings{nguyen2015topic,
  title={Topic modeling based sentiment analysis on social media for stock market prediction},
  author={Nguyen, Thien Hai and Shirai, Kiyoaki},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1354--1364},
  year={2015}
}

@InProceedings{10.1007/978-3-030-34223-4_5,
author="Tang, Ning
and Shen, Yanyan
and Yao, Junjie",
editor="Cheng, Reynold
and Mamoulis, Nikos
and Sun, Yizhou
and Huang, Xin",
title="Learning to Fuse Multiple Semantic Aspects from Rich Texts for Stock Price Prediction",
booktitle="Web Information Systems Engineering -- WISE 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="65--81",
isbn="978-3-030-34223-4"
}

@inproceedings{xu2018stock,
  title={Stock movement prediction from tweets and historical prices},
  author={Xu, Yumo and Cohen, Shay B},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1970--1979},
  year={2018}
}

@inproceedings{liu2019transformer,
  title={Transformer-based capsule network for stock movement prediction},
  author={Liu, Jintao and Lin, Hongfei and Liu, Xikai and Xu, Bo and Ren, Yuqi and Diao, Yufeng and Yang, Liang},
  booktitle={Proceedings of the First Workshop on Financial Technology and Natural Language Processing},
  pages={66--73},
  year={2019}
}

@inproceedings{hutto2014vader,
  title={Vader: A parsimonious rule-based model for sentiment analysis of social media text},
  author={Hutto, Clayton and Gilbert, Eric},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={8},
  pages={216--225},
  year={2014}
}

@inproceedings{10.1145/3442381.3450032,
author = {Xu, Wentao and Liu, Weiqing and Xu, Chang and Bian, Jiang and Yin, Jian and Liu, Tie-Yan},
title = {REST: Relational Event-Driven Stock Trend Forecasting},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450032},
doi = {10.1145/3442381.3450032},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1–10},
numpages = {10},
keywords = {Graph-based Learning, Stock Trend Forecasting, Computational Finance, Event-driven},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{9378170,
author = {Dogan, Mustafa and Metin, Omer and Tek, Elif and Yumusak, Semih and Oztoprak, Kasim},
year = {2020},
month = {12},
pages = {4559-4566},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title = {Speculator and Influencer Evaluation in Stock Market by Using Social Media},
doi = {10.1109/BigData50022.2020.9378170}
}

@article{DBLP:journals/corr/abs-1908-10063,
  author    = {Dogu Araci},
  title     = {FinBERT: Financial Sentiment Analysis with Pre-trained Language Models},
  journal   = {CoRR},
  volume    = {abs/1908.10063},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.10063},
  eprinttype = {arXiv},
  eprint    = {1908.10063},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10063.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Roberta-fine-tuned,
  key = {Sentiment Inferencing model for stock related commments},
  title = {Sentiment Inferencing model for stock related commments},
  howpublished = {\url{https://huggingface.co/zhayunduo/roberta-base-stocktwits-finetuned}},
  note = {Accessed: 2022-12-10}
}

@misc{all-MiniLM-L6-v2,
  key = {Sentence transformer model all-MiniLM-L6-v2},
  title = {Sentence transformer model all-MiniLM-L6-v2},
  howpublished = {\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}},
  note = {Accessed: 2022-12-10}
}

@misc{https://doi.org/10.48550/arxiv.2203.05794,
  doi = {10.48550/ARXIV.2203.05794},
  
  url = {https://arxiv.org/abs/2203.05794},
  
  author = {Grootendorst, Maarten},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{https://doi.org/10.48550/arxiv.2106.09305,
  doi = {10.48550/ARXIV.2106.09305},
  
  url = {https://arxiv.org/abs/2106.09305},
  
  author = {Liu, Minhao and Zeng, Ailing and Chen, Muxi and Xu, Zhijian and Lai, Qiuxia and Ma, Lingna and Xu, Qiang},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2205.13504,
  doi = {10.48550/ARXIV.2205.13504},
  
  url = {https://arxiv.org/abs/2205.13504},
  
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Are Transformers Effective for Time Series Forecasting?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.1912.09363,
  doi = {10.48550/ARXIV.1912.09363},
  
  url = {https://arxiv.org/abs/1912.09363},
  
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2002.05202,
  doi = {10.48550/ARXIV.2002.05202},
  
  url = {https://arxiv.org/abs/2002.05202},
  
  author = {Shazeer, Noam},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GLU Variants Improve Transformer},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SALINAS20201181,
title = {DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
journal = {International Journal of Forecasting},
volume = {36},
number = {3},
pages = {1181-1191},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301888},
author = {David Salinas and Valentin Flunkert and Jan Gasthaus and Tim Januschowski},
keywords = {Probabilistic forecasting, Neural networks, Deep learning, Big data, Demand forecasting}
}

@misc{https://doi.org/10.48550/arxiv.1711.11053,
  doi = {10.48550/ARXIV.1711.11053},
  
  url = {https://arxiv.org/abs/1711.11053},
  
  author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  
  keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Multi-Horizon Quantile Recurrent Forecaster},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{10.1145/2505515.2505665,
author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
title = {Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/2505515.2505665},
doi = {10.1145/2505515.2505665},
booktitle = {Proceedings of the 22nd ACM International Conference on Information \& Knowledge Management},
pages = {2333–2338},
numpages = {6},
keywords = {web search, semantic model, deep learning, clickthrough data},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@article{JMLR:v23:21-1177,
  author  = {Julien Herzen and Francesco Lassig and Samuele Giuliano Piazzetta and Thomas Neuer and Lao Tafti and Guillaume Raille and Tomas Van Pottelbergh and Marek Pasieka and Andrzej Skrodzki and Nicolas Huguenin and Maxime Dumonal and Jan Koacisz and Dennis Bader and Frederick Gusset and Mounir Benheddi and Camila Williamson and Michal Kosinski and Matej Petrik and Gaal Grosch},
  title   = {Darts: User-Friendly Modern Machine Learning for Time Series},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {124},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v23/21-1177.html}
}

@misc{key,
  title = {https://github.com/iandroid1812/HSE_Masters},
  howpublished = {svn checkout -r187},
}

@misc{Source-code,
  key = {Project source code},
  year = {2023},
  title = {Stock price prediction using sentiment and sentence embeddings [Source code]},
  author = {Andrei Zaichenko},
  howpublished = {\url{https://github.com/iandroid1812/HSE_Masters}}
}

@misc{all-mpnet-base-v2,
  key = {Sentence transformer model all-mpnet-base-v2},
  title = {Sentence transformer model all-mpnet-base-v2},
  howpublished = {\url{https://huggingface.co/microsoft/mpnet-base}},
  note = {Accessed: 2023-01-15}
}

@misc{yfinance,
  key = {yfinance},
  title = {Python library yfinance},
  howpublished = {\url{https://pypi.org/project/yfinance/}},
  note = {Accessed: 2022-10-01}
}

@misc{https://doi.org/10.48550/arxiv.2005.14165,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{akbik-etal-2018-contextual,
    title = "Contextual String Embeddings for Sequence Labeling",
    author = "Akbik, Alan  and
      Blythe, Duncan  and
      Vollgraf, Roland",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1139",
    pages = "1638--1649",
    abstract = "Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",
}

@article{de_Myttenaere_2016,
	doi = {10.1016/j.neucom.2015.12.114},
  
	url = {https://doi.org/10.1016\%2Fj.neucom.2015.12.114},
  
	year = 2016,
	month = {jun},
  
	publisher = {Elsevier {BV}
},
  
	volume = {192},
  
	pages = {38--48},
  
	author = {Arnaud de Myttenaere and Boris Golden and B{\'{e}}n{\'{e}}dicte Le Grand and Fabrice Rossi},
  
	title = {Mean Absolute Percentage Error for regression models},
  
	journal = {Neurocomputing}
}

@article{article,
author = {Chai, Tianfeng and Draxler, R.R.},
year = {2014},
month = {06},
pages = {1247-1250},
title = {Root mean square error (RMSE) or mean absolute error (MAE)?– Arguments against avoiding RMSE in the literature},
volume = {7},
journal = {Geoscientific Model Development},
doi = {10.5194/gmd-7-1247-2014}
}

@article{10.7717/peerj-cs.623,
 title = {The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation},
 author = {Chicco, Davide and Warrens, Matthijs J. and Jurman, Giuseppe},
 year = 2021,
 month = jul,
 keywords = {Regression, Regression evaluation, Regression evaluation rates, Coefficient of determination, Mean square error, Mean absolute error, Regression analysis},
 abstract = {
Regression analysis makes up a large part of supervised machine learning, and consists of the prediction of a continuous independent target from a set of other predictor variables. The difference between binary classification and regression is in the target range: in binary classification, the target can have only two values (usually encoded as 0 and 1), while in regression the target can have multiple values. Even if regression analysis has been employed in a huge number of machine learning studies, no consensus has been reached on a single, unified, standard metric to assess the results of the regression itself. Many studies employ the mean square error (MSE) and its rooted variant (RMSE), or the mean absolute error (MAE) and its percentage variant (MAPE). Although useful, these rates share a common drawback: since their values can range between zero and +infinity, a single value of them does not say much about the performance of the regression with respect to the distribution of the ground truth elements. In this study, we focus on two rates that actually generate a high score only if the majority of the elements of a ground truth group has been correctly predicted: the coefficient of determination (also known as \textit{R}-squared or \textit{R}\textsuperscript{2}) and the symmetric mean absolute percentage error (SMAPE). After showing their mathematical properties, we report a comparison between \textit{R}\textsuperscript{2} and SMAPE in several use cases and in two real medical scenarios. Our results demonstrate that the coefficient of determination (\textit{R}-squared) is more informative and truthful than SMAPE, and does not have the interpretability limitations of MSE, RMSE, MAE and MAPE. We therefore suggest the usage of \textit{R}-squared as standard metric to evaluate regression analyses in any scientific domain.
},
 volume = 7,
 pages = {e623},
 journal = {PeerJ Computer Science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.623},
 doi = {10.7717/peerj-cs.623}
}

@misc{eco2ai,
  key = {eco2ai},
  title = {Eco2AI - a python library for CO2 emission tracking},
  howpublished = {\url{https://github.com/sb-ai-lab/Eco2AI}},
  note = {Accessed: 2022-10-01}
}

@Article{Chandola2022,
author={Chandola, Deeksha
and Mehta, Akshit
and Singh, Shikha
and Tikkiwal, Vinay Anand
and Agrawal, Himanshu},
title={Forecasting Directional Movement of Stock Prices using Deep Learning},
journal={Annals of Data Science},
year={2022},
month={Aug},
day={01},
abstract={Stock market's volatile and complex nature makes it difficult to predict the market situation. Deep Learning is capable of simulating and analyzing complex patterns in unstructured data. Deep learning models have applications in image recognition, speech recognition, natural language processing (NLP), and many more. Its application in stock market prediction is gaining attention because of its capacity to handle large datasets and data mapping with accurate prediction. However, most methods ignore the impact of mass media on the company's stock and investors' behaviours. This work proposes a hybrid deep learning model combining Word2Vec and long short-term memory (LSTM) algorithms. The main objective is to design an intelligent tool to forecast the directional movement of stock market prices based on financial time series and news headlines as inputs. The binary predicted output obtained using the proposed model would aid investors in making better decisions. The effectiveness of the proposed model is assessed in terms of accuracy of the prediction of directional movement of stock prices of five companies from different sectors of operation.},
issn={2198-5812},
doi={10.1007/s40745-022-00432-6},
url={https://doi.org/10.1007/s40745-022-00432-6}
}

@Article{Chen2022,
author={Chen, Jia
and Chen, Tao
and Shen, Mengqi
and Shi, Yunhai
and Wang, Dongjing
and Zhang, Xin},
title={Gated three-tower transformer for text-driven stock market prediction},
journal={Multimedia Tools and Applications},
year={2022},
month={Sep},
day={01},
volume={81},
number={21},
pages={30093-30119},
abstract={Effective stock market prediction can significantly assist individual and institutional investors to make better trading decisions and help government stabilize the market. Therefore, a variety of methods have been proposed to tackle the issue of stock market prediction recently. However, it is still quite challenging to effectively extract the correlations and temporal information from multivariate time series of market data and integrate various kinds of features as well as auxiliary information, which is important for improving the performance of stock market prediction. This paper proposes an entirely Transformer based model, namely Gated Three-Tower Transformer (GT3), to incorporate numerical market information and social text information for accurate stock market prediction. Firstly, we devise a Channel-Wise Tower Encoder (CWTE) to capture the channel-wise features from transposed numerical data embeddings. Secondly, we design a Shifted Window Tower Encoder (SWTE) with Multi-Temporal Aggregation to extract and aggregate the multi-scale temporal features from the original numerical data embeddings. Then we adopt the encoder of vanilla Transformer as a Text Tower Encoder (TTE) to obtain the high-level textual features. Furthermore, we design a Cross-Tower Attention mechanism to assist the model to learn the trend-relevant significance of each daily text representation by leveraging the temporal features from SWTE. Finally, we unify CWTE, SWTE, and TTE as the GT3 model through a self-adaptive gate layer to perform end-to-end text-driven stock market prediction by fusing three types of features effectively and efficiently. Extensive experimental results on a real-world dataset show that the proposed model outperforms state-of-the-art baselines.},
issn={1573-7721},
doi={10.1007/s11042-022-11908-1},
url={https://doi.org/10.1007/s11042-022-11908-1}
}

@article{Aseeri_2023, title={Effective short-term forecasts of Saudi stock price trends using technical indicators and large-scale multivariate time series}, volume={9}, url={http://dx.doi.org/10.7717/peerj-cs.1205}, DOI={10.7717/peerj-cs.1205}, journal={PeerJ Computer Science}, publisher={PeerJ}, author={Aseeri, Ahmad O.}, year={2023}, month={Jan}, pages={e1205} }

@Article{app13010222,
AUTHOR = {Li, Chengyu and Qian, Guoqi},
TITLE = {Stock Price Prediction Using a Frequency Decomposition Based GRU Transformer Neural Network},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {1},
ARTICLE-NUMBER = {222},
URL = {https://www.mdpi.com/2076-3417/13/1/222},
ISSN = {2076-3417},
ABSTRACT = {Stock price prediction is crucial but also challenging in any trading system in stock markets. Currently, family of recurrent neural networks (RNNs) have been widely used for stock prediction with many successes. However, difficulties still remain to make RNNs more successful in a cluttered stock market. Specifically, RNNs lack power to retrieve discerning features from a clutter of signals in stock information flow. Making it worse, by RNN a single long time cell from the market is often fused into a single feature, losing all the information about time which is essential for temporal stock prediction. To tackle these two issues, we develop in this paper a novel hybrid neural network for price prediction, which is named frequency decomposition induced gate recurrent unit (GRU) transformer, abbreviated to FDGRU-transformer or FDG-trans). Inspired by the success of frequency decomposition, in FDG-transformer we apply empirical model decomposition to decompose the complete ensemble of cluttered data into a trend component plus several informative and independent mode components. Equipped with the decomposition, FDG-transformer has the capacity to extract the discriminative insights from the cluttered signals. To retain the temporal information in the observed cluttered data, FDG-transformer utilizes hybrid neural network of GRU, long short term memory (LSTM) and multi-head attention (MHA) transformers. The integrated transformer network is capable of encoding the impact of different weights from each past time step to the current one, resulting in the establishment of a time series model from a deeper fine-grained level. We appy the developed FDG-transformer model to analyze Limit Order Book data and compare the results with that obtained from other state-of-the-art methods. The comparison shows that our model delivers effective price forecasting. Moreover, an ablation study is conducted to validate the importance and necessity of each component in the proposed model.},
DOI = {10.3390/app13010222}
}

@article{GUPTA2022117986,
title = {StockNet—GRU based stock index prediction},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117986},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117986},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012131},
author = {Umang Gupta and Vandana Bhattacharjee and Partha Sarathi Bishnu},
keywords = {Gated recurrent unit, Overfitting, Stock market index},
abstract = {Predicting financial trends of stock indexes is important for investors to reduce risk on investment and efficient decision making if the prediction is made accurately. Researchers, in recent times have applied deep learning approaches in this field which have essentially beaten conventional machine learning approaches. To overcome the issue of overfitting we presented a new data augmentation approach in our GRU based StockNet model consisting of two modules. Injection module to prohibit overfitting and Investigation module for stock index forecasting. The proposed approach has been validated on Indian stock market (CNX-Nifty). Proposed StockNet-c model produces 65.59\%, 27.30\% and 14.89 \% less test loss in terms of RMSE, MAE and MAPE respectively, in comparison to TargetNet model where overfitting prohibition injection module is missing.}
}

@article{WANG2022118128,
title = {Stock market index prediction using deep Transformer model},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118128},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118128},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013100},
author = {Chaojie Wang and Yuanyuan Chen and Shuqi Zhang and Qiuhui Zhang},
keywords = {Deep learning, Transformer, Stock index prediction},
abstract = {Applications of deep learning in financial market prediction have attracted widespread attention from investors and scholars. From convolutional neural networks to recurrent neural networks, deep learning methods exhibit superior ability to capture the non-linear characteristics of stock markets and, accordingly, achieve a high performance on stock market index prediction. In this paper, we utilize the latest deep learning framework, Transformer, to predict the stock market index. Transformer was initially developed for the natural language processing problem, and has recently been applied to time series forecasting. Through the encoder–decoder architecture and the multi-head attention mechanism, Transformer can better characterize the underlying rules of stock market dynamics. We implement several back-testing experiments on the main stock market indices worldwide, including CSI 300, S&P 500, Hang Seng Index, and Nikkei 225. All the experiments demonstrate that Transformer outperforms other classic methods significantly and can gain excess earnings for investors.}
}

@misc{numhtml,
  doi = {10.48550/ARXIV.2201.01770},
  
  url = {https://arxiv.org/abs/2201.01770},
  
  author = {Yang, Linyi and Li, Jiazheng and Dong, Ruihai and Zhang, Yue and Smyth, Barry},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Econometrics (econ.EM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Economics and business, FOS: Economics and business},
  
  title = {NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@INPROCEEDINGS{10027785,
  author={Lin, Fan and Li, Pengfei and Lin, Yuanguo and Chen, Zhennan and You, Huanyu and Feng, Shibo},
  booktitle={2022 IEEE International Conference on Data Mining (ICDM)}, 
  title={Kernel-based Hybrid Interpretable Transformer for High-frequency Stock Movement Prediction}, 
  year={2022},
  volume={},
  number={},
  pages={241-250},
  doi={10.1109/ICDM54844.2022.00034}}

@inproceedings{budennyy2023eco2ai,
  title={Eco2ai: carbon emissions tracking of machine learning models as the first step towards sustainable ai},
  author={Budennyy, SA and Lazarev, VD and Zakharenko, NN and Korovin, AN and Plosskaya, OA and Dimitrov, DV and Akhripkin, VS and Pavlov, IV and Oseledets, IV and Barsola, IS and others},
  booktitle={Doklady Mathematics},
  pages={1--11},
  year={2023},
  organization={Springer}
}

@software{reback2020pandas,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@Article{Lin2022,
author={Lin, Chin-Teng
and Wang, Yu-Ka
and Huang, Pei-Lun
and Shi, Ye
and Chang, Yu-Cheng},
title={Spatial-temporal attention-based convolutional network with text and numerical information for stock price prediction},
journal={Neural Computing and Applications},
year={2022},
month={Sep},
day={01},
volume={34},
number={17},
pages={14387-14395},
abstract={In the financial market, the stock price prediction is a challenging task which is influenced by many factors. These factors include economic change, politics and global events that are usually recorded in text format, such as the daily news. Therefore, we assume that real-world text information can be used to forecast stock market activity. However, only a few works considered both text and numerical information to predict or analyse stock trends. These works used preprocessed text features as the model inputs; therefore, latent information in text may be lost because the relationships between the text and stock price are not considered. In this paper, we propose a fusion network, i.e. a spatial-temporal attention-based convolutional network (STACN) that can leverage the advantages of an attention mechanism, a convolutional neural network and long short-term memory to extract text and numerical information for stock price prediction. Benefiting from the utilisation of an attention mechanism, reliable text features that are highly relevant to stock value can be extracted, which improves the overall model performance. The experimental results on real-world stock data demonstrate that our STACN model and training scheme can handle both text and numerical data and achieve high accuracy on stock regression tasks. The STACN is compared with CNNs and LSTMs with different settings, e.g. a CNN with only stock data, a CNN with only news titles and LSTMs with only stock data. CNNs considering only stock data and news titles have mean squared errors of 28.3935 and 0.1814, respectively. The accuracy of LSTMs is 0.0763. The STACN can achieve an accuracy of 0.0304, outperforming CNNs and LSTMs in stock regression tasks.},
issn={1433-3058},
doi={10.1007/s00521-022-07234-0},
url={https://doi.org/10.1007/s00521-022-07234-0}
}

@misc{https://doi.org/10.48550/arxiv.1301.3781,
  doi = {10.48550/ARXIV.1301.3781},
  
  url = {https://arxiv.org/abs/1301.3781},
  
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Estimation of Word Representations in Vector Space},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@misc{https://doi.org/10.48550/arxiv.2202.08904,
  doi = {10.48550/ARXIV.2202.08904},
  
  url = {https://arxiv.org/abs/2202.08904},
  
  author = {Muennighoff, Niklas},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SGPT: GPT Sentence Embeddings for Semantic Search},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1810.04805,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}