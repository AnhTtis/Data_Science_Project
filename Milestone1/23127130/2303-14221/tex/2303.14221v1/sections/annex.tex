\section*{Appendix}

\subsection*{Hyperparameters}

Listed hyperparameters were used in gridsearch procedure. Finally chosen parameters are highlighted in bold.

Hyperparameters used for TFT training:
\begin{itemize}
    \item lookback window = 5, \textbf{15}, 32
    \item hidden\_size = 15, 32, \textbf{64}, 80 - the main hyper parameter among TFT architecture, which describes the number of neurons of each dense layer during variable selection process, static enrichment section and position-wise feed forward;
    \item lstm\_layers = \textbf{1}, 2 - number of layers for the LSTM encoder and decoder;
    \item num\_attention\_heads = 2, \textbf{4} -  number of attention heads;
    \item feed\_forward = ReLU, \textbf{SwiGLU} - according to the paper \cite{https://doi.org/10.48550/arxiv.2002.05202}, GLU activations improve transformer-based architecture performance that proved to be the case in our problem as well;
    \item dropout = 0.10, 0.15, \textbf{0.25}, 0.5 - fraction of neurons that are affected by dropout;
    \item hidden\_continuous\_size = 15, \textbf{32}, 64 - hidden size for processing continuous variables;
    \item norm\_type = LinearNorm, \textbf{RMSNorm} - a simplification of the original layer normalization;
    \item optimizer\_cls = \textbf{Adam}, AdamW, Adagrad - standard optimization algorithm;
    \item batch\_size = 8, 16, \textbf{32}, 64.
\end{itemize}

Hyperparameters used for Nlinear training:
\begin{itemize}
    \item lookback window = 5, \textbf{15}, 32
    \item const\_init = False, \textbf{True} - initialize the weights to $1 / input\_length$ instead of default PyTorch initialization;
    \item optimizer\_cls = \textbf{Adam}, AdamW, Adagrad - standard optimization algorithm;
    \item batch\_size = 8, 16, \textbf{32}, 64.
\end{itemize}

\subsection*{Data preprocessing}

We have a total of 3717964 tweets in the dataset from 140131 writers. This means that a significant amount of tweets were written by the same accounts through the period from the beginning of 2015 to the end of 2019, and that is why during the data preparation step, we had to deal with duplicate tweets. Less than 1\% of NaN values were observed only in the writer column (deleted accounts) and were dropped.

First, we removed the tweets with more than one stock mentioned in the body of a tweet to avoid ambiguous cases, which resulted in the removal of 421101 tweets â€“ 11.3\% of the total amount. Then we drop intra-day duplicate tweets generated by the bot accounts by complete matches in the body, getting rid of another 4.9\% of remaining tweets. After that, a text cleaning function was applied, removing web links, mentions, tickers, unnecessary punctuation, revealing another 11.5\% of duplicate tweets, getting to about 2.7 million total tweets.

\subsection{Selection of prediction model and sensitivity analysis}

We considered several architectures when choosing a close price prediction model. As the baseline solution, we took the Naive Seasonal model, which returned the close price for $N$ steps ahead. Also, we experimented with N-Linear predictive model. 

The results for each company, feature set, and model are demonstrated in Table \ref{table:total-error-table}. Three-day and five-day forecast horizons were tested. The three-day window performs better, so the corresponding results are included in the main part of this paper. The five-day model output gives worse metrics but is worth analysis. Metrics in Table \ref{table:total-error-table} are calculated for five days ahead prediction. There is a tendency for TFT to be more capable of extracting the information from the embedding vector compared to N-Linear.

For Apple, the best performing model is TFT with embedding vector as a feature. The usage of sentiment score does not help to improve the model's accuracy for both TFT and N-Linear. Only two model instances outperform the baseline Naive Seasonal approach - N-Linear with sentiment score and embedding vector.

For Amazon, we observe different behaviour of the model's performance given the input features. Both N-Linear and TFT receive performance boost with the utilization of sentiment score. Embedding vector yields higher error values. Even the baseline solution outperforms the embedding approach by a significant margin. Thus, no useful data are extracted from this feature in this case.

Tesla is the most volatile stock amount the five considered. It is evident that all models struggle with stock price prediction for Tesla, bringing the highest error values among all stocks. Only TFT in combination with the sentiment score performs better than the naive baseline solution. All of the other models and features show worse accuracy.

\begin{table}[!ht]
    \centering
    \caption{Metrics for five days ahead prediction experiments. Different input feature sets are considered. For each company, the composite ranking takes into account multiple measures of model performance.}
    \begin{tabular}{llcccccc}
    \toprule
          Company & Model &  Rank & MAPE & MAE &      $R^{2}$ &    RMSE &   SMAPE \\
    \midrule
          Apple & baseline  &     5 &   2.5654 &  1.3132 &  0.9344 &  1.6736 &  2.5867 \\
          & nlinear\_HLOV &     4 &   2.5506 &  1.3033 &  0.9280 &  1.7074 &  2.5489 \\
          & nlinear\_HLOVS &     6 &  2.6844 &  1.3622 &  0.9253 &  1.7383 &  2.6814 \\
          & nlinear\_HLOVE &     7 &   4.4765 &  2.2690 &  0.8050 &  2.8094 &  4.4628 \\
          & tft\_HLOV &     2 &   2.4696 &  1.2583 &  0.9383 &  1.5796 &  2.4805 \\
          & tft\_HLOVS &     3   &   2.4981 &  1.2772 &  0.9292 &  1.6928 &  2.4892 \\
          & \textbf{tft\_HLOVE} &     1 &   \textbf{2.1485} &  \textbf{1.0922} &  \textbf{0.9502} &  \textbf{1.4203} &  \textbf{2.1468} \\ \hline
          \\
          Amazon & baseline  &     3 &   1.4832 &  0.7907 &  0.7235 &  1.0273 &  1.4837 \\
          & nlinear\_HLOV &     6 &   1.6984 &  0.9044 &  0.6192 &  1.2135 &  1.7003 \\
          & nlinear\_HLOVS &     5 &  1.6619 &  0.8865 &  0.6403 &  1.1795 &  1.6650 \\
          & nlinear\_HLOVE &     7 &   3.7163 &  1.9763 & -0.6065 &  2.4926 &  3.7144 \\
          & tft\_HLOV &     2 &   1.3785 &  0.7356 &  \textbf{0.7622} &  \textbf{0.9589} &  1.3780 \\
          & \textbf{tft\_HLOVS} &     1 &  \textbf{1.3161} &  \textbf{0.7005} &  \textbf{0.7622} &  0.9590 &  \textbf{1.3129} \\
          & tft\_HLOVE &     4 &   1.6166 &  0.8594 &  0.6640 &  1.1399 &  1.6013 \\  \hline
          \\
           Google & baseline  &     4 &   2.2939 &  1.2319 &  0.7119 &  1.6087 &  2.2950 \\
          & nlinear\_HLOV &     6 &   2.8414 &  1.5251 &  0.4790 &  2.1344 &  2.8412 \\
          & nlinear\_HLOVS &     5 &  2.8261 &  1.5128 &  0.4703 &  2.1523 &  2.8240 \\
          & nlinear\_HLOVE &     7 &   5.3571 &  2.9040 & -0.5161 &  3.6410 &  5.3650 \\
          & tft\_HLOV &     3 &   2.0767 &  1.1163 &  0.7479 &  1.4848 &  2.0781 \\
          & \textbf{tft\_HLOVS} &     1 &  \textbf{2.0209} &  \textbf{1.0844} &  \textbf{0.7565} &  \textbf{1.4592} &  \textbf{2.0192} \\
          & tft\_HLOVE &     2 &   2.0657 &  1.1022 &  0.7088 &  1.5956 &  2.0486 \\ \hline
          \\
          Microsoft & baseline  &     4 &   1.8159 &  1.1821 &  0.9303 &  1.4773 &  1.8287 \\
          & nlinear\_HLOV &     5 &   2.0021 &  1.3021 &  0.9119 &  1.6424 &  2.0035 \\
          & nlinear\_HLOVS &     6 &  2.0701 &  1.3441 &  0.9072 &  1.6850 &  2.0715 \\
          & nlinear\_HLOVE &     7 &   3.7334 &  2.4173 &  0.6771 &  3.1433 &  3.7348 \\
          & tft\_HLOV &     3 &   1.7392 &  1.1442 &  0.9294 &  1.4703 &  1.7546 \\
          & tft\_HLOVS &     2 &   \textbf{1.6815} &  1.1088 &  0.9311 &  1.4521 &  1.6937 \\
          & \textbf{tft\_HLOVE} &     1 &   1.6936 &  \textbf{1.0969} &  \textbf{0.9359} &  \textbf{1.4003} &  \textbf{1.6819} \\ \hline
          \\
          Tesla & baseline &     2 &   5.1027 &  2.0321 &  0.7996 &  2.7298 &  5.0911 \\
          & nlinear\_HLOV &     5 &   5.9677 &  2.4198 &  0.7107 &  3.2337 &  5.9809 \\
          & nlinear\_HLOVS &     6 &  5.9770 &  2.4363 &  0.7090 &  3.2432 &  5.9771 \\
          & nlinear\_HLOVE &     7 &   6.4840 &  2.5655 &  0.6965 &  3.3120 &  6.4805 \\
          & tft\_HLOV &     4 &   5.2499 &  2.1137 &  0.7462 &  3.0284 &  5.1821 \\
          & \textbf{tft\_HLOVS} &     1 &  \textbf{4.8706} &  \textbf{1.9418} &  \textbf{0.8143} &  \textbf{2.5904} &  \textbf{4.8720} \\
          & tft\_HLOVE &     3 &   5.1824 &  2.0705 &  0.7840 &  2.7940 &  5.1357 \\  
    \bottomrule
    \end{tabular}
    \label{table:total-error-table}
\end{table}


% One of the most important discovery is that the error scores for sentiment and semantic feature groups demonstrated similar performance for multiple instances. This is further demonstrated in the next experiments directly eliminating the need to solve the problem of sentiment extraction without a significant loss in quality for final predictions.