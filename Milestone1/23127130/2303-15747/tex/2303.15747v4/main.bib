% CVPR

@InProceedings{He_2022_CVPR,
    author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2022},
    pages = {16000-16009}
}

@InProceedings{yu2021metaformer,
    author    = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
    title     = {MetaFormer Is Actually What You Need for Vision},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10819-10829}
}
% article{yu2021metaformer,
%     title={MetaFormer Is Actually What You Need for Vision},
%     author={Weihao Yu and Mi Luo and Pan Zhou and Chenyang Si and Yichen Zhou and Xinchao Wang and Jiashi Feng and Shuicheng Yan},
%     year={2021},
%     journal={arXiv preprint arXiv:2111.11418}
% }

% -----------------------------------------------
% ACL
% arXiv -> ACL 2019
@inproceedings{1906.01787,
    title = "Learning Deep Transformer Models for Machine Translation",
    author = "Wang, Qiang  and
      Li, Bei  and
      Xiao, Tong  and
      Zhu, Jingbo  and
      Li, Changliang  and
      Wong, Derek F.  and
      Chao, Lidia S.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1176",
    doi = "10.18653/v1/P19-1176",
    pages = "1810--1822",
    abstract = "Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT{'}16 English-German and NIST OpenMT{'}12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",
}
% misc{1906.01787,
%     Author = {Qiang Wang and Bei Li and Tong Xiao and Jingbo Zhu and Changliang Li and Derek F. Wong and Lidia S. Chao},
%     Title = {Learning Deep Transformer Models for Machine Translation},
%     Year = {2019},
%     Eprint = {arXiv:1906.01787}
% }

@inproceedings{herzig-etal-2020-tapas,
    title = "{T}a{P}as: Weakly Supervised Table Parsing via Pre-training",
    author = {Herzig, Jonathan  and
      Nowak, Pawel Krzysztof  and
      M{\"u}ller, Thomas  and
      Piccinno, Francesco  and
      Eisenschlos, Julian},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.398",
    doi = "10.18653/v1/2020.acl-main.398",
    pages = "4320--4333",
    abstract = "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT{'}s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
}

% -----------------------------------------------
% Transaction of ACL
@article{badaro:hal-03877085,
  TITLE = {{Transformers for Tabular Data Representation: A Survey of Models and Applications}},
  AUTHOR = {Badaro, Gilbert and Saeed, Mohammed and Papotti, Paolo},
  URL = {https://hal.science/hal-03877085},
  JOURNAL = {{Transactions of the Association for Computational Linguistics}},
  PUBLISHER = {{The MIT Press}},
  YEAR = {2023},
  MONTH = Jan,
  PDF = {https://hal.science/hal-03877085/file/publi-7123.pdf},
  HAL_ID = {hal-03877085},
  HAL_VERSION = {v1},
}

% -----------------------------------------------
% NAACL
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and Chang, Ming-Wei and Lee, Kenton  andToutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
% article{devlin2018bert,
%     title={BERT: Pre-training of Deep Bidirectional Transformers for Language
%   Understanding},
%     author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
%     year={2018},
%     journal={arXiv preprint arXiv:1810.04805}
% }

@inproceedings{iida-etal-2021-tabbie,
    title = "{TABBIE}: Pretrained Representations of Tabular Data",
    author = "Iida, Hiroshi  and
      Thai, Dung  and
      Manjunatha, Varun  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.270",
    doi = "10.18653/v1/2021.naacl-main.270",
    pages = "3446--3456",
    abstract = "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.",
}

% -----------------------------------------------
% NeurIPS
@inproceedings{
wang2022transtab,
title={{TransTab}: Learning Transferable Tabular Transformers Across Tables},
author={Zifeng Wang and Jimeng Sun},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=A1yGs_SWiIi}
}

@inproceedings{NEURIPS2020_7d97667a,  author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and van der Schaar, Mihaela},  booktitle = {Advances in Neural Information Processing Systems},  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},  pages = {11033--11043},  publisher = {Curran Associates, Inc.},  title = {VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain},  url = {https://proceedings.neurips.cc/paper/2020/file/7d97667a3e056acab9aaf653807b4a03-Paper.pdf},  volume = {33},  year = {2020} } 

@inproceedings{ucar2021subtab,
 author = {Ucar, Talip and Hajiramezanali, Ehsan and Edwards, Lindsay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18853--18865},
 publisher = {Curran Associates, Inc.},
 title = {SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/9c8661befae6dbcd08304dbf4dcaf0db-Paper.pdf},
 volume = {34},
 year = {2021}
}
% inproceedings{
% ucar2021subtab,
% title={SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning},
% author={Talip Ucar and Ehsan Hajiramezanali and Lindsay Edwards},
% booktitle={Advances in Neural Information Processing Systems},
% editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
% year={2021},
% url={https://openreview.net/forum?id=vrhNQ7aYSdr}
% }

@inproceedings{NEURIPS2018_14491b75,
 author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {CatBoost: unbiased boosting with categorical features},
 url = {https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{NEURIPS2019_bdbca288,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{gorishniy2021revisiting,
 author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18932--18943},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting Deep Learning Models for Tabular Data},
 url = {https://proceedings.neurips.cc/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf},
 volume = {34},
 year = {2021}
}
% misc{https://doi.org/10.48550/arxiv.2106.11959,
%     doi={10.48550/ARXIV.2106.11959},
%     url={https://arxiv.org/abs/2106.11959},
%     author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
%     keywords={Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
%     title={Revisiting Deep Learning Models for Tabular Data},
%     publisher={arXiv},
%     year={2021},
%     copyright={arXiv.org perpetual, non-exclusive license}
% }
% article{gorishniy2021revisiting,
%     title={Revisiting Deep Learning Models for Tabular Data},
%     author={Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
%     year={2021},
%     journal={arXiv preprint arXiv:2106.11959}
% }

@inproceedings{kossen2021selfattention,
 author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28742--28756},
 publisher = {Curran Associates, Inc.},
 title = {Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/f1507aba9fc82ffa7cc7373c58f8a613-Paper.pdf},
 volume = {34},
 year = {2021}
}
% article{kossen2021selfattention, title={Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning}, author={Jannik Kossen and Neil Band and Clare Lyle and Aidan N. Gomez and Tom Rainforth and Yarin Gal}, year={2021}, journal={arXiv preprint arXiv:2106.02584} }



@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
% article{vaswani2017attention,
%     title={Attention Is All You Need},
%     author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
%     year={2017},
%     journal={arXiv preprint arXiv:1706.03762}
% }


% -----------------------------------------------
% ICLR
@inproceedings{
levin2023transfer,
title={Transfer Learning with Deep Tabular Models},
author={Roman Levin and Valeriia Cherepanova and Avi Schwarzschild and Arpit Bansal and C. Bayan Bruss and Tom Goldstein and Andrew Gordon Wilson and Micah Goldblum},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=b0RuGUYo8pA}
}
% inproceedings{
% anonymous2023transfer,
% title={Transfer Learning with Deep Tabular Models},
% author={Anonymous},
% booktitle={Submitted to The Eleventh International Conference on Learning Representations },
% year={2023},
% url={https://openreview.net/forum?id=b0RuGUYo8pA},
% note={under review}
% }

@inproceedings{ bahri2022scarf, title={{SCARF}: Self-Supervised Contrastive Learning using Random Feature Corruption}, author={Dara Bahri and Heinrich Jiang and Yi Tay and Donald Metzler}, booktitle={International Conference on Learning Representations}, year={2022}, url={https://openreview.net/forum?id=CuV_qYkmKb3} }

% inproceedings{
% anonymous2023have,
% title={Have Missing Data? Make It Miss More! Imputing Tabular Data with Masked Autoencoding},
% author={Anonymous},
% booktitle={Submitted to The Eleventh International Conference on Learning Representations },
% year={2023},
% url={https://openreview.net/forum?id=yzE6LtZSHo},
% note={under review}
% }

@inproceedings{
gidaris2018unsupervised,
title={Unsupervised Representation Learning by Predicting Image Rotations},
author={Spyros Gidaris and Praveer Singh and Nikos Komodakis},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1v4N2l0-},
}

@inproceedings{
sanh2021multitask,
title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=9Vrb9D0WI4}
}
% misc{sanh2021multitask,
%       title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
%       author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Stella Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},
%       year={2021},
%       eprint={2110.08207},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }
@inproceedings{
loshchilov2017sgdr,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Skq89Scxx}
}
@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

% -----------------------------------------------
% ICML
@InProceedings{pmlr-v119-chen20j,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

% -----------------------------------------------
% VLDB
@article{deng2020turl,
  title={TURL: table understanding through representation learning},
  author={Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
  journal={Proceedings of the VLDB Endowment},
  volume={14},
  number={3},
  pages={307--319},
  year={2020},
  publisher={VLDB Endowment}
}

% -----------------------------------------------
% AAAI
@article{Arik_Pfister_2021,
    title={{TabNet}: Attentive Interpretable Tabular Learning},
    volume={35},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/16826},
    DOI={10.1609/aaai.v35i8.16826},
    abstractNote={We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior. Finally, we demonstrate self-supervised learning for tabular data, significantly improving performance when unlabeled data is abundant.},
    number={8},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Arik, Sercan Ö. and Pfister, Tomas},
    year={2021},
    month={May},
    pages={6679-6687}
}
% article{arik2019tabnet, title={{TabNet}: Attentive Interpretable Tabular Learning}, author={Sercan O. Arik and Tomas Pfister}, year={2019}, journal={arXiv preprint arXiv:1908.07442} }

% -----------------------------------------------
% JMLR
@article{JMLR:v12:pedregosa11a,
  author  = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
  title   = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {85},
  pages   = {2825--2830},
  url     = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

% -----------------------------------------------
% KDD
@inproceedings{Chen:2016:XST:2939672.2939785,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}

@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-Generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {hyperparameter optimization, Bayesian optimization, black-box optimization, machine learning system},
location = {Anchorage, AK, USA},
series = {KDD '19}
}
% article{akiba2019optuna,
%     title={Optuna: A Next-generation Hyperparameter Optimization Framework},
%     author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
%     year={2019},
%     journal={arXiv preprint arXiv:1907.10902}
% }

% -----------------------------------------------
% IEEE
@ARTICLE{borisov2021deep,
  author={Borisov, Vadim and Leemann, Tobias and Seßler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Deep Neural Networks and Tabular Data: A Survey}, 
  year={2022},
  volume={},
  number={},
  pages={1-21},
  doi={10.1109/TNNLS.2022.3229161}}
% article{borisov2021deep,
%     title={Deep Neural Networks and Tabular Data: A Survey},
%     author={Vadim Borisov and Tobias Leemann and Kathrin Seßler and Johannes Haug and Martin Pawelczyk and Gjergji Kasneci},
%     year={2021},
%     journal={arXiv preprint arXiv:2110.01889}
% }



% -----------------------------------------------
% Information Fusion
@article{SHWARTZZIV202284,
title = {Tabular data: Deep learning is not all you need},
journal = {Information Fusion},
volume = {81},
pages = {84-90},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521002360},
author = {Ravid Shwartz-Ziv and Amitai Armon},
keywords = {Tabular data, Deep neural networks, Tree-based models, Hyperparameter optimization},
abstract = {A key element in solving real-life data science problems is selecting the types of models to use. Tree ensemble models (such as XGBoost) are usually recommended for classification and regression problems with tabular data. However, several deep learning models for tabular data have recently been proposed, claiming to outperform XGBoost for some use cases. This paper explores whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to XGBoost on various datasets. In addition to systematically comparing their performance, we consider the tuning and computation they require. Our study shows that XGBoost outperforms these deep models across the datasets, including the datasets used in the papers that proposed the deep models. We also demonstrate that XGBoost requires much less tuning. On the positive side, we show that an ensemble of deep models and XGBoost performs better on these datasets than XGBoost alone.}
}

% -----------------------------------------------
% ICCV
@InProceedings{Doersch_2015_ICCV,
author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
title = {Unsupervised Visual Representation Learning by Context Prediction},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

% -----------------------------------------------
% ECCV
@InProceedings{10.1007/978-3-319-46466-4_5,
author="Noroozi, Mehdi
and Favaro, Paolo",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="69--84",
abstract="We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with {\$}{\$}51.8{\backslash},{\backslash}{\%}{\$}{\$}for detection and {\$}{\$}68.6{\backslash},{\backslash}{\%}{\$}{\$}for classification, and reduce the gap with supervised learning ({\$}{\$}56.5{\backslash},{\backslash}{\%}{\$}{\$}and {\$}{\$}78.2{\backslash},{\backslash}{\%}{\$}{\$}respectively).",
isbn="978-3-319-46466-4"
}

% -----------------------------------------------
% arXiv

@article{majmundar2022met, title={MET: Masked Encoding for Tabular Data}, author={Kushal Majmundar and Sachin Goyal and Praneeth Netrapalli and Prateek Jain}, year={2022}, journal={arXiv preprint arXiv:2206.08564} }

@article{huang2020tabtransformer, title={TabTransformer: Tabular Data Modeling Using Contextual Embeddings}, author={Xin Huang and Ashish Khetan and Milan Cvitkovic and Zohar Karnin}, year={2020}, journal={arXiv preprint arXiv:2012.06678} }

@article{grinsztajn2022why,
    title={Why do tree-based models still outperform deep learning on tabular data?},
    author={Léo Grinsztajn and Edouard Oyallon and Gaël Varoquaux},
    year={2022},
    journal={arXiv preprint arXiv:2207.08815}
}

@article{bommasani2021on,
    title={On the Opportunities and Risks of Foundation Models},
    author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
    year={2021},
    journal={arXiv preprint arXiv:2108.07258}
}

@article{lin2021a,
    title={A Survey of Transformers},
    author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
    year={2021},
    journal={arXiv preprint arXiv:2106.04554}
}

@article{hegselmann2022tabllm,
    title={TabLLM: Few-shot Classification of Tabular Data with Large Language
  Models},
    author={Stefan Hegselmann and Alejandro Buendia and Hunter Lang and Monica Agrawal and Xiaoyi Jiang and David Sontag},
    year={2022},
    journal={arXiv preprint arXiv:2210.10723}
}

@inproceedings{
    dinh2022lift,
    title={{LIFT}: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks},
    author={Tuan Dinh and Yuchen Zeng and Ruisu Zhang and Ziqian Lin and Michael Gira and Shashank Rajput and Jy-yong Sohn and Dimitris Papailiopoulos and Kangwook Lee},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=s_PJMEGIUfa}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{somepalli2022saint,
  title={Saint: Improved neural networks for tabular data via row attention and contrastive pre-training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}
% misc{
% somepalli2022saint,
% title={{SAINT}: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training},
% author={Gowthami Somepalli and Avi Schwarzschild and Micah Goldblum and C. Bayan Bruss and Tom Goldstein},
% year={2022},
% url={https://openreview.net/forum?id=nL2lDlsrZU}
% }
@misc{https://doi.org/10.48550/arxiv.1706.02677,
doi = {10.48550/ARXIV.1706.02677},

url = {https://arxiv.org/abs/1706.02677},

author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},

keywords = {Computer Vision and Pattern Recognition (cs.CV), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},

publisher = {arXiv},

year = {2017},

copyright = {arXiv.org perpetual, non-exclusive license}
}
