\section{Related Work}
Deep learning (DL)-based and tree-based methods are the two main streams of supervised learning for tabular data.
Which one works better depends on the task, as they capture different aspects of the input characteristics~\citep{grinsztajn2022why,SHWARTZZIV202284}.
However, DL-based methods are primarily adapted to transfer learning because they are easier to pre-train than tree-based methods.
Among others, self-supervised learning with Transformers is the most common pre-training approach of DL-based methods (see, e.g.,~\citet{badaro:hal-03877085} for the survey).

Despite the popularity, most Transformer-based methods have not adapted to the case where pre-training and fine-tuning tasks have different column sets.
To the best of our knowledge, TransTab~\citep{wang2022transtab}, TabLLM~\citep{hegselmann2022tabllm}, and LIFT~\citep{dinh2022lift} are the few exceptions that can train on tabular datasets with different column sets.
TransTab creates column-by-column representations of each row from the column name and value, allowing pre-trained models to be applied to unseen columns.
TabLLM and LIFT convert a row into a sequence of tokens and feed them to a pre-trained language model~\citep{sanh2021multitask, gpt-j}.
They assumed there were semantic correspondences of column descriptions between different columns.
In practice, however, there are many cases where column descriptions are not informative (e.g., random alphabet) or do not exist. 
Since TabRet does not explicitly use the column description information, it can transfer to a different column set in such a situation.
