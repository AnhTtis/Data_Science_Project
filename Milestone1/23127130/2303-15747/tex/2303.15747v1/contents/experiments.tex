\section{Experiments}\label{sec:experiments}
\paragraph{Datasets.}
As pre-training data, we preprocessed behavioral risk factor surveillance system (BRFSS), a collection of public health surveys in the US, and created a single table consisting of 2.03 million rows and 74 columns. As downstream tasks, we selected four classification datasets in the healthcare domain from Kaggle: Diabetes, HDHI, PKIHD, and Stroke.
Each dataset has about 50\% overlap with BRFSS columns. However, some of these overlapping columns have different representations of their values. For example, the age column in BRFSS is categorical, but is represented as continuous in the downstream datasets, such as Stroke. We pre-processed the column representations of the downstream datasets to adjust BRFSS columns.
We used 20\% of the data as a test set, 100 data points from 80\% for fine-tuning (and retokenizing), and the remaining data as a validation set. 
The dataset specifications are described in Appendix~\ref{sec:datasets}.

\paragraph{Baselines.}

We used two groups of baselines: supervised methods trained only on the downstream tasks and self-supervised methods pre-trained on BRFSS. As supervised methods, we compared logistic regression (LR), XGBoost, CatBoost, MLP, and Feature Tokenizer Transformer~\citep{gorishniy2021revisiting} (FTTrans). Self-supervised methods were SCARF~\citep{bahri2022scarf} and TransTab~\citep{wang2022transtab}. The hyperparameters, such as learning rate, were optimized by Optuna~\citep{10.1145/3292500.3330701} with the validation set for each method.
Details of the baseline methods are described in Appendix~\ref{sec:baselines}.

\paragraph{Results.}

Table~\ref{tab:ood} shows that TabRet outperformed the baselines for all the datasets. Among the pre-trained models, TabRet consistently achieved the best. In addition to absolute performance, TabRet had relatively little variability (i.e., small variance) in the results, which is one of the desirable properties of pre-training.

We also ablated TabRet. Since the default masking ratios mentioned in Section~\ref{sec:method} were determined based on the performance with all the learning options (pre-training, shuffle augmentation, retokenizing), we also optimized them as hyperparameters here for a fair comparison. The results (Table~\ref{tab:ablation}) show a positive trend of pre-training. Although shuffle augmentation and retokenizing sometimes worked negatively, they provide additional gains on average. 

To make a fair comparison, we evaluated if the shuffle augmentation is effective for TransTab. However, the performance did not consistently improve (refer to Appendix\ref{sec:additional_exp_transtab}).

\begin{table}[tb]
 \caption{\textbf{Test AUC performance.} The methods from LR to FTTrans were trained only on fine-tuning data (i.e., no pre-training). Each cell reports the mean and standard deviation over 20 random seeds. The best scores are underlined, and those with statistical significance at a significance level of 0.05 (Welchâ€™s t-test) are in bold. 
 }
 \label{tab:ood}
 \centering
  \begin{tabular}{rcccc}
   Methods & Diabetes & HDHI & PKIHD & Stroke\\
   \midrule
   LR & $75.10\pm 3.55$ & $75.55\pm 3.43$ & $76.91\pm 2.52$ & $74.29\pm 6.08$ \\
   XGBoost & $79.52\pm 0.79$ & $80.29\pm 1.25$ & $79.74\pm 0.93$ & $69.02\pm 9.63$ \\
   CatBoost & $77.83\pm 1.40$ & $77.65\pm 1.82$ & $76.50\pm 1.79$ & $76.14\pm 3.46$ \\
   MLP & $78.20\pm 1.02$ & $79.39\pm 1.09$ & $77.51\pm 1.68$ & $76.27\pm 5.92$ \\
   FTTrans & $79.11\pm 1.07$ & $78.96\pm 1.30$ & $76.45\pm 2.38$ & $76.48\pm 4.92$ \\
   \midrule
   SCARF & $78.43\pm 1.35$ & $80.36\pm 1.26$ & $81.01\pm 0.94$ & $76.74\pm 5.04$ \\
   TransTab & $78.30\pm 1.18$ & $78.77\pm 1.34$ & $78.56\pm 1.58$ & $75.00\pm 4.80$ \\
   TabRet & \underline{$79.94\pm 1.03$} & $\mathbf{81.65\pm 1.60}$ & $\mathbf{82.70\pm 0.79}$ & $\mathbf{80.73\pm 3.83}$ \\
  \end{tabular}
\end{table}

\begin{table}[tb]
\caption{\textbf{Ablation study.} \emph{Supervised} indicates TabRet that was trained only with fine-tuning data. \emph{Supervised} row reports AUC scores and the remaining rows report the performance gains averaged over 10 random seeds.}
\label{tab:ablation}
\centering
\begin{tabular}{lllllr}
    %\toprule
    & Diabetes & HDHI & PKIHD & Stroke & Ave. Gain\\
    \midrule
    Supervised       & $78.71$ & $78.42$ & $75.47$ & $74.29$ & N/A\\
    \midrule
    %\midrule
    + Pre-training   & $+0.12$ & $+2.06$ & $+4.98$ & $+6.12$ & $+3.07$\\
    + Shuffle aug.   & $+0.90$ & $+2.86$ & $+4.99$ & $+5.16$ & $+3.21$ \\
    + Retokenizing & $+0.64$ & $+2.65$ & $+6.89$ & $+5.30$ & $+3.87$
    %\bottomrule
\end{tabular}
\end{table}