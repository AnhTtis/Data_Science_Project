\section{Details of Experiment Settings}


\subsection{Datasets}\label{sec:datasets}
\begin{table}[t]
 \caption{Dataset specifications. All datasets are binary classification tasks. Cat: the number of categorical features. Num.: the number of numerical features. Positive: the ratio of data points with the positive target. Overlap: the ratio of columns that exist in the pre-training dataset BRFSS.}
 \label{tab:dataset}
 \centering
  \begin{tabular}{lrrrrr}
   % \toprule
    Name & Data Points & Cat. & Num. & Positive & Overlap \\
   \midrule
   Diabetes & 253,680 & 20 & 1 & 0.139 & 0.524 \\
   HDHI & 253,680 & 20 & 1 & 0.094 & 0.524 \\
   PKIHD  & 319,795 & 15 & 2 & 0.086 & 0.529 \\
   Stroke & 4,909 & 2 & 8 & 0.049 & 0.5 \\
   % \bottomrule
  \end{tabular}\label{sec:dataset-specification}
\end{table}


We used 4 datasets in OOD experiment.
Table~\ref{sec:dataset-specification} summarizes the specifications of the datasets and Table~\ref{sec:dataset-source} lists their sources.

\subsubsection{Pre-training Datasets}

\begin{table*}[t]
 \caption{Splitting of BRFSS dataset. The numbers represent the sample sizes. The fine-tuning and test data were used in the experiments in Appendix~\ref{sec:additional_experiment}.}
 \label{tab:diabetesl}
 \centering
  \begin{tabular}{cccc}
   % \toprule
   \multicolumn{4}{c}{\textbf{All}} \\
   \multicolumn{4}{c}{2,038,772} \\ 
   \midrule
   \multicolumn{3}{c}{\textbf{Train}} & \textbf{Test} \\
   \multicolumn{3}{c}{1,631,018} & 497,754 \\
   \cmidrule(lr){1-3}
   \textbf{Pre-training} & \textbf{Validation} & \textbf{Fine-tuning} & \\
   1,453,235 & 163,102 & 100 &\\
   % % \bottomrule
  \end{tabular}
\end{table*}

We used telephone survey datasets obtained from Behavioral Risk Factor Surveillance System (BRFSS)\footnote{\url{https://www.cdc.gov/brfss}} as a pre-training dataset.
These datasets collected state-specific risk behaviors related to chronic diseases, injuries, and preventable infectious diseases of adults in the United States.
We combined the datasets from 2011--2015 and removed missing values by deleting rows and columns by the following steps: 1. deleted columns with more than 10\% missing values; 2. deleted rows with missing values.
We call the resulting dataset \textbf{BRFSS}.
We split the BRFSS as shown in Table~\ref{tab:diabetesl} and used the pre-train and validation datasets. 

\subsubsection{Fine-tuning Datasets}

We prepared four datasets for the OOD Transfer Learning:
\begin{itemize}
    \item \textbf{Diabetes}: The dataset made from the BRFSS dataset of 2015 to predict whether a subject had diabetes. 
    \item \textbf{HDHI} (Heart Disease Health Indicator): The dataset made from the BRFSS dataset of 2015 to predict whether the subject has heart disease.
    \item \textbf{PKIHD} (Personal Key Indicator of Heart Disease): Similarly to HDHI, the task is to predict whether the patient has heart disease, but was made from the BRFSS dataset of 2020.
    \item \textbf{Stroke}: The dataset recording clinical events. The task is to predict whether a subject is likely to get a stroke using 10 features: gender, age, hypertension, heart disease, marriage, work type, residence type, glucose, BMI, and smoking status.
\end{itemize}
These datasets were divided into 80\% training dataset and 20\% test dataset.
Of the training data, 100 samples were separated as a fine-tuning dataset.

 In some columns, BRFSS and the above four data are named differently but have the same meaning. Therefore, to create overlap, the column names of the above four data were changed to match the column names in BRFSS. In addition, we performed the same feature engineering performed in BRFSS, such as categorizing age.


\subsubsection{Data Preprocessing}

For all methods except the tree-based methods, numerical features were transformed using the Quantile Transformation from the scikit-learn library~\citep{JMLR:v12:pedregosa11a}, and categorical features were transformed using the Ordinal Encoder. For the tree-based methods, only categorical features were transformed using the Ordinal Encoder.


\subsection{Baselines}\label{sec:baselines}

\begin{table}[t]
\caption{Hyperparameters of Transformer layer in TransTab. The default value of n\_layer is 2. FFN stands for Feed-Forward Network and is often used in the Transformer layer.}
\label{tab:hypara_transformer_transtab}
 \centering
 \begin{tabular}{l|ccc}
% \toprule
n\_layer & 2 & 6 \\
\midrule
Hidden\_size & 128 & 384 \\
Num\_attention\_head & \multicolumn{2}{c}{8} \\
Hidden\_dropout\_prob & \multicolumn{2}{c}{0.0} \\
FFN\_dim & 128 & 512\\
Activation & \multicolumn{2}{c}{ReLU} \\
% \bottomrule
\end{tabular}
\end{table}

We compared our model with the following baselines:

\begin{itemize}
  \item \textbf{Logistic Regression} (LR). We implemented it using \texttt{LogisticRegression} module of the scikit-learn package
  \item \textbf{Multi-layer Perceptron} (MLP). A simple deep learning model. We implemented using PyTorch~\citep{NEURIPS2019_bdbca288}. 
  \item \textbf{Gradient Boosting Decision Tree}: One of the most standard tree-based algorithms. We used two implementations in the experiments, \textbf{XGBoost}~\citep{Chen:2016:XST:2939672.2939785} and \textbf{CatBoost}~\citep{NEURIPS2018_14491b75}.   
  \item \textbf{FT-Transformer}~\citep{gorishniy2021revisiting}. One of the standard Transformer models for tabular data, on which our model is based. We implemented using \texttt{rtdl} package\footnote{\url{https://github.com/Yura52/rtdl}}.
  \item \textbf{SCARF}~\citep{bahri2022scarf} \textbf{with FT-Transformer}. Self-supervised learning for tabular data using contrastive learning. Although~\citet{bahri2022scarf} employed a multi-layer perceptron as an encoder, we substituted it with FT-Transformer to remove the effect of the choice of backbones. In the pre-training phase, the class token in the Encoder's output was fed to Projector, and computed the infoNCE loss using the output of Projector. In the fine-tuning phase, the class token of the Encoder's output was fed to the classification head to solve the downstream task. We set the number of Encoder's blocks to 6. Details of Encoder are shown in Table~\ref{tab:hypara_transformer}.
  \item \textbf{TransTab}~\citep{wang2022transtab}. A transformer-based model that supports transfer learning across different column sets. TransTab assumes that column names have semantic meanings, which may not be the case in the BRFSS dataset (see Section~\ref{sec:experiments} and Appendix~\ref{sec:datasets} for details of dataset characteristics.) For a fair comparison, we used the same Feature Tokenizer as our model for the tokenization of features. We set the number of Encoderâ€™s blocks to 6. Details of Encoder are shown in Table~\ref{tab:hypara_transformer_transtab}.
\end{itemize}

We applied only supervised learning using the fine-tuning dataset to those methods that did not support transfer learning, namely LR, MLP, GBDT, and FT-Transformer.
For SCARF and TransTab, we used part of the authors' implementation and modified the model as described above.
Even for those models, we implemented the training part of the models by ourselves.
For the methods that require pre-training, including our model, we trained the pre-trained models with multi-node distributed learning using PyTorch's \texttt{DistributedDataParallel}. 


\subsection{Implementation of TabRet}
\begin{table}[t]
\caption{Hyperparameters of Transformer layer. FFN stands for Feed-Forward Network and is often used in the Transformer layer.}
\label{tab:hypara_transformer}
 \centering
 \begin{tabular}{l|cccccc}
% \toprule
n\_blocks & 1 & 2 & 3 & 4 & 5 & 6 \\
\midrule
Token size & 96 & 128 & 192 & 256 & 320 & 384 \\
Head count & \multicolumn{6}{c}{8} \\
Activation \& FFN size factor & \multicolumn{6}{c}{(ReGLU, 4/3)} \\
Attention dropout & \multicolumn{6}{c}{0.1} \\
FFN dropout & 0.0 & 0.05 & 0.1 & 0.15 & 0.2 & 0.25 \\
Residual dropout & \multicolumn{6}{c}{0.0} \\
Initialization & \multicolumn{6}{c}{Kaiming} \\
% \bottomrule
\end{tabular}
\end{table}

In all experiments, unless otherwise specified, We set the number of Encoder blocks to 6. Details of each hyperparameter are shown in Table~\ref{tab:hypara_transformer}. We changed only the FFN dropout of Encoder to 0.1. The output dimension of the Feature Tokenizer and the input/output dimension of the Alignment Layer were aligned with the input dimension of the Encoder: $d_\FT = d_\AL = d_\enc$.

\subsection{Training and Hyperparameter Optimization}
\label{sec:training_tuning}

We performed hyperparameter optimization with Optuna~\citep{10.1145/3292500.3330701} for all methods except Logistic Regression, where the number of optimization trials was 500 trials for the tree-based method and 100 trials for the DL-based method. And we set the early stopping patience to 20 for all methods except Logistic Regression.

\subsubsection{Non-DL Methods}
\paragraph{Logistic Regression (LR).} We used the default settings, except that the maximum number of iterations was set to 1000.
\paragraph{XGBoost.} The hyperparameter space for Optuna is shown in Table~\ref{tab:xgb}.
\paragraph{CatBoost.} The hyperparameter space for Optuna is shown in Table~\ref{tab:cat}.

\begin{table}[ht]
 \caption{XGBoost hyperparameter space, the same hyperparameter space searched by \citet{grinsztajn2022why}. We used defaults for the other hyperparameters.}
 \label{tab:xgb}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    max\_depth & UniformInt[1, 11]\\
    n\_estimators & UniformInt[100, 6000, 200]\\
    min\_child\_weight & UniformInt[1, 1e2]\\
    subsample & Uniform[0.5, 1.0]\\
    learning\_rate & LogUniform[1e-5, 0.7]\\
    colsample\_bylevel & Uniform[0.5, 1.0]\\
    colsample\_bytree & Uniform[0.5, 1.0]\\
    gamma & LogUniform[1e-8, 7]\\
    lambda & LogUniform[1, 4]\\
    alpha & LogUniform[1e-8, 1e2]\\
   % \bottomrule
  \end{tabular}
\end{table}

\begin{table}[ht]
 \caption{CatBoost hyperparameter space. We used defaults for the other hyperparameters.}
 \label{tab:cat}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    max\_depth & UniformInt[3, 10]\\
    learning\_rate & LogUniform[1e-5, 1]\\
    bagging\_temperature & Uniform[0, 1]\\
    l2\_leaf\_reg & LogUniform[1, 10]\\
    leaf\_estimation\_iterations & UniformInt[1, 10]\\
   % \bottomrule
  \end{tabular}
\end{table}

\subsubsection{DL Methods}
\begin{table}[tb]
    \centering
    \caption{Detail about training for DL methods.}
    \label{tab:training_configs}
    \begin{tabular}{l|rr}
    % \toprule
    Config & Pre-training & Fine-tuning \\
    \midrule
    optimizer & \multicolumn{2}{c}{AdamW} \\
    weight decay & \multicolumn{2}{c}{1e-5}\\
    optimizer momentum & \multicolumn{2}{c}{$\beta_1, \beta_2 = 0.9, 0.99$}\\
    learning rate schedule & \multicolumn{2}{c}{cosine decay~\citep{loshchilov2017sgdr}}\\
    \midrule
    epochs & 1000 & 200\\
    base learning rate & 1.5e-5 & Searched by Optuna \\
    batch size & 4096 & 32\\
    warmup epochs~\citep{https://doi.org/10.48550/arxiv.1706.02677} & 40 & 5\\
    % \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:training_configs} shows the details of training for DL methods. For pre-training and fine-tuning, we use the linear $lr$ scaling rule~\citep{https://doi.org/10.48550/arxiv.1706.02677} for both pre-training and fine-tuning: $lr = base\_lr \times batchsize / 256$.

\paragraph{Multi-layer Perceptron (MLP).} The hyperparameter space for Optuna is shown in Table~\ref{tab:mlp}. Note that the sizes of the first and last layers were tuned and set separately, while the size of the in-between layers is the same for all of them.
\paragraph{FT-Transfomrer.} The hyperparameter space for Optuna is shown in Table~\ref{tab:fttrans}. Other parameters, such as token size, are determined from the corresponding parameters in Table~\ref{tab:hypara_transformer} according to the selected n\_blocks.
\paragraph{SCARF.} We corrupted features independently using the Bernoulli distribution. For all datasets, the probability of corruption was fixed at 0.6. The Feature Tokenizer and Encoder parameters learned during pre-training were frozen for all fine-tuning phases, as it was confirmed that this improved the fine-tuning accuracy. The hyperparameter space for Optuna is shown in Table~\ref{tab:scarf}.
\paragraph{TransTab.} We used Self-VPCL proposed by \citet{wang2022transtab} and pre-trained with overlap ratio = 0.1 and num\_partition = \{2, 3, 4\}. The hyperparameter space for Optuna is shown in Table~\ref{tab:transtab}.
\paragraph{TabRet.} For all datasets, the mask ratio for pre-training was fixed at 0.7 and the mask ratio for retokenizing was fixed at 0.5.The hyperparameter space for Optuna is shown in Table~\ref{tab:ours}.

\begin{table}[ht]
 \caption{MLP hyperparameter space.}
 \label{tab:mlp}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    layers & UniformInt[1, 8]\\
    layer\_size & UniformInt[1, 512]\\
    dropout & Uniform[0, 0.5]\\
    category\_embedding\_size & UniformInt[64, 512]\\
    base\_learning\_rate & LogUniform[1e-4, 1e-1]\\
   % \bottomrule
  \end{tabular}
\end{table}

\begin{table}[ht]
 \caption{FT-Transformer hyperparameter space.}
 \label{tab:fttrans}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    n\_blocks & UniformInt[1, 6]\\
    base\_learning\_rate & LogUniform[1e-4, 1e-1]\\
   % \bottomrule
  \end{tabular}
\end{table}

\begin{table}[ht]
 \caption{SCARF hyperparameter space.}
 \label{tab:scarf}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    base\_learning\_rate & LogUniform[1e-4, 1e-1]\\
   % \bottomrule
  \end{tabular}
\end{table}

\begin{table}[ht]
 \caption{TransTab hyperparameter space.}
 \label{tab:transtab}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    num\_partitions & UniformInt[2, 4]\\
    base\_learning\_rate & LogUniform[1e-4, 1e-1]\\
   % \bottomrule
  \end{tabular}
\end{table}

\begin{table}[ht]
 \caption{TabRet hyperparameter space.}
 \label{tab:ours}
 \centering
  \begin{tabular}{ll}
   % \toprule
    Parameter & Distribution\\
    \midrule
    base\_learning\_rate (Retokenizing) & LogUniform[1e-4, 1e-1]\\
    base\_learning\_rate & LogUniform[1e-4, 1e-1]\\
   % \bottomrule
  \end{tabular}
\end{table}
