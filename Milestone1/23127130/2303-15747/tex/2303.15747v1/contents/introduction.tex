\section{Introduction}

Transformer-based pre-trained models have been successfully applied to various domains such as text and images~\citep{bommasani2021on}. The Transformer-like architecture consists of two modules: a \emph{tokenizer}, which converts an input feature into a token embedding, and a \emph{mixer}, which repeatedly manipulates the tokens with attention and Feed-Forward Networks (FFN)~\citep{lin2021a,yu2021metaformer}. During pre-training, both modules are trained to learn representations that generalize to downstream tasks.

What has often been overlooked in the literature are scenarios where the input space change between pretext and downstream tasks.  A supervised problem on tabular data is a typical example, where rows or records represent data points and columns represent input features. Since the data scale is not as large as text and images, pre-trained models are expected to be beneficial~\citep{borisov2021deep}. A key challenge is that each table has a different set of columns, and it is difficult to know at the pre-training phase which columns will appear in the downstream task. We need to train the tokenizers from scratch for unseen columns with a small amount of data. Previous studies use text data such as a column name or a description to obtain the embeddings directly from language pre-trained models~\citep{wang2022transtab,hegselmann2022tabllm}, but we cannot do this when there is no such side information.

To address the above issue, we propose \emph{TabRet}, a pre-trainable Transformer network that can adapt to unseen columns in downstream tasks. First, TabRet is pre-trained based on the reconstruction loss with masking augmentation~\citep{devlin-etal-2019-bert,He_2022_CVPR}. Then, when unseen columns appear in a downstream task, their tokenizers are trained through masked modeling while freezing the mixer before fine-tuning, which we call \emph{retokenizing}. In experiments, we pre-trained TabRet with a table having more than two million rows and evaluated the performance on four tables containing $\sim 50$\% unseen columns. The results show TabRet outperformed baseline methods for all the datasets. Furthermore, the ablation study confirmed that retokenizing and random shuffling of columns within a batch further enhanced the pre-training effect. 


\begin{figure}[tb]
\centering
\includegraphics[width=.95\textwidth]{img/concept.pdf}
\label{fig:concept}
\caption{\textbf{Problem setting and our approach.} (a) We consider transfer learning across tables that have different columns, where the pretext data is not available in the downstream task, and vice versa. (b) Our model consists of two modules: a tokenizer for each column and a mixer. We train the two modules based on the masked autoencoder loss in pre-training. (c) We introduce additional tokenizers for new columns and train them while freezing the old tokenizers and the mixer in a downstream task. }
\end{figure}