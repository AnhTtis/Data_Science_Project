\section{Discussion}

Tabular data have been notorious for transfer learning due to the difference in column sets. We addressed the problem and presented the transformer network with two additional steps --- random shuffling in pre-training and retokenizing before fine-tuning --- excelled in its potential as a pre-trained model. We hope our results will open a new research direction of pre-trained models on tabular data or other data domains where the input space can be changed. 

The current limitation is that pre-training does not always provide performance gain, especially when the domain of a downstream task is irrelevant to the pretext data. We evaluated the pre-trained model used in the experiments on several datasets having no column overlap. The preliminary result was that the performance was worse than the supervised methods. It may be because the generative data process is entirely different, and there is no transferable knowledge in the task pair. Another possibility is that there was transferable knowledge, but our framework failed to capture it. Further investigation of this topic would be promising as future work. 