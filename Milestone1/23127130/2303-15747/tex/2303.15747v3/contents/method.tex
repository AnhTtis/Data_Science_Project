
\section{Method}\label{sec:method}

\paragraph{Problem Formulation.}
In pre-training, suppose we have a table consisting of a finite set of columns $\columns$. Let $\xspace_c$ be the space where column $c\in\columns$ takes its value. A row $\vx$ is then defined on the product space $\xspace(\columns)=\prod_{c\in\columns}\xspace_c$. Suppose a downstream task is defined as a supervised task on an input-output pair $(\vx', y)$. Here we consider the case where the input $\vx'\in\xspace(\columns')$ is a row  of another column set $\columns'\neq\columns$ (Figure~\ref{fig:concept} a). As an example, consider healthcare records where $\columns=\{\mathtt{age}, \mathtt{gender}, \mathtt{weight}\}$ are given as a column set in pre-training and $\columns'=\{\mathtt{gender}, \mathtt{BMI} \}$ are in fine-tuning. In this case, the column $\mathtt{gender}$ appears in common, but other columns $\{\mathtt{age}, \mathtt{weight}, \mathtt{BMI}\}$ appear only in either pre-training or fine-tuning. We assume that the pre-training table is inaccessible during fine-tuning. 

\paragraph{Model Structure.} 
Given a set of columns $\columns$, we define a tokenizer $t_c: \xspace_c \to \mathcal{E}$ for each column $c\in \columns$ as a function that converts the column value $x_c\in\xspace_c$ into an embedding vector $e_c\in\mathcal{E}$. The tokenizers then map an entire row $\vx$ to a set of embeddings $\{e_c \mid c\in\columns\}$, which are passed to a transformer-based encoder $h: \mathcal{E}^{\columns} \to \zspace$ to produce a latent representation $z\in\zspace$. We also use a decoder $d: \zspace \to \xspace(\columns)$ to reconstruct the input in pre-training/retokenizing and a head $p: \zspace \to \yspace$ to predict the target variable in fine-tuning. 
%We employ Feature Tokenizer~\citep{gorishniy2021revisiting} as $t_c$. 
Note that part of the decoder and the head have the same network structure, and their parameters are partially shared. More details about the network architecture are described in Appendix~\ref{sec:model}.

\paragraph{Pre-training with Shuffle Augmentation.} \label{sec:shuffle-augmentation}
We train the tokenizers of the columns $\columns$, the encoder, and the decoder by following the approach of the masked autoencoder~\citep{He_2022_CVPR}; that is, we randomly select the columns with a masking ratio and replace the corresponding embeddings with a special embedding called a mask token. We then reconstruct the values of the masked columns and compute the loss to update the parameters. We empirically find that the masking ratio = 0.7 works well for pre-training and 0.5 for retokenizing. We use these values throughout the experiments unless otherwise mentioned.

During pre-training, we apply the shuffle augmentation as follows. Let $\vx_c = (x_{1c}, \dots, x_{nc})\in\xspace_c^n$ be a batch of column $c$ of size $n$ and $\mathrm{perm}(\cdot)$ be a random permutation. Suppose $\tilde{\columns} \subseteq\columns$ is a set of columns chosen uniformly randomly based on a shuffle ratio. Then the shuffle augmentation replaces $\vx_c$ with $\mathrm{perm}(\vx_c)$ for $c \in \tilde{\columns}$. We set the shuffle ratio to 0.1.


\paragraph{Retokenizing and Fine-tuning.} 
In a downstream task, we first add initialized tokenizers for the newly appearing columns. Part of the decoder is also initialized to match the fine-tuning table. Then we train them by masked autoencoding. During this time, we do not update the parameters of the old tokenizers, encoder, and decoder (Figure~\ref{fig:concept} c). Afterward, the head is added to the backbone network (tokenizer + encoder) and fine-tuned to predict the target variable. 

The primary motivation for retokenizing is to efficiently train the new tokenizers with a relatively small number of data points. Although we can train them in a supervised manner, the model is easily overfitted because, in addition to the tokenizers, the head must be trained with a single target signal. In contrast, the masked autoencoding loss used in retokenizing provides more signals to reconstruct than the target variable, which is expected to induce better token representations. 
