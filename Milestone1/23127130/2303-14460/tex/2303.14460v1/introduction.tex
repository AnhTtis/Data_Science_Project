\section{Introduction}
Deep Neural Networks (DNNs) have achieved remarkable success in a variety of tasks, but their vulnerability against adversarial examples~\cite{szegedy2013intriguing, goodfellow2014explaining} have caused serious concerns about their application in safety-critical scenarios \cite{chen2015deepdriving,ma2019understanding}. DNNs can be easily fooled by adding small, even imperceptible perturbations to the natural examples. To address this issue, numerous defense approaches have been proposed~\cite{papernot2016distillation,das2017keeping,xie2019feature,bai2019hilbert,mo2022adversarial}, among which Adversarial Training (AT)~\cite{madry2017towards,wang2021convergence} has been demonstrated as the most effective method to improve the model robustness against such attacks~\cite{athalye2018obfuscated,wu2020adversarial}.
Adversarial training can be formulated as the following min-max optimization problem:
\begin{equation}
\label{AT}
    \min_{\boldsymbol\theta} \mathbb{E}_{(x, y)\sim\mathcal D} \max_{\|{x'}-{x}\|
    \le \epsilon} \mathcal L({\boldsymbol\theta};{x'},y),
\end{equation}
where $\mathcal D$ is the data distribution, $\epsilon$ is the margin of perturbation and $\mathcal L$ is the loss function, \textit{e.g.} the cross-entropy loss. Generally, Projected Gradient Descent (PGD) attack~\cite{madry2017towards} has shown satisfactory effectiveness to find adversarial examples in the perturbation bound $\mathcal B(x,\epsilon)=\{x':\|x'-x\|\le\epsilon\}$, which is commonly used in solving the inner maximization problem in \eqref{AT}:
\begin{equation}
    x^{t+1}=\Pi_{\mathcal B(x,\epsilon)} (x^t+\alpha\cdot\text{sign}(\nabla_{x^t} \mathcal L(\boldsymbol\theta;x^t,y))),
\end{equation}
where $\Pi$ is the projection function and $\alpha$ controls the step size of gradient ascent. TRADES~\cite{zhang2019theoretically} is another variant of AT, which adds a regularization term to adjust the trade-off between robustness and accuracy \cite{tsipras2018robustness,wang2023simple}:
\begin{equation}
\label{TRADES}
    \min_{\boldsymbol\theta}\mathbb{E}_{(x, y)\sim\mathcal D}\ \{\mathcal L({\boldsymbol\theta};{x},y) + \beta\max_{\|{x'}-{x}\|\le\epsilon} \mathcal{K} (f_{\boldsymbol\theta}({x}), f_{\boldsymbol\theta}({x'}))\},
\end{equation}
where $\mathcal K(\cdot)$ is the KL divergence and $\beta$ is the \textit{robustness regularization} to adjust the robustness-accuracy trade-off. 

Although certain robustness has been achieved by AT and its variants, there still exists a stark difference among class-wise robustness in adversarially trained models, \textit{i.e.}, the model may exhibit strong robustness on some classes while it can be highly vulnerable on others, as firstly revealed in \cite{xu2021robust,tian2021analysis,benz2021robustness}. This disparity raises the issue of robustness fairness, which can lead to further safety concerns of DNNs, as the models that exhibit good overall robustness may be easily fooled on some specific classes, \textit{e.g.}, the stop sign in automatic driving. To address this issue, Fair Robust Learning (FRL)~\cite{xu2021robust} has been proposed, which adjusts the margin and weight among classes when fairness constraints are violated. However, this approach only brings limited improvement on robust fairness while causing a drop on overall robustness. 

In this paper, we first present some theoretical insights on how different adversarial configurations impact class-wise robustness, and reveal that strong attacks can be detrimental to the \textit{hard} classes (classes that have lower clean accuracy). This finding is further empirically confirmed through evaluations of models trained under various adversarial configurations. Additionally, we observe that the worst robustness among classes fluctuates significantly between different epochs during the training process. It indicates that simply selecting the checkpoint with the best overall robustness like the previous method \cite{rice2020overfitting} may result in poor robust fairness, \textit{i.e.}, the worst class robustness may be extremely low.

Inspired by these observations, we propose to dynamically customize different training configurations for each class. Note that unlike existing instance-wise customized methods that aim to enhance overall robustness ~\cite{ding2018mma,balaji2019instance,wang2019improving,cheng2020cat,zhang2020attacks}, we also focus on the fairness of class-wise robustness. Furthermore, we modify the weight averaging technique to address the fluctuation issue during the training process. Overall, we name the proposed framework as \textbf{C}lass-wise calibrated \textbf{F}air \textbf{A}dversarial training (CFA). 

Our contributions can be summarized as follows:
\begin{itemize}
    \item We show both theoretically and empirically that different classes require appropriate training configurations. In addition, we reveal the fluctuating effect of the worst class robustness during adversarial training, which indicates that selecting the model with the best overall robustness may result in poor robust fairness.
    
    \item We propose a novel approach called Class-wise calibrated Fair Adversarial training (CFA), which dynamically
    customizes adversarial configurations for different classes during the training phase, and modifies the weight averaging technique to improve and stabilize the worst class robustness.
    
    \item Experiments on benchmark datasets demonstrate that our CFA outperforms state-of-the-art methods in terms of both overall robustness and fairness, and can be also easily incorporated into other adversarial training approaches to further improve their performance.
    
\end{itemize}