\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}          

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{multicol}
\newtheorem{theorem}{Theorem}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}


\linespread{0.97}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{9439}
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{CFA: Class-wise Calibrated Fair Adversarial Training}

\author{
    Zeming Wei\textsuperscript{1}, 
    Yifei Wang\textsuperscript{1},
    Yiwen Guo\textsuperscript{2},
    Yisen Wang\textsuperscript{3,4}\thanks{Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn)}\\
    \textsuperscript{1}School of Mathematical Sciences, Peking University 
    \textsuperscript{2}Independent Researcher\\
    \textsuperscript{3}National Key Lab of General Artificial Intelligence \\ School of Intelligence Science and Technology, Peking University\\
\textsuperscript{4}Institute for Artificial Intelligence, Peking University
}


\maketitle

\begin{abstract}
   Adversarial training has been widely acknowledged as the most effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). So far, most existing works focus on enhancing the overall model robustness, treating each class equally in both the training and testing phases. Although revealing the disparity in robustness among classes, few works try to make adversarial training fair at the class level without sacrificing overall robustness. In this paper, we are the first to theoretically and empirically investigate the preference of different classes for adversarial configurations, including perturbation margin, regularization, and weight averaging. Motivated by this, we further propose a \textbf{C}lass-wise calibrated \textbf{F}air \textbf{A}dversarial training framework, named CFA, which customizes specific training configurations for each class automatically. Experiments on benchmark datasets demonstrate that our proposed CFA can improve both overall robustness and fairness notably over other state-of-the-art methods. Code is available at \url{https://github.com/PKU-ML/CFA}.
\end{abstract}

\input{introduction}

\input{Analysis}

\input{Observations}

\input{CCAT}

\input{Experiment}


\section{Conclusion}
In this paper, we first give a theoretical analysis of how attack strength in adversarial training impacts the performance of different classes. Then, we empirically show the influence of adversarial configurations on class-wise robustness and the fluctuate effect of robustness fairness, and point out there should be some appropriate configurations for each class.
Based on these insights, we propose a \textbf{C}lass-wise calibrated \textbf{F}air \textbf{A}dversarial training (CFA) framework to adaptively customize class-wise train  configurations for improving robustness and fairness.
Experiment shows our CFA outperforms state-of-the-art methods both in overall and fairness metrics, and can be easily incorporated into existing methods to further enhance their performance. 

\section*{Acknowledgement}
Yisen Wang is partially supported by the National Key R\&D Program of China (2022ZD0160304), the National Natural Science Foundation of China (62006153), and Open Research Projects of Zhejiang Lab (No. 2022RC0AB05).


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\input{appendix}



\end{document}
