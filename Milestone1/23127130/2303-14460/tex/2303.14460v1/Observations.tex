\section{Observations on Class-wise Robustness}
In this section, we present our empirical observations on the class-wise robustness of models adversarially trained under different configurations. Taking vanilla AT \cite{madry2017towards} and TRADES \cite{zhang2019theoretically} as examples, we compare two key factors in the training configurations: the perturbation margin $\epsilon$ in vanilla AT and the regularization $\beta$ in TRADES.
We also reveal the fluctuation effect of the worst class robustness during the training process, which has a significant impact on the robust fairness in adversarial training.

\subsection{Different Margins}
\label{margin analysis}

Following the vanilla AT~\cite{madry2017towards}, we train 8 models on the CIFAR10 dataset~\cite{krizhevsky2009learning} with $l_\infty$-norm perturbation margin $\epsilon$ from $2/255$ to $16/255$ and analyze their overall and class-wise robustness. 
\begin{figure*}[t]
    \centering
    \begin{tabular}{ccc}
    \includegraphics[width=0.3\textwidth]{fig/compare_epsilon_overall.png} &
    \includegraphics[width=0.3\textwidth]{fig/compare_epsilon_cw_101_120.png} &
    \includegraphics[width=0.3\textwidth]{fig/compare_epsilon_cw_181_200.png}
    \\
    (a) &  
    (b) & 
    (c)
    \end{tabular}
    \vspace{-0.15 in}
    \caption{Comparison of overall and class-wise robustness of models adversarially trained on CIFAR10 with different perturbation margin $\epsilon$. (a): Overall robust accuracy with different perturbation margin $\epsilon$ from 2/255 to 16/255. (b): Average class-wise robust accuracy at epoch $101-120$ (each line represents a class). (c): Average class-wise robust accuracy at epoch $181-200$ (each line represents a class).}
    \label{fig:margin}
\end{figure*}

The comparison of overall robustness is shown in Fig.~\ref{fig:margin}(a). The robustness is evaluated under PGD-10 attack bounded by $\epsilon_0=8/255$, which is commonly used for robustness evaluation.
Intuitively, using a larger margin can lead to better robustness. For $\epsilon<\epsilon_0$, the attack is too weak and hence the robust accuracy of the trained model is not comparable with $\epsilon\ge \epsilon_0$. 
However, for the three models trained with $\epsilon>\epsilon_0$, although their robustness outperforms the case of $\epsilon=\epsilon_0$ at the last epoch, they do not make significant progress on the best-case robustness (around 100-th epoch). 

We take a closer look at this phenomenon by investigating their class-wise robustness in Fig.~\ref{fig:margin}(b) and Fig.~\ref{fig:margin}(c). For each class, we calculate the average class-wise robust accuracy among the 101$-$120-th epochs (where the model performs the best robustness) and 181$-$200-th epochs, respectively. From Fig.~\ref{fig:margin}(b), we can see that a larger training margin $\epsilon$ does not necessarily result in better class-wise robustness. For the \textit{easy} classes which perform higher robustness, their robustness monotonously increase as $\epsilon$  enlarges from $2/255$ to $16/255$. By contrast, for the \textit{hard} classes (especially class $2, 3, 4$), their robustness drop when $\epsilon$ enlarges from $8/255$. However, for the last several checkpoints in Fig.~\ref{fig:margin}(c), we can see a consistent increase on class-wise robustness when the $\epsilon$ enlarges. Revisiting the overall robustness, we can summarize that the class-wise robustness is boosted mainly by reducing the robust over-fitting problem in the last checkpoint.
This can explain why Fair Robust Learning (FRL)~\cite{xu2021robust} can improve robust fairness  by enlarging the margin for the hard classes, since the model reduces the over-fitting problem on these classes. Considering the overall robustness is lower in the last checkpoint (robust fairness is better though), we hope to improve the best-case robust fairness in the situation of a relatively high overall robustness. 

In summary, larger perturbation is harmful to the hard classes in the best case, while can marginally improve the class-wise robustness in the later stage of training. For easy classes, larger perturbation is useful at whatever the best and last checkpoints. Therefore, a specific and proper perturbation margin is needed for each class. 

\subsection{Different Regularizations}
\label{regularizations}
In this section, we also conduct a similar experiment on the selection of \textit{robustness regularization} $\beta$ in TRADES. We compare models trained on CIFAR10 with $\beta$ from 1 to 8, and plot the average class-wise robust and clean accuracy among the $151-170$-th epochs (where TRADES performs the best performance) in Fig.~\ref{fig:trades}. We can see that bias more weight on robustness (use larger $\beta$) cause different influences among classes. Specifically, for \textit{easy classes}, improving $\beta$ can improve their robustness at the cost of little clean accuracy reduction, while for \textit{hard classes} (\textit{e.g.}, classes $2,3,4$), improving $\beta$ can only obtain limited robustness improvement but drop clean accuracy significantly. 

\begin{figure}[h]
    \centering
    \begin{tabular}{cc}
         \includegraphics[width=0.22\textwidth]{fig/compare_trades.png}
         & 
         \includegraphics[width=0.22\textwidth]{fig/compare_trades_clean.png}
         \\
         (a) & (b)
    \end{tabular}
    \vspace{-0.15 in}
    \caption{Comparison of class-wise robustness trained by TRADES with different robustness regularization parameters $\beta$. (a) Class-wise robust accuracy. (b) Class-wise clean accuracy.}
    \label{fig:trades}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.22\textwidth]{fig/worst_and_overall.png} &  
        \includegraphics[width=0.22\textwidth]{fig/EMA_worst_and_overall.png}
        \\
        (a) & (b)
    \end{tabular}
    \vspace{-0.15 in}
    \caption{Comparison of overall robustness, the worst class robustness, and the absolute variation of the worst class robustness between adjacent checkpoints. (a): Vanilla AT. (b): AT with fairness aware weight averaging (FAWA), start from epoch 50.}
    \label{fig:worst and overall}
\end{figure}

This result is consistent with the Theorem \ref{theorem:compare}. Recall that in the toy model, hard class $y=-1$ costs more clean accuracy to exchanges for little robustness improvement than easy class $y=+1$. Therefore, similar to the analysis on perturbation margin $\epsilon$, we also point out that there exists a proper $\beta_y$ for each class.





\subsection{Fluctuation Effect}
\label{fluctuate}
In this section, we reveal an intriguing property regarding the fluctuation of class-wise robustness during adversarial training. In Fig.~\ref{fig:worst and overall}(a), we plot the overall robustness, the worst class robustness, and the variance of the worst robustness between adjacent epochs in vanilla adversarial training. While the overall robustness tends to be more stable between adjacent checkpoints (except when the learning rate decays), the worst class robustness fluctuates significantly. Particularly, many adjacent checkpoints between the $101-120$-th epochs exhibit a nearly 10\% difference in the worst class robustness, while changes in overall robustness are negligible (less than 1\%).
Therefore, previously widely used selecting the best checkpoint based on overall robustness may result in an extremely unfair model. Taking the plotted training process as an example, the model achieves the highest robust accuracy of 53.2\% at the 108-th epoch, which only has 23.5\% robust accuracy on the worst class. In contrast, the checkpoint at epoch 110, which has 52.6\% overall and 28.1\% worst class robust accuracy, is preferred when considering fairness.
