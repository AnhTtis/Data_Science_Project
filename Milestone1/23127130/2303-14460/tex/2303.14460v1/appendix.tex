\newpage
\section{Details of the illustration Example}
\label{illustration Example}
In Sec.~\ref{sec:theoretical insight}, we use an illustration example to draw the theoretical results. Here we show the implementation detail of this toy example.

Note that for the case $d=1$, the data distribution is 
\begin{equation}
    x_1 = \begin{cases}
    + y,& \text{w.p.}\ p_y\\
    - y,& \text{w.p.}\ 1-p_{y}
    \end{cases}  \text{and} \quad
    x_2\overset{\text{i.i.d}}{\sim}\mathcal N(\eta y, \sigma^2).
\end{equation}

In this toy model, we select $p_{+1} = 0.85 > 0.7 = p_{-1}$ and $\eta=0.4$. The variance $\sigma^2$ is set to be 0.6 for better visualization in this toy model, and in the following theoretical analysis, we set $\sigma^2=1$ for simplicity. In Fig.~\ref{fig:toy model}(a), we randomly sample 100 pairs of $(x_1,x_2)$ for each class $y\in\{+1,-1\}$. In Fig.~\ref{fig:toy model}(b), the robustness is evaluated under perturbation bound $\epsilon=2\eta=0.8$, which is consistent to the evaluation in~\cite{tsipras2018robustness}.

\section{Proofs for Theorems in Sec.~\ref{sec:theoretical insight}}
\label{proof}
\subsection{Preliminaries}
We denote the distribution function and the  probability density function  of the \textit{normal distribution} $\mathcal N(0,1)$ as $\phi(x)$ and $\Phi(x)$:
\begin{equation}
    \begin{split}
        \Phi(x) = & \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-\frac {x^2} {2}}{\mathrm d}t = \Pr.(\mathcal N(0,1) < x),\\
    \phi(x)   = & \frac{1}{\sqrt{2\pi}}e^{-\frac {x^2} {2}} = \Phi'(x).
    \end{split}
\end{equation}
Recall that the data distribution is 
\begin{equation}
\begin{split}
        &x_1 = \begin{cases}
    + y,& \text{w.p.}\ p_y,\\
    - y,& \text{w.p.}\ 1-p_{y},
    \end{cases} \\
        &x_2, \cdots, x_{d+1}\overset{\text{i.i.d}}{\sim}\mathcal N(\eta y, 1),
\end{split}
\end{equation}
where $1>p_{+1}>p_{-1}>\frac 1 2$.
First we calculate the clean accuracy $\mathcal A_y(f_w)$ and the robust accuracy $\mathcal R_y(f_w)$ for any class $y\in\{+1, -1\}$ and $w>0$. Also recall that the classifier 
\begin{equation}
    f_w=\text{sign}(x_1+\frac{x_2+\cdots+x_{d+1}}{w})
\end{equation}

Note that $w>0$, we have

\begin{equation}
\label{eq:A+1}
\begin{split}
    \mathcal A_{+1}(f_w) &= \Pr.(\text{sign}(f_w)=1)\\
    &= \Pr.(x_1 + \frac{x_2+\cdots +x_{d+1}}{w}>0)\\
    &= p_{+1}\cdot \Pr.(1 + \frac{x_2+\cdots +x_{d+1}}{w}>0) \\ 
    &+ (1-p_{+1})\cdot \Pr.(-1 + \frac{x_2+\cdots +x_{d+1}}{w}>0)\\
    &= p_{+1}\cdot \Pr.({x_2+\cdots +x_{d+1}}>-w) \\ 
    &+ (1-p_{+1})\cdot \Pr.(x_2+\cdots +x_{d+1}>w) \\
    &= p_{+1}\cdot \Pr.(\mathcal N(d\eta , d) > -w)\\ 
    &+ (1-p_{+1})\cdot \Pr.(\mathcal N(d\eta , d) > w) \\
    &= p_{+1} \cdot \Pr.(\mathcal N(0,d)> -d\eta-w)\\
    &+ (1-p_{+1}) \cdot \Pr. (\mathcal N(0,d) > -d\eta + w)\\
    &= p_{+1}\cdot \Pr. (\mathcal N(0,1) < \frac{d\eta + w}{\sqrt d})\\
    &+ (1-p_{+1})\cdot \Pr. (\mathcal N(0,1) < \frac{d\eta - w}{\sqrt d})\\
    & = p_{+1}\Phi(\frac{d\eta + w}{\sqrt d}) + (1-p_{+1}) \Phi(\frac{d\eta - w}{\sqrt d}).
\end{split}
\end{equation}
Similarly, we have 
\begin{equation}
\label{eq:A-1}
    \mathcal A_{-1}(f_w) = p_{-1}\Phi(\frac{d\eta + w}{\sqrt d}) + (1-p_{-1}) \Phi(\frac{d\eta - w}{\sqrt d}).
\end{equation}
For the robustness, following the evaluation in the original model~\cite{tsipras2018robustness}, we evaluate the  robustness $\mathcal R_y$ under $l_\infty$-norm perturbation bound $\epsilon=2\eta<1$. Consider the distribution of adversarial examples $\hat x = (\hat x_1, \hat x_2, \cdots, \hat x_{d+1})$. Since we restrict the robust feature $x_1\in\{-1, +1\}$ and $\epsilon < 1$, we have $\hat x_1 = x_1$. For the non-robust features $x_i\sim\mathcal N(\eta y, 1)$, the corresponding adversarial example has $\hat x_i\sim \mathcal N(-\eta y, 1)$ under the perturbation bound $\epsilon=2\eta$. Therefore, the distribution of adversarial examples is
\begin{equation}
\label{eval_adv_distribution}
    \hat x_1 = \begin{cases}
    + y,& \text{w.p.}\ p_y\\
    - y,& \text{w.p.}\ 1-p_{y}
    \end{cases}  \text{and} \quad
    \hat x_2, \cdots, \hat x_{d+1}\overset{\text{i.i.d}}{\sim}\mathcal N(-\eta y, 1).
\end{equation}
By simply replacing $\eta$ with $-\eta$ in derivative process of (\ref{eq:A+1}), for any $w>0$, we have
\begin{equation}
\label{R}
\begin{split}
    \mathcal R_{+1}(f_w) &= p_{+1}\Phi(\frac{-d\eta + w}{\sqrt d}) + (1-p_{+1}) \Phi(\frac{-d\eta - w}{\sqrt d}),\\
    \mathcal R_{-1}(f_w) &= p_{-1}\Phi(\frac{-d\eta + w}{\sqrt d}) + (1-p_{-1}) \Phi(\frac{-d\eta - w}{\sqrt d}).
\end{split}
\end{equation}

\subsection{Proof of Theorem~\ref{theorem:difficulty}}
The theorem~\ref{theorem:difficulty} shows the class $y=-1$ is intrinsically difficult to learn than class $y=+1$:

\noindent\textbf{Theorem 1}
For any $w>0$ and the classifier $f_w=\text{sign}(x_1+\frac{x_2+\cdots+x_{d+1}}{w})$, we have $\mathcal A_{+1}(f_w) > \mathcal A_{-1} (f_w)$ and  $\mathcal R_{+1}(f_w)>R_{-1}(f_w)$.

\noindent\textit{Proof.} Note that $p_{+1} > p_{-1}$, and $\Phi(\frac{d\eta + w}{\sqrt d})> \Phi(\frac{d\eta - w}{\sqrt d})$, we have
\begin{equation}
\begin{split}
\mathcal{A}_{+1}(f_w)&=p_{+1}\Phi(\frac{d\eta + w}{\sqrt d}) + (1-p_{+1}) \Phi(\frac{d\eta - w}{\sqrt d})  \\
&= p_{+1}(\Phi(\frac{d\eta + w}{\sqrt d})- \Phi(\frac{d\eta - w}{\sqrt d})) + \Phi(\frac{d\eta - w}{\sqrt d}) \\
&>p_{-1}(\Phi(\frac{d\eta + w}{\sqrt d})- \Phi(\frac{d\eta - w}{\sqrt d})) + \Phi(\frac{d\eta - w}{\sqrt d}) \\
&= \mathcal{A}_{-1}(f_w).
\end{split}
\end{equation}

\subsection{Proof of Theorem~\ref{theorem:eps}}
The theorem~\ref{theorem:eps} shows the relation between the parameter $w$ and the attack strength (perturbation bound $\epsilon$) in adversarial training:

\noindent\textbf{Theorem 2} For any $0\le\epsilon\le\eta$, the optimal parameter $w$ for adversarial training with perturbation bound $\epsilon$ is monotone increasing at $\epsilon$.

\noindent\textit{Proof.} Similar to the adversarial example distribution analysis (\ref{eval_adv_distribution}), under the perturbation bound $\epsilon$, the data distribution of the crafted adversarial example for training is 
\begin{equation}
\label{train data distribution}
\begin{split}
    &\tilde x_1 = \begin{cases}
    + y,& \text{w.p.}\ p_y\\
    - y,& \text{w.p.}\ 1-p_{y}
    \end{cases},\\
    &\tilde x_2, \cdots, \tilde x_{d+1}\overset{\text{i.i.d}}{\sim}\mathcal N((\eta-\epsilon) y, 1).
\end{split}
\end{equation}
We use $\tilde {\mathcal A}(f_w)$, $\tilde {\mathcal A}_y(f_w)$ to denote the overall and class-wise \textit{train accuracy} of the classifier $f_w$ on training data distribution (\ref{train data distribution}).
Let $p = p_{+1}+p{-1}$.
Then the overall train accuracy of $f_w$ is 
\begin{equation}
\begin{split}
   & \tilde{\mathcal A}(f_w)= \frac{1}{2}(\tilde{\mathcal A}_{+1}(f_w) + \tilde{\mathcal A}_{-1}(f_w)) \\
   & =\frac{1}{2}(p_{+1}\Phi(\frac{d(\eta-\epsilon) + w}{\sqrt d}) + (1-p_{+1}) \Phi(\frac{d(\eta-\epsilon) - w}{\sqrt d})\\
   & + p_{-1}\Phi(\frac{d(\eta-\epsilon) + w}{\sqrt d}) + (1-p_{-1}) \Phi(\frac{d(\eta-\epsilon) - w}{\sqrt d})
    )\\
    & = \frac{1}{2}(p\Phi(\frac{d(\eta-\epsilon) + w}{\sqrt d}) + (2-p)\Phi(\frac{d(\eta-\epsilon) - w}{\sqrt d})).
\end{split}
\end{equation}

Now we calculate the best parameter $w$ for $\tilde{\mathcal A}(f_w)$. Note that $\Phi'(x)=\phi(x)$, we have

\begin{equation}
\begin{split}
    & \frac{\partial \tilde{\mathcal A}(f_w)}{\partial w}=\frac {1}{2\sqrt d} (p\phi(\frac{d(\eta-\epsilon) + w}{\sqrt d}) - (2-p)\phi(\frac{d(\eta-\epsilon) - w}{\sqrt d}))\\
   &  = \frac {1}{2\sqrt{2\pi d}}\{ p\exp[-\frac 1 2(\frac{d(\eta-\epsilon) + w}{\sqrt d})^2]\\& -(2-p)\exp[-\frac 12(\frac{d(\eta-\epsilon)-w}{\sqrt{d}})^2] \}
\end{split}
\end{equation}
Therefore, $\frac{\partial \tilde{\mathcal A}(f_w)}{\partial w}>0$ is equivalent to
\begin{equation}
\begin{split}
&p\exp[-\frac 1 2(\frac{d(\eta-\epsilon) + w}{\sqrt d})^2]> (2-p)\exp[-\frac 12(\frac{d(\eta-\epsilon)-w}{\sqrt{d}})^2]\\
&\iff \exp[-\frac 1 2((\frac{d(\eta-\epsilon) + w}{\sqrt d})^2-(\frac{d(\eta-\epsilon)-w}{\sqrt{d}})^2)]>\frac {2-p}{p}\\
&\iff \exp[-\frac 1 {2d} \cdot (4d(\eta-\epsilon)w)]>\frac {2-p}{p}\\
&\iff \exp[-2(\eta-\epsilon)w]>\frac {2-p}{p}\\
&\iff -2(\eta-\epsilon)w > \ln(\frac {2-p}{p})\\
&\iff w < \frac{1}{2(\eta-\epsilon)}\ln(\frac{p}{2-p}):=\hat w_\epsilon.
\end{split}
\end{equation}
Recall that we assume $p_{+1}, p_{-1}>\frac 1 2$, thus $p=p_{+1}+p_{-1}>1$ and $\frac p {2-p} > 1$. 
Therefore, $\frac{\partial \tilde{\mathcal A}(f_w)}{\partial w}>0$ when $w<\hat w_\epsilon$, and $\frac{\partial \tilde{\mathcal A}(f_w)}{\partial w}<0$ when $w>\hat w_\epsilon$. We can conclude that $f_w$ obtains the optimal parameter $w$, \textit{i.e.}, $w$ achieves the highest train accuracy,  when $w=\hat w_\epsilon = \frac{1}{2(\eta-\epsilon)}\ln(\frac{p}{2-p})$, which is monotone increasing at $\epsilon$.

\subsection{Proof of Theorem~\ref{theorem:best}}
Theorem~\ref{theorem:best} shows the clean accuracy of the hard class $y=-1$ drops earlier than class $y=+1$ as  the attack strength increases:

\noindent\textbf{Theorem 3} Let $w_y^* = \arg\max\limits_w \mathcal A_y(f_w)$ be the parameter for the best clean accuracy of class $y$, then $w_{+1}^*>w_{-1}^*$.

\noindent\textit{Proof.} 
As calculated in (\ref{eq:A+1}) and (\ref{eq:A-1}), we have $\mathcal A_y(f_w)  = p_y\Phi(\frac{d\eta + w}{\sqrt d}) + (1-p_y) \Phi(\frac{d\eta - w}{\sqrt d})$ and 
\begin{equation}
\begin{split}
    &\frac{\partial {\mathcal A}(f_w)}{\partial w} = \frac{1}{\sqrt d}(p_y\phi(\frac{d\eta + w}{\sqrt d}) - (1-p_y) \phi(\frac{d\eta - w}{\sqrt d})).
    \end{split}
\end{equation}

Therefore, $\frac{\partial {\mathcal A}(f_w)}{\partial w}>0$ is equivalent to
\begin{equation}
\begin{split}
    &\exp\{-\frac 1 2 [(\frac{d\eta + w}{\sqrt d})^2 - (\frac{d\eta - w}{\sqrt d}) ^2]\} > \frac{1-p_y}{p_y}\\
    &\iff \exp\{-2\eta w\}>\frac{1-p_y}{p_y}\\
    & \iff -2\eta w > \ln (\frac{1-p_y}{p_y})\\
    & \iff w < \frac{1}{2\eta}\ln(\frac{p_y}{1-p_y}).
\end{split}
\end{equation}
Similar to the proof of Theorem 2, we have $w^*_y = \arg\max \mathcal A_y(f_w)=\frac{1}{2\eta}\ln(\frac{p_y}{1-p_y})$. Since $1>p_{+1}>p_{-1}>\frac 1 2$, we have $\frac{p_{+1}}{1-p_{+1}}>\frac{p_{-1}}{1-p_{-1}}>1$ and hence $w^*_{+1}>w^*_{-1}$.

\subsection{Proof of Theorem~\ref{theorem:compare}}
Theorem 4 shows how strong attack in adversarial training hurts the hard class $y=-1$:

\noindent\textbf{Theorem 4} Suppose $\Delta_w>0$, then for $\forall w>w_{+1}^*
$, $\mathcal A_{-1}(f_{w+\Delta_w}) - \mathcal A_{-1}(f_w) < \mathcal A_{+1}(f_{w+\Delta_w}) - \mathcal A_{+1}(f_w)<0$, and for $\forall w>0$, $0<\mathcal R_{-1}(f_{w+\Delta_w}) - \mathcal R_{-1}(f_w) < \mathcal R_{+1}(f_{w+\Delta_w}) - \mathcal R_{+1}(f_w)$. 

\noindent \textit{Proof.} First we prove for $u>w^*_{+1}$,
\begin{equation}
    \mathcal A_{-1}(f_{w+\Delta_w}) - \mathcal A_{-1}(f_w) < \mathcal A_{+1}(f_{w+\Delta_w}) - \mathcal A_{+1}(f_w)<0.
\end{equation}
Since we have
\begin{equation}
\begin{split}
    \mathcal A_{y}(f_{w+\Delta_w}) - \mathcal A_{y}(f_w)=\int_{w}^{w+\Delta w} \frac{\partial {\mathcal A_y}(f_u)}{\partial u} \mathrm{d} u,
\end{split}
\end{equation}
It's suffice to show that 
\begin{equation}
\frac{\partial {\mathcal A_{-1}}(f_u)}{\partial u}<\frac{\partial {\mathcal A_{+1}}(f_u)}{\partial u}<0,\quad \forall u>w^*_{+1}.
\end{equation}

Recall that in the proof of Theorem 3, we have shown \begin{equation}
\begin{split}
&\frac{\partial {\mathcal A}(f_u)}{\partial w} = \frac{1}{\sqrt d}(p_y\phi(\frac{d\eta + w}{\sqrt d}) - (1-p_y) \phi(\frac{d\eta - w}{\sqrt d}))\\
& = \frac{1}{\sqrt d} \{p_y[\phi(\frac{d\eta + w}{\sqrt d})+ \phi(\frac{d\eta - w}{\sqrt d})]-\phi(\frac{d\eta - w}{\sqrt d})\}.
\end{split}
\end{equation}
Therefore, since $p_{-1}<p_{+1}$ and $\phi(\frac{d\eta + w}{\sqrt d})+ \phi(\frac{d\eta - w}{\sqrt d}) > 0$, we have 
\begin{equation}
    \frac{\partial {\mathcal A_{-1}}(f_u)}{\partial u}<\frac{\partial {\mathcal A_{+1}}(f_u)}{\partial u}.
\end{equation}
Further, since $u>w^{*}_{+1}$, we have $\frac{\partial {\mathcal A_{+1}}(f_u)}{\partial u}<0$ as shown in the proof of Theorem 3.

Next, we prove that for $\forall w>0$,
\begin{equation}
    0<\mathcal R_{-1}(f_{w+\Delta_w}) - \mathcal R_{-1}(f_w) < \mathcal R_{+1}(f_{w+\Delta_w}) - \mathcal R_{+1}(f_w).
\end{equation}
Similarly, it suffice to show
\begin{equation}
    0<\frac{\partial {\mathcal R_{-1}}(f_u)}{\partial u}<\frac{\partial {\mathcal R_{+1}}(f_u)}{\partial u},\quad \forall u>0.
\end{equation}

Recall the expression (\ref{R}), we have
\begin{equation}
    \mathcal R_y =  p_{y}\Phi(\frac{-d\eta + w}{\sqrt d}) + (1-p_y) \Phi(\frac{-d\eta - w}{\sqrt d}),\\
\end{equation}
hence
\begin{equation}
\begin{split}
    &\frac{\partial \mathcal R_y(f_w)}{\partial w} = \frac{1}{\sqrt d}
    \{p_y\phi(\frac{-d\eta + w}{\sqrt d}) - (1-p_y)\phi(\frac{-d\eta - w}{\sqrt d})\}\\
    &=\frac{1}{\sqrt d}\{ p_y[\phi(\frac{-d\eta + w}{\sqrt d})+\phi(\frac{-d\eta - w}{\sqrt d})] - \phi(\frac{-d\eta - w}{\sqrt d})\}\\
\end{split}
\end{equation}
Since $p_{+1}>p_{-1}$ and $\phi(\frac{-d\eta + w}{\sqrt d})+\phi(\frac{-d\eta - w}{\sqrt d})>0$, we have 
\begin{equation}
    \frac{\partial {\mathcal R_{-1}}(f_u)}{\partial u}<\frac{\partial {\mathcal R_{+1}}(f_u)}{\partial u}.
\end{equation}
Finally, as $d, \eta, w>0$, we have $(\frac{-d\eta + w}{\sqrt d})^2<(\frac{-d\eta - w}{\sqrt d})^2$ by comparing their absolute value. This indicates $\phi(\frac{-d\eta + w}{\sqrt d})>\phi(\frac{-d\eta - w}{\sqrt d})$. Also note that $p_{-1}>\frac 1 2$ and $p_{-1} > (1-p_{-1})$, we have 
\begin{equation}
 \frac{1}{\sqrt d}
    \{p_{-1}\phi(\frac{-d\eta + w}{\sqrt d}) - (1-p_{-1})\phi(\frac{-d\eta - w}{\sqrt d})\}>0,
\end{equation}
which completes our proof.


\section{More Experiments}
\label{More experiment}
Here we present additional experimental results.

\subsection{Experiment on Tiny-ImageNet}
\label{C1}
Besides CIFAR-10, we additionally compare CFA with baseline+EMA on \textbf{Tiny-ImageNet} with ResNet-18 under $\ell_\infty$-norm bound $\epsilon=4/255$.
Since the worst class robustness is extremely low and there are only 50 images for each class in the test set, we report the average of the worst-20\% class robustness. The threshold of FAWA is also set on the average robustness of these classes  on validation set.
The results in Table~\ref{tab:ti} shows that our CFA framework still outperforms baseline+EMA on Tiny-ImageNet.




\subsection{Class-wise robustness comparison}
\label{C2}
To evaluate class-wise robustness, we present a comparison between CFA and EMA on CIFAR-10, as shown in Fig.~\ref{fig:class-wise}. The results show that
CFA significantly outperforms EMA on classes \{2,3,4,6\}, while slightly dropping on classes \{7,8\}.
Compared with the improvements, the decreases are very slight. 
Moreover, the variance of class-wise robustness, which measures differences between classes,  is also lower for CFA (0.15) compared to EMA (0.17). This indicates that CFA indeed reduces the difference among class-wise robustness and improves the fairness without harming other classes. 




\subsection{Selection of $\lambda_2$}
\label{C3}
Following our analysis on the selection of the perturbation budget $\lambda_1$ for AT+CCM in Sec.~\ref{ablation}, we conduct a similar analysis on the influence of regularization budget $\lambda_2$ for TRADES + CCM  + CCR in Fig.~\ref{fig:ccr analysis}.

In Fig.~\ref{fig:ccr analysis}(a), we compare the selection of $\lambda_2$ from 0.3 to 0.7. The robustness is evaluated under PGD-10.
The base perturbation budget $\lambda_1$ of CCM is still selected as 0.3. Comparing to vanilla TRADES, our TRADES+CCM+CCR outperforms in the worst class robustness significantly, and the overall robustness is marginal higher than TRADES for $\lambda_2=0.4$, 0.5 and 0.6.

Fig.~\ref{fig:ccr analysis}(b) shows the $\beta_y$ used in the case $\lambda_2=0.4$. We can see that the hard classes use $\beta_y\approx6$, while the easy classes use higher $\beta_y$. This is consistent to our analysis on class-wise robustness under different regularization $\beta$ in Sec~\ref{regularizations}.

\begin{table}[!t]
    \centering
    \small
    \caption{Overall comparison of experiment on Tiny-ImageNet.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|cc}
    \toprule[2pt]
        \textbf{Tiny-ImageNet} & \multicolumn{2}{c|}{Best (Avg. / Worst-20\%)} & \multicolumn{2}{c}{Last (Avg. / Worst-20\%)} 
        \\
        \textbf{Method} & Clean   & AA. & Clean & AA.
        \\ \midrule[1pt]
        AT & 41.1/\textbf{16.4}
        & 20.2/3.6 & 39.4/17.3 & 14.9/1.6 \\
        AT + EMA & 
40.7/14.7 & 21.8/4.2 & 41.6/19.8 & 17.4/3.9
        \\
AT + CFA & 
\textbf{41.2}/{16.2} & \textbf{22.4}/\textbf{5.2} & \textbf{42.3}/\textbf{20.0} & \textbf{19.9}/\textbf{4.8}
        \\
        \midrule[1pt]
        TRADES & \textbf{43.2}/18.4
        & 20.9/3.7 & 42.5/18.7 & 18.8/3.4
        \\
TRADES + EMA &
41.2/19.5  & 21.6/4.1 & \textbf{43.3}/\textbf{19.8} & 19.9/3.8
        \\
TRADES + CFA&
{41.7}/\textbf{20.0} & \textbf{22.3}/\textbf{5.5} & 42.4/19.6 & \textbf{21.2}/\textbf{5.2}
        
        \\         
        \midrule[1pt]
        FAT
        & 43.6/19.2 & 19.2/2.6 & 39.7/17.8 & 14.3/1.7
        \\
        FAT + EMA &
43.4/18.6 & 21.0/4.1 & 42.9/19.9 & 17.0/2.6
        
        \\
        FAT + CFA & 
\textbf{43.7}/\textbf{19.6} & \textbf{21.6}/\textbf{4.9} & \textbf{43.6}/\textbf{21.3} & \textbf{19.1}/\textbf{3.4}
        
        \\
        \bottomrule[2pt]
        \end{tabular}}

    \label{tab:ti}
\end{table}


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.4\textwidth]{bar.png}
    \caption{Class-wise robustness comparison between TRADES+EMA and TRADES+CFA on CIFAR-10 dataset at the best checkpoint. Robustness evaluated under AutoAttack.}
    \label{fig:class-wise}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.22\textwidth]{fig/compare_ccr.png}  &  \includegraphics[width=0.22\textwidth]{fig/visualize_beta.png}
         \\
        (a) & (b)
    \end{tabular}
    \caption{Analysis on the base regularization budget $\lambda_2$. (a): Average and the worst class robustness of models trained with different $\lambda_2$ (solid) and vanilla TRADES (dotted). (b): Class-wise calibrated regularization $\beta_y$ in the training phase of $\lambda_2=0.4$.}
    \label{fig:ccr analysis}
\end{figure}