\section{Experiment}
\label{sec:experiment}
In this section, we demonstrate the effectiveness of our proposed CFA framework to improve both overall and class-wise robustness. 

\begin{table*}[!t]
    \centering
    \small
        \caption{Overall comparison of our proposed CFA framework with original methods.}
    \begin{tabular}{l|cc|cc}
    \toprule[2pt]
        & \multicolumn{2}{c|}{Best (Avg. / Worst)} & \multicolumn{2}{c}{Last (Avg. / Worst)} 
        \\
        \textbf{Method} & Clean Accuracy  & AA. Accuracy &
        Clean Accuracy  & AA. Accuracy 
        \\ \midrule[1pt]
        AT & 
        \textbf{82.3} {\scriptsize  $\pm 0.8$ } 
        /
        63.9 {\scriptsize  $\pm1.6$ }
        &  
        46.7 {\scriptsize  $\pm0.5$ }
        /
        20.1 {\scriptsize $\pm1.3$}
        &
        84.1 {\scriptsize  $\pm0.2$ }
        /
        65.1{\scriptsize  $\pm 2.4$ }
        &
        43.0 {\scriptsize  $\pm0.4$ }
        /
        15.5 {\scriptsize  $\pm1.8$ }
         \\
         AT + EMA &
        81.9 {\scriptsize  $\pm 0.3$ }
         /
        61.6 {\scriptsize  $\pm0.5$ }
        &
        49.6 {\scriptsize  $\pm0.2$ }
        /
        21.3 {\scriptsize  $\pm0.8$ }
        &
        \textbf{84.8} {\scriptsize  $\pm0.1$ }
        /
        67.7 {\scriptsize  $\pm0.7$ }
        &
        44.3 {\scriptsize  $\pm0.5$ }        
        /
        18.1 {\scriptsize  $\pm0.5$ }
        \\
        \textbf{AT + CFA} &
        80.8 {\scriptsize  $\pm0.3$ }
        /
        \textbf{64.6} {\scriptsize  $\pm0.4$ }
        &
        \textbf{50.1} {\scriptsize  $\pm0.3$ }
        /
        \textbf{24.4} {\scriptsize  $\pm0.3$ }
        &
        83.6 {\scriptsize  $\pm0.2$ }
        /
        \textbf{68.7} {\scriptsize  $\pm0.7$ }
        &
        \textbf{47.7} {\scriptsize  $\pm0.4$ }
        /
        \textbf{20.5} {\scriptsize  $\pm0.4$ }
        \\ \midrule[1pt]
        TRADES &
        \textbf{82.3} {\scriptsize  $\pm0.1$ }
        /
        \textbf{67.8} {\scriptsize  $\pm0.6$ }
        &
        48.3 {\scriptsize  $\pm0.3$ }
        /
        21.7 {\scriptsize  $\pm0.5$ }
        &
        83.9 {\scriptsize  $\pm0.3$ }
        /
        66.9 {\scriptsize  $\pm1.5$ }
        &
        46.9 {\scriptsize  $\pm0.3$ }
        /
        18.5 {\scriptsize  $\pm1.3$ }
        \\
        TRADES + EMA & 
        81.2 {\scriptsize  $\pm0.4$ }
        /
        65.0 {\scriptsize  $\pm0.7$ }
        &
        49.7 {\scriptsize  $\pm0.3$ }
        /
        24.2 {\scriptsize  $\pm0.6$ }
        &
        \textbf{84.5} {\scriptsize  $\pm0.1$ }
        /
        67.9 {\scriptsize  $\pm0.1$ }
        &
        48.3 {\scriptsize  $\pm0.2$ }
        /
        20.7 {\scriptsize  $\pm0.3$ }
        \\
        \textbf{TRADES + CFA} &
        80.4 {\scriptsize  $\pm0.2$ }
        /
        66.2 {\scriptsize  $\pm0.5$ }
        &
        \textbf{50.1} {\scriptsize  $\pm0.2$ }
        /
        \textbf{26.5} {\scriptsize  $\pm0.4$ }
        &
        83.0 {\scriptsize  $\pm0.1$ }
        /
        \textbf{68.1} {\scriptsize  $\pm0.3$ }
        &
        \textbf{49.3} {\scriptsize  $\pm0.1$ }
        /
        \textbf{21.5} {\scriptsize  $\pm0.3$ }
        \\ \midrule[1pt]

        FAT &
        84.6 {\scriptsize  $\pm0.4$ }
        /
        \textbf{69.2} {\scriptsize  $\pm 0.8$ }
        &
        45.7 {\scriptsize  $\pm0.6$ }
        /
        17.2 {\scriptsize  $\pm1.3$ }
        &
        85.4 {\scriptsize  $\pm0.2$ }
        /
        70.8 {\scriptsize  $\pm1.9$ }
        &
        42.1 {\scriptsize  $\pm0.1$ }
        /
        14.8 {\scriptsize  $\pm1.6$ }
        \\
        FAT + EMA & 
        \textbf{85.2} {\scriptsize  $\pm0.2$ }
        /
        66.7 {\scriptsize  $\pm0.6$ }
        &
        48.6 {\scriptsize  $\pm0.1$ }
        /
        18.3 {\scriptsize  $\pm0.5$ }
        &
        \textbf{85.7} {\scriptsize  $\pm0.2$ }
        /
        \textbf{71.2} {\scriptsize  $\pm0.4$ }
        &
        43.2 {\scriptsize  $\pm0.1$ }
        /
        15.7 {\scriptsize  $\pm0.7$ }
        \\
        \textbf{FAT + CFA} & 
        82.1 {\scriptsize  $\pm0.3$ }
        /
        64.7 {\scriptsize  $\pm0.9$ }
        &
        \textbf{49.6} {\scriptsize  $\pm 0.1$ }
        /
        \textbf{20.9} {\scriptsize  $\pm0.8$ }
       &
       84.3 {\scriptsize  $\pm0.1$ }
       /
       69.4 {\scriptsize  $\pm0.3$ }
       &
       \textbf{45.1} {\scriptsize  $\pm0.2$ }
       /
       \textbf{16.7} {\scriptsize  $\pm0.2$ }
        \\ \midrule[1pt]
        FRL & 
        {82.8} {\scriptsize $\pm  0.1 $}
        /
        \textbf{71.4} {\scriptsize $\pm  2.4 $}
        &
        45.9 {\scriptsize $\pm  0.3 $}
        /
        25.4 {\scriptsize $\pm  2.0 $}
        &
        \textbf{82.8} {\scriptsize $\pm  0.2 $}
        /
        72.9 {\scriptsize $\pm  1.5 $}
        &
        44.7 {\scriptsize $\pm  0.2 $}
        /
        23.1 {\scriptsize $\pm  0.8 $}
        \\
        FRL + EMA &
        \textbf{83.6} {\scriptsize  $\pm0.3$ }
        /
        69.5 {\scriptsize  $\pm0.7$ }
        &
        \textbf{46.1} {\scriptsize  $\pm0.2$ }
        /
        \textbf{25.6} {\scriptsize  $\pm0.4$ }
        &
        {81.9} {\scriptsize  $\pm0.2$ }
        /
        \textbf{74.2} {\scriptsize  $\pm0.3$ }
        &
        \textbf{44.9} {\scriptsize  $\pm0.2$ }
        / 
        \textbf{24.5} {\scriptsize  $\pm0.3$ }
        \\
        
     \bottomrule[2pt]
    \end{tabular}

    \label{tab:overall}

\end{table*}

\subsection{Experimental Setup}
We conduct our experiments on the benchmark dataset CIFAR-10~\cite{krizhevsky2009learning} using PreActResNet-18 (PRN-18)~\cite{he2016identity} model. Experiments on Tiny-ImageNet can be found in Appendix~\ref{C1}.

\noindent \textbf{Baselines.} We select vanilla adversarial training (AT)~\cite{madry2017towards} and TRADES~\cite{zhang2019theoretically} as our baselines.
Additionally, since our {Fairness Aware Weight Average (FAWA)} method is a variant of the weight average method with \textit{Exponential Moving Average (EMA)}, we include baselines with EMA as well.
For instance-wise adaptive adversarial training approaches, we include FAT~\cite{zhang2020attacks}, which adaptively adjusts attack strength on each instance. Finally, we compare our approach with FRL~\cite{xu2021robust}, the only existing adversarial training algorithm that focuses on improving the fairness of class-wise robustness.

\noindent \textbf{Training Settings.} Following the best settings in ~\cite{rice2020overfitting}, we train a PRN-18 using SGD with momentum 0.9, weight decay $5\times 10^{-4}$, and initial learning rate 0.1 for 200 epochs. The learning rate is divided by $10$ after epoch 100 and 150. All experiments are conducted by default perturbation margin $\epsilon=8/255$, and for TRADES, we initialize $\beta=6$. For the base attack strength for Class-wise Calibrated Margin (CCM), we set $\lambda_1=0.5$ for AT and $\lambda_1=0.3$ for TRADES since the training robust accuracy of TRADES is higher than AT. For FAT, we set $\lambda_1=0.7$ to avoid the attack being too weak to hard classes. Besides, we set $\lambda_2=0.5$ for Class-wise Calibrated Regularization (CCR) in TRADES. For the weight average methods, the decay rate of FAWA and EMA is set to 0.85, and the weight average processes begin at the 50-th epoch for better initialization. We draw 2\% samples from each class as the validation set for FAWA, and train on the rest of 98\% samples, hence FAWA does not lead to extra computational costs.
The fairness threshold for FAWA is set to 0.2. 

\noindent \textbf{Metrics.} We evaluate the clean and robust accuracy both in average and the worst case among classes. The robustness is evaluated by \textbf{AutoAttack (AA)}~\cite{croce2020reliable}, a well-known reliable attack for robustness evaluation.
To perform the best performance during the training phase, we adopt early stopping in adversarial training~\cite{rice2020overfitting} and present both the best and last results among training checkpoints. Further, as discussed in Sec.~\ref{fluctuate} that the worst class robust accuracy changes drastically, we select the checkpoint that achieves the highest sum of overall and the worst class robustness to report the results for a fair comparison.

\subsection{Robustness and Fairness Performance}
We implement our proposed training configuration schedule on AT, TRADES, and FAT. To evaluate the effectiveness of our approach, we conduct five independent experiments for each method and report the mean result and standard deviation.

As summarized in Table~\ref{tab:overall}, CFA helps each method achieve a significant robustness improvement both in average and the worst class at the best and last checkpoints.
Furthermore, when compared with baselines that use weight average (EMA), our CFA still achieves higher overall and the worst class robustness for each method, especially in the worst class at the best checkpoints, where the improvement exceeds 2\%. 
Note that the vanilla FAT only achieves 17.2\% the worst class robustness at the best checkpoint which is even lower than TRADES, which verifies the discussion in Sec. \ref{dis:instance} that instance-wise adaptive approaches are not helpful for robustness fairness. We also visualize and compare the robustness for each class in Appendix~\ref{C2}, which shows that CFA indeed reduces the difference among class-wise robustness and improves the fairness without harming other classes. 

We also compare our approach with FRL~\cite{xu2021robust}. However, since FRL also applies a remargin schedule, we cannot incorporate our CFA into FRL.
Therefore, we only report results of FRL with and without EMA in Table~\ref{tab:overall}.
As FRL is a variant of TRADES that applies the loss function of TRADES, we compare the results of FRL with TRADES and TRADES+CFA.
From Table~\ref{tab:overall}, we observe that FRL and FRL+EMA show only marginal progress (less than 2\%) in the worst class robustness as compared to TRADES+EMA, but at a expensive cost (about 3\%) of reducing the average performance.
As demonstrated in Sec.~\ref{margin analysis}, larger margin which is adopted in FRL mainly mitigates the robust over-fitting issue but does not bring satisfactory best performance. 
This is further confirmed by the performance of final checkpoints of FRL,
where FRL exhibits better performance in the worst class robustness.
In contrast, we calibrate the appropriate margin for each class rather than simply enlarging them, thus achieving both better robustness and fairness at the best checkpoint, \textit{i.e.}, our TRADES+CFA outperforms FRL+EMA in both average (about 4\%) and the worst class (about 1\%) robustness.


\subsection{Ablation Study}
In this section, we show the usefulness of each component of our CFA framework. Note that we still apply \textbf{AutoAttack (AA)} to evaluate robustness.

\subsubsection{Effectiveness of Calibrated Configuration}
\label{ablation}
First, we compare our calibrated adversarial configuration including CCM $\epsilon_y$ and CCR $\beta_y$ with vanilla ones for AT, TRADES, and FAT. As Table~\ref{tab:calibrated conf} shows, both the average and worst class robust accuracy are improved for all three methods by applying CCM. Besides, CCR, which is customized for TRADES, also improves the performance of vanilla TRADES. All experiments verify that our proposed class-wise adaptive adversarial configurations are effective for robustness and fairness improvement.

We also investigate the influence of base perturbation budget $\lambda_1$ by conducting 5 experiments of AT incorporated CCM with $\lambda_1$ varies from 0.3 to 0.7. The comparison is plotted in Fig.~\ref{fig:ccm analysis}(a). We can see that all models with different $\lambda_1$ show better overall and the worst class robustness than vanilla AT, among which $\lambda_1=0.5$ performs best. 
We can say that CCM has satisfactory adaptive ability on adjusting $\epsilon_k$ and is not heavily rely on the selection of $\lambda_1$. Fig.~\ref{fig:ccm analysis}(b) shows the class-wise margin used in the training phase for $\lambda_1=0.5$. We can see the hard classes (class 2,3,4,5) use smaller $\epsilon_k$ than the original $\epsilon=8/255$, while the easy classes use larger ones, which is consistent with our empirical observation on different margins in Sec.~\ref{margin analysis} and can explain why CCM is helpful to improve performance. We also present a similar comparison experiments on $\lambda_2$ for CCR in Appendix~\ref{C3}.



\subsubsection{FAWA Improves Worst Class Robustness}
Here we present the results of our Fairness Aware Weight Averaging (FAWA) compared with the simple EMA method in Table~\ref{tab:fawa}. 
By eliminating the unfair checkpoints out,  our FAWA achieves significantly better performance than EMA on the worst class robustness (nearly 2\% improvement) with negligible decrease on the overall robustness (less than 0.3\%). This verifies the effectiveness of FAWA on improving robustness fairness.


\begin{table}[!t]
    \centering
        \caption{Comparison of models with/without our class-wise calibrated configurations including margin $\epsilon$ and regularization $\beta$.}
    \begin{tabular}{l|cc}
        \toprule[1.5pt]
        \textbf{Method} & Avg. Robust & Worst Robust \\ 
        \midrule[1pt]
        AT  &  46.7 & 20.1\\
        + CCM &  \textbf{47.6} & \textbf{22.8}\\ \midrule
        TRADES & 48.3 & 21.7\\
        + CCM & 48.4 & 22.5\\
        + CCR & {48.9} & 23.5\\
        + CCM + CCR &  \textbf{49.2} & \textbf{23.8} \\ \midrule
        FAT & 45.7 & 17.2 \\
        + CCM & \textbf{46.8} & \textbf{18.9} \\
        \bottomrule[1.5pt]
    \end{tabular}

    \label{tab:calibrated conf}
\end{table}

\begin{figure}[!t]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.22\textwidth]{fig/compare_ccm.png}  &  \includegraphics[width=0.22\textwidth]{fig/visualize_eps.png}
         \\
        (a) & (b)
    \end{tabular}
    \caption{Analysis on the base perturbation budget $\lambda_1$. (a): Average and the worst class robustness of models trained with different $\lambda_1$ (solid) and vanilla AT (dotted). (b): Class-wise calibrated margin $\epsilon_k$ in the training phase of $\lambda_1=0.5$.}
    \label{fig:ccm analysis}
\end{figure}


\begin{table}[!t]
    \centering
        \caption{Comparison of simple EMA and our FAWA.}
    \begin{tabular}{l|cc}
    \toprule[1.5pt]
     Method & Avg. Robust & Worst Robust
     \\ \midrule[1pt]
        AT + EMA &  \textbf{49.6} & 21.3\\
        AT + FAWA &  49.3 & \textbf{23.1} \\ \midrule
        TRADES + EMA & \textbf{49.7} & 24.2 \\
        TRADES + FAWA & 49.4 & \textbf{25.1} \\ \midrule
        FAT + EMA & \textbf{48.6} & 18.3\\
        FAT + FAWA & 48.5 & \textbf{19.9}\\
    \bottomrule[1.5pt]
    \end{tabular}

    \label{tab:fawa}
\end{table}
