\section{Theoretical Class-wise Robustness Analysis}
In this section, we present our theoretical insights on the influence of different adversarial configurations on class-wise robustness. 

\subsection{Notations}
For a $K$-classification task, we use $f:\mathcal X\to\mathcal Y$ to denote the classification function which maps from the input space $\mathcal X$ to the output labels $\mathcal Y=\{1,2,\cdots,K\}$. For an example $x\in\mathcal X$, we use $\mathcal B(x,\epsilon) = \{x'|\|x'-x\|\le \epsilon\}$ to restrict the perturbation. 
In this paper, we mainly focus on the $l_\infty$ norm $\|\cdot\|_\infty$, and note that our analysis and  approach can be generalized to other norms similarly.

We use $\mathcal A(f)$ and $\mathcal R(f)$ to denote the clean and robust accuracy of the trained model $f$:
\begin{equation}
\begin{split}
    &\mathcal{A}(f) = \mathbb E_{( x,y)\sim\mathcal D}\ \mathbf{1}(f(x) = y ),\\
        &\mathcal{R}(f) = \mathbb E_{( x, y)\sim\mathcal D}\ \mathbf{1}(\forall x'\in\mathcal B(x,\epsilon), f(x')=y).
\end{split}
\end{equation}
We use $\mathcal A_k(f)$ and $\mathcal R_k(f)$ to denote the clean and robust accuracy of the $k$-th class respectively to analyze the class-wise robustness. 

\subsection{A Binary Classification Task}
We consider a simple binary classification task that is similar to the data model used in~\cite{tsipras2018robustness}, but the properties (hard or easy) of the two classes are different. 

\noindent \textbf{Data Distribution.}
Consider a binary classification task where the data distribution $\mathcal D$ consists of input-label pairs $(x,y)\in\mathbb R^{d+1}\times \{-1,+1\}$.
The label $y$ is uniformly sampled, \textit{i.e.,} $y\overset{\text{u.a.r.}}{\sim}\{-1,+1\}$.
For input $x=(x_1,x_2, \cdots, x_{d+1})$, let $x_1\in\{-1,+1\}$ be the \textit{robust feature}, and $x_2,\cdots, x_{d+1}$ be the \textit{non-robust features}. The robust feature $x_1$ is labeled as $x_1 = y$ with probability $p$ and $x_1=-y$ with probability $1-p$ where $0.5 \le p <1$. For the non-robust features, they are sampled from $x_2,\cdots,x_{d+1}\overset{\text{i.i.d}}{\sim}\mathcal N(\eta y, 1)$ where $\eta<1/2$ is a small positive number.
Intuitively, as discussed  in~\cite{tsipras2018robustness}, $x_1$ is robust to perturbation but not perfect (as $p<1$), and $x_2,\cdots, x_{d+1}$ are useful for classification but sensitive to small perturbation. In our model, we introduce some differences between the two classes by letting the probability of $x_1=y$ correlate with its label $y$.
Overall, the data distribution is 
\begin{equation}
    x_1 = \begin{cases}
    + y,& \text{w.p.}\ p_y\\
    - y,& \text{w.p.}\ 1-p_{y}
    \end{cases}  \text{and} \quad
    x_2, \cdots, x_{d+1}\overset{\text{i.i.d}}{\sim}\mathcal N(\eta y, 1).
\end{equation}
We set $p_{+1}>p_{-1}$ in our model.
Therefore, the robust feature $x_1$ is more reliable for class $y=+1$, while for class $y=-1$, the robust feature $x_1$ is noisier and their classification depends more on the non-robust features $x_2, \cdots, x_{d+1}$.

\noindent \textbf{Hypothesis Space.} Consider a SVM classifier (without bias term) $f(x) = \text{sign}(w_1x_1+w_2x_2+\cdots+ w_{d+1}x_{d+1})$. 
For the sake of simplicity,
we assume  $w_1,w_2\ne 0$, and $w_2=w_3=\cdots =w_{d+1}$ since $x_2,\cdots, x_{d+1}$ are equivalent. Then, let $w = \frac{w_1}{w_2}$, the model can be simplified as $f_w(x) = \text{sign}(x_1+\frac{x_2+ \cdots +x_{d+1}}{w})$.
 Without loss of generality, we further assume $w>0$ since $x_2,\cdots,x_{d+1}\sim\mathcal N(\eta y, 1)$ tend to share the same sign symbol with $y$.

\subsection{Theoretical Insights}
\label{sec:theoretical insight}
\noindent \textbf{Illustration Example.}
An example of the data distribution for the case  $d=1$ is visualized in Fig.~\ref{fig:toy model}(a). The data points for class $y=+1$ are colored red and for $y=-1$ are colored blue. We can see that the robust feature $x_1$ of class $y=-1$ seems to be noisier than $y=+1$, since the frequency of blue dots appearing on the line $x_1=1$ is higher compared to the frequency of red dots appearing on the line $x_1=-1$, with
$p_{+1}>p_{-1}$. Therefore, class $y=-1$ might be more difficult to learn. Furthermore, we plot the clean and robust accuracy of the two classes of $f_w$ for different $w$ in Fig.~\ref{fig:toy model}(b).
Implementation details of this example can be found in Appendix~\ref{illustration Example}. The parameter $w$ can be regarded as the strength of adversarial attack in adversarial training, since larger $w$ indicates the classifier $f_w$ bias less weight on non-robust features $w_2,\cdots,w_{d+1}$ and pay more attention on robust feature $w_1$. 
We can see that as $w$ increases, the clean accuracy of $y=-1$ drops  significantly faster than $y=+1$, but the robustness improves slower. We formally prove this observation in the following. \\


\begin{figure}[!t]
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.22\textwidth]{fig/toy_model.png} &
        \includegraphics[width=0.22\textwidth]{fig/test_cdf.png}
        \\
        (a) & (b) 
    \end{tabular}
    \vspace{-0.15 in}
    \caption{An visualization of the toy model for the case $d=1$. (a): Sampled data from the distribution. Red dots are labeled $y=+1$ and blue dots are labeled $y=-1$. (b): Clean and robust accuracy of the two classes. Solid lines indicate robust accuracy and dotted lines indicate clean accuracy.}
    \label{fig:toy model}
\end{figure}

\noindent \textbf{The Intrinsically Hard Class.}
First we formally distinct class $y=-1,+1$ as the \textit{hard} and \noindent \textit{easy} class in Theorem~\ref{theorem:difficulty}.
\begin{theorem} 
For any $w>0$ and the classifier $f_w=\text{sign}(x_1+\frac{x_2+\cdots+x_{d+1}}{w})$, we have $\mathcal A_{+1}(f_w) > \mathcal A_{-1} (f_w)$ and  $\mathcal R_{+1}(f_w)>R_{-1}(f_w)$.
\label{theorem:difficulty}
\end{theorem}
Theorem~\ref{theorem:difficulty} shows that the class $y=-1$ is more difficult to learn than class $y=+1$ both in robust and clean settings. This reveals the potential reason why some classes are intrinsically difficult to learn in the adversarial setting, that is, their robust features are less reliable.\\

\noindent \textbf{Relation Between $w$ and Attack Strength.}
Consider the model is adversarially trained with perturbation margin $\epsilon$. The following Theorem~\ref{theorem:eps} shows using larger $\epsilon$ enlarges $w$.
\begin{theorem} 
\label{theorem:eps}
    For any $0\le\epsilon\le\eta$, let $w^* = \arg\max\limits_w \mathcal R(f_w)$ be
    the optimal parameter for adversarial training with perturbation bound $\epsilon$, then $w^*$  is monotone increasing at $\epsilon$.
\end{theorem}
Theorem~\ref{theorem:eps} bridges the gap between model parameter and attack strength in adversarial training. Next, we can implicitly investigate the influence of attack strength on class-wise robustness by analyzing the parameter $w$. \\


\noindent \textbf{Impact of Attack Strength on Class-wise Robustness.}
Here, we demonstrate how adversarial strength influences class-wise clean and robust accuracy. 

\begin{theorem}
Let $w_y^* = \arg\max\limits_w \mathcal A_y(f_w)$ be the parameter for the best clean accuracy of class $y$, then $w_{+1}^*>w_{-1}^*$.
\label{theorem:best}
\end{theorem}
Theorem~\ref{theorem:best} shows that the clean accuracy of the hard class $y=-1$ reaches its best performance \textit{earlier} than $y=+1$. In other words, $\mathcal A_{-1}(f_w)$ starts dropping earlier than $\mathcal A_{+1}(f_w)$. As the model further distracts its attention from its clean accuracy to robustness by increasing the parameter $w$, the hard class $y=-1$ losses more clean accuracy yet gains less robust accuracy as shown in Theorem~\ref{theorem:compare}.

\begin{theorem} Suppose $\Delta_w>0$, then for $\forall w>w_{+1}^*
$, $\mathcal A_{-1}(f_{w+\Delta_w}) - \mathcal A_{-1}(f_w) < \mathcal A_{+1}(f_{w+\Delta_w}) - \mathcal A_{+1}(f_w)<0$, and for $\forall w>0$, $0<\mathcal R_{-1}(f_{w+\Delta_w}) - \mathcal R_{-1}(f_w) < \mathcal R_{+1}(f_{w+\Delta_w}) - \mathcal R_{+1}(f_w)$. 
\label{theorem:compare}
\end{theorem}

The proof of the theorems can be found in Appendix~\ref{proof}.
In this section, we demonstrate the unreliability of robust features is
a possible explanation for the intrinsic difficulty in learning some classes.
Then, by implicitly expressing the attack strength with parameter $w$, we analyze how adversarial configuration influence class-wise robustness. Theorems~\ref{theorem:best} and \ref{theorem:compare} highlight the negative impact of strong attack on the hard class $y=-1$. 
