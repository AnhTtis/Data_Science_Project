% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mathtools}
% \usepackage{arydshln}

\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{bbm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\usepackage{float}

\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bm}
\usepackage{etoolbox}
\usepackage{icomma} % for comma
\usepackage{array}
\usepackage{tabulary}
\usepackage[table]{xcolor} % for rowcolor
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{adjustbox}

% \usepackage[caption=false,font=footnotesize]{subfig}

% \usepackage[sort,nocompress]{cite} %sort inline

% === for table caption
\usepackage{caption}
\captionsetup[table]{format=plain,labelformat=simple,labelsep=period}%
% \usepackage{float}  % table follow text by [H]
% === for figure subcaption
\usepackage{subcaption}

% ======= ADDED commands

\usepackage{arydshln}
% \usepackage{tabulary}
% \usepackage[table]{xcolor}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\definecolor{lightblue}{HTML}{6495ed}
\definecolor{lightred}{HTML}{F19C99}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\lightblue}[1]{\textcolor{lightblue}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\bbf}[1]{\lightblue{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}
\newcommand{\obf}[1]{\textcolor{orange}{\bf{\fn{(#1)}}}}
\definecolor{graytablerow}{gray}{0.6}
\newcommand{\grow}[1]{\textcolor{graytablerow}{#1}}

\definecolor{revised_color}{HTML}{0066CC}
\definecolor{revised_color_PKY}{HTML}{FF2E82}
\definecolor{revised_color_SH}{HTML}{007FFF}
\newcommand{\YKL}[1]{\textcolor{red}{#1}}
\newcommand{\ZJM}[1]{\textcolor{orange}{#1}}
\newcommand{\SH}[1]{\textcolor{revised_color_SH}{#1}}
\newcommand{\PKY}[1]{\textcolor{revised_color_PKY}{#1}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\hypersetup{colorlinks, citecolor=blue}%teal
%\hypersetup{colorlinks, citebordercolor=black}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% 提高视力障碍可读性
\usepackage[accsupp]{axessibility}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{3} % *** Enter the CVPR Paper ID here
\def\confName{OmniCV}
\def\confYear{2023}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
    \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
    \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation
%
}

\author{Hao Shi$^{1,5,}$\thanks{The first two authors contribute equally to this work.}\, ,
~~Yu Li$^{2,*}$,
~~Kailun Yang$^{3,}$\thanks{Corresponding author (e-mail: {\tt kailun.yang@hnu.edu.cn, wangkaiwei@zju.edu.cn}).}\, ,
~~Jiaming Zhang$^{2,4}$,
~~Kunyu Peng$^{2}$,
~~Alina Roitberg$^{2}$,\\
Yaozu Ye$^{1}$,
~~Huajian Ni$^{5}$,
~~Kaiwei Wang$^{1,\dagger}$,
~~Rainer Stiefelhagen$^{2}$\\
\normalsize
$^{1}$Zhejiang University
~~$^{2}$Karlsruhe Institute of Technology
~~$^{3}$Hunan University
~~$^{4}$University of Oxford
~~$^{5}$Supremind
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
\input{Tex_content/abstract_cam}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
%
\begin{figure}[!t]
   \centering
   \includegraphics[width=0.95\linewidth]{Figures/teaser.pdf}
   \vspace{-1em}
   \caption{\emph{Illustrations of our proposed \textbf{Fisheye Semantic Completion}.} (a) Input single-view fisheye image. (b)(c) Fisheye semantic completion result: our model jointly predicts texture, structure and object categories for each pixel beyond the fisheye vision.}
   \label{fig:teaser}
  \vspace{-1.0em}
\end{figure}

Benefiting from a larger field-of-view (FoV), fisheye cameras have been widely used in autonomous driving and mobile robots~\cite{yogamani2019woodscape,qian2022survey_fisheye,kumar2023surround_survey,gao2022review_panoramic_imaging,sekkat2020omniscape}. However, due to the special optical design of the fisheye camera, there are invalid black areas at the edge of the image plane. Interestingly, humans have a natural ability to infer complete semantic information from partial visual observations~\cite{pessoa2003filling,lin2022neural} (\eg, a partially occluded car) to navigate and interact in the real world. Similarly, for an ego-view agent, the ability to estimate the full field-of-view of a given scene is beneficial for mid-level tasks such as obstacle avoidance~\cite{kim2015rear}, while perceiving semantic concepts is a prerequisite for complex cognitive tasks such as high-level scene understanding, planning the next step or answering questions about the space~\cite{guerrero2020s, cartillier2021semantic}.

%
With this motivation, our goal is to build a model that can simultaneously complete the missing image areas and generate predictions for semantic object categories from a single-shot fisheye image in an end-to-end manner.
We refer to this novel task as ``Fisheye Semantic Completion'', with an overview of the proposed problem given in Fig. \ref{fig:teaser}.
%
Our key idea is grounded by the  observation  that the distribution of pixel values of an entity within an image is tightly coupled to its semantic label.
Therefore, the two problems of outpainting the content outside the fisheye camera's FoV and the semantics of objects are strongly conjugated, which we believe is  vital for achieving good performance in both tasks.
In other words, if we know the semantic categories of an incomplete object, we can predict its pixel pattern on the image plane even without direct observation (\eg, seeing a tree trunk appearing in the FoV and then inferring the the presence of tree canopy outside). 
%
Conversely, having a complete observation of an object can help us recognize its semantic class more accurately.

%
To achieve this goal, we must overcome several key challenges: First, how to effectively take advantage of the strong coupling characteristics of these two sub-tasks (\ie fisheye outpainting and segmentation) to realize a win-win situation? Second, since existing fisheye semantic segmentation datasets cannot provide images and semantic ground truth outside the FoV, how can we obtain fisheye beyond-FoV training data with complete annotations?

%
To address the first challenge,  we propose \textbf{FishDreamer}, which benefits from the successful Visual Transformer (ViT) structure as the backbone, and integrates a novel Polar-aware Cross Attention module (PCA) to enhance the flow of visual cues between two sub-tasks.
Specifically, PCA takes into account the unique polar distribution and distortion patterns of fisheye cameras, considers the heterogeneity of different polar coordinate locations when querying relevant visual features, and leverages the rich semantic context from the semantic head to guide the outpainting head in hallucinating semantically continuous and plausible content outside the fisheye FoV.
As for the data challenge, we leverage the popular Cityscapes~\cite{cordts2016cityscapes} and KITTI360~\cite{liao2022kitti} semantic segmentation datasets via pinhole-to-fisheye projection and derive the new CityScapes-BF and KITTI360-BF as beyond-FoV benchmark variants, therefore enabling training and evaluation of fisheye semantic completion.

Extensive experiments demonstrate that the proposed FishDreamer, which jointly learns semantics and content outside the fisheye FoV, outperforms approaches that address the two sub-tasks in isolation. 
The proposed PCA module, which focuses on the natural polarity distribution of the fisheye image and extracts visual cues from semantic priors, significantly improves performance.
On the derived CityScapes-BF benchmark, FishDreamer achieves state-of-the-art performance with a mIoU of $54.54\%$ and a peak-SNR of $25.05dB$, a $0.42dB$ performance gain from the best published result ($24.63dB$).
FishDreamer also surpasses alternative approaches on KITTI360-BF as it hallucinates more realistic content in the blind area of the fisheye and gives clearer and sharper segmentation results.


In summary, we deliver the following contributions:
\begin{compactitem}
    \item We raise the new Fisheye Semantic Completion task, which extends beyond fisheye vision and enables  outpainting and semantic segmentation of the full scene.
    
    \item We establish the CityScapes-BF and KITTI360-BF benchmarks and validate existing models on this  new fisheye semantic completion track.

    \item We propose FishDreamer, which utilizes a novel Polar-aware Cross Attention (PCA) module to effectively guide fisheye outpainting using semantic context.

    \item  Extensive experiments demonstrate that the proposed FishDreamer outperforms alternative approaches that address the sub-tasks separately.
\end{compactitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related_work}
\input{Tex_content/related_work_cam}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:methodology}
\subsection{Overview}
\begin{figure}[!t]
   \centering
   \includegraphics[width=0.925\linewidth]{Figures/overview.pdf}
  \vspace{-1em}
   \caption{\emph{FishDreamer: Fisheye semantic completion transformer.} Taking a single fisheye image as input, the model predicts pixel patterns and object labels beyond fisheye vision. Swin~\cite{liu2021swin} is used as the backbone. The two sub-task heads interact each other via the proposed Polar-aware Cross Attention module bidirectionally.}
   \label{fig:overview}
  \vspace{-1.5em}
\end{figure}

\input{Tex_content/3.1}

\subsection{Feature Extractor}
\label{sec:feature_extractor}
\input{Tex_content/feature_extractor}

\subsection{Outpainting}
\label{sec:outpainting}
\input{Tex_content/3.2}

\subsection{Segmentation}
\label{sec:segmentation}
\begin{figure}[!t]
   \centering
   \includegraphics[width=1.0\linewidth]{Figures/upernet-v3.png}
  %
   \caption{\emph{UPerNet with PCA.} Hierarchical UPerNet with extra information from the image completion head.}
   \label{fig:upernet}
  \vspace{-1.0em}
\end{figure}
\input{Tex_content/3.3}


\subsection{Polar-aware Cross Attention}
\label{sec:polar_aware_cross_attention}
\begin{figure*}[!t]
   \centering
   \includegraphics[width=1.0\linewidth]{Figures/pca.png}
  %
   \caption{\emph{Polar-aware Cross Attention module (PCA)}. Given the feature maps from two sub-task heads, PCA explicitly limits the polarity distribution in each patch by introducing $N$ ring-shape masks, and exploit the multi-head attention to query polar-aware correlated feature from coupled sub-task feature map. Note this behavior can be either unidirectional or bidirectional.}
   \label{fig:pca}
%
\end{figure*}

%

\input{Tex_content/PCA_cam}

%
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\linewidth]{Figures/city_statis-v5.pdf}
  \vspace{-2em}
  \caption{\emph{Distributions of Cityscapes~\cite{cordts2016cityscapes}, Cityscapes-BF, KITTI360~\cite{liao2022kitti}, and KITTI360-BF in terms of class-wise pixel percentage across the datasets.} We use the logarithmic scaling of the vertical axis and insert the pixel frequency above the bar.}
  \label{fig:city_statis}
  \vspace{-1.0em}
\end{figure*}
\subsection{Training}
\input{Tex_content/3.5_cam}

%
\begin{table}[!t]
\renewcommand{\thetable}{1}
    \begin{center}
        \caption{\emph{The training and validation sets splits of the derived Cityscapes-BF and KITTI360-BF.}}
        \label{tab:fov_expansion}
        \vspace{-1.0em}
        \input{Tables/dataset_statatistic-v2}
        \vspace{-2.25em}
    \end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
\input{Tex_content/dataset_cam}

\subsection{Implementation Details}
FishDreamer was implemented in PyTorch and trained for fisheye semantic completion end-to-end  on an NVIDIA RTX 3090 graphics card. Backbone weights are initialized from  models pretrained on ImageNet~\cite{deng2009imagenet}.
We choose the AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $2.5{\times}10^{-4}$, coefficients $\beta_{1}{=}0.9$, $\beta_{2}{=}0.999$, and weight decay $\eta{=}10^{-2}$. 
We use batch size of $8$ and train our model for $50$/$70$ epochs for the ablation experiments and the final model experiments respectively.
%
We adopt Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Fréchet Inception Distance (FID) to  evaluate the extrapolation performance, while mean Intersection over Union (mIoU) is used as the semantic completion metric.

%
\begin{table}[!t]
\renewcommand{\thetable}{2}
    \begin{center}
        \caption{\emph{Quantitative comparison of fisheye semantic completion on Cityscapes-BF dataset.} * indicates results from~\cite{liao2022fisheyeex}.}
        \label{tab:cityscapes_result}
        \vspace{-1.0em}
        \input{Tables/city_compare}
        \vspace{-2em}
        % \vspace{-1em}
    \end{center}
\end{table}

\subsection{Results}
%
\begin{figure*}[!t]
   \centering
   \includegraphics[width=1.0\linewidth]{Figures/compare-v3.pdf}
   \vspace{-1.5em}
   \caption{\emph{Qualitative comparisons with alternative methods on Cityscapes-BF and KITTI360-BF. We compare the proposed FishDreamer with our simple baseline since it already surpasses previous works~\cite{liao2022fisheyeex} in terms of PSNR without the help of PCA. We also compare with the fisheye semantic segmentation method FisheyeSeg~\cite{ye2020universal}. Best viewed in color.}}
   \label{fig:compare}
  \vspace{-1.0em}
\end{figure*}

%
\begin{table}[!t]
\renewcommand{\thetable}{3}
    \begin{center}
        \caption{\emph{Quantitative comparison of fisheye semantic completion on KITTI360-BF dataset.} * indicates results from~\cite{liao2022fisheyeex}.}
        \label{tab:kitti_result}
        \vspace{-1.0em}
        \input{Tables/kitti_compare}
        \vspace{-3em}
    \end{center}
\end{table}

\noindent \textbf{Cityscapes-BF results.} In Table~\ref{tab:cityscapes_result}, we conduct comparison between the state-of-the-art methods for image completion, \eg, FisheyeEX~\cite{liao2022fisheyeex}, those for semantic segmentation, \eg, SegFormer~\cite{xie2021segformer}, and the proposed FishDreamer approach. Compared to the previous FisheyeEX, FishDreamer obtains better results on the sub-task of image completion, which are respective $25.05$, $0.93$, and $30.14$ in PSNR, SSIM, and FID. 
On the sub-task of semantic completion,  FishDreamer obtains the best score of $54.54\%$ in mIoU, yielding a large performance boost in ${+}7.23\%$ as compared to the previous fisheye image semantic segmentation model FisheyeSeg~\cite{ye2020universal}. Besides, compared to the methods for general image semantic segmentation, such as Swin~\cite{liu2021swin} and SegFormer~\cite{xie2021segformer}, our FishDreamer model also yields considerable improvement.
As the completion involves severe distortions and demands inferring semantics beyond the FoV, the segmentation transformers deliver clearly lower scores compared to their performances on standard segmentation benchmarks.
%
Yet, the state-of-the-art performance achieved in both tasks of FishDreamer demonstrates that outpainting the content outside the fisheye camera's FoV and completing the semantics of objects are conjugated. In other words, by using a single end-to-end model for both problems we can effectively leverage this complementary information, yielding clear benefits for both tasks.

\noindent \textbf{KITTI360-BF results.} 
%
As shown in Table~\ref{tab:kitti_result}, the results on the KITTI360-BF dataset are compared among fisheye image completion methods, semantic completion methods, and our two fisheye semantic completion approaches. Compared to image completion methods like Boundless~\cite{teterwak2019boundless} and FisheyeEX~\cite{liao2022fisheyeex}, our simple baseline achieves the second-best result ($22.38$) with significant improvement. Moreover, our FishDreamer model attains the best results in the image completion sub-task, with respective scores of $22.51$, $0.91$, and $27.89$ in PSNR, SSIM, and FID. In addition to image completion, our FishDreamer achieves the best semantic completion performance with an mIoU of $43.57$. These results and improvements further demonstrate the promising performance of our proposed method, which effectively couples both sub-tasks in an end-to-end manner.
%

\subsection{Ablation Studies}

%
\begin{table}[!t]
\renewcommand{\thetable}{4}
    \begin{center}
        \caption{\emph{Ablation of different backbones.}}
        \label{tab:ab_backbone}
        \vspace{-1.0em}
        \input{Tables/ab_backbone}
        \vspace{-1.95em}
    \end{center}
\end{table}

\noindent \textbf{Analysis of the backbones.}
To investigate the effect of model backbones, we perform ablation study of FishDreamer with three different methods, including Swin~\cite{liu2021swin}, MiT~\cite{xie2021segformer}, and Conformer~\cite{peng2021conformer}. As shown in Table~\ref{tab:ab_backbone}, each method has two model scales. The best and the second best results are marked with bold and underline, respectively. Our method based on Conformer~\cite{peng2021conformer} models have better performance on the sub-task of fisheye image completion, and the model based on Conformer-S  achieves respective $24.57$ and $0.9237$ scores in PSNR and SSIM, but obtains sub-optimal performance in semantic completion, yielding only $49.81$ in mIoU. Compared to the Conformer models, based on MiT-B0 and -B2~\cite{xie2021segformer} backbones that are specific for semantic segmentation, our method achieves better results on the sub-task of semantic completion with $44.83$ and $50.72$ scores in mIoU. The MiT-based models have a smaller number of parameters ($9.3$M and $30.9$M), however, the performance on the fisheye image completion is  lower as compared to the ones using Conformer counterparts. 
%
To achieve a balance between the two sub-tasks, the Swin-based~\cite{liu2021swin} backbone  strikes a good balance between the fisheye image completion performance and the semantic completion quality.
Our method based on Swin-S backbone obtains the best semantic completion result with $54.01$ in mIoU, while it provides the second best result on fisheye image completion with $24.46$ in PSNR.
%
This result aligns with our observation that a backbone with superior semantic completion capabilities can provide complementary advantages for the image completion sub-task.
%
\begin{table}[!t]
\renewcommand{\thetable}{5}
    \begin{center}
        \caption{\emph{Analysis of Polar-aware Cross Attention.}}
        \label{tab:ab_pca}
        \vspace{-1.0em}
        \input{Tables/ab_pca}
        \vspace{-1.95em}
    \end{center}
\end{table}

\noindent \textbf{Analysis of the Polar-aware Cross Attention (PCA).}
%
The  PCA mechanism is vital for the fisheye semantic completion task.
%
To examine the impact of mask selection and direction, we perform an ablation study of PCA  in Table~\ref{tab:ab_pca}.
Without using the polar mask, the three ways of semantic-to-outpainting (S2P), outpainting-to-semantic (P2S), and Bi-direction achieve respective $53.73$, $54.17$, and $53.89$ in mIoU of semantic completion task, and $24.93$, $24.56$, and $24.82$ in image completion PSNR.
When using the polar mask and Bi-direction method, we further ablate the mask generation with different mask numbers in $\{2, 4, 8\}$. As shown in Table~\ref{tab:ab_pca}, our PCA module is robust to different mask numbers, since each of them obtains comparable performance.
%
Nonetheless, we found that using $4$ polar masks could provide better results on both sub tasks, yielding $54.21$, $25.01$, and $0.9257$ in mIoU, PSNR, and SSIM, respectively.
%
This analysis demonstrates that our proposed PCA module is effective in simultaneously addressing semantic understanding and image completion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\input{Tex_content/conclusion}

%------------------------------------------------------------------------

%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
