\noindent \textbf{Loss function.} 
%
Multiple well-established loss functions are employed to simultaneously ensure the accuracy of blind-area prediction and complete-FoV semantic segmentation.
Considering the full-scene image completion task of the fisheye image, we make use of a high receptive field perceptual loss, \ie, $ \mathcal{L}_{hp}$, an adversarial loss, \ie,  $\mathcal{L}_{adv}$, a reconstruction loss, \ie, $\mathcal{L}_{rec}$  and a feature matching loss, \ie, $\mathcal{L}_{fm}$.
Next, we discuss these loss functions in detail.

First, leverage the high receptive field perceptual loss~\cite{suvorov2022resolution} $\mathcal{L}_{hp}$, which calculates the difference between the feature maps of the predicted results and the target images.
%
This loss function does not require an exact reconstruction workflow, which is particularly suitable for our case when addressing the challenge of limited information in the blind area.
$\mathcal{L}_{hp}$ can be calculated as:
\begin{equation}
    \mathcal{L}_{hp}(z_p, z_t)=\mathcal{M}\left(D(\phi(z_p),\phi(z_t))^{2}\right),
\end{equation}
%
where $D(\cdot)$ denotes an element-wise distance function (mean-squared error) loss and $\mathcal{M}$ denotes the sequential two-stage mean operation.
The $\mathcal{L}_{hp}$ calculates the distance between the extracted features of the prediction ($z_p$) and the ground truth ($z_t$).
%
$\phi$ denotes dilated convolutions. 
$\mathcal{L}_{hp}$ does not require an exact reconstruction, which is a very good property in our case due to the lack of information of the blind area. 

Second, an adversarial loss is used to ensure the preservation of local details. A discriminator, $D_{\xi}(\cdot)$, is used to distinguish between "real" and "fake" patches. The visible parts of the built images are marked as real, while patches that intersect with the blind area are marked as fake. We then compute the non-saturating adversarial loss $\mathcal{L}_{adv}$ as:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{D}=-\mathbb{E}_{x}[ {\left[\log D_{\xi}(x)\right]-\mathbb{E}_{x, m}\left[\log D_{\xi}(\hat{x}) \odot M\right] } \\
 -\mathbb{E}_{x, m}\left[\log \left(1-D_{\xi}(\hat{x})\right) \odot(1-M)\right],    
\end{aligned}
\end{equation}
\begin{equation}
     \mathcal{L}_{G}=-\mathbb{E}_{x, m}\left[\log D_{\xi}(\hat{x})\right],
\end{equation}
\begin{equation}
    L_{adv}=\operatorname{sg}_{\theta}\left(\mathcal{L}_{D}\right)+\operatorname{sg}_{\xi}\left(\mathcal{L}_{G}\right) \rightarrow \min _{\theta, \xi},
\end{equation}
where $x$ denotes a sample from dataset, $\hat{x}$ is the outpainting prediction, and $M$ denotes the circular masks to synthesized fisheye images. $\operatorname{sg}_{k}$ stops gradient \wrt \textit{k}, and $\mathcal{L}_{adv}$ is the joint adversarial loss which needs to be optimized. 

Third, $L_{rec}$ measures the L1 distance between the outpainted  image generated by FishDreamer and the ground truth, which can be calculated via the following equation:
\begin{equation}
    \mathcal{L}_{rec}=\Vert\hat{x}-x\Vert_1.
\end{equation}
Then, $L_{fm}$~\cite{wang2018high} is leveraged to denote a discriminated-based perceptual loss, which stabilizes training and improves the performance.
%

Finally, for the semantic completion we employ further two additional loss functions, \ie,  a cross entropy loss $ \mathcal{L}_{ce}$ and a Lov{\'a}sz-softmax loss $ \mathcal{L}_{lovasz}$~\cite{berman2018lovasz}.

The final FishDreamer loss $\mathcal{L}$ becomes:
%
\begin{equation}
\begin{aligned}
    \mathcal{L} &= \lambda_{adv} \cdot L_{adv} + \lambda_{hp} \cdot \mathcal{L}_{hp} + \lambda_{fm} \cdot \mathcal{L}_{fm} \\
    &+ \lambda_{rec} \cdot \mathcal{L}_{rec} + \lambda_{ce} \cdot \mathcal{L}_{ce} + \lambda_{lovasz} \cdot {L}_{lovasz},
\end{aligned}
\end{equation}
which is the weighted sum of the above losses. We empirically set $\lambda_{adv}{=}20, \lambda_{hp}{=}60, \lambda_{fm}{=}200, \lambda_{rec}{=}20, \lambda_{ce}{=}30$, and $\lambda_{lovasz}{=}10$ in all the experiments.





