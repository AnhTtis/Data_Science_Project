As image outpainting and semantic segmentation tasks are tightly intertwined, we design the novel Polar-aware Cross Attention  (PCA) module to encourage the information flow between the two heads.
%


Given the two feature maps from the semantic segmentation head ($z_{s}$) and the image completion head ($z_{c}$), PCA initially constrains the polar distribution within each patch using a polar mask obtained from a newly designed polar-mask generator. This generator creates polar masks with varying quantities and radius.
Let the total number of the generated masks be $N_{mask}$ and the set of polar masks be $M =\{M_{i}~|~ i ~\in~ [0, N_{mask}]\}$. A linear projection layer is employed for the masked feature map of each sub-task. The projected and masked patch partitions for both the feature maps from the semantic segmentation- ($z_{s}^{}$) and the image completion heads ($z_{c}^{}$) can be computed as follows:
\begin{equation}
  z_{s}^{*}, z_{c}^{*} = L_{s}(M\odot z_{s}),  L_{c}(M\odot z_{c}),
\end{equation}
where the $L_{s}$ and $L_{c}$ indicate the linear projection layers of the semantic segmentation branch and the image completion branch, respectively.
%
A multi-head cross-attention mechanism is utilized to integrate the focuses from the image completion branch into the semantic segmentation branch. To this intent, linear projection layers $P_{Q}$, $P_{K}$, and $P_{V}$ are employed to compute the necessary query, key, and value components for the MSA. The resulting merged feature map is then passed through an additional bottleneck layer to obtain the combined feature map. This final merged feature map is subsequently added to $z_{s}^{*}$ to produce the ultimate output.
This workflow can be formalized as:
\begin{equation}
     \centering
     z_{pca} = z_{s}^{*}+C_{BN}(SA(P_{Q}(z_{s}^{*}),P_{K}(z_{c}^{*}), P_{V}(z_{c}^{*})),
\end{equation}
where $z_{pca}$ denotes the final mixed feature output of the proposed PCA mechanism, the $C_{BN}$ denotes the bottleneck layer and SA denotes the self-attention mechanism which is calculated as $SA(Q, K, V){=}Softmax(QK^{T}/\sqrt{{\rho}})V$ and $\rho$ denotes the scale factor~\cite{dosovitskiy2020image}.
 
 
In comparison to the previous FishFormer work~\cite{yang2022fishformer}, which solely focuses on one task, \ie, fisheye distortion correction, our model (shown in Fig.~\ref{fig:pca}) simultaneously addresses two critical tasks for autonomous driving, namely semantic segmentation and image completion, which involves predicting the unseen regions of fisheye images by considering the entire scene. 
%
This blind zones information is crucial for autonomous vehicles in order to minimize potential risks, such as route planning and risk alerting within the blind zones of fisheye sensors.
%
Our PCA mechanism is employed  at the end of the model, merging feature maps from both the semantic segmentation and image completion heads.
In contrast, the Layer Attention Mechanism (LAM) proposed by FishFormer, is integrated between every two transformer blocks, which is less efficient in terms of the number of attention blocks used. Additionally, while FishFormer encodes different annular slices, we force each token to encode features of a specific polar distribution. Tokens lacking valid features are discarded to enhance computational efficiency.  

%