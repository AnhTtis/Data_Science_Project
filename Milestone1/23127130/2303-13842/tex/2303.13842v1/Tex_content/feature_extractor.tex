We begin by describing our feature extraction backbone, the Swin Transformer~\cite{liu2021swin}, which facilitates informative hierarchical feature learning and has been proven very effective, \eg, in image and video classification~\cite{liu2021swin, liu2022video}, activity recognition~\cite{peng2022transdarc} and vanilla semantic segmentation~\cite{lin2022ds}. 
Similar to other transformer-based models Swin leverages self-attention~\cite{vaswani2017attention}, but also employs a non-overlapping shifted window partitioning mechanism which enhances efficiency by focusing on the generated windows while preserving cross-window communication capabilities. 

We utilize a four-stage Swin Transformer as our feature extraction backbone.
In the first stage, multiple non-overlapping image patches are generated. As we address outpainting and semantic segmentation using 2D image data as input, a 2D shifted window pipeline is employed, operating within the 2D spatial domain.
Assuming the 2D spatial dimensions of the input image are $[H, W]$ and the shifted window size is chosen as $[N, N]$, a total of $\frac{H}{N}\times\frac{W}{N}$ patches are extracted using the aforementioned window partitioning technique. Next, these patches are projected from $\textsc{R}^{3}$ to $\textsc{R}^{C}$ using a linear projection layer. Next, we will describe further details of the Swin Transformer blocks .

The Swin Transformer block leverages its own  Shifted-Window based Multi-head Self-Attention (SW-MSA) mechanism, as opposed to the standard Multi-head Self-Attention (MSA) found in ViT~\cite{dosovitskiy2020image}. This approach mitigates the limitations of the vanilla ViT structure, specifically concerning the lack of cross-window connections and restricted model capacity. The workflow of the Swin Transformer block can be described as:
\begin{equation}
\begin{split}
    \hat{z}_{l-1} = SW-MSA(LN(z_{l-1})),\\
    z_{l} = MLP(LN(\hat{z}_{l-1}) + \hat{z}_{l-1},
\end{split}
\end{equation}
where the LN indicates layer normalization, MLP is multi layer perception with GELU nonlinearity, $l$ marks the layer number, and $z_{l-1}$ and $z_{l}$ are  the output of $\{l-1\}{-}th$ and $\{l\}{-}th$ modules, respectively.
After each module there is a residual connection.
All Swin blocks are equipped with the shifted window partitioning approach described above.
Swin  processes the image in a hierarchical manner, since it splits the input into non-overlapping patches and subsequently merges them at different resolutions.





