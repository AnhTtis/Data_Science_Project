\section*{Additional implementation details}
\noindent \textbf{Training details.} 
At the training time, we first project the query ray instead of a single point to each source view and fetch the corresponding ray-based feature, which contains rich contextual information in each intra-view. 
For pre-training, we train on a single NVIDIA RTX3090-Ti GPU with 24GB memory. On this hardware, we train our S-Ray for 260k iterations in 60 different scenes of ScanNet~\cite{scannet} (real-world data) and 100k iterations in 12 different scenes of Replica~\cite{replica}  (synthetic data). For finetuning, we only require 10min finetuning time corresponding to 2k iterations. This finetuning result is comparable and even better than 100k optimizations of Semantic-NeRF \cite{semantic-nerf} from each independent scene.

We do not show the specific details of the semantic loss design in the paper. In code implementation, we apply two-stage (coarse and fine) ray sampling as done in NeRF~\cite{NeRF}. Therefore, our semantic loss is actually computed as
\begin{equation}
    L_{sem}=-\sum_{\mathbf{r} \in \mathcal{R}}\left[\sum_{l=1}^L p^l(\mathbf{r}) \log \hat{p}_c^l(\mathbf{r})+\sum_{l=1}^L p^l(\mathbf{r}) \log \hat{p}_f^l(\mathbf{r})\right]
\end{equation}

where $\mathcal{R}$ are the set of sample rays within a training batch, $1\leq l\leq L$ is the class index, and $p^l, \hat{p}^l_c, \hat{p}^l_f$ are the multi-class probability at class $l$ of the ground truth, coarse semantic logits and fine semantic logits for the query ray $\mathbf{r}$. Actually, for fair comparison in Section 4.2 of our paper, we adopt the same training loss with Semantic-NeRF~\cite{semantic-nerf} as:
\begin{equation}
    \mathcal{L}_{total} = \lambda_1 \mathcal{L}_{sem} + \lambda_2 \mathcal{L}_{photometric}, 
\end{equation}
where the color head is from the geometry aware network with photometric loss same as~\cite{semantic-nerf}. Like Semantic-NeRF, we also set $\lambda_1 = \lambda_2 =1$ in Section 4.2 and set $\lambda_1 = 0, \lambda_2 = 1$ as NeRF for ablation study in Table 2 of the paper. 

\noindent \textbf{Data split.} Our training data consists of both synthetic data and real data. For real data training, we choose 60 different scenes from ScanNet~\cite{scannet} as training datasets and use the image resolution of $320 \times 240$. We then choose 10 unseen novel scenes as test datasets to evaluate the generalizability of S-Ray in real data. For synthetic data, we choose 12 different scenes (\ie, 2 rooms, 2 offices, 7 apartments, 1 hotel) from Replica~\cite{replica} for the training set and the remains (\ie, 2 apartments, 3 offices, 1 room) as test set with the image resolution of $640\times 480$. 
For each test scene, we select 20 nearby views; we then select 8 views as source input views, 8 as additional input for per-scene fine-tuning, and take the remaining 4 as testing views. Our training data includes various camera setups and scene types, which allows our method to generalize well to unseen semantic scenarios.

