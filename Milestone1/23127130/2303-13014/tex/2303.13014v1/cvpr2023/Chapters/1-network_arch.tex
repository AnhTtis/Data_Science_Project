\section*{Network architecture}
\noindent \textbf{Semantic feature extraction.} Given input views and a query ray, we project the ray to each input view and apply the semantic feature extraction module in Table~\ref{table:feature_extraction} to learn contextual features and build an initial 3D contextual space. The details can be found in Table~\ref{table:feature_extraction} and Section 3.2 in the paper.
\begin{table*}[!h]
\centering

\begin{tabular}{lcccc}
\toprule
Type & Size/Channels & Activation & Stride & Normalization\\
\midrule
Input 1: RGB images & - & - & - & -\\
Input 2: View direction differences & - & - & - & -\\
L1: Conv $7\times 7$ & $3, 16$ & ReLU & $2$ & Instance\\
% layer1
L2: ResBlock $3\times 3$ & $16, 32, 32$ & ReLU & $2, 1$ & Instance\\
% layer2
L3: ResBlock $3\times 3$ & $32, 64, 64$ & ReLU & $2, 1$ & Instance\\
L4: ResBlock $3\times 3$ & $64, 64, 64$ & ReLU & $1, 1$ & Instance\\
% layer3
L5: ResBlock $3\times 3$ & $64, 128, 128$ & ReLU & $2, 1$ & Instance\\
L6: ResBlock $3\times 3$ & $128, 128, 128$ & ReLU & $1, 1$ & Instance\\
L7: ResBlock $3\times 3$ & $128, 128, 128$ & ReLU & $1, 1$ & Instance\\
L8: ResBlock $3\times 3$ & $128, 128, 128$ & ReLU & $1, 1$ & Instance\\
L9: ResBlock $3\times 3$ & $128, 128, 128$ & ReLU & $1, 1$ & Instance\\
L10: ResBlock $3\times 3$ & $128, 128, 128$ & ReLU & $1, 1$ & Instance\\

L11: Conv $3\times3$ & $128, 64$ & - & $1$ & Instance\\
L12: Up-sample 2$\times$ & - & - & - & -\\
L13: Concat (L12, L4) & - & - & - & -\\
L14: Conv $3\times3$ & $128, 64$ & - & $1$ & Instance\\

L15: Conv $3\times3$ & $64, 32$ & - & $1$ & Instance\\
L16: Up-sample $2\times$ & - & - & - & -\\
L17: Concat (L16, L2) & - & - & - & -\\
L18: Conv $3\times3$ & $64, 32$ & - & $1$ & Instance\\
L19: Conv $1\times1$ & $32, 32$ & - & $1$ & Instance\\

L20: Reprojection\\
L21: MLP (Input 2) & $4, 16, 32$ & ELU & - & -\\
L22: Add (L21, L20) & - & - & - & -\\

\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{Semantic feature extraction.}
\label{table:feature_extraction}
\end{table*}




\noindent \textbf{Cross-Reprojection Attention.} To model full semantic-aware dependencies from the 3D contextual space with computational efficiency, we design the Cross-Reprojection Attention module in Table~\ref{table:CRA_module} to learn dense and global contextual information, which can finally benefit the performance of semantic segmentation. The details of architecture and design can be found in Table~\ref{table:CRA_module} and Section 3.3 in the paper.

\begin{table*}[!h]
\centering

\begin{tabular}{lccc}
\toprule
Type & Feature dimension & Activation \\
\midrule
Input: Initial 3D contextual space & - & -\\
L1: Transpose (Input) & - & -\\
L2: Position Embeddings & - & -\\
L3: Add (L1, L2) & - & -\\
L4: Multi-head Attention (nhead=4) (L3) & 32 & ReLU \\
L5: Transpose (L4) & - & -\\
L6: Multi-head Attention (nhead=4) (L5) & 32 & ReLU \\
\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{Cross-Reprojection Attention module.}
\label{table:CRA_module}
\end{table*}

\begin{table*}[!h]
\centering
\begin{tabular}{lccc}
\toprule
Type & Feature dimension & Activation \\
\midrule
Input 1: Initial 3D contextual space & - & -\\
Input 2: View direction differences & - & -\\
L1: Concat (Input 1, Input 2) & - & - \\
L2: MLP (L1) & $37, 16, 8, 1$ & ELU\\
L3: Sigmoid (L2) & - & -\\
\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{Semantic-aware weight network.}
\label{table:semantic_aware_weight_net}
\end{table*}


\noindent \textbf{Semantic-aware weight network.} To construct the final semantic ray from refined 3D contextual space and learn the semantic consistency along the ray, we introduce the semantic-aware weight network in Table~\ref{table:semantic_aware_weight_net} to rescore the significance of each source view. More experiments about the semantic-aware weight net can be found in Table~\ref{tab:tau}, and we show architecture details in Table~\ref{table:semantic_aware_weight_net} and Section 3.4 of the paper.

\noindent \textbf{Geometry-aware network.} To build our generalizable semantic field, we adopt a geometry-aware network to predict density $\sigma$ and render the final semantic field with semantic logits. Moreover, we also leverage this network to produce the radiance and render a radiance field to show our rendering quality. We show the details of this network in Table~\ref{table:geometry_aware_net} and Section 3.4 of the paper.









\begin{figure*}[!h]
    \centering
    \includegraphics[width=\linewidth]{cvpr2023/Figures/new_weight.pdf}
    \caption{Different significance weight of source view. Given the query ray, we apply the semantic-aware weight network to learn the significance weight $\mathbf{w}$ to restore each source view. Note that the greater weight will be assigned to the more important source view. }
    \label{fig:new_weight}
\end{figure*}


\begin{table*}[!t]
\centering
\begin{tabular}{lccc}
\toprule
Type & Feature dimension & Activation\\
\midrule
Input: Initial 3D contextual space & - & -\\
L1: MLP (Input) & $32, 32$ & ELU\\
L2: MLP (Input) & $32, 1$ & ELU\\
L3: Sigmoid (L2) & - & -\\
L4: Dot-product (L1, L3) & - & -\\
L5: Cross-view Mean (L4) & - & - \\
L6: Cross-view Varience (L4) & - & -\\
L7: Concat (L5, L6) & - & - \\
L8: MLP (L7) & $64, 32, 16$ & ELU\\
L9: Multi-head Attention (nhead=4) (L8) & 16 & ReLU \\
L10: MLP (L9) & $16$ & ELU\\
L11: MLP (L10) & $1$ & ReLU\\
\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{Geometry-aware network.}
\label{table:geometry_aware_net}
\end{table*}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{cvpr2023/Figures/weight.pdf}
%     \caption{fddasfasd }
%     \label{fig:signifi}
% \end{figure*}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{cvpr2023/Figures/semantic_render.pdf}
%     \caption{\textbf{Semantic rendering quality comparison}. On the left, we show direct semantic rendering results of our method and NeuRay~\cite{neuray} with semantic head. Limited by insufficient generalization, NeuRay with semantic head has difficulty to render semantics in unseen scenes and fails to capture contextual structure, while our method is able to learn the semantic structural prior, thus showing strong generalizability across different scenes. On the right, we show the experimental comparison between our S-Ray with $2k$ iterations finetuning ($\sim$10min) and Semantic-NeRF \cite{semantic-nerf} with $100k$ iterations. }
%     \label{fig:result}
% \end{figure*}
