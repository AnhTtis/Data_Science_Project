\section{Introduction}

Recently, Neural Radiance Field (NeRF) \cite{NeRF}, a new novel view synthesis method with implicit representation, has taken the field of computer vision by storm \cite{NeRF_review2022}. NeRF and its variants \cite{NeRF,nerf++, mipnerf, pixelnerf} adopt multi-layer perceptrons (MLPs) to learn continuous 3D representations and utilize multi-view images to render unseen views with fine-grained details. NeRF has shown state-of-the-art visual quality, produced impressive demonstrations, and inspired many subsequent works~\cite{mvsNeRF, GeoNeRF, code_nerf, IBRnet, pixelnerf}.

While the conventional NeRFs have achieved great success in low- and middle-level vision tasks such as neural scene rendering, image synthesis, and multi-view reconstruction \cite{mvsNeRF, neuray, kilo_nerf, Fast_nerf, grf, GIRAFFE, UNISURF}, it is interesting to explore their more possibilities in high-level vision tasks and applications. Learning high-level semantic information from 3D scenes is a fundamental task of computer vision with a wide range of applications \cite{dl_for_medical_img_seg, autonomous_driving_seg, garcia2017review, robotic_semseg}. For example, a comprehensive semantic understanding of scenes enables intelligent agents to plan context-sensitive actions in their environments. One notable attempt to learn interpretable semantic understanding with the NeRF structure is Semantic-NeRF \cite{semantic-nerf}, which regresses a 3D-point semantic class together with radiance and density. Semantic-NeRF shows the potential of NeRF in various high-level tasks, such as scene-labeling and novel semantic view synthesis.

However, Semantic-NeRF follows the vanilla NeRF by estimating the semantic label from a single ray with a new semantic head. While this operation is reasonable to learn low-level information including color and density, a single ray fails to provide rich semantic patterns -- we can tell the color from observing a single pixel, but not its semantic label. To deal with this, Semantic-NeRF heavily relies on positional encoding to learn semantic features, which is prone to overfit the current scene and only applicable to novel \textit{views} within the same scene~\cite{NeSF}. As a result, Semantic-NeRF has to train one model from scratch for every scene independently or provides very limited novel scene generalization by utilizing other pretrained models to infer 2D segmentation maps as training signals for unseen scenes. This significantly limits the range of applications in real-world scenarios.

In this paper, we propose a neural semantic representation called \textbf{Semantic Ray} (S-Ray) to build a generalizable semantic field, which is able to learn from multiple scenes and directly infer semantics on novel viewpoints across novel scenes as shown in Figure~\ref{fig:teaser}. 
% To our best knowledge, this is the first work to learn a generalizable semantic field in real-world scenes. 
As each view provides specific high-level information for each ray regarding of viewpoints, occlusions, etc., we design a Cross-Reprojection Attention module in S-Ray to fully exploit semantic information from the reprojections on multiple views, so that the learned semantic features have stronger discriminative power and generalization ability. While directly performing dense attention over the sampled points on each reprojected ray of multiple views would suffer from heavy computational costs, we decompose the dense attention into intra-view radial and cross-view sparse attentions to learn comprehensive relations in an efficient manner.

More specifically, for each query point in a novel view, different from Semantic-NeRF that directly estimates its semantic label with MLP, we reproject it to multiple known views. It is worth noting that since the emitted ray from the query point is virtual, we cannot obtain the exact reprojected point on each view, but a reprojected ray that shows possible positions. Therefore, our network is required to simultaneously model the uncertainty of reprojection within each view, and comprehensively exploit semantic context from multiple views with their respective significance. To this end, our Cross-Reprojection Attention consists of an intra-view radial attention module that learns the relations among sampled points from the query ray, and a cross-view sparse attention module that distinguishes the same point in different viewpoints and scores the semantic contribution of each view. As a result, our S-Ray is aware of the scene prior with rich patterns and generalizes well to novel scenes. We evaluate our method quantitatively and qualitatively on synthetic scenes from the Replica dataset \cite{replica} and real-world scenes from the ScanNet dataset \cite{scannet}. Experiments show that our S-Ray successfully learns from multiple scenes and generalizes to unseen scenes. By following Semantic-NeRF \cite{semantic-nerf}, we design competitive baselines based on the recent MVSNeRF \cite{mvsNeRF} and NeuRay \cite{neuray} architectures for generalizable semantic field learning. Our S-Ray significantly outperforms these baselines which demonstrates the effectiveness of our cross-reprojection attention module.

% 如果实验结果有时间优势，一定放到这个里面！！！！！
