\section{Experiments}
We conduct extensive experiments and evaluate our method with basically two settings. 1) We directly evaluate our pretrained model on test scenes (\ie, unseen scenes) without any finetuning. Note that we train only \textit{one} model called S-Ray and evaluate it on all test scenes. 2) We finetune our pretrained model for a small number of steps on each unseen scene before evaluation. While training from scratch usually requires a long optimization time, we evaluate our S-Ray by achieving comparable performance with well-trained models by much less finetuning time.
\subsection{Experiment Setup}
\label{sec:exp_set}
\noindent \textbf{Datasets.} To test the effectiveness of our method comprehensively, we conduct experiments on both synthetic data and real data. For synthetic data, we use the Replica \cite{replica}, a dataset with 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each Scene consists of dense geometry, high-dynamic-range textures, and per-primitive semantic class. Then, we choose 12 scenes from the Replica as training datasets and the remaining unseen scenes as test datasets. For Real data, we use the ScanNet \cite{scannet}, which is a real-world large labeled RGB-D dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses surface reconstructions and semantic segmentation. We choose 60 different scenes as training datasets and 10 unseen novel scenes as test datasets to evaluate generalizability in real data. More details about the split of datasets will be shown in the supplementary.

\noindent \textbf{Metrics.} To accurately measure the performance of our S-Ray, we adopt mean Intersection-over-Union (mIoU) as well as average accuracy and total accuracy to compute segmentation quality. What's more, we will discuss our rendering quality if we compute the color from the \textit{Geometry-Aware Network} in the posterior subsection. To evaluate rendering quality, we follow NeRF~\cite{NeRF}, adopting peak signal-to-noise ratio (PSNR), the structural similarity index measure (SSIM)~\cite{ssim}, and learned perceptual image patch similarity (LPIPS) \cite{lpips}. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{cvpr2023/Figures/results_v2.pdf}
    \vspace{-8mm}
    \caption{\textbf{Semantic rendering quality comparison}. On the left, we show direct semantic rendering results of our method and NeuRay~\cite{neuray}+semantic head. Limited by insufficient generalization, NeuRay+semantic head has difficulty to render semantics in unseen scenes and fails to capture contextual structure, while our method is able to learn the semantic structural prior, thus showing strong generalizability across different scenes. On the right, we show the experimental comparison between our S-Ray with $2k$ iterations finetuning ($\sim$10min) and Semantic-NeRF \cite{semantic-nerf} with $100k$ iterations. }
    \label{fig:result}
\end{figure*}

\noindent \textbf{Baselines.} To evaluate our fast generalizability in an unseen scene, we choose Semantic-NeRF \cite{semantic-nerf} as our baseline. Since we are the first to learn a generalizable semantic field in real-world scenes, we have no baselines to compare our generalizable performance. In order to further show our strong generalizability, we add the semantic head by following Semantic-NeRF settings to the NeRF-based methods which have shown generalizability in the reconstruction task. Specifically, we compare our method against MVSNeRF \cite{mvsNeRF} with semantic head and NeuRay \cite{neuray} with semantic head in generalization and finetuning. Due to the space limitation, We provide a more detailed discussion and comparison with~\cite{mvsNeRF, neuray, IBRnet, semantic-nerf, GPNR} in the supplementary.


\subsection{Comparison with Baselines}
To render each semantic map of the test view, we sample 8 source views from the training set for all evaluation datasets in generalization settings. For the per-scene optimization, we follow \cite{semantic-nerf} to train it on the new scene from scratch. To compare our results fairly, we follow the Semantic-NeRF \cite{semantic-nerf} to resize the images to $640\times 480$ for Replica \cite{replica} and $320\times 240$ for ScanNet \cite{scannet}. Results can be seen in Table~\ref{tb:rendering} and in Figure~\ref{fig:result}.

Table~\ref{tb:rendering} shows that our pretrained model generalizes well to unseen scenes with novel contextual information. we observe that the generalization ability of our S-Ray consistently outperforms both NeuRay \cite{neuray} and MVSNeRF \cite{mvsNeRF} with semantic heads. Although they are the recent methods that have strong generalization ability, we show that directly following Semantic-NeRF by adding a semantic head fails to fully capture the semantic information. Instead, our Cross-Reprojection Attention can extract relational features from multi-view reprojections of the query ray, thus achieving better accuracy and stronger generalization ability. 

After finetuning for only 10 minutes, our performance is competitive and even better than Semantic-NeRF with $100k$ iters per-scene optimization. The visual results in Figure~\ref{fig:result} clearly reflect the quantitative results of Table~\ref{tb:rendering}. The generalization ability of our S-Ray is obviously stronger than NeuRay \cite{neuray} with semantic head. As MVSNeRF \cite{mvsNeRF} shows even worse generalization performance than NeuRay with the semantic head as shown in Table~\ref{tb:rendering}, we do not show its visualization results due to the limited space. In general, the comparison methods directly use point-based features which benefit to per-pixel reconstruction instead of semantics. Our approach utilizes the full spatial context information around the query ray to build our semantic ray, thus leading to the best generalization ability and high segmentation across different test scenes.

\subsection{Ablation Studies and Analysis}
\noindent \textbf{Reconstruction quality of S-Ray.} To evaluate the reconstruction quality of S-Ray, we follow (\ref{eq:render_discrete}) to render the radiance from geometry aware network with photometric loss same as \cite{semantic-nerf}. In Table~\ref{tab:render_quality}, we test the S-Ray for color rendering and compare it with Semantic-NeRF\cite{semantic-nerf}, MVSNeRF \cite{mvsNeRF} and NeuRay~\cite{neuray}. More comparisons can be found in the supplementary. As shown in Table \ref{tab:render_quality} and Figure \ref{fig:color_result}, our S-Ray can also generalize well in reconstruction. This shows that our network cannot only capture the contextual semantics but also learn geometry features well.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.98\linewidth]{cvpr2023/Figures/fine_tune.pdf}
    \vspace{-3mm}
    \caption{Qualitative results of scene rendering for generalization (w/o ft) and finetuning (w/ ft) settings in real data~\cite{scannet}.}
    \label{fig:color_result}
\end{figure}

\noindent \textbf{Training from scratch.} In order to further present the generalization ability of our S-Ray, we train our model on a scene from scratch without pretraining the model. We strictly follow the same process as Semantic-NeRF \cite{semantic-nerf} to train S-Ray. Results in Table \ref{tab:ablation} (ID 9 and 10) show that training our method from scratch can also achieve similar results as finetuning the pretrained model, and it obtains even better results than Semantic-NeRF.
\begin{table}[tb]
% \tabcolsep=0.2cm
    \centering
    \small
%\begin{footnotesize}
    \begin{tabular}{l |  c  c  c }
% \hline
\toprule 
Method & PSNR$\uparrow$ & SSIM$\uparrow$ & LIPIPS$\downarrow$ \\  % ScanNet Scene0101_03
\midrule
Semantic-NeRF~\cite{semantic-nerf}  & 25.07 & 0.797 & 0.196\\

MVSNeRF~\cite{mvsNeRF}  & 23.84 & 0.733 & 0.267\\

NeuRay~\cite{neuray}  & 27.22 & 0.840 & 0.138\\\

\textbf{S-Ray} (\textbf{Ours})  & 26.57 & 0.832 & 0.173 \\

\textbf{S-Ray}$_{ft}$ (\textbf{Ours})  & \textbf{29.27} & \textbf{0.865} & \textbf{0.127}\\

\bottomrule  
\end{tabular}
%\end{footnotesize}
\vspace{-3mm}
\caption{Comparisons of scene rendering in real data~\cite{scannet}.}
     \label{tab:render_quality}
\end{table} 

\noindent \textbf{Evaluation of Cross-Reprojection Attention module.} In Table \ref{tab:ablation}, we adopt ablation studies for Cross-Reprojection module shown in Figure \ref{fig:cross}. We compare four models, the full model (ID 1), the model without Cross-Reprojection Attention (ID 2, 4), the model only with intra-view radial attention module (ID 3, 7), and the model only with cross-view attention (ID 4, 8). The results show that Cross-Reprojection Attention in S-Ray enables our network to learn more contextual information as discussed in Sec.~\ref{sec:cross-reprojection}.

\begin{table}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        ID & Description & Setting & mIoU & Total Acc \\
        \midrule
        1 & full S-Ray                                & Gen & \textbf{57.15}  & \textbf{78.24}  \\
        2 & w/o cross-reprojection Att             & Gen &  45.34 & 53.67 \\
        3 & only intra-view Att & Gen & 49.09 & 58.53 \\
        % 4 & $M,B,P$ with init-NeuRay-D & Gen &  29.79 & 24.69 \\
        4 & only cross-view Att    & Gen & 52.56 & 63.25 \\
        \midrule
        5 & full S-Ray      & Ft & \textbf{91.08} & \textbf{98.20} \\
        6 & w/o cross-reprojection Att & Ft & 76.30 & 86.02 \\
        7 & only intra-view Att  & Ft & 81.24 & 89.58 \\
        8 &  only cross-view Att  & Ft & 87.01 & 93.34 \\
        \midrule
        9 &  S-Ray  & Sc & \textbf{95.31}  &  \textbf{98.40} \\
        10 &  Semantic-NeRF~\cite{semantic-nerf} & Sc & 94.48 & 95.32\\
        \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{Ablation studies. mIoU and Total Acc on the real data from ScanNet~\cite{scannet}. ``Gen'' means the generalization setting, ``Ft'' means to finetune on the scene and ``Sc'' means to train from scratch.}
    \label{tab:ablation}
\end{table}

\noindent \textbf{Few-step finetuning of S-Ray.} Table \ref{tab:time_compare} reports mIoU and finetuning time of different models with on the ScanNet \cite{scannet} dataset. We observe that by finetuning with limited time, our model is able to achieve a better performance than a well-trained Semantic-NeRF \cite{semantic-nerf} with much longer training time. As Semantic-NeRF fails to present cross-scene generalization ability, it still requires full training on the unseen scene. Instead, our S-Ray is able to quickly transfer to the unseen scenes. Moreover, S-Ray outperforms other competitive baselines with similar finetuning time, which further demonstrates that our Cross-Reprojection Attention operation successfully improves the generalization ability.
\begin{table}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
         Method & Train Step & Train Time & mIoU  \\
         \midrule
        %  NeRF~\cite{mildenhall2020nerf} & 10k  & $\sim$30min &       \\
         Semantic-NeRF~\cite{semantic-nerf}    & 50k & $\sim$2h  & 89.33 & \\
         MVSNeRF~\cite{mvsNeRF} w/ s-Ft & 5k  & $\sim$20min & 52.02 & \\
         NeuRay~\cite{neuray} w/ s-Ft   & 5k   & $\sim$32min & 79.23 & \\
         \textbf{S-Ray}-Ft (\textbf{Ours})                         & 5k   & $\sim$20min & \textbf{92.04} & \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{mIoU and training steps/time on real data~\cite{replica}. ``w/ s'' means adding a semantic head on the baseline architectures. }
    \label{tab:time_compare}
\end{table}


From the experiments above, we have the following key observations:
\begin{enumerate}[1)]
    \item Our Semantic Ray can exploit contextual information of scenes and  presents strong generalization ability to adapt to unseen scenes. It achieves encouraging performance without finetuning on the unseen scenes, and also obtains comparable results to the well-trained Semantic-NeRF with much less time of finetuning.
    \item We show the effectiveness of our Cross-Reprojection Attention module through comprehensive ablation studies. Experiments demonstrate that both intra-view and cross-view attentions are crucial for S-Ray, and we achieve the best performance by simultaneously exploiting relational information from both modules.
    \item Our performance in radiance reconstruction shows the great potential of our attention strategy, which is able to learn both dense contextual connections and geometry features with low computational costs. 
\end{enumerate}