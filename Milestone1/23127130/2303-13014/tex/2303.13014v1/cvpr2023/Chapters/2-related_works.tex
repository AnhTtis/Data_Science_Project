\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{cvpr2023/Figures/final_pipeline_v1.pdf}
    \vspace{-7mm}
    \caption{\textbf{Pipeline of semantic rendering with S-Ray}. Given input views and a query ray, we first reproject the ray to each input view and apply a CNN-based module to extract contextual features to build an initial 3D contextual space (Sec.~\ref{sec:contextualSpace}). Then, we apply the Cross-Reprojection Attention module to learn dense semantic connections and build a refined contextual space (Sec.~\ref{sec:cross-reprojection}). For semantic ray rendering, we adopt the semantic-aware weight net to learn the significance of each view to construct our semantic ray from refined contextual space (Sec.~\ref{sec:semantic-ray}). Finally, we leverage the geometry-aware net to get the density and render the semantics along the query ray.}
    \label{fig:pipeline}
    \vspace{-10pt}
\end{figure*}
\section{Related Work}
\noindent \textbf{Semantic Segmentation.}
Semantic segmentation is one of the high-level tasks that paves the way toward complete scene understanding, with most methods targeting a fully supervised, single-modality problem (2D \cite{SegNet, DeepLab, rethinking_atrous_for_seg, FCN, U-net} or 3D \cite{segsota-review, cylindrical_conv_seg, survey_3d_point_cloud, rangenet++}). Recently, machine learning methods have proven to be valuable in semantic segmentation \cite{U-net, criss-cross-attention, SegNet, SegGroup} which aims to assign a separate class label to each pixel of an image. However, most methods suffer from severe performance degradation when the scenes observed at test time mismatch the distribution of the training data \cite{genralizable_model_agnostic_semseg, semseg_ood}. To alleviate the issue, 2D-based architectures \cite{U-net, FCN, garcia2017review} are often trained on large collections of costly annotated data \cite{COCO_dataset} while most 3D prior works \cite{lidar_seg, kinectfusion, Dense_vis_tracking, JSENet, pointnet,pointnet++} rely on 3D sensors. Though straightforward to apply, 2D-based approaches only produce per-pixel annotations and fail to understand the 3D structure of scenes \cite{NeSF} and 3D sensors are too expensive to be widely available as RGB cameras. In contrast to these methods, our method reconstructs and then segments a 3D semantic representation from 2D inputs and supervision alone without ground truth 3D annotations or input geometry.

% \noindent \textbf{Implicit 3D Representations.} The implicit 3D representations have recently emerged as a promising direction to recover 3D geometries. It is initially formulated as level sets by optimizing neural nets that map the coordinates to an occupancy field \cite{Occupancy_networks}. Some methods use implicit representations to reconstruct a 3D scene \cite{conv_occupancy_net, iMAP} or shape \cite{local_sdf, bspnet, learning_implicit_field_gen, neural_geo_level_of_detail}. Similar to our target, Atlas~\cite{atlas} learns a 3D implicit TSDF reconstruction from 2D images and can also segment the predicted scene geometry. However, this method requires 3D data supervision, while our approach uses only 2D images at both training and test time.



\noindent \textbf{Neural Radiance Fields.} 
Recently, implicit neural representations have advanced neural processing for 3D data and multi-view 2D images \cite{SRN, Occupancy_networks, Deep_SDF, multiview_by_disentangle}. In particular, Neural Radiance Fields (NeRF) \cite{NeRF} has drawn great attention, which is a fully-connected neural network that can generate novel views of complex 3D scenes, based on a partial set of 2D images. A NeRF network aims to map from 3D location and viewing direction (5D input) to opacity and color (4D input). Several following works emerge trying to address its limitations and improve its performance, including fast training \cite{direct_voxel_grid_opt_super_fast, Depth-supervised_NeRF}, efficient inference \cite{DeRF, Neural_sparse_voxel_field, Fast_nerf, kilo_nerf, PlenOctrees}, unbounded scenes training \cite{nerf++, mipnerf}, better generalization \cite{GRAF, grf, IBRnet, pixelnerf, GeoNeRF, GPNR}, generative modeling \cite{GNeRF, GIRAFFE, GRAF}, editing \cite{editing_conditional_RF, clip-nerf, code_nerf}. 
As NeRF achieves very impressive results for novel view synthesis, researchers start to explore high-level tasks in NeRF such as semantic segmentation \cite{semantic-nerf, NeSF}. However, Semantic-NeRF\cite{semantic-nerf} is only applicable in the single-scene setting while NeSF \cite{NeSF} only conducts experiments in synthetic data with insufficient generalization and high computational costs. In contrast, taking NeRF as a powerful implicit scene representation, our method can learn a generalizable semantic representation of new, real-world scenes with high quality. 
% if add the however to claim the current segmentation task



