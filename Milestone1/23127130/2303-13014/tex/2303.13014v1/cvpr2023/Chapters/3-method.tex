\section{Method}

Given a set of $N$ input views with known camera poses, our goal is to synthesize novel semantic views from arbitrary angles across unseen scenes with strong generalizability. Our method can be divided into three stages: (1) build the 3D contextual space from source multiple views, (2) decompose the semantic information from 3D space along reprojected rays and cross multiple views with cross-reprojection attention, (3) reassemble the decomposed contextual information to collect dense semantic connections in 3D space and re-render the generalizable semantic field. Our pipeline is depicted in Figure~\ref{fig:pipeline}. Before introducing our S-Ray in detail, we first review the volume rendering on a radiance field \cite{volume_rendering_origin, NeRF}.
\subsection{Preliminaries}
\noindent \textbf{Neural Volume Rendering.} Neural volume rendering aims to learn two functions: $\sigma(\mathbf{x}; \theta): \mathbb{R}^3 \mapsto \mathbb{R}$, which maps the spatial coordinate $\mathbf{x}$ to a density $\sigma$, and $\mathbf{c}(\mathbf{x}, \mathbf{d}; \theta): \mathbb{R} ^3 \times \mathbb{R}^3 \mapsto \mathbb{R}^3$ that maps a point with the viewing direction to the RGB color $\mathbf{c}$. The density and radiance functions are defined by the parameters $\theta$. To learn these functions, they are evaluated through a ray emitted from a query view. The ray is parameterized by $\mathbf{r}(t)=\mathbf{o}+t \mathbf{d}$, $t \in\left[t_n, t_f\right]$, where $\mathbf{o}$ is the start point at the camera center, $\mathbf{d}$ is the unit direction vector of the ray, and $\left[t_n, t_f\right]$ is the near-far bound along the ray. Then, the color for the associated pixel of this ray can be computed through volume rendering \cite{volume_rendering_origin}:
\begin{align}
    \hat{\mathbf{C}}(\mathbf{r} ; \theta)=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \mathrm{d} t, 
\end{align}
where $T(t)=\exp \left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) \mathrm{d} s\right)$. In practice, the continuous integration can be approximated by a summation of discrete samples along the ray by the quadrature rule. For selected $N$ random quadrature points $\{t_k\}_{k=1}^N$ between $t_n$ and $t_f$, the approximated expected color is computed by:
\begin{align}
\label{eq:render_discrete}
    \hat{\mathbf{C}}(\mathbf{r}; \theta) = \sum_{k=1}^{N} T(t_k)\alpha(\sigma(t_k)\delta_k) \mathbf{c}(t_k), 
\end{align}
where $T(t_k) = \exp \left(-\sum_{k^{'}=1}^{k-1} \sigma\left(t_k\right) \delta_k\right)$, $\alpha(x) = 1 - exp(-x)$, and $\delta_k = t_{k+1} - t_k$ are intervals between sampled points. 

\subsection{Building 3D Contextual Space across Views}
\label{sec:contextualSpace}
% if add related: In contrast to IBRnet: only reproject a point, we reproject a ray
NeRF-based generalization rendering methods \cite{grf, pixelnerf, IBRnet} construct a radiance field by reprojecting a single point of  the query ray to source views and extracting the point-based feature. It is reasonable to predict color from a single point but is quite insufficient for semantics which needs more contextual information. Therefore, we build the 3D contextual space to learn rich semantic patterns by reprojecting the whole query ray across views. Given the $N$ points $\{\mathbf{p}_{i}\}_{i = 1,2,..., N}$ sampled from the ray emitting from the query view and known camera pose (\ie, the rotation matrix $\mathbf{R}$ and the translation vector $\mathbf{t}$). Without losing generality, we can rewrite the ray as:
\begin{equation}
    \mathbf{r}(z) = \mathbf{p}_i + z\frac{\mathbf{p}_j - \mathbf{p}_i}{||\mathbf{p}_j - \mathbf{p}_i||}, \quad z\in \mathbb{R}.
\end{equation}
Then, the ray warping function is defined as:
\begin{equation}
    w(\mathbf{r}(z), \mathbf{R}, \mathbf{t}) := \mathbf{K} \pi (\mathbf{R} \cdot \mathbf{r}(z) + \mathbf{t}), 
\end{equation}
which allows reprojecting the ray onto source views to obtain the plane ray $\mathbf{r}^{*}(z)$, \ie, $ \mathbf{r}^{*}(z) = w(\mathbf{r}(z), \mathbf{R}, \mathbf{t})$, where $\mathbf{K}$ is the camera calibration matrix and $\pi (\mathbf{u}):= [\mathbf{u}_x / \mathbf{u}_z, \mathbf{u}_y / \mathbf{u}_z]$ is the projection function. 

Let $\mathcal{F}_j(\mathbf{r}^{*}(z)) (j = 1, 2, ..., m)$ denote the ray-based feature of the query ray reprojected on the $j$-th source view,  $\mathcal{F}_j(\mathbf{r}^{*}(z_i))$ be the $\mathbf{p}_i$ point-based feature within the $j$-th source view. 
Due to the permutation invariance of the source views, we use a shared U-Net-based convolutional neural network to extract dense contextual information $\mathcal{F}$ from these views. Then, we build our 3D contextual space $\mathcal{M}$ across views as:

\small
\begin{equation}
   \mathcal{M}
=\begin{bmatrix}
\mathcal{F}_1(\mathbf{r}^{*}(z_1)) &  \mathcal{F}_1(\mathbf{r}^{*}(z_2))  & \cdots\ &\mathcal{F}_1(\mathbf{r}^{*}(z_N))\\
\mathcal{F}_2(\mathbf{r}^{*}(z_1)) &  \mathcal{F}_2(\mathbf{r}^{*}(z_2)) & \cdots\ & \mathcal{F}_2(\mathbf{r}^{*}(z_N))\\
 \vdots   & \vdots & \ddots  & \vdots  \\
 \mathcal{F}_m(\mathbf{r}^{*}(z_1)) & \mathcal{F}_m(\mathbf{r}^{*}(z_2)) & \cdots\ & \mathcal{F}_m(\mathbf{r}^{*}(z_N))\\
\end{bmatrix}
\end{equation}
\normalsize
which is a 3D matrix (\ie, $\mathcal{M} \in \mathbb{R}^{m \times N \times C}$) to describe a full space contextual information around the query ray with feature dimension $C$.


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{cvpr2023/Figures/cross_reproj_v4.pdf}
    \vspace{-6mm}
    \caption{\textbf{Pipeline of Cross-Reprojection Attention}. Given the initial 3D contextual space $\mathcal{M}$ form Sec.~\ref{sec:contextualSpace}, we first decompose $\mathcal{M}$ along the radial direction (\ie, each intra-view). Then, we apply the \textit{intra-view radial attention module} to each $\mathcal{F}_i (i=1,...m)$ to learn the ray-aligned contextual feature from each source view and build the $\mathcal{M}^{'}$. We further decompose the $\mathcal{M}^{'}$ cross multiple views and employ the \textit{cross-view sparse attention module} to each $\mathcal{F}^{'}_{\mathbf{r}^{*}(i)}$, thus capturing sparse contextual patterns with their respective significance to semantics. After the two consecutive attention modules, we fuse the decomposed contextual information with the final refined 3D contextual space $\mathcal{M}^{''}$, which models dense semantic collections around the ray. (\textit{Best viewed in color})}
    \label{fig:cross}
\end{figure*}

\subsection{Cross-Reprojection Attention}
\label{sec:cross-reprojection}
% \noindent \textbf{Decomposing Semantic Correspondence}
% \noindent \textbf{Reassembling Semantic Connections}
To model full semantic-aware dependencies from above 3D contextual space $\mathcal{M}$, a straightforward approach is to perform dense attention over $\mathcal{M}$. However, it would suffer from heavy computational costs. To address the problem, we propose Cross-Reprojection Attention as shown in Figure~\ref{fig:cross}, including intra-view radial attention and cross-view sparse attention to approximate dense semantic connections in 3D space with lightweight computation and memory. 

\noindent \textbf{Intra-view Radial Attention.}
First, we rewrite the contextual space as $\mathcal{M} = [\mathcal{F}_1, \mathcal{F}_2, ..., \mathcal{F}_m]^T$, where $\mathcal{F}_i = [\mathcal{F}_i(\mathbf{r}^{*}(z_1)), \mathcal{F}_i(\mathbf{r}^{*}(z_2)), ..., \mathcal{F}_i(\mathbf{r}^{*}(z_N))]$ is the radial feature in the $i$-th view. 
Then, we decompose the 3D contextual space $\mathcal{M}$ along the radial direction in source views, \ie, consider the $\mathcal{F}_i (i=1,...,m)$ which encodes the intra-view contextual information within each view. Taking the $\mathcal{F}_i \in \mathbb{R}^{N \times C}$ as input, our intra-view radial attention with $H$ heads is formulated as 
\begin{equation}
    Q_R = \mathcal{F}_i W_q, \quad K_R = \mathcal{F}_i W_k, \quad V_R = \mathcal{F}_i W_v,
\end{equation}
\begin{equation}
    A^{(h)} = \sigma\big(\frac{Q_R^{(h)}K_R^{(h)T}}{\sqrt{d_k}}\big) V_R^{(h)} , h = 1,..., H  ,
\end{equation}
\begin{equation}
    f(Q_R, K_R, V_R) = \text{Concat}(A^{(1)}, ..., A^{(H)}) W_o,
\end{equation}
where $\sigma(\cdot)$ denotes the softmax function, and $d_k = C/H$ is the dimension of each head. $A^{(h)}$ denotes the embedding output from the $h$-th attention head, $Q_R^{(h)}, K_R^{(h)}, V_R^{(h)} \in \mathbb{R}^{N \times d_k}$ denote query, key, and value of radial embeddings respectively. $W_q, W_k, W_v, W_o \in \mathbb{R}^{C\times C}$ are the projection matrices. Then, we obtain a refined $\mathcal{F}^{'}_i$ as 
\begin{equation}
    \mathcal{F}^{'}_i = f(Q_R, K_R, V_R),
\end{equation}
which contains global semantic-aware patterns along the reprojected ray in $i$-th view. Similarly, we apply this intra-view radial attention module to each $\mathcal{F}_i (i=1,...,m)$ to refine the 3D contextual space, denoted as $\mathcal{M}^{'} = [\mathcal{F}^{'}_1, \mathcal{F}^{'}_2, ..., \mathcal{F}^{'}_m]^T$.


\noindent \textbf{Cross-view Sparse Attention.}
After the intra-view radial attention module, we decompose $\mathcal{M}^{'}$ cross multiple views and rewrite $\mathcal{M}^{'} = [\mathcal{F}^{'}_{\mathbf{r}^{*}(1)}, ..., \mathcal{F}^{'}_{\mathbf{r}^{*}(N)}]$, where $\mathcal{F}^{'}_{\mathbf{{r}^{*}(i)}} = [\mathcal{F}_1^{'}(\mathbf{r}^{*}(z_i)),...,\mathcal{F}_m^{'}(\mathbf{r}^{*}(z_i))]^T$, which encodes the global ray-based feature in each view. Aiming to exploit semantic information from multiple views with their respective significance which is sparse, we put $\mathcal{M}^{'}$ to the cross-view sparse attention module. Following the predefined notation, we compute the $\mathcal{F}^{''}_{\mathbf{{r}^{*}(i)}}$ from
\begin{equation}
    \mathcal{F}^{''}_{\mathbf{{r}^{*}(i)}} = f(\mathcal{F}^{'}_{\mathbf{{r}^{*}(i)}} \widetilde{W_q} , \mathcal{F}^{'}_{\mathbf{{r}^{*}(i)}} \widetilde{W_k} , \mathcal{F}^{'}_{\mathbf{{r}^{*}(i)}} \widetilde{W_v} ),
\end{equation}
where $\widetilde{W_q}, \widetilde{W_k}, \widetilde{W_v}$ are the projection matrices in cross-view sparse attention. Therefore, we get our final 3D contextual space $\mathcal{M}^{''} = [\mathcal{F}^{''}_{\mathbf{r}^{*}(1)}, ..., \mathcal{F}^{''}_{\mathbf{r}^{*}(N)}]$, which collects dense semantic connections around the query ray. 

%\noindent \textbf{discussion}

\subsection{Semantic Ray}
\label{sec:semantic-ray}
\noindent \textbf{Semantic Ray Construction.} As done in the previous pipeline, we have built a semantic-aware space $\mathcal{M}^{''} = [\mathcal{F}^{''}_1, \mathcal{F}^{''}_2, ..., \mathcal{F}^{''}_m]^T$ which encodes refined 3D contextual patterns around the light ray emitted from the query view. To construct our final semantic ray from $\mathcal{M}^{''}$ and better learn the semantic consistency along the ray, we introduce a \textit{Semantic-aware Weight Network} to rescore the significance of each source view. Then, we can assign distinct weights to different views and compute the final semantic ray $\mathbf{s}$ as
\begin{equation}
    \mathbf{s} = w_1 \mathcal{F}_1^{''} + w_2 \mathcal{F}_2^{''} + ... + w_m \mathcal{F}_m^{''},
\end{equation}
\small
\begin{equation}
     \mathbf{w} \in \mathbb{C}(\tau):=\left\{\mathbf{w}: 0<\frac{\tau}{m}<w_i<\frac{1}{\tau m}, \sum_{i=1}^m w_i=1\right\},
\end{equation}
\normalsize
where $\mathbf{w}$ is the view reweighting vector with length $m$ indicating the importance of source views, and $\tau$ is the small constant with $\tau > 0$. The deviation of the weight distribution from the uniform distribution is bound by the hyperparameter $\tau$, which keeps the semantic consistency instead of bias across views.

\noindent \textbf{Semantic Field Rendering.} Finally, we use the rendering scheme introduced in NeRF to render semantic logits from the ray $\mathbf{r}$ with N sampled points, namely $\{ z_k\}_{k=1}^N$. The semantic logit $\hat{\mathbf{S}}(\mathbf{r})$ is defined as
\begin{equation}
    \hat{\mathbf{S}}(\mathbf{r}) = \sum_{k=1}^{N} T(z_k) \{1- \text{exp}(-\sigma(z_k) \delta_k)\} \mathbf{s}(z_k),
\end{equation}
where $\quad T(z_k) = \exp(- \sum_{k^{'}=1}^{k-1} \sigma(z_k) \delta_k)$, $\delta_k = z_{k+1} - z_{k}$ is the distance between two adjacent quadrature points along the semantic ray and $\sigma$ is predicted by a \textit{Geometry-aware Network}.

\begin{table*}[!t]

	\centering
        \resizebox{\textwidth}{!}{
	\begin{tabular}{lccccccc}
		\toprule
		\multirow{2}{*}{Method} & \multirow{2}{*}{Settings} & \multicolumn{3}{c}{Synthetic Data (Replica \cite{replica}) } & \multicolumn{3}{c}{Real Data (ScanNet \cite{scannet})  } \\
		
		\cmidrule(lr){3-5}\cmidrule(lr){6-8}
		
		&& \multicolumn{1}{c}{mIoU$\uparrow$} &\multicolumn{1}{c}{Total Acc$\uparrow$} &\multicolumn{1}{c}{Avg Acc$\uparrow$} &\multicolumn{1}{c}{mIoU$\uparrow$} &\multicolumn{1}{c}{Total Acc$\uparrow$} &\multicolumn{1}{c}{Avg Acc$\uparrow$}\\

        \midrule
            MVSNeRF~\cite{mvsNeRF} + Semantic Head  & \multirow{3}{*}{\shortstack{Generalization}}  & 23.41 & 54.25 & 33.70 & 39.82 & 60.01 & 46.01 \\
		  NeuRay~\cite{neuray} + Semantic Head  &  & 35.90 & 69.35 & 43.97 & 51.03 & 77.61 & 57.12 \\
		  \textbf{S-Ray} (\textbf{Ours}) & & \textbf{41.59} & \textbf{70.51} & \textbf{47.19} & \textbf{57.15} & \textbf{78.24} & \textbf{62.55} \\
           
  
		\midrule
		 Semantic-NeRF~\cite{semantic-nerf}    &  \multirow{4}{*}{\shortstack{Finetuning}} & 75.06 & 94.36 & 70.20 & \textbf{91.24} & 97.54 & 93.89  \\ % 5000 steps
		MVSNeRF \cite{mvsNeRF} + Semantic Head$_{ft}$ &  & 53.77  & 79.48 & 62.85 &  55.26 & 76.25 & 69.70 \\


        NeuRay\cite{neuray} + Semantic Head$_{ft}$ &  & 63.73  & 85.54 & 70.05 & 77.48 & 91.56 & 81.04 \\
		\textbf{S-Ray}$_{ft}$ (\textbf{Ours})  &  & \textbf{75.96} & \textbf{96.38} & \textbf{80.81} & 91.08 & \textbf{98.20} & \textbf{93.97} \\  % 5000 steps

		\bottomrule
	\end{tabular}}
\rule{0pt}{0.01pt}
\vspace{-3mm}
\caption{\textbf{Quantitative comparison.} We show averaged results of mIoU, Total Acc, and Average Acc (higher is better) as explained in Sec.~\ref{sec:exp_set}. On the top, we compare S-Ray (Ours) with NeuRay~\cite{neuray}+semantic head and MVSNeRF~\cite{mvsNeRF}+semantic head with direct network inference. On the bottom, we show our results with only 10 minutes of optimization. }
\label{tb:rendering}
\end{table*}

\noindent \textbf{Network Training.} More specifically, we discuss the formulation of the semantic loss functions. We apply our S-Ray with a set of RGB images with known camera parameters denoted by $\mathcal{I}$. 
The losses are computed for the set of all rays denoted as $\mathcal{R}$, which is emitted from the query image $I \in \mathcal{I}$. The semantic loss is computed as multi-class cross-entropy loss to encourage the rendered semantic labels to be consistent with the provided labels, where $1 \leq l \leq L$ denotes the class index
\begin{equation}
    \mathcal{L}_{sem}(I)=-\sum_{\mathbf{r} \in \mathcal{R}}\left[\sum_{l=1}^L p^l(\mathbf{r}) \log \hat{p}^l(\mathbf{r})\right],
    \label{eq:Lsem}
\end{equation}
where $p^{l}, \hat{p}^l$ are the multi-class semantic probability at class $l$ of the ground truth map. Unlike Semantic-NeRF \cite{semantic-nerf} which needs heavy prior training for the radiance only in a single scene, we can train our S-Ray Network with semantic supervision in multiple scenes simultaneously and fast generalizes to a novel scene. 

\subsection{Discussion and Implementation}
\noindent \textbf{Discussion with GPNR~\cite{GPNR}. }
The recent work GPNR~\cite{GPNR} shares a similar motivation by aggregating features along epipolar line. It is proposed for color rendering with carefully-designed positional encoding to encode information of view direction, camera pose, and location. In contrast, we focus on learning a generalizable semantic field for semantic rendering through merely image features. In this sense, S-Ray creates a neat design space without any positional encoding engineering. While GPNR requires training 24 hours on 32 TPUs, S-Ray only needs a single RTX3090-Ti GPU with similar training time. 

\noindent \textbf{Implementation details. }
Given multiple views of a scene, we construct a training pair of source and query view by first randomly selecting a target view, and sampling $m$ nearby but sparse views as source views. We follow \cite{neuray} to build our sampling strategy, which simulates various view densities during training, thus helping the network generalize across view densities. We implement our model in PyTorch \cite{pytorch} and train it end-to-end on a single RTX3090-Ti GPU with 24GB memory. The batch size of rays is set to 1024 and our S-Ray is trained for $250k$ steps using Adam \cite{Adam} with an initial learning rate of $1e-3$ decaying to $5e-5$. S-Ray is able to generalize well to novel scenes and can also be finetuned per scene using the same objective in (\ref{eq:Lsem}). More details of network training, architecture design, and hyperparameter settings can be found in the supplementary.
