\section*{Additional experiments and analysis}
\noindent \textbf{More discussion of loss function.} When adding color rendering, it is interesting to see the effect of the weighting factor, thus conducting the following experiments in Table~\ref{tab:weight-factor}. We observe that color rendering can benefit semantics but color rendering is not sensitive to semantics. Furthermore, Table~\ref{tab:weight-factor} shows that the semantic loss alone can also drive our model to learn reasonable contextual geometry for semantic information as visualized in Figure~\ref{fig: vis}.

\begin{table}[!h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccccc}
\hline
$\lambda_1/\lambda_2$ & 1/0   & 0.75/0.25 & 0.5/0.5 & 0.25/0.75 & 0/1   \\ \hline
PSNR                  & 17.49 & 25.26     & 25.35   & 26.24     & 26.57 \\
mIoU(\%)              & 55.10 & 56.51     & 57.15   & 58.12     & 3.62  \\ \hline
\end{tabular}%
}
\vspace{-3mm}
\caption{Different weighting factors effect under ScanNet~\cite{scannet} generalization settings. }
\label{tab:weight-factor}
\end{table}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{cvpr2023/Figures/rebuttal.fig1.pdf}
    \vspace{-6mm}
    \caption{Visualization of 2D CNN features from ResUnet and intra-view attention map. It shows that our ResUnet can help S-Ray learn reasonable geometry for contextual semantics and the intra-view attention map is closely related to the visibility.}
    \label{fig: vis}
\end{figure}

\noindent \textbf{Effectiveness of the CRA module.} To further validate the computational effectiveness of our Cross-Reprojection Attention (CRA) module, we provide the comparisons with Dense Attention in FLOPs and Memory usage. 



\begin{table}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
         Description & GFLOPs & mIoU(\%) &  Total Acc(\%)\\
         \midrule
        %  NeRF~\cite{mildenhall2020nerf} & 10k  & $\sim$30min &       \\
        w/o CRA      & 0 & 76.30  & 86.02 & \\
         Dense Attention &  10.25 & 90.46  & 94.52  & \\
         only intra-view Att  & 3.05  & 81.24 & 89.58 & \\
         only cross-view Att  & 2.35  & 87.01 & 93.34 & \\
         full CRA     & \textbf{5.40} & \textbf{91.08}  & \textbf{98.20} & \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{Performance on real data \cite{scannet} for different settings of Cross-Reprojection Attention module (CRA). FLOPs increments are estimated for the input of $1024 \times 64 \times 8 \times 32$. }
    \label{tab:flop_compare}
\end{table}

\begin{table}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
         Description & Memory(MB) & mIoU(\%) &  Total Acc(\%)\\
         \midrule
        %  NeRF~\cite{mildenhall2020nerf} & 10k  & $\sim$30min &       \\
        w/o CRA      & 0 &  76.30 &86.02  & \\
         Dense Attention &  17647 & 90.46 & 94.52 & \\
         only intra-view Att  & 3899  & 81.24 & 89.58 & \\
         only cross-view Att  &  1583 & 87.01 & 93.34 & \\
          full CRA     & \textbf{4143} & \textbf{91.08}  & \textbf{98.20} & \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{Performance on real data \cite{scannet} for different settings of Cross-Reprojection Attention module (CRA). Memory increments are estimated for an input of $1024 \times 64 \times 8 \times 32$.}
    \label{tab:mem_compare}
\end{table}

Table~\ref{tab:flop_compare} and Table~\ref{tab:mem_compare} show the computational performance of real data by adopting different settings of our Cross-Reprojection Attention (CRA) module. We observe that directly applying the dense attention over multi-view reprojected rays suffers from heavy computational cost and high GPU memory. In contrast, our CRA module can achieve the comparable performance of dense attention with friendly GPU memory and high computational efficiency. Specifically, the design of CRA can improve the performance by $47.3\%$ in FlOPs and $76.5\%$ in GPU memory. These results prove that the proposed cross-reprojection attention can achieve high mIoU and total accuracy by capturing dense and global contextual information with computational efficiency.
\newline
\newline
\noindent \textbf{Semantic ray construction.} To construct the final semantic ray in Section 3.4 of our paper, we assign distinct weights to different source views and compute the semantic ray with semantic consistency. Specifically, we design the \textit{Semantic-aware Weight Network} to rescore the significance of each view with a hyperparameter $\tau$, as 
\begin{equation}
     \mathbf{w} \in \mathbb{C}(\tau):=\left\{\mathbf{w}: 0<\frac{\tau}{m}<w_i<\frac{1}{\tau m}, \sum_{i=1}^m w_i=1\right\},
\end{equation}
where $\mathbf{w}$ is the view reweighting vector with length $m$ indicating the importance of source views. Instead of mean aggregation which ignores the different significance of different source views, the hyperparameter $\tau$ controls the semantic awareness of each view. The effectiveness of $\tau$ can be seen in Table~\ref{tab:tau}.

\begin{table}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
         hyperparameter $\tau$ & mIoU(\%) & Total Accuracy(\%)  & Average Accuracy(\%)\\
         \midrule
        %  NeRF~\cite{mildenhall2020nerf} & 10k  & $\sim$30min &       \\
        1    & 54.21 &   77.13  &  59.05\\
         0.8 &  56.33 & 78.01 & 60.37& \\
         0.7  & \textbf{57.15} & \textbf{78.24} & \textbf{62.55} & \\
         0.5 & 55.70  & 76.64 & 60.80 & \\
          0.2  &54.03  & 77.25 &  61.34 & \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{Performance on real data \cite{scannet} for different settings of hyperparameter $\tau$ in test set.}
    \label{tab:tau}
\end{table}

From Table~\ref{tab:tau}, we observe that we can improve the performance of semantic segmentation by assigning different weights to each source view with hyperparameter $\tau$. Note that $\tau = 1$ means the mean aggregation operation.
\newline
\newline
\noindent \textbf{Training process.}  Given multiple views of a scene, we construct a training pair of source and query view (\ie, target view) by first randomly selecting a target view, and sampling 8 nearby but sparse views as source views. We follow \cite{neuray} to build our sampling strategy. The performance of our method in different training iterations can be found in Table~\ref{tab:training_detail}. The results show that we only require 260k iterations for 20 hours to train our S-Ray over 60 different real scenes, which demonstrates the efficiency and effectiveness of our network design.
\newline
\newline
\noindent \textbf{More comparisons with Semantic-NeRF.} To further show our strong and fast generalizability in a novel unseen scene, we compare our performance with Semantic-NeRF~\cite{semantic-nerf} in per-scene optimization. The results are shown in Table~\ref{tab:compare_semanticnerf}. While Semantic-NeRF~\cite{semantic-nerf} needs to train one independent model for an unseen scene, we observe that our network S-Ray can effectively generalize across unseen scenes. What's more, our direct result can be improved by fine-tuning on more images for only 10 min (2k iterations), which achieves comparable quality with Semantic-NeRF for 100k iterations per-scene optimization. Moreover, Semantic-NeRF shows very limited generalizability by first generating pseudo semantic labels for an unseen scene with a pretrained model, and then training on this scene with the pseudo labels. In this way, Semantic-NeRF is able to apply to new scenes without GT labels. In contrast, our S-Ray provides stronger generalization ability by enabling directly test on unseen scenes. We provide additional experiments in Table~\ref{tab:pseudo}.
\newline
\newline
\noindent \textbf{Comparison with GPNR.} The recent work GPNR~\cite{GPNR} also generates novel views from unseen scenes by enabling cross-view communication through the attention mechanism, which makes it a bit similar to our S-Ray. To further justify the motivation and novelty, we summarize several key differences as follows. \textbf{Tasks:} GPNR utilizes fully attention-based architecture for color rendering while our S-Ray focuses on learning a generalizable semantic field for semantic rendering. \textbf{Embeddings:} GPNR applies three forms of positional encoding to encode the information of location, camera pose, view direction, etc. In contrast, our proposed S-Ray only leverages image features with point coordinates without any handcrafted feature engineering. In this sense, our S-Ray enjoys a simpler design in a more efficient manner. \textbf{Training cost.} While GPNR requires training 24 hours on 32 TPUs, S-Ray only needs a single RTX3090-Ti GPU with similar training time.
\begin{table}[!h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccccccc}
\hline
       & w/o ft & ft 5k(p) & ft 5k(gt) & ft 50k(p) & ft 50k(gt) & ft converge(p) & ft converge(gt) \\ \hline
S-NeRF & N/A    & 78.59    & 86.32     & 85.64     & 91.33      & 92.10          & 95.24           \\
S-Ray  & 77.82  & 88.07    & 93.40     & 91.25     & 95.15      & 92.43          & 95.39           \\ \hline
\end{tabular}%
}
\vspace{-3mm}
\caption{More mIoU comparisons with SemanticNeRF(S-NeRF) in the scene0160-01 from ScanNet. Same with S-NeRF, we choose pretrained DeepLabV3+~\cite{deeplabv3} to generate pseudo semantic labels for finetuning. ``p'' means finetuning with pseudo labels, and ``gt'' means finetuning with ground truth.}

\label{tab:pseudo}
\end{table}
\newline
\newline
\noindent \textbf{More discussion for reconstruction quality.} To further demonstrate the reconstruction quality and generalizability of S-Ray, we evaluate S-Ray with NeuRay~\cite{neuray}, MVSNeRF~\cite{mvsNeRF}, and IBR-Net~\cite{IBRnet} on two typical benchmark datasets (\ie, Real Forward-facing~\cite{mildenhall2019local} and Realistic Synthetic 360$^\circ$~\cite{NeRF}) in Table~\ref{tab:benchmark}. In general, Table~\ref{tab:benchmark} shows our Cross-Reprojection Attention module is also useful for generalizable NeRFs with out semantic supervision.
\begin{table}[!h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccccc}
\hline
            & \multicolumn{3}{c}{Realistic Synthetic 360Â°}        & \multicolumn{3}{c}{Real Forward-facing}             \\ \hline
Method      & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\ \hline
MVSNeRF & 23.46 & 0.851 & 0.172 & 22.93 & 0.794 & 0.260  \\
IBRNet  & 24.52 & 0.874 & 0.158 & 24.17 & 0.802 & 0.215 \\
NeuRay  & 26.73 & 0.908 & 0.123 & 25.35 & 0.824 & 0.198 \\
S-Ray(Ours) & \textbf{26.84} & \textbf{0.917} & \textbf{0.115}    & \textbf{25.68} & \textbf{0.833} & \textbf{0.180}     \\ \hline
\end{tabular}%
}
\vspace{-3mm}
\caption{Quantitative comparisons of scene rendering in the generalization setting. All generalization methods including our method are pretrained on the same scenes and tested on unseen test scenes. }
\label{tab:benchmark}
\end{table}
While the three mentioned methods in Table~\ref{tab:benchmark} and our method are image-based rendering, the main difference lies in how to extract useful features: (a) MVS-NeRF leverages cost volume to extract geometry features, which benefits the acquisition of density; IBRNet performs feature attention on rays in 3D space and NeuRay further extracts the occlusion-aware features by explicitly modeling occlusion. Their features are sparse in 3D space but sufficient for color rendering. (b) Our method goes back to the 2D reprojection space and obtains dense attention by cascading two sparse attention modules, thus extracting rich semantic and geometric features. A key point is that we apply a ResUnet segmentation network fro context feature extraction to get semantic priors, which is not present in the previous methods.
\newline
\newline
\noindent \textbf{Disscusion of the number of source views.} 
\begin{table}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
         $N_s$ & mIoU(\%) & Total Acc(\%)  & Avg Acc(\%) & PSNR & SSIM\\
         \midrule
        %  NeRF~\cite{mildenhall2020nerf} & 10k  & $\sim$30min &       \\
        1    & 67.55 &  86.15   & 73.73 & 26.47 & 0.9077 \\
         4&  75.41 & 90.51  & 81.06 & 30.90 & 0.9368 \\
         8  & {79.97} & {93.06} & {84.92}  &  \textbf{29.52} & \textbf{0.9106}\\
         12 & {83.21} & 93.89 & 88.07 & 28.57 & 0.8969 \\
          16  & \textbf{84.84}& \textbf{94.33} & \textbf{89.78} & 27.85 & 0.8859 \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{Performance(mIoU, Total accuracy, Average accuracy, PSNR, SSIM) on the real data scene~\cite{scannet} wiht different source view numbers $N_s$. } 
    \label{tab:Ns_views}
\end{table}
We observe that using more source views on our S-Ray model can improve semantic rendering quality. The results are shown in Table~\ref{tab:Ns_views}. The reason is that adding more reference views in training means leveraging more contextual information for semantic feature learning to build a larger 3D contextual space and reconstruct the final semantic ray, which improves the view consistency and accuracy of semantic segmentation.
\newline
\newline
\noindent \textbf{Disccusion of semantic-aware weight.} In semantic ray construction, we learn the view reweighting vector $\mathbf{w}$ to rescore the significance of each source view. To further demonstrate the effectiveness of this rescore strategy, we show the example in Figure~\ref{fig:new_weight}. The results show that $\mathbf{w}$ can distinct the different significance of different source views to the query semantic ray.

