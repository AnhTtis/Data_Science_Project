\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\begin{document}

% \title{GP3D: Generalized Pose Estimation in 3D Point Clouds:
% A case study on bin picking}
\title{KeyMatchNet: Zero-Shot Pose Estimation in 3D Point Clouds by Generalized Key-Point Matching} %:
% Reduce and reuse
% A case study on bin picking}
% \title{KeyMatchNet: Generalized Pose Estimation in 3D Point Clouds:
% A case study on bin picking}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\author{\IEEEauthorblockN{1\textsuperscript{st} Frederik Hagelskj√¶r}
\IEEEauthorblockA{\textit{SDU Robotics} \\
\textit{University of Southern Denmark}\\
Odense, Denmark \\
frhag@mmmi.sdu.dk}
}

\maketitle

\begin{abstract}
In this paper, we present KeyMatchNet, a novel network for zero-shot pose estimation in 3D point clouds. The method generalizes to new objects by using not only the scene point cloud as input, but also the object point cloud. The network is trained to match key-points from the object with scene-points, and these matches are then used for pose estimation. By having a generalized network we avoid the need for training new models for novel objects, thus significantly decreasing the computational requirements of the method. We demonstrate that the trained network is able to accurately estimate poses for novel objects, and demonstrate the ability of the network to perform outside of the trained class. We believe that the presented method is valuable for many real-world scenarios. Code, trained network and dataset is available at \url{https://github.com/fhagelskjaer/keymatchnet}
\end{abstract}

\begin{IEEEkeywords}
zero-shot, pose estimation, point cloud
\end{IEEEkeywords}

\begin{figure}[ht]
\begin{center}
   \includegraphics[width=0.75\linewidth]{gfx/example_net_key_small.drawio.pdf}
     \vspace{-3mm}
\end{center}
   \caption{Illustration of the pose estimation method. 
The input to the network is a scene point cloud and an object point cloud with keypoints spread evenly over the object. The network output is both instance segmentation and keypoint predictions, which are combined to provide predictions only for the object. Finally these predictions are used in RANSAC for pose estimation.
The striped keypoint prediction pattern is the result of the objects' rotational symmetry. 
   }
  \vspace{-6mm}
\label{fig:example}
\end{figure}


%%%%%%%%% BODY TEXT
\section{Introduction}
Pose estimation enables greater flexibility in robotics as new objects can be manipulated without the need for mechanical fixtures or teaching specific robot positions. This enables shorter changeover times and faster adaptation to production demands. However, the set-up of computer vision algorithms can itself be a very time-consuming task \cite{hagelskjaer2018does}. There is, therefore, great interest in pose estimation solutions with simple set-ups. Deep learning has allowed learning the specifics of the object and the scenario, thus giving much better performance than human fine-tuning \cite{hodavn2020bop, sundermeyer2023bop}. However, collecting the data for training the deep neural networks can be very time-consuming, limiting the usability. To avoid this data collection, synthetic data has gained widespread use \cite{hodavn2020bop}. But generating large amounts of data, and then training the networks for each new object is computationally expensive and increases set-up time. To address these problems, we introduce KeyMatchNet, a neural network built on the principles of reusability. The reusability consists of two parts. Firstly, rather than training a single model for each object, a zero-shot pose estimation algorithm is built. Instead of learning specific features from the object, the network is trained to match key-points from the object to the scene. Thus, the network input is both scene and object information. For a novel object, the network can be reused without re-training. Additionally, compared with similar methods \cite{gou2022unseen} our network is split into parallel computation of object- and scene-features. This allows for pre-computing the object features, giving a significant speed-up at run-time. Additionally, if multiple objects are pose estimated the  scene feature can be reused.
%
%if the method is pose estimating multiple objects, the scene feature can be reused for multiple object features. 

Zero-shot pose estimation methods are, generally, less accurate compared with methods trained for specific objects \cite{labbe2022megapose}, as they integrate object information into the model. However, these methods use synthetic data and therefore do not integrate scene information. As our model is reused for multiple objects, we can use real data and integrate scene information into the network. For many set-ups this much better fits the tasks at hand. New objects can be introduced faster as training is not required, and the power consumption for training will be removed. An example is robotic work-cells where the set-up is permanent while new objects are often introduced. In this paper we focus on the task of bin picking with homogeneous bins, which is a difficult challenge that often occurs in industry \cite{yokokohji2019assembly}. The homogeneous bin also removes the need for object detection and allows us to only focus on pose estimation. We recognize that the general approach as shown in e.g., \cite{sundermeyer2023bop} has huge importance, but also state it is not the best solution for all tasks. In this paper we show that generalized pose estimation can obtain very good performance when restricting the scenario. We believe that these results invite further research into this topic, as creating flexible set-ups with lower power consumption is an important topic. 



%-------------------------------------------------------------------------

\begin{figure*}[ht]
\begin{center}
   \includegraphics[angle=90, width=0.95\linewidth]{gfx/network_reverse.drawio.pdf}
   \caption{The network structure of the developed method. Object and scene features are initially computed independently. This allows us to compute the object feature once and match it to multiple scene point clouds. The Segmentation and Keypoint Prediction outputs are shown along with the combination. It is seen that the Keypoint Predictions are calculated for both objects in the scene and the instance segmentation filters out the other object.} % The "Edge Feature" is specific to DGCNN and computes and concatenates neighbor points.}
\label{fig:network}
     \vspace{-6mm}
\end{center}
\end{figure*}

% \begin{figure}[ht]
% \begin{center}
%    \includegraphics[width=0.95\linewidth]{gfx/network_reverse.drawio.pdf}
%    \caption{The network structure of the developed method. Object and scene features are initially computed independently. This allows us to compute the object feature once and match it to multiple scene point clouds. The Segmentation and Keypoint Prediction outputs are shown along with the combination. It is seen that the Keypoint Predictions are calculated for both objects in the scene and the instance segmentation filters out the other object.} % The "Edge Feature" is specific to DGCNN and computes and concatenates neighbor points.}
% \label{fig:network}
%      \vspace{-6mm}
% \end{center}
% \end{figure}

\section{Related Works}

Visual pose estimation has been studied for many years, and several different approaches have been developed.
% Many different approaches for visual pose estimation have been developed.
% has is an important topic, and many different approaches have been developed.

\textbf{Classic Pose Estimation:} The classic pose estimation approach is based on matching features between the scene and object, and computing the pose by e.g. RANSAC \cite{fischler1981random}. The matches are computed using handcrafted features, which is generally computed in 3D point clouds. A huge amount of handcrafted features have been developed \cite{guo2016comprehensive}, with Fast Point Feature Histograms (FPFH) \cite{rusu2009fast} being one of the best performing features.

\textbf{Deep Learning Based:} Generally deep learning based methods are based on color information \cite{hodavn2020bop, sundermeyer2023bop}. This is possibly a result of many deep learning based methods developed for this space, with huge pre-trained networks available. These methods have vastly outperformed the classical methods, however, a network is often trained per object \cite{hodavn2020bop, sundermeyer2023bop}. 
Deep learning for pose estimation has also been performed in point clouds with methods such as PointVoteNet \cite{hagelskjaer2020pointvotenet}. We base our method on PointVoteNet, but only train a single network for all objects.
% but were that method trained a network per object, we only train a single network. 

\textbf{Generalized Pose Estimation:} Several approaches have been developed for generalized pose estimation.
As in the deep learning based pose estimation, the field of generalized pose estimation is also dominated by color-based methods \cite{shugurov2022osop, labbe2022megapose, he2022fs6d}. The general approach for these methods is to match templates of the object with the real image. These templates can either be generated synthetically as in \cite{nguyen2022templates} or with a few real images as in FS6D \cite{he2022fs6d}. The same approach have also been used for tracking of unknown objects \cite{nguyen2022pizza}. 
MegaPose6D \cite{labbe2022megapose} is a notable example where the network is trained on a huge dataset with 2 million images. 
The method most similar to ours is a point cloud based method \cite{gou2022unseen}. It also uses the object point cloud as input along with the scene point cloud. 
The method also employs a segmentation step as in our method, however the pose estimation  part of the method whereas our approach matches specific keypoint.
Several other differences are also present compared with our approach; the method includes color information which is often much more difficult to obtain. It does not limit the scenario and thus does not obtain the increased performance from this. And it does not separate the object and scene features, thus the object features must be computed for each pose estimation.



% TODO someone else mentioned out of class

% \subsection{Previous Approach}

% The input in this approach is a scene point cloud, and the output are matches to the object cad model.



% The network can then be used with e.g. RANSAC \cite{fischler1981random} to obtain a pose estimation for the object.

% The network can learn predict correct key point votes even in the face of noise and occlusion. The network learns to focus on the correct features as compared with classically engineered features.

% The downside of this approach is that to obtain a trained network large amount of training data is necessary. This increases the set-up time of the system and 

% This problem can be mitigated by using synthetic data, however, while removing the need for manual work there is still a time consumption for generating the data and training the network. Additionally training a network for each component adds an electricity consumption to the pipeline.





\section{Method}
% 
% What is the data?
% We present the first depth only generalized pose estimation method. 
The developed method is a network that matches scene-points to object key-points. These matches are then feed into a pose estimation solver, in our case RANSAC. 
The scene and object data are point clouds without any color information.
Color information is not used as it is seldom available for CAD (Computer Aided Design) models and would limit the usability of the method.
%
The network structure is made with two parallel components computing both the scene and the object features independently. The network structure is shown in Fig.~\ref{fig:network}. 
% 
The scene features are computed for all points using the standard DGCNN \cite{wang2019dynamic}. However, for the object only the key-point features are computed.
% as only the keypoints from the object point cloud is needed, a down-sampling is performed. 
Thus, after the second neighbor computation the object features are down-sampled to only contain the key-points.
% the object point cloud is down-sampled to only contain the keypoints after the second neighbor computation.
For the object feature computation the kNN size is set to the number of key-points,
ensuring that all key-points share information.

% After both object and scene features are computed, they are extended to length  $(n*k)$. The features are then concatenated to create a matrix of size $(n*k x f_{scene}+f_{key})$, where $n$ is the number of scene point, $k$ is the number of object keypoints, $f_{scene}$ is the feature  size for scene points and $f_{key}$ is the feature size for keypoints.

After both object and scene features are computed, they are combined by concatenating each scene point with each key-point.
% 
% This concatenated matrix allow us to compute the match between each scene point and feature point. 
% 
An MLP (Multi Layer Perceptron) then processes each scene-keypoint pair independently and a local maxpool combines all key-point information at the scene point.

% The feature concatenation so matrix size $(n*k x f_{scene}+f_{key})$ and feature computation is then repeated. 
This new scene feature is again combined with the key-point features, and processed by an MLP.
Two different MLPs are then used to compute the segmentation and the keypoint predictions.
To compute the segmentation a local maxpool is applied followed by an MLP which gives segmentation for each point. 
% 
To compute the keypoint matches an MLP is applied to the combined features 
% concatenated matrix which 
giving a score for each scene-keypoint pair.
% , resulting in a $n x k$ matrix. 
A softmax is then applied across the keypoint domain for each scene point, predicting the best scene-keypoint match.
% 
Finally, RANSAC is used with the segmentation and keypoint predictions to compute the final object pose. We employ the vote threshold as in \cite{hagelskjaer2020pointvotenet} to allow multiple keypoint matches at a single scene point. The vote threshold is set to 0.7.

% \subsection{Generating scene data}

% As the scenario is homogeneous bin-picking the detection is vastly simplified. As the bin position is known beforehand, and the contents are homogeneous, any point belongs to an object.
% % 
% Thus we can randomly sample point a point, and this point will belong to the object. By then extracting using a radius set to the object model diagonal, all points in the scene belonging to the object will be obtained. The point cloud is then centered around the sampled point to allow for instance segmentation.
% % 
% The scene point clouds are generated using BlenderProc \cite{denninger2019blenderproc}.

\textbf{Generating scene data:}
As the scenario is homogeneous bin-picking the detection is vastly simplified. By segmenting the known bin, all remaining points belong to objects. As the contents are homogeneous any random point is known to belong to a correct object. 
%
By then extracting a point cloud around the sampled point using the object diagonal, all points in the scene belonging to the object will be obtained. The point cloud is then centered around the sampled point to allow for instance segmentation. 
% 
The scene point clouds are generated using BlenderProc \cite{denninger2019blenderproc}.


% \subsection{Generating object data}
\textbf{Generating object data:}
% 
The object point clouds is generated using Poisson sampling to obtain 2048 evenly sampled points on the surface. Farthest point sampling is then used to obtain the keypoints spread evenly on the object.
%

% \subsection{Computing object features off-line}
\textbf{Computing object features off-line:}
% 
As shown in Fig.~\ref{fig:network} the features computed from the object point cloud are independent of the features computed from the scene point cloud. This is opposed to \cite{gou2022unseen}. This allow us to use the same object feature for multiple pose estimations. The object features can also be computed offline to reduce the run-time and the computational cost.
%
During training the keypoints are continuously re-computed with random initialization, to avoid the network over-fitting to a specific combination.

% \begin{figure*}[htb]
% \begin{center}
%    \includegraphics[width=0.95\linewidth]{gfx/objects.pdf}
% \end{center}
%    \caption{The seven electronic components used for testing the performance.}
% \label{fig:objects}
% \end{figure*}

% \begin{figure*}[htb]
% \begin{center}
%    \includegraphics[width=0.95\linewidth]{gfx/objects_combined.drawio.pdf}
% \end{center}
%    \caption{The seven electronic components used for testing, along with the seven components from the WRS dataset used for out of class testing. }
% \label{fig:objects}
% \end{figure*}


\section{Experiments}
To test the developed method several experiments are performed. A single model is trained, and used for all experiments. The training data consists of 1500 CAD models of electric components introduced in \cite{hagelskjaer2022hand}.
% from an online database of electronic components\footnote{\url{https://www.pcb-3d.com/membership_type/free/}}.
Fifty models excluded from the training to test the networks pose estimation ability on unseen objects.
% 
% The methods pose estimation ability is tested on seven electrical components from a different dataset\cite{hagelskjaer2022hand}.
% To test the 
% The seven test objects are shown at the top of Fig.~\ref{fig:objects}.
% 
Additionally we test the ability of the network on out of class objects. Seven industrial objects from the WRS \cite{yokokohji2019assembly} dataset is used for testing. On this dataset we show the networks ability to generalize to other objects outside of the training scope.
% The objects from the WRS dataset are shown in the bottom of Fig.~\ref{fig:objects}.
% 
% \begin{figure*}[htb]
% \begin{center}
%    \includegraphics[width=0.95\linewidth]{gfx/objects_wrs.pdf}
% \end{center}
%    \caption{The seven components from the WRS dataset used for testing.}
% \label{fig:wrs}
% \end{figure*}
% 
% All point cloud processing and pose estimation is performed using the Open3D framework, \cite{Zhou2018}.
% The network processing was performed using PyTorch \cite{Paszke_PyTorch_An_Imperative_2019}.

\subsection{Network Training}
%
The network was trained on a PC environment with two NVIDIA GeForce RTX 2080 GPUs. The network was trained for 120 epochs lasting approximately six days (141 hours). For each object we generate 160 point clouds giving an epoch size of 232000.
% 
% The 1500 components are split into a 1450/50 train-validation dataset. During each epoch we generate 160 point clouds for each component. Thus each training epoch consists of 232000 point clouds. 
% 
The network is trained with a batch size of 14, 7 on each GPU, using the Adam optimizer \cite{kingma2014adam}, with an initial learning rate of 0.0001. We use a step scheduler with a step size of 20 and the gamma parameter set to 0.7. The loss is calculated with cross entropy using a 0.2/0.8 split for segmentation and keypoint loss. For the keypoint loss only points belonging to the object is used. 
% 
Group norm \cite{wu2018group} with size 32 is used as opposed to Batch Norm as a result of the small batch size.
% 
% To generalize the network Group Norm \cite{wu2018group} with group size 32 is used after each linear layer. Group Norm is used as opposed to Batch Norm as a result of the small batch size. 
Dropout is set to 40 \%, and up to 0.75 \% Gaussian noise is applied to the object and scene point clouds, and 10 \% position shift is applied to the object point cloud.

% Dropout is used for the object features, as the network should not overfit to a specific part of the object, and is used after the to concurrent linear layers. The dropout is set to 40 \%, used after the last two linear layers of the object feature and the first two of the combined feature.
% %
% Additionally, up to 0.75 \% Gaussian noise is applied to the object and scene point clouds, and 10 \% position shift is applied to the object point cloud.



% \subsection{Generating training data}

% During training the object points are projected to compute the segmentation and the keypoints are projected to find the keypoints.

\subsection{Training and test performance}

The performance for the trained networks is show in Tab.~\ref{tab:train}. Performance is shown for both the loss, segmentation accuracy, and keypoint accuracy. We present the performance on the training data both with and without generalization. The network does not appear to overfit to the training data, and actually shows better performance on the test set. 
% For the test data with electronic objects not from the same dataset, the network performs well with a slightly lower performance. 
%
As the objects are symmetric to varying levels, the keypoint accuracy despite being low, still gives good pose estimations. This is seen in Fig.~\ref{fig:example}, where the symmetry of the object results in striped matching of keypoints. However, these matches are still very useful for the pose estimation.

% 0.989851, test avg acc: 0.977308, test key acc: 0.293383
% 0.989006, test avg acc: 0.972449, test key acc: 0.290565

% \begin{tabular}{|l|c|c|c|}
% \hline
% Split & Loss & Seg. Acc & Key. Acc \\
% \hline\hline
% Train (w/ gen.)     & 1.59  & 0.98 & 0.27 \\
% Train (w/o gen.)    & 1.42  & 0.97 & 0.29 \\
% Val                 & 1.33  & 0.96 & 0.31 \\
% Test                & 1.57  & 0.95 & 0.25 \\
% \hline
% \end{tabular}


\begin{table}
\begin{center}
\caption{The loss and accuracy of the network on the different datasets. Results are shown for the training data both with and without the generalization applied during training.}
\begin{tabular}{|l|c|c|c|}
\hline
Split & Train (w/ gen.) & Train (w/o gen.) & Test \\
\hline\hline
Loss      & 1.59 & 1.42 & 1.33 \\ 
Seg. Acc  & 0.98 & 0.97 & 0.96 \\ 
Key. Acc  & 0.27 & 0.29 & 0.31 \\ 
\hline
\end{tabular}
\label{tab:train}
 \vspace{-6mm}
\end{center}
\end{table}

% \begin{table}
% \begin{center}
% \caption{The loss and accuracy of the network on the different datasets. Results are shown for the training data both with and without the generalization applied during training.}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Split & Train (w/ gen.) & Train (w/o gen.) & Val & Test \\
% \hline\hline
% Loss      & 1.59 & 1.42 & 1.33 & 1.57 \\ 
% Seg. Acc  & 0.98 & 0.97 & 0.96 & 0.95 \\ 
% Key. Acc  & 0.27 & 0.29 & 0.31 & 0.25 \\ 
% \hline
% \end{tabular}
% \label{tab:train}
% \end{center}
% \end{table}

% \subsection{Performance for each component}

% To analyze the network, performance for each component is shown in Tab.~\ref{tab:components}. The two objects with the highest performance is "3" and "5". These two object are both very similar to objects in the training data. The most challenging object is "2". The split between the two parts of the object makes it very dissimilar to the training data. The other components perform very well, especially for the segmentation task.


% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|c|c|}
% \hline
% Object & Loss & Seg. Acc & Key. Acc \\
% \hline\hline
% 1 & 1.50 & 0.99 & 0.24 \\
% 2 & 2.03 & 0.81 & 0.16 \\
% 3 & 1.12 & 0.98 & 0.43 \\
% 4 & 1.53 & 0.98 & 0.29 \\
% 5 & 1.42 & 0.95 & 0.31 \\
% 6 & 1.84 & 0.97 & 0.22 \\
% 7 & 1.53 & 0.99 & 0.22 \\
% \hline
% \end{tabular}
% \end{center}
% \caption{Loss and accuracy of each of the individual test objects.}
% \label{tab:components}
% \end{table}

% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|c|c|c|c|c|c|}
% \hline
% Object   & 1    & 2    & 3    & 4    & 5    & 6    & 7    \\ \hline 
% \hline
% Loss     & 1.50 & 2.03 & 1.12 & 1.53 & 1.42 & 1.84 & 1.53 \\ 
% Seg. Acc & 0.99 & 0.81 & 0.98 & 0.98 & 0.95 & 0.97 & 0.99 \\ 
% Key. Acc & 0.24 & 0.16 & 0.43 & 0.29 & 0.31 & 0.22 & 0.22 \\ \hline
% \end{tabular}
% \end{center}
% \caption{Loss and accuracy of each of the individual test objects.}
% \label{tab:components}
% \end{table}



\subsection{Pose Estimation Performance}

To test the pose estimation accuracy of the developed method we compare it with the classic pose estimation method FPFH \cite{rusu2009fast}. For both methods, RANSAC is used \cite{fischler1981random}. The performance is measured by the ADI score, as it is well suited for the symmetric objects in the dataset. The classic method obtains a $0.57\%$ mean accuracy whereas our method obtains a $0.95\%$ mean accuracy. Our method thus vastly outperforms the classic pose estimation method.

% To further test the robustness of the system, varying levels of noise is added to the points clouds. The results are shown in Tab.~\ref{tab:classic}. It can be seen that our method outperforms the classic method for all objects. 
% When adding noise the difference becomes even more pronounced. However, for the $5\%$ noise the performance also drops significantly for our method.

% \begin{table}
% \begin{center}
% \caption{Pose estimation accuracy for each of the test components. A comparison with classic pose estimation is shown. Results are also shown with noise added to the scene point cloud.}
%     \begin{tabular}{|l|c|c|c|c|c|c|c|}
%     \hline
%     Object  & 1  & 2  & 3  & 4  & 5  & 6  & 7  \\ \hline
%     \hline
%     % Ours    & 77 & 66 & 79 & 64 & 70 & 74 & 76 \\ 
%     % Classic & 45 & 47 & 47 & 16 & 28 & 55 & 39 \\ 
%     % Ours $1\%$ noise   & 73 & 66 & 80 & 56 & 69 & 66 & 74 \\ 
%     % Classic $1\%$ noise & 42 & 46 & 39 & 19 & 23 & 50 & 24 \\
%     % Ours $5\%$ noise   & 42 & 48 & 74 & 49 & 66 & 67 & 51 \\ 
%     % Classic $5\%$ noise & 28 & 35 & 30 & 15 & 40 & 46 & 18 \\ \hline
%     % Ours    & 0.96 & 0.83 & 0.99 & 0.80 & 0.88 & 0.93 & 0.95 \\ 
%     % Classic & 0.56 & 0.59 & 0.59 & 0.20 & 0.35 & 0.69 & 0.49 \\ 
%     % \hline
%     % Ours $1\%$ noise   & 0.91 & 0.83 & 1.00 & 0.70 & 0.86 & 0.83 & 0.93 \\ 
%     % Classic $1\%$ noise & 0.53 & 0.58 & 0.49 & 0.24 & 0.29 & 0.63 & 0.30 \\
%     % \hline
%     % Ours $5\%$ noise   & 0.53 & 0.60 & 0.93 & 0.61 & 0.83 & 0.84 & 0.64 \\ 
%     % Classic $5\%$ noise & 0.35 & 0.44 & 0.38 & 0.19 & 0.50 & 0.58 & 0.23 \\ \hline
%     Ours                & 0.95 & 0.82 & 0.99 & 0.77 & 0.91 & 0.90 & 0.92 \\
%     Classic             & 0.55 & 0.62 & 0.61 & 0.30 & 0.36 & 0.67 & 0.51 \\
%     \hline
%     Ours $1\%$ noise    & 0.93 & 0.79 & 0.99 & 0.74 & 0.86 & 0.88 & 0.90 \\
%     Classic $1\%$ noise & 0.43 & 0.57 & 0.50 & 0.25 & 0.31 & 0.66 & 0.40 \\
%     \hline
%     Ours $5\%$ noise    & 0.55 & 0.60 & 0.91 & 0.62 & 0.83 & 0.82 & 0.53     \\
%     Classic $5\%$ noise & 0.30 & 0.54 & 0.45 & 0.20 & 0.43 & 0.56 & 0.25 \\
%     \hline
%     \end{tabular}
% \label{tab:classic}
% \end{center}
%   % \vspace{-6mm}
% \end{table}


\begin{table}
\begin{center}
\caption{Pose estimation accuracy on the out-of-class dataset. The objects are numbered according to \cite{yokokohji2019assembly}.
A comparison with classic pose estimation is shown. Results are also shown with noise added to the scene point cloud.
}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
% Object    & 24    & 21    & 20    & 15    & 11    & 09    & 12    \\ \hline
Number   & 4   & 11    & 13    & 7    & 8    & 14    & 5    \\ 
Type    & {\scriptsize Motor} & {\scriptsize Pulley} & {\scriptsize Idler} & {\scriptsize Bearing} & {\scriptsize Shaft} & {\scriptsize Screw} & {\scriptsize Pulley} \\ 
\hline
\hline
Ours & 1.00 & 0.99 & 0.90 & 0.91 & 0.93 & 0.93 & 0.96 \\
Classic & 0.98 & 0.93 & 0.78 & 0.78 & 0.67 & 0.37 & 0.90 \\ \hline

Ours  $1\%$ & 1.00 & 0.99 & 0.89 & 0.94 & 0.93 & 0.92 & 0.96 \\
Cla.  $1\%$ & 0.95 & 0.91 & 0.75 & 0.79 & 0.65 & 0.30 & 0.89 \\ \hline

Ours $5\%$ & 0.97 & 0.98 & 0.87 & 0.94 & 0.82 & 0.73 & 0.95 \\
Cla. $5\%$ & 0.67 & 0.89 & 0.74 & 0.82 & 0.57 & 0.28 & 0.80 \\ \hline

\end{tabular}
\label{tab:wrs}
\end{center}
 \vspace{-4mm}
\end{table}

\begin{figure}[tb]
\begin{center}
   \includegraphics[width=0.99\linewidth]{gfx/multi_objects.drawio.pdf}
\end{center}
   \caption{An illustration of the networks ability to correctly match keypoints from different out of class objects. From the matches a final pose of the object is found. The objects are Number 4 and 11 from the WRS dataset.}
\label{fig:multi}
 \vspace{-5mm}
\end{figure}

% \subsection{Testing out of class}
\textbf{Testing out of class:}
% 
The out-of-class dataset consists of industrial objects, such as motors and pulleys. The objects were used for the WRS assembly challenge held in 2018 \cite{yokokohji2019assembly}. The objects were chosen as they represent an industrial challenge, and because of the variety. To further test the robustness of the system, varying levels of noise is added to the points clouds. The results of the network performance and pose estimation is shown in Tab.~\ref{tab:wrs}. It is seen that our developed method obtains very good performance on these objects that appear quite different compared with the training data. When adding noise the difference becomes even more pronounced. The ability to correctly compute features for both in and out-of-class objects is shown in Fig.~\ref{fig:multi}.



\subsection{Run-time}
% 
% For a single batch with batch size two.
% 
The run-time of the network was tested both with and without computing the object features at run-time. With the object features computed at run-time the processing lasts 14.9 ms. While by pre-computing the features the run-time is only 7.9 ms. The separation of object and scene feature computations is thus a significant speed-up.
% 
% However, for the full system the run-time is currently 84.7 ms. This is mainly related to the RANSAC search, which would be a focus to replace.

% \begin{figure}[tb]
% \begin{center}
%    \includegraphics[width=0.99\linewidth]{gfx/pose_estimation.drawio.pdf}
% \end{center}
%    \caption{Visualization of the pose estimation accuracy despite low keypoint precision. In the central figure the prediction accuracy of the network is visualized. "White" represents correctly predicted background, "red" false negatives and "yellow" false positives. "Blue" is correctly predicted segmentation of the object, but wrong keypoints and "green" is both correct segmentation and keypoint prediction. To the right the pose estimation is shown. }
% \label{fig:pe}
% \end{figure}



% Linemod or Real data?

\section{Conclusion}

This paper presents a novel method for zero-shot pose estimation. The main contributions of the paper is a novel network structure with independent object and scene feature computation, and a scenario-specific approach. This method shows very good generalizability across different objects, including out-of-class objects. This proves the validity of creating object independent networks for specific scenarios, which can be useful for many real world applications.

% This paper presents a novel method for zero-shot pose estimation. The method consists of a novel network structure and a scenario-specific approach. This method shows very good generalizability across different objects, including out-of-class objects. This proves the validity of creating object independent networks for specific scenarios, which can be useful for many real world applications.

In future work, it will be very interesting to test the method with real data, both for training and testing. Additionally, the network could be adapted to perform full pose estimation to simplify the pipeline and improve the run-time. 
% The objects from the MegaPose6D \cite{labbe2022megapose} dataset could also be used to diversify the object types, and test on benchmark datasets. 

% \bibliographystyle{plain} % We choose the "plain" reference style
\bibliographystyle{IEEEtran} % We choose the "plain" reference style
\bibliography{egbib}

% {\small
% \bibliographystyle{ieee_fullname}
% \begin{thebibliography}{00}
% \bibliography{egbib}
% \end{thebibliography}
% }

\end{document}
