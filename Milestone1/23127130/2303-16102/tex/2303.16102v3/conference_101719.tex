\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{GP3D: Generalized Pose Estimation in 3D Point Clouds:
A case study on bin picking}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\author{\IEEEauthorblockN{1\textsuperscript{st} Frederik Hagelskj√¶r}
\IEEEauthorblockA{\textit{SDU Robotics} \\
\textit{University of Southern Denmark}\\
Odense, Denmark \\
frhag@mmmi.sdu.dk}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}
In this paper, we present GP3D, a novel network for generalized pose estimation in 3D point clouds. 
The method generalizes to new objects by using both the scene point cloud and the object point cloud with keypoint indexes as input. The network is trained to match the object keypoints to scene points. 

To address the pose estimation of novel objects we also present a new approach for training pose estimation.
The typical solution is a single model trained for pose estimation of a specific object in any scenario. This has several drawbacks: training a model for each object is time-consuming, energy consuming, and by excluding the scenario information the task becomes more difficult.

In this paper, we present the opposite solution; a scenario-specific pose estimation method for novel objects that do not require retraining. 
The network is trained on 1500 objects and is able to learn a generalized solution.
We demonstrate that the network is able to correctly predict novel objects, and demonstrate the ability of the network to perform outside of the trained class.
We believe that the demonstrated method is a valuable solution for many real-world scenarios.
Code and trained network will be made available after publication.
% 
% In the traditional approach, a method is expected to handle pose estimation in unknown scenarios. While this can be beneficial it also limits the amount of information the network can use and makes the task more difficult.
% 
% By using a network for novel objects, the need to generate training data, train networks, and possibly further fine-tuning is reduced. While reducing the set-up time, providing a pre-trained model for robotic solutions will also allow for less power consumption.
% 
% by successfully performing pose estimations of the industrial objects from the World Robot Challenge. 
\end{abstract}

\begin{IEEEkeywords}
pose estimation, point cloud, deep learning
\end{IEEEkeywords}

\begin{figure}[ht]
\begin{center}
   \includegraphics[width=0.75\linewidth]{gfx/example_net.drawio.pdf}
     \vspace{-3mm}
\end{center}
   \caption{Illustration of the pose estimation method. 
   The input to the network is a scene point cloud and an object point cloud with keypoints spread over the object. The output of the network is object segmentation and keypoint votes.
   Notice how the rotational symmetry makes a striped pattern of keypoint predictions. 
   Finally these votes are used in RANSAC for pose estimation.
   }
  \vspace{-6mm}
\label{fig:example}
\end{figure}


%%%%%%%%% BODY TEXT
\section{Introduction}
% 
Pose estimation enables much greater flexibility in robotics. New objects can be manipulated without the need for mechanical fixtures or teaching specific robot positions. 
This enables a more adaptive production with a shorter changeover time and faster adaptation to production demands.
However, the set-up of computer vision set-ups can itself be a very time-consuming task \cite{hagelskjaer2018does}. There is, therefore, great interest in pose estimation solutions with simple set-ups.
% 
Deep learning has allowed learning the specifics of the object and the scenario thus giving much better performance than human fine-tuning \cite{hodavn2020bop, sundermeyer2023bop}. However, collecting the data for training the deep neural networks can be very time-consuming, thereby limiting the usability. 
%
To avoid this data collection, the use of synthetic data has gained widespread use \cite{hodavn2020bop}. But, this introduces a domain gap between the real world and the training data, which is often handled by adding large amounts of domain randomization during training \cite{sundermeyer2023bop}.
%
In this solution a single model is trained for one object adapted to any scenario. However, we believe that by utilizing the scenario information a single model can be trained for multiple objects. 
%
% The solution is, therefore, generally, a single model trained for pose estimation of a specific object in any scenario. This has several drawbacks; training a model for each object is computationally expensive and excluding the scenario information decreases performance.
% 
% In this paper we present the opposite solution; scenario specific pose estimation for novel objects that does not need retraining. 
% 
% 
For many set-ups this much better fit the tasks at hand. It allow us to train on real data as training is not required for new objects. New objects can be introduced faster as training is not required, and the power consumption for training will be removed.
%
An example is robotic work-cells where the scenario is constant while new objects are introduced. 
In this paper we focus on the task of bin picking with homogeneous bins, which is a difficult challenge that often occurs in industry.
The homogeneous bin, also removes the need for object detection and allow us to only focus on pose estimation.
% This also allow us not to perform pose estimation as all points in the bin are known to belong to the object.

We recognize that the general approach as shown in e.g. \cite{sundermeyer2023bop} has huge importance, but also state it is not the best solution for all tasks. 
In this paper we show that generalized pose estimation can show very good performance when restricting the scenario. 

We believe that these results invite further research into this topic, as more flexible robotic set-ups with lower power consumption is an important topic.
% The focus of this paper is pose estimation in 3D point clouds as CAD models in industry rarely include information about color or texture.


% Further research in this area.
% However, for many cases this is not the optimal solutions. 

% For many robotic set-ups the environment.


% Especially with bin-picking!

% Robots can accommodate unknown environments avoiding the need for mechanical solutions and manual teaching of any changes. 



% However, the uncertainty of the environment also makes it very difficult to set-up new pose estimation solutions. 




% deep learning. Better robustness to noise, occlusion and clutter. Requiring data collection and network training. 

% Does not fit well with more general solutions.
% Industrial applications require very high precision as the manipulation tasks.

% Therefore a new network is required to be trained for a new pose estimation task.


% In this network we 

% In our case we limit the scenario, but keep a huge range of objects.

% The focus in this paper is 






% The test dataset was chosen for two different reasons. 
% It is objects that could be used in a bin-picking setting, espically in a flexible production. As some components could be required. Espically through hole components.

% It is available freely online at:

% We further more test on the components from the 

% , with sometimes few. There is variation inside that class but, they are not completely similar.


% This paper is a demonstration that this is possible. We show that it does not completely-

% Further work in this field is necessary, but the idea is important.

% 


% In the pose estimation case with key points from a neural network. The network is trained to predict the specific keypoints.

% This gives several benefits: 	
% * Not training new networks, shorter set-up time, less power consumption.
% * Combined weights allow for better generalization
% * Allow us to train on real data to bridge the domain gap
% * Only load in one model for multiple object pose estimation

% However, for many tasks, the scenario is well known. Thus by limiting the scenario the network can more easily adapt to new objects. In our approach, we focus on the scenario of homogeneous bin-picking. 
% While this is only a single scenario, it is a challenge faced in many aspects of the industry.

% The developed network is trained on 1500 electrical components. The network adapts well to the many objects and also performs well with novel electrical components. The method outperforms a classical pose estimation method based on engineered feature, especially when adding noise to the point clouds.  
% We furthermore demonstrate the ability of the network to perform outside of the trained class by performing pose estimations of the mechanical objects from the World Robot Challenge. 
% For this paper all data was synthetically generated, however, as a result of the good performance, further work will show the performance on real training and test data.

%-------------------------------------------------------------------------
\section{Related Works}

Visual pose estimation is an important topic, and many different approaches have been developed.

\textbf{Classic Pose Estimation:} The classic pose estimation approach is based on matching features between the scene and object, and computing the pose by e.g. RANSAC \cite{fischler1981random}. The matches are computed using handcrafted features, which is generally computed in 3D point clouds. A huge amount of handcrafted features have been developed \cite{guo2016comprehensive}, with Fast Point Feature Histograms (FPFH) \cite{rusu2009fast} being one of the best performing features.

\textbf{Deep Learning Based:} Generally deep learning based methods are based on color information \cite{hodavn2020bop, sundermeyer2023bop}. This is possibly a result of many deep learning based methods developed for this space, with huge pre-trained networks available. These methods have vastly outperformed the classical methods, however, a network is often trained per object \cite{hodavn2020bop, sundermeyer2023bop}. 
Deep learning for pose estimation has also been performed in point clouds with methods such as PointVoteNet \cite{hagelskjaer2020pointvotenet}. We base our method on PointVoteNet, but only train a single network for all objects.
% but were that method trained a network per object, we only train a single network. 

\textbf{Generalized Pose Estimation:} Several approaches have been developed for generalized pose estimation.
As in the deep learning based pose estimation, the field of generalized pose estimation is also dominated by color-based methods \cite{shugurov2022osop, labbe2022megapose, he2022fs6d}. The general approach for these methods is to match templates of the object with the real image. These templates can either be generated synthetically as in \cite{nguyen2022templates} or with a few real images as in FS6D \cite{he2022fs6d}. The same approach have also been used for tracking of unknown objects \cite{nguyen2022pizza}. 
MegaPose6D \cite{labbe2022megapose} is a notable example where the network is trained on a huge dataset with 2 million images. 
% The webpage: https://github.com/liuyuan-pal/Awesome-generalizable-6D-object-pose
The method most similar to ours is a point cloud based method \cite{gou2022unseen}. It also uses the object model as input.
% and train on a thousand objects. 
However, several differences are present compared with our approach; the method includes color information, it does not limit the scenario and thus does not obtain the increased performance from this, and it does not separate the object and scene features.


% TODO someone else mentioned out of class

% \subsection{Previous Approach}

% The input in this approach is a scene point cloud, and the output are matches to the object cad model.



% The network can then be used with e.g. RANSAC \cite{fischler1981random} to obtain a pose estimation for the object.

% The network can learn predict correct key point votes even in the face of noise and occlusion. The network learns to focus on the correct features as compared with classically engineered features.

% The downside of this approach is that to obtain a trained network large amount of training data is necessary. This increases the set-up time of the system and 

% This problem can be mitigated by using synthetic data, however, while removing the need for manual work there is still a time consumption for generating the data and training the network. Additionally training a network for each component adds an electricity consumption to the pipeline.

\begin{figure}[tb]
\begin{center}
   \includegraphics[width=0.95\linewidth]{gfx/network_reverse.drawio.pdf}
   \caption{The network structure for the developed method. Initially features are computed independently for the object and the scene. This allow us to compute the object features a single time and match it with multiple scene point clouds. The "Edge Feature" is specific to DGCNN and computes and concatenates neighbor points.}
\label{fig:network}
\end{center}
\end{figure}

\section{Method}
% 
The developed method is based on DGCNN \cite{wang2019dynamic} and bears a similarity to a similar pose estimation network \cite{hagelskjaer2020pointvotenet}.  

Point clouds are well suited for industrial objects as precise CAD models are generally available, but color and surface is rarely available. Compared with PointVoteNet we introduce several differences. 

Instead of learning the prediction of cad model keypoints directly in the network, as in previous methods, we instead train the network to match keypoint features from the object to the scene point cloud. The network structure is shown in Fig.~\ref{fig:network}. 
% 
Initially the object and keypoint features are computed independently using the standard feed-forward part of DGCNN. However, as only the keypoints from the object point cloud is needed the object point cloud is down-sampled to the twenty keypoints after the second neighbor computation in the DGCNN. As the number of neighbors is set to the same as the number of keypoints, all keypoints include information about the other.

After both object and scene features are computed they are joined together to create a matrix of length $(n*k)$, where $n$ is the number of point and $k$ is the number of keypoints. A linear layer then processes each join independently and a local maxpool reduces the matrix to length $(n)$, combining all keypoint information for the scene point.
The object features are then joined again and a linear layer computes features per keypoint-scene point pair. By applying a softmax the prediction for each keypoint is computed.
To compute the segmentation a local maxpool is applied followed by a linear layer. In Fig.~\ref{fig:multi} the networks ability to correctly classify keypoints for different objects is shown.

Finally, RANSAC is used with the segmentation and keypoint predictions to compute the final object pose. We employ the vote threshold as in \cite{hagelskjaer2020pointvotenet} to allow multiple predictions at a single point. The vote threshold is set to 0.7.


\begin{figure}[tb]
\begin{center}
   \includegraphics[width=0.99\linewidth]{gfx/multi_objects.drawio.pdf}
\end{center}
   \caption{An illustration of the networks ability to correctly match keypoints from multiple objects. Both objects are unknown to the object, with the right object being out of class.}
\label{fig:multi}
\end{figure}

\subsection{Generating scene data}

As the scenario is homogeneous bin-picking the detection is vastly simplified. As the bin position is known beforehand, and the contents are homogeneous, any point belongs to an object.
% 
Thus we can randomly sample point a point, and this point will belong to the object. By then extracting using a radius set to the object model diagonal, all points in the scene belonging to the object will be obtained. The point cloud is then centered around the sampled point to allow for instance segmentation.
% 
The scene point clouds are generated using BlenderProc \cite{denninger2019blenderproc}.

\subsection{Generating object data}
% 
The object point clouds is generated using Poisson sampling to obtain 2048 evenly sampled points on the surface. Farthest point sampling is then used to obtain the keypoints spread evenly on the object.
%
During training the keypoints are continuously re-computed with random initialization, to avoid the network over-fitting to a specific combination.

\subsection{Computing object features off-line}
% 
As shown in Fig.~\ref{fig:network} the features computed from object point cloud are independent of the features computed from the scene point cloud. This is opposed to \cite{gou2022unseen}. This allow us to use the same object feature for multiple pose estimations. The object features can also be computed offline to reduce the run-time and the computational cost.

% \begin{figure*}[htb]
% \begin{center}
%    \includegraphics[width=0.95\linewidth]{gfx/objects.pdf}
% \end{center}
%    \caption{The seven electronic components used for testing the performance.}
% \label{fig:objects}
% \end{figure*}

\begin{figure*}[htb]
\begin{center}
   \includegraphics[width=0.95\linewidth]{gfx/objects_combined.drawio.pdf}
\end{center}
   \caption{The seven electronic components used for testing, along with the seven components from the WRS dataset used for out of class testing. }
\label{fig:objects}
\end{figure*}


\section{Experiments}

To test the developed method several experiments are performed. A single network is trained and tested for all the experiments. The network is trained on 1500 free cad models from an online database of electronic components\footnote{\url{https://www.pcb-3d.com/membership_type/free/}}. Fifty models are used as validation to test the networks ability on unseen objects.
% 
The method is also tested on novel objects. Seven electrical components from a different database are tested \cite{hagelskjaer2022hand}. The seven test objects are shown at the top of Fig.~\ref{fig:objects}.
% 
Additionally we test the ability of the network on industrial objects from the WRS \cite{yokokohji2019assembly} dataset. On this dataset we show the networks ability to generalize to other objects outside of the training scope. The objects from the WRS dataset are shown in the bottom of Fig.~\ref{fig:objects}.
% 
% \begin{figure*}[htb]
% \begin{center}
%    \includegraphics[width=0.95\linewidth]{gfx/objects_wrs.pdf}
% \end{center}
%    \caption{The seven components from the WRS dataset used for testing.}
% \label{fig:wrs}
% \end{figure*}
% 
All point cloud processing and pose estimation is performed using the Open3D framework, \cite{Zhou2018}.
The network processing was performed using PyTorch \cite{Paszke_PyTorch_An_Imperative_2019}.

\subsection{Network Training}
%
The 1500 components are split into a 1450/50 train-validation dataset. During each epoch we generate 160 point clouds for each component. Thus each training epoch consists of 232000 point clouds. 
% 
The network is trained with a batch size of 14, 7 on each GPU, using the Adam optimizer \cite{kingma2014adam}, with an initial learning rate of 0.0001. We use a step scheduler with a step size of 20 and the gamma parameter set to 0.7. The loss is calculated with cross entropy using a 0.2/0.8 split for segmentation and keypoint loss. For the keypoint loss only points belonging to the object is used. 

To generalize the network Group Norm \cite{wu2018group} with group size 32 is used after each linear layer. Group Norm is used as opposed to Batch Norm as a result of the small batch size. Dropout is used for the object features, as the network should not overfit to a specific part of the object, and is used after the to concurrent linear layers. The dropout is set to 40 \%, used after the last two linear layers of the object feature and the first two of the combined feature.
%
Additionally, up to 0.75 \% Gaussian noise is applied to the object and scene point clouds, and 10 \% position shift is applied to the object point cloud.

The network was trained on a PC environment with two NVIDIA GeForce RTX 2080 GPUs. The network was trained for 120 epochs lasting approximately six days (141 hours).

% \subsection{Generating training data}

% During training the object points are projected to compute the segmentation and the keypoints are projected to find the keypoints.

\subsection{Training and test performance}

The performance for the trained networks is show in Tab.~\ref{tab:train}. Performance is shown for both the loss, segmentation accuracy, and keypoint accuracy. We present the training performance both with and without generalization. The network does not appear to overfit to the training data, and actually shows better performance on the validation set. For the test data with electronic objects not from the same dataset, the network performs well with a slightly lower performance. 
%
As the objects are symmetric to varying levels, the keypoint accuracy despite being low, still gives good pose estimations. This is seen in Fig.~\ref{fig:example}, where the symmetry of the object results in striped matching of keypoints. However, these matches are still very useful for the pose estimation.

% 0.989851, test avg acc: 0.977308, test key acc: 0.293383
% 0.989006, test avg acc: 0.972449, test key acc: 0.290565

% \begin{tabular}{|l|c|c|c|}
% \hline
% Split & Loss & Seg. Acc & Key. Acc \\
% \hline\hline
% Train (w/ gen.)     & 1.59  & 0.98 & 0.27 \\
% Train (w/o gen.)    & 1.42  & 0.97 & 0.29 \\
% Val                 & 1.33  & 0.96 & 0.31 \\
% Test                & 1.57  & 0.95 & 0.25 \\
% \hline
% \end{tabular}


\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Split & Train (w/ gen.) & Train (w/o gen.) & Val & Test \\
\hline\hline
Loss      & 1.59 & 1.42 & 1.33 & 1.57 \\ 
Seg. Acc  & 0.98 & 0.97 & 0.96 & 0.95 \\ 
Key. Acc  & 0.27 & 0.29 & 0.31 & 0.25 \\ 
\hline
\end{tabular}
\end{center}
\caption{The loss and accuracy of the network on the different datasets. Results are shown for the training data both with and without the generalization applied during training.}
\label{tab:train}
\end{table}

\subsection{Performance for each component}

To analyze the network, performance for each component is shown in Tab.~\ref{tab:components}. The two objects with the highest performance is "3" and "5". These two object are both very similar to objects in the training data. The most challenging object is "2". The split between the two parts of the object makes it very dissimilar to the training data. The other components perform very well, especially for the segmentation task.


% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|c|c|}
% \hline
% Object & Loss & Seg. Acc & Key. Acc \\
% \hline\hline
% 1 & 1.50 & 0.99 & 0.24 \\
% 2 & 2.03 & 0.81 & 0.16 \\
% 3 & 1.12 & 0.98 & 0.43 \\
% 4 & 1.53 & 0.98 & 0.29 \\
% 5 & 1.42 & 0.95 & 0.31 \\
% 6 & 1.84 & 0.97 & 0.22 \\
% 7 & 1.53 & 0.99 & 0.22 \\
% \hline
% \end{tabular}
% \end{center}
% \caption{Loss and accuracy of each of the individual test objects.}
% \label{tab:components}
% \end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Object   & 1    & 2    & 3    & 4    & 5    & 6    & 7    \\ \hline 
\hline
Loss     & 1.50 & 2.03 & 1.12 & 1.53 & 1.42 & 1.84 & 1.53 \\ 
Seg. Acc & 0.99 & 0.81 & 0.98 & 0.98 & 0.95 & 0.97 & 0.99 \\ 
Key. Acc & 0.24 & 0.16 & 0.43 & 0.29 & 0.31 & 0.22 & 0.22 \\ \hline
\end{tabular}
\end{center}
\caption{Loss and accuracy of each of the individual test objects.}
\label{tab:components}
\end{table}



\subsection{Pose Estimation Performance}

To test the pose estimation ability of the network we compare it with a classic pose estimation method. The classic pose estimation method is FPFH \cite{rusu2009fast} with RANSAC \cite{fischler1981random}. The performance is measured by the ADD-S score, as it is well suited for the symmetric objects in our dataset \cite{hinterstoisser2013model}. To further test the robustness of the system, varying levels of noise is added to the points clouds. The results are shown in Tab.~\ref{tab:classic}. It can be seen that our method outperforms the classic method for all objects. When adding noise the difference becomes even more pronounced. However, for the $5\%$ noise the performance also drops significantly for our method.

\begin{table}
\begin{center}
    \begin{tabular}{|l|c|c|c|c|c|c|c|}
    \hline
    Object  & 1  & 2  & 3  & 4  & 5  & 6  & 7  \\ \hline
    \hline
    % Ours    & 77 & 66 & 79 & 64 & 70 & 74 & 76 \\ 
    % Classic & 45 & 47 & 47 & 16 & 28 & 55 & 39 \\ 
    % Ours $1\%$ noise   & 73 & 66 & 80 & 56 & 69 & 66 & 74 \\ 
    % Classic $1\%$ noise & 42 & 46 & 39 & 19 & 23 & 50 & 24 \\
    % Ours $5\%$ noise   & 42 & 48 & 74 & 49 & 66 & 67 & 51 \\ 
    % Classic $5\%$ noise & 28 & 35 & 30 & 15 & 40 & 46 & 18 \\ \hline
    % Ours    & 0.96 & 0.83 & 0.99 & 0.80 & 0.88 & 0.93 & 0.95 \\ 
    % Classic & 0.56 & 0.59 & 0.59 & 0.20 & 0.35 & 0.69 & 0.49 \\ 
    % \hline
    % Ours $1\%$ noise   & 0.91 & 0.83 & 1.00 & 0.70 & 0.86 & 0.83 & 0.93 \\ 
    % Classic $1\%$ noise & 0.53 & 0.58 & 0.49 & 0.24 & 0.29 & 0.63 & 0.30 \\
    % \hline
    % Ours $5\%$ noise   & 0.53 & 0.60 & 0.93 & 0.61 & 0.83 & 0.84 & 0.64 \\ 
    % Classic $5\%$ noise & 0.35 & 0.44 & 0.38 & 0.19 & 0.50 & 0.58 & 0.23 \\ \hline
    Ours                & 0.95 & 0.82 & 0.99 & 0.77 & 0.91 & 0.90 & 0.92 \\
    Classic             & 0.55 & 0.62 & 0.61 & 0.30 & 0.36 & 0.67 & 0.51 \\
    \hline
    Ours $1\%$ noise    & 0.93 & 0.79 & 0.99 & 0.74 & 0.86 & 0.88 & 0.90 \\
    Classic $1\%$ noise & 0.43 & 0.57 & 0.50 & 0.25 & 0.31 & 0.66 & 0.40 \\
    \hline
    Ours $5\%$ noise    & 0.55 & 0.60 & 0.91 & 0.62 & 0.83 & 0.82 & 0.53     \\
    Classic $5\%$ noise & 0.30 & 0.54 & 0.45 & 0.20 & 0.43 & 0.56 & 0.25 \\
    \hline
    \end{tabular}
\end{center}
\caption{Pose estimation accuracy for each of the test components. A comparison with classic pose estimation is shown. Results are also shown with noise added to the scene point cloud.}
\label{tab:classic}
  \vspace{-6mm}
\end{table}


\subsection{Testing out of class}
% 
The WRS dataset consists of industrial objects, such as motors and pulleys. The objects were used for the WRS assembly challenge held in 2018 \cite{yokokohji2019assembly}. 
% bin-picking during
%During the competition the bin-picking was the most difficult of the assembly challenges proving their difficulty.
We have chosen they represent an industrial challenge and because of the variety. The results of the network performance and pose estimation is shown in Tab.~\ref{tab:wrs}. It is seen that our developed method obtains very good performance on these objects that appear quite different than the training dataset. As seen in Fig.~\ref{fig:pe}, because the objects are quite symmetric, the pose estimation succeeds even though the keypoints prediction is not correct.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Object    & 24    & 21    & 20    & 15    & 11    & 09    & 12    \\ \hline
\hline
Pose Est. & 1.00 & 0.99 & 0.90 & 0.91 & 0.93 & 0.93 & 0.96 \\ \hline
\end{tabular}
\end{center}
\caption{The pose estimation accuracy of our method on the unseen WRS dataset.}
\label{tab:wrs}
\end{table}


\subsection{Run-time}
% 
% For a single batch with batch size two.
% 
The run-time of the network was tested both with and without computing the object features at run-time. With the object features computed at run-time the processing lasts 14.9 ms. While by pre-computing the features the run-time is only 7.9 ms. The separation of object and scene feature computations is thus a significant speed-up.

However, for the full system the run-time is currently 84.7 ms. This is mainly related to the RANSAC search, which would be a focus to replace.

\begin{figure}[tb]
\begin{center}
   \includegraphics[width=0.99\linewidth]{gfx/pose_estimation.drawio.pdf}
\end{center}
   \caption{Visualization of the pose estimation accuracy despite low keypoint precision. In the central figure the prediction accuracy of the network is visualized. "White" represents correctly predicted background, "red" false negatives and "yellow" false positives. "Blue" is correctly predicted segmentation of the object, but wrong keypoints and "green" is both correct segmentation and keypoint prediction. To the right the pose estimation is shown. }
\label{fig:pe}
\end{figure}



% Linemod or Real data?

\section{Conclusion}

In this paper we have presented a novel method for generalized pose estimation. The method consists of a novel network structure and a scenario specific approach. The method show very good generalizability across different objects, even with out of class objects. This proves the validity of creating object independent networks for specific scenarios, which can be useful for many real world applications.

In future work, it will be very interesting to test the method with real data, both for training and testing. Additionally, the network could be adapted to perform full pose estimation to simplify the pipeline and improve the run-time. The objects from the MegaPose6D \cite{labbe2022megapose} dataset could also be used to diversify the object types, and test on benchmark datasets. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
