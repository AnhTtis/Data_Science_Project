\section{Using Demonstrations Online}
\label{sec:use_demo_online}

In addition to offline learning from demonstrations, it is also possible to utilize demonstration data directly during online learning, without the need for a preceding offline learning stage. In this section, we will discuss the different methods of online reinforcement learning or planning that can be enhanced through the use of demonstrations.


\subsection{Demo as Off-Policy Experience}

One approach to incorporating demonstration data directly into online learning is to integrate it with modern off-policy reinforcement learning methods \cite{dqn, rainbow, ddpg, td3, sac}, which maintain a replay buffer \cite{dqn} and optimize the policy by training on the experience sampled from the replay buffer. In this approach, demonstration trajectories are simply added to the replay buffer, and off-policy reinforcement learning is run as usual. The demonstration data is never overwritten in the replay buffer, ensuring that it remains available throughout the training process. This strategy, which is simple and easy to implement, has been widely adopted by methods such as \cite{dqnfd,r2d2,r2d3,ddpgfd,nair2018overcoming}. By including the demonstration data in the replay buffer, the agent is exposed to a more diverse set of states before exploring them, addressing the exploration problem, which is a central challenge in reinforcement learning.


Despite its potential advantages, the aforementioned strategy has certain limitations that prevent it from effectively exploiting the demonstration data. Firstly, the approach relies solely on expert trajectories as learning references and may fail to fully utilize their effect when the number of demonstrations is limited, as confirmed by \cite{pofd}. Secondly, the demonstrated trajectories must be associated with rewards for each state transition to be compatible with collected data during training. However, the rewards used in the demonstrations may differ from those used to learn the policy in the current environment \cite{max_entropy_irl}, or they may be unavailable.

In situations where rewards are absent from the demonstrations, the SQIL algorithm \cite{sqil} can still be applied. This approach adds the demonstrations to the replay buffer with a constant reward of 1, while all other experiences collected online are added to the buffer with a constant reward of 0. This can be viewed as a regularized variant of behavior cloning that employs a sparsity prior to encourage long-horizon imitation.


\subsection{Demo as On-Policy Regularization}
\label{sec:demo_as_reg}


Off-policy methods have the potential to be more sample efficient, but are often plagued with instability issues \cite{duan2016benchmarking, drl_matters}. In contrast, on-policy methods tend to be more stable and perform well in high-dimensional spaces \cite{duan2016benchmarking}. Consequently, several studies have explored ways to incorporate demonstrations into on-policy reinforcement learning algorithms.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/pofd.png}
    % \vspace{-0.5cm}
    \caption{
        POfD \cite{pofd} explores in the high-reward regions (red arrows), with the aid of demonstrations (the blue curve). It thus performs better than random explorations (olive green dashed curves) in sparse-reward environments.
    }
    % \vspace{-0.5cm}
    \label{fig:pofd}
\end{figure}

In POfD \cite{pofd}, the authors propose using expert demonstrations to guide exploration in reinforcement learning by enforcing occupancy measure matching between the learned policy and the demonstrations. This is achieved by introducing a demonstration-guided exploration term to the vanilla RL objective $\eta(\pi_\theta)$:
\begin{equation}
\mathcal{L}\left(\pi_\theta\right)=-\eta\left(\pi_\theta\right)+\lambda_1 D_{J S}\left(\rho_\theta, \rho_E\right)
\end{equation}
Here, $D_{JS}$ denotes the Jensen-Shannon divergence, while $\rho_\theta$ and $\rho_E$ represent the occupancy measures of the agent policy and the expert policy, respectively. By adding this regularization term, the agent is encouraged to explore areas demonstrated by the expert policy, as depicted in Figure \ref{fig:pofd}. This approach is compatible with a variety of policy gradient methods, such as TRPO \cite{trpo} and PPO \cite{ppo}.


DAPG \cite{dapg} first pre-trains the policy using behavior cloning to provide an informed initialization that can efficiently guide exploration. During the online RL fine-tuning phase, DAPG adds an additional term to the policy gradient to make full use of the information in the demonstration data:
\begin{equation}
\begin{split}
    g_{aug}=\sum_{(s,a)\in\rho_\pi} \nabla_\theta\ln\pi_\theta(a|s)A^\pi(s,a) +  \\
    \sum_{(s,a)\in\rho_D} \nabla_\theta\ln\pi_\theta(a|s)w(s,a)
\end{split}
\end{equation}

Here, $\rho_\pi$ represents the dataset obtained by executing policy $\pi$ on the MDP, and $w(s,a)$ is a weighting function. The first term is the gradient from the RL algorithm, and the second term is the gradient from the behavior cloning loss. In practice, $w(s,a)$ is implemented using a heuristic weighting function:
\begin{equation}
    w(s, a)=\lambda_0 \lambda_1^k \max _{\left(s^{\prime}, a^{\prime}\right) \in \rho_\pi} A^\pi\left(s^{\prime}, a^{\prime}\right) \quad \forall(s, a) \in \rho_D
\end{equation}

where $\lambda_0$ and $\lambda_1$ are hyperparameters, and $k$ is the iteration counter. The decay of the weighting term via $\lambda_1^k$ is motivated by the premise that, initially, the actions suggested by the demonstrations are at least as good as the actions produced by the policy. However, towards the end, when the policy is comparable in performance to the demonstrations, we do not want to bias the gradient. Thus, the auxiliary objective asymptotically decays.



\subsection{Demo as Reference for Reward}
\label{sec:demo_as_ref}

In cases where the reward is sparse or unavailable, demonstrations can be used to compute the reward. This can be accomplished by either directly defining a reward based on the demonstration or by matching the agent's trajectory distribution with the demonstration distribution. The latter is a common strategy employed in inverse reinforcement learning. We discuss both of these methods in the following subsections.

\subsubsection{Directly Define Reward based on Demo}

If only a single (or a few) demonstration trajectories are available, a reward function can be manually designed to incentivize the agent to follow the demonstration trajectory. This is typically accomplished by defining the reward as some kind of distance between the current state and the demonstration trajectory.

Various approaches can be used to specify the reward/distance function. For example, in \cite{aytar2018playing}, a sequence of important checkpoints is extracted from the demonstration trajectory, and the agent is rewarded if it visits the checkpoints in a soft order. A checkpoint is considered visited when the distance to it is less than a threshold, and the distance is defined as cosine similarity in a learned latent space. Similarly, AVID \cite{smith2019avid} divides a demonstration trajectory into stages and trains a classifier to predict whether a state belongs to a specific stage. These classifiers can then be combined to form a reward function.

In \cite{brys2015reinforcement}, the potential of a state is first defined as the highest similarity with the states in the demonstration, and then the $\gamma$-difference of the potential function is used to construct a potential-based reward function \cite{ng1999policy}. In \cite{wu2021learning} and \cite{graph_irl}, the reward is defined as the distance to the goal in the learned latent space, where the goal is chosen as the final state in the demonstration trajectory. Recent works on motion imitation \cite{peng2018deepmimic, peng2020learning, peng2022ase} carefully design the reward as a weighted distance to the reference state, taking joint orientations, joint velocities, end effectors, and centers of mass into account.


\subsubsection{Match the Distribution of Demo}

In addition to defining a reward based on demonstrations, another approach involves allowing the agent to interact with the environment and attempting to match the distributions of agent and demonstration trajectories. During this process, the distance or divergence between agent and demonstration trajectories can be converted to a reward and used to improve the agent policy. This approach forms the basis of most inverse reinforcement learning (IRL) methods. The process can be summarized using the algorithm template shown in Algorithm \ref{alg:IRL_template} (from \cite{irl_survey}).

\begin{algorithm}[ht!]
    \caption{Template for Typical Inverse RL \cite{irl_survey}}	
    \label{alg:IRL_template}
    
    \textbf{Input:} $\mathcal{M}\backslash_{R_E}$= $ \langle S,A,T,\gamma \rangle $, Set   of   trajectories  demonstrating   desired   behavior:
        $\mathcal{D}=\{\langle
            (s_0,a_0),(s_1,a_1),\ldots,(s_t,a_t)  \rangle, \ldots  \}$,  $s_t\in S$,
            $a_t\in A$,  
        $t \in \mathbb{N}$, 
        or expert's policy: $\pi_E$, and reward function features \\
    \textbf{Output:}   $\hat{R}_E$
    \begin{algorithmic}[1]
        \State Model the expert's observed  behavior as the solution of an MDP whose reward  function is not known\; 
        \State Initialize  the parameterized  form of  the reward  function using any  given features (linearly weighted  sum of feature values, distribution over rewards, or other)\;
        \State Solve  the MDP with  the current reward function  to generate the learned behavior or policy\; 
        \State Update  the optimization parameters  to minimize  the divergence  between the  observed behavior (or policy)  and the  learned behavior (policy)\; 
        \State Repeat the previous two steps till the divergence is reduced to the desired level. 
    \end{algorithmic}
\end{algorithm}


As shown in Algorithm \ref{alg:IRL_template}, IRL methods typically involve an iterative process that alternates between reward estimation and RL, which can result in poor sample efficiency. Earlier IRL methods \cite{algo_irl, abbeel2004apprenticeship, max_entropy_irl} typically required multiple calls to a Markov decision process solver \cite{mdp_book}.

Recently, adversarial imitation learning (AIL) approaches have been proposed \cite{gail, dac, airl, ghasemipour2020divergence} that interleave the learning of the reward function with the learning process of the agent.

AIL methods operate similarly to generative adversarial networks (GANs) \cite{gan}. In these methods, a generator (the policy) is trained to maximize the confusion of a discriminator (the reward) that is itself trained to differentiate between the agent's state-action pairs and those of the expert. Adversarial IL methods essentially boil down to a distribution matching problem, i.e., the problem of minimizing the distance between probability distributions in a metric space. According to the analysis in \cite{ghasemipour2020divergence}, these methods implicitly minimize an $f$-divergence between the state-action distribution of an expert and that of the learning agent.

In addition, the POfD algorithm \cite{pofd}, mentioned in Section \ref{sec:demo_as_reg}, can also be regarded as a variant of AIL. In practice, POfD is implemented by adding a shaped reward to the environment's original reward, and the shaped reward is also computed from a discriminator that differentiates between the agent's and expert's behaviors.


Besides adversarial imitation learning (AIL) methods, there are also imitation learning-based methods that approach the problem as a distribution matching problem but do not use adversarial approaches. PWIL \cite{pwil}, for example, matches the demonstration distribution by minimizing the primal form of the Wasserstein distance, also known as the earth mover's distance. An upper bound on the Wasserstein distance can be used to compute the reward offline, avoiding the computationally expensive min-max optimization problem encountered in AIL.



\subsection{Demo as Curriculum of Start States}


When the environment is a simulator that can be fully controlled, demonstrations can be used to construct a curriculum of start states. This approach is a highly efficient exploration strategy in RL and has been studied in several prior works.

Typically, these works assume the ability to reset the simulator to arbitrary states. Therefore, they can modify the distribution of start states based on the demonstrations to simplify exploration. Some methods, such as \cite{hosu2016playing, nair2018overcoming, peng2018deepmimic}, uniformly sample states from the demonstrations as start states, while others generate curriculums of start states \cite{salimans2018learning, zhu2018reinforcement}.

Using a curriculum of start states is helpful because it reduces the exploration difficulty and accelerates learning. For example, when attempting a backflip, starting from a standing pose requires learning the early phases before progressing towards the later phases. However, if start states are sampled from the demonstrations, the agent encounters desirable states earlier and can learn faster.

