\section{Background \& Prelimiary Knowledge}

\label{sec:background}

In this section, we first define the Markov Decision Process (Sec \ref{sec:mdp}), which is the environment or task to be solved in the sequential decision making problem. And Sec \ref{sec:rl}, \ref{sec:il}, and \ref{sec:planning} briefly introduce three types of methods used to solve MDP.


\subsection{Markov Decision Process}
\label{sec:mdp}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/agent_env_interaction.png}
    \caption{
        The agent–environment interaction in a Markov decision process from \cite{rl_book}
    }
    \label{fig:mdp}
\end{figure}

An MDP, denoted as $\mathcal{M}:=\langle S,A,T,R,\gamma \rangle$, serves as a model for an agent's sequential decision-making process. Here, $S$ represents a finite set of states, and $A$ is a set of actions. The mapping $T:S\times A \to \textsf{Prob}(S)$ defines a probability distribution over the set of next states, given that the agent takes action $a$ in state $s$. Here, $\textsf{Prob}(S)$ refers to the set of all probability distributions over $S$. The transition probability $T(s'|s,a)\in[0,1]$ denotes the probability that the system transitions to state $s'$.

The reward function $R$ can be specified in various ways. When $R:S \to \mathbb{R}$, it gives the scalar reinforcement at state $s$. Alternatively, $R:S \times A \to \mathbb{R}$ maps a tuple (state $s$, action $a$ taken in state $s$) to the reward received upon performing the action. Finally, $R:S \times A \times S \to \mathbb{R}$ maps a triplet (state $s$, action $a$, resultant state $s'$) to the reward obtained upon performing the transition.

The discount factor $\gamma \in [0,1]$ serves as the weight for past rewards accumulated in a trajectory $\langle (s_{0},a_{0}),(s_{1},a_{1}),\ldots,(s_{j},a_{j}) \rangle$, where $s_{j}\in S$, $a_{j}\in A$, and $j \in\mathbb{N}$. The trajectory represents a sequence of state-action pairs that the agent takes during its decision-making process.

Fig \ref{fig:mdp} shows the agent–environment interaction interface in a Markov decision process.


\subsection{Reinforcement Learning}
\label{sec:rl}

Reinforcement Learning (RL) aims to determine an optimal policy in an MDP $\mathcal{M}$ by interacting with it.

A \emph{policy} refers to a function that maps the current state to the next action choice(s). It can be deterministic, $\pi:S\to A$, or stochastic, $\pi:S\to \textsf{Prob}(A)$. For a given policy $\pi$, the value function $V^{\pi}:S \to \mathbb{R}$ provides the value of a state $s$, which is the long-term expected cumulative reward incurred by following $\pi$ from that state. Specifically, the value of a policy $\pi$ for a given start state $s_0$ is defined as follows:

\begin{equation}
V^{\pi}(s_0) = E_{s,\pi(s)}\left[ \sum_{t=0}^\infty \gamma^t
R(s_t,\pi(s_t))|s_0 \right]
\label{eqn:state_value}
\end{equation}

In RL, the goal is to find an optimal policy $\pi^*$ that satisfies $V^{\pi^*}(s)=V^*(s)=\sup_\pi~ V^{\pi}(s)$ for all $s\in S$. The action-value function for $\pi$, $Q^{\pi}: S\times A  \to \mathbb{R}$, maps a state-action pair to the long-term expected cumulative reward incurred after taking action $a$ from $s$ and following policy $\pi$ thereafter. Additionally, we define the optimal action-value function as $Q^*(s,a)=\sup_\pi  Q^{\pi}(s,a)$. Then, $V^*(s) = \sup_{a \in A} Q^*(s,a)$.

RL offers an online approach for solving an MDP. The input for RL consists of sampled experiences in the form of $(s, a, r)$ or $(s, a, r, s')$, which includes the reward or reinforcement due to the agent performing action $a$ in state $s$. In the model-free setting of RL, the transition function $T$ is unknown. Both the transition function and policy are estimated from the samples, and the goal of RL is to learn an optimal policy.

\subsection{Imitation Learning}
\label{sec:il}

Imitation learning (IL) is an alternative approach to finding good policies for an MDP $\mathcal{M}$. IL seeks to extract knowledge from demonstrations provided by experts in order to replicate their behaviors. It is often used in complex sequential decision-making problems where pure reinforcement learning would require a large number of samples to solve.

There are generally two types of imitation learning methods. Behavior cloning (BC) \cite{bc} is a simple yet effective method that learns a policy in a supervised manner by cloning the expert's actions at each state in the demonstration dataset. Another approach to imitating expert behaviors is through inverse reinforcement learning (IRL) \cite{irl, gail}, which finds a reward function that allows for the solution of the expert demonstrations.

\subsection{Planning with Models}
\label{sec:planning}
When the environment model, including the transition function $T$ and reward function $R$, is available to the agent, planning can be used to solve the MDP $\mathcal{M}$. \emph{Planning} refers to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment. This approach is sometimes considered a type of model-based RL.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/mpc.png}
    \caption{
        An illustration of Model Predictive Control from \cite{pets}
    }
    \label{fig:mpc}
\end{figure}



Model predictive control (MPC) \cite{mpc_book} is a model-based control method that plans an optimized sequence of actions in the model. Typically, at each time step, MPC obtains an optimal action sequence by sampling multiple sequences and applying the first action of the sequence to the environment. Formally, at time step $t$, an MPC agent seeks an action sequence $a_{t:t+\tau}$ by optimizing:

\begin{equation}
\label{eq_mpc_basic}
    \max_{a_{t:t+\tau}}~\mathbb{E}_{s_{t'+1}\sim p(s_{t'+1}|s_{t'},a_{t'})}\left[ \sum_{t'=t}^{t+\tau}r(s_{t'}, a_{t'}) \right]
\end{equation}
where $\tau$ denotes the planning horizon. Then the agent will choose the first action $a_t$ from the action sequence and apply it to the environment. An illustration of MPC is shown in Fig \ref{fig:mpc}.

Monte Carlo tree search (MCTS) \cite{mcts} is an extension of Monte Carlo sampling methods that aim to solve Eq.(\ref{eq_mpc_basic}). Unlike the Monte Carlo methods used in the MPC approach, MCTS uses a tree-search method. At each time step, MCTS incrementally extends a search tree from the current environment state. Each node in the tree corresponds to a state, which is evaluated using some approximated value functions or the return obtained after rollouts in the model with a random policy~\cite{mcts} or a neural network policy~\cite{alphago, alphago_zero, alpha_zero}. Finally, an action is chosen that is more likely to transition the agent to a state with a higher evaluated value. In MCTS, models are typically used to generate the search tree and evaluate the state.

