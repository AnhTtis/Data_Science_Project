\section{Using Demonstrations Offline}
\label{sec:use_demo_offline}

Most methods that utilize demonstrations can be divided into two stages: an offline stage and an online stage, both of which are optional. During the offline stage, the agent cannot access the environment and can only learn from the demonstrations. In the online stage, the agent is allowed to interact with the environment and can learn from both the environment and the demonstrations to boost the learning process.

In this section, we will mainly discuss how to learn from demonstrations in the offline stage. There can be a subsequent online stage to further improve the agent's performance, which we will discuss in Sec \ref{sec:use_demo_online}. Note that demonstrations can be used in both stages, but we separate them to better categorize different strategies for utilizing demonstrations.

When learning from demonstrations offline, there are various things that can be learned, such as policies, skills, world models, rewards, representations, and more. We will discuss these topics one by one.


\subsection{Learning Policy}

Learning policies directly from demonstrations is perhaps the most straightforward way to leverage the knowledge contained in the demonstrations. The basic idea behind \textit{imitation learning (IL)}, as we introduced in Sec \ref{sec:il}, is to imitate the actions in demonstrations based on observations. IL methods can be roughly divided into two types: behavior cloning (BC) and inverse reinforcement learning (IRL). Although behavior cloning may seem simple, it is widely used in practice, even for challenging tasks like real-world robot manipulation \cite{saycan, rt1}. IRL is a bit more complex because it first estimates a reward function from the demonstration data and then solves a forward RL problem using this reward function. We will discuss IRL in more detail in Sec \ref{sec:learn_reward_offline} and \ref{sec:demo_as_ref}.

One limitation of IL is that it requires expert demonstrations, meaning that the policy used to collect the demonstrations should be as optimal as possible. Since the agent only imitates the behavior in the demonstration, it cannot outperform the demonstrator. However, in many scenarios, we may only have access to sub-optimal demonstrations, such as random-play demonstrations. In this case, we need a method that can potentially learn a better policy solely from the demonstrations, and this is the goal of offline reinforcement learning.

Recently, there have been many exciting works in the area of offline RL \cite{bcq, bear, brac, rem, cql}, which focus on learning a policy using only the offline data without any online learning or online fine-tuning.

In the methods we discussed above, the policy is learned solely from the demonstrations, and therefore, it is limited by the performance of the policy that generated the demonstration data. Even with offline RL methods, the learned policy may not be very strong due to the distributional shift between the demonstrated states and the policy's own states \cite{distribution_shift}. Therefore, a common practice is to fine-tune the policies learned offline through online learning. For example, in AlphaGo \cite{alphago}, the policy network is first pre-trained by cloning human demonstrations and then fine-tuned through reinforcement learning. Similarly, \cite{awac, aw_opt} fine-tunes the policy pre-trained by offline RL.
Another approach is to leverage a pre-trained policy to generate potential action candidates during online planning, similar to the approach used by \cite{yinplanning}. 

% ========================================




\subsection{Skill Discovery}

%%%%%%%%%%% intro

In some cases, the demonstrations may not be for the exact same task that we want to solve. However, those tasks may share the same skill set. For instance, making coffee and washing a cup both require the skill "pick and place a cup." Here, we use the term "skill" to refer to some short-horizon policies shared among many tasks. It can be challenging to learn a monolithic RL policy for complex, long-horizon tasks due to issues such as high sample complexity, complicated reward design, and inefficient exploration. Therefore, a natural approach is to learn a set of skills from demonstrations and then use the learned skills in downstream tasks.

%%%%%%%%%%%% works

There is a line of research \cite{niekum2013incremental, fox2017multi, krishnan2017ddco, sharma2018directed, kipf2019compile, bera2019podnet, pertsch2020accelerating, zhou2020plas, zhao2020augmenting, lee2020learning, ajay2020opal, Shankar2020Discovering, shankar2020learning, lu2021learning, zhu2021bottom, rao2021learning, tanneberg2021skid, yang2021trail} that predominantly learns skills using a latent variable model, where the latent variables partition the experience into skills, and the overall model is learned by maximizing (a lower bound on) the likelihood of the experiences. This approach is structurally similar to a long line of work in hierarchical Bayesian modeling \cite{ghahramani2000variational, blei2001topic, fox2011sticky, linderman2017bayesian}.

Another approach is to use a combination of the maximum likelihood objective and a penalty on the description length of the skills, which is proposed by LOVE \cite{love}. This penalty incentivizes the skills to maximally identify and extract common structures from the experiences.

Goal-conditioned policies can also be viewed as a type of skill. For instance, \cite{franka_kitchen} learns a goal-conditioned policy from unstructured demonstrations by goal relabeling \cite{andrychowicz2017hindsight}. This goal-conditioned policy provides a good policy initialization for subsequent hierarchical reinforcement fine-tuning.

To use the learned skills to boost online reinforcement learning, Parrot \cite{singh2020parrot} provides a concrete example. Parrot first learns a behavioral prior from a multi-task demonstration dataset, which is essentially an invertible mapping that maps noise to useful action. When training to solve a new task, instead of directly executing its actions in the original MDP, the agent learns a policy that outputs $z$, which is taken by the behavioral prior as input. Then, the output from the behavioral prior is executed in the environment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Learning World Model}
\label{sec:learn_model}

In the methods we discussed earlier, the components learned from demonstrations, such as policies and skills, are usually fine-tuned online using model-free reinforcement learning algorithms. However, in model-based RL, which is a crucial category of RL algorithms, the world model is an essential component. As noted in the previous section, the world model can be represented as the MDP $\langle S, A, T, R, \gamma \rangle$, where $S$, $A$, and $\gamma$ are typically predefined, and the state transition dynamics $T$ and reward function $R$ must be learned. Since the reward function can be learned in a similar manner to the transition function, we will focus on the learning of the transition function for simplicity.


When given a demonstration dataset, one straightforward approach to learning the world model is to fit the one-step transitions. If $T_\theta$ is deterministic, the model learning objective can be the mean squared prediction error of $T_\theta$ on the next state.

\begin{equation}
    \min_{\theta} \mathbb{E}_{(s, a) \sim D, s^\prime \sim T^* (\cdot|s, a)}  \left[ \left\Vert s^\prime - T_\theta (s, a) \right\Vert_{2}^2 \right].
    \label{eq:model_objective_mse}
\end{equation}

Here $T^*$ is the real transition dynamics, and $D$ is the demonstration dataset. 

However, deterministic transition models fail to capture the aleatoric uncertainty \cite{pets} that arises from the inherent stochasticities of the environment. To model this uncertainty, a natural approach is to use a probabilistic transition model $T_\theta (\cdot|s, a)$ \cite{pets}. In this case, the objective can be to minimize the KL divergence between the true transition distribution $T^* (\cdot|s, a)$ and the learned distribution $T_\theta (\cdot|s, a)$, as shown in Eq. \eqref{eq:model_objective_kl}.
\begin{equation}
\begin{split}
\label{eq:model_objective_kl}
    \min_{\theta} \mathbb{E}_{(s, a) \sim D} \left[D_{\mathrm{KL}}\left(T^*(\cdot | s, a), T_{\theta}(\cdot | s, a)\right)\right] := \\
    \mathbb{E}_{(s, a) \sim D, s^\prime \sim T^* (\cdot|s, a)} \ls \log \lp \frac{T^*\left(s^{\prime} \mid s, a\right)}{T_{\theta}\left(s^{\prime} \mid s, a\right)} \rp \rs .
\end{split}
\end{equation}


When learning the world model, the requirements of the demonstration dataset are different from those used for learning policy. A good world model should cover the most possible states, so the trajectories in the demonstration dataset should be as diverse as possible. Furthermore, the actions in the demonstrations do not necessarily have to be optimal. In other words, the demonstration dataset should provide a diverse set of experiences.


After the world model is learned offline, it can be used to solve the MDP in two different ways.

The first way is to use it as a proxy of the true MDP and run any RL algorithm within this learned model. If the agent is allowed to interact with the true MDP, the collected data can be used to further improve the learned world model. This strategy is adopted by several works such as \cite{me_trpo, dreamer, mbrl_atari}.

The second way is to plan in it. For example, \cite{planet} runs MPC in a learned world model to perform vision-based continuous control, and \cite{muzero} uses MCTS with the learned model to play various video games.

However, directly planning in the world model learned from demonstrations can have arbitrarily large sub-optimality, as demonstrated by \cite{ross2012agnostic}, because the demonstration dataset may not span the entire state space, and the learned world model may not be globally accurate.

One simple solution is to fine-tune the world model by interacting with the real MDP, which is also a common practice in model-based RL. Recently, more sophisticated methods have been proposed to address this issue. For instance, MOReL \cite{kidambi2020morel} constructs a pessimistic MDP (P-MDP) using the learned model and an unknown state-action pair detector, which penalizes policies that venture into unknown parts of state-action space. Another approach is taken by \cite{yu2020mopo}, which learns a world model from the demonstration dataset along with an uncertainty estimator $u(s,a)$ of the world model. This uncertainty estimator is then used to penalize the reward function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Inferring Reward}
\label{sec:learn_reward_offline}

In some scenarios, the reward function is not present in the demonstrations, making it challenging to learn the reward as part of the world model. However, it is still possible to infer the reward function if the trajectories have sparse labels, such as success and failure. Note that the methods described in this section can be regarded as inverse RL methods, but they differ from the most common inverse RL methods, which typically involve interaction with the environment. Online IRL methods will be discussed in Sec \ref{sec:demo_as_ref}.

One approach to inferring the reward function from demonstrations is to learn a goal classifier from the demonstrations and use it as the reward function \cite{xie2018few, vecerik2019practical, singh2019end}. Specifically, the goal observations (usually in the form of images) can be extracted from successful trajectories, and the negative samples can be extracted from failed trajectories. The reward function (goal classifier) can then be trained with a binary cross-entropy loss and used in online reinforcement learning problems where the reward function is not presented.

Another approach that leverages almost all data samples is proposed by \cite{zolna2020offline}. This method first learns a reward function by contrasting observations from the demonstrator and unlabeled trajectories, annotates all data with the learned reward, and finally trains an agent via offline reinforcement learning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Learning Representations}

Representation learning via supervised / self-supervised / unsupervised pre-training on large-scale datasets has emerged as a powerful paradigm in computer vision \cite{simclr, moco, doersch2015unsupervised, oord2018representation} and natural language processing \cite{devlin2018bert, brown2020language, chowdhery2022palm}, where large datasets are available. When a large demonstration dataset is available, it is natural to apply representation learning to improve policy learning. In this case, the demonstration dataset does not need to be in the form of trajectories since the observation alone can be sufficient for representation learning.

The intuition behind using representation learning to improve policy learning is straightforward. Standard visual policy learning frameworks attempt to simultaneously learn a succinct but effective representation from diverse visual data while also learning to associate actions with such representations. Such joint learning often requires large amounts of demonstrations to be effective. Decoupling the two problems by first learning a visual representation encoder from offline data using standard supervised and self-supervised learning methods and then training the policy based on the learned, frozen representations can be an effective approach.

Empirically, VINN \cite{pari2021surprising} demonstrates that this simple decoupling improves the performance of visual imitation models on both offline demonstration datasets and real-robot door-opening tasks. Similarly, \cite{yang2021representation} shows that pre-training with unsupervised learning objectives can significantly enhance the performance of policy learning algorithms that otherwise yield mediocre performance.


Recently, there has been rapid progress in the development of algorithms for representation learning, but the importance of data in representation learning for sequential decision making cannot be overstated. To this end, \cite{zhan2022learning} has proposed collecting an in-domain dataset and pre-training the visual encoder on it. An in-domain dataset is one that is obtained from the same domain as the tasks to be solved. Conversely, several works have explored learning policies with visual representations pre-trained on large external datasets \cite{shah2021rrl, pari2021surprising, nair2022r3m, wang2022vrl3, mvp2}. However, it remains unclear whether one approach is inherently superior to the other.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Learning Trajectory Imitator}
% one-shot IL

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/one_shot_il.png}
    \caption{
        An illustration of One-Shot Imitation Learning from \cite{one_shot_il}
    }
    \label{fig:one_shot_il}
\end{figure}

Imitation learning is a commonly used technique for solving different tasks, but each new task requires separate training of the agent. Ideally, agents should be able to perform a novel task by imitating a demonstration trajectory given at test time. This is the \textit{one-shot imitation learning} problem proposed by \cite{one_shot_il}. We refer to the agent with such capability as a trajectory imitator, which can also be learned from demonstrations offline.

Given multiple demonstrations from \textit{different tasks}, the one-shot imitation learning problem can be formulated as a supervised learning problem. To train the trajectory imitator, in each iteration a demonstration is sampled from one of the training tasks and fed to the network. Then, a state-action pair is sampled from a second demonstration of the same task. The network is trained to output the corresponding action when conditioned on both the first demonstration and this state. An illustration of the problem setting is shown in Fig \ref{fig:one_shot_il}. This framework has been extended to visual imitation \cite{finn2017one, pathak2018zero}, cross-domain imitation \cite{yu2018one}, language-conditioned imitation \cite{lynch2020language}, and more.

The process of providing a demonstration trajectory at test time is similar to the prompting process \cite{wei2022chain} used in large language models. Recently, Gato \cite{gato} combined the idea of one-shot imitation learning with the large language model to build a single generalist agent beyond the realm of text outputs. The agent works as a multi-modal, multi-task, multi-embodiment generalist policy. It can play Atari, caption images, chat, stack blocks with a real robot arm, and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Learning Algorithm}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/algo_distillation.pdf}
    \vspace{-0.7cm}
    \caption{
        An illustration of Algorithm Distillation from \cite{laskin2022context}
    }
    % \vspace{-0.5cm}
    \label{fig:algo_distillation}
\end{figure}


Assuming the demonstration dataset does not only contain the expert demonstration trajectories, but also the entire \textit{learning history} of an agent, e.g., the whole replay buffer of an RL agent, is it possible to learn the "learning progress" from the dataset?

Algorithm Distillation (AD) \cite{laskin2022context} is a recent approach that allows us to learn reinforcement learning algorithms directly from offline datasets, including the complete learning histories of agents. AD involves two steps. First, we train multiple instances of an RL algorithm to solve a variety of tasks and save their learning histories. Then, using this dataset of learning histories, we train a transformer model to predict actions given the preceding learning history. Since the policy improves over time, accurately predicting actions requires the transformer to model policy improvement. The resulting transformer can explore, exploit, and maximize return when its weights are frozen, allowing it to learn the reinforcement learning algorithm itself. Figure \ref{fig:algo_distillation} provides a visual illustration of the entire algorithm distillation pipeline.