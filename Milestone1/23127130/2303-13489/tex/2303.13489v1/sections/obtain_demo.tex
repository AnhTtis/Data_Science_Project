\section{Demonstration Collection}
\label{sec:obtain_demo}

In this section, we discuss how to collect demonstrations. 
Particularly, we focus on demonstration collection for embodied AI (or learning-based robotics) tasks, because it involves demonstrations from both artificial agents and humans, as well as from both simulation environments and the real world. We primarily discuss how to acquire expert (or near-expert) demonstrations, as suboptimal demonstrations are relatively easy to obtain.

In demonstrations for embodied AI tasks (e.g., object manipulation), the \textit{embodiment} of the agent to complete the task can be either robot or human, and the \textit{operator} of the agent can be either an autonomous system or human. We, therefore, categorize the demonstration collection methods by these two perspectives. 


\subsection{Robot Embodiment - Human Operator}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/rfuniverse_vr.png}
    % \vspace{-0.5cm}
    \caption{
       The VR interface with HTC Vive and Noitom Glove in RFUniverse \cite{rfuniverse}.
    }
    % \vspace{-0.5cm}
    \label{fig:rfu_vr}
\end{figure}

\subsubsection{Simulation}

In the context of Embodied AI, demonstrations are often collected from human operators via teleoperation. This can be achieved using simple devices such as a mouse, keyboard, or smartphone. As these devices are easily accessible, many robot simulation environments provide interfaces that allow users to control the robot in the simulation. For example, robosuite \cite{zhu2020robosuite} provides utilities for collecting human demonstrations using a keyboard or 3D mouse devices, while iGibson \cite{igibson} is equipped with a graphical user interface that facilitates user interactions with the simulation environment. RoboTurk \cite{roboturk} offers intuitive 6 degrees-of-freedom motion control, which maps smartphone movement to robot arm movement. The simplicity of these devices makes it possible to leverage crowdsourcing to obtain data for various robotic tasks.

Recently, virtual reality (VR) interfaces have gained popularity for collecting robot demonstrations due to their flexibility. For instance, the demonstrations used in DAPG \cite{dapg} were collected in VR. Franka Kitchen environment \cite{franka_kitchen} collects demonstrations using the PUPPET MuJoCo VR system \cite{mujoco_vr}. iGibson 2.0 \cite{igibson2} includes a novel VR interface that is compatible with major commercially available VR headsets through OpenVR \cite{openvr}. RFUniverse \cite{rfuniverse} can connect to different VR devices through SteamVR, including novel VR gloves, as illustrated in Fig. \ref{fig:rfu_vr}.

One of the major advantages of VR is the immersive experience: humans can embody an avatar in the same scene and for the same task as the AI agents. This allows for a more realistic interaction between humans and robots. For example, as shown in Fig \ref{fig:igibson2_vr}, the virtual reality avatar in iGibson 2.0 is composed of a main body, two hands, and a head. The human controls the motion of the head and the two hands via the VR headset and hand controllers, with an optional additional tracker for control of the main body. Humans receive stereo images as generated from the point of view of the head of the virtual avatar.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/igibson_2_vr.png}
    % \vspace{-0.5cm}
    \caption{
       The VR interface in iGibson 2.0 \cite{igibson2}.
    }
    % \vspace{-0.5cm}
    \label{fig:igibson2_vr}
\end{figure}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/rt1_data.png}
    % \vspace{-0.5cm}
    \caption{
       RT-1’s \cite{rt1} large-scale, real-world training (130k demonstrations) and evaluation (3000 trials) dataset.
    }
    % \vspace{-0.5cm}
    \label{fig:rt1_data}
\end{figure*}

\subsubsection{Real World}

Furthermore, VR-based teleoperation has also been used to collect demonstrations in the real world. The SayCan system \cite{saycan} is an example of such an approach, where 68,000 teleoperated demonstrations were performed by 10 robots over 11 months. To collect the demonstrations, the operators used VR headset controllers to track the motion of their hand, which was then mapped onto the robot’s end-effector pose. In addition, a joystick was used to move the robot’s base. Human raters filtered the data to exclude unsafe, undesirable, or infeasible episodes.

With sufficient resources, VR-based teleoperation can be used to collect even larger demonstration datasets. For instance, the multi-task robot transformer RT-1 \cite{rt1} is trained on a large-scale real-world robotics dataset of 130k episodes covering over 700 tasks, which was collected using a fleet of 13 robots over 17 months. The demonstration collection process involves each of the robots autonomously approaching its station at the beginning of the episode and communicating the instruction that the operator should demonstrate to the robot. Demonstrations are collected with direct line-of-sight between the operator and robot using two virtual reality remotes, with 3D position and rotational displacements of the remote mapped to 6D displacements of the robot tool. The joystick's X and Y position is mapped to the turning angle and driving distance of the mobile base, respectively. An illustration of their dataset is shown in Fig \ref{fig:rt1_data}.


\subsection{Robot Embodiment - Autonomous Operator}

\subsubsection{Simulation}

While collecting demonstrations by humans is a straightforward approach, scalability is still a potential issue. To address this, a number of works have explored the use of autonomous agents to generate demonstrations in a more scalable manner.

One approach to generate large-scale demonstrations is by using a \textit{planner}. In some high-level planning tasks, such as Watch-And-Help \cite{puig2020watch} and ALFRED \cite{shridhar2020alfred}, symbolic planners \cite{hoffmann2001ff} can be used to generate demonstrations at a low cost. Meanwhile, in low-level control tasks, such as robot manipulation, motion planners \cite{kuffner2000rrt, prm} can be utilized. For example, RLbench \cite{rlbench} relies on an infinite supply of generated demonstrations collected via motion planners. Compared to the crowd-sourcing way to collect demonstrations, the planner-based system cannot be run in the real world, but in exchange, it receives the ability to generate a diverse range of tasks in a scalable way.

In addition to planning, learning-based methods can also be used to generate demonstrations. Various different methods are deployed in D4RL \cite{d4rl}. For Gym-MuJuCo tasks, expert demonstrations are generated by RL agents. For AntMaze tasks, they are generated by training a goal-reaching policy and using it in conjunction with a high-level waypoint generator. In Adroit, a large amount of expert data can be obtained by a learning-from-demonstration agent trained with a small number of human demonstrations.

ManiSkill \cite{mu2021maniskill} also uses RL agents to collect demonstrations, but training a single RL agent to solve a task can be challenging due to the difficulty of the tasks. Therefore, it trains RL agents in a divide-and-conquer way, which is a more scalable way to solve tasks and generate demonstrations. This will be discussed in detail in Section \ref{ms_demo_collection}.


\subsubsection{Real World}

Collecting demonstration data in the real world can be challenging due to the high cost of sample collection and the limitations of some methods, such as motion planners. To address these challenges, demonstration collection systems must be designed carefully.

One example of such a system is QT-opt \cite{qt_opt}, which collects self-supervised demonstration data for robot grasping in the real world. The system runs a reinforcement learning algorithm directly on the robot, receiving a binary reward for lifting an object successfully without any other reward shaping. Success is determined using a background subtraction test after the picked object is dropped. The setup for collecting data in QT-opt is shown in Figure \ref{fig:qt_opt}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/qt_opt.png}
    % \vspace{-0.5cm}
    \caption{
       QT-opt \cite{qt_opt} collects grasping demonstration data with autonomous self-supervision.
    }
    % \vspace{-3cm}
    \label{fig:qt_opt}
\end{figure}

MT-opt \cite{mt_opt} is another demonstration collection system that continuously improves multi-task policies. Like QT-opt, it uses success detectors for self-supervised reward labeling, but in MT-opt, the success detectors are trained on data from all tasks and continuously updated to account for distribution shifts caused by factors such as varying lighting conditions and changing backgrounds in the real world. Additionally, MT-opt leverages the solutions to easier tasks to effectively bootstrap learning of more complex tasks, allowing the robot to train policies for harder tasks over time and collect better data for those tasks.

\subsection{Human Embodiment - Human Operator}

While robot demonstrations are naturally good for learning robotic tasks, there are a lot of works that try to let robots learn from human videos \cite{smith2019avid, yu2018one, nair2022r3m}. Therefore, in this section, we briefly discuss the data collection process of some representative datasets of human daily activities.


Ego4D \cite{ego4d} is a large-scale egocentric video dataset and benchmark suite that includes 3,670 hours of daily life activity videos captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The dataset spans hundreds of scenarios, such as household, outdoor, workplace, leisure, and more. The key challenge of data collection is to recruit participants to record their daily activities. To overcome this challenge, the Ego4D project consists of 14 teams from universities and labs in 9 countries and 5 continents. Each team recruited participants to wear a camera for 1 to 10 hours at a time, and participants in 74 cities were recruited through word of mouth, ads, and postings on community bulletin boards.

RoboTube \cite{robotube} is a recently proposed dataset that provides a human video dataset and its digital twins for learning various robotic manipulation tasks. To collect the human video datasets, a portable video collection system with two RealSense D435 cameras was designed. The recording process streams two viewpoints: one is the first-person perspective from the camera mounted on the human head, and the other is the third-person perspective from the camera fixed on a tripod placed near the scene. These two streams are temporally synchronized, which enables the collection of high-quality video data for learning robotic manipulation tasks.