\section{Case Study: The Pipeline of Collecting and Utilizing Demonstrations in ManiSkill}
\label{sec:case_study}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/ms2.png}
    % \vspace{-0.5cm}
    \caption{
       The tasks in ManiSkill2 \cite{maniskill2}.
    }
    % \vspace{-0.5cm}
    \label{fig:ms2}
\end{figure*}

In this section, we present ManiSkill \cite{mu2021maniskill} (as well as its successor \cite{maniskill2}) as an example to demonstrate the entire pipeline of collecting and utilizing demonstrations. Henceforth, we refer to this series of works as ManiSkill for the sake of brevity.

ManiSkill is a benchmark for manipulation skills that can be generalized across a range of tasks. It comprises 20 manipulation task families with over 2000 object models and more than 4 million demonstration frames. These tasks include stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation, with 2D/3D-input data simulated by fully dynamic engines. It provides a unified interface and evaluation protocol to support a wide range of algorithms such as classic sense-plan-act, RL, and IL. It also accommodates various visual observations (point cloud, RGBD) and controllers (e.g., action type and parameterization). Fig \ref{fig:ms2} displays the visualization of tasks in ManiSkill2.

\subsection{Collecting Demonstrations}
\label{ms_demo_collection}

ManiSkill's approach of using different methods to generate demonstrations is motivated by the fact that the various tasks in the benchmark exhibit distinct characteristics, and thus, require tailored techniques. Specifically, task and motion planning (TAMP), model predictive control (MPC), and reinforcement learning (RL) are employed to generate demonstrations based on the task's difficulty and requirements.

TAMP is a suitable approach for many stationary manipulation tasks such as pick-and-place, as it does not require crafting rewards. However, it can face difficulties in dealing with underactuated systems like pushing chairs or moving buckets. In contrast, MPC is capable of searching for solutions to difficult tasks when given well-designed shaped rewards without the need for training or observations. However, designing a universal shaped reward for a variety of objects is non-trivial. On the other hand, RL requires additional training and hyperparameter tuning but is more scalable than MPC during inference.

For most rigid-body manipulation tasks and soft-body manipulation tasks, the demonstrations are generated through TAMP. However, for tasks involving manipulating articulated objects, the demonstration collection becomes more complex. Not only is manipulating articulated objects more challenging, but finding a universal solution for all object instances within a single task is also not straightforward.

For stationary manipulation tasks, such as turning a faucet, some faucet models can be solved by TAMP, while the remaining ones require solutions from MPC-CEM, utilizing designed dense rewards.

In the case of mobile manipulation tasks, such as pushing a swivel chair, TAMP is not a viable approach, and training a single RL agent to collect demonstrations for all objects is difficult. However, since training an agent to solve a specific object is feasible and well-studied, the demonstrations can be generated using a divide-and-conquer approach, wherein an RL agent is trained for each environment to generate successful demonstrations.


\subsection{Utilizing Demonstrations}

In ManiSkill, various demonstration-based baselines are evaluated to demonstrate the effectiveness of utilizing demonstrations.

To utilize demonstrations offline, ManiSkill benchmarks behavior cloning (BC), Batch-Constrained Q-Learning (BCQ) \cite{bcq}, and Twin-Delayed DDPG \cite{td3} with Behavior Cloning (TD3+BC) \cite{td3_bc}. The results show that when near-optimal demonstrations are available, BC achieves the strongest performance.

To augment online reinforcement learning with demonstrations, ManiSkill tests DAPG \cite{dapg}, which significantly outperforms the PPO \cite{ppo} baseline.

In addition, \cite{shen2022learning} investigates how to leverage large-scale demonstrations in ManiSkill by combining GAIL \cite{gail} and GASIL \cite{gasil} to utilize demonstrations in online reinforcement learning. The results show that combining offline imitation learning and online reinforcement learning yields much better object-level generalization in manipulation tasks.