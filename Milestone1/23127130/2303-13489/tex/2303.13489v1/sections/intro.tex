\section{Introduction}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IEEEPARstart{R}{einforcement} learning (RL) and planning are two popular methods to solving sequential decision making problems. 
Both approaches, as well as their combination, made remarkable progress in solving difficult tasks (e.g., Go \cite{alphago, alphago_zero}, video games \cite{dqn, openai_five}, robot control \cite{hwangbo2019learning}) when given the advantages of deep learning. 
RL algorithms allow agents to learn through trial and error, adjusting their behavior based on the consequences of their actions. Planning algorithms, on the other hand, involve the use of a model of the environment to make decisions based on the predicted outcomes of different actions.
While traditional planning techniques demand a model as an input, many recent works \cite{pets, planet, muzero, efficient_zero, td_mpc} learn a model by interacting with environments and then plan in the learned model, which is very reminiscent of the RL framework. As a result, nowadays the distinction between RL and planning is hazy.



Typically, deep RL algorithms require tremendous training samples, resulting in very high sample complexity, which refers to the number of samples required for learning an approximately optimal policy. Particularly, unlike the supervised learning paradigm that learns from historically labeled data, typical RL algorithms require the interaction data by running the latest policy in the environment. Once the policy updates, the underlying data distribution (formally the occupancy measure \cite{syed2008apprenticeship}) changes, and the data has to be collected again by running the policy.
This also applies to the planning methods that learn the model interactively.
As such, RL algorithms with high sample complexity are hard to be directly applied in real-world tasks, where trial-and-errors can be highly costly.

However, many domains, such as autonomous driving \cite{sun2020scalability} and robotics \cite{dasari2019robonet} contain a wealth of pre-collected demonstrations for various tasks, which can be leveraged to boost reinforcement learning and planning agents. 
One key benefit of using demonstrations is that it allows agents to learn from expert knowledge, rather than having to discover the optimal actions through exploration. This can be particularly useful in complex or dynamic environments, where trial-and-error learning may be impractical or infeasible. For example, learning from demonstrations has been used to train agents to perform tasks such as robot manipulation \cite{saycan, rt1}, where the expert knowledge of a human operator can be used to guide the learning process.
Another advantage of leveraging demonstrations is that it can reduce the amount of data and computational resources required for training. Because the agent can learn from expert demonstrations, rather than having to discover the optimal actions through exploration, the amount of data needed to train the agent can be significantly smaller. This can make learning from demonstrations particularly attractive in situations where data collection is expensive or time-consuming.

The demonstration data can be used in numerous ways. When used offline, one can learn the policies, skills, world models, rewards, representations, or even the learning algorithm itself from the demonstrations. When used online, the demonstrations can serve as experience, regularization, reference, or curriculum. 
In Sec \ref{sec:use_demo_offline} and \ref{sec:use_demo_online}, we will go into greater detail about these various ways to use demonstrations.

It is crucial to research effective methods for collecting demonstrations of high quality and quantity to support those approaches that rely on them. Note that the demonstrations could be collected either from human experts or artificial agents (like learned policy or planners). Therefore, in Sec \ref{sec:obtain_demo} we discuss the systems that collect demonstrations with and without humans, in simulated environments and in the real world.

To summarize, we conduct a thorough analysis of numerous aspects of boosting reinforcement learning and planning through demonstrations. 
We begin by summarizing the background and preliminary knowledge about RL and planning in Sec \ref{sec:background}.
The use of demonstrations in planning and reinforcement learning is then covered in Sec \ref{sec:use_demo_offline} and \ref{sec:use_demo_online}.
Sec \ref{sec:obtain_demo} describes several existing methods of collecting demonstrations. 
In Sec \ref{sec:case_study}, a complete pipeline of collecting and utilizing demonstrations is demonstrated using ManiSkill \cite{mu2021maniskill} as an example.
Sec \ref{sec:case_study} takes ManiSkill \cite{mu2021maniskill} as an example to show a full pipeline of collecting and utilizing demonstrations. 
In Sec \ref{sec:discussion}, we conclude the entire survey and discuss the future directions in this area.
