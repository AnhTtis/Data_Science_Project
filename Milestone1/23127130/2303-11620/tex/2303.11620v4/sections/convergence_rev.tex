In this section, we describe the RGD algorithm for solving the alignment problem in Eq.~(\ref{eq:GPOP}). Using the theory of Morse functions, we show that if the sequence of iterates generated by RGD converges to a non-degenerate alignment then the convergence is linear. Moreover, We obtain an estimate of the radius and the rate of convergence. We also present an exact recovery and noise stability analysis of RGD, initialized using the output of the spectral algorithm (SPEC) \citea{chaudhury2015global}.
%We end with a simulation demonstrating that the alignment error decays in a manner consistent with linear order of convergence, as RGD refines the spectral alignment.
% In Section~\ref{subsec:rgd_algo}, we describe the RGD algorithm for solving the alignment problem in Eq.~(\ref{eq:GPOP}). Then in Section~\ref{subsec:loc_lin_conv}, using the theory of Morse functions and Proposition~\ref{prop:HessVicinity}, we show that if the sequence of iterates generated by RGD converges to a non-degenerate alignment then the convergence is linear (Theorem~\ref{thm:rgd_conv}). Moreover, combining \citeb[Theorem~4.2]{udriste2013convex} with the bounds on Hessian at a non-degenerate alignment in Proposition~\ref{prop:HessVicinity}, we obtain an estimate of the radius and the rate of local linear convergence of RGD (Theorem~\ref{thm:rgd_conv2}). Finally, in Section~\ref{subsec:noise_stability}, we present an exact recovery and noise stability analysis of our RGD algorithm, initialized using the output of a spectral relaxation-based algorithm \citea{chaudhury2015global} to solve Eq.~(\ref{eq:GPOP}). We further provide a simulation demonstrating that the alignment error decays in a manner consistent with linear order of convergence, as RGD refines the spectral alignment.

\subsection{RGD Algorithm}
\label{subsec:rgd_algo}
A standard way to find a local minimum of Eq.~(\ref{eq:GPOP}) is to use RGD with a suitable initial point, step size and retraction strategy. In this work we use retraction based on the exponential map on $\mathbb{O}(d)$ \citea{van1996matrix, udriste2013convex}. Define,
\begin{align}
    R_{\EXP }: \cup_{\mathbf{S} \in \mathbb{O}(d)^m}(\{\mathbf{S}\} \times T_\mathbf{S}\mathbb{O}(d)^m) &\mapsto \mathbb{O}(d)^m\\
    R_{\EXP }\left([\mathbf{S}_i]_1^m, [\boldsymbol{\xi}_i]_1^m\right) &= [\mathbf{S}_i\exp(\mathbf{S}_i^T\boldsymbol{\xi}_i)]_1^m. \label{eq:R_PF}
\end{align}
where $\exp (\mathbf{A})$ denotes the matrix exponential of $\mathbf{A}$ \citea{van1996matrix, absil2009optimization}. Then the following lemma provides a consistent definition of a retraction on the quotient manifold $\mathbb{O}(d)^m/_{\sim}$.
\begin{lem}
\label{lem:retraction}
Let $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^{m}/_{\sim}$ and $\mathbf{S}^a, \mathbf{S}^b \in \pi^{-1}(\widetilde{\mathbf{S}})$. If $\mathbf{Z}^a \in T_{\mathbf{S}^a}\mathbb{O}(d)^m$ and $\mathbf{Z}^b \in T_{\mathbf{S}^b}\mathbb{O}(d)^m$ are the horizontal lifts of $\widetilde{\mathbf{Z}} \in T_{\widetilde{\mathbf{S}}}\mathbb{O}(d)^{m}/_{\sim}$ then $\pi(R_{\EXP }(\mathbf{S}^a, \mathbf{Z}^a)) = \pi(R_{\EXP }(\mathbf{S}^b, \mathbf{Z}^b))$ (see Eq.~\ref{eq:pi}). As a result, the retraction
\begin{align}
    \widetilde{R}_{\EXP }: \cup_{\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim} }(\{\widetilde{\mathbf{S}}\} \times T_{\widetilde{\mathbf{S}}}\mathbb{O}(d)^m/_{\sim}) &\mapsto \mathbb{O}(d)^m/_{\sim}\\
    \widetilde{R}_{\EXP }\left(\widetilde{\mathbf{S}}, \widetilde{\mathbf{Z}}\right) &=  \pi(R_{\EXP }(\mathbf{S}, \mathbf{Z}))\label{eq:Rtilde_PF}
\end{align}
is well defined for any $\mathbf{S} \in \pi^{-1}(\widetilde{\mathbf{S}})$ and $\mathbf{Z}$ being the horizontal lift of $\widetilde{\mathbf{Z}}$ at $\mathbf{S}$.
\end{lem}

The step direction will always be the horizontal lift of $-\grad \widetilde{F}(\widetilde{\mathbf{S}})$ at some $\mathbf{S} \in \pi^{-1}(\widetilde{\mathbf{S}})$. Consequently, due to Proposition~\ref{prop:gradFS}, the step direction is $\boldsymbol{\xi} = -\grad F(\mathbf{S}) = [[\mathbf{C}\mathbf{S}]_i - \mathbf{S}_i[\mathbf{C}\mathbf{S}]_i^T\mathbf{S}_i]_1^m$, the projection of the antigradient $-\nabla F(\mathbf{S})$ onto $T_\mathbf{S}\mathbb{O}(d)^m$. The step size $\alpha$ is calculated using the Armijo-type rule with parameters $\beta,\gamma \in (0,1)$ (here $g$ is the canonical metric on $\mathbb{O}(d)^m$ as in Eq.~(\ref{eq:g_Z_W})),
\begin{equation}
    \alpha = \max_{l \geq 0}\{\beta^l\ \vertbar\ F(R_{\EXP }(\mathbf{S}, -\beta^l\grad F(\mathbf{S}))) - F(\mathbf{S}) \leq -\gamma \beta^l g(\nabla F(\mathbf{S}),  \grad F(\mathbf{S})) \}. \label{eq:armijo_step}
\end{equation}
Since $F$ extends to a continuously differentiable non-negative function on $\mathbb{R}^{md \times d}$ containing $\mathbb{O}(d)^m$, it follows from \citeb[Proposition 2.8]{schneider2015convergence} that $\alpha$ is well-defined.
\begin{algorithm}[H]
\caption{Riemannian gradient descent for solving GPOP \label{algo:rgd}}
\begin{algorithmic}[1]
\REQUIRE $\widetilde{\mathbf{S}}^0 \in \mathbb{O}(d)^{m-1}$, $\Gamma$, $\{\mathbf{x}_{k,i}: (k,i) \in E(\Gamma)\}$, $\beta, \gamma \in (0,1)$
\STATE Construct $\mathbf{C}$ as in Eq.~(\ref{eq:GPOP}).
\REPEAT
    \STATE set $\mathbf{S}^k = [\mathbf{I}_d; \widetilde{\mathbf{S}}^k] \in \pi^{-1}(\widetilde{\mathbf{S}}^k) \subset \mathbb{O}(d)^m$ (Eq.~\ref{eq:pi_inv_wtS}).
    \STATE calculate the descent direction $-\grad F(\mathbf{S}^k)$ at $\mathbf{S}^k$ using Eq.~(\ref{eq:gradFS}).
    \STATE calculate the step size $\alpha_k$ according to the Armijo-type rule (see Eq.~(\ref{eq:armijo_step})).
    \STATE set $\widetilde{\mathbf{S}}^{k+1} = \pi(R_{\EXP}(\mathbf{S}^k, -\alpha_k \grad F(\mathbf{S}^k)))$ using Eq.~(\ref{eq:R_PF}, \ref{eq:pi}).
    \STATE $k \leftarrow k + 1$.
\UNTIL{convergence.}
\end{algorithmic}
\end{algorithm}
\subsection{Local linear Convergence of RGD}
\label{subsec:loc_lin_conv}
% We proceed to show the local linear convergence of Algorithm~\ref{algo:rgd} to a non-degenerate alignment (see Definition~\ref{def:non_deg_alignment0}). First, using the convergence analysis framework presented in \citea{schneider2015convergence} and as used in \citea{liu2019quadratic}, we show that if the sequence of iterates $\{\widetilde{\mathbf{S}}^k\}_{k\geq 0}$ generated by Algorithm~\ref{algo:rgd} converges to a non-degenerate $\widetilde{\mathbf{S}}^*$ then the convergence is linear. Then, using \citeb[Theorem~4.2]{udriste2013convex} and Proposition~\ref{prop:HessVicinity} we obtain an estimate of the radius and rate of convergence.
We proceed to show the local linear convergence of Algorithm~\ref{algo:rgd} to a non-degenerate alignment using the convergence analysis framework presented in \citea{schneider2015convergence} and as used in \citea{liu2019quadratic}.
To this end, we note that $\widetilde{F}$ and $F$ are real-analytic functions bounded from below by zero. $\mathbb{O}(d)^m/_{\sim}$  (whose elements are identified with $\mathbb{O}(d)^{m-1}$ here) and $\mathbb{O}(d)^m$ are compact submanifolds of $\mathbb{R}^{(m-1)d \times d}$ and $\mathbb{R}^{md \times d}$, respectively.
However, since $F(\mathbf{S}\mathbf{Q}) = F(\mathbf{S})$ for all $\mathbf{Q} \in \mathbb{O}(d)$, therefore every critical point of $F$ is degenerate and in particular $F$ is not a Morse-function \citea{cohen_iga_norbury_2006}. Nevertheless, if $\mathbf{S}^*$ is a non-degenerate alignment then $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ is a non-degenerate critical point of $\widetilde{F}$. As a result $\widetilde{F}$ is a Morse function at $\widetilde{\mathbf{S}}^*$ and, due to \citeb[Proposition 4.2]{hu2018convergence}, the Lojasiewicz gradient inequality is satisfied.
\begin{prop}
Let $\mathbf{S}^*$ be a non-degenerate alignment and define $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$. Then there exist $\delta, \eta > 0$ such that $$|\widetilde{F}(\widetilde{\mathbf{S}}) - \widetilde{F}(\widetilde{\mathbf{S}}^*)| \leq \eta \left\| \grad \widetilde{F}(\widetilde{\mathbf{S}})\right\|_F^2$$
% \begin{equation}
%     |\widetilde{F}(\widetilde{\mathbf{S}}) - \widetilde{F}(\widetilde{\mathbf{S}}^*)| \leq \eta \left\| \grad \widetilde{F}(\widetilde{\mathbf{S}})\right\|_F^2. \label{eq:Lojasiewicz_gradient_ineq_half}
% \end{equation}
holds for every $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim}$ satisfying $\left\|\widetilde{\mathbf{S}}-\widetilde{\mathbf{S}}^*\right\|_F < \delta$.
\end{prop}
% Thus, the Lojasiewicz gradient inequality \citea{lojasiewicz1965ensembles}, \citeb[Section 2.2]{schneider2015convergence} holds at every $\widetilde{\mathbf{S}}^* \in \mathbb{O}(d)^m/_{\sim}$ and in particular for every $\widetilde{\mathbf{S}}^* \in \widetilde{\mathcal{C}}$ (see Eq.~(\ref{eq:crit_pts})) i.e. there exist $\delta, \eta > 0$ and $\theta \in (0,1/2]$ (generally dependent on $\widetilde{\mathbf{S}}^*$) such that
% \begin{equation}
%     |\widetilde{F}(\widetilde{\mathbf{S}}) - \widetilde{F}(\widetilde{\mathbf{S}}^*)|^{1-\theta} \leq \eta \left\| \grad \widetilde{F}(\widetilde{\mathbf{S}})\right\|_F. \label{eq:Lojasiewicz_gradient_ineq}
% \end{equation}
% holds for every $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim}$ satisfying $\left\|\widetilde{\mathbf{S}}-\widetilde{\mathbf{S}}^*\right\|_F < \delta$.

Moreover, the iterates $\{\widetilde{\mathbf{S}}^k\}_{k \geq 0}$ generated by Algorithm~\ref{algo:rgd} satisfy the (\textbf{A1}) sufficient descent, (\textbf{A2}) stationarity and (\textbf{A3}) safeguard assumptions below. The proofs are in the appendix and we make use of the following results to prove them.
\begin{prop}
\label{prop:liu_pf}
For all $\mathbf{S}_i \in \mathbb{O}(d)$ and $\mathbf{Z}_i \in T_{\mathbf{S}_i}\mathbb{O}(d)$ satisfying $\left\|\mathbf{Z}_i\right\|_F \leq 1$,
$$\left\|\mathbf{S}_i\exp (\mathbf{S}_i^T\mathbf{Z}_i) - (\mathbf{S}_i + \mathbf{Z}_i)\right\|_F \leq (e-1)\left\|\mathbf{Z}_i\right\|_F^2.$$
\end{prop}
% \begin{prop}
% \label{prop:second_order_boundedness_of_RPF}
% For $\mathbf{S} \in \mathbb{O}(d)^m$ and $\boldsymbol{\xi} \in T_{\mathbf{S}}\mathbb{O}(d)^m$ satisfying $\left\|\boldsymbol{\xi}\right\|_F \leq 1$,
% \begin{enumerate}[label=(\alph*)]
%     \item $\left\|R_\EXP(\mathbf{S}, \boldsymbol{\xi}) - (\mathbf{S} + \boldsymbol{\xi})\right\|_F \leq (e-1)\left\|\boldsymbol{\xi}\right\|_F^2$ and
%     \item $\left\|R_\EXP(\mathbf{S}, \boldsymbol{\xi})(\mathbf{S}_1\exp (\mathbf{S}_1^T\boldsymbol{\xi}_1))^T - (\mathbf{S} + \boldsymbol{\xi})(\mathbf{S}_1 + \boldsymbol{\xi}_1)^T\right\|_F \leq 2(e-1)\sqrt{m}\left\|\boldsymbol{\xi}\right\|_F^2$.
% \end{enumerate}
% \end{prop}
\begin{prop}
\label{prop:second_order_boundedness_of_Rtilde}
For $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim}$ and $\widetilde{\mathbf{Z}} \in T_{\widetilde{\mathbf{S}}}\mathbb{O}(d)^m/_{\sim}$ satisfying  $\left\|\widetilde{\mathbf{Z}}\right\|_F \leq 1/2$,
\begin{enumerate}[leftmargin=*,label=(\alph*)]
    \item $\left\|R_\EXP(\mathbf{S}, \mathbf{Z}) - (\mathbf{S} + \mathbf{Z})\right\|_F \leq (e-1)\left\|\mathbf{Z}\right\|_F^2$ for any $\mathbf{S} \in \pi^{-1}(\widetilde{\mathbf{S}})$ and $\mathbf{Z} \in T_{\mathbf{S}}\mathbb{O}(d)^m$, the horizontal lift of $\widetilde{\mathbf{Z}}$ at $\mathbf{S}$. 
    \item $\left\|\widetilde{R}_\EXP(\widetilde{\mathbf{S}}, \widetilde{\mathbf{Z}}) - (\widetilde{\mathbf{S}} + \widetilde{\mathbf{Z}})\right\|_F \leq (e-1)\left\|\widetilde{\mathbf{Z}}\right\|_F^2$.
\end{enumerate}
\end{prop}
\begin{prop}
\label{prop:alpha_grad}
%$(\alpha_k)_{k \geq 0}$, $(\widetilde{\mathbf{S}}^k)_{k \geq 0}$ and $(\mathbf{S}^k)_{k \geq 0}$ satisfy 
$\lim \alpha_k \left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F = 0$ and $\lim \alpha_k \left\|\grad F(\mathbf{S}^k)\right\|_F = 0$.
\end{prop}

\noindent \textbf{(A1)}. \textit{(Sufficient Descent)} There exist $\kappa_0 > 0$ and $k_1 \in \mathbb{N}$ such that, the inequality $\widetilde{F}(\widetilde{\mathbf{S}}^{k+1}) - \widetilde{F}(\widetilde{\mathbf{S}}^k) \leq - \kappa_0 \left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F \cdot \left\|\widetilde{\mathbf{S}}^{k+1}-\widetilde{\mathbf{S}}^k\right\|_F$ holds for all $k \geq k_1$.
\smallskip

\noindent \textbf{(A2)}. \textit{(Stationarity)} There exist $k_2 \in \mathbb{N}$ such that for all $k \geq k_2$, if $\left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F = 0$ then $\widetilde{\mathbf{S}}^{k+1} = \widetilde{\mathbf{S}}^k$. The sequence $\{\widetilde{\mathbf{S}}^{k}\}_{k \geq 0}$ satisfies this trivially.
\smallskip

\noindent \textbf{(A3)}. \textit{(Safeguard)} There exist a constant $\mu > 0$ and $k_3 \in \mathbb{N}$ such that the inequality $\left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F \leq \mu \left\|\widetilde{\mathbf{S}}^{k+1}-\widetilde{\mathbf{S}}^k\right\|_F$ holds for all $k \geq k_3$.

Combined with Theorem 2.3 in \citea{schneider2015convergence} and the fact that $\mathbb{O}(d)^m/_{\sim}$ is compact (thus every sequence on it has a cluster point), we obtain the following result.
\begin{thm}
\label{thm:rgd_conv}
Let $\mathbf{S}^*$ be a non-degenerate alignment and $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$. If the sequence $\{\widetilde{\mathbf{S}}^k\}_{k \geq 0}$ due to Algorithm~\ref{algo:rgd} converges to $\widetilde{\mathbf{S}}^*$ then the convergence is linear.
\end{thm}

Finally, we obtain an estimate of the radius and rate of linear convergence. The proof follows directly from \citeb[Chapter 7, Theorem~4.2]{udriste2013convex} (here $d_{\widetilde{g}}$ is the geodesic distance induced by the metric $\widetilde{g}$ on $\mathbb{O}(d)^m/_\sim$ as defined in Proposition~\ref{prop:g_tilde}).
% \begin{thm}
% \label{thm:rgd_conv2}
% Let $\mathbf{S}^*$ be a non-degenerate alignment
% %i.e. $\mathbf{S}^* \in \mathcal{C}$ (Eq.~(\ref{eq:crit_pts2})) and $\lambda_{d(d-1)/2+1}(\mathbb{L}(\mathbf{S}^*)) > 0$, 
% and $\zeta \in (0,1)$ be fixed. Let $\lambda_{-}(\mathbf{S}^*)$, $\lambda_{+}(\mathbf{S}^*)$ and $\delta(\mathbf{S}^*)$ be as defined in Proposition~\ref{prop:HessVicinity} (Corollary~\ref{cor:HessVicinity}) for the noisy (noiseless) setting. Then Algorithm~\ref{algo:rgd} converges to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ linearly when initialized with $\widetilde{\mathbf{S}}^0 = \pi(\mathbf{S}^0)$ satisfying $\min_{\mathbf{Q}\in \mathbb{O}(d)}\left\|\mathbf{S}^0-\mathbf{S}^*\mathbf{Q}\right\|_F < \zeta\delta(\mathbf{S}^*)$. Moreover,
% \begin{align}
%     \widetilde{F}(\widetilde{\mathbf{S}}^k) - \widetilde{F}(\widetilde{\mathbf{S}}^*) &\leq q^{k}(\widetilde{F}(\widetilde{\mathbf{S}}^0)-\widetilde{F}(\widetilde{\mathbf{S}}^*))\label{eq:alignment_err_ratio}\\
%     %\left\|\widetilde{\mathbf{S}}^k-\widetilde{\mathbf{S}}^*\right\|_F &\leq C q^{(k-1)/2}
%     d_{\widetilde{g}}(\widetilde{\mathbf{S}}^k, \widetilde{\mathbf{S}}^*) &\leq C q^{(k-1)/2}
% \end{align}
% where $C > 0$ is a constant, $q = 1 - 2\gamma (1-\gamma)r(1+r) \in (0,1)$ and $r = \frac{(1-\zeta)\lambda_{-}(\mathbf{S}^*)}{\lambda_{+}(\mathbf{S}^*) +\zeta\lambda_{-}(\mathbf{S}^*)}$.
% \end{thm}
\begin{thm}
\label{thm:rgd_conv2}
Let $\mathbf{S}^*$ be a non-degenerate alignment
%i.e. $\mathbf{S}^* \in \mathcal{C}$ (Eq.~(\ref{eq:crit_pts2})) and $\lambda_{d(d-1)/2+1}(\mathbb{L}(\mathbf{S}^*)) > 0$, 
and $\zeta \in (0,1)$ be fixed. Let $\lambda_{-}(\mathbf{S}^*)$, $\lambda_{+}(\mathbf{S}^*)$ and $\delta(\mathbf{S}^*)$ be as defined in Proposition~\ref{prop:HessVicinity}. If  the initialization $\widetilde{\mathbf{S}}^0 = \pi(\mathbf{S}^0)$ of Algorithm~\ref{algo:rgd} and and the subsequent iterates $\widetilde{\mathbf{S}}^k = \pi(\mathbf{S}^k)$ generated by it satisfy $\min_{\mathbf{Q}\in \mathbb{O}(d)}\left\|\mathbf{S}^k-\mathbf{S}^*\mathbf{Q}\right\|_F < \min\left\{2,\frac{2}{\pi}\zeta\delta(\mathbf{S}^*)\right\}$, then the sequence $\{\widetilde{\mathbf{S}}^k\}_{k \geq 0}$ converges to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ linearly. Moreover,
\begin{align}
    \widetilde{F}(\widetilde{\mathbf{S}}^k) - \widetilde{F}(\widetilde{\mathbf{S}}^*) &\leq q^{k}(\widetilde{F}(\widetilde{\mathbf{S}}^0)-\widetilde{F}(\widetilde{\mathbf{S}}^*))\label{eq:alignment_err_ratio}\\
    %\left\|\widetilde{\mathbf{S}}^k-\widetilde{\mathbf{S}}^*\right\|_F &\leq C q^{(k-1)/2}
    d_{\widetilde{g}}(\widetilde{\mathbf{S}}^k, \widetilde{\mathbf{S}}^*) &\leq C q^{(k-1)/2}
\end{align}
where $C > 0$ is a constant, $q = 1 - 2\gamma (1-\gamma) r(1+r)\in (0,1)$ and $r = \frac{(1-\zeta)\lambda_{-}(\mathbf{S}^*)}{\lambda_{+}(\mathbf{S}^*) +\zeta\lambda_{-}(\mathbf{S}^*)}$.
\end{thm}

\subsection{Exact Recovery and Noise Stability}
\label{subsec:noise_stability}
A direct consequence of Theorem~\ref{thm:rgd_conv2} and Corollary~\ref{cor:HessVicinity} to the noiseless setting is that RGD converges locally linearly to a perfect alignment under a condition weaker than affine rigidity.
\begin{thm}
\label{thm:exact_recovery}
Suppose $\mathbf{S}^*$ is a perfect alignment. Then Algorithm~\ref{algo:rgd} converges locally linearly to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ if any of the following holds:
\begin{enumerate}
    \item $\rank(\mathbf{C}) = (m-1)d$.
    \item $\rank(\mathbb{L}(\mathbf{S}^*))  = (m-1)d(d-1)/2$.
\end{enumerate}
The first condition which characterizes affine rigidity implies the second that characterizes infinitesimal rigidity as well as generic local rigidity (Figure~\ref{fig:rigidity_flow}).
\end{thm}
% \begin{cor}
% \label{cor:rgd_conv3}
% Let $\mathbf{S}^*$ be a non-degenerate perfect alignment
% %i.e. $\mathbf{S}^* \in \mathcal{C}$ (Eq.~(\ref{eq:crit_pts2})) and $\lambda_{d(d-1)/2+1}(\mathbb{L}(\mathbf{S}^*)) > 0$, 
% and $\zeta \in (0,1)$ be fixed. Let $\delta_0(\mathbf{S}^*)$ be as defined in Corollary~\ref{cor:HessVicinity}. Then Algorithm~\ref{algo:rgd} converges to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ linearly when initialized with $\widetilde{\mathbf{S}}^0 = \pi(\mathbf{S}^0)$ that satisfies $\min_{\mathbf{Q}\in \mathbb{O}(d)}\left\|\mathbf{S}^0-\mathbf{S}^*\mathbf{Q}\right\|_F < \zeta\delta_0(\mathbf{S}^*)$. 
% % Moreover, $\widetilde{F}(\widetilde{\mathbf{S}}^k) \leq q^{k}\widetilde{F}(\widetilde{\mathbf{S}}^0)$ and $d_{\widetilde{g}}(\widetilde{\mathbf{S}}^k, \widetilde{\mathbf{S}}^*) \leq C q^{(k-1)/2}$
% % % \begin{align}
% % %     \widetilde{F}(\widetilde{\mathbf{S}}^k) &\leq q^{k}\widetilde{F}(\widetilde{\mathbf{S}}^0)\label{eq:alignment_err_ratio2}\\
% % %     %\left\|\widetilde{\mathbf{S}}^k-\widetilde{\mathbf{S}}^*\right\|_F &\leq C q^{(k-1)/2}
% % %     d_{\widetilde{g}}(\widetilde{\mathbf{S}}^k, \widetilde{\mathbf{S}}^*) &\leq C q^{(k-1)/2}
% % % \end{align}
% % where $C > 0$ is a constant, $q = 1 - 2\gamma (1-\gamma)r(1+r) \in (0,1)$ and $r = \frac{(1-\zeta)\lambda_{0_-}(\mathbf{S}^*)}{\lambda_{0_+}(\mathbf{S}^*) +\zeta\lambda_{0_-}(\mathbf{S}^*)}$.
% \end{cor}

While RGD achieves local linear convergence under less restrictive conditions, a key challenge lies in selecting an initial alignment that is sufficiently close to a non-degenerate perfect alignment $\mathbf{S}^*$. In \citea{chaudhury2015global}, the authors showed that under the affine rigidity constraints, SPEC recovers the perfect alignment, eliminating the need for RGD. However, under weaker non-degeneracy (equivalently, infinitesimal/local rigidity) constraints, it is yet to be established whether the output of SPEC recovers/remains close to a perfect alignment of noiseless views (similarly, to an optimal alignment of the noisy views). We aim to address this in our future work.

Nevertheless, under the bounded noise model and affine rigidity constraints, the spectral solution $\mathbf{S}_{spec}$ has been shown to approximate a perfect alignment $\mathbf{S}_0$ of the noiseless counterparts of the noisy views \citea{chaudhury2015global}. While $\mathbf{S}_0$ is generally not the optimal alignment $\mathbf{S}^*$ of the noisy views, one can expect them to be relatively close. Therefore, under affine rigidity constraints, we expect $\mathbf{S}_{spec}$ to be near the optimal alignment $\mathbf{S}^*$ of the noisy views and thus, refining $\mathbf{S}_{spec}$ using RGD could potentially yield $\mathbf{S}^*$. Here, we provide a noise stability analysis of RGD which support this idea.
%A more challenging problem is to develop a new initialization method for RGD or prove that the output of SPEC remains close to an optimal alignment under weaker infinitesimal/local rigidity constraints. We aim to address this in future work.

% Although the Algorithm~\ref{algo:rgd} enjoys local linear convergence under weaker constraints than those adopted in previous works, one limitation is how to choose an initial alignment $\widetilde{\mathbf{S}}^{0}$ so that it is close to the optimal alignment $\widetilde{\mathbf{S}}^*$. Previous works have shown that alignment obtained due to the spectral relaxation of the problem tend to be close to noiseless optimal alignment under affine rigidity constraints \citea{chaudhury2015global}. This means, under such stronger constraints one can expect the RGD algorithm initialized with spectral alignment to converge to a better if not optimal alignment. We provide a noise stability analysis of Algorithm~\ref{algo:rgd} that is consistent with the above idea. A more challenging problem is to devise a procedure to initialize RGD or show that the solution of spectral relaxation is close to a \textit{nice} alignment under weaker local rigidity constraints. We hope to deal with this in our future work.

%Here we investigate the noise stability of Algorithm~\ref{algo:rgd} when initialized with SPEC \citea{chaudhury2015global}.
We start with a set of noiseless views and inject them with bounded noise i.e.  $\mathbf{x}_{k,i} \leftarrow \mathbf{x}_{k,i} + \boldsymbol{\epsilon}_{k,i}$ where $\left\|\boldsymbol{\epsilon}_{k,i}\right\|_2 \leq \varepsilon$ for a fixed noise level $\varepsilon > 0$. Let $\mathbf{C}_0$ and $\mathbf{C}$ be the patch-stress matrices (Eq.~(\ref{eq:GPOP})) corresponding to the noiseless views and their noisy counterparts, respectively.
% Then the following lemma bounds the difference in the optimization landscapes in terms of the noise level.
% \begin{lem}
% \label{lem:landascape_diff}
% Let $\mathbf{S} \in \mathbb{O}(d)^m$ be a fixed alignment. Then,
% \begin{equation}
%     |\Tr(\mathbf{C}\mathbf{S}\mathbf{S}^T) - \Tr(\mathbf{C}_0\mathbf{S}\mathbf{S}^T)| \leq m\sqrt{d}(k_1\varepsilon + k_2\varepsilon^2)
% \end{equation}
% where the constants $k_1$ and $k_2$ are
% \begin{align}
%     k_1 &= 2\sqrt{n |E(\Gamma)|}\left(4 \max_1^n\left\|\mathbf{x}_k\right\|_2\frac{\sqrt{n|E(\Gamma)|}}{\lambda_{2}(\mathbf{\mathcal{L}}_{\Gamma})} + 1\right)\\
%     k_2 &= 2\sqrt{n |E(\Gamma)|}\left(2\frac{\sqrt{n|E(\Gamma)|}}{\lambda_{2}(\mathbf{\mathcal{L}}_{\Gamma})} + 1\right).
% \end{align}
% \end{lem}
% The proof directly follows from Cauchy-Schwarz inequality, the fact that $\left\|\mathbf{S}\mathbf{S}^T\right\|_F = m\sqrt{d}$ and the $\left\|\mathbf{C}-\mathbf{C}_0\right\|_F \leq k_1\varepsilon + k_2\varepsilon^2$ provided in \citeb[Eq.(5.12)]{chaudhury2015global}. Also note that $\lambda_2(\mathbf{\mathcal{L}}_{\Gamma}) > 0$ due to Assumption~\ref{assump:connected_gamma}.
Then the following lemma establishes a quadratic growth condition at an optimal alignment in the noiseless setting. The subsequent lemma bounds the distance between the optimal alignments of noisy and noiseless views.
\begin{lem}
\label{lem:quadgrowth}
Let $\rank(\mathbf{C}_0) = (m-1)d$, and consequently $\mathbf{S}_0$ be a unique perfect alignment of the noiseless views (Figure~\ref{fig:rigidity_flow}). Then,
\begin{equation}
    \Tr(\mathbf{C}_0\mathbf{S}\mathbf{S}^T) \geq \frac{\lambda_{d+1}(\mathbf{C}_0)}{2} \min_{\mathbf{Q} \in \mathbb{O}(d)}\left\|\mathbf{S}- \mathbf{S}_0\mathbf{Q}\right\|_F^2.
\end{equation}
% Consequently, for all $\mathbf{S} \in \mathbb{O}(d)^m$ such that
% \begin{equation}
%     \min_{\mathbf{Q} \in \mathbb{O}(d)^m} \left\|\mathbf{S}- \mathbf{S}_0\mathbf{Q}\right\|_F > \sqrt{\frac{4m\sqrt{d}(k_1\varepsilon + k_2\varepsilon^2)}{\lambda_{d+1}(\mathbf{C}_0)}}
% \end{equation}
% we have $\Tr(\mathbf{C}_0\mathbf{S}\mathbf{S}) > 2m\sqrt{d}(k_1\varepsilon + k_2\varepsilon^2)$.
\end{lem}
\begin{lem}
\label{lem:distS_0Sstar}
Let $\rank(\mathbf{C}_0) = (m-1)d$ and $\mathbf{S}_0$ be a unique perfect alignment of the noiseless views. Let $\mathbf{S}^*$ be an optimal alignment of the noisy views. Then
\begin{equation}
    \min_{\mathbf{Q}\in\mathbb{O}(d)}\left\|\mathbf{S}^* - \mathbf{S}_0\mathbf{Q}\right\|_F \leq \frac{4 m \left\|\mathbf{C}-\mathbf{C}_0\right\|_F}{\lambda_{d+1}(\mathbf{C}_0)}.
\end{equation}
\end{lem}

Finally, we obtain a bound on the noise level for RGD, initialized with  the spectral alignment, to converge locally linearly to the optimal alignment of the noisy views. 
\begin{thm}
\label{thm:rgd_noise_stability}
Let $\rank(\mathbf{C}_0) = (m-1)d$ and $\mathbf{S}_0$ be a unique perfect alignment of the noiseless views. Let $\mathbf{S}^*$ be an optimal alignment of the noisy views and suppose $\mathbf{S}^*$ is non-degenerate. Let $\zeta \in (0,1)$ be fixed. Then Algorithm~\ref{algo:rgd}, initialized with $\pi(\mathbf{S}_{spec}(\mathbf{C}))$, converges locally linearly to $\pi(\mathbf{S}^*)$ if the noise level $\varepsilon$ satisfies
\begin{equation}
    4\sqrt{m}\left(\frac{\pi\sqrt{d(d+1)}}{\lambda_{d+1}(\mathbf{C})} + \frac{\sqrt{m}}{\lambda_{d+1}(\mathbf{C}_0)}\right)(K_1 \varepsilon + K_2\varepsilon^2) < \min\left\{2, \frac{2}{\pi}\zeta\delta(\mathbf{S}^*)\right\}
\end{equation}
and the subsequent iterates satisfy $\min_{\mathbf{Q}\in \mathbb{O}(d)}\left\|\mathbf{S}^k-\mathbf{S}^*\mathbf{Q}\right\|_F < \min\left\{2, \frac{2}{\pi}\zeta\delta(\mathbf{S}^*)\right\}$.
Here,
\begin{align}
    K_1 &= 2\sqrt{n |E(\Gamma)|}\left(4 \max_1^n\left\|\mathbf{x}_k^*\right\|_2\frac{\sqrt{n|E(\Gamma)|}}{\lambda_{2}(\mathbf{\mathcal{L}}_{\Gamma})} + 1\right)\\
    K_2 &= 2\sqrt{n |E(\Gamma)|}\left(2\frac{\sqrt{n|E(\Gamma)|}}{\lambda_{2}(\mathbf{\mathcal{L}}_{\Gamma})} + 1\right),
\end{align}
$\mathbf{x}_k^*$ is the realization of the noiseless views due to $\mathbf{S}_0$ and $\delta(\mathbf{S}^*)$ is defined in Proposition~\ref{prop:HessVicinity}.
\end{thm}
In the result above, we assumed $\lambda_{d+1}(\mathbf{C}) > 0$, as was the case in \citea{chaudhury2015global}, where the authors observed (and as we validate below) that $\lambda_{d+1}(\mathbf{C})$ increases with noise level $\varepsilon$.
\begin{figure}
    \centering
    \begin{tabular}{cc}
       \includegraphics[width=0.46\linewidth,keepaspectratio]{fig/fig0/rgd_convergence3_new.pdf}
      &  \includegraphics[width=0.46\linewidth,keepaspectratio]{fig/fig0/rgd_convergence2_new.pdf}
    \end{tabular}
    \caption{(left) The eigenvalue $\lambda_{d+1}(\mathbf{C})$ against the noise levels. (right) The evolution of the ratio (Eq.~(\ref{eq:alignment_err_ratio})) due to the iterates generated by Algorithm~\ref{algo:rgd} when initialized with the output of SPEC \citea{chaudhury2015global}.}
    \label{fig:rgd_iterates}
\end{figure}

To demonstrate, we provide a simple simulation showing that when Algorithm~\ref{algo:rgd} is initialized with SPEC, it produce iterates with lower alignment error. We took about $n=5000$ points arranged in a unit square grid with a resolution of $70$ points per dimension and subsequently obtained $m=331$ overlapping views. We added random bounded noise in each view for a fixed noise level $\varepsilon$, obtained the corresponding patch-stress matrix $\mathbf{C}$, and computed the spectral alignment $\mathbf{S}_{spec}(\mathbf{C})$ of the noisy views. Finally, we refined it using Algorithm~\ref{algo:rgd} for $100$ iterations. Figure~\ref{fig:rgd_iterates} shows the eigenvalue $\lambda_{d+1}(\mathbf{C})$ against the noise level $\varepsilon$ and confirms the observation in \cite{chaudhury2015global} that the eigenvalue increases with the noise level. Figure~\ref{fig:rgd_iterates} also shows the ratio of
%$\frac{\widetilde{F}(\widetilde{\mathbf{S}}^k)- \widetilde{F}(\widetilde{\mathbf{S}}^*)}{\widetilde{F}(\widetilde{\mathbf{S}}^0) - \widetilde{F}(\widetilde{\mathbf{S}}^*)}$ for the alignments $\widetilde{\mathbf{S}}^k$
$\widetilde{F}(\widetilde{\mathbf{S}}^k)- \widetilde{F}(\widetilde{\mathbf{S}}^*)$ and $\widetilde{F}(\widetilde{\mathbf{S}}^0) - \widetilde{F}(\widetilde{\mathbf{S}}^*)$ for the alignments $\widetilde{\mathbf{S}}^k$ produced by RGD. The evolution of the ratio is consistent with the linear convergence predicted by Theorem~\ref{thm:rgd_conv2}.
% Finally, we state a sufficient condition on the structure of the noiseless local views that enables local linear convergence of RGD. This condition is stronger than the non-degeneracy condition.
% \begin{cor}
% \label{cor:G_conv}
% If the local views are noiseless and $\mathbb{G}$ is connected then RGD converges locally linearly to a perfect alignment.
% \end{cor}
% Noise stability analysis:
% \begin{thm}
% If $\mathbf{S}^*$ is an alignment such that $\mathbf{S}^* \in \mathcal{C}$, $\mathbb{L}(\mathbf{S}^*)$ is positive semi-definite and of rank $(m-1)d(d-1)/2$, then Algorithm~\ref{algo:rgd} converges locally linearly to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$.
% \end{thm}