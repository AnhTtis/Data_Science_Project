In this section we borrow and build upon the patch framework setup described in \citea{chaudhury2015global}, elucidate the structure of the objects underlying a framework, define an alignment, optimal alignment and perfect alignment of views, and the realization of a framework due to a perfect alignment. 

Suppose that a sequence of $m$ point clouds in $\mathbb{R}^d$ is available where each cloud represents a local view of the dataset $(\mathbf{x}_k)_1^n$.  Let $\Gamma$ be a graph of $m+n$ vertices, where the $k$th vertex represents the $k$th point for $k \in [1,n]$ and the $(n+i)$th vertex represents the $i$th view for $i \in [1,m]$. An edge $(k,i) \in E(\Gamma)$ means that the $k$th point has a local representation due to $i$th view, given by $\mathbf{x}_{k,i} \in \mathbb{R}^d$. In particular, $\Gamma$ is bipartite. The tuple $\Theta = (\Gamma, (\mathbf{x}_{k,i}))$ is called the patch framework.

Given a patch framework $\Theta$, the task is to align the overlapping views, precisely, to find orthogonal matrices $(\mathbf{S}_i)_{1}^{m} \subseteq \mathbb{O}(d)$ and translation vectors $(\mathbf{t}_i)_{1}^{m} \subseteq \mathbb{R}^{d}$ so that the local representations of the $k$th point across the rigidly transformed views are close in the $2$-norm. This naturally leads to the following problem,
\begin{equation}
    \mathcal{A}_0 \coloneqq \textstyle\min_{\substack{(\mathbf{S}_i)_1^m \subseteq \mathbb{O}(d)\\(\mathbf{t}_i)_1^m \subseteq \mathbb{R}^d}}\textstyle\sum_{\substack{(k,i)\in E(\Gamma)\\(k,j)\in E(\Gamma)}}\left\|(\mathbf{S}_i^T\mathbf{x}_{k,i}+\mathbf{t}_i)-(\mathbf{S}_j^T\mathbf{x}_{k,j}+\mathbf{t}_j)\right\|_2^2. \label{eq:A_0}
\end{equation}
An equivalent problem is to find $(\mathbf{x}_k)_1^n$, $(\mathbf{S}_i)_{1}^{m}$ and $(\mathbf{t}_i)_{1}^{m}$ that minimize
\begin{equation}
    \mathcal{A}_1 \coloneqq \textstyle\min_{\substack{(\mathbf{S}_i)_1^m \subseteq \mathbb{O}(d),\\ (\mathbf{t}_i)_1^m, (\mathbf{x}_k)_1^n \subseteq \mathbb{R}^d}}\textstyle\sum_{\substack{(k,i)\in E(\Gamma)}}\left\|\mathbf{x}_k-(\mathbf{S}_i^T\mathbf{x}_{k,i}+\mathbf{t}_i)\right\|_2^2. \label{eq:A_1}
\end{equation}
Here, $\mathcal{A}_0$ and $\mathcal{A}_1$ are the optimal alignment errors. %The proof of the following proposition derives the optimal $\mathbf{x}_k$ in Eq.~(\ref{eq:A_1}) from which it follows that the minimizers $(\mathbf{S}_i)_{1}^{m}$ and $(\mathbf{t}_i)_{1}^{m}$ of the two problems coincide.
% \revadd{It follows trivially that the minimizers $(\mathbf{S}_i)_{1}^{m}$ and $(\mathbf{t}_i)_{1}^{m}$ of the two problems coincide, and in fact $\mathcal{A}_0 = 2\mathcal{A}_1$.}
\begin{prop}
\label{prop:A_0_2_A_1}
$\mathcal{A}_0 = 2\mathcal{A}_1$.
\end{prop}

Note that translating the optimal $\mathbf{x}_k$ and $\mathbf{t}_i$ by the same amount does not change the objective and still leads to an optimal solution. To avoid that, we add a centering constraint $\sum_1^n \mathbf{x}_k = 0$. A more concise representation of $\mathcal{A}_1$ is then derived as follows. Define $ \mathbf{H} \coloneqq [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n, \mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_m] \in \mathbb{R}^{d \times (n+m)}$ and
% \begin{align}
%     \mathbf{H} \coloneqq [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n, \mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_m] \in \mathbb{R}^{d \times (n+m)}. \label{eq:H}
% \end{align}
%Define
$\mathbf{e}_{ki} \coloneqq \mathbf{e}^{n+m}_k - \mathbf{e}^{n+m}_{n+i}$, then $\mathbf{x}_k - \mathbf{t}_i = \mathbf{H}\mathbf{e}_{ki}$. Let $\mathbf{S} = [\mathbf{S}_i]_1^m \in \mathbb{O}(d)^m \subseteq \mathbb{R}^{md \times d}$. Then
\begin{align}
    \mathcal{A}_1 &= \textstyle\min_{\substack{\mathbf{S} \in \mathbb{O}(d)^m\\\mathbf{H}\mathbf{1}^{n+m}_n = 0}}\textstyle\sum_{(k,i) \in E(\Gamma)}\left\|\mathbf{H}\mathbf{e}_{ki}-\mathbf{S}^T(\mathbf{e}^m_i \otimes \mathbf{I}_d)\mathbf{x}_{k,i}\right\|_2^2\\
    &= \textstyle\min_{\substack{\mathbf{S} \in \mathbb{O}(d)^m\\\mathbf{H}\mathbf{1}^{n+m}_n = 0}}\Tr\left(\begin{bmatrix}\mathbf{H} & \mathbf{S}^T\end{bmatrix}\begin{bmatrix}\boldsymbol{\mathcal{L}}_{\Gamma} & -\mathbf{B}^T\\-\mathbf{B} & \mathbf{D}\end{bmatrix}\begin{bmatrix}\mathbf{H}^T\\\mathbf{S}\end{bmatrix}\right) \text{, where} \label{eq:A_1_}
\end{align}
\begin{align}
    \boldsymbol{\mathcal{L}}_{\Gamma} &= \textstyle\sum_{(k,i) \in E(\Gamma)}\mathbf{e}_{ki}\mathbf{e}_{ki}^T\retainlabel{eq:L_Gamma}\\
    \mathbf{B} &= \textstyle\sum_{(k,i) \in E(\Gamma)} (\mathbf{e}^m_i \otimes \mathbf{I}_d)\mathbf{x}_{k,i}\mathbf{e}_{ki}^T \label{eq:B}\\
    \mathbf{D} &= \textstyle\sum_{(k,i) \in E(\Gamma)} (\mathbf{e}^m_i \otimes \mathbf{I}_d)\mathbf{x}_{k,i}\mathbf{x}_{k,i}^T(\mathbf{e}^m_i \otimes \mathbf{I}_d)^T. \retainlabel{eq:D}
\end{align}
% where 
% \begin{align}
%     \boldsymbol{\mathcal{L}}_{\Gamma} &= \sum_{(k,i) \in E(\Gamma)}\mathbf{e}_{ki}\mathbf{e}_{ki}^T\retainlabel{eq:L_Gamma}\\
%     \mathbf{B} &= \sum_{(k,i) \in E(\Gamma)} (\mathbf{e}^m_i \otimes \mathbf{I}_d)\mathbf{x}_{k,i}\mathbf{e}_{ki}^T \label{eq:B}\\
%     \mathbf{D} &= \sum_{(k,i) \in E(\Gamma)} (\mathbf{e}^m_i \otimes \mathbf{I}_d)\mathbf{x}_{k,i}\mathbf{x}_{k,i}^T(\mathbf{e}^m_i \otimes \mathbf{I}_d)^T. \retainlabel{eq:D}
% \end{align}
\begin{rmk}
\label{rmk:L0DB}
Note that $\boldsymbol{\mathcal{L}}_{\Gamma}$ is the combinatorial Laplacian of the graph $\Gamma$. Due to the bipartite structure of $\Gamma$, $\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger$ can be computed in $O(nm^2)$ \citea{HO2005917}, against the complexity of $O((m+n)^3)$ for the general case.  The matrix $\mathbf{D}$ is a block diagonal matrix where the $i$th block is $\sum_{(k,i) \in E(\Gamma)}\mathbf{x}_{k,i}\mathbf{x}_{k,i}^T$. The matrix $\mathbf{B} = [\mathbf{B}_i]_1^m$ is a vertical stack of $m$ matrices, one for each view, where each $\mathbf{B}_i \in \mathbb{R}^{d \times (n+m)}$. For a fixed $i$, $\mathbf{B}_i(:,n+i) = -\sum_{(k,i)\in E}\mathbf{x}_{k,i}$, if $(k,i) \in E(\Gamma)$ then $\mathbf{B}_i(:,k) = \mathbf{x}_{k,i}$ otherwise zero. Thus, $\mathbf{1}_{n+m} \in \ker(\mathbf{B}_i)$. Finally, the $j$th row of $\mathbf{B}_i$ contains information about the $j$th coordinates of the points in the $i$th view. We will use the following result later,
% Note that for $i \in [1,m]$,
% \begin{align}
%     \{\mathbf{1}_{n+m}\} \cup \{\mathbf{1}^{n+m}_{n} + \mathbf{e}^{n+m}_i: j \in [1,m], j \neq i\} \subseteq \text{ker}(\mathbf{B}_i).\label{eq:kerBi}
% \end{align}
\end{rmk}
\begin{prop}
\label{prop:kerB}
$\ker (\boldsymbol{\mathcal{L}}_{\Gamma}) \subseteq \ker (\mathbf{B})$.
\end{prop}
\begin{assump}
\label{assump:connected_gamma}
The dimension of the kernel of $\boldsymbol{\mathcal{L}}_{\Gamma}$ equals the number of connected components in $\Gamma$. To keep the subsequent calculations simple, we assume that $\Gamma$ is connected and thus $\ker (\boldsymbol{\mathcal{L}}_{\Gamma})$ is the span of a single vector $\mathbf{1}_{n+m}$.
\end{assump}

It is clear from Eq.~(\ref{eq:A_1_}) that the the optimal $\mathbf{H}$ satisfies $\mathbf{H}^*\boldsymbol{\mathcal{L}}_{\Gamma} = \mathbf{S}^T\mathbf{B}$. Using the fact that $\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger \boldsymbol{\mathcal{L}}_{\Gamma} = \boldsymbol{\mathcal{L}}_{\Gamma}\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger = \mathbf{I}_{n+m} - (n+m)^{-1}\mathbf{1}_{n+m}\mathbf{1}_{n+m}^T$, the solution $\mathbf{H}^*$ is of the form $\mathbf{S}^T\mathbf{B}\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger - \mathbf{h}\mathbf{1}_{n+m}^T$ for some translation vector $\mathbf{h} \in \mathbb{R}^d$. Since $\mathbf{H}\mathbf{1}^{n+m}_n = 0$, the optimal value of $\mathbf{h}$ is $\mathbf{S}^T\mathbf{B}\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger \mathbf{1}^{n+m}_n/n$. Substituting back,
\begin{equation}
    \mathbf{H}^* = \mathbf{S}^T\mathbf{B}\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger \left(\mathbf{I}_{n+m} - n^{-1}\mathbf{1}^{n+m}_n\mathbf{1}_{n+m}^T\right). \label{eq:opt_Z}
\end{equation}
% \begin{rmk}
% \begin{align}
%     B_i\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger B_i^T &= \mathbf{x}_{k,i}e_{ki}^T\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger e_{k'i}x_{k',i}^T
% \end{align}
% \end{rmk}
Substituting Eq.~(\ref{eq:opt_Z}) back in Eq.~(\ref{eq:A_1_}), the problem reduces to
\begin{equation}
    \mathcal{A}_1 = \min_{\mathbf{S} \in \mathbb{O}(d)^m} F(\mathbf{S}) = \min_{\mathbf{S} \in \mathbb{O}(d)^m} \Tr(\mathbf{C}\mathbf{S}\mathbf{S}^T) \text{ where } \mathbf{C} = \mathbf{D} - \mathbf{B}\boldsymbol{\mathcal{L}}_{\Gamma}^{\dagger}\mathbf{B}^T. \label{eq:GPOP}
\end{equation}
Here $\mathbf{C}$ is named the \textit{patch-stress matrix} and is positive semidefinite \citea{chaudhury2015global}.
\begin{dfn}
The objective $F$ depends only on $\mathbf{S}$, so we define an \textit{alignment} of local views as an element of $\mathbb{O}(d)^m$. If $\mathbf{S}$ is a global minimum of $F$, it's called an optimal alignment. If $F(\mathbf{S}) = 0$, it's called a \textit{perfect alignment}. Since $\mathbf{C} \succeq 0$, every perfect alignment is optimal, but not every optimal alignment is perfect.
% Since the objective $F$ depends only on $\mathbf{S}$, we identify an \textit{alignment} of local views with an element of $\mathbb{O}(d)^m$. If $\mathbf{S}$ is a global minimum of $F$ then the alignment is called an optimal alignment. If $\mathbf{S}$ is such that $F(\mathbf{S}) = 0$ then the alignment $\mathbf{S}$ is called a \textit{perfect alignment}. Note that since $\mathbf{C} \succeq 0$, every perfect alignment is optimal, while the converse may not hold.
\end{dfn}

\begin{dfn}
\label{def:realization}
The consensus representation of the framework $\Theta$ due to an alignment $\mathbf{S}$ is given by $\Theta(\mathbf{S}) \coloneqq \mathbf{H}^*(:,1:n) = \mathbf{S}^T\mathbf{B}\boldsymbol{\mathcal{L}}_{\Gamma}^\dagger(:,1:n)\left(\mathbf{I}_n - n^{-1}\mathbf{1}_{n}\mathbf{1}_{n}^T\right)$. The one due to a perfect alignment is called a realization of the framework \citea{gortler2010affine}.
\end{dfn}