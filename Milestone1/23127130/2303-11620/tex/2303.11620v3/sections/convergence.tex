In Section~\ref{subsec:rgd_algo}, we describe the RGD algorithm for solving the alignment problem in Eq.~(\ref{eq:GPOP}). Then in \revdel{Section~\ref{subsec:loc_sub_conv} we prove the local sublinear convergence of RGD to a critical point of \revdel{$F$}\revadd{$\widetilde{F}$}. In} Section~\ref{subsec:loc_lin_conv}, using the theory of Morse\revdel{-Bott} functions \revadd{ and Proposition~\ref{prop:HessVicinity}}, we \revdel{extend the result to}\revadd{show} the local linear convergence of RGD to a non-degenerate alignment (see Section~\ref{subsec:non_deg_gen_setting}). \revadd{TODO: add one more line about contrasting conditions and exact recovery.}

\subsection{RGD Algorithm}
\label{subsec:rgd_algo}
A standard way to find a local minimum of Eq.~(\ref{eq:GPOP}) is to use RGD with a suitable initial point, step size and retraction strategy.
%In this work, our choice of retraction is based on \revdel{QR decomposition}\revadd{polar decomposition},
\revadd{In this work we use retraction based on the exponential map on $\mathbb{O}(d)$. Define,}
\revdel{
\begin{align}
    R_{\QR }: \cup_{\mathbf{S} \in \mathbb{O}(d)^m}(\{\mathbf{S}\} \times T_\mathbf{S}\mathbb{O}(d)^m) &\mapsto \mathbb{O}(d)^m\\
    R_{\QR }\left(\begin{bmatrix}\mathbf{S}_1\\\vdots\\\mathbf{S}_m\end{bmatrix}, \begin{bmatrix}\boldsymbol{\xi}_1\\\vdots\\\boldsymbol{\xi}_m\end{bmatrix}\right) &= \begin{bmatrix}\qf (\mathbf{S}_1+\boldsymbol{\xi}_1)\\\vdots\\\qf (\mathbf{S}_m+\boldsymbol{\xi}_m)\end{bmatrix}. \label{eq:R_QR}
\end{align}
}
\revadd{
\begin{align}
    R_{\EXP }: \cup_{\mathbf{S} \in \mathbb{O}(d)^m}(\{\mathbf{S}\} \times T_\mathbf{S}\mathbb{O}(d)^m) &\mapsto \mathbb{O}(d)^m\\
    R_{\EXP }\left([\mathbf{S}_i]_1^m, [\boldsymbol{\xi}_i]_1^m\right) &= [\mathbf{S}_i\exp(\mathbf{S}_i^T\boldsymbol{\xi}_i)]_1^m. \label{eq:R_PF}
\end{align}
}\revdel{where $\qf (\mathbf{A})$ denotes the $\mathbf{Q}$ factor in the thin QR decomposition of $\mathbf{A}$ \citea{van1996matrix, absil2009optimization}}\revadd{where $\exp (\mathbf{A})$ denotes the matrix exponential of $\mathbf{A}$ \citea{van1996matrix, absil2009optimization}. Then the following lemma provides a consistent definition of retraction on the quotient manifold $\mathbb{O}(d)^m/_{\sim}$.}
\revadd{
\begin{lem}
\label{lem:retraction}
Let $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^{m}/_{\sim}$ and $\mathbf{S}^a, \mathbf{S}^b \in \pi^{-1}(\widetilde{\mathbf{S}})$. If $\mathbf{Z}^a \in T_{\mathbf{S}^a}\mathbb{O}(d)^m$ and $\mathbf{Z}^b \in T_{\mathbf{S}^b}\mathbb{O}(d)^m$ are the horizontal lifts of $\widetilde{\mathbf{Z}} \in T_{\widetilde{\mathbf{S}}}\mathbb{O}(d)^{m}/_{\sim}$ then $\pi(R_{\EXP }(\mathbf{S}^a, \mathbf{Z}^a)) = \pi(R_{\EXP }(\mathbf{S}^b, \mathbf{Z}^b))$. As a result, the retraction
\begin{align}
    \widetilde{R}_{\EXP }: \cup_{\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim} }(\{\widetilde{\mathbf{S}}\} \times T_{\widetilde{\mathbf{S}}}\mathbb{O}(d)^m/_{\sim}) &\mapsto \mathbb{O}(d)^m/_{\sim}\\
    \widetilde{R}_{\EXP }\left([\widetilde{\mathbf{S}}_i]_1^m, [\widetilde{\mathbf{Z}}_i]_1^m\right) &=  \pi(R_{\EXP }(\mathbf{S}, \mathbf{Z}))\label{eq:Rtilde_PF}
\end{align}
is well defined for any $\mathbf{S} \in \pi^{-1}(\widetilde{\mathbf{S}})$ and $\mathbf{Z}$ being the horizontal lift of $\widetilde{\mathbf{Z}}$ at $\mathbf{S}$.
\end{lem}
}

\revadd{The step direction will always be the horizontal lift of $-\grad \widetilde{F}(\widetilde{\mathbf{S}})$ at some $\mathbf{S} \in \pi^{-1}(\widetilde{\mathbf{S}})$. Consequently, due to Proposition~\ref{prop:gradFS},} the step direction is $\boldsymbol{\xi} = -\grad F(\mathbf{S})$ which is the projection of the antigradient $-\nabla F(\mathbf{S})$ onto $T_\mathbf{S}\mathbb{O}(d)^m$. Recall (from the proof of Proposition~\ref{prop:gradFS}) that $\grad F(\mathbf{S}) = [[\mathbf{C}\mathbf{S}]_i - \mathbf{S}_i[\mathbf{C}\mathbf{S}]_i^T\mathbf{S}_i]_1^m$. Then the step size $\alpha$ is calculated using the Armijo-type rule with parameters $\beta,\gamma \in (0,1)$ (here $g$ is the canonical metric on $\mathbb{O}(d)^m$ as in Eq.~(\ref{eq:g_Z_W})),
\begin{equation}
    \alpha = \max_{l \geq 0}\{\beta^l\ \vertbar\ F(R_{\EXP }(\mathbf{S}, -\beta^l\grad F(\mathbf{S}))) - F(\mathbf{S}) \leq -\gamma \beta^l g(\nabla F(\mathbf{S}),  \grad F(\mathbf{S})) \}. \label{eq:armijo_step}
\end{equation}
\revadd{Since $F$ extends to a continuously differentiable non-negative function on $\mathbb{R}^{md \times d}$ containing $\mathbb{O}(d)^m$, it follows from \citeb[Proposition 2.8]{schneider2015convergence} that $\alpha$ is well-defined.}

\revdel{
\begin{algorithm}
\caption{Riemannian gradient descent for solving GPOP \label{algo:rgd_old}}
\revdel{
\begin{algorithmic}[1]
\REQUIRE $\mathbf{S}^0 \in \mathbb{O}(d)^m$, $\Gamma$, $\{\mathbf{x}_{k,i}: (k,i) \in E(\Gamma)\}$, $\beta, \gamma \in (0,1)$
\STATE Construct $\mathbf{C}$ as in Eq.~(\ref{eq:GPOP}).
\REPEAT
    \STATE calculate the descent direction $-\grad F(\mathbf{S}^k)$ at $\mathbf{S}^k$ using Eq.~(\ref{eq:gradFS}).
    \STATE calculate the step size $\alpha_k$ according to the Armijo-type rule (see Eq.~(\ref{eq:armijo_step})).
    \STATE set $\mathbf{S}^{k+1} = R_{\QR }(\mathbf{S}^k, -\alpha_k \grad F(\mathbf{S}^k))$ using Eq.~(\ref{eq:R_PF}).
    \STATE $k \leftarrow k + 1$.
\UNTIL{convergence.}
\end{algorithmic}
}
\end{algorithm}
}

\begin{algorithm}
\caption{Riemannian gradient descent for solving GPOP \label{algo:rgd}}
\revadd{
\begin{algorithmic}[1]
\REQUIRE $\widetilde{\mathbf{S}}^0 \in \mathbb{O}(d)^{m-1}$, $\Gamma$, $\{\mathbf{x}_{k,i}: (k,i) \in E(\Gamma)\}$, $\beta, \gamma \in (0,1)$
\STATE Construct $\mathbf{C}$ as in Eq.~(\ref{eq:GPOP}).
\REPEAT
    \STATE set $\mathbf{S}^k = [\mathbf{I}_d; \widetilde{\mathbf{S}}^k] \in \pi^{-1}(\widetilde{\mathbf{S}}^k) \subset \mathbb{O}(d)^m$ (Eq.~\ref{eq:pi_inv_wtS}).
    \STATE calculate the descent direction $-\grad F(\mathbf{S}^k)$ at $\mathbf{S}^k$ using Eq.~(\ref{eq:gradFS}).
    \STATE calculate the step size $\alpha_k$ according to the Armijo-type rule (see Eq.~(\ref{eq:armijo_step})).
    \STATE set $\widetilde{\mathbf{S}}^{k+1} = \widetilde{R}_{\EXP}(\mathbf{S}^k, -\alpha_k \grad F(\mathbf{S}^k))$ using Eq.~(\ref{eq:R_PF}, \ref{eq:pi}).
    \STATE $k \leftarrow k + 1$.
\UNTIL{convergence.}
\end{algorithmic}
}
\end{algorithm}

\revdel{
\subsection{Local Sublinear Convergence of RGD}
\label{subsec:loc_sub_conv}
}
\revadd{
\subsection{Local linear Convergence of RGD}
\label{subsec:loc}
}
We proceed to show the local sublinear convergence of Algorithm~\ref{algo:rgd} to a non-degenerate alignment (see Definition~\ref{def:non_deg_alignment0}). Our main tool will be the convergence analysis framework presented in \citeb[Section 2.3]{schneider2015convergence} as used in \citea{liu2019quadratic}.
\revdel{To this end, we first note that $F$ is a real-analytic function bounded from below by zero and $\mathbb{O}(d)^m$ is a compact submanifold of $\mathbb{R}^{md \times d}$. Thus, the Lojasiewicz gradient inequality \citea{lojasiewicz1965ensembles}, \citeb[Section 2.2]{schneider2015convergence} holds at every $\mathbf{S}^* \in \mathbb{O}(d)^m$ and in particular for every $\mathbf{S}^* \in \mathcal{C}$ (see Eq.~(\ref{eq:crit_pts2})) i.e. there exist $\delta, \eta > 0$ and $\theta \in (0,1/2]$ (generally dependent on $\mathbf{S}^*$) such that}
\revadd{To this end, we first note that $\widetilde{F}$ is a real-analytic function bounded from below by zero and $\mathbb{O}(d)^m/_{\sim}$ (whose elements are identified with $\mathbb{O}(d)^{m-1}$ here) is a compact submanifold of $\mathbb{R}^{(m-1)d \times d}$. Thus, the Lojasiewicz gradient inequality \citea{lojasiewicz1965ensembles}, \citeb[Section 2.2]{schneider2015convergence} holds at every $\widetilde{\mathbf{S}}^* \in \mathbb{O}(d)^m/_{\sim}$ and in particular for every $\widetilde{\mathbf{S}}^* \in \widetilde{\mathcal{C}}$ (see Eq.~(\ref{eq:crit_pts})) i.e. there exist $\delta, \eta > 0$ and $\theta \in (0,1/2]$ (generally dependent on $\widetilde{\mathbf{S}}^*$) such that}
\revdel{
\begin{equation}
    |F(\mathbf{S}) - F(\mathbf{S}^*)|^{1-\theta} \leq \eta \left\| \grad F(\mathbf{S})\right\|_F. \label{eq:Lojasiewicz_gradient_ineq_old}
\end{equation}
holds for every $\mathbf{S} \in \mathbb{O}(d)^m$ satisfying $\left\|\mathbf{S}-\mathbf{S}^*\right\|_F < \delta$.
}
\revadd{
\begin{equation}
    |\widetilde{F}(\widetilde{\mathbf{S}}) - \widetilde{F}(\widetilde{\mathbf{S}}^*)|^{1-\theta} \leq \eta \left\| \grad \widetilde{F}(\widetilde{\mathbf{S}})\right\|_F. \label{eq:Lojasiewicz_gradient_ineq}
\end{equation}
holds for every $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim}$ satisfying $\left\|\widetilde{\mathbf{S}}-\widetilde{\mathbf{S}}^*\right\|_F < \delta$.
}

\revdel{
Then using Theorem 2.3 in \citea{schneider2015convergence} and the fact that $\mathbb{O}(d)^m$ is compact (thus every sequence on it has a cluster point), for the Algorithm~\ref{algo:rgd} to converge at least sublinearly to a non-degenerate alignment $\mathbf{S}^*$, it suffices to show that the iterates $\{\mathbf{S}^k\}_{k \geq 0}$ generated by the algorithm satisfy the following:\\
}
\revdel{
\noindent \textbf{(A1)}. \textit{(Sufficient Descent)} There exist $\kappa_0 > 0$ and $k_1 \in \mathbb{N}$ such that, the inequality $ F(\mathbf{S}^{k+1}) - F(\mathbf{S}^k) \leq - \kappa_0 \left\|\grad F(\mathbf{S}^k)\right\|_F \cdot \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F$ holds for all $k \geq k_1$.
    % \begin{equation}
    %     F(\mathbf{S}^{k+1}) - F(\mathbf{S}^k) \leq - \kappa_0 \left\|\grad F(\mathbf{S}^k)\right\|_F \cdot \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F
    % \end{equation}
\smallskip\\
}
\revdel{
\noindent \textbf{(A2)}. \textit{(Stationarity)} There exist $k_2 \in \mathbb{N}$ such that for all $k \geq k_2$, if $\left\|\grad F(\mathbf{S}^k)\right\|_F = 0$ then $\mathbf{S}^{k+1} = \mathbf{S}^k$. The sequence $\{\mathbf{S}^{k}\}_{k \geq 0}$ satisfies this trivially.
\smallskip\\
}
\revdel{
\noindent \textbf{(A3)}. \textit{(Safeguard)} There exist a constant $\mu > 0$ and $k_3 \in \mathbb{N}$ such that the inequality $\left\|\grad F(\mathbf{S}^k)\right\|_F \leq \mu \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F$ holds for all $k \geq k_3$.
    % \begin{equation}
    %     \left\|\grad F(\mathbf{S}^k)\right\|_F \leq \mu \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F.
    % \end{equation}
\smallskip\\
}
\revdel{
To prove \textbf{(A1)} and \textbf{(A3)}, we need
\begin{prop}{\citeb[Appendix E.2]{liu2019quadratic}}
\label{prop:liu_qr}
There exist $\phi, M > 0$ such that for all $\mathbf{S}_i \in \mathbb{O}(d)$ and $\boldsymbol{\xi}_i \in T_{\mathbf{S}_i}\mathbb{O}(d)$ satisfying $\left\|\boldsymbol{\xi}_i\right\|_F \leq \phi$, $\left\|\qf (\mathbf{S}_i + \boldsymbol{\xi}_i) - (\mathbf{S}_i + \boldsymbol{\xi}_i)\right\|_F \leq M \left\|\boldsymbol{\xi}_i\right\|_F^2$. In particular, $M = \sqrt{10}/4$ and $\phi = 1/2$ satisfy the above inequality.
\end{prop}
\begin{prop}
\label{prop:second_order_boundedness_of_RQR}
There exist $\phi, M > 0$ such that for all $\mathbf{S} \in \mathbb{O}(d)^{m}$ and $\boldsymbol{\xi} \in T_{\mathbf{S}}\mathbb{O}(d)^m$ satisfying $\left\|\boldsymbol{\xi}\right\|_F \leq \phi$, $\left\|R_{\QR }(\mathbf{S},\boldsymbol{\xi}) - (S + \boldsymbol{\xi})\right\|_F \leq M \left\|\boldsymbol{\xi}\right\|_F^2$.
\end{prop}
}

\revadd{Then using Theorem 2.3 in \citea{schneider2015convergence} and the fact that $\mathbb{O}(d)^m/_{\sim}$ is compact (thus every sequence on it has a cluster point), for the Algorithm~\ref{algo:rgd} to converge at least sublinearly to a non-degenerate alignment $\widetilde{\mathbf{S}}^*$, it suffices to show that the iterates $\{\widetilde{\mathbf{S}}^k\}_{k \geq 0}$ generated by the algorithm satisfy the following:}
\smallskip

\noindent \textbf{(A1)}. \revadd{\textit{(Sufficient Descent)} There exist $\kappa_0 > 0$ and $k_1 \in \mathbb{N}$ such that, the inequality $\widetilde{F}(\widetilde{\mathbf{S}}^{k+1}) - \widetilde{F}(\widetilde{\mathbf{S}}^k) \leq - \kappa_0 \left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F \cdot \left\|\widetilde{\mathbf{S}}^{k+1}-\widetilde{\mathbf{S}}^k\right\|_F$ holds for all $k \geq k_1$.}
    % \begin{equation}
    %     F(\mathbf{S}^{k+1}) - F(\mathbf{S}^k) \leq - \kappa_0 \left\|\grad F(\mathbf{S}^k)\right\|_F \cdot \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F
    % \end{equation}
\smallskip

\noindent \textbf{(A2)}. \revadd{\textit{(Stationarity)} There exist $k_2 \in \mathbb{N}$ such that for all $k \geq k_2$, if $\left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F = 0$ then $\widetilde{\mathbf{S}}^{k+1} = \widetilde{\mathbf{S}}^k$. The sequence $\{\widetilde{\mathbf{S}}^{k}\}_{k \geq 0}$ satisfies this trivially.}

\smallskip

\noindent \textbf{(A3)}. \revadd{\textit{(Safeguard)} There exist a constant $\mu > 0$ and $k_3 \in \mathbb{N}$ such that the inequality $\left\|\grad \widetilde{F}(\widetilde{\mathbf{S}}^k)\right\|_F \leq \mu \left\|\widetilde{\mathbf{S}}^{k+1}-\widetilde{\mathbf{S}}^k\right\|_F$ holds for all $k \geq k_3$.}
    % \begin{equation}
    %     \left\|\grad F(\mathbf{S}^k)\right\|_F \leq \mu \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F.
    % \end{equation}

\revadd{To prove \textbf{(A1)} and \textbf{(A3)}, we need
\begin{prop}
\label{prop:liu_pf}
For all $\mathbf{S}_i \in \mathbb{O}(d)$ and $\boldsymbol{\xi}_i \in T_{\mathbf{S}_i}\mathbb{O}(d)$ satisfying $\left\|\boldsymbol{\xi}_i\right\|_F \leq 1$, $\left\|\mathbf{S}_i\exp (\mathbf{S}_i^T\boldsymbol{\xi}_i) - (\mathbf{S}_i + \boldsymbol{\xi}_i)\right\|_F \leq (e-1)\left\|\boldsymbol{\xi}_i\right\|_F^2$.
\end{prop}
\begin{prop}
\label{prop:second_order_boundedness_of_RPF}
For $\mathbf{S} \in \mathbb{O}(d)^m$ and $\boldsymbol{\xi} \in T_{\mathbf{S}}\mathbb{O}(d)^m$ satisfying $\left\|\boldsymbol{\xi}\right\|_F \leq 1$, $\left\|R_\EXP(\mathbf{S}, \boldsymbol{\xi})(\mathbf{S}_1\exp (\mathbf{S}_1^T\boldsymbol{\xi}_1))^T - (\mathbf{S} + \boldsymbol{\xi})\right\|_F \leq 2\sqrt{m}\left\|\boldsymbol{\xi}\right\|_F^2$.
%For all $\mathbf{S}_i \in \mathbb{O}(d)$ and $\boldsymbol{\xi}_i \in T_{\mathbf{S}_i}\mathbb{O}(d)$, $i \in [1,m]$, satisfying $\left\|\boldsymbol{\xi}_i\right\|_F \leq 1/2$, $\left\|\EXP (\mathbf{S}_i + \boldsymbol{\xi}_i)\EXP (\mathbf{S}_1 + \boldsymbol{\xi}_1)^T - (\mathbf{S}_i + \boldsymbol{\xi}_i)\right\|_F \leq \left\|\boldsymbol{\xi}_i\right\|_F^2 + \left\|\boldsymbol{\xi}_1\right\|_F^2$.
\end{prop}
}\begin{prop}
\label{prop:alpha_grad}
$(\alpha_k)_{k \geq 0}$ and $(\mathbf{S}^k)_{k \geq 0}$ satisfy $\lim \alpha_k \left\|\grad F(\mathbf{S}^k)\right\|_F = 0$.
\end{prop}
\revdel{
\subsection{Local Linear Convegence of RGD}
\label{subsec:loc_lin_conv}
}
Now we extend the above result to the R-linear convergence of the sequence $(\widetilde{\mathbf{S}}^k)$ generated through Algorithm~\ref{algo:rgd} to a non-degenerate alignment. \revadd{Since $F(\mathbf{S}\mathbf{Q}) = F(\mathbf{S})$ for all $\mathbf{Q} \in \mathbb{O}(d)$, therefore every critical point of $F$ is degenerate and in particular $F$ is not a Morse-function \citea{cohen_iga_norbury_2006}. However, if $\mathbf{S}^*$ is a non-degenerate alignment then $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ is a non-degenerate critical point of $\widetilde{F}$, as a result $\widetilde{F}$ is a Morse function at $\widetilde{\mathbf{S}}^*$.
%due to \citeb[Remark 6.6]{usevich2020approximate}. As a result, due to \citeb[Theorem 6.7 and 6.8]{usevich2020approximate} and equivalently
Consequently, due to \citeb[Proposition 4.2]{hu2018convergence}, the Lojasiewicz gradient inequality is satisfied with $\theta = 1/2$. Precisely,} \revdel{In turn, it suffices to show that $F$ is a Morse-Bott function at $\mathbf{S}^*$ \citeb[Section 6.2]{usevich2020approximate} \citeb[Definition 1.5]{feehan2021optimal}.}
\begin{prop}
\revadd{Let $\mathbf{S}^*$ be a non-degenerate alignment and define $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$. Then there exist $\delta, \eta > 0$ such that
\begin{equation}
    |\widetilde{F}(\widetilde{\mathbf{S}}) - \widetilde{F}(\widetilde{\mathbf{S}}^*)|^{1/2} \leq \eta \left\| \grad \widetilde{F}(\widetilde{\mathbf{S}})\right\|_F. \label{eq:Lojasiewicz_gradient_ineq_half}
\end{equation}
holds for every $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim}$ satisfying $\left\|\widetilde{\mathbf{S}}-\widetilde{\mathbf{S}}^*\right\|_F < \delta$.}
\end{prop}
\revdel{
\begin{rmk}
Since $F(\mathbf{S}\mathbf{Q}) = F(\mathbf{S})$ for all $\mathbf{Q} \in \mathbb{O}(d)$, therefore no critical point of $F$ is non-degenerate and in particular $F$ is not a Morse-function \citea{cohen_iga_norbury_2006}.
\end{rmk}
}
% \revadd{Despite the above remark, from Proposition~\ref{prop:hlift_frob_ineq}, \ref{prop:d_g_tilde} and \ref{prop:gradFS}, we obtain $d_{\widetilde{g}}(\widetilde{\mathbf{S}},\widetilde{\mathbf{S}}^*) < d_g(\mathbf{S}, \mathbf{S}^*)$ and $\left\|\grad F(\mathbf{S})\right\|_F \leq \left\| \grad \widetilde{F}(\widetilde{\mathbf{S}})\right\|_F \leq \sqrt{(m+1)}\left\|\grad F(\mathbf{S})\right\|_F$. Combining these with the above proposition and the fact that $F(\mathbf{S}) = \widetilde{F}(\widetilde{\mathbf{S}})$ for all $\mathbf{S} \in \pi^{-1}(\widetilde{\mathbf{S}})$, we obtain the Eq.~(\ref{eq:Lojasiewicz_gradient_ineq}) with $\theta=1/2$.}
\revadd{Then, due to \citeb[Theorem 2.3]{schneider2015convergence} and the fact that non-degenerate critical points are isolated due to Morse Lemma\citeb[Proposition 4.2]{hu2018convergence}, we have the following result,}
% \begin{cor}
% \revadd{Let $\mathbf{S}^*$ be a non-degenerate alignment. Then there exist $\delta, \eta > 0$ such that
% \begin{equation}
%     |F(\mathbf{S}) - F(\mathbf{S}^*)|^{1/2} \leq \eta \left\| \grad F(\mathbf{S})\right\|_F. \label{eq:Lojasiewicz_gradient_ineq}
% \end{equation}
% holds for every $\widetilde{\mathbf{S}} \in \mathbb{O}(d)^m/_{\sim}$ satisfying $\left\|\widetilde{\mathbf{S}}-\widetilde{\mathbf{S}}^*\right\|_F < \delta$.}
% \end{cor}
%\begin{prop}
%\label{prop:morse_bott_1}
\revdel{Let $\mathbf{S}^*$ be a non-degenerate alignment. Then $\widetilde{F}$ is Morse-Bott at $\pi(\mathbf{S}^*)$ and consequently $F$ is Morse-Bott at $\mathbf{S}^*$. Combining this with \citeb[Theorem 6.3]{usevich2020approximate} (equivalently \citeb[Theorem 2.3]{schneider2015convergence}) we obtain}
%\end{prop}
% Combining previous propositions with \citea{usevich2020approximate}[Proposition $6.8$, Theorem $6.7$], \citea{hu2018convergence}[proposition $4.1$, Proposition $4.2$], we have the following result
% \begin{thm}
% If $S^* \in \mathcal{C}$ satisfies the rigidity constraints then there exist $\delta, \eta > 0$ such that for every $S \in \mathbb{O}(d)^m$ with $\left\|S-S^*\right\|_F < \delta$,
% \begin{align}
%     |F(S) - F(S^*)| \leq \eta \left\|\grad F(S)\right\|_F^2.
% \end{align}
% \end{thm}
% \begin{cor}
% We conclude that $\theta = 1/2$ in Eq.~(\ref{eq:Lojasiewicz_gradient_ineq}). Then invoking the convergence theorem in \citea{schneider2015convergence}[Theorem 2.3], we conclude that the sequence $\{\mathbf{S}^k\}_{k \geq 0}$ generated by Algorithm~\ref{algo:rgd} converges linearly to a critical point in $\mathcal{C}$.
% \end{cor}
\revdel{As a consequence of the above proposition, we have the following result}
\begin{thm}
\label{thm:rgd_conv}
Let $\mathbf{S}^*$ be a non-degenerate alignment and define $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$. Then there exist $\delta > 0$ such that RGD converges to $\widetilde{\mathbf{S}}^*$ \revadd{R-}linearly when initialized with $\widetilde{\mathbf{S}}^0$ such that $\left\|\widetilde{\mathbf{S}}^0-\widetilde{\mathbf{S}}^*\right\|_F < \delta$.
\end{thm}
\begin{rmk}
\revadd{Here the radius of convergence $\delta$ depends on the size of the neighborhood on which the Morse Lemma is applicable. In fact, due to Proposition~\ref{prop:HessFSZ}, Proposition~\ref{prop:HessVicinity}, Corollary~\ref{cor:HessVicinity} and $\min_{\mathbf{Q}\in\mathbb{O}(d)}\left\|\mathbf{O}-\mathbf{S}\mathbf{Q}\right\|_F \leq \left\|\pi(\mathbf{O})-\pi(\mathbf{S})\right\|_F$,
%the choice of $\mathbf{S}^{k} = [\mathbf{I}_d; \widetilde{\mathbf{S}}^{k}]$ (consequently, $\left\|\mathbf{S}^{k+1}-\mathbf{S}^{k}\right\|_F = \left\|\widetilde{\mathbf{S}}^{k+1}-\widetilde{\mathbf{S}}^{k}\right\|_F$),
the radius $\delta$ is given by
\begin{align}
    \delta = \begin{cases}(2(\delta_1 + 2 \delta_{3}))^{-1}|\lambda|, & \text{if } \mathbf{S}^* \text{ is a perfect alignment}\\
    (2(\delta_1 + \delta_2(\mathbf{S}^*) + 2 \delta_{3}(\mathbf{S}^*))))^{-1}|\lambda|, & \text{otherwise.}\end{cases}
\end{align}
}
\end{rmk}

Combining the above with Theorem~\ref{thm:non_deg_loc_min}, Corollary~\ref{cor:suff_cond_views_non_deg} and Theorem~\ref{thm:loc_rigid}, we obtain the following corollaries.
\begin{cor}
If $\mathbf{S}^*$ is an alignment such that $\mathbf{S}^* \in \mathcal{C}$, $\mathbb{L}(\mathbf{S}^*)$ is negative semi-definite and of rank $(m-1)d(d-1)/2$, then RGD converges locally linearly to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$.
\end{cor}

\revadd{Exact recovery:}
\revadd{\citea{chaudhury2015global} showed that under the affine rigidity condition, the algorithms based on spectral and semidefinite relaxation exactly recovers the perfect alignment. While \citea{ling2021generalized} showed exact recovery due to generalized power method in a special setting which in fact exhibits affine rigidity where each point is represented in every local view i.e. the bipartite graph $\Gamma$ is complete. Here we advocate local linear convergence of RGD to a perfect alignment under affine rigidity as well as under weaker conditions of global and local rigidity. Precisely,}
\revadd{
\begin{cor}
Suppose $\mathbf{S}^*$ is a perfect alignment. Then RGD converges locally linearly to $\widetilde{\mathbf{S}}^* = \pi(\mathbf{S}^*)$ if any of the following conditions hold:
\begin{enumerate}
    \item $\Theta(\mathbf{S}^*)$ is affinely rigid, equivalently $\mathbf{C}$ is of rank $(m-1)d$.
    \item $\Theta(\mathbf{S}^*)$ is globally rigid, equivalently $\mathbf{S}^*$ is a unique perfect alignment.
    \item $\Theta(\mathbf{S}^*)$ is locally rigid, equivalently $\mathbb{L}(\mathbf{S}^*)$ is of rank $(m-1)d(d-1)/2$.
\end{enumerate}
Note that $1 \implies 2 \implies 3$.
\end{cor}
}
% \begin{cor}
% \label{cor:G_star_1_conv}
% If $\mathbf{S}^*$ is a perfect alignment such that $|\mathbb{G}^*(\mathbf{S})| = 1$ (see Theorem~\ref{thm:G_star_1}), then RGD converges locally linearly to $\mathbf{S}^*$.
% \end{cor}
\revadd{Finally, we state a sufficient condition on the structure of the noiseless local views that enables local linear convergence of RGD. This condition is stronger than the local rigidity condition and weaker than the global rigidity condition in the above corollary.}
\begin{cor}
\label{cor:G_conv}
If the local views are noiseless and $\mathbb{G}$ is connected then RGD converges locally linearly to a perfect alignment.
\end{cor}
\revdel{
\begin{cor}
If each local view is affinely non-degenerate (see Remark~\ref{rmk:non_deg_views}) and $\mathbf{S}^*$ is a perfect alignment such that the realization $\Theta(\mathbf{S}^*)$ is locally rigid then RGD converges locally linearly to $\mathbf{S}^*$.
\end{cor}
}