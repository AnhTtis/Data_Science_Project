In Section~\ref{subsec:rgd_algo}, we describe the RGD algorithm for solving the alignment problem in Eq.~(\ref{eq:GPOP}). Then in Section~\ref{subsec:loc_sub_conv} we prove the local sublinear convergence of RGD to a critical point of $F$. In Section~\ref{subsec:loc_lin_conv}, using the theory of Morse-Bott functions, we extend the result to the local linear convergence of RGD to a non-degenerate alignment (see Section~\ref{subsec:non_deg_gen_setting}).

\subsection{RGD Algorithm}
\label{subsec:rgd_algo}
A standard way to find a local minimum of Eq.~(\ref{eq:GPOP}) is to use RGD with a suitable initial point, step size and retraction strategy. In this work, our choice of retraction is based on QR decomposition,
\begin{align}
    R_{\QR }: \cup_{\mathbf{S} \in \mathbb{O}(d)^m}(\{\mathbf{S}\} \times T_\mathbf{S}\mathbb{O}(d)^m) &\mapsto \mathbb{O}(d)^m\\
    R_{\QR }\left(\begin{bmatrix}\mathbf{S}_1\\\vdots\\\mathbf{S}_m\end{bmatrix}, \begin{bmatrix}\boldsymbol{\xi}_1\\\vdots\\\boldsymbol{\xi}_m\end{bmatrix}\right) &= \begin{bmatrix}\qf (\mathbf{S}_1+\boldsymbol{\xi}_1)\\\vdots\\\qf (\mathbf{S}_m+\boldsymbol{\xi}_m)\end{bmatrix}. \label{eq:R_QR}
\end{align}
where $\qf (\mathbf{A})$ denotes the $\mathbf{Q}$ factor in the thin QR decomposition of $\mathbf{A}$ \citea{van1996matrix, absil2009optimization}. The step direction will always be $\boldsymbol{\xi} = -\grad F(\mathbf{S})$ which is the projection of the antigradient $-\nabla F(\mathbf{S})$ onto $T_\mathbf{S}\mathbb{O}(d)^m$. Recall (from the proof of Proposition~\ref{prop:gradFS}) that $\grad F(\mathbf{S}) = [[\mathbf{C}\mathbf{S}]_i - \mathbf{S}_i[\mathbf{C}\mathbf{S}]_i^T\mathbf{S}_i]_1^m$. Then the step size $\alpha$ is calculated using the Armijo-type rule with parameters $\beta,\gamma \in (0,1)$ (here $g$ is the canonical metric on $\mathbb{O}(d)^m$ as defined in Eq.~(\ref{eq:g_Z_W})),
\begin{align}
    \alpha = \max_{l \geq 0}\{\beta^l\ \vertbar\ F(R_{\QR }(\mathbf{S}, -\beta^l\grad F(\mathbf{S}))) - F(\mathbf{S}) \leq -\gamma \beta^l g(\nabla F(\mathbf{S}),  \grad F(\mathbf{S})) \}. \label{eq:armijo_step}
\end{align}

\begin{algorithm}
\caption{Riemannian gradient descent for solving GPOP \label{algo:rgd}}
\begin{algorithmic}[1]
\Require $\mathbf{S}^0 \in \mathbb{O}(d)^m$, $\Gamma$, $\{\mathbf{x}_{k,i}: (k,i) \in E(\Gamma)\}$, $\beta, \gamma \in (0,1)$
\State Construct $\mathbf{C}$ as in Eq.~(\ref{eq:GPOP}).
\Repeat
    \State calculate the descent direction $-\grad F(\mathbf{S}^k)$ at $\mathbf{S}^k$ using Eq.~(\ref{eq:gradFS}).
    \State calculate the step size $\alpha_k$ according to the Armijo-type rule (see Eq.~(\ref{eq:armijo_step}))
    \State set $\mathbf{S}^{k+1} = R_{\QR }(\mathbf{S}^k, -\alpha_k \grad F(\mathbf{S}^k))$ using Eq.~(\ref{eq:R_QR}).
    \State $k \leftarrow k + 1$.
\Until{convergence.}
\end{algorithmic}
\end{algorithm}

\subsection{Local Sublinear Convergence of RGD}
\label{subsec:loc_sub_conv}
We proceed to show the local sublinear convergence of Algorithm~\ref{algo:rgd} to a non-degenerate alignment (see Definition~\ref{def:non_deg_alignment0}). Our main tool will be the convergence analysis framework presented in \citeb[Section 2.3]{schneider2015convergence} as used in \citea{liu2019quadratic}. To this end, we first note that $F$ is a real-analytic function bounded from below by zero and $\mathbb{O}(d)^m$ is a compact submanifold of $\mathbb{R}^{md \times d}$. Thus, the Lojasiewicz gradient inequality \citea{lojasiewicz1965ensembles}, \citeb[Section 2.2]{schneider2015convergence} holds at every $\mathbf{S}^* \in \mathbb{O}(d)^m$ and in particular for every $\mathbf{S}^* \in \mathcal{C}$ (see Eq.~(\ref{eq:crit_pts2})) i.e. there exist $\delta, \eta > 0$ and $\theta \in (0,1/2]$ (generally dependent on $\mathbf{S}^*$) such that
\begin{align}
    |F(\mathbf{S}) - F(\mathbf{S}^*)|^{1-\theta} \leq \eta \left\| \grad F(\mathbf{S})\right\|_F. \label{eq:Lojasiewicz_gradient_ineq}
\end{align}
holds for every $\mathbf{S} \in \mathbb{O}(d)^m$ satisfying $\left\|\mathbf{S}-\mathbf{S}^*\right\|_F < \delta$.

Then using Theorem 2.3 in \citea{schneider2015convergence} and the fact that $\mathbb{O}(d)^m$ is compact (thus every sequence on it has a cluster point), for the Algorithm~\ref{algo:rgd} to converge atleast sublinearly to a non-degenerate alignment $\mathbf{S}^*$, it suffices to show that the iterates $\{\mathbf{S}^k\}_{k \geq 0}$ generated by the algorithm satisfy the following:

\begin{enumerate}
    \item [(A1).] \textit{(Sufficient Descent)} There exist constants $\kappa_0 > 0$ and $k_1 \in \mathbb{N}$ such that for $k \geq k_1$,
    \begin{align}
        F(\mathbf{S}^{k+1}) - F(\mathbf{S}^k) \leq - \kappa_0 \left\|\grad F(\mathbf{S}^k)\right\|_F \cdot \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F
    \end{align}
    \item [(A2).] \textit{(Stationarity)} There exist $k_2 \in \mathbb{N}$ such that for $k \geq k_2$,
    \begin{align}
        \left\|\grad F(\mathbf{S}^k)\right\|_F = 0 \implies \mathbf{S}^{k+1} = \mathbf{S}^k.
    \end{align}
    \item [(A3).] \textit{(Safeguard)} There exist a constant $\mu > 0$ and $k_3 \in \mathbb{N}$ such that for $k \geq k_3$,
    \begin{align}
        \left\|\grad F(\mathbf{S}^k)\right\|_F \leq \mu \left\|\mathbf{S}^{k+1}-\mathbf{S}^k\right\|_F.
    \end{align}
\end{enumerate}
The sequence $\{\mathbf{S}^{k}\}_{k \geq 0}$ satisfies (A2) trivially. We need the following propositions to prove (A1) and (A3).
\begin{prop}{\citeb[Appendix E.2]{liu2019quadratic}}
\label{prop:liu_qr}
There exist $\phi, M > 0$ such that for all $\mathbf{S}_i \in \mathbb{O}(d)$ and $\boldsymbol{\xi}_i \in T_{\mathbf{S}_i}\mathbb{O}(d)$ satisfying $\left\|\boldsymbol{\xi}_i\right\|_F \leq \phi$,
\begin{align}
    \left\|\qf (\mathbf{S}_i + \boldsymbol{\xi}_i) - (\mathbf{S}_i + \boldsymbol{\xi}_i)\right\|_F \leq M \left\|\boldsymbol{\xi}_i\right\|_F^2.
\end{align}
In particular, $M = \sqrt{10}/4$ and $\phi = 1/2$ satisfy the above inequality.
\end{prop}
\begin{prop}
\label{prop:second_order_boundedness_of_RQR}
There exist $\phi, M > 0$ such that for all $\mathbf{S} \in \mathbb{O}(d)^{m}$ and $\boldsymbol{\xi} \in T_{\mathbf{S}}\mathbb{O}(d)^m$ satisfying $\left\|\boldsymbol{\xi}\right\|_F \leq \phi$,
\begin{align}
    \left\|R_{\QR }(\mathbf{S},\boldsymbol{\xi}) - (S + \boldsymbol{\xi})\right\|_F \leq M \left\|\boldsymbol{\xi}\right\|_F^2.
\end{align}
\end{prop}

\begin{prop}
\label{prop:alpha_grad}
The sequence $(\alpha_k)_{k \geq 0}$ and $(\mathbf{S}^k)_{k \geq 0}$ satisfies $\lim \alpha_k \left\|\grad F(\mathbf{S}^k)\right\|_F = 0$.
\end{prop}

\subsection{Local Linear Convegence of RGD}
\label{subsec:loc_lin_conv}
Now we extend the above result to the local linear convergence of Algorithm~\ref{algo:rgd} to a non-degenerate alignment $\mathbf{S}^*$. It suffices to show that $\theta = 1/2$ in Eq.~(\ref{eq:Lojasiewicz_gradient_ineq}) \citeb[Theorem 2.3]{schneider2015convergence}. In turn, it suffices to show that $F$ is a Morse-Bott function at $\mathbf{S}^*$ \citeb[Section 6.2]{usevich2020approximate} \citeb[Definition 1.5]{feehan2021optimal}.

\begin{rmk}
Since $F(\mathbf{S}\mathbf{Q}) = F(\mathbf{S})$ for all $\mathbf{Q} \in \mathbb{O}(d)$, therefore no critical point of $F$ is non-degenerate and in particular $F$ is not a Morse-function \citea{cohen_iga_norbury_2006}.
\end{rmk}

\begin{prop}
\label{prop:morse_bott_1}
Let $\mathbf{S}^*$ be a non-degenerate alignment. Then $\widetilde{F}$ is Morse-Bott at $\pi(\mathbf{S}^*)$ and consequently $F$ is Morse-Bott at $\mathbf{S}^*$.
\end{prop}

% Combining previous propositions with \citea{usevich2020approximate}[Proposition $6.8$, Theorem $6.7$], \citea{hu2018convergence}[proposition $4.1$, Proposition $4.2$], we have the following result
% \begin{thm}
% If $S^* \in \mathcal{C}$ satisfies the rigidity constraints then there exist $\delta, \eta > 0$ such that for every $S \in \mathbb{O}(d)^m$ with $\left\|S-S^*\right\|_F < \delta$,
% \begin{align}
%     |F(S) - F(S^*)| \leq \eta \left\|\grad F(S)\right\|_F^2.
% \end{align}
% \end{thm}

% \begin{cor}
% We conclude that $\theta = 1/2$ in Eq.~(\ref{eq:Lojasiewicz_gradient_ineq}). Then invoking the convergence theorem in \citea{schneider2015convergence}[Theorem 2.3], we conclude that the sequence $\{\mathbf{S}^k\}_{k \geq 0}$ generated by Algorithm~\ref{algo:rgd} converges linearly to a critical point in $\mathcal{C}$.
% \end{cor}

As a consequence of the above proposition, we have the following result
\begin{thm}
\label{thm:rgd_conv}
If $\mathbf{S}^*$ is a non-degenerate alignment, then there exist $\delta > 0$ such that RGD (Algorithm~\ref{algo:rgd}) converges to $\mathbf{S}^*$ linearly when initialized with $\mathbf{S}^0$ such that $\left\|\mathbf{S}^0-\mathbf{S}^*\right\|_F < \delta$.
\end{thm}

Combining the above theorem with Theorem~\ref{thm:non_deg_loc_min}, Theorem~\ref{thm:loc_rigid} and Theorem~\ref{thm:G_star_1}, we obtain the following corollaries.
\begin{cor}
If $\mathbf{S}^*$ is an alignment such that $\mathbf{S}^* \in \mathcal{C}$, and $\mathbb{L}(\mathbf{S}^*)$ is negative semi-definite and of rank $(m-1)d(d-1)/2$, then RGD converges locally linearly to $\mathbf{S}^*$.
\end{cor}

\begin{cor}
\label{cor:G_star_1_conv}
If $\mathbf{S}^*$ is a perfect alignment such that $|\mathbb{G}^*| = 1$ (see Theorem~\ref{thm:G_star_1}), then RGD converges locally linearly to $\mathbf{S}^*$.
\end{cor}

\begin{cor}
Under Assumption~\ref{assump:non_deg_views}, if $\mathbf{S}^*$ is a perfect alignment such that the realization $\Theta(\mathbf{S}^*)$ is locally rigid then RGD converges locally linearly to $\mathbf{S}^*$.
\end{cor}