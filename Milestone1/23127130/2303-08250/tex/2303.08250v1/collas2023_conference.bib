@inproceedings{real2019regularized,
  title={Regularized evolution for image classifier architecture search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4780--4789},
  year={2019}
}


@inproceedings{liu2018rethinking,
  author    = {Zhuang Liu and
               Mingjie Sun and
               Tinghui Zhou and
               Gao Huang and
               Trevor Darrell},
  title     = {Rethinking the Value of Network Pruning},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=rJlnB3C5Ym},
  timestamp = {Thu, 25 Jul 2019 14:25:43 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LiuSZHD19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BatchNorm, 
   Publisher = {JMLR Workshop and Conference Proceedings}, 
   title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", 
   Author={Ioffe, Sergey and Szegedy, Christian}, 
   year="2015", 
   Booktitle="Proceedings of the 32nd International Conference on Machine Learning (ICML-15)", 
   Editor={David Blei and Francis Bach}, 
   pages="448-456", 
   url="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" 
   }

@article{yu2022metaformerbaseline,
  title={MetaFormer Baselines for Vision},
  author={Yu, Weihao and Si, Chenyang and Zhou, Pan and Luo, Mi and Zhou, Yichen and Feng, Jiashi and Yan, Shuicheng and Wang, Xinchao},
  journal={arXiv preprint arXiv:2210.13452},
  year={2022}
}

@inproceedings{yu2022metaformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10819--10829},
  year={2022}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}


@article{voitov2022cortical,
  title={Cortical feedback loops bind distributed representations of working memory},
  author={Voitov, Ivan and Mrsic-Flogel, Thomas D},
  journal={Nature},
  volume={608},
  number={7922},
  pages={381--389},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{christophel2017distributed,
  title={The distributed nature of working memory},
  author={Christophel, Thomas B and Klink, P Christiaan and Spitzer, Bernhard and Roelfsema, Pieter R and Haynes, John-Dylan},
  journal={Trends in cognitive sciences},
  volume={21},
  number={2},
  pages={111--124},
  year={2017},
  publisher={Elsevier}
}

@book{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  year={2011},
  publisher={Macmillan}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@incollection{thrun1995lifelong,
  title={Lifelong robot learning},
  author={Thrun, Sebastian and Mitchell, Tom M},
  booktitle={The biology and technology of intelligent autonomous agents},
  pages={165--196},
  year={1995},
  publisher={Springer}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}


@InProceedings{network-of-experts,
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
title = {Expert Gate: Lifelong Learning With a Network of Experts},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{learn-to-grow,
  author    = {Xilai Li and
               Yingbo Zhou and
               Tianfu Wu and
               Richard Socher and
               Caiming Xiong},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Learn to Grow: {A} Continual Structure Learning Framework for Overcoming
               Catastrophic Forgetting},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {3925--3934},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/li19m.html},
  timestamp = {Wed, 13 Jan 2021 14:02:25 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/LiZWSX19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
rethinking-arch-selection,
title={Rethinking Architecture Selection in Differentiable {NAS}},
author={Ruochen Wang and Minhao Cheng and Xiangning Chen and Xiaocheng Tang and Cho-Jui Hsieh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PKubaeJkw3}
}

@InProceedings{spos,
author="Guo, Zichao
and Zhang, Xiangyu
and Mu, Haoyuan
and Heng, Wen
and Liu, Zechun
and Wei, Yichen
and Sun, Jian",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Single Path One-Shot Neural Architecture Search with Uniform Sampling",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="544--560",
abstract="We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally.",
isbn="978-3-030-58517-4"
}

@inproceedings{
darts,
title={{DARTS}: Differentiable Architecture Search},
author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eYHoC5FX},
}

@inproceedings{
vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@InProceedings{learning-to-prompt,
    author    = {Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
    title     = {Learning To Prompt for Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {139-149}
}

@InProceedings{meta-attention,
    author    = {Xue, Mengqi and Zhang, Haofei and Song, Jie and Song, Mingli},
    title     = {Meta-Attention for ViT-Backed Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {150-159}
}

@InProceedings{pool-of-adapters,
    author    = {Ermis, Beyza and Zappella, Giovanni and Wistuba, Martin and Rawal, Aditya and Archambeau, C\'edric},
    title     = {Continual Learning With Transformers for Image Classification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {3774-3781}
}

@misc{three-things-about-vits,
  doi = {10.48550/ARXIV.2203.09795},
  
  url = {https://arxiv.org/abs/2203.09795},
  
  author = {Touvron, Hugo and Cord, Matthieu and El-Nouby, Alaaeldin and Verbeek, Jakob and Jégou, Hervé},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Three things everyone should know about Vision Transformers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{touvron2022three,
  title={Three things everyone should know about Vision Transformers},
  author={Touvron, Hugo and Cord, Matthieu and El-Nouby, Alaaeldin and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2203.09795},
  year={2022}
}


@InProceedings{resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{
vision-moe,
title={Scaling Vision with Sparse Mixture of Experts},
author={Carlos Riquelme Ruiz and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and Andr{\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=NGPmH3vbAA_}
}

@InProceedings{adapter-bert,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract = 	 {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8\%$ of the performance of full fine-tuning, adding only $3.6\%$ parameters per task. By contrast, fine-tuning trains $100\%$ of the parameters per task.}
}

@inproceedings{vdd,
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning multiple visual domains with residual adapters},
 url = {https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{adversarial-continual-learning,
author="Ebrahimi, Sayna
and Meier, Franziska
and Calandra, Roberto
and Darrell, Trevor
and Rohrbach, Marcus",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Adversarial Continual Learning",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="386--402",
abstract="Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at https://github.com/facebookresearch/Adversarial-Continual-Learning.",
isbn="978-3-030-58621-8"
}

@misc{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@misc{dualprompt,
  doi = {10.48550/ARXIV.2204.04799},
  
  url = {https://arxiv.org/abs/2204.04799},
  
  author = {Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{dytox,
    author    = {Douillard, Arthur and Ram\'e, Alexandre and Couairon, Guillaume and Cord, Matthieu},
    title     = {DyTox: Transformers for Continual Learning With DYnamic TOken eXpansion},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {9285-9295}
}

@InProceedings{towards-exemplar-free-continual-learning-vits,
    author    = {Pelosin, Francesco and Jha, Saurav and Torsello, Andrea and Raducanu, Bogdan and van de Weijer, Joost},
    title     = {Towards Exemplar-Free Continual Learning in Vision Transformers: An Account of Attention, Functional and Weight Regularization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {3820-3829}
}

@article{improving-vits,
  title={Improving vision transformers for incremental learning},
  author={Yu, Pei and Chen, Yinpeng and Jin, Ying and Liu, Zicheng},
  journal={arXiv preprint arXiv:2112.06103},
  year={2021}
}

@article{continual-obj-det-kd,
  title={Technical report for iccv 2021 challenge sslad-track3b: Transformers are better continual learners},
  author={Li, Duo and Cao, Guimei and Xu, Yunlu and Cheng, Zhanzhan and Niu, Yi},
  journal={arXiv preprint arXiv:2201.04924},
  year={2022}
}

@inproceedings{attention-is-all-you-need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{thrun,
title = {Lifelong robot learning},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {1},
pages = {25-46},
year = {1995},
note = {The Biology and Technology of Intelligent Autonomous Agents},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00004-Y},
url = {https://www.sciencedirect.com/science/article/pii/092188909500004Y},
author = {Sebastian Thrun and Tom M. Mitchell},
abstract = {Learning provides a useful tool for the automatic design of autonomous robots. Recent research on learning robot control has predominantly focused on learning single tasks that were studied in isolation. If robots encounter a multitude of control learning tasks over their entire lifetime there is an opportunity to transfer knowledge between them. In order to do so, robots may learn the invariants and the regularities of the individual tasks and environments. This task-independent knowledge can be employed to bias generalization when learning control, which reduces the need for real-world experimentation. We argue that knowledge transfer is essential if robots are to learn control with moderate learning times in complex scenarios. Two approaches to lifelong robot learning which both capture invariant knowledge about the robot and its environments are presented. Both approaches have been evaluated using a HERO-2000 mobile robot. Learning tasks included navigation in unknown indoor environments and a simple find-and-fetch task.}
}

@incollection{mccloskey,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
author = {Michael McCloskey and Neal J. Cohen},
abstract = {Publisher Summary
Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.}
}

@InProceedings{piggyback,
author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
title = {Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@InProceedings{nettailor,
author = {Morgado, Pedro and Vasconcelos, Nuno},
title = {NetTailor: Tuning the Architecture, Not Just the Weights},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{spottune,
author = {Guo, Yunhui and Shi, Honghui and Kumar, Abhishek and Grauman, Kristen and Rosing, Tajana and Feris, Rogerio},
title = {SpotTune: Transfer Learning Through Adaptive Fine-Tuning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{memory-transformer,
  title={A Memory Transformer Network for Incremental Learning},
  author={Iscen, Ahmet and Bird, Thomas and Caron, Mathilde and Fathi, Alireza and Schmid, Cordelia},
  journal={arXiv preprint arXiv:2210.04485},
  year={2022}
}

@article{s-prompts,
  title={S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning},
  author={Wang, Yabin and Huang, Zhiwu and Hong, Xiaopeng},
  journal={arXiv preprint arXiv:2207.12819},
  year={2022}
}

@inproceedings{
dynamic-expandable-nets,
title={Lifelong Learning with Dynamically Expandable Networks},
author={Jaehong Yoon and Eunho Yang and Jeongtae Lee and Sung Ju Hwang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Sk7KsfW0-},
}

@article{pathnet,
  title={Pathnet: Evolution channels gradient descent in super neural networks},
  author={Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A and Pritzel, Alexander and Wierstra, Daan},
  journal={arXiv preprint arXiv:1701.08734},
  year={2017}
}

@article{pnn,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}

@inproceedings{gradient-based-sample-selection,
 author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Gradient based sample selection for online continual learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{mir,
 author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Online Continual Learning with Maximal Interfered Retrieval},
 url = {https://proceedings.neurips.cc/paper/2019/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{large-scale-er,
  title={The effectiveness of memory replay in large scale continual learning},
  author={Balaji, Yogesh and Farajtabar, Mehrdad and Yin, Dong and Mott, Alex and Li, Ang},
  journal={arXiv preprint arXiv:2010.02418},
  year={2020}
}

@InProceedings{rainbow-memory,
    author    = {Bang, Jihwan and Kim, Heesu and Yoo, YoungJoon and Ha, Jung-Woo and Choi, Jonghyun},
    title     = {Rainbow Memory: Continual Learning With a Memory of Diverse Samples},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8218-8227}
}

@article{hindsight, title={Using Hindsight to Anchor Past Knowledge in Continual Learning}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16861}, DOI={10.1609/aaai.v35i8.16861}, abstractNote={In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call ``anchoring’’, where the learner uses bilevel optimization to
update its knowledge on the current task, while keeping intact the predictions on some anchor points of past tasks. These anchor points are learned using gradient-based optimization to maximize forgetting, which is approximated by fine-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memory.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet and Torr, Philip and Lopez-Paz, David}, year={2021}, month={May}, pages={6993-7001} }

@inproceedings{agem,
  author    = {Arslan Chaudhry and
               Marc'Aurelio Ranzato and
               Marcus Rohrbach and
               Mohamed Elhoseiny},
  title     = {Efficient Lifelong Learning with {A-GEM}},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Hkf2\_sC5FX},
  timestamp = {Thu, 25 Jul 2019 14:25:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChaudhryRRE19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{continual-prototype-evolution,  author={De Lange, Matthias and Tuytelaars, Tinne},  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},   title={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},   year={2021},  volume={},  number={},  pages={8230-8239},  doi={10.1109/ICCV48922.2021.00814}}

@InProceedings{remind,
author="Hayes, Tyler L.
and Kafle, Kushal
and Shrestha, Robik
and Acharya, Manoj
and Kanan, Christopher",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="REMIND Your Neural Network to Prevent Catastrophic Forgetting",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="466--483",
abstract="People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While there is neuroscientific evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND's generality by pioneering online learning for Visual Question Answering (VQA) (https://github.com/tyler-hayes/REMIND).",
isbn="978-3-030-58598-3"
}

@inproceedings{gem,
 author = {Lopez-Paz, David and Ranzato, Marc\textquotesingle Aurelio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Gradient Episodic Memory for Continual Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{gdumb,
author="Prabhu, Ameya
and Torr, Philip H. S.
and Dokania, Puneet K.",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="GDumb: A Simple Approach that Questions Our Progress in Continual Learning",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="524--540",
abstract="We discuss a general formulation for the Continual Learning (CL) problem for classification---a learning task where a stream provides samples to a learner and the goal of the learner, depending on the samples it receives, is to continually upgrade its knowledge about the old classes and learn new ones. Our formulation takes inspiration from the open-set recognition problem where test scenarios do not necessarily belong to the training distribution. We also discuss various quirks and assumptions encoded in recently proposed approaches for CL. We argue that some oversimplify the problem to an extent that leaves it with very little practical importance, and makes it extremely easy to perform well on. To validate this, we propose GDumb that (1) greedily stores samples in memory as they come and; (2) at test time, trains a model from scratch using samples only in the memory. We show that even though GDumb is not specifically designed for CL problems, it obtains state-of-the-art accuracies (often with large margins) in almost all the experiments when compared to a multitude of recently proposed algorithms. Surprisingly, it outperforms approaches in CL formulations for which they were specifically designed. This, we believe, raises concerns regarding our progress in CL for classification. Overall, we hope our formulation, characterizations and discussions will help in designing realistically useful CL algorithms, and GDumb will serve as a strong contender for the same.",
isbn="978-3-030-58536-5"
}

@InProceedings{icarl,
author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
title = {iCaRL: Incremental Classifier and Representation Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@article{tiny-replay,
  title={On tiny episodic memories in continual learning},
  author={Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip HS and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.10486},
  year={2019}
}

@INPROCEEDINGS{hayes-icra,  author={Hayes, Tyler L. and Cahill, Nathan D. and Kanan, Christopher},  booktitle={2019 International Conference on Robotics and Automation (ICRA)},   title={Memory Efficient Experience Replay for Streaming Learning},   year={2019},  volume={},  number={},  pages={9769-9776},  doi={10.1109/ICRA.2019.8793982}}

@inproceedings{dark-experience-replay,
 author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, SIMONE},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15920--15930},
 publisher = {Curran Associates, Inc.},
 title = {Dark Experience for General Continual Learning: a Strong, Simple Baseline},
 url = {https://proceedings.neurips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf},
 volume = {33},
 year = {2020}
}

@InProceedings{large-scale-inc-learning,
author = {Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Fu, Yun},
title = {Large Scale Incremental Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{fast-and-slow,
 author = {Pham, Quang and Liu, Chenghao and Hoi, Steven},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16131--16144},
 publisher = {Curran Associates, Inc.},
 title = {DualNet: Continual Learning, Fast and Slow},
 url = {https://proceedings.neurips.cc/paper/2021/file/86a1fa88adb5c33bd7a68ac2f9f3f96b-Paper.pdf},
 volume = {34},
 year = {2021}
}

@InProceedings{contrastive-continual-learning,
    author    = {Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
    title     = {Co2L: Contrastive Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9516-9525}
}

@inproceedings{gan-memory,
 author = {Cong, Yulai and Zhao, Miaoyun and Li, Jianqiao and Wang, Sijia and Carin, Lawrence},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {16481--16494},
 publisher = {Curran Associates, Inc.},
 title = {GAN Memory with No Forgetting},
 url = {https://proceedings.neurips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{generative-replay,
 author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Continual Learning with Deep Generative Replay},
 url = {https://proceedings.neurips.cc/paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{what-not-to-forget,
author="Aljundi, Rahaf
and Babiloni, Francesca
and Elhoseiny, Mohamed
and Rohrbach, Marcus
and Tuytelaars, Tinne",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Memory Aware Synapses: Learning What (not) to Forget",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="144--161",
abstract="Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule, which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting <subject, predicate, object> triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.",
isbn="978-3-030-01219-9"
}

@inproceedings{
selfless-sequential-learning,
title={Selfless Sequential Learning},
author={Rahaf Aljundi and Marcus Rohrbach and Tinne Tuytelaars},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkxbrn0cYX},
}

@InProceedings{podnet,
author="Douillard, Arthur
and Cord, Matthieu
and Ollion, Charles
and Robert, Thomas
and Valle, Eduardo",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="86--102",
abstract="Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks -- a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively.",
isbn="978-3-030-58565-5"
}

@inproceedings{
variational-continual-learning,
title={Variational Continual Learning},
author={Cuong V. Nguyen and Yingzhen Li and Thang D. Bui and Richard E. Turner},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkQqq0gRb},
}

@article{
kirkpatrick-overcoming,
author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
title = {Overcoming catastrophic forgetting in neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {114},
number = {13},
pages = {3521-3526},
year = {2017},
doi = {10.1073/pnas.1611835114},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1611835114},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.}}

@ARTICLE{lwf,  author={Li, Zhizhong and Hoiem, Derek},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Learning without Forgetting},   year={2018},  volume={40},  number={12},  pages={2935-2947},  doi={10.1109/TPAMI.2017.2773081}}


@InProceedings{synaptic-intelligence,
  title = 	 {Continual Learning Through Synaptic Intelligence},
  author =       {Friedemann Zenke and Ben Poole and Surya Ganguli},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3987--3995},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/zenke17a/zenke17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/zenke17a.html},
  abstract = 	 {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.}
}


@InProceedings{progress-and-compress,
  title = 	 {Progress \& Compress: A scalable framework for continual learning},
  author =       {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4528--4537},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/schwarz18a/schwarz18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/schwarz18a.html},
  abstract = 	 {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress &amp; compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.}
}


@article{imagenet,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@article{cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@techreport{aircraft,
   title         = {Fine-Grained Visual Classification of Aircraft},
   author        = {S. Maji and J. Kannala and E. Rahtu
                    and M. Blaschko and A. Vedaldi},
   year          = {2013},
   archivePrefix = {arXiv},
   eprint        = {1306.5151},
   primaryClass  = "cs-cv",
}

@ARTICLE{daimlerpedcls,  author={Munder, S. and Gavrila, D.M.},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={An Experimental Study on Pedestrian Classification},   year={2006},  volume={28},  number={11},  pages={1863-1868},  doi={10.1109/TPAMI.2006.217}}

@article{
omniglot,
author = {Brenden M. Lake  and Ruslan Salakhutdinov  and Joshua B. Tenenbaum },
title = {Human-level concept learning through probabilistic program induction},
journal = {Science},
volume = {350},
number = {6266},
pages = {1332-1338},
year = {2015},
doi = {10.1126/science.aab3050},
URL = {https://www.science.org/doi/abs/10.1126/science.aab3050},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aab3050},
abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.}}

@inproceedings{svhn,
title	= {Reading Digits in Natural Images with Unsupervised Feature Learning},
author	= {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
year	= {2011},
URL	= {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
booktitle	= {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}
}

@INPROCEEDINGS{vgg-flowers,  author={Nilsback, Maria-Elena and Zisserman, Andrew},  booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing},   title={Automated Flower Classification over a Large Number of Classes},   year={2008},  volume={},  number={},  pages={722-729},  doi={10.1109/ICVGIP.2008.47}}

@article{gtsrb,
title = {Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition},
journal = {Neural Networks},
volume = {32},
pages = {323-332},
year = {2012},
note = {Selected Papers from IJCNN 2011},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2012.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0893608012000457},
author = {J. Stallkamp and M. Schlipsing and J. Salmen and C. Igel},
keywords = {Traffic sign recognition, Machine learning, Convolutional neural networks, Benchmarking},
abstract = {Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons.}
}

@InProceedings{riemannian-walk,
author="Chaudhry, Arslan
and Dokania, Puneet K.
and Ajanthan, Thalaiyasingam
and Torr, Philip H. S.",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="556--572",
abstract="Incremental learning (il) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the il problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of il. The main challenge for an il algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, il also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of il algorithms. Furthermore, we present RWalk, a generalization of ewc++ (our efficient version of ewc [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various il algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.",
isbn="978-3-030-01252-6"
}



@article{ucf1,
  author    = {Khurram Soomro and
               Amir Roshan Zamir and
               Mubarak Shah},
  title     = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
               The Wild},
  journal   = {CoRR},
  volume    = {abs/1212.0402},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.0402},
  eprinttype = {arXiv},
  eprint    = {1212.0402},
  timestamp = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-0402.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{ucf2,  author={Bilen, Hakan and Fernando, Basura and Gavves, Efstratios and Vedaldi, Andrea and Gould, Stephen},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Dynamic Image Networks for Action Recognition},   year={2016},  volume={},  number={},  pages={3034-3042},  doi={10.1109/CVPR.2016.331}}

@INPROCEEDINGS{dtd,  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition},   title={Describing Textures in the Wild},   year={2014},  volume={},  number={},  pages={3606-3613},  doi={10.1109/CVPR.2014.461}}

@article{fashion-mnist,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@ARTICLE{mnist,  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}

@misc{notmnist,
  author = {Yaroslav Bulatov},
  title = {notMNIST dataset},
  year = {2011},
  howpublished = {\url{http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html}}
}


@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@misc{adam,
  doi = {10.48550/ARXIV.1412.6980},
  url = {https://arxiv.org/abs/1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
