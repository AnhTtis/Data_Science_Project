% This file was adapted from ICLR2022_conference.tex example provided for the ICLR conference
\documentclass{article} % For LaTeX2e
\usepackage{collas2023_conference,times}
\usepackage{easyReview}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{cuted}
% \usepackage{capt-of}
% \usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{subfig}
% \usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig} 
\usepackage{sidecap}
% \usepackage{subfig}
\usepackage{enumitem}

% Please leave these options as they are
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=purple,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


\definecolor{skip}{HTML}{F2ACCA}
\definecolor{reuse}{HTML}{9CCEA7}
\definecolor{adapt}{HTML}{FEB24C}
\definecolor{new}{HTML}{9EC9E2}

\newcommand{\subfiggrid}[5]{
    \begin{subfigure}[t]{0.15\linewidth}
        \centering
        \includegraphics[width=#3cm,height=#4cm]{figures/#1_samples/#2/000001.jpg}
        \includegraphics[width=#3cm,height=#4cm]{figures/#1_samples/#2/000002.jpg}
        \caption{#5}
    \end{subfigure}
}

\title{Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \collasfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Chinmay Savadikar \thanks{Department of Electrical and Computer Engineering, North Carolina State University} \\
% Department of Electrical and Computer Engineering \\
% North Carolina State University\\
% Country \\
\texttt{csavadi@ncsu.edu} \\
\And % Use And to have authors side by side
Michelle Dai \thanks{Operations Research \& Financial Engineering, Princeton University}  \\
% TBD \\
% Princeton University \\
% Another Country \\
\texttt{mdai@princeton.edu} \\
\And % Use AND to have authors block one under the other
Tianfu Wu \footnotemark[1] \\
% Department of Electrical and Computer Engineering \\
% North Carolina State University \\
\texttt{tianfu\_wu@ncsu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \collasfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\preprintcopy % Uncomment for the preprint version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by  Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. 
  This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing  ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, which is maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new  in lifelong learning. The LM of a task consists of two parts: the dedicated expert components (as model parameters) at different layers of a ViT learned via NAS, and the mean class-tokens (as stored latent vectors for measuring task similarity) associated with the expert components. For a new task, a hierarchical task-similarity-oriented  exploration-exploitation sampling based NAS is proposed to learn the expert components.  The task similarity is measured based on the normalized cosine similarity between the mean class-token of the new task and those of old tasks. The proposed method is complementary to prompt-based lifelong learningwith ViTs. 
  % and can be combined with prompting-based approaches for stronger results.
  In experiments, the proposed method is tested on the challenging Visual Domain Decathlon (VDD) benchmark and the recently proposed 5-Dataset benchmark. It obtains consistently better performance than the prior art with sensible ArtiHippo learned continually.
\end{abstract}

% figure
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{-6pt}
% equation
\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{1pt}
\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{1pt} 

\section{Introduction}
\label{sec:intro} \vspace{-2mm}
Developing lifelong learning machines is one of the hallmarks of AI, to mimic human intelligence in terms of learning-to-learn to be adaptive and skilled at streaming tasks, or  even to emulate human intelligence in some application domains. However, state-of-the-art machine (deep) learning systems realized by Deep Neural Networks (DNNs) have yet to be intelligent in the biological sense from the perspective of lifelong learning, especially plagued with the critical issue known as \textit{catastrophic forgetting} at streaming tasks in a dynamic environment~\citep{mccloskey,thrun}. 
Catastrophic forgetting simply means that these systems ``forget" how to solve old tasks after sequentially and continually trained on a new task using the data of the new task only. 
% Catastrophic forgetting means that these systems ``forget" previously incorporated data of old tasks (distilled into model parameters) when  trained only with data of a new task. This occurs due to the systematic change of model parameters  driven solely by the chain-rule based gradient backpropagation of the loss of the new task. 
Addressing catastrophic forgetting in lifelong learning is a pressing need with potential paradigm-shift impacts in the next wave of trustworthy and/or brain-inspired AI. 



To address catastrophic forgetting, one direct methodology is to utilize exemplar-based settings in which a small number of selected training samples stored for each previous task is used in conjunction with the data of a new task in training the model for the new task. Those exemplar-based methods are also referred as \textit{Experience Replay} based methods~\citep{gradient-based-sample-selection,hayes-icra,large-scale-inc-learning}.   How the exemplars are selected and how they are incorporated in learning a new task distinguish different exemplar-based methods. Although shown as an effective strategy, retaining raw data samples may induce issues on data security and privacy, as well as the long-run sustainability.

To develop exemplar-free methods, three main strategies have been studied in the literature. The first is to regularize the change in model parameters when trained on a new task as done in~\citep{kirkpatrick-overcoming}.  The second is to adapt the structure of a network (with the parameters) for the new task from the network learned for the previous tasks, as done in the learn-to-grow (L2G) method~\citep{learn-to-grow} which uses differentiable Neural Architecture Search (NAS) to find whether to {\tt reuse}, {\tt adapt} or {\tt new} each layer of a Multi-Layer Percetpron (MLP) or a convolultional neural network (CNN). More recently, with the availability of powerful pretrained Transformers~\citep{attention-is-all-you-need} based Large Foundation Models (LFMs)~\citep{bommasani2021opportunities} (such as the CLIP models~\citep{clip}), the 
\begin{wrapfigure} {r}{0.48\textwidth} \vspace{-4mm}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/artihippo_flow-v2.pdf}
    \caption{Illustration of the proposed method for task-incremental lifelong learning without any  catastrophic forgetting. \textit{Left:} The Multi-Head Self-Attention (MHSA) block in Vision Transformers~\citep{vit} with the proposed Artificial Hippocampi (ArtiHippo) replacing the original linear projection layer. \textit{Middle:} The ArtiHippo growing is maintained by four operations, similar in spirit to the learn-to-grow method~\citep{learn-to-grow}.      
    \textit{Right:} The ArtiHippo is represented by a mixture of experts with an example for different tasks (e.g., $j$) started from the task $i$. See text for details.}
    \label{fig:flow}\vspace{-3mm}
\end{wrapfigure} third is to freeze pretrained LFMs and then to learn prompts (or task tokens) instead for lifelong learning, e.g., the learn-to-prompt (L2P) methods~\citep{learning-to-prompt,dualprompt,s-prompts,dytox}. 

% The third is the learn-to-grow method~\citep{learn-to-grow} which exploits neural architecture search (NAS) using the differentiable architecture search (DARTS) method~\citep{darts} to find whether to {\tt skip, reuse, adapt} or {\tt new} each layer in the feature backbone network. 

% The first regularization based approach often retains the same feature backbone network across tasks and may suffer from the trade-off between how much the model parameters can be tuned for a new task to alleviate catastrophic forgetting of old tasks and how well the tuned model will work at the new task.
% The second learn-to-prompt based approach has shown remarkable progress, thanks to the expressive power of pretrained LFMs (that has been interpreted as the ``intuition" or System 1~\citep{kahneman2011thinking} of AI). The learned prompts can be interpreted as a form of external long-term memory or latent exemplars. However, it needs to carry on the LFM for all tasks, even when a task is a relatively easier one, and thus may suffer from the unnecessary high computational cost from tasks to tasks. Although pretrained LFMs are powerful, leaving them frozen in lifelong learning may still encounter corner cases that will fail the learning of prompts.  
% The third learn-to-grow approach can maintain dynamic feature backbone networks for different tasks based on NAS, which leads to the desired selectivity and plasticity of networks in lifelong learning. It has been mainly studied with Convolutional Neural Networks (CNNs), and often apply NAS for all layers with respect to the four learning-to-grow ``skills", which is time consuming and may be less effective when a new task has little data. 

In this paper, we are interested in studying examplar-free resilient lifelong learning with Vision Transformers (ViTs)~\citep{vit}. Our goal is to seek alternative formulations that are not built on completely frozen pretrained Transformer models, but can induce plastic and reconfigurable structures for streaming tasks. We are motivated by some fantastic observations of natural intelligence possessed by biological systems (e.g., the human brain) which exhibit remarkable capacity of learning and adapting their structure and function for tackling different tasks  throughout their lifespan, while retaining the stability of their core functions. It has been observed in neuroscience that learning and memory are entangled together in a highly sophisticated way~\citep{christophel2017distributed,voitov2022cortical}. For lifelong learning, the long-term memory (LM) maintained by the hippocampal system plays an important role. A question naturally arises: \textbf{What would be the counterpart, Artificial Hippocampi (ArtiHippo), in ViTs to facilitate resilient lifelong learning? } 
% In this paper, we are interested in studying examplar-free resilient lifelong learning with Vision Transformers (ViTs)~\citep{vit}. Our goal is to seek alternative formulations that are not built on completely frozen pretrained Transformer models, but can induce plastic and reconfigurable structures for streaming tasks. We are motivated by some remarkable properties exhibited by natural intelligence possessed by biological systems (e.g., human brain) like capacity of learning and adapting their structure and function for tackling different tasks  throughout their lifespan, while retaining the stability of their core functions. It has been observed in neuroscience that learning and memory are entangled together in a highly sophisticated way~\citep{christophel2017distributed,voitov2022cortical}. For lifelong learning, the long-term memory (LM) maintained by the hippocampal system plays an important role. A question naturally arises: \textbf{What would be the counterpart, Artificial Hippocampi (ArtiHippo), in ViTs to facilitate resilient lifelong learning? } 

% harness the best of the second and the third strategies above in a way that moves a small step forward towards being potentially more biologically-plausible. Biological systems, including, but not limited to, the human brain, exhibit a remarkable capacity of learning and adapting their structure and function for tackling different tasks  throughout their lifespan (i.e., learning to be adaptive and skilled at streaming tasks), while retaining the stability of their core functions. It has been observed in neuroscience that learning and memory are entangled together in a highly sophisticated way~\citep{christophel2017distributed,voitov2022cortical}. For lifelong learning, the long-term memory maintained by the hippocampal system plays an important role.
% {\color{blue}Given that Transformers, especially when assembled into LFMs, have emerged as the counterpart ``brain" of AI, there is a pressing need to explore long-term memory mechanisms and realization in Transformers that enable resilient lifelong learning. }{\color{red}Chinmay: Do we need to include this?} 

% As aforementioned, 

The L2P methods exploit external LM memory to store the learned task prompts/tokens while using frozen Transformer backbones, which may lack the plasticity needed in resilient lifelong learning, increases the cost (due to the quadratic complexity of Transformer models with respect to the number of tokens),  and unnecessarily enforces all tasks encountered in lifelong learning (regardless of their underlying difficulty levels) to use the same frozen network.  
% Built on Transformers, the learned prompts in the learning-to-prompt methods are stored in an external long-term memory, which rely on the frozen LFMs to address the lifelong learning challenges. 
In this paper, \textbf{we seek more integrative memory mechanisms that introduce learnable parts into Transformers (in contrast to be entirely frozen) to induce reconfigurability, selectivity and plasticity in lifelong learning.} To that end, we adapt the learn-to-grow method~\citep{learn-to-grow}, but do not apply the NAS with respect to the four operations uniformly across layers in a ViT. Instead, we aim to find the ArtiHippo inside the ViT. As illustrated in Figure \ref{fig:flow}, the final projection layer in the multi-head self-attention (MHSA) block of a ViT is identified and selected as the ArtiHippo (Sec.~\ref{sec:identify_artihippo}). The learn-to-grow NAS is only applied in maintaining ArtiHippo layers, while other components are frozen to maintain the stability of core functions, as illustrated in Figure \ref{fig:nas}. Rather than adopting the DARTS~\citep{darts} NAS used in~\citep{learn-to-grow}, the learn-to-grow NAS in this paper is built on the single-path one-shot (SPOS) NAS~\citep{spos}, in which we propose a hierarchical exploration-exploitation sampling (Figure \ref{fig:nas-sampling}) strategy for lifelong learning (Sec.~\ref{sec:grow_artihippo}).        

In experiments,  this paper considers lifelong learning with task indices available in both training and inference, which is often referred to as \textit{task-incremental setup}. When different tasks consist of data from different domains such as the Visual Domain Decathlon (VDD) benchmark~\citep{vdd}, it is also related to domain-incremental setup, but without assuming the same output space between tasks, e.g., the same number of classes in classification in different domains. The right of Figure~\ref{fig:flow} illustrates an example of learned ArtiHippo. With the task indices, the execution of the computational graph for a given task is straightforward. The proposed method achieves zero-forgetting on old tasks. %Figure~\ref{fig:nas} illustrates the learning-to-grow NAS. Unlike the learn-to-grow method~\citep{learn-to-grow} which utilizes the differentiable architecture search (DARTS) method~\citep{darts}, we propose a task-similarity-oriented NAS. 
We show the potential of applying our proposed method for class-incremental lifelong learning settings, especially when integrated with the complementary L2P methods. 




% Without loss of generality, consider a machine learning system initially parameterized by a Deep Neural Network (DNN), $f_{\Theta}(\cdot)$, with trainable parameters $\Theta$ in learning a sequence of $T$ streaming tasks. Denote by $\Theta^{t}$ the parameters associated with a task $t$ and by $\Theta^{1:t}$ the current full model for tasks $1$ to $t$. In training, the system may learn to grow the model capacity to tackle different tasks, and thus we have $\Theta^{i}\subseteq \Theta^{1:t}$ ($i=1,\cdots, t$). Starting from the model trained for the first task (e.g., the ImageNet-1k classification~\citep{}), $f_{\Theta^1}(\cdot)$,  for the next (second) task, it could be of any nature, e.g., the Omniglot classification~\citep{} that is significantly different from the first task, or the CIFAR-100 classification that is very much similar to the first task.   



% \section{Problem Description and Notation}
% TODO: General description of the lifelong learningsetup by referring to learn to grow paper

% In the proposed approach, we denote an expert as the transformation which follows the MHSA mechanism. For the case of a new expert, the transformation is a new Linear layer, whereas for an adapt expert, the trasformation is the adapted expert followed by the adapter MLP. 
%-------------------------------------------------------------------------

% \subsection{Retrieval of style files}

% The style files for CoLLAs and other conference information are available online at:
% \begin{center}
%    \href{http://www.lifelong-ml.cc/}{http://www.lifelong-ml.cc/}
% \end{center}
% The file \verb+collas2023_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your CoLLAs paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+collas2023_conference.sty+ and \verb+collas2023_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+collas2023_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{Related Work and Our Contributions} \vspace{-2mm}
The catastrophic forgetting problem in neural networks~\citep{mccloskey} refers to an exponential loss in performance on previous tasks as the network is trained on data from new tasks. Continual learning~\citep{thrun} aims to solve this problem by enabling the network to perform well on previous tasks when new tasks are introduced. The most straightforward approach to the problem is to retain some exemplars from the previous tasks and replay them to the model along with the data from the current task, referred to as \textit{Experience Replay Based approaches}~\citep{gradient-based-sample-selection,mir,large-scale-er,rainbow-memory,hindsight,agem,continual-prototype-evolution,remind,gem,gdumb,icarl,tiny-replay,hayes-icra,dark-experience-replay,large-scale-inc-learning,fast-and-slow,contrastive-continual-learning}. Instead of storing raw exemplars, \textit{Generative replay methods}~\citep{generative-replay,gan-memory} learn the generative process for the data of a task, and replay exemplars sampled from that process along with the data from the current task. For exemplar-free continual learning, \textit{Regularization Based approaches} explicitly control the plasticity of the model by preventing the parameters of the model from deviating too far from their stable values learned on the previous tasks when learning the current task~\citep{what-not-to-forget,selfless-sequential-learning,podnet,variational-continual-learning,kirkpatrick-overcoming,lwf,synaptic-intelligence,progress-and-compress}. These approaches are similar in principle: they aim to balance the stability and plasticity of a fixed-capacity model.

To overcome this constraint, \textit{Dynamic Models} aim to use different parameters per task to avoid use of stored exemplars. Dynamically Expandable Network~\citep{dynamic-expandable-nets} adds neurons to a network based on learned sparsity constraints and heuristic loss thresholds. PathNet~\citep{pathnet} finds task-specific submodules from a dense network, and only trains submodules not used by other tasks. Progressive Neural Networks~\citep{pnn} learn a new network per task and adds lateral connections to the previous tasks' networks.~\citep{vdd} learns residual adapters which are added between the convolutional and batch normalization layers. \citep{network-of-experts} learns an expert network per task by transferring the expert network from the most related previous task. Learn to Grow~\citep{learn-to-grow} uses Differentiable Architecture Search (DARTS~\citep{darts}) to determine if a layer can be reused, adapted, or renewed (3 fundamental skills: reuse, adapt, new) for a task. Our approach is most closely related to Learn to Grow~\citep{learn-to-grow} which can also be interpreted as a Mixture of Experts framework. Dynamic models have also been explored for efficient transfer learning~\citep{nettailor,spottune,piggyback}.
% NetTailor~\citep{nettailor} learns shortcut connections that can bypass multiple layers. SpotTune~\citep{spottune} finds optimal layers to finetune. Piggyback~\citep{piggyback} learns binary masks to find a subset of parameters useful for downstream tasks without changing their values.
% Learn to Grow uses 3 fundamental skills: reuse, adapt and renew to dynamically grow a Neural Network using Differential Neural Architecture Search (DARTS)~\citep{darts}. It constructs an operation search space using a cross-product of the experts from all the previous tasks, and 3 fundamental skills. It then jointly learns a set of coefficients $\alpha$ along with the network parameters $\Theta$, and chooses the expert+skill with the largest $\alpha$.

Recently, there has been increasing interest in lifelong learning using Vision Transformers~\citep{learning-to-prompt,dualprompt,meta-attention,pool-of-adapters,dytox,towards-exemplar-free-continual-learning-vits,improving-vits,continual-obj-det-kd,memory-transformer,s-prompts}. \textit{Prompt Based approaches} learn external parameters that encode task-specific information useful for classification~\citep{learning-to-prompt,s-prompts,dytox}. Learning to Prompt (L2P)~\citep{learning-to-prompt} learns a pool of prompts and uses a key-value based retrieval to infer the task index and retrieve the correct set of prompts at test time. DualPrompt~\citep{dualprompt} learns generic and task-specific prompts and extends Learning to Prompt.~\citep{meta-attention} uses a ViT pretrained on ImageNet and learns binary masks to enable/disable parameters of the Feedforward Network (FFN), and the attention between image tokens for downstream tasks.

% ~\citep{pool-of-adapters} keep a fixed budget of adapters~\citep{adapter-bert} and learn an adapter per task till the budget is exhausted, and finetune the adapter with the closest previous task then. DyTox~\citep{dytox} learns a common encoder across tasks and a task -- specific decoder using a task token per task.~\citep{towards-exemplar-free-continual-learning-vits} employ regularization techniques on the Attention Maps of the self-attention mechanism to balance the stability-plasticity dilemma across tasks.

% {\color{red}Here, we evaluate ViTs for exemplar-free lifelong learningon the VDD benchmark: a large-scale and dataset containing diverse tasks. Previous methods using ViTs evaluate on ImageNet with 100 classes~\citep{dytox}, 300 classes~\citep{towards-exemplar-free-continual-learning-vits} and 1000 classes~\citep{dytox,improving-vits}, SplitCIFAR10~\citep{towards-exemplar-free-continual-learning-vits}, SplitCIFAR100~\citep{towards-exemplar-free-continual-learning-vits,dytox,improving-vits}}

% \textit{To the best of our knowledge, we are the first to evaluate ViTs for exemplar free lifelong learningon a large number of diverse tasks like the VDD benchmark~\citep{vdd}}. Although L2P evaluates on the 5-Datasets benchmark (which contains tasks from diverse domains), each task therein has abundant data and is easy to learn in isolation. In contrast, the VDD dataset contains tasks from varied domains, many of which contain very few labelled samples.

\textbf{Our Contributions} 
We make four main contributions to the field of lifelong learning with ViTs. (i) We propose and identify Artifical Hippocampi (ArtiHippo) in ViTs, i.e., the final projection layers of the multi-head self-attention blocks in a ViT, to realize a long-term task-similarity-oriented memory mechanism for resilient lifelong learning. We also present a new usage for the class-token in ViTs as the memory growing guidance. (ii) We present a hierarchical task-similarity-oriented, sampling-empowered single-path one-shot neural architecture search method for learning to grow ArtiHippo continually with respect to four basic growing operations: {\tt Skip}, {\tt Reuse}, {\tt Adapt}, and {\tt New}, which not only overcomes catstrophic forgetting, but also leads to efficient forward transfer. (iii) We are the first, to the best of our knowledge, to evaluate lifelong learning with ViTs on the large-scale, diverse and imbalanced VDD benchmark~\citep{vdd} with strong empirical performance obtained. Although the L2P method~\citep{learning-to-prompt} evaluates on the 5-Datasets benchmark (which contains tasks from diverse domains), each task therein has abundant data and is easy to learn in isolation. In contrast, the VDD dataset contains tasks from varied domains, many of which contain very few labelled samples. (iv) We show that our method is complementary to prompting-based approaches, and combining the two leads to even higher performance.
% Our key contributions are as follows: (i) Through ablation experiments, we identify the final linear projection layer of the multi-head self attention block (MHSA) as the ideal candidate to realize a Long-Term Memory (ArtiHippo) for lifelong learningthrough a Mixture of Experts framework in Vision Transformers.
% (ii) We propose a task-similarity driven sampling scheme which converts task-similarities into a probability distribution over the operation operations {\tt skip}, {\tt reuse}, {\tt adapt} and {\tt new} applied on the existing experts in a semantically meaningful way.
% (iii) We extend the Single Path One Shot Neural Architecture Search~\citep{spos} by introducing an exploration-exploitation driven sampling for training the supernet and initial population generation in the evolutionary search.
% (iv) We are the first to evaluate lifelong learningwith ViTs on a large scale, diverse and imbalanced dataset like the Visual Domain Decathlon (VDD) dataset.


\section{Approach}\vspace{-2mm}
In this section, we first present the ablation study on identifying the ArtiHippo in a Transformer block (Figure~\ref{fig:flow}). Then, we present details of learning to grow ArtiHippo in lifelong learning (Figures~\ref{fig:nas} and~\ref{fig:nas-sampling}). 

% \begin{wraptable} {r}{0.5\textwidth}%[t]
%     \vspace{-9mm}
%     \centering
%     \resizebox{0.5\textwidth}{!}{
%     \begin{tabular}{l|l|c|c}
%         \toprule
%         Index & Finetuned Component &  Avg. Accuracy & Avg. Forgetting \\
%         \midrule
%         1 & $\text{LN}_1$+$\text{LN}_2$ & 81.76 & 21.24 \\
%         \midrule
%         2 & $\text{MLP}^{\text{down}}$+$\text{MLP}^{\text{up}}$+$\text{LN}_2$ & 84.20 & 44.76 \\
%         3 & \text{MLP}^{\text{down}} & 83.66 & 37.99 \\
%         4 & $\text{LN}_2$ & 80.04 & 16.35 \\
%         \midrule
%         5 & MHSA + $\text{LN}_1$ & 85.26 & 54.38 \\
%         6 & LN$_1$ & 81.18 & 19.04 \\
%         7 & Query & 81.57 & 19.69 \\
%         8 & Key & 81.56 & 19.19 \\
%         9 & Query+Key & 81.49 & 31.10 \\
%         10 & Value & 84.99 & 37.58 \\
%         11 & Proj  (ArtiHippo) & 85.11 & 30.50 \\
%         \midrule
%         \multicolumn{2}{c|}{Adapt the Last MHSA Block Only} & 72.55 & - \\
%         \multicolumn{2}{c|}{Train a Classifier w/ Frozen Backbone} & 70.78 & - \\
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Ablation study on identifying ArtiHippo from 11 components in a Transformer block (Eqn.~\ref{eq:mhsa_proj}, Eqn.~\ref{eq:ffn} and Eqn.~\ref{eq:mhsa}). Starting from the first task model from the VDD benchmark, i.e., ImageNet-pretrained ViT, we finetune a given component with other components frozen continually and sequentially across the remaining 9 task in a predefined order, and train the task head classifier from scratch for each individual task. Average Accuracy (Eqn.~\ref{eq:avg_acc}) and Average Forgetting (Eqn.~\ref{eq:avg_forgetting}) are computed in comparison. The projection layer in the MHSA block is identified and selected as ArtiHippo. For comparisons, we provide results of two conventional transfer learning settings in the last two rows: adapting the last MHSA block, and training the head classifier only.   %Finetuning the MHSA block alone achieves the highest average accuracy, while finetuning the Projection layer from the MHSA block achieves almost equivalent performance, with much lower parameters.
%     % See text for details.
%     }
%     \label{tab:acc_vs_forgetting} \vspace{-3mm}
% \end{wraptable}

\subsection{Identifying ArtiHippo in Transformers}\label{sec:identify_artihippo}\vspace{-2mm}
\label{identifying-artihippo}
The left of Figure~\ref{fig:flow} shows a Vision Transformer (ViT) block~\citep{vit}. Without loss of generality, denote by $x_{L,d}$ an input sequence consisting of $L$ tokens encoded in a $d$-dimensional space. In ViTs, the first token is the so-called class-token. The remaining $L-1$ tokens are formed by patch embedding of an input image, together with additive positional encoding. A Transformer block consists of a MHSA component and a feed-forward network (FFN) with skip/residual connections~\citep{resnet} and layer normalization (LN)~\citep{ba2016layer}. The FFN is often implemented by a multi-layer perceptron (MLP) with a feature expansion layer $\text{MLP}^{\text{up}}$ and a feature reduction layer $\text{MLP}^{\text{down}}$. It is defined by,
\begin{align}
    z_{L, d}  = x_{L, d} + \text{Proj}(\text{MHSA}(\text{LN}_1(x_{L, d}))), \qquad
    y_{L, d}  = z_{L, d} + \text{MLP}^{\text{down}}(\text{MLP}^{\text{up}}((\text{LN}_2(z_{L, d})))), \label{eq:transformer}
\end{align}
where $\text{Proj}(\cdot)$ is a linear transformation fusing the multi-head outputs from the MHSA module. 

%% move to suppl
% \begin{figure} [t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/acc_vs_forgetting.pdf}
%     \caption{Breakdown of individual task Accuracy and Average Forgetting of the finetuning components: 1, 4, 6 and 11 in Table~\ref{tab:acc_vs_forgetting}, which all are lightweight and have reasonable trade-off between Average Accuracy and Average Forgetting. Overall, the projection layer is selected favoring its consistently good accuracy and the forgetting can be handled with the proposed learning-to-grow ArtiHippo method.    
%     % Accuracy and Average Forgetting on each task. The plot shows that the Layer Normalization layers undergo the least forgetting. However, the Projection layers form the MHSA blocks gives the best accuracy. This suggests that the projection layers are the more suitable for lifelong learningif the weights are preserved.
%     See text for details.}
%     \label{fig:acc-vs-forgetting} \vspace{-3mm}
% \end{figure}

The MHSA realizes the dot-product self-attention in $h$ different $d_h$-dimensional sub-space ($d = h\times d_h$). Each sequence can be rewritten in the multi-head form accordingly, e.g., $x_{h, L, d_h}$. Let $\bar{X}_{L,d}=\text{LN}_1(X_{L,d})$, and the Query/Key/Value be $Q_{L,d}=\text{Linear}(\bar{X}_{L,d})\triangleq Q_{h, L, d_h}$, $K_{h,L,d_h}$ and $V_{h,L,d_h}$. The MHSA is defined by,
\begin{equation}
\small 
    \text{MHSA}(Q, K, V) = \text{Softmax}\left(\frac{Q\cdot K^T}{\sqrt{d_h}}\right)V\triangleq U_{L, (h,d_h)} \label{eq:mhsa}
\end{equation}
where $\text{Softmax}()$ is applied along the last dimension of the attention matrix. The projection layer $\text{Proj}(U_{L, (h,d_h)})$ is used to fuse the multi-head information. 

\begin{wraptable} {r}{0.5\textwidth}%[t]
    \vspace{-3.7mm}
    \centering
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{l|l|c|c}
        \toprule
        Index & Finetuned Component &  Avg. Accuracy & Avg. Forgetting \\
        \midrule
        1 & $\text{LN}_1$ + $\text{LN}_2$ & 81.76 & 21.24 \\
        \midrule
        2 & $\text{MLP}^{\text{down}}$ + $\text{MLP}^{\text{up}}$ + $\text{LN}_2$ & 84.20 & 44.76 \\
        3 & $\text{MLP}^{\text{down}}$ & 83.66 & 37.99 \\
        4 & $\text{LN}_2$ & 80.04 & 16.35 \\
        \midrule
        5 & MHSA + $\text{LN}_1$ & 85.26 & 54.38 \\
        6 & LN$_1$ & 81.18 & 19.04 \\
        7 & Query & 81.57 & 19.69 \\
        8 & Key & 81.56 & 19.19 \\
        9 & Query+Key & 81.49 & 31.10 \\
        10 & Value & 84.99 & 37.58 \\
        11 & Proj  (ArtiHippo) & 85.11 & 30.50 \\
        \midrule
        % \multicolumn{2}{c|}{Adapt the Last MHSA Block Only} & 72.55 & - \\
        \multicolumn{2}{c|}{Train a Classifier w/ Frozen Backbone} & 70.78 & - \\
        \bottomrule
    \end{tabular}
    }
    \caption{Ablation study on identifying the ArtiHippo in a Transformer block (Eqns.~\ref{eq:transformer} and~\ref{eq:mhsa}). Starting from the first task model from the VDD benchmark, i.e., ImageNet-pretrained ViT, we finetune a given component with other components frozen continually and sequentially across the remaining 9 task in a predefined order, and train the task head classifier from scratch for each individual task. Average Accuracy (Eqn.~\ref{eq:avg_acc}) and Average Forgetting (Eqn.~\ref{eq:avg_forgetting}) are computed in comparison. The projection layer in the MHSA block is identified and selected as ArtiHippo. The last row shows the result of a conventional transfer learning setting in which the head classifier only is trained.   %Finetuning the MHSA block alone achieves the highest average accuracy, while finetuning the Projection layer from the MHSA block achieves almost equivalent performance, with much lower parameters.
    % See text for details.
    }
    \label{tab:acc_vs_forgetting} \vspace{-3mm}
\end{wraptable}

For resilient task-incremental lifelong learning using ViTs, \textbf{our very first step is to investigate whether there is a simple yet expressive ``sweet spot" in the Transformer block that plays the functional role of  Hippocampi in the human brain (i.e., ArtiHippo)}, that is converting short-term streaming task memory into long-term memory to support lifelong learning without catastrophic forgetting. The proposed identification process is straightforward. Without introducing any modules handling forgetting, we compare both the task-to-task forward transferrability and the sequential forgetting of different components in a Transformer block. Our intuition is that a desirable ArtiHippo component must enable strong transferrability with manageable forgetting. 

To that end, we use the 10 tasks in the VDD benchmark~\citep{vdd}. We first compare the transferrability of the ViT trained with the first task, ImageNet to the remaining 9 tasks in a pairwise task-to-task manner and compute the average Top-1 accuracy on the 9 tasks. Then, we start with the ImageNet-trained ViT, and train it on the remaining 9 tasks continually and sequentially in a predefined order with the average forgetting~\citep{riemannian-walk} on the first 9 tasks (including ImageNet) compared. %(see the x-axis of Figure~\ref{fig:acc-vs-forgetting}). 
As shown in Table~\ref{tab:acc_vs_forgetting}, we compare 11 components or composite components across all blocks in the ImageNet-pretrained ViT. 

Denote by $T_1, T_2, \cdots, T_{N}$ a sequence of $N$ tasks (e.g., $N=10$ in the VDD benchmark). A model consists a feature backbone and a task head classifier. 
Let $f_{T_{n|1}}$ be the backbone trained for the task $n$ ($n=2,\cdots N$) with weights warmped-up from the model of task $1$, and $C_n$ the learned head classifier from scratch. The average transfer learning accuracy of 
 the first task model to the remaining $N-1$ tasks is defined by, 
\begin{equation}
    A_{N} = \frac{1}{N-1}\sum_{n=2}^N \text{Accuracy}(T_n; f_{T_{n|1}, C_n}), \label{eq:avg_acc}
\end{equation}
where $\text{Accuracy}()$ uses the Top-1 accuracy in classification.

Let $f_{T_{1:n}}$ be the backbone trained sequentially after task $T_n$ and and $C_n$ the head classifier trained for task $T_n$. Denote by $a_{n,i}=\text{Accuracy}(T_i; f_{T_{1:n}}, C_i)$, the accuracy on the task $i$ using the backbone that has been trained on tasks from $1$ to $n$ ($i<n$).
The average forgetting on the first $N-1$ tasks is defined by,
\begin{equation}
    \mathbb{F}_N = \frac{1}{N-1}\sum_{n=1}^{N-1} \left(\max_{j\in [n, N-1]} a_{j,n} - a_{N,n}\right), \label{eq:avg_forgetting}
\end{equation}




From Table~\ref{tab:acc_vs_forgetting}, we have a few observations that lead us to identify and select the projection layer as ArtiHippo: % on which the proposed lifelong learning method is built: 

(i) Continually finetuning the entire MHSA block (i.e., MHSA+LN$_1$) obtains the best average accuracy, which has been observed in~\citep{touvron2022three} in terms of finetuning ImageNet-pretained ViTs on downstream tasks. However, \citep{touvron2022three} does not consider lifelong learning settings, and as shown here finetuning the entire MHSA block incurs the highest average forgetting, which means that it is task specific. 

(ii)  Continually finetuning the entire FFN block (i.e., MLP$^{\text{down}}$+MLP$^{\text{up}}$+LN$_2$) has a similar effect as finetuning the entire MHSA block. In the literature,  the Vision Mixture of Expert framework~\citep{vision-moe} where an expert is formed by an entire MLP block takes advantage of the high average performance preservation. 

\begin{wrapfigure}{r}{0.514\textwidth} \vspace{-5mm}
    \centering
    \includegraphics[width=0.514\textwidth]{figures/nas.pdf}
    \caption{ Illustration of ArtiHippo growing via NAS using the four learning-to-grow operations in lifelong learning. The NAS is built on the single-path one-shot (SPOS) formulation~\citep{spos}. It consists of two components: Supernet construction and training, and Evolutionary search. Given the current model, the ArtiHippo in a MHSA is represented by a Mixture of Experts %each of which is an on-site variant of the linear projection layer 
    and the associated mean class-token (e.g., $\mu_1$ and $\mu_2$). Then, the supernet is constructed using the four operations (illustrated by the dotted arrows in blue). We train the supernet using SPOS with a proposed task-similarity oriented hierarchical exploration-exploitation based sampling method. The task similarity between a new task and old tasks is computed by the normalized cosine similarity between the mean class-tokens, e.g., $(\mu_{3|1}', \mu_1)$ and $(\mu_{3|2}', \mu_2)$, (shown by the arrows in red). After the supernet is trained, we use evolutionary search using the same hierarchical exploration-exploitation sampling to seek the target network for the new task. After the evolutionary search, the long-term memory of the new task is maintained (e.g., the newly added $\text{Adapt}_3$ and $\mu_3$). 
    % The graphical model used to convert the scalar similarity score into a categorical distribution over operation search space in NAS. The search space is constructed by reusing and adapting the existing experts, and adding a new and a skip expert. $\psi$ denotes the probability of sampling an expert, and $\rho_e$ denotes the retnetion probability for expert $e$.
    See text for details.}\label{fig:nas}\vspace{-8mm}
\end{wrapfigure}



(iii)  In lifelong learning scenarios, maintaining either the entire MHSA block or the entire FFN block could address the catastrophic forgetting, but at the expense of high model complexity and heavy computational cost in both learning and inference. 
% {\color{blue}It also becomes less biologically plausible.} 

(iv) The final projection layer and the Value layer in the MHSA block, which have been overlooked, can maintain high average accuracy (as well as manageable average forgetting, to be elaborated). It is also much more ``affordable" to maintain it in lifelong learning, especially with respect to the four basic growing operations (skip, reuse, adapt and new). Intuitively, the final projection layer is used to fuse multi-head outputs from the self-attention module. In ViTs, the self-attention module is used to mix/fuse tokens spatially and it has been observed  in MetaFormers~\citep{yu2022metaformer,yu2022metaformerbaseline} that simple local average pooling and even random mixing can perform well. So, it makes sense to keep the self-attention module frozen from the first task (at worst it can play the role of a random mixing operation for a new task) and maintain the projection layer to fuse the outputs. However, the Value layer is implemented as a parallel computation along with the Key and Query, which makes it inefficient to incorporate into the Mixture of Experts framework.
    
    % \item Finetuning MHSA+LN$_1$ incurs the most forgetting, followed by MLP$^{\text{down}}$+MLP$^{\text{up}}$+LN$_2$. This indicates that these mechanisms are highly task specific. However, the level of forgetting incurred by each of the sub - components of the MHSA and MLP blocks can be different. This can be seen in Table \ref{tab:acc_vs_forgetting}. The Layer Normalization (LN) layer in MLP incurrs the least forgetting, followed by the LN layer in the MHSA block. This indicates that these layers are task agnostic. Furthermore, Key and Query in isolation are also task agnostic (evidenced by low average forgetting), and training them together is task specific (higher average forgetting). In the MHSA, the the Value and the Projection layers show high average forgetting, indicating that they are task specific.

In sum, due to the strong forward transfer ability, maintaining simplicity and for less invasive implementation under the learn-to-grow settings in practice, we select the Projection layer (instead of the Value layer) in the MHSA block as ArtiHippo to develop our proposed long-term task-similarity-oriented memory based lifelong learning. We experimentally show that exploiting the Value layer as the ArtiHippo leads to comparable performance. 



\subsection{Learning to Grow ArtiHippo Continually}
\label{sec:grow_artihippo}\vspace{-2mm}
This section presents details of learning to grow ArtiHippo based on NAS. As illustrated in Figure~\ref{fig:nas},  for a new task $t$ given the network learned for the first $t-1$ tasks, it consists of three components: the Supernet construction (the parameter space of growing ArtiHippo), the Supernet training (the parameter estimation of growing ArtiHippo), and the target network selection and finetuning (the consolidation of the ArtiHippo for the task $t$).
% {\color{red}Our proposed method to learn to grow ArtiHippo for lifelong learning uses a task-similarity oriented sampling based Single Path One Shot Neural Architecture Search, illustrated in} Figure \ref{fig:nas}. In this section, we present details of supernet construction, supernet training and evolutionary search for target {\color{red} architecture selection and fientuning for task $t$ given the architecture learned for tasks $t-1$}. 


\vspace{-3mm}
\subsubsection{Supernet Construction}\label{sec:supernet_construction}\vspace{-2mm}

We start with a vanilla $D$-layer ViT model (e.g., the 12-layer ViT-Base)~\citep{vit} and train it on the first task (e.g., ImageNet in the VDD benchmark~\citep{vdd}) following the conventional recipe. 
The proposed ArtiHippo is represented by a mixture of experts, similar in spirit to~\citep{vision-moe}. After the first task, the ArtiHippo at the $l$-th layer in the ViT model consists of a single expert which is defined by a tuple, $E^{l, 1}=(P^{l, 1}, \mu^{l, 1})$,  where $P^{l, 1}$ is the projection layer and $\mu^{l, 1}\in R^d$ is the associated mean class-token pooled from the training dataset after the model is trained. Without loss of generality, we consider how the growing space of ArtiHippo is constructed at a single layer and assume the current ArtiHippo consists of two experts, $\{E^{l, 1}, E^{l, 2}\}$ (the left in Figure~\ref{fig:nas}). 

Inspired by the L2G method~\citep{learn-to-grow}, we utilize four operations in the Supernet construction: 
\begin{itemize}[leftmargin=*]
\itemsep0em
    \item The {\tt skip} operation skips the entire MHSA block (i.e., the hard version of the drop-path method that is widely used in training ViT models), which encourages the adaptivity accounting for the diverse nature of tasks. 
    \item The {\tt reuse} operation exploits the projection layer from an old task for the new task unchanged (including associated mean class-token), which will help task synergies in learning. 
    \item The {\tt adapt} operation introduces a new lightweight layer on top of the projection layer of an old task, implemented by a MLP with one squeezing hidden layer, and a new mean class-token computed at the added adapt MLP layer.
    \item The {\tt new} operation adds a new projection layer and a mean class-token, enabling the model to handle corner cases and novel situations.
\end{itemize}
The bottom of Figure \ref{fig:nas} shows the growing space. The Supernet is constructed by {\tt reusing} and {\tt adapting} each existing expert at layer $l$, and adding a {\tt new} expert and a {\tt skip} expert. The newly added {\tt adapt} MLPs and projection layers will be trained from scratch using the data of a new task only. %We refer to the existing experts, the {\tt adapt}, new and skip experts as the \textit{operations} in the NAS search space.



\textbf{How to {\tt Adapt} in a sustainable way?} The proposed {\tt Adapt} operation will effectively increase the depth of the network in a plain way. In the worst case, if too many tasks use {\tt Adapt} on top of each other, we will end up stacking too many MLP layers together. This may lead to unstable training due to gradient vanishing and exploding. Shortcut connections~\citep{resnet} have been shown to alleviate the gradient vanishing and exploding  problems, making it possible to train deeper networks. Due to the residual architecture for the adapter layers, the training can ignore an adapter if needed, and leads to a better performance. However, in the lifelong learning setup, where subsequent tasks might have different distributions, the search process might disproportionately encourage {\tt Adapt} operations because of this ability. To counter this, we propose a hybrid {\tt Adapter} which acts as a plain 2-layer MLP during Supernet training and target network selection, and a residual MLP during finetuning. With this hybrid adapter, as we shall show in the ablation study (Table~\ref{tab:adapter_ablation}), much more compact models can be learned with negligible loss in accuracy.

% {\tt Adapt} can be thought of as a conditional renewing of the previous task. This form of conditional renewing can be treated as a transformation of the original output, like Adapter-BERT~\citep{adapter-bert}. Adapter-BERT proposed bottleneck layers called Adapter modules for Transformers, which consists of a 2 feedforward layers and a residual connections inserted after Attention and FFN blocks. They show that this form of modular architecture enables efficient transfer of the base model and achieves very high performance on downstream tasks.

 

% We confirm the effectiveness of the hybrid adapter through ablation experiments. We choose a lifelong learningsetup with 2 tasks: ImageNet and Omniglot. This setup is chosen because the Omniglot dataset presents two major challenges for a lifelong learningsystem. First, Omniglot is a few shot dataset, which will need a lifelong learningmethod using a large ViT to find a compact model. Second, Omniglot has a significantly different data distribution than ImageNet, making it necessary to add additional parameters. These two conflicting properties make this setup ideal for testing. Table \ref{tab:ablations-results} shows that using a residual adapter during Search and Finetuning achieves an accuracy of $0.8232$, which is higher than $0.7816$ achieved by plain adapter. This comes at a higher increase in the number of parameters ($5.2\%$ using residual adapters vs. $3.47\%$ using plain adapters). Using plain adapters during search and residual adapters during finetuning achieves the best of both, achieving an accuracy of $0.8218$. Note that we do not use any data augmentations for the ablation experiments, but use a Drop Path rate of 0.25 to prevent overfitting.

% With the constructed Supernet for a new task in lifelong learning, we build on the single-path one-shot (SPOS) NAS method~\citep{spos} to train the Supernet and to select the target network for the new task.

% \vspace{-3mm}
% \subsubsection{Supernet Training}\vspace{-2mm}
% SPOS NAS \citep{spos} samples a single-path sub-network from the Supernet at each training iteration by uniformly sampling an operation at each layer, which is a pure exploration strategy. In lifelong learning, the pure exploration strategy is not desirable since it ignores inter-task similarities. We propose a hierarchical exploration-exploitation based sampling method using task similarities.

% For task $t$, let $\mathbb{E}^l$ be the set of learned Experts at the $l$-the layer, learnt till task $t-1$. The  Supernet is constructed via the four operations for each expert $e\in \mathbb{E}^l$ in the ArtiHippo. For a new task, we first sample an expert $e$ to use from a categorical distribution formed over the experts, followed by sampling which operation to use. Figure~\ref{fig:nas-sampling} illustrates the proposed sampling method. For each candidate expert $e\in \mathbb{E}^l$, we first compute the mean class-token for the $t$-task, $\hat{\mu}_e^{t}$ using the current model, and compute the task similarity between the $t$-th task and the Expert $e$,

% \begin{wrapfigure}{r}{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/sampling_process_v3.pdf}
    \caption{
    Illustration of the proposed exploration-exploitation sampling strategy used in facilitating the SPOS NAS~\citep{spos} for lifelong learning. It harnesses the best of the vanilla  pure exploration strategy (left) and the proposed exploitation strategy (right) using a simple epoch-wise scheduling. See text for details.
    % Comparison of the two sampling methods used in training the Supernet and searching the target network based on SPOS when learning a new task. \textbf{Bottom Left}: Uniform sampling scheme (Exploration) used in the original SPOS NAS \citep{spos}. $|\mathcal{O}|$ denotes the number of operations in the NAS search space. \textbf{Right}: The proposed hierarchical task-similarity-oriented sampling method (Exploitation). The hierarchical sampling scheme assigns probabilities proportional to the task similarities between the current task and the task for which an expert was learned. \textbf{Top Left}: To balance exploration and exploitation, at the start of each supernet training epoch, we choose exploration with a probability $\epsilon$, and exploitation with probability $1 - \epsilon$ (Exploration-Exploitation).
    }
    \label{fig:nas-sampling} \vspace{-3mm}
\end{figure}


\vspace{-2mm}
\subsubsection{Supernet Training}\vspace{-2mm}
To train the Supernet constructed for a new task $t$, many NAS methods can be used such as the DARTS~\citep{darts} and the more recent single-path one-shot (SPOS) NAS method~\citep{spos}. We build on the SPOS method due to its efficiency. The basic idea of SPOS is to sample a single-path sub-network from the Supernet by sampling an expert at every layer in each iteration (mini-batch) of training. One key aspect is the sampling strategy. The vanilla SPOS method uses uniform sampling (i.e., the \textit{pure exploration} strategy, as illustrated in the left of Figure~\ref{fig:nas-sampling}), which has the potential of traversing all possible realizations of the mixture of experts of the ArtiHippo in the long run, but may not be desirable (or sufficiently effective) in a lifelong learning setup because it ignores inter-task similarities/synergies.

To overcome this, we propose an exploitation strategy (as illustrated in the right of Figure~\ref{fig:nas-sampling}), which utilizes a hierarchical sampling method that forms the categorical distribution over the operations in the search space explicitly based on task similarities. %For a new task, we first sample an expert $e$ to use from a categorical distribution formed over the experts (using the same similarity scores), followed by sampling which operation to use. As illustrated in Figure~\ref{fig:nas-sampling} the this hierarchical sampling method can convert task similarities into a categorical distribution over the operations, which acts as a better, semantically meaningful prior instead of a uniform distribution. 
We first present details on the task-similarity oriented sampling in this section. Then we show a simple exploration-exploitation integration strategy to harness the best of the two in Sec.~\ref{sec:balancing}. 

% Let $\mathbb{E}^l$ be the set of Experts at the $l$-the layer learned till task $t-1$. For task $t$, the  Supernet is constructed as defined in Section \ref{sec:supernet_construction}. Following SPOS NAS \citep{spos}, we sample a single-path sub-network from the Supernet by sampling an expert at every layer. However, SPOS samples each operation uniformly, which may not be desirable in a lifelong learning setup because it ignores inter-task similarities. 

\textbf{Task-Similarity Oriented Sampling}: Let $\mathbb{E}^l$ be the set of Experts at the $l$-the layer learned till task $t-1$. For each candidate expert $e\in \mathbb{E}^l$, we first compute the mean class-token for the $t$-task, $\hat{\mu}_e^{t}$ using the current model, and compute the task similarity between the $t$-th task and the Expert $e$,
\begin{equation}
    S_e(t) = \text{NormCosine}(\hat{\mu}_e^{t}, \mu_e),
\end{equation}
where NormCosine$(\cdot, \cdot)$ is the Normalized Cosine Similarity, which is calculated by scaling the Cosine Similarity score between $-1$ and $1$ using the minimum and the maximum scores from all the experts in all the MHSA blocks of the ViT. This normalization is necessary to increase the difference in magnitudes of the similarities between tasks, which results in better Expert sampling distributions during the sampling process in our experiments. %{\color{blue}\st{(see the Appendix for comparisons)}}. %Figure \ref{fig:similarities} shows the difference between the similarity values and the Probability Mass Functions obtained using Cosine Similarity and Normalized Cosine Similarity.


The categorical distribution is then computed via Softmax across all the scores of all the Experts at a layer. The probability of sampling a candidate Expert $e\in \mathbb{E}^l$ is defined by 
$\psi_e = \frac{\exp(S_e(t))}{\sum_{e'\in \mathbb{E}^l} \exp(S_{e'}(t))}$. 
With an Expert $e$ sampled (with a probability $\psi_e$), we further compute its retention Bernoulli probability via a Sigmoid transformation of the task similarity score defined by $\rho_e = \frac{1}{1+\exp(-S_e(t))}$.



If the Expert $e$ is retained (with a probability $\rho_e$), we further use the same Bernoulli probability to sample the two operations, {\tt Reuse} with probability $\rho_e$ and {\tt Adapt} with probability $1-\rho_e$. If the Expert $e$ is ignored (with probability $1-\rho_e$), we randomly sample the two operations: {\tt Skip} and {\tt New} with probability 0.5. 

% The sampling probabilities of the four growing operations are defined by, 
% \begin{align}
%     p^{\tt Reuse}_e = \psi_e\rho_e^2, \quad 
%     p^{\tt Adapt}_e = \psi_e\rho_e(1 - \rho_e), \quad
%     p^{\tt New}  = p^{\tt Skip} = 0.5\left(\sum_{e'\in \mathbb{E}^l}\psi_{e'}(1 - \rho_{e'})\right)
% \end{align}
% It is straightforward to verify the validity of this PMF for the Categorical Distribution over operations in the search space.



% \vspace{-2mm}
\subsubsection{Target Network Selection and Finetuning}\vspace{-2mm}
\label{sec:target-network-selection}
After the Supernet is trained, we adopt the same evolutionary search used in the SPOS method~\citep{spos} based on the proposed hierarchical sampling strategy. %\textcolor{red}{briefly summarizing the steps in the evolutionary search}
The evolutionary search is performed on the validation set to select the path which gives the best validation accuracy. 

After the target network for a new task is selected, we retrain the newly added layers by the {\tt New}  and {\tt Adapt} operations from scratch (random initialization), rather than keeping or warming-up from the weights from the Supernet training. This is based on the observations in network pruning that it is the neural architecture topology that matters and that the warm-up weights may not need to be preserved to ensure good performance on the target dataset~\citep{liu2018rethinking}. We empirically observe this in our experiments as well.    

% \vspace{-2mm}
\subsubsection{Balancing Exploration and Exploitation:}\label{sec:balancing}\vspace{-2mm}
% The proposed hierarchical task-similarity-oriented sampling is an exploitation based strategy in training the Supernet and in the evolutionary search. However, the Supernet training and evolutionary search may benefit from exploration of the entire search space, as done in the vanilla SPOS NAS. 
As illustrated in Figure~\ref{fig:nas-sampling}, to harness the best of the vanilla pure exploration strategy and the proposed exploitation strategy, we apply epoch-wise exploration and exploitation sampling for simplicity. At the beginning of an epoch in the Supernet training, we choose the pure exploration strategy with a probability of $\epsilon_1$ (e.g., 0.3), and the hierarchical sampling strategy with  a probability of $1-\epsilon_1$. Similarly, when generating the initial population during the evolutionary search, we draw a candidate target network from a uniform distribution over the operations with a probability of $\epsilon_2$, and from the hierarchical sampling process with a probability of $1-\epsilon_2$, respectively. In practice, we set $\epsilon_2 > \epsilon_1$ (e.g., $\epsilon_2=0.5$) to encourage more exploration during the evolutionary search, while encouraging more exploitation for faster learning in the Supernet training. As we shall show in the experiment, this exploration-exploitation strategy achieves higher Average Accuracy and results in a lower parameter increase than pure exploration. %Section \ref{sec:ablations} provides ablation studies showing the effect of sampling strategies on the average accuracy on the VDD dataset.

\subsection{Integrating our ArtiHippo with the Learn-to-Prompt Method}
\label{sec:combining-with-prompts}\vspace{-2mm}
As aforementioned, the learn-to-prompt methods~\citep{learning-to-prompt,dualprompt,s-prompts,dytox} are complementary to our proposed learning-to-grow ArtiHippo, we propose a simple method of harnessing the best of the two for resilent lifelong learning. 
% Prompting (add citations) is an efficient method for learning task-specific parameters by leveraging the stability and generalization ability of a pretrained backbone model. This method is especially efficient when the distribution of tasks is similar to the base task (Table \ref{tab:vdd-results}, rows 3 and 4). This property can be leveraged to complenent the proposed approach. 
At the beginning of the Supernet training, a task-specific classification token is learned using the ImageNet backbone (similar to S-Prompts~\citep{s-prompts}). Then, instead of using the {\tt cls} token from the ImageNet task, we used the learned task token during NAS. When finetuning the learned architecture, we first train the task token using the fixed ImageNet backbone, and then use this trained token to train the architecture components. We show that this leads to further improvement. %, and verifies that prompting-based methods and our proposed methods are complementary to each other.




\section{Experiments}\vspace{-2mm}
In this section, we test the proposed method on two benchmarks and compare with the prior art. We evaluate our method in the task-incremental setting, where each task contains a disjoint set of classes and/or domains and task index is available in inference. The proposed method obtains better performance than the prior art in comparisons.  
\textbf{Our PyTorch source code will be released.} 
Due to space limitations, we provide the implementation details in the Appendix. We use 1 Nvidia Quadro RTX 8000 GPU for all experiments.

\textbf{Data and Metrics}:
We evaluate our approach on the Visual Domain Decathlon (VDD) dataset~\citep{vdd} and a sequence of 5 Datasets introduced in~\citep{adversarial-continual-learning}. Each individual dataset in these two datasets is treated as separate a task with no overlap. The VDD dataset is a challenging benchmark for lifelong learning because of the large variations in tasks as well as small number of samples in many tasks, which makes it a favorable benchmark for lifelong learning. \textit{Details of the two benchmarks are provided in the Appendix.}
% The VDD dataset consists of 10 tasks: ImageNet12~\citep{imagenet}, CIFAR100~\citep{cifar}, FGVC-Aircraft~\citep{aircraft}, Daimler Pedestrian Classification~\citep{daimlerpedcls}, Omniglot~\citep{omniglot}, SVHN~\citep{svhn}, VGG Flowers~\citep{vgg-flowers}, German Traffic Signs~\citep{gtsrb}, UCF101 Dynamic Images~\citep{ucf1,ucf2}, and Describable Textures~\citep{dtd}.  5-Dataset consists of a seqence of 5 tasks: CIFAR10~\citep{cifar}, MNIST, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST, and SVHN~\citep{svhn}.
% \st{The average forgetting} (Eqn. \ref{eq:avg_forgetting}) \st{is used to evaluate the performance of our approach.} 
Since catastrophic forgetting is fully addressed by our method, we evaluate the the performance of our method using the average accuracy defined by,
\begin{equation}
\mathbf{A}_N = \frac{1}{N}\sum_{i=n}^N a_{N,n}, \label{eq:avg_accuracy_lll}    
\end{equation} 
where $N$ is the total number of tasks, and $a_{n,i}=\text{Accuracy}(T_i; f_{T_{1:n}}, C_i)$ (see Eqn.~\ref{eq:avg_acc}). 
% We use the average accuracy across tasks as our evaluation metric, given by
% \begin{equation}
%     A = \frac{1}{T}\sum_{t=1}^Ta_t
% \end{equation}
% where $T$ is the total number of tasks, and $a_t$ is the accuracy for task $t$.


\textbf{Baselines:} %On the VDD benchmark, we compare with two prior dynamic model based approaches: Residual Adapters~\citep{vdd} and the Learning-to-Grow (L2G) method~\citep{learn-to-grow}. Both are built on ResNets~\citep{resnet}. As aforementioned, the proposed method is the first to evaluate lifelong learning with ViTs on the VDD, to our knowledge. 
% on the validation data from VDD dataset. For fair comparison, we use the same task sequence used in Learn to Grow: ImageNet, CIFAR100, SVHN, UCF101, Omniglot, GTSRB, Pedestrian Classification, VGG-Flowers, Aircraft, and Describable Textures. Results with different task sequences are provided in the ablation studies. 
On the VDD benchmark, we compare with the Learning-to-Prompt (L2P) method~\citep{learning-to-prompt}, which comes closest in terms of evaluating lifelong learningwith ViTs on large-scale and diverse tasks and S-Prompts~\citep{s-prompts}. We use our own implementation for evaluating L2P and S-Prompts, and modify the methods to work in the task-incremental setting for fair comparison with our approach, denoted as L2P$^\dagger$ and S-Prompts$^\dagger$ in Table~\ref{tab:vdd-results}. We provide the implementation details in the Appendix.% The L2P method is evaluated on the 5-Datasets benchmark~\citep{adversarial-continual-learning}. %, which consists of 5 easy tasks in sequence. Except of not-MNIST, we report the accuracy on the official test set. For not-MNIST, we closely match the setting of L2P and construct the test set by randomly sampling 20\% of the data from the not-MNIST Small dataset.

% We base the implementation of the Vision Transfromer on the {\tt timm}~\citep{timm} library. We use a patch size of $8\times8$ and the Base architecture from~\citep{vit} (ViT-B/8). For all the experiments, we use an image size of $72\times72$. For each task, we train the supernet for 310 epochs and run the evolutionary search for 20 epochs. The details of the evolutionary search algorithm can be found in~\citep{spos}. We use a population size of 50, mutation probability of 0.1. The final architecture found using  the evolutionary search is finetuned for 35 epochs with a learning rate of 0.001 and a Cosine Learning Rate scheduler. For each epoch, a minimum of 30 batches are drawn, with a batch size of 512. We use data augmentations during supernet traning and finetuning. A full list of augmentations are provided in the Appendix. We use 10\% and 15\% of the training data from the tasks in VDD dataset and 5-Dataset respectively as validation set.

% \textbf{ImageNet training}: We train a base Vision Transformer on ImageNet, which is used as the first task. We use the ImageNet data provided in the VDD dataset to train the base Vision Transformer model, where images are scaled such that the shortest side is 72 pixels. We initialize the weights from the ViT-B/8 trained on the full resolution ImageNet dataset (224x224), and finetune it for 30 epochs on the downsized version of ImageNet. We use SGD with Momentum of 0.9 and learning rate of 0.01 with Cosine learning rate scheduler and a label smoothing of 0.1. We use a dropout rate of 0.1. When training on the ImageNet dataset, we take a random crop from a randomly scaled image between 8\% and 100\% of the original scale with ascspect ratio between 0.75 to 1.33, and use a color jitter of 0.4. The final crop is resized to 72x72. During testing, we take a center crop of 72x72 from an image scaled with the shortest side to 80 pixels.

% \textbf{Downstream task training}: Each task in the VDD dataset and 5-Dataset is applied a different data augmentation scheme. The augmentation schemes used for all the datasets have been described in the Appendix. At test time, the images are resized to 72x72 with bicubic interpolation. We also use label smoothing of $0.1$ when training the supernet and finetuning the final architecture. Training of all the tasks use the Adam optimizer~\citep{adam} with a learning rate of 0.001 and Cosine Learning Rate schedule. The supernet os trained for 310 epochs, and the final model found using the search is finetuned for 35 epochs.

\begin{table} %[h]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            {S-Prompts$^\dagger$} (p=1/task) & 82.65 & 86.69 & 69.89 & 51.95 & 49.55 & 94.07 & 99.25 & \textbf{90.39} & 35.03 & \textbf{56.19} & $71.57 \pm 0.31$ \\
            {L2P$^\dagger$} (p=12/task) & 82.65 & 89.06 & 81.43 & 63.99 & 62.86 & 98.21 & 99.77 & \textbf{94.58} & 45.00 & \textbf{60.78} & $77.83 \pm 0.28$ \\
            % \cline{2-14}
            \midrule
            ArtiHippo (Uniform Sampling) & 82.65 & {85.50} & {95.63} & {74.09} & {82.53} & \textbf{99.93} & \textbf{99.85} & {79.31} & {41.62} & {41.21} & {$78.23 \pm 0.93$} \\ 
            % \cline{2-14}
             ArtiHippo (Hierarchical Sampling) & 82.65 & \textbf{90.92} & \textbf{95.93} & \textbf{77.08} & \textbf{84.14} & \textbf{99.92} & {99.80} & {77.76} & \textbf{47.11} & {45.79} & {$\boldsymbol{80.11} \pm \boldsymbol{1.17}$} \\ 
            % \cline{3-14}
            ArtiHippo (Hierarchical Sampling + p=1/task) & 82.65 & \textbf{90.99} & \textbf{95.87} & \textbf{78.98} & \textbf{86.12} & 99.91 & \textbf{99.89} & 88.20 & \textbf{45.86} & 51.31 & $\boldsymbol{81.98 \pm 0.95}$ \\ 
        \bottomrule
    \end{tabular}}
    \vspace{0.3em}
    \caption{Results on the VDD benchmark~\citep{vdd}. Our method shows clear improvements over the previous approaches. The proposed hierarchical sampling performs better than uniform sampling. All the results from our experiments are averaged over 3 different seeds. The 2 highest accuracies per task have been highlighted. All the methods use the same ViT-B/8 backbone containing 86.04M parameters and having 7.11G FLOPs}% The average accuracy (Eqn.~\ref{eq:avg_accuracy_lll}) are not directly comparable due to different models used. Due to the lack of results on the VDD by ViT based lifelong learning approaches in the literature, we provide the Upper Bound performance as reference, which are obtained by finetuning 13 choices of components of the ImageNet-pretrained ViT (Table~\ref{tab:acc_vs_forgetting}) under a 2-task transfer learning setting, and then choosing the variant which gives the maximum accuracy on a task.  This represents the maximum attainable accuracy for a task under our experimental settings. The proposed lifelong learning method can achieve performance close to the Upper Bound, showing its effectiveness. The task order is based on the one used in the L2G method. \color{red}{$^*$ The results averaged over 3 different seeds.}}
    \label{tab:vdd-results} \vspace{-3mm}
\end{table}

\subsection{Results on the VDD Benchmark}\vspace{-2mm}
Table \ref{tab:vdd-results} shows the results and comparisons.  Our method shows consistent performance improvement across tasks compared with ViT based S-Prompts$^\dagger$ and L2P$^\dagger$ methods. The gains are particularly significant for tasks with a significantly different distribution than the base ImageNet task (Omniglot, SVHN, UCF101). These results show that introducing new parameters to the base model, as opposed to freezing it and learning external prompts, can improve the performance significantly, which justifies our motivation of seeking more integrative memory mechanisms (Section \ref{sec:intro}). With the same ViT backbone, our method shows significant improvements over the prompting-based lifelong learning approaches proposed for ViTs (S-Prompts and L2P). However, for tasks which are similar to the base task and have very less data (VGG-Flowers, DTD), the performance of S-Prompts and L2P are better. Thus, prompt-based methods and our growing-based method are complementary and combining them could lead to even better performance. Section \ref{sec:combining-with-prompts} describes a preliminary approach to combine the two approaches, and Table \ref{tab:vdd-results} last row shows that this indeed improves the performance of the proposed ArtiHippo. A more comprehensive integration of ArtiHippo with prompt based approaches is left for future work.

As shown in Table~\ref{tab:vdd-results-full} in the Appendix, prompting-based lifelong learning approaches (S-Prompts$^\dagger$ and L2P$^\dagger$) do not perform better than L2G and Adapter, even though they use a better backbone model (ViT-B/8 vs. ResNet26 in L2G). 
% We thus set a benchmark on the VDD using ViTs. The gains are particularly significant for tasks with a significantly different distribution than the base ImageNet task (Omniglot, SVHN, UCF101). 
% {\color{red}These results show that introducing new parameters to the base model, as opposed to freezing it and learning external prompts, can improve the performance significantly, which justifies our motivation of seeking more integrative memory mechanisms (~\cref{sec:intro}). However, for tasks which are similar to the base task and have very less data (VGG-Flowers, DTD), the performance of S-Prompts and L2P is better. Thus, prompt-based methods and our growing-based method are complementary and combining them could lead to even better performance, which is left for future study.} % The effectiveness of the proposed method is further justified by the comparisons with the empirical Upper Bound. 
This shows that the gains of our method are because of our parameter growing approach, together with the initial backbone model. The proposed task-similarity oriented sampling procedure outperforms uniform random sampling with lesser number of added parameters (Figure \ref{fig:epochs-vs-acc} right). This shows that the proposed task-similarity oriented sampling coupled with SPOS NAS~\citep{spos} is effective for lifelong learningwith ViTs.
% The effectiveness of the proposed method is further justified by the comparisons with the empirical Upper Bound.  










% Adding the scores for ER, GDumb, BiC, DER, C2L from the L2P paper as is for now. 
% Maybe move the full table with standard deviations to supplementary? will save space.
% Adding results with buffer size 5/class will take up too much space
% \begin{strip}\centering
% \vspace{-1em}

\begin{wraptable} {r}{0.45\textwidth}%[t]
    \vspace{-10mm}
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{Num. Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts$^\dagger$ & 1 & $91.40$ \\
        L2P$^\dagger$ & 12 & $93.76 \pm 0.25$ \\
        ArtiHippo (Projection) & - & $\boldsymbol{96.52 \pm 0.4}$ \\
         % & & $^{\times}$Attn Head & 0 & - & 0 & $97.34$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.3em}
    \caption{Results on the 5-Dataset benchmark~\citep{adversarial-continual-learning}. 
    % The results using L2P, Learning without Forgetting (LwF), and Elastic Weight Consolidation (EWC) have been reported from~\citep{learning-to-prompt}. 
    % All the methods use the same ViT-B/8 backbone pretained on the ImageNet task from the VDD benchmark, and upsampling the images from the 5-dataset benchmark to $72\times72$. 
    The results have been averaged over 5 different task orders.}%\vspace{-5mm}
    \label{tab:5-dataset-results}
\end{wraptable}

\subsection{Results on the 5-Dataset Benchmark}\vspace{-2mm}
Table \ref{tab:5-dataset-results} shows the comparisons. We use the same ViT-B/8 backbone pretrained on the ImageNet images from the VDD benchmark for all the experiments across all the methods and upsample the images in the 5-Dataset benchmark (consisting of CIFAR10~\citep{cifar}, MNIST \citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST \citep{notmnist}, and SVHN~\citep{svhn}) to $72\times72$. 
We can see that ArtiHippo significantly outperforms L2P and S-Prompts under the task-incremental setting. 
% Following L2P, we report the average accuracy over 5 different task orders.

% \begin{minipage}{0.48\linewidth}
%     \subsection{Results on the 5-Dataset Benchmark}
%     Table \ref{tab:5-dataset-results} shows the comparisons of Average Accuracy (Eqn. \ref{eq:avg_accuracy_lll}). {\color{red}We use the same ViT-B/8 backbone pretrained on the ImageNet images from the VDD benchmark for all the experiments across all the methods and upsample the images in the 5-Dataset benchmark (consisting of CIFAR10~\citep{cifar}, MNIST \citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST \citep{notmnist}, and SVHN~\citep{svhn}) to $72\times72$. Table \ref{tab:5-dataset-results} shows that ArtiHippo significantly outperforms L2P and S-Prompts under task-incremental setting. Following L2P, we report the average accuracy over 5 different task orders.}

%     % \textit{Remarks:} Our method can be applied for class-incremental settings too. Please refer to the supplementary for some preliminary experiments and discussions. 
% \end{minipage}
% \hfill
% \begin{minipage}{0.48\linewidth}
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{c|c|c}
%         \textbf{Method} & \textbf{Num. Prompts} & \textbf{Avg. Acc.} \\
%         \toprule
%         S-Prompts$^\dagger$ & 1 & $91.40$ \\
%         L2P$^\dagger$ & 12 & $93.76 \pm 0.25$ \\
%         ArtiHippo (Projection) & - & $\boldsymbol{96.52 \pm 0.4}$ \\
%          % & & $^{\times}$Attn Head & 0 & - & 0 & $97.34$ \\
%         \bottomrule
%     \end{tabular}
%     }
%     \captionof{table}{Results on the 5-Dataset benchmark~\citep{adversarial-continual-learning}. 
%     % The results using L2P, Learning without Forgetting (LwF), and Elastic Weight Consolidation (EWC) have been reported from~\citep{learning-to-prompt}. 
%     All the methods use the same ViT-B/8 backbone pretained on the ImageNet task from the VDD benchmark, and upsampling the images from the 5-dataset benchmark to $72\times72$. The results have been averaged over 5 different task orders.
%     % $^{\times}$Average Accuracy obtained by finetuning the MHSA block for each task. See text for details.
%     }
%     \label{tab:5-dataset-results} \vspace{-4mm}
% \end{minipage}

% \begin{table}[t]
%     \centering
%     \resizebox{0.48\textwidth}{!}{
%     \begin{tabular}{c|c|c}
%         \textbf{Method} & \textbf{Num. Prompts} & \textbf{Avg. Acc.} \\
%         \toprule
%         S-Prompts$^\dagger$ & 1 & $91.40$ \\
%         L2P$^\dagger$ & 12 & $93.76 \pm 0.25$ \\
%         ArtiHippo (Projection) & - & $96.52 \pm 0.4$ \\
%          % & & $^{\times}$Attn Head & 0 & - & 0 & $97.34$ \\
%         \bottomrule
%     \end{tabular}}
%     \caption{Results on the 5-Dataset benchmark~\citep{adversarial-continual-learning}. 
%     % The results using L2P, Learning without Forgetting (LwF), and Elastic Weight Consolidation (EWC) have been reported from~\citep{learning-to-prompt}. 
%     All the methods use the same ViT-B/8 backbone pretained in ImageNet task from the VDD benchmark, and upsampling the images from the 5-dataset benchmark to $72\times72$.
%     % $^{\times}$Average Accuracy obtained by finetuning the MHSA block for each task. See text for details.
%     }
%     \label{tab:5-dataset-results} \vspace{-4mm}
% \end{table}

% It should be noted that we use task identities when making predictions, whereas L2P infers them by running an additional forward pass through the pretrained backbone. Although a direct comparison cannot be drawn, the table shows that adding new parameters the the base architecture, as opposed to keeping it fixed and learning prompts, can improve the performance many fold. This shows a need for lifelong learningmethods using Vision Transformers need adaptive structures, and not only external prompts. Both these approaches are complementary to each other and combining them could lead to better performance. This line of work has been left for future study.






% \begin{table}[]
%     \centering
%     \resizebox{0.49\textwidth}{!}{
%     \begin{tabular}{@{}c|c|c|c|c|c@{}}
%         \toprule
%         Supernet Sampling     & Explr & Explt & Explt & Explr-Explt & Explr-Explt \\
%         \midrule
%         Evolutionary Search   & Explr & Explr & Explt & Explr-Explt & Explr \\
%         \midrule
%         Avg. Accuracy (\%)      & \textbf{83.27}      & 82.50      & 82.29       & 82.84                   & 82.98 \\
%         \#Params (M) Added & 17.74    & 18.91    & \textbf{15.36}     & 18.61                 & 26.01 \\
%         Rel. Increase (\%)    & 20.80     & 22.18     & \textbf{18.01}      & 21.83                  & 30.50 \\
%         \bottomrule
%     \end{tabular}}
%     \caption{Comparison of search strategies on the average accuracy on the VDD dataset along with the increase in parameter count.}
%     \label{tab:exploration-vs-exploitation}
% \end{table}



% \begin{table*}[]
%     \centering
%     \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
% \toprule
% Task & ImNet & C100 &     SVHN &      UCF &  OGlt &     GTSR &  DPed &  Flwr &     Airc &   DTD  \\
% \midrule
% Acc & 79.38 &    90.96 &    95.87 &    82.38 & 84.74 &    99.92 & 99.91 & 89.41 &     49.5 & 50.32  \\
% \midrule
% \midrule
% Task & ImNet &     OGlt & C100 &     SVHN &   UCF &     GTSR &  DPed &  Flwr &     Airc &   DTD  \\
% \midrule
% Acc & 79.38 &     85.2 &    91.22 &    95.82 & 81.92 &    99.95 & 99.91 & 86.76 &    54.31 & 53.88  \\
% \midrule
% \midrule
% Task & ImNet &      DTD &     Airc &     Flwr &  DPed &     GTSR &   UCF &  SVHN & C100 &  OGlt  \\
% \midrule
% Acc & 79.38 &    62.45 &    51.97 &    87.25 & 99.95 &    99.94 & 77.41 & 95.84 &    91.22 &  82.5  \\
% \midrule
% \midrule
% Task & ImNet &     Airc &     GTSR & C100 &  DPed &     SVHN &  OGlt &   UCF &      DTD &  Flwr  \\
% \midrule
% Acc & 79.38 &    55.03 &    99.94 &    90.43 & 99.95 &    95.93 & 82.67 & 78.89 &    50.11 &   88.82  \\
% \midrule
% \midrule
% Task & ImNet &     GTSR &      DTD &     Airc &   UCF & C100 &  DPed &  OGlt &     Flwr &  SVHN  \\
% \midrule
% Acc & 79.38 &    99.92 &    60.16 &     54.7 & 81.56 &    90.59 & 99.98 & 83.47 &    88.82 &   95.78  \\
% \bottomrule
% \bottomrule
% \end{tabular}

%     \caption{Accuracy on each task with different task orders on the VDD dataset.}
%     \label{tab:task-order-acc-vdd}
% \end{table*}

% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%   \toprule
%   Task & Average Accuracy \\
%   \midrule
%   CIFAR100 & $90.88 \pm 0.36$ \\
%   SVHN & $95.85 \pm 0.06$ \\
%   UCF & $80.43 \pm 2.17$ \\
%   Omniglot & $83.72 \pm 1.21$ \\
%   GTSR & $99.93 \pm 0.01$ \\
%   DPed & $99.94 \pm 0.03$ \\
%   VGG-Flowers & $88.21 \pm 1.14$ \\
%   Aircraft & $53.10 \pm 2.35$ \\
%   DTD & $55.38 \pm 5.67$ \\
%   \bottomrule
% \end{tabular}
%     \caption{Average Accuracy and standard deviation for each task, when evaluated in different sequences.}
%     \label{tab:task-seq-acc-comparison}
% \end{table}

% \begin{table}
%         \centering
%         \resizebox{0.32\textwidth}{!}{
%         \begin{tabular}{@{}l|c|c|c@{}}
%                 % \toprule
%                  & \multicolumn{3}{c}{Shortcut in {\tt Adapter}} \\
%                 \midrule
%                 {\tt Adapter} in NAS & \multicolumn{2}{c|}{w/o}  & w/  \\
%                 \midrule
%                 \#Params (M) Added & \multicolumn{2}{c|}{2.96} & 4.14 \\
%                 \midrule
%                 Rel. Increase (\%) & \multicolumn{2}{c|}{3.47} & 4.89 \\
%                 \midrule
%                 {\tt Adapter} in Finetuning & w/  & w/o  & w/   \\
%                 \midrule
%                 Test Accuracy (\%) & 82.18 & 78.16 & 82.32 \\
%                 \midrule
                
%                  ImageNet$\rightarrow$ Omniglot & \multicolumn{3}{c}{Learned Operations}  \\
%                 \midrule
%                 Block 1\& 2 & \multicolumn{2}{c|}{\cellcolor{adapt}{Adapt}}   & \cellcolor{adapt}{Adapt}  \\
%                 % Block 2 & \multicolumn{2}{c|}{{\color{orange}adapt}} & {\color{orange}adapt}  \\
%                 Block 3 \& 4 & \multicolumn{2}{c|}{\cellcolor{reuse}{Reuse} } & \cellcolor{adapt}{Adapt}  \\
%                 % Block 4 & \multicolumn{2}{c|}{{\color{olive}reuse}} & {\color{orange}adapt}  \\
%                 Block 5 & \multicolumn{2}{c|}{\cellcolor{adapt}{Adapt}} & \cellcolor{adapt}{Adapt}  \\
%                 Block 6 & \multicolumn{2}{c|}{\cellcolor{reuse}{Reuse}} & \cellcolor{adapt}{Adapt}  \\
%                 Block 7 & \multicolumn{2}{c|}{\cellcolor{adapt}{Adapt}} & \cellcolor{new}{New}  \\
%                 Block 8 \& 9  & \multicolumn{2}{c|}{\cellcolor{new}{\ New\ }} & \cellcolor{adapt}{Adapt}  \\
%                 % Block 9 & \multicolumn{2}{c|}{{\color{red}new}} & {\color{orange}adapt}  \\
%                 Block 10 & \multicolumn{2}{c|}{\cellcolor{new}{New}} & \cellcolor{new}{New}  \\
%                 Block 11 & \multicolumn{2}{c|}{\cellcolor{skip}{Skip}} & \cellcolor{adapt}{Adapt}  \\
%                 Block 12 & \multicolumn{2}{c|}{\cellcolor{skip}{Skip}} & \cellcolor{new}{New}  \\
%                 \bottomrule
%         \end{tabular}}
%         \caption{Results of the ablation study on the {\tt Adapter} implementation (Sec.~\ref{sec:supernet_construction}): with (w/) vs without (w/o) shortcut connection for the MLP {\tt Adapt} layer (Figure~\ref{fig:flow}). We test lifelong learning from ImageNet to Omniglot in the VDD. The proposed combination of w/o shortcut in Supernet NAS training and target network selection and w/ shortcut in finetuning (retraining newly added layers) is the best in terms of the trade-off between performance and cost.}
%         \label{tab:adapter_ablation} \vspace{-3mm}
% \end{table}

\subsection{Learned architecture and architecture efficiency}\vspace{-2mm}
Figure \ref{fig:vdd_arch-sim} shows the experts learned for each task in the VDD dataset. When learning CIFAR100 after ImageNet, the search process learns to reuse most of the ImageNet experts. This is an intuitive result since both the tasks represent natural images. In contrast, while learning Omniglot (OGlt, Task 2), the search adds many new experts. 
Interestingly, when learning Omniglot, the search learns to adapt the ImageNet expert in Block 1 and reuse the SVHN expert in Block 2. Both Omniglot and SVHN consist of digit-like images. However, SVHN is in a natural setting, whereas Omniglot contains clean, black and white images. The search process can also use ImageNet experts (experts for natural images) for tasks VGG-Flowers (8th task) and Aircraft (9th task), both of which contain natural images. The sensible architectures learned continually show the effectiveness of the proposed task-similarity-oriented ArtiHippo.

In addition to learning qualitatively meaningful architectures, the proposed method also shows quantitative advantages. On the VDD dataset, because of the {\tt Skip} operation in the proposed framework, the number of parameters \textit{reduces} by 0.92M/task (averaged over 3 different runs). Our method also reduces the number of FLOPs by 0.005G/task (averaged over 3 different runs), which is advantageous as compared to the \textit{increase} of 1.06G/task of the L2P method.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/v2/vdd-structure-v2.pdf}
    \caption{ArtiHippo learned-to-grow on VDD. Starting from the ImageNet-pretrained 12-layer ViT (B1 -- B12 in Tsk1\_ImNet), sensible architectures are continually learned for the remaining 9 tasks on the VDD dataset. 
    % {\color{orange}orange} denotes {\tt Adapt}, {\color{red}red} denotes {\tt New}, {\color{olive}olive} denotes {\tt Reuse}, {\color{gray}gray} denotes {\tt Skip}. 
    Each column denotes a Transformer block of the ViT in which only the projection layer of the MHSA block is maintained as ArtiHippo in lifelong learning with the remaining components frozen. The proposed Exploration-Exploitation method gradually forms interesting long-term memory structures at different blocks from task to task: some are task specific (e.g., B2 is adapted in SVHN and reused only by Omniglot: two similar tasks) and some have more task synergies (e.g. B3). \colorbox{skip}{S}, \colorbox{reuse}{R}, \colorbox{adapt}{A} and \colorbox{new}{N} represents {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New} respectively.  \textit{Best viewed in color and magnification. 
    % A large version is reproduced in the supplementary.
    }}
    \label{fig:vdd_arch-sim} \vspace{-4mm}
\end{figure}

\begin{table} [H]
        \centering
        \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{c|l|l|l|l|l|cllllllll} \toprule

      & \multicolumn{14}{c}{ImageNet $\rightarrow$ Omniglot under the lifelong learning setting} \\ \midrule
      & \multicolumn{2}{c|}{{\tt Adapter} in} &  \multirow{3}{1cm}{ \#Param Added}     &    \multirow{3}{*}{Rel. $\uparrow$}          &   \multirow{3}{1cm}{Test Acc.}    &     \multicolumn{9}{c}{Learned Operation per Block} \\
      \cline{2-3} \cline{7-15}
      & \multirow{2}{*}{NAS} & \multirow{2}{*}{Finetune} & &  &    & 1 &   3 &   \multirow{2}{*}{5} &   \multirow{2}{*}{6} &   \multirow{2}{*}{7} &    8 &   \multirow{2}{*}{10} &   \multirow{2}{*}{11} &   \multirow{2}{*}{12} \\
      &           &           &   &              &    &                               2 &   4 &    &   &   &    9 &    &    &    \\
      \cline{1-15}
Shorcut &        \multirow{2}{*}{w/o A \& S} &         w/ A &  \multirow{2}{*}{2.96M} &         \multirow{2}{*}{3.47\%} & 82.18 & \multirow{2}{*}{\colorbox{adapt}{A}} &   \multirow{2}{*}{\colorbox{reuse}{R}} &   \multirow{2}{*}{\colorbox{adapt}{A}} &   \multirow{2}{*}{\colorbox{reuse}{R}} &   \multirow{2}{*}{\colorbox{adapt}{A}} &    \multirow{2}{*}{\colorbox{new}{N}} &    \multirow{2}{*}{\colorbox{new}{N}} &    \multirow{2}{*}{\colorbox{skip}{S}} &    \multirow{2}{*}{\colorbox{skip}{S}} \\
     \cline{3-3}\cline{6-6}
     in &           &        w/o S &      &             & 78.16 &                               &   &   &   &   &    &    &    &    \\
     \cline{2-15}
{\tt Adapter} &         w/ S \& A &         w/ S \& A &  4.14M &         4.89\% & 82.32 &                               \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{new}{N} &    \colorbox{adapt}{A} &    \colorbox{new}{N} &    \colorbox{adapt}{A} &    \colorbox{new}{N} \\
\bottomrule
\end{tabular}}
\vspace{0.3em}
        \caption{Results of the ablation study on the {\tt Adapter} implementation (Sec.~\ref{sec:supernet_construction}): with (w/) vs without (w/o) shortcut connection for the MLP {\tt Adapt} layer. We test lifelong learning from ImageNet to Omniglot in the VDD. The proposed combination of w/o shortcut in Supernet NAS training and target network selection and w/ shortcut in finetuning (retraining newly added layers) is the best in terms of the trade-off between performance and cost.}
        \label{tab:adapter_ablation} \vspace{-3mm}
\end{table}

\vspace{-2mm}
\section{Ablation Studies}
\label{sec:ablations} \vspace{-2mm}
% \subsection{Mean Class Token vs. Mean Image Token}
% \subsection{Exploration and Exploitation vs. Pure Exploitation vs. Pure Exploration}
% \subsection{Random vs. Exploration-Exploitation based Evolutionary Search}

\subsection{The Structure of {\tt Adapter}}\vspace{-2mm} 
We verify the effectiveness of the proposed hybrid adapter (Sec.~\ref{sec:supernet_construction}) using a lifelong learning setup with 2 tasks: ImageNet and Omniglot. The Omniglot dataset presents two major challenges for a lifelong learning system. First, Omniglot is a few-shot dataset, for which we may expect a lifelong learning system can learn a model less complex than the one for ImageNet. Second,  Omniglot has a significantly different data distribution than ImageNet, for which we may expect a lifelong learning system will need to introduce new parameters, but hopefully in a sensible and explainable way. Table \ref{tab:adapter_ablation} shows the results. In terms of the learned neural architecture, a more compact model (row 3) is learned without the shortcut in the adapter during Supernet training and target network selection: the last two MHSA blocks are skipped and three blocks are reused. Skipping the last two MHSA blocks makes intuitive sense since Omniglot may not need those high-level self-attention (learned for ImageNet) due to the nature of the dataset. The three consecutive {\tt new} operations (in Blocks 8,9,10) also make sense in terms of learning new self-attention fusion layers (i.e., new memory) to account for the change of the nature of the task. Adding shortcut connection back in the finetuning shows significant performance improvement (from 78.16\% to 82.18\%), making it very close to the performance (82.32\%) obtained by the much more expensive and less intuitively meaningful alternative (the last row). 

% First, Omniglot is a few-shot dataset, which will entail a lifelong learning method that starts with a large ViT to find a compact model. Second, Omniglot has a significantly different data distribution than ImageNet, making it necessary to add additional parameters. These two conflicting properties make this setup ideal for testing. Table \ref{tab:adapter_ablation} shows that using a residual adapter during Search and Finetuning achieves an accuracy of $0.8232$, which is higher than $0.7816$ achieved by plain adapter. This comes at a higher increase in the number of parameters ($5.2\%$ using residual adapters vs. $3.47\%$ using plain adapters). Using plain adapters during search and residual adapters during finetuning achieves the best of both, achieving an accuracy of $0.8218$. Note that we do not use any data augmentations for the ablation experiments, but use a Drop Path rate of 0.25 to prevent overfitting.



\begin{figure} [h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/v2/epochwise_plot_v3.pdf}
    \caption{Results of the ablation study on the Exploration-Exploitation (EE) guided sampling in the Supernet NAS training using  the VDD benchmark~\citep{vdd}. The proposed EE sampling strategy is much more efficient than the pure exploration based strategy (i.e., the vanilla SPOS NAS~\citep{spos}). It uses the Supernet training efficiently even at 25 epochs and achieves better performance than pure Exploration, which is desirable for fast adaptation in dynamic environments using lifelong learning. The \% increase in parameters shows that EE strategy is effective in reusing experts from the previous tasks and limiting the increase in parameters. Note that the net increase is negative because the {\tt Skip} operation removes the entire attention head. The results have been averaged over 2 task sequences, with 3 runs per sequence with different seeds.
    }
    \label{fig:epochs-vs-acc} \vspace{-2mm}
\end{figure}



\subsection{The Exploration-Exploitation Sampling Method} \vspace{-2mm}
Figure~\ref{fig:epochs-vs-acc}, left shows that the proposed exploration-exploitation strategy can consistently obtain higher accuracy than pure exploration even when the supernet is trained a small number of epochs. Although training the supernet for a longer duration improves the accuracy for pure exploration, it also leads to a higher number of parameters in the learned architecture (Figure \ref{fig:epochs-vs-acc} right). We thus verify that the proposed exploration-exploitation strategy is effective and efficient. This also shows that proposed task-similarity metric is meaningful.
% As shown in Figure~\ref{fig:epochs-vs-acc}, Exploration-exploitation during supernet training and exploration when generating the evolutionary search population achieves the highest accuracy at the cost of the highest parameter increase. Exploitation during both the phases achieves the lowest parameter increase with lower average accuracy. Exploration+Exploitation strategy achieves the best trade of.




% \subsection{Task Order on VDD}
% We experiment with 5 different task orders on the VDD dataset. Table \ref{tab:task-order-acc-vdd} shows the accuracies when different task sequences are used. Table \ref{tab:task-seq-acc-comparison} shows the accuracy of each task averaged across 5 sequences. The large standard deviation for UCF101, Aircraft, and DTD shows that our method is sensitive to the task order.

\begin{table*} [h]
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{l|l|l|cccccccccc|l}
        \toprule
            Component &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
             Projection & 82.65 & 90.92 & \textbf{95.93} & \textbf{77.08} & \textbf{84.14} & {99.92} & {99.80} & {77.76} & \textbf{47.11} & {45.79} & {$80.11 \pm 1.17$} \\ 
            Value & 82.65 & 84.98 & 95.74 & 76.25 & 83.55 & \textbf{99.93} &	99.90 &	\textbf{84.80} &	44.97 &	49.80 &	$\boldsymbol{80.26} \pm \boldsymbol{1.62}$ \\ 
            Query & {82.65} & 89.47 & 94.09 & 68.85 & 75.71 & 99.86 & \textbf{99.91} & 77.81 & 34.51 & \textbf{51.47} & $77.43 \pm 0.43$ \\ 
            FFN & {82.65} & 90.92 & 94.95 & 68.02 & 70.16 & 99.91 & 99.63 & 63.37 & 18.85 & 29.70 & $71.82 \pm 1.80$ \\  
        \bottomrule
    \end{tabular}}
    \vspace{0.3em}
    \caption{Results of ablation study on the other components of the ViT used for realizing the ArtiHippo. Realizing ArtiHippo at the Value shows slightly better performance than the Projection layer. However, the Value layer is often coupled with the Query and Key layers for efficient computation. Using the Projection layer offers negligible drop in Average Accuracy without sacrificing efficiency. $^*$ The results averaged over 3 different seeds.}
    \label{tab:vdd-component-ablation} \vspace{-2mm}
\end{table*}

\subsection{Evaluating feasibility of other ViT components as ArtiHippo}\vspace{-2mm}
Table \ref{tab:vdd-component-ablation} shows the accuracy with other components if the ViT used for learning the Mixture of Experts using the proposed NAS method. Interestingly, the Value performs slightly better than the Projection layer. However, using the the Value layer would be inefficient, as it is implemented as a parallel linear layer with the Query and Key. The projection layer can be efficiently replaced with the Mixture of Experts framework. Table \ref{tab:vdd-component-ablation} also shows that the Query component from the MHSA block, and the FFN which is typically used in the Mixture of Experts framework for Transformers \citep{vision-moe}, are ill suited. This reinforces our identification of the ArtiHippo in Section \ref{identifying-artihippo}.




%-------------------------------------------------------------------------
\section{Conclusions} \vspace{-2mm}
This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers for resilient lifelong learning. The final projection layers in the Multi-Head Self-Attention blocks of a Vision Transformer are identified and selected as ``Hippocampi" to implement long-term task-similarity-oriented memory in lifelong learning. The ArtiHippo is maintained by a proposed hierarchical task-similarity-oriented sampling based single-path one-shot Neural Architecture Search algorithm with exploration and exploitation handled in the sampling. The NAS space of ArtiHippo is defined by four basic learning-to-grow operations: {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New}, with the {\tt Adapt} realized with a hybrid Adapter. The task similarity is defined by the normalized cosine similarity between the mean class-tokens of a new task and old tasks. In experiments, the proposed method is tested on the challenging VDD and the 5-Datasets benchmarks. It obtains better performance than the prior art with sensible ArtiHippo learned continually. 


\section*{Acknowledgements}
This research is partly supported by NSF IIS-1909644,  ARO Grant W911NF1810295, ARO Grant W911NF2210010, NSF IIS-1822477, NSF CMMI-2024688, NSF IUSE-2013451 and DHHS-ACL Grant 90IFDV0017-01-00.  
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, ARO, DHHS or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon.

\bibliography{collas2023_conference}
\bibliographystyle{collas2023_conference}

\appendix

\section{Overview}
In the Appendix, we elaborate on the following aspects that are not presented in the submission due to space limit: 
\begin{itemize}
    % \item \textbf{Source Code and Training Logs:} The code for our implementation is available in the directory {\tt artihippo}. Due to size limit, we are not able to upload the pretrained checkpoints. We provide the anonymized training logs ({\tt artihippo/logs}). The pretrained checkpoints will be released after the reviewing process. 
    \item \textit{Details of the two benchmarks}: the Visual Domain Decathlon (VDD)~\citep{vdd} benchmark (Sec.~\ref{sec:vdd}) and the 5-Datasets~\citep{adversarial-continual-learning} benchmark (Sec.~\ref{sec:5datasets}).
    \item \textit{The Base Model and Its Training Details}: the Vision Transformer (ViT) model specification (ViT-B/8) used in our experiments (Sec.~\ref{sec:model}), and training details on the ImageNet (Sec.~\ref{sec:imagenet}). 
    \item \textit{Background:} To be self-contained, we give a brief introduction to the learn-to-grow method~\citep{learn-to-grow} in Sec.~\ref{sec:learn-to-grow-review} and the single-path one-shot (SPOS) neural architecture search (NAS) algorithm~\citep{spos} in Sec.~\ref{sec:spos}. 
    \item \textit{Comparison of Learn to Prompt, S-Prompts and ArtiHippo with Learn to Grow}: In Section \ref{sec:comparison-with-l2g}, we compare the performance of L2P$^\dagger$ \citep{learning-to-prompt}, S-Prompts$^\dagger$, and the proposed ArtiHippo with Learn to Grow on the VDD benchmark. We show that even with a stronger backbone model (ViT-B/8 instead of ResNet26 used in Learn to Grow), S-Prompts$^\dagger$ performs significantly worse, and L2P$^\dagger$ shows marginal improvements. ArtiHippo can outperform Learn to Grow by taking advantage of a strong backbone along with the proposed task-similarity oriented NAS.
    \item \textit{Modifying Learn to Prompt and S-Prompts for task-incremental setting}: In Section \ref{sec:task-inc-l2p-sprompt}, we describe our implementation of L2p$^\dagger$ and S-Prompts$^\dagger$ used for comparisons.
    \item \textit{Settings and Hyperparameters in the Proposed Lifelong Learning:} We provide them in Sec.~\ref{sec:training-details}, together with the breakdown of results of 5 different sequences of task orders tested in the VDD benchmark and the corresponding ArtiHippo learned-to-grow with respect the different task orders.
    \item \textit{Learned architecture with pure exploration and different task order}: We show that with a different task order, the proposed method can still learn to exploit inter-task similarities. We also show that pure exploration cannot effectively exploit task similairties by comparing the learned architecture with the architecture learned using the exploration-exploitation strategy.
    \item \textit{Preliminary Results on Class-Incremental Settings:} We show that the task similarity used in our sampling may have the potential of developing class-incremental lifelong learning in Sec.~\ref{sec:class-incremental}.
     
\end{itemize}

% we first briefly review the Vision Transformer (ViT) design, and provide the description of the ViT-B/8 model used in our experiments (Section \ref{sec:implementation}). We then provide details of the training of the ViT-B/8 model on the downscaled ImageNet12 dataset from the Visual Domain Decathlon (VDD) benchmark~\citep{vdd} (Section \ref{sec:imnet-training}), followed by the hyperparameters for the Neural Architecture Search (NAS) used for maintaining the ArtiHippo framework (Section \ref{sec:training-details}). We review Learn to Grow~\citep{learn-to-grow} (Section \ref{sec:learn-to-grow-review}), from which our method is inspired, and address its limitations. In Section \ref{sec:spos}, we review the Single-Path One-Shot (SPOS) NAS method~\citep{spos}, and describe how we extend it in learning to grow for resilient lifelong learning. We provide the statistics of the VDD benchmark and the 5-Datasets benchmark~\citep{adversarial-continual-learning} in Section \ref{sec:dataset-details}. In Section \ref{sec:normalized-cosine-similarity}, we verify the use of the Normalized Cosine Similarity measure as opposed to plain Cosine Similarity by comparing the values of the probability of sampling experts at each block ($\psi_e^l$), and the values of the retention probabilities ($\rho_e^l$). In Section \ref{sec:exploration-exploitation-ablations} we provide insights into the behavior of Exploration and Exploitation sampling schemes used by the Neural Architecture Search and discuss which type of experts are encouraged by the Exploration and the Exploitation schemes. We provide ablation studies with different task sequences on the VDD benchmark and show the Average Accuracies and the architectures learned for 5 task sequences. Finally, in Section \ref{sec:class-incremental}, we show preliminary experiments to extend our method for class-incremental settings, i.e., predictions without task indices. The code for our implementation is available at the anonymized repository \href{https://link}{https://link}

\section{Dataset Details}
\label{sec:dataset-details}
\subsection{The VDD Benchmark}\label{sec:vdd}
It consists of 10 tasks: ImageNet12~\citep{imagenet}, CIFAR100~\citep{cifar}, SVHN~\citep{svhn}, UCF101 Dynamic Images (UCF)~\citep{ucf1,ucf2}, Omniglot~\citep{omniglot}, German Traffic Signs (GTSR)~\citep{gtsrb}, Daimler Pedestrian Classification (DPed)~\citep{daimlerpedcls},  VGG Flowers~\citep{vgg-flowers}, FGVC-Aircraft~\citep{aircraft},    and Describable Textures (DTD)~\citep{dtd}. All the images in the VDD benchmark have been scaled such that the shorter side is 72 pixels. Table~\ref{tab:vdd-num-samples} shows the number of samples in each task. Figure~\ref{fig:vdd-examples} shows examples of images from each task of the VDD benchmark.

In our experiments, we use 10\% of {\tt the official training data} from each of the tasks for validation (e.g., used in the target network selection in Section \ref{sec:target-network-selection}), and report the accuracy on {\tt the official validation set} for fair comparison with the learn-to-grow method~\citep{learn-to-grow} in Table \ref{tab:vdd-results-full}. In Table~\ref{tab:vdd-num-samples}, the \texttt{train, validation} and \texttt{test} splits are thus referred to 90\% of the official training data, 10\% of the official training data, and the entire official validation data respectively. When finetuning the learned architecture, we use the entire {\tt the official training data} to train and report results on the {\tt the official validation set}.%Note that the we refer to the official Validation set from the VDD benchmark as our test set, since the labels for the official test set are not available.

\begin{figure*}[t]
    \centering{
    % CIFAR100
        \subfiggrid{vdd}{imagenet}{1.25}{1.25}{ImageNet}\vspace{3mm}
        ~
        \subfiggrid{vdd}{cifar100}{1.25}{1.25}{CIFAR100}
        ~
        \subfiggrid{vdd}{svhn}{1.25}{1.25}{SVHN}
        ~
        \subfiggrid{vdd}{ucf101}{1.25}{1.25}{UCF101}
        ~
        \subfiggrid{vdd}{omniglot}{1.25}{1.25}{Omniglot}
        ~
        \subfiggrid{vdd}{gtsrb}{1.25}{1.25}{GTSR}
        ~
        \subfiggrid{vdd}{daimlerpedcls}{0.8}{1.25}{Pedestrian Cls.}
        ~
        \subfiggrid{vdd}{vgg-flowers}{1.25}{1.25}{VGG-Flowers}
        ~
        \subfiggrid{vdd}{aircraft}{1.25}{0.9}{FGVC-Aircraft}
        ~
        \subfiggrid{vdd}{dtd}{1.25}{1.25}{DTD}
    }\vspace{3mm}
    \caption{Example images from the VDD benchmark~\citep{vdd}. Each task has a significantly different domain than others, making VDD a challenging benchmark for lifelong learning.}
    \label{fig:vdd-examples}
\end{figure*}

\begin{figure*}[h]
    \centering{
    % CIFAR100
        \subfiggrid{5-datasets}{mnist}{1.25}{1.25}{MNIST}
        ~
        \subfiggrid{5-datasets}{cifar10}{1.25}{1.25}{CIFAR10}
        ~
        \subfiggrid{5-datasets}{svhn}{1.25}{1.25}{SVHN}
        ~
        \subfiggrid{5-datasets}{not-mnist}{1.25}{1.25}{not-MNIST}
        ~
        \subfiggrid{5-datasets}{fashion-mnist}{1.25}{1.25}{Fashion-MNIST}
    }\vspace{1em}
    \caption{Example images from the 5-Datasets benchmark~\citep{adversarial-continual-learning}.}
    \label{fig:5-dataset-examples}
\end{figure*}

\subsection{The 5-Datasets Benchmark}\label{sec:5datasets}
It consists of 5 tasks: CIFAR10~\citep{cifar}, MNIST~\citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST~\citep{notmnist}, and SVHN~\citep{svhn}. Table~\ref{tab:5-dataset-num-samples} shows the data statistics.  Figure~\ref{fig:5-dataset-examples} shows examples of images from each task. To be consistent with the settings used on the VDD benchmark, we use 15\% of \texttt{the training data} for validation and report the results on \texttt{the official test data}, except for not-MNIST for which an official test split is not available. So, for the not-MNIST dataset, we use the small version of that dataset, with which we construct the test set by randomly sampling 20\% of the samples. From the remaining 80\%, we use 15\% for validation, and the rest as the training set.

\begin{minipage}{0.48\linewidth}
    \centering
    \begin{tabular}{c|c|c|c}
  \toprule
  Task & Train & Validation & Test \\
  \midrule
  ImageNet12 & 1108951 & 123216 & 49000 \\
  CIFAR100 & 36000 & 4000 & 10000 \\
  SVHN & 42496 & 4721 & 26040 \\
  UCF & 6827 & 758 & 1952 \\
  Omniglot & 16068 & 1785 & 6492 \\
  GTSR & 28231 & 3136 & 7842 \\
  DPed & 21168 & 2352 & 5880 \\
  VGG-Flowers & 918 & 102 & 1020 \\
  Aircraft & 3001 & 333 & 3333 \\
  DTD & 1692 & 188 & 1880 \\
  \bottomrule
\end{tabular}
    \captionof{table}{The number of samples in training, validation and testing sets per task used in our experiments on the VDD benchmark~\citep{vdd}.}
    \label{tab:vdd-num-samples}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \begin{tabular}{c|c|c|c}
    \toprule
         Task & Train & Validation & Test  \\
         \midrule
         MNIST & 51000 & 9000 & 10000 \\
         not-MNIST & 12733 & 2247 & 3744 \\
         SVHN & 62269 & 10988 & 26032 \\
         CIFAR10 & 42500 & 7500 & 10000 \\
         Fashion MNIST & 51000 & 9000 & 10000 \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Number of samples in training, validation, and test sets per task in the 5-Datasets benchmark~\citep{adversarial-continual-learning}. The test samples have been reported from the official test data provided by each individual dataset, except for not-MNIST. See text for details.}
    \label{tab:5-dataset-num-samples}
\end{minipage}



\section{The Vision Transformer: ViT-B/8}
\label{sec:model}
We use the base Vision Transformer (ViT) model, with a patch size of $8 \times 8$ (ViT-B/8) model from~\citep{vit}. The base ViT model which contains 12 Transformer blocks with residual connections for each block. A Transformer block is defined by stacking a Multi-Head Self-Attention (MHSA) block and a Multi-Layer Perceptron (MLP) block with resudual connections for each block. ViT-B/8 uses 12 attention heads in each of the MHSA blocks, and a feature dimension of 768. The MLP block expands the dimension size to 3072 in the first layer and projects it back to 768 in the second layer. For all the experiments, we use an image size of $72\times72$ following the VDD setting. We base the implementation of the ViT on the {\tt timm}~\citep{timm} package. 
% For the evolutionary search, we use the implementation provided by SPOS~\citep{spos}, and the details of the evolutionary search algorithm can be found therein.







\section{ImageNet Training}
\label{sec:imagenet}
To train the ViT-B/8 model, we use the ImageNet data provided by the VDD benchmark (the \texttt{train} split in Table~\ref{tab:vdd-num-samples}). To save the training time, we initialize the weights from the ViT-B/8 trained on the full resolution ImageNet dataset (224$\times$224) and available in the \texttt{timm} package, and finetune it for 30 epochs on the downsized version of ImageNet (72$\times$72) in the VDD benchmark. We use a batch size of 2048 split across 4 Nvidia Quadro RTX 8000 GPUs. We follow the standard training/finetuning recipes for ViT models. The file {\tt artihippo/logs/imagenet\_pretraining/args.yaml} provides all the training hyperparameters used for training the the ViT-B/8 model on ImageNet.
During testing, we take a single center crop of 72$\times$72 from an image scaled with the shortest side to scaled to 72 pixels.



% Add Learn to Grow details: Base model, search on each layer, DARTS
\section{Background on the Learn-to-Grow Method}
\label{sec:learn-to-grow-review}
% To further motivate our approach, we describe Learn to Grow~\citep{learn-to-grow}, the method upon which we build ArtiHippo. 
The learn-to-grow method~\citep{learn-to-grow} uses Differentiable Architecture Search (DARTS)~\citep{darts}, a supernet based NAS algorithm to learn a strategy for reusing, adapting, or renewing the parameters learned for the previous tasks (skipping is not applied). Consider an $L$-layer backbone network with $\mathcal{S}^l$ choice of parameters for a layer $l$ learned for the previous tasks. The parameters can be the weights and biases of the layer in the case of fully-connected layers, or the filters and the biases in the case of a Convolutional layers. For a new task, the learn-to-grow method constructs the search space for NAS (referred to as operations) by applying the operation operations {\tt reuse}, {\tt adapt}, and {\tt new} to $\mathcal{S}^l$ for all layers $l \in [1,L]$. The total number of choices for layer $l$ is $C_l = 2|\mathcal{S}^l|+1$ \{$|\mathcal{S}^l|$ reuse, $|\mathcal{S}^l|$ adapt, and $1$ new operation\}. Following DARTS, the output of a layer when training the NAS supernet is given by
\begin{equation}
    x_{out}^l = \sum_{c=1}^{C_l} \frac{exp(\alpha_c^l)}{\sum_{c^\prime=1}^{C_l}exp(\alpha_{c^\prime}^l)}g_c^l(x_{in}^l), 
\end{equation}
where $g_c^l(\cdot)$ is defined by,
\begin{equation}
\small 
    g_c^l(x_{l-1}) = 
    \begin{cases}
        S_c^l(x_{in}^l) & \text{if } c \le |\mathcal{S}^l|, \\
        S_c^l(x_{in}^l) + \gamma_{c-|\mathcal{S}^l|}^l(x_{in}^l) & \text{if } |\mathcal{S}^l| < c \le 2|\mathcal{S}^l|, \\
        \texttt{new}^l(x_{in}^l) & \text{if } c = 2|\mathcal{S}^l| + 1, 
    \end{cases}
\end{equation}
where $\gamma$ denotes the additional operation used in parallel ($1\times1$ convolution in the case of a Convolutional layer) which implements the {\tt adapt} operation. Using DARTS, the operations are trained jointly with architecture coefficients $\alpha_c^l$. Once the supernet is trained, the optimal operation is selected such that $(c^*)^l \gets \text{argmax}_c\ \alpha_c^l$. In experiments, the learn-to-grow method uses a 26-layer ResNet~\citep{resnet} on the VDD benchmark.

\textbf{Remarks on the proposed learning-to-grow with ViTs.} The learn-to-grow method~\citep{learn-to-grow} can maintain dynamic feature backbone networks for different tasks based on NAS, which leads to the desired selectivity and plasticity of networks in lifelong learning. It has been mainly studied with Convolutional Neural Networks (CNNs), and often apply DARTS for all layers with respect to the three operations, which is time consuming and may be less effective when a new task has little data. 

More importantly, the vanilla learn-to-grow method with DARTS and ResNets does not have the motivation of integrating learning and (long-term) memory in lifelong learning, which is the focus of our proposed ArtiHippo method. In learning a new task, unlike the vanilla learn-to-grow method in which all the previous tasks are treated equally when selecting the operation operations, our proposed ArtiHippo leverages the task similarities which in turn exploits the class-token specialized in ViTs. 

% More specifically,  each block of the Transformer architecture (the MHSA and MLP components) performs a different function, and constructing the Mixture of Experts framework using NAS can be made much more efficient by selecting a single component based on how appropriate it is for the lifelong learningsetup. This motivates our approach for using a single component to realize the ArtiHippo framework in Vision Transformers as opposed to applying Neural Architecture Search at every layer. 

% Moreover, as the formulation shows, all the previous tasks are treated equally when selecting the operation operations. The search operation could be made more efficient if the task similarities are taken into account. Our method addresses this limitation.

\section{Comparison with Learn to Grow \citep{learn-to-grow}}
\label{sec:comparison-with-l2g}
For completeness, we Table \ref{tab:vdd-results} with the results reported on the VDD benchmark by Learn to Grow \citep{learn-to-grow}. Although not directly comparable, our method does show consistent performance trend across tasks, compared with the ResNet-based~\citep{resnet} DARTS-optimized~\citep{darts}.
\begin{table} [h]
    \centering
    \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{l|l|l|cccccccccc|l}
        \toprule
            Model & NAS & Method &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
           \multirow{2}{*}{ResNet} & - &Adapter &  69.84 & 79.82 &  94.21 & 70.72 &  85.1 & 99.89 & 99.58 & 60.29 &  50.11 & 50.60 &    76.02 \\
            & DARTS & L2G &  69.84 & 79.59 & 95.28 & 72.03 &  \textbf{86.6} & 99.72 & 99.52 & 71.27 &  \textbf{53.01} & 49.89 & 77.68 \\ \midrule

            \multirow{5}{*}{\parbox{2cm}{ViT-B/8 (\#Params: 86.04M, FLOPs: 7.11G)}} & - & {S-Prompts$^\dagger$} (p=1/task) & 82.65 & 86.69 & 69.89 & 51.95 & 49.55 & 94.07 & 99.25 & 90.39 & 35.03 & 56.19 & $71.57 \pm 0.31$ \\
            & - & {L2P$^\dagger$} (p=12/task) & 82.65 & 89.06 & 81.43 & 63.99 & 62.86 & 98.21 & 99.77 & \textbf{94.58} & 45.00 & \textbf{60.78} & $77.83 \pm 0.28$ \\
            \cline{2-14}
            & Vanilla SPOS & ArtiHippo & 82.65 & {85.50} & {95.63} & {74.09} & {82.53} & \textbf{99.93} & {99.85} & {79.31} & {41.62} & {41.21} & {$78.23 \pm 0.93$} \\ 
            \cline{2-14}
             & \multirow{2}{*}{\parbox{2.5cm}{SPOS + Hierarchical Sampling}} & ArtiHippo & 82.65 & 90.92 & \textbf{95.93} & {77.08} & 84.14 & {99.92} & {99.80} & {77.76} & {47.11} & {45.79} & {$80.11 \pm 1.17$} \\ 
            \cline{3-14}
            & & ArtiHippo (p=1/task) & 82.65 & \textbf{90.99} & 95.87 & \textbf{78.98} & 86.12 & 99.91 & \textbf{99.89} & 88.20 & 45.86 & 51.31 & $\boldsymbol{81.98 \pm 0.95}$ \\ 
        \bottomrule
    \end{tabular}}
    \caption{Results on the VDD benchmark~\citep{vdd}. The results on L2G and Adapter are adopted from the L2G paper~\citep{learn-to-grow}. The task order is based on the one used in the L2G method. Our method shows clear improvements over the previous approaches. The proposed hierarchical sampling performs better than uniform sampling. All the results from our experiments are averaged over 3 different seeds.}% The average accuracy (Eqn.~\ref{eq:avg_accuracy_lll}) are not directly comparable due to different models used. Due to the lack of results on the VDD by ViT based lifelong learning approaches in the literature, we provide the Upper Bound performance as reference, which are obtained by finetuning 13 choices of components of the ImageNet-pretrained ViT (Table~\ref{tab:acc_vs_forgetting}) under a 2-task transfer learning setting, and then choosing the variant which gives the maximum accuracy on a task.  This represents the maximum attainable accuracy for a task under our experimental settings. The proposed lifelong learning method can achieve performance close to the Upper Bound, showing its effectiveness. The task order is based on the one used in the L2G method. \color{red}{$^*$ The results averaged over 3 different seeds.}}
    \label{tab:vdd-results-full} \vspace{-3mm}
\end{table}

\section{Background on the Single-Path One-Shot Neural Architecture Search Method}
\label{sec:spos}
DARTS~\citep{darts} trains the entire supernet jointly in learning a new task, and thus might not be practically scalable and sustainable after the supernet ``grew too fat" at each layer. 
% The formulation of Learn to Grow, which uses DARTS\citep{darts} as its Neural Architecture Search algorithm, can in principle incorporate a similarity based search. A straightforward way to to achieve that would be to regularize the output of the Softmax function used in each layer of the supernet to match the distribution defined by the similarity scores. However, the similarity defined distribution over the operations is a weak prior, and the optimal architecture could be different. This requires an exploration of the search space. Regularizing the Softmax function could hamper this exploration. 
The strategy used in Single-Path One-Shot (SPOS)~\citep{spos} NAS offers an alternative strategy based on a \textit{stochastic} supernet. It uses a bi-level optimization formulation consisting of the supernet training and the target network selection,
\begin{equation}
\label{eq:spos-train}
    W_{\mathcal{A}} = \text{argmin}_W\mathcal{L}_{train}(\mathcal{N}(\mathcal{A}, W)),
\end{equation}
\begin{equation}
\label{eq:spos-val}
    a^* = \text{argmax}_{a\in\mathcal{A}}\ Acc_{val}(\mathcal{N}(a, W_{\mathcal{A}}(a))),
\end{equation}
where Eqn.~\ref{eq:spos-train} is solved by defining a prior distribution $\Gamma(\mathcal{A})$ over the choice of operations in the stochastic supernet and optimizing,
\begin{equation}
    W_{\mathcal{A}} = \text{argmin}_W \mathbf{E}_{a \sim \Gamma(\mathcal{A})}\mathcal{L}_{train}(\mathcal{N}(\mathcal{A}, W)).
\end{equation}
This amounts to sampling one operation per layer (i.e., one-shot) of the neural network, and to forming a single path in the stochastic supernet. Eqn.~\ref{eq:spos-val} is optimized using an evolutionary search based on the validation performance for different candidates of the target network in a population, which is efficient since only inference is executed. The SPOS method empirically finds that a uniform prior works well in practice, especially when sufficient exploration is afforded. Note that this prior is also applied in generating the initial population for the evolutionary search.

The evolutionary search method used in the SPOS method is adopted from~\citep{real2019regularized}. It first initializes a population with a predefined number of candidate architectures sampled from the supernet. It then ``evolves" the population via the crossover and the mutation operations. At each ``evolving" iteration, the population is evaluated and sorted based on the validation performance. With the top-$k$ candidates after evaluation and sorting (the number $k$ is predefined), for crossover, two randomly sampled candidate networks in the top-$k$ are crossed to produce a new target network. For mutation, a randomly selected candidate in the top-$k$ mutates its every choice block with probability (e.g., $0.1$) to produce a new candidate. Crossover and mutation are repeated to generate sufficient new candidate target networks to form the population for the next ``evolving" iteration.

\textbf{Remarks on the proposed hierarchical task-similarity-oriented sampling with exploration-exploitation trade-off}. We modify the core sampling component in the SPOS algorithm for resilient lifelong learning with a long-term memory. During exploitation, we generate the prior $\Gamma(\mathcal{A})$ using our proposed hierarchical sampling scheme, and use the uniform prior during exploration. The same sampling scheme is applied when generating the initial population for the evolutionary search as well.

\section{Modifying S-Promts and L2P for task-incremental setting}
\label{sec:task-inc-l2p-sprompt}
Both, Learn to Prompt and S-Prompts, can be modified for task-incremental setting without altering the core algorithm for learning the prompts. For S-Prompts, this is done by training a separate task token (i.e. a \texttt{cls} token) per task and retrieving the correct token using the task ID. For L2P, we follow the official implementation\footnote{L2P official implementation: \href{https://github.com/google-research/l2p}{https://github.com/google-research/l2p}} used for evaluating on the 5-datasets benchmark. L2P first trains a set of $N$ prompts of length $L_p$ (i.e. $NL_p$ tokens) per task. It then learns a set of $N$ keys such that the distance between the keys and the image encoding (using a fixed feature extractor) is maximized. We retrieve the retrieve the correct prompts using the Task ID instead of using a key-value matching and make L2P compatible with a task-incremental setting. We initialize the the values for the prompts for task $t$ from the trained values of task $t-1$ following the original implementation.





\section{Settings and Hyperparameters in the Proposed Lifelong Learning}\label{sec:training-details}
Starting with the ImageNet pretrained ViT-B/8, the proposed lifelong learning methods consists of three components in learning new tasks continually and sequentially: supernet training, evolutionary search for target network selection, and target network finetuning. The supernet training and target network finetuning use the \texttt{train} split, while the evolutionary search uses the \texttt{validation} split, both shown in Table~\ref{tab:vdd-num-samples} and Table~\ref{tab:5-dataset-num-samples}. We use the vanilla data augmentation in both supernet training and target network finetuning.  

\textbf{Data Augmentations.}
A full list of data augmentations used for the VDD benchmark is provided in Table \ref{tab:vdd-augmentations}, and the data augmentations used for the tasks in the 5-datasets benchmark is provided in Table \ref{tab:5-dataset-augmentations}. The augmentations are chosen so as not to affect the nature of the data. Scale and Crop transformation scales the image randomly between 90\% to 100\% of the original resolution and takes a random crop with an aspect ratio sampled from a uniform distribution over the original aspect ratio $\pm0.05$.
In evaluating the supernet and the finetuned model on the validation set and test set respectively, images are simply resized to $72\times72$ with bicubic interpolation.

\begin{minipage}{0.48\linewidth}
    \centering
    \begin{tabular}{c|c|c|c}
         \toprule
         Task & Scale and Crop & Hor. Flip & Ver. Flip  \\
         \midrule
         CIFAR100 & Yes & p=0.5 & No \\
         Aircraft & Yes & p=0.5 & No \\
         DPed & Yes & p=0.5 & No \\
         DTD & Yes & p=0.5 & p=0.5 \\
         GTSR & Yes & p=0.5 & No \\
         OGlt & Yes & No & No \\
         SVHN & Yes & No & No \\
         UCF101 & Yes & p=0.5 & No \\
         Flwr. & Yes & p=0.5 & No \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Data augmentations for the 9 tasks in the VDD benchmark.}
    \label{tab:vdd-augmentations}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \begin{tabular}{c|c|c}
    \toprule
         Task & Scale and Crop & Hor. Flip  \\
         \midrule
         MNIST & Yes & No \\
         not-MNIST & Yes & No \\
         SVHN & Yes & No \\
         CIFAR100 & Yes & p=0.5 \\
         Fashion MNIST & Yes & No \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Data augmentations used for each task in the 5-Datasets benchmark.}
    \label{tab:5-dataset-augmentations}
\end{minipage}


\subsection{Supernet Training}
\textit{VDD Benchmark}: For each task, we train the supernet for 300 epochs. We use a label smoothing of 0.1. No other form of regularization is used since the {\tt skip} operation provides implicit regularization, which plays the role of Drop Path during training. We use a learning rate of 0.001 and the Adam optimier~\citep{adam} with a Cosine Decay Rule. For experiments with separate class tokens per task, we use a learning rate of 0.0005 for training the supernet, and 0.003 for training the task token. For each epoch, a minimum of 15 batches are drawn, with a batch size of 512. If the number of samples present in the task allows, we draw the maximum possible number of batches that covers the entire training data.

\textit{5-datasets Benchmark}: We use the same hyperparameters as those used in the VDD Benchmark, but train the supernet for 150 epochs.



\subsection{Evolutionary Search} 
The evolutionary search is run for 20 epochs. We use a population size of 50 and a mutation probability of 0.1. 25 candidates are generated by mutation, and 25 candidates are generated using crossover. The top 50 candidates are retained. The crossover is performed among the top 10 candidates. 

\subsection{Finetuning}
The target network for a task selected by the evolutionary search is finetuned for 30 epochs with a learning rate of 0.001, Adam optimizer, and a Cosine Learning Rate scheduler. Drop Path of 0.25 and label smoothing of 0.1 is used for regularization. We use a batch size of 512, and a minimum of 30 batches are drawn. When using a separate task token for each task, the task token is first finetuned with a learning rate of 0.001.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/v2/vdd-structure-seq2-v2.pdf}
    \caption{Architecture learned for task sequence ImNet, OGlt, C100, SVHN, UCF, GTSR, DPed, Flwr, Airc, DTD}
    \label{fig:vdd_arch_seq2} \vspace{-4mm}
\end{figure}\vspace{3mm}


% \textbf{Breakdown of Results on 5 Sequences of Different Task Orders on VDD.} 
% In Table \textcolor{blue}{2} of the submission, we report results of our proposed method based on 5 different task orders. We provide the breakdown of results with analyses here. 
% Table \ref{tab:task-order-acc-vdd} shows the individual accuracy when different task sequences are used. Table \ref{tab:task-seq-acc-comparison} shows the accuracy of each task averaged across 5 sequences. The relatively large standard deviation for UCF101, Aircraft, and DTD shows that our method is still sensitive to the task order. Further study of this phenomenon is left for future work.

% In addition to Figure \textcolor{blue}{4} in the submission, we show the remaining 4 ArtiHippo learned-to-grow with respect to different task orders in Figures ~\ref{fig:vdd_arch_seq1}, ~\ref{fig:vdd_arch_seq3}, ~\ref{fig:vdd_arch_seq4}, and ~\ref{fig:vdd_arch_seq5} respectively. 



% \begin{table*}[ht]
%     \centering
%     \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
% \toprule
% \multirow{2}{*}{\textbf{Sequence 1}} & Task & \textbf{ImNet} & \textbf{C100} &     \textbf{SVHN} &      \textbf{UCF} &  \textbf{OGlt} &     \textbf{GTSR} &  \textbf{DPed} &  \textbf{Flwr} &     \textbf{Airc} &   \textbf{DTD}  \\
% % \midrule
% & Acc & 79.38 &    90.96 &    95.87 &    82.38 & 84.74 &    99.92 & 99.91 & 89.41 &     49.5 & 50.32  \\
% \midrule
% % \midrule
% \multirow{2}{*}{\textbf{Sequence 2}} & Task & \textbf{ImNet} &     \textbf{OGlt} & \textbf{C100} &     \textbf{SVHN} &   \textbf{UCF} &     \textbf{GTSR} &  \textbf{DPed} &  \textbf{Flwr} &     \textbf{Airc} &   \textbf{DTD}  \\
% % \midrule
% & Acc & 79.38 &     85.2 &    91.22 &    95.82 & 81.92 &    99.95 & 99.91 & 86.76 &    54.31 & 53.88  \\
% % \midrule
% \midrule
% \multirow{2}{*}{\textbf{Sequence 3}} &  Task & \textbf{ImNet} &      \textbf{DTD} &     \textbf{Airc} &     \textbf{Flwr} &  \textbf{DPed} &     \textbf{GTSR} &   \textbf{UCF} &  \textbf{SVHN} & \textbf{C100} &  \textbf{OGlt}  \\
% % \midrule
% & Acc & 79.38 &    62.45 &    51.97 &    87.25 & 99.95 &    99.94 & 77.41 & 95.84 &    91.22 &  82.5  \\
% % \midrule
% \midrule
% \multirow{2}{*}{\textbf{Sequence 4}} & Task & \textbf{ImNet} &     \textbf{Airc} &     \textbf{GTSR} & \textbf{C100} &  \textbf{DPed} &     \textbf{SVHN} &  \textbf{OGlt} &   \textbf{UCF} &      \textbf{DTD} &  \textbf{Flwr}  \\
% % \midrule
% & Acc & 79.38 &    55.03 &    99.94 &    90.43 & 99.95 &    95.93 & 82.67 & 78.89 &    50.11 &   88.82  \\
% % \midrule
% \midrule
% \multirow{2}{*}{\textbf{Sequence 5}} & Task & \textbf{ImNet} &     \textbf{GTSR} &      \textbf{DTD} &     \textbf{Airc} &   \textbf{UCF} & \textbf{C100} &  \textbf{DPed} &  \textbf{OGlt} &     \textbf{Flwr} &  \textbf{SVHN}  \\
% % \midrule
% & Acc & 79.38 &    99.92 &    60.16 &     54.7 & 81.56 &    90.59 & 99.98 & 83.47 &    88.82 &   95.78  \\
% % \bottomrule
% \bottomrule
% \end{tabular}

%     \caption{Accuracy on each task with different task orders on the VDD dataset. Sequence 2 is used in Table \textcolor{blue}{2} in the submission.}
%     \label{tab:task-order-acc-vdd}
% \end{table*}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{c|c}
%   \toprule
%   \textbf{Task} & \textbf{Average Accuracy} \\
%   \midrule
%   CIFAR100 & $90.88 \pm 0.36$ \\
%   SVHN & $95.85 \pm 0.06$ \\
%   UCF & $80.43 \pm 2.17$ \\
%   Omniglot & $83.72 \pm 1.21$ \\
%   GTSR & $99.93 \pm 0.01$ \\
%   DPed & $99.94 \pm 0.03$ \\
%   VGG-Flowers & $88.21 \pm 1.14$ \\
%   Aircraft & $53.10 \pm 2.35$ \\
%   DTD & $55.38 \pm 5.67$ \\
%   \bottomrule
% \end{tabular}
%     \caption{Average Accuracy and standard deviation for each task, when evaluated in different sequences.}
%     \label{tab:task-seq-acc-comparison}
% \end{table}



% \begin{figure}[]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/architectures/seq3.pdf}
%     \caption{Architecture learned for task sequence ImNet, DTD, Airc, Flwr, DPed, GTSR, UCF, SVHN, C100, OGlet}
%     \label{fig:vdd_arch_seq3} \vspace{-4mm}
% \end{figure}

% \begin{figure}[]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/architectures/seq4.pdf}
%     \caption{Architecture learned for task sequence ImNet, Airc, GTSR, C100, DPed, SVHN, OGlt, UCF, DTD, Flwr}
%     \label{fig:vdd_arch_seq4} \vspace{-4mm}
% \end{figure}

% \begin{figure}[]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/architectures/seq5.pdf}
%     \caption{Architecture learned for task sequence ImNet, GTSR, DTD, Airc, UCF, C100, DPed, OGlt, Flwr, SVHN}
%     \label{fig:vdd_arch_seq5} \vspace{-4mm}
% \end{figure}



% \section{More Ablation Studies}
% \subsection{Effects on Network Architectures by the Exploration-Exploitation Tradeoff in Sampling}
% \label{sec:exploration-exploitation-ablations}

% We investigate the dynamics of neural architectures learned for different tasks. Through ablation experiments, we show that pure exploration encourages {\tt reuse} and {\tt adaptation}, and adding {\tt adapt} achieves the highest Average Accuracy. Exploitation, on the other hand, encourages more expert {\tt reuse}, {\tt skip}, and {\tt new}, and achieves worse accuracy but can find more dynamic structures through the application of {\tt skip} which are more efficient. Our proposed Exploration-Exploitation algorithm achieves the overall best trade off and finds more dynamic models through {\tt skip} experts at a slight drop in accuracy than pure exploitation. Table \ref{tab:exploration-vs-exploitation} shows a comparison of the Average Accuracy, number of parameters added by the lifelong learningalgorithm, and a breakdown of the types of experts added. All the hyperparameters for the search and finetuning process in all the methods is kept constant for fair comparison. Details of the hyperparameters can be found in Section \ref{sec:training-details}. 

% For ease of notation, we will denote the sampling strategies for supernet training and evolutionary search as $x$/$y$, where $x$ is the sampling strategy for supernet training, and $y$ is the strategy for initial population generation for the evolutionary search. Comparing column 3 (Explt/Explr) and column 4 (Explt/Explt), we can see that exploration in the evolutionary search adds more {\tt adapt} experts, and improves the Average Accuracy. Comparing Column 2 (Explr/Explr) and Column 3 (Explt/Explr), the number of {\tt adapt} experts increases with subsequently higher Average Accuracy when pure exploration is used in supernet training as well. This shows that exploration encourages {\tt adapt} experts, and that adding {\tt adapt} experts achieves higher Average Accuracy. Comparing Column 2 (Explr/Explr) and Column 4 (Explt/Explt), we can see that \texttt{reuse} can be achieved by either exploration or exploitation, but exploration encourages more \texttt{adaptation}. This is further validated by comparing Column 5 (Explr-Explt/Explr-Explt) and Column 6 (Explr-Explt/Explr), where Explr-Explt/Explr adds more {\tt adapt} experts and {\tt reuses} lesser number of experts, but achieves higher accuracy.

% \begin{table}[ht]
%     \centering
%     \resizebox{0.49\textwidth}{!}{
%     \begin{tabular}{p{3.2cm}|ccccc@{}}
%         \toprule
%         % & 1 & 2 & 3 & 4 & 5 \\ \midrule
%         Supernet Sampling        & Explr & Explt & Explt & Explr-Explt & Explr-Explt \\
%         \midrule
%         Evolutionary Search      & Explr & Explr & Explt & Explr-Explt & Explr \\
%         \midrule
%         Avg. Accuracy (\%)       & 83.27 & 82.50 & 82.29 & 82.84       & 82.98 \\
%         \midrule
%         \#Params Added (M) by {\tt New} and {\tt Adapt}      & 21.29 & 26.01 & 23.64 & 25.70       & 28.37 \\
%         \midrule
%         % Rel. Increase (\%)       & \cellcolor{blue1}{24.97} & \cellcolor{blue4}{30.50} & \cellcolor{blue2}{27.72} & \cellcolor{blue3}{30.15}       & \cellcolor{blue5}{33.28} \\
%         % \midrule
%         % \#Avg. Params Added & \multirow{2}{*}{2.37} & \multirow{2}{*}{2.89} & \multirow{2}{*}{2.76} & \multirow{2}{*}{2.86} & \multirow{2}{*}{3.15} \\
%         % per task (M) & & & & & \\        \midrule
%         \#Params Reduced (M) due to {\tt Skip}  & \multirow{2}{*}{7.09} & \multirow{2}{*}{14.18} & \multirow{2}{*}{16.55} & \multirow{2}{*}{14.18} & \multirow{2}{*}{4.73} \\
%         \midrule
%         \#{\tt Skip} experts  & 3     & 6     & 7     & 6           & 2     \\
%         \midrule
%         \#{\tt Reuse} experts      & 46    & 38    & 47    & 43          & 37    \\
%         \midrule
%         \#{\tt Adapt} experts & 46    & 40    & 28    & 31          & 42    \\
%         \midrule
%         \#{\tt New} experts   & 13    & 24    & 26    & 28          & 27    \\
%         \bottomrule
%     \end{tabular}}
%     \caption{Comparison of search strategies on the average accuracy on the VDD dataset along with the increase in parameter count.}
%     \label{tab:exploration-vs-exploitation}
% \end{table}

% Comparing Columns 2, 3, and 4 shows that incorporating exploitation in either supernet training or evolutionary search encourages more {\tt skip} experts, and subsequently enables finding more dynamic models at the cost of a slight drop in the accuracy. \textbf{Our proposed Exploration-Exploitation scheme in both the supernet training and the evolutionary search achieves the overall best tradeoff between accuracy and efficiency}.




% Add Learn to Grow details: Base model, search on each layer, DARTS

%% move to suppl
% \begin{figure*} [t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/acc_vs_forgetting.pdf}
%     \caption{Breakdown of individual task Accuracy and Average Forgetting of the finetuning components: 1, 4, 6 and 11 in Table~\ref{tab:acc_vs_forgetting}, which all are lightweight and have reasonable trade-off between Average Accuracy and Average Forgetting. Overall, the projection layer is selected favoring its consistently good accuracy and the forgetting can be handled with the proposed learning-to-grow ArtiHippo method.    
%     % Accuracy and Average Forgetting on each task. The plot shows that the Layer Normalization layers undergo the least forgetting. However, the Projection layers form the MHSA blocks gives the best accuracy. This suggests that the projection layers are the more suitable for lifelong learningif the weights are preserved.
%     See text for details.}
%     \label{fig:acc-vs-forgetting} \vspace{-3mm}
% \end{figure*}








%-------------------------------------------------------------------------







\subsection{Normalized Cosine Similarity}
\label{sec:normalized-cosine-similarity}
To verify the use of the Normalized Cosine Similarity as our similarity measure (Eqn. \textcolor{blue}{6} in the main text), we refer to Figure \ref{fig:similarities}. Figure \ref{subfig:cosine-sim} shows the Cosine Similarity between the mean class-tokens learned for tasks ImageNet, CIFAR100, SVHN, and UCF101 (in order), and the mean class-tokens calculated for each expert using the data from the current task Omniglot. Empirically, we observe that the Cosine Similarity between the mean class-tokens calculated using the data of the task associated with an expert and the mean class-token calculated with the current task in training is high. However, the difference between the similarity values for each expert are more important than the absolute values of the similarity. This difference can be increased by scaling the similarity such that it increases the magnitude difference between the similarities of different tasks, but maintains the relative similarity. This can be achieved by scaling the Cosine Similarities between -1 and 1 using the minimum and the maximum values from all the experts and all the blocks (Figure \ref{subfig:norm-cosine-sim}). Using the Normalized Cosine Similarity leads to better and more intuitive probability distributions for sampling candidate experts and the retention probabilities for the sampled experts. For example, comparing the probability values for sampling an expert at Block 6 calculated using Cosine Similarity (Figure \ref{subfig:expert-sampling}) vs. Normalized Cosine Similarity (Figure \ref{subfig:expert-sampling-normalized}), the probability of sampling the ImageNet expert increases, and those of sampling UCF101 anf Omniglot decrease. Similarly, for Blocks 3, 4 and 5, the retention probability calculated using the Normalized Cosine Similarity (Figure \ref{subfig:retention-probability-normalized}) reduces by a large factor than that calculated using the Cosine Similarity (Figure \ref{subfig:retention-probability}). This will encourage sampling the {\tt new} and the {\tt skip} experts.

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_cosine_similarity.png}
%         \caption{Cosine Similarity between the mean class-tokens from the previous tasks and mean class-token for each block.}
%         \label{subfig:cosine-sim}
%     \end{subfigure}
%     ~
%     \begin{subfigure}[t]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_cosine_similarity.png}
%         \caption{Normalized Cosine Similarity at each block calculated by scaling the Cosine Similarities between $-1$ and $1$.}
%         \label{subfig:norm-cosine-sim}
%     \end{subfigure}\vspace{-1.5mm}
%     ~
%     \begin{subfigure}[t]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_softmax.png}
%         \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Cosine Similarity}
%         \label{subfig:expert-sampling}
%     \end{subfigure}
%     ~
%     \begin{subfigure}[t]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_softmax.png}
%         \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Normalized Cosine Similarity}
%         \label{subfig:expert-sampling-normalized}
%     \end{subfigure}\vspace{-1.5mm}
%     ~
%     \begin{subfigure}[t]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_sigmoid.png}
%         \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Cosine Similarity.}
%         \label{subfig:retention-probability}
%     \end{subfigure}
%     ~
%     \begin{subfigure}[t]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_sigmoid.png}
%         \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Normalized Cosine Similarity.}
%         \label{subfig:retention-probability-normalized}
%     \end{subfigure}\vspace{1em}
%     \caption{Comparison of probability values for sampling the Experts (Middle row) and the retention probabilities (Bottom row) using the Cosine Similarity and the Normalized Cosine Similarity. Using the Normalized Cosine Similarity gives better probability values for $\psi_e$, as can be seen in Block 6. Using Normalized Cosine Similarity increases the probability of sampling the SVHN expert. The effect on the retention probability $\rho_e$ can be prominently seen on the Omniglot experts. The retention probability in Blocks 4 to 6 \ref{subfig:retention-probability-normalized} reduces below 0.5, which will encourage the {\tt new} and {\tt skip} experts to be trained and selected in the evolutionary search even if Omniglot experts were sampled.}
%     \label{fig:similarities}
% \end{figure}

\section{Learned architecture for different task order and pure exploration}
\label{sec:other-architectures}
Figure \ref{fig:vdd_arch_seq2} shows the architecture learned for a different task sequence. It can be seen that even with a different task sequence, the proposed method can learn to exploit task similarities. For example, a new projection layer is learned in Block 2 for Omniglot, which is adapted by SVHN and GTSRB. Figure \ref{fig:vdd_arch_seq1_exp} shows the architecture learned using a pure exploration strategy. It can be seen that pure exploration does not reuse components from similar tasks. For example, a large number of {\tt Adapt} and {\tt New} are added when learning CIFAR100. In contrast, the exploitation-exploitation strategy can learn to reuse the components from the ImageNet task (Figure \ref{fig:vdd_arch-sim}), and achieves better accuracy as well (Table \ref{tab:vdd-results}).

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/v2/vdd-structure-seq2-v2.pdf}
%     \caption{Architecture learned for task sequence ImNet, OGlt, C100, SVHN, UCF, GTSR, DPed, Flwr, Airc, DTD}
%     \label{fig:vdd_arch_seq2} \vspace{-4mm}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/v2/vdd-structure-exploration-v2.pdf}
    \caption{Architecture learned using pure exploration. Pure Exploration based method adds many unnecessary Adapt and New operations even though the tasks are similar (ImNet  C100), proving the effectiveness of the proposed sampling method}
    \label{fig:vdd_arch_seq1_exp} \vspace{-4mm}
\end{figure}

% between the mean class-tokens for the previous tasks and the mean class-token using the current task Omniglot at each block of the ViT for a task sequence ImageNet, CIFAR100, SVHN, UCF101.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_cosine_similarity.png}
        \caption{Cosine Similarity between the mean class-tokens from the previous tasks and mean class-token for each block.}
        \label{subfig:cosine-sim}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_cosine_similarity.png}
        \caption{Normalized Cosine Similarity at each block calculated by scaling the Cosine Similarities between $-1$ and $1$.}
        \label{subfig:norm-cosine-sim}
    \end{subfigure}\vspace{-1.5mm}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_softmax.png}
        \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Cosine Similarity}
        \label{subfig:expert-sampling}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_softmax.png}
        \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Normalized Cosine Similarity}
        \label{subfig:expert-sampling-normalized}
    \end{subfigure}\vspace{-1.5mm}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_sigmoid.png}
        \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Cosine Similarity.}
        \label{subfig:retention-probability}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_sigmoid.png}
        \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Normalized Cosine Similarity.}
        \label{subfig:retention-probability-normalized}
    \end{subfigure}\vspace{1em}
    \caption{Comparison of probability values for sampling the Experts (Middle row) and the retention probabilities (Bottom row) using the Cosine Similarity and the Normalized Cosine Similarity. Using the Normalized Cosine Similarity gives better probability values for $\psi_e$, as can be seen in Block 6. Using Normalized Cosine Similarity increases the probability of sampling the SVHN expert. The effect on the retention probability $\rho_e$ can be prominently seen on the Omniglot experts. The retention probability in Blocks 4 to 6 \ref{subfig:retention-probability-normalized} reduces below 0.5, which will encourage the {\tt new} and {\tt skip} experts to be trained and selected in the evolutionary search even if Omniglot experts were sampled.}
    \label{fig:similarities}
\end{figure}

\section{Preliminary Results on Extending the ArtiHippo for Class-Incremental Settings}
\label{sec:class-incremental}
The similarity based sampling method can be naturally extended to a class-incremental setting where the task indices are not available at test time. We calculate the similarity between the class token at each block of the ViT when performing predictions with an image. This is done for each task by activating the correct set of experts associated with the task. For each task, this generates $L$ similarity scores, where $L$ is the number of blocks in the transformer
\begin{equation}
    S_t^l = \text{CosineSim}(\mu_{e_t}, x_0^l)
\end{equation}
where $x_0^l$ is the class token at block $l$. Note that we do not normalize the cosine similarity. The similarity scores for all the blocks are combined to form an indicator score $I^t(x)$ per task. The test image is assigned the index of the task with the highest indicator score: $t \gets \text{argmax}_t\ I^t(x)$.

We explore two methods:

\begin{table}[t]
    \vspace{2em}
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{Average Accuracy} \\
        \cline{2-3}
        & 5-Datasets & VDD \\
        \midrule
        Max & $34.35 \pm 15.75$ & $32.96 \pm 2.07$ \\
        Minimum Entropy & $66.09 \pm 3.98$ & $45.69 \pm 1.96$ \\
        \bottomrule
    \end{tabular}
    \caption{Average Accuracy in a class-incremental setting using the proposed methods. The results for 5-datasets benchmark have been averaged over 5 different task orders, whereas the results on the VDD benchmark have been averaged over the same tasksequence but different random seeds.}
    \label{tab:task-agnostic-acc}
\end{table}

\textbf{Max}: For each task, the maximum similarity score from all the blocks is chosen as the indicator score
\begin{equation*}
    I^t(x) = \text{max}(S_{1:L}^t)
\end{equation*}

\textbf{Minimum Entropy}: For each task, the entropy using each path is calculated, and the task for which the classification head shows the minimum entropy is chosen.

% \textbf{Sum}: For each task, the sum of the similarity scores from all the blocks is chosen as the indicator.
% \begin{equation*}
%     I^t(x) = \sum_{l=1}^LS_l^t
% \end{equation*}

% \textbf{Top-3 Sum}: For each task, three blocks with the highest similarity scores are chosen, and the indicator is calculated by summing the similarity scores.
% \begin{equation*}
%     I^t(x) = \sum_{j=1}^3 \text{SortDescending}(S_{1:L}^t)
% \end{equation*}

% \textbf{Majority Voting}: The expert with the maximum similarity score is chosen at each block. The task index is inferred by taking the task with which the maximum number of experts are associated.

Table \ref{tab:task-agnostic-acc} shows the average accuracies obtained on the 5-datasets benchmark. The low accuracy scores suggest that although the mean class token is suitable for sampling in the supernet training and the evolutionary search, its applicability is limited for task agnostic inference. However, the results are promising, and could be made better by introducing a mixture of class-tokens per task to account for the variations in tasks, similar in spirit to the mixture modeling of experts of ArtiHippo and the learn-to-prompt method~\citep{learning-to-prompt}, which we leave for future work. %exploring other network components for finding task similarities.

% \begin{table}[t]
%     \vspace{2em}
%     \centering
%     \begin{tabular}{c|cc}
%     \toprule
%         \multirow{2}{*}{Method} & \multicolumn{2}{c}{Average Accuracy} \\
%         \cline{2-3}
%         & 5-Datasets & VDD \\
%         \midrule
%         Max & $34.35 \pm 15.75$ & $32.96 \pm 2.07$ \\
%         Minimum Entropy & $66.09 \pm 3.98$ & $45.69 \pm 1.96$ \\
%         \bottomrule
%     \end{tabular}
%     \caption{Average Accuracy in a class-incremental setting using the proposed methods. The results for 5-datasets benchmark have been averaged over 5 different task orders, whereas the results on the VDD benchmark have been averaged over the same tasksequence but different random seeds.}
%     \label{tab:task-agnostic-acc}
% \end{table}

\end{document}
