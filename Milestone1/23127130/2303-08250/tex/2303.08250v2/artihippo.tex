\documentclass{article}


\usepackage[numbers]{natbib}




    \usepackage[preprint]{neurips_2023}






\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{wrapfig} %
\usepackage{graphicx} %
\usepackage{amsmath} %
\usepackage{amssymb} %
\usepackage{enumitem} %
\usepackage{multirow}
\usepackage{multicol}
\usepackage{ulem} %
\usepackage{sidecap}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}


\definecolor{skip}{HTML}{F2ACCA}
\definecolor{reuse}{HTML}{9CCEA7}
\definecolor{adapt}{HTML}{FEB24C}
\definecolor{new}{HTML}{9EC9E2}

\newcommand{\subfiggrid}[5]{
    \begin{subfigure}[t]{0.15\linewidth}
        \centering
        \includegraphics[width=#3cm,height=#4cm]{figures/#1_samples/#2/000001.jpg}
        \includegraphics[width=#3cm,height=#4cm]{figures/#1_samples/#2/000002.jpg}
        \caption{#5}
    \end{subfigure}
}


\title{Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning}


\author{%
  Chinmay Savadikar \\
  Department of ECE\\
  North Carolina State University\\
  \texttt{csavadi@ncsu.edu} \\
  \And
  Michelle Dai \\
  Operations Research \& Financial Engineering\\
  Princeton University\\
  \texttt{mdai@princeton.edu} \\
  \And
  Tianfu Wu \\
  Department of ECE \\
  North Carolina State University \\
  \texttt{tianfu\_wu@ncsu.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by  Hippocampi. With the dominance of Transformers in deep learning, it is a pressing need to explore what would be, and how to implement, Artificial Hippocampi (ArtiHippo) in Transformers. This paper presents a method of learning to grow ArtiHippo in Vision Transformers (ViTs) for resilient lifelong learning. We study four aspects: (i) Where to place ArtiHippo in ViTs to enable plasticity while preserving the core function of ViTs at streaming tasks? (ii) What representational scheme to use to realize ArtiHippo to ensure expressivity and adaptivity for tackling tasks of different nature in lifelong learning? (iii) How to learn to grow ArtiHippo to exploit task synergies and to overcome catastrophic forgetting? (iv) How to harness the best of our proposed ArtiHippo and prompting-based approaches? In experiments, the proposed method is tested on the challenging Visual Domain Decathlon (VDD) benchmark and the recently proposed 5-Dataset benchmark. It obtains consistently better performance than the prior art with sensible ArtiHippo learned continually.
\end{abstract}


\setlength{\abovecaptionskip}{0pt}
\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{1pt}
\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{1pt} 

\section{Introduction}
\label{sec:intro} %
Developing lifelong learning machines is one of the hallmarks of AI, to mimic human intelligence in terms of learning-to-learn to be adaptive and skilled at streaming tasks. However, state-of-the-art machine (deep) learning systems realized by Deep Neural Networks (DNNs) are not yet intelligent in the biological sense from the perspective of lifelong learning, especially plagued with the critical issue known as \textit{catastrophic forgetting} at streaming tasks in a dynamic environment~\citep{mccloskey,thrun}. 
Catastrophic forgetting means that these systems ``forget" how to solve old tasks after sequentially and continually trained on a new task using the data of the new task only. 
Addressing catastrophic forgetting in lifelong learning is a pressing need with potential paradigm-shift impacts in the next wave of trustworthy and/or brain-inspired AI. 



To address catastrophic forgetting, one direct methodology is to utilize exemplar-based settings (i.e., \textit{Experience Replay})~\citep{gradient-based-sample-selection,hayes-icra,large-scale-inc-learning} in which a small number of selected training samples stored for each previous task is used in conjunction with the data of a new task in training the model for the new task.
Although shown as an effective strategy, retaining raw data samples may induce issues on data security and privacy, as well as the long-run sustainability.

To develop exemplar-free methods, three main strategies have been studied in the literature. The first is to regularize the change in model parameters when trained on a new task as done in~\citep{kirkpatrick-overcoming}.  The second is to adapt the structure of a network (with the parameters) for the new task from the network learned for the previous tasks, as done in Learn to Grow (L2G)~\citep{learn-to-grow} which uses differentiable Neural Architecture Search (NAS) to find whether to {\tt reuse}, {\tt adapt} or {\tt renew} each layer of a Multi-Layer Percetpron (MLP) or a convolultional neural network (CNN). More recently, with the availability of powerful pretrained Transformer~\citep{attention-is-all-you-need} based Large Foundation Models (LFMs)~\citep{bommasani2021opportunities} (such as the CLIP models~\citep{clip}), the third is to freeze pretrained LFMs and then to learn prompts (or task tokens) appended to the input tokens instead for lifelong learning, i.e. prompting-based methods \citep{learning-to-prompt,dualprompt,s-prompts}.

\begin{wrapfigure} {r}{0.48\textwidth} \vspace{-4mm}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/artihippo_flow-v3.pdf}
    \caption{Illustration of the proposed method for task-incremental lifelong learning without any  catastrophic forgetting. \textit{Left:} The Multi-Head Self-Attention (MHSA) block in Vision Transformers~\citep{vit} with the proposed Artificial Hippocampi (ArtiHippo) replacing the original linear projection layer. \textit{Middle:} The ArtiHippo growing is maintained by four operations, similar in spirit to the Learn to Grow \citep{learn-to-grow}.      
    \textit{Right:} The ArtiHippo is represented by a mixture of experts with an example for different tasks (e.g., $j$) started from the $i$. %
    }
    \label{fig:flow}\vspace{-2mm}
\end{wrapfigure}  

In this paper, we are interested in studying examplar-free resilient lifelong learning with Vision Transformers (ViTs)~\citep{vit}. Although shown to be data efficient, prompting-based methods may lack the plasticity needed in resilient lifelong learning, increase the cost (due to the quadratic complexity of Transformer models with respect to the number of tokens),  and unnecessarily enforce all encountered tasks (regardless of their underlying difficulty) to use the same frozen network. We are motivated by some observations of natural intelligence possessed by biological systems (e.g., the human brain) which exhibit remarkable capacity of learning and adapting their structure and function for tackling different tasks throughout their lifespan, while retaining the stability of their core functions. Our goal is to seek alternative formulations that are not built on completely frozen pretrained Transformer models, but can induce plastic and reconfigurable structures for streaming tasks. It has been observed in neuroscience that learning and memory are entangled together in a highly sophisticated way~\citep{christophel2017distributed,voitov2022cortical}. In this paper, we think of learning model parameters as a process of interacting with the sensory information (data) to convert Short-Term Memory (activations) into a Long-Term Memory (learned parameters), and selectively adding task-specific parameters as expanding the memory. With this analogy, we study the limitations of prompting-based approaches under lifelong learning settings, and seek more integrative mechanisms that selectively induce learnable parts into Transformers (in contrast to entirely freezing) to induce reconfigurability, selectivity, and plasticity for lifelong learning. We term our framework \textbf{ArtiHippo}, which stands for Artificial Hipppocampi, after the hippocampal system which plays an important role in converting short-term memory into Long-Term Memory (LM) for lifelong learning. 

To this end, we adapt the Learn-to-Grow (L2G) framework~\citep{learn-to-grow}, but do not apply NAS uniformly across all the layers of a ViT. Instead, we aim to find a lightweight component which can preserve and interact with the stable components (e.g., the MHSA) of a ViT, while inducing plasticity in a lifelong learning setting, i.e., a growing memory. As illustrated in Fig.~\ref{fig:flow}, the final projection layer in the multi-head self-attention (MHSA) block of a ViT is identified and selected as the ArtiHippo (Sec.~\ref{sec:identify_artihippo}). The learn-to-grow NAS is only applied in maintaining ArtiHippo layers, while other components are frozen to maintain the stability of core functions, as illustrated in Fig.~\ref{fig:nas}. Rather than adopting the DARTS~\citep{darts} NAS used in~\citep{learn-to-grow}, the learn-to-grow NAS in this paper is built on the single-path one-shot (SPOS) NAS~\citep{spos}, in which we propose a hierarchical exploration-exploitation sampling (Fig.~\ref{fig:nas-sampling}) strategy for lifelong learning (Sec.~\ref{sec:grow_artihippo}).




In experiments,  this paper considers lifelong learning with task indices available in both training and inference, which is often referred to as \textit{task-incremental setup}\citep{types-of-inc-learning}. When tasks consist of data from different domains such as the Visual Domain Decathlon (VDD) benchmark~\citep{vdd}, it is also related to domain-incremental setup, but without assuming the same output space between tasks.
The right of Fig.~\ref{fig:flow} illustrates an example of learned ArtiHippo. With the task indices, the execution of the computational graph for a given task is straightforward. The proposed method achieves zero-forgetting on old tasks. 
We show the potential of applying our proposed method for class-incremental lifelong learning settings, especially when integrated with the complementary L2P methods. 

\textit{Experience Replay Based approaches} aim to retain some exemplars from the previous tasks and replay them to the model along with the data from the current task~\citep{gradient-based-sample-selection,mir,large-scale-er,rainbow-memory,hindsight,agem,continual-prototype-evolution,remind,gem,gdumb,icarl,tiny-replay,hayes-icra,dark-experience-replay,large-scale-inc-learning,fast-and-slow,contrastive-continual-learning}. Instead of storing raw exemplars, \textit{Generative replay methods}~\citep{generative-replay,gan-memory} learn the generative process for the data of a task, and replay exemplars sampled from that process along with the data from the current task. For exemplar-free continual learning, \textit{Regularization Based approaches} explicitly control the plasticity of the model by preventing the parameters of the model from deviating too far from their stable values learned on the previous tasks when learning the current task~\citep{what-not-to-forget,selfless-sequential-learning,podnet,variational-continual-learning,kirkpatrick-overcoming,lwf,synaptic-intelligence,progress-and-compress}. Both these approaches aim to balance the stability and plasticity of a fixed-capacity model.

\textit{Dynamic Models} aim to use different parameters per task to avoid use of stored exemplars. Dynamically Expandable Network~\citep{dynamic-expandable-nets} adds neurons to a network based on learned sparsity constraints and heuristic loss thresholds. PathNet~\citep{pathnet} finds task-specific submodules from a dense network, and only trains submodules not used by other tasks. Progressive Neural Networks~\citep{pnn} learn a new network per task and adds lateral connections to the previous tasks' networks.~\citep{vdd} learns residual adapters which are added between the convolutional and batch normalization layers. \citep{network-of-experts} learns an expert network per task by transferring the expert network from the most related previous task. Learn to Grow~\citep{learn-to-grow} uses Differentiable Architecture Search (DARTS~\citep{darts}) to determine if a layer can be reused, adapted, or renewed (3 fundamental skills: {\tt reuse}, {\tt adapt}, {\tt new}) for a task. Our approach is most closely related to Learn to Grow~\citep{learn-to-grow} which can also be interpreted as a Mixture of Experts framework. \citet{task-driven-priors} use task priors derived from a task similarity measure and use those to train a stochastic network and retain some layers of the most similar task, and retrain other layers. \citep{wang-task-difficulty-aware} use a task difficulty metric and threshold hyperparameters to either impose regularization constraints on the previous network, to use the same architecture as the previous tasks, or learn an entirely new architecture and parameters using NAS. Although similar to our method, they rely on (fixed) manually chosen threshold, whereas our method does not have any such heuristics. Dynamic models have also been explored for efficient transfer learning~\citep{nettailor,spottune,piggyback}.

Recently, there has been increasing interest in lifelong learning using Vision Transformers~\citep{learning-to-prompt,dualprompt,meta-attention,pool-of-adapters,dytox,towards-exemplar-free-continual-learning-vits,improving-vits,continual-obj-det-kd,memory-transformer,s-prompts}. \textit{Prompt Based approaches} learn external parameters that encode task-specific information useful for classification~\citep{learning-to-prompt,s-prompts,dytox}. Learning to Prompt (L2P)~\citep{learning-to-prompt} learns a pool of prompts and uses a key-value based retrieval to infer the task index and retrieve the correct set of prompts at test time. DualPrompt~\citep{dualprompt} learns generic and task-specific prompts and extends Learning to Prompt.~\citep{meta-attention} uses a ViT pretrained on ImageNet and learns binary masks to enable/disable parameters of the Feedforward Network (FFN), and the attention between image tokens for downstream tasks.

\textbf{Our Contributions} 
We make four main contributions to the field of lifelong learning with ViTs. (i) We propose and identify ArtiHippo in ViTs, i.e., the final projection layers of the multi-head self-attention blocks in a ViT, to realize a long-term task-similarity-oriented memory mechanism for resilient lifelong learning. We also present a new usage for the class-token in ViTs as the memory growing guidance. (ii) We present a hierarchical task-similarity-oriented exploration-exploitation-sampling-based NAS method for learning to grow ArtiHippo continually with respect to four basic growing operations: {\tt Skip}, {\tt Reuse}, {\tt Adapt}, and {\tt New} to overcome catstrophic forgetting. (iii) We are the first, to the best of our knowledge, to evaluate lifelong learning with ViTs on the large-scale, diverse and imbalanced VDD benchmark~\citep{vdd} with strong empirical performance obtained. Although L2P \citep{learning-to-prompt} evaluates on the 5-Datasets benchmark (which contains tasks from diverse domains), each task therein has abundant data and is easy to learn in isolation. In contrast, the VDD dataset contains tasks from varied domains, many of which contain very few labelled samples. (iv) We show that our method is complementary to prompting-based approaches, and combining the two leads to even higher performance.


\section{Approach}%
In this section, we first present the ablation study on identifying the ArtiHippo in a ViT block (Fig.~\ref{fig:flow}). Then, we present details of learning to grow ArtiHippo (Figs.~\ref{fig:nas} and~\ref{fig:nas-sampling}). 

\begin{wraptable} {r}{0.5\textwidth}\vspace{-4mm}
    \centering
    \resizebox{0.5\textwidth}{!}{
        \begin{tabular}{l|l|c|c}
            \toprule
            Index & Finetuned Component &  Avg. Accuracy & Avg. Forgetting \\
            \midrule
            1 & $\text{LN}_1$ + $\text{LN}_2$ & $81.76$ & $21.24$ \\
            \midrule
            2 & $\text{FFN}$ & $84.20$ & $44.76$ \\
            3 & $\text{MLP}^{\text{d}}$ & $83.66$ & $37.99$ \\
            4 & $\text{LN}_2$ & $80.04$ & $16.35$ \\
            \midrule
            5 & MHSA + $\text{LN}_1$ & $85.26$ & $54.38$ \\
            6 & LN$_1$ & $81.18$ & $19.04$ \\
            7 & Query & $81.57$ & $19.69$ \\
            8 & Key & $81.56$ & $19.19$ \\
            9 & Query+Key & $81.49$ & $31.10$ \\
            10 & Value & $84.99$ & $37.58$ \\
            11 & Projection  (ArtiHippo) & $85.11$ & $30.50$ \\
            \midrule
            \multicolumn{2}{c|}{Classifier w/ Frozen Backbone} & $70.78$ & - \\
            \bottomrule
        \end{tabular}
    }
    \caption{\small Ablation study on identifying the ArtiHippo in a Transformer block (Eqns.~\ref{eq:mhsa_proj} and~\ref{eq:ffn}). See text for detail. 
    The last row shows the result of a conventional transfer learning setting in which only the head classifier is trained.
    }
    \label{tab:acc_vs_forgetting} %
\end{wraptable}

\subsection{Identifying ArtiHippo in Transformers}\label{sec:identify_artihippo}%
\label{identifying-artihippo}
The left of Fig.~\ref{fig:flow} shows a Vision Transformer (ViT) block \citep{vit}. Without loss of generality, denote by $x_{L,d}$ an input sequence consisting of $L$ tokens encoded in a $d$-dimensional space. In ViTs, the first token is the so-called {\tt class-token}. The remaining $L-1$ tokens are formed by patch embedding of an input image, together with additive positional encoding. A ViT block can be expressed as, 
\begin{align}
    z_{L, d} & = x_{L, d} + \text{Proj}(\text{MHSA}(\text{LN}_1(x_{L, d}))) \label{eq:mhsa_proj} \\
    y_{L, d} & = z_{L, d} + \text{FFN}(\text{LN}_2(z_{L, d})))) \label{eq:ffn}
\end{align}
where $\text{Proj}(\cdot)$ is a linear transformation fusing the multi-head outputs from the MHSA module. 

The MHSA realizes the dot-product self-attention between Query and Key, followed by aggregating with Value, where Query/Key/Value are linear transformatons of the input token sequence. The FFN is often implemented by a multi-layer perceptron (MLP) with a feature expansion layer $\text{MLP}^{\text{u}}$ and a feature reduction layer $\text{MLP}^{\text{d}}$ with a nonlinear activation function (such as GELU) in the between.




For resilient task-incremental lifelong learning using ViTs, \textbf{our very first step is to investigate whether there is a simple yet expressive ``sweet spot" in the Transformer block that plays the functional role of  Hippocampi in the human brain (i.e., ArtiHippo)}, that is converting short-term streaming task memory into long-term memory to support lifelong learning without catastrophic forgetting. The proposed identification process is straightforward. Without introducing any modules handling forgetting, we compare both the task-to-task forward transferrability and the sequential forgetting of different components in a Transformer block. Our intuition is that a desirable ArtiHippo component must enable strong transferrability with manageable forgetting. 

To that end, we use the 10 tasks in the VDD benchmark~\citep{vdd}. We first compare the transferrability of the ViT trained with the first task, ImageNet to the remaining 9 tasks in a pairwise task-to-task manner and compute the average Top-1 accuracy on the 9 tasks. Then, we start with the ImageNet-trained ViT, and train it on the remaining 9 tasks continually and sequentially in a predefined order with the average forgetting~\citep{riemannian-walk} on the first 9 tasks (including ImageNet) compared.
As shown in Table~\ref{tab:acc_vs_forgetting}, we compare 11 components or composite components across all blocks in the ImageNet-pretrained ViT. 


Denote by $T_1, T_2, \cdots, T_{N}$ a sequence of $N$ tasks (e.g., $N=10$ in the VDD benchmark). A model consists a feature backbone and a task head classifier. 
Let $f_{T_{n|1}}$ be the backbone trained for the task $n$ ($n=2, .. N$) with weights initialized from the model of task $1$, and the learned head classifier $C_n$ from scratch. The average transfer learning accuracy of the first task model to the remaining $N-1$ tasks is defined as:
\begin{equation}
    A_{N} = \frac{1}{N-1}\sum_{n=2}^N \text{Acc}(T_n; f_{T_{n|1}, C_n}) \label{eq:avg_acc}
\end{equation}
where $\text{Accuracy}()$ uses the Top-1 accuracy in classification.

Let $f_{T_{1:n}}$ be the backbone trained sequentially after task $T_n$ and and $C_n$ the head classifier trained for task $T_n$. Denote by $a_{n,i}=\text{Accuracy}(T_i; f_{T_{1:n}}, C_i)$, the accuracy on the task $i$ using the backbone that has been trained on tasks from $1$ to $n$ ($i<n$).
The average forgetting on the first $N-1$ tasks is:
\begin{equation}
    \mathbb{F}_N = \frac{1}{N-1}\sum_{n=1}^{N-1} \left(\max_{j\in [n, N-1]} a_{j,n} - a_{N,n}\right)
    \label{eq:avg_forgetting}
\end{equation}



From Table~\ref{tab:acc_vs_forgetting}, \textbf{the following observations lead us to select the projection layer as ArtiHippo}:

(i) Continually finetuning the entire MHSA block (i.e., MHSA+LN$_1$) obtains the best average accuracy, which has been observed in~\citep{touvron2022three} in terms of finetuning ImageNet-pretained ViTs on downstream tasks. However, \citep{touvron2022three} does not consider lifelong learning settings, and as shown here finetuning the entire MHSA block incurs the highest average forgetting, which means that it is task specific. 



(ii)  Continually finetuning the entire FFN block (i.e., MLP$^{\text{down}}$+MLP$^{\text{up}}$+LN$_2$) has a similar effect as finetuning the entire MHSA block. In the literature,  the Vision Mixture of Expert framework~\citep{vision-moe} where an expert is formed by an entire MLP block takes advantage of the high average performance preservation. 



(iii)  In lifelong learning scenarios, maintaining either the entire MHSA block or the entire FFN block could address the catastrophic forgetting, but at the expense of high model complexity and heavy computational cost in both learning and inference. 


(iv) The final projection layer and the Value layer in the MHSA block, which have been overlooked, can maintain high average accuracy (as well as manageable average forgetting, to be elaborated). It is also much more ``affordable" to maintain it in lifelong learning, especially with respect to the four basic growing operations ({\tt skip}, {\tt reuse}, {\tt adapt} and {\tt new}). Intuitively, the final projection layer is used to fuse multi-head outputs from the self-attention module. In ViTs, the self-attention module is used to mix/fuse tokens spatially and it has been observed  in MetaFormers~\citep{yu2022metaformer,yu2022metaformerbaseline} that simple local average pooling and even random mixing can perform well. So, it makes sense to keep the self-attention module frozen from the first task (at worst it can play the role of a random mixing operation for a new task) and maintain the projection layer to fuse the outputs. However, the Value layer is implemented as a parallel computation along with the Key and Query, which makes it inefficient to incorporate into the Mixture of Experts framework.

In sum, due to the strong forward transfer ability, manageable forgetting, maintaining simplicity and for less invasive implementation in practice, we select the Projection layer (instead of the Value layer) in the MHSA block as ArtiHippo to develop our proposed long-term task-similarity-oriented memory based lifelong learning. Through ablation studies, we verify that using the Projection layer and the Value layer show comparable performance. We also show that using other layers of the ViT leads to significantly worse performance, thus empirically validating our hypothesis.

\subsection{Learning to Grow ArtiHippo Continually}
\label{sec:grow_artihippo}%
This section presents details of learning to grow ArtiHippo based on NAS. As illustrated in Fig.~\ref{fig:nas},  for a new task $t$ given the network learned for the first $t-1$ tasks, it consists of three components: the Supernet construction (the parameter space of growing ArtiHippo), the Supernet training (the parameter estimation of growing ArtiHippo), and the 
target network selection and finetuning (the consolidation of the ArtiHippo for the task $t$).



\begin{wrapfigure}{r}{0.5\textwidth} \vspace{-10mm}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/nas_v2.pdf}
    \caption{Illustration of ArtiHippo growing via NAS using the four learning-to-grow operations in lifelong learning. See text for details. }
    \label{fig:nas}\vspace{-2mm}
\end{wrapfigure}
\subsubsection{Supernet Construction}\label{sec:supernet_construction}%

We start with a vanilla $D$-layer ViT model (e.g., the 12-layer ViT-Base)~\citep{vit} and train it on the first task (e.g., ImageNet in the VDD benchmark~\citep{vdd}) following the conventional recipe. 
The proposed ArtiHippo is represented by a mixture of experts, similar in spirit to~\citep{vision-moe}. After the first task, the ArtiHippo at the $l$-th layer in the ViT model consists of a single expert which is defined by a tuple, $E^{l, 1}=(P^{l, 1}, \mu^{l, 1})$,  where $P^{l, 1}$ is the projection layer and $\mu^{l, 1}\in R^d$ is the associated mean class-token pooled from the training dataset after the model is trained. Without loss of generality, we consider how the growing space of ArtiHippo is constructed at a single layer, assuming the current ArtiHippo consists of two experts, $\{E^{l, 1}, E^{l, 2}\}$ (Fig.~\ref{fig:nas}, left). Inspired by the Learn to Grow \citep{learn-to-grow}, we utilize four operations in the Supernet construction: 

\begin{itemize}[leftmargin=*]
    \itemsep0em
    \item {\tt skip}: Skips the entire MHSA block (i.e., the hard version of the drop-path method widely used in training ViT models), which encourages the adaptivity accounting for the diverse nature of tasks. 
    \item {\tt reuse}: Uses the projection layer from an old task for the new task unchanged (including associated mean class-token), which will help task synergies in learning. 
    \item {\tt adapt}: Introduces a new lightweight layer on top of the projection layer of an old task, implemented by a MLP with one squeezing hidden layer, and a new mean class-token computed at the added adapt MLP layer. We propose a hybrid adapter which acts as a plain 2-layer MLP during searcch, and a residual MLP during finetuning (details in the Appendix \ref{sec:hybrid-adapter}).
    \item {\tt new}: Adds a new projection layer and a mean class-token, enabling the model to handle corner cases and novel situations.
\end{itemize}\vspace{-4mm}

The bottom of Fig.~\ref{fig:nas} shows the growing space. The Supernet is constructed by {\tt reusing} and {\tt adapting} each existing expert at layer $l$, and adding a {\tt new} and a {\tt skip} expert. The newly added {\tt adapt} MLPs and projection layers will be trained from scratch using the data of a new task only. 

\begin{wrapfigure}{r}{0.5\textwidth}\vspace{-4mm}
    \centering
    \includegraphics[width=\linewidth]{figures/sampling_process-v4.pdf}
    \caption{
    Illustration of the proposed exploration-exploitation sampling strategy used in facilitating the SPOS NAS~\citep{spos} for lifelong learning. It harnesses the best of the vanilla  pure exploration strategy (left) and the proposed exploitation strategy (right) using a simple epoch-wise scheduling. See text for details.
    }
    \label{fig:nas-sampling} \vspace{-3mm}
\end{wrapfigure}

\subsubsection{Supernet Training}%
To train the Supernet constructed for a new task $t$, 
we build on the SPOS method \citep{spos} due to its efficiency. The basic idea of SPOS is to sample a single-path sub-network from the Supernet by sampling an expert at every layer in each iteration (mini-batch) of training. One key aspect is the sampling strategy. The vanilla SPOS method uses uniform sampling (i.e., the \textit{pure exploration} strategy, Fig.~\ref{fig:nas-sampling} top), which has the potential of traversing all possible realizations of the mixture of experts of the ArtiHippo in the long run, but may not be desirable (or sufficiently effective) in a lifelong learning setup because it ignores inter-task similarities/synergies.

To overcome this, we propose an exploitation strategy (Fig.~\ref{fig:nas-sampling} bottom), which utilizes a hierarchical sampling method that forms the categorical distribution over the operations in the search space explicitly based on task similarities. 
We first present details on the task-similarity oriented sampling in this section. Then we show a simple exploration-exploitation integration strategy to harness the best of the two in Sec.~\ref{sec:balancing}. 

\textbf{Task-Similarity Oriented Sampling}: Let $\mathbb{E}^l$ be the set of Experts at the $l$-the layer learned till task $t-1$. For each candidate expert $e\in \mathbb{E}^l$, we first compute the mean class-token for the $t$-task, $\hat{\mu}_e^{t}$ using the current model, and compute the task similarity between the $t$-th task and the Expert $e$ as
$S_e(t) = \text{{\tt NormCosine}}(\hat{\mu}_e^{t}, \mu_e)$,
where {\tt NormCosine}$(\cdot, \cdot)$ is the Normalized Cosine Similarity, which is calculated by scaling the Cosine Similarity score between $-1$ and $1$ using the minimum and the maximum Cosine Similarity scores from all the experts in all the MHSA blocks of the ViT. This normalization is necessary to increase the difference in magnitudes of the similarities between tasks, which results in better Expert sampling distributions during the sampling process in our experiments.


The categorical distribution is then computed via Softmax across all the scores of all the Experts at a layer. The probability of sampling a candidate Expert $e\in \mathbb{E}^l$ is defined by 
$\psi_e = \frac{\exp(S_e(t))}{\sum_{e'\in \mathbb{E}^l} \exp(S_{e'}(t))}$. 
With an Expert $e$ sampled (with a probability $\psi_e$), we further compute its retention Bernoulli probability via a Sigmoid transformation of the task similarity score defined by $\rho_e = \frac{1}{1+\exp(-S_e(t))}$.



If the Expert $e$ is retained (with a probability $\rho_e$), we further use the same Bernoulli probability to sample the two operations, {\tt Reuse} with probability $\rho_e$ and {\tt Adapt} with probability $1-\rho_e$. If the Expert $e$ is ignored (with probability $1-\rho_e$), we randomly sample the two operations: {\tt Skip} and {\tt New} with a probability 0.5. 

\subsubsection{Target Network Selection and Finetuning}%
\label{sec:target-network-selection}
After the Supernet is trained, we adopt the same evolutionary search used in the SPOS method~\citep{spos} based on the proposed hierarchical sampling strategy. The evolutionary search is performed on the validation set to select the path which gives the best validation accuracy. After the target network for a new task is selected, we retrain the newly added layers by the {\tt New}  and {\tt Adapt} operations from scratch (random initialization), rather than keeping or warming-up from the weights from the Supernet training. This is based on the observations in network pruning that it is the neural architecture topology that matters and that the warm-up weights may not need to be preserved to ensure good performance on the target dataset~\citep{liu2018rethinking}. %

\subsubsection{Balancing Exploration and Exploitation:}\label{sec:balancing}%
As illustrated in Fig.~\ref{fig:nas-sampling}, to harness the best of the pure exploration strategy and the proposed exploitation strategy, we apply epoch-wise exploration and exploitation sampling for simplicity. At the beginning of an epoch in the Supernet training, we choose the pure exploration strategy with a probability of $\epsilon_1$ (e.g., 0.3), and the hierarchical sampling strategy with  a probability of $1-\epsilon_1$. Similarly, when generating the initial population during the evolutionary search, we draw a candidate target network from a uniform distribution over the operations with a probability of $\epsilon_2$, and from the hierarchical sampling process with a probability of $1-\epsilon_2$, respectively. In practice, we set $\epsilon_2 > \epsilon_1$ (e.g., $\epsilon_2=0.5$) to encourage more exploration during the evolutionary search, while encouraging more exploitation for faster learning in the Supernet training. Our experiments show that this exploration-exploitation strategy achieves higher Average Accuracy and results in a lower parameter increase than pure exploration (Fig. \ref{fig:epochs-vs-acc} in the Appendix).

\subsection{Integrating ArtiHippo with Learning to Prompt}
\label{sec:combining-with-prompts}%
Since prompting-based methods ~\citep{learning-to-prompt,dualprompt,s-prompts,dytox} are complimentary to our proposed method, we propose a simple method for harnessing the best of both, and show that this leads to further improvement.  
At the beginning of the Supernet training, a task-specific classification token is learned using the ImageNet backbone (similar to S-Prompts~\citep{s-prompts}). Then, instead of using the {\tt cls} token from the ImageNet task, we used the learned task token during NAS. When finetuning the learned architecture, we first train the task token using the fixed ImageNet backbone, and then use this trained token to train the architecture components. 

\section{Experiments}%
In this section, we test the proposed method on two benchmarks and compare with the prior art. We evaluate our method in the task-incremental setting, where each task contains a disjoint set of classes and/or domains and task index is available during inference. The proposed method obtains better performance than the prior art in comparisons.  \textbf{Our PyTorch source code is provided with the supplementary material.} Due to space limitations, we provide the implementation details in the Appendix. We use 1 Nvidia Quadro RTX 8000 GPU for experiments on the VDD and 5-Dataset benchmarks. We also perform initial experiments for evaluating the potential of our method for large scale transfer learning from a CLIP pretrained model to ImageNet, described in the Appendix \ref{sec:clip-imnet}.

\textbf{Data and Metrics}:
We evaluate our approach on the Visual Domain Decathlon (VDD) datasets~\citep{vdd} and the 5-Datasets benchmark introduced in\citep{adversarial-continual-learning}. Each of the individual dataset in these two benchmarks are treated as separate non-overlapping tasks. The VDD benchmark is challenging because of the large variations in tasks as well as small number of samples in many tasks, which makes it a favorable for evaluating lifelong learning algorithms. \textit{Details of the benchmarks are provided in the Appendix}.
Since catastrophic forgetting is fully addressed by our method, we evaluate the performance of our method using the average accuracy defined by
\begin{equation}
\mathbf{A}_N = \frac{1}{N}\sum_{i=n}^N a_{N,n}, \label{eq:avg_accuracy_lll}    
\end{equation} 
where $N$ is the total number of tasks, and $a_{n,i}=\text{Accuracy}(T_i; f_{T_{1:n}}, C_i)$ %

\textbf{Baselines:}
We compare with Learn to Grow (L2G) \citep{learn-to-grow}, Learning to Prompt (L2P)  \citep{learning-to-prompt} and S-Prompts~\citep{s-prompts}. We use our implementation for evaluating L2P and S-Prompts, and modify them to work in the task-incremental setting for fair comparison, denoted as L2P$^\dagger$ and S-Prompts$^\dagger$ in Table~\ref{tab:vdd-results}. We compare L2G with DARTS, and with more advanced $\beta$-DARTS \citep{beta-darts}. We provide the implementation details, the choice for the number of prompts per task ($L$) in L2P, and ablations for the number of prompts in the Appendix \ref{sec:task-inc-l2p-sprompt}.

\begin{table} %
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method-Backbone &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            {S-Prompts$^\dagger$}-ViT ($L$=1)~\citep{s-prompts} & $82.65$ & $86.69$ & $69.89$ & $51.95$ & $49.55$ & $94.07$ & $99.25$ & $90.39$ & $35.03$ & $56.19$ & $71.57 \pm 0.31$ \\
            {S-Prompts$^\dagger$}-ViT ($L$=12)~\citep{s-prompts} & $82.65$ & $89.10$ & $73.27$ & $60.93$ & $55.17$ & $96.37$ & $99.63$ & $\boldsymbol{95.23}$ & $41.60$ & $\boldsymbol{61.54}$ & $75.55 \pm 0.28$ \\
            {L2P$^\dagger$}-ViT ($L$=12)~\citep{learning-to-prompt} & $82.65$ & $89.06$ & $81.43$ & $63.99$ & $62.86$ & $98.21$ & $99.77$ & $\boldsymbol{94.58}$ & $45.00$ & $\boldsymbol{60.78}$ & $77.83 \pm 0.28$ \\
            \midrule
            L2G-ResNet26 (DARTS)~\citep{learn-to-grow} &  $69.84$ & $79.59$ & $95.28$ & $72.03$ &  $\boldsymbol{86.6}$ & $99.72$ & $99.52$ & $71.27$ &  $\boldsymbol{53.01}$ & $49.89$ & $77.68$ \\
            \midrule
            $^*$L2G~\citep{learn-to-grow}-ViT (DARTS) & $82.65$ & $86.31$ & $67.09$ & $75.20$ & $79.47$ & $97.89$ & $ 99.77$ & $83.56$ & $23.03$ & $49.77$ & $74.47 \pm 0.84$ \\ 
            $^*$L2G~\citep{learn-to-grow}-ViT ($\beta$-DARTS) & $82.65$ & $87.12$ & $94.91$ & $64.17$ & $69.35$ & $97.83$ & $98.57$ & $78.46$ & $26.89$ & $54.98$ & $75.49 \pm 1.24$ \\ 
            \midrule
            Our ArtiHippo-ViT (Uniform) & $82.65$ & $85.50$ & $95.63$ & $74.09$ & $82.53$ & $\boldsymbol{99.93}$ & $\boldsymbol{99.85}$ & $79.31$ & $41.62$ & $41.21$ & $78.23 \pm 0.93$ \\ 
             Our ArtiHippo-ViT (Hierarchical) & $82.65$ & $\boldsymbol{90.92}$ & $\boldsymbol{95.93}$ & $\boldsymbol{77.08}$ & $84.14$ & $\boldsymbol{99.92}$ & $99.80$ & $77.76$ & $\boldsymbol{47.11}$ & $45.79$ & $\boldsymbol{80.11} \pm \boldsymbol{1.17}$ \\ 
            Our ArtiHippo-ViT (Hierarchical+$L$=1) & $82.65$ & $\boldsymbol{90.99}$ & $\boldsymbol{95.87}$ & $\boldsymbol{78.98}$ & $\boldsymbol{86.12}$ & $99.91$ & $\boldsymbol{99.89}$ & $88.20$ & $45.86$ & $ 51.31$ & $\boldsymbol{81.98} \pm \boldsymbol{0.95}$ \\ 
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{Results on the VDD benchmark~\citep{vdd}. Our ArtiHippo shows clear improvements over the previous approaches. 
    All the results from our experiments are averaged over 3 different seeds. The 2 highest accuracies per task have been highlighted. All the methods use the same ViT-B/8 backbone containing 86.04M parameters and having 7.11G FLOPs. $^\dagger$ means our modifications to the task-incremental setting. $^*$ means our reproducing results with the vanilla L2G method~\citep{learn-to-grow} for the ViT backbone. }
    \label{tab:vdd-results} \vspace{-3mm}
\end{table}


\subsection{Results and Analysis on the VDD Benchmark}%
Table \ref{tab:vdd-results} shows the results and comparisons. Our method shows consistent performance improvement across tasks compared to Learn to Grow, S-Prompts$^\dagger$ and L2P$^\dagger$. Following is an analysis of the results showing the effectiveness of our proposed method for ViTs:
\begin{itemize}[leftmargin=*]
    \item Table \ref{tab:vdd-results} shows that L2P$^\dagger$ (which uses ViTB/8) performs slightly better than L2G (which uses ResNet26), showing that prompting based methods which leverage the robust features learned by the ViT are indeed effective for lifelong learning.
    \item However, a closer look at task level accuracies shows that for tasks which are significantly different from the base task of the ViT (Omniglot, UCF101, SVHN), L2G significantly outperforms L2P$^\dagger$, showing that introducing new parameters can be beneficial, which justifies our motivation for for seeking more integrative memory mechanisms (Section \ref{sec:intro}).
    \item The poor average accuracy of L2G, when applied to the attention projection layer of ViTs shows that L2G is ill suited for learning to grow for ViTs, and even Uniform sampling (original SPOS) can obtain better average accuracy and higher accuracy on Omniglot, UCF101 and SVHN. Our proposed Hiererchical sampling method outperforms both, prompting-based methods and L2G.
\end{itemize}\vspace{-0.8em}
\begin{itemize}[leftmargin=*]
    \item S-Prompts$^\dagger$ and L2P$^\dagger$ perform better for tasks which are similar to the base task and have very less data (VGG-Flowers, DTD), which suggests that prompting based methods are more data-efficient. Thus, prompt-based methods and our growing-based method are complementary and combining them leads to even better performance (Tab. \ref{tab:vdd-results}). Section \ref{sec:combining-with-prompts} describes a preliminary approach to combine the two approaches. A more comprehensive integration of ArtiHippo with prompt based approaches is left for future work.
\end{itemize}

\begin{wraptable} {r}{0.485\textwidth}%
    \vspace{-11mm}
    \centering
    \resizebox{0.485\textwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{Num. Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts$^\dagger$ & 1 & $91.40$ \\
        S-Prompts$^\dagger$ & 12 & $93.13$ \\
        L2P$^\dagger$ & 12 & $93.76 \pm 0.25$ \\
        \midrule
        L2G (DARTS) & - & $93.89 \pm 2.86$ \\
        L2G ($\beta$-DARTS) & - & $92.19 \pm 1.48$ \\
        \midrule
        ArtiHippo (Projection) & - & $\boldsymbol{96.52 \pm 0.4}$ \\
        \bottomrule
    \end{tabular}
    }
    \caption{Results on the 5-Dataset benchmark~\citep{adversarial-continual-learning}. 
    The results have been averaged over 5 different task orders.}
    \vspace{-1mm}
    \label{tab:5-dataset-results}
\end{wraptable}

\subsection{Results on the 5-Dataset Benchmark}%
Table \ref{tab:5-dataset-results} shows the comparisons. We use the same ViT-B/8 backbone pretrained on the ImageNet images from the VDD benchmark for all the experiments across all the methods and upsample the images in the 5-Dataset benchmark (consisting of CIFAR10~\citep{cifar}, MNIST \citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST \citep{notmnist}, and SVHN~\citep{svhn}) to $72\times72$. 
We can see that ArtiHippo significantly outperforms Learn to Grow, L2P$^\dagger$, S-Prompts$^\dagger$ under the task-incremental setting. 

\subsection{Learned architecture and architecture efficiency}%
Fig.~\ref{fig:vdd_arch-sim} shows the experts learned and the long-term memory structures formed at each block tasks in the VDD dataset. When learning CIFAR100 after ImageNet, the search process learns to reuse most of the ImageNet experts. This is an intuitive result since both the tasks represent natural images. In contrast, while learning Omniglot (OGlt, Task 2), the search adds many new experts. 
Interestingly, when learning Omniglot, the search learns to adapt the ImageNet expert in Block 1 and reuse the SVHN expert in Block 2, forming a very task-specific structure. Both Omniglot and SVHN consist of digit-like images. However, SVHN is in a natural setting, whereas Omniglot contains clean, black and white images. The search process can also use ImNet experts (experts for natural images) for tasks VGG-Flowers (8th task) and Aircraft (9th task), both of which contain natural images. Some blocks have more synergy, e.g. B3. The sensible architectures learned continually show the effectiveness of the proposed task-similarity-oriented ArtiHippo.

In addition to learning qualitatively meaningful architectures, the proposed method also shows quantitative advantages. On the VDD dataset, because of the {\tt Skip} operation in the proposed framework, the number of parameters \textit{reduces} by 0.92M/task (averaged over 3 different runs). Our method also reduces the number of FLOPs by 0.005G/task (averaged over 3 different runs), which is advantageous as compared to the \textit{increase} of 1.06G/task of the L2P method.

\begin{SCfigure}%
    \centering
    \includegraphics[width=0.65\textwidth]{figures/vdd-structure.pdf}
    \caption{\small ArtiHippo learned-to-grow on VDD. Starting from the ImageNet-pretrained ViT (B1 -- B12 in Tsk1\_ImNet), sensible architectures are continually learned for the remaining 9 tasks on the VDD dataset.
    Each column denotes a Transformer block of the ViT in which only the projection layer of the MHSA block is maintained as ArtiHippo with the remaining components frozen.
    \colorbox{skip}{S}, \colorbox{reuse}{R}, \colorbox{adapt}{A}, \colorbox{new}{N} represent {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New} respectively.  \textit{Best viewed in color and magnification.}
    }
    \label{fig:vdd_arch-sim} %
\end{SCfigure}

\section{Limitations}
The main limitation of our approach is the time required for the supernet training followed by the evolutionary search. Although the proposed similarity-guided sampling process is efficient in training the supernet faster than pure uniform sampling (Fig. \ref{fig:epochs-vs-acc} in the Appendix), it presents an overhead. This is especially problematic for large tasks like learning ImageNet from a CLIP model. We plan to address this in future work. The second limitation is that our method requires task IDs (task incremental setup) during inference. Although we show preliminary results of using our method in a class-incremental setup, we leave this study for future work.


\section{Broader Impact}
Our initial analysis of transferring the pretrained CLIP model to ImageNet (details in the Appendix \ref{sec:clip-imnet}) shows that using the projection layer as ArtiHippo results in highly task-specific adaptation. This can be potentially be exploited for continually improving large foundation models which have been shown to learn robust and generic features, thus accentuating their positive impact.

\section{Conclusions} %
This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers for resilient lifelong learning. The final projection layers in the Multi-Head Self-Attention blocks of a Vision Transformer are identified and selected as ``Hippocampi" to implement long-term task-similarity-oriented memory in lifelong learning. The ArtiHippo is maintained by a proposed hierarchical task-similarity-oriented sampling based single-path one-shot Neural Architecture Search algorithm with exploration and exploitation handled in the sampling. The NAS space of ArtiHippo is defined by four basic learning-to-grow operations: {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New}, with the {\tt Adapt} realized with a hybrid Adapter. The task similarity is defined by the normalized cosine similarity between the mean class-tokens of a new task and old tasks. In experiments, the proposed method is tested on the challenging VDD and the 5-Datasets benchmarks. It obtains better performance than the prior art with sensible ArtiHippo learned continually.

\section*{Acknowledgements}
This research is partly supported by NSF IIS-1909644,  ARO Grant W911NF1810295, ARO Grant W911NF2210010, NSF IIS-1822477, NSF CMMI-2024688, NSF IUSE-2013451 and DHHS-ACL Grant 90IFDV0017-01-00.  
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, ARO, DHHS or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon.


\bibliographystyle{plainnat}
\bibliography{references}

\newpage

\appendix


\section{Overview}
In the Appendix, we elaborate on the following aspects that are not presented in the submission due to space limit: 
\begin{itemize}[leftmargin=*]
    \item \textbf{Source Code and Training Logs:} The code for our implementation on the VDD and 5-datasets benchmarks is available in the directory {\tt artihippo}. Due to size limit, we are not able to upload the pretrained checkpoints. We provide the anonymized training logs of the experiments on the VDD benchmark and the 5-datasets benchmarks in {\tt artihippo/logs}, and the experiments with CLIP and ImageNet in {\tt artihippo-imagenet/logs}. The code for the experiments on CLIP and ImageNet is available in {\tt artihippo-imagenet}. The pretrained checkpoints will be released after the reviewing process.
    \item \textbf{Section \ref{sec:ablations}: Ablation Studies}: 
    \begin{itemize}
        \item \textbf{Section \ref{sec:hybrid-adapter}}: We describe the proposed hybrid adapter and an ablation study to verify its effect.
        \item \textbf{Section \ref{sec:artihippo-components}}: We perform ablation experiments on the feasibility of other components of the Vision Transformer, and verify our hypothesis of selecting the final linear projection layer of the MHSA block as the ArtiHippo (Section 3.1 in the main text).
        \item \textbf{Section \ref{sec:exp-expl-sampling}}: Through ablation experiments, we show that the proposed exploration-exploitaiton sampling strategy obtains higher average accuracy and model efficiency, and requires less training epochs for the supernet.
    \end{itemize}
    \item \textbf{Section \ref{sec:task-inc-l2p-sprompt}: Implementation details for S-Prompts and Learn to Prompt}: We describe our implementation of L2P$^\dagger$ and S-Prompts$^\dagger$ used for comparisons, show the results of ablation studies for the number of prompts used in S-Prompts \citep{s-prompts}, and describe how we calculate the number of prompts used in L2P$^\dagger$.
    \item \textbf{Section \ref{sec:clip-imnet}: Application to Large Foundation Models}
    \item \textbf{Section \ref{sec:dataset-details}: Details of the two benchmarks}: the Visual Domain Decathlon (VDD)~\citep{vdd} benchmark (Section~\ref{sec:vdd}) and the 5-Datasets~\citep{adversarial-continual-learning} benchmark (Section~\ref{sec:5datasets}).
    \item \textbf{Section \ref{sec:base-vit-model}: The Base Model and Its Training Details}: the Vision Transformer (ViT) model specification (ViT-B/8) used in our experiments on the VDD and 5-Dataset benchmarks, and training details on the ImageNet (Section~\ref{sec:imagenet-training}). 
    \item \textbf{Section \ref{sec:background}: Background:} To be self-contained, we give a brief introduction to the learn-to-grow method~\citep{learn-to-grow} in Section~\ref{sec:learn-to-grow-review} and the single-path one-shot (SPOS) neural architecture search (NAS) algorithm~\citep{spos} in Section~\ref{sec:spos}.
    \item \textbf{Section \ref{sec:training-details}: Settings and Hyperparameters in the Proposed Lifelong Learning:} We provide the hyperparameters used for training on the VDD and 5-dataset benchmarks.
    \item \textbf{Section \ref{sec:other-architectures}: Learned architecture with pure exploration and different task order}: We show that with a different task order, the proposed method can still learn to exploit inter-task similarities. We also show that pure exploration cannot effectively exploit task similairties by comparing the learned architecture with the architecture learned using the exploration-exploitation strategy.
    \item \textbf{Section \ref{sec:class-incremental}: Preliminary Results on Class-Incremental Settings:} We show that the task similarity used in our sampling may have the potential of developing class-incremental lifelong learning.
     
\end{itemize}

    

\begin{table}[h]
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tabular}{c|l|l|l|l|l|clllllllllll} \toprule
    
          & \multicolumn{17}{c}{ImageNet $\rightarrow$ Omniglot under the lifelong learning setting} \\ \midrule
          & \multicolumn{2}{c|}{{\tt Adapter} in} &  \multirow{2}{1cm}{\#Param Added}     &    \multirow{2}{*}{Rel. $\uparrow$}          &   \multirow{2}{1cm}{Test Acc.}    &     \multicolumn{12}{c}{Learned Operation per Block} \\
          \cline{2-3} \cline{7-18}
          & NAS & Finetune & &  &    & 1 & 2 &   3 & 4 &   5 &   6 & 7 &    8 & 9 &   10 &   11 &   12 \\
          \cline{1-18}
            Shorcut & \multirow{2}{*}{w/o A \& S} & w/ A &  \multirow{2}{*}{2.96M} &         \multirow{2}{*}{3.47\%} & 82.18 & \multirow{2}{*}{\colorbox{adapt}{A}} & \multirow{2}{*}{\colorbox{adapt}{A}} &   \multirow{2}{*}{\colorbox{reuse}{R}} & \multirow{2}{*}{\colorbox{reuse}{R}} &   \multirow{2}{*}{\colorbox{adapt}{A}} &   \multirow{2}{*}{\colorbox{reuse}{R}} &   \multirow{2}{*}{\colorbox{adapt}{A}} &    \multirow{2}{*}{\colorbox{new}{N}} & \multirow{2}{*}{\colorbox{new}{N}} &    \multirow{2}{*}{\colorbox{new}{N}} &    \multirow{2}{*}{\colorbox{skip}{S}} &    \multirow{2}{*}{\colorbox{skip}{S}} \\
                 \cline{3-3}\cline{6-6}
                 in &           &        w/o S &      &             & 78.16 &                               &   &   &   &   &    &    &    &    \\
                 \cline{2-18}
            {\tt Adapter} &         w/ S \& A &         w/ S \& A &  4.14M &         4.89\% & 82.32 &                               \colorbox{adapt}{A} & \colorbox{adapt}{A} & \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{new}{N} &    \colorbox{adapt}{A} & \colorbox{adapt}{A} &    \colorbox{new}{N} &    \colorbox{adapt}{A} &    \colorbox{new}{N} \\
            \bottomrule
        \end{tabular}
    }
\vspace{0.3em}
        \caption{Results of the ablation study on the {\tt Adapter} implementation (Section 3.2.1): with (w/) vs without (w/o) shortcut connection for the MLP {\tt Adapt} layer. We test lifelong learning from ImageNet to Omniglot in the VDD. The proposed combination of w/o shortcut in Supernet NAS training and target network selection and w/ shortcut in finetuning (retraining newly added layers) is the best in terms of the trade-off between performance and cost.}
        \label{tab:adapter_ablation} \vspace{-3mm}
\end{table}

\section{Ablation Studies}
\label{sec:ablations} %

\subsection{The Structure of {\tt Adapter}}%
\label{sec:hybrid-adapter}
\paragraph{How to {\tt Adapt} in a sustainable way?} The proposed {\tt Adapt} operation will effectively increase the depth of the network in a plain way. In the worst case, if too many tasks use {\tt Adapt} on top of each other, we will end up stacking too many MLP layers together. This may lead to unstable training due to gradient vanishing and exploding. Shortcut connections~\citep{resnet} have been shown to alleviate the gradient vanishing and exploding  problems, making it possible to train deeper networks. Due to this residual architecture, the training can ignore an adapter if needed, and leads to a better performance. However, in the lifelong learning setup, where subsequent tasks might have different distributions, the search process might disproportionately encourage {\tt Adapt} operations because of this ability. To counter this, we propose a hybrid {\tt Adapter} which acts as a plain 2-layer MLP during Supernet training and target network selection, and a residual MLP during finetuning. With an ablation study (Table~1 in Supplementary), we show that much more compact models can be learned with negligible loss in accuracy.

We verify the effectiveness of the proposed hybrid adapter using a lifelong learning setup with 2 tasks: ImageNet and Omniglot. The Omniglot dataset presents two major challenges for a lifelong learning system. First, Omniglot is a few-shot dataset, for which we may expect a lifelong learning system can learn a model less complex than the one for ImageNet. Second,  Omniglot has a significantly different data distribution than ImageNet, for which we may expect a lifelong learning system will need to introduce new parameters, but hopefully in a sensible and explainable way. Table \ref{tab:adapter_ablation} shows the results. In terms of the learned neural architecture, a more compact model (row 3) is learned without the shortcut in the adapter during Supernet training and target network selection: the last two MHSA blocks are skipped and three blocks are reused. Skipping the last two MHSA blocks makes intuitive sense since Omniglot may not need those high-level self-attention (learned for ImageNet) due to the nature of the dataset. The three consecutive {\tt new} operations (in Blocks 8,9,10) also make sense in terms of learning new self-attention fusion layers (i.e., new memory) to account for the change of the nature of the task. Adding shortcut connection back in the finetuning shows significant performance improvement (from 78.16\% to 82.18\%), making it very close to the performance (82.32\%) obtained by the much more expensive and less intuitively meaningful alternative (the last row).

\begin{table*} [t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|ccccccccccc}
        \toprule
            Component &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
             Projection & 82.65 & 90.92 & \textbf{95.93} & \textbf{77.08} & \textbf{84.14} & {99.92} & {99.80} & {77.76} & \textbf{47.11} & {45.79} & {$80.11 \pm 1.17$} \\ 
            Value & 82.65 & 84.98 & 95.74 & 76.25 & 83.55 & \textbf{99.93} &	99.90 &	\textbf{84.80} &	44.97 &	49.80 &	$\boldsymbol{80.26} \pm \boldsymbol{1.62}$ \\ 
            Query & {82.65} & 89.47 & 94.09 & 68.85 & 75.71 & 99.86 & \textbf{99.91} & 77.81 & 34.51 & \textbf{51.47} & $77.43 \pm 0.43$ \\ 
            FFN & {82.65} & 90.92 & 94.95 & 68.02 & 70.16 & 99.91 & 99.63 & 63.37 & 18.85 & 29.70 & $71.82 \pm 1.80$ \\  
        \bottomrule
    \end{tabular}}
    \vspace{0.3em}
    \caption{Results of ablation study on the other components of the ViT used for realizing the ArtiHippo. Realizing ArtiHippo at the Value shows slightly better performance than the Projection layer. However, the Value layer is often coupled with the Query and Key layers for efficient computation. Using the Projection layer offers negligible drop in Average Accuracy without sacrificing efficiency. The results have been averaged over 3 different seeds.}
    \label{tab:vdd-component-ablation} \vspace{-2mm}
\end{table*}

\subsection{Evaluating feasibility of other ViT components as ArtiHippo }
\label{sec:artihippo-components}
Table \ref{tab:vdd-component-ablation} shows the accuracy with other components if the ViT used for learning the Mixture of Experts using the proposed NAS method. Interestingly, the Value performs slightly better than the Projection layer. However, using the the Value layer would be inefficient, as it is implemented as a parallel linear layer with the Query and Key. The projection layer can be efficiently replaced with the Mixture of Experts framework. Table \ref{tab:vdd-component-ablation} also shows that the Query component from the MHSA block, and the FFN which is typically used in the Mixture of Experts framework for Transformers \citep{vision-moe}, are ill suited. This reinforces our identification of the ArtiHippo in Section 3.1 in the main text.

\begin{wrapfigure}{r}{0.5\textwidth}\vspace{-5mm}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/epochwise_plot-v2.pdf}
    \caption{Results of the ablation study on the Exploration-Exploitation (EE) guided sampling in the Supernet NAS training using  the VDD benchmark~\citep{vdd}. The proposed EE sampling strategy is much more efficient than the pure exploration based strategy (i.e., the vanilla SPOS NAS~\citep{spos}). It uses the Supernet training efficiently even at 25 epochs and achieves better performance than pure Exploration, which is desirable for fast adaptation in dynamic environments using lifelong learning. The \% increase in parameters shows that EE strategy is effective in reusing experts from the previous tasks and limiting the increase in parameters. Note that the net increase is negative because the {\tt Skip} operation removes the entire attention head. The results have been averaged over 2 task sequences, with 3 runs per sequence with different seeds.
    }
    \label{fig:epochs-vs-acc} \vspace{-12mm}
\end{wrapfigure}


\newpage
\subsection{The Exploration-Exploitation Sampling Method} 
\label{sec:exp-expl-sampling}
Figure~\ref{fig:epochs-vs-acc}, left shows that the proposed exploration-exploitation strategy can consistently obtain higher accuracy than pure exploration even when the supernet is trained a small number of epochs. Although training the supernet for a longer duration improves the accuracy for pure exploration, it also leads to a higher number of parameters in the learned architecture (Figure \ref{fig:epochs-vs-acc} right). We thus verify that the proposed exploration-exploitation strategy is effective and efficient. This also shows that proposed task-similarity metric is meaningful.



\begin{table}[b]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            {S-Prompts$^\dagger$} (p=1/task) & $82.65$ & $86.69$ & $69.89$ & $51.95$ & $49.55$ & $94.07$ & $99.25$ & $90.39$ & $35.03$ & $56.19$ & $71.57 \pm 0.31$ \\
            {S-Prompts$^\dagger$} (p=5/task) & $82.65$ & $89.01$ & $71.29$ & $60.21$ & $58.50$ & $96.20$ & $99.64$ & $95.33$ & $41.49$ & $62.25$ & $75.66 \pm 0.14$ \\
            {S-Prompts$^\dagger$} (p=10/task) & $82.65$ & $88.99$ & $74.45$ & $60.42$ & $57.89$ & $96.27$ & $99.57$ & $95.20$ & $41.76$ & $61.76$ & $75.90 \pm 0.16$ \\
            {S-Prompts$^\dagger$} (p=12/task) & $82.65$ & $89.10$ & $73.27$ & $60.93$ & $55.17$ & $96.37$ & $99.63$ & $95.23$ & $41.60$ & $61.54$ & $75.55 \pm 0.28$ \\
            {S-Prompts$^\dagger$} (p=15/task) & $82.65$ & $89.22$ & $74.03$ & $59.96$ & $52.39$ & $96.29$ & $99.64$ & $95.13$ & $41.35$ & $61.76$ & $75.24 \pm 0.31$ \\
            {L2P$^\dagger$} (p=12/task) & $82.65$ & $89.06$ & $81.43$ & $63.99$ & $62.86$ & $98.21$ & $99.77$ & $94.58$ & $45.00$ & $60.78$ & $77.83 \pm 0.28$ \\
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{Results on the VDD benchmark~\cite{vdd} with various number of prompts. Increasing the number of prompts above 5 does not lead to a significant gain in accuracy.}
    \label{tab:vdd-results-prompts} \vspace{-3mm}
\end{table}

\begin{table}%
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{Num. Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts$^\dagger$ & 1 & $91.40$ \\
        S-Prompts$^\dagger$ & 5 & $92.35$ \\
        S-Prompts$^\dagger$ & 10 & $92.89$ \\
        S-Prompts$^\dagger$ & 12 & $93.13$ \\
        S-Prompts$^\dagger$ & 15 & $93.03$ \\
        L2P$^\dagger$ & 12 & $93.76 \pm 0.25$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.3em}
    \caption{Results on the 5-Dataset benchmark~\cite{adversarial-continual-learning}. 
    The results have been averaged over 5 different task orders following L2P.}
    \label{tab:5-dataset-results-prompt}
\end{table}

\section{Modifying S-Promts and L2P for task-incremental setting}
\label{sec:task-inc-l2p-sprompt}
Both, Learn to Prompt and S-Prompts, can be modified for task-incremental setting without altering the core algorithm for learning the prompts. For S-Prompts, this is done by training a separate prompt of length $L$, i.e., (i.e. a $L$ \texttt{cls} tokens) per task and retrieving the correct prompt using the task ID. For L2P, we follow the official implementation\footnote{L2P official implementation: \href{https://github.com/google-research/l2p}{https://github.com/google-research/l2p}} used for evaluating on the 5-datasets benchmark. L2P first trains a set of $N$ prompts of length $L_p$ (i.e. $NL_p$ tokens) per task. It then learns a set of $N$ keys such that the distance between the keys and the image encoding (using a fixed feature extractor) is maximized. We retrieve the retrieve the correct prompts using the Task ID instead of using a key-value matching and make L2P compatible with a task-incremental setting. We initialize the the values for the prompts for task $t$ from the trained values of task $t-1$ following the original implementation. 

\subsection{Ablation study with varying number of prompts}
Table \ref{tab:vdd-results-prompts} and Table \ref{tab:5-dataset-results-prompt} show the results of varying the number of prompt tokens for S-Prompts$^\dagger$ on the VDD and 5-dataset benchmarks respectively. Varying the number of prompts beyond 5 does not affect the performace significantly, as observed in the original paper \citep{s-prompts}. Hence, for L2P$^\dagger$, we calculate the number of prompt tokens by scaling the number of prompts used in the original implementation of L2P \citep{learning-to-prompt} on the 5-datasets benchmark to suit an image size of $72\times72$. For the 5 datasets benchmark, L2P uses 4 prompts of length 10 (40 tokens) for an image size of $224\times224$. We scale the prompt length to {\tt{floor}}$(10/3) = 3$, which gives 4 prompts of length 3, i.e., 12 tokens. We use a scaling factor of 3 since 224/72=3.




\section{Application to Large Foundation Models}
\label{sec:clip-imnet}

\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|c|ccccc}
        Method & Evaluation Style & IN-Val & IN-Sketch & IN-V2 & IN-A & IN-R & OOD-Avg \\
        \toprule
        OpenAI's CLIP~\citep{clip} & \multirow{1}{*}{Zero-Shot} & 63.38 & 42.28 & 56.37 & 31.44 & 69.22 & 49.83 \\
        \midrule
        Full Finetuning & \multirow{5}{*}{Classification Head} & \textbf{79.77} & 40.66 & \textbf{68.96} & 21.07 & 53.63 & 46.08 \\       
        LoRA~\citep{lora} on MHSA (r=8) & & 78.19 & 41.18 & 66.46 & 24.01 & 59.33 & 47.74 \\
        Zero-Init Attn.\citep{llama-adapter} & & 76.42 & 40.23 & 64.90 & \textbf{24.04} & \textbf{60.68} & 47.46 \\ 
        Our ArtiHippo (Proj) &  & {78.43} & \textbf{42.67} & 66.94 & 22.49 & 59.36 & \textbf{47.87} \\
        Our ArtiHippo (Value) &  & 77.54 & 41.96 & 66.10 & 21.49 & 59.21 & 47.19 \\
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{Results of transferring the ViT-B/32 CLIP~\citep{clip} image encoder to the ImageNet-1k~\citep{imagenet}. It is evaluated on the vanilla ImageNet-1k validation (IN-Val), and the related out-of-distribution (OOD) validation datasets including IN-Sketch~\citep{in-sketch}, IN-V2~\citep{in-v2}, IN-A~\citep{in-a} and IN-R~\citep{in-r}. }
    \label{tab:imagenet-results}
\end{table}

With the remarkable progress recently made by large foundation models, parameter-efficient fine-tuning of those large models have attracted much attention. Along this direction, we conduct some preliminary experiments to test the potential applicability of our proposed ArtiHippo. We consider the task of transferring/finetuning the pretrained CLIP \citep{clip} image encoder to the ImageNet-1000 classification~\citep{imagenet} at an image size of $224\times224$. We compare with three methods: the traditional full finetuning which simply uses the pretrained model weights as the initialization followed by the vanilla supervised training with some chosen recipe, and two more recently popularized methods, the low-rank adapter (LoRA)~\citep{lora} and the Zero-Init Attention method~\citep{llama-adapter} that is proposed to finetune the large language model -- the LLaMA family of models~\citep{llama}. 

We use the pure exploration sampling strategy in learning the ArtiHippo since the data from the OpenAI's CLIP model~\citep{clip} is not accessible to compute the mean class token from the pretrained task. Moreover, we initialize the {\tt new} operation from the weights of the CLIP backbone.


Table \ref{tab:imagenet-results} shows that finetuning the entire image encoder achieves the best accuracy on the IN-Val, but results in the worst mean accuracy on the OOD datasets, which indicates the trend of catastrophic forgetting. Compared with the two recent adapters, LoRA and Zero-Init Attention, our ArtiHippo (applied to the projection layer in the MHSA) achieves the best performance in terms of both IN-Val and OOD-avg. We hypothesize that if there are more tasks to be evaluated under the (task-incremental) life-long learning setting for the pretrained large models like OpenAI's CLIP, our ArtiHippo could be utilized as a more comprehensive scheme that can preserve the capability of the pretrained models, while learning to tackle different streaming tasks.

\textbf{Hyperparameters:} For training of LoRA and Zero-Init Attention methods, we only use random horizontal flip and train for 30 epoch with a learning rate of $2.5\times10^{-4}$ and cosine learning rate schedule. We use 5 warmup epochs with a warmup learning rate of $10^{-6}$ (hyperparameters present in {\tt artihippo-imagenet/configs/imagenet/finetune/minimal\_aug.yaml}). The hyperparameters and the augmentations for finetuning the full backbone are present in {\tt artihippo-imagenet/configs/imagenet/finetune/extensive\_aug.yaml}. For training the supernet for ArtiHippo, we only use horizontal flips with a learning rate of $10^{-3}$, a cosine learning rate schedule, warmup learning rate of $10^{-4}$ with 5 warmup epochs, and train for 100 epochs. Once the target network has been found, we finetune the model for 30 epochs with a learning rate of $2.5\times10^{-4}$ with 5 warmup epochs. We use a warmup learning rate of $10^{-6}$ and a cosine learning rate decay. The finetuning hyperparameters are present in the file {\tt artihippo-imagenet/configs/imagenet/finetune/reduced\_aug.yaml}. The hyperparameters of the evolutionary search are the same as that of the experiments on the VDD benchmark (Section \ref{sec:evolutionary-search-params}), except we only use 12 epochs. For all the experiments, we use a batch size of 1280 split across 10 Nvidia A100 GPUs.




\section{Dataset Details}
\label{sec:dataset-details}
\subsection{The VDD Benchmark}\label{sec:vdd}
It consists of 10 tasks: ImageNet12~\citep{imagenet}, CIFAR100~\citep{cifar}, SVHN~\citep{svhn}, UCF101 Dynamic Images (UCF)~\citep{ucf1,ucf2}, Omniglot~\citep{omniglot}, German Traffic Signs (GTSR)~\citep{gtsrb}, Daimler Pedestrian Classification (DPed)~\citep{daimlerpedcls},  VGG Flowers~\citep{vgg-flowers}, FGVC-Aircraft~\citep{aircraft},    and Describable Textures (DTD)~\citep{dtd}. All the images in the VDD benchmark have been scaled such that the shorter side is 72 pixels. Table~\ref{tab:vdd-num-samples} shows the number of samples in each task. Figure~\ref{fig:vdd-examples} shows examples of images from each task of the VDD benchmark.

In our experiments, we use 10\% of {\tt the official training data} from each of the tasks for validation (e.g., used in the target network selection in Section 3.2.3 in main text), and report the accuracy on {\tt the official validation set} for fair comparison with the learn-to-grow method~\citep{learn-to-grow} in Table 2. In Table~\ref{tab:vdd-num-samples}, the \texttt{train, validation} and \texttt{test} splits are thus referred to 90\% of the official training data, 10\% of the official training data, and the entire official validation data respectively. When finetuning the learned architecture, we use the entire {\tt the official training data} to train and report results on the {\tt the official validation set}.%

\begin{figure*}[h]
    \centering{
        \subfiggrid{vdd}{imagenet}{1.25}{1.25}{ImageNet}\vspace{3mm}
        ~
        \subfiggrid{vdd}{cifar100}{1.25}{1.25}{CIFAR100}
        ~
        \subfiggrid{vdd}{svhn}{1.25}{1.25}{SVHN}
        ~
        \subfiggrid{vdd}{ucf101}{1.25}{1.25}{UCF101}
        ~
        \subfiggrid{vdd}{omniglot}{1.25}{1.25}{Omniglot}
        ~
        \subfiggrid{vdd}{gtsrb}{1.25}{1.25}{GTSR}
        ~
        \subfiggrid{vdd}{daimlerpedcls}{0.8}{1.25}{Pedestrian Cls.}
        ~
        \subfiggrid{vdd}{vgg-flowers}{1.25}{1.25}{VGG-Flowers}
        ~
        \subfiggrid{vdd}{aircraft}{1.25}{0.9}{FGVC-Aircraft}
        ~
        \subfiggrid{vdd}{dtd}{1.25}{1.25}{DTD}
    }\vspace{3mm}
    \caption{Example images from the VDD benchmark~\citep{vdd}. Each task has a significantly different domain than others, making VDD a challenging benchmark for lifelong learning.}
    \label{fig:vdd-examples}
\end{figure*}

\begin{figure*}[h]
    \centering{
        \subfiggrid{5-datasets}{mnist}{1.25}{1.25}{MNIST}
        ~
        \subfiggrid{5-datasets}{cifar10}{1.25}{1.25}{CIFAR10}
        ~
        \subfiggrid{5-datasets}{svhn}{1.25}{1.25}{SVHN}
        ~
        \subfiggrid{5-datasets}{not-mnist}{1.25}{1.25}{not-MNIST}
        ~
        \subfiggrid{5-datasets}{fashion-mnist}{1.25}{1.25}{Fashion-MNIST}
    }\vspace{1em}
    \caption{Example images from the 5-Datasets benchmark~\citep{adversarial-continual-learning}.}
    \label{fig:5-dataset-examples}
\end{figure*}

\subsection{The 5-Datasets Benchmark}\label{sec:5datasets}
It consists of 5 tasks: CIFAR10~\citep{cifar}, MNIST~\citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST~\citep{notmnist}, and SVHN~\citep{svhn}. Table~\ref{tab:5-dataset-num-samples} shows the data statistics.  Figure~\ref{fig:5-dataset-examples} shows examples of images from each task. To be consistent with the settings used on the VDD benchmark, we use 15\% of \texttt{the training data} for validation and report the results on \texttt{the official test data}, except for not-MNIST for which an official test split is not available. So, for the not-MNIST dataset, we use the small version of that dataset, with which we construct the test set by randomly sampling 20\% of the samples. From the remaining 80\%, we use 15\% for validation, and the rest as the training set.

\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c}
  \toprule
  Task & Train & Validation & Test \\
  \midrule
  ImageNet12 & 1108951 & 123216 & 49000 \\
  CIFAR100 & 36000 & 4000 & 10000 \\
  SVHN & 42496 & 4721 & 26040 \\
  UCF & 6827 & 758 & 1952 \\
  Omniglot & 16068 & 1785 & 6492 \\
  GTSR & 28231 & 3136 & 7842 \\
  DPed & 21168 & 2352 & 5880 \\
  VGG-Flowers & 918 & 102 & 1020 \\
  Aircraft & 3001 & 333 & 3333 \\
  DTD & 1692 & 188 & 1880 \\
  \bottomrule
\end{tabular}}
    \captionof{table}{The number of samples in training, validation and testing sets per task used in our experiments on the VDD benchmark~\citep{vdd}.}
    \label{tab:vdd-num-samples}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c}
    \toprule
         Task & Train & Validation & Test  \\
         \midrule
         MNIST & 51000 & 9000 & 10000 \\
         not-MNIST & 12733 & 2247 & 3744 \\
         SVHN & 62269 & 10988 & 26032 \\
         CIFAR10 & 42500 & 7500 & 10000 \\
         Fashion MNIST & 51000 & 9000 & 10000 \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Number of samples in training, validation, and test sets per task in the 5-Datasets benchmark~\citep{adversarial-continual-learning}. The test samples have been reported from the official test data provided by each individual dataset, except for not-MNIST. See text for details.}
    \label{tab:5-dataset-num-samples}
\end{minipage}


\section{The Vision Transformer: ViT-B/8}
\label{sec:base-vit-model}
We use the base Vision Transformer (ViT) model, with a patch size of $8 \times 8$ (ViT-B/8) model from~\citep{vit}. The base ViT model contains 12 Transformer blocks with residual connections for each block. A Transformer block is defined by stacking a Multi-Head Self-Attention (MHSA) block and a Multi-Layer Perceptron (MLP) block with resudual connections for each block. ViT-B/8 uses 12 attention heads in each of the MHSA blocks, and a feature dimension of 768. The MLP block expands the dimension size to 3072 in the first layer and projects it back to 768 in the second layer. For all the experiments, we use an image size of $72\times72$ following the VDD setting. We base the implementation of the ViT on the {\tt timm} package ~\citep{timm}. 

\subsection{ImageNet Training on the VDD benchmark}
\label{sec:imagenet-training}
To train the ViT-B/8 model, we use the ImageNet data provided by the VDD benchmark (the \texttt{train} split in Table~\ref{tab:vdd-num-samples}). To save the training time, we initialize the weights from the ViT-B/8 trained on the full resolution ImageNet dataset (224$\times$224) and available in the \texttt{timm} package, and finetune it for 30 epochs on the downsized version of ImageNet (72$\times$72) in the VDD benchmark. We use a batch size of 2048 split across 4 Nvidia Quadro RTX 8000 GPUs. We follow the standard training/finetuning recipes for ViT models. The file {\tt artihippo/logs/imagenet\_pretraining/args.yaml} provides all the training hyperparameters used for training the the ViT-B/8 model on ImageNet.
During testing, we take a single center crop of 72$\times$72 from an image scaled with the shortest side to scaled to 72 pixels.


\section{Background: Learn to Grow and SPOS}
\label{sec:background}
\subsection{Background on Learn to Grow}
\label{sec:learn-to-grow-review}
The learn-to-grow method~\citep{learn-to-grow} uses Differentiable Architecture Search (DARTS)~\citep{darts}, a supernet based NAS algorithm to learn a strategy for reusing, adapting, or renewing the parameters learned for the previous tasks (skipping is not applied). Consider an $L$-layer backbone network with $\mathcal{S}^l$ choice of parameters for a layer $l$ learned for the previous tasks. The parameters can be the weights and biases of the layer in the case of fully-connected layers, or the filters and the biases in the case of a Convolutional layers. For a new task, the learn-to-grow method constructs the search space for NAS (referred to as operations) by applying the operations {\tt reuse}, {\tt adapt}, and {\tt new} to $\mathcal{S}^l$ for all layers $l \in [1,L]$. The total number of choices for layer $l$ is $C_l = 2|\mathcal{S}^l|+1$ \{$|\mathcal{S}^l|$ reuse, $|\mathcal{S}^l|$ adapt, and $1$ new operation\}. Following DARTS, the output of a layer when training the NAS supernet is given by
\begin{equation}
    x_{out}^l = \sum_{c=1}^{C_l} \frac{exp(\alpha_c^l)}{\sum_{c^\prime=1}^{C_l}exp(\alpha_{c^\prime}^l)}g_c^l(x_{in}^l), 
\end{equation}
where $g_c^l(\cdot)$ is defined by,
\begin{equation}
\small 
    g_c^l(x_{l-1}) = 
    \begin{cases}
        S_c^l(x_{in}^l) & \text{if } c \le |\mathcal{S}^l|, \\
        S_c^l(x_{in}^l) + \gamma_{c-|\mathcal{S}^l|}^l(x_{in}^l) & \text{if } |\mathcal{S}^l| < c \le 2|\mathcal{S}^l|, \\
        \texttt{new}^l(x_{in}^l) & \text{if } c = 2|\mathcal{S}^l| + 1, 
    \end{cases}
\end{equation}
where $\gamma$ denotes the additional operation used in parallel ($1\times1$ convolution in the case of a Convolutional layer) which implements the {\tt adapt} operation. Using DARTS, the operations are trained jointly with architecture coefficients $\alpha_c^l$. Once the supernet is trained, the optimal operation is selected such that $(c^*)^l \gets \text{argmax}_c\ \alpha_c^l$. In experiments, the learn-to-grow method uses a 26-layer ResNet~\citep{resnet} on the VDD benchmark.

\textbf{Remarks on the proposed learning-to-grow with ViTs.} The learn-to-grow method~\citep{learn-to-grow} can maintain dynamic feature backbone networks for different tasks based on NAS, which leads to the desired selectivity and plasticity of networks in lifelong learning. It has been mainly studied with Convolutional Neural Networks (CNNs), and often apply DARTS for all layers with respect to the three operations, which is time consuming and may be less effective when a new task has little data. 

More importantly, the vanilla learn-to-grow method with DARTS and ResNets does not have the motivation of integrating learning and (long-term) memory in lifelong learning, which is the focus of our proposed ArtiHippo method. In learning a new task, unlike the vanilla learn-to-grow method in which all the previous tasks are treated equally when selecting the operation operations, our proposed ArtiHippo leverages the task similarities which in turn exploits the class-token specialized in ViTs.



\subsection{Background on the Single-Path One-Shot Neural Architecture Search Method}
\label{sec:spos}
DARTS~\citep{darts} trains the entire supernet jointly in learning a new task, and thus might not be practically scalable and sustainable after the supernet ``grew too fat" at each layer. 
The strategy used in Single-Path One-Shot (SPOS)~\citep{spos} NAS offers an alternative strategy based on a \textit{stochastic} supernet. It uses a bi-level optimization formulation consisting of the supernet training and the target network selection,
\begin{equation}
\label{eq:spos-train}
    W_{\mathcal{A}} = \text{argmin}_W\mathcal{L}_{train}(\mathcal{N}(\mathcal{A}, W)),
\end{equation}
\begin{equation}
\label{eq:spos-val}
    a^* = \text{argmax}_{a\in\mathcal{A}}\ Acc_{val}(\mathcal{N}(a, W_{\mathcal{A}}(a))),
\end{equation}
where Eqn.~\ref{eq:spos-train} is solved by defining a prior distribution $\Gamma(\mathcal{A})$ over the choice of operations in the stochastic supernet and optimizing,
\begin{equation}
    W_{\mathcal{A}} = \text{argmin}_W \mathbf{E}_{a \sim \Gamma(\mathcal{A})}\mathcal{L}_{train}(\mathcal{N}(\mathcal{A}, W)).
\end{equation}
This amounts to sampling one operation per layer (i.e., one-shot) of the neural network, and to forming a single path in the stochastic supernet. Eqn.~\ref{eq:spos-val} is optimized using an evolutionary search based on the validation performance for different candidates of the target network in a population, which is efficient since only inference is executed. The SPOS method empirically finds that a uniform prior works well in practice, especially when sufficient exploration is afforded. Note that this prior is also applied in generating the initial population for the evolutionary search.

The evolutionary search method used in the SPOS method is adopted from~\citep{real2019regularized}. It first initializes a population with a predefined number of candidate architectures sampled from the supernet. It then ``evolves" the population via the crossover and the mutation operations. At each ``evolving" iteration, the population is evaluated and sorted based on the validation performance. With the top-$k$ candidates after evaluation and sorting (the number $k$ is predefined), for crossover, two randomly sampled candidate networks in the top-$k$ are crossed to produce a new target network. For mutation, a randomly selected candidate in the top-$k$ mutates its every choice block with probability (e.g., $0.1$) to produce a new candidate. Crossover and mutation are repeated to generate sufficient new candidate target networks to form the population for the next ``evolving" iteration.

\textbf{Remarks on the proposed hierarchical task-similarity-oriented sampling with exploration-exploitation trade-off}. We modify the core sampling component in the SPOS algorithm for resilient lifelong learning with a long-term memory. During exploitation, we generate the prior $\Gamma(\mathcal{A})$ using our proposed hierarchical sampling scheme, and use the uniform prior during exploration. The same sampling scheme is applied when generating the initial population for the evolutionary search as well.





\section{Settings and Hyperparameters in the Proposed Lifelong Learning}\label{sec:training-details}
Starting with the ImageNet pretrained ViT-B/8, the proposed lifelong learning methods consists of three components in learning new tasks continually and sequentially: supernet training, evolutionary search for target network selection, and target network finetuning. The supernet training and target network finetuning use the \texttt{train} split, while the evolutionary search uses the \texttt{validation} split, both shown in Table~\ref{tab:vdd-num-samples} and Table~\ref{tab:5-dataset-num-samples}. We use the vanilla data augmentation in both supernet training and target network finetuning. We use a weight of 1 for the beta loss in all the experiments with $\beta$-DARTS.  

\textbf{Data Augmentations.}
A full list of data augmentations used for the VDD benchmark is provided in Table \ref{tab:vdd-augmentations}, and the data augmentations used for the tasks in the 5-datasets benchmark is provided in Table \ref{tab:5-dataset-augmentations}. The augmentations are chosen so as not to affect the nature of the data. Scale and Crop transformation scales the image randomly between 90\% to 100\% of the original resolution and takes a random crop with an aspect ratio sampled from a uniform distribution over the original aspect ratio $\pm0.05$.
In evaluating the supernet and the finetuned model on the validation set and test set respectively, images are simply resized to $72\times72$ with bicubic interpolation.

\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c}
         \toprule
         Task & Scale and Crop & Hor. Flip & Ver. Flip  \\
         \midrule
         CIFAR100 & Yes & p=0.5 & No \\
         Aircraft & Yes & p=0.5 & No \\
         DPed & Yes & p=0.5 & No \\
         DTD & Yes & p=0.5 & p=0.5 \\
         GTSR & Yes & p=0.5 & No \\
         OGlt & Yes & No & No \\
         SVHN & Yes & No & No \\
         UCF101 & Yes & p=0.5 & No \\
         Flwr. & Yes & p=0.5 & No \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Data augmentations for the 9 tasks in the VDD benchmark.}
    \label{tab:vdd-augmentations}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
         Task & Scale and Crop & Hor. Flip  \\
         \midrule
         MNIST & Yes & No \\
         not-MNIST & Yes & No \\
         SVHN & Yes & No \\
         CIFAR100 & Yes & p=0.5 \\
         Fashion MNIST & Yes & No \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Data augmentations used for each task in the 5-Datasets benchmark.}
    \label{tab:5-dataset-augmentations}
\end{minipage}


\subsection{Supernet Training}
\textit{VDD Benchmark}: For each task, we train the supernet for 300 epochs. We use a label smoothing of 0.1. No other form of regularization is used since the {\tt skip} operation provides implicit regularization, which plays the role of Drop Path during training. We use a learning rate of 0.001 and the Adam optimier~\citep{adam} with a Cosine Decay Rule. For experiments with separate class tokens per task, we use a learning rate of 0.0005 for training the supernet, and 0.003 for training the task token. For each epoch, a minimum of 15 batches are drawn, with a batch size of 512. If the number of samples present in the task allows, we draw the maximum possible number of batches that covers the entire training data. For the Exploration-Exploitation sampling scheme, we use an exploration probability $\epsilon = 0.3$.

\textit{5-datasets Benchmark}: We use the same hyperparameters as those used in the VDD Benchmark, but train the supernet for 150 epochs.

\textit{L2G with DARTS and $\beta$-DARTS}: We train the supernet for Learn to Grow \citep{learn-to-grow} for 150 epochs for the VDD benchmark and 50 epochs for the 5-datasets benchmark.




\subsection{Evolutionary Search}
\label{sec:evolutionary-search-params}
The evolutionary search is run for 20 epochs. We use a population size of 50. 25 candidates are generated by mutation, and 25 candidates are generated using crossover. The top 50 candidates are retained. The crossover is performed among the top 10 candidates, and the top 10 candidates are mutated with a probability of $0.1$. For the Exploration-Exploitation sampling scheme, we use an exploration probability $\epsilon = 0.5$ when generating the initial population.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_cosine_similarity.png}
        \caption{Cosine Similarity between the mean class-tokens from the previous tasks and mean class-token for each block.}
        \label{subfig:cosine-sim}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_cosine_similarity.png}
        \caption{Normalized Cosine Similarity at each block calculated by scaling the Cosine Similarities between $-1$ and $1$.}
        \label{subfig:norm-cosine-sim}
    \end{subfigure}\vspace{-1.5mm}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_softmax.png}
        \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Cosine Similarity}
        \label{subfig:expert-sampling}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_softmax.png}
        \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Normalized Cosine Similarity}
        \label{subfig:expert-sampling-normalized}
    \end{subfigure}\vspace{-1.5mm}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_sigmoid.png}
        \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Cosine Similarity.}
        \label{subfig:retention-probability}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_sigmoid.png}
        \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Normalized Cosine Similarity.}
        \label{subfig:retention-probability-normalized}
    \end{subfigure}\vspace{1em}
    \caption{Comparison of probability values for sampling the Experts (Middle row) and the retention probabilities (Bottom row) using the Cosine Similarity and the Normalized Cosine Similarity. Using the Normalized Cosine Similarity gives better probability values for $\psi_e$, as can be seen in Block 6. Using Normalized Cosine Similarity increases the probability of sampling the SVHN expert. The effect on the retention probability $\rho_e$ can be prominently seen on the Omniglot experts. The retention probability in Blocks 4 to 6 \ref{subfig:retention-probability-normalized} reduces below 0.5, which will encourage the {\tt new} and {\tt skip} experts to be trained and selected in the evolutionary search even if Omniglot experts were sampled.}
    \label{fig:similarities}
\end{figure}

\subsection{Finetuning}
The target network for a task selected by the evolutionary search is finetuned for 30 epochs with a learning rate of 0.001, Adam optimizer, and a Cosine Learning Rate scheduler. Drop Path of 0.25 and label smoothing of 0.1 is used for regularization. We use a batch size of 512, and a minimum of 30 batches are drawn. When using a separate task token for each task, the task token is first finetuned with a learning rate of 0.001.


\subsection{Normalized Cosine Similarity}
\label{sec:normalized-cosine-similarity}
To verify the use of the Normalized Cosine Similarity as our similarity measure (Eqn. \textcolor{blue}{6} in the main text), we refer to Figure \ref{fig:similarities}. Figure \ref{subfig:cosine-sim} shows the Cosine Similarity between the mean class-tokens learned for tasks ImageNet, CIFAR100, SVHN, and UCF101 (in order), and the mean class-tokens calculated for each expert using the data from the current task Omniglot. Empirically, we observe that the Cosine Similarity between the mean class-tokens calculated using the data of the task associated with an expert and the mean class-token calculated with the current task in training is high. However, the difference between the similarity values for each expert are more important than the absolute values of the similarity. This difference can be increased by scaling the similarity such that it increases the magnitude difference between the similarities of different tasks, but maintains the relative similarity. This can be achieved by scaling the Cosine Similarities between -1 and 1 using the minimum and the maximum values from all the experts and all the blocks (Figure \ref{subfig:norm-cosine-sim}). Using the Normalized Cosine Similarity leads to better and more intuitive probability distributions for sampling candidate experts and the retention probabilities for the sampled experts. For example, comparing the probability values for sampling an expert at Block 6 calculated using Cosine Similarity (Figure \ref{subfig:expert-sampling}) vs. Normalized Cosine Similarity (Figure \ref{subfig:expert-sampling-normalized}), the probability of sampling the ImageNet expert increases, and those of sampling UCF101 and Omniglot decrease. Similarly, for Blocks 3, 4 and 5, the retention probability calculated using the Normalized Cosine Similarity (Figure \ref{subfig:retention-probability-normalized}) reduces by a large factor than that calculated using the Cosine Similarity (Figure \ref{subfig:retention-probability}). This will encourage sampling the {\tt new} and the {\tt skip} experts.

\begin{figure}[b]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/vdd-structure-exploration.pdf}
    \caption{Architecture learned using pure exploration. Pure Exploration based method adds many unnecessary Adapt and New operations even though the tasks are similar (ImNet  C100), proving the effectiveness of the proposed sampling method}
    \label{fig:vdd_arch_seq1_exp} \vspace{-4mm}
\end{figure}


\section{Learned architecture for different task order and pure exploration}
\label{sec:other-architectures}
Figure \ref{fig:vdd_arch_seq2} shows the architecture learned for a different task sequence. It can be seen that even with a different task sequence, the proposed method can learn to exploit task similarities. For example, a new projection layer is learned in Block 2 for Omniglot, which is adapted by SVHN and GTSRB. Figure \ref{fig:vdd_arch_seq1_exp} shows the architecture learned using a pure exploration strategy. It can be seen that pure exploration does not reuse components from similar tasks. For example, a large number of {\tt Adapt} and {\tt New} are added when learning CIFAR100. In contrast, the exploitation-exploitation strategy can learn to reuse the components from the ImageNet task (Figure 4 in the main text), and achieves better accuracy as well (Table 2 in the main text).



\section{Preliminary Results on Extending the ArtiHippo for Class-Incremental Settings}
\label{sec:class-incremental}
The similarity based sampling method can be naturally extended to a class-incremental setting where the task indices are not available at test time. We calculate the similarity between the class token at each block of the ViT when performing predictions with an image. This is done for each task by activating the correct set of experts associated with the task. For each task, this generates $L$ similarity scores, where $L$ is the number of blocks in the transformer
\begin{equation}
    S_t^l = \text{CosineSim}(\mu_{e_t}, x_0^l)
\end{equation}
where $x_0^l$ is the class token at block $l$. Note that we do not normalize the cosine similarity. The similarity scores for all the blocks are combined to form an indicator score $I^t(x)$ per task. The test image is assigned the index of the task with the highest indicator score: $t \gets \text{argmax}_t\ I^t(x)$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/vdd-structure-seq2.pdf}
    \caption{Architecture learned for task sequence ImNet, OGlt, C100, SVHN, UCF, GTSR, DPed, Flwr, Airc, DTD}
    \label{fig:vdd_arch_seq2} \vspace{-4mm}
\end{figure}\vspace{3mm}

We explore two methods:

\begin{table}[t]
    \vspace{2em}
    \centering
    \begin{tabular}{c|cc}
    \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{Average Accuracy} \\
        \cline{2-3}
        & 5-Datasets & VDD \\
        \midrule
        Max & $34.35 \pm 15.75$ & $32.96 \pm 2.07$ \\
        Minimum Entropy & $66.09 \pm 3.98$ & $45.69 \pm 1.96$ \\
        \bottomrule
    \end{tabular}
    \caption{Average Accuracy in a class-incremental setting using the proposed methods. The results for 5-datasets benchmark have been averaged over 5 different task orders, whereas the results on the VDD benchmark have been averaged over the same tasksequence but different random seeds.}
    \label{tab:task-agnostic-acc}
\end{table}

\textbf{Max}: For each task, the maximum similarity score from all the blocks is chosen as the indicator score
\begin{equation*}
    I^t(x) = \text{max}(S_{1:L}^t)
\end{equation*}

\textbf{Minimum Entropy}: For each task, the entropy using each path is calculated, and the task for which the classification head shows the minimum entropy is chosen.

Table \ref{tab:task-agnostic-acc} shows the average accuracies obtained on the 5-datasets benchmark. The low accuracy scores suggest that although the mean class token is suitable for sampling in the supernet training and the evolutionary search, its applicability is limited for task agnostic inference. However, the results are promising, and could be made better by introducing a mixture of class-tokens per task to account for the variations in tasks, similar in spirit to the mixture modeling of experts of ArtiHippo and the learn-to-prompt method~\citep{learning-to-prompt}, which we leave for future work.



\end{document}