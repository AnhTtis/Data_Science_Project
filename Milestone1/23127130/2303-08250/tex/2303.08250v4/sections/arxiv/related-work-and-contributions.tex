\section{Related Work}


\textit{Experience Replay Based approaches} aim to retain some exemplars,  in the form of either raw data or latent features, from the previous tasks and replay them to the model along with the data from the current task~\cite{gradient-based-sample-selection,mir,large-scale-er,rainbow-memory,hindsight,agem,continual-prototype-evolution,remind,gem,gdumb,icarl,tiny-replay,hayes-icra,dark-experience-replay,large-scale-inc-learning,fast-and-slow,contrastive-continual-learning}. Instead of storing raw exemplars, \textit{Generative replay methods}~\cite{generative-replay,gan-memory} learn the generative process for the data of a task, and replay exemplars sampled from that process along with the data from the current task. 
For exemplar-free continual learning, \textit{Regularization Based approaches} explicitly control the plasticity of the model by preventing the parameters of the model from deviating too far from their stable values learned on the previous tasks when learning the current task~\cite{what-not-to-forget,selfless-sequential-learning,podnet,variational-continual-learning,kirkpatrick-overcoming,lwf,synaptic-intelligence,progress-and-compress}. Both these approaches aim to balance the stability and plasticity of a fixed-capacity model.

\textit{Dynamic Models} aim to use different parameters for each task to eliminate the use of stored exemplars. Dynamically Expandable Network~\cite{dynamic-expandable-nets} adds neurons to a network based on learned sparsity constraints and heuristic loss thresholds. PathNet~\cite{pathnet} finds task-specific submodules from a dense network, and only trains submodules not used by other tasks. Progressive Neural Networks~\cite{pnn} learn a new network per task and adds lateral connections to the previous tasks' networks. ~\cite{vdd} learns residual adapters which are added between the convolutional and batch normalization layers. \cite{network-of-experts} learns an expert network per task by transferring the expert network from the most related previous task. The L2G~\cite{learn-to-grow} uses Differentiable Architecture Search (DARTS)~\cite{darts} to determine if a layer can be reused, adapted, or renewed for a task, which is tested for ConvNets and the learning-to-grow operations are applied uniformly at each layer in a ConvNet. Our method is motivated by the L2G method, but with substantially significant differences. 

Recently, there has been increasing interest in continual learning using Vision Transformers~\cite{learning-to-prompt,dualprompt,meta-attention,pool-of-adapters,dytox,towards-exemplar-free-continual-learning-vits,improving-vits,continual-obj-det-kd,memory-transformer,s-prompts,lifelong-vision-transformer,d3former,Gao_2023_ICCV}. \textit{Prompt Based approaches} learn external parameters appended to the data tokens that encode task-specific information useful for classification~\cite{learning-to-prompt,s-prompts,dytox,coda-prompt}. 
Our proposed method is complementary to those prompting-based ones. 

