\section{Our Proposed CHEEM}\label{sec:approach}
In this section, we present details of our proposed CHEEM.  

\subsection{Identifying a Task-Synergy Memory in ViT}

We start with a vanilla $D$-layer ViT model (e.g., the 12-layer ViT-Base)~\citep{vit}.
The left of Fig.~\ref{fig:flow} shows a ViT block. Denote by $x_{L,d}$ an input sequence consisting of $L$ tokens encoded in a $d$-dimensional space. In ViTs, the first token is the so-called class-token, {\tt CLS}. The remaining $L-1$ tokens are formed by patchifying an input image and then embedding patches, together with additive positional encoding. A ViT block is defined by, 
\begin{align}
    z_{L, d} & = x_{L, d} + \text{Proj}(\text{MHSA}(\text{LN}_1(x_{L, d}))), \label{eq:mhsa_proj} \\
    y_{L, d} & = z_{L, d} + \text{FFN}(\text{LN}_2(z_{L, d})))), \label{eq:ffn}
\end{align}
where $\text{LN}(\cdot)$ represents the layer normalization~\citep{ba2016layer}, and $\text{Proj}(\cdot)$ is a linear transformation fusing the multi-head outputs from MHSA module. 
The MHSA realizes the dot-product self-attention between Query and Key, followed by aggregating with Value, where Query/Key/Value are linear transformatons of the input token sequence. The FFN is often implemented by a multi-layer perceptron (MLP) with a feature expansion layer $\text{MLP}^{\text{u}}$ and a feature reduction layer $\text{MLP}^{\text{d}}$ with a nonlinear activation function (such as the GELU~\citep{hendrycks2016gaussian}) in the between.


\begin{wraptable} {r}{0.5\textwidth}
\vspace{-3mm}
    \centering
    \caption{Ablation study on identifying a component in a ViT block (Eqns.~\ref{eq:mhsa_proj} and~\ref{eq:ffn}) as the task-synergy memory in TCL. 
    The last row shows the result of a conventional transfer learning setting in which only the head classifier is trained. 
    }
    \vspace{1mm}
    \resizebox{0.5\textwidth}{!}{
        \begin{tabular}{l|l|c|c}
            \toprule
            Index & Finetuned Component &  Avg. Acc. & Avg. Forgetting \\
            \midrule
            1 & $\text{LN}_1$ + $\text{LN}_2$ & $81.76$ & $21.24$ \\
            \midrule
            2 & $\text{FFN}$ & $84.20$ & $44.76$ \\
            3 & $\text{MLP}^{\text{d}}$ & $83.66$ & $37.99$ \\
            4 & $\text{LN}_2$ & $80.04$ & $16.35$ \\
            \midrule
            5 & MHSA + $\text{LN}_1$ & $85.26$ & $54.38$ \\
            6 & LN$_1$ & $81.18$ & $19.04$ \\
            7 & Query & $81.57$ & $19.69$ \\
            8 & Key & $81.56$ & $19.19$ \\
            9 & Query+Key & $81.49$ & $31.10$ \\
            10 & Value & $84.99$ & $37.58$ \\
            \rowcolor{gray!30}  11 & Projection  (\textbf{CHEEM}) & $85.11$ & $30.50$ \\
            \midrule
            \multicolumn{2}{c|}{Classifier w/ Frozen Backbone} & $70.78$ & - \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:acc_vs_forgetting}
    \vspace{-2mm}
\end{wraptable}

The proposed identification process is straightforward. Without introducing any modules handling forgetting, we compare both the task-to-task forward transferrability and the sequential forgetting of different components in a ViT block.\textbf{ Our intuition is that a desirable component for playing the role of task-synergy memory must enable strong transferrability with manageable forgetting. }






To that end, we use the 10 tasks in the VDD benchmark~\citep{vdd} (see Fig.~\ref{fig:vdd-dataset-overview}). We first compare the transferrability of the ViT trained with the first task (ImageNet) to the remaining 9 tasks in a pairwise task-to-task manner and compute the average Top-1 accuracy on the 9 tasks (see Eqn.~\ref{eq:avg_acc}). Then, we start with the ImageNet-trained ViT, and train it on the remaining 9 tasks continually and sequentially in a predefined order (i.e., sequentially fine-tuning the ImageNet-trained ViT backbone in a plain way). We compute the average forgetting on the first 9 tasks (including ImageNet) (see Eqn.~\ref{eq:avg_forgetting}).
As shown in Table~\ref{tab:acc_vs_forgetting}, we compare 11 components or composite components across all blocks in the ImageNet-pretrained ViT. 
Consider the strong forward transfer ability, manageable forgetting, maintaining simplicity and for less invasive implementation in practice, \textbf{we select the Projection layer after the MHSA (Fig.~\ref{fig:flow} and Eqn.~\ref{eq:mhsa_proj}) as the task-synergy memory to realize our proposed CHEEM.} %



\subsection{Learning CHEEM}

\textbf{The proposed CHEEM is represented by a Mixture of Experts}, similar in spirit to~\citep{vision-moe}. After the first task, the CHEEM at the $l$-th layer in the ViT model consists of a single expert defined by a tuple, 
\begin{equation}
{\tt E}_l^{(1,)}=(\theta_l^{(1,)}, \mu_l^{1}),    
\end{equation}
\begin{wrapfigure}{r}{0.55\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/cheem_nas_v3.pdf}
    \caption{Illustration of CHEEM learning via NAS.  }
    \label{fig:nas}
\end{wrapfigure} 
where the subscript presents the layer index and the list-based superscript shows which task(s) use this expert. $\theta_l^{(1,)}$ are the parameters of the projection layer and $\mu_l^{1}\in R^d$ is the associated mean class-token, {\tt CLS} pooled from the training dataset after the model is trained, which is task specific (as indicated by the superscript).  For example, if an expert is reused by another task (say, 3) in continual learning, we will have ${\tt E}_l^{(1,3,)}=(\theta_l^{(1,3,)}, \mu_l^{1}, \mu_l^{3})$. 




As shown in Fig.~\ref{fig:nas},  for a new task $t$, learning to update CHEEM consists of three components: i) the Supernet construction (the parameter space of updating CHEEM), ii) the Supernet training (the parameter estimation of updating CHEEM), and iii) the target network selection and finetuning (the consolidation of the CHEEM for the task $t$).

\subsubsection{Supernet Construction via {\tt Reuse, Adapt, New} and {\tt Skip}}\label{sec:supernet_construction}

For clarity, we consider how the space of CHEEM is constructed at a single layer $l$ for a new task, assuming the current CHEEM consists of two experts, $\{{\tt E}_l^{(1,)}, {\tt E}_l^{(2,)}\}$ (Fig.~\ref{fig:nas}, left). {We utilize four operations in the Supernet construction:} 
\begin{itemize} [leftmargin=*,noitemsep,topsep=0pt]
    \item  {\tt Skip}: Skips the entire MHSA block, which encourages the adaptivity accounting for the diverse nature of tasks. 
    \item   {\tt Reuse}: Uses the projection layer from an old task for the new task unchanged, which will help task synergies in learning. 
    \item   {\tt Adapt}: Introduces a new lightweight layer on top of the projection layer of an old task, implemented by a MLP with one squeezing hidden layer.
    \item  {\tt New}: Adds a new projection layer, which enables the model to handle corner cases and novel situations.
\end{itemize}

The bottom of Fig.~\ref{fig:nas} shows the search space. The Supernet is constructed by {\tt reusing} and {\tt adapting} each existing expert at layer $l$, and adding a {\tt new} and a {\tt skip} expert. The newly added {\tt adapt} MLPs and projection layers will be trained from scratch using the data of a new task only. The right-top of Fig.~\ref{fig:nas} shows the {\tt Adapt} operation on top of ${\tt E}_l^{(2,)}$ is learned and added, ${\tt E}_l^{(3,)}=(\theta_l^{(3,)}, \mu_l^3)$ where $\theta_l^{(3,)}$ represents parameters of the {\tt adapt} MLPs learned for the task 3, and $\mu_l^3$ is the mean {\tt CLS} token pooled for the task 3. The expert ${\tt E}_l^{(2,)}$  is updated to ${\tt E}_l^{(2,3,)}=(\theta_l^{(2,3,)}, \mu_l^2)$ indicating its weights will be  shared with task 3.

\textit{How to {\tt Adapt} in a sustainable way?} The proposed {\tt Adapt} operation will effectively increase the depth of the network in a plain way. In the worst case, if too many tasks use {\tt Adapt} on top of each other, we will end up stacking too many MLP layers together. This may lead to unstable training due to gradient vanishing. Shortcut connections~\citep{resnet} have been shown to alleviate the gradient vanishing problems. We introduce the shortcut connection in adding a MLP {\tt Adapt} operation. We test two different implementations: with shortchut in all the three components (supernet training, target network selection and target network finetuing)  versus with shortcut only in target network finetuning. The latter is preferred as analyzed in Appendix~\ref{sec:hybrid-adapter}. 



\subsubsection{Supernet Training via the Proposed HEE-based NAS}\label{sec:supernet_train}



To train the Supernet constructed for a new task $t$, we build on the SPOS method \citep{spos} due to its efficiency. The basic idea of SPOS is to train a single-path sub-network from the Supernet by sampling an expert at every layer in each mini-batch of training. One key aspect is the sampling strategy. The vanilla SPOS method uses uniform sampling (i.e., the \textit{pure exploration} (PE) strategy, Fig.~\ref{fig:nas-sampling} left).
We propose an exploitation strategy (Fig.~\ref{fig:nas-sampling} right), which utilizes a hierarchical sampling method that forms the categorical distribution over the operations in the search space \textit{explicitly based on task synergies computed based on the pooled task-specific {\tt CLS} tokens}. 




Consider a new task $t$ with the training dataset $D^{train}_t$, with the current supernet consisting of $t-1$ task-specific target networks, we first run inference of the $t-1$ target networks on $D^{train}_t$ to pool initial {\tt CLS} tokens for each expert, e.g., $\mu_l^{1\rightarrow 3}$ and $\mu_l^{2\rightarrow 3}$ in the bottom of Fig.~\ref{fig:nas}. 
Consider one expert ${\tt E}_l^{(i,j,)}$ at the $l$-th layer which is shared by two previous tasks $i$ and $j$ with their mean {\tt CLS} tokens $\mu_l^i$ and $\mu_l^j$ respectively, we have the pooled {\tt CLS} tokens for the current task $t$,  $\mu_l^{i\rightarrow t}$ and $\mu_l^{j\rightarrow t}$, computed accordingly. The task similarity is computed by, 
\begin{equation}
    S_l^{i,t} = \texttt{NormCosine}(\mu_l^i, \mu_l^{i\rightarrow t}),
\end{equation}
where {\tt NormCosine}$(\cdot, \cdot)$ is the Normalized Cosine Similarity, which is calculated by scaling the Cosine Similarity score between $-1$ and $1$ using the minimum and the maximum Cosine Similarity scores from all the experts in all the MHSA blocks of the ViT. This normalization is necessary to increase the difference in magnitudes of the similarities between tasks, which results in better Expert sampling distributions during the sampling process in our experiments. The task similarity score will be used in sampling the {\tt Reuse} and {\tt Adapt} operations. 

For the new task $t$, we also have the {\tt New} expert and the {\tt Skip} expert at each layer $l$, for which we do not have similarity scores. Instead, we introduce an auxiliary expert, {\tt Aux} (see the right of Fig.~\ref{fig:nas-sampling}) which gives equally-likely chance to select the {\tt New} expert or the {\tt Skip} expert  once sampled in NAS. For the {\tt Aux} expert itself, the similarity score between it and the new task $t$ is specified by,
\begin{equation}
    S_l^{aux, t}=- \max_{i=1}^{t-1} S_l^{i,t} ,
\end{equation}
which intuitively means we probabilistically resort to the {\tt New} operation or the {\tt Skip} operation when other experts turn out not ``helpful'' for the task $t$.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/nas-sampling-v3.pdf}
    \caption{
    Illustration of the proposed hierarchical task-synergy exploration-exploitation (HEE) sampling based NAS. For efficiency, we build on the single-path one-shot (SPOS) NAS method~\citep{spos}. It integrates the vanilla  exploration strategy (left) and the proposed exploitation strategy (right) with an epoch-wise scheduling.}
    \label{fig:nas-sampling}
\end{figure}





At each layer $l$ in the ViT, for a new task $t$, the task-similarity oriented operation sampling is realized by a 2-level hierarchical sampling, as illustrated in the right of Fig.~\ref{fig:nas-sampling}: 
\begin{itemize} [leftmargin=*,noitemsep,topsep=0pt]
\itemsep0em
    \item The first level uses a categorical distribution with the maximum number of entries being $t$ consisting of at most the previous $t-1$ tasks (some of which may use {\tt Skip} and thus will be ignored) and the {\tt Aux} expert.  The categorical distribution $(\psi_1, \cdots, \psi_i, \cdots, \psi_{I-1}, \psi_I)$ is computed by the Softmax function over the similarity scores defined above, where $I\leq t$.  
    \item With a previous task $i$ sampled with the probability $\psi_i$, at the second level of sampling, we sample the {\tt Reuse} operation for the associated expert using a Bernoulli distribution with the succcess rate computed by the Sigmoid function of the task similarity score defined by $\rho_i = \frac{1}{1+\exp(-S^{i,t}_l)}$, and the {\tt Adapt} operation with probability $1-\rho_i$. 
\end{itemize}



\subsubsection{Target Network Selection and Finetuning}
\label{sec:target-network-selection}

After the Supernet is trained, we adopt the same evolutionary search used in the SPOS method~\citep{spos} based on the proposed hierarchical sampling strategy. The evolutionary search is performed on the validation set to select the path which gives the best validation accuracy. After the target network for a new task is selected, we retrain the newly added layers by the {\tt New}  and {\tt Adapt} operations from scratch (random initialization), rather than keeping or warming-up from the weights from the Supernet training. This is based on the observations in network pruning that it is the neural architecture topology that matters and that the warm-up weights may not need to be preserved to ensure good performance on the target dataset~\citep{liu2018rethinking}.

\subsubsection{Balancing Exploration and Exploitation}\label{sec:balancing}
As illustrated in Fig.~\ref{fig:nas-sampling}, to harness the best of the pure exploration strategy and the proposed exploitation strategy, we apply epoch-wise exploration and exploitation sampling for simplicity. For the pure exploration, we directly uniformly sample the experts at a layer $l$, consisting of the $n$ experts from the previous $t-1$ tasks, and the {\tt New} and {\tt Skip} operations, where $n\leq t-1$. 
At the beginning of an epoch in the Supernet training, we choose the pure exploration strategy with a probability of $\epsilon_1$ (e.g., 0.3), and the hierarchical sampling strategy with  a probability of $1-\epsilon_1$. Similarly, when generating the initial population during the evolutionary search, we draw a candidate target network from a uniform distribution over the operations with a probability of $\epsilon_2$, and from the hierarchical sampling process with a probability of $1-\epsilon_2$, respectively. 
In practice, we set $\epsilon_2 > \epsilon_1$ (e.g., $\epsilon_2=0.5$) to encourage more exploration during the evolutionary search, while encouraging more exploitation for faster learning in the Supernet training. 
































