\section{Conclusion}
This paper presents a method of transforming Vision Transformers (ViTs) for resilient task-incremental continual learning (TCL) with catastrophic forgetting overcome. It identifies  the final projection layers of the multi-head self-attention blocks as the task-synergy memory in ViTs, which is then updated in a task-aware way using four operations, {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New}. The learning of task-synergy memory is realized by a proposed hierarchical exploration-exploitation sampling based single-path one-short Neural Architecture Search algorithm, where the exploitation utilizes task similarities defined by the normalized cosine similarity between the mean class tokens of a new task and those of old tasks. The proposed method is dubbed as CHEEM (Continual Hierarchical-Exploration-Exploitation Memory). In experiments, the proposed method is tested on the challenging VDD and the 5-Datasets benchmarks.  
It obtains better performance than the prior art with sensible CHEEM learned continually. 
We also take great efforts in materializing several state-of-the-art baseline methods for ViTs and tested on the VDD, which are released in our code. 




