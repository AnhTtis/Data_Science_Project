\section{Experiments}

We test our CHEEM on two benchmarks and compare with the prior art. 
It obtains better performance than the prior art in comparisons.  \textbf{Our PyTorch code is provided in the supplementary. The Appendix provides implementation details~\ref{sec:training-details} and more results (e.g., effects of different task orders on the VDD benchmark~\ref{sec:task_orders}).} 
We use 1 Nvidia A100 GPU in all experiments.

\textbf{Data}:
We evaluate our approach on the VDD benchmark~\citep{vdd} and the 5-Datasets benchmark introduced in\citep{adversarial-continual-learning}. 
The VDD benchmark is challenging because of the large variations in tasks as well as small number of samples in many tasks, which makes it a favorable for evaluating continual learning algorithms. More details are in Appendix~\ref{sec:dataset-details}.

\textbf{Metric}: We consider two task-incremental settings and compare with the corresponding prior art. 
\textbf{i) TCL via Task-to-Task Transfer Learning.} It always starts from the model trained for the first task with the feature backbone and head classifier $(f_{1}, C_1)$ for any subsequent tasks. Let $f_{T_{n|1}}$ be the backbone trained for the task $n$ ($n=2, .. N$) with weights initialized from the model of task $1$, and $C_n$ the  head classifier trained from scratch. The average accuracy is defined as:
\begin{equation}
    A_{1:N} = \frac{1}{N}\sum_{n=1}^N \text{Acc}(T_n; f_{T_{n|1}}, C_n) \label{eq:avg_acc}
\end{equation}
where $\text{Acc}()$ uses the Top-1 accuracy, and $f_{T_{1|1}} = f_1$. Under this paradigm, there is no catastrophic forgetting. Depending on how $f_{T_{n|1}}$ is trained, it may require to keep $N$ separate model checkpoints.   
\textbf{ii) TCL via Sequential and Continual Learning.} Let $f_{T_{1:n}}$ be the backbone trained sequentially and continually after task $T_n$ and $C_n$ is its head classifier. 
The average accuracy is, 
\begin{equation}
\mathbf{A}_{1:N} = \frac{1}{N}\sum_{n=1}^N \text{Acc}(T_n; f_{T_{1:N}}, C_n). \label{eq:avg_accuracy_lll}    
\end{equation} 
The average forgetting~\citep{riemannian-walk} on the first $N-1$ tasks is,
\begin{equation}
    \mathbb{F}_{1:N-1} = \frac{1}{N-1}\sum_{n=1}^{N-1} \left(\max_{j\in [n, N-1]} a_{j,n} - a_{N,n}\right),
    \label{eq:avg_forgetting}
\end{equation}
where $a_{j,n}=\text{Acc}(T_n; f_{T_{1:j}}, C_n), j>n$.
Depending whether $f_{T_{1:n}}$ is task-aware or not, the average forgetting could be either zero or not. This continual learning paradigm (rather than task-to-task paradigm) is more interesting in the literature, and also our focus in this paper. 

\begin{table} [t]
    \centering
    \caption{Results on the VDD benchmark~\citep{vdd} using ViT-B/8~\citep{vit} over 3 different seeds. The average accuracy is computed based on Eqn.~\ref{eq:avg_accuracy_lll}. The results of \textcolor{gray}{L2G-ResNet26 (DARTS)} are directly extracted from~\citep{learn-to-grow} (in which the learning-to-grow is applied at every layer). The number of training images / \#\textit{categories} per task is shown.
    }
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{l|l|l|cccccccccc|l}
        \toprule
            \multirow{2}{*}{Method} &  \multicolumn{2}{c|}{Epochs} &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  \multirow{2}{*}{Avg. Accuracy} \\ \cline{2-3}
            & Super & Target & 1.1M/\textit{1000} &36k/\textit{100}&42.4k/\textit{10}&6.8k/\textit{101}&16.1k/\textit{1623}&28.2k/\textit{43}&21.2k/\textit{2}&0.92k/\textit{102}&3.0k/\textit{100}&1.7k/\textit{47}&\\
        \midrule           
            {S-Prompts} (\#Prompts=12) & n/a & 100 & $82.65$ & $89.32$ & $88.91$ & $64.52$ & $72.17$ & $99.29$ & ${99.89}$ & $\boldsymbol{96.93}$ & $45.55$ & $\boldsymbol{60.76}$ & $80.00 \pm 0.07$ \\
            {L2P} (\#Prompts=12) & n/a & 100 & $82.65$ & $89.32$ & $89.89$ & $65.63$ & $72.34$ & $99.55$ & $\boldsymbol{99.94}$ & ${96.63}$ & $45.24$ & ${59.57}$ & $80.08 \pm 0.10$ \\
            \midrule
            L2G (DARTS) & 50 & 30 & $82.65$ & $88.47$ & $85.20$ & $\boldsymbol{79.22}$ & $80.19$ & $99.28$ & $ 98.06$ & $76.14$ & $39.29$ & $46.01$ & $77.45 \pm 2.41$ \\ 
            L2G ($\beta$-DARTS) & 50 & 30 & $82.65$ & $88.95$ & $94.73$ & $75.31$ & $79.76$ & $99.84$ & $99.76$ & $78.86$ & $34.50$ & $47.09$ & $78.14 \pm 0.54$ \\ 
            \textcolor{gray}{L2G-ResNet26 (DARTS)} & - & - &  \textcolor{gray}{$69.84$} & \textcolor{gray}{$79.59$} & \textcolor{gray}{$95.28$} & \textcolor{gray}{$72.03$} &  \textcolor{gray}{$\boldsymbol{86.6}$} & \textcolor{gray}{$99.72$} & \textcolor{gray}{$99.52$} & \textcolor{gray}{$71.27$} &  \textcolor{gray}{${53.01}$} & \textcolor{gray}{$49.89$} & \textcolor{gray}{$77.68$} \\
            \midrule
            \rowcolor{gray!30} Our CHEEM & 100 & 30 & $82.65$ & $\boldsymbol{90.54}$ & $\boldsymbol{96.12}$ & $75.53$ & ${83.81}$ & $\boldsymbol{99.93}$ & $99.88$ & $91.21$ & $\boldsymbol{55.59}$ & $59.18$ & $\boldsymbol{83.44} \pm {0.50}$ \\
        \bottomrule
    \end{tabular}}
    \label{tab:vdd-results} \vspace{-2mm}
\end{table}

\subsection{Results on the VDD Benchmark}
We report results using the two settings stated  above as follows. 

\subsubsection{Results by Sequential and Continual Learning}
We compare with three state-of-the-art methods: the Learning-to-Grow (L2G) method \citep{learn-to-grow}, the Learning-to-Prompt (L2P) method  \citep{learning-to-prompt} and the S-Prompts method~\citep{s-prompts}. We modify L2G to work with ViTs using two NAS algorithm, the DARTS~\citep{darts} used by the vanilla L2G and a recently improved $\beta$-DARTS~\citep{beta-darts}. We also apply L2G only to the final projection layer in the MHSA.  We modify both L2P and S-Prompts for VDD under the task-incremental setting (Appendix~\ref{sec:task-inc-l2p-sprompt}).  

As shown in Table~\ref{tab:vdd-results}, \textbf{our CHEEM obtains the best overall performance with a significant margin around $3.3\%$.} We observe: 





\textbf{i) Frozen/Fixed Backbone vs Task-Aware Dynamic Backbone.} Both L2P and S-Prompts use the frozen backbone from the first ImNet task, while L2G and our CHEEM learn to grow the backbone. Compared to L2P and S-Prompts, although L2G is worse in terms of overall performance, it works much better for tasks that are different from the ImNet such as the three, SVHN, UCF and OGlt, where L2G can outperform L2P by more than $4\%$, $13\%$ and $7\%$ respectively. Overall, we observe that learning task-aware dynamic backbone is beneficial in continual learning with sensible architectures learned (Fig.~\ref{fig:vdd_arch-sim}), verifying our motivation stated in Sec.~\ref{sec:intro}. 



To further show the advantage of learning to grow the feature backbone in sequential and continual learning. We also compare with state-of-the-art weight regularization based continual learning methods:  the L2 Parameter Regularization \citep{smith2023closer} and the Elastic Weight Consolidation (EWC) \citep{kirkpatrick-overcoming}, and a strong baseline of Experience Replay \citep{icarl}. As shown in Tabe~\ref{tab:weight-reg}, the three methods suffer from catastrophic forgetting significantly. We note that all the methods initially have the same performance on the first ImNet task (82.65). The two weight regularization based methods only maintain the ImNet trained weights via different regularization strategies without introducing any new task-specific parameters, and the experience play method keep track of a small coreset of examples of previous tasks, all the three leading to catastrophic forgetting of earlier tasks after trained on all tasks.

 \begin{table} [t] %
    \centering
    \caption{Both weight regularization based methods and experience replay methods (30 samples/class in the coreset) suffer from catastrophic forgetting. When trained for 50 epochs, EWC achieves an Avg. Accuracy of 62.38, L2 Regularization achieves 66.01 and Experience Replay achieves 64.30, indicating recency bias.}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{l|l|cccccccccc|c}
        \toprule
            Method & Epoch &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            
           EWC & 20 & $58.19$ & $87.69$ & $69.64$ & $57.27$ & $45.89$ & $95.01$ & $98.47$ & $90.20$ & $36.57$ & ${61.97}$ & $70.09$ \\ 
            L2 Regularization & 20 &$55.28$ & $87.10$ & $55.23$ & $58.86$ & $40.48$ & $95.07$ & $99.17$ & $90.20$ & $37.53$ & $\boldsymbol{62.55}$ & $68.15$ \\ \midrule
            Experience Replay  &20 & $55.88$ & $78.70$ & $87.40$ & $58.20$ & $76.03$ & $97.92$ & $48.55$ & $84.41$ & $40.98$ & $54.68$ & $68.27$ \\ 
            \midrule
            \rowcolor{gray!30} Our CHEEM &\textcolor{gray}{100+}30 & $  82.65 $ &    $  \boldsymbol{90.54} $ &$  \boldsymbol{96.12} $ &  $  \boldsymbol{75.53} $ &    $  \boldsymbol{83.81} $ & $  \boldsymbol{99.93} $ &         $  \boldsymbol{99.88} $ &       $  \boldsymbol{91.21} $ &    $  \boldsymbol{55.59} $ &$  59.18 $ &   $  \boldsymbol{83.44} \pm 0.50 $ \\
            
        \bottomrule
    \end{tabular}}
    \label{tab:weight-reg} \vspace{-2mm}
\end{table}

\textbf{Efficiency.} On the VDD benchmark, the number of parameters of our CHEEM increases by 1.06M/task (averaged over 3 different runs). Although this is higher than L2P~\citep{learn-to-grow} which uses 12 extra prompts 
 (0.01M/task), our method increases the number of FLOPs only by 0.17G/task (due to {\tt Adapt} experts), as compared to the increase of 2.02G/task of the L2P, $10$ times more expensive than ours, due to the quadratic complexity of MHSA with respect to the length of input token sequence. Similarly, our method requires less memory footprint.
 


\textbf{ii) Pure Exploration (PE) vs Our Proposed Hierarchical Task-Synergy Exploration and Exploitation (HEE) in the SPOS NAS}. We verify the effectiveness of our HEE sampling (Table~\ref{tab:uniform-hier}), which is significantly better by a large margin, $6\%$ absolute average accuracy increase. 
\begin{table} [h] 
    \centering
    \caption{The effectiveness of our proposed hierarchical exploration and exploitation sampling empowered SPOS NAS (Fig.~\ref{fig:nas-sampling}). The structure updates learned by the PE strategy are visualized in Fig.~\ref{fig:vdd_arch_seq1_pe} in the Appendix.}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{l|c|cccccccccc|l}
        \toprule
            Method & Epochs &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            
            PE & 100 & $82.65$ & $82.22$ & $95.23$ & $73.14$ & $81.42$ & ${99.92}$ & $99.83$ & $72.75$ & $38.27$ & $39.10$ & $76.45 \pm 0.90$ \\           
            \rowcolor{gray!30} HEE & 100 & $  82.65 $ &    $  \boldsymbol{90.54} $ &$  \boldsymbol{96.12} $ &  $  \boldsymbol{75.53} $ &    $  \boldsymbol{83.81} $ & $  \boldsymbol{99.93} $ &         $  \boldsymbol{99.88} $ &       $  \boldsymbol{91.21} $ &    $  \boldsymbol{55.59} $ &$  \boldsymbol{59.18} $ &   $  \boldsymbol{83.44} \pm 0.50 $ \\
            
        \bottomrule
    \end{tabular}}
    \label{tab:uniform-hier} %
\end{table}

 
\begin{wrapfigure}{r}{0.4\textwidth} 
    \centering
    \includegraphics[width=0.4\textwidth]{figures/epochwise_statistics_combined.pdf}
    \caption{Ablation studies of comparing the effectiveness of the vanilla PE and our proposed HEE sampling algorithm. 
    } 
    \label{fig:epochwise-efficiency} %
\end{wrapfigure} 
 To further verify whether the vanilla PE sampling can catch up the performance gap using more supernet training epochs, we compare it with our proposed  HEE sampling using 6 different numbers   of training epochs from 50 to 300 on the VDD benchmark across 3 different runs. Fig.~\ref{fig:epochwise-efficiency} shows the comparison in terms of both the average accuracy and the average number of parameter increased per task.  In terms of average accuracy, our proposed HEE with 50-epoch supernet training already consistently outperforms the vanilla PE with 300-epoch training. In terms of the average number of parameters increased per task, our proposed HEE is also significantly better with much less new parameters introduced, thanks to its task-similarity oriented sampling (resulting in many {\tt Reuse} and {\tt Adapt}). \textbf{We can clearly see the significance of our proposed HEE sampling to the success of learning-to-grow CHEEM in continual learning. } The similar observations remain in terms of changing task orders (Appendix~\ref{sec:task_orders}).    

 

 \textbf{iii) SPOS vs DARTS in the learning-to-grow NAS Algorithm.} As shown in Table~\ref{tab:vdd-results}, L2G+ViT-B using DARTS or $\beta$-DARTS do not show significant improvement over L2G+ResNet26 using DARTS, although ViT-B has significantly better performance than ResNet26~\citep{resnet} on the first ImNet task (82.65 vs 69.84). Furthermore, as shown in Table~\ref{tab:uniform-hier}, our CHEEM with uniform sampling based SPOS obtains similar performance with L2G. These may suggest that the vanilla L2G+DARTS do not suit ViTs well.  

\textbf{iv) Similar Tasks vs Dissimilar Tasks.} Both L2P and S-Prompts perform better on tasks similar to the first ImNet task and with less training data such as Flwr (918 training images) and DTD (1692 training images), which makes intuitive sense. 
 It also indicates that we could harness the best of prompting-based methods and our CHEEM, which we leave for the feature work. 
 

            



\subsubsection{Results by Task-to-Task Transfer based continual Learning}

 We compare with three state-of-the-art methods: Supermasks in Superposition (SupSup)~\citep{supsup}, Efficient Feature Transformation (EFT) \citep{eft} and Lightweight Learner (LL) \citep{ll}. We modify them to work with ViTs with details provided in Appendix~\ref{sec:modification-t2t}.

\begin{table} [h] %
    \centering
    \caption{Results on the VDD benchmark~\citep{vdd} using ViT-B/8~\citep{vit} under the task-to-task transfer based continual learning protocol. The learned CHEEM are visualized in Fig.~\ref{fig:residual-adapter-comparison-t2t} in the Appendix.}
    \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{l|c|cccccccccc|l}
        \toprule
            Method & Epochs &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
            \midrule
            {SupSup} & 50 & $82.65$ & $89.96$ & $\boldsymbol{96.05}$ & $\boldsymbol{81.68}$ & $\boldsymbol{84.60}$ & $\boldsymbol{99.97}$  & ${99.97}$ & $78.76$ & $44.18$ & $51.60$ & $81.14 \pm 0.04$ \\
            {EFT} & 50 & $82.65$ & ${91.86}$ & $93.51$ & $73.89$ & $75.62$ & $99.58$ & $\boldsymbol{99.98}$ & ${96.34}$ & $48.17$ & $\boldsymbol{64.40}$ & $82.60 \pm 0.07$ \\
            {LL} & 50 & $82.65$ & $\boldsymbol{91.92}$ & $93.90$ & $75.63$ & $77.07$ & $99.71$ & $99.96$ & $\boldsymbol{96.47}$ & ${49.33}$ & ${64.34}$ & ${83.10} \pm {0.02}$ \\ \midrule
            \rowcolor{gray!30} Our CHEEM & 50 & $82.65$ & $90.93$ & ${95.96}$ & ${80.74}$ & ${83.25}$ & ${99.94}$ & $99.96$ & $94.12$ & $\boldsymbol{58.90}$ & $60.05$ & $\boldsymbol{84.65} \pm {0.33}$ \\
        \bottomrule
    \end{tabular}}
    \label{tab:t2t} %
\end{table}


As shown Table \ref{tab:t2t}, our CHEEM also obtains the best overall performance. The SupSup method achieves a higher accuracy on tasks  different from the  first ImNet task such as UCF and OGlt, but does not perform well on similar tasks such as Flwr, Airc. and DTD, whereas both EFT and LL show the opposite behavior. Our CHEEM can perform well across all the tasks, indicating it is much less sensitive to the domain shifts in continual learning, thanks to its learning-to-grow core with a task-similarity oriented NAS. We note that our CHEEM under task-to-task transfer-based learning protocol obtains better performance than its counterpart under the sequential and continual learning protocol at 
the expense of introducing much more {\tt New} experts (the first one in Fig.~\ref{fig:residual-adapter-comparison-t2t} in the Appendix), compared to only one {\tt New} expert in Fig.~\ref{fig:vdd_arch-sim}. It also has more {\tt Skip}  experts.
The different behaviors between continual learning and task-to-task transfer learning with CHEEM indicate that there are some room in further improving our HEE-based NAS. %




 




\newpage
\subsection{Results on the 5-Dataset Benchmark} 


\begin{wraptable} {r}{0.4\textwidth}%
\vspace{-4mm}
    \centering
    \caption{Results on the 5-Dataset benchmark averaged over 5 different task orders.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{\#Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts & 12 & $92.42 \pm 0.11$ \\
        L2P & 12 & $92.73 \pm 0.10$ \\
        \midrule
        L2G (DARTS) & - & $93.88 \pm 2.86$ \\
        L2G ($\beta$-DARTS) & - & $92.19 \pm 1.48$ \\
        \midrule
        \rowcolor{gray!30} Our CHEEM & - & $\boldsymbol{94.82} \pm 0.02$ \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:5-dataset-results} %
\end{wraptable} 

Table \ref{tab:5-dataset-results} shows the comparisons using the continual learning protocol. We use the same ViT-B/8 backbone pretrained on the ImageNet from the VDD benchmark for all the experiments across all the methods and upsample the images in the 5-Dataset benchmark (consisting of CIFAR10~\citep{cifar}, MNIST \citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST \citep{notmnist}, and SVHN~\citep{svhn}) to $72\times72$. 
Our CHEEM obtains the best performance. 



\subsection{Limitations}
\label{sec:limitations}

One main limitation of our proposed CHEEM is the assumption of task index available in inference. To move forward from task-incremental continual learning to class-incremental continual learning with our proposed CHEEM, a potential solution is to infer the task index of a testing data on the fly. To that end, we may develop methods similar in spirit to S-Prompts~\citep{s-prompts}. S-Prompts focuses on domain-incremental learning (DIL) by learning domain-specific prompts using frozen pretrained Transformer backbones. It leverages the K-NN method in identifying the domain index in inference, which assumes separable Gaussian-type distributions of domain prompts and works well on the DIL benchmarks in their experiments.  
We will investigate how to leverage the task-aware mean {\tt CLS} tokens learned by each Expert in our CHEEM for explicit task index inference. In our preliminary investigation, we observed that the K-NN is a too strong assumption for benchmarks like the VDD which includes very similar tasks (e.g., CIFAR100 vs ImageNet). One potential solution is to integrate the task-aware mean {\tt CLS} tokens at multiple layers to learn a task index predictor.   



