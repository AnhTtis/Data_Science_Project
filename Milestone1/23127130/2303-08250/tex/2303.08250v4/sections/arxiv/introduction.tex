\section{Introduction}\label{sec:intro}

Developing continual learning machines is one of the hallmarks of Artificial Intelligence (AI), to mimic human intelligence in terms of learning-to-learn to be adaptive and skilled at streaming tasks. However, state-of-the-art Deep Neural Networks (DNNs) are not yet intelligent in the biological sense from the perspective of continual learning, especially plagued with the critical issue known as \textit{catastrophic forgetting} at streaming tasks in a dynamic environment~\citep{mccloskey,thrun}. 
To address catastrophic forgetting, there are two main categories of continual learning methods: exemplar-based methods~\citep{gradient-based-sample-selection,hayes-icra,large-scale-inc-learning} and exemplar-free methods~\citep{kirkpatrick-overcoming,learn-to-grow,learning-to-prompt,dualprompt,s-prompts}. Both have witnessed promising progress, while the latter is more challenging and has attracted more attention recently.  


In this paper, we focus the exemplar-free setting. Since no data of previous tasks in any forms will be available, the challenge is typically on how to retain the model parameters trained for previous tasks in updating the model for a new task, i.e. balancing the plasticity and the stability.
There have a vast literature of exemplar-free continual learning using Convolutional Neural Networks. 
One pioneering approach is to regularize the change in model parameters such as the popular Elastic Weight Consolidation (EWC) method~\citep{kirkpatrick-overcoming}. To be more flexible in handling complexities at streaming tasks, a popular paradigm is to dynamically select and/or expand the model such as the learn-to-grow method~\citep{learn-to-grow}, Supermask in Superposition (SupSup)~\citep{supsup}, Lightweight Learner~\citep{ll}, the calibration method~\citep{singhCalibratingCNNsLifelong2020}, the efficient feature transformation method~\citep{eft} and the Channel-wise Lightweight Reprogramming (CLR) method~\citep{clr}. 

More recently, with the availability of powerful pretrained Transformer models~\citep{attention-is-all-you-need,vit,clip},  
for continual learning, as illustrated in Fig.~\ref{fig:method-overview}, two main approaches are the prompt-based design~\citep{learning-to-prompt,dualprompt,s-prompts,coda-prompt}, and the parameter-tuning based design~\citep{supsup,meta-attention,piggyback,ll,clr,pool-of-adapters,nettailor}, both of which do not dynamically and structurally update the model architectures. \textbf{It remains an open problem of structurally and dynamically updating Transformer models for continual learning due to the challenge of taming Transformer models in general.} The intuition underlying the need of doing such updates in continual learning is to flexibly handle the varying complexities of streaming tasks in the wild.

\begin{figure} [t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/method-overview_v3.pdf}
    \caption{Comparisons of different continual learning methods using Vision Transformers~\citep{vit,attention-is-all-you-need}. \textit{(a) Prompt-based methods}~\citep{learning-to-prompt,dualprompt,s-prompts,coda-prompt} leverage a pretrained and frozen Transformer and learn task-specific prompts. \textit{(b) Parameter-tuning based methods} introduce task-specific layer-wise parameters on top of a pretrained and frozen Transformer, and are different in how the pretrained layer and the task-specific layer are fused, e.g., the parameter-masking methods~\citep{supsup,meta-attention,piggyback} and the output-addition methods~\citep{ll,clr,pool-of-adapters,nettailor}. They often introduce task-specific layers at every pretrained layer. \textit{(c) Our proposed method} utilizes four  operations to sequentially and continually maintain task-synergy memory: {\tt Reuse}, {\tt New}, {\tt Adapt} and {\tt Skip}  at streaming tasks.
    }
    \label{fig:method-overview}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/VDD-overview.pdf}
    \caption{Illustration of task-incremental continual learning on the Visual Domain Decathlon (VDD)~\citep{vdd} benchmark, which consists of 10 tasks with \#training images/\#categories significantly varying across different tasks. As commonly adopted in the literature, we assume the first (base) task has sufficient data to train a base model, for which we use the ImageNet-1k in our experiments.}
    \label{fig:vdd-dataset-overview} %
\end{figure}



We aim to address the above challenge in this paper. To that end, we study task-incremental continual learning (TCL), in which tasks often have significantly different label spaces and task IDs of data are available in both training and testing. For example, we consider the challenging VDD benchmark~\citep{vdd}, as illustrated in Fig.~\ref{fig:vdd-dataset-overview}. We are aware that class-incremental continual learning (CCL) has gained much more attention recently, which does not assume the availability of task IDs of testing data. However, we note that most of existing CCL methods using Transformers often assume a common classification space (e.g., all the stream tasks have the same number of classes such as L2P~\citep{learning-to-prompt}, CODAPrompt~\citep{coda-prompt}, and DER~\citep{der}), or the total number of classes throughout continual learning is assumed to be known~\citep{intro-lifeling-supervised-learning}), or assume the streaming (domain-incremental) tasks are mostly separable via $k$-NN in the prompt space such as S-Prompts~\citep{s-prompts}, which are not applicable in the VDD benchmark~\citep{vdd} of interest in this paper. 

















\textbf{So, we choose to take one step forward by studying TCL to gain insights on how to structurally and dynamically update Transformer models at streaming tasks in the VDD benchmark}. 
We aim to improve the overall streaming-task performance without catastrophic forgetting (i.e., resiliency) by learning task synergies (e.g., a new task learns to automatically reuse/adapt modules from previous similar tasks, or to introduce new modules when needed, or to skip some modules when it appears to be an easier task). 
It is not feasible to devote all components of a Transformer model to be dynamic \begin{wrapfigure} {r}{0.65\textwidth} \vspace{-2mm}
        \centering
    \includegraphics[width=1.0\linewidth]{figures/cheem_flow-v2.pdf}
    \caption{Illustration of the proposed CHEEM. \textit{Left:} A Transformer block in ViTs~\citep{vit} with the proposed CHEEM placed at the original output projection layer after the MHSA. \textit{Middle:} The CHEEM is maintained by four operations.      
    \textit{Right:} An example of learned CHEEM for different tasks (e.g., $j$) starting from $i$. %
    }
    \label{fig:flow}%

\end{wrapfigure}
 in continual learning due to considerations in two-fold: the computational cost to compensate the quadratic complexity of multi-head self-attention (MHSA) in Transformer models, and the trade-off between the plasticity and the stability. 
As illustrated in Fig.~\ref{fig:flow}, we identify the output projection layer after MHSA as the task-synergy memory that will be structurally and dynamically updated in continual learning. 
We present an effective hierarchical task-synergy exploration-exploitation (HEE) sampling based NAS method to learn to select the four operations in Fig.~\ref{fig:method-overview} (c).  The proposed method is thus dubbed as \textbf{CHEEM} (\textit{Continual Hierarchical-Exploration-Exploitation Memory}). 
With our proposed CHEEM, we observe sensible structures learned to update as illustrated in Fig.~\ref{fig:vdd_arch-sim}. For example, Tsk2\_C100 is very similar to Task1\_ImNet, thus {\tt Reuses} most of the blocks from Tsk1\_ImNet, but {\tt Adapts} `IN\_B4' (low-to-middle level features) and introduces a {\tt New} B12 block (the high-level features). Tsk3\_SVHN is very different from both Tsk1\_ImNet and Tsk2\_C100 and a relatively easier task, which learns to {\tt Adapt} many of the blocks from Tsk1\_ImNet with two (B10 and B11) blocks {\tt Skipped}. We note that the learned task synergies make a lot intuitive sense considering the nature of different tasks, which shows the effectiveness of our proposed CHEEM.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/structure-ee-100-4242.pdf}
    \caption{An example of the CHEEM learned on the VDD benchmark~\citep{vdd} with the task sequence shown in Fig.~\ref{fig:vdd-dataset-overview}. \colorbox{skip}{S}, \colorbox{reuse}{R}, \colorbox{adapt}{A} and \colorbox{new}{N} represent {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New} respectively. Starting from the ImageNet-trained ViT~\citep{vit} (B1 \--- B12 in Tsk1\_ImNet), sensible structures  are continually learned for the subsequent 9 tasks.  The last two columns show the number of new task-specific parameters and added FLOPs respectively, in comparison with the first task, ImNet model. See text for details and  Appendix~\ref{sec:other-architectures} for more examples. \textit{Best viewed in magnification}. } 
    \label{fig:vdd_arch-sim} \vspace{-4mm}
\end{figure}










{\textbf{Our Contributions}.} 
This paper makes  three main contributions to the field of task-incremental continual learning with ViTs.  (i) It presents a hierarchical task-synergy exploration-exploitation sampling based NAS method for learning task-aware dynamic models continually with respect to four operations: {\tt Skip}, {\tt Reuse}, {\tt Adapt}, and {\tt New} to overcome catastrophic forgetting. (ii) It identifies a ``sweet spot'' in ViTs as the task-synergy memory, i.e., the output projection layers after MHSA in ViTs. It also presents a new usage for the class-token {\tt CLS} in ViTs as the task-synergy memory updating guidance. (iii) It is the first work, to the best of our knowledge, to evaluate continual learning with ViTs on the large-scale, diverse and imbalanced VDD benchmark~\citep{vdd}, with better performance than the prior art. 
















