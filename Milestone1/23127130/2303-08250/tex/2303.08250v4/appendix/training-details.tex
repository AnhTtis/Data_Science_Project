\section{Settings and Hyperparameters in 
\label{sec:hyperparameters}
Learning CHEEM}\label{sec:training-details}
Starting with the ImageNet trained ViT-B/8, the proposed CHEEM learning consists of three components: \textit{supernet training, evolutionary search for target network selection, and target network finetuning}. 

\begin{table} [h]
\begin{minipage}{0.48\linewidth}
    \centering
    \captionof{table}{Data augmentations for the 9 tasks in the VDD benchmark.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c}
         \toprule
         Task & Scale and Crop & Hor. Flip & Ver. Flip  \\
         \midrule
         CIFAR100 & Yes & p=0.5 & No \\
         Aircraft & Yes & p=0.5 & No \\
         DPed & Yes & p=0.5 & No \\
         DTD & Yes & p=0.5 & p=0.5 \\
         GTSR & Yes & p=0.5 & No \\
         OGlt & Yes & No & No \\
         SVHN & Yes & No & No \\
         UCF101 & Yes & p=0.5 & No \\
         Flwr. & Yes & p=0.5 & No \\
    \bottomrule
    \end{tabular}}
    \label{tab:vdd-augmentations}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \captionof{table}{Data augmentations used for each task in the 5-Datasets benchmark.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
         Task & Scale and Crop & Hor. Flip  \\
         \midrule
         MNIST & Yes & No \\
         not-MNIST & Yes & No \\
         SVHN & Yes & No \\
         CIFAR100 & Yes & p=0.5 \\
         Fashion MNIST & Yes & No \\
    \bottomrule
    \end{tabular}}
    \label{tab:5-dataset-augmentations}
\end{minipage}\vspace{-3mm}
\end{table}

\paragraph{Data Augmentations}
A full list of data augmentations used for the VDD benchmark is provided in Table \ref{tab:vdd-augmentations}, and the data augmentations used for the tasks in the 5-datasets benchmark is provided in Table \ref{tab:5-dataset-augmentations}. The augmentations are chosen so as not to affect the nature of the data. Scale and Crop transformation scales the image randomly between 90\% to 100\% of the original resolution and takes a random crop with an aspect ratio sampled from a uniform distribution over the original aspect ratio $\pm0.05$.
In evaluating the supernet and the finetuned model on the validation set and test set respectively, images are simply resized to $72\times72$ with bicubic interpolation.


\paragraph{Supernet Training}
\textit{The VDD Benchmark}: For each task, we train the supernet for 100 epochs, unless otherwise stated. We use a label smoothing of 0.1. 
We use a learning rate of 0.001 and the Adam optimier~\cite{adam} with a Cosine Decay Rule. 
We use a batch size of 512, and ensure the minimum number of batches in an epoch is 15 (via repeatedly sampling when the number of total samples of a task is not sufficient). 
As stated in the paper, for the Exploration-Exploitation sampling scheme, we use an exploration probability $\epsilon = 0.3$.

\textit{The 5-datasets Benchmark}: We use the same hyperparameters as those used in the VDD Benchmark, but train the supernet for 50 epochs to account for its relatively lower complexity.

\textit{L2G with DARTS and $\beta$-DARTS}: We train the supernet of the Learn-to-Grow (L2G) \cite{learn-to-grow} for 50 epochs on the VDD benchmark and 25 epochs on the 5-datasets benchmark, since DARTS simultaneously trains all sub-networks (i.e. the entire supernet) at each epoch. We use a weight of 1 for the beta loss in all the experiments with $\beta$-DARTS. 

\paragraph{Evolutionary Search}
The evolutionary search is run for 20 epochs. We use a population size of 50. We use 25 candidates both in the mutation stage and the crossover stage. The top 50 candidates are retained. The crossover is performed among the top 10 candidates, and the top 10 candidates are mutated with a probability of $0.1$. For the Exploration-Exploitation sampling scheme, we use an exploration probability $\epsilon = 0.5$ when generating the initial population.

\paragraph{Finetuning}
The target network for a task selected by the evolutionary search is finetuned for 30 epochs with a learning rate of 0.001, Adam optimizer, and a Cosine Learning Rate scheduler. Drop Path of 0.25 and label smoothing of 0.1 are used for regularization. We use a batch size of 512, and a minimum of 30 batches are drawn.

We use a single Nvidia A100 GPU for all the experiments.





