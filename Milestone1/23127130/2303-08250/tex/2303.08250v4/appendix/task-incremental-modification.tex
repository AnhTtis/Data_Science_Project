
\section{Details of Modifying Baseline Methods on the VDD Benchmark}\label{sec:baseline}


\subsection{Modifying  S-Promts and L2P for Task-Incremental Setting on the VDD Benchmark}
\label{sec:task-inc-l2p-sprompt}
Both the S-Prompts~\cite{s-prompts} and the Learn-to-Prompt (L2P)~\cite{learning-to-prompt} can be modified for task-incremental setting (i.e., task ID is available in both training and inference) on the VDD benchmark without altering the core algorithm for learning the prompts. 

For the S-Prompts method, the modification is done by training $L$ prompts (randomly initialized) per task and then by retrieving the correct task-specific prompts with the task ID. Table \ref{tab:vdd-results-prompts} and Table \ref{tab:5-dataset-results-prompt} show the results of varying the number of prompts on the VDD benchmark and 5-dataset benchmark respectively, from which we can observe that varying the number of prompts beyond 10 does not affect the performance significantly, which is also observed in the original paper \cite{s-prompts}. 


\begin{table}[h]
    \centering
    \caption{Results of S-Prompts on the VDD benchmark~\cite{vdd} with various number of prompts. The results have been averaged over 3 different seeds.
    }
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            {S-Prompts} (L=1/task) & $82.65$ & $87.06$ & $76.42$ & $54.82$ & $62.10$ & $96.74$ & $99.59$ & $95.52$ & $37.62$ & $57.78$ & $75.03 \pm 0.19$ \\
            {S-Prompts} (L=5/task) & $82.65$ & $88.91$ & $85.23$ & $62.23$ & $70.64$ & $99.08$ & $99.87$ & $97.35$ & $45.32$ & $60.74$ & $79.20 \pm 0.53$ \\
            {S-Prompts} (L=10/task) & $82.65$ & $89.62$ & $88.69$ & $65.20$ & $72.18$ & $99.37$ & $99.91$ & $97.06$ & $45.17$ & $60.94$ & $80.08 \pm 0.30$ \\
            {S-Prompts} (L=12/task)~\cite{s-prompts} & $82.65$ & $89.32$ & $88.91$ & $64.52$ & $72.17$ & $99.29$ & $99.89$ & $96.93$ & $45.55$ & $60.76$ & $80.00 \pm 0.07$ \\
            {S-Prompts} (L=15/task) & $82.65$ & $89.63$ & $89.36$ & $65.88$ & $72.54$ & $99.37$ & $99.94$ & $97.03$ & $45.07$ & $61.17$ & $80.26 \pm 0.09$ \\ 
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \label{tab:vdd-results-prompts} \vspace{-3mm}
\end{table}

\begin{table}[h]
    \centering
    \caption{Results of S-Prompts on the 5-Dataset benchmark~\cite{adversarial-continual-learning}. 
    The results have been averaged over 5 different task orders.}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{\#Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts & 1 & $88.93 \pm 0.34$ \\
        S-Prompts & 5 & $91.14 \pm 0.78$ \\
        S-Prompts & 10 & $92.28 \pm 0.16$ \\
        S-Prompts & 12 & $92.42 \pm 0.11$ \\
        S-Prompts & 15 & $92.39 \pm 0.05$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.3em}
    \label{tab:5-dataset-results-prompt}
\end{table}

For the L2P method, we follow the official implementation\footnote{L2P official implementation: \href{https://github.com/google-research/l2p}{https://github.com/google-research/l2p}} which is tested on the 5-datasets benchmark~\cite{adversarial-continual-learning}. The vanilla L2P first trains a set of $N$ prompts of length $L_p$ (i.e. $N\cdot L_p$ tokens) per task. It then learns a set of $N$ keys such that the distance between the keys and the image encoding (using a fixed feature extractor) is maximized. 
In modifying the vanilla L2P for the task-incremental setting, we can directly retrieve the correct prompts using the Task ID instead of using a key-value matching. 
We initialize the  prompts for task $t$ from the trained prompts of task $t-1$ following the original implementation. We note that the prompt initialization is the only difference between the modified S-Prompts and the modified L2P.  Base on the above observations of performance changes w.r.t. the number of prompts in modifying S-Prompts, we use $L=12$ for L2P in our experiments.



