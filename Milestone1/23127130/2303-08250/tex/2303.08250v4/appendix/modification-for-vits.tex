\subsection{Modifying SupSup, EFT and LL to Work With ViTs}\vspace{-2mm}
\label{sec:modification-t2t}
In the main paper, we compare with Supermasks in Superposition (SupSup)~\cite{supsup}, Efficient Feature Transformation (EFT)~\cite{eft}, and Lightweight Learner (LL)~\cite{ll} in Table 5 under the task-to-task transfer learning paradigm. The three methods are originally developed for Convolutional Neural Networks. We modify them to be compatible with ViTs for a fair comparison with our CHEEM. 

We use the same ViT-B/8 base model (Sec.~\ref{sec:base-vit-model}) for SupSup, EFT and LL. 
For the SupSup method~\cite{supsup}, we learn masks for the weights of the final linear projection layer of the Multi-Head Self-Attention block using the straight through estimator \cite{straight-through-estimator}.
We apply the EFT~\cite{eft} on all the linear layers in the ViT-B/8 (i.e., all the Query/Key/Value projection layers, the final projection layer, and the FFN layers) by scaling their activation maps via the Hadamard product with learnable scaling vectors, following the original proposed formulation for fully-connected layers in the EFT~\cite{eft}. 
For the LL method~\cite{ll} which learns a task-specific bias vector that is added to all the feature maps of convolutional layers, we learn a similar bias vector and add it to the output of all the linear layers of the ViT. 
