\section{The Base Vision Transformer: ViT-B/8}
\label{sec:base-vit-model}
We use the base Vision Transformer (ViT) model, with a patch size of $8 \times 8$ (ViT-B/8) model from~\cite{vit}. The base ViT model contains 12 Transformer blocks. A Transformer block is defined by stacking a Multi-Head Self-Attention (MHSA) block and a Multi-Layer Perceptron (MLP) block with resudual connections for each block. ViT-B/8 uses 12 attention heads in each of the MHSA blocks, and a feature dimension of 768. The MLP block expands the dimension size to 3072 in the first layer and projects it back to 768 in the second layer. For all the experiments, we use an image size of $72\times72$ following the VDD setting. We base the implementation of the ViT on the {\tt timm} package ~\cite{timm}. 

\paragraph{Training the Base Model}
To train the ViT-B/8 model, we use the ImageNet data provided by the VDD benchmark (the \texttt{train} split in Table~\ref{tab:vdd-num-samples}). To save the training time, we initialize the weights from the ViT-B/8 trained on the full resolution ImageNet dataset (224$\times$224) and available in the \texttt{timm} package, and finetune it for 30 epochs on the downsized version of ImageNet (72$\times$72) in the VDD benchmark. We use a batch size of 2048 split across 4 Nvidia Quadro RTX 8000 GPUs. We follow the standard training/finetuning recipes for ViT models. The file {\tt cheem/artifacts/imagenet\_pretraining/args.yaml} in our code folder provides all the training hyperparameters used for training the the ViT-B/8 model on ImageNet.
During testing, we take a single center crop of 72$\times$72 from an image scaled with the shortest side to scaled to 72 pixels.