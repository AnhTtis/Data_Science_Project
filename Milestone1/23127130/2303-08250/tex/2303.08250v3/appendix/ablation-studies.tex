\begin{table}[h]
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tabular}{c|l|l|l|l|l|clllllllllll} \toprule
    
          & \multicolumn{17}{c}{ImageNet $\rightarrow$ Omniglot under the lifelong learning setting} \\ \midrule
          & \multicolumn{2}{c|}{{\tt Adapter} in} &  \multirow{2}{1cm}{\#Param Added}     &    \multirow{2}{*}{Rel. $\uparrow$}          &   \multirow{2}{1cm}{Test Acc.}    &     \multicolumn{12}{c}{Learned Operation per Block} \\
          \cline{2-3} \cline{7-18}
          & NAS & Finetune & &  &    & 1 & 2 &   3 & 4 &   5 &   6 & 7 &    8 & 9 &   10 &   11 &   12 \\
          \cline{1-18}
            Shorcut & \multirow{2}{*}{w/o A \& S} & w/ A &  \multirow{2}{*}{2.96M} &         \multirow{2}{*}{3.47\%} & 82.18 & \multirow{2}{*}{\colorbox{adapt}{A}} & \multirow{2}{*}{\colorbox{adapt}{A}} &   \multirow{2}{*}{\colorbox{reuse}{R}} & \multirow{2}{*}{\colorbox{reuse}{R}} &   \multirow{2}{*}{\colorbox{adapt}{A}} &   \multirow{2}{*}{\colorbox{reuse}{R}} &   \multirow{2}{*}{\colorbox{adapt}{A}} &    \multirow{2}{*}{\colorbox{new}{N}} & \multirow{2}{*}{\colorbox{new}{N}} &    \multirow{2}{*}{\colorbox{new}{N}} &    \multirow{2}{*}{\colorbox{skip}{S}} &    \multirow{2}{*}{\colorbox{skip}{S}} \\
                 \cline{3-3}\cline{6-6}
                 in &           &        w/o S &      &             & 78.16 &                               &   &   &   &   &    &    &    &    \\
                 \cline{2-18}
            {\tt Adapter} &         w/ S \& A &         w/ S \& A &  4.14M &         4.89\% & 82.32 &                               \colorbox{adapt}{A} & \colorbox{adapt}{A} & \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{adapt}{A} &   \colorbox{new}{N} &    \colorbox{adapt}{A} & \colorbox{adapt}{A} &    \colorbox{new}{N} &    \colorbox{adapt}{A} &    \colorbox{new}{N} \\
            \bottomrule
        \end{tabular}
    }
\vspace{0.3em}
        \caption{Results of the ablation study on the {\tt Adapter} implementation (Section 3.2.1): with (w/) vs without (w/o) shortcut connection for the MLP {\tt Adapt} layer. We test lifelong learning from ImageNet to Omniglot in the VDD. The proposed combination of w/o shortcut in Supernet NAS training and target network selection and w/ shortcut in finetuning (retraining newly added layers) is the best in terms of the trade-off between performance and cost.}
        \label{tab:adapter_ablation} \vspace{-3mm}
\end{table}

\section{Ablation Studies}
\label{sec:ablations} \vspace{-2mm}

\subsection{The Structure of {\tt Adapter}}\vspace{-2mm} 
\label{sec:hybrid-adapter}
\paragraph{How to {\tt Adapt} in a sustainable way?} The proposed {\tt Adapt} operation will effectively increase the depth of the network in a plain way. In the worst case, if too many tasks use {\tt Adapt} on top of each other, we will end up stacking too many MLP layers together. This may lead to unstable training due to gradient vanishing and exploding. Shortcut connections~\citep{resnet} have been shown to alleviate the gradient vanishing and exploding  problems, making it possible to train deeper networks. Due to this residual architecture, the training can ignore an adapter if needed, and leads to a better performance. However, in the lifelong learning setup, where subsequent tasks might have different distributions, the search process might disproportionately encourage {\tt Adapt} operations because of this ability. To counter this, we propose a hybrid {\tt Adapter} which acts as a plain 2-layer MLP during Supernet training and target network selection, and a residual MLP during finetuning. With an ablation study (Table~1 in Supplementary), we show that much more compact models can be learned with negligible loss in accuracy.

We verify the effectiveness of the proposed hybrid adapter using a lifelong learning setup with 2 tasks: ImageNet and Omniglot. The Omniglot dataset presents two major challenges for a lifelong learning system. First, Omniglot is a few-shot dataset, for which we may expect a lifelong learning system can learn a model less complex than the one for ImageNet. Second,  Omniglot has a significantly different data distribution than ImageNet, for which we may expect a lifelong learning system will need to introduce new parameters, but hopefully in a sensible and explainable way. Table \ref{tab:adapter_ablation} shows the results. In terms of the learned neural architecture, a more compact model (row 3) is learned without the shortcut in the adapter during Supernet training and target network selection: the last two MHSA blocks are skipped and three blocks are reused. Skipping the last two MHSA blocks makes intuitive sense since Omniglot may not need those high-level self-attention (learned for ImageNet) due to the nature of the dataset. The three consecutive {\tt new} operations (in Blocks 8,9,10) also make sense in terms of learning new self-attention fusion layers (i.e., new memory) to account for the change of the nature of the task. Adding shortcut connection back in the finetuning shows significant performance improvement (from 78.16\% to 82.18\%), making it very close to the performance (82.32\%) obtained by the much more expensive and less intuitively meaningful alternative (the last row).

\begin{table*} [h]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|cccccccccc|c}
        \toprule
            Component &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy & Avg. Param. Inc./task (M) \\
        \midrule           
             Projection & $82.65$ & $\boldsymbol{90.86}$ & $\boldsymbol{96.06}$ & $\boldsymbol{75.63}$ & $84.06$ & $\boldsymbol{99.92}$ & $99.83$ & $\boldsymbol{89.28}$ & $\boldsymbol{51.94}$ & $\boldsymbol{55.78}$ & $\boldsymbol{82.60} \pm \boldsymbol{0.55}$ & $1.12 \pm 0.03$\\
            Value & $82.65$ & $85.59$ & $95.82$ & $72.25$ & $\boldsymbol{84.20}$ & $99.89$ &	$99.89$ & $84.05$ & $45.80$ &	$53.37$ & $80.35 \pm 1.10$ & $1.73 \pm 0.13$ \\ 
            Query & {$82.65$} & $90.00$ & $94.56$ & $70.08$ & $78.56$ & $99.83$ & $99.91$ & $85.00$ & $43.37$ & $55.53$ & $79.95 \pm 0.76$ & $2.65 \pm 0.16$ \\ 
            Key & {$82.65$} & $89.57$ & $94.41$ & $71.31$ & $81.12$ & $99.89$ & $\boldsymbol{99.92}$ & $87.03$ & $45.10$ & $\boldsymbol{56.01}$ & $80.00 \pm 0.70$ & $2.47 \pm 0.23$ \\ 
            FFN & {$82.65$} & $\boldsymbol{90.70}$ & $\boldsymbol{96.18}$ & $\boldsymbol{79.59}$ & $\boldsymbol{85.44}$ & $\boldsymbol{99.92}$ & $\boldsymbol{99.92}$ & $\boldsymbol{87.19}$ & $\boldsymbol{51.66}$ & $54.02$ & $\boldsymbol{82.73} \pm \boldsymbol{1.06}$ & $4.44 \pm 0.83$\\  
        \bottomrule
    \end{tabular}}
    \vspace{0.3em}
    \caption{Results of ablation study on the other components of the ViT used for realizing the ArtiHippo. Realizing ArtiHippo at the FFN shows slightly better performance than the Projection layer. However, the Projection layer is much more parameter efficient than the FFN. Using the Projection layer offers negligible drop in Average Accuracy without sacrificing parameter efficiency. The results have been averaged over 3 different seeds.}
    \label{tab:vdd-component-ablation} \vspace{-2mm}
\end{table*}

\subsection{Evaluating feasibility of other ViT components as ArtiHippo }
\label{sec:artihippo-components}
\vspace{-2mm}
Table \ref{tab:vdd-component-ablation} shows the accuracy with other components if the ViT used for learning the Mixture of Experts using the proposed NAS method. The Query component from the MHSA block and the Value do not perform as well as the Projection layer. The FFN performs only slightly better than the Projection layer, but as a much larger parameter cost This reinforces our identification of the ArtiHippo in Section \ref{sec:identify_artihippo} in the main text as a lightweight plastic component.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/epochwise_plot.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/taskwise_increase.pdf}
        \caption{}
    \end{subfigure}\vspace{2mm}
    \caption{(a) Results of the ablation study on the Exploration-Exploitation (EE) guided sampling in the Supernet NAS training using  the VDD benchmark~\citep{vdd}. The proposed EE sampling strategy is much more efficient than the pure exploration based strategy (i.e., the vanilla SPOS NAS~\citep{spos}). It uses the Supernet training efficiently even at 50 epochs and achieves better performance than pure Exploration, which is desirable for fast adaptation in dynamic environments using lifelong learning. The \% increase in parameters shows that EE strategy is effective in reusing experts from the previous tasks and limiting the increase in parameters. The results have been averaged over 3 runs with different seeds. (b) Percent increase in the number of parameters over tasks. All New refers to a new projection layer for every block as new task arrives (similar for All Adapt). This shows a linear increase in the number of parameters. The proposed exploration-exploitation method stays well below the ``All Adapt" curve as opposed to pure exploration which almost approaches ``All Adapt".}
    \label{fig:epochs-vs-acc}
\end{figure}


\subsection{The Exploration-Exploitation Sampling Method} 
\label{sec:exp-expl-sampling}
\vspace{-2mm}
Figure~\ref{fig:epochs-vs-acc}, left shows that the proposed exploration-exploitation strategy can consistently obtain higher accuracy than pure exploration even when the supernet is trained a small number of epochs. Even when the supernet is trained for a longer duration, the proposed exploration-exploitation strategy still outperforms pure exploration (Figure \ref{fig:epochs-vs-acc} top). Moreover, the exploration-exploitation strategy adds a lot less additional parameters than pure exploration (Figure \ref{fig:epochs-vs-acc} bottom). We thus verify that the proposed exploration-exploitation strategy is effective and efficient in utilizing the parameters learned by the previous tasks, thus making the ``selective addition" of parameters mentioned in Section \ref{sec:intro} possible. This also shows that proposed task-similarity metric is meaningful.

\subsection{Parameter growth over time}
\label{sec:param-growth-over-time}
Since dynamic model based methods add parameters as new tasks arrive, its necessary to study the rate at which the number of parameters grow. Since the proposed ArtiHippo adds new experts (parameters) dynamically, we cannot analytically determine the rate of growth. However, our experiments show that the proposed exploration-exploitation strategy achieves sub-linear growth in the number of parameters, which again shows that our method can effectively leverage the parameters learned in the previous tasks.

\section{Comparison with additional methods}
\label{sec:comparison-additional}
For completeness, we also compare with a baseline of L2 Parameter Regularization \citep{smith2023closer}, and Elastic Weight Consolidation \citep{kirkpatrick-overcoming} applied to the linear projection layer of the Multi Head Self-Attention block. We also compare with Experience Replay with a buffer update strategy used in \citet{icarl}. This comparison is not completely fair, since these methods are class-incremental in nature. However, this comparison serves as a baseline to observe meaningful trade-off. Table \ref{tab:additional-comp} shows that EWC, L2 Parameter Regularization and Experience Replay cannot completely overvome catastraophic forgetting, and hence lose accuracy over time.

\begin{table} %
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method-Backbone &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule
            {S-Prompts$^\dagger$} ($L$=12)~\citep{s-prompts} & $82.65$ & $89.32$ & $88.91$ & $64.52$ & $72.17$ & $99.29$ & $\boldsymbol{99.89}$ & $\boldsymbol{96.93}$ & $45.55$ & $60.76$ & $80.00 \pm 0.07$ \\
            {L2P$^\dagger$} ($L$=12)~\citep{learning-to-prompt} & $82.65$ & $89.32$ & $89.89$ & $65.63$ & $72.34$ & $99.55$ & $\boldsymbol{99.94}$ & $\boldsymbol{96.63}$ & $45.24$ & $59.57$ & $80.08 \pm 0.10$ \\
            \midrule
            $^*$L2G~\citep{learn-to-grow} (DARTS) & $82.65$ & $88.47$ & $85.20$ & $\boldsymbol{79.22}$ & $80.19$ & $99.28$ & $ 98.06$ & $76.14$ & $39.29$ & $46.01$ & $77.45 \pm 2.41$ \\ 
            $^*$L2G~\citep{learn-to-grow} ($\beta$-DARTS) & $82.65$ & $88.95$ & $94.73$ & $75.31$ & $79.76$ & $99.84$ & $99.76$ & $78.86$ & $34.50$ & $47.09$ & $78.14 \pm 0.54$ \\ 
            \midrule
            EWC \citep{kirkpatrick-overcoming} & $58.19$ & $87.69$ & $69.64$ & $57.27$ & $45.89$ & $95.01$ & $98.47$ & $90.20$ & $36.57$ & $\boldsymbol{61.97}$ & $70.09$ \\ 
            L2 Regularization \citep{smith2023closer} & $55.28$ & $87.10$ & $55.23$ & $58.86$ & $40.48$ & $95.07$ & $99.17$ & $90.20$ & $37.53$ & $\boldsymbol{62.55}$ & $68.15$ \\ 
            Experience Replay \citep{icarl} & $55.88$ & $78.70$ & $87.40$ & $58.20$ & $76.03$ & $97.92$ & $48.55$ & $84.41$ & $40.98$ & $54.68$ & $68.27$ \\ 
            \midrule
            Our ArtiHippo (Uniform, 150 epochs) & $82.65$ & $76.20$ & $95.60$ & $75.14$ & $80.72$ & $\boldsymbol{99.92}$ & $99.86$ & $76.41$ & $42.74$ & $41.74$ & $77.10 \pm 0.75$ \\ 
             Our ArtiHippo (Hierarchical, 50 epochs) & $82.65$ & $\boldsymbol{90.97}$ & $96.05$ & $75.20$ & $82.36$ & $\boldsymbol{99.91}$ & $99.58$ & $87.16$ & $42.10$ & $52.54$ & $\boldsymbol{80.85} \pm \boldsymbol{0.72}$ \\
             Our ArtiHippo (Hierarchical, 150 epochs) & $82.65$ & $\boldsymbol{90.86}$ & $\boldsymbol{96.06}$ & $75.63$ & $\boldsymbol{84.06}$ & $\boldsymbol{99.92}$ & $99.83$ & $89.28$ & $\boldsymbol{51.94}$ & $55.78$ & $\boldsymbol{82.60} \pm \boldsymbol{0.55}$ \\
            Our ArtiHippo (Hierarchical+$L$=1, 150 epochs) & $82.65$ & $\boldsymbol{90.50}$ & $\boldsymbol{96.19}$ & $\boldsymbol{79.70}$ & $\boldsymbol{85.71}$ & $\boldsymbol{99.91}$ & $99.83$ & $92.42$ & $\boldsymbol{52.23}$ & $58.99$ & $\boldsymbol{83.55} \pm \boldsymbol{0.09}$ \\
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{Results on the VDD benchmark~\citep{vdd}. Our ArtiHippo shows clear improvements over the previous approaches. 
    All the results from our experiments are averaged over 3 different seeds. The 2 highest accuracies per task have been highlighted. All the methods use the same ViT-B/8 backbone containing 86.04M parameters and having 14.21G FLOPs unless otherwise stated. $^\dagger$ our modifications for the task-incremental setting. $^*$ our reproduction with the vanilla L2G method~\citep{learn-to-grow} for the ViT backbone. }
    \label{tab:additional-comp} \vspace{-3mm}
\end{table}


