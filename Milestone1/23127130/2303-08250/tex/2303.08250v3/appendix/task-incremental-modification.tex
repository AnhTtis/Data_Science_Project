
\section{Modifying S-Promts and L2P for task-incremental setting}
\label{sec:task-inc-l2p-sprompt}
Both, Learn to Prompt and S-Prompts, can be modified for task-incremental setting without altering the core algorithm for learning the prompts. For S-Prompts, this is done by training a separate prompt of length $L$, i.e., (i.e. a $L$ \texttt{cls} tokens) per task and retrieving the correct prompt using the task ID. For L2P, we follow the official implementation\footnote{L2P official implementation: \href{https://github.com/google-research/l2p}{https://github.com/google-research/l2p}} used for evaluating on the 5-datasets benchmark. L2P first trains a set of $N$ prompts of length $L_p$ (i.e. $NL_p$ tokens) per task. It then learns a set of $N$ keys such that the distance between the keys and the image encoding (using a fixed feature extractor) is maximized. We retrieve the retrieve the correct prompts using the Task ID instead of using a key-value matching and make L2P compatible with a task-incremental setting. We initialize the the values for the prompts for task $t$ from the trained values of task $t-1$ following the original implementation. 

\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            {S-Prompts$^\dagger$} (p=1/task) & $82.65$ & $87.06$ & $76.42$ & $54.82$ & $62.10$ & $96.74$ & $99.59$ & $95.52$ & $37.62$ & $57.78$ & $75.03 \pm 0.19$ \\
            {S-Prompts$^\dagger$} (p=5/task) & $82.65$ & $88.91$ & $85.23$ & $62.23$ & $70.64$ & $99.08$ & $99.87$ & $97.35$ & $45.32$ & $60.74$ & $79.20 \pm 0.53$ \\
            {S-Prompts$^\dagger$} (p=10/task) & $82.65$ & $89.62$ & $88.69$ & $65.20$ & $72.18$ & $99.37$ & $99.91$ & $97.06$ & $45.17$ & $60.94$ & $80.08 \pm 0.30$ \\
            {S-Prompts$^\dagger$} ($L$=12)~\citep{s-prompts} & $82.65$ & $89.32$ & $88.91$ & $64.52$ & $72.17$ & $99.29$ & $99.89$ & $96.93$ & $45.55$ & $60.76$ & $80.00 \pm 0.07$ \\
            {S-Prompts$^\dagger$} (p=15/task) & $82.65$ & $89.63$ & $89.36$ & $65.88$ & $72.54$ & $99.37$ & $99.94$ & $97.03$ & $45.07$ & $61.17$ & $80.26 \pm 0.09$ \\
            {L2P$^\dagger$} ($L$=12)~\citep{learning-to-prompt} & $82.65$ & $89.32$ & $89.89$ & $65.63$ & $72.34$ & $99.55$ & $99.94$ & $96.63$ & $45.24$ & $59.57$ & $80.08 \pm 0.10$ \\
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{Results on the VDD benchmark~\cite{vdd} with various number of prompts. Increasing the number of prompts above 10 does not lead to a significant gain in accuracy.}
    \label{tab:vdd-results-prompts} \vspace{-3mm}
\end{table}

\begin{table}[h]
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{Num. Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts$^\dagger$ & 1 & $88.93 \pm 0.34$ \\
        S-Prompts$^\dagger$ & 5 & $91.14 \pm 0.78$ \\
        S-Prompts$^\dagger$ & 10 & $92.28 \pm 0.16$ \\
        S-Prompts$^\dagger$ & 12 & $92.42 \pm 0.11$ \\
        S-Prompts$^\dagger$ & 15 & $92.39 \pm 0.05$ \\
        L2P$^\dagger$ & 12 & $92.73 \pm 0.10$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.3em}
    \caption{Results on the 5-Dataset benchmark~\cite{adversarial-continual-learning}. 
    The results have been averaged over 5 different task orders following L2P.}
    \label{tab:5-dataset-results-prompt}
\end{table}

\subsection{Ablation study with varying number of prompts}
Table \ref{tab:vdd-results-prompts} and Table \ref{tab:5-dataset-results-prompt} show the results of varying the number of prompt tokens for S-Prompts$^\dagger$ on the VDD and 5-dataset benchmarks respectively. Varying the number of prompts beyond 10 does not affect the performace significantly, as observed in the original paper \citep{s-prompts}. Hence, for L2P$^\dagger$, we calculate the number of prompt tokens by scaling the number of prompts used in the original implementation of L2P \citep{learning-to-prompt} on the 5-datasets benchmark to suit an image size of $72\times72$. For the 5 datasets benchmark, L2P uses 4 prompts of length 10 (40 tokens) for an image size of $224\times224$. We scale the prompt length to {\tt{floor}}$(10/3) = 3$, which gives 4 prompts of length 3, i.e., 12 tokens. We use a scaling factor of 3 since 224/72=3.