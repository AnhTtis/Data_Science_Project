\section{Background: Learn to Grow and SPOS}
\label{sec:background}
\subsection{Background on Learn to Grow}
\label{sec:learn-to-grow-review}
The learn-to-grow method~\citep{learn-to-grow} uses Differentiable Architecture Search (DARTS)~\citep{darts}, a supernet based NAS algorithm to learn a strategy for reusing, adapting, or renewing the parameters learned for the previous tasks (skipping is not applied). Consider an $L$-layer backbone network with $\mathcal{S}^l$ choice of parameters for a layer $l$ learned for the previous tasks. The parameters can be the weights and biases of the layer in the case of fully-connected layers, or the filters and the biases in the case of a Convolutional layers. For a new task, the learn-to-grow method constructs the search space for NAS (referred to as operations) by applying the operations {\tt reuse}, {\tt adapt}, and {\tt new} to $\mathcal{S}^l$ for all layers $l \in [1,L]$. The total number of choices for layer $l$ is $C_l = 2|\mathcal{S}^l|+1$ \{$|\mathcal{S}^l|$ reuse, $|\mathcal{S}^l|$ adapt, and $1$ new operation\}. Following DARTS, the output of a layer when training the NAS supernet is given by
\begin{equation}
    x_{out}^l = \sum_{c=1}^{C_l} \frac{exp(\alpha_c^l)}{\sum_{c^\prime=1}^{C_l}exp(\alpha_{c^\prime}^l)}g_c^l(x_{in}^l), 
\end{equation}
where $g_c^l(\cdot)$ is defined by,
\begin{equation}
\small 
    g_c^l(x_{l-1}) = 
    \begin{cases}
        S_c^l(x_{in}^l) & \text{if } c \le |\mathcal{S}^l|, \\
        S_c^l(x_{in}^l) + \gamma_{c-|\mathcal{S}^l|}^l(x_{in}^l) & \text{if } |\mathcal{S}^l| < c \le 2|\mathcal{S}^l|, \\
        \texttt{new}^l(x_{in}^l) & \text{if } c = 2|\mathcal{S}^l| + 1, 
    \end{cases}
\end{equation}
where $\gamma$ denotes the additional operation used in parallel ($1\times1$ convolution in the case of a Convolutional layer) which implements the {\tt adapt} operation. Using DARTS, the operations are trained jointly with architecture coefficients $\alpha_c^l$. Once the supernet is trained, the optimal operation is selected such that $(c^*)^l \gets \text{argmax}_c\ \alpha_c^l$. In experiments, the learn-to-grow method uses a 26-layer ResNet~\citep{resnet} on the VDD benchmark.

\textbf{Remarks on the proposed learning-to-grow with ViTs.} The learn-to-grow method~\citep{learn-to-grow} can maintain dynamic feature backbone networks for different tasks based on NAS, which leads to the desired selectivity and plasticity of networks in lifelong learning. It has been mainly studied with Convolutional Neural Networks (CNNs), and often apply DARTS for all layers with respect to the three operations, which is time consuming and may be less effective when a new task has little data. 

More importantly, the vanilla learn-to-grow method with DARTS and ResNets does not have the motivation of integrating learning and (long-term) memory in lifelong learning, which is the focus of our proposed ArtiHippo method. In learning a new task, unlike the vanilla learn-to-grow method in which all the previous tasks are treated equally when selecting the operation operations, our proposed ArtiHippo leverages the task similarities which in turn exploits the class-token specialized in ViTs.

\subsection{Background on the Single-Path One-Shot Neural Architecture Search Method}
\label{sec:spos}
DARTS~\citep{darts} trains the entire supernet jointly in learning a new task, and thus might not be practically scalable and sustainable after the supernet ``grew too fat" at each layer. 
The strategy used in Single-Path One-Shot (SPOS)~\citep{spos} NAS offers an alternative strategy based on a \textit{stochastic} supernet. It uses a bi-level optimization formulation consisting of the supernet training and the target network selection,
\begin{equation}
\label{eq:spos-train}
    W_{\mathcal{A}} = \text{argmin}_W\mathcal{L}_{train}(\mathcal{N}(\mathcal{A}, W)),
\end{equation}
\begin{equation}
\label{eq:spos-val}
    a^* = \text{argmax}_{a\in\mathcal{A}}\ Acc_{val}(\mathcal{N}(a, W_{\mathcal{A}}(a))),
\end{equation}
where Eqn.~\ref{eq:spos-train} is solved by defining a prior distribution $\Gamma(\mathcal{A})$ over the choice of operations in the stochastic supernet and optimizing,
\begin{equation}
    W_{\mathcal{A}} = \text{argmin}_W \mathbf{E}_{a \sim \Gamma(\mathcal{A})}\mathcal{L}_{train}(\mathcal{N}(\mathcal{A}, W)).
\end{equation}
This amounts to sampling one operation per layer (i.e., one-shot) of the neural network, and to forming a single path in the stochastic supernet. Eqn.~\ref{eq:spos-val} is optimized using an evolutionary search based on the validation performance for different candidates of the target network in a population, which is efficient since only inference is executed. The SPOS method empirically finds that a uniform prior works well in practice, especially when sufficient exploration is afforded. Note that this prior is also applied in generating the initial population for the evolutionary search.

The evolutionary search method used in the SPOS method is adopted from~\citep{real2019regularized}. It first initializes a population with a predefined number of candidate architectures sampled from the supernet. It then ``evolves" the population via the crossover and the mutation operations. At each ``evolving" iteration, the population is evaluated and sorted based on the validation performance. With the top-$k$ candidates after evaluation and sorting (the number $k$ is predefined), for crossover, two randomly sampled candidate networks in the top-$k$ are crossed to produce a new target network. For mutation, a randomly selected candidate in the top-$k$ mutates its every choice block with probability (e.g., $0.1$) to produce a new candidate. Crossover and mutation are repeated to generate sufficient new candidate target networks to form the population for the next ``evolving" iteration.

\textbf{Remarks on the proposed hierarchical task-similarity-oriented sampling with exploration-exploitation trade-off}. We modify the core sampling component in the SPOS algorithm for resilient lifelong learning with a long-term memory. During exploitation, we generate the prior $\Gamma(\mathcal{A})$ using our proposed hierarchical sampling scheme, and use the uniform prior during exploration. The same sampling scheme is applied when generating the initial population for the evolutionary search as well.