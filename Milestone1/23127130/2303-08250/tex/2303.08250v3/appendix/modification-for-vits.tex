\section{Modifying dynamic model based methods for Vision Transformers}
\label{sec:modification-for-vits}
Supermasks in Superposition (SupSup, \citep{supsup}), Efficient Feature Transformation (EFT, \citep{eft}), and Lightweight Learner (LL, \citep{ll}) have originally been developed for Convolutional Neural Networks. Here, we describe our modifications to theoriginal methods to make them compatible with Vision Transformers for a fair comparison with our ArtiHippo. Following ArtiHippo, which learns a base Vision Transformer model with ImageNet data from the VDD benchmark \citep{vdd}, we initialize the network for SupSup, EFT and LL with the same backbone. For SupSup, we learn masks for the weights of the final linear projection layer of the Multi-Head Self-Attention block using the straight through estimator \citep{straight-through-estimator}. We apply EFT on all the linear layers of the ViT by scaling all the activations by a learnable scaling vector using the Hadamard product following the original proposed formulation for fully-connected layers. Finally, for LL, which learns a task-specific bias vector which is added to all the feature maps of convolutional layers, we learn a similar bias vector and add it to the output of all the linear layers of the ViT. 
