\section{Identifying ArtiHippo in Transformers}
\label{sec:more-identifying-artihippo}
We provide the definitions of accuracy and forgetting used in Section \ref{sec:identify_artihippo} and analysis of how we choose the final linear Projection layer as the ArtiHippo. We first compare the transferability of the ViT trained with the first task, ImageNet to the remaining 9 tasks in a pairwise task-to-task manner and compute the average Top-1 accuracy on the 9 tasks from the VDD benchmark \citep{vdd}. Then, we start with the ImageNet-trained ViT, and train it on the remaining 9 tasks continually and sequentially in a predefined order with the average forgetting~\citep{riemannian-walk} on the first 9 tasks (including ImageNet) compared.
As shown in Table~\ref{tab:acc_vs_forgetting}, we compare 11 components or composite components across all blocks in the ImageNet-pretrained ViT. 

Let $T_1, T_2, \cdots, T_{N}$ be a sequence of $N$ tasks (e.g., $N=10$ in the VDD benchmark). A model consists of a feature backbone and a task head classifier. 
Let $f_{T_{n|1}}$ be the backbone trained for the task $n$ ($n=2, .. N$) with weights initialized from the model of task $1$, and the learned head classifier $C_n$ from scratch. The average transfer learning accuracy of the first task model to the remaining $N-1$ tasks is defined as:
\begin{equation}
    A_{N} = \frac{1}{N-1}\sum_{n=2}^N \text{Acc}(T_n; f_{T_{n|1}, C_n}) \label{eq:avg_acc}
\end{equation}
where $\text{Accuracy}()$ uses the Top-1 accuracy in classification.

Let $f_{T_{1:n}}$ be the backbone trained sequentially after task $T_n$ and and $C_n$ the head classifier trained for task $T_n$. Denote by $a_{n,i}=\text{Accuracy}(T_i; f_{T_{1:n}}, C_i)$, the accuracy on the task $i$ using the backbone that has been trained on tasks from $1$ to $n$ ($i<n$).
The average forgetting on the first $N-1$ tasks is:
\begin{equation}
    \mathbb{F}_N = \frac{1}{N-1}\sum_{n=1}^{N-1} \left(\max_{j\in [n, N-1]} a_{j,n} - a_{N,n}\right)
    \label{eq:avg_forgetting}
\end{equation}

The average accuracy in lifelong learning is defined by, 
\begin{equation}
\mathbf{A}_N = \frac{1}{N}\sum_{i=n}^N a_{N,n}, \label{eq:avg_accuracy_lll}    
\end{equation} 
where $N$ is the total number of tasks, and $a_{n,i}=\text{Accuracy}(T_i; f_{T_{1:n}}, C_i)$



From Table~\ref{tab:acc_vs_forgetting}, \textbf{the following observations lead us to select the projection layer as ArtiHippo}:

(i) Continually finetuning the entire MHSA block (i.e., MHSA+LN$_1$) obtains the best average accuracy, which has been observed in~\citep{touvron2022three} in terms of finetuning ImageNet-pretained ViTs on downstream tasks. However, \citep{touvron2022three} does not consider lifelong learning settings, and as shown here finetuning the entire MHSA block incurs the highest average forgetting, which means that it is task specific. Continually finetuning the entire FFN block (i.e., MLP$^{\text{down}}$+MLP$^{\text{up}}$+LN$_2$) has a similar effect as finetuning the entire MHSA block. In the literature,  the Vision Mixture of Expert framework~\citep{vision-moe} where an expert is formed by an entire MLP block takes advantage of the high average performance preservation. 


(ii)  In lifelong learning scenarios, maintaining either the entire MHSA block or the entire FFN block could address the catastrophic forgetting, but at the expense of high model complexity and heavy computational cost in both learning and inference.


(iii) The final projection layer and the Value layer in the MHSA block, which have been overlooked, can maintain high average accuracy (as well as manageable average forgetting, to be elaborated). It is also much more ``affordable" to maintain it in lifelong learning, especially with respect to the four basic growing operations ({\tt skip}, {\tt reuse}, {\tt adapt} and {\tt new}). Intuitively, the final projection layer is used to fuse multi-head outputs from the self-attention module. In ViTs, the self-attention module is used to mix/fuse tokens spatially and it has been observed  in MetaFormers~\citep{yu2022metaformer,yu2022metaformerbaseline} that simple local average pooling and even random mixing can perform well. So, it makes sense to keep the self-attention module frozen from the first task and maintain the projection layer to fuse the outputs. However, the Value layer is implemented as a parallel computation along with the Key and Query, which makes it inefficient to incorporate into the Mixture of Experts framework.

Through ablation studies (Appendix \ref{sec:artihippo-components}), we show that using the projection layer as ArtiHippo achieves higher performance than Query and Key, while achieving almost the same performance as that of the FFN but with much smaller number of parameters.