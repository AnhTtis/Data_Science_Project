\vspace{-3mm}
\section{Dataset Details}
\label{sec:dataset-details}
\subsection{The VDD Benchmark}\label{sec:vdd}
It consists of 10 tasks: ImageNet12~\citep{imagenet}, CIFAR100~\citep{cifar}, SVHN~\citep{svhn}, UCF101 Dynamic Images (UCF)~\citep{ucf1,ucf2}, Omniglot~\citep{omniglot}, German Traffic Signs (GTSR)~\citep{gtsrb}, Daimler Pedestrian Classification (DPed)~\citep{daimlerpedcls},  VGG Flowers~\citep{vgg-flowers}, FGVC-Aircraft~\citep{aircraft},    and Describable Textures (DTD)~\citep{dtd}. All the images in the VDD benchmark have been scaled such that the shorter side is 72 pixels. Table~\ref{tab:vdd-num-samples} shows the number of samples in each task. Figure~\ref{fig:vdd-examples} shows examples of images from each task of the VDD benchmark.

In our experiments, we use 10\% of {\tt the official training data} from each of the tasks for validation (e.g., used in the target network selection in Section 3.2.3 in main text), and report the accuracy on {\tt the official validation set} for fair comparison with the learn-to-grow method~\citep{learn-to-grow} in Table 2. In Table~\ref{tab:vdd-num-samples}, the \texttt{train, validation} and \texttt{test} splits are thus referred to 90\% of the official training data, 10\% of the official training data, and the entire official validation data respectively. When finetuning the learned architecture, we use the entire {\tt the official training data} to train and report results on the {\tt the official validation set}.%

\begin{figure*}[h]
    \centering{
        \subfiggrid{vdd}{imagenet}{1.25}{1.25}{ImageNet}\vspace{3mm}
        ~
        \subfiggrid{vdd}{cifar100}{1.25}{1.25}{CIFAR100}
        ~
        \subfiggrid{vdd}{svhn}{1.25}{1.25}{SVHN}
        ~
        \subfiggrid{vdd}{ucf101}{1.25}{1.25}{UCF101}
        ~
        \subfiggrid{vdd}{omniglot}{1.25}{1.25}{Omniglot}
        ~
        \subfiggrid{vdd}{gtsrb}{1.25}{1.25}{GTSR}
        ~
        \subfiggrid{vdd}{daimlerpedcls}{0.8}{1.25}{Pedestrian Cls.}
        ~
        \subfiggrid{vdd}{vgg-flowers}{1.25}{1.25}{VGG-Flowers}
        ~
        \subfiggrid{vdd}{aircraft}{1.25}{0.9}{FGVC-Aircraft}
        ~
        \subfiggrid{vdd}{dtd}{1.25}{1.25}{DTD}
    }\vspace{3mm}
    \caption{Example images from the VDD benchmark~\citep{vdd}. Each task has a significantly different domain than others, making VDD a challenging benchmark for lifelong learning.}
    \label{fig:vdd-examples}
\end{figure*}

\begin{figure*}[h]
    \centering{
        \subfiggrid{5-datasets}{mnist}{1.25}{1.25}{MNIST}
        ~
        \subfiggrid{5-datasets}{cifar10}{1.25}{1.25}{CIFAR10}
        ~
        \subfiggrid{5-datasets}{svhn}{1.25}{1.25}{SVHN}
        ~
        \subfiggrid{5-datasets}{not-mnist}{1.25}{1.25}{not-MNIST}
        ~
        \subfiggrid{5-datasets}{fashion-mnist}{1.25}{1.25}{Fashion-MNIST}
    }\vspace{1em}
    \caption{Example images from the 5-Datasets benchmark~\citep{adversarial-continual-learning}.}
    \label{fig:5-dataset-examples}
\end{figure*}

\subsection{The 5-Datasets Benchmark}\label{sec:5datasets}
It consists of 5 tasks: CIFAR10~\citep{cifar}, MNIST~\citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST~\citep{notmnist}, and SVHN~\citep{svhn}. Table~\ref{tab:5-dataset-num-samples} shows the data statistics.  Figure~\ref{fig:5-dataset-examples} shows examples of images from each task. To be consistent with the settings used on the VDD benchmark, we use 15\% of \texttt{the training data} for validation and report the results on \texttt{the official test data}, except for not-MNIST for which an official test split is not available. So, for the not-MNIST dataset, we use the small version of that dataset, with which we construct the test set by randomly sampling 20\% of the samples. From the remaining 80\%, we use 15\% for validation, and the rest as the training set.

\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c}
  \toprule
  Task & Train & Validation & Test \\
  \midrule
  ImageNet12 & 1108951 & 123216 & 49000 \\
  CIFAR100 & 36000 & 4000 & 10000 \\
  SVHN & 42496 & 4721 & 26040 \\
  UCF & 6827 & 758 & 1952 \\
  Omniglot & 16068 & 1785 & 6492 \\
  GTSR & 28231 & 3136 & 7842 \\
  DPed & 21168 & 2352 & 5880 \\
  VGG-Flowers & 918 & 102 & 1020 \\
  Aircraft & 3001 & 333 & 3333 \\
  DTD & 1692 & 188 & 1880 \\
  \bottomrule
\end{tabular}}
    \captionof{table}{The number of samples in training, validation and testing sets per task used in our experiments on the VDD benchmark~\citep{vdd}.}
    \label{tab:vdd-num-samples}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c}
    \toprule
         Task & Train & Validation & Test  \\
         \midrule
         MNIST & 51000 & 9000 & 10000 \\
         not-MNIST & 12733 & 2247 & 3744 \\
         SVHN & 62269 & 10988 & 26032 \\
         CIFAR10 & 42500 & 7500 & 10000 \\
         Fashion MNIST & 51000 & 9000 & 10000 \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Number of samples in training, validation, and test sets per task in the 5-Datasets benchmark~\citep{adversarial-continual-learning}. The test samples have been reported from the official test data provided by each individual dataset, except for not-MNIST. See text for details.}
    \label{tab:5-dataset-num-samples}
    \vspace*{0.4in}
\end{minipage}