\section{Learned architecture for different task order and pure exploration}
\label{sec:other-architectures}
Figure \ref{fig:vdd_arch_seq2} shows the architecture learned for a different task sequence. It can be seen that even with a different task sequence, the proposed method can learn to exploit task similarities. For example, an adapt operation is layer is learned at Block 4 for Omniglot, which is reused by GTSR. At Block 7, a New operation is learned for Omniglot, which is adapted for SVHN. Even though CIFAR100 is learnt as the last task, the search process can still learn to reuse many ImageNet experts. Figure \ref{fig:vdd_arch_seq1_exp} shows the architecture learned using a pure exploration strategy. It can be seen that pure exploration does not reuse components from similar tasks. For example, a large number of {\tt Adapt} and {\tt New} are added when learning CIFAR100. In contrast, the exploitation-exploitation strategy can learn to reuse the components from the ImageNet task (Figure \ref{fig:vdd_arch-sim} in the main text), and achieves better accuracy as well (Table \ref{tab:vdd-results} in the main text).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/vdd-iclr-42-ee-150-seq2.pdf}
    \caption{Architecture learned for task sequence ImNet, OGlt, UCF, Airc, Flwr, SVHN, DTD, GTSR, DPed, C100}
    \label{fig:vdd_arch_seq2} \vspace{-4mm}
\end{figure}\vspace{3mm}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/vdd-iclr-42-e-150.pdf}
    \caption{Architecture learned using pure exploration. Pure Exploration based method adds many unnecessary Adapt and New operations even though the tasks are similar (ImNet â†’ C100), proving the effectiveness of the proposed sampling method}
    \label{fig:vdd_arch_seq1_exp} \vspace{-4mm}
\end{figure}
