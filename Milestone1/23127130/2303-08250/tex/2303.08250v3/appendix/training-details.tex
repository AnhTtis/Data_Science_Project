\section{Settings and Hyperparameters in the Proposed Lifelong Learning}\label{sec:training-details}
Starting with the ImageNet pretrained ViT-B/8, the proposed lifelong learning methods consists of three components in learning new tasks continually and sequentially: supernet training, evolutionary search for target network selection, and target network finetuning. The supernet training and target network finetuning use the \texttt{train} split, while the evolutionary search uses the \texttt{validation} split, both shown in Table~\ref{tab:vdd-num-samples} and Table~\ref{tab:5-dataset-num-samples}. We use the vanilla data augmentation in both supernet training and target network finetuning. We use a weight of 1 for the beta loss in all the experiments with $\beta$-DARTS.  

\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c}
         \toprule
         Task & Scale and Crop & Hor. Flip & Ver. Flip  \\
         \midrule
         CIFAR100 & Yes & p=0.5 & No \\
         Aircraft & Yes & p=0.5 & No \\
         DPed & Yes & p=0.5 & No \\
         DTD & Yes & p=0.5 & p=0.5 \\
         GTSR & Yes & p=0.5 & No \\
         OGlt & Yes & No & No \\
         SVHN & Yes & No & No \\
         UCF101 & Yes & p=0.5 & No \\
         Flwr. & Yes & p=0.5 & No \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Data augmentations for the 9 tasks in the VDD benchmark.}
    \label{tab:vdd-augmentations}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c}
    \toprule
         Task & Scale and Crop & Hor. Flip  \\
         \midrule
         MNIST & Yes & No \\
         not-MNIST & Yes & No \\
         SVHN & Yes & No \\
         CIFAR100 & Yes & p=0.5 \\
         Fashion MNIST & Yes & No \\
    \bottomrule
    \end{tabular}}
    \captionof{table}{Data augmentations used for each task in the 5-Datasets benchmark.}
    \label{tab:5-dataset-augmentations}
\end{minipage}

\textbf{Data Augmentations.}
A full list of data augmentations used for the VDD benchmark is provided in Table \ref{tab:vdd-augmentations}, and the data augmentations used for the tasks in the 5-datasets benchmark is provided in Table \ref{tab:5-dataset-augmentations}. The augmentations are chosen so as not to affect the nature of the data. Scale and Crop transformation scales the image randomly between 90\% to 100\% of the original resolution and takes a random crop with an aspect ratio sampled from a uniform distribution over the original aspect ratio $\pm0.05$.
In evaluating the supernet and the finetuned model on the validation set and test set respectively, images are simply resized to $72\times72$ with bicubic interpolation.


\vspace{-1mm}
\subsection{Supernet Training}
\textit{VDD Benchmark}: For each task, we train the supernet for 150 epochs, unless otherwise stated. We use a label smoothing of 0.1. No other form of regularization is used since the {\tt skip} operation provides implicit regularization, which plays the role of Drop Path during training. We use a learning rate of 0.001 and the Adam optimier~\citep{adam} with a Cosine Decay Rule. For experiments with separate class tokens per task, we use a learning rate of 0.0005 for training the supernet, and 0.001 for training the task token. For each epoch, a minimum of 15 batches are drawn, with a batch size of 512. If the number of samples present in the task allows, we draw the maximum possible number of batches that covers the entire training data. For the Exploration-Exploitation sampling scheme, we use an exploration probability $\epsilon = 0.3$.

\textit{5-datasets Benchmark}: We use the same hyperparameters as those used in the VDD Benchmark, but train the supernet for 50 epochs.

\textit{L2G with DARTS and $\beta$-DARTS}: We train the supernet for Learn to Grow \citep{learn-to-grow} for 50 epochs for the VDD benchmark and 25 epochs for the 5-datasets benchmark.

\vspace{-1mm}
\subsection{Evolutionary Search}
\label{sec:evolutionary-search-params}
The evolutionary search is run for 20 epochs. We use a population size of 50. 25 candidates are generated by mutation, and 25 candidates are generated using crossover. The top 50 candidates are retained. The crossover is performed among the top 10 candidates, and the top 10 candidates are mutated with a probability of $0.1$. For the Exploration-Exploitation sampling scheme, we use an exploration probability $\epsilon = 0.5$ when generating the initial population.

\vspace{-1mm}
\subsection{Finetuning}
The target network for a task selected by the evolutionary search is finetuned for 30 epochs with a learning rate of 0.001, Adam optimizer, and a Cosine Learning Rate scheduler. Drop Path of 0.25 and label smoothing of 0.1 is used for regularization. We use a batch size of 512, and a minimum of 30 batches are drawn. When using a separate task token for each task, the task token is first finetuned with a learning rate of 0.001.

\subsection{Normalized Cosine Similarity}
\label{sec:normalized-cosine-similarity}
To verify the use of the Normalized Cosine Similarity as our similarity measure, we refer to Figure \ref{fig:similarities}. Figure \ref{subfig:cosine-sim} shows the Cosine Similarity between the mean class-tokens learned for tasks ImageNet, CIFAR100, SVHN, UCF101, and Omniglot (in order), and the mean class-tokens calculated for each expert using the data from the current task GTSR. Empirically, we observe that the Cosine Similarity between the mean class-tokens calculated using the data of the task associated with an expert and the mean class-token calculated with the current task in training is high. However, the difference between the similarity values for each expert are more important than the absolute values of the similarity. This difference can be increased by scaling the similarity such that it increases the magnitude difference between the similarities of different tasks, but maintains the relative similarity. This can be achieved by scaling the Cosine Similarities between -1 and 1 using the minimum and the maximum values from all the experts and all the blocks (Figure \ref{subfig:norm-cosine-sim}). Using the Normalized Cosine Similarity leads to better and more intuitive probability distributions for sampling candidate experts and the retention probabilities for the sampled experts. For example, comparing the probability values for sampling an expert at Block 6 calculated using Cosine Similarity (Figure \ref{subfig:expert-sampling}) vs. Normalized Cosine Similarity (Figure \ref{subfig:expert-sampling-normalized}), the probability of sampling the ImageNet expert increases, and those of sampling UCF101 and Omniglot decrease. Similarly, for Blocks 5 and 6, the retention probability calculated using the Normalized Cosine Similarity (Figure \ref{subfig:retention-probability-normalized}) reduces by a large factor than that calculated using the Cosine Similarity (Figure \ref{subfig:retention-probability}). This will encourage sampling the {\tt adapt} operation when these experts are sampled, thus adding plasticity to the network. The retention probability of the ImageNet experts at these blocks also reduces slightly, which will avoid imposing a strict prior.


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_cosine_similarity.pdf}
        \caption{Cosine Similarity between the mean class-tokens from the previous tasks and mean class-token for each block.}
        \label{subfig:cosine-sim}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_cosine_similarity.pdf}
        \caption{Normalized Cosine Similarity at each block calculated by scaling the Cosine Similarities between $-1$ and $1$.}
        \label{subfig:norm-cosine-sim}
    \end{subfigure}\vspace{-1.5mm}
    ~
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_softmax.pdf}
        \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Cosine Similarity}
        \label{subfig:expert-sampling}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_softmax.pdf}
        \caption{Probabilities of sampling candidate Expert $e$ ($\psi_e^l$) at each block calculated using the Normalized Cosine Similarity}
        \label{subfig:expert-sampling-normalized}
    \end{subfigure}\vspace{-1.5mm}
    ~
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_sigmoid.pdf}
        \caption{Retention probabilities ($\rho_e^l$) for each expert in each block, calculated using the Cosine Similarity.}
        \label{subfig:retention-probability}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gtsrb_similarity/gtsrb_normalized_sigmoid.pdf}
        \caption{Retention probabilities ($\rho_e^l$) for each expert, calculated using the Normalized Cosine Similarity.}
        \label{subfig:retention-probability-normalized}
    \end{subfigure}\vspace{1em}
    \caption{Comparison of probability values for sampling the Experts (Middle row) and the retention probabilities (Bottom row) using the Cosine Similarity and the Normalized Cosine Similarity. Using the Normalized Cosine Similarity gives better probability values for expert sampling probabilities $\psi_e$, as seen in Blocks 5 and 6, where the expert sampling probability for ImageNet increases, thus reducing the probability of sampling {\tt new} and {\tt skip}. This will encourage maximal reuse. The effect on the retention probability $\rho_e$ can be prominently seen on the Omniglot experts. The retention probability in Blocks 6 \ref{subfig:retention-probability-normalized} reduces, which will encourage the {\tt adapt} operation to be trained even if the Omniglot expert even if Omniglot experts were sampled.}
    \label{fig:similarities}
\end{figure}


