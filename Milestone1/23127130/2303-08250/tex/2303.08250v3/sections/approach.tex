\section{Approach}\vspace{-2mm}
In this section, we first present the ablation study on identifying the ArtiHippo in a ViT block (Fig.~\ref{fig:flow}). Then, we present details of learning to grow ArtiHippo (Figs.~\ref{fig:nas} and~\ref{fig:nas-sampling}). 

\begin{wraptable} {r}{0.45\textwidth}\vspace{-2mm}
    \vspace{-4mm}
    \centering
    \resizebox{0.45\textwidth}{!}{
        \begin{tabular}{l|l|c|c}
            \toprule
            Index & Finetuned Component &  Avg. Acc. & Avg. Forgetting \\
            \midrule
            1 & $\text{LN}_1$ + $\text{LN}_2$ & $81.76$ & $21.24$ \\
            \midrule
            2 & $\text{FFN}$ & $84.20$ & $44.76$ \\
            3 & $\text{MLP}^{\text{d}}$ & $83.66$ & $37.99$ \\
            4 & $\text{LN}_2$ & $80.04$ & $16.35$ \\
            \midrule
            5 & MHSA + $\text{LN}_1$ & $85.26$ & $54.38$ \\
            6 & LN$_1$ & $81.18$ & $19.04$ \\
            7 & Query & $81.57$ & $19.69$ \\
            8 & Key & $81.56$ & $19.19$ \\
            9 & Query+Key & $81.49$ & $31.10$ \\
            10 & Value & $84.99$ & $37.58$ \\
            11 & Projection  (\textbf{ArtiHippo}) & $85.11$ & $30.50$ \\
            \midrule
            \multicolumn{2}{c|}{Classifier w/ Frozen Backbone} & $70.78$ & - \\
            \bottomrule
        \end{tabular}
    }
    \caption{\small Ablation study on identifying the ArtiHippo in a Transformer block (Eqns.~\ref{eq:mhsa_proj} and~\ref{eq:ffn}). See text for detail. 
    The last row shows the result of a conventional transfer learning setting in which only the head classifier is trained.
    }
    \label{tab:acc_vs_forgetting} \vspace{-2mm}
\end{wraptable}

\vspace{-2mm}
\subsection{Identifying ArtiHippo in Transformers}\label{sec:identify_artihippo}\vspace{-2mm}
\label{identifying-artihippo}
The left of Fig.~\ref{fig:flow} shows a Vision Transformer (ViT) block \citep{vit}. Without loss of generality, denote by $x_{L,d}$ an input sequence consisting of $L$ tokens encoded in a $d$-dimensional space. In ViTs, the first token is the so-called {\tt class-token}. The remaining $L-1$ tokens are formed by patch embedding of an input image, together with additive positional encoding. A ViT block can be expressed as, 
\begin{align}
    z_{L, d} & = x_{L, d} + \text{Proj}(\text{MHSA}(\text{LN}_1(x_{L, d}))) \label{eq:mhsa_proj} \\
    y_{L, d} & = z_{L, d} + \text{FFN}(\text{LN}_2(z_{L, d})))) \label{eq:ffn}
\end{align}
where $\text{Proj}(\cdot)$ is a linear transformation fusing the multi-head outputs from MHSA module. 

The MHSA realizes the dot-product self-attention between Query and Key, followed by aggregating with Value, where Query/Key/Value are linear transformatons of the input token sequence. The FFN is often implemented by a multi-layer perceptron (MLP) with a feature expansion layer $\text{MLP}^{\text{u}}$ and a feature reduction layer $\text{MLP}^{\text{d}}$ with a nonlinear activation function (such as GELU) in the between.

For resilient task-incremental lifelong learning using ViTs, \textbf{our very first step is to investigate whether there is a simple yet expressive ``sweet spot" in the Transformer block that plays the functional role of  Hippocampi in the human brain (i.e., ArtiHippo)}, that is converting short-term streaming task memory into long-term memory to support lifelong learning without catastrophic forgetting. The proposed identification process is straightforward. Without introducing any modules handling forgetting, we compare both the task-to-task forward transferrability and the sequential forgetting of different components in a Transformer block. Our intuition is that a desirable ArtiHippo component must enable strong transferrability with manageable forgetting. Table~\ref{tab:acc_vs_forgetting} shows the abaltion study of identifying ``sweet spot" candidates. \textbf{Due to space limit, please refer to the Appendix~\ref{sec:more-identifying-artihippo}} for the detailed experimental settings, the definitions of average accuracy (Eqn.~\ref{eq:avg_acc}) and average forgetting (Eqn.~\ref{eq:avg_forgetting}), and our analyses. 

In sum, due to the strong forward transfer ability, manageable forgetting, maintaining simplicity and for less invasive implementation in practice, \textbf{we select the Projection layer in the MHSA block as ArtiHippo to develop our proposed long-term task-similarity-oriented memory based lifelong learning.} Through ablation studies in Appendix \ref{sec:artihippo-components} Table \ref{tab:vdd-component-ablation}, we verify that using the Projection layer is better than the Value layer. We also show that using other layers of the ViT leads to significantly worse performance while using the FFN obtains similar performance as that of the Projection layer in line with Table \ref{tab:acc_vs_forgetting} but at a much higher parameter cost, thus empirically validating our hypothesis.


\vspace{-2mm}
\subsection{Learning to Grow ArtiHippo Continually}
\label{sec:grow_artihippo} \vspace{-2mm}
This section presents details of learning to grow ArtiHippo based on NAS. As illustrated in Fig.~\ref{fig:nas},  for a new task $t$ given the network learned for the first $t-1$ tasks, it consists of three components: the Supernet construction (the parameter space of growing ArtiHippo), the Supernet training (the parameter estimation of growing ArtiHippo), and the target network selection and finetuning (the consolidation of the ArtiHippo for the task $t$).
\begin{wrapfigure}{r}{0.5\textwidth} %
    \centering
    \includegraphics[width=0.5\textwidth]{figures/nas_v2.pdf}
    \caption{\small 
    Illustration of ArtiHippo growing via NAS using the four learning-to-grow operations in lifelong learning. See text for details. }
    \label{fig:nas}\vspace{-4mm}
\end{wrapfigure}



\vspace{-3mm}
\subsubsection{Supernet Construction}\label{sec:supernet_construction}\vspace{-2mm}

We start with a vanilla $D$-layer ViT model (e.g., the 12-layer ViT-Base)~\citep{vit} and train it on the first task (e.g., ImageNet in the VDD benchmark~\citep{vdd}) following the conventional recipe. 
The proposed ArtiHippo is represented by a mixture of experts, similar in spirit to~\citep{vision-moe}. After the first task, the ArtiHippo at the $l$-th layer in the ViT model consists of a single expert which is defined by a tuple, $E^{l, 1}=(P^{l, 1}, \mu^{l, 1})$,  where $P^{l, 1}$ is the projection layer and $\mu^{l, 1}\in R^d$ is the associated mean class-token pooled from the training dataset after the model is trained. Without loss of generality, we consider how the growing space of ArtiHippo is constructed at a single layer, assuming the current ArtiHippo consists of two experts, $\{E^{l, 1}, E^{l, 2}\}$ (Fig.~\ref{fig:nas}, left). We utilize four operations in the Supernet construction: 
\begin{itemize}[leftmargin=*]
    \itemsep0em
    \item {\tt skip}: Skips the entire MHSA block (i.e., the hard version of the drop-path method widely used in training ViT models), which encourages the adaptivity accounting for the diverse nature of tasks. 
    \item {\tt reuse}: Uses the projection layer from an old task for the new task unchanged (including associated mean class-token), which will help task synergies in learning. 
    \item {\tt adapt}: Introduces a new lightweight layer on top of the projection layer of an old task, implemented by a MLP with one squeezing hidden layer, and a new mean class-token computed at the added adapt MLP layer. We propose a hybrid adapter which acts as a plain 2-layer MLP during searcch, and a residual MLP during finetuning (details in the Appendix Sec. 2.1).
    \item {\tt new}: Adds a new projection layer and a mean class-token, enabling the model to handle corner cases and novel situations.
\end{itemize}\vspace{-3mm}

The bottom of Fig.~\ref{fig:nas} shows the growing space. The Supernet is constructed by {\tt reusing} and {\tt adapting} each existing expert at layer $l$, and adding a {\tt new} and a {\tt skip} expert. The newly added {\tt adapt} MLPs and projection layers will be trained from scratch using the data of a new task only. 



\vspace{-2mm}
\subsubsection{Supernet Training}\vspace{-2mm}
To train the Supernet constructed for a new task $t$, we build on the SPOS method \citep{spos} due to its efficiency. The basic idea of SPOS is to sample a single-path sub-network from the Supernet by sampling an expert at every layer in each iteration (mini-batch) of training. One key aspect is the sampling strategy. The vanilla SPOS method uses uniform sampling (i.e., the \textit{pure exploration} strategy, Fig.~\ref{fig:nas-sampling} left), which has the potential of traversing all possible realizations of the mixture of experts of the ArtiHippo in the long run, but may not be desirable (or sufficiently effective) in a lifelong learning setup because it ignores inter-task similarities/synergies. 
To overcome this, we propose an exploitation strategy (Fig.~\ref{fig:nas-sampling} right), which utilizes a hierarchical sampling method that forms the categorical distribution over the operations in the search space explicitly based on task similarities. 
We first present details on the task-similarity oriented sampling in this section. Then we present an exploration-exploitation integration strategy to harness the best of the two in Sec.~\ref{sec:balancing}. 

\textbf{Task-Similarity Oriented Sampling}: Let $\mathbb{E}^l$ be the set of Experts at the $l$-the layer learned till task $t-1$. For each candidate expert $e\in \mathbb{E}^l$, we first compute the mean class-token for the $t$-task, $\hat{\mu}_e^{t}$ using the current model, and compute the task similarity between the $t$-th task and the Expert $e$ as
$S_e(t) = \text{{\tt NormCosine}}(\hat{\mu}_e^{t}, \mu_e)$,
where {\tt NormCosine}$(\cdot, \cdot)$ is the Normalized Cosine Similarity, which is calculated by scaling the Cosine Similarity score between $-1$ and $1$ using the minimum and the maximum Cosine Similarity scores from all the experts in all the MHSA blocks of the ViT. This normalization is necessary to increase the difference in magnitudes of the similarities between tasks, which results in better Expert sampling distributions during the sampling process in our experiments. The similarities for the newly added {\tt new} and {\tt skip} experts cannot be calculated, since the parameters are randomly initialized. Instead, we treat them together as a pseudo \textit{Ignore} expert, for which the similarity is calculated as $S_{ig}(t) = -\text{{\tt max}}_e{S_e(t)}$. Intuitively, this means we only ignore the previous experts in proportion to the best possible expert (which has the maximum similarity).


\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/sampling_process.pdf}
    \caption{\small 
    Illustration of the proposed exploration-exploitation sampling based SPOS NAS~\citep{spos} for lifelong learning. It harnesses the best of the vanilla  pure exploration strategy (top) and the proposed exploitation strategy (bottom) using a simple epoch-wise scheduling. See text for details.}
    \label{fig:nas-sampling}\vspace{-2mm}
\end{figure}

The categorical distribution is then computed via Softmax across all the scores of all the Experts at a layer. The probability of sampling a candidate Expert $e\in \mathbb{E}^l \cup \{\mathbb{E}_{ignore}^l\}$ is defined by 
$\psi_e = \frac{\exp(S_e(t))}{\sum_{e'\in \mathbb{E}^l\cup \{\mathbb{E}_{ignore}^l\}} \exp(S_{e'}(t))}$. 
With an Expert $e$ sampled (with a probability $\psi_e$), we further compute its retention Bernoulli probability via a Sigmoid transformation of the task similarity score defined by $\rho_e = \frac{1}{1+\exp(-S_e(t))}$.
If the sampled Expert $e \in \mathbb{E}^l$, we sample the {\tt Reuse} operation with a probability $\rho_e$ and the {\tt Adapt} operation with probability $1-\rho_e$. If the Expert $e = \mathbb{E}_{ignore}^l$, we randomly sample from the two operations: {\tt Skip} and {\tt New} with a probability 0.5.

\subsubsection{Target Network Selection and Finetuning}\vspace{-2mm}
\label{sec:target-network-selection}
After the Supernet is trained, we adopt the same evolutionary search used in the SPOS method~\citep{spos} based on the proposed hierarchical sampling strategy. The evolutionary search is performed on the validation set to select the path which gives the best validation accuracy. After the target network for a new task is selected, we retrain the newly added layers by the {\tt New}  and {\tt Adapt} operations from scratch (random initialization), rather than keeping or warming-up from the weights from the Supernet training. This is based on the observations in network pruning that it is the neural architecture topology that matters and that the warm-up weights may not need to be preserved to ensure good performance on the target dataset~\citep{liu2018rethinking}.

\vspace{-2mm}
\subsubsection{Balancing Exploration and Exploitation:}\label{sec:balancing}\vspace{-2mm}
As illustrated in Fig.~\ref{fig:nas-sampling}, to harness the best of the pure exploration strategy and the proposed exploitation strategy, we apply epoch-wise exploration and exploitation sampling for simplicity. At the beginning of an epoch in the Supernet training, we choose the pure exploration strategy with a probability of $\epsilon_1$ (e.g., 0.3), and the hierarchical sampling strategy with  a probability of $1-\epsilon_1$. Similarly, when generating the initial population during the evolutionary search, we draw a candidate target network from a uniform distribution over the operations with a probability of $\epsilon_2$, and from the hierarchical sampling process with a probability of $1-\epsilon_2$, respectively. In practice, we set $\epsilon_2 > \epsilon_1$ (e.g., $\epsilon_2=0.5$) to encourage more exploration during the evolutionary search, while encouraging more exploitation for faster learning in the Supernet training. Our experiments show that this exploration-exploitation strategy achieves higher Average Accuracy and results in a lower parameter increase than pure exploration by a large margin (Fig. \ref{fig:epochs-vs-acc} in the Appendix).
\vspace{-2mm}

\subsection{Integrating ArtiHippo with Learning to Prompt}
\label{sec:combining-with-prompts}\vspace{-2mm}
Since prompting-based methods ~\citep{learning-to-prompt,dualprompt,s-prompts,dytox} are complimentary to our proposed method, we propose a simple method for harnessing the best of both, and show that this leads to further improvement.  
At the beginning of the Supernet training, a task-specific classification token is learned using the ImageNet backbone (similar to S-Prompts~\citep{s-prompts}). Then, instead of using the {\tt cls} token from the ImageNet task, we used the learned task token during NAS. When finetuning the learned architecture, we first train the task token using the fixed ImageNet backbone, and then use this trained token to train the architecture components. 
\vspace{-1mm}