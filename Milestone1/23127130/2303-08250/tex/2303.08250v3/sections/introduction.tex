\section{Introduction}
\label{sec:intro} \vspace{-2mm}
Developing lifelong learning machines is one of the hallmarks of AI, to mimic human intelligence in terms of learning-to-learn to be adaptive and skilled at streaming tasks. However, state-of-the-art machine (deep) learning systems realized by Deep Neural Networks (DNNs) are not yet intelligent in the biological sense from the perspective of lifelong learning, especially plagued with the critical issue known as \textit{catastrophic forgetting} at streaming tasks in a dynamic environment~\citep{mccloskey,thrun}. 
Catastrophic forgetting means that these systems ``forget" how to solve old tasks after sequentially and continually trained on a new task using the data of the new task only. 
Addressing catastrophic forgetting in lifelong learning is a pressing need with potential paradigm-shift impacts in the next wave of trustworthy and/or brain-inspired AI. 

To address catastrophic forgetting, there are two main categories of lifelong learning methods: exemplar-based methods~\citep{gradient-based-sample-selection,hayes-icra,large-scale-inc-learning} and exemplar-free methods~\citep{kirkpatrick-overcoming,learn-to-grow,learning-to-prompt,dualprompt,s-prompts}, both of which have witnessed promising progress. The former is also referred to Experience Replay, where a small number of selected samples is stored, using either raw data or latent feature representations, for each previous task, and then incorporated in conjunction with the data of a new task in training the model for the new task. The key lies in how to select the so-called coreset of experience and how the experience replaying is actually performed. For the latter, since no data of previous tasks in any forms will be available, the focus is typically on how to retain the model parameters trained for previous tasks in training the model for a new task. There are three main strategies in adapting a current model for a new task. The first is to regularize the change in model parameters such as the popular Elastic Weight Consolidation (EWC) method~\citep{kirkpatrick-overcoming}. The second is to dynamically expand the model such as the learn-to-grow method~\citep{learn-to-grow}, the Supermask in Superposition (SupSup) method~\citep{supsup}, the Lightweight Learner~\citep{ll}, the calibration method~\citep{singhCalibratingCNNsLifelong2020}, the efficient feature transformation method~\citep{eft} and the Channel-wise Lightweight Reprogramming (CLR) method~\citep{clr}, all of which have been mainly developed for convolutional neural networks. More recently, with the availability of powerful pretrained Transformer~\citep{attention-is-all-you-need} based Large Foundation Models (LFMs)~\citep{bommasani2021opportunities} (such as the CLIP models~\citep{clip}), the third is to freeze pretrained LFMs and then to learn prompts (or task tokens) appended to the input tokens instead for lifelong learning, i.e. prompting-based methods \citep{learning-to-prompt,dualprompt,s-prompts}.



\begin{SCfigure}[50][t]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/method-idea2.pdf}
    \caption{\small 
    Illustration of the difference between the prior art of lifelong learning using Transformers~\citep{attention-is-all-you-need} and our proposed learn-to-grow method. \textit{(a) Prompt-based methods}~\citep{learning-to-prompt,dualprompt,s-prompts} leverage a pretrained and frozen Transformer and learn task-specific prompts. \textit{(b) Parameter-tuning based methods} introduce task-specific layer-wise parameters on top of a pretrained and frozen Transformer. One key lies in how the pretrained layer and the task-specific layer are fused, e.g., the parameter-masking methods~\citep{supsup,meta-attention,piggyback} and the output-addition methods~\citep{ll,clr,pool-of-adapters,nettailor}.
    Another key is where to apply the parameter tuning, which is often overlooked. \textit{(c) Our proposed method} belongs to the parameter-tuning category with more fine-grained control. It utilizes four learn-to-grow operations: {\tt Reuse}, {\tt New}, {\tt Adapt} and {\tt Skip}, motivated by the learn-to-grow method~\citep{learn-to-grow}. We study where to place the learning-to-grow module in Transformers, what representational scheme to use, how to select the best among the four learn-to-grow operations, and how to integrate with prompt-based methods. See text for details. 
    }
    \label{fig:method-overview}
    \vspace{-10mm}
\end{SCfigure}



With the increasing dominance of Transformers in deep learning, in this paper, we are interested in studying how to dynamically expand Transformers in a lightweight way for resilient lifelong learning, that is to develop an exemplar-free lifelong learning method for Transformers, which is complementary to the prompt-based approaches. Fig.~\ref{fig:method-overview} illustrates the proposed method which is not built on completely frozen pretrained Transformer models, but can induce plastic and reconfigurable structures for streaming tasks.
We are motivated by some observations of natural intelligence possessed by biological systems (e.g., the human brain) which exhibit remarkable capacity of learning and adapting their structure and function for tackling different tasks throughout their lifespan, while retaining the stability of their core functions. 
It has been observed in neuroscience that learning and memory are entangled together in a highly sophisticated way~\citep{christophel2017distributed,voitov2022cortical}. In this paper, we think, at a high level, of learning model parameters as a process of interacting with the sensory information (data) to convert Short-Term Memory (activations) into a Long-Term Memory (learned parameters), and of selectively adding task-specific parameters as expanding the memory. With this (loose) analogy, we selectively induce learnable parts into Transformers (in contrast to entirely freezing) to induce reconfigurability, selectivity, and plasticity for lifelong learning. We term our framework \textbf{ArtiHippo}, which stands for Artificial Hipppocampi, after the hippocampal system that  plays an important role in converting Short-Term Memory into Long-Term Memory for lifelong learning. 

\begin{SCfigure}[50][t]
        \centering
    \includegraphics[width=0.5\textwidth]{figures/artihippo_flow.pdf}
    \caption{\small 
    Illustration of the proposed ArtiHippo. \textit{Left:} The Multi-Head Self-Attention (MHSA) block in ViTs~\citep{vit} with the proposed Artificial Hippocampi (ArtiHippo) replacing the original linear projection layer. \textit{Middle:} The ArtiHippo growing is maintained by four operations, similar in spirit to the L2G \citep{learn-to-grow}.      
    \textit{Right:} ArtiHippo is represented by a mixture of experts with an example for different tasks (e.g., $j$) starting from $i$. %
    }
    \label{fig:flow}\vspace{-3mm}
\end{SCfigure}


To this end, we adapt the Learn-to-Grow (L2G) framework~\citep{learn-to-grow} for ViTs~\citep{vit}, but do not apply Neural Architecture Search (NAS) uniformly across all the layers of a ViT (which is prohibitively computationally expensive). Instead, we aim to find a lightweight component which can preserve and interact with the stable components of a ViT~\citep{vit}, while inducing plasticity in a lifelong learning setting, i.e., a growing memory. As illustrated in Fig.~\ref{fig:flow}, the final projection layer in the multi-head self-attention (MHSA) block of a ViT is   identified and selected as the ArtiHippo (Sec.~\ref{sec:identify_artihippo}). The learn-to-grow NAS is only applied in maintaining ArtiHippo layers, 
while other components are frozen to maintain the stability of core functions, as illustrated in Fig.~\ref{fig:nas}. We propose a hierarchical task-similarity-oriented exploration-exploitation sampling method (Fig.~\ref{fig:nas-sampling}) to account for task synergies and to facilitate a much more effective single-path one-shot (SPOS) NAS~\citep{spos} (Sec.~\ref{sec:grow_artihippo}). 

In experiments,  this paper considers lifelong learning with task indices available in both training and inference, which is often referred to as \textit{task-incremental setup} \citep{types-of-inc-learning}. When tasks consist of data from different domains such as the Visual Domain Decathlon (VDD) benchmark~\citep{vdd}, it is also related to domain-incremental setup, but without assuming the same output space between tasks. 
The right of Fig.~\ref{fig:flow} illustrates an example of learned ArtiHippo. With the task indices, the execution of the computational graph for a given task is straightforward. The proposed method achieves zero-forgetting on old tasks. The proposed method is tested on both the VDD benchmark and the recent 5-Dataset benchmark. It obtains consistently better performance than the prior art with sensible ArtiHippo learned continually. For a comprehensive comparison, we take great efforts in modifying several state-of-the-art methods that were developed for convolutional neural networks (ConvNets) to work with ViTs on VDD.  
