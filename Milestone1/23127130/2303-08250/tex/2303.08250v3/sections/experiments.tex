\section{Experiments}\vspace{-2mm}
In this section, we test the proposed method on two benchmarks and compare with the prior art. We evaluate our method in the task-incremental setting, where each task contains a disjoint set of classes and/or domains and task index is available during inference. The proposed method obtains better performance than the prior art in comparisons.  \textbf{Our PyTorch source code is provided with the supplementary material.} Due to space limitations, we provide the implementation details in the Appendix. We use 1 Nvidia A100 GPU for experiments on the VDD and 5-Dataset benchmarks.

\textbf{Data and Metrics}:
We evaluate our approach on the Visual Domain Decathlon (VDD) datasets~\citep{vdd} and the 5-Datasets benchmark introduced in\citep{adversarial-continual-learning}. Each of the individual dataset in these two benchmarks are treated as separate non-overlapping tasks. The VDD benchmark is challenging because of the large variations in tasks as well as small number of samples in many tasks, which makes it a favorable for evaluating lifelong learning algorithms. \textit{Details of the benchmarks are provided in the Appendix \ref{sec:dataset-details}}.
Since catastrophic forgetting is fully addressed by our method, we evaluate the performance of our method using the average accuracy defined by Eqn.~\ref{eq:avg_accuracy_lll} in the Appendix~\ref{sec:more-identifying-artihippo}.

\textbf{Baselines:}
We compare with Learn to Grow (L2G) \citep{learn-to-grow}, Learning to Prompt (L2P)  \citep{learning-to-prompt} and S-Prompts~\citep{s-prompts}. We use our implementation for evaluating L2P and S-Prompts, and take efforts of modifing them to work in the task-incremental setting for fair comparisons, denoted as L2P$^\dagger$ and S-Prompts$^\dagger$ in Table~\ref{tab:vdd-results}. We compare L2G with DARTS, and with more advanced $\beta$-DARTS \citep{beta-darts}.  We provide the modification and implementation details, the choice for the number of prompts per task ($L$) in L2P, and ablations for the number of prompts in the Appendix \ref{sec:training-details}. For completeness, we also compare with a baseline of Experience Replay \citep{icarl}, L2 Parameter Regularization \citep{smith2023closer}, and Elastic Weight Consolidation \citep{kirkpatrick-overcoming}, shown in Tab. \ref{tab:additional-comp} in the Appendix due to space limit. The three methods suffer from catastrophic forgetting significantly.

\begin{table} %
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
        \midrule           
            {S-Prompts$^\dagger$} ($L$=12)~\citep{s-prompts} & $82.65$ & $89.32$ & $88.91$ & $64.52$ & $72.17$ & $99.29$ & $\boldsymbol{99.89}$ & $\boldsymbol{96.93}$ & $45.55$ & $\boldsymbol{60.76}$ & $80.00 \pm 0.07$ \\
            {L2P$^\dagger$} ($L$=12)~\citep{learning-to-prompt} & $82.65$ & $89.32$ & $89.89$ & $65.63$ & $72.34$ & $99.55$ & $\boldsymbol{99.94}$ & $\boldsymbol{96.63}$ & $45.24$ & $\boldsymbol{59.57}$ & $80.08 \pm 0.10$ \\
            \midrule
            L2G-ResNet26 (DARTS)~\citep{learn-to-grow} &  $69.84$ & $79.59$ & $95.28$ & $72.03$ &  $\boldsymbol{86.6}$ & $99.72$ & $99.52$ & $71.27$ &  $\boldsymbol{53.01}$ & $49.89$ & $77.68$ \\
            \midrule
            $^*$L2G~\citep{learn-to-grow} (DARTS) & $82.65$ & $88.47$ & $85.20$ & $\boldsymbol{79.22}$ & $80.19$ & $99.28$ & $ 98.06$ & $76.14$ & $39.29$ & $46.01$ & $77.45 \pm 2.41$ \\ 
            $^*$L2G~\citep{learn-to-grow} ($\beta$-DARTS) & $82.65$ & $88.95$ & $94.73$ & $75.31$ & $79.76$ & $99.84$ & $99.76$ & $78.86$ & $34.50$ & $47.09$ & $78.14 \pm 0.54$ \\ 
            \midrule
            Our ArtiHippo (Uniform, 150 epochs) & $82.65$ & $76.20$ & $95.60$ & $75.14$ & $80.72$ & $\boldsymbol{99.92}$ & $99.86$ & $76.41$ & $42.74$ & $41.74$ & $77.10 \pm 0.75$ \\ 
             Our ArtiHippo (Hierarchical, 50 epochs) & $82.65$ & $\boldsymbol{90.97}$ & $96.05$ & $75.20$ & $82.36$ & $\boldsymbol{99.91}$ & $99.58$ & $87.16$ & $42.10$ & $52.54$ & $\boldsymbol{80.85} \pm \boldsymbol{0.72}$ \\
             Our ArtiHippo (Hierarchical, 150 epochs) & $82.65$ & $\boldsymbol{90.86}$ & $\boldsymbol{96.06}$ & $75.63$ & $84.06$ & $\boldsymbol{99.92}$ & $99.83$ & $89.28$ & $\boldsymbol{51.94}$ & $55.78$ & $\boldsymbol{82.60} \pm \boldsymbol{0.55}$ \\
            Our ArtiHippo (Hierarchical+$L$=1, 150 epochs) & $82.65$ & $90.50$ & $\boldsymbol{96.19}$ & $\boldsymbol{79.70}$ & $\boldsymbol{85.71}$ & $\boldsymbol{99.91}$ & $99.83$ & $92.42$ & $\boldsymbol{52.23}$ & $58.99$ & $\boldsymbol{83.55} \pm \boldsymbol{0.09}$ \\
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{\small 
    Results on the VDD benchmark~\citep{vdd}. Our ArtiHippo shows clear improvements over the previous approaches. 
    All the results from our experiments are averaged over 3 different seeds. The 2 highest accuracies per task have been highlighted. All the methods use the same ViT-B/8 backbone containing 86.04M parameters and having 14.21G FLOPs unless otherwise stated. $^\dagger$ our modifications for the task-incremental setting. $^*$ our reproduction with the vanilla L2G method~\citep{learn-to-grow} for the ViT backbone. ``Hierarchical$+L=1$" means the integration between our ArtiHippo and the L2P method (Sec.~\ref{sec:combining-with-prompts}).}
    \label{tab:vdd-results} \vspace{-3mm}
\end{table}


\vspace{-1mm}
\subsection{Results and Analysis on the VDD Benchmark}\vspace{-2mm}
Table \ref{tab:vdd-results} shows the results and comparisons. Our method shows consistent performance improvement across tasks compared to Learn to Grow, S-Prompts$^\dagger$ and L2P$^\dagger$. Following is an analysis of the results showing the advantages and effectiveness of our proposed method for ViTs:
\begin{itemize}[leftmargin=*]
\itemsep0em 
    \item Table \ref{tab:vdd-results} shows that L2P$^\dagger$ (which uses ViTB/8) performs better than L2G (which uses ResNet26), showing that prompting based methods which leverage the robust features learned by the ViT are indeed effective for lifelong learning.
    \item However, a closer look at task level accuracies shows that for tasks which are significantly different from the base task of the ViT (Omniglot, UCF101, SVHN), L2G significantly outperforms L2P$^\dagger$, showing that introducing new parameters can be beneficial, which justifies our motivation for for seeking more integrative memory mechanisms. %
    \item The poor average accuracy of L2G, when applied to the attention projection layer of ViTs shows that L2G is ill suited for learning to grow for ViTs, and even Uniform sampling (original SPOS) can obtain similar average accuracy. Our proposed Hiererchical sampling method outperforms both, prompting-based methods and L2G.
    \item Our hierarchical sampling scheme performs better than pure exploration (which uses 150 epochs of supernet training) with just 50 epochs of supernet training, which shows the effectiveness of the proposed sampling strategy. In fact, pure exploration cannot match the hierarchical sampling even if the supernet is trained for 300 epochs, as shown in Fig. \ref{fig:epochs-vs-acc} in the Appendix.
    \item S-Prompts$^\dagger$ and L2P$^\dagger$ perform better for tasks which are similar to the base task and have very less data (VGG-Flowers, DTD), which suggests that prompting based methods are more data-efficient. Thus, prompt-based methods and our growing-based method are complementary and combining them leads to even better performance (Tab. \ref{tab:vdd-results}). Section \ref{sec:combining-with-prompts} describes a preliminary approach to combine the two approaches. A more comprehensive integration of ArtiHippo with prompt based approaches is left for future work.
\end{itemize}

\begin{wraptable} {r}{0.4\textwidth}%
    \vspace{-13mm}
    \centering
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Method} & \textbf{\#Prompts} & \textbf{Avg. Acc.} \\
        \toprule
        S-Prompts$^\dagger$ & 1 & $88.93 \pm 0.34$ \\
        S-Prompts$^\dagger$ & 12 & $92.42 \pm 0.11$ \\
        L2P$^\dagger$ & 12 & $92.73 \pm 0.10$ \\
        \midrule
        L2G (DARTS) & - & $93.88 \pm 2.86$ \\
        L2G ($\beta$-DARTS) & - & $92.19 \pm 1.48$ \\
        \midrule
        ArtiHippo (Projection) & - & $\boldsymbol{94.33 \pm 0.45}$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.3em}
    \caption{\small Results on the 5-Dataset benchmark. 
    The results have been averaged over 5 different task orders.}
    \label{tab:5-dataset-results}
\end{wraptable}

\subsection{Results on the 5-Dataset Benchmark}\vspace{-2mm}
Table \ref{tab:5-dataset-results} shows the comparisons. We use the same ViT-B/8 backbone pretrained on the ImageNet images from the VDD benchmark for all the experiments across all the methods and upsample the images in the 5-Dataset benchmark (consisting of CIFAR10~\citep{cifar}, MNIST \citep{mnist}, Fashion-MNIST~\citep{fashion-mnist}, not-MNIST \citep{notmnist}, and SVHN~\citep{svhn}) to $72\times72$. 
We can see that ArtiHippo significantly outperforms Learn to Grow, L2P$^\dagger$, S-Prompts$^\dagger$ under the task-incremental setting. 


\vspace{-1mm}
\subsection{Learned architecture and architecture efficiency}\vspace{-2mm}
Fig.~\ref{fig:vdd_arch-sim} shows the experts learned and the long-term memory structures formed at each block tasks in the VDD dataset. When learning CIFAR100 after ImageNet, the search process learns to reuse most of the ImageNet experts. This is an intuitive result since both the tasks represent natural images. Task-specific structures can be observed between related tasks. For example, at B1, the search process learns to adapt the ImageNet expert when lerning SVHN, which is further adapted when learning Omniglot. At B3, the search process learns to adapt ImageNet when learning SVHN, which is then reused for Omniglot. At B12, the search process learns to adapt the new expert learned for CIFAR100 for SVHN, which is subsequently adapted again for Omniglot and GTSRB (three tasks which rely on some form of symbol recognition). At Block B3, the expert learned for UCF101 is reused for Pedestrian Classification (UCF101 is an action recognition benchmark, making it similar to DPed). Some blocks have more synergy: the search process reuses the ImageNet experts at blocks B2 and B11 for all the downstream tasks. The sensible architectures learned continually show the effectiveness of the proposed task-similarity-oriented ArtiHippo.

In addition to learning qualitatively meaningful architectures, the proposed method also shows quantitative advantages. On the VDD dataset, the number of parameters increases by 0.68M/task (averaged over 3 different runs). Although this is higher than L2P, our method increases the number of FLOPs only by 0.23G/task (averaged over 3 different runs), which is advantageous as compared to the increase of 2.02G/task of the L2P.




\subsection{Can ArtiHippo efficiently leverage a strong backbone?}\vspace{-2mm}
\begin{table} [h]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|l}
        \toprule
            Method-Backbone &  ImNet &  C100 &    SVHN &   UCF &  OGlt &  GTSR &  DPed &  Flwr &  Airc. &   DTD &  Avg. Accuracy \\
            \midrule
            {SupSup-Proj}~\citep{supsup} & $82.65$ & $89.96$ & $\boldsymbol{96.05}$ & $\boldsymbol{81.68}$ & $\boldsymbol{84.60}$ & $\boldsymbol{99.97}$ & $\boldsymbol{99.97}$ & $78.76$ & $44.18$ & $51.60$ & $81.14 \pm 0.04$ \\
            {EFT}~\citep{eft} & $82.65$ & $\boldsymbol{91.86}$ & $93.51$ & $73.89$ & $75.62$ & $99.58$ & $\boldsymbol{99.98}$ & $\boldsymbol{96.34}$ & $48.17$ & $\boldsymbol{64.40}$ & $82.60 \pm 0.07$ \\
            {Lightweight Learner}~\citep{ll} & $82.65$ & $\boldsymbol{91.92}$ & $93.90$ & $75.63$ & $77.07$ & $99.71$ & $99.96$ & $\boldsymbol{96.47}$ & $\boldsymbol{49.33}$ & $\boldsymbol{64.34}$ & $\boldsymbol{83.10} \pm \boldsymbol{0.02}$ \\
            ArtiHippo-T2T (50 epochs) & $82.65$ & $90.93$ & $\boldsymbol{95.96}$ & $\boldsymbol{80.74}$ & $\boldsymbol{83.25}$ & $\boldsymbol{99.94}$ & $99.96$ & $94.12$ & $\boldsymbol{58.90}$ & $60.05$ & $\boldsymbol{84.65} \pm \boldsymbol{0.33}$ \\
        \bottomrule
    \end{tabular}}
    \vspace{0.1em}
    \caption{\small Results on the VDD benchmark~\citep{vdd} under the task-to-task based lifelong learning setting. Our ArtiHippo-T2T shows clear improvements over the previous approaches. The 2 highest accuracies per task have been highlighted. All the methods use the same ViT-B/8 backbone.}
    \label{tab:t2t} \vspace{-1mm}
\end{table}


Recently, there has been increasing interest in learning techniques which can effectively leverage a pretrained backbone, and add task-specific parameters for lifelong learning, which we call as a task-to-task (T2T) setting. We evaluate our method in this setting by modifying ArtiHippo to always learn from the ImageNet trained ViT model and evaluate on the VDD benchmark. We compare with recent methods which propose a similar setting: Supermasks in Superposition (SupSup, \citep{supsup}), Efficient Feature Transformation (EFT, \citep{eft}), and Lightweight Learner (LL, \citep{ll}). We take efforts to modify all the methods to work for ViTs, and provide the details in Appendix \ref{sec:modification-for-vits}. As seen in Table \ref{tab:t2t} ArtiHippo-T2T outperforms all the other methods with just 50 epochs of supernet training, making it very efficient. Table \ref{tab:t2t} shows that SupSup achieves a higher accuracy on tasks which are very different from the  original task on which the backbone is trained (Omniglot and UCF101) but does not perform well on tasks which are similar to the base task (VGG-Flowers, Aircraft, DTD), whereas EFT and LL shows the exact opposite behavior. Our ArtiHippo-T2T can perform well across all the tasks and also achieves the highest average accuracy, which shows that our method is more generic across tasks of different nature.


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/vdd-iclr-42-ee-150.pdf}
    \caption{\small ArtiHippo learned-to-grow on VDD. Starting from the ImageNet-pretrained ViT (B1 -- B12 in Tsk1\_ImNet), sensible architectures are continually learned for the remaining 9 tasks on the VDD dataset.
    Each column denotes a Transformer block of the ViT in which only the projection layer of the MHSA block is maintained as ArtiHippo with the remaining components frozen.
    \colorbox{skip}{S}, \colorbox{reuse}{R}, \colorbox{adapt}{A}, \colorbox{new}{N} represent {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New} respectively.  \textit{Best viewed in color and magnification.}
    } \vspace{-2mm}
    \label{fig:vdd_arch-sim}
\end{figure}
