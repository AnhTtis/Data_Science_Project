\vspace{-1mm}
\section{Discussion and Conclusion}
\vspace{-2mm}
This paper presents a method of transforming Vision Transformers (ViTs) for resilient lifelong learning under the task-incremental setting. It learns to dynamically grow the final projection layer of the multi-head self-attention of a ViT in a task-aware way using four operations, {\tt Skip}, {\tt Reuse}, {\tt Adapt} and {\tt New}. The final projection layer is identified as the Artificial Hippocampi (ArtiHippo) of ViTs. The learning-to-grow of ArtiHippo is realized by our proposed hierarchical exploration-exploitation sampling based single-path one-short Neural Architectural Search (NAS), where the exploitation utilizes task similarities (synergies) defined by the normalized cosine similarity between the mean class tokens of a new task and those of old tasks. In experiments, the proposed method is tested on the challenging VDD and the 5-Datasets benchmarks. We also take great efforts in materializing several state-of-the-art baseline methods for ViTs and tested on the VDD. It obtains better performance than the prior art with sensible ArtiHippo learned continually. 

Our future work is to adapt the proposed ArtiHippo for handling class-incremental lifelong learning, and hopefully for fine-tuning large language models, or large foundation models in general. 



