
\documentclass{article} %
\usepackage{iclr2024_arxiv,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}       %
\usepackage{xcolor}         %
\usepackage{wrapfig} %
\usepackage{graphicx} %
\usepackage{amsmath} %
\usepackage{amssymb} %
\usepackage{enumitem} %
\usepackage{wrapfig} %
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{ulem} %
\usepackage[bottom]{footmisc}
\usepackage{float}

\usepackage[
]{sidecap}   
\sidecaptionvpos{figure}{t} 

\definecolor{skip}{HTML}{F2ACCA}
\definecolor{reuse}{HTML}{9CCEA7}
\definecolor{adapt}{HTML}{FEB24C}
\definecolor{new}{HTML}{9EC9E2}

\newcommand{\subfiggrid}[5]{
    \begin{subfigure}[t]{0.15\linewidth}
        \centering
        \includegraphics[width=#3cm,height=#4cm]{figures/#1_samples/#2/000001.jpg}
        \includegraphics[width=#3cm,height=#4cm]{figures/#1_samples/#2/000002.jpg}
        \caption{#5}
    \end{subfigure}
}


\title{Transforming Transformers for Resilient Lifelong Learning}



\author{Chinmay Savadikar \thanks{Department of ECE, North Carolina State University} \\
\texttt{csavadi@ncsu.edu} \\
\And
Michelle Dai \thanks{Operations Research \& Financial Engineering, Princeton University} \\
\texttt{mdai@princeton.edu} \\
\And
Tianfu Wu \footnotemark[1] \\
\texttt{tianfu\_wu@ncsu.edu}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrpreprint
\begin{document}

\maketitle

\begin{abstract}
Lifelong learning without catastrophic forgetting (i.e., resiliency) remains an open problem for deep neural networks. The prior art mostly focuses on convolutional neural networks. With the increasing dominance of Transformers in deep learning, it is a pressing need to study lifelong learning with Transformers. Due to the complexity of training Transformers in practice, for lifelong learning, a question naturally arises: Can Transformers be learned to grow in a task aware way, that is to be dynamically transformed by introducing lightweight learnable plastic components to the architecture, while retaining the parameter-heavy, but stable components at streaming tasks? To that end, motivated by the lifelong learning capability maintained by the functionality of Hippocampi in human brain, we explore what would be, and how to implement, Artificial Hippocampi (ArtiHippo) in Transformers. We present a method to identify, and learn to grow, ArtiHippo in Vision Transformers (ViTs) for resilient lifelong learning in four aspects: (i) Where to place ArtiHippo to enable plasticity while preserving the core function of ViTs at streaming tasks? (ii) How to represent and realize ArtiHippo to ensure expressivity and adaptivity for tackling tasks of different nature in lifelong learning? (iii) How to learn to grow ArtiHippo to exploit task synergies (i.e., the learned knowledge) and overcome catastrophic forgetting? (iv) How to harness the best of our proposed ArtiHippo and prompting-based approaches? In experiments, we test the proposed method on the challenging Visual Domain Decathlon (VDD) benchmark and the 5-Dataset benchmark under the task-incremental lifelong learning setting. It obtains consistently better performance than the prior art with sensible ArtiHippo learned continually. To our knowledge, it is the first attempt of lifelong learning with ViTs on the challenging VDD benchmark.
\end{abstract}


\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{-6pt}
\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{1pt}
\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{1pt} 

\input{sections/introduction}
\input{sections/related-work-and-contributions}
\input{sections/approach}
\input{sections/experiments}
\input{sections/discussions-and-conclusion}
\input{sections/acknowledgements}


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix


\input{appendix/overview}
\input{appendix/identifying-artihippo}
\input{appendix/ablation-studies}
\input{appendix/modification-for-vits}
\input{appendix/task-incremental-modification}
\input{appendix/dataset-details}
\input{appendix/vit-description}
\input{appendix/background}
\input{appendix/training-details}
\input{appendix/learned-architecture}

\end{document}
