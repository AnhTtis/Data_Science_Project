\section{Methodology}
\label{section:methodology}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/vector/exported-pdf/methodology.pdf}
    \caption{Overview of \emphasizedworkname{}. The required inputs are on the left, while the main contribution is highlighted in red as Attribution. The results are symbolized on the right in the form of plots and statistics on the fault injection campaign.}
    \label{figure:methodology}
\end{figure}

We approach the research questions mentioned in Section \ref{section:introduction:subsection:research_questions}, by developing a novel sampling technique, based on importance sampling. We show an overview of \emphasizedworkname{} in Figure \ref{figure:methodology}. As described in Section \ref{section:background:subsection:fault_injection_for_neural_networks}, we employ fault injections on the tensors corresponding to neuron weight and neuron outputs, as they are the most representative of possible \titlecaseabbreviationpl{sdc}. We describe the rest of the \emphasizedworkname{} methodology in the following sub-sections.

\subsection{Inputs}

\emphasizedworkname{} requires a complete neural system as input, comprising a \titlecaseabbreviation{dnn} model, a dataset and a device optimized for running them.
There are no other inputs required, as the fault selection is done internally by \emphasizedworkname{}, hence not requiring any other human inputs as for other \gls{sota} works.

\subsection{Attribution Computation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{images/vector/exported-pdf/attribution.pdf}
    \caption{A comparison of sampling faults without and with attribution: when not using attribution, all the target elements have the same weight, hence the random uniform sampling behavior. When using attribution, the importance of each element on the output computation is used as a weight for the sampling algorithm, directing it towards faults with more potential impact.}
    \label{figure:attribution}
\end{figure}

Attribution computation is the main novel contribution provided by \emphasizedworkname{}.
It is used to generate the weights to be used when sampling the faults, as shown in Figure \ref{figure:attribution}. When attribution is not used, each element has the same weight, hence resulting in random uniform sampling. However, attribution takes into account the importance of each element when computing the model output, hence resulting in asymmetric weights for the sampling.
The attribution implementation is different if we are computing the attribution for the neuron weight or the neuron output, as different formulas are used. Additionally, the attribution computation reaches up to each bit in the target tensor.
The implementations are detailed in the following paragraphs.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{images/vector/exported-pdf/matmul.pdf}
%     \caption{}
%     \label{figure:matmul}
% \end{figure}

\subsubsection{Neuron Output}
\label{section:methodology:subsection:attribution_computation:subsubsection:neuron_output}

For the neuron output attribution, we use the layer conductance definition, as defined in \cite{shrikumarComputationallyEfficientMeasures2018,dhamdhereHowImportantNeuron2018}. We report the used equation here:

\begin{equation*}
    \label{equation:weight_conductance}
    \ms{Cond}^{y}(x) ::= \sum_{i}  (x_i-x'_i)\cdot\int_{\alpha=0}^{1} \tfrac{\partial F(x' + \alpha (x-x'))}{\partial y} \cdot \tfrac{\partial y}{\partial x_i} ~d\alpha 
\end{equation*}

$y$ is the output of the neuron, $x$ the test inputs to the network, varying over $i$, $x'$ the baseline input, $F$ is the model function. 
We sum the deviation from the baseline of each input weighted by the integral of the gradient computed between the actual input and the baseline, further weighing the gradient by the gradient of the neuron output with respect to the actual input. In this way we can compute the deviations with respect to a baseline execution, making the attribution closer to the real importance of each neuron output.

\subsubsection{Neuron Weight}
\label{section:methodology:subsection:attribution_computation:subsubsection:neuron_weight}

The attribution for the weight of a neuron is computed with respect to the generated outputs, using the following formula:

\begin{equation*}
    \label{equation:layer_conductance}
    \ms{WeightAttr}_j^L(x) =\sum_{i = 1}^{N} \tfrac{\partial L(x_i)}{\partial w_j}
\end{equation*}

$L$ is the function of the layer we compute the attribution for, $w_j$ is the weight we are computing the attribution for, with $j$ being the index, $x_i$ are the possible inputs used in the gradient computation.
The formula sums all the gradients computed from all the outputs for each weight tensor, and this is used as a weight for the sampling algorithm.
The number of inputs $N$ to be used depends on the application and the specific implementation, but can vary from 1 to a full dataset.

\subsubsection{Tensor Bit}
\label{section:methodology:subsection:attribution_computation:subsubsection:tensor_bit}

\import{algorithms/}{methodology_bit_gradient}

For computing the attribution of each bit in the target tensor, we re-implemented the definition of floating point over 32 bits \cite{IEEEStandardFloatingPoint2019}, computing the real value out of the bit list. By carrying out the computations directly, we can exploit the auto-differentiation capabilities of different libraries, e.g., PyTorch, to compute the gradients based on the importance of the tensor, and use them as importance for each bit.
We use the gradient as attribution as it can be seen as the approximation of the linear coefficient for a weighted sum across all the bits generating the tensor value.
The algorithm is shown in Algorithm \ref{algorithm:methodology_bit_gradient}. There are differences with the implementation in the IEEE standard, however, they revolve around handling exceptions and special cases: we opted for not using directly the \titlecaseabbreviation{nan} and $\pm\infty$ values, as doing so would affect the computed gradients. Hence, we use flags to represent these special cases, as shown in Lines \ref{algorithm:line:flag_start}-\ref{algorithm:line:flag_end}, and then we multiply them by constants to allow for the auto-differentiation algorithm to compute the gradients. In the case of $\pm\infty$, we normalize by the value, and then multiply by the maximum non-infinite number divided by 33, so that even if the value is close to infinity it would not be exactly $\pm\infty$. This process is shown in Line \ref{algorithm:line:flag_multiply_inf}.

\subsection{Fault Model}

After computing the attributions, the fault model uses the attributions for the selected injection target, neuron weight or neuron output, as weights for the sampling algorithm.
Depending on the application and the user, the fault model can also incorporate a percentage of random uniform fault samples, to increase coverage while still providing higher precision with the importance sampling.
The fault model is easily customizable, so that extra information can be added in how the faults are sampled.

\subsection{Fault Injection Framework}

The faults sampled by the fault model are then injected through the use of a fault injection framework, which compares the baseline accuracy over the whole testing dataset of the model-under-test with the accuracy when using the faults.
The choice of the fault injection framework is dictated by the campaign requirements in terms of resources and customizability, but \emphasizedworkname{} is independent of the fault injection framework as long as it can receive the fault location as input.

\subsection{Output}

The results of the injection are summarized in a SQLite database or CSV file, depending on the fault injection framework, which can be accessed and used for further visualization, as it will be shown in Section \ref{section:evaluation}.

\subsection{Application Example: Fault-Aware Training}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/vector/exported-pdf/fat_methodology.pdf}
    \caption{Methodology overview for \glsentrytitlecase{fat}{long}. The model, shown as input on the bottom left, is trained until a certain accuracy threshold is reached, and if so, faults are injected using \emphasizedworkname{}, while training continues until all faults are injected and the target accuracy is reached. The resulting model is fault-resilient against the injected faults, shown on the top right.}
    \label{figure:fat_methodology}
\end{figure}

\titlecaseabbreviation{fat} \cite{mahmoudPyTorchFIRuntimePerturbation2020} is a technique used to make neural network models more resilient to faults and the corresponding \titlecaseabbreviationpl{sdc} that might happen in real-world scenarios. Generally, faults are uniformly sampled from a database, or they follow some specific human-devised fault model, and they are integrated in the training loop, as shown in Figure \ref{figure:fat_methodology}. In our use-case, we use \emphasizedworkname{} to inject faults, showing it can be used to speed up the search for faults leading to \titlecaseabbreviationpl{sdc}.