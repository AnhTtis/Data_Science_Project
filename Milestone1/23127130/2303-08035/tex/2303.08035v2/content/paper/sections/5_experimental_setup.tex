\section{Experimental Setup}
\label{section:experimental_setup}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/vector/exported-pdf/experimental_setup.pdf}
    \caption{Experimental setup for evaluating \emphasizedworkname{}. Different models on different datasets are trained on GPU, and then injected using enpheeph as fault injection framework and \emphasizedworkname{} to generate the faults. The results are analyzed and plotted to be compared with random uniform fault sampling.}
    \label{figure:experimental_setup}
\end{figure}

To evaluate \emphasizedworkname{}, we use the experimental setup shown in Figure \ref{figure:experimental_setup}. First the models are trained on different datasets, as shown in \GoodDarkGreenCircled{A}, and they are then used for the fault injection campaign with \emphasizedworkname{} in \GoodDarkGreenCircled{B}. The technical details of the implementations are discussed in the following sub-sections.

\subsection{Hardware}

We employ 4 Nvidia RTX 2080 Ti GPUs \cite{GraphicsReinventedNVIDIA}, with AMD Ryzen Threadripper 2990WX CPU \cite{AMDRyzenThreadripper}, for training the selected models. The models are being read/written on SSD, to avoid the speed limit of hard drives.
We use Manjaro Linux as operating system.

\subsection{Software}

We train \gls{sota} \titlecaseabbreviation{dnn} models, VGG-11 \cite{simonyanVeryDeepConvolutional2015} and ResNet18 \cite{heDeepResidualLearning2016}, on common benchmark datasets, CIFAR10 \cite{Krizhevsky2009LearningML} and GTSRB \cite{stallkampManVsComputer2012}, using \gls{sota} hyperparameter settings and techniques, such as the Adam optimizer \cite{kingmaAdamMethodStochastic2015} with learning rate \num{0.001}, 30 epochs of training with batch size 64, and validation. These models are trained using PyTorch Lightning \cite{Falcon_PyTorch_Lightning_2019} and its auxiliary libraries, PyTorch Lightning Bolts \cite{falcon2020framework} and PyTorch Lightning Flash \cite{PyTorchLightningLightningflash2022}.
We implement \emphasizedworkname{} using PyTorch \cite{paszkePyTorchImperativeStyle2019} and the enpheeph fault injection framework \cite{colucciEnpheephFaultInjection2022a}, to inject the faults and compare them with random uniform sampling. Additionally, we use Captum  \cite{kokhlikyanCaptumUnifiedGeneric2020} for implementing the attribution algorithm for the neuron output.
The outcomes are recorded in CSV, analyzed with pandas \cite{the_pandas_development_team_2023_7549438} and plotted using Plotly \cite{hossainVisualizationBioinformaticsData2019}.

\subsection{Fault-Aware Training}

The setup for the \titlecaseabbreviation{fat} is similar to the general setup, however, the model is a custom smaller \titlecaseabbreviation{dnn} employing 2 convolutional layers with ReLU as activation, followed by 2 fully-connected layers with ReLU as activation. We employ a smaller model as it is easier to analyze the generated fault patterns compared to more complex models.
The model is trained on FashionMNIST \cite{xiaoFashionMNISTNovelImage2017} with the Adam optimizer, using the AMD CPU.

\subsection{Metrics}
\label{section:experimental_setup:subsection:metrics}

To measure the efficacy of \emphasizedworkname{} compared to random uniform sampling, we compute the rate of discovered \titlecaseabbreviationpl{sdc} over the total number of injected faults, computing the \ms{Precision} of \emphasizedworkname{}.
We use the statistical definition of precision:

\begin{equation}
    0 \leq \ms{Precision} = \frac{\ms{Positive_{True}}}{\ms{Positive_{True}} + \ms{Positive_{False}}} \leq 1
\end{equation}

where \ms{Positive} represents the elements which are recognized as part of the target set of elements by \emphasizedworkname{}, with \ms{True} meaning that they are part of the actual target set, and \ms{False} as they were wrongly categorized, and they are not part of the target set. The target set is defined based on the accuracy drop caused by the \titlecaseabbreviation{sdc} generated by the fault: when we write \glsxtrshort{sdc}\num{0.05} we mean all the faults that lead to a \titlecaseabbreviation{sdc} generating an accuracy drop of \num{5}\% compared to the fault-free baseline.

We also define the \ms{Recall}, as the number of faults leading to a predetermined \titlecaseabbreviation{sdc} threshold drop over the total number of possible faults in the whole search space:

\begin{equation}
    0 \leq \ms{Recall} = \frac{\ms{Positive_{True}}}{\ms{Positive_{True}} + \ms{Negative_{False}}} \leq 1
\end{equation}

where $\ms{Negative_{False}}$ represents all the possible non-\titlecaseabbreviation{sdc} faults, and therefore must be either estimated or enumerated over the whole space of possible faults. We use the \ms{Recall} metric only for comparison with \titlecaseabbreviation{sota}.

\subsection{Experiments}
\label{section:experimental_setup:subsection:experiments}

We run the whole set of experiments for each of the 4 model-dataset configurations which have been trained:
\begin{itemize}
    \item VGG11-CIFAR10;
    \item VGG11-GTSRB;
    \item ResNet18-CIFAR10;
    \item ResNet18-GTSRB.
\end{itemize}

Each configuration undergoes the same set of experiments, to understand how the importance sampling through the different attributions compares to the random uniform sampling. To this end, we additionally implement two extra bit attribution algorithms:

\begin{itemize}
    \item linear bit-weighting: where each bit is weighted linearly starting from 1, so the \titlecaseabbreviation{lsb} in position 0 has a weight of 1, and the \titlecaseabbreviation{msb} in position 31 has a weight of 32, as in $\ms{weight} = i + 1, \forall i = 0, \ldots, 31 $;
    \item exponential bit-weighting: each bit is weighted in the form $2^i$, where $i$ is the index of the bit, starting from 0 with the \titlecaseabbreviation{lsb} and reaching 31 for the \titlecaseabbreviation{msb}, as in $\ms{weight} = 2^i, \forall i = 0, \ldots, 31 $.
\end{itemize}

\import{tables/}{experiment_legend.tex}

These alternative bit attribution algorithms are different approximations of the bit gradient algorithm, depending on the amount of overhead that is tolerable in the fault injection campaign.
Therefore, we use the codes shown in Table \ref{table:experiment_legend} to represent each experiment in Section \ref{section:evaluation}. As an example, we can choose the codes \textbf{GBINo} and \textbf{RBRNw}, with the former meaning we have used the \textbf{G}radient \textbf{B}it attribution together with the \textbf{I}mportance attribution sampling for the \textbf{N}euron \textbf{o}utput, while the latter stands for \textbf{R}andom \textbf{B}it with \textbf{R}andom uniform sampling for the \textbf{N}euron \textbf{w}eight.

Each experiment is run for \num{5} times with the \num{5} different fixed seeds across all the Python libraries, to improve reproducibility of the results and reduce non-deterministic operations, while also averaging out any out-of-distribution irregularities, and each single experiment per seed lasts for exactly \num{24} hours, covering roughly \num{10000} faults. The achieved standard deviation is roughly 0.05 for the precision metric.
% When plotted, some experiments may be omitted for clarity, as results are similar to those of the shown experiments.

\subsubsection[Fault Aware Training]{\glsentrytitlecase{fat}{long}}
\label{section:experimental_setup:subsection:experiments:subsubsection:fault_aware_training}

Regarding the \titlecaseabbreviation{fat}, we employ a similar setup for the baseline training, training the network for 30 epochs with batch size 64, using the Adam optimizer with \num{0.01} learning rate.
After training the model for the baseline accuracy measurements, we retrain it from scratch, and after each epoch we run 3 fault injection simulations with a different random seed each, and for each simulation we measure the time required to reach different \titlecaseabbreviation{sdc} thresholds for 3 times in a row, to reduce the effect of initialization in the experiments. Each simulation covers all the experiments shown in Table \ref{table:experiment_legend}. The \titlecaseabbreviation{sdc} thresholds range from \num{0.00} to \num{0.90} in \num{0.05} steps, as the base accuracy of the trained model is \num{90}\%, therefore it would not be possible to reach below-zero accuracy.
Finally, we select the first 5 faults from the \emphasizedworkname{} simulations, for both gradient bit weighting with importance sampling for neurons and random uniform sampling, and we inject them after 5 epochs of training, to confirm possibility of increasing the \titlecaseabbreviation{dnn} robustness to the chosen faults by means of training techniques. The results will be shown in Section \ref{section:evaluation:subsection:fault_aware_training}. 


