\section{Evaluation}
\label{section:evaluation}

We evaluate \emphasizedworkname{} based on the metrics and experiments mentioned in Sections \ref{section:experimental_setup:subsection:metrics} and \ref{section:experimental_setup:subsection:experiments}.

\subsection[Comparison with state of the art]{Comparison with \glsentrylong{sota}}

\import{tables/}{evaluation_sota}

We compare \emphasizedworkname{} with other \gls{sota} fault models, namely BinFI \cite{chenBinFIEfficientFault2019} and AVFI \cite{jhaMLBasedFaultInjection2019}. We get the other models' results from their own papers. The comparison is shown in Table \ref{table:evaluation_sota}.
BinFI has very high recall, however it requires running $\frac{1}{5}$ of the total possible faults occurring in the system-under-test. AVFI reaches a high precision of \num{82}\%, however it still needs to execute around \num{100000} faults. Both models require external a-priori knowledge, assuming the data type used by the model and its characteristics in the former, while assuming the system configuration and the importance of the different components in the latter.

On the other hand, \emphasizedworkname{} requires no a-priori knowledge for running the importance algorithms, and while its precision is slightly lower than \gls{sota} with \num{59.4}\%, it is the average of multiple experiments and not a single campaign as in the \gls{sota}, and it requires less than 100 samples to reach it, while at the same time having \num{100}\% recall if the fault injection campaign runs for a longer time.

\subsection{Attribution Efficacy}
\label{section:evaluation:subsection:attribution_efficacy}

First, we focus our attention on the attribution algorithms which have been developed as part of \emphasizedworkname{}. We compare the precision of the fault injection campaigns when running all the possible experiments as shown in Table \ref{table:experiment_legend}.
Therefore, we can understand whether the methodology applied by \emphasizedworkname{} is better than random uniform sampling, while still not relying on external knowledge when selecting the faults.

\subsubsection{Neuron Output}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/raster/pdf/histogram_activation.pdf}
    \caption{Average precision of \emphasizedworkname{} across different \titlecaseabbreviation{sdc} thresholds when running different neuron output experiment configurations on ResNet18-GTSRB. Importance sampling for neurons increases the precision for bit-weighting algorithms which are closer to the real importance of the bits, as in gradient and exponential weights, while having less impact with random uniform and linear weights.}
    \label{figure:histogram_activation}
\end{figure}

We start with the analysis of the neuron output simulations on ResNet18-GTSRB. The results for the average precision are shown in Figure \ref{figure:histogram_activation}. We can see importance sampling with neuron output attribution achieving higher precision than with random uniform sampling of neurons, hence highlighting the importance used by \emphasizedworkname{} as a working algorithm. As pointed by \GoodRedCircled{A}, there is a \num{0.087} difference in precision between \textbf{GBINo} and \textbf{GBRNo} at \titlecaseabbreviation{sdc}\num{0.05}, which further increases to \num{0.121} when considering \textbf{EBINo} and \textbf{EBRNo}, as shown by \GoodRedCircled{B}. This is due to the capability of selecting the neurons which have the most effect on the \titlecaseabbreviation{dnn} execution, hence leading to wider accuracy drops when generating \titlecaseabbreviationpl{sdc}. However, importance sampling for neurons does not perform as well if the target bit is randomly or linearly chosen, as shown by \GoodRedCircled{C}: this can be attributed to the effects of using random uniform sampling and linear weighting for the target bit, which makes the effects of the fault too much dependent on the chosen bit rather than the chosen neuron output, hence limiting the efficacy of the importance attribution sampling. \emph{This highlights the requirement of employing attributions both at the tensor and at the bit level to achieve optimal results, as developed in \workname{}}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/raster/pdf/line_activation.pdf}
    \caption{Average precision of \emphasizedworkname{} over number of samples for different neuron output experiment configurations with ResNet18-GTSRB. We can notice traces employing importance sampling for neurons having higher precision in the first 100 samples, decreasing to the steady-state values afterwards.}
    \label{figure:line_activation}
\end{figure}

We then analyze the same experiments on ResNet18-GTSRB but along the number of samples, shown in Figure \ref{figure:line_activation}. Each line represents the precision with increasing number of samples, and there are some interesting patterns: with a low number of samples, importance neuron sampling has higher precision than the steady-state average found in the previous Figure \ref{figure:histogram_activation}, as pointed by \GoodRedCircled{D} for the \textbf{GBINo} and \textbf{EBINo} traces. It then decreases to the steady-state value, while for random neuron sampling in \textbf{GBRNo} the behavior is opposite, being lower and then increasing to the steady-state value. These steady-state values are reached fairly soon in the campaign, as shown by \GoodRedCircled{E} at $\sim$\num{450} samples. These two groups, one at the top and one at the bottom, highlight yet again the choice of the bit weighting algorithm to be paramount for achieving high precision, together with the use of importance sampling for neurons, as linear and random uniform bit weights have precision lower than \num{0.10}.

\subsubsection{Neuron Weight}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/raster/pdf/histogram_weight.pdf}
    \caption{Average precision of \emphasizedworkname{} across different neuron weight experiment configurations on ResNet18-CIFAR10. The drop across experiment configurations is higher when employing a different bit weighting technique, rather than switching from importance sampling to random uniform sampling for neurons.}
    \label{figure:histogram_weight}
\end{figure}

We repeat a similar in-depth analysis for neuron weight injections on ResNet18-CIFAR10 using \emphasizedworkname{}. As we can see in Figure \ref{figure:histogram_weight}, both importance sampling for neuron and bit attribution are important when running fault injection campaigns. As pointed by \GoodRedCircled{F}, there is a bigger drop when going from \textbf{GBINw} to \textbf{EBINw} than going from \textbf{GBINw} to \textbf{GBRNw}. This highlights the importance of choosing the appropriate bit weighting technique, and with support from importance sampling for neurons the results can be further enhanced. This is further proved by noticing how linear bit weights and random uniform sampling for bits have much lower precision, even when using importance sampling for neurons, as pointed by \GoodRedCircled{G}. Therefore, the choice of the bit weighting technique is important in achieving high precision early on in the fault injection campaign.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/raster/pdf/line_weight.pdf}
    \caption{Average precision of \emphasizedworkname{} over number of samples for different neuron weight experiment configurations on ResNet18-CIFAR10. Importance sampling for neurons provide a precision value close to the steady-state value as soon as 50 samples after the start of the fault injection campaign, showing \emphasizedworkname{} can achieve high precision early in the campaign.}
    \label{figure:line_weight}
\end{figure}

Additionally, we analyze the evolution of precision of \emphasizedworkname{} running on ResNet18-CIFAR10 over an increasing number of samples, plotted in Figure \ref{figure:line_weight}. This plot shows steady-state results similar to the histograms in Figure \ref{figure:histogram_weight}. However, as pointed by \GoodRedCircled{H}, we can notice how the average precision for \textbf{GBINw} reaches the steady-state value in less than 50 samples. This reinforces the value of importance sampling with \emphasizedworkname{} for achieving high precision early in the fault injection campaign. 

\subsection{Comparison across different configurations}
\label{section:evaluation:subsection:different_configurations}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/raster/pdf/alltogether.pdf}
    \caption{Average precision of \emphasizedworkname{} across different neuron weight experiment configurations on all the model-dataset pairs analyzed in this work. The average precision achieved by \emphasizedworkname{} is up to \num{15}$\times$ higher than the precision achieved by using random uniform sampling for both neuron and bit.}
    \label{figure:alltogehter_histogram}
\end{figure*}

Here, in Figure \ref{figure:alltogehter_histogram}, we show the average precision for all the different experiment configurations across all the possible model-dataset pairs. \emphasizedworkname{} has lower efficacy on the VGG11 model, as exponential bit weights achieve higher precision than gradient-based ones, as pointed by \GoodRedCircled{I}: this is related to the different internal architecture of the VGG model series compared to ResNet, and is dependent on the exact implementation of the importance sampling algorithm.
This effect is further extended as highlight by pointer \GoodRedCircled{J}, where the configuration \textbf{RBRNw} has higher precision than all the other ones up to \textbf{LBINw}.
For ResNet18-GTSRB, we have the results pointed by \GoodRedCircled{K} analyzed in deeper details in Figure \ref{figure:histogram_activation}. We see that the choice of importance sampling for neuron and the bit weighting technique increases the average precision during the fault injection campaign. In the case of weight injection, \emphasizedworkname{} with the \textbf{GBINw} experiment configuration reaches up to \num{15}$\times$ higher precision than the \textbf{RBRNw} experiment configuration, as shown by \GoodRedCircled{L}.  

\subsection[Fault Aware Training]{\glsentrytitlecase{fat}{long}}
\label{section:evaluation:subsection:fault_aware_training}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/raster/pdf/fat.pdf}
    \caption{Comparison of average latency across different experiment configurations for achieving \num{3} consecutive critical faults with different \titlecaseabbreviation{sdc} thresholds. We can notice how gradient bit weighting outperforms other methods, and importance sampling for neurons further reduces the required number of cycles, reaching to \num{12.2}$\times$ lower latency.}
    \label{figure:fat_latency}
\end{figure}

We now analyze the usage of \emphasizedworkname{} in a real-world use case scenario, \glsentrytitlecase{fat}{long}. We analyze the time and cycle latency required to gather 3 faults leading to different \titlecaseabbreviation{sdc} thresholds, as defined in Section \ref{section:experimental_setup:subsection:experiments:subsubsection:fault_aware_training}. We can notice from the histogram of the latency in nanoseconds and the number of cycles in Figure \ref{figure:fat_latency}, that \textbf{GBINo} has \num{12.2}$\times$ lower number of cycles than \textbf{RBRNo}, pointed by \GoodRedCircled{M}, therefore showcasing that \emphasizedworkname{} can provide fault injection speed ups compared to random uniform sampling.
Additionally, in some cases, the computation of the attribution can increase the overhead of the fault injection, as pointed by \GoodRedCircled{N}, showing that \textbf{GBINw} has more latency in nanoseconds than latency in number of cycles than \textbf{GBRNw}.

\import{tables/}{fat_training}

After selecting the first 5 faults from the sampled ones in the previous experiment for both the \textbf{GBINo} and \textbf{RBRNo} experiment configurations, we try training the test network with them, and we report the different accuracies in Table \ref{table:fat_training}: the final post-\titlecaseabbreviation{fat} accuracy is very close to the original one for both \textbf{GBINo} and \textbf{RBRNo}, as well as the accuracy when injected with the set of faults they were trained against. However, as the faults genered by \textbf{GBINo} were critical, this leads to an overall more resilient model, while for \textbf{RBRNo} this does not hold. Therefore, when the \textbf{RBRNo}-trained model is injected with the \textbf{GBINo}-generated faults, the accuracy is as low as for the baseline model, with a drop from the baseline more than \num{4}$\times$ larger than for \textbf{GBINo}, showing that \emphasizedworkname{} can generate critical faults useful in improving the \titlecaseabbreviation{dnn} model resilience in practical applications like \titlecaseabbreviation{fat}.