\section{Round 0}


\textbf{Idea:} Importance Sampling acceleration of fault injection, with fault-aware training as use case


\textbf{Novel contributions:}
\begin{itemize}
    \item importance sampling based on output/weight importance
    \item ??? fault-aware training use case for importance sampling
    \item comparison of importance and random sampling regarding convergence speed, diversification
    \item comparison of full importance sampling (bit position and neuron position are based on weights) to partial importance sampling (only bit position is based on weights) to full random sampling
\end{itemize}

% why SotA has not addressed this yet, what are the challenges
\textbf{Scientific Challenges:}
\begin{itemize}
    \item missing proper fault injection framework optimized for fast execution (enpheeph, LLFI)
    \item 
    \item 
\end{itemize}

\textbf{Current Results:}
\begin{itemize}
    \item Better than state-of-the-art
    \begin{itemize}
        \item Wider coverage in same amount of time
        \item Faster convergence for same number of critical faults
    \end{itemize}
    \item Similar accuracy/fault coverage as state-of-the-art
\end{itemize}

\textbf{Comparison with SotA:}
\begin{itemize}
    \item Comparison with random sampling, both for convergence speed and coverage of faults for bad errors
\end{itemize}

\textbf{Expected Results:}
\begin{itemize}
    \item fault-aware training? Comparison with normal random sampling for fault collection on the newly trained network, faster convergence wins
    \item 
\end{itemize}