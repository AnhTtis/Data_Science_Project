\section{Conclusion}
\label{section:conclusion}
\glslocalresetall
In the nano-era, devices have become more susceptible to transient faults. As \titlecaseabbreviation{dl} systems became common in many applications, it is of paramount importance to guarantee their safety against all possible faults. Current \gls{sota} techniques for fault injection testing are inefficient and time-expensive.
We develop the \emphasizedworkname{} methodology, developing and implementing multiple importance sampling algorithms, increasing the weights of the most important neurons and bits when sampling for specific fault locations.
We employ \emphasizedworkname{} to compute critical faults of different \titlecaseabbreviation{dnn} model-dataset pairs, namely VGG11 and ResNet18 for the models and CIFAR10 and GTSRB for the datasets. In this analysis, we show that importance sampling for neurons and bits leads to up to \num{15}$\times$ higher precision in selecting critical faults against random uniform sampling, achieving this precision in less than \num{100} faults. This is in contrast with \gls{sota} fault models, which require external a-priori knowledge and hundreds of thousands of faults to achieve high precision.
We additionally test \emphasizedworkname{} in a real-world use case scenario, \titlecaseabbreviation{fat}, to generate faults and use them while training a \titlecaseabbreviation{dnn} model to make it resilient to the generated faults. When compared to random uniform sampling, \emphasizedworkname{} generates more critical faults with up to \num{12}$\times$ lower overhead.
We have shown \emphasizedworkname{} to be more effective than random uniform sampling and various \gls{sota} fault models, future work directions will focus on better analyzing the resilience of different models and implementations to develop optimal fault mitigation techniques.
