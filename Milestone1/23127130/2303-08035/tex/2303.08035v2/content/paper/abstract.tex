\glslocalresetall \titlecaseabbreviation{dl} systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced \titlecaseabbreviation{dl} systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the \titlecaseabbreviation{dl} application level, where faults can lead to undetectable and unrecoverable errors.
Using fault injection, we can perform resilience investigations of the \titlecaseabbreviation{dl} system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Typically, the way the faults are chosen for such fault injection is based on random uniform sampling of the space of all the possible faults. However, this method of random uniform sampling of neurons and faults is extremely inefficient for large \titlecaseabbreviationpl{dnn}, leading to huge inaccuracies and slow execution of fault injection experiments. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space.
Therefore, we propose \emphasizedworkname{}, a novel methodology that employs neuron sensitivity to generate importance sampling-based fault-scenarios. Without any a-priori knowledge of the model-under-test, \emphasizedworkname{} provides an equivalent reduction of the search space as existing works, while allowing long simulations to cover all the possible faults, improving on existing model requirements.
Our experiments show that the importance sampling provides up to \num{15}$\times$ higher precision in selecting critical faults than the random uniform sampling, reaching such precision in less than \num{100} faults. Additionally, we showcase another practical use-case for importance sampling for reliable \titlecaseabbreviation{dnn} design, namely \emph{\titlecaseabbreviation{fat}}. By using \emphasizedworkname{} to select the faults leading to errors, we can insert the faults during the \titlecaseabbreviation{dnn} training process to harden the \titlecaseabbreviationpl{dnn} against such faults. Using importance sampling in \titlecaseabbreviation{fat} reduces the overhead required for finding faults that lead to a predetermined drop in accuracy by more than \num{12}$\times$.

% Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors.
% Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Typically, the way the faults are chosen for such fault injection is based on the random uniform sampling of the space of all the possible faults. However, this method of random uniform sampling of neurons and faults is extremely inefficient for large Deep Neural Networks (DNNs), leading to huge inaccuracies and slow execution of fault injection experiments. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space.
% Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate importance sampling-based fault-scenarios. ISimDL provides an equivalent reduction of the search space as existing works, while allowing long simulations to cover all the possible faults, and without any a-priori knowledge of the model-under-test, improving on existing model requirements.
% Our experiments show that the importance sampling provides up to 15x higher precision in selecting critical faults than the random uniform sampling, reaching such precision in less than 100 faults. Additionally, we show case another practical use-case for importance sampling for reliable DNN design, i.e., Fault Aware Training (FAT). By using ISimDL to select the faults leading to errors, we can insert the faults during the DNN training process to harden the DNN against such faults. Using importance sampling in FAT reduces the overhead required for finding faults that lead to a predetermined drop in accuracy by more than 12x.

% \glslocalresetall \titlecaseabbreviation{dl} systems have proliferated in many applications. In the nano-era, these systems are highly susceptible to transient faults. Using fault injection, we can simulate the behavior of the system if neurons are affected by a randomly-occurring transient fault. However, random sampling of neurons and faults is extremely inefficient for large \titlecaseabbreviationpl{dnn}, leading to poor coverage of fault analysis. We propose a novel methodology that employs neuron sensitivity to generate importance sampling-based fault-scenarios. Our experiments show that the accuracy spread using importance sampling is 5x greater than when using random sampling, while providing 30\% faster convergence to the final distribution.


% \glslocalresetall \textbf{TEMPORARY} \titlecaseabbreviation{dl} systems have proliferated in many applications. In the nano-era, these systems are highly susceptible to transient faults. Using fault injection, we can simulate the behavior of the system if neurons are affected by a randomly-occurring transient fault. However, random sampling of neurons and faults is extremely inefficient for large \titlecaseabbreviationpl{dnn}, leading to poor coverage of fault analysis. We propose a novel methodology that employs neuron sensitivity to generate importance sampling-based fault-scenarios. Our experiments show that the accuracy spread using importance sampling is 5x greater than when using random sampling, while providing 30\% faster convergence to the final distribution.

% \glslocalresetall Research on \titlecaseabbreviationpl{dnn} has focused on improving performance and accuracy for real-world deployments, leading to new models, such as \titlecaseabbreviationpl{snn}, and optimization techniques, e.g., quantization and pruning for compressed networks. 
% However, the deployment of these innovative models and optimization techniques introduces possible reliability issues, which is a pillar for \glspl{dnn} to be widely used in safety-critical applications, e.g., autonomous driving. Moreover, scaling technology nodes have the associated risk of multiple faults happening at the same time, a possibility not addressed in state-of-the-art resiliency analyses.

% Current state-of-the-art frameworks for resiliency analysis are tailored to analyze distinctive fault patterns happening in specific \glspl{dnn}, not allowing the analysis of newer models and techniques without long implementation delays. In addition, they lack optimized operations and support for non-standard fault patterns, which are required to make the analyses future-proof.

% Towards better reliability analysis for \glspl{dnn}, we present
% \emphasizedworkname, a Fault Injection Framework for Spiking and Compressed \glspl{dnn}. The \emphasizedworkname~framework enables optimized execution on hardware devices, e.g., \glsxtrshortpl{gpu}, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on \glspl{snn} as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools.

% To evaluate our~\emphasizedworkname~framework, we analyze the resiliency of different \gls{dnn} and \gls{snn} models, with different compression techniques. By injecting a random and increasing number of faults, we show that \glspl{dnn} can show a reduction in accuracy with a fault rate as low as $7 \times 10 ^{-7}$ faults per parameter, with an accuracy drop higher than $40\%$. Additionally, we show the increased resiliency of \glspl{snn}, which do not show any accuracy reduction with fault rates as high as $6 \times 10 ^{-2}$. Run-time overhead when executing \emphasizedworkname~is less than $20\%$ of the baseline execution time when executing $\numprint{100000}$ faults concurrently, at least $10 \times$ lower than state-of-the-art frameworks, making \emphasizedworkname~future-proof for complex fault injection scenarios.

% We will additionally release the source code of our \emphasizedworkname~framework under an open-source license.


% Research in \titlecaseabbreviationpl{dnn} has focused on improving performance and accuracy for real-world deployments, leading to new models and optimization techniques, such as \titlecaseabbreviationpl{snn} and quantization with pruning for compressed networks.
% However, the deployments of these innovative models and optimization techniques neglect possible reliability issues, which is a pillar for \glspl{dnn} to be widely used in safety-critical applications, e.g., autonomous driving.
% Current SotA frameworks for reliability analysis are tailored to analyze specific fault patterns happening in specific DNNs, not allowing the analysis of newer models and techniques without long implementation delays.

% Towards better reliability analysis for DNNs, we present \emphasis{enpheeph}, a Neural Fault Injection Framework.
% The \emphasis{enpheeph} framework enables optimized fault execution on GPUs and other devices while providing complete customizability to investigate different fault models, emulating different reliability constraints and use-cases.
% Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools.

% To test the \emphasis{enpheeph} framework, we analyze the resiliency of different \glspl{dnn} and \glspl{snn}, showing that as little as XXX random faults is capable of reducing the accuracy of the networks by XXX\%. Additionally, the fault-injection campaigns were run XXX faster than when using other SotA frameworks.

% We will additionally release the source code of our \emphasis{enpheeph} framework at Link.

\glslocalresetall


% \begin{comment}
% Research in Deep Neural Networks (DNNs) has focused on improving performance and accuracy for real-world deployments, leading to new models such as Spiking Neural Networks (SNNs) and optimization techniques,e.g., quantization and pruning for compressed networks. 
% However, the deployment of these innovative models and optimization techniques neglects possible reliability issues, which is a pillar for DNNs to be widely used in safety-critical applications, e.g., autonomous driving.
% Current state-of-the-art frameworks for reliability analysis are tailored to analyze distinctive fault patterns happening in specific DNNs, not allowing the analysis of newer models and techniques without long implementation delays.
% Towards better reliability analysis for DNNs, we present
% enpheeph, a Neural Fault Injection Framework. The enpheeph framework enables optimized execution on hardware devices, e.g., GPUs, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications
% to the underlying code, a feat that is not achievable by other state-of-the-art tools.
% To test the enpheeph framework, we analyze the resiliency of different DNNs and SNNs, showing that as little as XXX random faults are capable of reducing the accuracy of the networks by XXX\%. Additionally, enpheeph shows XX times less execution overhead (in terms of run time) on the fault-injection experiments, than SotA frameworks.
% We will additionally release the source code of our enpheeph framework at Link.
% \end{comment}