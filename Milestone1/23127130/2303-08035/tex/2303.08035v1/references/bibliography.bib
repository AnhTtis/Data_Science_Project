@inproceedings{agarwalLLTFIFrameworkAgnostic2022,
  title = {{{LLTFI}}: {{Framework Agnostic Fault Injection}} for {{Machine Learning Applications}} ({{Tools}} and {{Artifact Track}})},
  shorttitle = {{{LLTFI}}},
  booktitle = {2022 {{IEEE}} 33rd {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  author = {Agarwal, Udit Kumar and Chan, Abraham and Pattabiraman, Karthik},
  date = {2022-10},
  pages = {286--296},
  publisher = {{IEEE}},
  location = {{Charlotte, NC, USA}},
  doi = {10.1109/issre55969.2022.00036},
  url = {https://ieeexplore.ieee.org/document/9978979/},
  urldate = {2023-01-17},
  eventtitle = {2022 {{IEEE}} 33rd {{International Symposium}} on {{Software Reliability Engineering}} ({{ISSRE}})},
  isbn = {978-1-66545-132-1},
  file = {/home/alexei95/Zotero/storage/EFNMU4ZM/Agarwal et al. - LLTFI Framework Agnostic Fault Injection for Mach.pdf}
}

@online{AMDRyzenThreadripper,
  title = {{{AMD Ryzen}}™ {{Threadripper}}™ {{2990WX}} | {{AMD}}},
  url = {https://www.amd.com/en/product/7921},
  urldate = {2022-07-27},
  file = {/home/alexei95/Zotero/storage/WKZ4F4VM/7921.html}
}

@article{capraHardwareSoftwareOptimizations2020,
  title = {Hardware and {{Software Optimizations}} for {{Accelerating Deep Neural Networks}}: {{Survey}} of {{Current Trends}}, {{Challenges}}, and the {{Road Ahead}}},
  shorttitle = {Hardware and {{Software Optimizations}} for {{Accelerating Deep Neural Networks}}},
  author = {Capra, M. and Bussolino, B. and Marchisio, A. and Masera, G. and Martina, M. and Shafique, M.},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {225134--225180},
  issn = {2169-3536},
  doi = {10/ghwncn},
  url = {10/ghwncn},
  abstract = {Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.},
  eventtitle = {{{IEEE Access}}},
  keywords = {adversarial attacks,AI,area,artificial intelligence,Biological neural networks,brain-inspired models,capsule networks,CNNs,Computational modeling,computer architecture,Computer architecture,convolutional neural networks,data flow,data security,deep learning,deep neural network,deep neural networks,DNNs,efficiency,energy,Field programmable gate arrays,Hardware,hardware accelerator,hardware optimization,high energy-efficiency,latency,learning (artificial intelligence),machine learning,Machine learning,ML,neural nets,Neurons,optimization,performance,power aware computing,power consumption,security of data,SNN,software optimization,spiking neural network,spiking neural networks,Training,VLSI},
  annotation = {14 citations (Crossref) [2022-03-02] ZSCC: 0000001},
  file = {/home/alexei95/Zotero/storage/E7LC5V43/Capra_IEEE_Access_2020_Hardware_and_Software_Optimizations_for_Accelerating_Deep_Neural_Networks.pdf;/home/alexei95/Zotero/storage/3GFDTQX3/9269334.html}
}

@inproceedings{chenBinFIEfficientFault2019,
  title = {{{BinFI}}: An Efficient Fault Injector for Safety-Critical Machine Learning Systems},
  shorttitle = {{{BinFI}}},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Chen, Zitao and Li, Guanpeng and Pattabiraman, Karthik and DeBardeleben, Nathan},
  date = {2019-11-17},
  series = {{{SC}} '19},
  pages = {1--23},
  publisher = {{Association for Computing Machinery}},
  location = {{Denver, Colorado}},
  doi = {10/ggw6kg},
  url = {10/ggw6kg},
  urldate = {2020-05-25},
  abstract = {As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors. In this work, we propose BinFI, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56\% of safety-critical bits (with 99.63\% precision) in the systems, which significantly outperforms random FI, with much lower costs.},
  isbn = {978-1-4503-6229-0},
  keywords = {error resilience,fault injection,machine learning},
  annotation = {28 citations (Crossref) [2022-03-02] ZSCC: 0000004  http://web.archive.org/web/20200525225814/https://dl.acm.org/doi/10.1145/3295500.3356177},
  file = {/home/alexei95/Zotero/storage/RDJ9YHHY/Chen_Proceedings_of_the_International_Conference_for_High_Performance_Computing,_Networking,_Storage_and_Analysis_2019_BinFI.pdf}
}

@inproceedings{chenLowcostFaultCorrector2021,
  title = {A {{Low-cost Fault Corrector}} for {{Deep Neural Networks}} through {{Range Restriction}}},
  booktitle = {2021 51st {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks}} ({{DSN}})},
  author = {Chen, Zitao and Li, Guanpeng and Pattabiraman, Karthik},
  date = {2021-06},
  pages = {1--13},
  issn = {2158-3927},
  doi = {10/gm42cz},
  url = {10/gm42cz},
  abstract = {The adoption of deep neural networks (DNNs) in safety-critical domains has engendered serious reliability concerns. A prominent example is hardware transient faults that are growing in frequency due to the progressive technology scaling, and can lead to failures in DNNs. This work proposes Ranger, a low-cost fault corrector, which directly rectifies the faulty output due to transient faults without re-computation. DNNs are inherently resilient to benign faults (which will not cause output corruption), but not to critical faults (which can result in erroneous output). Ranger is an automated transformation to selectively restrict the value ranges in DNNs, which reduces the large deviations caused by critical faults and transforms them to benign faults that can be tolerated by the inherent resilience of the DNNs. Our evaluation on 8 DNNs demonstrates Ranger significantly increases the error resilience of the DNNs (by 3x to 50x), with no loss in accuracy, and with negligible overheads.},
  eventtitle = {2021 51st {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks}} ({{DSN}})},
  keywords = {Deep learning,Fault Correction,Hardware,Machine Learning,Reliability,Resilience,Transforms,Transient analysis},
  annotation = {4 citations (Crossref) [2022-03-02] ZSCC: 0000006},
  file = {/home/alexei95/Zotero/storage/AWDRM7QK/Chen_2021_51st_Annual_IEEEIFIP_International_Conference_on_Dependable_Systems_and_Networks_(DSN)_2021_A_Low-cost_Fault_Corrector_for_Deep_Neural_Networks_through_Range_Restriction.pdf;/home/alexei95/Zotero/storage/V4Y3KAI8/9505066.html}
}

@inproceedings{colucciEnpheephFaultInjection2022a,
  title = {Enpheeph: {{A Fault Injection Framework}} for {{Spiking}} and {{Compressed Deep Neural Networks}}},
  shorttitle = {Enpheeph},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Colucci, Alessio and Steininger, Andreas and Shafique, Muhammad},
  date = {2022-10},
  pages = {5155--5162},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9982181},
  abstract = {Research on Deep Neural Networks (DNNs) has focused on improving performance and accuracy for real-world deployments, leading to new models, such as Spiking Neural Networks (SNNs), and optimization techniques, e.g., quantization and pruning for compressed networks. However, the deployment of these innovative models and optimization techniques introduces possible reliability issues, which is a pillar for DNNs to be widely used in safety-critical applications, e.g., autonomous driving. Moreover, scaling technology nodes have the associated risk of multiple faults happening at the same time, a possibility not addressed in state-of-the-art resiliency analyses. Towards better reliability analysis for DNNs, we present enpheeph, a Fault Injection Framework for Spiking and Compressed DNNs. The enpheeph framework enables optimized execution on specialized hardware devices, e.g., GPUs, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools. To evaluate our enpheeph framework, we analyze the resiliency of different DNN and SNN models, with different compression techniques. By injecting a random and increasing number of faults, we show that DNNs can show a reduction in accuracy with a fault rate as low as \textsuperscript{-7\$} faults per parameter, with an accuracy drop higher than 40\%. Run-time overhead when executing enpheeph is less than 20\% of the baseline execution time when executing 100 000 faults concurrently, at least 10× lower than state-of-the-art frameworks, making enpheeph future-proof for complex fault injection scenarios. We release the source code of our enpheeph framework under an open-source license at https://github.com/Alexei95/enpheeph.},
  eventtitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Codes,Compressed Networks,Deep learning,Deep Neural Networks,Fault Injection,Licenses,Neural networks,Quantization (signal),Quantized Neural Networks,Reliability,Resiliency,Source coding,Sparse Neural Networks,Spiking Neural Networks},
  file = {/home/alexei95/Zotero/storage/YRZCTD9H/Colucci_2022_IEEERSJ_International_Conference_on_Intelligent_Robots_and_Systems_(IROS)_2022_enpheeph.pdf;/home/alexei95/Zotero/storage/PLJYS2KF/9982181.html}
}

@online{CoordinateFormatCOO,
  title = {Coordinate {{Format}} ({{COO}}) — {{Scipy}} Lecture Notes},
  url = {https://scipy-lectures.org/advanced/scipy_sparse/coo_matrix.html},
  urldate = {2022-02-28},
  file = {/home/alexei95/Zotero/storage/8DT6HY5I/coo_matrix.html}
}

@inproceedings{cupy_learningsys2017,
  title = {{{CuPy}}: {{A NumPy-Compatible}} Library for {{NVIDIA GPU}} Calculations},
  booktitle = {Proceedings of Workshop on Machine Learning Systems ({{LearningSys}}) in the Thirty-First Annual Conference on Neural Information Processing Systems ({{NIPS}})},
  author = {Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman},
  date = {2017},
  url = {http://learningsys.org/nips17/assets/papers/paper₁6.pdf},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000122},
  file = {/home/alexei95/Zotero/storage/ERHTT4IN/Okuta_Proceedings_of_workshop_on_machine_learning_systems_(LearningSys)_in_the_thirty-first_annual_conference_on_neural_information_processing_systems_(NIPS)_2017_CuPy.pdf}
}

@misc{desislavovComputeEnergyConsumption2021,
  title = {Compute and {{Energy Consumption Trends}} in {{Deep Learning Inference}}},
  author = {Desislavov, Radosvet and Martínez-Plumed, Fernando and Hernández-Orallo, José},
  date = {2021-09-12},
  number = {arXiv:2109.05472},
  eprint = {2109.05472},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.05472},
  url = {http://arxiv.org/abs/2109.05472},
  urldate = {2023-02-05},
  abstract = {The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/alexei95/Zotero/storage/I5WMLBEN/Desislavov et al_2021_Compute and Energy Consumption Trends in Deep Learning Inference.pdf;/home/alexei95/Zotero/storage/TL6NDR4I/2109.html}
}

@misc{dhamdhereHowImportantNeuron2018,
  title = {How {{Important Is}} a {{Neuron}}?},
  author = {Dhamdhere, Kedar and Sundararajan, Mukund and Yan, Qiqi},
  date = {2018-05-30},
  number = {arXiv:1805.12233},
  eprint = {1805.12233},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.12233},
  url = {http://arxiv.org/abs/1805.12233},
  urldate = {2023-01-30},
  abstract = {The problem of attributing a deep network's prediction to its \textbackslash emph\{input/base\} features is well-studied. We introduce the notion of \textbackslash emph\{conductance\} to extend the notion of attribution to the understanding the importance of \textbackslash emph\{hidden\} units. Informally, the conductance of a hidden unit of a deep network is the \textbackslash emph\{flow\} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/alexei95/Zotero/storage/4E8HNC9Q/Dhamdhere et al_2018_How Important Is a Neuron.pdf;/home/alexei95/Zotero/storage/XRD5VSH2/1805.html}
}

@article{dongSurveyDeepLearning2021,
  title = {A Survey on Deep Learning and Its Applications},
  author = {Dong, Shi and Wang, Ping and Abbas, Khushnood},
  date = {2021-05-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {40},
  pages = {100379},
  issn = {1574-0137},
  doi = {10/gjqgtn},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013721000198},
  urldate = {2023-01-23},
  abstract = {Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal—artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning.},
  langid = {english},
  keywords = {Convolutional neural network,Deep belief networks,Deep Boltzmann machine,Deep learning,Stacked auto encoder},
  file = {/home/alexei95/Zotero/storage/YTK8TMNJ/Dong et al_2021_A survey on deep learning and its applications.pdf;/home/alexei95/Zotero/storage/P25IBL3U/S1574013721000198.html}
}

@software{Falcon_PyTorch_Lightning_2019,
  title = {{{PyTorch}} Lightning},
  author = {Falcon, William and {The PyTorch Lightning team}},
  date = {2019-03},
  doi = {10.5281/zenodo.3828935},
  url = {https://github.com/PyTorchLightning/pytorch-lightning},
  version = {1.4},
  annotation = {ZSCC: NoCitationData[s1]}
}

@unpublished{falcon2020framework,
  title = {A Framework for Contrastive Self-Supervised Learning and Designing a New Approach},
  author = {Falcon, William and Cho, Kyunghyun},
  date = {2020},
  eprint = {2009.00104},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000047}
}

@online{FSDChipTesla,
  title = {{{FSD Chip}} - {{Tesla}} - {{WikiChip}}},
  url = {https://en.wikichip.org/wiki/tesla_(car_company)/fsd_chip},
  urldate = {2020-04-25},
  langid = {english},
  annotation = {http://web.archive.org/web/20200425015319/https://en.wikichip.org/wiki/tesla\_(car\_company)/fsd\_chip},
  file = {/home/alexei95/Zotero/storage/ZU9IBYBA/fsd_chip.html}
}

@inproceedings{goldsteinReliabilityEvaluationCompressed2020,
  title = {Reliability {{Evaluation}} of {{Compressed Deep Learning Models}}},
  booktitle = {2020 {{IEEE}} 11th {{Latin American Symposium}} on {{Circuits Systems}} ({{LASCAS}})},
  author = {Goldstein, Brunno F. and Srinivasan, Sudarshan and Das, Dipankar and Banerjee, Kunal and Santiago, Leandro and Ferreira, Victor C. and Nery, Alexandre S. and Kundu, Sandip and França, Felipe M. G.},
  date = {2020-02},
  pages = {1--5},
  issn = {2473-4667},
  doi = {10/ggxvpd},
  url = {10/ggxvpd},
  abstract = {Neural networks are becoming deeper and more complex, making it harder to store and process such applications on systems with limited resources. Model pruning and data quantization are two effective ways to simplify the necessary hardware by compressing the network with relevant-only nodes and reducing the required data precision. Such optimizations, however, might come at a cost of reliability since critical nodes are now more exposed to faults and the network is more sensitive to small changes. In this work, we present an extensive empirical investigation of transient faults on compressed deep convolutional neural networks (CNNs). We evaluate the impact of a single bit flip over three CNN models with different sparsity configurations and integer-only quantizations. We show that pruning can increase the resilience of the system by 9× when compared to the dense model. Quantization can outperform the 32-bit floating-point baseline by adding 27.4× more resilience to the overall network and up to 108.7× when combined with pruning. This makes model compression an effective way to provide resilience to deep learning workloads during inference, mitigating the need for explicit error correction hardware.},
  eventtitle = {2020 {{IEEE}} 11th {{Latin American Symposium}} on {{Circuits Systems}} ({{LASCAS}})},
  keywords = {32-bit floating-point baseline,CNN models,compressed deep convolutional neural networks,compressed deep learning models,Computational modeling,convolutional neural nets,Data models,data precision,data quantization,Deep Learning,error correction,explicit error correction hardware,integer-only quantizations,learning (artificial intelligence),Machine learning,Mathematical model,model compression,model pruning,Neural Network,Neural networks,optimisation,Quantization (signal),Reliability,reliability evaluation,Resilience,Soft Error,sparsity configurations,Transient Fault,transient faults},
  annotation = {2 citations (Crossref) [2022-03-02] ZSCC: 0000000},
  file = {/home/alexei95/Zotero/storage/ML6E2JCA/Goldstein et al. - 2020 - Reliability Evaluation of Compressed Deep Learning.pdf;/home/alexei95/Zotero/storage/N4ZNILVK/9069026.html}
}

@online{GraphicsReinventedNVIDIA,
  title = {Graphics {{Reinvented}}: {{NVIDIA GeForce RTX}} 2080 {{Ti Graphics Card}}},
  shorttitle = {Graphics {{Reinvented}}},
  url = {https://www.nvidia.com/en-me/geforce/graphics-cards/rtx-2080-ti/},
  urldate = {2022-07-27},
  abstract = {A revolution in gaming realism and performance. Latest NVIDIA Turing GPU architecture, 11 GB of next-gen, ultra-fast GDDR6 memory. The world’s ultimate gaming graphics card.},
  langid = {english},
  organization = {{NVIDIA}},
  file = {/home/alexei95/Zotero/storage/SHKEG9T6/rtx-2080-ti.html}
}

@article{harrisArrayProgrammingNumPy2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  options = {useprefix=true},
  date = {2020-09},
  journaltitle = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  url = {10.1038/s41586-020-2649-2},
  urldate = {2022-03-01},
  abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  issue = {7825},
  langid = {english},
  keywords = {Computational neuroscience,Computational science,Computer science,Software,Solar physics},
  annotation = {ZSCC: NoCitationData[s3]  558 citations (Crossref) [2022-03-02] ZSCC: NoCitationData[s0]},
  file = {/home/alexei95/Zotero/storage/R2J3WI2F/Harris_Nature_2020_Array_programming_with_NumPy.pdf;/home/alexei95/Zotero/storage/HWE34HEH/s41586-020-2649-2.html}
}

@inproceedings{heDeepResidualLearning2016,
  ids = {heDeepResidualLearning2016a},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  issn = {1063-6919},
  doi = {10/gdcfkn},
  url = {10/gdcfkn},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {CIFAR-10,COCO object detection dataset,COCO segmentation,Complexity theory,deep residual learning,deep residual nets,deeper neural network training,Degradation,ILSVRC & COCO 2015 competitions,ILSVRC 2015 classification task,image classification,image recognition,Image recognition,Image segmentation,ImageNet dataset,ImageNet localization,ImageNet test set,learning (artificial intelligence),neural nets,Neural networks,object detection,residual function learning,residual nets,resnet,Training,VGG nets,visual recognition tasks,Visualization},
  annotation = {45520 citations (Crossref) [2022-03-02] ZSCC: 0043557},
  file = {/home/alexei95/Zotero/storage/489UXJGY/He_2016_IEEE_Conference_on_Computer_Vision_and_Pattern_Recognition_(CVPR)_2016_Deep_Residual_Learning_for_Image_Recognition.pdf;/home/alexei95/Zotero/storage/XZZE7EXV/He_2016_IEEE_Conference_on_Computer_Vision_and_Pattern_Recognition_(CVPR)_2016_Deep_Residual_Learning_for_Image_Recognition.pdf;/home/alexei95/Zotero/storage/TXNTD7EW/7780459.html;/home/alexei95/Zotero/storage/XP7W5QQQ/7780459.html}
}

@article{hossainVisualizationBioinformaticsData2019,
  title = {Visualization of {{Bioinformatics Data}} with {{Dash Bio}}},
  author = {Hossain, Shammamah},
  date = {2019},
  journaltitle = {Proceedings of the 18th Python in Science Conference},
  pages = {126--133},
  doi = {10/grp758},
  url = {https://conference.scipy.org/proceedings/scipy2019/shammamah_hossain.html},
  urldate = {2023-01-30},
  eventtitle = {Proceedings of the 18th {{Python}} in {{Science Conference}}},
  file = {/home/alexei95/Zotero/storage/NJ4E6WUH/Hossain_2019_Visualization of Bioinformatics Data with Dash Bio.pdf}
}

@article{IEEEStandardFloatingPoint2019,
  title = {{{IEEE Standard}} for {{Floating-Point Arithmetic}}},
  date = {2019-07},
  journaltitle = {IEEE Std 754-2019 (Revision of IEEE 754-2008)},
  pages = {1--84},
  doi = {10/gnmvw4},
  abstract = {This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.},
  eventtitle = {{{IEEE Std}} 754-2019 ({{Revision}} of {{IEEE}} 754-2008)},
  keywords = {arithmetic,binary,computer,decimal,exponent,floating-point,Floating-point arithmetic,format,IEEE 754,IEEE Standards,interchange,NaN,number,rounding,significand,subnormal.},
  file = {/home/alexei95/Zotero/storage/YD5E298J/2019_IEEE Standard for Floating-Point Arithmetic.pdf;/home/alexei95/Zotero/storage/VGNP7UES/8766229.html}
}

@online{internationalorganizationforstandardizationISO2626212018,
  title = {{{ISO}} 26262-1:2018},
  shorttitle = {{{ISO}} 26262-1},
  author = {{International Organization for Standardization}},
  url = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/06/83/68383.html},
  urldate = {2021-12-31},
  abstract = {Road vehicles — Functional safety — Part 1: Vocabulary},
  langid = {english},
  organization = {{ISO}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/alexei95/Zotero/storage/3JF2X3FT/68383.html}
}

@inproceedings{jhaMLBasedFaultInjection2019,
  title = {{{ML-Based Fault Injection}} for {{Autonomous Vehicles}}: {{A Case}} for {{Bayesian Fault Injection}}},
  shorttitle = {{{ML-Based Fault Injection}} for {{Autonomous Vehicles}}},
  booktitle = {2019 49th {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks}} ({{DSN}})},
  author = {Jha, Saurabh and Banerjee, Subho and Tsai, Timothy and Hari, Siva K. S. and Sullivan, Michael B. and Kalbarczyk, Zbigniew T. and Keckler, Stephen W. and Iyer, Ravishankar K.},
  date = {2019-06},
  pages = {112--124},
  issn = {1530-0889},
  doi = {10/gg2vv2},
  url = {10/gg2vv2},
  abstract = {The safety and resilience of fully autonomous vehicles (AVs) are of significant concern, as exemplified by several headline-making accidents. While AV development today involves verification, validation, and testing, end-to-end assessment of AV systems under accidental faults in realistic driving scenarios has been largely unexplored. This paper presents DriveFI, a machine learning-based fault injection engine, which can mine situations and faults that maximally impact AV safety, as demonstrated on two industry-grade AV technology stacks (from NVIDIA and Baidu). For example, DriveFI found 561 safety-critical faults in less than 4 hours. In comparison, random injection experiments executed over several weeks could not find any safety-critical faults.},
  eventtitle = {2019 49th {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks}} ({{DSN}})},
  keywords = {accidental faults,Accidents,Autonomous Vehicles,AV development today,AV safety,AV systems,Baidu,Bayes methods,bayesian fault injection,control engineering computing,DriveFI,end-to-end assessment,Engines,fault diagnosis,Fault Injection,fully autonomous vehicles,Hardware,headline-making accidents,learning (artificial intelligence),Machine Learning,machine learning-based fault injection engine,ML-based fault injection,mobile robots,NVIDIA,public domain software,random injection experiments,realistic driving scenarios,Resilience,Safety,safety-critical faults,safety-critical software,Software,software fault tolerance},
  annotation = {24 citations (Crossref) [2022-03-02] ZSCC: 0000012},
  file = {/home/alexei95/Zotero/storage/YG23NVA8/Jha_2019_49th_Annual_IEEEIFIP_International_Conference_on_Dependable_Systems_and_Networks_(DSN)_2019_ML-Based_Fault_Injection_for_Autonomous_Vehicles.pdf;/home/alexei95/Zotero/storage/46U9SMJ3/8809495.html}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {{ICLR}} 2015, San Diego, {{CA}}, {{USA}}, May 7-9, 2015, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2015},
  url = {http://arxiv.org/abs/1412.6980},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s2]},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  file = {/home/alexei95/Zotero/storage/LH82KPNL/Kingma_3rd_international_conference_on_learning_representations,_ICLR_2015,_san_diego,_CA,_USA,_may_7-9,_2015,_conference_track_proceedings_2015_Adam.pdf}
}

@article{kloekBayesianEstimatesEquation1978,
  title = {Bayesian {{Estimates}} of {{Equation System Parameters}}: {{An Application}} of {{Integration}} by {{Monte Carlo}}},
  shorttitle = {Bayesian {{Estimates}} of {{Equation System Parameters}}},
  author = {Kloek, T. and van Dijk, H. K.},
  options = {useprefix=true},
  date = {1978},
  journaltitle = {Econometrica},
  volume = {46},
  number = {1},
  eprint = {1913641},
  eprinttype = {jstor},
  pages = {1--19},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10/dbh8p4},
  abstract = {Monte Carlo (MC) is used to draw parameter values from a distribution defined on the structural parameter space of an equation system. Making use of the prior density, the likelihood, and Bayes' Theorem it is possible to estimate posterior moments of both structural and reduced form parameters. The MC method allows a rather liberal choice of prior distributions. The number of elementary operations to be preformed need not be an explosive function of the number of parameters involved. The method overcomes some existing difficulties of applying Bayesian methods to medium size models. The method is applied to a small scale macro model. The prior information used stems from considerations regarding short and long run behavior of the model and form extraneous observations on empirical long term ratios of economic variables. Likelihood contours for several parameter combinations are plotted, and some marginal posterior densities are assessed by MC.},
  file = {/home/alexei95/Zotero/storage/GYLHXRKE/Kloek_van Dijk_1978_Bayesian Estimates of Equation System Parameters.pdf}
}

@misc{kokhlikyanCaptumUnifiedGeneric2020,
  title = {Captum: {{A}} Unified and Generic Model Interpretability Library for {{PyTorch}}},
  shorttitle = {Captum},
  author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
  date = {2020-09-16},
  number = {arXiv:2009.07896},
  eprint = {2009.07896},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.07896},
  url = {http://arxiv.org/abs/2009.07896},
  urldate = {2023-01-30},
  abstract = {In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/alexei95/Zotero/storage/CIPRMVBK/Kokhlikyan et al_2020_Captum.pdf;/home/alexei95/Zotero/storage/VKJATFZD/2009.html}
}

@inproceedings{Krizhevsky2009LearningML,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex},
  date = {2009},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0014481}
}

@article{liangPruningQuantizationDeep2021,
  title = {Pruning and Quantization for Deep Neural Network Acceleration: {{A}} Survey},
  shorttitle = {Pruning and Quantization for Deep Neural Network Acceleration},
  author = {Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  date = {2021-10-21},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {461},
  pages = {370--403},
  issn = {0925-2312},
  doi = {10/gn47pt},
  url = {10/gn47pt},
  urldate = {2022-01-17},
  abstract = {Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.},
  langid = {english},
  keywords = {Convolutional neural network,Low-bit mathematics,Neural network acceleration,Neural network pruning,Neural network quantization},
  annotation = {11 citations (Crossref) [2022-03-02] ZSCC: NoCitationData[s1]},
  file = {/home/alexei95/Zotero/storage/ALXVQR9H/Liang_Neurocomputing_2021_Pruning_and_quantization_for_deep_neural_network_acceleration.pdf;/home/alexei95/Zotero/storage/3SRCDLFD/S0925231221010894.html}
}

@inproceedings{liTensorFIConfigurableFault2018,
  title = {{{TensorFI}}: {{A Configurable Fault Injector}} for {{TensorFlow Applications}}},
  shorttitle = {{{TensorFI}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Software Reliability Engineering Workshops}} ({{ISSREW}})},
  author = {Li, Guanpeng and Pattabiraman, Karthik and DeBardeleben, Nathan},
  date = {2018-10},
  pages = {313--320},
  publisher = {{IEEE}},
  location = {{Memphis, TN}},
  doi = {10/ggq6f9},
  url = {10.1109/ISSREW.2018.00024},
  urldate = {2020-04-06},
  abstract = {Machine Learning (ML) applications have emerged as the killer applications for next generation hardware and software platforms, and there is a lot of interest in software frameworks to build such applications. TensorFlow is a high-level dataflow framework for building ML applications and has become the most popular one in the recent past. ML applications are also being increasingly used in safety-critical systems such as self-driving cars and home robotics. Therefore, there is a compelling need to evaluate the resilience of ML applications built using frameworks such as TensorFlow. In this paper, we build a high-level fault injection framework for TensorFlow called TensorFI for evaluating the resilience of ML applications. TensorFI is flexible, easy to use, and portable. It also allows ML application programmers to explore the effects of different parameters and algorithms on error resilience.},
  eventtitle = {2018 {{IEEE International Symposium}} on {{Software Reliability Engineering Workshops}} ({{ISSREW}})},
  isbn = {978-1-5386-9443-5},
  langid = {english},
  annotation = {19 citations (Crossref) [2022-03-02] ZSCC: 0000008  http://web.archive.org/web/20200528152443/https://ieeexplore.ieee.org/document/8539213/},
  file = {/home/alexei95/Zotero/storage/I2UXYFPQ/Li et al. - 2018 - TensorFI A Configurable Fault Injector for Tensor.pdf}
}

@inproceedings{lotfiResiliencyAutomotiveObject2019a,
  title = {Resiliency of Automotive Object Detection Networks on {{GPU}} Architectures},
  booktitle = {2019 {{IEEE International Test Conference}} ({{ITC}})},
  author = {Lotfi, Atieh and Hukerikar, Saurabh and Balasubramanian, Keshav and Racunas, Paul and Saxena, Nirmal and Bramley, Richard and Huang, Yanxiang},
  date = {2019-11},
  pages = {1--9},
  issn = {2378-2250},
  doi = {10/ghfck4},
  abstract = {Safety is the most important aspect of an autonomous driving platform. Deep neural networks (DNNs) play an increasingly critical role in localization, perception, and control in these systems. The object detection and classification inference are of particular importance to construct a precise picture of a vehicle's surrounding objects. Graphics Processing Units (GPU) are well-suited to accelerate such DNN-based inference applications since they leverage data and thread-level parallelism in GPU architectures. Understanding the vulnerability of such DNNs to random hardware faults (including transient and permanent faults) in GPU-based systems is essential to meet the safety requirements of auto safety standards such as the ISO 26262, as well as to influence the design of hardware and software-based safety features in current and future generations of GPU architectures and GPU-based automotive platforms. In this paper, we assess the vulnerability of object detection and classification DNNs to permanent and transient faults using fault injection experiments and accelerated neutron beam testing respectively. We also evaluate the effectiveness of chip-level safety mechanisms in GPU architectures, such as ECC and parity, in detecting these random hardware faults. Our studies demonstrate that such object detection networks tend to be vulnerable to random hardware faults, which cause incorrect or mispredicted object detection outcomes. The neutron beam experiments show that existing chip-level protections successfully mitigate all silent data corruption events caused by transient faults. For permanent faults, while ECC and parity are effective in some cases, our results suggest the need for exploring other complementary detection methods, such as periodic online and offline diagnostic testing.},
  eventtitle = {2019 {{IEEE International Test Conference}} ({{ITC}})},
  keywords = {Automotive,Functional Safety,Graphic processing unit,Object detection network,Resilience},
  file = {/home/alexei95/Zotero/storage/SMPCBLLJ/Lotfi et al_2019_Resiliency of automotive object detection networks on GPU architectures.pdf;/home/alexei95/Zotero/storage/BWVKCCIT/authors.html}
}

@inproceedings{mahmoudPyTorchFIRuntimePerturbation2020,
  title = {{{PyTorchFI}}: {{A Runtime Perturbation Tool}} for {{DNNs}}},
  shorttitle = {{{PyTorchFI}}},
  booktitle = {2020 50th {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks Workshops}} ({{DSN-W}})},
  author = {Mahmoud, Abdulrahman and Aggarwal, Neeraj and Nobbe, Alex and Vicarte, Jose Rodrigo Sanchez and Adve, Sarita V. and Fletcher, Christopher W. and Frosio, Iuri and Hari, Siva Kumar Sastry},
  date = {2020-06},
  pages = {25--31},
  issn = {2325-6664},
  doi = {10/gn7gt7},
  url = {10/gn7gt7},
  abstract = {PyTorchFI is a runtime perturbation tool for deep neural networks (DNNs), implemented for the popular PyTorch deep learning platform. PyTorchFI enables users to perform perturbations on weights or neurons of DNNs at runtime. It is designed with the programmer in mind, providing a simple and easy-to-use API, requiring as little as three lines of code for use. It also provides an extensible interface, enabling researchers to choose from various perturbation models (or design their own custom models), which allows for the study of hardware error (or general perturbation) propagation to the software layer of the DNN output. Additionally, PyTorchFI is extremely versatile: we demonstrate how it can be applied to five different use cases for dependability and reliability research, including resiliency analysis of classification networks, resiliency analysis of object detection networks, analysis of models robust to adversarial attacks, training resilient models, and for DNN interpertability. This paper discusses the technical underpinnings and design decisions of PyTorchFI which make it an easy-to-use, extensible, fast, and versatile research tool. PyTorchFI is open-sourced and available for download via pip or github at: https://github.com/pytorchfi.},
  eventtitle = {2020 50th {{Annual IEEE}}/{{IFIP International Conference}} on {{Dependable Systems}} and {{Networks Workshops}} ({{DSN-W}})},
  keywords = {Hardware,Machine learning,Neurons,Perturbation methods,Reliability,Runtime,Tools},
  annotation = {13 citations (Crossref) [2022-03-02] ZSCC: 0000016},
  file = {/home/alexei95/Zotero/storage/T9WQ5WPE/Mahmoud_2020_50th_Annual_IEEEIFIP_International_Conference_on_Dependable_Systems_and_Networks_Workshops_(DSN-W)_2020_PyTorchFI.pdf;/home/alexei95/Zotero/storage/84MG22B9/9151812.html}
}

@article{nealeNeutronRadiationInduced2016,
  title = {Neutron {{Radiation Induced Soft Error Rates}} for an {{Adjacent-ECC Protected SRAM}} in 28 Nm {{CMOS}}},
  author = {Neale, Adam and Sachdev, Manoj},
  date = {2016-06},
  journaltitle = {IEEE Transactions on Nuclear Science},
  volume = {63},
  number = {3},
  pages = {1912--1917},
  issn = {1558-1578},
  doi = {10/f82rx4},
  url = {10/f82rx4},
  abstract = {We report on measured neutron radiation induced soft error rates (SER) for an academic, full-custom, low-power 75 kb SRAM macro in a commercial 28 nm CMOS process protected with an adjacent-bit error correction circuit (ECC). In low voltage (0.5 V) power saving mode, measurements show an 189x improvement in SER over an unprotected baseline array and a 5x improvement over a traditional single-error-correcting-double-error-detecting (SEC-DED) code using the same number of parity check-bits. Further, simulation results show an over 2x reduction in SER compared to other multi-bit ECCs while using three fewer check-bits. Chip measurement results show that the low voltage retention mode provides a 89.7\% reduction in cell leakage current/bit compared to the nominal supply voltage. All radiation measurement data has been collected at the TRIUMF Neutron Irradiation Facility in Vancouver, Canada.},
  eventtitle = {{{IEEE Transactions}} on {{Nuclear Science}}},
  keywords = {Adjacent-bit upsets,CMOS integrated circuits,Error analysis,error correction circuits,Error correction codes,multi-cell upsets,neutron radiation effects,Neutrons,Particle beams,Radiation effects,Random access memory,soft error rate,SRAM},
  annotation = {9 citations (Crossref) [2022-03-02] ZSCC: 0000009},
  file = {/home/alexei95/Zotero/storage/LCSQHDTA/7497732.html}
}

@article{owidtechnologicalchange,
  title = {Technological Change},
  author = {Roser, Max and Ritchie, Hannah and Mathieu, Edouard},
  date = {2013},
  journaltitle = {Our World in Data},
  keywords = {⛔ No DOI found}
}

@incollection{paszkePyTorchImperativeStyle2019,
  ids = {paszkePyTorchImperativeStyle2019a},
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché- Buc, F. and Fox, E. and Garnett, R.},
  options = {useprefix=true},
  date = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/alexei95/Zotero/storage/6MMFQ3Z6/Paszke_Advances_in_neural_information_processing_systems_32_2019_PyTorch.pdf}
}

@inproceedings{portierAsymptoticOptimalityAdaptiveImportanceSampling,
  title = {Asymptotic Optimality of Adaptive Importance Sampling},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Portier, François and Delyon, Bernard},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/file/1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf},
  keywords = {⛔ No DOI found},
  file = {/home/alexei95/Zotero/storage/2Z9VNQBC/Portier_Delyon_2018_Asymptotic optimality of adaptive importance sampling.pdf;/home/alexei95/Zotero/storage/5FBJ6X2A/NeurIPS-2018-asymptotic-optimality-of-adaptive-importance-sampling-Supplemental.zip}
}

@software{PyTorchLightningLightningflash2022,
  title = {{{PyTorchLightning}}/Lightning-Flash},
  date = {2022-02-25T13:41:32Z},
  origdate = {2021-01-28T18:47:16Z},
  url = {https://github.com/PyTorchLightning/lightning-flash},
  urldate = {2022-02-26},
  abstract = {Your PyTorch AI Factory - Flash enables you to easily configure and run complex AI recipes for over 15 tasks across 7 data domains},
  organization = {{Pytorch Lightning}},
  keywords = {classification,deep-learning,fiftyone,icevision,machine-learning,object-detection,open3d,pytorch,pytorch-lightning,pytorch-video,tabular-data,tasks-flash,torch-geometric}
}

@inproceedings{sevillaComputeTrendsThree2022a,
  title = {Compute {{Trends Across Three Eras}} of {{Machine Learning}}},
  booktitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  date = {2022-07},
  pages = {1--8},
  issn = {2161-4407},
  doi = {10/grpnwh},
  abstract = {Compute, data, and algorithmic advances are the three fundamental factors that drive progress in modern Machine Learning (ML). In this paper we study trends in the most readily quantified factor - compute. We make three novel contributions: (1) we curate a dataset with the training compute of 123 milestone ML systems, 3× larger than previous such datasets. (2) We frame the trends in compute in in three eras - the Pre Deep Learning Era, the Deep Learning Era, and the Large-Scale Era, based on our identification of a novel trend emerging around 2015. (3) We find a Deep Learning Era compute doubling time of around 6 months, significantly longer than previous findings. Overall, our work highlights the fast-growing compute requirements for training advanced ML systems.},
  eventtitle = {2022 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {AI accelerators,artificial intelligence,backpropagation,computational efficiency,Computational modeling,deep learning,Deep learning,high performance computing,History,machine learning,Machine learning algorithms,Market research,Neural networks,Training},
  file = {/home/alexei95/Zotero/storage/CTEBBMQF/Sevilla et al_2022_Compute Trends Across Three Eras of Machine Learning.pdf;/home/alexei95/Zotero/storage/U79NNS2A/9891914.html}
}

@misc{shrikumarComputationallyEfficientMeasures2018,
  title = {Computationally {{Efficient Measures}} of {{Internal Neuron Importance}}},
  author = {Shrikumar, Avanti and Su, Jocelin and Kundaje, Anshul},
  date = {2018-07-25},
  number = {arXiv:1807.09946},
  eprint = {1807.09946},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.09946},
  url = {http://arxiv.org/abs/1807.09946},
  urldate = {2023-01-30},
  abstract = {The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a "natural refinement of Integrated Gradients" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/alexei95/Zotero/storage/B7C9E395/Shrikumar et al_2018_Computationally Efficient Measures of Internal Neuron Importance.pdf;/home/alexei95/Zotero/storage/6LQ2Q948/1807.html}
}

@article{silverGeneralReinforcementLearning2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018-12-07},
  journaltitle = {Science},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10/cxq3},
  url = {10/cxq3},
  urldate = {2022-01-26},
  abstract = {AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.},
  langid = {english},
  annotation = {778 citations (Crossref) [2022-03-02] ZSCC: 0002077},
  file = {/home/alexei95/Zotero/storage/6RJ9FTVH/Silver_Science_2018_A_general_reinforcement_learning_algorithm_that_masters_chess,_shogi,_and_Go.pdf;/home/alexei95/Zotero/storage/JVZXKLN9/science.html}
}

@inproceedings{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2015},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2022-02-28},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: 0000347},
  file = {/home/alexei95/Zotero/storage/PKA7WSP6/Simonyan_arXiv1409.1556_[cs]_2015_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition.pdf;/home/alexei95/Zotero/storage/PVGGV56C/Simonyan_International_conference_on_learning_representations_2015_Very_deep_convolutional_networks_for_large-scale_image_recognition.pdf;/home/alexei95/Zotero/storage/6JSCJG7H/1409.html}
}

@article{stallkampManVsComputer2012,
  title = {Man vs. Computer: {{Benchmarking}} Machine Learning Algorithms for Traffic Sign Recognition},
  shorttitle = {Man vs. Computer},
  author = {Stallkamp, J. and Schlipsing, M. and Salmen, J. and Igel, C.},
  date = {2012-08-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  series = {Selected {{Papers}} from {{IJCNN}} 2011},
  volume = {32},
  pages = {323--332},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.02.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608012000457},
  urldate = {2022-12-23},
  abstract = {Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons.},
  langid = {english},
  keywords = {Benchmarking,Convolutional neural networks,Machine learning,Traffic sign recognition},
  file = {/home/alexei95/Zotero/storage/Y9TMUS5H/Stallkamp_Neural_Networks_2012_Man_vs.pdf;/home/alexei95/Zotero/storage/U9HTQMHB/S0893608012000457.html}
}

@misc{sunSummarizingCPUGPU2020,
  title = {Summarizing {{CPU}} and {{GPU Design Trends}} with {{Product Data}}},
  author = {Sun, Yifan and Agostini, Nicolas Bohm and Dong, Shi and Kaeli, David},
  date = {2020-07-13},
  number = {arXiv:1911.11313},
  eprint = {1911.11313},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.11313},
  url = {http://arxiv.org/abs/1911.11313},
  urldate = {2023-02-05},
  abstract = {Moore's Law and Dennard Scaling have guided the semiconductor industry for the past few decades. Recently, both laws have faced validity challenges as transistor sizes approach the practical limits of physics. We are interested in testing the validity of these laws and reflect on the reasons responsible. In this work, we collect data of more than 4000 publicly-available CPU and GPU products. We find that transistor scaling remains critical in keeping the laws valid. However, architectural solutions have become increasingly important and will play a larger role in the future. We observe that GPUs consistently deliver higher performance than CPUs. GPU performance continues to rise because of increases in GPU frequency, improvements in the thermal design power (TDP), and growth in die size. But we also see the ratio of GPU to CPU performance moving closer to parity, thanks to new SIMD extensions on CPUs and increased CPU core counts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/alexei95/Zotero/storage/SM8TD7D2/Sun et al_2020_Summarizing CPU and GPU Design Trends with Product Data.pdf;/home/alexei95/Zotero/storage/66RMSS95/1911.html}
}

@article{szeEfficientProcessingDeep2020,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  date = {2020-06-16},
  journaltitle = {Synthesis Lectures on Computer Architecture},
  shortjournal = {Synthesis Lectures on Computer Architecture},
  volume = {15},
  number = {2},
  pages = {1--341},
  issn = {1935-3235, 1935-3243},
  doi = {10/fbtn},
  url = {10.2200/S01004ED1V01Y202004CAC050},
  urldate = {2022-01-17},
  langid = {english},
  annotation = {35 citations (Crossref) [2022-03-02] ZSCC: 0000066},
  file = {/home/alexei95/Zotero/storage/DILAYKFX/Sze_Synthesis_Lectures_on_Computer_Architecture_2020_Efficient_Processing_of_Deep_Neural_Networks.pdf}
}

@software{the_pandas_development_team_2023_7549438,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {pandas development {team}, The},
  date = {2023-01},
  doi = {10.5281/zenodo.7549438},
  url = {https://doi.org/10.5281/zenodo.7549438},
  organization = {{Zenodo}},
  version = {v1.5.3}
}

@article{tunyasuvunakoolHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction for the Human Proteome},
  author = {Tunyasuvunakool, Kathryn and Adler, Jonas and Wu, Zachary and Green, Tim and Zielinski, Michal and Žídek, Augustin and Bridgland, Alex and Cowie, Andrew and Meyer, Clemens and Laydon, Agata and Velankar, Sameer and Kleywegt, Gerard J. and Bateman, Alex and Evans, Richard and Pritzel, Alexander and Figurnov, Michael and Ronneberger, Olaf and Bates, Russ and Kohl, Simon A. A. and Potapenko, Anna and Ballard, Andrew J. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Clancy, Ellen and Reiman, David and Petersen, Stig and Senior, Andrew W. and Kavukcuoglu, Koray and Birney, Ewan and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
  date = {2021-08},
  journaltitle = {Nature},
  volume = {596},
  number = {7873},
  pages = {590--596},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10/gk9kp7},
  url = {10/gk9kp7},
  urldate = {2022-01-26},
  abstract = {Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17\% of the total residues in human protein sequences are covered by an experimentally determined structure1. Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold2, at a scale that covers almost the entire human proteome (98.5\% of human proteins). The resulting dataset covers 58\% of residues with a confident prediction, of which a subset (36\% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions that are likely to be disordered. Finally, we provide some case studies to illustrate how high-quality predictions could be used to generate biological hypotheses. We are making our predictions freely available to the community and anticipate that routine large-scale and high-accuracy structure prediction will become an important tool~that will allow new questions to be addressed from a structural perspective.},
  issue = {7873},
  langid = {english},
  keywords = {Machine learning,Protein structure predictions,Proteomic analysis,Structural biology},
  annotation = {315 citations (Crossref) [2022-03-02] ZSCC: NoCitationData[s1]  Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Machine learning;Protein structure predictions;Proteomic analysis;Structural biology Subject\_term\_id: machine-learning;protein-structure-predictions;proteomic-analysis;structural-biology},
  file = {/home/alexei95/Zotero/storage/KIH82AEF/Tunyasuvunakool_Nature_2021_Highly_accurate_protein_structure_prediction_for_the_human_proteome.pdf;/home/alexei95/Zotero/storage/KK8QMK42/s41586-021-03828-1.html}
}

@misc{villalobosMachineLearningModel2022,
  title = {Machine {{Learning Model Sizes}} and the {{Parameter Gap}}},
  author = {Villalobos, Pablo and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Ho, Anson and Hobbhahn, Marius},
  date = {2022-07-05},
  number = {arXiv:2207.02852},
  eprint = {2207.02852},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.02852},
  url = {http://arxiv.org/abs/2207.02852},
  urldate = {2023-02-05},
  abstract = {We study trends in model size of notable machine learning systems over time using a curated dataset. From 1950 to 2018, model size in language models increased steadily by seven orders of magnitude. The trend then accelerated, with model size increasing by another five orders of magnitude in just 4 years from 2018 to 2022. Vision models grew at a more constant pace, totaling 7 orders of magnitude of growth between 1950 and 2022. We also identify that, since 2020, there have been many language models below 20B parameters, many models above 70B parameters, but a scarcity of models in the 20-70B parameter range. We refer to that scarcity as the parameter gap. We provide some stylized facts about the parameter gap and propose a few hypotheses to explain it. The explanations we favor are: (a) increasing model size beyond 20B parameters requires adopting different parallelism techniques, which makes mid-sized models less cost-effective, (b) GPT-3 was one order of magnitude larger than previous language models, and researchers afterwards primarily experimented with bigger models to outperform it. While these dynamics likely exist, and we believe they play some role in generating the gap, we don't have high confidence that there are no other, more important dynamics at play.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/alexei95/Zotero/storage/PIFVW9UY/Villalobos et al_2022_Machine Learning Model Sizes and the Parameter Gap.pdf;/home/alexei95/Zotero/storage/WLGPY58M/2207.html}
}

@misc{xiaoFashionMNISTNovelImage2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  date = {2017-09-15},
  number = {arXiv:1708.07747},
  eprint = {1708.07747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1708.07747},
  url = {http://arxiv.org/abs/1708.07747},
  urldate = {2023-01-31},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/alexei95/Zotero/storage/8I94UV5P/Xiao et al_2017_Fashion-MNIST.pdf;/home/alexei95/Zotero/storage/XEEA3YP6/1708.html}
}

@article{zhangEnablingTimingError2020,
  title = {Enabling {{Timing Error Resilience}} for {{Low-Power Systolic-Array Based Deep Learning Accelerators}}},
  author = {Zhang, Jeff and Ghodsi, Zahra and Garg, Siddharth and Rangineni, Kartheek},
  date = {2020-04},
  journaltitle = {IEEE Design \& Test},
  volume = {37},
  number = {2},
  pages = {93--102},
  issn = {2168-2364},
  doi = {10/grqt54},
  abstract = {Hardware-accelerated learning and inference algorithms are quite popular in edge devices where predictable timing behavior and minimal energy consumption are required, while maintaining robustness to timing errors. To achieve this, dynamic voltage scaling techniques have been utilized in several accelerators. Therefore, this article presents Thundervolt, a framework allowing adaptive aggressive voltage underscaling while maintaining the robustness (reliability, predictability, performance) of such accelerators.},
  eventtitle = {{{IEEE Design}} \& {{Test}}},
  keywords = {Clocks,Computer architecture,Deep learning,Deep Neural Network,Energy Efficiency,Error analysis,Hardware Accelerator,Neural networks,Speech recognition,Systolic Arrays,Timing,Timing Error,Timing Speculation},
  file = {/home/alexei95/Zotero/storage/8BIX33BC/Zhang et al_2020_Enabling Timing Error Resilience for Low-Power Systolic-Array Based Deep.pdf;/home/alexei95/Zotero/storage/AG526DSJ/authors.html}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }
