\section{Introduction}
\label{section:introduction}

In recent years, \titlecaseabbreviation{dl} has become common in many fields, such as medical imaging, autonomous driving and voice recognition \cite{dongSurveyDeepLearning2021}, requiring specialized hardware accelerators and chips \cite{szeEfficientProcessingDeep2020}.
In the nano-era, error resilience has increased in importance as technology scaling leads to more faults happening per transistor \cite{nealeNeutronRadiationInduced2016}, and newer applications require higher safety standard than before \cite{internationalorganizationforstandardizationISO2626212018}. Companies have tried to approach error resilience \cite{lotfiResiliencyAutomotiveObject2019a,zhangEnablingTimingError2020}, however, it is difficult to strike a balance between efficiency and error resilience, as shown in examples like \cite{FSDChipTesla}, where the implemented solution is the expensive \titlecaseabbreviation{dmr}.
Therefore, many research works have focused on improving error analysis tools and mitigation techniques. One of such analysis tools is fault injection, purposely injecting faults in the execution of \titlecaseabbreviationpl{dnn} to verify whether the injected fault leads to an error, and develop appropriate mitigation techniques. Many \gls{sota} works have developed tools to increase fault injection simulation performance \cite{liTensorFIConfigurableFault2018,mahmoudPyTorchFIRuntimePerturbation2020,colucciEnpheephFaultInjection2022a,agarwalLLTFIFrameworkAgnostic2022}, or to decrease the fault search space \cite{chenBinFIEfficientFault2019,jhaMLBasedFaultInjection2019}, and finally to build on these advancements to generate new mitigation techniques \cite{chenLowcostFaultCorrector2021}.
Decreasing the fault search space will speed up fault injection simulations, while making the error model less accurate and effective, as faults which have been discarded might be leading to errors. This is shown in \gls{sota}, as they achieve only \num{80}\% precision for critical faults, a measure of the efficacy of the fault sampling\footnotemark, while still requiring hundreds of thousands of faults, instead of quadrillions in the whole search space \cite{chenBinFIEfficientFault2019,jhaMLBasedFaultInjection2019}.
Additionally, current state-of-the-art fault-sampling models reduce the fault search space by employing human-made fault models, hence requiring the external a-priori knowledge to select the fault search space.
\emphasizedworkname{} aims to increase the effectiveness and efficiency of the fault injection campaign while still covering the whole fault search space over long-running simulations.

% use footnotetext if putting a ref in it
\footnotetext{Precision and recall, two important metrics for measuring efficacy and effectiveness of fault injection, will be mathematically defined and explained in Section \ref{section:experimental_setup:subsection:metrics}}



\subsection{Motivational Case Study}
\label{section:introduction:subsection:motivationalcasestudy}

% \begin{figure}[h]
%     \centering
%     \begin{minipage}{.54\linewidth}
%         \includegraphics[width=\linewidth]{images/vector/exported-pdf/motivational_case_study_plot.pdf}
%         \caption{Trend of number of \titlecaseabbreviation{gpu} cycles required to perform a complete fault injection campaign. The trend is exponential, increasing by one order of magnitude every 2 years.}
%         \label{figure:motivational_case_study:subfigure:quantitative_campaign_duration}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.44\linewidth}
%         \includegraphics[width=\linewidth]{images/vector/exported-pdf/motivational_case_study.pdf}
%         \caption{Qualitative comparison among \gls{sota}, random sampling and \emphasizedworkname{}. \gls{sota} reaches higher precision but it does not cover all the possible faults, random sampling has very low precision but can cover all the faults, while \emphasizedworkname{} has very high precision with the first few samples while reaching random sampling level of coverage, merging the best of both worlds.}
%         \label{figure:motivational_case_study:subfigure:qualitative_coverage_precision_plot}
%     \end{minipage}  
% \end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/vector/exported-pdf/motivational_case_study_plot.pdf}
    \caption{Trend of number of \titlecaseabbreviation{gpu} cycles required to perform a complete fault injection campaign. The trend is exponential, increasing by more than one order of magnitude every 2 years.}
    \label{figure:motivational_case_study:subfigure:quantitative_campaign_duration}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/vector/exported-pdf/motivational_case_study.pdf}
    \caption{Qualitative comparison among \gls{sota}, random sampling and \emphasizedworkname{}. \gls{sota} reaches higher precision but it does not cover all the possible faults, random sampling has very low precision but can cover all the faults, while \emphasizedworkname{} has very high precision with the first few samples while reaching random sampling level of coverage, merging the best of both worlds.}
    \label{figure:motivational_case_study:subfigure:qualitative_coverage_precision_plot}
\end{figure}

In Figure \ref{figure:motivational_case_study:subfigure:quantitative_campaign_duration}, we merge the latest trends in terms of \titlecaseabbreviation{dnn} parameter count \cite{villalobosMachineLearningModel2022} and required execution \titlecaseabbreviation{flops} \cite{desislavovComputeEnergyConsumption2021}, hardware transistor count \cite{sunSummarizingCPUGPU2020,owidtechnologicalchange} and generated hardware \titlecaseabbreviation{flops} \cite{sunSummarizingCPUGPU2020}, to show the approximate number of \titlecaseabbreviation{gpu} cycles required to perform a full injection campaign on \gls{sota} \titlecaseabbreviation{dnn} models. We can see the exponential trend, increasing by more than one order of magnitude every 2 years. \emph{This highlights the necessity of efficiently selecting the faults to be injected, as it is infeasible to analyze all the possible faults.} 

In Figure \ref{figure:motivational_case_study:subfigure:qualitative_coverage_precision_plot}, we show a comparison between \gls{sota} against random fault sampling. We can see how \titlecaseabbreviation{sota} reaches high precision quickly, around \num{80}\%, however, it covers a very small percentage of the whole fault search space, roughly \num{20}\%. On the other hand, random fault sampling covers all the reachable fault search space, but it has very low precision as most of the faults do not lead to errors. Therefore, we need a new fault-sampling model to achieve very high precision with the first few samples, but still capable of covering the remaining fault search space if required, without using any external a-priori knowledge. \emph{\workname{} manages to achieve this balance, reaching up to \num{60}\% precision with less than \num{100} faults.}

\subsection{Research Questions}
\label{section:introduction:subsection:research_questions}

The aforementioned case study leads to the following research questions:

\begin{itemize}
    \item How can we provide a fault model which is as precise as state-of-the-art for the initial samples, while covering the whole search space, and without requiring a-priori knowledge?
    \item How do the different importance sampling algorithms compare? Which one is best suited to achieve \gls{sota} performance?
    \item How can importance sampling algorithms benefit specific resilience applications, e.g., \titlecaseabbreviation{fat}?
\end{itemize}

\subsection{Novel Contributions}
\label{section:introduction:subsection:novel_contributions}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/vector/exported-pdf/novel_contributions.pdf}
    \caption{Novel contributions presented in \emphasizedworkname{}.}
    \label{figure:novel_contributions}
\end{figure}

To answer the research questions, we provide the following novel contributions in \emphasizedworkname{}, as shown in Figure \ref{figure:novel_contributions}:

\begin{itemize}
    \item we develop and implement an importance sampling algorithm for computing the importance of both neuron weights and neuron outputs (Section \ref{section:methodology:subsection:attribution_computation:subsubsection:neuron_output} \& \ref{section:methodology:subsection:attribution_computation:subsubsection:neuron_weight});
    \item we develop and implement importance sampling attribution for tensor bits (Section \ref{section:methodology:subsection:attribution_computation:subsubsection:tensor_bit});
    \item we test \emphasizedworkname{} on \gls{sota} \titlecaseabbreviationpl{dnn}, VGG11 and ResNet18, trained on different datasets, CIFAR10 and GTSRB, comparing it with random sampling (Section \ref{section:evaluation:subsection:attribution_efficacy} \& \ref{section:evaluation:subsection:different_configurations});
    \item we employ \emphasizedworkname{} to speed up a possible use-case of fault injection, \titlecaseabbreviation{fat} (Section \ref{section:evaluation:subsection:fault_aware_training}).
\end{itemize}

\subsection{Paper Organization}

After presenting background information in Section \ref{section:background} and \gls{sota} in Section \ref{section:related_work}, we show the methodology of \emphasizedworkname{} in Section \ref{section:methodology}, as well as the setup used for the experiments in Section \ref{section:experimental_setup}. We finally present the results and the comparisons with \gls{sota} and random sampling in Section \ref{section:evaluation}, before drawing the conclusions in Section \ref{section:conclusion}.


% \begin{itemize}
%     \item In recent years, deep learning has become common in many applications \cite{dongSurveyDeepLearning2021a}
%     \item However, focus has been mostly on improving performance \cite{sevillaComputeTrendsThree2022} than error resilience
%     \item Resiliency to transient faults is of paramount importance \cite{nealeNeutronRadiationInduced2016,internationalorganizationforstandardizationISO2626212018}, but it is not efficient enough to be easily approachable \cite{FSDChipTesla}
%     \item Hence, many works have tried improving the performance of different fault analysis and compensation methods, by improving the tools \cite{liTensorFIConfigurableFault2018,mahmoudPyTorchFIRuntimePerturbation2020,colucciEnpheephFaultInjection2022a,agarwalLLTFIFrameworkAgnostic2022}, the models \cite{chenBinFIEfficientFault2019,jhaMLBasedFaultInjection2019}, and the hardware mitigations \cite{chenLowcostFaultCorrector2021}
%     \item This work focuses on further improving the fault models used to assert whether a specific fault might lead to an error
%     \item The aforementioned state-of-the-art fault models focus on removing specific fault patterns out of the search area, to analyze only the error-inducing faults
%     \item However, these approaches might leave out some faults in the process, and they cannot be used for long-term executions, as they would not cover all the possible faults in the system
%     \begin{itemize}
%         \item Additionally, these approaches require specific knowledge of the model-under-tests and extra human-in-the-loop, hence they are difficult to automate
%         \item Therefore, this work aims to provide an enhanced fault model which is model-agnostic, providing a proper speed-up in finding the error-inducing faults, while at the same time allowing for longer executions to cover all the possible fault combinations of the model
%     \end{itemize}
%     \item This is useful for many application which rely on fault analysis to improve their algorithms, e.g. fault-aware training
%     \item Motivational Case Study
%     \begin{itemize}
%         \item State-of-the-art has very low precision (random sampling) or high coverage of a limited set of faults
%         \item We are going in between them FIGURE IF WE HAVE TIME
%         \begin{itemize}
%             \item Figure can be showing qualitative precision (y) and speed to reach it, against total search space (x), TO BE DONE IN PLOTLY
%             \item State-of-the-art would be very limited on x, with straight path and then stopping, starting from 50\% or so
%             \item We start at very high value but then fall down to random levels
%             \item Random is almost a straight horizontal line, growing to steady-state from zero with slow pace
%         \end{itemize}
%     \end{itemize}
%     \item Research Questions
%     \begin{itemize}
%         \item How can we provide a fault model which is as efficient as state-of-the-art, while covering the whole search space?
%         \item How do the different importance algorithms compare? How many are needed to achieve state-of-the-art performance?
%         \item How do applications of importance sampling compare to random sampling, e.g. fault-aware training?
%     \end{itemize}
%     \item Novel Contributions (MAYBE FIGURE WITH NOVEL CONTRIBUTIONS? NO, BETTER FOCUS ON MOTIVATIONAL CASE STUDY)
%     \begin{itemize}
%         \item Algorithm based on layer conductance to speed up fault injection experiments
%         \item Faster convergence and wider coverage than random sampling
%         \item Use case with fault-aware training to compare random and importance sampling
%     \end{itemize}
%     \item \blue{add section references here}
% \end{itemize}


% In the last decade, \titlecaseabbreviationpl{dnn} have seen an exponential increase in practical applications~\cite{abiodunStateoftheartArtificialNeural2018,dongSurveyDeepLearning2021}, due to their ability to learn complex patterns beyond classical hard-coded algorithms. A possible application is autonomous driving, which is becoming more prominent at different capability levels~\cite{on-roadautomateddrivingoradcommitteeTaxonomyDefinitionsTerms}. However, strong error-tolerance and resiliency are required to reach high autonomous capability levels, as detailed in ISO 26262~\cite{internationalorganizationforstandardizationISO26262102018}, which indicates a \titlecaseabbreviation{fit} rate of fewer than 100 failures in 1 billion hours of operation for the highest safety level. However, the exponential increase of multiple upset events in advanced technology nodes~\cite{blackPhysicsMultipleNodeCharge2013,nealeNeutronRadiationInduced2016}, makes this threshold complex to achieve and maintain.

% Even though resiliency to faults and errors is of foremost importance, there have been few in-depth resiliency analyses for the effect of faults on \glspl{dnn}. Some examples focus on permanent faults~\cite{zhangThundervoltEnablingAggressive2018,ozenBoostingBitErrorResilience2020,reagenAresFrameworkQuantifying2018}, while others only consider \titlecaseabbreviationpl{cnn}~\cite{liUnderstandingErrorPropagation2017,liTensorFIConfigurableFault2018,chenBinFIEfficientFault2019,chenLowcostFaultCorrector2021}. These tools focus on analyzing a single fault happening at a certain time inside the model, a fault model which is bound to be superseded by multiple fault events due to the aforementioned technology scaling. Hence, state-of-the-art tools are not optimized for scalability, making it very difficult to inject multiple faults in the models without affecting the run-time.

% Additionally, many new techniques and architectures for improving \gls{dnn} efficiency have been developed, such as quantization~\cite{hanLearningBothWeights2015}, pruning~\cite{hanLearningBothWeights2015} or \titlecaseabbreviationpl{snn}~\cite{maassNoisySpikingNeurons1996,maassNetworksSpikingNeurons1997}, making them more challenging to analyze using traditional methodologies. As state-of-the-art tools are tailored to specific platform/model configurations, it proves that it is difficult to quickly adapt them to the constantly-evolving model space and optimization techniques.

% \subsection{Motivational Case Study}

% We show a comparison of run-time overhead when using different state-of-the-art fault injection frameworks in Fig.~\ref{figure:motivation}. By running from 1 to \numprint{100000} injections, we can see how much overhead is incurred when using multiple frameworks, which grows exponentially. Hence, using these frameworks for multiple faults makes injection experiments very slow, which affects the  system design phase.
% Additionally, these frameworks are not easily adaptable to new technologies or different deep learning libraries, as their code is tied to the framework on which they are implemented.
% Hence, we can see how a scalable and adaptable framework is necessary for making resiliency analysis of \glsxtrshortpl{dnn} future-proof.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{images/proprietary/PowerPoint/exported-pdf/Motivation.pdf}
%     \caption{Comparative analysis of run-time overhead between different tools. We can note the exponential growth pattern against the number of concurrently injected faults.
%     }
%     \label{figure:motivation}
% \end{figure}

% \subsection{Research Questions}

% The aforementioned case study leads us to formulate the following research questions:

% \begin{itemize}
%     \item How can we maintain performance when executing multiple fault injections simultaneously?
%     \item How can we develop a generic fault injection framework capable of adapting to different models with minimal modifications?
%     \item How can we carry out the resiliency analysis for \glspl{snn} and compressed \glspl{dnn}?
% \end{itemize}

% \subsection{Novel Contributions}

% To answer the research questions, we provide the following novel contributions:

% \begin{itemize}
%     \item we develop \emphasis{enpheeph}, a modern fault injection framework, capable of handling multiple fault injections with minimum overhead, and adaptable to all models and configurations with minimal-to-none modifications;
%     \item we employ \emphasis{enpheeph} to analyze the resiliency of different \glspl{dnn}, as well as \gls{snn} for gesture recognition, employing different compression techniques;
% \end{itemize}

% After a brief background and related work analysis, in Sections~\ref{section:background} and~\ref{section:related_work}, we discuss the methodology and the implementation behind our \emphasis{enpheeph} framework in Section~\ref{section:methodology}. Then, we show our experimental setup in Section~\ref{section:experimental_setup}, and analyze the fault injection results in Section~\ref{section:evaluation}. We draw the conclusion on our work in Section~\ref{section:conclusion}.
 


% Some examples in this direction are Spiking Neural Networks , which are the third version of neural networks, allowing temporal coding in addition to the spatial one.
% New techniques have also been developed to make the neural networks more efficient in terms of memory and computational latency [INTEGER-ONLY INFERENCE QUANTIZATION][FIXED QUANTIZATION CNNS][LEARNING WEIGHTS AND CONNECTIONS], such as quantization and pruning.
% Quantization reduces the number of bits used for storing the tensors, reducing memory use and the computational latency.
% Pruning and sparse tensors, on the other hand, remove excess connections and represent the remaining ones in a sparse fashion, reducing memory usage as well as reducing computational latency if there is support for sparse operations.

% Current \gls{sota} research focuses on different aspects of reliability: [AVFI][BINFI] focus on practical applications, while neglecting compressed and spiking networks; [ARES][PATTABIRAMAN KECLER UNDERSTANDING] analyze the impact of compression on \gls{dnn} reliability, however their tools are not optimized for fast execution, and achieve high coverage by algorithmic simplication \red{????????}; \red{MISSING METRIC FOR RESULT COMPARISON}

% \subsection{Motivational Case Study}

% \begin{itemize}
%     \item Compare sota in terms of execution time and required resources/customizability, a small version of the related works table
%     \item Analysis on why these things are limiting factor, since there are quadrillion of fault possibilities which need to be analyzed
% \end{itemize}

% Field and use-cases:
% \begin{itemize}
%     \item deep learning (also spiking) for autonomous driving, which is susceptible to errors induced by faults, and for which safety requirements can be very high (ISO 26262:2018);
%     \item IoT- and edge-environments for image recognition applications, requiring compressed deep neural networks, which are more power- and memory-efficient while maintaining high accuracy;
% \end{itemize}

% Major research problems in achieving use-cases:
% \begin{itemize}
%     \item fault injection campaigns must be fast and customizable to bit-level precision;
%     \item full customizability to analyze different fault patterns
% \end{itemize}

% Main target research problem, why it is worthy to investigate:
% \begin{itemize}
%     \item improve fault injection campaigns;
%     \item reliability analysis of compressed deep neural networks, which are becoming wide-spread in low-power applications;
%     \item reliability analysis for spiking neural networks;
% \end{itemize}

% \subsection{Major State-of-the-Art and Their Limitations}

% List of different research bodies with scope and limitations:
% \begin{itemize}
%     \item fault injection frameworks for deep learning (\cite{reagenAresFrameworkQuantifying2018}, \cite{liTensorFIConfigurableFault2018}, \cite{chenBinFIEfficientFault2019}), [REF INJECTTF2 AND INJECTTF, TORCHFI] do not focus on compressed networks, and target specific operations in each layer; additionally they do not use different available accelerators, such as GPU/TPU, while our framework is capable of doing so;
%     % \item hardware-aware fault injection has been addressed in some works (\cite{pattabiramanSymPLFIEDSymbolicProgramlevel2008}, \cite{rehmanReliabilityDrivenSoftwareTransformations2014}), however the focus is mostly on standard processing architectures (CPUs), neglecting specialized accelerators (GPUs, ASICs);
%     % \item other frameworks for GPU-based fault injection exist (\cite{fangGPUQinMethodologyEvaluating2014}, \cite{hariSASSIFIArchitecturelevelFault2017}, \cite{nieFaultSitePruning2018}), but they do not focus on deep learning applications, and their approach to fault injection is from the hardware perspective at execution time, neglecting the modeling and application aspects;
% \end{itemize}

% \subsection{Key Scientific Challenges}

% List of open challenges, why they are not addressed by SOTA:
% \begin{itemize}
%     \item customizability and support different libraries, as they are difficult to support properly across different versions;
%     \item compressed neural networks have very broad definitions, leading to difficult high-level modelization and high injection efficiency;
% \end{itemize}

% \subsection{Motivational Case Study}

% Quantitative case study: comparison of execution time/memory for state-of-the-art frameworks (TensorFI, BinFI), which follow non-ideal trends (execution time for TensorFI increases exponentially, coverage is hardware-independent)

% Analysis of reproducibility with comparison table of functionality

% \subsection{Novel Contributions \& Key Results}

% List of key scientific contributions:
% \begin{itemize}
%     \item framework using hardware-aware fault injection for deep learning, providing a customizable environment for injecting faults based on different hardware and injected particle properties;
%     \item reliability analysis of compressed deep neural networks, analyzing different compressing techniques such as quantization and pruning;
% \end{itemize}