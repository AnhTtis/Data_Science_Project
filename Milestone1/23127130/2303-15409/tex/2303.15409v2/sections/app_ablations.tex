

\section{Ablation Study}
\label{app:ablation}


We perform an ablation study, evaluating the influence of different elements on the performance.
We use Rebuffi \emph{et al.} \cite{rebuffi2021fixing} AT model WRN28-10 trained on CIFAR10 and threat model $\ell_{\infty}=8/255$.
We evaluate the performance over clean examples and 4 attacks: $(\ell_{\infty}, \epsilon=8/255)$, $(\ell_{\infty}, \epsilon=16/255)$, $(\ell_{2}, \epsilon=0.5)$, $(\ell_{2}, \epsilon=1.0)$. 
First, we investigate the necessity of the AT model.
Next, we check the impact of the distance metric by which we make the prediction.
Moreover, we check the influence of the transformation hyper-parameters $\alpha, \gamma$, and transformation steps $M$.
Finally, we check different $k$ values for \AlgoNameTopNoSpace.

\paragraph{\normalfont\textbf{PAG Property}}
Our method is based on image transformations, and we show the effectiveness of the proposed transformation through extensive evaluation.
We claim that the transformation is possible due to the PAG property, which is known to be possessed by AT classifiers \cite{engstrom2019adversarial, etmann2019connection,ross2018improving,tsipras2018robustness}. 
To validate this claim we use our method on WRN28-10 classifier that was trained on clean images.
We show that the model enhancement is limited compared to the results achieved by AT models.

\input{tables/ablation}


\paragraph{\normalfont\textbf{Distance Metric}}
Our method operates through two phases: transformation and distance measurement, where the distance is used for the classification decision. 
\AlgoName operates through $\ell_2$ norm, however, there are other reasonable choices such as LPIPS \cite{zhang2018unreasonable}, which measures the perceptual similarity between two images.
We demonstrate that $\ell_2$ better suit our method. 

\paragraph{\normalfont\textbf{Hyper-Parameters}}
We investigate the influence of each hyper-parameter, $\alpha, \gamma, M$, of \AlgoName over the model performance.
First, we look at different values of $\alpha$ which determines the transformation step size. 
The proposed transformation is performed via iterative gradient updates, where each step is of size $\alpha$.
Hence, the step size holds an important key. 
While small steps might better follow the function, the progress is slower since it requires additional steps.
On the other hand a large step size, although much faster,  might lead to insufficient results. 
Our results in \cref{table:ablation} support the theory, as a large step size leads to bad results.
While small step size, with additional transformation steps, leads to equally good performance.

Next, we examine the influence of $\gamma$ on the performance.
This parameter regulates the transformation distance.
Small values allow the transformation to change the image, while large values restrict the transformation to be minimal. 

Finally, we examine the number of transformation steps. 
When the number of steps is small, the transformation can not reach its objective.
For a sufficient number of steps, we get good results, even when we have more than the minimal required number of steps.  

\paragraph{\normalfont\textbf{Top-$k$}}
We investigate the influence of different $k$ values on \AlgoNameTop.
For small values, our method performance is getting closer to the AT classifier performances as we rely on its predictions.
When we increase $k$, we rely more on the transformation performance, which leads to better robustness.



