

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/teaser.pdf}
    \caption{
        \textbf{An Overview of \AlgoName}
            At first, the input image (clean or attacked) is class conditioned transformed through $\{\text{T}(\cdot|1), \dots,\text{T}(\cdot|N)\}$ to each one of the dataset classes, creating images $\{\text{Image}_1, \dots, \text{Image}_N\}$. Next, the $\ell_2$ distance is calculated between the input image and the transformed images $\{\text{Image}_1, \dots, \text{Image}_N\}$, and prediction is made based on the shortest distance.
    }
    \label{fig:teaser}
\end{figure*}

\section{Introduction}
\label{sec:intro}

% explain AT
Adversarial attacks are a significant concern in the field of computer vision, where maliciously designed perturbations are injected into an image to fool classifiers \cite{carlini2017adversarial,goodfellow2014explaining,kurakin2016adversarial, athalye2018synthesizing,biggio2013evasion,szegedy2013intriguing,hosseini2017google,dodge2017study,geirhos2017comparing,temel2018cure,temel2018traffic, kurakin2018adversarial,nguyen2015deep}. 
One common method of generating an adversarial example is through an iterative optimization process that searches for a norm-bounded perturbation $||\delta||_{p} \leq \epsilon$ that is added to the input image. 
The properties of the perturbation are determined by a threat model, characterized by the choice of the radius $\epsilon$ and the norm $\ell_p$ (typical choices for the latter are $p\in\{1,2,\infty\})$.
Following the discovery of adversarial attacks several defenses were introduced, such as Adversarial Training (AT) \cite{gowal2020uncovering,rebuffi2021fixing,madry2017towards,carlini2017adversarial,zhang2019theoretically,goodfellow2014explaining,kannan2018adversarial}.
AT incorporates adversarial examples into the training process alongside the true labels, training the model to correctly classify malicious examples.


%test time methods 
Test-time methods \cite{cohen2019certified, raff2019barrage, perez2021enhancing, schwinn2022improving, wu2021attacking} aim to improve trained AT classifiers by changing their inference methodology. 
These methods change the standard prediction process where an instance is fed into a model that outputs a class prediction. Instead, they follow an inference algorithm that strengthens the prediction at the cost of additional inference time. While these methods are appealing, they have been used only to enhance the performance of a model using the same threat model used during training.
Test-time methods have yet to be applied for the threat model-agnostic problem \cite{laidlaw2020perceptual, maini2020adversarial, kaufmann2019testing, blau2022threat}, methods that defend a model against threat models that were not used during training.
Threat model-agnostic property is desirable in real-world applications as the attacker is not limited to a single type of attack and will likely exploit the model's weaknesses, using threat models that were not used for training.






% motivation
One property of adversarial attacks is that they do not substantially alter the input image $x$, as the perturbation $\delta$ is usually imperceptible.
With this motivation in mind, we hypothesize that if we transform the attacked image towards the true class, the transformed image remains close to the input image, while, contrarily, a class-conditioned transformation toward a different class would lead to significant semantic changes to the input image.
Hence, we conceive of a solution that operates through an optimal class-conditioned transformation with a regularization, which keeps the transformed image close to the input image.
Unfortunately, such an optimal transformation does not exist and transformations usually require an additional model.  


We show that such a transformation can be performed using the recently discovered perceptually aligned gradients (PAG) property possessed by AT models \cite{engstrom2019adversarial, etmann2019connection,ross2018improving,tsipras2018robustness}. 
Simply put, the gradients of a classifier with respect to the input image manifest an intelligible spatial structure perceptually similar to that of some class of the dataset. 
As a result, maximizing the conditional probability of such classifiers towards some target class, via a gradient-based method, results in class-related semantic visual features. 





% our method overview
In this work, we propose COnditional transformation and DIstance-based Prediction (\AlgoNameNoSpace), a novel test-time threat model-agnostic defense that operates through two steps, as depicted in \cref{fig:teaser}.
First, we transform the input image into each one of the dataset classes, while regularizing the transformation, creating $\{\text{Image}_1 , \dots, \text{Image}_N\}$.
The transformation is regularized since we want to change the class of the input image while remaining as close as possible to the input image.
Moreover, the transformation utilizes the PAG property possessed by any AT model and spares the need for additional models or training. 
Next, we classify based on the shortest distance between the input image and the transformed images $\{\text{Image}_1 , \dots, \text{Image}_N\}$ with the assumption that the input image needs to cross a shorter distance to turn into the true class.





% motivation exp + key contributions
Our method can be applied to any differentiable classifier. Furthermore, it also equips the user with control over the clean-robust accuracy trade-off and enhances the model's performance for both seen and unseen threat models.
Additionally, we propose \AlgoNameTop, an efficient algorithm that filters the lowest probability classes, leading to speed up of inference time.  
We validate our method through extensive experiments on various AT methods, classifier architectures, and datasets. 
We empirically demonstrate that \AlgoName boosts the accuracy of any differentiable AT classifier, convolution or transformer based, across a variety of attacks, white-box or black-box, and generalizes well to unseen ones. 
Specifically, applying \AlgoName leads to substantial improvement of up to $+23\%$, $+20\%$, $+26\%$, and $+22\%$ on CIFAR10, CIFAR100, ImageNet and Flowers datasets, leading to state-of-the-art performance. 


To summarize, the key contributions of our work are:
\begin{itemize}[nolistsep,leftmargin=*]
    \item \AlgoName, a novel test-time algorithm that operates through conditional transformation and distance-based prediction, utilizing the PAG property and does not require an additional transformation model or training.  
    \item  \AlgoNameTop, an efficient algorithm that speeds up the inference time.
    \item A controllable clean-robust accuracy trade-off mechanism, that can be adapted by the user as needed, without training.
    \item State-of-the-art results for seen and unseen threat models, white-box or black-box. Demonstrated over multiple models, convolution or transformer based, AT methods, datasets, and provide extensive ablations to each component of our method.
\end{itemize}






% \begin{figure*}[ht!]
%     % \vspace{-100pt}
%     \centering
%     \includegraphics[width=1.\textwidth]{images/baselines_comparison.pdf}
%     \caption{
%     \textbf{Baselines Comparison} Performance comparison between \AlgoName and other test-time methods.
%     The plot presents the average accuracy of four different attacks.
%     The compared methods are Base (the AT classifier), TTE \cite{perez2021enhancing}, DRQ \cite{schwinn2022improving}, and \AlgoNameNoSpace.
%     The AT methods are \cite{madry2017towards}, Rebuffi \emph{et al.} \cite{rebuffi2021fixing} and Gowal \emph{et al.} \cite{gowal2020uncovering}, which were trained with the threat model specified below each method.
%     }
%     \label{fig:baselines_comparison}
% \end{figure*}








