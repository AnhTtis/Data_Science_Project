\section{Related work}
\label{sec:related}

\paragraph{\normalfont\textbf{Adversarial Training}} methods \cite{madry2017towards,carlini2017adversarial,goodfellow2014explaining} were first introduced to robustify a classifier against a specific threat model. 
However, a group of works \cite{kaufmann2019testing,brown2018unrestricted} have shown that such adversarially trained classifiers are less effective against unseen attacks. 
This finding has led to research that focuses on defenses for unseen attacks by training a model on a group of threat models.
The work done by Jordan \emph{et al.} \cite{jordan2019quantifying} suggests an attack that targets both texture and geometry, while Maini \emph{et al.} \cite{maini2020adversarial} suggests a combination of PGD-based attacks.
Another attempt was made by Laidlaw \emph{et al.} \cite{laidlaw2020perceptual}, which provides a latent space norm-bounded AT.
This line of work, however, has two major drawbacks.
First, it leads to inferior performance for every threat model compared to training a designated model for each specific threat model. 
Second, even a group of threat models cannot cover all possible threats, leaving the model vulnerable to attacks not included in the training. 



\paragraph{\normalfont\textbf{Generative Model-Based Defences}} are methods that aim at cleaning the attack from the image, shifting the image back to the image manifold, before feeding it into the classifier for classification. 
The work by Song \emph{et al.} \cite{song2017pixeldefend} purifies the image using pixelCNN and \cite{du2019implicit} suggest restoring the corrupt image with EBM. 
Hill \emph{et al.} ~\cite{hill2020stochastic} suggest to apply long-run Langevin sampling and Yoon \emph{et al.} ~\cite{yoon2021adversarial} offer gradient ascent score based-model. 
The work done by Blau \emph{et al.} \cite{blau2022threat} combines the two worlds of generative model and threat model agnostic defense, using a vanilla-trained diffusion model and classifier.
While these methods are close in spirit to ours in the sense that they perform a transformation over the attacked image, they all require additional models to perform the transformation. 



\paragraph{\normalfont\textbf{Test-Time Methods}} improve the performance of trained classifiers solely by using an inference phase of the neural network, while not requiring specialized training. 
However, they may result in longer inference times. 
Cohen \emph{et al.} \cite{cohen2019certified} and Raff \emph{et al.} \cite{raff2019barrage} suggest multiple realizations of the augmented image to be performed during test-time.
Cohen \emph{et al.} \cite{cohen2019certified} suggest augmenting with Gaussian noise, while Raff \emph{et al.} \cite{raff2019barrage} advocate using other types of augmentation; both methods involve averaging the classification predictions from the augmented images. 
Perez \emph{et al.} \cite{perez2021enhancing} perform test augmentation and predict the average. 
Schwinn \emph{et al.} \cite{schwinn2022improving} suggest analyzing the robustness of a local decision region near the attacked image, and Wu \emph{et al.} \cite{wu2021attacking} suggest to attack towards every class. 
The methods proposed by \cite{cohen2019certified,raff2019barrage} require fine-tuning the model, while \cite{schwinn2022improving,wu2021attacking} require neither fine-tuning nor access to the training data.


There are several distinctions between our approach and existing test-time methods. 
Firstly, while these methods focus on improving the performance of the threat model used for training the AT model, they fail to address the crucial issue of threat model-agnostic attacks. 
Secondly, unlike these methods, which rely on the classifier to make predictions, our predictions are based on measuring the distance between the input image and all the transformed images. 
Lastly, the works of \cite{schwinn2022improving,wu2021attacking}, which are most similar to our method, operate only in a local neighborhood around the input image, preventing the model from making significant semantic changes to the image. 
In contrast, our model is not limited to a specific neighborhood around the input image, as it transforms the image and alters its semantic appearance.






