\section{White-Box Attack}
\label{app:white_box}


\input{tables/white_box_attack}

Throughout this work, we followed the common practice \cite{laidlaw2020perceptual, hill2020stochastic, yoon2021adversarial}, and evaluated the performance with an attack that is not exposed to the whole defense, but to the classifier alone.
We perform such an attack since attacking such an iterative defense, with hundreds of steps, is computationally challenging.
Yet, there is no empirical evidence that these defenses remain strong when attacking them with a white-box attack.


To this end, we evaluate our method with a white-box attack in \cref{table:white_box_attack}, evaluating the robustness of \AlgoNameTop under PGD\footnote{We follow the implementation of https://github.com/MadryLab/robustness} \cite{madry2017towards}. 
We use the AT method Rebuffi \emph{et al.} \cite{rebuffi2021fixing} and compare the `Base' defense to \AlgoNameTopNoSpace.
As presented, our method improves the base model, by $9\%$ and up to $29\%$ for seen and unseen attacks respectively. 


We use the PGD \cite{madry2017towards} attack that operates through $20$ update steps, and we use four threat models, specified In Tab. 3. 
We evaluate the `Base' model, which is the AT model without additional test-time defense, and our method \AlgoNameTopNoSpace. 


Applying a white-box attack to such an iterative defense is computationally challenging, as we need to keep in memory all of the transformation steps for all of the target classes. 
This leads to a memory consumption that grows linearly with the number of transformation steps $M$ and the number of target classes $N$. 
To overcome this difficulty, even at the cost of decreasing the robust accuracy, we use \AlgoNameTop with $k=5$ and $M=20$ transformation steps. 
Additionally, we set the hyper-parameters $\gamma=300$ and $\alpha=0.3$.





