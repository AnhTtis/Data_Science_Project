

\input{tables/imagenet}


\section{Experiments}
\label{sec:experiments}
In this section, we provide implementation details, along with qualitative and quantitative evidence supporting our method. 
We present the datasets, baselines, and implementation details, followed by the results on CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, ImageNet \cite{deng2009imagenet}, and Flowers \cite{nilsback2008automated}. 
We compare our method using common practice attacks, including black-box attacks. 
Next, we introduce a method to control the clean-robust accuracy trade-off and conclude with a thorough runtime comparison.

In the Appendix, we include an ablation study of each component of our method in Appendix A, qualitative results in Appendix B, white-box attack in Appendix C, the influence of a designated Top-$k$ attack in Appendix D, further implementation details in Appendix E and further explanations in Appendix F, Appendix G, Appendix H.



\subsection{Experimental Settings}
\paragraph{\normalfont\textbf{Datasets}} The dataset CIFAR10 and CIFAR100 \cite{krizhevsky2009learning} both containing $50\text{K}$ samples for training and $10\text{K}$ samples for test, each of size $(3 \times 32 \times 32)$. 
They differ in the number of classes, where CIFAR10 contains $10$ classes and CIFAR100 contains $100$. 
The ImageNet dataset \cite{deng2009imagenet} contains $1\text{M}$ images for training and $50\text{K}$ for validation, the images are resized to $(3 \times 256 \times 256)$ and cropped to $(3 \times 224 \times 224)$.
The Flowers dataset \cite{nilsback2008automated} contains $103$ classes, $1030$ images for training and $6129$ for test, the images are resized to $(3 \times 256 \times 256)$ and cropped to $(3 \times 224 \times 224)$.




\paragraph{\normalfont\textbf{Evaluation}} For evaluation we use the test set of CIFAR10, CIFAR100 and Flowers, and the validation set of ImageNet as we follow the convention of ImageNet evaluation, and we report the average accuracy. 
We employ AutoAttack \cite{croce2020reliable} as a robust and widely accepted benchmark for adversarial attack. 
AutoAttack is an ensemble of four diverse attacks, where all four attacks are applied to perform a successful attack and allow a reliable robustness evaluation. 
It includes the attacks: APGD-CE and APGD-DLR which were proposed in \cite{croce2020reliable}, FAB \cite{croce2020minimally} and Square Attack \cite{andriushchenko2020square}. 
For CIFAR10 and CIFAR100 we evaluate the performance using the following thereat models $(\ell_{\infty}, \epsilon=8/255)$, $(\ell_{\infty}, \epsilon=16/255)$, $(\ell_{2}, \epsilon=0.5)$, $(\ell_{2}, \epsilon=1.0)$, and for ImageNet and Flowers we use $(\ell_{\infty}, \epsilon=4/255)$, $(\ell_{\infty}, \epsilon=8/255)$, $(\ell_{2}, \epsilon=3.0)$, $(\ell_{2}, \epsilon=6.0)$.
Following previous works \cite{laidlaw2020perceptual, hill2020stochastic, yoon2021adversarial}, we attack on the classifier, as it is computationally impossible to attack these methods. 

We follow the common data split and portion, proposed by Trustworthy-AI-Group\footnote{https://github.com/Trustworthy-AI-Group/TransferAttack}, for all of the transformer based models and for the black-box attacks.


\paragraph{\normalfont\textbf{Implementation Details}}
We check our method performance on SOTA AT trained classifiers.
For CIFAR10 and CIFAR100 we use Rebuffi \emph{et al.} \cite{rebuffi2021fixing}, Gowal \emph{et al.} \cite{gowal2020uncovering} and Madry \emph{et al.} \cite{madry2017towards}, for ImageNet we use Madry \emph{et al.}, Salman \emph{et al.} \cite{salman2020adversarially} and Debenedetti et al. \cite{debenedetti2023light}, and for Flowers we use Debenedetti et al. \cite{debenedetti2023light}.
For each AT classifier, we specify the chosen hyper-parameters in Appendix E, and for each transformation we perform $30$ iterations. 
We use the following architectures: ResNet50, WideResNet50-2, WideResNet28-10, WideResNet70-16, XCiT-S/M.




\paragraph{\normalfont\textbf{Baselines}} 
To evaluate our method we compare it to a few baselines.
We start with `Base' which is the AT classifier and PAT \cite{laidlaw2020perceptual} which suggests an AT threat model-agnostic defense. 
PAT \cite{laidlaw2020perceptual} and Randomized Smoothing \cite{cohen2019certified} were evaluated only on the CIFAR10 dataset.
Additionally, we compare our method to two other test-time methods: TTE \cite{perez2021enhancing} and DRQ \cite{schwinn2022improving}. 
We evaluate both of these methods on different threat models, allowing a fair comparison.
TTE was evaluated on CIFAR10, CIFAR100, and ImageNet, however, DRQ can not be applied to ImageNet, as stated in their work, since it is computationally infeasible.


We propose two variants of our method: method \AlgoName and \AlgoNameTopNoSpace.
Both are presented in \cref{sec:our_method} and while \AlgoName performs conditioned transformation of the input image towards all of the dataset's classes, \AlgoNameTop transforms only toward the group of classes that is more likely to contain the true class. 
\AlgoNameTop is essential when employing our method on datasets with a large number of classes.
To ensure a fair comparison, we explore a designated Top-$k$ attack which was proposed by Zhang \emph{et al.} \cite{zhang2022investigating} and we discuss our findings in Appendix D. 




\subsection{Comparison}
% \vspace{20pt}

We present the performance over four datasets: 
CIFAR10 and CIFAR100 in \cref{table:cifar10_and_cifar100}, and ImageNet and Flowers in \cref{table:imagenet}.
We present AT models that were trained on different Trained Threat Model and with different architecture, and we employ different test-time method: Base, TTE DRQ, \AlgoName and \AlgoNameTop. 
For CIFAR10 and CIFAR100 datasets we compare our method to PAT \cite{laidlaw2020perceptual}, Randomized Smoothing \cite{cohen2019certified}, TTE \cite{perez2021enhancing} and to DRQ \cite{schwinn2022improving}, and for ImageNet we use TTE \cite{perez2021enhancing}.



\AlgoName enhances the base AT methods (represented by `Base' under `Test-Time Method') for both seen and unseen attacks. 
In particular, we enhance the performance for seen attacks by up to $9\%$, $10\%$, $15\%$, $11\%$, and 
for unseen attacks by up to $10\%$, $20\%$, $26\%$, $22\%$ over CIFAR10, CIFAR100, Imagenet, and Flowers respectively. 





When comparing to other test-time methods, we enhance the performance for seen attacks by up to $1\%$, $3\%$ and $12\%$, and for unseen attacks by up to $10\%$, $20\%$ and $24\%$ over CIFAR10, CIFAR100 and Imagenet respectively.
Additionally, for CIFAR10 we compare \AlgoName to the PAT \cite{laidlaw2020perceptual}, a method that addresses the issue of robustness to unseen attacks, and we report up to $52\%$ higher accuracy.
We further compare our method to Randomized Smoothing \cite{cohen2019certified}, a test-time method requiring the robust classifier to be trained on Gaussian noise. 
Typically, robust classifiers are not trained using this augmentation, which often leads to ineffective outcomes with Randomized Smoothing, particularly with the models we employ, such as AT \cite{madry2017towards} RN50.
Moreover, for CIFAR100 we use both \AlgoName and \AlgoNameTop ($k=10$) demonstrating that not only do we benefit from much faster inference time but also that the performances are slightly better.
For ImageNet we use \AlgoNameTop ($k=20$), as \AlgoName is computationally not feasible, and for Flowers we use \AlgoNameTop ($k=10$).

\paragraph{\normalfont\textbf{Top-$k$}} The Top-$k$ version of CODIP sometimes outperforms the full CODIP because it focuses on the most likely classes, reducing the influence of less relevant or noisy class transformations. By concentrating on a smaller set of high-confidence predictions, the Top-$k$ version can enhance accuracy by avoiding the potential noise introduced when evaluating all possible classes. This selective attention allows it to maintain or even improve performance in scenarios where the correct class is consistently within the top $k$ predictions. Additional information in Appendix F.




















\input{tables/black_box}


\subsection{Black Box Attack}
\label{exp:black_box}


We present the effectiveness of our method against advanced black box attacks in \cref{table:black_box}. 
including MI-FGSM \cite{dong2018boosting} and Admix \cite{wang2021admix}, and L2T \cite{zhu2024learning} which is currently considered the strongest one. 
We utilize the official implementation Trustworthy-AI-Group\footnote{https://github.com/Trustworthy-AI-Group/TransferAttack}, maintain the same data split, and use an ensemble of surrogate models under the $L_{\infty}=16/255$ attack. As demonstrated, our defense enhances the performance under black-box attacks as well.






\input{tables/runtime}








\subsection{Clean-Robust Accuracy Trade-off}
% \vspace{20pt}
The clean-robust accuracy trade-off is a well-known phenomenon \cite{zhang2019theoretically, tsipras2018robustness}. 
Training a classifier with adversarially perturbed images can significantly improve the robust accuracy, but it often comes at the expense of clean accuracy. 
When robustifying a model using AT, this trade-off is determined during training and cannot be controlled afterwards. 
Similarly, test-time methods also operate with a fixed clean-robust accuracy trade-off and do not offer a way to control it. Further details are provided in Appendix G.


\begin{figure}[h!]
    
  \begin{center}
    \includegraphics[width=0.5\textwidth]{images/tradeoff_plot.pdf}
  \end{center}
  \caption{
    \textbf{Clean-Robust Accuracy Trade-off} A demonstration of our proposed controlled clean-robust accuracy tradeoff. 
    The tradeoff is controlled by adapting the step size value $\alpha$, specified beside each of \AlgoName workpoints.
    The used test-time methods: `Base' which is the base AT model, TTE \cite{perez2021enhancing}, DRQ \cite{schwinn2022improving}, and \AlgoNameNoSpace.
}
  \label{fig:tradeoff}
\end{figure}

Our method, on the other hand, allows the user to control the trade-off by adjusting $\alpha$, which is the transformation's step size, presented in \cref{alg:CODIP}. 
By increasing $\alpha$, the user can prioritize robust accuracy, while decreasing $\alpha$ prioritize clean accuracy. 
In \cref{fig:tradeoff}, we present the clean-robust accuracy tradeoff for AT model trained on CIFAR10 by Madry et al.'s \cite{madry2017towards} on threat model $L_{2}, \epsilon=0.5$ and architecture ResNet-50.
We compare the performance of the robust classifier under the attack threat model ($L_{\infty}, \epsilon=16/255$) using the AT model and three other test-time methods: Base which is the performance of the AT classifier, TTE \cite{perez2021enhancing}, DRQ \cite{schwinn2022improving}, and our method, \AlgoName. 
As depicted in the figure, \AlgoName we can control the trade-off by adjusting $\alpha$, prioritizing clean or robust accuracy as needed.
We show that we can improve the clean accuracy or enhance robust accuracy by up to $25\%$.













\subsection{Runtime Analysis}
\label{exp:runtime}

This section provides a runtime comparison across different architectures and datasets, detailed in \cref{table:runtime}.
For a fair comparison, all experiments were conducted using one RTX A6000 with a batch size of $1$.

We compare our method to the base classifier, TTE \cite{perez2021enhancing} and to DRQ \cite{schwinn2022improving}, reporting the relative inference time compared to the base classifier.
We can see that TTE \cite{perez2021enhancing} increase the inference time due to the predetermined number of augmentations performed for each image.
This extra time is added since we perform a predetermined number of augmentation for each image. 
The other methods, DRQ \cite{schwinn2022improving} and our approach, also result in increased inference time compared to the base classifier. 
However, both \AlgoName and, in particular, \AlgoNameTop demonstrate significantly faster performance. Specifically, \AlgoNameTop is up to 100 times faster than DRQ \cite{schwinn2022improving}.


Another crucial consideration is memory consumption, detailed under both \AlgoName and $\text{CODIP}_{\text{Top-}5}$ columns.
We specify the Maximum Batch Size (MBS) that fits into memory.
It is evident that $\text{CODIP}_{\text{Top-}5}$ enables an increase in batch size of up to $23$ times.













