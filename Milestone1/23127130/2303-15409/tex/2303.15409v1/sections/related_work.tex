\section{Related work}
\label{sec:related}



This section presents related works that use preprocessing and test-time inference methods for enhanced classification robustness. The former preprocesses the image before sending it to the classifier, whereas the latter changes the inference methodology, using an inference algorithm. Finally, we discuss several adversarial defense methods that aim at unseen attacks.

The first work on prepossessing methods, which improve classification robustness, includes random resizing and padding \cite{xie2017mitigating}, thermometer encoding \cite{buckman2018thermometer}, feature squeezing \cite{xu2017feature}, defense GAN \cite{samangouei2018defense} and masking and reconstructing \cite{yang2019me}. A newer line of preprocessing methods uses probabilistic models. These methods aim to leverage their generative power to clear perturbations from an attacked image. They perform it by projecting the attacked image back to the data manifold. This line of work includes purification by pixelCNN \cite{song2017pixeldefend}, EBM for restoring corrupt images \cite{du2019implicit} and a density aware classifier \cite{grathwohl2019your}. The most recent works in this field includes Langevin sampling \cite{hill2020stochastic} and a gradient ascent score-based model \cite{yoon2021adversarial}. Similarly, two recent studies utilize the generative power of diffusion models \cite{nie2022diffusion,blau2022threat}.


Another group of adversarial defense schemes uses test-time methods. Cohen \emph{et al.} \cite{cohen2019certified} and Raff \emph{et al.} \cite{raff2019barrage} suggest multiple realizations of the augmented image to be performed during test-time, followed by averaging the classification predictions. Schwinn \emph{et al.} \cite{schwinn2022improving} suggest to analyze the robustness of a local decision region nearby the attacked image. While \cite{cohen2019certified,raff2019barrage} require fine-tuning the model, Schwinn \emph{et al.} require neither fine-tuning nor access to the training data, similar to our method.

The last group of studies aims to provide robustness against unseen attacks. Laidlaw \emph{et al.} \cite{laidlaw2020perceptual} provides a latent space norm-bounded AT, and Blau \emph{et al.} \cite{blau2022threat} uses a vanilla trained diffusion model as a preprocessing step.

To the best of our knowledge, Schwinn \emph{et al.} \cite{schwinn2022improving} offer the only test time method that enhances adversarially trained robust classifiers without further training, similar to our method.

