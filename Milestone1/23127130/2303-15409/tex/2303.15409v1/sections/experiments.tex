\section{Experiments}
\label{sec:experiments}



In this section, we provide empirical evidence supporting our method. First, we apply TETRA and FETRA to various adversarially trained methods and architectures, evaluating the performance on CIFAR10, CIFAR100 \cite{krizhevsky2009learning} and ImageNet \cite{deng2009imagenet} datasets. Next, we evaluate the RPGD attack, providing visual evidence and intuition for our method. Finally, we offer an ablation study in Appendix \ref{app:ablation} that presents the necessity of the PAG property for TERTA and evaluates different distance metrics. Throughout our experiments, we evaluate multiple threat models for every robust classifier, using AutoAttack \cite{croce2020reliable}, as it is a strong and widely accepted attack benchmark. Further experimental details are provided in \cref{app:exp_setup}.


\subsection{CIFAR10 and CIFAR100}
In this part, we supply results over CIFAR10 and CIFAR100 \cite{krizhevsky2009learning}, which are two of the most common datasets used for robustness evaluation. We compare the proposed TETRA to the DQR method from \cite{schwinn2022improving} which is a test-time method that boosts a robust classifier's performance. Hence, for every robust classifier, we show its results and the results of the other methods, creating a few consecutive lines of results per AT classifier. Moreover, we use four common threat models: $(\ell_{\infty}, \epsilon=8/255)$, $(\ell_{\infty}, \epsilon=16/255)$, $(\ell_{2}, \epsilon=0.5)$, $(\ell_{2}, \epsilon=1.0)$ and evaluate the performance over the complete test dataset, as the top $1$ classification accuracy. The test dataset contains $10,000$ images, in both CIFAR10 and CIFAR100 datasets.

The results on CIFAR10 are summarized in \cref{table:cifar10}. We use the top performing methods \cite{madry2017towards,rebuffi2021fixing,gowal2020uncovering} as baselines and demonstrate that TETRA significantly improves their performance. Interestingly, TETRA boosts baseline methods both for seen and unseen attacks. In particular, for seen attacks, we enhance the baseline method performance by up to $9\%$, outperforming the other methods by up to $1\%$. For the unseen attacks, TETRA boosts the performance of baseline methods by up to $23\%$, outperforming the other methods by up to $10\%$. Additionally, we compare TETRA to the PAT method \cite{laidlaw2020perceptual} that addresses the issue of robustness to unseen attacks, and report up to $52\%$ higher accuracy.

\input{tables/cifar10}

The results on CIFAR100 are presented in \cref{table:cifar100}. We present our defenses, TETRA and FETRA, where FETRA uses the top $10$ classes and accelerates the calculation by a factor of $10$. We use the defense methods \cite{rebuffi2021fixing,gowal2020uncovering} as baseline methods and demonstrate that TETRA improves performance both for seen and unseen attacks. For seen attacks, we enhance the baseline method performance by up to $10\%$ and the performance of other methods by up to $3\%$, while for unseen attacks the baseline performance is increased by up to $20\%$, and the performance of other methods by up to $8\%$. It is important to emphasize that both methods, TETRA and FETRA, present similar results as the average gap is $0.66\%$. This demonstrates that FETRA's filtering is not the reason for the performance enhancement, and that it is just a speed-up method.



\input{tables/cifar100}

\subsection{ImageNet}
The results over ImageNet are reported in \cref{table:imagenet}. Evaluating ImageNet is infeasible using TETRA because of memory and runtime issues. Therefore, we only evaluate the faster defense variant FETRA, with top $k=20$ classes, which accelerates the calculation by a factor of $50$. Moreover, we use four common threat models: $(\ell_{\infty}, \epsilon=4/255)$, $(\ell_{\infty}, \epsilon=8/255)$, $(\ell_{2}, \epsilon=3.0)$, $(\ell_{2}, \epsilon=6.0)$ and evaluate the performance over the complete test dataset ($50,000$ images), as the top $1$ classification accuracy. We use the defense methods \cite{madry2017towards,salman2020adversarially} as baselines, demonstrating that FETRA improves the performance both for seen and unseen attacks. For seen attacks, we enhance the baseline performance by up to $15\%$ and for unseen attacks we enhance the baseline by up to $26\%$.


\input{tables/imagenet}


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{images/TETRA_vs_PGD.pdf}
    \caption{TETRA vs. PGD targeted transformation. In this figure, we present a comparison between targeted PGD and TETRA's transformation, where both methods transform an image into the desired class. PGD changes the image considerably in order to change the classification into the \emph{toucan} class, while TETRA remains pixel-wise close to the input image and changes the wing of the \emph{lorikeet} only slightly so it resembles a \emph{toucan}. Pixel-level distance is presented next to the method as the $\ell_2$ distance.}
    \label{fig:TETRA_vs_PGD}
\end{figure}


\subsection{Ranking projected gradient descent}
In this part, we compare the performance of two attacks over ImageNet dataset. We compare RPGD to PGD (RPGD, see  \cref{sec:our_method}). We show that RPGD better fits FETRA as it is designed for its weakness. Furthermore, we demonstrate that the accuracy drop is neglectable compared to the gain achieved by using FETRA.

We start by comparing the top $k$ accuracy performances of both RPGD and PGD. As stated before in \cref{sec:our_method}, AutoAttack does not utilize the known weakness of FETRA, hence does not create its worst-case examples. FETRA keeps only the top $k$ classes, estimated by a robust classifier, and then applies TETRA to classify the remaining ones. AutoAttack does not exploit the knowledge regarding filtering, which leads us to the introduction of RPGD. RPGD is designed to enable a fair robust evaluation, searching for FETRA's worst-case examples. This is achieved by pushing the rank of the true class as low as possible and out of the top $k$ classes. To verify that RPGD indeed achieves its goal, we suggest the analysis presented in \cref{fig:RPGD_imagenet}. This analysis is performed using Madry \emph{et al.}'s defense method, trained on $\ell_{2}, \epsilon=3.0$. On the left-hand side of the figure, we analyze the performance of both attacks, RPGD and PGD, by evaluating the top $k$ accuracy for different $k$ values. To better display the gap between the methods, we also provide the right-hand of the graph. This plot presents the difference between the two left-side graphs and demonstrates that each attack is preferred for its objective. For low $k$ values, PGD achieves lower accuracy, but for high $k$ values, RPGD achieves up to $5\%$ lower accuracy. 

Next, we verify that FETRA's performance enhancement still holds, even when considering the performance drop caused by attacking with RPGD. To this end, we show \cref{table:imagenet}, which presents an evaluation of the ImageNet dataset attacked by AutoAttack and defended by FETRA with top $k=20$. FETRA enhances the base performance by at least $9\%$ and up to $26\%$. The performance reduction caused by RPGD for the same $k$ value is $2.5\%$, much lower than the demonstrated enhancement. To conclude, RPGD is a stronger attack for FETRA, resulting in a decreased performance. Nevertheless, FETRA achieves a significant performance enhancement. A similar analysis for CIFAR10 and CIFAR100 is presented in \cref{app:RPGD}.


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/RPGD_imagenet.pdf}
    \caption{ImageNet top $k$ accuracy comparison PGD vs RPGD. the x-axis of the left-hand side of the figure represents the top $k$ group size that we select, using Madry et al. \cite{madry2017towards} $\ell_{2}, \epsilon=3.0$. The y-axis represents the top $k$ accuracy, the probability that the true label is contained in the top $k$ group. For example, if we examine $k=20$ it means that we seek the probability that the true label is in one of the top $20$ predictions of the classifier, which is around $60\%$. On the right-hand side of the figure, we present the difference between the two graphs of the left-hand side, $PGD - RPGD$.}
    \label{fig:RPGD_imagenet}
\end{figure*}



\subsection{Transformed images}
We proceed to the visualization of the TETRA transformation, supplying an intuitive explanation of what the transformation looks like and why our method works.  

First, we compare our method to PGD. As shown in \cref{fig:TETRA_vs_PGD}, PGD transforms the image's appearance significantly in order to change its classification. TETRA also modifies the image appearance to the target class, however, it keeps the transformed image pixel-wise close to the input image. The transformation is performed in a rather artistic way, as it is almost imperceptible that the \emph{toucan} appears on the \emph{lorikeet}'s wing.

TETRA relies on the hypothesis that the extent of the image modification relates to the probability of belonging to a certain class. Therefore, it is important to supply visual evidence and intuition. To this end, we present \cref{fig:generation_of_classes}, where we demonstrate TETRA transformation towards multiple classes. In the first column, we present the clean image. In the following four columns, we transform the image into target classes. First, to the true class, then to a similar class, and finally to two entirely different class categories. This figure emphasizes that a transformation of an image to the true class, or similar ones, requires minor changes. However, when transforming an image into a different category, the image is modulated considerably by adding some features belonging to the target class. 







