\section{Background}
\label{sec:background}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/generation_of_y_target.pdf}
    \caption{Visualization of TETRA's transformation. Clean images from the ImageNet dataset \cite{deng2009imagenet} (left column) are transformed by TETRA into (left-to-right): the true class, a class from the same category, and two classes from different categories. Pixel-level distance is more noticeable as the perceptual distance to the target class grows (presented in the images' titles as the $\ell_2$ distance).}
    \label{fig:generation_of_classes}
\end{figure*}


Since the discovery of adversarial attacks \cite{szegedy2013intriguing, goodfellow2014explaining, kurakin2016adversarial, athalye2018synthesizing,biggio2013evasion, carlini2017adversarial, kurakin2018adversarial,nguyen2015deep}, there has been a continuous development of defense and attack techniques. In this section, we briefly overview key results starting with attacks, moving to defense strategies, and finishing with the PAG property.

An adversarial attack is a perturbation $\delta$, added to an image $x$, intended to push a classifier decision away from the correct prediction. Many important studies researched adversarial attacks, of which it is important to mention Madry \emph{et al.} \cite{madry2017towards} who laid the foundation for many following works, including ours. Madry \emph{et al.} introduced the projected gradient descent (PGD) algorithm, which is an iterative optimization process that searches for the worst-case adversarial example. PGD has access to the classifier's weights, and is, therefore, able to find the worst-case adversary in a small radius around a clean data sample. The allowed perturbation is defined by a threat model, characterized by an $\ell_{p}$ norm and a radius $\epsilon$, such that $||\delta||_{p} \le \epsilon$. There exist two variants for the loss term. The objective of the first variant is to maximize the classification loss of the perturbed input, given the true label. In other words, the input image is manipulated in order to increase the error, aiming for a wrong classifier decision (\emph{any} class except the correct one). The second variant's objective is to minimize the classification loss of the perturbed image given a \emph{specific} wrong class label $y$. As a result, the classifier is more likely to predict label $y$. Our method utilizes the latter \emph{targeted PGD} variant.

One of the leading robustification methods known as adversarial training (AT) \cite{madry2017towards, zhang2019theoretically, rebuffi2021fixing, gowal2020uncovering, salman2020adversarially,goodfellow2014explaining,carlini2017adversarial,croce2020reliable,tramer2020adaptive}. AT is a training method that robustifies a classifier against a specific attack. This is achieved by introducing adversarial examples during the training process, and driving the classifier to learn to infer the true labels of malicious examples. While training, the classifier weights are constantly being changed. As a result, the classifier's worst-case examples keep on changing as well. Hence, in every training batch, one must calculate new adversarial examples that fit the current state of the classifier.

It has been recently discovered that some classifiers exhibit an intriguing property called perceptually aligned gradients (PAG) \cite{engstrom2019adversarial, etmann2019connection,ross2018improving,tsipras2018robustness}. PAG is manifested through the classification loss gradients, with respect to the input image, appearing visually meaningful to humans, and resembling one of the dataset classes. The structure of the gradients is different when performing untargeted vs. targeted PGD. When performing untargeted PGD, the attack is not leading to a specific class; therefore, the gradients transform the image arbitrarily. When performing targeted PGD, however, the gradients transform the image into the target class, removing current class features. Using this property, our method enables, one to transform an image into a target class, as used by our method.

