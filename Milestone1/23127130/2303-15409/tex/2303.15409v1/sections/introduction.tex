\section{Introduction}
\label{sec:intro}

% * classifiers vulnerability 
% * adversarial training
% * pag property 
% * other methods for boosting 
% * our method + contribution



Deep neural networks (DNNs) have revolutionized the visual recognition domain by achieving unprecedentedly high classification accuracy. DNN classifiers, however, have been shown to be highly sensitive to minor input perturbations \cite{hosseini2017google,dodge2017study,geirhos2017comparing,temel2018cure,temel2018traffic}, thereby giving rise to the field of adversarial attacks \cite{szegedy2013intriguing, goodfellow2014explaining, kurakin2016adversarial, athalye2018synthesizing,biggio2013evasion, carlini2017adversarial, kurakin2018adversarial,nguyen2015deep}. Adversarial attacks are maliciously designed imperceptible perturbations added to an image intended to fool a classifier. These examples are worrisome because of the vast use of DNNs in critical tasks such as autonomous driving. Consequently, this field has gained considerable research attention, leading to the development of both new defenses and attack methods. One common way to create an adversarial example is achieved by an iterative optimization process that searches for a norm-bounded perturbation $||\delta||_{p} \leq \epsilon$ that is added to the input image. The properties of the perturbation are determined by a threat model, characterized by the choice of the radius $\epsilon$ and the norm $\ell_p$ (typical choices for the latter are $p\in\{1,2,\infty\})$.

\begin{figure*}[th!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/method.pdf}
    \caption{TETRA algorithm. A cartoon depiction
    of the classifier decision rule describes regions of the input space classified as distinct classes in different colors. Initially, a clean input image (green dot) belonging to the \emph{cat} class is attacked, creating an adversarial example (red dot) that has a wrong predicted class (\emph{horse} instead of \emph{cat}). When TETRA is applied, in its first phase, it transforms the adversarial image into each one of the dataset classes. The transformation is represented by the dotted black arrows leading to a set of new images, one per class (blue dots). In its second phase, TETRA calculates the distances between the adversarial example to all of the generated images. Finally, TETRA classifies the input based on the calculated shortest distance, which leads to a correct class label prediction (\emph{cat}).}
    \label{fig:method}
\end{figure*}


One leading defense method is adversarial training (AT) \cite{madry2017towards, zhang2019theoretically, rebuffi2021fixing, gowal2020uncovering, salman2020adversarially,goodfellow2014explaining,carlini2017adversarial,croce2020reliable,tramer2020adaptive}, in which the classifier is trained in the presence of adversarial perturbation and learns to classify the maliciously modified training samples correctly. A major drawback of AT is its reliance on a specific attack type, which leads to poor generalization to unseen ones. This issue has been mentioned in \cite{hendrycks2021unsolved} as one of the unsolved problems in this field, and a few methods \cite{bai2021recent, blau2022threat} have recently attempted to address it.

Another line of research \cite{zhang2021memo, schwinn2022improving} focuses on test-time methods that aim to improve trained AT classifiers by changing their inference methodology. These methods change the standard prediction process where an instance is fed into a DNN model that outputs a class prediction. Instead, they follow an inference algorithm that strengthens the prediction. These methods are appealing since they can be applied to any trained classifier, without any further training or access to the training dataset. These advantages significantly enhance DNNs' robustness to adversarial attacks.

Recently, researchers discovered that AT classifiers exhibit a fascinating property that was called perceptually aligned gradients (PAG) \cite{engstrom2019adversarial, etmann2019connection,ross2018improving,tsipras2018robustness}. Simply put, the gradients of a classifier, with respect to the input image, manifest an intelligible spatial structure perceptually similar to that of some class of the dataset. As a result, maximizing the conditional probability of such classifiers towards some target class, via a gradient-based method, results in class-related semantic visual features. The discovery of PAG has drawn considerable research interest in the past few years, leading to a sequence of works. Tsipiras \emph{et al.} \cite{tsipras2018robustness} showed that PAG does not arise in every classifier; they emerge in AT classifiers and not in ``vanilla-trained'' ones. The work of \cite{ganz2022perceptually} demonstrated the surprising bidirectional connection between PAG and robustness. From the applicative point of view, to date, PAG has mainly been studied in the context of generative tasks ~\cite{ganz2021bigroc,kawar2022enhancing}.

In this work, we propose a novel test-time defense method that leverages the PAG property for further improving the robustness of trained robust classifiers. Our method improves a classifier's accuracy even when attacking with the same attack that it was trained on and significantly improves the robustness to unseen attacks. It can be applied to any differentiable classifier -- hence the name Classifier Robustness Enhancement Via Test-Time Transformation (TETRA). Our method operates in two phases, as depicted in \cref{fig:method}. First, it modifies the input image via a designated targeted adversarial attack into each dataset class. This modification is meaningful due to the PAG property, which guarantees worthy gradients. Then, it classifies based on the modification distances, with the assumption that the shortest attack relates to the true class.


We validate our method through extensive experiments on a variety of defense methods, classifier architectures, and datasets. We empirically demonstrate that TETRA can boost the accuracy of any differentiable AT classifier across a variety of attacks, generalizing well to unseen ones. Specifically, applying TETRA leads to substantial improvement of up to $+23\%$, $+20\%$, and $+26\%$ on CIFAR10, CIFAR100 and ImageNet, leading to state-of-the-art performance on test-time defense tasks. 
To summarize, the key contributions of our work are:
\begin{itemize}[nolistsep,leftmargin=*]
    \item We present a novel test-time robust classification method that utilizes the PAG property and requires neither additional training nor access to the original training set.
    \item We show improved classifiers' robustness, evaluated both on trained attacks and on unseen ones.
    \item We evaluate our method's performance, achieving state-of-the-art results compared to other test-time methods.
\end{itemize}

