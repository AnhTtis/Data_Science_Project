\section{Our method}
\label{sec:our_method}




In this section, we present the proposed TETRA method -- a novel test-time defense against adversarial attacks. We introduce TETRA intuition and supporting evidence. We then propose a speed-up method referred to as FETRA, and a novel attack dubbed ranking projected gradient descent (RPGD), designed to find the worst-case examples for FETRA.

\subsection{Test-time transformation}


An adversarial example is a small norm perturbation that humans will hardly notice, yet it changes the classifier's prediction. It is widely accepted that the attack deviates an image off the image's manifold \cite{shamir2021dimpled,tanay2016boundary,khoury2018geometry,stutz2019disentangling}. Hence, it seems reasonable to preprocess perturbed images \cite{xie2017mitigating,buckman2018thermometer,xu2017feature,samangouei2018defense,yang2019me,song2017pixeldefend,du2019implicit,grathwohl2019your,hill2020stochastic,yoon2021adversarial,nie2022diffusion,blau2022threat}, resulting in a projection back to the image's manifold. One unexplored way to perform a projection is through a transformation into a target class, which we refer to as targeted transformation. When performing targeted transformation toward the true class, we are projecting the image back to the image's manifold, resulting in an image with close resemblance to the input image. Unfortunately, we do not know the true label; therefore, we perform a targeted transformation of the image into each and every one of the dataset classes. We assume that a transformation into a different class changes the image considerably. Hence, we can classify the image based on the distances between the input image to the transformed images, where the shortest distance relates to the true class.

As presented in \cref{sec:background}, AT classifiers possess the PAG property, which was studied in the context of generative tasks \cite{ganz2021bigroc,kawar2022enhancing}. We propose to leverage this generative power for the targeted transformation task that lies at the core of TETRA, avoiding the use of a specialized generative model. PAG guarantees that the classifier gradients possess a meaningful structure. When these are put into an iterative optimization task, the input image is gradually transformed into a target class. When transforming an image without regularization, however, it might undergo significant changes, greatly departing from the initial image. \cref{fig:TETRA_vs_PGD} visualizes targeted image transformations obtained from PGD and TETRA. While PGD changes the image considerably as it moves toward the target class, TETRA's effect is more gentle, pertaining only to certain regions of the image and adding semantic features that are related to the target class. An additional demonstration is provided in \cref{fig:generation_of_classes}, where we visualize a targeted transformation of images into different classes. 


To better explain our method, we move now to the cartoon depiction presented in \cref{fig:method}. In this example, we attack an image of a \emph{cat} (green dot), creating an attacked image (red dot). The attacked image looks like a \emph{cat}, but the classifier mistakenly confuses it with a \emph{horse}. Next, we transform the attacked image into every one of the dataset's classes, resulting in transformed images (blue dots). Without regularization, the transformed images would end up in the classes' centroids. However, a regularized transformation produces more gentle transformations (blue points). Finally, we calculate the distances between the attacked image and the transformed images, correctly classifying based on the shortest distance (blue star).

\begin{algorithm}[H]
    \caption{Test-Time Transformation}
    \label{alg:TETRA}
    \hspace*{\algorithmicindent}\textbf{Input} classifier $f(\cdot)$, input $x$, target label $y$, norm \hspace*{\algorithmicindent}\hspace*{\algorithmicindent}\hspace*{\algorithmicindent}radius $\epsilon$, step size $\alpha$, number of steps $N$, \\ \hspace*{\algorithmicindent}\hspace*{\algorithmicindent}\hspace*{\algorithmicindent}hyperparameter $\gamma$, classification classes $K$ 
    \begin{algorithmic}[1]
    \Procedure{TETRA}{}
        \For{\texttt{$i$ in $1:K$}}
            \State $\delta_{i} \gets 0$
            \For{\texttt{$j$ in $1:N$}}
                \State $\delta_{i} \gets \Pi \left( \delta_{i} - \alpha \, \frac{\nabla_{\delta_{i}} L_{CE}\left( f \left( x+\delta_{i} \right), y \right) + \gamma \, \delta_{i}}{\|\nabla_{\delta_{i}} L_{CE}\left( f \left( x+\delta_{i} \right), y \right) + \gamma \, \delta_{i} \|_2} \right)$
            \EndFor
            \State $d_{i} \gets \|\delta_{i}\|_{2}$ 
        \EndFor
    \State $\hat{y} = \mathrm{arg}\min_{i=1,\dots,K} d_i$
    \State \Return $\hat{y}$
    \EndProcedure
    
    \end{algorithmic}
    The operator $\Pi(z)$ projects $z$ onto the image domain $\mathbb{R}^{M \times N \times 3} \in [0,1]^{M  \times N \times 3}$. 
\end{algorithm}


In more detail, let us be given an input image $x$, which might be attacked. We then perform an iterative optimization task, which is presented in \cref{alg:TETRA}. The optimization objective,
\begin{equation}
L_{\mathrm{CE}}\left( f \left( x+\delta \right), y \right) + \frac{\gamma}{2} \cdot \|\delta\|^2_2
\label{eq:TETRA_step}
\end{equation}
comprises two terms: a cross-entropy (CE) loss, and an $\ell_2$ loss. The first term drives the image to change in a manner that makes the classifier predict the target class $y$. Because the classifier possesses PAG, the CE gradients exhibit a target class related semantic. In contrast, the second term regularizes the transformation so that the transformed image remains close to the input image $x$. More specifically, this expression is optimized iteratively for $N$ steps via gradient descent, where every step is of size $\alpha$. In addition, to balance these two terms, the hyperparameter $\gamma$ needs to be fine-tuned. 



Our method performs an exhaustive search for the correct class, as we transform the image into each dataset class. When scaling to datasets with many classes, this method becomes impracticable. Hence, we developed fast TETRA (FETRA) -- a method that prunes some classes, saves computations, and allows scaling to large datasets. FETRA operates by pruning the classifier's low-rank class predictions and keeping only the top $k$ classes. These classes, in turn, are classified using TETRA. FETRA can speed up runtime drastically. For example, we can boost performance by $\times 100$ over a dataset with $1000$ classes, when performing FETRA with $k=10$. FETRA, however, relies heavily on the hypothesis that the true class is highly ranked among the classifier's predictions. This assumption can be used maliciously by the attacker, who can exploit this knowledge to modify the attack. Standard attacks focus on changing the image so that the classifier does not predict the true class as the most likely one. Nevertheless, there might be an attack that aims at lowering the rank of an image, not just to the second most likely class, but even further down. 


\subsection{Ranking projected gradient descent}

To be able to evaluate FETRA appropriately, we created a novel attack called ranking projected gradient descent (RPGD). This attack is tailored to our defense, trying to attack it at its known weaknesses. The objective of RPGD is to reduce the rank of the true class, predicted by the classifier, out of the top $k$ most probable classes. We modify the loss term of the PGD algorithm \cite{madry2017towards}, replacing it with a differentiable ranking loss \cite{blondel2020fast}\footnote{ We use the implementation supplied in \url{https://github.com/teddykoker/torchsort}}. Ranking loss is very useful for many tasks such as in ranking statistics. This, however, is a nontrivial expression for a loss term, since it involves ranking and sorting, which are non-differentiable operations. To conclude, we aim at increasing the rank of the true class, attacking FETRA using its worst-case examples. 
