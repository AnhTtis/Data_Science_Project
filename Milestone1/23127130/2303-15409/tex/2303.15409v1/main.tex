\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% My packages
\usepackage{multirow}
\usepackage{multicol}
\usepackage{dblfloatfix}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumitem}
\usepackage{authblk}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand\cb[1]{\textcolor{red}{(\textbf{Chaim}: #1)}}
\newcommand\TB[1]{\textcolor{blue}{(\textbf{Tsachi}: #1)}}
\newcommand\rg[1]{\textcolor{green}{(\textbf{Ganz}: #1)}}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9486} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Classifier Robustness Enhancement Via Test-Time Transformation}

% \author[1]{Tsachi Blau\\
% \texttt{tsachiblau@campus.technion.ac.il}}
% \author[2]{
% Roy Ganz\\
% \texttt{tsachiblau@campus.technion.ac.il}}

% \affil[1]{Institution One}
% \affil[2]{Institution One}

\author[1]{Tsachi Blau}
\author[1]{Roy Ganz}
\author[2]{Chaim Baskin}
\author[2]{Michael Elad}
\author[2]{Alex Bronstein}

\affil[ ]{Department of \{Electrical Engineering\textsuperscript{1}, Computer Science\textsuperscript{2}\}, Israel Institute of Technology}

\affil[ ]{\emph{ \{tsachiblau, ganz\}@campus.technion.ac.il \\ \{chaimbaskin, elad, bron\}@cs.technion.ac.il}}



% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
   It has been recently discovered that adversarially trained classifiers exhibit an intriguing property, referred to as perceptually aligned gradients (PAG). PAG implies that the gradients of such classifiers possess a meaningful structure, aligned with human perception. Adversarial training is currently the best-known way to achieve classification robustness under adversarial attacks. The PAG property, however, has yet to be leveraged for further improving classifier robustness. In this work, we introduce Classifier Robustness Enhancement Via Test-Time Transformation (TETRA) -- a novel defense method that utilizes PAG, enhancing the performance of trained robust classifiers. Our method operates in two phases. First, it modifies the input image via a designated targeted adversarial attack into each of the dataset's classes. Then, it classifies the input image based on the distance to each of the modified instances, with the assumption that the shortest distance relates to the true class. We show that the proposed method achieves state-of-the-art results and validate our claim through extensive experiments on a variety of defense methods, classifier architectures, and datasets. We also empirically demonstrate that TETRA can boost the accuracy of any differentiable adversarial training classifier across a variety of attacks, including ones unseen at training. Specifically, applying TETRA leads to substantial improvement of up to $+23\%$, $+20\%$, and $+26\%$ on CIFAR10, CIFAR100, and ImageNet, respectively.
\end{abstract}

\input{sections/introduction}
\input{sections/background}
\input{sections/related_work}
\input{sections/our_method}
\input{sections/experiments}
\input{sections/discussion_conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{appendix}

\end{document}