%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color,soul}
\usepackage{multirow}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\def\x{\mathbf{x}}
\def\y{\mathbf{y}}
\def\q{\mathbf{q}}

\def\Q{\mathbf{Q}}
\def\H{\mathbf{H}}
\def\l{\bm{\ell}}
\def\w{\bm{\omega}}
\def\L{\mathcal{L}}
\def\D{\mathcal{D}}
\def\det{\text{det}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2023}

\begin{document}

\twocolumn[
\icmltitle{Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhi Chen}{equal,duke}
\icmlauthor{Chudi Zhong}{equal,duke}
\icmlauthor{Margo Seltzer}{ubc}
\icmlauthor{Cynthia Rudin}{duke}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{duke}{Department of Computer Science, Duke University, USA}
\icmlaffiliation{ubc}{Department of Computer Science, University of British Columbia, Canada}

\icmlcorrespondingauthor{Zhi Chen}{zhi.chen1@duke.edu}
\icmlcorrespondingauthor{Chudi Zhong}{chudi.zhong@duke.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity, direct editing); (3) investigating sudden changes in the shape functions. Experiments demonstrate the fidelity of the approximated Rashomon set and its effectiveness in solving practical challenges.
\end{abstract}

\section{Introduction}
A key ingredient for trust in machine learning models is \textit{interpretability}; it is more difficult to trust a model whose computations we do not understand. 
However, building interpretable models is not easy; even if one creates a model that is sparse and monotonic in the right variables, it likely is still not what the domain expert is looking for. In fact, domain experts often cannot fully articulate the constraints their problem requires. This leads to an often painful iterative process where a machine learning algorithm, which is typically designed to produce only one model, is now asked to produce another, and possibly many more after that. Hence, the  classical machine learning paradigm is broken because it was not designed for \textit{choice} among models. In other words, the classical machine learning paradigm was not designed for the situation in which domain experts would be able to see and understand a model, criticize it, and expect it to change instantly in a specific way.

Here, we work in a new paradigm where an \textit{algorithm produces many models from which to choose}, instead of just one \cite{breiman2001statistical,xin2022exploring,SemenovaRuPa2022}. Algorithms in this new paradigm produce or approximate the \textit{Rashomon set} of a given function class.
The Rashomon set is the set of models that are approximately as good as the best model in the class. That is, it is the set of all good models. Rashomon sets for many real problems are surprisingly large \cite{d2020underspecification,marx2020predictive, hsu2022rashomon}, and there are theoretical reasons why we expect large numbers of simple, good models to exist \cite{SemenovaRuPa2022}. The question we ask here is how to explicitly find the Rashomon set for the class of \textit{sparse generalized additive models} (sparse GAMs).

GAMs are one of the most widely used forms of interpretable, predictive models \cite{hastie1990generalized, lou2013accurate,agarwal2021neural} and have been applied to complex problems such as medical records \cite{caruana2015intelligible,lengerich2022automated}, where they can provide the same accuracy as the best black box models. Sparse GAMs include the class of scoring systems, which are widely used in hospitals for almost every subfield of medicine \cite{kessler2005world,moreno2005saps,six2008chest} and in criminal justice for a century \cite{burgess1928factors, borden1928factors,
austin2010kentucky,latessa2010creation}.  
Thus, an important previously-missing piece of the Rashomon set paradigm is \textit{how to obtain the Rashomon set of sparse GAMs.} 
%This is the contribution of this work.

Because the Rashomon set of sparse GAMs is continuous and has no analytical form, we present methods to approximate it, knowing that it is convex for each support set. We first study how to approximate the Rashomon set with an ellipsoid for a fixed support set. Our gradient-based optimization algorithm is able to find the largest volume ellipsoid inscribed in the true Rashomon set. Leveraging the properties of ellipsoids, we can also efficiently approximate Rashomon sets of numerous support sets that are subsets of the original support set. We show experimentally in Section \ref{sec:exp} that this approximation captures most of the Rashomon set while not including too many models outside of it. 

Approximating the Rashomon set by ellipsoids enables us to solve many practical challenges of GAMs through efficient convex optimization.
We first use our Rashomon set approximation to study the importance of variables among a set of well-performing models, called \textit{variable importance range}. This is essentially the Model Class Reliance, which measures the variable importance of models in the Rashomon set \cite{fisher2019all}; previously, ranges of variance importance have been studied only for linear models \cite{fisher2019all} and tree models \citep{xin2022exploring,smith2020model}. 
We also show how to efficiently use the ellipsoid approximation to search for near-optimal models with monotonicity constraints. 
The Rashomon set empowers users to be able to arbitrarily manipulate models. As users edit models, their edits are either already in the Rashomon set (which we can check easily) or they can easily be projected back into the Rashomon set using the technique in Section \ref{sec:proj}. Thus, being able to approximate the full Rashomon set for GAMs brings users entirely new functionality, going way beyond what one can do with just a set of diverse models \cite{danna2007generating,ahanor2022diversitree, hara2017enumerate, hara2018approximate,ruggieri2017enumerating, ruggieri2019complete,kissel2021forward}.
%\citep{danna2007generating,ahanor2022diversitree}, linear models \citep{hara2017enumerate}, rule lists or rule sets \citep{hara2018approximate}, and decision trees \citep{ruggieri2017enumerating, ruggieri2019complete}.
Also, the ability to sample GAMs efficiently from the approximated Rashomon set allows us to investigate whether the variations or sudden changes observed in a single model are indicative of true patterns in the dataset or simply random fluctuations.

Since our work provides the first method for constructing Rashomon sets for sparse GAMs, there is no direct prior work. There has been prior work on related problems, such as constructing Rashomon sets for decision trees \cite{xin2022exploring}, and the Rashomon sets for linear regression are simply described by ellipsoids \cite{SemenovaRuPa2022}; Rashomon sets for linear regression models have been used for decision making \citep{tulabandhula2014robust}, robustness of estimation \citep{coker2021theory}, and holistic understanding of variable importance \citep{fisher2019all,dong2020exploring,xin2022exploring}. Our work will allow these types of studies to generalize to GAMs.


% \section{Related Works}
 
 
%Other works find diverse near-optimal solutions for integer linear programs  \citet{kissel2021forward} propose a model agnostic method to find collections of accurate models through forward model path selection. Our work differs from these works in that we find the Rashomon set of a class of sparse generalized additive models on real problems. 

%\textbf{Generalized additive models}. Generalized additive models \citep{hastie1990generalized} have been used in many domains \citep{}. Some recent works focus on the simplicity and interpretability for GAMs by applying convex regularizers \citep{} or $\ell_0$ penalty \citep{dedieu2021learning,liu2022fast}.We refer readers to two recent reviews of this area \citep{rudin2022interpretable}.  

% \textbf{Uncertainty quantification (relationship with hessian)}

\section{Background}
A sample is denoted as $(\x,y)$, where $\x$ is the $p$ dimensional feature vector and $y$ is the target. The $j^{th}$ dimension of the feature vector is $x_j$. A generalized additive model (GAM) \citep{hastie1990generalized} has the form
\begin{equation}
    g(E[y]) = \omega_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p)
\end{equation}
where $\omega_0$ is the intercept, $f_j$'s are the shape functions and $g$ is the link function, e.g., the identity function for regression, or the logistic function for classification.
% Since one can add any offset to $f_j$ while subtracting it from $\beta_0$ or other shape functions, we usually \textit{center} each shape function by setting the population mean of $f_j$, i.e., $E_{x\sim \mathcal{X}}[f_j(x_j)]$ to 0. 
Each shape function $f_j$ operates only on one feature $x_j$, and thus the shape function can directly be plotted. This makes GAMs interpretable since the entire model can be visualized through 2D graphs. In practice, a continuous feature is usually divided into bins \cite{lou2013accurate}, thereby its shape function can be viewed as a step function, i.e.,
\begin{equation}
    f_j(x_j) = \sum_{k=0}^{B_j-1}\omega_{j,k}\cdot \bm{1}[b_{j,k}<x_j \leq b_{j,k+1}],
\end{equation}
where $\{b_{j,k}\}_{k=0}^{B_j}$ are the bin edges of feature $j$, leading to  $B_j$ total bins. This is a linear function on the binned dataset whose features are one-hot vectors. $\w = \{\omega_{j,0},\omega_{j,1},\cdots,\omega_{j,B_{j}-1}
\}_{j=1}^p$ is the weight vector. For simplicity, we use this formulation in the rest of our paper.

Given a dataset $\D = \{(\x_i, y_i)\}_{i=1}^n$, %where $\x_i \in \{0,1\}^{\sum_{j=1}^p B_j}$ is the $i$-th observation (after binning) and $y_i$ is the label, 
we use the logistic loss $\L_c(\w, \omega_0, \D)$ as the classification loss. %, where $\w$ can be simplified to $\{\omega_j\}_{j=1}^m$. 
To regularize the shape function, we also consider a weighted $\ell_2$ loss $\L_2(\w)$ on the coefficients and total number of steps in the shape functions $\L_s(\w)$ as penalties. Specifically, $$\L_2(\w):=\sum_{j=1}^{p} \sum_{k=0}^{B_j-1} \pi_{j,k} \omega_{j,k}^2,$$ 
where $\pi_{j,k} = \frac{1}{n}\sum_{i=1}^{n}\bm{1}[b_{j,k}<x_{ij}\leq b_{j, k+1}]$ is the proportion of samples in bin $k$ of feature $j$. The weighted $\ell_2$ term not only penalizes the magnitude of the shape function, but also implicitly \textit{centers} the shape function by setting the population mean to 0: any other offsets would lead to suboptimal $\L_2(\w)$. This is inspired by \citet{lou2013accurate}, which explicitly sets the population mean to 0. To make the shape functions fluctuate less, we penalize the total number of steps in all shape functions, i.e.,
%$$\L_s(\w)\!=\!\!\!\sum_{j=1}^{m-1} \!\!\bm{1}[w_j\!\!\neq\! w_{j+1}]\cdot \bm{1}[j, j+1\textrm{ in same shape function}],$$
$$\L_s(\w)=\sum_{j=1}^{p}\sum_{k=0}^{B_j-1} \bm{1}[w_{j,k}\neq w_{j, k+1}],$$
which is similar to the $
\ell_0$ penalty of \citet{liu2022fast}. Combining classification loss and penalty yields the total loss:
\begin{equation}
    \L(\w, \omega_0, \D) = \L_c(\w, \omega_0, \D) + \lambda_2 \L_2(\w) + \lambda_s \L_s(\w).
\end{equation}

% $Obj(f, \x, \y) = \ell(f, \x, \y) + \lambda_0 ||\w||_0 + \lambda_2 ||\w||_2^2$, where $\lambda_0$ and $\lambda_2$ are regularization parameters. 
Following the definition of \citet{SemenovaRuPa2022}, we define the Rashomon set of sparse GAMs as follows:

\begin{definition}($\theta$-Rashomon set) For a binned dataset $\D$ with $n$ samples and $m$ binary features, where $\w \in \mathbb{R}^m$ defines a generalized additive model. The $\theta$-Rashomon set is a set of all $\w \in \mathbb{R}^m$ with $\L(\w, \omega_0, \D)$ at
most $\theta$:
\begin{equation}
    R(\theta,\D):=\{\w \in \mathbb{R}^m, \omega_0 \in \mathbb{R}: \L(\w, \omega_0, \D) \leq \theta\}.
\end{equation}
\end{definition}

Note that we use $\L(\w, \omega_0)$ to represent $\L(\w, \omega_0, \D)$ and $R(\theta)$ to represent $R(\theta, \D)$ when $\D$ is clearly defined. 

\section{Approximating the Rashomon Set}\label{sec:approx-rset}
\subsection{GAM with Fixed Support Set (Method 1)}
\label{sec:approx:fixed}
Suppose we merge all adjacent bins with the same $\omega_j$. The set of bins after merging is called the support set. For a GAM with fixed support set, the $\L_s$ term is fixed and the problem is equivalent to logistic regression with a weighted $\ell_2$ penalty. For simplicity, in the remaining paper, $\w$ also includes $\omega_0$. In this case, the loss $\L(\w)$ is a convex function, and the Rashomon set $R(\theta)$ is a convex set. Hence, we propose to approximate $R(\theta)$ with an ellipsoid centered at $\w_c$. Specifically, the approximated Rashomon set is: 
\begin{equation}\label{eqn:r_hat}
\hat{R}:= \{\w\in \mathbb{R}^m: (\w-\w_c)^T \Q (\w-\w_c) \leq 1\},
\end{equation}
where $\Q$ and $\w_c$ are the parameters defining the ellipsoid that we optimize given different $\theta$s. We discuss our optimization method next. 

\textbf{Initialization:} Since the support set is fixed, for now, we treat $\L_s$ as a constant. A Taylor expansion on $\L(\w)$ at the empirical risk minimizer $\w^*$ yields:
%\begin{equation}
%    \begin{aligned}
    \begin{eqnarray*}
    \L(\w) &\hspace*{-5pt}\approx \hspace*{-5pt}& \L(\w^*)\! +\! \nabla\L^T (\w\!-\!\w^*) \!+\! \frac{1}{2}(\w\!-\!\w^*)^T \H (\w\!-\!\w^*) \\
    &&+ O((\w-\w^*)^3).
    \end{eqnarray*}
%    \end{aligned}
%\end{equation}
Since $\w^*$ is the empirical risk minimizer, the gradient $\nabla\L=\bf{0}$. If we ignore the higher order terms, we get a quadratic function, i.e., $\L(\w) \approx \L(\w^*) + \frac{1}{2}(\w\!-\!\w^*)^T \H (\w\!-\!\w^*)$. Plugging this quadratic function into the Rashomon set formula, we get exactly an ellipsoid, i.e., $$\frac{1}{2}(\w\!-\!\w^*)^T \H (\w\!-\!\w^*) \leq \theta - \L(\w^*).$$
Given this approximation, we initialize $\w_c$ with $\w^*$ and $\Q$ with $\H/(2(\theta-\L(\w^*))$. 

\textbf{Gradient-based optimization}: The initialization might be imperfect in practice, because the gradient is assumed to be 0, and the higher order terms are completely ignored. Therefore, we propose to further optimize $\Q$ and $\w_c$ through gradient descent. We aim to maximize the volume of $\hat{R}$ and guarantee that almost all points in $\hat{R}$ are also in $R(\theta)$. Mathematically, the objective is
\begin{equation}\label{eqn:obj}
    \min_{\Q, \w_c} \det(\Q)^{\frac{1}{2m}} + C \cdot \mathbb{E}_{\w \sim \hat{R}(\Q, \w_c)}[\max(\L(\w)-\theta, 0)].
\end{equation}
The volume of $\hat{R}$ is proportional to $\det(\Q)^{-1/2}$, therefore we minimize $\det(\Q)^{1/(2m)}$ in the objective. $m$ is included in the exponent to normalize by the number of dimensions. In the second term of Eq~\eqref{eqn:obj}, we draw samples from $\hat{R}(\Q, \w_c)$ and penalize it if the sample is outside $R(\theta)$; $C$ is a large constant to make sure this happens rarely. Note that we can use the reparameterization trick to convert the sample $\w$ drawn from $\hat{R}$ into a purely stochastic part and a deterministic part related to $\Q$ and $\w_c$, and calculate the gradients accordingly. Details are shown in Appendix \ref{app:sample_ellipsoid}.  

\subsection{GAMs with Different Support Sets (Method 2)}
\label{sec:approx:diff}
Method 1 can accurately approximate the Rashomon set given fixed support set. However, if we want the Rashomon set for many different support sets, applying Method 1 repeatedly can be time-consuming. In this subsection, we propose a blocking method, Method 2, to efficiently approximate the Rashomon sets with many smaller support sets after getting an approximated Rashomon set for a large support set by Method 1. 

Approximating the Rashomon set with an ellipsoid can be helpful in approximating the Rashomon sets with smaller support sets. Specifically, if the bins of the smaller support set are generated by merging bins in the original support set, it is equivalent to forcing the coefficients of these merged bins to be the same. Formally, this is a hyperplane $P$ in the solution space defined by a set of linear constraints 
\begin{equation}\label{eqn:P}
P:=\{\w\in \mathbb{R}^m:\omega_{k}=\omega_{k+1}=\cdots=\omega_{\kappa}\},
\end{equation}
where the $k^{th}$ to the $\kappa^{th}$ bins in the original support set are merged to create a single bin in the new support set. The hyperplane's intersection with any ellipsoid, e.g., $\hat{R}$, is still an ellipsoid, which can be calculated analytically. Specifically, if we rewrite $\Q$ in form of block matrices, i.e.,
$$\Q = \left[ 
\begin{array}{c|c|c} 
  \Q_{1:k-1,1:k-1} & \Q_{1:k-1,k:\kappa} & \Q_{1:k-1,\kappa+1:m}\\ 
  \hline 
  \Q_{1:k-1,k:\kappa}^T & \Q_{k:\kappa,k:\kappa} & \Q_{\kappa+1:m,k:\kappa}^T\\ 
  \hline
   \Q_{1:k-1,\kappa+1:m}^T & \Q_{\kappa+1:m,k:\kappa} & \Q_{\kappa+1:m,\kappa+1:m}
\end{array}
\right] , $$
we can obtain the quadratic matrix $\tilde{\Q}$ for the new support set by summing up the $k^{th}$ to $\kappa^{th}$ rows and columns, i.e.,
$$\tilde{\Q} = \left[ 
\begin{array}{c|c|c} 
  \Q_{1:k-1,1:k-1} & \q_{1:k-1} & \Q_{1:k-1,\kappa+1:m}\\ 
  \hline 
  \q_{1:k-1}^T & q_{\textrm{mid}} & \q_{k+1:m}^T\\ 
  \hline
   \Q_{1:k-1,\kappa+1:m}^T & \q_{k+1:m} & \Q_{\kappa+1:m,\kappa+1:m}
\end{array}
\right] , $$
where $q_{\textrm{mid}} = \sum_{i=k}^{\kappa}\sum_{j=k}^{\kappa} Q_{i,j}$, $\q_{1:k-1}$ and $\q_{\kappa+1:m}$ are two column vectors obtained by summing all columns of $\Q_{1:k-1,k:\kappa}$ and $\Q_{\kappa+1:m,k:\kappa}$. The linear term of the original support set is $\l:=\Q\w_c$. Similarly, we can obtain the linear term of the new support set $\tilde{\l}$ by summing up $k^{th}$ to $\kappa^{th}$ elements of the $\l$ vector, i.e.,
$$\tilde{\l}=[\ell_1,\cdots, \ell_{k-1}, \sum_{i=k}^{\kappa} \ell_i, \ell_{\kappa+1},\cdots, \ell_{m}]^T.$$
With this, we can calculate the new center of ellipsoid,
$\tilde{\w}_c = \tilde{\Q}^{-1}\tilde{\l},$ and the upper bound of the new ellipsoid $ u = 1 - \w_c^T\Q\w_c + \tilde{\w}_c^T \tilde{\Q} \tilde{\w}_c.$
This new ellipsoid $R_P:=P\cap\hat{R}$, which satisfies both Equation \ref{eqn:r_hat} and \ref{eqn:P}, is defined as
$$R_P:= \{\w\in \mathbb{R}^m: (\w-\tilde{\w}_c)^T \tilde{\Q} (\w-\tilde{\w}_c) \leq u\},$$
where if $u\leq 0$, $R_P$ is empty. If we want to merge multiple set of bins, the calculations can be slightly modified to summing up sets of rows and columns separately corresponding to the sets of bins to be merged.
$R_P$ serves as a simple approximation of the Rashomon set for the smaller support set. Every $\w$ in $R_P$ is also in the Rashomon set because it is a subset of the original ellipsoid $\hat{R}$, and all points in $\hat{R}$ are in the Rashomon set.  In fact, since the support set gets smaller, $\L_s$ becomes smaller too, and the loss is even lower, i.e., for $\w$ in $R_P$, $\L(\w)\leq \theta - \lambda_0(\kappa-k)$. 

\textbf{Explore different support sets with size $\Tilde{K}$}: Using the blocking method mentioned above, we can approximate the Rashomon set for many different support sets that are the subsets of the original support set. The approximation of the ellipsoid for a smaller support set is more accurate when the size difference between the original support set and the new support set is smaller. Therefore, instead of using a very large support set at the beginning, we use a sparse GAM algorithm such as FastSparse \cite{liu2022fast} with a relatively weaker sparsity penalty to get a support set $S$ whose size $K$ is moderately larger than the size of support sets we want to explore, but covers all bins we want to potentially merge to form the new support set. For simplicity of defining the loss threshold of the Rashomon set, here we consider only support sets with size $\tilde{K}.$ If we want to explore different $\tilde{K}$, we can repeat this process. Suppose our goal is to explore the Rashomon set with loss threshold $\delta$. The first step is to use the methods in Section \ref{sec:approx:fixed} to obtain the ellipsoid $\hat{R}$ approximating the $\theta$-Rashomon set on the original support set, where $\theta=\delta + \lambda_0(K-\tilde{K}).$ Then we can enumerate all possible ways ($\binom{K-p}{K-\tilde{K}}$ ways in total) to merge bins in the original support set to get any subset $\tilde{S}$ whose size is $\tilde{K}$, and calculate the intersection (i.e., ellipsoid) $R_{P_{\tilde{S}}}:= P_{\tilde{S}}\cap \hat{R}$ between $\hat{R}$ and the hyperplane $P_{\tilde{S}}$ corresponding to merging $S$ into $\tilde{S}$. All nonempty $R_{P_{\tilde{S}}}$ are valid approximations of the Rashomon set.

\section{Applications of the Rashomon Set}\label{sec:app}
We now show four practical challenges related to GAMs that practitioners can now solve easily using the approximated Rashomon sets.

\subsection{Variable Importance within the Model Class}\label{sec:vir}
Variable importance can be easily measured for GAMs. For example, Explainable Boosting Machines \cite{lou2013accurate,nori2019interpretml} measures the importance of each variable by the weighted mean absolute coefficient value of bins corresponding to this variable, i.e.,
$$VI(\x_{\cdot, j}) = \sum_{k=0}^{B_j-1} \pi_{j, k} |w_{j,k}|.$$
This measure is based on one model, but given the Rashomon set, we can now provide a more holistic view of how important a variable is by calculating the range of variable importance among many well-performing models; this is called the model class reliance \citep{fisher2019all}. Specifically, we can get $VI_-$ and $VI_+$ as the lower and upper bounds of this range, respectively. 
For example, the range of importance of feature $j$ is defined as 
\begin{equation}
    [VI_-(\x_{\cdot, j}), VI_+(\x_{\cdot, j})] = [\min_{\w \in R(\theta)} VI(\x_{\cdot, j}), \max_{\w \in R(\theta)} VI(\x_{\cdot, j})].
\end{equation}

A feature with a large $VI_-$ is important in all well-performing models; a feature with a small $VI_+$ is unimportant to every well-performing model. We use $\hat{R}$ to approximate $R(\theta)$ and estimate $VI_-$ by solving a linear programming problem with a quadratic constraint, and we estimate $VI_+$ by solving a mixed-integer programming problem. Solving these problems also gives the user a model in the Rashomon set that relies the most (or the least) on a certain feature, which can be useful in many real applications.

\textbf{Lower bound of variable importance}: $VI_-$ of feature $j$ can be obtained by solving the following linear programming problem. 
\begin{equation}\label{prob:vi-}
    \begin{aligned}
        &\min_{[\omega_{j,0},...,\omega_{j,B_j-1}]} \sum_{k=0}^{B_j-1}\pi_{j,k}|\omega_{j,k}|\\
        & \textrm{such that}\;\; (\w-\w_c)^T \Q (\w-\w_c) \leq 1.
    \end{aligned}
\end{equation}
Since $\pi_{j,k}\geq 0$, Problem~\eqref{prob:vi-} can be solved using an LP solver. The objective minimizes variable importance, and the constraint ensures the solution is in the Rashomon set. 

\textbf{Upper bound of variable importance}: The maximization problem cannot be solved through linear programming since $[\omega_{j,0},...,\omega_{j,B_j-1}]$ can be arbitrarily large. Instead, we formulate it as a mixed-integer program. Let $M$ be a relatively large number (e.g., 200 is large enough for real applications) and let $I_k$ be a binary variable. 
\begin{equation}\label{prob:vi+}
    \begin{aligned}
        &\max_{[\omega'_{j,0},...,\omega'_{j,B_j-1}]} \sum_{k=0}^{B_j-1}\pi_{j,k}\omega'_{j,k}\\
        & \textrm{s.t.}\;\; (\w-\w_c)^T \Q (\w-\w_c) \leq 1\\
        & \omega_{j,k} + M\times I_k \geq \omega'_{j,k}, \ \forall k \in \{0, ..., B_j-1\}\\
        & -\omega_{j,k} + M\times(1-I_k)\geq \omega'_{j,k}, \ \forall k \in \{0, ..., B_j-1\}\\
        & \omega_{j,k} \leq \omega'_{j,k}, -\omega_{j,k} \leq \omega'_{j,k}, \ \forall k \in \{0, ..., B_j-1\}.
    \end{aligned}
\end{equation}
The last two constraints ensure that $\omega'_{j,k}$ is defined as the absolute value of $\omega_{j,k}$.
However, solving mixed-integer programs is usually time-consuming. We propose another way to solve the maximization problem. Since $\omega_{j,k}$ can be either positive or negative, we enumerate all positive-negative combinations for $\omega_{j,k}, k\in \{0, ..., B_j-1\}$ and solve the LP problem with the sign constraint enforced (see Algorithm \ref{alg:vi+_constraint} in the Appendix).


\subsection{Monotonic Constraints}
Another use-case for the GAM Rashomon set is finding an accurate model that satisfies monotonicity constraints. This can be obtained by solving a quadratic programming problem. For example, if a user wants $f_j(\x_j)$ to be monotonically increasing, the optimization problem can be formalized as:
\begin{equation}\label{prob:monotonic}
\begin{aligned}
    & \min_{\w} (\w-\w_c)^T \Q (\w-\w_c)\\
    & s.t.\  \omega_{j, k} \leq \omega_{j, k+1}, k\in \{0,..., B_j-1\},
\end{aligned}
\end{equation}
where we check that the solution is $\leq 1$; that is, whether the solution $\w$ is in the approximated Rashomon set.

\subsection{Changes in the shape functions}
\label{sec:change}
Sudden changes (e.g., a jump or a spike) in the shape functions of GAMs are known to be useful signals for knowledge discovery \cite{caruana2015intelligible,lengerich2022automated} and debugging the dataset \cite{chen2021using}. However, due to the existence of many almost equally good models that have different shape functions, it is hard to identify if a change is a true pattern in the dataset or just a random artifact of model fitting. However, with the approximated Rashomon set, we can calculate the proportion of models that have such a change among all near-optimal models and visualize the shape functions (see Section \ref{sec:exp_change}). %The probability measures the confidence of such change.

\subsection{Find the model within the Rashomon set closest to the shape function requested by users}
\label{sec:proj}
As discussed, the main benefit of the Rashomon set is to provide users a choice between equally-good models. After seeing a GAM's shape functions, the user might want to make edits directly, but their edits can produce a model outside of the Rashomon set. We thus provide a formulation that projects back into the Rashomon set, producing a model within the Rashomon set that follows the users' preferences as closely as possible. We find this model by solving a quadratic programming problem with a quadratic constraint. Let $\w_{req}$ be the coefficient vector that the user requests. The problem can be formulated as follows:
\begin{equation}\label{prob:proj}
    \min_{\w} \|\w-\w_{req}\|_2^2 \ s.t. \  (\w-\w_c)^T \Q (\w-\w_c) \leq 1.
\end{equation}
Problem~\eqref{prob:proj} can be easily solved through a quadratic programming solver. 

\section{Experiments}
\label{sec:exp}
Our evaluation answers the following questions: 1. How does our method compare to baseline
methods for approximating the Rashomon set? 2. How does the approximated Rashomon set help us understand the range of variable importance in real datasets? 3. How does the approximated Rashomon set deliver a model that satisfies users' requirements? 4. How does the Rashomon set help us investigate changes in the shape function?

We use four datasets: a recidivism dataset (COMPAS) \citep{LarsonMaKiAn16}, the Fair Isaac (FICO) credit risk dataset \citep{competition}, a Diabetes dataset \citep{smith1988using}, and an ICU dataset MIMIC-II \citep{saeed2002mimic}.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{1\textwidth}
\centering \includegraphics[width=\textwidth]{figures/eps_vs_precision.png}
    \caption{Precision of the approximated Rashomon sets as a function of the Rashomon set threshold $\theta$. The larger the value of $(\theta-\L^*)/\L^*$, the larger the Rashomon threshold. The precision obtained by our method is dominant compared with other baselines, indicating that when the size of the approximated Rashomon sets is similar (similar recall), the Rashomon set approximated by our method is better inscribed in the true Rashomon set. ($\lambda_s=0.001, \lambda_2=0.001$). Hessian is the starting point for our optimization.}
    \label{fig:precision_theta}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\textwidth]{figures/scale_vs_precision.png}
        \caption{Tradeoff between precision and the size of the approximated Rashomon sets. Our method dominates baselines on all four datasets. As the scaling ratio increases, the precision starts to decrease. ($\lambda_s=0.001, \lambda_2=0.001, \theta=1.01\L^*$). Hessian is our starting point. }
    \label{fig:precision_scale}
    \end{subfigure}
    \caption{Precision versus the size of the approximated Rashomon set. }
    \label{fig:precision_baselines}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.235\textwidth]{figures/box_plot_compas_0.0005_0.001_1.01_merge_bins_13.png}
    \includegraphics[width=0.235\textwidth]{figures/box_plot_fico_0.0005_0.001_1.01_merge_bins_24.png}
    \caption{Box plots of the volume ratio and precision ratio of Method 1 over Method 2 across different $\tilde{K}$, for two datasets. It shows Method 2 performs similarly to Method 1. \vspace*{-5pt}}
    \label{fig:blocking_baseline}
\end{figure}

\begin{figure*}[ht]
\begin{subfigure}[b]{0.47\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/diabetes_0.001_0.001_1.01_mcr.png}
\caption{Variable importance range of the Diabetes dataset.}
\end{subfigure}
\begin{subfigure}[b]{0.53\textwidth}
\includegraphics[width=1\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_Glucose.png}
\caption{Shape functions of ``Glucose.''}
\end{subfigure}
\caption{The variable importance range of the Diabetes dataset. ``Not fix'' means the shape function is obtained by solving Problem~\eqref{prob:vi+}. ``Fix'' means coefficients of all other features except ``Glucose'' are set to the same values as in $\w_c$. ($\lambda_s=0.001, \lambda_2=0.001, \theta=1.01\L^*$)}
\label{fig:vir-diabetes}
\end{figure*}

\begin{figure*}
\begin{subfigure}[b]{0.33\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/monotone_mimic2_0.0002_0.001_1.01_PFratio.png}
\caption{Monotonically decreasing}
\end{subfigure}
\begin{subfigure}[b]{0.33\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/proj2_mimic2_0.0002_0.001_1.01_PFratio.png}
\caption{User required case 1}
\end{subfigure}
\begin{subfigure}[b]{0.33\textwidth}
\centering
\includegraphics[width=1\textwidth]{figures/proj1_mimic2_0.0002_0.001_1.01_PFratio.png}
\caption{User required case 2}
\end{subfigure}
\caption{Example shape functions of ``PFratio'' that satisfy users requirement. (a) The shape function of ``PFratio'' should be monotonically decreasing. But it has a large jump at PFratio $\sim$330. (b) Algorithm removes the jump by connecting to the right step. (c) Removes the jump by connecting to the left step. ($\lambda_0=0.0002, \lambda_2=0.001, \theta=1.01\L^*$).}
\label{fig:user-example}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figures/diff_shape_pfratio.png}
    \caption{Different shape functions of ``PFratio.''}
    \label{fig:diff_shape_pfratio}
\end{figure*}


\subsection{Precision and volume of the approximated Rashomon set}
\label{sec:precision}
As we mentioned in Section \ref{sec:approx-rset}, our goal is to create an approximation of the Rashomon set whose volume is as large as possible, while guaranteeing that most points in the approximated Rashomon set are within the true Rashomon set. To measure precision of our approximation, models within and outside the true Rashomon set $R(\theta)$ are considered as ``positive'' and ``negative'' respectively; models within and outside the approximated Rashomon set $\hat{R}$ are considered as ``predicted positive/negative.'' Accordingly, the precision is defined as the proportion of points $(\w, \omega_0)$, equally weighted and  within $\hat{R}$, having $\L(\w, \omega_0)\leq\theta$, i.e., they are in $R(\theta)$. The recall is defined as the proportion of points in $R(\theta)$ that are also in $\hat{R}$. Note that $\textrm{recall} = \frac{\textrm{Vol}(\hat{R}\cap R(\theta))}{\textrm{Vol}(R(\theta))} = \textrm{precision} \times \frac{\textrm{Vol}(\hat{R})}{\textrm{Vol}(R(\theta))}.$ Since $\textrm{Vol}(R(\theta))$ is a constant, given the same volume $\textrm{Vol}(\hat{R})$, recall is proportional to precision. 

To fairly compare with other methods, we first rescale the Rashomon set approximated by baselines to have the same volume as our $\hat{R}$. Then, we sample 10,000 points from our $\hat{R}$ and rescaled baseline Rashomon sets, calculate their loss and estimate the precision. 

One baseline is using the hessian initialization of $\Q$, i.e. $\H/(2(\theta-\L(\w^*)))$, denoted as ``hessian'' in our comparison. Another baseline is to use the identity matrix to approximate $\Q$, i.e., the approximated set is a sphere. 
Another type of baseline comes from fitting GAMs on bootstrap sampled subsets of data. We thus get coefficient vectors from two different methods (logistic regression and EBMs, \citealt{lou2013accurate}, fitted on the same support set) on many subsets of data. This approach only samples a finite set of coefficient vectors rather than producing a closed infinite set such as an ellipsoid, and therefore its volume is not measurable. Thus, we use optimization to find the minimum volume ellipsoid that covers most coefficient vectors. This is the MVEE problem \citep{todd2007khachiyan}; more details are provided in Appendix \ref{app:precision_volume}. Sampling from the posterior distribution of a Bayesian model is also a way to get many different GAMs. We tried Bayesian logistic regression (using Hamiltonian Monte Carlo), but it is too slow ($\geq$6 hours) to converge and produce enough samples to construct the minimum volume ellipsoid. So we did not include this baseline. 

Figure \ref{fig:precision_baselines} compares the precision and volume of our Rashomon set with baselines. Figure \ref{fig:precision_theta} shows our Rashomon set has dominant precision compared with baselines when the volume of the approximated Rashomon sets are the same. In other words, the Rashomon set approximated by our methods is the largest intersection with the true Rashomon set. The Rashomon set approximated by the hessian is worse than ours but better than other baselines, which means our proposed initialization is already relatively better than approximating by a sphere and other baselines. Also, as $\theta$ becomes larger, the hessian method becomes better and sometimes close to the result after optimization. Logistic regression and EBM with bootstrapping do not achieve good precision. This is because coefficient vectors trained by these two methods on many subsets of data can be too similar to each other, leading to ellipsoids with defective shapes that do not match the true Rashomon set boundary even after rescaling. 

In general, shrinking the approximated Rashomon set leads to higher precision and expanding it leads to better recall. Figure \ref{fig:precision_scale} shows the performance of different methods under this tradeoff. Our method still dominates others even when we rescaled the volume with different factors. 


The previous experiments evaluate the performance when the support set is fixed. To explore many different support sets, we use the blocking method (Method 2) from Section \ref{sec:approx:diff}. Method 2 is much faster than Method 1, so we need to check that it performs similarly, according to both the volume and precision of the approximated Rashomon set. %We compare the volume and precision of our blocking method with the baselines. 
The \textit{volume ratio}, defined by $\sqrt[\tilde{K}]{\textrm{Vol}(\hat{R}_{\textrm{method1}})/\textrm{Vol}(\hat{R}_{\textrm{method2}})}$, should be 1 if the two methods perform similarly. The \textit{precision ratio}, the precision of Method 1 over the precision of Method 2, again should be close to 1 if the methods perform similarly. 
Figure \ref{fig:blocking_baseline} shows the precision ratio and volume ratio of 100 different new support sets where $u > 0$, i.e., the approximated Rashomon set is non-empty. Clearly, Method 2 will break down if we choose the support size $\tilde{K}$ to be too small. The results from Figure \ref{fig:blocking_baseline} indicate what we expected: precision and volume ratios close to 1, even for relatively small $\tilde{K}$. 
%The precision ratio is usually around 1 across different $\tilde{K}$, indicating the approximated Rashomon set constructed by the blocking method (Method 2) is comparable with the Rashomon set approximated by optimizing (Method 1). The volume ratio varies from 0.8 to 1.5, which means that Rashomon sets approximated by the two methods do not change by a lot in each dimension. Therefore, the blocking method in general has comparable precision and volume. 
Importantly, Method 2 is much more efficient than Method 1. For example, the running time is  $<$0.001 seconds for Method 2 but ranges from 300 to 1100 seconds for Method 1 on all three datasets (COMPAS, FICO, MIMIC-II). More results are in Appendix \ref{app:precision_volume}.   




\subsection{Variable importance range}

We solve Problem~\eqref{prob:vi-} and \eqref{prob:vi+} to bound the variable importance; results on the Diabetes dataset are shown by dark blue segments in Figure \ref{fig:vir-diabetes}a. The light blue segments show the range with an additional constraint that coefficients of features other than the one we are interested in are fixed. Adding this extra constraint leads to lower $VI_+$ but has negligible impacts on $VI_-$. Feature ``Glucose'' has dominant $VI_-$ and $VI_+$ compared with other features, indicating that for all well-performing GAMs, this feature is the most important. In fact, the large $VI_-$ indicates that Glucose remains important even in the model that relies least on it in the Rashomon set. This makes sense, because an oral glucose tolerance test indicates diabetes if the plasma glucose concentration is greater than 0 two hours into the test. Features ``BMI'' and ``Age'' also have high $VI_-$, indicating both of them are more important than other features. 

Figure \ref{fig:vir-diabetes}b shows the shape functions of $\w_c$ and the coefficient vectors obtained when the lowest and highest variable importance are achieved for ``Glucose.'' All these shape functions follow the same almost-monotonic trend, which means the higher ``Glucose'' level, the more likely it is that a patient has diabetes. The step occurring when ``Glucose'' is less than or equal to 0 corresponds to the fact that glucose concentration greater than 0 at 2 hours into an oral glucose tolerance test is indicative of diabetes.  
%Shape functions generated by $\w_{VI_-}$ with fixed and non-fixed other coefficients do not change much with respect to $\w_c$, while 
Shape functions based on $\w_{VI_+}$ without fixing other coefficients have a larger magnitude for each step; when we do not fix other coefficients, there is more flexibility in the shape functions. 



\subsection{User preferred shape functions}
A significant advantage of the Rashomon set is that it provides the opportunity for users to contribute their domain knowledge or preferences flexibly without retraining the model. Note that all models within the approximated Rashomon set predict equally well out of sample (see Table \ref{tab:test-performance} in Appendix), so the user can choose any of them. We use feature ``PFratio'' in the public MIMIC-II dataset as an example. PFratio is a measurement of lung function; it measures how well patients convert oxygen from the air into their blood. Missing values commonly exist in the real dataset and imputation is widely used. \citet{chen2021using} uses mean imputation, which often causes sudden jumps in GAMs. Figure \ref{fig:user-example} shows the shape function of ``PFratio'' (in gray), in which a sudden jump occurs when ``PFratio'' is around 330. This jump is caused by mean imputation, and has no physical meaning. Since all other steps of this shape function are monotonically decreasing, a potential user requirement might be to make the shape function monotonically decreasing. By solving Problem~\eqref{prob:monotonic} with the constraint on coefficients related to ``PFratio,'' we get the shape function colored in yellow in Figure \ref{fig:user-example}a. The four left steps are only slightly different from $\w_c$. The jump is reduced while the last step is dragged down by 0.2. 

Suppose a user prefers to remove the jump while keeping the last step unchanged (shown in the inset plot in Figure \ref{fig:user-example}b). Fortunately, by solving Problem~\eqref{prob:proj}, we find that the specified shape function is within the Rashomon set (shown by the green curve in Figure \ref{fig:user-example}b). Another user might not like this idea, preferring instead to remove the jump by connecting to the left step (shown in the inset plot in Figure \ref{fig:user-example}c). However, this specified shape function is not within the Rashomon set, and we find the closest solution in green, which still has a small jump at 330 (see the inset plot). Different user-specified shape functions lead to different solutions. The approximated Rashomon set can thus provide a computationally efficient way for users to find models that agree with their expectations, and the data.
Tools like GAMChanger \citep{wang2021gam} allows users to directly edit the GAM, but there is no guarantee that models still perform well after editing.


\subsection{Changes in shape functions}\label{sec:exp_change}
When we see a jump in the shape function, we might want to know whether this jump commonly exists in other well-performing models. Using the approximated Rashomon set, we can give an answer. For example, we might wonder if the sudden change in the shape function of ``PFratio'' in Figure \ref{fig:user-example}a is also present in other well-performing GAMs. We sample 10,000 points from our $\hat{R}$ and find that 7012 samples have a downward jump at the same position. Also, we are surprised to find that in some of the 2988 samples, the jump is instead upward. Figure \ref{fig:diff_shape_pfratio} shows four different shape functions of ``PFratio'' sampled from $\hat{R}$. Note that the diverse set of shape functions is an advantage of the Rashomon set. Using other methods such as logistic regression, which minimizes loss, might always return similar coefficient vectors even with different random seeds. 

The dramatic magnitude change in either direction could be caused by the small number of data points falling in this bin ($\pi_{\textrm{bin}}$ = 1.2e-4). Since the weight is so small, even a tremendous change leads to only a  small impact on the loss. In practice, using our proposed optimization methods in Section \ref{sec:app}, users can reduce or amplify the jump in the shape function or even create a different curve, as in Figure \ref{fig:user-example}. 




\section{Conclusion}
Our work approximates the Rashomon set of GAMs with a more structured and user-friendly space. This enables users to explore, visualize, modify, and gain insights from the shape functions of GAMs. Our work represents a paradigm shift in machine learning, enabling unprecedented flexibility for users and domain experts to interact with models without compromising performance. Our contributions open a door for enhancing human-model interaction through using the new toolkit provided by the Rashomon set.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\newpage
\bibliography{refs}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Summary of datasets}
We show experimental results on four datasets: a recidivism dataset (COMPAS) \citep{LarsonMaKiAn16}, the Fair Isaac (FICO) credit risk dataset \citep{competition} used for the Explainable ML Challenge, the Diabetes dataset \citep{smith1988using}, and an ICU dataset MIMIC-II \citep{saeed2002mimic}. Table \ref{tab:datasets} summarizes all the datasets. 

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|l|}\hline
       Dataset & Samples & Features & Classification task\\\hline\hline
      COMPAS & 6907 & 7 & Predict if someone will be arrested $\leq$ 2 years of release\\\hline
      FICO & 10459 & 23 & Predict if someone will default on a loan \\\hline
      Diabetes & 768 & 9 & Predict whether a pregnant woman has diabetes \\\hline 
      MIMIC-II & 24508 & 17 &  Predict whether a patient dies in ICU\\\hline
    \end{tabular}
    \caption{Summary of datasets}
    \label{tab:datasets}
\end{table}

\section{Sampling Uniformly from Ellipsoid}
\label{app:sample_ellipsoid}
\definecolor{comment}{rgb}{0.2,0.4,0.2}
\def\comment#1{\textcolor{comment}{\textit{// #1 }}}

\begin{algorithm}
\caption{\textit{SampleFromEllipsoid}$(\Q,\w_c)$}
\label{alg:sample_ellipsoid}
\begin{tabbing}
% xxx \= xx \= xx \= xx \= xx \= xx \kill
\textbf{Input}: parameters of the ellipsoid $\Q\in \mathbb{R}^{m\times m}$, $\w_c\in \mathbb{R}^{m}$ \\
\textbf{Output}: a point inside the ellipsoid $\w\in \mathbb{R}^{m}$\\
1: $\mathbf{u}\sim \mathcal{N}(\mathbf{0},\mathbf{I})$ \comment{sample a $m$ dimensional vector from standard multivariate Gaussian distribution}\\
2: $\mathbf{u}\leftarrow \mathbf{u}/\|\mathbf{u}\|_2$ \comment{normalize it to get a unit-vector}\\
3: $r\sim U(0,1)$ \comment{get a sample from uniform distribution}\\
4: $r\leftarrow r^{1/m}$ \comment{rescale to get the radius of a sample in a unit sphere}\\
5: $\y\leftarrow r\mathbf{u}$ \comment{$\mathbf{y}$ is a random point inside a unit sphere}\\
6: $\mathbf{\Lambda}, \mathbf{V}=\textrm{Eig}(\mathbf{Q})$ \comment{Eigen-decomposition, diagonal of $\mathbf{\Lambda}$ are the eigenvalues, and columns of $\mathbf{V}$ are eigenvectors}\\
7: $\x \leftarrow \mathbf{\Lambda}^{-\frac{1}{2}}\mathbf{V}^T \y$ \comment{get a point in the ellipsoid $\x^T\Q\x\leq 1$}\\
8: $\w = \x + \w_c$ \comment{shift it so that the center is $\w_c$}
\\ \textbf{return} $\w$
\end{tabbing}
\end{algorithm}

Algorithm \ref{alg:sample_ellipsoid} describes the algorithm to uniformly sample from the ellipsoid $\{\w\in \mathbb{R}^m: (\w-\w_c)^T \Q (\w-\w_c) \leq 1\}.$ The algorithm first samples a random point inside a high dimensional unit sphere (line 1-5), and applies linear transformation (calculated from $\Q$ and $\w_c$) to get the point in the target ellipsoid (line 6-8). The whole process can be decomposed into a purely stochastic part, i.e., sample in the unit sphere, and a deterministic part which is differentiable. Using this sampling algorithm, we can get samples for objective \ref{eqn:obj} and use gradient-based methods to optimize $\Q$ and $\w_c$. In addition, the algorithm is also used to sample data for the problem in Section \ref{sec:change}, and to calculate precisions in Section \ref{sec:precision}.



\section{Precision and volume of the approximated Rashomon set}\label{app:precision_volume}
To run our method, we set the learning rate to 0.0001 and run 1000 iterations. $C$ is set to 500. For the logistic regression and EBM baselines, we sample 2000 coefficient vectors by fitting GAMs on the bootstrap sampled subsets of data. We run logistic regression with $\ell_2$ penalty and EBM with no interaction terms. 
We then find the minimum volume ellipsoid that can cover most coefficient vectors. Given a set of coefficient vectors $\w_{\textrm{samples}}$, we solve the following problem: 
\begin{equation}
    \min_{\Q, \w_c} - \textrm{det}(\Q)^{\frac{1}{2m}} + C\cdot \frac{1}{2000}\sum_{i=1}^{2000} \max(\|\Q^{1/2}(\w_{\textrm{sample}_i} - \w_c)\|^2-1, 0).
\end{equation}
We solve this problem via gradient descent. $C$ is set to 1000. The number of iterations and learning rate in GD are set to 1000 and 0.01, respectively. We initialize $\Q$ by the ZCA whitening matrix and $\w_c$ by the average of $\w_{\textrm{samples}}$.

After rescaling the Rashomon set approximated by baselines, we sample 10,000 points from our $\hat{R}$ and rescaled baseline Rashomon sets, calculate the loss, and get the precision. We include more figures in this appendix that are similar to Figure \ref{fig:precision_baselines} to compare with baselines using different values of $\lambda_s$ and $\theta$.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/all_eps_precision.png}
\caption{Precision of the approximated Rashomon sets as a function of $\theta$. Our method always dominates other baselines. }
    \label{fig:more_eps_vs_precision}
\end{figure}

Figure \ref{fig:more_eps_vs_precision} compares the precision of our method and baselines when the volume is fixed. The Rashomon set approximated by our method has the largest intersection with the true Rashomon set. And this pattern is consistent across all datasets and values of $\lambda_s$. The Rashomon set approximated by hessian has lower precision but is always better than the other baselines. As $\theta$ becomes larger, the hessian method becomes better and sometimes comes close to the result after optimization. 

Figure \ref{fig:more_scale_vs_precision} shows the tradeoff between the size and precision of the approximated Rashomon set for each method. The Rashomon set approximated by our optimization method is better than baselines given different values of $\theta$. 



\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/all_scale_precision.png}
\caption{Tradeoff between precision and the size of the approximated Rashomon set given different $\theta$.}
\label{fig:more_scale_vs_precision}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}\hline
       Dataset & $\tilde{K}$ & precision ratio & volume ratio & time ours (sec) & time baseline (sec) \\\hline\hline
      COMPAS & 15 & 1.01 $\pm$ 0.03 & 1.05 $\pm$ 0.07 & 4.84e-4 $\pm$ 6.25e-5 & 324.85 $\pm$ 8.70\\\hline
      COMPAS & 14 & 1.01 $\pm$ 0.05 & 1.12 $\pm$ 0.12 & 4.76e-4 $\pm$ 9.33e-5 & 296.51 $\pm$ 41.46\\\hline
      COMPAS & 13 & 0.99 $\pm$ 0.01 & 1.20 $\pm$ 0.12 & 4.77e-4 $\pm$ 1.13e-4 & 272.64 $\pm$ 6.64\\\hline
      FICO & 28 & 1.01 $\pm$ 0.04 & 0.90 $\pm$ 0.06 & 9.99e-4 $\pm$ 4.54e-3 & 377.10 $\pm$ 14.95\\\hline
      FICO & 26 & 0.99 $\pm$ 0.06 & 1.10 $\pm$ 0.13 & 5.59e-4 $\pm$ 1.12e-4 & 374.28 $\pm$ 10.24\\\hline
      FICO & 24 & 0.99 $\pm$ 0.02 & 1.29 $\pm$ 0.13 & 5.55e-4 $\pm$ 4.04e-5 & 297.15 $\pm$ 8.26\\\hline
      MIMIC-II & 35 & 1.00 $\pm$ 0.20 & 0.93 $\pm$ 0.10 & 5.89e-4 $\pm$ 1.72e-4 & 1127.10 $\pm$ 141.42\\\hline
      MIMIC-II & 32 & 0.99 $\pm$ 0.01 & 1.10 $\pm$ 0.06 & 5.29e-4 $\pm$ 2.76e-5 & 1067.50 $\pm 12.90$\\\hline
    \end{tabular}
    \caption{Precision, volume, and time comparison between blocking method (Method 2) and optimization (Method 1).}
    \label{tab:change_support_set}
\end{table}

\textbf{GAMs with different Support Sets}: In these experiments, we keep 90\%, 80\%, and 70\% of bins trained with $\lambda_s=0.0005, \lambda_2=0.001$. For the baseline method, $C$ is set to 3000, and the learning rate and the number of iterations are set to 0.001 and 2500, respectively. Since $\binom{K-p}{K-\tilde{K}}$ could be very large, we first sample 10,000 different merging strategies and compare at most 100 $R_{P_{\tilde{S}}}$. Table \ref{tab:change_support_set} shows more detailed results. Merging 30\% of bins for the MIMIC-II dataset leads to empty $R_{P_{\tilde{S}}}$ for 10,000 merging strategies and merging bins for the Diabetes dataset also leads to empty $R_{P_{\tilde{S}}}$. 






\begin{algorithm}[ht]
\caption{\textit{Get$VI_+$FromLP}$(\bm{\pi}_j,  \Q,\w_c)$}
\label{alg:vi+_constraint}
\begin{tabbing}
% xxx \= xx \= xx \= xx \= xx \= xx \kill
\textbf{Input}: proportion of samples in each bin of feature $j$ $\bm{\pi}_j \in \mathbb{R}^{B_j}$, parameters of the ellipsoid $\Q\in \mathbb{R}^{m\times m}$, $\w_c\in \mathbb{R}^{m}$\\
\textbf{Output}: a point inside the ellipsoid $\w\in \mathbb{R}^{m}$\\
1: $Obj^* \leftarrow  -\infty$\\
2: \textbf{for} $I \in \{-1,1\}^{B_j}$ \comment{try all positive-negative combinations for $\omega_{j,k}, k\in \{0,...B_j-1\}$}\\
\ \ \ \quad \comment{solve the LP problem}\\
3: \quad $Obj, \w \leftarrow \max_{\w_j} (\bm{\pi}_j \odot I)^T \w_j$ such that $(\w-\w_c)^TQ(\w-\w_c)\leq 1$ \\
4: \quad \textbf{if} $Obj > Obj^*$:
\\ 
5: \quad \quad $\w^* \leftarrow \w, Obj^* \leftarrow Obj$\\
6: \textbf{return} $\w^*$
\end{tabbing}
\end{algorithm}




\section{Variable importance range}
In this section, we first show more results on variable importance range and then compare the timing for obtaining $VI_+$. 

We show the shape functions of ``Glucose'' when the lowest and highest variable importance are achieved in Figure \ref{fig:vir-diabetes}. When the importance of ``Glucose'' is minimized or maximized, one might be interested in how the shape function changes for other features. We show such variations in Figure \ref{fig:shape_diabetes}. Most features keep the trend as $\w_c$ with some variation in magnitude. 

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_Glucose.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_Pregnancies.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_BloodPressure.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_SkinThickness.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_Insulin.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_BMI.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_DiabetesPedigreeFunction.png}
    \includegraphics[width=0.49\textwidth]{figures/vir_shape_diabetes_0.001_0.001_1.01_Age.png}
    \caption{Shape functions of other features in the Diabetes dataset when the importance of ``Glucose'' is minimized or maximized. }
    \label{fig:shape_diabetes}
\end{figure}

Figure \ref{fig:vir_mimic2_fico} shows the variable importance range on the MIMIC-II, FICO, and COMPAS datasets. For the MIMIC-II dataset (left subfigure), features ``PFratio'', ``GCS'', and ``Age'' have relatively higher $VI_-$, which means these features are in general important for GAMs in the Rashomon set. For the FICO dataset (mid subfigure), features ``ExternalRiskEstimate'' and ``MSinceMostRecentInqexcl7days'' have higher $VI_-$ either fixing or not fixing other coefficients, indicating that these two features are important. Feature ``prior\_count'' in the COMPAS dataset has slightly higher $VI_-$ than feature ``age.'' 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/mimic2_fico_0.001_0.001_1.01_mcr.png}
    \caption{The variable importance range of the MIMIC-II, FICO, and COMPAS datasets. Each line connects $VI_-$ and $VI_+$. ($\lambda_s=0.001, \lambda_2=0.001, \theta=1.01 \L^*$)}
    \label{fig:vir_mimic2_fico}
\end{figure}



As we mentioned in Section \ref{sec:vir}, the lower bound of variable importance $VI_-$ is obtained by solving a linear programming problem with a quadratic constraint, while to get the upper bound of variable importance we need to solve a mixed integer programming problem. We use Python CVXPY package \citep{diamond2016cvxpy, agrawal2018rewriting} for solving LP problems and Cplex 12.8 for MIP problems. Since solving a MIP problem is usually time-consuming, we also propose another way to get $VI_+$. Note that it's the absolute value in the objective that restricts us to use LP solver. Therefore, we enumerate all positive-negative combinations of $\omega_{j,k}, k\in \{0, B_j-1\}$, and then solve LP problem with the sign constraint enforced (see Algorithm \ref{alg:vi+_constraint}). Table \ref{tab:vi+_time} compares the running time by solving MIP and LP problem for $VI_+$. Time consumption for solving LP problem with the sign constraint enforced is usually less than solving a MIP problem. 

\begin{table}[ht]
\centering
\begin{tabular}{|c|cc|cc|}
\hline
\multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{Fix other coefs} & \multicolumn{2}{c|}{Not fix other coefs}\\ \cline{2-5} 
& \multicolumn{1}{c|}{MIP} & LP & \multicolumn{1}{c|}{MIP} & LP\\ \hline
Diabetes & \multicolumn{1}{c|}{18.876 $\pm$ 10.025} & 4.780$\pm$4.996  & \multicolumn{1}{c|}{35.597 $\pm$ 17.413} & 1.961 $\pm$ 2.164 \\ \hline
MIMIC-II  & \multicolumn{1}{c|}{11.450 $\pm$ 6.259}  & 0.497$\pm$ 0.166 & \multicolumn{1}{c|}{14.977 $\pm$ 12.921}  & 0.265 $\pm$ 0.139\\ \hline
COMPAS & \multicolumn{1}{c|}{23.193 $\pm$ 0.176} & 4.138 $\pm$ 0.033 & \multicolumn{1}{c|}{46.782 $\pm$ 5.709} &  3.043 $\pm$ 0.017\\ \hline
FICO & \multicolumn{1}{c|}{11.749 $\pm$ 7.841} & 0.816 $\pm$ 0.766 & \multicolumn{1}{c|}{30.133 $\pm$ 7.11} & 0.425 $\pm$ 0.457\\ \hline
\end{tabular}
\caption{Time comparison in seconds between solving MIP and LP problem with the sign constraint enforced for $VI_+$.}
    \label{tab:vi+_time}
\end{table}



\section{Shape functions of GAMs in the Rashomon set}
Next, we show a diverse set of coefficient vectors sampled from the approximated Rashomon set.  
Figure \ref{fig:shape_combo_diabetes} and Figure \ref{fig:shape_combo_mimic2} depict 100 coefficient vectors (in red) sampled from the ellipsoid and $\w_c$ (in gray), the center of the optimized ellipsoid of the Diabetes and MIMIC-II dataset, respectively. Various different red lines in each subfigure indicate that the approximated Rashomon set contains many different coefficient vectors. And these models are actually within the true Rashomon sets. Many of them may generally follow similar patterns as we can see from the trend of these shape functions, while some of them may have some variations (see Figure \ref{fig:diff_shape_pfratio}). In summary, using the approximated Rashomon set, we can easily get a diverse set of shape functions for each feature. 

\begin{figure}
    \centering
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_Pregnancies.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_Glucose.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_BloodPressure.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_SkinThickness.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_Insulin.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_BMI.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_DiabetesPedigreeFunction.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_diabetes_0.001_0.001_1.01_Age.png}
    \caption{100 different shape functions of the Diabetes dataset. The shape function at $\w_c$ is colored in gray. ($\lambda_s=0.001, \lambda_2=0.001, \theta=1.01\L^*$)}
    \label{fig:shape_combo_diabetes}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Age.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_GCS.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_SBP.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_HR.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Temperature.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_PFratio.png}
    \includegraphics[width=0.23\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Renal.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Urea.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_WBC.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_CO2.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Na.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Bilirubin.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_MetastaticCancer.png}
    \includegraphics[width=0.24\textwidth]{figures/shape_combo_mimic2_0.0002_0.001_1.01_Lymphoma.png}
    
    \caption{100 different shape functions of the MIMIC-II dataset. The shape function at $\w_c$ is colored in gray. ($\lambda_s=0.0002, \lambda_2 = 0.001, \theta=1.01\L^*$)}
    \label{fig:shape_combo_mimic2}
\end{figure}

\section{User preferred shape functions}
In real applications, users might have preference for shape functions that are consistent with their domain knowledge. Our approximated Rashomon set makes it easy to incorporate such user preferences by finding a model within the set that satisfies some constraints. We show two case studies here. 

\textbf{Diabetes}: There is a jump in the shape function of ``BMI'' in Figure \ref{fig:shape_combo_diabetes}. One possible user requirement might be making this BMI shape function monotonically increasing. By solving the quadratic programming problem with linear constraints, we can get the shape functions shown in Figure \ref{fig:monotone_BMI}. The updated shape function for ``BMI'' is monotonically increasing (in yellow). With the constraint, the sudden decrease that occurs at ``BMI''=40 is removed by connecting the left step. Because of the change imposed on ``BMI'', the shape functions of other features are also updated. Almost all of them follow the trend in $\w_c$ with small changes in magnitude. Note that this optimization problem is solved in 0.0007 seconds. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_Pregnancies.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_Glucose.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_BloodPressure.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_SkinThickness.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_Insulin.png}
    \fbox{\includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_BMI.png}}
    \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_DiabetesPedigreeFunction.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_BMI_diabetes_0.001_0.001_1.01_Age.png}
     \caption{Shape functions of the Diabetes dataset with the monotonic constraint on the ``BMI'' (in yellow). The optimization problem is solved in 0.0007 seconds.}
     \label{fig:monotone_BMI}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_Pregnancies.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_Glucose.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_BloodPressure.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_SkinThickness.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_Insulin.png}
    \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_BMI.png}
    \fbox{\includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_DiabetesPedigreeFunction.png}}
    \includegraphics[height=0.18\textwidth]{figures/monotone_DiabetesPedigreeFunction_diabetes_0.001_0.001_1.01_Age.png}
    \caption{Shape functions of the Diabetes dataset with the monotonic constraint on the ``DiabetesPedigreeFunction'' (in yellow). The optimization problem is solved in 0.0006 seconds.}
    \label{fig:monotone_DPF}
    \end{subfigure}

    \caption{Shape functions with monotonic constraints. }
\end{figure}

One might also be interested in making the shape function of ``DiabetesPedigreeFunction'' monotonically increasing. \citet{smith1988using} developed the Diabetes Pedigree Function (DPF) to provide a synthesis of the diabetes mellitus history in relatives and the genetic relationship of those relatives to the subject. In general, the DPF increases as the number of relatives who developed diabetes increases, as the age at which those relatives developed diabetes decreases, and as the percentage of genes that they share with the subject increases. 

The yellow curves in Figure \ref{fig:monotone_DPF} show the updated shape functions after solving the optimization problem. The shape function of ``DiabetesPedigreeFunction'' is now monotonically increasing. Similar to Figure \ref{fig:monotone_BMI}, the updated shape functions of other features follow the trend in $\w_c$ with small changes in magnitude. And this optimization problem is solved in 0.0006 seconds. 



Sometimes a monotonic constraint is not what users want; they might have more specific preferences on the shape functions. Here we show some examples using hypothetical shape functions. Figure \ref{fig:proj_BMI} shows shape functions after imposing two different requirements on ``BMI.'' In Figure \ref{fig:proj_BMI_case1}, the required shape function of ``BMI'' (colored in red in the top-left subfigure) shifts below $\w_c$ but is monotonically increasing. Though the closest shape function in the approximated Rashomon set is also below $\w_c$, it's not necessarily monotonically increasing. It's more likely to match the trend in $\w_c$, because we try to minimize the Euclidean distance between the required shape function and the shape function in $\hat{R}$. Shape functions of other features change only slightly in magnitude.  

Figure \ref{fig:proj_BMI_case2} shows an extreme case. The required shape function of ``BMI'' is totally different from $\w_c$. But after solving the QP problem, the updated shape function shown in green in the top-middle subfigure has a similar trend to $\w_c$. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/proj_BMI_case_1.png}
        \caption{Case 1}
        \label{fig:proj_BMI_case1}
    \end{subfigure}

    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/proj_BMI_case_2.png}
        \caption{Case 2}
        \label{fig:proj_BMI_case2}
    \end{subfigure}
    \caption{Shape functions on the Diabetes dataset after a hypothetical shape function on ``BMI'' is required. The red curve in the top-left subfigure is the required shape function. The shape function colored in green in the top-middle subfigure is the closest shape function within $\hat{R}$.}
    \label{fig:proj_BMI}
\end{figure}


\textbf{MIMIC-II}: 
In Figure \ref{fig:shape_combo_mimic2}, we can see jumps in several shape functions. We can impose monotonic constraints on two features ``PFratio'' and ``Bilirubin'' simultaneously. We want the shape function of ``PFratio'' to be monotonically decreasing, while the shape function of ``Bilirubin'' is monotonically increasing. Figure \ref{fig:monotone_comb} shows the shape functions after optimization. The shape functions of ``PFratio'' and ``Bilirubin'' satisfy the requirement as shown in the inset plots, and the shape functions of other features are only slightly changed. 

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/monotone_combo.png}
    \caption{Shape functions of the MIMIC-II dataset with the monotonic constraints on the ``PFratio'' and ``Bilirubin'' (in yellow). }
    \label{fig:monotone_comb}
\end{figure}



Suppose a user prefers to remove the jump in the shape function of ``PFratio.'' One way is to remove the jump while keeping the last step unchanged (top-left subfigure in Figure \ref{fig:proj_PFratio}a). Fortunately, by solving Problem~\eqref{prob:proj}, we find that the specified shape function is within the Rashomon set (shown by the green curve in the top-mid subfigure in Figure \ref{fig:proj_PFratio}a). Another user might not like this idea and prefers to remove the jump by connecting to the left step (Figure \ref{fig:proj_PFratio}b). However, this specified shape function is not within the Rashomon set, and we find the closest solution in green, which still has a small jump at 330 (see the inset plot). Different user-specified shape functions lead to different solutions. The Rashomon set can serve as a small but computationally efficient space in which users can find a model that is closest to their needs. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=1\textwidth]{figures/proj_PFratio_case_1.png}
        \caption{Case 1}
    \end{subfigure}
\hfill
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=1\textwidth]{figures/proj_PFratio_case_2.png}
        \caption{Case 2}
    \end{subfigure}
    
    \caption{Shape functions on the MIMIC-II dataset after a hypothetical shape function on ``PFratio'' is required. The red curve in the top-left subfigure is the required shape function. The shape function colored in green in the top-middle subfigure is the closest shape function within $\hat{R}$.}
    \label{fig:proj_PFratio}
\end{figure}

\newpage
\section{Test performance}
We now show the test performance of models sampled from our approximated Rashomon set. We compare the test accuracy and AUC between $\w^*$ and $\w$s sampled from $\hat{R}$ on the four datasets with different values of $\theta$ and results are shown in Table \ref{tab:test-performance}. We sample 1000 $\w$ from $\hat{R}$ and show the average and one standard deviation. The larger value of $\theta$ leads to a larger Rashomon set, which means we allow models with higher loss. Therefore, as $\theta$ increases, both accuracy and AUC decrease. But when the constant is slightly larger than 1, such as 1.005 and 1.01, the test performance of models sampled from $\hat{R}$ usually covers the performance achieved by $\w^*$ in one standard deviation. This means in general our approximated Rashomon set can return a diverse set of models without compromising the performance. 


\begin{table}
\centering
\begin{tabular}{|l|c|c|c|l|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Dataset}} & \multicolumn{1}{c|}{\multirow{2}{*}{$\theta$ (constant * $\L^*$)}} & \multicolumn{2}{c|}{$\w^*$} & \multicolumn{2}{c|}{$\w$ sampled from $\hat{R}$} \\ \cline{3-6} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{accuracy} & auc & \multicolumn{1}{c|}{accuracy} & \multicolumn{1}{c|}{auc}\\ \hline\hline
\multirow{4}{*}{COMPAS} & 1.005 & \multicolumn{1}{c|}{\multirow{4}{*}{0.696}} & \multirow{4}{*}{0.748} & 0.683 $\pm$ 0.010 & 0.744 $\pm$ 0.004 \\ \cline{2-2} \cline{5-6} 
 & 1.01 &  & & 0.683 $\pm$ 0.010 & 0.742 $\pm$ 0.005\\ \cline{2-2} \cline{5-6} 
 & 1.05 & & & 0.668 $\pm$ 0.017 & 0.724 $\pm$ 0.012 \\ \cline{2-2} \cline{5-6} 
 & 1.1 &  &  & 0.649 $\pm$ 0.026 & 0.704 $\pm$ 0.021 \\ \hline\hline

 \multirow{4}{*}{FICO} & 1.005 & \multicolumn{1}{c|}{\multirow{4}{*}{0.720}} & \multirow{4}{*}{0.792} & 0.717 $\pm$ 0.003 & 0.791 $\pm$ 0.001 \\ \cline{2-2} \cline{5-6} 
 & 1.01 &  & & 0.716 $\pm$ 0.004 & 0.790 $\pm$ 0.002\\ \cline{2-2} \cline{5-6} 
 & 1.05 & & & 0.708 $\pm$ 0.008 & 0.780 $\pm$ 0.006 \\ \cline{2-2} \cline{5-6} 
 & 1.1 &  &  & 0.700 $\pm$ 0.010 & 0.770 $\pm$ 0.009 \\ \hline\hline

 \multirow{4}{*}{Diabetes} & 1.005 & \multicolumn{1}{c|}{\multirow{4}{*}{0.760}} & \multirow{4}{*}{0.819} & 0.761 $\pm$ 0.004 & 0.818 $\pm$ 0.002 \\ \cline{2-2} \cline{5-6} 
 & 1.01 &  & & 0.760 $\pm$ 0.005 & 0.818 $\pm$ 0.003\\ \cline{2-2} \cline{5-6} 
 & 1.05 & & & 0.758 $\pm$ 0.011 & 0.816 $\pm$ 0.006 \\ \cline{2-2} \cline{5-6} 
 & 1.1 &  &  & 0.755 $\pm$ 0.014 & 0.814 $\pm$ 0.009 \\ \hline\hline

 \multirow{4}{*}{MIMIC-II} & 1.005 & \multicolumn{1}{c|}{\multirow{4}{*}{0.886}} & \multirow{4}{*}{0.803} & 0.886 $\pm$ 0.001 & 0.803 $\pm$ 0.002 \\ \cline{2-2} \cline{5-6} 
 & 1.01 &  & & 0.886 $\pm$ 0.001 & 0.802 $\pm$ 0.002\\ \cline{2-2} \cline{5-6} 
 & 1.05 & & & 0.885 $\pm$ 0.002 & 0.794 $\pm$ 0.005 \\ \cline{2-2} \cline{5-6} 
 & 1.1 &  &  & 0.884 $\pm$ 0.003 & 0.784 $\pm$ 0.009 \\ \hline
 
\end{tabular}
\caption{Test accuracy and AUC comparison between $\w^*$ and $\w$ sampled from the approximated Rashomon set with respect to different $\theta$s.}
\label{tab:test-performance}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2023. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
