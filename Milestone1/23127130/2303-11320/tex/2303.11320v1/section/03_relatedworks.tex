\vspace{-2mm}
\section{Related Work}
\vspace{-2mm}
\paragraph{Interactive image segmentation with click.}
Click-based interactive segmentation methods aim to obtain masks of the targeted objects with reference to user-provided clicks. Early methods \cite{grady2006random, boykov2001interactive,gulshan2010geodesic, kim2010nonparametric} focused on optimization-based solutions. DIOS \cite{DIOS} was the first deep learning method that proposed embedding positive and negative clicks into distance maps, then stacking them together with the image as input to the network. BRS \cite{jang2019brs} proposed an online optimization scheme for interactive segmentation, and f-BRS \cite{fbrs} sped it up by optimizing only the auxiliary variables of the network.  Later methods \cite{chen2021cdnet, firstclick, PseudoClick} have also employed a similar model architecture and provided further improvements. RITM \cite{sofiiuk2021ritm} improved model performance by taking the previous mask along with the click maps and image as input. FocalClick \cite{focalclick} performed prediction and update in localized areas, and has improved the model's efficiency and mask refinement performance.  These works all follow the train/val protocol proposed by DIOS~\cite{DIOS}, and report the performance with the same metrics,  
thus a good research community is formed.
However, the biggest disadvantage of click-based methods is that clicks embed little information, and the model, therefore, requires extensive annotations to segment objects with complicated shapes.

\vspace{-5mm}
\paragraph{Interactive image segmentation with scribbles.}
Compared to click-based segmentation, scribble-based interactive segmentation has a lot fewer methods proposed. 

Early works~\cite{ScribbleSup, GRanking,bai2014error,GFilter} used graph constraints, energy functions, or Gabor filters to deal with scribbles. 
DeepIGeoS \cite{DeepIGeoS} encoded scribbles with geodesic distance transforms and performed mask refinement with it. \cite{IFIS} allowed the sharing of scribble annotations across multiple object regions.  \cite{appearancesimilarity} leveraged the appearance similarity to propagate scribble information to other regions. In the current field, however, there has yet to be a standard training and validation protocol proposed. Researchers would use IOU~\cite{IFIS}, Dice Coefficient~\cite{bai2014error}, and annotating time~\cite{appearancesimilarity} as metrics and report evaluation result on different benchmarks.

\begin{figure*}[t]
\newcommand{\image}{\includegraphics[width=1.98\columnwidth]}
\centering 
\image{Figures/pipeline_v3.pdf} 
\vspace{-2pt}
\caption{The demonstration for the pipeline of ScribbleSeg. We take the image, the scribble maps, and the previous mask as inputs to extract the mask of the target object. The positive and negative scribbles are marked in green and blue.
\textbf{PAM} denotes Prototype Adaption Module. \textbf{CRM} means Corrective Refine Module. The detailed structure of KAM and CRM can be found at the bottom part of the figure.}
\label{fig:pipeline}
\vspace{-4mm}
\end{figure*}

\vspace{-5mm}
\paragraph{Scribble-based video object segmentation.}
DAVIS-2018~\cite{davisinteractive} provide a track for scribble-based video object segmentation, which aims to produce masks for object annotated in all frames of a video, and use scribbles to make target indications and corrections. Although using scribbles to refine masks is an important step in this task, DAVIS-2018~\cite{davisinteractive} only cares about the performance on the whole video sequence, and therefore previous works~\cite{Nagaraja2015VideoSW,FastVOS,GIVOS,IVOS,MIVO} often use a simple module to deal with scribbles and focus on the information propagation between frames.