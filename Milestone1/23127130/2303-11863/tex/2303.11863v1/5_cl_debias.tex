

% \Input{$x_i$, $y_i$, $t_i$, $a_i$}
% \If{}

\section{Bias-aware Continual learning} 
\label{sec:comparison}
We demonstrated that the bias of each task can be transferred by naive CL methods in various continual learning situations, highlighting the need for a new approach to continual learning that considers debiasing each task. To address this need, we propose a simple, yet efficient baseline method, dubbed as \methodnamefull (\ours), that can be easily combined with existing CL methods while preventing forgetting of the learned tasks.


\subsection{\methodnamefull (\ours)}
Our proposed method is inspired by a recent CL and debiasing method, specifically GDumb (Greedy Sampler and Dumb Learner) \cite{prabhu2020gdumb} and DFR (Deep Feature Re-weighting) \cite{kirichenko2022last}. GDumb greedily stores a small number of data into an exemplar memory equally for each class in all tasks and uses them to train a model from scratch during the test time. By learning all tasks at the same time, GDumb can improve the average accuracy over learned tasks. On the other hand, DFR retrains only the classification head using a small number of group-balanced data after training a model from scratch using an entire training data that may contain any dataset bias. The authors demonstrate that even if the model learned the biased feature  representations, DFR can remove them by only re-training the last layer using a small portion of group-balanced data. We emphasize that both methods are simple to implement and easily applicable to various settings, and can achieve better or comparable performance for CL or debiasing, respectively, compared to more complex and recent methods in its literature. 


\begin{table}[t]
\caption{\small {\bf The comparison of methods on Split CIFAR-100S.} The average accuracy and BMR over 10 tasks are shown. The $k$ denotes the size of exemplar memory, and the numbers in parentheses stand for the standard deviations of each result obtained with different seeds. }
\centering
\resizebox{0.85\columnwidth}{!} {
\begin{tabular}{lcc}
\toprule
Method & Avg. Acc. (\%) & Avg. BMR (\%) \\
\midrule
Fine-tuning     & 19.49 (1.46)                & 51.96 (10.96)          \\
+ BGS (k=1000)   & \textbf{42.75 (2.27)}                & \textbf{34.38 (5.70)}           \\
+ BGS (k=2000)   & \textbf{47.44 (2.80)}                & \textbf{30.42 (5.71)}           \\
\midrule
LWF \cite{li2017learning}             & 59.61 (2.04)                & 28.00 (2.38)           \\
+ BGS (k=1000)   & \textbf{65.00 (1.20)}                & \textbf{18.18 (0.84)}           \\
+ BGS (k=2000)   & \textbf{66.88 (1.02)}                & \textbf{16.82 (0.90)}           \\
\midrule
EWC \cite{ewc}            & 35.57 (1.71)                & 46.65 (2.31)           \\
+ BGS (k=1000)   & \textbf{46.20 (1.08)}                & \textbf{31.65 (2.12)}           \\
+ BGS (k=2000)   & \textbf{49.30 (1.37)}                & \textbf{28.49 (1.84)}           \\
\midrule
ER (k=1000) \cite{er}    & 54.77 (1.68)                & 29.17 (3.81)           \\
+ BGS        & \textbf{59.38 (0.98)}                & \textbf{20.89 (2.28)}           \\
ER (k=2000)     & 60.75 (1.12)                & 25.47 (1.41)           \\
+ BGS    & \textbf{65.00 (0.74)}                & \textbf{17.62 (1.46)}           \\
\midrule
PackNet \cite{mallya2018packnet}            & 47.46 (1.52)                & 33.97 (6.10)           \\
+ BGS (k=1000)   & \textbf{47.69 (2.88)}                & \textbf{31.26 (3.93)}           \\
+ BGS (k=2000)   & \textbf{49.39 (2.86)}                & \textbf{29.15 (4.05)}           \\
\midrule
GDumb (k=1000) \cite{prabhu2020gdumb}  & 32.33 (1.98)                & 52.15 (4.95)           \\
GDumb (k=2000)  & 41.31 (1.99)                & 45.43 (3.56)           \\
% \midrule
% BGS (k=1000)     & 34.99 (1.29)                & 29.82 (2.55)           \\
% BGS (k=2000)     & 46.67 (8.22)                & 20.71 (1.3)            \\
\midrule
\midrule
LWF + Group DRO \cite{groupdro} & 59.39 (1.41)                & 23.35 (2.72)          \\
\bottomrule
\end{tabular}
}
\vspace{-.1in}
\label{tab:cifar}
\end{table}


In a similar spirit, the algorithm of our \ours is two steps: first, during CL using a employed typical CL method, \ours stores group-class balanced data over all seen classes and groups in a greedy manner as the same process as GDumb, which shown is in Appendix. \ours then retrains only the classification heads of a neural network trained by the CL method by using the group-class balanced exemplar memory. By doing this process, we expect that \ours can mitigate the bias of the model like DFR while preventing forgetting of previous tasks. Moreover, we emphasize that \ours can be used in conjunction with any existing CL method without any additional hyperparameters. 

\subsection{Performance comparison}
We evaluate our \ours using 10-task sequences on Split CIFAR-100S and 8-task sequences on CelebA$^8$. Each task has a random bias level ranging from 0 to 6 (we did not conduct experiments on Split ImageNet-100 due to the lack of group labels (\ie, watermark labels) in the training dataset). We compare the standard CL methods including GDumb, and combinations with the CL methods and \ours. Additionally, we evaluate naive combination with the best performing regularization CL method for each dataset and a debiasing technique, Group DRO. We tuned the hyperparameters for each CL method and Group DRO based on the average accuracy and BMR up to $T_3$, following the hyperparameter selection protocol used in \cite{mai2022online} and used them to learn the rest of the tasks. 

Table \ref{tab:cifar} and \ref{tab:celeba} present the average accuracy and BMR over all tasks for each method on Split CIFAR-100S and CelebA$^8$. We evaluate replay based methods, ER, GDumb, and \ours, with the two kinds of memory size, which correspond to 10 or 20 images per class, respectively. From the tables, we first observe that applying \ours into the standard CL methods leads to improve BMR in all cases and the average accuracy on Split CIFAR-100S, while slightly dropping the average accuracy on CelebA. In addition, we obviously see that the performance gain by \ours in terms of CL and debiasing performances increases when using a large exemplar memory. We emphasize that such improvements from \ours require any additional hyperparameter tuning for debiasing. On the other hand, although applying Group DRO to CL methods shows good performance on CelebA$^8$ in terms of BMR, it needs additional hyperparameter tuning for Group DRO, which may be prohibitive for practice. Moreover, we observet that its improvement is marginal on Split CIFAR-100S since LWF + Group DRO  may fail to find a good hyperparameter for Group DRO when tuning it with only three tasks, not ten tasks. 

\begin{table}[t]
\caption{\small {\bf The comparison of methods on CelebA$^8$.} The other settings are identical to Table \ref{tab:cifar}. If there is an improvement by \ours, the result is shown in bold.}
\centering
\resizebox{0.9\columnwidth}{!} {
\begin{tabular}{lcc}
\toprule
Method          & Avg. Acc. (\%) & Avg. DCA (\%) \\
\midrule
Fine-tuning     & 79.77 (1.06)        & 34.31 (7.53)  \\
+ BGS (k=320)    & 79.62 (1.24)       & \textbf{31.42 (7.04)}   \\
+ BGS (k=640)    & 79.33 (1.86)        & \textbf{30.30 (6.95)}   \\
\midrule
EWC \cite{ewc}         & 80.19 (1.4)        & 36.30 (9.8)   \\
+ BGS(k=320)     & 79.15 (2.10)        & \textbf{33.87 (11.74)}   \\
+ BGS (k=640)    & 78.98 (1.72)        & \textbf{32.70 (11.64)}   \\
\midrule
ER (k=320) \cite{er}     & 80.99 (0.74)        & 37.68 (6.51)   \\
+ BGS (k=320)    & 80.19 (1.89)        & \textbf{31.04 (5.11)}   \\
ER (k=640)      & 81.00 (0.80)        & 37.50 (5.92)   \\
+ BGS (k=640)    & 79.98 (1.50)        & \textbf{30.49 (2.54)}   \\
\midrule
GDumb (k=320) \cite{prabhu2020gdumb}  & 69.38 (1.19)        & 36.45 (5.54)   \\
GDumb (k=640)   & 72.55 (1.26)        & 42.25 (1.48)   \\
% \midrule
% BGS (k=320)      & 59.75 (1.8)        & 12.53 (3.8)   \\
% BGS (k=640)      & 61.11 (0.5)        & 10.05 (1.4)   \\
\midrule
\midrule
% EWC + MFD \cite{mfd}       & 72.66 (7.6)        & 11.31 (3.6)   \\
EWC + Group DRO \cite{groupdro} & 75.32 (3.58)        & 21.79 (3.98)  \\
\bottomrule
\end{tabular}
}
\label{tab:celeba}
\vspace{-.1in}
\end{table}

\noindent \textbf{Remarks for limitations}. 
Although we demonstrated that \ours can improve average bias metric values in CL scenarios without requiring additional hyperparameter, it is important to note that our method does not fundamentally solve the bias-aware CL problem. Namely, even with \ours, the feature representation of a CL model could be still biased. Furthermore, \ours does not work in the absence of group labels in the training dataset, such as Split ImageNet-100. Nevertheless, we hope that \ours serves the purpose of a standard baseline for the bias-aware CL problems.


% \label{sec:experiemnts}
% In experiments, we investigate the bias of models as well as the accuracy in various scenarios of continual learning using a synthetic dataset, Split CIFAR-100S, and two real world datasets: ACSTravelTime \cite{retiring_adult} and FMoW-WILDS \cite{fmow2018, koh2021wilds}. 
% % we compare our MBC with various approaches using a synthetic dataset, Split CIFAR-100S, and a real world dataset, ACSTravelTime \cite{retiring_adult}.

% \noindent\textbf{Baselines} \hspace{2pt} We adopt a naive finetuning approach and typical continual learning baselines of LWF \cite{li2017learning}, EWC \cite{ewc}, ER \cite{er}. Each of them maintains the past knowledge in different forms of knowledge distillation \cite{hintonKD}, structural regularization, and memory-based rehearsal respectively. In addition, we consider naive combinations of the continual learning method, LWF, and existing debiasing techniques: Group DRO \cite{groupdro}, MMD-based fair distillation (MFD) \cite{mfd}, and label bias correction (LBC) \cite{jiang2020identifying}.

% \noindent\textbf{Implementation details} For Split CIFAR-100S, we use the same setting as in Figure  \ref{sec:case_study}. The details are provided in the supplementary material. For ACSTravelTime, we adopt a fully-connected network having two hidden layers each with 128 hidden nodes. We train the network for 20 epochs using AdamW optimizer \cite{adamw} with a learning rate of 0.0003 and a weight decay of 0.02. We did the grid search for hyperparameters of each method and report the results with the best ones after trained on the first five tasks. To consider continual learning debiased models, we train models on the first task with debiased techniques for all datasets. We give the results after the debiased learning and training details of this in the supplementary material.

% \subsection{Synthetic dataset}
% In this section, we compare \ours with baselines on the whole sequence of Split CIFAR-100S. We evaluate the accuracy and DCA of the models on all learned tasks after learning each task, and report the average accuracy and the average DCA over the tasks. The results are summarized in Figure \ref{fig:splitcifar100s_overall_results}.

% Other than finetuning and EWC, our \ours and all baselines show comparable average accuracy as shown in Figure \ref{fig:splitcifar100s_overall_acc}. However, looking into Figure \ref{fig:splitcifar100s_overall_dca}, existing continual learning approaches show that they do not mitigate the bias as expected. 
% While LWF with MFD reduces the average DCA as learning more tasks, our \ours outperforms it in terms of both the average accuracy and the average DCA throughout the learning.
% For finetuning, the overall bias also decreases, but it is since catastrophic forgettings occurs in the learned tasks and the accuracy between groups become similar.

% \subsection{Real world datasets}
% \noindent\textbf{ACSTravelTime} \hspace{2pt} ACSTravelTime \cite{retiring_adult} is a binary classification task to predict whether commute time of a person in the US is longer than 20 minutes. 
% % It is originally composed of data from different states in the US, and
% For this task, we select 10 states with the most data from 2018 US-wide ACS PUMS data. Since there are large distribution shifts between different states, a model trained for a certain state needs to be retrained for another state.  
% To consider a continual learning scenario, we suppose each state's data is collected and given to a model at different time of the year.
% For the group label, we binarize the race by dividing it into two groups: Caucasian and non-Caucasian. With this dataset, we investigate the change of accuracy and DCA between the two race groups over the states.


% \noindent\textbf{FMoW-WILDS} \hspace{2pt} FMoW-WILDS \cite{fmow2018, koh2021wilds} is a dataset for land use classification based on satellite images. It contains images from different geographical regions, and each region has the different number of the images.
% Without the regions and the classes of few images, we choose 40 classes from 3 different regions (Asia,~,~) and compose 10 different tasks having 4 distinct classes. We set the region as the group label to check the bias over the regions.





