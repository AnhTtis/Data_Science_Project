\section{Introduction}
Continual learning (CL) is essential for a system that needs to learn (potentially increasing number of) tasks from sequentially arriving data. The main challenge of CL is to overcome the \textit{stability-plasticity} dilemma \cite{mermillod2013stability}; when a CL model focuses too much on the stability, it would suffer from low plasticity for learning a new task (and vice versa). Recent deep neural networks (DNNs) based CL methods \cite{ewc, agscl, li2017learning} attempted to address the dilemma by devising mechanisms to attain stability while improving plasticity thanks to the \textit{knowledge transferability} \cite{tan2018survey}, which is one of the standout properties of DNNs. Namely, while maintaining the learned knowledge, the performance on a new task (resp. past tasks) is improved by transferring knowledge of past tasks (resp. a new task). Such phenomena are called the forward and backward transfer, respectively.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/intro_figure.pdf}
    \caption{\small{\bf An illustration for bias transfer in continual learning.} The three tasks sequentially arrive,  where each sample contains blond hair, black hair and hat. When a naive CL model is used to update the model $h_t$ from $T_t$, the gender bias is obtained by the model, and the bias is transferred forward and backward, \eg, the model falsely predicts young man images to ``old'' class in $T_{t-1}$ and $T_{t+1}$.} 
    \label{fig:intro_figure}
    \vspace{-.2in}
\end{figure}

While such DNN-based approaches for CL have been successful to some extent, they have not explicitly considered a more realistic and challenging setting in which the \textit{dataset bias} \cite{torralba2011unbiased} exists; \textit{i.e.}, a training dataset may contain unintended correlations between some spurious features and class labels. 
In such a case, it is widely known that DNNs often dramatically fail to generalize to the test data without the correlation due to learning the spurious features \cite{groupdro, bahng2020learning}.
For instance, consider a DNN that can accurately classify birds in the sky. However, when presented with images of birds outside of their typical sky background, the model may fail due to relying on shortcut strategies that exploit the background \cite{geirhos2020shortcut}.
% a DNN that classifies birds in the sky perfectly may fail on classifying images in which birds are outside the typical sky background when the model has learned a \textit{shortcut} strategy relying on the background \cite{geirhos2020shortcut}. 
This issue has been the subject of various attempts to resolve it, with earlier approaches \cite{learningfromfailure, liu2021just} often being based on empirical observations of DNN behavior, which yielded suboptimal results.


% There have been many attempts to address this issue, with earlier approaches \cite{learningfromfailure, liu2021just} often based on empirical findings about DNNs, resulting in suboptimal results. 
% More recently, there have been efforts to address the bias issue in a more principled way by using a structural causal model (SCM) to clarify the causal relationship between input, label, bias, and context priors \cite{liu2022contextual, seo2022information}. 
% By obtaining direct causal effects from inputs without the confounding influence of bias, these approaches have shown improved performance on various vision tasks, including object classification \cite{liu2022contextual}, semantic segmentation \cite{zhang2020causal}  and few-shot learning \cite{yue2020interventional}, highlighting the importance of causal learning in solving the bias problem.


Now, we claim that the issue of learning spurious correlations in the context of CL can be a significant problem because it can lead to the issue of \textit{bias transfer}. In a recent study \cite{salman2022does}, it is shown that the bias learned by a model can be (forward) transferred to the downstream model even when it is fine-tuned with unbiased downstream task data. 
% that is fine-tuned wi
% remain in the model even when it is fine-tuned with unbiased downstream task data. 
% be transferred to the models fine-tuned with downstream tasks even when the downstream task dataset does not necessarily contain additional bias. 
In CL, this issue can be potentially exacerbated since CL involves learning a sequence of tasks, and the bias transfer can occur in both forward and backward directions. 
% the transfer of bias can even occur when fine-tuning pre-trained models on downstream tasks. In CL, this issue can be potentially exacerbated 
% % is potentially more problematic 
% since it involves learning a sequence of tasks, and the transferred bias can affect not only the future tasks, but also the past tasks. 
For instance, consider an example of domain-incremental learning (Domain-IL) setting shown in Figure \ref{fig:intro_figure}, in which the goal is to incrementally learn the classifier that predicts whether the face in the image is \textit{young} or not, as the training data arrives.  
Now, assume that among three tasks, $T_{t-1}$, $T_{t}$ and $T_{t+1}$, only the dataset for $T_t$ possesses the \textit{gender} bias due to the data imbalance; namely, ``male'' and ``female'' face tends to spuriously correlate with ``old'' and ``young'' class, respectively. 
% Now, assume that the dataset for the $t$-th task, $T_t$, possesses the \textit{gender} bias due to the data imbalance; namely, ``male'' and ``female'' face tends to spuriously correlate with ``old'' and ``young'' class, respectively. 
We argue that when a naive CL method, which does not concern about the bias transfer, is used to update the model $h_t$ from $T_t$, the \textit{gender} bias picked up by the model can adversely affect the prediction for the test images in previous or future task, \textit{i.e.}, $T_{t-1}$ or $T_{t+1}$. 
% Namely, in Figure \ref{fig:intro_figure}, assume the training datasets for $T_{t-1}$ and $T_{t+1}$ do not contain any spurious correlations; \textit{i.e.,} the attributes ``blonde'', ``hat'', or ``gender'' are not correlated with the target class label ``young'' or ``old''. 
In other words, while the models independently trained with $T_{t-1}$ or $T_{t+1}$ would not contain any bias, the naive CL-updated $h_{t}$ and $h_{t+1}$ can falsely predict the test ``male'' images for $T_{t-1}$ or $T_{t+1}$ to be ``old'', respectively, due to the ``gender'' bias in $h_t$ transferring to the predictions for past and future tasks. To the best of our knowledge, this issue has not been carefully investigated in the CL research community. 

% there is a lack of research that is carefully investigating this issue for CL. 




% as shown in Figure \ref{fig:intro_figure}, a model continuously learning different sets of face images to classify a facial attribute can learn spurious correlation between the target (\eg, \textit{young}) and another attribute (\eg, \textit{gender}) from a biased dataset at a specific task. Then, the model would start to learn a new set of images based on the bias and it could lead the model to make wrong decisions relying on the gender for the new task. When this model is used to inference images for the previous tasks, it is also possible to give gender-dependent predictions due to the inherent bias in the model.
% Additionally, the severity of bias transfer in CL may be greater depending on how the learned knowledge is utilized. However, to the best of our knowledge, there is a lack of research that is carefully investigating this issue for CL. 

% We show our method is better than a naive combination of CL, such as LwF \cite{li2017learning}, and debiased learning, such as Group DRO\cite{groupdro}. 
To that end, through systematic and carefully designed experiments, we quantitatively show that the above issue of bias transfer in CL, both forward and backward, indeed exists and significantly affects the model performance. More specifically, using one synthetic and two real-world datasets, we first carry out extensive two-task CL experiments and identify that when a typical CL method focuses on the stability (resp. plasticity), the bias learned from the past task (resp. current task) gets transferred and affects the model learned for the current task (resp. past task). Furthermore, we show such forward and backward bias transfer also exists and even \textit{accumulates} when naively applying CL methods for a longer sequence of tasks with dataset bias. Finally, we present a simple yet strong plug-in method, dubbed as \methodnamefull (\ours), which can be easily combined with any existing CL methods. Using an class-group balanced exemplar memory, \ours retrains the last layer of a DNN-based CL model after learning the last task. As a result, we show that our method can always reduce the bias of a CL model with a slight loss of accuracy at most.
Despite of our improvements, our results clearly call for a fundamental and novel approach for continually learning each task with potential bias while debiasing the task. 

The remainder of this paper is organized as follows. Section \ref{sec:relwork} discusses some related works for continual and debiasing learning, and \ref{sec:setup} introduces the experimental setup. In Section \ref{sec:two_task_studies}, we first investigate the forward and backward transfers of bias in two-task CL scenarios, and in Section \ref{sec:longer}, further study them in longer sequences of tasks. Then, Section \ref{sec:comparison} gives a detailed description of \ours and comparison results with other baseline methods.


% even in a longer sequence for CL and accumulates 



% exists in a longer sequence for CL as well and the bias transfer gets accumulated 



% depending on the emphasis on the stability or plasticity of the CL algorithms, the forward and backward bias transfer occurs, respectively. 



% % show that depending on the emphasis on the stability or plasticity of the CL algorithms,   




% we show that when a certain task in a CL scenario contains a dataset bias, applying naive CL methods to learn such a task would be problematic since they can maintain unwarranted knowledge (\eg, background bias) and transfer it to the models for future or past tasks.
% To test this, we construct a synthetic dataset with color bias, and 
% systematically conduct extensive experiments on various two task scenarios with varying levels of bias.
% We quantitatively identify that the forward and backward transfer of bias indeed exist when naive CL methods are applied. More specifically, we show that 
% a typical CL method preserves the knowledge such that the bias of the knowledge learned from the past task is reused to train on a new task (\textit{i.e.}, forward transfer of bias), resulting in severer bias for the new task. Furthermore, it is shown that the biased knowledge learned from the current task also affects the decision rules for the past tasks to be biased (\textit{i.e.}, backward transfer of bias), and a naive debiasing for the current task could also cause the catastrophic forgetting of the past task.
% Our results clearly call for a principled, novel approach for taking the causal learning into account while continual learning from potentially biased datasets, in order to prevent both bias transfers and forgetting.  


%%%%%%%%%%%% PREVIOUS VERSION %%%%%%%%%%%%
% Continual learning (CL) is a long-lasting open problem, where a model has to learn the sequence of tasks in a continuous manner.
% Continual learning tackles two major issues: 1) to prevent \textit{catastrophic forgetting} \cite{mccloskey1989catastrophic} of past tasks (\ie, stability), 2) to effectively solve a new task simultaneously (\ie, plasticity). Too much focus on one of each could harm the other, which is known as the \textit{stability-plasticity dilemma} \cite{carpenter1987art, mermillod2013stability}. To address the dilemma, numerous studies for neural network-based continual learning was conducted under three categories: structural regularization \cite{ewc, si, ucl, agscl}, dynamic architecture \cite{xxx}, and memory replay \cite{er, AGEM}.

% Although most of continual learning methods only focus on improving stability-plasticity dilemma,  they often obtain unwanted biased models when the training data at a specific task contains severe bias. Namely, decision of the model relies on unintended \textit{spurious correlations} between some features of inputs and class labels \cite{ cadene2019rubi, weinzaepfel2021mimetics, groupdro, bahng2020learning}. For instance, a model would exploit a feature of scene for classifying camel images since most of camel are taken in desert scenes and hence using the scene feature only can yield high accuracy for training dataset.
% However, such models fail to generalize for bias-conflicting samples (\eg, a camel on the beach).
% In this paper, we first study the bias problem in CL setting via extensive experiments on various two task scenarios with different degree of bias. We empirically show that such bias issues could be more problematic in CL setting due to \textit{two sources}, called as the forward and backward transfer of bias. Specifically, after a model learns a biased knowledge at a task, a  typical continual learning method preserves the knowledge such that the bias of the knowledge is reused to train on the new task, (\ie, forward transfer of bias). Furthermore, the biased knowledge learned from the current task influences model's prediction for previous tasks to be more biased (\ie, backward transfer of bias). We further show that applying existing debiasing  techniques as a naive solution for these issues can lead to severe catastrophic forgetting since it does not take into account the stability of continual learning. 

% Inspired by our study of bias transfer, we propose a novel framework to selectively preserve the learned knowledge without bias \textit{simultaneously}, dubbed as \methodnamefull (\ours). Our MDC distills debiased knowledge from the previous model based on the maximum mean discrepancy \cite{gretton2012kernel} and makes the current model learn from it, so that it remembers unbiased knowledge related to the tasks.
% Experimentally, in addition to SplitCIFAR-100S, we adopt real world datasets and show that our MDC can effectively mitigate the bias over all tasks with comparable overall accuracy to various continual learning baselines over the datasets. Moreover, we show that MDC has the improved overall accuracy-bias trade-off and has a flexibility in its usage.
% We hope that our study is a step towards developing an unbiased, fair model for the overlooked but nontrivial bias problem in continual learning.

