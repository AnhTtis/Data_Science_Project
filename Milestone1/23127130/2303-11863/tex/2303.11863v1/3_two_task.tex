\section{Case for CL with two tasks}
\label{sec:two_task_studies}
We begin our analysis by examining CL scenarios consisting of two tasks in sequence. Our goal is to identify \textit{forward} and \textit{backward} transfers of the bias of a CL model through both quantitative analyses and figure out how the CL methods promote each of these transfers. 

\subsection{Forward transfer of bias}
To investigate the forward transfer of the bias, we evaluated CL methods by varying the degree of bias of $T_1$, while that of $T_2$ is fixed to level 0. 
Figure \ref{fig:two_forward} reports bias metric values of $T_2$ along with Normalized $\mathcal{F}-\mathcal{I}$ on three datasets after learning $T_2$ with two different bias levels of $T_1$, \ie, level 0 \& 6. 
In the figure, we plot the results of each CL method by varying their own hyperparameter for controlling the stability-plasticity trade-off. The upper point on each plot represents a lower regularization strength, a smaller memory size, or a lower pruning ratio. 
% We note that for PackNet, we only adjust a pruning ratio for the second task while a pruning ratio for the first task is fixed, because the knowledge learned from $T_1$ would be changed depending on the first pruning ratio. 

The following are our observations from the figures.
First, from the gap of blue triangles in Split CIFAR-100S and Split ImageNet-100 results, we observe that even with simple fine-tuning, the bias of $T_1$ adversely affects one of $T_2$, \ie, forward transfer of bias exists, which is consistent with Salman \etal \cite{salman2022does}. However, the overlapping blue triangles on CelebA show that the bias of $T_1$ does not always persist when not considering the stability, implying that the degree of forgetting for the bias of a model acquired from previous tasks can be different depending on the types of bias.
Second, we observe that when applying CL methods, the gap between colored and uncolored points for similar Normalized $\mathcal{F}-\mathcal{I}$ is mostly larger than fine-tuning. Moreover, the gap increases as Normalized $\mathcal{F}-\mathcal{I}$ becomes lower. Namely, these results clearly demonstrate that CL methods promote the forward transfer of bias because they tend to remember the knowledge of past tasks even if it contains some bias features. Furthermore, the extent of the transfer is increasing as CL methods focus on stability more. 
Finally, we observe that bias of $T_2$ is mostly better when learned after $T_1$ with bias level 0 than with bias level 6, under similar Normalized $\mathcal{F}-\mathcal{I}$. Therefore, we argue that before learning a new task, biases of a CL model obtained from previous tasks should be mitigated for learning the new tasks correctly. 

We additionally report the results on Split CIFAR-100S when the bias level of $T_2$ is 2 or 4 in Appendix, and observe similar trends from Figure \ref{fig:two_forward}. 


% \\ [Note that the model learning tasks with bias level 0 is not unbiased] \\


\subsection{Backward transfer of bias}
Now, we investigate the backward transfer of bias. Figure \ref{fig:two_backward} compares the bias of a model at $T_1$ by varying the bias of $T_2$, while the bias level of $T_1$ is fixed as level 0. We omit the results for PackNet, as it freezes the parameters updated in the previous tasks and thereby the predictions for previous tasks are not changed, \ie, any backward transfer does not occur. 

Figure \ref{fig:two_backward_cifar} and \ref{fig:two_backward_celeba} show the opposite trend of our previous results. Firstly, we observe that the bias gap for $T_1$ under similar Normalized $\mathcal{F}-\mathcal{I}$ is maximized by fine-tuning and minimized by model-freezing. Also, for each CL method, the gap becomes severer as the Normalized $\mathcal{F}-\mathcal{I}$ increases. This means that the more CL methods prioritize plasticity over stability, the more bias obtained from $T_2$ is transferred to $T_1$, i.e., the backward transfer of bias occurs more. Additionally, when the bias level of $T_2$ is 0, $T_1$ consistently exhibits lower bias, highlighting the need to address the potential bias of new incoming tasks. 

We identify that BMRs between colored and uncolored points are nearly identical in Figure \ref{fig:two_backward_imagenet}. To reason about this phenomenon, we analyzed the predictions from models and found out that it may be due to the old-new bias which is an inherent issue of Class-IL --- namely, predictions are biased towards new classes due to the imbalance between old and new task data samples. In other words, although $T_2$ does not contain the carton class, the converted image containing watermarks can be predicted to one of the new classes, leading to no difference in BMR. Please refer to Appendix for the related experimental results and more discussions. 

\noindent\textbf{Remarks on developing a new CL method}. 
From two-task analyses in Figure \ref{fig:two_forward} and \ref{fig:two_backward}, we observed that the bias of each task negatively affects the other tasks even if they do not contain the bias. This appeals that whenever we encounter a bias of an incoming task, we should consider learning the task without the bias and preventing forgetting of the previous tasks at the same time. As a straightforward solution, one may naively consider applying an existing debiasing technique (\eg, Group DRO) to the model obtained after learning a current task by a CL method. However, we can easily expect that the accuracy of previous tasks significantly drops whereas the bias of the current task can be successfully reduced
% with similar accuracy 
(we report the results of this scenario in Appendix). Hence, we argue that it is necessary to develop a novel approach for taking debiased learning into account while continual learning.

 \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/cka_forward.pdf}
    \caption{\small\textbf{CKA on Split CIFAR-100S.} To observe the forward transfer of bias, the CKA between color and grayscale images in $T_2$ is shown according to the bias level of $T_1$ and the regularization strength, after learning up to $T_2$ by EWC.}
    \label{fig:cka_forward}
    \vspace{-.1in}
\end{figure}

\subsection{Feature representation analysis}
To provide more direct evidence of bias transfer, we analyze feature representations extracted from the penultimate layer of a DNN-based model using centered kernel alignment (CKA) with the linear kernel \cite{kornblith2019similarity}. CKA is an isotropic scaling-invariant metric for measuring the similarity between two representations of a model. The two plots in Figure \ref{fig:cka_forward} compare the CKA values on Split CIFAR-100S for EWC under the two-tasks settings similar to Figure \ref{fig:two_forward_cifar}. Namely, we evaluate models after learning $T_2$ by varying the regularization strength and the bias level of $T_1$. We then compute the CKA similarity between color and grayscale images in the test dataset of $T_2$. That means, a low CKA value indicates that representations for each group are different, \ie, the model possesses the color bias more. Figure \ref{fig:cka_forward} clearly shows that as the regularization strength is stronger and the bias of $T_1$ is more severe, CKA values decrease. Thus, we also observe the forward transfer of the bias through the analysis of feature representations. The CKA result showing the backward transfer is reported in Appendix.

