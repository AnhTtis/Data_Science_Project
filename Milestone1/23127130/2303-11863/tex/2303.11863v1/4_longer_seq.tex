
\section{Case for CL with a longer sequence of tasks}
\label{sec:longer}
\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/long_cifar_forward.pdf}
        \caption{\small {Forward transfer of bias}}
        \label{fig:long_forward_cifar}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/long_cifar_backward.pdf}
        \caption{\small {Backward transfer of bias}}
        \label{fig:long_backward_cifar}
    \end{subfigure}
    \caption{\small {\bf Bias transfers in a sequence of 10 tasks on Split-CIFAR100S}. The BMRs of $T_{10}$ or $T_1$ are shown after learning up to $T_{10}$ by CL methods.}
    \label{fig:long_cifar}
    \vspace{-.1in}
\end{figure}

We confirmed in the previous section that the bias is transferred forward and backward in tow-tasks CL scenarios. In this section, we further investigate the bias transferability in a sequence of multiple tasks. 

We note that for all figures in this section, we simplify the visual format for better clarity of the comparison; in detail, we divide a range of the normalized $\mathcal{F}-\mathcal{I}$ into three equal intervals and report a result for each interval. Given a CL scenario, we evaluate CL methods several times for varying their hyperparameters and select one of the results with the highest average accuracy for each interval. 
% Our findings indicate that biases not only transfer across tasks, but also tend to persist and accumulate while learning several tasks.

\subsection{Persistence of bias transfer in longer sequences}
\label{subsec:long_transfer}
% to verify that biases persist in longer CL scenarios,
Firstly, we consider a sequence of 10 tasks on Split CIFAR-100S to observe that the bias transfer exists in longer CL scenarios.
Similar to settings in Figure \ref{fig:two_forward}, we vary the bias level of the first or last task with level 0 or 6, while the bias level of all other tasks is fixed as level 0. The two plots in Figure \ref{fig:long_cifar} show BMR of $T_{10}$ (resp. $T_1$) according to the bias of $T_1$ (resp. $T_{10}$) for each CL method. 
% a long-term effect of the bias in a CL model. 
% we pick the result with similar accuracy among results of which the normalized $\mathcal{F}-\mathcal{I}$ corresponds to each interval. 

Figure \ref{fig:long_cifar} reveals an analogous trend with two-task analyses. Specifically, the two plots exhibit the BMR of $T_{10}$ (resp. $T_1$) always higher when the bias level of $T_1$ (resp. $T_{10}$) is 6.
Furthermore, the gap of BMR between them is at its widest when the normalized $\mathcal{F}-\mathcal{I}$ is low (resp. high). 
Namely, the bias transfers also occur in longer sequences of tasks. 
We additionally display the accuracy of $T_{10}$ and $T_{1}$ for each plot in Appendix and find that the accuracies are roughly the same, meaning that the gap of BMR is due to the bias transfer. 
Finally, we emphasize that from Figure \ref{fig:long_forward_cifar}, the color bias of $T_1$ can \textit{persist} even if a CL model learns nine additional tasks with the bias level of 0, especially when focusing on the stability.
% Hence, we can infer that the bias of a task at any point can influence the biases of all tasks in a CL scenario.

\subsection{Accumulation of the same type of bias}
We now verify whether the \textit{same} type of bias of each task would be \textit{accumulated} by CL methods; namely, when the number of previous tasks with the same type of dataset bias is increasing, the CL models make much more biased predictions for the current task. To this end, we take a sequence of 5 tasks with a bias level of 0 in Split CIFAR-100S. We then randomly select some of the four tasks except the last task and change the bias level of them to 4.

Figure \ref{fig:accumul_cifar} reports BMR values for LWF depending on the number of biased tasks. We clearly check that the BMR of $T_5$ increases as the number of biased tasks increases, and the increase is more significant in the low and middle ranges. Namely, it demonstrates that when tasks with the same type of bias are continuously upcoming, their biases accumulate in the CL model. In addition, we observe that when the number of biased tasks is low in the low range, the gap of BMR is small. It would be because if only the tasks in the middle of sequences have the dataset bias, their dataset bias could not be sufficiently learned under the low plasticity.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/accumul_forward_cifar.pdf}
    \caption{\small {\bf Accumulation of the same type of bias on Split CIFAR-100S.} BMR of $T_5$ is shown depending on the number of biased tasks after up to learning $T_5$ by LWF.}
    \label{fig:accumul_cifar}
    \vspace{-.2in}
\end{figure}

\subsection{Accumulation of the different types of bias}
In order to investigate that the \textit{different} type of biases can also accumulate, we consider sequences of three tasks, each randomly picked from CelebA$^8$. We suppose that each task can include one of two kinds of dataset bias. That is, in the training datasets of each task, the class attribute, ``young'', can spuriously correlate with one of two group attributes, ``gender'' or `smiling''. Then, when bias levels of $T_3$  for both group attributes are fixed to 0, we compare the BMRs of $T_3$ depending on whether or not $T_1$ and $T_2$ have gender or smiling biases, respectively. 

The results are shown in Figure \ref{fig:accumul_celeba}, which displays the degree of gender and smiling bias at $T_3$ for EWC and ER. We find that when $T_1$ and $T_2$ are biased towards gender and smiling, respectively, BMR at $T_3$ are much higher in terms of both group attributes. Moreover, in most cases, we again observe the stronger forward transfer of bias when CL methods focus on the stability more. In Figure \ref{fig:accumul smiling bias}, the gap between colored and uncolored bars for EWC is bigger in the middle range, compared to the low range. We infer that this would be because the dataset bias of $T_2$ could be learned more under higher plasticity, resulting in the bias being more transferred to $T_3$. 

\begin{figure}
    \centering
    \begin{subfigure}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/accumul_gender_celeba.pdf}
        \caption{\small{DCA of $T_3$ for the gender bias}}
        \label{fig:accumul gender bias}
    \end{subfigure}
    \begin{subfigure}{0.7\linewidth}
        \centering 
        \includegraphics[width=\linewidth]{figures/accumul_smiling_celeba.pdf}
        \caption{\small{DCA of $T_3$ for the smiling bias}}
        \label{fig:accumul smiling bias}
    \end{subfigure}
    \caption{\small {\bf Accumulation of different types of bias on CelebA$^8$.} DCAs of $T_3$ for ``gender'' and ``smiling'' attributes are reported.}
    \label{fig:accumul_celeba}
    \vspace{-.1in}
\end{figure}