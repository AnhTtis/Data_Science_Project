% model 얘기 아직 안씀.
\section{Experimental setup}
\label{sec:setup}

\subsection{Notations and problem setting}
We consider the \textit{bias-aware continual learning} which is composed of a sequence of classification tasks which may contain a dataset bias. Each data sample in the $t$-th task $T_t$ consists of the tuple $(x_i,a_i,y_i)$ where $a_i\in\mathcal{A}$ and $y_i\in\mathcal{Y}_t$ are group and class labels of an input $x_i$, respectively. 
% an input, $x_i$, a group label $a_i\in\mathcal{A}$, a class label $y_i\in\mathcal{Y}_t$, and a task label $t\in [T]$.
Unless otherwise noted, we consider the single type of bias in a CL scenario for simplicity of analysis. In addition, the sets of class label, $\{\mathcal{Y}_i\}_{i\in\{ 1, 2, \dots \}}$, can be identical or not, depending on the type of CL scenarios. 

\subsection{Benchmark datasets}
We use one synthetic dataset, Split CIFAR-100S, and two real-world datasets, CelebA$^2$ (or CelebA$^8$) and Split ImageNet-100, which are applied for Task-IL, Domain-IL, and Class-IL settings, respectively. Followings are descriptions for our datasets (more details are in Appendix). 

\noindent\textbf{Split CIFAR-100S} is a modified dataset from Split CIFAR-100 \cite{si, er, vandeven2020brain}, which randomly divides CIFAR-100 \cite{cifar10} into 10 tasks with 10 distinct classes. Similarly as in \cite{wang2020towards}, we modify split CIFAR-100 such that, given a skew-ratio $\alpha\geq 0.5$, half of the classes in each task are skewed toward the grayscale group and the other half toward the color group; namely, the training images of each class are split into $\alpha$ and $1-\alpha$ ratios for each group. Thus, the ``color'' becomes the bias of the dataset. We set 7 bias levels (0-6) by dividing the range of skew-ratio from 0.5 to 0.99 evenly on a log scale for systematic control of the degree of bias. 

\noindent\textbf{CelebA} \cite{liu2015deep} contains more than 200K face images, each annotated with 40 binary attributes. It is notorious for containing representation biases towards specific attributes such as race, age, or gender \cite{torfason2017face,fabbrizzi2022survey}; for instance, the sub-populations of young women and old men are over-represented in the CelebA dataset. Unless otherwise specified, we use ``gender'' and ``young'' attributes as a group and a class label, respectively. We additionally select one or three other attributes and based on them, divide CelebA into two tasks (used in \cref{sec:two_task_studies}) or eight tasks (used in \cref{sec:longer} and \ref{sec:comparison}), which are denoted as CelebA$^2$ or CelebA$^8$, respectively. Since the representation bias in CelebA can be controlled by adjusting imbalances between the class and group labels, we set 7 bias levels varying on the degree of imbalance from 0.5 to 0.99, as the same procedure in Split CIFAR-100S.

% It is known that DNN models trained from ImageNet-1000 rely on various spurious features such as texture \cite{geirhos2018}, watermark \cite{li2022whac}, and co-occurring class pairs \cite{singla2021salient}, but we only consider watermark bias for simplicity of analysis. 
\noindent\textbf{Split ImageNet-100} divides ImageNet-100 \cite{(Imagenet)Deng09} into 10 tasks with disjoint 10 classes which are randomly sampled from original 1000 classes in ImageNet-1000. 
It is known that DNN models trained from ImageNet are biased towards watermark \cite{singla2021salient}; namely, the ImageNet-pretrained models predict an image as the carton class when injecting a watermark to it, since most of carton images in the ImageNet training dataset include the watermark. 
% only carton images in the ImageNet training dataset include the watermark. 
% Thus, 
Following the recent study, we also consider the watermark bias in Split ImageNet-100.
% To study this bias in our analysis, we compare the degree of bias of a CL model depending on the presence of the carton class. 
We utilize two types of test datasets, the original ImageNet validation dataset, and ImageNet-W with watermarks injected by style transfer \cite{li2022whac, gatys2016image}, in order to measure the degree of bias. We set a bias level of a task as 0 or 6, depending on the presence of a carton class in the task.
% , and measure the degree of bias by comparing the accuracy between them.

% \begin{figure*}[t!]
%     \centering
%     \begin{minipage}{.3\linewidth}
%         \begin{subfigure}[t]{\linewidth}
%             \includegraphics[width=\linewidth]{figures/two_cifar_forward.pdf}
%             \caption{Results on Split CIFAR-100S}
%             \label{fig:two_forward_cifar}
%         \end{subfigure}
%         \begin{subfigure}[b]{\linewidth}
%             \includegraphics[width=\linewidth]{figures/two_celeba_forward.pdf}
%             \caption{Results on CelebA$^2$}
%             \label{fig:two_forward_celeba}
%         \end{subfigure}
%         \caption{Forward transfer of bias}
%         \label{fig:two_forward}        
%     \end{minipage}
%     \hfill
%     \begin{minipage}{.3\linewidth}
%         \begin{subfigure}[t]{\linewidth}
%             \includegraphics[width=\linewidth]{figures/two_cifar_backward.pdf}
%             \caption{Results on Split CIFAR-100S}
%             \label{fig:two_backward_cifar}
%         \end{subfigure}
%         \begin{subfigure}[b]{\linewidth}
%             \includegraphics[width=\linewidth]{figures/two_celeba_backward.pdf}
%             \caption{Results on CelebA$^2$}
%             \label{fig:two_backward_celeba}
%         \end{subfigure}
%         \caption{Backward transfer of bias}
%         \label{fig:two_backward}
%     \end{minipage}    
%     \hfill
%     \begin{minipage}{.33\linewidth}
%         \centering
%         \begin{subfigure}[t]{\linewidth}
%             \includegraphics[width=\linewidth]{figures/cka_forward.pdf}
%             \caption{Forward transfer}
%         \end{subfigure}
%         \centering
%         \begin{subfigure}[b]{\linewidth}
%             \includegraphics[width=\linewidth]{figures/cka_backward.pdf}
%             \caption{Backward forward}
%         \end{subfigure}
%         \caption{CKA on Split CIFAR-100S} 
%     \end{minipage}
%     \label{fig:cka}
% \end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=0.9\linewidth]{figures/two_cifar_forward.pdf}
        \caption{\small{Split CIFAR-100S in Task-IL}}
        \label{fig:two_forward_cifar}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=0.9\linewidth]{figures/two_celeba_forward.pdf}
        \caption{CelebA$^2$ in Domain-IL}
        \label{fig:two_forward_celeba}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=0.9\linewidth]{figures/two_imagenet_forward.pdf}
        \caption{Split ImageNet-100 in Class-IL}
        \label{fig:two_forward_imagenet}
    \end{subfigure}
    \vspace{-.05in}
    \caption{\small {\bf Forward transfer of bias in two tasks-continual learning.}}
    \label{fig:two_forward}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=0.9\linewidth]{figures/two_cifar_backward.pdf}
        \caption{Split CIFAR-100S in Task-IL}
        \label{fig:two_backward_cifar}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=0.9\linewidth]{figures/two_celeba_backward.pdf}
        \caption{CelebA$^2$ in Domain-IL}
        \label{fig:two_backward_celeba}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=0.9\linewidth]{figures/two_imagenet_backward.pdf}
        \caption{Split ImageNet-100 in Class-IL}
        \label{fig:two_backward_imagenet}
    \end{subfigure}    
    \vspace{-.05in}
    \caption{\small {\bf Backward transfer of bias in two tasks-continual learning.}}
    \label{fig:two_backward}
    \vspace{-.1in}
\end{figure*}


\subsection{Continual learning and debiasing baselines}
We compare two naive methods and six representative CL methods: \textit{fine-tuning} without any consideration of CL, \textit{model-freezing} with freezing model parameters updated from previous tasks, LWF \cite{li2017learning} and EWC \cite{ewc} for regularization based methods, ER \cite{er} and iCaRL \cite{rebuffi2017icarl} for rehearsal based methods and PackNet \cite{mallya2018packnet} for parameter isolation based methods. We note that each CL method can control the stability-plasticity trade-off by adjusting their own hyperparameters such as the regularization strength, the size of the exemplar memory  or the pruning ratio. We further note that PackNet is designed only for task-IL settings, LWF for task-IL and class-IL, and iCaRL for class-IL settings.
Additionally, we employ a widely used debiasing technique, Group DRO \cite{groupdro}.
For implementation details, please refer to the Appendix.

\subsection{Metrics} 
We utilize \textit{average accuracy} over learned tasks as a metric for CL performance and \textit{Normalized $\mathcal{F} - \mathcal{I}$} as a metric for the relative weight on plasticity and stability. 
% In addition, we compute forgetting ($\mathcal{F}$) and intransigence ($\mathcal{I}$) measures \cite{Chaudhry2018ECCV,cha2021cpr} for evaluating stability and plasticity of a CL method, respectively, and use \textit{Normalized $\mathcal{F} - \mathcal{I}$} as a metric for the relative weight on plasticity and stability. 
The concrete definition of Normalized $\mathcal{F} - \mathcal{I}$ is given below. 
Let $h_t$ and $ h^*_t$ be the classifiers learned up to $T_t$ tasks which are trained by a CL method and the fine-tuning, respectively. 
% =\{(x_{t}^{(i)}, a_{t}^{(i)}, y_{t}^{(i)})\}_{i=1}^{N_t}$ be a test dataset for task $T_t$ where , where $a_{t}^{(i)}\in \mathcal{A}$ is the group label of the input $x_{t}^{(i)}\in\mathcal{X}$, and $y_{t}^{(i)}\in\mathcal{Y}_t$ is the class label where $\mathcal{Y}_t$ is the set of classes of $T_t$. 
The forgetting and intransigence measures \cite{Chaudhry2018ECCV,cha2021cpr}, $\mathcal{F}_t$ and $\mathcal{I}_t$, after learning up to task $T_t$ are then defined as follows:
\begin{align}
\mathcal{F}_t &: \frac{1}{t-1} \sum_{j=1}^{t-1} \max_{l\in[t-1]} \operatorname{A}(h_l, \mathcal{D}_j)-\operatorname{A}(h_t, \mathcal{D}_j) \label{eq:F} \\
\mathcal{I}_t &: \frac{1}{t} \sum_{j=1}^{t} \operatorname{A}(h^*_j, \mathcal{D}_j)-\operatorname{A}(h_j, \mathcal{D}_j), \label{eq:I}
\end{align}
in which $\operatorname{A}(h, \mathcal{D}_t)$ is the test accuracy of a model for a test dataset of $T_t$, $\mathcal{D}_t$. Note that the two measures evaluate the stability and plasticity of a CL method, respectively. 
Then, for each CL scenario and method, the differences between the two measures are normalized by the maximum and minimum values of $\mathcal{F}-\mathcal{I}$, which are obtained by varying the hyperparameter of each CL method. Especially, in the case of regularization based methods, the maximum and minimum values of $\mathcal{F}-\mathcal{I}$ mostly correspond to $\mathcal{F}-\mathcal{I}$ of the fixed model and the fine-tuning. Notably, the Normalized $\mathcal{F}-\mathcal{I}$ indicates the model focuses more on stability as the value becomes lower and on plasticity as it becomes higher.

The degree of bias of the model can be evaluated by observing its behavior for predicting a sample when a bias feature of the sample is changed. Formally, we measure a model bias using the bias-flipped mis-classification rate (BMR):
\begin{align}
    \text{BMR} = \frac{\sum_{\{x_i \in \mathcal{D}|h(x_i)=y_i\}} \mathbb{I}(h(x_i^*)\neq y_i)}{|\{x_i \in \mathcal{D}|h(x_i)=y_i\}|},
    % \text{BMR} = \mathbb{E}_{\{x_i \in \mathcal{D}|h(x_i)=y_i\}} \mathbb{I}(h(x_i^*)\neq y_i),
\end{align}
in which $x_i^*$ is a bias-flipped sample with other features fixed, \eg, changes of presence only for color (for Split CIFAR-100S) or watermark (for Split-ImageNet). Namely, BMR considers the number of falsely predicted samples after only flipping bias features for all correctly predicted samples. 
% We note that BMR has close connection with average treatment effect (ATE) \cite{xxx} widely used in the causality literature. 

In most real-world datasets such as CelebA, it might be challenging to generate bias-flipped samples such as transforming a woman image to look like a man. Thus, for CelebA, we use the difference of classwise accuracy (DCA) \cite{berk2021fairness} as a surrogate metric: 
\begin{align}
\small
\text{DCA}(h, \mathcal{D}_{t}) &= \frac{1}{|\mathcal{Y}_{t}|}\sum_{y\in\mathcal{Y}_{t}}{\max_{a, a'\in\mathcal{A}}} \lvert \operatorname{A}(h, \mathcal{D}_{t}^{y,a}) - \operatorname{A}(h, \mathcal{D}_{t}^{y,a'}) \rvert,  \nonumber 
\end{align}
in which $\mathcal{D}^{y,a}_{t}$ is the subset of $\mathcal{D}_{t}$ that is confined to the
samples with class-group label pair $(y,a)$. DCA means the average (over class) of per-class maximum accuracy difference between domains. Informally, DCA is regarded as an approximation of BMR by calculating the difference of predictions in group levels, not in sample levels.

We note that we use BMR for Split CIFAR-100S and ImageNet-100 and DCA for CelebA. Further note that high BMR and DCA correspond to $h$ possessing large bias. 


% adjusted by the 
% min-max normalization; the maximum value is calculated with maximum $\mathcal{F}$ and minimum $\mathcal{I}$ by setting the second terms of \cref{eq:F} and \cref{eq:I} to 0 and 1, respectively, and the minimum value vice versa.
% we first compute the minimum and the maximum of $\mathcal{F}$ and $\mathcal{I}$ as: $\mathcal{F}_{min} = \operatorname{A}(h_1,\mathcal{D}_{1}) - 1.0$ and $\mathcal{I}_{min} = \operatorname{A}(h^*_2,\mathcal{D}_{2}) - 1.0$; for the maximum,  $\mathcal{F}_{max} = \operatorname{A}(h_1,\mathcal{D}_{1})$ and $\mathcal{I}_{max}= \operatorname{A}(h^*_2,\mathcal{D}_{2})$. Then we use $(\mathcal{F} - \mathcal{I})_{min}$ and $(\mathcal{F} - \mathcal{I})_{max}$ for normalization.
% that can be achieved theoretically. 


% \subsection{Continual learning scenario}
% We consider the task-incremental learning for Split CIFAR-100S and the domain-incremental learning for CelebA; the former assumes tasks with different class labels and a multi-headed model and the latter assumes tasks with the same class labels and a single-headed model. 
% % a task identifier $t \in {\mathcal{T}}\triangleq \{1,2,3,\cdots\}$ is given during inference time 
% % and the latter assumes that \cite{van2019three}
% We further assume that the domain of a training sample is known. 

% For simplicity of our analysis, in this section, we only considered the scenario of sequentially learning \textit{two} tasks. For Split CIFAR-100S, we randomly chose 2 out of 10 tasks in every run.  
% and reported the averaged results over 4 different runs
% . We denote the $t$-th task as $T_t$ with $t\in\{1,2\}$.
% We denote the first and second task as $T_1$ and $T_2$, respectively.

