%=====================================
\section{Related Work}
%=====================================

\noindent \textbf{Large Language Models (LLMs)}
We are fortunate to witness the emergent abilities~\cite{wei2022emergent} of Large Language Models (LLMs,~\citealt{2020_gpt3, 2022_PaLM, 2022_gopher}) very recently. With qualitative progress on model \underline{scales} (parameters, training corpus, training compute, etc.~\citealt{2020_gpt3, 2022_PaLM}) and training \underline{strategies} (code tuning, instruction tuning, human feedback, chain-of-thought tuning, etc.~\citealt{2021_codex, 2022_flan, 2022_instructgpt, 2022_flant5}), LLMs show unprecedented reasoning and/or memorization abilities and benefit diverse NLP tasks. 


\noindent \textbf{In-context Learning (ICL)} 
In our work, we use LLMs via ICL since fine-tuning LLMs for every downstream task is not practical. ICL enables LLMs to learn tasks through instructions and/or a few exemplars at inference stage without the need for model parameter updating. There are various approaches to improve the ICL ability of LLMs: (1) \textit{Chain-of-Thought Reasoning (CoT)}~\citep{2022_cot, 2022_zero-cot, 2023_auto-cot} leverages manual or auto-generated rationales to elicit the power of LLMs. (2) \textit{Demonstration Selection (DS)}~\citep{liu-etal-2022-makes, rubin-etal-2022-learning, 2022_select-annot} retrieves appropriate samples as demos via unsupervised sentence similarity or supervised neural retriever. (3) \textit{Self-consistency}~\cite{2023_self-consist} or \textit{Self-ensemble (SE)} runs LLMs multiple times and determines the final results from different results by majority voting. We explore all three variants in Section~\ref{subsec: LLM}.

\begin{figure*}[htbp!]
    \includegraphics[width=\linewidth]{figs/diff_tasks_2.pdf}
    \vspace{-1.5em}
    \caption{Prompts used in Vanilla ICL approach. We box out the instruction in green and demonstrations in blue. Actually the instruction lists all labels and demos usually contain tens of examples. Here we show only three labels and two examples for convenience of visualization. The outputs generated by LLMs are colored in red.}
    \label{figs: diff tasks}
    \vspace{-1em}
\end{figure*}

\noindent \textbf{ICL in Information Extraction Tasks} 
There have been two branches exploring the use of LLMs in few-shot IE tasks with the aid of ICL. The first branch~\cite{2022_gpt_annotator, 2023_synIE} views LLMs as an annotator and generates abundant samples with (pseudo) labels via ICL approaches. They then train SLMs using augmented data to achieve superior performance. Another branch, which includes our work, directly employs LLMs for inference. We contend that our approach differs from theirs in at least two ways. Firstly, previous work has been constrained to a single task type or a limited number of label types, and conducted on different experimental settings. For example, \citet{code2struct_2022} concentrated on the Event Argument Extraction (EAE) task, while \citet{qin2023_benchmark} focused solely on the NER task. Although \citet{jimenez-gutierrez-etal-2022-thinking} studied both NER and RE tasks, they chose datasets with simple schemas (no more than 2 entity types and 5 relation types). These inconsistencies in experimental setups have led to inconclusive findings on whether LLMs outperform SLMs. To address the issue, our work concurrently tackles multiple tasks and conducts experiments on widely-used datasets with varying schema complexities ranging from 4 to 168 label types. Secondly, previous work solely relied on LLMs, while we have developed an adaptive \textit{filter-then-rerank} approach based on comprehensive empirical research, which combines the strengths of both SLMs and LLMs.