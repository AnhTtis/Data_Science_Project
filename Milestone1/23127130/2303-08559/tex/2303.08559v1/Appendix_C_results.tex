\section{Pivot Experiments}
\label{sec: pivot exp}
LLMs require enormous financial and time cost during inference. Therefore we conduct several pivot experiments to prune some experimental settings with (1) potential unaffordable cost and (2) significant unsatisfactory performance.


\subsection{CODEX v.s. InstructGPT}
\label{subsec: two LLMs comparison}
We tend to use CODEX rather than InstructGPT as much as possible in our empirical study since CODEX is now free access to the public. Therefore we choose one representative setting from each dataset and test the performance difference between CODEX and InstructGPT. We show their results in Table~\ref{tab: pivot exp for two LLMs} and observe that there is no significant difference between these two LLMs. Based on this finding, we determine to only use CODEX for empirical study in Section~\ref{subsec: compare result}.

\begin{table}[htbp!]
\setlength\tabcolsep{1.0pt}
\small
\centering
\caption{The F1-score difference between with and without Auto-CoT. We generate rationales by InstructGPT, then adopt \textbf{ICL w. Auto-CoT} approach and use CODEX as our backbone for inference.}
    \label{tab: pivot exp for auto-cot}
    \begin{tabular}{l|ccc}
    \toprule
    \textbf{10-shot train set} & \textbf{FewNERD}  & \textbf{TACREV} & \textbf{ACE05} \\
    \midrule
    wo. Auto-CoT & $\textbf{54.0}${\tiny $(1.4)$} & $\textbf{57.3}${\tiny $(1.8)$} & $\textbf{47.7}${\tiny $(2.8)$} \\
    w. Auto-CoT & $36.6${\tiny $(1.7)$} & $22.0${\tiny $(1.2)$} & $43.1${\tiny $(3.4)$} \\
    \bottomrule
    \end{tabular}
    \vspace{-1em}
\end{table}

We also find out, however, InstructGPT achieves much better results than CODEX in our adaptive \textit{filter-then-rerank} system. Therefor we use InstructGPT in Section~\ref{sec: LLMs are Good Few-shot Reranker} and Section~\ref{sec: filter-then-rerank}. Including this pivot experiments, we pay about 1000 dollars to call InsturctGPT API for this work. 

\begin{table*}[htbp!]
\small
\centering
\caption{The F1-score difference between CODEX and InstructGPT. We adopt \textbf{ICL w. DS} approach for two LLMs.}
    \label{tab: pivot exp for two LLMs}
    \begin{tabular}{l|ccc|cc|ccc}
    \toprule
     & \multicolumn{3}{c|}{\textbf{NER (20-shot)}} & \multicolumn{2}{c|}{\textbf{RE (20-shot)}} & \multicolumn{3}{c}{\textbf{ED (20-shot)}}   \\
    & \textbf{CONLL} & \textbf{OntoNotes} & \textbf{FewNERD}  & \textbf{TACREV} & \textbf{TACRED} & \textbf{ACE05} & \textbf{MAVEN} & \textbf{ERE}\\
    \midrule
    InstructGPT & 77.2 & 47.7 & \textbf{57.2} & \textbf{60.1} & 52.1 & \textbf{49.3} & \textbf{25.4} & \textbf{40.8} \\
    CODEX & \textbf{81.1} & \textbf{55.6} & 55.9 & 59.1 & \textbf{53.6} & 47.9 & 22.8 & 39.0 \\
    \bottomrule
    \end{tabular}
\end{table*} 


\subsection{ICL w. Auto-CoT}
\label{subsec: auto-cot}
This section we explore whether ICL with Auto-CoT approach achieves competitive performance as we expected. Though CODEX achieves similar performance with InstructGPT on IE tasks, we do observe that InstructGPT is able to generate more fluent and reasonable explanations. Therefore we generate rationales using InstructGPT with temperature $t=0.7$. We select several representative settings and compare the performance with and without Auto-CoT as shown in Table~\ref{tab: pivot exp for auto-cot}.

We are frustrated to find Auto-CoT degrades the performance with a large margin. We speculate this degration could be attributed to 3 main reasons. (1) The rationale increase the length of each sample and thus decrease the overall example number in demos. (2) There exists an obvious discrepancy between sentences with and without positive labels. As shown in Figure~\ref{fig: auto-cot}, the rationales are only provided for sentences with positive labels because it is hard to explain why a sentence dose not contain any label. (3) Some auto-generated rationales are low-quality, especially for RE tasks. We would explore better strategy to exploit auto-genertaed rationales in the future work.

\subsection{Random Sampling for LLM Outputs}
\label{subsec: t-value}
Previous work\footnote{https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api} tells us that it is better to set the sampling temperature $t=0$ for tasks with structured outputs, including IE tasks. We validate this conclusion in Table~\ref{tab: temerature}, from which we could see the generated quality when $t=0$ is much higher than the quality when $t \neq 0$. Therefore we set $t=0$ in all main experiments, and do not take self-consistency~\cite{2023_self-consist} into account. Instead we adopt self-ensemble since it does not require the generation randomness.

\begin{table}[htbp!]
\small
\setlength\tabcolsep{1.0pt}
\centering
\caption{The F1-score difference between with and without non-zero $t$ value.}
    \label{tab: temerature}
    \begin{tabular}{l|ccc}
    \toprule
    \textbf{10-shot train set} & \textbf{FewNERD}  & \textbf{TACREV} & \textbf{ACE05} \\
    \midrule
    $t=0$  & $48.5${\tiny $(1.9)$} & $53.7${\tiny $(2.3)$} & $42.9${\tiny $(2.2)$} \\
    \quad + self-ensemble & $\textbf{53.5}${\tiny $(1.3)$} & $\textbf{58.6}${\tiny $(1.5)$} & $\textbf{46.3}${\tiny $(0.8)$} \\
    \midrule
    $t=0.7$ & $40.9${\tiny $(2.3)$} & $39.9${\tiny $(1.2)$} & $35.6${\tiny $(1.0)$} \\
    \quad + self-consistency & $52.1${\tiny $(0.9)$}  & $53.4${\tiny $(1.3)$} & $45.6${\tiny $(3.0)$} \\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Do More Samples in Demos help?}
\label{subsec: more demos}
We wonder whether longer demos bring more performance gains. Thus we gradually increase the number of demos, and observe the changes of performance as the input length increases. We show the results in Figure~\ref{fig: more demos} and observe that (1) The performance of RE task increase consistently. Thus RE task shows potential benefiting from more demos. (2) The performance of NER and ED gradually become stable even degrade with the increase of demos. It frustratingly implies these two tasks have been bounded even before achieving the maximum input length of LLMs.

\begin{figure*}
\centering
    \includegraphics[width=0.9\linewidth]{figs/demo_num.pdf}
    \caption{The F1-score difference with different demo number among three datasets: FewNERD for NER task, TACREV for RE task and ACE05 for ED task. We adopt \textbf{ICL w. DS} approach and use CODEX as the LLM in this experiment. The x-axis in each subfigure represents the number of demos (not the shot value $K$) during ICL.}
    \label{fig: more demos}
\end{figure*}