\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=\linewidth]{figs/filter-then-rerank.pdf}
    \caption{The overall architecture of our adaptive \textit{filter-then-rerank} paradigm. We color easy samples in orange and hard samples in pink. For easy samples, the final predictions are exactly from the SLM-based methods. For hard samples, the top-$N$ predictions from SLMs are fed into LLMs as the format of multiple-choice questions (pink box). The question is paired with demos (green box, we only provide 1 demo for convenience of visualization). LLMs rerank these $N$ candidates and generate the final prediction.
    }
    \label{figs: model}
    \vspace{-1.2em}
\end{figure*}

\vspace{-2.2em}
\section{An Adaptive Reranker: Only Solve \textit{Hard} Samples!}
\label{sec: filter-then-rerank}
\vspace{-0.3em}
We summarize our findings above: (1) SLMs outperform LLMs under most scenarios, particularly with more training samples and complicated schema. (2) SLMs are much more lightwise and economical information extractor than LLMs. (3) LLMs could act as strong rerankers on \textit{hard} samples with which SLMs fail to deal smoothly. Based on these findings, we propose a simple, efficient and effective adaptive reranker to incorporate the strength of SLMs and LLMs. With the minimal intervention of LLMs, \ie only reranking hard samples, our method shows consistent and significant improvement on three few-shot IE tasks, surpassing the SOTA by 2.1\% absolute F1 gains on average. 

\vspace{-0.2em}
\subsection{Method}
We call our method as \textit{adaptive filter-then-rerank} and illustrate it in Figure~\ref{figs: model}. We first train SLMs with supervised approach and use it to predict each test sample. For samples with confidence score higher than a threshold, we retain their predictions from SLMs as final results. Otherwise we select their top-$N$ predictions and rerank them via LLMs. Here the threshold is adaptively determined by maximizing F1-score of the validation set.

As shown in right part of Figure~\ref{figs: model}, the prompt used in our adaptive reranker is composed of demos and an unanswered question. The demos (green part) contain several exemplars and each of them is an answered multiple-choice question as shown in Section~\ref{subsec: multiple-choice question}. The unanswered question (pink part) is exactly a \textit{hard} sample with $N$ candidate labels to be reranked. The LLMs would rerank the candidates by answering this question.

\begin{table*}[htbp]
 \centering
 \caption{Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed \textit{filter-then-rerank} (S+L) methods. The best results are in bold face and the second best are underlined. All results except InstructGPT are averaged over 5 repeated experiments, and sample standard deviations are in the round bracket (the same below). The standard deviations are derived from different sampling few-shot datasets instead of random seeds. Thus high standard deviation values do not mean that no significant difference among these methods.}
 \label{tab: rerank performance}
 \setlength\tabcolsep{1.6pt}
 \small
    \begin{threeparttable}
        \begin{tabular}{@{}ll|ccc|ccc|ccc@{}} 
        \toprule
        & \textbf{Method} & \multicolumn{3}{c|}{\textbf{FewNERD}} & \multicolumn{3}{c|}{\textbf{TACREV}} & \multicolumn{3}{c}{\textbf{ACE}} \\
         & & 5-shot & 10-shot & 20-shot & 20-shot & 50-shot & 100-shot & 5-shot & 10-shot & 20-shot \\
         \midrule
        \parbox[t]{2.0mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{LLM}}}} & CODEX & $53.8$ {\tiny $(0.5)$} & $54.0$ {\tiny $(1.4)$} & $55.9$ {\tiny $(0.5)$} & $59.1$ {\tiny $(1.4)$} & $60.3$ {\tiny $(2.4)$} & $62.4$ {\tiny $(2.6)$} & $47.1$ {\tiny $(1.2)$} & $47.7$ {\tiny $(2.8)$} & $47.9$ {\tiny $(0.5)$} \\
        & InstructGPT & $53.6$ {\tiny $(-)$} & $54.6$ {\tiny $(-)$} & $57.2$ {\tiny $(-)$} & $60.1$ {\tiny $(-)$} & $58.3$ {\tiny $(-)$} & $62.7$ {\tiny $(-)$} & $52.9$ {\tiny $(-)$} & $52.1$ {\tiny $(-)$} & $49.3$ {\tiny $(-)$} \\
        \midrule
         \parbox[t]{2.0mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{SLM}}}} & FSLS / KnowPrompt & $59.4$ {\tiny $(1.5)$} & $61.4$ {\tiny $(0.8)$} & $60.7$ {\tiny $(1.9)$} & $62.4$ {\tiny $(3.8)$} & $68.5$ {\tiny $(1.6)$} &$72.6$ {\tiny $(1.5)$} & $55.1$ {\tiny $(4.6)$} & $63.9$ {\tiny $(0.8)$} & $65.8$ {\tiny $(2.0)$}  \\
        & \quad + Ensemble & $59.6$ {\tiny $(1.7)$} & $61.8$ {\tiny $(1.2)$} & $62.6$ {\tiny $(1.0)$} & $64.9$ {\tiny $(1.5)$} & $71.9$ {\tiny $(2.2)$} & $74.1$ {\tiny $(1.7)$} & $56.9$ {\tiny $(4.7)$} &  $64.2$ {\tiny $(2.1)$} & $66.5$ {\tiny $(1.7)$} \\
        \midrule
         \parbox[t]{0.2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{S+L}}}} & \quad + LLM Rerank & $\underline{60.6}$ {\tiny $(2.1)$} & $\underline{62.7}$ {\tiny $(0.8)$} & $\underline{63.3}$ {\tiny $(0.6)$} & $\underline{66.8}$ {\tiny $(2.6)$} & $\underline{72.3}$ {\tiny $(1.4)$} & $\underline{75.4}$ {\tiny $(1.5)$} & $\underline{57.8}$ {\tiny $(4.6)$} & $\textbf{65.3}$ {\tiny $(1.7)$} & $\underline{67.3}$ {\tiny $(2.2)$}  \\
        & \quad + Ensemble + LLM Rerank & $\textbf{61.3}$ {\tiny $(1.9)$} & $\textbf{63.2}$ {\tiny $(0.9)$} & $\textbf{63.7}$ {\tiny $(1.8)$}& $\textbf{68.9}$ {\tiny $(1.3)$} & $\textbf{74.8}$ {\tiny $(1.3)$} & $\textbf{76.8}$ {\tiny $(1.2)$} & $\textbf{59.5}$ {\tiny $(3.7)$} & $\underline{65.3}$ {\tiny $(1.9)$} & $\textbf{67.8}$ {\tiny $(2.1)$}  \\
        \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table*}

\vspace{-0.5em}
\subsection{Experimental Setup}
We conduct experiments on FewNERD for NER task, TACREV for RE task and ACE05 for ED task. We set K as (5, 10, 20) for NER/ED tasks, and (20, 50, 100) for RE task. We use the competitive SLM-based methods shown in previous experiments (FSLS for NER and ED tasks, KnowPrompt for RE task. Both use \texttt{RoBERTa-large} as backbones) as the filter, and a 175B InstructGPT (\texttt{text-davinci-003}) as the reranker. 

We select the top-3 candidate labels for NER/RE tasks, and top-2 candidate labels for ED task from SLMs' predictions and feed them to LLMs. We also add \texttt{No-label} choice if it is not in the top-$N$ predictions of each sample. We select 20 samples from validation set and write their rationales manually for each dataset. We randomly choose 4 example from them as demos each time. See some demo examples in Appendix~\ref{sec: demo example}. We follow template in \citet{lu-etal-2022-summarization} as our choice-template for TACREV dataset, and write templates used in FewNERD and ACE05 dataset by ourselves. We list all these templates in Appendix~\ref{sec: template}.


\subsection{Main Results}
We compare 5 methods in this section to validate the effectiveness of our \textit{filter-then-rerank} method.

\noindent \textbf{(1) LLM-based ICL approach}: We select the most competitive LLM-based variant shown in Section~\ref{subsec: compare result}, ICL with DS, as a baseline.

\noindent \textbf{(2) FSLS/KnowPrompt}: The most competitive SLM-based methods (FSLS for NER and ED tasks, KnowPrompt for RE task) shown in Section~\ref{subsec: compare result}.

\noindent \textbf{(3) FSLS/KnowPrompt + Ensemble}: We ensemble two SLMs (trained with the same dataset but different seeds), to validate the score gains from reranking is not only due to the ensemble effect.

\noindent \textbf{(4) FSLS/KnowPrompt + Rerank}: Our adaptive reranker with \textbf{one} SLM model as the filter.

\noindent \textbf{(5) FSLS/KnowPrompt + Ensemble+ Rerank}: Our adaptive reranker with ensemble of \textbf{two} SLMs as the filter, to further validate the gains from ensemble and reranking are complementary. 

We overview the results listed in Table~\ref{tab: rerank performance} and observe that our \textit{filter-then-rerank} method achieves consistent and significant improvement on nine different settings across three datasets. The reranker brings a 2.4\% average F1-gains without ensemble (line 3 v.s 5) and 2.1\% F1-gains with ensemble (line 4 v.s 6). Thus we conclude that (1) the reranking approach is effective, and (2) the gains it brings are different and complementary to the ensemble.


\begin{table}
 \centering
  \small
  \setlength\tabcolsep{2.0pt}
   \caption{Ablation study on three datasets. 20-shot settings for FewNERD and ACE05. 100-shot setting for TACREV. The filter is two ensembled SLMs.}
    \begin{threeparttable}
        \begin{tabular}{ccc|ccc}
        \toprule
         \multirow{2}{*}{\textbf{\shortstack{Manual \\ CoT}}} & \multirow{2}{*}{\textbf{\shortstack{Demo \\ Selection}}} & \multirow{2}{*}{\textbf{\shortstack{Cand \\ Filter}}}  \\
        & & & \textbf{FewNERD} & \textbf{TACREV} & \textbf{ACE05} \\
        \midrule
        \cmark & \cmark & \cmark & $\textbf{63.7}$ {\tiny $(1.8)$} & $\textbf{76.8}$ {\tiny $(1.2)$} & $\textbf{67.8}$ {\tiny $(2.1)$} \\
        \midrule
        \xmark & \cmark & \cmark & $63.4$ {\tiny $(1.2)$} & $74.9$ {\tiny $(1.7)$} & $66.7$ {\tiny $(2.2)$} \\
        \xmark & \xmark & \cmark & $63.1$ {\tiny $(1.3)$} & $74.6$ {\tiny $(2.7)$} & $66.8$ {\tiny $(2.5)$} \\
        \xmark & \cmark & \xmark & $62.7$ {\tiny $(1.3)$} & $73.9$ {\tiny $(1.1)$} & $66.9$ {\tiny $(2.4)$}  \\ 
        \xmark & \xmark & \xmark & $62.7$ {\tiny $(0.9)$} & $72.8$ {\tiny $(3.2)$} & $66.2$ {\tiny $(1.7)$} \\
        \midrule
        \multicolumn{3}{c}{FSLS/KnowPrompt} \vline & $62.6$ {\tiny $(1.0)$} & $74.1$ {\tiny $(1.7)$} & $66.5$ {\tiny $(1.7)$} \\
        \bottomrule
        \end{tabular}
    \end{threeparttable}
    \label{tab: Ablation study}
\end{table}

\subsection{Ablation Study}
We investigate the effectiveness of modules in LLM-reranker by removing each of them in turn. (1) \textbf{Manual CoT}: We remove the rationales of examples in demos. (2) \textbf{Demonstrations}: We further remove all demos, making the reranking procedure as a zero-shot QA (question answering) problem. (3) \textbf{Candidate Filter}: We keep all samples rather than only top-$N$ labels for reranking.

We show their results in Table~\ref{tab: Ablation study} and observe that (1) Demos with manual CoT achieves consistent improvement on all three datasets. It indicates that the rationales on correct choices further elicit the reranking ability of LLMs. (2) Even the demos without rationales improve the performance to some extent, see the comparison line 2 v.s. 3 and line 4 v.s. 5. (3) The filtering of candidate labels usually brings gains, especially on TACREV dataset. Furthermore, it significantly reduces the input length of LLMs and thus the inference cost.



\begin{table}
 \centering
  \small
  \setlength\tabcolsep{2.0pt}
   \caption{The F1-score differences of all samples (the left three columns), reranked samples (the middle three columns), and the ratio of reranked samples (the last column) over three datasets. 20-shot settings for FewNERD and ACE05. 100-shot setting for TACREV. The filter is two ensembled SLMs.}
    \begin{threeparttable}
        \begin{tabular}{l|ccc|ccr|r}
        \toprule
         & \multicolumn{3}{c}{\textbf{Overall}} \vline & \multicolumn{3}{c}{\textbf{Reranked}} \vline & \multirow{2}{*}{\textbf{\shortstack{Reranked \\ Ratio}}} \\
         & before & after & $\triangle$ & before & after & $\triangle$  \\
         \midrule
         FewNERD & $62.6$ & $63.7$ & $1.1$ & $31.4$ & $28.3$ & $-3.1$ & $3.3\%$ \\
         TACREV & $74.1$ &$76.8$ & $2.7$ & $33.8$ & $43.4$ & $9.6$ & $13.2\%$ \\
         ACE05 & $66.5$ & $67.8$ & $1.3$ & $35.6$ & $55.7$ & $20.1$ & $0.5\%$\\
        \bottomrule
        \end{tabular}
    \end{threeparttable}
    \label{tab: big diff}
\end{table}
\subsection{Analysis}
\label{subsec: reranker analysis}
\noindent \textbf{Few makes big difference}
We know from Figure~\ref{figs: rerank result grouped by conf score} that most of samples are \textit{easy} for SLMs (with high confidence score). Therefore only a tiny minority of samples are fed to LLMs for reranking, as shown in Table~\ref{tab: big diff} (the last column). However, we figure out from Table~\ref{tab: big diff} that the reranking brings impressive improvement on them, see \textasciitilde $10\%$ for TACREV and \textasciitilde $20\%$ for ACE05 dataset. Such upheaval on small amount of samples leads to an overall significant improvement.\footnote{An exception occurs at FewNERD dataset, on which the performance of reranked samples seems to degrade slightly. We dive deep into the result and observe that LLM-rerankers correct (eliminate, in other word) many false-positive samples. Therefore the overall performance actually improves even though the metric values on reranked samples decrease.}



\noindent \textbf{Few makes small cost} We compare the inference cost between direct ICL via InstructGPT and \textit{filter-then-rerank} method from two aspects, financial and time, in Figure~\ref{figs: cost}. It shows that \textit{filter-then-rerank} achieves a reduction of \textasciitilde $80\%$-$90\%$ on budgets and latency compared with the direct ICL methods. We point out that it is much more efficient due to three main reasons: (1) It calls LLMs API  for only a small portion of samples. (2) It reranks only a small set of labels. (3) It requires less demos.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/cost.pdf}
    \caption{The financial and time cost over 500 sentences among three kinds of methods. Note that we do not take the financial cost of SLMs into account since they could run locally, and the financial cost of LLMs are estimated.}
    \label{figs: cost}
    \vspace{-1em}
\end{figure}
