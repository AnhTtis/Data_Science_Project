%==============================
\section{Introduction}
\label{sec:intro}
%==============================

Large Language Models (LLMs,~\citealt{2020_gpt3, 2022_PaLM, 2022_gopher}) are becoming a fundamental tool for general task solver. They have shown amazing emergent (\eg memorization and reasoning) abilities through in-context learning (ICL) on various applications, including arithmetic reasoning, commonsense reasoning, and open-domain question answering~\cite{2022_cot, yu2023generate, sun2023recitationaugmented}. 

Recent studies have compared the performance between LLMs with ICL\footnote{All LLMs discussed in this paper are not fine-tuned, and results for LLMs are based on ICL through APIs.} and Small Language Models (SLMs) with conventional fine-tuning techniques\footnote{We define SLMs as relatively small pre-trained language models (PLMs) that can be easily fine-tuned locally, such as BERT, RoBERTa, BART and T5.} across many tasks. Focusing on information extraction (IE), some claim that LLMs are good few-shot extractors~\cite{code2struct_2022, agrawal-etal-2022-large}, while some others hold opposite opinions~\cite{jimenez-gutierrez-etal-2022-thinking}. We attribute the disagreement to the different IE sub-tasks, datasets, and settings in experiments. Given the disagreement, we claim a systematic evaluation on \textit{whether LLMs perform competitively on various few-shot IE tasks} is non-trivial for further research. 

In this paper, we target such a thorough evaluation on the advantages and disadvantages of LLMs and SLMs on various IE tasks. We aim to answer the following questions: \textbf{1)} Do LLMs really outperform SLMs in few-shot IE tasks? \textbf{2)} Can more annotations improve LLMs and SLMs? \textbf{3)} In terms of financial and time cost, which is preferable? \textbf{4)} Are LLMs and SLMs respectively adept at handling different types of samples?

To answer the first three questions, we conduct an extensive empirical study on eight datasets across three common IE tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Event Detection (ED). The results show that \textbf{1)} LLMs outperform SLMs only when the overall number of annotations is limited, \ie both label types\footnote{Label types denote \textit{entity/relations/event types} in different tasks. We use them interchangeably there-in-after.} and the samples\footnote{Samples refer to (i) demonstrations in ICL of LLMs, or (ii) training samples for SLMs' fine-tuning.} per label are extremely scarce. 
However, \textbf{2)} when we increase the number of samples (\eg a few hundreds), SLMs outperform LLMs by a large margin. We speculate it is caused by the natural limitations of ICL in two main ways. First, only a small subset of available samples can be used as demonstrations (demos) to prompt LLMs due to the maximum input length of ICL. Moreover, more samples in demos might not bring extra performance gains. Second, as the schema (or number of label types) grows, the number of samples per label in prompts decreases. It is thus difficult to well understand tens (even hundreds) of label types and their semantic interactions via instruction. Besides, \textbf{3)} calling LLMs API suffers from much higher inference latency and financial cost than finetuning SLMs locally, especially when there are excessively long demos in ICL. Therefore, we claim that \textbf{LLM is not a good few-shot information extractor in general}.


We next investigate whether LLMs and SLMs exhibit different abilities to handle various types of samples, hoping LLMs could complement SLMs. We partition the testing samples into different groups according to their \textit{difficulty} (measured by the confidence score of SLMs-based models) and compare the results of LLMs and SLMs on each group. We find that \textbf{4) LLMs are good at hard samples, though bad at easy samples}.
We speculate the \textit{hard} samples (\ie low confidence scores) require external knowledge or complex reasoning, which go beyond the abilities of SLMs but could be well solved by LLMs. For relatively \textit{easy} samples, however, SLMs learn them well by fine-tuning parameters and perform much better than LLMs.

Building on these findings, we propose a novel adaptive \textit{filter-then-rerank} framework to combine SLMs and LLMs considering both performance and cost in practice. The basic idea is that SLMs serve as a filter and LLMs as a reranker. In specific, SLMs make the first round of prediction, and if the sample is a hard one, we further pass the top-$N$ candidate labels with highest prediction scores by SLMs to LLMs for reranking. The reranking mechanism leverages both LLMs and SLMs to deal with samples on which they are more proficient and thus combines their strengths. Moreover, it reranks only a small subset of test samples and minimizes the extra latency and budgets for calling LLM APIs. We outline our main contributions as follows:

\begin{itemize}
    \item We conduct an extensive empirical study comparing LLMs and SLMs on IE tasks. Our findings suggest that LLMs are generally not well-suited for IE tasks, especially when dealing with many samples and complicated schema.
    \item We propose a filter-then-rerank paradigm that combines the strengths of both LLMs and SLMs. We note that LLMs can be effective rerankers for challenging samples.
    \item With only $0.5\%$-$13.2\%$ of the samples being reranked, our adaptive filter-then-rerank system surpasses the previous state-of-the-art methods by an average $2.1\%$ F1-score gain.
\end{itemize}