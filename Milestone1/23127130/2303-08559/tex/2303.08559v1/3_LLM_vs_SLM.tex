\vspace{-0.5em}
\section{Large LMs v.s. Small LMs}
We wonder whether LLMs can outperform supervised SLMs in few-shot IE scenarios purely through ICL. To this end, we evaluate SLMs and LLMs on eight commonly used datasets spanning three representative IE tasks. (1) \textbf{Named Entity Recognition} (NER): CONLL'03~\cite{tjong-kim-sang-de-meulder-2003-introduction}, OntoNotes 5.0~\cite{weischedel2013ontonotes} and FewNERD~\cite{ding-etal-2021-nerd}. (2) \textbf{Relation Extraction} (RE): TACRED~\cite{zhang-etal-2017-position} and TACREV~\cite{alt-etal-2020-tacred}. (3) \textbf{Event Detection} (ED): ACE05~\cite{doddington-etal-2004-automatic}, MAVEN~\cite{wang-etal-2020-maven} and ERE~\cite{song-etal-2015-light}. We list the statistics of these eight datasets in Appendix~\ref{subsec: full datasets}.

\subsection{Experimental Setup}
We construct few-shot datasets from the original eight datasets mentioned above.

\noindent \textbf{Training and Validation Set} \noindent We adopt $K$-shot sampling strategy to construct few-shot datasets, \ie sampling $K$ samples for each label type. We set 4 different $K$-values (1, 5, 10, 20) for NER and ED tasks, and 6 different $K$-values (1, 5, 10, 20, 50, 100) for RE tasks. For each constructed dataset with more than 300 sentences, we split 10\% sentences as validation set and the remaining sentences as training set. See Appendix~\ref{subsec: few-shot dataset details} for details and the statistics of the sampled few-shot IE datasets.

\noindent \textbf{Test Set} \noindent To reduce the inference time and cost of LLMs, we randomly sample 250 sentences from the original test sets for NER and ED tasks, and 500 sentences for RE task, as our test benchmark.

\noindent \textbf{Evaluation Metric}
We adopt micro-F1 score as evaluation metric. The reported value of each setting is the averaged score w.r.t 5 sampled train/validation sets to reduce random fluctuation.

\begin{figure*}[htb]
\centering
    \subfigure[Named Entity Recognition (NER)]{
    \includegraphics[width=0.95\linewidth]{figs/res_ner.pdf}
    }\vspace{-1.5mm}
    \subfigure[Relation Extraction (RE)]{
    \includegraphics[width=0.95\linewidth]{figs/res_re.pdf}
    }\vspace{-1.5mm}
    \subfigure[Event Detection (ED)]{
    \includegraphics[width=0.95\linewidth]{figs/res_ed.pdf}
    }\vspace{-1.5mm}
\caption{Overall results of 4 supervised SLM-based methods (dashed lines) and 3 LLM-based ICL methods (solid lines) on eight datasets. \textbf{FT}: Fine-tuning. \textbf{ICL}: In-context Learning. \textbf{DS}: Demonstration Selection. \textbf{SE}: Self-ensemble. The results are averaged over 5 repeated experiments. See detailed values in Tables~\ref{tab: NER results},~\ref{tab: RE results} and~\ref{tab: ED results}.}
\label{figs: comparison result}
\vspace{-1em}
\end{figure*}


\subsection{Small Language Models}
\label{subsec: SLM}
We adopt four representative supervised methods to evaluate the ability of SLMs on few-shot IE tasks. We choose \texttt{RoBERTa-large}~\cite{2019_roberta} as the backbone of extractive-based methods and \texttt{T5-large}~\cite{2019-T5} as the backbone of generation-based methods, respectively. Next, we brief these methods and leave their implementation details in Appendix~\ref{subsec: details for SLMs}.

\noindent \textbf{(1). Fine-tuning (FT)}: Add a classifier head on SLMs to predict the labels of each sentence/word.

\noindent \textbf{(2). FSLS}~\cite{ma-etal-2022-label}: The state-of-the-art extractive-based method for few-shot NER task. \citet{2022-few_shot_ed} also validate its competitive performance on few-shot ED tasks.

\noindent \textbf{(3). KnowPrompt}~\cite{2022_knowprompt}: The best extractive-based method for few-shot RE task.

\noindent \textbf{(4). UIE}~\cite{lu-etal-2022-unified}: A competitive unified generation-based method for few-shot IE tasks.



\subsection{Large Language Models}
\label{subsec: LLM}
We adopt the current\footnote{\texttt{2023-03-03}} strongest \textbf{CODEX}\footnote{\texttt{code-davinci-002}}~\cite{2021_codex} rather than \textbf{InstructGPT}\footnote{\texttt{text-davinci-003}}~\cite{2022_instructgpt} in our experiments out of two primary reasons: (1) They share similar scales (\textasciitilde 175B) and functionality. Previous study~\cite{code2struct_2022} showed CODEX has comparable, if not better, ICL capability. Our pivot experiments in Appendix~\ref{subsec: two LLMs comparison} further validates it. (2) Calling InstructGPT via API\footnote{https://openai.com/} is very costly.\footnote{The estimated cost of using InstructGPT is about 40K dollars to reproduce all the experiments in this section.} In contrast, CODEX is currently available for free, allowing us to reduce cost and improve the reproducibility of our experiments.

We adopt several ICL approaches to evaluate the LLM ability on IE tasks. We introduce them in brief here and leave their details in Appendix~\ref{subsec: details for LLMs}.

\noindent \textbf{(1). Vanilla ICL} utilizes the common prompts consisting of instruction, demonstrations (demos) and question. We show such format in Figure~\ref{figs: diff tasks}.

\noindent \textbf{(2). ICL w. Automatic Chain-of-thought} (Auto-CoT,~\citealt{2023_auto-cot}) first bootstrap rationales from original examples. These generated rationales then act as intermediate reasoning steps in demos.

\noindent \textbf{(3). ICL w. Demonstration Selection} (DS, ~\citealt{liu-etal-2022-makes}) retrieves similar training examples as demos for each test example. We adopt a heuristic unsupervised approach here measuring the similarity of each sentence by their embeddings.

\noindent \textbf{(4). ICL w. Self-ensemble} (SE) predicts each test example by multiple times. Each time the examples (and/or their orders) are different in the demos. These predicted outputs are ensemebled by major-voting to determine the final prediction. \footnote{Note that this approach is different from ICL with self-consistency~\cite{2023_self-consist}. The randomness in self-consistency lies in output rationales (and answers) generated from nucleus sampling~\cite{2020_sampling}. The randomness in self-ensemble, however, comes from the different examples in input demos. We find setting the samping temperature coefficient $t=0$, \ie greedy decoding, achieves the optimal result according to pivot experiments in Appendix~\ref{subsec: t-value}. Therefore we use \textbf{self-ensemble} rather than \textbf{self-consistency} (which requires a non-zero $t$) in this work.}

\begin{table}[htbp!]
    \centering
    \setlength\tabcolsep{3.0pt}
    \small
    \caption{
        The inference seconds over 500 sentences. \textbf{B}: \texttt{RoBERTa/T5-base}. \textbf{L}: \texttt{RoBERTa/T5-large}. \textbf{C}: \texttt{code-davinci-002}. \textbf{T}: \texttt{text-davinci-003}.
    }
    \begin{threeparttable}
    \begin{tabular}{@{}lc|ccrrrr}
        \toprule
         \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Task}}} & \multicolumn{2}{c}{\textbf{FT} (Roberta)} & \multicolumn{2}{c}{\textbf{UIE} (T5)} & \multicolumn{2}{c}{\textbf{ICL} (GPT-3)} \\
         & & B & L & B & L & C & T \\
         \midrule
        \parbox[t]{0.5mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{NER}}}} & CONLL03 & 0.6 & 1.6 & 3.0 & 10.3 & 128.8 & 113.8 \\
        & OntoNotes & 1.6 & 4.1 & 9.3 & 22.9 & 134.1 & 114.6 \\
        & FewNERD & 1.1 & 2.8 & 14.6 & 39.4 & 179.4 & 166.5 \\
         \midrule
        \parbox[t]{0.5mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{\textbf{RE}}}} & TACRED & 0.4 & 1.4 & 14.1 & 43.8 & 164.4 & 132.4 \\
        & TACREV & 0.4 & 1.4 & 14.5 & 45.6 & 151.6 & 127.1 \\
         \midrule
         \parbox[t]{0.5mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{ED}}}} & ACE05 & 0.8 & 2.4 & 3.0 & 8.9 & 135.2 & 112.1\\
        & ERE & 0.9 & 1.9 & 5.2 & 15.8 & 136.6 & 102.2 \\
        & MAVEN & 2.6 & 6.6 & 31.5 & 62.5 & 171.7 & 156.2  \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    \label{tab: speed}
    \vspace{-1em}
\end{table}



\subsection{Comparison Result}
\label{subsec: compare result}
We evaluate eight approaches, four SLM-based supervised methods and four LLM-based ICL methods introduced above, on eight datasets across three IE tasks. We first conduct pivot experiments and observe ICL with Auto-CoT delivers much poorer results than we expected (see results and analysis in Appendix~\ref{subsec: auto-cot}). Therefore we do not include ICL with Auto-CoT in main experiments.

We first overview the performance of the remaining seven methods in Figure~\ref{figs: comparison result}. 

\noindent \textbf{Comparison among ICL Approaches} We observe that vanilla ICL achieves the worst results among three remaining ICL approaches. Since the number of examples in demos is bounded by LLMs' maximum input length, the increase of sample number brings no benefit and the performance of ICL is at a standstill once the sample number exceeds some threshold. Both DS and SE slightly alleviate such problem to some extent. Overall speaking, SE is better than DS with fewer shots, while DS outperforms the other two methods with more samples, \ie more retrieval candidates.

\noindent \textbf{LLMs are not good few-shot Information Extractor}. Even compared with the best LLM-based methods, SLMs mostly outperform by a large margin .  (1) For most of the NER and RE datasets (except CONLL'03 with only 4 defined entities), the best LLM-based methods merely present significantly superior performance than SLMs under extremely low-resource settings (1-shot). The most competitive SLMs usually achieve comparable results with LLMs under 5-10 shot settings. With more samples, LLM-based methods perform worse than almost all SLM-based methods. (2) For three ED datasets, SLMs consistently beat all LLM-based methods even under 1-shot settings.

\noindent \textbf{LLMs show limited inference speed} We additionally compare the inference speed of different methods and show their results in Table~\ref{tab: speed}. Expectedly we observe that the most efficient LLM-based method, \ie vanilla ICL, is still much slower than SLMs since they have much more parameters and longer input context length.


\subsection{Discussion: Why LLMs not present satisfactory performance on IE tasks?}
We dive deep into above results and analyze why LLMs fail to achieve satisfactory performance.

\noindent \textbf{Underutilized Annotations}
We notice SLMs benefit much more than LLMs from extra annotations, \ie more training samples and label types. We speculate LLMs are constrained by ICL from two aspects. \textbf{(1) More samples}: The number of actual effective samples for LLMs, \ie the sample number in demos, is bounded by the maximum input length. Furthermore, we observe that in some tasks, the performance of LLMs have been at a standstill before reaching the input bounds (see Appendix~\ref{subsec: more demos}). In contrast, SLMs could learn from much more samples by parameter updating. Therefore the average difference between SLMs and LLMs grows as the K-shot value increases. \textbf{(2) More labels}: LLMs perform relatively best on CONLL'03 dataset with only 4 defined entities, and perform worst on MAVEN dataset with 168 event types.  As \citet{2022_LLM_plan} suggest, too many labels are likely to cause LLMs hard to understand all labels and their semantic interactions from the provided instruction and demos. Moreover, the number of examples per label in demos decreases as the number of label types increases.


\noindent \textbf{Unexplored Task format}
We find LLMs achieve relatively worse performance on NER and ED tasks. We speculate it is partly due to their task formats. Both of these two tasks require structured outputs, \ie the (label, span) tuples as shown in Figure~\ref{figs: diff tasks}. Moreover, the number of outputs and the extracted span within each output are not fixed. Standing with~\citet{2023_synIE}, we believe ICL approaches are not experienced on such task formats.

\noindent \textbf{Abstract Event Understanding}
We observe that ED achieves the worst performance among three tasks. In addition to its task format, (1) the definition of events is more abstract than that of entity and relations, (2) and the rules for labeling events are more complicated\footnote{e.g., when a word triggers event and when does not, which word to annotate if more than one words trigger the event within the single sentence, and so on.}. Therfore we speculate the abilities required to solve ED task are unlikely to be learned during the pre-training of LLMs or be generalized through instructions during ICL.

