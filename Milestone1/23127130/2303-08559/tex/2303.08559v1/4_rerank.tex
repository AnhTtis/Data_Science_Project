\section{LLMs are Good Few-shot Reranker}
\label{sec: LLMs are Good Few-shot Reranker}

Despite their unsatisfactory performance and their significant time and monetary costs, we believe that LLMs' abilities in memorization and reasoning remain crucial strengths for solving IE tasks. Consequently, we aim to identify more effective methods to harness the strengths of LLMs while minimizing their limitations in this section.

We propose a novel \textit{filter-then-rerank} paradigm. Based on such paradigm, we observe the complementary results of LLMs and SLMs on samples with varying levels of difficulty. We would detail them in the following two subsections.


\begin{figure*}[htb]
\centering
    \subfigure[Fine-tuning]{
    \includegraphics[width=\linewidth]{figs/hier_FT.pdf}
    }
    
    \subfigure[FSLS/KnowPrompt]{
    \includegraphics[width=\linewidth]{figs/hier_FSLS.pdf}
    }
\caption{Relationship between confidence score and F1-score of samples, before (colored in purple) and after (colored in red) LLM reranking. The x-axis shows 9 sample groups divided by their confidence scores. The left y-axis corresponds to the F1-score curves depicting the performance of samples within each group. The right y-axis (log scale) corresponds to the bars indicating the sample number in each group. Each subfigure is titled as \texttt{[X]-[Y]}, where \texttt{[X]} represents the dataset and \texttt{[Y]} represents the filter (SLM-based) methods. Among various datasets and the filter methods, we observe the similar trend between F1-scores before and after reranking.}
\label{figs: rerank result grouped by conf score}
\vspace{-1em}
\end{figure*}

\subsection{Filter-then-rerank Paradigm}
\label{subsec: multiple-choice question}
Our \textit{filter-then-rerank} paradigm combines SLMs and LLMs, as its name implies, by utilizing both as a filter and a reranker within it. SLMs act as a filter which eliminates unlikely labels and retains only the top-$N$ candidates. LLMs then rerank these $N$ labels and output the final answer.

Within our \textit{filter-then-rerank} paradigm, we dynamically obtain $N$ candidate labels for each test sample to be reranked by LLMs. We argue the prompts used in Section~\ref{subsec: LLM}, which contain the whole schema in their instructions, are no longer needed. Instead, we reformulate the reranking procedure as a multiple-choice question. We show the format of our new multiple-choice question prompt as below, and leave real examples in Appendix~\ref{sec: demo example}.

\begin{tcolorbox}
\small
    \textbf{Instruction}: Read the sentence and determine the relation between \texttt{<h ent>} and \texttt{<t ent>}.
    
    \textbf{Sentence}: <Sentence>
    
    (a) \texttt{T(<Label 1>,  <h ent>, <t ent>)}
    
    (b) \texttt{T(<Label 2>,  <h ent>, <t ent>)}
    
    (c) \texttt{T(<Label 3>,  <h ent>, <t ent>)}

    \textcolor{red}{\textbf{[Optional] Analysis}: <Rationale>}

    \textcolor{red}{\textbf{Answer}: <Correct choice>}

\end{tcolorbox}

As shown above, the instruction asks LLMs to determine the relation between entities in the sentence followed by. Then $N$ candidate labels filtered from SLMs are provided. Each label is rephrased as a piece of choice text using the template \texttt{T(<label>, <h ent>, <t ent>)}. The template describes that the head entity \texttt{<h ent>} and the tail entity \texttt{<e ent>} have \texttt{<label>} relation. For example, \texttt{T}(cities\_of\_residence, Charles, Abidjan) = \underline{Charles lives in the city Abidjan}. Then LLMs are expected to answer this multiple-choice question (optionally associated with rationales) as colored in red. We believe the format of multiple-choice question has various advantages: (1) It narrows the label scope significantly. (2) It lowers the difficulty of IE tasks since LLMs are more familiar with this format. 

\vspace{-0.2em}
\subsection{\hspace{-0.5em}LLMs are and only are \textit{Hard} Sample Solver}
Unfortunately, we find such a \textit{filter-then-rerank} system still performs far from satisfactory. Moreover, it leads to longer latency since LLMs rerank candidates per sample, rather than per sentence.

We intuitively speculate LLMs are more proficient than SLMs on \textit{hard} samples. The \textit{hard} samples here refer to those requiring external knowledge or complex reasoning beyond the capability of SLMs, see examples in Appendix~\ref{sec: case study}. SLMs could not solve them well with limited model capacity and data amount, while LLMs could solve them better. In contrast, SLMs could learn \textit{easy} samples well from more samples by updating their parameters and exceed the performance of LLMs.

We design experiments to validate our conjecture. We group the samples by their \textit{difficulty} and evaluate their performance before and after reranking within each group. To measure the \textit{difficulty} of a sample $x$ for SLMs, we adopt the maximum probability among all labels as the confidence score,
\begin{align}
    s(x) = \max_{y \in Y} p(y|x)
\end{align}
where $p(y|x)$ represents the probability of $x$ having label $y$ computed by SLMs. We call samples with low confidence scores as \textit{hard} samples.

We conduct experiments on various datasets with SLM-based methods (\ie the filter). We select FewNERD, TACREV and ACE05 as the datasets, fine-tuning and the best baseline in Section~\ref{subsec: compare result} (FSLS for FewNERD and ACE05 datasets, KnowPrompt for TACREV dataset) as two SLM-based methods.
The results illustrated in Figure~\ref{figs: rerank result grouped by conf score} validate our assumption. (1) There exists a consistent trend among different tasks and SLM-based methods that the performance of low-confidence samples improves after reranking while the high-confidence ones get degraded from reranking (see the red curves are usually higher than the purple curves when the confidence being less than a threshold, but lower after exceeding such threshold). In other word, \textbf{LLMs are more proficient than SLMs on hard samples, but often make mistake on easy samples}. (2) The hit@3 scores of SLMs usually achieve more than 95\%\footnote{Note that the proportions of unhit samples, \ie the orange bar, are distorted Figure~\ref{figs: rerank result grouped by conf score} due to the log scale.} even under low-confidence scenarios. It ensures almost all true answers are included in candidate options for LLMs to rerank, making our reranking feasible. (3) The performance of LLMs might collapse under samples with very high-confidence. It is likely due to that LLMs sometimes tend to give false-positive predictions for negative samples, most of which are easy for SLMs and lie in high-confidence interval. 