\section{Models}
\subsection{Implementations Details on SLMs}
\label{subsec: details for SLMs}
We introduce the implementation details of 4 methods based on Small Language Models (SLMs).

\noindent \textbf{Fine-tuning/FSLS.} We implement these two methods by ourselves. We use \texttt{RoBERTa-base} and \texttt{RoBERTa-large}~\cite{2019_roberta} as our backbones. We adopt Automatic Mixed Precision (AMP) training strategy\footnote{https://pytorch.org/docs/stable/amp.html} to save memory. We run each experiment on a single NVIDIA V100 GPU. We train each model with the AdamW~\cite{2019_AdamW} optimizer with linear scheduler and 0.1 warm-up steps. We set the weight-decay coefficient as 1e-5 and maximum graidient norms as 1.0. We set the batch size as 64, the maximum input sequence length as 192, the training step as 500 and the learning rate as 5e-5.

\noindent \textbf{KnowPrompt}  We implement this method based on original source code\footnote{https://github.com/zjunlp/KnowPrompt}, and use \texttt{RoBERTa-base} and \texttt{RoBERTa-large} as our backbones. We set 10 maximum epochs for 50- and 100-shot datasets, and as 50 epochs for other datasets. We keep all other hyperparameters as default, and run each experiment on a single NVIDIA V100 GPU.

\noindent \textbf{UIE} We implement this method based on original source code\footnote{https://github.com/universal-ie/UIE}, and use \texttt{T5-base} and \texttt{T5-large}~\cite{2019-T5} as the backbones. We run each experiment on a single NVIDIA Quadro RTX8000 GPU. We set the batch size as 16 with 1000 training steps for base model, and the batch size as 4 with 4000 training steps for large model. We set the maximum input sequence length as 800 and the learning rate as 1e-4.
 
\subsection{Implementations Details on LLMs}
\label{subsec: details for LLMs}
We mainly use CODEX~\cite{2021_codex} as backbones of our ICL approaches. We also use InstructGPT~\cite{2022_instructgpt} in pivot experiments (as shown in Appendix~\ref{sec: pivot exp}) and our adaptive \textit{filter-then-rerank} system. We set the maximum input length as 3600 for all tasks and models. The only exception occurs when we use CODEX to solve RE tasks: here we set the maximum input length as 7000. We unify the maximum output length as 96 for NER and ED tasks, and as 32 for RE task. We set the sampling temperature coefficient $t=0$, \ie greedy decoding. We would detail the special design for each variant below.


\noindent \textbf{Vanilla ICL} Our prompts are composed of three parts: (1) \texttt{Instruction}: a short piece of natural language description about the task. (2) \texttt{Demonstrations}: some (input, output) pairs as train examples for LLMs. (3) \texttt{Question}: the input as test example. Most time the training sample number exceeds the limitation of examples in demos. Under this case, we would randomly sample a subset as demo examples for each test instance.

\begin{figure*}
\centering
    \includegraphics[width=0.85\linewidth]{figs/CoT.pdf}
    \caption{The diagram of ICL approach with Auto-CoT.}
    \label{fig: auto-cot}
\end{figure*}


\noindent \textbf{ICL w. Automatic Chain-of-thought} (Auto-CoT,~\citealt{2023_auto-cot}) If a sentence has positive labels, we would query LLMs \textit{According to [sentence], Why [span] is a [label]}. For example, given the sentence \textit{``\underline{DSC} and Traction Control on all \underline{Speed3} models is also standard.''} in which \textit{Speed3} is a \textit{car-product} entity and \textit{DSC} is an \textit{other-product} entity, it is to auto-generate rationales for these two entities. By asking \textit{``Could you explain why Speed3 is a kind of car''}, the LLMs would answer \textit{``the term Speed3 refers to a specific car model produced by Mazda. Mazda is an automobile manufacturer, and as such, Speed3 is likely a car product from their lineup'' }. As shown in Figure~\ref{fig: auto-cot}, we collect these auto-generated rationales for each samples, and insert them between the sentences and answers (see blue box in left part of the figure). If a sentence has no positive labels, however, we do not ask LLMs and keep the original format as the vanilla ICL approach.


\noindent \textbf{ICL w. Demonstration Selection} (DS, ~\citealt{liu-etal-2022-makes, 2022_select-annot}) The format of prompts is the same as that of vanilla ICL approach. The difference from vanilla ICL lies in that we retrieve similar samples as demo examples for each instance rather than sample randomly. We adopt the cosine similarity of two sentence embeddings to measure their similarity. We compute sentence embeddings by $\text{SimCSE-RoBERTa}_\texttt{large}$~\cite{gao-etal-2021-simcse}.

\noindent \textbf{(4). ICL w. Self Ensemble} (SE) We repeatedly test each sample 5 times with different demos and count the results. We select the winners with more than 1 votes as our final results.
