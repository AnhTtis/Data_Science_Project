\begin{figure*}[!htbp]
\centering
    \includegraphics[width=0.91\linewidth]{figs/prompt_examples.pdf}
    \caption{
    Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction, demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs. We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1.
    }
    \label{figs: prompt_examples}
\end{figure*}

\begin{figure*}[!htbp]
\centering
    \subfigure[Named Entity Recognition (NER)]{
    \includegraphics[width=0.9\linewidth]{figs/res_ner_new.pdf}
    }
    \subfigure[Relation Extraction (RE)]{
    \includegraphics[width=0.9\linewidth]{figs/res_re_new.pdf}
    }
    \subfigure[Event Detection (ED)]{
    \includegraphics[width=0.9\linewidth]{figs/res_ed_new.pdf}
    }
    \subfigure[Event Argument Extraction (EAE)]{
    \includegraphics[width=0.9\linewidth]{figs/res_eae_new.pdf}
    }
\caption{Overall results of SLM-based methods (dashed lines) and LLM-based methods (solid lines) on nine datasets across four IE tasks. The black, horizontal dashed lines represent the SoTA performance on full dataset.}
\label{figs: comparison result}
\end{figure*}


\section{Large LMs v.s. Small LMs}
\label{sec: slm_llm}
In this section, we compare the performance between LLMs and SLMs to evaluate whether LLMs perform competitively. 

\subsection{Task, Dataset and Evaluation}
We run experiments on nine widely-used datasets across four IE tasks. 
(1) Named Entity Recognition (NER): CONLL03~\cite{tjong-kim-sang-de-meulder-2003-introduction}, OntoNotes~\cite{weischedel2013ontonotes} and FewNERD~\cite{ding-etal-2021-nerd}. (2) Relation Extraction (RE): TACRED~\cite{zhang-etal-2017-position} and TACREV~\cite{alt-etal-2020-tacred}. (3) Event Detection (ED): ACE05~\cite{doddington-etal-2004-automatic}, MAVEN~\cite{wang-etal-2020-maven} and ERE~\cite{song-etal-2015-light}. (4) Event Argument Extraction (EAE): ACE05, ERE and RAMS~\cite{ebner-etal-2020-multi}. 
With label numbers ranging from 4 to 168, we assess LLMs' performance under different schema complexities. See their details in Appendix~\ref{subsec: full datasets}.

\noindent \textbf{Few-shot Set} 
We construct few-shot datasets from the original datasets above. For training and validation set, we adopt $K$-shot sampling strategy, \ie sampling $K$ samples for each label type. See more details in Appendix~\ref{subsec: few-shot dataset details}.
For test set, we downsample their original test sets to reduce the cost of LLMs. We randomly sample 500 sentences for RE tasks, and 250 sentences for other task. We ensure that each label has at least one corresponding sample to avoid the absence of rare labels.

\noindent \textbf{Evaluation}
We adopt micro-F1 score in NER, RE and ED tasks. For EAE task, we follow previous work~\cite{wang-etal-2023-code4struct} and adopt head-F1 score, which merely considers matching of the head word rather than the whole content of a text span. We report averaged score w.r.t 5 sampled train/validation sets unless otherwise stated.

\subsection{Small Language Models}
\label{subsec: SLM}
We adopt five supervised methods to evaluate the abilities of SLMs.
(1) Vanilla fine-tuning for all tasks, (2) FSLS~\cite{ma-etal-2022-label} for NER and ED tasks, (3) KnowPrompt~\cite{2022_knowprompt} for RE task, (4) PAIE~\cite{ma-etal-2022-prompt} for EAE task, and (5) UIE~\cite{lu-etal-2022-unified} for all tasks. See their details in Appendix~\ref{sec: details for SLMs}.

\subsection{Large Language Models}
\label{subsec: LLM}
Detailed in Appendix~\ref{sec: details for LLMs}, we evaluate the ICL abilities of LLMs. Given labeled sentences $D = \{(s_i, y_i)\}$ and a test sentence $s$, our goal is to predict structured information $y$ from $s$ using a frozen LLM $\mathcal{L}$. We feed LLM with prompt $\mathcal{P}_{\mathcal{E}, I, f}(D, s)$:

\begin{equation}
    \mathcal{P}_{\mathcal{E}, I, f}(D, s) = [I; f(\mathcal{E}(D, s)); f(s)]
\end{equation}

We give examples of prompts on four IE tasks in Figure~\ref{figs: prompt_examples}. The prompts consist of three parts: instruction $I$ (color in green in Figure~\ref{figs: prompt_examples}), demonstration $f(\mathcal{E}(D, s))$ (demo; color in blue) and the question $f(x)$ (color in black). Here $\mathcal{E}$ denotes demo selector and $\mathcal{E}(D, s) \subset D$ denotes selected sentences as the demo to predict $s$. Prompt format $f$~\footnote{We slightly abuse the notation $f$ to allow $s$, $y$ and $\{(s, y)\}$ as the input for simplicity.} refers to the template which converts demo $\mathcal{E}(D, s)$ and sample $s$ to input context for LLMs. Then LLM generates $f(y)$ (color in red) from which we could readily parse the extraction results $y$. 

\noindent \textbf{Models $\mathcal{L}$}: We explore six LLMs from two sources. (1) OpenAI models~\footnote{The versions of model we use are: \texttt{gpt-3.5-turbo-0301}, \texttt{code-davinci-002}, \texttt{text-davinci-003} and \texttt{gpt-4-0314}. Due to budget constraints, we execute InstructGPT and GPT-4 only once per setting. We do not conduct EAE task on CODEX since it had been unavailable at that time.}: we employ ChatGPT, CODEX~\cite{chen2022program} and InstructGPT~\cite{2022_instructgpt} for main experiments. We also evaluate GPT-4 in Appendix~\ref{subsec: gpt-4}. (2) Open-source models: we use LLaMA-13B~\cite{touvron2023llama} and its instruction-tuned counterpart, Vicuna-13B~\cite{vicuna2023}.

\noindent \textbf{Instruction $I$}: The instruction (1) describes the task and (2) enumerates all possible labels for reference. we adopt instructions shown in Figure~\ref{figs: prompt_examples}.

\noindent \textbf{Demo selector $\mathcal{E}$}: The maximum input length of LLMs usually limits the sentence number in demos even under few-shot settings. Therefore for each test sentence $s$, we demand a demo retriever $\mathcal{E}(D, s)$ which selects a small subset from $D$ as the sentences in demo. Following previous methods~\cite{liu-etal-2022-makes, 2022_select-annot}, we retrieve demos according to their sentence embedding similarity to the test samples.

\begin{figure*}[!htbp]
\centering
    \includegraphics[width=0.95\linewidth]{figs/prompt_analysis.pdf}
    \caption{
        LLMs' performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets in Appendix~\ref{subsec: instruction variants}-~\ref{subsec: prompt format variants}. \textbf{Left}: ChatGPT's performance (F1 Score) across six instruction variants. \textbf{Middle}: F1 Score changes over varying numbers of demo. \textbf{Right}: ChatGPT's performance across three demo selection strategies. Random: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever~\cite{rubin-etal-2022-learning}.
    }
    \label{figs: prompt analysis}
\end{figure*}

\noindent \textbf{Prompt format $f$}: We use simple textual templates to format the demos and the test sample in main experiments. For example, the template for NER is \texttt{``Sentence: [S], Entities: ([type1], [entity1]), ([type2], [entity2])..."}. 

\subsection{Main Results}
\label{subsec: main results}
We summarize the main experimental outcomes in Figure~\ref{figs: comparison result}, indicating that LLMs only outperform SLMs in environments with restricted labels and samples. Conversely, SLMs are generally more effective. Given (1) the practicality of fine-grained IE tasks and the manageable effort of obtaining 10-20 annotations per label and (2) the excessive time and budget demands of LLM inference, we conclude that LLMs are not as effective as supervised SLMs for few-shot IE tasks under real scenarios. We detail our findings as below.

\noindent \textbf{Performance w.r.t sample number.} The performance dynamics of SLMs and LLMs are influenced by variations in sample size. Under extremely low-resource (1-shot or 5-shot) settings, LLMs sometimes present superior performance than SLMs. Yet, LLMs tend to reach a performance plateau with only modest increases in sample size. Conversely, SLMs demonstrate marked performance enhancement as sample sizes grow. This trend is evident in Figure~\ref{figs: comparison result}, where the SLM trajectories (represented by dashed lines) ascend more steeply compared to the LLM ones (solid lines).

\noindent \textbf{Performance w.r.t label number.} Compared with SLMs, LLMs tend to struggle on fine-grained datasets. For instance, LLMs perform \textit{relatively} worse on MAVEN and RAMS datasets (with 168/139 labels) than on CONLL (4 labels only). Detailed quantitative results are shown in Appendix~\ref{subsec: LLM on fine-grained dataset}, illustrating a clear negative correlation between the label number and the result disparity between LLMs and SLMs across various IE tasks.

\begin{table}[!b]
    \centering
    \setlength\tabcolsep{2.2pt}
    \small
    \caption{
        The inference seconds over 500 sentences (run on single V100 GPU). Here LLaMA is extremely slow since we set batch size as 1 due to memory limit.
    }
    \begin{threeparttable}
    \begin{tabular}{l|ccccc}
        \toprule
         \textbf{Dataset (Task)} & Roberta & T5 & LLaMA & CODEX \\
         \midrule
        FewNERD (NER) & 2.8 & 39.4 & 1135.4 & 179.4  \\
         TACREV (RE) & 1.4 & 45.6 & 1144.9 & 151.6 \\
         ACE05 (ED) & 6.6 & 62.5 & 733.4 & 171.7  \\
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    \label{tab: speed}
\end{table}

\noindent \textbf{Comparisons among LLMs.} We observe performance variability among LLMs. (1) Open-source models, LLaMA and Vicuna, significantly lag behind proprietary LLMs across all few-shot IE tasks. (2) Among proprietary LLMs, ChatGPT performs better on NER and EAE tasks, but poorer so on RE and ED tasks. InstructGPT and CODEX demonstrate comparable performance across these tasks.

\noindent \textbf{LLMs show limited inference speed.} We compare the inference speed of different methods and show their results in Table~\ref{tab: speed}. We observe that LLMs is much slower than SLMs since they have much more parameters, longer input contexts and extra response decay (if external APIs applied).


\subsection{Analysis on Prompt Sensitivity}
\label{subsec: prompt analysis}
Previous work~\cite{lu-etal-2022-fantastically} indicates that the efficacy of LLMs on specific tasks can be significantly influenced by the construction of the prompt.  To ensure that LLMs' suboptimal outcomes are not erroneously ascribed to inappropriate prompt designs, we meticulously examine the impact of diverse prompt variations from four aspects, \ie instruction format, demo number, demo selector and prompt format. We leave comprehensive details of the variants and their results to Appendix~\ref{subsec: instruction variants}-~\ref{subsec: prompt format variants}, and illustrate salient findings in Figure~\ref{figs: prompt analysis}. Our findings include that (1) diverse instruction strategies yield comparable results in IE task; (2) increasing the number of samples in demonstrations does not unequivocally enhance performance; and (3) The selection strategy of demonstration matters, and retrieval based on sentence embedding (what we used) proves sufficiently effective. Consequently, we believe that there unlikely exists a \textit{lottery} prompt that substantially alters our conclusions that LLMs are not good few-shot IE solver.


\subsection{Discussion: Why LLMs Fail to Obtain Satisfactory Performance on IE Tasks?}
\label{subsec: discussion}

\noindent \textbf{Underutilized Annotations.}
We notice that LLMs appear to benefit less from additional annotations, \ie more training samples and label types, than SLMs. We speculate that LLMs are constrained by ICL in two ways. (1) More samples: The number of effective samples for LLMs, those in demos, is limited by maximum input length. Moreover, we also observe LLMs' performance plateaus in some tasks before reaching this limit (see Appendix~\ref{subsec: demo num}). Meanwhile, SLMs can continually learn from more samples through supervised learning, widening the performance gap as annotated samples increase. (2) More labels: LLMs struggle with fine-grained datasets. It suggests a difficulty in understanding numerous labels and their subtle interactions merely from the given instruction and exemplars for LLMs. Also, the examples per label in demos decrease as label types increase.

\noindent \textbf{Unexplored Task format.} As stated in~\citet{zhang-etal-2023-aligning}, IE-related tasks are scarce in the widely-used instruction tuning datasets like \citet{2022_flan} and \citet{wang-etal-2022-super}. Furthermore, the highly-flexible format of NER and ED tasks impair the ICL abilities~\footnote{These two tasks require unfixed numbers of (label, span) tuple. Furthermore, the length of each span is also unfixed.}. Therefore it is likely that instruction-tuned LLMs are not well-acquainted with such IE-related task formats.