\section*{Limitations}

We do work hard to find better prompts to elicit the power of LLMs on few-shot IE tasks in Section~\ref{subsec: prompt analysis}, by exploring various kinds of LLMs, demonstration strategies and prompt formats. We find that different prompt variants do not significantly impact in-context learning abilities. As an empirical study, we acknowledge the potential existence of a \textit{lottery} prompt superior to our  explored prompts. However, it seems unlikely that an improved prompt would substantially alter our conclusions.

Another common risk when evaluating LLMs on public benchmark is their potential memorization of samples tested. To mitigate such potential contamination, we use earlier and stable versions of these models rather than the newer and updated ones (for example, \texttt{gpt-4-0314} instead of \texttt{gpt-4}). Even if such contamination makes abilities of LLMs overestimated, our primary conclusions remain unchanged because we find that LLMs are \textbf{NOT} good few-shot information extractors.

Regarding our adaptive \textit{filter-then-rerank} paradigm, a key limitation lies in how to assess sample difficulty. In this work, we employ a simple unsupervised metric, \ie the maximum probabilities from SLMs. This is predicated on the assumption that SLMs are well-calibrated~\cite{guo2017calibration}. However, it is an obviously imperfect assumption. We envision that calibrating SLMs-based filters or developing an advanced difficulty metric could substantially enhance LLM rerankers' performance. We leave them for future work.