\section{Auxiliary Experiments}

\begin{table}[!b]
    \centering
    \setlength\tabcolsep{2.5pt}
    \small
    \caption{Performance comparison between LLMs (ChatGPT) and SLM-based methods among datasets with various schema complexities.}
    \label{tab: performance v.s. label number}
    \begin{tabular}{lccc}
    \toprule
    \multicolumn{4}{c}{\cellcolor{gray!15}\textbf{Named Entity Recognition}} \\
     & \textbf{CoNLL} & \textbf{OntoNotes} & \textbf{FewNERD} \\
    \# Entity & 4 & 18 & 66 \\
    Micro-F1 (SLM) & 52.5 & 59.7 & 59.4 \\
    Micro-F1 (LLM) & 77.8 & 59.4 & 55.5 \\
    $\Delta$F1 (LLM, SLM) & 25.3 & -0.3 & -3.9 \\
    \midrule
    \multicolumn{4}{c}{\cellcolor{gray!15}\textbf{Event Detection}} \\
     & \textbf{ACE05} & \textbf{ERE} & \textbf{MAVEN} \\
    \# Event & 33 & 38 & 168 \\
    Micro-F1 (SLM) & 55.1 & 48.0 & 49.4 \\
    Micro-F1 (LLM) & 39.6 & 33.8 & 25.3 \\
    $\Delta$F1 (LLM, SLM) & -15.5 & -14.2 & -24.1 \\
    \midrule
    \multicolumn{4}{c}{\cellcolor{gray!15}\textbf{Event Argument Extraction}} \\
     & \textbf{ACE05} & \textbf{ERE} & \textbf{RAMS} \\
    \# Event / \#Role & 33 / 22 & 38 / 26 & 139 / 65 \\
    Head-F1 (SLM) & 45.9 & 40.4 & 54.1 \\
    Head-F1 (LLM) & 52.8 & 40.7 & 44.2 \\
    $\Delta$F1 (LLM, SLM) & 6.9 & 0.3 & -9.9 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{LLMs struggle on Fine-grained Datasets}
\label{subsec: LLM on fine-grained dataset}
Based on the results shown in Figure~\ref{figs: comparison result}, we additionally provide a quantitative analysis to show that LLMs struggle with fine-grained datasets. Under the 5-shot setting, we compare the performance difference of LLMs (ChatGPT) and SLMs (SoTA few-shot models) among different datasets. For each IE task, we observe a clear negative correlation between the label number (row 2) and the performance difference (row 5). In other words, with more label types, LLMs tend to perform relatively worse than SLMs. Therefore we conclude that LLMs struggle on fine-grained datasets.

\subsection{Finding Better Instruction} 
\label{subsec: instruction variants}
To investigate whether LLMs would benefit from complex instructions, we explored six instruction variants from simple to complex. Take NER task as an example, we illustrate them as below.

\noindent \textbf{Instruction0}: \texttt{[empty]}

\noindent \textbf{Instruction1}: \texttt{Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are: [Type\_1], [Type\_2], ..., [Type\_N]. If you do not find any entity in this sentence, just output `Answer: No entities found.'}

\noindent \textbf{Instruction2}: \texttt{Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are:}
\begin{itemize}
    \item \texttt{[Type\_1]: [Definition\_1]}
    \item \texttt{[Type\_2]: [Definition\_2]}
    \item \texttt{...}
    \item \texttt{[Type\_N]: [Definition\_N]}
\end{itemize}
\texttt{If you do not find any entity in this sentence, just output `Answer: No entities found.'}

\noindent \textbf{Instruction3}: \texttt{Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below:
[Type\_1], [Type\_2], …, [Type\_N].
Please note that your annotation results must follow such format: '''Answer: ([Type\_1] <SEP> identified\_entity:[Entity\_1]), ([Type\_2] <SEP> identified\_entity:[Entity\_2]), ......'''. 
If you do not find any entity in this sentence, just output `Answer: No entities found.'}

\noindent \textbf{Instruction4}: \texttt{Assume you are an entity-instance annotator. Your objective is to perform a series of intricate steps for Named Entity Recognition. Firstly, you have to identify a particular word or phrase in the sentence that corresponds to an entity. Following this, classify the entity into one of the potential entity types. The potential entity types are provided as below:
[Type\_1], [Type\_2], …, [Type\_N].
Please note that your annotation results must follow such format: `Answer: ([Type\_1] <SEP> identified\_entity:[Entity\_1]), ([Type\_2] <SEP> identified\_entity:[Entity\_2]), ......'. If you do not find any entity in this sentence, just output `Answer: No entities found.'}

\noindent \textbf{Instruction5}: \texttt{Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below:}
\begin{itemize}
    \item \texttt{[Type\_1]: [Definition\_1]}
    \item \texttt{[Type\_2]: [Definition\_2]}
    \item \texttt{...}
    \item \texttt{[Type\_N]: [Definition\_N]}
\end{itemize}
\texttt{Please note that your annotation results must follow such format: `Answer: ([Type\_1] <SEP> identified\_entity:[Entity\_1]), ([Type\_2] <SEP> identified\_entity:[Entity\_2]), ......'. If you do not find any entity in this sentence, just output `Answer: No entities found.'}

Regarding these six instructions, we evaluate their performance of ChatGPT on four 20-shot IE tasks. As shown in Table~\ref{tab: instruction variants}, there is no significant correlation between the instruction complexity and LLMs’ performance. Even the prompt without instruction (I0) leads to comparable, if not better, results than prompt with complex instructions. Therefore, we use simple instruction (I1) in our main experiment.

\begin{table}
    \small
    \centering
    \caption{F1-scores across six instruction formats. Experiments run on 20-shot settings with ChatGPT.}
    \begin{tabular}{c|c|c|c|c}
        \toprule
        & \makecell{\textbf{FewNERD} \\ (NER)} & \makecell{\textbf{TACREV} \\ (RE)} & \makecell{\textbf{ACE} \\ (ED)} & \makecell{\textbf{ACE} \\ (EAE)} \\
        \midrule
        I0 & $57.6${\tiny $(2.1)$} & $49.1${\tiny $(2.4)$} & $44.0${\tiny $(1.4)$} & $50.9${\tiny $(0.1)$} \\
        I1 & $58.3${\tiny $(0.5)$} & $49.6${\tiny $(1.2)$} & $42.6${\tiny $(1.0)$} & $51.5${\tiny $(1.1)$} \\
        I2 & $57.7${\tiny $(1.0)$} & $50.0${\tiny $(1.5)$} & $41.8${\tiny $(0.9)$} & $50.3${\tiny $(1.5)$} \\
        I3 & $57.6${\tiny $(2.3)$} & $52.3${\tiny $(1.8)$} & $42.9${\tiny $(1.3)$} & $49.2${\tiny $(2.3)$} \\
        I4 & $56.8${\tiny $(0.9)$} & $49.6${\tiny $(2.9)$} & $41.6${\tiny $(1.9)$} & $49.9${\tiny $(1.2)$} \\
        I5 & $57.8${\tiny $(0.5)$} & $47.2${\tiny $(1.8)$} & $43.1${\tiny $(1.8)$} & $50.6${\tiny $(1.8)$} \\
        \bottomrule
    \end{tabular}
    \label{tab: instruction variants}
\end{table}

\subsection{Do More Samples in Demos Help?}
\label{subsec: demo num}
We wonder whether longer demos bring more powerful ICL abilities for LLMs. Thus we investigate the impact of increasing the number of demonstrations on LLMs' performance in Figure~\ref{figs: demo num}. We observe that: (1) The performance of the RE task consistently improves with more demos, indicating its potential benefiting from additional annotations. (2) The NER and ED tasks reach a stable or degraded performance with increased demo numbers, suggesting that they are limited even before reaching the maximum input length. (3) Open-source LLMs, \ie LLaMA and Vicuna, have more limited capacities in leveraging demos compared to OpenAI models, with their performance stagnating or even collapsing with only a few (2-4) demos.

\begin{figure*}[!htbp]
\centering
    \subfigure[OpenAI LLMs]{
    \includegraphics[width=0.95\linewidth]{figs/res_dnum_openai.pdf}
    }
    \subfigure[Open-source LLMs]{
    \includegraphics[width=0.95\linewidth]{figs/res_dnum_openllm.pdf}
    }
\caption{Relationship between demo number and F1-score among three datasets. Note that the x-axis in each subfigure represents the number of demos (not the shot value $K$) during ICL. We adopt sentence embedding as the demo selection strategy and text prompt in this experiment.}
\label{figs: demo num}
\end{figure*}

\subsection{Finding Better Demo Selection Strategy} 
\label{subsec: demo selection}
The maximum input length of LLMs usually limits the sentence number in demos even under few-shot settings. For each test sentence $s$, we demand a demo retriever $\mathcal{E}(D, s)$ which selects a subset from $D$ as the sentences in demo. Following previous work, we consider three commonly-used strategies. (1) Random sampling. (2) Sentence-embedding~\cite{liu-etal-2022-makes, 2022_select-annot}: retrieving the top-K nearest sentences measured by sentence embedding. We compute the embeddings by \texttt{SimCSE-RoBERTa-large}~\cite{gao-etal-2021-simcse}.
\begin{equation}
    \mathcal{E}(D, s) = \text{arg-topK}_{s' \in D} [\text{Sent-embed}(s', s)]
\end{equation}
(3) Efficient Prompt Retriever~\cite{rubin-etal-2022-learning}: retrieving by a neural retriever $R$ trained on $D$.
\begin{equation}
    \mathcal{E}(D, s) = \text{arg-topK}_{s' \in D} [R_D(s', s)]
\end{equation}
For each test sentence $s$, we pre-retrieve $M$ similar sentences $\bar{D} = \{(s'_i, y'_i)\}_{i=1}^{M} \subset D$. Then we score each sentence in $\bar{D}$ by their likelihoods $P_\mathcal{L}(f(y'_i)|f(s'_i))$ where $f$ denotes the prompt format adopted and $\mathcal{L}$ the scoring LM. We randomly select positive samples ${s'}_i^{(\text{pos})}$ from the top-$K_D$ sentences and hard negative samples ${s'}_i^{(\text{hard-neg})}$ from the bottom-$K_D$ ones. Then we train $R_D$ by in-batch contrastive learning~\cite{Chen_2020}. For each sentence $s'_i$ within the batch, there are 1 positive sentences ${s'}_i^{(\text{pos})}$ and $2B-1$ negative sentences $\{{s'}_j^{(\text{hard-neg})}\}_{j=1}^B  \cup \{{s'}_j\}_{j \neq i}^B$.
Here we adopt $M$ as 40, $K_D$ as 5, $f$ as text prompt, the batch size $B$ as 128, and the scoring LM $\mathcal{L}$ as \texttt{FLAN-T5-xl}. 

\begin{table}[htbp!]
\small
\setlength\tabcolsep{1.0pt}
\centering
\caption{F1-scores on three demo-selection strategies. Experiments run on 20-shot settings with ChatGPT.}
    \label{tab: demo-retrieval}
    \begin{tabular}{c|ccc}
    \toprule
& \makecell{\textbf{FewNERD} \\ (NER)} & \makecell{\textbf{TACREV} \\ (RE)} & \makecell{\textbf{ACE} \\ (ED)}  \\
    \midrule
    Random Sampling  & $53.2${\tiny $(0.4)$} & $43.0${\tiny $(3.3)$} & $38.0${\tiny $(1.5)$} \\
    Sentence Embedding & $\textbf{57.6}${\tiny $(2.3)$} & $\textbf{49.6}${\tiny $(1.2)$} & $42.9${\tiny $(1.3)$} \\
    Efficient Prompt Retriever & $57.2${\tiny $(0.6)$} & $48.0${\tiny $(0.8)$} & $\textbf{43.5}${\tiny $(1.4)$} \\
    \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab: demo-retrieval} demonstrates the F1-score performance on different selection strategies. We find that both the sentence embedding and EPR surpass random sampling by a large margin. Given the simplicity of the sentence embedding, we adopt it, rather than EPR, as our selection strategy in main experiment.

\begin{table}[htbp!]
\small
\setlength\tabcolsep{1.0pt}
\centering
\caption{F1-scores across three prompt formats. Experiments run on 20-shot settings with ChatGPT.}
    \label{tab: prompt format}
    \begin{tabular}{l|cccc}
    \toprule
    & \makecell{\textbf{FewNERD} \\ (NER)} & \makecell{\textbf{TACREV} \\ (RE)} & \makecell{\textbf{ACE} \\ (ED)} & \makecell{\textbf{ACE} \\ (EAE)} \\
    \midrule
    Text  & $57.6${\tiny $(2.3)$} & $49.6${\tiny $(1.2)$} & $42.9${\tiny $(1.3)$} & $\textbf{51.5}${\tiny $(1.1)$} \\
    Code & $53.2${\tiny $(0.9)$} & $\textbf{50.2}${\tiny $(1.8)$} & $\textbf{44.3}${\tiny $(2.0)$} & $47.3${\tiny $(1.5)$} \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Finding Better Prompt Format}
\label{subsec: prompt format variants}
Previous studies on LLMs for few-shot IE tasks have explored different prompt formats and highlighted the importance of selecting an appropriate format for achieving competitive performance. Therefore, we investigate two commonly-used variants in previous work: (1) Text prompt as shown in Figure~\ref{figs: prompt_examples}. (2) Code prompt: We follow ~\citet{wang-etal-2023-code4struct, li-etal-2023-codeie} and recast the output of IE tasks in the form of code. See more details about this format in their original papers.

Table~\ref{tab: prompt format} shows comparable performance across all formats. Based on simplicity, we choose the text prompt for our main experiment.