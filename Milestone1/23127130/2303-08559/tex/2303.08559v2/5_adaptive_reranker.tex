\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/filter-then-rerank-new.pdf}
    \caption{The overall architecture of our adaptive \textit{filter-then-rerank} paradigm. We color easy samples in orange and hard samples in pink. For easy samples, the final predictions are exactly from the SLM-based methods. For hard samples, the top-$N$ predictions from SLMs are fed into LLMs as the format of multiple-choice questions (pink box). The question is paired with demos (green box). LLMs rerank these $N$ candidates and generate the final prediction.
    }
    \label{figs: model}
\end{figure*}


\section{Adaptive Filter-then-rerank Paradigm}
Above findings can be summarized as: (1) SLMs generally outperform LLMs, especially with more training samples and fine-grained labels. (2) SLMs are much more time- and cost-efficient. (3) LLMs serve as powerful rerankers on \textit{hard} samples that challenge SLMs. Based on them, we propose a simple, efficient, and effective adaptive reranker that combines the strengths of SLMs and LLMs.

\subsection{Method} Our \textit{adaptive filter-then-rerank} approach, shown in Figure~\ref{figs: model}, uses supervised SLMs as a filter to make preliminary decisions. Samples with confidence scores exceeding threshold are viewed as easy samples otherwise hard ones. For easy samples, we retain SLM predictions as final results. For hard samples, top-$N$ predictions from SLMs are reranked via LLMs using ICL. Here LLMs employ MCQ prompts (Figure~\ref{figs: multi-choice question}), containing demos and a sample to be reranked. The LLMs then generate the final answer and optionally provide an explanation.

\subsection{Experimental Setup} We conduct experiments on FewNERD for NER task, TACREV for RE task and ACE05 for ED task. We employ top-performing SLM-based methods from Section~\ref{sec: slm_llm} (FSLS or KnowPrompt) as the filter, and Vicuna-13B, InstructGPT or GPT-4 as the reranker. The threshold $\tau$ to determine sample difficulty is optimized on the valid set. For hard sample, the top-3 SLM predictions and \texttt{None} (if not included) are feed to LLMs for reranking. Each LLM prompt has 4-shot demos. See demo examples in Appendix~\ref{sec: demo example}. We follow templates in \citet{lu-etal-2022-summarization} for TACREV and carefully design others. See these templates in Appendix~\ref{sec: template}. We adopt chain-of-thought reasoning~\cite{2022_cot}, \ie prefacing the answer with an explanation, to facilitate LLMs' reranking procedure.


\noindent \textbf{Baseline} We compare our method with two kinds of baselines to validate its effectiveness.

\noindent (1) LLMs with ICL: We follow the prompts in Section~\ref{subsec: LLM} and conduct experiments on three LLMs.

\noindent (2) Supervised SLMs: We follow previous SoTA methods shown in Section~\ref{subsec: main results} (FSLS or KnowPrompt). We additionally combine two SLMs with ensemble or reranking approach (\ie replace the LLM with another SLM as the reranker) to verify that improvements from our SLM-LLM integrated system are not solely due to the ensemble effects.

\begin{table*}[htbp!]
 \centering
 \caption{Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed \textit{filter-then-rerank} (SLM+LLM) methods. The best results are in bold face and the second best are underlined. 
 All results except InstructGPT and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket.}
 \label{tab: rerank performance}
 \setlength\tabcolsep{1.6pt}
 \small
    \begin{threeparttable}
        \begin{tabular}{@{}lp{4.2cm}|ccc|ccc|ccc@{}} 
        \toprule
        & & \multicolumn{3}{c|}{\textbf{FewNERD (NER)}} & \multicolumn{3}{c|}{\textbf{TACREV (RE)}} & \multicolumn{3}{c}{\textbf{ACE (ED)}} \\
         & & 5-shot & 10-shot & 20-shot & 20-shot & 50-shot & 100-shot & 5-shot & 10-shot & 20-shot \\
         \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{LLM}}}} & CODEX & $53.8${\tiny $(0.5)$} & $54.0${\tiny $(1.4)$} & $55.9${\tiny $(0.5)$} & $59.1${\tiny $(1.4)$} & $60.3${\tiny $(2.4)$} & $62.4${\tiny $(2.6)$} & $47.1${\tiny $(1.2)$} & $47.7${\tiny $(2.8)$} & $47.9${\tiny $(0.5)$} \\
        & InstructGPT & $53.6${\tiny $(-)$} & $54.6${\tiny $(-)$} & $57.2${\tiny $(-)$} & $60.1${\tiny $(-)$} & $58.3${\tiny $(-)$} & $62.7${\tiny $(-)$} & $52.9${\tiny $(-)$} & $52.1${\tiny $(-)$} & $49.3${\tiny $(-)$} \\
        & GPT-4 & - & - & $57.8${\tiny $(-)$} & - & - & $59.3${\tiny $(-)$} & - &  - & $52.1${\tiny $(-)$} \\
        \midrule
         \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{SLM}}}} & Previous SoTA & $59.4${\tiny $(1.5)$} & $61.4${\tiny $(0.8)$} & $61.9${\tiny $(1.2)$} & $62.4${\tiny $(3.8)$} & $68.5${\tiny $(1.6)$} &$72.6${\tiny $(1.5)$} & $55.1${\tiny $(4.6)$} & $63.9${\tiny $(0.8)$} & $65.8${\tiny $(2.0)$}  \\
        & \quad + Ensemble (S) & $59.6${\tiny $(1.7)$} & $61.8${\tiny $(1.2)$} & $62.6${\tiny $(1.0)$} & $64.9${\tiny $(1.5)$} & $71.9${\tiny $(2.2)$} & $74.1${\tiny $(1.7)$} & $56.9${\tiny $(4.7)$} &  $64.2${\tiny $(2.1)$} & $66.5${\tiny $(1.7)$} \\
        & \quad + Rerank (S) & $59.4${\tiny $(1.5)$} & $61.0${\tiny $(1.7)$} & $61.5${\tiny $(1.7)$} & $64.2${\tiny $(2.3)$} & $70.8${\tiny $(2.3)$} & $74.3${\tiny $(2.2)$}   & $56.1${\tiny $(0.3)$} &  $64.0${\tiny $(1.0)$} & $66.7${\tiny $(1.7)$} \\
        \midrule
        \parbox[t]{0.2mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{\textbf{SLM + LLM}}}} & \multicolumn{10}{c}{\cellcolor{gray!15}\textbf{Vicuna-13B}} \\
         & \quad + Rerank (L) & $60.0${\tiny $(1.8)$} & $61.9${\tiny $(2.1)$} &  $62.2${\tiny $(1.4)$} & $65.2${\tiny $(1.4)$} & $70.8${\tiny $(1.6)$} & $73.8${\tiny $(1.7)$} & $56.9${\tiny $(4.0)$} & $63.5${\tiny $(2.7)$} & $66.0${\tiny $(2.6)$}  \\
         & \quad + Ensemble (S) + Rerank (L) & $59.9${\tiny $(0.7)$} & $62.1${\tiny $(0.7)$} &  $62.8${\tiny $(1.1)$} & $66.5${\tiny $(0.5)$} & $73.6${\tiny $(1.4)$} & $75.0${\tiny $(1.5)$} & $57.9${\tiny $(5.2)$} & $64.4${\tiny $(1.2)$} & $66.2${\tiny $(2.4)$} \\
        & \multicolumn{10}{c}{\cellcolor{gray!15}\textbf{InstructGPT}} \\
        & \quad + Rerank (L) & $60.6${\tiny $(2.1)$} & ${62.7}${\tiny $(0.8)$} & $63.3${\tiny $(0.6)$} & $66.8${\tiny $(2.6)$} & $72.3${\tiny $(1.4)$} & $75.4${\tiny $(1.5)$} & ${57.8}${\tiny $(4.6)$} & $\underline{65.3}${\tiny $(1.7)$} & $67.3${\tiny $(2.2)$}  \\
        & \quad + Ensemble (S) + Rerank (L) & $\textbf{61.3}${\tiny $(1.9)$} & $\textbf{63.2}${\tiny $(0.9)$} & $\textbf{63.7}${\tiny $(1.8)$}& $\textbf{68.9}${\tiny $(1.3)$} & $\textbf{74.8}${\tiny $(1.3)$} & $\textbf{76.8}${\tiny $(1.2)$} & ${59.5}${\tiny $(3.7)$} & ${65.3}${\tiny $(1.9)$} & $\underline{67.8}${\tiny $(2.1)$}  \\
        & \multicolumn{10}{c}{\cellcolor{gray!15}\textbf{GPT-4}} \\
        & \quad + Rerank (L)  & ${60.8}${\tiny $(2.3)$} & ${62.6}$ {\tiny $(2.7)$}& ${63.0}${\tiny $(1.3)$} & ${65.9}${\tiny $(2.7)$} & ${72.3}${\tiny $(0.3)$} & ${74.5}${\tiny $(1.5)$} & $\underline{59.6}${\tiny $(2.9)$} &${64.9}${\tiny $(2.5)$} & ${67.1}${\tiny $(2.5)$} \\
        & \quad + Ensemble (S) + Rerank (L)  & $\underline{61.1}${\tiny $(2.2)$}  & $\underline{62.8}${\tiny $(0.9)$} &  $\underline{63.6}${\tiny $(1.2)$} & $\underline{68.6}${\tiny $(1.3)$} & $\underline{73.9}${\tiny $(1.4)$} & $\underline{75.9}${\tiny $(2.4)$} & $\textbf{60.9}${\tiny $(3.9)$} & $\textbf{65.6}${\tiny $(1.5)$} & $\textbf{67.8}${\tiny $(1.7)$} \\
        \bottomrule
        \end{tabular}
    \end{threeparttable}
\end{table*}


\subsection{Main Results}
Table~\ref{tab: rerank performance} shows that our \textit{filter-then-rerank} method consistently improves performance across three datasets and nine settings. For instance, with InstructGPT, reranking provides an average F1 gain of 2.4\% without SLM ensemble (Lines 4 vs. 7). Based on ensemble SLMs as the filter, our method still achieves 2.1\% (Lines 5 vs. 8) gains on average. This confirms (1) the effectiveness of the LLM reranking and 
(2) its gains are different and (almost) orthogonal to the SLM ensemble.


\subsection{Analysis}
\noindent \textbf{Few makes big difference}
Our method selectively reranks hard samples. Table~\ref{tab: big diff} shows that (1) only a minor fraction (0.5\%\textasciitilde10\%) of samples are deemed hard and are reranked by LLMs. (2) Despite their limited quantity, reranking results in a substantial performance boost on these samples (10\%\textasciitilde25\% absolute F1 gains). This uplift on a small subset significantly enhances the overall performance.

\begin{table}[]
 \centering
  \small
  \setlength\tabcolsep{1.8pt}
   \caption{
        The F1-score differences before and after reranking on the reranked samples, as well as their proportion of the total samples. 
    }
    \begin{threeparttable}
        \begin{tabular}{l|cccc|cccc}
        \toprule
         & \multicolumn{4}{c}{\textbf{GPT-4}} \vline & \multicolumn{4}{c}{\textbf{InstructGPT}} \\
         & before & after & $\triangle$ &ratio & before & after & $\triangle$ & ratio \\
         \midrule
         FewNER & $31.9$ & $40.7$ & $8.8$ & $3.2\%$ & $31.4$ & $28.3$ & $-3.1$ & $3.3\%$ \\
         TACREV & $25.3$ &$43.0$ & $17.7$ & $9.1\%$ & $33.8$ & $43.4$ & $9.6$ & $7.1\%$\\
         ACE05 & $31.1$ & $57.9$ & $26.8$ & $1.6\%$ & $35.6$ & $55.7$ & $20.1$ & $0.5\%$\\
        \bottomrule
        \end{tabular}
    \end{threeparttable}
    \label{tab: big diff}
\end{table}


\noindent \textbf{GPT-4 is more aggressive} 
From Tables~\ref{tab: rerank performance} and~\ref{tab: big diff}, GPT-4 generally improves more on hard samples, yet InstructGPT surpasses GPT-4 in NER and RE tasks when evaluated overall. This discrepancy arises from GPT-4's aggressive reranking which introduces more true positives. InstructGPT, however, focuses more on reducing false positives.

\noindent \textbf{Few makes small cost} 
Figure~\ref{figs: cost} demonstrates that our method impressively reduces budget and latency by approximately 80\%\textasciitilde90\% compared to direct ICL. This reduction is due to (1) fewer LLM callings (only for hard samples) and (2) shorter prompts (fewer candidate labels and demos).

\begin{figure}[!b]
    \centering
    \includegraphics[width=\linewidth]{figs/cost.pdf}
    \caption{The financial and time cost over 500 sentences. InstructGPT as the reranker.}
    \label{figs: cost}
\end{figure}


\subsection{Ablation Study}
We investigate the effectiveness of the modules in adaptive \textit{filter-then-rerank} system by removing each of them in turn: (1) \textbf{CoT}: We exclude the explantion for each examples in demo. (2) \textbf{Demo}: We remove all examples, rendering the reranking a zero-shot problem. (3) \textbf{LF} (label filtering): We retain all labels as candidate choices for reranking, instead of only the top-$N$ labels from the SLMs. (4) \textbf{AD} (adaptive): We feed all samples, not just hard ones, to the LLMs.


We show their results in Table~\ref{tab: Ablation study} and see that (1) Demos with explanations consistently enhance the reranking ability of LLMs across all datasets. (2) Demos without explanations also contribute to performance improvement. (3) Label filtering results in gains and notably reduces the demo length, hence cutting inference costs. (4) The performance collapses without a filter to identify sample difficulty, reiterating the need for an integrated SLM-LLM system to complement each other.


\begin{table}
 \centering
  \small
  \setlength\tabcolsep{2.0pt}
   \caption{Ablation study on three datasets. The filter is ensembled SLMs and the reranker is GPT-4.}
    \begin{threeparttable}
        \begin{tabular}{cccc|ccc}
        \toprule
         \textbf{CoT} & \textbf{Demo} & \textbf{LF} & \textbf{\shortstack{AD}} & \textbf{\shortstack{FewNERD\\(20-shot)}} & \textbf{\shortstack{TACREV\\(100-shot)}} & \textbf{\shortstack{ACE05\\(20-shot)}} \\
        \midrule
        \cmark & \cmark & \cmark & \cmark & $\textbf{63.6}${\tiny $(1.2)$} & $\textbf{75.9}${\tiny $(2.4)$} & $\textbf{67.8}${\tiny $(1.7)$} \\
        \midrule
        \xmark & \cmark & \cmark & \cmark & $63.2${\tiny $(1.2)$} & ${75.4}${\tiny $(2.4)$} & $67.2${\tiny $(1.7)$} \\
        \xmark & \xmark & \cmark & \cmark & $63.0${\tiny $(1.4)$} & ${74.9}${\tiny $(2.2)$} & $66.6${\tiny $(1.5)$} \\
        \xmark & \xmark & \xmark & \cmark & $62.4${\tiny $(2.1)$} & $73.8${\tiny $(2.5)$}  & $66.5${\tiny $(1.3)$} \\
        \xmark & \xmark & \xmark & \xmark & $12.5${\tiny $(2.7)$} & $59.9${\tiny $(6.0)$} & $5.4${\tiny $(1.1)$} \\
        \midrule
        \multicolumn{4}{c}{Previous SoTA methods} \vline & $62.6${\tiny $(1.0)$} & $74.1${\tiny $(1.7)$} & $66.5${\tiny $(1.7)$} \\
        \bottomrule
        \end{tabular}
    \end{threeparttable}
    \label{tab: Ablation study}
\end{table}