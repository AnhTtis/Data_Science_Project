\section{Related Work}

\subsection{LLMs for Information Extraction}
Recent studies have increasingly explored Information Extraction (IE) tasks using LLMs. Drawing inspiration from instruction tuning~\cite{2022_flan}, several methods~\cite{wadhwa-etal-2023-revisiting, Wang2023InstructUIEMI, Lu2023PIVOINEIT} transform annotated samples into instruction-answer pairs and then fine-tune LLMs, such as FlanT5~\cite{2022_flant5}, on them. Nonetheless, this method necessitates a vast range of samples with diverse schemas and often yields suboptimal results in low-resource scenarios. In the context of few-shot IE tasks, prevalent strategies bifurcate into two main streams. The first approach perceives LLMs as efficient annotators~\cite{ding-etal-2023-gpt, 2023_synIE}. In these methods, they produce a plethora of pseudo-labeled samples through LLMs and leverage the enhanced annotations to train SLMs. Conversely, the latter approach employs LLMs in inference using the ICL paradigm, which is the focus of our subsequent discussion.

\subsection{Few-shot IE with ICL}
Regarding few-shot IE tasks, recent studies intensively compare the performance between SLMs and LLMs but yield inconsistent conclusions. Some studies favor LLMs as competent few-shot extractors~\cite{agrawal-etal-2022-large, wang-etal-2023-code4struct, li-etal-2023-codeie, zhang-etal-2023-aligning, wadhwa-etal-2023-revisiting}, while others dispute this claim~\cite{jimenez-gutierrez-etal-2022-thinking, qin2023_benchmark, wei2023zeroshot, gao2023exploring}. This discrepancy leaves the question of \textit{whether LLMs perform competitively on few-shot IE tasks} unresolved, thus hindering the advances of this domain.

We attribute such disagreement to the absence of an comprehensive and unified benchmark. Existing studies usually vary in tasks, datasets, and few-shot settings. Furthermore, some studies rely on overly simplistic datasets~\cite{jimenez-gutierrez-etal-2022-thinking, li-etal-2023-codeie} and may exaggerate the effectiveness of LLMs. Driven by these findings, our research undertakes comprehensive experiments across four IE tasks, nine datasets with various schema complexities (from coarse-grained to fine-grained) and low-resource settings. 

In addition to the empirical study, we develop an innovative \textit{filter-then-rerank} paradigm to combine the strengths of both LLMs and SLMs. It utilizes prompting strategies akin to QA4RE~\cite{zhang-etal-2023-aligning}, transforming IE tasks into multi-choice questions. However, our method stands apart by integrating SLMs and LLMs within a single framework. This incorporation (1) enables our paradigm applicable to various IE tasks by providing candidate spans in the text and (2) achieves promising performance under low-resource IE scenarios.