%==============================
\section{Introduction}
\label{sec:intro}
%==============================

Large Language Models (LLMs,~\citealt{2020_gpt3, 2022_PaLM, touvron2023llama}) have shown remarkable abilities on various NLP applications such as factual question answering~\cite{yu2023generate, sun2023recitationaugmented}, arithmetic reasoning~\cite{chen2022program, qian2023creator} and logical reasoning~\cite{jung-etal-2022-maieutic, pan2023logiclm}. Given the reasoning, memorization, instruction-following and few-shot adaption capabilities emerging from LLMs, it prompts a compelling question: Can LLMs be used to boost performance in few-shot information extraction (IE) tasks?

To answer this question, we conduct an extensive empirical study to compare the performance between LLMs using \textit{in-context learning}~\footnote{All LLMs discussed in this paper are not fine-tuned, and results for LLMs are based on in-context learning.} (ICL) and \textit{fine-tuned} Small Language Models (SLMs). We fairly evaluate SLMs-based and LLMs-based methods across nine datasets spanning four common IE tasks: (1) Named Entity Recognition, (2) Relation Extraction, (3) Event Detection and (4) Event Argument Extraction. For each dataset, we explored four to six settings to encompass typical low-resource extents, from 1-shot to 20-shot or even more. Given the potential sensitivity of LLMs' performance to the prompt context, we meticulously considered variations in instruction, demonstration number and selection strategy, prompt format, \etc.
Our study reveals that LLMs excel over SLMs only when annotations are extremely limited, \ie both label types\footnote{Label types denote \textit{entity/relation/event/role types} in different tasks. We use them interchangeably there-in-after.} and the samples\footnote{Samples refer to (i) demonstrations in ICL of LLMs, or (ii) training samples for SLMs' fine-tuning.} per label are extremely scarce. With more (\eg hundreds of) samples, SLMs significantly outperform LLMs. Furthermore, LLMs incur greater inference latency and costs than fine-tuned SLMs. Hence, we claim that \textbf{current LLMs are not good few-shot information extractors in general}.

We further investigate whether LLMs and SLMs exhibit different abilities to handle various types of samples. We categorize samples according to their difficulty measured by SLMs' confidence scores, and compare LLMs' and SLMs' results within each group. We find that \textbf{LLMs are good at hard samples, though bad at easy samples}. We posit that the knowledge and reasoning abilities in LLMs enable them to handle hard samples (which are simply beyond SLMs' capabilities) well. Nevertheless, LLMs demonstrate strong predisposition to false-positive predictions on negative samples. Since most negative samples are easy samples (which could be solved readily by SLMs), the performance of LLMs on easy samples sometimes collapses and are usually much worse than fine-tuned SLMs.

Leveraging these findings, we pursue an approach to incorporate LLMs and SLMs within a single system and combine their merits.
To this end, we propose a novel \textit{filter-then-rerank} framework. The basic idea is that SLMs serve as a filter and LLMs as a reranker. Specifically, SLMs initially predict and determine the difficulty of each sample. If the sample is a hard one, we further pass the top-$N$ most-likely candidate labels from SLMs to LLMs for reranking. Otherwise we view the prediction from SLMs as the final decision. By providing easy/hard samples with different solution strategies, our system utilizes each model's strengths to complement each other. Also, it reranks only a small subset of samples and minimizes the extra latency and budgets for calling LLMs. With a modest cost increase, our framework yields a consistent F1 improvement, averaging 2.4\% higher than previous methods on various few-shot IE tasks. To the best of our knowledge, this is the first successful attempt to use LLMs to enhance few-shot IE tasks.


