\section{Details on SLMs}
\label{sec: details for SLMs}

We adopt five representative supervised methods to evaluate the ability of SLMs on few-shot IE tasks.

\noindent \textbf{(1). Fine-tuning (FT)}: Add a classifier head on SLMs to predict the labels of each sentence/word.

\noindent \textbf{(2). FSLS}~\cite{ma-etal-2022-label}: The state-of-the-art extractive-based method for few-shot NER task. \citet{ma-etal-2023-shot} also validate its competitive performance on few-shot ED tasks.

\noindent \textbf{(3). KnowPrompt}~\cite{2022_knowprompt}: The best extractive-based method for few-shot RE task.

\noindent \textbf{(4). PAIE}~\cite{ma-etal-2022-prompt}: The best extractive-based method for few-shot EAE task.

\noindent \textbf{(5). UIE}~\cite{lu-etal-2022-unified}: A competitive unified generation-based method for few-shot IE tasks.
We introduce their implementation details below:

\noindent \textbf{Fine-tuning/FSLS.} We implement these two methods by ourselves. We use \texttt{RoBERTa-large}~\cite{2019_roberta} as the backbones. We adopt Automatic Mixed Precision (AMP) training strategy\footnote{https://pytorch.org/docs/stable/amp.html} to save memory. We run each experiment on a single NVIDIA V100 GPU. We train each model with the AdamW~\cite{2019_AdamW} optimizer with linear scheduler and 0.1 warm-up steps. We set the weight-decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 64, the maximum input length as 192, the training step as 500 and the learning rate as 5e-5.

\noindent \textbf{KnowPrompt}  We implement this method based on original source code\footnote{https://github.com/zjunlp/KnowPrompt}, and use \texttt{RoBERTa-large} as our backbones. We set 10 maximum epochs for 50- and 100-shot datasets, and as 50 epochs for other datasets. We keep all other hyperparameters as default, and run each experiment on a single NVIDIA V100 GPU.

\noindent \textbf{PAIE}  We implement this method on original source code\footnote{https://github.com/mayubo2333/PAIE}, and use \texttt{BART-large}~\cite{lewis-etal-2020-bart} as backbones. We keep all hyperparameters as default for ACE and RAMS dataset. For ERE dataset, we set the training step as 1000, the batch size as 16 and the learning rate as 2e-5. We run each experiment on a single NVIDIA V100 GPU.

\noindent \textbf{UIE} We implement this method based on original source code\footnote{https://github.com/universal-ie/UIE}, and use \texttt{T5-large}~\cite{2019-T5} as the backbones. We run each experiment on a single NVIDIA Quadro RTX8000 GPU. We set the batch size as 4 with 4000 training steps. We set the maximum input length as 800 and the learning rate as 1e-4.

