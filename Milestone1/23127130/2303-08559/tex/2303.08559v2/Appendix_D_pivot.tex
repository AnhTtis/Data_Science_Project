\section{Pivot Experiments on LLMs}

\subsection{Sampling Temperature}
Existing prompt-engineering discussion\footnote{https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api} suggests setting the sampling temperature $t=0$ for tasks with structured outputs, including IE tasks. We validate this conclusion in Table~\ref{tab: temerature}, from which we could see the generated quality when $t=0$ is much higher than the quality when $t \neq 0$. Therefore we set $t=0$ in all main experiments, and do not take self-consistency~\cite{2023_self-consist} into account.

\begin{table}[]
\small
\setlength\tabcolsep{1.0pt}
\centering
\caption{F1-scores across different $t$ values. Experiments run on 10-shot settings with CODEX.}
    \label{tab: temerature}
    \begin{tabular}{l|ccc}
    \toprule
     & \textbf{FewNERD}  & \textbf{TACREV} & \textbf{ACE05} \\
    \midrule
    $t=0$  & $48.5${\tiny $(1.9)$} & $53.7${\tiny $(2.3)$} & $42.9${\tiny $(2.2)$} \\
    \quad + 5-ensemble & $\textbf{53.5}${\tiny $(1.3)$} & $\textbf{58.6}${\tiny $(1.5)$} & $\textbf{46.3}${\tiny $(0.8)$} \\
    \midrule
    $t=0.7$ & $40.9${\tiny $(2.3)$} & $39.9${\tiny $(1.2)$} & $35.6${\tiny $(1.0)$} \\
    \quad + self-consistency & $52.1${\tiny $(0.9)$}  & $53.4${\tiny $(1.3)$} & $45.6${\tiny $(3.0)$} \\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Automatic Chain-of-thought}
We additionally investigate whether rationales could facilitate LLMs' performance on few-shot IE tasks. Since there exists no golden rationales in original datasets, we follow Automatic Chain-of-thought (Auto-CoT;~\citealt{2023_auto-cot}) method as below. Regarding each sample, we query LLMs 

\textit{According to [sentence], Why [span] is a [label]}. 

\noindent For example, given the sentence \textit{``DSC and Traction Control on all \underline{Speed3} models is also standard.''}, we would feed LLM the query that \textit{``Could you explain why Speed3 is a kind of car''}. Then we insert the bootstrapped rationales between the sentences and ground-truth answers. If a sentence has no positive labels, however, we do not ask LLMs and keep the original format as the vanilla ICL approach. Here we prompt InstructGPT to generate the rationales with temperature $t=0.7$. We compare the performance with and without Auto-CoT as shown in Table~\ref{tab: pivot exp for auto-cot}.

\begin{table}[htbp!]
\setlength\tabcolsep{1.0pt}
\small
\centering
\caption{The F1-score difference between with and without Auto-CoT. We generate rationales by InstructGPT, then adopt \textbf{ICL w. Auto-CoT} approach and use CODEX as our backbone for inference.}
    \label{tab: pivot exp for auto-cot}
    \begin{tabular}{l|ccc}
    \toprule
    \textbf{10-shot train set} & \makecell{\textbf{FewNERD} \\ (NER)}  & \makecell{\textbf{TACREV} \\ (RE)}& \makecell{\textbf{ACE05} \\(ED)} \\
    \midrule
    wo. Auto-CoT & $\textbf{54.0}${\tiny $(1.4)$} & $\textbf{57.3}${\tiny $(1.8)$} & $\textbf{47.7}${\tiny $(2.8)$} \\
    w. Auto-CoT & $36.6${\tiny $(1.7)$} & $22.0${\tiny $(1.2)$} & $43.1${\tiny $(3.4)$} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table*}[ht!]
\small
\setlength\tabcolsep{1.5pt}
\centering
\caption{F1-scores difference among GPT-4, CODEX and InstructGPT.}
    \label{tab: pivot exp for two LLMs}
    \begin{tabular}{l|ccc|cc|ccc|ccc}
    \toprule
     & \multicolumn{3}{c|}{\textbf{NER (20-shot)}} & \multicolumn{2}{c|}{\textbf{RE (100-shot)}} & \multicolumn{3}{c}{\textbf{ED (20-shot)}} & \multicolumn{3}{c}{\textbf{EAE (20-shot)}} \\
    & \textbf{CONLL} & \textbf{OntoNotes} & \textbf{FewNERD}  & \textbf{TACREV} & \textbf{TACRED} & \textbf{ACE05} & \textbf{MAVEN} & \textbf{ERE} & \textbf{ACE05} & \textbf{RAMS} & \textbf{ERE} \\
    \midrule
    InstructGPT & 77.2 & 47.7 & 57.2 & \textbf{62.7} & \textbf{53.8} & 49.3 & 25.4 & \textbf{40.8} & \textbf{45.8} & \textbf{42.2} & \textbf{41.9} \\
    CODEX & 81.1 & 55.6 & 55.9 & 62.4 & 53.6 & 47.9 & 22.8 & 39.0 & - & - & - \\
    GPT-4 & \textbf{84.7} & \textbf{65.6} & \textbf{57.8} & 59.3 & 50.4 & \textbf{52.1} & \textbf{30.2} & 40.5 & 42.9 & 38.6 & 38.2\\
    \midrule
    Supervised SoTA & 72.3 & 74.9 & 61.4 & 72.6 & 63.1 & 65.8 & 54.7 & 56.2 & 55.2 & 57.7 & 55.6 \\
    \bottomrule
    \end{tabular}
\end{table*} 

We are frustrated to find Auto-CoT degrades the performance with a large margin. We speculate this degration could be attributed to three main reasons. (1) The rationale increase the length of each sample and thus decrease the overall example number in demos. (2) There exists an obvious discrepancy between sentences with and without positive labels. The rationales are only provided for sentences with positive labels because it is hard to explain why a sentence dose not contain any label. (3) Some auto-generated rationales are low-quality, especially for RE tasks. We would explore better strategy to exploit auto-genertaed rationales in the future work.


\subsection{GPT-4 v.s. Others}
\label{subsec: gpt-4}
We tend to minimize the GPT-4 calls due to its high price. Thus we utilize 20-/100-shot settings across each dataset to compare GPT-4's performance with other LLMs. Table~\ref{tab: pivot exp for two LLMs} reveals that GPT-4 does not outperform other LLMs significantly, except on OntoNotes and MAVEN. However, even on these datasets, GPT-4 still falls behind supervised SLMs by a significant margin. Consequently, the exclusion of GPT-4 does not undermine the conclusions drawn from our main experiments, and we omit it from our empirical study.