\section{Datasets}
\subsection{Full Datasets}
\label{subsec: full datasets}
We construct few-shot IE datasets and conduct the empirical study on nine datasets spanning four tasks, \textbf{with varying schema complexities ranging from 4 to 168}. We show their statistics in Table~\ref{tab:full dataset}.

\begin{table*}[t]
\small
\centering
\caption{Statistics of nine datasets used. Note that the \#\textit{mentions} for event detection tasks refers to the number of trigger words, while the \#\textit{mentions} for event argument extraction tasks refers to the number of arguments.}
    \label{tab:full dataset}
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{ll|ccc|cc|ccc|ccc}
    \toprule
    & & \multicolumn{3}{c|}{\textbf{Named Entity Recognition}} & \multicolumn{2}{c|}{\textbf{Relation Extraction}} & \multicolumn{3}{c}{\textbf{Event Detection}} & \multicolumn{3}{c}{\textbf{Event Arg Extraction}}  \\
    \multicolumn{2}{c|}{\textbf{Dataset}} & \textbf{CONLL} & \textbf{OntoNotes} & \textbf{FewNERD}  & \textbf{TACREV} & \textbf{TACRED} & \textbf{ACE05} & \textbf{MAVEN} & \textbf{ERE} & \textbf{ACE05} & \textbf{RAMS} & \textbf{ERE} \\
    \midrule
    \multicolumn{2}{c|}{\textbf{\#Label Type}} & 4 & 18 & 66 & 41 & 41 & 33 & 168 & 38 & 33 & 139 & 38\\
    \midrule
    \multirow{2}{*}{\textbf{\#Sents}} & Train & 14,041 & 49,706 & 131,965 & 68,124 & 68,124 & 14,024 & 32,360 & 14,736 & 14,024 & 7329 & 14,736\\
    & Test & 3,453 & 10,348 & 37,648 & 15,509 & 15,509 & 728 & 8,035 & 1,163 & 728 & 871 & 1,163\\
    \midrule
    \multirow{2}{*}{\textbf{\#Mentions}} & Train & 23,499 & 128,738 & 340,247 & 13,012 & 13,012 & 5,349 & 77,993 & 6,208 & 4859 & 17026 & 8924 \\
    & Test & 5,648 & 12,586 & 96,902 & 3,123 & 3,123 & 424 & 18,904 & 551 & 576 & 2023 & 822 \\
    \bottomrule
    \end{tabular}
\end{table*}



\subsection{Details of Few-shot IE Datasets}
\label{subsec: few-shot dataset details}
\noindent \textbf{Sampling Algorithm for Train/Valid Datasets.}
We downsample sentences from original training dataset to construct few-shot training and valid datasets. We adopt $K$-shot sampling strategy that each label has (at least) $K$ samples. We set 6 $K$-values (1, 5, 10, 20, 50, 100) for RE tasks and 4 $K$-values (1, 5, 10, 20) for other tasks. For RE task, each sentence has exactly one relation and we simply select $K$ sentences for each label. For NER, ED and EAE tasks, each sentences is possible to contain more than one entities/events/arguments. Since our sampling is at sentence-level, the algorithm of accurate sampling , \ie finding exactly $K$ samples for each label, is NP-complete\footnote{The \textit{Subset Sum Problem}, a classical NP-complete problem, can be reduced to this sampling problem.} and unlikely to find a practical solution. Therefore we follow \citet{yang-katiyar-2020-simple} adopting a greedy sampling algorithm to select sentences for NER and ED tasks, as shown in Algorithm~\ref{alg:sample algorithm}. Note that the actual sample number of each label can be larger than $K$ under this sampling strategy. For all three tasks, we additionally sample negative sentences (without any defined labels) and make the ratio of positive sentences (with at least one label) and negative sentences as 1:1. The statistics of the curated datasets are listed in Table~\ref{tab:fewshot dataset}. 


\begin{table}
\centering
\small
\caption{The statistics of few-shot training sets. We set different random seeds and generate 5 training sets for each setting. We report their average statistics.} 
\label{tab:fewshot dataset}        
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{0.8}
    \begin{tabular}{@{}cc|crrr} 
    \toprule
    \multicolumn{2}{c|}{\textbf{Dataset Settings}} & \# Labels  &\# Sent & \# Sample & \# Avg shot \\ \midrule
    \multirow{4}{*}{CONLL'03} & 1-shot & \multirow{4}{*}{4} & 4.8 & 5.8 & 1.4 \\
    & 5-shot &  & 16.2 & 21.8 & 5.5 \\
    & 10-shot &  & 29.2 & 42.6 & 10.7 \\
    & 20-shot &  & 65.6 & 82.0 & 20.5 \\
    \midrule
    \multirow{4}{*}{OntoNotes} & 1-shot & \multirow{4}{*}{18} & 20.0 & 33.4 & 1.9 \\
    & 5-shot &  & 84.8 & 148.0 & 8.2 \\
    & 10-shot &  & 158.6 & 281.0 & 15.6 \\
    & 20-shot &  & 332.8 & 547.2 & 30.4 \\
    \midrule
    \multirow{4}{*}{FewNERD} & 1-shot & \multirow{4}{*}{66} & 89.8 & 147.0 & 2.2 \\
    & 5-shot &  & 286.2 & 494.8 & 7.5 \\
    & 10-shot &  & 538.0 & 962.0 & 14.6 \\
    & 20-shot &  & 1027.2 & 1851.4 & 28.1 \\
    \midrule
    \midrule
    \multirow{6}{*}{TACREV} & 1-shot & \multirow{6}{*}{41} & 81.6 & 41.0 & 1.0 \\
    & 5-shot &  & 387.6 & 205.0 & 5.0 \\
    & 10-shot &  & 741.2 & 406.0 & 9.9 \\
    & 20-shot &  & 1367.2 & 806.0 & 19.7 \\
    & 50-shot &  & 2872.0 & 1944.0 & 47.4 \\
    & 100-shot &  & 4561.0 & 3520.0 & 85.9 \\
    \midrule
    \multirow{6}{*}{TACRED} & 1-shot & \multirow{6}{*}{41} & 81.6 & 41.0 & 1.0 \\
    & 5-shot &  & 387.6 & 205.0 & 5.0 \\
    & 10-shot &  & 741.2 & 406.0 & 9.9 \\
    & 20-shot &  & 1367.2 & 806.0 & 19.7 \\
    & 50-shot &  & 2871.2 & 1944.0 & 47.4 \\
    & 100-shot &  & 4575.2 & 3520.0 & 85.9 \\
    \midrule
    \midrule
    \multirow{4}{*}{ACE05} & 1-shot & \multirow{4}{*}{33} & 47.4 & 41.0 & 1.2 \\
    & 5-shot &  & 192.8 & 165.0 & 5.0 \\
    & 10-shot &  & 334.6 & 319.4 & 9.7 \\
    & 20-shot &  & 579.4 & 598.2 & 18.1 \\
    \midrule
    \multirow{4}{*}{MAVEN} & 1-shot & \multirow{4}{*}{168} & 157.6 & 298.0 & 1.8 \\
    & 5-shot &  & 540.4 & 1262.2 & 7.5 \\
    & 10-shot &  & 891.2 & 2413.8 & 14.4 \\
    & 20-shot &  & 1286.4 & 4611.4 & 27.4 \\
    \midrule
    \multirow{4}{*}{ERE} & 1-shot & \multirow{4}{*}{38} & 48.4 & 54.6 & 1.4 \\
    & 5-shot &  & 175.0 & 219.2 & 5.8 \\
    & 10-shot &  & 304.8 & 432.4 & 11.4 \\
    & 20-shot &  & 521.6 & 806.6 & 21.2 \\
    \midrule
    \midrule
    \multirow{4}{*}{ACE05} & 1-shot & \multirow{4}{*}{33} & 23.4 & 40.2 & 1.2 \\
    & 5-shot &  & 79.8 & 178.2 & 5.4 \\
    & 10-shot &  & 130.8 & 337.4 & 10.2 \\
    & 20-shot &  & 213.4 & 630.2 & 19.1 \\
    \midrule
    \multirow{4}{*}{RAMS} & 1-shot & \multirow{4}{*}{139} & 130.2 & 332.6 & 2.4 \\
    & 5-shot &  & 514.0 & 1599.6 & 11.5 \\
    & 10-shot &  & 795.2 & 3193.2 & 23.0 \\
    & 20-shot &  & 1070.4 & 6095.4 & 43.9 \\
    \midrule
    \multirow{4}{*}{ERE} & 1-shot & \multirow{4}{*}{38} & 21.6 & 102.8 & 2.7 \\
    & 5-shot &  & 74.2 & 403.4 & 10.6 \\
    & 10-shot &  & 127.2 & 775.6 & 20.4 \\
    & 20-shot &  & 190.2 & 1397.2 & 36.8 \\
    \bottomrule 
    \end{tabular}
\end{table}


\begin{algorithm}
\caption{Greedy Sampling}
\label{alg:sample algorithm}
    \begin{algorithmic}[1]
    \Require shot number $K$, original full dataset $\mathcal{D} = \{(\mathbf{X}, \mathbf{Y})\}$ tagged with label set $E$
    
    \State Sort $E$ based on their frequencies in $\{\mathbf{Y}\}$ as an ascending order
    \State $S \gets \phi $, $\text{Counter} \gets \text{dict}() $ 
    \For{$y \in E$}
    \State $\text{Counter}(y) \gets 0$
    \EndFor
    
    \For{$y \in E$}
    \While{$\text{Counter}(y) < K$}
    \State Sample $(\mathbf{X},\mathbf{Y}) \in \mathcal{D}$ s.t.$\exists j, y_j=y$
    \State $\mathcal{D} \gets \mathcal{D}\backslash(\mathbf{X},\mathbf{Y})$
    \State Update Counter (not only $y$ but all event types in $\mathbf{Y}$)
    \EndWhile
    \EndFor
    
    \For{$s \in \mathcal{S}$}
    \State $\mathcal{S} \gets \mathcal{S}\backslash s$ and update Counter
    \If{$\exists y \in E$, s.t. $\text{Counter}(y) < K$}
    \State $\mathcal{S} \gets \mathcal{S} \bigcup s$
    \EndIf
    \EndFor
    \State \Return $\mathcal{S}$
    \end{algorithmic}
\end{algorithm}

Based on the subsets constructed above, we optionally further split them into training and valid sets. For few-shot datasets with more than 300 sentences, we additionally split 10\% sentences as the valid set and the remaining sentences as training set. Otherwise, we do not construct valid set and conduct 5-fold cross validation to avoid overfitting. 