\section{LLMs are Good Few-shot Reranker}
\label{sec: LLMs are Good Few-shot Reranker}

\subsection{Filter-then-rerank Paradigm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/QA_prompt.pdf}
    \caption{Multi-choice question (MCQ) prompt.}
    \label{figs: multi-choice question}
\end{figure}

To mitigate LLMs' drawbacks mentioned above, we propose a \textit{filter-then-rerank} paradigm to integrate both SLMs and LLMs within the same system. This paradigm uses SLMs as filters to select the top-$N$ candidate labels, then LLMs rerank them to make final decisions. By using SLM-generated candidate answers, the focus of LLMs shifts from \textbf{sentence-level} (\ie identifying all entities/events in the sentence) to \textbf{sample-level} (\ie determining single entity/event candidate provided). Each question now corresponds to a single sample, allowing us to reframe prompts as multi-choice questions (MCQ; shown in Figure~\ref{figs: multi-choice question}) problem. Under such format, each candidate label is converted to a choice by pre-defined templates. 
We claim \textit{filter-then-rerank} paradigm is more likely to elicit the powers of LLMs and smoothly solve few-shot IE tasks because: (1) LLMs are more familiar with MCQ prompts than IE-format prompts~\cite{zhang-etal-2023-aligning}. (2) This paradigm reduces the label scopes significantly, since $N$ is usually much smaller than fine-grained label numbers.

\subsection{LLMs are \textit{Hard} Sample Solver}
Our \textit{filter-then-rerank} paradigm, unfortunately, presents unsatisfactory performance (and even suffers longer latency since LLMs rerank candidates per sample). Given LLMs' abilities in memorization and reasoning, however, we still believe that LLMs are potential to solve \textbf{some}, if not most, IE samples effectively. We hypothesize that LLMs are more proficient than SLMs on \textit{hard} samples. These samples are characterized by their requisite for external knowledge acquisition or sophisticated reasoning strategies, areas where LLMs can leverage their extensive parametric knowledge bases and inherent reasoning mechanisms. In contrast, SLMs often falter with such samples, constrained by their restricted modeling capacities.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/hier_new.pdf}
    \caption{Relationship between confidence scores and performance with/without LLM reranking. We adopt \texttt{RoBERTa-large} as filter and InstructGPT as reranker.}
    \label{figs: rerank result grouped by conf score}
\end{figure}

We leverage an unsupervised metric from SLMs to evaluate the \textit{difficulty} of samples. Given a sample $x$ in the sentence $s$, we define the highest probability across all labels as the confidence score:
\begin{align}
    \text{conf}(x) = \max_{l \in L} P_{SLM}(l|x; s)
\end{align}
where $L$ denotes the label set and $P_{SLM}(l|x; s)$ the probability of a span $x$ (in the sentence $s$) referring to label $l$ computed by SLMs. We classify samples with low confidence scores as \textit{hard} samples. Otherwise we view them as easy samples.


We conduct experiments to confirm our hypothesis that LLMs excel on \textit{hard} samples. We group samples by confidence scores and compare two methods within each group: (a) SLM-based methods without LLM reranking, and (b) SLMs as the filter and LLMs as the reranker. Method (b) differs from (a) by adding a single LLM to rerank the top-$N$ SLM predictions, using MCQ prompts.

The results in Figure~\ref{figs: rerank result grouped by conf score} substantiate our assumption. (1) LLM-based reranking (blue lines) enhances performance on hard samples (left areas in the figure). We provide a detailed analysis of specific challenging instances where LLM rerankers prove advantageous in Appendix~\ref{subsec: case study for hard samples}. These instances demonstrate the efficacy of LLMs in harnessing external knowledge and complex reasoning to rectify erroneous predictions initially made by SLMs (red lines). (2) Conversely, LLM-based reranking impedes performance on easy samples (right areas), resulting in a significant degradation, particularly for very easy samples (rightmost areas). In conclusion, LLMs exhibit greater proficiency in handling hard samples compared to SLMs, yet they underperform relative to SLMs on easy samples.

\begin{table}
\centering
\small
\setlength\tabcolsep{1.8pt}
\caption{Comparative ratios of negative to positive samples across various datasets and subsets. We set fixed threshold $\tau$ here for simplicity.}
\label{tab: NP ratio}
    \begin{tabular}{lccc}
    \toprule
     & \textbf{FewNERD} & \textbf{TACREV}  & \textbf{ACE05} \\
    \midrule
    Overall & 5.88 & 3.03 & 38.2 \\
    Easy samples ($\tau > 0.9$) & 9.44 &  3.21 & 44.0 \\
    Hard samples ($\tau < 0.6$) & 1.28 & 2.68 & 1.36 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Why LLMs Fail on Easy Samples}
\label{subsec: Why LLMs Fail on Easy Samples}
We investigate why LLMs (relatively) fail on easy samples in this section. As shown in Table~\ref{tab: NP ratio}, we observe significant higher negative sample ratios for easy samples across diverse IE tasks. In other words, most negative samples are easy samples for SLMs. Here we refer negative samples to those labeled as \texttt{None}. We speculate that the proficiency of SLMs with negative samples stems from their ability to adeptly discern apparent patterns during the fine-tuning stages. Therefore, SLMs could predict negative samples with (relatively) high confidence and accuracy. Due to LLMsâ€™ predisposition to false-positive predictions on negative samples, however, the performance of LLMs on easy samples collapses. We attribute such false-positive predictions to (1) hallucination and (2) span boundary mismatch. We detail such two kinds of mistakes with cases in Appendix~\ref{subsec: case study for easy samples}.

