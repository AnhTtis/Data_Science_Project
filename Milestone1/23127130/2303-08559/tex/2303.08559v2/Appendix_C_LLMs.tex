\section{LLMs Implementations}
\label{sec: details for LLMs}

Regarding our empirical study, we explore the ICL abilities of LLMs on few-shot IE tasks. We mainly use five LLMs from two sources. (1) OpenAI models: CODEX (\texttt{code-davinci-002};~\citealt{2021_codex}), InstructGPT (\texttt{text-davinci-003};~\citealt{2022_instructgpt}), and ChatGPT (\texttt{gpt-3.5-turbo-0301}). (2) Open-source models: LLaMA-13B~\cite{touvron2023llama} and its instruction-tuned counterpart, Vicuna-13B~\cite{vicuna2023}. We detail their implementation details in the next sections below.

\subsection{Open-source Models}
We implement multiple ICL approaches on LLaMA-13B and Vicuna-13B without fine-tuning. We set the maximum input length as 2048 and the batch size as 1. We run each experiment on a single NVIDIA V100 GPU. To achieve this, we leverage the \texttt{Accelerate}~\footnote{https://huggingface.co/docs/accelerate} framework and fp16 inference to save memory. We set maximum output length as 96 and sampling temperature as 0 (\ie greedy decoding). We set both \texttt{frequency\_penalty} and \texttt{presence\_penalty} as 0.

\subsection{OpenAI Models}
We implement multiple ICL approaches on OpenAI models by calling their official APIs~\footnote{https://openai.com/blog/openai-api}. We set the maximum input length as 3600 for all tasks and models. The only exception occurs when we use CODEX on RE tasks, where we set the maximum input length as 7000. We unify the maximum output length as 32 for RE task, and 96 for other three tasks. We set the sampling temperature coefficient as 0, \ie greedy decoding.