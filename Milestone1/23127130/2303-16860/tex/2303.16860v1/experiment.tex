\section{Experiments}\label{sec:exp}
\label{expav}

\begin{figure}[htbp]
	\centering
	\subfigure{
	\includegraphics[width=0.75\columnwidth]{ok.pdf}\hspace{-0.35cm}
	}
	\caption{Mechanical analog of inverted pendulums.}
        \label{df}
\end{figure}

We demonstrate the proposed Phy-DRL in an inverted pendulum case study, whose mechanical analog is shown in Figure~\ref{df}. The inverted pendulum system is characterized by the angle of the pendulum from vertical $\theta$, angular velocity of $\omega  \buildrel \Delta \over = \dot \theta$, the position of the cart $x$ and cart velocity $v \buildrel \Delta \over = \dot x$. The control goal is to stabilize the pendulum at the equilibrium $\mathbf{s}^* = [x^{*},v^{*},\theta^{*},\omega^{*}]^\top = [0,0,0,0]^\top$.  

To demonstrate the robustness of Phy-DRL, the following system matrix $\mathbf{A}$ and control structure matrix $\mathbf{B}$ are obtained without considering friction force, while the real plant is subject to friction force.  Specifically, we first take the dynamic model of the inverted pendulum described in~\cite{florian2007correct} and linearize it around the equilibrium $[x^{*},v^{*},\theta^{*},\omega^{*},] = [0,0,0,0]$, for which we assume that the obtained subsystem stays within a small neighborhood of this equilibrium, and we use the approximations: $\cos \theta  \approx 1$, $\sin \theta  \approx \theta$ and ${\omega ^2}\sin \theta  \approx 0$. 
\begin{align} 
\mathbf{A} &= \left[{\begin{array}{*{20}{c}}
1&{0.0333}&0&0\\
0&1&{ - 0.0565}&0\\
0&0&1&{0.0333}\\
0&0&{0.8980}&1
\end{array}} \right], \label{PM1}\\
\mathbf{B} &= \left[
0~~0.0334~~0~~- 0.0783
\right]^\top.  \label{PM2} 
\end{align}
The considered safety conditions are 
\begin{align} 
-0.6 \le x \le 0.6, ~~~-0.4 \le \theta < 0.4. \label{safetycond}
\end{align}
We let $\alpha = 0.8$. The matrices $\mathbf{P}$ and $\mathbf{F}$ are solved from LMIs \eqref{th1}--\eqref{th4a} via Matlab LMI toolbox:
\begin{align} 
&\mathbf{P} = \left[ {\begin{array}{*{20}{c}}
{{\rm{2}}{\rm{.0120}}}&{{\rm{0}}{\rm{.2701}}}&{{\rm{1}}{\rm{.4192}}}&{{\rm{0}}{\rm{.2765}}}\\
{{\rm{0}}{\rm{.2701}}}&{{\rm{2}}{\rm{.2738}}}&{{\rm{5}}{\rm{.1795}}}&{{\rm{1}}{\rm{.0674}}}\\
{{\rm{1}}{\rm{.4192}}}&{{\rm{5}}{\rm{.1795}}}&{{\rm{31}}{\rm{.9812}}}&{{\rm{4}}{\rm{.9798}}}\\
{{\rm{0}}{\rm{.2765}}}&{{\rm{1}}{\rm{.0674}}}&{{\rm{4}}{\rm{.9798}}}&{{\rm{1}}{\rm{.0298}}}
\end{array}} \right], \label{PM3}\\
&\mathbf{F} = \left[ {\begin{array}{*{20}{c}}
{0.7400}&{3.6033}&{35.3534}&{6.9982}
\end{array}} \right]. \label{PM4}
\end{align}

We let the high-performance reward $g( \mathbf{s}(k),\mathbf{a}(k)) = -a^2(k)$. Given the sub-reward and the knowledge \eqref{PM1}--\eqref{PM2}, the residual control \eqref{residual} and reward \eqref{reward} of Phy-DRL can be formed. 

\begin{figure}[ht]
\centering
\subfigure{\includegraphics[width=0.99\columnwidth]{compare_traj.pdf}}
\caption{The plot shows an example of state trajectories of the system controlled by the proposed Phy-DRL controller and model-based controller.}
\label{trajectory}
\end{figure}

\begin{figure}[ht]
\centering   
\subfigure[Cost during training]{\label{fig:a}\includegraphics[width=78mm]{cost.pdf}}
\vskip -3pt
\centering  
\subfigure[Critic loss during training]{\label{fig:b}\includegraphics[width=75mm]{criticloss.pdf}}
\caption{The plot illustrate the training progress with and w.o residual mechanism.}
\label{training}
\end{figure}


The DRL-controller is constructed using \textit{Multi-layer-perception} (MLP) that maps states to continuous actions. As shown in \ref{df}, the DRL-controller works together with the model-based controller 
to form the terminal control command $\mathbf{a}(k)$ as in~\eqref{residual}. For training, we take the cart-pole simulation provided in Open-AI gym~\cite{OpenAI-gym} and adapt it to a more realistic system with frictions and continuous action space. We leverage an off-policy \textit{actor-critic} algorithm DDPG~\cite{lillicrap2015continuous} to train the DRL-controller with the reward proposed in \eqref{reward}. 


In the first experiment, we compare the stability performance of the system controlled by the model-based controller and Phy-DRL controller. We initialize the inverted pendulum in the neighbourhood of the equilibrium and let these two controllers to control the system respectively. As shown in Figure~\ref{trajectory}, the model based-controller fails in stablizing the inverted pendulum and eventually goes out of the safety bound. The reason is that the model-based controller is derived from the linearized dynamic model without friction force, which contains large model mismatch compared to the dynamics during test. In contrast, the Phy-DRL controller can stabilize the pendulum robustly around the equilibrium, as the DRL agent learned to deal with the modeling uncertainties and compensate the weakness of the model-based controller.

In the second experiment, we showcase the influence of the model-based controller during training. We implement the stability (S) encouraging reward function derived in~\cite{westenbroek2022lyapunov} without residual mechanism as a baseline. As shown in Figure~\ref{training}, the training with the proposed reward~\eqref{reward}, stability and safety (S$\&$S) encouraging reward, using residual control converges significantly faster than the baseline. The similar effect can also observed in the training with S reward using residual control. Since the S$\&$S reward has the similar scale as S reward, they eventually converge at similar level after approximately forty thousand training steps. 


% To better demonstrate the robustness of the proposed physical RL framework, the dynamics model of the inverted pendulum for obtaining the linear model ignores the friction force, which can induce a large model mismatch with the model of ground truth. The model is adopted from the one derived in the literature~\cite{florian2007correct}: 

% \begin{subequations}
% \begin{align}
% \hspace{-.5cm}
% \dot{\theta} &= \omega, \nonumber\\
% \hspace{-.5cm}
% \dot{\omega} &= \frac{{3\left( {{m_c} + {m_p}} \right) \cdot g \cdot \sin \theta  - 3{m_p} \cdot l \cdot {{\omega}^2} \cdot \sin \theta  \cdot \cos \theta  - 3F \cdot \cos \theta }}{{4l \cdot \left( {{m_c} + {m_p}} \right) - 3l \cdot {m_p} \cdot {{\cos }^2}\theta }}, \nonumber\\
% \hspace{-.5cm}\dot{x} &= v, \nonumber\\
% \hspace{-.5cm} \dot{v} &= \frac{{F + {m_p} \cdot l \cdot {{\omega}^2} \cdot \sin \theta }}{{{m_c} + {m_p}}} - \frac{{{m_p} \cdot l \cdot \cos \theta }}{{{m_c} + {m_p}}} \cdot \frac{{3\left( {{m_c} + {m_p}} \right) \cdot g \cdot \sin \theta  - 3{m_p} \cdot l \cdot {{\omega}^2} \cdot \sin \theta  \cdot \cos \theta  - 3F \cdot \cos \theta }}{{4l \cdot \left( {{m_c} + {m_p}} \right) - 3l \cdot {m_p} \cdot {{\cos }^2}\theta }}, \nonumber
% \end{align}
% \end{subequations}



% \begin{subequations}
% \begin{align}
% \hspace{-.5cm}
% \dot{\theta} &= \omega, \nonumber\\
% \hspace{-.5cm}
% \dot{\omega} &= \frac{{3\left( {{m_c} + {m_p}} \right) \cdot g}}{{4l \cdot \left( {{m_c} + {m_p}} \right) - 3l \cdot {m_p}}}\theta  - \frac{3}{{4l \cdot \left( {{m_c} + {m_p}} \right) - 3l \cdot {m_p}}}F, \nonumber\\
% \hspace{-.5cm}\dot{x} &= v, \nonumber\\
% \hspace{-.5cm} \dot{v} &= {- \frac{{{m_p}}}{{{m_c} + {m_p}}} \cdot \frac{{3\left( {{m_c} + {m_p}} \right) \cdot g}}{{4\left( {{m_c} + {m_p}} \right) - 3{m_p}}}} \theta  + \left( {1 + \frac{{3{m_p}}}{{4\left( {{m_c} + {m_p}} \right) - 3{m_p}}}} \right)\frac{F}{{{m_c} + {m_p}}}. \nonumber
% \end{align}\nonumber
% \end{subequations}
% The sampling technique transforms the continuous-time reference model to the discrete-time model:  
% \begin{align}
% \mathbf{x}(k+1) = \mathbf{A}\mathbf{x}(k) +  \mathbf{B}u(k), ~~~~~u(k) = F(k)\nonumber
% \end{align}
% where we denote the sampling period by $T$.
% \begin{align}
% \hspace{-0.5cm}z\left( k \right) = \left[ {\begin{array}{*{20}{c}}
% {x\left( k \right)}\\
% {v\left( k \right)}\\
% {\theta \left( k \right)}\\
% {\omega \left( k \right)}
% \end{array}} \right]\!, ~A = \left[ {\begin{array}{*{20}{c}}
% 1&T&0&0\\
% 0&1&{\frac{{ - T \cdot {m_p}}}{{{m_c} + {m_p}}} \cdot \frac{{3\left( {{m_c} + {m_p}} \right) \cdot g}}{{4\left( {{m_c} + {m_p}} \right) - 3{m_p}}}}&0\\
% 0&0&1&T\\
% 0&0&{\frac{{3T \cdot \left( {{m_c} + {m_p}} \right) \cdot g}}{{4l \cdot \left( {{m_c} + {m_p}} \right) - 3l \cdot {m_p}}}}&1
% \end{array}} \right]\!, ~B = \left[ \begin{array}{l}
% 0\\
% \left( {1 + \frac{{3{m_p}}}{{4\left( {{m_c} + {m_p}} \right) - 3{m_p}}}} \right)\frac{T}{{{m_c} + {m_p}}}\\
% 0\\
% \frac{{ - 3T}}{{4l \cdot \left( {{m_c} + {m_p}} \right) - 3l \cdot {m_p}}}
% \end{array} \right]\!. \nonumber
% \end{align}

