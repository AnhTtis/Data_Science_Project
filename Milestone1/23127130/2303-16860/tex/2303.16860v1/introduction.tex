\section{Introduction}
Over the past decades, reinforcement learning (RL) has demonstrated breakthroughs for sequential decision making in broad areas,  ranging from autonomous driving \cite{kendall2019learning} and finance \cite{abe2010optimizing} to chemical processes \cite{savage2021model} and games \cite{silver2018general}. These success of RL were inherently limited to fairly low-dimensional problems, i.e., the previous RL frameworks lacked scalability. To remove the limitation, deep reinforcement learning (DRL) arises, which relies on deep neural networks for the powerful function approximation and representation learning of action value function, action policy, environment states, to name a few \cite{mnih2015human, silver2016mastering}. DRL has achieved tremendous success in many complex decision making tasks with high-dimensional state and action spaces, such as vision-based control of robots \cite{levine2016end}. DRL thus holds a promise for revolutionizing the artificial intelligence (AI) towards a higher-level understanding of the visual world, with tangible industrial and economic impact. But the recent frequent incidents of AI-assisted autonomous systems overshadow the revolutionizing potential of DRL as well, especially for the safety-critical systems. For instance, according to statistics released by the US National Highway Traffic Safety Administration (NHTSA) in the year 2022,  automakers reported nearly 400 crashes linked to self-driving, driver-assist technologies in 11 months \cite{NHTSA}. Unfortunately, NHTSA recently added 11 new deaths to a growing list of fatalities tied to the use of (semi)-automated driving system \cite{NHTSA11}. Hence, the particularly safety enhanced DRL is even more vital today, which aligns well with the marketâ€™s need for reliable deep learning technologies and motivates the research of safety and stability of DRL-enabled autonomous system, including the training and inference \cite{wachi2020safe,brunke2022safe,bucsoniu2018reinforcement}. 


Generally, a stable system shall have a property that, if the system starts from a safe region, it will eventually converge to the goal state, known as \textit{asymptotically stable} \cite{drazin1992nonlinear}.
The safety guarantee is thus desirable to prompt reliable DRL. To do so, the control Lyapunov function (CLF) is to proposed to encode such property into the reward function, such that the learning agent is regulated to learn to stabilize the system. For example, Chang and Gao in \cite{chang2021stabilizing} proposed to learn a Lyapunov function from sampled data and use it as an additional critic network to regulate the control policy optimization toward the decrease of the Lyapunov critic values. The challenge moving forward is how to design DRL to exhibit (mathematically) provable stability guarantee.  Importantly, the seminal study in \cite{westenbroek2022lyapunov} discovered that if the reward of DRL is CLF-like, the stability of DRL-enabled autonomous systems is mathematically guaranteed. 


Alternatively, CLF can be used to constrain the exploration state into the safety set, such that all actions will lead the system to decent on defined CLF~\cite{perkins2002lyapunov}, i.e., towards being stable.~\cite{berkenkamp2017safe} proposes to use Lyapunov stability theory to define a safety set and get the agent only explore and learn policy in the safety set. It also shows that the safety set can be expanded by using the Gaussian process to learn the dynamics. ~\cite{perkins2002lyapunov} used Lyapunov to construct basic-level control laws that can enjoy safety and performance guarantees. A reinforcement learning agent is introduced to switch the low-level control laws to finish the task optimally. Similarly, ~\cite{qin2022quantifying, wachi2020safe, fisac2018general, cheng2019end} propose to use prior knowledge of the model to constrain the exploration state with desired safety specifications, and the DRL is only allowed to explore in the constrained space to ensure safety. Typically, the safe region is derived by analyzing the stability of a linearized model and is often conservative, limiting the performance of the learned policy. Therefore, those approaches need to construct a more accurate dynamic model from the interaction data to expand the safe region~\cite{berkenkamp2017safe, cheng2019end}. In addition, the safety architectures~\cite{sha2001using,alshiekh2018safe,hewing2020learning} proposed in the control community can also be employed to ensure system-level safety for DRL-enabled systems for both training and inference. However, these architectures normally do not make assumptions about the internals of the learning agent, and thus can not encourage stability or safety during learning. Moreover, the safety envelope developed in those architectures might be limited by under-modeling errors presented in linearized models.

Although the success, the challenges moving forward are 
\begin{itemize}
    \item What is formal guidance of constructing control Lyapunov function for reward of DRL? 
    \item How to design DRL to have provable concurrent safety and stability guarantees? 
\end{itemize}


To address the challegnes, we proposed the physical deep reinforcement learning framework (Phy-DRL). The novelty of Phy-DRL is twofold, which can be summarized as follows. 
\begin{itemize}
    \item \textit{Physics-Model-Regulated Reward}, which provides guidance of constructing safety- and stability-aware reward.  
    \item \textit{Residual Control}, an integration of physics-model-based control and data-driven control, which in conjunction with regulated reward empower the Phy-DRL the (mathematically) provable safety and stability guarantees. 
\end{itemize}

This paper is organized as follows. In Section II, we present







