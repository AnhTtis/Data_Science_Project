\section{Introduction}

%\paragraph{introduction to multimodal social media}
% The growing popularity of multimedia is revolutionizing the  communications on social media.
Interpersonal communications in multimedia are gaining growing popularity on social media.
% The conventional text-only form has been expanded to cross modalities involving texts and images in information exchange.
%Many online platforms, such as Twitter, allow easy posting with both images and text.
More and more social media users are turning to pair images to text and vice versa to better voice opinions, exchange information, and share ideas, exhibiting rich and ever-updating resources in multimedia.
% Multimodal social platforms such as Twitter make users easy to create content by integrating texts and images.
%It is substantially because many online platforms, such as Twitter, allow easy posting with both images and text.
% \jing{Here is a gap: we should further discuss why we need to automate multimedia analysis, e.g., better use of the social media resources compared to the massive amounts of multimedia posts.}
%Millions of users together create 
While potentially benefiting people's everyday decision making, the huge volume of multimedia content might also challenge users in finding what they need.
Towards a more efficient and effective way to process the online multimodal data,
%enable easy access of the information in need for individuals, 
substantial efforts have been made to automatically understand the vision and language on social media through 
a broad range of  
multimodal classification tasks for predicting
%, e.g.,
%For better use of the social media resources compared to the massive amounts of multimedia posts,
%analyzing the multimodal posts is valuable for the areas of computional social science and derives many different multimodal classification tasks (i.g., 
image-text relations 
%classification 
\cite{DBLP:conf/acl/VempalaP19}, 
%multimodal 
sarcasm 
%detection 
\cite{DBLP:conf/acl/sarcasm}, 
%multimodal 
metaphor 
%understanding 
\cite{DBLP:conf/acl/ZhangZZ0L20}, point-of-interest 
%prediction
\cite{DBLP:conf/emnlp/VillegasA21}, 
%multimodal 
hate speech 
%detection 
\cite{DBLP:conf/acl/BotelhoHV21}, 
%and 
%multimodal 
sentiment 
%classification 
\cite{DBLP:conf/ijcai/Yu019}, etc.
%in social media.

Despite the success of visual-lingual understanding witnessed in common domains \cite{DBLP:conf/acl/HuangWCZCLL20, shi-etal-2020-improving, DBLP:conf/emnlp/WangJSYS21}, existing models' performance is likely to be compromised on social media posts. 
The possible reason lies in the relatively more implicit and obscure image-text relations therein \cite{DBLP:conf/acl/VempalaP19}, whereas the image-text pairs in the widely-used datasets outside social media (e.g., COCO dataset \cite{DBLP:conf/eccv/COCO}, VQA dataset  \cite{DBLP:conf/iccv/VQA}, VCR dataset \cite{DBLP:conf/cvpr/VCR}) tend to present explicit information overlap.
Such issue is nevertheless ignored in many previous solutions, which follow the common practice to fuse visual and lingual features \cite{DBLP:conf/acl/VempalaP19,DBLP:conf/acl/ZhangZZ0L20,DBLP:conf/emnlp/HesselL20,DBLP:conf/acl/BotelhoHV21}, making it hard for a multimodal model to well align cross-modal semantics attributed to their weak correlations \cite{nature2022_weak_semantic}. 

%\paragraph{The weakness of previous work (ignore the gap in img-text pair)}
%As indicated by \cite{DBLP:conf/acl/VempalaP19}, images and texts usually play different roles when expressing the overall multimodal posts, and the semantic relationships are split into four types based on the information overlap, which means there are often significant gaps of content in the image-text pairs. 
%However, most previous work (i.g., \cite{DBLP:conf/acl/VempalaP19}, \cite{DBLP:conf/acl/ZhangZZ0L20}, \cite{DBLP:conf/acl/BotelhoHV21}) ignore this potential problem and directly fuse the image features ans text features to understand the overall meaning of posts. 
%It's hard for the multimodal model to learn the alignment between the images and texts due to the weak semantic correlation \cite{DBLP:journals/corr/wenlan}. 

\input{figures/intro_case}

Nonetheless, human readers seem to have no problem in digesting the cross-modal meanings on social media;
%, regardless of the possibly implicit image-text relations;
in response to what they capture from a post, some readers may drop user comments, where some clues to hint cross-modal understanding may be hidden, e.g., an echo of the keypoints.
For instance, in Figure \ref{fig:intro}, ``snow'' in retrieved comments might strengthen the connection of ``weather'' in text with the snowing visual scene.
%\jing{We may discuss Figure 1 here.}
The helpfulness of user comments has also been demonstrated in the previous NLP practice for text-only posts \cite{DBLP:conf/naacl/WangLKLS19}.
Inspired by that, we propose ``borrowing'' the senses from human readers and modeling user comments to learn the hinting features therein to bridge the image-text gap.
%\paragraph{Retrieve comments to bridge the gap between img and text}
%To strengthen the semantic correlation, we utilize the comments posted by other users to fill the content gaps. 
%The comments under original posts usually contain the understanding of human readers to the posts, and could provide a bystander perspective to highlight the important content and bridge the potential correlation in the multimodal posts. 
%Due to that comments are missing in most multimodal social media datasets and not all multimodal posts contain comments, 
To further benefit posts without user comments, a comment retrieval algorithm is designed to gather comments from other posts in similarity, which is measured via balancing the visual and lingual semantics (henceforth \textbf{cross-modal similarity}). 
For the related experimental studies,  a large-scale dataset  is constructed to mimic the open environment (henceforth \textbf{wild dataset}). It contains over 27M multimodal tweets, each with 3 comments on average.

% \jing{It seems that the statistics in intro may distract the readers when they just start understanding our stories. Probably we can put it in the dataset part.}
 
Then, we explore how to leverage the retrieved comments in multimodal classification and exploit a self-training framework to identify comments' hints which shape the cross-modal understanding (henceforth \textbf{comment-aware self-training}).
This considers method feasibility in scenarios where large-scale labeled data is unavailable, which commonly appears in the realistic practice, because 
the annotation for multimodal data from social media is extremely expensive \cite{DBLP:conf/mm/MaYL0C19}.
Concretely, we adopt a teacher-student prototype \cite{DBLP:conf/emnlp/MengZHXJZH20, DBLP:conf/naacl/ShenQMSRH21} and tailor-make it to learn multimodal understanding with the help of user comments.
%\paragraph{Using self-training to generate pseudo-labeled data}
%Furthermore, large amount of supervised data are needed for multimodal classfication tasks to learn the interactions between image modality and text modality. However, manually annotating the labels for multimodal data is costly. Significant amount of multimodal unlabeled data could be obtained on the social platforms.
%Inspired by \cite{DBLP:conf/emnlp/MengZHXJZH20, DBLP:conf/naacl/ShenQMSRH21} which use self-training to solve the task of text classification, we propose a retrieval-based multimodal self-training method to automatically annotate the unlabeled data in this paper. 
%Clearly, we first train
%a teacher model with the labeled data, and generate pseudo labels for the unlabeled data 
A teacher model is first trained with the labeled data and pseudo-label the similar posts with comments retrieved from the wild dataset. 
%based on cross-modal similarity.
%which is retrieved from the constructed large-scale multimodal social media dataset based on the similarity to the labeled data. 
Then, a student model is trained in guidance of both the knowledge gained by the teacher model and hinting features offered by user comments. %\jing{Comments were not mentioned in the original version. Check if the description is correct.}
%the labeled data and pseudo-labeled data are both utilized to train the student model. 



%\paragraph{summarize key observations in experiments}
%For empirical studies, 
To evaluate our method in practice, it is comprehensively experimented on four popular social media multimodal benchmarks for varying  classification tasks.
In the setup of each benchmark, our comment-aware self-training module is customized to BERT-based state-of-the-art (SOTA) architectures.
% , where the all SOTA performance have been advanced.
Ablation studies then exhibit the individual benefits provided by comments and self-training.
%that comments enables significant advance of all SOTA results forwards and self-training further boosts the model performance.
%datasets. 
%The proposed unified comment retrieval and self-training framework could be easy added to the state-of-the-art models. 
%The main comparison results on all datasets indicate that the model with retrieved comments could obtain significant improvement compared with the SOTA models without comments. 
%And utilizing the self-training architecture can further boost the performance of classification. 
Then, we analyze the effects of retrieved post number and find the use of more posts would result in both the benefits and noise.
%the num of retrieved unlabeled  image-text pairs to the performance. 
Next, we probe into comment retrieval and explore the contributions of visual and lingual modality in cross-modal similarity measure, where we observe the joint effects allow the best results.
%impacts of image modality and text modality to the multimodal retrieval process and show that  the modality fusion could achieve the best results. 
At last, a case study shows how the retrieved comments mitigate cross-modal semantic gap, followed by the an error analysis to discuss the existing limitation.
%a case study is demonstrated how the retrieved comment bridge the text modality and image modality.

In summary, our contributions are three fold.

$\bullet$ We demonstrate the potential to employ retrieved user comments from similar posts for a better visual-lingual understanding on social media and gather 27M cross-media tweets with comments to released to support future research in this line.
% \footnote{The code and dataset will be released upon publication to support the future work in the effects of user comments over the learning of social media multimodal understanding and beyond.}
\footnote{Our code and dataset are released at \url{https://github.com/cpaaax/Multimodal_CAST}.}


$\bullet$ A comment-aware self-training method is proposed for cross-modal learning from both human senses underlying retrieved comments and knowledge distilled from labeled data in limited scales.

$\bullet$ An empirical study with substantial results is provided, where SOTA models of four popular social media benchmarks for multimodal classification perform better with the help of our comment-aware self-training module and the retrieved comments bridge social media images and text via hinting the connecting points for models to attend to.

%propose a   comment-aware self-training framework for social media multimodal classification, where the retrieved comments are utilized to connect the semantic gap between image and text, and the retrieved image-text pairs are used to generate pseudo-labeled data for self-training.


% \paragraph{summarize the  contributions}
% Three main contributions in the paper are summarized as follows:

% \begin{itemize}
% \item We propose a general unified retrieval-based comment-aware self-training framework for social media multimodal classification, where the retrieved comments are utilized to connect the semantic gap between image and text, and the retrieved image-text pairs are used to generate pseudo-labeled data for self-training.

% \item A large-scale dataset consists of about 26 million cross-media tweets are collected, where each tweet contains at least one comments.

% \item Comprehensive experimental results on three social media multimodal datasets show that the proposed method could significantly improve the accuracy of classification and achieve the SOTA performance.
% \end{itemize}