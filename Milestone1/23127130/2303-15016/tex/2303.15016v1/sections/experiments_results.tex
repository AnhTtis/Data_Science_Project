\section{Experimental Discussions}

\subsection{Main Comparison Results}\label{ssec:exp:main}



\input{sections/Tables/ablation}

Table \ref{tab:MVSA_results}$\sim$\ref{tab:MHP_results} shows the main comparison results on MVSA, ITR, MSD, and MHP, respectively. 
%The observations are summarized as follows: 
%(1) The 

%We first observe 
The full model significantly outperforms
%improve the performance compared with 
all baselines and advances their base ablation on all test benchmarks (measured by paired t-test; $p-value<0.05$). 
This indicates that our
%the retrieved comments and 
comment-aware self-training can universally benefit varying tasks  
and classification architectures.
%retrieval-based comment-aware self-training method 
%can be employed on different models and different multimodal social media tasks. 
%Specially, our method could obtain gains 
It enables performance gains on both the simple architecture (e.g., BERT-CNN for ITR) and other more complicated models. 
%(i.e., CoMN-BERT for MVSA, NMSD-BERT for MSD, and MMBT for MHP). 
The possible reasons are twofold. 
%for the improvement 
First, retrieved comments, carrying viewpoints from human readers, may provide
%could provide 
complementary context hinting the cross-modal semantic understanding for weakly-connected image-text pairs.
%for the image-text pairs 
Second, our self-training may enable the models to leverage both labeled data and unlabeled retrieved data, potentially mitigating the overfitting issue caused by insufficient labeled data scales.
%more data are included for training by using the self-training framework; 


We also observe the models with BERT encoders 
%BERT-based models 
consistently outperform their counterparts with LSTM encoders, either in 
%LSTM-based models 
%on both 
multimodal or unimodal architectures. 
These demonstrate the benefit of pre-training on large-scale text, where the gained generic language understanding capability may enable models to well induce cross-modal meanings.
%enable promising results on downstream visual-lingual social media tasks.
%This demonstrates that pre-trained language models have the excellent ability to capture the semantics of texts.  

\subsection{Ablation study}\label{ssec:exp:ablation}
The general superiority of our method has been demonstrated in $\S$\ref{ssec:exp:main} compared to previous benchmark results.
Here we conduct an ablation study to further probe the relative contribution of varying components and 
%To examine the effects of different components of the proposed comment-aware self-training method, we conduct an ablation study and 
show the results 
%for the four tasks 
in Table \ref{tab:ablation_results}. 
%Both comments and self-training module contribute greatly to the model. 

The obvious performance drop of Base+Com and Base+ST, compared to the Full Model, together suggest the positive effects individually from retrieved comments and self-training.
These strengthen our previous findings: the comments may enrich context with human hints to bridge visual and lingual semantics and self-training may enrich the data scales with semantically related posts and comments to allow better robustness.  
%This demonstrates that comments could provide necessary context and bridge the images and texts while utilizing the self-training module to add semantically similar data into the training set is helpful for the four tasks. 

For the results of Base+Com+ST, though better than other ablations, are slightly worse than the Full Model.
It implies the extra benefit of modeling retrieved comments in self-training, while randomly dropping some of them may enable the student model to better catch up with the teacher, mitigating the least favorable perturbation phenomena \cite{DBLP:conf/cvpr/XieLHL20} in teacher-student alignment.

%Furthermore, from the results of Base+Com+ST and Base+Com+ST, adopting the strategy of randomly dropping comments could further boost the performance. 
%The reason might be that the student model with fewer comments would have more difficulty to align with the predictions of teacher model, which leads to favorable perturbation for the training step.



\subsection{Quantitative Analysis}

$\S$\ref{ssec:exp:ablation} shows the crucial roles self-training and comment retrieval play in our method. 
Here we further quantify the effects of varying unlabeled data scales on self-training and those of individual modality (images or text) on comment retrieval.

\input{figures/self_train_num}
\input{figures/modality}

%\paragraph{The effects of the num of retrieved image-text pairs used in the self-training}
\paragraph{Self-training w/ Varying Unlabeled Data Scales.}
%To examine model sensitivities to varying num of image-text pairs used for the step of self-training, 
Here we train our full model via self-training with varying number of retrieved posts ($K$) and show the performance gain compared to Base Classifier in Figure \ref{fig:self_train_num} (F1 difference of the Full Model and Base Classifier).
%setting the num $k$ of similar posts to $1,3,5,7,9$. 
%As shown in Figure \ref{fig:self_train_num}, 
%We observe that most models obtain the best performance when $k$ is 3 or 5. And the performance world decrease with more data (i.e., $k$=7 or 9). 
We observe the results peak at $K=3$ or $5$, implying  self-training may benefit from some similar unlabeled data while further retrieving more data may result in noise as well.

%The reason might be that there are not enough semantically similar posts in the retrieval dataset, and it's might be hard for the model to learn from the pseudo-labeled posts which are not similar to the original labeled data.

%\paragraph{The efftects of the modality used for the retrieval}
\paragraph{Modality Effects on Comment Retrieval.}
Recall that in comment retrieval, we balance the visual and lingual similarity to retrieve similar posts (and obtain their comments). 
To further study how features in varying modalities affect comment retrieval results, we examine two ablations relying on the similarity in image (Only Image) and text (Only Text) in comparison to the full model trading-off image and text similarities (Image+Text).

%To investigate the effectiveness of the proposed comment-aware self-training method when used with different retrieval modalities, we conduct experiments with only using image features, only using text features, and using image and text features for the retrieval process. 
The results (F1 gain compared to Base Classifier) are shown in Figure \ref{fig:modality}.
Varying tasks might prefer the similarity measure with image or text semantics,
whereas the full model leveraging posts' visual and lingual features achieves the best results.

%We can observe that the model with both modalities could consistently obtain the best performance on all tasks compared with the unimodality. This demonstrates that semantic gaps exists in the image-text pairs, and it's necessary to consider the both modalities for the understanding.


\subsection{Qualitative Analysis}\label{ssec:exp:qualitative}

The discussions above are from a quantitative view. 
To provide more insight, a case study will be presented here, followed by analyses for error cases.

\input{figures/case_study}

\paragraph{Case Study.}
% To analyze how comments hint cross-modal understanding,  attention maps over comments  are shown in Figure \ref{fig:case_study}, where a case is sampled from each benchmark. 
To analyze how comments hint at cross-modal understanding,  attention maps over comments  are shown in Figure \ref{fig:case_study}, where the case is sampled from the MSD benchmark. 
% \footnote{More cases could be found in Appendix \ref{appendix:case}.}
As can be seen, models tend to capture the salient comments mentioning the key visual objects, e.g., ``owl'' and ``traffic camera'' in the case, helpfully connecting visual semantics to lingual.
It is probably because human readers are likely to echo crucial points in their comments in response to what they viewed from a post, which inspires models to explicitize the weakly-connected visual-lingual semantics.


%We can observe that the retrieved comments could provide the necessary context from human views to fill the semantic gap between images and texts. Taking the third case for example, the text ``snowy owl is being ticketed for not reading the signs'', the model might be fused to distinguish the label of the post. However, the comment ``a traffic camera caught this amazing pic of an owl in flight'' could add external background context of the post. Therefore, the model can reasoning from the comments and  predict the true label.

\input{figures/error_analysis}

\paragraph{Error Analysis.}
%\input{figures/error_analysis}
%As mentioned above, the main gain of the proposed comment-aware self-training method come from the utilization of retrieval results. 
%Here we analyze error cases, which are mostly caused by 
The potential benefit has been potentially observed in varying cross-modal learning scenarios; however, many errors are also related to the retrieved comments and posts used in self-training.
Figure \ref{fig:error_analysis} summarizes the two major error types.
%Therefore, the quality of retrieval results could greatly influence the performance. 
%Here we summarize two main types of bad retrieval results which 
%are shown in : 

First, general comments, e.g., ``thank you'', are retrieved, useless in learning specific meanings in social media posts. This calls for a future direction for comment selection in a more effective manner.
%is general and useless for understanding the post. 
%the potential solution is 
%to design special rules to filter the retrieved comments. 
%
Second, semantically unrelated posts might be retrieved due to the misunderstanding to the query and hence result in irrelevant comments. 
For example, the posts concerning ``tomato'' are wrongly retrieved because of its similar color to ``beet sause''.
Future work may consider the advance in similarity measurement of cross-modal posts and the detection of high-quality unlabeled data 
(e.g., selecting the 
%highest-confidence 
pseudo-labeled data by confidence) for self-training.
 
%for self-training.

%The retrieved image-text pairs used for self-training are not related to the query post. 
%This reason might be that the similar posts are not included in the retrieval dataset, and the image encoder couldn't capture accurate objects. 
%Expanding the scale of the retrieval dataset and using more advanced image encoder could improve the quality of retrieved similar posts. Additionally, selecting the highest-confidence pseudo-labeled data instead of all retrieved similar posts might solve the dissimilar problem and improve the performance, which could be tried in future.