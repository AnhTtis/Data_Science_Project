\section{Comment-Aware Self-Training}
\input{figures/method_model}
This section presents the entire  comment-aware self-training workflow illustrated in Figure \ref{fig:model}.
In the following,
we first describe how we gather the wild dataset for retrieval, followed by the
%the details of the retrieval dataset 
%and conduct 
related analysis in $\S$\ref{ssec:method:dataset}. 
Then we introduce the retrieval module to 
%obtain 
find similar posts and their
%related 
comments in $\S$\ref{ssec:method:retrieval_module}. 
Next, we describe the usage of comments in the multimodal architecture in $\S$\ref{ssec:method:comment_usage}.
At last, $\S$\ref{ssec:method:self-training} presents the comment-aware 
%general 
%retrieval-based 
self-training framework based on retrieved results.


\input{sections/Tables/retrieval_dataset}
\subsection{Wild Dataset Construction for Retrieval }\label{ssec:method:dataset}

To simulate retrieval from the open environment, large-scale visual-lingual tweets with comments are gathered to form a wild dataset.
The detailed steps are described in the following. 
First, 
%we followed \citet{DBLP:conf/emnlp/NguyenVN20} to 
we downloaded the large-scale corpus used by \citet{DBLP:conf/emnlp/NguyenVN20} to pre-train their BERTweet. %containing
%, we first 
%download the Twitter data grabbed from the 
The dataset contains general Twitter streams 
%from the Internet Archive library.\footnote{\url{https://archive.org}} 
and we removed non-English tweets with 
fastText library \cite{DBLP:conf/eacl/fastText}.
Then, text-only tweets were removed and for the remaining 60 million, 
%60
%without images to obtain cross-meida posts. 
%About 60 
%million tweets with images remained.
%are acquired through this process. 
%Next, we filtered out non-English tweets with 
%Similar to \cite{DBLP:conf/emnlp/NguyenVN20}, we use the 
%fastText library \cite{DBLP:conf/eacl/fastText}.
%to classify the language of the tweets and remove non-English tweets. 
%Afterwards, 
images and comments were gathered using 
%Next, we collect the images and comments related to the tweet using 
Twitter stream API.\footnote{\url{https://developer.twitter.com/en/docs/twitter-api}}
% based on the tweet link. 
Next, 
%The 
user mentions and url links 
%in tweet texts and the comments 
are converted to generic tags @USER and HTTPURL for privacy concern. 
Finally, tweets without comments were further removed, resulting in
%keep the cross-media posts which have at least one comment, and get the retrieval corpus of about 
27.8 million tweets with images, text, and comments.
% We construct a large-scale multimodal social media dataset where each post contains at least one comment and obtain comments for the query post from similar posts. 


The over-year statistics of wild
%retrieval 
dataset is shown in Table \ref{tab:retrieval_dataset_analysis}.
There exhibits an interesting observation --- 
%from Table \ref{tab:retrieval_dataset_analysis} 
%is that 
the average text length in recent years (2017-2019) is 
%recent years (i.e., 2017, 2018, and 2019) is 
obviously shorter than earlier.
%that of previous years (i.e., 2014, 2015, and 2016). 
It might be because images these years allow the provision of gradually richer information, partially taking the text's roles in multimedia communications.
%users gradually prefer to use images exchange inforamtion  
%are gaining habits to exchange information with images while multimedia communications are becoming more and more popular. 
%This might because users tend to utilize images to replace part of texts to express content.

\subsection{Post and Comment Retrieval}\label{ssec:method:retrieval_module}

Given a post in image-text pair, we then discuss how to retrieve similar posts and their comments.
%from the wild dataset.

\paragraph{Retrieval of Similar Posts}
Post similarity is measured via balancing the effects of the image and text features.
The former is learned with
%we employ
%each tweets in the wild dataset, 
%we adopt 
ResNet-152 \cite{DBLP:conf/cvpr/Resnet} pre-trained on the ImageNet \cite{DBLP:journals/ijcv/ImageNet} and we take
%to encode images in the wild dataset and 
%dataset as the image encoder and 
the output of final pooling layer for representation.
%to represent image features.
%of ResNet-152 
%is utlized to represent the image features. 
For text features, SimCSE is adopted because of its effectiveness in similarity measure \cite{DBLP:conf/emnlp/SimCSE}. 
%, which significantly improve the sentence embeddings on the text similarity tasks, is used to extract the text features of text representations. 
%Given a query image-text pair in the dataset of target task, ResNet-152 and SimCSE are also employed to encode the image and text, separately. 
% Suppose that $s_i^I$ reflects the similarity measure 
% Then the 
For any post (the query), we score its similarity to the $i$-th multimedia post in the wild dataset with $s_i$: 

%semantic similarity score $s_i$ between the query post and the $i$-th post in retrieval dataset is calculated as:

\begin{equation}\small\label{eq:post-retrieval}
s_{i} = \alpha s^{I}_{i} + (1-\alpha) s^{T}_{i}
\end{equation}

\noindent where $s^{I}_{i}$ and $s^{T}_{i}$ respectively indicates the image and text similarity, traded-off by the parameter $\alpha$.

%indicates the similarity between the query image and the $i$-th retrieval image, while $s^{T}_{i}$ represented the similarity based on text modality. $\alpha$ is the parameter to balance the effects of text image and text. 

Here the weight $\alpha$ (balancing image and text effects) is empirically estimated with the averaged statistics measured over the wild datasets (for retrieval) and all experimental data (as a query set).
%To make $\alpha$ automatically determined by the internal characteristics of image-text pairs  and adaptive for different multimodal tasks, $\alpha$ is computed by:

\begin{equation}\small
\alpha = \frac{T_{mean}}{I_{mean}+T_{mean}}
\end{equation}
\begin{equation}\small
I_{mean} = \frac{1}{MK}\sum_{m=1}^{M}\sum_{k=1}^{K} p_{m,k}
\end{equation}
\begin{equation}\small
T_{mean} = \frac{1}{MK}\sum_{m=1}^{M}\sum_{k=1}^{K} q_{m,k}
\end{equation}

\noindent where $p_{m,k}$ and $q_{m,k}$ respectively refer to the image and text similarity between the $m$-th query and its rank-$k$ most similar post retrieved with the corresponding modality. 
$M$ is the query set size and $K$ the cut-off number of retrieved posts to be selected. 
%is the image similarity score between the $m$-th query image and the most similar $n$-th retrieval image, while $q_{m,n}$ is used to measure the text similarity. $N$ indicates the selected num of the most similar post, $M$ represents the size of the query dataset. Thus, $I_{mean}$ and $T_{mean}$ represent the overall average similarity of the image and text, separately.

% Furthermore, we use the Faiss library \cite{johnson2019billion} for fast similarity search. Clearly, the Inverted File Index Product Quantization(IVFPQ) \cite{johnson2019billion} index is built for feature vector compression and efficient kNN queries. IVFPQ index is trained with the images features to perform clustering on the original feature vectors. Given a query image features, IVFPQ would return $L$-nearest\footnote{$L$ is set to 0.1M in the experiments.} image indices and related image similarity score. Similarly, $L$-nearest text indices and related text similarity score could be obtained. For efficient search, the post related to the overlap index between text indices and image indices would be regarded as candidate similar posts, and participated in searching the final similar posts by Eq.\ref{eq:post-retrieval}.

In this way, for any query, we rank posts in wild dataset by the 
%the multimodal posts of the overlap index by the semantic similarity score 
$s_i$ score (leveraging image and text similarity) and the top-$K$ most similar posts will be retrieved.
%acquire top-$K$ similar posts for each query post in the target dataset. 
%Specially, 

% Here we give the details of using the Faiss library \cite{johnson2019billion} for fast similarity search. 
Here the Faiss library \cite{johnson2019billion} is employed for fast similarity search.
Concretely, the Inverted File Index Product Quantization(IVFPQ) \cite{johnson2019billion} index is built for feature vector compression and efficient kNN queries. First, IVFPQ index is trained with the images features to perform clustering on the original feature vectors. Then, given a query image features, IVFPQ would return $R$-nearest\footnote{$R$ is set to 0.1M in the experiments.} image indexes and related image similarity score. Similarly, $R$-nearest text indexes and related text similarity score could be obtained. At last, to allow an efficient search, the post related to the overlap indexes between text indexes and image indexes would be regarded as candidate similar posts, and used for searching the final similar posts by Eq.\ref{eq:post-retrieval}.

% Here the Faiss library \cite{johnson2019billion} is employed for fast similarity search, whose implementation details are put in Appendix \ref{appendix:similarity_search}.

\paragraph{Retrieval of Comments}\label{ssec:method:comment_retrieval}
As discussed above, comments (written by human readers)  potentially help in hinting the visual-lingual relations.
%Comments written by other users might be helpful to the understanding of the cross-media posts because a bystander's point of view is provided to bridge the semantic gap between the images and text. 
However, while building the wild dataset, we observe only 46\% multi-modal tweets contain comments. 
For those posts where comments are absent or inaccessible, the comments of the retrieved posts may be useful as well, because intuitively, similar posts may result in similar comments.

%on the one hand, not all posts contain comments. Actually, there are at least one comment in about 46\% cross-media posts in our observation. On the other hand, the comments are not collected and missing in most multimodal social media datasets. 
%Thus, we retrieve comments from similar posts based on the assumption that semantically similar cross-media posts might obtain similar comments.

However, because of the noisy nature of social media data, comments may vary in their quality and effects in hinting  cross-modal learning.
We therefore need to shortlist high-quality comments. 


%The comments of top-$K$ similar posts are collected together and construct the comment pool $P$, where the comments are noisy since the comments are written from different human readers. 

Concretely, from the comment pool $P$ gathering all retrieved posts' comments, we follow   \citet{DBLP:journals/corr/consensus} to extract representative comments as consensus comments.
%consensus comments are defined to filter out noisy comments and select the representative comments which have 
They tend to exhibit relatively higher semantic similarity to other comments in $P$ and can be ranked with:
%And the consensus score is measured as follows:

\begin{equation}\small
q_{i}=\frac{1}{|P|}\sum_{p' \in P } Sim\left ( p_{i},p' \right )
\label{Eq:consensus}
\end{equation}

\noindent where $q_i$ is the consensus score of the $i$-th comment $p_i\in P$. 
%in the pool $P$. 
$Sim\left ( p_{i},p' \right )$ indicates the $p_i$-$p'$ similarity (with SimCSE).
%represents the similarity score between 
%the comment 
%$p_i$ and $p'$. 
$|P|$ is $P$'s comment size.
%is the num of 
%comments 
%in the pool . 

In practice, for any post, we query and retrieve
the top-$N$ consensus comments to
%with the highest consensus scores are obtained as 
form a set $C$ later used to bridge the gap between image modality and text modality (next discussed in $\S$\ref{ssec:method:comment_usage}).
%to engage in self-training.
%to bridge the semantic gap between images and texts. 
%The comments, encoded by BERT, are then injected into the attention mechanism fusing visual and lingual features to help explore cross-modal interactions.

%\jing{Still not very clear.}

%through the attention mechanism to interact with the text features and image features.

\subsection{Leveraging Comments in a Multimodal Classification Architecture}
\label{ssec:method:comment_usage}
The previous discussions concern how to retrieve the similar posts and their (consensus) comments. In the following, we describe the implementation details of how  comments can be leveraged in different multimodal classification architectures.  
Based on the different schemes for fusing image features and text features \cite{DBLP:conf/emnlp/AlbertiLCR19}, most multimodal classifiers could be divided into early fusion and late fusion. In the early fusion sheme, image features are embedded with text tokens on the same level (e.g., MMBT).  In the late fusion sheme, image features and text features are encoded separately and interacted by the concatenation (e.g., BERT-CNN) or the attention mechanism (e.g., CoMN-BERT and MMSD-BERT). 

Here we give the details of using comments to bridge the gap between images and texts without changing the original architecture of base classifiers. The comments injection methods are slightly different for early fusion and late fusion schemes, where the details come in the following.

\paragraph{Early Fusion Scheme}
The image features are projected into token space and concated with the word token embeddings as the input of multimodal bitransformer in MMBT. Then the hidden state $h^f$ related to the [CLS] token is used as the representation of the fusion vector for the classification in MMBT. We adopt the same encoding strategy to fuse each comment and the image features, and related hidden states $\{h^c_1,...,h^c_{N} \}$ are obtained. After that, the attention mechanism is used to compute the attended vector $u$, which is concated with the original fusion vector $h^f$ for the classification:
\begin{equation} 
u = \sum_{n=1}^{N}\beta_{n}h^c_{n}
\label{Eq:attention}
\end{equation}
\begin{equation} 
\beta_{i} = \frac{{\rm{exp}} (z_n)}{ \sum_{n=1}^{N}{\rm{exp}} (z_n)}; \quad z_n=\sigma(h^f, h^c_n)
\end{equation}
where $\sigma$ is a feed-forward neural network.

\paragraph{Late Fusion Scheme}
Assume the image features extracted by ResNet, text features extracted by BERT, and the original fusion vector, which is the output of the base classifier before the softmax layer, are $h^v$, $h^t$, and $h^f$, separately. And the comments features encoded by the BERT are denoted as $\{h^c_1,...,h^c_N \}$. Similar to Eq.\ref{Eq:attention}, then attention mechanism is employed to fuse the image features and comment features and obtain the image attended vector $v$. Similarly, the text attended vector $t$ which is fused by text features and comment features, could be acquired. At last, we concate the image attended vector $v$, text attended vector $t$, and original fusion vector $h^f$ for the final classification.



\subsection{ Self-training with Retrieved Posts}\label{ssec:method:self-training}


Here we further describe how the retrieved posts (i.e., the retrieved image-text pairs) is explored in multimodal classification.
Its data is commonly formulated as
%Formally, for conventional multimodal social media tasks like image-text relation classification, 
a labeled parallel dataset $L=\{x_i, c_i, y_i\}_{i=1}^{l}$, where $x_i$ is an image-text pair, $c_i$ indicates the retrieved $N$ comments, and $y_i$ a label specified by the task.
%is provided. 

The labeled dataset $L$ is usually limited in scales \cite{DBLP:conf/mm/MaYL0C19}, posing the over-fitting concern.
Meanwhile the retrieved posts, similar to the data in $L$, could form an unlabeled set ($U=\{x^{'}_i, c_i\}_{i=1}^{Kl}$) to enrich  training data. Note that $x^{'}_i$, one retrieved image-text pair of $x_i$, shares the same consensus comments $c_i$ with $x_i$.
Then $L$ and $U$ may be integrated to allow more robust learning in a semi-supervised manner. 


%Compared with the  unlabeled retrieval dataset constructed in \ref{ssec:method:dataset}, the size of $L$ is usually small in most tasks. Thus, we utilize the top-$K$ similar posts obtained in \ref{ssec:method:retrieval_module} for each query post in training set to expand the scale of the training set. 
%The unlabeled dataset $U$ would be $K$ times the size of labeled dataset $L$. 
%The labeled data and the related similar retrieved unlabeled data share the same consensus comments. 
% Similar to labeled data, we also retrieve comments for each post in the unlabeled data.

Here we adopt self-training based on the popular teacher-student framework \cite{DBLP:conf/cvpr/XieLHL20}.  
A teacher model (the classifier) is first trained on the labeled data $L$ to gain task-specific knowledge and pseudo-label the unlabeled data with soft labels as ``teaching samples''.
Then a student model, sharing the same architecture as the teacher, is trained with both the pseudo-labeled $U$ and labeled $L$.

In the training of both teacher and student, their modality fusion mechanism is fed with the comment features (described in $\S$\ref{ssec:method:self-training}).
% , embedded via a BERT encoder.\footnote{Due to space limitation, we refer readers to the Appendix \ref{appendix:comments} for more implementation details of how comments are injected into varying architectures of multimodal classifiers.} 
It enables the models to explore cross-modal interactions in aware of the comments.

%which later educate the student model via pseudo-labeling the unlabeled data $U$.



%The self-training framework is adopted to utilize the labeled data and unlabeled data to improve the performance of the multimodal classification. 
%Clearly, a teacher model is trained with the labeled data integrated with related comments. 
%Then the trained teacher model is used to generate the soft labels, which are combined with the parallel data $L$ to train a student model. 

In practice, the student training randomly drops 50\% comments while teacher employs the full comment set.
It enables the student model to learn from the noised data for a better generalization instead of simply mimicking teacher's behavior.
%Additionally, the comments of labeled data and unlabeled data are dropped at the ratio of 0.5 when used for training the student model while the comments of labeled data are complete for training the teacher model. 
%Thus, the student model is enforced to learn from the noised input data and classify the accurate labels as much as possible

The teacher model is trained with the cross-entropy loss for classification while KL divergence loss is additionally used for student training:

%The student model is optimized with the cross-entropy loss and the KL divergence loss:
\vspace{-1em}
\begin{equation} \small
\mathcal{L} = \frac{1}{|L|} \sum_{i\in L} y_{i}\mathrm{log}y_{i} +\frac{1}{|U|} \sum_{i\in U} \mathrm{KL}(t_i||s_i)
\end{equation}

\noindent where $|L|$ and $|U|$ indicate $L$'s and $U$'s dataset size. 
%the size of labeled dataset and unlabeled dataset, separately. 
$t_i$ is the soft label predicted by the teacher model while $s_i$ is the output of the student model.

