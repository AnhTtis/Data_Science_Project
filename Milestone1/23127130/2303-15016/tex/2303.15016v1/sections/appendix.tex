\appendix
\section{Appendix}
% \subsection{Leveraging Comments in a Multimodal Classification Architecture
% %The details of comments used in the full models
% }\label{appendix:comments}
% In the following, we describe the implementation details of how  comments can be leveraged in different multimodal classification architectures.  
% Based on the different schemes for fusing image features and text features \cite{DBLP:conf/emnlp/AlbertiLCR19}, most multimodal classifiers could be divided into early fusion and late fusion. In the early fusion sheme, image features are embedded with text tokens on the same level (e.g., MMBT).  In the late fusion sheme, image features and text features are encoded separately and interacted by the concatenation (e.g., BERT-CNN) or the attention mechanism (e.g., CoMN-BERT and MMSD-BERT). 

% Here we give the details of using comments to bridge the gap between images and texts without changing the original architecture of base classifiers. The comments injection methods are slightly different for early fusion and late fusion schemes, where the details come in the following.

% \paragraph{Early Fusion Scheme}
% The image features are projected into token space and concated with the word token embeddings as the input of multimodal bitransformer in MMBT. Then the hidden state $h^f$ related to the [CLS] token is used as the representation of the fusion vector for the classification in MMBT. We adopt the same encoding strategy to fuse each comment and the image features, and related hidden states $\{h^c_1,...,h^c_{N} \}$ are obtained. After that, the attention mechanism is used to compute the attended vector $u$, which is concated with the original fusion vector $h^f$ for the classification:
% \begin{equation} 
% u = \sum_{n=1}^{N}\beta_{n}h^c_{n}
% \label{Eq:attention}
% \end{equation}
% \begin{equation} 
% \beta_{i} = \frac{{\rm{exp}} (z_n)}{ \sum_{n=1}^{N}{\rm{exp}} (z_n)}; \quad z_n=\sigma(h^f, h^c_n)
% \end{equation}
% where $\sigma$ is a feed-forward neural network.

% \paragraph{Late Fusion Scheme}
% Assume the image features extracted by ResNet, text features extracted by BERT, and the original fusion vector, which is the output of the base classifier before the softmax layer, are $h^v$, $h^t$, and $h^f$, separately. And the comments features encoded by the BERT are denoted as $\{h^c_1,...,h^c_N \}$. Similar to Eq.\ref{Eq:attention}, then attention mechanism is employed to fuse the image features and comment features and obtain the image attended vector $v$. Similarly, the text attended vector $t$ which is fused by text features and comment features, could be acquired. At last, we concate the image attended vector $v$, text attended vector $t$, and original fusion vector $h^f$ for the final classification.



% \paragraph{CoMN-BERT (full) for MVSA task}
% Assume the output of the CoMN-BERT before the softmax layer is the fusion vector $v$, which combines the image features and text features. The retrieved comments $C=\{c_1,..,c_5\}$ are encoded by the BERT, and the output of the last layer in the BERT are passed into a max-pooling layer to get the final comment representations $H=\{h1,...,h_5\}$. Then the attention mechanism is utilized to automatically select the comments to attend to:

% \begin{equation} 
% \hat{u} = \sum_{i=1}^{5}\alpha_{i}h_{i}
% \end{equation}
% \begin{equation} 
% \alpha_{i} = \frac{\rm{exp}(z_i)}{ \sum_{i=1}^{5}\rm{exp}(z_i)}; \quad z_i=\sigma(v, h_i)
% \end{equation}

% Finally, the attended vector are concated with the fusion vector to get the prediction results.

% \paragraph{BERT-CNN (full) for ITR task}
% Note that the image features and text features are concated to get the label in BERT-CNN model. The attention mechanism is used in BERT-CNN (full) model to make comments interacted with image features and text features. Two attended vectors are obtained by using the image features and text features as query vector, separately.

\subsection{Similarity Search Details with Faiss Library
%The details of similarity search
}\label{appendix:similarity_search}

Here we give the details of using the Faiss library \cite{johnson2019billion} for fast similarity search. Concretely, the Inverted File Index Product Quantization(IVFPQ) \cite{johnson2019billion} index is built for feature vector compression and efficient kNN queries. First, IVFPQ index is trained with the images features to perform clustering on the original feature vectors. Then, given a query image features, IVFPQ would return $L$-nearest\footnote{$R$ is set to 0.1M in the experiments.} image indexes and related image similarity score. Similarly, $R$-nearest text indexes and related text similarity score could be obtained. At last, to allow an efficient search, the post related to the overlap indexes between text indexes and image indexes would be regarded as candidate similar posts, and used for searching the final similar posts by Eq.\ref{eq:post-retrieval}.

% \subsection{More Cases with  Visualized Attention}\label{appendix:case}

% \input{figures/case_appendix}