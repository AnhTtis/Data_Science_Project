\section{Experimental Setup}

\input{sections/Tables/four_dataset_statistics}

\subsection{Evaluation Datasets}
%\input{sections/Tables/four_dataset_statistics}

Our evaluation is conducted on four Twitter classification benchmarks on  
%We demonstrate our experiments on four public multimodal social media datasets: 
multimodal sentiment classification (MVSA) \cite{DBLP:conf/mmm/MVSA}, image-text relation (ITR) \cite{DBLP:conf/acl/VempalaP19}, multimodal sarcasm detection (MSD) \cite{DBLP:conf/acl/sarcasm}, and multimodal hate speech detection (MHP) \cite{DBLP:conf/acl/BotelhoHV21}. 
% ITR contains four classes data  which is annotated based on whether text or image provide additional content outside the other modality. MHP focus on the online hate scenario and  the label of the  
%The data in all datasets are 
Each data instance is an image-text pair 
%collected from Twitter 
and it is annotated with
%, and consists of a image-text pair and 
a single class label. 
For MVSA, MHP and MSD, we adopt the same 
%train, validation and test 
dataset split 
%for training, validation, and text 
as their original papers 
%\cite{DBLP:conf/acl/BotelhoHV21} and \cite{DBLP:conf/acl/sarcasm} 
for fair comparisons. 
For ITR, 
%lacking such setup details, 
we randomly
split 80\%, 10\% and, 10\% for training, validation, and test instead of their 10-fold cross-validation setup for the concern of experimental efficiency.

The statistics of evaluation datasets are shown in Table \ref{tab:four_dataset_analysis}, where we observe the small-scale training data in MVSA, ITR, and MHP.
For MVSA, though relatively larger in scales, its automatic labeling under hashtag-based distant supervision, may result in noisy labels, which further require larger data scales for robustness. 
These imply the annotation difficulties and potential benefits from self-training.


\subsection{Implementation and Evaluation Details}

All experimental models 
%for the experiments 
are implements with PyTorch\footnote{\url{https://pytorch.org/}} and HuggingFace Transformers\footnote{\url{https://github.com/huggingface/transformers}}. 
%The maximum length is 50 for both 

Both text and comment are capped at 50 words for encoding. 
The batch size is set to 8, 8, 16, and 16 for ITR, MHP, MVSA, MSD. 
The learning rate is set to 1e-5 with a warm-up rate to 0.1. 
Classifiers are trained with an AdamW optimizer. 
The maximum of consensus comments ($N$) is set to 5.
We run the self-training for three iterations. At each iteration,
the teacher model is fine-tuned for 10 epochs on the labeled training data. 
The teacher model performing the best in validation is adopted to predict the pseudo-labels for the unlabeled retrieved data. 
The student model is then 
fine-tuned for 10 epochs. After that, the student model is used as teacher for the next iteration.
%with the labeled training dataset and pseudo-labeled data. 

For evaluation metrics, we follow the benchmark practice to use precision (pre), recall (rec), and F1-score (F1) for
%to evaluate the performance of models for 
ITR, MVSD, and MHP, 
%tasks 
and accuracy (acc) and F1 
%is used 
for MSVA task.






\subsection{Baselines and Comparisons}

To investigate our universal benefits over different classification tasks varying in SOTA methods, we integrate our comment-aware self-training module into the BERT-based SOTA architectures and examine the results following baselines and comparisons employed in the original paper for fair comparison.
%For fair comparisons with previous work, different baselines are adopted for the four tasks due to the characteristics of different tasks 
%\footnote{I.g., the attributes of images are usually adopted for MSD while the texts in the images are extracted by using OCR for MHP.}.

%\paragraph{Baselines for MVSA.}





\paragraph{MVSA Comparisons.} This benchmark presents baselines of
MultiSentiNet \cite{DBLP:conf/cikm/MultiSentiNet} (a deep semantic network with the visual clues guided attention),
%mechanism 
%for multimodal sentiment classification
CoMN \cite{DBLP:conf/sigir/Co-MN} 
%is 
(a co-memory network to learn image-text interactions),
%the interactions between image features and text features
%for multimodal sentiment analysis. 
MMMU-BA \cite{DBLP:conf/emnlp/MMMU-BA} 
%proposes 
(enriching context for cross-modal fusion), and
%a fusion method by utilizing the context from neighboring utterances to generate richer multi-modal representation)
Self-MM \cite{DBLP:conf/aaai/Self-MM} (joint training of uni-modal and multi-modal tasks to explore cross-modal consistency).
%jointly learns the uni-modal tasks and multimodal sentiment analysis to capture the the consistency and difference among different modalities.) 
CoMN-BERT is a SOTA architecture combining CoMN and the pre-trained BERT \cite{DBLP:conf/naacl/BERT}, which will later be combined with our module for comparison.

%which employs the CoMN architecture and uses BERT \cite{DBLP:conf/naacl/BERT} 
%to encode text is used as our base model for MVSA.

%\paragraph{Baselines for ITR} 
\paragraph{ITR Comparisons.}
In the original paper \cite{DBLP:conf/acl/VempalaP19}, LSTM-CNN performs the best via combining CNN-encoded  visual features \cite{DBLP:conf/cvpr/InceptionNet} and LSTM-encoded lingual features \cite{LSTM}.
It is compared with the baseline ablations CNN and LSTM using uni-modal features only.
To line up with the SOTA, we implement BERT-CNN to employ pre-trained BERT for text encoding instead of LSTM, which is likewise compared to a BERT classifier using lingual features only.
Based on BERT-CNN, we integrate in our module to examine its effectiveness over SOTA. 
%Following \citet{DBLP:conf/acl/VempalaP19}, LSTM \cite{LSTM} and CNN \cite{DBLP:conf/cvpr/InceptionNet} are adopted to encode text and image, respectively.
%are adopted as the text-based method and image-based method for image-text relation classification, separately. 
%LSTM-CNN \cite{DBLP:conf/acl/VempalaP19} combines the image features and text features to jointly learn the interaction of image-text pairs. 
%BERT, a pretrained model for text, is also taking as the text-modality approach. BERT-CNN is utilized as our base model for ITR.

%\paragraph{Baselines for MSD}
\paragraph{MSD Comparisons.} Here the MMSD baseline is introduced in the original paper \cite{DBLP:conf/acl/sarcasm}, which employs a hierarchical fusion model to explore visual and lingual features with optical characters.
%\footnote{
%Here visual features include 
%Image attributes refer to optical characters in images.
%are usually adopted for MSD while 
%the texts in the images are extracted by using OCR for MHP.
%}
%introduces a hierarchical fusion model to learn the joint representation of text features, image features and image attributes. 
We also compare with the following more advanced models on the benchmark: D\&R Net \cite{DBLP:conf/acl/D_R} (using decomposition and relation network to learn visual-lingual interactions),
%to capture the semantic interactions between image modality and text modality.
Res-BERT \cite{DBLP:conf/emnlp/Pan_sarcasm} (concatenating visual features from ResNet (cite) and lingual features from BERT),
%(concates the image features and text features for sarcasm classification) 
Att-BERT \cite{DBLP:conf/emnlp/Pan_sarcasm} (with attention mechanism to capture image-text semantic consistency), and
%attention mechanism to capture the inconsistency between images and texts),
CMGCN \cite{DBLP:conf/acl/CMGCN} (building a  graph to explore cross-modal interactions).
%constructs a cross-modal graph to utilize the inconsistent implications between different modalities. 
MMSD-BERT is based on MMSD with a pre-trained BERT to encode texts, where we will later architect with our proposed module.

%which uses the NMSD architecture and uses BERT to encode text is used as our base model for MSD.

\input{sections/Tables/MVSA}
\input{sections/Tables/ITR}
\input{sections/Tables/MSD}
\input{sections/Tables/MHP}



%\paragraph{Baselines for MHP}
\paragraph{MHP Comparisons.}
Following the setup in  \citet{DBLP:conf/acl/BotelhoHV21}, we consider the Xception \cite{DBLP:conf/cvpr/Xception} baseline using visual features only.
%, which only encodes image content for classification, 
For the text-only comparison, 
%is used as the unimodal-image model while 
LSTM and RoBERTa \cite{DBLP:journals/corr/RoBERTa} are adopted.
%are utilized as the context-based unimodal-text models. 
MMBT \cite{DBLP:conf/acl/BotelhoHV21} is the SOTA model learning cross-modal representations with pre-trained MultiModal BiTransformers and will be employed as the base to experiment with our module.

%our base model, integrate the image features with text tokens to the 
%MultiModal BiTransformers, initialized with pre-trained
%BERT weights, for classification.


\footnotetext{
The baseline results are copied from the original paper, where the numbers are rounded to one decimal place.
%The results of baselines only have three significant figures, and the results of the full model also employ the same format for consistency.
}

\paragraph{Integrating our Comment-aware Self-training.}
Based on aforementioned SOTA architectures ( \textbf{base classifiers} --- CoMN-BERT, BERT-CNN, MMSD-BERT, and MMBT, selected for the four benchmarks), we further employ comment-aware self-training in their training and
%integrate our comment-aware self-training module with the 
%base classifiers,
%, which yield 
%The newly combined models are 
therefore result in CoMN-BERT (full), BERT-CNN (full), MMSD-BERT (full), and MMBT (full). 
%The model CoMN-BERT (full), BERT-CNN (full), NMSD-BERT (full), and MMBT (full) represent the corresponding base model adding the comment-aware self-training for MVSA, ITR, MSD, and MHP, separately. 

%To demonstrate the effects of different modules in the proposed retrieval-based comment-aware self-attention method, 
To further examine the relative contributions of each sub-module in comment-aware self-training, the following ablations are considered in comparison:
%we conduct experiments on four variants based on the base model:
(1) Base+Com,
integrating  BERT-encoded comment features in the base classifiers.
%utilize the retrieved comments to add context with the base model (Base+Com); 
(2) Base+ST,  self-training with retrieved tweets yet without comments. 
%with retrieved similar image-text pairs based on the base models
(3) Base+Com+ST, the full model without randomly dropping the retrieved comments in student model training.
%The full model
%use retrieved comments and self-training framework (Base+Com+ST);
%(4) randomly drop the retrieved comments for the student model in the self-training framework (Base+Com+ST+Drop).


