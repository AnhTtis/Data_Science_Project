\section{Related Work}

Our work is in line with multimodal learning and self-training, which are discussed below in turn.

%\subsection
\paragraph{Multimodal Learning.}
Previous work in this field focuses on
%Multimodal learning, 
%aiming at process and 
fusing features 
%extracted 
from different modalities (e.g., vision and language) \cite{DBLP:journals/pami/BaltrusaitisAM19} to tackle cross-modal 
%It has drawn growing 
%great 
%attention and has been widely used in conventional multimodal 
classification tasks, such as visual question answering (VQA) \cite{DBLP:conf/cvpr/TapaswiZSTUF16, DBLP:conf/cvpr/GoyalKSBP17, DBLP:conf/cvpr/JohnsonHMFZG17}, visual commonsense reasoning \cite{DBLP:conf/cvpr/VCR, DBLP:conf/nips/ViLBERT}, and image-text  retrieval \cite{DBLP:conf/eccv/LeeCHHH18, DBLP:conf/aaai/Unicoder-VL, DBLP:conf/eccv/Oscar}. 
%The image-text pairs in the dataset of these tasks 
Most benchmark data in vision and language assumes strong image-text correlations, and many multimodal models are hence designed to explore the common semantics shared by the two modalities.
%strongly correlated 
%and the multimodal models 
%are required to learn the semantic alignment contained in the cross-media data. 
However, it has been recently pointed out that many real-world scenarios, including social media, tend to present image-text pairs with weak and intricate cross-modal interactions  \cite{DBLP:conf/acl/VempalaP19,DBLP:conf/emnlp/HesselLM19,nature2022_weak_semantic}.
%as indicated by \cite{DBLP:journals/corr/wenlan}, the strong correlation assumption is invalid in real-world scenarios (i.g., social media) and the implicit interaction usually appears in the image-text pairs. 
%Additionally, \cite{DBLP:conf/acl/VempalaP19} 
%analyzes and defines the image-text relationships of social media posts into several types, which also illustrates existence of the weak correlation.

Despite the substantial efforts made in applying multimodal learning in social media to tackle various visual-lingual tasks, e.g., 
%In recent years, multimodal learning has also applied in social multimodal social media tasks, for instance, 
%multimodal 
sarcasm detection \cite{DBLP:conf/acl/sarcasm}, %multimodal 
hate speech detection \cite{DBLP:conf/acl/BotelhoHV21}, 
%mutimodal 
metaphor detection \cite{DBLP:conf/acl/ZhangZZ0L20}, etc., most existing methods follow the common practice to fuse visual and lingual features.
It is hence challenging for them to figure out the cross-modal meanings exhibit with implicit image-text links.
We thus resort to the retrieved user comments and study how models can find the hinting features therein to mitigate the cross-modal gaps.
%However, these work ignores the semantic gap in the cross-media posts, and it is challenging for the multimodal models to automatically learn the implicit correlation. 
%In the paper, we retrieve comments from similar posts to explicitly bridge the images and texts.


Our work is also related to \citet{DBLP:conf/emnlp/GurNSLKR21}, where the retrieved similar data show helpful in better aligning visual-lingual features for multi-modal classification. 
%where a cross-modal alignment module is trained for retrieving similar multimodal samples, 
%and the original input is augmented by concatenating the retrieved samples. 
Different from them, we additional retrieve comments and learn the hints therein via  self-training, which allows easy integration to most multi-modal classification archetectures.
%that, the  retrieval module designed in our work could adaptively retrieve similar posts and related comments by considering the weak semantics in the image-text pairs. 
%And the retrieved posts are involved in the self-training to improve the performance.



%\subsection
\paragraph{Self-training.}
Our method is inspired by previous work in
self-training
%Self-training
\cite{DBLP:journals/tit/Scudder65a, DBLP:conf/acl/Yarowsky95}, where labeled data is employed to train models and generate pseudo-labels for unlabeled data.
It is simple yet effective to enable model robustness with limited labeled data, potentially helpful in multimodal social media tasks owing to the expensive annotation and small-scale labeled data in most benchmarks \cite{DBLP:conf/acl/VempalaP19, DBLP:conf/acl/BotelhoHV21}.

Here we adopt the trendy self-training paradigm in a teacher-student framework, where a teacher model is trained with labeled data and self-label unlabeled data to generate synthetic data for the student model training.
It has been widely used in many tasks in CV (e.g., image classification \cite{DBLP:conf/cvpr/XieLHL20, DBLP:conf/nips/ZophGLCLC020}, object detection \cite{DBLP:conf/cvpr/YangWW0021}) and NLP (e.g, question answering \cite{DBLP:conf/naacl/SachanX18, DBLP:conf/emnlp/ZhangB19, DBLP:conf/emnlp/RennieMMNG20}, text classification \cite{DBLP:conf/nips/MukherjeeA20, DBLP:conf/emnlp/MengZHXJZH20, DBLP:conf/naacl/ShenQMSRH21}).
However, in existing work, limited attention has been drawn to its effectiveness in social media multimodal classification and how it works with retrieved comments to gain the cross-modal understanding for noisy data.
%which is a simple and old semi-supervised method, 
%typically employs the teacher-student framework, 
%where a teacher model trained with annotated data is utilized to generate synthetic data based on the large amount of unlabeled data, and then a student model is trained with the labeled data and the synthetic data.
%While self-training method  has been widely used in CV (i.g., image classification \cite{DBLP:conf/cvpr/XieLHL20, DBLP:conf/nips/ZophGLCLC020} and object detection \cite{DBLP:conf/cvpr/YangWW0021}) and NLP (i.g, question answering \cite{DBLP:conf/naacl/SachanX18, DBLP:conf/emnlp/ZhangB19, DBLP:conf/emnlp/RennieMMNG20} and text classification \cite{DBLP:conf/nips/MukherjeeA20, DBLP:conf/emnlp/MengZHXJZH20, DBLP:conf/naacl/ShenQMSRH21}), few work utilize the self-training method to solve multimodal classification tasks. Different from previous work where the highest confidence data predicted by the teacher model on the whole unlabeled dataset are used as the synthetic data, we directly retrieve similar multimodal data based on the similarity to each image-text pair in the training set as the source of synthetic data before training the student model to improve the efficiency.