\pdfoutput=1
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{EMNLP2022}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

\usepackage[T1]{fontenc}
\usepackage{booktabs}

\usepackage[utf8]{inputenc}
\urlstyle{same}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{multirow}
\newcommand{\jing}[1]{[\textcolor{red}{JL: #1}]}
\newcommand{\xu}[1]{[\textcolor{blue}{xu: #1}]}
\usepackage{tablefootnote}
\usepackage{inconsolata}
\title{
Borrowing Human Senses: Comment-Aware Self-Training for \\Social Media Multimodal Classification
%Self-Training with Retrieved Comments for Social Media Multimodal Classification
%\jing{Learning from Readers: Social Media Multimodal Classification via Comment Retrieval and Self-Training}
}


\author{Chunpu Xu, Jing Li\thanks{~~~Corresponding author}
\\ % All authors must be in the same font size and format. Use \Large and \textbf to achieve this result when breaking a line
Department of Computing, The Hong Kong Polytechnic University, China\\
\texttt{chun-pu.xu@connect.polyu.hk}\\ \texttt{jing-amelia.li@polyu.edu.hk}}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
%Millions of cross-media posts consisting of image-text pairs are created on social platform every day to convey users' ideas and feelings.
%\jing{Social media is daily exhibiting huge volume of cross-media posts in the form of  image-text pairs.}
% Thus, amounts of multimodal classification tasks are derived to analyse social media for different applications.
% \jing{Social media is daily exhibiting massive multimedia content with pair-wised image and text.
% It hence presents the pressing need to automate the vision and language understanding with various multimodal classification tasks.}
Social media is daily creating massive multimedia content with 
paired image and text, presenting the pressing need to automate the vision and language understanding for various multimodal classification tasks.
%The advance of multimodal classification would crucially automate the vision and language understanding 
%on social media exhibiting massive multimedia content every day.
% However, different from traditional  multimodal classification tasks (i.g., visual entailment and visual question answering) where strong semantic interrelation are in the image-text pair, weak semantic correlation are contained in the cross-media posts. 
% Most existing methods focus on exploring the cross-modal interactions, despite of their implicit and intricate nature on social media.
% As a result, it's hard for the multimodal model to learn the interactions between images and texts since the content of images and texts are not aligned.  
% To tackle the problem, we use the comments obtained from similar posts as the bridge to connect the image modality and text modality.
Compared to the commonly researched visual-lingual data, social media posts tend to exhibit more implicit image-text relations.
To better glue the cross-modal semantics therein, 
% \jing{which tend to be implicit on social media}, 
%which tend to be implicit on social media, 
we capture hinting features from user comments, which are retrieved via jointly leveraging visual and lingual similarity.
% Additionally, the size of most datasets of multimodal social media tasks are small due to the expensive annotation cost. To alleviate the problem, we present a self-training method, which employs pseudo-labeled data constructed from the  retrieved similar image-text pairs to improve learning. 
Afterwards, the classification tasks are explored via self-training in a teacher-student framework,
%based on pseudo-labeling, 
motivated by the usually limited labeled data scales in existing benchmarks.  
%of labeled multimodal data for social media tasks. 
%multimodal classification. 
%labeled data in most multimodal social media datasets.
%We evaluate our approach 
Substantial experiments are conducted on four multimodal social media benchmarks for image-text relation classification, 
%multimodal 
sarcasm detection, %multimodal 
sentiment classification, and 
%multimodal 
hate speech detection.
%In the experiments, 
The results show that our method further advances the performance of
%enables better results of 
previous state-of-the-art models, which do not employ comment modeling or self-training.
\end{abstract}


\input{sections/introduction}

\input{sections/related_work}

\input{sections/method}

\input{sections/experiments_setup}

\input{sections/experiments_results}



\input{sections/conclusion}
\input{sections/Acknowledgements}

\input{sections/limitation}
\input{sections/ethics}
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology}
\bibliographystyle{acl_natbib}
\clearpage
% \input{sections/appendix}



\end{document}