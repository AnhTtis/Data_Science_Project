
@inproceedings{simonyan_two-stream_2014,
	title = {Two-Stream Convolutional Networks for Action Recognition in Videos},
	volume = {27},
	NOT_USED-url = {https://proceedings.neurips.cc/paper/2014/hash/00ec53c4682d36f5c4359f4ae7bd7ba1-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	NOT_USED-publisher = {Curran Associates, Inc.},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2022-03-10},
	year = {2014},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
}


@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
NOT_USED-url={https://openreview.net/forum?id=YicbFdNTTy}
}

@misc{VisionTrans-NOT_USED,
  doi = {10.48550/ARXIV.2010.11929},
  
  url = {https://arxiv.org/abs/2010.11929},
  
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{GoogleVid-NOT_USED,
  doi = {10.48550/ARXIV.2103.15691},
  
  url = {https://arxiv.org/abs/2103.15691},
  
  author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ViViT: A Video Vision Transformer},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Arnab_2021_ICCV,
    author    = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu\v{c}i\'c, Mario and Schmid, Cordelia},
    title     = {ViViT: A Video Vision Transformer},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {6836-6846}
}


@misc{facebookVid-NOT_USED,
  doi = {10.48550/ARXIV.2102.05095},
  
  url = {https://arxiv.org/abs/2102.05095},
  
  author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Is Space-Time Attention All You Need for Video Understanding?},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{pmlr-v139-bertasius21a,
  title = 	 {Is Space-Time Attention All You Need for Video Understanding?},
  author =       {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {813--824},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/bertasius21a/bertasius21a.pdf},
  NOT_USED-url = 	 {https://proceedings.mlr.press/v139/bertasius21a.html},
  abstract = 	 {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named “TimeSformer,” adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that “divided attention,” where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.}
}




@misc{adam,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{gradcam,
	doi = {10.1007/s11263-019-01228-7},
  
	NOT_USED-url = {https://doi.org/10.1007/s11263-019-01228-7},
  
	year = 2019,
	month = {oct},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {128},
  
	number = {2},
  
	pages = {336--359},
  
	author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
  
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
  
	journal = {International Journal of Computer Vision}
}


@InProceedings{farneback,
author="Farneb{\"a}ck, Gunnar",
editor="Bigun, Josef
and Gustavsson, Tomas",
title="Two-Frame Motion Estimation Based on Polynomial Expansion",
booktitle="Image Analysis",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="363--370",
abstract="This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.",
isbn="978-3-540-45103-7"
}


@article{JALAL2020101088,
title = {Fish detection and species classification in underwater environments using deep learning with temporal information},
journal = {Ecological Informatics},
volume = {57},
pages = {101088},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101088},
NOT_USED-url = {https://www.sciencedirect.com/science/article/pii/S1574954120300388},
author = {Ahsan Jalal and Ahmad Salman and Ajmal Mian and Mark Shortis and Faisal Shafait},
keywords = {Automatic fish detection, Fish species classification, Fish sampling, Biomass estimation, Underwater video imagery, Gaussian mixture models, Optical flow, Deep learning},
abstract = {It is important for marine scientists and conservationists to frequently estimate the relative abundance of fish species in their habitats and monitor changes in their populations. As opposed to laborious manual sampling, various automatic computer-based fish sampling solutions in underwater videos have been presented. However, an optimal solution for automatic fish detection and species classification does not exist. This is mainly because of the challenges present in underwater videos due to environmental variations in luminosity, fish camouflage, dynamic backgrounds, water murkiness, low resolution, shape deformations of swimming fish, and subtle variations between some fish species. To overcome these challenges, we propose a hybrid solution to combine optical flow and Gaussian mixture models with YOLO deep neural network, an unified approach to detect and classify fish in unconstrained underwater videos. YOLO based object detection system are originally employed to capture only the static and clearly visible fish instances. We eliminate this limitation of YOLO to enable it to detect freely moving fish, camouflaged in the background, using temporal information acquired via Gaussian mixture models and optical flow. We evaluated the proposed system on two underwater video datasets i.e., the LifeCLEF 2015 benchmark from the Fish4Knowledge repository and a dataset collected by The University of Western Australia (UWA). We achieve fish detection F-scores of 95.47\% and 91.2\%, while fish species classification accuracies of 91.64\% and 79.8\% on both datasets respectively. To our knowledge, these are the best reported results on these datasets, which show the effectiveness of our proposed approach.}
}


@article{novelmethod,
author = {Siddiqui, Shoaib and Salman, Ahmad and Malik, Imran and Shafait, Faisal and Mian, Ajmal and Shortis, Mark and Harvey, Euan},
year = {2017},
month = {05},
pages = {},
title = {Automatic fish species classification in underwater videos: Exploiting pretrained deep neural network models to compensate for limited labelled data},
volume = {75},
journal = {ICES Journal of Marine Science},
doi = {10.1093/icesjms/fsx109}
}


@article{MALOY2019105087,
title = {A spatio-temporal recurrent network for salmon feeding action recognition from underwater videos in aquaculture},
journal = {Computers and Electronics in Agriculture},
volume = {167},
pages = {105087},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105087},
NOT_USED-url = {https://www.sciencedirect.com/science/article/pii/S0168169919313262},
author = {Håkon Måløy and Agnar Aamodt and Ekrem Misimi},
keywords = {Fish action/behaviour recognition, Fish feeding, Aquaculture, Convolutional neural network, Recurrent neural network, Action recognition, Video analysis, Optical flow},
abstract = {Recent developments have shown that Deep Learning approaches are well suited for Human Action Recognition. On the other hand, the application of deep learning for action or behaviour recognition in other domains such as animal or livestock is comparatively limited. Action recognition in fish is a particularly challenging task due to specific research challenges such as the lack of distinct poses in fish behavior and the capture of spatio-temporal changes. Action recognition of salmon is valuable in relation to managing and optimizing many aquaculture operations today such as feeding, as one of the most costly operations in aquaculture. Inspired by these application domains and research challenges we introduce a deep video classification network for action recognition of salmon from underwater videos. We propose a Dual-Stream Recurrent Network (DSRN) to automatically capture the spatio-temporal behavior of salmon during swimming. The DSRN combines the spatial and motion-temporal information through the use of a spatial network, a 3D-convolutional motion network and a LSTM recurrent classification network. The DSRN shows an accuracy that is suitable for industrial use in prediction of salmon behavior with a prediction accuracy of 80\%, validated on the task of predicting Feeding and NonFeeding behavior in salmon at a real fish farm during production. Our results show that the DSRN architecture has high potential in feeding action recognition for salmon in aquaculture and for applications domains lacking distinct poses and with dynamic spatio-temporal changes.}
}


@article{RAHMAN2014574,
title = {Fast action recognition using negative space features},
journal = {Expert Systems with Applications},
volume = {41},
number = {2},
pages = {574-587},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.07.082},
NOT_USED-url = {https://www.sciencedirect.com/science/article/pii/S0957417413005812},
author = {Shah Atiqur Rahman and Insu Song and M.K.H. Leung and Ickjai Lee and Kyungmi Lee},
keywords = {Action recognition, Negative space action descriptors, Silhouette, Fuzzy membership, Implicit method, Cycle length, Fish actions},
abstract = {Due to the number of potential applications and their inherent complexity, automatic capture and analysis of actions have become an active research area. In this paper, an implicit method for recognizing actions in a video is proposed. Existing implicit methods work on the regions of subjects, but our proposed system works on the surrounding regions, called negative spaces, of the subjects. Extracting features from negative spaces facilitates the system to extract simple, yet effective features for describing actions. These negative-space based features are robust to deformed actions, such as complex boundary variations, partial occlusions, non-rigid deformations and small shadows. Unlike other implicit methods, our method does not require dimensionality reduction, thereby significantly improving the processing time. Further, we propose a new method to detect cycles of different actions automatically. In the proposed system, first, the input image sequence is background segmented and shadows are eliminated from the segmented images. Next, motion based features are computed for the sequence. Then, the negative space based description of each pose is obtained and the action descriptor is formed by combining the pose descriptors. Nearest Neighbor classifier is applied to recognize the action of the input sequence. The proposed system was evaluated on both publically available action datasets and a new fish action dataset for comparison, and showed improvement in both its accuracy and processing time. Moreover, the proposed system showed very good accuracy for corrupted image sequences, particularly in the case of noisy segmentation, and lower frame rate. Further, it has achieved highest accuracy with lowest processing time compared with the state-of-art methods.}
}


@misc{UCF101,
  doi = {10.48550/ARXIV.1212.0402},
  
  url = {https://arxiv.org/abs/1212.0402},
  
  author = {Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{UCF101}: A Dataset of 101 Human Actions Classes From Videos in The Wild},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{resnet-NOT_USED,
  doi = {10.48550/ARXIV.1512.03385},
  
  url = {https://arxiv.org/abs/1512.03385},
  
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Residual Learning for Image Recognition},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{resnet,

  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Deep Residual Learning for Image Recognition}, 

  year={2016},

  volume={},

  number={},

  pages={770-778},

  doi={10.1109/CVPR.2016.90}}



@INPROCEEDINGS{imagenet,  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={ImageNet: A large-scale hierarchical image database},   year={2009},  volume={},  number={},  pages={248-255},  doi={10.1109/CVPR.2009.5206848}}

@techreport{thorhallsson_tr2018_trawling,
  author      = "Thorhallsson, Torfi and Hreinsson, Einar and Karlsson, Hjalti and Gudmundsson, Geir and Jónsdóttir, Halla and Haney, Georg",
  title       = "Trawling with light",
  type        = "Technical Final Report,  [In Icelandic]",
  institution = "RANNIS Technology Development Fund, Project 153487-0613",
  month       = "Dec",
  year        = "2018"
}


