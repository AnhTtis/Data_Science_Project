\section{Background}
\label{sec:background}
\subsection{DL Training}

Distributed DL can take several forms: data-parallelism, model-parallelism, and hybrid-parallelism. Data-parallism places a full model replica on each processor, and splits the training data among processors. Model parallelism splits the model across processors, and propagates each data sample through each device. Hybrid-parallelism splits the model across sets of processors, and splits the training data among complete-model sets of processors. There are tradeoffs for each parallelism scheme: data-parallelism is the simplest and has low communication overhead but is restricted to models that fit in processor memory. Hybrid and model-parallelism can accommodate any model size, but can require complex communication with high overheads. All distributed DL schemes are increasingly deployed on HPC systems \cite{mtf, megatron-turing-nlg}.

\subsection{Distributed DL Frameworks}

Horovod is a distributed DL framework with a focus on distributed data-parallelism to train DNNs \cite{horovod}. As such, Horovod primarily relies on Allreduce and Bcast collectives. Due to Horovod's focus, they provide a simple API, quick installations, and powerful data-parallel optimizations and profiling tools. Horovod supports many major DL frameworks and communication backends, including MPI and NCCL \cite{nccl}. 

PyTorch's distributed module is a built-in communication API within the PyTorch \cite{pytorch} DL framework. PyTorch distributed supports most communication operations, and contains several optimizations for distributed training (e.g. mixed-precision, gradient bucketing, sharded optimizer states). While official PyTorch wheels come packaged with the NCCL backend, other backends require a PyTorch source installation. 

DeepSpeed is a distributed DL framework built atop PyTorch's distributed module. DeepSpeed's focus is on efficient training of large-scale models that don't fit into a single processor's memory. A myriad of parallelism schemes and optimizer sharding techniques are included in DeepSpeed. 

\subsection{Communication Backends}

MPI is a parallel programming standard that enables processes to communicate with each other. CUDA-aware MPI libraries such as SpectrumMPI ~\cite{spectrum-mpi}, OpenMPI ~\cite{openmpi}, and MVAPICH2 ~\cite{MVAPICH2} provide optimized support for heterogeneous systems containing GPUs. GPU communication optimizations such as staging, CUDA Inter-Process Communication (IPC), and GPUDirect RDMA enable MPI libraries to provide superior performance across different combinations of GPU and interconnect \cite{Kawthar:IWOPH19}.

NCCL implements optimized collective communication patterns for NVIDIA GPUs ~\cite{nccl}. The various collective communication primitives found in NCCL are: Allgather, Allreduce, Reduce, ReduceScatter, Alltoall, Point-to-Point, and Broadcast. NCCL is not MPI-compliant, however, and does not provide support for many common MPI operations such as gather, scatter, and variable message-size collectives. Microsoft's Synthesized Collective Communication Library (MSCCL) \cite{sccl} creates custom collective algorithms for a given hardware topology. MSCCL supports both AMD and NVIDIA GPUs, and supports all major collective operations. 

\subsection{Mixture-of-Experts}

Mixture-of-experts (MoE) is an ensemble machine learning technique where a collection of "expert" feed-forward networks (FFNs) are trained on subtasks of the problem. Only a few experts are applied to a given data sample. In recent years, the MoE technique has been applied to transformer DL models in an effort to increase the model size (and therefore accuracy) while lessening the computational burden. MoE models require less computation to train than equivalent standard (i.e. "dense") models because each token only propagates through an expert subset of the full model. Incoming tokens are routed to existing expert FFNs via a gating function, and this routing as well as its subsequent combination of FFN outputs require Alltoall operations. Such Alltoall operations scale with the number of devices, and quickly become a dominant communication overhead at large scales. The distributed DL frameworks DeepSpeed \cite{ds-moe-latest,ds-moe-orig} and Fairseq \cite{fairseq-moe} have recently added support for MoE transformer models.


\subsection{Deep Learning Recommendation Models}

Deep Learning Recommendation Models (DLRMs) are a family of recommendation models that rely upon at least one deep neural network (DNN) \cite{dlrm, dlrm-scale}. Such models are composed of sparse embedding tables and dense multilayer perceptrons (MLPs). Note that a MLP is a special case of an FFN where every layer is fully connected to the next layer in the network. While sparse categorical data must be processed via embedding lookups (and are memory-bound), dense continuous data is fed through the bottom MLPs (and are compute-bound). The MLPs are trained via data-parallelism, and hence depend on Allreduce. The embedding tables are split across processes, and must be shuffled with an Alltoall prior to being fed into the top MLP. Each batch's Alltoall operation is overlapped with the previous top MLP's forward pass from the previous batch, which necessitates non-blocking Alltoall. 