\section{Design}
\label{sec:design}

MCR-DL is split into a C++ implementation layer underneath a thin Python wrapper. Each backend is implemented as an object of a class, and implements the MCR-DL API in accordance with each backend's requirements.

\subsection{MCR-DL API}
\label{sec:design-api}

MCR-DL implements all communication operations as depicted below in Listing \ref{lst:api}. 

%def wait(list<str> backends)
\begin{lstlisting}[language=Python, linewidth=9cm, xleftmargin=2.0ex, label={lst:api}, caption=High-level MCR-DL API, otherkeywords={torch.Tensor}, deletendkeywords={input}, literate={get\_size}{\bfseries get\_size}{8}{get\_rank}{\bfseries get\_rank}{8}{wait}{\bfseries wait}{4}{init}{\bfseries init}{4}{get\_backends}{\bfseries get\_backends}{12}{finalize}{\bfseries finalize}{9}{synchronize}{\bfseries synchronize}{11}{send}{\bfseries send}{4}{recv}{\bfseries recv}{4}{all\_to\_all}{\bfseries all\_to\_all}{10}{all\_to\_allv}{\bfseries all\_to\_allv}{11}{all\_to\_all\_single}{\bfseries all\_to\_all\_single}{17}{all\_reduce}{\bfseries all\_reduce}{10}{all\_gather}{\bfseries all\_gather}{10}{gather}{\bfseries gather}{6}{scatter}{\bfseries scatter}{7}{reduce}{\bfseries reduce}{6}{reduce\_scatter}{\bfseries reduce\_scatter}{14}{gatherv}{\bfseries gatherv}{7}{scatterv}{\bfseries scatterv}{7}{bcast}{\bfseries bcast}{5}{all\_gatherv}{\bfseries all\_gatherv}{11}]
def get_backends()
def init(list<str> backends)
def finalize(list<str> backends)
def synchronize(list<str> backends)
def get_size(str backend)
def get_rank(str backend)
def send(str backend, torch.Tensor t, int rank, bool async_op)
def recv(str backend, torch.Tensor t, int rank, bool async_op)
def all_to_all_single(str backend, torch.Tensor output, torch.Tensor input, bool async_op)
def all_to_all(str backend, list<torch.Tensor> output, list<torch.Tensor> input, bool async_op)
def all_reduce(str backend, torch.Tensor output, ReduceOp op, bool async_op)
def all_gather(str backend, torch.Tensor output, torch.Tensor input, bool async_op)
def gather(str backend, torch.Tensor output, int root, bool async_op)
def scatter(str backend, torch.Tensor output, int root, bool async_op)
def reduce(str backend, torch.Tensor output, int root, ReduceOp op, bool async_op)
def reduce_scatter(str backend, torch.Tensor output, int root, ReduceOp op, bool async_op)
def bcast(str backend, torch.Tensor output, int root, bool async_op)
def gatherv(str backend, torch.Tensor output, int root, list<int> rcounts, list<int> displs, bool async_op)
def scatterv(str backend, torch.Tensor output, int root, list<int> scounts, list<int> displs, bool async_op)
def all_to_allv(str backend, torch.Tensor output, torch.Tensor input, list<int> scounts, list<int> rcounts, list<int> def sdispls, list<int> rdispls, bool async_op)
def all_gatherv(str backend, torch.Tensor output, int root, list<int> rcounts, list<int> displs, bool async_op)
\end{lstlisting}

\begin{lstlisting}[language=Python,label={lst:productivity}, linewidth=9cm, xleftmargin=2.0ex, deletendkeywords={input}, otherkeywords={torch.int64}, literate={mcr\_ll}{\bfseries mcr\_dll}{6}, caption=Example of simplified prototyping with MCR-DL]
# Before MCR-DL
def allgather_host(self,
                   comm,
                   cupy_sign,
                   cupy_rbuf_sign,
                   cupy_scale,
                   cupy_rbuf_scale):

    # 1. Convert cupy to numpy
    numpy_rbuf_sign = np.zeros(
        [comm.Get_size(),
         cupy_sign.size],
        dtype=cupy_sign.dtype)
    numpy_rbuf_scale = np.zeros([comm.Get_size(),
                                           1],
                                          dtype=cupy_scale.dtype)

    numpy_sign = cupy.asnumpy(cupy_sign)
    numpy_rbuf_sign = cupy.asnumpy(cupy_rbuf_sign)
    numpy_scale = cupy.asnumpy(cupy_scale)
    numpy_rbuf_scale = cupy.asnumpy(cupy_rbuf_scale)
    cupy.cuda.get_current_stream().synchronize()

    # 2. Communicate numpy buffers
    comm.Allgather(numpy_sign, numpy_rbuf_sign)
    comm.Allgather(numpy_scale, numpy_rbuf_scale)
    comm.Barrier()

    # 3. Convert numpy back to cupy
    cupy_sign = cupy.asarray(numpy_sign)
    cupy_rbuf_sign = cupy.asarray(numpy_rbuf_sign)
    cupy_scale = cupy.asarray(numpy_scale)
    cupy_rbuf_scale = cupy.asarray(numpy_rbuf_scale)
    cupy.cuda.get_current_stream().synchronize()

    return cupy_sign, cupy_rbuf_sign, cupy_scale, cupy_rbuf_scale
	
	
# After MCR-DL
def allgather_host(self,
                   comm,
                   sign,
                   rbuf_sign,
                   scale,
                   rbuf_scale):
             
	comm.all_gather_base(rbuf_sign, sign)
	comm.all_gather_base(rbuf_sign, sign)
	
	return sign, rbuf_sign, scale, rbuf_scale
\end{lstlisting}


There are a few key takeaways from this API listing:

\begin{itemize}
    \item All operations take either a single \textbf{backend} string that matches an underlying backend class (e.g. "mv2-gdr", "nccl", etc) or a special backend flag "auto", which will dynamically choose the best message size for a given scale and message size if tuning tables are available. (Note: MCR-DL comes packaged with a tuning suite which first runs communication operation benchmarks for each backend, and uses this data to map each message size, scale, and operation to a given backend. This optimal backend choice is then used at runtime if "auto" is chosen)
    \item We conform to the PyTorch distributed module API conventions when possible to ease code refactoring to MCR-DL. An example of this is \textit{all\_to\_all}, which shuffles lists of tensors rather than individual tensor elements. This is a common usecase in distributed PyTorch applications. Another example is \textit{all\_to\_all\_single}, which directly shuffles the tensor elements themselves on each rank.
    \item Vectored collectives (e.g. gatherv/scatterv) and non-blocking collectives are supported for all backends.
\end{itemize}

\subsection{Advanced Communication Support}
Most distributed DL frameworks do not support the full underlying communication backend, only the operations that matter for DL parallelism (e.g. Allreduce). If a user needs a communication operation that is not currently supported by their distributed DL framework (e.g. advanced parallelism or data processing), they would need to sacrifice performance or productivity as mentioned in Section \ref{sec:motivation}. 

MCR-DL is a thin layer atop each currently-supported backend, and fully implements each backend on PyTorch tensors (See Figure \ref{fig:dl-stack} for the software stack). The MCR-DL "Backend" class can be easily extended to new communication backends such as MSCCL \cite{sccl}, Gloo, oneAPI, etc. 

%directly reference these figs
\begin{figure}[htbp]
\centering
    \includegraphics[width=.8\linewidth]{Figures/dl-stack.png}
    \caption{The MCR-DL Software Stack. MCR-DL is a thin layer between a target DL framework and the HPC system, and supports any number of stream-aware communication backends along with CUDA-Aware MPI.}
    \vspace{-3ex}
    \label{fig:dl-stack}
\end{figure}

\subsection{Synchronization}
\label{sec:design-synchronization}

One of the most important design considerations for a distributed framework is synchronization. We seek to add enough synchronization to rid the programmer of having to frequently debug deadlocks and data validation issues, while achieving enough overlap to maintain high performance at scale. With the right synchronization strategy, we are able to efficiently both overlap computation with communication, and overlap across communication backends without deadlocks or data validation issues.

\begin{lstlisting}[language=Python,label={lst:synch}, linewidth=9cm, xleftmargin=2.0ex, deletendkeywords={input}, otherkeywords={torch.int64}, literate={mcr\_dl}{\bfseries mcr\_dl}{6}, caption=Example of available overlap between communication and computation in a DL setting]
import torch
import mcr_dl

def tensor():
    return torch.rand(1,1)

x = tensor().cuda()
y = tensor().cuda()
mcr_dl.init('nccl')

h = mcr_dl.all_reduce('nccl', x, async_op=True)
y = y + y
h.wait('nccl')
result = x + y
\end{lstlisting}
\vspace{-3ex}

%\begin{comment}
\begin{figure}[htbp]
  \begin{center}
      \mbox {
          \hspace{-1\columnsep}
          \subfigure[Naive synchronization]
          {
            \includegraphics[width=.35\linewidth]{Figures/nosynch-fig.png}
            \label{fig:default-synch}
          }
          \subfigure[Synchronization in MCR-DL]
          {
            \includegraphics[width=.54\linewidth]{Figures/synch-fig.png}
            \label{fig:MCR-DL-synch}
          }
      }
      \vspace*{-0.5\baselineskip}
      \caption{Synchronization diagrams of Listing \ref{lst:synch} for the naive scheme and MCR-DL's fine-grained CUDA event scheme}
      \label{fig:single-backend-synchs}
  \end{center}
\vspace*{-1\baselineskip}
\end{figure}
%\end{comment}

First consider a naive synchronization scheme where a) all communication operations are posted to the PyTorch default stream, and b) we synchronize operations with cudaStreamSynchronize on that stream. We demonstrate the behavior of this scheme in Listing \ref{lst:synch}, a prototypical example of available communication/computation overlap faced in distributed DL. The resulting serial execution is depicted in Figure \ref{fig:default-synch}\footnote{The length of operation boxes in Figures is purely for synchronization discussion}. In MCR-DL, we exploit communication/computation overlap by creating a pool of \textbf{communication streams} for each backend. These streams are managed internally to MCR-DL. Communication operations posted to a backend's stream(s) are synchronized with fine-grained CUDA events. For figure \ref{fig:MCR-DL-synch}, this translates to: \textbf{(1):} An \textit{all\_reduce(x)} operation is posted to a NCCL communication stream in MCR-DL, and a distributed work handle is stored in \textit{h}, \textbf{(2):} MCR-DL records a CUDA event \textit{e} onto the communication stream and begins executing the \textit{all\_reduce(x)}, \textbf{(3):} the PyTorch default stream is able to progress with operations unrelated to \textit{x}, \textbf{(4):} when a data-dependency on \textit{x} is encountered, the user must call \textit{wait()} on the work handle \textit{h}, which MCR-DL uses internally to wait on the prior event \textit{e}. 

This scheme is similar to PyTorch's distributed module, but there are a few key implementation details that enable greater performance: \textbf{(1):} The use of multiple streams enables concurrent small-message operations (concurrent large-message operations are bandwidth-bound and show no benefit), \textbf{(2):} Instead of having an overall communication stream, each backend contains its own stream for overlap across backends. This synchronization behavior is extended to multiple backends in MCR-DL, which we will now discuss.

\subsection{Mixed-Backend Communications}
\label{sec:design-mixed-backend}

Since MCR-DL is a thin layer atop communication backends, we can pass the desired backend for any given communication operation dynamically within a Python script. An example of this is depicted below in Listing \ref{lst:mixed}.

\begin{comment}
\begin{figure}[htbp]
   \centering
        \includegraphics[width=.3\linewidth]{Figures/nosynch-fig.png}
        \caption{In MCR-DL, communication backends can be explicitly chosen, or users can dynamically choose the best backend for a given operation with "auto"} 
        \label{fig:MCR-DL synch}
\end{figure}

\begin{figure}[htbp]
   \centering
        \includegraphics[width=.6\linewidth]{Figures/synch-fig.png}
        \caption{In MCR-DL, communication backends can be explicitly chosen, or users can dynamically choose the best backend for a given operation with "auto"} 
        \label{fig:MCR-DL synch}
\end{figure}
\end{comment}

\begin{lstlisting}[language=Python,label={lst:mixed}, linewidth=9cm, xleftmargin=2.0ex,  deletendkeywords={input}, otherkeywords={torch.int64}, literate={mcr\_dl}{\bfseries mcr\_dl}{6}, caption=Example of explicit mixed-backend communications in MCR-DL. All inter-backend synchronization is performed internally. MCR-DL can dynamically choose the best backend to use at runtime if \textit{'auto'} is passed as the backend (See Section \ref{sec:design-tuning})]
import torch
import mcr_dl

def tensor():
    return torch.rand(1,1)

x = tensor().cuda()
y = tensor().cuda()
z = tensor().cuda()
mcr_dl.init(['nccl', 'mpi'])

h1 = mcr_dl.all_reduce('nccl', x, async_op=True)
h2 = mcr_dl.all_reduce('mpi', y, async_op=True)
z = z + z
h1.wait()
h2.wait()
result = x + y + z
\end{lstlisting}

\begin{figure}[htbp]
   \centering
        \includegraphics[width=.89\linewidth]{Figures/multi-synch-fig.png}
        \caption{In MCR-DL, communication backends can be explicitly chosen, or users can dynamically choose the best backend for a given operation with "auto"} 
        \label{fig:MCR-DL multisynch}
        \vspace{-2ex}
\end{figure}

\begin{comment}
\begin{lstlisting}[language=Python,label={lst:mixed}, deletendkeywords={input}, otherkeywords={torch.int64}, caption=Example of mixed-backend communications in MCR-DL]
import mcr_dl
import torch
# Declare backends and initialize 
backends=["nccl", "mvapich2-gdr"]
mcr_dl.init(backends)
# Declare PyTorch tensors and shuffle with NCCL\_Alltoall
input = torch.arange(4)
output = torch.empty([4], dtype=torch.int64)
mcr_dl.alltoall("nccl", output, input, async_op=True)

# Overlap Alltoall with compute here
...

# Synchronize runtime (all backends must be passed so that MCR-DL can avoid deadlocks!)
mcr_dl.synchronize(backends)
# Sum tensors with MPI\_Allreduce
mcr_dl.allreduce("mvapich2-gdr", output, async_op=False)
\end{lstlisting}
\end{comment}

However, each communication backend conforms to its own synchronization scheme. NCCL and its derivatives are synchronized on the CUDA streams, while MPI is synchronized on a host thread. If we are to mix backends without deadlocks, we will need to loop over each implemented backend and synchronize with their respective thread/stream. For the mixture of CUDA-Aware MPI and non-blocking NCCL, for example, this entails a call to CUDA-event based synchronization for NCCL (as discussed in section \ref{sec:design-synchronization}), followed by an \textit{MPI\_Wait} for MPI. Handling CUDA-aware MPI is a challenge since CUDA streams are not exposed by MPI to the application, this leads to two options (which MCR-DL provide at the initialization of an MPI backend): \textbf{(1):} Allow MPI to handle all streams, which sacrifices some MCR-DL overlap across backends, but preserves multiple CUDA stream logic (if it exists) within MPI. \textbf{(2):} Intercept calls to cudaStreamCreate and manage streams in MCR-DL, which exploits overlap across backends, but could potentially lead to deadlocks if multi-stream logic is used in MPI\footnote{In our experiments, we find that the best choice for this option is dependent on the MPI library}. An example of streams managed by MCR-DL is depicted by Figure \ref{fig:MCR-DL multisynch}. For ease of synchronization, every work handle's \textit{wait()} call waits on the PyTorch default stream (i.e. synchronization purely between communication streams is not supported). We note that stream-aware MPI like the implemention by MPICH \cite{stream-mpich} allows MCR-DL to fully overlap communication backends by self-managing streams.

While Figure \ref{fig:MCR-DL multisynch} depicts the mixture of a stream-aware backend (NCCL) and a backend without streams exposed to the user (MPI), the combination of any number of stream-aware backends (NCCL, SCCL, etc) is supported in MCR-DL and synchronized with CUDA events. Further, the combination of ABI-compatible MPI backends is supported\footnote{In our experiments, we found that mixing at most one non-stream-aware backend is optimal for overlap}. In our experiments, the initialization overhead for multiple communication libraries is negligible after being amortized over a few $(<10)$ DL training steps.

%Therefore, we follow the suggestion in %\cite{nithin-thesis} to replace calls to \textit{cudaStreamSynchronize} with a loop that calls both \textit{MPI\_Iprobe} (to check incoming messages without actually receiving them) and \textit{cudaStreamQuery} to check if the stream is blocking. With this method, we are able to guarantee deadlock-free mixed-communications benchmarks with \textit{mcr\_dl.synchronize()}.

\subsection{Communication Optimization Extensibility}
\label{sec:design-extensibility}

In PyTorch's distributed module and Horovod, there are a number of commmunication optimizations (e.g. Tensor Fusion, Padding, etc) built atop the communication layer to improve performance. Similarly, by encapsulating all communication operations into MCR-DL, these optimizations can be easily integrated into all communication operations and backends. One can utilize the rich Python ecosystem to insert optimizations into MCR-DL's Python layer as depicted in Figure \ref{fig:MCR-DL}. As examples, we have implemented lossy communication compression with zfp \cite{zfp}, Tensor Fusion (combining small tensors into a bandwidth-optimal large tensor), and communication logging (which is used to generate Figures \ref{fig:comms-profiles} and \ref{fig:computevcomms-after}). Further, future optimizations (e.g. persistent collectives) can be easily added with minimal changes among backends and operations. These optimizations can be applied to incoming messages with only a few lines of Python code before routing the operation to its respective C++ backend. 

There are two parameters for Tensor Fusion: the maximum fusion buffer size $B$ and the maximum time $T$ to wait for that fusion buffer to fill with small tensors. MCR-DL introduces a small optimization for Tensor Fusion, where if the Fusion buffer does not reach $B$ before $T$ (and therefore does not saturate bandwidth), the communication is overlapped with other backends' Fusion buffers, if available. This Tensor Fusion optimization is used in all DL training results in Section \ref{sec:results}.

\begin{figure}[htbp]
   \centering
        \includegraphics[width=\linewidth]{Figures/mixdl-flowchart-extensible.png}
        \caption{In MCR-DL, communication backends can be explicitly chosen, or users can dynamically choose the best backend for a given operation with the "auto" backend option. Further, MCR-DL routes all communication operations through an optional set of optimizations, including Tensor Fusion (combining small tensors into a bandwidth-optimal large tensor), message compression, and logging.} 
        \label{fig:MCR-DL}
\vspace{-3ex}
\end{figure}

\subsection{Communication Tuning}
\label{sec:design-tuning}

Tuning is an established problem in distributed communication \cite{tuning-algo-selection, tuning-star, tuning-ml}. MCR-DL comes packaged with a tuning suite that seeks to map an input communication operation (with associated message size) to the best-performing backend (e.g. \textit{all\_reduce} $\rightarrow$ NCCL). This introduces additional complication since not only are distinct communication operations mixed-backend (e.g. all\_reduce and gather), but MCR-DL allows a single operation to choose the best backend with the "\textbf{auto}" backend option (e.g. mcr\_dl.gather("auto") routes \{small-message gather\} $\rightarrow$ MPI, and \{large-message gather\} $\rightarrow$ NCCL). This behavior is depicted in Figure \ref{fig:MCR-DL}, where solid lines depict the backend chosen for a given operation.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\renewcommand{\arraystretch}{1.2}
\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
Message Size & Backend      \\ \hline
256          & MVAPICH2-GDR \\ \hline
512          & MVAPICH2-GDR \\ \hline
1024         & MVAPICH2-GDR \\ \hline
2048         & MVAPICH2-GDR \\ \hline
4096         & NCCL         \\ \hline
8192         & NCCL         \\ \hline
16384        & SCCL         \\ \hline
32768        & SCCL         \\ \hline
\end{tabular}
\vspace{1ex}
\caption{Example tuning table for the all\_gather collective operation at a single world size generated by MCR-DL}
\vspace{-1ex}
\label{tab:tuning-table}
\vspace{-3ex}
\end{table}
%\vspace{-1ex}

This tuning is implemented as a static tuning table. The tuning suite is composed of a set of micro-benchmark scripts that evaluate end-to-end time on a set of overlapped communication operations for each backend. 
By choosing the backend with the minimum end-to-end time for each input tensor size, MCR-DL generates a table like Table \ref{tab:tuning-table} for each world size (i.e. the number of GPUs) trained over.
Every collective requires its own static tuning table. 
The size of each collective's tuning table is dependent both on the number of specific message sizes we wish to tune for, as well as the number of scales (world size) we are tuning over. Specifically, a given table entry is first mapped by the world size, then by the message size.
Therefore, the total number of tuning table entries is given by:
$(\text{Num\_Collectives} \times \text{Num\_Scales} \times \text{Num\_Message\_Sizes})$. Since the performance of each communication backend depends heavily on the combination of inter-node fabric, intra-node fabric, and compute hardware used, tuning tables are not transferable across HPC systems. However, we find that general trends tend to hold across systems with a coarsely similar architecture (e.g. MVAPICH2-GDR consistently performs the best for small messages).