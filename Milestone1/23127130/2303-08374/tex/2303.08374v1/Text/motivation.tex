\section{Motivation}
\label{sec:motivation}


As the parallelism schemes for DL models become more complex, more advanced communication operations are required. Since modern distributed DL frameworks such as Horovod and PyTorch's Distributed module don't support all MPI or NCCL operations (e.g. variable message-size versions of collectives such as Gatherv are not supported by PyTorch, Horovod only supports major collectives such as Allgather, Alltoall, and Allreduce), DL researchers are required to either:

\begin{enumerate}
    \item Implement their desired collectives via point-to-point operations (if point-to-point operations are supported in the chosen framework)
    \item Transfer tensors between the distributed DL framework and an external MPI Python wrapper such as mpi4py [TODO cite mpi4py]
\end{enumerate}

Item 1 above sacrifices the performance enhancements present in NCCL and most CUDA-Aware libraries, while item 2 introduces significant complication to the program. We believe that a single unified interface between the DL framework (PyTorch) and the communication backend (MPI, CUDA, etc) will both alleviate these performance and productivity bottlenecks, while introducing the possibility of mixed backend communication (e.g. MPI Alltoall and NCCL Allreduce).

Consider the case of DeepSpeed-MoE (DS-MoE). Given the collective performance in Figure \cite{fig:colls-lassen-64gpu}, which communication backend should be used? A myriad of application questions would need to be answered such as which collectives DS-MoE uses, their relative frequencies, and the range of message sizes for each collective. Any decision on a single communication backend will lose out on some collectives and at some message ranges. Further, such a decision will need to be reevaluated at each subsequent release cycle of the communication backends. If the user is able to switch among communication backends, they could squeeze more performance out of their application while reducing the setup cost of changing communication backends if future communication backend releases change. 

\begin{comment}
\begin{figure}[thbp!]
  \begin{center}
      \mbox {
          \hspace{-1\columnsep}
          \subfigure[Compute vs. Communication]
          {
              \includegraphics[width=.4\linewidth]{Graphs/computevcomms.png}
              \label{fig:computevcomms}
          }
          \subfigure[Communication Breakdown]
          {
              \includegraphics[width=.4\linewidth]{Graphs/comms-breakdown.png}
              \label{fig:commsbrreakdown}
          }
      }
      \vspace*{-0.5\baselineskip}
      \caption{Profiling of DeepSpeed-MoE}
      \label{fig:dsmoe-prof}
  \end{center}
\vspace*{-1\baselineskip}
\end{figure}
\end{comment}