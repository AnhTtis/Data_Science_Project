% make single para, move to end before concl
\section{Related Work}
\label{sec:related}

\subsection{DL Communication Framework Design}

DeepSpeed \cite{deepspeed} uses PyTorch's distributed module \cite{pytorch-dist} to implement optimized DL communication at extreme scales. Recently, DeepSpeed has added support for Mixture-of-Experts (MoE) DL models \cite{ds-moe-latest, ds-moe-orig}. Horovod \cite{horovod} is a data-parallel focused framework that experimentally supports mixed communications without deadlock-avoidance support. The Livermore Big Artificial Neural Network Toolkit (LBANN) is an HPC-centric distributed DL framework that supports multiple parallelism levels. The MPI for Python package \cite{mpi4py} supplies Python bindings for the MPI standard. Our work competes with these works by seeking to unify communication calls into a single interface built atop PyTorch.

\subsection{Mixing MPI with an External Framework}

The work in \cite{jose-upc} combined an MPI runtime with UPC in a deadlock-free architecture by unifying the runtimes. The resulting runtime shared resources between MPI and UPC to avoid data-dependencies. In recent releases, the MVAPICH2-GDR \cite{MVAPICH2} CUDA-Aware MPI library has added support for NCCL collectives. However, this support is not optimized for non-blocking communication operations like those required by DLRM. Aluminum \cite{aluminum} is a DL-focused communication library built on MPI and NCCL, but is focused on latency-bound communication operations. Our work is complementary to the above works, since we choose the best backend for each communication operation.

\subsection{Scaling Mixture-of-Experts and DLRM Models}

The work in \cite{gshard} scaled a 600 billion parameter Mixture-of-Experts (MoE) model to 2048 TPU v3 processors. DeepSpeed has recently added support for MoE DL models \cite{ds-moe-orig} and scaled beyond a trillion parameters \cite{ds-moe-latest}. MoE models are gradually being applied to other domains such as vision \cite{vit-moe}. DLRM \cite{dlrm} has scaled beyond a trillion parameters with 4D parallelism techniques \cite{dlrm-scale}. We demonstrate that our work further improves the scaling behavior of these complex parallel DL models.