\section{Performance Characterization}
\label{sec:results}

% Add arrows and bold/red font

% put overhead in legends?
\begin{figure}[!b]
\centering
    \includegraphics[width=\linewidth]{Graphs/python-overhead-new.png}
    \caption{Overhead over OMB for MCR-DL and PyTorch Distributed for a fixed backend on ThetaGPU (32 A100 GPUs). MCR-DL reduces overhead by ensuring top-level Python logic is minimal.} 
    \label{fig:overhead}
\end{figure}

\begin{figure*}[!b]
  \begin{center}
      \mbox {
          \hspace{-1\columnsep}
          \subfigure[DS-MoE Throughput]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 2,clip]{Graphs/throughput-moe.png}
              \label{fig:moe-throughput}
          }
          \hspace{4ex}
          \subfigure[DS-MoE Scaling Efficiency]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 1,clip]{Graphs/efficiency-moe.png}
              \label{fig:moe-efficiency}
          }
      }
      \vspace*{-0.5\baselineskip}
      \caption{Throughput and scaling efficiency improvements for DS-MoE with pure MVAPICH2-GDR, pure NCCL, and mixed-backends with MCR-DL on Lassen}
      \label{fig:moe-results}
  \end{center}
\vspace*{-1\baselineskip}
\end{figure*}

\begin{figure*}[!b]
  \begin{center}
      \mbox {
          \hspace{-1\columnsep}
          \subfigure[DLRM Throughput]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 1,clip]{Graphs/throughput-dlrm.png}
              \label{fig:dlrm-throughput}
          }
          \hspace{4ex}
          \subfigure[DLRM Scaling Efficiency]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 2,clip]{Graphs/efficiency-dlrm.png}
              \label{fig:dlrm-efficiency}
          }
      }
      \vspace*{-0.5\baselineskip}
      \caption{Throughput and scaling efficiency improvements for DLRM with pure MVAPICH2-GDR, pure NCCL, and mixed-backends with MCR-DL on ThetaGPU}
      \label{fig:dlrm-results}
  \end{center}
\vspace*{-1\baselineskip}
\end{figure*}

\begin{figure*}[!b]
  \begin{center}
      \mbox {
          \hspace{-1\columnsep}
          \subfigure[Megatron-DeepSpeed Dense Model Throughput]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 1,clip]{Graphs/throughput-dense.png}
              \label{fig:dense-throughput}
          }
          \hspace{4ex}
          \subfigure[Megatron-DeepSpeed Dense Model Scaling Efficiency]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 1,clip]{Graphs/efficiency-dense.png}
              \label{fig:dense-efficiency}
          }
      }
      \vspace*{-0.5\baselineskip}
      \caption{Throughput and scaling efficiency improvements for dense Megatron-DeepSpeed with pure MVAPICH2-GDR, pure SCCL, and mixed-backends with MCR-DL on ThetaGPU}
      \label{fig:dense-results}
  \end{center}
\vspace*{-1\baselineskip}
\end{figure*}

\subsubsection{Node Architecture}
\label{sec:node-arch}

All experimental evaluations\footnote{The choice of cluster for a given application was purely made out of external factors such as available compute and ease of software compatibility} were carried out on the Lassen cluster at Lawrence Livermore National Laboratory and the ThetaGPU cluster at Argonne Leadership Computing Facility \cite{thetagpu}. Lassen is composed of 792 nodes each consisting of four 16 GB NVIDIA V100 GPUs and two 44-core IBM Power 9 CPUs. Nodes are connected via Mellanox Infiniband EDR in a fat-tree topology. ThetaGPU is composed of 24 NVIDIA DGX A100 nodes, each containing two AMD Rome CPUs and eight 40 GB NVIDIA A100 GPUs.

\subsubsection{Communication Backends}
\label{sec:comm-backends}

We used a mixture of MVAPICH2-GDR 2.3.7 \cite{MVAPICH2}, Open-MPI v5.1.0 \cite{openmpi} (built with UCX v1.13.1), the latest MSCCL \cite{sccl}, and NCCL 2.14.3-1 \cite{nccl} for all DL experiments. All backends and frameworks were built with CUDA 11.4.152 on ThetaGPU and CUDA 11.4.100 on Lassen.

\subsubsection{Software Libraries}
\label{sec:software}

All micro-benchmark evaluations were carried out with OSU Micro-Benchmarks (OMB) 6.1. For our DL evaluations, we used source-built PyTorch v1.12.1 and DeepSpeed v0.7.4. 

\subsubsection{DL Training Settings}
\label{sec:dl-settings}

For both DS-MoE and DLRM, we had to replace all dependencies on PyTorch's distributed module with MCR-DL calls. Since MCR-DL conforms to the PyTorch API wherever possible, this step is a straightforward search-and-replace.

We trained a 4B parameter DS-MoE model (350M+PR-MoE-32/64) on the Pile \cite{pile}. For more details on this model and on DS-MoE, see \cite{ds-moe-latest}.

For DLRM, we trained 100 synthetic data batches of size 8k with bottom and top MLPs of size (512-512-64) and (1024-1024-1024-1), respectively. The embedding table size used is 1e6 $\times$ (num\_ranks).

The dense Megatron-DeepSpeed model contained 6.7B parameters with a model-parallelism degree of 2 and ZeRO stage 2. It was also trained on the Pile \cite{pile}.

\subsection{Micro-Benchmarks}
\label{sec:results-microbench}

Before proceeding to application-level performance evaluations, we first created simple collective and point-to-point benchmarks to ensure MCR-DL doesn't introduce significant performance overhead when compared to micro-benchmarks implemented at the C-level, as investigated earlier with OMB.  As demonstrated in Figure \ref{fig:overhead}, MCR-DL introduces an overhead of around 5\% for small MPI\_Alltoall operations (under 4kB). However, this overhead quickly reduces to 1\% in the MB message range, which is the message range expected for most DL training applications \cite{hvprof}. PyTorch's distributed module built atop MVAPICH2-GDR, however, has a high overhead (18\%) for small messages, and converges to a higher overhead (4\%) in the MB message range. MCR-DL doesn't introduce significant overhead for communication operations. 

In order to spare users the OMB evaluations like Figure \ref{fig:colls-lassen-64gpu}, we created a tuning suite to generate a static tuning table for later use in applications. The tuning suite first runs basic collective and point-to-point evaluations over a range of message sizes, scales, and backends. Then, the tuning scripts create a tuning table which maps a given message size and number of processes to a given communication backend. The tables for Lassen and ThetaGPU are used in subsequent DL evaluations. This tuning table is used whenever the "auto" backend is passed to a collective as described in Section \ref{sec:design}. The difference between static-backend mixing and tuned mixing is depicted in all DL training figures as MCR-DL and MCR-DL-T, respectively.
\vspace{-0.25ex}

\subsection{DL Training}
\label{results-dl-training}

With the setup described above in \ref{sec:node-arch} through \ref{sec:dl-settings}, we carried out DL training evaluations with MCR-DL on the Lassen HPC system. Baseline experiments were carried out with PyTorch's distributed module built against a single communication backend (e.g. ``Baseline SCCL`` is PyTorch distributed built with the SCCL backend). Neither tensor fusion nor compression from Section \ref{sec:design-extensibility} were used in evaluations\footnote{While we expect performance benefits from tensor fusion and compression, we wish to isolate the effect of mixing communication backends.}. Further, to compare coarse-grained mix-and-match (i.e. one backend per collective such as NCCL \textit{Allreduce} and MPI \textit{Alltoall}) against fine-grained mix-and-match (i.e. one backend per (collective, message size) pair such as NCCL \textit{Allreduce} for 1MB messages and MPI \textit{Allreduce for 512KB messages}. These two settings of MCR-DL are depicted in Figures \ref{fig:moe-results}-\ref{fig:dense-results} as \textbf{MCR-DL} and \textbf{MCR-DL-T}, respectively. First, we run pre-training throughput experiments DS-MoE for pure NCCL, pure MVAPICH2-GDR and mixed backends. Results are depicted in \ref{fig:moe-throughput}. At smaller scales, NCCL performs better than MVAPICH2-GDR because Alltoall is not yet a dominant factor in communication time. We see a crossover threshold from Allreduce-bound to Alltoall-bound communication at around 32 GPUs, beyond which MVAPICH2-GDR's improved Alltoall starts to show benefits. The performance difference between pure NCCL and pure MVAPICH2-GDR is still small, however, because NCCL's Allreduce collective is more performant than MVAPICH2-GDR's at this message range.

MCR-DL is able to exploit MVAPICH2-GDR's improved Alltoall and NCCL's improved Allreduce to perform best at all scales without deadlocks. At 256 GPUs, we see a 31\% improvement over pure MVAPICH2-GDR and a 35\% improvement over pure NCCL. Scaling efficiency \ref{fig:moe-efficiency} is also greatly improved with MCR-DL, maintaining a 81\% efficiency at 256 V100 GPUs.

Second, we have evaluated pure NCCL, pure MVAPICH2-GDR and mixed backends on the ThetaGPU HPC system for DLRM. Results are depicted in Figure \ref{fig:dlrm-throughput}. NCCL again beats MVAPICH2-GDR at small scales due to its improved Allreduce. At higher scales, MVAPICH2-GDR again starts to perform better due to Alltoall's scaling, and MCR-DL is able to use each backend's strengths to improve performance, achieving a 25\% improvement over pure MVAPICH2-GDR and a 30\% improvement over pure NCCL. Scaling efficiency is less that of DS-MoE, but still improved by MCR-DL, maintaining a 75\% efficiency at 32 A100 GPUs. 

\vspace{-1\baselineskip}
\begin{figure}[htbp]
\centering
    \includegraphics[width=\linewidth]{Graphs/transformer_related.png}
    \vspace{-1\baselineskip}
    \caption{Comparison of MCR-DL against competing PyTorch-compatible frameworks on a Mixture-of-Experts transformer using 256 Lassen V100 GPUs.}
    \label{fig:related-results}
    \vspace{-.5\baselineskip}
\end{figure}

In order to directly compare the performance of MCR-DL with all PyTorch-compatible\footnote{LBANN does not provide any MoE implementation, and is not compatible with any mainstream DL frameworks such as PyTorch} competing frameworks in Table \ref{tab:related}, we swapped all communication operations in Megatron-DeepSpeed with each respective framework's implementation. The results on 256 Lassen V100 GPUs is depicted in Figure \ref{fig:related-results}. In order to compare each framework's best performance, MCR-DL, Horovod, and PyTorch-distributed were run with tensor fusion enabled, which leads to the performance gap between mpi4py and both Horovod and PyTorch-distributed. MCR-DL performs the best due to its mixed-backend optimizations coupled with tensor fusion.

For completeness, we have also trained a dense Megatron-DeepSpeed model on the ThetaGPU cluster with a mixture of MSCCL \cite{sccl} and MVAPICH2-GDR \cite{MVAPICH2}. As a secondary result, we have taken the compute vs. communication breakdown for DS-MoE and DLRM when using MCR-DL at 256 Lassen V100 GPUs and 32 ThetaGPU A100 GPUs, respectively. MCR-DL is an important component in reducing the computation bottleneck at scale, demonstrating a 9\% reduction in communication time for DS-MoE and a 7\% reduction in communication time for DLRM.

\begin{figure}[thbp!]
  \begin{center}
      \mbox {
          \hspace{-1\columnsep}
          \subfigure[DS-MoE]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 2,clip]{Graphs/computevcomms-dsmoe-after.png}
              \label{fig:python-overhead-allreduce}
          }
          \subfigure[DLRM]
          {
              \includegraphics[width=.45\linewidth,trim=2 2 2 2,clip]{Graphs/computevcomms-dlrm-after.png}
              \label{fig:python-overhead-alltoall}
          }
      }
      \vspace*{-0.5\baselineskip}
      \caption{Communication overhead reduction with MCR-DL at 256 Lassen V100 GPUs (DS-MoE) and 32 ThetaGPU A100 GPUs (DLRM).}
      \label{fig:computevcomms-after}
  \end{center}
\vspace*{-2\baselineskip}
\end{figure}

\section{Discussion}
\label{sec:discussion}

The throughput and scaling efficiency improvements in Figures \ref{fig:moe-results}, \ref{fig:dlrm-results}, and \ref{fig:dense-results} demonstrate that a mixed-backend DL communication framework can significantly improve the performance of emerging DL models by reducing the communication bottleneck. Further, it was confirmed that a C++ backbone underneath a thin Python layer ensures low-overhead communication operations, which enables the exploration of small-message latency-bound operations for emerging models.

These results are in agreement with the original observation that modern communication backends vary widely in performance characteristics across operations, within operations, and across releases. By mix-and-matching backends for a given operation (and within an operation), significant communication performance improvements were achieved. Further, since our communication operations are implemented in low-latency C++ code underneath a thin Python interface, we have maintained low overhead while ensuring compatibility with Python-based DL frameworks.

The performance improvements inherent in mixing communication backends are consistent with the findings of previous NCCL and MPI studies \cite{nithin-thesis} and studies exploring the mixture of MPI with external runtimes in \cite{jose-upc}. 