\section{Conclusion}
\label{sec:conclusion}

State-of-the-art deep learning (DL) models are pushing the boundaries of existing fields while pioneering entirely new areas of study. However, such DL models are often impossible or impractical to train on single processors or small-scale workstations. Further work in novel parallelism schemes and optimizations will require a robust and extensible interface between DL frameworks and communication backends. In this paper, we present and evaluate MCR-DL: a Mix-and-Match Communication Runtime for DL. MCR-DL supports all communication operations and backends, and enables mixed-backend communication to ensure the most performant backend is being used for a given communication operation. The proposed design is demonstrated on state-of-the-art DL models such as DLRM \cite{dlrm} and Mixture-of-Experts (MoE) \cite{ds-moe-orig, ds-moe-latest}. We report up to a 31\% improvement in DeepSpeed-MoE throughput on 256 V100 GPUs on the Lassen HPC system and a 25\% improvement in DLRM on 32 A100 GPUs on the Theta-GPU HPC system. We believe that MCR-DL will pave the way for designing and implementing future DL communication enhancements and distributed DL frameworks.