{
    "arxiv_id": "2303.11916",
    "paper_title": "CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion",
    "authors": [
        "Geonmo Gu",
        "Sanghyuk Chun",
        "Wonjae Kim",
        "HeeJae Jun",
        "Yoohoon Kang",
        "Sangdoo Yun"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2024-02-27"
    ],
    "latest_version": 3,
    "categories": [
        "cs.CV",
        "cs.IR"
    ],
    "abstract": "This paper proposes a novel diffusion-based model, CompoDiff, for solving zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8 million reference images, conditions, and corresponding target image triplets to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the previous CIR approaches, such as poor generalizability due to the small dataset scale and the limited types of conditions. CompoDiff not only achieves a new state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO, and GeneCIS, but also enables a more versatile and controllable CIR by accepting various conditions, such as negative text, and image mask conditions. CompoDiff also shows the controllability of the condition strength between text and image queries and the trade-off between inference speed and performance, which are unavailable with existing CIR methods. The code and dataset are available at https://github.com/navervision/CompoDiff",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11916v1",
        "http://arxiv.org/pdf/2303.11916v2",
        "http://arxiv.org/pdf/2303.11916v3"
    ],
    "publication_venue": "First two authors contributed equally; 28 pages, 6.2MB"
}