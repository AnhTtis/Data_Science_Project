\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lccc|ccc|ccc|c}
\toprule
 & \multicolumn{3}{c|}{COCO 5k} & \multicolumn{3}{c|}{CxC} & \multicolumn{3}{c|}{ECCV Caption} & Throughput \\
Method & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & mAP@R & R-P & R@1 & images/sec\\
\midrule
\multicolumn{11}{c}{Image to text retrieval} \\
\midrule
Prior \citep{kakaobrain2022karlo-v1-alpha} & 32.04 & 56.84 & 67.68 & 33.76 & 60.50 & 71.32 & 14.19 & 23.37 & 46.47& 497.28 \\
Prior-like Stage1 & 34.32 & 58.40 & 69.52 & 35.01 & 62.35 & 74.21 & 16.35 & 25.20 & 49.01 & 497.28 \\
Ours (Stage 1 only) & 35.13 & 59.46 & 70.26 & 35.30 & 62.62 & 74.02 & 16.01 & 25.44 & 49.64 & 1475.52 \\
Ours (Stage 1 + Stage2) & 33.20 & 58.00 & 68.94 & 34.78 & 61.68 & 72.96 & 15.07 & 24.39 & 47.03 & 1473.92\\
\midrule
\multicolumn{11}{c}{Text to image retrieval} \\
\midrule
Prior \citep{kakaobrain2022karlo-v1-alpha} & 17.05 & 34.25 & 43.16 & 18.62 & 37.43 & 47.07 & 18.10 & 26.46 & 46.40 & 497.28 \\
Prior-like Stage1 & 22.62 & 38.31 & 48.11 & 21.42 & 41.42 & 51.79 & 20.70 & 29.80 & 51.50 & 497.28 \\
Ours (Stage 1 only) & 22.47 & 39.18 & 49.08 & 22.51 & 42.40 & 52.77 & 21.46 & 30.30 & 53.75 & 1475.52 \\
Ours (Stage 1 + Stage2) & 20.00 & 38.63 & 48.25 & 21.57 & 41.71 & 51.99 & 20.82 & 29.84 & 51.65 & 1473.92\\
\bottomrule
\end{tabular}
\caption{\small \textbf{Comparisons of design choices for handling textual embeddings on cross-modal retrieval benchmarks.} Throughput was measured on 1 A100 GPU with a batch size of 32. For all metrics, higher is better.}
\label{tab:i2t_retrieval}
\vspace{-.5em}
\end{table*}