\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{xspace}
\usepackage{enumitem}
\usepackage{subfigure}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}


\iccvfinalcopy %

\newcommand{\oursfull}{Composed Image Retrieval with Latent Diffusion (\ours)\xspace}
\newcommand{\ours}{\mbox{CompoDiff}\xspace}
\newcommand{\ourdataset}{SynthTriplets18M\xspace}

\newcommand{\rawrefimg}{\ensuremath{x_{i_R}}\xspace}
\newcommand{\rawcond}{\ensuremath{x_{c}}\xspace}
\newcommand{\rawcondtext}{\ensuremath{x_{c_T}}\xspace}
\newcommand{\rawcondnegtext}{\ensuremath{x_{c_{T^{\text{-}}}}}\xspace}
\newcommand{\rawcondmask}{\ensuremath{x_{c_M}}\xspace}
\newcommand{\rawtarimg}{\ensuremath{x_i}\xspace}
\newcommand{\rawtriplet}{\ensuremath{\langle \rawrefimg, \rawcond, \rawtarimg \rangle}\xspace}
\newcommand{\rawreftxt}{\ensuremath{x_{t_R}}\xspace}
\newcommand{\rawtartxt}{\ensuremath{x_t}\xspace}
\newcommand{\rawcaptriplet}{\ensuremath{\langle \rawreftxt, \rawcond, \rawtartxt \rangle}\xspace}
\newcommand{\rawcappair}{\ensuremath{\langle \rawreftxt, \rawtartxt \rangle}\xspace}

\newcommand{\refimg}{\ensuremath{z_{i_R}}\xspace}
\newcommand{\cond}{\ensuremath{z_{c}}\xspace}
\newcommand{\condtext}{\ensuremath{z_{c_T}}\xspace}
\newcommand{\condnegtext}{\ensuremath{z_{c_{T^{\text{-}}}}}\xspace}
\newcommand{\condmask}{\ensuremath{z_{c_M}}\xspace}
\newcommand{\tarimg}{\ensuremath{z_i}\xspace}
\newcommand{\triplet}{\ensuremath{\langle \refimg, \cond, \tarimg \rangle}\xspace}
\newcommand{\reftxt}{\ensuremath{z_{t_R}}\xspace}
\newcommand{\tartxt}{\ensuremath{z_t}\xspace}
\newcommand{\captriplet}{\ensuremath{\langle \reftxt, \cond, \tartxt \rangle}\xspace}
\newcommand{\cappair}{\ensuremath{\langle \reftxt, \tartxt \rangle}\xspace}

\def\iccvPaperID{1253} %
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\usepackage{xcolor}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion}

\author{Geonmo Gu$^{*,\,1}$, ~Sanghyuk Chun$^{*,\,2}$, ~Wonjae Kim$^{2}$, ~HeeJae Jun$^{1}$, ~Yoohoon Kang$^{1}$, ~Sangdoo Yun$^{2}$\\
{\small $^*$ Equal contribution}\\
\\
{$^{1}${NAVER Vision} \qquad $^{2}${NAVER AI Lab}}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff not only achieves a new zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also enables a more versatile CIR by accepting various conditions, such as negative text and image mask conditions, which are unavailable with existing CIR methods. In addition, the CompoDiff features are on the intact CLIP embedding space so that they can be directly used for all existing models exploiting the CLIP space. The code and dataset used for the training, and the pre-trained weights are available at \url{https://github.com/navervision/CompoDiff}.
\end{abstract}

\section{Introduction}

Imagine there is a virtual customer seeking a captivating cloth serendipitously found on social media but with different materials and colors. The customer needs a search engine that can process composed queries, \eg, the reference cloth image along with texts specifying the preferred material and color.
This task has been recently formulated as \textit{Composed Image Retrieval (CIR)} \cite{cirr, fashioniq} as shown in the top of \cref{fig:teaser}. CIR systems offer the benefits of visually similar item searches as image-to-image retrieval \cite{philbin2007object}, while providing a high degree of freedom to depict the desired item as text-to-image retrieval \cite{faghri2018vsepp}. 
CIR also can be helpful for improving the search quality by iteratively taking user feedback by text conditions. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \caption{\small \textbf{Composed Image Retrieval (CIR) scenarios.} Top row shows an example of a standard CIR scenario. From the second to the bottom row, we present versatile CIR scenarios with mixed conditions (\eg, negative instruction and mask). All results are retrieved by our \ours on LAION-5B images.}
    \label{fig:teaser}
\end{figure}

To train a model for CIR tasks, we need a dataset of triplets \rawtriplet \footnote{Throughout this paper, we will use $x$ to denote raw data and $z$ to denote a vector encoded from $x$ using an encoder,\eg, CLIP \cite{radford2021clip}.} consisting of a reference image (\rawrefimg), a condition (\rawcond), and the corresponding target image (\rawtarimg). However, obtaining such triplets can be costly and sometimes impossible. %
Fortunately, existing CIR benchmarks \cite{fashioniq, cirr} provide such training triplets, which allows CIR models to achieve promising performance within the train-validation split of each dataset.
However, the amount of the training triplet is not sufficient (\eg, 30K triplets for Fashion-IQ \cite{fashioniq} and 36K triplets for CIRR \cite{cirr}) for learning generalizable CIR models, as pointed out in \cite{saito2023pic2word}.
Furthermore, the existing CIR methods usually consider a scenario where the condition (\rawcond) is given by a text (\rawcondtext). Therefore, the CIR models cannot handle more versatile conditions beyond a limited text one, \eg, complex text conditions like negative prompts (\rawcondnegtext) or indicating where (\rawcondmask) the condition is applied (See \cref{fig:teaser}).
In short, the previous CIR models suffer from poor generalizability due to the small dataset scale and the limited types of conditions.





Recent methods \cite{baldrati2021clip4cir_v1, baldrati2022clip4cir, saito2023pic2word} address the scalability issue of CIR datasets by leveraging external knowledge from models pre-trained on large-scale datasets such as CLIP \cite{radford2021clip}. CLIP4Cir \cite{baldrati2021clip4cir_v1, baldrati2022clip4cir} fine-tunes the CLIP text encoder on the triplet dataset, but its zero-shot CIR performance is still lacking. 
In contrast, Pic2Word \cite{saito2023pic2word} only fine-tunes projectors on top of frozen CLIP encoders with image-text pairs instead of triplets, achieving zero-shot CIR ability. However, its performance on CIR benchmarks, such as FashionIQ, falls short due to the lack of triplet-based training. 






To resolve the data scale issue, we synthesized a vast set of high-quality \textbf{18M} triplets of \rawtriplet by leveraging the large-scale pre-trained generative models such as OPT \cite{zhang2022opt}, and Stable Diffusion \cite{rombach2022latentdiffusion}. %
Our massive dataset, named \textbf{\ourdataset}, is over 500 times larger than existing CIR datasets and covers a diverse and extensive range of conditioning cases. Our dataset leads to a significant performance improvement for any CIR model. 
\Eg, ARTEMIS \cite{delmas2022artemis} trained exclusively with our dataset achieves outperforming zero-shot performance with 40.62 average recalls in FashionIQ, against the fine-tuned counterpart of 38.17.
We also achieve state-of-the-art FashionIQ and CIRR results by pre-training the previous best model on \ourdataset and fine-tuning it on the target dataset.




We also introduce a diffusion-based CIR model to compose versatile and complex conditions, named \oursfull.
Motivated by recent advances in diffusion models \cite{rombach2022latentdiffusion, dall-e2, brooks2022instructpix2pix}, we train a latent diffusion model translating the embedding of the reference image (\refimg) to the embedding of the target image (\tarimg) guided by the embedding of the given condition (\cond). 
Thanks to the classifier-free guidance of the diffusion model, \ours can easily handle versatile and complex conditions. 
As \cref{fig:teaser}, \ours can search images from a given Eiffel Tower image by compositionally instructing ``with cherry blossom'' (text condition \rawcondtext), ``without France'' (negative text condition \rawcondnegtext) or specifying desired locations (mask condition \rawcondmask), which is not possible with standard CIR scenario with limited text condition \rawcondtext. 

Another notable benefit of our method is the ability to control the conditions during inference, inherited from the nature of diffusion models:
Users can adjust the weight of conditions to make the model focus on the user's preference. Users can also manipulate the randomness of the models to vary the level of serendipity. 
Moreover, \ours can control the inference speed with minimal sacrifice of retrieval performance, accomplished by adjusting the number of steps in the diffusion model. 
As a result, our model can be deployed in various scenarios with different computation budgets. 
Note that all these controllability features can be achieved by controlling inference parameters of classifier-free guidance, without any model training.






Our contributions are as follows: (1) We generate \ourdataset, a diverse and massive synthetic dataset of 18M triplets that can make CIR models achieve zero-shot generalizability. (2) We propose a diffusion-based CIR method (\ours) that can handle versatile CIR scenarios with composed queries with high controllability. 
(3) \ours shows significant zero-shot and comparable fine-tuned performances on FashionIQ and CIRR datasets. Also, we achieve a new state-of-the-art by pre-training the previous best model on our dataset.
(4) Finally, we show the versatility of \ours in the real-world zero-shot CIR scenario, \eg, controllability or various conditions.







\section{Related Works}

\paragraph{Composed image retrieval.}
Until very recently, the mainstream CIR models were centered around multi-modal fusion methods, which combine image and text features extracted from separately trained visual and text encoders.
For example, Vo \etal \cite{vo2019tirg} and Yu \etal \cite{yu2020curlingnet} used CNN and RNN, and Chen \etal \cite{chen2022mur} and Anwaar \etal \cite{Anwaar_Labintcev_Kleinsteuber_2021} used CNN and Transformer for extracting their respective features.
However, as shown in other vision-language tasks (\eg, visual semantic embeddings \cite{faghri2018vsepp, song2019polysemous, chun2021pcme, chen2021learning} for COCO Caption \cite{cococaption} retrieval), these separately trained encoders only on the target dataset are easily overwhelmed by large-scale pre-trained models, \eg, CLIP \cite{radford2021clip}, ALIGN \cite{Jia_Yang_Xia_Chen_Parekh_Pham_Le_Sung_Li_Duerig_2021} or BLIP \cite{li2022blip}.

Baldrati \etal \cite{baldrati2022clip4cir} proposed CLIP4Cir, which consists of a first stage to fine-tune a CLIP text encoder and a second stage to train a late-fusion module called a Combiner, showed powerful performance. However, it still requires massive training triplets.
Saito \etal \cite{saito2023pic2word} proposed a zero-shot CIR model, named Pic2Word, trained on aligned image-text pairs without using the triplet dataset at all. Pic2Word projects a CLIP image feature into the text embedding space and then uses the concatenated feature of the converted image feature and the given text condition as input of the text encoder. Pic2Word achieves reasonable CIR performances without direct training on the expensive triplet datasets. Since both methods freeze and use the CLIP visual encoder during the training process, the gallery (the set of images targeted for retrieval) features can be used exactly as it was extracted from the CLIP visual encoder.
A detailed description of these models follows in \cref{subsec:baselines}.

All existing CIR models only focus on text conditions (\condtext) (\eg, the first row of \cref{fig:teaser}). Compared to the previous CIR approaches, our method enables multiple various conditions (\eg, the second to fourth rows of \cref{fig:teaser}), but strong zero-shot and fine-tuned performances by employing (1) a massive synthetic triplet dataset and (2) a latent diffusion model with classifier-free guidance.

\paragraph{Dataset creation with diffusion models.}
A conventional data collection process for \rawtriplet is two-staged: collecting candidate reference-target image pairs and manually annotating the modification sentences by human annotators \cite{berg2010automatic, han2017automatic, shoes, fashioniq, cirr}. For example, FashionIQ \cite{fashioniq} collects the candidate pairs from the same item category (\eg, shirt, dress, and top) and manually annotates the relative captions by crowd workers. CIRR \cite{cirr} gathers the candidate pairs from real-life images
from the NLVR$^2$ dataset \cite{suhr2018corpus}. The data collection process for \rawtriplet inevitably becomes expensive and it makes scale CIR datasets difficult. We mitigate this issue by generating a massive synthetic dataset instead of relying on human annotators.

Recently, there have been attempts to generate synthetic data for improving model performances \cite{brooks2022instructpix2pix, Nair_Bandara_Patel_2022, Shipard_Wiliem_Thanh_Xiang_Fookes_2023} by utilizing powerful generation ability by diffusion models \cite{ho2020denoising, rombach2022latentdiffusion, ho2022classifier}.
Unlike previous attempts to synthesize training data points by GAN \cite{choe2017face, lee2018training, sandfort2019data}, recent diffusion model-based approaches show high-quality images and high controllability by text prompts (\eg, by classifier-free guidance \cite{ho2022classifier} and latent diffusion \cite{rombach2022latentdiffusion}).
Particularly, Brooks \etal \cite{brooks2022instructpix2pix} proposed a generation process for \rawtriplet to train an image editing model.
We scale up the dataset synthesis process of Brooks \etal from 450K triplets to 18M. We also make the triplets more diverse by employing the object-level modification process. We describe the details of the InstructPix2Pix dataset generation and our modification of the generation process in the upcoming \cref{sec:cir_dataset_gen}.




\begin{figure*}[t]
    \centering
    \includegraphics[width=.98\linewidth]{figures/dataset_generation_overview.pdf}
    \caption{\small {\bf Overview of the generation process for \ourdataset.} \rawtriplet from \rawcaptriplet. SD denotes StableDiffusion.}
    \label{fig:dataset_generation_overview}
\end{figure*}

\section{\ourdataset: Massive High-Quality Synthesized CIR Dataset}
\label{sec:cir_dataset_gen}
CIR needs a dataset with triplets \rawtriplet of a reference image (\rawrefimg), a condition (\rawcond), and the corresponding target image (\rawtarimg). Instead of collecting a dataset by humans, we propose to generate massive triplet data points by using large generative models. We follow the main idea of Instuct Pix2Pix \cite{brooks2022instructpix2pix}. First, we generate \rawcaptriplet where \rawreftxt is a reference caption, \rawcondtext is a modification instruction text, and \rawtartxt is the modified caption by \rawcondtext. We employ two strategies to generate \rawcaptriplet: (1) We collect massive captions from the existing caption datasets and generate the modified captions by replacing the keywords in the reference caption. (2) We fine-tune a large language model, OPT-6.7B \cite{zhang2022opt}, on the generated caption triplets by Brooks \etal. After generating massive triplets of \rawcaptriplet, we generate images from the caption triplet using StableDiffusion \cite{rombach2022latentdiffusion} and Prompt-to-Prompt \cite{hertz2022prompttoprompt} following Brooks \etal \cite{brooks2022instructpix2pix}. We illustrate the overall generation process in \cref{fig:dataset_generation_overview}.

Compared to manual dataset collections \cite{fashioniq, cirr}, our approach can easily generate more diverse triplets even if a triplet rarely happens in reality (See the ``pak choi tart'' example in \cref{fig:dataset_generation_overview}). Compared to the synthetic dataset by Brooks \etal \cite{brooks2022instructpix2pix}, our dataset contains a larger number of triplets (450k vs. 18M). 
Furthermore, as our caption triplets are synthesized based on keywords, our synthetic captions cover more diverse keywords than Instuct Pix2Pix (47k vs. 586k as shown in \cref{tab:keyword_stats}).





\subsection{Keyword-based diverse caption generation}
As the first approach to generating caption triplets, we collect captions from the existing caption datasets and modify the captions by replacing the object terms in the captions, \eg, $\langle$``a strawberry tart is ...'', ``covert strawberry to pak choi'', ``a pak choi tart is ...''$\rangle$ in \cref{fig:dataset_generation_overview}. For the caption dataset, 
We use the captions from COYO 700M \cite{kakaobrain2022coyo-700m}, StableDiffusion Prompts\footnote{\url{https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts}} (user-generated prompts that make the quality of StableDiffusion better), LAION-2B-en-aesthetic (a subset of LAION-5B \cite{schuhmann2022laion}) and LAION-COCO datasets \cite{laioncoco} (synthetic captions for LAION-5B subsets with COCO style captions \cite{cococaption}. LAION-COCO less uses proper nouns than the real web texts).

\begin{table}[t]
\small
\centering
\begin{tabular}{lcc}
\toprule
  & IP2P \cite{brooks2022instructpix2pix} & \ourdataset \\ \midrule
\rawcaptriplet     & 450k & 60M \\
Unique object terms & 47,345 & 586,369 \\
\midrule
\rawtriplet ~~{\scriptsize (Keyword-based)} & - & 11.4M \\
\rawtriplet ~~{\scriptsize (LLM-based)} & 1M & 7.4M \\ \midrule
\rawtriplet ~~{\scriptsize (Total)}   & 1M & 18.8M \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf Statistics of generated datasets.} We compare our \ourdataset and the dataset by Instruct Pix2Pix (IP2P) in terms of the dataset statistics. \rawcaptriplet denotes the triplet of captions \{original caption, modification instruction, and modified caption\} and \rawtriplet denotes the triplet of \{original image, modification instruction, and modified image\}.
}
\label{tab:keyword_stats}
\end{table}

We extract the object terms from the captions using a part-of-speech (POS) tagger, provided by \texttt{Spacy}\footnote{\url{https://spacy.io/}}. After frequency filtering, we have 586k unique object terms. For each caption, we replace the object term with other similar keywords by using the CLIP similarity score. More specifically, we extract the textual feature of keywords using the CLIP ViT-L/14 text encoder, and we choose an alternative keyword from keywords that have a CLIP similarity between 0.5 and 0.7. By converting the original object to a similar object, we have caption pairs of \rawcappair.

Using the caption pair \rawcappair, we generate the modification instruction text \rawcondtext based on a randomly chosen template from 47 pre-defined templates
We show examples of templates in \cref{tab:example_templates} and the Appendix. After this process, we have the triplet of \rawcaptriplet. We generate $\approx$30M triplets by the keyword-based method.

\begin{table}[t]
\newcommand{\srctab}{\texttt{\$\{source\}}\xspace}
\newcommand{\trgtab}{\texttt{\$\{target\}}\xspace}
\small
\centering
\begin{tabular}{l}
\toprule
Templates to change \srctab to \trgtab \\ \midrule
\textit{``change \srctab to \trgtab''} \\
\textit{``alter \srctab to \trgtab''} \\
\textit{``\trgtab is added after \srctab is removed''} \\
\textit{``if it is \trgtab''} \\
$\ldots$\\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf Example keyword converting templates.} The full 47 templates are shown in \cref{subsec:keyword_templates_supp}.}
\label{tab:example_templates}
\end{table}

\subsection{Amplifying Instruct Pix2Pix triplets by LLM}
We also re-use the generated \rawcaptriplet by Instruct Pix2Pix. We amplify the number of Instruct Pix2Pix triplets by fine-tuning a large language model, OPT-6.7B \cite{zhang2022opt}, on the generated 452k caption triplets provided by Brook \etal. Using the fine-tuned OPT, we generate $\approx$30M triplets.

\subsection{Triplet generation from caption triplets}

We generate 60M caption triplets \rawcaptriplet by the keyword-based generation process and the LLM-based generation process (See \cref{subsec:dataset_stats_supp} for the statistics of the captions).
Using the triplets, we generate 60M \rawtriplet by using state-of-the-art text-to-image generation models. Note that two captions do not have a guarantee to be semantically similar. Following Instruct Pix2Pix \cite{brooks2022instructpix2pix}, we employ Prompt-to-Prompt \cite{hertz2022prompttoprompt}, which aims to generate similar images for multiple generations by sharing cross-attention weights during the denoising steps of diffusion models.

We employ multiple state-of-the-art text-to-image generation models, including StableDiffusion (SD) 1.5, SD 2.0, SD 2.1, and SD anime models to generate diverse images not biased towards a specific model. We apply the CLIP-based filtering following Brook \etal \cite{brooks2022instructpix2pix} to remove the low quality \rawtriplet (See \cref{subsec:filtering_supp} for details). After the filtering, we have 11.4M \rawtriplet from the keyword-based generated captions and 7.4M \rawtriplet from the LLM-based generated captions. It implies that the fidelity of our keyword-based method is higher than OPT fine-tuning in terms of text-to-image generation. As a result, our dataset contains 18.8M synthetic \rawtriplet.
We illustrate examples of \ourdataset in \cref{subsec:dataset_examples}.



















\section{Method}
\label{sec:method}

In this section, we introduce our \oursfull to solve CIR tasks. Thanks to the massive and diverse synthetic dataset (\cref{sec:cir_dataset_gen}), we can train a versatile model to handle various conditions based on a latent diffusion model \cite{rombach2022latentdiffusion}, \ie, the diffusion process is performed in the latent space, not the pixel space. We will describe the training and inference details and how \ours handles various conditions.




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/training_details.pdf}
    \caption{\small \textbf{Overview of our two-stage training strategy.} $e_t$ denotes the embedding vector for timestamp $t$. For stage 2, we illustrate an example of all three conditions given. In practice, we randomly drop each condition to make \ours handle various scenarios (\eg, text only, image-text composed query).}
    \label{fig:method_training_overview}
\end{figure}

\subsection{Training}
\label{subsec:training}
We employ a two-staged pre-training fine-tuning strategy.
In stage 1, we pre-train a transformer decoder to convert CLIP textual embeddings to CLIP visual embeddings with LAION 2B dataset \cite{schuhmann2022laion} consisting of (image, text) pairs. This stage is similar to training the prior model in Dalle-2 \cite{dall-e2}, but our model takes only two tokens; a noised CLIP image embedding and an embedding for the diffusion timestep. In the prior model \cite{dall-e2}, 77 encoded text embeddings using the CLIP are employed as the input of the model, which increased the computational cost. However, our CompoDiff uses the 77 encoded text embeddings as conditions through cross-attention mechanisms, which speeds up the process by three times while maintaining similar performance (See \cref{subsec:ablation_study_supp}). Instead of using the noise prediction \cite{ho2020denoising}, we observe it is more stable to train the transformer decoder to predict the denoised $x_i$ directly.

With CLIP image embeddings of an input image $z_i$, encoded CLIP text embeddings for text condition $z_{c_T}$, and the transformer $\epsilon_\theta$. The objective of the first stage is as follows:
\begin{equation}
\mathcal{L}_\text{stage1} = \mathbb{E}_{t\sim[1,T]}  \|z_i - \epsilon_\theta(z_i^{(t)},t \vert z_{c_T}) \|^2
\end{equation}

In stage 2, we fine-tune the model for composed image retrieval on \ourdataset. By stage 2, we make the model able to generate modified image embeddings based on the context of a reference image, incorporating an instruction text and a mask. We use the CLIP visual and textual encoders to encode a reference image and a text condition (we also tested other text encoders in \cref{subsec:impact_text_encoder_supp}).
We generate masks by using a zero-shot text-conditioned segmentation model, ClipSeg \cite{luddecke2022image}. We extract object terms from the given caption using a POS tagger, and we generate a segmentation map for each object term using ClipSeg.
The mask is resized to 64$\times$64 and projected to the same dimensional embedding as that of CLIP embeddings using an MLP. All conditions, including reference image embeddings, mask embeddings, and text embeddings are concatenated for the condition of the denoising Transformer.

We empirically observed that the knowledge from the LAION-2B dataset by stage 1 is easily forgotten during stage 2. To overcome the catastrophic forgetting, we introduce multi-task learning, with text-to-image learning and inpainting each accounting for 30\%, and the remaining 40\% of training done on the triplet dataset (\ourdataset). The datasets used for text-to-image and inpainting are based on the 2B-scale LAION dataset to maintain the textual representational capability of our model learned in stage 1.


Let us define CLIP image embeddings of a reference image as $z_{i_R}$, CLIP image embeddings of a modified target image embeddings as $z_{i_T}$, and mask embeddings as $z_{c_M}$. We minimize the following objective:
\begin{equation}
\mathcal{L}_\text{stage2} = \mathbb{E}_{t\sim[1,T]}  \|z_{i_T} - \epsilon_\theta(z_{i_T}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) \|^2.
\end{equation}
We illustrate the overview of our training scheme in \cref{fig:method_training_overview}.


\begin{figure}[t]
    \centering
    \includegraphics[width=.965\linewidth]{figures/cfg_variation.pdf}
    \caption{\small \textbf{Controlling image and text weights for inference.} We show example retrieval results on the LAION images by varying image and text weights, $w_I$ and $w_T$ in \cref{eq:cfg}, respectively}
    \label{fig:cfg_variation_images}
\end{figure}

\subsection{Inference}
\label{subsec:inference}
We apply Classifier-free guidance (CFG) \cite{ho2022classifier}, a commonly used technique in text-conditional image generation methods for better text conditioning, for \ours.
We randomly drop each condition in stage 1 and stage 2 with 10\% probability, except mask conditions. During inference, noisy image embeddings are denoised as follows:
\begin{equation}
\label{eq:cfg}
{\scriptsize
\begin{aligned}
\tilde{\epsilon}_\theta(&z_{i}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) = \epsilon_\theta(z_{i}^{(t)},t \vert \O, \O, z_{c_M}) \\
&+ w_I(\epsilon_\theta(z_{i}^{(t)},t \vert \O, z_{i_R}, z_{c_M}) -\epsilon_\theta(z_{i}^{(t)},t \vert \O, \O, z_{c_M})) \\
&+ w_T(\epsilon_\theta(z_{i}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) -\epsilon_\theta(z_{i}^{(t)},t \vert \O, z_{i_R}, z_{c_M}))
\end{aligned}
}
\end{equation}
We show the top-1 retrieved item by varying the image and text weights $w_I$ and $w_T$ from our LAION database in \cref{fig:cfg_variation_images}. By increasing $w_I$, our model behaves more like an image-to-image retrieval model. By increasing $w_T$, on the other hand, our model focuses more on the text condition ``pencil sketch''. We use ($w_I$, $w_T$) = (1.5, 7.5) for our experiments. The retrieval performances by varying $w_I$ and $w_T$ are shown in \cref{subsec:ablation_study_supp}.
We also describe how \cref{eq:cfg} is changed for negative texts in \cref{subsec:negative_text_supp}.

\section{Experiments}

This section describes the experimental results on the zero-shot and fine-tuned composed image retrieval tasks for \ours.
We compare \ours with the existing works on the standard CIR benchmarks (\eg, FashionIQ \cite{fashioniq} and CIRR \cite{cirr}) and zero-shot benchmarks (\eg, domain conversion and object compositional tasks, following \cite{saito2023pic2word}).
We also provide qualitative examples of \ours beyond performances, \eg, handling versatile CIR scenarios. Our ablation study shows that by controlling inference hyperparameters, \ours shows trade-offs in various scenarios (\eg, performance vs. inference speed or focusing text more vs. image more).
We also conducted experiments on the strengths of \ours compared to other feature transformation models using diffusion models such as UnCLIP \cite{dall-e2}.
Unless otherwise noted, the model we used was trained with the training process described in \cref{sec:method} (See \cref{subsec:implementation_details_supp} for the detailed hyperparameter settings). We additionally show the advanced version of \ours by replacing the text encoder for text conditions (see a stage 2 training example in \cref{fig:method_training_overview} for details) from the CLIP textual encoder to the combination of the CLIP textual encoder and T5-XL \cite{t5}. Details can be found in \cref{subsec:impact_text_encoder_supp}.

\subsection{Comparision with the State-of-the-Art}

\paragraph{Comparison methods.}
\label{subsec:baselines}
We compare \ours with three state-of-the-art CIR methods as follows:

\begin{itemize}[nosep]
\item \textbf{CLIP4Cir} \cite{baldrati2022clip4cir}, also known as Combiner, involves a two-stage training process. First, the CLIP text encoder is fine-tuned by contrastive learning of \cond and \refimg $+$ \cond in the CLIP embedding space. The second stage replaces \refimg $+$ \cond to the learnable Combiner module. Only the Combiner module is trained during the second stage. Hence, its image embedding space is the same as the original CLIP space as \ours.
\item \textbf{ARTEMIS} \cite{delmas2022artemis} optimizes two similarities simultaneously. The implicit similarity is computed between the combined feature of \tarimg and \cond, and the combined one of \refimg and \cond. The explicit matching is computed between \tarimg and \cond. ARTEMIS suffers from the same drawback as previous CIR methods, \eg TIRG \cite{vo2019tirg}: As it should compute combined feature of \tarimg and \cond, it is not feasible to use an approximate nearest neighbor search algorithm, such as FAISS \cite{faiss}. This is not a big problem in a small dataset like FashionIQ, but it makes ARTEMIS infeasible in real-world CIR scenarios, \eg, the entire LAION-5B dataset is the target database.
\item \textbf{Pic2Word} \cite{saito2023pic2word} projects a visual feature into text embedding space, instead of combining them. Pic2Word performs a text-to-image retrieval by using the concatenated feature as the input of the CLIP textual encoder. As the projection module is solely trained on cheap paired datasets without expensive triplet datasets, it is able to solve CIR in a zero-shot manner.
\end{itemize}
For the qualitative comparison, we re-train Pic2Word on LAION-2B-en \cite{schuhmann2022laion} because their pre-trained weight is not publically available. We also demonstrate the effectiveness of our synthetic dataset by training the comparison methods on \ourdataset with the default hyperparameters.

\paragraph{FashionIQ.}
\input{tables/fashion_iq_main}
FashionIQ, the most popular CIR benchmark, has (46.6k / 15.5k / 15.5k) (training / validation / test) images with three fashion categories: Shirt, Dress, and Toptee.
Each category has 18k training triplets and 12k evaluation triplets of \rawtriplet.
\cref{tab:fashioniq} shows the comparison of \ours with baselines.
Following the standard choice, we use recall@K as the evaluation metric. ``Zero-shot'' means that the models are not trained on FashionIQ. ARTEMIS and CLIP4Cir are originally designed for the supervised setting, but, we trained them on our synthetic dataset for a fair comparison with our method. Namely, we solely train them on our dataset for the zero-shot benchmark and fine-tune the zero-shot weights on the FashionIQ training set for the supervised benchmark.
As the result show, \ours achieves a new state-of-the-art zero-shot performance and performes competitively in a supervised manner.


\paragraph{CIRR.}
\input{tables/cirr_main}
As FashionIQ is fashion domain-specific, we also compare the methods on more generic images in CIRR. CIRR has 36k open-domain triplets divided into the train, validation, and test sets in 8:1:1 split.
\cref{tab:cirr} shows the results, and all experimental settings were identical to FashionIQ.
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ours_vs_pic2word.pdf}
    \caption{\small {\bf Qualitative comparison of zero-shot CIR for Pic2Word and \ours.} We conduct CIR on LAION with two composed query scenarios. As Pic2Word cannot take a simple instruction, we made a simple modification for the given instruction as shown. Pic2Word inserts the image-converted text embedding at the position of * in the original text embedding.}
    \label{fig:ours_vs_pic2word}
\end{figure*}
Similar to FashionIQ, \ours also achieves a new state-of-the-art CIRR zero-shot performance.
It is noteworthy that CLIP4Cir performs great in the supervised setting but performs worse than \ours in the zero-shot setting. We presume that the fine-tuned CLIP4Cir text encoder is overfitted to long-tailed CIRR captions. It is partially supported by our additional experiments using the combination of CLIP encoder and T5-XL text encoder \cite{t5}; a better understanding of complex texts provides better performances (Details and more results are in \cref{subsec:impact_text_encoder_supp}).


For both datasets, we achieve state-of-the-art by fine-tuning the CLIP4Cir model trained on our dataset to the target dataset. It shows the benefits of our dataset compared to the limited number of CIR triplet datasets.

\paragraph{Additional tasks.}
We also conduct additional zero-shot evaluation results following Saito \etal \cite{saito2023pic2word}. We report the results of a domain conversation task on ImageNet-R \cite{imagenet-r} (\cref{subsec:domain_conversion_supp}), and an object compositional task on MS-COCO \cite{lin2014coco} (\cref{subsec:object_conversion_supp}). In summary, \ours outperforms Pic2Word on ImageNet-R, but not on COCO. Detailed discussions are in \cref{sec:more_experiments_supp}.

\subsection{Qualitative examples}
\label{subsec:examples}
We qualitatively show the versatility of \ours for handling various conditions. For example, \ours not only can handle a text condition, but it can also handle a \textit{negative} text condition (\eg, removing specific objects or patterns in the retrieval results), masked text condition (\eg, specifying the area for applying the text condition). \ours even can handle all conditions simultaneously (\eg, handling positive and negative text conditions with a partly masked reference image at the same time). To show the quality of the retrieval results, we conduct a zero-shot CIR on entire LAION dataset \cite{schuhmann2022laion} using FAISS \cite{faiss} for simulating billion-scale CIR scenarios.
We show an example in \cref{fig:teaser} and more examples are shown in \cref{subsec:more_examples_supp}.

\cref{fig:ours_vs_pic2word} shows qualitative examples of zero-shot CIR results by Pic2Word and \ours. \ours results in semantically high-quality retrieval results (\eg, understanding the ``crowdedness'' of the query image and the meaning of the query text at the same time). However, Pic2Word shows poor understanding of the given queries, resulting in unfortunate retrieval results (\eg, ignoring ``grown up'' of text query, or the  ``crowdedness'' of the query image). We provide the details and more examples in \cref{subsec:more_examples_supp}.

Finally, it is worth noting that \ours generates a feature belonging to the CLIP space.
It means that we can apply the unCLIP generator \cite{dall-e2} on our composed features. We compare the retrieval results from the LAION dataset and the generated images in \cref{subsec:unclip_supp}. \ours can manipulate the given input reflecting the given conditions.



\subsection{Ablation Studies}
We provide ablation studies for our design choices, including the architecture design, the trade-off between the denoising step (\ie, inference time) and performances, and the retrieval performances by varying the image and text weights ($w_I$ and $w_T$). In summary, our design choice shows $\times$ 3 faster inference time than the prior model \cite{dall-e2} but better text-to-image retrieval performances on COCO. Also, we observe that \ours performs better by increasing the de-noising step, but empirically shows good performances with the de-noising step size 5.
Due to the page limit, the full results can be found in \cref{subsec:ablation_study_supp}.








\section{Conclusion}
In this paper, we have introduced \oursfull, a novel method for solving complex composed image retrieval (CIR) tasks. We have created a large and diverse dataset named \ourdataset, consisting of 18M triplets of images, modification texts, and modified images. Our model has demonstrated impressive zero-shot CIR capabilities, as well as remarkable versatility in handling diverse conditions, such as negative text or image masks. Additionally, by pre-training previous state-of-the-art CLIP4Cir on \ourdataset and fine-tuning it on each target dataset, we have achieved state-of-the-art performance in both FashionIQ and CIRR benchmarks. We strongly encourage future researchers to leverage our dataset for advancing the field of CIR.


\section*{Societal Impact}

Our work is primarily focused on solving complex composed image retrieval (CIR) challenges and is not designed for image editing purposes. However, we are aware that with the use of additional public resources, such as the community version of the unCLIP feature decoder \cite{dall-e2}, our method can potentially be utilized as an image editing method. We would like to emphasize that this unintended application is not the primary objective of our research, and we cannot guarantee the effectiveness or safety of our method in this context.

It is important to note that we have taken steps to mitigate potential risks associated with the unintended use of our method for image editing. For instance, we applied NSFW filters to filter out potentially malicious samples during the creation of \ourdataset. Nevertheless, we recognize the need for continued research into the ethical and societal implications of AI technologies and pledge to remain vigilant about potential unintended consequences of our work.


\input{appendix}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{reference}
}

\end{document}