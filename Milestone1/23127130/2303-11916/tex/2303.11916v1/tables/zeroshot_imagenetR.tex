\begin{table*}[t]
\small
\centering
\begin{tabular}{l|cc|cc|cc|cc|cc}
\toprule
& \multicolumn{2}{c}{Cartoon} & \multicolumn{2}{c}{Origami} & \multicolumn{2}{c}{Toy} & \multicolumn{2}{c}{Sculpture} & \multicolumn{2}{c}{Average} \\
\midrule
& R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 \\
\midrule
Pic2Word \cite{saito2023pic2word} (reported) & 8.0 & 21.9 & 13.5 & 25.6 & 8.7 & 21.6 & 10.0 & 23.8 & 10.1 & 23.2 \\
Pic2Word (CC-3M \cite{sharma2018conceptual}) & 7.35 & 18.53 & 12.79 & 25.54 & 10.39 & 22.96 & 10.24 & 23.76 & 10.19 & 22.70 \\
Pic2Word (LAION 2B-en \cite{schuhmann2022laion}) & 8.17 & 20.86 & 14.08 & 25.06 & 8.73 & 22.07 & 10.43 & 23.63 & \textbf{10.35} & \textbf{22.91} \\ \midrule
ARTEMIS \cite{delmas2022artemis} w/ our dataset & 11.42 & 23.81 & 15.49 & 25.44 & 11.21 & 24.01 & 10.84 & 21.07 & 12.24 & 23.58 \\
CLIP4Cir \cite{baldrati2022clip4cir} w/ our dataset & 10.90 & 24.12 & 16.08 & 25.60 & 11.01 & 23.57 & 10.45 & 21.86 & 12.11 & 23.79 \\ \midrule
CompoDiff (T5-XL) & 8.43 & 20.40 & 15.73 & 25.69 & 11.19 & 22.48 & 9.19 & 18.45 & 11.14 & 21.76 \\
CompoDiff (CLIP, T5-XL) & 12.91 & 24.40 & 17.22 & 26.40 & 11.57 & 26.11 & 11.53 & 22.54 & \textbf{13.31} & \textbf{24.86} \\
CompoDiff (CLIP) & 13.21 & 24.06 & 17.03 & 26.17 & 11.22 & 26.25 & 11.24 & 22.96 & 13.18 & 24.86 \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small \textbf{Domain conversion task on ImageNet-R.} All numbers, except Pic2Word (reported), are reproduced numbers. The first three rows show the effect of different training datasets. We also report zero-shot results of ARTEMIS and CLIP4Cir trained by our dataset in the fourth and fifth rows.}
\label{tab:zeroshot_imagenetR}
\end{table*}