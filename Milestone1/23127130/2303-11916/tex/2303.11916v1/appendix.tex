
\appendix
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

\section*{Appendix}

In this additional document,
we describe the details of our data generation process in \cref{sec:cir_dataset_gen_supp}, including the statistics of our generated dataset (\cref{subsec:dataset_stats_supp}), the full keyword converting templates (\cref{subsec:keyword_templates_supp}), the dataset filtering process (\cref{subsec:filtering_supp}) and examples of \ourdataset (\cref{subsec:dataset_examples}). We also provide more details for \ours in \cref{sec:more_method_details_supp}, including how to handle negative texts (\cref{subsec:negative_text_supp}) and denoising Transformer details (\cref{subsec:denoising_transformer_supp}). \cref{sec:exp_details_supp} contains more experimental details of \ours, such as the implementation details (\cref{subsec:implementation_details_supp}) and the details of LAION dataset for retrieval (\cref{subsec:laiondataset_supp}). Finally, we provide more experimental results in \cref{sec:more_experiments_supp}, including the impact of the text encoder (\cref{subsec:impact_text_encoder_supp}), the domain conversion task (\cref{subsec:domain_conversion_supp}), the object compositional task (\cref{subsec:object_conversion_supp}), more qualitative examples (\cref{subsec:more_examples_supp}), ablation study (\cref{subsec:ablation_study_supp}) and generation examples by using the unCLIP generator (\cref{subsec:unclip_supp}).

\section{More Details for \ourdataset}
\label{sec:cir_dataset_gen_supp}

\subsection{Dataset Statistics}
\label{subsec:dataset_stats_supp}

We show the statistics of our generated caption dataset (\ie, before text-to-image generation, \rawreftxt and \rawtartxt). We use the CLIP tokenizer to measure the statistics of the captions. \cref{fig:data_stats_supp} shows the cumulative ratio of captions with tokens less than X. About half of the captions have less than 13 tokens, and 90\% of the captions have less than 20 tokens. Only 0.8\% of the captions have more than 40 tokens.

We also compare our dataset, FashionIQ \cite{fashioniq} and CIRR \cite{cirr} in terms of the token statistics of instructions (\ie, \rawcond). \cref{fig:instruction_stats_supp} shows that our dataset has relatively shorter instructions than other human-annotated instructions. We presume that this is why \ours performs better when fine-tuning on the target dataset.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/compodiff_data_stats.pdf}
    \caption{\small {\bf Statistics of our captions.} We show the population of our captions by the number of tokens per caption. We include captions having larger than 40 tokens in ``40+''.}
    \label{fig:data_stats_supp}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/instruction_data_stats.pdf}
    \caption{\small {\bf Statistics of instructions for three CIR datasets.} We show the population of instruction captions (\eg, ``change A to B'') by the number of tokens. We include captions having larger than 60 tokens in ``60''.}
    \label{fig:instruction_stats_supp}
\end{figure}


\subsection{The Full Keyword Converting Templates}
\label{subsec:keyword_templates_supp}

\cref{tab:example_templates_supp} shows the 47 templates to convert keywords.

\begin{table}[ht!]
\newcommand{\srctab}{\texttt{\$\{source\}}\xspace}
\newcommand{\trgtab}{\texttt{\$\{target\}}\xspace}
\footnotesize
\centering
\begin{tabular}{l}
\toprule
Templates to change \srctab to \trgtab \\ \midrule
\textit{``replace \srctab with \trgtab''} \\
\textit{``substitute \trgtab for \srctab''} \\
\textit{``change \srctab to \trgtab''} \\
\textit{``\trgtab''} \\
\textit{``apply \trgtab''} \\
\textit{``add \trgtab''} \\
\textit{``exchange \srctab with \trgtab''} \\
\textit{``alter \srctab to \trgtab''} \\
\textit{``convert \srctab to \trgtab''} \\
\textit{``transform \srctab into \trgtab''} \\
\textit{``swap \srctab for \trgtab''} \\
\textit{``replace \srctab with \trgtab''} \\
\textit{``remodel \srctab into \trgtab''} \\
\textit{``redesign \srctab as \trgtab''} \\
\textit{``update \srctab to \trgtab''} \\
\textit{``revamp \srctab into \trgtab''} \\
\textit{``if it is \trgtab''} \\
\textit{``substitute \trgtab for \srctab''} \\
\textit{``modify \srctab to become \trgtab''} \\
\textit{``turn \srctab into \trgtab''} \\
\textit{``alter \srctab to match \trgtab''} \\
\textit{``customize \srctab to become \trgtab''} \\
\textit{``adapt \srctab to fit \trgtab''} \\
\textit{``upgrade \srctab to \trgtab''} \\
\textit{``change \srctab to match \trgtab''} \\
\textit{``tweak \srctab to become \trgtab''} \\
\textit{``amend \srctab to fit \trgtab''} \\
\textit{``\trgtab is the new option''} \\
\textit{``choose \trgtab instead''} \\
\textit{``\trgtab is the updated version''} \\
\textit{``use \trgtab from now on''} \\
\textit{``\trgtab is the new choice''} \\
\textit{``opt for \trgtab''} \\
\textit{``\trgtab is the updated option''} \\
\textit{``\trgtab is the new selection''} \\
\textit{``\trgtab is the new option available''} \\
\textit{``\trgtab is the updated choice''} \\
\textit{``\srctab is replaced with \trgtab''} \\
\textit{``\srctab is removed and \trgtab is added''} \\
\textit{``\trgtab is introduced after \srctab is removed''} \\
\textit{``\srctab is removed and \trgtab takes its place''} \\
\textit{``\trgtab is added after \srctab is removed''} \\
\textit{``\srctab is removed and \trgtab is introduced''} \\
\textit{``\trgtab is added in place of \srctab''} \\
\textit{``\trgtab is introduced after \srctab is retired''} \\
\textit{``\trgtab is added as a replacement for \srctab''} \\
\textit{``\trgtab is introduced as the new option after}\\
~~\textit{\srctab is removed''} \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf The full 47 keyword converting templates.}}
\label{tab:example_templates_supp}
\end{table}

\subsection{Filtering process}
\label{subsec:filtering_supp}
In the first iteration of our generation process, we generated approximately 60 million triplet images. Then, we apply a CLIP-based filter to filter out low-quality triplets.
We filter the generated images for an image-image CLIP threshold of 0.70 to ensure that the images are not too different, an image-caption CLIP threshold of 0.2 to ensure that the images correspond to their captions, and a directional CLIP similarity of 0.2 to ensure that the change in before/after captions correspond with the change in before/after images. Additionally, in the case of keyword-based data generation, we filter out for a keyword-image CLIP threshold of 0.20 to ensure that images contain the context of the keyword, and in the case of instruction-based data generation, we filter out for an instruction-modified image CLIP threshold of 0.20 to ensure that an image is consistent with the given instructions.

\subsection{More examples of \ourdataset}
\label{subsec:dataset_examples}

We illustrate examples of \ourdataset in \cref{fig:dataset_examples}. Our dataset can express the change of overall context (\eg, ``make the landscape a cityscape''), the seasonal change (\eg, ``make it sprint''), the change of mood (\eg, ``make it a watercolor''), and the change of local objects (\eg, ``have the person be a dog'').

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/dataset_examples.pdf}
    \caption{\small {\bf Examples of \ourdataset.} We show examples of \rawtriplet, \ie, \{original image, modification instruction, and modified image\}, as well as the generation prompt for \rawrefimg and \rawtarimg.}
    \label{fig:dataset_examples}
\end{figure*}

\section{More Details for \ours}
\label{sec:more_method_details_supp}

\subsection{Details for negative text}
\label{subsec:negative_text_supp}
\ours employs a diffusion probabilistic model, allowing us to leverage the advantages of the diffusion model as described in \cref{subsec:inference} (Classifier-free guidance).
Let us define the null embeddings for a text as $\O_{c_T}$ and for an image as $\O_{i_R}$.
Then, we can rewrite \cref{eq:cfg} as follows:

\begin{equation}
\label{eq:cfg_with_null_embeddings}
{\scriptsize
\begin{aligned}
\tilde{\epsilon}_\theta(&z_{i}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) = \epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, \O_{i_R}, z_{c_M}) \\
&+ w_I(\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, z_{i_R}, z_{c_M}) -\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, \O_{i_R}, z_{c_M})) \\
&+ w_T(\epsilon_\theta(z_{i}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) -\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, z_{i_R}, z_{c_M}))
\end{aligned}
}
\end{equation}

For $\O_{c_T}$, CLIP textual ebmeddings for empty text (``"") is used and we employ an all-zero vector for $\O_{i_R}$. When a negative text is employed, we simply replace $\O_{i_R}$ with the CLIP textual embeddings $c_{T^-}$ for the negative text.


\subsection{Denoising Transformer implementation details}
\label{subsec:denoising_transformer_supp}
Instead of using the denoising U-Net \cite{rombach2022latentdiffusion}, we employ a simple transformer architecture for the denoising procedure. We empirically observe that our transformer architecture works slightly better than the U-Net architecture, but is a lot simpler. Since the exhaustive architecture search is expensive, we did not test various alternatives, but we fix our denoising module as a Transformer.
We use the multi-head self-attention blocks as the original Transformer \cite{vaswani2017attention}, where the depth, the number of heads, and the dimensionality of each head are set to 12, 16, and 64, respectively. 

We use two inputs as the input of the denoising Transformer: a noisy visual embedding and a time step embedding. The conditions (\eg, text conditions, mask conditions, image conditions) are applied only to the cross-attention layer of the Transformer, so it is computationally efficient even if the number of conditions becomes larger. Our implementation is similar to the ``DiT block with cross-attention'' by Peebles \etal \cite{peebles2022scalable}, but our implementation handles much various conditions, such as text conditions, mask conditions, and image conditions.

\section{More Experimental Details}
\label{sec:exp_details_supp}
\subsection{Implementation details}
\label{subsec:implementation_details_supp}
\input{tables/parameters}
We report the detailed hyperparameters in \cref{tab:parameters}. All models were trained using AdamW \cite{loshchilov2017decoupled} with $\beta_1=0.9$ and $\beta_2=0.999$. For computationally efficient training, we precomputed CLIP visual embeddings of the entire image from our training dataset. Since our training dataset was sufficiently large, we did not use any image augmentation methods to extract CLIP visual embeddings. Since the text as a condition can vary each time in training according to the 47 templates (\cref{tab:example_templates_supp}), we do not precompute any textual embeddings. In the case of keyword-based generated triplets, we are able to randomly switch query and modified images during training because the instruction for keyword-based triplets is generated according to the 47 templates.

\subsection{LAION dataset for retrieval}
\label{subsec:laiondataset_supp}
We employed all images from the LAION 5B \cite{schuhmann2022laion} dataset to build a search index. We use the 2B English caption subset of LAION for training stages.
We use the full LAION-5B images for the qualitative evaluation of zero-shot CIR in the later section.

\section{More Experimental Results}
\label{sec:more_experiments_supp}

\subsection{Impact of text encoder}
\label{subsec:impact_text_encoder_supp}
\input{tables/improved_text_encoder}
As shown in Balaji \etal \cite{balaji2022ediffi}, using a text-oriented model such as T5 \cite{raffel2020t5} in addition to the CLIP textual encoder results in improved performance of text-to-image generation models. Motivated by this observation, we also use both the CLIP textual encoder and the language-oriented encoder. We also observed the positive effect of the text-oriented model and experimental results showed that T5-XL, which has 3B parameters, could improve the performance by a large margin in the overall evaluation metrics. As described in \cref{subsec:implementation_details_supp}, all training text embeddings are extracted at every iteration. To improve computational efficiency, we reduced the number of input tokens of the T5 models to 77, as in CLIP (as shown in \cref{fig:data_stats_supp} and \cref{fig:instruction_stats_supp}, most of the captions in our dataset have lengths less than 77).

We compare different text encoder choices on four different tasks (ImageNet-R and COCO for domain conversion tasks -- see \cref{subsec:domain_conversion_supp} and \cref{subsec:object_conversion_supp} for details -- and Fashion IQ and CIRR datasets for CIR tasks) with zero-shot and fine-tuned scenarios. We report all experimental results in \cref{tab:improved_text_encoder}. When using the CLIP textual encoder and the T5-XL were used together, the experimental results improved by a large margin. We suspect that this is because the strong T5 encoder can help the CLIP text encoder to better understand given captions. Interestingly, we observe that using T5 alone degrades the performance even compared to using the CLIP textual encoder alone. We suspect that this is because T5-XL is specified for long text sequences (\eg, larger than 100 tokens) and text-only data. On the other hand, our caption dataset has an extremely short average length (see \cref{fig:data_stats_supp} and \cref{fig:instruction_stats_supp}), which is not specialized by T5. Also, our dataset is based on captions, paired with an image; we also need to consider image information to understand the given caption, but we cannot handle image information alone with T5.

\subsection{Domain conversion task}
\label{subsec:domain_conversion_supp}
\input{tables/zeroshot_imagenetR}
Following Pic2Word \cite{saito2023pic2word}, we evaluate the ability to compose domain information. We use ImageNet \cite{russakovsky2015imagenet} as reference images and ImageNet-R \cite{imagenet-r} as the target images. Domains of ImageNet-R (cartoon, origami, toy, and sculpture) are used as text conditions with simple prompt engineering, \eg, we use ``as a cartoon, drawing'', ``as an origami'', ``as a toy, plastic model'', and ``as a sculpture'' for text conditions. As described in \cref{tab:zeroshot_imagenetR}, \ours shows the best results in the ImageNet-R benchmark. We also observe that by training models on our dataset, the zero-shot ARTEMIS \cite{delmas2022artemis} and CLIP4Cir \cite{baldrati2022clip4cir} show competitive results with our method.

Note that we have reported three different results for Pic2Word. Pic2Word (reported) is the result of the original Pic2Word paper \cite{saito2023pic2word}. We reproduce Pic2Word by using the ConceptualCaption-3M dataset \cite{sharma2018conceptual} and the LAION 2B-en dataset. Despite increasing the size of the dataset from 3M to 2B, we do not observe significant performance changes between CC-3M and LAION-trained Pic2Word.

\subsection{Object compositional task}
\label{subsec:object_conversion_supp}

\input{tables/zeroshot_COCO}
This section focuses on assessing the capability to generate an instance by providing an image and additional textual descriptions of other scenes or objects. Following Pic2Word \cite{saito2023pic2word}, we randomly select and crop an object from the image of the COCO validation set \cite{lin2014coco} and apply its instance mask to remove the background. The text specification is based on the list of object classes present in the image, which is then combined into a single sentence using commas. Since the experimental protocol requires a random selection of objects, we repeated the experiment five times and reported full results in \cref{tab:zeroshot_COCO}. Here, we use the average of CLIP visual embeddings and textual embeddings (\textbf{Image + Text}) as a baseline. Interestingly, we observe that the simple \textbf{Image + Text} approach shows the best performance out of all zero-shot approaches, including Pic2Word and \ours. This could be due to two reasons. First, our dataset does not contain multiple changes or multiple objects in a single text guidance \cref{sec:cir_dataset_gen}.
This may limit the performance of \ours limited. Second, the random object selection process could select very small objects with high probability (almost half of the objects are smaller than 1\% of the full image \cite{openimages}), which can make the edited images noisy. As shown in the table, we observe that the variances of the benchmark are not negligible; it can show the noisiness of the object compositional task by Pic2Word \cite{saito2023pic2word}.

\subsection{More qualitative examples}
\label{subsec:more_examples_supp}

\paragraph{Open world zero-shot CIR comparisons with Pic2Word.} We illustrate further comparisons with Pic2Word in \cref{fig:ours_vs_pic2word_more_supp}. Here, we can draw the same conclusions as in the main text: Pic2Word often cannot understand images or instructions (\eg, ignores the ``crowdedness'' of the images, or retrieves irrelevant images such as images with a woman in the last example). All retrieved results in our paper were obtained using Pic2Word trained on the LAION 2B-en dataset (\ie, Pic2Word LAION 2B-en in \cref{tab:zeroshot_imagenetR}).

\paragraph{More versatile CIR examples on LAION.}
We illustrate more qualitative examples in \cref{fig:more_examples_supp_0}, \cref{fig:more_examples_supp_1}, \cref{fig:more_examples_supp_2}, and \cref{fig:more_examples_supp_3}. We will describe the details of ``Generated by unCLIP'' in the later section.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ours_vs_pic2word_more.pdf}
    \caption{\small {\bf More qualitative comparison of zero-shot CIR for Pic2Word and CompoDiff.}}
    \label{fig:ours_vs_pic2word_more_supp}
\end{figure*}


\subsection{Ablation study}
\label{subsec:ablation_study_supp}

\paragraph{How to handle text input?}
\input{tables/i2t_retrieval}
\ours does not take CLIP textual embeddings for text guidance as input tokens of the denoising Transformer but as a condition of cross-attention. Our design choice allows for faster throughput compared to the counterpart that takes CLIP textural embeddings directly as input tokens. We compare the impact of different design choices for handling textual embeddings. First, we evaluate the ``Prior'' model which converts CLIP textual embeddings into CLIP visual embeddings and proposed in unCLIP \cite{dall-e2} (we use a public community model \footnote{\url{https://huggingface.co/kakaobrain/karlo-v1-alpha}} because the official model is not yet publically available). Second, we test the ``Prior-like'' model by using the denoising Transformer, but taking text guidance as input tokens instead of cross-attention. We also test two more \ours models from our two-stage training strategy.

To measure the ability to understand raw text, we evaluate the models on image-to-text and text-to-image retrieval benchmarks on the MS-COCO Caption dataset \cite{cococaption}. We also evaluate them on the extension of COCO Caption to mitigate the false negative problem of COCO, namely, CxC \cite{cxc} and ECCV Caption \cite{chun2022eccv_caption}. \cref{tab:i2t_retrieval} shows the average metrics of each benchmark for image-to-text and text-to-image. In the table, we first observe that our design choice is three times faster than the ``Prior-ish'' counterparts by handling textual embeddings with cross-attention. Second, we observe that Stage 1 only \ours shows a better understanding of image-to-caption and caption-to-image retrieval tasks. We speculate that this is because Ours (Stage 1 only) is directly optimized by the image-to-text (ITM) matching style dataset, while Ours (Stage 1 + Stage 2) is also trained with other types of conditions (\eg, masks, negative texts, image conditions).


\paragraph{The number of denoising steps.}
\input{tables/ablation_results_per_steps}
Since our CompoDiff is a diffusion model, denoising steps are required to denoise noisy image embeddings. However, it is possible to obtain reliable denoised image embeddings with just a few steps. As shown in \cref{tab:denoising_steps}, we conducted a CIR evaluation on FashionIQ and it shows that even with only 5 iterations, our model can produce competitive results. If we use 100 steps, we have a slightly better performance (45.83 vs. 45.03 in the supervised scenario), but a much slower inference time (2.02 sec vs. 0.12 sec). Therefore, we can control the number of steps to improve the quality of the retrieved images and the inference time.

\paragraph{Condition strength}
\begin{figure}[t]
    \centering
    \subfigure[Zero-shot results]{\label{fig:zero-shot-results}\includegraphics[width=0.5\linewidth]{figures/heatmap_zeroshot.png}}%
    \hfill
    \subfigure[Supervised results]{\label{fig:supervised-results}\includegraphics[width=0.5\linewidth]{figures/heatmap_finetune.png}}%
    \caption{\small {\bf Fashion IQ CIR results by controlling $w_T$ and $w_I$.} Red denotes higher scores and blue denotes lower scores.}
    \label{fig:heatmaps}
\end{figure}


As $w_I$ increases, the generated image embeddings become more dependent on the reference image, while increasing $w_T$ results in a greater influence of the text guidance. However, high-valued $w_I$ and $w_T$ are not always beneficial. If $w_I$ or $w_T$ is too high, it can lead to unwanted results. To find a harmonious combination of $w_I$ and $w_T$, we performed a sweeping process as shown in \cref{fig:heatmaps}. We use $w_I$ as 1.5 and $w_T$ as 7.5 considering the best content-condition trade-off.

\paragraph{Data scale vs. performances.}
\input{tables/ablation_dataset_scale}
In this section, we performed stage 2 training while changing the size of the dataset by randomly sampling the 18.8M generated dataset. The results are shown in \cref{tab:dataset_scale}. First, at a scale of 1M, the performance of CompoDiff trained on our 1M subset significantly outperformed that publicly provided by the authors of InstructPix2Pix \cite{brooks2022instructpix2pix}. This result indicates that our dataset has a more diverse representation capability. As the size of our dataset increases, the performance gradually improves, and the overall performance is the best when using the entire dataset (18.8M).

\paragraph{Stage2 ablation.}
\input{tables/ablation_stage2}
The reason for training with T2I retrieval and masked CIR together in stage 2 is not only to provide these features to users but also to serve as multi-task learners that enhance the representational ability of standard CIR. Although we generate triplets for CIR up to 18.8M scale it is clear that our generated dataset still lacks textual and visual representational ability compared to the LAION dataset, which has a scale of billions. In order to include the LAION dataset in stage 2 training, we use the LAION dataset for T2I retrieval and masked CIR. Experimental results between stage 2 using only 18.8M generated dataset and the LAION dataset together are shown in \cref{tab:ablation_stage2}. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example_4.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours.} Using the transformed image feature by \ours, we show a generated image using unCLIP \cite{dall-e2} and top-1 retrieved image from LAION.}
    \label{fig:more_examples_supp_0}
\end{figure*}

\subsection{Image decoding using unCLIP generator.}
\label{subsec:unclip_supp}
unCLIP \cite{dall-e2} consists of a prior module that converts text embeddings into image embeddings, a decoder that converts image embeddings into low-resolution images, and super-resolution models that convert low-resolution images into high-resolution images. As the official unCLIP model is not publicly available, we employ the community version of unCLIP. Fortunately, since the community unCLIP model uses embeddings from CLIP-L/14, we can directly use this model to generate images from the image embeddings generated by our CompoDiff. To do this, we simply replace Prior with CompoDiff. The generated images are shown in \cref{fig:more_examples_supp_0}, \ref{fig:more_examples_supp_1}, \ref{fig:more_examples_supp_2}, and \ref{fig:more_examples_supp_3}. To clarify, the unCLIP model is trained for \textbf{text-to-image} generation, not to edit input images and our CompoDiff generates image embeddings rather than generating images. As shown, the results are very promising. It seems that incorporating unCLIP into the search service could potentially improve the user experience by generating images when the desired search results are not available.


\clearpage

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours (Continue).}}
    \label{fig:more_examples_supp_1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example_2.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours (Continue).}}
    \label{fig:more_examples_supp_2}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example_3.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours (Continue).}}
    \label{fig:more_examples_supp_3}
\end{figure*}

\clearpage