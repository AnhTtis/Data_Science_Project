
\documentclass{article} %
\usepackage{iclr2024_arxiv,times}

\input{math_commands.tex}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,linkcolor=blue,citecolor=blue,bookmarks=false]{hyperref}
\usepackage{url}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{xspace}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage[capitalize]{cleveref}

\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\Eg}{\textit{E.g.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\Ie}{\textit{I.e.}\xspace}
\newcommand{\etal}{\textit{et al.}\xspace}

\newcommand{\oursfull}{Composed Image Retrieval with Latent Diffusion (\ours)\xspace}
\newcommand{\ours}{\mbox{CompoDiff}\xspace}
\newcommand{\ourdataset}{SynthTriplets18M\xspace}

\newcommand{\rawrefimg}{\ensuremath{x_{i_R}}\xspace}
\newcommand{\rawcond}{\ensuremath{x_{c}}\xspace}
\newcommand{\rawcondtext}{\ensuremath{x_{c_T}}\xspace}
\newcommand{\rawcondnegtext}{\ensuremath{x_{c_{T^{\text{-}}}}}\xspace}
\newcommand{\rawcondmask}{\ensuremath{x_{c_M}}\xspace}
\newcommand{\rawtarimg}{\ensuremath{x_i}\xspace}
\newcommand{\rawtriplet}{\ensuremath{\langle \rawrefimg, \rawcond, \rawtarimg \rangle}\xspace}
\newcommand{\rawreftxt}{\ensuremath{x_{t_R}}\xspace}
\newcommand{\rawtartxt}{\ensuremath{x_t}\xspace}
\newcommand{\rawcaptriplet}{\ensuremath{\langle \rawreftxt, \rawcond, \rawtartxt \rangle}\xspace}
\newcommand{\rawcappair}{\ensuremath{\langle \rawreftxt, \rawtartxt \rangle}\xspace}

\newcommand{\refimg}{\ensuremath{z_{i_R}}\xspace}
\newcommand{\cond}{\ensuremath{z_{c}}\xspace}
\newcommand{\condtext}{\ensuremath{z_{c_T}}\xspace}
\newcommand{\condnegtext}{\ensuremath{z_{c_{T^{\text{-}}}}}\xspace}
\newcommand{\condmask}{\ensuremath{z_{c_M}}\xspace}
\newcommand{\tarimg}{\ensuremath{z_i}\xspace}
\newcommand{\triplet}{\ensuremath{\langle \refimg, \cond, \tarimg \rangle}\xspace}
\newcommand{\reftxt}{\ensuremath{z_{t_R}}\xspace}
\newcommand{\tartxt}{\ensuremath{z_t}\xspace}
\newcommand{\captriplet}{\ensuremath{\langle \reftxt, \cond, \tartxt \rangle}\xspace}
\newcommand{\cappair}{\ensuremath{\langle \reftxt, \tartxt \rangle}\xspace}

\usepackage{xcolor}
\usepackage{kotex}
\newcommand{\sh}[1]{\textcolor{orange}{#1\xspace}}

\title{CompoDiff: Versatile Composed \\Image Retrieval With Latent Diffusion}


\author{Geonmo Gu$^{*,\,1}$ Sanghyuk Chun$^{*,\,2}$ Wonjae Kim$^{2}$ HeeJae Jun$^{1}$ Yoohoon Kang$^{1}$ Sangdoo Yun$^{2}$\\
\\
{$^{1}${NAVER Vision} \qquad $^{2}${NAVER AI Lab}} \qquad {\small $^*$ Equal contribution}\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\usepackage[bottom]{footmisc}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset, named SynthTriplets18M, of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff and SynthTriplets18M tackle the shortages of the previous CIR approaches, such as poor generalizability due to the small dataset scale and the limited types of conditions. CompoDiff not only achieves a new zero-shot state-of-the-art on four CIR benchmarks, including FashionIQ, CIRR, CIRCO, and GeneCIS, but also enables a more versatile and controllable CIR by accepting various conditions, such as negative text and image mask conditions, and the controllability to the importance between multiple queries or the trade-off between inference speed and the performance which are unavailable with existing CIR methods.
The code and dataset are available at \url{https://github.com/navervision/CompoDiff}
\end{abstract}

\section{Introduction}

Imagine a customer seeking a captivating cloth serendipitously found on social media but not the most appealing materials and colors. In such a scenario, the customer needs a search engine that can process composed queries, \eg, the reference garment image along with text specifying the preferred material and color. This task has been recently formulated as \textit{Composed Image Retrieval (CIR)} \citep{cirr} -- \cref{fig:teaser} (a). CIR systems offer the benefits of searching for visually similar items as image-to-image retrieval while providing a high degree of freedom to depict text query as text-to-image retrieval. CIR can also improve the search quality by iteratively taking user feedback. 

The existing CIR methods address the problem by combining image and text features using additional fusion models, \eg, $z_i = \texttt{fusion}(z_{i_R}, z_c)$ where $z_{i}$, $z_c$, $z_{i_R}$ are the target image, conditioning text, and reference image features, respectively\footnote{Throughout this paper, we will use $x$ to denote raw data and $z$ to denote a vector encoded from $x$.}. Although the fusion-based approaches have shown great success (\eg, training an additional module that combines the image and text features \citep{baldrati2022clip4cir}), they have fundamental limitations. First, they need a dataset of triplets \rawtriplet consisting of a reference image (\rawrefimg), a condition (\rawcond), and the corresponding target image (\rawtarimg) to train the fusion module. However, obtaining such triplets can be costly and sometimes impossible. For this reason, the existing CIR methods are trained only on small-scale triplet CIR datasets (\eg, 30K triplets for Fashion-IQ \citep{fashioniq} and 36K triplets for CIRR \citep{cirr}), resulting in a lack of generalizability to other real-world datasets. Second, the fusion module is not flexible; it cannot handle versatile conditions beyond a limited textual one. 
For instance, a user might want to include a negative text that is not desired for the search (\rawcondnegtext) (\eg, an image $+$ ``with cherry blossom'' $-$ ``France'', as in \cref{fig:teaser} (b)), indicate where (\rawcondmask) the condition is applied (\eg, an image $+$ ``balloon'' $+$ indicator, as in \cref{fig:teaser} (c)), or construct a complex condition with a mixture of them.
Furthermore, once the fusion model is trained, it will always produce the same $z_i$ for the given $z_{i_R}$ and $z_c$ to users. However, a practical retrieval system needs to control the level of conditions by its applications (\eg, more focusing on the image while the changes by the text should be small).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser_v3.pdf}
    \caption{\small \textbf{Composed Image Retrieval (CIR) scenarios.} (a) A standard CIR scenario. (b-d) Our versatile CIR scenarios with mixed conditions (\eg, negative text and mask). Results by \ours on LAION-2B.}
    \label{fig:teaser}
    \vspace{-1em}
\end{figure}

This paper aims to achieve a generalizable CIR model with diverse and versatile conditions by using latent diffusion. We treat the CIR task as a conditional image editing task on the latent space, \ie, $z_i = \texttt{Edit}(z_{i_R} | z_c, \ldots)$. Our diffusion-based CIR model, named \ours, can easily deal with versatile and complex conditions, benefiting from the flexibility of the latent diffusion model \citep{rombach2022latentdiffusion} and the classifier-free guidance \citep{ho2022classifier}. Motivated by recent advances in diffusion models, we train a latent diffusion model that translates the embedding of the reference image (\refimg) into the embedding of the target image (\tarimg) guided by the embedding of the given condition (\cond). As in \cref{fig:teaser}, \ours can handle various conditions, which is not possible with the standard CIR scenario with the limited text condition \rawcondtext. Although our method has an advantage over the existing fusion-based methods in terms of versatility, our approach also needs to be trained with triplet datasets where the scale of the existing CIR triplet datasets is extremely small.

To address the data scale issue, we synthesize a vast set of high-quality \textbf{18M} triplets of \rawtriplet by utilizing the large-scale generative models such as OPT \citep{zhang2022opt} and Stable Diffusion \citep{rombach2022latentdiffusion}. We follow InstructPix2Pix (IP2P) \citep{brooks2022instructpix2pix} for synthesizing triplets, while our dataset contains $\times$40 more triplets and $\times$12.5 more keywords (\eg, objects, background details, or textures) than IP2P. Our massive dataset, named \textbf{\ourdataset}, is over 500 times larger than existing CIR datasets and covers a diverse and extensive range of conditioning cases. Our dataset itself makes a significant contribution, resulting in a notable performance improvement for any CIR model. \Eg, ARTEMIS \citep{delmas2022artemis} trained exclusively with \ourdataset outperforms its FashionIQ-trained counterpart (40.6 vs. 38.2 in FashionIQ recall).

To show the generalizability of the models, we evaluate the models on the ``zero-shot'' CIR scenario using four CIR benchmarks: FashionIQ \citep{fashioniq}, CIRR \citep{cirr}, CIRCO \citep{circo}, and GeneCIS \citep{genecis}; \ie, we report the retrieval results by the models trained on our \ourdataset and a large-scale image-text paired dataset without access to the target triplet datasets. In all experiments, \ours achieves the best zero-shot performances with significant gaps (See \cref{tab:main}). Moreover, we observe that the fusion-based approaches solely trained on \ourdataset (\eg, Combiner \citep{baldrati2022clip4cir}) show comparable or outperforming zero-shot CIR performances compared to the previous SOTA zero-shot CIR methods, \eg, Pic2Word \citep{saito2023pic2word} and SEARLE \citep{circo}. Furthermore, we qualitatively observe that the retrieval results of \ours are semantically better than previous zero-shot CIR methods, such as Pic2Word, on a large-scale image database, \eg, LAION-2B.

Another notable advantage of \ours is the ability to control various conditions during inference, which is inherited from the nature of diffusion models: Users can adjust the weight of conditions to make the model focus on the user's preference. Users can also manipulate the randomness of the models to vary the degree of serendipity. In addition, \ours can control the speed of inference with minimal sacrifice in retrieval performance, accomplished by adjusting the number of sampling steps in the diffusion model. As a result, \ours can be deployed in various scenarios with different computational budgets. Note that all of these controllability features can be achieved by controlling the inference parameters of classifier-free guidance, without any model training.


\section{Related Works}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/method_overview_v2.pdf}
    \caption{\small {\bf Comparisons of CIR approaches.} (a) Fusion-based approaches, such as ARTEMIS \citep{delmas2022artemis} and Combiner \citep{baldrati2022clip4cir} fuse the image and text features using a fusion module. (b) Inversion-based approaches, such as Pic2Word \citep{saito2023pic2word} and SEARLE \citep{circo}, project the image feature into the text space. Then, it performs text-to-image retrieval to CIR. (c) Our editing-based approach applies a diffusion process to the image feature with classifier-free guidance (CFG) by additional conditions, such as text condition and mask. (b) and (c) use frozen encoders, and (a) usually tunes the encoders.}
    \label{fig:compodiff_overview}
    \vspace{-.5em}
\end{figure}

\paragraph{Composed image retrieval.}
The overview of the differences between CIR approaches is illustrated in \cref{fig:compodiff_overview}.
The mainstream CIR models have focused on \textit{multi-modal fusion methods}, which combine image and text features extracted from separate visual and text encoders, as in \cref{fig:compodiff_overview} (a). For example, \citet{vo2019tirg} and \citet{yu2020curlingnet} used CNN and RNN, and \citet{chen2022mur} and \citet{Anwaar_Labintcev_Kleinsteuber_2021} used CNN and Transformer. More recent methods address CIR tasks by leveraging external knowledge from models pre-trained on large-scale datasets such as CLIP \citep{radford2021clip}. For example, Combiner \citep{baldrati2022clip4cir} fine-tunes the CLIP text encoder on the triplet dataset to satisfy the relationship of $z_{i} = z_{i_R} + z_{c}$, and then trains a Combiner module on top of the encoders. However, it still requires expensive triplets of the target domain.

To solve this problem, recent studies aim to solve CIR tasks in a zero-shot manner. \textit{Inversion-based zero-shot CIR methods}, such as Pic2Word \citep{saito2023pic2word} and SEARLE \cite{circo}, tackle the problem through text-to-image retrieval tasks where the input image is projected into the condition text -- \cref{fig:compodiff_overview} (b). Note that our zero-shot CIR scenario is slightly different from theirs; our zero-shot CIR denotes that the models are not trained on the target triplet datasets, but trained on our synthetic dataset, \ourdataset, and image-text paired dataset, \eg, the LAION-2B dataset \citep{schuhmann2022laion}. On the other hand, \citet{saito2023pic2word} and \citet{circo} use the term zero-shot when the CIR models are trained without a triplet dataset. 

All the existing CIR models only focus on text conditions (\condtext) (\eg, \cref{fig:teaser} (a)) and have difficulties in handling versatile scenarios (\eg, \cref{fig:teaser} (b-d)) with a lack of the controllability. On the other hand, our method enables multiple various conditions and controllabilities with strong zero-shot performances by employing (1) a latent diffusion model \citep{rombach2022latentdiffusion} with classifier-free guidance \citep{ho2022classifier} and (2) a massive high-quality synthetic dataset, \ourdataset.

\paragraph{Dataset creation with diffusion models.}
A conventional data collection process for \rawtriplet is two-staged: collecting candidate reference-target image pairs and manually annotating the modification sentences by human annotators \citep{fashioniq, cirr}. For example, FashionIQ \citep{fashioniq} collects the candidate pairs from the same item category (\eg, shirt, dress, and top) and manually annotates the relative captions by crowd workers. CIRR \citep{cirr} gathers the candidate pairs from real-life images from NLVR$^2$ \citep{suhr2018corpus}. The data collection process for \rawtriplet inevitably becomes expensive, making it difficult to scale CIR datasets. We mitigate this problem by generating a massive synthetic dataset instead of relying on human annotators.

Recently, there have been attempts to generate synthetic data to improve model performance \citep{brooks2022instructpix2pix, Nair_Bandara_Patel_2022, Shipard_Wiliem_Thanh_Xiang_Fookes_2023} by exploiting the powerful generation capabilities of diffusion models \citep{ho2020denoising, rombach2022latentdiffusion}.
In contrast to previous attempts to synthesize training data points by GAN \citep{choe2017face, lee2018training, sandfort2019data}, recent diffusion model-based approaches show high image quality and high controllability by textual prompts (\eg, by classifier-free guidance \citep{ho2022classifier} and latent diffusion \citep{rombach2022latentdiffusion}).
In particular, \citet{brooks2022instructpix2pix} proposes a generation process for \rawtriplet to train an image editing model.
We scale up the dataset synthesis process of \citet{brooks2022instructpix2pix} from 450K triplets to 18M. We also make the triplets more diverse by applying the object-level modification process. We describe the details of our dataset generation process in \cref{sec:cir_dataset_gen}.

\section{\ours: Composed Image Retrieval with Latent Diffusion}
\label{sec:method}

This section introduces \oursfull. \cref{fig:compodiff_overview} (c) shows an overview. \ours employs a diffusion process in the frozen CLIP latent feature space with classifier-free guidance (CFG). Unlike previous latent diffusion models \citep{rombach2022latentdiffusion}, we adopt the Transformer architecture for the denoiser. As we train our model with various tasks, such as text-to-image (T2I) generation, masked T2I and triplet-based generation, \ours can handle various conditions (\eg, negative text, mask, or a mixture of conditions), while the previous approaches only limit beyond the positive text instruction. 
We will describe the details of training and inference, and how \ours handles various conditions through latent diffusion and CFG.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/training_details_v2.pdf}
    \vspace{-1em}
    \caption{\small \textbf{Training overview.} $e_t$ denotes the embedding vector for timestamp $t$. For stage 2, we illustrate an example when all three conditions are given. In practice, we randomly drop conditions to make \ours handle various scenarios (\eg, text only, image-text query). Here, only Denoising Transformer $\epsilon_\theta$ is trainable.}
    \label{fig:method_training_overview}
    \vspace{-.5em}
\end{figure}

\subsection{Training}
\label{subsec:training}

Due to the training stability, \ours uses a two-stage training strategy (\cref{fig:method_training_overview}). In stage 1, we train a text-to-image latent diffusion model on a large-scale image-text dataset, LAION-2B \citep{schuhmann2022laion}. In stage 2, we fine-tune the model on our synthetic triplet dataset, \ourdataset, and LAION-2B. Below, we describe the details of each stage.

In stage 1, we train a transformer decoder to convert CLIP textual embeddings into CLIP visual embeddings. This stage is similar to training the prior model in Dalle-2 \citep{dall-e2}, but our model takes only two tokens; a noised CLIP image embedding and a diffusion timestep embedding. The Dalle-2 prior model is computationally inefficient because it also takes 77 encoded CLIP text embeddings as an input. However, \ours uses the encoded text embeddings as conditions through cross-attention mechanisms, which speeds up the process by a factor of three while maintaining similar performance (See \cref{subsec:ablation_study_supp}). Instead of using the noise prediction of \citet{ho2020denoising}, we train the transformer decoder to predict the denoised $z_i$ directly due to the stability.

Now, we introduce the objective of the first stage with CLIP image embeddings of an input image $z_i$, encoded CLIP text embeddings for text condition $z_{c_T}$, and the denoising Transformer $\epsilon_\theta$:
\begin{equation}
\label{eq:stage1-t2i}
\mathcal{L}_\text{stage1} = \mathbb{E}_{t\sim[1,T]}  \|z_i - \epsilon_\theta(z_i^{(t)},t \vert z_{c_T}) \|^2
\end{equation}
During training, we randomly drop the text condition by replacing $z_{c_T}$ with a null text embedding $\O_{c_T}$ in order to induce the classifier-free guidance \citep{ho2022classifier}. We use the CLIP text embedding for empty text (``'') for the null embedding.

\begin{wrapfigure}{r}{0.34\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/cfg_variation.pdf}
    \caption{\small \textbf{Controlling image and text weights for inference.} Example retrieval results on the LAION images by varying image and text weights, $w_I$ and $w_T$ in \cref{eq:cfg}, respectively}
    \label{fig:cfg_variation_images}
    \vspace{-1em}
\end{wrapfigure}

In stage 2, we incorporate condition embeddings, injected by cross-attention, into CLIP text embeddings, along with CLIP reference image visual embeddings and mask embeddings (See \cref{fig:method_training_overview}).
We fine-tune the model with three different tasks: a conversion task that converts textual embeddings into visual embeddings (\cref{eq:stage1-t2i}), a mask-based conversion task (\cref{eq:stage2-inpaint}), and the triplet-based CIR task with \ourdataset (\cref{eq:stage2-triplet}). The first two tasks are trained on LAION-2B and the last one is trained on \ourdataset. We alternatively update the model with \cref{eq:stage1-t2i}, \cref{eq:stage2-inpaint}, and \cref{eq:stage2-triplet} with the proportions 30\%, 30\%, 40\%. When we train a model with \cref{eq:stage2-triplet} only, we observe that the representation power learned in stage 1 is easily forgotten (See \cref{tab:ablation_stage2}).
Below, we describe the details of the mask-based conversion task and the triplet-based CIR task.

The mask-based conversion task learns a diffusion process that recovers the full image embedding from a masked image embedding. For the masked image embedding $z_{i, \text{masked}}$, we make an input mask using a zero-shot text-conditioned segmentation model, ClipSeg \citep{luddecke2022image} with nouns from the given caption extracted by a POS tagger of \texttt{Spacy}. Then, we add a Gaussian random noise to the mask region of the image and extract $z_{i, \text{masked}}$. We also introduce mask embedding $z_{c_M}$ by projecting a 64$\times$64 resized segmentation mask to the CLIP embedding dimension using a single MLP. Now, we introduce the mask-based conversion task as follows:
\begin{equation}
\label{eq:stage2-inpaint}
\mathcal{L}_\text{stage2\_masked\_conversion} = \mathbb{E}_{t\sim[1,T]}  \|z_i - \epsilon_\theta(z_{i, \text{masked}}^{(t)},t \vert z_{c_T}, z_{i, \text{masked}}, z_{c_M}) \|^2,
\end{equation}
where $z_{c_M}$ is a mask embedding.
Finally, we introduce the triplet-based training objective to solve CIR tasks while \cref{eq:stage1-t2i} and \cref{eq:stage2-inpaint} are trained on LAION-2B. Our last objective is as follows:
\begin{equation}
\label{eq:stage2-triplet}
\mathcal{L}_\text{stage2\_triplet} = \mathbb{E}_{t\sim[1,T]}  \|z_{i_T} - \epsilon_\theta(z_{i_T}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) \|^2,
\end{equation}
where $z_{i_R}$ is a reference image feature and $z_{i_T}$ is a modified target image feature

We randomly drop the conditions of stage 2, as in stage 1, except for the mask conditions. We use an all-zero mask condition for the tasks that do not use a mask condition. When we drop the image condition of \cref{eq:stage2-triplet}, we replace $z_{i_R}$ with the null image feature, an all zero vector.





\subsection{Inference}
\label{subsec:inference}
Given a reference image feature $z_{i_R}$, a text condition feature $z_{c_T}$, and a mask embedding $z_{c_M}$, we apply a denoising diffusion process based on CFG \citep{ho2022classifier} as follows:
\begin{equation}
\label{eq:cfg}
{
\begin{aligned}
\tilde{\epsilon}_\theta(_{i}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) 
= &\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, \O_{i_R}, z_{c_M}) \\
&+ w_I(\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, z_{i_R}, z_{c_M}) -\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, \O_{i_R}, z_{c_M})) \\
&+ w_T(\epsilon_\theta(z_{i}^{(t)},t \vert z_{c_T}, z_{i_R}, z_{c_M}) -\epsilon_\theta(z_{i}^{(t)},t \vert \O_{c_T}, z_{i_R}, z_{c_M}))
\end{aligned}
}
\end{equation}
where $\O$ denotes null embeddings, \ie, the CLIP textual embedding for empty text (``'') for the text null embedding and an all-zero vector for the image null embedding.
One of the main advantages of \cref{eq:cfg} is the ability to handle various conditions at the same time. When using negative text, we simply replace $\O_{i_T}$ with the CLIP text embeddings $c_{T^-}$ for the negative text.

Another advantage of CFG is the controllability of the queries without training, \eg, it allows to control the degree of focus on image features to preserve the visual similarity with the reference by simply adjusting the weights $w_I$ or $w_T$ in \cref{eq:cfg}.
We show the top-1 retrieved item by varying the image and text weights $w_I$ and $w_T$ from LAION-2B in \cref{fig:cfg_variation_images}. By increasing $w_I$, \ours behaves more like an image-to-image retrieval model. Increasing $w_T$, on the other hand, makes \ours focus more on the ``pencil sketch'' text condition. We use ($w_I$, $w_T$) = (1.5, 7.5) for our experiments. The retrieval performance by varying $w_I$ and $w_T$ is shown in \cref{subsec:ablation_study_supp}.

Since our retrieval model is based on a diffusion process, we can easily control the balance between the inference time and the retrieval quality of the modified feature by varying step size. In practice, we set the step size to 5, which shows the best trade-off. More details can be found in \cref{subsec:abl}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.98\linewidth]{figures/dataset_generation_overview_v2.pdf}
    \caption{\small {\bf Overview of the generation process for \ourdataset.} \rawtriplet from \rawcaptriplet.
    }
    \label{fig:dataset_generation_overview}
\end{figure*}

\section{\ourdataset: Massive High-Quality Synthesized Dataset}
\label{sec:cir_dataset_gen}

CIR requires a dataset of triplets \rawtriplet of a reference image (\rawrefimg), a condition (\rawcond), and the corresponding target image (\rawtarimg). Instead of collecting a dataset by humans, we propose to generate massive triplet data points by using large generative models. We follow the main idea of Instuct Pix2Pix (IP2P) \citep{brooks2022instructpix2pix}. First, we generate \rawcaptriplet where \rawreftxt is a reference caption, \rawcondtext is a modification instruction text, and \rawtartxt is the caption modified by \rawcondtext. We use two strategies to generate \rawcaptriplet: (1) We collect massive captions from the existing caption datasets and generate the modified captions by replacing the keywords in the reference caption. (2) We fine-tune a large language model, OPT-6.7B \citep{zhang2022opt}, on the generated caption triplets from \citet{brooks2022instructpix2pix}. After generating massive triplets of \rawcaptriplet, we generate images from the caption triplets using StableDiffusion (SD) \citep{rombach2022latentdiffusion} and Prompt-to-Prompt \citep{hertz2022prompttoprompt} following \citet{brooks2022instructpix2pix}. The entire generation process is illustrated in \cref{fig:dataset_generation_overview}.

Compared to manual dataset collections \citep{fashioniq, cirr}, our approach can easily generate more diverse triplets even if a triplet rarely occurs in reality (See the examples in \cref{fig:dataset_generation_overview}). Compared to the synthetic dataset of \citet{brooks2022instructpix2pix}, \ourdataset contains more massive triplets (450k vs. 18M). 
Furthermore, since our caption triplets are synthesized based on keywords, \ourdataset covers more diverse keywords than IP2P (47k vs. 586k as shown in \cref{tab:keyword_stats}).

Due to the page limit, we describe the details of the entire dataset process in \cref{sec:cir_dataset_gen_supp}.

\begin{table}[t]
\small
\centering
\begin{tabular}{lcc}
\toprule
  & IP2P \citep{brooks2022instructpix2pix} & \ourdataset \\ \midrule
\rawcaptriplet     & 450k & 60M \\
Unique object terms & 47,345 & 586,369 \\
\midrule
\rawtriplet ~~{\scriptsize (Keyword-based)} & - & 11.4M \\
\rawtriplet ~~{\scriptsize (LLM-based)} & 1M & 7.4M \\ \midrule
\rawtriplet ~~{\scriptsize (Total)}   & 1M & 18.8M \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf Statistics of generated datasets.} We compare our \ourdataset and the dataset by Instruct Pix2Pix (IP2P) in terms of the dataset statistics. \rawcaptriplet denotes the triplet of captions \{original caption, modification instruction, and modified caption\} and \rawtriplet denotes the triplet of \{original image, modification instruction, and modified image\}.
}
\label{tab:keyword_stats}
\end{table}

\section{Experiments}


\subsection{Implementation details and experiment settings}

\ours is built upon frozen CLIP features. We use two different CLIP models, the official CLIP ViT-L/14 \citep{radford2021clip} and CLIP ViT-G/14 by OpenCLIP \citep{openclip}, where the feature dimension of the former is 768 and the latter is 1280.
In the experiments, we observe that the quality of the frozen encoders is critical to the final performance. Beyond the backbone size, we observe the text condition encoder is also important (\cref{tab:improved_text_encoder}).
On top of the frozen CLIP feature, we use a simple Transformer architecture for the denoising procedure, instead of the denoising U-Net \citep{rombach2022latentdiffusion}. We empirically observe that our transformer architecture performs slightly better than the U-Net architecture, but is much simpler. We use the multi-head self-attention blocks as the original Transformer \citep{vaswani2017attention}. We set the depth, the number of heads, and the dimensionality of each head to 12, 16, and 64, respectively. The hidden dimension of the Transformer is set to 768 and 1280 for ViT-L and ViT-G, respectively.
The denoising Transformer takes two inputs: a noisy visual embedding and a time-step embedding. The conditions (\eg, text, mask and image conditions) are applied only to the cross-attention layer; thereby it is computationally efficient even as the number of conditions becomes larger. Our method is similar to the ``DiT block with cross-attention'' by \citet{peebles2022scalable}, but \ours handles more various conditions.

All models were trained using AdamW \citep{loshchilov2017decoupled}. We used DDIM \citep{song2020denoising} for the sampling variance method. We did not apply any image augmentation but used pre-extracted CLIP image features for computational efficiency; text features were extracted on the fly as text conditions can vary in \ourdataset.
We report the detailed hyperparameters in \cref{tab:parameters}. 


We evaluate the zero-shot (ZS) capability of \ours on four CIR benchmarks, including FashionIQ \citep{fashioniq}, CIRR \citep{cirr}, CIRCO \citep{circo} and GeneCIS \citep{genecis}. 
We compare \ours to the recent ZS CIR methods, including Pic2Word \citep{saito2023pic2word} and SEARLE \citep{circo}. We also reproduce the fusion-based methods, such as ARTEMIS \citep{delmas2022artemis} and Combiner \citep{baldrati2022clip4cir}, on \ourdataset and report their ZS performances.
Due to the page limit, the details of each task and method are in \cref{subsec:baselines_supp} and \ref{subsec:dataset_details_supp}.
The fine-tuned performances on FashionIQ and CIRR are in \cref{subsec:full_experiment_results_supp}.


\input{tables/main_table}
\subsection{Zero-shot CIR comparisons}

\cref{tab:main} shows the overview of our zero-shot CIR comparison results. CLIP + IP2P denotes the naive editing-based approach by editing the reference image with the text condition using IP2P and performing image-to-image retrieval using CLIP ViT-L/14. In the table, \ours outperforms all the existing methods in all benchmarks with significant gaps. The table shows the effectiveness both of our diffusion-based CIR approach and our massive synthetic dataset. In the \ourdataset-trained group, \ours outperforms previous SOTA fusion-based CIR methods with a large gap, especially on CIRR and CIRCO, which focus on real-life images and complex descriptions. We also can observe that the \ourdataset-trained group also enables the fusion-based methods to have the zero-shot capability competitive to the SOTA zero-shot CIR methods. More interestingly, ARTEMIS even outperforms its supervised counterpart on FashionIQ with a large gap (40.6 vs. 38.2 in average recall).
We provide the full results of \cref{tab:main}, the supervised results of ARTEMIS, Combiner, and \ours trained on \ourdataset, and more discussions in \cref{subsec:full_experiment_results_supp}.

\subsection{Analysis on \ours}
\label{subsec:abl}

\paragraph{Stage 2 ablation.}
As we described in \cref{subsec:training}, we alternatively update the model using three different objectives. Here, we conduct an ablation study of our design choice. \cref{tab:ablation_stage2} shows the result. Our multi-task learning strategy improves the overall performance. It is because although \ourdataset is a vast and diverse dataset, its diversity is weaker than LAION. As LAION is not used for \textit{only} \cref{eq:stage2-triplet}, we employ additional tasks using LAION, \ie, \cref{eq:stage1-t2i} and \cref{eq:stage2-inpaint}.
\input{tables/ablation_stage2}

\vspace{-1em}
\paragraph{Impact of dataset scale.}
We provide an ablation study on stage 2 of \ours by varying the size of the triplet dataset for \cref{eq:stage2-triplet}. \cref{tab:dataset_scale} shows the results. First, at a scale of 1M, the performance of CompoDiff trained on our 1M subset significantly outperformed that publicly provided by \citet{brooks2022instructpix2pix}. This result indicates that our dataset has a more diverse representation capability. As the size of our dataset increases, the performance gradually improves. This result supports that the dataset scale is crucial to the generalizability of CIR methods.
\input{tables/ablation_dataset_scale}

\vspace{-1em}
\paragraph{Denoising step size.}
\ours needs denoising steps to denoise noisy image embeddings. However, it is possible to obtain reliable denoised image embeddings with just a few steps as shown in \cref{tab:denoising_steps}. Even with only 5 iterations, our model can produce competitive results. If we use 100 steps, we have a slightly better performance (42.65 vs. 42.33), but a much slower inference time (2.02 sec vs. 0.12 sec). In the experiments, we set the denoising step size to 10.
\input{tables/ablation_results_per_steps}

\vspace{-1em}
\paragraph{Condition text encoder.}
As observed by \citet{balaji2022ediffi}, using a text-oriented model in addition to the CLIP textual encoder results in improved performance of image-text tasks. Motivated by this observation, we also use both the CLIP textual encoder and the language-oriented encoder for extracting the text features of \cref{eq:stage2-triplet}. In \cref{tab:improved_text_encoder}, we show the choice of the text encoder affects a lot to the performance. The details of each model are in \cref{subsec:impact_text_encoder_supp}.
We use both T5-XL encoder \citep{raffel2020t5} and CLIP text encoder for the ViT-L model. For the ViT-G model, we use the CLIP text encoder only because we empirically observe that the textual representation power of the ViT-G is much more powerful than the ViT-L.
We also conducted the ablation study on how to handle text inputs by the model. The results can be found in \cref{subsec:ablation_study_supp}.

\vspace{-1em}
\begin{wraptable}{r}{0.5\linewidth}
\small
\centering
\vspace{-1.5em}
\begin{tabular}{l|ccc}
\toprule
Text Enc & T5-XL & CLIP & CLIP + T5-XL \\
\midrule
FashionIQ & 38.20 & 42.33 & \textbf{44.11} \\
CIRR & 29.19 & 37.83 & \textbf{39.25} \\
\bottomrule
\end{tabular}
\caption{\small \textbf{Text encoder impact of \ours ViT-L}}
\label{tab:improved_text_encoder}
\vspace{-4em}
\end{wraptable}
\paragraph{The choice of $w_I$ and $w_T$.}
We include the retrieval performances by varying conditions in \cref{fig:heatmaps}. In summary, we can easily control the weight of conditions by $w_I$ and $w_T$.






\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\linewidth]{figures/ours_vs_pic2word.pdf}
    \caption{\small {\bf Qualitative comparison of zero-shot CIR for Pic2Word and \ours.} We conduct CIR on LAION. As Pic2Word cannot take a simple instruction, we made a simple modification for the given instruction.}
    \label{fig:ours_vs_pic2word}
\end{figure*}
\subsection{Qualitative examples}
\label{subsec:examples}
We qualitatively show the versatility of \ours for handling various conditions. For example, \ours not only can handle a text condition, but it can also handle a \textit{negative} text condition (\eg, removing specific objects or patterns in the retrieval results), masked text condition (\eg, specifying the area for applying the text condition). \ours even can handle all conditions simultaneously (\eg, handling positive and negative text conditions with a partly masked reference image at the same time). To show the quality of the retrieval results, we conduct a zero-shot CIR on entire LAION dataset \citep{schuhmann2022laion} using FAISS \citep{faiss} for simulating billion-scale CIR scenarios.
We show an example in \cref{fig:teaser} and more examples are shown in \cref{subsec:more_examples_supp}, \ref{subsec:unclip_supp}.

\cref{fig:ours_vs_pic2word} shows qualitative examples of zero-shot CIR results by Pic2Word and \ours. \ours results in semantically high-quality retrieval results (\eg, understanding the ``crowdedness'' of the query image and the meaning of the query text at the same time). However, Pic2Word shows poor understanding of the given queries, resulting in unfortunate retrieval results (\eg, ignoring ``grown up'' of text query, or the  ``crowdedness'' of the query image). More examples are in \cref{subsec:more_examples_supp}.

Finally, it is worth noting that \ours generates a feature belonging to the CLIP space.
It means that we can apply the unCLIP generator \citep{dall-e2} on our composed features. We compare the retrieval results from the LAION dataset and the generated images in \cref{subsec:unclip_supp}. \ours can manipulate the given input reflecting the given conditions.










\section{Conclusion}
In this paper, we have introduced \ours, a novel diffusion-based method for solving complex CIR tasks. We have created a large and diverse dataset named \ourdataset, consisting of 18.8M triplets of images, modification texts, and modified images. Our model has demonstrated impressive zero-shot CIR capabilities, as well as remarkable versatility in handling diverse conditions, such as negative text or image masks, and the controllability to enhance user experience, such as adjusting image text query weights. Furthermore, by training the previous methods on \ourdataset, the models became comparable zero-shot predictors to the SOTA zero-shot methods. We strongly encourage future researchers to leverage our dataset to advance the field of CIR.



\section*{Societal Impact}

Our work is primarily focused on solving complex composed image retrieval (CIR) challenges and is not designed for image editing purposes. However, we are aware that with the use of additional public resources, such as the community version of the unCLIP feature decoder \citep{dall-e2}, our method can potentially be utilized as an image editing method. We would like to emphasize that this unintended application is not the primary objective of our research, and we cannot guarantee the effectiveness or safety of our method in this context.

It is important to note that we have taken steps to mitigate potential risks associated with the unintended use of our method for image editing. For instance, we applied NSFW filters to filter out potentially malicious samples during the creation of \ourdataset. Nevertheless, we recognize the need for continued research into the ethical and societal implications of AI technologies and pledge to remain vigilant about potential unintended consequences of our work.



{\small
\bibliography{reference}
\bibliographystyle{iclr2024_conference}
}

\clearpage
\appendix
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

\section*{Appendix}

In this additional document,
we describe the details of our data generation process in \cref{sec:cir_dataset_gen_supp}, including the keyword-based caption generation process (\cref{subsec:gen_keyword_supp}), the LLM-based caption generation process (\cref{subsec:gen_llm_supp}), the triplet generation process by the generated captions (\cref{subsec:triplet_gen_supp}) the statistics of our generated dataset (\cref{subsec:dataset_stats_supp}), and example samples from \ourdataset (\cref{subsec:dataset_examples}).
\cref{sec:exp_details_supp} contains more experimental details of \ours, such as the details of comparisons methods (\cref{subsec:baselines_supp}), CIR tasks (\cref{subsec:dataset_details_supp}), implementation details (\cref{subsec:implementation_details_supp}) and text encoder details (\cref{subsec:impact_text_encoder_supp}).
Finally, we provide more experimental results in \cref{sec:more_experiments_supp}, including the full results (\cref{subsec:full_experiment_results_supp}),
more qualitative examples (\cref{subsec:more_examples_supp}), ablation study (\cref{subsec:ablation_study_supp}), and retrieved examples from LAION and generation examples by using the unCLIP generator (\cref{subsec:unclip_supp}).

\section{Dataset Construction Details for \ourdataset}
\label{sec:cir_dataset_gen_supp}

\subsection{Keyword-based diverse caption generation}
\label{subsec:gen_keyword_supp}
As the first approach to generating caption triplets, we collect captions from the existing caption datasets and modify the captions by replacing the object terms in the captions, \eg, $\langle$``a strawberry tart is ...'', ``covert strawberry to pak choi'', ``a pak choi tart is ...''$\rangle$ in \cref{fig:dataset_generation_overview}. For the caption dataset, 
We use the captions from COYO 700M \citep{kakaobrain2022coyo-700m}, StableDiffusion Prompts\footnote{\url{https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts}} (user-generated prompts that make the quality of StableDiffusion better), LAION-2B-en-aesthetic (a subset of LAION-5B \citep{schuhmann2022laion}) and LAION-COCO datasets \citep{laioncoco} (synthetic captions for LAION-5B subsets with COCO style captions \citep{cococaption}. LAION-COCO less uses proper nouns than the real web texts).

We extract the object terms from the captions using a part-of-speech (POS) tagger, provided by \texttt{Spacy}\footnote{\url{https://spacy.io/}}. After frequency filtering, we have 586k unique object terms. For each caption, we replace the object term with other similar keywords by using the CLIP similarity score. More specifically, we extract the textual feature of keywords using the CLIP ViT-L/14 text encoder, and we choose an alternative keyword from keywords that have a CLIP similarity between 0.5 and 0.7. By converting the original object to a similar object, we have caption pairs of \rawcappair.

Using the caption pair \rawcappair, we generate the modification instruction text \rawcondtext based on a randomly chosen template from 48 pre-defined templates shown in \cref{tab:example_templates_supp}. After this process, we have the triplet of \rawcaptriplet. We generate $\approx$30M triplets by the keyword-based method.

\subsection{Amplifying InstructPix2Pix (IP2P) triplets by LLM}
\label{subsec:gen_llm_supp}
We also re-use the generated \rawcaptriplet by IP2P. We amplify the number of IP2P triplets by fine-tuning a large language model, OPT-6.7B \citep{zhang2022opt}, on the generated 452k caption triplets provided by \citet{brooks2022instructpix2pix}. Using the fine-tuned OPT, we generate $\approx$30M triplets.

\begin{table}[ht!]
\newcommand{\srctab}{\texttt{\$\{source\}}\xspace}
\newcommand{\trgtab}{\texttt{\$\{target\}}\xspace}
\scriptsize
\centering
\begin{tabular}{ll}
\toprule
\multicolumn{2}{c}{Templates to change \srctab to \trgtab} \\ \midrule
\textit{``replace \srctab with \trgtab''} &
\textit{``substitute \trgtab for \srctab''} \\
\textit{``change \srctab to \trgtab''} &
\textit{``\trgtab''} \\
\textit{``apply \trgtab''} &
\textit{``add \trgtab''} \\
\textit{``exchange \srctab with \trgtab''} &
\textit{``alter \srctab to \trgtab''} \\
\textit{``convert \srctab to \trgtab''} &
\textit{``transform \srctab into \trgtab''} \\
\textit{``swap \srctab for \trgtab''} &
\textit{``replace \srctab with \trgtab''} \\
\textit{``remodel \srctab into \trgtab''} &
\textit{``redesign \srctab as \trgtab''} \\
\textit{``update \srctab to \trgtab''} &
\textit{``revamp \srctab into \trgtab''} \\
\textit{``if it is \trgtab''} &
\textit{``substitute \trgtab for \srctab''} \\
\textit{``modify \srctab to become \trgtab''} &
\textit{``turn \srctab into \trgtab''} \\
\textit{``alter \srctab to match \trgtab''} &
\textit{``customize \srctab to become \trgtab''} \\
\textit{``adapt \srctab to fit \trgtab''} &
\textit{``upgrade \srctab to \trgtab''} \\
\textit{``change \srctab to match \trgtab''} &
\textit{``tweak \srctab to become \trgtab''} \\
\textit{``amend \srctab to fit \trgtab''} &
\textit{``\trgtab is the new option''} \\
\textit{``choose \trgtab instead''} &
\textit{``\trgtab is the updated version''} \\
\textit{``use \trgtab from now on''} &
\textit{``\trgtab is the new choice''} \\
\textit{``opt for \trgtab''} &
\textit{``\trgtab is the updated option''} \\
\textit{``\trgtab is the new selection''} &
\textit{``\trgtab is the new option available''} \\
\textit{``\trgtab is the updated choice''} &
\textit{``\srctab is replaced with \trgtab''} \\
\textit{``\srctab is removed and \trgtab is added''} &
\textit{``\trgtab is introduced after \srctab is removed''} \\
\textit{``\srctab is removed and \trgtab takes its place''} &
\textit{``\trgtab is added after \srctab is removed''} \\
\textit{``\srctab is removed and \trgtab is introduced''} &
\textit{``\trgtab is added in place of \srctab''} \\
\textit{``\trgtab is introduced after \srctab is retired''} &
\textit{``\trgtab is added as a replacement for \srctab''} \\
\textit{``\trgtab is introduced as the new option after}&
~~\textit{\srctab is removed''} \\
\bottomrule
\end{tabular}
\vspace{.5em}
\caption{\small {\bf The full 48 keyword converting templates.}}
\label{tab:example_templates_supp}
\end{table}
\subsection{Triplet generation from caption triplets}
\label{subsec:triplet_gen_supp}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/compodiff_data_stats.pdf}
        \caption{\small {\bf Statistics of \ourdataset captions.} We show the population of our captions by the number of tokens per caption. We include captions having larger than 40 tokens in ``40+''.}
        \label{fig:data_stats_supp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/instruction_data_stats.pdf}
    \caption{\small {\bf Statistics of instructions of the CIR datasets.} We show the population of instruction captions (\eg, ``change A to B'') by the number of tokens. We include captions having larger than 60 tokens in ``60''.}
    \label{fig:instruction_stats_supp}
    \end{subfigure}
    \caption{\small {\bf Statistics of \ourdataset.}}
    \label{fig:enter-label}
\end{figure}
We generate 60M caption triplets \rawcaptriplet by the keyword-based generation process and the LLM-based generation process
Using the triplets, we generate 60M \rawtriplet by using state-of-the-art text-to-image generation models. Note that two captions do not have a guarantee to be semantically similar. Following \citet{brooks2022instructpix2pix}, we generate images using Prompt-to-Prompt \citep{hertz2022prompttoprompt}, which aims to generate similar images for multiple generations by sharing cross-attention weights during the denoising steps of diffusion models.

We employ multiple state-of-the-art text-to-image (T2I) generation models, including StableDiffusion (SD) 1.5 \citep{rombach2022latentdiffusion}, SD 2.0, SD 2.1, and SD anime models to generate diverse images not biased towards a specific model.
On the other hand, \citet{brooks2022instructpix2pix} only employs SD 1.5 for generating images, resulting in a lack of diversity of images compared to our dataset.

We apply a filtering process following \citet{brooks2022instructpix2pix} to remove the low-quality \rawtriplet.
We filter the generated images for an image-image CLIP threshold of 0.70 to ensure that the images are not too different, an image-caption CLIP threshold of 0.2 to ensure that the images correspond to their captions, and a directional CLIP similarity of 0.2 to ensure that the change in before/after captions correspond with the change in before/after images. Additionally, for keyword-based data generation, we filter out for a keyword-image CLIP threshold of 0.20 to ensure that images contain the context of the keyword, and for instruction-based data generation, we filter out for an instruction-modified image CLIP threshold of 0.20 to ensure consistency with the given instructions.

After the filtering, we have 11.4M \rawtriplet from the keyword-based generated captions and 7.4M \rawtriplet from the LLM-based generated captions. It implies that the fidelity of our keyword-based method is higher than OPT fine-tuning in terms of T2I generation. As a result, \ourdataset contains 18.8M synthetic \rawtriplet.
Examples of our dataset are shown in \cref{subsec:dataset_examples}.
\ificlrfinal
\else
\textbf{As our dataset is larger than 4.1TB, it is impossible to serve the dataset while keeping anonymity. We provide a small subset of images in the supplementary materials for the reviewers, and the dataset will be hosted through the HuggingFace hub.}
\fi

\subsection{Dataset Statistics}
\label{subsec:dataset_stats_supp}

We show the statistics of our generated caption dataset (\ie, before T2I generation, \rawreftxt and \rawtartxt). We use the CLIP tokenizer to measure the statistics of the captions. \cref{fig:data_stats_supp} shows the cumulative ratio of captions with tokens less than X. About half of the captions have less than 13 tokens, and 90\% of the captions have less than 20 tokens. Only 0.8\% of the captions have more than 40 tokens.

We also compare our dataset, FashionIQ \citep{fashioniq} and CIRR \citep{cirr} in terms of the token statistics of instructions (\ie, \rawcond). \cref{fig:instruction_stats_supp} shows that our dataset has relatively shorter instructions than other human-annotated instructions. We presume that this is why \ours performs better when fine-tuning on the target dataset.

\subsection{Example samples of \ourdataset}
\label{subsec:dataset_examples}

We illustrate example samples of \ourdataset in \cref{fig:dataset_examples}. Our dataset can express the change of overall context (\eg, ``make the landscape a cityscape''), the seasonal change (\eg, ``make it sprint''), the change of mood (\eg, ``make it a watercolor''), and the change of local objects (\eg, ``have the person be a dog'').
The full dataset will be hosted through the HuggingFace hub.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/dataset_examples.pdf}
    \caption{\small {\bf Examples of \ourdataset.} We show examples of \rawtriplet, \ie, \{original image, modification instruction, and modified image\}, as well as the generation prompt for \rawrefimg and \rawtarimg.}
    \label{fig:dataset_examples}
    \vspace{-1em}
\end{figure}








\section{More Experimental Details}
\label{sec:exp_details_supp}

\subsection{Comparison methods.}
\label{subsec:baselines_supp}
We compare \ours with four state-of-the-art CIR methods as follows:

\textbf{Combiner} \citep{baldrati2022clip4cir} involves a two-stage training process. First, the CLIP text encoder is fine-tuned by contrastive learning of \cond and \refimg $+$ \cond in the CLIP embedding space. The second stage replaces \refimg $+$ \cond to the learnable Combiner module. Only the Combiner module is trained during the second stage. Hence, its image embedding space is the original CLIP space as \ours.

\textbf{ARTEMIS} \citep{delmas2022artemis} optimizes two similarities simultaneously. The implicit similarity is computed between the combined feature of \tarimg and \cond, and the combined one of \refimg and \cond. The explicit matching is computed between \tarimg and \cond. ARTEMIS suffers from the same drawback as previous CIR methods, \eg TIRG \citep{vo2019tirg}: As it should compute combined feature of \tarimg and \cond, it is not feasible to use an approximate nearest neighbor search algorithm, such as FAISS \citep{faiss}. This is not a big problem in a small dataset like FashionIQ, but it makes ARTEMIS infeasible in real-world scenarios, \eg, when the entire LAION-2B is the target database.

\textbf{Pic2Word} \citep{saito2023pic2word} projects a visual feature into text embedding space, instead of combining them. Pic2Word performs a text-to-image retrieval by using the concatenated feature as the input of the CLIP textual encoder. As the projection module is solely trained on cheap paired datasets without expensive triplet datasets, it is able to solve CIR in a zero-shot manner.

\textbf{SEARLE} \citep{circo} is a similar to Pic2Word, while SEARLE employes text-inversion task instead of projection. \citet{circo} also proposed a technique, named Optimization-based Textual Inversion (OTI), a GPT-powered regularization method. In this paper, we compare SEARLE-XL and SEARLE-XL-OTI, which use the ViT-L/14 CLIP backbone for a fair comparison.

We also compare a naive editing-based retrieval method: editing an image using IP2P with an instruction and retrieving images using the CLIP image encoder.


\subsection{CIR Datasets.}
\label{subsec:dataset_details_supp}

\paragraph{FashionIQ.}
FashionIQ, the most popular CIR benchmark, has (46.6k / 15.5k / 15.5k) (training / validation / test) images with three fashion categories: Shirt, Dress, and Toptee.
Each category has 18k training triplets and 12k evaluation triplets of \rawtriplet. Examples of $x_c$ looks like: ``is more strappy and emerald'', ``is brighter'' or ``is the same''. The main drawback of FashionIQ is that the dataset is limited to the specific subsets of fashion domains, hence it is not possible to evaluate whether the methods are truly useful for real-world CIR tasks.


\paragraph{CIRR.}

CIRR contains more generic images than FashionIQ. CIRR uses the images from NLVR$^2$ \citep{suhr2018corpus} with more complex and long descriptions. CIRR has 36k open-domain triplets divided into the train, validation, and test sets in 8:1:1 split.
The examples of $x_c$ are ``remove all but one dog and add a woman hugging it'', ``It's a full pizza unlike the other with the addition of spice bottles'', or ``Paint the counter behind the brown table white''.
As the example captions show, one of the main drawbacks of CIRR is that the text queries are not realistic compared to the real user scenario; we may need more shorter and direct instructions. As the text instruction distribution of CIRR is distinct from others (\cref{fig:instruction_stats_supp}), we observe that sometimes CIRR results are a not reliable measure of open-world CIR tasks (\eg, retrieval on LAION).
\citet{circo} also observe another main drawback of CIRR. CIRR contains a lot of false negatives (FNs) due to the nature of the image-text dataset \citep{chun2021pcme, chun2022eccv_caption, chun2023pcmepp}

\paragraph{CIRCO.}

To tackle the FN issue of CIRR, \citet{circo} introduces the CIRCO dataset. This dataset comprises of 1020 queries, where 220 and 800 of them are used for validation and test, respectively. The images are from the COCO dataset \citep{lin2014coco}, where the size of the image database is 120K COCO images.
Example $x_c$ of CIRCO includes ``has two children instead of cats'' or ''is on a track and has the front wheel in the air``. We use mAP scores following \citet{circo} which is known to be a robust retrieval metric \citep{musgrave2020metric,chun2022eccv_caption}.

\paragraph{GeneCIS.} GeneCIS \citep{genecis} is a dataset to evaluate different conditional similarities of four categories: (1) focus on an attribute, (2) change an attribute, (3) focus on an object, and (4) change an object.
The first two categories are based on the VAW dataset \citep{vaw}, which contains massive visual attributes in the wild. The other categories are sampled from the COCO dataset \citep{lin2014coco}.

\subsection{Implementation details}
\label{subsec:implementation_details_supp}
\input{tables/parameters}
We report the detailed hyperparameters in \cref{tab:parameters}. All models were trained using AdamW \citep{loshchilov2017decoupled} with $\beta_1=0.9$ and $\beta_2=0.999$. For computationally efficient training, we precomputed CLIP visual embeddings of the entire image from our training dataset. Since our training dataset was sufficiently large, we did not use any image augmentation methods to extract CLIP visual embeddings. Since the text as a condition can vary each time in training according to the 48 templates (\cref{tab:example_templates_supp}), we do not precompute any textual embeddings. In the case of keyword-based generated triplets, we are able to randomly switch query and modified images during training because the instruction for keyword-based triplets is generated according to the 48 templates.


\subsection{Text encoder details}
\label{subsec:impact_text_encoder_supp}
As shown \citet{balaji2022ediffi}, using a text-oriented model such as T5 \citep{raffel2020t5} in addition to the CLIP textual encoder results in improved performance of text-to-image generation models. Motivated by this observation, we also use both the CLIP textual encoder and the language-oriented encoder. We also observed the positive effect of the text-oriented model and experiment results showed that T5-XL, which has 3B parameters, could improve the performance by a large margin in the overall evaluation metrics. As described in \cref{subsec:implementation_details_supp}, all training text embeddings are extracted at every iteration. To improve computational efficiency, we reduced the number of input tokens of the T5 models to 77, as in CLIP (as shown in \cref{fig:data_stats_supp} and \cref{fig:instruction_stats_supp}, most of the captions in our dataset have lengths less than 77). A single-layer perceptron was employed to align the dimension of text embeddings extracted from T5 XL with that of CLIP large.

In \cref{tab:improved_text_encoder}, when the CLIP textual encoder and the T5-XL were used together, the experimental results improved by a large margin. We suspect that this is because the strong T5 encoder can help the CLIP text encoder to better understand given captions. Interestingly, we observe that using T5 alone degrades the performance even compared to using the CLIP textual encoder alone. We suspect that this is because T5-XL is specified for long text sequences (\eg, larger than 100 tokens) and text-only data. On the other hand, our caption dataset has an extremely short average length (see \cref{fig:data_stats_supp} and \cref{fig:instruction_stats_supp}), which is not specialized by T5. Also, our dataset is based on captions, paired with an image; we also need to consider image information to understand the given caption, but we cannot handle image information alone with T5.

\section{More Experimental Results}
\label{sec:more_experiments_supp}

\subsection{Full experiment results of \cref{tab:main}}
\label{subsec:full_experiment_results_supp}

In this subsection, we report the full experiment results of \cref{tab:main}. For FashionIQ and CIRR datasets, we also report supervised fine-tuning results of \ourdataset-trained CIR methods.

\input{tables/fashion_iq_main}
\input{tables/cirr_main}

\cref{tab:fashioniq} shows the comparison of \ours with baselines on the FashionIQ dataset.
Following the standard choice, we use recall@K as the evaluation metric. ``Zero-shot'' means that the models are not trained on FashionIQ. ARTEMIS and Combiner were originally designed for the supervised setting, but, we trained them on \ourdataset for a fair comparison with our method. Namely, we solely train them on our dataset for the zero-shot benchmark and fine-tune the zero-shot weights on the FashionIQ training set for the supervised benchmark.

\cref{tab:cirr} shows the CIRR results and all experimental settings were identical to FashionIQ. Similar to FashionIQ, \ours also achieves a new state-of-the-art CIRR zero-shot performance.
It is noteworthy that Combiner performs great in the supervised setting but performs worse than \ours in the zero-shot setting. We presume that the fine-tuned Combiner text encoder is overfitted to long-tailed CIRR captions. It is partially supported by our additional experiments on text encoder in \cref{subsec:abl}; a better understanding of complex texts provides better performances.

For both datasets, we achieve state-of-the-art by fine-tuning the Combiner model trained on \ourdataset to the target dataset. It shows the benefits of our dataset compared to the limited number of CIR triplet datasets.

\input{tables/circo_main}
\input{tables/genecis_main}

The detailed CIRCO results are shown in \cref{tab:circo}. In the table, we can observe that in all metrics, \ours achieves the best performances among the zero-shot CIR methods. This result supports the effectiveness of \ours and \ourdataset in real-world CIR tasks.

Finally, we report detailed GeneCIS in \cref{tab:genecis}. In the table, \ours shows the best performance in the average recall. Our method especially outperforms the other methods in ``Change Attribute'', ``Focus Object'' and ``Change Object''. On the other hand, \ours is less effective than Pic2Word and SEARLE in ``Focus attribute''. We presume that it is because the instruction distribution of ``Focus Attribute'' differs a lot from the instruction of \ourdataset. Among other CIR methods in the \ourdataset-trained group, \ours shows the best performances.







\subsection{More qualitative examples}
\label{subsec:more_examples_supp}

\paragraph{Open world zero-shot CIR comparisons with Pic2Word.} We illustrate further comparisons with Pic2Word in \cref{fig:ours_vs_pic2word_more_supp}. Here, we can draw the same conclusions as in the main text: Pic2Word often cannot understand images or instructions (\eg, ignores the ``crowdedness'' of the images, or retrieves irrelevant images such as images with a woman in the last example). All retrieved results in our paper were obtained using Pic2Word trained on the LAION 2B dataset.

\paragraph{More versatile CIR examples on LAION.}
We illustrate more qualitative examples of retrieval results in \cref{fig:more_examples_supp_0}, \cref{fig:more_examples_supp_1}, \cref{fig:more_examples_supp_2}, and \cref{fig:more_examples_supp_3}. We will describe the details of ``Generated by unCLIP'' in the later section.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ours_vs_pic2word_more.pdf}
    \caption{\small {\bf More qualitative comparison of zero-shot CIR for Pic2Word and CompoDiff.}}
    \label{fig:ours_vs_pic2word_more_supp}
\end{figure*}


\subsection{More Ablation study}
\label{subsec:ablation_study_supp}

\paragraph{How to handle text input?}
\input{tables/i2t_retrieval}
\ours does not take CLIP textual embeddings for text guidance as input tokens of the denoising Transformer but as a condition of cross-attention. Our design choice allows for faster throughput compared to the counterpart that takes CLIP textural embeddings directly as input tokens. We compare the impact of different design choices for handling textual embeddings. First, we evaluate the ``Prior'' model which converts CLIP textual embeddings into CLIP visual embeddings and proposed in unCLIP \citep{dall-e2} (we use a public community model \footnote{\url{https://huggingface.co/kakaobrain/karlo-v1-alpha}} because the official model is not yet publically available). Second, we test the ``Prior-like'' model by using the denoising Transformer, but taking text guidance as input tokens instead of cross-attention. We also test two more \ours models from our two-stage training strategy.

To measure the ability to understand raw text, we evaluate the models on image-to-text and text-to-image retrieval benchmarks on the MS-COCO Caption dataset \citep{cococaption}. We also evaluate them on the extension of COCO Caption to mitigate the false negative problem of COCO, namely, CxC \citep{cxc} and ECCV Caption \citep{chun2022eccv_caption}. \cref{tab:i2t_retrieval} shows the average metrics of each benchmark for image-to-text and text-to-image. In the table, we first observe that our design choice is three times faster than the ``Prior-ish'' counterparts by handling textual embeddings with cross-attention. Second, we observe that Stage 1 only \ours shows a better understanding of image-to-caption and caption-to-image retrieval tasks. We speculate that this is because Ours (Stage 1 only) is directly optimized by the image-to-text (ITM) matching style dataset, while Ours (Stage 1 + Stage 2) is also trained with other types of conditions (\eg, masks, negative texts, image conditions). Throughput was measured on a single A100 GPU with a batch size of 32.

In summary, our design choice shows $\times$ 3 faster inference time than the prior model \citep{dall-e2} but better text-to-image retrieval performances on COCO.



\paragraph{Condition strength}

As $w_I$ increases, the generated image embeddings become more dependent on the reference image, while increasing $w_T$ results in a greater influence of the text guidance. However, large $w_I$ and $w_T$ are not always beneficial. If $w_I$ or $w_T$ is too large, it can lead to unexpected results. To find a harmonious combination of $w_I$ and $w_T$, we performed a sweeping process as shown in \cref{fig:heatmaps}. We use $w_I$ as 1.5 and $w_T$ as 7.5 considering the best content-condition trade-off.




\subsection{Image decoding using unCLIP generator.}
\label{subsec:unclip_supp}
unCLIP \citep{dall-e2} consists of a prior module that converts text embeddings into image embeddings, a decoder that converts image embeddings into low-resolution images, and super-resolution models that convert low-resolution images into high-resolution images. As the official unCLIP model is not publicly available, we employ the community version of unCLIP. Fortunately, since the community unCLIP model uses embeddings from CLIP-L/14, we can directly use this model to generate images from the image embeddings generated by our CompoDiff. To do this, we simply replace Prior with CompoDiff. The generated images are shown in \cref{fig:more_examples_supp_0}, \ref{fig:more_examples_supp_1}, \ref{fig:more_examples_supp_2}, and \ref{fig:more_examples_supp_3}. To clarify, the unCLIP model is trained for \textbf{text-to-image} generation, not to edit input images and our CompoDiff generates image embeddings rather than generating images. As shown, the results are very promising. It seems that incorporating unCLIP into the search service could potentially improve the user experience by generating images when the desired search results are not available.

\clearpage
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/heatmap_zeroshot.png}
        \caption{Zero-shot results}
        \label{fig:zero-shot-results}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/heatmap_finetune.png}
        \caption{Supervised results}
        \label{fig:supervised-results}
    \end{subfigure}
    \caption{\small {\bf Fashion IQ CIR results by adjusting $w_T$ and $w_I$.} A red/blue cell denotes a higher/lower score.}
    \label{fig:heatmaps}
\end{figure}%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example_4.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours.} Using the transformed image feature by \ours, Generated images using unCLIP and top-1 retrieved image from LAION.}
    \label{fig:more_examples_supp_0}
\end{figure}

\clearpage

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours (Continue).}}
    \label{fig:more_examples_supp_1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example_2.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours (Continue).}}
    \label{fig:more_examples_supp_2}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/generation_example_3.pdf}
    \caption{\small {\bf Generated vs. retrieved images by \ours (Continue).}}
    \label{fig:more_examples_supp_3}
\end{figure}

\clearpage

\end{document}