
We proposed an interpretable reinforcement learning approach to multi-echelon inventory optimization. Our approach combines the interpretability of static inventory optimization methods with the flexibility of neural policies obtained trained via reinforcement learning with proximal policy optimization (PPO). We have demonstrated that Neural Additive Models (NAM) are good candidates to induce interpretability to an agent's policy. In a benchmark environment, the resulting policy produces results on-par with a neural black-box architecture such as a simple feed-forward network. We use the resulting feature importances and shape functions of the NAM policy to investigate the behavior of the agent in an example rollout and demonstrate how the complex interaction of ordering in a multi-echelon environment can be broken apart. This clearly demonstrates the use of interpretable policies to communicate dynamic policies to relevant stakeholders. 

This work opens up the study of many real-world applications of interpretable reinforcement learning for pressing supply chain and other operations research problems in the current economy. In order to unlock the full potential of NAM-PPO for these problems, we plan to analyze the addition of higher-order NAM features to the policy as well as extending the action space for more fine-grained supply chain control in future works. Goals are to understand long-time generalization, disruption robustness, as well as managing more complex supply chains. 

\section*{Acknowledgements}
This work has received funding from the German Federal Ministry for Economic Affairs and Climate Action as part of the ResKriVer project under grant no. 01MK21006H. We thank Ziyad Sheebaelhamd for in-depth discussions on Reinforcement Learning related problems. We thank Johanna Kim Kippenberger, John-Christopher Maleki and Michael Dominik GÃ¶rtz for their fruitful discussions and insights on supply chain optimization. 





