
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth, height=190pt]{figures/method/main_figure.pdf}
    \caption{(Left) An agent, modeled as a NAM, is optimized via RL to produce the reorder amount $R_i$ for each stage $i$ of a multi-echelon supply chain in a simulation. The agent observes the current state of the inventory $I_i$ at each stage $i$ and recalls the last actions to determine the next one. (Right) Once  deployed, the shape functions may be extracted for each output of the NAM, since each feature contributes independently to the target. The shape functions allow the Supply Chain Manager to inspect how each feature in the observation space impacts the decision of the reorder amount.}
    \label{fig:method:overview}
\end{figure}

At a high level, our setup consists of three key components: the environment simulator providing us with a corresponding supply chain, an RL training framework to determine a dynamic supply chain policy, and an interpretable model to parameterize the policy. We visualize these components in Figure~\ref{fig:method:overview}.




\paragraph{Supply chain environment.} To keep the environment simple and focus on the use of interpretability when learning neural policies, we restrict the problem to linear-chain multi-echelon inventory optimization (MEIO) as depicted in Figure~\ref{fig:method:overview}.


To solve the ordering policy of this and similar supply chains, the supply chain manager typically implements a base-stock policy: Each facility keeps a minimum stock of the items and places a restocking order with their supplier whenever the inventory drops below the base-stock threshold. While this policy is able to meet the requirements of being able to fulfill the orders at the retailer level, it is apparent that a static base stock policy can also be suboptimal, as, e.g., a large base-stock in times of low demand leads to a high holding cost overhead. A dynamic and at the same time simple-to-understand policy could hence help in reducing the supply chain costs.

\paragraph{Training a dynamical supply chain using Proximal Policy Optimization.} Since its inception, Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} has quickly become the workhorse for many RL applications. It has been successfully scaled to large games such as playing Dota 2~\citep{berner2019dota} and Multi-Agent Hide \& Seek~\citep{Baker2019EmergentTU}, but also applications such as automated stock trading~\citep{Liu2020FinRLAD} or Neural Architecture Search~\citep{Zoph2017LearningTA}. PPO is an on-policy algorithm that improves Trust Region Policy Optimization (TRPO)~\citep{schulman2015trust} by removing the computational bottlenecks of the latter. Specifically, we use the clip version of PPO which regularizes the update size of the policy's parameters based on the current policy and the advantage given the current state by clipping the reward signal,
\begin{align}
    \mathcal{L}^\textrm{PPO}(\theta, \theta_\textrm{old}) = \mathbb{E}\left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\textrm{old}}(a_t|s_t)}A^\pi(s_t), \textrm{clip}\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\textrm{old}}(a_t|s_t)}, 1-\varepsilon, 1+\varepsilon \right)A^\pi(s_t)\right) \right] \EQDOT
\end{align}

Our implementation of PPO is provided by {Stable-Baselines3}~\citep{Raffin2021StableBaselines3RR}, which uses distinct networks for the policy $\pi_\theta(a|s)$ as well as the value function $v_\phi(s)$. The advantage values are generated using Generalized Advantage Estimation~\citep{Schulman2015HighDimensionalCC}, $A^\pi(s) = R - v_\phi(s)$, where $R$ is the discounted reward. We note that the advantage function's sole purpose is to support the computation of the policy gradient during training. In particular, it has no bearing on the agent's action selection which is fully governed by the policy network. 

Within the standard RL approaches, the policy is parameterized using fully connected multi-layer perceptrons (MLP). As a consequence, interpreting and explaining the action selection of the agent is challenging, thus limiting their utility in real world scenarios that require stakeholder management, such as in critical supply chains. To address this issue, we make use of interpretable models to replace the opaque MLP policy networks with interpretable Neural Additive Models (NAM)~\citep{agarwal2021neural}. In section~\ref{sec:experiments} we compare the standard MLP-PPO to its interpretable NAM-PPO counterpart. Here we make use of the fact that {Stable-Baselines3} allows us to separate policy and value networks, as we use the same value network design for both policy setups for fair comparison.

\paragraph{Neural Additive Models as interpretable supply chain policies.} \label{sec:method:nam}As pointed out before, our goal is to design an interpretable policy network to aid communicating the agent's decision to supply chain managers. We use NAMs, a form of Generalized Additive Model (GAM)~\citep{hastie1987generalized,hastie2017generalized}, whose functional form is given by
\begin{equation}
    \hat{y} = \beta + \sum\nolimits_{i=1}^N f_i(x_i) \EQCOMMA
\end{equation}
where $(x_i)_i \in \mathbb{R}^N$ is a feature vector of $N$ scalar-valued entries, $f_i: \mathbb{R} \to \mathbb{R}$ are univariate, continuous functions describing the individual feature contribution, $\beta \in \mathbb{R}$ is a global offset or bias term, and $y$ denotes the target variable.
GAMs are an extension of linear regression and add non-linear feature contributions while retaining the ease of interpretability. 
The model is interpretable by tracing its so-called \emph{shape functions}, i.e.\ the graphs $\left\{\big(x_i, f_i(x_i)\big), x_i \in \mathcal{D}_i\right\}$, in the respective domain of interest $\mathcal{D}_i \subset \mathbb{R}$.
The shape functions describe the contribution of the corresponding features \emph{exactly} as the prediction is given by a simple point-wise addition of feature contributions.
This way, domain experts or other stakeholders can inherently understand the model's decision making or gain insight into the data the model was trained on to validate or expand expert knowledge.

Until recently, the feature contributions $f_i$ were modelled by splines and then replaced by boosted decision trees which perform more favorably in many applications~\citep{lou2012intelligible,lou2013accurate,caruana2015intelligible}.
NAMs follow the same idea but express the single-feature contributions $f_i$ using MLPs. 
Hence, NAMs are differentiable by construction and benefit from recent advances in deep learning and hardware acceleration. Furthermore, the shape functions can also be used to deploy the model in an efficient way, e.g., via look-up tables. In that case, the actual NN implementation of the NAM can be traced and discarded after training.
Note that the input features and target are standardized for numerical stability, for example by using a min-max- or z-scaler.

Finally, NAMs are easily adaptable to multi-task learning with weight sharing between tasks: Each task target $y_t$, $t=1, ..., T$, is modelled as
\begin{equation}
    \hat{y}_t = \beta_t + \sum\nolimits_{i, s} w_{t, i, s}f_{i, s}(x_i) \EQCOMMA
\end{equation}
where $s=1,...,S$ denotes a subnet index, $f_{i,s}$ are parametrized as MLPs, and the scalar weights $w_{t,i,s}$ are also trainable. Note, that each task-specific feature contribution or shape function $f_{t,i}$ is effectively given by a weighted sum
$f_{t,i}(x_i) = \sum\nolimits_{s} w_{t, i, s}f_{i, s}(x_i)$
of subnet contributions $f_{i,s}$. Using weighted subnets as opposed to training independent feature contributions for each task allows for efficient parameter sharing of feature contributions across tasks. For more technical details, we refer to the original work on NAMs~\citep{agarwal2021neural} and the follow-up work~\citep{chang2021node}.






    



