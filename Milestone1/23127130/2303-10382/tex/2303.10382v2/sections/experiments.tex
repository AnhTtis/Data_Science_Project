

In the following, we provide details on our hyperparameter optimization and evaluation strategy in Section~\ref{sec:exp-setup}. 
In Section~\ref{sec:exp:benchmark_comparison}, we compare two PPO-based setups using a standard MLP policy and an interpretable NAM policy head-to-head and leverage the interpretability to provide insights on the learned strategies. 
We refer to these setups as MLP-PPO and NAM-PPO, respectively.

\subsection{Experimental Setup}\label{sec:exp-setup}



\paragraph{MEIO in OR-Gym.} Our environment is provided by OR-Gym~\citep{hubbs2020or}. More specifically we use \textsc{InvManagement-v0}, which provides an MEIO problem consisting of a linear chain with three echelons. Each echelon is characterized by its storage capacity, holding and fulfillment price, and demand fulfillment lead time. In addition to the previous parameters, the environment includes an order backlog, i.e. if an echelon is unable to fulfill an order at a given time, it is added to a backlog and fulfilled at a later time at reduced price instead of writing off the unfulfilled orders. The randomness in the environment is introduced via a stochastic customer demand at the lowest echelon, i.e. the retailer, of the supply chain. The retailer tries to fulfill the customer demand from its inventory and is able to post a replenishment order to its supplier which takes a certain lead time to arrive. Similarly, every supplier has an inventory and can order from a single upstream supplier. The first supplier in the supply chain has no source to order from, but accesses an infinitely large inventory.

During the experiments we found two key settings in the environment to enable the agent to learn meaningful policies. First, we note that we deal with finite time horizons and hence can remove the reward discounting factor. This change is critical for the agent to recognize the long-term benefit of replenishing stocks, as otherwise the initial (undiscounted) order and inventory prices outweigh the realizable downstream revenue and learned policies degenerate. Second, we add additional stochasticity by randomizing the initial inventory using samples from a Gaussian distribution centered at the default initial OR-Gym inventory levels (I0: 100, I1: 100, I2: 200) and a standard deviation of 50 units.

As inputs to the RL agent, we use the OR-Gym default observation space containing the current inventory state $I_i$ in each echelon (3 values) and the actions of the last 10 time-steps (30 values). Hence, the observation space is 33-dimensional in total. The action space at each time-step is given by the agent's reorder quantity per echelon, resulting in a 3-dimensional vector. Finally, the reward is defined as the revenue generated by fulfilling customer demand through the retailer, minus the inventory cost at each echelon. For more details we refer to the OR-Gym work by~\citet{hubbs2020or}.





\paragraph{Hyperparameter Configuration and Optimization.} There are multiple hyperparameters used throughout the experiments.
As the evaluation is restricted to a single example supply chain, the number of target tasks (i.e., the dimension of the action space) for the NAM and MLP policies is fixed and equal to 3.
For both considered policy network architectures, the PPO critic is a simple MLP with two layers of fixed width.
Furthermore, we utilize ELU activation functions throughout the policy and critic networks as we have found the one originally proposed in the NAM context, the so-called Exp-centered Unit (ExU)~\citep{agarwal2021neural}, as well as conventional Rectified Linear Units (ReLU) to be unstable. We leave the investigation of normalization layers to mitigate instabilities to future work. The depth and width of the MLPs, for MLP-PPO as well as NAM-PPO, are determined through hyperparameter optimization.
Note, that both the actor policy as well as the critic operate directly on the raw features, e.g.\ no shared feature extractor is used.
While this might be less parameter-efficient, it is required to achieve interpretable feature contributions using the NAM actor.


We optimize the hyperparameters of the used optimizer, PPO, and the policy networks using Random Search~\citep{bergstra2012random}. 
In total, 30 hyperparameter configurations are sampled. We evaluate each configuration by training three policies based on different random initialisations and averaging their cumulative reward over 60 steps in a validation environment with a fourth, fixed seed. The used hyperparameters and corresponding search spaces for optimization are presented in detail in Appendix~\ref{app:sec:hyperparameter_optimization}. All remaining hyperparameters of PPO follow the {Stable-Baselines3} defaults.


\paragraph{Training and Evaluation.}\label{sec:exp:evaluation}
We follow the recommendations of~\citet{agarwal2021deep} for a reliable evaluation of RL policies and use the interquartile mean (IQM) with 5\% and 95\% bootstrap confidence bounds for tabular comparisons of different methods across seeds and agent rollouts.  More specifically, we retrain the incumbent of the hyperparameter optimization using 20 different random seeds and roll out each of the resulting agents 50 times. The IQM is hence computed based on 1000 evaluations in total. Each agent is trained and evaluated for an episode length of 60.

\begin{figure}[t]
     \hfill
     \centering    
     \hfill
       \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/experiments/reward_trajectories/nam/reward_per_step_time_31-01-2023_15-55-30.jpg}
         \caption{}
         \label{fig:exp:rollout_per_step}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/experiments/feature_importance.pdf}
         \caption{}
         \label{fig:exp:nam_feature_importance}
     \end{subfigure}
     \hfill
     \hspace{5mm}
    \caption{(a) Example per-step reward trajectory of the NAM agent. Shown are the 50 rollouts for one of the 20 trained agents. (b) Feature importance in the observation space, the median importance per feature across the 20 model trainings is shown, shaded is the standard deviation.}
\end{figure}

\subsection{Results}\label{sec:exp:benchmark_comparison}

For the default training and evaluation setup, we find that the IQM of the NAM-PPO is 792.69, with a confidence interval of [779.96, 804.90], and 730.27  for MLP-PPO with a confidence interval of [709.12, 750.38]. 
Hence, NAM-PPO performs slightly better than the standard MLP-PPO in the considered example.
This is remarkable as the NAM agent does not consider higher-order feature interaction and can, by design, model less complex policies as compared to an MLP agent.
In particular, a NAM-based policy can only linearly combine the different inventory state contributions or those of the previous reordering quantities which can, in principle, interact in highly complex ways in the MLP-based policy.
Hence, these initial results encourage further investigation into NAM-based RL.

Example rollouts of the NAM-PPO policy for one of the 20 seeds is shown in Figure~\ref{fig:exp:rollout_per_step}. We observe that the NAM policy learns to produce an oscillating reward/profit sequence. 
Ostensibly, the agent waits for inventories to empty (decreasing cost) before replenishing stocks (jump in cost) imitating a classic base-stock policy, which -- given the simplicity of the considered supply chain -- may well be optimal. However, it is difficult to say whether the learned policy is based on inventory thresholds, a schedule, or other heuristics.

With the help of NAM-based policy networks, we can assess the relevance of different environment aspects for overall decision making, by averaging and comparing the respective absolute feature contributions across a set of states of interest. We can further investigate the relationship between agent behavior and any individual environment feature by examining the matching shape function relating variations in the feature value to increased or decreased task (action) contribution.

Figure~\ref{fig:exp:nam_feature_importance} depicts the feature importance of each variable in the observation space estimated by the NAM-based policy for the individual actions $R_0$, $R_1$, and $R_2$, denoting the reordering quantities of the echelon inventories $I_0$, $I_1$, and $I_2$, respectively.
The actions from previous time steps are denoted as $a_i(t-c)$, $i=1,2,3$, where $c$ denotes the time delay. For example, $a_2(t-1)$ denotes the reordering quantity of echelon $2$ in the previous time step.
For $R_0$ (top row), the reordering amount at the producer closest to the customer, we see that inventories $I_0$ and $I_1$ are the most important features. This makes sense as $I_1$ determines how much can be reordered by the producer with inventory $I_0$ from the preceding producer in the supply chain. 
Similarly, $R_1$ (middle row) is influenced most strongly by the current inventory $I_2$. Interestingly, the current level $I_1$ of the intermediate supplier himself is of little consequence compared to the retailer stock $I_0$ which is an indicator for future orders.
For $R_2$ (bottom row), the reordering quantity from the producer with infinite inventory, the current inventory states $I_i$ are almost negligible whereas the actions taken in the previous time-steps are most important.
This seems reasonable, as it likely is important to keep track of the previous actions taken for $R_2$, which may indicate what quantity of goods is still in the pipeline and will arrive at a later time, due to lead times in the delivery. 
This trend seems to be observable for all reordering quantities, in particular for those with increasing distance to the customer. However, more detailed investigations using more complex supply chains are required.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/experiments/NAM-PPO-shape_functions.png}
    \caption{Example shape functions for target action $R_0$ corresponding to the reorder quantity of the first stage in the supply chain extracted from one of the trained NAMs in Section~\ref{sec:exp-setup}. Shown are the three features with highest feature importance in descending order from left to right. The shaded areas denote the data density in this interval, which, in the RL case, corresponds to how often the agent saw the corresponding feature in this state.}
    \label{fig:shape_functions}
\end{figure}

Finally, the interpretation using feature importance, while offering first insights into the policy's behavior, is limited in its expressiveness.
To this end, the shape functions of the NAM-based policies can be visualized as shown in Figure~\ref{fig:shape_functions}.
Here, we show the top-3 reordering features, i.e. it excludes the inventory observations, of the reordering history $R_0$ of the learned NAM-based policy. Recall, the shape functions exactly describe the contribution of the corresponding feature to the target, here, the reordering quantity $R_0$.
The background shading indicates data density for certain values and can be seen as a proxy for the state-visit distribution of the deployed policy. This offers first insights into the scenarios encountered by the agent and may also serve as an indicator for relevant parts of the state space.
In our example we see that order levels $R_0$ typically remain low (order quantity $0$--$15$), with sporadic increases (up to $60$). The main driver for these increases are immediately preceding orders $a_0(t-1)$ which have an amplifying effect, which we hypothesize hints to the fact that high preceding orders may indicate a backlog of unfulfilled demand. This effect saturates to avoid excessively large single orders. To avoid continuously overstocking, this contribution is counteracted by historic order levels further back $a_1(t-7)$ where larger orders have a dampening effect on orders at echelon $1$ to compensate for overstocking for a reduced demand. This type of analysis provides a simple, yet effective tool into understanding the complex behavior of neural policies in RL, that is also accessible to non-practitioners.

We provide additional preliminary evaluations in Appendix~\ref{app:further-evaluation}.
In particular, we analyze the temporal stability and extrapolation of the derived policies by evaluating them for more environment steps than they encountered during training. Moreover, we evaluate the robustness of both policy variants towards disruptions in the demand distribution.










    
