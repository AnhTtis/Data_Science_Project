
\paragraph{Inventory Optimization}
Traditionally, inventory optimization has been approached by finding a static base-stock policy~\citep{snyder2019fundamentals}. This strategy reorders whenever the inventory stock levels fall below a certain threshold.
The base-stock levels are commonly determined via Mixed-Integer Linear Programs (MILP). Due to its simplicity, this approach offers the advantage of being interpretable and the resulting policy can easily be communicated to relevant stakeholders. We refer to~\citet{snyder2019fundamentals} for a thorough introduction to static-policy inventory optimization.

RL has been proposed to make inventory optimization more responsive towards changes in the environment, due to its flexible dynamic policies. One of the earliest works on RL in inventory optimization is by~\citet{giannoccaro2002inventory} which models replenishment orders via tabular Q-learning to optimize the profit across the supply chain. Subsequently,~\citet{mortazavi2015designing} use Q-learning via NNs to maximize the profit of a four-echelon supply chain.
These early works address single- rather than multi-product supply chains and consider only a single retailer, while usually warehouses supply their products to various retailers. \citet{sultana2020reinforcement} deal with these challenges by optimizing supply chains of 50 to 1000 products with three retailers.

\citet{hubbs2020or} proposed the first RL benchmark for operations research including environments for inventory optimization. It is based on the OpenAI Gym environments~\citep{brockman2016openai} making it easy to benchmark a variety of RL algorithms via RLlib~\citep{liang2018rllib} or Stable Baselines~\citep{stable-baselines, Raffin2021StableBaselines3RR}.
The only other work we are aware of investigating a similar interpretable inventory optimization is~\citet{bravo2020mining}. The authors propose to identify optimal order strategies through collecting data generated by an optimal policy found via Dynamic Programming. It then frames the decision whether to reorder as a classification problem and fits a separate model to approximate the reorder quantity in the case a reorder takes place.

For a comprehensive introduction to RL-based inventory optimization, we refer the interested reader to~\citet{boute2021deep}.

\paragraph{Interpretable Reinforcement Learning}
To distinguish between \textit{interpretability} and \textit{explainability}, we follow the definitions by~\citet{glanois2021survey}. 
They define \textit{interpretability} as an architectural design of a machine learning model that allows humans to intuitively understand the model's decision making.
In contrast, \textit{explainability} is a post-hoc process that seeks to justify the prediction of a model after the fact.
Following this definition, explainable machine learning can be viewed as a collection of model-agnostic methods to reason about the model's decision whereas interpretable machine learning utilizes specific model architectures to enable human inspection of its decisions.

While interpretable and explainable artificial intelligence method have been gaining attention in recent years, research has mostly focused on applications such as computer vision or natural language processing using supervised or self-supervised architectures.
In contrast, interpretability and explainability in RL have attracted only little attention. In this context, \citet{verma2018programmatically} represent policies using a high-level description language which is found via a custom search method. As a result of the description language, the derived policies may be verified using traditional program verification tools. Their policy provides more stable steering directions in a car steering task and is much more robust to noise as compared to previous approaches.
\citet{coppens2019distilling} develop a method to learn soft-decision trees via distillation of a NN agent. Using the Mario benchmark environment~\citep{karakovskiy2012mario}, they record the state-to-action probability mapping of a trained NN policy. Subsequently, they fit a soft-decision tree on the collected data. We refer to~\citet{glanois2021survey} for an in-depth review of interpretable RL.
