\section{Appendix}


\subsection{Further Evaluations}\label{app:further-evaluation}

\paragraph{Temporal stability}
    
We compare the temporal stability of a NAM-based policy with that of an MLP-based policy. 
Recall, the policies investigated in Section~\ref{sec:exp:benchmark_comparison} were trained for an episode length of 60 environment steps. Here, we vary the episode length between 30 and 420 environment steps and analyze whether the same policies continue to be profitable, i.e., if the cumulative reward remains positive. The results are shown in Figure~\ref{fig:rollout_length_analysis}. 
    
We find that the NAM policies generalize worse to an increased episode length than standard MLP policies. 
In fact, the NAM policy fail to remain profitable in the long run while the MLP policy remains stable and profitable.
We hypothesize that this may be due to the NAM's neglect of higher-order feature interactions which could be necessary to produce a stable policy.
However, this remains speculative and further works are necessary to investigate the long-term behavior of trained NAM-PPO policies.
    
\begin{figure}[!h]
\centering
\includegraphics[width=0.6\linewidth]{figures/experiments/rollout_length_analysis.pdf}
\caption{Robustness of the learned MLP- and NAM-PPO policies towards increasing the roll-out length in the environment.}
\label{fig:rollout_length_analysis}
\end{figure}


\paragraph{Robustness towards disruptions in customer demand}

Next, we evaluate the robustness of the trained MLP- and NAM-PPO agents under sudden changes in customer demand, similarly to how they occur in a crisis. 
At time step $c$ in the environment, we add a disruption customer demand distribution to the default customer demand distributions.
The default distribution is modelled as a Poisson distribution with parameter $\lambda_{\textrm{s}}=20$.
The disruption distribution is also modeled as a Poisson distribution with $\lambda_{\textrm{d}}=s_{\textrm{d}} \cdot \lambda_{\textrm{s}}$, where $s_{\textrm{d}}$ denotes the disruption strength.
The samples from the crisis distribution are exponentially attenuated to zero over time with an attenuation factor of 0.8. Therefore, our disrupted demand changes suddenly and subsequently returns to the original demand distribution.

In all our experiments we set the time step $c$, at which the demand distribution changes, to be half the total number of environment steps in the experiments. This gives the policy enough time to stabilize (hence not purely relying on the initial inventory) as well as time to potentially recover.

The results are shown in Table~\ref{tab:customer-demand-disruption}. We see that for both MLP- and NAM-PPO the IQM of the reward degrades very similarly as we increase the disruption strength. In Figure~\ref{fig:customer-demand-disruption-60} we observe the same behaviour as reflected in the per-step reward, where the change in demand affects both MLP- and NAM-based agents.
    Overall, we find that both MLP- and NAM-PPO agents are similarly affected by changes in the demand distribution. 

Finally, we retrained the same PPO agents but this time on environments in which we introduce the same kind of disturbances ($s_{\textrm{d}}$ = 1, $c$ = 30) This can be interpreted as a form of hardening the RL policies towards these disruptions. Our preliminary results were collected over 5 random seeds with 50 evaluations each and are shown in Table~\ref{tab:customer-demand-disruption} (denoted as \emph{hardened}). They indicate that the disruptions in the environment made both the MLP- and NAM-based PPO policies more brittle than without introducing them.
That is, the hardening decreases the performance and makes the policies more susceptible towards the changes in the demand. 
Yet, MLP-PPO performs noticeably better than the NAM-based policy.
Overall, these results are surprising and require a more detailed investigation in future works.
    
\begin{table}
\centering
\small
\begin{tabular}{@{}S[table-format=1.1]llll@{}}
        \toprule
        {Disruption} & 
        NAM &
        MLP & 
        NAM hardened & 
        MLP hardened \\
        {strength $s_{\textrm{d}}$} & 
         &
         & 
        (5 seeds) & 
        (5 seeds) \\ \midrule
        0.0                                                           & 792.69  {\tiny {[}779.96, 804.9{]}}  & \phantom{--}730.27 {\tiny {[}709.12, 750.38{]}}  & \phantom{--}320.81 {\tiny {[}259.13, 379.49{]}}    & \phantom{--}605.72 {\tiny {[}561.15, 644.60{]}}     \\
        0.5                                                              & 804.21  {\tiny {[}788.19, 819.74{]}} & \phantom{--}718.28 {\tiny {[}690.88, 743.25{]}}   & \phantom{--}237.04 {\tiny {[}168.71, 303.13{]}}    & \phantom{--}556.71 {\tiny {[}501.31, 606.07{]}}    \\
        1.0                                                              & 727.32  {\tiny {[}707.34, 746.91{]}} & \phantom{--}629.68 {\tiny {[}599.39, 658.15{]}}   & \phantom{--}117.03 {\tiny {[}46.74, 185.30{]}}      & \phantom{--}451.96 {\tiny {[}391.87, 506.09{]}}    \\
        2.0                                                              & 490.14  {\tiny {[}469.41, 510.89{]}} & \phantom{--}388.06 {\tiny {[}358.38, 416.84{]}}   & --139.57 {\tiny {[}--209.49, --72.15{]}}  & \phantom{--}203.02 {\tiny {[}141.43, 260.71{]}}    \\
        4.0                                                              & --26.96 {\tiny {[}--48.42, --5.61{]}}   & --126.09 {\tiny {[}--155.18, --98.26{]}} & --661.41 {\tiny {[}--731.12, --593.59{]}} & --320.04 {\tiny {[}--382.29, -261.65{]}} \\ \bottomrule
\end{tabular}
\caption{Evaluation of the robustness towards disruptions in the demand distribution for 60 environment steps. We increase the disruption strength $s_d$ and show the IQM of the total reward. The experiment was carried out for each of the 20 models and over 50 rollouts each. The 5\% and 95\% bootstrap confidence intervals of the IQM are given in brackets.}
\label{tab:customer-demand-disruption}
\end{table}


\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/experiments/disruption_analysis_60.pdf}
\caption{IQM of the per-step reward over an environment rollout for different disurption strengths. The dashed vertical line indicates the time step of the disruptive demand distribution change. The disruption factor $s_d$ is denoted above each subplot.}
\label{fig:customer-demand-disruption-60}
\end{figure}



\newpage
\subsection{Hyperparameter Details and Optimization}\label{app:sec:hyperparameter_optimization}

The hyperparameters and search space for hyperparameter optimization used throughout the provided experiments are summarized in Table~\ref{tab:hyperparameter_search_space}.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{@{}llll@{}}
        \toprule
        Type & Hyperparameter                              & Value / Range            & Scaling   \\ \midrule
        Optimizer  & Learning rate                               & $[10^{-4}, 10^{-3}]$ & logarithmic  \\[0.75mm]
                   & Batch size                                & $[32, 128]$   & linear \\[2mm]
        Actor MLP       & \# of hidden layers                          & $[1, 4]$       & linear \\[0.75mm]
                    &    \# of neurons per layer              & $[8, 32]$      & linear \\[0.75mm]
                     &    Activation functions              & ELU      & fixed \\[2mm]
        Actor NAM       & \# of hidden layers                          & $[1, 4]$       & linear \\[0.75mm]
                    &    \# of neurons per layer              & $[8, 32]$      & linear \\[0.75mm]
                    &    \# of subnets $S$               & $30$      & fixed \\[0.75mm]
                    &    Hidden activation functions              & ELU      & fixed \\[2mm]
        Critic MLP       & \# of layers                          & $2$       & fixed \\[0.75mm]
                    &    \# of neurons per layer              & $64$      & fixed \\[0.75mm]
                     &    Activation functions              & ELU      & fixed \\[2mm]
        PPO     & \# of epochs                                & $[2, 51]$   & linear \\[0.75mm]
             & \# of steps                        & $2048$      & fixed \\[0.75mm]
             & Entropy coefficient                         & $0.01$      & fixed \\[0.75mm]
                & Gamma                         & $0.99$      & fixed \\\bottomrule
    \end{tabular}
    \caption{Hyperparameters used in all experiments. Hyperparameter optimization via Random Search is performed on non-fixed parameters in the specified value range. The search space is chosen comparably small in order to not introduce immense computational overhead.}
    \label{tab:hyperparameter_search_space}
\end{table}