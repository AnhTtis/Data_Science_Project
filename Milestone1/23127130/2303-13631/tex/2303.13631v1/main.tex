

% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.1 distribution.
%   Version 4.1r of REVTeX, August 2010
%
%   Copyright (c) 2009, 2010 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.1
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commazands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%showpacs,preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}

\preprint{APS/123-QED}

\title{In-depth analysis of music structure  as a self-organized network}% Force line breaks with \\


\author{Ping-Rui Tsai$^{1}$, Yen-Ting Chou$^{1}$, Nathan-Christopher Wang$^{2}$, Hui-Ling Chen$^{3}$, Hong-Yue Huang$^{1}$,  Zih-Jia Luo$^{4}$, and Tzay-Ming Hong$^{1\ast}$}
\affiliation{%
$^1$Department of Physics, National Tsing Hua University, 
Hsinchu 30013, Taiwan, R.O.C}
\affiliation{%
$^2$College of Pharmacy, University of Michigan, Ann Arbor, MI 48109 U.S.A.}%
\affiliation{%
$^3$Department of Chinese Literature, National Tsing Hua University, 
Hsinchu 30013, Taiwan, R.O.C}
\affiliation{%
$^4$Advanced Semiconductor Engineering, INC., Kaohsiung 76027628, Taiwan, R.O.C}


\begin{abstract}
Words in natural language not only transmit information, but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio to text.  The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipf's law for the frequency and rank of clustering coefficient enables us to generate and regard the semantic relationships as word. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions to the complex network behind music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the  applications of Natural Language Processing (NLP), but also  an easier and more objective way to analyze the evolution and development of  music.
 
%\begin{description}
%\item[Usage]
%%Secondary publications and information retrieval purposes.
%\item[PACS numbers]
%May be entered usintog the \verb+\pacs{#1}+ command.
%\item[Structure]
%You may use the \texttt{description} environment to structure your abstract;
%use the optional argument of the \verb+\item+ command to give the category of each item. 

%\end{description}
\end{abstract}

\pacs{Valid PACS appear here}% PACS, the Physics and Astronomy
                             % Classification Scheme.
%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

Music is often considered as a form of natural language \cite{Music_Nature1,Music_Nature2,Music_Nature3} because it exhibits the ability to adapt \cite{Nature1} and co-evolve with the human civilization. According to Soviet musicologist Henry Orlov, music can participate in communication and unite people through a single emotion. 
The development of languages and music can be categorized into distinct stages that reflect the historic events of each era, and both originate from the imitation of sounds from the environment. Music, like language, has developed its own notation and system of reading and writing. The meaning conveyed by music is less precise than spoken language due to its lack of tenor and vehicle \cite{tenor1,tenor2}. Despite this ostensible weakness, music is no less capable of evoking our memories of specific experiences \cite{music_memory}.
By using magnetoencephalography (MEG), it has been confirmed that music and language are governed by the same mechanism in the cerebral cortices.  This implies the similarity in their processing of data that are transmitted through the sounds associated with spoken language and music harmony \cite{music_text1}. Although language is rarely expressed through singing or playing an instrument like music, they both make use of the same cognitive mechanism which gives rise to the active discipline of ``Shared Synthetic Integration Resource Hypothesis" \cite{music_text2}.

Let's proceed to compare their structure. Both involve minimal units, such as words and chunking, that serve as the building blocks to construct the corpus and scores through ever larger hierarchical units, i.e., phrases/sentences and the  deep structure \cite{structure1,structure2}. It is equally interesting to look into the correlation within the same unit and across different units. With the development of statistical linguistics, Zipf \cite{Zipfs1,Zipfs2} empirically found that the frequency-rank distribution of corpus and natural language utterances follows the power law: $y=a/x^{b}$ where $a,b$ are constants. This distribution was later established to be prevalent in the ranking of many natural and man-made systems, such as web link \cite{Power1} and brain functional network \cite{Power2,Power3}. 
Although Zipf's law has been confirmed to exist in musical composition, there is no consensus on what unit plays the role of word. For example, it has been proposed via segmentation aided by the application of equal temperament to pitch, timbre, and loudness or through binary coding on the power spectrum \cite{zipfs_music1,zipfs_music2}. 

In this Letter, we will answer five questions: (1) How to generate text from music using all of the essential elements in music without any simplifications?
(2) Trained musicians are able to distinguish music from different periods, but how to achieve such a feat objectively by deep learning and with less information than commonly needed?
(3) Current methods of deep learning analyze music via training through big data. However, the process is like a black box that is difficult to decipher. How to overcome such a weakness while retaining the efficiency of deep learning? 
(4) Throughout history, musicians have debated how to distinguish music from non-music. Can we offer a quantitative and more objective definition to settle the debate?
(5) How does music evolve and develop? And how do composers interact with and influence each other? These questions are normally answered by analyzing the structure, motifs, and melody of each piece that requires profound knowledge of each composer. Is there an easier and more objective way to achieve this? 

 Our approach is based on using the basic elements of music, such as rhythm, timbre, pitch, melody, articulation, meter, and tempo. After analyzing these items, it can be found that they are functions of and can be modeled by scales, time and volume, which will, therefore, be termed Essential Elements (EE). The scale and time, along with its corresponding beat, are important representations of the universal features of music\cite{UF}.

Firstly, we calculate the changes between these EE and set a threshold below which order can be achieved in the network representing each musical score. This approach is consistent with the structuralism of musical material form\cite{Syntax1}, and also follows the spirit of natural language modeling\cite{text_mining1,text_mining2}, such as linguist John Rupert's interpretation of ``You shall know a word by the company it keeps"\cite{John}. This spirit expresses the role of endowing meaning to a word via its interaction with neighbors. Secondly, through the empirical Zipf's law in linguistics, we study the statistics of word frequency and select weights and thresholds that can best organize the distribution. Finally, we can understand the structure between music from different periods by this method, and examine differences in  texts by placing words in the corresponding scales and time. We can also compare the properties of musical and non musical  structures, as the composer Edgar Var\'ese said, ``Music is organized sound"\cite{Edgar}.

 
 To begin with, we transformed the time-frequency representation of an audio file into a scale-time-volume representation. The scale consists of 84 keys, which correspond to those of a piano in equal temperament and cover a frequency range from 1 to 8192 Hz. The time interval is 0.1 second, and the volume is expressed in normalized decibels from 0 to 10 based on the power-time spectrum in order to eliminate differences in recording quality. We define the information (I) via comparing changing of scale (S), time (T), and volume (V) of positions 1 and 2 in scale and time coordinate to information transfer: 
 \begin{equation}
 \begin{split}
 I \equiv &w_1\cdot|{\rm S}(1)-{\rm S}(2)|+w_2\cdot|{\rm T}(1)-{\rm T}(2)|\\
 +&w_3\cdot|{\rm V}(1)-{\rm V}(2)|+w_4\cdot|{\rm V}(1)+{\rm V}(2)|
 \end{split}
 \end{equation}
  where the self-organized weights  $w_{1,2,3,4}$ will be explained shortly. The first three terms in Eq. (1) reflect the signature of composition, while the fourth is an additional term that represents the energy carried by the positions and emotions the composer tried to convey \cite{Emotion_dynamic}.

In Fig. 1, the amount of information carried by any music is limited and determined by both the composer's style and motives \cite{M1}. The amount of information conveyed between pixels must be less than a threshold in order for them to be treated as being linked. Using the total number of links in the network, we calculate the clustering coefficient (CC) of each pixel, ${\rm CC}=2N/[K(K-1)]$, where degree $N$ counts the number of neighbors and $K$ the number of links among them. In order for CC to be interpreted as a word, it has to be significant enough at supplying the semantic meaning of the cluster based on the quantities $N$ and $K$. 

The optimal values for the self-organized weights in Eq. (1) were determined by two criteria: first, the distribution of Zipf's law for CC in the music score must exhibit an R-square value exceeding 0.8 after deleting the first rank and plotting the frequency vs  rank in full logarithm. Second, the largest type of CC should be selected as the optimization condition in order to extract the maximum number of word types to maintain diversity.
\begin{figure}
\centering
\includegraphics[width=8cm]{NF1_1215.eps}\caption{Volumes are stated   in the scale-time plot for the music score by Ryuichi Sakamoto.  The exchange of information between two pixels is determined by their elements and  weights. A valid/invalid link, denoted by a dash/solid line, means the information is less/greater than the threshold. The CC is considered as a word with links to selective neighbors.}
\end{figure}  

The process of extracting the optimal weights for  4032 combinations is described in Supplemental Material (SM): Sec. I \cite{SM}. From now on, we shall name our model as the Essential Element Network (EEN) to emphasize its inclusion of essential EE in music. Our analyses focus on piano pieces from the Common Practice Period (CPP) which spans from 1650 to 1900 AD\cite{CPP}. The CPP marks the establishment of Western musical system and the definition of harmony system which is essential for the interaction between musical elements \cite{IN1,IN2}. Within the CPP, the Baroque period (B) is the earliest, followed by the period of classical music such as Beethoven (BN), and the Romantic period (R) is the most recent.
% ok 20221214

\begin{figure}
\centering
\includegraphics[width=9cm]{NRE_1.eps}\caption{(a) A t-SNE mapping of the four weights and threshold value onto the eigenspace. The dash line is to highlight the existence of two clusters, as indicated by the statistical population in parentheses. (b) A t-test is conducted to assess the statistical significance of the weight selection where the orange dotted line is for p-value on the right y-axis.  (c) This full logarithmic plot for the Zipf distribution in different periods. (d) The Zipf distribution  of different types of sounds under the range of weight selection, We use the initial letter to represent a line, in which ambient sound includes bird, river, and city traffic.} 
\end{figure} 

 We used the t-distributed stochastic neighbor embedding (t-SNE) as a dimension reduction method to visualize the distribution of weights in three different musical periods \cite{TSN1}. The clustering of data points for music from the Beethoven and Baroque periods in Fig. 2(a) implies their weights are more similar to each other than to the Romantic. In Fig. 2(b), it can be seen that during the Baroque period, $w_3$ is the most prominent weight, which is related to the performance of clear gaps between notes, with the aim of maintaining clear voices. The $w_1$ and $w_2$ are chosen with similar weights, representing the characteristics of Baroque polyphonic music, emphasizing rigor counterpoint\cite{ABS_M}. However, the priority of weights shifts in the Romantic period when people tried to break free from the constraints of composition, leading to a greater freedom and evolution of more diverse composition styles \cite{RM1, RM2}. A t-test analysis of the weights shows that there exists a significant distinction between $w_1$ and $w_3$ if the p-value is less than 0.001. The combination of weights behind the EEN reflects the characteristic style of composition in each period by use of Zipf's law, as shown in Fig. 2(c).  In Fig. 2(d), we also compare different types of audio to test the applicability in 4032 combinations of weights. It turns out that  Morse code did not follow the expected power-law distribution, which suggests that the chosen weights are not suitable.
\begin{figure}
\centering
\includegraphics[width=9cm]{NRE_3.eps}
\caption{Clustering is plotted against sequence. Two samples of EEN distributions in one dimension are shown in (a-b), both of which exhibit unique periodic characteristics. (c) The Baroque period and the ambient sound are shown to share the same trend, as opposed to the  Romantic period which adopts more diverse words to make the musical form, see SM: Sec. II \cite{SM}. (d) The distribution of different types of audios.} 
\end{figure}  

If CC is calculated by scanning all time points from the first to the last scale, we can obtain a sequence with periodic characteristics, as  in Fig. 3(a, b). Based on the sequence, we can analyze the representation of words displayed in a 1D EEN direction. Figure 3(c) shows that the Baroque period, which emphasized musical formalism, has a uniform development in performance. The Romantic period adopted a strategy of destroying or escaping musical form. Ambient sound has a distribution similar to the Baroque period, but with more fluctuations and different CC intervals. In Fig. 3(d), we compare the 1D EEN of Morse code, noise (white and pink), and 38 piano songs with a length of less than two minutes. Morse code, which has clear tenor and vehicle, is distinct from music, which contains elements of noise that are abstract and hard to be assigned any meaning.

\begin{figure}
\centering
\includegraphics[width=9cm]{NRE_4_2.eps}
\caption{(a) The accuracy of distinguishing Baroque, Beethoven, and Romantic periods is shown with different sizes of 2D EEN information, as defined in the text. (b) Following the label in Fig. 2(a), the dotted and solid lines represent Shannon entropy (S) and accuracy (ACC). The features of Romantic period are enhanced by discarding information. (c) The score distribution of 2D information is shown in grad CAM. (d) From Skewness, it can be observed that the change in feature distribution due to the destruction of structure in the Romantic period is relatively small.} 
\end{figure}  

To identify the unique characteristics of  Baroque and Romantic periods, we use word mapping to represent them by a 2D EEN space and apply a convolutional neural network (CNN) \cite{CNN1} to classify musical periods. CNN is a powerful image classification model that can predict the label of an image, relying on its ability to extract local spatial features and keep translation invariant. We design two tasks to (1) determine the minimum size of text to represent each three CPP periods and (2) test the rigor of conventional definition for these periods.  

Figure 4(a) displays the result of task (1) in which the test accuracy of  CNN is above 80\% when using 42 scales and 1.9 seconds of information. As for task (2), we systematically increase the percentage of words that are randomly removed from a text to see if a trained CNN either changes or reaffirms its  prediction. The Softmax layer in a CNN can convert features into probabilities for predicting labels \cite{SOFT}, which allow us to compute Shannon entropy for three periods. By analyzing 1000 samples from each period, we find in Fig. 4(b) that the accuracy of the Baroque and Beethoven samples decrease with more deletions. In contrast, the accuracy of the Romantic samples improves. This means that the CPP music before the Romantic period emphasized a more rigorous pursuit of musical form, and the coordination between the different musical elements did not allow for arbitrary disruptions\cite{RM1}. In order for  CNN to understand the characteristics of word distribution in a 2D EEN, we use grad CAM to extract the significant locations from the CNN \cite{GRA1}. Grad CAM, through the concept of back propagation, allows us to explore  the scoring criteria for labeling\cite{GRA2}, see Fig. 4(c). In Fig. 4(d), we used Skewness to calculate and plot the score histogram by averaging over the samples. By calculating the skewness of the score histogram in grad-CAM, we can determine that the two periods before the Romantic period did not have as many prominent features. This also means that the features were considered in a more holistic manner and were composed of lower scores. Additionally, the score distribution during the Romantic period is closer to Gaussian. Classical period composers, such as Beethoven, are seen as the successors of Baroque composers so they should show similar traits. The fact that this conclusion is reproduced in Fig. 4 vindicates the strength and accuracy of our approach. Details of the deep learning models for image processing are listed in SM: Sec. III \cite{SM}.
%20221214 !!!
\begin{figure}
\centering
\includegraphics[width=8.5cm]{OF_F_2023_5_3.eps}
\caption{In (a-b), the blue circle and red cross denote non music and music. In (a), the solid and dashed lines represent accuracy (Acc) and Shannon entropy (S). (b) The Dashed/solid line follows the scale of the right/left y axes. The t-test for both is detailed in SM: Sec. V \cite{SM}. (c-f) show the term changes of Baroque, Beethoven and Romantic  periods, and modern Japanese music, such as those by Ryuichi Sakamoto, respectively. The destruction of terms is demonstrated by the termination of arrows, while new words can be seen to emerge.} 
\end{figure}  
%OF_5_GAN_grad_cam.eps


It is widely believed that the origin of music can be traced back to ancient humans imitating natural and environmental sounds\cite{O1,O2}. To investigate this idea, we trained a network to distinguish between music and non music  including  ambient sound and noises by 64 scale and 10.1 sec, see SM: Sec. IV \cite{SM}. By the same process in Fig. 4(b), we detected the characteristics of 2D EEN in Fig. 5(a), and found that random destruction of the structure actually preserved its characteristics. We know that ambient sound and noise lack clear rhythm, melody, and harmony.  However,  their Shannon entropy reaches maximum when the loss rate exceeds 0.8. This implies that the word loss renders the ambient sound more music-like. We suspect that the deletion causes the originally continuous noise to become a combination of discrete segments, which is a feature of music. When the loss rate approaches 0.9, all non music is transformed into music.


In Fig. 5(b), we normalized the grad CAM score into 10 equal parts, not only to calculate the proportion of 2D EEN, but also to understand the grouping structure of Fig. 4(c). We used graph theory to calculate the density, ${2L}/[N(N-1)]$ where $L,N$ denote the link and node numbers.
A link is defined if the distance is less than the  average separation between each score point and all its neighbors. We found that, although non music  has a higher proportion than music in each scoring part, its density is lower than that of music. This suggests that non-music characteristics have a more scattered features than music.
 We also analyzed the word frequency of texts from the three periods of CPP and Japanese modern music using word cloud technology\cite{cloud}. Since CC=1 remains the most frequent word in all periods, we removed it to concentrate on the rest of words in Fig. 5(c-f). They show that words in EEN evolve throughout each period, while some are eventually terminated, just like words in natural language.

 By mapping the words of  musical texts into 1 and 2 D in EEN, we discovered different regularity in composition structures of Baroque, Classical and Romantic period music. Our approach from a more scientific angle allows us to not only (1) obtain several results that are in line with current musical understandings, but also 
 (2) differentiate non music through its higher graph density from music,
 (3) offer insights into the evolution of musical texts, and
 (4) demonstrate a promising way to automatically generate music based solely on a 2D EEN image without audio inputs.
 Among the conclusion for (1), we found that (i) Baroque period is characterized by a more rigorous and ordered structure that emphasizes on repeating the same form, as exemplified by the repetition of a particular pattern in works like Fugue and Johann Pachelbel’s Canon \cite{FU,CANON},  
(ii) although Beethoven falls between the Baroque and Romantic periods, the self-organizing characteristics of his music are closer to the structure of the Baroque period, (iii) the arrangement of words for Romantic music distinctly differs from the two preceding  periods, which supports the emphasis on individualism by Romantic composers.

Preliminary results  for (3) suggest that it is promising to promote  EEN to music with non-equal temperament laws. For instance, we may use the self-organized weights of Austronesian music as an indicator for the evolution and migration of Austronesian peoples \cite{S1,S2}, much like the role of genes \cite{ge1}. Our approach has good potential to substantially advance the field of anthropology \cite{ant1}.

In support of (4), we  utilized the generated antagonism network (GAN) \cite{GAN1} and successfully produced new 2D EEN images, as detailed in SM: Sec. VI \cite{SM}. Different from directly manipulating music by audio format \cite{NM1}.
In the meantime, it is recommendable to incorporate cycle-GAN \cite{ CYC_G} between 2D EEN and the Mel spectrum \cite{MEL1,MEL2} to explore how words are presented in music information.
% We set the epoch to 200, the learning rate to 0.0001, and the batch size to 64

Schopenhauer thinks that music is an embodiment of will \cite{SCP}, and how exactly emotions are expressed through music has always been a topic of debate \cite{Philosophy of Music}. By offering a basis to extract the meaning behind music in a systematic and quantitative way, we show that there may indeed be a ``language of the emotions"\cite{Philosophy of Music} that musicians have cultivated throughout history.

This work was supported by MoST in Taiwan under Grant
No. 108-2112-M007-011-MY3.

\begin{thebibliography}{}

%†^\dagger] These two authors contribute equally.
\item[$^\ast$] ming@phys.nthu.edu.tw

\bibitem{Music_Nature1} J. N. Chiang, M. H. Rosenberg, C. A. Bufford, D. Stephens, A. Lysy, and M. M. Monti, Brain and Language \textbf{185}, 30 (2018).
\bibitem{Music_Nature2} H. Orlov, The Sign in Music and Literature, edited by Wendy Steiner, p. 131 (University of Texas Press, 1981). 
\bibitem{Music_Nature3}Samuel A.  Mehr {\it {\it et al.}.}, Science \textbf{366}, 970 (2019).
\bibitem{Nature1} J. McDermott, Nature \textbf{453}, 287 (2008).
\bibitem{tenor1} D. Douglass, Western J. Communication 64, 405 (2000).
\bibitem{tenor2} I. A. Richards and J. Constable, The philosophy of rhetoric (London, Routledge, 2018).
\bibitem{music_memory} S. Koelsch, E. Kasper, D. Sammler, K. Schulze, T. Gunter, and A. D. Friederici, Nature Neuroscience \textbf{7}, 302 (2004).
\bibitem{music_text1} B. Maess, S. Koelsch, T. C. Gunter, and A. D. Friederici, Nature Neuroscience \textbf{4}, 540 (2001).
\bibitem{music_text2} A. D. Patel, Nature Neuroscience \textbf{6}, 674 (2003).
\bibitem{structure1} M. Tettamanti and D. Weniger, Cortex \textbf{42}, 491 (2006). 

\bibitem{structure2} S. Koelsch, Frontier in Psychology \textbf{2}, 110 (2011). 
\bibitem{Zipfs1} S. T. Piantadosi, Psychonomic Bulletin \& Review \textbf{21}, 1112 (2014).
\bibitem{Zipfs2} C. Furusawa and K. Kaneko, Phys. Rev. Lett. \textbf{90}, 088102 (2003). 
\bibitem{Power1} A. Broder, R. Kumar, F. Maghoul, P. Raghavean, S. Rajagopalan, R. Stata, A. Tomkins, and J. Wiener, Comput. Netw. {\bf 33}, 309 (2000).
\bibitem{Power2}P.-R. Tsai {\it et al.} Sci. Rep. \textbf{11}, 3463 (2021).
\bibitem{Power3} V. M. Eguiluz, D. R. Chialvo, G. A. Cecchi, M. Baliki, and A. V. Apkarian, Phys. Rev. Lett. \textbf{94}, 018102 (2005). 
\bibitem{zipfs_music1} M. Haro, J. Serrà, P. Herrera, and Á. Corral, PLoS ONE \textbf{7}, e33993 (2012).
\bibitem{zipfs_music2} J. I. Perotti and O. V. Billoni, Physica A: Statistical Mechanics and Its Applications \textbf{549}, 124309 (2020). 
\bibitem{UF} P. E. Savage and S. Brown,  Frontiers in Psychology \textbf{4}, 436 (2013).
\bibitem{Syntax1} G. Karl, Music Theory Spectrum \textbf{19}, 13 (1997). 
\bibitem{text_mining1} V. Sorin, Y. Barash, E. Konen, and E. Klang, Journal of the American College of Radiology \textbf{17}, 639 (2020). 
\bibitem{text_mining2} S. Kuang and B. D. Davison, 2018 IEEE International Conference on Big Data and Smart Computing (BigComp), 1 (2018). 
\bibitem{John}Z. Sadeghi, J. L. McClelland, and P. Hoffman, Neuropsychologia \textbf{76}, 52 (2015). 
\bibitem{Edgar} K. Tedman, Edgard Var\'ese: Concepts of Organized Sound, University of Sussex, D. Phil. dissertation (1983). 
\bibitem{Emotion_dynamic} P. N. Juslin and R. Timmers, Handbook of Music and Emotion: Theory, Research, Applications, 452 (1993). 
\bibitem{M1} S. Hallam, Music Education Research \textbf{4}, 225 (2002). 
\bibitem{SM} Supplemental material.
\bibitem{CPP}D. Tymoczko, Geometry of Music Harmony and Counterpoint in the Extended Common Practice (Oxford University Press, Cary, 2014).  
\bibitem{IN1} R. P. Morgan, Music Theory Spectrum \textbf{20}, 1 (1998).
\bibitem{IN2} J. Harbison, Contemporary Music Review \textbf{6}, 71 (1992). 
\bibitem{TSN1} H. Cho and S. M. Yoon, 2017 International Conference on Culture and Computing (Culture and Computing), 1 (2017). 
\bibitem{ABS_M} G. S. Johnston, Early Music XXVI \textbf{26}, 51 (1998). 
\bibitem{RM1} V. K. Agawu, Music as Discourse: Semiotic Adventures in Romantic Music (Oxford University Press, Oxford, 2014).  
\bibitem{RM2} C. Dahlhaus and E. Sanders, 19th-Century Music \textbf{11}, 194 (1987). 
\bibitem{CNN1} N. Milosevic, Introduction to Convolutional Neural Networks: With Image Classification Using PyTorch (Apress, 2020).  
\bibitem{SOFT} R. Hu, B. Tian, S. Yin, and S. Wei, 2018 IEEE 23rd International Conference on Digital Signal Processing (DSP) (2018).   
\bibitem{GRA1} R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, International Journal of Computer Vision \textbf{128}, 336 (2019). 
\bibitem{GRA2} K. Vinogradova, A. Dibrov, and G. Myers, Proceedings of the AAAI Conference on Artificial Intelligence \textbf{34}, 13943 (2020).
\bibitem{O1} W. T. Fitch,  Cognition \textbf{100}, 173 (2006).
\bibitem{O2} S. Mithen,   The singing Neanderthals: The origins of music, language, mind, and body (Harvard University Press, 2005).
\bibitem{cloud} W. Cui, Y. Wu, S. Liu, F. Wei, M. Zhou, and H. Qu, 2010 IEEE Pacific Visualization Symposium (PacificVis) (2010).  
\bibitem{FU} S. Gut and I. Bent, Revue De Musicologie \textbf{85}, 373 (1999).  
\bibitem{CANON} Carlos Agon and Moreno Andreatta, Perspectives of New Music \textbf{49}, 66 (2011). 
\bibitem{S1} T. Rzeszutek, P. E. Savage, and S. Brown, Proceedings of the Royal Society B: Biological Sciences \textbf{279}, 1606 (2011). 
\bibitem{S2} B. Abels, Austronesian Soundscapes Performing Arts in Oceania and Southeast Asia (Amsterdam University Press, Amsterdam, 2011). 
\bibitem{ge1}  C. Furusawa and K. Kaneko, Phys. Rev. Lett. \textbf{90}, 258101 (2003).
\bibitem{ant1} S. Brown {\it et al}, Proceedings of the Royal Society B: Biological Sciences \textbf{281}, 20132072 (2014). 
\bibitem{GAN1} X. Mao and Q. Li, Generative Adversarial Networks for Image Generation, 1st ed. (Springer, 2020).  
\bibitem{NM1} N. Jiang, S. Jin, Z. Duan, and C. Zhang, Proceedings of the AAAI Conference on Artificial Intelligence \textbf{34}, 710 (2020). 
%\bibitem{NM2} Y. Qin, H. Xie, S. Ding, B. Tan, Y. Li, B. Zhao, and M. Ye, Applied Intelligence (2022). 
\bibitem{CYC_G} J. Harms, Y. Lei, T. Wang, R. Zhang, J. Zhou, X. Tang, W. J. Curran, T. Liu, and X. Yang, Medical Physics \textbf{46}, 3998 (2019). 
\bibitem{MEL1} Y. Khasgiwala and J. Tailor, 2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON), 339 (2021). 
\bibitem{MEL2} N. Perraudin, P. Balazs, and P. L. Sondergaard, 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (2013). 
\bibitem{SCP} Arthur Schopenhauer, Die welt als wille und vorstellung. (BoD–Books on Demand, 2016).
\bibitem{Philosophy of Music} Andrew Kania, The Philosophy of Music, The Stanford Encyclopedia of Philosophy, Edward N. Zalta \& Uri Nodelman eds.  (Spring 2023 Edition).

\end{thebibliography}{}
\end{document}
% ****** End of file apssamp.tex ******