\documentclass{uai2023} % for initial submission
% \documentclass[accepted]{uai2023} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2023} % ptmx math instead of Computer
                                         % Modern (has noticable issues)
% \documentclass[mathfont=newtx]{uai2023} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
\usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some potentially useful packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{textcase}
\usepackage{multirow}
\usepackage{bm}
% \usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{tabularx}   % https://tex.stackexchange.com/questions/16766/how-can-i-set-the-width-of-a-table
% \usepackage{slashbox}   % https://tex.stackexchange.com/questions/27193/latex-table-cell-with-a-diagonal-line-and-2-sub-cells
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% self-defined commands-----------------------------------
\newcommand{\replay}{\mathcal{D}}
\newcommand{\jointaction}{\mathbf{a}}
\newcommand{\jointhist}{\boldsymbol{\tau}}
\newcommand{\statespace}{\mathcal{S}}
\newcommand{\actionspace}{\mathcal{A}}
\newcommand{\indparam}{\boldsymbol{\phi}}
\newcommand{\depparam}{\boldsymbol{\psi}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\qi}{Q_{i}}
\newcommand{\qjt}{Q_{jt}}
\newcommand{\subact}{\text{act}}
\newcommand{\subboot}{\text{boot}}
\newcommand{\subrew}{\text{rew}}
\newcommand{\mixer}{\text{Mixer}}
\newcommand{\rom}[1]{\romannumeral #1}

\newcommand{\xutong}[1]{\textcolor{blue}{#1}}

\NewDocumentCommand{\anote}{}{\makebox[0pt][l]{$^*$}}
% --------------------------------------------------------

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning}
% \title{Conditionally Optimistic Exploration: An UCT-Inspired Exploration Method for Cooperative Deep Multi-Agent Reinforcement Learning}
% some other titles below:
% Conditionally Optimistic Exploration for Cooperative Multi-Agent Reinforcement Learning
% COE: an efficient exploration method for cooperative MARL with Conditional Optimism

% The standard author block has changed for UAI 2023 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
% \author[1,2]{\href{mailto:<xutong.zhao@mila.quebec>?Subject=Your UAI 2023 paper}{Xutong Zhao}{}}
\author[1,2]{Xutong Zhao}
\author[3]{Yangchen Pan}
\author[4]{Chenjun Xiao}
\author[1,2]{Sarath Chandar}
\author[1,5]{Janarthanan Rajendran}
% \author[3]{Further~Coauthor}
% \author[3,1]{Further~Coauthor}
% Add affiliations after the authors
\affil[1]{%
    % Computer Science Dept.\\
    % Cranberry University\\
    % Pittsburgh, Pennsylvania, USA
    Mila - Quebec AI Institute
}
\affil[2]{%
    % Second Affiliation\\
    % Address\\
    % …
    Polytechnique Montreal
}
\affil[3]{%
    % Another Affiliation\\
    % Address\\
    % …
    University of Oxford
  }
\affil[4]{
    University of Alberta
}
\affil[5]{
    University of Montreal
}
  
  \begin{document}
\maketitle

\begin{abstract}\label{sec:abs}
% \xutong{what it is, application to marl: treat each layer as one agent; how we implement}
% MARL <=> tree search, COE <=> UCT

Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL).
In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees).
The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents.
At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node.
We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE).
We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent.
COE computes each agent's state-action value estimate with an optimistic bonus derived from the visitation count of the state and joint actions taken by agents up to the current agent.
COE is adaptable to any value decomposition method for centralized training with decentralized execution.
Experiments across various cooperative MARL benchmarks show that COE outperforms current state-of-the-art exploration methods on hard-exploration tasks.

% To maintain sufficient optimism along training, the count-bonus augmentation is performed at both action selection and bootstrap target.
% adjusts the cooperative behaviour
% \xutong{not targeting to solve their issue}
% space and guides the agents to learn effective collaboration.
% Previous works commonly direct cooperative exploration by encouraging collaboration among agents, which is often realized by maximizing the mutual information (MI) of quantities that defines agents' behaviour.
% However, as there may exist several different modes of collaboration associated with the task, one particular strong collaboration may not guarantee the optimal joint strategy.
% To address this issue, 
% \xutong{Abstract too long? Think of a better name?}

\end{abstract}

\section{Introduction}\label{sec:intro}

% to achieve a common goal. The common goal is realized by

% cooperative marl:
% 1. recent success and applications
% 2. the problem: common shared reward
% 2. challenges: non-stationarity, credit assignment, scalability, partial obs
% 3. ctde: idea, example methods (a few from actor-critic and value-based)
In recent years multi-agent reinforcement learning (MARL) has drawn much attention and has shown high potential to be applied to various real-world scenarios, such as transportation \citep{seow2009collaborative}, robotics \citep{perrusquia2021multi}, and autonomous driving \citep{shalev2016safe}.
Cooperative MARL is a multi-agent learning setting where the objective is to train multiple agents that can cooperate to maximize the expected return defined by the same reward function shared across all agents.
There are several major challenges posed by this setting, such as credit assignment, scalability, non-stationarity, and partial observability.
To address those challenges, \citet{bernstein2002complexity} propose the Centralized Training with Decentralized Execution (CTDE) learning paradigm.
In this paradigm, information is shared across agents during training, guiding the learning of individual agents' policies and promoting cooperation during training, while agents still being able to run independently during decentralized execution.

% value decomp
% 1. idea, ind utility (often referred to ind Q), IGM (alignment between global \& local; methods examples),
% 2. implicit credit assignment to learn inter-dependency
% 2. sota on smac
One important line of research in CTDE is value decomposition, a general approach upon which many cooperative exploration methods build.
Value decomposition learns a centralized action-value function that can be factorized into the individual utility function (i.e., individual Q-function) of each agent.
To ensure the centralized policy is aligned with individual policies, \citet{son2019qtran} propose the Individual-Global-Max (IGM) principle that guarantees consistency between global and local greedy actions.
A common approach to value decomposition is to learn a mixing network that computes the centralized action value from the utilities of all agents.
Depending on the specific way to satisfy IGM, different methods have been introduced, including VDN \citep{sunehag2017value}, QMIX \citep{rashid2018qmix}, QTRAN \citep{son2019qtran}, and QPLEX \citep{wang2020qplex}.
% They achieve the state-of-the-art performance on the Starcraft micromanagement challenge \citep{}.

% exploration
% 1. inefficiency of eps-greedy: marl and single-agent rl
% 2. exploration: bandit, single-a rl; those methos are not directly applicable
% 3. recent marl exploration work; limitations: sub-optimal collaboration, independent agents hindering joint action explr, %not very theoretically grounded?
% However, since only global reward signals are backpropagated to train decentralized policies through centralized training, the inter-agent dependency captured by each agent is implicit, and often insufficient to promote highly cooperative behaviour.
% Hence IGM itself is insufficient to guarantee efficient cooperative exploration strategies.
% However, since IGM only states conditions regarding greedy actions, IGM itself is insufficient to promote efficient cooperative exploration strategies.
Cooperative exploration adds another level of difficulty to the exploration challenge.
In cooperative MARL, agents need to cooperatively explore the joint state-action space as the optimal joint policy may require a high degree of collaboration among them.
In addition, there may exist different types of cooperative strategies associated with a task.
Sound cooperative exploration methods should be able to identify the optimal strategy from potentially many sub-optimalities.
For instance, if the task for a group of robots is to move desks and chairs to another location, the optimal strategy is that at each time several agents together lift a heavy desk that one single agent cannot lift, meanwhile each other spare agent carries one chair.
In this task delivering items either only collectively or only separately is sub-optimal, even though either way agents achieve a cooperative strategy.
Therefore cooperative exploration is challenging in cooperative MARL, especially when the reward signals are sparse.
Although directed exploration strategies have been widely studied in multi-armed bandit and single-agent RL settings, they fail to account for cooperation among agents.
Moreover, it is not straightforward to adopt single-agent methods to cooperative MARL, due to the exponentially large state-action space and multi-agent credit assignment.
The popular $\varepsilon$-greedy strategy has been shown to be ineffective in complex MARL coordination tasks \citep{wang2020qplex}.

Some recent works encourage cooperative exploration in MARL settings by maximizing the correlation among agents' behaviour, which trains each agent's policy to account for influences from other agents, hence agents achieve effective collaborative exploration behaviour.
Correlation maximization is often realized by maximizing the mutual information (MI) between some quantities that can determine or reflect agents' behaviours,
such as the trajectory history of each agent.
Utilizing this idea, some works have been proposed and empirically outperformed the value decomposition baselines across various benchmark tasks \citep{jaques2019social,mahajan2019maven,wang2019influence,kim2020maximum,li2021celebrating}.
However, two major issues may prohibit agents from learning the optimal joint strategy.
First, optimizing the MI quantity for every pair of agents is not scalable because the required computation to optimize all MI losses grows as the number of agents increases.
Second, agents could learn different types of cooperative strategies, therefore one particular high degree of collaboration does not guarantee high performance.
As pointed out by \citet{li2022pmic}, simply maximizing the MI may not lead to high returns because agents may learn sub-optimal joint strategy, regardless of how strong the correlation they have.
% with inferior collaboration would
% exacerbates the scalability issue as the required computation grows drastically with the number of agents.

% The order should remain fixed throughout the entire training stage.
% For instance, the identity of each agent is a straightforward choice.
% To address these issues, 
%Inspired by the theoretically sound tree search algorithm UCT \citep{kocsis2006bandit}, we introduce a novel method called Conditionally Optimistic Exploration (COE). Assuming at each time step of centralized training, agents take actions sequentially following some arbitrary and pre-specified order before training. For each agent, COE uses exploration bonuses derived from the visitation count of the current state and action conditioned on joint actions taken by prior agents. Since it captures the influence of preceding agents directly from their actions, the condition count explicitly accounts for the inter-dependency among agents during exploration. Thus, COE’s optimistic bonus promotes cooperative exploration to states of jointly uncertain agents.
% COE enables agents to adjust their collaboration strategy according to the learned action-value estimates and the decayed optimism level, guiding them to abandon previously explored inferior strategies and explore more promising policies.
%Importantly, at execution time, with the dependent optimistic bonus being absent, we naturally obtain decentralized agents. Therefore COE is compatible with any value decomposition method.

% In summary, our contributions are three-fold.
%First, we adopt the idea of the perfect-information game tree search to cooperative MARL.
%Second, inspired by the tree search algorithm UCT, we correspondingly propose a novel exploration method COE that conducts conditional optimism-based cooperative exploration.
%COE is designed to be compatible with any value decomposition %method.
%Third, we build COE on the MARL method QMIX \citep{rashid2018qmix}, and empirically show COE's high sample efficiency and performance improvement by comparing it against other state-of-the-art methods in a wide range of tasks across three benchmark domains.

% \xutong{go through this paragraph and other comments}
In this work, we revisit the idea of UCT exploration \citep{kocsis2006bandit} proposed in a perfect-information game setting, where the game state is accessible at all nodes, and introduce it to encourage cooperative exploration in MARL.
Our insight is simple: if each agent's optimism estimate encodes a structured dependency relationship with other agents, by performing optimism-based exploration, agents would be guided to explore cooperative strategies.
Assuming at each timestep agents take actions according to a sequential order, the action execution sequence can be viewed as a path from the root to a leaf of a tree.
At each node of the tree, the preceding agent's taken action can be considered as the parent node of the current agent.
Then we can perform optimism-based exploration by computing the upper confidence bounds of each action for the current agent, conditioned on its parent node’s visitation count.
We disable exploration after training to obtain decentralized agents for execution.
We first review the basic background in the MARL setting and the UCT algorithm. 
Then, we describe how the UCT exploration can be applied to the MARL setting to encourage cooperative exploration.
We build COE on commonly used value decomposition methods, and used the hash-based counting technique \citep{tang2017exploration} to enable counting the visitations in continuous state-action domains.
Our empirical results on various benchmark domains show that our method is more effective than well-known baselines in exploration-challenging tasks, and matches baseline performance in general MARL tasks. 

% so that our approach can be applied in deep learning settings to tackle difficult tasks.



\section{Background}\label{sec:background}

\subsection{Dec-POMDP}
We model the cooperative multi-agent task as a Dec-POMDP (Decentralized Partially Observable Markov Decision Process) \citep{oliehoek2016concise}, which is formally defined as a tuple $G = \langle \statespace, \actionspace, P, R, \Omega, O, n, \gamma \rangle$, where $\statespace$ is the global state space, $\actionspace$ is the action space, $\Omega$ is the observation space, $n$ is the number of agents in the environment, and $\gamma \in [0,1]$ is the discount factor. 
At each timestep $t$, on state $s \in \statespace$ each agent $i \in \mathcal{N} \equiv \{1,\dots,n\}$ takes an action $a_i \in \actionspace$. The joint action  $\mathbf{a} = [a_i]^n_{i=1} \in \boldsymbol{\actionspace} \equiv \actionspace^n$ leads to the next state $s'$ sampled from the transition probability $P(s'|s, \mathbf{a}): \statespace \times \boldsymbol{\actionspace} \times \statespace \rightarrow [0,1]$, and obtains a global reward $r$ according to the reward function $R(s, \mathbf{a}): \statespace \times \boldsymbol{\actionspace} \rightarrow \mathbb{R}$ shared across all agents.
Each agent $i$ has a local policy $\pi_i(a_i|s): \statespace \times \actionspace \rightarrow [0, 1]$.
Based on the joint policy $\boldsymbol{\pi} \equiv [\pi_i]^n_{i=1}$, the joint action-value function is defined as $Q_{\boldsymbol{\pi}}(s, \mathbf{a}) = \EE_{\boldsymbol{\pi}} [ \sum^\infty_{t=0} \gamma^t r_t| s, \mathbf{a}]$. The objective is to find a joint policy that maximizes the action-value function.

We consider the partially observable setting, where each agent $i$ does not observe the global state $s$, instead only has access to a local observation $o_i \in \Omega$ drawn from the observation function $O(s, i): \statespace \times \mathcal{N} \rightarrow \Omega$.
Hence each agent $i$ maintains its action-observation history $\tau_i \in T \equiv (\Omega \times \actionspace)^*$, on which it can condition its policy $\pi_i(a_i|\tau_i): T \times \actionspace \rightarrow [0, 1]$. 
With agent $i$ observing the next observation $o_i'$, the updated next history is represented by $\tau_i' = \tau_i \cup \{ o_i' \}$.
We denote the joint history by $\jointhist \equiv [\tau]^n_{i=1} \in \mathbf{T} \equiv T^n$, and similarly joint next history by $\jointhist' \equiv [\tau']^n_{i=1}$.
% Note that we may assume full observability for the rigorousness of proofs in subsequent sections.
% simplicity of notations or 
%  consistency between joint action-value function and individual utility functions

% \subsection{Centralized Training With Decentralized Execution (CTDE)}
% Centralized training with decentralized execution (CTDE) \citep{bernstein2002complexity,oliehoek2008optimal} is a learning paradigm where the learning algorithm is allowed to have access to the action-observation history of all agents and the global state during training, while each agent takes actions only based on its own local history information during execution.
% One category of CTDE methods is value decomposition, which learns local policies by decomposing the joint action-value function into the individual utility function (i.e., individual Q-function) of each agent.
% To ensure learning of the joint action-value brings improvement in the individual utilities, \citet{son2019qtran} introduce the concept of Individual-Global-Max (IGM), which states that the greedy actions of the utilities correspond to the greedy actions of the joint action-value.
% Formally utilities $[\qi: T \times \actionspace \rightarrow \mathbb{R}]^n_{i=1}$ satisfying IGM for a joint $\qjt: \mathbf{T} \times \boldsymbol{\actionspace} \rightarrow \mathbb{R}$ under history $\jointhist$ if
% \begin{align}\label{eq:igm}
%     \arg\max_\mathbf{a} \qjt (\jointhist, \mathbf{a}) = \left(
%     \begin{array}{c}
%          \arg\max_{a_1} Q_1(\tau_1, a_1)  \\
%          \vdots \\
%          \arg\max_{a_n} Q_n(\tau_n, a_n)
%     \end{array}
%     \right).
% \end{align}
% A common approach to value decomposition is to learn a mixing network that computes the centralized action-value from the utilities of all agents.
% Depending on the viewpoint to interpret IGM, many methods utilize this idea and satisfy IGM, such as VDN \citep{sunehag2017value}, QMIX \citep{rashid2018qmix}, QTRAN \citep{son2019qtran}, QATTEN \citep{yang2020qatten}, and QPLEX \citep{wang2020qplex}.

%  One line of research is the policy gradient methods, the actor of each agent is decentralized and the centralized critic is shared by all agents, such as MADDPG \citep{} and MAPPO \citep{}.
% The utility function is often referred to as the individual Q-function, as each agent's policy is obtained by taking greedy actions according to the utility during decentralized execution.
% Many approaches have been proposed and they demonstrated high performance gain in single-agent deep RL benchmarks.
% Popular categories are ...
% CTDE can take different forms, and many methods have been introduced recently, including MADDPG \citep{lowe2017multi}, MAPPO \citep{yu2021surprising}, VDN \citep{sunehag2017value}, and QMIX \citep{rashid2018qmix}.


\subsection{UCT}
UCT (Upper Confidence bounds applied to Trees) \citep{kocsis2006bandit} is a tree search algorithm commonly used in Monte-Carlo Tree Search for perfect-information games.
In UCT, node selection is treated as a multi-armed bandit problem, where at each node its children nodes correspond to the arms,
and the Upper Confidence Bound (UCB) bandit algorithm \citep{auer2002finite} is used to select the child node with the highest upper confidence.
In particular, consider a sequence of node selections from the root to a leaf of a search tree as a trajectory at one timestep, at each depth the child node $i$ with the highest upper confidence bound is selected:
\begin{equation}\label{eq:uct-bandit-act}
    B_i = X_i + c \sqrt{\frac{2 \log(p)}{n_i}},
\end{equation}
where $X_i$ is the empirical mean of the rewards that have been obtained by trajectories going through node $i$,
$c$ is a constant controlling the scale of exploration,
$n_i$ and $p$ are the number of times node $i$ and its parent node have been visited, respectively.
Intuitively, conditioned on preceding agents' actions, at the current node actions that have been taken fewer times will have a higher exploration bonus, hence UCT tends to take action combinations that are under-explored or promising actions with higher reward estimates.
When the trajectory is completed, a reward is received at the leaf. 
The visitation count and reward estimate of each selected node are updated accordingly.
The UCT paper provides a regret analysis of the UCT algorithm, proving that its expected regret is upper bounded by $O(\log t)$, where $t$ is the number of trajectories/timesteps.


% \begin{theorem}
% \textbf{UCB applied to Tree search}
% Consider the UCT algorithm with UCB1 upper confidence bound. Given the tree with depth $D$, at timestep $n$, the pseudo-regret is upper bounded by
% \begin{align*}
%     \Bar{R}_n & \leq  C_1 \log(n) + C_2
% \end{align*}
% where $C_1,C_2$ are independent of $n$.
% \end{theorem}


% old abs motivation, maybe move to background/method-----------
% As the learning objective is to obtain decentralized agents for execution, previous works commonly assume action selection to be independent, even during the centralized training phase.
% Although this assumption exhibits the flexibility to directly obtain decentralized agents, it may constrain effective coordinated exploration in the joint action space.
% To address this issue, we propose a novel method called Conditionally Optimistic Exploration (COE) that takes explicit advantage of inter-dependency among agents' actions. In particular, we assume agents take actions sequentially following some pre-specified order during training.
% Since the sequential action-taking assumption is only for driving effective exploration during training, at execution time agents are still decentralized. COE is thereby applicable to any value decomposition approach.
% ------------

\section{Related Work}\label{sec:related}
\paragraph{Single-agent exploration.}
Exploration strategies have been extensively studied in single-agent deep RL settings.
\citep{amin2021survey} provide a thorough literature survey of advanced exploration methods. %with a taxonomy of these methods.
In recent years, the category of bonus-based methods has been commonly applied to solve hard exploration tasks.
Based on the Optimism in the Face of Uncertainty (OFU) principle, the high-level idea is to capture some notion of uncertainty or novelty, and augment the extrinsic reward the environment emits with an intrinsic reward that quantifies the uncertainty.
For instance, the count-based method \citep{bellemare2016unifying,ostrovski2017count,tang2017exploration} measures novelty through the number of times the agent observes a state-action tuple.
\citet{houthooft2016vime} propose an intrinsic bonus based on the maximization of information gain about the agent’s belief of environment dynamics.
Despite their recent success, simply applying bonus-based methods to MARL does not guarantee effective exploration.
As the reward signal is shared across all agents, adding one additional centralized intrinsic reward may still be inefficient to learn structured cooperation due to the multi-agent credit-assignment challenge.
Other successful exploration approaches like BootstrappedDQN \citep{osband2016deep} are unscalable in MARL because of the exponentially large state-action space.


\paragraph{Multi-agent exploration.}
A recent branch of research proposes to drive multi-agent exploration by promoting collaboration among agents through the maximization of the correlation or influence of agents.
The correlation is commonly realized by the mutual information (MI) of quantities that define or reflect agents' behaviour,
such as the trajectory history of each agent.
For instance, MAVEN \citep{mahajan2019maven} learns a hierarchical policy to produce a latent variable that encodes the information about the joint policy, and maximizes the MI between this latent variable and the joint trajectories to encourage the correlation of agents' behaviour. 
Some other methods try to promote collaboration by maximizing pairwise MI between every two agents.
For instance, EITI \citep{wang2019influence} maximizes the MI between one agent's transition and the other's state-action.
VM3-AC \citep{kim2020maximum} maximizes the MI between two agents' policy distributions.
Pairwise MI is hard to scale to scenarios with a large number of agents, because the computation grows with the number of agents.
\citet{li2022pmic} claims one important downside of MI-based methods is the fact that a strong correlation does not necessarily correspond to high-return collaboration, especially when there exist multiple sub-optimal highly-cooperative strategies associated with the given task.
Aside from MI-based methods, there are other approaches based on different intuitions.
VACL \citep{chen2021variational} leverages variational inference and automatic curriculum learning to solve sparse-reward cooperative MARL challenges.
Since this method only aims to solve goal-conditioned problems, it is not generally applicable.
EMC \citep{zheng2021episodic} utilizes a value decomposition method to implicitly capture influence among agents and uses the prediction errors of individual Q-value functions as intrinsic rewards.
Our method also tries to capture agent-wise dependency to guide exploration.
Different from MI maximization or EMC, our method captures structured inter-dependency through each agent's conditional optimism estimate and performs optimism-based exploration.

% As the latent variable explores in the space of joint behaviours, it is inefficient in tasks with large state-action spaces.
% Besides, the latent variable is also required during execution, which violates the CTDE paradigm and becomes less suitable in tasks where agents should execute fully decentralized.
% Pairwise MI methods maximize the correlation between any two agents, making it difficult to scenarios with many agents in the environment.
% Along with their aforementioned specific issues, the most 
% However, as discussed above, learning decentralized policies is only through global reward backpropagation, the implicit correlation learned by value factorization maybe not reflect the optimal collaboration.

\paragraph{Action Conditioned Training.}
As the learning objective in CTDE is to obtain decentralized agents for execution, previous works commonly assume agents take actions independently and simultaneously, even during the centralized training phase.
A few recent works explicitly consider inter-dependency and cooperation learned through sequential action selection, where each agent's policy is conditioned on preceding agents' joint action.
MACPF \citep{wang2022more} learns a dependent joint policy and its independent counterpart by maximum-entropy RL \citep{ziebart2010modeling}.
ACE \citep{li2022ace} is a Q-learning method that models the multi-agent MDP into a single-agent MDP by making the bootstrap target dependent on subsequent agents' actions.
Leveraging the multi-agent advantage decomposition theorem \citep{kuba2021trust}, Multi-Agent Transformer (MAT) \citep{wen2022multi} casts MARL into a sequence modeling problem and uses a transformer architecture to map agents' observation sequences to agents' optimal action sequences.
These methods consider action conditioning to increase the expressiveness of the joint policy, hence improving its performance.
Our method leverages action conditioning from a different perspective: predecessors' actions reflect dependency among agents, therefore can be used to adjust the optimism level to achieve efficient cooperative exploration.

% UCT \citep{kocsis2006bandit} is a tree search algorithm based on Upper Confidence Bound (UCB) exploration.
% In UCT, each agent selects actions according to its reward estimates and optimistic bonus terms conditioned on preceding agents' actions.





\section{Method: Conditionally Optimistic Exploration (COE)}\label{sec:method}
% our method:
% 1. uct bandit
% 2. uct+iql (ie iql+ucb cond): direct adoptation of uct to marl, principled. limitations: (1) each agent observes local obs/traj instead of state, violates uct's sufficient condition; (2) need to make it ctde, which violates independent learning; (3) this ctde is not generalizable to other value decomp methods
% 3. hence uct+value decomp. Value decomp learns implicit inter-dependency among agents Q's, uct coordinates explr.

% TODO move to method
% COE enables agents to adjust their collaboration strategy when the learned action-value function and the decayed optimism indicate the inferiority of previously explored strategies, leading them to explore more promising policies.
% Each agent takes greedy action with respect to its bonus-augmented action-value function.
% At each timestep, COE augments each agent's action-value estimates with an optimistic bonus derived from the visitation count of the current state and the joint action taken by prior agents and itself.
% Specifically, agent $1$'s optimistic bonus is independently computed, agent $2$'s bonus is conditioned on agent $1$'s action, agent $3$'s bonus is conditioned on agent $1$ and $2$'s joint action, and so on.
% To solve the issue of sub-optimal cooperation and effectively drive exploration, we propose Conditionally Optimistic Exploration (COE).

% The intuition is that conditional visitation counts can capture the dependent uncertainty, which naturally decays as agents jointly gain more knowledge of corresponding state-action regions, guiding agents to learn optimal strategies.
% As the count-bonus is augmented only during training, COE is compatible with any CTDE value decomposition method.


In this section, we introduce our UCT-inspired method Conditionally Optimistic Exploration (COE) to effectively drive exploration in cooperative deep MARL.
We describe how we can view cooperative MARL as a sequence of tree search iterations.
We then discuss the challenges to directly applying UCT to MARL.
Then we present approaches to address these issues, which concludes details of our COE method.

% Tree search -> MARL, UCT -> our method
\subsection{Multi-Agent Exploration as UCT}
We first formulate action selection at each timestep of MARL as a bandit-based tree search procedure.
We consider the following sequential decision-making scheme: at each timestep $t$ all $n$ agents take actions sequentially following some arbitrary but fixed pre-determined order.
Without loss of generality, we use the identities of agents as the order, i.e., agent $i \in \mathcal{N} \equiv \{1,\dots,n\}$ is the $i$-th agent to select its action.
\cref{fig:marl-as-tree} depicts the formulation of MARL as UCT at each timestep $t$, where the tree is a partially shown binary tree for the simplicity of illustration.
To construct the tree structure, first the state $s^t$ is the root node.
Each agent $i$ has $k$ actions, corresponding to the $k$ children nodes of the parent node at depth $i-1$.
Each node at depth $i$ represents an intermediate stage to which the action sequence of agents $\{1,\dots,i\}$ sequentially transitions.
When agent $n$ takes its action, the action sequence reaches a leaf node, where the environment emits a reward to all agents, and transitions to the next state $s^{t+1}$, which is the root node of the next tree.

By applying the UCT tree search algorithm to each tree, we model the cooperative MARL exploration as a sequence of UCT procedures.
For action selection, conditioned on predecessors' joint action, denoted by $a_{<i}$, each agent $i$ estimates an action-value function $\qi(s, a_i | a_{<i})$.
Augmenting the Q-value by an optimistic bonus conditioned on prior agents' actions, we have the upper confidence bound, denoted as $B_i(s, a_i | a_{<i})$.
It is worth noting that the Q-value estimate is generalized across states and preceding agents' joint actions using neural network function approximators, which removes the need to maintain an empirical reward estimate at every node in every tree.
At depth $i$, agent $i$ uses the same Q-value function to take action, no matter which subtree the corresponding node is in.
% In this sense, the $k$ subtrees rooted at each node are equivalent because subsequent agents use the same Q-values to select actions.
% The visitation count of each node is recorded to perform exploration as in \cref{eq:uct-bandit-act}.

\begin{figure}[!tb] %!htb
  \centering
  \includegraphics[width=0.9\linewidth]{img/marl_as_tree_diagram.png}
  \caption{Modelling of MARL as Tree Search Procedure and Application of the UCT Algorithm.}\label{fig:marl-as-tree}
\end{figure}
It should be noted that there is a distinction between the conventional tree search problem setting and the cooperative MARL setting.
In UCT, the game state information is accessible at all nodes, but our Dec-POMDP setting assumes partial observability.
Accessing full state information enables agents to estimate action values based on the same global state and predecessors' actions, while in our setting we cannot have such estimates.
CTDE also requires agents to act independently at execution time, without following any sequential order or conditioning policies on other agents' actions.
As an approximate implementation, we build the conditional count module on value decomposition methods, and disable exploration after training to obtain decentralized agents.
We empirically test QMIX with conditional optimism in \cref{sec:experiment}. 

%Although the tree search formulation seems aligned with cooperative MARL, there are some challenges hindering the direct adoption of UCT to MARL.
%First, in UCT the game state information is accessible at all nodes, but our Dec-POMDP setting assumes partial observability.
%CTDE also requires agents to act independently and simultaneously at execution time, without following any sequential order or conditioning policies on other agents' actions.
%In order not to violate the problem settings, we build the conditional count component on a value decomposition method, where each agent $i$ has an action-value in the form $\qi(\tau_i, a_i)$, and remove exploration after training to obtain decentralized agents. 
%In \cref{sec:exp} we use a didactic example to illustrate that conditional optimism performs equally well despite different ways of value estimation. 
%Such restricted information agents can observe becomes the major distinction between UCT and our approach.
%Second, it is not straightforward to record visitation counts in large or continuous state-action spaces.
%Similar to the tree structure, for every action of each agent $i$, we need a count dependent on prior agents' actions.
%To tackle this challenge and apply our method to deep MARL, we utilize static hashing \citep{tang2017exploration} to obtain pseudo-counts.
%These designs help constitute our method Conditionally Optimistic Exploration.
%In the next subsections, we present details of computing pseudo-counts and the COE algorithm.

% With the conditional count component being kept during training, we could still perform optimism-based exploration without violating decentralized execution.
% The fact that different agents' Q-values are not learned on the same state information makes our method distinct from UCT.
% conditional optimism plays a dominant role over action value estimate.

% First, the MARL setting assumes partial observability.
% In UCT the game state information is accessible to all nodes, hence the action selected by the parent node reveals all information the child node needs.
% In MARL, however, since each agent only has access to its own local observation, the action-value estimates are not learned by conditioning on the same state information.
% In UCT the number of nodes in a tree is finite.
% Tracking the visitation count of each node requires one parameter.
% However, in MARL the state space may be very large or continuous, with exponentially large action space it is intractable to exactly record visitation counts.

% Moreover, it is worth making the exploration approach generally applicable to any value-based CTDE method, including Independent Q-Learning \citep{tan1993multi} and all value decomposition methods.
% Learning a dependent action-value estimate clearly violates decentralized training.
% We need to leverage pseudo-count techniques in deep RL literature.


\subsection{COE Algorithm}
We first briefly describe a common value decomposition learning paradigm.
Then we present how we utilize conditional counts to drive optimistic exploration.

Each agent $i$ has an independent Q-network $Q^{ind}_i (\tau_i, a_i; \phi_i)$ parameterized by $\phi_i$.
A mixing network $\mixer(\cdot ; \theta)$ parameterized by $\theta$ is used to compute the joint Q-values:
\begin{equation}\label{eq:q-joint-ind}
    Q^{ind}_{jt}(\jointhist, \jointaction) = \mixer( [ Q^{ind}_i(\tau_i, a_i) ]^N_{i=1}, s; \theta ).
\end{equation}
Individual agent's action-value networks $Q^{ind}_i$ and the mixing network $\mixer$ are trained by minimizing the mean-squared temporal-difference error:
\begin{equation} %\label{eq:td-err-joint}
    \mathcal{L}^{ind}( [\phi]^N_{i=1}, \theta ) = \EE_\replay [ (Q^{ind}_{jt}(\jointhist, \jointaction) - y^{ind} )^2 ] \label{eq:td-err-ind}
\end{equation}
where $y^{ind} = (r + \gamma \max_{\jointaction'} (Q^{ind}_{jt}(\jointhist', \jointaction') ) )$ is the update target, and $\replay$ is the replay buffer containing trajectory data collected by $Q^{ind}_i$'s.
It is worth noting that by IGM principle the greedy actions selected by $Q^{ind}_i$'s are the same actions $\qjt^{ind}$ would have taken. 
As centralized training backpropagates the global reward signal to learn the individual utilities $Q^{ind}_i$'s, value factorization implements an implicit multi-agent credit assignment that enables each agent to grasp the inter-dependency among all utilities.
% \xutong{mention the double q-learning case.}

Building on top of the value decomposition skeleton, we incorporate count-based optimism in both action selection and learning.
During action selection, each agent $i$ takes greedy actions with respect to its optimistic action-value
\begin{equation}\label{eq:ucb-act}
\!a_i \!=\! \arg\max_{a_i'} \left\{ \qi(\tau_i, a_i')\! +\! c_\subact \sqrt{\frac{2 \log(N(s, a_{<i})) }{N(s, a_{<i}, a_i') }} \right\}\!,\!
\end{equation}
where $a_{<i}$ represents the joint actions taken by agents prior to agent $i$,
and $c_\subact \in \mathbb{R}_+$ is a hyper-parameter controlling the scale of optimism.
Note that counting is performed in the global state space thanks to centralized training.
% where $N(s, a_{<i}))$ and $N(s, a_{<i}, a_i')$ are the number of times the state-action tuples $\left(s, a_{<i} \right)$ and $\left(s, a_{<i}, a_i' \right)$ have been visited, respectively,

% Interesting variant: how about augmenting the target with a centralized bonus? ie $Q^{ind}_{jt}(\jointhist', \jointaction') + \frac{c_\subboot}{\sqrt{N(s', \jointaction')}} $? Both reward and boot optimism propagates through centralized credit assignment
Moreover, we augment the global reward and the bootstrapped target each with a bonus term, such that the update target becomes
\begin{multline}\label{eq:ucb-target-joint}
\!y \!=\! \left( r(s, \jointaction) + \frac{c_\subrew}{\sqrt{N(s, \jointaction)}} \right) + \\
    \gamma \max_{\jointaction'} \mixer \! \left( \! \left[ Q_i(\tau_i', a_i') \!+\! \frac{c_\subboot}{\sqrt{N(s', a_{<i}', a_i')}} \right]^N_{i=1} \right)\!,
\end{multline}
where $c_\subrew, c_\subboot \in \mathbb{R}_+$ are hyper-parameters controlling the scale of the optimistic bias in reward and bootstrapped target, respectively.
These two bonus terms are added for two major reasons.
First, we intend to maintain long-term optimism in the Q-functions.
The acting-time optimism decreases as agents take actions, but unlike bandit or tabular MDP methods, COE's Q-value estimate is updated at a relatively slower rate due to the nature of gradient updates of neural networks.
To encourage COE to explore persistently, the augmentation to the bootstrap target allows the Q-value itself to encode optimism through TD loss update.
Second, since the bootstrap target is defined based on the Q-value estimates of the next states, the optimistic bootstrap target also captures uncertainty from subsequent agents and future timesteps.
The idea of learning optimistic Q-values is originally proposed by \citet{jin2018q,jin2020provably,yang2020function} and extended to deep RL by \citet{rashid2020optimistic}.
% If only the action-selection bonus is augmented, 
% by the time this optimism decays and action selection is dominated by Q-values, agents may not have performed sufficient exploration, hence the Q-value estimate may not have gained sufficient knowledge to take optimal actions.

With the count-based optimism introduced, the complete learning algorithm is presented in \cref{alg:uct-qlearning}.
During decentralized execution, the optimistic bonuses, although decayed to negligible magnitude, are disabled, and agents take independent actions according to $Q^{ind}_i$'s only.


To apply COE to deep MARL tasks, we need to approximate counts in high-dimensional or continuous state space.
In our experiments we use the SimHash method \citep{tang2017exploration} that projects states to a lower-dimensional feature space before counting.
We record the visitation count for the tuple of the state $s$ and all agents' joint action $\jointaction$, denoted by $N(s, \jointaction)$.
For each agent $i$, the count up to its action $a_i$ satisfies $N(s, a_{<i}, a_i)\! =\! \sum_{a_{i+1}}\! N(s, a_{<i}, a_i, a_{i+1}) \!=\! \sum_{a_{>i}}\! N(s, a_{<i}, a_i, a_{>i})$,
% \begin{align*}
% \begin{small}
%     N(s, a_{<i}, a_i)\! =\! \sum_{a_{i+1}}\! N(s, a_{<i}, a_i, a_{i+1}) \!=\! \sum_{a_{>i}}\! N(s, a_{<i}, a_i, a_{>i}),
% \end{small}
% \end{align*}
where $a_{<i}$ and $a_{>i}$ denote the joint actions taken by preceding and subsequent agents of $i$, respectively.
This relationship shows that we can obtain any count up to $a_i$ by summing up the counts of joint actions that overlap $a_{<i}$ at state $s$.
Details about SimHash counting are presented in 
\cref{apx:pseudo-count}.
% Appendix~A. % TODO xutong remove hardcoded ref for arxiv.


% \xutong{mention the combination of the two, discuss its pros cons}
% We also present a simplified variant of our method, which excludes the dependent individual Q-network $Q^{dep}_i$ while still performing action-dependent UCT-style exploration.
% The only change in this variant is the absence of $Q^{dep}_i$, meaning that $Q^{ind}_i$ is responsible for action selection during both the training and execution phases.
% The dependency among agents is learned through centralized training.
% \xutong{add detailed descriptions for \{independent Q\}+\{dependent count\} variant.}


\begin{algorithm}[tb]   %tb
   \caption{Conditionally Optimistic Exploration}
   \label{alg:uct-qlearning}
\begin{algorithmic}
    \STATE Initialize parameters $\indparam, \theta$ %\depparam,
    \STATE Visitation count $N(s,\jointaction) \leftarrow 0, \forall (s, \jointaction) \in \statespace \times \boldsymbol{\actionspace}$
    \STATE Replay buffer $\replay \leftarrow \{\}$
    \FOR{each episode $m=1, \dots, M$}
        % \STATE $s^0 \sim \rho(s^0) $
        \FOR{each environment timestep $t=1, \dots, T$}
            \FOR{agent $i=1, \dots, n $}
                \STATE Select action $a^t_i$ according to \cref{eq:ucb-act}
            \ENDFOR
            \STATE $N(s^t, \jointaction^t) \leftarrow N(s^t, \jointaction^t) + 1$
            \STATE $s^{t+1} \sim P(s'|s^t,\jointaction^t), r^t = r(s^t,\jointaction^t)$
            \STATE $\replay \leftarrow \replay \cup \{ (s^t, \jointaction^t, r^t, s^{t+1}) \}$
            % \STATE Perform a gradient update on \cref{eq:td-err-dep}
            \STATE Perform a gradient update on \cref{eq:td-err-ind}
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}



\section{Experiments}\label{sec:experiment}
In this section, we evaluate COE on two sets of cooperative MARL tasks across three commonly used benchmarks:
1) sparse-reward tasks that specifically pose the cooperative exploration challenge,
and 2) tasks that generally assess MARL methods' ability for effective coordination.
Empirical results show that COE achieves higher sample efficiency and performance than other state-of-the-art approaches in sparse-reward tasks, and matches their performance in general cooperative tasks.
We also present ablation studies to demonstrate the effectiveness of conditional optimism and COE's compatibility with common MARL methods.
As a sanity check, we examine conditional optimism in a didactic multi-agent bandit problem.
% highlight other challenges such as non-stationarity and partial observability.


\begin{table*}[!htb]
    \scriptsize
    \centering
    \caption{Average Returns and 95\% Confidence Interval for All Four Algorithms, and Average Win-rates for SMAC Tasks.}\label{tab:avg-return-baseline}
    \begin{tabular}{*{3}{l}*{4}{c}}
      \toprule
      && \multicolumn{1}{l}{\textbf{Tasks \textbackslash Algs.}} & \multicolumn{1}{c}{COE} & \multicolumn{1}{c}{EMC} & \multicolumn{1}{c}{MAVEN} & \multicolumn{1}{c}{QMIX} \\
      \midrule
      \multirow{3}{*}{\rotatebox[origin=c]{90}{MPE}}
      && Adversary & $17.77 \pm 0.71$ & $16.73 \pm 0.83$ & $\bm{19.57 \pm 0.51}$ & $18.20 \pm 0.56$ \\
      && \textbf{Sparse Tag} & $\bm{0.65 \pm 0.09}$ & $0.43 \pm 0.06$ & $0.01 \pm 0.00$ & $0.40 \pm 0.05$ \\
      && \textbf{Sparse Spread} & $\bm{0.79 \pm 0.09}$ & $0.41 \pm 0.18$ & $0.10 \pm 0.14$ & $0.29 \pm 0.05$ \\
      \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{LBF}}
      && \textbf{10x10-3p-3f} & $\bm{0.71 \pm 0.05}$ & $0.68 \pm 0.03${*} & $0.16 \pm 0.06$ & $0.49 \pm 0.01$ \\
      && \textbf{15x15-3p-5f} & $\bm{0.20 \pm 0.02}$ & $0.12 \pm 0.02$ & $0.03 \pm 0.00$ & $0.08 \pm 0.01$ \\
      && \textbf{15x15-4p-3f} & $\bm{0.41 \pm 0.06}$ & $0.25 \pm 0.07$ & $0.04 \pm 0.01$ & $0.19 \pm 0.02$ \\
      && \textbf{15x15-4p-5f} & $\bm{0.30 \pm 0.02}$ & $0.23 \pm 0.04$ & $0.04 \pm 0.00$ & $0.15 \pm 0.02$ \\
      \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{SMAC}}
      &\multirow{2}{*}{\rotatebox[origin=c]{90}{ret}} & 2s-vs-1sc & $17.83 \pm 0.16${*} & $17.88 \pm 0.74${*} & $17.78 \pm 1.26${*} & $\bm{18.21 \pm 0.39}$ \\
      && 3s-vs-5z & $\bm{16.03 \pm 1.58}$ & $9.66 \pm 2.62$ & $14.11 \pm 2.36${*} & $11.74 \pm 1.87$ \\
      \cmidrule{2-7}
      &\multirow{2}{*}{\rotatebox[origin=c]{90}{win}} & 2s-vs-1sc & $0.79 \pm 0.01$ & $\bm{0.83 \pm 0.04}$ & $0.82 \pm 0.08${*} & $\bm{0.83 \pm 0.02}$ \\
      && 3s-vs-5z & $\bm{0.45 \pm 0.09}$ & $0.08 \pm 0.14$ & $0.29 \pm 0.12${*} & $0.13 \pm 0.11$ \\
      \bottomrule
    \end{tabular}
\end{table*}




\subsection{Evaluation Setup}\label{sec:exp-eval-setup}
We evaluate all algorithms on nine tasks over three benchmark environments.
The tasks can be categorized into two sets according to their challenges:
(1) Challenging sparse-reward tasks focused on efficient exploration. This includes \textsl{SparseTag} and \textsl{Sparse Spread} from the Multi-agent Particle Environments (MPE) \citep{lowe2017multi,mordatch2018emergence}, and four tasks with different configurations from the Level-Based Foraging (LBF) environments \citep{albrecht2015game,christianos2020shared,papoudakis2020benchmarking}.
(2) Tasks that generally assess multi-agent coordination. This includes \textsl{Adversary} in MPE, and an easy task \textsl{2s-vs-1sc} and a hard task \textsl{3s-vs-5z} in StarCraft Multi-Agent Challenge (SMAC) \citep{samvelyan2019starcraft}.
Note that LBF tasks and \textsl{Adversary} are fully observable, whereas SMAC and other MPE domains are partially observable environments.
More detailed descriptions of the environments and the evaluation protocol can be found in
% Appendix~B and Appendix~C, respectively.
\cref{apx:exp-environ} and \cref{apx:eval-protocol}, respectively.
% TODO xutong remove hardcoded ref for arxiv.



To promote fair comparisons, we build all methods on the QMIX agents with the same agent and mixer network architectures.
For the same reason we implement the canonical version of all methods where the only additional component is the exploration module unless otherwise specified.
We follow the same protocol presented by \citet{papoudakis2020benchmarking} to optimize hyperparameters.
Specifically we sweep hyperparameters on one task of each environment with three random seeds, and run the best configuration for all tasks in the respective environment with five seeds for the final experiments.
% Appendix~F explains hyperparameter optimization in more detail.
\cref{apx:hyperparam} explains hyperparameter optimization in more detail.
% TODO xutong remove hardcoded ref for arxiv.


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\linewidth]{img/marl_learning_curves_baseline_onecolumn.pdf}
  \caption{Episodic Returns and 95\% Confidence Interval for All Algorithms in All Tasks except \textsl{Adversary}, with Sparse-Reward Tasks Marked Bold.}\label{fig:marl-learning-curves-baseline}
\end{figure}


\subsection{Performance}\label{sec:exp-result}
We evaluate COE and compare it with the following state-of-the-art baselines in the experiments:
(\rom{1}) QMIX \citep{rashid2018qmix}: $\varepsilon$-greedy QMIX with linearly annealed epsilon schedule;
(\rom{2}) EMC \citep{zheng2021episodic};
(\rom{3}) MAVEN \citep{mahajan2019maven}: combined with annealing $\varepsilon$-greedy.
Empirical results show that COE outperforms all baselines in the sparse-rewards domains well-known for exploration challenges, and matches strong baseline performance in general multi-agent tasks.




\cref{tab:avg-return-baseline} summarizes the average returns for the four algorithms in all nine tasks.
We highlight the maximum average return in bold.
We perform a two-sample t-test \citep{snedecor1980statistical} with a significance level $0.05$ between the best performing algorithm and each other algorithm in each task.
We mark the return values with an asterisk if the corresponding algorithm achieves a performance level that is not statistically significantly different from the highest performance.
Difficult exploration tasks are shown in bold.
The same table also reports the average win-rates in SMAC tasks as it is a common practice in MARL literature.
The table summarizing maximum returns over training is presented in 
\cref{apx:more-results}.
% Appendix~D. % TODO xutong remove hardcoded ref for arxiv.
% It is worth noting that algorithms learn to optimize the returns rather than the win-rates.
% Also in practice high returns do not necessarily correspond to high win-rates.
% Hence we report the win-rates as complementary results.



The results in \cref{tab:avg-return-baseline} and \cref{fig:marl-learning-curves-baseline} show that COE significantly outperforms other baselines in sparse-reward tasks that require efficient exploration.
Particularly COE has higher sample efficiency in difficult LBF domains.
In the early exploration stage, all algorithms gain performance slowly, resulting in indistinguishable learning curves.
As time progresses, COE makes improvements much faster than the baselines.
The sample efficiency improvement leads to higher final and overall return values.
In relatively easier exploration tasks \textsl{SparseTag}, \textsl{SparseSpread}, and \textsl{Foraging-10x10-3p-3f}, COE's outperformance is not as large as it is in the hard tasks.
Some other baselines also learn strong policies in these tasks.
Since all algorithms are built on the same QMIX agent, overall the results in sparse-reward domains demonstrate the effectiveness of conditional-optimism-guided exploration.



In the general MARL coordination tasks, COE has similar performance as the baselines.
\textsl{Adversary} is evidently the easiest task among all tested tasks, where all algorithms quickly converge to the optimal policy at almost identical speed.
In the hard \textsl{3s-vs-5z} task in SMAC, COE shows better sample efficiency and final performance in terms of the mean episodic returns.
This trend is similar to the trends we observe in sparse-reward tasks, although in this task the outperformance is not statistically significant.
These results indicate that COE is not only an effective approach to hard exploration tasks; it is also a strong algorithm generally applicable to common MARL domains.

% despite uniform exploration, the $\varepsilon$-greedy QMIX is competitive in these dense-reward tasks that are not designed to evaluate cooperative exploration.





% https://stackoverflow.com/questions/53819902/labeling-side-by-side-tables-in-latex
% https://tex.stackexchange.com/questions/10863/is-there-a-way-to-slightly-shrink-a-table-including-font-size-to-fit-within-th
\begin{table*}[!htb]
    % \parbox[t]{0.75\linewidth}{
    \scriptsize
    \centering
    \caption{Average Returns and 95\% Confidence Interval for All Ablations, and Average Win-rates for SMAC Tasks.}\label{tab:avg-return-ablation}
    \begin{tabular}{*{3}{l}*{5}{c}}
      \toprule
      && \multicolumn{1}{l}{\textbf{Tasks \textbackslash Algs.}} & \multicolumn{1}{c}{COE} & \multicolumn{1}{c}{COE-Cond-IQ} & \multicolumn{1}{c}{COE-Cond-CQ} & \multicolumn{1}{c}{UCB-Ind} & \multicolumn{1}{c}{UCB-Cen} \\
      \midrule
      \multirow{3}{*}{\rotatebox[origin=c]{90}{MPE}}
      && Adversary & $17.77 \pm 0.71$ & $15.46 \pm 0.68$ & $18.99 \pm 0.26$ & $17.70 \pm 0.37$ & $17.15 \pm 0.82$ \\
      && \textbf{Sparse Tag} & $0.65 \pm 0.09$ & $0.07 \pm 0.01$ & $0.83 \pm 0.17$ & $0.52 \pm 0.13$ & $0.49 \pm 0.13$ \\
      && \textbf{Sparse Spread} & $0.79 \pm 0.09$ & $0.36 \pm 0.05$ & $0.54 \pm 0.19$ & $0.59 \pm 0.31$ & $0.75 \pm 0.16$ \\
      \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{LBF}}
      && \textbf{10x10-3p-3f} & $0.71 \pm 0.05$ & $0.76 \pm 0.04$ & $0.68 \pm 0.01$ & $0.67 \pm 0.07$ & $0.64 \pm 0.02$ \\
      && \textbf{15x15-3p-5f} & $0.20 \pm 0.02$ & $0.19 \pm 0.02$ & $0.12 \pm 0.05$ & $0.15 \pm 0.03$ & $0.13 \pm 0.05$ \\
      && \textbf{15x15-4p-3f} & $0.41 \pm 0.06$ & $0.47 \pm 0.06$ & $0.24 \pm 0.06$ & $0.23 \pm 0.05$ & $0.16 \pm 0.10$ \\
      && \textbf{15x15-4p-5f} & $0.30 \pm 0.02$ & $0.23 \pm 0.04$ & $0.14 \pm 0.02$ & $0.23 \pm 0.07$ & $0.27 \pm 0.06$ \\
      \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{SMAC}}
      &\multirow{2}{*}{\rotatebox[origin=c]{90}{ret}} & 2s-vs-1sc & $17.83 \pm 0.16$ & $16.67 \pm 1.56$ & $18.64 \pm 0.48$ & $11.09 \pm 7.27$ & $15.77 \pm 1.45$ \\
      && 3s-vs-5z & $16.03 \pm 1.58$ & $16.85 \pm 1.55$ & $17.01 \pm 0.74$ & $11.03 \pm 3.03$ & $13.36 \pm 3.41$ \\
      \cmidrule{2-8}
      &\multirow{2}{*}{\rotatebox[origin=c]{90}{win}} & 2s-vs-1sc & $0.79 \pm 0.01$ & $0.66 \pm 0.13$ & $0.87 \pm 0.04$ & $0.47 \pm 0.35$ & $0.66 \pm 0.07$ \\
      && 3s-vs-5z & $0.45 \pm 0.09$ & $0.56 \pm 0.15$ & $0.57 \pm 0.06$ & $0.19 \pm 0.18$ & $0.25 \pm 0.21$ \\
      \bottomrule
    \end{tabular}
    % }
    % % \hfill
    % \hspace{0.03\textwidth}
    % \parbox[t]{.18\linewidth}{
    % \centering
    % \caption{Example Bandit Problem With One Optimal Joint Action, with Each Entry Representing the Probability of Bernoulli Reward.}\label{tab:bandit-payoff}
    % \resizebox{0.35\columnwidth}{!}{%
    % \begin{tabular}[t]{*{5}{|c}|}
    %   \hline
    %   $a_i$ \textbackslash $a_2$ & A & B & C & D \\
    %   \hline
    %   A & 0.1 & 0.1 & 0.1 & 0.1 \\
    %   \hline
    %   B & 0.1 & 0.1 & 0.1 & 0.1 \\
    %   \hline
    %   C & 0.1 & $\bm{0.9}$ & 0.1 & 0.1 \\
    %   \hline
    %   D & 0.1 & 0.1 & 0.1 & 0.1 \\
    %   \hline
    % \end{tabular}%
    % }
    % }
\end{table*}





\subsection{Ablations}\label{sec:exp-ablation}
COE consists of two major components, namely the independent Q-value functions learned through centralized training, and the conditional optimism.
In order to have a better understanding of COE, we test several ablation variants to evaluate these two components' contribution to performance gain.
Results suggest that conditional optimism plays a dominant role in performance improvement.
Compared to dependent Q-values conditioned on predecessors' actions, independent Q-values learned through value decomposition also work well with conditional optimism despite partial observability.


To evaluate the contributions of \textit{independent} Q-values in COE, we test the following ablation variants that learn \textit{conditional} Q-values:
(1) COE-Cond-IQ: We apply conditional optimism to IQL.
Each agent simultaneously learns an independent Q-network and a dependent Q-network that takes in predecessors' actions as extra inputs without centralized training.
% Both Q-networks estimate the \textit{Q-values} rather than the utilities.
The dependent network selects actions during training.
Two nets are trained on separate TD losses using the same replay batches.
% Note that the independent network is trained fully offline.
After training the independent network is responsible for decision-making at execution time.
This variant directly mimics UCT in MARL without considering each agent's partial observability issue.
% Note that COE-Cond-IQ is a CTDE method instead of an independent learning method, despite the base IQL agent; \\
(2) COE-Cond-CQ: We add a QMIX mixer to COE-Cond-IQ to enable centralized Q-value training. 
The same mixer computes the centralized Q-value $\qjt^{ind}$ for independent networks and $\qjt^{dep}$ for dependent networks.
This variant ignores the potential issue that within individual Q-values the inter-dependency among agents captured by centralized training and that captured by action conditioning may not be aligned with each other.

% individual Q-values learned through centralized training and conditional Q-values learned through action conditioning may not be aligned with each other.
% the inter-dependency captured by Q-values

To evaluate the contributions of \textit{conditional} optimism, we propose the following ablation variants that use \textit{non-conditional} optimism:
(1) UCB-Ind: Similar to COE, each agent performs UCB-based exploration except that optimism is not conditioned on others' actions.
(2) UCB-Cen: Agents receive UCB optimism only through the intrinsic reward $\frac{c_\subrew}{\sqrt{N(s, \jointaction)}}$ during centralized training.
 

We follow the same evaluation protocol described in \cref{sec:exp-eval-setup} to conduct experiments.
The average returns of the ablations are summarized in \cref{tab:avg-return-ablation}.
% \cref{apx:ablation} 
% \cref{apx:more-results}
% Appendix~D and Appendix~E % TODO xutong remove hardcoded ref for arxiv.
\cref{apx:more-results} and \cref{apx:ablation}
present the learning curves and a more detailed introduction to the ablations, respectively.
Results show that COE has a similar performance as COE-Cond-CQ and COE-Cond-IQ in the majority of tested tasks.
COE-Cond-IQ performs relatively worse in MPE tasks, but better in LBF tasks.
This may be attributed to the partial observability issue:
since LBF is fully observable, COE-Cond-IQ becomes a more legitimate adoption of UCT to cooperative MARL.
COE-Cond-CQ matches COE's performance in MPE and SMAC.
Although it underperforms COE in three LBF tasks, COE-Cond-CQ is still competitive and matches EMC's performance in LBF.
These results suggest that conditional optimism boosts sample efficiency and overall performance with different Q-value estimation approaches.


% In hard exploration LBF tasks, COE-Cond-IQ has the highest sample efficiency in the early learning stage and competitive overall performance.
% The plots also show that COE-Cond-IQ often fails to maintain the early-stage advantage and it is surpassed by COE in the end.

On the other hand, UCB-Ind underperforms COE in hard LBF tasks and SMAC tasks.
It also has a large variance across random seeds in SMAC tasks.
UCB-Cen matches COE in half of the tasks, but it also suffers from large variances.
Through these comparisons, we observe conditional optimism guides more steady performance improvement.


% On the other hand, UCB-Ind has shown to be inefficient in hard LBF tasks and SMAC tasks, although it matches COE in easy tasks.
% This implies that the lack of cooperative exploration could result in different consequences.
% In tasks that require directed coordinated exploration as in LBF or complex cooperation as in SMAC, independent optimism may be highly inefficient to learn a strong joint policy.


% \xutong{
% 4. intro, summarize baseline and benchmark
% 4.1. sparse reward
% 4.2. common tasks
% 4.3. ablation
% 4.4. bandit sanity check for rigorousness

% notes:
% 1. write evidence only. Write less implications.
% 2. emphasize our methods strengths.
% }
\subsection{Didactic Problem}
A rigorous application of UCT in MARL requires learning each agent's state-action value conditioned on earlier agents' state-action pairs.
Due to partial observability in MARL that prevents access to global states, we use value decomposition as an approximate implementation of conditional value estimation and empirically show its effectiveness in previous sections.
In this section, we provide a sanity check on the bandit problem to re-demonstrate conditional optimism is important whereas conditional value estimation is unnecessary.

% As a sanity check, we try to test an implementation of COE that matches the original UCT closer but incurs a higher computation and storage cost.
% The original UCT estimates an action's reward conditioned on the action's parent nodes.
% This means that a rigorous application of UCT in MARL would require learning each agent's state-action value conditioned on the earlier agents' state-action pairs.
% Though our MARL setting does not allow one agent to access other agents' observations, we can use the historical observation and action sequence as an input to learn an agent's state-action value function, as an approximate implementation to UCT's conditional reward/value estimation.
% However, we empirically show that this conditional value estimate is unnecessary. 

%verify the strengths of conditional optimism in a didactic problem.
%Empirical results demonstrate that conditional count-based exploration helps to quickly converge to the optimal strategy, no matter whether the action values are conditional or not.
%Independent count-based exploration, on the other hand, is susceptible to sub-optimality and has a large variance across runs.

We consider the cooperative multi-agent multi-armed stochastic bandit problem, where the reward is based on the joint action of a group of agents.
Suppose we have $n$ agents, each has $k$ actions.
In our didactic Bernoulli bandit problem, only one out of $k^n$ joint actions is optimal with distribution $\mathcal{B}(p=0.9)$, and all other joint actions are sub-optimal with distribution $\mathcal{B}(p=p_0)$, where the sub-optimality value $p_0$ is an environment hyper-parameter.
For each bandit instance, a uniformly sampled joint action from all combinations is set to be optimal.
% \cref{tab:bandit-payoff} shows payoff values for an example bandit problem with $2$ agents, $k=4$ actions each, and sub-optimal value $p_0=0.1$.


% We consider the cooperative multi-agent multi-armed stochastic bandit problem.
% Each agent $i \in \{1, \dots, n\}$ has action set $\actionspace$ with $k$ actions.
% At every round $t$, agent $i$ takes an action $a^t_i \in \actionspace$, and the environment samples a global reward from the reward distribution $r^t \sim P(\jointaction^t)$ based on the joint action.
% The objective is to minimize the regret $R_T = T \mu^* - \sum^T_{t=1} \mu_{\jointaction^t}$, where $\mu_{\jointaction^t} = \EE[r^t]$ is the expected reward.


\begin{figure}[!t] % !htb
  \centering
  \includegraphics[width=0.8\linewidth]{img/bandit_learning_curves_transpose.pdf}
  \caption{Mean Performance and Standard Error for Bandit Problem with $8$ Agents and $3$ Actions Each.}\label{fig:bandit-learning-curves}
\end{figure}



We test four UCB-based algorithm variants:
(1) DepRew-DepOpt: it performs UCT exploration as in \cref{eq:uct-bandit-act}, where both reward estimates and count-based optimism are dependent on prior agents' joint action;
(2) IndRew-DepOpt: each agent maintains its reward estimates independently, but the optimism is dependent on predecessors;
(3) IndRew-IndOpt: both reward estimates and optimism are independent;
(4) UCB-Cen: one UCB learner whose action space is the cartesian product of all agents' action set.
This variant is merely for performance comparison because it is not factorizable to decentralized agents.


We run experiments on a bandit problem with $8$ agents and $3$ actions of each agent over $50$ seeds, and report the performance of all four algorithms for different sub-optimality settings in \cref{fig:bandit-learning-curves}.
We evaluate algorithms with two metrics, the expected regret, which is preferably lower, and the percentage of optimal joint action being selected, which is preferably higher.
Results show that both DepRew-DepOpt and IndRew-DepOpt quickly converge to the optimal policy across different sub-optimality settings.
This suggests that in this bandit task, conditional optimism robustly drives efficient cooperative exploration, regardless of whether the reward estimates are learned independently.
IndRew-IndOpt, on the other hand, is inefficient to identify the optimal joint action and has high variances across random seeds.
These results highlight the significance of conditional count-based optimism, and its dominant role over the action-value estimates in coordinated exploration.
In general, these results are consistent with MARL ablation results from \cref{sec:exp-ablation}.
% resulting in almost indistinguishable learning curves
% In hard tasks, IndRew-IndOpt is easily trapped by sub-optimalities, leading to linear regret.
% Since agents are fully independent, the chance of hitting the optimal joint action is exponentially low, even though UCB exploration is sound in the single-agent setting.

% moved above, side-by-side with ablation results table
% \begin{table}
%     \centering
%     \caption{Example bandit problem with one optimal joint action. Each entry represents the probability of Bernoulli reward.}\label{tab:bandit-payoff}
%     \begin{tabular}{*{5}{|c}|}
%       \hline
%       $a_i$ \textbackslash $a_2$ & A & B & C & D \\
%       \hline
%       A & 0.1 & 0.1 & 0.1 & 0.1 \\
%       \hline
%       B & 0.1 & 0.1 & 0.1 & 0.1 \\
%       \hline
%       C & 0.1 & $\bm{0.9}$ & 0.1 & 0.1 \\
%       \hline
%       D & 0.1 & 0.1 & 0.1 & 0.1 \\
%       \hline
%     \end{tabular}
% \end{table}





\section{Conclusions}\label{sec:conclusion}
In this paper, we draw the connection between cooperative multi-agent reinforcement learning (MARL) and tree search.
Inspired by the tree search algorithm UCT, we propose a multi-agent exploration method Conditionally Optimistic Exploration (COE), that utilizes the sequential decision-making scheme and visitation count conditioned on previous agents' actions.
Empirical results show that our method significantly outperforms state-of-the-art MARL baselines in sparse-reward hard-exploration tasks, and matches their performance in general coordination tasks.

% 1. count
One limitation of our method is that it may require a large amount of memory due to storing visitation counts of state-action tuples during training, which makes our method hard to scale to tasks with very large state-action space.
An interesting future work is to utilize neural network density models to estimate pseudo-counts.
Training such a model would require more computation but the model itself only occupies constant memory, which could eliminate potentially high memory usage.
% 2. different levels of dependency among agents? survey theory works
% 3. a mixer that fixes COE-Cond-CQ?



% \xutong{move to discussion}
% COE's exploration scheme enables agents to effectively find high-performing cooperative strategies.
% As COE explores sufficiently times, agents gain knowledge of the corresponding region in the joint state-action space, with the uncertainty level spontaneously decreasing.
% The learned action value as well as decayed conditional optimism would reflect the sub-optimality of inferior collaboration modes, leading agents to explore and commit to more promising strategies.



\begin{acknowledgements}
We acknowledge the computational resources provided by the Digital Research Alliance of Canada.
Janarthanan Rajendran acknowledges the support of the IVADO postdoctoral fellowship.
Sarath Chandar acknowledges the support of the Canada CIFAR AI Chair program and an NSERC Discovery Grant.
\end{acknowledgements}


% References
% TODO xutong comment out the following and paste .bbl content for arxiv
% \bibliography{marl_references}
\input{marl_references}



% TODO xutong compile together with appendix for arxiv
\newpage
\onecolumn
\appendix
\input{marl_appendix_content}


\end{document}
