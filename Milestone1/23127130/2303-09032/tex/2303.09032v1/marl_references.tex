\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Albrecht and Ramamoorthy(2015)]{albrecht2015game}
Stefano~V Albrecht and Subramanian Ramamoorthy.
\newblock A game-theoretic model and best-response learning method for ad hoc
  coordination in multiagent systems.
\newblock \emph{arXiv preprint arXiv:1506.01170}, 2015.

\bibitem[Amin et~al.(2021)Amin, Gomrokchi, Satija, van Hoof, and
  Precup]{amin2021survey}
Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup.
\newblock A survey of exploration methods in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.00157}, 2021.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47:\penalty0 235--256, 2002.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and
  Zilberstein]{bernstein2002complexity}
Daniel~S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein.
\newblock The complexity of decentralized control of markov decision processes.
\newblock \emph{Mathematics of operations research}, 27\penalty0 (4):\penalty0
  819--840, 2002.

\bibitem[Chen et~al.(2021)Chen, Zhang, Xu, Ma, Yang, Song, Wang, and
  Wu]{chen2021variational}
Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang, Jiaming Song,
  Yu~Wang, and Yi~Wu.
\newblock Variational automatic curriculum learning for sparse-reward
  cooperative multi-agent problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 9681--9693, 2021.

\bibitem[Christianos et~al.(2020)Christianos, Sch{\"a}fer, and
  Albrecht]{christianos2020shared}
Filippos Christianos, Lukas Sch{\"a}fer, and Stefano Albrecht.
\newblock Shared experience actor-critic for multi-agent reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 10707--10717, 2020.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{houthooft2016vime}
Rein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter
  Abbeel.
\newblock Vime: Variational information maximizing exploration.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Jaques et~al.(2019)Jaques, Lazaridou, Hughes, Gulcehre, Ortega,
  Strouse, Leibo, and De~Freitas]{jaques2019social}
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro
  Ortega, DJ~Strouse, Joel~Z Leibo, and Nando De~Freitas.
\newblock Social influence as intrinsic motivation for multi-agent deep
  reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  3040--3049. PMLR, 2019.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020.

\bibitem[Kim et~al.(2020)Kim, Jung, Cho, and Sung]{kim2020maximum}
Woojun Kim, Whiyoung Jung, Myungsik Cho, and Youngchul Sung.
\newblock A maximum mutual information framework for multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2006.02732}, 2020.

\bibitem[Kocsis and Szepesv{\'a}ri(2006)]{kocsis2006bandit}
Levente Kocsis and Csaba Szepesv{\'a}ri.
\newblock Bandit based monte-carlo planning.
\newblock In \emph{Machine Learning: ECML 2006: 17th European Conference on
  Machine Learning Berlin, Germany, September 18-22, 2006 Proceedings 17},
  pages 282--293. Springer, 2006.

\bibitem[Kuba et~al.(2021)Kuba, Chen, Wen, Wen, Sun, Wang, and
  Yang]{kuba2021trust}
Jakub~Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang,
  and Yaodong Yang.
\newblock Trust region policy optimisation in multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2109.11251}, 2021.

\bibitem[Li et~al.(2021)Li, Wang, Wu, Zhao, Yang, and Zhang]{li2021celebrating}
Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie
  Zhang.
\newblock Celebrating diversity in shared multi-agent reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3991--4002, 2021.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Liu, Zhang, Wei, Niu, Yang, Liu, and
  Ouyang]{li2022ace}
Chuming Li, Jie Liu, Yinmin Zhang, Yuhong Wei, Yazhe Niu, Yaodong Yang, Yu~Liu,
  and Wanli Ouyang.
\newblock Ace: Cooperative multi-agent q-learning with bidirectional
  action-dependency.
\newblock \emph{arXiv preprint arXiv:2211.16068}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Tang, Yang, Hao, Sang, Zheng, Hao,
  Taylor, and Wang]{li2022pmic}
Pengyi Li, Hongyao Tang, Tianpei Yang, Xiaotian Hao, Tong Sang, Yan Zheng,
  Jianye Hao, Matthew~E Taylor, and Zhen Wang.
\newblock Pmic: Improving multi-agent reinforcement learning with progressive
  mutual information collaboration.
\newblock \emph{arXiv preprint arXiv:2203.08553}, 2022{\natexlab{b}}.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Pieter~Abbeel, and
  Mordatch]{lowe2017multi}
Ryan Lowe, Yi~I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter~Abbeel, and Igor
  Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and
  Whiteson]{mahajan2019maven}
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.
\newblock Maven: Multi-agent variational exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Mordatch and Abbeel(2018)]{mordatch2018emergence}
Igor Mordatch and Pieter Abbeel.
\newblock Emergence of grounded compositional language in multi-agent
  populations.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Oliehoek and Amato(2016)]{oliehoek2016concise}
Frans~A Oliehoek and Christopher Amato.
\newblock \emph{A concise introduction to decentralized POMDPs}.
\newblock Springer, 2016.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, Oord, and
  Munos]{ostrovski2017count}
Georg Ostrovski, Marc~G Bellemare, A{\"a}ron Oord, and R{\'e}mi Munos.
\newblock Count-based exploration with neural density models.
\newblock In \emph{International conference on machine learning}, pages
  2721--2730. PMLR, 2017.

\bibitem[Papoudakis et~al.(2020)Papoudakis, Christianos, Sch{\"a}fer, and
  Albrecht]{papoudakis2020benchmarking}
Georgios Papoudakis, Filippos Christianos, Lukas Sch{\"a}fer, and Stefano~V
  Albrecht.
\newblock Benchmarking multi-agent deep reinforcement learning algorithms in
  cooperative tasks.
\newblock \emph{arXiv preprint arXiv:2006.07869}, 2020.

\bibitem[Perrusqu{\'\i}a et~al.(2021)Perrusqu{\'\i}a, Yu, and
  Li]{perrusquia2021multi}
Adolfo Perrusqu{\'\i}a, Wen Yu, and Xiaoou Li.
\newblock Multi-agent reinforcement learning for redundant robot control in
  task-space.
\newblock \emph{International Journal of Machine Learning and Cybernetics},
  12:\penalty0 231--241, 2021.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, Schroeder, Farquhar, Foerster,
  and Whiteson]{rashid2018qmix}
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
  Foerster, and Shimon Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  4295--4304. PMLR, 2018.

\bibitem[Rashid et~al.(2020)Rashid, Peng, Boehmer, and
  Whiteson]{rashid2020optimistic}
Tabish Rashid, Bei Peng, Wendelin Boehmer, and Shimon Whiteson.
\newblock Optimistic exploration even with a pessimistic initialisation.
\newblock \emph{arXiv preprint arXiv:2002.12174}, 2020.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, De~Witt, Farquhar, Nardelli,
  Rudner, Hung, Torr, Foerster, and Whiteson]{samvelyan2019starcraft}
Mikayel Samvelyan, Tabish Rashid, Christian~Schroeder De~Witt, Gregory
  Farquhar, Nantas Nardelli, Tim~GJ Rudner, Chia-Man Hung, Philip~HS Torr,
  Jakob Foerster, and Shimon Whiteson.
\newblock The starcraft multi-agent challenge.
\newblock \emph{arXiv preprint arXiv:1902.04043}, 2019.

\bibitem[Seow et~al.(2009)Seow, Dang, and Lee]{seow2009collaborative}
Kiam~Tian Seow, Nam~Hai Dang, and Der-Horng Lee.
\newblock A collaborative multiagent taxi-dispatch system.
\newblock \emph{IEEE Transactions on Automation science and engineering},
  7\penalty0 (3):\penalty0 607--616, 2009.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Snedecor and Cochran(1980)]{snedecor1980statistical}
George~W Snedecor and William~G Cochran.
\newblock Statistical methods. iowa.
\newblock \emph{Iowa State University Press. Starkstein, SE, \& Robinson, RG
  (1989). Affective disorders and cerebral vascular disease. The British
  Journal of Psychiatry}, 154:\penalty0 170--182, 1980.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{son2019qtran}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock Qtran: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  5887--5896. PMLR, 2019.

\bibitem[Sunehag et~al.(2017)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et~al.]{sunehag2017value}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z Leibo, Karl
  Tuyls, et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning.
\newblock \emph{arXiv preprint arXiv:1706.05296}, 2017.

\bibitem[Tan(1993)]{tan1993multi}
Ming Tan.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, pages 330--337, 1993.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Xi~Chen, Duan,
  Schulman, DeTurck, and Abbeel]{tang2017exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi~Chen, Yan
  Duan, John Schulman, Filip DeTurck, and Pieter Abbeel.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Ye, and Lu]{wang2022more}
Jiangxing Wang, Deheng Ye, and Zongqing Lu.
\newblock More centralized training, still decentralized execution: Multi-agent
  conditional policy factorization.
\newblock \emph{arXiv preprint arXiv:2209.12681}, 2022.

\bibitem[Wang et~al.(2020)Wang, Ren, Liu, Yu, and Zhang]{wang2020qplex}
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang.
\newblock Qplex: Duplex dueling multi-agent q-learning.
\newblock \emph{arXiv preprint arXiv:2008.01062}, 2020.

\bibitem[Wang et~al.(2019)Wang, Wang, Wu, and Zhang]{wang2019influence}
Tonghan Wang, Jianhao Wang, Yi~Wu, and Chongjie Zhang.
\newblock Influence-based multi-agent exploration.
\newblock \emph{arXiv preprint arXiv:1910.05512}, 2019.

\bibitem[Wen et~al.(2022)Wen, Kuba, Lin, Zhang, Wen, Wang, and
  Yang]{wen2022multi}
Muning Wen, Jakub~Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang,
  and Yaodong Yang.
\newblock Multi-agent reinforcement learning is a sequence modeling problem.
\newblock \emph{arXiv preprint arXiv:2205.14953}, 2022.

\bibitem[Yang et~al.(2020)Yang, Jin, Wang, Wang, and Jordan]{yang2020function}
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael~I Jordan.
\newblock On function approximation in reinforcement learning: Optimism in the
  face of large state spaces.
\newblock \emph{arXiv preprint arXiv:2011.04622}, 2020.

\bibitem[Zheng et~al.(2021)Zheng, Chen, Wang, He, Hu, Chen, Fan, Gao, and
  Zhang]{zheng2021episodic}
Lulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen,
  Changjie Fan, Yang Gao, and Chongjie Zhang.
\newblock Episodic multi-agent reinforcement learning with curiosity-driven
  exploration.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3757--3769, 2021.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Brian~D Ziebart.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\end{thebibliography}