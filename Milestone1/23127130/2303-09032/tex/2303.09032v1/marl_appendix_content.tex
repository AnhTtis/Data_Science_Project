\section{Pseudo-count for Deep RL}\label{apx:pseudo-count}
% Extending the method to a continuous domain is a nontrivial task.
The original UCT was studied in a tabular setting.
This section introduces how we apply the static hashing \citep{tang2017exploration} method to obtain pseudo-counts in large or continuous state space.

In particular, the state $s \in \statespace$ is projected to a lower-dimensional feature space by $\phi(s) = sgn (A g(s)) \in \{-1, 1\}^k$, 
where $g: \statespace \rightarrow \mathbb{R}^D$ is an optional pre-processing function,
$A \in \mathbb{R}^{k \times D}$ is a projection matrix with entries drawn i.i.d. from a unit Gaussian distribution $\mathcal{N}(0,1)$,
and $sgn(\cdot)$ is the element-wise sign function.
This method clusters similar states in $\statespace$ to one feature in a small, countable feature space, which enables us to count.
The $k$ value controls the granularity of state approximation: higher $k$ leads to more distinguishable features yet less generalizability across similar states.
We record the visitation count for the tuple of the state feature $\phi(s)$ and all agents' joint action $\jointaction$, denoted by $N(s, \jointaction)$ for simplicity of notation.
Note that for each agent $i$, the count up to its action $a_i$ satisfies:
\begin{align*}
    N(s, a_{<i}, a_i) &= \sum_{a_{i+1}} N(s, a_{<i}, a_i, a_{i+1}) \\
        &= \sum_{a_{>i}} N(s, a_{<i}, a_i, a_{>i}),
\end{align*}
where $a_{<i}$ and $a_{>i}$ denote the joint actions taken by preceding and subsequent agents of $i$, respectively.
This relationship shows that we can obtain any count up to $a_i$ by summing up the counts of joint actions that overlap $a_{<i}$ at state $s$.
This relationship is naturally aligned with the tree structure, where the total count of each node equals the number of action sequences going through that node.
Thus we are able to perform optimistic exploration using conditional counts.
% This is more memory-efficient than recording the count for every single node.



\section{Environment details}\label{apx:exp-environ}

\paragraph{Multi-Agent Particle Environment}
Multi-Agent Particle Environment (MPE) \citep{lowe2017multi,mordatch2018emergence} is a suite of two-dimensional navigation tasks where the entities in the environment obey physics properties.
We choose three tasks that do not involve agent-wide communication: \textsl{Sparse Spread}, \textsl{Sparse Tag}, and \textsl{Adversary}.
In the first two tasks, reward signals are sparse and agents receive positive rewards only when they jointly complete the task.
They are almost fully observable except each agent does not observe the velocity of other agents.
\textsl{Adversary} is fully observable.
% TODO describe each task

\paragraph{Level-Based Foraging}
Level-Based Foraging (LBF) \citep{albrecht2015game,christianos2020shared,papoudakis2020benchmarking} is a set of food-collection tasks in a grid-world.
Each agent or food item is assigned a level value, such that a group of agents can pick up a food item if the sum of agents' levels is greater than or equal to the item's level.
Agents receive a positive reward only when a food item is picked up, hence LBF requires efficient coordinated exploration.
We choose four tasks with different grid dimensions, number of agents, and number of food items.
By default, they are all fully observable.

\paragraph{StarCraft Multi-Agent Challenge}
StarCraft Multi-Agent Challenge (SMAC) \citep{samvelyan2019starcraft} consists of battle tasks where a group of agents is learned to defeat another group.
Each agent could only observe entities within a fixed-sized window.
All tasks have dense rewards, and agents start engaging immediately after the game starts.
As \citet{mahajan2019maven} point out, SMAC tasks are not designed to evaluate cooperative exploration.
In order to assess coordination in partially-observable and non-stationary settings, we choose one easy task \textit{2s-vs-1sc} and one hard task \textit{3s-vs-5z}.
% TODO describe each task



\section{Evaluation Protocol}\label{apx:eval-protocol}
In each task we train all algorithms for four million timesteps.
During training we perform $41$ evaluations at constant timestep intervals, that is, 100k timestep intervals, and at each evaluation point we evaluate for 100 episodes.
We train each algorithm with parameter sharing, where all agent networks share the same set of parameters, and the one-hot identity of each agent as additional network input helps the neural network to develop diverse behaviour.


% \textbf{Performance metrics}
We evaluate algorithms' performance in a task by two metrics: maximum returns and average returns.
The maximum return refers to the highest mean evaluation return across five seeds achieved at one evaluation point during training.
This metric evaluates algorithms' best-reached performance in a task
The average return is the evaluation return averaged over all evaluation points during training.
This metric reflects both sample efficiency and final performance.



\section{Additional results}\label{apx:more-results}

Table~\ref{tab:max-return-all} summarizes the \textit{maximum} returns for all eight algorithms (including the ablations) in all nine tasks,
which also reports the maximum win-rates in SMAC tasks.
Figure~\ref{fig:marl-learning-curves-ablation} presents learning curves of the evaluation returns achieved during training by ablations in all nine tasks.
Sparse-reward tasks have bold titles.
% Conditional optimism improves performance with different Q-value estimation approaches.

% The highest value in each task is in bold. Asterisks denote algorithms that are not significantly different from the best algorithm in each task.

% https://tex.stackexchange.com/questions/89115/how-to-rotate-text-in-multirow-table
% https://tex.stackexchange.com/questions/66564/vertical-alignment-using-multirow-and-booktabs
% \sisetup{table-align-text-post=false}
\begin{table*}[!htb]
    \tiny
    \centering
    \caption{Maximum Returns and 95\% Confidence Interval for All Eight Algorithms in All Nine Tasks, and Maximum Win-rates for SMAC Tasks.}\label{tab:max-return-all}
    \begin{tabular}{*{3}{l}*{8}{c}}
      \toprule % from booktabs package
      && \multicolumn{1}{l}{\textbf{Tasks \textbackslash Algs.}} & \multicolumn{1}{c}{COE} & \multicolumn{1}{c}{COE-Cond-IQ} & \multicolumn{1}{c}{COE-Cond-CQ} & \multicolumn{1}{c}{UCB-Ind} & \multicolumn{1}{c}{UCB-Cen} & \multicolumn{1}{c}{EMC} & \multicolumn{1}{c}{MAVEN} & \multicolumn{1}{c}{QMIX} \\
      \midrule
      \multirow{3}{*}{\rotatebox[origin=c]{90}{MPE}}
      && Adversary & $22.68 \pm 0.80$ & $19.18 \pm 1.70$ & $24.14 \pm 0.83$ & $23.16 \pm 1.28$ & $23.02 \pm 0.93$  & $22.03 \pm 2.12$  & $23.52 \pm 1.50$  & $22.70 \pm 1.61$  \\
      && Sparse Tag & $1.60 \pm 0.41$ & $0.16 \pm 0.18$ & $1.98 \pm 0.77$ & $1.28 \pm 0.31$ & $1.44 \pm 0.05$  & $1.23 \pm 0.35$  & $0.06 \pm 0.03$ & $1.16 \pm 0.29$  \\
      && Sparse Spread & $2.11 \pm 1.86$ & $0.99 \pm 0.85$ & $1.46 \pm 1.05$ & $1.51 \pm 1.06$ & $1.80 \pm 1.15$  & $1.31 \pm 0.92$  & $0.43 \pm 0.85$  & $1.46 \pm 0.28$  \\
      \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{LBF}}
      && 10x10-3p-3f & $0.99 \pm 0.01$ & $0.98 \pm 0.02$ & $0.98 \pm 0.01$ & $0.98 \pm 0.02$ & $0.99 \pm 0.01$ & $0.96 \pm 0.04$  & $0.37 \pm 0.18$ & $0.94 \pm 0.03$ \\
      && 15x15-3p-5f & $0.45 \pm 0.10$ & $0.36 \pm 0.09$ & $0.29 \pm 0.15$ & $0.37 \pm 0.08$ & $0.31 \pm 0.14$  & $0.24 \pm 0.04$ & $0.04 \pm 0.01$ & $0.20 \pm 0.02$ \\
      && 15x15-4p-3f & $0.93 \pm 0.03$ & $0.89 \pm 0.02$ & $0.63 \pm 0.13$ & $0.75 \pm 0.11$ & $0.48 \pm 0.31$ & $0.71 \pm 0.13$ & $0.06 \pm 0.01$ & $0.51 \pm 0.09$ \\
      && 15x15-4p-5f & $0.69 \pm 0.08$ & $0.38 \pm 0.05$ & $0.32 \pm 0.07$ & $0.52 \pm 0.20$ & $0.57 \pm 0.15$  & $0.50 \pm 0.08$ & $0.05 \pm 0.01$ & $0.33 \pm 0.04$ \\
      \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{SMAC}}
      &\multirow{2}{*}{\rotatebox[origin=c]{90}{ret}} & 2s-vs-1sc & $20.25 \pm 0.01$ & $19.57 \pm 0.73$ & $20.24 \pm 0.00$ & $15.88 \pm 7.79$ & $20.19 \pm 0.07$  & $20.22 \pm 0.06$ & $20.22 \pm 0.04$  & $20.16 \pm 0.05$ \\
      && 3s-vs-5z & $21.32 \pm 0.75$ & $21.16 \pm 0.56$ & $21.47 \pm 0.59$ & $16.93 \pm 4.24$ & $19.86 \pm 5.03$  & $14.84 \pm 4.19$ & $20.15 \pm 1.43$  & $18.57 \pm 3.01$  \\
      \cmidrule{2-11}
      &\multirow{2}{*}{\rotatebox[origin=c]{90}{win}} & 2s-vs-1sc & $1.00 \pm 0.00$ & $0.92 \pm 0.09$ & $1.00 \pm 0.00$ & $0.77 \pm 0.38$ & $0.99 \pm 0.01$  & $1.00 \pm 0.00$ & $1.00 \pm 0.00$ & $0.99 \pm 0.00$ \\
      && 3s-vs-5z & $0.97 \pm 0.00$ & $0.93 \pm 0.05$ & $0.98 \pm 0.02$ & $0.56 \pm 0.45$ & $0.61 \pm 0.37$  & $0.27 \pm 0.37$ & $0.87 \pm 0.16$  & $0.65 \pm 0.30$  \\
      \bottomrule
    \end{tabular}
\end{table*}


\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{img/marl_learning_curves_ablation.pdf}
  \caption{Episodic Returns and 95\% Confidence Interval for All Ablations in All Tasks.}\label{fig:marl-learning-curves-ablation}
\end{figure*}



\section{Ablation details}\label{apx:ablation}

In this section, we present in detail the ablation variants
% : COE-Cond-IQ, COE-Cond-CQ, UCB-Ind, and UCB-Cen.
introduced in \cref{sec:exp-ablation}.
% TODO xutong remove hardcoded ref for arxiv.
% We discuss their potential limitations that may prohibit agents from efficient cooperative exploration.

% \subsection{COE with Explicitly Dependent Q-values}
% explicit correlation in q
% We introduce a variant called COE-Cond-IQ that directly adopts the idea of UCT.
% We discuss its limitations and reasons it may prohibit agents from efficient exploration.
% We then introduce another variant called COE-Cond-CQ that combines COE and COE-Cond-IQ.
% We also discuss its potential pitfalls.


% As opposed to the implicit influence learned through the centralized Q-value function, we could alternatively learn the explicit correlation among agents.
% This can be realized by directly adopting the idea of UCT tree search.

COE-Cond-IQ directly adopts the idea of UCT, without considering the partial observability issue of each agent.
In order to enable decentralized execution, we simultaneously learn a Q-value function dependent on preceding agents' actions and its independent counterpart.
Similar to the MACPF factorization \citep{wang2022more}, each agent $i$ has an independent Q-network $Q^{ind}_i (\tau_i, a_i; \phi_i)$ parameterized by $\phi_i$,
and a dependency correction network $c^{dep}_i(\tau_i, a_i | a_{<i}; \psi_i)$ parameterized by $\psi_i$,
whose sum constructs the dependent Q-network $Q^{dep}_i(\tau_i, a_i | a_{<i}; \phi_i, \psi_i) = Q^{ind}_i (\tau_i, a_i; \phi_i) + c^{dep}_i(\tau_i, a_i | a_{<i}; \psi_i)$.

% \begin{align}
% \label{eq:q-joint}
%     & \qjt^{dep}(\jointhist, \jointaction) = Mixer( [ Q^{dep}_i(a_i | \tau_i, a_{<i}) ]^N_{i=1}, s; \theta ) \\
%     & Q^{ind}_{jt}(\jointhist, \jointaction) = Mixer( [ Q^{ind}_i(a_i | \tau_i) ]^N_{i=1}, s; \theta )
% \end{align}

Individual agent's action-value networks $Q^{dep}_i$ and $Q^{ind}_i$ are separately trained by minimizing the mean-squared TD error on each Q-network:
\begin{align}
    \mathcal{L}^{dep}_i( \psi_i ) = \EE_\replay [ (Q^{dep}_i(\tau_i, a_i) - y^{dep}_i )^2 ] \label{eq:td-err-dep-i} \\
    \mathcal{L}^{ind}_i( \phi_i ) = \EE_\replay [ (Q^{ind}_i(\tau_i, a_i) - y^{ind}_i )^2 ] \label{eq:td-err-ind-i}
\end{align}
where $y^{dep}_i = (r + \gamma \max_{a_i'} (Q^{dep}_i(\tau_i', a_i') ) )$ and $y^{ind}_i = (r + \gamma \max_{a_i'} (Q^{ind}_i(\tau_i', a_i') ) )$ are the update targets, and $\replay$ contains trajectory data collected by $Q^{dep}_i$'s.
To ensure $Q^{dep}_i$ and $Q^{ind}_i$ achieve the same performance, they are constructed and trained in a way that strengthens their coupling:
$Q^{dep}_i$ is the combination of $Q^{ind}_i$ and a correction network;
during training the same mini-batch of trajectory data sampled from $\replay$ is used to compute both $\mathcal{L}^{dep}_i$ and $\mathcal{L}^{ind}_i$.
% consistency between

COE exploration is applied to this variant in a similar way as being applied to value decomposition methods.
The optimistic bonus is added to $Q^{dep}_i$ at action selection during training.
Note that for each agent $i$ the optimistic TD update target is applied to both \cref{eq:td-err-dep-i} and \cref{eq:td-err-ind-i}:
\begin{align}\label{eq:ucb-target-i}
    & y_i = \left( r(s, \jointaction) + \frac{c_\subrew}{\sqrt{N(s, a_{<i}, a_i)}} \right)
    + \gamma \max_{a_i'} \left( Q_i(\tau_i', a_i') + \frac{c_\subboot}{\sqrt{N(s', a_{<i}', a_i')}} \right),
\end{align}
where $c_\subrew, c_\subboot \in \mathbb{R}_+$ are hyper-parameters controlling the scale of the optimistic bias in reward and bootstrapped target, respectively.
During decentralized execution, agents take actions according to $Q^{ind}_i$'s only.

% \xutong{discuss its limitations}
% . uct+iql (ie iql+ucb cond): direct adoptation of uct to marl, principled. limitations: (1) each agent observes local obs/traj instead of state, violates uct's sufficient condition; (2) need to make it ctde, which violates independent learning; (3) this ctde is not generalizable to other value decomp methods
% is a principled MARL analogue to UCT, and it
We name this variant COE-Cond-IQ as it could be considered as a direct adoption of UCT to IQL \citep{tan1993multi}.
As opposed to the utility function that learns implicit dependency via centralized training in value decomposition methods, each agent learns a Q-value function, that explicitly captures the correlation among agents by conditioning on previous agents' actions.
COE-Cond-IQ also complies with the CTDE paradigm.
However, it ignores the partial observability of each individual agent.
Each agent only has access to its own local trajectory history.
% However, it has a few major limitations.
% The additional information each agent receives is merely prior agents' actions, hence the influence each agent learns about others is highly restricted.
% This limitation may hinder agents from performing efficient exploration, depending on how well the history can reflect the global state information.
% In this sense tree search is not suitable for Dec-POMDP, serving as a decisive factor that our COE is different from UCT.
% Another limitation of COE-Cond-IQ is its compatibility.
% Dependent Q-network renders COE-Cond-IQ a violation of independent learning, i.e., it is no longer \textit{independent} Q-learning.
% On the other hand, as a value-based CTDE method, it is not applicable to any value decomposition method.

% One way to solve COE-Cond-IQ's issues is to combine COE and COE-Cond-IQ.
% We name this variant COE-Cond-CQ.
Another ablation we introduce is COE-Cond-CQ, which combines centralized training and COE-Cond-IQ.
The same mixing network $Mixer(\cdot ; \theta)$ we use in COE is used to compute both dependent and independent joint Q-values:
\begin{align}
    & Q^{dep}_{jt}(\jointhist, \jointaction) = Mixer( [ Q^{dep}_i(\tau_i, a_i) ]^N_{i=1}, s; \theta ) \label{eq:q-joint-dep-condcq} \\
    & Q^{ind}_{jt}(\jointhist, \jointaction) = Mixer( [ Q^{ind}_i(a_i | \tau_i) ]^N_{i=1}, s; \theta ) \label{eq:q-joint-ind-condcq}
\end{align}
Similarly, centralized training also optimizes both dependent and independent mean-squared TD error:
\begin{align}
    \mathcal{L}^{dep}( [\psi]^N_{i=1}, \theta ) = \EE_\replay [ (Q^{dep}_{jt}(\jointhist, \jointaction) - y^{dep} )^2 ] \label{eq:td-err-dep-condcq} \\
    \mathcal{L}^{ind}( [\phi]^N_{i=1}, \theta ) = \EE_\replay [ (Q^{ind}_{jt}(\jointhist, \jointaction) - y^{ind} )^2 ] \label{eq:td-err-ind-condcq}
\end{align}
where $y^{dep} = (r + \gamma \max_{\jointaction'} (Q^{dep}_{jt}(\jointhist', \jointaction') ) )$ and $y^{ind} = (r + \gamma \max_{\jointaction'} (Q^{ind}_{jt}(\jointhist', \jointaction') ) )$ are update targets for dependent and independent networks, respectively.
Exploration is performed the same way as COE, and action selection is performed the same way as COE-Cond-IQ.

% In COE-Cond-CQ each agent's Q-function simultaneously captures both implicit and explicit dependency among agents through centralized training and action conditioning.
% This architecture increases the expressiveness of Q-functions.
% However, one potential trap is that these two types of dependencies may not align with each other, causing inefficient performance improvement.
% This variant ignores the potential issue that within individual Q-values the inter-dependency among agents captured by centralized training and that captured by action conditioning may not be aligned with each other.
% Designing a mixing network that satisfies IGM and coordinates dependencies from different sources is beyond the scope of this paper.

In the ablation UCB-Ind, each agent performs UCB-based exploration independently.
It is straightforward to obtain UCB-Ind: we simply replace any conditional count terms in COE with independent counts, which do not rely on other agents' actions.

The ablation UCB-Cen augments the global reward with an intrinsic reward $\frac{c_\subrew}{\sqrt{N(s, \jointaction)}}$.
Agents learn optimistic Q-values through centralized training.



\section{Hyperparameter settings}\label{apx:hyperparam}

To perform hyperparameter optimization we follow the same protocol presented by \citet{papoudakis2020benchmarking}.
We select one task from each benchmark environment and optimize the hyperparameters of all algorithms in this task.
In particular, we select \textit{Sparse Tag} from MPE, \textit{15x15-3p-5f} from LBF, and \textit{3s-vs-5z} from SMAC.
We perform a coarse grid search on hyperparameter settings and train each configuration with three seeds.
We identify the best configuration according to the maximum evaluation returns.
This best configuration on each task is then used for all tasks in the respective environment for the final experiments with five seeds.
% \cref{apx:hyperparam} presents the hyperparameter settings we sweep as well as the best hyperparameters for each method in each environment.

For methods that use intrinsic reward, we only test constant intrinsic reward scales.
For COE, the hyperparameter combination with $c_\subact = c_\subrew = c_\subboot = 0$ is ignored.
For MAVEN, We sweep the intrinsic scales only when "MI intrinsic" is True.
The hyperparameters "MI intrinsic" and "RNN discriminator" cannot both be True.
When MAVEN uses $\varepsilon$-greedy, the epsilon annealing time is 50k timesteps.
QMIX's starting epsilon value is 1.0.

\begin{table}[!htb]
    \centering
    \caption{Common QMIX Hyperparameters for All algorithms across All Tasks.}\label{tab:apx-common-hyperparam}
    \begin{tabular}{cc}
      \toprule 
      Hyperparameter Name & Value \\
      \midrule
      hidden dimension & 128 \\
      reward standardization & True \\
      network type & GRU \\
      evaluation epsilon & 0 \\
      target update & 0.01 (soft) \\
      \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Hyperparameter Values for COE Swept in Grid-search.}\label{tab:apx-hyper-coe-sweep}
    \begin{tabular}{cc}
      \toprule 
      Hyperparameter Name & Sweep values \\
      \midrule
      learning rate & 0.0001/0.0003/0.0005 \\
      feature dimension $k$ & 8/12/16 \\
      $c_\subact$ & 0/0.01/0.05 \\
      $c_\subrew$ & 0/0.01/0.05 \\
      $c_\subboot$ & 0/0.01/0.05 \\
      \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Hyperparameter Values for EMC Swept in Grid-search.}\label{tab:apx-hyper-emc-sweep}
    \begin{tabular}{cc}
      \toprule 
      Hyperparameter Name & Sweep values \\
      \midrule
      learning rate & 0.0001/0.0003/0.0005 \\
      curiosity scale & 0.001/0.005/0.01/0.05/0.1/0.5/1.0 \\
      \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Hyperparameter Values for MAVEN Swept in Grid-search.}\label{tab:apx-hyper-maven-sweep}
    \begin{tabular}{cc}
      \toprule 
      Hyperparameter Name & Sweep values \\
      \midrule
      learning rate & 0.0001/0.0003/0.0005 \\
      RNN discriminator & True/False \\
      MI intrinsic & True/False \\
      curiosity scale & 0.001/0.005/0.01/0.05/0.1/0.5/1.0 \\
      noise bandit & True/False \\
      epsilon start & 0.0/1.0 \\
      \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Hyperparameter Values for QMIX Swept in Grid-search.}\label{tab:apx-hyper-qmix-sweep}
    \begin{tabular}{cc}
      \toprule 
      Hyperparameter Name & Sweep values \\
      \midrule
      learning rate & 0.0001/0.0003/0.0005 \\
      epsilon anneal & 50,000/200,000 \\
      \bottomrule
    \end{tabular}
\end{table}

