\section{Methodology}\label{sec:methods}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/method_overview.png}
    \caption{\textbf{Overview of our method}. Our method processes inputs as dense and spike-based representations. The ANN uses the dense representation to perform state and output initialization at low rates. The SNN then uses spikes to generate high-rate outputs until the next dense input.}
    \label{fig:method_overview}
\end{figure}

\begin{figure*}[hbt]
\centering
  \includegraphics[width=1.0\textwidth]{imgs/method_hybrid_arch.pdf}
  \caption{\textbf{Hybrid ANN - SNN architecture.} The ANN (upper row of blocks) is fed with past events at time step $t_0$, where an initial output is predicted, and states of spiking neurons are initialized (orange blocks). Events of duration $\Delta T$ are fed sequentially to the SNN (lower row of blocks), for high-rate updates of the prediction.}
  \label{fig:hybrid_arch}
\end{figure*}

An overview of our approach is depicted in Fig.~\ref{fig:method_overview}. In our hybrid ANN-SNN approach, the ANN is utilized to accurately predict joint locations based on prior events and to simultaneously initialize the spiking neuron states. 
This sets the stage for high-frequency, low-latency updates using the SNN, where events of duration $\Delta T$ are sequentially fed to the SNN.
After the sequence duration $T$, the process is repeated where a new prediction is made by the ANN, and states of spiking neurons are re-initialized.
The integration of the ANN at low rates and the SNN at high rates enables precise predictions to be made with low-latency while maintaining energy efficiency.
The following sections provide a detailed explanation of our hybrid model and its constituent steps.

\subsection{Preliminaries}
Our method takes as input a sequence of spike-based and dense representations. The dense representation can be an image, if synchronized and aligned with events, or any dense event representation computed from raw events. For the remainder of this section, we let $Y_i$ be the dense representation at time $t_i$. In this work, we opted for stacked 2D histograms~\cite{Gehrig19iccv} in case of event data. They are computed by stacking $N=10$ two-channel histograms~\cite{Maqueda18cvpr} from a total of 7,500 events.
We then consider raw binary events up to time $t$ denoted as $X(t)$ with $t > t_i$:
\begin{equation}
\label{eq:spike_train}
    X(t) = \sum_{j | t_j < t} p_j\delta(t-t_j).
\end{equation}
In general, $t_j$ is the timestamp of the $j$-th events, and $p_j\in\mathbb{R}^2$ is the event polarity converted to a one-hot vector.
The ANN, $F_\text{ANN}$, processes the dense event representation and predicts both the output $o$ at time $t$ as well as the initial SNN states $\{s^k\}_{k=1}^L$ for all $L$ layers.
The SNN, $F_\text{SNN}$, then processes the incoming event stream $X$ to continuously update the prediction $o$.
The following equations summarize this process:
\begin{align}
    \{s^k_i\}_{k=1}^L, o_i &= F_\text{ANN}(Y_i)\\
    o(t) &= F_\text{SNN}(t; X, \{s^k_i\}_{k=1}^L, o_i).
\end{align}
Here, $\{s^k_i\}_{k=1}^L$ denote the membrane potential of the SNNs at layers $k=1,...,L$ for timestamp $t_i$. The variable $o_i$ denotes the output map for timestamp $t_i$. Finally, $o(t)$ denotes the human pose estimates at time $t$ represented as 13 heatmaps, one for each body joint. The value at each pixel of the heatmap indicates the probability of finding the joint at that position. It is generated by using the initialization $o_i$ and integrating the output of the SNN onto it.  
In summary, the task of the SNN is to incrementally update the initial prediction that the ANN provides.
While the ANN is a standard U-Net~\cite{Ronneberger15icmicci}, the SNN can be interpreted as a continuous-time model that takes a function $X$ (see Eq.~\eqref{eq:spike_train}) as input and generates a prediction at any time $t$. Next, we will go into more detail on how this model works. 

\subsection{Spiking Neural Network}
SNNs model individual neurons at layer $k$ as dynamical systems that update their membrane potential $V^k$ by integrating a series of input spikes in a learnable way. When their membrane potential exceeds a threshold, it generates spikes which are then transmitted to the next layer, followed by some resetting of the membrane potential. In our work, we use the Leaky Integrate \& Fire (LIF) neuron model~\cite{gerstner_neuronal_dynamics}.
The sub-threshold dynamics of a LIF neuron are defined as. 
\begin{equation}
    \label{eqn:lif}
    \tau \frac{dV^k(t)}{dt} = -(V^k(t) - V_{rest}) + X^k(t).
\end{equation}
Here $V^k(t)$ represents the neuron's membrane potential at time t, and layer $k$, $V_{rest}$ is the resting potential of the neuron, $X^k(t)$ denotes the integrated spike train at time t, and $\tau$ is the membrane time constant. After the membrane potential reaches the firing threshold $V_{th}$, a spike is emitted, and the membrane potential is immediately reset back to its resting potential.   

While conventionally, membrane potentials are initialized at 0 for all neurons and layers; we use the ANN to initialize these potentials in this work. We thus modify Eq.~\ref{eqn:lif} by adding a boundary condition at time $t_i$ for each layer:
\begin{align}
    \label{eqn:lif_modified}
    \tau \frac{dV^k(t)}{dt} &= -(V^k(t) - V_{rest}) + X^k(t). \\
    \nonumber\text{subj. to: } V^k(t_i) &= s^k_i
\end{align}
where $s^k_i$ are the activation initialization maps generated by the ANN. As will be shown later, this small change has a major impact on the SNN behavior since it mitigates delays due to convergence in Eq.~\ref{eqn:lif} and improves performance overall by providing a well-initialized state. Next, we will discuss how we emulate such a continuous dynamical system on conventional hardware. \\

\subsection{Discretization and Training}
To train our ANN-SNN model, we need to convert the SNN into a recurrent network. This is typically done by applying a forward Euler approximation to the differential Eq.~\eqref{eqn:lif_modified}. Eq.~\eqref{eqn:lif_discrete} shows the discretized sub-threshold dynamics, and the spiking mechanism where $H(.)$ is the Heaviside step function and $S_{t}$ denotes the spike output.

\begin{align}
\label{eqn:lif_discrete}
\begin{split}
    & V^k_{t} = V^k_{t-1} + \frac{1}{\tau}(X^k_{t} - (V^k_{t-1} - V_{rest})) \\
    & S^k_{t} = H(V^k_{t} - V_{th})  \\
    \text{subj. to: } & V^k_0 =s^k_i
\end{split}
\end{align}
In the discretized version, time $t$ takes integer values and starts at index 0, which previously corresponded to the timestamp $t_i$. 
To emulate the resetting behavior, we apply soft resets, which reduce the potential by the amount of the threshold value. This allows the residual potential to be re-used at the next steps, resulting in reduced information loss.
Using soft reset neurons has the effect that potential values can be initialized outside the range $[V_{rest}, V_{thr}]$. This allows extreme cases such as dead neurons or always ON neurons.
Soft reset neurons are also known as "Residual Membrane Potential Neurons"~\cite{rmp_neurons}. 


Eqs.~\eqref{eqn:lif_discrete} can be interpreted as a recurrent neural network that can be unrolled over multiple forward Euler steps and then trained using backpropagation through time~\cite{bptt}. However, a challenge in training the above spiking neural networks lies in its use of the Heaviside step function $H$, which is not differentiable. However, this problem can be addressed by using surrogate gradients~\cite{surrogate_gradient}, i.e., replacing the gradient of the Heaviside function with the approximate
\begin{equation}
    H'(x) \approx \frac{1}{1+(\pi x)^2} 
\end{equation}
For more details on SNN training, see \cite{surrogate_gradient}. 



\subsection{Network Details}
The hybrid network details are illustrated in Fig.~\ref{fig:hybrid_arch}. 
For the ANN (top row), we use a U-Net structure~\cite{unet}, adapted from Super SloMo~\cite{super_slomo}. It has a total of 23 layers, comprising a prediction layer, five encoders, and five decoders concatenated with skip connections at the same spatial resolution. 
The SNN (bottom row) is a variant of the U-Net architecture~\cite{unet}, modified from EVSNN~\cite{evsnn}. 
The architecture consists of 10 layers made up of a prediction layer, residual block, four encoder, and four decoder layers. 
At every timestep, events are presented to the network in two channels for each polarity. 
The network is trained with a discretization step of 10 ms. 
At the output, we use a simple convolutional layer and integrator, which allows predicting analog heatmaps.
Each SNN layer is initialized from ANN state initialization modules depicted with orange blocks in Fig.~\ref{fig:hybrid_arch}.
The initialization module reuses the ANN U-Net features and predicts the membrane potential of the SNN spiking neurons.
The initialization modules consist of only two convolutional layers followed by batch normalization.
An ablation study of the state initialization architectures can be found in Sec.~\ref{subsec:results_stateinit_ablation}.
Additional network details about the ANN and SNN are given in the appendix.










