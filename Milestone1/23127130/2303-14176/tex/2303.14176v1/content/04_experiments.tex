\section{Experiments}\label{sec:experiments}
We evaluate our model on two publicly available event-based human pose estimation datasets, DHP19~\cite{dhp19} and Event-Human3.6M~\cite{lifting_monocular_events}. 
Sec.~\ref{sec:exp:setup} starts off with details about the datasets, metrics, and training. We then demonstrate the effectiveness of our hybrid model in reducing power consumption while boosting accuracy in the ablation study section (Sec.~\ref{sec:exp:ablations}). We conclude with a spike activity analysis in Sec.~\ref{sec:spike_activity} and comparison with state-of-the-art event-based human pose estimators in Sec.~\ref{sec:exp:sota}.\\ 

\subsection{Setup}
\label{sec:exp:setup}
\paragraph{Datasets}
The DHP19 dataset \cite{dhp19} is a real-world event camera dataset recorded with 4 synchronized DAVIS346 cameras. Overall, the dataset features 17 different subjects performing a total of 33 movement patterns. DHP19 consists of 556 sequences with labels from a motion capture system at 100 Hz. 
With this dataset, we analyze our pure event-based implementation using real event data.

The Event-Human3.6M \cite{lifting_monocular_events} originates from the Human3.6M dataset~\cite{human36m} and uses an event simulator \cite{Gehrig_2020_CVPR} to generate synthetic event data.
The dataset features 11 subjects and 17 different activities that are more complex than the movements in the DHP19 dataset.
Human3.6M images are captured with 4 synchronized high-resolution cameras and labels with a motion capture system at 50 Hz. 
Our experiments on Event-Human3.6M examine whether our approach also performs well on more complex motion patterns at the cost of using synthetic event data.

\paragraph{Metrics} We measure the \emph{accuracy} using the Mean Per Joint Position Error (MPJPE). For each joint, the Euclidean distance between the predicted and ground truth positions of a joint is calculated.
The MPJPE score is computed as the mean of these errors across all joints in the skeleton body model.
This score is defined for both 2D skeleton estimations (in pixels) and 3D estimation (in millimeters) as: 

\begin{equation}
    \centering
    \label{eqn:mpjpe}
    \text{MPJPE} = \frac{1}{J} \sum_{i = 1}^J \|{x_i - \hat{x}_{i}}\|,
\end{equation}

where $J$ is the number of joints, $x_{i}$ is the ground truth, and $\hat{x}_{i}$ is the estimation of the joint in 2D or 3D space.

\paragraph{Energy Consumption} To measure \emph{energy consumption}, we compute the total number of multiply-accumulate (MAC) and accumulate (AC) operations used by a method. While ANNs perform dense MAC operations, SNNs perform sparse AC operations as a result of the binary nature of spikes. In most technologies, the addition operation is less costly than the multiplication operation. For 7nm CMOS technology, one 32-bit MAC operation uses 1.69 pJ, while one AC only uses 0.38 pJ~\cite{ieee_operation_cost_7nm}, which are the values we use to calculate the power consumption of all methods. For ANNs, we compute the total number of MACs throughout the layers as $k^2 W_o W_o C_i C_o$, where we use the kernel size $k$, output dimension $W_o\times H_o$ and input and output channel dimension $C_i$ and $C_o$. For SNNs, we count the total number of ACs as the above value multiplied by the average spiking activity $\zeta^{l}\in[0,1]$. It is the ratio of the total number of spikes in layer $l$ over all timesteps to the total number of neurons in layer $l$~\cite{spikeact1, spikeact2, spikethrift}. In these calculations, we assume every spike consumes constant energy~\cite{spike_consumption}.

\input{content/ablation_figure.tex}

\paragraph{Training Details}\label{subsec:methods_training}
For SNN training, we use the Spiking Jelly framework~\cite{spikingJelly}, an open-source deep learning framework based on PyTorch~\cite{pytorch}.
The network is unrolled for all time steps to perform BPTT~\cite{bptt} on the average loss of the sequence, $L_{\text{avg}}$. 
We adopt the loss function from~\cite{Calabrese19cvprw}, which computes the difference between heatmaps generated by our method and labels generated from 3D joint labels projected to the pixel space. 
For each 2D joint, ground-truth heatmaps are generated by creating a 2D tensor of zeros at the same spatial resolution as the input with the joint location pixel set to 1. 
To facilitate learning, Gaussian blurring is applied with a filter size of 11 and a standard deviation of 2 pixels to each heatmap.
The loss is computed with respect to the predicted and ground truth heatmaps using the Mean Square Error (MSE) and averaged over several timesteps
\begin{equation}
    \centering
    \label{eqn:mse}
    L_\text{avg} = \frac{1}{JT}\sum_{t=1}^T \sum_{i = 1}^J ({o_{ti} - \hat{o}_{ti}})^{2},
\end{equation}
All experiments were trained on a single GPU, Quadro RTX 8000, for approximately 72 hours.
We first train an ANN with the learning rate 1e-4 of batch size 8 for 60,000 iterations. 
We then train the hybrid model by freezing the ANN weights and only training the SNN. 
Hybrid experiments were trained for 280,000 iterations with an Adam optimizer~\cite{adam}, batch size 2, and learning rate 5e-5 with the neuronal time constant, firing threshold, and output decay $\tau$ set to 3, 1, and 0.8, respectively. 
Pure SNN experiments were trained for 160,000 iterations with the same parameters, with the exception of the time constant $\tau=2$.

\begin{table}[t!]
\centering
\caption{\textbf{State initialization module ablation study} on the validation set. Results are reported on the camera view \#2. The last bin provides the score of the $10^{th}$ time step, and the second column reports the average score of the sequence.}
\label{tab:state_init_ablations}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{State Initialization Mappings}}} & \multicolumn{2}{c}{\textbf{ MPJPE (2D) $\downarrow$}} \\ \cline{2-3} 
\multicolumn{1}{c}{}                          & Last Bin & Average \\ \midrule
\multicolumn{1}{c}{-}                         & 9.12     & 7.08    \\ \midrule
Conv + BN + LeakyReLU + Conv + BN + Sigmoid   & 16.59    & 9.83   \\
Conv + BN + LeakyReLU + Conv + BN + LeakyReLU & 6.32    & 6.14   \\
Conv + BN + LeakyReLU + Conv + BN             & \textbf{6.11}    & \textbf{6.02}   \\
Conv + BN + LeakyReLU + Conv                  & 6.22    & 6.23   \\
Conv + BN + LeakyReLU                         & 6.12    & 6.17   \\ \bottomrule
\end{tabular}
}
\end{table}


\subsection{Ablation Studies}
\label{sec:exp:ablations}

This section first examines the two main contributors to the performance of our proposed model, state initialization and output initialization. 
Second, we examine different state initialization architectures.



\subsubsection{Proposed Method}

Fig.~\ref{fig:method_ablations}~reports ablation studies comparing four variants of our method (A-D) in terms of 2D MPJPE on camera view~2 of DHP19. In E-F, we show the MPJPE over 100 ms of events. 
Model A is a pure SNN with a zero state initialization at time 0. Model B initializes the SNN states from an auxiliary ANN. This ANN processes a dense event representation constructed at time $t=0$ to generate these states, and the SNN then continues predicting within the time interval. Model C does not use state initialization but instead uses the SNN to learn a delta on the ANN prediction at timestamp $t=0$, which we call output initialization. Finally, model D combines the idea of output initialization and state initialization, which is our proposed method.

We see in Fig.~\ref{fig:method_ablations}~(F) that the SNN (A), initialized at 0, achieves a high error due to a slow convergence over the time interval shown. One way to solve this is to use output initialization, which achieves a low error at the beginning (E) but diverges back to pure-SNN error levels the further away we get from the first ANN prediction. Our proposed way to initialize SNN states via the ANN (B) accelerates the convergence of the SNN and thus leads to stable and low error rates in (E). Finally, adding back output initialization further reduces the error rate consistently throughout the interval. Crucially, using output initialization upper bounds the method's error rate at $t=0$ to that of the ANN.

\subsubsection{State Initialization Architecture}\label{subsec:results_stateinit_ablation}

We investigate different blocks for mapping ANN features to initialize membrane potentials (Fig.~\ref{fig:hybrid_arch}, orange blocks) and report the last bin and average bin scores in Tab.~\ref{tab:state_init_ablations}. For the last bin, we report the error achieved after 100 ms of events, and for the average bin score, we average over 10 time steps.

We compare these scores with membrane potentials initialized with zeros reported in the first row. 
In general, we found that putting a batch normalization layer at the end led to the best results. Interestingly, since batch normalization can generate values outside of the range [0,1], it can permanently kill or activate neurons which proved to be beneficial. This can be seen when comparing rows 2 to 4, where adding a range limiting function LeakyReLU or Sigmoid degrades performance. Following Tab.~\ref{tab:state_init_ablations}, we chose Conv + BN + LeakyReLU + Conv + BN.
\subsection{Spike Activity Analysis}
\label{sec:spike_activity}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.93\columnwidth]{imgs/spike_firing_rate_per_timestep.pdf}
    \caption{\textbf{Effect of state initialization on spike firing rates} across time steps. The SNN before state initialization consumes 46 mW of energy, while after state initialization, this decreases to 30 mW.}
    \label{fig:spike_firing_rate}
\end{figure}

This section compares spike activity before and after state initialization of spiking neurons.
Fig.~\ref{fig:spike_firing_rate} shows the average spike firing rate throughout the network for each time step. 
The spike rate gradually increases for the SNN experiment as membrane potentials build up and more spikes are emitted over time.
In contrast, spike firing rates in the hybrid model show high firing rates in the first time step due to the initialized states. 
Overall, there is reduced firing activity following state initialization, leading to a 35\% decrease in power consumption from 46~mW to 30~mW.

\subsection{Comparison with State-of-the-Art}
\label{sec:exp:sota}

\subsubsection{2D Pose Estimation}
\label{subsec:results_2d_hpe}

Here we compare the performance of our approach and its ANN counterpart with previous work on 2D pose estimation.
ANN experiments reported at a specific rate are achieved by inputting events in a sliding window manner.
Tab.~\ref{tab:dhp19_mjpe2d} reports 2D results on the test set for DHP19~\cite{dhp19} and compares MPJPE in pixels on the two camera views used in prior work. 
We compare against Calabrese et al.~\cite{Calabrese19cvprw}, which uses an Hourglass style network to process dense event representations, and Baldwin et al.~\cite{tore_volume}, which reuses the network from~\cite{Calabrese19cvprw} but instead uses specialized TORE volumes as inputs. Both methods use artificial neural networks. 

Tab.~\ref{tab:dhp19_mjpe2d} shows that our pure ANN achieves the best score with 5.03 px compared to all previous methods but consumes the most energy with 3.718 W.
In contrast, the hybrid model with 0.424 W is 8x more energy efficient compared to the pure ANN model, with only a 6\% and 3\% decrease in accuracy for camera views 3 and 2, respectively. 
In comparison with previous work, the hybrid model is the most energy efficient while outperforming previous works. 
A key advantage of using SNNs is that outputs are continuous time, meaning that we may increase the discretization step of the SNN to smaller values, i.e., higher rate outputs, without significantly impacting power consumption. Fig.~\ref{fig:pow_acc_plot} visualizes all methods' accuracy and power consumption at different rates. 
The SNN part of our hybrid model only consumes \%5 of the energy. 
Therefore, increasing the update rate allows higher rate predictions with little to no increase in power consumption while maintaining the same accuracy. 
For approaches relying only on ANNs, power consumption increases linearly with the prediction rate. 
From Fig.~\ref{fig:pow_acc_plot}, we see that our hybrid model at 200 Hz retains good accuracy with minimal change in power consumption.
Fig.~\ref{fig:2d_qualitative_plot} visualizes the tracking performance of our approach at 100 Hz for different movements and test subjects, in comparison to ANN at 10 and 100 Hz. 

\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{imgs/power_acc_plot.pdf}
    \caption{\textbf{2D pose estimation performance vs. power consumption} at 100 and 200 Hz.}
    \label{fig:pow_acc_plot}
\end{figure}


\begin{table}[t!]
\centering
\caption{\textbf{2D pose estimation performance on DHP19} at 100 Hz together with energy consumption results. Best values are highlighted in bold, and second best are underlined.}
\label{tab:dhp19_mjpe2d}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccccr}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{2D MPJPE $\downarrow$}} & \multicolumn{2}{c}{\textbf{\# Ops/s (G)}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Power \\ (W)\end{tabular}}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
               &                  & Cam 2      & Cam 3      &  MAC  & AC &               \\ \midrule
Calabrese~\cite{dhp19}         & ANN              & 7.72          & 7.61    &  255 & 0  & 0.431       \\
Baldwin~\cite{tore_volume} & ANN              & 5.98          & 5.25         & 949 & 0 & 1.605          \\
Ours           & ANN              & \textbf{5.03} & \textbf{4.67} & 2200 & 0 & 3.718            \\
Ours           & SNN              & 21.37 & 19.17 &  0.5 & 121 & \textbf{0.046}            \\ 
Ours           & Hybrid & \underline{5.19}          & \underline{4.97}         & 233 & 79  & \underline{0.424} \\ \bottomrule
\end{tabular}%
}
\end{table}

Next, we evaluate our approach on the Event-Human3.6M~\cite{lifting_monocular_events} dataset. 
We perform two experiments, feeding either RGB images or event representation to the ANN. 
This shows our approach's generality to work with multimodal (events+frames) data. The ANN is deployed at 5 Hz, while the SNN uses a discretization step of 10 ms, resulting in 100 Hz updates.
Tab.~\ref{tab:h36m_mjpe2d} reports our scores and power consumption.
Our hybrid approach with RGB images fed to the ANN reports on par accuracy of 4.66 pixels with previous work while being 26.7 times more energy efficient with 0.21 W power consumption compared to the 5.61 W of previous work.
Due to the model complexity of previous work, our hybrid experiments with only events, the last row of Tab.~\ref{tab:h36m_mjpe2d}, fall short of state-of-the-art but show competitive performance.

\begin{table}[t!]
\centering
\caption{\textbf{2D pose estimation score on Event-Human3.6M} at 100 Hz together with energy consumption results.}
\label{tab:h36m_mjpe2d}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccccr}
\toprule 
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Modality}} & \multirow{2}{*}{\textbf{2D MPJPE $\downarrow$}} & \multicolumn{2}{c}{\textbf{\# Ops/s (G)}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}} Power \\ (W)\end{tabular}}} \\ \cmidrule(lr){5-6}  
&                  &       &     & MAC  & AC &               \\ \midrule
Scarpellini~\cite{lifting_monocular_events} & ANN       & Events       & \underline{4.66} & 3321 & 0 & 5.61             \\
Ours               & ANN       & RGB          & \textbf{4.19} &  2025 & 0 & 3.42              \\
Ours               & ANN       & Events       & 5.09 & 2200 & 0 & 3.72              \\
Ours               & Hybrid & RGB + Events & \underline{4.66} & 108 & 75 & \textbf{0.21}                     \\
Ours               & Hybrid & Events       & 5.76    & 117 & 70 & \underline{0.22}                     \\ \bottomrule

\end{tabular}%
}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{imgs/qualitative_2d_mpjpe.pdf}
    \caption{\textbf{Qualitative 2D pose estimation results} for different test subjects and movements. From left to right, samples from ANN at 10 Hz, 100 Hz, and our hybrid ANN - SNN model at 100 Hz with \textcolor{my_green}{green} markers indicating \textcolor{green}{groundtruth} and \textcolor{my_pink}{pink} markers indicating \textcolor{my_pink}{predictions}. Large circles indicate the end prediction of the sequence.}
    \label{fig:2d_qualitative_plot}
\end{figure}


\begin{table}[ht]
\centering
\caption{\textbf{3D pose estimation scores on DHP19}. MPJPE is reported in millimeters.}
\label{tab:dhp19_mpjpe3d}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccccr}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Triang.}} & \multirow{2}{*}{\textbf{3D MPJPE $\downarrow$}}   & \multicolumn{2}{c}{\textbf{\# Ops/s (G)}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Power \\  (W)\end{tabular}}}\\ \cmidrule(lr){5-6} 
               &                  &       &     &  MAC  & AC &               \\ \midrule
Calabrese~\cite{dhp19}         & ANN       & Geom.    & 87.6     & 255 & 0  & \underline{0.431}    \\
Baldwin~\cite{tore_volume} & ANN       & NN & 58.4  & 984 & 0 & 1.664          \\
Ours           & Hybrid & Geom.    & \underline{57.7} & 233 & 79  & \textbf{0.424}     \\ 
Ours           & Hybrid & NN & \textbf{54.2}  & 268 & 79 & 0.483 \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{3D Pose Estimation}\label{subsec:results_3d_hpe}
For 3D HPE on the DHP19 dataset, we use the 2D detection provided by our method and then use two triangulation methods to generate 3D points:
First, we use geometrical triangulation from the two camera views to compare against Calabrese et al.~\cite{dhp19}.
Second, we use neural network-based (learned) triangulation for a fair comparison with Baldwin et al.~\cite{tore_volume}.
Tab.~\ref{tab:dhp19_mpjpe3d} summarizes the results and shows that our approach yields the best trade-off between performance and energy consumption.
Our hybrid approach with geometrical triangulation requires less power than Calabrese et al.~\cite{dhp19} while substantially reducing the MPJPE by 34\%, from 87.6 mm to 57.7 mm.
When using learned triangulation, our method achieves an MPJPE reduction of 4.2~mm compared to Baldwin et al.~\cite{tore_volume}, previous state-of-the-art, while at the same time consuming 3.4 times less energy.









