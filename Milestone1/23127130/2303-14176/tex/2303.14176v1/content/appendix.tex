\section{Appendix}
\subsection{Overview}
Here, we provide additional information supporting the main manuscript. In what follows we will refer to Figures, Tables, Sections, and Equations from the main manuscript with the prefix ``M-", and use no prefix for new references in the appendix. We start by providing further analysis of our state initialization scheme (Sec.~\ref{sec:app:init}), then provide additional network details in Sec.~\ref{sec:app:net_details}. We, also attach a supplementary video with visualizations of our low-latency human pose estimation network's output.

\subsection{Initialized Membrane Potential Values}
\label{sec:app:init}

\begin{figure}[bht]
    \centering
    \includegraphics[width=0.93\columnwidth]{imgs/MP_violin.pdf}
    \caption{\textbf{Violin plots of membrane potential values after state initialization across layers of the SNN.} Black lines indicate the mean state value at every layer. `Last' indicates the last state initialized layer before the output.}
    \label{fig:spike_firing_rate_appendix}
\end{figure}

In Fig.~\ref{fig:spike_firing_rate_appendix} we show the distribution of initial membrane potentials predicted by our ANN, grouped by the encoder, residual blocks, decoder, and last initialized layer before the output. Note that the firing threshold is 1, meaning that certain neurons are initialized in a firing state. In particular, the output layer shows a high proportion of these kinds of neurons. We call these states that are initialized close to firing, or even in a firing state meta-stable. This meta-stable state is important to reduce latency since it means that few input events can immediately elicit a network response since the membrane potentials are close to firing.
We also see a long tail of inhibited neurons that are initialized with a negative membrane potential.

\subsection{Network Details}
\label{sec:app:net_details}

The CNN and SNN architecture details are given in Tables~\ref{tab:cnn_arch} and~\ref{tab:snn_arch}, respectively.
For each layer, padding is calculated to preserve spatial dimensions.
Both tables are given with respect to the resolution of the DHP19 dataset, 256x256. 
For the Event-Human3.6M dataset, the resolution is 320x256.

Each convolutional layer in the CNN is followed by a leaky ReLU layer with a negative slope of 0.1.
Columns 1-6, are the encoder layers where an average pooling layer is followed by two convolutions. 
Columns 7-11 are decoder layers, and operations are as follows: (i) interpolation, (ii) convolutional layer, (iii) concatenation with skip connections of the same resolution, and (iv) convolution. 
Finally, the last column is a simple prediction layer with no activation function. 

Each convolutional layer in the SNN is followed by a batch norm, and leaky integrate \& fire neuron layer.
The first column is the spike encoder, columns 2-4 are encoder, 5-6 are residual, and 7-9 are decoder layers. 
Decoder blocks perform concatenation with skip connections at the same spatial resolution and are upsampled together. 
Finally, the last layer is a single convolutional layer.

\FloatBarrier
\begin{table}[h]
\centering
\caption{\textbf{CNN architecture details.} Changes in spatial resolution are due to 2x2 average pooling or bilinear interpolation by a scale of 2. The input channel is of size 20 for event representations or 3 for RGB images.}
\label{tab:cnn_arch}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cccccccccccc}
\toprule
\textbf{Layer} &
  \textbf{1} &
  \textbf{2} &
  \textbf{3} &
  \textbf{4} &
  \textbf{5} &
  \textbf{6} &
  \textbf{7} &
  \textbf{8} &
  \textbf{9} &
  \textbf{10} &
  \textbf{11} &
  \textbf{12} \\ \midrule
\textbf{Kernel size}    & 7   & 5   & 3   & 3   & 3   & 3   & 3   & 3   & 3   & 3   & 3   & 3   \\
\textbf{Output channel} & 32  & 64  & 128 & 256 & 512 & 512 & 512 & 256 & 128 & 64  & 32  & 13  \\
\textbf{Output H, W}    & 256 & 128 & 64  & 32  & 16  & 8   & 16  & 32  & 64  & 128 & 256 & 256 \\ \bottomrule
\end{tabular}}%
\end{table}
\FloatBarrier

\begin{table}[h]
\centering
\caption{\textbf{SNN architecture details.} Changes in spatial resolution are due to convolutions with stride 2 and bilinear interpolation of scale 2. The input channel is of size 2.}
\label{tab:snn_arch}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cccccccccc}
\toprule
\textbf{Layer} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\ \midrule
\textbf{Kernel size}    & 5   & 5   & 5   & 5   & 3   & 3   & 5   & 5   & 5   & 1 \\
\textbf{Stride}         & 1   & 2   & 2   & 2   & 1   & 1   & 1   & 1   & 1   & 1                      \\
\textbf{Output channel} & 32  & 64  & 128 & 256 & 256 & 256 & 128 & 64  & 32  & 13                     \\
\textbf{Output H, W}    & 256 & 128 & 64  & 32  & 32  & 32  & 64  & 128 & 256 & 256                    \\ \bottomrule
\end{tabular}%
}
\end{table}