\section{Related work}\label{sec:related} 
\subsection{Hybrid ANN-SNN Architectures}\label{subsec:related_hybrid}

In recent years, there has been a growing interest in exploring the potential benefits of combining Artificial Neural Networks (ANNs) and Spiking Neural Networks (SNNs)~\cite{hybrid_dashnet,spikeflownet,hybrid_obj_detection,hybrid_top_down,hybrid_units}. 
Different combination strategies have been explored for a variety of tasks.

A group of work employs the strategy of processing the accumulated spike train of SNNs with ANNs \cite{hybrid_obj_detection, hybrid_top_down, spikeflownet}.
In these works, the SNN is used as an efficient encoder of spatio-temporal event data from an event camera.
The output of the SNN is accumulated to summarize the temporal dimension before the ANN processes the accumulated features \cite{hybrid_obj_detection, spikeflownet}.
Liu et al. \cite{hybrid_top_down} extend this idea for object classification and use a feedback loop from the ANN to the input of the SNN.
The downside of the aforementioned approaches is that they need to execute a full forward pass of the ANN to extract results, which results in high power consumption and high computational latency. In contrast, our SNN directly updates the output of the ANN such that we do not need to execute the ANN for every iteration, thus retaining the low-power property.

A second line of work uses a strategy where the output of the independently operating SNN and ANN is fused \cite{lele2022bio, hybrid_dashnet, hybrid_units}.
This approach is especially suitable for multimodal processing of frame-based video and event data from event cameras.
The ANN is tasked with extracting features from the frames while the SNN processes events directly.
Finally, the output of both networks are fused based on heuristics \cite{lele2022bio}, temporal filtering \cite{hybrid_dashnet}, or accumulation based on the output of the ANN \cite{hybrid_units}.
However, these methods do not address the convergence of SNN's and also do not share features between networks, making their fusion shallow. By contrast, our approach not only reuses the output of the ANN \cite{hybrid_units} but also reuses the features from the ANN to initialize the SNN states. The initialization of SNN states drastically improves the performance and convergence of the SNN.

\subsection{Human Pose Estimation}\label{subsec:related_hpe}

\paragraph{Frame-based} Human pose estimation (HPE) is the task of estimating the 2D or 3D locations of body joints from a single image or video.
Current techniques for 3D HPE involve reconstructing the 3D pose from either single~\cite{chen2017_3d_singleframe, mehta2018single, pavlakos2017coarse, tome2017lifting, zhou2017towards, monocular_hpe_survey} or multiple~\cite{kocabas2019self, hofmann2012multi,elhayek2015efficient,rhodin2016general} camera views.
To estimate the 3D pose from multiple views, the traditional approach involves predicting the 2D pose in each view and using the camera characteristics and positions to triangulate it into the world coordinate~\cite{hpe_triangulation}. 
Alternatively, newer approaches include triangulation with neural networks~\cite{iskakov2019learnable, liu2020attention, liu2021enhanced, katircioglu2018learning} or direct regression of the 3D pose~\cite{yao2019densebody, rhodin2016general}.
Both single-view and multi-view approaches can be improved by using multiple frames to extract temporal information that can help disambiguate joint locations over time and reduce jitters~\cite{mehta2017vnect, hofmann2012multi, elhayek2015efficient,rhodin2016general,katircioglu2018learning}.
\vspace*{-0.3cm}
\paragraph{Event-based} Recently, human pose estimation with event cameras has gained traction due to their inherent ability to filter out temporally redundant information like the background~\cite {eventcap, eventhpe, dhp19, tore_volume, lifting_monocular_events}. These works adopt one of two main approaches.
The first direction of work utilizes volumetric human body models \cite{smpl} to estimate both the 3D pose and shape of the human body \cite{eventcap, eventhpe}.
EventCap~\cite{eventcap} and EventHPE~\cite{eventhpe} use a low-dimensional human shape representation called SMPL~\cite{smpl} to enable end-to-end shape and pose estimation from images and events.
The second approach, in contrast, focuses on extracting the pose information using a skeleton body model \cite{lifting_monocular_events, dhp19, tore_volume}. 
Scarpellini et al.~\cite{lifting_monocular_events} use events from a single camera view and predict the 3D pose with a Convolutional Neural Network (CNN).
Calabrese et al.~\cite{dhp19} and Baldwin et al.~\cite{tore_volume} estimate 2D joint locations using a CNN architecture and perform triangulation to obtain the 3D pose.
Different from previous work, we target low-power inference for human pose estimation using a hybrid ANN-SNN architecture.
Our approach benefits from the high accuracy of ANNs and low power consumption of SNNs to improve the accuracy to power tradeoff.
