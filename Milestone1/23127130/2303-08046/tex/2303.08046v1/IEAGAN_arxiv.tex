\documentclass{article} 

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[colorlinks=true,linkcolor=blue, allcolors=blue, backref=page]{hyperref}
%\usepackage{subfig}

%\usepackage{layouts}
\usepackage{graphicx} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{caption}
\usepackage[toc,page]{appendix}
\usepackage[compact]{titlesec}

% \usepackage{unicode-math}
\usepackage{cleveref}
\usepackage{siunitx}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{doi}
%\usepackage[colorlinks=true,linkcolor=blue, allcolors=blue, backref=page]{hyperref}
%\usepackage[backref=page]{hyperref}
%\usepackage{stackengine}
%\usepackage[table]{xcolor}
%\usepackage{calrsfs}
\titlespacing{\section}{0pt}{*0}{*0}
\setlength{\paperheight}{11in}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\La}{\mathcal{L}}
\newcommand{\Lb}{\pazocal{L}}
\newcommand{\la}{\mathcal{l}}
\newcommand{\lb}{\pazocal{l}}

\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

%\theoremstyle{remark}
%\newtheorem*{remark}{Remark}

%\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
%\DeclareFontShape{U}{mathx}{m}{n}{%
%<-6> mathx5
%<6-7> mathx6
%<7-8> mathx7
%<8-9> mathx8
%<9-10> mathx9
%<10-12> mathx10
%<12-> mathx12
%}{}

%\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
%\DeclareFontSubstitution{U}{mathx}{m}{n}

%\DeclareMathSymbol{\bigovoid}{\mathop}{mathx}{"EC}

\newcommand{\BigO}{\mathop{\stackinset{c}{}{c}{}{ \scalebox{1.1}{$\bigovoid$}}{ \scalebox{1.15}{$\bigovoid$}}}}

\newcommand{\bigO}{\mathop{\stackinset{c}{-2pt}{c}{}{ \scalebox{0.8}{$\bigovoid$}}{\scalebox{0.85}{$\bigovoid$}}}}

\DeclareMathOperator{\Tr}{Tr}



\title{Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and Self-Supervised Relational Reasoning}


\author{ \href{https://orcid.org/0000-0003-4095-9657}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Baran Hashemi}\\
	Faculty of Physics\\
	Ludwig Maximilians University of Munich,\\
	Munich, Germany \\
	\texttt{gh.hashemi@physik.uni-muenchen.de} \\
	%% examples of more authors
	 \AND
	 Nikolai Hartmann \\
	 Faculty of Physics\\
	 Ludwig Maximilians University of Munich,\\
	 Munich, Germany \\
	 \texttt{nikolai.hartmann@physik.uni-muenchen.de} \\
     \And
	 Sahand Sharifzadeh \\
	 Faculty of Computer Science\\
	 Ludwig Maximilians University of Munich,\\
	 Munich, Germany \\
	 \texttt{sharifzadeh@dbs.ifi.lmu.de} \\
     \And
	 James Kahn\\
	 Helmholtz AI\\
	 Steinbuch Centre for Computing~(SCC),\\
	 Munich, Germany \\
	 \texttt{james.kahn@kit.edu} \\
	 \And
	 Thomas Kuhr \\
	 Faculty of Physics\\
	 Ludwig Maximilians University of Munich,\\
	 Munich, Germany \\
	 \texttt{thomas.Kuhr@lmu.de} \\
}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{Under Review}
%%\unnumbered% uncomment this for unnumbered level heads

\hypersetup{
pdftitle={High-Resolution Detector Simulation with Intra-Event Aware Generative Adversarial Network and Self-Supervised Relational Reasoning},
pdfsubject={hep-ph, cs.LG, hep-ex, Data Analysis, Statistics and Probability},
pdfauthor={Baran.~Hashemi},
pdfkeywords={High-resolution and high-granularity detector simulation, Fine-grained conditional image generation, Relational Inductive Bias, GANs, Self-Supervised Learning with Knowledge Transfer},
}

\begin{document}
\maketitle

\begin{abstract}
Simulating high-resolution detector responses is a storage-costly and computationally intensive process that has long been challenging in particle physics. Despite the ability of deep generative models to make this process more cost-efficient, ultra-high-resolution detector simulation still proves to be difficult as it contains correlated and fine-grained mutual information within an event. To overcome these limitations, we propose Intra-Event Aware GAN~(IEA-GAN), a novel fusion of Self-Supervised Learning and Generative Adversarial Networks. IEA-GAN presents a Relational Reasoning Module that approximates the concept of an ''event'' in detector simulation, allowing for the generation of correlated layer-dependent contextualized images for high-resolution detector responses with a proper relational inductive bias. IEA-GAN also introduces a new intra-event aware loss and a Uniformity loss, resulting in significant enhancements to image fidelity and diversity. We demonstrate IEA-GAN's application in generating sensor-dependent images for the high-granularity Pixel Vertex Detector~(PXD), with more than 7.5M information channels and a non-trivial geometry, at the Belle II Experiment. Applications of this work include controllable simulation-based inference and event generation, high-granularity detector simulation such as at the HL-LHC~(High Luminosity LHC), and fine-grained density estimation and sampling. To the best of our knowledge, IEA-GAN is the first algorithm for faithful ultra-high-resolution detector simulation with event-based reasoning.
\end{abstract}

\keywords{High-resolution and high-granularity detector simulation \and Fine-grained conditional image generation \and Relational Inductive Bias \and GANs \and Self-Supervised Learning with Knowledge Transfer}

\clearpage
\section{Introduction}
\label{sec:intro}

The ``Fast and Efficient Simulation''~\cite{de2017learning, paganini2018accelerating,paganini2018calogan, vallecorsaGenerativeModelsFast2018, de2018controlling, deja2020generative,musella2018fast, chekalina2019generative, di2019dijetgan,erdmann2018generating, erdmann2019precise, butter2019gan} campaign in particle physics sparked the search for faster and more storage-efficient simulation methods of collider physics experiments.
Simulations play a vital role in various downstream tasks, including optimizing detector geometries, designing physics analyses, and searching for new phenomena beyond the Standard Model~(SM).
Efficient detector simulation has been revolutionized by the introduction of the Generative Adversarial Network~(GAN)~\cite{goodfellowGenerativeAdversarialNetworks2014} for image data.

GANs have been widely used in particle physics to achieve detector simulation for the LHC~\cite{paganiniCaloGANSimulating3D2017,oliveiraControllingPhysicalAttributes2018,khattakFastSimulationHigh2022}, mainly targeting calorimeter simulation or collision event generation~\cite{alanaziSurveyMachineLearningbased2021,butterHowGANLHC2019,ottenEventGenerationStatistical2021}.
Previous work on generating high spatial resolution detector responses includes the Prior-Embedding GAN~(PE-GAN) by Hashemi. et al.~\cite{hashemiPixelDetectorBackground2021}, which utilizes an end-to-end embedding of global prior information about the detector sensors, and the work by Srebre et al.~\cite{srebreGenerationBelleII2020} in which a Wasserstein GAN~\cite{arjovsky2017wasserstein} with gradient penalty~\cite{gulrajaniImprovedTrainingWasserstein2017} was used as a proof of concept to generate high resolution images without conditioning. For mid-granularity calorimeter simulation, the recent approaches~\cite{buhmann2021getting,krause2021caloflow}, experiment with several GAN-like and normalizing flow~\cite{rezende2015variational} architectures with less that 30k simulated channels, and 3DGAN~\cite{khattakFastSimulationHigh2022} for high granularity calorimeter simulation with only 65k pixel channels.

The task of learning to generate ultra-high resolution detector responses has several challenges. First, in general we are dealing with spatially asymmetric high-frequency hitmaps. With current state of the art~(SOTA) GAN setups for high-resolution image generation candidates, when the discriminator becomes much stronger than the generator, the fake images are easily separated from real ones, thus reaching a point where gradients from the discriminator vanish. This happens more frequently with asymmetric high-resolution images due to the difficulty of generating imbalanced high-frequency details. On the other hand, a less powerful discriminator results in a mode collapse, where the generator greedily optimizes its loss function producing only a few modes to deceive the discriminator.

Furthermore, the detector responses in an event, a single readout window after the collision of particles, share both statistical and semantic similarities~\cite{deselaers2011visual} with each other.
For example, the sparsity~(occupancy) of each image within a class, defined as the fraction of pixels with a non-zero value, shows statistical similarities between detector components as shown in~\Cref{fig:PXD_num_hits}.
Moreover, as the detector response images show extreme resemblance at the semantic and visual levels~\cite{deselaers2011visual}, they can be classified as fine-grained images.
When generating fine-grained images, the objective is to create visual objects from subordinate categories.
A similar scenario in computer vision is generating images of different dog breeds or car models.
The small inter-class and considerable intra-class variation inherent to fine-grained image analysis make it a challenging problem~\cite{weiFineGrainedImageAnalysis2021}.
The current state-of-the-art conditional GAN models focus on class and intra-class level image similarity, in which intra-image~\cite{zhangSelfAttentionGenerativeAdversarial2019b}, data-to-class~\cite{miyatoCGANsProjectionDiscriminator2018}, and data-to-data~\cite{kangContraGANContrastiveLearning2020} relations are considered.
However, in the case of detector simulation, classes become hierarchical and fine-grained, and the discrimination between generated classes that are semantically and visually similar becomes harder.
Therefore, the aforementioned models show extensive class confusion~\cite{kangRebootingACGANAuxiliary2021,rangwani2021class} at the inter-class level.
In addition, since the information in an event comes from a single readout window of the detector, the processes happening in this window affect all sensors simultaneously, leading to a correlation among them, as shown in~\Cref{fig:spearman_corr}.

To overcome all these challenges with high-resolution detector simulation,
we introduce the Intra-Event Aware GAN~(IEA-GAN), a new model to generate sensor/layer-dependent detector response images with the highest fidelity while satisfying all relevant metrics. Since we are dealing with a fine-grained and contextualized~(by each event) set of images that share information, first, we introduce a Relational Reasoning Module~(RRM) for the discriminator and generator to capture inter-class relations.
Then, we propose a novel loss function for the generator to imitate the discriminator's knowledge of dyadic class-to-class correspondence~\cite{caoCouplingLearningComplex2015}.
Finally, we introduce an auxiliary loss function for the discriminator to leverage its reasoning codomain by imposing a information uniformity condition~\cite{wangUnderstandingContrastiveRepresentation2020} to alleviate mode-collapse issue and increase the generalization of the model. IEA-GAN captures not only statistical-level and semantic-level information but also a correlation among samples in a fine-grained image generation task.

We demonstrate the IEA-GAN's application on the ultra-high dimensional data of the Pixel Vertex Detector~(PXD)~\cite{muellerAspectsPixelVertex2014} at Belle~II~\cite{abe2010belle} with more than 7.5M pixel channels- the highest spatial resolution detector simulation dataset ever analyzed with deep generative models. Then, we investigate several evaluation metrics and show that in all of them IEA-GAN is in much better agreement with the target distribution than other SOTA deep generative models for high-dimensional image generation. We also perform an ablation study and exploration of hyperparameters to provide insight to the model. In the end, by applying IEA-GAN to the PXD at Belle II we are able to reduce the storage demand for pre-produced background data by a factor of \num{2}.


\section{Results}
\label{sec:results}
\subsection{IEA-GAN Architecture}
IEA-GAN is a deep generative model based on a self-supervised relational grounding. A GAN is an unsupervised deep learning architecture that involves two networks, the Generator and the Discriminator, whose goal is to find a Nash equilibrium~\cite{nashNonCooperativeGames1951} in a two-player min-max problem. 

IEA-GAN's discriminator, $D$, takes the set of detector response images $x_i \in \mathbf{R}^d$ coming from one event and embeds them as input nodes within a fully connected event graph in a self-supervised way. It approximates the concept of an event by contextual reasoning using the a permutation equivariant Relational Reasoning Module~(RRM). RRM is a novel, GAN-compatible, fully connected, multi-head Graph Transformer Network~\cite{battaglia2018relational,sharifzadehClassificationAttentionScene2021,locatello2020object} that groups the image tokens in an event based on their contextual similarity. For multi-modal contrastive reasoning, the discriminator also takes the sensor embedding of the detector as class tokens~(see~\cref{sec:methods}). In the end, it compactifies both image and class modalities information by projecting the normalized graph onto a hypersphere, as shown in \Cref{fig:IEA_GAN}. 

To ensure that the Generator $G$ has a proper understanding of an event and captures the intra-event correlation, it first samples from a Normal distribution, $\mathcal{N}(0,1)$, at each event as random degrees of freedom (Rdof), and decorates the sensor embeddings with this four-dimensional learnable Rdof~(see~\cref{sec:methods}). Then, for a self-supervised contextual embedding of each event, the RRM acts on top of this. Notably, Rdof differs from the original GAN~\cite{goodfellowGenerativeAdversarialNetworks2014} Gaussian latent vectors. Rdof can be considered a event-level learnable segment embedding~\cite{devlin2018bert} and perturbation~\cite{zhangWordEmbeddingPerturbation2018} to the token embeddings, which can leverage the diversity of generated images. Combining these modules with the IEA Loss allows the Generator to gain insight and establish correlation among the samples in an event, thus improving its overall performance.

Apart from the adversarial loss, IEA-GAN also benefits from self-supervised and contrastive-based set of losses. The model understands the geometry of detector through a proxy-based contrastive 2C loss~\cite{kangContraGANContrastiveLearning2020} where the learnable proxies are the sensor embeddings over the hypersphere. Moreover, to improve diversity and stability of the training, we introduce a Uniformity loss for the discriminator. The uniformity loss can encourage the discriminator to give equal weight to all regions of the hypersphere~\cite{wangUnderstandingContrastiveRepresentation2020}, rather than just focusing on the areas where it can easily distinguish between real and fake data. Encouraging the discriminator to impose uniformity not only promotes more diverse and varied outputs, but also mitigates issues such as mode collapse.

Another essential part of IEA-GAN is the IEA loss that addresses the class-confusion~\cite{kangRebootingACGANAuxiliary2021,rangwani2021class} problem of the conditional generative models for fine-grained datasets. In the IEA-loss the generator tries to imitate the discriminator's understanding of each event through a dyadic information transfer with a stop-gradient for the discriminator. This can improve the ability of the generator to generate more fine-grained samples in the simulation process by being aware of the variability of conditions at each event.


\begin{figure}[!htp]
    \centering
    % \includegraphics[width=\textwidth,clip]{plots/IEAGAN.pdf}
    \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/IEAGAN_main.pdf}
    \caption{Rdof stands for Random degrees of freedom, which decorates the generator's sensor/layer embedding with an event-level learnable embedding responsible for the generator's intra-event correlation. The Relational Reasoning Modules~(RRM) in the generator and the discriminator do the intra-event reasoning by clustering class/image embeddings based on their contextual similarity, respectively.
    The red lines correspond to the forward and backward passes of the generator. The black lines correspond to the forward and backward passes of the discriminator. The discriminator is trained with the Adversarial loss, see~\cref{eq:hinge_loss}, 2C loss, see~\cref{eq:2C_loss} and the Uniformity loss, see~\cref{eq:uniformity_loss}. On the other hand, the generator uses the Adversarial loss, 2C loss, and the IEA loss, see~\cref{eq:iea_loss}. Sg means stop-gradient for the discriminator from the IEA loss, a self-supervised dyadic-aware loss for the generator.}
    \label{fig:IEA_GAN}
    \end{subfigure}
    \hfill%
    \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{plots/RRM.pdf}
    \caption{The Relational Reasoning Module for the generator~(left) and the discriminator~(right)}
    \label{fig:RRM_components}
    \end{subfigure}
    \caption{IEA-GAN architecture~(\textbf{a}) and Relational Reasoning Module components~(\textbf{b}). SN stands for spectrally normalized layers that only the discriminator incorporates.}
    \label{fig:IEAGAN_arch}
\end{figure}

\subsection{IEA-GAN Evaluation}
Our study showcases the performance of IEA-GAN in generating high-resolution detector responses, demonstrated through its successful application to the ultra-high dimensional data of the Pixel Vertex Detector~(PXD) at Belle~II, consisting of over 7.5 million pixel channels. Furthermore, for the first time, our findings reveal that the FID~\cite{heuselGANsTrainedTwo2017} metric for detector simulation is a very versatile and accurate unbiased estimator in comparison to marginal distributions, and is correlated with both high and low level metrics. We show that by using IEA-GAN, we are able to capture the underlying distributions such that we can generate and amplify detector response information with a very good agreement with the Geant4 distributions. We also find out that the SOTA models in high resolution image generation even with an in-depth hyperparameter tuning analysis do not perform well in comparison.

For evaluation, we have two categories of metrics: image level and physics level. As we are interested in having the best pixel-level properties, diversity, and correlation of the generated images simultaneously while adhering to minimal generator complexity due to computational limitations, choosing the best iteration to compare results is challenging.
Hence, we choose models' weights with the best FID for all comparisons.
Based on the recent Clean-FID project~\cite{parmarAliasedResizingSurprising2022}, we fine-tune the Inception-V3~\cite{szegedyRethinkingInceptionArchitecture2016} model on the PXD images, as the PXD images are very different from the natural images used in their initial training. This process can be done for any other detector dataset. FID measures the similarity of the generated samples’ representations to those of samples from the real distribution. Given a large sampling statistics, for each hidden activation of the Inception model, the FID fits a Gaussian distribution and then evaluates the Fréchet distance, also known as Wasserstein-2 distance, between those Gaussians. The lower the FID score, the more similar the distributions of the real and generated samples are.

We compare IEA-GAN with three other models and the reference, which is the Geant4-simulated~\cite{agostinelli2003geant4} dataset. 
The baselines are the SOTA in conditional image generation: BigGAN-deep~\cite{brock2018large} and ContraGAN~\cite{kangContraGANContrastiveLearning2020}.
We also compare IEA-GAN with the previous works on PXD image generation task: PE-GAN~\cite{hashemiPixelDetectorBackground2021} and WGAN-gp~\cite{srebreGenerationBelleII2020}\footnote{Only for FID.}. 

\Cref{tab:fid} demonstrates that generated images by IEA-GAN have the lowest FID score compared to the other models and outperforms them by a significant margin. This indicates that our model is able to generate synthetic samples that are much closer to the target data than the samples generated by the other models.

\begin{table}[ht]
\begin{minipage}{\textwidth}
    \begin{center}
    \caption{
    FID comparison between models (all models in the benchmark are highly tuned to the current problem and dataset), averaged across six random seeds.
    The lower the FID the better the image quality and diversity.
    }
    \label{tab:fid}
    \begin{tabular}{@{}l|lllll@{}}
        \toprule
        & WGAN-gp & BigGAN-deep & ContraGAN & PE-GAN & IEA-GAN \\ 
        \midrule
        \textbf{FID}  & $12.09$ & $4.40\pm 0.88$ & $3.14\pm 0.74$ & $2.61\pm 0.91$ & $\mathbf{1.50\pm 0.16}$ \\
        \bottomrule
    \end{tabular}
    \end{center}
\end{minipage}
\end{table}

At the pixel level, there are the pixel intensity distribution, occupancy distribution, and mean occupancy.
The pixel intensity distribution defines the distribution of the energy of the background hits. 
The occupancy distribution and the pixel intensity distribution are evaluated over all sensors of a given number of events, while the mean occupancy corresponds to the mean value of sparsity across events for each sensor.
This pixel-level information is essential since upon physics analysis via the basf2 software~\cite{braunBelleIICore2018}, when one wants to use the images and overlay the extracted information on the signal hits, the sparsity of the image defines the volume of the background hits on each sensor.
The pixel intensity distribution, the occupancy distribution, as well as the mean occupancy per sensor are shown in \Cref{fig:occupancy-intensity}. The distributions for the IEA-GAN model show the closest agreement with the reference. 

The bimodal distribution of the occupancy comes from the geometry of the detector as the sensors are not in a cylindrical shape like a calorimeter, but in an annulus shape. This indicates how challenging simulating this dataset is concerning both it's geometry and resolution. In order to capture the correct bi-modality of the occupancy distribution, RRM and the Uniformity loss play an important role. By using a uniformity loss in the discriminator, the generator is incentivized to produce samples that are not biased towards a particular mode or class, leading to a wider bimodal distribution of generated samples. 

Moreover, by utilizing the RRM module that considers the inter-dependencies and correlations among the samples within an event, the IEA-GAN exhibits a superior consistency with high-energy hits, which enhances the diversity of generated samples in regions with lower occurrence rates.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{plots/pixel_intensity_lin_new.pdf}%
    \hfill%
    \includegraphics[width=0.49\textwidth]{plots/pixel_intensity_log_new.pdf}
    \includegraphics[width=0.49\textwidth]{plots/occupancy_distribution_new.pdf}
    \hfill%
    \includegraphics[width=0.49\textwidth]{plots/mean_occupancy_new.pdf}
    \caption{Pixel intensity distribution in linear~(top left) and logarithmic scale~(top right), the distribution of the occupancy~(bottom left) and the mean occupancy per sensor~(bottom right) for \num{10000} events.}
    \label{fig:occupancy-intensity}
\end{figure}

Along with all these image-level metrics, we also need an intra-event sensitive metric.
All the above metrics are equivariant under permutation between the samples among events. In other words, if we randomly shuffle the samples between events while fixing the sensor number, all the discussed metrics are unchanged. Hence, we need a metric that looks at the context of each event individually in its event space and goes beyond the sample space.
Ergo, we compute the Spearman's correlation between the occupancy of the sensors along the population of generated events,

\begin{equation*}
r_s = \mathbf{Corr_p}(R(\biguplus_{i=1}^{M=40}(\|x_i\|_0)),R(\biguplus_{i=1}^{M=40}(\|x_i\|_0))),
\label{eq:spearman_corr}
\end{equation*}

where $R(.)$ is the rank variable, and $\mathbf{Corr_p}(.)$ is the Pearson Correlation function.
The coefficients by PE-GAN are random values in the range $[-0.2,0.2]$, whereas IEA-GAN images show a meaningful correlation among their generated images.
Even though the desired correlation is different from the reference, as shown in \Cref{fig:spearman_corr}, IEA-GAN understands a monotonically positive correlation for intra-layer sensors and a primarily negative correlation for inter-layer sensors. 

\begin{figure}[ht]
    \centering
    %\includegraphics[width=0.33\textwidth]{plots/corr_real}%
    %\includegraphics[width=0.33\textwidth]{plots/corr261z116}%
    %\includegraphics[width=0.33\textwidth]{plots/corr270z90}
    \includegraphics[width=\textwidth]{plots/corr_all}
    \caption{Spearman's correlation between the occupancy of Geant4 sensor images~(left), and sensor images from IEA-GAN~(center), sensor images from PE-GAN~(right).}
    \label{fig:spearman_corr}
\end{figure}

While image level metrics indicate the low-level quality of simulations, we must also confirm that the resulting simulations are reasonable physics-wise when the entire detector is considered as a whole.
For this, we examine the Helix Parameter Resolution~(HPR).
At the Belle~II experiment, after each collision event, tracks propagating in vacuum in a uniform magnetic ﬁeld move roughly along a helix path described by the ﬁve helix parameters $\{d_0, z_0, \phi_0,\omega, \tan\lambda\}$ with respect to a pivot point~\cite{kouBelleIIPhysics2020}.
The difference between the true and reconstructed helix parameters defines the resolution for the corresponding helix parameter.
The track parameter resolution is affected by the number of hits and the hit intensity.
The resolutions of all five helix parameters are shown in \Cref{fig:helix_param_res} for \num{5000} events. 
We observe good agreement between the IEA-GAN and Geant4, particularly in the tail segments where the largest difference between Geant4 and no background is found. 
Hence, not only does IEA-GAN demonstrate a close image level agreement with Geant4, it maintains a reconstructed physical behavior during track reconstruction as well.
%in two transverse momentum cuts, above \SI{0.2}{\giga\electronvolt} for high momentum background hits and below \SI{0.2}{\giga\electronvolt} for background hits with low momentum.
% As expected, the effect of the background is more visible over the tails of the resolution distribution.
% The other HPRs are shown in the appendix in \Cref{fig:hpr_phi0_omega_tlmd}.
% As a result, IEA-GAN generated PXD background shows a close resemblance to the Geant4 simulated background at the physics level as well.

\begin{figure}[ht]
\centering
\includegraphics[width=0.33\textwidth]{plots/physics_figs/d0_plot.pdf}%
\includegraphics[width=0.33\textwidth]{plots/physics_figs/z0_plot.pdf}%
\includegraphics[width=0.33\textwidth]{plots/physics_figs/phi0_plot.pdf}
\includegraphics[width=0.33\textwidth]{plots/physics_figs/omega_plot.pdf}%
\includegraphics[width=0.33\textwidth]{plots/physics_figs/tlmd_plot.pdf}
\caption{
Helix parameter resolutions for $d_0$~(top left), $z_0$~(top middle), $\phi_0$~(top right), $\omega_0$~(bottom left), and $\tan \lambda$~(bottom right).
Blue corresponds to the IEA-GAN simulated background, red to Geant4, and green to resolution with no background overlay.
}
\label{fig:helix_param_res}
\end{figure}

\section{Discussion}

In this work, we have proposed a series of robust methods for ultra-high-resolution, fine-grained, correlated detector response generation and conditional sampling tasks with our Intra-Event Aware GAN (IEA-GAN). IEA-GAN not only captures the dyadic class-to-class relations but also exhibits explainable intra-event correlations among the generated detector images while other models fail to capture any correlation. To achieve this, we present novel components, the Relational Reasoning Module~(RRM) and the IEA-loss, with the Uniformity loss~\cite{wangUnderstandingContrastiveRepresentation2020}, used in Deep Metric Learning. 
The RRM introduced a self-supervised relational contextual embedding for the samples in an event, which is compatible with GAN training policies, a task that is very challenging. It dynamically clusters the images in a collider event based on their inherent correlation culminating in approximating a collision event.
Our IEA-loss, a discriminator-supervised loss, helps the generator reach a consensus over the discriminator's dyadic relations between samples in each event.
Finally, we have demonstrated that the Uniformity loss plays a crucial role for the discriminator in maximizing the homogeneity of the information entropy over the embedding space, thus helping the model to overcome mode-collapse and to capture a better bi-modality of generated occupancy.

As a result, an improvement to all metrics compared to the previous SOTA occurs, achieving an FID score of $1.50$, an over $42\%$ improvement, and a storage release of more than $2$ orders of magnitude. Moreover, we have showed that the application of the FID metric, an unbiased estimator, for the detector simulation provides a powerful tool for evaluating the performance of deep generative models in detector simulation.
We have also conducted an in-depth study into the optimal design and hyperparameters of the RRM, the IEA-loss, and the Uniformity loss.

\begin{table}[ht]
% \begin{center}
\begin{minipage}{\textwidth}
    \centering
    \caption{
    Computational performance of IEA-GAN and PE-GAN generators on a
    single core of an Intel Xeon Silver~\num{4108}~\num{1.80}GHz~(CPU) and NVIDIA V100 with~\num{32} GB of memory~(GPU) compared to GEANT4. For the generative models, the mean and standard deviation were obtained for sets of~\num{10000} events. The time for GEANT4 refers to the theoretical time it would take to run the simulation of all background processes on-the-fly. The storage consumption corresponds only to \num{1} times the PXD background simulated information where the amplification coefficient $N$ is equal to \num{1}.}
    \label{tab:computational_perf}
    \begin{tabular}{llll}
        \toprule
        Hardware & Simulator & time/event~[s] & storage~[Mb]\\ 
        \midrule
        \multirow{3}{*}{CPU}  & Geant4 & $\approx 1500$ & $\approx 2000$\\
                              & PE-GAN & $11.781\pm 0.357$ & $\approx 47$\\
                              & IEA-GAN & $10.159\pm 0.208$ & $\approx 47$\\
        \midrule
        \multirow{2}{*}{GPU}  & PE-GAN & $0.090\pm 0.010$ & $\approx 47$\\
                              & IEA-GAN & $0.070\pm 0.006$ & $\approx 47$\\
        \bottomrule
    \end{tabular}
\end{minipage}
% \end{center}
\end{table}

The ability to capture the underlying correlation structure of the data in particle physics experiments where the physical interpretation of the results heavily relies on it, is very important. The true correlation between the occupancy of the sensors is determined by the underlying physical processes within the simulation. Although the true correlation differs from the one captured by IEA-GAN, the model might be learning biases or artifacts introduced by the training data or the discriminator. Therefore, while the IEA-GAN can provide valuable insights into the correlations and patterns present in the data, it is important to interpret its results in conjunction with the domain knowledge. To alleviate the discrepancy, we expect that incorporating perturbations directly to the discriminator's RRM module would improve its contextual understanding and thus the intra-event correlation. For example, using random masking~\cite{liu2019roberta} or inter-event permutation~\cite{yang2019xlnet} over the samples and asking the RRM module to predict the representation of perturbed sample will improve the robustness of the model.

This work has a significant impact on high-resolution fast and efficient detector response and collider event simulations. Since, they require fine-grained intra-event-correlated data generation, we believe that the Intra-Event Aware GAN~(IEA-GAN) offers a robust controllable sampling for all particle physics experiments and simulations, such as detector simulation~\cite{khattakFastSimulationHigh2022,haririGraphGenerativeModels2021} and event generation~\cite{butterHowGANLHC2019,verheyenEventGenerationDensity2022,alanazi2021survey,butter2022machine} at both Belle~II~\cite{abe2010belle} and LHC~\cite{evansLHCMachine2008a}. In particular, the High-Luminosity Large Hadron Collider~(HL-LHC)~\cite{apollinari2017high} is expected to surpass the LHC's design integrated luminosity by increasing it by a factor of 10. As a result, more effective and efficient high-resolution detector simulations are required. IEA-GAN is the first potential candidate for simulating the corresponding high-resolution and high-granular detector signatures with the remarkable capability of generating more than 7.5M pixel channels.

Finally, IEA-GAN also has potential applications in the protein design which is a process that involves the generation of novel amino acid sequences to produce proteins with desired functions, such as enhanced stability and foldability, new binding specificity, or enzymatic activity~\cite{huang2016coming}. Proteins can be grouped into different categories based on the arrangement and connectivity of their secondary structure features, such as alpha helices and beta sheets. Our developed intra-event aware methods, where an event represents the higher category of features, can also be applied to fine-grained density estimation~\cite{liu2021density} for generating novel foldable protein~\cite{repecka2021expanding,anand2018generative,strokach2022deep} structures where category-level reasoning is of paramount importance. 

\section{Methods}
\label{sec:methods}

\subsection{Dataset}
In this paper, we study the most challenging detector simulation problem with the highest spatial resolution dataset coming from the Pixel Vertex Detector~(PXD)~\cite{muellerAspectsPixelVertex2014}, shown in~\Cref{fig:pxd_location}, the innermost sub-detector of Belle~II~\cite{abe2010belle}.
The configuration of the PXD consists of \num{40} sensors within two detector layers, as shown in \Cref{fig:pxd_layout}.
The inner layer has \num{16} sensors, and the outer layer comprises \num{24} sensors.
Thus, each event includes \num{40} grey-scale images, each with a resolution of $250\times 768$ pixels, resulting in more than 7.5 million pixel channels per event as shown in~\Cref{fig:pxd_sensor_examples}. 

The problem with such a high-resolution background overlay~\cite{kim2017simulation} is that many resources are required for their readout, storage, and distribution. For example, the size of the PXD background overlay data needed for the simulation of a single event is approximately~\SI{200} kB. This is roughly \num{2} times the size of the background overlay data per event with respect to all other detector components together~\cite{kuhrComputingBelleII2011}. While storing such a massive amount of data is very inefficient for high-resolution detectors, we propose to generate them with IEA-GAN on the fly, which will free up a significant amount of storage space~(see~\Cref{tab:computational_perf} in~\cref{sec:ex_tables}). In this way, the only storage cost is for the weights of the IEA-GAN, depicted in~\Cref{fig:evtgen}.

For training and evaluation, we used \num{40000} and \num{10000} Monte Carlo simulated~\cite{kuhrComputingBelleII2011} events, respectively. The data in each event consists of \num{40} grey-scale $256\times 768$ zero-padded images. They are zero-padded on both sides from their original size of $250 \times 768$ to be divisible by \num{16} for training purposes.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/belleii_pxd.pdf}
        \caption{}
        \label{fig:pxd_location}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/pxd_ladder}
        \caption{}
        \label{fig:pxd_layout}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.55\textwidth}
        \centering
        %\includegraphics[width=\textwidth]{plots/sen07.png}%
        \includegraphics[width=\textwidth]{plots/sen07}
        \vfill
        \includegraphics[width=\textwidth]{plots/sen25}
        \caption{}
        \label{fig:pxd_sensor_examples}
    \end{subfigure}%
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{plots/evtgen_geant4.pdf}
        \vfill
        \includegraphics[width=0.9\textwidth]{plots/evtgen_IEA-GAN.pdf}
        \caption{}
        \label{fig:evtgen}
    \end{subfigure}%
    \caption{
        \textbf{Pixel detector layout} The pixel detector~(PXD) is the inner-most sub-detector of the Belle~II experiment (\textbf{a}) and is configured in a two-layered overlapping sensor structure (\textbf{b}).
        PXD image examples~(\textbf{c}) for sensors \num{7} (top) and \num{25} (down).
        (\textbf{d}) The event generation pipeline with Geant4~\cite{agostinelli2003geant4}~(top) and using IEA-GAN~(bottom).
        Generating PXD data on the fly of analysis avoids the need to store them offline.
    }
    \label{fig:PXD_overview}
\end{figure}

\subsection{Theory of Generative adversarial Networks}

GANs are generative models that try to learn to generate the input distribution as faithfully as possible.
For conditional GANs~\cite{mirza2014conditional}, the goal is to generate features given a label. 
Two player-based GAN models introduce a zero-sum game between two Synthetic Intelligent Agents, a generator network $G$, and a discriminator network $D$. 

\begin{definition}[Vanilla GAN]
Given the generator $G$, a function $G:\mathbb{R}^d\rightarrow \mathbb{R}^n$, that maps a latent variable $z\in \mathbb{R}^d$ sampled from a distribution to an n-dimensional observable, and the discriminator $D$, a functional $D:\mathbb{R}^n\rightarrow [0,1]$, that takes a generated image $x \in \mathbb{R}^n$ and assigns a probability to it, they are the players of the following two-player minimax game with value function $V(D, G)$~\cite{goodfellowGenerativeAdversarialNetworks2014},

\begin{equation*}
\mathop{min}_{G}\mathop{max}_{D}~V(D,G) = \mathbb{E}_{x\in \mathbb{R}^n}[\log D(x)]+\mathbb{E}_{z\in \mathbb{R}^d}[\log(1-D(G(z))].
% \label{eq:GAN_minmax}
\end{equation*}
\end{definition}

After introducing the vanilla GAN, a vast amount of research has been undertaken to improve its convergence and stability, as, in general, training GANs is a highly brittle task. It requires a significant amount of hyperparameter tuning for domain-specific tasks.
Many tricks, model add-ons, and structural changes have been introduced.
A recent and comprehensive study prompted a very powerful SOTA model, BigGAN-deep~\cite{brock2018large}, which incorporates the hinge-loss variation of the adversarial loss~\cite{limGeometricGAN2017},

\begin{align}
\Lb_D^{\mathrm{hinge}} &= -\mathbb{E}_{x\in \mathbb{R}^n}[\min(0,-1+D(x))]-\mathbb{E}_{z\in \mathbb{R}^d}[\min(0, -1-D(G(z))] ~, \\
\Lb_G^{\mathrm{hinge}} &= -\mathbb{E}_{z\in \mathbb{R}^d}[D(G(z))].
\label{eq:hinge_loss}
\end{align}

Furthermore, many schemes for capturing the class conditions have been proposed since conditional GANs over image labels have been introduced~\cite{mirza2014conditional}.
The main idea is to minimize a specific metric between a class identification output of the discriminator and the actual labels after injecting an embedding of the conditional prior information to the generator.
For example, ACGAN~\cite{odenaConditionalImageSynthesis2017} tries to capture data-to-class relations by introducing an auxiliary classifier.
The ProjGAN~\cite{miyatoCGANsProjectionDiscriminator2018} also tries to capture these data-to-class relations by projecting the class embeddings onto the output of the discriminator via an inner product that contributes to the adversarial loss.
The recent ContraGAN~\cite{kangContraGANContrastiveLearning2020} incorporated concepts from metric learning or self-supervised learning~(SSL) in order to seize data-to-data relations or intra-class relations by introducing the 2C~loss, derived from NT-Xent loss~\cite{chenSimpleFrameworkContrastive2020}, and proxy-based SSL,

\begin{equation}
\resizebox{.9\hsize}{!}{$\ell_{2C}(x_i,y_i) = -~\log(\frac{\exp{(S_c(h(x_i)^{\top}e(y_i)))}+\sum^m_{k=1}\mathbf{1}_{k=i}.\exp{(S_c(h(x_i)^{\top}h(x_k)))}}{\exp{(S_c(h(x_i)^{\top}e(y_i)))}+\sum^m_{k=1}\mathbf{1}_{k\neq i}.\exp{(S_c(h(x_i)^{\top}h(x_k)))}}).$}
\label{eq:2C_loss}
\end{equation}

Here, $x_i$ are the images, $y_i$ are the corresponding labels, $S_c(.,.)$ is a similarity metric, and $h(.)$ is the output of the image embeddings.
Although ContraGAN benefits from this loss by capturing the intra-class characteristics among the images that belong to the same class, it is prone to class-confusion~\cite{kangRebootingACGANAuxiliary2021,rangwani2021class} as different classes could also show similarity among themselves since their vector representation in the embedding space might not be orthogonal to each other, which is precisely what we are dealing with in a fine-grained dataset.

\subsection{Relational Reasoning}

\subsubsection{Background}

Transformers~\cite{vaswaniAttentionAllYou2017a} are widely used in different contexts. However, their application in Generative Adversarial Networks is either over the image manifold to learn long-range interactions between pixels~\cite{zhangSelfAttentionGenerativeAdversarial2019b,hudsonGenerativeAdversarialTransformers2021} or via pure Vision-Transformer based GANs~\cite{jiangTransGANTwoPure2021} in which they utilize a fully Vision-Transformer~\cite{dosovitskiyImageWorth16x162022} based generator and discriminator.
Given the fact that training the Transformers is notoriously difficult~\cite{liuUnderstandingDifficultyTraining2020} and task-agnostic when determining the best learning rate schedule, warm-up strategy, decay settings, and gradient clipping, fusing and adapting a Transformer encoder over a GAN learning regime is a highly non-trivial task.
In this paper, we successfully merge a Transformer-based module adapted to the GAN training schemes for the discriminator's image and the generator's class modalities without any of the aforementioned problems.


\begin{definition}[Attention]
Transformers utilize a self-attention mechanism, the data of $(K,Q,V,A)$. The vector spaces $K\in \mathbb{R}^{N\times d_k}$, $Q\in \mathbb{R}^{N\times d_k}$ and $V\in \mathbb{R}^{N\times d_v}$ are the set of Keys, Queries, and Values.
The bilinear map $a:K\times Q\rightarrow \mathbb{R}^{N\times N}$ is a similarity function between a key and a query. The attention, $A$, is defined as
\begin{equation*}
A(K,Q,V):= \mathrm{softmax}(a(K,Q))V ~,
% \label{eq:attention}
\end{equation*}
where $d_k$ and $d_v$ are the dimensions of the corresponding vector spaces.
\end{definition}

The attention mapping used in the vanilla Transformer~\cite{vaswaniAttentionAllYou2017a} adopts the scaled dot-product as the bilinear map between keys and queries as
\begin{equation}
A(K,Q,V):= \mathrm{softmax}(\frac{KQ^T}{\sqrt{d_k}}))V ~.
\label{eq:attention_map}
\end{equation}

The normalization factor $\frac{1}{\sqrt{d_k}}$ mitigates vanishing gradients for large inputs.
Rather than simply computing the attention once, the multi-head mechanism runs through the scaled dot-product attention of linearly transformed versions of keys, queries, and values multiple times in parallel via learnable maps $W_i^k$, $W_i^q$ and $W_i^v$.
The independent attention outputs over $h$ number of heads are then aggregated and projected back into the desired number of dimensions via $W^p$, 
\begin{equation}
\mathrm{MultiHead}(K,Q,V):= [\biguplus_{i=1}^h H_i]W^p ~,
\label{eq:attention_multi_head}
\end{equation}
where $H_i$ is given by $ A(KW_i^k,QW_i^q,VW_i^v)$. When used for processing sequences of tokens, the Self-Attention mechanism allows the transformer to figure out how important all other tokens in the sequence are, with respect to the target token, and then use these weights to build features of each token.

\subsubsection{Event Approximation}

Performing attention on all token pairs in a set to identify which pairs are the most interesting enables Transformers like Bert~\cite{devlin2018bert} to learn a context-specific syntax as the different heads in the multi-head attention might be looking at different syntactic properties~\cite{clarkWhatDoesBERT2019,yunAreTransformersUniversal2020}.
Therefore, to model the context-based similarity between the different detector sensors in each event rather than their absolute properties, we have to use a permutation-equivariant~\cite{guttenberg2016permutation,ravanbakhsh2017equivariance} block such that it can encode pairwise correspondence among elements in the input set.
For instance, Max-Pooling~(e.g.~DeepSets~\cite{zaheerDeepSets2017}) and Self-Attention~\cite{vaswaniAttentionAllYou2017a} are the common permutation equivariant modules for set-based problems.

Moreover, it is impossible to pre-define meaningful sparse connections among the sample nodes in an event.
For instance, the relation between images from different sensors can vary from event to event, albeit cumulatively, they follow a particular distribution. 
Hence, we use a self-attention mechanism with weighted sum pooling as a form of information routing to process meaningful connections between elements in the input set and create an event graph.

Each sample in an event is viewed as a node in a fully connected event graph, where the edges represent the learnable degree of similarity, as shown in \Cref{fig:IEA_GAN}.
Samples in each event go into message propagation steps of our Relational Reasoning Module~(RRM), a GAN-compatible fully connected multi-head Graph Transformer Network~\cite{battaglia2018relational,sharifzadehClassificationAttentionScene2021,locatello2020object}.


\subsubsection{Relational Reasoning Module}

Specifically designed to be compatible with GAN training policies, the Relational Reasoning Module~(RRM) can capture contextualized embeddings and cluster the image or class tokens in an event based on their inherent similarity. 

Let $\mathbf{X}=\{x_1,...,x_m\}$ be the set of the sampled images in each event, where $x_i \in \mathbb{R}^{d}$, and $\mathbf{y}=\{y_1,...,y_m\}$ be the set of labels, with $y_i \in [\![1,40]\!]$ for \num{40} detector~(PXD) sensors.
We also define two linear hypersphere projection diffeomorphisms, $\mathbf{h}_x:\mathbb{R}^k \rightarrow \mathbb{S}^n$ and $\mathbf{h}_y:\mathbb{Z} \rightarrow \mathbb{S}^n$, which map the image embedding manifold and the set of labels to a unit n-sphere, respectively.
The unit n-sphere is the set of points, $\mathbb{S}^n = \{s\in\mathbb{R}^{n+1}\mid \|s\|_2=1 \}$, that is always convex and connected.
The Relational Reasoning Module benefits from a variant of the Pre-Norm Transformer~\cite{vaswaniAttentionAllYou2017a} with a dot-product Multi-head Attention block such that

\begin{align}
\mathbf{p}'^{(l)}_i &= \mathbf{p}^{(l)}_i +\sum_{k=1}^h\sum_{j=1}^{m} a_{ij}^{(l,k)}\mathbf{W}_{\mathrm{SN}}^{(l)}\mathbf{LN}(\mathbf{p}_j^{(l)}) ~, 
\label{eq:dis_1}
\\
\mathbf{p}^{(L)}_i &= \mathbf{h}_x^{\mathrm{LN}}\left(\mathbf{LN}(\mathbf{\bigcirc}_{l=0}^{L}(\mathbf{p}'^{(l)}_i + \mathcal{F}_{\mathrm{SN}}[\mathbf{LN}(\mathbf{p}'^{(l)}_i)]))\right) ~,
\label{eq:dis_2}
\end{align}
where $\mathbf{p}'^{(l)}_i\in \mathbb{R}^{k}$ is the embedding of each image via the discriminator for layer $l$ of the RRM.
$\mathbf{LN}$ is the Layer Norm function~\cite{baLayerNormalization2016} and $h$ is the number of heads defined in \Cref{eq:attention_multi_head}.
$\mathcal{F}[~.~]$ is a two layer MLP functional defined as $\mathcal{F}_{\mathrm{SN}}[\mathbf{p}_i^{(l)}] = \textrm{ReLU}(p_i^{(l)}\mathbf{W}_{\mathrm{SN}}^{(l,1)})\mathbf{W}_{\mathrm{SN}}^{(l,2)}$ with Spectral Normalization~\cite{miyatoSpectralNormalizationGenerative2022b}.
The logits $a_{ij}^{(l,k)}$ are the normalized Attention weights of the bilinear function that monitor the dyadic interaction between image embeddings in layer $l$ and head $k$ defined in \Cref{eq:attention_map}. $\mathbf{W}_{\mathrm{SN}}^{(l)}$ in~\Cref{eq:dis_1} is the learnable multi-head projector at layer $l$ defined in \Cref{eq:attention_multi_head} with Spectral Normalization.
The output of the composition of all layers via the composition of $L$ functionals, $\bigcirc_{l=0}^{L}\Phi^l:= \phi_{w_L}\circ ... \circ \phi_{w_0}[\mathbf{p}_i^{(l=0)}]\in \mathbb{R}^{m\times k}$, goes into a Layer Normalization layer where $\Phi^l = \mathbf{p}'^{(l)}_i + \mathcal{F}[\mathbf{LN}(\mathbf{p}'^{(l)}_i)]$.
$\mathbf{h}_x^{\mathrm{LN}}(~.~)$ in~\Cref{eq:dis_2} is the hypersphere compactification while the vectors are being standardized over the unit n-sphere $\mathbb{S}^n$ by a Layer Normalization.

For the discriminator, this module takes the set of image embeddings as input nodes within a fully connected event graph, applies a dot-product self-attention over them, then updates each sample or node's embedding via the attentive message passing, as shown on the right of~\Cref{fig:RRM_components}.
In the end, it compactifies the information by projecting the normalized graph onto a hypersphere, as shown in~\Cref{fig:IEA_GAN}.
Embedding the samples in an event on the unit hypersphere provides several benefits.
In modern machine learning tasks such as face verification and face recognition~\cite{wangNormFaceL2Hypersphere2017}, when dot products are omnipresent, fixed-norm vectors are known to increase training stability.
In our case, this avoids gradient explosion in the discriminator.
Furthermore, as $S^n$ is homeomorphic to the 1-point compactification of $\mathbb{R}^n$ when classes are densely grouped on the n-sphere as a compact convex manifold, they are linearly separable, which is not the case for the Euclidean space~\cite{gorbanStochasticSeparationTheorems2017}. 

For the generator's RRM, we use a simpler version of the above dot-product Multi-head Attention block without the last hypersphere compactification due to the stability issues., as shown on the left of \Cref{fig:RRM_components}.
It finds a learnable contextual embedding for each event that will be fused to each class token via the feature mixing layer, which is a matrix factorization linear layer $\mathbf{W}_{\mathrm{SN}}(.)$. Formally we have,

\begin{align}
\mathbf{q}^{(0)}_i &= \mathbf{W}_{\mathrm{SN}}(\mathbf{r}_i\uplus\mathbf{e}_i) ~,
\label{eq:feat_mix_1}
\\
\mathbf{q}'^{(l)}_i &= \mathbf{q}^{(l)}_i+\sum_{k=1}^M\sum_{j=1}^{m} a_{ij}^{(l,k)}\mathbf{W}^{(l)}\mathbf{LN}(\mathbf{q}_j^{(l)}) ~,
\label{eq:feat_mix_2}
\\
\mathbf{q}^{L}_i &=\mathbf{LN}\left(\mathbf{\bigcirc}_{l=0}^{L}( \mathbf{q}'^{(l)}_i + \mathcal{F}[\mathbf{LN}(\mathbf{q}'^{(l)}_i)])\right) ~,
\label{eq:feat_mix_3}
\end{align}
where $\mathbf{e}_i:\mathbb{Z} \rightarrow \mathbb{R}^t$ is the embedding of each class token via the embedding layer of the generator. The logits $a_{ij}^{(l,k)}$ are the normalized Attention weights of the bilinear function that monitor the dyadic interaction between classes in the event embeddings in layer $l$ and head $k$ defined in~\Cref{eq:attention_map}. $\mathbf{W}^{(l)}$ in~\Cref{eq:feat_mix_2} is the learnable multi-head projector at layer $l$ defined in \Cref{,eq:attention_multi_head}. The output of the composition of all layers via the composition of $L$ functionals, $\bigcirc_{l=0}^{L}\Phi^l:= \phi_{w_L}\circ ... \circ \phi_{w_0}[\mathbf{q}_i^{(l=0)}]\in \mathbb{R}^{m\times t}$, goes into a Layer Normalization layer where $\Phi^l = \mathbf{q}'^{(l)}_i + \mathcal{F}[\mathbf{LN}(\mathbf{q}'^{(l)}_i)]$ as shown in~\Cref{eq:feat_mix_3}.

One input to the generator is the embedded labels, which can be considered rigid token embeddings that will be learned as a global representation bias of each sensor. As sensor conditions change for each event as a set, having merely class embeddings, as used in conditional GANs~\cite{mirza2014conditional}, is insufficient because the context-based information will not be learned.
Thus, the generator samples from a per-event shared distribution at each event as random degrees of freedom~(Rdof).
Rdofs are random samples from a shared Normal distribution for each class, $\mathbf{r}_i \sim \mathcal{N}(0,1)$, that introduces four-dimensional learnable degrees of freedom for the generator, see~\Cref{eq:feat_mix_1} 
This way, we ensure that the generator is aware of intra-event local changes, culminating in having an intra-event correlation among the generated images. Rdof can be interpreted as both perturbation~\cite{zhangWordEmbeddingPerturbation2018} to the token embeddings and an event-level segment embedding~\cite{devlin2018bert}, which can enhance the diversity of the generated images.


\subsection{Intra-Event Aware Loss}
Motivated by Self-Supervised Learning~\cite{weng2019selfsup}, to transfer the intra-event contextualized knowledge of the discriminator to the generator in an explicit way, we introduce an Intra-Event Aware~(IEA) loss for the generator that captures class-to-class relations,

\begin{equation}
\ell_\mathrm{IEA}(\mathbf{x}_r,\mathbf{x}_f) = \sum_{i,j} D_\mathrm{KL} \left( \sigma \left( \mathbf{h}(x^{(r)}_i)^{\top}\mathbf{h}(x^{(r)}_j) \right) \Big\Vert \sigma \left(\mathbf{h}(x^{(f)}_i)^{\top}\mathbf{h}(x^{(f)}_j)\right)\right),
\label{eq:iea_loss}
\end{equation}

where $\mathbf{x}_r=\{x^{(r)}_i\}_{i=1}^m$ is the set of real images, and $\mathbf{x}_f=\{G(z^i,y^i,r^i)=x^{(f)}_i\}_{i=1}^m$ the set of generated images. The softmax function, $\sigma:\mathbb{R}^m\rightarrow [0,1]^m$, normalizes the dot-product self-attention between the image embeddings.
The map $\mathbf{h}:\mathbb{R}^k \rightarrow \mathbb{S}^n$ is the unit hypersphere projection of the discriminator.
Therefore, the dot product is equivalent to the cosine distance.
$D_{\mathrm{KL}}(.\vert \vert.)$ is the Kullback-Leibler~(KL) divergence~\cite{kullbackInformationSufficiency1951} which takes two $m\times m$ matrices that have values in the closed unit interval (due to the softmax function).
Hence, having a KL divergence is natural here as we want to compare one probability density with another in an event.
We also tested other distance functions reported in \Cref{sec:ablation_studies}.
By considering the linear interaction~\cite{caoCouplingLearningComplex2015} between every sample in an event and assigning a weight to their similarity, the generator mimics the fine-grained class-to-class relations within each event and incorporates this information in its RRM module as shown in \Cref{fig:IEA_GAN}.

\begin{figure}[ht]
     \centering
     \includegraphics[width=\textwidth,clip]{plots/IEAloss.pdf}
     \caption{IEA-loss imposes a pair-wise fine-grained class-to-class imitation force for the generator. sg indicates that for the discriminator gradients are stopped and only the generator's gradients will be updated.}
     \label{fig:IEA_loss}
 \end{figure}

Upon minimizing it for the generator~(having the stop-gradient for the discriminator), we are putting a discriminator-supervised penalizing system over the intra-event awareness of the generator by encouraging it to look for more detailed dyadic connections among the images and be sensitive to even slight differences.
Ultimately, we want to maximize the consensus of data points on two unit hyperspheres of real images and generated image embeddings. 

\subsection{Uniformity Loss}
\label{sec:uniformity}

The other crucial loss function comes from contrastive representation learning.
With the task of learning fine-grained class-to-class relations among the images, we also want to ensure the feature vectors have as much hyperspherical diversity as possible.
Thus, by imposing a uniformity condition over the feature vectors on the unit hypersphere, they preserve as much information as possible since the uniform distribution carries a high entropy.
This idea stems from the Thomson problem~\cite{thomsonXXIVStructureAtom1904}, where a static equilibrium with minimal potential energy is sought to distribute N electrons on the unit sphere in the evenest manner.
To do that, we incorporate the uniformity metric~\cite{wangUnderstandingContrastiveRepresentation2020}, which is based on a Gaussian potential kernel,

\begin{equation}
\Lb_{\mathrm{uniform}}(x;s) = \log \mathbb{E}_{x_i,x_j\sim p_{\mathrm{event}}} [\exp(s\|\mathbf{h}(x_i)-\mathbf{h}(x_j)\|_2^2)].
\label{eq:uniformity_loss}
\end{equation}

Upon minimizing this loss for the discriminator, it tries to maintain a uniform distance among the samples that are not well-clustered and thus not similar.
In other words, eventually, we want to reach a maximum geodesic separation incorporating the Riesz s-kernel~\cite{liu2018learning} with $s=-2$ as a measure of geodesic similarity, to preserve maximal information over the Hypersphere.
Therefore, asymptotically it corresponds to the uniform distribution on the hypersphere~\cite{kuijlaarsAsymptoticsMinimalDiscrete1998}.
This loss is beneficial for capturing the exact distribution of the mean occupancy distribution and balancing the inter-class pulling force of the Relational Reasoning module. As a result, not only does it help generate more diverse and varied outputs, but it also can prevent issues such as mode collapse or overfitting.

\subsection{Model Details and Hyperparameters}
\label{sec:dataset_and_model_details}
To capture the intra-event mutual information among the images using the RRM and approximate the concept of an event, we have to sample properly at each iteration.
Hence, we utilize a one-sample per class sampler in data loading.
Using this sampler, we ensure that in each event, we have 40 unique classes of images from all 40 sensors that belong to the same event in the Monte-Carlo simulation.
All hyperparameters are chosen based on the model's stability and performance upon the evaluation set.
The learning rates for the Generator and Discriminator are $5\times10^{-5}$ with one sample per class sampler.
The Relational Reasoning Module of the Generator has two heads and one layer of non-spectrally normalized message propagation with an embedding dimension of 128 and ReLU non-linearity.
The input to the generator's RRM is embedded class tokens mixed with \num{4} random degrees of freedom by a spectrally normalized linear layer. 

For the Discriminator, the RRM has four heads with one layer of spectrally normalized message propagation with the embedding dimension \num{1024} as the hypersphere dimension and ReLU non-linearity.
All Generator and Discriminator modules use Orthogonal initialization~\cite{saxeExactSolutionsNonlinear2014}.
For the IEA-loss in \Cref{eq:iea_loss}, the coefficient $\lambda_{\mathrm{IEA}} = 1.0$, defined in \Cref{alg:iea} gives the best result.
The most stable contribution of the Uniformity loss, defined in \Cref{eq:uniformity_loss} and \Cref{alg:iea}, is with $\lambda_{\mathrm{uniform}}=0.1$.
For the backbone of both the discriminator and the generator, we use BigGAN-deep~\cite{brock2018large} with a non-local block at channel \num{32} for the discriminator only.
Since in GAN training, there is no meaningful way to define a minimal loss; our stopping point is the divergence of the Frechet Inception Score (FID)~\cite{heuselGANsTrainedTwo2017}, which is significantly correlated with the quality of other metrics.

\begin{algorithm}[ht]
\caption{Intra-Event Aware GAN}
\label{alg:iea}
\begin{algorithmic}[1]
\Require{generator and discriminator parameters $\theta_G$, $\theta_D$, Intra-Event-aware coefﬁcient $\lambda_{\mathrm{IEA}}$, Uniformity coefﬁcient $\lambda_{\mathrm{uniform}}$ and hyperparameter $s$, Adam hyperparameters $\alpha$, $\beta_1$, $\beta_2$, event size $M$, number of discriminator iteration steps per generator iteration $N_D$}
\Statex
\For{number of training iterations}
    \For{$t=1,...,N_D$}
        \State sample $\{z^{i}\}^M_{i=1}\sim p(z)$, 
        \State $\{x^{i},y^i\}^M_{i=1}\sim p_{\mathrm{event}}(x, y)$, $\{r^i\}^M_{i=1} \sim p_{\mathrm{Rdof}}(z)$ \Comment{\scriptsize Event Sampling.}
        \For{$i=1,...,M$}
            \State $\ell_{D_{\mathrm{hinge}}}^{(i)}\leftarrow \ell_{D_{\mathrm{hinge}}}(x^{(i)};G(z^{i}, y^{i}, r^i))$
        \EndFor
        \State $\Lb_{D_{\mathrm{hinge}}}\leftarrow \frac{1}{M}\sum_{i=1}^M\ell_{D_{\mathrm{hinge}}}^{(i)}$
        \State $\Lb_{\mathrm{uniform}}\leftarrow \Lb_{\mathrm{uniform}}(x;s)$
        \Comment{\scriptsize The Uniformity Loss.}
        \State $\Lb_{2C}^{\mathrm{real}}\leftarrow \frac{1}{M}\sum_{i=1}^M\ell_{2C}(x^i,y^i)$
        \State $\theta_D\leftarrow Adam(\Lb_{D_{\mathrm{hinge}}}+\lambda_{2C}\Lb_{2C}^{\mathrm{real}}+\lambda_{\mathrm{uniform}} \Lb_{\mathrm{uniform}}, \alpha, \beta_1, \beta_2)$
    \EndFor
    \State sample $\{z^{i}\}^M_{i=1}\sim p(z)$, 
    \State sample $\{r^i\}^M_{i=1} \sim p_{\mathrm{Rdof}}(z)$ \Comment{\scriptsize Event Sampling.}
    \For{$i=1,...,M$}
        \State $\ell_{G_{\mathrm{hinge}}}^{(i)}\leftarrow \ell_{G_{\mathrm{hinge}}}(G(z^{i}, y^{i}, r^{i}))$
    \EndFor
    \State $\Lb_{G_{\mathrm{hinge}}}\leftarrow \frac{1}{M}\sum_{i=1}^M\ell_{G_{\mathrm{hinge}}}^{(i)}$
    \State $\Lb_{\mathrm{IEA}}\leftarrow \frac{1}{M}\sum_{i=1}^M\ell_{\mathrm{IEA}}(G(z^i,y^i, r^i),x^{i})$
    \Comment{\scriptsize The Intra-Event Aware Loss.}
    \State $\Lb_{2C}^{\mathrm{fake}}\leftarrow \frac{1}{M}\sum_{i=1}^M\ell_{2C}(G(z^i,y^i,r^i),y^i)$
    \State $\theta_G\leftarrow \mathrm{Adam}(\Lb_{G_{\mathrm{hinge}}}+\lambda_{2C}\Lb_{2C}^{\mathrm{fake}}+\lambda_{\mathrm{IEA}}\Lb_{\mathrm{IEA}},\alpha, \beta_1, \beta_2)$
\EndFor

\end{algorithmic}
\end{algorithm}



\section*{Acknowledgments}
This research was supported by the collaborative project IDT-UM (Innovative Digitale Technologien zur Erforschung von Universum und Materie) funded by the German Federal Ministry of Education and Research~(BMBF) and the Deutsche Forschungsgemeinschaft under Germany’s Excellence Strategy – EXC 2094 “ORIGINS” – 390783311. We thank our colleagues from the Ludwig Maximilian University in Munich and the Computational Center for Particle and Astrophysics~(C2PAP) who provided expertise and computation power that greatly assisted the research. We also thank David Katheder for his assistance with the GitHub repository preparation. James Kahn's work is supported by the Helmholtz Association Initiative and Networking Fund under the Helmholtz AI platform grant.


\section*{Declarations}

\subsection*{Data availability}
The data used in this study will be openly available~(TBA).

\subsection*{Code availability}
The code for this study is available at~\href{https://github.com/Hosein47/IEA-GAN}{https://github.com/Hosein47/IEA-GAN}.


\begin{appendices}
\appendix

\section{Ablation Studies and Things We Tried But Did Not Work}
\label{sec:ablation_studies}

For the IEA-loss we tested several losses in order to achieve the best stability, shown in \Cref{fig:FID_IEAloss}.
Some of them have their own merits and downsides. We explored the KL divergence, the L1 loss, the Huber loss~\cite{huber1992robust}, and the L2 loss. KL divergence was more stable to capture differences between the real and fake self-similarities and more robust to outliers.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth,clip]{plots/fid_IEAloss.pdf}
    \caption{Comparison of the FID between different IEA-losses}
    \label{fig:FID_IEAloss}
\end{figure}

We probed a range of coefficients for the IEA-loss and the Uniformity loss.
For the KL divergence as the IEA-loss, we tried the values \{0.1, 1, 5, 10\} and selected $1$.
For the L1 loss, as well as the IEA-loss the best $\lambda_{\mathrm{IEA}}$ value is $10$.
For the Uniformity loss, we probed the values \{0.01, 0.1, 0.5, 0.75, 1, 5, 10\} and selected $0.1$.
Moreover, IEA-GAN without the IEA-loss and Uniformity loss suffers from the lack of agreement maximization penalty for the generator and information maximization for the discriminator.
Our study shows that having either of these losses without the other, causes training instability, divergence, and lower fidelity as shown in \Cref{tab:fid_ablation_losses}.

\begin{table}[ht]
\begin{minipage}{\textwidth}
 \begin{center}
    \caption{FID comparison between IEA-GAN,  IEA-GAN with RRM only, IEA-GAN with Uniformity loss only, and IEA-GAN with both IEA-loss, averaged across six random seeds.}
    \label{tab:fid_ablation_losses}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & IEA-GAN & Only RRM & RRM with Uniformity & RRM with IEA-loss \\ 
    \midrule
    \textbf{FID}  & $\mathbf{1.50\pm 0.16}$ & $2.74\pm 0.62$& $2.29\pm 0.14$& $3.42\pm 0.52$ \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{minipage}
\end{table}

For the hypersphere dimension, we probed the values \{512, 768, 1024, 2048\} and selected \num{1024}.
For dimensions smaller than \num{512} the discriminator fails to converge.
We also changed the position of the hypersphere projection layer and put it before and after the Multi-head attention.
The best position for the hypersphere projection is after the Multi-head attention and two layers of MLP.
Moreover, for the hypersphere projection, we also tried an inverse Stereographic Projection $h:\mathbb{R}^N \rightarrow \mathbb{S}^N/\{p\}$ with p as a north pole on the n-sphere~\cite{eybpooshApplyingInverseStereographic2022} instead of L2 compactification.
This map is conformal thus it locally preserves angles between the data points.
The results were more stable but the average FID was better with L2 compactification as shown in \Cref{tab:fid_hyper_comparison}.

\begin{table}[ht]
\begin{minipage}{\textwidth}
  \begin{center}
     \caption{FID comparison between two different Hypersphere projections for IEA-GAN's discriminator, averaged across six random seeds.}
     \label{tab:fid_hyper_comparison}
    \begin{tabular}{@{}lll@{}}
    \toprule
    & L2 compactification & Inverse Stereographic projection \\ 
    \midrule
    \textbf{FID} & $\mathbf{1.50\pm 0.16}$ & $2.01\pm 0.07$ \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{minipage}
\end{table}

Inside the RRM we tried a GeLU~\cite{hendrycksGaussianErrorLinear2020} non-linearity instead of ReLU and the result was in favor of the latter.
We also put the layer normalization before and after the Multi-head Attention.
The pre-norm version seems to be much more stable and adaptable to GAN training intricacies.
Another observation related to RRM is the weight normalization of the linear layers.
We observed that for the discriminator spectrally normalized MLPs show the best results.
For the Generator, applying Spectral Normalization to the linear layers destabilizes the training.
Our observation regarding the effect of RRM over the generator's label embedding shows that without it, the RRM in the discriminator becomes also unstable and the training diverges very early.

For the random degrees of freedom~(Rdof), first, we utilized the random vectors that are fed to the generator and applied the RRM on top of it. However, the FID did not reach values below \num{20} and there was no correlation. Hence, we introduced separate random sampling for event generation for which we probed dimensions \{2, 4, 8, 16, 32\}. \num{4} degrees of freedom was the most optimal choice. We observed that as the dimension of Rdof increases, the intra-event correlation fades away between the generated images. We also checked the Uniform distribution for event Rdof sampling, which did not lead to any stable result. 
Several ways to fuse the Rdof to the class embeddings were tested such as learnable neural network layer~(matrix factorization), concatenating, summing, and having an MLP with non-linear part, but eventually chose a learnable neural network layer~(matrix factorization) for the feature mixing layer.

We also looked at different combinations of learning rates for G and D.
Using TTUR~\cite{heuselGANsTrainedTwo2017} regime results in a severe mode collapse.
Thus, we used the same learning rates for both G and D.
We swept through $\{1\times10^{-5}, 2.5\times10^{-5}, 5\times10^{-5}, 7.5\times10^{-5}, 1\times10^{-4}\}$ and selected $5\times10^{-5}$.
For the backbone model, the shallow version of BigGAN-deep, BigGAN, lead to mode-collapse, therefore, we chose BigGAN-deep.

\section{Extended Figures and Tables}
\label{sec:ex_tables}
\begin{figure}[htb]
     \centering
         \centering
         \includegraphics[width=0.75\textwidth]{plots/num_hits}
         \caption{
         The number of PXD hits per layer and sensor (ladder).
         Each sensor has different occupancies for sensors in the inner and outer layers.
         The global asymmetry between them stems from the $\phi$ dependency of PXD images.
         }
         \label{fig:PXD_num_hits}
\end{figure}


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.75\textwidth,clip]{plots/data_storex2.pdf}
    \caption{Data volume comparison of PXD data per event compared to other sub-detectors at Belle~II}
    \label{data}
\end{figure}




\end{appendices}
\clearpage

\bibliographystyle{unsrtnat}
\bibliography{IEAGAN_arxiv}% common bib file

\end{document}
