% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
% 
% Include other packages here, before hyperref.
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{comment}
\usepackage{color}
\usepackage{makecell}
\usepackage{array}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%



% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2298} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}


\title{Rigidity-Aware Detection for 6D Object Pose Estimation -- Appendix}

\author{%
	{Yang Hai $^1$, \quad Rui Song $^1$, \quad Jiaojiao Li $^1$, \quad Mathieu Salzmann $^{2, 3}$, \quad Yinlin Hu $^{4}$} \\
	{\small $^1$ State Key Laboratory of ISN, Xidian University, \quad $^2$ EPFL, \quad $^3$ ClearSpace, \quad $^4$ MagicLeap} \\
    % {\tt\small yanghai1218@gmail.com, \{rsong, jjli\}@xidian.edu.cn, } \\
    % {\tt\small mathieu.salzmann@epfl.ch, yhu@magicleap.com}
}

\maketitle

\section{Appendix}
\input{supp_content.tex}

% \onecolumn
% \begin{center}
%     \Large \textbf{Rigidity-Aware Detection for 6D Object Pose Estimation Appendix}
% \end{center}




% \section{PFA Improvement}
% To attend the BOP Challenge, we improve PFA~\cite{pfa} in several aspects.

% \noindent \textbf{Online Rendering}
% PFA renders the pose exemplars offline, which will cause colossal storage usage when participating in the 2022 BOP Challenge because it has a total of 125 objects, resulting in 1.25 million images to be stored.
% While rendering is still potentially time-consuming, we observe Pytorch3D~\cite{pytorch3d}, efficient at rendering in batch, can relieve this situation.
% Therefore we construct the pose exemplars of all the detected instances to batch format and render them simultaneously.
% The cost is affordable compared to the overall inference speed, and the storage usage can be reduced.



% \noindent \textbf{Iterative Refinement}
% In practice, we find multi-exemplar aggregation brings only marginal improvement but largely slows the inference speed.
% On the other hand, recurrent refinement~\cite{deepim,cosypose, repose,rnnpose} can boost performance more significantly.
% For example, refining one time using four exemplars yields a similar throughput as refining four times using the single initial pose, but the latter can achieve better performance.
% Thus, we only use the initial pose instead and recurrently refine it four times.


% \noindent \textbf{Bi-directional Flow}
% Although PFA uses the 2D-to-2D correspondences from the pose exemplar to the target patch, it is feasible to solve the pose using the 2D-to-2D correspondences from the target patch to the pose exemplar.
% To do so, we additionally predicts the target patch's foreground mask, and extracts the foreground points.
% We then interpolate the corresponding points' depth, and lift the corresponding 2D points to 3D object frame with the known object pose and camera intrinsic.
% In this way, we obtain the 2D-3D correspondences.

% \noindent \textbf{Using Depth}
% We further extend PFA to be capable of handling RGB-D input.
% For the RGB setting, PFA uses a PnP algorithm~\cite{epnp} to solve the pose from 2D-3D correspondences.
% When depth images are available, we lift the 2D points to 3D, then utilize RANSAC-Kabsch~\cite{ncf,dpodv2} to solve the pose from 3D-3D correspondences.
% Compared to the dominant RGB-D methods~\cite{DenseFusion,ffb6d}, our method works with either RGB or RGB-D, which is more flexible.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}

