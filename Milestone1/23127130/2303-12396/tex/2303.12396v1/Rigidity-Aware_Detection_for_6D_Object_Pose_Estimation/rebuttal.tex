\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\definecolor{rvcolor}{RGB}{0,100,162}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2298} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\input{defs.tex}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Rigidity-Aware Detection for 6D Object Pose Estimation - Rebuttal}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

We thank all reviewers for their valuable comments and the acknowledgment of our new perspective of object detection in 6D object pose estimation.
To address most of the reviewers' concerns, we first report detailed ablation studies of our method on the YCB-V dataset. Below, we evaluate the performance of the proposed method with different $k$, $\mathcal{T}$, and $\alpha$. We use the default settings of $k$=10, $\mathcal{T}$=0.25, and $\alpha$=0.1, and vary only one parameter at a time.
%The following table summarizes the results. 

\vspace{-3mm}
\begin{table}[!htb]
    \centering
    \scalebox{0.65}{
    \begin{tabular}{cccc|cccc|cccc}
        \toprule
        $k$                         & AP        & AP$_{50}$     & AP$_{75}$    & $\mathcal{T}$     & AP        & AP$_{50}$     & AP$_{75}$  & $\alpha$ & AP        & AP$_{50}$     & AP$_{75}$ \\
        \midrule
        4                           & 83.4              & 98.4           & 93.5         & 0.1       & 84.1      & 98.5          & 96.0           & 0        & 83.9          & 98.8          & 94.3 \\
        10                          & {\bf 85.0}        & {\bf 99.4}    & {\bf 97.4}    & 0.2       & 84.7      & \underline{99.3}& 96.8          & 0.1       & {\bf 85.0}    & {\bf 99.4}    & {\bf 97.4}\\
        32                          &\underline{84.7}   &\underline{99.2}&\underline{96.9}& 0.25    & {\bf 85.0}& {\bf 99.4}    & {\bf 97.4}    & 0.2       & \underline{84.2} & \underline{99.2}&\underline{95.9}\\
        64                          & 84.2              & 98.9          & 96.3          & 0.3       & \underline{84.9}& {\bf 99.4}&\underline{97.0}& 0.3       & 83.1          & 98.8          & 93.2\\
        128                         & 83.5              & 98.5          & 93.7          & 0.4       & 84.0      & 98.6          & 96.6          & 0.4       & 81.2          & 98.0          & 90.8 \\
        256                         & 82.2              & 97.6          & 91.2          & 0.5       & 83.3      & 98.1          & 93.8          & 0.5       & 80.4          & 97.6          & 89.3 \\
        \bottomrule
    \end{tabular}
    }
    % \caption{{\bf Ablation study of different hyper-parameters on YCB-V.}}
    % \label{tab:my_label}
\end{table}
\vspace{-3mm}

$k$ is the desired number of positive cells for each object instance during training. The worse results when $k$=4 indicate that too small a $k$ provides limited supervision signal to the network, making it harder to train. Conversely, when $k>\text{64}$, the results drop because of too many duplicate positive cells,
%many repeated samplings of the same positive cells, 
especially for small occluded targets.

$\mathcal{T}$ is the threshold to filter out easy negative cells before sampling positive ones. The above results are stable when it is set between 0.2 and 0.3. We use the best value ($\mathcal{T}$=0.25) found on YCB-V for all the experiments in the main paper.

$\alpha$ is the weight balancing the Euclidean distance and the minimum barrier distance in Eq. 3.  Following [18], we use the default value 0.1 in all our experiments.
%, which reports most of its best superpixel segmentation results based on it. 
This is further supported by our results with different $\alpha$ values above.
%showing that the best choice of $\alpha$ in our setting is consistent with [18].
%, and we use 0.1 in all our experiments.

% \noindent \textbf{Choice of $k$.}
% From the table below, we can observe too few positive samples, that is, too small $k$, will lead to a weak supervision signal and result in underfitting.
% Since distance transform can effectively approximate the visibility, our method is quite insensitive to large $k$, as the repeated sampled foreground candidate positives will not harm the performance.
% Nonetheless, too large $k$ will still more likely select background cells, slightly decreasing the performance.

% \vspace{-3mm}
% \begin{table}[h]
%     \centering
%     \begin{tabular}{cccccccc}
%         \toprule
%         $k$     & 4         & 6         & 8         & {\bf 10}        & 12        & 14    & 16 \\
%         \midrule
%         AP      & 83.4      & 84.2      & 84.7       & {\bf 85.0}     & 84.8      & 84.9   & 84.7   \\
%         \bottomrule
%     \end{tabular}
%     % \caption{{\bf Effect of different values of $k$.}}
%     % \label{tab:ablate_k}
% \end{table}
% \vspace{-3mm}

% \noindent \textbf{Choice of $\tau$.}
% $\tau$ is the threshold to determine the candidate positives. Too small $\tau$ will introduce too many background cells, while too large $\tau$ will result in too few candidate positives to select, leading to overfitting.
% Overall, our method is robustness again $\tau$ in a wide range from 0.1 to 0.4.

% \vspace{-3mm}
% \begin{table}[h]
%     \centering
%     \begin{tabular}{cccccccc}
%         \toprule
%         $\tau$  & 0.05      & 0.1       & 0.2       & {\bf 0.25}      & 0.3       & 0.4       & 0.5 \\
%         \midrule
%         AP      & 83.2      & 84.1      & 84.7      & {\bf 85.0}      & 84.9       & 84.0     & 83.3   \\ 
%         \bottomrule
%     \end{tabular}
%     % \caption{{\bf Effect of different values of $\tau$.}}
%     % \label{tab:ablate_tau}
% \end{table}
% \vspace{-3mm}

% \noindent \textbf{Choice of $\alpha$.}
% $\alpha$ is the hyper-parameter proposed in ~\cite{minbarrier}. We don't consider much about it and directly use the default value of 0.1.
% In practice, we do observe the choice of 0.1 yields better visualization results. 
% Since $\alpha$ controls the contribution of geometric distance to the path cost, larger $k$ will degrade to the center-based sampling strategy, as the pixels around the patch center will have a larger distance to the patch borders. This is also evidenced by the table below, where larger $k$ leads to decreased performance.

% \vspace{-3mm}
% \begin{table}[h]
%     \centering
%     \begin{tabular}{ccccccccc}
%         \toprule
%         $\alpha$    & 0.0    & {\bf 0.1} & 0.2       &0.3    & 0.4   & 0.5          & 0.6  \\
%         \midrule
%         AP          & 83.9   & {\bf 85.0}   & 84.2      & 83.1   &81.2    & 80.4    & 80.7  \\
%         \bottomrule
%     \end{tabular}
%     % \caption{{\bf Effect of different values of $\alpha$}}
%     % \label{tab:ablate_alpha}
% \end{table}
% \vspace{-3mm}

\vspace{0.2em}
\noindent \textbf{\textcolor{rvcolor}{To Reviewer vxLa}}

\noindent \textbf{More discussion of single-stage detection methods.}
Most of the network components used in our method are standard building blocks for single-stage detection, and we summarize this standard framework in L248-263.
% Our contributions consist in identifying the weakness of this framework in the context of 6D object pose estimation and proposing a new sampling strategy leveraging the rigidity property of the targets in 6D pose estimation, making it more robust to occlusions. 
We will nonetheless revise the text based on the reviewer's suggestion.

% \noindent \textbf{Details of box regression loss.} 
% With the development of object detection, there are many works proposed for the loss function of box regression, like CIOU loss, GIOU loss, etc, and we use GIoU loss which is the standard experiment setting. Considering we focus on the sampling strategy, we did not put the thorough details of box regression loss function.

% \noindent \textbf{Emphasis of visibility.}
% Our work is oriented to rigid objects. In Figure 4, we show that nearly all the visible parts of rigid objects, \ie, YCB-V dataset, can contribute a reliable prediction, but this phenomenon does not exist in non-rigid general objects, \ie, COCO dataset. This is exactly the rigidity we emphasize, which is that rigid object detection can benefit from properly utilizing the object's visibility.  Thus, it inspires us to train the model in a visibility-guided sampling strategy, such that it can utilize all the visible parts to make a more robust prediction for rigid object detection.

% \noindent \textbf{Visibility-guided sampling for general object detection.}
% Although our approach can be applied to general object detection, the general object detection would not benefit from a lot, since . Nonetheless, our approach does not sacrifice in detecting general objects. On COCO benchmark, our approach achieves 40.2 AP, which is similar to PAA(40.4), and better than ATSS(39.2) and FCOSv2(38.9).

% \noindent \textbf{Motivation of distance transform for visibility modeling.}
% Distance transform is a classical tool for image segmentation and clustering, before the era of deep learning. In fact, we are not the first work to utilize distance transform to model the object's visibility. Utilizing the background prior~\cite{zhang2015minimum, wei2012geodesic}, \ie, the image boundary pixels are mostly background, distance transform is shown to be advantageous in salient object segmentation, which inspires our work to model the object's visibility using distance transform.

% \noindent \textbf{Failure cases of visibility modeling.} 
% We show some failure cases in Figure 9 and will supplement them in the revision.

\noindent \textbf{Rigidity and visibility.} Fig. 4(a) shows that different sampling strategies have similar effects on the general COCO dataset, even with ground-truth visibility guidance. However, the visibility-guided strategy yields a different behavior on the 6D pose dataset YCB-V, as shown in Fig. 4(b). This is due to the rigidity of the YCB-V targets, which implies that most visible parts can provide a reliable estimate of the complete bounding box. This is not true for COCO since local visible parts usually yield unreliable global box estimates for non-rigid targets. This motivated us to present our work in terms of rigidity instead of visibility.

\noindent \textbf{General scenario.} Our work is motivated by the rigidity of the targets in 6D object pose estimation. The general scenario, e.g., COCO, does not fully match our assumption. Nevertheless, we report results with the same experimental setting as FCOSv2 and PAA in the following table. Besides the average accuracy across the 80 COCO categories, we report the accuracy of the 5 categories on which our method outperforms the baselines most, and the 5 categories on which our method performs the worst. Our method outperforms the baselines significantly on categories like toaster, bottle, etc., which are mainly rigid objects. By contrast, the categories on which our method underperforms include cat, dog, etc., which are mainly non-rigid targets and break our assumption. Our method nevertheless achieves similar performance to the baselines in average accuracy.

\vspace{-3mm}
\begin{table}[!htb]
    \centering
    \scalebox{0.7}{
    \begin{tabular}{c|ccc|ccc|ccc}
        \toprule
        \multirow{2}{*}{Category}  & \multicolumn{3}{c|}{FCOSv2}                &\multicolumn{3}{c|}{PAA}              &\multicolumn{3}{c}{\bf Ours}         \\
        ~                           & AP        & AP$_{50}$     & AP$_{75}$     & AP            & AP$_{50}$     & AP$_{75}$ & AP            & AP$_{50}$     & AP$_{75}$ \\
        \midrule
        toaster                     & \underline{20.9} &\underline{38.4}& \underline{30.5}          & 16.1          & 34.0          & 25.7      & {\bf 31.8}    & {\bf 47.2}    & {\bf 42.3}         \\
        bottle                      & \underline{38.3}      & \underline{59.4}  & \underline{43.1}          & 37.0          & 58.5          & 41.9        & {\bf 40.5}  & {\bf 60.7}    & {\bf 45.7}     \\
        skateboard                  & 50.1       & \underline{69.7} & 59.4          & \underline{50.2}  & 69.3  & \underline{60.2}      & {\bf 52.6} & {\bf 71.5}  & {\bf 63.8}        \\
        % bicycle                     &           &               &               & 27.3          &               &           & 29.7          &               &           \\
        suitcase                    & 34.7      & \underline{52.5}          & 38.8          & \underline{35.1} & 52.0   & \underline{39.0}       & {\bf 36.5}   & {\bf 53.6}    & {\bf 40.7}        \\ 
        cup                         & 41.3      & 62.3  & \underline{50.6}  & \underline{41.5}  & \underline{62.7}  & \underline{50.6}      & {\bf 42.8}    & {\bf 63.2}    & {\bf 52.2}        \\
        ... & & ... & & & ... & & & ... \\
        \midrule
        cat                         & \underline{65.7}  & \underline{86.8} & \underline{68.4}          & {\bf 67.2}   & {\bf 88.9} & {\bf 69.8}     & 63.0          & 84.5          & 66.2        \\
        dog                         & \underline{60.7} & \underline{81.9}   & \underline{62.4}          & {\bf 62.1}    & {\bf 83.2}    & {\bf 63.9}       & 59.3          & 80.8         & 60.7     \\  
        cow                         & \underline{56.4}      & \underline{72.3}  & \underline{60.5}          & {\bf 58.0}    & {\bf 74.0}    & {\bf 62.8}      & 56.0           & 71.0       & 60.2   \\
        sheep                       & \underline{51.4} & \underline{71.2}  & \underline{60.7}          & {\bf 51.7}   & {\bf 71.6} & {\bf 61.2}      & 50.0          & 70.3          & 58.6         \\
        bird                        & {\bf 36.2}& {\bf 54.8}& {\bf 42.3}    & \underline{35.6}  & \underline{54.0}  & 41.4     & 35.1          & 53.5          & \underline{41.6}      \\
        ... & & ... & & & ... & & & ... \\
        \midrule
        {\bf Avg.}                  & 38.9      & \underline{57.5}  & 42.2          & {\bf 40.4}          & {\bf 58.4}          & {\bf 43.9}      & \underline{40.0}    & 57.1  & \underline{43.4}      \\
        \bottomrule
    \end{tabular}
    }
    % \caption{{\bf Ablation study of different hyper-parameters on YCB-V.}}
    % \label{tab:my_label}
\end{table}
\vspace{-3mm}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{c|ccc|ccc}
%         \toprule
%         \multirow{2}{*}{Method}  & \multicolumn{3}{c|}{Rigid}                &\multicolumn{3}{c}{Non-rigid}         \\
%         ~                           & AP        & AP$_{50}$     & AP$_{75}$     & AP            & AP$_{50}$     & AP$_{75}$ \\
%         \midrule
%         FCOSv2                      & 38.6      & 57.0          & 40.9          & 39.1          & 57.9          &  42.0      \\
%         ATSS                        & 39.0      & 57.1          & 41.2          & 39.4          & 58.2          &  42.4      \\
%         PAA                         & 40.7      & 58.0          & 43.4          & 40.1          & 58.8          &  43.2     \\
%         {\bf Ours}                  & {\bf 41.5}& {\bf 58.9}    & {\bf 44.7}    & {\bf 39.3}    & 58.0          &  42.3         \\
%         \bottomrule
        
%     \end{tabular}
%     % \caption{{\bf Evaluation on rigid and non-rigid subsets of COCO.}}
%     % \label{tab:my_label}
% \end{table}

\noindent \textbf{More comments on other pose methods.} Our goal is to study the detection problem in 6D pose estimation. Our detection method can be used with most pose regression frameworks as a first component to extract the objectâ€™s bounding box before pose regression. We will discuss pose methods more thoroughly in the final version.

\noindent \textbf{Training complexity.} We discuss the training complexity of different methods at L701-737. Our method is faster in training than the latest PAA and AutoAssign.

\vspace{0.2em}
\noindent \textbf{\textcolor{rvcolor}{To Reviewer h61s}}

\noindent \textbf{Seeds generation.}
We simply generate seeds at the four edges of the virtually extended bounding box of the target, and grow them toward the box center according to the minimum barrier distance. Fig. 5 depicts this procedure.

\noindent \textbf{Object assignment on FPN.}
We use the same assignment strategy for FPN as in FCOSv2, which assigns objects with size [0, 64], [64, 128], [128, 256], [256, 512], and [512, $\infty$] to the five pyramid levels of FPN, respectively.

\vspace{0.2em}
\noindent \textbf{\textcolor{rvcolor}{To Reviewer a3Tv}}

\noindent \textbf{More related work on detection for 6D pose estimation.} Most existing pose methods use a general detector in their first step and do not truly investigate detection itself. We are the first to study the deterioration problem of general methods in detecting rigid targets under occlusions in the context of 6D object pose estimation, and to propose a simple but effective solution for it. We will clarify this.
%We thank the reviewer's suggestion and will add more discussion in revising.

\noindent \textbf{Impact of detection on 6D pose estimation.}
We show the effect of different detection methods on pose estimation in Table 5, and demonstrate that our detection method consistently improves the pose results in different settings.

\noindent \textbf{Typos.} Thank you. We will fix them in the final version.

% %%%%%%%%% REFERENCES
% {
% \footnotesize
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
