
\section{Group equivariance}\label{sec:group_equivariance}

A feature extractor ${\Phi}$ is said to be equivariant to a geometric transformation $T_g$ if transforming an input $x \in X$ by $T_g$ and then passing it through ${\Phi}$ gives the same result as first passing $x$ through ${\Phi}$ and then transforming the resulting feature map by $T'_g$.
Formally, the equivariance can be expressed for transformation group $G$ and ${\Phi}: X \rightarrow Y$ as  
\begin{equation}
    \Phi[ T_g(x) ] = T'_g[ \Phi(x) ], 
\end{equation}
where $T_g$ and $T'_g$ represent transformations on each space of a group action $g \in G$.
If $T_t$ is a translation group $(\mathbb{R}^2, +)$, and $f$ is a feature mapping function $\mathbb{Z}^2 \rightarrow \mathbb{R}^K$ given convolution filter weights $\psi \in \mathbb{R}^{2 \times K}$, the translation equivariance of a convolutional operation can be expressed as follows:
\begin{equation}\label{eq:translation_equivariance}
     [T_t f] * \psi (x)  = [T_t [f * \psi]] (x),
\end{equation}
where $*$ indicates the convolution operation.

Recent studies~\cite{cohen2016group,cohen2019general,cohen2016steerable,weiler2019general,weiler2018learning} propose convolutional neural networks that are equivariant to symmetry groups of translation, rotation, and reflection. 
Let $H$ be a rotation group.
The group $G$ can be defined by $G \cong (\mathbb{R}^2, +) \rtimes H$ as the semidirect product of the translation group $(\mathbb{R}^2, +)$ with the rotation group $H$.  
Then, the rotation-equivariant convolution on group $G$ can be defined as:
\begin{equation}
     [T_g f] * \psi (g)  = [T_g [f * \psi]] (g),
\end{equation}
by replacing $t \in (\mathbb{R}^2,+)$ with $g \in G$ in Eq.~\ref{eq:translation_equivariance}. 
This operation can be applied to an input tensor to produce a translation and rotation-equivariant output.
Extending this, a network equivariant to both translation and rotation can be constructed by stacking translation and rotation-equivariant layers instead of conventional translation-equivariant layers.
Formally, let $\Phi = \{L_i | i \in \{1,2,3,...,M\}\}$, which consists of $M$ rotation-equivariant layers under group $G$. For one layer $L_i \in \Phi$, the transformation $T_g$ is defined as
\begin{equation}
     L_i[T_g(g)] = T_g[L_i(g)],
\end{equation}
which indicates that the output is preserved after $L_i$ about $T_g$. This can be extended to apply $T_g$ to input $I$ and then pass it through the network $\phi$ to preserve the transformation $T_g$ for the whole network.
\begin{equation}
     [\Pi_{i=1}^{M} L_i] (T_g I) = T_g[ \Pi_{i=1}^{M} L_i] (I).
\end{equation}


