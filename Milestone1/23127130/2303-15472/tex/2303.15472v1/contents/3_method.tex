



\section{Rotation-equivariant features, Rotation-invariant descriptors}
\begin{figure*}[t!]
    \centering
    \scalebox{0.85}{
    \includegraphics{figures/fig1_overall_arch.pdf}
    } \vspace{-0.2cm}
    \caption{\textbf{Overview of the proposed pipeline.} 
    An input image is forwarded through the equivariant networks to yield equivariant feature maps from multiple intermediate layers, encoding both low-level geometry and high-level semantic information.
    The feature maps are bilinearly interpolated to have equal spatial dimensions to be concatenated together.
    We use the first channel of the feature map $\textbf{F}$ as the orientation histogram map $\textbf{O}$ to predict the dominant orientations, which are used to shift the group-equivariant representation along the group dimension to yield discriminative rotation-invariant descriptors. 
    To learn to extract accurate dominant orientation $\hat{\theta}$, we use the orientation alignment loss $\mathcal{L}^{\mathrm{ori}}$.
    To obtain descriptors robust to illumination and geometric changes, we use a contrastive descriptor loss $\mathcal{L}^{\mathrm{desc}}$ using the ground-truth homography $\mathcal{H}_{\mathrm{GT}}$.
    }
    \label{fig:overall_arch} %
\end{figure*} 

\noindent

In this section, we first draw the line between the terms \textit{feature} and \textit{descriptor} which will be used throughout this paper.
The goal of our work is to learn to extract rotation-equivariant local \textit{features} from our rotation-equivariant backbone network, and then to align them by their dominant orientation to finally yield rotation-invariant \textit{descriptors}.
In the subsequent subsections, we elaborate on the process of rotation-equivariant feature extraction from steerable  CNNs (Sec.~\ref{sec:roteq_feat_extraction}), assignment of equivariant features to keypoints (Sec.~\ref{sec:keypoint_descriptor_assignment}), how group-aligning is performed to yield rotation-invariant yet discriminative descriptors (Sec.~\ref{sec:group_alignment}), how we formulate our orientation alignment loss (Sec.\ref{sec:orientation_loss}) and contrastive descriptor loss (Sec.\ref{sec:contrastive_loss}) to train our network to extract descriptors which are robust to not only rotation but also other imaging transformations, and finally how we obtain scale-invariant descriptors at test time using image pyramids (Sec.\ref{sec:scale_invariance}).
Figure~\ref{fig:overall_arch} shows the overall architecture of our method.








\subsection{Rotation-equivariant feature extraction}
\label{sec:roteq_feat_extraction}
As the feature extractor, we use ReResNet18~\cite{han2021redet}, which has the same structure as ResNet18~\cite{he2016deep} but is constructed using rotation-equivariant convolutional layers~\cite{weiler2019general}. 
The layer acts on a cyclic group $G_N$ and is equivariant for all translations and $N$ discrete rotations. 
At the first layer, the scalar field of the input image is lifted to the vector field of the group representation~\cite{weiler2019general}.
We leverage feature pyramids from the intermediate layers of the ReResNet18 backbone to construct output features as follows:
\begin{equation}
   \textbf{F} = \bigoplus_{i \in l} \eta(\textbf{f}_i), \ \ \ \textbf{f}_i = [\Pi_{j=1}^{i} L_j](I) , 
\end{equation}
where $\textbf{f}_i \in \mathbb{R}^{C_i\times |G|\times H_i\times W_i}$ is an intermediate feature from $L_i$, $L_i$ is the $i$-th layer of the equivariant network, $\eta$ denotes bilinear interpolation to $H \times W$, and $\bigoplus$ denotes concatenation along the $C$ dimension. 
We utilize the multi-layer feature maps to exploit the low-level geometry information and high-level semantics in the local descriptors~\cite{hariharan2015hypercolumns, min2019hyperpixel, kim2022transformatcher}.
The output features $\textbf{F} \in \mathbb{R}^{C\times |G|\times H\times W} $ contains  rotation-equivariant features with multiple layers containing different semantics and receptive fields. We set $H=H_1$ and $W=W_1$, which are $\frac{1}{2}$ of the input image size.









\subsection{Assigning local features to keypoints}
\label{sec:keypoint_descriptor_assignment}
During training, we extract $K$ keypoints from the source image using Harris corner detection~\cite{harris1988combined}.
We then use the ground-truth homography $\mathcal{H}_{\mathrm{GT}}$ to obtain ground-truth keypoint correspondences.
Also, we allocate a local feature $\textbf{p}\in \mathbb{R}^{C\times |G|\times K}$ to each keypoint,  using the interpolated location of the equivariant feature map $\textbf{F}$.
We experiment our descriptor with SIFT~\cite{lowe2004distinctive}, LF-Net~\cite{ono2018lf}, SuperPoint~\cite{detone2018superpoint}, and KeyNet~\cite{laguna2022key} as the keypoint detector during inference time.



\subsection{Group aligning for invariant mapping}
\label{sec:group_alignment}
To transform the rotation-equivariant feature to a rotation-invariant descriptor, we propose group aligning, an operation to shift the group-equivariant feature in the $G$-dimension using the dominant orientation $\hat{\theta}$.
Unlike existing methods that use group pooling, \textit{e.g.,} average pooling or max pooling, which collapses the group dimension, group aligning preserves the rich group information.
Figure~\ref{fig:group_aligning} illustrates the difference between group pooling and group aligning on an equivariant representation. 


\noindent
\textbf{Estimating the dominant orientation and the shifting value.}
We obtain the orientation histogram map $\textbf{O} \in \mathbb{R}^{|G|\times H\times W} =\textbf{F}_0$ by selecting the first channel of the rotation-equivariant tensor $\textbf{F}$ as an orientation histogram map. 
Note that the first channels of each group action are simultaneously used as the channels of the descriptors and to construct the orientation histogram.
The histogram-based representation of $\textbf{O}$ provides richer information than directly regressing the dominant orientation, as the orientation histogram enables predicting multiple (\textit{i.e.,} top-$k$) candidates as the dominant orientation.
We first select an orientation vector $\textbf{o} \in \mathbb{R}^{|G|}$ of a keypoint from the orientation histogram map $\textbf{O}$ using the coordinates of the keypoint.
Next, we estimate the dominant orientation value $\hat{\theta}$ from the orientation vector $\textbf{o}$ by selecting the index of the maximum score, 
$\hat{\theta} = \frac{360}{|G|} \argmax_g{\textbf{o}} $. 
Using the dominant orientation value $\hat{\theta}$, we obtain the shifting value $\hat{\Delta}=\frac{|G|}{360}\hat{\theta}$ in $G$-dim.  
At training time, we use the ground-truth rotation $\theta_{\mathrm{GT}}$ instead of the predicted dominant orientation value $\hat{\theta}$ to generate the shifting value $\Delta_{\mathrm{GT}}$.





\noindent
\textbf{Group aligning.}
Given a keypoint-allocated feature tensor $\textbf{p} \in \mathbb{R}^{C\times |G|}$ from the equivariant representation $\textbf{F}$, we obtain the rotation-invariant local descriptor $\textbf{d} \in \mathbb{R}^{C|G|}$ by group aligning using $\Delta$. 
After computing the dominant orientation $\hat{\theta}$ and the shifting value $\hat{\Delta}$ from $\textbf{o}$, we obtain the orientation-normalized descriptor $\textbf{d}' \in \mathbb{R}^{C|G|}$ by shifting $\textbf{p}$ in the $G$-dimension by $-\hat{\Delta}$ and flattening the descriptor to a vector. We use cyclic shifting in consideration of the cyclic property of rotation. We finally obtain the L2-normalized descriptor $\textbf{d}$ from the orientation-normalized descriptor $\textbf{d}'$, such that $||\textbf{d}||^{2}=1$.
Formally, this process can be defined as:
\begin{equation}
\begin{aligned}
    \textbf{p}'_{:,i}=T'_r(\textbf{p}_{:,i},\hat{\Delta})=\textbf{p}_{:,(i+\hat{\Delta}) \ \text{mod} \ |G|}, \\
    \textbf{d}'_{|G|i:|G|(i+1)}=\textbf{p}'_{i},  \\ 
    \textbf{d} = \frac{\textbf{d}'}{||\textbf{d}'||_{2}}, 
\end{aligned}
\end{equation}
where $T'_r$ is shifting operator in vector space, and $\textbf{p}'$ is a group-aligned descriptor before flattening.
This shifting by $\hat{\Delta}$ aligns all the descriptors in the direction of their dominant orientations, creating orientation-normalized descriptors.
This process is conceptually similar to subtracting the dominant orientation value of the orientation histogram in the classical descriptor SIFT~\cite{lowe2004distinctive}, but we apply this concept to the equivariant neural features.
The proposed group aligning preserves the group information, so our invariant descriptors have more representative power than the existing group-pooled descriptors which collapse the group dimension for invariance.

\begin{figure}[!t]
    \centering
    \scalebox{1.25}{
    \includegraphics{figures/fig2_group_aligning.pdf}
    } \vspace{-0.3cm}
    \caption{\textbf{Difference between group pooling and group aligning.}
    In group pooling, the group dimension is collapsed to yield an invariant descriptor ($\mathbb{R}^{C\times |G|} \rightarrow \mathbb{R}^{C}$).
    In group aligning, the entire feature is cyclically shifted in the group dimension to obtain an invariant descriptor ($\mathbb{R}^{C\times |G|} \rightarrow \mathbb{R}^{C|G|}$) while preserving the group information and discriminability. 
    }
    \label{fig:group_aligning} %
\end{figure}



\begin{figure}[!t]
    \centering
    \scalebox{1.25}{
    \includegraphics{figures/fig3_orientation_loss.pdf}
    } \vspace{-0.45cm}
    \caption{\textbf{Illustration of orientation alignment loss.} 
    Given two rotation-equivariant tensors $\textbf{p}^{\mathrm{A}}, \textbf{p}^{\mathrm{B}} \in \mathbb{R}^{C \times |G|}$ obtained from two different rotated versions of the same image, we apply cyclic shift on one of the descriptors in the group dimension using the GT difference in rotation.
    The orientation alignment loss supervises the output orientation vectors of the two descriptors to be the same.
    }
    \label{fig:orientation_loss} %
\end{figure}

\subsection{Orientation alignment loss}\label{sec:orientation_loss}

To learn to obtain the dominant orientations from the orientation vectors, we use an orientation alignment loss~\cite{lee2021self, lee2022self, yan2022learning} to supervise the orientation histograms in $\textbf{O}$ to be rotation equivariant under the photometric/geometric transformations. 
Figure~\ref{fig:orientation_loss} shows the illustration of orientation alignment loss.
The cyclic shift of an orientation histogram map at the training time is formulated as follows:
\begin{equation}
        T'_r(\textbf{O}_i, \Delta_{\mathrm{GT}}) = \textbf{O}_{(i+\Delta_{\mathrm{GT}}) \ \text{mod} \ |G|},    
\end{equation}
where $\Delta_{\mathrm{GT}}= \frac{|G|}{360} \theta_{\mathrm{GT}}$ is the shifting value calculated from the ground-truth rotation $\theta_{\mathrm{GT}}$.
We formulate the orientation alignment loss in the form of a cross-entropy as follows:
\begin{equation}\label{eq:orientation_loss}
\begin{aligned}
    \mathcal{L}^{\mathrm{ori}} & (\textbf{O}^{\mathrm{A}}, \textbf{O}^{\mathrm{B}}, \Delta_{\mathrm{GT}}) = \\
    &     - \sum_{k \in K} \sum_{g \in G} \sigma( \textbf{O}^\mathrm{A}_{g, k}) \log( \sigma(    T'_r(\textbf{O}^{\mathrm{B}}_{g,k}, \Delta_{\mathrm{GT}})  )), 
\end{aligned}
\end{equation}
where $\textbf{O}^{\mathrm{A}}$ is the source orientation histogram map and $\textbf{O}^{\mathrm{B}}$ is the target orientation histogram map obtained from a synthetically warped source image, $\sigma$ is a softmax function applied to the $G$-dimension of the orientation histogram map to represent the orientation vector as a probability distribution for the cross-entropy loss to be applicable.
Using Equation~\ref{eq:orientation_loss}, the network learns to predict the characteristic orientations robustly against different imaging variations, such as photometric transformations and geometric transformations beyond rotation, as these transformations cannot be handled by equivariance to discrete rotations alone.
Note that it is not straightforward to define the characteristic orientation of a keypoint to provide strong supervision.
However, we facilitate the learning of characteristic orientations by formulating it as a self-supervised learning framework, leveraging the known relative orientation between two keypoint orientation histogram maps obtained from differently rotated versions of the same image.




\subsection{Contrastive descriptor loss}
\label{sec:contrastive_loss}
We propose to use a descriptor similarity loss motivated by contrastive learning~\cite{chen2020simple} to further empower the descriptors to be robust against variations apart from rotation, \textit{e.g.,} illumination or viewpoint.
The descriptor loss is formulated in a contrastive manner as follows:
\begin{equation}
\begin{aligned}
    \mathcal{L}^{\mathrm{desc}} & (\textbf{D}^{\mathrm{A}}, \textbf{D}^{\mathrm{B}})= \\
    &    \sum_{(\textbf{d}^{\mathrm{A}}_i,\textbf{d}^{\mathrm{B}}_i) \in (\textbf{D}^{\mathrm{A}}, \textbf{D}^{\mathrm{B}}) } -\log \frac{\text{exp}(\text{sim}(\textbf{d}^\mathrm{A}_i, \textbf{d}^\mathrm{B}_i) / \tau)}{ \sum_{k \in K \setminus i} \text{exp}(\text{sim}(\textbf{d}^\mathrm{A}_i, \textbf{d}^\mathrm{B}_k)) / \tau) },
\end{aligned}
\end{equation}
where \text{sim} is cosine similarity and $\tau$ is the softmax temperature.
Unlike the triplet loss with one hard negative sample, the contrastive loss can optimize the distance for all negative pairs. 
This contrastive loss with InfoNCE~\cite{oord2018representation} maximizes the mutual information between the encoded features and effectively reduces the low-level noise.
 Our overall self-supervised loss is formulated as $\mathcal{L} = \alpha \mathcal{L}^{\mathrm{ori}} + \mathcal{L}^{\mathrm{desc}}$, 
where $\alpha$ is a balancing term.


\subsection{Scale robustness}%
\label{sec:scale_invariance}
While we employ a rotation-equivariant network, it does not ensure that the descriptors are robust to scale changes.
Thus, at inference time, we construct an image pyramid using a scale factor of $2^{1/4}$ from a maximum of 1,024 pixels to a minimum of 256 pixels as in R2D2~\cite{revaud2019r2d2}.
After constructing the scale-wise descriptors $\in \mathbb{R}^{S\times C|G|\times K}$ with $S$ varying scales, we finally generate the scale-invariant local descriptors $\in \mathbb{R}^{C|G|\times K}$ by max-pooling in the scale dimension inspired by scale-space maxima as in SIFT~\cite{lowe2004distinctive, mikolajczyk2004scale}, for improved robustness to scale changes.

