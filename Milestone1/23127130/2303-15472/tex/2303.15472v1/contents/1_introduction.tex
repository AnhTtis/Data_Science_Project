


\section{Introduction}

Extracting local descriptors is an essential step for visual correspondence across images, which is used for a wide range of computer vision problems such as visual localization~\cite{sattler2018benchmarking,sattler2012improving,lynen2020large}, simultaneous localization and mapping~\cite{detone2017toward,detone2018superpoint,mur2015orb}, and 3D reconstruction~\cite{agarwal2011building,jared2015reconstructing,schonberger2016structure,jin2021image,zhu2018very}.
To establish reliable visual correspondences, the properties of invariance and discriminativeness are required for local descriptors; the descriptors need to be invariant to geometric/photometric variations of images while being discriminative enough to distinguish true matches from false ones.     
Since the remarkable success of deep learning for visual recognition, deep neural networks have also been adopted to learn local descriptors, showing enhanced performances on visual correspondence~\cite{yi2016lift,revaud2019r2d2,revaud2022pump}.
Learning {\em rotation}-invariant local descriptors, however, remains challenging; the classical techiniques~\cite{lowe2004distinctive,rublee2011orb, fan2011rotationally} for rotation-invariant descriptors, which are used for shallow gradient-based feature maps, cannot be applied to feature maps from standard deep neural networks, in which rotation of input induces unpredictable feature variations.
Achieving rotation invariance without sacrificing disriminativeness is particularly important for local descriptors as rotation is one of the most frequent imaging variations in reality.



In this work, we propose a self-supervised approach to obtain rotation-invariant and discriminative local descriptors by leveraging rotation-equivariant CNNs. 
First, we use group-equivariant CNNs~\cite{weiler2019general} to jointly extract rotation-equivariant local features and their orientations from an image.
To extract reliable orientations, we use an orientation alignment loss~\cite{lee2021self, lee2022self, yan2022learning}, which trains the network to predict the dominant orientation robustly against other imaging variations, including illumination or viewpoint changes.
Using group-equivariant CNNs enables the local features to be empowered with explicitly encoded rotation equivariance without having to perform rigorous data augmentations~\cite{weiler2019general, wang2022data}.
Second, to obtain discriminative rotation-invariant descriptors from rotation-equivariant features, we propose group-aligning that \textit{shifts} the group-equivariant features by their dominant orientation along their group dimension.
Conventional methods to yield invariant features from group-equivariant features collapse the group dimension by group-pooling, \textit{e.g.,} max-pooling or bilinear-pooling~\cite{liu2019gift}, resulting in a drop in feature discriminability and quality.
In contrast, our group-aligning preserves the group dimension, achieving rotation-invariance while eschewing loss of discriminability. 
Furthermore, by preserving the group dimension, we can obtain multiple descriptors by performing group-aligning using multiple orientation candidates, which improves the matching performance by compensating for potential errors in dominant orientation prediction.
Finally, we evaluate our rotation-invariant descriptors against existing local descriptors, and our group-aligning scheme against group-pooling methods on various image matching benchmarks to demonstrate the efficacy of our method.




The contribution of our paper is fourfold:
\begin{itemize}
\item[$\bullet$] We propose to extract discriminative rotation-invariant local descriptors to tackle the task of visual correspondence by utilizing rotation-equivariant CNNs.%
\item[$\bullet$] We propose group-aligning, a method to shift a group-equivariant descriptor in the group dimension by its dominant orientation to obtain a rotation-invariant descriptor without having to collapse the group information to preserve feature discriminability.
\item[$\bullet$] We use self-supervisory losses of orientation alignment loss for orientation estimation, and a contrastive descriptor loss for robust local descriptor extraction.
\item[$\bullet$] We demonstrate state-of-the-art performances under varying rotations on the Roto-360 dataset and show competitive transferability on the HPatches dataset~\cite{balntas2017hpatches} and the MVS dataset~\cite{strecha2008benchmarking}.
\end{itemize}















