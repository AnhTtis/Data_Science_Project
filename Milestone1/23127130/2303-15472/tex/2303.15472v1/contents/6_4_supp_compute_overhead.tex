

\section{Computational overhead and the number of parameters} \label{sec:compute_overhead_the_number_params}
\subsection{Computational overhead}
\begin{table}[h]
\centering
\scalebox{1.0}{
\begin{tabular}{c|cc}
  method        & speed (ms)  & GPU usage (GB) \\ \hline
ours                 & \underline{147.4} & 5.21 GB \\
ours$\dagger$ & 206.4 & 4.83 GB \\
SuperPoint~\cite{detone2018superpoint}           & \textbf{66.0}    & \textbf{2.35 GB} \\
GIFT~\cite{liu2019gift}                 & 198.8 & \underline{2.93 GB} \\
LISRD~\cite{pautrat2020online}                & 781.0   & 2.85 GB \\
PosFeat~\cite{li2022decoupling}              & 208.8 & 4.67 GB    
\end{tabular}} \vspace{-0.2cm}
\caption{\textbf{Comparison of computational overhead.} We compare inference time (milliseconds) and GPU memory usage (gigabytes) while fixing the number of keypoints.}
\label{tab:computational_overhead}
\end{table}
Table~\ref{tab:computational_overhead} compares an average of the inference time and GPU usage with other descriptor extraction methods above. 
While achieving strong rotational invariance,  speed and GPU usage of ours are similar to those of existing local descriptor methods.
Note that, our group aligning has a time complexity of $O(1)$ on the GPU with the predicted orientation because it is a transposition operation and does not take up extra memory. 
In addition, the time complexity of our group-equivaraint feature extractor is the same to the conventional CNNs on GPU since the steerable CNNs multiply the basis kernels and the learnable parameters in test time. (Section 2.8 of~\cite{weiler2019general}) 






\subsection{The number of parameters}\label{sec:number_of_parameters}
\begin{wraptable}{r}{0.19\textwidth} \vspace{-0.25cm}
\scalebox{0.9}{
\hspace{-0.8cm}
\begin{tabular}{c|c}
method & \# params \\ \hline
ours   & \underline{0.6M}    \\
ours$\dagger$  & 2.6M    \\
GIFT~\cite{liu2019gift}   & \textbf{0.4M}    \\
LISRD~\cite{pautrat2020online}  & 3.7M    \\ 
PosFeat~\cite{li2022decoupling} & 21.1M \\
HardNet~\cite{mishchuk2017working} & 9.0M \\ %
HyNet~\cite{hynet2020} & 1.3M \\ \hline
SuperPoint~\cite{detone2018superpoint}     & \underline{1.3M}    \\
LF-Net~\cite{ono2018lf} & 2.6M    \\
RF-Net~\cite{shen2019rf} & 1.4M   \\
D2-Net~\cite{dusmanu2019d2} & 7.6M \\
R2D2~\cite{revaud2019r2d2} & \textbf{0.5M}  
\end{tabular} 
} \vspace{-0.2cm}
\hspace{-0.8cm}
\end{wraptable}
The right table shows the number of parameters in millions, where the first group (top) are descriptor-only models and the second group (bottom) are joint detection and description models. 
Our model in the first row has a second smallest model size among those of descriptor-only models.
When using our model with the deeper backbone denoted denoted by ours$\dagger$, the number of model parameters increases, but it does not increase significantly compared to other comparison groups, where is still similar to that of LF-Net~\cite{ono2018lf}.


