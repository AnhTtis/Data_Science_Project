
\section{Additional qualitative results}\label{sec:additional_qualitative}

\subsection{Visualization of the consistency of orientation estimation}
We provide more examples for Figure 5 of the main paper, which visualize the consistency of orientation estimation. Additionally, we show the similarity map \textit{w.r.t.} a keypoint under varying rotations.
To visualize Figure~\ref{fig:supp_rotation_acc}, we create a sequence of $480\times640$ images augmented by random in-plane rotation with Gaussian noise sourced by ILSVRC2012~\cite{russakovsky2015imagenet}.
Figure~\ref{fig:supp_rotation_acc} shows the qualitative comparison of the estimated orientation consistency.
Given the dominant orientations estimated from the image pair, we calculate the relative angle between the corresponding keypoint orientations and measure the difference between the relative angle and the ground-truth rotation. 
We evaluate the relative angle to be correct \textit{i.e.,} the dominant orientation estimation is consistent if the difference with the ground-truth rotation is within a $30\degree$ threshold.
Our rotation-equivariant model trained with the orientation alignment loss inspired by~\cite{lee2021self, lee2022self} consistently estimates more correct keypoint orientations than LF-Net~\cite{ono2018lf} and RF-Net~\cite{shen2019rf}.


\subsection{Visualization of the similarity maps of a keypoint under varying rotations}
Figure~\ref{fig:supp_rotation_heatmap} shows the similarity maps with respect to a keypoint under varying rotations of images with a resolution of $180\times180$, with uniform rotation intervals of $45\degree$.
We compare one descriptor of a red keypoint from the source image at 0$\degree$ to all other descriptors extracted across the rotated image using cosine similarity to compute the similarity maps.
Yellow circles in the rotated images show the correct locations of the keypoint correspondences.
We visualize 5 locations with the highest similarity scores with the query keypoint for better visibility.
Our descriptor localizes the correct keypoint locations more precisely compared to GIFT~\cite{liu2019gift} and LF-Net~\cite{ono2018lf}. 
Specifically, although GIFT~\cite{liu2019gift} uses group-equivariant features constructed using rotation augmentation, their descriptor fails to locate the corresponding keypoints accurately in rotated images - which shows that the explicit rotation-equivariant networks~\cite{weiler2019general} yield better rotation-invariant features than constructing the group-equivariance features with image augmentation~\cite{liu2019gift}.

\subsection{Visualization of the predicted matches on the extreme rotation}
Figure~\ref{fig:supp_match_er} visualize the predicted matches on the ER dataset~\cite{liu2019gift}. 
We extract a maximum of 1,500 keypoints from each image and find matches using the mutual nearest neighbor algorithm. 
The results show that our method consistently finds matches more accurately compared to GIFT~\cite{liu2019gift} and LF-Net~\cite{ono2018lf}. 

\subsection{Visualization of the predicted matches on the HPatches viewpoint}
Figure~\ref{fig:supp_match_hv} visualize the predicted matches on the HPatches~\cite{balntas2017hpatches} viewpoint variations
We extract a maximum of 1,500 keypoints from each image and find matches using the mutual nearest neighbor algorithm. 
The results show that our method consistently finds matches more accurately compared to GIFT~\cite{liu2019gift} and LF-Net~\cite{ono2018lf}. 


\begin{figure*}[h!]
    \centering
    \scalebox{0.7}{
    \includegraphics{figures/supp_5_rdnim360_examples.pdf}
    } \vspace{-0.1cm}
    \caption{\textbf{Example of ERDNIM image pairs augmented from~\cite{pautrat2020online,zhou2016evaluating}.} 
    The left two columns show the day reference benchmark with day-day and day-night image pairs. 
    The right two columns show the night reference benchmark with night-day and night-night image pairs. 
    The reference image of a pair is augmented with random rotation in the range $[0\degree, 360\degree)$, and the target image is augmented by homographies generated with random translation, rotation, scale, perspective distortion.
    The regions with black artifacts by homographies are masked out to measure the correctness of matching.}
    \label{fig:sample_of_rdnim360} %
\end{figure*} %





\begin{figure*}[t]
    \begin{center}
    \scalebox{0.42}{
    \includegraphics{figures/supp_1_rotation_acc.pdf}
    } \vspace{-0.1cm}
    \caption{\textbf{Visualization of consistency of dominant orientation estimation.} 
    We extract the source keypoints using SuperPoint~\cite{detone2018superpoint} and obtain the target keypoints using GT homography. 
    We evaluate the consistency of orientation estimation by comparing the relative angle difference and the ground-truth angle at a threshold of $30\degree$.
    The green and red arrows represent consistent and inconsistent orientation estimations, respectively.
    We spatially align the target images and its' orientations to the source images for better visibility.
    Our method predicts more consistent orientations of keypoints compared to LF-Net~\cite{ono2018lf} and RF-Net~\cite{shen2019rf}.
    }
    \label{fig:supp_rotation_acc} 
    \end{center} %
\end{figure*}

\begin{figure*}[t]
    \begin{center}
    \scalebox{0.4}{
    \includegraphics{figures/supp_2_rotation_heatmap.pdf}
    } \vspace{-0.3cm}
    \caption{\textbf{Similarity maps with respect to a keypoint under rotation.}
    We compare one descriptor about the red keypoint from the source image at 0$\degree$ to all other descriptors extracted across the rotated images, with yellow circles representing corresponding keypoints.
    For better visibility, we visualize the top 5 pixels with the highest similarity to the keypoints.
    }
    \label{fig:supp_rotation_heatmap} 
    \end{center} %
\end{figure*}

\begin{figure*}[t]
    \begin{center}
    \scalebox{0.4}{
    \includegraphics{figures/supp_3_match_er.pdf}
    } \vspace{-0.3cm}
    \caption{\textbf{Visualization of predicted matches in the ER dataset~\cite{liu2019gift}.} We use a maximum of 1,500 keypoints for matching by the mutual nearest neighbor algorithm. 
    We measure the correctness at a three-pixel threshold. 
    The green lines denote the correct matches, and the red lines denote the incorrect matches.
    }
    \label{fig:supp_match_er} 
    \end{center} %
\end{figure*}

\begin{figure*}[t]
    \begin{center}
    \scalebox{0.4}{
    \includegraphics{figures/supp_4_match_hv.pdf}
    } \vspace{-0.3cm}
    \caption{\textbf{Visualization of the predicted matches in HPatches viewpoint variations.} We use a maximum of 1,500 keypoints, the mutual nearest neighbor matcher, and a three-pixel threshold for correctness. 
    In this experiment, we use the rotation-equivariant WideResNet16-8 (ReWRN) backbone, which is `ours$\dagger$' in table 4 of the main paper.}
    \label{fig:supp_match_hv} 
    \end{center} %
\end{figure*}

