

\section{Related work}

\noindent
\textbf{Classical invariant local descriptors.}
Classical methods to extract invariant local descriptors first aggregate image gradients to obtain a rotation-equivariant representation, \textit{i.e.}, histogram, from which the estimated dominant orientation is subtracted to obtain rotation-invariant features~\cite{lowe2004distinctive,rublee2011orb}. Several studies~\cite{fan2011rotationally,wang2015exploring,bellavia2017rethinking} suggest extracting local descriptors by invariant mapping of the order-based gradient histogram of a patch.
However, these classical methods for shallow gradient-based feature maps cannot be applied to deep feature maps from standard neural networks, in which rotation induces unpredictable feature variations.
Therefore, we propose a deep end-to-end pipeline to obtain orientation-normalized local descriptors by utilizing rotation-equivariant CNNs~\cite{weiler2019general} with additional losses.

\noindent
\textbf{Learning-based invariant local descriptors.}
A branch of learning-based methods learns to obtain invariant local descriptors in an explicit manner.
GIFT~\cite{liu2019gift} constructs group-equivariant features by rotating or rescaling the images, and then collapses the group dimension using bilinear pooling to obtain invariant local descriptors.  
However, their groups are limited to non-cyclic discrete rotations ranging from $-90\degree$ to $90\degree$.
Furthermore, their reliance on data augmentation implies a lower sampling efficiency compared to group-equivariant networks.
LISRD~\cite{pautrat2020online} jointly learns meta descriptors with different levels of regional variations and selects the most appropriate level of invariance given the context.
Another branch of learning methods aims to learn the invariance implicitly using descriptor similarity losses from the image pair using camera pose or homography supervision.
These methods are either patch-based~\cite{ebel2019beyond,mishchuk2017working,tian2020hynet,tian2019sosnet} or image-based~\cite{detone2018superpoint,mishkin2018repeatability,revaud2019r2d2,tyszkiewicz2020disk, lee2021learning,luo2020aslfeat,ono2018lf,shen2019rf,dusmanu2019d2,li2022decoupling}.
While these methods may be robust to rotation, they cannot be said to be equivariant or invariant to rotation.
We construct group-equivariant local features using the steerable networks~\cite{weiler2019general}, which explicitly encodes cyclic rotational equivariance to the features without having to rely on data augmentation.
We can then yield rotation-invariant features by group-aligning that shifts the group-equivariant features along the group dimension by their dominant orientations, preserving feature discriminability.




\noindent
\textbf{Equivariant representation learning.}
There has been a constant pursuit to learn equivariant representations by explicitly incorporating group equivariance into the model architecture design~\cite{memisevic2012multi,memisevic2010learning,sohn2012learning,marcos2017rotation,zhou2017oriented, weiler2019general}.
For example, G-CNNs~\cite{cohen2016group} use group equivariant convolutions that reduce sample complexity by exploiting symmetries on discrete isometric groups; SFCNNs~\cite{weiler2018learning} and H-Nets~\cite{worrall2017harmonic} extract features from more diverse groups and continuous domains by using harmonics as filters.
There are also studies that focus on scale-equivariant representation learning~\cite{sosnovik2021transform, lee2021self, barroso2022scalenet}.
\cite{han2021redet, pielawski2020comir, lee2022self, moyer2021equivariant, kim2022selca} leverage equivariant neural networks to tackle vision tasks \textit{e.g.,} keypoint detection.
In this work, we also propose to use equivariant neural networks to facilitate the learning of discriminative rotation-invariant descriptors.
We guide the readers to section 1 of the supplementary material for a brief introduction to group equivariance.
