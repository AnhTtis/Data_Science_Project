

\begin{table}[!t]
    \begin{center}
        \begin{tabular}{c|c|c}
            & MMA   & \multirow{2}{*}{pred.} \\ \cline{2-2}
         & @1px   &                        \\ \hline
        Align                & \textbf{97.54} & \textbf{84.90}                   \\
        Avg                  & 33.72 & 33.72                  \\
        Max                  & 57.92 & 57.92                  \\
        None                 & 23.97 & 23.97                  \\
        Bilinear             & 43.60  & 26.42                 
        \end{tabular}
    \end{center} \vspace{-0.6cm}
    \caption{\label{tab:compare_pool_with_gt_keypoint_pair}\textbf{Evaluation with GT keypoint pairs on Roto-360 without training.} 
    `Align' uses GT rotation difference to apply group-aligning to demonstrate the upper-bound.
    `None' does not use pooling nor aligning, demonstrating the lower-bound.
    We use an average of 111 keypoint pairs extracted using SuperPoint~\cite{detone2018superpoint}.
    } %
\end{table}

\section{Experiment}

\noindent \textbf{Implementation details.}
We use rotation-equivariant ResNet-18 (ReResNet-18)~\cite{han2021redet} implemented using the rotation-equivariant layers of $E(2)$-CNN~\cite{weiler2019general} as our backbone. 
We remove the first maxpool layer to preserve the spatial size, so that the spatial resolution of the rotation-equivariant feature $\textbf{F}$ is $H=\frac{H'}{2}$ and $W=\frac{W'}{2}$, where $H'$ and $W'$ are the height and width of an input image.
We use 16 for the order of cyclic group $G$. 
We use a batch size of 8, a learning rate of $10^{-4}$, and a weight decay of 0.1.
We train our model for 12 epochs with 1,000 iterations using a machine with an Intel i7-8700 CPU and an NVIDIA GeForce RTX 3090 GPU.
We use the temperature $\tau$ of $\mathcal{L}^{\mathrm{desc}}$ as 0.07. 
The loss balancing factor $\alpha$ is 10.
The final output descriptor size is 1,024, with $C=64$, $|G|=16$.
We use SuperPoint~\cite{detone2018superpoint} as the keypoint detector to evaluate our method except Table~\ref{tab:fix_keypoints}.
For all descriptors, we use the mutual nearest neighbour matcher to predict the correspondences.



\begin{table}[!t]
        \begin{center}
            \begin{tabular}{c|ccc|c}
            \multirow{2}{*}{} & \multicolumn{3}{c|}{MMA} & \multirow{2}{*}{pred.} \\ \cline{2-4}
                                 & @10px   & @5px    & @3px   &        \\ \hline
            Align                & \textbf{93.08}  & \textbf{91.35}  & \textbf{90.18} & 688.3 \\
            Avg                  & 85.84  & 82.12  & 81.05 & \textbf{705.9}                         \\
            Max                  & 82.61  & 78.00     & 77.79 & 686.0                           \\
            None                 & 19.68  & 18.81  & 18.57 & 349.1                         \\
            Bilinear         & 42.69  & 41.03  & 40.51 & 332.5                 
            \end{tabular}
        \end{center} \vspace{-0.6cm}
        \caption{\label{tab:compare_pool_wo_keep_points}\textbf{Evaluation with predicted keypoint pairs on Roto-360 with training.}
        `Max' and `Avg' collapses the group dimension of the features through max pooling or average pooling.
        'pred.' denotes the average number of predicted matches. 
        We use an average of 1161 keypoint pairs extracted using SuperPoint~\cite{detone2018superpoint}.
        } %
\end{table}


\subsection{Datasets and metrics}
We use a synthetic training dataset to train our model in a self-supervised manner. 
We evaluate our model on the Roto-360 dataset and show the transferability on real image benchmarks, \textit{i.e.,} HPatches~\cite{balntas2017hpatches} and MVS~\cite{strecha2008benchmarking} datasets. 


\noindent
\textbf{Training dataset.}
We generate a synthetic dataset for self-supervised training from the MS-COCO dataset~\cite{lin2014microsoft}. 
We warp images with random homographies for geometric robustness and transform the colors by jitter, noise, and blur for photometric robustness. 
As we need the ground-truth rotation $\theta_{\mathrm{GT}}$ for our orientation alignment loss, we decompose the synthetic homography $\mathcal{H}$ as follows: $\theta_{\mathrm{GT}} = \arctan(\frac{\mathcal{H}_{21}}{\mathcal{H}_{11}})$, where we assume that a $3 \times 3$ homography matrix $\mathcal{H}$ with no significant tilt can be approximated to an affine matrix.
We sample $K=512$ keypoints for an image using Harris corner detector~\cite{harris1988combined}, obtaining 512 corresponding keypoint pairs for each image pair using homography and rotation.
Note that this dataset generation protocol is the same as that of GIFT~\cite{liu2019gift} for a fair comparison.


\begin{table}[t]
\begin{center} %
\scalebox{0.87}{
\begin{tabular}{c|ccc|cc}
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{MMA} & \multirow{2}{*}{pred.} & \multirow{2}{*}{total.} \\ \cline{2-4}
                        & @10px     & @5px     & @3px    &                        &                         \\ \hline
SIFT~\cite{lowe2004distinctive}                    & 78.86       & 78.59       & 78.23      &  774.1                      &  1500.0                       \\
ORB~\cite{rublee2011orb}                     &   86.78     & 85.29       &  78.73     &  607.6                      &  1005.2                       \\
SuperPoint~\cite{detone2018superpoint}              & 22.85       & 22.10       & 21.83      &  462.6                      &   1161.0                      \\
LF-Net~\cite{ono2018lf}                       & 75.05     & 74.30   & 72.61        &  386.7                      &    1024.0                     \\
RF-Net~\cite{shen2019rf}                     & 15.64     & 15.18   & 14.58         & 1602.5                       &  5000.0                       \\
D2-Net~\cite{dusmanu2019d2}                  & 15.56       & 9.30       & 5.21      &   386.9                     &   1474.5                      \\
R2D2~\cite{revaud2019r2d2}                    & 15.80       & 14.97       & 13.50      &  197.9                      &  1500.0                       \\
GIFT~\cite{liu2019gift}                    & 42.35       & 42.05       & 41.59      &  589.2                      &   1161.0                      \\
LISRD~\cite{pautrat2020online}                                 & 16.96 & 16.04 & 15.64     &  323.6                      &    1781.1                     \\ 
ASLFeat~\cite{luo2020aslfeat}   &  19.34  &  16.38  & 13.13   & 1366.9   & 6764.2        \\  
DISK~\cite{tyszkiewicz2020disk}   & 13.22 &	12.43 &	12.04 &	359.1 & 2048.0 \\ 
PosFeat~\cite{li2022decoupling}   & 13.76 &	11.79 &	9.82 &	717.2 &	7623.5\\ \hline
ours                    & 93.08    & {91.35}       &  {90.18}     &     688.3               &     1161.0                   \\
ours*   & \textbf{94.35}  & \textbf{92.82} & \textbf{91.69} & 1333.0 & 2340.4  
\end{tabular}      
} \vspace{-0.3cm}  %
\caption{\textbf{Comparison to existing local descriptors on Roto-360.} 
We use mutual nearest matching for all methods to establish matches between images.
`total.' and `pred.' denotes the average number of detected keypoints and predicted matches, respectively.
`ours*' denotes selecting multiple candidate descriptors based on the ratio of max value in the orientation histogram. 
We use SuperPoint keypoint detector~\cite{detone2018superpoint} same to the GIFT descriptor~\cite{liu2019gift}.
}\label{tab:compare_baselines} %
\end{center}
\end{table}


\noindent
\textbf{Roto-360} is an evaluation dataset that consists of 360 image pairs with in-plane rotation ranging from $0\degree$ to $350\degree$ at $10\degree$ intervals, created using ten randomly sampled images from HPatches~\cite{balntas2017hpatches}.
Roto-360 is more suitable to evaluate the rotation invariance of our descriptors, as the extreme rotation (ER) dataset~\cite{liu2019gift} only covers $180\degree$, and includes photometric variations.
We use mean matching accuracy (MMA) as the evaluation metric with pixel thresholds of 3/5/10 pixels and the number of predicted matches following~\cite{dusmanu2019d2, mikolajczyk2005performance}.

\noindent
\textbf{HPatches}~\cite{balntas2017hpatches} has 57 scenes with illumination variations and 59 scenes with viewpoint variations. 
Each scene contains five image pairs with ground-truth planar homography. 
We use the same evaluation metrics to Roto-360 to show the transferability of our local descriptors.


\noindent
\textbf{MVS dataset}~\cite{strecha2008benchmarking} has six image sequences of outdoor scenes with GT camera poses. 
We evaluate the relative pose estimation accuracy at $5\degree$/$10\degree$/$20\degree$ angular difference thresholds.








\subsection{Comparison to other invariant mappings}
Table~\ref{tab:compare_pool_with_gt_keypoint_pair} compares group aligning to various group pooling methods on the Roto-360 dataset using ground-truth keypoint pairs, \textit{i.e.,} no keypoint deviation, without training. The purpose is to compare the invariant mapping operations only while keeping the backbone network and the number of keypoints fixed. We use $\Delta_{\mathrm{GT}}$ to shift the equivariant features, and group aligning shows almost perfect keypoint correspondences with 97.54\% matching accuracy. Group pooling, such as max pooling or average pooling, significantly reduces discriminative power compared to group aligning. The results show that group aligning shows the best results, proving that leveraging the full group-equivariant features instead of collapsing the groups shows higher discriminability. Note that the bilinear pooling~\cite{liu2019gift} does not guarantee the rotation-invariant matching.


Table~\ref{tab:compare_pool_wo_keep_points} compares the proposed group aligning to the existing group pooling methods on the Roto-360 dataset, this time with predicted keypoint pairs and with training.
Note that while other methods are trained only with $\mathcal{L}^{\mathrm{desc}}$, our method is trained also with $\mathcal{L}^{\mathrm{ori}}$ to facilitate group aligning.
While the number of predicted matches is the highest for average pooling, the MMA results are significantly higher for group aligning, which shows group-aligned descriptors have a higher precision.
Overall, incorporating group aligning demonstrates the best results in terms of MMA compared to average pooling, max pooling or bilinear pooling~\cite{liu2019gift}. 
Note that pooling or aligning the group-equivariant features to obtain invariant descriptors shows consistent improvements over not pooling nor aligning the group-equivariant features.





\begin{table}[t]
\centering
\scalebox{0.725}{
\begin{tabular}{c|c|ccc|cc}
\multirow{2}{*}{Det.} & \multirow{2}{*}{Desc.} & \multicolumn{3}{c|}{MMA} & \multirow{2}{*}{pred.} & \multirow{2}{*}{total.} \\ \cline{3-5}
                       &          & @10px     & @5px     & @3px    &                        &                         \\ \hline
\multirow{4}{*}{SIFT~\cite{lowe2004distinctive}}     
    & SIFT~\cite{lowe2004distinctive}       & 78.86          & 78.59          & \underline{78.23}          & 774.1 & 1500                           \\
     & GIFT~\cite{liu2019gift}       & 37.97          & 36.82          & 36.09          & 531.2 & 1500                           \\
     & ours       & \underline{84.67}          & \underline{79.85}          & 77.96          & 558.3 & 1500                           \\
        & ours*      & \textbf{84.91} & \textbf{80.09} & \textbf{78.18} & 759.8 & 2219                         \\ \hline
\multirow{4}{*}{LF-Net~\cite{ono2018lf}}     
    & LF-Net~\cite{ono2018lf}     & 75.05          & \textbf{74.30}  & \textbf{72.61} & 386.7 & 1024   \\
     & GIFT~\cite{liu2019gift}       & 35.56          & 33.82          & 32.29          & 426.3 & 1024                           \\
     & ours       & \underline{79.90}           & 71.63          & 67.39          & 431.8 & 1024                           \\
    & ours*      & \textbf{80.32} & \underline{71.99}          & \underline{67.62}          & 591.4 & 1503                         \\ \hline
\multirow{4}{*}{SuperPoint~\cite{detone2018superpoint}}  
    & SuperPoint~\cite{detone2018superpoint} & 22.85          & 22.10           & 21.83          & 462.6 & 1161                           \\
     & GIFT~\cite{liu2019gift}       & 42.35          & 42.05          & 41.59          & 589.2 & 1161                           \\
     & ours       & \underline{93.08}          & \underline{91.35}          & \underline{90.18}          & 688.3 & 1161                           \\
        & ours*      & \textbf{94.35} & \textbf{92.82} & \textbf{91.69} & 1333  & 234                       \\ \hline
\multirow{4}{*}{KeyNet~\cite{laguna2022key}}    
    & HyNet~\cite{hynet2020}      & 24.43          & 22.82          & 20.64          & 288.7 & 995                            \\
     & GIFT~\cite{liu2019gift}       & 34.08          & 32.31          & 29.17          & 275.7 & 995                            \\
     & ours       & \underline{72.95}          & \underline{61.36}          & \underline{41.33}          & 257.2 & 995                            \\
     & ours*      & \textbf{72.48} & \textbf{60.69} & \textbf{40.95} & 356.6 & 1484                        
\end{tabular}} \vspace{-0.3cm}
\caption{
\textbf{Comparison to existing local descriptors when using the same keypoint detector on Roto-360.} Results in bold indicate the best result, and underlined results indicate the second best.
}\label{tab:fix_keypoints}
\end{table}








\subsection{Comparison to existing local descriptors}\label{sec:comparison_baselines}
Table~\ref{tab:compare_baselines} shows the matching accuracy compared to existing local descriptors on the Roto-360 dataset.
We evaluate the descriptors using their own keypoint detectors~\cite{detone2018superpoint,dusmanu2019d2,lowe2004distinctive,mur2015orb,ono2018lf,revaud2019r2d2,shen2019rf}, or combined with off-the-shelf detectors~\cite{liu2019gift,pautrat2020online,li2022decoupling}.
While the classical methods~\cite{lowe2004distinctive,rublee2011orb} achieve better matching accuracy than the existing learning-based methods, our method achieves the best results overall. This is because the learning-based methods learn only a limited degree of invariance in a data-driven manner without guaranteeing full invariance to rotation by design.

Table~\ref{tab:fix_keypoints} shows the performance of our method in comparison to existing local descriptors when using the same keypoint detector, where our method shows consistent performance improvement. 
In particular, our rotation-invariant descriptor shows consistently higher matching accuracy than GIFT~\cite{liu2019gift}, which is a representative learning-based group-invariant descriptor. 
While our model shows a lower MMA than the LF-Net~\cite{ono2018lf} descriptor when using the LF-Net detector at 5px and 3px thresholds, we conjecture that this is due to the better integrity of the detector and descriptor of LF-Net due to their joint training scheme.

These results show that our descriptors obtained using the proposed group aligning show the highest matching accuracy under rotation changes compared to existing methods.
The improvement of our method is also attributed to the usage of rotation-equivariant networks, which have a higher sampling efficiency, \textit{i.e.,} do not require intensive rotation augmentations to learn rotation invariance.





\noindent
\textbf{Multiple descriptor extraction using orientation candidates.}
Group aligning can extract multiple descriptors with different alignments by using multiple orientation candidates, denoted by 'ours*', whose scores are at least 60\% of the maximum score in the orientation histogram. When there is a single keypoint position with $k$ descriptors that are differently aligned, we treat it as if there are $k$ detected keypoints.  Multiple descriptor extraction compensates for incorrect orientation predictions and further enhances matching accuracy. Figure~\ref{fig:top_k_illustration} illustrates an example of multiple descriptor extraction with a score ratio threshold of 0.6.


\begin{figure}[t]
        \centering
        \scalebox{0.25}{
        \includegraphics{figures/rebuttal_orient_candidates.pdf}
        }               
        \vspace{-0.3cm}
        \caption{
        \textbf{An example of multiple descriptor extraction.} 
        The distribution is an orientation histogram $\textbf{o} \in \mathbb{R}^{16}$, and the scores are confidence values for each bin from group-equivariant features. Arrows indicate the orientation candidates for multiple descriptor extraction. The example shows selecting three orientations to obtain three candidate descriptors for a feature point, which is possible as we predict a score for each orientation.
        } \label{fig:top_k_illustration} 
\end{figure}







\noindent
\textbf{Consistency of matching accuracy with respect to rotation changes.}
Figure~\ref{fig:rotation_robustness} illustrates how the matching accuracy changes with respect to varying degrees of rotation.
Our method shows the highest consistency, proving the enhanced invariance of descriptors obtained using group aligning against different rotations.
While MMA of SIFT~\cite{lowe2004distinctive} and ORB~\cite{rublee2011orb} are high at the upright rotations, they tend to fluctuate significantly with varying rotations.
The existing learning-based group-invariant descriptor, GIFT~\cite{liu2019gift}, fails to find correspondences beyond $60\degree$.


\begin{figure}
    \begin{center}
    \scalebox{0.32}{
    \includegraphics{figures/fig4_rotation_robustness.pdf}
    } \vspace{-0.3cm}
    \caption{\textbf{Matching accuracies according to varying degree of rotations on Roto-360.} 
    }
    \label{fig:rotation_robustness} %
    \end{center}
\end{figure}


\subsection{Transferability to real image benchmarks}\label{sec:transferability}

\begin{table}[t]
\centering
\scalebox{0.6}{
\begin{tabular}{c|cc|cc|cc|ccc}
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{HP-all} & \multicolumn{2}{c|}{HP-illu} & \multicolumn{2}{c|}{HP-view}  & \multicolumn{3}{c}{Pose} \\ \cline{2-10}
                        & @5px          & @3px         & @5px          & @3px         & @5px          & @3px                & $20\degree$     & $10\degree$     & $5\degree$      \\ \hline
SIFT~\cite{lowe2004distinctive}                    & 51.36        & 46.32        & 49.08        & 44.62        & 53.57         & 47.96             & 0.02   & 0.00      & 0.00      \\
ORB~\cite{rublee2011orb}                     & 52.22        & 47.40        & 50.85        & 46.29       & 53.55        & 48.47             & 0.06   & 0.00      & 0.00      \\
SuperPoint~\cite{detone2018superpoint}              & 69.71        & 61.75       & 74.63        & 67.53       & 64.96        & 56.17           & 0.20    & 0.07   & 0.01   \\
LF-Net~\cite{ono2018lf}                  & 56.45        & 52.22       & 62.21        & 57.63       & 50.88        & 47.00              & 0.06   & 0.03   & 0.01   \\
RF-Net~\cite{shen2019rf}                  & 59.08        & 54.42       & 61.63        & 57.46       & 56.62        & 51.49           & 0.10    & 0.04   & 0.01   \\
D2-Net~\cite{dusmanu2019d2}                  & 50.18        & 32.54       & 63.80         & 44.09       & 37.02        & 21.38           & 0.11   & 0.05   & 0.01   \\
GIFT~\cite{liu2019gift}                    & \underline{76.03}        & \underline{67.31}       & \textbf{79.71}        & \textbf{71.89}       & 72.48        & 62.88        & \textbf{0.60}    & {0.28}   & 0.09   \\
LISRD~\cite{pautrat2020online}                   & 62.16        & 56.12       & 70.09        & 63.64       & 54.50         & 48.85            & 0.05   & 0.02   & 0.00      \\ \hline
ours$_\textrm{avgpool}$                 & 64.10         & 57.94       & 62.28        & 56.27       & 65.85        & 59.55              & 0.27   & 0.10    & 0.05   \\
ours$_\textrm{maxpool}$                 & 61.57        & 55.81       & 59.66        & 53.91       & 63.42        & 57.64            & 0.27   & 0.11   & 0.03   \\
ours$_\textrm{bilinearpool}$~\cite{liu2019gift} & 45.59        & 41.90        & 45.13        & 41.57       & 46.03        & 42.22          & 0.35   & 0.17   & 0.09   \\
ours$_\textrm{bilinearpool}\dagger$~\cite{liu2019gift}   & 58.72 &	53.77 &	57.32 &	52.67 &	60.06 &	54.83 & 0.24 &	0.11 &	0.03             \\
ours$_\textrm{groupalign}$                    & 70.69        & 63.42       & 70.39        & 62.88       & 70.97        & 63.95           & \underline{0.58}   & 0.26   & \underline{0.12}   \\
ours$_\textrm{groupalign}$*         & 73.92                   & 66.37                   & 73.13        & 65.33       & \underline{74.69}        & \underline{67.38}          & 0.56   & \underline{0.30}    & \underline{0.12}   \\
ours$_\textrm{groupalign}\dagger$         & \textbf{78.00}                      & \textbf{69.70}                    & \underline{77.94}        & \underline{69.35}       & \textbf{78.06}        & \textbf{70.03}          & 0.56  & \textbf{0.33} & \textbf{0.14} 
\end{tabular} 
} \vspace{-0.3cm}
\caption{\textbf{Evaluation with predicted keypoint pairs on real image benchmarks.}
The first group of methods includes existing local feature extraction methods.
The second group of methods includes comparisons to other group pooling methods by replacing our group aligning with them.
`ours*' denotes the extraction of multiple descriptors using the orientation candidates, whose scores are at least 60\% of the maximum score in the orientation histogram.
`ours$\dagger$' denotes our method using the rotation-equivariant WideResNet16-8 (ReWRN) backbone for feature extraction. 
We use SuperPoint~\cite{detone2018superpoint} keypoint detector to evaluate ours.
}\label{tab:transferability} 
\end{table}

Table~\ref{tab:transferability} shows the matching performance of local descriptors on HPatches illumination/viewpoint~\cite{balntas2017hpatches} and pose estimation~\cite{strecha2008benchmarking}.
Our model shows the highest performance overall on the HPatches dataset. 
The performance gain of ours becomes smaller compared to the Roto-360 dataset due to the absence of extreme rotations in HPatches.
While GIFT shows a higher performance under illumination changes that only contain identity mappings, ours$\dagger$, which uses a larger backbone network (ReWRN), improves matching accuracy by 7.15\%p at 3px and 5.58\%p at 5px, and ours* improves by 4.5\%p at 3px,  2.21\%p at 5px under viewpoint changes compared to GIFT~\cite{liu2019gift}.
It should be noted that the core difference between ours$_\textrm{bilinearpool}$ and GIFT is the usage of explicit rotation-equivariant CNNs~\cite{weiler2019general}, which clearly shows that bilinear pooling is not well-compatible with the equivariant CNNs in comparison to group aligning.
Using the same network with bilinear pooling (ours$_\textrm{bilinearpool}\dagger$) proposed in~\cite{liu2019gift} shows significantly lower results compared to ours$_\textrm{groupalign}\dagger$.


In the MVS dataset~\cite{strecha2008benchmarking} to evaluate relative camera pose estimation, our model shows a higher performance than GIFT at finer error thresholds of $10\degree$ and $5\degree$. 
This shows that our model can find more precise correspondences under 3D viewpoint changes.
Overall, these results show that our descriptors using rotation-equivariant representation exhibit strong transferability to the real-world examples. 









\subsection{Ablation study and design choice}
Table~\ref{tab:ablations} shows the results of ablation studies on the HPatches and Roto-360 datasets.
The matching accuracy drops when either the orientation alignment loss or the contrastive descriptor loss is not used.
Specifically, even when using the ground truth rotation difference for group alignment, not using the descriptor loss results in lower performance, highlighting the importance of robustness to other sorts of variations, \textit{e.g.}, illumination or viewpoint.
Not using the image pyramid at inference time results in a slight drop in HPatches, but the performance on Roto-360 remains nearly unchanged.
When training without equivariant layers, ResNet-18 with conventional convolutional layers was used - this results in a drastic drop in performance especially on Roto-360, with a rapid increase in the number of model parameters.
This demonstrates the significance of high sample efficiency of group-equivariant layers.

We also demonstrate the effect of the order of cyclic group $G$ on the performance of our method in the second group of Table~\ref{tab:ablations}.
We fix the computational cost $C \times |G| = 1,024$, and vary the order of group to show the parameter efficiency of the group equivariant networks.
Our design choice $|G|=16$ yields the best results, and the performance drops gracefully as $G$ increases.
This is because with a higher order of groups, the precision of dominant orientation estimation is likely to decrease, leading to lower results.
Reducing the order of group to $|G|=8$ reduces the MMA in both benchmarks as well, which we suspect is because the range of rotation covered by one group action becomes too wide, leading to increased approximation errors.

\begin{table}[t]
\begin{center}
\scalebox{0.75}{
\begin{tabular}{c|cc|cc|c}
                           & \multicolumn{2}{|c|}{HP-all}                  & \multicolumn{2}{c|}{Roto-360}                         & params.   \\
                           & @5px                    & @3px                        & @5px                  & @3px                           &    (millions)     \\ \hline
ours (proposed $|G|=16$)       & \textbf{70.69}                   & \textbf{63.42}                   & \underline{91.35}                & \underline{90.18}                        & 0.62M   \\
w/o orientation loss       & 66.41                   & 58.61                   & 85.29                & 83.26                         & 0.62M   \\
w/o descriptor loss        & 27.49                   & 24.83                   & 25.64                & 24.98                         & 0.62M   \\
w/o image scale pyramid &  \underline{68.77} & \underline{62.25} & \textbf{91.47} & \textbf{90.43}   & 0.62M \\
w/o equivariant backbone  & 47.25                    & 42.52                  & 8.65                     & 8.51                                 & 11.18M  \\  \hline
$|G|=64$                       & 63.96                   & 57.35                   & 85.12                & 83.32                         & \textbf{0.16M}   \\
$|G|=36$                       & 68.17                   & 60.95                   & 87.78                & 85.89                         & 0.26M   \\
$|G|=32$                       & 69.44                   & 62.08                   & 89.10                 & 87.31                         & 0.31M   \\
$|G|=24$                       & 69.72                   & 62.21                   & 90.27                & 88.34                         & 0.39M   \\
$|G|=8$                        & 65.74                   & 58.92                   & 87.16                & 85.57                         & 1.24M 
\end{tabular} 
} \vspace{-0.3cm}
\caption{\textbf{Ablation test on HPatches and Roto-360.} 
`params.' denotes the number of model parameters. 
} \label{tab:ablations}
\end{center}
\end{table}




\subsection{Qualitative results}

\begin{figure}
    \begin{center}
    \scalebox{0.33}{
    \includegraphics{figures/fig6_rotation_acc.pdf}
    } \vspace{-0.3cm}
    \caption{\textbf{Visualization of consistency of dominant orientation estimation.} Best viewed in electronics and colour.
    }
    \label{fig:orientation_direction}
    \end{center}  %
\end{figure}  


Figure~\ref{fig:orientation_direction} visualizes the consistency of dominant orientation estimation.
From the source (left) and target (middle) images, we estimate the dominant orientation for the same set of predicted keypoints.
We use the ground truth rotation to align the estimated orientation and the target image for better visibility (right).
The green and red arrows (middle, right) represent the consistent and inconsistent orientation predictions with respect to the initial estimations (left) at a $30\degree$ threshold.
The numbers on the left represent the number of consistent estimations/number of detected keypoints.
Compared to LF-Net~\cite{ono2018lf} and RF-Net~\cite{shen2019rf}, our method predicts more consistent dominant orientations of keypoints.








