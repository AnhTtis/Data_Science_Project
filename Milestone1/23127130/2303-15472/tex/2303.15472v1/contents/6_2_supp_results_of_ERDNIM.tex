


\section{Experiments in \textit{extreme} rotated day-night image matching (ERDNIM)}\label{sec:additional_results}

To show the robustness of our method under both geometric and illumination changes, we evaluate the matching performance of our method in the {\em extreme} rotated Day-Night Image Matching (ERDNIM) dataset, which rotates the reference images of the RDNIM dataset~\cite{pautrat2020online}, which is originally from the DNIM dataset~\cite{zhou2016evaluating}.
 
 
 \begin{table*}[ht!]
\centering
\scalebox{1.0}{
\begin{tabular}{ccccccccccc}
\hline 
       &             & SIFT & SuperPoint & D2Net & R2D2  & \begin{tabular}[c]{@{}c@{}}KeyNet+\\ HyNet\end{tabular} & GIFT  & LISRD          & ours  & ours*          \\ \hline
\multirow{2}{*}{\textit{Day}}   & HEstimation &              0.064         & 0.073      & 0.001 & 0.044 & 0.085        & 0.108 & 0.228          & \underline{0.232} & \textbf{0.272} \\
       & MMA   & 0.049                 & 0.082      & 0.024 & 0.054 & 0.068        & 0.123 & \underline{0.270}           & 0.245 & \textbf{0.277} \\ \hline
\multirow{2}{*}{\textit{Night}} & HEstimation & 0.108                 & 0.092      & 0.002 & 0.062 & 0.097        & 0.151 & 0.291          & \underline{0.316} & \textbf{0.364} \\
       & MMA   & 0.082                 & 0.111      & 0.033 & 0.076 & 0.093        & 0.177 & 0.358          & \underline{0.362} & \textbf{0.404} \\ \hline 
\end{tabular}  }  \vspace{-0.1cm}
\caption{\textbf{Comparison of matching quality on the ERDNIM dataset.} We use two evaluation metrics: homography estimation accuracy (HEstimation), and mean matching accuracy (MMA) at 3 pixel thresholds. %
Results in \textbf{bold} indicate the best score and \underline{underlined} results indicate the second best scores.
} \label{tab:erdnim_results} %
\end{table*}
\begin{figure*}[ht!]
    \centering
    \scalebox{0.4}{
    \includegraphics{figures/supp_0_rdnim360.png}
    } \vspace{-0.2cm}
    \caption{\textbf{Results of MMA with different pixel thresholds on the ERDNIM dataset.} 'ours*' uses $k$ differently group-aligned features based on top-$k$ selection. We use $k=4$ in this experiment.}
    \label{fig:results_of_rdnim}
\end{figure*}
 
\subsection{Data generation}
The source dataset DNIM~\cite{zhou2016evaluating} consists of 1722 images from 17 sequences of a fixed webcam taking pictures at regular time spans over 48 hours.
They construct the pairs of images to match by choosing a day and a night reference image for each sequence as follows: 
we first select the image with the closest timestamp to noon as the day reference image, and the image with the closest timestamp to midnight as the night reference image.
Next, we pair all the images within a sequence to both the day reference image and the night reference image.
Therefore, 1,722 image pairs are obtained for each of the day benchmark and night benchmark, where the day benchmark is composed of day-day and day-night image pairs, and the night benchmark is composed of night-day and night-night image pairs.
To evaluate the robustness under geometric transformation, the RDNIM~\cite{pautrat2020online} dataset is generated by warping the target image of each pair with homographies as in SuperPoint~\cite{detone2018superpoint} generated with random translations, rotations, scales, and perspective distortions. 
Finally, we add rotation augmentation to the reference image of each pair to evaluate the rotational robustness, and call this dataset {\em extreme} rotated Day-Night Image Matching (ERDNIM). 
We randomly rotate the reference images in the range $[0\degree, 360\degree)$.
The number of image pairs for evaluation remains the same as RDNIM~\cite{pautrat2020online}.
Figure~\ref{fig:sample_of_rdnim360} shows some examples of ERDNIM image pairs.

\subsection{Examples of ERDNIM image pairs}
\subsection{Evaluation metrics}
We use two evaluation metrics, HEstimation and mean matching accuracy (MMA), following LISRD~\cite{pautrat2020online}.
We measure the homography estimation score~\cite{detone2018superpoint} using RANSAC~\cite{fischler1981random} to fit the homography using the predicted matches.
To measure the estimation score, we first warp the four corners of the reference image using the predicted homography, and measure the distance between the warped corners and the corners warped using the ground-truth homography. 
The predicted homography is considered to be correct if the average distance between the four corners is less than a threshold:
HEstimation$=\frac{1}{4}\sum_{i=1}^{4}||\hat{c}_i-c_i||_{2} \leq \epsilon$, where we use $\epsilon=3$.
MMA~\cite{dusmanu2019d2,revaud2019r2d2} is the percentage of the correct matches over all the predicted matches, where we also use 3 pixels as the threshold to determine the correctness of matches.




\subsection{Results}
Table~\ref{tab:erdnim_results} shows the evaluation results on the ERDNIM dataset. 
We compare the descriptor baselines SIFT~\cite{lowe2004distinctive}, SuperPoint~\cite{detone2018superpoint}, D2-Net~\cite{dusmanu2019d2}, R2D2~\cite{revaud2019r2d2}, KeyNet+HyNet~\cite{laguna2022key,tian2020hynet}, GIFT~\cite{liu2019gift}, and LISRD~\cite{pautrat2020online}.
Our proposed model with the rotation-equivariant network (ReResNet-18) achieves state-of-the-art performance in terms of homography estimation. 
GIFT~\cite{liu2019gift}, an existing rotation-invariant descriptor, shows a comparatively lower performance on this extremely rotated benchmark with varying illumination.
Note that we use the same dataset generation scheme with the same source dataset~\cite{lin2014microsoft} to GIFT~\cite{liu2019gift}.
LISRD~\cite{pautrat2020online}, which selects viewpoint and illumination invariance online, demonstrates better MMA than ours on the \textit{Day} benchmark, but ours* which extracts top-$k$ candidate descriptors shows the best MMA and homography estimation on both \textit{Day} and \textit{Night} benchmarks.

Figure~\ref{fig:results_of_rdnim} shows the results of mean matching accuracy with different pixel thresholds on the ERDNIM dataset.
Our descriptor with top-$k$ candidate selection denoted by ours* achieves the state-of-the-art MMA at all pixel thresholds on both the day and night benchmarks.
The results show our local descriptors achieve not only rotational invariance, but also robustness to geometric changes with perspective distortions and day/night illumination changes.

