\begin{figure*}[t]
    \vspace{-.2in}
    \centering
    \includegraphics[width=1.0\textwidth]{figure/concept.png}
    \caption{Illustration of \Algname (\ALGname). The key ingredient of \ALGname is twofold: (a) self-imitation for training and (b) subgoal skipping for execution. For (a), we distill a planned-subgoal-conditioned policy into the target-goal-conditioned policy via our self-imitation loss term $\mathcal{L}_{\mathtt{\ALGname}}$. A policy is trained using the auxiliary $\mathcal{L}_{\mathtt{\ALGname}}$ along with off-the-shelf actor loss. 
    For (b), we randomize a subgoal provision from a planner.
    % explore a new exploration strategy that executes a policy with another subgoals in a planned path rather than strictly using the very first node only as previous works usually do. 
    % To select the subgoal for conditioning, we stochastically skip a node.
    }
    %along with a graph-based planning, we provide a subgoal depending on the strength of a current policy, 
    % measured by our proposed loss term $\mathcal{L}_{\mathtt{\ALGname}}$.}
    \label{fig:concept}
\end{figure*}