\begin{algorithm}[h]
   \caption{GCRL with planning + \highlight{\ALGname}}
   \label{alg:framework}
\begin{algorithmic}
\State {\bfseries Input}: Number of training episodes $M$, horizon $H$
\State Initialize replay buffer $\mathcal{B} \leftarrow \varnothing$.
\State Initialize the parameters of goal-conditioned policy $\pi_{\theta}$.
\State Initialize the parameters of action-value function $Q_{\phi}$.
\For{$m=1, 2, 3, \ldots M$}
\State Reset the environment.
\State Sample a target goal $g$ and an initial state $s_{0}$.
    \For{$t=1, 2, 3, \ldots H$}
    \State Build a graph $\mathcal{H} = (\mathcal{V}, \mathcal{E}, d)$ using $\mathcal{B}$.
    \State Find the shortest subgoal-path $\tau_{g}$ from $s_{t}$ to $g$.
    \State \highlight{Find a desired subgoal $l^{*}$ via Algorithm~\ref{alg:skip}.}
    \State Collect a transition $(s_{t}, a_{t}, r_{t})$ using $\pi_{\theta} (s_{t}, l^{*})$.
    \State Store the transition and the planned path $\tau_{g}$ in $\mathcal{B}$.
    \EndFor
\State Update $Q_{\phi}$ using $\mathcal{L}_{\mathtt{critic}} (\phi)$ of Equation~\ref{eq:ddpg_critic}
\State Update $\pi_{\theta}$ using $\mathcal{L}_{\mathtt{actor}} (\theta) + \highlight{\lambda \mathcal{L}_{\mathtt{\ALGname}} (\theta)} $ of Equation~\ref{eq:total_loss}
\EndFor
\end{algorithmic}
\end{algorithm}