\begin{algorithm}[tb]
   \caption{Self-Imitating Goal-Conditioned Policies}
   \label{alg:algo}
\begin{algorithmic}
\STATE {\bfseries Input}: Number of training episodes $N$, horizon $H$
\STATE Initialize replay buffer $\mathcal{B} \leftarrow \varnothing$.
\STATE Initialize the parameters of goal-conditioned policy $\pi_{\theta}$.
\FOR{$n=1, 2, 3, \ldots N$}
\STATE Sample a target goal $g$ and an initial state $s_{0}$.
    \FOR{$t=1, 2, 3, \ldots H$}
    \STATE Build a graph $G = (L, E, d)$ using $\mathcal{B}$.
    \STATE Find the shortest subgoal-path $\tau_{g}$ from $s_{t}$ to $g$.
    \STATE \junsu{Find a desired subgoal $l^{*}$ via Algorithm~\ref{alg:skip}.}
    \STATE Collect a transition $(s_{t}, a_{t}, r_{t})$ using $\pi_{\theta} (s_{t}, l^{*})$.
    \STATE Store the transition and the planned path $\tau_{g}$ in $\mathcal{B}$.
    \STATE Train a policy $\pi_{\theta}$  with $\mathcal{B}$ according to \junsu{Equation} \ref{eq:total_loss}.
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}