\section{Experiment}
\label{sec:experiment}
\input{figure/2_environment}
In this section, we design our experiments to answer the following questions:
\begin{itemize}[topsep=1.0pt,itemsep=1.0pt,leftmargin=5.5mm]
    \setlength\itemsep{-.05mm}
    \item Can \ALGname improve the sample-efficiency on long-horizon continuous control tasks over baselines (Figure~\ref{fig:main})?
    \item Can a policy trained by \ALGname perform well even without a planner at the test time (Figure~\ref{fig:planner})?
    \item How does \ALGname compare to another self-imitation strategy (Figure~\ref{fig:ablation})?
    \item Is the subgoal skipping effective for sample-efficiency (Figure~\ref{fig:skipping})?
    \item How does the balancing coefficient $\lambda$ affect performance (Figure~\ref{fig:lambda})?
\end{itemize}
\vspace{-.05in}
\subsection{Experimental setup}
\textbf{Environments.}
\input{figure/3_main_results}
We conduct our experiments on a set of challenging long-horizon continuous control tasks based on MuJoCo simulator \citep{todorov2012mujoco}.
Specifically, we evaluate our framework on 2DReach, Reacher, Pusher, and $\{\text{L}, \text{U}, \text{S},\omega,\Pi\}$-shaped AntMaze environments (see Figure~\ref{fig:environment} for the visualization of environments).
In 2DReach and AntMaze environments, we use a pre-defined 2-dimensional goal space that represents the $(x, y)$ position of the agent following prior works \citep{huang2019mapping, kim2021landmark}. For Reacher, the goal space is 3-dimension that represents the position of an end-effector. For Pusher, the goal space is 6-dimension that represents the positions of an end-effector and a puck. We provide more details of the environments in Supplemental material~\ref{supp:env}.

\textbf{Implementation.} We use DDPG algorithm \citep{Lillicrap2015continuous} as an underlying RL algorithm following the prior work \citep{huang2019mapping}. For a graph-based planner and hindsight goal-relabelling strategy, we follow the setup in MSS \citep{huang2019mapping}. 
We provide more details of the implementation, including the graph-construcion and hyperparameters in Supplemental material~\ref{supp:impl}.

\textbf{Evaluation.}
We run 10 test episodes without an exploration factor for every 50 training episodes. 
For the performance metric, we report the success rate defined as the fraction of episodes where the agents succeed in reaching the target-goal within a threshold.
We report mean and standard deviation, which are represented as solid lines and shaded regions, respectively, over eight runs for Figure~\ref{fig:main} and four runs for the rest of the experiments.
For visual clarity, we smooth all the curves equally.

\textbf{Baselines and our framework.}
We compare our framework with the following baselines on the environments of continuous action spaces:
\begin{itemize}[leftmargin=5.5mm]
    \item HER \citep{andrychowicz2017hindsight}: This method does not use a planner and trains a non-hierarchical policy using a hindsight goal-relabeling strategy.
    \item MSS \citep{huang2019mapping}: This method collects samples using a graph-based planner along with a policy and trains the policy using stored transitions with goal-relabeling by HER. A graph is built via farthest point sampling \citep{vassilvitskii2006k} on states stored in a replay buffer.
    \item 
    \LthreeP \citep{zhang2021world}:
    When building a graph, this method replaces the farthest point sampling of MSS with node-sampling on learned latent space, where nodes are scattered in terms of reachability estimates.
    \item HIGL \citep{kim2021landmark}: This method utilizes a graph-based planner to guide training a high-level policy in goal-conditioned hierarchical reinforcement learning. In contrast, \ALGname uses the planner to guide low-level policy. Comparison with HIGL evaluates the benefits of directly transferring knowledge from the planner to low-level policy without going through high-level policy.
    \item GCSL \citep{ghosh2021learning}: This method learns goal-conditioned policy via iterative supervised learning with goal-relabeling. Originally, GCSL does not utilize a graph-based planner, but we compare ours with GCSL-variant that uses the planner for further investigation in Figure~\ref{fig:ablation}.
\end{itemize}
For all experiments, we report the performance of \ALGname combined with MSS.
Nevertheless, we remark that our work is also compatible with other GCRL approaches because \ALGname does not depend on specific graph-building or planning algorithms, 
as can be seen in Algorithm~\ref{alg:framework} in Supplemental material~\ref{supp:algo_table}.
We provide more details about baselines in Supplemental material~\ref{supp:hyperparameters}.

\subsection{Comparative evaluation}
As shown in Figure~\ref{fig:main}, applying our framework on top of the existing GCRL method, MSS + \ALGname, improves sample-efficiency with a significant margin across various control tasks. Specifically, MSS + \ALGname achieves a success rate of 57.41\% in large U-shaped AntMaze at environment step $10 \times 10^{5}$, while MSS performs 19.08\%. We emphasize that applying \ALGname is more effective when the task is more difficult; MSS + \ALGname shows a larger margin in performance in more difficult tasks (i.e., U-, S-, and $\omega$- shaped mazes rather than L-shaped mazes).
Notably, we also observe that MSS + \ALGname outperforms \LthreeP, which shows that our method can achieve strong performance without the additional complexity of learning latent landmarks.
We remark that \ALGname is also compatible with other GCRL approaches, including \LthreeP, as our framework is agnostic to how graphs are constructed. 
To further support this, we provide additional experimental results that apply \ALGname on top of another graph-based GCRL method in Supplemental material~\ref{supp:l3p_pig}.

\input{figure/3_planner}
\label{sec:higl}
Also, we find that MSS + \ALGname outperforms HIGL in Figure~\ref{fig:main}. 
These results show that transferring knowledge from a planner to low-level policy is more efficient than passing through a high-level policy. 
Nevertheless, one can guide both high- and low- level policy via planning, i.e., 
HIGL + \ALGname, which would be interesting future work.
We also remark that the overhead of applying \ALGname is negligible in time complexity. Specifically, both the graph-based planning algorithms (MSS+\ALGname and MSS) spend 1h 30m for 500k steps of the 2DReach, while non-planning baseline (HER) spends 1h.

\input{figure/4_ablation}
\textbf{Reaching a goal without a planner at test time.}
To further investigate whether knowledge from graph-based planning is transferred into a policy, we additionally evaluate without the planner at the test time; in other words, the planner is only used at the training time and not anymore at the test time.
Intriguingly, we find that training with our \ALGname enables successfully reaching the target-goal even without the planner at test time. 
As shown in Figure~\ref{fig:planner}, this supports our training scheme indeed makes the policy much stronger.
Such deployment without planning could be practical in some real-world scenarios where a planning time or memory for storing a graph matter \citep{bency2019neural, qureshi2019motion}. We also provide experimental results with a larger maze in Supplemental material~\ref{supp:larger_maze_without_planner}.

\subsection{Ablation studies}
\label{sec:bc_loss}
\textbf{Effectiveness of our loss design.}
In order to empirically demonstrate that utilizing (a) the graph-based planner and (b) actions from a current policy is crucial,
we compare \ALGname (without subgoal skipping) to a GCSL-variant\footnote{Original GCSL use only $\mathcal{L}_{\mathtt{GCSL}}$, not RL loss term and does not use a planner in execution.} that optimizes the following auxiliary objective in conjunction with the RL objective of MSS framework:
\begin{align}
\label{eq:bc}
\mathcal{L_{\mathtt{GCSL}}} = \mathbb{E}_{(s,a,g) \sim \mathcal{B}} [\Vert \pi_{\theta}(s, g) - a\Vert_{2}^{2}],
\end{align}
that is, it encourages a goal-conditioned policy to imitate previously successful actions to reach a (relabeled) goal; a goal and a reward is relabeled in hindsight. In execution time, we also apply a graph-based planner to GCSL-variant for a fair comparison.
As shown in Figure~\ref{fig:ablation}, \ALGname is more effective than using the loss $\mathcal{L}_{\mathtt{GCSL}}$ in terms of sample-efficiency due to (a) knowledge transferred by a planner and (b) more plausible actions from a current policy (rather than an old policy). 

% \vspace{0.05in}
\input{figure/5_skipping}
\textbf{Subgoal skipping.}
\label{sec:exp_subgoal_skipping}
We evaluate whether the proposed subgoal skipping is effective in Figure~\ref{fig:skipping}. For 2DReach and Pusher, we observe that \ALGname with skipping achieves significantly better performance than without skipping. 
We understand this is because a strong policy may find a better goal-reaching path by ignoring some of the subgoals proposed by the planner.
On the other hand, we find that subgoal skipping does not provide a large gain on L-shaped Antmaze, which is a more complex environment. We conjecture that this is because learning a strong policy with high-dimensional state inputs of quadruped ant robots is much more difficult. Nevertheless, we believe this issue can be resolved when the base RL algorithm is 
more improved.
We provide more experiments related to subgoal skipping (i.e., comparison to random skipping) in Supplemental material~\ref{supp:random_skipping}, \ref{supp:tuning_cost}, and \ref{supp:subgoal_expl}.

\input{figure/6_coeff}
\textbf{Balancing coefficient $\lambda$.}
We investigate how the balancing coefficient $\lambda$ in Equation~\ref{eq:total_loss} that determines the effect of our proposed loss term $\mathcal{L}_{\mathtt{\ALGname}}$ affect the performance in Figure~\ref{fig:lambda}.
We find that \ALGname with $\lambda \in \{1e-3, 1e-4\}$ outperforms \ALGname with $\lambda = 0$, which shows the importance of the proposed loss.
We also observe that too large value of $\lambda$ harms the performance since it incapacitates the training signal of $\mathcal{L}_{\mathtt{actor}}$ excessively. Meanwhile, one can set the balancing coefficient $\lambda$ automatically in a task-agnostic way, which would guide researchers when they extend our work into new environments in the future.
We provide experimental results with automatic setting of $\lambda$ in Supplemental material~\ref{supp:tuning_cost}.
