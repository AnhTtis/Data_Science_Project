%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Algorithm table}
\label{supp:algo_table}
We provide algorithm tables that represent \ALGname in Algorithm \ref{alg:framework} and \ref{alg:skip}.
\input{algorithm/framework}
\input{algorithm/skip}
\newpage

\section{Environment details}
\label{supp:env}
\subsection{2DReach} 
A green point in a 2D U-shaped Maze of size $15 \times 15$ aims to reach a target goal represented by a red point. At each step, the agent can move within $[-1, 1] \times [-1, 1]$ in $x$ and $y$ directions.

\subsection{Reacher}
A robotic arm aims to make its end-effector reach the target position on 3D space. The state of the arm is 17-dimension, including the positions, angles, and velocities of itself, and the action-space is 7-dimension. Initial point and target goal are set randomly at the start of episode both at training and test time.
    
\subsection{Pusher} A robotic arm aims to make a puck in a plane reach a goal position by pushing the object. The state of the arm is 20-dimension, which is same to Reacher but additionally include position of a puck, and the action-space is 7-dimension. Initial point and target goal are set randomly at the start of episode both at training and test time.
    
\subsection{AntMaze} A quadruped ant robot is trained to reach a random goal from a random location and tested under the most difficult setting for each maze. The states of ant is 30-dimension, including positions and velocities. An ant should reach the target point within 500 steps for U-shaped mazes, and 1000 steps for S-, $\omega$-, and $\Pi$-shaped mazes.

\newpage
\section{Additional experiments}
\label{supp:additioanl_experiments}
\subsection{Applying \ALGname on top of another graph-based GCRL method.}
\label{supp:l3p_pig}
Additionally, we also observe that applying \ALGname on top of another planning-based GCRL method (i.e., \LthreeP rather than MSS) also demonstrates significant gains. As shown in Figure~\ref{fig:l3p}, \ALGname boost sample-efficiency for \LthreeP in U-shaped AntMaze and FetchPickAndPlace-v1~\citep{plappert2018multi}. These experiments further highlight that \ALGname is generic technique to improve performance of all the graph-based planning algorithms. 

\begin{figure}[h]
\centering
    \begin{subfigure}[b]{0.325\columnwidth}
    \includegraphics[width=\linewidth]{figure/L3P+PIG.pdf}
    \caption{U-shaped AntMaze}
    \label{fig:l3p_antmaze_u}
    \end{subfigure}
\centering
    \begin{subfigure}[b]{0.325\columnwidth}
    \includegraphics[width=\linewidth]{figure/L3P+PIG_pick.pdf}
    \caption{FetchPickAndPlace-v1}
    \label{fig:l3p_pick}
    \end{subfigure}
    \caption{Test time success rate of \ALGname on top of another planning-based GCRL method (i.e., \LthreeP) in (a) U-shaped AntMaze and (b) FetchPickAndPlace-v1.}
    \label{fig:l3p}
\end{figure}

\subsection{Comparison to alternatives for subgoal skipping.}
\label{supp:random_skipping}
We compare our subgoal skipping strategy to a simple baseline: random sampling of subgoals from the planned path. As shown in the Figure~\ref{fig:2dreach_random_skip} and \ref{fig:pusher_random_skip}, we find that the alternative performs close to ours in 2DReach, but ours outperforms in Pusher. Developing better skipping strategy is an interesting direction to explore. 

\begin{figure}[h]
\centering
    \begin{subfigure}[b]{0.325\columnwidth}
    \includegraphics[width=\linewidth]{figure/2DReach_random_skip.pdf}
    \caption{2DReach}
    \label{fig:2dreach_random_skip}
    \end{subfigure}
\centering
    \begin{subfigure}[b]{0.325\columnwidth}
    \includegraphics[width=\linewidth]{figure/Pusher_skip_random.pdf}
    \caption{Pusher}
    \label{fig:pusher_random_skip}
    \end{subfigure}
    \caption{Ablation studies about skipping strategy. We compare our skipping strategy to an alternative one: random skipping on (a) 2DReach and (b) Pusher.}
\end{figure}

\subsection{Hyperparameter tuning cost.}
\label{supp:tuning_cost}
Our \ALGname inevitably introduces new hyperparameters ($\lambda$ and $\alpha$) in addition to existing algorithms, but we can use a task-agnostic strategy to choose them without any computational overhead. To be specific, one can set the balancing coefficient $\lambda$ adaptively to satisfy $\lambda \times \mathcal{L}_{\mathtt{PIG}} = 0.01 \times \mathcal{L}_{\mathtt{actor}}$; see Figure~\ref{fig:lambda_2dreach}, \ref{fig:lambda_antmaze_l}. Next, we found that the performance of our algorithms is robust to the choice of skipping temperature $\alpha$; see Figure~\ref{fig:alpha_pusher}.

\begin{figure}[h]
  \begin{subfigure}[b]{0.3\columnwidth}
    \includegraphics[width=\linewidth]{figure/2DReach_auto_lambda.pdf}
    \caption{2DReach}
    \label{fig:lambda_2dreach}
  \end{subfigure}
  \hfill %%
  \begin{subfigure}[b]{0.35\columnwidth}
  \centering
    \includegraphics[width=0.85714\linewidth]{figure/L-shaped_AntMaze_Auto_Lambda.pdf}
    \caption{L-shaped AntMaze}
    \label{fig:lambda_antmaze_l}
  \end{subfigure}
  \hfill %%
  \begin{subfigure}[b]{0.3\columnwidth}
    \includegraphics[width=\linewidth]{figure/Pusher_alpha.pdf}
    \caption{Pusher}
    \label{fig:alpha_pusher}
  \end{subfigure}
  \caption{Experiments with (a, b) automatic hyperparameter setting of $\lambda$ and (c) varying $\alpha$.}
\end{figure}

\newpage
\subsection{Effect of subgoal skipping in exploration.}
\label{supp:subgoal_expl}
\begin{wrapfigure}{r}{0.325\linewidth}
\vspace{-.2in}
\centering
    \includegraphics[width=1.0\linewidth]{figure/2DReach_state_entropy.pdf}
    \caption{State entropy}
    \label{fig:state_entropy}
\vspace{-.2in}
\end{wrapfigure}
To further support our statement - subgoal skipping makes an agent could collect better trajectories via promoting exploration, we quantitatively measure how diverse an agent discovers states during training depending on subgoal skipping. Specifically, we employ particle-based k-nearest neighbors ($k$-NN) entropy estimator \citep{singh2003nearest} to measure how diverse collected samples are. Formally, let $X$ be a random variable whose probability density function is $p$, and $\{ x_{i} \}_{i=1}^{N}$ be its $N$ i.i.d realization. State entropy is defined as $\mathcal{H} (X) = - \mathbb{E}_{x \sim p(x)} [\log p(x)] $ and we can estimate $\mathcal{H} (X)$ as follows:
\begin{align}
\label{eq:state_entropy}
\hat{\mathcal{H}}_{N}^{K} (X) \propto \frac{1}{N} \sum_{i=1}^{N} \log \frac{1}{K} \sum_{k=1}^{K} \Vert x_{i} - x_{i}^{k - \text{NN} }\Vert_{2},
\end{align}
where $x_{i}^{k-\text{NN}}$ is the $k$-NN of $x_{i}$ within a set $\{ x_{i} \}_{i=1}^{N}$. We use $N=128$ and $K=10$ for an experiment using 2DReach environment.
As shown in Figure~\ref{fig:state_entropy}, we observe that using subgoal skipping makes high state entropy; that is, subgoal skipping makes an agent collect more diverse samples, which is likely to have more chance to include better samples.

\subsection{Reaching a goal without a planner at test time with a larger maze.}
\label{supp:larger_maze_without_planner}
\begin{wrapfigure}{r}{0.325\linewidth}
\vspace{-.2in}
\centering
    \includegraphics[width=1.0\linewidth]{figure/Large_U-shaped_AntMaze_Planner.pdf}
    \caption{Test time success rate of \ALGname and MSS on large U-shaped Ant Maze over four runs.}
    \label{fig:reaching_without_planner_large}
\vspace{-.8in}
\end{wrapfigure}

We also evaluate without a planner at test time with a large U-shaped AntMaze. As shown in Figure~\ref{fig:reaching_without_planner_large}, training with \ALGname enables successfully reaching the target-goal even without the planner at test time even in larger environment. Intriguingly, after $5 \times 10^5$ environment timesteps, a policy trained by our approach performs better even without access to a planner at test time compared to MSS, which uses a planner at test time. 

\newpage
\subsection{Experiments with stochastic transition model.}

\begin{wrapfigure}{r}{0.325\linewidth}
\vspace{-.2in}
\centering
    \includegraphics[width=1.0\linewidth]{figure/AntMazeL_stochastic.pdf}
    \caption{Learning curves on stochastic L-shaped AntMaze as measured on the success rate.}
    \label{fig:stochastic_antmazel}
\vspace{-.2in}
\end{wrapfigure}

\ALGname, along with our graph construction technique, is applicable to stochastic environments since our algorithmic component (self-imitation loss and subgoal skipping) and graph construction mechanism (farthest point sampling and assigning edge weights) are built on visited state spaces, regardless of transition dynamics.

To empirically show that \ALGname is effective in stochastic environments, we additionally provide experimental results on stochastic L-shaped AntMaze, where gaussian noise $\mathcal{N}(0, 0.05)$ is added to the $(x, y)$ position of an agent at every step following setups from \citet{zhang2020generating, kim2021landmark}. As shown in the Figure~\ref{fig:stochastic_antmazel}, we observe that \ALGname successfully solves tasks in the stochastic environment. Moreover, not only in (deterministic) L-shaped AntMaze, but also in stochastic L-shaped AntMaze, PIG shows significant gain compared to the baseline (MSS). This result supports that \ALGname trains a strong policy that is able to reach faraway goals more sample-efficiently than the baseline thanks to our self-imitation loss and subgoal skipping.

\subsection{Ablation studies with more environments.}
We provide ablation studies about self-imitation loss and subgoal skipping with more environments: Reacher and Large U-shaped AntMaze. As showin in Figure~\ref{supp:abaltion_loss} and \ref{supp:ablation_skipping}, including our self-imitation loss or subgoal skipping makes significant gains or performs on par.

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.325\textwidth}
    \includegraphics[width=1.0\linewidth]{figure/Reacher_gcsl.pdf}
    \caption{Reacher}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
    \includegraphics[width=1.0\linewidth]{figure/AntMazev0_gcsl.pdf}
    \caption{Large U-shaped AntMaze}
    \end{subfigure}
    \caption{Ablation sutides about self-imitation learning for training on (a) Reacher and (b) Large U-shaped AntMaze with four runs. MSS + $\mathcal{L}_{\mathtt{\ALGname}}$ and MSS + $\mathcal{L}_{\mathtt{GCSL}}$ refer to an algorithm that applies loss term $\mathcal{L}_{\mathtt{\ALGname}}$ and $\mathcal{L}_{\mathtt{GCSL}}$ on top of MSS method, respectively; subgoal skipping is not applied. We find that our loss term $\mathcal{L}_{\mathtt{\ALGname}}$ is more effective than $\mathcal{L}_{\mathtt{GCSL}}$ as an auxiliary term.}
    \label{supp:abaltion_loss}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.325\textwidth}
    \includegraphics[width=1.0\linewidth]{figure/Reacher_skipping.pdf}
    \caption{Reacher}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
    \includegraphics[width=1.0\linewidth]{figure/AntMazev0_skipping.pdf}
    \caption{Large U-shaped AntMaze}
    \end{subfigure}
    \caption{Learning curves of PIG with and without subgoal skipping on (a) Reacher and (b) Large U-shaped AntMaze tasks with four runs.}
    \label{supp:ablation_skipping}
\end{figure*}

\newpage

\subsection{Experiments with extended timesteps.}
\label{supp:extended_timesteps}
\begin{wrapfigure}{r}{0.325\linewidth}
\vspace{-.2in}
\centering
    \includegraphics[width=1.0\linewidth]{figure/Large_U-shaped_AntMaze_extended.pdf}
    \caption{Learning curves on U-shaped AntMaze as measured on the success rate.}
    \label{fig:extended}
\vspace{-1in}
\end{wrapfigure}

To assess whether the empirical improvements are in learning speed or also in asymptotic performance,
we evaluate PIG and MSS with extended timesteps (i.e., from $10 \times 10^5$ to $30 \times 10^5$ on Large U-shaped AntMaze. 
As shown in the Table below, we find that PIG can improve both sample-efficiency and asymptotic performances of MSS. This shows that enhanced policy learning via information distillation from the planner can also improve the asymptotic performance.

\vspace{.8in}

\section{Implementation details}
\label{supp:impl}
All of the experiments were processed using a single GPU (NVIDIA TITAN Xp) and 8 CPU cores (Intel Xeon E5-2630 v4).
For baselines, we employ open-source codes of MSS\footnote{\url{https://github.com/FangchenLiu/map\_planner}}, \LthreeP\footnote{\url{https://github.com/LunjunZhang/world-model-as-a-graph}}, and HIGL\footnote{\url{https://github.com/junsu-kim97/HIGL}}.
\subsection{Graph construction}
\label{supp:graph_construction}
\textbf{Collection of graph-constructing states.} 
We follow collecting scheme of graph-construction states from \citet{huang2019mapping}. The collection is proceeded in two steps: (a) random sampling of a fixed-sized pool $\mathcal{D}$ from an experience replay and (b) farthest point sampling (FPS) \citep{vassilvitskii2006k, huang2019mapping} from the pool $\mathcal{D}$ to build the final collection $\mathcal{V}$ of graph-constructing states. 

Specifically, any given time, let $D(s)$ denote the shortest distance from a state $s$ to the closest element in current $\mathcal{V}$. The set $\mathcal{V}$ is initialized with an empty set.
Then, FPS runs as follows:
\begin{itemize}[topsep=1.0pt,itemsep=1.0pt,leftmargin=5.5mm]
    \item Step A: Choose a state $s^{1}$ uniformly at random from the pool $\mathcal{D}$ and add $s^{1}$ into $\mathcal{V}$.
    \item Step B: Choose the next state $s^{i}$, whose $D(s^{i})$ is the largest among elements in $\mathcal{D}$. Add $s^{i}$ into $\mathcal{V}$.
    \item Step C: Repeat Step B until we have chosen a budget for the number of nodes in a graph. 
\end{itemize}

The diversity of the collection is ensured by farthest point sampling. Random sampling to build a fixed-sized pool makes the computational complexity of planning irrelevant to the size of experience replay, of which size is 1M in our experiments. 


\textbf{Edge connection.}
After collecting graph-constructing states, we complete a graph by adding directed edges \citep{huang2019mapping}. 
In detail, given two nodes $l^{1}$ and $l^{2}$, we connect them by adding two directed edges $(l^{1}, l^{2}) \in \mathcal{E}$ (from $l^{1}$ to $l^{2}$) and $(l^{2}, l^{1}) \in \mathcal{E}$ (from $l^{2}$ to $l^{1}$). Then we assign weights as an estimated distance $d(l^{1}, l^{2})$ and $d(l^{2}, l^{1})$, respectively.

\subsection{Hyperparameters}
\label{supp:hyperparameters}
We list hyperparameters used for \ALGname across all environments in Table~\ref{tbl:hyperparameters_common} and \ref{tbl:hyperparameters_specific}. 

For the baselines, we used the best hyperparameters reported in their source codes for shared environments: 2DReach of MSS and HER, Reacher and Pusher for HIGL, and AntMazes for MSS, L3P, HER, and HIGL (all). For unstudied environments in the baseline papers, we have searched hyperparameters for each baseline. For example, we search shift magnitude and adjacency degree for HIGL, clipping threshold and final goal adjacency threshold for L3P and MSS, and relabeling ratio for HER. We note that for PIG, two newly introduced hyperparameters (balancing coefficient $\lambda$ and skipping temperature $\alpha$) have been searched. We would like to remark that performance gain by PIG have been achieved without exhaustive efforts in hyperparameter search compared to baselines. For example, the baseline MSS conducted grid search on 30 (number of landmarks) $\times$ 30 (clipping threshold) values in their paper, but we searched among 5 $\times$ 4 values for PIG: $\{1.0, 0.1, 0.01, 0.001, 0.0001\}$ for $\lambda$ and $\{20, 10, 5, 1\}$ for $\alpha$.

\begin{table}[h]
\caption{Hyperparameters across all environments.}
\vskip 0.15in
\centering
\begin{center}
\begin{tabular}{l c}
\toprule

\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
\textit{DDPG} & \\
\midrule
Optimizer & Adam
\citep{kingma2014adam} \\
Actor learning rate & 0.0002 \\
Critic learning rate & 0.0002 \\
Replay buffer size & 1M \\
Number of hidden layers for actors & 4 \\
Number of hidden layers for critics & 5 \\
Number of hidden units per layer & 400 \\
Batch size & 200 \\
Nonlinearity & ReLU \\
Polyak for target network & 0.99 \\
Target update frequency per episode & 3 \\
Ratio between env vs optimization steps & 1 \\
Gamma & 0.99 \\
Hindsight relabelling ratio & 0.8 \\
\midrule
\midrule
\textit{Graph} & \\
\midrule
Number of soft value iteration & 20 \\
Temperature & 0.9 \\
\bottomrule
\end{tabular}
\label{tbl:hyperparameters_common}
\end{center}
\vskip -0.1in
\end{table} 

\begin{table}[h]
\caption{Hyperparameters that differ across the environments.}
\vskip 0.15in
\centering
\large
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l cccc}
\toprule

\textbf{Hyperparameter} & \textbf{2DReach} & \textbf{Reacher} & \textbf{Pusher} & \textbf{AntMaze} \\
\midrule
\textit{Ours-specific} & \\
\midrule
Balancing coefficient $\lambda$        & 1.0 & 0.0001 & 0.1 & 0.001 \\
Skipping temperature $\alpha$          & 1.0 & 10.0   & 1.0 & 10.0  \\
\midrule
\midrule
\textit{DDPG} & \\
\midrule
Initial random trajectories    & 2.5k & 20k & 20k & 100k (for L-, U- shaped Maze) \\
                               &      &     &     & 400k (for Large U-shaped Maze) \\   
                               &      &     &     & 800k (for S-, $\omega$-, $\Pi$ -shaped Maze) \\   
Hindsight relabelling range & 50 & 50 & 50 & 200 \\

Action L2 & 0.5 & 0.01 & 0.01 & 0.5 \\
Action noise & 0.2 & 0.1 & 0.1 & 0.2 \\
\midrule
\midrule
\textit{Graph} & \\
\midrule
Number of nodes in a graph & 100 & 80 & 80 & 400 \\
clipping threshold for distances & 4.0 & 4.0 & 4.0 & 38.0 \\

\bottomrule
\end{tabular}
\label{tbl:hyperparameters_specific}
}
\end{center}
\vskip -0.1in
\end{table} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%