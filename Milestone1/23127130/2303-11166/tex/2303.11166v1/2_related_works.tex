\section{Related work}

\textbf{Goal-conditioned reinforcement learning (GCRL).} 
GCRL aims to solve multiple tasks associated with target-goals \citep{andrychowicz2017hindsight, kaelbling1993learning, schaul2015universal}.
Typically, GCRL algorithms rely on the  universal value function approximator
(UVFA) \citep{schaul2015universal}, which is a single neural network that estimates the true value function given not just the states but also the target-goal. 
Furthermore, researchers have also investigated goal-exploring algorithms \citep{mendonca2021discovering, pong2020skew} to avoid any local optima of training the goal-conditioned policy. 

\textbf{Graph-based planning for GCRL.}
To solve long-horizon GCRL problems, graph-based planning can guide the agent to condition its policy on a series of subgoals that are easier to reach than the faraway target goal \citep{eysenbach2019search, hoang2021successor, huang2019mapping, laskin2020sparse, savinov2018semiparametric, zhang2021world}.
To be specific, the corresponding frameworks build a graph where nodes and edges correspond to states and inter-state distances, respectively. Given a shortest path between two nodes representing the current state and the target-goal, the policy conditions on a subgoal represented by a subsequent node in the path. 

For applying graph-based planning to complex environments, recent progress has mainly been made in building a graph that represents visited state space well while being scalable to large environments.
For example, \citet{huang2019mapping} and \citet{hoang2021successor} limits the number of nodes in a graph and makes nodes to cover visited state space enough by containing nodes that are far from each other in terms of L2 distance or successor feature similarity, respectively. Moreover, graph sparsification via two-way consistency \citep{laskin2020sparse} or learning latent space with temporal reachability and clustering \citep{zhang2021world} also have been proposed.
They have employed graph-based planning for providing the nearest subgoal to a policy at execution time, which utilizes the optimal substructure property in a limited context. In contrast, PIG aims to faithfully utilize the property both in training and execution via self-imitation and subgoal skipping, respectively.

\textbf{Self-imitation learning for goal conditioned policies.}
Self-imitation learning strengthens the training signal by imitating trajectories sampled by itself \citep{oh2018self,ding2019goal,chane2021goal,ghosh2021learning}. 
GoalGAIL \citep{ding2019goal} imitates goal-conditioned actions from expert demonstrations along with the goal-relabeling strategy \citep{andrychowicz2017hindsight}. 
Goal-conditioned supervised learning 
(GCSL) \citep{ghosh2021learning}
trains goal-conditioned policies via iterated supervised learning with goal-relabeling.
RIS \citep{chane2021goal} makes target-goal- and subgoal- conditioned policy be similar, where the subgoal is from a high-level policy that is jointly trained with a (low-level) policy. Compared to prior works, \ALGname faithfully incorporates optimal substructure property with two distinct aspects: (a) graph-based planning and (b) actions from a current policy rather than past actions, where we empirically find that these two differences are important for performance boost (see Section~\ref{sec:bc_loss}). Nevertheless, we remark that \ALGname is an orthogonal framework to them, so applying \ALGname on top of them (e.g., RIS) would be an interesting future work (e.g., leveraging both planning and high-level policy).

\textbf{Distilling planning into a policy.}
Our idea of distilling outcomes of planner into the goal-conditioned policy is connected to prior works in the broader planning context. For example, AlphaGo Zero \citep{silver2017mastering} distills the outcome of the Monte-Carlo Tree Search (MCTS) planning procedure into a prior policy. Similarly, SAVE \citep{Hamrick2020Combining} distills the MCTS outcomes into the action-value function. \ALGname aligns with them in that we distill planned-subgoal-conditioned policy into the target-goal-conditioned policy.