\section{Planning-guided self-imitation learning for GCRL}
In this section, we introduce a new framework, named \ALGname, for improving the sample-efficiency of GCRL using graph-based planning. Our framework adds two components on top of the existing methods: (a) training with self-imitation and (b) execution with subgoal skipping, which highlights the generality of our concept 
(colored as \highlight{purple} in Algorithm~\ref{alg:framework} in Supplementary material~\ref{supp:algo_table}).
Our main idea fully leverages the optimal substructure property; any sub-path of an optimal goal-reaching path is an optimal path for its endpoint (Figure~\ref{fig:comp1}).
 In the following sections, we explain our self-imitation loss as a new training objective in Section~\ref{subsec:loss} and subgoal skipping strategy for execution in Section~\ref{subsec:coarse}.
 We provide an illustration of our framework in Figure~\ref{fig:concept}.

\subsection{Training with self-imitation}
    \label{subsec:loss}
Motivated by the intuition that an agent should pass through a subgoal to reach a target-goal, we encourage actions from target-goal- and subgoal- conditioned policy to stay close, where the subgoals are nodes in a planned subgoal-path. By doing so, we expect that faraway goal-conditioned policy learns plausible actions that are produced by (closer) subgoal-conditioned policy. Specifically, we devise a loss term $\mathcal{L}_{\mathtt{\ALGname}}$ given a stored planned path $\tau_{g} = (l^{1}, l^{2}, \ldots, l^{N})$ and a transition $(s, g, \tau_{g})$ from a replay buffer $\mathcal{B}$ as follows:
\begin{align}
\label{eq:our_bc_loss}
\mathcal{L_{\mathtt{\ALGname}}} (\theta) = \mathbb{E}_{(s, \tau_{g}, g) \sim \mathcal{B}} \bigg[\frac{1}{N-1} \sum_{l^{k} \in \tau_{g} \setminus \{l^{1}\}} \Vert \pi_{\theta}(s, g) - \mathtt{SG} (\pi_{\theta}(s, l^{k})) \Vert_{2}^{2} \bigg]
\end{align}
where $\mathtt{SG}$ refers to a stop-gradient operation.
Namely, the goal-conditioned policy imitates behaviors of subgoal-conditioned policy. We incorporate our self-imitation loss term into the existing GCRL frameworks by plugging $\mathcal{L}_{\mathtt{\ALGname}}$ as an extra loss term into the original policy loss term as follows:
\begin{align}
\label{eq:total_loss}
\mathcal{L} (\theta) = \mathcal{L}_{\mathtt{actor}} (\theta) + \lambda \mathcal{L}_{\mathtt{\ALGname}} (\theta)
\end{align}
where $\lambda$ is a balancing coefficient, which is a pre-set hyperparameter.

One can also understand that self-imitating loss improves performance by enhancing the correctness of planning. Note that actor is used to estimate distance $d$ between two nodes $l^{1}, l^{2}$; $d(l^{1}, l^{2}) \approx -Q_{\phi}(s^{1}, \pi_{\theta} (s^{1}, l^{2}), l^{2})$ as mentioned in Section~\ref{sec:planning}. Our self-imitating loss makes $\pi_{\theta}$ more accurate for even faraway goals, so it leads to the precise construction of a graph. Then, planning gives more suitable subgoals for an actor in execution.

\subsection{Execution with subgoal skipping}
\label{subsec:coarse}
As an additional technique that fits our self-imitation loss, we propose \textit{subgoal skipping}, which randomizes a subgoal proposed by the graph-based planning to further improve the sample-efficiency.
Note that the existing graph-based planning for GCRL always provides the nearest node $l^{2}$ in the searched path $\tau_{g}$ as a desired goal $l^{*}$ regardless of how a policy is trained. 
Motivated by our intuition that an agent may find a better goal-reaching path (i.e., short-cuts) by ignoring some of the subgoals, we propose a new subgoal selecting strategy.

Our subgoal skipping is based on the following insight: 
when a policy for the planned subgoal and the final goal agree (small $\mathcal{L}_{\mathtt{\ALGname}}$), diversifying subgoal suggestions could help find unvisited routes. 
Namely, the goal-conditioned policy is likely to be trustworthy if final-goal- and planned-subgoal- conditioned policies align because it implies that the goal-conditioned policy have propagated information quite far. 
Leveraging generalization capability of the trained policy, suggesting the policy with diversified subgoals rather than only the nearest subgoal could help finding better routes.

To be specific, to select the desired goal $l^{*}$, we start from the nearest node $l^{2}$ in the planned shortest path $\tau_{g}$, and stochastically jump to the next node until our condition becomes unsatisfied with the following binomial probability:
\begin{align}
\begin{split}
\label{eq:path_hop}    
    P[\mathtt{jump}] = \min \bigg(\frac{\alpha}{\mathcal{L}_{\mathtt{\ALGname, latest}}}, 1 \bigg),
\end{split}
\end{align}
where $\alpha$ is pre-set skipping temperature and $\mathcal{L}_{\mathtt{\ALGname, latest}}$ denotes $\mathcal{L}_{\mathtt{\ALGname}}$ calculated at the latest parameter update. We set $l^{*}$ as the final subgoal after the jumping.
Intuitively, the jumping criterion is likely to jump more for a smaller $\mathcal{L}_{\mathtt{\ALGname, latest}}$, which is based on the fact that the policy is likely to behave more correctly for a faraway goal. 
As subgoals are sampled from the searched shortest path and it is not likely to choose a farther subgoal if a policy is not trustworthy for faraway subgoals, our sampled subgoals are likely to be appropriate for a current policy.
We describe our subgoal skipping procedure in Algorithm~\ref{alg:skip} of Supplemental material ~\ref{supp:algo_table}.
