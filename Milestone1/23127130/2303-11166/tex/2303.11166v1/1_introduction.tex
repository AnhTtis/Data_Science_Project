\section{Introduction}
Many sequential decision making problems can be expressed as reaching a given goal, e.g., navigating a walking robot 
% through a maze 
\citep{schaul2015universal,nachum2018data} and fetching an object using a robot arm \citep{andrychowicz2017hindsight}. 
Goal-conditioned reinforcement learning (GCRL) aims to solve this problem by training a goal-conditioned policy to guide an agent towards reaching the target-goal. 
In contrast to many of other reinforcement learning frameworks, GCRL is capable of solving different problems (i.e., different goals) using a single policy.

An intriguing characteristic of GCRL is its \textit{optimal substructure property}; any sub-path of an optimal goal-reaching path is an optimal path for its endpoint (Figure~\ref{fig:comp1}). 
This implies that a goal-conditioned policy is replaceable by a policy conditioned on a ``subgoal'' existing between the goal and the agent. 
Based on this insight, researchers have investigated graph-based planning to construct a goal-reaching path by (a) proposing a series of subgoals and (b) 
executing policies conditioned on the nearest subgoal \citep{savinov2018semiparametric,eysenbach2019search, huang2019mapping}. 
Since the nearby subgoals are easier to reach than the faraway goal, such 
% graph-based 
planning improves the success ratio of the agent reaching the target-goal during sample collection. 

In this paper, we aim to improve the existing GCRL algorithms to be even more faithful to the optimal substructure property.
To be specific, we first incorporate the optimal substructure property into the training objective of GCRL to improve the sample collection algorithm.
Next, when executing a policy, we consider using all the proposed subgoals as an endpoint of sub-paths instead of 
using just the subgoal nearest to the agent (Figure~\ref{fig:comp2}). 

\textbf{Contribution.}
We present \Algname (\ALGname), a novel and generic framework that builds upon the existing GCRL frameworks 
that use
graph-based planning.
\ALGname consists of the following key ingredients (see Figure~\ref{fig:concept}):
\begin{itemize}[topsep=1.0pt,itemsep=1.0pt,leftmargin=5.5mm]
    \item [$\bullet$] \textbf{Training with self-imitation:} 
    we propose a new training objective that encourages a goal-conditioned policy to imitate the subgoal-conditioned policy. Our intuition is that policies conditioned on nearby subgoals are more likely to be accurate than the policies conditioned on a faraway goal.
    In particular, we consider the imitation of policies conditioned on all the subgoals proposed by the graph-based planning algorithm. 
    \item [$\bullet$] \textbf{Execution\footnote{In this paper, we use the term ``execution'' to denote both (1) the roll-out in training phase and (2) the deployment in test phase.} with subgoal skipping:}
    As an additional technique that fits our self-imitation loss,
    we also propose \textit{subgoal skipping}, which randomizes a subgoal proposed by the graph-based planning to further improve the sample-efficiency.
    During the sample-collection stage and deployment stage, policies randomly ``skip'' conditioning on some of the subgoals proposed by the planner when it is likely that the learned policies can reach the proposed subgoals. Such a procedure is based on our intuition that an agent may find a better goal-reaching path by ignoring some subgoals proposed by the planner when the policy is sufficiently trained with our loss.
\end{itemize}

We demonstrate the effectiveness of \ALGname on various long-horizon continuous control tasks based on MuJoCo simulator \citep{todorov2012mujoco}. In our experiments, \ALGname significantly boosts the sample-efficiency of an existing GCRL method, i.e., mapping state space 
% (MSS; \citealt{huang2019mapping})
(MSS) \citep{huang2019mapping},\footnote{{We note that \ALGname is a generic framework that can be also incorporated into any planning-based GCRL methods, other than MSS.
%to improve their efficiency. 
Nevertheless, we choose MSS because it is one of the most representative GCRL works 
% in the literature 
as most recent works \citep{hoang2021successor, zhang2021world} could be considered as variants of MSS.
}} particularly in long-horizon tasks. For example, MSS + \ALGname achieves the success rate of 57.41\% in Large U-shaped AntMaze environment, while MSS only achieves 19.08\%. 
Intriguingly, we also find that the \ALGname-trained policy 
performs competitively 
even without any planner; 
this could be useful in some real-world scenarios where planning cost (time or memory) is expensive \citep{bency2019neural, qureshi2019motion}.
\input{figure/0_compositional}