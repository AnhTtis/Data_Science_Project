\section{Preliminary: goal-conditioned RL with graph-based planning}
\input{figure/1_concept}
%In this section, we describe the existing goal-conditioned reinforcement learning (GCRL) with a graph-based planning framework, upon which we build our work.
In this section, we describe the existing graph-based planning framework for goal-conditioned reinforcement learning, upon which we build our work.
To this end, in Section~\ref{sec:gcrl}, we describe the problem setting of GCRL. Next, in Section~\ref{sec:training}, we explain how to train the goal-conditioned policy using hindsight experience replay \citep{andrychowicz2017hindsight}. Finally, in Section~\ref{sec:planning}, we explain how graph-based planning can help the agent to execute better policy. We provide the overall pipeline in 
Algorithm~\ref{alg:framework} in Supplemental material~\ref{supp:algo_table}, colored as black.

\subsection{Problem description}
\label{sec:gcrl}

We formulate our control task as a finite-horizon, goal-conditioned Markov decision process 
% (MDP; \citealt{sutton2018reinforcement}) 
(MDP) \citep{sutton2018reinforcement}
as a tuple $(\mathcal{S}, \mathcal{G}, \mathcal{A}, p, r, \gamma, H)$ corresponding to state space $\mathcal{S}$, goal space $\mathcal{G}$, action space $\mathcal{A}$, transition dynamics  $p\left(s^\prime|s,a\right)$ for $s, s^{\prime} \in \mathcal{S}, a \in \mathcal{A}$, reward function $r\left(s, a, s^{\prime}, g\right)$, discount factor $\gamma \in [0,1)$, and horizon $H$. 

Following prior works \citep{huang2019mapping, zhang2021world}, we consider a setup where every state can be mapped into the goal space using a goal mapping function $\varphi:\mathcal{S}\rightarrow \mathcal{G}$. Then the agent attempts to reach a certain state $s$ associated with the target-goal $g$, i.e., $\varphi(s)=g$. For example, for a maze-escaping game with continuous locomotion, each state $s$ represents the location and velocity of the agent, while the goal $g$ indicates a certain location desired to be reached by the agent.

Typically, GCRL considers the reward function defined as follows:
\begin{align}
\begin{split}
\label{eq:reward}
    r(s,a, s^{\prime}, g) = \begin{cases}
                    0 \quad & \Vert \varphi(s') - g \Vert_{2} \leq \delta \\
                    -1 \quad & \text{otherwise}
               \end{cases}
\end{split}
\end{align}
where $\delta$ is a pre-set threshold to determine whether the state $s'$ from the transition dynamics $p(s^\prime|s, a)$ is close enough to the goal $g$. 
To solve GCRL, we optimize a deterministic goal-conditioned policy $\pi : \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$ to maximize the expected cumulative future return $V_{g, \pi} \left(s_{0}\right) = \sum_{t=0}^{\infty} \gamma^{t} r(s_{t}, a_{t}, s_{t+1}, g)$ where $t$ denotes timestep and $a_{t} = \pi(s_{t}, g)$.

\subsection{Training with hindsight experience replay}
\label{sec:training}
To train goal-conditioned policies, any off-the-shelf RL algorithm can be used.
Following prior works \citep{huang2019mapping, zhang2021world}, we use deep deterministic policy gradient 
% (DDPG; \citealt{Lillicrap2015continuous}) 
(DDPG) \citep{Lillicrap2015continuous}
as our base RL algorithm.
Specifically, we train an action-value function (critic) $Q$ with parameters $\phi$ and a deterministic policy (actor) $\pi$ with parameters $\theta$ given a replay buffer $\mathcal{B}$, by optimizing the following losses:
\begin{align}
\begin{split}
\label{eq:ddpg_critic}
    \mathcal{L}_{\mathtt{critic}} (\phi) = \mathbb{E}_{(s_{t}, a_{t}, r_{t}, g) \sim \mathcal{B}} \bigg[ (Q_{\phi}(s_{t}, a_{t}, g) - y_{t})^{2} \bigg] \\
    \text{where}\; y_{t} = r_{t} + \gamma Q_{\phi}(s_{t+1}, \pi_{\theta}(s_{t+1}, g), g) 
\end{split}
\end{align}
\begin{align}
\label{eq:ddpg_actor}
    \mathcal{L}_{\mathtt{actor}} (\theta) = - \mathbb{E}_{(s_{t}, a_{t}, g) \sim \mathcal{B}} [Q_{\phi}(s_{t}, \pi_{\theta}(s_{t}, g), g) ],
\end{align}
where the critic $Q_{\phi}$ is a universal value function approximator 
% (UVFA; \citealt{schaul2015universal})
(UVFA) \citep{schaul2015universal}
trained to estimate the goal-conditioned action-value.
However, it is often difficult to train UVFA because the target-goal can be far from the initial position, which makes the agents unable to receive any reward signal.
To address the issue, goal-relabeling technique proposed in hindsight experience replay 
% (HER; \citealt{andrychowicz2017hindsight}) 
(HER) \citep{andrychowicz2017hindsight}
is widely-used for GCRL methods.
The key idea of HER is to reuse any trajectory ending with state $s$ as supervision for reaching the goal $\varphi(s)$. This allows for relabelling any trajectory as success at hindsight even if the agent failed to reach the target-goal during execution.

\subsection{Execution with graph-based planning}
\label{sec:planning}
In prior works \citep{huang2019mapping, zhang2021world},
graph-based planning provides a subgoal, which is a waypoint to reach a target goal when executing a policy. 
A planner runs on 
% top of 
a weighted graph that abstracts visited state space.

\textbf{Graph construction.}
The planning algorithms build a weighted directed graph $\mathcal{H}=(\mathcal{V}, \mathcal{E}, d)$ where each node $l \in \mathcal{V} \subseteq \mathcal{G}$ is specified by a state $s$ visited by the agent, i.e., $l=\varphi(s)$. 
For populating states, we execute the two-step process following \citet{huang2019mapping}: (a) random sampling of a fixed-sized pool from an experience replay and (b) farthest point sampling \citep{vassilvitskii2006k} from the pool to build the final collection of landmark states.
Then each edge 
% $\{l^{1}, l^{2}\} \in \mathcal{E}$ 
$(l^{1}, l^{2}) \in \mathcal{E}$
is assigned for any pair of states that can be visited from one to another by a single transition in the graph. 
A weight $d(l^{1}, l^{2})$ is an estimated distance between the two nodes, i.e., the minimum number of actions required for the agent to visit node $l^{2}$ starting from $l^{1}$. Given $\gamma \approx 1$ and the reward shaped as in Equation~\ref{eq:reward}, one can estimate the distance $d(l^{1}, l^{2})$ as the corresponding value function $-V(s^{1}, l^{2}) \approx -Q_{\phi}(s^{1}, a^{1,2}, l^{2})$ where $l^{2} = \varphi(s^{2})$ and $a^{1,2} = \pi_{\theta}(s^{1}, l^{2})$ \citep{huang2019mapping}.
Next, we link all the nodes in the graph and give a weight $d (\cdot, \cdot)$ for each generated edge. 
Then, if a weight of an edge is greater than (pre-defined) threshold, cut the edge.
We provide further details of graph construction in Supplemental material~\ref{supp:graph_construction}.

\textbf{Planning-guided execution.}
The graph-based planning provides a policy with an emergent node to visit when executing the policy. 
To be specific, given a graph $\mathcal{H}$, a state $s$ and, a target goal $g$, 
we expand the graph by appending $s$ and $g$, and obtain
% one can obtain 
a shortest path $\tau_{g} = (l^{1},\ldots, l^{N})$ such that $l^{1} = \varphi(s)$ and $l^{N}=g$ using a planning algorithm. Then, a policy is conditioned on a nearby subgoal $l^{2}$, which is easier to reach than the faraway target-goal $g$. 
This makes it easy for the agent to collect successful samples reaching the target goals, leading to an overall performance boost.
Note that we re-plan for every timestep following prior works \citep{huang2019mapping, zhang2021world}.