
\documentclass{article} % For LaTeX2e

% if you need to pass options to natbib, use, e.g.:
    % \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

\usepackage{iclr2023_conference,times}
% \usepackage[square,numbers]{natbib}
% \bibliographystyle{abbrvnat}
% \bibliographystyle{agsm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb,graphicx,color,algorithm, algpseudocode,booktabs,bm,relsize,enumitem,multirow,amsthm,epsfig,caption}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{xspace}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{chngpage} 

\newcommand{\algname}{Planning-guided self-imitation learning for goal-conditioned policies\xspace}
\newcommand{\Algname}{\textbf{P}lanning-guided self-\textbf{I}mitation learning for \textbf{G}oal-conditioned policies\xspace}
\newcommand{\ALgname}{Planning-Guided Self-Imitation Learning for Goal-Conditioned Policies\xspace}
\newcommand{\ALGname}{PIG\xspace}
\newcommand{\LthreeP}{$L^{3}P$\xspace}

\definecolor{Blue9}{rgb}{0.098,0.3,0.9}
\definecolor{Red7}{rgb}{0.941, 0.243, 0.243}
\definecolor{Green7}{RGB}{55, 178, 77}
\definecolor{BrickRed}{rgb}{0.6,0,0}
\definecolor{RoyalBlue}{rgb}{0,0,0.8}
\definecolor{Tdgreen}{rgb}{0,0.4,0.7}
\definecolor{JSViolet}{RGB}{71,15,244}
\definecolor{JSRed}{RGB}{205,44,78}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{Teal4}{RGB}{56, 217, 169}
\definecolor{Cyan4}{RGB}{59, 201, 219}

\hypersetup{colorlinks=true}
\hypersetup{citecolor=JSViolet, linkcolor=JSRed}

\newcommand{\old}[1]{{\color{gray} #1}}
\newcommand{\highlight}[1]{{\color{JSViolet} #1}}
\newcommand{\junsu}[1]{{\color{magenta} #1}}
\newcommand{\sungsoo}[1]{{\color{cyan} #1}}
\newcommand{\younggyo}[1]{{\color{Blue9} #1}}

\title{Imitating Graph-Based Planning with \\ Goal-Conditioned Policies}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy

\author{Junsu Kim$^{1}$, Younggyo Seo$^{1}$, Sungsoo Ahn$^{2}$, Kyunghwan Son$^{1}$, Jinwoo Shin$^{1}$ \\
$^1$ Korea Advanced Institute of Science and Technology (KAIST) \\
$^2$ Pohang University of Science and Technology (POSTECH) \\
\texttt{\{junsu.kim, younggyo.seo, kevinson9473, jinwoos\}@kaist.ac.kr} \\
\texttt{sungsoo.ahn@postech.ac.kr}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Recently, graph-based planning algorithms have gained much attention to solve goal-conditioned reinforcement learning (RL) tasks:
they provide a sequence of subgoals to reach the target-goal, and the agents learn to execute subgoal-conditioned policies. However, the sample-efficiency of such RL schemes still remains a challenge, particularly for long-horizon tasks. 
To address this issue, we present a simple yet effective self-imitation
scheme which distills a subgoal-conditioned policy into the target-goal-conditioned policy.
Our intuition here is that to reach a target-goal, an agent should pass through a subgoal, so target-goal- and subgoal- conditioned policies should be similar to each other.
We also propose a novel scheme of stochastically skipping executed subgoals in a planned path, which further improves performance. 
Unlike prior methods that only utilize graph-based planning in an execution phase, our method transfers knowledge from a planner along with a graph into policy learning. 
We empirically show that our method can significantly boost the sample-efficiency of the existing goal-conditioned RL methods under various long-horizon control tasks.\footnote{Code is available at \url{https://github.com/junsu-kim97/PIG}}
\end{abstract}

\input{1_introduction}
\input{2_related_works}
\input{3_preliminary}
\input{4_method}
\input{5_experiment}
\vspace{-0.1in}
\section{Conclusion}
We present \ALGname, a new self-improving framework that boosts the sample-efficiency in goal-conditioned RL. We remark that \ALGname is the first work that proposes to guide training and execute with faithfully leveraging the optimal substructure property. Our main idea is (a) distilling planned-subgoal-conditioned policies into the target-goal-conditioned policy and (b) skipping subgoals stochastically in execution based on our loss term. We show that \ALGname on top of the existing GCRL frameworks enhances sample-efficiency with a significant margin across various 
% long-horizon 
control tasks.
Moreover, based on our findings that a policy could internalize the knowledge of a planner (e.g., reaching a target-goal without a planner), we expect that such a strong policy would enjoy better usage for the scenarios of transfer learning and domain generalization, which we think an interesting future direction.
% to explore.

\textbf{Limitation.} While our experiments demonstrate the \ALGname on top of graph-based goal-conditioned RL method is effective for solving complex control tasks, we only consider the setup where the state space of an agent is a (proprioceptive) compact vector (i.e., state-based RL) following prior works \citep{andrychowicz2017hindsight, huang2019mapping, zhang2021world}. 
In principle, \ALGname is applicable to environments with high-dimensional state spaces because our algorithmic components (self-imitation loss and subgoal skipping) do not depend on the dimensions of state spaces.
It would be interesting future work to extend our work into more high-dimensional observation space such as visual inputs.
We expect that combining subgoal representation learning \citep{nachum2018near, li2021learning} (orthogonal methodology to \ALGname) would be promising.


\section*{Reproducibility statement}
We provide the implementation details of our method in Section~\ref{sec:experiment} and Supplemental material~\ref{supp:impl}. We also open-source our codebase.

\section*{Ethics statement}
This work would promote the research in the field of goal-conditioned RL. However, there goal-conditioned RL algorithms could be misused; for example, malicious users could develop autonomous agents that harm society by setting a dangerous goal.
Therefore, it is important to devise an method that can take consideration of the consequence of its behaviors to a society.

\section*{Acknowledgments and Disclosure of Funding}
We thank Sihyun Yu, Jaeho Lee, Jongjin Park, Jihoon Tack, Jaeyeon Won, Woomin Song, Subin Kim, and anonymous reviewers for providing helpful feedbacks and suggestions in improving our paper. 
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST)).
This work was partly supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2022-0-00953,Self-directed AI Agents with Problem-solving Capability). This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government. (MSIT) (2022R1C1C1013366)

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\appendix
% \section{Appendix}
\input{6_appendix}

\end{document}
