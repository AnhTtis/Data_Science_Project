% the creation of new links is driven by reciprocation,
% preferential attachment [38], triangle closure \citep{leskovec2008microscopic}, and homophily [2, 58]. Globally, the number of edges in a social
% network grows superlinearly with its number of nodes, and
% the average path length shrinks with the addition of new
% nodes [34], after an initial expansion phase [1]. The regular patterns that drive the link creation process have enabled the development of accurate methods for link prediction and recommendation [28] based on either local [35] or
% global structural information [9, 8, 45]. Fine-grained temporal traces of user activity in online social platforms opened
% up new avenues to investigate in detail the impact of time
% on network growth [60]. For example, the relationship between the node age and its connectivity has been measured
% in several online social graphs including Flickr [33, 57]

% networks become denser:
% global properties change
% diameter shrinks
% superlinear number of edges
% small world phenomenin


% yw / below, now sure a good section title, but something like this would help make it clear that it is describing the methods for evaluation

\section{Methodology for evaluating delayed and indirect impacts}\label{sec:methodology}
% \propchange{To illustrate the complexities of evaluating recommendations in network in dynamic settings where there are feedback loops between natural evolution and algorithmic evolution ... The goal of this section is to provide a reasonable methodology ... TODO}
To illustrate temporal complexities of evaluating link recommendation algorithms in dynamic setting, we consider a stylized network evolution model. We propose an extension to the the classic Jackson-Rogers (JR) network evolution model \citep{jackson2007meeting}.  The JR model has been validated empirically and shown to display proprieties of real network such as decreasing diameter over time, increased edge densification and emergence of community structure \citep{jackson2007meeting, ruiz_stability_2017}. Our extension adds an optional recommendation phase to model the feedback loop between natural network evolution and algorithmic recommendations.

We analyze two simple baseline recommendation algorithms: a neighborhood-based and an affinity-based algorithm. While recommendation algorithms deployed in practice use both types of information, with potentially additional learning components, we opt for simplicity in recommendation algorithms to understand the underlying mechanisms behind evaluation phenomena in dynamic networks.
Algorithm \ref{alg:simulation} summarizes the simulation and evaluation procedure.

% To highlight aspects of temporal evaluation in dynamic setting, we propose a stylized network evolution model that explicitly considers feedback loops between natural network evolution and algorithmic recommendations. To that end, we propose an extension to the Jackson-Rogers \citep{jackson2007meeting} network evolution model to which we add an optional recommendation phase. 

% For illustrative purposes, we limit our study to two baseline recommendation algorithms: a neighborhood and an affinity based recommendations algorithm.To further emphasize simplicity:  we are using simple models for neighborhood and similarity based link recommenders. We choose simple baselines for illustrative purposes. 
% It's true that real social networks probably use more complex recommendation model; that combine in non-trivial ways affinity based information such as (type of the node, their expressed preferences) with neighborhood based information (who is this node connected to); however those 'real' models are opaque - the ones that we propose are simple and lead to unexpected temporal effects in evaluation. The simplicity is actually super valuable because it allows us to understand the mechanism and formulate hypotheses
% Having a network evolution model that combines simple natural growth models with simple and parsimonious interventions helps us understand, in a stylized setting, the mechanism behind delayed impacts and indirect effects. Understanding of these effects in stylized settings is an important first step for measuring them in real networks. 

% Simplicity is a design choice and comes in two flavors: first the use of a relatively simple network evolution modes in simulation and second the use of fairly simple recommendation algorithm that either rely on latent structure or affinity and one that only relies on neighborhood structure. This design allows us to understand the phenomena.
% \mc{Further defend the use of simulation: Synthetic graphs are important for answering the “what-if” scenarios especially when real data is missing.}
%We additionally model latent structure for nodes, which allows us to represent communities of varying levels of within-community and outside community link probabilities. This further allows us to capture within-community heterogeneity, and different levels of node activity. 
%As nodes with larger embedding norm will have higher expected number of neighbors and probability of connections between two nodes in the same group is higher for nodes with similar embeddings. 
%The natural evolution of the network is well defined even in the absence of algorithmic interventions. We introduce recommendations as an optional intervention at each time step.

% yw / above is there a reason why we need to show the "well-definedness" of the natural evolution? ----- previous works have a 'strawman' de-facto natural growth assumption - namely nothing changes -- or we add random edges -- here we say that even without recommendations this dynamic model is reasonable -- i.e. the growth model is not a post-facto consideration


% yw / below it is worth highlighting the uniqueness of the setup and algorithm. how it is different from existing approaches. right now it feels "similar to jackson-rogers model" but i think it is worth pointing out the difference and novelty.

\subsection{Dynamic network model}\label{subsec:model}
Let $G^t = (V^t, E^t)$ be an undirected network where $V^t$ and $E^t$ are the set of nodes and edges at time $t$. Nodes are characterized by their group identity $g_i$ and latent representation $v_i$. We assume group identities are static over time and latent representations are sampled from a group-specific distribution $v_i \sim \mathcal{D}_{g_i}$. At each time step, the network evolves via new nodes which first connect "organically" the existing nodes. Further at each step, the network evolves "algorithmically" via connections mediated by a link recommender.
% \pz{Should we also include how we initialize the edges? We currently initialize the edges with probability proportional to the sigmoid of inner product of latent vectors. We also implement the option to connect edges based on a fixed probability for each group}
% \mc{My overall intuition is that the intuition is not super important; but I guess we can mention that the initial network is initialized based on pairwise similarity (group based probabilities are a special case of probabilities based on inner product)}

\paragraph{Natural growth.}
Similar to the classic JR model, upon the arrival of a new node, the network evolves in two phases: "Meeting Strangers" and "Meeting Friends". In the "Meeting Strangers" phase, the new node makes connections with existing nodes at random. Unlike the classic model where connections are made with a fixed probability, we model connections probabilities based on latent representation of the nodes $v_i$. This additional modelling assumption allows us to consider various community structure.  In the "Meeting Strangers" phase, $N_s$ candidate nodes are sampled uniformly  from the existing network. The arriving node $i$ connects with each candidate node $j$ with probability proportional to the inner product of their respective latent embeddings: $p_{i,j} = \sigma(\langle v_i, v_j \rangle)$ where $\sigma(\cdot)$ is a scaled and translated sigmoid function\footnote{We consider $\sigma(x)=\frac{1}{1+e^{-ax + b}}$ where we set $a$ and $b$ to match desired average linkage probabilities.}. 

In the subsequent "Meeting Friends" phase, the incoming node $i$ makes additional connections based on neighborhood proximity. Here $N_f$ candidate nodes are sampled from the set of nodes at distance 2 (neighbors of neighbors) from the node $i$. The node $i$ connects with each candidate node $j$ with constant probability. Optionally, we model attrition effects by considering node departures from the network with a hazard function \footnote{$h(\mathbf{a}) = cd^{\mathbf{a}} + k$ where $\mathbf{a}$ is the age of the node and $c,d,k$ are tuned to model mean and variance of lifespans in the network.} that increases with the age of the node.

% yw / what does optionally mean here? %optionally means that we don't have to have recs at every time step for it to make sense to run the simulation -- so that we can run any treatment regime that we want

\paragraph{Algorithmic intervention.}
We introduce algorithmic intervention as a third "meeting recommendations" phase, whereby nodes in the network receive link recommendation. 
This phase applies not only to incoming nodes but also to existing nodes in the network. 
Given recommendation, nodes accept them according to a behavioral model. We consider neighborhood and affinity-based recommendations. The prototypical neighborhood recommendation is the friend-of-friend (\fof{}) recommendation where candidate nodes are selected uniformly from the set of nodes at distance 2: $\mathbb{P}(\textbf{rec}_i = j) = \frac{\textbf{1}(dist(i, j) = 2}{\sum_{j'}\textbf{1}(dist(i, j') = 2}$. Conversely, affinity-based (\latent{}) recommendations utilize the latent structure rather than local neighborhood structure to make recommendations. Specifically, the \latent{} recommender computes affinity scores for all the nodes in the network in terms of inner products and recommends candidates with probability proportional to the scores: $\mathbb{P}(\textbf{rec}_i = j) \propto e^{\beta s_j}$, where $s_j = \langle v_i, v_j\rangle$ is the embedding similarity between node $i$ and node $j$. High values of softmax temperature parameter $\beta$ correspond to more deterministic recommenders. Lower values of $\beta$ lead to more random recommendations and qualitatively capture the effects of estimation noise for learning-based recommenders.

\paragraph{Behavioral models.}
We consider two behavioral models for accepting link recommendations: the constant probability baseline and the embedding-based probability where nodes accept new links based on proximity in latent space. Embedding based probabilities model more granular notions of choice homophily \citep{asikainen2020cumulative}. We consider an additional behavioral option in which upon acceptance of a new recommended link, an edge is removed at random from the set of existing edges. This option is in line with several works which model recommendations as a \emph{rewiring} \citep{asikainen2020cumulative, fabbri_exposure_2021, santos_link_2021} process that does not impact the average degree.

\begin{algorithm}[ht]
\caption{Simulating network evolution}
\SetKwInOut{Input}{input}
\Input{initial $G_0$; time-steps $T$; hazard function; 
\textit{communities:} prevalence $c_g$, latent distribution $\mathcal{D}_g$;\\
\textit{natural growth:} $N_s$, $N_f$, dist-2 connect prob $p_2$;
\textit{intervention:}  window $[\underline{t},\overline{t}]$, \texttt{recommender}, \texttt{behavior};\\
}

\For{$t$ in $1\ldots T$}{
    \textbf{Natural growth}\;
    sample incoming node $i$ according to group prevalence $c_g$ and latent distribution $\mathcal{D}_g$\;
    strangers $\gets$ samples $N_s$ nodes\;
    \For{$s$ in strangers}{
        add edge $i-s$ with probability $\propto \langle v_i, v_s \rangle$
    }
    friends $\gets$ samples $N_f$ neighbors of neighbor nodes\;
    \For{$f$ in friends}{
        add edge $i-f$ with probability $p_2$
    }
    \If{\upshape{$t\in [\underline{t},\overline{t}]$}}
    { \textbf{Algorithmic intervention}\;
    \For{node $j$ in treatment group $G^{treatment}_t$}{
        candidate = \texttt{recommender}($j, G_t$)\;
        \If{\upshape{\texttt{behavior}($j$, candidate) = accept}}{ add edge $j-$candidate}
    }
    }
    nodes to remove $\gets$ hazard function
}
\end{algorithm}\label{alg:simulation}


\subsection{Structural metrics}\label{subsec:metrics}

% We evaluate the impact of recommendations on network and community structure through several structural metrics:

\emph{Clustering coefficient}: Clustering coefficient of a node is the ratio of triangles it forms $\Delta_i$ and the maximum number of triangles it could have formed given it's current degree $d_i$: $c_i = \frac{2\Delta_i}{d_i(d_i-1)}$. The clustering coefficient captures the micro-level cohesion in the network \citep{ferrara2022link}. The average clustering coefficient is the metric averaged over all the nodes. Averaging the metric over communities measure clustering at the community level.% \mc{Further we can average over any group of nodes --- relevant for A/B study sim}  %We expect neighborhood based recommendations to close more triangles and increase clustering whereas embedding based recommendations are more likely to make links that decrease the clustering of the graph.

\emph{Gini coefficient}: We measure the inequality in the degree distribution via Gini coefficient. An increase of the Gini coefficient resulting from the use of recommendations is often used to demonstrate biases in link recommendation \citep{ferrara2022link, fabbri_exposure_2021, fabbri2022exposure}. The Gini coefficient can be computed for an ordered list of node degrees as: $G = \frac{2\sum_{i=1}^n id_i}{n \sum_{i=1}^n d_i} - \frac{n+1}{n}$. Similarly, this metric can be computed for the entire graph or restricted to communities.

\emph{Homophily}: We define a monocromatic and bichromatic edges to be an edges that links two nodes from the same community or nodes from different communities, respectively. The homophily of a community measures the propensity of nodes to favor within group connections compared to a non-preferential baseline. We compute the homophily of a community $g$ as follows: $H_g = \frac{\vert E_{gg}\vert}{\vert E_g\vert} - \frac{n_g}{n}$, where $\vert E_{gg}\vert$ denotes the number of monochromatic edges within $g$, $\vert E_{g}\vert$ denotes the number of total edges that have at least one node in community $g$. Finally  $n_g$ denotes the size of $g$, and $n$ denotes the total size of the network, the ratio $\frac{n_g}{n}$ is the baseline fraction of within-group links when nodes have no group-based preferences. The vast majority of existing simulation based evaluation study changes in homophily or the closely related notions of within-community and between-community edges \citep{fabbri_effect_2020, abebe_effect_2022, daly2010network}.


%\emph{Betweeness centrality}: Given a list of all shortest paths between nodes. Betweeness centrality of a node $i$ is defined as the fraction of shortest paths that pass through $i$. Similarly the betweeness centrality of an edge $i-j$ is the fractions of shortest paths containing the edge. Further we can restrict the betweeness notion to consider only paths between nodes of different communities. This notion captures the importance of each node for mediating connections between communities.

    % \item Adamic-Adar Index: The Adamic-Adar Index (AAI) supposes that nodes with substantial shared links are more likely to generate additional links. It is defined as follows: $$ A(x, y) = \sum_{u\in N(x)\cap N(y)} \frac{1}{\text{log}|N(u)|} $$ where $N(v)$ is the neighbors of node $v$. (Something about contrasting with other link predictions) <----- Not a metric

% \subsection{Dynamics}
% We utilize a 3-phase dynamic model to simulate the evolution of our social network. This process is based on a variation of the Jackson-Rogers model. Upon arrival, each node first forms a link through a random meeting phase with another preexisting node; the node then runs a network search phase, during which it expands its set of neighbors by looking for neighbors of neighbors to connect with. 

% We adapt this process by 1) accounting for heterogeneous nodes (different color, vector, etc.), 2) adding a third phase for our recommendation system that is applied at each time step globally, and 3) adding a hazard rate function for each node according as a property of age. 

% If we are given a dataset with a matching structure, we may apply our dynamics with and without recommendation as required. Assuming we are not given a network however, we first initialize the nodes of our network randomly as follows. We first choose $M$ different colored groups each with $s_k$ nodes to be represented with a randomized mean vector. For each node within the group, we sample from a multivariate normal distribution and assign its vector property. The nodes are thus grouped both by node coloration, but also in $n-$dim space by Euclidean distance. Each node will also be given an age property based on whichever age distribution best represents our hypothetical network. Finally, we assign edges based on a probability function $p(v_i, v_j)$ that can be changed depending on if we want our probabilities to be based on vector properties or graph properties. Afterwards, for each time step, we have $n_t$ new nodes join the graph sequentially. We then apply the following phases:

% \begin{enumerate}
%     \item Initialization: Each incoming node is assigned an age, group, and randomized vector as described in our initialization. We then compute the probability of connection between the incoming node and every other node, then randomly sample the probabilities to choose the initial set of connections being made. We may choose to cap the number of total connections being made if desired.
%     \item Neighbors of neighbors: We create the neighbors of neighbors set and select for additional connections similar to above. Once again, we can choose under what conditions the nodes accept and reject neighbors of neighbors. Note that since this is a structural property of the graph, the impact of the vector representations is lessened.
%     \item Recommendation: We apply a recommendation algorithm on nodes in the network; this can either be done to a subset (such as all nodes below a certain age) or to every node in the network. Our recommendation algorithms are currently based on local community properties. For analyzing network dynamics without recommendations, we choose to skip this step.
%     \item Cleanup: At the end of each phase, we advance the age of every node by one timestep;  we then utilize a hazard function $f(A)$ as a function of age $A$, to compute the probability of death for every node in the network, thereby removing nodes and all former connections from our network. For analyzing networks on smaller timescales where there is no hazardous action, we similary choose to skip this step.
% \end{enumerate}

% \subsection{Difference}
% Here we see three major deviations between our dynamics and the Jackson Rogers model:
% \begin{enumerate}
%     \item We only consider reciprocal relationships. As such we can only model certain social relations, such as Facebook friends but cannot capture the idea of follows on Instagram.
%     \item we endow nodes with types and to allow for nodes of different types to connect with different probabilities such that we can study group dynamics. Typically groups are modeled with stochastic block models (SBMs); a drawback of SBMs is that they are rarely dynamic formation networks. Our model is dynamic by design and unlilke SBMs groups can range in how alike the nodes are since nodes are modeled by embedding vectors
%     \item we additionally add a process for the removal of nodes: nodes 'die' independently of any other process in the network -- this actually ensures that any network dynamic will be stationary -- and such it serves as an enabler of causal measurement of the impact of recommendation
% \end{enumerate}
% Current papers utilize (...) to do (...)


% \subsection{Properties of the model}

% Our model has several key characteristics that make it suitable for the analysis of recommendation impacts on networks:

% -> Can flexibly model community structure. It allows for different connection strengths within groups and between groups that can be easily expressed through embeddings (See C.2). Thus different setups for group homophily can be expressed in this network model (See C.1.) 

% -> Heavy right tails, the Jackson Rogers Model displays indirect preferential attachment. (See \citep{ruiz2017stability})

% -> Triangle rich network through phase 2 (Meeting Friends of friends). (Low dimensional embedding networks typically fail to capture triangle rich networks \citep{seshadhri2020impossibility}) 

% -> Having an age component to nodes models more realistically network scenarios in which nodes first arrive and then leave the network.

\subsection{Evaluation}\label{subsec:evaluation}
%We track the trajectory for a variety of structural metrics for a given network evolution model varying the length of the evaluation interval.  
%For a given network model and a recommendation algorithm we track counterfactual trajectories for the metrics above. In order to quantify the delayed and indirect impacts we consider treatment regimes with various intervention intervals.

We denote by $\mathcal{G}$ the evolution of the network under natural dynamics
and by $\mathcal{G}(\texttt{rec},[\underline{t}, \overline{t}])$ the evolution 
when the network receives link recommendation according to 
$\texttt{rec}$ algorithm over the $[\underline{t}, \overline{t}]$ intervention interval. The subscript $T$ in $\mathcal{G}_T(\cdot)$ is used to denote the snapshot of the network at time $T$. For a structural metric $m$, we denote by $m(\mathcal{G}_t(\texttt{rec},[\underline{t}, \overline{t}]))$ the value of this metric evaluated at time $t$. Given a network evolution model, we simulate the counter-factual trajectories of the metrics for intervention intervals  $[\underline{t}, \overline{t}]$ of different lengths. 


%To evaluate delayed impacts of recommendations, we vary the duration of the intervention $[\underline{t}, \overline{t}]$ and the time of the measurement $T$. Measurements of structural metrics during the intervention capture immediate impacts. Conversely, measurements after the intervention has stopped captures the delayed impacts of the intervention.
%A way in which in practice people try to get around it is via A/B studies in which a fraction of the nodes receives recommendations and the remaining nodes are control. Since nodes are connected in the network; the treated nodes will have spillover effects or externalities due to network . There is a cottage industry of papers proposing both design (how treatment nodes are chosen) and estimation schemes for modeling or correcting for these externalities. To study longitudinal evaluation is enough to just consider the treatment counter-factual. Whereas to study A/B evaluations we consider a node randomization and cluster randomization scheme. We will evaluate the 'wrongness' og A/B stuff by showing how the trajectory over metrics deviate more and more compared to ground truth counter-factual over time ...



% \mc{Given a network evolution model and proposed algorithmic intervention we examine the impact of the intervention by tracking counterfactual trajectories of the metrics above. To do so we consider applying the intervention to node subsets of varying size over intervention intervals of varying time.}
% \mc{ In simulating longitudinal studies, we treat the whole network, whereas in simulating A/B tests, we only apply recommendations to a subset of the nodes. \pz{It feels that the "A/B tests" occur very sudden here since we haven't talk about A/B tests. Maybe we want to remove it and we add a section titled "A/B tests" to describe the treatment groups.} \mc{yea you are right: i was thinking of changing this a little bit and defining 'treatment nodes' at the beginning of the paragraph. And then specify that in a longitudinal exp all nodes are treatment nodes whereas in an A/B study only a subset of nodes are treatment. The other option that I was considering is to define the concept of intervention at the level of the node and then in the next section where we talk about evaluations we can say that in A/B evaluations we apply alg intervention only to a subset of nodes. Lmk what you think}}
% \mc{Define a formal graph process}

\paragraph{Total effects.}
The total effect of intervening in a network evolution $\mathcal{G}$ with recommender \texttt{rec} for intervention interval $[\underline{t}, \overline{t}]$ on the metric $m$ at time $T$ is defined as the difference between two counterfactual measurements:
\begin{align*}
    \text{Effect}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]) := m(\mathcal{G}_T(\texttt{rec},[\underline{t}, \overline{t}])) - m(\mathcal{G}_T).
\end{align*}
This definition compares between the ``treatment'' universe where the whole network received an algorithmic intervention and a counterfactual ``control'' universe where the network evolved without algorithmic recommendations.


\paragraph{Delayed effects.}
When the measurement occurs during the intervention interval, i.e. $T \in [\underline{t}, \overline{t}]$ we call the effect \emph{immediate}. Conversely, when the measurement occurs after the intervention stops, i.e. $T > \overline{t}$, we can define the notion of \emph{delayed} impact as: 
$$\text{DelayedEffect}_{T}(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]) = \text{Effect}_{T}(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]) - \text{Effect}_{\overline{t}}(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]) $$

The notion of delayed impacts allows us to characterize the impact of link recommendations into three broad categories: diminishing, amplifying and persistent based on the sign of $\text{DelayedEffect}_{T}(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}])$. Note that the delayed impacts measure the difference in effect sizes between time $\overline{t}$ and time $T$, rather than the difference in the metric $m(\mathcal{G}_T(\texttt{rec},[\underline{t}, \overline{t}]))-m(\mathcal{G}_{\overline{t}}(\texttt{rec},[\underline{t}, \overline{t}]))$. This two notions are equivalent only when the metric remains constant from time $\overline{t}$ to time $T$ under natural network evolution dynamics.

\paragraph{Indirect effects.}\label{subsec:indirect_methodology}

The temporal evolution of the network in the presence of link recommendations is affected \emph{directly} by the addition of algorithmic edges; but also \emph{indirectly} as the addition of algorithmic edges biases the natural growth dynamics. Recommendations have an indirect impact on natural dynamics in the "Meeting Friends" phase of the network evolution model. In this phase, the arriving nodes connect with nodes at distance 2 (neighbors of neighbors). In the presence of algorithmic edges, there can be nodes at distance 2 that are only reachable via algorithmic edges. If the incoming node connects to such a node, the resulting edge is \emph{mediated} by recommendation. Figure \ref{fig:direct_indirect} illustrates the mechanism through which recommendations mediate the organic creation of new edges.

%The significance of indirect effects can be quantified by measuring the \emph{prevalence} and the \emph{persistence} of mediated edges among the edges created during phase 2 of natural growth. The prevalence is calculated as the fraction of mediated edges among all edges created in the "Meeting Friends" stage of natural growth. Furthermore, mediated edges are said to be persistent if their prevalence does not diminish in the long run, even after recommendation interventions are stopped. 

%While prevalence and persistence are interesting in their own right, our goal is to determine whether biasing the natural growth dynamics recommendations can indirectly impact the structural properties of the network.

%\pz{This sentence is a bit confusing. Is it biasing the recommendation or natural growth dynamics, or biases in the recommendation or natural growth dynamics} \mc{yea; I agree -- this is too convoluted; this sentence is trying to ask the question: "We know that these algorithmic interventions are biasing the natural dynamics -- can we quantify how this bias in the natural dynamics reflects on our metrics of interest -- does this make more sense?"} 
To measure indirect impacts, we design a counterfactual experimental procedure to remove the influence of algorithmic edges on natural growth, and thus discount mediated edges. The counterfactual procedure is defined as follows: upon arrival of a new node $i$, we modify the "Meeting Friends" phase to only consider node candidates that are at distance 2 from $i$ via non-algorithmic edges. For this counterfactual, the network contains no mediated edges, and therefore it isolates the \emph{direct} effects. Formally, we refer to this unmediated evolution as $\tilde{\mathcal{G}}(\texttt{rec},[\underline{t}, \overline{t}])$.  
%We note that a counterfactual that removes mediated edges would inadvertently decreas the edge density in the network.  In this setup we define indirect effects on structural metrics as the difference between total effects and direct effects:
Finally we can define direct effects as the difference between the metric evaluated for the unmediated counterfactual and the natural growth trajectory. The indirect effects are the difference between total effect and direct effect:
\begin{align*}
\text{DirectEffect}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]) &= m(\tilde{\mathcal{G}}_T(\texttt{rec},[\underline{t}, \overline{t}])) - m(\mathcal{G}_T),\\
\text{IndirectEffect}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]) &=  \text{Effect}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}])  - \text{DirectEffect}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]).
\end{align*}

%Finally, we study the relative importance of indirect and direct effects over time.
% we find that indirect effects explain an increasing fraction of the gap.


% For example when other works say that because of the addition of algorithmic edges the network homophily has increased assume implicitly that sans recommendations would either stay constant or it would evolve in ways which would keep the metric of interest constant. This is very often not the case. The natural evolution dynamics in networks are sometimes even more prone to segregation. Thus algorithmic growth should be evaluate with respect to what would have happened without recommendations. In a simulation setting with well specified natural evolution model we can specify that. 

% When evaluating metrics we are interested in temporal and in various treatment regimes. Do measurements of a network metric right after intervention differ substantially from measurements later on? If so this creates interesting questions about how to quantify the impact of recommendation in the context of time-scale dependence

% All of this is possible to study in our setting because we have a setting where natural growth interleaves with recommendation growth. Structure of the network affects properties of the recommendations which in turn affect properties of the network.
% \subsubsection{Temporal dynamics}
% For a given experimental setting we compute our metrics across time. To understand temporal aspects of the immediate and long term impacts we consider treatment regimes and observation windows of various lengths.

% \subsubsection{Indirect effects}
% Prevalence of indirect edges
% Are indirect effects attenuating or amplifying effects of rec?

\begin{figure}
    \centering
    \includegraphics[trim={5cm 5.5cm 8cm 9cm}, clip, width=0.9\linewidth]{figures/indirect_effects.pdf}
    \caption{Indirect effects of recommendation though mediated edges: Edge $A-B$ is a recommended edge and contributes to the direct effect of the recommendation. Edge $F-A$ is a mediated edge as node $A$ would not be reachable from $F$ in the absence of $A-B$.
    %Node $F$ would not have been able to connect to node $A$ since if edge $A-B$ was not present then node $A$ would not have been a neighbor of $B$. The addition of edge $F-A$ is a second order effect mediated by the edge $A-B$.\propchange{TODO: change indirect to mediated -- edges are (un)mediated -- effects are (in)direct}
   % \pz{can we change the caption "phase 1" and "phase 2" on the graph to "Meeting Strangers" and "Meeting Friends" phases, to match with our description}
    } 
    \label{fig:direct_indirect}
\end{figure}

%These set of counter-factual simulations allows us to establish the ground truth. However in practice when evaluating impacts of link recommendation algorithms in real network one does not have access to all the counter-factual. In the case of longitudinal evaluation we only have the intervention path and not the control path; we don't know what would have happened to the network under natural growth dynamics.


\paragraph{Longitudinal evaluation.}
In a simulated setting, one has access to both counterfactuals: the evolution of the network in the presence of recommendations and in their absence. In longitudinal evaluation which are commonly used in observational studies, effects are measured as the difference in a metric before and after the intervention:
$$\widehat{\text{Effect}}^{obs}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]):= m(\mathcal{G}_T, \texttt{rec},[\underline{t}, \overline{t}]) - m(\mathcal{G}_{\underline{t}})$$
This measure is biased whenever  $ m(\mathcal{G}_{T}) \ne m(\mathcal{G}_{\underline{t}}) $.

\paragraph{A/B evaluation.}
The lack of valid counterfactuals motivates the use of A/B testings in settings with interventional access. In an A/B test, nodes are divided into two groups: treatment and control; the treatment group receives recommendations, while the control group does not. However, in networks, the assumptions of Stable Unit Treatment Value Assumption (SUTVA) \citep{imbens_causal_2015} do not hold due to network interference. There are various methods for choosing treatment nodes, as well as methods to correct for network interference in estimation procedures.

Let a scheme for choosing treatment nodes exist and let $\mathcal{G}^{AB}(\texttt{rec}, [\underline{t}], \overline{t})$ be the network evolution where a group of nodes are assigned to the treatment group and receive recommendations. To estimate the impact of recommendations on a metric of interest, we compare the metric's values on the treatment and control groups:

$$\widehat{\text{Effect}}^{AB}_T(\mathcal{G},\texttt{rec},[\underline{t}, \overline{t}]):= m^{treatment}(\mathcal{G}^{AB}_T, \texttt{rec},[\underline{t}, \overline{t}]) -  m^{control}(\mathcal{G}^{AB}_T, \texttt{rec},[\underline{t}, \overline{t}]). $$

In evaluating the impact of recommendations, we consider both naive estimators that do not correct for network interference, and more sophisticated ones that take externalities into account.
