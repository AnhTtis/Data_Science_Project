\section{Introduction}
%%%%%%% have a look at this %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\url{https://www.youtube.com/watch?v=kyWyp-d0r94&ab_channel=TheConferenceonFairness%2CAccountability%2CandTransparency%28FAT%2a%29}
%https://www.linkedin.com/posts/ieaitum_ieai-2022-research-brief-ai-auditing-activity-6963460881514954752-snzA?utm_source=linkedin_share&utm_medium=android_app
% Intro to fairness and bias
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\columnwidth]{images/flowchart.png}
    \caption{Demographic Parity Inspector($g_\psi$) workflow. The training of models is shaded in red while the derivations of explanations  are shaded in blue. The  model $f_\theta$ is learned based on training data, $\{(x_i,y_i)\}$,  and outputs the explanations $\Ss(f_\theta,X)$. The \enquote{DP Inspector} receives the explanations to predict the protected attribute, $Z$. Then if the AUC is above $0.5$ then there is a demographic parity violation. We can interpret the reasons for demographic parity violations on $g_\psi$ with explainable AI techniques}
    \label{fig:workflow}
\end{figure}


% What is the problem?
There exists a range of metrics (demographic parity, equal opportunity, average absolute odds \ldots) that judge the \emph{global} degree of unfairness arising from the use of machine learning models. Together with some predictive performance measures (accuracy, precision, AUC \ldots), these metrics are commonly used to deliver classifiers that achieve high degrees of predictive performance and fairness e.g.\ \cite{zafat2017fairnessDisparate,negligible2021}.
% Why is it interesting??
Unfortunately,  approaches that enforce low measures of unfairness at the global level may cause new kinds of unfairness \emph{at the subgroup level}. As \cite{ruggieri23}
shows, pushing for well-balanced fairness among groups at such a global scale may imply novel discrimination at
inter-group levels leading to new kinds of unfairness introduced by an erroneous use of fair-AI methods. 

% Why is it hard?
The Yule’s effect~\cite{yule1900vii,simpson1951interpretation,pearson1899vi} occurs when positive and negative associations between the model predictions $f(X)$ and the protected attribute $Z$ cancel out producing a vanishing correlation in the mixture of the distribution. It occurs whenever we aim at group fairness, such as independence $f(X)\perp Z$, but we wrongly disregard control for the protected attribute $Z$. Fair machine learning algorithms may result in disparate effects on separate distributions, with some subgroups impacted positively (higher fairness) and other subgroups impacted negatively (lower fairness) \cite{ruggieri23}.
For example, enforcing fairness for one discriminated ethnic group at the country level may imply novel kinds of unfairness for the subgroup of this ethnicity in a particular local state.


% Pitfall of previous work 
Previous methods of measuring demographic parity have relied on the predictions in the output space whose low dimensionality is prone to fall under Yule's effect, and on statistical measures on input data that is model-independent. Furthermore, on many occasions detecting and quantifying fairness violations is not enough, there is also the need to pinpoint  \textit{What are the specific sources of discrimination?}. 

% What do we do
We address this problem by providing a novel kind of demographic parity measurement that analyzes how different protected groups are treated at the distributional and instance level and how each feature contributes to the demographic parity violation. This approach relies on the explanation space, a novel data space that has theoretical guarantees while performing demographic parity inspections.  Our main contributions are:
\begin{itemize}
    \setlength\itemsep{0.1em}

    \item We use the explanation space to measure fairness by providing theoretical guarantees and experiments with synthetic data and real data use cases.% The explanation space is more sensitive and indicative than input or prediction data spaces.

    %\item We overcome the Yule–Simpson's effect of demographic parity measurements on the predictions space.\carlos{I have no clue}
    
    \item We introduce a \enquote{Demographic Parity Inspector} method that allows for interpretable fairness quantification that sheds insights on the root causes of unfairness.

    \item We release an open-source Python package\footnote{to be released upon acceptance}, which implements our \enquote{Demographic Parity Inspector} that is \texttt{scikit-learn} compatible~\cite{pedregosa2011scikit}, together with code and tutorials for reproducibility.
    
\end{itemize}

