\section{Experiments}
We divide the experiments into two sections: a first section, using synthetic data, where we compare the \enquote{Demographic Parity Inspector} to previous methods for measuring demographic parity violations, and a second one, presenting use-cases on real data, that shows the auditing and inspection results of our approach. Thus, we show the reliability of the \enquote{Demographic Parity Inspector} in real datasets.

\subsection{Experiments with synthetic data}

The experiments of this section aim to show the sensibility of our method for false positives and true positives when detecting demographic parity violations.

To generate a synthetic dataset for both cases, we first draw $10,000$ samples from a  normal distribution $X_1 \sim N(0,1), X_2 \sim N(0,1), (X_3,X_4) \sim  N\left(\begin{bmatrix}0  \\ 0 \end{bmatrix},\begin{bmatrix}1 & \gamma \\ \gamma & 1 \end{bmatrix}\right)$. We then define a binary feature $Z$ with values $1\quad\texttt{if} \quad X_4>0,\quad \texttt{else}\quad 0$. We compare the fairness auditing methods while increasing the correlation $\gamma = r(X_3, Z)$ from 0 to 1. In both cases, our model $f_\beta(X_1, X_2, X_3)$, is a function over the domain of the features. 
Further, we introduce two experimental scenarios:


 
\textbf{Demographic Parity violation in the data and in the model (Indirect)}: In this case, there is a demographic parity violation in the input data that is learned by the model. 
The predictor's domain is $(X_1, X_2,X_3)$, and the features are independent of each other. The protected attribute is $Z$ (binary-valued) and its correlation with the predictor's domain $(X_3, Z)$ is parameterized by $\gamma(X_3, Z)$, allowing to adjust for discrimination. To generate the synthetic demographic parity violation in the model we create the target $Y = \sigma(X_1 + X_2 + X_3)$, where $\sigma$ is the logistic function


\textbf{Demographic Parity violation in the data but not in the model (Uninformative)}: In this case, the demographic parity violation on the input data remains the same but the relationship of the target variable changes, it is now independent of the protected attribute.
The target function is defined as $Y = \sigma(X_1 + X_2)$, and, the $\gamma$ parameter allows adjusting for discrimination in the training data even if the model does not capture it. The target is completely independent of the protected attribute, $Y \perp X_3 \Rightarrow Y \perp Z \Rightarrow f_\beta \perp Z$.



As ablation studies, we compare the \enquote{Demographic Parity Inspector}, $g_\psi$, that learns on the explanations space (eq. \ref{eq:fairDetector}) against learning on the following other spaces: on the input data space, to detect if there is information about the protected attribute in the data,$\Upsilon = \argmin_{\tilde{\Upsilon}} \sum_{(x, z) \in \Dd{val}
} \ell( g_{\tilde{\Upsilon}}(\textcolor{blue}{x}) , z )$
%\begin{gather}
%\Upsilon = \argmin_{\tilde{\Upsilon}} \sum_{(x, z) \in \Dd{val}
%} \ell( g_{\tilde{\Upsilon}}(\textcolor{blue}{x}) , z )
%\end{gather}
On the prediction space, to detect the protected attribute on the model predictions, suffering from Yule's paradox $\upsilon = \argmin_{\tilde{\upsilon}} \sum_{(x, z) \in \Dd{val}
} \ell( g_{\tilde{\psi}}(\textcolor{blue}{f_\theta(x)}) , z )$
%\begin{gather}
%\upsilon = \argmin_{\tilde{\upsilon}} \sum_{(x, z) \in \Dd{val}
%} \ell( g_{\tilde{\psi}}(\textcolor{blue}{f_\theta(x)}) , z )
%\end{gather}
And in the combination of both, where it learns the protected attribute in the input data and in the model predictions $\phi = \argmin_{\tilde{\phi}} \sum_{(x, z) \in \Dd{val}
} \ell( g_{\tilde{\phi}}(\textcolor{blue}{\{f_\theta(x),x\}}) , z )$. It overcomes the lack of dimensions of the prediction space case, but it will capture situations when there is bias in the data, that is not on the model (see Table~\ref{tab:AuditDetector}).
%\begin{gather}
% \phi = \argmin_{\tilde{\phi}} \sum_{(x, z) \in \Dd{val}
%} \ell( g_{\tilde{\phi}}(\textcolor{blue}{\{f_\theta(x),x\}}) , z )
%\end{gather}



\begin{figure*}[ht]
\centering
\includegraphics[width=.42\textwidth]{images/fairAuditSyntheticCaseA.png}
\includegraphics[width=.42\textwidth]{images/fairAuditSyntheticCaseB.png}
\caption{In the left figure, \enquote{indirect} experimental case. All spaces capture this fairness violation, only the prediction space is less sensitive due to its dimensionality. In the right figure, \enquote{uninformative} experimental case, in this case, only the explanation space, and prediction space detect that the model is non-discriminant. Measuring DP via the explanation space correctly detects false positives (DP in the data, but not on the model), and it is more sensitive to true positives.}
\label{fig:fairSyn}
\end{figure*}

In Figure \ref{fig:fairSyn} and in Table \ref{tab:AuditDetector} we present and compare the different experiments on synthetic data. We say that a method is \textit{Accountable} if the feature attributions identified are the ones that indeed contribute towards the synthetically generated discrimination for both methods.


\begin{table}[ht]
\caption{Conceptual table of \enquote{Demographic Parity Inspector} \ref{sec:demographicParityInspector} over the different spaces for the two synthetic examples of Figure \ref{fig:fairSyn}. The \enquote{Accountability} column refers to the ability of the algorithm to provide insights into the sources of discrimination regarding demographic parity violations of the model, and for the protected attribute.}\label{tab:AuditDetector}
\begin{tabular}{c|cccc}
\textbf{Learning Space} & \textbf{Indirect}        & \textbf{Uninform.} & \textbf{Accountable}  \\ \hline
Input        $g_\Upsilon$  & \cmark & \xmark   & \xmark \\
Prediction   $g_\upsilon$  & $\sim$ & \cmark   & \xmark \\
Input + Pred. $g_\phi$  & \cmark & \xmark   & \xmark \\
Explanations $g_\psi$  & \cmark & \cmark   & \cmark
\end{tabular}
\end{table}

\subsection{Use Case: US Income data}
\begin{figure*}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/detector_auc_ACSIncome.png}
\includegraphics[width=.49\textwidth]{images/feature_importance_ACSIncome.png}
\caption{In the left figure the AUC of the \enquote{DP Inspector} over different pairs of protected attributes on the US Income dataset. Different pair-wise comparisons achieve different degrees of Demographic Parity Violation. On the right figure, the Wasserstein distance between the coefficients of the model \enquote{DP Inspector} between different pairs and the randomly distributed pair. Higher values imply that there is a higher probability that the feature causes demographic parity violations}\label{fig:xaifolks}
\end{figure*}

In this section, we provide experiments on the US Income data set, \footnote{Please see the ACS PUMS data dictionary for the full list of variables available \url{https://www.census.gov/programs-surveys/acs/microdata/documentation.html}} which is derived from the US census data~\cite{ding2021retiring}. 
We divide the dataset into three equal splits $\Dd{tr},\Dd{val},\Dd{te} \subseteq \D$ and select our protected attribute, $Z$, to be a feature that indicates the ethnicity of an individual. We train our model $f_\beta$ on $\{X^{tr},Y^{tr}\}$ 
and, the \enquote{Demographic Parity Inspector} $g_\psi$ on $\{\Ss(f_\beta,X^{val}),Z^{val}\}$. Both methods are evaluated on $\{X^{te},Z^{te},y^{te}\}$. For the type of models, $f_\beta$, as we focus on tabular data, we choose $f_\theta$ to be a \texttt{xgboost}\cite{xgboost} that achieve state-of-the-art model performance~\cite{grinsztajn2022why,DBLP:journals/corr/abs-2101-02118,BorisovNNtabular}, and for the inspector $g_\psi$ a logistic regression. The final explanations are given by the coefficients of the logistic regression.

%The hypothesis is that if the performance of the \enquote{Demographic Parity Inspector} is high then there exists a demographic parity violation.
As the baseline (\enquote{no discrimination}) w.r.t. the US income data set, we shuffle the protected attribute and fit the \enquote{Demographic Parity Inspector} to predict the randomly assigned protected attribute.  We then compare pair-wise between the different ethnicity combinations (see Figure\ref{fig:xaifolks}).

For the inspection part, we calculate the Wasserstein distance between the coefficients of the linear regression on the baseline and coefficients of the different pair-wise comparisons. The statistical test between the coefficients is performed within 100 bootstraps of a fraction of $0.632\%$~\cite{statisticallearning} of both coefficient distributions (random and ethnicity pairs). The AUC of the model $f_\beta$ is $0.87$ while for the DP Inspector, $g_\psi$ it depends on which pairs of ethnicity it is trained.
In Figure \ref{fig:xaifolks} (left), we can see the performance of the \enquote{Demographic Parity Inspector} ranging from values between $0.60$ and $0.80$.  We observe how the \enquote{DP Inspector} identifies \enquote{Education} as a highly discriminatory proxy while the use of the feature \enquote{Worked Hours Per Week} is less discriminatory. 
In Figure \ref{fig:xaifolks} (right), the values correspond to the Wasserstein distance between the randomly assigned coefficient distributions, and the pairwise comparisons. 
Higher values imply that the coefficients are more distinct, which suggests that there is a higher probability that these features are the ones causing the demographic parity violation. In the appendix, we provide analysis across different prediction tasks.