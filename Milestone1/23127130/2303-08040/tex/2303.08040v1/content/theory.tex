\section{Theoretical Analysis}

Our theoretical analysis consists of three parts. The first one studies the relation of the explanation of the protected attribute and DP on the prediction space. The second one focuses on the DP on explanation space, relating it to DP on the prediction space and on the input data. The third part provides a mathematical analysis on specific scenarios for the \enquote{Demographic Parity Inspector}. Throughout this section, we assume an exact calculation of the Shapely values $\Ss(f_\beta, x))$ for an instance $x$, possibly for the interventional (\ref{def:val:int}) or the observational variant (\ref{def:val:obs}).

\subsection{Explanations of the protected feature and DP}\label{sec:expSpaceIndependence}

This first question we consider is whether by looking at the Shapely values of the protected feature is a viable approach to test for DP on the prediction space. 
%section studies the relationship between the Shapley values of the protected attribute and the demographic parity violations of a model. %We denote by $\mathbf{X}$ a collection of attributes (random variables) $X_1, \ldots, X_n$. 
%Without any loss of generality, we can assume that $E[X_i] = 0$ for the predictive feature $X_i$'s. 
The following result considers a linear model (with unknown coefficients) over \textit{independent} features. In such a case, resorting to Shapley values leads to an exact test. In the following, we write $\mathit{distinct}({\cal D}_i)$ for the number of distinct values in the $i$-th feature of dataset ${\cal D}$, and $\Ss(f_\beta, {\cal D})_i \equiv 0$ if the Shapley values of the $i$-th features are all $0$'s.

\textbf{Lemma 1.} Consider a linear model $f_\beta(x) = \beta_0 + \sum_j \beta_j \cdot x_j$. Let $Z$ be the $i$-th feature, i.e. $Z = X_i$, and let ${\cal D}$ be a dataset
such that $\mathit{distinct}({\cal D}_i)>1$.
If the features in $X$ are independent, then $\Ss(f_\beta, {\cal D})_i \equiv 0 \Leftrightarrow f_\beta(X) \perp Z$.\\
\textbf{Proof.}
It turns out $\Ss(f_\beta, x)_i = \beta_i \cdot (x_i - E[X_i])$ % = \beta_i \cdot x_i$ (since we assume $E[X_i] = 0$). 
This holds in general for the interventional variant (\ref{def:val:int}), and for independent features, also for the observational
variant (\ref{def:val:obs})
\cite{DBLP:journals/ai/AasJL21}. Since $\mathit{distinct}({\cal D}_i)>1$, we have that $\Ss(f_\beta, {\cal D})_i \equiv 0$ iff  $\beta_i = 0$. By independence of $X$, this is equivalent to $f(X) \perp X_i$, i.e., $f(X) \perp Z$.
\mbox{}\hfill QED\\

In words, the test in the lemma consists of checking that the Shapley values of the protected feature are zero's, i.e., that the protected feature does not contribute to the output of the model. 
%
Such a result does not extend to the case of dependent features. Consider $Z = X_2 = X_1^2$, and the linear model $f(x_1, x_2) = \beta_0 + \beta_1 \cdot x_1$, with $\beta_1 \neq 0$ and $\beta_2 = 0$.
In the interventional variant, since such a model does not use $x_2$, by the Uninformativeness property, $\Ss(f_\beta, x)_2 = 0$. However, clearly $f(X_1, X_2) = \beta_0 + \beta_1 \cdot X_1 \not \perp X_2$.
For the observational variant, \cite{DBLP:journals/ai/AasJL21} show that:
\begin{gather}\label{eq:sdep}
val(T) = \sum_{i \in N\setminus T} \beta_i \cdot E[X_i| X_T = x^{\star}_T] + \sum_{i \in T} \beta_i \cdot x^{\star}_i\end{gather}
%
%\begin{gather}
%v(\emptyset) = \beta_0 + \beta_1 \cdot E[X_1] + \beta_2 \cdot E[X_2]\\
%v(\{1\}) = \beta_0 + \beta_1 \cdot x^{\star}_1 + \beta_2 \cdot E[X_2|x_1 =x^{\star}_1]\\
%v(\{2\}) = \beta_0 + \beta_1 \cdot E[X_1|x_2 =x^{\star}_2] + \beta_2 \cdot x^{\star}_2\\
%v(\{1, 2\}) = \beta_0 + \beta_1 \cdot x^{\star}_1 + \beta_2 \cdot x^{\star}_2
%\end{gather}
%
\noindent from which, we calculate:
\begin{gather}\nonumber
%\Ss(f, x^{\star})_1 = \beta_1 \cdot x^{\star}_1  - \frac{\beta_1}{2} \cdot E[X_1|x_2 =x^{\star}_2]\\
\Ss(f, x^{\star})_2 = \frac{\beta_1}{2} E[X_1|X_2 =x^{\star}_2] 
\end{gather}
We have $\Ss(f, {\cal D})_2 \equiv 0$ iff $E[X_1|x_2 =x^{\star}_2] = 0$ for all $x^{\star}$ in ${\cal D}$. For the following marginal distribution $P(X_1=v) = 1/4$ for $v=1, -1, 2, -2$, since $X_2=X_1^2$, it holds that $E[X_1|x_2 =v] = 0$. Thus $\Ss(f, {\cal D})_2 \equiv 0$. However, again $f(X_1, X_2) = \beta_0 + \beta_1 \cdot X_1 \not \perp X_2$.

%We observe that:
%$$\Ss_2(x^{\star}) - \Ss_1(x^{\star}) = \beta_2 \cdot x^{\star}_2 - \beta_1 \cdot x^{\star}_1$$
The counterexample shows that focusing only on the Shapley values of the protected feature is not a viable way to reason about DP of the model on the prediction space.
%$$\Ss_1(x^{\star}) + \Ss_2(x^{\star}) = f(x^{\star}) - \Ss_0 = %\beta_0 - \Ss_0 + \beta_1 \cdot x^{\star}_1 + \beta_2 \cdot x^{\star}_2$$
%
%Both in the intervential and in the observational case, a zero value for the protected feature does not imply independence of the model's output from the protected feature.
%Looking at the Shapley values of the protected attribute suffers two shortcomings: it's nulity does not imply indepedence $\Ss_z(f_\theta,X)=0\centernot\implies f_\theta \perp X$, and calculating the SHAP of a feature not present in training data is non-trivial and relies on approximations~\cite{DBLP:journals/corr/ShapTrueModelTrueData}. To overcome those issues we look at the Shapley values of the whole space, not just at the Shapley value contribution of the protected attribute. Our approach relies on learning the protected attribute in the explanation space, in the next section we will discuss some of the properties of the approach.

\subsection{DP on the explanation and prediction spaces}

This section is divided into two parts that comprise the theoretical comparisons of DP on  the explanations space against $(i)$ the prediction space and $(ii)$ the input data.

%\subsubsection{Explanation vs Predictions spaces}

%Recall that Demographic Parity (DP) consists of independence $f_\theta(X) \perp Z$. 
We start by observing that DP on the explanation space is a sufficient condition for DP in the prediction space.

\textbf{Lemma 2.} If $\Ss(f_\theta, X) \perp Z$ then $f_\theta(X) \perp Z$.\\
\textbf{Proof.}
By the propagation of independence in probability distributions, this implies $(\sum_i \Ss_i(f_\theta, X) + c) \perp Z$ where $c$ is any constant. By setting $c=E[f(X)]$ and by the efficiency property (\ref{eq:eff}), we have the conclusion. \mbox{}\hfill QED\\

Therefore, a DP violation on the prediction space is also a DP violation in the explanation space, which accounts then for a stricter notion of fairness.
%
The other direction does not hold. We can have dependence of $Z$ from the explanation features, but the sum of such features cancel out resulting in perfect DP on the prediction space. This issue is also known as the Yule's effect~\cite{ruggieri23}.

\textbf{Example:} \textit{Consider the model $f(x_1, x_2) = x_1 + x_2$. Let $Z \sim Ber(0.5)$, $A \sim U(-3, -1)$, and $B \sim N(2, 1)$ be independent, and let us define:
\[ X_1 = A \cdot Z + B \cdot (1-Z) \quad X_2 = B \cdot Z + A \cdot (1-Z)\]
We have $f(X_1, X_2) = A + B \perp Z$ since $A, B, Z$ are independent.
%
Let us calculate $\Ss(f, X)$ in the two cases $Z=0$ and $Z=1$. If $Z=0$, we have $f(X_1, X_2) = B + A$, and then $\Ss(f, X)_1 = B-E[B] = B-2 \sim N(0, 1)$ and $\Ss(f, X)_2 = A-E[A] = A+2 \sim U(-1, 1)$. Similarly, for $Z=1$, we have $f(X_1, X_2) = A + B$, and then $\Ss(f, X)_1 = A-E[A]=A+2 \sim U(-1, 1)$ and $\Ss(f, X)_2 = B-E[B] = B-2 \sim N(0, 1)$. This shows:
\[ P(\Ss(f, X) | Z=0) \neq P(\Ss(f, X) | Z=1)\]
and then $\Ss(f, X) \not \perp Z$. Notice this example holds both for the interventional and the observational cases, as we exploited Shapley values of a linear model over independent features, namely $A, B, Z$.
}

%A general underpinning of measuring demographic parity in the prediction space is this second scenario, where the different contributions of each feature for the prediction cancel out and the total prediction remains the same $P(f_\theta(X)|Z=0)=P(f_\theta(X)|Z=1)$, even though the model is dependent on the protected attribute  $f_\theta \perp Z$ . 
 

%\subsubsection{Explanation space vs Input data}

Statistical dependence between the input space, $X$, and the protected attribute $Z$, is another technique, which, however, disregards the model $f_\theta$. For fair models, which are able not to (directly or indirectly) rely on $Z$, such a technique can lead to false positive detection of DP violation. %, as shown in the next example. %when a statistical test recognizes differences between the distributions of the protected attributes, though the differences do not affect the model~\cite{grinsztajn2022why}. 

\textbf{Example:}\textit{
Let $X = X_1,X_2,X_3$ be independent features 
such that $E[X_1] = E[X_2] = E[X_3] = 0$, and $X_1, X_2 \perp Z$, and $X_3 \not \perp Z$. The target feature is defined as $Y = X_1 + X_2$, hence it is also independent from $Z$. Assume a linear regression model $f_\beta(x_1, x_2,x_3) = \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_3 \cdot x_3$ trained from a sample data from $(X, Y)$ with $\beta_1, \beta_2 \approx 1$ and $\beta_3 \approx 0$. Intuitively, this occurs when a number of features are collected to train a classifier without a clear understanding of which of them contributes to the prediction. It turns out that $X \not \perp Z$ but, for $\beta_3 = 0$ (which can be obtained by some fairness regularization method \cite{6137441}), we have $f_\beta(X_1, X_2, X_3) = \beta_1 \cdot X_1 + \beta_2 \cdot X_2 \perp Z$. By reasoning as in the proof of Lemma 1, we have $\Ss(f_\beta, X) = (\beta_1 \cdot X_1, \beta_2 \cdot X_2, 0)$ and then $\Ss(f_\beta, X) \perp Z$. This holds both in the interventional and in the observational variants.}

The above represents an example where the input data depends on the protected feature, but the model and the explanations are independent. In this case, the explanation space correctly detects that there is no DP violation on the model.

\subsection{DP inspecting via the explanation space}

We provide here the theoretical foundations about the \enquote{Demographic Parity Inspector}. First, using the Bayesian optimal classifier as inspector, we show that if the performance of the \enquote{Demographic Parity Inspector} is null, then there is DP on the prediction space. 
Secondly, we show how from an interpretable inspector, such as a linear model, we can derive information on the source of the DP violation.

\subsubsection{The Bayesian Optimal Classifier as an auditor}\label{sec.theory.bayesian.optimal}

Let us start with an equivalent condition of the DP in the explanation space.

\textbf{Lemma 3.}
$\Ss(f_\theta, X) \perp Z$ iff $P(Z=1|\Ss(f_\theta, X))=c$ almost surely for some constant $c$.\\
\textbf{Proof.} 
By definition of independence, for any value $s$ with non-zero probability $P(\Ss(f_\theta, X)=s) > 0$,
$\Ss(f_\theta, X) \perp Z$ iff $P(Z=1|\Ss(f_\theta, X)) = P(Z=1)$ and $P(Z=0|\Ss(f_\theta, X)) = P(Z=0)$. Since $Z$ is binary valued, the second equivalence holds iff $P(Z=1|\Ss(f_\theta, X))$ $=$ $P(Z=1)$. This shows the conclusion, and also that $c$ must necessarily be $c=P(Z=1)$.
%
%First, it is necessarily the case that $c = P(Z=1)$. In fact: $P(Z=1) = \sum_s P(Z=1|\Ss(f_\theta, X)=s) \cdot P(\Ss(f_\theta, X)=s) = c \sum_s  P(\Ss(f_\theta, X)=s) = c$. 
%Using the Bayes formula, we have:
%$P(\Ss(f_\theta, X)=s|Z=1) = P(Z=1|\Ss(f_\theta, X)=s) \cdot P(\Ss(f_\theta, X)=s)/P(Z=1) = P(Z=1) \cdot P(\Ss(f_\theta, X)=s)/P(Z=1) = P(\Ss(f_\theta, X)=s)$. Similarly, we have: $P(\Ss(f_\theta, X)=s|Z=0) = P(Z=0|\Ss(f_\theta, X)=s) \cdot P(\Ss(f_\theta, X)=s)/P(Z=0) = (1-P(Z=1|\Ss(f_\theta, X)=s)) \cdot P(\Ss(f_\theta, X)=s)/P(Z=0) = (1-P(Z=1)) \cdot P(\Ss(f_\theta, X)=s)/P(Z=0) = P(\Ss(f_\theta, X)=s)$.
%This shows $\Ss(f_\theta, X) \perp Z$.
 \mbox{}\hfill QED\\

Another equivalent formulation can be stated in terms of AUC of the Bayes Optimal Classifier, which then provides a way of testing for the DP violation in the explanation space. Also, by Lemma 2, we have a sufficient condition for testing the DP in the prediction space. Putting together these observations we obtain the following key result.

\textbf{Theorem 1.}\textit{
Assume that the \enquote{Demographic Parity Inspector} $g_\psi$ is the Bayes Optimal Classifier, i.e., such that:
\[ g_\psi(s) = P(Z=1|\Ss(f_\theta, X)=s)  \]
The AUC of $g_\psi$ is $0.5$ iff $\Ss(f_\theta, X) \perp Z$. Moreover, if the AUC of $g_\psi$ is $0.5$, then $f_\theta(X) \perp Z$.}\\
\textbf{Proof.} 
First, we observe that the AUC of $g_\psi(s)$ is $0.5$ iff $g_\psi(s)$ is constant almost surely. In fact, 
the AUC \cite{DBLP:conf/ijcai/GaoZ15} of $g_\psi$:
\begin{align*}
AUC = E[I( (Z-Z')(g_\psi(\mathbf{X})-g_\psi(\mathbf{X}') > 0) +\\
\quad \quad 0.5 \cdot I(g_\psi(\mathbf{X})=g_\psi(\mathbf{X}')) | Z \neq Z'] 
\end{align*}
is bounded from below by $0.5$, since $E[I( (Z-Z')(g_\psi(\mathbf{X})-g_\psi(\mathbf{X}') > 0) | Z \neq Z'] \geq 0$ by definition of $g_\psi$.
By Lemma~3, $AUC=0.5$ is then equivalent to $\Ss(f_\theta, X) \perp Z$. Moreover, if $AUC=0.5$, we have $\Ss(f_\theta, X) \perp Z$, and then, by Lemma 2, $f_\theta \perp Z$.
 \mbox{}\hfill QED\\

The Bayes Optimal Classifier maximizes the AUC \cite{DBLP:conf/ijcai/GaoZ15}. Hence, there is DP violation in the explanation space iff any inspector has an AUC larger than $0.5$. Moreover, if all inspectors have an AUC of 0.5 or smaller, then there is a DP violation in the prediction space. Notice, however, that all the above theoretical results refer to the population statistics, while in experiments we consider estimators of those statistics over samples of the population and approximate calculations of the explanation space.
 
%The Bayesian Optimal Classifier is the one that satisfies the first equation~\cite{devroye2013probabilistic,DBLP:conf/nips/HardtPNS16}, then if the probability of belonging to any of the two classes is equivalent, then the predictions of the auditor are independent of the protected attribute, which implies that the distributions of the explanations is independent of the protected attribute
%\begin{gather}
%\Ss(f_\beta,X)\perp Z   
%\end{gather}



\subsubsection{Linear models and linear inspectors}

This example showcases one of our main contributions: detecting the source of demographic parity violation. For this, we have used the basic case of i.i.d. data and linear models. In the following experimental sections, we will provide the experiments with real-data and non-linear models.

\textbf{Example:} \textit{Let $X = (X_1, X_2, X_3)$ with $X_1, X_2, X_3$ independent variables with $E[X_i] = 0$, and such that $X_3 \not \perp Z$. %Assume now that $Y = \alpha_0 + \alpha_1 \cdot X_1 + \alpha_2 \cdot X_2 + \alpha_3 \cdot X_3$. 
Given a random sample of i.i.d. observations from $(X, Y)$, a linear model $f_\beta(x_1, x_2, x_3) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_3 \cdot x_3$ can be built by OLS (Ordinary Least Square) estimation. By reasoning as in the proof of Lemma 1, $\Ss(f_\theta, x)_i = \beta_i \cdot x_i$. Consider now a linear \enquote{Demographic Parity Inspector} $g_\psi(s) = \psi_0 + \psi_1 \cdot s_1 + \psi_2 \cdot s_2+\psi_3 \cdot s_3$, which can be written in terms of the $x$'s as: $g_\psi(x) = \psi_0 + \psi_1 \cdot \beta_1 \cdot x_1 + \psi_2 \cdot \beta_2 \cdot x_2+\psi_3 \cdot \beta_3 \cdot x_3$. By OLS estimation properties, we have $\psi_1 \approx cov(\beta_1 \cdot X_1, Z)/var(\beta_1 \cdot X_1) = cov(X_1, Z)/(\beta_1 \cdot var(X_1))  = 0$ and analogously $\psi_2 \approx 0$. Finally, $\psi_3 \approx cov(X_3, Z)/(\beta_3 \cdot var(X_3)) \neq 0$. In summary,  the coefficients of $g_\psi$ provide information on which feature contributes (and how much it contributes) to the dependence between the prediction $f_\beta(X)$ and the protected feature $Z$.}

%In this case, we provide the mathematical analysis extending from the previous IID section (cf. Section \ref{sec:expSpaceIndependence}), but one of the features is correlated with the protected attribute which is not present on the training data. Let us consider a simple linear model $f_\beta(x_1, x_2,x_3) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2+\beta_3 \cdot x_3$ with $X_1 \perp X_2 \perp X_3$ independent but $X_3$ and $Z$ dependent by the parameter $\gamma(X_3,Z)$. Let $(X_1,X_2) \sim  N\left(0,\begin{bmatrix}1 & 0 \\0& 1 \end{bmatrix}\right)$ and $(X_3,X_4) \sim  N\left(0,\begin{bmatrix}1 & \gamma \\\gamma& 1 \end{bmatrix}\right)$, then $Z$ is a binary protected attribute that takes value  $1\quad\texttt{if} \quad X_4>0,\quad \texttt{else}\quad 0$. Then our synthetic target $y = \alpha_1\cdot X_1+ \alpha_2\cdot X_2+ \alpha_3\cdot X_3+\epsilon$, where $\epsilon$ is white noise $\epsilon \sim N(0,0.1)$. Let us also consider a linear auditor $g_\psi = \psi_1 \cdot x_1 + \psi_2 \cdot x_2+\psi_3 \cdot x_3$. 
%\begin{theorem}
%If $X_3$ is not independent of the protected attribute $Z$ and the rest of the features are independent of the protected attribute $X_1 \perp Z$ and $X_2 \perp Z$, then for the coefficients of the auditor ($g_\psi$) only $\psi_3$ is not null, in formulas: if $\gamma(X_3,Z)\neq  0 $, and $ X_1 \perp X_2 \perp Z \Rightarrow \psi_1=0,\psi_2=0,\psi_3\neq0$ 
%\end{theorem}
%If the variables are IID and the models $f_\beta$ and $g_\psi$ belong to the linear regression family then we can calculate the exact Shapley values~\cite{DBLP:journals/ai/AasJL21}.
%\begin{gather}
%\texttt{If}\quad X_1\perp Z, X_2\perp Z,  X_3\not\perp Z\\ 
%\texttt{and}\quad y = \alpha_1 X_1+\alpha_2 X_2+\alpha_3 X_3 + \epsilon\\
%\Rightarrow f_\beta(x) = \beta_1 \cdot x_1+\beta_2\cdot x_2+\beta_3\cdot x_3\\
%\Ss_j(f_\beta,x) = \beta_j\cdot x_j
%\end{gather}
%Then we can train our \enquote{Demographic Parity Inspector}  $g_\psi$ on the explanation space to predict the protected attribute: 
%\begin{gather}
%\psi = \argmin_{\psi} \sum_{n\in X^{\text{val}}} \ell( g_{\psi}(\Ss(f_\beta,x^n)) , z^n )\\
%g_\psi = \psi_1 \Ss_1(f_\beta,x_i)+\psi_2 \Ss_2(f_\beta,x_i)+\psi_3 \Ss_3(f_\beta,x_i)\\
%\end{gather}
%Then since the features $X_1$ and $X_2$ are independent among themselves and w.r.t the protected attribute $P(S_i(f_\beta,X)|Z))=P(S_i(f_\beta,X))$ we have that
%\begin{gather}
%\Rightarrow \psi_1 = 0, \psi_2 = 0 , \psi_3 \neq 0
%\end{gather}
%Since we know the dependence of the feature $X_3$ with the protected attribute $Z$ is parametrized by $\gamma(X_3,Z)$, then the $\psi_3$ coefficient will be proportional to this relationship
%\begin{gather}
%    \psi_3 \propto \gamma(X_3,Z)
%\end{gather}

