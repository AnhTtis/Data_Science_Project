\section{Foundations and Related work}\label{sec:relatedWork}
This section introduces the foundations of fairness and explainable AI needed to establish the context within which this contribution is made. We then compare our work to the existing one and outline the differences.

\subsection{Explainable AI: Shapley values }
Explainability has become an important concept in legal and ethical guidelines for data processing and machine learning applications ~\cite{selbst2018intuitive}. A wide variety of methods have been developed, aiming to account for the decision of algorithmic systems ~\cite{guidotti_survey,DBLP:conf/fat/MittelstadtRW19,DBLP:journals/inffus/ArrietaRSBTBGGM20}. 
One of the most popular approaches to explainability in machine learning (ML) has been Shapley values
%\footnote{Shapley values have become one of the most common explainable AI methods with open-sourcesoftware implementations such as SHAP reaching a total download of 85 million}. 
These values are used to attribute relevance to features according to how the model relies on them ~\cite{shapTree,lundberg2017unified}. Shapley values are a coalition game theory concept  that aims to allocate the surplus generated by the grand coalition in a game to each of its players~\cite{shapley}. 
%
% formal definitions

For set of players $N = \{1, \ldots, p\}$, and a value function $\val:2^N \to \mathbb{R}$, the Shapley value $\mathcal{S}_j$ of the $j$'th player is defined as the averaged additional contribution of player $j$ in all possibles coalitions of players:
\begin{small}
\[
\mathcal{S}_j = \sum_{T\subseteq N\setminus \{j\}} \frac{|T|!(p-|T|-1)!}{p!}(\val(T\cup \{j\}) - \val(T))
\]
\end{small}
In the context of machine learning models, players correspond to features $X_1, \ldots, X_p$, and the contribution of the feature $X_j$ is with reference to the prediction of a model $f$ for an instance $x^{\star}$ to be explained. Thus, we write $\mathcal{S}(f, x^{\star})_j$ for the Shapley value of feature $X_j$ in the prediction $f(x^{\star})$. We denote by $\mathcal{S}(f, x^{\star})$ the vector of Shapely values $(\mathcal{S}(f, x^{\star})_1, \ldots, \mathcal{S}(f, x^{\star})_p)$. 
%\steffen{It is clear to me what $f(x)$ is, but what is $f(x^*,X_{N\setminus T})$? $X_{N\setminus T}$ is a distribution, how does it affect $f$? In(2), I consider $x^*$ to be a constant, but what should be an expected value if there is no variable that samples from a distribution? Should it be rather $E_{X_{N\setminus T}}$ to indicate the sampling?}
There are two variants for the term $\val(T)$~\cite{DBLP:journals/ai/AasJL21,DBLP:journals/corr/ShapTrueModelTrueData,Zern2023Interventional}: the \textit{observational} and the \textit{interventional}. When using the observational conditional expectation, we consider the expected value of $f$ over the joint distribution of all features conditioned to fix features in $T$ to the values they have in $x^{\star}$:
\begin{equation}\label{def:val:obs}
\val(T) = E[f(x^{\star}_T, X_{N\setminus T})|X_T=x^{\star}_T]
\end{equation}
where $f(x^{\star}_T, X_{N\setminus T})$ denotes that features in $T$ are fixed to their values in $x^{\star}$, and features not in $T$ are random variables over the joint distribution of features.
Opposed, the interventional conditional expectation considers the expected value of $f$ over the marginal distribution of features not in $T$: % and with features in $T$ being fixed to the values they have in $x^{\star}$:
\begin{equation}\label{def:val:int}
\val(T) = E[f(x^{\star}_T, X_{N\setminus T})]
\end{equation}
In the interventional variant, the marginal distribution is unaffected by the knowledge that $X_T=x^{\star}_T$. In general, the estimation of (\ref{def:val:obs}) is difficult, and some implementations (e.g., SHAP) actually consider (\ref{def:val:int}) as the default one. In the case of decision tree models, TreeSHAP offers both possibilities.


%\vspace{1cm}
%represents the prediction for the feature values in $T$ that are marginalized over features that are not included in $T$:
%\begin{gather}
%\mathrm{val}_x(T) = E_{X_{N\setminus T}}[\hat{f}(X)|X_T=x_T]-E_X[\hat{f}(X)]
%\end{gather}

The Shapley value framework is the only feature attribution method that satisfies the properties of Efficiency, Symmetry, Uninformative and Additivity~\cite{molnar2019,shapley,WINTER20022025,aumann1974cooperative}.  We recall next the key property of efficiency and uninformative:

\begin{description}
    \item[Efficiency]  Feature contributions add up to the difference of prediction for $x^{\star}$ and the expected value of $f$:
\begin{gather}\label{eq:eff}
    \sum_{j \in N} \Ss(f, x^{\star}) = f(x^{\star})_j - E[f(X)])
\end{gather}
\end{description}
The following property only holds for the interventional variant (SHAP values), but not for the observational one:
\begin{description}
\item[Uninformative] A feature $X_j$ that does not change the predicted value (i.e., for all $x, x'_j$: $f(x_{N\setminus \{j\}}, x_j) = f(x_{N\setminus \{j\}}, x'_j)$)
%
%-- regardless of which coalition of feature values it is added to -- 
 have a Shapley value of zero, i.e., $\Ss(f, x^{\star})_j = 0$.
\end{description}
%
%: Efficiency, Symmetry, Uninformative and Additivity ~\cite{molnar2019,shapley}. In this work we exploit the theoretical properties that Shapley values satisfy, to provide theoretical guarantees towards fairness audits.
%
%\textbf{Uninformative} A feature $j$ that does not change the predicted value -- regardless of which coalition of feature values it is added to -- should have a Shapley value of zero.\\
%
In the case of a linear model $f_\beta(x) = \beta_0 + \sum_j \beta_j \cdot x_j$, the SHAP values turns out to be $\Ss(f, x^{\star}) = \beta_i(x^{\star}_i-\mu_i)$ where $\mu_i = E[X_i]$. For the observational case, this holds only if the features are independent \cite{DBLP:journals/ai/AasJL21}.


%

%For simplicity, the Shapley value can be untangled  in cases where the model is linear regression where, and the features are independent, the Shapley value can be estimated by $\Ss(f_\theta, x_i) = a_i(x_i-\mu_i)$, where $a_i$ are the coefficients of the linear model and $\mu_i$ the mean of the features \cite{DBLP:journals/corr/ShapTrueModelTrueData}.

\subsection{Fairness notions}
Various definitions of fairness in machine learning have been proposed (see, e.g.,~\cite{mehrabi2021survey,finocchiaro2021bridging, barocas-hardt-narayanan} for recent overviews).

Algorithmic fairness can be decomposed into two central notions: \textit{between-group}, also known as group fairness, and a \textit{within-group} component or individual fairness \cite{DBLP:conf/kdd/SpeicherHGGSWZ18}. While the \textit{within-group} or individual notions of fairness emphasize on {\it treating similar individuals similarly}~\cite{DworkHPRZ2012_IndFair}, \textit{between-group} or group fairness notions aim to establish some form of parity between groups of individuals based on shared sensitive attributes like gender or race.

% Intro to group fairness
%The principle of group fairness aims to establish some form of parity between groups of individuals, based on protected or sensitive attributes like gender and race. 
Selecting a measure to compare fairness between two sensitive groups has been a highly discussed topic, where results such as~\cite{DBLP:journals/bigdata/Chouldechova17,DBLP:conf/nips/HardtPNS16,DBLP:conf/innovations/KleinbergMR17}, have highlighted the impossibility to satisfy simultaneously three type of fairness measures: demographic parity~\cite{DworkHPRZ2012_IndFair}, equalized odds~\cite{DBLP:conf/nips/HardtPNS16}, and predictive parity~\cite{DBLP:conf/kdd/Corbett-DaviesP17,DBLP:journals/corr/axa,wachter2020bias}. In this work, we focus on demographic parity as fairness metric, because it is not error dependent but relies specifically on the model predictions, allowing us to create \enquote{early warnings} in advance to avoid building/deploying machine learning models undesired behavior.


\subsection{Related work}

\subsubsection{Fairness measuring and auditing}
%% Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem https://facctconference.org/static/pdfs_2022/facct22-126.pdf?mc_cid=b1a21b7286&mc_eid=fa6fa6c768
% read \cite{DBLP:journals/aiethics/Ugwudike22}
% and Shea Brown, Jovana Davidovic, and Ali Hasan. 2021. The algorithm audit: Scoring the algorithms that score u
%We use the terms \enquote{AI audit} and \enquote{algorithmic audit} interchangeably to refer to a process, according to specific criteria, the behaviour of ML model is accounted~\cite{DBLP:conf/fat/Costanza-ChockR22}. 

In this work, we rely on the following definition of audit studies: \enquote{Audits are tools for interrogating complex processes, often to determine whether they comply with company policy, industry standards
or regulations}~\cite{liu2012enterprise,DBLP:conf/fat/RajiSWMGHSTB20}. 

Algorithmic audits are closely linked to audits studies as understood in the social sciences, with a strong emphasis on social justice and participatory action \cite{DBLP:conf/eaamo/VecchioneLB21}.
A recent survey on public algorithmic audit identified four categories of \enquote{problematic machine behaviour} that can be unveiled by audit studies: discrimination, distortion, exploitation and misjudgement \cite{DBLP:journals/pacmhci/Bandy21}. This taxonomy is highly helpful when putting forward auditing studies, and also relevant for this work: our \enquote{Demographic Parity Inspector} can be seen as a tool to help understanding discrimination in ML models, thus, it falls into the first category.

Previous work has relied on measuring and calculating demographic parity on the model predictions~\cite{DBLP:conf/fat/RajiSWMGHSTB20,kearns2018preventing}, or on the input data~\cite{DBLP:journals/cviu/FabbrizziPNK22,DBLP:conf/fat/YangQ0DR20,DBLP:conf/emnlp/ZhaoWYOC17}.  In this work, we perform demographic parity measures on the explanation space, a novel model projection space that allows us to overcome sensibility issues and Yule's paradox w.r.t fairness on the prediction space and avoid false positives when there is bias in the data but there is no demographic parity violation on the model. 



%discussing the production guidelines~\cite{lo2020ethical}, and/or reviewing/discussing software engineering practices~\cite{10.1145/3419764,the_turing_way_community_2019_3233986}.

%%%% Check this: TO DO 
%%% https://arxiv.org/pdf/2004.07999.pdf
\subsubsection{Explainability and fair supervised learning}

The intersection of fairness and explainable AI has been an active topic in recent years. For example, ~\cite{DBLP:conf/ssci/StevensDVV20} presents an approach to explaining fairness based on adapting the Shapley value function to explain model unfairness. They also introduce a new meta-algorithm  that considers the problem of learning an additive perturbation to an existing model in order to impose fairness. In our work we do not adopt the Shapley value function, instead, we use the theoretical Shapley properties to provide fairness auditing guarantees. Our \textit{Demographic Parity Inspector} is not perturbation based but uses Shapley values to project the model to the explanation space, and then measures demographic parity violations. It also allows us to pinpoint what are the specific features driving this violation.
 
 
In ~\cite{DBLP:conf/fat/GrabowiczPM22}, the authors present a post-processing method based on Shapley values aiming to detect and nullify the influence of a protected attribute on the output of the system. For this, they assume there are direct causal links from the data to the protected attribute and that there are no measured confounders. Our work does not make use of causal graphs but exploits the theoretical properties of the Shapley values in order to obtain fairness model auditing guarantees.

Other works have relied on other explainability techniques such as counterfactual \cite{DBLP:conf/nips/KusnerLRS17,9750356,DBLP:conf/aies/MutluYG22}, in our work we don't focus on counterfactual explanations but on providing fairness auditing guarantees for demographic parity using feature attribution explanations.