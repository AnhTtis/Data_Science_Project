\section{True to the model or true to the data?}
The \enquote{Demographic Parity Inspector} proposed in this work relies on the explanation space that satisfies efficiency and uninformative theoretical properties. 
We have used the Shapley values as an explainable AI method that satisfies these properties. 
In recent times a variety of papers discuss the application of Shapley values, for feature attribution in machine learning models~\cite{DBLP:journals/kais/StrumbeljK14,lundberg2020local2global,lundberg2017unified,lundberg2018}. 
However, the correct way to connect a model to a coalitional game, which is the central concept of Shapley values, is a source of controversy, with two main approaches $(i)$ an interventional \cite{DBLP:journals/ai/AasJL21,DBLP:conf/nips/FryeRF20,Zern2023Interventional} or $(ii)$ an observational formulation of the conditional expectation (cf. Section \ref{sec:relatedWork})\cite{DBLP:conf/icml/SundararajanN20,DBLP:conf/sp/DattaSZ16,DBLP:journals/corr/abs-1911-00467}.

In the following experiment, we compare what are the differences between estimating the Shapley values using one or the other approach.  We benchmark this experiment on the four prediction tasks based on the US census data~\cite{ding2021retiring} and using the \enquote{Demographic Parity Inspector}, where both the model $f_\theta(X)$ and $g_\psi(\Ss(f_\theta,X))$ are linear models. We will calculate the Shapley values using the SHAP linear explainer. \footnote{\url{https://shap.readthedocs.io/en/latest/generated/shap.explainers.Linear.html}}

The comparison depends on a feature perturbation hyperparameter: whether the approach to compute the SHAP values is either \textit{interventional} or \textit{correlation dependent}. The interventional SHAP values break the dependence structure between features in the model to uncover how the model would behave if the inputs are changed (as it was an intervention). 
This option is said to stay \enquote{true to the model} meaning it will only give allocation credit to the features that are actually used by the model.

On the other hand, the full conditional approximation of the SHAP values respects the correlations of the input features. If the model depends on one input that is correlated with another input, then both get some credit for the modelâ€™s behaviour. 
This option is said to say \enquote{true to the data}, meaning that it only considers how the model would behave when respecting the correlations in the input data~\cite{DBLP:journals/corr/ShapTrueModelTrueData}.

In our case, we will measure the difference between the two approaches by looking at the linear coefficients of the model $g_\psi$ and comparing the performance of predicting protected attributes, for this case only between White-Other.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[ht]
\centering
\caption{AUC comparison of the \enquote{Explanation Shift Detector} between estimating the Shapley values between the interventional and the correlation-dependent approaches for the four prediction tasks based on the US census dataset~\cite{ding2021retiring}. The $\%$ character represents the relative difference. The performance differences are negligible.}\label{tab:t2mt2d.auc}
\begin{tabular}{l|llc}
           & Interventional                                                          & Correlation & \%           \\ \hline
Income      & 0.736438                                                                & 0.736439              & 1.1e-06 \\
Employment  & 0.747923                                                                & 0.747923              & 4.44e-07 \\
Mobility    & 0.690734                                                                & 0.690735              & 8.2e-07 \\
Travel Time & 0.790512 & 0.790512              & 3.0e-07
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Linear regression coefficients comparison of the \enquote{Explanation Shift Detector} between estimating the Shapley values between the interventional and the correlation-dependent approaches for one of the US census based prediction tasks(ACS Income). The $\%$ character represents the relative difference. The coefficients show negligible differences between the calculation methods}\label{tab:t2mt2d.coefficients}
\centering
\begin{tabular}{l|rrr}

                & \multicolumn{1}{l}{Interventional} & \multicolumn{1}{l}{Correlation} & \multicolumn{1}{c}{\%} \\ \hline
Marital         & 0.348170                           & 0.348190                        & 2.0e-05                 \\
Worked Hours    & 0.103258                           & -0.103254                       & 3.5e-06                 \\
Class of worker & 0.579126                           & 0.579119                        & 6.6e-06                 \\
Sex             & 0.003494                           & 0.003497                        & 3.4e-06                 \\
Occupation      & 0.195736                           & 0.195744                        & 8.2e-06                 \\
Age             & -0.018958                          & -0.018954                       & 4.2e-06                 \\
Education       & -0.006840                          & -0.006840                       & 5.9e-07                 \\
Relationship    & 0.034209                           & 0.034212                        & 2.5e-06                

\end{tabular}
\end{table}


In Table \ref{tab:t2mt2d.auc} and Table \ref{tab:t2mt2d.coefficients} we can see the comparison of the effects of using the aforementioned approaches to learn our proposed method, the \enquote{Explanation Shift Detector}. 
Even though the two approaches differ theoretically, the differences become negligible when explaining the protected characteristic, i.e. when providing the linear regression coefficients.