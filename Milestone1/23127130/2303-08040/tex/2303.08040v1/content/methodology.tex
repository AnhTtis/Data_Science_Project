\section{Methodology}
\subsection{Formalization}

In supervised learning, a function $f_\theta: X  \to Y$, also called a model, is induced from a set of observations, called training set, $\Dd{tr} \subseteq X \times Y$, %=\{(x_1^{tr},y_1^{tr})\ldots, (x_n^{tr},y_n^{tr})\}$ which is a sample from $X \times Y$, 
where $X = \{X_1, \ldots, X_p\}$ are (predictive) features and $Y$ is a target feature.
$f_\theta$ belongs to a family of functions ${\cal F}$ parametric in $\theta$. The domain of the target feature is $dom(Y)=\{0, 1\}$ (binary classification) or $dom(Y) = \mathbb{R}$ (regression). For binary classification, we assume a probabilistic classifier, and we actually denote by $f_\theta(x)$ the estimates of the probability $P(Y=1|X=x)$ over the (unknown)  distribution of $X \times Y$. For regression, $f_\theta(x)$ estimates $E[Y|X=x]$.

A dataset of instances ${\cal D} = \{ x_1, \ldots, x_n\} \subseteq X$ is called an \textit{input space}. The dataset of predictions $f_\theta({\cal D}) = \{f_\theta(x) \ |\ x \in {\cal D} \}$ is called the \textit{prediction space}.
We now introduce a transformation mapping $f_\theta$ and  ${\cal D}$ into a new space, which we call the \textit{explanation space}.

\begin{definition}\textit{(Explanation Space)} Let $\Ss:{\cal F}\times \mathcal{X}\to \mathbb{R}^p$ be an explanation function for models in ${\cal F}$ and instances $x \in X$, e.g., Shapley values.  For a model $f_\theta$, we map an input space $\cal D$ into a dataset in $\mathbb{R}^p$ called the \textit{explanation space}:
$\Ss(f_\theta, {\cal D}) = \{ \Ss(f_\theta, x) | x \in {\cal D}\}$.
\end{definition}
%\textcolor{blue}{\st{The explanation space is then a matrix $E$ with elements $E_{i, j} = \Ss(f_\theta, x_i)_j$ for $x_i \in {\cal D}$ and $j = 1, \ldots, p$.}}

We assume a feature modeling protected social groups, is denoted by $Z$, called \textit{protected feature}, and to be binary valued in the theoretical analysis. $Z$ can either be included in the predictive features $X$ used by a model or not. If not, we assume that it is still available. Even in the absence of the protected feature in training data, a model can discriminate against the protected groups by using correlated features as a proxy of the protected one~\cite{DBLP:conf/kdd/PedreschiRT08}.

A fairness metric quantifies the degree of discrimination or unfairness of a model \cite{mehrabi2021survey}. 
In this work, we focus on  Demographic Parity (DP), as this fairness metric does not require a ground truth target variable, allowing for our method to work in its absence~\cite{DBLP:conf/aies/AkaBBGM21}, and under distribution shift conditions~\cite{explanationShift} where model performance metrics are not feasible to calculate~\cite{RATT,garg2022leveraging,mougan2022monitoring}. DP requires independence of the model's output from the protected features, written $f_\theta(X) \perp Z$ viewing $f_\theta(X)$ and $Z$ as random variables. %\textcolor{blue}{\st{We define two types of measures of Demographic Parity (DP) in the case of binary classification models: on the predicion space and on the explanations.}}
%
%\begin{definition}\textit{(Strong Demographic Parity Violation (DP) on the Prediction Space )}. We quantify the violation of Demographic Parity as 
%$d(P(f_\theta(X)|Z=0),P(f_\theta(X)|Z=1))$, %where $d(\cdot,\cdot)$ is a  distance function on probability distributions. 
%\end{definition}\label{def:demographicParity}
%
We introduce a similar notion with reference to the explanation space by considering the multivariate random variable $\Ss(f_\theta, X)$.

\begin{definition}\textit{(Demographic Parity (DP))}. We have demographic parity on the prediction space if $f_\theta(X) \perp Z$, and demographic parity on the explanation space if $\Ss(f_\theta,X) \perp Z$.
\label{def:dp}
\end{definition}
%
%
We say that there is DP violation if $f_\theta(X) \not \perp Z$ and $\Ss(f_\theta,X) \not \perp Z$ respectively. DP in the explanation space will be shown not to suffer from Yule's effect.
Notice that, for classifiers, DP is stronger than requiring independence of the predicted class and the protected feature~\cite{DBLP:conf/uai/JiangPSJC19,DBLP:conf/aaai/ChiappaJSPJA20} .

%$P(Y=0|Z=0) = P(Y=1|Z=1)$

%\begin{definition}\textit{(Strong Demographic Parity Violation (DP) on the Explanation Space )}. We quantify the violation of Demographic Parity as 
%$d(P(\Ss(f_\theta,X)|Z=0),P(\Ss(f_\theta,X)|Z=1))$. 
%\end{definition}\label{def:demographicParity:explanations}

%We use the Wasserstein metric as a distance between  distributions~\cite{vaserstein1969markov,kantorovich1960mathematical}.\carlos{Actually we dont, we use the AUC of DPI on the exps which its very similar}

\subsection{Demographic Parity Inspector}\label{sec:demographicParityInspector}

Our approach is based on the  properties of the Shapley values whose intuition is that the explanation of a certain model's output encapsulates more information than the output itself. %We start by fitting a model $f_\theta$ to the training data, $\Dd{tr}=\{(x_1^{tr},y_1^{tr})\ldots, (x_n^{tr},y_n^{tr})\}$, and calculate the SHAP values on a hold out set $\Ss(f_\theta,X^{\text{val}})$. Then
We split the available data into three equal parts $\Dd{tr},\Dd{val},\Dd{te} \subseteq X \times Y$. Where $\Dd{tr}$ is the training set of $f_\theta$. 
Following the intuition above, $\Dd{val}$  is used to train another model $g_\psi$ on the space $\Ss(f_\theta, \Dd{val}_{X\setminus{Z}}) \times \Dd{val}_Z$ where the predictive features are in the explanation space $\Ss(f_\theta, \Dd{val}_{X\setminus{Z}})$ (excluding $Z$) and the target feature is the  protected feature $Z$.  The model $g_\psi$ is called a \enquote{Demographic Parity Inspector}, and it belongs to a family $\cal G$ possibly different from $\cal F$. In this work, we restrict to linear models for $\cal G$. The parameter $\psi$ optimizes a loss function $\ell$:
\begin{gather}\label{eq:fairDetector}
\psi = \argmin_{\tilde{\psi}} \sum_{(x, z) \in \Dd{val} %\Dd{val}_{X\setminus{Z}}\times \Dd{val}_Z
} \ell( g_{\tilde{\psi}}(\Ss(f_\theta,x)) , z )
\end{gather}
%
%
%
Finally, we use $\Dd{te}$ for testing the approach and for comparison with baselines. See also workflow of Figure \ref{fig:workflow} for a visualization of the process.

Besides detecting and measuring fairness violations in machine learning models, a common desideratum is to understand what are the specific features driving the discrimination. Using the \enquote{Demographic Parity Inspector} as an auditor method that aims to depict and quantify possible fairness violations does not 
only report a metric but provides  information on \textit{what} features are the cause of this disparate treatment. We propose to solve this issue by applying explainable AI techniques to the \enquote{Inspector}

%\textcolor{blue}{\st{The \enquote{Demographic Parity Inspector} can provide different types of explanations, re-purposing the goal of the traditional explainable AI field, from understanding the model predictions to accounting for the reasons of demographic parity violations.}}

 
