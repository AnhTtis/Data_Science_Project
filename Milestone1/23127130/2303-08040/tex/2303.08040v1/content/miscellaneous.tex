
To illustrate the application lets consider a game with three players $M=\{1,2,3\}$ that leaves 8 possible subsets $\emptyset,\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}$. Then the theoretical Shapley value can be calculated as 
\begin{gather}
    \Ss_1 = \frac{1}{3}\left( v(\{1,2,3\} - v(\{2,3\})\right) + \frac{1}{6}\left( v(\{1,2\} - v(\{2\})\right)  + \frac{1}{6}\left( v(\{1,3\} - v(\{3\})\right)+ \frac{1}{3}\left( v(\{1\} - v(\emptyset)\right)\\
    \Ss_2 = \frac{1}{3}\left( v(\{1,2,3\} - v(\{1,2\})\right) + \frac{1}{6}\left( v(\{1,2\} - v(\{1\})\right)  + \frac{1}{6}\left( v(\{2,3\} - v(\{3\})\right)+ \frac{1}{3}\left( v(\{2\} - v(\emptyset)\right)\\
    \Ss_3 = \frac{1}{3}\left( v(\{1,2,3\} - v(\{1,2\})\right) + \frac{1}{6}\left( v(\{1,3\} - v(\{1\})\right)  + \frac{1}{6}\left( v(\{2,3\} - v(\{2\})\right)+ \frac{1}{3}\left( v(\{3\} - v(\emptyset)\right)
\end{gather}
\begin{definition}\textit{(Concept Drift)}
A type of distribution shift, where what changes is the relastionship between the input and target space $P(Y^{ood}|X^{ood})\neq P(Y^{tr}|X^{tr})$. 
Concept drift can happen even if the input space $P(X^{ood})=P(X^{tr})$ or the the target space  $P(Y^{ood})=P(Y^{tr})$ remains with the same distribution.
\end{definition}


To illustrate the application lets consider a game with three players $M=\{1,2,3\}$ that leaves 8 possible subsets $\emptyset,\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}$. Then the theoretical Shapley value can be calculated as 
\begin{gather}
    \Ss_1 = \frac{1}{3}\left( v(\{1,2,3\} - v(\{2,3\})\right) + \frac{1}{6}\left( v(\{1,2\} - v(\{2\})\right)  + \frac{1}{6}\left( v(\{1,3\} - v(\{3\})\right)+ \frac{1}{3}\left( v(\{1\} - v(\emptyset)\right)\\
    \Ss_2 = \frac{1}{3}\left( v(\{1,2,3\} - v(\{1,2\})\right) + \frac{1}{6}\left( v(\{1,2\} - v(\{1\})\right)  + \frac{1}{6}\left( v(\{2,3\} - v(\{3\})\right)+ \frac{1}{3}\left( v(\{2\} - v(\emptyset)\right)\\
    \Ss_3 = \frac{1}{3}\left( v(\{1,2,3\} - v(\{1,2\})\right) + \frac{1}{6}\left( v(\{1,3\} - v(\{1\})\right)  + \frac{1}{6}\left( v(\{2,3\} - v(\{2\})\right)+ \frac{1}{3}\left( v(\{3\} - v(\emptyset)\right)
\end{gather}


\subsubsection*{A synthetic data experiment}
We create the following synthetic data to illustrate how SHAP values change with the variations of demographic parity and equal opportunity fairness.

\begin{gather}\label{eq:fairSyn}
    (X_{11},X_{12},X_{21},X_{22}) \sim N(0,1),\quad A_1 = 1, A_2 = -1\\
    y_1 = \textbf{1}\quad \mathrm{If} \quad X_{11}+X_{12}>0 \quad \mathrm{else} \quad \textbf{0}\\
     y_2 = \textbf{1}\quad \mathrm{If} \quad \gamma + X_{21}+X_{22}>0 \quad \mathrm{else} \quad \textbf{0}\\
    X_1 = [X_{11},X_{12}],\quad X_2 = [X_{21},X_{22}],\quad A = [A_1,A_2], \quad y = [y_1,y_2]\\
    \gamma \in [0,5]
\end{gather}

For this experiment our data is drawn out of 5,000 samples from the previously indicated distributions. Our model $f_\theta$ is a Logistic regression trained on  $X = [X_1,X_2,A]$. The SHAP values are calculated using correlation dependent estimation algorithm \cite{DBLP:journals/corr/ShapTrueModelTrueData}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{images/syntheticFairnes.png}
\caption{Equal opportunity fairness, Demographic Parity and SHAP values of the protected attribute when modifying the gamma hyperparameter of the dataset \ref{eq:fairSyn}. As the disparate treatment increases among the classes the SHAP value of the protected attribute increases.}\label{fig:fairSyn}
\end{figure}


\subsubsection*{Synthetic data experiment}

We use the following synthetic data to evaluate the usage of Shapley values to detect :
\begin{gather}
    A \in \{-1,1\}\\
    X1 \sim N(0,\lambda_1),X2 \sim N(0,\lambda_2)\\
    X3 \sim \mathrm{If}\quad (A>0) \quad \gamma\cdot x1 + 2\cdot \gamma X2 + N(0,\lambda_3) \\
    (\mathrm{else}) \quad-(\gamma\cdot x1 + 2\cdot \gamma X2) + N(0,\lambda_3)\\
    Y \sim \gamma\cdot A + X1\cdot X2 + \epsilon \\
    (\lambda_A,\lambda_1,\lambda_2,\lambda_3) = (0.1,1,1,0.1)\\
    \gamma \in [0,1],\epsilon =1%\in [0,1]
\end{gather}


\newpage
\textbf{Efficiency} The feature contributions must add up to the difference of prediction for x and the average
\begin{gather*}
    \sum_{i=1}^p \Ss_i(f_\theta,x) = f_\theta(x) - E_X(f(X))
\end{gather*}

\textbf{Dummy} A feature $j$ that does not change the predicted value -- regardless of which coalition of feature values it is added to -- should have a Shapley value of 0.


% Concept bias vs Dataset Bias
%Dataset bias, when the statistical input data distributions of subgroups of sensitive attribute ($A$) are distinct $P(X|A)\neq P(X)$, can be one of the sources of source of machine learning discrimination, in this work we propose concept bias,when the relationships between input domain and the target domain by the model are distinct for the subgroups $P(Y|X_{A=v}) = P(Y|X)$, as a different type of bias that can be the source of discrimination.


% The problem 
%Selecting a measure to compare fairness between two sensitive groups is not a trivial task. While there are many possible measures of fairness, some of the most common notions are  \textit{demographic parity} and \textit{equal opportunity fairness}. Traditional methods to calculate this notions have relied on comparing distributions on the output space. 



\subsection{Concept Bias}

Inspired by the idea of concept drift \cite{datasetShift,early_drift}, in this section we differentiate between dataset bias and concep tibas, a type of bias that can arise during the learning stage of a ML problem.

\begin{definition}\textit{(Concept Bias)}. The learned relations by the model between two sensitive attribute group are different $P(Y^{g1}|X^{g1}) \neq  P(Y^{g2}|X^{g2}|)$.
\end{definition}
\carlos{Should not we use the model here? $f_\theta$}

There can be cases where the input data distributions are equal between two subgroups $P(X^{g1}) = P(X^{g2})$,  the $P(Y^{g1}) = P(Y^{g2})$ even the model $f_\theta$ achieves  demographic parity  $P(f_\theta(X)|G1) = P(f_\theta(X)|G2)$ or equal oportunity fairness $P(f_\theta(X_{g1})|Y_{g1}=1)=P(f_\theta(X^{g2})|Y^{g2}=1)$, but the learned relations by the model is different $P(Y^{g1}|X^{g1}) \neq  P(Y^{g2}|X^{g2}|)$. We denominate this type of discrimination as concept bias.

Concept bias can be harmfull for the following reasons:
\begin{itemize}
    \item First, it denotes that the underlying logic/mathematical structure of a model giving an outcome it's different between two protected attributes.
    
    \item Recent work has called to attention that fairness metrics that are satisfied on the training data might not hold if the data distribution changes ~\cite{DBLP:journals/corr/abs-2202-01034,DBLP:conf/icml/KallusZ18}. Models affected by concept bias are more proned to subpopulation concept drift,  
\end{itemize}



\begin{definition}\textit{$AUC$} for a function $g$ is given as follows. The target variable is a binary classification with two groups $G1$ and $G2$, from a dataset $G1,G2 \in X, G1 \cap G2 = \emptyset $.
\begin{equation}
    AUC(g) = \frac{\sum_{x^i \in X^G1}\sum_{x^j \in X^G2} \boldsymbol{1}[g(x^i)) < g(x^j)]}{|X^{G1}|\cdot |X^{G2}|}
\end{equation}
\end{definition}

Where $\boldsymbol{1}[g(x^i)) < g(x^j)]$ denotes and indicator function which returns 1 iff $g(x^i)) < g(x^j)$ and 0 otherwise.



\begin{definition}\textit{$AUC$} for the function $g(S(x))$ is given as follows. We assume \textit{two} protected groups $G1$ and $G2$ containing $\# G1$ and $\# G2$ elements in the dataset $x$.

\begin{equation}
    AUC(g(S(x))) = \frac{\sum_{x^i \in G1} ^ {\# G1}\sum_{x^j \in G2} ^ {\# G2} \texttt{1}[\hat{g}(S(x^i)) < \hat{g}(S(x^j))]}{\# G1 \# G2}
\end{equation}
The indices $i,j$ mark only whether an element belongs to group $G1$ or $G2$. 
\end{definition}




The quality of the predictor $g_\psi(\Ss(f_\theta,X))$ will carry some information about how unfair the base predictor $f(x)$ w.r.t. the protected attribute allowing to create  an indicator for discrimination


\begin{gather}
\mathrm{If} \quad \mathrm{AUC}(g_\psi(S(f_\theta,X)),Y) = 0 \rightarrow\\
\hat{g}(g_\psi(S(f_\theta,X^{G1}))) = \hat{g}(g_\psi(S(f_\theta,X^{G2}))) \\
S_{l, m}(x^i) = S_{l, m}(x^j) \\
\beta_m x_{l, m}^i - \beta_m E(x^i_m) = \beta_m x_{l, m}^j - \beta_m E(x^j_m) \\
\beta_m x_{l, m}^i = \beta_m x_{l, m}^j \\
\hat{f}(x_l^i) = \hat{f}(x_l^j)
\end{gather}

Line (8) to (9) (?) \\
Line (6) to (7) : using that expected values are the same. \\
Line (7) to (8) : sum over features (index $m$). 

Since in (8) we find that the predicted values of $f$ are the same independently of whether an element belongs to group $G1$ or $G2$, we found an independence: $\hat{f} \perp G1, G2$, i.e. that true positive/false positive rates are the same, i.e. $EO$ holds.

To be continued for theorem 2: $AUC(g(S(x))) > 0.5 \rightarrow EO(f(x)) \quad \text{violated}$
\begin{example}[Concept Bias]
Let $X^{g1} = (X_1,X_2) \sim N(\mu,I_2)$, and $X^{g2}= (X_1,X_2) \sim N(\mu,I_2)$, where $I_2$ is an identity matrix of order two and $\mu = (\mu_1,\mu_2)$.
We now create two synthetic targets $Y^{g1}=a + \alpha \cdot X_1 + \beta \cdot X_2 + \epsilon$ and $Y^{g2}=a + \beta \cdot X_1 + \alpha \cdot X_2 + \epsilon$.
Let $f_\theta$ be a  model trained on $f_\theta([X^{g1},X^{g2}],[Y^{g1},Y^{g2}])$.
Then $P(Y^{g1}) = P(Y^{g2})$, $P(X^{g1}) = P(X^{g2})$ but $\Ss(f_\theta,X^{g1})\neq \Ss(f_\theta, X^{g2})$
\end{example}

\newpage
\newpage


\begin{comment}
\begin{theorem}
If the  predictive performance of the model $g_\psi$ to predict the protected attribute is  better than random predictions, $AUC(g_\psi(\Ss(f_\theta,X)),A) > 0.5$ then  $f_\theta(X)$ violates demographic parity  $d(f_\theta(X)|Z), P(f_\theta(X))\neq 0$
\end{theorem}
\begin{gather}
AUC(g_\psi) = 0.5\\
\hat{g}(g_\psi(S(f_\theta,X^{G1}))) = \hat{g}(g_\psi(S(f_\theta,X^{G2}))) \\
S_{l, m}(x^i) = S_{l, m}(x^j) \\
\beta_m x_{l, m}^i - \beta_m E(x^i_m) = \beta_m x_{l, m}^j - \beta_m E(x^j_m) \\
\beta_m x_{l, m}^i = \beta_m x_{l, m}^j \\
\hat{f}(x_l^i) = \hat{f}(x_l^j)
\end{gather}
\end{comment}
\begin{comment}
\begin{definition}\textit{(Explanation Demographic Parity (EDP))}. The explanations $\Ss(f_\theta,X))$, for a  classifier $f_\theta$ is said to satisfy demographic parity for some dataset $\D$ if $P(\Ss(f_\theta,X)|A) = P(\Ss(f_\theta,X))$.
\end{definition}
\begin{definition}\textit{(Individual Fairness)}. IF $f_\theta$ is a decision model, given appropriate distance functions $d(\cdot, \cdot)$ as well as thresholds $\epsilon \geq 0$ and $\sigma \geq 0$, the model is individually fair if, for any pair of inputs $x, x'$ such that $d(x, x') \leq \epsilon$, we have $D(f_\theta(x), f_\theta(x')) \leq \sigma$.
\end{definition}
\end{comment}
\textbf{Symmetry} The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions. \\

\textbf{Additivity} Suppose you trained a random forest, which means that the prediction is an average of many decision trees. The Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest.

\subsubsection{In the presence of the protected attribute}

A simple way to see if there is disparate treatment in the model behaviour on some given data between two protected groups is seen if the protected attribute is used at the prediction stage. Using Shapley values we can detect differences in the disparate treatment of protected groups.
 
If the protected attribute is present  within the predictors training data $Z \in X^{tr}$. By the \enquote{Dummy} property of the Shapley values (cf. Section \ref{sec:relatedWork}), if the protected attribute does not contribute to the predictions $f_\theta(X) \perp Z$, the Shapley value, is by definition zero $\Ss_Z(f_\theta,X) \equiv 0$.  Then the model $f_\theta$, achieves demographic parity respect to the protected attribute $P(f_\theta(X)|Z) = P(f_\theta(X))$.

In the opposite case, i.e. if the Shapley value of the protected attribute is $\Ss_Z(f_\theta, X)\neq 0 $, demographic parity does not hold anymore. Monitoring changes in the Shapley explanations can provide information about fairness violations. 

\subsubsection{In the absence of the protected attribute}
The potentially harmful social impact of machine learning models is not limited to cases where there exist the use of the protected attribute but extends to cases where even if the protected attribute is not present in the training data it can be related to other present features (act as proxy) \cite{barocas-hardt-narayanan}. This section aims to provide some fairness auditing guarantees for simplified scenarios of our method when the protected attribute is not explicitly present in the training data $Z\notin \Dd{tr}$.


% And
In the last few years, discrimination and fairness have become a matter of grave concern within the machine learning community.
In efforts to mitigate these concerns, the research community has made an enormous effort to identify potential biases, depict metrics, audit model, and design algorithmic methods to improve fairness and discrimination metrics. Various publications reconcile the information of relevant papers, create taxonomies, define future opportunities and current gaps~\cite{barocas-hardt-narayanan,DBLP:journals/widm/NtoutsiFGINVRTP20,DBLP:journals/corr/Aequitas}.

% AI regulation
New AI regulation has emerged aiming to regulate discrimination on machine learning systems\cite{veale2021demystifying}, following the creation of new institutional bodies, such as AI Supervision Agencies, that aim to be responsible for the development, supervision, and monitoring of projects within the regulation frameworks\footnote{Coordinated Plan on Artificial Intelligence - European Commission \url{https://digital-strategy.ec.europa.eu/en/policies/plan-ai}}\footnote{European Data Protection Supervisor \url{https://edps.europa.eu/_en}}


% Therefore, xAI
The field of explainable AI has emerged as a way to understand model decisions ~\cite{molnar2019} and interpret the inner workings of black box models~\cite{guidotti_survey}. The core idea of this paper is to detect, quantify and shed insights on fairness violations using the explanation space as a more information-rich data space than input, output, comprised or latent spaces. Evaluating fairness metrics in the explanation space allows for more sensitive and indicative discrimination detection techniques than previous traditional methods, providing theoretical guarantees in the auditing method. As fairness metrics, we use the notion of group fairness, which measures how the model output is distributed over different protected subgroups.

Depending on the auditor party, we can distinguish between three different types of audit processes~\cite{DBLP:conf/fat/Costanza-ChockR22}: $(i)$ first-party AI audits are conducted by internal teams,  $(ii)$, second-party audits are conducted by contractors, and  $(iii)$ third-party audits are conducted by independent researchers or entities with no contractual relationship to the audit target.


\subsubsection{Meta-Learning as an explainable AI technique}

Approaches such as membership inference (or membership classifiers) have aimed to detect whether given a data record and a machine learning model, the record was in the modelâ€™s training dataset or not ~\cite{DBLP:conf/sp/ShokriSSS17}. In our work we do not attempt to predict if the data is either on training data or not, but to predict the protected attribute given the explanations $\Ss(f_\theta,X)$ of a model $f_\theta$ and data $X$. 

Another related technique from causal inference is double/debiased machine learning. It uses any machine learning model you want to first deconfound the feature of interest and then estimate the average causal effect of changing that feature ~\cite{10.1111/ectj.12097}. In our work, we do not aim to find the causal relationship of the data using a set of possible confounders. But to measure the discriminatory behavior given a machine learning model and a dataset.

%
%we have a  the following implication holds: Called $g(S_1, \ldots, S_p) = \sum_i S_i + c$,  where $c = E[f_\theta(X)]$, disequality (\ref{eq:421}) amounts at: $g(\Ss(f_\theta, X)) \not \perp Z$. Intuitively, this means that violation of DP of the model $f_\theta$ implies violation of DP of a simple linear model over the explanation space.
%
%As the prediction space represents the sum of all the feature contributions plus a constant of the explanation space, then if two groups have different prediction distributions it implies that they also have different explanations since at least one element of the sum of the contribution needs to change.
%
%\steffen{I would need a bit clearer argumentation here. I do not even understand what the term `DP violations on the explanations' means, as this term is nowhere defined. (Def 3.2 defines DP violation on prediction space, so this part is clear.)}
%

%\begin{gather}\nonumber
%    \mathrm{If} \quad P(f_\theta(X)|Z=0)\neq P(f_\theta(X)|Z=1)\\ \label{eq:421}
%    \mathrm{then}\   P\left(\Ss(f_\theta, X)\ |\  Z=0\right)\neq P\left(\Ss(f_\theta, X)\ |\  Z=1\right)
%\end{gather}

%By the efficiency property (\ref{sec:relatedWork}), the prediction space is a sum of the explanation space, so if there is a change in the total sum, it implies that at least one of the terms of the sum is distinct. Concluding 


% \sim N(\mu,c\cdot I)$, where $I$ is an identity matrix of order three and $\mu = (\mu_1,\mu_2,\mu_3)$, and $\mu_3=1$ if $z = 1$ and $\mu_3=0$ otherwise. We now create a synthetic target $Y=a_0 + a_1 \cdot X_1 + a_2 \cdot X_2 + \epsilon$ that is independent of $X_3$. We train a linear regression model $f_\beta$ on $(X,Y)$ which detects that $X_3$ is correlated with $Z$ and then it does not use it (an approach which is called in-processing fairness-aware learning)

%with coefficients $\beta_0,\beta_1,\beta_2,\beta_3$. Then if $P(\mu_3|Z=0)\neq P(\mu_3|Z=1)$ or $P(c_3|Z=0) \neq P(c_3|Z=1)$, then the input data depends on the protected attribute $P(X_3|Z=0)\neq P(X_3|Z=1)$ but the model $f_\beta \perp Z$ and the explanations do not $\Ss(f_\beta, X) \perp Z$}
%\steffen{I do not understand why things are explained with $\mu_3$ and with $c_3$. One of them is good enough.}
%\begin{gather}
%X_3\sim N(\mu_3,c_3),X_3^{ood} \sim N(\mu_3^{'}, c_3^{'})\\
%\mathrm{If} \quad  P(\mu_3|Z=0)\neq P(\mu_3|Z=1) \quad\mathrm{or}\\\nonumber
%\quad P(c_3|Z=0)\neq P(c_3|Z=1)\\
%\Rightarrow P(X_3|Z=0)\neq P(X_3|Z=1) \Rightarrow P(X_3)\not\perp Z\\
%\Ss(f_\beta,X) = \left(\begin{bmatrix} \beta_1(X_1 - \mu_1)  \\\beta_2(X_2 - \mu_2)  \\\beta_3(X_3 - \mu_3)   \end{bmatrix} \right) = \left(\begin{bmatrix} \beta_1(X_1 - \mu_1)  \\\beta_2(X_2 - \mu_2)  \\0   \end{bmatrix} \right)\\
%P(\Ss_3(f_\beta,X)|Z=0) = P(\Ss_3(f_\beta,X)|Z=1) \Rightarrow \Ss(f_\beta,X)\perp Z
%\end{gather}
