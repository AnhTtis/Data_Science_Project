\section{A Model for Monitoring Equal Treatment}
%\steffen{Do we need subheadings for 3.1 and 3.2?}
\subsection{Formalizing Equal Treatment}

To establish a criterion for equal treatment, we rely on the notion of explanation distributions.

\begin{definition}\textit{(Explanation Distribution)} An explanation function $\Ss:{\cal F}\times X\to \mathbb{R}^p$ 
maps a model $f_\theta \in {\cal F}$ and an input instance $x \in X$ into a vector of reals $\Ss(f_\theta, x) \in \mathbb{R}^p$. We extend it by mapping an
input distribution $\D_X$ into an (empirical) \textit{explanation distribution} as follows:
$\Ss(f_\theta, \D_X) = \{ \Ss(f_\theta, x)\ |\ x \in \D_X\} \subseteq \mathbb{R}^p$.
\end{definition}

We will use Shapley values as an explanation function (cf.\ Appendix~\ref{app:foundation.xai}).
 We introduce next the new fairness notion of Equal Treatment, which considers independence over the explanations of the model's outputs.

\begin{definition}\textit{(Equal Treatment (ET))}. A model $f_\theta$ achieves equal treatment if  $\Ss(f_\theta,X) \perp Z$.\label{def:et}
\end{definition}
%
%
As we will see later in Section~\ref{sec:theory}, Equal Treatment is a stronger definition than Demographic Parity since it not only requires that the distributions of the predictions are similar but that the process of how predictions are made is also similar.

\subsection{Equal Treatment Inspector}\label{sec:demographicParityInspector}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/DPI.drawio.pdf}
    \caption{Equal Treatment Inspector workflow. The  model $f_\theta$ is learned based on training data, $\D = \{(x_i,y_i)\}$,  and outputs the explanations $\Ss(f_\theta,\D_X)$. The Classifier Two-Sample Test receives the explanations to predict the protected attribute, $Z$. The AUC of the two-sample test classifier $g_\psi$ decides for or against \emph{equal treatment}.  We can interpret the driver for unequal treatment on $g_\psi$ with explainable AI techniques}
    \label{fig:workflow} 
     %\antonio{in the image the diagram we have auc = 0.5, while in the text later we use 1/2. maybe it is nicer if we always write 1/2}\carlos{In the word we use nicefrac which is not compatible in draw.io Classic frac looks too big}
\end{figure}

Our approach is based on the  properties of the Shapley values (cf. Appendix \ref{app:foundation.xai})  and the prior work of ~\cite{DBLP:conf/iclr/Lopez-PazO17} of classifier two-sample tests. We split the available data into three parts $\Dd{tr},\Dd{val},\Dd{te} \subseteq X \times Y$. Here $\Dd{tr}$ is the training set of $f_\theta \in \cal F$ (not required if $f_\theta$ is already trained). Following the intuition above, $\Dd{val}$  is used to train another model $g_\psi$ on the distribution $\Ss(f_\theta, \Dd{val}_{X\setminus{Z}}) \times \Dd{val}_Z$ where the predictive features are in the explanation distribution $\Ss(f_\theta, \Dd{val}_{X\setminus{Z}})$ (excluding $Z$) and the target feature is the  protected feature $Z$.  The model belongs to a family $\cal G$, possibly different from $\cal F$. The parameter $\psi$ optimizes a loss function $\ell$:
\begin{gather}\label{eq:fairDetector}
\psi = \argmin_{\tilde{\psi}} \sum_{(x, z) \in \Dd{val} } \ell( g_{\tilde{\psi}}(\Ss(f_\theta,x)) , z )
\end{gather}
%
%
%
Finally, we use $\Dd{te}$ for testing the approach and for comparison with baselines.  To evaluate whether there is a fairness violation or not, we perform a statistical test of independence based on the AUC. See the workflow of Figure \ref{fig:workflow} for a visualization of the process.

Besides detecting and measuring fairness violations in machine learning models, a common desideratum is to understand what are the specific features driving the discrimination. Using the \enquote{Equal Treatment Inspector} as an auditor method that aims to depict and quantify possible fairness violations does not 
only report a metric but provides information on \textit{which features are the cause of the un-equal treatment}. We propose to solve this issue by applying explainable AI techniques to the Inspector. The \enquote{Equal Treatment Inspector} can provide different types of explanations, re-purposing the goal of the traditional explainable AI field, from understanding the model predictions to accounting for the reasons of un-equal treatment.
 
