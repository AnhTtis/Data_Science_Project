\section{Definition and Properties of Shapley values}\label{app:foundation.xai}
Explainability has become an important concept in legal and ethical guidelines for data processing and machine learning applications ~\cite{selbst2018intuitive}. A wide variety of methods have been developed, aiming to account for the decision of algorithmic systems ~\citep{DBLP:journals/csur/GuidottiMRTGP19,DBLP:conf/fat/MittelstadtRW19,DBLP:journals/inffus/ArrietaRSBTBGGM20}. 
One of the most popular approaches to explainability in machine learning (ML) are Shapley values.

Shapley values are used to attribute relevance to features according to how the model relies on them ~\citep{DBLP:journals/corr/abs-1905-04610,DBLP:conf/nips/LundbergL17}. Shapley values are a coalition game theory concept that aims to allocate the surplus generated by the grand coalition in a game to each of its players~\citep{shapley1997value}. 

For set of players $N = \{1, \ldots, p\}$, and a value function $\val:2^N \to \mathbb{R}$, the Shapley value $\mathcal{S}_j$ of the $j$'th player is defined as the averaged additional contribution of player $j$ in all possibles coalitions of players:
\begin{small}
\[
\mathcal{S}_j = \sum_{T\subseteq N\setminus \{j\}} \frac{|T|!(p-|T|-1)!}{p!}(\val(T\cup \{j\}) - \val(T))
\]
\end{small}
In the context of machine learning models, players correspond to features $X_1, \ldots, X_p$, and the contribution of the feature $X_j$ is with reference to the prediction of a model $f$ for an instance $x^{\star}$ to be explained. Thus, we write $\mathcal{S}(f, x^{\star})_j$ for the Shapley value of feature $X_j$ in the prediction $f(x^{\star})$. We denote by $\mathcal{S}(f, x^{\star})$ the vector of Shapely values $(\mathcal{S}(f, x^{\star})_1, \ldots, \mathcal{S}(f, x^{\star})_p)$. 

There are two variants for the term $\val(T)$~\citep{DBLP:journals/ai/AasJL21,DBLP:journals/corr/abs-2006-16234,Zern2023Interventional}: the \textit{observational} and the \textit{interventional}. When using the observational conditional expectation, we consider the expected value of $f$ over the joint distribution of all features conditioned to fix features in $T$ to the values they have in $x^{\star}$:
\begin{equation}\label{def:val:obs}
\val(T) = E[f(x^{\star}_T, X_{N\setminus T})|X_T=x^{\star}_T]
\end{equation}
where $f(x^{\star}_T, X_{N\setminus T})$ denotes that features in $T$ are fixed to their values in $x^{\star}$, and features not in $T$ are random variables over the joint distribution of features.
Opposed, the interventional conditional expectation considers the expected value of $f$ over the marginal distribution of features not in $T$: % and with features in $T$ being fixed to the values they have in $x^{\star}$:
\begin{equation}\label{def:val:int}
\val(T) = E[f(x^{\star}_T, X_{N\setminus T})]
\end{equation}
In the interventional variant, the marginal distribution is unaffected by the knowledge that $X_T=x^{\star}_T$. In general, the estimation of (\ref{def:val:obs}) is difficult, and some implementations (e.g., SHAP) actually consider (\ref{def:val:int}) as the default one. In the case of decision tree models, TreeSHAP offers both possibilities.


The Shapley value framework is the only feature attribution method that satisfies the properties of efficiency, symmetry, uninformativeness and additivity~\citep{molnar2019,shapley1997value,winter2002shapley,aumann1974cooperative}.  We recall next the key properties of efficiency and uninformativeness:

\paragraph{Efficiency.} Feature contributions add up to the difference of prediction for $x^{\star}$ and the expected value of $f$:
\begin{gather}\label{eq:eff}
    \sum_{j \in N} \Ss_j(f, x^{\star}) = f(x^{\star}) - E[f(X)])
\end{gather}

The following property only holds for the interventional variant (SHAP values), but not for the observational variant.
\paragraph{Uninformativeness.}
A feature $X_j$ that does not change the predicted value (i.e., for all $x, x'_j$: $f(x_{N\setminus \{j\}}, x_j) = f(x_{N\setminus \{j\}}, x'_j)$) have a Shapley value of zero, i.e., $\Ss_j(f, x^{\star}) = 0$.
 

In the case of a linear model $f_\beta(x) = \beta_0 + \sum_j \beta_j \cdot x_j$, the SHAP values turns out to be $\Ss(f, x^{\star}) = \beta_i(x^{\star}_i-\mu_i)$ where $\mu_i = E[X_i]$. For the observational case, this holds only if the features are independent \citep{DBLP:journals/ai/AasJL21}.

\section{Detailed Related Work}\label{app:relatedWork}

This section provides a more in-depth review of the related theoretical work that informs our research. We contextualize our contribution within the broader field of explainable AI and fairness auditing. We discuss the use of fairness measures such as demographic parity, as well as explainability techniques like Shapley values and counterfactual explanations.

\subsection{Fairness Notions: Paper Blind Reviews Use Case}\label{app:fair.notions}

%\steffen{explain with formulas}\carlos{I have cross-reference the eq. of the main body}\steffen{The weakness of this use case is that one may argue that the prior probability of a paper to succeed is the same for Germany and UK. I think it is easier to argue the case if the prior probability between different groups is different. This is the case for university admissions. Or for COMPAS dataset.}\steffen{there is the assumption in that notion that the two countries indeed have the same performance --- which may not be the case}\carlos{There is no assumption, we are not saying which one is better. We could change for country A and country B, or institution A and institution B}

To illustrate the difference between equal opportunity, equal outcomes, and equal treatment, based on the previously discussed framework, we consider the example of conference papers' blind reviews and focus on the protected attribute of the country of origin of the paper, comparing Germany and the United Kingdom.


For \emph{equal opportunity}, we will first start defining the opportunity as a type of error i.e. the true positive rate(cf. Defition~\ref{def:eo}). In words, it is the acceptance ratio given that the quality of the paper is high. Achieving equal opportunity will imply that these ratios are similar between the two countries. In blind reviews, the purpose is to evaluate the paper's quality and the research's merit without being influenced by factors such as the author's identity, affiliations, background or country. If we were to enforce equal opportunity in this use case, we would aim for similar true positive rates for submissions from different countries. However, this approach could lead to unintended consequences, such as unintentionally favouring or quotas of affirmative action towards certain countries or institutions.


For \emph{equal outcomes}, we will measure that the distribution of acceptance rates is similar, independently of the quality of the paper(cf. Definition~\ref{def:dp}). Note that the outcomes can have similar rates due to random chance, even if there is a country or institutional bias in the acceptance procedure.

For \emph{equal treatment}, we will measure that the contribution of each of the features used to accept the paper has similar distributions(cf. Definition~\ref{def:et}). Equality of treatment through blindness is more desirable than equal opportunity or equal outcomes because it ensures that all submissions are evaluated solely on the basis of their quality, without any bias or discrimination towards any particular country or institution. By achieving equality of treatment through blindness, we can promote fairness and objectivity in the review process and ensure that all authors have an equal chance to evaluate their work on its merits.


In comparing our introduced measure of \emph{equal treatment} with \emph{equal outcomes} (or demographic or statistical parity, used synonymous), we note that the latter looks at the distributions of predictions and measures their similarity. Equal treatment goes a step further by evaluating whether how the contribution of features to the decision, is similar. Our definition of \emph{equal treatment} implies the notion of \emph{equal outcome}, but the converse is not necessarily true, as we will see in section \ref{sec:stat.independence}.


\subsection{Fairness and Auditing}

\enquote{Audits are tools for interrogating complex processes, often to determine whether they comply with company policy, industry standards
or regulations}~\citep{DBLP:conf/fat/RajiSWMGHSTB20}. Algorithmic fairness audits are closely linked to audits studies as understood in the social sciences, with a strong emphasis on social justice~\citep{DBLP:conf/eaamo/VecchioneLB21}.
A recent survey on public algorithmic audit identified four categories of \enquote{problematic machine behaviour} that can be unveiled by audit studies: discrimination, distortion, exploitation and misjudgement \citep{DBLP:journals/pacmhci/Bandy21,liu2012enterprise}. This taxonomy is highly helpful when putting forward auditing studies and also relevant for this work: our \enquote{Equal Treatment Inspector} can be seen as a tool to help understand discrimination in machine learning models, thus, falling into the first category.


%% Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem https://facctconference.org/static/pdfs_2022/facct22-126.pdf?mc_cid=b1a21b7286&mc_eid=fa6fa6c768
% read \citep{DBLP:journals/aiethics/Ugwudike22}
% and Shea Brown, Jovana Davidovic, and Ali Hasan. 2021. The algorithm audit: Scoring the algorithms that score u
%We use the terms \enquote{AI audit} and \enquote{algorithmic audit} interchangeably to refer to a process, according to specific criteria, the behaviour of ML model is accounted~\citep{DBLP:conf/fat/Costanza-ChockR22}. 
Selecting a measure to compare fairness between two sensitive groups has been a highly discussed topic, where results such as~\citep{DBLP:journals/bigdata/Chouldechova17,DBLP:conf/nips/HardtPNS16,DBLP:conf/innovations/KleinbergMR17}, have highlighted the impossibility to satisfy simultaneously three type of fairness measures: demographic parity~\citep{DBLP:conf/innovations/DworkHPRZ12}, equalized odds~\citep{DBLP:conf/nips/HardtPNS16}, and predictive parity~\citep{DBLP:conf/kdd/Corbett-DaviesP17,DBLP:journals/corr/abs-2102-08453,wachter2020bias}. 


%Within the fairness mitigation context~\citep{DBLP:journals/corr/EdwardsS15} formulates an adversarial learning problem of the model and data to remove statistical parity from images. A limitation is that their work is limited to images and neural networks. Our work focuses on tabular data and leverages Shapley value theory to provide equal treatment auditing guarantees. 

Previous work has relied on measuring and calculating demographic parity on the model predictions~\citep{DBLP:conf/fat/RajiSWMGHSTB20,DBLP:conf/icml/KearnsNRW18}, or on the input data~\citep{DBLP:journals/cviu/FabbrizziPNK22,DBLP:conf/fat/YangQ0DR20,DBLP:conf/emnlp/ZhaoWYOC17}.  In this work, we perform equal treatment measures on the explanation distribution, which measures that each feature contributes equally to the prediction, which differs from the previous notions. 

In this work, we focus on  Equal Treatment (ET), as this fairness metric does not require a ground truth target variable, allowing for our method to work in its absence~\citep{DBLP:conf/aies/AkaBBGM21}, and under distribution shift conditions~\citep{DBLP:journals/corr/abs-2210-12369} where model performance metrics are not feasible to calculate~\citep{DBLP:conf/icml/GargBKL21,DBLP:conf/iclr/GargBLNS22,DBLP:journals/corr/abs-2201-11676}. DP requires independence of the model's output from the protected features, written $f_\theta(X) \perp Z$.  

\subsection{Explainability and fair supervised learning}

The intersection of fairness and explainable AI has been an active topic in recent years. The most similar work is ~\citep{lundberg2020explaining} where they apply Shapley values to statistical parity.
Our work continues in-depth on this research line by formalizing the explanation distribution, deriving theoretical guarantees, and proposing novel methodology. Specifically, our approach allows for comparison across different protected groups. We also introduce the concept of accountability, which refers to the ability of the algorithm to provide insights into the sources of \emph{equal treatment} violation of the model. Additionally, we evaluate our method on multiple datasets and synthetic examples, demonstrating its effectiveness in identifying and mitigating equal treatment disparities in machine learning models.


~\cite{DBLP:conf/ssci/StevensDVV20} presents an approach to explaining fairness based on adapting the Shapley value function to explain model unfairness. They also introduce a new meta-algorithm  that considers the problem of learning an additive perturbation to an existing model in order to impose fairness. 
In our work, we do not adopt the Shapley value function. Instead, we use the theoretical Shapley properties to provide fairness auditing guarantees. Our \textit{Equal Treatment Inspector} is not perturbation-based but uses Shapley values to project the model to the explanation distribution, and then measures \emph{un-equal treatment}. It also allows us to pinpoint what are the specific features driving this violation.
 
 
In ~\cite{DBLP:conf/fat/GrabowiczPM22}, the authors present a post-processing method based on Shapley values aiming to detect and nullify the influence of a protected attribute on the output of the system. For this, they assume there are direct causal links from the data to the protected attribute and that there are no measured confounders. Our work does not use causal graphs but exploits the theoretical properties of the Shapley values to obtain fairness model auditing guarantees.

Instead of using feature attribution explanation, other works have researched fairness using other explainability techniques such as counterfactual explanations \citep{DBLP:conf/nips/KusnerLRS17,DBLP:conf/cogmi/ManerbaG21,DBLP:conf/aies/MutluYG22}.
We don't focus on counterfactual explanations but on feature attribution methods that allow us to measure unequal feature contribution to the prediction. Further work can be envisioned by applying explainable AI techniques to the \enquote{Equal Treatment Inspector} or constructing the explanation distribution out of other techniques.


\subsection{Classifier two-sample test}
The use of classifiers to obtain statistical tests or measure independence between two distributions has been previously explored in the literature \citep{DBLP:conf/iclr/Lopez-PazO17}. Specifically, the first authors introduced the use of \enquote{Classifier Two-Sample Tests (C2ST)} to represent the data that returns an interpretable unit test statistic, allowing it to measure how two distributions differ from each other. Their approach establishes the main theoretical properties, compares performance against state-of-the-art methods, and outlines applications of C2ST. 
Similar work of~\cite{DBLP:conf/icml/LiuXL0GS20} propose a kernel-based to two-sample
tests classification, aiming to determine whether two samples are drawn from the same underlying distribution. 
Alike work has also been used in Kaggle competitions under the name of \enquote{Adversarial Validation}~\citep{kaggleAdversarial,howtowinKaggle}, a technique which aims to detect which features are distinct between train and leaderboard datasets to avoid possible leaderboard shakes. 
Our work builds on previous approaches by adopting their interpretable power test analysis and theoretical properties. Still, we focus on testing for fairness notions of equal treatment in the explanation distribution. As the test statistic for our approach, we use the Area Under the Curve (AUC) of the \enquote{Equal Treatment Inspector}.

Another related work in the literature is by \cite{DBLP:journals/corr/EdwardsS15}, who focuses on removing statistical parity from images by
using an adversary that tries to predict the relevant sensitive variable from the model representation and censoring the learning of the representation of the model and data on images and neural networks. While methods for images or text data are often developed specifically for neural networks and cannot be directly applied to traditional machine learning techniques, we focus on tabular data where techniques such as gradient boosting decision trees achieve state-of-the-art model performance \citep{DBLP:conf/nips/GrinsztajnOV22,DBLP:journals/corr/abs-2101-02118,BorisovNNtabular}. Furthermore, our model and data projection into the explanation distributions leverages Shapley value theory to provide fairness auditing guarantees. In this sense, our work can be viewed as an extension of their work, both in theoretical and practical applications.




