\section{Foundations and Related work}\label{sec:relatedWork}

This section briefly surveys philosophical and technical foundations of our contribution as well as related work. We also build on Shapley values, which are generally known in the machine learning community, but for self-containedness 
we provide their mathematical definition in Appendix~\ref{app:foundation.xai}.

\subsection{Basic Notations and Formal Definitions of Fairness in Related Work}

In supervised learning, a function $f_\theta: X  \to Y$, also called a model, is induced from a set of observations, called the training set, $\Dd{} =\{(x_1,y_1),\ldots,(x_n,y_n)\} \sim X \times Y$, %=\{(x_1^{tr},y_1^{tr})\ldots, (x_n^{tr},y_n^{tr})\}$ which is a sample from $X \times Y$, 
where $X = \{X_1, \ldots, X_p\}$ represents predictive features and $Y$ is the target feature. %$f_\theta$ belongs to a family of functions ${\cal F}$ parametric in $\theta$.
The domain of the target feature is $\mathrm{dom}(Y)=\{0, 1\}$ (binary classification) or $\mathrm{dom}(Y) = \mathbb{R}$ (regression). For binary classification, we assume a probabilistic classifier, and we  denote by $f_\theta(x)$ the estimates of the probability $P(Y=1|X=x)$ over the (unknown)  distribution of $X \times Y$. For regression, $f_\theta(x)$ estimates $E[Y|X=x]$. We call the projection of $\D$ on $X$, written $\D_X{} = \{x_1, \ldots, x_n\} \sim X$,  the empirical \textit{input distribution}. 
The dataset $f_\theta({\D_X}) = \{f_\theta(x) \ |\ x \in \D_X{} \}$ is called the empirical \textit{prediction distribution}.

We assume a feature modeling protected social groups denoted by $Z$, called \textit{protected feature}, and assume it to be binary valued in the theoretical analysis. $Z$ can either be included in the predictive features $X$ used by a model or not. If not, we assume that it is still available for a test dataset. Even without the protected feature in training data, a model can discriminate against the protected groups by using correlated features as a proxy of the protected one~\citep{DBLP:conf/kdd/PedreschiRT08}.



%\steffen{reinstate if needed later}
%Let two datasets $\D, \D'$ define two empirical distributions $\PP(\D), \PP(\D')$. We write $\PP(\D) \nsim \PP(\D')$ to express that $\PP(\D)$ is sampled from a different underlying distribution than $\PP(\D')$ with high probability $p>1-\epsilon$
We write $A \bot B$
to denote statistical independence between the two sets of random variables $A$ and $B$, or equivalently, between two multivariate proability distributions. We define two common fairness notions and corresponding fairness metrics that quantify a model's degree of discrimination or unfairness \citep{DBLP:journals/csur/MehrabiMSLG21}. 

\begin{definition}\textit{(Demographic Parity (DP))}. A model $f_\theta$ achieves demographic parity if $f_\theta(X) \perp Z$\label{def:dp}.
\end{definition}

Thus, demographic parity holds if  $\forall z.\,P(f_\theta(X)|Z=z)=P(f_\theta(X))$. For binary $Z$'s, we can derive an unfairness metric as $d(P(f_\theta(X)|Z=1),P(f_\theta(X))$, where $d(\cdot)$ is a distance between probability distributions.

\begin{definition}\textit{(Equal Opportunity (EO))}\label{eq:TPR} A model $f_\theta$ achieves equal opportunity if $\forall z.\,P(f_\theta(X)|Y=1,Z=z) = P(f_\theta(X)=1|Y=1)$.\label{def:eo}
\end{definition}
As before, we can measure unfairness for binary $Z$'s as  $d(P(f_\theta(X)|Y=1,Z=1),P(f_\theta(X)=1|Y=1))$. Equal opportunity comes with the problem that labels for correct outcomes are required.


\subsection{Philosophical Foundations and Computable Fairness Metrics}

Political and moral philosophers from the \textbf{egalitarian} school of thought often consider \emph{equal opportunity} to be the key promoter of fairness and social justice,  providing individuals with equal chances to succeed regardless of their background or circumstances ~\cite{rawls1958justice,rawls1991justice}, \cite{dworkin1981equality,dworkin1981equality2}, \cite{arneson1989equality}, \cite{cohen1989currency}. \cite{DBLP:conf/nips/HardtPNS16} proposed translating equal opportunity into the fair machine learning field, formalizing it  as the inter-group difference on the true positive rate. \citep{DBLP:conf/fat/HeidariLGK19} provided a moral framework to ground such metrics of equal opportunity. 
% \steffen{I do not give an example now because it becomes too tricky for equal treatment}
%To provide a simple example from the egalitarian point-of-view,  decisions to admit students to university become fairer by affirmative action considering applicants' full potential.

The \textbf{liberalism} school of thought argues that  individuals should be treated equally independently of outcomes~\cite{friedman1990free,nozick1974anarchy}. Equal treatment has also been defined as equal treatment-as-blindness or neutrality~\cite{sunstein1992neutrality,miller1959myth}. From the technical implementation perspective, the notion of \emph{equal treatment} has often been understood as \emph{equal outcomes} and translated to technical measures such as demographic or statistical parity (used synonymously). 
As we will analyze in Section~\ref{sec:outcome-fairness}, equal outcomes imply that two demographic groups experience the same distribution of outcomes, even if the first of the two groups have much better prospects for achieving the predicted outcome. Thus, a model $f$ that achieves equal outcome may have to prefer individuals from one group over those from another group, violating the requirement for equal treatment of all individuals. Our metrics for equal treatment remedy this drawback.
%Nonetheless, our approach differs from previous metrics in that we measure the \emph{equal treatment} of a model based on how each feature contributes to the prediction instead of relying on notions of outcomes based on aggregated model predictions. If \emph{equal treatment} is achieved, then \emph{equal outcome} too, but not vice-versa (see Lemma \ref{lemma:inc}). 

In Appendix~\ref{app:fair.notions}, we provide an illustrative use-case of when equal treatment is in general desired, but  neither equal opportunity nor equal outcomes can model this: scientific paper blind reviews.





\subsection{Related Work}
We briefly review related works below. See Appendix \ref{app:relatedWork} for an in-depth comparison of our work to existing research.
%For an in-depth comparison of our work to existing research, the reader may refer to Appendix~\ref{app:relatedWork}.


\textbf{Fairness and auditing}. Previous work has relied on the notion of equal outcomes by measuring and calculating demographic and statistical parity on the model predictions \citep{DBLP:conf/fat/RajiSWMGHSTB20,DBLP:conf/icml/KearnsNRW18}.
The arguments of ~\cite{DBLP:conf/aies/SimonsBW21} provide background and motivation towards interpretations of equal treatment as blindness and discuss its importance in practical policy applications. 

\textbf{Explainability for fair supervised learning}.  \cite{lundberg2020explaining} apply Shapley values to statistical parity.
While there is a slight overlap with our work,  their 2-page extended abstract is not comparable to our paper wrt.\ objectives, breadth, and depth.
%They do not discuss the difference between \emph{equal outcome} and \emph{equal treatment}. 
%work builds on this research approach and contributes by formalizing the explanation distributions, deriving theoretical guarantees, proposing novel methodology, and investigating Shapley values for equal treatment.


A recent line of work assumes knowledge about causal relationships between random variables. For example,~\cite {DBLP:conf/fat/GrabowiczPM22} presents a post-processing method based on Shapley values aiming to detect and nullify the influence of a protected attribute on the output of the system. They assume known direct causal links from the data to the protected attribute and no measured confounders. Our work does not rely on causal graphs knowledge but exploits the Shapley values' theoretical properties to obtain fairness model auditing guarantees. 


\textbf{Classifier two-sample test}. Our work uses a classifier two-sample test to measure the independence of the model concerning the protected attribute. The use of classifiers to measure statistical tests or independence has been previously explored, with \cite{DBLP:conf/iclr/Lopez-PazO17} proposing a classifier-based approach that returns test statistics to interpret differences between the two distributions. Our two-sample test rely on AUC rather than accuracy, with the advantage of a direct application to the case of non-equal distribution of target labels.
