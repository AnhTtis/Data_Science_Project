\section{Introduction}

% What is the problem?
In philosophy, long-held discussions about what constitutes  a fair or an unfair political system have led to established frameworks of distributive justice~\citep{distributiveJustice}. 
From the \emph{egalitarian} school of thought, proponents like~\cite{rawls1958justice} argued for \emph{equal opportunity}, which has been translated into computable metrics by the same name~\citep{DBLP:conf/nips/HardtPNS16}. From a machine learning perspective, the technical drawback is that  metrics for equal opportunity require label annotations for true positive outcomes, which are not always available for minorities.  



The \emph{liberalism}  school of thought\footnote{We use the term \emph{liberalism} to refer to the perspective exemplified by~\cite{friedman1990free}. This perspective can also  be referred to as \emph{neoliberalism} or \emph{libertarianism}~\citep{distributiveJustice,wiki_friedman}. }, put forward  by scholars such as~\cite{friedman1990free} or~\cite{nozick1974anarchy}, requires \emph{equal treatment} of individuals regardless of their protected characteristics. 
This requirement has been translated by the machine learning literature~\citep{DBLP:conf/aies/SimonsBW21,DBLP:conf/fat/HeidariLGK19,wachter2020bias} into the requirement that machine learning predictions should achieve \emph{equal outcomes} for groups with different protected characteristics. The corresponding measure,  \textit{demographic parity} (also called \emph{statistical  parity}), compares the different distributions of predicted outcomes  of a model $f$ for different demographic groups. We will show, however, that the measure of demographic parity may indicate fairness, although groups are indeed treated differently.
%\steffen{Which sentence do you refer to in your remark?}\carlos{I don't understand the --following-- sentence "demographic groups will likewise succeed if predicted to become successful"}. We will show that demographic parity measures equal treatment only if the assumption holds that different demographic groups will likewise succeed if predicted to become successful. In practice, this assumption may be easily violated. 

We leave the normative discussion of which philosophical paradigm should be pursued by policy to the discourse in the social sciences and the broad public. However, our analysis contributes to a foundational understanding of fairness in machine learning. Moreover, we remedy the divergence between the liberalism-oriented philosophical requirement of equal treatment and actual measures used in fair machine learning approaches by proposing a novel method for measuring {equal treatment}. 

Comparing different demographic groups, we measure how non-protected features of individuals interact with the trained model $f$ as explained by Shapley value attributions~\citep{DBLP:conf/nips/LundbergL17}. If two demographic groups are treated the same, the distributions of interaction behavior, which we call \emph{explanation distributions}, will not be distinguishable. We introduce a tool, the \emph{Equal Treatment Inspector}, that implements this idea. When recognizing disparate treatment, it explains the features involved in unequal treatment, supporting understanding the causes of  unfairness underlying the machine learning model. In summary, our contributions are:

\begin{enumerate}
    \setlength\itemsep{0.1em}

    \item The definition of \textit{explanation distributions} as a basis for measuring {equal treatment}.

    \item The definition of a novel method for \emph{recognizing and explaining disparate treatment}.

    \item  Theoretical properties about the relationship between protected attributes and explanation distributions.

    
    \item The formal relationship between metrics for \emph{equal outcome} and \emph{equal treatment}.

    \item A study of synthetic and natural data demonstrates our method's benefits. 

    \item An open-source Python package \texttt{explanationspace}, which implements the \emph{Equal Treatment Inspector} that is \texttt{scikit-learn} compatible together with documentation and tutorials.
     
\end{enumerate}

