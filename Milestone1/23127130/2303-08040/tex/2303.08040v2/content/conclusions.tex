\section{Conclusions}
We introduced a novel approach for fairness in machine learning by measuring \emph{equal treatment}. While related work
employed model predictions to judge  \emph{equal outcomes}, our measure of {equal treatment} is more fine-grained, accounting for the usage of attributes by the model via explanation distributions. Consequently, {equal treatment} implies {equal outcomes}, but the converse is not necessarily true. 

This paper also seeks to improve the understanding of how theoretical concepts of fairness from liberalism-oriented political philosophy align with technical measurements. Our measure of equal treatment diverges from a naive understanding of liberalism. When one demographic group is structurally disadvantaged compared to another group, our measure of equal treatment will pick this up via all the non-protected attributes that are proxies for protected characteristics. Therefore, our measure of equal treatment is much more nuanced than approaches that would oversimplify by only ignoring protected attributes. Implications warrant further techno-philosophical discussions.

%\steffen{We should note that truly equal treatment, such as measured by us, is nearly impossible to achieve because our measure also considers the structural differences between demographic  groups. Therefore, the machine learning aim can rarely be to achieve perfect equal treatment, but rather an improvement towards better equal treatment measures, such as illustrated by our experiments.}\steffen{My last comment may be too hard to understand without reference to the university admissions example that we discussed, Carlos. In this fictitious example, there is only one non-protected attribute (SAT score), demographic group 1 scored a SAT below 1500, and demographic group 2 scored a SAT above 1500; 1500 is the college admission boundary. The non-protected attribute is a perfect proxy, and the only way to achieve equal treatment as measured by us is to ignore the SAT score. This is not exactly what radical liberalists would suggest. Still, it is the consequent implementation of not looking at demographic status --- which is only possible if proxy attributes of demographic status are also discarded.}
%\steffen{A discussion, like this, in conclusion, would tie the knot between the beginning and the end of our paper.}

\textbf{Limitations:}
Political philosophical notions of distributive justice are more complex than we can account for in this paper.
Our research has focused on tabular data using Shapley values, which allow for theoretical guarantees but may differ from their computational approximations. 
%open to multiple interpretations from different angles. While our results are promising, it's essential to acknowledge that our research is centered around tabular data and is based on Shapley values, which provide theoretical guarantees that may differ from the SHAP approximation (a computational approach). 
It is possible that alternative AI explanation techniques, such as feature attribution methods, logical reasoning, argumentation, or counterfactual explanations, could be useful and offer their unique advantages to definitions of equal treatment. It is important to note that employing fair AI techniques does not necessarily ensure fairness in socio-technical systems based on AI, as stated in~\cite{DBLP:conf/fat/KulynychOTG20}.
\subsection*{Reproducibility Statement}\label{sec:reproducibility}
To ensure the reproducibility of our results, we make the data, data preparation routines, code repositories, and methods publicly available \url{https://anonymous.4open.science/r/xAIAuditing-1841/README.md}.
Also, an open-source Python package \texttt{explanationspace}\url{https://anonymous.4open.science/r/explanationspace-B4B1/README.md}. We use default \texttt{scikit-learn} parameters \cite{DBLP:journals/jmlr/PedregosaVGMTGBPWDVPCBPD11},unless stated otherwise. We describe the system requirements and software dependencies of our experiments. Our experiments were run on a 4 vCPU server with 32 GB RAM.

