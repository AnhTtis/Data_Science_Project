\section{A Model for Monitoring Equal Treatment}
%\steffen{Do we need subheadings for 3.1 and 3.2?}
\subsection{Formalizing Equal Treatment}

To establish a criterion for equal treatment, we rely on the notion of explanation distributions.

\begin{definition}\textit{(Explanation Distribution)} An explanation function $\Ss:{\cal F}\times X\to \mathbb{R}^p$ 
maps a model $f_\theta \in {\cal F}$ and an input instance $x \in X$ into a vector of reals $\Ss(f_\theta, x) \in \mathbb{R}^p$. We extend it by mapping an
input distribution $\D_X$ into an (empirical) \textit{explanation distribution} as follows:
$\Ss(f_\theta, \D_X) = \{ \Ss(f_\theta, x)\ |\ x \in \D_X\} \subseteq \mathbb{R}^p$.
\end{definition}

We use Shapley values as an explanation function (cf.\ Appendix~\ref{app:foundation.xai}). In Appendix \ref{app:LIME}, we discuss the usage of LIME. Let us introduce next the new fairness notion of Equal Treatment, which considers the independence of the model's explanations with the membership to a~social~group.  

\begin{definition}\textit{(Equal Treatment (ET))} A model $f_\theta$ achieves ET if  $\Ss(f_\theta,X) \perp Z$.\label{def:et}
\end{definition}
%
%
Such a definition characterizes the philosophical notion of Equal Treatment by encoding the \enquote{treatment} performed by the model through the attribution of the importance of its input features.
As we will see later in Section~\ref{sec:theory}, Equal Treatment is a stronger notion than Demographic Parity since it not only requires that the distributions of the predictions are similar but that the processes of how predictions are made (i.e., the explanations) are also~similar.

\subsection{Equal Treatment Inspector}\label{sec:demographicParityInspector}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/DPI.drawio.pdf}
    \caption{Equal Treatment Inspector workflow. The  model $f_\theta$ is learned based on training data, $\D^{tr} = \{(x_i,y_i)\}$,  and outputs the explanations $\Ss(f_\theta,\D_X^{val})$. The C2ST receives the explanations to predict the protected attribute, $Z$ on validation data $\D^{val}$. The AUC of the C2ST classifier $g_\psi$ on test data $\D^{te}$ decides for or against \emph{equal treatment}.  We can interpret the driver for unequal treatment on $g_\psi$ with explainable AI techniques.}
    \label{fig:workflow} 
     %\antonio{in the image the diagram we have auc = 0.5, while in the text later we use 1/2. maybe it is nicer if we always write 1/2}\carlos{In the word we use nicefrac which is not compatible in draw.io Classic frac looks too big}
\end{figure}

Our approach is based on the  properties of the Shapley values (cf. Appendix \ref{app:foundation.xai})  and on a novel classifier two-sample test. We split the available data into three parts $\Dd{tr},\Dd{val},\Dd{te} \subseteq X \times Y$. Here $\Dd{tr}$ is the training set of $f_\theta \in \cal F$ (not required if $f_\theta$ is already trained). Following the intuition above, $\Dd{val}$  is used to train another model $g_\psi$, called the \enquote{Equal Treatment Inspector}. Here, the predictive features are the explanation distribution $\Ss(f_\theta, \Dd{val}_{X\setminus{Z}})$ (excluding $Z$) and the target feature is the  protected feature $Z$. The model $g_\psi$ belongs to a family $\cal G$, possibly different from $\cal F$. The parameter $\psi$ optimizes a loss function $\ell$:
\begin{gather}\label{eq:fairDetector}
\psi = \argmin_{\tilde{\psi}} \sum_{(x, z) \in \Dd{val} } \ell( g_{\tilde{\psi}}(\Ss(f_\theta,x)) , z )
\end{gather}
%
%
%
To evaluate whether there is an equal treatment violation, we perform a statistical test of independence based on the AUC of $g_\psi$ on a test set $\Dd{te}$. We also use $\Dd{te}$ for testing the approach w.r.t.~baselines.  
%
Besides detecting fairness violations, a common desideratum is to understand what are the specific features driving such violations. The \enquote{Equal Treatment Inspector} $g_\psi$ can provide information on \textit{which features are the cause of the un-equal treatment} either by-design, if it is an interpretable model, or by applying post-hoc explainations techniques, e.g., Shapley values. %The \enquote{Equal Treatment Inspector} can provide different types of explanations, re-purposing the goal of the traditional explainable AI field, from understanding the model predictions to accounting for the reasons of un-equal treatment.
%
See Figure \ref{fig:workflow} for a visualization of the whole workflow.
