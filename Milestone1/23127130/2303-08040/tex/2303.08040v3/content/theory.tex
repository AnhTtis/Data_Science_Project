\section{Theoretical Analysis}\label{sec:theory}

Throughout this section, we assume an exact calculation of the Shapley values $\Ss(f_\theta, x)$ for an instance $x$, possibly for the  observational and interventional variants (see (\ref{def:val:obs},\ref{def:val:int}) in Appendix~\ref{app:foundation.xai}). In the experimental section, we will use non-IID data and non-linear models.

%\steffen{I do not understand the following subsection title. Also, such a title should fit on one line to save space. However, it should work as a stand-alone piece of information even though it cannot tell the full story}\carlos{choose}
%\subsection{Limitations of the Shapley Values of the Protected Attribute in Measuring Equal Treatment}

\subsection{Equal Treatment Given Shapley Values of Protected Attribute}\label{sec:expSpaceIndependence}

Can we measure ET by looking only at the Shapley value of the protected feature? The following result considers a linear model (with unknown coefficients) over \textit{independent} features. In such a very simple case, resorting to Shapley values leads to an exact test of both DP and ET, which turn out to coincide. In the following, we write $\mathit{distinct}(\D_X{}, i)$ for the number of distinct values in the $i$-th feature of dataset $\D_X{}$, and $\Ss(f_\beta, \D_X{})_i \equiv 0$ if the Shapley values of the $i$-th feature are all $0$'s.

\begin{lemma}\label{lemma:1} Consider a linear model $f_\beta(x) = \beta_0 + \sum_j \beta_j \cdot x_j$. Let $Z$ be the $i$-th feature, i.e. $Z = X_i$, and let $\D_X{}$ be such that $\mathit{distinct}(\D_X{}, i)>1$.
If the features in $X$ are independent, then $\Ss(f_\beta, \D_X{})_i \equiv 0 \Leftrightarrow f_\beta(X) \perp Z \Leftrightarrow \Ss(f_\beta,X) \perp Z$.
\end{lemma}
\begin{proof}
It turns out $\Ss(f_\beta, x)_i = \beta_i \cdot (x_i - E[X_i])$. This holds in general for the interventional variant (\ref{def:val:int}), and assuming independent features, also for the observational
variant (\ref{def:val:obs})
\citep{DBLP:journals/ai/AasJL21}. Since $\mathit{distinct}(\D_X{}, i)>1$, we have that $\Ss(f_\beta, \D_X{})_i \equiv 0$ iff  $\beta_i = 0$. By independence of $X$, this is equivalent to $f_\beta(X) \perp X_i$, i.e., $f_\beta(X) \perp Z$. Moreover, by the propagation of independence, this is also equivalent to $\Ss(f_\beta,X) \perp Z$.
\end{proof}

However, the result does not extend to the case of dependent features. 

\begin{example}
Consider $Z = X_2 = X_1^2$, and the linear model $f_\beta(x_1, x_2) = \beta_0 + \beta_1 \cdot x_1$ with $\beta_1 \neq 0$ and $\beta_2 = 0$, i.e., the protected feature is not used by the model. In the interventional variant, the uninformativeness property implies that $\Ss(f_\beta, x)_2 = 0$. However, this does not mean that $Z = X_2$ is independent of the output because $f_\beta(X_1, X_2) = \beta_0 + \beta_1 \cdot X_1 \not \perp X_2$. In the observational variant, \cite{DBLP:journals/ai/AasJL21} show that:
\[
val(T) = \sum_{i \in N\setminus T} \beta_i \cdot E[X_i| X_T = x^{\star}_T] + \sum_{i \in T} \beta_i \cdot x^{\star}_i
\]
\noindent from which, we calculate:
$\Ss(f_\beta, x^{\star})_2 = \frac{\beta_1}{2} E[X_1|X_2 =x^{\star}_2]$.
We have $\Ss(f_\beta, \D_X)_2 \equiv 0$ iff $E[X_1|x_2 =x^{\star}_2] = 0$ for all $x^{\star}$ in $\D_X{}$. For the  marginal distribution $P(X_1=v) = 1/4$ for $v=1, -1, 2, -2$, and considering that $X_2=X_1^2$, it holds that $E[X_1|x_2 =v] = 0$ for all $v$. Thus $\Ss(f, \D_X)_2 \equiv 0$. However, again $f_\beta(X_1, X_2) = \beta_0 + \beta_1 \cdot X_1 \not \perp X_2$.
\end{example}
The counterexample shows that focusing only on the Shapley values of the protected feature is not a viable way to prove DP of a model -- and, a fortiori, neither to prove ET of the model, as will show in Lemma \ref{lemma:inc}.

%\subsection{Equal Treatment on the Explanation Distributions}

\subsection{Equal Treatment vs Equal Outcomes vs Fairness of the Input}\label{subsec:ETvsEOvsInd}

%\steffen{I would suggest discarding this introductory sentence. I would also suggest to remove the subsubsection titles and degrade them towards paragraph titles, saving space. The subsubsections are very short anyway. }

%This section is divided into two parts that comprise the theoretical comparisons of ET on  the explanations distributions against $(i)$ DP the prediction distribution and $(ii)$ ET on the input data.

\label{sec:outcome-fairness}
%\textbf{Explanation Distributions vs Predictions Distributions}\\
We start by observing that \emph{equal treatment} (independence of the explanation distribution from the protected attribute) is a sufficient condition for \emph{equal outcomes} measured as demographic parity (independence of the prediction distribution from the protected attribute).
%\steffen{Why does the proof-environment waste such a lot of whitespace before the proof and after the proof? Correcting this will not violate layout guidelines}\carlos{I have played with space, but I am not sure if it violates. Salvatore?}

\begin{lemma}\label{lemma:inc} If $\Ss(f_\theta, X) \perp Z$ then $f_\theta(X) \perp Z$.
\end{lemma}
\begin{proof}
By the propagation of independence in probability distributions, the premise implies $(\sum_i \Ss_i(f_\theta, X) + c) \perp Z$ where $c$ is any constant. By setting $c=E[f(X)]$ and by the efficiency property (\ref{eq:eff}), we have the conclusion.
\end{proof}

Therefore, a DP violation (on the prediction distribution) is also a ET violation (in the explanation distribution). ET accounts for a stricter notion of fairness. The other direction does not hold. We can have dependence of $Z$ from the explanation features, but the sum of such features cancels out resulting in perfect DP on the prediction distribution. This issue is also known as Yule's effect~\citep{DBLP:conf/aaai/Ruggieri0PST23}.

\begin{example}\label{ex42} Consider the model $f(x_1, x_2) = x_1 + x_2$. Let $Z \sim Ber(0.5)$, $A \sim U(-3, -1)$, and $B \sim N(2, 1)$ be independent, and let us define:
\[ X_1 = A \cdot Z + B \cdot (1-Z) \quad X_2 = B \cdot Z + A \cdot (1-Z)\]
We have $f(X_1, X_2) = A + B \perp Z$ since $A, B, Z$ are independent.
%
Let us calculate $\Ss(f, X)$ in the two cases $Z=0$ and $Z=1$. If $Z=0$, we have $f(X_1, X_2) = B + A$, and then $\Ss(f, X)_1 = B-E[B] = B-2 \sim N(0, 1)$ and $\Ss(f, X)_2 = A-E[A] = A+2 \sim U(-1, 1)$. Similarly, for $Z=1$, we have $f(X_1, X_2) = A + B$, and then $\Ss(f, X)_1 = A-E[A]=A+2 \sim U(-1, 1)$ and $\Ss(f, X)_2 = B-E[B] = B-2 \sim N(0, 1)$. This shows:
\[ P(\Ss(f, X) | Z=0) \neq P(\Ss(f, X) | Z=1)\]
and then $\Ss(f, X) \not \perp Z$. Notice this example holds both for the interventional and the observational cases, as we exploited Shapley values of a linear model over independent features, namely $A, B, Z$.
\end{example}

%A general underpinning of measuring demographic parity in the prediction space is this second scenario, where the different contributions of each feature for the prediction cancel out and the total prediction remains the same $P(f_\theta(X)|Z=0)=P(f_\theta(X)|Z=1)$, even though the model is dependent on the protected attribute  $f_\theta \perp Z$ . 
 

%\textbf{Explanation Distribution vs Input Data Distributions}\\
Statistical independence between the input $X$ and the protected attribute $Z$, i.e.,  $X \perp Z$, is another fairness notion. It targets fairness of the (input) datasets, disregarding the model $f_\theta$. For fairness-aware training algorithms, which are able not to (directly or indirectly) rely on $Z$, violation of such a notion of fairness does not imply ET violation nor DP violation.

\begin{example}
Let $X = X_1,X_2,X_3$ be independent features 
such that $E[X_1] = E[X_2] = E[X_3] = 0$, and $X_1, X_2 \perp Z$, and $X_3 \not \perp Z$. The target feature is defined as $Y = X_1 + X_2$, hence it is also independent from $Z$. Assume a linear regression model $f_\beta(x_1, x_2,x_3) = \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_3 \cdot x_3$ trained from a sample data from $(X, Y)$ with $\beta_1, \beta_2 \approx 1$ and $\beta_3 \approx 0$. Intuitively, this occurs when a number of features are collected to train a classifier without a clear understanding of which of them contributes to the prediction. It turns out that $X \not \perp Z$ but, for $\beta_3 = 0$ (which can be obtained by some fairness regularization method \citep{DBLP:conf/icdm/KamishimaAS11}), we have $f_\beta(X_1, X_2, X_3) = \beta_1 \cdot X_1 + \beta_2 \cdot X_2 \perp Z$. By reasoning as in the proof of Lemma \ref{lemma:1}, we have $\Ss(f_\beta, X) = (\beta_1 \cdot X_1, \beta_2 \cdot X_2, 0)$ and then $\Ss(f_\beta, X) \perp Z$. This holds both in the interventional and in the observational variants.
\end{example}

The above represents an example where the input data depends on the protected feature, but the model and the explanations are independent. %In this case, the explanation distribution correctly detects that there is no ET violation on the model.

\subsection{Equal Treatment Inspection via  Explanation Distributions}

%We provide here the theoretical foundations about the \enquote{Equal Treatment Inspector}. 


\subsubsection{Statistical Independence Test via Classifier AUC Test}\label{sec:stat.independence}

In this subsection, we introduce a statistical test of independence based on the AUC of a binary classifier. The test of $W \perp Z$ is stated in general form for multivariate random variables $W$ and a binary random variable $Z$ with $dom(Z) = \{0, 1\}$. In the next subsection, we will instantiate it to the case $W = \Ss(f_\theta, X)$.

Let $\D = \{ (w_i, z_i) \}_{i=1}^n$ be a dataset of realizations of the random sample $(W, Z)^n \sim \mathcal{F}^n$ where $\mathcal{F}$ is unknown.
The independence $W \perp Z$ can be tested via a two-sample test. In fact,
we have $W \perp Z$ iff $P(W|Z)=P(W)$ iff $P(W|Z=1) = P(W|Z=0)$. We test whether the positives and negatives instances in $\D$ are drawn from the same distribution by a novel two-sample test, which does not require permutation of data nor equal proportion of positive and negatives as in \cite[Sections 2 and 3]{DBLP:conf/iclr/Lopez-PazO17}. We rely on a probabilistic classifier $f: W \rightarrow [0, 1]$, for which $f(w)$ estimates $P(Z=1|W=w)$, and on its AUC:
\begin{gather}\label{eq:auc}
AUC(f) = E_{\small(W,Z), (W',Z') \sim \mathcal{F}}[I( (Z-Z')(f(W)-f(W')) > 0) + \nicefrac{1}{2} \cdot I(f(W)=f(W')) | Z \neq Z'] 
\end{gather}

Under the null hypothesis $H_0: W \perp Z$, we have $AUC(f)=\nicefrac{1}{2}$. 
\begin{lemma}\label{lemma:test}
If $W \perp Z$ then  $AUC(f)=\nicefrac{1}{2}$ for any classifier $f$.
\end{lemma}
\begin{proof}
Let us recall the definition of the Bayes Optimal classifier $f_{opt}(w) = P(Z=1|W=w)$.
For any classifier $f$, we have:
%
\begin{gather}\label{eq:aucineq} 
AUC(f_{opt}) \geq AUC(f) \geq 1 - AUC(f_{opt})
\end{gather}
The first bound $AUC(f_{opt}) \geq AUC(f)$ follows because the Bayes Optimal classifier minimizes the Bayes risk~\citep{DBLP:conf/ijcai/GaoZ15}.
Assume the second bound does not hold, i.e., for some $f$ we have $AUC(f_{opt}) < 1 - AUC(f)$. Consider the classifier $\bar{f}(w) = 1-f(w)$. We have $AUC(\bar{f}) \geq 1-AUC(f)$, and then $\bar{f}$ would contradict the first bound because $AUC(f_{opt}) < AUC(\bar{f})$.

If $W \perp Z$, then $P(Z=1|W=w) = P(Z=1)$, and then $f_{opt}(w)$ is constant. By (\ref{eq:auc}), this implies $AUC(f_{opt})=\nicefrac{1}{2}$.
By (\ref{eq:aucineq}), this implies
$AUC(f)=\nicefrac{1}{2}$ for any classifier.
\end{proof}

As a consequence, any statistics to test $AUC(f)=\nicefrac{1}{2}$ can be used for testing $W \perp Z$. %We proceed as follows.
%
%First, we randomly partition $\D = \D^{tr} \cup \D^{te}$ into a training and test set. Second, $\D^{tr}$ is used to train a probabilistic binary classifier $f: W \rightarrow [0,1]$ for which $f(w)$ is an estimate of the conditional probability $P(Z=1|W=w)$. Third, we test $H_0: AUC(f)=\nicefrac{1}{2}$.  
A classical choice is to resort to the Wilcoxon–Mann–Whitney test, which, however, assumes that the distributions of scores for positives and negatives have the same shape. Better alternatives include the Brunner–Munzel test \citep{DBLP:journals/csda/NeubertB07} and the Fligner–Policello test \citep{fligner1981robust}. The former is preferable, as the latter assumes that the distributions are symmetric.

\subsubsection{Testing for Equal Treatment via an Inspector}

We instantiate the previous AUC-based method for testing independence to the case of testing for Equal Treatment via an ET Inspector.

\begin{theorem}\label{thm:main}
Let $g_\psi: \Ss(f_\theta, X) \rightarrow [0,1]$ be an \enquote{Equal Treatment Inspector} for the model $f_{\theta}$, and $\alpha$ a significance level. We can test the null hypothesis $H_0: \Ss(f_\theta, X) \perp Z$ at $100 \cdot (1-\alpha)\%$ confidence level using a test statistics of $AUC(g_\psi) = \nicefrac{1}{2}$. 
\end{theorem}
\vspace{-0.3cm}
\begin{proof}
Under $H_0$, by Lemma \ref{eq:auc} with $W = \Ss(f_\theta, X)$ and $f = g_\psi$, we have $AUC(g_\psi) = \nicefrac{1}{2}$. 
\end{proof}
\vspace{-0.3cm}
Results of such a test can include $p$-values of the adopted test for $AUC(g_\psi) = \nicefrac{1}{2}$. Alternatively, confidence intervals for $AUC(g_\psi)$ can be reported, as returned by the Brunner–Munzel test or by the methods  \citep{delong1988comparing,DBLP:conf/nips/CortesM04,gonccalves2014roc}.



\subsubsection{Explaining Un-Equal Treatment}
%\todo[inline]{Could this be moved to the appendix?}

The following example showcases one of our main contributions: detecting \textit{the sources} of un-equal treatment through interpretable by-design (linear) inspectors. Here, we assume that the model is also linear. In the Appendix \ref{app:xai.eval}, we will experiment with non-linear models.

\begin{example} Let $X = X_1,X_2,X_3$ be independent features 
such that $E[X_1] = E[X_2] = E[X_3] = 0$, and $X_1, X_2 \perp Z$, and $X_3 \not \perp Z$. %Assume now that $Y = \alpha_0 + \alpha_1 \cdot X_1 + \alpha_2 \cdot X_2 + \alpha_3 \cdot X_3$. 
Given a random sample of i.i.d.~observations from $(X, Y)$, a linear model $f_\beta(x_1, x_2, x_3) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_3 \cdot x_3$ can be built by OLS (Ordinary Least Square) estimation, possibly with $\beta_1, \beta_2, \beta_3 \neq 0$. By reasoning as in the proof of Lemma \ref{lemma:1}, $\Ss(f_\beta, x)_i = \beta_i \cdot x_i$. Consider now a linear ET Inspector $g_\psi(s) = \psi_0 + \psi_1 \cdot s_1 + \psi_2 \cdot s_2+\psi_3 \cdot s_3$, which can be written in terms of the $x$'s as: $g_\psi(x) = \psi_0 + \psi_1 \cdot \beta_1 \cdot x_1 + \psi_2 \cdot \beta_2 \cdot x_2+\psi_3 \cdot \beta_3 \cdot x_3$. By OLS estimation properties, we have $\psi_1 \approx cov(\beta_1 \cdot X_1, Z)/var(\beta_1 \cdot X_1) = cov(X_1, Z)/(\beta_1 \cdot var(X_1))  = 0$ and analogously $\psi_2 \approx 0$. Finally, $\psi_3 \approx cov(X_3, Z)/(\beta_3 \cdot var(X_3)) \neq 0$. In summary,  the coefficients of $g_\psi$ provide information about which feature contributes (and how much it contributes) to the dependence between the explanation $\Ss(f_\beta, X)$ and the protected feature $Z$. Notice that also $f_\beta(X) \not \perp Z$, but we cannot explain which features contribute to such a dependence by looking at $f_\beta(X)$, since $\beta_i \approx cov(X_i, Y)/var(X_i)$ can be non-zero also for $i = 1, 2$.
\end{example}

%In this case, we provide the mathematical analysis extending from the previous IID section (cf. Section \ref{sec:expSpaceIndependence}), but one of the features is correlated with the protected attribute which is not present on the training data. Let us consider a simple linear model $f_\beta(x_1, x_2,x_3) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2+\beta_3 \cdot x_3$ with $X_1 \perp X_2 \perp X_3$ independent but $X_3$ and $Z$ dependent by the parameter $\gamma(X_3,Z)$. Let $(X_1,X_2) \sim  N\left(0,\begin{bmatrix}1 & 0 \\0& 1 \end{bmatrix}\right)$ and $(X_3,X_4) \sim  N\left(0,\begin{bmatrix}1 & \gamma \\\gamma& 1 \end{bmatrix}\right)$, then $Z$ is a binary protected attribute that takes value  $1\quad\texttt{if} \quad X_4>0,\quad \texttt{else}\quad 0$. Then our synthetic target $y = \alpha_1\cdot X_1+ \alpha_2\cdot X_2+ \alpha_3\cdot X_3+\epsilon$, where $\epsilon$ is white noise $\epsilon \sim N(0,0.1)$. Let us also consider a linear auditor $g_\psi = \psi_1 \cdot x_1 + \psi_2 \cdot x_2+\psi_3 \cdot x_3$. 
%\begin{theorem}
%If $X_3$ is not independent of the protected attribute $Z$ and the rest of the features are independent of the protected attribute $X_1 \perp Z$ and $X_2 \perp Z$, then for the coefficients of the auditor ($g_\psi$) only $\psi_3$ is not null, in formulas: if $\gamma(X_3,Z)\neq  0 $, and $ X_1 \perp X_2 \perp Z \Rightarrow \psi_1=0,\psi_2=0,\psi_3\neq0$ 
%\end{theorem}
%If the variables are IID and the models $f_\beta$ and $g_\psi$ belong to the linear regression family then we can calculate the exact Shapley values~\cite{DBLP:journals/ai/AasJL21}.
%\begin{gather}
%\texttt{If}\quad X_1\perp Z, X_2\perp Z,  X_3\not\perp Z\\ 
%\texttt{and}\quad y = \alpha_1 X_1+\alpha_2 X_2+\alpha_3 X_3 + \epsilon\\
%\Rightarrow f_\beta(x) = \beta_1 \cdot x_1+\beta_2\cdot x_2+\beta_3\cdot x_3\\
%\Ss_j(f_\beta,x) = \beta_j\cdot x_j
%\end{gather}
%Then we can train our \enquote{Demographic Parity Inspector}  $g_\psi$ on the explanation space to predict the protected attribute: 
%\begin{gather}
%\psi = \argmin_{\psi} \sum_{n\in X^{\text{val}}} \ell( g_{\psi}(\Ss(f_\beta,x^n)) , z^n )\\
%g_\psi = \psi_1 \Ss_1(f_\beta,x_i)+\psi_2 \Ss_2(f_\beta,x_i)+\psi_3 \Ss_3(f_\beta,x_i)\\
%\end{gather}
%Then since the features $X_1$ and $X_2$ are independent among themselves and w.r.t the protected attribute $P(S_i(f_\beta,X)|Z))=P(S_i(f_\beta,X))$ we have that
%\begin{gather}
%\Rightarrow \psi_1 = 0, \psi_2 = 0 , \psi_3 \neq 0
%\end{gather}
%Since we know the dependence of the feature $X_3$ with the protected attribute $Z$ is parametrized by $\gamma(X_3,Z)$, then the $\psi_3$ coefficient will be proportional to this relationship
%\begin{gather}
%    \psi_3 \propto \gamma(X_3,Z)
%\end{gather}

