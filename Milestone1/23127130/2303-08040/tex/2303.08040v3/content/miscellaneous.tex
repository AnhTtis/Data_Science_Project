Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of
(un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. 
However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a  new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide  a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release \texttt{nobias} an opensource Python package with methods, routines, and tutorials.

\section{Introduction}
%%%%%%% have a look at this %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\url{https://www.youtube.com/watch?v=kyWyp-d0r94&ab_channel=TheConferenceonFairness%2CAccountability%2CandTransparency%28FAT%2a%29}
%https://www.linkedin.com/posts/ieaitum_ieai-2022-research-brief-ai-auditing-activity-6963460881514954752-snzA?utm_source=linkedin_share&utm_medium=android_app
% Intro to fairness and bias
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/DPI.drawio.pdf}
    \caption{Demographic Parity Inspector($g_\psi$) workflow. The training of models is shaded in red while the derivations of explanations  are shaded in blue. The  model $f_\theta$ is learned based on training data, $\{(x_i,y_i)\}$,  and outputs the explanations $\Ss(f_\theta,X)$. The \enquote{DP Inspector} receives the explanations to predict the protected attribute, $Z$. Then if the AUC is above $0.5$ then there is a demographic parity violation. We can interpret the reasons for demographic parity violations on $g_\psi$ with explainable AI techniques}
    \label{fig:workflow}
\end{figure}


% What is the problem?

Notions of fair ML are ultimately based on redistributive justice and political philosophy frameworks. Then, those notions are interpreted by researchers and practitioners until they reach technical formulation that we can quantify in ML systems, by doing this reification of the entity often the metric divergers to the original notion.

%~\cite{rawls1958justice,rawls1991justice,rawls2020theory,arneson1989equality,dworkin1981equality,dworkin1981equality2}

In this paper, we focus on the notion of \enquote{equality of treatment}, a key concept in political philosophy that is often translated to the ML literature as \textit{statistical parity} or \enquote{demographic parity}.

The notion of \enquote{equality of treatment} implies that all individuals should be treated equally regardless of their protected characteristics. We identify this  notion as if the behaviour of a model $f$ is independent of a protected attribute $Z$, formally $f \perp Z$. 


This notion has been translated to Fair ML as \enquote{demographic or statistical parity}, which is often measured in the literature as  if the distributions of the predictions of a model $f(X)$ are equal between protected groups $d(P(f(X)|Z)=d(P(f(X)|Z)$.


There exists a range of metrics (demographic parity, equal opportunity, average absolute odds \ldots) that judge the \emph{global} degree of unfairness arising from the use of machine learning models. Together with some predictive performance measures (accuracy, precision, AUC \ldots), these metrics are commonly used to deliver classifiers that achieve high degrees of predictive performance and fairness e.g.\ \cite{zafat2017fairnessDisparate,negligible2021}.
% Why is it interesting??
Unfortunately,  approaches that enforce low measures of unfairness at the global level may cause new kinds of unfairness \emph{at the subgroup level}. As \cite{ruggieri23}
shows, pushing for well-balanced fairness among groups at such a global scale may imply novel discrimination at
inter-group levels leading to new kinds of unfairness introduced by an erroneous use of fair-AI methods. 

% Why is it hard?
The Yule’s effect~\cite{yule1900vii,simpson1951interpretation,pearson1899vi} occurs when positive and negative associations between the model predictions $f(X)$ and the protected attribute $Z$ cancel out producing a vanishing correlation in the mixture of the distribution. It occurs whenever we aim at group fairness, such as independence $f(X)\perp Z$, but we wrongly disregard control for the protected attribute $Z$. Fair machine learning algorithms may result in disparate effects on separate distributions, with some subgroups impacted positively (higher fairness) and other subgroups impacted negatively (lower fairness) \cite{ruggieri23}.
For example, enforcing fairness for one discriminated ethnic group at the country level may imply novel kinds of unfairness for the subgroup of this ethnicity in a particular local state.


% Pitfall of previous work 
Previous methods of measuring demographic parity have relied on the predictions in the output space whose low dimensionality is prone to fall under Yule's effect, and on statistical measures on input data that is model-independent. Furthermore, on many occasions detecting and quantifying fairness violations is not enough, there is also the need to pinpoint  \textit{What are the specific sources of discrimination?}. 

% What do we do
We address this problem by providing a novel kind of demographic parity measurement that analyzes how different protected groups are treated at the distributional and instance level and how each feature contributes to the demographic parity violation. This approach relies on the explanation space, a novel data space that has theoretical guarantees while performing demographic parity inspections.  Our main contributions are:
\begin{itemize}
    \setlength\itemsep{0.1em}

    \item We use the explanation space to measure fairness by providing theoretical guarantees and experiments with synthetic data and real data use cases.% The explanation space is more sensitive and indicative than input or prediction data spaces.

    %\item We overcome the Yule–Simpson's effect of demographic parity measurements on the predictions space.\carlos{I have no clue}
    
    \item We introduce a \enquote{Demographic Parity Inspector} method that allows for interpretable fairness quantification that sheds insights on the root causes of unfairness.

    \item We release an open-source Python package \texttt{nobias}, which implements our \enquote{Demographic Parity Inspector} that is \texttt{scikit-learn} compatible~\cite{pedregosa2011scikit}, together with code and tutorials for reproducibility.
    
\end{itemize}

