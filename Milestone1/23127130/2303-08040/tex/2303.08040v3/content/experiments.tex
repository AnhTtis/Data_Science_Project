\section{Experimental Evaluation}
\label{sec:exp}

We perform measures of equal treatment by systematically varying the model $f$, its parameters $\theta$, and the input data distributions $\D_X$. We complement experiments described in this section by adding further experimental results in the Appendix that \emph{(i)} compare the different types of Shapley values estimation (Appendix~\ref{app:truemodeltruedata}), \emph{(ii)} add experiments on further natural datasets (Appendix~\ref{app:extra.experiments}), \emph{(iii)} exhibit a larger range of modeling choices (Appendix~\ref{subsec:EstimatorInspectorVariations}),\emph{(iv)} compare AUC vs accuracy for the C2ST independence test  (Appendix~\ref{app:stat.independence.exp}),\emph{(v)} extend the comparison against DP (Appendix~\ref{app:stat.DP.ET}) and \emph{(vi)} include LIME as an explanation method (Appendix~\ref{app:LIME}). 

%\textbf{Comparison methods and baselines.} 
We adopt \texttt{xgboost} \citep{DBLP:conf/kdd/ChenG16} for the model $f_\theta$, and logistic regression for the inspectors. We compare the AUC performances of several inspectors: $g_\psi$ (see Eq. \ref{eq:fairDetector}) for ET (see Def. \ref{def:et}), $g_v$  for DP (see Def. \ref{def:dp}), $g_\Upsilon$ for fairness of the input (i.e., $X \perp Z$ as discussed in Section \ref{subsec:ETvsEOvsInd}), and a combination  $g_\phi$ of the last two inspectors to test $f_\theta(X), X \perp Z$. These are the formal definitions:
%In Appendix~\ref{subsec:EstimatorInspectorVariations} we vary the types of models. 
%These experiments are grounded on previously discussed theory (Section ~\ref{sec:theory}):
\begin{gather}\nonumber
    \Upsilon = \argmin_{\tilde{\Upsilon}} \sum_{(x, z) \in \Dd{val}} \ell( g_{\tilde{\Upsilon}}(\textcolor{blue}{x}) , z) \quad \upsilon = \argmin_{\tilde{\upsilon}} \sum_{(x, z) \in \Dd{val}} \ell( g_{\tilde{\upsilon}}(\textcolor{blue}{f_\theta(x)}) , z )\\\nonumber
    \phi = \argmin_{\tilde{\phi}} \sum_{(x, z) \in \Dd{val}} \ell( g_{\tilde{\phi}}(\textcolor{blue}{{f_\theta(x),x}}) , z )
\end{gather}\label{eq:c2st.all.dist}
\vspace{-0.7cm}
\subsection{Experiments with Synthetic Data}\label{subsec:exp.synthetic}


%\textbf{Dataset.} 
We generate synthetic datasets by first drawing $10,000$ samples from normally distributed features $X_1 \sim N(0,1), X_2 \sim N(0,1), (X_3,X_4) \sim  N\left(\begin{bmatrix}0  \\ 0 \end{bmatrix},\begin{bmatrix}1 & \gamma \\ \gamma & 1 \end{bmatrix}\right)$. Then, we define a binary protected feature $Z$ with values $Z = 1$ if $X_4>0$ and $Z=0$ otherwise. We compare the methods and baselines while varying the correlation $\gamma = r(X_3, Z)$ from $0$ to $1$.
We define two experimental scenarios below. In both of them, the model $f_\beta$ is a function over the domain of the features $X_1, X_2, X_3$ only.

\textbf{Indirect Case: }\textit{Unfairness in the data and in the model.} We consider all of the three features in the dataset $X_1, X_2, X_3$. This gives rise to unfairness of the input  parameterized by $\gamma=r(X_3, Z)$. To generate DP violation in the model, we create the target $Y = \sigma(X_1 + X_2 + X_3)$, where $\sigma$ is the logistic function.

\textbf{Uninformative Case: }\textit{Unfairness in the data and fairness in the model.} The unfairness in the input data remains the same as in the previous case, while we now remove unfairness in the model.
The target feature is now defined as $Y = \sigma(X_1 + X_2)$. The $\gamma$ parameter controls unfairness in the dataset, which should not be captured by the model, since $X_1, X_2 \perp Z$ implies $Y \perp Z$ by propagation of independence.


\begin{figure*}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/fairAuditSyntheticCaseA.pdf}
\includegraphics[width=.49\textwidth]{images/fairAuditSyntheticCaseB.pdf}
\caption{In the \enquote{Indirect case} (left): good unfairness detection methods should follow a increasing steady slope to capture the fairness  violation; the DT inspector appears less sensitive due to the low dimensionality of its input. In the \enquote{Uninformative case} (right): good unfairness detection methods should remain constant with an AUC $\approx$ 0.5; the  inspectors based on input data ($g_\Upsilon$ and $g_\phi$) flag a false positive case of unfairness.}\label{fig:fairSyn}

\end{figure*}

In Figure \ref{fig:fairSyn}, we compare the AUC performances of the different inspectors on synthetic data split into $\nicefrac{1}{3}$ for training the model,  $\nicefrac{1}{3}$for training the inspectors and $\nicefrac{1}{3}$ for testing them. Overall, the ET inspector $g_{\psi}$ is able to detect unfairness in both scenarios. The DP inspector $g_v$ works fine in the indirect case, but it is not sensitive to  unfairness both in the data and in the model in the indirect case. Finally, the inspectors $g_\Upsilon$ and $g_\phi$ detect unfairness in the input but not in the model.
%
% We say that a method is \textit{Accountable} if the feature attributions identified are the ones that indeed contribute towards the synthetically generated discrimination for both methods, see Appendix~\ref{app:xai.eval} for the experiment.
Further experiments are shown in Appendix~\ref{app:xai.eval} to investigate the contribution of the explanation distribution features, namely the $\Ss(f_\theta, x)_i$'s, to the ET inspector $g_{\psi}$.

\vspace{-0.5cm}
\subsection{Use Case: ACS US Income Data}\label{sec:experiments}
\vspace{-0.3cm}
\begin{figure*}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/detector_auc_ACSIncome.pdf}
\includegraphics[width=.49\textwidth]{images/feature_importance_ACSIncome.pdf}
\caption{In the left figure, a comparison of ET and DP measures on the US Income data. The AUC range for ET is notably wider, and aligning with the theoretical section, there are indeed instances where DP fails to identify discrimination that ET successfully detects. For a detailed statistical analysis, please refer to Appendix~\ref{app:stat.DP.ET}. Right figure provides insight into the influential features contributing to unequal treatment. Higher feature values correspond to a greater likelihood of these features being the underlying causes of unequal treatment.}\label{fig:xaifolks}
\end{figure*}

We experiment here with the ACS Income dataset\footnote{ACS PUMS documentation: \url{https://www.census.gov/programs-surveys/acs/microdata/documentation.html}} \citep{DBLP:conf/nips/DingHMS21}, and in the Appendix~\ref{app:extra.experiments} with three other ACS datasets. The fairness notions are tested against all pairs of groups from the protected attribute \enquote{Race}. Figure~\ref{fig:xaifolks} (left) shows the AUC performances of the ET inspector $g_{\psi}$ and the DT inspector $g_v$. The standard deviation of the AUC is calculated over $30$ bootstrap runs, each one splitting the data into $\nicefrac{1}{3}$ for training the model, $\nicefrac{1}{3}$ for training the inspectors and $\nicefrac{1}{3}$ for testing them. In the Appendix~\ref{app:stat.independence.exp}, the results of the C2ST test of Section~\ref{sec:expSpaceIndependence} are reported. The AUCs for the EP inspectors are greater than for the DP inspectors, as expected due to Lemmma \ref{lemma:inc}.



Figure~\ref{fig:xaifolks} (right) shows the Wasserstein distance between the coefficients of the linear regressor $g_{\psi}$ compared to a baseline where groups are assigned at random in the input dataset. This feature importance post-hoc explanation method provides insights into the impact of different features as sources of unfairness. We observe \enquote{Education} as a highly discriminatory proxy while the role of the feature \enquote{Worked Hours Per Week} is less relevant. This allows us to identify areas where adjustments or interventions may be needed to move closer to the ideal of equal treatment.

%\todo[inline]{SR: Wasserstein distance = absolute distance, right? Moreover, in the Random baseline, all coefficients should be zero, shouldn't?}
%\todo[inline]{CM:Yes, to both. I use \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html}}

%\emph{Equal Treatment} is a very strict notion that is difficult to satisfy in practice, which aligns with political philosophy expectations. But by comparing relative pairwise differences and explaining the coefficients of $g_\psi$, we can gain insights into the impact of different factors and better understand the sources of inequality or disparities. We observe \enquote{Education} as a highly discriminatory proxy while the use of the feature \enquote{Worked Hours Per Week} is less discriminatory. This allows us to identify areas where adjustments or interventions may be needed to move closer to the ideal of equal treatment.

