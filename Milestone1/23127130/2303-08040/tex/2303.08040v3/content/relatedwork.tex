\vspace{-0.3cm}
\section{Foundations and Related work}\label{sec:relatedWork}
\vspace{-0.3cm}
This section briefly surveys the philosophical and technical foundations of our contribution as well as related work. We also build on Shapley values, which are generally known in the machine learning community, but for self-containedness 
we provide their mathematical definition in Appendix~\ref{app:foundation.xai}.
\vspace{-0.3cm}
\subsection{Basic Notations and Formal Definitions of Fairness in Related Work}
\vspace{-0.3cm}
In supervised learning, a function $f_\theta: X  \to Y$, also called a model, is induced from a set of observations, called the training set, $\Dd{} =\{(x_1,y_1),\ldots,(x_n,y_n)\} \sim X \times Y$, %=\{(x_1^{tr},y_1^{tr})\ldots, (x_n^{tr},y_n^{tr})\}$ which is a sample from $X \times Y$, 
where $X = \{X_1, \ldots, X_p\}$ represents predictive features and $Y$ is the target feature. %$f_\theta$ belongs to a family of functions ${\cal F}$ parametric in $\theta$.
The domain of the target feature is $\mathrm{dom}(Y)=\{0, 1\}$ (binary classification) or $\mathrm{dom}(Y) = \mathbb{R}$ (regression). For binary classification, we assume a probabilistic classifier, and we  denote by $f_\theta(x)$ the estimate of the probability $P(Y=1|X=x)$ over the (unknown)  distribution of $X \times Y$. For regression, $f_\theta(x)$ estimates $E[Y|X=x]$. We call the projection of $\D$ on $X$, written $\D_X{} = \{x_1, \ldots, x_n\} \sim X$,  the empirical \textit{input distribution}. 
The dataset $f_\theta({\D_X}) = \{f_\theta(x) \ |\ x \in \D_X{} \}$ is called the empirical \textit{prediction distribution}.

We assume a feature modeling protected social groups denoted by $Z$, called \textit{protected feature}, and assume it to be binary valued in the theoretical analysis. $Z$ can either be included in the predictive features $X$ used by a model or not. If not, we assume that it is still available for a test dataset. Even without the protected feature in training data, a model can discriminate against the protected groups by using correlated features as a proxy of the protected one~\citep{DBLP:conf/kdd/PedreschiRT08}.



%\steffen{reinstate if needed later}
%Let two datasets $\D, \D'$ define two empirical distributions $\PP(\D), \PP(\D')$. We write $\PP(\D) \nsim \PP(\D')$ to express that $\PP(\D)$ is sampled from a different underlying distribution than $\PP(\D')$ with high probability $p>1-\epsilon$
We write $A \bot B$
to denote statistical independence between the two sets of random variables $A$ and $B$, or equivalently, between two multivariate probability distributions. We define two common fairness notions and corresponding fairness metrics that quantify a model's degree of discrimination or unfairness \citep{DBLP:journals/csur/MehrabiMSLG21}. 

\begin{definition}\textit{(Demographic Parity (DP))}. A model $f_\theta$ achieves demographic parity if $f_\theta(X) \perp Z$\label{def:dp}.
\end{definition}

Thus, demographic parity holds if  $\forall z.\,P(f_\theta(X)|Z=z)=P(f_\theta(X))$. For binary $Z$'s, we can derive an unfairness metric as $d(P(f_\theta(X)|Z=1),P(f_\theta(X))$, where $d(\cdot)$ is a distance between probability distributions.

\begin{definition}\textit{(Equal Opportunity (EO))}\label{eq:TPR} A model $f_\theta$ achieves equal opportunity if $\forall z.\,P(f_\theta(X)|Y=1,Z=z) = P(f_\theta(X)=1|Y=1)$.\label{def:eo}
\end{definition}
As before, we can measure unfairness for binary $Z$'s as  $d(P(f_\theta(X)|Y=1,Z=1),P(f_\theta(X)=1|Y=1))$. Equal opportunity comes with the problem that labels for correct outcomes are required, but difficult or even impossible to collect \cite{DBLP:conf/aaai/Ruggieri0PST23}.

\vspace{-0.3cm}
\subsection{Philosophical Foundations and Computable Fairness Metrics}
\vspace{-0.3cm}
Political and moral philosophers from the \textbf{egalitarian} school of thought often consider \emph{equal opportunity} to be the key promoter of fairness and social justice,  providing qualified individuals with equal chances to succeed regardless of their background or circumstances ~\cite{rawls1958justice,rawls1991justice}, \cite{dworkin1981equality,dworkin1981equality2}, \cite{arneson1989equality}, \cite{cohen1989currency}. In fair ML, \cite{DBLP:conf/nips/HardtPNS16} proposed translating equal opportunity into the inter-group difference of the true positive rates. \citep{DBLP:conf/fat/HeidariLGK19} provided a moral framework to ground such a metric of equal opportunity. 
% \steffen{I do not give an example now because it becomes too tricky for equal treatment}
%To provide a simple example from the egalitarian point-of-view,  decisions to admit students to university become fairer by affirmative action considering applicants' full potential.

The \textbf{liberalism} school of thought argues that  individuals should be treated equally independently of outcomes~\cite{friedman1990free,nozick1974anarchy}. Equal treatment has also been defined as ``equal treatment-as-blindness" or neutrality~\cite{sunstein1992neutrality,miller1959myth}. From a technical perspective, the notion of \emph{equal treatment} has often been understood as \emph{equal outcomes} and translated to metrics such as demographic parity or statistical parity (used synonymously). 
As we will analyze in Section~\ref{sec:outcome-fairness}, equal outcomes imply that two demographic groups experience the same distribution of outcomes, even if the first of the two groups have much better prospects for achieving the predicted outcome. Thus, a model $f$ that achieves equal outcome may have to prefer individuals from one group over those from another group, violating the requirement for equal treatment of all individuals. Our metric for equal treatment remedy this drawback.
%Nonetheless, our approach differs from previous metrics in that we measure the \emph{equal treatment} of a model based on how each feature contributes to the prediction instead of relying on notions of outcomes based on aggregated model predictions. If \emph{equal treatment} is achieved, then \emph{equal outcome} too, but not vice-versa (see Lemma \ref{lemma:inc}). 

In Appendix~\ref{app:fair.notions}, we provide an illustrative use-case of when equal treatment is in general desired, but  neither equal opportunity nor equal outcomes can model this: scientific paper blind reviews.

\vspace{-0.3cm}
\subsection{Related Work}
\vspace{-0.3cm}
We briefly review related works below. See Appendix \ref{app:relatedWork} for an in-depth comparison of our work to existing research.


%\todo[inline]{Do we need to compare to other measure of DP? Is C2ST on the predictions novel too?}
\textbf{Measuring Demographic Parity}. Previous work has relied on the notion of equal outcomes and measured DP on the model predictions using statistics such as Mannâ€“Whitney, Kolmogorov-Smirnov or Wasserstein distance~\citep{DBLP:conf/fat/RajiSWMGHSTB20,DBLP:conf/icml/KearnsNRW18,DBLP:conf/nips/ChoHS20}.
Other research lines have aimed to measure DP when the protected attribute is a continuous variable~\citep{
DBLP:conf/iclr/JiangHFYMH22}. We measure DP by using a classifier two-sample test (see later on) of statistical independence.


\textbf{Classifier two-sample test (C2ST)}. We introduce a new classifier two-sample test (C2ST) to measure the independence of sets of random variables. While such a kind of approach has been previously explored by \cite{DBLP:conf/iclr/Lopez-PazO17}, the novelty of our proposal is to rely on AUC rather than accuracy, with the advantage of a direct application to the case of non-equal distribution of target labels -- more in   \ref{app:stat.independence.exp}.

\textbf{Explainability for fair supervised learning}.  \cite{lundberg2020explaining} apply Shapley values to statistical parity.
While there is a slight overlap with our work,  their extended abstract is not comparable to our paper wrt.\ objectives, breadth, and depth -- more in Appendix~\ref{app:relatedwork.Lundberg}. Other recent lines of work assume knowledge about causal relationships between random variables, such as~\cite {DBLP:conf/fat/GrabowiczPM22}. Our work does not rely on causal graphs knowledge but exploits the Shapley values' theoretical properties to obtain fairness model auditing guarantees. 


