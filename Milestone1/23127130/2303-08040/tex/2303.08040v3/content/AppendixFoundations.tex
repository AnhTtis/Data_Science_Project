\section{Definition and Properties of Shapley values}\label{app:foundation.xai}

Explainability has become an important concept in legal and ethical guidelines for data processing and machine learning applications ~\citep{selbst2018intuitive}. A wide variety of methods have been developed, aiming to account for the decision of algorithmic systems ~\citep{DBLP:journals/csur/GuidottiMRTGP19,DBLP:conf/fat/MittelstadtRW19,DBLP:journals/inffus/ArrietaRSBTBGGM20}. 
One of the most popular approaches to explainability in machine learning is Shapley values.

Shapley values are used to attribute relevance to features according to how the model relies on them ~\citep{DBLP:journals/natmi/LundbergECDPNKH20,DBLP:conf/nips/LundbergL17,DBLP:conf/ijcai/RozemberczkiWBY22}. Shapley values are a coalition game theory concept that aims to allocate the surplus generated by the grand coalition in a game to each of its players~\citep{shapley1997value}. 

For set of players $N = \{1, \ldots, p\}$, and a value function $\val:2^N \to \mathbb{R}$, the Shapley value $\mathcal{S}_j$ of the $j$'th player is defined as the average marginal contribution of player $j$ in all possibles coalitions of players:
\begin{small}
\[
\mathcal{S}_j = \sum_{T\subseteq N\setminus \{j\}} \frac{|T|!(p-|T|-1)!}{p!}(\val(T\cup \{j\}) - \val(T))
\]
\end{small}
%
In the context of machine learning models, players correspond to features $X_1, \ldots, X_p$, and the contribution of the feature $X_j$ is with reference to the prediction of a model $f$ for an instance $x^{\star}$ to be explained. Thus, we write $\mathcal{S}(f, x^{\star})_j$ for the Shapley value of feature $X_j$ in the prediction $f(x^{\star})$. We denote by $\mathcal{S}(f, x^{\star})$ the vector of Shapely values $(\mathcal{S}(f, x^{\star})_1, \ldots, \mathcal{S}(f, x^{\star})_p)$. 

There are two variants for the term $\val(T)$~\citep{DBLP:journals/ai/AasJL21,DBLP:journals/corr/abs-2006-16234,Zern2023Interventional}: the \textit{observational} and the \textit{interventional}. When using the observational conditional expectation, we consider the expected value of $f$ over the joint distribution of all features conditioned to fix features in $T$ to the values they have in $x^{\star}$:
\begin{equation}\label{def:val:obs}
\val(T) = E[f(x^{\star}_T, X_{N\setminus T})|X_T=x^{\star}_T]
\end{equation}
where $f(x^{\star}_T, X_{N\setminus T})$ denotes that features in $T$ are fixed to their values in $x^{\star}$, and features not in $T$ are random variables over the joint distribution of features.
Opposed, the interventional conditional expectation considers the expected value of $f$ over the marginal distribution of features not in $T$: % and with features in $T$ being fixed to the values they have in $x^{\star}$:
\begin{equation}\label{def:val:int}
\val(T) = E[f(x^{\star}_T, X_{N\setminus T})]
\end{equation}
In the interventional variant, the marginal distribution is unaffected by the knowledge that $X_T=x^{\star}_T$. In general, the estimation of (\ref{def:val:obs}) is difficult, and some implementations (e.g., SHAP) actually consider (\ref{def:val:int}) as the default one. In the case of decision tree models, TreeSHAP offers both possibilities.


The Shapley value framework is the only feature attribution method that satisfies the properties of efficiency, symmetry, uninformativeness and additivity~\citep{molnar2019,shapley1997value,winter2002shapley,aumann1974cooperative}.  We recall next the key properties of efficiency and uninformativeness:

\paragraph{Efficiency.} Feature contributions add up to the difference of prediction for $x^{\star}$ and the expected value of $f$:
\begin{gather}\label{eq:eff}
    \sum_{j \in N} \Ss(f, x^{\star})_j = f(x^{\star}) - E[f(X)])
\end{gather}

The following property only holds for the interventional variant (e.g., for SHAP values), but not for the observational variant.
\paragraph{Uninformativeness.}
A feature $X_j$ that does not change the predicted value (i.e., for all $x, x'_j$: $f(x_{N\setminus \{j\}}, x_j) = f(x_{N\setminus \{j\}}, x'_j)$) has a Shapley value of zero, i.e., $\Ss(f, x^{\star})_j = 0$.
 

In the case of a linear model $f_\beta(x) = \beta_0 + \sum_j \beta_j \cdot x_j$, the SHAP values turns out to be $\Ss(f, x^{\star})_i = \beta_i(x^{\star}_i-\mu_i)$ where $\mu_i = E[X_i]$. For the observational case, this holds only if the features are independent \citep{DBLP:journals/ai/AasJL21}.

\section{Detailed Related Work}\label{app:relatedWork}

This section provides an in-depth review of the related theoretical work that informs our research. We contextualize our contribution within the broader field of explainable AI and fairness auditing. We discuss the use of fairness measures such as demographic parity, as well as explainability techniques like Shapley values and counterfactual explanations.

\subsection{Fairness Notions: Paper Blind Reviews Use Case}\label{app:fair.notions}

%\steffen{explain with formulas}\carlos{I have cross-reference the eq. of the main body}\steffen{The weakness of this use case is that one may argue that the prior probability of a paper to succeed is the same for Germany and UK. I think it is easier to argue the case if the prior probability between different groups is different. This is the case for university admissions. Or for COMPAS dataset.}\steffen{there is the assumption in that notion that the two countries indeed have the same performance --- which may not be the case}\carlos{There is no assumption, we are not saying which one is better. We could change for country A and country B, or institution A and institution B}

To illustrate the difference between equal opportunity, equal outcomes, and equal treatment, based on the previously discussed framework, we consider the example of conference papers' blind reviews and focus on the protected attribute of the country of origin of the paper's author, comparing Germany and the United Kingdom.


For \emph{equal opportunity}, we quantify fairness by the true positive rate (cf. Definition~\ref{def:eo}). In words, it is the acceptance ratio given that the quality of the paper is high. Achieving equal opportunity will imply that these ratios are similar between the two countries. In blind reviews, the purpose is to evaluate the paper's quality and the research's merit without being influenced by factors such as the author's identity, affiliations, background or country. If we were to enforce equal opportunity in this use case, we would aim for similar true positive rates for submissions from different countries. However, this approach could lead to unintended consequences, such as unintentionally favouring, reverse discrimination, overcorrection or quotas of affirmative action towards certain countries. 


For \emph{equal outcomes}, we require that the distribution of acceptance rates is similar, independently of the quality of the paper (cf. Definition~\ref{def:dp}). Note that the outcomes can have similar rates due to random chance, even if there is a country bias in the acceptance procedure.

For \emph{equal treatment}, we require that the contributions of the features used to make a decision on paper's acceptance has similar distributions (cf. Definition~\ref{def:et}). Equality of treatment through blindness is more desirable than equal opportunity or equal outcomes because it ensures that all submissions are evaluated solely on the basis of their quality, without any bias or discrimination towards any particular country. By achieving equality of treatment through blindness, we can promote fairness and objectivity in the review process and ensure that all papers have an equal chance to be evaluated on their merits.


In comparing our introduced measure of \emph{equal treatment} with \emph{equal outcomes} (or demographic or statistical parity, used as synonymous), we note that the latter looks at the distributions of predictions and measures their similarity. Equal treatment goes a step further by evaluating whether the contribution of features to the decision, is similar. Our definition of \emph{equal treatment} implies the notion of \emph{equal outcome}, but the converse is not necessarily true, as we showed in Section \ref{sec:stat.independence}.


\subsection{Measuring Fairness}

%\enquote{Audits are tools for interrogating complex processes, often to determine whether they comply with company policy, industry standards or regulations}~\citep{DBLP:conf/fat/RajiSWMGHSTB20}. Algorithmic fairness audits are closely linked to audits studies as understood in the social sciences, with a strong emphasis on social justice~\citep{DBLP:conf/eaamo/VecchioneLB21}. A recent survey on public algorithmic audit identified four categories of \enquote{problematic machine behaviour} that can be unveiled by audit studies: discrimination, distortion, exploitation and misjudgement \citep{DBLP:journals/pacmhci/Bandy21,liu2012enterprise}. This taxonomy is highly helpful when putting forward auditing studies and also relevant for this work: our \enquote{Equal Treatment Inspector} can be seen as a tool to help understand discrimination in machine learning models, thus, falling into the first category.


Selecting a measure to compare fairness between two sensitive groups has been a highly discussed topic, where results such as~\citep{DBLP:journals/bigdata/Chouldechova17,DBLP:conf/nips/HardtPNS16,DBLP:conf/innovations/KleinbergMR17}, have highlighted the impossibility to satisfy simultaneously three type of fairness measures: demographic parity~\citep{DBLP:conf/innovations/DworkHPRZ12}, equalized odds~\citep{DBLP:conf/nips/HardtPNS16}, and predictive parity~\citep{DBLP:conf/kdd/Corbett-DaviesP17,DBLP:journals/corr/abs-2102-08453,wachter2020bias}. 


%Within the fairness mitigation context~\citep{DBLP:journals/corr/EdwardsS15} formulates an adversarial learning problem of the model and data to remove statistical parity from images. A limitation is that their work is limited to images and neural networks. Our work focuses on tabular data and leverages Shapley value theory to provide equal treatment auditing guarantees. 

Previous work has relied on measuring and calculating demographic parity on the model predictions~\citep{DBLP:conf/fat/RajiSWMGHSTB20,DBLP:conf/icml/KearnsNRW18}, or on the input data~\citep{DBLP:journals/cviu/FabbrizziPNK22,DBLP:conf/fat/YangQ0DR20,DBLP:conf/emnlp/ZhaoWYOC17}.  In this work, we perform equal treatment measures on the explanation distribution, which measures that each feature contributes equally to the prediction, which differs from the previous notions. 

In this work, we focus on  Equal Treatment (ET), as this fairness metric does not require a ground truth target variable, allowing for our method to work in its absence~\citep{DBLP:conf/aies/AkaBBGM21}, and under distribution shift conditions~\citep{DBLP:journals/corr/abs-2210-12369} where model performance metrics are not feasible to calculate~\citep{DBLP:conf/icml/GargBKL21,DBLP:conf/iclr/GargBLNS22,DBLP:conf/aaai/MouganN23}. Demographic Parity requires independence of the model's output from the protected features, written $f_\theta(X) \perp Z$, while Equal Treatment requires independence accross the feature attributions of the model $\Ss(f_\theta(X),X) \perp Z$.

\subsection{Explainability and fair supervised learning}\label{app:relatedwork.Lundberg}


%\subsection{Formal Comparison with Previous Work on Explaining Quantitative Fairness Measures via Shapley Values}

The intersection of fairness and explainable AI has been an active topic in recent years. The work most close to our approach is~\cite{lundberg2020explaining} where Shapley values are aimed at testing for demographic parity.
%Our work continues in-depth on this research line by formalizing the explanation distribution, deriving theoretical guarantees, and proposing novel methodology. 
%Specifically, our approach allows for comparison across different protected groups. We also introduce the concept of accountability, which refers to the ability of the algorithm to provide insights into the sources of \emph{equal treatment} violation of the model. Additionally, we evaluate our method on multiple datasets and synthetic examples, demonstrating its effectiveness in identifying and mitigating equal treatment disparities in machine learning models.
%In this subsection, we formally compare our approach to the prior work of Lundberg et al. \cite{lundberg2020explaining} and the related SHAP Python package documentation. Lundberg et al. addressed the concept of demographic parity (DP) using SHAP value estimation. 
This concise workshop paper emphasizes the importance of \enquote{decomposing a fairness metric among each of a modelâ€™s inputs to reveal which input features may be driving any observed fairness disparities}.
%
In terms of statistical independence, the approach can be rephrased as decomposing $f_\theta(X) \perp Z$ by examining $S(f_\theta,X)_i \perp Z$ for $i \in [1, p]$. 
Actually, the paper limits to consider difference in means, namely testing for $E[\Ss(f_\theta,X)_i|Z=1] \neq E[\Ss(f_\theta,X)_i|Z=0]$. Our approach goes beyond this, as we consider different distributions, and introduce the ET fairness notion for that. On the contrary, \cite{lundberg2020explaining} claims a decomposition method specific of DP. % Instead, it attempts to decompose the DP fairness of the model's output into the DP of its input components. In doing that, however, the
However, the decomposition method proposed is not sufficient nor necessary to prove DP, as showed next.

\begin{lemma}\label{lemma:lundberg}
$f_\theta(X) \perp Z$ is neither implied by nor it implies ($\Ss(f_\theta,X)_i \perp Z$ for $i \in [1, p]$).
\end{lemma}
\begin{proof}
Consider $f_\theta(X_1,X_2) = X_1 - X_2$ with $X_1,X_2 \sim \texttt{Ber}(0.5)$ and $Z=1$ if $X_1=X_2$, and $Z=0$ otherwise. Hence $Z\sim \texttt{Ber}(0.5)$. We have $\Ss(f_\theta,X_1) = X_1 \perp Z$ and $\Ss(f,X_2) = -X_2 \perp Z$. However, $f_\theta(X_1,X_2) = X_1-X_2$ does not satisfy $f_\theta(X_1,X_2) \perp Z$, e.g., $P(Z=0|f_\theta(X_1,X_2)=0) = P(Z=0|X_1-X_2=0) = 1$. Example \ref{ex42} illustrates a case where $f_\theta(X) \perp Z$ yet $\Ss(f_\theta, X)_1$ and $\Ss(f_\theta,X)_2$ are not independent of $Z$. 
\end{proof}

Our approach to ET considers the independence of the \textit{multivariate} distribution of $\Ss(f,X)$ with respect to $Z$, rather than the independence of each marginal distribution $\Ss(f_\theta, X)_i \perp Z$. With such a definition, we obtain a sufficient condition for DP, as shown in Lemma \ref{lemma:inc}.


~\cite{DBLP:conf/ssci/StevensDVV20} presents an approach based on adapting the Shapley value function to explain model unfairness. They also introduce a new meta-algorithm  that considers the problem of learning an additive perturbation to an existing model in order to impose fairness. 
In our work, we do not adopt the Shapley value function. Instead, we use the theoretical Shapley properties to provide fairness auditing guarantees. Our \enquote{Equal Treatment Inspector} is not perturbation-based but uses Shapley values to project the model to the explanation distribution, and then measures \emph{un-equal treatment}. It also allows us to pinpoint what are the specific features driving this violation.
 
 
\cite{DBLP:conf/fat/GrabowiczPM22} present a post-processing method based on Shapley values aiming to detect and nullify the influence of a protected attribute on the output of the system. For this, they assume there are direct causal links from the data to the protected attribute and that there are no measured confounders. Our work does not use causal graphs but exploits the theoretical properties of the Shapley values to obtain fairness model auditing guarantees.

A few works have researched fairness using other explainability techniques such as counterfactual explanations \citep{DBLP:conf/nips/KusnerLRS17,DBLP:conf/cogmi/ManerbaG21,DBLP:conf/aies/MutluYG22}.
We don't focus on counterfactual explanations but on feature attribution methods that allow us to measure unequal feature contribution to the prediction. Further work can be envisioned by applying explainable AI techniques to the \enquote{Equal Treatment Inspector} or constructing the explanation distribution out of other techniques.

\subsection{Classifier Two-Sample Test (C2ST)}
The use of classifiers as a statistical tests of independence $W \perp Z$ for a binary $Z$ has been previously explored in the literature \citep{DBLP:conf/iclr/Lopez-PazO17}. The approach relies on testing accuracy of a classifier trained to distinguish $Z=1$ (positives) from $Z=0$ (negatives) given $W=w$. 
In the null hypothesis that the distributions of positives and negatives are the same, no classifier is better than a random answer with accuracy $\nicefrac{1}{2}$. This assumes equal proportion of instances of the two distributions in the training and test set. 
Our approach builds on this idea, but it considers testing the AUC instead of the accuracy. Thus, we remove the assumption of equal proportions\footnote{For unequal proportions, one can consider the accuracy of the majority class, but this still make the requirement to know the true proportion of positives and negatives.}. We also show in Section \ref{app:stat.independence.exp} that using AUC may achieve a better power than using accuracy.

\cite{DBLP:conf/icml/LiuXL0GS20} propose a kernel-based approach to two-sample
tests classification. 
Alike work has also been used in Kaggle competitions under the name of \enquote{Adversarial Validation}~\citep{kaggleAdversarial,howtowinKaggle}, a technique which aims to detect which features are distinct between train and leaderboard datasets to avoid possible leaderboard shakes. 


\cite{DBLP:journals/corr/EdwardsS15} focuses on removing statistical parity from images by
using an adversary that tries to predict the relevant sensitive variable from the model representation and censoring the learning of the representation of the model and data on images and neural networks. While methods for images or text data are often developed specifically for neural networks and cannot be directly applied to traditional machine learning techniques, we focus on tabular data where techniques such as gradient boosting decision trees achieve state-of-the-art model performance \citep{DBLP:conf/nips/GrinsztajnOV22,DBLP:journals/corr/abs-2101-02118,BorisovNNtabular}. Furthermore, our model and data projection into the explanation distributions leverages Shapley value theory to provide fairness auditing guarantees. In this sense, our work can be viewed as an extension of their work, both in theoretical and practical applications.


