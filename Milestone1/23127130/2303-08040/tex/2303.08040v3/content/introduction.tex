\section{Introduction}

In philosophy, long-held discussions about what constitutes  a fair or an unfair political system have led to established frameworks of distributive justice~\citep{distributiveJustice,kymlicka2002contemporary}. 
From the \emph{egalitarian} school of thought, the  \emph{equal opportunity} concept was argued by~\cite{rawls1958justice}. The concept has been translated into computable metrics with the same name~\citep{DBLP:conf/nips/HardtPNS16}. From a machine learning perspective, the technical drawback is that  metrics for equal opportunity require label annotations for true positive outcomes, which are not always available after the deployment of a model.

The \emph{liberalism}  school of thought\footnote{We use the term \emph{liberalism} to refer to the perspective exemplified by~\cite{friedman1990free}. This perspective can also  be referred to as \emph{neoliberalism} or \emph{libertarianism}~\citep{distributiveJustice,wiki_friedman}. }, put forward  by scholars such as~\cite{friedman1990free} and~\cite{nozick1974anarchy}, requires \emph{equal treatment} of individuals regardless of their protected characteristics. 
This concept has been translated by the machine learning literature~\citep{DBLP:conf/aies/SimonsBW21,DBLP:conf/fat/HeidariLGK19,wachter2020bias} into the requirement that machine learning predictions should achieve \emph{equal outcomes} for groups with different protected characteristics. The corresponding measure,  \textit{demographic parity} (also called \emph{statistical  parity}), compares the different distributions of predicted outcomes  of a model $f$ for different social groups, e.g., protected vs dominant. We will show, however, that the metric of demographic parity may indicate fairness, although groups are indeed treated differently.
%\steffen{Which sentence do you refer to in your remark?}\carlos{I don't understand the --following-- sentence "demographic groups will likewise succeed if predicted to become successful"}. We will show that demographic parity measures equal treatment only if the assumption holds that different demographic groups will likewise succeed if predicted to become successful. In practice, this assumption may be easily violated. 

We leave the normative discussion of which philosophical paradigm should be pursued by policy to the discourse in the social sciences and the broad public. However, our analysis contributes to a foundational understanding of fairness in machine learning. Moreover, we remedy the divergence between the liberalism-oriented philosophical requirement of equal treatment and actual measures used in fair machine learning approaches by proposing a novel method for measuring {equal treatment}. 

Comparing different social groups, we measure how non-protected features of individuals interact with the trained model $f$ as explained by Shapley value attributions~\citep{DBLP:conf/nips/LundbergL17}. If two social groups are treated the same, the distributions of interaction behavior, which we call \emph{explanation distributions}, will not be distinguishable. We introduce a tool, the \enquote{Equal Treatment Inspector}, that implements this idea. When detecting unequal treatment, it explains the features involved in such an inequality, supporting understanding of the causes of unfairness in the machine learning model. In summary, our contributions~are:

\begin{enumerate}
    \setlength\itemsep{0.1em}

    \item The definition of {explanation distributions} as a basis for measuring {equal treatment}.

    \item The definition of a novel method for {recognizing and explaining unequal treatment}.
    
    \item The study of the formal relationship between {equal outcome} and {equal treatment}.
    
    \item A novel Classifier Two Sample Test (C2ST) based on the AUC.

    \item A study of synthetic and natural data to demonstrate our method and compare with related work.

    \item An open-source Python package \texttt{explanationspace} implementing the \enquote{Equal Treatment Inspector} that is \texttt{scikit-learn} compatible together with documentation and tutorials.
     
\end{enumerate}

