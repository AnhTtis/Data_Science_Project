\section{LIME as an Alternative to Shapley Values}\label{app:LIME}

The definition of ET (Def. \ref{def:et}) is parametric in the explanation function. We used Shapley values for their theoretical advantages (see Appendix~\ref{app:foundation.xai}).
Another widely used feature attribution technique is %that satisfies the aforementioned properties (efficiency and uninformative features Section~\ref{app:foundation.xai}) and can be used to create the explanation distributions is 
LIME (Local Interpretable Model-Agnostic Explanations). The intuition behind LIME is to create a local linear model that approximates the behavior of the original model in a small neighbourhood of the instance to explain~\citep{ribeiro2016why,ribeiro2016modelagnostic}, whose mathematical intuition is very similar to the Taylor/Maclaurin series. %In this work, we have proposed distributions of Shapley values to measure the philosophical liberal-political oriented notion of  Equal Treatment. 
This section discusses the differences in our approach when adopting LIME instead of the SHAP implementation of Shapley values. First of all, LIME has certain drawbacks:

\begin{itemize}
    \item \textbf{Computationally Expensive:} Its current implementation is more computationally expensive than current SHAP implementations such as TreeSHAP~\citep{DBLP:journals/natmi/LundbergECDPNKH20}, Data SHAP \citep{DBLP:conf/aistats/KwonRZ21,DBLP:conf/icml/GhorbaniZ19}, or Local and Connected SHAP~\citep{DBLP:conf/iclr/ChenSWJ19}. This problem is exacerbated when producing explanations for multiple instances (as in our case). In fact, LIME requires sampling data and fitting a linear model, which is a computationally more expensive approach than the aforementioned model-specific approaches to SHAP. A comparison of the execution  time is reported in the next sub-section.
    
    \item \textbf{Local Neighborhood:} The randomness in the calculation of local neighbourhoods can lead to instability of the LIME explanations. %Slight variations of this explanation hyperparameter can lead to different local explanations. 
    Works including \cite{DBLP:conf/aies/SlackHJSL20,alvarez2018towards,adebayo2018sanity} highlight that several types of feature attributions explanations, including LIME, can vary greatly.
    
    \item \textbf{Dimensionality:} LIME requires, as a hyperparameter, the number of features to use for the local linear model. For our method, all the features in the explanation distribution should be used. However, linear models suffer from the curse of dimensionality. In our experiments, this is not apparent, since our synthetic and real datasets are low-dimensional.  
    %This creates a dimensionality problem because, for our method to work, the explanation distributions have to be from the exact same dimensions as the input data. Reducing the number of features to be explained might improve the computational burden. Particularly on the experiments on synthetic data of this section it is not since the number of features is limited to three.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/fairAuditSyntheticCaseALIME.pdf}\hfill
\includegraphics[width=.49\textwidth]{images/fairAuditSyntheticCaseBLIME.pdf}
\caption{AUC of the ET inspect using SHAP vs using LIME.}
\label{fig:lime}
\end{figure}

Figure \ref{fig:lime} compares the AUC of the ET inspector using SHAP and LIME as explanation functions over the synthetic dataset of Section \ref{subsec:exp.synthetic}. In both scenarios (indirect case and uninformative case), the two approaches have similar results. In both cases, however, the stability of using SHAP is better than using LIME.

%The left plot shows the sensitivity under the Indirect Discrimination Case, using the synthetic data experimental setup of Section~\ref{subsec:exp.synthetic} and figure Figure~\ref{fig:fairSyn} of the main body of the paper, both distributions capture this equal treatment violation; LIME is less sensitive than SHAP. The right plot shows the sensitivity to detect uninformative discrimination. The performance for measuring equal treatment of the model  is similar between the two types of distributions, but LIME suffers from two drawbacks: its theoretical properties rely on the definition of a local neighborhood, which can lead to unstable explanations (false positives or false negatives on explanation shift detection), and its computational runtime required is much higher than that of SHAP (see experiments below).

\subsection{Runtime}

We conduct an analysis of the runtimes of generating the explanation distributions using TreeShap vs LIME. %The experiments were run on a server with 4 vCPUs and 32 GB of RAM. 
We adopt\texttt{shap} version $0.41.0$ and \texttt{lime} version $0.2.0.1$ as software packages. In order to define the local neighborhood for both methods in this example, we used all the data provided as background data. The model $f_{\theta}$ is set to \texttt{xgboost}. As data we produce a randon generated data matrix, of varying dimensions. When varying the number of samples, we use 5 features, and when varying the number of features, we use$1000$ samples.
%
Figure \ref{fig:computational} shows the elapsed  time for generating explanation distributions with varying numbers of samples and columns. 

The runtime required to generate explanation distributions using LIME is considerably greater than using SHAP. %This is due to the fact that LIME requires training a local model for each instance of the input data to be explained, which can be computationally expensive. In contrast, SHAP relies on heuristic approximations to estimate the feature attribution with no need to train a model for each instance. 
The difference becomes more pronounced as the number of samples and features increases.
%
%We note that the computational burden of generating the explanation distributions can be further reduced by limiting the number of features to be explained, as this reduces the dimensionality of the explanation distributions. However, this will inhibit the quality of the explanation shift detection as it won't be able to detect changes in the distribution shift that impact the model on those features.
%
%Given the current state-of-the-art of software packages we have used, SHAP values due to lower runtime required and that theoretical guarantees hold with the implementations. 
%Further work can be envisioned on developing novel methods and software to select the best approach between SHAP, LIME and possibly other approaches for a dataset at hand.



\begin{figure}[ht]
\centering
\includegraphics[width=.49\textwidth]{images/computational_samples.pdf}\hfill
\includegraphics[width=.49\textwidth]{images/computational_features.pdf}
\caption{Elapsed time for generating explanation distributions using SHAP and LIME with different numbers of samples (left) and features (right) on synthetically generated datasets. Note that the y-scale is logarithmic.} %The experiments were run on a server with 4 vCPUs and 32 GB of RAM. The runtime required to create explanation distributions with LIME is far greater than SHAP for a gradient-boosting decision tree.}
\label{fig:computational}
\end{figure}
