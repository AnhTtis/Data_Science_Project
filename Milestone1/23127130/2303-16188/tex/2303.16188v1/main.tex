\documentclass{article}

%\usepackage[nonatbib]{neurips_2022}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow, hhline}
\usepackage[table]{xcolor}
\usepackage[para,online,flushleft]{threeparttable}

%arxiv version
\usepackage[letterpaper,margin=1.0in]{geometry}
\usepackage{graphicx}


%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{bbding}
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{enumitem}
%\usepackage{multirow}
\usepackage[numbers,sort]{natbib}
\usepackage{color}
\input{math.tex}


%\input{math.tex}
\usepackage{tcolorbox}
\usepackage{pifont}
\definecolor{mydarkgreen}{RGB}{39,130,67}
\definecolor{mydarkred}{RGB}{192,25,25}
\definecolor{bgcolor}{rgb}{0.93,0.99,1}
%\definecolor{bgcolor}{rgb}{0.8,1,1}
\definecolor{bgcolor2}{rgb}{0.8,1,0.8}
\definecolor{bgcolor3}{rgb}{0.50,0.90,0.50}


\newcommand{\green}{\color{mydarkgreen}}
\newcommand{\red}{\color{mydarkred}}
\newcommand{\cmark}{\green\ding{51}}%
\newcommand{\xmark}{\red\ding{55}}%
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cO}{\mathcal O}
\newcommand{\cC}{\mathcal C}


\usepackage{relsize} %SEB: feel free to revert, I think the font looked a bit too big otherwise
\newcommand{\algname}[1]{{\sf\green\relscale{0.90}#1}\xspace}
\newcommand{\algnameS}[1]{{\sf\green\relscale{0.90}#1}\xspace}
\newcommand{\dataname}[1]{{\tt\color{blue}#1}\xspace}
\title{Symmetric Rank-$k$ Methods}
\author{
    Chengchang Liu\thanks{The Chinese Unversity of Hong Kong; 7liuchengchang@gmail.com} \qquad \qquad Cheng Chen\thanks{East China Normal University; jackchen1990@gmail.com} \qquad \qquad Luo Luo\thanks{Fudan University; luoluo@fudan.edu.cn}
 }
 \date{}

\begin{document}
\maketitle

 \begin{abstract}
This paper proposes a novel class of block quasi-Newton methods for convex optimization which we call symmetric rank-$k$ (SR-$k$) methods.
Each iteration of SR-$k$ incorporates the curvature information with~$k$ Hessian-vector products achieved from the greedy or random strategy.
We prove SR-$k$ methods have the local superlinear convergence rate of $\OM\big((1-k/d)^{t(t-1)/2}\big)$ for minimizing smooth and strongly self-concordant function, where $d$ is the problem dimension and $t$ is the iteration counter.
This is the first explicit superlinear convergence rate for block quasi-Newton methods and it successfully explains why block quasi-Newton methods converge faster than standard quasi-Newton methods in practice.
 \end{abstract}

 
\section{Introduction}
We study the quasi-Newton methods for solving the minimization problem
\begin{align}\label{prob:main}
    \min_{\x\in\RB^d} f(\x),
\end{align}
where $f:\BR^d\to\BR$ is smooth and strongly self-concordant. 
Quasi-Newton methods~\cite{broyden1970convergence2,broyden1970convergence,shanno1970conditioning,broyden1967quasi,davidon1991variable,byrd1987global,yuan1991modified} are widely recognized for their fast convergence rates and efficient updates, which attracts growing attention in many fields such as statistics~\cite{jamshidian1997acceleration, zhang2011quasi,bishwal2007parameter},  economics~\cite{ludwig2007gauss,li2013dynamic} and machine learning~\cite{goldfarb2020practical, hennig2013quasi,liu2022quasinewton,liu2022partial,lee2018distributed}.
Unlike standard Newton methods which need to compute the Hessian and its inverse, quasi-Newton methods go along the descent direction by the following scheme
\begin{align*}
    \x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t),
\end{align*}
where $\G_t\in\BR^{d\times d}$ is an estimator of the Hessian  $\nabla^2 f(\x_t)$.
The most popular ways to construct the Hessian estimator are the Broyden family updates, including the Davidon--Fletcher--Powell (DFP) method~\cite{davidon1991variable,fletcher1963rapidly}, the Broyden--Fletcher--Goldfarb--Shanno (BFGS) method~\cite{broyden1970convergence2,broyden1970convergence,shanno1970conditioning}, and the symmetric rank 1 (SR1) method~\cite{broyden1967quasi,davidon1991variable}. 

The classical quasi-Newton methods with Broyden family update \cite{broyden1970convergence2,broyden1970convergence} find the Hessian estimator $\mG_{t+1}$ for the next round by the secant equation 
\begin{align}\label{eq:sec}
\G_{t+1}(\x_{t+1}-\x_t)=\nabla f(\x_{t+1})-\nabla f(\x_t).    
\end{align}
These methods have been proven to exhibit local superlinear convergence in 1970s~\cite{powell1971on,Dennis1974A,broyden1973on},
and their non-asymptotic superlinear rates were established in recent years~\cite{rodomanov2021rates,rodomanov2021new,ye2022towards,jin2022non}.
For example, \citet{rodomanov2021rates} showed classical BFGS method enjoys the local superlinear rates of $\OM\big((d\varkappa/t)^t/2\big)$ which has been later improved to $\OM\big((\exp (d\ln(\varkappa)/t)-1)^{t/2}\big)$~\cite{rodomanov2021new}, and \citet{ye2022towards} showed classical SR1 method converges with local rate of $\OM\big((d\ln(\varkappa)/t)^{t/2}\big)$, where $\varkappa$ is the condition number of the objective.


Some recent works \cite{gower2017randomized,rodomanov2021greedy} proposed new types of quasi-Newton methods, which construct the Hessian estimator by the following equation
\begin{align}
\label{eq:grracondi}
    \G_{t+1}\u_t=\nabla^2 f(\x_{t+1})\u_t,
\end{align}
where $\u_t\in\RB^{d}$ is chosen by greedy or random strategies.
% \begin{align*}
%     \color{red} \text{write down the expressions}.
% \end{align*}
\citet{rodomanov2021greedy} established the local superlinear rate of $\OM\big((1-{1}/{(\varkappa d)})^{t(t-1)/2}\big)$ for greedy quasi-Newton methods with Broyden family updates.
Later, \citet{lin2021greedy} provided the condition-number free superlinear rate of~$\OM\big((1-{1}/{d})^{t(t-1)/2}\big)$ for greedy and random quasi-Newton methods with specific BFGS and SR1 updates.

Block quasi-Newton methods construct the Hessian estimator along multiple directions at per iteration. 
The study of these methods dates back to 1980s. \citet{schnabel1983quasi} proposed first block BFGS method by extending equation~(\ref{eq:sec}) to multiple secant equations
\begin{align*}
    \G_{t+1}(\x_{t+1}-\x_{t+1-j})=\nabla f(\x_{t+1})-\nabla f(\x_{t+1-j})
\end{align*}
for $j=1,\cdots,k$.
Although block quasi-Newton methods usually have better empirical performance than classical ones~\cite{schnabel1983quasi,o1994linear, gao2018block,gower2016stochastic,gower2017randomized,kovalev2020fast}, 
their theoretical guarantees are mystery until \citet{gao2018block}  proved block BFGS method has asymptotic local superlinear convergence.
On the other hand, \citet{gower2016stochastic,gower2017randomized,kovalev2020fast} introduced the randomized block BFGS by generalizing condition~\eqref{eq:grracondi} to
\begin{align*}
    \G_{t+1}\U_t=\nabla^2 f(\x_{t+1})\U_t,
\end{align*}
where $\U_t\in\RB^{d\times k}$ is some random matrix. 
The empirical studies show randomized block BFGS performs well on real-world applications.
\citet{kovalev2020fast} showed randomized block BFGS method also has asymptotic local superlinear convergence, but its advantage over vanilla BFGS methods is still unclear in theory. 

The known results cannot explain why block quasi-Newton methods enjoy faster convergence behavior than vanilla quasi-Newton methods in practice. 
This naturally leads to the following question:
\begin{center}
\textit{Can we design a block quasi-Newton method with explicit superior convergence rate?}    
\end{center}
In this paper, we give an affirmative answer to above question by proposing symmetric rank-$k$ (SR-$k$) methods.
%for $k\in[d-1]$ and the quadratic rate of Newton methods for $k=d$. 
The construction of Hessian estimators in \srk~methods are based on generalizing the idea of symmetric rank 1 (SR1)~\cite{broyden1967quasi,davidon1991variable,ye2022towards} methods and the equation of the form~\eqref{eq:grracondi}. 
We provide the random and greedy strategies to determine $\mU_t$ for SR-$k$.
Both of these strategies lead to the explicit local superlinear convergence rate of $\OM\big((1-{k}/{d})^{t(t-1)/2}\big)$, where $k$ is the number of directions used to approximate Hessian at per iteration.
For~$k=1$, our convergence rate reduces to the one of greedy and random SR1 methods~\cite{lin2021greedy}.
For~$k\geq 2$, it is clear that the convergence rate of SR-$k$ methods is better than existing greedy and random quasi-Newton methods~~\cite{lin2021greedy,rodomanov2021greedy}.
We also follow the design of SR-$k$ to propose a variant of randomized block BFGS method~\cite{gower2016stochastic,gower2017randomized,kovalev2020fast}, resulting an explicit superlinear convergence of $\OM\big((1-k/(\varkappa d))^{t(t-1)}\big)$.
We compare proposed methods with existing quasi-Newton methods for minimizing strongly convex function in Table~\ref{tbl:main-2}. 

The remainder of this paper is organized as follows.
%\paragraph{Paper Organization}
In Section~\ref{sec:pre}, we introduce the notation and the preliminaries throughout this paper. 
In Section~\ref{sec:update}, we introduce the SR-$k$ update in the view of matrix approximation. 
In Section~\ref{sec:algorithm}, we propose the quasi-Newton methods with SR-$k$ updates for solving the strongly self-concordant function and provide their the superior local superlinear convergence rates. 
In Section~\ref{sec:Block BFGS}, we propose a variant of randomized block BFGS method with explicit local superlinear convergence rate.
In Section~\ref{sec:exp}, we conduct numerical experiments to show the outperformance of proposed methods.
Finally, we conclude our work in Section~\ref{sec:conclusion}.

\begin{table*}[!t]
	\centering
	\caption{We summarize the properties of quasi-Newton methods for convex optimization}\label{tbl:main-2}
\begin{threeparttable}
\setlength\tabcolsep{5.pt}
\vskip0.1cm
\begin{tabular}{c c c}
\toprule[.1em]
\begin{tabular}{c} 
    \bf Method  
\end{tabular} &
\bf Rank & 
\begin{tabular}{c}  $\displaystyle{\EB\left[{\lambda_{t+1}}/{\lambda_t}\right]}$  \end{tabular} \\
\toprule[.1em]

\begin{tabular}{c} 
    Newton\\ \cite{nesterov2018lectures,nocedal1999numerical} 
\end{tabular} 
& $d$ 
& $\displaystyle{\OM(\lambda_t)}$ 
\\  
\midrule 

\begin{tabular}{c}  Classical Quasi-Newton \\      
\cite{jin2022non,rodomanov2021rates,rodomanov2021new,ye2022towards} 
\end{tabular}
& $1$ or $2$ 
& $\displaystyle{\OM\left(1/t\right)}$  
\\  
\midrule 
           
\begin{tabular}{c}
    Greedy/Randomized Broyden Family \\ \cite{rodomanov2021greedy,lin2021greedy}
\end{tabular} 
& ~~$1$, $2$ or $3$~~ 
& ~~$\displaystyle{\OM\big((1-1/(\varkappa d))^{t}\big)}$~~  
\\  
\midrule 

\begin{tabular}{c}
    Greedy/Randomized BFGS \\ \cite{lin2021greedy}
\end{tabular}
& $1$ or $2$ 
& $\displaystyle{\OM\big((1-1/d)^{t}\big)}$  
\\
\midrule 

\begin{tabular}{c}
    Greedy/Randomized SR1 \\ \cite{lin2021greedy}
\end{tabular}
& $1$ or $2$ 
& $\displaystyle{\OM\big((1-1/d)^{t}\big)}$  
\\
\midrule 


\begin{tabular}{c}
    Multi-Secant Block-BFGS \\ 
    \cite{gao2018block}
\end{tabular} 
& $k\in[d]$ 
& implicit 
\\  
\midrule 

\begin{tabular}{c}
    Randomized Block-BFGS (v1) \\ 
    \cite{kovalev2020fast,gower2017randomized}
\end{tabular} 
& $k\in[d]$ 
& implicit 
\\  
\midrule 

\begin{tabular}{c}
    Randomized Block-BFGS (v2)  \\ 
    Algorithm~\ref{alg:bfgs} 
\end{tabular}
& $k\in[d]$  
& $\displaystyle{\OM\big((1-{k}/{(\varkappa d)})^{t}\big)}$  
\\
\midrule

\begin{tabular}{c}
    SR-$k$ \\ 
    Algorithm~\ref{alg:SRK}
\end{tabular} 
& $k\in[d]$  
& $\displaystyle{\begin{cases}\OM\big((1-{k}/{d})^t\big), & k\in[d-1] \\[0.15cm]
\OM\big(\lambda_t\big), & k=d 
\end{cases}}$  
\\[0.25cm]
\hline
\end{tabular} % default value: 6pt
\end{threeparttable}	
\end{table*}  


\section{Preliminaries}
\label{sec:pre}
We use $\{\e_1,\cdots,\e_d\}$ to present the the standard basis in space $\RB^d$ and let $\I_d\in\RB^{d\times d}$ be the identity matrix. 
We denote the trace of a square matrix by $\tr{\cdot}$. 
We use $\|\cdot\|$ to present the spectral norm and Euclidean norm of matrix and vector respectively. Given a positive definite matrix $\A\in\BR^{d\times d}$, we denote the corresponding weighted norm as~$\|\vx\|_\mA\triangleq(\vx^{\top}\mA\vx)^{1/2}$ for some $\vx\in\BR^d$. 
We use the notation $\Norm{\vx}_\vz$ to present~$\|\vx\|_{\nabla^2 f(\vz)}$ for positive definite Hessian $\nabla^2 f(\vz)$, if there is no ambiguity for the reference function~$f(\cdot)$.
We also define~
\begin{align}
    \label{eq:EA}
    \mE_{k}(\mA)\triangleq[\ve_{i_1};\cdots;\ve_{i_k}]\in\BR^{d\times k},
\end{align}
where $i_1,\dots,i_k$ are the indices for the largest $k$ entries in the diagonal of $\mA$.

% When 
% The expectation $\EB_{\U}[\,\cdot\,]$ or $\EB[\,\cdot\,]$ consider the randomness of the random matrix $\U$ for one iteration or all the randomness during the updates, we can view it with no randomness for the same notation when applied to deterministic updates.

Throughout this paper, we suppose the objective in problem (\ref{prob:main}) satisfies the following assumptions.
\begin{assumption}
\label{ass:smooth}
We assume the objective function $f:\BR^d\to\BR$ is $L$-smooth, i.e., there exists some constant~$L\geq 0$ such that $\norm{\vx-\vy}\leq L\norm{\vx-\vy}$ for any $\vx,\vy\in\BR^d.$
%$\nabla^2 f(\vx) \preceq L \I_d$.
\end{assumption}

\begin{assumption}
\label{ass:strongconvex}
We assume the objective function $f:\BR^d\to\BR$ is $\mu$-strongly-convex, i.e., there exists some constant~$\mu>0$ such that 
\begin{align*}
f(\lambda\vx+(1-\lambda)\vy)\leq \lambda f(\vx) + (1-\lambda)f(\vy)-\frac{\lambda(1-\lambda)\mu}{2}\norm{\vx-\vy}^2    
\end{align*}
for any $\vx,\vy\in\BR^d$ and $\lambda\in[0,1]$.
\end{assumption}

We define the condition number as~$\varkappa \triangleq L/\mu$. The following proposition shows the objective function has bounded Hessian under 
 Assumption \ref{ass:smooth} and~\ref{ass:strongconvex}.
\begin{proposition}
Suppose the objective function $f:\BR^d\to\BR$ satisfies Assumptions~\ref{ass:smooth} and \ref{ass:strongconvex}, then it holds 
\begin{align}
\mu\mI_d \preceq \nabla^2 f(\x) \preceq L\mI_d
\end{align}
for any $\x\in\BR^d$.
\end{proposition}


We also impose the assumption of strongly self-concordance~\cite{rodomanov2021greedy,lin2021greedy} as follows.
\begin{assumption}
\label{ass:strongself}
We assume the objective function $f:\RB^d\to\RB$ is $M$-strongly self-concordant, i.e., there exists some constant $M> 0$ such that
\begin{align}
    \nabla^2 f(\y)-\nabla^2 f(\x)\preceq M\|\y-\x\|_\z\nabla^2 f(\w), 
\end{align}
for any $\x,\y,\w,\z\in\RB^d$.
\end{assumption}

The strongly-convex function with Lipschitz-continuous Hessian is strongly self-concordant.

\begin{proposition}
Suppose the objective function $f:\BR^d\to\BR$ satisfies Assumptions \ref{ass:strongconvex} and its Hessian is $L_2$-Lipschitz continuous, i.e., we have
$\| \nabla^2 f(\x)-\nabla^2 f(\y)\|\leq L_2\|\x-\y\|$,
for all $\x$, $\y\in\RB^d$, then $f$ satisfies is $M$-strongly self-concordant with $M={L_2}/{\mu^{3/2}}$.
\end{proposition}



\section{Symmetric Rank-$k$ Updates}
\label{sec:update}
We propose the symmetric rank-$k$ (SR-$k$) update as follows.

\begin{definition}[SR-$k$ Update]
Let $\mA\in\BR^{d\times d}$ and $\mG\in\BR^{d\times d}$ be two positive-definite matrices with~$\A\preceq \G$. 
For any full rank matrix $\U\in\RB^{d\times k}$ with $k\leq d$, we define
$\srk(\G,\A,\U)\triangleq\G$ if $\G\U=\A\U$. Otherwise, we define
\begin{align}
    \srk(\G,\A,\U) \triangleq \G-(\G-\A)\U(\U^{\top}(\G-\A)\U)^{-1}\U^{\top}(\G-\A).
\end{align}
\end{definition}
 
\noindent We provide two strategies to select $\mU\in\BR^{d\times k}$ for \srk~update:
\begin{enumerate}
\item For randomized strategy, we sample each entry of $\mU$ according to $\fN(0,1)$ independently.
\item For greedy strategy, we construct
$\U=\mE_{k}(\mG-\mA)$, where $\E_k(\cdot)$ follows the notation of \eqref{eq:EA}. %[\u^{(i_1)};\cdots;\u^{(i_k)}],
%where $i_1,\dots,i_k$ are the indices for the largest $k$ entries in the diagonal of $\mG-\mA$ and $\u^{(i_j)}$ is the $i_j$-th column of $\mU$.
\end{enumerate}

For $k=1$, SR-$k$ updates with above two strategies reduce to randomized or greedy SR1 updates~\cite{rodomanov2021rates,lin2021greedy}.
The remain of this section shows the multiple directions in $\mU\in\BR^{d\times k}$ provably make \srk~update has the advantage over SR1 update in the view of estimating the target matrix $\mA$. 

First, we provide the following lemma to show the output $\mG$ of \srk~update does not increase the deviation from $\A$, which is similar to ordinary Broyden family updates~\cite{rodomanov2021greedy}.
\begin{lemma}
\label{lm:sr1good}
For any positive-definite matrices $\mA\in\BR^{d\times d}$ and $\mG\in\BR^{d\times d}$ with $\A\preceq\G\preceq\eta \A$ for some~$\eta\geq 1$, we let $\G_{+}={\text{\rm SR-$k$}}(\G,\A,\U)$ for some full rank matrix $\U\in\RB^{d\times k}$. %  with $k\leq d$ neccesary?
Then it holds that
\begin{align}
\label{eq:srk_good}
   \A\preceq\G_{+}\preceq \eta\A. 
\end{align}
\end{lemma}



  Then we introduce the quantity~\cite{lin2021greedy,ye2022towards}
 \begin{align}
 \label{eq:measure_srk}
     \tau_\A(\G)\triangleq\tr{\G-\A}
 \end{align}
to characterize the difference between $\mA$ and $\mG$.
We can prove \srk~updates with randomized or greedy strategies enjoy explicit faster convergence rate than SR1~updates for estimating $\mA$.

\begin{theorem}\label{thm:matrix}
Let
\begin{align}
\label{eq:block-update}
 \G_{+}={\text{\rm SR-$k$}}(\G,\A,\U)
\end{align}
with $\G\succeq\A\in\RB^{d\times d}$ and select $\U\in\RB^{d\times k}$ by one of the following strategies:
\begin{enumerate}
\item Sample each entry of $\mU$ according to $\fN(0,1)$ independently.
\item Construct $\U=\E_k(\mG-\mA)$.
\end{enumerate}

Then, we have
\begin{align}\label{ieq:srk-matrix}
    \EB\left[\tau_{\A}(\G_{+})\right]\leq \left(1-\frac{k}{d}\right)\tau_\A(\G).
\end{align}
\end{theorem}

The term $(1-k/d)$ in inequality (\ref{ieq:srk-matrix}) reveals the advantage of block-type update in \srk~methods, since the larger $k$ leads to the faster decay of $\tau_{\A}(\G_{+})$.
As a comparison, the results of randomized or greedy SR1 updates~\cite{lin2021greedy} match the special case of Theorem \ref{thm:matrix} with $k=1$.











\section{Minimization of Strongly Self-Concordant Function}
\label{sec:algorithm}
\begin{algorithm}[t]
\caption{Symmetric Rank-$k$ Method}\label{alg:SRK}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\G_0$, $M$ and $k$. \\[0.15cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.15cm]
\STATE \quad $\x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t)$ \\[0.15cm]
\STATE \quad $r_t=\|\x_{t+1}-\x_{t}\|_{\x_t}$ \\[0.15cm]
\STATE \quad  $\tilde{\G}_{t}=(1+Mr_t)\G_t$ \\[0.15cm]
\STATE \quad Construct $\U_t\in\RB^{d\times k}$ by  \\[0.05cm]
\quad\quad  (a) randomized strategy: $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim}{\fN(0,1)}$ \\[0.15cm]
\quad\quad  (b) greedy strategy: $\U_t=\E_k(\tilde{\G}_t-\nabla^2 f(\x_{t+1}))$ \\[0.15cm]
\STATE \quad $\G_{t+1}= \srk(\tilde{\G}_t,\nabla^2f(\x_{t+1}),\U_t)$ \\[0.15cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}

We propose \srk~methods for minimizing strongly self-concordant function in Algorithm~\ref{alg:SRK}, where $M>0$ follows the notation in 
Assumption~\ref{ass:strongself}.
Then we provide the convergence analysis for \srk~methods and show its superiority to existing quasi-Newton methods. 

 

The convergence of \srk~methods (Algorithm~\ref{alg:SRK}) is measured by the local gradient norm~\cite{rodomanov2021greedy}
\begin{align}
\label{eq:convergemeasure}
    \lambda(\x)\triangleq \sqrt{\nabla f(\x)^{\top}(\nabla^2f(\x))^{-1}\nabla f(\x)}.
\end{align}

The theoretical analysis starts from the following result for quasi-Newton iterations.

\begin{lemma}[Lemma 4.3 of \citet{rodomanov2021greedy}]
\label{lm:linear-quadra}
Suppose that the twice differentiable function $f:\BR^d\to\BR$ is strongly self-concordant with constant $M>0$ and the positive definite matrix $\G_t\in\BR^{d\times d}$ satisfies
\begin{align}
\label{eq:Gbound}
    \nabla^2 f(\x_t)\preceq \G_t\preceq \eta_t \nabla^2 f(\x_t)
\end{align}
for some $\eta_t\geq 1$.
Then the update formula 
\begin{align}\label{eq:iterbyG}
\x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t)    
\end{align}
holds that
\begin{align}
\label{eq:linear-quadra}
  \|\x_{t+1}-\x_t\|_{\x_t}\leq \lambda(\x_t)~~~\text{and}~~~  \lambda(\x_{t+1})\leq \left(1-\frac{1}{\eta_t}\right)\lambda(\x_t) + \frac{M}{2}(\lambda(\x_t))^2+\frac{M^2}{4\eta_t}(\lambda (\x_t))^3.
\end{align}
\end{lemma}

Applying Lemma \ref{lm:linear-quadra} with fixed $\eta_t = 3\eta_0/2$ and Lemma~\ref{lm:sr1good}, we can establish the linear convergence rate of \srk~methods.

 \begin{theorem}
 \label{thm:srklinear}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:SRK} with initial $\vx_0$ and $\mG_0$ such that
\begin{align*}
    \lambda(\vx_0)\leq \frac{\ln(3/2)}{4\eta_0M}
\qquad\text{and}\qquad
    \nabla^2 f(\x_0)\preceq \G_0\preceq \eta_0 \nabla^2f(\x_0)
\end{align*} 
for some $\eta_0\geq 1$. Then it holds that
\begin{align}
\label{eq:srklinear}
  \nabla^2 f(\x_{t})\preceq \G_t\preceq \frac{3\eta_0}{2}\nabla^2 f(\x_t)\qquad\text{and}\qquad\lambda(\x_t)\leq \left(1-\frac{1}{2\eta_0}\right)^t\lambda(\x_0).
\end{align}
\end{theorem}


Note that the choice of $\eta_t$ in inequality (\ref{eq:linear-quadra}) is very important for guarantee the convergence rate of the quasi-Newton method.
Specifically, we can obtain the superlinear rate for iteration \eqref{eq:iterbyG} if there exists some $\eta_t\geq 1$ that  converges to 1.
For example, the randomized and greedy SR1 methods~\cite{lin2021greedy} corresponds to some $\eta_t$ such that
\begin{align*}
    \EB[\eta_t-1]\leq  \OM \bigg(\bigg(1-\frac{1}{d}\bigg)^{t}\,\bigg).
\end{align*}

As the results shown in Theorem \ref{thm:matrix}, the proposed \srk~updates have the superiority in matrix approximation. 
So it is natural to construct some $\eta_t\geq 1$ for \srk~methods (Algorithm \ref{alg:SRK}) such that
\begin{align*}
    \EB[\eta_t-1]\leq  \OM \bigg(\bigg(1-\frac{k}{d}\bigg)^{t}\,\bigg).
\end{align*}
Based on above intuition, we derive the local superlinear convergence rate for \srk~methods, which is explicitly sharper than existing randomized and greedy quasi-Newton methods~\cite{lin2021greedy,rodomanov2021greedy}.
% for the SR-$k$ method by incorporating the local linear-quadratic rate of the $\lambda(\x_t)$ and the linear convergence rate of matrix approximation provided in section~\ref{sec:update}.
\begin{theorem}
\label{thm:srk}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, if we run Algorithm~\ref{alg:SRK} with $k<d$ and set the initial $\vx_0$ and $\mG_0$ such that
\begin{align}
\label{eq:initial}
    \lambda(\x_0)\leq \frac{\ln 2}{2} \cdot \frac{(d-k)}{ M\eta_0 d^2\varkappa}\qquad\text{and}\qquad\nabla^2 f(\x_0)\preceq\G_0\preceq \eta_0\nabla^2f(\x_0)
\end{align} 
for some $\eta_0\geq 1$. 
Then we have
\begin{align}
\label{eq:E_lambda_srk}
     \EBP{\frac{\lambda(\x_{t+1})}{\lambda(\x_t)}}\leq 2d\varkappa\eta_0\left(1-\frac{k}{d}\right)^{t},
\end{align}
which naturally indicates the following two stage convergence:
\begin{itemize}
\item For \srk~method with randomized update, we have
    \begin{align*}
    \lambda(\x_{t_0+t})\leq\left(1-\frac{k}{d+k}\right)^{t(t-1)/2}\cdot\left(\frac{1}{2}\right)^t\cdot\left(1-\frac{1}{2\eta_0}\right)^{t_0}\lambda(\x_0),
\end{align*}
with probability at least $1-\delta$ for some $\delta\in(0,1)$, where $t_0=\OM(d\ln(\eta_0\varkappa d/\delta)/k)$.
\item For \srk~method with greedy update, we have
    \begin{align*}
    \lambda(\x_{t_0+t})\leq\left(1-\frac{k}{d}\right)^{t(t-1)/2}\cdot\left(\frac{1}{2}\right)^t\cdot\left(1-\frac{1}{2\eta_0}\right)^{t_0}\lambda(\x_0),
\end{align*}
where $t_0=\OM(d\ln(\eta_0\varkappa d)/k)$.
\end{itemize}
\end{theorem}
Additionally, \srk~methods with $k=d$ have the local quadratic convergence rate.




\begin{corollary}
\label{cor:recoverNewton}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:SRK} with $k=d$ and set the initial $\vx_0$ and $\mG_0$ such that
\begin{align}
\label{eq:k=dinitial}
\lambda(\vx_0)\leq \frac{\ln (3/2)}{4M\eta_0} 
~~~\text{and}~~~\nabla^2 f(\x_0)\preceq\G_0\preceq \eta_0\nabla^2f(\x_0).
\end{align}
Then we have
\begin{align}
  \EBP{\lambda(\x_{t+1})}\leq M(\lambda(\x_t))^2
\end{align}
for any  $t\geq 1$. 
\end{corollary}






\section{Improved Results for Block BFGS}

\begin{figure}[tp]
	\vspace*{-\baselineskip}
\begin{minipage}[t]{.49\textwidth} 
\begin{algorithm}[H]
\caption{Randomized Block BFGS Method (v1)}\label{alg:bfgs-v1}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\G_0$ and $k$. \\[0.20cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.20cm]
\STATE \quad
$\x_+=\x_t-\G_t^{-1}\nabla f(\x_t)$ \\[0.20cm]
\STATE \quad $\x_{t+1}=\argmin_{\vx\in\{\vx_t,\vx_+\}}f(\x)$\\[0.20cm]
\STATE \quad Construct $\U_t$ by $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim} {\fN(0,1)}$ \\[0.20cm]
\STATE \quad 
$\G_{t+1}= \bfgs(\G_t,\nabla^2f(\x_{t}),\U_t)$\\[0.20cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}
\end{minipage}
~~
\begin{minipage}[t]{.49\textwidth}
\begin{algorithm}[H]
\caption{Randomized Block BFGS Method (v2)}\label{alg:bfgs}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\G_0$, $M$ and $k$. \\[0.12cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.12cm]
\STATE \quad $\x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t)$ \\[0.12cm]
\STATE \quad $r_t=\|\x_{t+1}-\x_{t}\|_{\x_t}$ \\[0.12cm]
\STATE \quad   $\tilde{\G}_{t}=(1+Mr_t)\G_t$\\[0.11cm]
\STATE \quad Construct $\U_t$ by $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim} {\fN(0,1)}$ \\[0.11cm]
\STATE \quad 
$\G_{t+1}= \bfgs(\tilde{\G}_t,\nabla^2f(\x_{t+1}),\U_t)$\\[0.11cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}

 \end{minipage}
\end{figure}

\label{sec:Block BFGS}

In this section, we present the non-asymptotic superlinear convergence rate of randomized block BFGS method \cite{gower2016stochastic,gower2017randomized}  by following the idea of \srk.

The block BFGS update~\cite{schnabel1983quasi,gower2017randomized,gower2016stochastic} is defined as follows.
\begin{definition}
%[\citet{schnabel1983quasi,gower2016stochastic,gower2017randomized}]
Let $\A\in\RB^{d\times d}$ and $\G\in \RB^{d\times d}$ be two positive-definite symmetric matrices with $\A\preceq \G$. For any full rank matrix $\U\in\RB^{d\times k}$ with $k\leq d$, we define~$\bfgs(\G,\A,\U)\triangleq\G$ if $\G\U=\A\U$. Otherwise, we define
\begin{align}\label{update:RaBFGS}
    \bfgs(\G,\A,\U) \triangleq \G-\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G+\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\A.
\end{align}
\end{definition}

\noindent \citet{gower2016stochastic,kovalev2020fast} proposed randomized block BFGS method (Algorithm~\ref{alg:bfgs-v1}) by constructing the Hessian estimator with formula (\ref{update:RaBFGS}) and showed it has asymptotic local superlinear convergence rate.

For achieving the explicit superlinear convergence rate, we require providing some properties of randomized BFGS update, which is similar to the counterpart of \srk~updates.
First, we observe that randomized block BFGS update also has non-increasing deviation from the target matrix.
\begin{lemma}
\label{lm:bfgsnofar}
For any positive-definite matrices $\mA\in\BR^{d\times d}$ and $\mG\in\BR^{d\times d}$ with $\A\preceq\G\preceq\eta \A$ for some~$\eta\geq 1$, we let $\G_{+}={ \text{\rm BlockBFGS}}(\G,\A,\U)$ for some full rank matrix $\U\in\RB^{d\times k}$. Then, it holds that
\begin{align}
\label{eq:blockneq}
   \A\preceq\G_{+}\preceq \eta\A. 
\end{align}
\end{lemma}
\noindent Then we introduce the quantity~\cite{rodomanov2021greedy}
\begin{align}
\label{eq:measurebfgs}
    \sigma_{\A}(\G)\triangleq\tr{\A^{-1}(\G-\A)},
\end{align}
to measure the difference of two positive definite matrices. 
We show that randomized block BFGS update converges to the target matrix with a faster rate than the ordinary randomized BFGS update~\cite{rodomanov2021greedy,lin2021greedy}.


\begin{theorem}
\label{thm:bfgs}
Consider the block BFGS update
\begin{align}
\label{eq:bfgsupdate}
    \G_{+}={\text{\rm BlockBFGS}}(\G,\A,\U),
\end{align}
where $\G\succeq\A\in\RB^{d\times d}$. If $\mu\I_d\preceq\A\preceq L\I_d$ and  $\U\in\RB^{d\times k}$ is selected by sample each entry of $\mU$ according to $\fN(0,1)$ independently. Then, we have
\begin{align}
\label{eq:bfgssigma}
  \EB\left[\sigma_{\A}(\G_{+})\right]\leq \left(1-\frac{k}{d\varkappa}\right)\sigma_\A(\G).
\end{align}
\end{theorem}

We proposed a variant randomized block BFGS method in Algorithm~\ref{alg:bfgs}.
Based on the observation in Theorem \ref{thm:bfgs}, we establish its explicit superlinear convergence rate as follows.
\begin{theorem}
\label{thm:BFGS}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:bfgs} and set the initial $\vx_0$ and $\mG_0$ such that
\begin{align}
\label{eq:bfgsini}
 \lambda(\x_0)\leq \frac{\ln 2}{4}\cdot \frac{1}{M \eta_0 d} 
\qquad\text{and}\qquad
\nabla^2 f(\x_0)\preceq\G_0\preceq \eta_0\nabla^2f(\x_0),
\end{align}
for some $\eta_0\geq 1$. 
Then we have
\begin{align*}
    \EBP{\frac{\lambda(\x_{t+1})}{\lambda(\x_t)}}\leq 2d\eta_0\left(1-\frac{k}{d\varkappa}\right)^{t}.
\end{align*}
\end{theorem}

\begin{remark}
For $k=1$, Theorem~\ref{thm:bfgs} and \ref{thm:BFGS} match the results of ordinary randomized BFGS methods~\cite{lin2021greedy}. 
\end{remark}


\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[scale=0.27]{img/a9a.res_5.pdf} &
\includegraphics[scale=0.27]{img/w8a.res_5.pdf} &
\includegraphics[scale=0.27]{img/made.res_5.pdf}
\\[-0.1cm]
\small (a) a9a (iteration) & \small  (b) w8a (iteration)  &\small (c) madelon (iteration) \\[0.2cm]
\includegraphics[scale=0.27]{img/a9a.time_5.pdf} & 
\includegraphics[scale=0.27]{img/w8a.time_5.pdf} &
\includegraphics[scale=0.27]{img/made.time_5.pdf}
\\[-0.1cm]
\small  (d) a9a (time) & \small  (e) w8a (time)  &\small (f) madelon (time)
\end{tabular}\vskip-0.15cm
\caption{We demonstrate ``\#iteration vs. $\|\nabla f(\x)\|_2$'' and ``running time (s) vs. $\|\nabla f(\x)\|_2$'' on datasets ``a9a'', ``w8a'' and ``madelon'', where we take $k=5$ for all of the block quasi-Newton methods.}\label{fig:experiment-5}\vskip0.3cm
\end{figure}

\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[scale=0.27]{img/a9a.res.pdf} &
\includegraphics[scale=0.27]{img/w8a.res.pdf} &
\includegraphics[scale=0.27]{img/made.res.pdf}
\\[-0.1cm]
\small (a) a9a (iteration) & \small  (b) w8a (iteration) &\small (c) madelon (iteration) \\[0.2cm]
\includegraphics[scale=0.27]{img/a9a.time.pdf} & 
\includegraphics[scale=0.27]{img/w8a.time.pdf} &
\includegraphics[scale=0.27]{img/made.time.pdf}
\\[-0.1cm]
\small  (d) a9a (time) & \small  (e) w8a (time) &\small (f) madelon (time)
\end{tabular}\vskip-0.15cm
\caption{We demonstrate ``\#iteration vs. $\|\nabla f(\x)\|_2$'' and ``running time (s) vs. $\|\nabla f(\x)\|_2$'' on datasets ``a9a'', ``w8a'' and ``madelon'', where we take $k=10$ for all of the block quasi-Newton methods.}\label{fig:experiment-10}\vskip0.3cm
\end{figure}




\section{Numerical Experiments}\label{sec:exp}

We conduct the experiments on the model of regularized logistic regression, which can be formulated as 
\begin{align}\label{prob:logstic}
    \min_{\vx\in\BR^d} f(\x)\triangleq \frac{1}{n}\sum_{i=1}^{n}\ln(1+\exp(-b_i\va_i^{\top}\x))+\frac{\gamma}{2}\|\x\|^2,
\end{align}
where $\{\va_i,b_i\}_{i=1}^{n}$ are the training set with $\va_i\in\RB^d$, $b_i\in\{-1,+1\}$ and $\gamma>0$ is the regularization hyperparameter.

We refer to \srk~methods (Algorithm~\ref{alg:SRK}) with randomized and greedy strategies as Ra\srk~and Gr\srk~respectively.
The corresponding SR1 methods with randomized and greedy strategies are referred as RaSR1 and GrSR1~\cite[Algorithm 4]{lin2021greedy} respectively.
We also refer to randomized block BFGS (Algorithm~\ref{alg:bfgs-v1} \cite{gower2016stochastic,gower2017randomized}) and its variant (Algorithm~\ref{alg:bfgs}) as BlockBFGSv1 and BlockBFGSv2. 
We compared the proposed Ra\srk, Gr\srk~and BlockBFGSv2 with baseline methods on problem~(\ref{prob:logstic}).
For all methods, We set the parameters $\G_0$ and $M$ from $\{\I_d, 10\cdot\I_d, 10^2\cdot\I_d, 10^3\cdot\I_d, 10^4\cdot\I_d\}$ and $\{2,20,200,2000\}$ respectively.
We evaluate the performance for all of methods on four real-world datasets ``a9a'', ``w8a'' and ``madelon''. 
We conduct our experiments on a PC with Apple M1 and the implement all algorithms in Python 3.8.12.

We present the results of ``iteration numbers vs. gradient norm'' and ``running time (second) vs. gradient norm'' in Figure \ref{fig:experiment-5} and Figure \ref{fig:experiment-10}, which corresponds to the settings of $k=5$ and $10$ for block quasi-Newton methods Ra\srk, Gr\srk, BlockBFGSv1 and BlockBFGSv2.
We observe that the proposed \srk~methods (Ra\srk~and Gr\srk) always significantly outperform baselines.








\section{Conclusion}
\label{sec:conclusion}
In this paper, we have proposed symmetric rank-$k$ (\srk) methods for convex optimization.
We have proved \srk~methods enjoy the explicit local superlinear convergence rate of $\OM\left((1-k/d)^{t(t-1)/2}\right)$.
Our result successfully reveals the advantage of block-type updates in quasi-Newton methods, building a bridge between the theories of ordinary quasi-Newton methods and standard Newton method.
As a byproduct, we also provide the convergence rate of $\OM\left((1-k/(\varkappa d))^{t(t-1)/2}\right)$ for randomized block BFGS method.
In future work, it would be interesting to establish the global convergence of \srk~methods and study the convergence for limited memory block quasi-Newton methods.

 

\bibliographystyle{plainnat}
\bibliography{ref}
 
\appendix



\section{The Proofs in Section~\ref{sec:update}}

We provide the proofs for the properties of \srk~updates shown in Section~\ref{sec:update}.
We focus on the case of~$\mG\mU\neq\mA\mU$, since the results are obvious for  $\mG\mU=\mA\mU$.

\subsection{The Proof of Lemma~\ref{lm:sr1good}}
\begin{proof}
Define $\R=\G-\A\succeq\0$.
According to the update rule, we have
\begin{align*}
    &\G_{+}-\A\\
    =& \R-\R\U(\U^{\top}\R\U)^{-1}\U^{\top}\R\\
    =&\left(\I_d-\R\U(\U^{\top}\R\U)^{-1}\U^{\top}\right)\R\left(\I_d-\U^{\top}(\U\R\U)^{-1}\U^{\top}\R\right)\\
    =&\left(\I_d-\R\U(\U^{\top}\R\U)^{-1}\U^{\top}\right)\R\left(\I_d-\U(\U^{\top}\R\U)^{-1}\U^{\top}\R\right)\succeq\0,
\end{align*}
which means
\begin{align*}
    \G_{+}\succeq\A.
\end{align*}
The condition $\mG\preceq\eta\mA$ means
\begin{align*}
    \G_{+}&\preceq \eta\A - \underbrace{\R\U(\U^{\top}\R\U)^{-1}\U^{\top}\R}_{\succeq\0}\preceq \eta\A,
\end{align*}
which finish the proof.
\end{proof}





\subsection{The Proof of Theorem~\ref{thm:matrix}}
We first provide several lemmas for random matrix and the trace of positive definite matrix.

\begin{lemma}
\label{lm:EP}
Let $\U\in\RB^{d\times k}$ be a random matrix and each of its entry is independent and identically distributed according to $\fN(0,1)$, then it holds that
\begin{align}
\label{eq:EP}    \EB\left[\U(\U^{\top}\U)^{-1}\U^{\top}\right] = \frac{k}{d}\I_d.
\end{align}
\end{lemma}
\begin{proof}
We use $\fV_{d,k}$ to present the Stiefel manifold which is the set of all $d\times k$ column orthogonal matrices.
We denote $\fP_{k,d-k}$ as the set of all $m\times m$ orthogonal projection matrices idempotent of rank $k$.

According to Theorem 2.2.1 (iii) of \citet{chikuse2003statistics}, the random matrix
\begin{align*}
    \Z=\U(\U^{\top}\U)^{-1/2}
\end{align*}
is uniformly distributed on the Stiefel manifold $\fV_{d,k}$.
Applying Theorem 2.2.2 (iii) of \citet{chikuse2003statistics}, the random matrix
\begin{align*}
    \P=\Z\Z^\top=\U(\U^{\top}\U)^{-1}\U^{\top} 
\end{align*}
is uniformly distributed on $\fP_{k,d-k}$. 
Combining above results with Theorem 2.2.2 (i) of \citet{chikuse2003statistics} on $\mP$ achieves
\begin{align*}
    \EB[\P]= \frac{k}{d}\I_d.
\end{align*}
\end{proof}
\begin{remark}
The above proof requires the knowledge for statistics on manifold. 
For the readers who are not familiar with this, we also present a elementary proof of Lemma~\ref{lm:EP} by induction in Appendix~\ref{appen:addiproof}.
\end{remark}


\begin{lemma}
For positive semi-definite matrix $\S\in\RB^{d\times d}$ and the column orthonormal matrix $\Q\in\RB^{d\times k}$, we have
\begin{align}
    \label{eq:trace}
    \tr{\Q^{\top}\S\Q}\leq \tr{\S}.
\end{align}
\end{lemma}
\begin{proof}
Since matrix $\Q$ is column orthonormal, we have
\begin{align*}
    \Q\Q^{\top}=\Q(\Q^{\top}\Q)^{-1}\Q^{\top}\preceq\I_d.
\end{align*}
According to Lemma~\ref{lm:trmulti}, we have
\begin{align*}
      \tr{\Q^{\top}\S\Q}=\tr{\S\Q\Q^{\top}}\leq \tr{\S}.
\end{align*}
\end{proof}
\begin{lemma}
\label{lm:pdneq2}
For positive semi-definite matrix $\B\in\RB^{d\times d}$ and full rank matrix $\U\in\RB^{d\times k}$ with $d\geq k$, it holds that
\begin{align}
\label{eq:keyeq}
  \tr{\B\U\left(\U^{\top}\B\U\right)^{-1}\U^{\top}\B} \geq \tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\B}.
\end{align}
\end{lemma}
\begin{proof}
We denote SVD of $\U$ as $\U=\Q\mSigma\V^{\top}$, where $\Q\in\RB^{d\times k}$, $\V\in\RB^{k\times k}$ are (column) orthogonal and~$\mSigma\in\RB^{k\times k}$ is diagonal. 
We have
\begin{align*}
    \tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\B}& = \tr{\Q\mSigma\V^{\top}\left(\V\mSigma^2\V^{\top}\right)^{-1}\V\mSigma\Q^{\top}\B}  =\tr{\Q\Q^{\top}\B},
\end{align*}
and
\begin{align*}
    \tr{\B\U\left(\U^{\top}\B\U\right)^{-1}\U^{\top}\B} 
    &= \tr{\B\Q\mSigma\V^{\top}\left(\V\mSigma\Q^{\top}\B\Q\mSigma\V^{\top}\right)^{-1}\V\mSigma\Q^{\top}\B}\\
    &=\tr{\B\Q\left(\Q^{\top}\B\Q\right)^{-1}\Q^{\top}\B}\\
    &\!\!\overset{\eqref{eq:trace}}{\geq} \tr{\Q^{\top}\B\Q\left(\Q^{\top}\B\Q\right)^{-1}\Q^{\top}\B\Q}\\
    &=\tr {\Q^{\top}\B\Q},
\end{align*}
which leads to inequality \eqref{eq:keyeq}.
\end{proof}

Now, we present the proof of Theorem~\ref{thm:matrix}.
\begin{proof}
{\bf Randomized SR-$k$ Update:}
Combining Lemma~\ref{lm:EP} with Lemma~\ref{lm:pdneq2}, we have
\begin{align*}
    \EB \left[\tr{(\U^{\top}\B\U)^{-1}(\U^{\top}\B^2\U)}\right] &\overset{\eqref{eq:keyeq}}{\geq} \EB \left[\tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\B}\right]\\
    &=\tr{\EB\left[\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\right]\B}\\
    &=\frac{k}{d}\tr{\B},
\end{align*}
for any positive semi-definite $\B\in\BR^{d\times d}$.
The update rule of SR-$k$ update leads to
\begin{align*}
    \G_{+}-\A&= \G-\A - (\G-\A)\U(\U^{\top}(\G-\A)\U)^{-1}\U^{\top}(\G-\A)\\
            &=\B-\B\U(\U^{\top}\B\U)^{-1}\U^{\top}\B,
\end{align*}
where we set $\B=\G-\A\succeq \0$. Then, we have
\begin{align*}
    \EB \left[\tr{\G_+-\A} \right]&= \EB \left[\tr{\B-\B\U(\U^{\top}\B\U)^{-1}\U^{\top}\B}\right]\\
    &=\tr{\B}-\EB\left[\tr{\B\U(\U^{\top}\B\U)^{-1}\U^{\top}\B}\right]\\
    &=\tr{\B}- \EB\left[\tr{(\U^{\top}\B\U)^{-1}(\U^{\top}\B^2\U)}\right]\\
    &\leq\tr{\B}-\frac{k}{d} \tr{\B}\\
    &=\left(1-\frac{k}{d}\right)\tr{\B}.
\end{align*}
\end{proof}

{\bf Greedy \srk~Update:} 
Given positive semi-definite matrix $\B\in\BR^{d \times d}$, we use $\{b_i\}_{i=1}^{k}$ to denote its diagonal entries such that
\begin{align*}
     b_1\geq b_2 \geq \cdots \geq b_d,
\end{align*}
which implies
\begin{align}
\label{eq:bigeq}
    \sum_{i=1}^d b_i =\tr{\B}\qquad{\text{and}}\qquad \sum_{i=1}^{k}b_i\geq\frac{k}{d}\tr{\B}.
\end{align}
Then we have
\begin{align}  
\label{eq:grleq}
\begin{split}
\tr{(\U^{\top}\U)^{-1}\U^{\top}\B\U}&=\tr{\I_k\U^{\top}\B\U}=\tr{\U^{\top}\B\U}\\
    &=
    \sum_{p=1}^{k} \u_{p}^{\top}\B \u_{p}\\
   & =b_1+b_2+\cdots+ b_k\\
   &\overset{\eqref{eq:bigeq}}{\geq} \frac{k}{d} \sum_{i=1}^{d}b_{i} = \frac{k}{d}\tr{\B},
  \end{split}
\end{align}
where $\vu_p$ is the $p$-th column of $\mU$.
Setting $\B=\G-\A$ and applying Lemma~\ref{lm:pdneq2}, we have
\begin{align*}
    \tr{\G_{+}-\A}&=\tr{\B}-\tr{(\U^{\top}\B\U)^{-1}(\U\B^2\U)}\\
    &\overset{\eqref{eq:keyeq}}{\leq} \tr{\B}-\tr{(\U^{\top}\U)^{-1}\U^{\top}\B\U}\\
    &\overset{\eqref{eq:grleq}}{\leq} \left(1-\frac{k}{d}\right)\tr{\B} \\
    & \leq \tr{\mG-\mA}.
\end{align*}

\subsection{An Elementary Proof of Lemma~\ref{lm:EP}}
\label{appen:addiproof}
We first provide the following lemma for multivariate normal distribution.
\begin{lemma} \label{lem:1d_gauss}
Assume $\mP\in\BR^{d\times k}$ is column orthonormal $(k\le d)$ and  $\vp\sim\mathcal{N}(\vzero,\mP\mP^{\top})$ is a $d$-dimensional multivariate normal distributed vector. Then we have 
$$\BE\left[\frac{\vp\vp^\top}{\vp^\top \vp}\right]=\frac{1}{k}\mP\mP^\top.$$
\end{lemma}

\begin{proof}
The distribution $\vp\sim\mathcal{N}(\vzero,\mP\mP^{\top})$ implies there exists a $k$-dimensional multivariate normal distributed vector~$\vp_1\sim\mathcal{N}(\vzero,\mI_k)$ such that $\vp=\mP\vp_1$. Thus we have
\begin{align*}
\BE\left[\frac{\vp\vp^\top}{\vp^\top \vp}\right] &= \BE\left[\frac{(\mP\vp_1)(\mP\vp_1)^\top}{(\mP\vp_1)^\top (\mP\vp_1)}\right]\\
&= \BE\left[\frac{\mP\vp_1\vp_1^\top\mP^\top}{\vp_1^\top\vp_1}\right]\\
&= \mP\BE\left[\frac{\vp_1\vp_1^\top}{\vp_1^\top\vp_1}\right]\mP^\top \\
& = \frac{1}{k}\mP\mP^\top.
\end{align*}
\end{proof}

Then we provide an elementary proof of 
 Lemma~\ref{lm:EP}.
\begin{proof}
We prove inequality (\ref{eq:EP}) by induction on $k$.
The induction base $k=1$ is easily verified. 
Now we assume 
\begin{align*}
\BE\left[\U(\U^\top\U)^{-1}\U^\top\right]=\frac{k}{d}\mI_d 
\end{align*}
holds for any $\mU\in\BR^{d\times k}$ that each of its entries are independently distributed according to $\fN(0,1)$. 
We define the random matrix 
\begin{align*}
\bar{\U} = \begin{bmatrix}
\U & \vq 
\end{bmatrix}\in\BR^{d\times(k+1)},
\end{align*}
where $\vq\sim\fN(\vzero,\mI_d)$ is independent distributed to $\mU$.
Then we have
\begin{align*}
    \bar{\U}(\bar{\U}^\top\bar{\U})\bar{\U}^\top &=\begin{bmatrix}\U & \vq \end{bmatrix}\left(\begin{bmatrix}\U^\top \\ \vq^\top \end{bmatrix} \begin{bmatrix}\U & \vq \end{bmatrix} \right)^{-1} \begin{bmatrix}\U^\top \\ \vq^\top \end{bmatrix}
    = \mA+\frac{(\mI_d-\mA)\vq\vq^\top(\mI_d-\mA)}{\vq^\top(\mI_d-\mA)\vq},
\end{align*}
where $\mA=\U(\U^\top\U)^{-1}\U^\top$.
Since the rank of projection matrix $\mI_d-\mA$ is $d-k$, we have $\mI_d-\mA=\Q\Q^\top$ for some column orthonormal matrix $\Q\in\BR^{d\times (d-k)}$. 
Thus, we achieve
\begin{align*}
\BE[\bar{\U}(\bar{\U}^\top\bar{\U})\bar{\U}^\top] &=\frac{k}{d}\mI_d +\BE_{\U}\left[\BE_{\vq}\left[\frac{(\mI_d-\mA)\vq\vq^\top(\mI_d-\mA)}{\vq^\top(\mI_d-\mA)\vq}\,\Bigg|\,\U\right]\right]\\
&=\frac{k}{d}\mI_d +\BE_{\U}\left[\BE_{\vq}\left[\frac{(\Q\Q^\top\vq)(\vq^\top\Q\Q^\top)}{(\vq^\top\Q\Q^\top)(\Q\Q^\top\vq)}\,\Bigg|\,\U\right]\right]\\
&=\frac{k}{d}\mI_d +\frac{1}{d-k}\BE_{\U}[\Q\Q^\top]\\
&=\frac{k}{d}\mI_d +\frac{1}{d-k}\BE_{\U}[\I_d-\mA]\\
&=\frac{k}{d}\mI_d +\frac{1}{d-k}\frac{d-k}{d}\mI_d\\
&=\frac{k+1}{d}\mI_d,
\end{align*}
which completes the induction. In above derivation, the second equality is due to Lemma \ref{lem:1d_gauss} and the fact~$\Q\Q^{\top}\vq\sim\mathcal{N}(\vzero,\Q\Q^{\top})$ for given $\Q$; the third equality comes from the inductive hypothesis.
\end{proof}


\input{proof}


\section{The Proof of Section~\ref{sec:Block BFGS}}

We provide the proofs for the results of randomized block BFGS method shown in Section~\ref{sec:Block BFGS}.


\subsection{The Proof of Lemma~\ref{lm:bfgsnofar}}
\begin{proof}
According to Woodbury formula~\cite{woodbury1950inverting}, we have
\begin{align*}
    \G_{+}^{-1}&=\U(\U^{\top}\A\U)^{-1}\U^{\top}+\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\G^{-1}\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right).
\end{align*}
The condition \eqref{eq:blockneq} means we have 
\begin{align}
\label{eq:blockneq1}
    \frac{1}{\eta}\A^{-1}\preceq\G\preceq \A^{-1}.
\end{align}
Thus, we have
\begin{align*}
    G_{+}^{-1}&\overset{\eqref{eq:blockneq1}}{\preceq} \U(\U^{\top}\A\U)^{-1}\U^{\top}+\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\A^{-1}\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\\
    &\preceq\A^{-1},
\end{align*}
and
\begin{align*}
    \G_{+}^{-1}&\overset{\eqref{eq:blockneq1}}{\succeq}\U(\U^{\top}\A\U)^{-1}\U^{\top}+\frac{1}{\eta}\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\A^{-1}\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\\
    &=\frac{1}{\eta}\A ^{-1}+\left(1-\frac{1}{\eta}\right)\U(\U^{\top}\A\U)^{-1}\U^{\top}\\
    &{\succeq} \frac{1}{\eta}\A.
\end{align*}
Thus, we can obtain
\begin{align*}
    \frac{1}{\eta}\A^{-1}\preceq\G^{-1}_{+}\preceq \A^{-1},
\end{align*}
which is equivalent to the desired result \eqref{eq:blockneq}.
\end{proof}
\subsection{The Proof of Theorem~\ref{thm:bfgs}}
We first provide a lemma for the traces of positive definite matrices.
\begin{lemma}
\label{lm:trmulti}
For any positive semi-definite matrices $\B,\P_1,\P_2\in\BR^{d\times d}$ such that $\P_1\succeq \P_2$, we have
\begin{align}
    \tr{\P_1\B}\geq\tr{\P_2\B}.
\end{align}
\end{lemma}
\begin{proof}
We denote $\R=\P_1-\P_2\succeq\0$, then
\begin{align*}
    \R^{1/2}\B\R^{1/2}\succeq\0,
\end{align*}
which means 
\begin{align*}
    \tr{(\P_1-\P_2)^{1/2}\B(\P_1-\P_2)^{1/2}}\geq 0.
\end{align*}
So we have
\begin{align*}
 & \tr{\P_1\B}-\tr{\P_2\B} \\
 = & \tr{(\P_1-\P_2)\B}\\
=&\tr{(\P_1-\P_2)^{1/2}\B(\P_1-\P_2)^{1/2}}\\
\geq & 0.
\end{align*}
\end{proof}

Then we provide the proof of Theorem~\ref{thm:bfgs}
\begin{proof}
We rewrite the term of $\sigma_{\A}(\G_{+})$ by substituting $\G_{+}={ \text{\rm BlockBFGS}}(\G,\A,\U)$, that is
\begin{align}
\label{eq:sigmaupdate}
\begin{split}
 &\sigma_{\A}(\G_{+})\\
    =&\tr{(\G_+-\A)\A^{-1}}\\
    =& \sigma_{\A}(\G)-\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}+\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}.
\end{split}
\end{align}
Let $\P=\A^{1/2}\U$, then 
\begin{align}
\label{eq:matrixpsd}
\begin{split}
  &\U^{\top}\G{\left(\A^{-1}-\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\right)}\G\U\\
  =& \U^{\top}\G\A^{-1/2}\left(\I_d -\A^{1/2}\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\A^{1/2}\right)\A^{-1/2}\G\U\\
    {=}& \U^{\top}\G\A^{-1/2}\underbrace{\left(\I_d-\P\left(\P^{\top}\P\right)^{-1}\P^{\top}\right)}_{\succeq \0}\A^{-1/2}\G\U\\
    \succeq &\0.
\end{split}
\end{align}
So we have
\begin{align}
\label{eq:trineq}
\begin{split}               &\tr{\left(\U^{\top}\G\U\right)^{-1}\left(\U^{\top}\G\A^{-1}\G\U\right)}-\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}\\    =&\tr{\left(\U^{\top}\G\U\right)^{-1}\left(\left(\U^{\top}\G\A^{-1}\G\U\right)-(\U^{\top}\G\U)\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)\right)}\\    =&\tr{\left(\U^{\top}\G\U\right)^{-1/2}\U^{\top}\G{\left(\A^{-1}-\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\right)}\G\U\left(\U^{\top}\G\U\right)^{-1/2}}\\
    \overset{\eqref{eq:matrixpsd}}{\geq} &0.
\end{split}
\end{align}
Recall we assume that 
\begin{align}
    \label{eq:Aneq}
    \mu\I_d\preceq\A\preceq L\I_d.
\end{align}
Lemma~\ref{lm:trmulti} leads to
\begin{align}
\label{eq:trineq2}
\begin{split}           
    &\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\\
    =&\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}(\G-\A)\U\right)}\\
    \overset{\eqref{eq:Aneq}}{\geq} &\frac{1}{L}\tr{\left(\U^{\top}\U\right)^{-1}\left(\U^{\top}(\G-\A)\U\right)}\\
    = &\frac{1}{L}\tr{(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}\\
    \overset{\eqref{eq:Aneq}}{\geq} &\frac{\mu}{L}\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}.
\end{split}
\end{align}
Combining with Lemma~\ref{lm:EP}, we have
\begin{align}
\label{eq:trneq3}
\begin{split}
    &\EB\left[\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right]\\
    =&\EB\left[\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}\right] \\
    &~~~~~~+ \EB\left[\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right]\\
    {\geq}&\frac{\mu}{L}\EB\left[\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}\right]\\
    =&\frac{\mu}{L}\tr{\E\left[\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\right]\A^{-1}(\G-\A)} \\
    =&\frac{k}{d\varkappa}\sigma_\A(\G),
\end{split}
\end{align}
where the inequality is based on the results of \eqref{eq:trineq}  and \eqref{eq:trineq2}.
Finally, we have
\begin{align*}
    &\EB[\sigma_\A(\G_{+})]\\
    \overset{\eqref{eq:sigmaupdate}}{=}&\sigma_\A(\G)-\EB_\U\left[\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right]\\
    \overset{\eqref{eq:trneq3}}{\leq}& \left(1-\frac{k}{d\varkappa}\right)\sigma_\A(\G).
\end{align*}
\end{proof}


\subsection{The Proof of Theorem~\ref{thm:BFGS}}
We first provide a lemma which shows the linear convergence for the proposed block BFGS method (Algorithm~\ref{alg:bfgs}).
\begin{lemma}
\label{lm:BFGSlinear}
Under the setting of Theorem~\ref{thm:BFGS},  Algorithm~\ref{alg:bfgs} holds that 
\begin{align}
\label{eq:BFGSlinear}
    \lambda(\vx_t)\leq \left(1-\frac{1}{2\eta_0}\right)^t\lambda(\vx_0)~~~\text{and}~~~ \nabla^2 f(\x_t)\preceq \G_t\preceq \frac{3\eta_0}{2}\nabla^2 f(\x_t)
\end{align}
for all $t\geq 0$.
\end{lemma}
\begin{proof}
We can obtain this result by following the proof of Theorem~\ref{thm:srklinear} by replacing the use of  Lemma~\ref{lm:sr1good} with Lemma~\ref{lm:bfgsnofar}.
\end{proof}

Now we prove Theorem~\ref{thm:BFGS}.
\begin{proof}
We denote $\delta_t\triangleq\tr{(\G_{t}-\nabla^2 f(\x_t))(\nabla^2 f(\x_t))^{-1}}$ and $\lambda_t\triangleq \lambda(\x_t)$. The initial condition means we have the results of Lemma~\ref{lm:BFGSlinear}. 
According to Lemma~\ref{lm:GHneq}, we have 
\begin{align*}
   \nabla^2 f(\x_t)\overset{\eqref{eq:BFGSlinear}}{\preceq} \G_t\preceq (1+\delta_t) \nabla^2 f(\x_t).
\end{align*}
Using Theorem~\ref{thm:bfgs}, we have
\begin{align}
\label{eq:BFGSconverge1}
    \EB_t[\delta_{t+1}] = \EB_t[\sigma_{\nabla^2 f(\x_{t+1})}(\G_{t+1})] \overset{\eqref{eq:bfgssigma}}{\leq} \left(1-\frac{k}{d\varkappa}\right)\sigma_{\nabla^2 f(\x_{k+1})}(\tilde{\G}_t).
\end{align}
Using Lemma~\ref{lm:strong_self}, we have
\begin{align}
\label{eq:BFGSconverge2}
  \sigma_{\nabla^2 f(\x_{k+1})}(\tilde{\G}_t) \overset{\eqref{eq:delta_BFGS_lin}}{\leq}(1+Mr_t)^2(\delta_t+2dMr_t).
\end{align}
Thus, we can obtain following result
\begin{align*}
    \EB_{t}[\delta_{t+1}]\overset{\eqref{eq:linear-quadra}\,\eqref{eq:BFGSconverge1}\,\eqref{eq:BFGSconverge2}}{\leq} \left(1-\frac{k}{d\varkappa}\right)(1+M\lambda_t)^2(\delta_t+2dM\lambda_t).
\end{align*}
According to Lemma~\ref{lm:linear-quadra}, we have
\begin{align*}
    \lambda_{t+1}\overset{\eqref{eq:linear-quadra}}{\leq}\left(1+\frac{M\lambda_t}{2}\right)\frac{\delta_t+{M\lambda_t}/{2}}{1+\delta_t}\lambda_t\leq (1+M\lambda_t)^2(\delta_t+2dM\lambda_t)\lambda_t.
\end{align*}
According to Lemma~\ref{lm:BFGSlinear}, we have
\begin{align*}
    \lambda_{t}\leq \left(1-\frac{1}{2\eta_0}\right)^{t}\lambda_0.
\end{align*}
According to Lemma~\ref{lm:GHneq_2} and the initial condition (\ref{eq:bfgsini}), we have 
\begin{align*}
    \delta_0=\tr{(\G_0-\nabla^2f(\x_0))(\nabla^2 f(\x_0))^{-1}}\overset{\eqref{eq:GHneq_2}}{\leq} d(\eta_0-1)~~~\text{and}~~~\theta_0 = \delta_0+2dM\lambda_0\overset{\eqref{eq:bfgsini}}{\leq} d\eta_0.
\end{align*}
Hence, the random sequences of $\{\lambda_t\}$ and $\{\delta_t\}$ satisfy the conditions of Lemma~\ref{lm:superlinear} with
\begin{align*}
m=M,\quad b=2 dM,\quad c=2 dM,\quad
\alpha=\frac{d\varkappa}{k},\quad
\beta={2\eta_0}\quad\text{and}\quad s=\eta_0 d,
\end{align*}
which means we have proved Theorem~\ref{thm:BFGS}.

\end{proof}


\section{Extension for Solving Nonlinear Equations}
\begin{algorithm}[t]
\caption{Symmetric Rank-$k$ Method for Nonlinear Equation}\label{alg:SRK-NE}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\H_0$, $M$ and $k$. \\[0.15cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.15cm]
\STATE \quad  $\z_{t+1}=\z_t-\H_t^{-1}\J(\z_t)^{\top}  \F(\z_t)$ \\[0.15cm]
\STATE \quad $r_t=\|\z_{t+1}-\z_{t}\|_2$ \\[0.15cm]
\STATE \quad $\tilde{\H}_{t}=(1+Mr_t)\H_t$ \\[0.1cm]
\STATE \quad construct $\U_t$ by $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim} {\fN(0,1)}$ \\[0.15cm]
\STATE \quad  $\H_{t+1}= \srk(\tilde{\H}_t, \J(\z_{t+1})^\top\J(\z_{t+1}),\U_t)$\\[0.15cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}

In this section, we apply \srk~methods to solve the nonlinear equations
\begin{align}
\label{eq:NE}
    \F(\z) =\0,
\end{align}
where $\F:\RB^{d}\to\RB^{d}$ is a differentiable vector-valued function. We use $\J(\vz)$ to represent the Jacobian of $\F(\cdot)$ at $\z\in\RB^d$ and impose the following assumptions.

\begin{assumption}
\label{ass:NElip}
We assume the vector-valued function
$F:\RB^{d}\to\RB^d$ is differentiable and its Jacobian is $\tilde L_2$-Lipschitz continuous, i.e., there exists some $\tilde L_2\geq 0$ such that 
\begin{align}
    \|\J(\z)-\J(\z')\|\leq \tilde L_2\|\z-\z'\|.
\end{align}
for any $\vz,\vz'\in\BR^d$.
\end{assumption}

\begin{assumption}
\label{ass:nonde}
We assume there exists equation (\ref{eq:NE}) has a solution $\z^*$ such that $\J(\z^*)$ is non-degenerate.
\end{assumption}

According to Assumption~\ref{ass:nonde}, we denote
\begin{align*}
    \tilde\mu \triangleq \frac{\sigma_{\min}(\J(\z^*))}{\sqrt{2}},\qquad \tilde L \triangleq 2\sigma_{\max}(\J(\z^*)) \qquad \text{and} \qquad {\tilde\kappa}\triangleq \frac{\tilde L}{\tilde \mu},
\end{align*}
where $\sigma_{\min}(\cdot)$ and $\sigma_{\max}(\cdot)$ are the smallest and the largest singular values of given matrix respectively. 

We present \srk~methods for solving nonlinear equations in Algorithm~\ref{alg:SRK-NE}.
The design of this algorithm is inspired by the recent work of~\citet{liu2022quasinewton}, which applies the quasi-Newton methods to estimate the information of non-degenerate indefinite matrix by its square.
We use the Euclidean norm $\tilde\lambda(\z)\triangleq\norm{\F(\z)}$ to measure the convergence of our algorithm. 
The advantage of block updates in \srk~updates results a faster superlinear convergence than~\citet{liu2022quasinewton}'s methods.
Following the analysis of \srk~methods for convex optimization, we obtain the results for solving nonlinear equations as follows.


\begin{theorem}
\label{thm:srk-NE}
Under {Assumption \ref{ass:NElip} and \ref{ass:nonde}}, we run Algorithm~\ref{alg:SRK-NE} with $k<d$, $\tilde M={2\tilde{\varkappa}^2 \tilde 
 L_2}/{\tilde L}$ and set the initial~$\vz_0$ and $\H_0$ such that
\begin{align*}
    \tilde \lambda(\z_0)\leq \frac{\ln 2}{8}\cdot \frac{ (d-k)\tilde{\mu}}{\tilde  M\eta_0 d^2{\tilde \varkappa}^2} 
    \qquad \text{and} \qquad 
    \J(\z_0)^{\top}\J(\z_0)\preceq\H_0\preceq \eta_0\J(\z_0)^{\top}\J(\z_0)
\end{align*} 
for some $\eta_0\geq 1$. 
Then we have
\begin{align*}
     \EBP{\frac{\tilde{\lambda}({\z}_{t+1})}{\tilde{\lambda}(\z_t)}}\leq 2d\tilde{\varkappa}^2\eta_0\left(1-\frac{k}{d}\right)^{t}.
\end{align*}

\end{theorem}


\end{document}


