\documentclass{article}

%\usepackage[nonatbib]{neurips_2022}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow, hhline}
\usepackage[table]{xcolor}
\usepackage[para,online,flushleft]{threeparttable}

%arxiv version
\usepackage[letterpaper,margin=1.0in]{geometry}
\usepackage{graphicx}


%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{bbding}
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{enumitem}
%\usepackage{multirow}
\usepackage[numbers,sort]{natbib}
\usepackage{color}
\input{math.tex}

%\input{math.tex}
\usepackage{tcolorbox}
\usepackage{pifont}
\definecolor{mydarkgreen}{RGB}{39,130,67}
\definecolor{mydarkred}{RGB}{192,25,25}
\definecolor{bgcolor}{rgb}{0.93,0.99,1}
%\definecolor{bgcolor}{rgb}{0.8,1,1}
\definecolor{bgcolor2}{rgb}{0.8,1,0.8}
\definecolor{bgcolor3}{rgb}{0.50,0.90,0.50}


\newcommand{\green}{\color{mydarkgreen}}
\newcommand{\red}{\color{mydarkred}}
\newcommand{\cmark}{\green\ding{51}}%
\newcommand{\xmark}{\red\ding{55}}%
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cO}{\mathcal O}
\newcommand{\cC}{\mathcal C}


\usepackage{relsize} %SEB: feel free to revert, I think the font looked a bit too big otherwise
\newcommand{\algname}[1]{{\sf\green\relscale{0.90}#1}\xspace}
\newcommand{\algnameS}[1]{{\sf\green\relscale{0.90}#1}\xspace}
\newcommand{\dataname}[1]{{\tt\color{blue}#1}\xspace}
\begin{document}

\title{Symmetric Rank-$k$ Methods}
% \author{
%     Chengchang Liu\thanks{The Chinese Unversity of Hong Kong; 7liuchengchang@gmail.com} \qquad \qquad Cheng Chen\thanks{East China Normal University; jackchen1990@gmail.com} \qquad \qquad Luo Luo\thanks{Fudan University; luoluo@fudan.edu.cn}
%  }
\author{
    Chengchang Liu\thanks{The Chinese Unversity of Hong Kong; 7liuchengchang@gmail.com} \qquad \qquad Cheng Chen\thanks{East China Normal University; jackchen1990@gmail.com} \qquad \qquad Luo Luo\thanks{Fudan University; luoluo@fudan.edu.cn}
 }
 \date{}
 
% \date{Received: date / Accepted: date}


\maketitle

 \begin{abstract}
This paper proposes a novel class of block quasi-Newton methods for convex optimization which we call symmetric rank-$k$ (SR-$k$) methods.
Each iteration of SR-$k$ incorporates the curvature information with~$k$ Hessian-vector products achieved from the greedy or random strategy.
We prove SR-$k$ methods have the local superlinear convergence rate of $\OM\big((1-k/d)^{t(t-1)/2}\big)$ for minimizing smooth and strongly convex function, where $d$ is the problem dimension and $t$ is the iteration counter.
This is the first explicit superlinear convergence rate for block quasi-Newton methods, 
and it successfully explains why block quasi-Newton methods converge faster than ordinary quasi-Newton methods in practice.

 \end{abstract}
 

 

\section{Introduction}
We study quasi-Newton methods for solving the minimization problem
\begin{align}\label{prob:main}
    \min_{\x\in\RB^d} f(\x),
\end{align}
where $f:\BR^d\to\BR$ is smooth and strongly convex. 
Quasi-Newton methods~\cite{broyden1970convergence2,broyden1970convergence,shanno1970conditioning,broyden1967quasi,davidon1991variable,byrd1987global,yuan1991modified} are widely recognized for their fast convergence rates and efficient updates, which attracts growing attention in many fields such as statistics~\cite{jamshidian1997acceleration, zhang2011quasi,bishwal2007parameter},  economics~\cite{ludwig2007gauss,li2013dynamic} and machine learning~\cite{goldfarb2020practical, hennig2013quasi,liu2022quasi,liu2022partial,lee2018distributed}.
Unlike standard Newton methods which need to compute the Hessian and its inverse, quasi-Newton methods go along the descent direction by the following scheme
\begin{align*}
    \x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t),
\end{align*}
where $\G_t\in\BR^{d\times d}$ is an estimator of the Hessian  $\nabla^2 f(\x_t)$.
The most popular ways to construct the Hessian estimator are the Broyden family updates, including the Davidon--Fletcher--Powell (DFP) method~\cite{davidon1991variable,fletcher1963rapidly}, the Broyden--Fletcher--Goldfarb--Shanno (BFGS) method~\cite{broyden1970convergence2,broyden1970convergence,shanno1970conditioning}, and the symmetric rank-one (SR1) method~\cite{broyden1967quasi,davidon1991variable}. 

The classical quasi-Newton methods with Broyden family updates \cite{broyden1970convergence2,broyden1970convergence} find the Hessian estimator $\mG_{t+1}$ for the next round by the secant equation 
\begin{align}\label{eq:sec}
\G_{t+1}(\x_{t+1}-\x_t)=\nabla f(\x_{t+1})-\nabla f(\x_t).    
\end{align}
These methods have been proven to exhibit local superlinear convergence in 1970s~\cite{powell1971on,Dennis1974A,broyden1973on},
and their non-asymptotic superlinear rates were established in recent years~\cite{rodomanov2021rates,rodomanov2021new,ye2022towards,jin2022non}.
For example, \citet{rodomanov2021rates} showed classical BFGS method enjoys the local superlinear rates of $\OM\big((d\varkappa/t)^t/2\big)$, where~$\varkappa$~is the condition number of the objective. They also improved this result to $\OM\big((\exp{d\ln(\varkappa)/t}-1)^{t/2}\big)$~\cite{rodomanov2021new}. 
Later, \citet{ye2022towards} showed classical SR1 method converges with local superlinear rate of $\OM\big((d\ln(\varkappa)/t)^{t/2}\big)$.


Some recent works \cite{gower2017randomized,rodomanov2021greedy} proposed another type of quasi-Newton methods, which construct the Hessian estimator by the following equation
\begin{align}
\label{eq:grracondi}
    \G_{t+1}\u_t=\nabla^2 f(\x_{t+1})\u_t,
\end{align}
where $\u_t\in\RB^{d}$ is chosen by greedy or randomized strategies.
% \begin{align*}
%     \color{red} \text{write down the expressions}.
% \end{align*}
\citet{rodomanov2021greedy} established the local superlinear rate of $\OM\big((1-{1}/{(\varkappa d)})^{t(t-1)/2}\big)$ for greedy quasi-Newton methods with Broyden family updates.
Later, \citet{lin2021greedy} provided the condition-number free superlinear rate of~$\OM\big((1-{1}/{d})^{t(t-1)/2}\big)$ for greedy and randomized quasi-Newton methods with specific BFGS and SR1 updates.

Block quasi-Newton methods construct the Hessian estimator along multiple directions at per iteration. 
The study of these methods dates back to 1980s. \citet{schnabel1983quasi} proposed the first block BFGS method by extending equation~(\ref{eq:sec}) to multiple secant equations
\begin{align*}
    \G_{t+1}(\x_{t+1}-\x_{t+1-j})=\nabla f(\x_{t+1})-\nabla f(\x_{t+1-j})
\end{align*}
for $j=1,\dots,k$.
Although block quasi-Newton methods usually have better empirical performance than ordinary quasi-Newton methods without block fashion updates~\cite{schnabel1983quasi,o1994linear, gao2018block,gower2016stochastic,gower2017randomized,kovalev2020fast}, 
their theoretical guarantees are mystery until \citet{gao2018block}  proved block BFGS method has asymptotic local superlinear convergence.
On the other hand, \citet{gower2016stochastic,gower2017randomized,kovalev2020fast} introduced the randomized block BFGS by generalizing condition~\eqref{eq:grracondi} to
\begin{align*}
    \G_{t+1}\U_t=\nabla^2 f(\x_{t+1})\U_t,
\end{align*}
where $\U_t\in\RB^{d\times k}$ is some randomized matrix. 
Their empirical studies showed randomized block BFGS performs well on real-world applications.
In addition, \citet{kovalev2020fast} proved randomized block BFGS method also has asymptotic local superlinear convergence, but its advantage over ordinary BFGS methods is still unclear in theory. 

% The existing theoretical analysis for block quasi-Newton methods lacks the non-asymptotic convergence results, which naturally leads to the following question:
% \begin{center}
% \color{blue}
% \textit{Can we explain why block quasi-Newton method enjoy faster convergence behavior in practice?}
% \end{center} 
The theoretical results of existing block quasi-Newton methods cannot explain why they enjoy faster convergence behavior than ordinary quasi-Newton methods in practice. 
This naturally leads to the following question:
\begin{center}    
\textit{Can we provide a block quasi-Newton method with explicit superior convergence rate?}
\end{center}
In this paper, we give an affirmative answer by proposing symmetric rank-$k$ (SR-$k$) methods.
%for $k\in[d-1]$ and the quadratic rate of Newton methods for $k=d$. 
The constructions of Hessian estimators in \srk~methods are based on generalizing the idea of symmetric rank-1 (SR1)~\cite{broyden1967quasi,davidon1991variable,ye2022towards} methods and the equation of the form~\eqref{eq:grracondi}. 
We provide the randomized and greedy strategies to determine $\mU_t\in\BR^{d\times k}$ for SR-$k$.
Both of them lead to the explicit local superlinear convergence rate of~$\OM\big((1-{k}/{d})^{t(t-1)/2}\big)$, where $k$~is the number of directions used to approximate Hessian at per iteration.
For~$k=1$, our methods reduce to randomized and greedy SR1 methods~\cite{lin2021greedy}.
For~$k\geq 2$, it is clear that SR-$k$ methods have faster superlinear rates than existing greedy and randomized quasi-Newton methods~\cite{lin2021greedy,rodomanov2021greedy}.
We also follow the design of SR-$k$ to propose a variant of randomized block BFGS method~\cite{gower2016stochastic,gower2017randomized,kovalev2020fast} and a randomized block DFP method, resulting an explicit superlinear convergence rate of~$\OM\big((1-k/(\varkappa d))^{t(t-1)}\big)$.
We compare the results of proposed methods with existing quasi-Newton methods in Table~\ref{tbl:main-2}. 

The remainder of this paper is organized as follows.
%\paragraph{Paper Organization}
In Section~\ref{sec:pre}, we introduce the notation and the preliminaries throughout this paper. 
In Section~\ref{sec:update}, we propose the SR-$k$ update in the view of matrix approximation. 
In Section~\ref{sec:algorithm}, we propose the quasi-Newton methods with SR-$k$ updates for minimizing smooth and strongly convex function and provide their the superior local superlinear convergence rates. 
In Section~\ref{sec:Block BFGS}, we propose a new randomized block BFGS method and randomized block DFP method with explicit local superlinear convergence rates.
In Section~\ref{sec:exp}, we conduct numerical experiments to show the outperformance of proposed methods.
Finally, we conclude our work in Section~\ref{sec:conclusion}.

\begin{table*}[!t]
	\centering
	\caption{We summarize the local convergence rates of existing and proposed quasi-Newton methods for strongly convex optimization. 
    The second column displays the rank of modification (i.e., $\mG_{+}-\mG$ in SR-$k$ update (\ref{eq:srk})-(\ref{iter:srk})) in per iteration of quasi-Newton methods.}\label{tbl:main-2}
\begin{threeparttable}
\setlength\tabcolsep{5.pt}
\begin{tabular}{c c c c}
\toprule[.1em]
\begin{tabular}{c} 
    \bf Methods  
\end{tabular} &
\bf Rank & 
\begin{tabular}{c}  \bf Convergence \end{tabular} & \bf Reference \\
\toprule[.1em]

\begin{tabular}{c} 
  Newton Method\\  
\end{tabular} 
& $d$ 
& quadratic & \cite{nesterov2018lectures,nocedal1999numerical}
\\  
\midrule 

% \begin{tabular}{c}  DFP \\      
% \cite{rodomanov2021rates,rodomanov2021new} 
% \end{tabular}
% & $1$ or $2$ 
% & $\displaystyle{\OM\left(\big(\frac{d\varkappa^2}{t}\big)\right)}$  
% \\  
% \midrule 


\begin{tabular}{c}  Classical BFGS/DFP \\      
\end{tabular}
& $2$ 
&\begin{tabular}{c}
$\displaystyle{\OM\big(\big({1}/{t}\big)^t\big)}$\\ [0.1cm]
    $\displaystyle{\OM\big(\big(\exp{{d\ln(\varkappa)}/{t}}-1\big)^t\big)}$  \\  [0.1cm]
   $\displaystyle{\OM\big(\big({1}/{t}\big)^t\big)}$~\tnote{(*)}  
\end{tabular}   
& \begin{tabular}{c}
  \cite{rodomanov2021rates}  \\[0.1cm] 
 \cite{rodomanov2021new} \\[0.1cm]
 \cite{jin2022non}
\end{tabular} 
\\  
\midrule 


% \begin{tabular}{c}  BFGS/DFP \\      
% \cite{jin2022non} 
% \end{tabular}
% & $2$ 
% & $\displaystyle{\OM\big(\big({1}/{t}\big)^t\big)}$~\tnote{(*)}  
% \\  
% \midrule 


\begin{tabular}{c} Classical SR1 \\      
\end{tabular}
& $1$ 
&  $\displaystyle{\OM\big(\big({d\ln(\varkappa)}/{t}\big)^t\big)}$  
&  \cite{ye2022towards}
\\  
\midrule 
           
\begin{tabular}{c}
    Greedy/Randomized Broyden Family 
\end{tabular} 
& ~~$1$ or $2$~~ 
& ~~$\displaystyle{\OM\big((1-1/(\varkappa d))^{{t(t-1)}/{2}}\big)}$~~  
&\cite{rodomanov2021greedy,lin2021greedy}
\\  
\midrule 

\begin{tabular}{c}
    Greedy/Randomized BFGS
\end{tabular}
& $2$ 
& $\displaystyle{\OM\big((1-1/d)^{{t(t-1)}/{2}}\big)}$  
& \cite{lin2021greedy}
\\
\midrule 

\begin{tabular}{c}
    Greedy/Randomized SR1 
\end{tabular}
& $1$
& $\displaystyle{\OM\big((1-1/d)^{{t(t-1)}/{2}}\big)}$  
&\cite{lin2021greedy}
\\
\midrule 


\begin{tabular}{c}
    Multi-Secant Block-BFGS 
\end{tabular} 
& $k\in[d]$ 
& asymptotic superlinear  
&  \cite{schnabel1983quasi,gao2018block}
\\  
\midrule 
\begin{tabular}{c}
Randomized Block-BFGS (v1) 
\end{tabular} 
& $k\in[d]$ 
& asymptotic superlinear 
&   \cite{kovalev2020fast,gower2017randomized} 
\\  
\midrule
\cellcolor{bgcolor}	
\begin{tabular}{c}
\!\!Randomized Block-BFGS (v2)  
\end{tabular}
&\cellcolor{bgcolor}	 \!$k\in[d]$  
& \cellcolor{bgcolor}	$\displaystyle{\OM\big((1-{k}/{(\varkappa d)})^{{t(t-1)}/{2}}\big)}$  
&  \cellcolor{bgcolor}	   Algorithm~\ref{alg:bfgs} 
\\  
\midrule
\cellcolor{bgcolor}	
\begin{tabular}{c}
    Randomized Block-DFP
\end{tabular}
&\cellcolor{bgcolor}	 \!$k\in[d]$  
& \cellcolor{bgcolor}	$\displaystyle{\OM\big((1-{k}/{(\varkappa d)})^{{t(t-1)}/{2}}\big)}$  
& \cellcolor{bgcolor}	 Algorithm~\ref{alg:bfgs} 
\\
\midrule
\cellcolor{bgcolor}	
Greedy/Randomized SR-$k$ 
\cellcolor{bgcolor}	
& 
\begin{tabular}{c}
    ~~~$k\in[d-1]$   \\[0.1cm]
    ~$k=d$  
\end{tabular}
\cellcolor{bgcolor}	
& \cellcolor{bgcolor}	
\begin{tabular}{c}
$\OM\left((1-{k}/{d})^{{t(t-1)}/{2}}\right)$ \\[0.1cm]
quadratic
\end{tabular}
% $\displaystyle{\begin{cases}\OM\left((1-{k}/{d})^{{t(t-1)}/{2}}\right), & k\in[d-1] \\[0.15cm]
% \OM\left(\lambda_0^{2^t}\right), & k=d 
% \end{cases}}$  
&  \cellcolor{bgcolor} Algorithm~\ref{alg:SRK}
\\[0.25cm]
 \bottomrule[.1em]
\end{tabular} % default value: 6pt
\begin{tablenotes}
    	{\scriptsize     
  			\item [{ (*)}] The convergence rate(s) require an additional assumption that the initial Hessian estimator be sufficiently closed to the true Hessian.}
\end{tablenotes}
\end{threeparttable}	
\end{table*}  


\section{Preliminaries}
\label{sec:pre}
We first introduce the notations used in this paper. We use $\{\e_1,\cdots,\e_d\}$ to present the the standard basis in space $\RB^d$ and let $\I_d\in\RB^{d\times d}$ be the identity matrix. 
We use $\S^{\dag}$ to denote the Moore-Penrose inverse of matrix $\S$.
We denote the trace of a square matrix by~$\tr{\cdot}$. 
We use $\|\cdot\|$ to present the spectral norm and Euclidean norm of matrix and vector respectively. Given a positive definite matrix $\A\in\BR^{d\times d}$, we denote the corresponding weighted norm as~$\|\vx\|_\mA\triangleq(\vx^{\top}\mA\vx)^{1/2}$ for some $\vx\in\BR^d$. 
We use the notation~$\Norm{\vx}_\vz$ to present~$\|\vx\|_{\nabla^2 f(\vz)}$ for positive definite Hessian $\nabla^2 f(\vz)$, if there is no ambiguity for the reference function~$f(\cdot)$.
We also define~
\begin{align}
    \label{eq:EA}
    \mE_{k}(\mA)\triangleq[\ve_{i_1};\cdots;\ve_{i_k}]\in\BR^{d\times k},
\end{align}
where $i_1,\dots,i_k$ are the indices for the largest $k$ entries in the diagonal of $\mA$.

% When 
% The expectation $\EB_{\U}[\,\cdot\,]$ or $\EB[\,\cdot\,]$ consider the randomness of the random matrix $\U$ for one iteration or all the randomness during the updates, we can view it with no randomness for the same notation when applied to deterministic updates.

Throughout this paper, we suppose the objective in problem (\ref{prob:main}) satisfies the following assumptions.
\begin{assumption}
\label{ass:smooth}
We assume the objective function $f:\BR^d\to\BR$ is $L$-smooth, i.e., there exists some constant~$L\geq 0$ such that $\|\nabla f( \vx)-\nabla f(\vy)\|\leq L\|\vx-\vy\|$ for any $\vx,\vy\in\BR^d.$
%$\nabla^2 f(\vx) \preceq L \I_d$.
\end{assumption}

\begin{assumption}
\label{ass:strongconvex}
We assume the objective function $f:\BR^d\to\BR$ is $\mu$-strongly-convex, i.e., there exists some constant~$\mu>0$ such that 
\begin{align*}
f(\lambda\vx+(1-\lambda)\vy)\leq \lambda f(\vx) + (1-\lambda)f(\vy)-\frac{\lambda(1-\lambda)\mu}{2}\norm{\vx-\vy}^2    
\end{align*}
for any $\vx,\vy\in\BR^d$ and $\lambda\in[0,1]$.
\end{assumption}
We define the condition number as $\varkappa \triangleq L/\mu$. 
The following proposition shows the objective function has bounded Hessian under Assumption \ref{ass:smooth} and~\ref{ass:strongconvex}.
\begin{proposition}
Suppose the objective function $f:\BR^d\to\BR$ satisfies Assumptions~\ref{ass:smooth} and \ref{ass:strongconvex}, then it holds 
\begin{align}
\mu\mI_d \preceq \nabla^2 f(\x) \preceq L\mI_d
\end{align}
for any $\x\in\BR^d$.
\end{proposition}


We also impose the assumption of strongly self-concordance~\cite{rodomanov2021greedy,lin2021greedy} as follows.
\begin{assumption}
\label{ass:strongself}
We assume the objective function $f:\RB^d\to\RB$ is $M$-strongly self-concordant, i.e., there exists some constant $M\geq 0$ such that
\begin{align}
    \nabla^2 f(\y)-\nabla^2 f(\x)\preceq M\|\y-\x\|_\z\nabla^2 f(\w), 
\end{align}
for any $\x,\y,\w,\z\in\RB^d$.
\end{assumption}

The strongly-convex function with Lipschitz-continuous Hessian is strongly self-concordant.

\begin{proposition}
Suppose the objective function $f:\BR^d\to\BR$ satisfies Assumptions \ref{ass:strongconvex} and its Hessian is $L_2$-Lipschitz continuous, i.e., we have
\begin{align*}
\| \nabla^2 f(\x)-\nabla^2 f(\y)\|\leq L_2\|\x-\y\|,    
\end{align*}
for all $\x$, $\y\in\RB^d$, then the function $f(\cdot)$ is $M$-strongly self-concordant with $M={L_2}/{\mu^{3/2}}$.
\end{proposition}



\section{Symmetric Rank-$k$ Updates}
\label{sec:update}
We propose the block version of classical SR1 update, called the symmetric rank-$k$ (SR-$k$) update, as follows.

\begin{definition}[SR-$k$ Update]
Let $\mA\in\BR^{d\times d}$ and $\mG\in\BR^{d\times d}$ be two positive-definite matrices with~$\A\preceq \G$. 
For any matrix $\U\in\RB^{d\times k}$, we define
\begin{align}
\label{eq:srk}    
    \srk(\G,\A,\U) \triangleq \G-(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A).
\end{align}
\end{definition}
In this section, 
% we denote  $\R\triangleq \G-\A\succeq\0$, 
we always use $\G_{+}$ to denote the output of \srk~update such that 
\begin{align}\label{iter:srk}
\G_{+}\triangleq\srk(\G,\A,\U).     
\end{align}
We provide the following lemma to show that \srk~update does not increase the deviation from the target matrix $\A$, which is similar to ordinary Broyden family updates~\cite{rodomanov2021greedy}.
\begin{lemma}
\label{lm:sr1good}
Given any positive-definite matrices $\mA\in\BR^{d\times d}$ and $\mG\in\BR^{d\times d}$ with $\A\preceq\G\preceq\eta \A$ for some~$\eta\geq 1$, it holds that
\begin{align}
\label{eq:srk_good}
   \A\preceq\G_{+}\preceq \eta\A. 
\end{align}
\end{lemma}
\begin{proof}
According to the update rule (\ref{eq:srk}), we have
\begin{align*}
    &\G_{+}-\A\\
    &= (\G-\A)-(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)\\
    &=\left(\I_d-(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}\right)(\G-\A)\left(\I_d-\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)\right)\succeq\0,
\end{align*}
which indicates $\G_{+}\succeq \A$.
% \begin{align*}
%     \G_{+}\succeq\A.
% \end{align*}
The condition $\mG\preceq\eta\mA$ means
\begin{align*}
    \G_{+}&\preceq \eta\A - \underbrace{(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)}_{\succeq\0}\preceq \eta\A,
\end{align*}
which finish the proof. 
\end{proof}


To evaluate the convergence of SR-$k$ update, we introduce the following quantity~\cite{lin2021greedy,ye2022towards}
 \begin{align}
 \label{eq:measure_srk}
     \tau_\A(\G)\triangleq\tr{\G-\A}
 \end{align}
which characterizes the difference between target matrix $\mA$ and the current estimator $\mG$.
% The randomized or greedy SR1 update~\cite{lin2021greedy} omits the rate of 
% \begin{align*}
%     \EBP{\tau_{\A}(\G_{+})}\leq \left(1-\frac{1}{d}\right)\tau_{\A}(\G),
% \end{align*}
% when take $k=1$ in \eqref{eq:srk} and choose a proper $\U\in\RB^{d\times 1}$.

In the remainder of this section, we aim to establish the convergence guarantee  
\begin{align}
\label{eq:srk-aim}
    \EBP{\tau_{\A}(\G_{+})}\leq \left(1-\frac{k}{d}\right)\tau_{\A}(\G),
\end{align}
for approximating the target matrix $\mA$ by \srk~iteration (\ref{iter:srk}) with appropriate choice of $\mU\in\BR^{d\times k}$.
Note that if we take $k=1$, the equation (\ref{eq:srk-aim}) will reduce to $ \EBP{\tau_{\A}(\G_{+})}\leq \left(1-{1}/{d}\right)\tau_{\A}(\G)$, which corresponds the convergence result of randomized and greedy SR1 updates~\cite{lin2021greedy}.
%which exhibit the advantage of using block updates in terms of approximating the object matrix.
% {\color{red}The following theorem} show that \srk~updates with randomized or greedy strategies enjoy explicit faster convergence rate than SR1~updates for estimating $\mA$ in terms of the measure $\tau_\A(\cdot)$.



Observe that we can split $\tau_{\A}(\G_{+})$ as follows
\begin{align}
\label{eq:tauAG+}
\begin{split}
    \tau_{\A}(\G_{+}) &\!\overset{\eqref{eq:srk}, \eqref{iter:srk}}{=}\tr{\G-\A - (\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)} \\
    &~~=\underbrace{\tr{\G-\A}}_{\text{Part}~{\rm \uppercase\expandafter{\romannumeral1}}} - \underbrace{\tr{(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)}}_{\text{Part}~{\rm \uppercase\expandafter{\romannumeral2}}},
    \end{split}
\end{align}
where Part I is equal to $\tau_{\A}(\G)$ and Part II encourage the \srk~update to decrease $\tau_\A(\cdot)$.
To obtain the desired result of \eqref{eq:srk-aim}, we only need to prove Part II satisfies
\begin{align*}
\EBP{\tr{(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)}}\geq \frac{k}{d}\tr{\G-\A}.
\end{align*}

We first provide the following lemma to bound Part II (in the view of $\mR=\mG-\mA$). 
% the Part~{\rm \uppercase\expandafter{\romannumeral2} is lower bounded. This guarantees a sufficient decrease of $\tau_\A(\cdot)$.
\begin{lemma}
\label{lm:pdneq2}
For positive semi-definite matrix $\R\in\RB^{d\times d}$ and full rank matrix $\U\in\RB^{d\times k}$ with $k\leq d$, it holds that
\begin{align}
\label{eq:keyeq}  \tr{\R\U\left(\U^{\top}\R\U\right)^{\dag}\U^{\top}\R} \geq \tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\R}.
\end{align}
\end{lemma}
\begin{proof}
Let $\U=\Q\mSigma\V^{\top}$ be SVD of $\mU$, where $\Q\in\RB^{d\times k}$, $\V\in\RB^{k\times k}$ are (column) orthogonal and~$\mSigma\in\RB^{k\times k}$ is diagonal, then the right-hand side of inequality (\ref{eq:keyeq}) can be written as
\begin{align*}
\tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\R}& = \tr{\Q\mSigma\V^{\top}\left(\V\mSigma^2\V^{\top}\right)^{-1}\V\mSigma\Q^{\top}\R}  =\tr{\Q\Q^{\top}\R}.
\end{align*}
Consequently, we upper bound the left-hand side of inequality (\ref{eq:keyeq}) as follows 
\begin{align*}
    &\tr{\R\U\left(\U^{\top}\R\U\right)^{\dag}\U^{\top}\R} \\
    &= \tr{\R\Q\mSigma\V^{\top}\left(\V\mSigma\Q^{\top}\R\Q\mSigma\V^{\top}\right)^{\dag}\V\mSigma\Q^{\top}\R}\\
    &=\tr{\R\Q\mSigma\left(\mSigma\Q^{\top}\R\Q\mSigma\right)^{\dag}\mSigma\Q^{\top}\R}\\
    &=\tr{\Q^{\top}\R\Q\mSigma\left(\mSigma\Q^{\top}\R\Q\mSigma\right)^{\dag}\mSigma\Q^{\top}\R\Q}+\tr{(\I_d-\Q\Q^{\top})^{1/2}\R\Q\mSigma\left(\mSigma\Q^{\top}\R\Q\mSigma\right)^{\dag}\mSigma\Q^{\top}\R(\I_d-\Q\Q^{\top})^{1/2}}
    \\
    &\geq \tr{\Q^{\top}\R\Q\mSigma\left(\mSigma\Q^{\top}\R\Q\mSigma\right)^{\dag}\mSigma\Q^{\top}\R\Q}\\
&=\tr{\mSigma^{-1}\mSigma\Q^{\top}\R\Q\mSigma\left(\mSigma\Q^{\top}\R\Q\mSigma\right)^{\dag}\mSigma\Q^{\top}\R\Q\mSigma\mSigma^{-1}}\\
&=   \tr{\mSigma^{-1}\mSigma\Q^{\top}\R\Q\mSigma\mSigma^{-1}} \\
&= \tr{\Q^{\top}\R\Q},
    % &=\tr{\R\Q\left(\Q^{\top}\R\Q\right)^{\dag}\Q^{\top}\R}\\
    % &=\tr{\Q^{\top}\R\Q\left(\Q^{\top}\R\Q\right)^{\dag}\Q^{\top}\R\Q}+\tr{\R\Q\left(\Q^{\top}\R\Q\right)^{\dag}\Q^{\top}\R(\I-\Q\Q^{\top})}\\
    % &=\tr{\Q^{\top}\R\Q\left(\Q^{\top}\R\Q\right)^{\dag}\Q^{\top}\R\Q}+\tr{(\I_d-\Q\Q^{\top})^{1/2}\R\Q\left(\Q^{\top}\R\Q\right)^{\dag}\Q^{\top}\R(\I_d-\Q\Q^{\top})^{1/2}}
    % \\
    % &\geq\tr {\Q^{\top}\R\Q},
\end{align*}
where the inequality is due to the fact that
\begin{align*}
    \I_d-\Q\Q^{\top} = \I_d-\Q(\Q^{\top}\Q)^{-1}\Q^{\top} \succeq \0.
\end{align*}
Combining all above results leads to inequality \eqref{eq:keyeq}.  
\end{proof}

We then provide two strategies to select matrix $\mU\in\BR^{d\times k}$ for \srk~update:
\begin{enumerate}
\item For randomized strategy, we sample each entry of $\mU$ according to standard normal distribution independently, i.e.,
\begin{align*}
%\label{eq:U_greedy}
    [\mU]_{ij} \overset{{\rm i.i.d}}{\sim} \fN(0,1).
\end{align*}
\item For greedy strategy, we construct $\mU$ as 
\begin{align*}
%\label{eq:U_random}
\U=\mE_{k}(\G-\A),     
\end{align*}
where $\E_k(\cdot)$ follows the notation of \eqref{eq:EA}. 
\end{enumerate}
% \begin{remark}
% For $k=1$, SR-$k$ updates with above two strategies reduce to randomized or greedy SR1 updates~\cite{rodomanov2021rates,lin2021greedy}.
% \end{remark}
The following lemma indicates that the term of  $\tr{\U(\U^{\top}\U)^{-1}\U\R}$ in \eqref{eq:keyeq} can be further lower bounded when we choose $\U$ by the above randomized or greedy strategy, which guarantees a sufficient decrease of~$\tau_{\A}(\cdot)$ for \srk~updates.

% The remain of this section shows the multiple directions in $\mU\in\BR^{d\times k}$ provably make \srk~update has the advantage over SR1 update in the view of estimating the target matrix $\mA$. The following lemma indicates that constructing $\U$ by randomized or greedy strategy can lower bound $\tr{\U(\U^{\top}\U)^{-1}\U^{\top}\R}$ by~$\tau_\A(\G)=\tr{\R}$.

\begin{lemma}
\label{lm:explicitbound}
\srk~updates with randomized and greedy strategies  have the following properties:
\begin{enumerate}[label=(\alph*),topsep=0pt, leftmargin=0.7cm,itemsep=0.15cm]
\item If $\U\in\BR^{d\times k}$ is chosen as $[\mU]_{ij} \overset{{\rm i.i.d}}{\sim} \fN(0,1)$, then we have    
    % If $\U$ is chosen by randomized strategy, i.e., each entry of $\mU$ is independently distributed to $\fN(0,1)$, then 
\begin{align}\label{eq:random-up}         \EBP{\tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\R}} = \frac{k}{d} \tr{\R},
\end{align}
for any matrix $\R\in\BR^{d\times d}$.     
\item If $\U\in\BR^{d\times k}$ is chosen by $\U=\E_k(\R)$, then we have
\begin{align}\label{eq:greedy-up}
{\tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\R}} \geq \frac{k}{d} \tr{\R},
\end{align}
for any matrix $\R\in\BR^{d\times d}$. 
\end{enumerate}
\end{lemma}
\begin{proof}
We first consider the randomized strategy such that each entry of $\mU\in\BR^{d\times k}$ is independently sampled from~$\fN(0,1)$.
We use $\fV_{d,k}$ to present the Stiefel manifold which is the set of all $d\times k$ column orthogonal matrices and denote $\fP_{k,d-k}$ as the set of all $m\times m$ orthogonal projection matrices of rank $k$.

According to Theorem 2.2.1 (iii) and Theorem 2.2.2 (iii) of \citet{chikuse2003statistics}, the random matrix
\begin{align*}
    \Z=\U(\U^{\top}\U)^{-1/2}
\end{align*}
is uniformly distributed on $\fV_{d,k}$ and the random matrix
\begin{align*}
    \Z\Z^\top=\U(\U^{\top}\U)^{-1}\U^{\top} 
\end{align*}
is uniformly distributed on $\fP_{k,d-k}$.
Applying Theorem 2.2.2 (i) of \citet{chikuse2003statistics} on $\Z\Z^{\top}$ achieves
\begin{align}\label{eq:EP}    \EB\left[\U(\U^{\top}\U)^{-1}\U^{\top}\right] = \frac{k}{d}\I_d.
\end{align}
Consequently, we have
\begin{align*}
    \EB \left[\tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\R}\right]&=\tr{\EB\left[\U\left(\U^{\top}\U\right)^{-1}\U^{\top}\right]\R}\overset{\eqref{eq:EP}}{=}\frac{k}{d}\tr{\R}.
\end{align*}

% We first prove the following fact for the random matrix $\U$ that
% \begin{align}
% \label{eq:EP}    \EB\left[\U(\U^{\top}\U)^{-1}\U^{\top}\right] = \frac{k}{d}\I_d.
% \end{align}
% We use $\fV_{d,k}$ to present the Stiefel manifold which is the set of all $d\times k$ column orthogonal matrices.
% We denote $\fP_{k,d-k}$ as the set of all $m\times m$ orthogonal projection matrices of rank $k$.

% According to Theorem 2.2.1 (iii) of \citet{chikuse2003statistics}, the random matrix
% \begin{align*}
%     \Z=\U(\U^{\top}\U)^{-1/2}
% \end{align*}
% is uniformly distributed on the Stiefel manifold $\fV_{d,k}$.
% Applying Theorem 2.2.2 (iii) of \citet{chikuse2003statistics}, the random matrix
% \begin{align*}
%     \Z\Z^\top=\U(\U^{\top}\U)^{-1}\U^{\top} 
% \end{align*}
% is uniformly distributed on $\fP_{k,d-k}$. 
% Combining above results with Theorem 2.2.2 (i) of \citet{chikuse2003statistics} on $\Z\Z^{\top}$ achieves~\eqref{eq:EP}.

% The proof of \eqref{eq:EP} is formally presented in  Lemma~\ref{lm:EP} in Appendix~\ref{sec:auli}. 

Then we consider the greedy strategy such that $\U=\E_k(\R)$.
We use $\{r_{i}\}_{i=1}^{d}$ to denote the diagonal entries of $\R$ such that
\begin{align*}
     r_{1}\geq r_{2} \geq \cdots \geq r_{d},
\end{align*}
which implies
\begin{align}
\label{eq:bigeq}
    \sum_{i=1}^d r_{i} =\tr{\R}\qquad{\text{and}}\qquad \sum_{i=1}^{k}r_{i}\geq\frac{k}{d}\tr{\R}.
\end{align}
We let $\vu_i\in\BR^d$ be the $i$-th column of $\mU$, then we have
\begin{align}  
\label{eq:grleq}
\begin{split}
&\tr{\U(\U^{\top}\U)^{-1}\U^{\top}\R}  \\
&=\tr{(\U^{\top}\U)^{-1}\U^{\top}\R\U} \\
&=\tr{\I_k\U^{\top}\R\U} =\tr{\U^{\top}\R\U}\\
&= \sum_{i=1}^{k} \u_{i}^{\top}\R \u_{i} =\sum_{i=1}^{k}r_{i}\\
&\!\!\overset{\eqref{eq:bigeq}}{\geq} \frac{k}{d}\tr{\R}.
\end{split}
\end{align}
 
\end{proof}

\begin{remark}
The proof of result (\ref{eq:random-up}) in Lemma~\ref{lm:explicitbound}) for the randomized strategy  requires the knowledge for statistics on manifold. 
For the readers who are not familiar with this, we also provide an elementary proof by induction in Appendix~\ref{appen:addiproof}.
\end{remark}


Now, we formally present the convergence result~\eqref{eq:srk-aim} for approximating positive definite matrices by \srk~update.
\begin{theorem}\label{thm:matrix}
Let $ \G_{+}={\text{\rm SR-$k$}}(\G,\A,\U)$
% \begin{align*}
% %\label{eq:block-update}
%  \G_{+}={\text{\rm SR-$k$}}(\G,\A,\U)
% \end{align*}
with $\G,\mA\in\RB^{d\times d}$ such that $\mG\succeq\mA$ and select $\U\in\RB^{d\times k}$ by randomized strategy~$[\mU]_{ij} \overset{{\rm i.i.d}}{\sim} \fN(0,1)$ or greedy strategy $\mU=\mE_k(\G-\A)$, where $k\leq d$. 
% one of the following strategies:
% \begin{enumerate}
% \item Sample each entry of $\mU$ according to $\fN(0,1)$ independently.
% \item Construct $\U=\E_k(\mG-\mA)$.
% \end{enumerate}
Then we have 
\begin{align*}
\EB\left[\tau_{\A}(\G_{+})\right]\leq \left(1-\frac{k}{d}\right)\tau_\A(\G).  
\end{align*}
% , we have
% \begin{align}\label{ieq:srk-matrix}
%     \EB\left[\tau_{\A}(\G_{+})\right]\leq \left(1-\frac{k}{d}\right)\tau_\A(\G).
% \end{align}
\end{theorem}
\begin{proof}
Applying Lemma~\ref{lm:pdneq2} and Lemma~\ref{lm:explicitbound} by taking $\R=\G-\A$, we have
\begin{align*}
    \EBP{\tau_{\A}(\G_{+})}&~~\overset{\eqref{eq:tauAG+}}{=}\tau_{\A}(\G)-\EBP{\tr{(\G-\A)\U(\U^{\top}(\G-\A)\U)^{\dag}\U^{\top}(\G-\A)}}\\
    &~~\overset{\eqref{eq:keyeq}}{\leq}\tau_{\A}(\G) -\EBP{\tr{\U\left(\U^{\top}\U\right)^{-1}\U^{\top}(\G-\A)}}\\
    &\overset{\eqref{eq:random-up},\eqref{eq:greedy-up}}{\!\leq} \!\!\!\!\tau_{\A}(\G) -\frac{k}{d}\tr{(\G-\A)} \\
    &~~~\,=\left(1-\frac{k}{d}\right)\tau_{\A}(\G).
\end{align*}
 
\end{proof}

The term $(1-k/d)$ before $\tau_\mA(\mG)$ reveals the advantage of block-type update in \srk~methods, since the larger $k$ leads to the faster decay of $\tau_{\A}(\cdot)$.
As a comparison, the results of randomized or greedy SR1 updates~\cite{lin2021greedy} only match the special case of Theorem \ref{thm:matrix} when $k=1$.







%\section{Minimizing of Strongly Self-Concordant Function}
\section{Minimizing Strongly Convex Function}
\label{sec:algorithm}
\begin{algorithm}[t]
\caption{Symmetric Rank-$k$ Method}\label{alg:SRK}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\vx_0, \G_0$, $M$, and $k$. \\[0.15cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.15cm]
\STATE \quad $\x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t)$ \\[0.15cm]
\STATE \quad $r_t=\|\x_{t+1}-\x_{t}\|_{\x_t}$ \\[0.15cm]
\STATE \quad  $\tilde{\G}_{t}=(1+Mr_t)\G_t$ \\[0.15cm]
\STATE \quad Construct $\U_t\in\RB^{d\times k}$ by  \\[0.05cm]
\quad\quad  (a) randomized strategy: $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim}{\fN(0,1)}$ \\[0.15cm]
\quad\quad  (b) greedy strategy: $\U_t=\E_k(\tilde{\G}_t-\nabla^2 f(\x_{t+1}))$ \\[0.15cm]
\STATE \quad $\G_{t+1}= \srk(\tilde{\G}_t,\nabla^2f(\x_{t+1}),\U_t)$ \\[0.15cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}

By leveraging the proposed \srk~updates, we introduce a novel block quasi-Newton method, which we call \srk~method.
We present the details of \srk~method in Algorithm~\ref{alg:SRK}, where $M>0$ is the self-concordant parameter that follows the notation in Assumption~\ref{ass:strongself}.
% \cc{By leveraging the proposed \srk~update, we study a novel block quasi-Newton method for strongly self-concordant function, which we call \srk~method. We present the \srk~method in Algorithm~\ref{alg:SRK}, where $M>0$ follows the notation in Assumption~\ref{ass:strongself}.}
% %We propose the \srk~method for minimizing strongly self-concordant function in Algorithm~\ref{alg:SRK}, where $M>0$ follows the notation in Assumption~\ref{ass:strongself}.

We shall consider the convergence rate of \srk~method (Algorithm~\ref{alg:SRK}) and show its superiority to existing quasi-Newton methods. 
Our convergence analysis is based on the measure of local gradient norm~\cite{nesterov2018lectures}
\begin{align}
\label{eq:convergemeasure}
    \lambda(\x)\triangleq \sqrt{\nabla f(\x)^{\top}(\nabla^2f(\x))^{-1}\nabla f(\x)}.
\end{align}

The analysis starts from the following result for quasi-Newton iterations.

\begin{lemma}[{\cite[Lemma 4.3]{rodomanov2021greedy}}]
\label{lm:linear-quadra}
Suppose that the twice differentiable function $f:\BR^d\to\BR$ is strongly self-concordant with constant $M\geq0$ and the positive definite matrix $\G_t\in\BR^{d\times d}$ satisfies
\begin{align}
\label{eq:Gbound}
    \nabla^2 f(\x_t)\preceq \G_t\preceq \eta_t \nabla^2 f(\x_t)
\end{align}
for some $\eta_t\geq 1$ and $M\lambda(\x_t)\leq 2$.
Then the update formula 
\begin{align}\label{eq:iterbyG}
\x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t)    
\end{align}
holds that
\begin{align}
\label{eq:linear-quadra}
  \|\x_{t+1}-\x_t\|_{\x_t}\leq \lambda(\x_t)~~~\text{and}~~~  \lambda(\x_{t+1})\leq \left(1-\frac{1}{\eta_t}\right)\lambda(\x_t) + \frac{M(\lambda(\x_t))^2}{2}+\frac{M^2(\lambda (\x_t))^3}{4\eta_t}.
\end{align}
\end{lemma}

Note that the choice of $\eta_t$ in (\ref{eq:linear-quadra}) is crucial to achieve the convergence rate of the quasi-Newton methods.
Applying Lemma \ref{lm:linear-quadra} with $\eta_t = 3\eta_0/2$ and combining with Lemma~\ref{lm:sr1good}, we can establish the linear convergence rate of \srk~methods as follows.
 
\begin{theorem}\label{thm:srklinear}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:SRK} with initial $\vx_0\in\BR^d$ and $\mG_0\in\BR^{d\times d}$ such that
\begin{align*}
    M\lambda(\vx_0)\leq \frac{\ln(3/2)}{4\eta_0}
\qquad\text{and}\qquad
    \nabla^2 f(\x_0)\preceq \G_0\preceq \eta_0 \nabla^2f(\x_0)
\end{align*} 
for some $\eta_0\geq 1$. Then it holds that
\begin{align}
\label{eq:srklinear}
  \nabla^2 f(\x_{t})\preceq \G_t\preceq \frac{3\eta_0}{2}\nabla^2 f(\x_t)\qquad\text{and}\qquad\lambda(\x_t)\leq \left(1-\frac{1}{2\eta_0}\right)^t\lambda(\x_0).
\end{align}
\end{theorem}
\begin{proof}
We present the detailed proof in Appendix~\ref{sec:srklinear}. 
\end{proof}





Specifically, we can obtain the superlinear rate for iteration \eqref{eq:iterbyG} if there exists some sequence $\{\eta_t\}$ such that $\eta_t\geq 1$ for all $t$ and $\lim_{t\to\infty}\eta_t=1$.
For example, the randomized and greedy SR1 methods~\cite{lin2021greedy} correspond to some $\{\eta_t\}$ such that
\begin{align*}
    \EB[\eta_t-1]\leq  \OM \bigg(\bigg(1-\frac{1}{d}\bigg)^{t}\,\bigg).
\end{align*}

As the results shown in Theorem \ref{thm:matrix}, the proposed \srk~updates have the superiority in matrix approximation. 
So it is natural to construct some $\{\eta_t\}$ for \srk~methods (Algorithm \ref{alg:SRK}) such that
\begin{align*}
    \EB[\eta_t-1]\leq  \OM \bigg(\bigg(1-\frac{k}{d}\bigg)^{t}\,\bigg).
\end{align*}
Based on above intuition, we derive the local superlinear convergence rate for \srk~methods, which is explicitly sharper than existing randomized and greedy quasi-Newton methods~\cite{lin2021greedy,rodomanov2021greedy}.
% for the SR-$k$ method by incorporating the local linear-quadratic rate of the $\lambda(\x_t)$ and the linear convergence rate of matrix approximation provided in section~\ref{sec:update}.
\begin{theorem}
\label{thm:srk}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:SRK} with $k<d$ and set the initial point $\vx_0$ and the corresponding Hessian estimator $\mG_0$ such that
\begin{align}
\label{eq:initial}
   M \lambda(\x_0)\leq \frac{\ln 2}{2} \cdot \frac{(d-k)}{ \eta_0 d^2\varkappa}\qquad\text{and}\qquad\nabla^2 f(\x_0)\preceq\G_0\preceq \eta_0\nabla^2f(\x_0)
\end{align} 
for some $\eta_0\geq 1$. 
Then we have
\begin{align}
\label{eq:E_lambda_srk}
     \EBP{\frac{\lambda(\x_{t+1})}{\lambda(\x_t)}}\leq 2d\varkappa\eta_0\left(1-\frac{k}{d}\right)^{t},
\end{align}
which naturally indicates the following two stage convergence:
\begin{enumerate}[label=(\alph*),topsep=0.05cm, itemsep=0.1cm, leftmargin=0.6cm]
\item For \srk~method with randomized strategy, we have
    \begin{align*}
    \lambda(\x_{t_0+t})\leq\left(1-\frac{k}{d+k}\right)^{t(t-1)/2}\cdot\left(\frac{1}{2}\right)^t\cdot\left(1-\frac{1}{2\eta_0}\right)^{t_0}\lambda(\x_0),
\end{align*}
with probability at least $1-\delta$ for some $\delta\in(0,1)$, where $t_0=\OM(d\ln(\eta_0\varkappa d/\delta)/k)$. 
\item For \srk~method with greedy strategy, we have
    \begin{align*}
    \lambda(\x_{t_0+t})\leq\left(1-\frac{k}{d}\right)^{t(t-1)/2}\cdot\left(\frac{1}{2}\right)^t\cdot\left(1-\frac{1}{2\eta_0}\right)^{t_0}\lambda(\x_0),
\end{align*}
where $t_0=\OM(d\ln(\eta_0\varkappa d)/k)$.
\end{enumerate}
\end{theorem}
\begin{proof}
We present the detailed proof in Appendix~\ref{sec:srk_proof}. 
\end{proof}

\begin{remark}
The condition $\nabla^2 f(\x_0)\preceq\G_0\preceq \eta_0\nabla^2f(\x_0)$ in (\ref{eq:initial}) can be satisfied by simply setting $\mG_0=L\mI_d$, then we can efficiently implement the update on $\mG_t$ by Woodbury formula~\cite{woodbury1950inverting}.
In a follow-up work, \citet{liu2023block} analyzed block quasi-Newton methods for solving nonlinear equations. However, their convergence guarantees require the Jacobian estimator at the initial point be sufficiently accurate, which leads to potentially expensive cost for the step of initialization. 
\end{remark}

Additionally, we can also set $k=d$ for \srk~methods, which holds that $\eta_t=1$ almost surely for all $t\geq 1$. This results the quadratic convergence rate like standard Newton methods.
% \srk~methods with $k=d$ have the local quadratic convergence rate because we have $\eta_t=1$ almost surely for all $t\geq 1$.
\begin{corollary}
\label{cor:recoverNewton}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:SRK} with $k=d$ and set the initial point $\vx_0$ such that $M\lambda(\x_t)\leq 2$,
then we have $\lambda(\x_{t+1})\leq M(\lambda(\x_t))^2$ holds almost surely 
% \begin{align}
%   \lambda(\x_{t+1})\leq M(\lambda(\x_t))^2
% \end{align}
for all  $t\geq 1$. 
\end{corollary}
\begin{proof}
    The update rule of $\G_{t}=\srk(\tilde{\G}_{t-1},\nabla^2 f(\x_{t}),\U_{t-1})$ implies that
    \begin{align*}
     &\U_{t-1}^{\top}\G_{t}\U_{t-1} \\
     &= \U_{t-1}^{\top}\tilde{\G}_{t-1}\U_{t-1}-\U_{t-1}^{\top}(\tilde{\G}_{t-1}-\nabla^2 f(\x_{t}))\U_{t-1}\big(\U^{\top}(\tilde{\G}_{t-1}-\nabla^2 f(\x_{t}))\U_{t-1}\big)^{\dag}\U_{t-1}^{\top}(\tilde{\G}_{t-1}-\nabla^2 f(\x_{t}))\U_{t-1}\\
     &=\U_{t-1}^{\top}\nabla^2 f(\x_{t})\U_{t-1}.   
    \end{align*}
    The way we choose $\U_t\in\RB^{d\times d}$ means $\U_t$ is non-singular almost surely, so that $  \G_{t}=\nabla^2 f(\x_{t})$
    for all $t\geq 1$. Taking $\eta_{t}=1$ in Lemma~\ref{lm:linear-quadra}, we can directly obtain the quadratic convergence rate of \srk~method when choose $k=d$, that is
    \begin{align*}
        \lambda(\x_{t+1})\leq \frac{M(\lambda(\x_t))^2}{2} + \frac{M^2(\lambda(\x_t))^3}{2} \leq M(\lambda(\x_t))^2,
    \end{align*}
    holds for all $t\geq 1$ almost surely.
\end{proof}




%  \begin{proof}
% According to Theorem~\ref{thm:matrix}, we have
% \begin{align*}
%     \BE[\tau_{\nabla^2 f(\x_{t+1})}(\G_{t+1})]{=}0.
% \end{align*}
% According to Theorem~\ref{thm:srklinear} and Lemma~\ref{lm:GHneq} in Appendix~\ref{sec:srk_proof}, we have
% \begin{align}
% \label{eq:k=dlambdaneq}
%   \nabla^2f(\x_{t+1})\overset{\eqref{eq:srklinear}}{\preceq}  \G_{t+1}\preceq \underbrace{\left(1+\frac{d\varkappa\tau_{\nabla^2 f(\x_{t+1})}(\G_{t+1})}{\tr{\nabla^2 f(\x_{t+1})}}\right)}_{a_{t+1}}\nabla^2 f(\x_{t+1})~~~\text{and}~~~\lambda_{t+1}\leq \lambda_0,
% \end{align}
% where 
% \begin{align*}
%     \EB[a_{t+1}]=1+\frac{d\varkappa}{\tr{\nabla^2 f(\x_{t+1})}}\EBP{\tau_{\nabla^2 f(\x_{t+1})}(\G_{t+1})}=1.
% \end{align*}
% According to Lemma~\ref{lm:linear-quadra}, we have
% \begin{align}
% \label{eq:k=dlambda}
%     \lambda_{t+2}\leq \underbrace{\left(1-\frac{1}{a_{t+1}}\right)}_{b_{t+1}}\lambda_{t+1} + \frac{M\lambda_t^2}{2} + \frac{M^2\lambda_{t+1}^3}{4a_{t+1}}.
% \end{align}
% It also holds that
% \begin{align*}
%    0\leq \EB[b_{t+1}]=1 -\EB[1/a_{t+1}]{\leq} 1-1/\EB[a_{t+1}]= 0,
% \end{align*}
% where the first inequality comes from the fact that $a_{t+1}\geq 1$ and the second inequality comes from the fact 
% $\EB[1/X]\geq 1/\EB[X]$ for positive random variable $X>0$ by using Jensen's inequality.

% Thus, we have
% \begin{align*}
%     \EB[\lambda_{t+2}]\overset{\eqref{eq:k=dlambda}}{\leq} \frac{M\lambda_{t+1}^2}{2}+ \frac{M^2\lambda_{t+1}^3}{4}\overset{\eqref{eq:k=dlambdaneq}}{\leq}  \frac{M\lambda_{t+1}^2}{2}+ \frac{M\lambda_{t+1}^2}{2}\cdot\frac{M\lambda_{0}}{2}\overset{\eqref{eq:k=dinitial}}{\leq} M\lambda_{t+1}^2,
% \end{align*}
% for all $t\geq0$.
%   
% % \begin{align}
% %     \EBP{}
% % \end{align}
%  \end{proof}



\section{Improved Results for Block BFGS and DFP Methods}
% DFP
% Unified Broyden
% Greedy version of BFGS/DFP



\begin{figure}[tp]
	\vspace*{-\baselineskip}
\begin{minipage}[t]{.46\textwidth} 
\begin{algorithm}[H]
\caption{Randomized Block BFGS}\label{alg:bfgs-v1}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\x_0$, $\G_0$, and $k$. \\[0.26cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.26cm]
\STATE \quad
$\x_+=\x_t-\G_t^{-1}\nabla f(\x_t)$ \\[0.26cm]
\STATE \quad $\x_{t+1}=\argmin_{\vx\in\{\vx_t,\vx_+\}}f(\x)$\\[0.24cm]
\STATE \quad Construct $\U_t$ by $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim} {\fN(0,1)}$ \\[0.26cm]
\STATE \quad 
$\G_{t+1}= \bfgs(\G_t,\nabla^2f(\x_{t}),\U_t)$\\[0.24cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}
\end{minipage}
~~
\begin{minipage}[t]{.50\textwidth}
\begin{algorithm}[H]
\caption{Randomized Block BFGS (v2)/DFP}\label{alg:bfgs}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\x_0$, $\G_0$, $M$, and $k$. \\[0.03cm]
\STATE \textbf{for} $t=0,1\dots$\\[0.03cm]
\STATE \quad $\x_{t+1}=\x_t-\G_t^{-1}\nabla f(\x_t)$ \\[0.03cm]
\STATE \quad $r_t=\|\x_{t+1}-\x_{t}\|_{\x_t}$ \\[0.08cm]
\STATE \quad $\tilde{\G}_{t}=(1+Mr_t)\G_t$ \\[0.02cm]
\STATE \quad Construct $\U_t$ by $\left[\U_{t}\right]_{ij}\overset{\rm{i.i.d}}{\sim} {\fN(0,1)}$ \\[0.03cm]
\STATE \quad Update $\G_t$ by  \\[0.03cm]
\quad\quad  (a) $\G_{t+1}= \bfgs(\tilde{\G}_t,\nabla^2f(\x_{t+1}),\U_t)$ \\[0.03cm]
\quad\quad  (b) $\G_{t+1}= \dfp(\tilde{\G}_t,\nabla^2f(\x_{t+1}),\U_t)$ \\[0.03cm]
\STATE \textbf{end for}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}

\label{sec:Block BFGS}


Following our our investigation on \srk~methods, 
we can also achieve the non-asymptotic superlinear convergence rates of randomized block BFGS method \cite{gower2016stochastic,gower2017randomized} and randomized block DFP method.
%In this section, we provide the non-asymptotic superlinear convergence rates of randomized block BFGS method \cite{gower2016stochastic,gower2017randomized} and randomized block DFP method by following the idea of \srk.

The block BFGS update~\cite{schnabel1983quasi,gower2017randomized,gower2016stochastic} and block DFP update~\cite{gower2017randomized}~are defined as follows.
\begin{definition}[Block BFGS Update $\&$ Block DFP Update]
%[\citet{schnabel1983quasi,gower2016stochastic,gower2017randomized}]
Let $\A\in\RB^{d\times d}$ and $\G\in \RB^{d\times d}$ be two positive-definite symmetric matrices with $\A\preceq \G$. For any full rank matrix $\U\in\RB^{d\times k}$ with $k\leq d$, we define 
%{\color{red}quadratic convergence when $k=d$}
\begin{align}\label{update:RaBFGS}
    \bfgs(\G,\A,\U) \triangleq \G-\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G+\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\A,
\end{align}
and
\begin{align}
\label{eq:dfp-update}
\begin{split}
    {\rm \text{BlockDFP}}(\G,\A,\U)& \triangleq \A\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\\
  & ~~~~~+\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\G\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right).
  \end{split}
\end{align}
\end{definition}

\citet{gower2016stochastic,kovalev2020fast} proposed randomized block BFGS method (Algorithm~\ref{alg:bfgs-v1}) by constructing the Hessian estimator with formula (\ref{update:RaBFGS}) and showed it has asymptotic local superlinear convergence rate. 
On the other hand, \citet{gower2017randomized} considered the randomized block DFP update (\ref{eq:dfp-update}) for matrix approximation but did not study how to apply it to solve optimization problem.



To achieve the explicit superlinear convergence rate, we are required to provide some properties of block BFGS and DFP updates which are similar to the counterpart of \srk~update.
First, we observe that block BFGS and DFP updates also have non-increasing deviation from the target matrix.
\begin{lemma}
\label{lm:bfgsnofar}
For any positive-definite matrices $\mA\in\BR^{d\times d}$ and $\mG\in\BR^{d\times d}$ with 
$\A\preceq\G\preceq\eta \A$
% \begin{align}
% \label{eq:blockneq-condi}
% $\A\preceq\G\preceq\eta \A$
% \end{align}
for some~$\eta\geq 1$, both of updates $\G_{+}={ \text{\rm BlockBFGS}}(\G,\A,\U)$ and $\G_{+}={ \text{\rm BlockDFP}}(\G,\A,\U)$ for full rank matrix~$\U\in\RB^{d\times k}$ hold that
\begin{align}
\label{eq:blockneq}
   \A\preceq\G_{+}\preceq \eta\A. 
\end{align}
\end{lemma}
\begin{proof}
%\textbf{Block BFGS Update.}
We first consider $\G_{+}={ \text{\rm BlockBFGS}}(\G,\A,\U)$. 
According to Woodbury formula~\cite{woodbury1950inverting}, we have
\begin{align*}
    \G_{+}^{-1}&=\U(\U^{\top}\A\U)^{-1}\U^{\top}+\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\G^{-1}\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right).
\end{align*}
The condition $\A\preceq\G\preceq\eta \A$ means $\eta^{-1}\A^{-1}\preceq\G^{-1}\preceq \A^{-1}$,
% \begin{align}
% \label{eq:blockneq1}
%     \frac{1}{\eta}\A^{-1}\preceq\G^{-1}\preceq \A^{-1},
% \end{align}
which implies
\begin{align*}
    \G_{+}^{-1}&\overset{\eqref{eq:bfgsupdate}}{\preceq} \U(\U^{\top}\A\U)^{-1}\U^{\top}+\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\A^{-1}\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\preceq\A^{-1},
\end{align*}
and
\begin{align*}
\G_{+}^{-1}&\overset{\eqref{eq:bfgsupdate}}{\succeq}\U(\U^{\top}\A\U)^{-1}\U^{\top}+\frac{1}{\eta}\cdot\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\A^{-1}\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\\
    &~=~~\frac{1}{\eta}\A ^{-1}+\left(1-\frac{1}{\eta}\right)\U(\U^{\top}\A\U)^{-1}\U^{\top}\succeq \frac{1}{\eta}\A^{-1}.
\end{align*}
Thus, we have $ {\eta}^{-1}\A^{-1}\preceq\G^{-1}_{+}\preceq \A^{-1}$
which is equivalent to the desired result \eqref{eq:blockneq}.

We then consider $\G_{+}={\text{\rm BlockDFP}}(\G,\A,\U)$.
The condition $\A\preceq\G\preceq\eta \A$ means
\begin{align*}
\G_{+}\overset{\eqref{eq:dfp-update}}{\succeq} 
\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\A +\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\A\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)=\A,
\end{align*}
and
\begin{align*}
 \G_{+}&\overset{\eqref{eq:dfp-update}}{\preceq} 
\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\A +\eta\left(\I_d-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\right)\A\left(\I_d-\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\right)\\
&~~~~=~~~ \eta\A +(1-\eta)\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\A \preceq \eta\A,
\end{align*}
where the last inequality is due to the fact that $\eta\geq 1$ and $\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\A\succeq\0$.
 
\end{proof}


Then we introduce the quantity~\cite{rodomanov2021greedy}
\begin{align}
\label{eq:measurebfgs}
    \sigma_{\A}(\G)\triangleq\tr{\A^{-1}(\G-\A)},
\end{align}
to measure the difference between target matrix $\mA$ and the current estimator $\mG$. 
In the following theorem, we show that randomized block BFGS and DFP updates converge to the target matrix with a faster rate than the ordinary randomized BFGS and DFP updates~\cite{rodomanov2021greedy,lin2021greedy}.


\begin{theorem}\label{thm:bfgs}
Consider the randomized block update
\begin{align}
\label{eq:bfgsupdate}
    \G_{+}={\text{\rm BlockBFGS}}(\G,\A,\U)~~\text{or}~~~\G_{+}={\text{\rm BlockDFP}}(\G,\A,\U)
\end{align}
where $\G\succeq\A\in\RB^{d\times d}$. If $\mu\I_d\preceq\A\preceq L\I_d$ and  $\U\in\RB^{d\times k}$ is selected by sample each entry of $\mU$ according to~$\fN(0,1)$ independently. Then, we have
\begin{align}
\label{eq:bfgssigma}
  \EB\left[\sigma_{\A}(\G_{+})\right]\leq \left(1-\frac{k}{d\varkappa}\right)\sigma_\A(\G).
\end{align}
\end{theorem}

\begin{proof}
We first prove the fact that
\begin{align}
\label{eq:trineq2}
    \tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\geq \frac{1}{\varkappa}\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}},
\end{align}
holds for both block BFGS and block DFP updates.

The condition on $\A$ means we have
\begin{align}
    \label{eq:Aneq}
    \U^{\top}\A\U\succeq\frac{1}{L}\cdot\left(\U^{\top}\U\right)^{-1}~~~\text{and}~~~\frac{1}{\mu}\cdot\I_d-\A\succeq\0,
\end{align}
which leads to
\begin{align*}
\begin{split}           
    &\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\\
    &~=~\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}(\G-\A)\U\right)}\\
    &~=~ \tr{\left(\U^{\top}(\G-\A)\U\right)^{1/2}\left(\left(\U^{\top}\A\U\right)^{-1}-\frac{1}{L}\cdot (\U^{\top}\U)^{-1}\right)\left(\U^{\top}(\G-\A)\U\right)^{1/2}}\\
    &~~~~~~+\tr{\left(\U^{\top}(\G-\A)\U\right)^{1/2}\frac{1}{L}\cdot\left(\U^{\top}\U\right)^{-1}\left(\U^{\top}(\G-\A)\U\right)^{1/2}}\\
    &\overset{\eqref{eq:Aneq}}{\geq} \frac{1}{L}\tr{\left(\U^{\top}\U\right)^{-1}\left(\U^{\top}(\G-\A)\U\right)}\\
     &~=~\frac{1}{L}\tr{(\G-\A)^{1/2}\U\left(\U^{\top}\U\right)^{-1}\U^{\top}(\G-\A)^{1/2}}\\
     &~=~\frac{\mu}{L}\tr{\left(\frac{1}{\mu}\cdot\I_d-\A^{-1}\right)^{1/2}(\G-\A)^{1/2}\U\left(\U^{\top}\U\right)^{-1}\U^{\top}(\G-\A)^{1/2}\left(\frac{1}{\mu}\cdot\I_d-\A^{-1}\right)^{1/2}} \\
     &~~~~~+ \frac{\mu}{L}\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}\\
   & \overset{\eqref{eq:Aneq}}{\geq} \frac{1}{\varkappa}\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}.
\end{split}
\end{align*}

We then prove the desired bound for the block BFGS and block DFP respectively by incorporating their update rules respectively.
\begin{enumerate}[topsep=0.05cm, itemsep=0.1cm, leftmargin=0.6cm]
\item For the block BFGS update, we let $\P=\A^{1/2}\U$ and achieve
\begin{align*}
\begin{split}
  &\U^{\top}\G{\left(\A^{-1}-\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\right)}\G\U\\
  &= \U^{\top}\G\A^{-1/2}\left(\I_d -\A^{1/2}\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\A^{1/2}\right)\A^{-1/2}\G\U\\
    &= \U^{\top}\G\A^{-1/2}\underbrace{\left(\I_d-\P\left(\P^{\top}\P\right)^{-1}\P^{\top}\right)}_{\succeq \0}\A^{-1/2}\G\U
    \succeq \0,
\end{split}
\end{align*}
which implies
\begin{align}
\label{eq:trineq}
\begin{split}               &\tr{\left(\U^{\top}\G\U\right)^{-1}\left(\U^{\top}\G\A^{-1}\G\U\right)}-\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}\\    &=\tr{\left(\U^{\top}\G\U\right)^{-1}\left(\left(\U^{\top}\G\A^{-1}\G\U\right)-(\U^{\top}\G\U)\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)\right)}\\    &=\text{tr}\Big(\left(\U^{\top}\G\U\right)^{-1/2}\underbrace{\U^{\top}\G{\left(\A^{-1}-\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}\right)}\G\U}_{\succeq \0}\left(\U^{\top}\G\U\right)^{-1/2}\Big)
    \geq 0.
\end{split}
\end{align}
Using the update rule of $\G_{+}=\bfgs(\G,\A,\U)$ and the results of \eqref{eq:trineq}, we have
\begin{align*}
\tr{(\G_{+}-\A)\A^{-1}}  &\overset{\eqref{eq:bfgsupdate}}{=}\tr{(\G-\A)\A^{-1}}+\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\\
 &\overset{\eqref{eq:trineq}}{\leq} \tr{(\G-\A)\A^{-1}}- \left(\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right),
\end{align*}
Taking expectation in both sides, we obtain
\begin{align}
\label{eq:sigmaupdate}
\begin{split}
 \EBP{\sigma_{\A}(\G_{+})}
   &~\leq \sigma_{\A}(\G)-\EBP{\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}} \\
   &\overset{\eqref{eq:trineq2}}{\leq }\sigma_{\A}(\G) -\EBP{ \frac{1}{\varkappa}\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}}\\
   &~=\sigma_{\A}(\G)- \frac{k}{d\varkappa}\sigma_\A(\G),
\end{split}
\end{align}
where the last equality is by taking $\R=\A^{-1}(\G-\A)$ in Lemma~\ref{lm:explicitbound}(a). 
\item For the block DFP update, the scheme~\eqref{eq:dfp-update} means we have
\begin{align*}
    & \tr{ (\G_{+}-\A)\A^{-1}}\\
   &\overset{\eqref{eq:dfp-update}}{=} \tr{\A\U(\U^{\top}\A\U)^{-1}\U^{\top}}+\tr{(\G-\A)\A^{-1}}\\
   &~~~~~~~~+\tr{\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\G\U(\U^{\top}\A\U)^{-1}\U^{\top}-\A\U(\U^{\top}\A\U)^{-1}\U^{\top}\G-\G\U(\U^{\top}\A\U)^{-1}\U^{\top}}\\
   &~=~\tr{(\G-\A)\A^{-1}} - \left(\tr{(\U^{\top}\A\U)^{-1}(\U^{\top}\G\U)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right).
\end{align*}
We take the expectation on both sides of above equation, then we can prove the desired result by following the analysis of the block BFGS update from the step of~\eqref{eq:sigmaupdate}.  
\end{enumerate}
% Then we have
% \begin{align}
% \label{eq:trneq3}
% \begin{split}
%     &\EB\left[\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right]\\
%     &=~~~\EB\left[\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}\right] \\
%     &~~~~~~+ \EB\left[\tr{\left(\U^{\top}\A\U\right)^{-1}\left(\U^{\top}\G\U\right)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right]\\
%     &\!\!\!\!\overset{\eqref{eq:trineq},\eqref{eq:trineq2}}{\geq}\frac{\mu}{L}\EB\left[\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}\right]=\frac{k}{d\varkappa}\sigma_\A(\G),
% \end{split}
% \end{align}
% where the last equality is by taking $\R=\A^{-1}(\G-\A)$ in Lemma~\ref{lm:explicitbound} (a).
% Finally, we have
% \begin{align*}
%     &\EB[\sigma_\A(\G_{+})]\\
%    & \overset{\eqref{eq:sigmaupdate}}{=}\sigma_\A(\G)-\EB_\U\left[\tr{\G\U\left(\U^{\top}\G\U\right)^{-1}\U^{\top}\G\A^{-1}}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}\right]
%    \\
%    &\overset{\eqref{eq:trneq3}}{\leq} \left(1-\frac{k}{d\varkappa}\right)\sigma_\A(\G).
% \end{align*}

% which implies
% \begin{align*}
%     \EBP{\sigma_{\A}(\G_{+})} 
%     &\leq~ \sigma_{\A}(\G) - \EBP{\tr{(\U^{\top}\A\U)^{-1}(\U^{\top}\G\U)}-\tr{\A\U\left(\U^{\top}\A\U\right)^{-1}\U^{\top}}}\\
%     &\!\!\overset{\eqref{eq:trineq2}}{\leq}\sigma_{\A}(\G)-\frac{1}{\varkappa}\EBP{\tr{\A^{-1}(\G-\A)\U\left(\U^{\top}\U\right)^{-1}\U^{\top}}}\\
%     &=~\sigma_{\A}(\G)- \frac{k}{d\varkappa}\sigma_\A(\G).
% \end{align*}
\end{proof}
% \begin{remark}
% \label{rmk:BFGS}
%     For $k=d$, the update rules of block BFGS and block DFP imply that $\G_{+}\U=\A\U$ for $\U\in\RB^{d\times d}$. Since $\U$ is non-singular a.s, we have $\G_{+}=\A$ a.s. Thus, we have a sharper rate over \eqref{eq:bfgsupdate} such that
%     \begin{align*}
%         \EBP{\sigma_{\A}(\G_{+})} = 0.
%     \end{align*}
% \end{remark}
\begin{remark}
For $k=1$, the convergence rate of Theorem \ref{thm:bfgs} matches the results of \citet{lin2021greedy} for ordinary randomized BFGS and DFP updates. 
On the other hand, \citet{gower2017randomized} established the convergence of randomized block BFGS and DFP updates with respect to the measure of Frobenius norm, while rates cannot be sharper even if the value of $k$ is increased.   
\end{remark}


We proposed randomized block BFGS and DFP methods for minimizing strongly convex function in Algorithm~\ref{alg:bfgs}.
Based on the observation in Theorem \ref{thm:bfgs}, we establish the explicit superlinear convergence rate for these methods as follows.


\begin{theorem}
\label{thm:BFGS}
Under Assumption~\ref{ass:smooth}, \ref{ass:strongconvex} and \ref{ass:strongself}, we run Algorithm~\ref{alg:bfgs} with $k<d$ andd set the initial point $\vx_0\in\BR^d$ and the corresponding Hessian estimator $\mG_0\in\BR^{d\times d}$ such that
\begin{align}
\label{eq:bfgsini}
 \lambda(\x_0)\leq \frac{\ln 2}{4}\cdot \frac{1}{M \eta_0 d} 
\qquad\text{and}\qquad
\nabla^2 f(\x_0)\preceq\G_0\preceq \eta_0\nabla^2f(\x_0),
\end{align}
for some $\eta_0\geq 1$. 
Then we have
\begin{align*}
    \EBP{\frac{\lambda(\x_{t+1})}{\lambda(\x_t)}}\leq 2d\eta_0\left(1-\frac{k}{d\varkappa}\right)^{t}.
\end{align*}
\end{theorem}
\begin{proof}
We present the proof details in Appendix~\ref{sec:bfgs_proof}. 
\end{proof}
\begin{remark}
For $k=1$, Theorem \ref{thm:BFGS} matches the results of ordinary randomized BFGS and DFP methods~\cite{lin2021greedy}. For~$k=d$, we have $\G_{+}\U=\A\U$ for both $\G_{+}={ \text{\rm BlockBFGS}}(\G,\A,\U)$ and $\G_{+}={ \text{\rm BlockDFP}}(\G,\A,\U)$, which means the output of block BFGS and block DFP update is identical to the target matrix almost surely and Algorithm~\ref{alg:bfgs} can achieve the local quadratic convergence rate like standard Newton method.
\end{remark}





\section{Numerical Experiments}\label{sec:exp}
\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[scale=0.27]{Graph/MNIST.res.pdf} &
\includegraphics[scale=0.27]{Graph/sido0.res.pdf} &
\includegraphics[scale=0.27]{Graph/gisette.res.pdf}
\\[-0.1cm]
\small (a) MNIST (iteration) & \small  (b) sido0 (iteration) &\small (c) gisette (iteration) \\[0.2cm]
\includegraphics[scale=0.27]{Graph/MNIST.time.pdf} & 
\includegraphics[scale=0.27]{Graph/sido0.time.pdf} &
\includegraphics[scale=0.27]{Graph/gisette.time.pdf}
\\[-0.1cm]
\small  (d) MNIST (time) & \small  (e) sido0 (time) &\small (f) gisette (time)
\end{tabular}\vskip-0.15cm
\caption{We demonstrate ``\#iteration vs. $\|\nabla f(\x)\|$'' and ``running time (s) vs. $\|\nabla f(\x)\|$'' on datasets ``MNIST'', ``sido0'' and ``gisette'', where we take $k=200$ for all of the block quasi-Newton methods.}\label{fig:experiment-10}\vskip0.3cm
\end{figure}

\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[scale=0.27]{Graph_rasrk/MNIST.res.pdf} &
\includegraphics[scale=0.27]{Graph_rasrk/sido0.res.pdf} &
\includegraphics[scale=0.27]{Graph_rasrk/gisette.res.pdf}
\\[-0.1cm]
\small (a) MNIST (iteration) & \small  (b) sido0 (iteration) &\small (c) gisette (iteration) \\[0.2cm]
\includegraphics[scale=0.27]{Graph_rasrk/MNIST.time.pdf} & 
\includegraphics[scale=0.27]{Graph_rasrk/sido0.time.pdf} &
\includegraphics[scale=0.27]{Graph_rasrk/gisette.time.pdf}
\\[-0.1cm]
\small  (d) MNIST (time) & \small  (e) sido0 (time) &\small (f) gisette (time)
\end{tabular}\vskip-0.15cm
\caption{We demonstrate ``\#iteration vs. $\|\nabla f(\x)\|$'' and ``running time (s) vs. $\|\nabla f(\x)\|$'' on datasets ``MNIST'', ``sido0'' and ``Gisette'' with different $k=\{1,80,200,500,1000\}$ for Ra\srk.}\label{fig:diff_k_ra}\vskip0.3cm
\end{figure}


\begin{figure}[t]
\centering
\begin{tabular}{cccc}
\includegraphics[scale=0.27]{Graph_grsrk/MNIST.res.pdf} &
\includegraphics[scale=0.27]{Graph_grsrk/sido0.res.pdf} &
\includegraphics[scale=0.27]{Graph_grsrk/gisette.res.pdf}
\\[-0.1cm]
\small (a) MNIST (iteration) & \small  (b) sido0 (iteration) &\small (c) gisette (iteration) \\[0.2cm]
\includegraphics[scale=0.27]{Graph_grsrk/MNIST.time.pdf} & 
\includegraphics[scale=0.27]{Graph_grsrk/sido0.time.pdf} &
\includegraphics[scale=0.27]{Graph_grsrk/gisette.time.pdf}
\\[-0.1cm]
\small  (d) MNIST (time) & \small  (e) sido0 (time) &\small (f) gisette (time)
\end{tabular}\vskip-0.15cm
\caption{We demonstrate ``\#iteration vs. $\|\nabla f(\x)\|$'' and ``running time (s) vs. $\|\nabla f(\x)\|$'' on datasets ``MNIST'', ``Sido0'' and ``Gisette'' with different $k=\{1,80,200,500,1000\}$ for Gr\srk.}\label{fig:diff_k_gr}\vskip0.3cm
\end{figure}
We conduct the experiments on the model of regularized logistic regression, which can be formulated as 
\begin{align}
\label{prob:logstic}
    \min_{\vx\in\BR^d} f(\x)\triangleq \frac{1}{n}\sum_{i=1}^{n}\ln\left(1+\exp{-b_i\va_i^{\top}\x}\right)+\frac{\gamma}{2}\|\x\|^2,
\end{align}
where $\va_i\in\RB^d$ and $b_i\in\{-1,+1\}$ are the feature and the corresponding label of the $i$-th sample respectively, and $\gamma>0$ is the regularization hyperparameter.

We refer to \srk~methods (Algorithm~\ref{alg:SRK}) with randomized and greedy strategies as Ra\srk~and Gr\srk~respectively.
The corresponding SR1 methods with randomized and greedy strategies are referred as RaSR1 and GrSR1 (Algorithm~\ref{alg:SRK} with $k=1$ or Algorithm 4 of \citet{lin2021greedy}) respectively.
We also refer to the randomized block BFGS proposed by \citet{gower2016stochastic,gower2017randomized} (Algorithm~\ref{alg:bfgs-v1} \cite{gower2016stochastic,gower2017randomized}) as BlockBFGSv1 
and refer the new proposed Algorithm~\ref{alg:bfgs} with block BFGS and block DFP updates as BlockBFGSv2 and BlockDFP respectively. 
We compare the proposed Ra\srk, Gr\srk, BlockBFGSv2 and BlockDFP with baseline methods on problem~(\ref{prob:logstic}).
For all methods, We tune the parameters $\G_0$ and $M$ from~$\{\I_d, 10\cdot\I_d, 10^2\cdot\I_d, 10^3\cdot\I_d, 10^4\cdot\I_d\}$ and $\{1,10,100,1000,10000\}$ respectively.
We evaluate the performance for all of methods on three real-world datasets ``MNIST'', ``sido0'' and ``gisette''. 
We conduct our experiments on a PC with Apple M1 and implement all algorithms by Python 3.8.12.

We present the results of ``iteration numbers vs. gradient norm'' and ``running time (second) vs. gradient norm'' in Figure \ref{fig:experiment-10}, which corresponds to the settings of $k=200$ for block quasi-Newton methods Ra\srk, Gr\srk, BlockBFGSv1, BlockBFGSv2 and BlockDFP.
We observe that the proposed \srk~methods (Ra\srk~and Gr\srk) significantly outperform baselines. Besides, we also observe that BlockBFGS is faster than BlockDFP, which is similar to the advantage of the greedy BFGS method over the greedy DFP method~\cite{rodomanov2021greedy}.

We also test the performance of \srk~methods under the setting of $k\in\{1, 80, 200, 500, 1000\}$. We present the results of Ra\srk~and Gr\srk~in Figure \ref{fig:diff_k_ra} and \ref{fig:diff_k_gr} respectively. 
We observe that the larger $k$ leads to faster convergence in terms of the iterations ((a), (b) and (c) of Figure~\ref{fig:diff_k_ra} and \ref{fig:diff_k_gr}). 
In addition, we find that SR-$k$ methods with $k>1$ significantly outperform SR-$1$ methods in terms of the running time. 
This is because the block update can reduce cache miss rate and take the advantage of parallel computing.  
However, increasing~$k$ does not always result less running time because the acceleration caused by the block update is limited by the cache size. 
Therefore, there is a trade-off between the convergence rate and the running time per iteration in practice. 
In our experimental environment and candidates of $k$, the Ra\srk~method with $k=200$ has the best performance for ``MNIST'' and ``sido0'' ((d), (e) of Figure~\ref{fig:diff_k_ra}) and with $k=500$ has the best performance for ``gisette'' ((f) of Figure~\ref{fig:diff_k_ra}); the Gr\srk~method with $k=200$ has the best performance for ``MNIST'' ((d) of Figure~\ref{fig:diff_k_gr}) and with $k=500$ has the best performance  for ``sido0'' and ``gisette'' ((e), (f) of Figure~\ref{fig:diff_k_gr}).






\section{Conclusion}
\label{sec:conclusion}
In this paper, we have proposed symmetric rank-$k$ (\srk) methods for convex optimization.
We have proved \srk~methods enjoy the explicit local superlinear convergence rate of $\OM\left((1-k/d)^{t(t-1)/2}\right)$.
Our result successfully reveals the advantage of block-type updates in quasi-Newton methods, building a bridge between the theories of ordinary quasi-Newton methods and standard Newton method.
As a byproduct, we also provide the convergence rate of $\OM\left((1-k/(\varkappa d))^{t(t-1)/2}\right)$ for randomized block BFGS and randomized block DFP methods.

The application of \srk~methods is not limited to the convex optimization, one can also leverage the idea of~\srk~update to solve minimax problems~\cite{liu2022quasi, liu2022partial}. 
It is also possible to further accelerate the sharpened quasi-Newton methods~\cite{jin2022sharpened}.
In future work, it would be interesting to establish the global convergence results of \srk~methods by taking the recent advances of~\citet{jiang2023online} and study the limited memory variant of block quasi-Newton methods by using the displacement aggregation~\cite{berahas2022limited}.

 


\appendix

% \begin{align*}
%    \G_{+}= \G-\underbrace{(\G-\A)\U}_{\P}\underbrace{\left(\U^{\top}(\G-\A)\U\right)^{\dag}}_{\C}\underbrace{\U^{\top}(\G-\A)}_{\P^{\top}},
% \end{align*}
% which implies that
% \begin{align*}
%     \G_{+}^{-1} = \G^{-1}-\G^{-1}\P(\C+\P^{\top}\G\P)^{-1}\P^{\top}\G^{-1}
% \end{align*}
% \section{Auxiliary Lemmas}
% \label{sec:auli}
% \begin{lemma}
% \label{lm:trmulti}
% For any positive semi-definite matrices $\B,\P_1,\P_2\in\BR^{d\times d}$ such that $\P_1\succeq \P_2$, we have
% \begin{align}
%     \tr{\P_1\B}\geq\tr{\P_2\B}.
% \end{align}
% \end{lemma}
% \begin{proof}
% $\P_1-\P_2\succeq\0$ means
% \begin{align*}
%    (\P_1-\P_2)^{1/2}\B (\P_1-\P_2)^{1/2}\succeq\0,
% \end{align*}
% which implies 
% \begin{align*}
%  & \tr{\P_1\B}-\tr{\P_2\B} \\
%  = & \tr{(\P_1-\P_2)\B}\\
% =&\tr{(\P_1-\P_2)^{1/2}\B(\P_1-\P_2)^{1/2}}\\
% \geq & 0.
% \end{align*}
% % means 
% % \begin{align*}
% %     \tr{(\P_1-\P_2)^{1/2}\B(\P_1-\P_2)^{1/2}}\geq 0.
% % \end{align*}
% % So we have
% % \begin{align*}
% %  & \tr{\P_1\B}-\tr{\P_2\B} \\
% %  = & \tr{(\P_1-\P_2)\B}\\
% % =&\tr{(\P_1-\P_2)^{1/2}\B(\P_1-\P_2)^{1/2}}\\
% % \geq & 0.
% % \end{align*}
%  
% \end{proof}

% \begin{lemma}
% For positive semi-definite matrix $\S\in\RB^{d\times d}$ and the column orthonormal matrix $\Q\in\RB^{d\times k}$, we have
% \begin{align}
%     \label{eq:trace}
%     \tr{\Q^{\top}\S\Q}\leq \tr{\S}.
% \end{align}
% \end{lemma}
% \begin{proof}
% Since matrix $\Q$ is column orthonormal, we have
% \begin{align*}
%     \Q\Q^{\top}=\Q(\Q^{\top}\Q)^{-1}\Q^{\top}\preceq\I_d.
% \end{align*}
% According to Lemma~\ref{lm:trmulti}, we have
% \begin{align*}
%       \tr{\Q^{\top}\S\Q}=\tr{\S\Q\Q^{\top}}\leq \tr{\S}.
% \end{align*}
%  
% \end{proof}


% \begin{lemma}
% \label{lm:EP}
% Let $\U\in\RB^{d\times k}$ be a random matrix and each of its entry is independent and identically distributed according to $\fN(0,1)$, then it holds that
% \begin{align*}
%    \EB\left[\U(\U^{\top}\U)^{-1}\U^{\top}\right] = \frac{k}{d}\I_d.
% \end{align*}
% \end{lemma}
% \begin{proof}
% We use $\fV_{d,k}$ to present the Stiefel manifold which is the set of all $d\times k$ column orthogonal matrices.
% We denote $\fP_{k,d-k}$ as the set of all $m\times m$ orthogonal projection matrices idempotent of rank $k$.

% According to Theorem 2.2.1 (iii) of \citet{chikuse2003statistics}, the random matrix
% \begin{align*}
%     \Z=\U(\U^{\top}\U)^{-1/2}
% \end{align*}
% is uniformly distributed on the Stiefel manifold $\fV_{d,k}$.
% Applying Theorem 2.2.2 (iii) of \citet{chikuse2003statistics}, the random matrix
% \begin{align*}
%     \P=\Z\Z^\top=\U(\U^{\top}\U)^{-1}\U^{\top} 
% \end{align*}
% is uniformly distributed on $\fP_{k,d-k}$. 
% Combining above results with Theorem 2.2.2 (i) of \citet{chikuse2003statistics} on $\mP$ achieves
% \begin{align*}
%     \EB[\P]= \frac{k}{d}\I_d.
% \end{align*}
%  
% \end{proof}
% \begin{remark}
% The above proof requires the knowledge for statistics on manifold. 
% For the readers who are not familiar with this, we also present a elementary proof of Lemma~\ref{lm:EP} by induction in Appendix~\ref{appen:addiproof}.
% \end{remark}

%{\color{blue} Luo has finished editing for above paragraphs.}


\section{An Elementary Proof of Lemma~\ref{lm:explicitbound}(a)}
\label{appen:addiproof}
Before prove Lemma \ref{lm:explicitbound}(a), we first provide the following lemma.
\begin{lemma}\label{lem:1d_gauss}
Assume $\mP\in\BR^{d\times k}$ is column orthonormal $(k\le d)$ and  $\vv\sim\mathcal{N}_d(\vzero,\mP\mP^{\top})$ is a $d$-dimensional multivariate normal distributed vector, then we have 
\begin{align*}
\BE\left[\frac{\vv\vv^\top}{\vv^\top \vv}\right]=\frac{1}{k}\mP\mP^\top. 
\end{align*}
\end{lemma}
\begin{proof}
The distribution $\vv\sim\mathcal{N}_d(\vzero,\mP\mP^{\top})$ implies there exists a $k$-dimensional normal distributed vector~$\vw\sim\mathcal{N}_k(\vzero,\mI_k)$ such that $\vv=\mP\vw$. Thus, we have
\begin{align*}
\BE\left[\frac{\vv\vv^\top}{\vv^\top \vv}\right] 
= & \BE\left[\frac{(\mP\vw)(\mP\vw)^\top}{(\mP\vw)^\top (\mP\vw)}\right] 
=  \BE\left[\frac{\mP\vw\vw^\top\mP^\top}{\vw^\top\mP^\top\mP\vw}\right] \\
= & \mP\BE\left[\frac{\vw\vw^\top}{\vw^\top\vw}\right]\mP^\top 
 = \frac{1}{k}\mP\mP^\top,
\end{align*}
where the last step is because of $\vw/\norm{\vw}$ is uniform distributed on $k$-dimensional unit sphere and its covariance matrix is~$k^{-1}\mI_k$.
 
\end{proof}

Now we prove statement (a) of Lemma~\ref{lm:explicitbound}.
\begin{proof}    
We prove this result by induction on $k$. We only need to prove equation \eqref{eq:EP}.


The induction base $k=1$ have been verified by Lemma~\ref{lem:1d_gauss}. 
Now we assume 
\begin{align*}
\BE\left[\mU(\mU^\top\mU)^{-1}\mU^\top\right]=\frac{k}{d}\mI_d 
\end{align*}
holds for any $\mU\in\BR^{d\times k}$ that each of its entries are independently distributed according to $\fN(0,1)$. 
We define the random matrix 
\begin{align*}
\hat{\mU} = \begin{bmatrix}
\mU & \vv 
\end{bmatrix}\in\BR^{d\times(k+1)},
\end{align*}
where $\vv\sim\fN_d(\vzero,\mI_d)$ is independent distributed to $\mU$. We define $\B=\mU(\mU^\top\mU)^{-1}\mU^\top$, then we use block matrix inversion formula to compute $(\hat{\mU}^\top\hat{\mU})^{-1}$:
\begin{align*}
  &(\hat{\mU}^\top\hat{\mU})^{-1} \\
  &=   \left(\begin{bmatrix}\mU^\top \\ \vv^\top \end{bmatrix} \begin{bmatrix}\mU & \vv \end{bmatrix} \right)^{-1} = \left(\begin{bmatrix}
      \U^{\top}\U&\U^{\top}\v\\
      \v^{\top}\U&\v^{\top}\v
  \end{bmatrix}\right)^{-1}\\
  &=\begin{bmatrix}
   (\U^{\top}\U)^{-1}+(\U^{\top}\U)^{-1}\U^{\top}\v(\v^{\top}(\I-\B)\v)^{-1}\v^{\top}\U(\U^{\top}\U)^{-1}   &~ -(\U^{\top}\U)^{-1}\U^{\top}\v(\v^{\top}(\I-\B)\v)^{-1}\\
   -(\v^{\top}(\I-\B)\v)^{-1}\v^{\top}\U(\U^{\top}\U)^{-1}  &~ (\v^{\top}(\I-\B)\v)^{-1}
  \end{bmatrix}.
\end{align*}
Thus, we have
\begin{align*}
     & \hat{\mU}(\hat{\mU}^\top\hat{\mU})^{-1}\hat{\mU}^\top\\
     &= \begin{bmatrix}\mU & \vv \end{bmatrix}\left(\begin{bmatrix}\mU^\top \\ \vv^\top \end{bmatrix} \begin{bmatrix}\mU & \vv \end{bmatrix} \right)^{-1} \begin{bmatrix}\mU^\top \\ \vv^\top \end{bmatrix} \\
     &= \B+\frac{\B\v\v^{\top}\B}{\v^{\top}(\I-\B)\v}-\frac{\B\v\v^{\top}}{\v^{\top}(\I-\B)\v}-\frac{\v\v^{\top}\B}{\v^{\top}(\I-\B)\v}+\frac{\v\v^{\top}}{\v^{\top}(\I-\B)\v}\\
     &=\B+\frac{(\mI_d-\B)\vv\vv^\top(\mI_d-\B)}{\vv^\top(\mI_d-\B)\vv}.
\end{align*}


% and compute $\hat{\mU}(\hat{\mU}^\top\hat{\mU})^{-1}\hat{\mU}^\top$ by using block matrix inversion formula and Woodbury matrix identity:
% \begin{align*}
%     & \hat{\mU}(\hat{\mU}^\top\hat{\mU})^{-1}\hat{\mU}^\top \\
%     &=  \begin{bmatrix}\mU & \vv \end{bmatrix}\left(\begin{bmatrix}\mU^\top \\ \vv^\top \end{bmatrix} \begin{bmatrix}\mU & \vv \end{bmatrix} \right)^{-1} \begin{bmatrix}\mU^\top \\ \vv^\top \end{bmatrix}
%     &= 
%     %\\
%    % = & \mA+\frac{(\mI_d-\mA)\vv\vv^\top(\mI_d-\mA)}{\vv^\top(\mI_d-\mA)\vv},
% \end{align*}
% where $\mA=\mU(\mU^\top\mU)^{-1}\mU^\top$.


Since the rank of projection matrix $\mI_d-\B$ is $d-k$, we can write $\mI_d-\B=\mQ\mQ^\top$ for some column orthonormal matrix $\mQ\in\BR^{d\times (d-k)}$. 
Thus, we achieve
\begin{align*}
 & \BE[\hat{\mU}(\hat{\mU}^\top\hat{\mU})\hat{\mU}^\top] \\
=&\frac{k}{d}\mI_d +\BE_{\mU}\left[\BE_{\vv}\left[\frac{(\mI_d-\B)\vv\vv^\top(\mI_d-\B)}{\vv^\top(\mI_d-\B)\vv}\,\Bigg|\,\mU\right]\right]\\
=&\frac{k}{d}\mI_d +\BE_{\mU}\left[\BE_{\vv}\left[\frac{(\mQ\mQ^\top\vv)(\vv^\top\mQ\mQ^\top)}{(\vv^\top\mQ\mQ^\top)(\mQ\mQ^\top\vv)}\,\Bigg|\,\mU\right]\right]\\
=&\frac{k}{d}\mI_d +\frac{1}{d-k}\BE_{\mU}[\mQ\mQ^\top]\\
=&\frac{k}{d}\mI_d +\frac{1}{d-k}\BE_{\mU}[\mI_d-\B]\\
=&\frac{k}{d}\mI_d +\frac{1}{d-k}\left(\mI_d-\frac{k}{d}\mI_d\right) \\
=&\frac{k+1}{d}\mI_d,
\end{align*}
which completes the induction. In above derivation, the second equality is due to Lemma \ref{lem:1d_gauss} and the fact~$\mQ\mQ^{\top}\vv\sim\mathcal{N}_d(\vzero,\mQ\mQ^{\top})$; the third equality comes from the inductive hypothesis.
 
\end{proof}


\section{Auxiliary Lemmas for Non-negative Sequences}
We provide the following lemmas on non-negative sequences, which will be useful to obtain the explicit convergence rates of the proposed block quasi-Newton methods.

\begin{lemma}
\label{lm:superlinear}
Let $\{\lambda_t\}$ and $\{\delta_t\}$ be two non-negative random sequences that satisfy
\begin{align}
\label{eq:supercondi}
\begin{split}
  &\lambda_{t+1}\leq (1+c_1\lambda_t)^2(\delta_t+c_2\lambda_t)\lambda_t,~~~\BE_{t}\left[\delta_{t+1}\right]\leq \left(1-\frac{1}{\alpha}\right)(1+m\lambda_t)^2(\delta_t+c_3\lambda_t),
  \\
  &\delta_0+c\lambda_0 \leq s,~~~\text{and}~~~ \lambda_{t}\leq \left(1-\frac{1}{\beta}\right)^t\lambda_0,
  \end{split}
\end{align}
% and 
% \begin{align}
% \label{eq:supercondi2}
%     \BE_{t}\left[\delta_{t+1}\right]\leq \left(1-\frac{1}{\alpha}\right)(1+m\lambda_t)^2(\delta_t+c\lambda_t),
% \end{align}
{for some $c_1, c_2, c_3\geq 0$ with $c=\max\{c_2,c_3\}$}, $s\geq 0$, $\alpha>1$, and $\beta\in (0,1)$, where $\EB_{t}[\,\cdot\,]\triangleq \EB[\,\cdot\,|\delta_0,\cdots,\delta_{t},\lambda_0,\cdots,\lambda_{t}]$. 
If $\lambda_0$ is sufficient small such that 
\begin{align}
\label{eq:supercondiini}
    \lambda_0\leq \frac{\ln 2}{\beta (2c_1+c(\alpha/(\alpha-1)))}
\end{align}
then it holds that
\begin{align*}
    \BE\left[\frac{\lambda_{t+1}}{\lambda_t}\right]\leq 2\left(1-\frac{1}{\alpha}\right)^{t}s.
\end{align*}
\end{lemma}

\begin{proof}
We denote
\begin{align}
\label{eq:thetadef}
    \theta_t\triangleq \delta_t+c \lambda_t.
\end{align}
Since the index $t+1\geq 1$, we have
\begin{align}
\label{eq:lambdat_leq_thetat}
    \EB_{t}[\delta_{t+1}] \overset{\eqref{eq:supercondi}}{\leq}\left(1-\frac{1}{\alpha}\right)(1+c_1\lambda_t)^2(\delta_t+c\lambda_t)\leq \left(1-\frac{1}{\alpha}\right)\theta_t\exp{2c_1\lambda_t}
    \quad\text{and}\quad
    \lambda_{t+1} \overset{\eqref{eq:supercondi}}{\leq} \theta_t\lambda_t\exp{2c_1\lambda_t}.
\end{align}
We denote $\tilde{c}\triangleq2c_1+c\alpha/(\alpha-1)$, then it holds that
\begin{align}
\label{eq:thetat+1leqs}
\begin{split}
    \EB_{t}[\theta_{t+1}]&\overset{\eqref{eq:thetadef}}{\leq} \left(1-\frac{1}{\alpha}\right)\cdot\left(1+\frac{\alpha c\lambda_t}{\alpha-1}\right)\theta_t \exp{2c_1\lambda_t}\leq\left(1-\frac{1}{\alpha}\right)\theta_t\exp{(2c_1+c\alpha/(\alpha-1))\lambda_t}\\
    &=\left(1-\frac{1}{\alpha}\right)\theta_t\exp{\tilde{c}\lambda_t}
    \overset{\eqref{eq:supercondi}}{\leq} \left(1-\frac{1}{\alpha}\right)\theta_t\exp{\tilde{c}(1-{1}/{\beta})^t\lambda_0}.
    \end{split}
\end{align}
Taking expectation on both sides of \eqref{eq:thetat+1leqs}, we have
\begin{align}
\label{eq:expthetat}
    \EB[\theta_{t+1}]\leq \left(1-\frac{1}{\alpha}\right)\exp{{\tilde{c}}(1-{1}/{\beta})^t\lambda_0}\EB[\theta_t],
\end{align}
where we use the fact $\EB[\EB_{t}[\delta_{t+1}]] = \EB[\delta_{t+1}]$.
Therefore, we have
\begin{align*}    
 \BE\left[\frac{\lambda_{t+1}}{\lambda_t}\right]
&\overset{\eqref{eq:lambdat_leq_thetat}}{\leq} \BE[\theta_{t}\exp{2c_1\lambda_t}] 
{\leq}  \left(1-\frac{1}{\alpha}\right)\EBP{\theta_{t}}\exp{\tilde{c}(1-{1}/{\beta})^t\lambda_0}\\
&\overset{\eqref{eq:expthetat}}{\leq}  \left(1-\frac{1}{\alpha}\right)^2\EBP{\theta_{t-1}}\exp{(\tilde{c}(1-{1}/{\beta})^t+\tilde{c}(1-{1}/{\beta})^{t-1})\lambda_0}\\
&\overset{\eqref{eq:expthetat}}{\leq}\left(1-\frac{1}{\alpha}\right)^{t} \EBP{\theta_0}\exp{\tilde{c}\sum_{p=0}^{t}(1-{1}/{\beta})^p\lambda_0}\\
&~~{\leq}~~  \left(1-\frac{1}{\alpha}\right)^t \EBP{\theta_0}\exp{\tilde{c}\beta\lambda_0}\overset{\eqref{eq:supercondiini},\eqref{eq:supercondi}}{\leq} \left(1-\frac{1}{\alpha}\right)^t 2s.
\end{align*}
 
\end{proof}


\begin{lemma}
\label{lm:linear}
Let~$\{\lambda_t\}$ and $\{\tilde{\eta}_t\}$ be two positive sequences where $\tilde{\eta}_t\geq 1$ that satisfy
\begin{align}
\label{eq:lambda_t_1}
\begin{split}
    &\lambda_{t+1}\leq \left(1-\frac{1}{\tilde{\eta}_t}\right)\lambda_t + \frac{m_1\lambda_t^2 }{2} + \frac{m_1^2\lambda_t^3}{4\tilde{\eta}_t}~~~\text{for}~~~{m_1\lambda_t\leq 2},\\
    &\tilde{\eta}_{t+1}\leq (1+m_2\lambda_t)^2\tilde{\eta}_t,
\end{split}
\end{align}
for some $m_1$ and $m_2>0$.
If
\begin{align}
    \label{eq:linear_initial}
    m\lambda_0\leq \frac{\ln ({3}/{2})}{4\tilde{\eta}_0},
\end{align}
where $m\triangleq\max\{m_1,m_2\}$,
then it holds that
\begin{align}\label{eq:tilde_eta}
    \tilde{\eta}_t\leq\tilde{\eta}_0 \exp{2m\sum_{i=0}^{t-1}\lambda_t}\leq \frac{3\tilde{\eta}_0}{2}
\end{align}
and
\begin{align}\label{eq:lambda_t_linear}
\lambda_t&\leq\left(1-\frac{1}{2\tilde{\eta}_0}\right)^t\lambda_0.
\end{align}
\end{lemma}
\begin{proof}
%see \citet{Lin2021greedy} Theorem 23
Our analysis follows the proof Theorem 4.7 of \citet{rodomanov2021greedy} and Theorem 23 of \citet{lin2021greedy}.
We prove results of~\eqref{eq:tilde_eta} and \eqref{eq:lambda_t_linear} by induction. 
In the case of $t=0$, inequalities \eqref{eq:tilde_eta} and \eqref{eq:lambda_t_linear} are satisfied naturally. 
Now we suppose inequalities~\eqref{eq:tilde_eta} and \eqref{eq:lambda_t_linear} holds for $t=0,\dots,\hat{t}$, then we have
\begin{align}    m\sum_{i=0}^{\hat{t}}\lambda_i\overset{\eqref{eq:lambda_t_linear}}{\leq} m\lambda_0\sum_{i=0}^{\hat{t}}\left(1-\frac{1}{2\tilde{\eta}_0}\right)^{i} \leq 2\tilde{\eta}_0 m \lambda_0 \overset{\eqref{eq:linear_initial}}{\leq} 1.
\end{align}
In the case of $t=\hat{t}+1$ we have
\begin{align}
\label{eq:simple_induct}
   \frac{ 1-{m_1\lambda_{\hat{t}}}/{2}}{\tilde{\eta}_{\hat{t}}}{\geq} \frac{\exp{-m_1\lambda_{\hat{t}}}}{\tilde{\eta}_{\hat{t}}}\overset{\eqref{eq:tilde_eta}}{\geq} \frac{\exp{-2m\sum_{i=0}^{t}\lambda_i}}{\tilde{\eta}_0}\geq \frac{2}{3\tilde{\eta}_0}\qquad\text{and}\qquad m\lambda_{\hat{t}}\leq m\lambda_0\overset{\eqref{eq:linear_initial}}{\leq} \frac{1}{8\tilde{\eta}_0}.
\end{align}
According to condition \eqref{eq:lambda_t_1}, we have
\begin{align*}
    \lambda_{\hat{t}+1}&\overset{\eqref{eq:lambda_t_1}}{\leq} \left(1+\frac{m_1\lambda_{\hat{t}}}{2}\right)\cdot\left(1-\frac{1-{m_1\lambda_{\hat{t}}}/{2}}{\tilde{\eta}_{\hat{t}}}\right)\lambda_{\hat{t}}\overset{\eqref{eq:simple_induct}}{\leq}\left(1-\frac{1}{2\tilde{\eta}_0}\right)\lambda_{\hat{t}}\leq \left(1-\frac{1}{2\tilde{\eta}_0}\right)^{{\hat{t}}+1}\lambda_0,
\end{align*}
where the last step is based on induction.
We also have
\begin{align*}
   \tilde{ \eta}_{\hat{t}+1}\overset{\eqref{eq:thetat+1leqs}}{\leq}(1+m_2\lambda_{\hat{t}})^2\tilde{\eta}_t\leq \tilde{\eta}_{\hat{t}} \exp{2m\lambda_{\hat{t}}}\overset{\eqref{eq:tilde_eta}}{\leq} \tilde{\eta}_0\exp{2m\sum_{i=0}^{\hat{t}}\lambda_{\hat{t}}}\overset{\eqref{eq:linear_initial}}{\leq } \frac{3\tilde{\eta}_0}{2}.
\end{align*}
 
\end{proof}

\input{proof}




\section{The Proof Details of Theorem~\ref{thm:BFGS}}
\label{sec:bfgs_proof}
We first provide a lemma which shows the linear convergence for the proposed block BFGS and DFP method (Algorithm~\ref{alg:bfgs}).
\begin{lemma}
\label{lm:BFGSlinear}
Under the setting of Theorem~\ref{thm:BFGS},  Algorithm~\ref{alg:bfgs} holds that 
\begin{align}
\label{eq:BFGSlinear}
    \lambda(\vx_t)\leq \left(1-\frac{1}{2\eta_0}\right)^t\lambda(\vx_0)~~~\text{and}~~~ \nabla^2 f(\x_t)\preceq \G_t\preceq \frac{3\eta_0}{2}\nabla^2 f(\x_t)
\end{align}
for all $t\geq 0$.
\end{lemma}

\begin{proof}
We can obtain this result by following the proof of Theorem~\ref{thm:srklinear} by replacing the steps of using Lemma~\ref{lm:sr1good} with using Lemma~\ref{lm:bfgsnofar}. 
\end{proof}

We then provide some auxiliary lemmas from \citet{rodomanov2021greedy,lin2021greedy}.
\begin{lemma}[{\cite[Lemma 4.8.]{rodomanov2021greedy}}]
\label{lm:strong_self_bfgs}
If the twice differentiable function $f:\BR^d\to\BR$ is $M$-strongly self-concordant and $\mu$-strongly convex and the positive definite matrix $\G\in\RB^{d\times d}$ and $\vx\in\BR^d$ satisfy $\nabla^2 f(\x)\preceq \G\preceq \eta\nabla^2 f(\x)$ for some $\eta>1$, then we have
\begin{align}
 \sigma_{\nabla^2f(\x_{+})}(\tilde{\G})\leq (1+Mr)^2 (\sigma_{\nabla^2f(\x)}({\G})+2dMr),   \label{eq:delta_BFGS_lin}
\end{align}
for any $\vx,\vx_{+}\in\BR^d$ where $\tilde{\G}=(1+Mr)\G$, $r=\|\x-\x_{+}\|_{\x}$ and $\sigma_\H(\G)$ follow the expression of \eqref{eq:measurebfgs}.
\end{lemma}

\begin{lemma}\label{lm:GHneq_bfgs}
For any positive definite symmetric matrices $\G,\H\in\RB^{d\times d}$ such that $\H\preceq\G$, 
it holds that
\begin{align}
\label{eq:GHneq_bfgs}
\G\preceq (1+\sigma_{\H}(\G))\H.
\end{align}
If it further holds that $\G\preceq\eta\H$, then we have
\begin{align}
\label{eq:GHneq_2_bfgs}
  \sigma_{\H}(\G)\leq d(\eta -1)
\end{align}
where $\hat\varkappa$ is the condition number of $\H$ and the notation of $\sigma_\H(\G)$ follow the expression of \eqref{eq:measurebfgs}.
\end{lemma}
\begin{proof}
The inequality \eqref{eq:GHneq_bfgs} is directly obtained by following the statement 1) of Equation (42) by \citet[Lemma 25]{lin2021greedy}.
The inequality \eqref{eq:GHneq_2_bfgs} is obtained by the definition of $\sigma_{\H}(\G)$, that is
\begin{align*}
      \sigma_{\H}(\G)=\tr{\H^{-1}(\G-\H)}=\tr{\H^{-1/2}(\G-\H)\H^{-1/2}}\leq \tr{(\eta-1)\H^{-1/2}\H\H^{-1/2}} = (\eta-1)d.
\end{align*}
 
\end{proof}


Now we are ready to present the proof for Theorem~\ref{thm:BFGS}.
\begin{proof}
We denote $\delta_t\triangleq\tr{(\G_{t}-\nabla^2 f(\x_t))(\nabla^2 f(\x_t))^{-1}}$ and $\lambda_t\triangleq \lambda(\x_t)$. The initial condition means we have the results of Lemma~\ref{lm:BFGSlinear}. 
According to Lemma~\ref{lm:GHneq}, we have 
\begin{align*}
   \nabla^2 f(\x_t)\overset{\eqref{eq:BFGSlinear}}{\preceq} \G_t\preceq (1+\delta_t) \nabla^2 f(\x_t).
\end{align*}
Using Theorem~\ref{thm:bfgs}, we have
\begin{align}
\label{eq:BFGSconverge1}
    \EB_t[\delta_{t+1}] = \EB_t[\sigma_{\nabla^2 f(\x_{t+1})}(\G_{t+1})] \overset{\eqref{eq:bfgssigma}}{\leq} \left(1-\frac{k}{d\varkappa}\right)\sigma_{\nabla^2 f(\x_{k+1})}(\tilde{\G}_t).
\end{align}
Using Lemma~\ref{lm:strong_self_bfgs}, we have
\begin{align}
\label{eq:BFGSconverge2}
  \sigma_{\nabla^2 f(\x_{k+1})}(\tilde{\G}_t) \overset{\eqref{eq:delta_BFGS_lin}}{\leq}(1+Mr_t)^2(\delta_t+2dMr_t).
\end{align}
Thus, we can obtain following result
\begin{align*}
    \EB_{t}[\delta_{t+1}]\overset{\eqref{eq:linear-quadra}\,\eqref{eq:BFGSconverge1}\,\eqref{eq:BFGSconverge2}}{\leq} \left(1-\frac{k}{d\varkappa}\right)(1+M\lambda_t)^2(\delta_t+2dM\lambda_t).
\end{align*}
According to Lemma~\ref{lm:linear-quadra}, we have
\begin{align*}
    \lambda_{t+1}\overset{\eqref{eq:linear-quadra}}{\leq}\left(1+\frac{M\lambda_t}{2}\right)\cdot\frac{\delta_t+{M\lambda_t}/{2}}{1+\delta_t}\lambda_t\leq (1+M\lambda_t)^2(\delta_t+2dM\lambda_t)\lambda_t.
\end{align*}
According to Lemma~\ref{lm:BFGSlinear}, we have
\begin{align*}
    \lambda_{t}\leq \left(1-\frac{1}{2\eta_0}\right)^{t}\lambda_0.
\end{align*}
According to Lemma~\ref{lm:GHneq_bfgs} and the initial condition (\ref{eq:bfgsini}), we have 
\begin{align*}
    \delta_0=\tr{(\G_0-\nabla^2f(\x_0))(\nabla^2 f(\x_0))^{-1}}\overset{\eqref{eq:GHneq_2_bfgs}}{\leq} d(\eta_0-1)~~~\text{and}~~~\theta_0 = \delta_0+2dM\lambda_0\overset{\eqref{eq:bfgsini}}{\leq} d\eta_0.
\end{align*}
Hence, the random sequences of $\{\lambda_t\}$ and $\{\delta_t\}$ satisfy the conditions of Lemma~\ref{lm:superlinear} with
\begin{align*}
c_1=M,\quad c_2=2 dM,\quad c_3=2 dM,\quad
\alpha=\frac{d\varkappa}{k},\quad
\beta={2\eta_0}\quad\text{and}\quad s=\eta_0 d,
\end{align*}
which means we have proved Theorem~\ref{thm:BFGS}.
 
\end{proof}


%\input{nonlinear}

\bibliographystyle{plainnat}
\bibliography{ref}
 
\end{document}


