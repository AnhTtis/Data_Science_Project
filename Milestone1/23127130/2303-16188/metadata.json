{
    "arxiv_id": "2303.16188",
    "paper_title": "Symmetric Rank-$k$ Methods",
    "authors": [
        "Chengchang Liu",
        "Cheng Chen",
        "Luo Luo"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2024-02-02"
    ],
    "latest_version": 5,
    "categories": [
        "math.OC"
    ],
    "abstract": "This paper proposes a novel class of block quasi-Newton methods for convex optimization which we call symmetric rank-$k$ (SR-$k$) methods. Each iteration of SR-$k$ incorporates the curvature information with $k$ Hessian-vector products achieved from the greedy or random strategy. We prove SR-$k$ methods have the local superlinear convergence rate of $\\mathcal{O}\\big((1-k/d)^{t(t-1)/2}\\big)$ for minimizing smooth and strongly self-concordant function, where $d$ is the problem dimension and $t$ is the iteration counter. This is the first explicit superlinear convergence rate for block quasi-Newton methods and it successfully explains why block quasi-Newton methods converge faster than standard quasi-Newton methods in practice.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16188v1",
        "http://arxiv.org/pdf/2303.16188v2",
        "http://arxiv.org/pdf/2303.16188v3",
        "http://arxiv.org/pdf/2303.16188v4",
        "http://arxiv.org/pdf/2303.16188v5"
    ],
    "publication_venue": null
}