\appendices

\section{Proof of Theorem 1}
We provide the proof details of our main result, presented in Theorem~\ref{th:Snorm}. The main procedure follows the seminar work\cite{halko2011finding} and we note that the challenge lies in Lemma.~\ref{lemma:proj_upper} which we believe is new and different from results in\cite{li2010making,kumar2012sampling}.

\subsection{Preliminaries and Existing Results}
For the sake of completeness, we present the definitions and existing results used in the proof.

\begin{definition}
(\textbf{Orthogonal Projector}, \cite{drineas2005nystrom}). 
A matrix $\mathbf{P}$ is called an orthogonal projector if $\mathbf{P}=\mathbf{P}^\top=\mathbf{P}^2$.
\end{definition}

\begin{lemma}\label{lemma:stewart}
As shown in \cite{stewart1990matrix}, the matrices $\mathbf{G}\in\mathbb{R}^{n\times{n}}$ and $\mathbf{F}\in\mathbb{R}^{n\times{n}}$ satisfy, 
\begin{eqnarray}
\max_{1\le{i}\le{n}} 
    \left| \sigma_i(\mathbf{G}) - \sigma_i(\mathbf{F}) \right|
\le& \parallel \mathbf{G} - \mathbf{F} \parallel_2 \\
\sum_{i=1}^n 
    \Big(
        \sigma_i(\mathbf{G}) - \sigma_i(\mathbf{F})
        \Big)^2
\le& \parallel \mathbf{G} - \mathbf{F} \parallel^2_F. 
\end{eqnarray}
\end{lemma}

\begin{theorem}\label{th:mli}
(\textbf{Proposition 1, \cite{halko2011finding}}).
Given a real $l$-by-$l$ matrix $\mathbf{A}$ with eigenvalues $\sigma_{1}\geq\ldots\geq\sigma_{l}$, choose a target rank $k$ and an oversampling parameter $p \geq 2,$ where $k+p\leq{l}$. Draw an $l$-by-$(k+p)$ standard Gaussian matrix $\Omega$, then construct the sample matrix $\mathbf{A}^q\mathbf{\Omega}$ where $q\ge{1}$. The orthonormal basis $\mathbf{Q}$ of matrix $\mathbf{A}^q\mathbf{\Omega}$ (i.e., $\mathbf{A}^q\mathbf{\Omega}=\mathbf{QQ}^\top\mathbf{A}^q\mathbf{\Omega}$) satisfies
\begin{flalign}
\mathbb{E}\parallel(\mathbf{I}-\mathbf{QQ}^\top)\mathbf{A}\parallel_{2} \leq \zeta^{1/q}\sigma_{k+1}(\mathbf{A}),
\end{flalign}where $\zeta=1+\sqrt{\frac{k}{p-1}} + \frac{e\sqrt{k+p}}{p}\sqrt{l-k}$.
\end{theorem}

\begin{theorem}\label{th:halko}
(\textbf{Theorem 10.5, \cite{halko2011finding}}).
Suppose that $\mathbf{A}$ is a real $l$-by-$l$ matrix with eigenvalues $\sigma_{1}\geq\ldots\geq\sigma_{l}$. Choose a target rank $k$ and an oversampling parameter $p \geq 2,$ where $k+p\leq{l}$. Draw an $l\times(k+p)$ standard Gaussian matrix $\Omega$, and construct the sample matrix $\mathbf{A\Omega}$. Then,
\begin{flalign}
\mathbb{E}\parallel(\mathbf{I}-\mathbf{QQ}^\top)\mathbf{A}\parallel_{F}
\leq\left(1+\frac{k}{p-1}\right)^{1/2}\left(\sum_{i>k} \sigma_{i}^{2}\right)^{1/2},
\end{flalign}where $\mathbf{Q}$ is the orthonormal basis of the range of matrix $\mathbf{A\Omega}$ such that $\mathbf{A\Omega}=\mathbf{QQ}^\top\mathbf{A\Omega}$.
\end{theorem}

\begin{theorem}\label{th:kumar}
(\textbf{Corollary 2, \cite{kumar2012sampling}}).
Suppose that $\mathbf{X}$ is a real $d$-by-$n$ matrix. Choose a set $\mathcal{S}$ of size $l$ at random without replacement from $\{1, 2, \dots, n\}$, and let $\mathbf{H}$ equals the columns of $\mathbf{X}$ corresponding to indices in $\mathcal{S}$. Let $\mathbf{HH}^\top$ be an approximation to $\mathbf{XX}^\top$, then
\begin{flalign}
\mathbb{E}
\parallel \mathbf{XX}^\top - \kappa\mathbf{HH}^\top \parallel_F
\leq \frac{n}{\sqrt{l}}  
        \max_i \parallel\mathbf{X}_{*,i}\parallel^2,
\end{flalign}where $\kappa = \frac{n}{l}$ is a non-zero scaling parameter, and $\parallel\mathbf{X}_{*,i}\!\parallel$ is the Euclidean norm of the $i^\mathrm{th}$ column of matrix $\mathbf{X}$.
\end{theorem}


\subsection{Proof of Theorem~\ref{th:Snorm}}
\begin{proof}
Since the graph Laplacian matrix $\mathbf{L}$ is a symmetric positive semidefinite matrix, we can write it as:
\begin{flalign}
    \mathbf{L}=\mathbf{X}^\top\mathbf{X}, \nonumber
\end{flalign}where $\mathbf{X}\in\mathbb{R}^{d\times{n}}$ and $d$ is the rank of matrix $\mathbf{L}$.

Let $\mathbf{S}=\{0, 1\}^{n\times{s}}$ be a column sampling matrix where $\mathbf{S}_{i,j}$ equals to $1$ if the $i^\mathrm{th}$ column of $\mathbf{L}$ is chosen in the $j^\mathrm{th}$ random trial and equals to $0$ otherwise. Then, $\mathbf{C}=\mathbf{X}^\top\mathbf{H}$ and $\mathbf{A}=\mathbf{H}^\top\mathbf{H}$ where $\mathbf{H}=\mathbf{XS}$.

We take $\mathbf{R}=\mathbf{H}\mathbf{A}^{-1/2}\mathbf{Q}$, then 
\begin{flalign}
\widetilde{\mathbf{L}}
&=\mathbf{CA}^{-1/2}\widetilde{\mathbf{V}}\widetilde{\mathbf{V}}^\top\mathbf{A}^{-1/2}\mathbf{X}^\top \nonumber\\
&= \mathbf{X}^\top\mathbf{H}\mathbf{A}^{-1/2}\mathbf{Q}{\mathbf{Q}}^\top\mathbf{A}^{-1/2}\mathbf{H}^\top\mathbf{X}\nonumber\\
&= \mathbf{X}^\top\mathbf{P_RX} = \mathbf{X}^\top\mathbf{U_RU_R}^\top\mathbf{X}, \nonumber
\end{flalign}where $\mathbf{P_R}\!=\!\mathbf{H}\mathbf{A}^{-1/2}\mathbf{Q}{\mathbf{Q}}^\top\mathbf{A}^{-1/2}\mathbf{H}^\top$ is an orthogonal projector and $\mathbf{U_R}$ is the orthonormal basis of matrix $\mathbf{R}$.

To bound the approximate error, we have
\begin{flalign}
&\parallel \mathbf{L} - \widetilde{\mathbf{L}} \parallel_2\nonumber\\
= &\parallel 
    \mathbf{X}^\top\mathbf{X} -
    \mathbf{X}^\top\mathbf{U_RU_R}^\top\mathbf{X} 
\parallel_2\nonumber\\
\stackrel{(a)}{=} &\parallel 
    \mathbf{X}^\top\mathbf{X} -
    (\mathbf{P_RX})^\top\mathbf{P_R}\mathbf{X} 
\parallel_2\nonumber\\
\stackrel{(b)}{=} &\parallel 
    \mathbf{X} -
    \mathbf{U_RU_R}^\top\mathbf{X} 
\parallel^2_2 \nonumber\\
=&\parallel 
    \mathbf{X} -
    \mathbf{X}\mathbf{U_RU_R}^\top
\parallel^2_2\nonumber\\
=& \parallel 
    \mathbf{X}\mathbf{X}^\top -
    \mathbf{X}\mathbf{U_RU_R}^\top\mathbf{X}^\top
\parallel_2,\nonumber
\end{flalign}where (a) holds due to the orthogonal project $\mathbf{P_R}$ satisfying $\mathbf{P_R}=\mathbf{P_R}^\top=\mathbf{P_R}^2$; 
(b) holds due to $\parallel\mathbf{AB}\parallel_2=\parallel\mathbf{BA}\parallel_2$ for any $\mathbf{A}\in\mathbb{R}^{m\times{n}}$ and $\mathbf{B}\in\mathbb{R}^{n\times{m}}$; 

Using Lemma~\ref{lemma:proj_upper} in Sec.~\ref{sec:lemma2},
\begin{flalign}
&\parallel 
    \mathbf{X}\mathbf{X}^\top -
    \mathbf{X}\mathbf{U_RU_R}^\top\mathbf{X}^\top
\parallel^2_2\nonumber\\
\le& \parallel 
    \mathbf{X}\mathbf{X}^\top -
    \kappa\mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top
\parallel_2\nonumber\\
\le& \parallel
    \mathbf{X}\mathbf{X}^\top - \kappa\mathbf{H}^\top\mathbf{H}
\parallel_2 \nonumber\\
&\quad\quad\quad+ \kappa\parallel 
    \mathbf{H}\mathbf{H}^\top -
    \mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top
\parallel_2, \label{eqn:spc_err}
\end{flalign}where the last step holds due to the triangle inequality.


Since $\mathbf{A}^{-1/2}\mathbf{H}^\top\mathbf{H}\mathbf{A}^{-1/2}=\mathbf{I}_s$, it gives
\begin{flalign}
&\parallel\mathbf{HH}^\top - 
    \mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top
    \parallel_2 \nonumber\\
=& \parallel \mathbf{HH}^\top - 
    \mathbf{HA}^{-1/2}\mathbf{Q}(\mathbf{HQ})^\top\mathbf{HQQ}^\top\mathbf{A}^{-1/2}\mathbf{H}^\top
    \parallel_2 \nonumber\\
=& \parallel \mathbf{HA^{-1/2}}
        (\mathbf{A^{1/2}H}^\top - 
    \mathbf{QQ}^\top\mathbf{H}^\top\mathbf{H}\mathbf{A}^{-1/2}\mathbf{H}^\top)
    \parallel_2 \nonumber\\
=& \parallel \mathbf{H}^\top\mathbf{H} -
    \mathbf{QQ}^\top\mathbf{H}^\top\mathbf{H}
    \parallel_2 
= \parallel (\mathbf{I} - \mathbf{QQ}^\top)\mathbf{A} \parallel_2.\nonumber
\end{flalign}

Using Theorem~\ref{th:mli}, we can bound the expected error,
\begin{flalign}
&\kappa\mathbb{E} 
\parallel \mathbf{HH}^\top\!\! - 
    \mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top\!\!
    \parallel_2 \nonumber\\
=& \kappa\mathbb{E}\parallel (\mathbf{I} - \mathbf{QQ}^\top)\mathbf{A} \parallel_2 \nonumber\\ 
\le& \zeta^{1/q}\sigma_{r+1}(\kappa\mathbf{A}) 
= \zeta^{1/q}\sigma_{r+1}(\kappa\mathbf{HH}^\top) \nonumber\\
\le& \zeta^{1/q}\sigma_{r+1}(\mathbf{XX}^\top) + \zeta^{1/q}\parallel
    \mathbf{XX}^\top - \kappa\mathbf{HH}^\top\!\!
\parallel_2, \label{enq:spc_A_err}
\end{flalign}where the last inequality holds because of $\sigma_{r+1}(\kappa\mathbf{HH}^\top) - \sigma_{r+1}(\mathbf{XX}^\top) \le \max_i | \sigma_{i}(\mathbf{XX}^\top) - \sigma_{i}(\kappa\mathbf{HH}^\top) |$ and Lemma~\ref{lemma:stewart}.

Combining Eq. (\ref{eqn:spc_err}) and (\ref{enq:spc_A_err}), we conclude our result
\begin{flalign}
&\mathbb{E} \parallel \mathbf{L} - \widetilde{\mathbf{L}} \parallel_2 \nonumber\\
&\le \zeta^{1/q}\sigma_{r+1}(\mathbf{XX}^\top) 
    + (1 + \zeta^{1/q})\parallel
    \mathbf{XX}^\top - \kappa\mathbf{HH}^\top\!\!
\parallel_2 \nonumber\\
&\le \zeta^{1/q}\parallel 
    \mathbf{L} - \mathbf{L}_r \parallel_2
+ (1 + \zeta^{1/q}) 
    \frac{n}{\sqrt{s}}\mathbf{L}^\ast_{i,i}, \nonumber
\end{flalign}where the last step is due to $\parallel\mathbf{XX}^\top - \kappa\mathbf{HH}^\top\!\!\parallel_2 \le \parallel\mathbf{XX}^\top - \kappa\mathbf{HH}^\top\!\!\parallel_F$ and Theorem~\ref{th:kumar}.
\end{proof}


\subsection{Proof of Lemma~\ref{lemma:proj_upper}}
\label{sec:lemma2}
\begin{lemma}\label{lemma:proj_upper}
Given $\mathbf{X}\in\mathbb{R}^{d\times{n}}$, let $\mathbf{U_R}$ be the orthonormal basis of the range of matrix $\mathbf{R}\in\mathbb{R}^{d\times{s}}$. Then for any $\mathbf{HQ}\in\mathbb{R}^{s\times{s}}$,
\begin{flalign}
\parallel \mathbf{XX}^\top\!\!-& \mathbf{XU_R}(\mathbf{XU_R})^\top\!\!\parallel_2 \nonumber\\
&\le \parallel 
    \mathbf{XX}^\top\!\!- \kappa\mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top \!\!\parallel_2. \nonumber
\end{flalign}where $\kappa = \frac{n}{s}$ is a non-zero scaling parameter.
\end{lemma}
\begin{proof}
Let $\mathbf{P_R}=\mathbf{U_RU_R^\top}$. On using the property of orthogonal projector (i.e., $\mathbf{P_R}=\mathbf{P_R^\top}=\mathbf{P_R}^2$), we have
\begin{flalign}
&\parallel 
    \mathbf{XX}^\top\!\! -
    \mathbf{XU_R}(\mathbf{XU_R})^\top \!\! 
    \parallel_2 \nonumber\\
=& \parallel \mathbf{XX}^\top - \mathbf{XP_R}(\mathbf{XP_R})^\top \parallel_2 \label{eqn:x_norm}\\
=& \parallel \mathbf{X} - \mathbf{P_RX} \parallel^2_2  
= \max_{\parallel\mathbf{v}\parallel=1}
    \parallel
        \mathbf{v}^\top( \mathbf{X} - \mathbf{P_RX} )
    \parallel^2. \nonumber
\end{flalign}

We then decompose the vector $\mathbf{v}$ as $\mathbf{v}=\alpha\mathbf{y} + \beta\mathbf{z}$, where $\mathbf{y}\in\mathrm{ran}(\mathbf{R}),\mathbf{z}\in\mathrm{ran}^{\perp}(\mathbf{R})$ and $\alpha^2 + \beta^2 = 1$. It is clear to see that $\mathbf{y}^\top\mathbf{P_R}=\mathbf{y}^\top$, and $\mathbf{z}^\top\mathbf{P_R}=0$. Thereby,
\begin{flalign}
&\parallel \mathbf{X} 
- \mathbf{P_RX} \parallel_2 \nonumber\\
\le& \max_{\mathbf{y}\in\mathrm{ran}(\mathbf{R}),\parallel\mathbf{y}\parallel=1}
    \parallel
        \mathbf{y}^\top(\mathbf{X} - \mathbf{P_RX})
    \parallel \nonumber\\
&\quad\quad\quad + \max_{\mathbf{y}\in\mathrm{ran}^{\perp}(\mathbf{R}),\parallel\mathbf{z}\parallel=1}
    \parallel
        \mathbf{z}^\top(\mathbf{X} - \mathbf{P_RX}) 
    \parallel \nonumber\\
\le& \max_{\mathbf{z}\in\mathrm{ran}^{\perp}(\mathbf{R}),\parallel\mathbf{z}\parallel=1}
    \parallel
        \mathbf{z}^\top\mathbf{X} 
    \parallel. \label{eqn:x_reform}
\end{flalign}

For $\mathbf{z}\in\mathrm{ran}^{\perp}(\mathbf{R})$, $\mathbf{z}^\top\mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top\mathbf{z}=0$. Then, 
\begin{flalign}
\parallel \mathbf{z}^\top\mathbf{X} \parallel^2
&= \mathbf{z}^\top\mathbf{XX}^\top\mathbf{z} \nonumber\\
&= \mathbf{z}^\top(\mathbf{XX}^\top 
    - \kappa\mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top)\mathbf{z} \nonumber\\
&\le \max_{\parallel\mathbf{z}\parallel=1}
    \mathbf{z}^\top(\mathbf{XX}^\top 
    - \kappa\mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top)\mathbf{z} \nonumber\\
&= \parallel 
        \mathbf{XX}^\top 
        - \kappa\mathbf{R}(\mathbf{HQ})^\top\mathbf{HQ}\mathbf{R}^\top\!\!
    \parallel_2 \label{eqn:x_upper}.
\end{flalign}Combining Eq. (\ref{eqn:x_norm}-\ref{eqn:x_upper}) concludes the lemma.
\end{proof}


\section{Implementation Details}\label{sec:impdetails}
In this section, we present the details of our implementation in order for reproducibility. All experiments are conducted on the machines with Xeon 3175X CPU, 128G memory and RTX8000 GPU with 48 GB memory. The configurations and packages are listed below:
\begin{itemize}
	\item Ubuntu 16.04
	\item CUDA 10.2
	\item Python 3.7 
	\item Tensorflow 1.15.3
	\item Pytorch 1.10
	\item DGL 0.8.2
	\item NumPy 1.19.0 with MKL Intel
\end{itemize}

\subsection{EasyDGL Architectures for Three Tasks on Graph}
\subsubsection{Dynamic Link Prediction:}
\begin{itemize}
    \item Use maximum sequence length to $30$ with the masked probability $0.2$.
	\item Two-layer Attention-Intensity-Attention with two heads.
	\item Use \textit{ReLU} as the activation.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

\subsubsection{Dynamic Node Classification}
\begin{itemize}
    \item Randomly mask graph nodes with probability $0.2$.
	\item Two-layer \textit{GATConv} and one-layer Attention-Intensity-Attention block with two heads.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for multi-class prediction. 
\end{itemize}

\subsubsection{Traffic Forecasting}
\begin{itemize}
    \item Randomly mask graph nodes with probability $0.2$.
	\item Two-layer \textit{SAGEConv} and one-layer Attention-Intensity-Attention with eight heads.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for prediction. 
\end{itemize}

\subsection{Baseline Architectures for Dynamic Link Prediction}
\textbf{As mentioned, we follow IDCF\cite{wu2021towards} to build typical GNN architectures. Here we introduce the details for them.}

{\bf GAT.} We use the \textit{GATConv} layer available in DGL for implementation. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of one-layer  \textit{GATConv} with four heads. 
	\item Add self-loop and use batch normalization for graph convolution in each layer.
	\item Use \textit{tanh} as the activation.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf GraphSAGE.} We use the \textit{SAGEConv} layer available in DGL for implementation. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two-layer \textit{SAGEConv}.
	\item Add self-loop and use batch normalization for graph convolution in each layer.
	\item Use \textit{ReLU} as the activation.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf GCN.} We use the \textit{SGConv} layer available in DGL for implementation. The detailed architecture description is as below:
\begin{itemize}
	\item One-layer \textit{SGConv} with two hops.
	\item Add self-loop and use batch normalization for graph convolution in each layer.
	\item Use \textit{ReLU} as the activation.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf ChebyNet.} We use the \textit{ChebConv} layer available in DGL for implementation. The detailed architecture description is as below:
\begin{itemize}
	\item One-layer \textit{ChebConv} with two hops.
	\item Add self-loop and use batch normalization for graph convolution in each layer.
	\item Use \textit{ReLU} as the activation.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf ARMA.} We use the \textit{ARMAConv} layer available in DGL for implementation. The detailed architecture description is as below:
\begin{itemize}
	\item One-layer \textit{ARMAConv} with two hops.
	\item Add self-loop and use batch normalization for graph convolution in each layer.
	\item Use \textit{tanh} as the activation.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf We also summarize the implementation details of the compared sequential and temporal baselines as follows.}

{\bf GRU4REC.}\footnote{\textcolor{blue}{https://github.com/hidasib/GRU4Rec}} We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two GRU cells.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf SASREC.}\footnote{\textcolor{blue}{https://github.com/kang205/SASRec}} We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two-block Transformer with four heads on Koubei, eight heads on Tmall and eight head on Netflix.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf GREC.}\footnote{\textcolor{blue}{https://github.com/fajieyuan/WWW2020-grec}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of six-layer dilated CNN with degree $1,2,2,4,4,8$.
	\item Use maximum sequence length to $30$ with the masked probability $0.2$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf S2PNM.}\footnote{\textcolor{blue}{https://github.com/cchao0116/S2PNM-TKDE2022}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of one-block GRU-Transformer.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf BERT4REC.}\footnote{\textcolor{blue}{https://github.com/FeiSun/BERT4Rec}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of three-block Transformer with eight heads.
	\item Use maximum sequence length to $30$ with the masked probability $0.2$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf DyREP.}\footnote{\textcolor{blue}{https://github.com/uoguelph-mlrg/LDG}}
We use the software provided by the third party for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of one Attention-RNN Layer.
	\item Use maximum sequence length to $30$.
	\item Use linear layer of user embedding and item embedding with softplus activation as ranking score. 
\end{itemize}

{\bf TGAT.}\footnote{\textcolor{blue}{https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of three-block Transformer with one head, time sinusoidal embeddings.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf TiSASREC.}\footnote{\textcolor{blue}{https://github.com/JiachengLi1995/TiSASRec}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two-block Transformer with eight heads with time embedding.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf TGREC.}\footnote{\textcolor{blue}{https://github.com/DyGRec/TGSRec}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of three-block Transformer with time sinusoidal embeddings.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

{\bf TimelyREC.}\footnote{\textcolor{blue}{https://github.com/Junsu-Cho/TimelyRec}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of one-block Attention-Attention.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}


{\bf CTSMA.}\footnote{\textcolor{blue}{https://github.com/cchao0116/CTSMA-ICML21}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two-block Transformer with four heads.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}

\reviewerone{
{\bf TiCoSeREC.}\footnote{\textcolor{blue}{https://github.com/KingGugu/TiCoSeRec}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two-block Transformer with two heads.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}}

\reviewerone{
{\bf PTGCN.}\footnote{\textcolor{blue}{https://github.com/drhuangliwei/PTGCN}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item A sequence of two-block Transformer with four heads.
	\item Use maximum sequence length to $30$.
	\item Use inner product between user embedding and item embedding as ranking score. 
\end{itemize}}

\reviewerone{
{\bf NeuFilter.}\footnote{\textcolor{blue}{https://github.com/Yaveng/NeuFilter}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item Two GRU cells separately for item and user sequence.
	\item Use maximum sequence length to $30$.
	\item Concatenate user embeddings and item embeddings as the input of an multi-layer network which outputs ranking score. 
\end{itemize}}


\subsection{Baseline Architectures for Dynamic Node Classification}
The configurations for static graph models are identical to the dynamic link prediction task except the decoder module that is replaced by one-layer \textit{Linear}. 

\textbf{In the following, we present the architectures for new dynamic graph models.}

{\bf DySAT.}\footnote{\textcolor{blue}{https://github.com/aravindsankar28/DySAT}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past five graph snapshots as input.
	\item A sequence of three-layer \textit{GATConv} and one-layer TemporalAttention with two heads.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for multi-class prediction.
\end{itemize}

{\bf EvolveGCN.}\footnote{\textcolor{blue}{https://github.com/IBM/EvolveGCN}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past five graph snapshots as input.
	\item One-layer gated GRUCell to update the parameters of two-layer \textit{GCNConv}.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for multi-class prediction.
\end{itemize}

{\bf JODIE.}\footnote{\textcolor{blue}{https://github.com/claws-lab/jodie}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past five graph snapshots as input.
	\item One-layer gated GRUCell to update the hidden states read out from two-layer TemporalTransformer.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for multi-class prediction.
\end{itemize}

{\bf TGN.}\footnote{\textcolor{blue}{https://github.com/twitter-research/tgn}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past five graph snapshots as input with sinusoidal time embedding. 
	\item Three-layer TemporalTransformer with five heads.
	\item One-layer time-decayed Recurrent unit to sequentially update node embeddings over time.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for multi-class prediction.
\end{itemize}

\reviewerone{
{\bf RoLAND.}\footnote{\textcolor{blue}{https://github.com/snap-stanford/roland}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
	\item Two-layer node embedding updation modules. In each layer, we first adopt the vanilla \textit{message-passing} to obtain preliminary node embeddings. Then, these preliminary node embeddings and the node embeddings output from the previous layer are both input into a one-layer \textit{GRU} to obtain the final node embedding for this layer.
	\item Use \textit{PReLU} as the activation.
	\item Use a \textit{MLP} implemented by two-layer \textit{Linear} for multi-class prediction.
\end{itemize}
}


\reviewerone{
{\bf DEFT.}\footnote{\textcolor{blue}{https://github.com/ansonb/DEFT}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past five graph snapshots as input. 
	\item Two-layer \textit{ChebConv} with extra spectral filtering.
	\item Use \textit{RReLU} as the activation.
	\item Use a \textit{MLP} implemented by two-layer \textit{Linear} for multi-class prediction.
\end{itemize}
}


\reviewerone{
{\bf SpikeNet.}\footnote{\textcolor{blue}{https://github.com/EdisonLeeeee/SpikeNet}} 
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use all past graph snapshots as input.
    \item Wwo-layer \textit{GraphSage} with temporal neighborhood sampler on graphs.
    \item Use \textit{LIF} (Leaky integrate-and-Ô¨Åre) as the activation.
    \item Use one-layer \textit{Linear} for multi-class prediction. 
\end{itemize}
}



\subsection{Baseline Architectures for Traffic Forecasting}
The configurations for static graph models are identical to the dynamic link prediction task except the decoder module that is replaced by one-layer \textit{Linear}.

\textbf{In the following, we present the architectures for new dynamic graph models.}

{\bf DCRNN.}
We use the software provided in DGL library. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
    \item One-layer \textit{ChebConv} with two hops.
    \item One-layer \textit{GraphRNN} to sequential update the node embeddings over time.
    \item Use \textit{ReLU} as the activation.
    \item Use one-layer \textit{Linear} for prediction.
\end{itemize}

{\bf GaAN.}
We use the software provided in DGL library. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
    \item One-layer gated graph attention that considers the edge weights.
    \item One-layer \textit{GraphRNN} to sequential update the node embeddings over time.
    \item Use \textit{ReLU} as the activation.
    \item Use one-layer \textit{Linear} for prediction.
\end{itemize}

{\bf STGCN.}
We use the software provided in DGL library. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
	\item A sequence of TNTSTNTST, where T, N, S represents temporal convolutional neural network, \textit{LayerNorm} and spatial graph convolutional neural network, respectively.
	\item Use \textit{ReLU} as the activation.
	\item Use one-layer \textit{Linear} for prediction.
\end{itemize}

{\bf DSTAGNN.}\footnote{\textcolor{blue}{https://github.com/SYLan2019/DSTAGNN}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
	\item Four-layer DSTAGNN with four heads each of which uses \textit{ChebConv} with three hops.
	\item Use \textit{ReLU} as the activation.
	\item Use \textit{Conv2d} and \textit{Linear} for prediction.
\end{itemize}





\reviewerone{
{\bf Trafformer.}\footnote{\textcolor{blue}{https://github.com/jindi-tju/Trafformer}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
	\item Two-layer Transformer-style encoder with two heads.
        \item One-layer Transformer-style decoder with two heads.
	\item Use \textit{GeLU} as the activation.
	\item Use one-layer \textit{Linear} for prediction.
\end{itemize}
}


\reviewerone{
{\bf TrendGCN.}\footnote{\textcolor{blue}{https://github.com/juyongjiang/TrendGCN}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
        \item Two-layer \textit{Linear} discriminator with \textit{LeakyReLU} which focuses on the trend of individual time series.
        \item Two-layer \textit{Linear} discriminator with \textit{LeakyReLU} which emphasizes the correlation of multivariate time series.
        \item Two-layer \textit{ChebConv} and two-layer \textit{GRU}.
	\item Use one-layer \textit{Linear} for prediction.
 
\end{itemize}
}

\reviewerone{
{\bf STID}\footnote{\textcolor{blue}{https://github.com/zezhishao/STID/}}
We use the software provided by the authors for experiments. The detailed architecture description is as below:
\begin{itemize}
    \item Use past twelve graph snapshots as input.
        \item proposes aa framework with a embedding layer (by \textit{Conv2d}), multiple MLP layers, and a regression layer.
        \item Use \textit{ReLU} as the activation.
        \item Use \textit{Linear} for prediction.
\end{itemize}
}

\subsection{Choice of Hyper-parameters}
Regarding the choice of optimizer, we use Adam \cite{kingma2015adam} if not specified and the number of epochs is $200$. We search by grid the embedding size ranging in $\{64,128,\dots,512\}$, learning rate $\{1e\!-\!5,1e\!-\!4,\dots,1e\!-\!1\}$, dropout rate $\{0.1,0.2,\dots,0.7\}$, batch size $\{64,128,\dots,512\}$ and $\ell_2$ regularizer $\{1e\!-\!5,1e\!-\!4,\dots,1e\!-\!1\}$. We also study the influence of the neighborhood hops ranging from $1$ to $3$, the number of graph filtering blocks from $1$ up to $4$ and the number of heads in $\{1,2,\dots,8\}$.

We warn that we set the embedding size to $32$ for fair comparisons when evaluating the performance on the Ellicit and META-LA datasets. This is because TGN, DCRNN and DSTAGNN take days to complete the training if embedding size is greater than 64.

With regard to EasyDGL, we search by grid the masking rate in $\{10\%,20\%,\dots,50\%\}$ where, in majority of cases, $20\%$ produces the best results. We also search the best parameter for the TPPLE term in $\{1e\!-\!7,1e\!-\!6,\dots,1e\!-\!3\}$.

\section{Dataset Processing}
We present more dataset details in this section.

\subsection{Time Scaling}
We use one hour, one day and one week to scale the time data on the Netflix, Tmall and Koubei datasets, respectively. The choice of time unit is determined by the averaged time between two consecutive events for each user. We warn that again the time is discrete on the Elliptic data and the traffic speed readings on META-LA are record every five minutes. For both of these two datasets, we apply no modifications to the time data.

\subsection{Random Seed}
We use five random seeds to yield different data splits, i.e., $12345$, $54321$, $56789$, $98765$ and $7401$.

\subsection{Normalization on the META-LA data}
We calculate the mean and the standard deviation of the training readings on each road (node). When making predictions, we use these quantities to scale down the input readings and scale up the output readings. We found by experiments that this treatment can significantly reduce the RMSE and MAPE errors.
