There are many examples of computational problems for which a slight variant can become much harder than the original: 2-SAT (in P) vs.\ 3-SAT (NP-complete), perfect matching (in P) vs.\ its counting version (\#P-complete), and so on. Fine-grained complexity (FGC) has provided explanations for such differences for many natural problems -- e.g.\ finding a minimum weight triangle in a node-weighted graph is ``easy'' (in $O(n^\omega)$ time \cite{CzumajL09}, where $\omega<2.373$ \cite{alman2021refined} is the matrix multiplication exponent) while finding a minimum weight triangle in an edge-weighted graph is ``hard'' (subcubically equivalent to All-Pairs Shortest Paths (\APSP{}) \cite{focsy,focsyj}). Nevertheless, many fundamental questions remain unanswered:

\begin{itemize}
\item Seidel \cite{seidel1995} showed that \APSP{} in unweighted {\em undirected}  graphs can be solved  in $\tilde{O}(n^\omega)$ time\footnote{The notation $\tilde{O}(f(n))$ denotes $f(n)\poly\log(n)$.}. The fastest algorithm for \APSP{} in unweighted {\em directed}  graphs by Zwick~\cite{zwickbridge} runs in $O(n^{2.53})$ for the current bound of rectangular matrix multiplication~\cite{legallurr}. If $\omega=2$, then undirected \APSP{} would have an essentially optimal $\tilde{O}(n^2)$ time algorithm, whereas Zwick's algorithm for directed \APSP{}
would run in, slower, $\tilde{O}(n^{2.5})$ time. 

Assuming that the runtime of Zwick's algorithm is the best possible for unweighted directed  APSP (abbreviated \uAPSP{}) has been used as a hardness hypothesis (see e.g. \cite{lincoln2020monochromatic,williamsxumono}). However, so far there has been {\em no explanation} for why \uAPSP{} should be harder than its undirected counterpart.
\begin{center}
{\em \hypertarget{question:Q1}{Q1}: Does \APSP{} in directed unweighted graphs require superquadratic time, even if $\omega=2$?}
\end{center}

\item Boolean matrix multiplication (\BMM{}) asks to compute, for two $n\times n$ Boolean matrices $A$ and $B$, for every $i,j\in \{1,\ldots,n\}$ whether there is some $k$ so that $A[i,k]=B[k,j]=1$. The Min-Witness product (\MinWitness{}) is a well-studied \cite{KowalukLingas, VassilevskaWY10,shapira2011all,CohenY14,KowalukL21} generalization of \BMM{} in which for every $i,j$, one needs to instead compute the {\em minimum} $k$ for which $A[i,k]=B[k,j]=1$.  Similarly to \uAPSP{}, the fastest algorithm for \MinWitness{}  runs in $O(n^{2.53})$ time \cite{CzumajKL07}, and would run in $\tilde{O}(n^{2.5})$ time if $\omega=2$, whereas \BMM{} would have an essentially optimal algorithm in that case.

The assumption that \MinWitness{} requires $n^{2.5-o(1)}$ time has been used as a hardness hypothesis (e.g. \cite{lincoln2020monochromatic}), but similar to \uAPSP{}, so far there has been no explanation as to why \MinWitness{} should be harder than \BMM{}.
\begin{center}
{\em \hypertarget{question:Q2}{Q2}: Does \MinWitness{} require superquadratic time, even if $\omega=2$?}
\end{center}

\item For a large number of problems of interest within FGC, the known algorithms for finding a solution can also compute the {\em number} of solutions in the same time. This is true for triangle detection in graphs, \BMM{}, Min-Plus product, \ThreeSUM{}, Exact Triangle, Negative Triangle, and many more. Is this merely a coincidence, or are the decision variants of these problems equivalent to the counting variants?

A natural and important question is:
\begin{center}
{\em
\hypertarget{question:Q3}{Q3}: Are the core FGC problems like \ThreeSUM{}, Min-Plus product and Exact Triangle easier than their exact counting variants?
}
\end{center}

A recent line of work \cite{DellL21,DellLM20} gives fine-grained reductions from {\em approximate counting} to decision for several core problems of FGC, showing that if the decision versions have improved algorithms, then one can also obtain fast approximation schemes. As an approximate count can solve the decision problem, we get that
 for many key problems {\em approximate counting} and decision are equivalent.

This does not imply that {\em exact counting} would be equivalent to decision however. In fact, quite often the decision and counting versions of computational problems can have vastly different complexities. There are many examples of polynomial time decision problems (e.g.\ perfect matching \cite{Valiant79}) whose counting variants become $\#$P-complete. 
For many of these problems, polynomial time approximation schemes are possible (see e.g.\ \cite{JerrumSV04} for perfect matching), however obtaining a fast {\em exact} counting algorithm is considered infeasible.

Within FGC, there are more examples. Consider for example the case of induced subgraph isomorphism for pattern graphs of constant size $k$.  It is known (see e.g.\  \cite{CurticapeanDM17}) that for {\em every} $k$-node pattern graph $H$, {\em counting} the induced copies of $H$ in an $n$-node graph is fine-grained equivalent to {\em counting} the $k$-cliques in an $n$-node graph.
Meanwhile, the work of \cite{DellL21,DellLM20} implies that {\em approximately} counting the induced $k$-paths in a graph is fine-grained equivalent to detecting a single $k$-path. However, induced $k$-paths can be found (and hence can be approximately counted) {\em faster} than counting $k$-cliques: combinatorially, there's an $O(n^{k-2})$ time algorithm \cite{BlaserKS18}, whereas $k$-clique counting is believed to require $n^{k-o(1)}$ time; with the use of fast matrix multiplication, if $k<7$, $k$-paths can be found and approximately counted in the best known running time for $(k-1)$-clique detection \cite{WilliamsWWY15,BlaserKS18}, faster than $k$-clique.

To reiterate Question \hyperlink{question:Q3}{Q3} in this context, it asks whether the core FGC problems are like induced $k$-path above, or perhaps surprisingly, are equivalent to their counting versions?


The fine-grained problems of interest such as \ThreeSUM{}, Exact-Triangle, Orthogonal Vectors and more, admit efficient self-reductions.
For such problems, prior work~\cite{focsyj} has given generic techniques to show that the problems are fine-grained equivalent to the problem of {\em listing} any ``small'' number of solutions.
For example, \ThreeSUM{} is subquadratically equivalent to listing any truly subquadratic\footnote{Truly subquadratic means $O(n^{2-\eps})$ for some constant $\eps>0$; truly subcubic means $O(n^{3-\eps})$ for constant $\eps>0$, etc.} number of \ThreeSUM{} solutions.
 Thus, as long as the count is small, the listing problem is equivalent to the decision problem. This technique has become an important technique in FGC, used in many subsequent works (e.g.~\cite{HenzingerKNS15,backurs2017better,KunnemannPS17,cygan2019problems}).
The issue is, however, that when the count is actually ``large'', say $\Omega(n^2)$ for \ThreeSUM{}, the listing approach is too expensive. 
The question becomes, how do we count faster than listing when the count is large? Until now (more than 10 years after the conference version of~\cite{focsyj}) there has been no technique to do this.

\item
Recently, wide classes of structured instances of Min-Plus
matrix products, Min-Plus convolutions, and \ThreeSUM{} have been
identified which can be solved faster.
Chan and Lewenstein~\cite{ChanLewenstein} obtained the first truly 
subquadratic algorithm for Min-Plus convolution for monotone 
sequences over $[n]:=\{1,\ldots,n\}$ (and also integer sequences with bounded differences),
and Bringmann, Grandoni, Saha and Vassilevska W.~\cite{BringmannGSW16} 
obtained the first truly subcubic algorithm for Min-Plus 
product for integer matrices with bounded differences. 
The latter result is in some sense more general, as bounded-difference Min-Plus convolution
reduces to (row/column) bounded-difference Min-Plus matrix product.  
Both
results have subsequently been generalized or improved \cite{WilliamsX20, GuPWX21, ChiDXsoda22, ChiDXZstoc22}.  Interestingly, Chan and Lewenstein's approach made use of a
famous result in additive combinatorics known as the \emph{Balog--Szemer\'edi--Gowers Theorem} (the ``BSG Theorem''), whereas \cite{BringmannGSW16}'s approach
and all  subsequent algorithms use more direct techniques
without the need for additive combinatorics.   So far, there has been no explanation for why
there are two seemingly different approaches.
This leads to our last main question:

\begin{center}\em
\hypertarget{question:Q4}{Q4}: What is the relationship between the BSG-based approach and the direct approach to monotone Min-Plus
product/convolution, and could they be unified?
\end{center}

Question \hyperlink{question:Q4}{Q4} might appear unrelated to the preceding questions, and is more vague
or conceptual.  But the hope is that by understanding the relationship better,
we may obtain new improved algorithms, since Chan and Lewenstein's approach
has several applications, e.g.\ to \ThreeSUM{} with preprocessed universes and \ThreeSUM{} for 
$d$-dimensional monotone sets in $[n]^d$, which are not handled by the subsequent approaches.



\end{itemize}

\subsection{Summary of Our Contributions and New Tool}

\begin{enumerate}

\item {\bf Conditional hardness for \uAPSP{}.}  One of the main hypotheses of fine-grained complexity, known as the ``APSP Hypothesis'', is that \APSP{} in $n$-node graphs with polynomial integer edge weights requires $n^{3-o(1)}$ time. Meanwhile, the fastest algorithms for \APSP{} \cite{seidel1995, zwickbridge, Williams18} still fail to solve the problem in truly subcubic time when the edge weights are in $[n]$. In fact, even Min-Plus product on $n\times n$ matrices with entries in $[n]$ is not known to be solvable in truly subcubic time: the known algorithm for small integer entries runs in $\tilde{O}(Mn^\omega)$ time \cite{ALONGM1997}, and even if $\omega=2$, this is no better than the brute-force cubic time algorithm when $M=n$. Moreover, Zwick \cite{zwickbridge} explicitly asks whether there is a truly subcubic time algorithm for \APSP{} with weights in $[n]$.

We formulate a version of the APSP Hypothesis, which we call the \StrongAPSP{}, stating (for $\omega=2$) that \APSP{} in graphs with edge weights in  $[n]$ requires $n^{3-o(1)}$ time. Under this hypothesis we show that  \uAPSP{} requires $n^{7/3-o(1)}$ time, even if $\omega=2$. 

Thus, we conditionally resolve question \hyperlink{question:Q1}{Q1}: either {\em unweighted directed} APSP requires super-quadratic time and is harder than unweighted undirected \APSP{} (when $\omega=2$), or \APSP{} in weighted graphs with weights in $[n]$ is in truly subcubic time.

This is the first fine-grained connection between unweighted and weighted \APSP{}.

\item {\bf Conditional hardness for \MinWitness{}.} 
We present the first fine-grained reduction from \APSP{} in unweighted directed graphs to \MinWitness{} (which was  %
left open by \cite{lincoln2020monochromatic}). 
Our reduction implies that, when $\omega = 2$, \MinWitness{} requires $n^{7/3 - o(1)}$ time unless the current best algorithm for \uAPSP{}~\cite{zwickbridge} can be improved. 
Alternatively, working under the \StrongAPSP{},
we also obtain an $n^{11/5-o(1)}$ lower bound for \MinWitness{}.

Thus, either \MinWitness{} is truly harder than \BMM{} (if $\omega=2$), or there is a breakthrough in unweighted or weighted \APSP{} algorithms. This gives an answer to question \hyperlink{question:Q2}{Q2}.

See Section~\ref{sec:interm} for further discussion and details of our results on \MinWitness{} and \uAPSP{}.

\item  {\bf More hardness results.} We give many more fine-grained lower bounds under the \StrongAPSP{} for problems such as Batched Range Mode, All Pairs Shortest Lightest Paths, Min Witness Equality Product, dynamic shortest paths in unweighted planar graphs and more.

\item {\bf Counting is equivalent to detection.} We resolve question \hyperlink{question:Q3}{Q3} for several core problems in FGC\@. 
We show that the \MinPlus{} problem is subcubically equivalent to its counting version, the \ThreeSUM{}  problem is subquadratically equivalent to its counting version, and  the Exact-Weight Triangle  problem (\ExactTri) is subcubically equivalent to its counting version. These are the first  fine-grained equivalences between exact counting and decision problem variants, to our knowledge.
See Section~\ref{sec:count} for further results and discussion.




\item {\bf New variants of the BSG Theorem and new algorithms.}
We formulate a new decomposition theorem for zero-weight triangles of weighted graphs,
which may be viewed as a substitute to the BSG Theorem but has
a simple direct proof, providing an answer to question \hyperlink{question:Q4}{Q4}.
Besides being applicable to monotone Min-Plus convolution/products,
\ThreeSUM{} with preprocessed universes and \ThreeSUM{} with $d$-dimensional monotone sets,
the theorem yields the first truly subquartic algorithm for the counting version of the 
general weighted \APSP{} problem.  Our ideas also lead to a new bound on
the BSG Theorem itself which is an improvement for a certain range of parameters.
As a result, we obtain an improved new algorithm for
\ThreeSUM{} with preprocessed universes.
See Sections \ref{sec:newalgs} and \ref{sec:bsg} for more details.


\end{enumerate}

Surprisingly, we are able to achieve all of these results with a {\bf single new tool},
 a careful combination of two known techniques in tackling shortest paths problems in graphs: Fredman's trick \cite{fredman1976new} and Matou\v{s}ek's approach for dominance product~\cite{MatIPL}.

It has long been known that \APSP{} in general $n$-node graphs is equivalent to computing the Min-Plus product of two $n\times n$ matrices $A$ and $B$ (\MinPlus{}), defined as the matrix $C$ with $C_{ij}=\min_k (A_{ik}+B_{kj})$.
Fredman \cite{fredman1976new} introduced the following powerful ``trick'' for dealing with the above minimum: 
 to determine if $A_{ik}+B_{kj}\leq A_{i\ell}+B_{\ell j}$, we simply need to check if $$A_{ik}-A_{i\ell}\leq B_{\ell j}-B_{kj}.$$

While seemingly trivial, this idea of comparing a left-hand side that is purely in terms of entries of $A$ to a right-hand side that is purely in terms of entries of $B$, leads to a variety of amazing results. Fredman used it to show that the decision tree complexity of \MinPlus{} is $O(n^{2.5})$, and not cubic as previously thought. Practically all subcubic algorithms for \APSP{} (e.g.\  \cite{fredman1976new,Takaoka98,Chan10,Williams18}) including the current fastest $n^3/\exp(\Theta(\sqrt{\log n}))$ time algorithm by Williams \cite{Williams18} use Fredman's trick. Several new truly subcubic time algorithms for variants of \APSP{} (e.g. \cite{ChiDXsoda22} and, implicitly, \cite{BringmannGSW16}) and recent algorithms for \ThreeSUM{} (e.g.\ \cite{gronlund2014,chan3sum}) also use it.

A completely different technique is Matou\v{s}ek's truly subcubic time algorithm \cite{MatIPL} for Dominance Product (\Dominance{}). The dominance product of two $n\times n$ matrices $A$ and $B$ is the $n\times n$ matrix $C$ such that for all $i,j\in [n]$,
$C_{ij}= |\{k \in [n]: A_{ik}\leq B_{kj}\}|$. Matou\v{s}ek gave an approach that combined fast matrix multiplication with brute-force to obtain an $\OO(n^{(3+\omega)/2})$ time
algorithm for \Dominance{}. \Dominance{} is known to be equivalent to the so-called Equality Product (\Equality{}) problem\footnote{The equivalence was proven e.g.\ by \cite{labib2019hamming,vnotes}, but also Matou\v{s}ek's algorithm almost immediately works for \Equality{}.} which asks to compute $C_{ij}= |\{k \in [n]: A_{ik}=B_{kj}\}|$, so we sometimes refer to \Equality{} instead.


Matou\v{s}ek's subcubic time techniques have been used to obtain truly subcubic time algorithms for All-Pairs Bottleneck Paths \cite{VassilevskaWY07,duanpettiebott}, All-Pairs Nondecreasing Paths \cite{nondecreasingv,DuanJW19}, \APSP{} in node-weighted graphs \cite{Chan10} and more. Unfortunately, the technique has fallen short when applied directly to the general \APSP{} problem.

In this paper we give a combination of these two techniques that allows us to obtain reductions that exploit fast matrix multiplication in a new way, thus allowing us to overcome many difficulties, such as counting solutions when the number of solutions is large.
The main ideas involve:
(i)~division into ``few-witnesses'' and ``many-witnesses'' cases,
(ii)~standard witness-finding techniques to handle the ``few-witnesses'' case,
(iii)~hitting sets to hit the ``many-witnesses'',
(iv)~Fredman's trick (the obvious equivalence of $a+b=a'+b'$ with $a-a'=b'-b$), and lastly 
(v) ~Matou\v sek's technique for dominance or equality products (which also involves a division into ``low-frequency'' and
``high-frequency'' cases). Steps (iv) and (v) crucially allow us to handle the ``many-witnesses'' case, delicately exploiting the small hitting set from (iii).  Individually, each of these ideas is simple and has appeared before.  But the novelty lies in how they are pieced together (see Sections 
\ref{sec:intapsp-lower-bound},
\ref{sec:counting:preview}, and
\ref{sec:decomposition-zero-tri}), and the realization that these ideas are powerful enough to yield all the new results in the above bullets!



In the remainder of the introduction we give more details on each of the bullets above. 

\subsection{Conditional Lower Bounds for Unweighted Directed APSP, Min-Witness Product and Other Problems with Intermediate Complexity}
\label{sec:interm}
\uAPSP{}  and \MinWitness{} are both ``intermediate'' problems as dubbed by Lincoln, Polak and Vassilevska W.~\cite{lincoln2020monochromatic}, a class of matrix product and all-pairs graph problems whose running times are $\OO(n^{2.5})$ when $\omega = 2$ (and hence right in the middle between the brute-force $n^3$ and and the desired optimal $n^2$), and for which no $O(n^{2.5-\eps})$ time algorithms are known.
More examples of intermediate problems include 
Min-Equality Product and Max-Min Product.
A similar class of ``intermediate'' {\em convolution} problems that are known to be solvable in $\OO(n^{1.5})$ time but not much faster~\cite{lincoln2020monochromatic} includes Max-Min Convolution, Minimum-Witness Convolution,  Min-Equality Convolution, %
and pattern-to-text Hamming distances.
For instance, $\OO(n^{1.5})$-time algorithms were known since the 1980s for Max-Min convolution~\cite{Kosaraju89a} and 
pattern-to-text Hamming distances~\cite{Abrahamson87}, and these remain the fastest algorithms for these problems.

None of these intermediate problems currently have nontrivial conditional lower bounds under standard hypotheses in FGC.  (Two exceptions are All-Edges Monochromatic Triangle (\AEMonoTri{}) and Monochromatic Convolution (\MonoConv{}), where
a near-$n^{2.5}$ lower bound is known for the former under the 3SUM or the APSP Hypothesis~\cite{lincoln2020monochromatic,williamsxumono} and a near-$n^{1.5}$ lower bound is known for the latter under the 3SUM Hypothesis~\cite{lincoln2020monochromatic}, but one may argue that these monochromatic problems are not true matrix product or convolution problems since 
their inputs involve {\em three} matrices or sequences rather than two.)



Thus, an important research direction is to prove super-quadratic conditional lower bounds for intermediate matrix product problems, and super-linear lower bounds for intermediate convolution problems. 
Some relationships are known between intermediate problems, some of which are illustrated in  Figure~\ref{fig:previous}.
However, many questions remain; for instance, it was open whether \uAPSP{} and \MinWitness{} are related.  

\input{previous_work_figure}

As \uAPSP{} and \MinWitness{} are among the ``lowest'' problems in this class, proving conditional lower bounds for these two problems is especially fundamental.
(Besides, \MinWitness{} has been extensively studied, arising in many applications \cite{KowalukLingas, VassilevskaWY10,shapira2011all,CohenY14,KowalukL21}.)
The precise time bounds of the current best algorithms for \uAPSP{} by Zwick~\cite{zwickbridge} and 
\MinWitness{} by Czumaj, Kowaluk and Lingas~\cite{CzumajKL07} are both
$\OO(n^{2+\rho})$, where $\rho\in [1/2, 0.529)$ satisfies\footnote{
$\omega(a, b, c)$ denotes the rectangular matrix multiplication exponent between an $n^a \times n^b$ matrix and an $n^b \times n^c$ matrix.
} $\omega(1,\rho,1)=1+2\rho$.












  
  

To explain the hardness of \uAPSP{}, \MinWitness{}  and more, we 
introduce a strong version of the APSP Hypothesis: %
\begin{hypothesis}[\StrongAPSP]
In the Word-RAM model with $O(\log n)$-bit words, computing APSP for an undirected\footnote{
The version of this hypothesis for directed graphs is equivalent: see Remark~\ref{rmk:strongapsp:dir}.
} graph with edge weights in $[n^{3-\omega}]$ requires $n^{3-o(1)}$ randomized time. 
\end{hypothesis}


For $\omega=2$, the above asserts that the standard  textbook cubic time algorithms for \APSP{} are near-optimal when the edge weights are integers in $[n]$.  
Due to a tight equivalence~\cite{ShoshanZwick} between undirected \APSP{} with edge weights in $[M]$ and \MinPlus{} of two matrices with entries in $[O(M)]$, 
the above hypothesis is equivalent to the following:

\begin{customhypo}{1$'$}[\StrongAPSP{}, restated]
In the Word-RAM model with $O(\log n)$-bit words, computing the Min-Plus product between two $n \times n$ matrices with entries in $[n^{3-\omega}]$ requires $n^{3-o(1)}$ randomized time.
\end{customhypo}


The current upper bound for \MinPlus{} between two $n\times n$ matrices with entries in $[M]$ is $\OO(\min\{Mn^\omega,n^3\})$~\cite{ALONGM1997}, which is cubic for $M=n^{3-\omega}$, and this has not been improved for over 30 years.  


The above strengthening of the APSP Hypothesis is reminiscent of a strengthening of the 3SUM Hypothesis proposed by Amir, Chan, Lewenstein and Lewenstein~\cite{AmirCLL14}, which they called the ``Strong 3SUM-hardness assumption'',
asserting that the 3SUM Convolution problem requires near-quadratic time for integers in $[n]$.  %






We prove the following results:

\begin{theorem}
\label{thm:intro:strong-apsp}
Under the \StrongAPSP{}, 
\uAPSP{} requires $n^{7/3-o(1)}$ time, and
    \MinWitness{} requires $n^{11/5-o(1)}$ time (on a Word-RAM with $O(\log n)$-bit words).
\end{theorem}

In fact, for \uAPSP{}, we can obtain a conditional lower bound of $n^{2+\beta - o(1)}$ for graphs with 
 $n^{1+\beta}$ edges, for any $0 < \beta \le \frac{1}{3}$.  This time bound is \emph{tight} for graphs of such sparsity.  In other words, under the \StrongAPSP{}, 
the naive $O(mn)$ time algorithm for \uAPSP{} with $m$ edges by repeated BFSs is essentially optimal for sufficiently sparse graphs, and fast matrix multiplication helps only for sufficiently large densities.
Such lower bounds for sparse graphs
were known only for \emph{weighted} APSP~\cite{LincolnWW18,AgarwalR18}.
 



Our technique also yields new conditional lower bounds for many other problems, such as All-Pairs Shortest Lightest Paths (\APSLPIntro{})
or All-Pairs Lightest Shortest Paths (\APLSPIntro{}) for undirected small-weighted graphs (which was first studied by Zwick~\cite{ZwickAPLSP}), a batched version of the range mode problem (\BatchMode{})
(which has received considerable recent attention~\cite{CDLMW, WilliamsX20, GuPWX21, GaoHe22, JinX22}), and dynamic shortest paths in planar graphs~\cite{AbbDah}.
See Table~\ref{tab:bounded-min-plus}  for some of the specific results, and Section~\ref{sec:intapsp-lower-bound:more} for more discussion on all these problems.  As demonstrated by the applications to range mode and dynamic planar shortest paths,
our new technique will likely be useful to proving
conditional lower bounds for other data structure problems, %
serving as an alternative to existing techniques based on the Combinatorial BMM Hypothesis (see e.g. \cite{abboud2014popular}) or the OMv Hypothesis~\cite{HenzingerKNS15}.






We also consider conditional lower bounds based on the hardness of \uAPSP{} itself.
The following hypothesis has been proposed by Chan, Vassilevska W. and Xu~\cite{CVXicalp21}:


\begin{hypothesis}[\uAPSPH{}]
In the Word-RAM model with $O(\log n)$-bit words, computing APSP for an $n$-node unweighted directed graph requires at least $n^{2+\rho-o(1)}$ randomized time where $\rho$ is the constant satisfying  $\omega(1,\rho,1)=1+2\rho$.
\end{hypothesis}

As noted in Remark~\ref{rmk:uapsp},
the \uAPSPH{} implies the \StrongAPSP{} if $\omega=2$.
The \StrongAPSP{} is thus more believable in some sense, but the \uAPSPH{} allows us to prove higher lower bounds.
For example,  we prove the following conditional lower bound for \MinWitness{}: 

\begin{theorem}
\label{thm:intro:minwit}
Under the \uAPSPH{}, 
\MinWitness{}
requires $n^{2.223}$ time, or $n^{7/3-o(1)}$ time if $\omega=2$ (on a Word-RAM with $O(\log n)$-bit words).
\end{theorem}


Earlier papers~\cite{lincoln2020monochromatic,CVXicalp21} were unable to obtain such a reduction
from \uAPSP{} to \MinWitness{} (Chan, Vassilevska W. and Xu~\cite{CVXicalp21} were only able to reduce from \uAPSP{} to \MinWitnessEq{}, but not  \MinWitness{}).
We similarly obtain higher lower bounds for all the other problems, as indicated in Table~\ref{tab:bounded-min-plus}.
Our results thus show that the \uAPSPH{} is far
more versatile for proving conditional lower bounds than what previous papers~\cite{lincoln2020monochromatic,CVXicalp21} were able to show.


Lastly, to deal with convolution-type problems, we introduce a similar strong version of the Min-Plus Convolution Hypothesis:

\begin{hypothesis}[\StrongConv{}]
In the Word-RAM model with $O(\log n)$-bit words, Min-Plus convolution between two length $n$ arrays with entries in $[n]$ requires $n^{2-o(1)}$ randomized time. 
\end{hypothesis}



The best algorithm for \MinPlusConv{} with numbers in $[M]$ runs in $\OO(Mn)$ time (by combining Alon, Galil and Margalit's technique~\cite{ALONGM1997} and Fast Fourier transform\@), and no truly subquadratic time algorithm is known for $M=n$. 





We do not know any direct relationship between the \StrongAPSP{} and the \StrongConv{} (although when there are no restrictions on weights, it was known that \MinPlusConv{} reduces to \MinPlus{} \cite{bremner2006necklaces}).





We prove the first conditional lower bounds for one intermediate convolution problem, \MinEqualityConv{}, under the \StrongAPSP{}, the \uAPSPH{} or the \StrongConv{}:


\begin{theorem}\label{thm:intro:convol}
\MinEqualityConv{} requires 
        $n^{1+1/6-o(1)}$ time under the \StrongAPSP{}; or
    $n^{1+\rho/2-o(1)}$ time under the \uAPSPH{}; or
    $n^{1+1/11-o(1)}$ time under the \StrongConv{}. 
\end{theorem}



In the above theorem, the lower bound under the \StrongConv{} is the most interesting, requiring the use of one of our new variants of the BSG Theorem. 

We should emphasize that the significance of Theorem~\ref{thm:intro:convol} stems from the relative rarity of nontrivial lower bounds for convolution and related string problems.  Some problems may have $n^{\omega/2}$ lower bounds by reduction from \BMM{} (e.g.\ there was a well-known reduction from \BMM{} to pattern-to-text Hamming distances, attributed to Indyk -- see also \cite{GawrychowskiU18}), but such bounds become  meaningless when $\omega=2$.  A recent paper~\cite{CVXstoc22} 
obtained 
an $n^{1.5-o(1)}$ lower bound under the OV Hypothesis for a problem called ``pattern-to-text distinct Hamming similarity'', which is far less natural and basic than \MinEqualityConv{}.








\input{bounded_min_plus_table}







\subsection{Equivalence of Counting and Detection}
\label{sec:count}
Here we summarize our main results on counting vs detection.

\paragraph{Equivalence between counting and detection for \ThreeSUM.} In \ThreeSUMCount{} we are given three sets of numbers $A,B,C$ and we want to count the number of triples $a \in A,b\in B,c\in C$ such that $a+b=c$.\footnote{There are several equivalent definitions of \ThreeSUM: the predicate can be $a+b=c$, or $a+b+c=0$; the integers could come from the same set $A$, or the input could consist of three sets $A,B,C$ and we require $a\in A, b\in B, c\in C$. We work with the version with three input sets and predicate $a+b=c$.} More complex variants include \AllThreeSUMCount{} in which we want to count for every $c\in C$, the number of $a\in A,b\in B$ such that $a+b=c$. All of \ThreeSUM, \ThreeSUMCount{} and \AllThreeSUMCount{} can be solved in $O(n^2)$ time, via the folklore algorithm. The 3SUM %
Hypothesis (see \cite{virgisurvey}) asserts that (in the word-RAM model of computation with $O(\log n)$-bit words), $n^{2-o(1)}$ time is needed to solve \ThreeSUM. As no reduction from \ThreeSUMCount{} to \ThreeSUM{} is known till now, a priori it could be that the \ThreeSUM{} Hypothesis is false, but that \ThreeSUMCount{} still requires $n^{2-o(1)}$ time.
We prove:
\begin{theorem}
\ThreeSUMCount{} and \ThreeSUM{} are equivalent under truly subquadratic randomized fine-grained reductions.
\end{theorem}


\paragraph{Counting of \APSP{} and \MinPlus.}
It is known that \APSP{} in $n$-node graphs is equivalent to \MinPlus{} of $n\times n$ matrices \cite{Fischer71}. 
The counting variant \APSPCount{} of \APSP{} asks to count for every pair of nodes in the given graph, the number of shortest paths between them.
The counting variant of \MinPlus{} asks, given two matrices $A,B$ to count for all pairs $i,j$, the number of $k$ such that $A_{ik}+B_{kj}=\min_{\ell} (A_{i\ell}+B_{\ell j})$, i.e. the number of witnesses of the \MinPlus.

While \APSP{} and \MinPlus{} are equivalent,  \APSPCount{} and \MinPlusCount{}  are not known to be. The main issue is that the shortest paths counts can be exponential, and operations on such large numbers are costly in any reasonable model of computation such as the Word-RAM (see the discussion in Section \ref{sec:newalgs}). Because of this the fastest known algorithm for \APSPCount{} is actually {\em quartic} in the number of nodes, and not cubic. While we are able to improve upon the quartic running time (see Section \ref{sec:newalgs}), the running time is still supercubic, so it is unclear whether a tight reduction is possible from \APSPCount{} to \MinPlusCount.

Chan, Vassilevska W. and Xu \cite{CVXicalp21} defined several variants of \APSPCount{} that mitigate the existence of exponential counts. One variant, \APSPCountMod{U} computes the counts modulo any $O(\poly\log n)$-bit integer $U$, and thus no computations with large integers are necessary.  The problem can be solved in $\OO(n^3)$ time, and can be reduced to \MinPlusCount{} (see Appendix~\ref{sec:apsp-count-mod}). 
We prove:
\begin{theorem}
\MinPlusCount{} and \MinPlus{} are equivalent under truly subcubic fine-grained reductions. For any $O(\poly\log n)$-bit integer $U \ge 2$, \APSPCountMod{U} and \APSP{} are equivalent under truly subcubic fine-grained reductions.
\end{theorem}

\paragraph{Counting Exact Triangles.} 
The Exact Triangle Problem (\ExactTri{}) is: given an $n$-node graph with $O(\log n)$-bit integer edge weights, to determine whether the graph contains a triangle whose edge weights sum to some target value $t$. This problem is known to be at least as hard as both \ThreeSUM{} and \APSP{}~\cite{patrascu2010towards,VWfindingcountingj,focsyj}, so that if its brute-force cubic time algorithm can be improved upon, then both the \ThreeSUM{} Hypothesis and the \APSP{} Hypothesis would be false. \ExactTri{} is among the hardest problems in fine-grained complexity.
The counting variant \ExactTriCount{} asks for the number of triangles with weight $t$. We prove:

\begin{theorem}
\ExactTri{} and \ExactTriCount{} are equivalent under truly subcubic fine-grained reductions.
\end{theorem}



Abboud, Feller and Weimann~\cite{AbboudFW20} previously considered the problem of computing the {\em parity} of the number of zero-weight triangles, and it is equivalent to the problem of 
computing the  parity of the number of triangles with weight $t$ for a given $t$. Let's call this {\sf Parity-Exact-Tri}. They showed that \ExactTri{} can be reduced to  {\sf Parity-Exact-Tri} via a randomized subcubic fine-grained reduction. They were {\em not} able to obtain an equivalence, as it is not obvious at all that the decision problem should be able to solve the parity problem. Since \ExactTriCount{} can easily solve {\sf Parity-Exact-Tri} (if one knows the count, one can take it mod $2$), we also get that {\sf Parity-Exact-Tri} is equivalent to \ExactTri.

More equivalence results can be found in Section~\ref{sec:counting} and Appendix~\ref{sec:more_counting}.  See Section~\ref{sec:counting:discuss} for
further discussion on possible implications of our results.

\paragraph{New nondeterministic and quantum counting algorithms.}
Using our new techniques, we can also provide  efficient nondeterministic algorithms for \NegTriCount{} (counting the number of triangles with negative weights), \ExactTriCount{} and \ThreeSUMCount{} even when there are real-valued inputs. 
Ours are the first nondeterministic algorithms for these problems that beat their essentially cubic and quadratic deterministic algorithms.  See Section~\ref{sec:nondet:discuss} for the complexity-theoretic implications of these results.

Our equivalence results can be used to obtain new quantum algorithms.
It is known that \ThreeSUM{} can be solved in  $\OO(n)$ quantum time by an algorithm that uses Grover search (see e.g. \cite{AmbainisL20}). However, it is not clear how to adapt that algorithm to solve \ThreeSUMCount{} since Grover search cannot count the number of solutions exactly. By combining the $\OO(n)$ quantum time \ThreeSUM{} algorithm with our subquadratic equivalence between \ThreeSUM{} and \ThreeSUMCount{} in a black box way, we immediately obtain the first truly subquadratic time quantum algorithm for \ThreeSUMCount{}.



\subsection{New Variants of the BSG Theorem with Algorithmic Applications}
\label{sec:newalgs}



As a way to address question \hyperlink{question:Q4}{Q4}, we formulate a new ``Triangle Decomposition Theorem'', which follows from our techniques.
The theorem %
can be stated roughly as follows:
\begin{quote}
For any weighted $n$-node graph $G$ and parameter $s$, there exist $O(s^3)$ subgraphs
$G^{(\lam)}$ such that the set of all triangles of weight zero (or any fixed target value) in $G$
can be decomposed as the union of the set of all triangles in $G^{(\lam)}$, plus a small remainder set of
$O(n^3/s)$ triangles.  (All triangles in each $G^{(\lam)}$ are guaranteed to have weight zero.)
\end{quote}
Thus, the set of all zero-weight triangles in a graph is highly structured in some sense (in particular,
if there are many zero-weight triangles, one can extract large subgraphs in which all triangles are zero-weight triangles).
See Theorem~\ref{thm:tri:decompose} for the precise statement.
From this theorem, we can easily rederive a subcubic algorithm for monotone Min-Plus product (as shown in Appendix~\ref{app:bd:diff}),
if we don't care about optimizing the exponent in the running time.  
The theorem also leads to several new algorithms, as listed below.

This Triangle Decomposition Theorem resembles a covering version of the BSG Theorem, as formulated
by Chan and Lewenstein~\cite{ChanLewenstein}, which can be roughly stated as follows:
\begin{quote}
For any sets $A,B,C$ in an abelian group and parameter $s$, there exist $O(s)$ pairs
of subsets $(A^{(\lam)},B^{(\lam)})$ such that $\{(a,b)\in A\times B: a+b\in C\}$ can be covered
by the union of $A^{(\lam)}\times B^{(\lam)}$, plus a small remainder set of $O(n^2/s)$ pairs,
where the total size of the sum sets\footnote{
Throughout the paper, $A+B$ denotes the sum set $\{a+b: a\in A,\, b\in B\}$, and $A-B$ denotes the difference set $\{a-b: a\in A,\, b\in B\}$.
}
$A^{(\lam)} + B^{(\lam)}$ is $O(s^6 n)$.
\end{quote}
Thus, the set of all solutions to \ThreeSUM{} is highly structured in some sense (in particular,
if there are many solutions to \ThreeSUM{}, one can extract  pairs of large subsets $(A^{(\lam)},B^{(\lam)})$ whose
sum sets are small).
See Section~\ref{sec:bsg} for the precise statement and for more background on the BSG Theorem.

We show that 
our approach, combined with some extra ideas, can actually prove a form of the BSG Theorem, namely,
with the above $O(s^6 n)$ bound replaced by $\OO(s^2 n^{3/2})$.  At first, this new bound appears weaker---indeed,
researchers in additive combinatorics were  concerned more with obtaining a linear bound in $n$ on the sum set size, and less with the dependency on $s$.  However, Chan and Lewenstein's algorithmic applications require
nonconstant choices of $s$, and the new bound lowering the $s^6$ factor to $s^2$ turns out to yield better results
in at least one application below
(namely, \ThreeSUM{} with preprocessed universes).





We now mention a few applications of the new theorems:

\paragraph{\#APSP for arbitrary weighted graphs.}
Recall the \APSPCount{} problem
(counting the number of shortest paths from $u$ to $v$,
for every pair of nodes $u,v$ in a weighted graph).  This basic problem has applications
to %
\emph{betweenness centrality}.
As  mentioned earlier, because counts may be exponentially big,
known algorithms run in near-$n^4$ time
in the
standard Word-RAM model with $O(\log n)$-bit words.
An $\OO(n^3)$-time
algorithm for \APSPCount{} was given by Chan, Vassilevska W. and Xu~\cite{CVXicalp21}, but only for \emph{unweighted} graphs.
We give the first truly subquartic \APSPCount{} algorithm for  arbitrary weighted graphs with running time $O(n^{3.83})$
(or $\OO(n^{15/4})$ if $\omega=2$).

\paragraph{3SUM in preprocessed universes.}
Chan and Lewenstein~\cite{ChanLewenstein} studied a ``preprocessed universe'' setting for the \ThreeSUM{}
problem: preprocess three sets $A,B,C$ of $n$ integers (the \emph{universes})
so that for any given query subsets $A'\subseteq A$, $B'\subseteq B$, and $C'\subseteq C$, we can
solve \ThreeSUM{} on the instance $(A',B',C')$ more quickly than from scratch.
(The problem was first stated by Bansal and Williams~\cite{BansalW12}.)
Chan and Lewenstein showed intriguingly that after preprocessing the universes in $\OO(n^2)$ expected time,
\ThreeSUM{} on the given query subsets can be solved in time truly subquadratic in $n$, namely, $\OO(n^{13/7})$.
Our techniques yield a new, simpler solution with $\OO(n^2)$ expected preprocessing time and
an improved query time of $\OO(n^{11/6})$.  Furthermore, with the same $\OO(n^{11/6})$ query time,
the new algorithm can solve a slight generalization
where the query subset $C'$ can be an arbitrary set of $n$ integers (i.e., the universe
$C$ needs not be specified).  Here, our improvement is even bigger:
Chan and Lewenstein's previous solution for this generalized case
required $\OO(n^{19/10})$ query time.

We also obtain the first
result for the analogous problem of \ExactTri{} in preprocessed universes:
 we can preprocess any weighted $n$-node graph in $\OO(n^3)$ time, so that
for any given query subgraph, we can solve \ExactTri{} in time truly subcubic in $n$,
namely, $O(n^{2.83})$ (or $\OO(n^{11/4})$ if $\omega=2$).
This result can be viewed as a generalization of the result for \ThreeSUM{}, by 
known reductions from \ExactTri{} to \ThreeSUM{} (for integers).


\paragraph{3SUM for monotone sets in $[n]^d$.}
Chan and Lewenstein~\cite{ChanLewenstein} also studied a special case of \ThreeSUM{} for
monotone sets $A,B,C$ in $[n]^d$ for a constant dimension~$d$, where a set of points is \emph{monotone} if it is of the form $\{a_1,\ldots,a_n\}$ where the $j$-th coordinates of  $a_1,\ldots,a_n$ is a monotone sequence for each $j$.  The problem is related to
an important special case of Min-Plus convolution for monotone sequences in $[n]$ (or integer sequences with bounded differences), which reduces to  monotone \ThreeSUM{} in 2 dimensions.  This is also  related to a data structure problem for strings known as \emph{jumbled indexing}, where %
$d$ corresponds to the alphabet size.  Chan and Lewenstein gave the first truly subquadratic algorithm for the problem, with running time of the form $O(n^{2-1/(d+O(1))})$ using randomization.  (See \cite{AmirCLL14,HsuUmans17} for conditional lower bounds on this and related problems.)  However, they obtained subquadratic deterministic algorithms only for %
$d\le 7$ under the current fast matrix multiplication bounds.  Our techniques give the first \emph{deterministic} algorithm for all constant $d$, with running time $O(n^{2-1/O(d)})$.
Although the new bound is not better (and for monotone Min-Plus convolution, a recent paper by Chi, Duan, Xie and Zhang~\cite{ChiDXZstoc22} presented even faster randomized algorithms), the new approach is simpler, besides being deterministic.










\subsection{Paper Organization}

In Section~\ref{sec:prelim}, we define notations and problems. 
The rest of the paper has three main threads:

\begin{itemize}
\item \emph{Conditional lower bounds for problems with intermediate complexity}:
In Section~\ref{sec:intapsp-lower-bound}, we illustrate our approach by proving
the first superquadratic lower bound for \uAPSP{} under the \StrongAPSP{}.
In Sections~\ref{sec:intapsp-lower-bound:more}--\ref{sec:uapsp-lower-bound}, we prove lower bounds for other problems, including
\MinWitness{}, under both the \StrongAPSP{} 
 and \uAPSPH{}.
\item \emph{Equivalences between counting and detection problems}:
In Section~\ref{sec:counting:preview}, we illustrate our basic idea by proving
the subcubic equivalence between \ExactTriCount{} and \ExactTri{}.  In Section~\ref{sec:counting}, we prove more results of this kind, including the subquadratic equivalence between \ThreeSUMCount{} and \ThreeSUM{}.  (Still more examples can be
found in Appendix~\ref{sec:more_counting}\@.)  
In Section~\ref{sec:other_models}, we further adapt these ideas to obtain new nondeterministic and quantum algorithms for counting problems.
\item \emph{BSG-related theorems}: In Section~\ref{sec:decomposition-zero-tri}, we present our new Triangle Decomposition Theorem and describe its applications. 
In Section~\ref{sec:bsg}, we describe our new variants of the BSG theorem.  (Still more applications and variants can be found in Appendices~\ref{app:bd:diff} and \ref{app:bsg}\@.)  Finally, in Section~\ref{sec:min-equal-conv}, we prove the conditional lower bounds for \MinEqualityConv{}, the most sophisticated of which use one of our new BSG theorems. 
\end{itemize}

Although the paper is lengthy,
Sections~\ref{sec:intapsp-lower-bound}, \ref{sec:counting:preview} and
\ref{sec:decomposition-zero-tri} should suffice to give the readers an overview of our main proof techniques.  (Readers interested in diving deeper into any of the above threads may proceed to the subsequent sections.)  




