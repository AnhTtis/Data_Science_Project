\section{Preliminaries}
\label{sec:preliminaries}

\begin{definition}[Approximate counting]\label{def:stream_approx_count}
Let $c,n>1$. In problem $\prac$,
the input is a stream of $l\leq (c+1)n$ identical items.
The goal is to output $0$ if $l\leq n$ and $1$ if $l>cn$ 
(and otherwise the output can be either $0$ or $1$). 
\end{definition}


Let $A$ be a PD algorithm for problem $\prac$,
and let $F:[0,(c+1)n]\to\set{0,1}$ be the canonical function of $A$.
Thus, there is a fixed string $P\in\{0,1\}^{(c-1)n}$ such that
\[
F(x) = 
\begin{cases}
0 & \text{if } x\in[0,n]; \\
1 & \text{if } x\in [cn+1,(c+1)n]; \\
P(x-n) & \text{otherwise.}
\end{cases}
\]
For $s^*\in[0,n]$, let $F_{s^*}:[0,(c+1)n-s^*]\to\set{0,1}$ be a shifted version of $F$,
namely the function $F_{s^*}: x\mapsto F(s^*+x)$.
We use these notations throughout the paper.


Our proofs are based on a reduction
from a simple one-way communication problem, called MESSAGE and denoted $\prmsg$,
where Alice's input $x$ is from an alphabet $\Sigma$ that is fixed in advance,
Bob has no input,
and the goal is that Bob outputs $x$ with probability at least $2/3$.
It is well known that this problem requires $\Omega(\log |\Sigma|)$ bits of communication,
even for randomized protocols using shared randomness.
We provide a proof for completeness.


\begin{lemma}\label{lem:comm_problem_message}
    For every alphabet $\Sigma$, every one-way communication protocol (even with shared randomness) for problem $\prmsg$ must use $\Omega(\log |\Sigma|)$ bits of communication.
\end{lemma}
\begin{proof}
    Let $\mathcal{A}$ be a protocol for problem $\prmsg$.
    For a random string $r$ representing the randomness of $\mathcal{A}$, let $\Sigma_r\subset \Sigma$ be the set of all $s\in \Sigma$ for which Bob correctly recovers $s$.
    % 
    Let $r^*$ be a string maximizing $|\Sigma_r|$,
    then by averaging, $|\Sigma_{r^*}|\geq \tfrac{2}{3}|\Sigma|$.
    Consider an instance of $\mathcal{A}$ that uses $r^*$ as its random string.
    Assume by contradiction that the number of communication bits is less than $\log |\Sigma_{r^*}|$,
    then by the pigeonhole principle there are two distinct inputs $s, s'\in \Sigma_{r^*}$
    such that $\mathcal{A}(s)$ and $\mathcal{A}(s')$ result in the same message.
    Bob then
    cannot distinguish between (i.e., has the same output distribution for) $s$ and $s'$, a contradiction.
    Hence, the number of bits of communication is at least $\log |\Sigma_{r^*}| = \Omega(\log |\Sigma|)$.
\end{proof}


  
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:


