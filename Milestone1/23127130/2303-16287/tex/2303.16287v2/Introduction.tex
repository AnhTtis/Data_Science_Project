\section{Introduction}
\label{sec:intro} 

Computing over data streams is a rich algorithmic area
that has developed enormously, 
and actually started with the simple-looking problem 
of approximate counting~\cite{DBLP:journals/cacm/Morris78a}. 
%
Let us first recall the streaming model:
The input is a stream, i.e., a sequence of items, 
and the goal is to compute a pre-defined function of these items, 
such as the number of items (or number of the distinct items),
while making one sequential pass over the stream (or sometimes a few passes). 
Many useful functions actually depend on the items as a multiset,
i.e., ignoring their order, or even only on their frequencies 
(like the famous $\ell_p$-norm of the frequency vector).
Another possible goal is to produce a sample, rather than computing a function,
e.g., to produce a uniformly random item.

The primary measure of efficiency for streaming algorithms
is their space complexity,
and for many problems, researchers have designed space-efficient algorithms, 
often with space complexity that is even polylogarithmic in the input size.
However, this comes at a price ---
these algorithms are usually randomized (and not deterministic)
and/or compute an approximate solution (rather than exact one).
In fact, oftentimes both relaxations are needed in order to achieve low space complexity.  
For example, to count the number of items in a stream of length at most $n$,
there is a
randomized approximation algorithm 
using $O(\loglog n)$ bits of space,
but algorithms that are exact or deterministic must use $\Omega(\log n)$ bits~\cite{DBLP:journals/cacm/Morris78a}. 
Another example is the $\ell_2$-norm of the frequency vector of items from a ground set $[d]$
(or equivalently, of a $d$-dimensional vector under a sequence of additive updates) 
---
there is a randomized approximation algorithm that uses $O(\log d)$ bits of space,
% 
but algorithms that are exact or deterministic must use $\Omega(d)$ bits of space~\cite{DBLP:conf/stoc/AlonMS96}. 

%


Gat and Goldwasser~\cite{DBLP:journals/eccc/GatG11} initiated
the study of \emph{pseudo-deterministic} algorithms,
which informally means that when run (again) on the same input,
with high probability they produce exactly the same output.
%
This notion combats a potential issue with randomized algorithms,
that independent executions on the same input might return different outputs,
depending on the algorithm's coin tosses.
Many known streaming algorithms suffer from this issue,
which is a serious concern for some users and applications. 
Pseudo-deterministic algorithms were later considered in the streaming model
by Goldwasser, Grossman, Mohanty and Woodruff~\cite{DBLP:conf/innovations/GoldwasserGMW20},
and these are formally defined as follows.


\begin{definition}\label{def:PD_streaming}
A streaming algorithm $A$ is \emph{pseudo-deterministic} (PD)
if there is a function $F(\cdot)$ defined on inputs of $A$ (streams),
such that for every stream $\sigma$,
$$
  \P [A(\sigma)=F(\sigma)] \geq 9/10,
$$
where the probability is over the random choices of the algorithm.
We shall refer to $F$ as the \emph{canonical function} of algorithm $A$.%
\footnote{
% 
The canonical function $F$ depends on the order arrival of the stream items.
In an alternative definition, the canonical function depends on the items only as a multiset, i.e., ignoring their order in the stream. % $I_\sigma$.
% 
These two definitions are equivalent in the setting of approximate counting, which is the focus of our work.
}
\end{definition}



We focus on
\emph{estimation problems},
which ask to approximate a numerical value, 
and are very popular in the streaming model.
%
For such problems, the notion of PD relaxes the exact setting and the deterministic one,
% 
since exact algorithms have one canonical output (the exact numerical value), and hence they are PD.
Thus the known lower bounds for these settings do not apply for PD algorithms,
and a central question, identified in~\cite{DBLP:conf/innovations/GoldwasserGMW20}, remains open:

\begin{center}
  \emph{Are there efficient PD streaming algorithms for estimation problems?}
\end{center}



Currently, no lower bounds are known for natural
% 
estimation problems,
although for several search problems,
like reporting an element from a stream with deletions
(equivalently, an index from the support of the frequency vector), 
it is known that lower bounds for deterministic algorithms
extend to PD algorithms~\cite{DBLP:conf/innovations/GoldwasserGMW20}.


\subsection{Main Result: Approximate Counting}

Perhaps the most basic problem in the streaming model is to count the number of stream items.
% 
Exact counting, i.e., computing the number of items exactly, 
requires $\Theta(\log n)$ bits of space when the stream has length at most $n$,
even for randomized algorithms with some error probability.
%
Work by Morris~\cite{DBLP:journals/cacm/Morris78a}, later refined in~\cite{DBLP:journals/bit/Flajolet85,DBLP:journals/mst/GronemeierS09,NelsonYu22},
showed that the number of stream items
can be $(1+\epsilon)$-approximated with probability $9/10$
using $O_\epsilon(\loglog n)$ bits of space,
where $\epsilon>0$ is arbitrary but fixed.
Throughout, we refer to multiplicative approximation,
and use the notations $O_c(\cdot)$ and $\Omega_c(\cdot)$
to hide factors that are polynomial in $c$. 
% 
Morris's algorithm has found many applications,
both in theory and in practice~\cite{DBLP:journals/corr/abs-1805-00612_Lumbroso,NelsonYu22}.
% 
An open question stated explicitly by Goldwasser, Grossman, Mohanty and Woodruff~\cite{DBLP:conf/innovations/GoldwasserGMW20} is 
whether there is a PD algorithm for this problem using $O(\loglog n)$ bits of space.
We answer their question negatively, by proving the following lower bound. 

\begin{theorem}[Main Result]\label{cor:LB_PD_counting}%maybe bad name for the label
For every $c,n>1$,
every PD streaming algorithm that $c$-approximates the number of items in a stream of length at most $(c+1)n$
must use $\Omega_c(\sqrt{{\log n}/{\loglog n}})$ bits of space.
\end{theorem}


To be more precise, our lower bound is actually 
$\Omega\big(\tfrac{\log n}{\sqrt{\log n\loglog (cn)} + \log c}\big)$,
which is still $\Omega(\sqrt{\tfrac{\log n}{\loglog n}})$
as long as $c<2^{\sqrt{\log n \loglog n}}$. 
Previously, there was a large gap for this problem,
% 
between $O(\log n)$ bits (by a deterministic algorithm)
and $\Omega(\loglog n)$ bits (from the randomized setting)~\cite{NelsonYu22}.
See \Cref{table:Approx_Count_Bounds} for a summary of the known bounds.



\begin{table*}[t] % or [h]?
\caption{
\label{table:Approx_Count_Bounds} 
Known space bounds (in bits) for $2$-approximate counting in a stream of length at most $n$.
Folklore bounds are stated without a reference. 
}
\begin{center}
\begin{tabulary}{\textwidth}{|L  | rl | rl|}
\hline
Algorithms
& \multicolumn{2}{c|}{Upper bound}
&  \multicolumn{2}{c|}{Lower bound}
\\ 
\hline
Exact or deterministic
& $O(\log n)$ &
& $\Omega(\log n)$ & \\ 
Randomized and approximate
& $O(\loglog n)$ & \cite{DBLP:journals/cacm/Morris78a}
& $\Omega(\loglog n)$ & \cite{NelsonYu22}
\\
Pseudo-deterministic
& $O(\log n)$  & 
& $\Omega(\sqrt{{\log n}/{\loglog n}})$ & [Thm.~\ref{cor:LB_PD_counting}] \\
Pseudo-deterministic &               &
& $\Omega(\log n)$ & \cite{grossman2023tight} \\
\hline
\end{tabulary}
\end{center}
\end{table*}


Our proof analyzes the promise variant 
of $c$-approximate counting for streams of length at most $(c+1)n$,
which we denote by $\prac$;
this variant asks to distinguish whether the number of stream items
is $\leq n$ or $>cn$ (see Definition \hyperref[sec:preliminaries]{2.1}). % BUG: \Cref{def:stream_approx_count}) lead to the wrong place
A crucial property of PD algorithms is that they
have to be PD also for inputs in the range $[n+1,cn]$ (i.e., outside the promise).
We rely on this property of PD algorithms to prove the following result,
which immediately yields \Cref{cor:LB_PD_counting} as a corollary.


\begin{theorem}[Main Result]\label{thm:LB_PD_counting}
For every $c,n>1$, 
every PD streaming algorithm for problem $\prac$ must use $\Omega_c(\sqrt{ {\log n}/{\loglog n}})$ bits of space.
\end{theorem}

Our proof of Theorem~\ref{thm:LB_PD_counting}
appears in Section~\ref{sec:LB_PD_counting}.
It is based on a problem that we call Shift Finding,
which may be of independent interest, 
as it is very natural and likely to find connections to other problems.
In addition, it can potentially lead to a near-tight
$\Omega(\log n/\loglog n)$ lower bound for PD streaming,
by simply improving our algorithmic result for Shift Finding. 
%
A very recent independent work by
Grossman, Gupta and Sellke~\cite{grossman2023tight} 
shows a tight $\Omega(\log n)$ bound for $\prac$,
using a very different technique,
which views the PD streaming algorithm as a Markov chain with a limited number of states. 


\subsection{Main Technique: The Shift Finding Problem}

Our main result relies on \emph{algorithms} for the shift Finding problem $\prsf$, which is defined below. 
Let us first introduce some basic terminology.
A function $F:[m]\to\{0,1\}$ can also be viewed as a string $F\in\{0,1\}^m$, 
and vice versa, and we sometimes use these interchangeably. 
Given $s\in[0,n]$, let the shifted version of this $F$
be the function $F_{s}: x\mapsto F(s+x)$, with a properly restricted domain,
see \Cref{sec:preliminaries}. 


\begin{definition}[Shift Finding] \label{def:shift_finding}
Let $c,n>1$.
In problem $\prsf$,
the input is a string $P\in \{0,1\}^{(c-1)n}$,
and one has query access to a string $F_{s^*}$ that is the concatenation
of $n-s^*$ zeros, then $P$, and finally $s^*$ ones, 
for an unknown $s^*\in [0,n]$. 
Thus, a query for $x\in[0,cn]$ returns $F_{s^*}(x)$.
The goal is to output $s^*$. 
\end{definition}

The measure of complexity of an algorithm for this problem
is the number of queries that it makes to $F_{s^*}$.
A randomized algorithm is required to be correct (in its output $s^*$)
with probability $9/10$.



This problem may be also of independent interest.
% 
In a different variant of shift finding,
the input is a random string $c\in\set{0,1}^n$ and a vector $x$ that is obtained from the string $c$ by a cyclic shift $\tau$ and some noise (random bit flips),
and the goal is to compute the shift $\tau$ with high probability.
% 
This problem is related to GPS synchronization, see~\cite{DBLP:conf/mobicom/HassaniehAKI12,DBLP:conf/soda/AndoniIKH13} for more details.
There is a sublinear time algorithm for this problem, running in time roughly $O(n^{0.641})$~\cite{DBLP:conf/soda/AndoniIKH13}.
One main difference is that in our \Cref{def:shift_finding},
one string is completely known to the algorithm, and the only concern is the number of queries to the second string.



\subsubsection{Connection to PD Counting}

We show that an algorithm for Shift Finding ($\prsf$) implies
a space lower bound for PD streaming algorithm for counting ($\prac$).

\begin{theorem}\label{thm:connection_Shift_Counting}
Let $c,n>1$, and suppose that the Shift Finding problem $\prsf$
admits a randomized algorithm that makes at most $q=q(c,n)$ queries (possibly adaptive).
Then, every PD streaming algorithm for the approximate counting problem $\prac$
must use $\Omega(\tfrac{\log n}{\log q})$ bits of space.
\end{theorem}


It immediately follows that if the Shift Finding problem $\prsf$ can be solved using $\polylog(n)$ queries (for fixed $c>1$), then PD approximate counting requires $\Omega(\tfrac{\log n}{\loglog n})$ bits of space.
However, our current upper bound for Shift Finding is $q=O(\sqrt{cn})$ queries (\Cref{thm:shift_finding_alg}) and is not strong enough to yield a nontrivial lower bound for PD approximate counting. 

Therefore, to prove our main lower bound (\Cref{thm:LB_PD_counting}),
we revert to a generalization of \Cref{thm:connection_Shift_Counting}
where the Shift Finding algorithm is still given an instance of problem $\prsf$
(namely, a string $F$ and query access to $F_{s^*}$), 
but reports a small set $R\subset[0,n]$ (say of size $|R|\leq t$) 
that contains the unknown shift (i.e., $s^*\in R$).
This algorithm may be randomized provided that it is PD,
and its canonical function maps each instance of problem $\prsf$
to a set $R$ of size $t$ that contains $s^*$.


\begin{theorem}\label{thm:generalized_connection_Shift_Counting}
Let $c,n>1$,
and suppose there is a PD algorithm $Q$ that,
given an instance of problem $\prsf$,
makes at most $q=q(c,n)$ queries (possibly adaptive) to $F_{s^*}$
and its canonical function $M$ maps the input to a set
$R\subset [0,n]$ of size $t=t_c(n)$ that contains $s^*$.
Then every PD streaming algorithm for problem $\prac$ must use
$\Omega(\tfrac{\log (n/t)}{\log q})$ bits of space.
\end{theorem}

We use \Cref{thm:generalized_connection_Shift_Counting},
(more precisely its proof arguments rather than its statement) 
to prove our main result (\Cref{thm:LB_PD_counting}),
see \Cref{sec:LB_PD_counting}.
At a high level,
the proof of \Cref{thm:LB_PD_counting} proceeds by splitting into two cases,
depending on the canonical function $F$.
Roughly speaking, in one case we show a Shift Finding algorithm
that returns a set of size $t=n/2^{\sqrt{\log n}}$
% the relaxed version of Shift Finding is solved 
using $q=O(\log n)$ queries by binary search,
and in the other case an algorithm to find the shift (i.e., $t=1$)
with probability $9/10$ using $q=2^{\sqrt{\log n}}$ uniformly random queries.



As a corollary of \Cref{thm:connection_Shift_Counting},
we get that the \emph{tracking} version of approximate counting
must use $\Omega(\log n)$ bits of space,
which is tight with a straightforward deterministic counting.
Tracking means that the algorithm produces an output
% 
after every stream item rather than at the end of the stream,
and with probability $9/10$, all the outputs are simultaneously correct (i.e., approximate the number of items seen so far).

\begin{corollary}[Tracking]\label{thm:LB_PD_tracking_counting}
For every $c,n>1$, every PD tracking algorithm that $c$-approximates the number of items in a stream of length $(c+1)n$ must use $\Omega(\log n)$ bits of space.
\end{corollary}

In contrast, for standard randomized algorithms, 
there is a tracking algorithm for  $(1+\epsilon)$-approximate counting that uses $O_\epsilon(\loglog n)$ bits of space, for any fixed $\epsilon>0$ \cite{NelsonYu22}.
% 
\Cref{thm:LB_PD_tracking_counting} follows by an easy modification of the proof of \Cref{thm:connection_Shift_Counting}.
That proof uses $O(\log q)$ repetitions of a PD streaming algorithm,
and then employs a union bound on $q$ input streams,
which is not necessary for tracking algorithms and thus the bound follows.
%

A more direct argument is essentially by equivalence to exact counting. 
For a stream with $s<n$ items, 
the state of a PD tracking algorithm with canonical function $F$
can be used to compute $s$, as follows.
Simulate insertion of more items to the stream until the output of the algorithm changes to $1$ (which corresponds to the first $1$ in $F_s$), from which $s$ can be computed.





\subsubsection{An Algorithm for Shift Finding}

Consider a special case of the Shift Finding problem $\prsf$,
where the input string $P$
is a run of zeros followed by a run of ones (viewed as a function, it is a step function);
then the algorithm can perform a binary search using $O(\log (cn))$ queries,
and find the unique location where $F_{s^*}$ switches from value $0$ to $1$,
and hence recover $s^*$. 
At the other extreme, suppose the input string $P$ is random;
then with high probability
every set of $O(\log n)$ queries from $P$ (and thus from $F_{s^*}$)
will be answered differently (viewed as a string in $\{0,1\}^{O(\log n)}$). 
Based on these observations, one may hope that problem $\prsf$
admits an algorithm that makes $\polylog (cn)$ queries.
We leave this as an open question
and prove a weaker bound of $O(\sqrt{cn})$ queries.

\begin{theorem}[Shift Finding Algorithm]\label{thm:shift_finding_alg}
There is a deterministic algorithm for problem $\prsf$ that makes $O(\sqrt{cn})$ queries.
\end{theorem}

A key observation in our result, that may be useful in future work,
is that for every shift $s^*$
there is a ``short witness'' that uses exactly $2$ queries.
We formalize this as verifying a given guess $s$ for the shift $s^*$. 

\begin{lemma}[Short Witness]\label{lem:shift_verifier_witness_2queries}
There is a deterministic algorithm that,
given as input an instance of problem $\prsf$ and $s<n$,
makes $2$ queries to $F_{s^*}$ and returns 'yes' if $s=s^*$ and 'no' otherwise.
\end{lemma}

The proofs of \Cref{thm:shift_finding_alg} and \Cref{lem:shift_verifier_witness_2queries} appear in \Cref{sec:shift_finding_alg}.
At a high level, 
the Shift Finding algorithm in \Cref{thm:shift_finding_alg} queries the set $\{F_{s^*}(0),F_{s^*}(\sqrt{cn}),F_{s^*}(2\sqrt{cn}),...,F_{s^*}(cn)\}$, and then uses the short witness (\Cref{lem:shift_verifier_witness_2queries}) to check every feasible $s\in [n]$ (i.e., that agrees with the query answers).
Following an observation by Peter Kiss, we are able to improve our Shift Finding algorithm to use only $O((cn)^{1/3}\log n)$ queries; details omitted.


\subsection{Related Work}
\label{sec:related}

\paragraph*{Pseudo-deterministic algorithms}
The notion of pseudo-deterministic algorithms was introduced by~\cite{DBLP:journals/eccc/GatG11} (they originally called them Bellagio algorithms), followed by a long sequence of works that studied it in different models~\cite{DBLP:conf/innovations/GoldreichGR13,DBLP:journals/eccc/Grossman15,DBLP:journals/eccc/GoldwasserG15,DBLP:conf/stoc/OliveiraS17,DBLP:journals/corr/Holden17,DBLP:conf/innovations/GoldwasserGH18,DBLP:conf/mfcs/DixonPV18,DBLP:conf/approx/OliveiraS18,DBLP:journals/corr/abs-1910-00994_GoemansGH2019,DBLP:conf/soda/GrossmanL19,DBLP:journals/eccc/Goldreich19,DBLP:conf/innovations/GoldwasserGMW20,DBLP:conf/stoc/LuOS21,DBLP:conf/innovations/00020V21,DBLP:conf/coco/GoldwasserIPS21,DBLP:conf/approx/GhoshG21,DBLP:conf/stoc/0002PWV22}.
In the streaming and sketching models, \cite{DBLP:conf/innovations/GoldwasserGMW20} proved strong lower bounds for finding a non-zero entry in a vector (given in a stream with deletions), and for sketching $\ell_2$-norms.
Another related setting is that of
sublinear time computation. 
Under certain assumptions, PD algorithms (in the sublinear time region) were shown to admit the following relation with deterministic algorithms -- if for a certain problem there is a PD algorithm using $q$ queries, then there is a deterministic algorithm using $O(q^4)$ queries~\cite{DBLP:conf/innovations/GoldreichGR13}.
The techniques of~\cite{DBLP:conf/innovations/GoldreichGR13} do not seem to extend to streaming algorithms.



\paragraph*{Adaptive adversarial streams}
In this setting, the stream items are chosen adversarially and depend on past outputs of the streaming algorithm (i.e., the stream is adaptive)~\cite{DBLP:journals/jacm/Ben-EliezerJWY22}.
This model is considered to be between PD algorithms and the standard randomized setting, in the sense that for streams of length $m$, amplifying a PD algorithm to success probability $1-\tfrac{1}{10m}$ (by $O(\log m)$ repetitions and taking the median) guarantees (by a union bound) that the algorithm outputs the canonical solution after every stream item with probability $9/10$, thus the adversary acts as an oblivious one (the adversary knows in advance the output of the streaming algorithm, which is the canonical function).
% 
For approximate counting, adaptive streams and standard (oblivious) streams are equivalent (since the stream items are identical) and thus admit an algorithm using $O(\loglog n)$ bits of space.


There is a vast body of work designing algorithms for adaptive streams, 
% 
but not much is known in terms of lower bounds.
%
Lower bounds are known for some search problems, like finding a spanning forest in a graph undergoing edge insertions and deletions, but also for graph coloring~\cite{DBLP:conf/innovations/ChakrabartiGS22}.
% 
Regarding estimation problems, the only lower bound we are aware of is
for some artificial problem~\cite{DBLP:conf/crypto/KaplanMNS21}.  
%
Recently, Stoeckl~\cite{doi:10.1137/1.9781611977554.ch32_Stoeckl} showed a lower bound on streaming algorithms that use a bounded amount of randomness, conditioned on a lower bound for PD algorithms.
%
In the related model of linear sketching, Hardt and Woodruff~\cite{10.1145/2488608.2488624_HW13} 
showed lower bounds on the dimensions of sketching algorithms, which
% 
applies to many classical problems, like $\ell_p$-norm estimation and heavy hitters.










%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:


