\section{Introduction}
\label{sec:intro} 

Computing over data streams is a rich algorithmic area
%\cite{DBLP:journals/cacm/Morris78a,DBLP:journals/toms/Vitter85,DBLP:conf/focs/FlajoletM83,DBLP:conf/stoc/AlonMS96,DBLP:conf/icalp/CharikarCF02,DBLP:journals/jal/CormodeM05,DBLP:conf/stoc/Indyk04,DBLP:journals/ijcga/FrahlingIS08,DBLP:conf/pods/JowhariST11,DBLP:conf/soda/AhnGM12a,DBLP:conf/pods/AhnGM12b,DBLP:conf/stoc/ClarksonW09} (also see surveys~\cite{DBLP:journals/fttcs/Muthukrishnan05,DBLP:journals/sigmod/McGregor14,cormode_yi_2020}).
% \aknote{We should cite more here.}
% \ssnote{Done.}
% \rnote{Actually, I think such ``generic'' references are useless, so I commented them out, and added instead a brief overview slightly geared towards ``our'' context. }
%that has generated powerful algorithmic techniques, 
that has developed enormously, 
and actually started with the simple-looking problem 
of approximate counting~\cite{DBLP:journals/cacm/Morris78a}. 
%
Let us first recall the streaming model:
The input is a stream, i.e., a sequence of items, 
and the goal is to compute a pre-defined function of these items, 
such as the number of items (or number of the distinct items),
while making one sequential pass over the stream (or sometimes a few passes). 
Many useful functions actually depend on the items as a multiset,
i.e., ignoring their order, or even only on their frequencies 
(like the famous $\ell_p$-norm of the frequency vector).
Another possible goal is to produce a sample, rather than computing a function,
e.g., to produce a uniformly random item.

The primary measure of efficiency for streaming algorithms
is their space complexity,
%
% For this reason, problems in the streaming model are primarily concerned with the space complexity of algorithms solving them.
% \vbnote{I don't like the phrase "problems are concerned with space complexity of solving the problems", sounds cyclical. Maybe "the streaming model is primarily concerned with space complexity of approximating functions"}
and for many problems, researchers have designed space-efficient algorithms, 
often with space complexity that is even polylogarithmic in the input size.
However, this comes at a price ---
these algorithms are usually randomized (and not deterministic)
and/or compute an approximate solution (rather than exact one).
In fact, oftentimes both relaxations are needed in order to achieve low space complexity.  
% Algorithms for problems in the streaming model usually fall under two kinds of categories, exact or approximate and deterministic or randomized\vbnote{This is true for general algorithms, not sure why we need this sentence. I would delete it and just keep the next one.}.
% Often approaches focus on randomized and approximate solutions since exact solutions or deterministic ones
% must use significantly more space for many problems. 
%
For example, to count the number of items in a stream of length at most $n$,
there is a
randomized approximation algorithm %s can solve the problem
using $O(\loglog n)$ bits of space,
but algorithms that are exact or deterministic must use $\Omega(\log n)$ bits~\cite{DBLP:journals/cacm/Morris78a}. 
% \ssnote{I think the credits should apply also to the lower bounds (and not only to the upper bounds)? At least for AMS?}
%
Another example is the $\ell_2$-norm of the frequency vector of items from a ground set $[d]$
(or equivalently, of a $d$-dimensional vector under a sequence of additive updates) 
% \ssnote{I think the first description is difficult to parse, but the one in the brackets is fine. I don't know how to improve the first one.}
% \rnote{I changed the wording slightly }
---
there is a randomized approximation algorithm that uses $O(\log d)$ bits of space,
% \rnote{``space bits'' sounds awkward, please change throughout to ``bits of space''}
but algorithms that are exact or deterministic must use $\Omega(d)$ bits of space~\cite{DBLP:conf/stoc/AlonMS96}. 

%


Gat and Goldwasser~\cite{DBLP:journals/eccc/GatG11} initiated
the study of \emph{pseudo-deterministic} algorithms,
which informally means that when run (again) on the same input,
with high probability they produce exactly the same output.
%
This notion combats a potential issue with randomized algorithms,
that independent executions on the same input might return different outputs,
depending on the algorithm's coin tosses.
Many known streaming algorithms suffer from this issue,
which is a serious concern for some users and applications. 
Pseudo-deterministic algorithms were later considered in the streaming model
by Goldwasser, Grossman, Mohanty and Woodruff~\cite{DBLP:conf/innovations/GoldwasserGMW20},
and these are formally defined as follows.


\begin{definition}\label{def:PD_streaming}
A streaming algorithm $A$ is \emph{pseudo-deterministic} (PD)
if there is a function $F(\cdot)$ defined on inputs of $A$ (streams),
such that for every stream $\sigma$,
$$
  \P [A(\sigma)=F(\sigma)] \geq 9/10,
$$
where the probability is over the random choices of the algorithm.
We shall refer to $F$ as the \emph{canonical function} of algorithm $A$.%
\footnote{
% For many streaming problems, the computational task does not depend on the order arrival of the stream items.
% In particular, for a stream $\sigma$ \sout{, which is} \textcolor{red}{of} 
% \ssnote{I think the stream is an ordered sequence by definition?} 
% an ordered sequence of items, the output is with respect to the set of items $I_\sigma=\set{a\in\sigma}$.
The canonical function $F$ depends on the order arrival of the stream items.
In an alternative definition, the canonical function depends on the items only as a multiset, i.e., ignoring their order in the stream. % $I_\sigma$.
% We leave open the question of whether these two definitions are equivalent, which is the case
These two definitions are equivalent in the setting of approximate counting, which is the focus of our work.
}
\end{definition}



%

% No lower bounds are known for
We focus on
\emph{estimation problems},
which ask to approximate a numerical value, 
and are very popular in the streaming model.
%
For such problems, the notion of PD relaxes the exact setting and the deterministic one,
% the deterministic setting, and furthermore, it relaxes the exact setting for numerical problems,
% This notion relaxes the exact setting and the deterministic one,
since exact algorithms have one canonical output (the exact numerical value), and hence they are PD.
Thus the known lower bounds for these settings do not apply for PD algorithms,
and a central question, identified in~\cite{DBLP:conf/innovations/GoldwasserGMW20}, remains open:

\begin{center}
  \emph{Are there efficient PD streaming algorithms for estimation problems?}
\end{center}

% \aknote{Since the previous para (and our main result) is about lower bounds, should we frame this question slightly differently? Something like -- ``Are there pseudo-deterministic algorithms for estimation problems on streams that are more space-efficient than deterministic algorithms?''? Apologies for raising questions so close to the deadline.}
% \ssnote{I prefer to keep this question short. I think the context of comparison to deterministic or exact algorithms is already set.}

Currently, no lower bounds are known for natural
% \ssnote{Actually, there is some artificial problem that separates adaptive streaming from oblivious streaming, and hence their LB apply here. TODO add it to related work?}
estimation problems,
although for several search problems,
like reporting an element from a stream with deletions
(equivalently, an index from the support of the frequency vector), 
it is known that lower bounds for deterministic algorithms
extend to PD algorithms~\cite{DBLP:conf/innovations/GoldwasserGMW20}.


\subsection{Main Result: Approximate Counting}

Perhaps the most basic problem in the streaming model is to count the number of stream items.
% \footnote{Without loss of generality we may assume the items are identical.}
Exact counting, i.e., computing the number of items exactly, 
requires $\Theta(\log n)$ bits of space when the stream has length at most $n$,
even for randomized algorithms with some error probability.
%
Work by Morris~\cite{DBLP:journals/cacm/Morris78a}, later refined in~\cite{DBLP:journals/bit/Flajolet85,DBLP:journals/mst/GronemeierS09,NelsonYu22},
showed that the number of stream items
can be $(1+\epsilon)$-approximated with probability $9/10$
using $O_\epsilon(\loglog n)$ bits of space,
where $\epsilon>0$ is arbitrary but fixed.
Throughout, we refer to multiplicative approximation,
and use the notations $O_c(\cdot)$ and $\Omega_c(\cdot)$
to hide factors that are polynomial in $c$. 
%\footnote{We say that $y\ge 0$ is a $c$-approximation for $x\ge0$ if $x\leq y\leq cx$.}
%\footnote{The notations $O_c(\cdot)$ and $\Omega_c(\cdot)$ hide factors that depend on $c$ (in our case they are all polynomial in $c$).}
Morris's algorithm has found many applications,
both in theory and in practice~\cite{DBLP:journals/corr/abs-1805-00612_Lumbroso,NelsonYu22}.
% \rnote{\sout{However, it is not known to what extent one can derandomize the algorithm.} This sentence is dubious because it cannot be derandomized in the usual sense or making it deterministic. }
An open question stated explicitly by Goldwasser, Grossman, Mohanty and Woodruff~\cite{DBLP:conf/innovations/GoldwasserGMW20} is 
whether there is a PD algorithm for this problem using $O(\loglog n)$ bits of space.
We answer their question negatively, by proving the following lower bound. 

\begin{theorem}\label{cor:LB_PD_counting}%maybe bad name for the label
For every $c,n>1$,
every PD streaming algorithm that $c$-approximates the number of items in a stream of length at most $(c+1)n$
must use $\Omega_c(\sqrt{{\log n}/{\loglog n}})$ bits of space.
\end{theorem}

% The important setting is where $c$ is fixed and $n$ increases. 
% Our bounds in \Cref{thm:LB_PD_counting,cor:LB_PD_counting} are stronger, and 
To be more precise, our lower bound is actually 
$\Omega\big(\tfrac{\log n}{\sqrt{\log n\loglog (cn)} + \log c}\big)$,
which is still $\Omega(\sqrt{\tfrac{\log n}{\loglog n}})$
as long as $c<2^{\sqrt{\log n \loglog n}}$. 
Previously, there was a large gap for this problem,
% For PD streaming algorithms, there is a gap 
between $O(\log n)$ bits (by a deterministic algorithm)
and $\Omega(\loglog n)$ bits (from the randomized setting)~\cite{NelsonYu22}.
See \Cref{table:Approx_Count_Bounds} for a summary of the known bounds.
% \ssnote{Credit for the $\Omega(\loglog n)$ bound? Or is it obvious since the alg has to output this number of distinct values?}
% \rnote{I don't know for sure. I agree it follows from counting distinct outputs and can be considered folkloe, but please double check in \cite{NelsonYu22} (they probably optimize dependence on $\epsilon$, which is less relevant for us). } 


\begin{table*}[t] % or [h]?
\caption{
\label{table:Approx_Count_Bounds} 
Known space bounds (in bits) for $2$-approximate counting in a stream of length at most $n$.
Folklore bounds are stated without a reference. 
}
\begin{center}
\begin{tabulary}{\textwidth}{|L  | rl | rl|}
\hline
Algorithms
& \multicolumn{2}{c|}{Upper bound}
&  \multicolumn{2}{c|}{Lower bound}
\\ 
\hline
Exact or deterministic
& $O(\log n)$ &
& $\Omega(\log n)$ & \\ 
Randomized and approximate
& $O(\loglog n)$ & \cite{DBLP:journals/cacm/Morris78a}
& $\Omega(\loglog n)$ & \cite{NelsonYu22}
\\
Pseudo-deterministic
& $O(\log n)$  & 
& $\Omega(\sqrt{{\log n}/{\loglog n}})$ & [Thm.~\ref{cor:LB_PD_counting}] \\
\hline
\end{tabulary}
\end{center}
\end{table*}


Our proof analyzes the promise variant 
of $c$-approximate counting for streams of length at most $(c+1)n$,
which we denote by $\prac$;
this variant asks to distinguish whether the number of stream items
is $\leq n$ or $>cn$ (see Definition \ref{sec:preliminaries}\textcolor{blue}{.1}) % BUG: \Cref{def:stream_approx_count}) lead to the wrong place
A crucial property of PD algorithms is that they
have to be PD also for inputs in the range $[n+1,cn]$ (i.e., outside the promise).
We rely on this property of PD algorithms to prove the following result,
which immediately yields \Cref{cor:LB_PD_counting} as a corollary.


\begin{theorem}\label{thm:LB_PD_counting}
For every $c,n>1$, 
every PD streaming algorithm for problem $\prac$ must use $\Omega_c(\sqrt{ {\log n}/{\loglog n}})$ bits of space.
\end{theorem}
%A bound for $c$-approximate counting immediately follows.
We suspect that the correct bound for \Cref{thm:LB_PD_counting}
(and thus also for \Cref{cor:LB_PD_counting})
is $\Omega(\log n)$.
% Our approach 
% for achieving an $\Omega(\log n/\loglog n)$ bound 
% is by solving a problem which we call Shift Finding and is of independent interest.
Our proof of Theorem~\ref{thm:LB_PD_counting}
appears in Section~\ref{sec:LB_PD_counting}.
It is based on a problem that we call Shift Finding and may be of independent interest,
and can potentially lead to an $\Omega(\log n/\loglog n)$ bound,
by improving our algorithmic result for Shift Finding. 


\subsection{Main Technique: The Shift Finding Problem}

Our main result relies on \emph{algorithms} for the shift Finding problem $\prsf$, which is defined below. 
Let us first introduce some basic terminology.
A function $F:[m]\to\{0,1\}$ can also be viewed as a string $F\in\{0,1\}^m$, 
and vice versa, and we sometimes use these interchangeably. 
Given $s\in[0,n]$, let the shifted version of this $F$
be the function $F_{s}: x\mapsto F(s+x)$, with a properly restricted domain,
see \Cref{sec:preliminaries}. 


\begin{definition}[Shift Finding] \label{def:shift_finding}
Let $c,n>1$.
In problem $\prsf$,
the input is a string $P\in \{0,1\}^{(c-1)n}$,
and one has query access to a string $F_{s^*}$ that is the concatenation
of $n-s^*$ zeros, then $P$, and finally $s^*$ ones, 
for an unknown $s^*\in [0,n]$. 
Thus, a query for $x\in[0,cn]$ returns $F_{s^*}(x)$.
The goal is to output $s^*$. 
\end{definition}

The measure of complexity of an algorithm for this problem
is the number of queries that it makes to $F_{s^*}$.
A randomized algorithm is required to be correct (in its output $s^*$)
with probability $9/10$.



This problem may be also of independent interest.
% It is related to the well-studied pattern matching problem,
% where the input is two strings $T,P$ and a number $k$;
% and the goal is to find substrings of $T$ that are within Hamming distance
% (or edit distance) at most $k$ from the string $P$,
% see e.g.~\cite{DBLP:conf/stoc/ChanGKKP20}.
% \rnote{I don't think this is the most relevant reference, because this paper seeks approximate solutions, and because our problems refers to $k=0$. One should search (in their intro?) for papers that are sublinear, i.e., sample/query one of the strings. }
% Algorithms for this problem run in time at least linear in the length of the strings. %, and in fact read both strings $T$ and $P$ entirely.
% In contrast, our problem asks to minimize the number of queries to one string.
%
In a different variant of shift finding,
the input is a random string $c\in\set{0,1}^n$ and a vector $x$ that is obtained from the string $c$ by a cyclic shift $\tau$ and some noise (random bit flips),
and the goal is to compute the shift $\tau$ with high probability.
% \rnote{can they really possible to approximate $\tau$ and not compute it exactly? it feels awkward because similar shifts $\tau$ and $\tau+1$ are unrelated. }
% \rnote{$\tau$ is the shift operation or its magnitude (number)?}
This problem is related to GPS synchronization, see~\cite{DBLP:conf/mobicom/HassaniehAKI12,DBLP:conf/soda/AndoniIKH13} for more details.
There is a sublinear time algorithm for this problem, running in time roughly $O(n^{0.641})$~\cite{DBLP:conf/soda/AndoniIKH13}.
One main difference is that in our \Cref{def:shift_finding},
one string is completely known to the algorithm, and the only concern is the number of queries to the second string.



\subsubsection{Connection to PD Counting}

We show that an algorithm for Shift Finding ($\prsf$) implies
a space lower bound for PD streaming algorithm for counting ($\prac$).

\begin{theorem}\label{thm:connection_Shift_Counting}
Let $c,n>1$, and suppose that the Shift Finding problem $\prsf$
admits a randomized algorithm that makes at most $q=q(c,n)$ queries (possibly adaptive).
Then, every PD streaming algorithm for the approximate counting problem $\prac$
must use $\Omega(\tfrac{\log n}{\log q})$ bits of space.
\end{theorem}


It immediately follows that if the Shift Finding problem $\prsf$ can be solved using $\polylog(n)$ queries (for fixed $c>1$), then PD approximate counting requires $\Omega(\tfrac{\log n}{\loglog n})$ bits of space.
However, our upper bound for Shift Finding is $q=O(\sqrt{cn})$ queries (\Cref{thm:shift_finding_alg}) and is therefore not strong enough to yield a nontrivial lower bound for PD approximate counting. 

Therefore, to prove our main lower bound (\Cref{thm:LB_PD_counting}),
we revert to a generalization of \Cref{thm:connection_Shift_Counting}
where the shift-finding algorithm is still given an instance of problem $\prsf$
(namely, a string $F$ and query access to $F_{s^*}$), 
but reports a small set $R\subset[0,n]$ (say of size $|R|\leq t$) 
that contains the unknown shift (i.e., $s^*\in R$).
This algorithm may be randomized provided that it is PD,
and its canonical function maps each instance of problem $\prsf$
to a set $R$ of size $t$ that contains $s^*$.


\begin{theorem}\label{thm:generalized_connection_Shift_Counting}
Let $c,n>1$,
and suppose there is a PD algorithm $Q$ that,
given an instance of problem $\prsf$,
makes at most $q=q(c,n)$ queries (possibly adaptive) to $F_{s^*}$
and its canonical function $M$ maps the input to a set
$R\subset [0,n]$ of size $t=t_c(n)$ that contains $s^*$.
Then every PD streaming algorithm for problem $\prac$ must use
$\Omega(\tfrac{\log (n/t)}{\log q})$ bits of space.
\end{theorem}


As an easy corollary of \Cref{thm:connection_Shift_Counting},
we get that the \emph{tracking} version of approximate counting
must use $\Omega(\log n)$ bits of space,
which is tight with a straightforward deterministic counting.
Tracking means that the algorithm produces an output
% i.e., approximates the current count,
after every stream item rather than at the end of the stream,
and with probability $9/10$, all the outputs are simultaneously correct (i.e., approximate the number of items seen so far).

\begin{corollary}\label{thm:LB_PD_tracking_counting}
For every $c,n>1$, every PD tracking algorithm that $c$-approximates the number of items in a stream of length $(c+1)n$ must use $\Omega(\log n)$ bits of space.
\end{corollary}

In contrast, for standard randomized algorithms, 
there is a tracking algorithm for  $(1+\epsilon)$-approximate counting that uses $O_\epsilon(\loglog n)$ bits of space, for any fixed $\epsilon>0$ \cite{NelsonYu22}.
% We remark that this result does not follow from standard amplification of having several repetitions of algorithm with constant failure probability and taking the majority, and such amplification would require $O(\log n)$ repetitions in order to get the "for all" guarantee of tracking algorithms.
%
\Cref{thm:LB_PD_tracking_counting} follows by an easy modification of the proof of \Cref{thm:connection_Shift_Counting}.
That proof uses $O(\log q)$ repetitions of a PD streaming algorithm,
and then employs a union bound on $q$ input streams,
which is not necessary for tracking algorithms and thus the bound follows.
%

A more direct argument is essentially by equivalence to exact counting. 
For a stream with $s<n$ items, 
the state of a PD tracking algorithm with canonical function $F$
can be used to compute $s$, as follows.
Simulate insertion of more items to the stream until the output of the algorithm changes to $1$ (which corresponds to the first $1$ in $F_s$), from which $s$ can be computed.


% \rnote{I couldn't really understand this, which is not surprising because it's not even next to the proof. But I do know and vaguely remember the proof and it'sstill hard to follow. I vaguely understand that tracking gives you $q=1$ for free, but what about $t$? }
% \ssnote{It's a corollary of \Cref{thm:connection_Shift_Counting}, not \Cref{thm:generalized_connection_Shift_Counting}, so there is no $t$. Maybe it's better to explain it independently --- "The bound essentially follows from equivalence to exact counting. 
% For a stream with $s<n$ items, 
% the state of a PD tracking algorithm with canonical function $F$
% % can be used to compute $s$ exactly by 
% can be used to compute $s$, as follows.
% Simulate
% insertion of more items to the stream until the output of the algorithm changes to $1$ (which corresponds to the first $1$ in $F_s$), from which $s$ can be computed."}
% \rnote{okay I see, it makes sense, but please rewrite the explanation above about "easy corollary".}


\subsubsection{An Algorithm for Shift Finding}

Consider a special case of the Shift Finding problem $\prsf$,
where the input string $P$ is a step function;
then the algorithm can perform a binary search using $O(\log (cn))$ queries,
and find the unique location where $F_{s^*}$ switches from value $0$ to $1$,
and hence recover $s^*$. 
At the other extreme, suppose the input string $P$ is random;
then with high probability
every set of $O(\log n)$ queries from $P$ (and thus from $F_{s^*}$)
will be answered differently (viewed as a string in $\{0,1\}^{O(\log n)}$). 
Based on these observations, one may hope that problem $\prsf$
admits an algorithm that makes $\polylog (cn)$ queries.
We leave this as an open question
and prove a weaker bound of $O(\sqrt{cn})$ queries.

\begin{theorem}\label{thm:shift_finding_alg}
There is a deterministic algorithm for problem $\prsf$ that makes $O(\sqrt{cn})$ queries.
\end{theorem}

A key observation in our result, that may be useful in future work,
is that for every shift $s^*$
there is a ``short witness'' that uses exactly $2$ queries.
We formalize this as verifying a given guess $s$ for the shift $s^*$. 

\begin{lemma}\label{lem:shift_verifier_witness_2queries}
There is a deterministic algorithm that,
given as input an instance of problem $\prsf$ and $s<n$,
makes $2$ queries to $F_{s^*}$ and returns 'yes' if $s=s^*$ and 'no' otherwise.
\end{lemma}



\subsection{Related Work}
\label{sec:related}

\paragraph*{Pseudo-deterministic algorithms}
The notion of pseudo-deterministic algorithms was introduced by~\cite{DBLP:journals/eccc/GatG11} (they originally called them Bellagio algorithms), followed by a long sequence of works that studied it in different models~\cite{DBLP:conf/innovations/GoldreichGR13,DBLP:journals/eccc/Grossman15,DBLP:journals/eccc/GoldwasserG15,DBLP:conf/stoc/OliveiraS17,DBLP:journals/corr/Holden17,DBLP:conf/innovations/GoldwasserGH18,DBLP:conf/mfcs/DixonPV18,DBLP:conf/approx/OliveiraS18,DBLP:journals/corr/abs-1910-00994_GoemansGH2019,DBLP:conf/soda/GrossmanL19,DBLP:journals/eccc/Goldreich19,DBLP:conf/innovations/GoldwasserGMW20,DBLP:conf/stoc/LuOS21,DBLP:conf/innovations/00020V21,DBLP:conf/coco/GoldwasserIPS21,DBLP:conf/approx/GhoshG21,DBLP:conf/stoc/0002PWV22}.
In the streaming and sketching models, \cite{DBLP:conf/innovations/GoldwasserGMW20} proved strong lower bounds for finding a non-zero entry in a vector (given in a stream with deletions), and for sketching $\ell_2$-norms.
Another related setting is that of
sublinear time computation. 
Under certain assumptions, PD algorithms (in the sublinear time region) were shown to admit the following relation with deterministic algorithms -- if for a certain problem there is a PD algorithm using $q$ queries, then there is a deterministic algorithm using $O(q^4)$ queries~\cite{DBLP:conf/innovations/GoldreichGR13}.
The techniques of~\cite{DBLP:conf/innovations/GoldreichGR13} do not seem to extend to streaming algorithms.
% \ssnote{As far as I understand them.}


\paragraph*{Adaptive adversarial streams}
In this setting, the stream items are chosen adversarially and depend on past outputs of the streaming algorithm (i.e., the stream is adaptive)~\cite{DBLP:journals/jacm/Ben-EliezerJWY22}.
This model is considered to be between PD algorithms and the standard randomized setting, in the sense that for streams of length $m$, amplifying a PD algorithm to success probability $1-\tfrac{1}{10m}$ (by $O(\log m)$ repetitions and taking the median) guarantees (by a union bound) that the algorithm outputs the canonical solution after every stream item with probability $9/10$, thus the adversary acts as an oblivious one (the adversary knows in advance the output of the streaming algorithm, which is the canonical function).
% most problems, 
% every PD algorithm is robust against an adaptive adversary, with a logarithmic blowup in space (by standard amplification).
For approximate counting, adaptive streams and standard (oblivious) streams are equivalent (since the stream items are identical) and thus admit an algorithm using $O(\loglog n)$ bits of space.


There is a vast body of work designing algorithms for adaptive streams, 
% but for the context of our work, we focus on lower bounds.
but not much is known in terms of lower bounds.
%
Lower bounds are known for some search problems, like finding a spanning forest in a graph undergoing edge insertions and deletions, but also for graph coloring~\cite{DBLP:conf/innovations/ChakrabartiGS22}.
% Lower bounds are known for graph coloring
%
Regarding estimation problems, the only lower bound we are aware of is
for some artificial problem~\cite{DBLP:conf/crypto/KaplanMNS21}.  
%
Recently, Stoeckl~\cite{doi:10.1137/1.9781611977554.ch32_Stoeckl} showed a lower bound on streaming algorithms that use a bounded amount of randomness, conditioned on a lower bound for PD algorithms.
%
In the related model of linear sketching, Hardt and Woodruff~\cite{10.1145/2488608.2488624_HW13} 
showed lower bounds on the dimensions of sketching algorithms, which
% designed an attack that uses polynomial number of adaptive queries to a linear sketch and forces it to fail. 
% Their attack 
applies to many classical problems, like $\ell_p$-norm estimation and heavy hitters.
% This implies that PD linear sketches for $\ell_p$-norm estimation and heavy hitters must have dimension $\tilde{\Omega}(n)$~\cite{10.1145/2488608.2488624_HW13}, as proved for $\ell_2$-norm estimation by~\cite{DBLP:conf/innovations/GoldwasserGMW20}.



% Recently, Stoeckl~\cite{doi:10.1137/1.9781611977554.ch32_Stoeckl} showed a connection in the other direction, for adversarially robust streaming algorithms that use a bounded amount of randomness. 
% They considered the missing item finding problem (MIF), and introduced a technique for proving that an adversarially robust streaming algorithm using $r$ bits of randomness can be forced to be PD after $\poly (r)$ stream items.
% That is, lower bounds for PD streaming algorithms for MIF imply lower bounds for adversarially robust streaming algorithms (with bounded randomness) for MIF.
% They left open the question of proving such lower bounds for PD algorithms, and we hope our techniques can be used to answer this question.


% Recently, Stoeckl~\cite{doi:10.1137/1.9781611977554.ch32_Stoeckl} showed a lower bound for that use a bounded amount of randomness, conditioned on a lower bound for PD algorithms.



\iffalse  % Open questions
    
    \subsection{Open Questions}
    
    \subparagraph{Shift Finding}
    Is there an algorithm for the Shift Finding problem $\prac$ using $\polylog(n)$ queries?
    
    \subparagraph{Approximate counting}
    Is there a PD streaming algorithm of $c$-approximate counting using $o(\log n)$ bits of space, for any fixed $c>1$?
    
    
    \subparagraph{Frequency vectors}
    In the frequency vector model, the stream items are additive $\pm1$ updates to the coordinates of an $n$ dimensional vector.
    This model was extensively studied, e.g. approximating the $\ell_p$ norms, heavy hitters, and more.
    %It is open whether these problems admit PD algorithm using $o(n)$ bits of space.
    Most of the classical (not PD) algorithms in this model are in fact linear sketches, and it follows from~\cite{10.1145/2488608.2488624_HW13} that adversarially robust sketches
    must use $\Omega(n)$ dimensions, which immediately translates to PD algorithms.
    This observation was proved by~\cite{DBLP:conf/innovations/GoldwasserGMW20} for the special case of $\ell_2$-norm approximation.
    It is open whether similar bounds hold for PD streaming algorithms.
    
    \subparagraph{Missing Item Finding~\cite{doi:10.1137/1.9781611977554.ch32_Stoeckl}}
    For $r<n$, problem $\prmif$ is as follows. The input is a stream $a_1,...,a_r$ of elements from $[n]$ (possibly with repetitions), and the goal is to output $x\in[n]$ that is different from all of the $a_i$.
    Is there a PD streaming algorithm for problem $\prmif$ using $o(r)$ bits of space?
    % This question was introduced by~\cite{doi:10.1137/1.9781611977554.ch32_Stoeckl}.

\fi % Open questions


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:


