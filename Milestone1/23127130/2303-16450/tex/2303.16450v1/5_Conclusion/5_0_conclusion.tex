\label{sec:5}
In this paper, we propose SPoTr, a Transformer for point clouds, which captures both local and global shape context without the quadratic complexity of input points.
SPoTr includes two attention modules: self-positioning point-based attention~(SPA) and local points attention~(LPA).
SPA is a novel global cross-attention, which aggregates information via disentangled attention and non-locally distributes information to entire points.
% Our self-positioning mechanism allows a small number of SP points to cover overall shapes and provides improved expressive power.
% Properly located self-positioning points enable improved expressive power with a small number of
Our experiments show superior performance across various tasks, including ScanObjectNN, SN-Part, and S3DIS.
% and visualizations demonstrate the effectiveness and interpretability of SPA. 
% Our codes are available in the supplement.

% Limitations and negative societal impacts are discussed in the supplement.

% With self-positioning receptive fields based attention~(SPA) and local points attention~(LPA), SPoTr captures both local and global information.
% aggregates local inforation and distributes the global shape context.
% We also analyze SPoTr in both quantitative and qualitative manner with various benchmark datasets to probe the applicability.
% Further, as shown in visualizations, the behaviour of self-positioning receptive fields is fully interpretable.
% Further, the visualization shows the 
% We present a novel semi-supervised strategy with Metropolis-Hastings algorithm based augmentation
% method. This is the first work to impose data augmentation on graph-structured data from a perspective
% of a Markov chain Monte Carlo sampling. We theoretically and experimentally show the convergence
% of augmented samples to target distribution and demonstrate its consistent performance improvement
% over baselines across five benchmark datasets.
% In this paper, we present a Transformer architecture called SPoTr for point clouds, which efficiently captures both local and global shape context.
% SPoTr includes two attention modules: self-positioning receptive fields based attention~(SPA) and local points attention~(LPA).
% SPA is a novel global cross-attention mechanism with self-positioning receptive fields.
% SPA aggregates information of local neighborhoods via disentangled attention and non-locally influence on entire points. 
% In our experiment, SPoTr consistently shows competitive performance on various tasks.
% Especially, SPoTr achieves new state-of-the-art performance in the ScanObjectNN dataset with reduced complexity.
% Our visualizations demonstrate the effectiveness and interpretability of SPA. 
% Limitations and negative societal impacts are discussed in the supplement.