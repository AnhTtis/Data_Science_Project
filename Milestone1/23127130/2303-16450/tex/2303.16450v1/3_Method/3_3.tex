\label{sec:3.3}

\input{Fig_tex/spotr_arch.tex}

% \jyp{Next, we propose the SPoTr block that utilizes \emph{local points attention}~(LPA) module and \emph{self-postioning receptive fields based attention}~(SPA) module to capture not only local fine-grained and short-distance information but also long-distance and global information}.
We now propose the SPoTr block that utilizes \emph{self-positioning point-based attention}~(SPA) with \emph{local point attention}~(LPA).
By combining LPA and SPA, it captures not only local and short-distance information but also long-distance and global information.
% Next, we propose the SPoTr block that utilizes SPA with \emph{local points self-attention}~(LPA).
% Each component operates in order as illustrated in \Cref{fig:fig3}.
% First, we capture the local shape context by LPA, then conduct PGE to abstract local point groups into higher-level feature representations. 
% Lastly, we capture the overall shape context with SPA.\\
% The overall architecture of the SPoTr block is illustrated in \Cref{fig:fig3}.
% First, we capture the local shape context by LPA, then conduct PGE to abstract local point groups into higher-level feature representations. 
% Lastly, we capture the overall shape context with SPA.
% \jyp{LPA captures the locally grouped information with attention mechanism and SPA deals with long-range and global information using self-positioning receptive fields.}\\

\paragraph{Local points attention (LPA).} 
We adopt local points attention~(LPA) defined on a local group to learn local shape context.
% A local point group $\Pointgroup_i$ consists of $k$ nearest neighbors from an anchor point $x_i$, which is selected by the farthest point sampling~(FPS). 
A local point group consists of neighbors in ball query centered on an anchor point $x_i$.
The attention for each local point group $\Pointgroup_i$ and points $\left\{x_j | \forall j \in \Pointgroup_i\right\}$ is defined as
% \yunyang{
\begin{equation}
\label{eq:lpa}
    \Feat_{i}^{\text{LPA}} = \mbox{CWPA}\left(x_{i}, \Feat_i, \left\{x_j\right\}_{j \in \Pointgroup_i}, \left\{\Feat_j\right\}_{j \in \Pointgroup_i}\right),
\end{equation}
% \begin{equation}
% \label{eq:lpa}
%     \Feat_{i}^{\text{LPA}} = \sum_{m=1}^M \mathbf{W}_m \left[\sum_{x_{j} \in \mathcal{G}_i} \textbf{A}_{mij}\cdot \left(\mathbf{W}_m^\prime \mathbf{f}_j + \phi_{ij}\right) \right],
% \end{equation}
% where $\Feat_{j}$ is an input feature vector, $\Feat^\text{LPA}_{i}$ is an output feature vector of LPA, $\Cen_{ij}$ is a 3D coordinate, and $\mathcal{T}$ is a function for injecting the positional information.
where $\Feat_{j}$ is the feature vector of point $x_j$, $\Feat^\text{LPA}_{i}$ is an output feature vector of LPA.
We adopt channel-wise point attention operation~(CWPA) same as SPA.
% The positional encoding can be categorized into two groups: absolute positional encoding and relative positional encoding.
% Herein, we use the \textit{relative} positional encoding for better capturing relationship within local groups as in~\cite{qi2017pointnet++,zhao2021point}.
% Our positional encoding is defined as 
% Following \cite{zhao2021point}, the attention weight $\mathbf{A}_{mij}$ is defined as $\text{softmax}_j\left(\left\{\text{MLP}\left(\varphi(\mathbf{f}_i) - \alpha(\mathbf{f}_{j^\prime}) +\PE_{ij^\prime} \right)\right\}_{j^\prime}\right)$, where $\varphi, \alpha$ are linear transformations and $\delta$ is a positional encoding.
% Note that $i$ is an anchor point index and $j$ is a local point index.
% Positional Encoding
% We use a multi-layer perceptron~(MLP) with one hidden layer.
% We apply LayerNorm~(LN) and residual connections.

% \paragraph{Positional encoding.}
% Generally speaking, Transformers utilize positional encoding such as sinusoidal positional encoding~\cite{vaswani2017attention,dosovitskiy2020image} to capture positional information between data points.
% The positional encoding can be categorized into two groups: absolute positional encoding and relative positional encoding.
% We here use the \textit{relative} positional encoding for capturing better relationships within local groups as in~\cite{qi2017pointnet++,zhao2021point}.
% The relative positional encoding $\PE_{ij} $ of the point $\Cen_{j}$ is defined as
% % \yunyang{
% \begin{equation}
% \label{eq:posemb}
%     \PE_{ij} = \text{MLP}\left(\left[\left(\Cen_{j}-\Cen_{i}\right)^\top||d_{ij}  \right]\right)\text{, where }d_{ij}= \left\lVert \Cen_{j} - \Cen_{i} \right\rVert_2,
% \end{equation}
% where $||$ indicates a concatenation operation.
% \jyp{
% Following \cite{zhao2021point}, we use positional encoding for computing attention weights and feature transformations. 
% So the positional encoding is added to key vectors of attention operations.
% }
% % , $\Cen_{i}$ is an anchor point of the local point group $\Pointgroup_i$,
% %$d_{ij}$ is the Euclidean distance between $\Cen_{ij}$ and $\Cen_{i}$, which is calculated as $\left\lVert \Cen_{ij} - \Cen_{i} \right\rVert_2$. 
% To inject positional information as well as semantic features, we aggregate the positional encoding $\PE_{ij}$ and the feature ${\Feat}_{ij}$ by the function $\mathcal{T}$.
% There are several aggregation methods such as element-wise addition, multiplication, and concatenation.
% Here we apply concatenation followed by a multi-layer perceptron (MLP), \ie, $\mathcal{T}\left(\Cen_{ij}, \Feat_{ij} \right) = \text{MLP}\left(\left[\PE_{ij}||\Feat_{ij}  \right]\right)$.\\

% \noindent\textbf{Point group embedding~(PGE).}
% For downsampling, we apply point group embedding~(PGE), which is the set abstraction~\cite{qi2017pointnet++} with a skip-connection. 
% PGE abstracts the points $\Cen_{ij}$ in the local point group~$\Pointgroup_i$ with the representation vector ${\Feat}_{ij}$.
% It plays a similar role as the patch merging in hierarchical Vision Transformers~\cite{liu2021swin,wang2021pyramid}.
% In our works, the point group embedding is implemented as 
% \begin{equation}
% \label{eq:ffn_pool}
%     \Feat_i^{\text{PGE}} = \mathcal{A}\left(\left\{\FFN\left(\LN\left({\Feat}_{ij} \right) \right) + \Feat_{ij}\right\}_{j \in \Gc_i}\right),
% \end{equation}

% where $\mathcal{A}$ is a symmetric function (\eg, max-pooling) and $\Feat_{ij}$ is the output of LPA defined in \eqref{eq:lpa}.
% $\Feat_i^{\text{PGE}}$ is the representation for encoding shape information of group $\Pointgroup_i$.\\

% \input{Table&Figure/fig2}

% \noindent\textbf{SPA: Global Cross-Attention for Point Cloud.} %Self-positioning Receptive Fields based Attention

% \input{Table&Figure/fig3}

\paragraph{SPoTr block.}
We construct a SPoTr block by combining the local points attention~(LPA) module and self-positioning point-based attention (SPA) module to capture local and global information simultaneously.
As shown in \Cref{fig:fig4}~(bottom right), the SPoTr block is defined as follows:
\begin{equation}
    \hat{\Feat}_i = \alpha \cdot \Feat_i^{\text{SPA}} + (1-\alpha) \cdot \Feat_i^{\text{LPA}} 
\end{equation}
where $\alpha$ is a learnable parameter that softly selects the representations generated by self-positioning point-based attention and local points attention.
% As shown in~\Cref{fig:fig3a}, the SPoTr block first linearly transforms input features followed by batch normalization.
% Then, the generated representations are processed by both LPA and SPA with batch normalization and residual connections.
% Next, we perform farthest point sampling~(fps), kNN, and local pooling~(\ie, max-pooling) to reduce the cardinality of the point set and aggregate local points similar to Pointnet++~\cite{qi2017pointnet++}. 
Finally, LPA and MLP with batch normalization and the residual connection are applied to extract point-wise high-level representations.

\paragraph{Connection between SPoTr block and Set abstraction.}
% We find a interesting connection between point channel-wise attention and set abstraction.
% By discussing the point channel-wise attention operation with set abstraction, we demonstrate why the point channel-wise attention is more powerful than the standard attention.
We show the superiority of the SPoTr block by discussing it with set abstraction in PointNet++~\cite{qi2017pointnet++}. 
% Unlike standard attention, since CWPA in SPoTr block computes the attention value for each channel, our SPoTr block can express the set abstraction in PointNet++~\cite{qi2017pointnet++}: 
\begin{remark}
A SPoTr block with proper $\alpha, \mathcal{M},\mathcal{M}',\mathcal{R},\mathcal{R}'$ can express set abstraction.
\end{remark}
When the value of $\tau$ is sufficiently low, the function $\mathcal{M}^\prime$ is the same as $\mathcal{M}$, $\alpha=0$, and $\mathcal{R}(\mathbf{f}_q, \mathbf{f}_k)=\mathcal{R}^\prime(\mathbf{f}_q, \mathbf{f}_k)=\mathbf{f}_k$, the channel-wise point attention becomes equivalent to the set abstraction.   
In this setting, the attention weight between the query point and $k$-th key point on $c$-th channel becomes 1 if $k = \underset{k^\prime \in \Omega_k}{\mathrm{argmax}}\  \mathcal{M}\left([\mathbf{f}_{k^\prime};\phi_{qk^\prime}] \right)_{q,k^\prime,c}$.
Otherwise, the attention score is 0.
It means that the attention only activates the maximum channels alike a max-pooling operation. 
Therefore, the SPoTr block can play a role as a max-pooling operation following the mapping function, which is the set abstraction.
This fact supports the improved expressive power of SPoTr on point cloud analysis.
% The detailed proof is in the supplement.

% \paragraph{SPoTr-U block.}
% For semantic segmentation tasks, we present a SPoTr-U block as illustrated in \Cref{fig:fig3b}.
% To combine features of downsampled points $\Pc_1 \subset \Pc_2$ with its superset $\Pc_2$, which is provided by a skip-connection, trilinear interpolation is applied to upsample points.
% Then, we adopt multi-layer perceptron~(MLP) with batch normalization.
% To the end, similar to the SPoTr block, the combined features are updated by both LPA and SPA with batch normalization and residual connections.
% We construct a SPoTr block by combining the local points attention~(LPA) module and self-positioning receptive fields based attention (SPA) module to capture local and global information simultaneously.
% As shown in Figure~\Cref{fig:fig4}, the SPoTr block first linearly transforms input features followed by batch normalization.
% Then, the generated representations are processed by both LPA and SPA with batch normalization and residual connections.
% Next, we perform farthest point sampling~(fps), kNN graphs, and local pooling~(\ie, max-pooling) to reduce the cardinality of the point set and aggregate local points similar to Pointnet++~\cite{qi2017pointnet++}. 
% Finally, multi-layer perceptron with batch normalization and residual connection is applied to extract deep aggregated point features.
% Latency comparison experiments.
% For instance, in the last block of scene segmentation, we opt $m=128$ compared to the original $n=4096$ points, making the global attention approximately ${\frac{4096}{128}}=\mathbf{32}\times$ faster.
