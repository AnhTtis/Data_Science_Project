\label{sec:3}
\input{Fig_tex/spa.tex}

The goal of our framework SPoTr is to learn point representations for various point cloud processing tasks with a Transformer architecture using \textit{self-positioning points}.
First, we shortly describe the background regarding the point-based approaches including PointNet++ and Point Transformer~(\Cref{sec:3.1}). 
Second, we propose self-positioning point-based attention to efficiently capture the global context~(\Cref{sec:3.2}).
% \jyp{Second, we demonstrate a SPoTr block, which is composed of two types of attention modules: (i) \emph{local points attention}~(LPA) and (ii) \emph{self-positioning receptive fields based attention}~(SPA), where LPA captures local information and GSA deals with long-range and global information~(\Cref{sec:3.2}).} 
Third, we delineate the SPoTr block, which compromises both global cross-attention and local self-attention, and discuss the relation with a popular point-based network~(\Cref{sec:3.3}). 
Finally, we present the overall architecture of SPoTr, which is composed of multiple SPoTr blocks, for shape classification and segmentation tasks~(\Cref{sec:3.4}).

% To capture global long-range information beyond local short-range information, we present self-positioning points based attention (SPA) block, which consists of local self-attention and global cross-attention with dynamically positioning points.

% Then, we introduce our deformable \textbf{po}int \textbf{tr}ansformer~(Deformable PoTR). 
% Our proposed Deformable PoTR is composed of two attention techniques. In \textbf{LGP-A} (\Cref{sec:3.2}), self-attention is computed on spatially grouped patches. 
% After that, cross-attention over deformable patches is used considering long-range dependencies in \textbf{DGP-A} (\Cref{sec:3.3}). 
% With \textbf{LGP-A} and \textbf{DGP-A}, we present entire architecture of Deformable PoTR in \Cref{sec:3.4}. \SH{Finally, we will discuss about the relationship and competitiveness to prior works in \Cref{sec:3.5}}.
