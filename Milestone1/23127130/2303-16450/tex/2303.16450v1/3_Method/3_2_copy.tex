%Self-positioning Receptive Fields based Attention
\label{sec:3.2}

% \jyp{We propose the SPoTr block that utilizes \emph{local points attention}~(LPA) module and \emph{self-postioning receptive fields based attention}~(SPA) module to capture not only local fine-grained and short-distance information but also long-distance and global information}.\\

We propose an efficient global attention called \textit{self-positioning point-based attention} (SPA) to resolve the inherent limitation of Transformer for point clouds (\eg, scalability).
% While global attention is important to understand the global shape context, directly computing a standard global self-attention on the whole input points requires a high cost, which thus would be impractical for large-scale data.
Herein, SPA computes attention weights with only a small set of self-positioning points named \textit{SP points}, as illustrated in~\Cref{fig:fig2}.
Since the overall shape context is captured by only a small number of SP points, the position of the SP points $\Vpoint_s$ needs to be \textit{flexible}, so that the points become sufficiently representative of local part shapes.
With our method, the SP points are adaptively located according to the input shape (see~\Cref{sec:4.3} for visualizations).
Similar to the offsets in deformable convolutional neural networks~\cite{dai2017deformable} calculated with the feature of each pixel, we calculate $\Vpoint_s$ with its latent vector $\Zfeat_s$ and feature $\Feat_i$ of all input points $\forall x_i \in \mathcal{P}$, where $\Pc$ is the set of points.
We adopt adaptive interpolation and compute the positions of SP points as
\begin{equation}
\label{eq:spgp}
    \Vpoint_s = \sum_{i} \text{softmax}\left(\Feat_i^\top \Zfeat_s\right)\Cen_i.
\end{equation}
Hence, the SP points are always located nearby input points and more precisely they stay within the convex hull of input points.
% We adopt \textit{adaptive interpolation} and compute the self-positioning focal point positions as:
% \begin{equation}
% \label{eq:spgp}
%     \Vpoint_k = \sum_i \alpha_{ik}\Cen_i, where 
% \end{equation}
% A coefficient $\alpha_{ik}$ is computed as $\alpha_{ik} = \text{softmax}\left(\Feat_i^\top \Zfeat_k\right)$, where $\Feat_i$ is the corresponding feature vector for the point $x_i$ and $\Zfeat_k$ is the latent vector for self-positioning.

Then, SPA, equipped with SP points, performs global cross-attention in two steps: aggregation and distribution.
At the aggregation step, SPA aggregates the features from all input points considering both spatial and semantic proximity. It can be written as
\begin{equation}
\label{eq:bilateral}
    \Vfeat_s = \sum_i g\left(\Vpoint_s, \Cen_i  \right)\cdot h\left(\Zfeat_s, \Feat_i\right)\cdot \Feat_i,
\end{equation}
where $g, h$ are spatial and semantic kernel functions, respectively.
For the spatial kernel function $g$, we use the Radial Basis Function~(RBF) as
\begin{equation}
\label{eq:spatial}
    g\left(\Vpoint_s, \Cen_i\right) = \sum_i \exp\left(-\gamma \left\lVert \Vpoint_s - \Cen_i\right\rVert^2 \right),
\end{equation}
where $\gamma \in \mathbb{R}_{++}$ is a bandwidth that adjusts the size of receptive fields.
If $\gamma$ has a higher value, the size of receptive fields gets smaller.
For the semantic kernel function $h$, we utilize the attention-based kernel as below:
\begin{equation}
\label{eq:semantic}
    h\left(\Zfeat_s, \Feat_i\right) = \frac{\exp\left(\Feat_i^\top \Zfeat_s\right)}{\sum_{i^\prime}\exp\left(\Feat_{i^\prime}^\top \Zfeat_s\right)}.
\end{equation}
% A phrase or sentence about bilateral filter and discuss edge preserving (or unnecessary smoothing) -> our case.
Only considering the spatial information can cause \textit{information smoothing}, \ie, the information from neighboring points with different semantics can reduce the descriptive power.
Thus, we consider both spatial proximity and semantic proximity through two separate kernels $h$ and $g$.
These separate kernels can be interpreted as \textit{disentangled attention}.
The disentangled attention, which works similarly to the bilateral filter, allows the SP points to have greater descriptive power.
Further analyses on how it processes, can be seen in ~\Cref{sec:4.3}. 
Finally, at the distribution step, SPA performs the cross-attention between SP points and input points as follows:
% \begin{equation}
% \label{eq:globalatt}
%     \Feat_i^\text{SPA} = \mbox{CWAtt}\left(\Cen_i, \Feat_i, \Vpoint_k, \Vfeat_k\right)\odot \mathcal{M}\left(\left[\Vfeat_k;\phi_{ik}\right] \right),
% \end{equation}
\begin{equation}
\label{eq:globalatt}
    \Feat_i^\text{SPA} = \mbox{PCWA}\left(\Cen_i, \Feat_i, \left\{\Vpoint_s\right\}_{s\in \Omega_{\text{sp}}}, \left\{\Vfeat_s\right\}_{s \in \Omega_{\text{sp}}}\right),
\end{equation}
where $\Feat_i^\text{SPA}$ is the final output of the SPA, $\mbox{PCWA}$ is a point channel-wise attention \yx{Here makes me ask what is a point channel-wise attention. Should we add "that will be described next.}. 

% \noindent\textit{Remarks.} The SP points in SPA are used in two ways. In the aggregation step  \eqref{eq:bilateral}, SP points act like `query', and in the distribution step \eqref{eq:globalatt} they play the role of `key' in the attention. 
% Also, SPA is closely related to other cross-attention based architectures. For instance, the role of our SP points is similar to the \textit{latent array} in Perceiver IO \cite{jaegle2021perceiver}. 

\paragraph{Point Channel-wise Attention.}
Point Channel-Wise Attention~(PCWA) computes attention weight between query and key points for each channel, different from the standard attention that generates the same attention weight across channels.
PCWA is formulated as follows:
\begin{equation}
\begin{split}
    &\mbox{PCWA}\left(x_{q}, \Feat_q, \left\{x_k\right\}_{k \in \Omega_{\text{key}}}, \left\{\Feat_k\right\}_{k \in \Omega_{\text{key}}}\right) \\
    &=  \sum_{k \in \Omega_{\text{key}}} \mathbb{A}_{q,k,:} \odot \mathcal{M}\left([\mathcal{R}(\mathbf{f}_q,\mathbf{f}_k);\phi_{qk}] \right),
\end{split}
\end{equation}
where $x_q \in \mathbb{R}^3$ is a position of the query point and $\Feat_q \in \mathbb{R}^C$ is the corresponding feature, $x_k \in \mathbb{R}^3$ is a position of the key point and $\Feat_k \in \mathbb{R}^C$ is corresponding feature, and $\Omega_{key}$ indicates the set of key points. 
To take into account the relative information of contexts and positions, we use $\phi_{qk}$ and $\mathcal{R}$ as the input.
$\phi_{qk}$ is a normalized relative position of the key point based on the query point, $\mathcal{R}$ is the relation function between query and key feature~(\eg, $\mathbf{f}_q-\mathbf{f}_k$) and $\mathcal{M}$ indicates the mapping function.
 The channel-wise attention $\mathbb{A}_{q,k,:} \in \mathbb{R}^C$ between the query point and the key point is defined as follows:
 \begin{equation}
     \mathbb{A}_{q,k,c} = \text{softmax}\left(\mathcal{M}'\left([\mathcal{R}'(\mathbf{f}_q,\mathbf{f}_k);\phi_{q,k}]/\tau \right)_c \right),
 \end{equation}
where c is the index of channel, $\tau$ denotes temperature, $\mathcal{M}^\prime$ is the mapping function, and $\mathcal{R}^\prime$ is the relation function.
By adopting channel-wise attention, point channel-wise attention can learn more powerful and flexible point cloud representations compared to standard attention.
% aggregate features and globally distribute the aggregated information to all input points. In other words, the focal points play the role of query and key in Transformer attention for aggregation in \eqref{eq:bilateral} and distribution in  \eqref{eq:globalatt}, respectively. 
% Also focal points can be viewed as \textit{latent array} in Perceiver IO \cite{abc}.}

% \paragraph{Complexity Analysis of SPA.}
% A na\"ive implementation of the standard global self-attention requires a huge computation cost with the complexity of $\mathcal{O}\left(nd^2+n^2d \right)$ when $n$ is the number of input points and $d$ is the dimensionality of the hidden representation.
% Due to the quadratic complexity with respect to $n$, the global self-attention is not practical when $n$ is too large.
% On the contrary, our SPA only costs the complexity of $\mathcal{O}(n d^2+m d^2+ nmd)$, $m$ being the number of self-positioning focal points. 
% Since we use a small number of focal points, \ie,  $ m \ll n$, SPA is more efficient. For further discussion, see Section~\ref{sec:4.2}. 
% Latency comparison experiments.
% For instance, in the last block of part segmentation, we opt $m=32$ compared to the original $n=2048$ points, making the global attention approximately ${\frac{2048}{32}}=\mathbf{64}\times$ faster.
