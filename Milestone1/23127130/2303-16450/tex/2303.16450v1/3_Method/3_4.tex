\label{sec:3.4}

We design a Transformer-based architecture called SPoTr for point cloud tasks as illustrated in~\Cref{fig:fig4}. 
% Similar to vision Transformer architectures~\cite{dosovitskiy2020image,liu2021swin} in image domains, 
% we do \emph{not} utilize separate convolutional layers.
% the entire architecture is only based on attention blocks and multi-layer perceptions (MLP).

\paragraph{Classification.}
For the shape classification task, we build our Transformer encoder by stacking the SPoTr blocks  described in \Cref{sec:3.3}.
To increase the representational power, we first apply an MLP before operating the attention blocks following \cite{zhao2021point}.
Then, the SPoTr blocks are sequentially applied on sampled points, which are sampled through the farthest point sampling~(FPS).
In shape classification, we set $l=0$ for the SPoTr block since we empirically found that it is enough in shape classification task.
Besides, the sampling rates are 1/4 for every stage.
We use the ball query that selects points within a radius (an upper limit of the number of neighborhoods is set in implementation) to generate a local point group $\Pointgroup_i$ centered at point $x_i$ following \cite{qi2017pointnet++}.
After the last stage, the features are aggregated by a max-pooling function and processed by an MLP.
% with a dropout probability of 0.5.

\paragraph{Segmentation.}
The encoder for semantic segmentation contains the SPoTr blocks and FPS.
Following previous studies~\cite{qi2017pointnet++}, we apply a U-net designed architecture, which contains the feature propagation layers and the SPoTr blocks for dense prediction.
Same to the classification, we use a ball query to generate a local group.
% For the number of neighbors, we use $k=32$ as the number of input points which is usually larger than classification tasks.
% k 추가로 작성 
% Since the local attention block includes the aggregate operation with a symmetric function, we only use the global attention blocks for upsampling.
The outputs of the final block are processed by an MLP.
More details on SPoTr architectures are in the supplement.
% with a dropout probability of 0.5.
