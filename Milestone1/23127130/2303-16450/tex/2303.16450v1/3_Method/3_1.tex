\label{sec:3.1}
% PointNet++
% PointTransformer <- point transformer
% We briefly revisit the formulation of multi-head attention~(MHA) in Transformer.
% Let $\mathbf{z}_q \in \mathbb{R}^{c}$ be a query vector and $\mathcal{F} = \left\{\mathbf{f}_k \right\}_k$ be a set of key vectors, where $q \in \mathrm{\Omega}_q$ is an index of the query, $k \in \mathrm{\Omega}_k$ is an index of the key, and $\mathrm{\Omega}_q, \mathrm{\Omega}_k$ are the set of query and key indices, respectively. 
% Then, the multi-head attention~(MHA) operation can be formulated as follows~\cite{vaswani2017attention,zhu2020deformable}:

% \begin{equation}
% \label{eq:mha}
%     \text{MHA}\left(\mathbf{z}_q, \mathcal{F}\right) = \sum_{m=1}^M \mathbf{W}_m \left[\sum_{k \in \mathrm{\Omega}_k} \textbf{A}_{mqk}\cdot \mathbf{W}_m^\prime \mathbf{f}_k \right],
% \end{equation}
% where each $\mathbf{W}_m, \mathbf{W}_m^\prime \in \mathbb{R}^{c_v \times c}$ are learnable weight matrices, $\mathbf{A}_{mqk}$ is an attention weight between $q$-th query and $k$-th key and $m$ denotes the index of the $M$ attention heads.

% {In this subsection, we briefly revisit the attention in Transformer and the point-based approaches such as  PointNet++~\cite{qi2017pointnet++} and Point Transformer~\cite{zhao2021point}.
% }
In this subsection, we briefly revisit the point-based approaches such as  PointNet++~\cite{qi2017pointnet++} and Point Transformer~\cite{zhao2021point}.


% Assume that a point set $\mathcal{P} = \left\{x_i, \mathbf{f}_i \right\}_{i=1}^N$, where $x_i$ is the position of $i$-th point and $\mathbf{f}_i$ is corresponding feature.


% \paragraph{Standard attention~\cite{vaswani2017attention}}proposed in Transformer has shown success in various domains.
% Let $\mathbf{z}_q \in \mathbb{R}^{c}$ be a query vector and $\mathcal{F} = \left\{\mathbf{f}_k \right\}_k$ be a set of key vectors, where $q \in \mathrm{\Omega}_\text{query}$ is an index of the query, $k \in \mathrm{\Omega}_\text{key}$ is an index of the key, and $\mathrm{\Omega}_\text{query}, \mathrm{\Omega}_\text{key}$ are the set of query and key indices, respectively. 
% Then, the attention operation can be formulated as follows~\cite{vaswani2017attention,zhu2020deformable}:
% \begin{equation}
% \label{eq:mha}
%     \text{Attention}\left(\mathbf{z}_q, \mathcal{F}\right) = \sum_{k \in \mathrm{\Omega}_{\text{key}}} \textbf{A}_{qk}\cdot \left(\mathbf{W} \mathbf{f}_k \right),
% \end{equation}
% {where $\mathbf{W} \in \mathbb{R}^{C \times C}$ is a learnable weight matrix, $\mathbf{A}_{qk}$ is an attention weight between $q$-th query and $k$-th key. 
% The attention weights are computed as $\text{SoftMax}\left({\mathbf{z}_q^\top \mathbf{U}^\top \mathbf{V} \mathbf{f}_k} \right)$, where $\mathbf{U}, \mathbf{V} \in \mathbb{R}^{C \times C}$ are learnable weight matrices.}

% where each $\mathbf{W} \in \mathbb{R}^{c \times c}$ are learnable weight matrices, $\mathbf{A}_{mqk}$ is an attention weight between $q$-th query and $k$-th key. The attention weights are computed as $\exp\left({\mathbf{z}_q^\top \mathbf{U}_m^\top \mathbf{V}_m \mathbf{f}_k} \right)$, where $\mathbf{U}_m, \mathbf{V}_m \in \mathbb{R}^{c \times c}$ are learnable weight matrices. 

\paragraph{PointNet++~\cite{qi2017pointnet++}} captures local shape information through set abstraction and local grouping.
Given that a point set $\mathcal{P} = \left\{x_i \right\}_{i=1}^N$, where $x_i$ is the position of the $i$-th point, and its corresponding feature $\mathbf{f}_i$, PointNet++ proposed local set abstraction as follows:
\begin{equation}
    \mathbf{f}_i' = \mathcal{A}\left(\left\{\mathcal{M}\left([\mathbf{f}_j;\phi_{ij}] \right),\  \forall j \in \mathcal{G}_i \right\}\right),
\end{equation}
where $\mathcal{M}$ is the mapping function~(\eg, MLP), $\mathcal{A}$ is the aggregation function such as max-pooling, and $\mathcal{G}_i$ is the index set of the local group centered on the $i$-th point.


\paragraph{Point Transformer~\cite{zhao2021point}} leverages self-attention operations~\cite{zhao2020exploring} to represent local point groups.
% Different from standard self-attention, Point Transformer applies vector subtraction attention to calculating attention map.
Similar to PointNet++, Point Transformer leverages local grouping to represent local point groups with a self-attention mechanism as follows:
% \jyp{Several works~\cite{zhao2021point,ran2021learning} tried to harness the power of the attention on the point cloud domain.
% For instance, Point Transformer~\cite{zhao2021point} leverages vector self-attention operations~\cite{zhao2020exploring} to represent local point groups.
% It is based on vector self-attention~\cite{zhao2020exploring}.
% Given a point set $\left\{x_i\right\}$ and its corresponding feature $\left\{\mathbf{f}_i\right\}$, the self-attention in~\cite{zhao2021point} operates on \textit{local} point group $\mathcal{G}_i$} as below:
% \begin{equation}
%     % \mathbf{y}_q = \sum_{k \in \mathrm{\Omega}_k} \mathbf{A}_{qk} \odot \alpha\left(\mathbf{f}_k\right), \quad \text{where } 
%     \mathbf{A}_{mqk} = \rho\left(\eta\left(\varphi\left(\mathbf{z}_q\right) - \zeta\left(\mathbf{f}_k\right) + \phi_{qk} \right)\right),    
% \end{equation}
%%%%%%%%%%%%%%
\begin{equation}
    % \mathbf{y}_q = \sum_{k \in \mathrm{\Omega}_k} \mathbf{A}_{qk} \odot \alpha\left(\mathbf{f}_k\right), \quad \text{where } 
    \begin{split}
    &\mathbf{f}_i^\prime = \sum_{j \in \Gc_i} \mathbf{A}_{ij} \odot\left(\mathbf{W}_1\mathbf{f}_j + \Delta_{ij} \right), \\
    &
    \mathbf{A}_{ij} = \text{SoftMax}\left(\mathcal{M}\left(\mathbf{W}_2\mathbf{f}_i - \mathbf{W}_3\mathbf{f}_j + \Delta_{ij} \right)\right),  
    \end{split}
\end{equation}
where $\odot$ denotes element-wise multiplication,  $\mathbf{W}_1$, $\mathbf{W}_2$, $\mathbf{W}_3$ are learnable weight matrices, $\mathcal{M}$ is a mapping function such as multi-layer perceptron, and $\Delta$ is a positional encoding.
% where $\varphi, \zeta$ is linear projection functions, $\eta$ is a mapping function such as multi-layer perceptron, $\phi$ is a positional encoding and $\rho$ is a normalization function (\ie, softmax).
Point Transformer~\cite{zhao2021point} has shown the advantage of the attention mechanism on point clouds only with `local attention' since computing the global attention on whole input points is almost infeasible on large-scale data.

% While several methods have shown the advantage of attention mechanism on point clouds, they are limited to `local attention'.
% While it is possible to apply existing attention methods such as scaled dot product attention on whole input points, it is challenging to naively apply it due to heavy computation cost with the complexity of $\mathcal{O}\left(nd^2 + n^2d\right)$ when $n$ is the number of input points.

% Why global attention is important 
% interpretability
% However, most works are limited to `local attention' due to heavy computation for global attention on the whole input points.
% \jyp{However, most works are limited to `local attention' without considering  of point clouds.}

% The input of the multi-head attention consists of a query and a set of keys and values.

% the multi-head attention operation adaptively aggregates a set of values with attention weights, which represent the compatibility between the query and the key.


% The success of self-attention has led researchers to study its adoption on point clouds.
% There has been papers employing self-attention for point cloud analysis.
% However, prior works~\cite{zhao2021point,ran2021learning}, naturally trying to harness the power of self-attention on the point cloud domain, are limited to \textit{local} self-attention due to heavy computation for global attention on the whole input points.

% \jyp{
% % With multi-head attention and self-attention operations, the transformers have shown remarkable success in the 2D computer vision as well as natural language processing by learning long-range dependencies between representations.
% Even if the transformers have shown remarkable success with the attention operations, most existing transformer architectures for point cloud are designed for only capturing local dependencies without considering long-range dependencies between point representations.
% To address this limitation, our proposed transformer architecture is based on SPA block, which consists of local self-attention operation and global cross-attention operation with self-positioning representative points to capture fine-grained short-range information and long-range information.  
% % To address this limitation, our proposed transformer architecture is composed of two attention blocks : \emph{(i) locally-grouped point attention block}~(\Cref{sec:3.2}) and \emph{(ii) deformable global point attention block}~(\Cref{sec:3.3}) to capture fine-grained short-distance information and global long-range information.
% }

