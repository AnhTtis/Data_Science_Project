\label{sec:4.1}

\input{Tables/ScanObjectNN.tex}

\paragraph{Shape Classification.}
% \subsubsection{Datasets.} 
For the shape classification, we validate SPoTr on a real-world dataset ScanObjectNN (\textbf{SONN})~\cite{uy2019revisiting}. 
% and synthetic dataset ModelNet40 (\textbf{MN40})~\cite{wu20153d}.
SONN has 2,902 objects categorized into 15 classes from SceneNN~\cite{hua2016scenenn} and ScanNet~\cite{dai2017scannet}.
Among diverse variants of SONN, we use PB\_T50\_RS (\textbf{SONN\_PB}), which is the most challenging version with random perturbation and contains 14,510 objects in total.
We follow the official split of \cite{uy2019revisiting}, where they divide SONN into 80\% for training and 20\% for evaluation.
Also, we sample 1,024 points for training and evaluating the models. 
% MN40 is a synthetic dataset with 12,311 meshed CAD models from 40 categories. 
% We use 9,843 models for training and 2,468 models for evaluation.

% Our model achieves state-of-the-art performance SONN\_PB.
\Cref{tab:sonn} shows that SPoTr outperforms all baselines with the mean of class accuracy (mAcc) of 86.8\% and overall accuracy (OA) of 88.6\% (+1.0\% mAcc, +0.9\% OA).
This result shows that capturing long-range context is important for recognizing 3D shapes in real-world datasets.
% In \Cref{tab:mn40}, SPoTr achieves an overall accuracy of 94.1\% on MN40 without any additional information~(\eg, normal vector). %or voting strategy in~\cite{liu2019relation}.
% Compared to baselines with the identical setting as ours, \ie, without a normal vector, SPoTr outperforms all baseline models.