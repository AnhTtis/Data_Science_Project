\label{sec:4.3}
For a deeper understanding of each component in SPA, such as self-positioning points~(SP points) and disentangled attention, we provide qualitative results in this section.
We use SN-Part for visualizations.

\paragraph{Self-positioning points.}
As mentioned in Section~\ref{sec:3.2}, it is important that SP points are adaptively located considering the input shape.
% We visualized SP points (colored in cyan) of the first SpoTr block in our model.
\Cref{fig:SPpoints} shows that the SP points are adaptively located on various samples from different categories.
Interestingly, a specific SP point (colored in red) appears at a \textit{semantically similar} place for each category.
For example, red dots from airplanes are always near the left-wing.
% , red dots from the table are always on the leg.
This consistent placement of SP points implies that each SP point learns to represent semantically similar regions.

\paragraph{Disentangled attention.}
SPA aggregates feature considering spatial proximity as well as semantic proximity via disentangled attention as introduced in \Cref{sec:3.2}. 
\Cref{fig:dis_att} shows the weights of the spatial kernel $g$ and the effective receptive field of the disentangled attention $g\cdot h$.
Cyan-colored points are selected SP points and kernel weights are illustrated in heatmaps.
With only the spatial kernel $g$, SP point blindly aggregates the information of neighbor points inducing \textit{irrelevant} information from close regions (\eg, a wing and a body of an airplane are strongly colored in the second row of the figure).
Conversely, with our disentangled attention $g\cdot h$, the same SP point selectively aggregates information considering both spatial proximity and semantic proximity~(\eg, the right-wing is only colored in the figure).
The obvious difference suggests disentangled attention is crucial for enhancing the descriptive power by suppressing irrelevant information.

% Furthermore, we qualitatively validate the global cross-attention in the distribution step.
% \Cref{fig:fig6}(c) displays the attention weight, which indicates how the SP point distributes information to entire input points.
% As shown in the figure, the SP points \textit{non-locally} influence semantically related points.
% For example, on the airplane in \Cref{fig:fig6}(c), the SP point conveys the information aggregated from one wing to semantically related points including the remote points at the other wings.


% \SH{focal points need to be} adaptively located considering the input shape.
%it is important that focal points are adaptively located considering the input shape.

% For each shape, cyan-colored points are entire points from the latent vector $\Zfeat_k$, and the red-colored point is a point from the specific index of the $\Zfeat_k$.
% By and large, the focal points are self-positioned in accordance with the various shape from the learned latent vector $\Zfeat_k$ without any explicit information regarding their positions. 
% In addition, we observed that red dots appear at a \textit{semantically similar} place for each category.
% For example, red dots from airplanes are always near the right wing, red dots from guitars are always on the neck.
% This consistent observation implies each element from the latent vector implicitly represents the specific semantics.
% points from the same latent index understand the same semantics.

% Interestingly, our attention weights look similar to part segmentation labels as if part labels have emerged from the global cross-attention.

% \jyp{
% To validate the effectiveness of disentangled attention, we compute the receptive field of the self-positioning global point by measuring the weight of spatial kernel $h$ and disentangled attention $h\cdot g$. 
% \Cref{fig:fig6} illustrates the measured weights between input points of the shape~(colored with a white) and a single global point~(colored with a cyan) with the intensity of yellow color.
% % As shown in the leftmost of the figure of each sample, when applying only the spatial kernel, the weight has a high value when the input point is just located near the global point.
% % smoothing이야기 같이 위에 녹여내야 한다.
% % Whereas, the disentangled attention refines the local information and receives information from the semantically similar points.
% As shown in the figure, the receptive fields of spatial kernel only considers local information, whereas the disentangled attention seems to simultaneously adapt spatial and semantic information.
% In the case of the airplane in the figure, the global point aggregates noisy information such as a body part when applying spatial kernel.  
% On the contrary, the disentangled attention refines local information and receives semantically similar information such as a wing part.
% This flexible receptive fields show that disentangled attention helps self-positioning global points construct powerful representations by receiving information from meaningful points.
% }
% \paragraph{Global cross-attention visualization.}

% \jyp{
% Further, based on the representations of self-positioning points $\Vfeat$, we analyze the behaviour of cross-attention~\eqref{eq:globalatt} in SPA.
% \Cref{fig:fig6}(c) shows the attention weight, which is measure of how global points influence on input points.  
% We can see that the global points non-locally influence a lot on related points.
% As shown in the figure, higher weights are assigned in arms of chair, tires of motorcycle, upper part of desks, and wings of airplane.
% This adaptive behaviour demonstrates that a small set of global points globally convey important information to relevant input points.  
% % Indeed, the global point aggregates information from 
% }
