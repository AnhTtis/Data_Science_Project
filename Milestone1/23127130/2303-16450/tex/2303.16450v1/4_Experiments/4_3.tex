\label{sec:4.2} 
\input{Tables/abl_tr.tex}
\input{Tables/ablation_rfunc.tex}
\input{Fig_tex/SPpoints.tex}
\input{Fig_tex/dis_att.tex}

\paragraph{Ablation studies.}
We explore how self-positioning positions (SP) and disentangled attention contribute to SPA.
\Cref{table5} shows the final results on SONN, where the baseline~(\textit{w/o SPA}) learns only with local point attention.
In the case of \textit{w/o self-positioning}, we use FPS to randomly select a small set of points for cross-attention, and for \textit{w/o disentangled attention}, we only adopt the spatial kernel function $g$.
Our model with all the components of SPA achieves the best performance of 88.6\% in overall accuracy.
This superior performance verifies that every component is crucial for SPA.
In particular, when we use FPS instead of SP, the performance is even worse than the baseline as overall accuracy dropped from 87.9\% to 87.7\%.
This observation suggests the positions of SP points \textit{matter} for global cross-attention.
Rather than simple sampling, our learnable approach successfully locates SP points and makes global cross-attention effective.
Next, with \textit{w/o disentangled attention}, the performance gain in OA is minimal (0.3\%) over the baseline compared to using disentangled attention (0.7\%).
It indicates that disentangled attention improves the descriptive power by filtering semantically irrelevant information.

\paragraph{Attention types and semantic relation $\mathcal{R}$.}
In \Cref{table6}, we conduct experiments to compare the models with different attention types (Standard attention in Transformer~\cite{vaswani2017attention} and our CWPA) and semantic relations ($\mathcal{R}(\mathbf{f}_q, \mathbf{f}_k) = $ $\mathbf{f}_k$, $\mathbf{f}_q+\mathbf{f}_k$, $\mathbf{f}_q\odot\mathbf{f}_k$, and $\mathbf{f}_q-\mathbf{f}_k$).
The models adopting the CWPA outperform the model with the standard attention, which shows that the channel-wise point attention operation is more powerful to represent point clouds compared to the standard attention.
Furthermore, the results demonstrate that Sub~($\mathbf{f}_q-\mathbf{f}_k$) is most appropriate to model the semantic relation between points. 

% \SH{We verify that every component is critical for SPA as utilizing both components achieves the best performance of 85.1\% in overall accuracy.}


% SP, disentangled 강조, 키워드 살리기, represntative, long-range 

% This observation suggests positions learned by SP
% Also, We can know that learning positions of focal points is more effective than simply locating them via heuristic without considering semantic information.  
% Also, in the case of using only the spatial kernel, the performance gain is minimal (0.2\%) over the baseline compared to using disentangled attention (0.6\%).

% \SH{
% To better understand the contribution of each component in our model, we conduct ablation studies on PB\_T50\_RS variant of ScanObjectNN. 
% % Specifically, we measure the effect of SPA, self-positioning, and disentangeld attention  of the SPA.
% % The results are provided in~\Cref{table5}.
% \Cref{table5} reports the performance after excluding each component of SPoTr.
% The table shows that all components contribute to improve performance.
% % Compared to SPoTr with the model without SPA, SPA achieves 0.6\% improvement on overall accurac.
% % Model A is set to SPoTr block without SPA while model B is SPoTr block without GPA.
% % Both models show better performance compared to the other baselines in \Cref{table1}.
% % \paragraph{Ablation studies.} 
% % All SPoTr variants without some components show performance drops compared to the full architecture, which means that each component contributes to the performance improvement of SPoTr. 
% % Compared to model C, which uses FPS for determining the position of the focal point instead of self-positioning~(SP), model E gets 1.9\% higher overall accuracy~(OA).
% In particular, compared to SPoTr and  
% This indicates that the focal points are adaptively located with the self-positioning mechanism. 
% % Also, we can observe that disentangled attention kernel contributes the improvement of performance by comparing SPoTr and the model without semantic kernel $h\left(\cdot\right)$. 
% Lastly, we observe that using the disentangled attention kernel~(Model E), instead of spatial kernel~(Model D), improves the overall accuracy with 0.4\%.
% }
%by comparing SPoTr and the model without $h$.  
% For the model without self-positioning~(SP), we use farthest point sampling~(FPS) for selecting global points.
% Finally, the best accuracy 85.1\% is obtained when applying both LPA and SPA. 
% As mentioned in~\Cref{sec:3.2}, we think that the performance improvement is achieved by capturing local structural information and resolving the limitation of the sole local-attention.}
% Otherwise, if model considers only global information without LPA, it achieves 84.0\% which is still surpassing the previous state-of-the-art models. 
% Further, we deeper explore the self-positioning and bilateral filter in SPA.
% \begin{itemize}
%     \item LGPA
%     \item DGPA
% \end{itemize}


% \paragraph{Number of Self-positioning Points.}
%relation between \# of points -> graph로 그림하나 넣기


%keyword : scalability, long-range dependency 
\input{Tables/complexity.tex}
% \paragraph{Efficiency comparison with baselines.}
\paragraph{Complexity analysis on SN-Part.} 
We analyze the space and time complexity to validate the computational efficiency of SPoTr during inference time with a batch size of 8.
For a baseline, SPA in SPoTr is replaced by the standard global self-attention (abbreviated in GSA) with CWPA. 
For a comparison with GSA requiring the quadratic complexity, we inevitably use the variants of SPoTr, where the channel size of each layer is reduced by $\times 1/4$.
For space complexity, we measure  the number of parameters and total memory usage, and for time complexity, we measure FLOPs and throughput performance.
\Cref{tab:complexity} empirically proves the efficiency over GSA.
For space complexity, GSA shares a similar number of parameters with SPA but introduces a large memory usage of 24.2 (GB). Instead, Our SPA only uses 2.5 (GB) (-89.7\%). Also, SPA largely reduces the time complexity from 114.0 GFLOPS with a throughput of 17.7 (shapes/s) to 10.8 GFLOPS (-90.5\%) with a throughput of 281.5 (shapes/s) ($\times$15.9).
% To investigate the efficiency of SPoTr, we compare our method with recent baselines~\cite{ma2022rethinking, ran2022surface} on ScanObjectNN~\cite{uy2019revisiting}. Specifically, we use DeepSpeed~\cite{rasley2020deepspeed} library as a profiler for calculating the number of parameters and FLOPS during inference. For comparison, we also opt a light version of SPoTr (SPoTr*), where the channel size of each layer is 2/3. The results are summarized in~\Cref{tab:efficiency}. It is worth noting that SPoTr achieves the best performance (88.6\%) with fewer parameters of 3.3(M) than 13.2(M) of PointMLP and 6.8(M) of RepSurf. Further, although SPoTr* only requires 1.6(M) parameters and 5.5 GFLOPS, it still shows significant gains over the previous best methods (+2.2\%). In short, we demonstrate that SPoTr is a computation-efficient and memory-efficient method. 
% \input{Tables/efficiency.tex}

% SPA introduces 188 seconds additional time with $+1.8$MB of memory usage, whereas 
% In sum, our SPA reduces space and time overhead about ${3.5}\times$ and $5\times$ lower than GSA, respectively.

% \SH{
% To probe the efficiency of SPA, we analyze the space and time complexity on S3DIS during training with a single Nvidia RTX A6000 GPU.
% For a baseline, SPA in SPoTr is replaced with global self-attention(GSA).
% For space complexity, we measure memory usage with a batch size of 8.
% Also, the time complexity is measured by considering the latency per epoch of SPA and GSA block, respectively.
% % To probe the efficiency of SPA, we compare SPoTr with the model
% % From the \Cref{table6}
% SPA introduces 188(s) additional computational time with 10.8(GB) of memory usage.
% Whereas, GSA requires much more extra costs: 35.6(GB) memory usage and +910 seconds.
% Remarkably, our SPA reduces extra space and time overhead with about ${3.5}\times$ and $5\times$ lower than GSA, respectively.
% % The model B is SPoTr, which uses both LPA and SPA.  
% % From the table, 
% % In \Cref{table6}, we report the memory usage and  
% % For complexity analysis, using single Nvidia RTX A6000, we measure the cost on S3DIS with the batch size of 8 during training.
% % We measure the costs on a single Nvidia RTX A6000 GPU with the batch size of 8 during training for complexity analysis.
% % Baseline is model A that only considers local shape context with LPA.
% % Compared to baseline, model B~(SPoTr) introduces 188(s) additional computation with +1GB increment of memory usage. 
% % Whereas, with model C, where we add global self-attention(GSA) instead SPA, it requires much more extra costs: +25.8GB memory and +910 seconds. 
% % Remarkably, our SPA reduces extra space and time overhead with ${26}\times$ and $5\times$, respectively, lower cost than model C, respectively.
% % Whereas, although global self-attention(GSA) is natural for capturing global shape context, it is ineffective in both space and time complexity. 
% % As shown in table, compared to baseline model without SPA, 
% % we observe that SPoTr $w/$ SPA outperforms SPoTr $w/$ GSA with fewer latency. 
% }
% %Flops & time complexity table using shapenet part

