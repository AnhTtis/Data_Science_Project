%\newcommand\inlineeqno{\stepcounter{equation}\ (\theequation)}

\section{Asymptotic Normality}
\label{sec:asymptotic_normality}
%
We now establish asymptotic normality of \method~under additional conditions.
%
Since \method~is agnostic to the learning algorithm used in the horizontal regression, as demonstrated by the discussion on CART, the Lasso can be replaced by any regression technique that achieves asymptotic normality. 
%
For example, previous works have studied variants of the Lasso \cite{liu2013asymptotic,zou2006adaptive}, where it has been established that the predictions are asymptotically normal.
%
Here, we propose replacing the Lasso with the ``Select + Ridge'' (SR) procedure of \cite{liu2013asymptotic}, and specify the conditions on the donor units' observation pattern under which SR achieves asymptotic normality. 
%
Next, using asymptotic normality of the horizontal regression predictions, asymptotic normality of the vertical regression step of \method~is established.
%


\subsubsection{Horizontal Regression Asymptotic Normality}
\label{subsec:horizontal_regression_asymptotic_normlity}
The SR estimator consists of the following two steps. 

\noindent \textbf{Step 1: Subset Selection.} For every unit $u$ in the donor set $\mathcal{I}$, fit a Lasso model with regularization parameter $\lambda_u$ using  $\bchi(\Pi_u)$ as described in Step 1 of \method~to obtain $\hat{\balpha}_u$.
%
Let $\hat{\mathcal{S}}_u = [S \subset [p]: \hat{\alpha}_{u,S} \neq 0]$ denote the non-zero coefficients of $\hat{\balpha}_u$.
%

\noindent Some additional notation needs to be defined for the next step.
%
For a unit $u \in \mathcal{I}$, and combination $\pi$, let $\bchi_{\hat{\mathcal{S}}_u}^{\pi} = [\chi^{\pi}_{S} : S \in \hat{\mathcal{S}}_u] \in \{-1,1\}^{|\hat{\mathcal{S}}_u|}$. 
%
Let $\bchi_{\hat{\mathcal{S}}_u}(\Pi_u) = [\bchi_{\hat{\mathcal{S}}_u}^{\pi}: \pi \in \Pi_{u}] \in \{-1,1\}^{|\Pi_{u}| \times |\hat{\mathcal{S}}_u|}$.
%


\vspace{1mm}


\noindent \textbf{Step 2: Ridge Regression.} In this step, SR forms predictions using the selected subsets in Step 1, and ridge regression. 
%
Specifically, for every $u \in \mathcal{I}$, compute the Fourier coefficient as follows,
\begin{align}\label{eq:Ridge_estimator}
\hat{\balpha}^{SR}_{u} =  \left(\left(\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)\right)^T\bchi_{\hat{\mathcal{S}}_u}(\Pi_u) + \frac{1}{|\Pi_u|}\mathbf{I}_{|\hat{\mathcal{S}}_u|} \right)^{-1} \left(\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)\right)^T \bY_{\Pi_u},
\end{align}
where $\mathbf{I}_{|\hat{\mathcal{S}}_u|} \in \mathbb{R}^{|\hat{\mathcal{S}}_u| \times |\hat{\mathcal{S}}_u|}$ is the identity matrix of size $|\hat{\mathcal{S}}_u|$. 
%
Then, let $\hat{\E}_{SR}[Y_u^{(\pi)}] = \langle  \hat{\balpha}^{SR}_{u}, \bchi^{\pi} \rangle$ denote the estimated potential outcome $\E[Y_u^{(\pi)}]$ for a combination $\pi$.
%\hat{\balpha}^{SR}_{u} =  \left(\bchi^T_{\hat{\mathcal{S}}_u}\bchi_{\hat{\mathcal{S}}_u} + \frac{1}{|\Pi_u|}\mathbf{I}_{|\hat{\mathcal{S}}_u|} \right)^{-1}, \bchi^T_{\hat{\mathcal{S}}_u} Y_u^{(\pi)}] 

%

Next, we present our result establishing asymptotic normality of SR predictions.
%
Some additional notation is required for our result. 
%
For a given  unit $u \in \mathcal{I}$,  let $\mathcal{S}_u = \{S \subset [p] ~ | ~ \alpha_{u,S} \neq 0 \}$. 
%
Like above, define $\bchi_{\mathcal{S}_u}^{\pi} = [\chi^{\pi}_{S} : S \in \mathcal{S}_u] \in \{-1,1\}^{|\mathcal{S}_u|}$ and $\bchi_{\mathcal{S}_u}(\Pi_u) = [\bchi_{\mathcal{S}_u}^{\pi}: \pi \in \Pi_{u}] \in \{-1,1\}^{|\Pi_{u}| \times |\mathcal{S}_u|}$.
%
Denote $\mathbf{K}_{u} = \frac{1}{|\Pi_u|}((\bchi_{\mathcal{S}_u}(\Pi_u))^T\bchi_{\mathcal{S}_u}(\Pi_u))$ as the covariance matrix of the observed Fourier characteristics for a donor unit $u$.
%%For a subset $S \subset [p]$, let $\bchi_{S}(\Pi_u) = [\chi_S(\pi) : \pi \in \Pi_u] \in \{-1,1\}^{|\Pi_u|}$.
For a square matrix $\bX$, let $\lambda_{\min}(\bX)$ denote its smallest eigenvalue. 



\begin{proposition} 
\label{prop:horizontal_asymptotic_normality}

Conditioned on $\mathcal{A}$, let Assumptions \ref{ass:observation_model} and \ref{ass:boundedness_potential_outcome} hold. Additionally, assume the following conditions hold for every donor unit $u$ and combination $\pi$,

\begin{enumerate}
    \item[(a)] $\epsilon_u^{\pi}$ are independent mean-zero independent Gaussian random variables with variance $\sigma^2$,
    \item [(b)] For every $S \subset [p]$, $\sum_{\pi \in \Pi_u} \chi_{S}(\pi) = 0$,
    \item [(c)]  $\lambda_{\min}(\mathbf{K}_u) \geq c_1$ for a positive constant $c_1$,
    \item [(d)] There exists constants $ 0 < c_2 \leq 1 $ and $0 < c_3 < 1 - c_2$ such that the sparsity $s = O(|\Pi_u|^{c_2})$ and $p = O(|\Pi_u|^{c_3})$,
    \item [(e)] $\P(\hat{\mathcal{S}_u} \neq \mathcal{S}_u) = o(e^{-|\Pi_u|^{c_3}})$
   
\end{enumerate}
Then, as $|\Pi_u| \rightarrow \infty$, we have
\begin{equation}
    \sqrt{\frac{|\Pi_u|}{\sigma^2 (\bchi^{\pi})^TK_u^{-1}\bchi^\pi }} \left(\hat{\E}_{SR}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right)  \xrightarrow{d} \mathcal{N}(0,1)
\end{equation}
\end{proposition}
%    \sqrt{\frac{|\Pi_u|}{ }} \left(\hat{\E}_{SR}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right)  \xrightarrow{d} 

\noindent Proposition \ref{prop:horizontal_asymptotic_normality} builds upon Theorem 3 of \cite{liu2013asymptotic} to establish asymptotic normality and as a result allows for construction of valid confidence intervals for the donor units. 
%
Condition (b) can be enforced by simply normalizing each row of $\bchi(\Pi_u)$ to have mean zero. 
%
Condition (c) is mild, and is required in order to ensure that the model is identifiable even if $\mathcal{S}_u$ is known \cite{wainwright2019high}. 
%
Condition (d) limits the maximum sparsity and number of interventions allowed in terms of the sample size $|\Pi_u|$.
%
Equivalently, condition (d) states that $|\Pi_u| = \Omega(s^{1/c_2})$ and $|\Pi_u| = \Omega(p^{1/c_3})$, i.e., at least a polynomial number of samples is required in both $s$ and $p$. 
%
Condition (e) states the probability of recovering the wrong support set (i.e., $\hat{\mathcal{S}}_u \neq \mathcal{S}_u$) decays exponentially quickly with $|\Pi_u|$.
%
Consistency of recovering the support is necessary for asymptotic normality, since $\hat{\balpha}^{SR}_u$ needs to be unbiased, which requires $\hat{\mathcal{S}}_u = \mathcal{S}_u$.  
%
Previous works \cite{wainwright2019high,zhao2006model} have established that $\P(\hat{\mathcal{S}_u} \neq \mathcal{S}_u)$ decays exponentially quickly for the Lasso under certain regularity conditions. 
%
Next, we use normality of the horizontal regression predictions to establish asymptotic normality for non-donor units. 

\subsubsection{Vertical Regression Asymptotic Normality}
%
This section establishes asymptotic normality for non-donor units assuming that the horizontal regression predictions are asymptotically normal.
%
This can be achieved, for example, by using the SR procedure described above. 
%
However, the result in this section does not assume the use of any particular horizontal regression method since step 1 of \method~is method agnostic. 
%
Rather, it is just assumed that the donor unit predictions are normal, allowing for the use of any horizontal regression method with asymptotically normal predictions---the variant of Lasso in Section \ref{subsec:horizontal_regression_asymptotic_normlity} being one such concrete example.
%
Intuitively, since non-donor unit predictions are a linear combination of donor set estimates, normality of the horizontal regression is necessary to ensure asymptotic normality of the vertical regression. 

We define some necessary notation for our result. 
%
Let $\Tilde{\bw}^n = \bV_{\mathcal{I}}^{(\Pi_n)}(\bV_{\mathcal{I}}^{(\Pi_n)})^T\bw^n \in \mathbb{R}^{|\mathcal{I}|}$, where $\bV_{\mathcal{I}}^{(\Pi_n)}$ are the right singular vectors of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$.
%
That is, $\Tilde{\bw}^n$ denotes the orthogonal projection of $\bw^n$ onto the rowspace of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$.
%
Let $\Tilde{w}_u^n \in \mathbb{R}$ denote a coordinate of $\Tilde{\bw}^n$ corresponding to a given donor unit $u \in \mathcal{I}$.
%
Additionally, recall that $M \coloneqq \min_{u \in \mathcal{I}} |\Pi_u| $.
%
%Next, we present our result establishing asymptotic normality of \method~for a given non-donor unit $n$, and combination $\pi$:

\begin{theorem} 
\label{thm:vertical_regression_normality}

For a given non-donor unit and combination pair $(n,\pi)$, let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. 
%
Define 
\begin{equation*}
    \Tilde{\sigma}_u= \left(\sqrt{\frac{\sigma^2 (\bchi^{\pi})^TK_u^{-1}\bchi^\pi }{|\Pi_u|}} \right)  \Tilde{w}_u^n
\end{equation*}
% \left(\sqrt{\sigma^2 (\bchi^{\pi})^TK_u^{-1}\bchi^\pi} \right) \Tilde{w}_u^n
Additionally, let the following conditions hold,
\begin{enumerate}
    \item [(a)] $|\Pi_n|, \ M \rightarrow \infty $,
    \item [(b)]
    \begin{equation} 
    \label{eq:asymptotic_normality_condition_1}
       \frac{\log^{3}(|\Pi_n||\mathcal{I}|)}{\lVert \Tilde{\bw}_n \rVert_2}  \left(sr^2_n\sqrt{\frac{p}{{M}}} +  \frac{r_n}{|\Pi_n|^{1/4}}\right) = o(1)
    \end{equation}
    
\end{enumerate}
\begin{enumerate}
    \item [(c)] For every donor unit $u \in \mathcal{I}$, as $|\Pi_u| \rightarrow \infty$, assume
    \begin{equation} 
    \label{eq:asymptotic_normality_condition_2}
        \sqrt{\frac{|\Pi_u|}{\sigma^2 (\bchi^{\pi})^TK_u^{-1}\bchi^\pi }} \left(\hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right)  \xrightarrow{d} \mathcal{N}(0,1),
    \end{equation}
\end{enumerate}
Then, we have that
\begin{equation} \label{eq:asymptotic_normality_vertical_regression}
    \frac{\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]}{\sqrt{\sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u}} \xrightarrow{d} \mathcal{N}(0,1).
\end{equation}
\end{theorem} % \left(\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right)

\noindent Theorem \ref{thm:vertical_regression_normality} establishes asymptotic normality for non-donor unit and combination pair $(n,\pi)$, allowing for construction of valid confidence intervals. 
%
For \eqref{eq:asymptotic_normality_condition_1} to hold,  $\lVert \Tilde{\bw}_n \rVert_2$ needs to be sufficiently large (e.g., $\lVert \Tilde{\bw}_n \rVert_2 \geq c$, for a positive constant $c$), and (ignoring $\log$ factors) if  $M = \omega(r^{4}_n s^2p)$, and $|\Pi_n| = \omega(r_n^4)$. 
%
Recall from the discussion in Section \ref{sec:theoretical_analysis}, $M = \omega(r^{4}_n s^2p)$, and $|\Pi_n| = \omega(r_n^4)$ are precisely the conditions required for consistency as well. 
%
As shown in Section \ref{subsec:horizontal_regression_asymptotic_normlity}, \eqref{eq:asymptotic_normality_condition_2} holds when using the SR estimator for horizontal regression. 
%


%Intuitively, since the non-donor unit predictions are a linear combination of the donor set estimates, normality of the horizontal regression predictions is required in order to achieve asymptotic normality for the vertical regression step of Synthetic Combinations. 

%
% given the horizontal regression predictions are asymptotically normal (i.e., if \eqref{eq:asymptotic_normality_condition_2} holds).



%Intuitively, this condition is re

%since $\hat{\E}[Y_n^{(\pi)}]$ is a linear combination of the horizontal regression estimates, non-donor units predictions are asymptotically normal as long as 


%For any combination $\pi$, let $\bchi_{\mathcal{S}_u}^{\pi} = [\chi^{\pi}_{S} : S \in \mathcal{S}_u] \in \{-1,1\}^{|\mathcal{S}_u|}$ denote the projection of $\bchi^\pi$ to the non-zero coordinates of $\balpha_u$. Let $\bchi_{\mathcal{S}_u}(\Pi_u) = [\bchi_{\mathcal{S}_u}^{\pi}: \pi \in \Pi_{u}] \in \{-1,1\}^{|\Pi_{u}| \times |\mathcal{S}_u|}$. 
%