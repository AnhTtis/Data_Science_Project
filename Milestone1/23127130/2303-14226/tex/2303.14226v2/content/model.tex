\section{Setup and Model}\label{sec:model}
%
In this section, we first describe requisite notation, background on the Fourier expansion of real-valued functions over Booleans, and how it relates to potential outcomes over combinations. 
%
Next, we introduce the key modeling assumptions and data generating process (DGP), along with the target causal parameter of interest.
%
We also provide a brief summary of how the formalism can be extended to potential outcomes over permutations in Section \ref{subsec:rankings_summary}, and a more detailed discussion in Appendix \ref{sec:permutations}.

\subsection{Notation}\label{sec:notation}

\noindent
{\bf Representing combinations as binary vectors}.
%
Let $[p] = \{1,\ldots p\}$ denote the set of $p$ interventions. 
%
Denote by $\Pi$ the power set of $[p]$, the set of all possible combinations of $p$ interventions, where we note $|\Pi| = 2^p$.
%
Then, any given combination $\pi \in \Pi$ induces the following binary representation $\bv(\pi) \in \{-1,1\}^p$ defined as follows: $\bv(\pi)_{i} = 2~\mathbbm{1}\{i \in \pi\} -1.$
%

\vspace{2mm}
\noindent
{\bf Fourier expansion of Boolean functions.}
%
Let $\mathcal{F}_{\text{bool}} = \{f : \{-1,1\}^p \to \mathbb{R}\}$ be the set of all real-valued functions defined on the hypercube $\{-1,1\}^p$. 
%
Then $\mathcal{F}_{\text{bool}}$ forms a Hilbert space defined by the following inner product: for any $f,g \in \mathcal{F}_{\text{bool}}$, $\langle f, g \rangle_{B} = \frac{1}{2^p}\sum_{\bx \in \{-1,1\}^p}f(\bx)g(\bx).$
%
This inner product induces the norm $\langle f, f \rangle_{B} \coloneqq \lVert f \rVert_{B}^2 = \frac{1}{2^p}\sum_{\bx \in \{-1,1\}^p}f^2(\bx).$
%
We construct an orthonormal basis for $\mathcal{F}_{\text{bool}}$ as follows: for each subset $S \subset [p]$, define a basis function $\chi_{S}(\bx) = \prod_{i \in S} x_i$ where $x_i$ is the $i^\text{th}$ coefficient of $\bx \in  \{-1,1\}^p$. 
%
One can verify that for any $S \subset [p]$ that $\lVert \chi_{S} \rVert_B = 1$, and that $\langle \chi_S, \chi_{S'} \rangle_B = 0$ for any $S' \neq S$. 
%
Since $|\{\chi_S: S \subset [p] \}| = 2^p$, the functions $\chi_S$ are an orthonormal basis of $\mathcal{F}_{\text{bool}}$.
%
We will refer to $\chi_S$ as the Fourier character for the subset $S$.

%
Hence, any $f \in \mathcal{F}_{\text{bool}}$ can be expressed via the following Fourier decomposition: $f(\bx) = \sum_{S \subset[p]}\alpha_{S}\chi_{S}(\bx),$
%
where the Fourier coefficient $\alpha_S$ is given by computing $ \alpha_S = \langle f, \chi_S \rangle_B $.
%
For a function $f$, we refer to $\balpha_f = [\alpha_S]_{S \in [p]} \in \mathbb{R}^{2^p}$ as the vector of Fourier coefficients associated with it. 
%
For a given binary vector $\bx \in \{-1, 1\}^{p}$, we refer to $\bchi(x) = [\chi_{S}(\bx)]_{S \in [p]} \in \{-1, 1\}^{2^p}$ as the vector of Fourier character outputs associated with it.
%
Hence any function $f : \{-1,1\}^p \to \mathbb{R}$ can be re-expressed as follows:
$f(\bx) =  \langle \balpha_f, \bchi(x) \rangle.$
%
For $\pi \in \Pi$, abbreviate $\chi_S(\bv(\pi))$ and $\bchi(\bv(\pi))$ as $\chi^{\pi}_S$ and $\bchi^\pi$ respectively.

\vspace{2mm}
\noindent
{\bf Observed and potential outcomes.}
%
Let $Y^{(\pi)}_n \in \mathbb{R}$ denote the {\em potential outcome} for unit $n$ under combination $\pi$ and $Y_{n\pi} \in \{\mathbb{R} \cup \star \}$ as the {\em observed outcome}, where $\star$ indicates a missing value, i.e., the outcome associated with the unit-combination pair $(n, \pi)$ was not observed. 
%
Let $\bY = [Y_{n\pi}] \in \{\mathbb{R} \cup \star \}^{N \times 2^p}.$
%
Let $\mathcal{D} \subset [N] \times [2^p],$ refer to the subset of observed unit-combination pairs, i.e., 
%
\begin{equation}\label{eq:SUTVA}
Y_{n\pi} = 
%
\begin{cases}
%
Y^{(\pi)}_n, & \text{if $(n, \pi) \in \mathcal{D}$}\\
%
\star, & \text{otherwise}.
%
\end{cases}
%
\end{equation}
%
Note that \eqref{eq:SUTVA} implies stable unit treatment value assignment (SUTVA)  holds.
%
Let $\Pi_S \subseteq \Pi$ denote a subset of combinations. 
%
For a given unit $n$, let $\bY_{\Pi_S,n} = [Y_{n\pi_i} : \pi_i \in \Pi_S] \in \{\mathbb{R} \cup \star \}^{|\Pi_S|}$ represent the vector of observed outcomes for all $\pi \in \Pi_S$. %{\color{red} Why is this in $\mathbb{R}$, can't things be missing?}
%
Similarly, let $\bY_n^{(\Pi_S)} = [Y^{(\pi_i)}_{n} : \pi_i \in \Pi_S] \in \mathbb{R}^{|\Pi_S|}$ represent the vector of potential outcomes. Denote $\bchi(\Pi_S) = [\bchi^{\pi_i} : \pi_i \in \Pi_S] \in \{-1, 1\}^{|\Pi^S| \times 2^p }$.


\subsection{Model and Target Causal Parameter}\label{sec:causal_model}
%
Define $Y_n^{(\cdot)} : \pi \to \mathbb{R}$ as a real-valued function over the hypercube $\{-1,1\}^p$ associated with unit $n$.
%
It takes as input a combination $\pi$, converts it to a $p$-dimensional binary vector $\bv(\pi)$, and outputs the real-valued potential outcome $Y_n^{(\pi)}$.
%
Given the discussion in Section \ref{sec:notation} on the Fourier expansion of Boolean functions, it follows that $Y_n^{(\pi)}$ always has the representation $\langle \balpha_n, \bchi^\pi \rangle$ for some $\balpha_n \in \mathbb{R}^{2^p}$. 
%
Thus, without any loss of generality, the $\balpha_n$ are unit-specific latent variables, i.e., the Fourier coefficients, encoding the treatment response function. 
%
Below, we state our key assumption on these induced Fourier coefficients.

\begin{assumption} [Potential Outcome Model]
\label{ass:observation_model} 
%
For any unit-combination pair $(n,\pi)$,  assume the potential outcome has the following representation,
%
\begin{equation}\label{eq:observed_outcome}
%
Y^{(\pi)}_{n} = \langle \balpha_{n}, \bchi^\pi \rangle + \epsilon_n^{\pi},
%
\end{equation}
%
where $\balpha_n \in \mathbb{R}^{2^p}$ and $ \bchi^\pi \in \{-1, 1\}^{2^p}$ are the Fourier coefficients and characters, respectively.
%
We assume the following properties: (a) low-rank: the matrix $\mathcal{A} = [\balpha_{n}]_{n \in [N]}$ has rank $r \in [\min\{N, 2^p\}]$; (b) sparsity: $\balpha_{n}$ is $s$-sparse (i.e. $\lVert \balpha_n \rVert_0 \leq s$, where $s \in [2^p]$) for every unit $n \in [N]$; (c) $\epsilon^{\pi}_n$ is a residual term specific to $(n,\pi)$ which satisfies $\E[\epsilon_n^{\pi} ~ | ~ \mathcal{A}] = 0$. 

\end{assumption}
%
The assumption is then that each $\balpha_n$ is $s$-sparse and $\mathcal{A}$ is rank-$r$; $\epsilon_n^{\pi}$ is the residual from this sparse and low-rank approximation, and it serves as the source of uncertainty in our model. 

%
Given $\E[\epsilon_n^{\pi} ~ | ~ \mathcal{A}] = 0$ and that $\mathcal{A}$ is rank-$r$, it implies the matrix $\E[\bY_N^{(\Pi)}] = [\E[\bY_n^{(\Pi)}]: n \in [N]] \in \mathbb{R}^{2^p \times N}$ is also rank $r$, where the expectation is defined with respect to $\epsilon_n^{\pi}$.
%
This is because $\E[\bY_N^{(\Pi)}]$ can be written as $\E[\bY_N^{(\Pi)}] = \bchi(\Pi) \mathcal{A}^T$, and since $\bchi(\Pi)$ is an invertible matrix, $\text{rank}(\E[\bY_N^{(\Pi)}]) = \text{rank}(\mathcal{A})$.
%{\color{red} Briefly add why this is the case.}
%
The low-rank property places \emph{structure across units}; that is, there is sufficient similarity across units so that $\E[\bY_n^{(\Pi)}]$ for any unit $n$ can be written as a linear combination of $r$ other rows of $\E[\bY_N^{(\Pi)}]$. 
%
This is a standard assumption used to encode latent similarity across units in matrix completion and its related applications (e.g., recommendation engines).

%
Sparsity establishes \emph{unit-specific} structure; that is, the potential outcomes for a given user only depend on a small subset of the Fourier characters $\{\chi_S: S \subset [p] \}$. 
%
This subset of Fourier characters can be different across units.
%
As discussed in Section \ref{sec:related_work}, sparsity is commonly employed when studying the learnability of Boolean functions. 
%
In the context of recommendation engines, sparsity is implied if the ratings for a set of goods only depend on a small number of combinations of items within that set---if say only $k < p$ items mattered, then $s \le 2^k$.
%
Sparsity is also often assumed implicitly in factorial design experiments, where analysts typically only include pairwise interaction effects between interventions and ignore higher-order interactions \citep{george2005statistics}---here $s \le p^2$.
%
These various models of imposing combinatorial structure in greater detail in Section \ref{sec:combinatorial_inference_applications}. 

%
Next, we present an assumption that formalizes the dependence between the missingness pattern induced by the treatment assignments $\mathcal{D}$ and the potential outcomes $Y^{(\pi)}_n$, i.e., the type of confounding we can handle, and provide an interpretation of the induced data generating process (DGP) for potential outcomes.


\begin{assumption}[Selection on Fourier coefficients]
\label{ass:selection_on_fourier} 
%
For all $n \in [N]$, $\pi \in \Pi$, $Y^{(\pi)}_n \independent \mathcal{D} \mid \mathcal{A}$.
%
\end{assumption}
%
\vspace{-2mm}
\noindent \textbf{Data Generating Process.} 
%
Given Assumptions \ref{ass:observation_model} and \ref{ass:selection_on_fourier}, one sufficient DGP that is line with the assumptions made can be summarized as follows: (i) unit-specific latent Fourier coefficients $\mathcal{A}$ are either deterministic or sampled from an unknown distribution; we will condition on this quantity throughout.
%
(ii) Given $\mathcal{A}$, we sample mean-zero random variables $\epsilon_n^{\pi}$, and generate potential outcomes according to our model $Y_n^{(\pi)} = \langle \balpha_n, \chi^{\pi} \rangle + \epsilon_n^{\pi}$.
%
(iii) $\mathcal{D}$ is allowed to depend on unit-specific latent Fourier coefficients $\mathcal{A}$ (i.e., $\mathcal{D}=f(\mathcal{A})$).
%
We define all expectations w.r.t.~noise, $\epsilon_n^{\pi}$.
%
This DGP introduces unobserved confounding since $Y_n^{(\pi)} \not\independent \mathcal{D}$. 
%
However, this DGP does imply that Assumption \ref{ass:selection_on_fourier} holds, i.e., conditional on the Fourier coefficients $\mathcal{A}$, the potential outcomes are independent of the treatment assignments $\mathcal{D}$.
%
This conditional independence condition can be thought of as ``selection on latent Fourier coefficients”, which is analogous to the widely made assumption of “selection on observables.” The latter requires that potential outcomes are independent of treatment assignments conditional on {\em observed} covariates---we reemphasize that $\mathcal{A}$ is unobserved. 

\vspace{2mm}
\noindent 
\textbf{Target parameter.} 
%
For any unit-combination pair $(n,\pi)$, we aim to estimate $\E[Y^{(\pi)}_{n} \mid \mathcal{A}]$, where the expectation is w.r.t.~$\epsilon_n^{\pi}$, and we condition on the set of Fourier coefficients $\mathcal{A}$.
%
\subsection{Extension to Permutations}
\label{subsec:rankings_summary}
%
We provide a brief summary of how the formalism established above for combinations can be extended to permuations (i.e., rankings), and provide a detailed discussion in Appendix \ref{sec:permutations}.

\noindent \textbf{Binary Representation of Permutations.} 
%
Let $\tau: [p] \rightarrow [p]$ denote a permutation on a set of $p$ items such that $\tau(i)$ denotes the rank of item $i \in [p]$.  
%
There are a total of $p!$ different permutations, and denote the set of permutations by $\mathbb{S}_p$.
%
Every permutation $\tau \in \mathbb{S}_p$ induces a binary representation $\mathbf{v}(\tau) \in \{-1,1\}^{\binom{p}{2}}$, which can be constructed as follows.
%
For an item $i$, define  $\mathbf{v}^{i}(\tau) \in \{-1,1\}^{p-i}$ as follows: $\mathbf{v}_j^{i}(\tau)= \mathbbm{1}\{ \tau(i) > \tau(j)\} - \mathbbm{1}\{ \tau(i) < \tau(j)\}$ for items $1 \leq i  <  j \leq p$.
%
That is, each coordinate $\mathbf{v}_j^{i}(\tau)$ indicates whether items $i$ and $j$ have been swapped for items $j > i$. 
%
Then, $\mathbf{v}(\tau) = [\mathbf{v}^{i}(\tau): i \in [p]] \in \{-1,1\}^{\binom{p}{2}}$.
%
For example, with $p = 4$, the permutation $\tau([1,2,3,4]) = [1,3,4,2]$ has the binary representation $\mathbf{v}(\tau) = (-1,-1,-1,1,1,-1)$.
%

\noindent \textbf{Fourier Expansion of Functions of Permutations.} Since a permutation $\tau$ can be expressed as a binary vector, any function $f: \mathbb{S}_p \rightarrow \mathbb{R}$ can be thought of as a Boolean function. 
%
Then, given the discussion on Fourier expansions of Boolean functions in Section \ref{sec:notation}, any function $f: \mathbb{S}_p \rightarrow \mathbb{R}$ admits the following Fourier decomposition:
%
$f(\tau) = \sum_{S \subset [\binom{p}{2}]} \alpha_S \chi_{S}(\mathbf{v}(\tau)) \coloneqq \langle  \balpha_f, \bchi^{\tau} \rangle$, where $\balpha_f = [\alpha_S]_{S \in [\binom{p}{2}]} \in \mathbb{R}^{\binom{p}{2}}$, and $\bchi^{\tau} = [\chi_S(\mathbf{v}(\tau)]_{S \in [\binom{p}{2}]} \in \{-1,1\}^{\binom{p}{2}}$ for $\tau \in \mathbb{S}_p$. 

\noindent \textbf{Model and DGP.} We propose a similar model to the one discussed for combination, that is, model the potential outcome for a unit-permutation pair $(n,\tau)$ as
$Y_n^{(\tau)} = \langle \balpha_n, \bchi^{\tau} \rangle + \epsilon_n^{\tau}$, where we assume sparsity ($||\balpha||_0 = s \leq 2^{\binom{p}{2}}$), low-rank structure ($\text{rank}(\mathcal{A}) = r \in \{\min\{N,2^{\binom{p}{2}}\}\}$), and $\E[\epsilon_n^{\tau} ~|~ \mathcal{A}] = 0$. 
%
Additionally, one sufficient DGP for permutations is one that is analogous to what is stated above for combinations. 


Expressing potential outcomes over permutations $Y_n^{(\tau)}$ as Boolean functions allows us to easily adapt the proposed estimator, and our theoretical results to rankings. 
%
For simplicity, we focus on combinations for the rest of the paper and provide a detailed discussion of our results for permutations in Appendix \ref{sec:permutations}. 






  
 
