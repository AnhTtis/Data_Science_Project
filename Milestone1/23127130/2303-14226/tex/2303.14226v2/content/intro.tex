\section{Introduction}
%
Modern-day decision-makers, in settings from e-commerce to public policy to medicine, often encounter settings where they have to pick a combination of interventions and ideally would like to do so in a highly personalized manner.
%
Examples include recommending a curated basket of items to customers on a commerce platform, deciding on a combination of therapies for a medical patient, enacting a collection of socio-economic policies for a specific geographic location, feature selection in a machine learning model, doing a conjoint analysis in surveys, etc.
%
Despite the ubiquity of this setting, it comes with significant empirical challenges: with $p$ interventions and $N$ units, a decision maker must evaluate $N \times 2^p$ potential combinations in order to confirm the optimal personalized policy.
%
With large $N$ and even with relatively small $p$ (due to its exponential dependence), it becomes infeasible to run that many experiments.
%
Of course, in observational data there is the additional challenge of potential unobserved confounding.
%
Current methods tackle this problem by following one of two approaches: (i) they impose structure on how combinations of interventions interact, or (ii) they assume latent similarity in potential outcomes across units. 
%
However, as we discuss in detail below, these approaches require a large number of observations to estimate all $N \times 2^p$ potential outcomes because they do not exploit structure across both units and combinations.
%
Hence the question naturally arises: {\em how can one effectively share information across both units and combinations of interventions?}

\vspace{2mm}
\noindent
{\bf Contributions.} 
%
Our contributions may be summarized as follows.

%
\textbf{(1)} For a given unit $n \in [N]$, we represent its potential outcomes over the $2^p$  combinations as a Boolean function from $\{-1, 1\}^p$ to $\mathbb{R}$, expressed in the Fourier basis.
%
To impose structure across combinations, we assume that for a unit $n$, the Fourier coefficients $\balpha_n \in \mathbb{R}^{2^p}$ induced by this basis are sparse, i.e., have at most $s$ non-zero entries.
%
To impose structure across units, we assume that this matrix of Fourier coefficients across units $\mathcal{A} = [\balpha_n]_{n \in [N]} \in \mathbb{R}^{N \times 2^p}$ has rank $r$.
%
This simultaneous sparsity and low-rank assumption is indeed what allows one to share information across both units and combinations.

%
\textbf{(2)} We establish identification for the $N \times 2^p$ potential outcomes of interest, which requires that any confounding is mediated by the (unobserved) matrix of Fourier coefficients $\mathcal{A}$.

%
\textbf{(3)} We design a two-step algorithm ``\method'' and prove it consistently estimates the various causal parameters, despite potential unobserved confounding. 
%
The first step of \method, termed ``horizontal regression'', learns the structure across combinations of interventions via the Lasso. 
%
The second step, termed ``vertical regression'', learns the structure across units via principal component regression (PCR).

\begin{table}[t!]
\scriptsize
    \centering
    \begin{tabular}{|c|c|c|c|}
     \hline
    Learning Algorithm & Exploits combinatorial structure ($\lVert \balpha_n \rVert_0 = s$) & Exploits inter-unit structure $\text{rank}(\mathcal{A}) = r$) & Sample Complexity \\
    \hline 
    Lasso     & \cmark & \xmark & $O(N \times s^2p)$ \\
    \hline 
    Matrix Completion     & \xmark & \cmark & $O(\text{poly}(r) \times (N + 2^p)$ \\
    \hline 
    \method   & \cmark & \cmark & $O(\text{poly}(r) \times (N + s^2p)$ \\
    \hline
    \end{tabular}
    \caption{Comparison of sample complexity of different methods to estimate all $N \times 2^p$ causal parameters. \method~combines the strengths of both the Lasso and matrix completion methods by exploiting structure across both combinations and units to estimate all potential outcomes with significantly fewer observations. }
    \label{tab:sample_complexity_summary}
\end{table}
    
%
\textbf{(4)} Our results imply that \method~is able to consistently estimate unit-specific potential outcomes given a total of $\text{poly}(r) \times \left( N + s^2p\right)$ observations (ignoring logarithmic factors).
%
This improves over previous methods that do not exploit structure across both units and combinations, which have sample complexity scaling as $\min(N \times s^2p, \ \ \text{poly}(r) \times (N + 2^p))$.
%
A summary of the sample complexities required for different methods can be found in Table \ref{tab:sample_complexity_summary}. 
%
We provide an example showing how replacing Lasso with ``classification and regression trees'' (CART) leads to a sample complexity of $\text{poly}(r) \times \left( N + s^2\right)$ under additional regularity assumptions on the Fourier coefficients.
%
A key technical challenge in our theoretical analysis is studying how the error induced in the first step of \method~percolates through to the second step. 
%
To tackle it, we reduce this problem to that of high-dimensional error-in-variables regression with linear model misspecification, and do a novel analysis of this statistical setting.
%
Under additional conditions, we establish asymptotic normality of the estimator.
%
%{\color{red} Anish: Should we add exact dependence on $r$ - I believe it is $r^4$.}

%
\textbf{(5)} We show how \method~can be used to inform experiment design for combinatorial interventions.
%
In particular, using ideas from compressed sensing, we propose and analyze an experimental design mechanism that ensures the key assumptions required for consistency of \method~are satisfied. 

%
\textbf{(6)} To empirically evaluate \method, we apply it to a real-world dataset for user ratings on sets of movies \cite{sharma2019sets}, and find it outperforms both Lasso and matrix completion methods.
%
Further, we show two of our key modeling assumptions---low-rank and sparse Fourier coefficients---hold in this dataset.
%
We perform additional numerical simulations that corroborate our theoretical findings and show the robustness of \method~to unobserved confounding.

%
\textbf{(7)}
%
We discuss how to extend \method~to estimate counterfactual outcomes when the intervention is a permutation over $p$ items, i.e., rankings. 
%
Learning to rank is a widely studied problem and has many applications such as search engines and matching markets.


