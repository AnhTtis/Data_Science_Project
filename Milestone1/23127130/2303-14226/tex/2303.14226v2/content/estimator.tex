\section{\method~Estimator}\label{sec:estimator_descripton}
%
We now describe the \method~estimator, a simple and flexible two-step procedure for estimating our target causal parameter. 
%
A pictorial representation of the estimator is presented in Figure \ref{fig:estimator}.
%

\vspace{2mm}
\noindent \textbf{Step 1: Horizontal Regression.}
%
For notational simplicity, denote the vector of observed responses $\bY_{n,\Pi_n} = [Y_{n\pi} : \pi \in \Pi_n] \in \mathbb{R}^{|\Pi_n|}$ for any unit $n$ as $\bY_{\Pi_n}$.
%
Then, for every unit $u$ in the donor set $\mathcal{I}$, $\E[Y_u^{(\pi)}]$ is estimated via the Lasso, i.e., by solving the following convex program with penalty parameter $\lambda_u$: 
%
\begin{align}\label{eq:Lasso_estimator}
%
\hat{\balpha}_u =  \argmin_{\balpha} \ \frac{1}{|\Pi_u|}\lVert \bY_{\Pi_u} - \bchi(\Pi_u)\balpha \rVert^2_2 + \lambda_u \lVert \balpha \rVert_1
%
\end{align}
%
where recall that $\bchi(\Pi_u) = [\bchi^\pi: \pi \in \Pi_u] \in \mathbb{R}^{|\Pi_u| \times 2^p}$.
%
Then, for any donor unit-combination pair $(u,\pi)$, let $\hat{\E}[Y_u^{(\pi)}] = \langle  \hat{\balpha}_u, \bchi^{\pi} \rangle$ denote the estimate of the potential outcome $\E[Y_u^{(\pi)}]$.
%\begin{equation}
%\label{eq:Lasso_predicted_potential_outcome}
%    \hat{\E}[Y_u^{(\pi)}] = \langle  \hat{\balpha}_u, \bchi^{\pi} \rangle, 
%\end{equation}
 %

\vspace{2mm}
\noindent \textbf{Step 2: Vertical Regression.} 
%
Next, estimate potential outcomes for all units $n \in [N] \setminus \mathcal{I}$.
%
To do so, some additional notation is required. 
%
For $\Pi_S \subseteq \Pi$, define the vector of estimated potential outcomes $\hat{\E}[\bY^{(\Pi_S)}_{u}] = [ \hat{\E}[Y_u^{(\pi)}]: \pi \in \Pi^S] \in \R^{|\Pi_S|}$. 
%
Additionally, let $\hat{\E}[\bY^{(\Pi_S)}_{\mathcal{I}}] = [\hat{\E}[\bY^{(\Pi_S)}_{u}]: u \in \mathcal{I}] \in \mathbb{R}^{|\Pi_S| \times |\mathcal{I}|}$.
%

\vspace{2mm}
\noindent \emph{Step 2(a): Principal Component Regression.} 
%
Perform a singular value decomposition (SVD) of $\hat{\E}[\bY^{(\Pi_n)}_{\mathcal{I}}]$ to get $\hat{\E}[\bY^{(\Pi_n)}_{\mathcal{I}}] = \sum^{\min(|\Pi_n|,|\mathcal{I}|)}_{l = 1} \hat{s_l}\hat{\bmu}_{l}\hat{\bnu}^T_{l}$. 
%
Using a hyper-parameter $\kappa_n \leq \min(|\Pi_n|,|\mathcal{I}|)$
%
\footnote{ 
%
Both $\lambda_u$ and $\kappa_n$ can be chosen in a data-driven manner (e.g., via cross-validation).}, 
%
compute $\hat{\bw}^{n} \in \mathbb{R}^{|\mathcal{I}|}$ as follows:
\begin{equation}
\label{eq:pcr_linear_model_def}
    \hat{\bw}^{n} = \left(\sum^{\kappa_n}_{l = 1} \hat{s_l}^{-1}\hat{\bnu}_{l}\hat{\bmu}^T_{l}\right)\bY_{\Pi_n} 
\end{equation}

\noindent \emph{Step 2(b): Estimation.} Using $\hat{\bw}^{n} = [\hat{w}_u^n : u \in \mathcal{I}]$, we have the following estimate for any intervention $\pi \in \Pi$
\begin{equation}
\label{eq:potential_outcome_estimate_vertical_regression}
     \hat{\E}[Y_n^{(\pi)}] = \sum_{u \in \mathcal{I}} \hat{w}_u^{n} \hat{\E}[Y_u^{(\pi)}]
\end{equation}
%

\begin{figure}[htbp]
    \centering
    \includegraphics[width = \textwidth]{figures/SC_image_final.pdf}   
     \caption{
     A visual description of \method. Figure \ref{fig:estimator}(a) depicts an example of a particular observation pattern with outcome for unit-combination pair $(n,\pi)$ missing. 
     %
     Figure \ref{fig:estimator}(b) demonstrates horizontal regression for donor unit $u$ to estimate all potential outcomes $\E[Y_u^{(\Pi)}]$.
     %
     Figure \ref{fig:estimator}(c) displays repeating the horizontal regression for the entire donor set $\mathcal{I}$. 
     %
     Figure \ref{fig:estimator}(d) visualizes vertical regression the  estimated outcomes from the donor set $\mathcal{I}$ are extrapolated to estimate outcomes for the unit-combination pair $(n,\pi)$.}
    \label{fig:estimator}
\end{figure}

\noindent \textbf{Suitability of Lasso and PCR.} Lasso is appropriate for the horizontal regression step because it adapts to the sparsity of $\balpha_u$. However, it can be replaced with other algorithms that adapt to sparsity (see below for a larger discussion).
%
For vertical regression, PCR is appropriate because  $\mathcal{A}$ is low rank. 
%
As \cite{agarwal2019robustness,agarwal2020principal} show, PCR implicitly regularizes the regression by adapting to the rank of the covariates, i.e., $\E[\bY^{(\Pi_n)}_{\mathcal{I}}]$. 
%
As a result, the out-of-sample error of PCR scales with $r$ rather than the ambient covariate dimension, which is given by $\mathcal{I}$. 
%
\vspace{1mm}

%
\noindent \textbf{Determining Donor Set $\mathcal{I}$.} 
%
\method~requires the existence of a subset of units $\mathcal{I} \subset [N]$ such that one can (i) accurately estimate their potential outcomes under all possible combinations, and (ii) transfer these estimated outcomes to a unit $n \in [N] \setminus \mathcal{I}$.
%
Theoretically, we give sufficient conditions on the observation pattern such that we can perform (i) and (ii) accurately via the Lasso and PCR, respectively. 
%
In practice, the following practical guidance to determine the donor set $\mathcal{I}$ is recommended.
%
For every unit $n \in [N]$, learn a separate Lasso model $\hat{\balpha}_n$ and assess its performance through $k$-fold cross-validation (CV). 
%
Assign units with low CV error (with a pre-determined threshold) into the donor set $\mathcal{I}$, and estimate outcomes $\hat{\E}[Y_u^{(\pi)}]$ for every unit $u \in \mathcal{I}$ and $\pi \in \Pi$. 
%
For non-donor units, PCR performance can also be assessed via k-fold CV. 
%
For units with low PCR error, linear span inclusion (Assumption \ref{ass:donor_set_identification}(b)) and the assumptions required for the generalization for PCR likely hold, and hence we estimate their potential outcomes as in \eqref{eq:potential_outcome_estimate_vertical_regression}. 
%
For units with large PCR error, it is either unlikely that this set of assumptions holds or that $|\Pi_n|$ is not large enough (i.e., additional experiments need to be run for this unit), and hence we do not recommend estimating their counterfactuals. 
%
In our real-world empirics in Section \ref{sec:real_world_case_study}, we choose donor units via thie approach, and find \method~outperforms other methods. 
%

\vspace{1mm}
\noindent \textbf{Horizontal Regression Model Selection.} \method~allows for any ML algorithm (e.g., random forests, neural networks, ensemble methods) to be used in the first step. 
%
We provide an example of this flexibility by showing how the horizontal regression can also be done via CART in Section \ref{subsec:CART_finite_sample}.
%
Theorem \ref{thm:informal_CART_potential_outcome_convergence_rate} shows CART leads to better finite-sample rates when $\E[Y_n^{(\pi)}]$ is a $k$-Junta. 
%
This model-agnostic approach allows the analyst to tailor the horizontal learning procedure to the data at hand and include additional structural information for better performance. 
%
%under stronger regularity conditions on the potential outcomes $\E[Y_n^{(\pi)}]$ 
%(see Corollary \ref{cor:potential_outcome_convergence_rate_CART} and Appendix \ref{sec:CART_horizontal_regression}