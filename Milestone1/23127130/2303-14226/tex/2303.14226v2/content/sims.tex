\section{Simulations}
\label{sec:sims}
In this section, we corroborate our theoretical findings with numerical simulations in both the observational and experiment design setting.
%
\method~is compared to the Lasso (i.e., running a separate horizontal regression  for every unit) and two benchmark matrix completion algorithms: \texttt{SoftImpute} \citep{mazumder2010spectral}, and  \texttt{IterativeSVD} \citep{troyanskaya2001missing} . 
%
We also try running our experiments using the nuclear-norm minimization method introduced in \citep{candes2012exact}, but the method was unable to be run within a reasonable time frame (6 hours).\footnote{Code for \method~can be found at \url{https://github.com/aagarwal1996/synth_combo}.}
%


\subsection{Observational Setting}
\label{subsec:observational_sims}
For this numerical experiment, we simulate an observation pattern commonly found in applications such as recommendation engines, where most users tend to provide ratings for combinations of goods they either particularly liked or disliked, i.e., there is confounding.
%
The precise experimental set-up for this setting is as follows.  


\noindent \textbf{Experimental Set-up.} We consider $N = 100$ units, and vary the number of interventions $p \in \{10,11, \ldots 15\}$. 
%
Further, let $r = 3$, and $s = rp^{3/2}$. 
%
Next, we describe how we generate the potential outcomes and observation. 

%To proceed, we first describe how the potential outcomes are generated, followed by how we generate the observation pattern. 



\noindent \emph{Generating potential outcomes.} For every $i \in [r],$ $\balpha_i$ is generated by sampling $p^{3/2}$ non-zero coefficients at random.  
%
Every non-zero coefficient of $\balpha_i$ is then sampled from a standard normal distribution. 
%
Denote $\mathcal{A}_r = [\balpha_i : i \in [r]] \in \mathbb{R}^{r \times 2^p}$.  
%
Next, let $\mathcal{A}_{N - r}= \mathbf{B}\mathcal{A}_r \in \mathbb{R}^{(N - r) \times 2^p}$, where $\mathbf{B} \in \mathbb{R}^{(N - r) \times r}$ is sampled i.i.d from a standard Dirichlet distribution. 
%
Let $\mathcal{A} = [\mathcal{A}_r, \mathcal{A}_{N-r}]$; by construction,  $\text{rank}(\mathcal{A}) = r$, and $\lVert \balpha_n \rVert_0 \leq  rp^{3/2} = s$ for all $n \in [N]$.
%
Normally distributed noise $\epsilon \sim N(0,\sigma^2)$ is added to the potential outcomes $\E[Y_n^{(\pi)}]$, where $\sigma^2$ is chosen such that the  signal-to-noise ratio is 1.0.



\noindent \emph{Observation pattern.} Denote $p_{n,\pi}$ as the probability that $\E[Y_n^{(\pi)}]$ is observed.
%
Let $p_{n,\pi} = |\E[Y_n^{(\pi)}]|/\sum_{\pi \in \Pi} |\E[Y_n^{(\pi)}]|$; this definition induces the missingness pattern where outcomes with larger absolute values are more likely to be seen.
%
$|\mathcal{I}| = 2r$ donor units are chosen uniformly at random. 
%
For each donor unit $u \in \mathcal{I}$, randomly sample $|\Pi_u| = 2p^{5/2}$ combinations without replacement, where each combination $\pi$ is chosen with probability $p_{u,\pi}$.
%
That is, each donor unit $u$ is assigned $2p^{5/2}$ treatments, where the outcome $Y_u^{(\pi)}$ is observed with probability $p_{u,\pi}$.
%
For each non-donor unit $n$, generate $2r^4$ observation according to the same procedure. That is, $|\Pi_n| = 2r^{4}$ combinations are randomly sampled without replacement, where each combination $\pi$ is chosen with probability $p_{n,\pi}$.



\noindent \emph{Hyper-parameter choices}. The hyper-parameters of the sub-procedures of~\method, i.e., Lasso and PCR, are tuned via $5-$fold CV. 
%
\texttt{SoftImpute} and \texttt{IterativeSVD} require that the rank $r$ of the underlying matrix to be recovered is provided as a hyper-parameter.
%
We provide the true rank $r = 3$ to both algorithms. 


\noindent \textbf{Results.} We measure mean squared error (MSE) averaged over 5 repetitions between the estimated potential outcome matrix and the true potential outcome matrix for each method. 
%
The results are displayed in Figure \ref{fig:sim_results} (a).
%
\method~outperforms other approaches as $p$ grows.  
%
Further, the gap in performance between \method~and the Lasso enforces the utility of using PCR for non-donor units that do not have sufficient measurements.
%

\subsection{Experimental Design Simulations}
\label{subsec:experimental_design_sims}


\noindent \textbf{Experimental Set-up.} Outcomes are generated as in the observational setting. 
%
The observation pattern is generated by the experimental design mechanism described in Section \ref{sec:experimental_design}.
%
%Finally, hyper-parameters of all estimation methods are chosen as described in the experimental set-up for the simulations in the observational setting. 

\noindent \textbf{Results.} We plot the MSE (averaged over 5 repetitions) for the different methods as the number of interventions is increased $p \in \{10,11,\ldots 15\}$ in Figure \ref{fig:sim_results} (b). 
%
\method~significantly outperforms other methods (and itself in the observational setting), which corroborates our theoretical findings that this experimental design utilizes the strengths of the estimator effectively. 

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/observational_design_results_0.9.png}  
        \caption*{(a) Observational setting simulations.}
    \end{minipage}\hfill
    % Remove or comment out this line
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/experimental_design_results_0.9.png} % second figure itself
        \caption*{(b) Experimental design simulations.}
    \end{minipage}
    \caption{We simulate data in an observational setting (a) and under our experimental design mechanism (b). \method~outperforms both  Lasso (green) and matrix completion methods (red and blue) as measured by MSE in both settings.}
    \label{fig:sim_results}
\end{figure}

%SE for different methods as we vary the number of interventions
    
%with $N = 100, r = 3, |\mathcal{I}| = 5, s = rp^{3/2}$. We sample $p^{5/2}$ observations for the donor units, and $r^4$ observations for non-donor units. MSE for different methods as we vary the number of interventions in (a) the observational setting and (b) under our proposed experimental design mechanism. \method~outperforms both Lasso (green) and matrix completion methods (red and blue) in both settings.


%setting and under our experimental design mechanism described in Section \ref{sec:experimental_design}. 
%Specifically, we compare \method~to the following matrix completion algorithms: (i) 
%The expected potential outcomes are simply constructed by computing $\bchi(\Pi) \mathcal{A}^T$.
%In the context of recommendation engines, this can be interpreted as saying that we are only likely to observe the ratings for combinations that users either strongly like or dislike.
%This ensures that the Fourier coefficients of $\mathcal{A}_{N-r}$ lie in the span of the Fourier coefficients in $\mathcal{A}_r$.
%We then define $\mathcal{A} = [\mathcal{A}_r, \mathcal{A}_{N-r}]$. 
%By construction, $\text{rank}(\mathcal{A}) = r$, and $\lVert \balpha_n \rVert_0 \leq  rp^{3/2} = s$ for every unit. 
%
%Finally, we generate noisy potential outcomes by adding normally distributed noise $\epsilon \sim N(0,\sigma^2)$ to the potential outcome $\E[Y_n^{(\pi)}]$ for every unit $n$ and combination $\pi$. 
%
%We choose $\sigma^2$ such that the percentage of variance explained (PVE), defined by the formula $\text{PVE} = \text{Var}(\E[Y_N^{(\Pi)}])/(\sigma^2 + \text{Var}(\E[Y_N^{(\Pi)}]))$ is equal to $0.9$, and where $\text{Var} (\cdot)$ denotes the variance. 
%
%PVE is proportional to signal to noise ratio, and can be thought of as the fraction of variance that is explained by the underlying potential outcome matrix. 

%Next, we discuss how we generate the observation pattern. 
%
%Let $p_{n,\pi}$ denote the probability that the expected potential outcome for unit $n$ and combination $\pi$ is revealed. 
%
%In this simulation, we define $p_{n,\pi} = |\E[Y_n^{(\pi)}]|/\sum_{\pi \in \Pi} |\E[Y_n^{(\pi)}]|$.
%
%Our definition of $p_{n,\pi}$ induces the missingness pattern where we are more likely to see outcomes with larger absolute values. 
%


%Next, we choose $|\mathcal{I}| = 2r$ donor units. 
%
%For a donor unit $u$, we generate $2p^{5/2}$ observations where a combination $\pi$ is picked with probability $p_{u,\pi}$.
%
%We do the same for non-donor units but generate only $2r^4$ observations.
%
%For the Lasso, we tune the regularization parameter $\lambda$ via $5$-fold cross-validation (CV). 
%
%For PCR (i.e., step 2 of \method), we tune $\kappa$, the number of singular values retained, via 5-fold CV as well. 
%
%We also note that the sample sizes chosen here for the donor set are less than that required by our theoretical results. Nonetheless, \method~performs well, suggesting that our theoretical results may not be entirely tight.  

%We begin by describing the simulation set-up for the observation pattern induced by our experimental design mechanism.

%for the observation pattern induced by our design mechanism. 
%The performance of \method~corroborates our theoretical findings that this experimental design mechanism is able to utilize the strengths of \method~effectively. 
%
