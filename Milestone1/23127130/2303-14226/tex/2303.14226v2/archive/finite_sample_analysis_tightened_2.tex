\section{Theoretical Analysis}\label{sec:theoretical_analysis}
%
In this section, we establish finite-sample consistency of \method, starting with a discussion of the additional assumptions required for our results. 

\subsection{Additional Assumptions}\label{subsec:finite_sample_analysis_assumptions}
%
\begin{assumption}[Bounded Potential Outcomes]\label{ass:boundedness_potential_outcome}
We assume that $\E[Y_{n}^{(\pi)}] \in [-1,1]$ for any unit-combination pair $(n,\pi)$.
\end{assumption}
%
\begin{assumption}[Sub-Gaussian Noise]\label{ass:subgaussian_noise}
Conditioned on $\mathcal{A}$, for any unit-combination pair $(n,\pi)$, $\epsilon_n^{\pi}$ are independent zero-mean sub-Gaussian random variables with $\text{Var}[\epsilon_n^{\pi} ~ | ~  \mathcal{A}] \le \sigma^2$ and $\lVert \epsilon_n^{\pi} ~ | ~ \mathcal{A} \rVert_{\psi_2} \leq C\sigma$ for some constant $C > 0$. 
\end{assumption}
%
\begin{assumption} [Incoherence of Donor Fourier characteristics]
\label{ass:incoherence}
For every unit $u \in \mathcal{I}$, assume $\bchi(\Pi_u)$ satisfies incoherence:
$
\left\lVert \frac{\bchi(\Pi_u)^T\bchi(\Pi_u)}{|\Pi_u|} - \mathbf{I}_{2^p}\right\rVert_{\infty} \leq \frac{C'}{s}, 
$ for a universal constant $C' >0$. 
\end{assumption}
%
To define our next set of assumptions, we introduce necessary notation. For any subset of combinations $\Pi_S \subset \Pi$, let $\E[\bY_{\mathcal{I}}^{(\Pi_S)}] = [\E[\bY_u^{(\Pi_S)}]: u \in \mathcal{I}] \in \mathbb{R}^{|\Pi_S| \times |\mathcal{I}|}$.
%
\begin{assumption} [Donor Unit Balanced Spectrum]\label{ass:balanced_spectrum} For a given unit $n \in [N] \setminus \mathcal{I}$, let $r_n$ and $s_{1} \ldots s_{r_n}$ denote the rank and non-zero singular values of $ \E[\bY_{\mathcal{I}}^{(\Pi_n)} \ | \ \mathcal{A}]$, respectively. 
%
We assume that the singular values are well-balanced, i.e., for universal constants $c,c' > 0$, we have that $s_{r_n}/s_{1} \geq c$, and $\lVert \E[\bY^{(\Pi_n)}_{\mathcal{I}} ~ | ~ \mathcal{A} ]\rVert^2_F \geq c'|\Pi_n||\mathcal{I}|$.
\end{assumption}

\begin{assumption} [Subspace Inclusion]\label{ass:rowspace_inclusion} 
For a given unit $n \in [N] \setminus \mathcal{I}$ and intervention $\pi \in \Pi \setminus \Pi_n$,
assume that $\E[\bY^{(\pi)}_{\mathcal{I}}]$ lies within the row-span of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$
\end{assumption}
%
\vspace{-2mm}
Assumption \ref{ass:incoherence} is necessary for finite-sample consistency when estimating $\balpha_n$ via the Lasso estimator \eqref{eq:Lasso_estimator}, and is commonly made when studying the Lasso \citep{rigollet2015high}. 
%
Incoherence can also seen as a inclusion criteria for a unit $n$ to be included in the donor set $\mathcal{I}$.  
%
Lemma 2 in \cite{negahban2012learning} shows that the Assumption \ref{ass:incoherence} is satisfied with high probability if $\Pi_u$ is chosen uniformly at random and grows as $\omega(s^2p)$. 
%
If Assumption \ref{ass:incoherence} does not hold, then the Lasso may not estimate $\balpha_u$ accurately, and alternative horizontal regression algorithms may be required instead. 
%
Assumption \ref{ass:balanced_spectrum} requires that the non-zero singular values of $\E[\bY_{\mathcal{I}}^{(\Pi_n)} ~ | ~ \mathcal{A}]$ are well-balanced. 
%
This assumption is standard when studying PCR \cite{agarwal2019robustness,agarwal2020synthetic}, and within the econometrics literature \cite{bai2021matrix,fan2018eigenvector}. 
%
It can also be empirically validated by plotting the spectrum of $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$; if the singular spectrum of $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$ displays a natural elbow point, then Assumption \ref{ass:balanced_spectrum} is likely to hold. 
%
Assumption \ref{ass:rowspace_inclusion} is also commonly made when analyzing PCR \cite{agarwal2020principal,agarwal2020synthetic,agarwal2021causal}.
%
It can be thought of as a ``causal transportability'' condition from the model learnt using $\Pi_n$ to the interventions $\pi \in \Pi \setminus \Pi_n$. 
%
That is, subspace inclusion allows us to generalize well, and accurately estimate $\E[Y_n^{(\pi)} ~ | ~ \mathcal{A} ]$ using $\langle \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}], \hat{\bw}^n \rangle$. 
%
In Section \ref{sec:experimental_design}, we propose a simple experimental design mechanism that ensures that Assumptions \ref{ass:incoherence}, \ref{ass:balanced_spectrum}, \ref{ass:rowspace_inclusion} (and Assumption  \ref{ass:donor_set_identification}), i.e., the key conditions for \method~to work, hold with high probability. 
%As we show in Section \ref{sec:experimental_design}, subspace inclusion holds with high probability if $|\Pi_n|$ is chosen uniformly and independently at random from $\Pi$ and has size $\omega(r\log(|\mathcal{I}|))$.
\vspace{-2mm}

\subsection{Finite Sample Consistency}\label{subsec:finite_sample_consistency}
\vspace{-2mm}
%
The following result establishes finite-sample consistency of \method. 
%
Without loss of generality, we will focus on estimating the pair of quantities $(\E[Y_u^{(\pi)}],\E[Y_n^{(\pi)}])$ for a given donor unit $u \in \mathcal{I}$, and non-donor unit $n \in [N] \setminus \mathcal{I}$ under treatment assignment $\pi \in \Pi$.
%
To simplify notation, we will use $O_p$ notation:
%
for any sequence of random vectors $X_n$, $X_n$ = $O_p(\gamma_n)$ if, for any $\epsilon > 0$, there exists constants $c_\epsilon$ and $n_\epsilon$ such that $\mathbb{P}(\lVert X_n \rVert_2 \geq c_\epsilon\gamma_n) \leq \epsilon$ for every $n \geq n_\epsilon$. 
%
Additionally, we absorb dependencies on $\sigma$ into $O_p$.
%
For simplicity, we also define $\tilde{O_p}(\gamma_n)$ which suppresses  logarithmic terms. 
%

\begin{theorem} [Finite Sample Consistency of \method]
\label{thm:potential_outcome_convergence_rate}
Conditioned on $\mathcal{A}$, let Assumptions \ref{ass:observation_model}, \ref{ass:selection_on_fourier}, \ref{ass:donor_set_identification}, \ref{ass:boundedness_potential_outcome}, \ref{ass:subgaussian_noise}, and \ref{ass:incoherence} hold. Then, the following statements hold. 
\begin{itemize}
    \item [(a)] For any donor unit-combination pair $(u,\pi)$, let the Lasso regularization parameter satisfy $\lambda_u = \Omega(\sqrt{\frac{p}{|\Pi_u|}})$, then we have that
        $$
            |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]| = O_p\left(s\sqrt{\frac{p}{|\Pi_u|}}\right).
        $$
    \item [(b)] Additionally, let Assumptions \ref{ass:balanced_spectrum}, and \ref{ass:rowspace_inclusion} hold. Then, for any non-donor unit-combination pair $(n,\pi)$ where $n \in [N] \setminus \mathcal{I}$, if $\kappa_n = \text{rank}(\E[\bY_{\mathcal{I}}^{(\Pi_n)}]) \coloneqq r_{n}$, and $ \min_{u \in \mathcal{I}} |\Pi_u| \coloneqq M = \omega(s^2p)$, we have that,
        $$
            \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right|  = \Tilde{O_p}\left(r^{3/2}_n\left[\frac{1}{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|}\}} + s\sqrt{\frac{{p}}{{M}}}  \right]\right).
        $$
\end{itemize}

\end{theorem}
\vspace{-2.5mm}
%
Establishing Theorem \ref{thm:potential_outcome_convergence_rate} requires a novel analysis of error-in-variables (EIV) linear regression.
%
Specifically, the general EIV linear model is as follows: $Y = \bX \beta + \epsilon$, $\mathbf{Z} = \bX + \mathbf{H}$, where $Y$ and $\mathbf{Z}$ are observed.
%
In our case, $Y = \bY_{\Pi_n}$, $\bX = \E[\bY_{\mathcal{I}}^{(\Pi_n)}]$, $\beta = \bw^n$ , $\bZ = \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$, and $\mathbf{H}$ is the error arising in estimating $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$ via the Lasso.
%
Typically one assumes that $\mathbf{H}$ is is a matrix of independent sub-gaussian noise. 
%
Our analysis requires a novel worst-case analysis of $\mathbf{H}$ (due to the 2-step regression of Lasso and then PCR), in which each entry of is  $\mathbf{H}$ simply bounded.

%Previous works assume that $\bH$ is is a matrix of independent sub-gaussian noise. Our analysis requires a worst-case analysis of H (due to the 2-step regression of Lasso and then PCR), which each entry of is H simply bounded. } 
%$\bZ = \bX + \bH$, where $Y$ and $\bZ$ are observed. }


%in our setting, the covariate matrix used in PCR (i.e., $\hat{\E}[Y_{\mathcal{I}}^{(\Pi_n)}]$) is corrupted by bounded noise that arises from estimating potential outcomes via the Lasso for the donor set. This differs from previous work in which the covariate matrix is corrupted by independent sub-gaussian noise. }

Next, we describe the conditions required on $|\Pi_n|$,$|\mathcal{I}|$, $M$, for \method~to consistently estimate $(\E[Y_u^{(\pi)}], \ \E[Y_n^{(\pi)}])$. 
%
Specifically, if $\min\{|\Pi_n|,|\mathcal{I}|\} = \omega(r^3_n)$ and $M = \omega(r^3_ns^2p)$, then $\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \\ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1)$.
%
Next, we present a corollary that discusses how quickly the parameters $r_n$, $s$, and $p$ can grow with the number of observations $|\Pi_n|$, $\mathcal{I}$, and $M$.


\begin{corollary}\label{cor:potential_outcome_convergence_rate_simplified}
%
With the set-up of Theorem \ref{thm:potential_outcome_convergence_rate}, if the following conditions hold: $r_n = o(\min\{|\Pi_n|,|\mathcal{I}|\}^{1/3})$, and $s = o\left(\sqrt{\frac{M}{pr^3_n}} \right)$, then 
$
\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \\ \ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1) 
$ as $|\Pi_n|, |\mathcal{I}|, $M$ \to \infty$. 
%
\end{corollary}

\vspace{-1.5mm}
%
\noindent Corollary \ref{cor:potential_outcome_convergence_rate_simplified} quantifies how $s, r_n$ can scale with the number of observations to achieve consistency. 
%
That is, Corollary \ref{cor:potential_outcome_convergence_rate_simplified} reflects the maximum ``complexity'' allowed for a given sample size. 

\subsection{Finite-Sample Consistency of CART}
\label{subsec:CART_finite_sample}

\noindent As discussed in Section \ref{sec:estimator_descripton}, \method~allows for any ML algorithm to be used for horizontal regression. 
%
In this section, we present theoretical results when the horizontal regression is done via CART. 
%
We show that when the potential outcome function is a $k$-Junta (see equation \eqref{eq:k_junta} for the definition), CART is able to achieve stronger sample complexity guarantees than the Lasso. 
%
We refer the reader to Appendix \ref{sec:CART_horizontal_regression} for a description of the CART algorithm.
%
Our CART-based horizontal regression procedure consists of the following two steps. 


\noindent \textbf{Step 1: Feature Identification.} For each donor unit $u \in \mathcal{I}$, divide the observed interventions $\Pi_u$ equally into two sets $\Pi^{a}_u$ and $\Pi^{b}_u$. Fit a CART model $\mathcal{\hat{T}}$ on the dataset $\mathcal{D}^a_u = [\{\bv(\pi),Y_{u\pi}\}: \pi \in \Pi^a_u]$. Let $\hat{K}_u = \{\bv(\pi)_j : \bv(\pi)_j$ $\in \mathcal{\hat{T}}\}$ denote every feature that the CART splits on. 

\noindent \textbf{Step 2: Estimation.} For every subset $S \subset \hat{K}_u$, compute $\hat{\alpha}_{u,S} = \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi^b_u }Y_{u\pi}\chi^{\pi}_{S}$. We then estimate our target causal parameter as follows: $\hat{\E}[Y_u^{(\pi)}] = \sum_{S \in \hat{K}_u}\hat{\alpha}_{u,S}\chi^{\pi}_{S}$.

\noindent CART is able to take advantage of the $k$-Junta structure of the potential outcomes by first performing feature selection (i.e., learning the relevant feature set $\hat{K}_u$), before estimating the Fourier coefficient for each relevant subset.
%
In contrast, Lasso estimates the Fourier coefficient of all $2^p$ subsets simultaneously. 
%

Next, we present our theoretical results that establishes CART exploits this $k$-Junta structure to provide better sample complexity guarantees than the Lasso.
%
Specifically, we present an informal theorem for the horizontal regression finite sample error when the horizontal regression is done via CART.  
%
The formal result (Theorem \ref{thm:CART_convergence_rate}) can be found in Appendix \ref{supp:CART_finite_sample}. 

\begin{theorem} [Informal]
\label{thm:informal_CART_potential_outcome_convergence_rate}
For a given donor unit $u$, suppose $\E[Y_u^{(\pi)}]$ is a $k$-Junta, then, if the horizontal regression is done via CART, we have
        $
            |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]| = \Tilde{O}_p\left(\frac{s}{\sqrt{|\Pi_u|}}\right).
       $
\end{theorem}
%
Compared to the Lasso horizontal regression error (see Theorem \ref{thm:potential_outcome_convergence_rate} (a)), CART removes an additional factor of $p$ by exploiting the $k$-Junta structure. 
%
As a result, it can be verified that CART also removes a factor of $p$ for the vertical regression error (see Theorem \ref{thm:potential_outcome_convergence_rate} (b)). 
%
Next, we describe the conditions required for $\Pi_n$, $M$ to consistently estimate $(\E[Y_u^{(\pi)}], \ \E[Y_n^{(\pi)}])$ for a given donor unit $u$, non-donor unit $n$, and combination $\pi$ with \method~when the horizontal regression is done via CART.
%
Given this assumption, it can be verified that if $\min\{|\Pi_n|,|\mathcal{I}|\} = \omega(r^3_n)$, and $M = \omega(r^3_ns^2)$, then $\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1)$.
%
That is, CART reduces the number of required observations for donor units by a factor of $p$.



%\begin{corollary}
%\label{cor:potential_outcome_convergence_rate_CART}
%
%Let the set-up of Theorems \ref{thm:potential_outcome_convergence_rate}, \ref{thm:CART_convergence_rate}, and condition (a) of Corollary \ref{cor:potential_outcome_convergence_rate_simplified} hold. 
%
%Then, if $s =  o\left(\sqrt{M/(\max\{|\Pi_n|,|\mathcal{I}|\}})\right)$, 
%$\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1)$
%
%\end{corollary} 
%\vspace{-2mm}
%
%\noindent Corollary \ref{cor:potential_outcome_convergence_rate_CART} 
%shows CART can achieve consistency while allowing the sparsity $s$ to scale more quickly (i.e., by a factor of $p$) than the Lasso. 
%
%That is, CART exploits the additional regularity conditions placed on the potential outcomes to achieve consistency with $M = \omega(r_n^4s^2)$ samples. 
%
%As discussed above, the superior sample complexity of CART arises from the fact that it identifies the relevant feature subset before estimating the Fourier coefficient.
%

%shows CART exploits the additional regularity conditions placed on the potential outcomes to allow the sparsity $s$ to scale more quickly (i.e., by a factor of $p$) while maintaining consistency. 



\subsection{Sample Complexity}
\label{subsec:sample_complexity_synth_combo}
\vspace{-2.5mm}
We discuss the sample complexity of \method~to estimate all $N \times 2^p$ causal parameters, and compare it to that of other methods.
%
To ease our discussion, we will ignore dependence on logarithmic factors, and assume that $|\Pi_n| = |\mathcal{I}|$.
%
Further, we remind the reader that we denote $\min_{u \in \mathcal{I}} \coloneqq M$. 
%
We also assume that the horizontal regression is done via the Lasso. 

%
Even if potential outcomes $Y_n^{(\pi)}$ were observed for all unit-combination pairs, consistently estimating $\E[\bY_N^{(\Pi)}]$ is not trivial. 
%
This is because, we only get to observe a single and noisy version $Y_{n\pi} = \langle \balpha_n, \bchi^{\pi} \rangle + \epsilon_n^{\pi}$. 
%
Hypothetically, if we observe $K$ independent samples of $Y_{n\pi}$ for a given $(n,\pi)$, denoted by $Y^{1}_{n\pi}, \ldots, Y^{K}_{n\pi}$, the maximum likelihood estimator would be the empirical average $\frac{1}{K}\sum^K_{i=1}Y^{i}_{n\pi}$. 
%
The empirical average would concentrate around $\E[Y_n^{(\pi)}]$ at a rate $O(1/\sqrt{K})$ and hence would require $K = \Omega(\delta^{-2})$ samples to estimate $\E[Y_n^{(\pi)}]$ within error $O(\delta)$. 
%
Therefore, this naive (unimplementable) solution would require $N \times 2^p \times \delta^{-2}$ observations to estimate $\E[\bY_N^{(\Pi)}]$. 
%

On the other hand, \method~produces consistent estimates of the potential outcome despite being given {\em at most only a single noisy sample} of each potential outcome. 
%
As our discussion after Theorem \ref{thm:potential_outcome_convergence_rate} shows, 
$|\mathcal{I}| \times r^3s^2p/\delta^2$ observations for the donor set, and $(N - |\mathcal{I}|) \times r^3/\delta^2$ observations for the non-donor units to achieve an estimation error of $O(\delta)$ for all $N \times 2^p$ causal parameters. 
%
Assuming $|\Pi_n| = |\mathcal{I}|$, this implies that the number of observations required to achieve an estimation error of $O(\delta)$ for all pairs $(n,\pi)$ via \method~scales as $O\left(\text{poly}(r/\delta) \times \left(N + s^2p \right) \right)$.


\vspace{2mm}
\noindent \textbf{Sample Complexity Comparison to Other Methods.}  

\noindent \emph{Horizontal regression:} An alternative algorithm would be to learn an individual model for each unit $n \in [N]$. 
%
That is, run a separate horizontal regression via the Lasso for every unit.
%
This alternative algorithm has sample complexity that scales at least as $O(N \times s^2p/\delta^2)$  rather than $O\left(\text{poly}(r)/\delta^4  \times \left(N + s^2p \right) \right)$ required by \method.  
%
It suffers because it does not utilize any structure across units (i.e., the low-rank property of $\mathcal{A}$), whereas \method~captures the similarity between units via PCR. 
%

\vspace{1mm}
\noindent \emph{Matrix completion:} \method~can be thought of as a matrix completion method; estimating $\E[Y_n^{(\pi)}]$ is equivalent to imputing $(n,\pi)$-th entry of the observation matrix $\bY \in \{\mathbb{R} \cup \star\}^{N \times 2^p}$, where recall $\star$ denotes an unobserved unit-combination outcome. 
%
Under the low-rank property (Assumption \ref{ass:observation_model}(b)) and various models of missingness (i.e., observation patterns), recent works on matrix completion \cite{candes2010matrix,ma2019missing,agarwal2021causalmatrix} (see Section \ref{sec:related_work} for an overview) have established that estimating $\E[Y_n^{(\pi)}]$ to an accuracy $O(\delta)$ requires at least $O\left(\text{poly}(r/\delta) \times \left(N + 2^p \right)\right)$ samples.
%
This is because matrix completion techniques do not leverage the sparsity of $\balpha_n$.
%leverage additional structural information such as the Fourier expansion of Boolean functions and the resulting sparsity in $\balpha_n$.
%
Moreover, matrix completion results typically report error in the Frobenius norm, whereas we give entry-wise guarantees. 
%
This leads to an extra factor of $s$ in our analysis as it requires proving convergence for $\lVert \hat{\balpha_n} - \balpha_n \rVert_{1}$ rather than $\lVert \hat{\balpha_n} - \balpha_n \rVert_{2}$. 
%
% Hence, \method~combines the best of both approaches by leveraging both the structure of the potential outcomes \emph{and} the similarity across units. 

\vspace{2mm}
\noindent \textbf{Natural Lower Bound on Sample Complexity.} We provide an informal discussion on the lower bound sample-complexity to estimate all $N \times 2^p$ potential outcomes. 
%
As established in Lemma \ref{lem:number_nonzeros}, $\mathcal{A}$ has at most $rs$ non-zero columns. 
%
Counting the parameters in the singular value decomposition of $\mathcal{A}$, only $r\times (N + rs)$ free parameters are required to be estimated. 
%
Hence, a natural lower bound on the sample complexity scales as $O(Nr + r^2s)$.  
%
Hence, \method~is only sub-optimal by a factor (ignoring logarithmic factors) of $sp$ and $\text{poly}(r)$.  
%
As discussed earlier, an additional factor of $s$ can be removed if we focus on deriving Frobenius norm error bounds.
%
Further, the use of non-linear methods such as CART can remove the factor of $p$ (Theorem \ref{thm:informal_CART_potential_outcome_convergence_rate}) under stronger assumptions on the potential outcomes such as when $\E[Y_n^{(\pi)}]$ is a $k$-Junta that satisfies regularity conditions. 
%
It remains interesting future work to derive estimation procedures that are able to achieve this lower bound.


%
%To simplify the discussion, we assume that $\min\{|\Pi_n|,|\mathcal{I}|\} = \mathcal{|I|}$. 
%
%This condition can be enforced in practice by simply picking a subset of donor units such that $|\mathcal{I}| \leq |\Pi_n|$ when performing PCR (i.e., step 2 of \method).


%Next, we present a corollary analogous to Corollary \ref{cor:potential_outcome_convergence_rate_simplified} that discusses how quickly 
%that assumes the horizontal regression is done via CART instead. 



%present the additional assumptions required on the potential outcomes for finite-sample consistency of \method~when using CART instead of Lasso. 
%
%We only present an informal discussion of these assumptions, and leave the precise formulation to Appendix \textcolor{red}{H}.  

% how quickly the parameters $r_n,s,p$ are allowed to grow with the number of observations $|\Pi_n|$, and $M$ for \method~with CART to achieve consistency. 

% o\left(\sqrt{M/(p\max\{|\Pi_n|,|\mathcal{I}|\}})\right) $




%for estimating our target causal parameter via \method.  
%
%{\color{red} Specifically, we establish a corollary analogous to Corollary \ref{cor:potential_outcome_convergence_rate_simplified} that quantifies the maximum complexity (as measured by $r_n,s,p$) allowed by CART for a given sample size. 
%
%Before presenting the corollary, we provide an informal discussion of the assumptions imposed on the potential outcomes $\E[Y_n^{(\pi)}]$ required for CART to achieve finite-sample consistency. 
%
%We assume: (i) $\E[Y_n^{(\pi)}]$ is a $k$-Junta as discussed above, (ii) $\E[Y_n^{(\pi)}]$ satisfies a ``sub-modularity'' condition which is necessary for greedy-tree algorithms such as CART to achieve consistency (see Definition \ref{def:strong_partition_sparsity} for an exact characterization of this sub-modularity condition), and (iii) the combinations of donor units are observed uniformly at random. 
%
%Given this discussion on the required assumptions, we present our theoretical result as follows.}

%To satisfy linear span inclusion (Assumption \ref{ass:donor_set_identification} (b)), and the necessary conditions for PCR (i.e., Assumption \ref{ass:rowspace_inclusion}), we require that donor set has size $|\mathcal{I}| = \omega(r)$. 
%Indeed, we verify in our experimental design mechanism in Section \ref{sec:experimental_design} that uniformly choosing a donor set of size $|\mathcal{I}| = O(r)$ satisfies these assumptions.

%Hence, at most $N \times r^4/\delta^4$ + $|\mathcal{I}| \times r^4s^2p/\delta^2$ observations are required to estimate $\E[Y_n^{(\pi)}]$ with error $O(\delta)$ for all pairs $(n,\pi)$. 
%
%Of course, this is assuming that the key assumptions (see Section \ref{subsec:finite_sample_analysis_assumptions}) are satisfied. 
%
%Hence, we have that the number of observations required to achieve an estimation error of $O(\delta)$ scales as $O\left(\text{poly}(r)/\delta^4 \times \left(N + s^2p \right) \right)$. 
%