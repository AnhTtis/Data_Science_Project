\allowdisplaybreaks

\section{Asymptotic Normality}
\label{sec:asymptotic_normality_supp}

In this section, we provide proofs for Proposition \ref{prop:horizontal_asymptotic_normality} and Theorem \ref{thm:vertical_regression_normality}.


%we first discuss the required conditions for the SR estimator introduced in Section \ref{sec:asymptotic_normality} to achieve support recovery (condition (e) in Proposition %\ref{prop:horizontal_asymptotic_normality}), i.e., for $\P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) = O(e^{-|\Pi_u|^{c_3}})$.
%%
%Then, we provide proofs for Proposition \ref{prop:horizontal_asymptotic_normality} and Theorem \ref{thm:vertical_regression_normality}.
%


%\subsection{Assumptions for Support Recovery}
%\label{subsec:support_recovery}



%we present a result by \cite{zhao2006model} and the required assumptions to establish condition (e) of Proposition \ref{prop:horizontal_asymptotic_normality}.



 %\item [(d)] $\lambda_{\min}\left(K_u \right) \geq c_{3}$ where $c_{3}$ is a positive constant,
    %\item [(e)] For $\eta > 0$, assume $\max_{S \in \mathcal{S}^c_u} \lVert K_u^{-1} \bchi^T_{\mathcal{S}_u}\bchi_{S}  \rVert_1 \leq \eta|\Pi_u|$,

\subsection{Proof of Proposition \ref{prop:horizontal_asymptotic_normality}}
For ease of exposition, we suppress conditioning on $\mathcal{A}$.
%
Then, from the law of total expectation, this yields, 
\begin{align*}
    &\hat{\E}_{SR}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] 
    %
    \\ &= \left( \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u = \mathcal{S}_u] -  \E[Y_u^{(\pi)}] \right) \P(\hat{\mathcal{S}}_u = \mathcal{S}_u) + \left( \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u \neq \mathcal{S}_u]  - \E[Y_u^{(\pi)}]\right) \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) 
    %
    \\ & =  \left( \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u = \mathcal{S}_u] -  \E[Y_u^{(\pi)}] \right) \P(\hat{\mathcal{S}}_u = \mathcal{S}_u) +  \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u \neq \mathcal{S}_u]  \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u)  - \E[Y_u^{(\pi)}] \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) 
 \end{align*}

\noindent We scale each of three terms in the equation above by $\sqrt{|\Pi_u|}/\sqrt{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}$, and analyze each of them separately. 

\noindent \emph{Term 1.} 
To proceed, we state the following lemma.
\begin{lemma}
\label{lem:fourier_coefficient_asymptotic_normality}
Let the set-up of Proposition \ref{prop:horizontal_asymptotic_normality} hold. Then, as $|\Pi_u| \rightarrow \infty$, we have, 
\begin{equation}
\label{eq:fourier_coefficient_asymptotic_normality}
     \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}}\left( \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u = \mathcal{S}_u ] - \E[Y_u^{(\pi)}] \right)\xrightarrow{d} N(0,1).
\end{equation}
\end{lemma}

   
\noindent Next, from condition (e) of Proposition \ref{prop:horizontal_asymptotic_normality}, as  $|\Pi_u| \rightarrow \infty$, $\P(\hat{\mathcal{S}}_u = \mathcal{S}_u) \rightarrow 1$.
%
Then, using this observation, substituting \eqref{eq:fourier_coefficient_asymptotic_normality} into term 1, and using Slutsky's theorem, gives the following
\begin{equation}
\label{eq:horizontal_regression_term1_normality}
      \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}}\left( \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u = \mathcal{S}_u] -  \E[Y_u^{(\pi)}] \right) \P(\hat{\mathcal{S}}_u = \mathcal{S}_u) \xrightarrow{d} N (0,1),
\end{equation}
as $|\Pi_u| \rightarrow \infty$.
\vspace{1mm}


\noindent \emph{Term 2.} We introduce the following lemma for the analysis of term 2. 
\begin{lemma}
\label{lem:largest_eigenvalue_covariance_matrix}
Let the set-up of Proposition \ref{prop:horizontal_asymptotic_normality} hold. Then, we have 
\begin{equation*}
      \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}} = O\left(\sqrt{\frac{s|\Pi_u|}{2^p}} \right)
\end{equation*}
    
\end{lemma}

\noindent Substituting the result of Lemma \ref{lem:largest_eigenvalue_covariance_matrix}, and conditions (d) and (e) into the expression for term 2 yields,
\begin{equation}
\label{eq:horizontal_regression_term2_intermediate1}
    \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}} \hat{\E}_{SR}[Y_u^{(\pi)} |\hat{\mathcal{S}}_u  \neq \mathcal{S}_u]  \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) = o\left(\sqrt{\frac{|\Pi_u|^{1+c_2}}{2^p} }e^{-|\Pi_u|^{c_3}} \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u  \neq \mathcal{S}_u] \right)
\end{equation}
     
\noindent We continue by stating the following lemma.

\begin{lemma}
\label{lem:ridge_bound}
Let the set-up of Proposition \ref{prop:horizontal_asymptotic_normality} hold. Then, we have that
\begin{equation*}
   \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u  \neq \mathcal{S}_u]= O_p \left(|\Pi_u| \sqrt{2^{p}} \right)
\end{equation*}
\end{lemma}

\noindent Then, substituting the result of Lemma \ref{lem:ridge_bound} into \eqref{eq:horizontal_regression_term2_intermediate1} gives us 
\begin{equation}
\label{eq:horizontal_regression_term2}
    \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}} \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u  \neq \mathcal{S}_u]  \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) = o_p \left(\sqrt{|\Pi_u|^{3+c_2}} e^{-|\Pi_u|^{c_3}} \right) = o_p(1)
\end{equation}

\vspace{1mm}


\noindent \emph{Term 3.} By Lemma \ref{lem:largest_eigenvalue_covariance_matrix}, Assumption \ref{ass:boundedness_potential_outcome}, conditions (d) and (e) of Proposition \ref{prop:horizontal_asymptotic_normality}, we have 
\begin{equation}
\label{eq:horizontal_asymptotic_normalty_term3_bound}
     \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}} \E[Y_u^{(\pi)}]\P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) =  o\left(\sqrt{\frac{|\Pi_u|^{1+c_2}}{2^p}} e^{-|\Pi_u|^{c_3}}\right) = o(1), 
\end{equation}

\vspace{1mm}

\noindent Collecting \eqref{eq:horizontal_regression_term1_normality}, \eqref{eq:horizontal_regression_term2}, and \eqref{eq:horizontal_asymptotic_normalty_term3_bound} gives us the claimed result.

\subsection{Helper Lemmas for Proposition \ref{prop:horizontal_asymptotic_normality}}


\subsubsection{Proof of Lemma \ref{lem:fourier_coefficient_asymptotic_normality}}
\noindent We establish some notation required for the proof. 
%
Define $\balpha_{\mathcal{S}_u} = [\alpha_{u,S}: S \in \mathcal{S}_u] \in \mathbb{R}^{|\mathcal{S}_u|}$, and $\hat{\balpha}^{SR}_{\mathcal{S}_u} = [\hat{\alpha}^{SR}_{u,S}: S \in \mathcal{S}_u] \in \mathbb{R}^{|\mathcal{S}_u|}$.
%
Then, we begin our proof by stating the following lemma, and state the delta theorem. 

\begin{lemma} [Theorem 3 of \cite{liu2013asymptotic}]
\label{lem:fourier_coefficient_asymptotic_normality_liu}
Let the set-up of Proposition \ref{prop:horizontal_asymptotic_normality} hold. Then, as $|\Pi_u| \rightarrow \infty$, we have that 
\begin{equation*}
    \sqrt{|\Pi_u|} \left( \hat{\balpha}^{SR}_{\mathcal{S}_u} - \balpha_{\mathcal{S}_u} \right) \xrightarrow{d} N\left(0,\sigma^2\mathbf{K}^{-1}_u\right)
\end{equation*}
\end{lemma}

\begin{theorem} [Delta Theorem \cite{ding2024linear}] 
\label{thm:delta_theorem}
Let $f(\mathbf{z})$ be a function from $\mathbb{R}^p \rightarrow \mathbb{R}$, and $\frac{\partial f(\bz)}{\partial \bz} \in \mathbb{R}^p$ denote the partial derivative of $f$ with respect to $\bz$.
%
Additionally, suppose $\sqrt{n}(\mathbf{Z}_n - \mathbf{\theta}) \xrightarrow{d} N(0,\Sigma)$ as $n \rightarrow \infty$. 
%
Then, we have
\begin{equation*}
    \sqrt{n}\left(f(\mathbf{Z}_n) - f(\mathbf{\theta}) \right) \xrightarrow{d} N(0,(\frac{\partial f(\bz)}{\partial \bz})^T \Sigma \frac{\partial f(\bz)}{\partial \bz} )
\end{equation*}
as $n \rightarrow \infty$. 
\end{theorem}

\noindent Applying Lemma \ref{lem:fourier_coefficient_asymptotic_normality}, and Theorem \ref{thm:delta_theorem} (with $f(\cdot) = \langle \cdot, \bchi^\pi \rangle$) gives us 
\begin{equation*}
     \sqrt{|\Pi_u|} \left(\hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u =\mathcal{S}_u ]  - \E[Y_u^{(\pi)}] \right) =  \sqrt{|\Pi_u|} \left(\langle  \hat{\balpha}^{SR}_{\mathcal{S}_u}, \bchi^{\pi} \rangle - \langle  \balpha_{\mathcal{S}_u}, \bchi^{\pi} \rangle  \right) \xrightarrow{d} N(0, \sigma^2 (\bchi^{\pi})^T\mathbf{K}^{-1}_u \bchi^{\pi} ),
\end{equation*}
as $|\Pi_u| \rightarrow \infty$. 
%
Scaling both sides of the equation above by $\sqrt{1/\sigma^2 (\bchi^{\pi})^T\mathbf{K}^{-1}_u \bchi^{\pi}}$ finishes the proof. 

\subsubsection{Proof of Lemma \ref{lem:largest_eigenvalue_covariance_matrix}} 

We begin by defining some notation.
%
For a square matrix $\bX$, let $\lambda_{\min}(\bX)$ and let $\lambda_{\max}(\bX)$ denote the minimum and maximum eigenvalues of $\bX$ respectively. 
%
Additionally, for a positive-definite invertible matrix $\bX$, recall the fact that $\lambda_{\min}(\bX^{-1}) = 1/\lambda_{\max}(\bX)$.
%
Finally, for a general matrix $\mathbf{A}$, let $s_{\max}(\mathbf{A})$ denote its largest singular value. 
%
Then, this gives

\begin{align}
    \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}}  & \leq \sqrt{\frac{|\Pi_u|}{\sigma^2 \lambda_{\min}(\mathbf{K}^{-1}_u) (\bchi^{\pi})^T \bchi^{\pi}}}  \nonumber \\
    & = \sqrt{\frac{|\Pi_u| \lambda_{\max}(\mathbf{K}_u) }{\sigma^2 (\bchi^{\pi})^T \bchi^{\pi}}}  \nonumber \\
    & = \sqrt{\frac{|\Pi_u| \lambda_{\max}(\mathbf{K}_u) }{\sigma^2 2^p}} \nonumber \\
    & =  \sqrt{\frac{\lambda_{\max}((\bchi_{\mathcal{S}_u}(\Pi_u))^T \bchi_{\mathcal{S}_u}(\Pi_u)) }{\sigma^2 2^p}} \nonumber \\
    & =  \frac{s_{\max}(\bchi_{\mathcal{S}_u}(\Pi_u))}{\sqrt{\sigma^2 2^p}} ,\label{eq:horizontal_regression_term2_largest_singular_value}
\end{align}
where in the last line we use the fact that for a matrix $\mathbf{A}$, $s_{\max}(\mathbf{A}) = \sqrt{\lambda_{\max}(\mathbf{A}^T\mathbf{A})}$. 
%
To proceed, recall that $s_{\max}(\mathbf{A}) \leq \lVert \mathbf{A} \rVert_{F}$, where $\lVert \cdot \rVert_{F}$ denote the Frobenius norm.
%
Since $\bchi_{\mathcal{S}_u}(\Pi_u) \in \{-1,1\}^{|\Pi_u| \times |\mathcal{S}_u|}$, we have $\lVert \bchi_{\mathcal{S}_u}(\Pi_u) \rVert_{F} = \sqrt{|\Pi_u| \times |\mathcal{S}_u|} \leq \sqrt{s|\Pi_u|}$. 
%
Substituting $s_{\max}(\bchi_{\mathcal{S}_u}(\Pi_u)) \leq \sqrt{s|\Pi_u|}$ into \eqref{eq:horizontal_regression_term2_largest_singular_value} gives us
\begin{equation*}
     \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}}  = O\left(\sqrt{\frac{s|\Pi_u|}{2^p}}\right),
\end{equation*}
which is the claimed result. 

\subsubsection{Proof of Lemma \ref{lem:ridge_bound}}
Using Cauchy-Schwarz gives,
\begin{equation*}
    \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u \neq \mathcal{S}_u] = \langle \hat{\balpha}_u^{SR}, \bchi^{\pi} \rangle \leq \lVert  \hat{\balpha}_u^{SR} \rVert_2 \lVert \bchi^{\pi} \rVert_2 =  \sqrt{2^p} \lVert  \hat{\balpha}^{SR}_{u} \rVert_2 
\end{equation*}
\noindent Next, we upper bound $\lVert  \hat{\balpha}^{SR}_{u} \rVert_2 $.
%
Let $\mathbf{U}_{\hat{\mathcal{S}}_u} \mathbf{D}_{\hat{\mathcal{S}}_u} \mathbf{V}^T_{\hat{\mathcal{S}}_u}$ denote the SVD of $\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)$. 
%
Then, substituting the SVD of $\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)$ into the definition of $\hat{\balpha}^{SR}_{u}$ (see \eqref{eq:Ridge_estimator}), it is easy to obtain 
\begin{equation} \label{eq:ridge_SVD_1}
    \hat{\balpha}^{SR}_{u} = \mathbf{V}_{\hat{\mathcal{S}}_u}\left(\mathbf{D}^2_{\hat{\mathcal{S}}_u} + \frac{1}{|\Pi_u|} \mathbf{I}_{|\hat{\mathcal{S}}_u|}\right)^{-1}\mathbf{D}_{\hat{\mathcal{S}}_u} \mathbf{U}^T_{\hat{\mathcal{S}}_u} \bY_{\Pi_u}
\end{equation}

\noindent To proceed, define the diagonal matrix $\mathbf{D}'_{\hat{\mathcal{S}}_u} =  \left(\mathbf{D}^2_{\hat{\mathcal{S}}_u} + \frac{1}{|\Pi_u|} \mathbf{I}_{|\hat{\mathcal{S}}_u|}\right)^{-1} \mathbf{D}_{\hat{\mathcal{S}}_u} $.
%
Then, using \eqref{eq:ridge_SVD_1}, we obtain the following upper bound for $\lVert  \hat{\balpha}^{SR}_{u} \rVert^2_2$.
\begin{align}
    \lVert  \hat{\balpha}^{SR}_{u} \rVert^2_2 & = \left(\mathbf{V}_{\hat{\mathcal{S}}_u}  \mathbf{D}'_{\hat{\mathcal{S}}_u} \mathbf{U}^T_{\hat{\mathcal{S}}_u} \bY_{\Pi_u} \right)^T \left(\mathbf{V}_{\hat{\mathcal{S}}_u}  \mathbf{D}'_{\hat{\mathcal{S}}_u} \mathbf{U}^T_{\hat{\mathcal{S}}_u} \bY_{\Pi_u} \right) \nonumber \\
    &  = \bY^T_{\Pi_u} \mathbf{U}_{\hat{\mathcal{S}}_u} \mathbf{D}'_{\hat{\mathcal{S}}_u} \mathbf{V}^T_{\hat{\mathcal{S}}_u} \mathbf{V}_{\hat{\mathcal{S}}_u}  \mathbf{D}'_{\hat{\mathcal{S}}_u} \mathbf{U}^T_{\hat{\mathcal{S}}_u} \bY_{\Pi_u} \nonumber\\
    & = \bY^T_{\Pi_u} \mathbf{U}_{\hat{\mathcal{S}}_u} (\mathbf{D}'_{\hat{\mathcal{S}}_u})^2 \mathbf{U}^T_{\hat{\mathcal{S}}_u} \bY_{\Pi_u} \nonumber \\
    & \leq s_{\max}\left((\mathbf{D}'_{\hat{\mathcal{S}}_u})^2\right) \lVert \mathbf{U}^T_{\hat{\mathcal{S}}_u} \bY_{\Pi_u} \rVert^2_2 \nonumber \\
    & \leq  s_{\max}\left((\mathbf{D}'_{\hat{\mathcal{S}}_u})^2\right) \lVert \mathbf{U}_{\hat{\mathcal{S}}_u}  \rVert^2_2 
 \lVert \bY_{\Pi_u} \rVert^2_2 \nonumber \\
    & \leq s_{\max}\left((\mathbf{D}'_{\hat{\mathcal{S}}_u})^2\right) \lVert \bY_{\Pi_u} \rVert^2_2 \label{eq:ridge_SVD_2}
\end{align}
%
\noindent where the last inequality follows from the fact that $\mathbf{U}_{\hat{\mathcal{S}}_u}$ is a orthonormal matrix. 
%


For the rest of the proof, some additional notation is required.
%
Define $\bepsilon^{\Pi_u} = [\epsilon_n^{\pi}: \pi \in \Pi_u] \in \R^{|\Pi_u|}$.
%
Let $s_{i}(\bchi_{\hat{\mathcal{S}}_u}({\Pi_u}))$ denote the $i$th singular value of $\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)$ for $i = 1 \ldots |\hat{\mathcal{S}}_u|$.
%
For simplicity, suppress dependence on $\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)$, and denote  $s_{i}(\bchi_{\hat{\mathcal{S}}_u}({\Pi_u}))$ as $s_i$.
%

To proceed, observe that the matrix $(\mathbf{D}'_{\hat{\mathcal{S}}_u})^2$ is diagonal with elements $s^2_i/(s^2_i + 1/|\Pi_u|)^2$ for $i = 1 \ldots |\hat{\mathcal{S}}_u|$.
%
Next, noticing that $(a+b)^2 \geq ab$ for any $a,b \geq 0$, we have $s^2_i/(s^2_i + 1/|\Pi_u|)^2 \leq |\Pi_u|$ for all $i = 1 \ldots |\hat{\mathcal{S}}_u|$.
%
As a result, 
$s_{\max}\left((\mathbf{D}'_{\hat{\mathcal{S}}_u})^2\right) \leq |\Pi_u|$.
%
Substituting this inequality into \eqref{eq:ridge_SVD_2}, and simplifying further gives us, 
\begin{align}
    \lVert \hat{\balpha}^{SR}_{u} \rVert^2_2 & \leq |\Pi_u| \lVert \bY_{\Pi_u} \rVert^2_2 \nonumber \\ 
    & =|\Pi_u| \lVert \E[\bY_u^{(\Pi_u)}] + \bepsilon^{\Pi_u} \rVert^2_2 \nonumber \\
    & \leq 2|\Pi_u|\lVert \E[\bY_u^{(\Pi_u)}] \rVert^2_2 + 2|\Pi_u|\lVert \bepsilon^{\Pi_u} \rVert^2_2 \nonumber \\
    & \leq 2|\Pi_u|^2 + 2|\Pi_u| \lVert \bepsilon^{\Pi_u} \rVert^2_2 \label{eq:ridge_SVD_3},
\end{align}

\noindent where the last inequality uses Assumption \ref{ass:boundedness_potential_outcome}. 
%
Finally, it follows from Theorem 3.1.1 of \cite{vershynin_2018} that $\lVert \bepsilon^{\Pi_u} \rVert^2_2 = O_p (|\Pi_u|)$. 
%
Substituting this into \eqref{eq:ridge_SVD_3} completes the proof. 









%Further, define the diagonal matrix $\mathbf{D}'_{\hat{\mathcal{S}}_u} =  \left(\mathbf{D}^2_{\hat{\mathcal{S}}_u} + \frac{1}{|\Pi_u|} \mathbf{I}_{|\hat{\mathcal{S}}_u|}\right)^{-1}$.
%
%Using \eqref{eq:ridge_SVD_1}, and the notation defined above we have the following expression for $\lVert  \hat{\balpha}_{\mathcal{S}_u} \rVert^2_2$
%\begin{align*}
%    \lVert  \hat{\balpha}_{\mathcal{S}_u} \rVert^2_2 = \left( \right)
%\end{align*}
%Let $s_{i}(\bchi_{\hat{\mathcal{S}}_u}(\Pi_u))$ denote the $i$th singular value of $\bchi_{\hat{\mathcal{S}}_u}(\Pi_u)$. 
%Note that $\mathbf{D}'_{\hat{\mathcal{S}}_u}$ has elements $(s^2_i + 1/|\Pi_u|)^{-1}$ for $i = 1 \ldots |\hat{\mathcal{S}}_u|$.

\subsubsection{Proof of Lemma \ref{lem:fourier_coefficient_asymptotic_normality}}
The proof of this lemma follows immediately from adapting notation of Theorem 3 in \cite{liu2013asymptotic} to this paper. 
%
In particular, $Y = \bY_{\Pi_u}$, $X = \bchi(\Pi_u)$, $p = 2^p$, $\beta = \balpha_u$, $\Tilde{\beta}_{\text{Select + Ridge}} = \hat{\balpha}_u^{SR}$, $\beta_S = \balpha_{\mathcal{S}_u}$, $\Tilde{\beta}_{\text{Select + Ridge}, \mathcal{S}_u}= \hat{\balpha}^{SR}_{\mathcal{S}_u} $,  $\mu_n = 1/|\Pi_u|$, where the left hand side of each equality is the notation used in \cite{liu2013asymptotic}.
%




\subsection{Proof of Theorem \ref{thm:vertical_regression_normality}}



In this section, we provide the proof of Theorem \ref{thm:vertical_regression_normality}, i.e., establish asymptotic normality of the vertical regression step of \method. We also recall the following notation, let $\Delta^n_w = \hat{\bw}^n - \Tilde{\bw}^n \in \R^{|\mathcal{I}|}$, and $\Delta_{\mathcal{I}}^{\pi} = \hat{\E}[\bY_{\mathcal{I}}^{(\pi)}] - \E[\bY_{\mathcal{I}}^{(\pi)}] \in \R^{|\mathcal{I}|}$. 

\noindent Using Lemma \ref{lem:w_tilde_transfer_outcomes}, and the notation established above, we have
\begin{align}
     \hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}] & = \langle \hat{\E}[\bY_{\mathcal{I}}^{(\pi)}], \hat{\bw}^n \rangle - \langle \E[\bY_{\mathcal{I}}^{(\pi)}], \Tilde{\bw}^n \rangle  \nonumber \\
     & = \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle  +   \langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\Delta^n_w   \rangle  +\langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle. \nonumber
\end{align}
From Assumption \ref{ass:rowspace_inclusion}, it follows that $\E[\bY_{\mathcal{I}}^{(\pi)}] = \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} \E[\bY_{\mathcal{I}}^{(\pi)}]  $ , where $\bV_{\mathcal{I}}^{(\Pi_n)}$ are the right singular vectors of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$. Plugging this into the equation above gives us
\begin{equation}
\label{eq:three_term_asymptotic_normality}
    \hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}] = \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle  + \langle   \E[\bY_{\mathcal{I}}^{(\pi)}] ,\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w   \rangle  +\langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle. 
\end{equation}
Next, we scale the left-hand side (LHS) of \eqref{eq:three_term_asymptotic_normality}  by $ \left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{-1/2}$, and analyze term 1, and terms 2 \& 3 together  on the right-hand side (RHS) of \eqref{eq:three_term_asymptotic_normality} separately. 

\noindent \emph{Term 1.} Expanding the first term gives us
\begin{equation} \label{eq:vertical_asymptotic_normality_term1_intermediate1}
   \frac{1}{\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{1/2}}  \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle = \frac{1}{\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{1/2}}  \sum_{u \in \mathcal{I}}  \Tilde{w}^n_u \left(\hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right)
\end{equation}

\noindent From \eqref{eq:asymptotic_normality_condition_2}, as $|\Pi_u| \rightarrow \infty$, we have that 
\begin{equation*}
     \Tilde{w}^n_u \left(\hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right) \xrightarrow{d} N(0,\Tilde{\sigma}^2_u)
\end{equation*}

\noindent Since $\Tilde{w}^n_u \left(\hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right)$ is independent across $u$, as $M \rightarrow \infty$, the equation above implies
\begin{equation*}
    \sum_{u \in \mathcal{I}}
     \Tilde{w}^n_u \left(\hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right) \xrightarrow{d} N(0,\sum_{u \in \mathcal{I}}\Tilde{\sigma}^2_u)
\end{equation*}

\noindent Substituting the equation above into \eqref{eq:vertical_asymptotic_normality_term1_intermediate1} yields the following, 
\begin{equation} \label{eq:vertical_asymptotic_normality_term1}
   \frac{1}{\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{1/2}}  \sum_{u \in \mathcal{I}}  \Tilde{w}^n_u \left(\hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right) \xrightarrow{d} \mathcal{N}(0,1),
\end{equation}
as $M \rightarrow \infty$.

\noindent \emph{Terms 2 and 3.} We scale the right hand side of \eqref{eq:collecting_terms_simplified_log} by $ \left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{-1/2}$ which yields the following,
\begin{equation} \label{eq:vertical_asymptotic_normality_term23_intermediate1}
      \frac{1}{\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{1/2}} \left(\langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\Delta^n_w   \rangle   +\langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right) = O_p\left(\frac{\log^{3}(|\Pi_n||\mathcal{I}|)}{\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{1/2}}\left(r^2_n\sqrt{\frac{{s^2p}}{{M}}} +  \frac{r_n}{|\Pi_n|^{1/4}}\right) \right)
\end{equation}

\vspace{1mm}

\noindent To proceed, we state the following lemma 

\begin{lemma} \label{lem:scaled_variance_upper_bound}
Let the set-up of Theorem \ref{thm:vertical_regression_normality} hold. Then, we have
\begin{equation*}
  \left( \sum_{u \in \mathcal{I}}  \Tilde{\sigma}^2_u \right)^{-1/2} = O\left(\frac{1}{\lVert \Tilde{\bw}^n \rVert_2} \right) 
\end{equation*}
  
\end{lemma}

\noindent Substituting the result of Lemma \ref{lem:scaled_variance_upper_bound} into \eqref{eq:vertical_asymptotic_normality_term23_intermediate1}, and recalling \eqref{eq:asymptotic_normality_condition_1} yields
\begin{equation} \label{eq:asymptotic_normality_term23_op_bound}
     \frac{1}{\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{1/2}} \left(\langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\Delta^n_w   \rangle   +\langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right) = o_p(1)
\end{equation}

\noindent Collecting \eqref{eq:vertical_asymptotic_normality_term1}, and \eqref{eq:asymptotic_normality_term23_op_bound} gives us the result. 


\subsection{Proof of Lemma \ref{lem:scaled_variance_upper_bound}}
\label{subsec:scale_variance_proof}
%
For a donor unit $u \in \mathcal{I}$, we have, 
\begin{align}
    \Tilde{\sigma}^{2}_u & = \frac{(\bchi^{\pi})^T \mathbf{K}^{-1}_u \bchi^{\pi} \left( \Tilde{w}^n_u 
    \right)^2 }{|\Pi_u|}  \nonumber \\
    & \geq \frac{\lambda_{\min}(\mathbf{K}^{-1}_u)(\bchi^{\pi})^T \bchi^{\pi} \left( \Tilde{w}^n_u 
    \right)^2}{|\Pi_u|} \nonumber \\
    & = \frac{\lambda_{\min}(\mathbf{K}^{-1}_u)2^p \left( \Tilde{w}^n_u 
    \right)^2}{|\Pi_u|} \nonumber \\
    & = \frac{2^p \left( \Tilde{w}^n_u 
    \right)^2 }{\lambda_{\max}(\mathbf{K}_u)|\Pi_u|} \nonumber \\
    & \geq \frac{\left( \Tilde{w}^n_u 
    \right)^2}{\lambda_{\max}(\mathbf{K}_u)} \label{eq:sigma_tilde_largest_eigenvalue},
\end{align}
where in the last line we use the fact that $|\Pi_u| \leq 2^p$. 
%
To proceed, we upper bound $\lambda_{\max}(\mathbf{K}_u)$.
%
To do so, we define some notation. 
%
Let $\mathbf{I}_m \in \mathbb{R}^{m \times m}$ denote the identity matrix of dimension $m$.
%
Additionally, recall the following fact: for a matrix $\mathbf{A} \in \mathbb{R}^{m \times m}$, we have that $\lVert \mathbf{A} \rVert_2 \leq m \lVert \mathbf{A} \rVert_{\infty}$.
%
Using this and Assumption \ref{ass:incoherence}, we have that
\begin{align*}
    \lVert \mathbf{K}_u - \mathbf{I}_{|\mathcal{S}_u|} \rVert_2 & = \lVert \frac{(\bchi_{\mathcal{S}_u}(\Pi_u))^T\bchi_{\mathcal{S}_u}(\Pi_u)}{|\Pi_u|} - \mathbf{I}_{|\mathcal{S}_u|} \rVert_2 \\ 
    & \leq s \lVert \frac{(\bchi_{\mathcal{S}_u}(\Pi_u))^T\bchi_{\mathcal{S}_u}(\Pi_u)}{|\Pi_u|} - \mathbf{I}_{|\mathcal{S}_u|} \rVert_\infty \\
    & \leq  s \lVert \frac{(\bchi(\Pi_u))^T\bchi(\Pi_u)}{|\Pi_u|} - \mathbf{I}_{2^p} \rVert_\infty \leq C
\end{align*}
%
\noindent for a positive constant $C > 0$.
%
Next, using the equation above and Weyl's inequality \cite{wainwright2019high}, we have that 
\begin{equation*}
    \lambda_{\max}(\mathbf{K}_u)  \leq \lVert \mathbf{K}_u - \mathbf{I}_{|\mathcal{S}_u|}  \rVert_2 +  \lambda_{\max}(\mathbf{I}_{|\mathcal{S}_u|}) \leq 1 + C
\end{equation*}


\noindent Substituting the result of Lemma \ref{lem:largest_eigenvalue_covariance_matrix} into \eqref{eq:sigma_tilde_largest_eigenvalue} gives us $\Tilde{\sigma}^{2}_u \geq \left( \Tilde{w}^n_u \right)^2/(1 + C)$. 
%    
Hence, we have $\left( \sum_{u \in \mathcal{I}} \hspace{0.5mm} \Tilde{\sigma}^2_u \right)^{-1/2} = O(1/\lVert \Tilde{\bw}^n \rVert_2)$, which is the claimed result. 
%









































































%To proceed, we state the following Lemma
%\begin{equation}
%\label{eq:horizontal_regression_term1_intermediate_1}
%      \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}}\left( \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u = \mathcal{S}_u] -  \E[Y_u^{(\pi)}] \right) \P(\hat{\mathcal{S}}_u = \mathcal{S}_u) =   \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}} \langle \hat{\balpha}^{SR}_{\mathcal{S}_u} - \balpha_{\mathcal{S}_u} , \bchi^{\pi} \rangle  \P(\hat{\mathcal{S}}_u = \mathcal{S}_u)
%\end{equation}




%\begin{align}
%  \sqrt{\frac{|\Pi_u|}{\sigma^2(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}}} \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u \neq \mathcal{S}_u]  \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) & \leq \sqrt{|\Pi_u|} \langle \hat{\balpha}^{SR}_u,\bchi^{\pi} \rangle \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) \nonumber \\ 
%    & \leq \left(\sqrt{|\Pi_u|} \lVert \hat{\balpha}^{SR}_u \rVert_2 \lVert \bchi^{\pi} \rVert_2 \right)\P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) \nonumber \\
%    & \leq \left(\lVert  \hat{\balpha}^{SR}_u \rVert_2 \sqrt{|\Pi_u| 2^p} \right) \P(\hat{\mathcal{S}}_u \neq \mathcal{S}_u) \label{eq:horizontal_regression_term2_intermediate_1}
%\end{align}


%\noindent where the first inequality follows from Cacuhy-Schwarz, and the second follows from the fact that $\bchi^{\pi} \in \{-1,1\}^{2^p}$. Next, using condition (d) of Proposition \ref{prop:horizontal_asymptotic_normality}, we have $2^p = O(2^{|\Pi_u|^{c_3}})$. 
%
%Plugging in  $2^p = O(2^{|\Pi_u|^{c_3}})$, and condition (e) into \eqref{eq:horizontal_regression_term2_intermediate_1}, gives us
%\begin{equation}
%     \hat{\E}_{SR}[Y_u^{(\pi)} | \hat{\mathcal{S}}_u \neq \mathcal{S}_u] = \lVert \hat{\balpha}^{SR}_u \rVert_2 o(\sqrt{|\Pi_u|} 2^{\frac{|\Pi_u|^{c_3}}{2}} e^{-|\Pi_u|^{c_3}})
%\end{equation}

%\begin{equation*}
%    \sqrt{|\Pi_u|} \left( \hat{\E}_{SR}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] \right) \xrightarrow{d} N\left(0,(\bchi^{\pi})^T\mathbf{K}^{-1}_u\bchi^{\pi}\right), %\end{equation*}


%\begin{lemma} 
%\label{lem:op_sequence_bound}
%For a strictly positive sequence $a_n$, and positive constants  $c, %\hspace{0.25mm} C$, we have that 
%\begin{equation*}
%    o_p(a^{c}_n  e^{-a_n^{C}}) = o_p(1)
%\end{equation*}
%as $a_n \rightarrow \infty$.
%\end{lemma}



%\begin{equation} \label{eq:term1_asymptotic_normality}
%    \sum_{u \in \mathcal{I}} \sqrt{\frac{|\Pi_u|}{\Tilde{\sigma}^2_u|\mathcal{I}|}} \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle =  \sum_{u \in \mathcal{I}}   \sqrt{\frac{|\Pi_u|}{\Tilde{\sigma}^2_u|\mathcal{I}|}} \left( \right)
%    = \frac{\langle \hat{\E}[\bY_{\mathcal{I}}^{(\pi)}] - \E[\bY_{\mathcal{I}}^{(\pi)}] , \Tilde{\bw}^n \rangle }{\lVert \Tilde{\bw}^n \rVert_2}
%\end{equation}

%
%\noindent For the second and third term, we scale the RHS of \eqref{eq:collecting_terms_simplified_log} by $1/\lVert \Tilde{\bw}^n \rVert_2$, which gives us
%\begin{equation*}
%     \frac{1}{\lVert \Tilde{\bw}^n \rVert_2}\left(\langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\Delta^n_w   \rangle   +\langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right) =  O_p\left(\log^{3}(|\Pi_n||\mathcal{I}|) r^2_n\sqrt{\frac{{s^2p}}{{M}}} +  \frac{r_n}{|\Pi_n|^{1/4}} \right).
%\end{equation*}

%\noindent To proceed, we substitute the result of 
%Then, substituting \eqref{eq:asymptotic_normality_condition_1} into the RHS of the equation above, gives us 
%\begin{equation}\label{eq:asymptotic_normality_term2_3_op_bound}