\section{Proof of Theorem \ref{thm:potential_outcome_convergence_rate}}

\subsection{Proof of Theorem  \ref{thm:potential_outcome_convergence_rate} (a)}
We have that 
\label{subsec:horizontal_regression_theorem_proof}
\begin{align} 
     \hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}] & = \langle  \hat{\balpha}^u, \bchi^{\pi} \rangle - \langle  \balpha_u, \bchi^{\pi} \rangle \nonumber\\
     & = \langle  \hat{\balpha}_u - \balpha_u, \bchi^{\pi} \rangle \nonumber \\
     & \leq \lVert \hat{\balpha}_u - \balpha_u \rVert_1 \lVert \bchi^{\pi} \rVert_{\infty} \nonumber \\ 
     & = \lVert \hat{\balpha}_u - \balpha_u \rVert_1 \label{eq:lasso_proof_holder_inequality}
\end{align}

\noindent To finish the proof, we quote the following Theorem which we adapt to our notation. 

\begin{theorem}[Theorem 2.18 in \cite{rigollet2015high}] 
\label{thm:rigollet_lasso_high_prob_bound}
Fix the number of samples $n \geq 2$. Assume that the linear model $Y = \bX\theta^* + \epsilon$, where $\bX \in \mathbb{R}^{n \times d}$ and $\epsilon$ is a sub-gaussian random variable with noise variance $\sigma^2$. Moreover, assume that $\lVert \theta^* \rVert_0 \leq k$, and that 
$\bX$ satisfies the incoherence condition (Assumption \ref{ass:incoherence}) with parameter $k$. Then, the lasso estimator $\hat{\theta}^L$ with regularization parameter defined by 
\begin{equation*}
    2\tau = 8\sigma\sqrt{\log(2d)/n} +  8\sigma\sqrt{\log(1/\delta)/n}
\end{equation*}
satisfies 
\begin{equation}
    \lVert \theta^* - \hat{\theta}^L \rVert^2_2 \leq k\sigma^2\frac{\log(2d/\delta)}{n}
\end{equation}
with probability at least $1 - \delta$. 
\end{theorem}
Further, as in established in the proof of Theorem 2.18 in \cite{rigollet2015high}, $\lVert \theta^* - \hat{\theta}^L \rVert_1 \leq \sqrt{k} \lVert \theta^* - \hat{\theta}^L \rVert_2$. Note that the set-up of Theorem \ref{thm:rigollet_lasso_high_prob_bound} holds in our setting with the following notational changes: $Y = \bY_{\Pi_u}$, $\bX = \bchi(\Pi_u)$, $\theta^* = \balpha_u$, $\hat{\theta}^L = \hat{\balpha}_u$,  $k = s$ as well as our assumptions on the regularization parameter $\lambda_u$ and that $\bepsilon_u^{\pi}$ is sub-gaussian (Assumption \ref{ass:subgaussian_noise}). Applying Theorem \ref{thm:rigollet_lasso_high_prob_bound} gives us
\begin{equation*}
    \lVert \hat{\balpha}_u - \balpha_u \rVert_1 = O_p \left( \sqrt{\frac{s^2p}{|\Pi_u|}}\right)
\end{equation*}
Substituting this bound into \eqref{eq:lasso_proof_holder_inequality} yields the claimed result. 


%\section{Lasso Horizontal Regression Analysis}
%\label{sec:Lasso_finite_sample_analysis}

%\begin{proposition} 
%\label{prop:Lasso_convergence_rate} 
%Let Assumptions \ref{ass:observation_model}, \ref{ass:subgaussian_noise}, and \ref{ass:incoherence} hold. Then, conditioned on $\mathcal{A}$, we have
%\begin{equation}
%\label{eq:Lasso_convergence_rate}
% \hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] = O_p\left(\sqrt{\frac{s^2p}{|\Pi_u|}} ~ \bigg | ~ \mathcal{A} \right) 
%\end{equation}
%for any donor unit $u \in\mathcal{I}$ and $\pi \in \Pi$
%\end{proposition}



\subsection{Proof of Theorem  \ref{thm:potential_outcome_convergence_rate} (b)}
For any matrix $\mathbf{A}$ with orthonormal columns, let $\mathcal{P}_{A} = \mathbf{A}\mathbf{A}^T$ denote the projection matrix on the subspace spanned by the columns of $\mathbf{A}$. Define $\Tilde{\bw}^n = \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\bw^n$, where $\bV_{\mathcal{I}}^{(\Pi_n)}$ are the right singular vectors of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$. Let $\Delta^n_w = \hat{\bw}^n - \Tilde{\bw}^n \in \R^{|\mathcal{I}|}$, and $\Delta_{\mathcal{I}}^{\pi} = \hat{\E}[\bY_{\mathcal{I}}^{(\pi)}] - \E[\bY_{\mathcal{I}}^{(\pi)}] \in \R^{|\mathcal{I}|}$. Denote $\Delta_E = \max_{u \in \mathcal{I}, \pi \in \Pi} \left |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}] \right |$. In order to proceed, we first state the following result,  
\begin{lemma} 
\label{lem:w_tilde_transfer_outcomes}
Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. Then, we have 
\begin{equation*}
    \E[Y_{n}^{(\pi)}] =  \langle \E[\bY_{\mathcal{I}}^{(\pi)}], \Tilde{\bw}^n \rangle
\end{equation*}
\end{lemma}

\noindent Using Lemma \ref{lem:w_tilde_transfer_outcomes}, and the notation established above, we have
\begin{align}
     \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right| & = \left| \langle \hat{\E}[\bY_{\mathcal{I}}^{(\pi)}], \hat{\bw}^n \rangle - \langle \E[\bY_{\mathcal{I}}^{(\pi)}], \Tilde{\bw}^n \rangle \right| \nonumber \\
     & \leq \left| \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle \right| +  \left| \langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\Delta^n_w   \rangle \right| + \left| \langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right| \label{eq:three_term_bound_pre_projection}
\end{align}
From Assumption \ref{ass:rowspace_inclusion}, it follows that $\E[\bY_{\mathcal{I}}^{(\pi)}] = \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} \E[\bY_{\mathcal{I}}^{(\pi)}]  $ , where $\bV_{\mathcal{I}}^{(\Pi_n)}$ are the right singular vectors of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$. Using this in \eqref{eq:three_term_bound_pre_projection} gives us
\begin{equation}
 \label{eq:three_term_bound_post_projection}
   \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right| \leq  \left| \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle \right| +  \left| \langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w   \rangle \right| + \left| \langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right|
\end{equation}
Below, we bound the three terms on the right-hand-side of \eqref{eq:three_term_bound_post_projection} separately. Before we bound each term, we state a Lemma that will be useful to help us establish the results. 

\begin{lemma} 
\label{lem:l1_bound_w_tilde}
Let Assumptions \ref{ass:observation_model}, \ref{ass:donor_set_identification}, \ref{ass:boundedness_potential_outcome}, and \ref{ass:balanced_spectrum} hold. Then, $\lVert \tilde{\bw}^n \rVert_2 \lesssim \sqrt{\frac{r_n}{|\mathcal{I}|}}$
\end{lemma}


\noindent \emph{Bounding Term 1.} By Holder's inequality and Lemma \ref{lem:l1_bound_w_tilde}, we have that 
\begin{equation}
\label{eq:Term1_Op_bound}
    \left| \langle \Delta_{\mathcal{I}}^{\pi}, \Tilde{\bw}^n \rangle \right| \leq \lVert \Tilde{\bw}^n \rVert_1 \lVert \Delta_{\mathcal{I}}^{\pi} \rVert_\infty \leq \lVert \Tilde{\bw}^n \rVert_1 \Delta_E \leq  \sqrt{|\mathcal{I}|}\lVert \Tilde{\bw}^n \rVert_2 \Delta_E \lesssim \sqrt{r_n}\Delta_E
\end{equation}
\noindent This concludes the analysis for the first term. \\ 

\noindent \emph{Bounding Term 2}. By Cauchy-Schwarz and Assumption \ref{ass:boundedness_potential_outcome} we have,  
\begin{align}
    \left| \langle \E[\bY_{\mathcal{I}}^{(\pi)}] ,\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rangle \right | & \leq \lVert \E[\bY_{\mathcal{I}}^{(\pi)}] \rVert_2 \lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w  \rVert_2 \nonumber \\
    & \leq \sqrt{|\mathcal{I}|} \lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w  \rVert_2 \label{eq:term2_cauchy_schwarz}
\end{align}
\noindent We now state a lemma that will help us conclude our bound of Term 2. The proof is given in Appendix \ref{subsubsec:proof_lemma_projection_delta_w_bound}. 
\begin{lemma} 
\label{lem:projection_delta_w_bound}
Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. Then, 
\begin{align}
& \lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rVert_2 \nonumber \\
& = \tilde{O_p} \left(  r_n^{2}\left[\frac{\Delta_E  }{\sqrt{|\mathcal{I}|}\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \Delta^2_E \right] + \frac{r_n^{3/2}\Delta_E}{\sqrt{|\mathcal{I}|}} + \frac{r_n\left( 1 + \sqrt{\Delta_E}r^{1/4}_n\right)}{\sqrt{|\mathcal{I}|}|\Pi_n|^{1/4}}  \right) \label{eq:projection_delta_w_bound}
\end{align}
%\begin{equation}
%\label{eq:projection_delta_w_bound}
%    \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rVert_2  = O_p \left(\left(\log^3\left(|\Pi_n|N_{D}\right) r_n^{3/2}\left[\frac{\lVert \Tilde{\bw}^{n} \rVert_1 \Delta_E  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}} }  + \Delta^2_E \right] \right) + \frac{r_n\left( 1 + \sqrt{\Delta_E\lVert \Tilde{\bw}^n \rVert_1}\right)}{\sqrt{|\mathcal{I}|}|\Pi_n|^{1/4}}  \right)
%\end{equation}
\end{lemma}
\noindent Incorporating the result of the lemma above into \eqref{eq:term2_cauchy_schwarz} gives us
\begin{align}
     & \left| \langle \E[\bY_{\mathcal{I}}^{(\pi)}] \nonumber ,\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rangle  \right| \\
     & = \tilde{O_p} \left(  r_n^{2}\left[\frac{\Delta_E  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \sqrt{|\mathcal{I}|}\Delta^2_E \right]  + r_n^{3/2}\Delta_E + \frac{r_n\left( 1 + \sqrt{\Delta_E}r^{1/4}_n\right)}{|\Pi_n|^{1/4}}  \right)  \label{eq:Term2_Op_bound}
\end{align}

\noindent \emph{Bounding Term 3.} By Holder's inequality,  we have that
\begin{align}
    \left| \langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right| & \leq \lVert \Delta^n_w \rVert_2 \lVert \Delta_{\mathcal{I}}^{\pi} \rVert_2 \nonumber \\
    & \leq \sqrt{|\mathcal{I}|} \lVert \Delta^n_w \rVert_2 \lVert \Delta_{\mathcal{I}}^{\pi} \rVert_\infty \nonumber \\
    & \leq  \sqrt{|\mathcal{I}|} \Delta_E \lVert \Delta^n_w \rVert_2 \label{eq:term3_holder} \\ 
\end{align}
\noindent We now state a proposition that will help us conclude our proof of Term 3. The proof is given in Appendix \ref{subsec:pcr_proofs}.

\begin{proposition}
\label{prop:pcr_convergence_rate} Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. Then, conditioned on $\mathcal{A}$, we have 
\begin{equation}
    \hat{\bw}^n - \Tilde{\bw}^n=  \tilde{O}_p \left(r_n \left[\frac{\lVert \Tilde{\bw}^{n} \rVert_2  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \Delta_E \right] \right) 
\end{equation}
\end{proposition}


\noindent As a result of Proposition \ref{prop:pcr_convergence_rate} and Lemma \ref{lem:l1_bound_w_tilde}, we have the following bound for Term 3,  
\begin{equation}
\label{eq:Term3_Op_bound}
     \left| \langle \Delta^n_w, \Delta_{\mathcal{I}}^{\pi} \rangle \right| = \Tilde{O_p} \left( r^{3/2}_n \left[\frac{\Delta_E  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \sqrt{|\mathcal{I}|}\Delta^2_E \right] \right)
\end{equation}

\noindent \emph{Collecting Terms.} Combining equations \eqref{eq:Term1_Op_bound}, \eqref{eq:Term2_Op_bound}, \eqref{eq:Term3_Op_bound} gives us
\begin{equation}
\label{eq:collecting_terms}
\begin{split}
&\left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right|   
\\ &= \Tilde{O_p}\left( r_n^{2}\left[\frac{\Delta_E  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \sqrt{|\mathcal{I}|}\Delta^2_E \right] + r_n^{3/2}\Delta_E + \frac{r_n\left( 1 + \sqrt{\Delta_E}r^{1/4}_n\right)}{|\Pi_n|^{1/4}}  \right)  
\end{split}
\end{equation} \\

\noindent By Theorem \ref{thm:potential_outcome_convergence_rate} (a), we have that 
\begin{equation}
\label{eq:delta_E_subsitution}
    \Delta_E = \max_{u \in \mathcal{I}} \ O_p \left(\sqrt{\frac{s^2p}{|\Pi_u|}}\right) = O_p \left(\sqrt{\frac{s^2p}{M}}\right)
\end{equation}
\noindent where we remind the reader that $M = \min_{u \in \mathcal{I}} |\Pi_u|$. Substituting \eqref{eq:delta_E_subsitution}, and our assumption that $M = \omega(r^2_ns^2p)$ into \eqref{eq:collecting_terms}, we get 
\begin{equation}
\label{eq:collecting_terms_simplified}
\begin{split}
\left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right|  = \Tilde{O_p}\left(  \frac{r^2_n\sqrt{s^2p}}{\sqrt{M \times \min\{|\Pi_n|,|\mathcal{I}|\}}} + \frac{r^2_ns^2p\sqrt{|\mathcal{I}|}}{M} + \frac{r_n}{|\Pi_n|^{1/4}}\right)
\end{split}
\end{equation}



%\sqrt{\frac{s^2p}{M}} r_n^{2}\left[\frac{1}{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  +  \sqrt{\frac{s^2p|\mathcal{I}|}{M}} \right]  + \frac{r_n}{|\Pi_n|^{1/4}}  \right)  
%\end{split} %\left( \right)
%We now state a lemma that will help us simplify the result, and conclude the proof.
%The proof of this lemma can be found in Appendix \ref{subsec:proof_of_l1_bound_w_tilde}.
%Substituting the result of Lemma \ref{lem:l1_bound_w_tilde} and Theorem \ref{thm:potential_outcome_convergence_rate} (a)  into \eqref{eq:collecting_terms}, and simplifying completes the proof. \textcolor{red}{define $\Delta_E$, help simplifying}

\section{Proofs of Helper Lemmas for Theorem  \ref{thm:potential_outcome_convergence_rate}.}
\label{sec:helper_lemma_proofs}
In this section we provide proofs of Lemmas \ref{lem:w_tilde_transfer_outcomes}, \ref{lem:l1_bound_w_tilde}, \ref{lem:projection_delta_w_bound}, and Proposition \ref{prop:pcr_convergence_rate} which were required for the Proof of Theorem \ref{thm:potential_outcome_convergence_rate}.  

\subsection{Proof of Lemma \ref{lem:w_tilde_transfer_outcomes}}
\label{subsec:proof_of_w_tilde_transfer_outcomes}

By Assumption \ref{ass:observation_model} and \ref{ass:donor_set_identification} (b), we have
\begin{align*}
    \E[Y_n^{(\pi)}] & = \E[\langle \balpha_n, \bchi^{\pi} \rangle + \epsilon_n^{\pi}] \\
    & =  \E[\langle \balpha_n, \bchi^{\pi} \rangle ] \\
    & =  \sum_{u \in \mathcal{I}}w^n_u\E[\langle \balpha_u, \bchi^{\pi} \rangle] \\
    & = \sum_{u \in \mathcal{I}}w^n_u \E[Y_u^{(\pi)}] \\
    & = \E[\bY_{\mathcal{I}}^{(\pi)}]^T \bw^{n}
\end{align*}
From Assumption \ref{ass:rowspace_inclusion}, we have that  $\E[\bY_{\mathcal{I}}^{(\pi)}] = \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} \E[\bY_{\mathcal{I}}^{(\pi)}]$. Substituting this into the equation above completes the proof. 


\subsection{Proof of Lemma \ref{lem:l1_bound_w_tilde}}
\label{subsec:proof_of_l1_bound_w_tilde}
For simplicity, denote $\E[\bY_n^{(\Pi_n)}] = \E[\bY^{(\Pi_n)}]$.  By definition, $\tilde{\bw}^n$ is the solution to the following optimization program 
\begin{align}
\min_{ \bw \in \mathbb{R}^{|\mathcal{I}|}} \quad & ||{\bw}||_2 \nonumber \\
\textrm{s.t.} \quad & \E[\bY^{(\Pi_n)}] = \E[\bY_{\mathcal{I}}^{(\Pi_n)}]\bw
\label{eq:linear_system_donor_set}
\end{align}

\noindent Let $\bU_{\mathcal{I}}^{(\Pi_n)}, \mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_n)}, \bV_{\mathcal{I}}^{(\Pi_n)}$ denote the SVD of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$. Further, let $\bU_{\mathcal{I}}^{(\Pi_n),r_n}, \mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_n),r_n}, \bV_{\mathcal{I}}^{(\Pi_n),r_n}$ denote the rank $r_n$ truncation of the SVD. Then, define $\bw_{r_n} =  \bV_{\mathcal{I}}^{(\Pi_n),r_n} (\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_n),r_n})^{\dagger} (\bU_{\mathcal{I}}^{(\Pi_n),r_n})^T \E[\bY^{(\Pi_n)}]$, where $\dagger$ is pseudo-inverse. We first show that $\bw_{r_n}$ is a solution to \eqref{eq:linear_system_donor_set}. 


\begin{align*}
    \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \bw_{r_n} &= \left( \bU_{\mathcal{I}}^{(\Pi_n)} \mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_n)} (\bV_{\mathcal{I}}^{(\Pi_n)})^T \right) \bV_{\mathcal{I}}^{(\Pi_n),r_n} (\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_n),r_n})^{\dagger} (\bU_{\mathcal{I}}^{(\Pi_n),r_n})^T \E[\bY^{(\Pi_n)}] \\
    & = \left(\sum^{r_n}_{i=1} s_i u_i v^{T}_i\right) \left(\sum^{r_n}_{j=1} \frac{1}{s_j}v_j u^T_j \E[\bY^{(\Pi_n)}]\right) \\
    & = \sum^{r_n}_{i,j=1} \frac{s_i}{s_j} u_i v^T_i v_j u_j^T \E[\bY^{(\Pi_n)}] \\
    & = \sum^{r_n}_{i=1} u_i u^T_i \E[\bY^{(\Pi_n)}] \\
    & = \E[\bY^{(\Pi_n)}] 
\end{align*}


Next, we bound $\lVert \bw_{r_n} \rVert_2$ using Assumptions \ref{ass:boundedness_potential_outcome} and \ref{ass:balanced_spectrum} as follows 
\begin{align*}
    \lVert \bw_{r_n} \rVert_2 & \leq \lVert (\mathbf{\Sigma}_{\mathcal{I}}^{(\Pi_n),r_n})^{\dagger} \rVert_2  \lVert \E[\bY^{(\Pi_n)}] \rVert_2 \\
    & \leq \frac{\sqrt{|\Pi_n|}}{s_{r_n}(\E[\bY_{\mathcal{I}}^{(\Pi_n)}])} \\
    & \leq \sqrt{\frac{cr_n}{|\mathcal{I}|}}
\end{align*}
Therefore, we have 
\begin{align*}
    \lVert \Tilde{\bw}^n \rVert_1 \leq \sqrt{|\mathcal{I}|} \lVert \Tilde{\bw}_{r_n} \rVert_2 \leq \sqrt{cr_n}
\end{align*}



\subsection{Proof of Lemma \ref{lem:projection_delta_w_bound}}
\label{subsubsec:proof_lemma_projection_delta_w_bound}

\noindent First, we introduce some necessary notation required for the proof. Let $\hat{\E}\left[\bY_{\mathcal{I}}^{(\Pi_n)} \right] = \hat{\bU}_{\mathcal{I}}^{(\Pi_n)}\hat{\bS}_{\mathcal{I}}^{(\Pi_n)}\hat{\bV}_{\mathcal{I}}^{(\Pi_n)}$ denote the rank $r_n$ SVD of $\hat{\E}\left[\bY_{\mathcal{I}}^{(\Pi_n)} \right]$. Then, to establish Lemma \ref{lem:projection_delta_w_bound}, consider the following decomposition:
\begin{equation*}
    \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w = \left(\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right)\Delta^n_w  + \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w 
\end{equation*}
\noindent We bound each of these terms separately again. \\

\noindent \emph{Bounding Term 1}. We have
\begin{equation}
\label{eq:projection_operator_error_cauchy}
     \lVert \left(\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right)\Delta^n_w \rVert_2  \leq \left\lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right\rVert_{\text{op}} \lVert \Delta^n_w \rVert_2
\end{equation}

\begin{theorem} [Wedin's Theorem \cite{wedin1972perturbation}]
\label{thm:wedin_theorem}
Given $\mathbf{A},\mathbf{B} \in \R^{m \times n}$, let $(\bU,\bV),(\hat{\bU},\hat{\bV})$ denote their respective left and right singular vectors. Further, let $(\bU_k,\bV_k) \in $(respectively, $(\hat{\bU}_k,\hat{\bV}_k)$) correspond to the truncation of $(\bU,\bV)$ (respectively, $(\hat{\bU},\hat{\bV})$), respectively, that retains the columns correspondng to the top $k$ singular values of $\mathbf{A}$ (respectively, $\mathbf{B}$). Let $s_i$ represent the $i$-th singular values of $A$. Then, $\max(\lVert \mathcal{P}_{U_k} - \mathcal{P}_{\hat{U}_k}\rVert_{\text{op}}, \lVert \mathcal{P}_{V_k} - \mathcal{P}_{\hat{V}_k}\rVert_{\text{op}}) \leq \frac{2\lVert \mathbf{A} - \mathbf{B} \rVert_{\text{op}}}{s_{k}-s_{k+1}}$   
\end{theorem}

\noindent Applying Theorem \ref{thm:wedin_theorem} gives us 
\begin{align}
\max\left(\left\lVert \mathcal{P}_{U_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{U}_{\mathcal{I}}^{(\Pi_n)}}  \right\rVert_{\text{op}}, \left\lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right\rVert_{\text{op}} \right) & \leq  \frac{2\lVert \E[\bY_{\mathcal{I}}^{(\Pi_n)}] - \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert_{\text{op}}}{s_{r_n} - s_{r_n + 1}}  \nonumber \\
& \leq \frac{2\sqrt{|\mathcal{I}||\Pi_n|}\lVert \E[\bY_{\mathcal{I}}^{(\Pi_n)}] - \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert_{\text{max}}}{s_{r_n} - s_{r_n + 1}} \nonumber  \\
& = \frac{2\sqrt{|\mathcal{I}||\Pi_n|}\Delta_E}{s_{r_n} - s_{r_n + 1}} \nonumber \\ 
& = \frac{2\sqrt{|\mathcal{I}||\Pi_n|}\Delta_E}{s_{r_n}} \label{eq:projection_operator_error_operator_bound}
\end{align}
\noindent where the last equality follows from the fact that  $\text{rank}(\E[\bY_{\mathcal{I}}^{\Pi_n}]) = r_n$, hence $s_{r_n + 1} = 0$. Now, plugging Assumption \ref{ass:balanced_spectrum}, \eqref{eq:projection_operator_error_operator_bound}  into \eqref{eq:projection_operator_error_cauchy} gives us
\begin{equation}
\label{eq:projection_operator_error_bound_simplified}
    \max\left(\left\lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right\rVert_{\text{op}}, \left\lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right\rVert_{\text{op}} \right) \leq C\sqrt{r_n}\Delta_E 
\end{equation}
Substituting the result of Proposition \ref{prop:pcr_convergence_rate} and \eqref{eq:projection_operator_error_bound_simplified} into \eqref{eq:projection_operator_error_cauchy} gives us
\begin{equation}
\label{eq:projection_operator_term1_bound}
      \left \lVert \left(\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right)\Delta^n_w \right \rVert_2 = O_p \left(\log^3\left(|\Pi_n||\mathcal{I}|\right) r_n^{3/2}\left[\frac{\lVert \Tilde{\bw}^{n} \rVert_2 \Delta_E  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \Delta^2_E \right] \right)
\end{equation} \\

\noindent To further simplify \eqref{eq:projection_operator_term1_bound}, we substitute the result of Lemma \ref{lem:l1_bound_w_tilde}

\noindent Substituting the Lemma \ref{lem:l1_bound_w_tilde} into \eqref{eq:projection_operator_term1_bound} gives us
\begin{equation}
\label{eq:projection_operator_term1_bound_simplified}
      \left \lVert \left(\mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}} - \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}  \right)\Delta^n_w \right \rVert_2 = \Tilde{O_p} \left( r_n^{2}\left[\frac{\Delta_E  }{\sqrt{|\mathcal{I}|}\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \Delta^2_E \right] \right)
\end{equation}





\noindent  \emph{Bounding Term 2.}  We introduce some necessary notation required to bound the second term. 
Let $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] = \sum^{r_n}_{l=1}\hat{s}_l\hat{\mu}_l\hat{v}'_l$ denote the $r_n$ decomposition of $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$. Let, $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] = \hat{\bU}_{\mathcal{I}}^{(\Pi_n)}\hat{\bS}_{\mathcal{I}}^{(\Pi_n)}(\hat{\bV}_{\mathcal{I}}^{(\Pi_n)})^T$. 
%
Further, define $\bepsilon^{\Pi_n} = [\epsilon_n^{\pi}: \pi \in \Pi_n] \in \R^{|\Pi_n|}$. \\
%
\noindent Then to begin, note that since $\hat{\bV}^{(\Pi_n)}_{\mathcal{I}}$ is a isometry, we have that
\begin{equation*}
    \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w  \rVert^2_2  = \lVert (\hat{\bV}^{(\Pi_n)}_{\mathcal{I}})^T\Delta^n_w \rVert^2_2
\end{equation*}

\noindent To upper bound $\lVert (\hat{\bV}^{(\Pi_n)}_{\mathcal{I}})^T \Delta^n_w \rVert^2_2$ as follows, consider 
\begin{equation*}
   \lVert \hat{\E}[\bY^{(\Pi_n)}_{\mathcal{I}}] \Delta^n_w\rVert^2_2 = \left( (\hat{\bV}^{(\Pi_n)}_{\mathcal{I}})^T\Delta^n_w \right)^T \left(\hat{\bS}_{\mathcal{I}}^{(\Pi_n)}\right)^2 \left( (\hat{\bV}^{(\Pi_n)}_{\mathcal{I}})^T \Delta^n_w \right) \geq \hat{s}^2_{r_n} \lVert (\hat{\bV}^{(\Pi_n)}_{\mathcal{I}})^T \Delta^n_w  \rVert^2_2
\end{equation*}

\noindent Using the two equations above gives us
\begin{equation}
\label{eq:projection_term2_intermediate_bound1}
    \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w  \rVert^2_2   \leq \frac{ \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \Delta^n_w\rVert^2_2}{\hat{s}^2_{r_n}}
\end{equation}

\noindent To bound the numerator in \eqref{eq:projection_term2_intermediate_bound1}, note that  by definition $\E[\bY^{(\Pi_n)}] = \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \Tilde{\bw}^n$. Using this observation, we have
\begin{align}
    \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \Delta^n_w\rVert^2_2 & \leq 2 \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2 + 2\lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \tilde{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2 \nonumber \\
    & = 2\lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2 + 2\lVert (\hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}])\tilde{\bw}^n  \rVert^2_2\label{eq:term2_numerator_bound}
\end{align} 
\noindent To proceed, we then use the following inequality: for any $\mathbf{A} \in \mathbb{R}^{a \times b}, v \in \mathbb{R}^b$, we have
\begin{equation}
\label{eq:l2_infty_inequality}
    \lVert \mathbf{A}v \rVert_2 = \lVert \sum^b_{j =1} \mathbf{A}_{.j} v_j \rVert_2 \leq \left( \max_{j \leq b} \lVert \mathbf{A}_{.j} \rVert_2 \right) \left( \sum^b_{j=1} v_j \right) = \lVert \mathbf{A} \rVert_{2,\infty} \lVert v \rVert_1
\end{equation} 
\noindent Substituting \eqref{eq:term2_numerator_bound} into \eqref{eq:projection_term2_intermediate_bound1} and then applying inequality \eqref{eq:l2_infty_inequality} gives us  
\begin{equation}
\label{eq:projection_term2_intermediate_bound2}
     \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w  \rVert^2_2  \leq \frac{2}{\hat{s}^2_{r_n}}\left(\lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2  + \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert^2_{2,\infty} \lVert \tilde{\bw}^n \rVert^2_1\right)
\end{equation}
Next, we bound $\lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2$. To this end, note that Assumption \ref{ass:observation_model} implies that
\begin{align}
     &\lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \bY^{(\Pi_n)} \rVert^2_2 \nonumber \\
     & = \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] - \bepsilon^{\Pi_n} \rVert^2_2 \nonumber \\
    & =  \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2 + \lVert \bepsilon^{\Pi_n} \rVert^2_2 - 2\langle  \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}], \bepsilon^{\Pi_n} \rangle \label{eq:pcr_training_error_bound_noiseless}
\end{align}
\noindent Next, we proceed by calling upon Property 4.1 of \cite{agarwal2020principal} which states that $\hat{\bw}^n$ as given by \eqref{eq:pcr_linear_model_def} is the unique solution to the following convex program: 
\begin{align}
\label{eq:hat_w_min_l2_property}
& \min_{\bw \in \mathbb{R}^{|\mathcal{I}|}} \quad  ||{\bw}||_2 \nonumber \\
& \textrm{s.t. } \bw \in \argmin_{\bw \in \mathbb{R}^{|\mathcal{I}|}} \lVert \bY^{(\Pi_n)} - \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}]\bw \rVert^2_2
\end{align}
Using this property, we have that
\begin{align}
    & \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \bY^{(\Pi_n)} \rVert^2_2 \nonumber 
    \\ & \leq   \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \tilde{\bw}^n - \bY^{(\Pi_n)} \rVert^2_2 \nonumber \\
    & = \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \tilde{\bw}^n - \E[\bY^{(\Pi_n)}] - \bepsilon^{\Pi_n}  \rVert^2_2 \nonumber\\
    & = \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \tilde{\bw}^n - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]\tilde{\bw}^n  - \bepsilon^{\Pi_n}  \rVert^2_2 \nonumber \\
    & = \lVert (\hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}])\tilde{\bw}^n \rVert^2_2 + \lVert \bepsilon^{\Pi_n} \rVert^2_2 - 2 \langle (\hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}])\tilde{\bw}^n, \bepsilon^{\Pi_n} \rangle \label{eq:pcr_training_error_bound}
\end{align}

\noindent Substituting \eqref{eq:pcr_training_error_bound_noiseless} and \eqref{eq:pcr_training_error_bound} into \eqref{eq:projection_term2_intermediate_bound2} and using \eqref{eq:l2_infty_inequality}, we get
\begin{align*}
& \lVert \hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}] \hat{\bw}^n - \E[\bY^{(\Pi_n)}] \rVert^2_2 \\
& \leq \lVert (\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),r_n}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}])\tilde{\bw}^n \rVert^2_2 + 2 \langle (\hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}])\Delta_w^n, \bepsilon^{\Pi_n} \rangle \\
& \leq \lVert (\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),r_n}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}])\rVert^2_{2,\infty} \lVert \tilde{\bw}^n  \rVert^2_1 + 2 \langle (\hat{\E}[\bY^{(\Pi_n),r_n}_{\mathcal{I}}])\Delta_w^n, \bepsilon^{\Pi_n} \rangle 
\end{align*}
    
\noindent Then substituting this equation into \eqref{eq:projection_term2_intermediate_bound2} gives us 
\begin{equation}
\label{eq:projection_operator_term2_intermediate_bound}
   \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rVert^2_2 \leq \frac{4}{\hat{s}^2_{r_n}}\left(\lVert \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert^2_{2,\infty} \lVert \Tilde{\bw}^n \rVert^2_1 +  \langle  \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] \Delta^n_w, \bepsilon^{\Pi_n} \rangle \right)
\end{equation}
We state three lemmas that help us bound the equation above with their proofs given in Appendices \ref{subsubsec:proof_singular_value_lower_bound}, \ref{subsubsec:proof_of_lemma_rank_r_approximation_l2_infty_bound} and \ref{subsubsec:proof_of_lemma_rank_r_approximaton_noise_inner_product_bound} respectively. 
\begin{lemma}
\label{lem:singular_value_lower_bound}
Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. Then, 
\begin{equation*}
    \hat{s}_{r_n} - s_{r_n} = O_p(1)
\end{equation*}
\end{lemma}

\begin{lemma}
\label{lem:rank_r_approximation_l2_infty_bound}
Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. Then for a universal constant $C > 0$, we have 
\begin{equation}
   \lVert \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert_{2,\infty} \leq C \sqrt{r_n|\Pi_n|} \Delta_E
\end{equation}
\end{lemma}

\begin{lemma} Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. Then, 
\label{lem:rank_r_approximaton_noise_inner_product_bound}
 \begin{equation}
  \langle  \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] \Delta^n_w, \bepsilon^{\Pi_n} \rangle = O_p \left(\sqrt{|\Pi_n|} + r_n +  \lVert \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert_{2,\infty} \lVert \Tilde{\bw}^n \rVert_1 \right)
\end{equation}
\end{lemma}
\noindent Using the results of three lemmas above, applying Assumption \ref{ass:balanced_spectrum} in \eqref{eq:projection_operator_term2_intermediate_bound}, and then simplifying gives us 
\begin{equation}
\label{eq:projection_operator_term2_bound}
    \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rVert_2  = O_p \left(\frac{r_n\Delta_E\lVert \Tilde{\bw}^n \rVert_1}{\sqrt{|\mathcal{I}|}} + \frac{r_n\left( 1 + \sqrt{\Delta_E\lVert \Tilde{\bw}^n \rVert_1}\right)}{\sqrt{|\mathcal{I}|}|\Pi_n|^{1/4}}\right)
\end{equation}
\noindent This concludes the proof for term 2. Using the result of Lemma \ref{lem:l1_bound_w_tilde}, we have that $\lVert \Tilde{\bw}^n \rVert_1 \leq \sqrt{r_n}$. Substituting this into \eqref{eq:projection_operator_term2_bound} gives us
\begin{equation}
\label{eq:projection_operator_term2_bound_simplified}
    \lVert \mathcal{P}_{\hat{V}_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rVert_2  = O_p \left(\frac{r_n^{3/2}\Delta_E}{\sqrt{|\mathcal{I}|}} + \frac{r_n\left( 1 + \sqrt{\Delta_E}r^{1/4}_n\right)}{\sqrt{|\mathcal{I}|}|\Pi_n|^{1/4}}\right)
\end{equation}

\noindent \emph{Collecting Terms. }Combining the results of \eqref{eq:projection_operator_term1_bound_simplified} and \eqref{eq:projection_operator_term2_bound_simplified}, gives us
\begin{align*}
& \lVert \mathcal{P}_{V_{\mathcal{I}}^{(\Pi_n)}}\Delta^n_w \rVert_2 \nonumber \\
& = \tilde{O_p} \left( r_n^{2}\left[\frac{\Delta_E  }{\sqrt{|\mathcal{I}|}\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \Delta^2_E \right]  + \frac{r_n^{3/2}\Delta_E}{\sqrt{|\mathcal{I}|}} + \frac{r_n\left( 1 + \sqrt{\Delta_E}r^{1/4}_n\right)}{\sqrt{|\mathcal{I}|}|\Pi_n|^{1/4}}  \right) 
\end{align*}

\subsubsection{Proof of Lemma \ref{lem:singular_value_lower_bound}}
\label{subsubsec:proof_singular_value_lower_bound}
We first state Weyl's inequality which will be useful for us to establish the results. 
\begin{theorem}[Weyl's Inequality]
\label{thm:weyl_inequality}
Given two matrices $\mathbf{A},\mathbf{B} \in \R^{m \times n}$, let $s_i$ and $\hat{s}_i$ denote the $i$-th singular values of $\mathbf{A}$ and $\mathbf{B}$ respectively. Then, for all, $i \leq \min\{n,m\}$, we have $\left| s_i - \hat{s}_i \right| \leq \left\lVert\mathbf{A} - \mathbf{B} \right\rVert_{\text{op}}$
\end{theorem}
\noindent Using Weyl's inequality gives us 
\begin{align*}
    | \hat{s}_{r_n} - s_{r_n} | & \leq \lVert \E[\bY_{\mathcal{I}}^{(\Pi_n)}] - \hat{E}[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert_{\text{op}} \\
    & \leq \sqrt{|\mathcal{I}||\Pi_n|}\Delta_E
\end{align*}
\noindent Using the inequality above and Assumption \ref{ass:balanced_spectrum}, we have 
\begin{align*}
\hat{s}_{r_n} & \geq s_{r_n} - \sqrt{|\mathcal{I}|\Pi_n}\Delta_E \\
        & = s_{r_n}\left(1 - \frac{\sqrt{|\mathcal{I}|\Pi_n}\Delta_E}{s_{r_n}}\right) \\
        & \geq s_{r_n}\left(1 - \sqrt{r_n}\Delta_E \right) \\
\end{align*}
Then, substituting \eqref{eq:delta_E_subsitution} into the equation above gives us that
\begin{equation*}
    \frac{\hat{s}_{r_n}}{s_{r_n}} \geq \left(1 - C\sqrt{\frac{r_ns^2p}{M}}\right)
\end{equation*}
holds with high probability for some universal constant $C \geq 0$. Finally, using the assumption that $M = \omega(r_ns^2p)$ yields the claimed result. 
    

\subsubsection{Proof of Lemma \ref{lem:rank_r_approximation_l2_infty_bound}}
\label{subsubsec:proof_of_lemma_rank_r_approximation_l2_infty_bound}
For notational simplicity, let $\hat{\bU}_{r_n},\hat{\bS}_{r_n},\hat{\bV}_{r_n}$ denote $\hat{\bU}_{\mathcal{I}}^{(\Pi_n)},\hat{\bS}_{\mathcal{I}}^{(\Pi_n)},\hat{\bV}_{\mathcal{I}}^{(\Pi_n)}$ respectively.  For a matrix $\mathbf{A}$, let $A_{.,j}$ denote its $j$-th column. Additionally, denote  $\Delta_{j} = \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}$. Then,
\begin{align*}
    & \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \\
    & = \left(\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right) + \left(\hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}  - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right)
\end{align*}
\noindent We have that $\left(\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right)$ belongs to the subspace spanned by the column vectors of $\hat{U}_{r_n}$, while $\left(\hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}  - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right)$ belongs to its orthogonal complement. Therefore, 
\begin{align*}
     & \lVert \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \rVert^2_2 \\
      & = \left\lVert\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2 + \left\lVert\hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}  - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2
\end{align*}

\noindent \emph{Bounding $\left\lVert\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2$}. 
Observe that, we have 
\begin{align*}
    \hat{\bU}_{r_n}\hat{\bU}^T_{r_n}\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} & =   \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]\mathbf{e_{j}}  = \hat{\bU}_{r_n}\hat{\bU}^T_{r_n}\hat{\bU}_{\mathcal{I}}^{(\Pi_n)}\hat{\bS}_{\mathcal{I}}^{(\Pi_n)}\hat{\bV}_{\mathcal{I}}^{(\Pi_n)} \mathbf{e_{j}} \\
    & =  \hat{\bU}_{r_n}\hat{\bS}_{r_n}\hat{\bV}^T_{r_n}\mathbf{e}_j =    \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j}
\end{align*}
\noindent Therefore, we have
\begin{align*}
    \left\lVert\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}]_{.,j} - \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2  & = \left\lVert\hat{\bU}_{r_n}\hat{\bU}^T_{r_n}\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} - \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2 \\
    & \leq \left\lVert\hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \right\rVert^2_2 \left \lVert \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}\right\rVert^2_2 \\ 
    & \leq |\Pi_n|\Delta^2_E
\end{align*} 

\noindent \emph{Bounding $\left\lVert\hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}  - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2$.} Note that $ \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} = \bU_{\mathcal{I}}^{(\Pi_n)}\left(\bU_{\mathcal{I}}^{(\Pi_n)}\right)^T\E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}$. Using Assumption \ref{ass:boundedness_potential_outcome} and \eqref{eq:projection_operator_error_bound_simplified}, we have 
\begin{align*}
    \left\lVert\hat{\bU}_{r_n}\hat{\bU}^T_{r_n} \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j}  - \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \right\rVert^2_2 & \leq \left\lVert \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} -  \bU_{\mathcal{I}}^{(\Pi_n)}\left(\bU_{\mathcal{I}}^{(\Pi_n)}\right)^T \right\rVert^2_2 \lVert \E[\bY_{\mathcal{I}}^{(\Pi_n)}]_{.,j} \rVert^2_2 \\
    & \leq \left\lVert \hat{\bU}_{r_n}\hat{\bU}^T_{r_n} -  \bU_{\mathcal{I}}^{(\Pi_n)}\left(\bU_{\mathcal{I}}^{(\Pi_n)}\right)^T \right\rVert^2_2 \left|\Pi_n \right| \\
    & \leq C r_n |\Pi_n| \Delta^2_E
\end{align*}
\noindent for a universal constant $C > 0$. 
\noindent Combining the two bounds gives us, 
\begin{equation*}
    \max_{j} \ \Delta_j \leq C\sqrt{r_n|\Pi_n|}\Delta_E
\end{equation*}
\noindent for a universal constant $C > 0.$

\subsubsection{Proof of Lemma \ref{lem:rank_r_approximaton_noise_inner_product_bound}}
\label{subsubsec:proof_of_lemma_rank_r_approximaton_noise_inner_product_bound}

    
The proof of Lemma \ref{lem:rank_r_approximaton_noise_inner_product_bound} follows from adapting the notation of Lemma H.5 in \cite{agarwal2020synthetic} to that used in this paper. Specifically, we have the notational changes established in Table \ref{tab:notational_changes}, as well as the following changes: $r_{pre} = r_n, \bY_{pre,\mathcal{I}^{(d)}}^{r_{pre}} = \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}], \E[\bY_{pre,\mathcal{I}^{(d)}}], \epsilon_{pre,n} = \bepsilon^{\Pi_n}, T_0 = \Pi_n$  where the l.h.s of each equality is the notation used in \cite{agarwal2020synthetic} , and the r.h.s is the notation used in this work. Using the notation established, we can use the result of Lemma H.5 in \cite{agarwal2020synthetic} to give us 
\begin{equation*}
      \langle  \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] \Delta^n_w, \bepsilon^{\Pi_n} \rangle = O_p \left(\sqrt{|\Pi_n|} + r_n +  \lVert \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n),{r_n}}] - \E[\bY_{\mathcal{I}}^{(\Pi_n)}] \rVert_{2,\infty} \lVert \Tilde{\bw}^n \rVert_1 \right)
\end{equation*}
which completes the proof. 



\subsection{Proof of Proposition \ref{prop:pcr_convergence_rate}}
\label{subsec:pcr_proofs}
We prove Proposition \ref{prop:pcr_convergence_rate}, which also serves as the finite-sample analysis of the vertical regression procedure (step 2 of \method) done via principal component regression. The proof follows from adapting the notation of Proposition E.3 in \cite{agarwal2021causal} to that used in this paper. Specifically, we present table \ref{tab:notational_changes} that matches their notation to  ours. %\begin{center}
\begin{table}[htbp]
\centering
\begin{tabular}{||c c||} 
 \hline
 Notation of \cite{agarwal2021causal} & Our Notation \\ [1ex] 
 \hline
 $\bY$ & $\bY_{\Pi_n}$  \\ [1ex]
 \hline
 $\bX$ & $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$ \\  [1ex]
 \hline
 $\bZ$ &  $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$  \\ [1ex]
 \hline 
 $n$ & $|\Pi_n|$ \\
 \hline 
 $p$ & $|\mathcal{I}|$ \\
 \hline 
$\bbeta^*$ & $\Tilde{\bw}$ \\
\hline
$\hat{\bbeta}$ & $\hat{\bw}$ \\
\hline
$\Delta_E$ & $\Delta_E$ \\
\hline 
$r$ & $r_n$ \\
\hline 
$\phi^{lr}$ & $0$ \\ [1ex]
 \hline
 $\Bar{A}$ & $1$ \\ 
 \hline
 $\Bar{K}$ & 0  \\ [1ex] 
 \hline
 $K_{a}, \kappa, \Bar{\sigma}$ & $C\sigma$ \\
 \hline 
 $\rho_{min}$ & 1 \\
 \hline 
 \end{tabular}
 \vspace{1mm}
\caption{A summary of the main notational differences between our setting and that of \cite{agarwal2021causal}.}
\label{tab:notational_changes}
\end{table}
%\end{center}

Using the table above, we have that the following holds with probability $1 - O\left(\left(|\mathcal{I}||\Pi_n|\right)^{-10}\right)$ 
\begin{equation*}
    \lVert \hat{\bw}^n - \Tilde{\bw}^n \rVert_2 \leq C\sigma^3\log^3\left(\left(|\mathcal{I}||\Pi_n|\right)\right)r_n\left[\frac{\lVert \Tilde{\bw}^{n} \rVert_2  }{\min\{\sqrt{|\Pi_n|},\sqrt{|\mathcal{I}|\}}}  + \Delta_E \right]
\end{equation*}

