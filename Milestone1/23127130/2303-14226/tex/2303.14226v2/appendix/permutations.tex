\section{Extension to Permutations}
\label{sec:permutations}
In this section, we show how we can extend our causal framework to estimate potential outcomes under different permutations of $p$ items, i.e., rankings. 
%
As discussed earlier, learning to rank has been widely studied and is of great practical interest in a number of important practical applications such as personalizing results on search engines or matching markets. 
%
To show how \method~can be adapted to permutations, we first show how  functions over permutations can be re-expressed as Boolean functions.
%
As a result, we establish that functions over permutations admit a Fourier representation as shown in Section \ref{sec:model}.
%
Then, we adapt our generating model established for combinations in Section \ref{sec:model} to permutations.  
%
Next, we discuss how our theoretical guarantees can be adapted to this setting as well. 
%

\subsection{Fourier Expansion of Functions of Permutations}
\label{subsec:fourier_series_permutations}
We show how we can represent functions of permutations as Boolean functions. 
%
We had discussed how permutations can be represented as binary vectors in Section \ref{sec:model}, but repeat it here for completeness.

\noindent \textbf{Binary Representation of Permutations.} 
%
Let $\tau: [p] \rightarrow [p]$ denote a permutation on a set of $p$ items such that $\tau(i)$ denotes the rank of item $i \in [p]$.  
%
There are a total of $p!$ different rankings, and we denote the set of permutations by $\mathbb{S}_p$.
%
Every permutation $\tau \in \mathbb{S}_p$ induces a binary representation $\mathbf{v}(\tau) \in \{-1,1\}^{\binom{p}{2}}$, which can be constructed as follows.
%
For an item $i$, define  $\mathbf{v}^{i}(\tau) \in \{-1,1\}^{p-i}$ as follows: $\mathbf{v}_j^{i}(\tau)= \mathbbm{1}\{ \tau(i) > \tau(j)\} - \mathbbm{1}\{ \tau(i) < \tau(j)\}$ for items $1 \leq i  <  j \leq p$.
%
That is, each coordinate of $\mathbf{v}^{i}(\tau)$ indicates whether items $i$ and $j$ have been swapped for items $j > i$. 
%
Then, $\mathbf{v}(\tau) = [\mathbf{v}^{i}(\tau): i \in [p]] \in \{-1,1\}^{\binom{p}{2}}$.
%
For example, with $p = 4$, the permutation $\tau([1,2,3,4]) = [1,3,4,2]$ has the binary representation $\mathbf{v}(\tau) = (-1,-1,-1,1,1,-1)$.
%

\noindent \textbf{Fourier Expansion of Functions of Permutations.}
Since a permutation $\tau$ can be expressed as a binary vector, any function $f: \mathbb{S}_p \rightarrow \mathbb{R}$ can be thought of as a Boolean function. 
%
Then, given the discussion on Fourier expansions of Boolean functions in Section \ref{sec:notation}, we have that any function $f: \mathbb{S}_p \rightarrow \mathbb{R}$ admits the following Fourier decomposition:
%
$f(\tau) = \sum_{S \subset [\binom{p}{2}]} \alpha_S \chi_{S}(\mathbf{v}(\tau)) \coloneqq \langle  \balpha_f, \bchi^{\tau} \rangle$, where $\balpha_f = [\alpha_S]_{S \in [\binom{p}{2}]} \in \mathbb{R}^{\binom{p}{2}}$, and $\bchi^{\tau} = [\chi_S(\mathbf{v}(\tau)]_{S \in [\binom{p}{2}]} \in \{-1,1\}^{\binom{p}{2}}$ for $\tau \in \mathbb{S}_p$.
%

%
\subsection{Model}
\label{subsec:model_permutations}

In this section we show how we can adapt our notation for observed  and potential outcomes as well as our model to the permutation setting.

\noindent \textbf{Notation, and Outcomes.} Let $Y_n^{(\tau)}$ denote the potential outcome for a unit $n$ under permutation $\tau$, and $Y_{n\tau} \in \{\mathbb{R} \cup \star \}$ denote an observed value, where $\star$ denotes an unobserved entry. 
%
For a given unit $n$ and subset $G \subseteq \mathbb{S}_p$, let $\bY_{G,n} = [Y_{n\tau_i} : \tau_i \in G] \in \{\mathbb{R} \cup \star \}^{|G|}$ represent the vector of observed outcomes. 
%
Similarly, let $\bY_n^{(G)} = [Y^{(\tau_i)}_{n} : \tau_i \in G] \in \mathbb{R}^{|G|}$ represent the vector of potential outcomes.
%
Denote $\bchi(G) = [\bchi^{\tau_i} : \tau_i \in G] \in \{-1, 1\}^{|G| \times 2^{\binom{p}{2}}}$.
%
Let $\mathcal{D} \subset [N] \times [\mathbb{S}_p],$ refer to the subset of unit-combination pairs we do observe, i.e., 
%
\begin{equation}\label{eq:SUTVA_permutation}
Y_{n\tau} = 
%
\begin{cases}
%
Y^{(\tau)}_n, & \text{if $(n, \tau) \in \mathcal{D}$}\\
%
\star, & \text{otherwise}.
%
\end{cases}
%
\end{equation}
%
Note that \eqref{eq:SUTVA_permutation} implies stable unit treatment value assignment (SUTVA)  holds. 
%
Further, for a unit $n \in [N]$, denote the subset of permutations we observe them under as $G_n \subseteq \mathbb{S}_p$. 
%
Our target causal parameter is the potential outcome $\E[Y_n^{(\tau)}]$ for all $N
\times p!$ unit-permutation pairs. 

\noindent \textbf{Model and DGP.} We propose a similar model to the one discussed for combinations in Section \ref{sec:model}. That is, we model $Y_n^{(\tau)} = \langle \balpha_n, \bchi^{\tau} \rangle + \epsilon_n^{\tau}$, where we assume sparsity ($||\balpha||_0 = s \leq 2^{\binom{p}{2}}$), low-rank structure ($\text{rank}(\mathcal{A}) = r \in \{\min\{N,2^{\binom{p}{2}}\}\}$), and $\E[\epsilon_n^{\tau} ~|~ \mathcal{A}] = 0$. 
%
Additionally, the DGP for permutations is identical to that for combinations discussed in Section \ref{sec:model}.


\subsection{Synthetic Permutations Estimator}
\label{subsec:synth_combo_permutations}

We introduce the Synthetic Permutations Estimator, which is identical to \method~estimator, where we first perform horizontal regression for the donor set, and then transfer these outcomes to non-donor units via PCR. 
%
Specifically, we detail the two steps of the Synthetic Permutations estimator as follows. 

\emph{Step 1: Horizontal Regression.} For every donor unit $u$, we run a Lasso regression (or fit a ML model of choice) on the outcomes $\bY_{u,G_u} = [Y_{u,\tau_u}: \tau_u \in G_u] \in \mathbb{R}^{|G_u|}$ against the Fourier characteristics $\bchi(G_u)  \in \mathbb{R}^{|G_u| \times 2^{\binom{p}{2}}}$. 
%
For any unit $n$, we denote $\bY_{n,G_n}$ as $\bY_{G_n}$
%
Then, we solve the following convex program with penalty parameter $\lambda_u$: 
\begin{align}\label{eq:Lasso_estimator_permutation}
 \hat{\balpha}_u=  \argmin_{\balpha} \ \frac{1}{|G_u|}\lVert \bY_{G_u} - \bchi(G_u)\balpha \rVert^2_2 + \lambda_u \lVert \balpha \rVert_1
\end{align}
%
For any donor unit-permutation pair $(u,\tau)$, let $\hat{\E}[Y_u^{(\tau)}] = \langle  \hat{\balpha}_u, \bchi^{\tau} \rangle$ denote the estimate of the potential outcome $\E[Y_u^{(\tau)}]$.

% 
\emph{Step 2: Vertical Regression.} Before we detail the second step for permutations, we first define some notation: for $G \subseteq \mathbb{S}_p$, let the vector of estimated potential outcomes $\hat{\E}[\bY^{(G)}_{u}] = [ \hat{\E}[Y_u^{(\tau)}]: \tau \in G] \in \R^{|G|}$. 
%
Additionally, let $\hat{\E}[\bY^{(G)}_{\mathcal{I}}] = [\hat{\E}[\bY^{(G)}_{u}]: u \in \mathcal{I}] \in \mathbb{R}^{|G| \times |\mathcal{I}|}$.
%

\vspace{2mm}
\emph{Step 2(a): Principal Component Regression.} Perform a singular value decomposition (SVD) of $\hat{\E}[\bY^{(G_n)}_{\mathcal{I}}]$ to get $\hat{\E}[\bY^{(G_n)}_{\mathcal{I}}] = \sum^{\min(|G_n|,|\mathcal{I}|)}_{l = 1} \hat{s_l}\hat{\bmu}_{l}\hat{\bnu}^T_{l}$. Using a hyper-parameter $\kappa_n \leq \min(|G|,|\mathcal{I}|)$, compute $\hat{\bw}^{n} \in \mathbb{R}^{|\mathcal{I}|}$ as follows:
\begin{equation}
\label{eq:pcr_linear_model_def_permutation}
    \hat{\bw}^{n} = \left(\sum^{\kappa_n}_{l = 1} \hat{s_l}^{-1}\hat{\bnu}_{l}\hat{\bmu}^T_{l}\right)\bY_{G_n} 
\end{equation}

\emph{Step 2(b): Estimation.} Using $\hat{\bw}^{n} = [\hat{w}_u^n : u \in \mathcal{I}]$, we have the following estimate for any intervention $\pi \in \Pi$
\begin{equation}
\label{eq:potential_outcome_estimate_vertical_regression_permutation}
     \hat{\E}[Y_n^{(\tau)}] = \sum_{u \in \mathcal{I}} \hat{w}_u^{n} \hat{\E}[Y_u^{(\tau)}]
\end{equation}
%

\subsection{Theoretical Results}
We discuss how we can adapt our key theoretical results, in particular identification, and finite-sample consistency of Synthetic Permutations.
%
In particular, all of our key theoretical results for combinations are easily adapted under the appropriate change in notation. 
%
We present the following table that summarizes the changes in the notation from combinations to permutations.
%

\begin{table}[H]
\centering
\begin{tabular}{||c c||} 
 \hline
 Combinations & Permutations \\ [1ex] 
 \hline
 $\pi$ & $\tau$ \\ 
 \hline
  $\Pi$ & $\mathbb{S}_p$  \\ [1ex]
 \hline
 $\Pi_n$ & $G_n$ \\  [1ex]
 \hline
  $\E[Y_n^{(\pi)}]$ &  $\E[Y_n^{(\tau)}]$\\
 \hline
 $Y_n^{(\pi)}$ &  $Y_n^{(\tau)}$  \\ [1ex]
 \hline 
 $Y_{n\pi}$ & $Y_{n\tau}$ \\
 \hline 
 $Y_n^{(\Pi)}$ & $Y_n^{(G)}$ \\
 \hline 
$Y_{\Pi_n}$ & $Y_{G_n}$ \\
\hline 
$p$ & $\binom{p}{2}$ \\
 \hline 
 \end{tabular}
 \vspace{1mm}
\caption{Notational substitutions between combinations and permutations.}
\label{tab:notational_changes_permutation_to_combination}
\end{table}

\noindent Given this summary of notational changes, we now present our identification, and finite-sample consistency results. 

\subsubsection{Identification}
\label{subsec:identification_permutation}

\begin{theorem}
\label{thm:identification_permutation}

Let  Assumptions \ref{ass:observation_model},  \ref{ass:selection_on_fourier}, \ref{ass:donor_set_identification} hold where we make the notational changes presented in Table \ref{tab:notational_changes_permutation_to_combination}. Then, the following hold.


\noindent (a) Donor units: For $u \in \mathcal{I}$, and $\tau \in \mathbb{S}_p$, 
$
 \E[Y^{(\tau)}_{u} ~ | ~ \mathcal{A}]  =  \sum_{\tau_u \in G_{u}} \beta_{\tau_{u}}^{\tau}
 \E[Y_{u,\tau_{u}} \ | \  \mathcal{A}, \ \mathcal{D}].
$

\noindent
(b) Non-donor units: For $n \in [N] \setminus \mathcal{I}$, and $\tau \notin G_n$,
$
\E[Y^{(\tau)}_{n} ~ | ~ \mathcal{A}] = \sum_{u \in \mathcal{I},\tau_u \in G_u}w_{u}^{n}  \beta_{\tau_{u}}^{\tau}  \E[Y_{u,\tau_u} \ | \  \mathcal{A}, \ \mathcal{D}].
$

\end{theorem} 

\noindent Theorem \ref{thm:identification_permutation} establishes  identification for permutations of items for both the donor set and non-donor units.
%
Our identification strategy for permutations is identical to the combination setting, where we first identify outcomes of donor units, and then transfer them to non-donor units. 
%
As with combinations, our result allows for identification even though most permutations are observed for no units. 

\subsubsection{Finite-Sample Consistency and Asymptotic Normality }
\label{subsec:finite_sample_permutations}

First, we present our finite-sample consistency result for Synthetic Permutations when applied to permutations. 
%



\begin{theorem} [Finite Sample Consistency for Permutations]
\label{thm:potential_outcome_convergence_rate_permutations}
Conditioned on $\mathcal{A}$, let Assumptions \ref{ass:observation_model}--\ref{ass:donor_set_identification}, and \ref{ass:boundedness_potential_outcome}--\ref{ass:incoherence} hold with the notational changes presented in Table \ref{tab:notational_changes_permutation_to_combination}. Then, the following statements hold. 
\begin{itemize}
    \item [(a)] For a given donor unit-permutation pair $(u,\tau)$, let the Lasso regularization parameter satisfy $\lambda_u = \Omega(\frac{p^2}{\sqrt{|G_u|}})$. Then, we have that,
        $$
            |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]| = \Tilde{O}_p\left(\frac{sp}{\sqrt{|G_u|}} \right).
        $$
    \item [(b)] Additionally, let Assumptions \ref{ass:balanced_spectrum}, and \ref{ass:rowspace_inclusion} hold under the appropriate notational changes. For the given unit-permutation pair  $(n,\tau)$ where $n \in [N] \setminus \mathcal{I}$, let $\kappa_n = \text{rank}(\E[\bY_{\mathcal{I}}^{(G_n)}]) \coloneqq r_{n}$. Then, provided that  $ \min_{u \in \mathcal{I}} |G_u| \coloneqq M = \omega(r_ns^2p^2)$, we have that,
        $$
            \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right| = \Tilde{O_p}\left(  \frac{r^{2}_nsp}{\sqrt{M}} + \frac{r_n}{|G_n|^{1/4}}\right).
        $$
\end{itemize}
\end{theorem}

\noindent Theorem \ref{thm:potential_outcome_convergence_rate_permutations}
establishes finite-sample consistency of Synthetic Permutations when applied to permutations.
%
The proof follows in an identical fashion to the proof of Theorem \ref{thm:potential_outcome_convergence_rate} under the approriate notational changes. 
%
Following the argument in Section \ref{subsec:sample_complexity_synth_combo}, the sample complexity of Synthetic Permutations to estimate all $N \times p!$ outcomes with accuracy $O(\delta)$ scales as $O(\text{poly}(r/\delta) \times (N + s^2p^2))$.
%
In comparison, the sample complexity of Lasso and Matrix completion algorithms scales as $O(N\times s^2p^2/\delta^2)$ and $O(\text{poly}(r/\delta) \times (N + p!))$ respectively.
%
As compared to the result established for combinations, the sample complexity for permutations is worse by a factor of $p$. 
%
This additional factor of $p$ arises because the binary representation $\bv(\tau) \in \mathbb{R}^{\binom{p}{2}}$ for a permutation $\tau$ as compared to $\bv(\pi) \in \mathbb{R}^{p}$ for a combination $\pi$. 
%
Since the error of the Lasso is proportional to the dimension of the binary encoding, the estimation error for donor units is inflated by a factor of $p$ for permutations as compared to combinations. 
%
As a result, the sample complexity for permutations worse by a factor of $p$. 

%As a result, the Fourier characteristic for combinations $\bchi^{\pi} \in \{-1,1\}^{2^p}$, whereas the Fourier characteristic for permutations $\bchi^{\tau} \in \{-1,1\}^{2^{\binom{p}{2}}}$. 

Next, we present an informal discussion of the sample complexity of Synthetic Permutations with CART when applied to rankings. 
%
Under analogous assumptions to Theorem \ref{thm:CART_convergence_rate} (i.e., $\E[Y_n^{(\tau)}]$ is a $k$-Junta), $\hat{\E}[Y_u^{(\tau)}] - \E[Y_u^{(\tau)}] = \Tilde{O}_p\left(s/\sqrt{|G_u|} \right)$
for every donor unit $u$.
%
That is, like combinations, the estimation error for the horizontal regression with CART is independent of $p$.
%
Then, following the arguments in Sections \ref{subsec:CART_finite_sample} and \ref{subsec:sample_complexity_synth_combo}, the sample complexity of CART  scales as  $O(\text{poly}(r/\delta) \times (N + s^2))$. 
%
This efficiency is a result of CART's ability to exploit the $k$-Junta structure of $\E[Y_n^{(\tau)}]$.
\vspace{1mm}




%Hence, in the Fourier expansion of $f$ , we have that $||\balpha_f||_0 < 2^{\binom{p}{2}}$.
%
%That is, there is redundancy in the Fourier expansion of $f$ since there are subsets $S \subset [\binom{p}{2}]$ where $\alpha_S = 0$.
%
%This redundancy can be overcome using recent work \cite{erginbas2023efficiently} that computes the Fourier transform of functions of permutations directly without one-hot encoding the permutation.



%Specifically, let $G \subset \mathbb{S}_p$ be a subset of permutations, then any function $f :\mathbb{S}_p \rightarrow \mathbb{R}$ has the representation $f(\tau) = \sum_{G \subset \mathbb{S}_p} \alpha_{G}\chi(\mathbf{v}(\tau))$, where $\alpha_{G}$ is the Fourier coefficient, and $\chi(\mathbf{v}(\tau) = \prod_{}$