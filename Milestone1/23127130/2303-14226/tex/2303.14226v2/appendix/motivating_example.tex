\section{Formal Results for Motivating Examples in Section \ref{sec:identification}}
\label{sec:example_proofs}
%\ref{prop:motivating_example}}

In this section, we provide the proofs for both of the motivating examples described in Section \ref{sec:identification}. 

\subsection{Proof of Proposition \ref{prop:motivating_example}}
\label{subsec:proof_experimental_motivating_example}
%\begin{proposition}
%\label{prop:motivating_example}
%Let Assumptions \ref{ass:observation_model} and Assumptions \ref{ass:selection_on_fourier} hold. Moreover, suppose that for some $c > 0$, there are at least $cN/r$ units of each type. Then, under the proposed sampling scheme described above, Assumption \ref{ass:donor_set_identification} is satisfied with probability at least $1 - \gamma$.  
%\end{proposition}

\begin{proof} Let $\mathcal{E}_{H}$, $\mathcal{E}_{L}$ denote the events that horizontal span and linear inclusion holds. We proceed by showing that both $\mathcal{E}_{H}$, $\mathcal{E}_{L}$ hold with high probability. 
\vspace{2mm}

\noindent \emph{Step 1: $\mathcal{E}_H$ high probability bound.} Recall the following notation defined in Section \ref{subsec:experiment_design_lemmas_proof_horizontal_regression}. For a given  unit $u \in \mathcal{I}$, and let $\mathcal{S}_u = \{S \subset [p] ~ | ~ \alpha_{u,S} \neq 0 \}$. That is, $\mathcal{S}_u$ denotes the subset of coordinates where $\balpha_u \in \mathbb{R}^{2^p}$ is non-zero. Note that by Assumption \ref{ass:observation_model},  $|S_u| \leq s$. For any combination $\pi$, let $\bchi_{\mathcal{S}_u}^{\pi} = [\chi^{\pi}_{S} : S \in \mathcal{S}_u] \in \{-1,1\}^{|\mathcal{S}_u|}$ denote the projection of $\bchi^\pi$ to the non-zero coordinates of $\balpha_u$. Let $\bchi_{\mathcal{S}_u}(\Pi_{\mathcal{I}}) = [\bchi_{\mathcal{S}_u}^{\pi}: \pi \in \Pi_{\mathcal{I}}] \in \{-1,1\}^{|\Pi_{\mathcal{I}}| \times |\mathcal{S}_u|}$. Additionally, define $\Tilde{\bchi}_u(\Pi_{\mathcal{I}}) = [\Tilde{\bchi}^{\pi}_u : \pi \in \Pi_{\mathcal{I}}] \in \{-1,1\}^{2^{p}}$ where $\Tilde{\bchi}_u$ is defined in Section \ref{sec:identification}. 
\vspace{2mm}

\noindent Notice that our sampling scheme for assigning combinations to donor units is precisely the same as that described in our experimental design mechanism in Section \ref{sec:experimental_design}.  Using this observation and Equation \eqref{eq:lower_bound_min_singular_value_fourier_characteristic} from the proof of Lemma \ref{lem:experiment_design_horizontal_span_inclusion} gives us that $s_{\text{min}}(\bchi_{\mathcal{S}_u}(\Pi_{\mathcal{I}})) > 0$ with probability at least $1 - 2|\mathcal{S}_u|\cdot\exp(-c|\Pi_{\mathcal{I}}|/4|\mathcal{S}_u|)$ for a given unit $u$. Plugging in our assumption that $|\Pi_{\mathcal{I}}| \geq Cs\log(s|\Pi_{\mathcal{I}}|/\gamma)$ for an appropriate universal constant ensures that 
\begin{equation*}
    \P\left(s_{\text{min}}(\bchi_{\mathcal{S}_u}(\Pi_{\mathcal{I}})) > 0\right) \geq 1 - \gamma/2|\mathcal{I}|
\end{equation*}
\noindent Next, observe that $s_{\text{min}}(\bchi_{\mathcal{S}_u}(\Pi_{\mathcal{I}})) = s_{\text{min}}(\Tilde{\bchi}_{u}(\Pi_{\mathcal{I}}))$. Using this fact shows that horizontal linear span inclusion holds for unit $u$ with probability at least $1 - \gamma/2|\mathcal{I}|$. Taking a union bound over all donor units shows that $\P(\mathcal{E}_H) \geq 1 - \gamma/2$. 
\vspace{2mm}

\noindent \emph{Step 2: $\mathcal{E}_L$ high probability bound.} We begin by defining some notation. Let $N_j$ denote the number of times a unit of type $j$ appears. More formally, we have
\begin{equation*}
    N_j = \sum_{u \in \mathcal{I}} \ \mathbbm{1}[\balpha_{u} = \balpha(j)]
\end{equation*}
Additionally, let $X \sim \text{Bin}(n,p)$ denote a draw from a binomial distribution with parameters $n$ and $p$, where $n$ is the nymber of trials and $p \in [0,1]$ is the success probability of each trial.  Note that $N_j$  is distributed as a Binomial random variable with parameters $|\mathcal{I}|,p_j$ where $p_j$ is the probability of choosing a unit of type $j$. Since each donor unit is drawn independently and uniformly at random, and we assume that there are at least $cN/r$ units of each type, we have that $p_j \geq c/r$. 
\vspace{2mm}

\noindent To show linear span inclusion holds, it suffices to show that $\min_{j \in [r]} N_j > 0$. Note that for a fixed $j \in [r]$, we have
\begin{equation}
\label{eq:motivating_example_zero_type_probability}
    \P(N_j = 0) = (1 - p_j)^{|\mathcal{I}|} \leq (1 - c/r)^{|\mathcal{I}|} \leq \exp(-c|\mathcal{I}|/r)
\end{equation}
where we use the inequality $(1 - x)^a \leq \exp(-ax)$. Next, by Demorgan's law, the union bound and \eqref{eq:motivating_example_zero_type_probability}, we have
\begin{align*}
    \P((\min_{j \in [r]} N_j > 0)^c) & =  \P((\cap^r_{j=1}N_j > 0)^c)  \\
    & =  \P(\cup ^r_{j=1}N_j = 0) \\
    & \leq \sum^r_{j=1} \P(N_j = 0) \\
    & \leq r\exp(-c|\mathcal{I}|/r)
\end{align*}
Plugging in our assumption $|\mathcal{I}| \geq cr\log(r/\gamma)$ into the equation above gives us that 
\begin{equation*}
     \P((\min_{j \in [r]} N_j > 0)^c)  \leq \gamma/2
\end{equation*}
for an appropriately chosen constant $c > 0$. Hence, we have that $\P(\mathcal{E}_L) \geq 1 - \gamma/2$. 
\vspace{2mm}

\noindent \emph{Step 3: Collecting Terms.} Let $\mathcal{E} = \mathcal{E}_H \cap \mathcal{E}_L$. To complete the proof, it suffices to show that $\P(\mathcal{E}) \geq 1 - \gamma$. From steps 1 and 2, we have that $\P(\mathcal{E}_H)$ and $\P(\mathcal{E}_L)$ occur with probability at least $ 1 - \gamma/2$. Hence, we have that
\begin{equation*}
    \P(\mathcal{E}^c) =  \P(\mathcal{E}_H^c \cup \mathcal{E}_L^c) \leq  \P(\mathcal{E}_H^c) + \P(\mathcal{E}_L^c) \leq 1 - \gamma
\end{equation*}
This completes the proof. 
\end{proof}

\subsection{Proofs for Natural Model of Unobserved Confounding}
\label{subsec:proofs_natural_model}

We verify that the under the DGP for the natural model of unobserved confounding described in Section \ref{sec:identification}, units of type $\{\balpha(3),\balpha(4)\}$ do not satisfy horizontal span inclusion. 
%
For units of these type, we observe combinations $\{\pi_1,\pi_2,\pi_3\}$ with the following binary representations $\{v(\pi_1) = (1,1,1,1,\ldots,1),v(\pi_2) = (1,-1,-1,1,\ldots,1), v(\pi_3)\} = (1,-1,1,1,\ldots,1),$. 
%
These combinations have restricted Fourier characteristics $\Tilde{\bchi}^\pi$ as follows: $\{\Tilde{\bchi}^{\pi_1} = (1,1,1,0,\ldots,0), \Tilde{\bchi}^{\pi_2} = (1,-1,-1,0,\ldots,0), \Tilde{\bchi}^{\pi_3} = (1,-1,1,0,\ldots,0)\}$. 
%
It is easy to check that $\Tilde{\bchi}^{\pi} = (1,1,-1,0,\ldots,0)$ does not belong to the span of $\{\Tilde{\bchi}^{\pi_1},\Tilde{\bchi}^{\pi_2},\Tilde{\bchi}^{\pi_3}\}$.
%
Hence horizontal span inclusion does not hold of type $\{\balpha(3),\balpha(4)\}$.




%\begin{lemma} [Linear Span Inclusion]
%\label{lem:experimental_example_linear_span_inclusion}
%Suppose there are $N$ units $n \in [N]$ such that $\balpha_n = \balpha(\lfloor (n-1)/r \rfloor)$ for one of $r$ ``types'' $\{\balpha(1), \balpha(2), \ldots, \balpha(r)\} \subset \mathbb{R}^{2^p}$. Moreover, suppose that for some $\gamma > 0$, there are at least $\gamma N/r$ units of each type. Then, if we sample $m$ units $\{i_1, \ldots, i_m\}$ uniformly at random without replacement, then with probability $r\cdot\exp(-c\gamma m/r)$ (for a universal constant $c \ge 1/64$) it holds 
%\[\min_{1\le j \le r}|\left\{l: \balpha_{i_l} = \balpha(j) \right\}| \ge \frac{\gamma m}{2r} - \sqrt{\frac{16\pi \gamma m}{r}}.\] In particular, the above holds with high probability when $m = \omega(r\log r)$.
%\end{lemma}
%\begin{proof}
%We will begin by lower bounding the number of times a unit of type $j$ appears. Then we will conclude by a union bound.Let $X_j$ denote the number of times a unit of type $j$ appears. We may sample from the law of $X_j$ using the following procedure. Let $s \sim \mathrm{Unif}(\mathbb{S}_N)$ be a uniform random permutation of $[N]$. Then, clearly \[X_j \sim f(s) = \sum_{i=1}^m \mathbbm{1}\{\balpha_{s(i)} = \balpha(j)\}.\]
%Let $\nu$ be the median of $X_j$. We then use the following inequality, which is a conveniently stated version of Talagrand's inequality for the symmetric group \cite[Theorem 5.1]{talagrand1995concentration}, and which we restate in a simplified form.
%\begin{theorem}[{\cite[Theorem 1.1]{mcdiarmid2002concentration}}]
%Let $s \sim \mathrm{Unif}(\mathbb{S}_N)$, and let $f:\mathbb{S}_N \to \mathbb{R}$ satisfy the following conditions.
%\begin{enumerate}
%    \item For any $s,s' \in \mathbb{S}_N$,  such that $s,s'$ differ by at most two indices, then  $|f(s) - f(s')| \leq c$. 
%    \item For all $s' \in \mathbb{S}_N$, if $f(s') = q$ then there exists a ``certificate'' $\mathcal I$ of at most $d \times q$ indices such that $s'(i) = s''(i)$ for all $i \in \mathcal{I}$, then $f(s'') \ge q$. 
%\end{enumerate}
%Then, if $\nu$ is a median of $f(s)$,
%\[\mathbb{P}(f(s) \le \nu - t) \le 2\exp\left\{\frac{-t^2}{16c^2d\nu}\right\}.\]
%\end{theorem}
%It is easy to see that our function $f$ satisfies the conditions above with $c = d = 1$. Transposing two indices can only change the value of $f$ by moving a unit $i$ with $\balpha_i = \balpha(j)$ in or out of the first $m$ units, which changes $f$ by at most $1$. To certify that $f(s) \ge q$, one needs to provide exactly $q$ indices $i$ such that $\balpha_i = \balpha(j)$ and $s^{-1}(i) \le m$. Thus, in our setting, we deduce that 
%\[\mathbb{P}(X_j \le \nu - t) \le 2\exp\left\{\frac{-t^2}{16\nu}\right\}.\]
%Our first task is to transport this inequality to the mean. To this end, we compute
%\begin{align*}
%    \mathbb{E}[\nu-X_j] 
%    &\le \int_{0}^\infty \mathbb{P}(\nu-X_j \ge t)\,dt \\
%    &= \int_{0}^\infty \mathbb{P}(X_j \le \nu - t)\,dt \\
%    &\le \int_{0}^\infty 2\exp\left\{\frac{-t^2}{16\nu}\right\}\,dt \\
%    &= \sqrt{16\pi\nu},
%\end{align*}
%where in the last step we have compared to the Gaussian density with variance $8\nu$.
%Thus $\nu - \mathbb{E}[X_j] \le  \sqrt{16\pi\nu}$.
%We consider cases. If $\nu \ge \gamma m/r$ then we have 
%\[\mathbb{P}(X_j \le \nu - t) \le  2\exp\left\{\frac{-t^2}{16\nu}\right\}.\]
%Choosing $t =  \nu/2$ we find
%\[\mathbb{P}\left\{X_j \le \gamma m/(2r)\right\} \le \mathbb{P}\left\{X_j \le \nu/2\right\} \le 2\exp\left\{\frac{-\nu}{64}\right\} \le 2\exp\left\{\frac{-\gamma m}{64r}\right\}.\]
%If $\nu \le \gamma m/r$ then since $\mathbb{E}[X_j] \ge \gamma m/r$,
%\[\mathbb{P}\left\{ X_j \le \nu - t\right\} \le \mathbb{P}\left\{X_j \le \gamma m/r -  \sqrt{16\pi(\gamma m/r)} - t\right\} \le  2\exp\left\%{\frac{-t^2}{16\nu}\right\} \le 2\exp\left\{\frac{-t^2}{16(\gamma m/r)}\right\}.\]
%Choosing $t =  \gamma m/(2r)$ we find
%\[\mathbb{P}\left( X_j \le \gamma m/(2r) - \sqrt{16\pi(\gamma m/r)}\right) \le 2\exp\left\{\frac{-(\gamma m/(2r))^2}{16(\gamma m/r)}\right\} \le 2\exp\left\{\frac{-\gamma m}{64r}\right\}.\]
%Thus, in either case, 
%\[\mathbb{P}\left(X_j \le \gamma m/(2r) - \sqrt{16\pi(\gamma m/r)}\right) \le 2\exp\left\{\frac{-\gamma m}{64r}\right\}.\]
%By a union bound over $1 \le j \le r$, we conclude  
%\[\mathbb{P}\left(\min_{j\le r} X_j \le \gamma m/(2r) - \sqrt{16\pi(\gamma m/r)}\right) \le  2r\exp\left\{\frac{-\gamma m}{64r}\right\}\] as %needed.
%\end{proof}





%\begin{lemma} [Horizontal Span Inclusion]
%\label{lem:experimental_example_horizontal_span_inclusion}
%Suppose all donor units receive  $\pi_1, \ldots, \pi_q$  combinations of interventions chosen uniformly and independently at random from $\Pi$. Then, horizontal span inclusion holds for every donor unit $u \in \mathcal{I}$
%\end{lemma}

%\begin{proof} Note that the sampling scheme for assigning combinations to the donor unit is precisely the same as that described in our experimental design mechanism in Section \ref{sec:experimental_design}. Using this observation, the proof is immediate from Lemma \ref{lem:experiment_design_horizontal_span_inclusion}. 
%\end{proof}

%\noindent Next, we show that sampling $\omega\left(r \log(r)\right)$ donor units at random satisfies linear span inclusion. In particular, we prove the following stronger result that shows for all $j \in [r]$ unit types, there exists at least $\omega(\log r)$ representatives in the donor set.