
\section{The \method~Estimator}\label{sec:estimator_descripton}
%
We now describe the \method~estimator, a simple and flexible two-step procedure for estimating our target causal parameter. 
%
In Figure \ref{fig:estimator}, we provide a pictorial representation of the estimator.
%

\vspace{2mm}
\noindent \textbf{Step 1: Horizontal Regression.}
For notational simplicity, we denote the vector of observed responses $\bY_{n,\Pi_n} = [Y_{n\pi} : \pi \in \Pi_n] \in \mathbf{R}^{|\Pi_n|}$ for any unit $n$ as $\bY_{\Pi_n}$.
%
Then, for every unit $u$ in the donor set $\mathcal{I}$, we estimate $\E[Y_u^{(\pi)}]$ via the Lasso, i.e., by solving the following convex program with penalty parameter $\lambda_u$: 
\begin{align}\label{eq:Lasso_estimator}
 \hat{\balpha}_u=  \argmin_{\balpha} \ \frac{1}{|\Pi_u|}\lVert \bY_{\Pi_u} - \bchi(\Pi_u)\balpha \rVert^2_2 + \lambda_u \lVert \bm{\alpha} \rVert_1
\end{align}
%
where recall that $\bchi(\Pi_u) = [\bchi^\pi: \pi \in \Pi_u] \in \mathbb{R}^{|\Pi_u| \times 2^p}$.
%
Then, for any donor unit-combination pair $(u,\pi)$, let $\hat{\E}[Y_u^{(\pi)}] = \langle  \hat{\balpha}_u, \bchi^{\pi} \rangle$ denote the estimate of the potential outcome $\E[Y_u^{(\pi)}]$.
%\begin{equation}
%\label{eq:Lasso_predicted_potential_outcome}
%    \hat{\E}[Y_u^{(\pi)}] = \langle  \hat{\balpha}_u, \bchi^{\pi} \rangle, 
%\end{equation}
 %

\vspace{2mm}
\noindent \textbf{Step 2: Vertical Regression.} 
%
Next, we estimate potential outcomes for all units $n \in [N] \setminus \mathcal{I}$.
%
To do so, we define some additional required notation. 
%
For $\Pi_S \subset \Pi$, define the vector of estimated potential outcomes $\hat{\E}[\bY^{(\Pi_S)}_{u}] = [ \hat{\E}[Y_u^{(\pi)}]: \pi \in \Pi^S] \in \R^{|\Pi_S|}$. 
%
Additionally, let $\hat{\E}[\bY^{(\Pi_S)}_{\mathcal{I}}] = [\hat{\E}[\bY^{(\Pi_S)}_{u}]: u \in \mathcal{I}] \in R^{|\Pi_S| \times |\mathcal{I}|}$.
%

\vspace{2mm}
\noindent \emph{Step 2(a): Principal Component Regression.} Perform a singular value decomposition (SVD) of $\hat{\E}[\bY^{(\Pi_n)}_{\mathcal{I}}]$ to get $\hat{\E}[\bY^{(\Pi_n)}_{\mathcal{I}}] = \sum^{\min(|\Pi_n|,|\mathcal{I}|)}_{l = 1} \hat{s_l}\hat{\bmu}_{l}\hat{\bnu}^T_{l}$. Using a hyper-parameter $\kappa \leq \min(|\Pi_n|,|\mathcal{I}|)$\footnote{Both $\lambda$ and $\kappa$ can be chosen in a data-driven manner (e.g., via cross-validation) as discussed in \cite{chetverikov2021cross} and \cite{agarwal2020principal} respectively.}, compute $\hat{\bw}^{n} \in \mathbb{R}^{|\mathcal{I}|}$ as follows:
\begin{equation}
\label{eq:pcr_linear_model_def}
    \hat{\bw}^{n} = \left(\sum^{\kappa}_{l = 1} \hat{s_l}^{-1}\hat{\bnu}_{l}\hat{\bmu}^T_{l}\right)\bY_{\Pi_n} 
\end{equation}

\noindent \emph{Step 2(b): Estimation.} Using $\hat{\bw}^{n} = [\hat{w}_u^n : u \in \mathcal{I}]$, we have the following estimate for any intervention $\pi \in \Pi$
\begin{equation}
\label{eq:potential_outcome_estimate_vertical_regression}
     \hat{\E}[Y_n^{(\pi)}] = \sum_{u \in \mathcal{I}} \hat{w}_u^{n} \hat{\E}[Y_u^{(\pi)}]
\end{equation}
%

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.9\textwidth]{figures/estimator_v7.png}
    \caption{A visual description of \method. Figure \ref{fig:estimator}(a) depicts an example of a particular observation pattern with outcome for unit-combination pair $(n,\pi)$ missing. Figure \ref{fig:estimator}(b) demonstrates horizontal regression for donor unit $u$ where we estimate potential outcome $\E[Y_u^{(\pi)}]$. Figure  \ref{fig:estimator}(c) visualizes vertical regression where we transfer estimated outcomes from the donor set $\mathcal{I}$ to the unit-combination pair $(n,\pi)$. }
    \label{fig:estimator}
\end{figure}

\noindent \textbf{Suitability of Lasso and PCR.} Lasso is appropriate for the horizontal regression step because it adapts to the sparsity of $\balpha_u$. However, it can be replaced with other algorithms that adapt to sparsity (see below for a larger discussion). For vertical regression, PCR is appropriate because  $\mathcal{A}$ is low rank. As \cite{agarwal2019robustness,agarwal2020principal} show, PCR implicitly regularizes the regression by adapting to the rank of the covariates ($\bY_{\Pi_n}$), i.e., the out-of-sample error of PCR scales with $r$ rather than the ambient covariate dimension. 
\vspace{1mm}

%
\noindent \textbf{Determining Donor Set $\mathcal{I}$.} 
%
\method~requires the existence of a subset of units $\mathcal{I} \subset [N]$ such that we are able to (i) accurately estimate their potential outcomes under all possible combinations, and (ii) transfer these estimated outcomes to a unit $n \in [N] \setminus \mathcal{I}$.
%
Theoretically, we detail sufficient conditions on the observation pattern such that we are able to perform (i) and (ii) accurately via the Lasso and PCR respectively. 
%
In practice, we recommend the following practical guidance to determining the donor set $\mathcal{I}$.
%
For every unit $n \in [N]$, learn a separate Lasso model $\balpha_n$ and assess its performance through $k$-fold cross-validation (CV). 
%
Assign units with low CV error (with a pre-determined threshold) as the donor set $\mathcal{I}$, and estimate outcomes $\hat{\E}[Y_u^{(\pi)}]$ for every unit $u \in \mathcal{I}$ and $\pi \in \Pi$. 
%
For any non-donor unit $n \in [N] \setminus \mathcal{I}$, we learn a model via PCR as discussed in step 2 of \method~and assess the performance of PCR through CV. 
%
For units with low PCR error, linear span inclusion (Assumption \ref{ass:donor_set_identification}(b)) and the assumptions required for the generalization for PCR likely hold, and hence we estimate their potential outcomes as in \eqref{eq:potential_outcome_estimate_vertical_regression}. 
%
For units with large PCR error, it is either unlikely that these set of assumptions holds or that $|\Pi_n|$ is not large enough (i.e., additional experiments need to be run for this unit), and hence we do not recommend estimating their counterfactuals. 
%

\vspace{2mm}
\noindent \textbf{Horizontal Regression Model Selection.} \method~allows for any ML algorithm (e.g., random forests, neural networks, ensemble methods) to be used in the first step. 
%
We provide an example of this flexibility by showing how the horizontal regression can also be done via CART in Appendix \ref{sec:CART_horizontal_regression}.
%
Our theoretical analysis of CART shows that it leads to better finite-sample rates under stronger regularity conditions on the potential outcomes $\E[Y_n^{(\pi)}]$ (see Corollary \ref{cor:potential_outcome_convergence_rate_CART}). 
%
This model-agnostic approach allows the analyst to tailor the horizontal learning procedure to the data at hand and include additional structural information for better performance. 
%
However, for simplicity, we focus on the Lasso for the remainder of this paper. 



%we estimate the target causal parameter an estimate of the target causal  causal parameter for any unit $u \in \mathcal{I}$, and $\pi \in \Pi$ can be constructed as follows: 
%\begin{equation}
%\label{eq:potential_outcome_estimate_horizontal_regression}
%    \hat{\E}[Y_u^{(\pi)}] = \hat{f}_u (\pi), 
%\end{equation} 

%{\color{red} Has $Y_{\pi_u}$ been defined? Not following this object - $\{\pi_u, Y_{\pi_u}\}^{|\Pi_u|}_{u=1}$}
