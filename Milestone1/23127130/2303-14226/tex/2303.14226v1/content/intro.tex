\section{Introduction}
%
Modern-day decision makers\textemdash in settings from e-commerce to public policy to medicine\textemdash often encounter settings 
where they have to pick a combination of actions, and ideally would like to do so in a highly personalized manner.
%
For example, recommending a curated basket of items to customers on a commerce platform, deciding on a combination of therapies for a health patient, enacting a collection of socio-economic policies for a specific geographic location, computing feature importances for machine learning models, etc.
%
Despite the ubiquity of such a setting, it also comes with significant empirical challenges: with $p$ actions and $N$ units, a decision maker must estimate $N \times 2^p$ potential combinations in order to confirm the optimal personalized policy. 
%
With large $N$ and even with relatively small $p$ (due to its exponential dependence), it likely becomes infeasible to run that many experiments; of course, in observational data there is the additional challenge of potential unobserved confounding.
%
Current methods tackle this problem by following one of two approaches: (i) they impose structure on how combinations of interventions interact, or (ii) they assume latent similarity in potential outcomes across units. However, as we discuss in detail below, these approaches require a large number of observations to estimate all $N \times 2^p$ potential outcomes because they do not exploit structure across both units and combinations.
%
%{\color{red} Abhi: Add 1-2 sentences before.}
%
Hence the question naturally arises: {\em how to effectively share information across both units and combinations of interventions?}

\vspace{0.5mm}
\noindent
{\bf Contributions.}
%
In this work, we formalize this combinatorial causal inference problem using the language of potential outcomes.
%
For a given unit $n \in [N]$, we represent its potential outcomes over the $2^p$  combinations as a Boolean function from $\{-1, 1\}^p$ to $\mathbb{R}$, expressed in the Fourier basis.
%
To impose structure across combinations, we assume that for a unit $n$, the linear coefficients $\balpha_n \in \mathbb{R}^{2^p}$ induced by this Fourier basis representation is sparse, i.e., has at most $s$ non-zero entries.
%
To impose structure across units, we assume that this matrix of Fourier coefficients across units $\mathcal{A} = [\balpha_n]_{n \in [N]} \in \mathbb{R}^{N \times 2^p}$ has rank $r$.
%
This simultaneous sparsity and low-rank assumption is indeed what allows one to share information across both units and combinations.
%
We discuss how various classic models studied in the literature fall relates with our proposed model.

We first establish an identification theorem for the $N \times 2^p$ potential outcomes of interest, which requires that any confounding is mediated through the matrix of Fourier coefficients $\mathcal{A}$.
%
We then design a two-step algorithm ``\method'' and prove it consistently estimates the various causal parameters, despite potential unobserved confounding. 
%
The first step of \method, termed ``horizontal regression'', learns the structure across combinations of interventions---assuming sparsity in the Fourier coefficients---via the Lasso. 
%
The second step of \method, termed ``vertical regression'', learns the structure across units---assuming low-rankness of the matrix Fourier coefficients---and we theoretically analyze the principal component regression (PCR) estimator for this subsequent step.
%
Our results imply that \method~is able to consistently estimate unit-specific potential outcomes given a total of $\text{poly}(r) \times \left( N + s^2p\right)$ observations (ignoring logarithmic factors).
%
This improves over previous methods that do not exploit structure across both units and combinations, which have sample complexity scaling as $\min(N \times s^2p, \ \ r \times (N + 2^p))$.
%
A summary of the sample complexities required for different methods can be found in Table \ref{tab:sample_complexity_summary}. 
%
A key technical challenge in our analysis is analyzing how the error induced in the first step of \method~percolates through to the second step. 
%
To tackle it, we reduce this problem to that of high-dimensional error-in-variables regression with linear model misspecification, and do a novel analysis of this statistical setting.
%
While we focus on the Lasso for the horizontal regression step, \method~allows for any ML algorithm to be used in the first step. 
%
This flexibility allows the analyst to tailor the horizontal regression procedure to take advantage of any additional structure in the potential outcomes. 
%
We provide an example of this by showing how replacing Lasso with ``classification and regression trees'' (CART) leads to a sample complexity (ignoring logarithmic factors) of $\text{poly}(r) \times \left( N + s^2\right)$ under additional regularity assumptions on the potential outcome function. 
%

Next, we show how \method~can be used to inform experiment design for combinatorial inference.
%
In particular, we propose and analyze an experimental design mechanism that ensures the key assumptions required for consistency of \method~are satisfied. 
%
%We then run simulations to empirically verify our theoretical findings in both the observational and experimental design setting. Our simulations corroborate the superior sample complexity of \method~as compared to other approaches. 
\begin{table}[htbp]
    \captionsetup{font=footnotesize}
    %\setlength\tabcolsep{2pt} % default value is 6pt
    \small
    \renewcommand{\arraystretch}{0.9}
    \centering
    \begin{tabular}{|p{37mm}|p{38mm}|p{37mm}|p{37mm}|}
    \hline
    Learning algorithm & Exploits structure across combinations ($\lVert \balpha_n \rVert_0 = s$) & Exploits structure across units ($\text{rank}(\mathcal{A}) = r$) & Sample complexity \\  %[0.5ex]
    \hline 
    Lasso & \hspace{12mm} \cmark &  \hspace{12mm} \xmark  & $O\left(N \times s^2p\right)$ \\ [0.25ex]
    \hline
    Matrix Completion &  \hspace{12mm} \xmark  &  \hspace{12mm} \cmark & $O\left(\text{poly}(r) \times (N + 2^p)\right)$ \\ 
    \hline
    \method & \hspace{12mm} \cmark  & \hspace{12mm} \cmark & $O\left(\text{poly}(r) \times (N + s^2p)\right)$ \\ [0.25ex]
    \hline 
    \end{tabular}
    \caption{Comparison of sample complexity of different methods to estimate all $N \times 2^p$ causal parameters. \method~exploits structure across both combinations and units to estimate all potential outcomes with significantly fewer observations. }
    \label{tab:sample_complexity_summary}
\end{table}

%\begin{table}[h!]  
%\begin{center}  
%\begin{tabular}{|l|r|} 
%\textbf{Learning Algorithm} & \textbf{Sample Complexity}\\  
%x$\alpha$ & $\beta$ & $\gamma$ \\  
%\hline  
%Lasso \citep{negahban2012learning} & $O(N \times s^2p \times \delta^{-2})$\\  
%Matrix Completion  & $O(N \times s^2p \times \delta^{-2})$\\  
%\method~(this paper) & $O(\text{poly}(r/\delta) \times \left( N + s^2p\right)$ \\  
%\end{tabular}  
%\caption{ the basic table}  
%\label{tab:Table1}  
%\end{center}  
%\end{table}  


%and we separately theoretically analyze the Lasso and ``classification and regression trees'' (CART) estimators for this step.
 