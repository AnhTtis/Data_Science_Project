\section{Horizontal Regression Procedures}
\label{sec:horizontal_regression_procedures}
In this section, we detail specific estimation procedures for estimating $\hat{\E}[Y_u^{(\pi)}]$ and their finite-sample analysis for a donor unit $u \in I^{D}$ . 

\subsection{Lasso}
\label{subsec:horizontal_regression_lasso}
We propose to estimate $\hat{\E}[Y_u^{(\pi)}]$ via the Lasso i.e. by solving the following convex program with penalty parameter $\lambda$\footnote{In practice, $\lambda$ can be chosen by cross-validation.}: 
\begin{align}\label{eq:lasso_estimator}
 \hat{\balpha}_u=  \argmin_{\balpha} \lVert \bY_{\Pi_u} - \bchi(\Pi_u)\balpha \rVert^2_2 + \lambda \lVert \bm{\alpha} \rVert_1
\end{align}
where recall that $\bchi(\Pi_u) = [\bchi^\pi: \pi \in \Pi_u] \in \mathbb{R}^{|\Pi_u| \times 2^p}$. Then for $\pi \in \Pi$, we estimate the target causal parameter as 
\begin{equation}
\label{eq:lasso_predicted_potential_outcome}
    \hat{\E}[Y_u^{(\pi)}] = \langle  \hat{\balpha}_u, \bchi^{\pi} \rangle  
\end{equation}

% \bm{\chi(\Pi_u)}\bm{\alpha} \rVert^2_2 
\noindent In order to establish finite-sample convergence analysis for the lasso, we make the standard incoherence assumption on the design matrix $\bchi(\Pi_u)$. 

Lemma 2 in \cite{negahban2012learning} show that the Assumption \ref{ass:incoherence} is satisfied with high probability if $\Pi_u$ is chosen uniformly at random and grows as $\omega(s^2p)$. 
%
\begin{proposition} \label{prop:lasso_convergence_rate} Let Assumptions \ref{ass:observation_model}, \ref{ass:subgaussian_noise}, and \ref{ass:incoherence} hold. Then, conditioned on $\mathcal{A}$, we have
\begin{equation}
\label{eq:lasso_convergence_rate}
 \hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] = O_p\left(\sqrt{\frac{s^2p}{|\Pi_u|}} ~ \bigg | ~ \mathcal{A} \right) 
\end{equation}
for any donor unit $u \in\mathcal{I}$ and $\pi \in \Pi$
\end{proposition}

%\noindent \textbf{Interpretation.} The Lasso estimator \eqref{eq:lasso_predicted_potential_outcome} consistently estimates the target causal parameter for a donor unit $u$ given that the number of observations $|\Pi_u|$ grows sufficiently quickly with the dimension of the binary mapping $p$ and the sparsity $s$ , i.e., $|\Pi_u| = \omega\left(\sqrt{s^2p} \right)$. This allows us to estimate the potential outcomes for all donor units under each intervention with $\omega\left(N_D \times (s^2p) \right)$ observations. This represents a \emph{exponential} improvement  as compared to the ``naive'' solution which would be to conduct $N_D \times 2^p$ experiments. 
%\noindent Theorem \ref{thm:lasso_high_prob_error_bound} above leads to the following corollary. 
%\begin{corollary} 
%\label{cor:lasso_Op_bound}
%Let the set-up of Theorem \ref{thm:lasso_high_prob_error_bound} hold. Then, 
%\begin{equation}
%\label{eq:lasso_Op_bound}
%     | \hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}]| = O_p \left(\sqrt{\frac{s^2p}{|\Pi_u|}} \right) 
%\end{equation}
%\end{corollary}


