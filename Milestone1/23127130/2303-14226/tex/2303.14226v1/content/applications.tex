%\section{Discussion and Future Research}
\section{Combinatorial Inference Applications}
\label{sec:combinatorial_inference_applications}
In this section, we discuss how classical models and applications could relate to our proposed potential outcome model. 
%
In particular, we discuss two well-studied models for functions over combinations: low-degree Boolean polynomials and $k$-Juntas.
%
We also discuss applications such as factorial design experiments and recommendation systems. 
%
\vspace{2mm}

\noindent \textbf{Low-degree Boolean Polynomials.} A special instance of sparse Boolean functions is a low-degree polynomial which we define as follows.
%
For a positive integer $d \leq p$,  $f : \{-1,1\}^p \rightarrow \mathbb{R}$ is a $d$-degree polynomial if its Fourier transform satisfies the following for any input $\bx \in \{-1,1\}^p$
\begin{equation}
\label{eq:low_degree_fourier_transform}
 f(\bx) = \sum_{S \subset |p|, |S| \leq d} \alpha_S \chi_S (\bx).
\end{equation}
%
\noindent In this setting, we see that $s  \leq \sum^d_{i=0} \binom{p}{i} \approx p^d$. That is, degree $d$-polynomials impose sparsity on the potential outcome by limiting the degree of interaction between interventions. 
\vspace{2mm}
%

\noindent \textbf{Applications of low-degree Boolean polynomials.} We discuss two applications when the potential outcomes can be modeled as a low-degree Boolean polynomial.
%

\vspace{2mm}
\noindent
\emph{Factorial Design Experiments.} Factorial design experiments consist of $p$ treatments where each treatment arm can take on a discrete set of values, and units are assigned different combinations of these treatments. 
%
In the special case that each treatment arm only takes on two possible values, this experimental design mechanism is referred to as a $2^p$ factorial experiment. 
%
Factorial design experiments are widely employed in the social sciences, agricultural and industrial applications \citep{duflo2007using,dasgupta2015causal,wu2011experiments}. 
%
A common strategy to determine treatment effects in this setting is to assume that the potential outcome only depends on main effects and pairwise interactions with higher-order interactions being negligible \citep{bertrand2004emily,eriksson2014employers,george2005statistics}. 
%
That is, analysts implicitly enforce sparsity of $\balpha_n$ by setting $d = 2$. We refer the reader to \cite{zhao2022regression} for a detailed discussion of various estimation strategies and their validity in different settings. 
%
The model we propose (see Assumption \ref{ass:observation_model}) captures these various modeling choices commonly used in factorial design experiments by imposing sparsity on $\balpha_n$. 
%
Further, as we show later, \method~can adapt to low-degree polynomials without pre-specifying the degree, $d$, i.e., it automatically adapts to the inherent level of interactions in the data.
%
The additional assumption we make however is that there is structure across units, i.e., the matrix $\mathcal{A} = [\balpha_n]_{n \in [N]}$ is low-rank.
%

\vspace{2mm}
\noindent
\emph{Optimal Team Selection.} Another use for combinatorial inference is to find  the optimal configuration for a team.
%
For example, a soccer team can be interested in finding the configuration of positions (i.e., defensive, midfield, offensive players) that leads to the highest winning percentage.
%
Here, units $n$ are different teams, and combinations $\pi$ represent possible configurations of the team. 
%
The potential outcomes $\E[Y_n^{(\pi)}]$ represent the winning percentage for a given team $n$ under a particular configuration $\pi$, with the coefficients of $\balpha_n$ representing the effect of different configurations on the win rate. 
%
In this setting, enforcing sparsity of $\balpha_n$ via low-degree polynomials may be appropriate because the number of wins may depend on all the players, however only interactions between players of the same type matter while higher-order interactions between players of different types can be negligible. 
%
Returning to the example of a soccer team, the win rate can depend strongly on the synergy of the offensive players, whereas interactions between offensive and defensive players have a smaller effect. 
%
Further, since different teams might play in similar ways and can have the same optimal configuration, one potential way to capture this structure across teams is by placing a low-rank structure on $\mathcal{A}$.
\vspace{2mm}


\noindent \textbf{$k$-Juntas.} Another special case of sparse Boolean functions are $k$-Juntas which only depend on $k < p$ input variables. More formally, a function $f : \{-1,1\}^p \to \mathbb{R}$ is a $k$-junta if there exists a set $K \subset [p]$ with $|K| = k$ such that the fourier expansion of $f$ can be represented as follows
\begin{equation}
\label{eq:k_junta}
    f(\bx) = \sum_{S \subset K} \alpha_{S} \chi_{S}(\bx)
\end{equation}
Therefore, in the setting of the $k$-Junta, the sparsity index $s \leq 2^k$. In contrast to low-degree polynomials where the function depends on all $p$ variables but limits the degree of interaction between variables, $k$-Juntas only depend on $k$ variables but allow for arbitrary interactions amongst them.  

\vspace{2mm}
\noindent 
\textbf{Applications of $k$-Juntas.}  We discuss two applications when the potential outcomes can be modeled as a $k$-Junta.

\vspace{2mm}
\noindent 
\emph{Recommendation Systems.} Recommendation platforms such as \emph{Netflix} are often interested in recommending a combination of movies that maximizes a user's engagement with a platform. 
%
Here, units $n$ can be individual users, and $\pi$ represents combinations of different movies.
%
The potential outcomes $\E[Y_n^{(\pi)}]$ are the engagement levels for a unit $n$ when presented with a combination of movies $\pi$, with $\balpha_n$ representing  the preferences for that user. 
%
In this setting, a user's engagement with the platform may only depend on a small subset of movies. For example, a user who is only interested in fantasy will only remain on the platform if they are recommended movies such as \emph{Harry Potter}.
%
Under this behavioral model, the potential outcomes (i.e., engagement levels) can be modeled as a $k$-Junta with the non-zero coefficients of $\balpha_n$ representing the combinations of the $k$ movies that affect engagement level for a user $n$. 
%
Our potential outcome observational model (Assumption \ref{ass:observation_model}) captures this form of sparsity while also reflecting the low-rank structure commonly assumed when studying recommendation systems.
%
Once again, our results show \method~can adapt to $k-$Juntas without pre-specifying the subset of features $K$, i.e., it automatically learns the important features for a given user. 
%
This additional structure in $k-$Juntas is well-captured by the CART estimator, which leads to tighter finite-sample bounds, as detailed in Appendix \ref{sec:CART_horizontal_regression}.

\vspace{2mm}
\noindent 
\emph{Knock-down Experiments in Genomics.} A key task in genomics is to identify which set of genes are responsible for a phenotype (i.e., physical trait of interest such as blood pressure) in a given individual. 
%
To do so, geneticists use knock-down experiments which measure the difference in the phenotype after eliminating the effect of a set of genes in an individual. 
%
To encode this process in the language of combinatorial causal inference, we can think of units $n$ as different individuals, an action as knocking a particular gene out,  and $\pi$ as a combination of genes that are knocked out. 
%
The potential outcomes $\E[Y_n^{(\pi)}]$ is the expression of the phenotype for a unit $n$ when the combination of genes $\pi$ are eliminated via knock-down experiments, and the coefficients of $\balpha_n$ represent the effect of different combination of genes on the phenotype. 
%
A typical assumption in genomics is that phenotypes only depend on a small set of genes and their interactions. 
%
In this setting, we can model this form of sparsity by thinking of the potential outcome function as a $k$-junta, as well as capturing the similarity between the effect of genes on different individuals via our low-rank assumption. 
%

