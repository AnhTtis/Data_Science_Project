
\section{Numerical Experiments}
\label{sec:sims}
In this section, we corroborate our theoretical findings with numerical simulations in both the observational setting and under our experimental design mechanism described in Section \ref{sec:experimental_design}. 
%
We compare \method~to the Lasso (i.e., running a separate horizontal regression  for every unit) and benchmark matrix completion algorithms. 
%
Specifically, we compare \method~to the following matrix completion algorithms: (i) \texttt{SoftImpute} \citep{mazumder2010spectral}, (ii) \texttt{IterativeSVD} \citep{troyanskaya2001missing} . 
%
We note that we also tried running our simulation experiments for nuclear-norm minimization method introduced in \citep{candes2012exact}, but the method was unable to be run within a reasonable time frame (6 hours) for any of our simulation settings. 
%
Code for implementing \method~and reproducing our experiments can be found at \url{https://github.com/aagarwal1996/synth_combo}. 

\subsection{Observational Setting}
\label{subsec:observational_sims}
For this numerical experiment, we simulate an observation pattern commonly found in applications such as recommendation engines, where most users tend to provide ratings for combinations of goods they either particularly liked or disliked, i.e., there is confounding.
%
The precise experimental set-up for this setting is as follows.  

\vspace{2mm}
\noindent \textbf{Experimental Set-up.} We consider $N = 100$ units, and vary the number of interventions $p \in \{10,11, \ldots 15\}$. 
%
We choose $r = 3$, and $s = rp^{3/2}$.
%
To proceed, we first describe how we generate the potential outcomes, followed by how we generate the observation pattern. 

\vspace{2mm}
\noindent
\emph{Generating Potential Outcomes.} For every $i \in [r],$ we generate $\balpha_i$ by first sampling $p^{3/2}$ non-zero coefficients at random.  
%
Then, we sample every non-zero coefficient of $\balpha_i$ from a standard normal distribution. 
%
Denote $\mathcal{A}_r = [\balpha_i : i \in [r]] \in \mathbb{R}^{r \times 2^p}$.  
%
Next, we construct $\mathcal{A}_{N - r}= \mathbf{B}\mathcal{A}_r \in \mathbb{R}^{(N - r) \times 2^p}$, where the entries in $\mathbf{B} \in \mathbb{R}^{(N - r) \times r}$ are sampled i.i.d from a standard Dirichlet distribution. 
%
This ensures that the Fourier coefficients of $\mathcal{A}_{N-r}$ lie in the span of the Fourier coefficients in $\mathcal{A}_r$.
%
We then define $\mathcal{A} = [\mathcal{A}_r, \mathcal{A}_{N-r}]$. 
%
By construction, $\text{rank}(\mathcal{A}) = r$, and $\lVert \balpha_n \rVert_0 \leq  rp^{3/2} = s$ for every unit. 
%
The expected potential outcomes are simply constructed by computing $\bchi(\Pi) \mathcal{A}^T$.
%
Finally, we generate noisy potential outcomes by adding normally distributed noise $\epsilon \sim N(0,\sigma^2)$ to the potential outcome $\E[Y_n^{(\pi)}]$ for every unit $n$ and combination $\pi$. 
%
We choose $\sigma^2$ such that the percentage of variance explained (PVE), defined by the formula $\text{PVE} = \text{Var}(\E[Y_N^{(\Pi)}])/(\sigma^2 + \text{Var}(\E[Y_N^{(\Pi)}]))$ is equal to $0.9$, and where $\text{Var} (\cdot)$ denotes the variance. 
%
PVE is proportional to signal to noise ratio, and can be thought of as the fraction of variance that is explained by the underlying potential outcome matrix. 

\vspace{2mm}
\noindent
\emph{Observation Pattern.} Next, we discuss how we generate the observation pattern. 
%
Let $p_{n,\pi}$ denote the probability that the expected potential outcome for unit $n$ and combination $\pi$ is revealed. 
%
In this simulation, we define $p_{n,\pi} = |\E[Y_n^{(\pi)}]|/\sum_{\pi \in \Pi} |\E[Y_n^{(\pi)}]|$.
%
Our definition of $p_{n,\pi}$ induces the missingness pattern where we are more likely to see outcomes with larger absolute values. 
%
In the context of recommendation engines, this can be interpreted as saying that we are only likely to observe the ratings for combinations that users either strongly like or dislike.
%
Next, we choose $|\mathcal{I}| = 2r$ donor units. 
%
For a donor unit $u$, we generate $2p^{5/2}$ observations where a combination $\pi$ is picked with probability $p_{u,\pi}$.
%
We do the same for non-donor units but generate only $2r^4$ observations.
%

\vspace{2mm}
\noindent
\emph{Hyper-parameter Choices for Estimation Methods}. For the Lasso, we tune the regularization parameter $\lambda$ via $5$-fold cross-validation (CV). 
%
For PCR (i.e., step 2 of \method), we tune $\kappa$, the number of singular values retained, via 5-fold CV as well. 
%
The matrix completion techniques that we compare to require that the rank $r$ of the underlying matrix to be recovered is provided as a hyper-parameter.
%
In this simulation, we provide the true rank $r = 3$ as the hyper-parameter to these matrix completion algorithms. 
\vspace{2mm}


\noindent \textbf{Results.} We measure the mean squared error (MSE) averaged over 5 repetitions between the estimated potential outcome matrix and the true potential outcome matrix for each method. 
%
The MSE for each method as we vary the number of interventions is visualized in Figure \ref{fig:sim_results} (a).  
%
The results show that \method~outperforms all other methods as the number of interventions grow. 
%
Further, the gap in performance between \method~and the Lasso enforce the utility of using PCR for non-donor units that do not have sufficient measurements.
%
We also note that the sample sizes chosen here for the donor set are less than that required by our theoretical results. Nonetheless, \method~performs well, suggesting that our theoretical results may not be entirely tight.  

\subsection{Experimental Design Simulations}
\label{subsec:experimental_design_sims}
We begin by describing the simulation set-up for the observation pattern induced by our experimental design mechanism.

\vspace{2mm}

\noindent \textbf{Experimental Set-up.} We generate potential outcomes, and noisy potential outcomes as we do in the observational setting. 
%
We generate the observation pattern as described by the experimental design mechanism in Section \ref{sec:experimental_design}.
%
Finally, hyper-parameters of all estimation methods are chosen as described in the experimental set-up for the simulations in the observational setting. 

\vspace{2mm}

\noindent \textbf{Results.} We plot the MSE (averaged over 5 repetitions) for the different methods as we increase the number of interventions $p \in \{10,11,\ldots 15\}$ in Figure \ref{fig:sim_results} (b). 
%
We see that \method~significantly outperforms all methods for the observation pattern induced by our design mechanism. 
%
The performance of \method~corroborates our theoretical findings that this experimental design mechanism is able to utilize the strengths of \method~effectively. 
%


\vspace{4mm}


\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{simulation_figures/observational_design_results_0.9.png}  
        \caption*{(a) Observational setting simulations.}
    \end{minipage}\hfill
    % Remove or comment out this line
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{simulation_figures/experimental_design_results_0.9.png} % second figure itself
        \caption*{(b) Experimental design simulations.}
    \end{minipage}
    \caption{MSE for different methods as we vary the number of interventions in (a) the observational setting and (b) under our proposed experimental design mechanism. \method~outperforms all other methods in both settings. }
    \label{fig:sim_results}
\end{figure}
