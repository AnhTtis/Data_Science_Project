
\section{Identification of Potential Outcomes}
\label{sec:identification}
%
In this section, we show that $\E[Y^{(\pi)}_{n} \mid \mathcal{A}]$ can be written as a function of observed outcomes, i.e., we establish identification of our target causal parameter.
%
Our identification argument allows for \emph{unobserved confounding}.
%
That is,  whether or not a unit is seen under a combination is correlated with its potential outcome under that combination due to unobserved factors.  
%
Below we list the required assumptions for identification.

\begin{assumption}[Selection on Fourier coefficients]
\label{ass:selection_on_fourier} 
%
For all $n \in [N]$ and $\pi \in \Pi$, $Y^{(\pi)}_n \independent \mathcal{D} \mid \mathcal{A}$.
%
\end{assumption}
%
Assumption \ref{ass:selection_on_fourier} states that conditional on the Fourier coefficients $\mathcal{A}$, the potential outcomes are conditionally independent of the treatment assignments $\mathcal{D}$.
%
However, the treatment assignment can be an arbitrary function of the Fourier coefficients.
%
This conditional independence condition can be thought of as ``selection on Fourier coefficients”, which is analogous to “selection on observables”, where it is assumed that potential outcomes are independent of treatment assignments conditional on {\em observed} covariates. 

To define the next assumption, we introduce necessary notation. 
%
For each unit $n \in [N]$, denote the subset of combinations we observe them under as $\Pi_n \subseteq \Pi$. 
%
%Recall from Section \ref{sec:notation} that $\bchi^{\pi} \in \mathbb{R}^{2^p}$ refers to the vector of Fourier characteristics for a given combination $\pi$. 
%
Additionally, for $\pi \in \Pi$, let $\tilde{\bchi}^{\pi}_n \in \mathbb{R}^{2^p}$ denote the vector where we zero out all coordinates of $\bchi^{\pi} \in \mathbb{R}^{2^p}$ that correspond to the coefficients of $\balpha_n$ which are zero. 
%
For example, if $\balpha_n = (1,1,0,0,\ldots 0)$ and $\bchi^{\pi} = (1,1,\ldots 1)$, then  $\tilde{\bchi}^{\pi}_n = (1,1,0, \ldots 0)$. 
%
%(recall the definition from Section \ref{sec:notation})


%Additionally, we let $P_{n,S}$ be the projection matrix to the subspace spanned by the non-zero coordinates of $\bm{\alpha}_n$, and for any vector $\bm{x} \in \mathbb{R}^{2^p}$, denote $\Tilde{\bm{x}} = P_{n,S}\bm{x}$.


\begin{assumption}[Donor Units]
\label{ass:donor_set_identification} We assume there exists a set of ``donor units'' $\mathcal{I} \subset [N]$, such that the following two conditions hold: 

\begin{enumerate}
\item [(a)] Horizontal span inclusion: For any donor unit $u \in \mathcal{I}$ and combination $\pi \in \Pi$, suppose  $\tilde{\bchi}_u^{\pi} \in  span(\tilde{\bchi}_u^{\pi_i}: \pi_i \in \Pi_u)$. That is, there exists exists $\bbeta_{\Pi_u}^{\pi} \in \mathbb{R}^{|\Pi_{u}|}$ such that $\tilde{\bchi}_u^{\pi} = \sum_{\pi_{i} \in  \Pi_{u}} \beta_{\pi_i}^{\pi} \tilde{\bchi}_u^{\pi_i}:$

\item [(b)]  Linear span inclusion: For any unit $n  \in [N] \setminus \mathcal{I}$, suppose $\bm{\alpha}_{n} \in span(\bm{\alpha}_{u} : u \in \mathcal{I})$. That is, there exists $\bw^{n}$ such that $\bm{\alpha}_{n} = \sum_{u \in \mathcal{I}}w_{u}^{n}\bm{\alpha}_{u}$
\end{enumerate}
%
\end{assumption}
%
\vspace{-1.5mm}
We interpret this assumption below.
%
\emph{Horizontal span inclusion:} 
%
This requires that the set of observed combinations for any donor unit is ``diverse'' enough that the projection of the Fourier characteristic for a target intervention is in the span of characteristics of observed interventions. 
%
\emph{Linear span inclusion:} 
%
This requires that the donor set is diverse enough such that the Fourier coefficient of any unit is in the span of the Fourier coefficients of the donor set. 
%
% This is what allows us to linearly represent the outcomes of any unit as a linear combination of the donor units, and is motivated by the low-rank structure of $\mathcal{A}$ (Assumption \ref{ass:observation_model} (a)).
%
% Consider the following natural case when linear span inclusion holds. 
% %
% Specifically consider the case where $span({\balpha_u : u \in \mathcal{I}}) = R^r$.
% %
% Since $\alpha_n \in R^r$, linear span inclusion would immediately hold.
% %
% Theorem 4.6.1 of \cite{vershynin_2018} implies that if ${v_n}_{n \in [N]}$ are sampled as independent, mean zero, sub-Gaussian vectors, then $span({\balpha_u : u \in \mathcal{I}}) = R^r$ holds with high probability as $| \mathcal{I} |$ grows.

% We note that if $\mathcal{A}$ has rank-$r$, then any row $\balpha_n$ can be represented as a combination of at most $r$ other rows in $\mathcal{A}$. 

\vspace{2mm}
\noindent \textbf{Motivating example.} 
%
We provide the following running example as a motivation for the existence of a donor set $\mathcal{I}$ that satisfies Assumption \ref{ass:donor_set_identification}. 
%
Suppose Assumption \ref{ass:observation_model} holds and that  each unit belongs to one of $r$ \emph{types}.
%
That is, for every unit $n \in [N]$ that
$\balpha_n \in \{\balpha(1),\balpha(2),\ldots,\balpha(r)\} \subset \mathbb{R}^{2^p},$ where each $\balpha(i)$ is $s$-sparse. 
%
Note that having $r$ distinct sets of Fourier coefficients implies the low-rank property in Assumption \ref{ass:observation_model} (a). 
%
Suppose we choose the donor set $\mathcal{I} \subset [N]$ by sampling a subset of units independently and uniformly at random with size satisfying $\Omega(r\log(r/\gamma))$. 
%
Sample $\Pi_\mathcal{I} \subset \Pi$ combinations independently and uniformly at random with size satisfying $\Omega(s\log(r|\mathcal{I}|/\gamma))$, and assign all donor units this set of combinations. 
%
Proposition \ref{prop:motivating_example} below (proved in Appendix \ref{sec:example_proofs})  establishes that  under this sampling scheme horizontal and linear span inclusion (i.e., Assumption \ref{ass:donor_set_identification}) is satisfied with high probability as long as each type is well-represented (i.e., there are $\Omega(N)$ units of each type).
%
\begin{proposition}
\label{prop:motivating_example}
Let Assumptions \ref{ass:observation_model} and Assumptions \ref{ass:selection_on_fourier} hold. Moreover, suppose that for some $c > 0$, there are at least $cN/r$ units of each type. Then, under the proposed sampling scheme described above, Assumption \ref{ass:donor_set_identification} is satisfied with probability at least $1 - \gamma$.  
\end{proposition}
%
Proposition \ref{prop:motivating_example} shows that (ignoring logarithmic factors) sampling $r$ units and assign these units $s$ combinations at random is sufficient to induce a valid donor set. 
%
Further, we show in Section \ref{sec:experimental_design} that our assumption of exactly $r$ unit types can be relaxed.
%
Specifically, we show that a randomly selected donor set of size $\omega(r\log(rs))$ will satisfy linear span inclusion as long as the $r$ non-zero singular values of the matrix $\mathcal{A}$ are of similar size (see Assumption \ref{ass:balanced_spectrum} below).
%



%Suppose also that each type is well represented, in that there are $\Omega(N)$ individuals of each type in the population.  
% Now consider the case where we sample a donor subset of size $m = \omega(r\log r)$ uniformly at random. 
% For all donor units $u \in \mathcal{I}$, we administer a sequence of $q = \omega(s\log(rs) )$ treatment combinations $(\pi_{1u}, \pi_{2u}, \ldots, \pi_{qu})$ selected independently and uniformly at random from $\Pi$. 
% Under these conditions we verify in Appendix \ref{sec:example_proofs} that Assumption \ref{ass:donor_set_identification} is satisfied with high probability. 
% In particular, for linear span inclusion (Assumption \ref{ass:donor_set_identification}(b)), we show that for for all $j \in [r]$ there exist at least $\Omega(m/r) = \omega(\log r)$ representatives $u \in \mathcal{I}$ with $\balpha_u = \balpha(j)$.
%


%For all donor units $u \in \mathcal{I}$, we administer a sequence of $q = \omega(s\log(rs) )$ treatment combinations $(\pi_{1u}, \pi_{2u}, \ldots, \pi_{qu})$ selected independently and uniformly at random from $\Pi$. 
%units size $m = \omega(r\log r)$ uniformly at random. 



%Specifically, (1) Horizontal span inclusion (Assumption \ref{ass:donor_set_identification}(a)) holds: for all units $u \in \mathcal{I}$, we have $\mathrm{dim}\, \mathrm{span}\left( \tilde{\bchi}_u^{\pi_i}: \pi_i \in \Pi_u \right) = \norm{\balpha_u}_0$. 
%
%(2) Linear span inclusion (Assumption \ref{ass:donor_set_identification}(b)) holds: for all $j \in [r]$ there exist at least $\Omega(m/r) = \omega(\log r)$ representatives $u \in \mathcal{I}$ with $\balpha_u = \balpha(j)$.
%
%In fact, we show in Section \ref{sec:experimental_design} that our assumption of $r$ unit types can be relaxed.
%
%Remarkably, a randomly selected donor set of size $\omega(r\log(rs))$ will satisfy linear span inclusion as long as the $r$ non-zero singular values of the matrix $\mathcal{A}$ are of similar size (see Assumption \ref{ass:balanced_spectrum} below).

\subsection{Identification Result}
\noindent Given these assumptions, we now present our identification theorem. 
\label{subsec:identification_theorem}

\begin{theorem}
\label{thm:identification}
Let Assumptions \ref{ass:observation_model} and \ref{ass:donor_set_identification} hold. Then, given $\bbeta_{\Pi_u}^{\pi}$ and $\bw_u^{n}$ defined in Assumption \ref{ass:donor_set_identification}, we have

\noindent
[(a)] Donor units: For $u \in \mathcal{I}$, and $\pi \in \Pi$, 
$
 \E[Y^{(\pi)}_{u} ~ | ~ \mathcal{A}]  =  \sum_{\pi_u \in \Pi_{u}} \beta_{\pi_{u}}^{\pi}
 \E[Y_{u,\pi_{u}} \ | \  \mathcal{A}, \ \mathcal{D}].
$
    
\vspace{0.5mm}
\noindent
[(b)] Non-donor units: For $n \in [N] \setminus \mathcal{I}$, and $\pi \notin \Pi_n$,
$
 \E[Y^{(\pi)}_{n} ~ | ~ \mathcal{A}] = \sum_{u \in \mathcal{I},\pi_u \in \Pi_u}w_{u}^{n}  \beta_{\pi_{u}}^{\pi}  \E[Y_{u,\pi_u} \ | \  \mathcal{A}, \ \mathcal{D}].
$
\end{theorem}
%
Theorem \ref{thm:identification} allows for identification despite \emph{unobserved confounding} (i.e, the combinations assigned to units are allowed to depend on the Fourier coefficients $\mathcal{A}$). 
%
Part (a) establishes that for every donor unit $u \in \mathcal{I}$, the causal estimand can be written as a function of its \emph{own} observed outcomes $\E[\bY_{\Pi_u}]$, given knowledge of $\bbeta^{\pi}_{\Pi_u}$. 
%
Part (b) states that the target causal estimand $\E[Y_n^{(\pi)}]$ for a non-donor unit and combination $\pi$ can be written as a linear combination of the outcomes of the donor set $\mathcal{I}$, given knowledge of $\bw_n^n$.
%
Thus, Theorem \ref{thm:identification} suggests that the key quantities in estimating $\E[Y_n^{(\pi)}]$ for any unit-combination pair $(n,\pi)$ are $\bbeta_{\Pi_u}^{\pi}$ and $\bw_u^n$. 
%
In the following section, we provide an algorithm to estimate both $\bbeta_{\Pi_u}^{\pi}$ and $\bw_u^n$, as well as concrete ways of determining the donor set $\mathcal{I}$. 

%\suhascomment{I checked the proof and it seems correct. My main confusion is the following: I thought identification also meant showing that there is only one set of $(w,\beta)$ that is consistent with the observed dgp? Is that clear? I think the above says that the effects are identified as long as we can estimate $(w,\beta)$, which follows from subsequent analysis.}   


%despite \emph{unobserved confounding} (i.e., potential outcomes can be dependent on observed interventions). Key to this result are our structural assumptions placed on the model, i.e., sparsity and low-rank, as well as Assumption \ref{ass:donor_set_identification}. Specifically, sparsity of $\balpha_n$ (Assumption \ref{ass:fourier_coefficient_sparsity}) and Assumption \ref{ass:donor_set_identification} (a) allow us to learn potential outcomes for the donor units. Then, the low-rank property of $\E[\bY^{(\Pi)}_{N}]$ (Assumption \ref{ass:low_rank_assumption}) and Assumption \ref{ass:donor_set_identification} (b) allow us to then transfer the potential outcomes across units. \\




