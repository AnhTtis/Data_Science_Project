\section{Setup and Model}\label{sec:model}

\subsection{Notation}\label{sec:notation}
%We describe requisite notation, and background on combinations and functions over combinations.

\vspace{2mm}
\noindent
{\bf Representation of Combinations as Binary Vectors}.
%
Let $[p] = \{1,\ldots p\}$ denote the set of $p$ interventions. 
%
Denote by $\Pi$ the power set of $[p]$, i.e., the set of all possible combinations of $p$ interventions, where we note $|\Pi| = 2^p$.
%
Then, any given combination $\pi \in \Pi$ induces the following binary representation $\bv(\pi) \in \{-1,1\}^p$ defined as follows: $\bv(\pi)_{i} = 2~\indicator\{{i \in \pi}\} -1.$
%

\vspace{2mm}
\noindent
{\bf Fourier Expansion of Boolean Functions.}
%
Let $\mathcal{F}_{\text{bool}} = \{f : \{-1,1\}^p \to \mathbb{R}\}$ be the set of all real-valued functions defined on the hypercube $\{-1,1\}^p$. 
%
Then $\mathcal{F}_{\text{bool}}$ forms a Hilbert space defined by the following inner product: for any $f,g \in \mathcal{F}_{\text{bool}}$, $\langle f, g \rangle_{B} = \frac{1}{2^p}\sum_{\bx \in \{-1,1\}^p}f(\bx)g(\bx).$
%
This inner product induces the norm $\langle f, f \rangle_{B} \coloneqq \lVert f \rVert_{B}^2 = \frac{1}{2^p}\sum_{\bx \in \{-1,1\}^p}f^2(\bx).$
%
We construct an orthonormal basis for $\mathcal{F}_{\text{bool}}$ as follows: for each subset $S \subset [p]$, define a basis function $\chi_{S}(\bx) = \prod_{i \in S} x_i$ where $x_i$ is the $i^\text{th}$ coefficient of $\bx \in  \{-1,1\}^p$. 
%
One can verify that for any $S \subset [p]$ that $\lVert \chi_{S} \rVert_B = 1$, and that $\langle \chi_S, \chi_{S'} \rangle_B = 0$ for any $S' \neq S$. 
%
Since $|\{\chi_S: S \subset [p] \}| = 2^p$, the functions $\chi_S$ are an orthonormal basis of $\mathcal{F}_{\text{bool}}$.
%
Hence, any $f \in \mathcal{F}_{\text{bool}}$ can be expressed via the following ``Fourier'' decomposition: $f(\bx) = \sum_{S \subset[p]}\alpha_{S}\chi_{S}(\bx),$
%
where the Fourier coefficient $\alpha_S$ is given by computing $ \alpha_S = \langle f, \chi_S \rangle_B $. 
%
We will refer to $\chi_S$ as the Fourier character. 
%
%We refer the reader to \cite{o2014analysis, mansour1994learning} for further background regarding Boolean Fourier analysis.
%
Define $\bm{\alpha} \in \mathbb{R}^{2^p}$ and  $\bm{\chi}(x) \in \{-1, 1\}^{2^p}$ as the vector of Fourier coefficients and characters respectively.
%
Hence any function $f : \{-1,1\}^p \to \mathbb{R}$ can be re-expressed as follows:
$f(\bx) =  \langle \bm{\alpha}, \bm{\chi}(x) \rangle.$
%
For $\pi \in \Pi$, abbreviate $\chi_S(\bv(\pi))$ and $\bchi(\bv(\pi))$ as $\chi^{\pi}_S$ and $\bchi^\pi$ respectively.




\vspace{2mm}
\noindent
{\bf Observed and potential outcomes.}
%
We refer to $Y^{(\pi)}_n \in \mathbb{R}$ as the {\em potential outcome} for unit $n$ under combination $\pi$ and $Y_{n\pi} \in \{\mathbb{R} \cup \star \}$ as the {\em observed outcome}, where $\star$ indicates a missing value, i.e., the outcome associated with the unit-combination pair $(n, \pi)$ was not observed. 
%
Let $\bY = [Y_{n\pi}] \in \{\mathbb{R} \cup \star \}^{N \times 2^p}.$
%
Let $\mathcal{D} \subset [N] \times [2^p],$ refer to the subset of unit-combination pairs we do observe.
%
That is, 
\begin{equation}\label{eq:SUTVA}
Y_{n\pi} = 
%
\begin{cases}
%
Y^{(\pi)}_n, & \text{if $(n, \pi) \in \mathcal{D}$}\\
%
\star, & \text{otherwise}.
%
\end{cases}
%
\end{equation}
%
Note that \eqref{eq:SUTVA} implies stable unit treatment value assignment (SUTVA)  holds.
%
Let $\Pi_S \subseteq \Pi$ denote a subset of combinations. 
%
For a given unit $n$, let $\bY_{\Pi_S,n} = [Y_{n\pi_i} : \pi_i \in \Pi_S] \in \{\mathbb{R} \cup \star \}^{|\Pi_S|}$ represent the vector of observed outcomes for all $\pi \in \Pi_S$. %{\color{red} Why is this in $\mathbb{R}$, can't things be missing?}
%
Similarly, let $\bY_n^{(\Pi_S)} = [Y^{(\pi_i)}_{n} : \pi_i \in \Pi_S] \in \mathbb{R}^{|\Pi_S|}$ represent the vector of potential outcomes. Additionally, denote $\bchi(\Pi_S) = [\bchi^{\pi_i} : \pi_i \in \Pi_S] \in \{-1, 1\}^{|\Pi^S| \times 2^p }$.

%, let $\bv(\Pi_S) = [\bv(\pi_i) : \pi_i \in \Pi_S] \in \mathbb{R}^{|\Pi_S| \times p }$, and 

%
%Let $I^{S} \subset [N]$ denote the subset of units, for which we observe their outcomes under $\Pi_S$, i.e., for $n \in I^{S}, \pi \in \Pi_S$ we have $(n, \pi) \in \mathcal{D}$. 
%
%Let $N_{S} = |I^{S}|$. 
%
%Denote {\color{red} $\bY_{\Pi_S,I^S} = [\bY_{\Pi_S,j}: j \in I^S] \in \{\mathbb{R} \cup \star \}^{N_{S} \times |\Pi_S|}$}. 
%Ugh
%Let $\bY^{(\Pi_S)}_{I^S}$ be defined analogously with respect to the potential outcomes rather than the observed outcomes. 
%

\subsection{Model \& Target Causal Parameter}\label{sec:causal_model}

\begin{assumption} [Potential Outcome Model]
\label{ass:observation_model} 
%
For any unit-combination pair $(n,\pi)$, we assume it has the following representation,
%
\begin{equation}\label{eq:observed_outcome}
%
Y^{(\pi)}_{n} = \langle \bm{\alpha}_{n}, \bm{\chi}^\pi \rangle + \epsilon_n^{\pi},
%
\end{equation}
%
where recall $\balpha_n \in \mathbb{R}^{2^p}, \bm{\chi}^\pi \in \{-1, 1\}^{2^p}$ are the Fourier coefficients and characters, respectively.
%
We assume the following properties: (a) low-rank property: $\mathcal{A} = [\balpha_{n}]_{n \in [N]}$ has rank $r \in [\min\{N, 2^p\}]$; (b) sparsity: $\balpha_{n}$ is $s$-sparse (i.e. $\lVert \bm{\alpha}_n \rVert_0 \leq s$, where $s \in [2^p]$) for every unit $n \in [N]$; (c) $\epsilon^{\pi}_n$ is a residual term specific to $(n,\pi)$, and we assume $\E[\epsilon_n^{\pi} ~ | ~ \mathcal{A}] = 0$. 
% %
% \begin{enumerate}
% %
% \item [(a)] Low-rank property: $\mathcal{A} = [\balpha_{n}]_{n \in [N]}$ has rank $r \in [\min\{N, 2^p\}]$.
% %
% \item [(b)] Sparsity: $\balpha_{n}$ is $s$-sparse (i.e. $\lVert \bm{\alpha}_n \rVert_0 \leq s$) for every unit $n \in [N]$
% %
% \item [(c)]  $\epsilon^{\pi}_n$ is a residual term specific to $(n,\pi)$, and we assume $\E[\epsilon_n^{\pi} ~ | ~ \mathcal{A}] = 0$. 
% %
% \end{enumerate}
%
\end{assumption}
%
Note that $Y^{(\cdot)}_{n}$ can be viewed as a function from $\{-1, 1\}^p \to \mathbb{R}$.
%
Given our discussion on Fourier representations of Boolean functions in Section \ref{sec:notation}, $Y^{(\pi)}_{n}$ can be represented as $\langle \bm{\alpha}_n, \bm{\chi}^\pi \rangle$ {\em without loss of generality}. 
%
Hence the assumption really is that $\balpha_n$ is $s$-sparse and $\mathcal{A}$ is rank-$r$. 
%
$\epsilon_n^{\pi}$ is the residual from this sparse and low-rank approximation, and it serves as the source of uncertainty in our model. 
%
Note that conditional on $\E[\epsilon_n^{\pi} ~ | ~ \mathcal{A}] = 0$, the matrix $\E[\bY_N^{(\Pi)}] = [\E[\bY_n^{(\Pi)}]: n \in [N]] \in \mathbb{R}^{2^p \times N}$ also is rank $r$, where the expectation is defined with respect to $\epsilon_n^{\pi}$.
%

The low-rank property places \emph{structure across units}; that is, we assume there is sufficient similarity across units so that $\E[\bY_n^{(\Pi)}]$ for any unit $n$ can be written as a linear combination of $r$ other rows of $\E[\bY_N^{(\Pi)}]$. %\anishcomment{Has $\E[Y_N^{(\Pi)}]$ been defined?} 
%
A special case of this property is to assume there are exactly $r$ different kinds of units, i.e., $\balpha_n$ is one of exactly $r$ different possibilities, an example we discuss in detail below. 
%
The low-rank assumption is commonly made to place structure across units, and has found much use in matrix completion and its related applications (e.g., recommendation engines). 

The sparsity assumption establishes \emph{unit-specific} structure; that is, we assume that the potential outcomes for a given user only depend on a small subset of the functions $\{\chi_S: S \subset [p] \}$. 
%
We emphasize that this subset of functions can be different across units.
%
As discussed in Section \ref{sec:related_work}, sparsity is commonly assumed when studying the learnability of Boolean functions. 
%
In say the context of recommendation engines, sparsity implies that the ratings for a combination of goods may only depend on a small number of items within that set.
%
Sparsity is also commonly assumed, at least implicitly, in factorial design experiments, where analysts typically only include pairwise interaction effects between interventions and ignore higher-order interactions \citep{george2005statistics}.
%
We discuss applications of combinatorial inference in greater detail in Section \ref{sec:combinatorial_inference_applications}. 
%In Appendix \ref{sec:combinatorial_inference_applications}, we expand on our discussion of how many of classical applications fit within our potential outcome model (Assumption \ref{ass:observation_model}). 
%

\vspace{2mm}
\noindent 
\textbf{Target causal parameter.} 
%
For any given unit $n \in [N]$ and combination $\pi \in \Pi$, we aim to estimate $\E[Y^{(\pi)}_{n} \mid \mathcal{A}]$, where the expectation is with respect to $\epsilon_n^{\pi}$, and we condition on the set of Fourier coefficients $\mathcal{A}$.
%





  
 



 
%\vspace{1mm}
%\noindent 
%\textbf{Examples of models that fit Assumption \ref{ass:observation_model}.} In Appendix \ref{sec:combinatorial_inference_applications}, we show how many classical problems and applications such as factorial design experiments, recommendations systems, and computing local (i.e., unit-specific) feature importances for ML models fit under 

%\abhicomment{We should have a list of applications and a corresponding discussion where combinatorial inference would be useful (i.e., factorial design, recommendation systems, matching markets, feature importance)}




