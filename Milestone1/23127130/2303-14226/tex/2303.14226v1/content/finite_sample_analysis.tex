\section{\method~Theoretical Analysis}\label{sec:theoretical_analysis}
%
Below, we establish the statistical properties of \method. 
%
In section \ref{subsec:finite_sample_analysis_assumptions}, we state the additional assumptions alongside their interpretation required for our theoretical results. In Section \ref{subsec:finite_sample_consistency}, we establish finite-sample consistency of \method. 
%
In Section \ref{subsec:sample_complexity_synth_combo}, we discuss our theoretical results and the required sample-complexity of \method~to estimate our causal parameter. 

\subsection{Additional Assumptions}\label{subsec:finite_sample_analysis_assumptions}
%
\begin{assumption}[Boundedness of Potential Outcomes]\label{ass:boundedness_potential_outcome}
We assume that $\E[Y_{n}^{(\pi)}] \in [-1,1]$ for any unit-combination pair $(n,\pi)$.
\end{assumption}
%
\begin{assumption}[Sub-Gaussian Noise]\label{ass:subgaussian_noise}
Conditioned on $\mathcal{A}$, for any unit-combination pair $(n,\pi)$, $\epsilon_n^{\pi}$ are independent zero-mean sub-Gaussian random variables with $\text{Var}[\epsilon_n^{\pi} ~ | ~  \mathcal{A}] \le \sigma^2$ and $\lVert \epsilon_n^{\pi} ~ | ~ \mathcal{A} \rVert_{\psi_2} \leq C\sigma$ for some constant $C > 0$. 
\end{assumption}
%
\begin{assumption} [Incoherence of Donor Fourier characteristics]
\label{ass:incoherence}
For every unit $u \in \mathcal{I}$, we assume $\bchi(\Pi_u)$ satisfies the incoherence condition:
$
\left\lVert \frac{\bchi(\Pi_u)^T\bchi(\Pi_u)}{|\Pi_u|} - \mathbf{I}_{2^p}\right\rVert_{\infty} \leq \frac{C'}{s}, 
$ for a universal constant $C' >0$. 
\end{assumption}
%
To define our next set of assumptions, we introduce necessary notation. For any subset of combinations $\Pi_S \subset \Pi$, let $\E[\bY_{\mathcal{I}}^{(\Pi_S)}] = [\E[\bY_u^{(\Pi_S)}]: u \in \mathcal{I}] \in \mathbb{R}^{|\Pi_S| \times |\mathcal{I}|}$.
%
\begin{assumption} [Donor Unit Balanced Spectrum]\label{ass:balanced_spectrum} For a given unit $n \in [N] \setminus \mathcal{I}$, let $r_n$ and $s_{1} \ldots s_{r_n}$ denote the rank and non-zero singular values of $ E[\bY_{\mathcal{I}}^{(\Pi_n)} \ | \ \mathcal{A}] $ respectively. Then, we assume that the singular values are well-balanced, i.e., for universal constant  $c,c' > 0$, we have that $s_{r_n}/s_{1} \geq c$, and $\lVert E[\bY^{(\Pi_n)}_{\mathcal{I}} ~ | ~ \mathcal{A} ]\rVert^2_F \geq c'|\Pi_n||\mathcal{I}|$
\end{assumption}
%For every subset of interventions $\Pi_S \in \Pi$, let $1 \leq r_S \leq r$ and $s_{1} \ldots s_{r_S}$ denote the rank and non-zero singular values of $E[\bY^{(\Pi_S)}_{I^S} ~ | ~ \mathcal{A}]$ respectively. Then, we assume that the singular values are well-balanced i.e., for universal constants $c,c' > 0$, we have that $s_{r_S}/s_{1} \geq c$, and $\lVert E[\bY^{(\Pi_S)}_{\mathcal{I}} ~ | ~ \mathcal{A} ]\rVert^2_F \geq c'N_{S}|\Pi_S|$
%
\begin{assumption} [Subspace Inclusion]\label{ass:rowspace_inclusion} 
For a given unit $n \in [N] \setminus \mathcal{I}$ and intervention $\pi \in \Pi \setminus \Pi_n$,
assume that $\E[\bY^{(\pi)}_{\mathcal{I}}]$ lies within the row-span of $\E[\bY_{\mathcal{I}}^{(\Pi_n)}]$
\end{assumption}
%
Assumption \ref{ass:boundedness_potential_outcome} and \ref{ass:subgaussian_noise} are self-explanatory.
%
Assumption \ref{ass:incoherence} is necessary for finite-sample consistency when estimating $\balpha_n$ via the Lasso estimator \eqref{eq:Lasso_estimator}, and is commonly made when studying the Lasso \citep{rigollet2015high}. 
%
Incoherence can also seen as a inclusion criteria for a unit $n$ to be included in the donor set $\mathcal{I}$.  
%
Lemma 2 in \cite{negahban2012learning} shows that the Assumption \ref{ass:incoherence} is satisfied with high probability if $\Pi_u$ is chosen uniformly at random and grows as $\omega(s^2p)$. 
%
Assumption \ref{ass:balanced_spectrum} requires that the non-zero singular values of $\E[\bY_{\mathcal{I}}^{(\Pi_n)} ~ | ~ \mathcal{A}]$ are well-balanced. 
%
Such an assumption is commonly made while studying PCR \cite{agarwal2019robustness,agarwal2020synthetic}, and is also standard within the econometrics literature \cite{bai2021matrix,fan2018eigenvector}. 
%
It can also be empirically validated by plotting the spectrum of $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$; if the spectrum of $\hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}]$ displays a natural elbow point in its singular spectrum, then Assumption \ref{ass:balanced_spectrum} is likely to hold.  
%
Assumption \ref{ass:rowspace_inclusion} is also commonly made when analyzing PCR \cite{agarwal2020principal,agarwal2020synthetic,agarwal2021causal}.
%
It can be thought of as a ``causal transportability'' condition from the model learnt using $\Pi_n$ to the interventions $\pi \in \Pi \setminus \Pi_n$. 
%
That is, subspace inclusion allows us to generalize well, and accurately estimate $\E[Y_n^{(\pi)}]$ using $\langle \hat{\E}[\bY_{\mathcal{I}}^{(\Pi_n)}], \hat{\bw}^n \rangle$. 
%
As we show in Section \ref{sec:experimental_design}, subspace inclusion holds with high probability if $|\Pi_n|$ is chosen uniformly and independently at random from $\Pi$ and has size $\omega(r\log(|\mathcal{I}|))$

\subsection{Finite Sample Consistency}\label{subsec:finite_sample_consistency}
%
The following result establishes finite-sample consistency of \method. 
%
Without loss of generality, we will focus on estimating the pair of quantities $(\E[Y_u^{(\pi)}],\E[Y_n^{(\pi)}])$ for a given donor unit $u \in \mathcal{I}$, and non-donor unit $n \in [N] \setminus \mathcal{I}$ under treatment assignment $\pi \in \Pi$.
%
To simplify notation, we will use $O_p$ notation which is a probabilistic version of big-$O$ notation.
%
Formally, for any sequence of random vectors $X_n$, $X_n$ = $O_p(\gamma_n)$ if, for any $\epsilon > 0$, there exists constants $c_\epsilon$ and $n_\epsilon$ such that $\mathbb{P}(\lVert X_n \rVert_2 \geq c_\epsilon\gamma_n) \leq \epsilon$ for every $n \geq n_\epsilon$. 
%
Equivalently, we say that $X_n/\gamma_n$ is “uniformly tight” or “bounded in probability”. Similarly, we define $\tilde{O_p}(\gamma_n)$ which suppresses dependence on logarithmic terms. 
%
Additionally, we will absorb dependencies on $\sigma$ and logarithmic factors into the constant within $\tilde{O}_p\left(\cdot\right)$. 
%
\begin{theorem} [Finite Sample Consistency of \method]
\label{thm:potential_outcome_convergence_rate}
Conditioned on $\mathcal{A}$, let Assumptions \ref{ass:observation_model}--\ref{ass:donor_set_identification}, and \ref{ass:boundedness_potential_outcome}--\ref{ass:rowspace_inclusion} hold. Then, the following statements hold. 
\begin{itemize}
    \item [(a)] For the given donor unit-combination pair $(u,\pi)$, let the Lasso regularization parameter satisfy $\lambda_u = \Omega(\sqrt{\frac{p}{|\Pi_u|}})$. Then, we have 
        \begin{equation*}
            |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]| = \Tilde{O}_p\left(\sqrt{\frac{s^2p}{|\Pi_u|}} \right)
        \end{equation*}
    \item [(b)] For the given unit-combination pair  $(n,\pi)$ where $n \in [N] \setminus \mathcal{I}$, let $\kappa = \text{rank}(\E[\bY_{\mathcal{I}}^{(\Pi_n)}]) \coloneqq r_{n}$. Then, provided that  $ \min_{u \in \mathcal{I}} |\Pi_u| \coloneqq M = \omega(r_n^2s^2p)$, we have
        \begin{equation*}
            \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right| = \Tilde{O_p}\left(  \frac{r^2_n\sqrt{s^2p}}{\sqrt{M \times \min\{|\Pi_n|,|\mathcal{I}|\}}} + \frac{r^2_ns^2p\sqrt{|\mathcal{I}|}}{M} + \frac{r_n}{|\Pi_n|^{1/4}}\right)
        \end{equation*}
\end{itemize}
    
\end{theorem}

\noindent We discuss the necessary conditions on the number of observations required for $\Pi_n$ and $\min_{u \in \mathcal{I}} |\Pi_u| = M$ to achieve consistency in estimating the potential outcome for the given  pair of quantities $(\E[Y_u^{(\pi)}],\E[Y_n^{(\pi)}])$. 
%
To simplify the discussion, we assume that $\min\{|\Pi_n|,|\mathcal{I}|\} = \mathcal{|I|}$. 
%
This condition can be easily be enforced in practice by picking a subset of donor units such that $|\mathcal{I}| \leq |\Pi_n|$ when performing PCR (i.e., step 2 of \method)
%
Given this assumption, it can be easily verified $|\Pi_n|$ and $M$ need to scale as $\omega(r_n^4)$ and $\omega(r^4_ns^2p)$ respectively to achieve $\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1)$
%
Next, we present a corollary that discusses how quickly the parameters $r_n,s,p$ can grow with the number of observations $|\Pi_n|,M$.

\begin{corollary}\label{cor:potential_outcome_convergence_rate_simplified}
Let the set-up of Theorem \ref{thm:potential_outcome_convergence_rate} hold. If the following hold, 
\begin{itemize}
    \item [(a)] $r_n = o(|\Pi_n|^{1/4})$
    \item [(b)] $s =  o\left(\sqrt{M/(p\max\{|\Pi_n|,|\mathcal{I}|\}})\right) $
\end{itemize}
then, we have 
$
\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1) 
$ as $M,|\Pi_n|,|\mathcal{I}| \rightarrow \infty$. 
\end{corollary}

Corollary \ref{cor:potential_outcome_convergence_rate_simplified} provides conditions on how the parameters $s,r_n$ can scale with the number of observations to achieve consistency. 
%
That is, Corollary \ref{cor:potential_outcome_convergence_rate_simplified} reflects the maximum ``complexity'' allowed for a given sample size. 
%
% These conditions also serve as empirical guidance for how to tune the parameters $r_n,s$ in practice with the number of observations available when performing both lasso and PCR. 
%

\vspace{1mm}
\noindent Next, we present an analogous corollary  that assumes the horizontal regression is done via CART instead of the Lasso. 
We defer the discussion on the CART estimator and required assumptions to Appendix \ref{subsec:horizontal_regression_CART}. 

\begin{corollary}
\label{cor:potential_outcome_convergence_rate_CART}
Let the set-up of Theorems \ref{thm:potential_outcome_convergence_rate}, \ref{thm:CART_convergence_rate}, and condition (a) of Corollary \ref{cor:potential_outcome_convergence_rate_simplified} hold. Then, if $s =  o\left(\sqrt{M/(\max\{|\Pi_n|,|\mathcal{I}|\}})\right)$, we have 
$$\max\left(|\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]|, \ |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]|\right) =\tilde{o}_p(1)$$
\end{corollary} 

\noindent Corollary \ref{cor:potential_outcome_convergence_rate_CART} shows CART exploits the regularity conditions placed on the potential outcomes allows the sparsity $s$ to scale more quickly (i.e., by a factor of $p$) while achieving consistency. That is, CART allows \method~to
achieve consistency with $M = \omega(r_n^4s^2)$ samples. The superior sample complexity of CART (under the additional structural assumptions on $\E[Y_n^{(\pi)}]$) reflect the flexibility of \method. 

%{\color{red} should this be $r_n$? what happened to $\sqrt{\mathcal{I}}$?}

\subsection{Sample Complexity}
\label{subsec:sample_complexity_synth_combo}
We discuss the sample complexity of \method~to estimate all $N \times 2^p$ causal parameters, and compare it to that of other methods.
%
To ease our discussion, we will ignore dependence on logarithmic factors and $\sigma$. 
%
Again, we assume that the horizontal regression is done via the Lasso. 

\vspace{2mm}
%
Note that even if all potential outcomes $Y_n^{(\pi)}$ were observed for all unit-combination pairs, consistently estimating $\E[\bY_N^{(\Pi)}]$ is not trivial. 
%
This is because, we only get to observe a single and noisy version $Y_{n\pi} = \langle \balpha_n, \bchi^{\pi} \rangle + \epsilon_n^{\pi}$. 
%
Hypothetically, if we observe $K$ independent samples of $Y_{n\pi}$ for a given $(n,\pi)$, denoted by $Y^{1}_{n\pi}, \ldots, Y^{K}_{n\pi}$, the maximum likelihood estimator would be the empirical average $\frac{1}{K}\sum^K_{i=1}Y^{i}_{n\pi}$. 
%
In this case, the empirical average would concentrate around $\E[Y_n^{(\pi)}]$ at a rate $O(1/\sqrt{K})$ and hence would require $K = \Omega(\delta^{-2})$ samples to estimate $\E[Y_n^{(\pi)}]$ within error $O(\delta)$. 
%
Therefore, for all $N \times 2^p$ causal parameters, this naive (unimplementable) solution would require $N \times 2^p \times \delta^{-2}$ observations.
%

On the other hand, \method~produces consistent estimates of the potential outcome despite being given {\em at most only a single noisy sample} of each potential outcome. 
%
As the discussion after Theorem \ref{thm:potential_outcome_convergence_rate} shows, if $|\mathcal{I} \leq |\Pi_n|$, \method~requires $|\mathcal{I}| \times r^4s^2p/\delta^2$ observations for the donor set, and $(N - |\mathcal{I}|) \times r^4/\delta^4$ observations for the non-donor units to achieve an estimation error of $O(\delta)$ for all $N \times 2^p$ causal parameters. 
%
That is, \method~requires at most $N \times r^4/\delta^4$ + $|\mathcal{I}| \times r^4s^2p/\delta^2$ observations to estimate $\E[Y_n^{(\pi)}]$ with error $O(\delta)$ for all unit-combination pairs $(n,\pi)$. 
%
Of course, this is assuming that the key assumptions (see Section \ref{subsec:finite_sample_analysis_assumptions}) are satisfied. 
%
Hence, we have that the number of observations required to achieve an estimation error of $O(\delta)$ scales as $O\left(\text{poly}(r)/\delta^4 \times \left(N + s^2p \right) \right)$. 
%

\vspace{2mm}
\noindent \textbf{Comparison to Other Methods.} We compare the sample complexity of our method to other algorithms.

\vspace{1mm}
\noindent \emph{Horizontal regression:} An alternative algorithm would be to learn an individual model for each unit $n \in [N]$. 
%
That is, run a separate horizontal regression via the Lasso for every unit.
%
As implied by the analysis of \cite{negahban2012learning}, this alternative algorithm has sample complexity that scales at least as $O(N \times s^2p/\delta^2)$  rather than $O\left(\text{poly}(r)/\delta^4  \times \left(N + s^2p \right) \right)$ required by \method.  
%
This alternative algorithm suffers because it does not utilize any structure across units (i.e., the low-rank property of $\mathcal{A}$), whereas \method~captures the similarity between units via PCR. 
%

\vspace{1mm}
\noindent \emph{Matrix completion:} \method~can be thought of as a matrix completion method; estimating $\E[Y_n^{(\pi)}]$ is equivalent to imputing $(n,\pi)$-th entry of the observation matrix $\bY \in \{\mathbb{R} \cup \star\}^{N \times 2^p}$. 
%
Under the low-rank property (Assumption \ref{ass:observation_model}(b)) and various models of missingness (i.e., observation patterns), recent works on matrix completion \cite{candes2010matrix,ma2019missing,agarwal2021causalmatrix} (see Section \ref{sec:related_work} for an overview) have established that estimating $\E[Y_n^{(\pi)}]$ to an accuracy $O(\delta)$ requires at least $O\left(\text{poly}(r/\delta) \times \left(N + 2^p \right)\right)$ samples.
%
This is because matrix completion techniques do not leverage additional structural information such as the Fourier expansion of Boolean functions and the resulting sparsity in $\balpha_n$.
%
Moreover,
matrix completion results typically report error in the Frobenius norm, whereas we give entry-wise guarantees. 
%
This leads to an extra factor of $s$ in our analysis as it requires proving convergence for $\lVert \hat{\balpha_n} - \balpha_n \rVert_{1}$ rather than $\lVert \hat{\balpha_n} - \balpha_n \rVert_{2}$. 
%

\vspace{1mm}
\noindent
Hence, \method~combines the best of both approaches by leveraging both the structure of the potential outcomes \emph{and} the similarity across units to estimate potential outcomes $\E[Y_n^{(\pi)}]$.
\vspace{1mm}

\noindent \textbf{Natural Lower Bound on Sample Complexity.} We provide an informal discussion on the lower bound on the number of samples required to estimate all $N \times 2^p$ potential outcomes. 
%
To do so, consider the matrix of Fourier coefficients $\mathcal{A}$ which is of rank $r$, and where every row (i.e., $\balpha_n$) is $s$-sparse. 
%
As established in Lemma \ref{lem:number_nonzeros}, this means that $\mathcal{A}$ has at most $rs$ non-zero columns. 
%
Counting the parameters in the singular value decomposition of $\mathcal{A}$, we observe that in fact only $r\times (N + rs)$ free parameters are required to be estimated. 
%
Hence, a natural lower bound on the sample complexity for any estimation algorithm scales as $O(Nr + r^2s)$.  
%
This implies \method~is only sub-optimal in terms of sample-complexity by a factor (ignoring logarithmic factors) of $sp$ and $\text{poly}(r)$.  
%
As discussed earlier, an additional factor of $s$ can be removed if we instead focus on deriving bounds for Frobenius norm errors.
%
It remains as interesting future to work derive estimation procedures that are able to achieve this lower bound.