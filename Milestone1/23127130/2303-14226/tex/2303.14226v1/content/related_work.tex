\section{Related Work}
\label{sec:related_work}

%{\color{red} Anish: revise}

%\vspace{1mm}
\noindent
{\bf Learning over Combinations.}
%the set of causal effects for
To place structure on the space of combinations, we use tools from the theory of learning (sparse) Boolean functions, in particular the Fourier transform. 
%
Sparsity of the Fourier transform as a complexity measure was proposed by \citet{karpovsky1976finite}, and was used by \cite{brandman1990spectral} and others to characterize and design learning algorithms for low-depth trees, low-degree polynomials, and small circuits. Learning Boolean functions is now a central topic in  learning theory, and is closely related to many important questions in ML more broadly; see e.g.~\cite{mossel2003learning} for discussion of the $k$-Junta problem and its relation to relevant feature selection. We refer to \citet[Chapter 3]{o2008some} for further background on this area. 
%
In this paper, we focus on general-purpose statistical procedures for learning sparse Boolean functions with noise. We build on the work of \cite{negahban2012learning} on variants of the Lasso procedure, and the works of \cite{breiman2017classification,syrgkanis2020estimation,klusowski2020sparse,klusowski2021universal} and others on CART. In particular, we highlight the works of \cite{negahban2012learning} and \cite{syrgkanis2020estimation} which showed that Lasso and CART can be used to efficiently learn sparse Boolean functions, an essential property in our setting since the dimension of the function class grows exponentially.
%

\vspace{2mm}
\noindent {\bf Matrix Completion.}  We build on the observation that imputing counterfactual outcomes in the presence of a latent factor structure can be equivalently expressed as low-rank matrix completion \citep{bai2019matrix, athey2021matrix, agarwal2020synthetic}.
%
The observation that low-rank matrices may typically be recovered from a small fraction of the entries by nuclear-norm minimization has had a major impact on modern statistics \cite{candes2010power,recht2011simpler,candes2012exact}. 
%
In the noisy setting, proposed estimators have generally proceeded by minimizing risk subject to a nuclear-norm penalty, such as in the SoftImpute algorithm of \citep{mazumder2010spectral}, or minimizing risk subject to a rank constraint as in the hard singular-value thresholding (HSVT) algorithms analyzed by \cite{keshavan2009matrix,keshavan2010matrix,gavish2014optimal, chatterjee2015matrix}.
%
We refer the reader to \cite{ieee_matrix_completion_overview, nguyen2019low} for a comprehensive overview of this vast literature. 
%


\vspace{2mm}
\noindent
{\bf Econometrics/causal inference.}
%
There is a rich literature on how to simultaneously learn the effects of multiple treatments, and, separately, how to learn personalized treatment effects for heterogeneous units. 
%
The latter problem is often formulated in terms of the ``conditional average treatment effect,'' see \cite{abrevaya2015estimating}. From this perspective, one learns a function $f(X_i)$ which predicts the causal effect (difference between counterfactual outcome) conditional upon individual features $X_i$.
%
There is also a rich literature dating back to \cite{rubin1976inference} that studies when and how a researcher can perform valid causal inference from observational data, building on the potential outcomes framework introduced by \cite{neyman1935statistical}. This problem is of particular importance in the social sciences and in medicine, where experimental data is limited, and has led to several promising approaches including instrumental variables (see \cite{angrist1996identification}), difference-in-differences, regression discontinuity, and others; see \cite{imbens2015causal} for an overview.
%
Of particular interest to us here is the ``synthetic control'' method of \cite{abadie2010synthetic}, which exploits an underlying factor structure to effectively ``share'' counterfactual information between treatment and control units.
%
Building on \cite{abadie2010synthetic}, recent work has shown that the same underlying structure can be used to estimate treatment effects with multiple treatments \emph{and} heterogeneous units, with or without unmeasured confounding. The resulting framework, called ``synthetic interventions'' (SI), uses the factor representation to efficiently share information across similar (yet distinct) units and treatments \citep{agarwal2020synthetic}. 
%
We generalize the SI framework to settings where the number of treatments is very large and most treatments have no units that receive it, but there is structure across combinations of interventions. In doing so, we enable its use in several highly practical cases where multiple treatments are delivered simultaneously, such as recommender systems, medical treatment regimens, and inference from factorial designs.






%
%Here we build on recent results of \citet{agarwal2019robustness} that refine the analysis of HSVT algorithms to allow recovery in stronger norms than the average entry-wise square loss.

%Finally, recent work on low-rank matrix estimation has studied how to use additional data associated to each row or column to improve accuracy \citep{xu2013speedup, chiang2015matrix}. Although we do not build directly upon this work, our results are closely related: our procedure uses data associated to each column (coming from the combinatorial structure of the column set) to further ''regularize’’ the matrix estimation problem, dramatically lowering the sample complexity. 


%%% CART Snippet 

%Due to the complexity of analyzing the greedy CART procedure, much earlier work proving consistency or generalization upper bounds for the algorithm typically modify the splitting criterion in the algorithm to ensure that the mesh of the learnt partition shrinks to zero \cite{breiman2004consistency,biau2008consistency,biau2012analysis,wager2018}. Scornet et al. \cite{scornet2015} was the first to overcome this by replacing the fully nonparametric regression model with an additive regression model \cite{friedman2001greedy}. Klusowski \cite{klusowski2020,klusowski2021universal} later extended this analysis to sparse additive regression models, and in so doing, was able to show that CART is universally consistent (irrespective of the underlying covariate distribution), even when the number of features grows exponentially in the number of training samples. Syrgkanis and Zampetakis \cite{syrgkanis2020estimation} provided a similar message, but adopted a different approach, and proved generalization bounds in a high-dimensional setting when assuming that the true regression function had a type of submodularity property.  They proved theoretically that assuming sparsity of the regression function


%and some other generative conditions, CART is able to overcome the curse of dimensionality by prioritizing splits on relevant featuresFor instance, under their assumption of strong sparsity, they showed that the expected squared error risk for CART is bounded by $O(2^s\log(d)/n)$, where $d$ is the total number of features, $s$ is the number of relevant features, while $n$ is the sample size.


% In particular, we highlight the work of \citet{klusowski2020sparse,klusowski2021universal} who establishes universal consistency of CART in a high-dimensional setting with non-uniform feature distribution, and \citet{syrgkanis2020estimation} who demonstrated that regression trees and forests are capable of greedily and adaptively learning sparse Boolean polynomials with a certain sub-modularity property. 
% For instance, under \citet{syrgkanis2020estimation} showed that under strong sparsity, expected squared loss for CART is bounded by $O(2^s\log(d)/n)$, where $d$ is the total number of features, $s$ is the number of relevant features, while $n$ is the sample size. 
%Greedy procedures such as CART, which quickly identify important features, are essential in our setting since the linear dimension of the function class is exponential in the number of interventions. 

% Learning Boolean functions is now a major topic in computer science and mathematics, particularly in algorithmic learning theory, and the ideas in this paper only scratch the surface. 