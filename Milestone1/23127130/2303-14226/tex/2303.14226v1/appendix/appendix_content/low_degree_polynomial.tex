 Next, we show how we can adapt \method~to learn low-degree polynomials more efficiently. To do so, we introduce some necessary notation, for $\bx \in \{-1,1\}^p$, let $\bchi_d(\bx) = [\chi_S(\bx):  S \subset [p], |S| \leq d ] \in \mathbb{R}^{p_d}$. Therefore, we see that any $d$-degree polynomial $f$ can be equivalently expressed as $\langle \balpha, \bchi^d \rangle$. For a combination $\pi \in \Pi$, denote $\bchi_d(v(\pi))$ as $\bchi^{\pi}_d$. For a subset of interventions $\Pi_S \in \Pi$, denote $\bchi_d (\Pi_S) = [\bchi_{d}^{\pi_i}: \pi_i \in \Pi_S] \in \mathbf{R}^{|\Pi_S| \times p_d}$. Given this notation, we now present how \method~can be adapted for low-degree polynomials. 

\vspace{2mm}
%
\noindent \textbf{\method~for low-degree polynomials}. For every donor unit $u$ in the donor set $\mathcal{I}$, we now solve Lasso as follows instead of \eqref{eq:Lasso_estimator}: 
\begin{align}\label{eq:low_degree_Lasso_estimator}
 \hat{\balpha}_u=  \argmin_{\balpha} \ \frac{1}{|\Pi_u|}\lVert \bY_{\Pi_u} - \bchi_d(\Pi_u)\balpha \rVert^2_2 + \lambda_u \lVert \bm{\alpha} \rVert_1
\end{align}
We then repeat the rest of \method~as described in Section \ref{sec:estimator_descripton}. Next, we adapt our finite-sample consistency results if the potential outcomes are low-degree Boolean polynomials instead. 
%
\begin{corollary} [Finite Sample Consistency for low-degree polynomials]
\label{cor:potential_outcome_convergence_rate_low_degree}
Conditioned on $\mathcal{A}$, let Assumptions \ref{ass:observation_model}--\ref{ass:donor_set_identification}, and \ref{ass:boundedness_potential_outcome}--\ref{ass:rowspace_inclusion} hold. Then, the following statements hold. 
\begin{itemize}
    \item [(a)] For the given donor unit-combination pair $(u,\pi)$, let the Lasso regularization parameter satisfy $\lambda_u = \Omega(\sqrt{\frac{p_d}{|\Pi_u|}})$. Then, we have 
        \begin{equation*}
            |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]| = \Tilde{O}_p\left(\sqrt{\frac{s^2\log(p_d)}{|\Pi_u|}} \right)
        \end{equation*}
    \item [(b)] For the given unit-combination pair  $(n,\pi)$ where $n \in [N] \setminus \mathcal{I}$, let $\kappa = \text{rank}(\E[\bY_{\mathcal{I}}^{(\Pi_n)}]) \coloneqq r_{n}$. Then, provided that  $ \min_{u \in \mathcal{I}} |\Pi_u| \coloneqq M = \omega(r_n^2s^2\log(p_d))$, we have
        \begin{equation*}
            \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right| = \Tilde{O_p}\left(  \frac{r^2_n\sqrt{s^2p_d}}{\sqrt{M \times \min\{|\Pi_n|,|\mathcal{I}|\}}} + \frac{r^2_ns^2p_d\sqrt{|\mathcal{I}|}}{M} + \frac{r_n}{|\Pi_n|^{1/4}}\right)
        \end{equation*}
\end{itemize}
    
\end{corollary}
