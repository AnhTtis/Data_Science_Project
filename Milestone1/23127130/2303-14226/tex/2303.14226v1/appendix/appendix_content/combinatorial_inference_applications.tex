\section{Combinatorial Inference Applications}
\label{sec:combinatorial_inference_applications}
In this section, we discuss how classical applications and models fit within our proposed potential outcome model. In particular, we first focus on how \method~can be adapted to learn low-degree Boolean polynomials and then show how two important applications, factorial design experiments and recommendations systems, both fit under this paradigm. 
\vspace{2mm}

\subsection{Combinatorial Inference for Low-degree Boolean Polynomials}
\label{subsec:low_degree_polynomials}

A special instance of sparse Boolean functions is a low-degree polynomial which we define as follows:

\begin{definition}[ Low-degree Boolean polynomials]
\label{def:boolean_polynomial}
Fix a positive integer $d \leq p$. Then we say $f : \{-1,1\}^p \rightarrow \mathbb{R}$ is a $d$-degree polynomial if its Fourier transform satisfies the following for any input $\bx \in \{-1,1\}^p$
\begin{equation}
\label{eq:low_degree_fourier_transform}
 f(\bx) = \sum_{S \subset |p|, |S| \leq d} \alpha_S \chi_S (\bx)
\end{equation}
\end{definition}
In this case, we see that $s  \leq p' \coloneqq \sum^d_{i=0} \binom{p}{i}$. Next, we show how we can adapt \method~to learn low-degree polynomials more efficiently. To do so, we introduce some necessary notation, for $\bx \in \{-1,1\}^p$, let $\bchi_d(\bx) = [\chi_S(\bx):  S \subset [p], |S| \leq d ] \in \mathbb{R}^{p'}$. Therefore, we see that any $d$-degree polynomial $f$ can be equivalently expressed as $\langle \balpha, \bchi^d \rangle$. For a combination $\pi \in \Pi$, denote $\bchi_d(v(\pi))$ as $\bchi^{\pi}_d$. For a subset of interventions $\Pi_S \in \Pi$, denote $\bchi_d (\Pi_S) = [\bchi_{d}^{\pi_i}: \pi_i \in \Pi_S] \in \mathbf{R}^{|\Pi_S| \times 2^d}$. Given this notation, we now present our \method~can be adapted for low-degree polynomials. 

\vspace{2mm}
%
\noindent \textbf{\method~for Low-degree polynomials}. For every donor unit $u$ in the donor set $\mathcal{I}$, we now solve Lasso as follows instead of \eqref{eq:Lasso_estimator}: 
\begin{align}\label{eq:low_degree_Lasso_estimator}
 \hat{\balpha}_u=  \argmin_{\balpha} \ \frac{1}{|\Pi_u|}\lVert \bY_{\Pi_u} - \bchi_d(\Pi_u)\balpha \rVert^2_2 + \lambda_u \lVert \bm{\alpha} \rVert_1
\end{align}
We then repeat the rest of \method~as described in Section \ref{sec:estimator_descripton}. Next, we adapt our finite-sample consistency results if the potential outcomes are low-degree Boolean polynomials instead. 
%




\subsubsection{Factorial Design}
\label{subsubsec:factorial_design}

\subsubsection{Recommendation Systems}
\label{subsubsec:recommendation_systems}

