\section{CART Horizontal Regression}
\label{sec:CART_horizontal_regression}

\subsection{CART Background}
\label{sec:CART_background}

Assume we have access to a training set $\data = \{(\bx_1,y_1) \ldots (\bx_n,y_n)\}$ where $\bx_i$ is sampled from a distribution $\mathcal{D}_x$ with support $\{-1,1\}^p$, and the responses are generated as $y = f(\bx_i) + \epsilon$.  In the context of combinations, $\bx_i$ can correspond to the binary vector $\bv(\pi)$. 

Let $S \subset [p]$ denote a subset of coordinate indices, and $\bz \in \{-1,1\}^{|S|}$.Then, we define a \emph{node} $\cell \subset \{-1,1\}^p$ as a subcube of the form $\cell(S,\bz) = \braces{\bx \in \{-1,1\}^p ~\colon~ x_j = z_j~\text{for}~j \in S}$. Let $N(\cell) \coloneqq | \braces{i :  \bx_{i} \in \cell } |$ denote the size of this set. Define the sub-cell $\cell_{j,1} = \{\bx \in \cell, x_j = 1\}$ with respect to a feature $j$.  Analogously, define the sub-cell $\cell_{j,-1} = \{\bx \in \cell, x_j = -1\}$. Furthermore, denote the mean response over the node by
$$
\hat\E_{\cell}[y] \coloneqq \hat\E[y|\bx \in \cell] = \frac{1}{N(\cell)}\sum_{\bx_i \in \cell} y_{i}.
$$


%{\color{red} What is $\mathcal{X}$, $d$, $\bx^{(i)}$?}

%A \emph{node} $\cell \subset \{-1,1\}^p$ is a subcube of the form $\cell(S,\bz) = \braces{\bx \in \{-1,1\}^p ~\colon~ x_j = z_j~\text{for}~j \in S}$, where $S \subset [p]$ is a subset of coordinate indices.  L

A \emph{partition} $\partition = \braces{\cell_1,\ldots,\cell_m}$ is a collection of nodes with disjoint interiors, whose union is the Boolean hypercube $\{-1,1\}^p$. For any $\bx \in \{-1,1\}^p$, we define $\partition(\bx)$ to be the cell of $\partition$ that contains $\bx$.  Given the training set $\data$, every partition yields an estimator $\hat f(-; \partition, \data)$ for $f$ via \emph{leaf-only averaging}: for every input $\bx$, the estimator outputs the mean response over the node containing $\bx$. In other words, we define
$$
\hat f(\bx; \partition, \data) \coloneqq \sum_{\cell \in \partition} \hat{\E}_\cell[y]~ \indicator\braces{\bx \in \cell}.
$$
    

%\begin{definition}[Expected Mean Squared Error of a partition, Equation 4.3 in \cite{syrgkanis2020estimation}]
%Define the expected mean squared error of a parititon $\partition$ of $\{-1,1\}^p$ as follows: 
%\begin{align}
%    \Bar{L}(\partition) & \coloneqq \E_{\bx \sim \mathcal{D}_x}\left[\left( f(\bx) - \E_{\bz \sim \mathcal{D}_x }[f(\bz) | z \in \partition(\bx)]\right)^2\right] \nonumber \\
%   &  = \E_{\bx \sim \mathcal{D}_x}[f^2(\bx)] - \E_{\bx \sim \mathcal{D}_x}\left[\left( \E_{\bz \sim \mathcal{D}_x }[f(\bz) | z \in \partition(\bx)] \right)^2  \right] \nonumber \\
%   & \coloneqq \E_{\bx \sim \mathcal{D}_x}[f^2(\bx)] - \Bar{V}(\partition) 
%\end{align}
%\end{definition}



%\begin{definition}[Expected Mean Squared Error of a cell, Equation 4.4 in \cite{syrgkanis2020estimation}]
%Define the expected mean squared error of a cell $\node$ in a partition $\partition$ of $\{-1,1\}^p$ as follows: 
%\begin{align}
%    \Bar{L}(\cell,\partition) & \coloneqq \E_{\bx \sim \mathcal{D}_x}\left[\left( f(\bx) - \E_{\bz \sim \mathcal{D}_x }[f(\bz) | z \in \partition(\bx)]\right)^2 | \bx \in  \cell \right] \nonumber \\
%   &  = \E_{\bx \sim \mathcal{D}_x}[f^2(\bx) \ | \ \bx \in \node] - \E_{\bx \sim \mathcal{D}_x}\left[\left( \E_{\bz \sim \mathcal{D}_x }[f(\bz) | z \in \partition(\bx)] \right)^2   | \bx \in \cell \right] \nonumber \\
%   & \coloneqq  \E_{\bx \sim \mathcal{D}_x}[f^2(\bx) \ | \ \bx \in \node ] - \Bar{V}(\cell,\partition)
%\end{align}

%\end{definition}



\noindent The CART algorithm constructs a partition $\mathcal{\hat{T}} = \braces{\cell_1,\ldots,\cell_m}$, by choosing a feature $j$ to split on at each intermediate node in a greedy fashion according to the impurity decrease split criterion defined as follows
\begin{equation*}
    \hat\Delta(\cell,j) \coloneqq \frac{1}{N(\cell)} \left( \sum_{\bx_i \in \cell}\left(y_{i} - \hat\E_\cell[y] \right)^2 - \sum_{\bx_i \in \cell_{j,1}}\left(y_{i} - \hat\E_{\cell_{j,1}}[y] \right)^2 - \sum_{\bx_i \in \cell_{j,-1}}\left(y_{i} - \hat\E_{\cell_{j,-1}}[y] \right)^2 \right).
\end{equation*}

The tree is stopped from growing upon reaching a stopping condition. For example, common stopping conditions include minimum number of samples in a node, maximum depth, or when the number of nodes exceeds a pre-sepcified threshold. 


\subsection{CART Estimator for Horizontal Regression}
\label{subsec:horizontal_regression_CART}
In this section, we show how the horizontal regression procedure can be conducted via greedy tree-based methods such as CART \citep{breiman2017classification}. We show CART can achieve better sample complexity under additional assumptions on the structure of $\E[Y_n^{(\pi)}]$. A description of how CART works can be found in Appendix \ref{sec:CART_background}. Our CART-based horizontal regression procedure consists of the following two steps. 

\vspace{2mm}
\noindent \textbf{Step 1: Feature Identification.} For each donor unit $u \in \mathcal{I}$, divide the observed interventions $\Pi_u$ equally into two sets $\Pi^{a}_u$ and $\Pi^{b}_u$. Fit a CART model $\mathcal{\hat{T}} = \{\cell_1, \ldots \cell_m \}$ on the dataset $\mathcal{D}^a_u = [\{\bv(\pi),Y_{u\pi}\}: \pi \in \Pi^a_u]$. Then, let $\hat{K} = \{\bv(\pi)_j : \bv(\pi)_j \in \mathcal{\hat{T}}\}$ denote every feature that the CART splits on. \\

\noindent \textbf{Step 2: Estimation.} For every subset $S \subset \hat{K}$, compute $\hat{\alpha}_{u,S} = \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi^b_u }Y_{u\pi}\chi^{\pi}_{S}$. We then estimate our target causal parameter as follows: 
\begin{equation}
\label{eq:CART_potential_outcome_estimate}
    \hat{\E}[Y_u^{(\pi)}] = \sum_{S \in \hat{K}}\hat{\alpha}_{u,S}\chi^{\pi}_{S}
\end{equation}

\subsection{CART Finite-Sample Analysis}

\noindent We now define the additional assumptions required for finite-sample analysis. Throughout our analysis, we follow the notation used in \cite{syrgkanis2020estimation}. 
%
%\begin{definition} [$k$-Junta]
%\label{def:k_junta} 
%A function $f : \{-1,1\}^p \to \mathbb{R}$ is a $k$-junta if there exists a set $K \subset [p]$ with $|K| = k$ such that the fourier expansion of $f$ can be represented as follows
%\begin{equation}
%    f(\bx) = \sum_{S \subset K} \alpha_{S} \chi_{S}(\bx)
%\end{equation}
%\end{definition}
%
%\noindent Therefore, in the setting of the $k$-Junta, the sparsity index $s \leq 2^k$. 
%

\begin{definition}
For a given partition $\partition$, and a cell $\node \in \partition$, define 
\begin{equation}
    \Bar{V}(\cell,\partition) = \E_{\bx \sim \mathcal{D}_x}\left[\left( \E_{\bz \sim \mathcal{D}_x }[f(\bz) | z \in \partition(\bx)] \right)^2   \ | \  \bx \in \cell \right]
\end{equation}
\end{definition}

\noindent In order to introduce the next assumption, we first introduce some additional notation. For a partition $\partition$, and cell $\node \in \partition$, define the split operator $\mathcal{S}\left(\partition,\cell,j\right)$ that outputs a partition with the cell $\cell$ split on feature $j$. That is, $\mathcal{S}\left(\partition,\cell,j\right)$ = $(\partition \setminus \{\cell\}) \cup \{\cell_{j,-1}, \cell_{j,1}\}$. The split operator for a partition $\partition$ and cell $\node$ can also be defined for a set of features $T \subset [p]$ by repeatedly splitting the cell $\node$ with respect to all the features in $T$. More formally, this can be defined inductively as follows: for all $j \in T$, let $\mathcal{S}\left(\partition,\cell,T\right) = \mathcal{S}\left(\mathcal{S}\left(\mathcal{S}(\partition, \cell,j), \cell_{j,-1}, T \setminus \{j\}\right), \cell_{j,1}, T \setminus \{j\}\right)$. Moreover, define the short-hand $\Bar{V}(\cell,T) =  \Bar{V}(\cell,\mathcal{S}\left(\partition,\cell,T\right))$

 
\begin{definition}[Strong Partition Sparsity, Assumption 4.2 in \cite{syrgkanis2020estimation}]
\label{def:strong_partition_sparsity}
A function $f: \{-1,1\}^p \to \mathbb{R}$ is $(\beta,k)$-strong partition sparse if $f$ is a $k-$junta (see \eqref{eq:k_junta}) with relevant features $K$ and the function $\Bar{V}$ satisfies: $\Bar{V}(\cell, T \cup \{j\}) - \Bar{V}(\cell, T) + \beta \leq \Bar{V}(\cell, T \cup \{i\}) - \Bar{V}(\cell, T)$, for all possible cells $\cell \in \partition$, any $T \subset [p]$, all $i \in K$, $j \in [p] \setminus K$. 
\end{definition}

\noindent  A function satisfying Definition \ref{def:strong_partition_sparsity} and using Equation (4.4) in \cite{syrgkanis2020estimation} imply that the reduction in expected mean square error (i.e., the impurity decrease) when splitting a cell on a relevant variable is larger than when splitting on an irrelevant variable by at least $\beta$. This generative assumption is required for greedy algorithms such as CART to identify the relevant feature subset $K$. 


\begin{assumption}[Bounded Noise]
\label{ass:bounded_noise}
Conditioned on the Fourier coefficients $\mathcal{A}$, for any unit-intervention pair $(n,\pi)$, $\epsilon_n^{\pi}$ are independent zero-mean bounded random variables with $\epsilon_n^{\pi} \in [-1/2,1/2]$.
\end{assumption}

\begin{theorem}
\label{thm:CART_convergence_rate} Suppose $\Pi_u \subset \Pi$ is chosen uniformly and independently at random. Additionally, let 
Assumptions \ref{ass:observation_model},  \ref{ass:boundedness_potential_outcome}, \ref{ass:bounded_noise}  hold and suppose that $\E[Y_u^{(\pi))}]$ satisfies definition \ref{def:strong_partition_sparsity} with parameters $(\beta,k)$. If $\hat{T}$ is grown with number of nodes\footnote{In practice, the number of nodes can be tuned by cross-validation.} $m = 2^k$, $s = 2^k$, and $|\Pi_u|  = \Omega(2^k\log(p)/\beta^2)$, then we have 
\begin{equation}
\label{eq:CART_convergene_rate}
  \hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] = O_p\left(\sqrt{\frac{s^2\log(s)}{|\Pi_u|}}  \right) 
\end{equation}
for any donor unit $u \in \mathcal{I}$ and any combination $\pi \in \Pi$.
\end{theorem}


\begin{remark}[CART Donor Set Inclusion] Under the set-up of Theorem \ref{thm:CART_convergence_rate}, any unit $n \in [N]$ that has  $|\Pi_n| =  \Omega(2^k\log(p)/\beta^2)$ randomly sampled observations can be considered a donor unit. 
\label{rem:CART_donor_set_inclusion}
\end{remark}


\noindent CART \eqref{eq:CART_potential_outcome_estimate} consistently estimates the target causal parameter for a donor unit $u$ provided that the structural assumptions on the potential outcomes are satisfied and the number of observations $|\Pi_u|$ grows as $\omega\left(s^2\max\{\log(p),
\log(s)\} \right)$. 
%
On the other hand, the Lasso estimator requires that $|\Pi_u|$ grows as $\omega(s^2p)$. 
%
The difference in sample complexity can be ascribed to the fact that CART performs feature selection (i.e., learns the subset of features that $\E[Y_u^{(\pi)}]$ depends on), thereby only having to learn the fourier coefficient over a $2^k$ dimensional space, instead of $2^p$ for the Lasso. 

\section{Proofs for CART Horizontal Regression}


\subsection{Proof of Theorem \ref{thm:CART_convergence_rate}}
\label{subsec:CART_horizontal_regression_proof}

\begin{proof} Let $\mathcal{E}_{1}$ denote the event that $\hat{K} = K$. Further, define the event $\mathcal{E}_{S}$ for any subset $S \subset K$ as  
\begin{equation*}
    \mathcal{E}_{S} =  \left\{|\hat{\alpha}_{u,S} - \alpha_{u,S} |\leq \sqrt{\frac{8\log(2^{k+3}/\delta)}{|\Pi_u^b|}}  \right\}
\end{equation*}
\noindent Additionally, let $\mathcal{E} = \mathcal{E}_1 \cap \left(\cap_{S \subset K}\mathcal{E}_S\right) $. We first show that $\mathcal{E}$ occurs with high probability. To establish this claim, we state the following two results proved in Appendix \ref{subsubsec:cart_helper_lemmas}. 


\begin{proposition} [Theorem D.9 in \cite{syrgkanis2020estimation}]
\label{prop:CART_feature_selection_high_prob_bound}
Let the set-up of Theorem \ref{thm:CART_convergence_rate} hold. Then, $\hat{K} = K$ with probability $1 - \delta/2$. 
\end{proposition}


\begin{lemma}
\label{lem:CART_fourier_coefficient_convergence}
Let the set-up of Theorem \ref{thm:CART_convergence_rate} hold. Then for any subset $S \subset K$, we have 
\begin{equation}
\label{eq:CART_fourier_coefficient_convergence}
\P\left(|\hat{\alpha}_{u,S} - \alpha_{u,S} |\geq \sqrt{\frac{8\log(2^{k+2}/\delta)}{|\Pi_u^b|}}\right) \leq \frac{\delta}{2^k}
\end{equation}
where $\hat{\alpha}_{u,S} =  \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi^b_u }Y_{u\pi}\chi^{\pi}_{S}$
\end{lemma}

\noindent Using Proposition \ref{prop:CART_feature_selection_high_prob_bound} and Lemma \ref{lem:CART_fourier_coefficient_convergence}, and applying the union bound alongside Demorgan's law, we get
\begin{equation*}
    \P\left(\mathcal{E}^c_3 \right) = \P\left(\mathcal{E}^c_1 \cup \left(\cup_{S \subset K}\mathcal{E}^c_S\right) \right) \leq \P\left(\mathcal{E}^c_1\right) + \P\left(\cup_{S \subset K}\mathcal{E}^c_{S}\right) \leq  \frac{\delta}{2} + \sum_{S \subset K} \frac{\delta}{2^{k+1}} \leq \delta
\end{equation*}

\noindent Then conditioned on $\mathcal{E}$, we get 
\begin{align*}
    | \hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] | & =  \left|\sum_{S \subset \hat{K}}\hat{\alpha}_{u,S}\chi^{\pi}_{S} -  \sum_{S \subset K}\alpha_{u,S}\chi^{\pi}_{S} \right| \\
    & = \left|\sum_{S \subset K}\left(\hat{\alpha}_{u,S} - \alpha_{u,S}\right)\chi^{\pi}_{S} \right| \\
    & \leq \sum_{S \subset K}  \left| \hat{\alpha}_{u,S} - \alpha_{u,S}\right | \\ 
    & \leq \sqrt{\frac{2^{2k+3}\log(2^{k+3}/\delta)}{|\Pi_u^b|}}
\end{align*}
\noindent Therefore, we have 
\begin{equation*}
    | \hat{\E}[Y_u^{(\pi)}] - \E[Y_u^{(\pi)}] | = O_p \left(\sqrt{\frac{2^{2k}k}{|\Pi_u|}} \right)
\end{equation*}

\noindent Substituting $s = 2^k$ concludes the proof. 
\end{proof}

\subsubsection{Proof of Lemma \ref{lem:CART_fourier_coefficient_convergence}}
\label{subsubsec:cart_helper_lemmas}
Fix any subset $S \subset K$. Then for any $t > 0$, we have 
\begin{align}
&\P\left(|\hat{\alpha}_{u,S} - \alpha_{u,S} |\geq t \right) 
%
\\& = \P\left(\left| \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\left(\E[Y_u^{(\pi)}]\chi_S^{\pi} + \epsilon_u^{\pi}\chi_S^{\pi}\right) - \alpha_{u,S} \right| \geq t  \right) \nonumber \\
%
& \leq \P\left( \left| \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} - \alpha_{u,S} \right| + \left|\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\epsilon_u^{\pi}\chi_S^{\pi} \right| \geq t \right) \nonumber \\
%
& \leq \P\left( \left| \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} - \alpha_{u,S} \right| \geq \frac{t}{3} \right) + \P\left(\left|\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\epsilon_u^{\pi}\chi_S^{\pi} \right| \geq \frac{2t}{3} \right) \nonumber
%
\\ & \leq \P\left( \left| \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} - \alpha_{u,S} \right| \geq \frac{t}{3} \right) + \P\left(\left|\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\epsilon_u^{\pi}\chi_S^{\pi} \pm \frac{1}{2^p}\sum_{\pi \in \Pi}\epsilon_u^{\pi}\chi_S^{\pi}\right| \geq \frac{2t}{3} \right) \nonumber
%
\\ & \leq \P\left( \left| \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} - \alpha_{u,S} \right| \geq \frac{t}{3} \right) + \P\left(\left|\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\epsilon_u^{\pi}\chi_S^{\pi} - \frac{1}{2^p}\sum_{\pi \in \Pi}\epsilon_u^{\pi}\chi_S^{\pi}\right| \geq \frac{t}{3} \right)  \nonumber
%
\\& \quad + \P\left(\left| \frac{1}{2^p}\sum_{\pi \in \Pi}\epsilon_u^{\pi}\chi_S^{\pi}\right| \geq \frac{t}{3} \right)
\label{eq:fourier_coefficient_intermediate_bound}
\end{align}
We bound each of the three terms above separately.  \\

\noindent \emph{Bounding Term 1.} We begin by showing that $\E\left[ \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} \right] = \alpha_{u,S}$, where the expectation is taken over the randomness is sampling $\pi$. Since each $\pi \in \Pi^b_{u}$ is chosen uniformly and independently at random from $\Pi$, we have that 
\begin{align*}
    \E\left[ \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} \right] = \frac{1}{2^p} \sum_{\pi \in \Pi} \E[Y_u^{(\pi)}]\chi_S^{\pi} = \langle \E[Y_u^{(\pi)}], \chi_S \rangle_B = \alpha_{u,S}
\end{align*}
Next, since each $\chi_{S}^{\pi} \in \{-1,1\}$, and $\E[Y_{u}^{(\pi)}] \in [-1,1]$, we have that  $\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi}$ is a sum of independent bounded random variables. Hence, applying Hoeffding's inequality gives us
\begin{equation*}
    \P\left( \left| \frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\E[Y_u^{(\pi)}]\chi_S^{\pi} - \alpha_{u,S} \right| \geq \frac{t}{3} \right) \leq 2\exp{\left(-\frac{|\Pi_u|t^2}{9} \right)}
\end{equation*} \\

\noindent \emph{Bounding Term 2.} By Assumption \ref{ass:bounded_noise}, and the mutual independence of $
\chi_S^{\pi}$ and $\epsilon_u^{\pi}$ for each $\pi$, we have that $\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\epsilon_u^{\pi}\chi_S^{\pi}$ is a sum of independent bounded random variables. Applying Hoeffding's inequality once again gives us
\begin{equation*}
    \P\left(\left|\frac{1}{|\Pi^b_u|}\sum_{\pi \in \Pi_u^{b}}\epsilon_u^{\pi}\chi_S^{\pi} - \frac{1}{2^p}\sum_{\pi \in \Pi}\epsilon_u^{\pi}\chi_S^{\pi}\right| \geq \frac{t}{3} \right) \leq 2\exp{\left(-\frac{|\Pi_u|t^2}{9}\right)}
\end{equation*}
\vspace{2mm}

\noindent \emph{Bounding Term 3.} By Assumption \ref{ass:bounded_noise}, $\epsilon_u^{\pi}$ are independent mean-zero bounded random variables for each $\pi$. Hence, applying Hoeffding's inequality gives us
\begin{equation*}
    \P\left(\left| \frac{1}{2^p}\sum_{\pi \in \Pi}\epsilon_u^{\pi}\chi_S^{\pi}\right| \geq \frac{t}{3} \right) \leq 2\exp{\left(-\frac{2^{p+1}t^2}{9} \right)} \leq 2\exp{\left(-\frac{2|\Pi_u|t^2}{9} \right)}
\end{equation*}
where the last inequality follows from the fact that $|\Pi_u| \leq 2^p$


\noindent \emph{Collecting Terms.} Choosing $t = \sqrt{\frac{9\log(2^{k+3}/\delta)}{|\Pi_u|}}$, and plugging the bounds of Term 1,2 and 3 into \eqref{eq:fourier_coefficient_intermediate_bound} gives us
\begin{align*}
    \P\left(|\hat{\alpha}_{u,S} - \alpha_{u,S} |\geq \sqrt{\frac{8\log(2^{k+2}/\delta)}{|\Pi_u^b|}}\right) & \leq
    2\exp{\left(-\frac{|\Pi_u|t^2}{9}\right)} + 2\exp{\left(-\frac{2|\Pi_u|t^2}{9}\right)} + 2\exp{\left(-\frac{|\Pi_u|t^2}{9}\right)} \\
    & \leq 6\exp{\left(-\frac{|\Pi_u|t^2}{9} \right)} \\
    & \leq \frac{\delta}{2^k}
\end{align*}
Observing that the result above holds uniformly across uniformly for all subsets $S \subset [K]$ completes the proof. 



%\subsection{Proof of Corollary \ref{cor:potential_outcome_convergence_rate_CART}}

%\textcolor{red}{needs to be updated now after all the changes.}
%\label{subsec:CART_potential_outcome_proof}

%From Theorem \ref{thm:CART_convergence_rate}, we have that for all donor units $u \in \mathcal{I}$,that $\max_{\pi \in \Pi}\hat{\E}[Y_u^{(\pi})] - \E[Y_u^{(\pi})] = \tilde{O}_p \left(\sqrt{\frac{s^2}{M}}\right)$. Then, we repeat the proof of Theorem \ref{thm:potential_outcome_convergence_rate} but instead substitute $\Delta_E = \tilde{O_p}(\frac{s^2}{M})$. Doing so gives us the following statements 
%\begin{itemize}
%    \item [(a)] For the given donor unit-combination pair $(u,\pi)$, we have 
%        \begin{equation*}
%            |\hat{\E}[Y_u^{(\pi)}] - \E[Y_{u}^{(\pi)}]| = \Tilde{O}_p\left(\sqrt{\frac{s^2}{|\Pi_u|}} \ \bigg | \ \mathcal{A}\right)
%        \end{equation*}
%    \item [(b)] For the given unit-combination pair  $(n,\pi)$, we have
%        \begin{equation*}
%            \left |\hat{\E}[Y_n^{(\pi)}] - \E[Y_{n}^{(\pi)}]\right| =\tilde{O}_p\Bigg( 
%            \sqrt{\frac{|\mathcal{I}|}{M}}  r_n^{2}\left[\frac{\sqrt{s^2}}{\min\{    \sqrt{|\mathcal{I}|},\sqrt{|\Pi_n|}\}} + \frac{s^2}{\sqrt{M}} \right]
%            + \frac{r_n}{|\Pi_n|^{1/4}} ~ \bigg | ~ \mathcal{A}\Bigg).
%        \end{equation*}
%\end{itemize}

%\noindent Then substituting our assumptions that $|\mathcal{I}| \leq |\Pi_n|$, $r_n|\Pi_n|^{-1/4} \rightarrow 0$ and $r_n^2s^2/M \rightarrow 0$ completes the proof. 