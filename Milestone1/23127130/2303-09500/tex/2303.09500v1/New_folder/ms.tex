%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


% Custom imports --------------------------------------------------------
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{amssymb, amsfonts}
\usepackage{relsize}
\usepackage[american]{babel}
\usepackage{multirow}
\usepackage{aligned-overset}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% Math ops
\newcommand{\argdot}{\,\cdot\,}
\DeclareMathOperator{\Exists}{\exists}
\DeclareMathOperator{\Forall}{\forall}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\allowdisplaybreaks

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{xcolor}
\definecolor{blue}{RGB}{28,132,218}
\definecolor{orange}{RGB}{237,125,49}
\definecolor{darkviolet}{rgb}{0.58, 0.0, 0.83}
\newcommand{\MB}[1]{\textcolor{darkviolet}{MB: #1}}
\newcommand{\FP}[1]{\textcolor{blue}{FP: #1}}
\newcommand{\NK}[1]{\textcolor{orange}{NK: #1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Enabling First-Order Gradient-Based Learning for Equilibrium Computation in Markets}

\begin{document}

\twocolumn[
\icmltitle{Enabling First-Order Gradient-Based Learning\\for Equilibrium Computation in Markets}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nils Kohring}{sch}
\icmlauthor{Fabian R.\ Pieroth}{sch}
\icmlauthor{Martin Bichler}{sch}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sch}{School of Computation, Information and Technology, Technical University of Munich}

\icmlcorrespondingauthor{Nils Kohring}{\href{mailto:nils.kohring@tum.de}{nils.kohring@tum.de}}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Understanding and analyzing markets is crucial, yet analytical equilibrium solutions remain largely infeasible. Recent breakthroughs in equilibrium computation rely on zeroth-order policy gradient estimation. These approaches commonly suffer from high variance and are computationally expensive.
	The use of fully differentiable simulators would enable more efficient gradient estimation. However, the discrete allocation of goods in economic simulations is a non-differentiable operation. This renders the first-order Monte Carlo gradient estimator inapplicable and the learning feedback systematically misleading. 
	We propose a novel smoothing technique that creates a surrogate market game, in which first-order methods can be applied.
	We provide theoretical bounds on the resulting bias which justifies solving the smoothed game instead. These bounds also allow choosing the smoothing strength a priori such that the resulting estimate has low variance. Furthermore, we validate our approach via numerous empirical experiments.
	Our method theoretically and empirically outperforms zeroth-order methods in approximation quality and computational efficiency.
\end{abstract}



\section{Introduction}

Auctions are at the center of modern economic theory. Given some private valuation of goods available for purchase, participants must place bids on the market that maximize their expected payoff while remaining unaware of the other participants' valuations.
In the seminal paper \cite{vickrey1961counterspeculation} the foundation for most auction theory results of today was laid. 
It is crucial to understand the strategic behavior in various auction applications, ranging from treasury and industrial procurement auctions to spectrum sales.
Depending on the circumstances and behavioral assumptions, optimal strategies may differ drastically, starting from strategies, such as understating demand (bid-shading) \cite{krishna2009auction} and overstating demand (overbidding) \cite{ott2013incentives}, or much more convoluted strategies. However, computing such equilibria and approximations a priori remains challenging. Analytical equilibria can only be derived under strong assumptions such as in single-item auctions or the independent private values model.

A recent approach based on policy optimization uses randomized finite difference approximations of the gradient \cite{bichler2021learning}. They proposed an algorithm called \emph{neural pseudogradient ascent} (NPGA), which parametrizes the bidding strategies using neural networks and follows the approximate gradient dynamics of the game via simultaneous gradient ascent of all agents. The gradients are computed via \emph{evolution strategies} (ES) \cite{salimans2017evolution}, which smoothen the objective by adding noise in the parameter space, thereby treating the environment as a black box.
Compared with the well-known REINFORCE algorithm, where the actions are perturbed, this also results in zeroth-order gradient estimates with better precision and lower variance but much higher computational cost.

Under the differentiable programming paradigm, there is a growing interest in computing gradients for numerous reinforcement learning applications that allow for first-order gradient estimates. It is possible to create a full computational graph for applications with a certain amount of structure. 
First-order methods have the advantage of much lower variance, which leads to faster convergence rates to local minima of non-convex objective functions \cite{mohamed2020monte}.
However, there are two common problems in employing first-order methods.
First, most reinforcement learning environments are provided only as black boxes. This implies that there is no explicit access to the underlying state transition function and the gradient can only be estimated by repeatedly evaluating the reward function. 
The wide applicability of zeroth-order policy optimization, like REINFORCE and more advanced actor-critic techniques \cite{schulman2017proximal}, contributes to their popularity.
Second, in some applications, such as the training of variational autoencoders, the computational path of the derivate is blocked (i.e., repeatably applying the chain rule to calculate the gradient of the reward with respect to the parameters of the policy) because it consists of sampling a random variable, which is a non-differentiable operation \cite{bangaru2021systematically}.

The situation is similar in auction games. The allocation of indivisible goods causes biased gradients of first-order methods. Example~\ref{ex:naive-failure} showcases this observation. It was observed that the first-order Monte Carlo gradient estimate does not converge to equilibrium and quickly causes consistent zero-bidding \cite{bichler2021learning}.
From a mathematical standpoint, the single-sample (ex post) utility has a discontinuity. Thus, the sample mean of its exact gradients is an inadequate estimate for its true (ex ante) utility gradient (the expected utility over all possible valuations).

\begin{example}\label{ex:naive-failure}
	Consider a first-price sealed-bid (FPSB) single-item auction. Two bidders compete for a single good, where the winner pays his or her bid. The derivative of the utility with respect to the bid is zero for losing bids and minus one for winning bids after a point of discontinuity. Either the bidder loses and receives no feedback or wins and could have won with an even smaller bid.
\end{example}

In this study, we propose transforming multi-agent auction games such that their utility functions are sufficiently regular for applying efficient first-order gradient methods while keeping the overall gradient dynamics close to the original game. In contrast to the original allocations of indivisible items, we use \emph{soft} allocations instead. We effectively treat the items as divisible and allocate the proportional fraction of an item to the bidders based on their reported bids. An additional adaption to the pricing rule eliminates the discontinuity at the threshold of winning and losing an object. However, this comes at the expense of introducing a bias in the utility function.
For example, a losing bidder has zero utility the original auction. However, in the smoothed auction, this bidder receives a small fraction of the good (and pays a correspondingly small price), such that the gradient indicates that a higher bid would have resulted in higher utility. The feedback to bid lower when winning remains of similar magnitude. Thus, there is always appropriate feedback on the current bidding strategy in the smoothed game. Figure~\ref{fig:ex-post-utility} shows the utility function and its relaxed version. % and are formally introduced in \autoref{sec:introduce-smoothing}.

This approach is applicable widely to economic models and general auction formats, such as sequential or simultaneous sales of multiple goods, as in combinatorial auctions with item bidding. It is further independent of the number of bidders, payment rule, risk preferences of the bidders, or correlations among the bidders' valuations. We demonstrate that the choice of a smoothing parameter follows a natural trade-off.
Importantly, computing equilibria in multi-agent games is not straightforward and many negative results are known \cite{chasnov2020convergence,mazumdar2020policy,letcher2020impossibility}. Therefore, changes to the game dynamics must be implemented with great caution, and we can prove that an approximate equilibrium in the smoothed game still constitutes an approximate equilibrium in the original game.

Computational-wise, the smoothing only comes with the cost of tracking the gradients of the individual operations, upon which the game dynamics are built. %Automatic differentiation frameworks are designed exactly for doing such a backpropagation in an efficient manner.
Compared with NPGA, learning in the \emph{smooth market} (SM) via the first-order estimator is more than ten times faster while yielding better results. For example, an iteration of NPGA in a small single-item auction with the default hyperparameters from \cite{bichler2021learning} takes approximately $0.16 \,$s, whereas first-order policy gradients applied to the SM take an average of $0.01 \,$s.

Our contribution can be summarized as follows: We introduce the SM and show that first-order methods provide an unbiased estimator of the utility gradient of the SM game. Furthermore, we provide theoretical guarantees showing that policy improvements in the SM result in improvements in the original game, and we provide theoretical and empirical insights showing that the empirical variance can be controlled. Finally, we demonstrate a substantial improvement to previous methods in performance and computational speed via multiple experiments.
%The article is structured as follows: The next section introduces related literature before formally introducing auction games with the necessary notation. Then, we we will consider gradient-based equilibrium computation and analyze the hurdles of its application to auction markets. Furthermore, we will introduce our smoothing technique in Section~\ref{sec:introduce-smoothing}, which will be validated theoretically and empirically before concluding.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\columnwidth]{figures/ex-post-utility.png}
	\caption{Ex post utility in the original FPSB single-item auction (blue) and its smoothed version (red) for a temperature of $\lambda = 0.01$ and a highest opponent bid of $0.5$. The utility in the original auction is zero for losing bids and decreases linearly for winning bids.}
	\label{fig:ex-post-utility}
\end{figure}

% The theory of learning in games largely considers complete-information normal form games and studies processes in which players are maximizing their own payoff by adapting to the actions played by their opponents.
% The question of when learning dynamics converge to an equilibrium remains largely open. Some of the most important results include the observation that no-regret dynamics converge to coarse correlated equilibria (a weaker solution concept than Nash equilibria) and results that rely on properties of the game such as the game being potential, two-player, zero-sum (fully competitive), or fully cooperative, among others. All of which are not satisfied by auctions in general.

% The problem of computing a Nash equilibrium is known to be computationally hard even for normal-form games with complete information \cite{daskalakis2009complexity}.
% Traditional techniques such as best response dynamics rely on discretization of type and action spaces. This includes a more recent approach to learn in auctions by \cite{bosshard2020computing} that explicitly computes point wise best responses in a linearization of the strategy space via Monte Carlo integration. 
% Discretization also plays a key role for solvers of imperfect information games such as those capable of outperforming humans in poker \cite{brown2017superhuman}.
% However, it is unclear how well a strategy from the discrete game performs in the original continuous action game in general \cite{waugh2009abstraction} and discretization always suffers from the curse of dimensionality in MARL, especially so in large markets with many goods which have a high dimensional continuous action space.
% Furthermore, most of the work in auction theory, focuses on a continuous formulation for which there is limited work on numerical techniques \cite{bichler2021learning}.


\section{Related Work}

The theory of learning in games largely considers complete-information finite games, hence, traditional techniques rely on discretization.
However, it is unclear how well a discretized strategy performs in the original continuous game in general \cite{waugh2009abstraction} and it suffers from the curse of dimensionality.
The first attempts to compute equilibria in imperfect-information auction games followed such an approach \cite{athey2001single} or expressed the game as a limit of a sequence of complete-information games \cite{armantier2008approximation}.  
In larger combinatorial auctions equilibria were first computed with an algorithm that computes pointwise best responses in a discretization of the strategy space via Monte Carlo integration \cite{bosshardComputingBayesNashEquilibria2020}. 
Besides the aforementioned NPGA, an approach that similarly learns continuous-action strategies was proposed \cite{li2021evolution}.
Both algorithms learn bid functions via zeroth-order gradient estimates that are used during simultaneous gradient ascent in self-play.
Our method considers a continuous surrogate game and enables the use of first-order gradient methods.

The idea of analytically smoothing markets is conceptually similar to that of differentiable physics simulations. Smooth approximations of the underlying dynamics were used in these simulations \cite{huang2021plasticinelab}.
Zeroth- and first-order methods were compared and the pros and cons of both when available were discussed \cite{suh2022differentiable}. Furthermore, they demonstrated that the presence of discontinuities in the objective causes the first-order estimator to be biased, whereas the zeroth-order estimator remains unbiased.
Smooth markets transfer these ideas to auctions.


\section{Preliminaries}
We restrict the formulations to the case of single-item auctions for brevity in the presentation. The extension to \emph{auctions of multiple independent items} is straightforward and we present some experimental results for both cases.

\subsection{Auctions as Bayesian Games}
A \emph{Bayesian auction game} is defined as a quintuple $G = (\mathcal I, \mathcal A, \mathcal V, F, u)$. $\mathcal I = \{1, \dots, n\}$ describes the set of bidders participating in the game. 
The set of possible bid profiles is given as $\mathcal A = \mathcal A_1 \times {\cdots} \times \mathcal A_{n}$, where $\mathcal A_i$ is the set of bids available to agent $i \in \mathcal I$. 
Whereas $\mathcal V = {\mathcal V_1 \times {\cdots} \times \mathcal V_{n}}$ is the set of \emph{valuation profiles}.
$F{:}\ \mathcal V \rightarrow [0, 1]$ defines the joint prior probability distribution over valuation profiles, which is assumed to be common knowledge among all agents and atomless. $F_i$ denotes agent $i$'s marginal distribution of valuations. In this study, the index $\text{-}i$ denotes a profile of valuations, bids, or strategies for all bidders, except bidder~$i$.

At the beginning of the game, nature draws a valuation profile $v \sim F$, and each agent $i$ is informed of his or her valuation $v_i \in \mathcal V_i$. We denote by $F_{i}$ the marginal distribution of bidder~$i$ and by $F_{\text{-}i|i}$ the conditional distribution of the opponents given $v_i$. Based on the drawn valuation $v_i$, each agent submits a bid $b_i$ according to the \emph{strategy}, \emph{policy}, or \emph{bid function} $\beta_i{:}\ \mathcal V_i \rightarrow \mathcal A_i$. We denote the resulting strategy space of bidder~$i$ as $\Sigma_i \subseteq \mathcal A_i^{\mathcal V_i}$ and the space of possible joint strategies as $\Sigma = \prod_i \Sigma_i$.
% Note that $\Sigma_i$ is infinite-dimensional unless $\mathcal V_i$ is finite.
% Each agent's valuation $v_i \in \mathcal V_i \subset \mathbb{R}^{\geq 0}$ is given by a scalar of \emph{private valuations}, i.e., each player submits a bid for the item. Analogously, usually one assumes real-valued bids, $\mathcal A_i \subset \mathbb{R}_{\geq 0}$.

As part of the environment, the auctioneer collects these bids and applies an \emph{auction mechanism} that determines allocations $x_i \in \{0, 1\}$ for each bidder~$i$, such that the item is allocated to at most one bidder. Also, it determines payments $p(b) \in \mathbb{R}_{\geq 0}^n$ according to a payment rule $p$, which the agents must pay to the auctioneer. We will consider bidders with risk-neutral utility functions given by $u_i{:}\ \mathcal V_i \times \mathcal A \rightarrow \mathbb{R}$,
\begin{align}
	u_i(v_i, b) &= v_i \, x_i(b) - p_i(b) \label{eq:general-post-utility}\\
	&= 
	\begin{cases}
		v_i - p_i(b) & b_i > \max b_{\text{-}i},\\
		0 & \text{else},
	\end{cases}
	\label{eq:utility}
\end{align}
i.e., the players' utility is given by how much they value the good allocated to them minus the price to be paid. We will also write $u_i(v_i, b_i, b_{\text{-} i}) = u_i(v_i, b)$ with a slight abuse of notation. Thus, the bidders' utilities depend on all bidders' actions but only on their own valuations. They aim to maximize their utility $u_i$.
We omit bidders with risk aversion or other forms of utility and valuation correlations for brevity. Notwithstanding, our treatment of equilibrium computation also extends to these settings.
We will differentiate between the \emph{ex ante} state of the game, where bidders know only the prior $F$, the \emph{interim} state, where bidders additionally know their valuation $v_i \sim F_i$, and the \emph{ex post} state, where all bids have been submitted; thus, $u_i(v_i, b)$ can be evaluated.


\subsection{Equilibria}

\emph{Nash equilibria} (NE) are often regarded as the central solution concept in game theory. Informally, given the equilibrium strategy of the opponents in an NE, no agent has an incentive to unilaterally deviate.
% A set of bids $b^*$ is a pure strategy NE of the complete-information game $G = (\mathcal{I}, \mathcal A, u)$ if $u_i(b_i^*, b_{\text{-}i}^*) \geq u_i(b_i, b_{\text{-}i}^*)$ for all bids $b_i \in \mathcal A_i$ and all agents $i \in \mathcal I$. 
\emph{Bayesian Nash equilibria} (BNE) extend this concept to games of incomplete information. Here, the expected utility over the distribution of opponent valuations is calculated instead.
For a private valuation $v_i \in \mathcal V_i$, bid $b_i \in \mathcal A_i$, and opponent strategies $\beta_{\text{-}i} \in \Sigma_{\text{-}i}$, we denote the \emph{interim utility} of bidder~$i$ as 
\begin{equation}
	\label{eq:iterim-util}
	\overline{u}_i(v_i, b_i, \beta_{\text{-}i}) \; = \; \mathbb{E}_{v_{\text{-}i}|v_i} [u_i(v_i, b_i , \beta_{\text{-}i}(v_{\text{-}i}))],
\end{equation}
where $v_{\text{-}i}|v_i$ denotes the expectation over the opponent's conditional prior distribution given the valuation $v_i$.
We also denote the \emph{interim utility loss} of bid $b_i$ incurred by not playing a best response, given $v_i$ and $\beta_{\text{-}i}$ by:
\begin{equation}
	\label{eq:ex-interim-loss}
	\overline \ell_i(v_i, b_i, \beta_{\text{-}i}) \; = \; \sup_{b'_i \in \mathcal A_i} \overline u_i(v_i, b'_i, \beta_{\text{-}i}) - \overline u_i(v_i, b_i, \beta_{\text{-}i}).
\end{equation}
% Note that $\overline \ell_i$ can generally not be observed in as best responses are not available.
An \emph{$\varepsilon$-Bayes Nash equilibrium} ($\varepsilon$-BNE) with $\varepsilon \geq 0$ is a strategy profile $\beta^* = (\beta^*_1, \dots, \beta^*_n) \in \Sigma$, such that no bidder can improve his or her interim expected utility more than $\varepsilon$ by deviating. Therefore, in an $\varepsilon$-BNE for all $i \in \mathcal I$, it holds that
\begin{equation}
	\label{eq:BNE}
	\sup_{v_i \in \mathcal{V}_i} \overline{\ell}_i(v_i, \beta_i^*(v_i), \beta^*_{\text{-}i}) \; \leq \; \varepsilon.
\end{equation}
A $0$-BNE is simply called a BNE. In a BNE, every bidder's strategy maximizes his or her expected interim utility across his or her valuation space, given the opponents' strategies. While BNEs are often defined at the \emph{interim} stage of the game, we also consider \emph{ex ante} equilibria as strategy profiles that concurrently maximize each bidder's \emph{ex ante} utility
\begin{equation}
	\tilde{u}_i(\beta_i, \beta_{\text{-}i}) \; = \; \mathbb{E}_{v_i} [\overline{u}_i(v_i, \beta_i(v_i), \beta_{\text{-}i})].
\end{equation}
To estimate the worst-case interim utility loss $\overline{\ell}_{\text{max}}$, we choose an equidistant grid of $n_\text{grid}$ alternative actions ranging from zero to the maximum valuation for all dimensions and calculate approximate best responses based on the average utility over a sample of $n_\text{batch}$ prior distributions. Taking the maximum over all valuations and bidders then gives an estimate of $\overline{\ell}_{\text{max}}$, bounding $\varepsilon$ for the ex ante case from above.
% While BNEs are often defined at the \emph{interim} stage of the game, we also consider \emph{ex ante} equilibria as strategy profiles that concurrently maximize each bidder's \emph{ex ante} expected utility $\tilde u$. Analogously, $\tilde u$ and the \emph{ex ante utility losses} $\tilde \ell$ of a strategy profile $\beta \in \Sigma$ can be defined as
% \begin{equation}
% 	\tilde{u}_i(\beta_i, \beta_{\text{-}i}) \; = \; \mathbb{E}_{v_i} [\overline{u}_i(v_i, \beta_i(v_i), \beta_{\text{-}i})]
% \end{equation}
% and
% \begin{equation}
% 	\label{eq:ex-ante-loss}
% 	\tilde \ell_i(\beta_i, \beta_{\text{-}i}) \; = \; \sup_{\beta'_i \in \Sigma_i} \tilde u_i(\beta'_i, \beta_{\text{-}i}) - \tilde u_i(\beta_i, \beta_{\text{-}i}).
% \end{equation}
% Then, an ex ante $\varepsilon$-BNE $\beta^* \in \Sigma$ can be characterized using the equations $\tilde \ell_i(\beta^*_i, \beta^*_{\text{-}i}) \leq \varepsilon$ for all $i \in \mathcal I$.
% We will be interested in finding ex ante equilibria.

As a second metric, we additionally report the probability-weighted root mean squared error of the learned strategy $\beta_i$ to the exact BNE strategy $\beta^*_i$ for those settings where an analytical BNE is known. For a sample from the prior valuation of size $n_\text{batch}$, this approximates the $L_2$ distance $\lVert \beta_i - \beta_i^*\rVert_{\Sigma_i}$ of these two functions as
\begin{equation}
	\label{eq:L_2}
	L_2(\beta_i) \; = \; \left(\frac{1}{n_\text{batch}}\sum_{v_i} \left(\beta_i(v_{i}) - \beta^*_i(v_{i})\right)^2\right)^{1/2}.
\end{equation}
Unlike $\overline{\ell}_{\text{max}}$, this metric is much easier to compute and does not suffer the drawback that a strategy with a negatable small loss may still be arbitrarily distant from the actual BNE. However, it is only computable when an analytical BNE is available and may need multiple evaluations when there are multiple BNE.


\subsection{Gradient Optimization Methods}
Policy gradient methods are concerned with learning a parameterized policy $\beta_{\theta_i}$ that selects actions based on the current observations \cite{sutton2018reinforcement}. To maximize utility, bidder~$i$ updates the parameters $\theta_i$ according to gradient ascent.
This process is intended to compute approximate ex ante BNEs, that is, to find mutual best responses of the bidders for all possible valuations.
The exact gradient update for valuation $v_i$ in iteration $t$ is
\begin{align}
	\theta_i^t \; = \; \theta_i^{t-1} + \eta \cdot \nabla_{\theta_i^{t-1}} \, \overline{u}_i(v_i, \beta_{\theta_i^{t-1}}(v_i), \beta_{\theta_{\text{-} i}^{t-1}}).%\\
	% &=\theta_i^{t-1} + \eta \cdot \nabla_{\theta_i^{t-1}} \, \mathbb{E}_{v_{\text{-}i}|v_i}\left[ u_i(v_i, \beta_{\theta_i^{t-1}}(v_i), \beta_{\theta_{\text{-}i}^{t-1}}(v_{\text{-}i})) \right].
\end{align}
This must be approximated in practice.
%During the actual application of this procedure, one would sample a large batch of $v_i$'s (and corresponding $v_{\text{-} i}$) and average over the gradients but this can be neglected for the following analysis.
% Note that it is well known that this naive first order update rule may not converge to an equilibrium in MARL environments \cite{chasnov2020convergence,mazumdar2020policy,letcher2020impossibility}. However, in auction games there is empirical evidence that more complex approaches may not be necessary \cite{bichler2021learning}.
Two common methods are zeroth- and first-order gradient approximations. The former solely relies on evaluating the objective function $u_i$, whereas the gradient $\nabla_{\theta_i} u_i$ can be evaluated in the latter.

As stated in the introduction, the discontinuous nature of the ex post utility function stems from the sampling of the opponents' priors and their corresponding actions. We encounter $u_i$ from Equation~\ref{eq:utility} and its derivative (in general) is discontinuous in $b_i$. The observation of this inapplicability persists for all pricing regimes and behavioral assumptions that are commonly considered in auctions.
Thus, an unbiased gradient estimate of the interim utility function \emph{cannot} be derived by sampling the ex post gradient. Specifically, interchanging taking an expectation and differentiating is invalid: 
\begin{equation}
	\label{eq:operator_interchange}
	\nabla_{\theta_i} \, \mathbb{E}_{v_{\text{-} i}|v_i} [u_i] \; \neq \; \mathbb{E}_{v_{\text{-} i}|v_i} [\nabla_{\theta_i} \, u_i].
\end{equation}
We supply the mathematical details in Appendix~\ref{sec:leibniz}.
Therefore, the naive application of backpropagating the accumulated exact ex post gradients may not be expected to provide a meaningful estimate of the ex ante gradient. This study establishes a path towards valid first-order gradient estimates in auction games.


\subsection{Zeroth-Order Approximation Methods}
\cite{bichler2021learning} employed ES to circumvent the interchange of differentiation and integration. ES rely on a randomized finite difference approximation of the gradient based on perturbations in the parameter space of the neural networks which can be computed after averaging over the priors \cite{salimans2017evolution}.
This is an alternative zeroth-order method to the REINFORCE algorithm. Unlike ES, REINFORCE relies on perturbations in the action space by using mixed strategies (typically Gaussian distributions) such that the gradient of the action probability density can be approximated. \cite{salimans2017evolution} compared these estimates for RL applications and argued that the variance of the ES estimate can be significantly lower.
We overload the notation for the ease of readability and write $u_i(\theta_i, v_{\text{-} i}) = u_i(v_i, \beta_{\theta_i}(v_i), \beta_{\theta_{\text{-} i}}(v_{\text{-} i}))$.
For a hyperparameter $\sigma > 0$, the ES estimator can be derived from
\begin{align}
	& \nabla_{\theta_i} \, \mathbb{E}_{v_{\text{-} i}|v_i} \left[u_i(\theta_i, v_{\text{-} i})\right]\nonumber\\
	&\quad \approx \nabla_{\theta_i} \, \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)} \mathbb{E}_{v_{\text{-} i} |v_i} \left[u_i(\theta_i + \sigma \epsilon, v_{\text{-} i})\right]\\
	&\quad = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)} \mathbb{E}_{v_{\text{-} i}|v_i} \left[\frac{\epsilon}{\sigma} u_i(\theta_i + \sigma \epsilon, v_{\text{-} i})\right].
\end{align}
The last term can now be approximated via sampling.
% Abstracting away from training neural networks, \cite{flaxman2005OnlineCO} prove that this type of approximation is an unbiased estimator of the gradient of a smoothed version of the objective function.
However, the ES gradient estimate comes at massive computational costs. It requires a large number of additional environment evaluations for the sampled population values of $\epsilon$. Parallelization is essentially unavailable, because it would reduce the number of samples from the prior when considering a fixed amount of memory. Latter of which is the main limiting factor in getting precise estimates of the expected utility in auction games. Thus, \cite{bichler2021learning} kept a large batch size and computed the ES sequentially using a default population size of 64.
Based on the variance of the estimate, \cite{salimans2017evolution} argued that ES are an attractive choice if the number of episodes is large, which is not the case for single-round auctions.
% Concluding, the ES estimate may be applicable to our application but its main benefits do not come into play.

% One may consider the splitting the utility function at the point of discontinuity into two continuous parts. But runs into trouble later on because the variable integral limits variant of the Leibniz rule is inapplicable neither. This is due to \todo{!!!}
% For the normalized two-bidder market, i.e., $\mathcal{V}_{\text{-} i} = [0, 1]$, this results in
% \begin{align*}
% 	&\int_{\mathcal{V}_{\text{-} i}} u_i(b_i, v_{\text{-} i}) f_{\text{-} i}(v_{\text{-} i}) \, \mathrm{d}v_{\text{-} i}\\
% 	&\quad = \int_{v_{\text{-} i}=0}^{\beta_{\text{-} i}^{-1}(b_i)} \left( v_i - p_i(b_i, \beta_{\text{-} i}(v_{\text{-} i})) \right) f_{\text{-} i}(v_{\text{-} i}) \, \mathrm{d}v_{\text{-} i}\\
% 	&\quad + \int_{v_{\text{-} i} = \beta_{\text{-} i}^{-1}(b_i)}^1 0 \cdot f_{\text{-} i}(v_{\text{-} i}) \, \mathrm{d}v_{\text{-} i}.
% \end{align*}
% For more than two bidders, the distribution over the maximum opponent bid would have to be considered.
% This now requires the application of another variant of the Leibniz rule that allows for variable integral bounds that depend on $b_i$ \cite{flanders1973differentiation}. However, ...
% \todo{Perhaps switch to the varibale integral bounds version altogether?}


\section{Smoothing Single-Item Auctions}\label{sec:introduce-smoothing}
This section proposes the market-specific approach.

\subsection{Allocation and Price Smoothing}
The allocation of indivisible objects in auction games is typically modeled as a binary vector, with a one indicating that the item is allocated to the corresponding buyer. The set of legitimate allocations is defined as
\begin{equation}
	\mathcal{X} = \Bigl\{ x \in \{0, 1\}^{n} \,\Big|\,  \sum_{i=1}^n x_{i} \leq 1 \Bigr\}.
\end{equation}
For all commonly considered auctions, the allocations label the bids as winning or losing to maximize the auctioneer's revenue. They are calculated according to
\begin{equation}
	x(b) = \argmax_{x' \,\in\, \mathcal{X}} \sum_{i=1}^n b_{i} x'_{i}.
\end{equation}
% For bidder~$i$, this simplifies to $x_{i}$ being one iff $b_{i}$ is the highest bid among all bidders, and zero otherwise.
% This \emph{winner determination problem} (WDP) itself has algorithmic complexity for large auctions as it is exponential in the number of items \cite{rothkopf1998computationally}.
Typical auction mechanisms only differ in their payment rules. Two noteworthy examples are the first-price mechanism, where bidders pay what they bid and the celebrated VCG mechanism (second-price), where they pay for the harm they cause others by competing \cite{krishna2009auction}.

These allocations result in the utilities not being continuous.
Therefore, we propose relaxing the calculation of the allocations using the softmax function as a surrogate for the argmax operation:
\begin{equation}
	x^{\text{SM}(\lambda)}_i(b) = \frac{\exp \left(\frac{b_{i}}{\lambda}\right)}{\sum_{j = 1}^n \exp \left(\frac{b_{j}}{\lambda}\right)}, \quad i = 1,\dots n.
\end{equation}
The temperature $\lambda > 0$ denotes the smoothing strength. This can be interpreted as dividing the item among all bidders according to their proportional bid magnitudes, where $\sum_{i} x^{\text{SM}(\lambda)}_{i}(b) = 1$ remains valid. The softmax asymptotically recovers the true argmax as $\lambda$ approaches zero.
As we are interested in a continuous utility surface, the discontinuity in the prices (only the winners pay) must also be considered. An obvious choice is to calculate the original prices of the good and then distribute the price according to the fractional allocations $x^{\text{SM}(\lambda)}$:
\begin{equation}
	p^{\text{SM}}(b) = \sum_{j=1}^n p_j(b).
\end{equation}
Hence, the ex post utility in the relaxed game takes the form
\begin{align}
	\label{eq:post-utility-smooth}
	u_i^{\text{SM}(\lambda)}(v_i, b) = \left(v_i - p^{\text{SM}}(b)\right) x_i^{\text{SM}(\lambda)}(b).
\end{align}
% Note that unlike in standard auction notation, where the valuations are multiplied with the allocations and then subtracted by the prices (see \autoref{eq:general-post-utility}), we extract the allocations to allow for the extension of payments to ranges where a bidder would lose. 
By definition, we have almost everywhere (a.e.) pointwise convergence of $x_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}(\argdot))$ to $x_i(v_i, b_i, \beta_{\text{-}i}(\argdot))$ as functions of $v_{\text{-}i}$, except at $b_i = \max b_{\text{-}i}$. Furthermore, the fractional prices $p^{\text{SM}(\lambda)}(b_i, \beta_{\text{-}i}(\argdot))$ also converge a.e.\ pointwise to $p_i(b_i, \beta_{\text{-}i}(\argdot))$. Thus, the ex post utilities are recovered (a.e.) for ever smaller temperature. The resulting utilities are visualized for the special case of an FPSB auction (Figure~\ref{fig:ex-post-utility}).
Throughout the rest of the article, we make the following regularity assumptions.

\begin{assumption}
	\label{ass:main-assumptions-main-text}
	Consider a Bayesian auction game $G$ and assume:
	\begin{enumerate}
		\item The action $\mathcal A_i$ and valuation spaces $\mathcal V_i$ are compact intervals. \label{ass:regularities-interim-bound-val-action-space}
		\item $F$ is an atomless prior. \label{ass:regularities-interim-bound-atomless_prior}
		\item The bidding and pricing functions are measurable.
	\end{enumerate}
\end{assumption}

We regain continuity of the ex post utility and its gradient by this smoothing of allocations and payments. Specifically, we have the following theorem:
\begin{theorem} \label{thm:smoothing-gives-unbiased-gradient-estimator}
	Let the conditions of Assumption~\ref{ass:main-assumptions-main-text} hold and assume the pricing function $p^{\text{SM}}$, the marginal density functions $\{f_{\text{-}i|i}\}_{v_i \in \mathcal{V}_i, i \in \mathcal{I}}$, and strategies $\{\beta_i\}_{i \in \mathcal{I}}$ to be Lipschitz continuous. Then, the estimator on the smooth interim utility's gradient by sampling from the smoothed ex post utilities' gradients is unbiased, i.e.,
	\begin{align}
		\nabla_{\theta_i} \, \overline{u}^{\text{SM}}_i(v_i, b_i)
		= \mathbb{E}_{v_{\text{-} i}|v_i} [\nabla_{\theta_i} u^{\text{SM}}_i(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i}))],
	\end{align}
	for all $i \in \mathcal{I}$, $v_i \in  \mathcal{V}_i$, and $b_i \in \mathcal{A}_i$.
\end{theorem}

We refer to Appendix~\ref{sec:leibniz} for the proof.
Importantly, this relaxation technique is applicable to general markets with different payment rules, utility functions, or correlated priors.
Compared with the ES gradient estimate, where the parameter space is perturbed, the SM gradient estimate perturbs the utility function. Thus, the origin of bias is different and can be controlled by $\sigma$ for ES and by $\lambda$ for SM.

\subsection{Approximation Quality} \label{subsec:approximation-quality}
We check the validity of the smoothing intervention by ensuring that the error to the original game dynamics can be controlled by choosing a sufficiently small value of $\lambda$. This ensures that conducting policy optimization in the smoothed game can be expected to result in policy improvements in the original game. Furthermore, this will clarify the question of an optimal choice of the temperature value. 

Generally, analytically computing equilibria of the SM game is infeasible. Instead, we focus on comparing the expected interim and ex ante utilities in the original and SM game. A small error implies similar utility surfaces and gradient dynamics. 
Note that the ex post utilities can be quite different. Suppose multiple bidders compete for a single commodity and bidder~$i$ has approximately the same bid magnitude as the strongest opponent. The smoothed allocation is close to one-half, whereas the true allocation is either zero or one. This would result in a significant difference in the ex post utility driven by the magnitude of the utility discontinuity in the original auction. The probability of such large errors decreases with smaller smoothing factors; however, this event cannot be completely ruled out.
We verify in the following theorem, that the error in expected interim and ex ante utility approaches zero under mild assumptions on the auction format.

\begin{theorem}\label{thm:asymptotic}
	Let the conditions of Assumption~\ref{ass:main-assumptions-main-text} hold and suppose the payment rule $p$ is bounded. Then, for bidder~$i$, we have convergence in interim and ex ante utility: 
	\begin{enumerate}
		\item Let $v_i \in \mathcal V_i$ and $b_i \in \mathcal A_i$, then
		\begin{equation}
			\lim_{\lambda \rightarrow 0} \overline{u}_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}) \; = \; \overline{u}_i(v_i, b_i, \beta_{\text{-}i}).
		\end{equation}
		\item Further assume $\beta_i$ to be measurable. Then,
		\begin{equation}
			\lim_{\lambda \rightarrow 0} \tilde{u}_i^{\text{SM}(\lambda)}(\beta_i, \beta_{\text{-}i}) \; = \; \tilde{u}_i(\beta_i, \beta_{\text{-}i}).
		\end{equation}
	\end{enumerate}
\end{theorem}
The proof is delegated to Appendix~\ref{sec:proof-asymptotic}.
Theorem~\ref{thm:asymptotic} ensures that for ever smaller $\lambda$, the bias in the expected utilities vanishes compared with the utilities in the original game. This implies that the smoothed gradients converge, thus justifying gradient-based learning in the perturbed game. 
Although Theorem~\ref{thm:asymptotic} ensures convergence, it does not state how fast the error approaches zero. However, this information is crucial for practical applications. Therefore, we make the following additional assumptions on the auction format.

\begin{assumption} \label{ass:regularities-for-interim-bound}
	For all $i \in \mathcal{I}$ assume:
	\begin{enumerate}
		\item $\beta_i$ is strictly increasing and Lipschitz continuous. \label{ass:regularities-interim-bound-strategies}
		\item $\beta_i^{-1}$ is Lipschitz continuous. \label{ass:regularities-interim-bound-inverse}
		\item There exists a uniform bound for all marginal conditional prior density functions $f_{i|\argdot}$. \label{ass:regularities-interim-bound-prior}
		\item $p_i$ is bounded. \label{ass:regularities-interim-bound-pricing}
	\end{enumerate}
\end{assumption}

Note that assuming Lipschitz continuous strategies is satisfied by common function approximations, e.g., neural networks.
With these stronger assumptions, we can present a worst-case convergence rate of the interim and ex ante utility errors. 

\begin{proposition} \label{thm:linear-convergence-utility-error}
	Consider an auction with $n$ bidders that satisfies Assumptions~\ref{ass:main-assumptions-main-text} and \ref{ass:regularities-for-interim-bound}. Then, the absolute interim and ex ante utility errors are of order $\mathcal{O}(\lambda)$. 
\end{proposition}

\begin{proof}[Proof Sketch]
	Use substitution on the opponents' bidding strategies, followed by iterated use of HÃ¶lder's inequality. The details of the proof can be found in Appendix~\ref{sec:proof-linear-conv}.
\end{proof}

Note that Restrictions~\ref{ass:regularities-interim-bound-strategies} and \ref{ass:regularities-interim-bound-pricing} in Assumption \ref{ass:regularities-for-interim-bound} are standard in the literature \cite{krishna2009auction}. Restriction \ref{ass:regularities-interim-bound-inverse} is slightly stronger by demanding that strategy $\beta_i$ cannot become infinitely flat (e.g., a saddle-point would not be allowed). However, this restriction can be somewhat lifted resulting in a worse convergence rate. Details on this can be found in Appendix~\ref{sec:proof-linear-conv}. Finally, Restriction~\ref{ass:regularities-interim-bound-prior} holds for all commonly used prior distributions, however, it rules out perfect correlation.
Based on the previous result, we can characterize how a learned $\varepsilon$-BNE of the SM game translates to an approximate BNE the original game:

\begin{theorem} \label{thm:eps-equ-in-smooth-game-to-original-game}
	In an auction with $n$ bidders that satisfies Assumptions~\ref{ass:main-assumptions-main-text} and \ref{ass:regularities-for-interim-bound}, let $\beta^*$ be an ex ante $\varepsilon$-BNE in the smoothed game with smoothing parameter $\lambda$. Then $\beta^*$ is an ex ante $\varepsilon + \mathcal{O}(\lambda)$-BNE of the original game. 
\end{theorem}

The proof can be found in Appendix~\ref{sec:proof-cor-1}. 
The derived bounds in the previous results consider worst-case scenarios. However, we observed that the error may be significantly lower in practice. To rationalize this observation, we compare the worst-case bound to the exact error in a restricted setting.
Consider an FPSB auction with two bidders, independent uniform priors, and a linear bidding function of the second bidder, $\beta_2(v_2)=sv_2 + t$. Then, the bound derived in Proposition~\ref{thm:linear-convergence-utility-error} translates to
\begin{align}\label{eq:linear-ante-bound}
	\left| \tilde{u}_1^{\text{SM}(\lambda)}(\beta_1, \beta_2) - \tilde{u}_1(\beta_1, \beta_2) \right| \; \leq \; \frac{\ln(2) + 1}{s} \lambda.
\end{align}
In Figure~\ref{fig:smooth_error_bound}, we compare this bound (for bidder~2's BNE strategy with $s=0.5$ and $t=0$) to the exact interim utility error, which can be derived for this restricted setting (see Appendix~\ref{sec:exact-interim-error}).
The convergence rate of the interim utilities depends on the specific prior sample $v_1$ and bid $b_1$. The ex ante utilities converge more rapidly than predicted by the worst-case bound. We conjecture that this often holds in practice, resulting in better learning behavior than suggested by Proposition~\ref{thm:linear-convergence-utility-error}.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\columnwidth]{figures/smooth-error-bound.png}
	\caption{Comparison of the absolute utility errors. (i) The linear ex ante bound that holds for all valuations (gray dashed line) from Equation~\ref{eq:linear-ante-bound}. (ii) Exact interim utility errors when both bidders act according to the BNE for some exemplary valuations (colorized lines) and their sampled mean values $\pm$ standard deviation (shaded areas). (iii) The approximate ex ante error (black dotted line).}
	\label{fig:smooth_error_bound}
\end{figure}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.9\columnwidth]{figures/smooth-error-bound-heatmap.pdf}
% 	\caption{Absolute interim utility error $\overline{\varepsilon}_\lambda(v_1, b_1)$ from \autoref{eq:error-bound} for $\lambda = 0.02$ and an equilibrium opponent. Around the BNE strategy of bidder~1 (dashed line), the error is highest for large valuations. \todo{This figure can go.}}
% 	\label{fig:smooth_error_bound_heat}
% \end{figure}

% For deprecated payment rule.
% \begin{proposition}
% 	\label{prop:utility-difference}
% 	Consider the single-item FPSB auction with two bidders having uniform priors on the unit interval. Suppose that bidder $2$ has a linear strategy $\beta_2(v_2) = s \, v_2$ with $s \in (0, 1]$. Then for bidder $1$'s absolute interim utility difference $\overline{\varepsilon}_\lambda = |\overline{u}_1 - \overline{u}_1^\text{SM}|$ and for valuation $v_1$ and bid $b_1 \leq s$, it holds that
% 	\begin{equation}
% 		\label{eq:error-bound}
% 		\overline{\varepsilon}_\lambda(v_1, b_1) \; = \; C \, \lambda \, \Bigg| \ln \left( \frac{ \mathrm{e}^\frac{2b_1 - s}{\lambda} + \mathrm{e}^{b_1/\lambda} }{ 1 + \mathrm{e}^{b_1/\lambda} } \right) \Bigg|,
% 	\end{equation}
% 	where $C = \frac{|v_1 - b_1|}{s}$. 
% \end{proposition}

% \begin{proof}
% 	% Proof for deprecated payment rule.
% 	For the interim utility of bidder $1$ in the original game, we have
% 	\begin{align*}
% 		\overline{u}_1(v_1, b_1) = \int_{v_2=0}^\frac{b_1}{s} (v_1-b_1) \, \mathrm{d}v_2 = \frac{v_1 - b_1}{s} \, b_1.
% 	\end{align*} 
% 	In the smoothed auction, we have
% 	\begin{align*}
% 		&\overline{u}_1^\text{SM}(v_1, b_1)\\
% 		&\quad= \int_{v_2=0}^1 (v_1 - s \,v_2) x_1^\text{SM} (b_1, s \,v_2) \, \mathrm{d}v_2\\
% 		&\quad= (v_1 - b_1) \left[ v_2 - \frac{\lambda}{s} \ln \left( \mathrm{e}^{b_1/\lambda} + \mathrm{e}^{\frac{s \, v_2}{\lambda}} \right) \right]_{v_2 \, = \, 0}^1.
% 	\end{align*} 
% 	Hence,
% 	\begin{align}
% 		& \overline{u}_1(v_1, b_1) - \overline{u}_1^\text{SM}(v_1, b_1)\nonumber\\
% 		&\quad = \frac{v_1 - b_1}{s} \, b_1 - (v_1-b_1) \int_{v_2=0}^1 x_1^\text{SM} (b_1, s \, v_2) \, \mathrm{d}v_2\nonumber\\
% 		&\quad= \frac{v_1 - b_1}{s} \left( b_1 + \left( \lambda \ln \left(\frac{\mathrm{e}^{b_1/\lambda} - \mathrm{e}^{s/\lambda}}{\mathrm{e}^{b_1/\lambda} + 1} \right) - s \right) \right)\nonumber\\
% 		&\quad= \frac{v_1 - b_1}{s} \left( \lambda \ln \left( \mathrm{e}^{\frac{b_1 - s}{\lambda}} \right) + \lambda \ln \left(\frac{\mathrm{e}^{b_1/\lambda} - \mathrm{e}^{s/\lambda}}{\mathrm{e}^{b_1/\lambda} + 1} \right) \right)\nonumber\\
% 		&\quad= \frac{v_1 - b_1}{s} \, \lambda \, \ln \left( \frac{ \mathrm{e}^\frac{2b_1 - s}{\lambda} + \mathrm{e}^{b_1/\lambda} }{ 1 + \mathrm{e}^{b_1/\lambda} } \right).
% 	\end{align}
% 	Now considering the absolute difference, we get \autoref{eq:error-bound}.
% \end{proof}

% \note{As we draw valuation profiles for all bidders in practice, we have single samples estimates of the interim utility.}

% Old payment rule
% \begin{proposition}
% 	Suppose the assumptions of Proposition~\ref{prop:utility-difference} hold. Then the interim utility difference decreases at least linearly in $\lambda$:
% 	\begin{equation}
% 		\overline{\varepsilon}_\lambda(v_1, b_1) \; \leq \; \left( \frac{1 - s}{s} \ln(4) \right) \lambda
% 	\end{equation}
% 	for all $v_1 \in \mathcal V_1$ and $b_1 \in \mathcal A_1$.
% \end{proposition}

% \begin{proof}
% 	As seen earlier, in the worst case, we have $b_1 = s$ and $v_1 = 1$ \todo{Argue!}, thus $\overline{\varepsilon}_\lambda(v_1, b_1) \leq \overline{\varepsilon}_\lambda(1, s)$. From \autoref{eq:error-bound}, it follows that
% 	\begin{align}
% 		\overline{\varepsilon}_\lambda(1,s) &= \frac{1-s}{s} \lambda \, \Bigg| \ln \left(\frac{e^\frac{2s-s}{\lambda} + e^{s/\lambda}}{1 + e^{s/\lambda}} \right) \Bigg|\nonumber\\
% 		&= (s - 1) + \frac{1-s}{s}\lambda \, (\ln(2) + \ln(e^{s/\lambda} + 1))\nonumber\\
% 		&\leq (s - 1) + \frac{1-s}{s}\lambda \, (\ln(2) + s/\lambda + \ln(2))\nonumber\\
% 		&= \frac{1 - s}{s} \ln(4) \lambda,
% 	\end{align}
% 	where we make use of the fact that $\ln(e^{s/\lambda} + 1) \leq s/\lambda + \ln(2)$ for all non-negative values of $s/\lambda$.
% \end{proof}

% An ex ante analysis additionally requires assuming a fixed strategy of bidder~$1$:

% \begin{proposition}
% 	Let the assumptions of Proposition~\ref{prop:utility-difference} hold and assume both bidders are playing according to their BNE strategy, $\beta_i^*(v_i) = 1/2 \, v_i$, then the ex ante error, we have
% 	\begin{align}
% 		\tilde{\varepsilon}_\lambda(\beta_1) \vcentcolon= \int_{v_1=0}^1 \overline{\varepsilon}_\lambda(v_1, \beta_1(v_1)) \, \mathrm{d}v_1 \; < \; 0.1015 \, \lambda.
% 	\end{align}
% \end{proposition}

% \begin{proof}
% 	The interim error now simplifies to
% 	\begin{align}
% 		\overline{\varepsilon}_\lambda(v_1, v_1/2) \; = \; v_1 \, \lambda \, \ln \left(\dfrac{\mathrm{e}^\frac{v_1}{2{\lambda}} + \mathrm{e}^\frac{v_1 - 1/2}{{\lambda}}}{\mathrm{e}^\frac{v_1}{2{\lambda}} + 1}\right).
% 	\end{align}
% 	Taking the expectation over $1$'s valuations and using a computer algebra system, we get
% 	\begin{align}
% 		\label{eq:ex_ante_bound}
% 		\tilde{\varepsilon}_\lambda(\beta_1^*) &= \Big[ 2 \lambda^2 \Big(v_1 \operatorname{Li}_2(-\mathrm{e}^{-\frac{v_1 - 1}{2 \lambda}})\nonumber\\
% 		& - v_1 \operatorname{Li}_2(-e^{-\frac{v_1}{2 \lambda}}) + 2 \lambda \Big(\operatorname{Li}_3(-\mathrm{e}^{-\frac{v_1 - 1}{2 \lambda}})\nonumber\\
% 		& - \operatorname{Li}_3(-\mathrm{e}^{-\frac{v_1}{2 \lambda}})\Big)\Big) + \frac{1}{12} (2 v_1 - 3) v_1^2 \Big]_{v_1 \, = \, 0}^1\nonumber\\
% 		& = -\lambda^3 (4 \operatorname{Li}_3(-\mathrm{e}^{\frac{1}{2 \lambda}}) + 6 \operatorname{\zeta}(3))\nonumber\\
% 		& - 4 \lambda^3 \operatorname{Li}_3(-\mathrm{e}^{-\frac{1}{2 \lambda}}) - 2 \lambda^2 \operatorname{Li}_2(-\mathrm{e}^{-\frac{1}{2 \lambda}})\nonumber\\
% 		& - \frac{\pi^2 \lambda^2}{6} - \frac{1}{12}.
% 	\end{align}
% 	Here, $\operatorname{Li}$ is the polylogarithm and $\zeta$ the Riemann zeta function. 
% 	One can now verify the linear upper bound numerically.
% \end{proof}


\subsection{Choosing the Smoothing Temperature}\label{sec:optimal-temp}
Let us consider the question of an optimal smoothing strength.
There is an incentive to keep temperature values as low as possible, such that the original game dynamics are distorted as little as possible. On the other hand, one does not want to decrease $\lambda$ too low, as this causes numerical problems. The magnitude of the gradient goes towards infinity at the former discontinuity as $\lambda$ decreases. Therefore, with finite sample size, the first-order gradient estimate might have a high empirical variance \cite{suh2022differentiable}.

We propose to use the utility sampling precision as a natural way to choose the temperature. For the special case presented in Figure~\ref{fig:smooth_error_bound} and the default batch size of $2^{18}$, one can see that the sample precision is reached at about $10^{\text{-}4}$. That is, for a drawn batch, the Monte Carlo estimation of ex ante utilities has a precision of about $10^{\text{-}4}$, and we can no longer distinguish between the smoothed and original utilities. Therefore, one can use Proposition~\ref{thm:linear-convergence-utility-error} to derive a lower bound for $\lambda$ for a given sampling precision. As discussed at the end of Section~\ref{subsec:approximation-quality}, the true ex ante utility error is usually lower, so that one can choose a higher $\lambda$ without losing any performance. 

The empirical sampling precision is affected by several factors, such as the valuation and bidding ranges, the number of bidders, prior distributions, and complexity of bidding functions. Some of these influences can be standardized, e.g., by normalizing the bidding ranges. Ultimately, a sufficiently high batch size can overcome any bias introduced by aforementioned factors, such that it should be chosen as high as computationally possible to achieve an optimal sampling precision.
% The next section concerns itself with analyzing these considerations and the bias variance trade-off in numerical experiments.


\section{Empirical Results}\label{sec:results}

We provide experimental evaluation of the new technique and compare the results with those of NPGA and REINFORCE by measuring how closely they approximate the analytical BNE. Results for settings with risk aversion or correlated valuations are similar and omitted for simplicity.
Furthermore, we provide some insights and guidance on appropriate choices of $\lambda$ and verify that our gradient estimate's variance is sufficiently small.
We list all hyperparameters and details on the network architecture in Appendix~\ref{sec:reproducibility}.

\begin{table}[t]\fontsize{8}{10}\selectfont
	\centering
	\caption{Learning results in FPSB and SPSB auctions with different numbers $m$ of items. We report the mean values of the $L_2$ and $\overline{\ell}_{\text{max}}$ losses (smaller is better) and the time per iteration across five runs. We also report the standard deviation in parentheses for the losses.}
	\vspace{0.1in}  % "with at least 0.1 inches of space before the title"
	\begin{tabular}{lllrrr}
		\toprule
		        			  & $m$ 				   & Algorithm 	   & $L_2$ 			& $\overline{\ell}_{\text{max}}$ & $t/$iter \\
		\midrule
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{FPSB}}} & \multirow{3}{*}{1} & NPGA	   &  0.011 (0.005) &       0.005 (0.002) &  0.155 \\
																		&					   & REINFORCE &  0.021 (0.008) &       \textbf{0.003 (0.000)} &  \textbf{0.009} \\
																		&					   & SM 	   &  \textbf{0.005 (0.003)} &       0.004 (0.002) &  \textbf{0.009} \\
																		\cline{2-6}
																		& \multirow{3}{*}{2}   & NPGA      &  0.013 (0.005) &       0.010 (0.002) &  0.150 \\
																		&					   & REINFORCE &  0.041 (0.020) &       0.016 (0.010) &  \textbf{0.009} \\
																		&					   & SM 	   &  \textbf{0.008 (0.002)} &       \textbf{0.006 (0.003)} &  \textbf{0.009} \\
																		\cline{2-6}
																		& \multirow{3}{*}{4}   & NPGA      &  0.028 (0.002) &       0.021 (0.003) &  0.148 \\
																		&   				   & REINFORCE &  0.064 (0.018) &       0.039 (0.012) &  \textbf{0.009} \\
																		&   				   & SM 	   &  \textbf{0.015 (0.004)} &       \textbf{0.011 (0.004)} &  \textbf{0.009} \\
																		\cline{2-6}
																		& \multirow{3}{*}{8}   & NPGA      &  0.104 (0.054) &       0.127 (0.109) & 0.206 \\
																		&   				   & REINFORCE &  0.187 (0.073) &       0.331 (0.169) & \textbf{0.012} \\
																		&   				   & SM 	   &  \textbf{0.036 (0.003)} &       \textbf{0.034 (0.009)} & \textbf{0.012} \\
		\cline{1-6}
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{SPSB}}} & \multirow{3}{*}{1} & NPGA      &  0.012 (0.001) &       0.002 (0.000) &  0.170 \\
																		&   				   & REINFORCE &  0.028 (0.005) &       0.002 (0.000) &  \textbf{0.009} \\
																		&   				   & SM 	   &  \textbf{0.004 (0.001)} &       \textbf{0.001 (0.000)} &  0.011 \\
																		\cline{2-6}
																		& \multirow{3}{*}{2}   & NPGA      &  0.018 (0.002) &       0.003 (0.000) &  0.264 \\
																		&   				   & REINFORCE &  0.082 (0.020) &       0.009 (0.002) &  0.011 \\
																		&   				   & SM 	   &  \textbf{0.007 (0.001)} &       \textbf{0.002 (0.000)} &  \textbf{0.015} \\
																		\cline{2-6}
																		& \multirow{3}{*}{4}   & NPGA      &  0.043 (0.002) &       0.011 (0.003) &  0.457 \\
																		&   			       & REINFORCE &  0.140 (0.045) &       0.028 (0.018) &  \textbf{0.017} \\
																		&   				   & SM	       &  \textbf{0.029 (0.003)} &       \textbf{0.006 (0.002)} &  0.024 \\
																		\cline{2-6}
																		& \multirow{3}{*}{8}   & NPGA      &  0.214 (0.112)	&       0.299 (0.238) &  0.869 \\
																		&   				   & REINFORCE &  0.320 (0.128) &       0.262 (0.174) &  \textbf{0.031} \\
																		&   				   & SM 	   &  \textbf{0.074 (0.002)} &       \textbf{0.020 (0.002)} &  0.043 \\
		\bottomrule
	\end{tabular}
	\label{tab:single_item}
\end{table}

\subsection{Single-Item Auctions}
For the two common payment rules of FPSB and second-price sealed-bid (SPSB) and a uniform prior on $[0,1]$, we can measure the distance in action space to the unique BNE, as described in Equation~\ref{eq:L_2} and compute an estimate of exploitability in the form of Equation~\ref{eq:BNE}. Table~\ref{tab:single_item} shows the results. The losses are computed after training 2,000 iterations with each algorithm. The time per iteration, $t$/iter, decreases notably when comparing NPGA to SM across both payment rules, while also achieving a better approximation quality. Since the estimation of $\overline{\ell}_{\text{max}}$ relies on a discretization of the action space and an exhaustive search thereon, $L_2$ detects smaller deviations, ceteris paribus. Although REINFORCE has a low iteration time, it is unable to learn high quality strategies due to its high variance (Section~\ref{sec:variance-empirical}).
% \autoref{fig:single_item} exemplarily shows learning in the FPSB auction.
We found that results for auctions with interdependent prior valuations or risk-aversion are quantitatively consistent with the results presented here.
% These include the single-item common value model (with conditionally independent observations) and the affiliated value model \cite{milgrom1982theory}.


% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.9\columnwidth]{figures/single_item_analysis.pdf}	
% 	\caption{Action space distance for learned strategies to BNE strategy for NPGA baseline and our approach in a two-bidder single-item FPSB auction. Depicted are the mean $\pm$ standard deviation over five runs each.}
% 	\label{fig:single_item}
% \end{figure}


\subsection{Large Simultaneous Auctions}
Furthermore, we study the separate sales of up to $m=8$ distinctive goods and an increase in the number of bidders of up to $n=4$.
% These are the largest configurations that are still computable with an adequate batch size on our machine.
For simplicity, we do not consider any synergy effects on the items (this would include cases such as those where a bidder only values the bundle of two items but not either one of them individually), such that the BNE simplifies to the single-item strategy profile for each item separately.
There are multiple motivations for these auctions. They can be considered as the base case of combinatorial auctions with item bidding and as a simple and practical alternative to full combinatorial auctions. Furthermore, combinatorial auctions with item bidding are being deployed, e.g., a bidder who is interested in a bundle of objects in parallel online display ad auctions or on a consumer shopping website is implicitly partaking in these auctions.
Finally, asking a bidder to submit bids on all possible combinations of bundles ($2^m - 1$) is practically infeasible and there are positive results on the welfare properties of limiting the action space in this way \cite{bhawalkar2011welfare}.
Again, we draw i.i.d.\ uniform valuations on $[0, 1]$ and consider the FPSB and SPSB auctions.
Learning in the SM game outperforms both previous approaches (Table~\ref{tab:single_item}).
Since first-order methods are generally faster, we assume that the strong results in these settings will scale to even larger ones.

% Local-global combinatorial auctions (LLG) serve as main benchmarks for BNE computation. In such auctions, there are two groups of bidders, locals and globals: Globals are interested in larger bundles of items while their priors allow them to draw higher valuations, so local bidders need to coordinate to outbid the globals. We consider settings where two locals have independent uniform priors and compete against a single global bidder that has a stronger uniform prior. This three-player auction is a standard setting in auction theory and one of the smallest CAs that requires strategic cooperation between bidders.
% Exemplary results for the nearest zero (NZ) and the VCG payment rule can be found in \autoref{tab:llg}. As for single-item auctions, our method's speed up carries over to the LLG auction while also matching or outperforming the approximation quality.
% Performance and run times are comparable for alternative first-price and nearest-VCG payment rules (omitted in table).

% The results are depicted in \autoref{fig:batch_analysis}.
% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.9\columnwidth]{figures/temperature_batch_size_analysis.pdf}
% 	\caption{Action space distance for learned strategies to BNE strategy for different batch sizes and different temperature values $\lambda$. The corresponding optimal values are depicted as triangles.}
% 	\label{fig:batch_analysis}
% \end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{figures/variance-analysis.png}
	\caption{Empirical variance of the NPGA and REINFORCE zeroth-order and the SM first-order gradient estimates. Both zeroth-order methods are run in the original auction game. The mean values $\pm$ standard deviations over five runs each are depicted. \emph{Left:} Comparing the variance throughout the learning procedure. \emph{Right:} Comparing the variance for different smoothing temperatures (averaged over complete training runs).}
	\label{fig:variance_analysis}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\columnwidth]{figures/temperature_players_analysis.png}
	\caption{Action space distance for learned strategies to the BNE for different numbers of bidders and temperature values $\lambda$. The mean values $\pm$ standard deviations over five runs each are depicted.}
	\label{fig:temperature_analysis}
\end{figure}

\subsection{Empirical Variance}\label{sec:variance-empirical}
As stated in Section~\ref{sec:optimal-temp}, there is a trade-off between low and high values of $\lambda$. 
Here, we consider the base setting of two bidders competing in a single-item FPSB auction. We decrease the batch size to $2^{16}$ as the single-sample gradients require more memory.
Considering NPGA that is based on a sample of 64 evaluations of the objective by default, the empirical variance of the SM estimate is lower for all $\lambda > 0.002$ (compare intersection of Figure~\ref{fig:variance_analysis}, right plot). Even after increasing NPGA's population size by a factor of two (which scales the run time in the same way), SM's variance remains lower for most choices, as can be seen in the left figure.
The empirical variance of REINFORCE rapidly increases as the mixed-strategies get closer to the pure-strategy BNE. This degradation is to be expected when the learned variance of the Gaussian distributed actions decreases, see Exercise~13.4 of \cite{sutton2018reinforcement}.

Results for markets of different sizes are depicted in Figure~\ref{fig:temperature_analysis}. Keeping everything else fixed, the highest achievable performance decreases for larger markets, as is expected in multi-agent learning. The optimal smoothing strength is only affected indirectly via the bid magnitudes. 
% Note that the number of items does not play a role for choosing an optimal smoothing value, because these are essentially auctioned off independently.
At last, we note that the performance boost of larger batch sizes diminishes and best results are achieved for similar values of $\lambda$ just below $0.01$, indicating that the variance of the gradient estimate counteracts the lower bias. The results are presented in Appendix~\ref{sec:bacth-size}.



\section{Conclusion and Future Work}

How can first-order gradient estimation methods be successfully applied to learning in auctions?
We showed that our proposed smooth game formulation of strategic interactions in auctions provides a strong answer to this question. We established theoretical bounds on the bias caused by the smoothing, and an empirical evaluation verified that the variance of the gradient estimate can be controlled, leading to low computational costs and high precision. Overall, we verified that equilibrium computation in smooth markets via fist-order gradient estimation is more efficient than previous learning methods.

% In future work, we plan to better understand the convergence behavior to a BNE and extend the method to more realistic markets such as large combinatorial and sequential auctions where analytical solutions are unavailable.


\input{ms.bbl}
%\bibliography{bibliography}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{technical-appendix.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \section{Stochastic Computation Graphs}

% \cite{schulman2015gradient} state:
% \begin{definition}[Stochastic Computation Graph] A directed, acyclic graph, with three types of nodes:
% \begin{enumerate}
% 	\item Input nodes, which are set externally, including the parameters we differentiate with respect to.
% 	\item Deterministic nodes, which are functions of their parents.
% 	\item Stochastic nodes, which are distributed conditionally on their parents.
% \end{enumerate}
% Each parent $v$ of a non-input node $w$ is connected to it by a directed edge $(v, w)$.
% \end{definition}
% The stochastic computation graph of auctions is sketched in \autoref{fig:computation_graph}.
% \begin{figure*}[h]
% 	\centering
% 	\includegraphics[width=0.7\textwidth]{figures/computation-graph.pdf}
% 	\caption{Stochastic computation graph of auctions as Bayesian games depicted from the perspective of agent $i$. Importantly, $a_i$ is a Bernoulli distribution (in the single item case) with an unknown probability of success determined by the opponents. \note{Actually, we could also depict $p_i$ as being dependent on $b_i$ and $a_i$, this way, we would only have a single edge $(b_i, a_i)$ that causes the issue.}}
% 	\label{fig:computation_graph}
% \end{figure*}



% \section{Related Work on Gradient Estimation}

% \todo{This section should be dropped or at least revised because REINFORCE does work in auctions.}

% Generally, this area is concerned with the problem of computing the gradient of an expectation of a function (a stochastic gradient).
% \textbf{\cite{mohamed2020monte}} survey Monte Carlo gradient estimation for learning the distributional parameters $\theta$ by following the gradient
% \begin{equation}
% 	\eta \; := \; \nabla_\theta \mathbb{E}_{x \sim p(x;\theta)}[f(x;\phi)],
% \end{equation}
% where we refer to $f$ as the cost and to $p(x; \theta)$ as the measure. (The structural parameters $\phi$ may be dropped.) They assume the measure is a probability distribution that is continuous in its domain and differentiable w.r.t.\ $\theta$.
% \begin{box_orange}
% 	In the context of reinforcement learning, one usually considers $f$ to be the reward (or a\mathrm{d}vantage) function, $p$ the policy network to be trained with parameters $\theta$ and outputting a distribution over actions for any observation.
% \end{box_orange}
% They follow a threefold structure: \todo{Why or why not are these estimates applicable?!}

% \subsection{Score Function Estimator}
% The \emph{score function estimators} (a.k.a.\ ``REINFORCE'', likelihood-ratio estimator) is one of the most general types of gradient estimators. \emph{Policy gradients} are a special case of this class of gradient estimators. It uses the log-derivative trick and is helpful when that derivative can be calculated:
% \begin{equation}
% 	\eta \; = \; \mathbb{E}_x [f(x) \nabla_\theta \log p(x;\theta)].
% \end{equation}
% The estimate then is
% \begin{equation}
% 	\bar{\eta}_N \; = \; \frac{1}{N} \sum_{i=1}^N f(\hat{x}^{(n)}) \nabla_\theta \log p(\hat{x}^{(n)};\theta), \quad \hat{x}^{(n)} \sim p(x;\theta).
% \end{equation}
% Any type of cost function can be used, allowing for the use of simulators and other black-box systems, as long as we are able to evaluate them easily. The measure must be differentiable with respect to its parameters. One must be able to easily sample from the measure. It is applicable to both discrete and continuous distributions of $x$. The estimator can be implemented using only a single sample if needed. Because there are many factors that affect the variance of the gradient estimator, it will be important to use some form of variance reduction to obtain competitive performance.


% \subsection{Pathwise Estimator}
% \emph{Pathwise estimators} (a.k.a.\ reparameterization trick, stochastic backpropagation) need information about the path underlying the probabilistic objective. Assume $x$ is deterministic and differentiable in $\theta$ and we can write it as a function of another random variable $x(z,\theta)$, then we have and
% \begin{equation}
% 	\eta \; = \; \mathbb{E}_z [\nabla_\theta f(x(z,\theta))].
% \end{equation}
% The estimate then is
% \begin{equation}
% 	\bar{\eta}_N \; = \; \frac{1}{N} \sum_{i=1}^N \nabla_\theta f \left(g(\hat{\epsilon}^{(n)})\right), \quad \hat{\epsilon}^{(n)} \sim p(\epsilon).
% \end{equation}
% Therefore, this class of gradient estimator is less general-purpose than the score-function estimator, but has a\mathrm{d}vantages in terms of lower variance and ease of implementation. It usually is the estimator of choice when it is applicable. It works for continuous random variables and relies on the law of the unconscious statistician. One can compute the expectation of a function of a random variable without knowing its distribution, if its corresponding sampling path and base distribution is available. Only cost functions that are differentiable can be used. The pathwise estimator has also found uses in reinforcement learning for continuous control \cite{heess2015learning}. \todo{ Look into this work.}
% \begin{box_red}
% 	\textbf{Observation.} The reparameterization trick (at least in its common form) cannot be used: Essentially, it only means that one ``outsources'' the sampling from the path along which back-propagation is performed. This is already done in our case (sampling $b_{\text{-}i}$); however, the problem is that the resulting single sample allocations are misleading.
% \end{box_red}

% \subsubsection*{Continuos Relaxations}
% \textbf{\cite{maddison2017}} introduce \emph{``concrete'' reparametrizations} as continuos relaxations of discrete random variables. They argue that the well known reparametrization trick only works well for continuous variables, where a reparametrization can be derived.
% The sampling of a discrete random variable via the \emph{Gumbel-Max trick} can be refactored into a deterministic function (component-wise addition followed by $\argmax$) of the parameters $\log p_i$ and fixed distribution. However, as the derivative of the $\argmax$ is 0 everywhere except at the boundary of state changes, where it is undefined, the Gumbel-Max trick is not a suitable reparameterization for use in SCGs with AD. Therefore, they first sample from a Gumbel distribution and then relax the $\argmax$ by a softmax operation.
% The proposed relaxation comes with a closed form density and leads to a biased estimator.
% \todo{Is this following up on \cite{mnih2014neural}?}

% \textbf{\cite{tucker2017rebar}} consider estimators for latent variables and improve the concrete reparametrizations from \cite{maddison2017}.

% \textbf{\cite{grathwohl2018backpropagation}} \todo{Todo.}

% \grey{\subsection{Measure-Valued Gradient Estimator}
% The class of \emph{measure-valued gradient estimators} exploits the underlying mea\-sure-theoretic properties of the probabilities and leads to an unbiased and general-purpose (cost function may not be differentiable) gradient estimator with favorable variance properties.
% The (component-wise $i$th) estimate then is
% \begin{equation}
% 	\bar{\eta}_{i,N} \; = \; \frac{c_{\theta_i}}{N} \left( \sum_{i=1}^N f(x'^{(n)}) - \sum_{i=1}^N f(x''^{(n)}) \right),
% \end{equation}
% with
% \begin{equation*}
% 	x' \sim p_i^+(x; \theta), \quad x'' \sim p_i^-(x;\theta),
% \end{equation*}
% where $p^+$ and $p^-$ are densities, referred to as the positive and negative components of $p$, such that $\nabla_{\theta_i} p(x; \theta) = c_{\theta_i} (p^+(x; \theta) - p^-(x; \theta))$ (weak derivative).
% It is not frequently used, because using the weak derivative requires manual derivation of the decomposition at first. The estimator relies on the fact that one can always decompose the derivative of a density into a difference of two densities, each multiplied by a constant. It is applicable to both discrete and continuous distributions.
% }
% \subsection{Further Notes}
% If the measure is discrete on its domain then the score-function or measure-valued gradient are available. \todo{What exactly does that mean?! Is it applicable or not?}

% Computational graphs \cite{bauer1974computational} are now the standard formalism that define a sequence of computations as directed acyclic graph from inputs and parameters to final outputs and costs. This is the framework in which automatic differentiation systems operate. If some of these nodes are random, this gives rise to \emph{stochastic computational graphs} which are analyzed in detail by \cite{schulman2015gradient}.

% % \subsubsection{Common Areas of Application}
% % Common areas of application are: (i) Variational inference where one wants to estimate a distribution, e.g., in image generation, (ii) reinforcement learning MCGE leads to the policy gradient theorem, (iii) sensitivity analysis as applied in, e.g., computational finance, (iv) in discrete event systems and queuing theory, the gradient of the expectation of a cost, described as a ratio, is needed to optimize a sequential process, and (v) experimental design collects problems of black box functions with high cost of evaluation.


% \subsubsection{Straight-Through Estimators}
% \todo{Can they be assigned to one of the above categories?}

% \textbf{\cite{bengio2013estimating}} introduce unbiased estimator and talk about the straight-through estimator for Bernoulli variables. \note{WIP.}

% \textbf{\cite{shekhovtsov2021bias}} analyze multiple single sample biased estimators. \note{WIP.}

% \textbf{\cite{raiko2014techniques}} look at stochastic binary hidden units in a MLP and show that the case of a single sample leads to a fundamentally different behavior. That is, the network always prefers to minimize the stochasticity, e.g., by increasing the input weights to a stochastic sigmoid unit such that it behaves as a deterministic step-function nonlinearity.

% \textbf{\cite{pervez2020low}} look at stochastic neural networks with discrete random variables. They argue that stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow stochastic models, however, their performance suffers with more complex models. Thus, they propose a low-bias and low-variance gradient estimation algorithm. \note{Not that relevant as they aim on solving a specific issue.}

% There is related work that uses PG wih weak derivatives \cite{bhatt2019policy}.

% \ \\
% \note{What are Discrete Latent Variable Models? $\rightarrow$ Latent variables are just hidden somewhere and not directly observable.}


\end{document}
