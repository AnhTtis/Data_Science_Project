% \documentclass{article}
% \usepackage[a4paper, total={6.5in, 9.5in}]{geometry}

% \usepackage{times}  % DO NOT CHANGE THIS
% \usepackage{helvet}  % DO NOT CHANGE THIS
% \usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
% %
% % These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

% %
% % These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
% \usepackage{newfloat}
% \usepackage{listings}

% \usepackage[utf8]{inputenc}
% \usepackage{mathtools}
% \usepackage{enumerate}
% \usepackage{amsmath, amsthm, amssymb, amsfonts}
% \usepackage{booktabs}
% \usepackage{relsize}
% \usepackage[american]{babel}
% \usepackage{multirow}

% % Comment out this line in the camera-ready submission
% \usepackage{lineno}
% \linenumbers

% \allowdisplaybreaks

% % Load main file with labels
% \usepackage{xr-hyper} 
% \usepackage[hidelinks]{hyperref}
% \externaldocument{main}

% % citations
% \bibliographystyle{abbrvnat}
% \setcitestyle{authoryear}

% \usepackage{aligned-overset}

% % WIP
% \usepackage{xcolor}
% \definecolor{blue}{RGB}{28,132,218}
% \definecolor{green}{RGB}{29,163,35}
% \usepackage[]{todonotes}
% \newcommand{\note}[1]{\textcolor{orange}{\emph{\textbf{NOTE:} #1}}}
% % \newcommand{\todo}[1]{\textcolor{red}{\emph{\textbf{TODO:} #1}}}
% \newcommand{\grey}[1]{\textcolor{grey}{#1}}
% \definecolor{darkviolet}{rgb}{0.58, 0.0, 0.83}
% \newcommand{\MB}[1]{\textcolor{darkviolet}{MB: #1}}
% \newcommand{\FP}[1]{\textcolor{green}{FP: #1}}
% \newcommand{\NK}[1]{\textcolor{blue}{NK: #1}}



% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}{Corollary}[section]
% \newtheorem{conjecture}{Conjecture}[section]
% \newtheorem{proposition}{Proposition}[section]
% \newtheorem{lemma}{Lemma}[section]
% \newtheorem{definition}{Definition}[section]
% \newtheorem{example}{Example}[section]
% \newtheorem{remark}{Remark}[section]
% \newtheorem{assumption}{Assumption}

% % Math ops
% \newcommand{\argdot}{\,\cdot\,}
% \DeclareMathOperator{\Exists}{\exists}
% \DeclareMathOperator{\Forall}{\forall}

% \DeclarePairedDelimiter\abs{\lvert}{\rvert}%
% \newcommand{\norm}[1]{\left\lVert#1\right\rVert}




% \title{\textbf{Technical Appendix for ``Gradient-Based Equilibrium Computation for Markets: A Smoothing Technique''}}
% \author {
%     \textbf{Anonymous submission}
% }
% % \author {
% %     % Authors
% %     % All authors must be in the same font size and format.
% %     Nils Kohring,\textsuperscript{\rm 1}
% %     Fabian Pieroth, \textsuperscript{\rm 1}
% %     Martin Bichler, \textsuperscript{\rm 1}
% % }
% % \affiliations {
% %     % Affiliations
% %     \textsuperscript{\rm 1} Technical University of Munich\\
% %     nils.kohring@tum.de, martin.bichler@tum.de
% % }



% \begin{document}

% \maketitle


% \appendix


\section{Proof of Theorem~\ref{thm:smoothing-gives-unbiased-gradient-estimator}}\label{sec:leibniz}

The Leibniz integral rule states conditions for which the operator interchange of taking the limit and integrating is valid. Let us recall its measure theory variant using the notation of our application. For a rigorous treatment of the assumptions and different variants, we refer to the work of \citet{talvilaNecessarySufficientConditions2001}, see Corollaries~5 and 8. We reformulate Condition~1 in our version using Fubini's Theorem.
\begin{theorem}[Leibniz integral rule]\label{thm:leibniz}
	Let $[a, b] \subset \mathbb{R}$ and $\Omega$ with $\mu$ be a probability space. Suppose $f: [a, b] \times \Omega \to \mathbb{R}$ satisfies the conditions: 
	\begin{enumerate}
		\item $f(x, \omega)$ is a measurable function in $x$ and $\omega$, and is integrable over $\omega \in \Omega$, for almost all $x \in [a, b]$. \label{thm:leibniz-condition-measurability-and-integrability}
		\item For almost all $\omega \in \Omega$, $f(x, \omega)$ is absolutely continuous in $x$. \label{thm:leibniz-condition-absolutely-continuous}
		\item For all compact intervals $[c, d]\subset [a, b]$:
		 \begin{align}
		 	\int_c^d \int_{\Omega} \left| \frac{\partial}{\partial x} f(x, \omega) \right| \mathrm{d} \mu(\omega) \mathrm{d} x < \infty.
		 \end{align} \label{thm:leibniz-condition-locally-integrable}
	\end{enumerate}
	Then 
	\begin{equation}
		\label{eq:leibniz}
		\frac{\partial}{\partial x} \int_{\Omega} f(x, \omega) \, \mathrm{d}\mu(\omega) \; = \; \int_{\Omega} \frac{\partial}{\partial x} f(x, \omega)\, \mathrm{d} \mu(\omega) \qquad \text{a.e.}
	\end{equation}
\end{theorem}

Equation~\ref{eq:leibniz} is assumed to hold for backpropagation. It is needed for the approximation of ex ante gradients based the sample mean of ex post gradients (compare Equation~\ref{eq:operator_interchange} from main text). We are now ready to prove Theorem~\ref{thm:smoothing-gives-unbiased-gradient-estimator}.

\begin{proof}
	We show that under the assumptions made in Theorem~\ref{thm:smoothing-gives-unbiased-gradient-estimator}, the Leibniz integral rule as formulated in Theorem~\ref{thm:leibniz} holds. We proceed to show that the smooth ex post utilities $u_i^{\text{SM}(\lambda)}$ are Lipschitz continuous, which essentially ensures all three conditions hold.
	So, for $i \in \mathcal{I}$, recall the smoothed ex post utility for some $v_i \in \mathcal{V}_i$ and $\lambda >0$:
	\begin{align}
		u_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i})) = \left(v_i - p^{\text{SM}}(b_i, \beta_{\text{-} i}(v_{\text{-}i})) \right) x_i^{\text{SM}(\lambda)}(b_i, \beta_{\text{-} i}(v_{\text{-}i})).
	\end{align}
	As the smooth pricing function $p^{\text{SM}}$ is a sum over Lipschitz continuous functions, it is Lipschitz continuous. As $\beta_{\text{-} i}$ and $x_i^{\text{SM}(\lambda)}$ are Lipschitz continuous, so is $x_i^{\text{SM}(\lambda)}(b_i, \beta_{\text{-} i}(\argdot))$. Finally, as both $p^{\text{SM}}$ and $x_i^{\text{SM}(\lambda)}$ are bounded and Lipschitz continuous, their product is Lipschitz continuous as well. Therefore, $u_i^{\text{SM}(\lambda)}$ is Lipschitz continuous in $b_i$ and $v_{\text{-}i}$. 
	
	The Lipschitz continuity of $u_i^{\text{SM}}$ ensures measurability, as well as integrability over $\mathcal{V}_{\text{-}i}$ for all $b_i \in \mathcal{A}_i$. Hence, Condition~\ref{thm:leibniz-condition-measurability-and-integrability} holds. As Lipschitz continuity is stronger than absolute continuity, Condition~\ref{thm:leibniz-condition-absolutely-continuous} holds as well. Finally, note that due to Lipschitz continuity, there exists an $L$ such that $\left| \frac{\partial}{\partial b_i} u_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i})) \right| \leq L$ for all $b_i \in \mathcal{A}_i$. This bound ensures that Condition~\ref{thm:leibniz-condition-locally-integrable} holds also. 
	
	In the case of conditional priors, consider the function $u_i^{\text{SM}(\lambda)} f_{\text{-}i|\argdot}$. This function is again Lipschitz continuous as product of bounded Lipschitz continuous functions. Repeating the steps above for this function finishes the proof. 
\end{proof}

\begin{remark}
	The original non-smooth ex post utility function $u_i$ does not satisfy the conditions of Theorem \ref{thm:leibniz}. For example, $u_i$ is not even continuous in $b_i$, so that the second condition is violated.
\end{remark}

\section{Proof of Theorem \ref{thm:asymptotic}}\label{sec:proof-asymptotic}

\begin{proof}
	Let us start with the first statement. For the interim utility of bidder~$i$ in the original game, we have
	\begin{align}
		\label{eq:iterim-utility}
		\overline{u}_i(v_i, b_i, \beta_{\text{-}i}) &= \mathbb{E}_{v_{\text{-}i}|v_i}\left[ u_i(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i}))\right]
	\end{align}
	with the ex post utility from Equation~\ref{eq:general-post-utility} in the main text rewritten as
	\begin{align}
		\label{eq:post-utility-original}
		u_i(v_i, b_i, b_{\text{-}i}) = (v_i - p_i(b_i, b_{\text{-}i})) x_i(b_i, b_{\text{-}i}).
	\end{align}
	In the smoothed auction, we have
	\begin{align}
		\label{eq:smooth-iterim-utility}
		\overline{u}_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}) = \mathbb{E}_{v_{\text{-}i}|v_i}\left[ u_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i})) \right]
	\end{align}
	with $u_i^{\text{SM}(\lambda)}$ as defined in Equation~\ref{eq:post-utility-smooth} in the main text.
	Note that $u_i^{\text{SM}(\lambda)}$ is integrable as composition of integrable functions.
	We first have a.e.\ pointwise convergence of $u_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i}))$ to $u_i(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i}))$ as $\lambda$ approaches zero. That is, for all $v_{\text{-}i}$, except for $b_i = \beta_{\text{-}i}(v_{\text{-}i})$, the smaller $\lambda$ gets the closer the allocations and the closer the utilities get.

	Second, it is easy to see that $\lvert u_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i})) \rvert$ is bounded via
	\begin{align*}
		&\lvert u_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i}(v_{\text{-}i})) \rvert \leq \lvert v_i - p^{\text{SM}}(b_i, \beta_{\text{-} i}(v_{\text{-}i})) \rvert
	\end{align*}
	and noting that this is a composition of bounded functions.
	With these two conditions satisfied, we can apply the dominated convergence theorem in its a.e.\ version (see, e.g., \citet{bogachev2007measure}, Theorem~2.8.1) on the terms from Equations \ref{eq:iterim-utility} and \ref{eq:smooth-iterim-utility} which proofs the first statement. 

	Let us now consider the ex ante utilities. From the interim convergence, we know that the expected interim utility $\overline{u}_i^{\text{SM}(\lambda)}(v_i, b_i, \beta_{\text{-}i})$ converges pointwise to $\overline{u}_i(v_i, b_i, \beta_{\text{-}i})$ for all $v_i$ and $b_i$. Again applying the dominated convergence theorem ensures equality of the expected utility in the ex ante state of the game. 
\end{proof}

\begin{remark}
	Technically, a tie-breaking rule should be specified for $x_i$ at the nullset $b_i = \beta_{\text{-}i}(v_{\text{-}i})$, but that is exactly the point which we neglect.
\end{remark}


\section{Proof of Proposition \ref{thm:linear-convergence-utility-error}}\label{sec:proof-linear-conv}
The following section provides a linear bound on the error in interim and ex ante utility. For clarity, we restate all major assumptions.

\begin{assumption}
	% \label{ass:main-assumptions}
	Consider a Bayesian auction game $G$ and assume:
	\begin{enumerate}
		\item The action $\mathcal A_i$ and valuation spaces $\mathcal V_i$ are compact intervals.
		% \label{ass:regularities-interim-bound-val-action-space}
		\item $F$ is an atomless prior.
		% \label{ass:regularities-interim-bound-atomless_prior}
		\item The bidding and pricing functions are measurable.
	\end{enumerate}
\end{assumption}

\begin{assumption}
	% \label{ass:regularities-for-interim-bound}
	For all $i \in \mathcal{I}$ assume:
	\begin{enumerate}
		\item $\beta_i$ is strictly increasing and Lipschitz continuous. 
		% \label{ass:regularities-interim-bound-strategies}
		\item $\beta_i^{-1}$ is Lipschitz continuous.
		% \label{ass:regularities-interim-bound-inverse}
		\item There exists a uniform bound for all marginal conditional prior density functions $f_{i|\argdot}$.
		% \label{ass:regularities-interim-bound-prior}
		\item $p_i$ is bounded.
		% \label{ass:regularities-interim-bound-pricing}
	\end{enumerate}
\end{assumption}

\begin{proof}
	
We use the H\"older inequality throughout the proof, which we denote by (H). Whenever we use a specific assumption, we denote it by the corresponding number. 
We begin with the interim utility error, i.e., we aim to bound the following term for all $i \in \mathcal{I}, v_i \in \mathcal{V}_i$, and $\lambda >0$:
\begin{align}
	\overline{\varepsilon}_i(v_i, b_i, \beta_{\text{-} i}, \lambda) 
	:= \left| \int_{\mathcal{V}_{\text{-}i}} u_i\left(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i}) \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i}) \right) \mathrm{d} F_{\text{-}i|i}(v_{\text{-}i}) \right| .
\end{align}
So, let $i \in \mathcal{I}$, $v_i \in \mathcal{V}_i$, $b_i \in \mathcal{A}_i$, and $\lambda >0$ be arbitrary. 
By splitting up the integral into the individual opponents, we get
\begin{align}
	\overline{\varepsilon}_i(v_i, b_i, \beta_{\text{-} i}, \lambda)
	& = \Bigg\vert \int_{\mathcal{V}_{1}} \cdots \int_{\mathcal{V}_{i-1}} \int_{\mathcal{V}_{i+1}} \cdots \int_{\mathcal{V}_{n}} u_i\left(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i}) \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i}) \right) \nonumber\\
	& \qquad \mathrm{d} F_{n|\text{-}n}(v_{n}) \dots \mathrm{d} F_{i+1|(1, \dots, i)}(v_{i+1}) \mathrm{d} F_{i-1|(1, \dots, i-2, i)}(v_{i-1}) \dots \mathrm{d} F_{1|i}(v_{1}) \Bigg\vert\\
	\overset{(\text{H}), (\text{A}1.\ref{ass:regularities-interim-bound-atomless_prior}), (\text{A}2.\ref{ass:regularities-interim-bound-prior})}&{\leq}
	\prod_{j \neq i} \norm{f_j}_{\infty} \int_{\mathcal{V}_{\text{-}i}} \left| u_i\left(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i}) \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, \beta_{\text{-} i}(v_{\text{-}i}) \right)  \right| \mathrm{d} v_{\text{-}i} =: (*_1).
\end{align}
We use $\norm{f_j}_{\infty}$ to denote the uniform bound for any marginal conditional prior density function $f_j|\cdot$ of bidder $j$. Next, we perform a change of variables \citep{katzChangeVariablesMultiple1982} through the inverse of the opponents' strategies
\begin{align}
	(*_1) \overset{(\text{A}2.\ref{ass:regularities-interim-bound-strategies}), (\text{A}2.\ref{ass:regularities-interim-bound-inverse})}&{=}
	 \prod_{j \neq i} \norm{f_j}_{\infty} \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\right)} \left| \left( u_i\left(v_i, b_i, b_{\text{-}i} \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, b_{\text{-}i} \right) \right) \right| \cdot \left| \det \text{D}\beta_{\text{-} i}^{-1}(b_{\text{-}i}) \right| \mathrm{d} b_{\text{-}i} \\
	\overset{(\text{H}), (\text{A}1.\ref{ass:regularities-interim-bound-val-action-space})}&{\leq} \prod_{j \neq i} \norm{f_j}_{\infty} \cdot \norm{\left(\beta^{-1}_j\right)^{\prime}}_{\infty} \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\right)} \left| u_i\left(v_i, b_i, b_{\text{-}i} \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, b_{\text{-}i} \right) \right| \mathrm{d} b_{\text{-}i}.
\end{align}
Note that $\det \text{D}\beta_{\text{-} i}^{-1}(b_{\text{-}i})$ is a diagonal matrix, as $\beta_j$ only depends on $v_j$ for every $j\in \mathcal{I}$, so that the determinate is given by the product of the individual inverse functions' derivatives. We continue with bounding the remaining integral. For this, define the set
\begin{align}
	A_{b_i}= \left\{v_{\text{-}i} \in \mathcal{V}_{\text{-}i} \,\Big|\, \max_{\text{-}i} \beta_{\text{-} i} (v_{\text{-}i}) \geq b_i  \right\},
\end{align}
which includes all valuations of bidder $i$'s opponents such that the item is \emph{not} allocated to bidder $i$ in the original game. 
The integral can then be split up in the following way
\begin{align}
	& \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\right)} \left| u_i\left(v_i, b_i, b_{\text{-}i} \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, b_{\text{-}i} \right) \right| \mathrm{d} b_{\text{-}i}  \\
	&=  \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } \left|  u_i\left(v_i, b_i, b_{\text{-}i} \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, b_{\text{-}i} \right)  \right| \mathrm{d} b_{\text{-}i} && \left(=: (\text{\#}_1) \right)\\
	& + \int_{\beta_{\text{-} i}\left(A_{b_i}\right)} \left|  u_i\left(v_i, b_i, b_{\text{-}i} \right) - u_i^{\text{SM}(\lambda)}\left(v_i, b_i, b_{\text{-}i} \right)  \right| \mathrm{d} b_{\text{-}i} && \left(=: (\text{\#}_2)\right).
\end{align}
It remains to bound the integrals $(\text{\#}_1)$ and $(\text{\#}_2)$. We proceed with $(\text{\#}_2)$, i.e., the integral over the set, where the item is not allocated to bidder $i$. We get
\begin{align}
	(\text{\#}_2) &= \int_{\beta_{\text{-} i}\left(A_{b_i}\right)} \left| u_i^{\text{SM}(\lambda)}\left(v_i, b_i, b_{\text{-}i} \right)  \right| \mathrm{d} b_{\text{-}i}
	= \int_{\beta_{\text{-} i}\left(A_{b_i}\right)} \left| \left( v_i - p^{\text{SM}}(b_i, b_{\text{-}i}) \right) x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i}) \right| \mathrm{d} b_{\text{-}i} \\
	\overset{(\text{H}), (\text{A}2.\ref{ass:regularities-interim-bound-pricing}), (\text{A}1.\ref{ass:regularities-interim-bound-val-action-space})}&{\leq} \norm{v_i - p^{\text{SM}}(b_i, \argdot)}_{\infty_{| \beta_{\text{-} i}\left(A_{b_i}\right)}} \cdot \int_{\beta_{\text{-} i}\left(A_{b_i}\right)} \left| x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i}) \right| \mathrm{d} b_{\text{-}i}.
\end{align}
The additional subscript of the supremum norm indicates that the domain is limited to $\beta_{\text{-} i}(A_{b_i})$.
This step reduced the problem for $(\text{\#}_2)$ to finding a bound for the integral over the soft-allocation function. Note that the softmax function is strictly positive and strictly decreasing in all components of $b_{\text{-}i}$. If $A_{b_i} = \emptyset$, the integral is zero and any positive number is an upper bound. Otherwise, there exists a $j \neq i$ such that $\beta_j(v_j) \geq b_i$ for all $(v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_j, \dots v_n) \in A_{b_i}$. Therefore, we can bound the integral by
\begin{align}
	\int_{\beta_{\text{-} i}\left(A_{b_i}\right)} \left| x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i}) \right| \mathrm{d} b_{\text{-}i} 
	&= \int_{\beta_{\text{-} i}\left(A_{b_i}\right)} \frac{1}{1 + \sum_{j \neq i} \exp\left( \frac{b_j - b_i}{\lambda} \right)} \mathrm{d} b_{\text{-}i} \\
	&\leq \int_{\{b_j \geq b_i\}} \frac{1}{1 + \exp\left(\frac{b_j - b_i}{\lambda} \right)} \mathrm{d} b_j \\
	&\leq \lim_{M \rightarrow  \infty} \int_{b_j = b_i}^{M} \frac{1}{1 + \exp\left(\frac{b_j - b_i}{\lambda} \right)} \mathrm{d} b_j \\
	& = \lim_{M \rightarrow  \infty} \left[ b_j - \lambda \ln \left(\exp\left(\frac{b_j}{\lambda} \right) + \exp\left(\frac{b_i}{\lambda} \right)\right) \right]_{b_j = b_i}^{M} \\
	& = \lim_{M \rightarrow  \infty} M - \lambda \ln\left(\exp\left(\frac{M}{\lambda} \right) + \exp\left(\frac{b_i}{\lambda} \right) \right) - b_i + \lambda \ln \left( 2 \exp\left(\frac{b_i}{\lambda} \right)\right) \\
	&\leq \lim_{M \rightarrow  \infty} M - \lambda \ln\left(\exp\left(\frac{M}{\lambda} \right) \right) - b_i + \lambda \ln \left( 2\right) + \lambda \ln \left( \exp\left(\frac{b_i}{\lambda} \right)\right)  \\
	& = \lambda \ln(2),
\end{align}
which finishes the bound for part $ (\text{\#}_2)$.

We perform similar steps for the integral $ (\text{\#}_1)$, that is taken over the opponents' valuations where bidder $i$ gets the item. Using the definition of the smooth pricing function $p^{\text{SM}}$ over this set, we get
\begin{align}
	(\text{\#}_1) &= \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } \left| v_i - p_i(b_i, b_{\text{-}i}) - \left( v_i - p^{\text{SM}}(b_i, b_{\text{-}i}) \right) x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i}) \right| \mathrm{d} b_{\text{-}i} \\
	&= \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } \left| \left( v_i - p_i(b_i, b_{\text{-}i}) \right) \left( 1 - x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i}) \right) \right| \mathrm{d} b_{\text{-}i} \\
	\overset{(\text{H}), (\text{A}2.\ref{ass:regularities-interim-bound-pricing})}&{\leq} \norm{v_i - p_i(b_i, \argdot)}_{\infty_{| \beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i} \setminus A_{b_i}\right)}} \cdot \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } \left| 1 - x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i}) \right| \mathrm{d} b_{\text{-}i}.
\end{align}

It remains to bound the integral of $h(b_{\text{-}i}) := 1 - x_i^{\text{SM}(\lambda)}(b_i, b_{\text{-}i})$ over the set where bidder $i$ wins the item. Note that $h$ is strictly positive and strictly increasing in all variables $b_{\text{-}i}$. If the set $\mathcal{V}_{\text{-}i} \setminus A_{b_i}$ is empty, we are done. Otherwise, we can bound the integral in the following way:
\begin{align}
	\int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } h(b_{\text{-}i}) \, \mathrm{d} b_{\text{-}i}
	&= \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } 1 - \frac{1}{1 + \sum_{j \neq i} \exp \left(\frac{b_j - b_i}{\lambda} \right)} \, \mathrm{d} b_{\text{-}i} \\
	& = \int_{\beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i}\setminus A_{b_i}\right) } \frac{\sum_{j \neq i} \exp \left(\frac{b_j - b_i}{\lambda} \right)}{1 + \sum_{j \neq i} \exp \left(\frac{b_j - b_i}{\lambda} \right)} \, \mathrm{d} b_{\text{-}i} \\
	& \leq \lim_{M \rightarrow - \infty} \int_{M}^{b_i} \cdots \int_{M}^{b_i} \frac{\sum_{j \neq i} \exp \left(\frac{b_j - b_i}{\lambda} \right)}{1 + \sum_{j \neq i} \exp \left(\frac{b_j - b_i}{\lambda} \right)} \mathrm{d} b_1 \dots \mathrm{d} b_{i-1} \mathrm{d} b_{i+1} \dots \mathrm{d} b_n \\ 
	& \leq \lim_{M \rightarrow - \infty} \int_{M}^{b_i} \cdots \int_{M}^{b_i} \sum_{j \neq i} \exp \left(\frac{b_j - b_i}{\lambda} \right) \mathrm{d} b_1 \dots \mathrm{d} b_{i-1} \mathrm{d} b_{i+1} \dots \mathrm{d} b_n \\ 
	& = \lim_{M \rightarrow - \infty} \sum_{j \neq i} \int_{M}^{b_i} \exp \left(\frac{b_j - b_i}{\lambda} \right) \mathrm{d} b_j \\
	&= \lim_{M \rightarrow - \infty} \sum_{j \neq i} \lambda \left[\exp \left(\frac{b_j - b_i}{\lambda} \right) \right]_{b_j = M}^{b_i} \\
	&= \left(n - 1\right) \lambda.
\end{align}
Combining the derived statements, the interim utility error is bounded by
\begin{align}
	\overline{\varepsilon}_i(v_i, b_i, \beta_{\text{-} i}, \lambda) \; \leq \; K(v_i, b_i, \beta_{\text{-} i}) \cdot \lambda,
\end{align}
where
\begin{align}
	K(v_i, b_i, \beta_{\text{-} i}) = 
	&\left(\prod_{j \neq i} \left(\norm{f_j}_{\infty} \cdot \norm{\left(\beta^{-1}_j\right)^{\prime}}_{\infty}\right) \right) \nonumber\\
	&\cdot 
	\left(\ln(2) \, \norm{v_i - p^{\text{SM}}(b_i, \argdot)}_{\infty_{| \beta_{\text{-} i}\left(A_{b_i}\right)}}
	+ \norm{v_i - p(b_i, \argdot)}_{\infty_{| \beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i} \setminus A_{b_i}\right)}} \left(n - 1\right) \right).
\end{align}
Consequently, we can bound the ex ante utility error by performing similar steps as above by
\begin{align}
	\tilde{\varepsilon}_i\left(\beta_i, \beta_{\text{-} i} \right) &:= \left| \int_{\mathcal{V}_{i}} \overline{u}_i\left(v_i, \beta_i(v_i), \beta_{\text{-} i}\right) - \overline{u}_i^{\text{SM}(\lambda)}\left(v_i, \beta_i(v_i), \beta_{\text{-} i} \right) \mathrm{d} F_{i}(v_{i}) \right| \\
	&\leq \lambda \int_{\mathcal{V}_i} K(v_i, \beta_i(v_i), \beta_{\text{-} i}) \, \mathrm{d} F_{i}(v_{i}) \\
	\overset{(\text{H}), (\text{A}2.\ref{ass:regularities-interim-bound-prior}), (\text{A}2.\ref{ass:regularities-interim-bound-strategies}), (\text{A}2.\ref{ass:regularities-interim-bound-inverse})}&{\leq} \tilde{K} \cdot \lambda,
\end{align}
where 
\begin{align}
	\tilde{K} = \mu\left(\beta_i\left(\mathcal{V}_i \right) \right) \left(\prod_{j \in \mathcal{I}} \norm{f_j}_{\infty} \cdot \norm{\left(\beta^{-1}_j\right)^{\prime}}_{\infty} \right)  \left( \ln(2) \, \norm{g_i^{\text{SM}}}_{\infty} + \left(n-1 \right) \norm{g_i}_{\infty} \right),
\end{align}
where $\mu$ denotes the Borel-measure and for the functions
\begin{align}
	g_i^{\text{SM}}(b_i, \beta_i^{-1}(b_i)) &= \norm{\beta_i^{-1}(b_i) - p^{\text{SM}}(b_i, \argdot)}_{\infty_{| \beta_{\text{-} i}\left(A_{b_i}\right)}},\\
	g_i(b_i, \beta_i^{-1}(b_i)) &= \norm{\beta_i^{-1}(b_i) - p_i(b_i, \argdot)}_{\infty_{| \beta_{\text{-} i}\left(\mathcal{V}_{\text{-}i} \setminus A_{b_i}\right)}}.
\end{align}

\end{proof}

\begin{remark}
	The proof uses the infinity norm of the H\"older inequality $\left(\norm{fg}_1 \leq \norm{f}_1 \norm{g}_{\infty} \right)$, so that one needs Condition~\ref{ass:regularities-interim-bound-inverse} of Assumption~\ref{ass:regularities-for-interim-bound}. One can use the $\norm{ \cdot}_2$-norm in the inequality $\left(\norm{fg}_1 \leq \norm{f}_2 \norm{g}_2 \right)$, so that one can weaken the assumption to $\norm{\left(\beta_{i}^{-1}\right)^{\prime}}_2 < \infty$. However, this leads to a worse convergence rate of the form $\mathcal{O}(\sqrt{\lambda})$.
\end{remark}


\begin{remark}
	The result also holds for a special case of risk-averse bidders. Assume the ex post utility functions additionally depend on a risk parameter $\rho \in (0, 1]$:
	\begin{align}
		u_i(v_i, b, \rho) \; &= \; \left(v_i \, x_i(b) - p_i(b)\right)^{\rho} \\
		u_i^{\text{SM}(\lambda)}(v_i, b, \rho) &= \left(\left(v_i - p^{\text{SM}}(b)\right) x_i^{\text{SM}(\lambda)}(b)\right)^{\rho}.
	\end{align}
	By using Jensen's inequality for the concave mapping $x\mapsto x^{\rho}$, one can derive a convergence rate of $\mathcal{O}(\lambda^\rho)$ by performing analogous steps as in the proof above.
\end{remark}

\section{Proof of Theorem~\ref{thm:eps-equ-in-smooth-game-to-original-game}}\label{sec:proof-cor-1}
Let us now prove that an $\varepsilon$-equilibrium in the smoothed game translates to an $\varepsilon + \mathcal{O}(\lambda)$-equilibrium in the original auction.

\begin{proof}
	We know there exists a constant $K$ such that $\abs{\tilde{u}_i^{\text{SM}(\lambda)} (\beta) - \tilde{u}_i(\beta)} \leq K \lambda$ due to Proposition~\ref{thm:linear-convergence-utility-error}. Therefore, the following holds for an $\varepsilon$-BNE in the smoothed game $\beta^*$ and any strategy $\beta_i$:
	\begin{align}
		\tilde{u}_i(\beta_i, \beta_{-i}^*) - \tilde{u}_i (\beta_i^*, \beta_{-i}^*) & = \tilde{u}_i(\beta_i, \beta_{-i}^*) - \tilde{u}_i^{\text{SM}(\lambda)} (\beta_i, \beta_{-i}^*) + \tilde{u}_i^{\text{SM}(\lambda)} (\beta_i, \beta_{-i}^*) \nonumber\\
		& - \tilde{u}_i^{\text{SM}(\lambda)} (\beta_i^*, \beta_{-i}^*) + \tilde{u}_i^{\text{SM}(\lambda)} (\beta_i^*, \beta_{-i}^*) - \tilde{u}_i (\beta_i^*, \beta_{-i}^*) \\
		& \leq K \lambda + \varepsilon + K \lambda = \varepsilon + 2K\lambda.
	\end{align}
	This is equivalent to
	\begin{align}
		\tilde \ell_i(\beta_i^*, \beta_{-i}^*) \leq \varepsilon + 2K \lambda.
	\end{align}
\end{proof}

\section{A Special Case: Exact Errors}
\label{sec:exact-interim-error}

Consider the single-item FPSB auction with two bidders having uniform priors on the unit interval. Suppose that bidder~$2$ has a linear strategy $\beta_2(v_2) = s \, v_2$ with $s \in (0, 1]$. Then we can derive an error rate for bidder~$1$'s absolute interim utility difference $\overline{\varepsilon}_\lambda = |\overline{u}_1 - \overline{u}_1^\text{SM}|$ and for valuation $v_1$ and bid $b_1 \leq s$.

For the interim utility of bidder~$1$ in the original game, we have
\begin{align}
	\overline{u}_1(v_1, b_1) = \int_{v_2=0}^\frac{b_1}{s} (v_1-b_1) \, \mathrm{d}v_2 = \frac{v_1 - b_1}{s} \, b_1.
\end{align} 
In the smoothed auction, we have
\begin{align}
	\overline{u}_1^\text{SM}(v_1, b_1) = \int_{v_2=0}^1 (v_1 - \max\{b_1, s \, v_2\}) x_1^\text{SM} (b_1, s \, v_2) \, \mathrm{d}v_2.
\end{align} 
When splitting the domain of the integral at $b_1 = s \, v_2$, the first integral evaluates to
\begin{align*}
	-\dfrac{b_1 - v_1}{s} \left({\lambda}\ln\left(\mathrm{e}^\frac{b}{{\lambda}}+1\right)-{\lambda}\ln\left(2\mathrm{e}^\frac{b_1}{{\lambda}}\right)+b_1\right)
\end{align*}
and the second to
\begin{align*}
	&\dfrac{1}{2s} \Big[ 2s{\lambda}v_2\ln\left(\mathrm{e}^{\frac{sv_2}{{\lambda}}-\frac{b_1}{{\lambda}}}+1\right)+2{\lambda}^2\operatorname{Li}_2\left(-\mathrm{e}^{\frac{sv_2}{{\lambda}}-\frac{b_1}{{\lambda}}}\right)-2v_1{\lambda}\ln\left(\mathrm{e}^\frac{sv_2}{{\lambda}}+\mathrm{e}^\frac{b_1}{{\lambda}}\right)-s^2v_2^2+2v_1sv_2 \Big]_{v_2 = \frac{b_1}{s}}^1\\
	&\quad= \dfrac{1}{s} \Big(\lambda (s\ln(\mathrm{e}^{s/\lambda - b_1/\lambda} + 1) - b_1\ln(2)) + \lambda^2 (\operatorname{Li}_2(-\mathrm{e}^{(s-b_1)/\lambda}) + \frac{1}{12}\pi^2)\\
	&\quad + v_1\lambda (\ln(2) + b_1/\lambda - \ln(\mathrm{e}^{s/\lambda} + \mathrm{e}^{b_1/\lambda})) + \frac{1}{2}(b_1^2 - s^2) + v_1(s-b_1) \Big).
\end{align*}
Combining these results, we arrive at an exact interim error of
\begin{align}
	\label{eq:interim-error}
	\overline{\varepsilon}_\lambda(v_1, b_1) &= \frac{\lambda}{s} \Bigg( - s \ln\left(\mathrm{e}^{\frac{s-b_1}{\lambda}} + 1\right) - \lambda \left(\operatorname{Li}_2\left(-\mathrm{e}^{\frac{s-b_1}{\lambda}}\right) + \frac{1}{12}\pi^2\right)\nonumber\\
	&\quad + v_1 \ln(\mathrm{e}^{s/\lambda} + \mathrm{e}^{b_1/\lambda}) + \frac{1}{\lambda} \left(\frac{s^2 - b_1^2}{2} - v_1s\right)\nonumber\\
	&\quad + (b_1-v_1) \left(\ln(\mathrm{e}^{-b_1/\lambda} + 1)\right)\Bigg).
\end{align}
Here, $\operatorname{Li}_2$ is the dilogarithm.
This result shows vastly different convergence rates across the valuation and action space.
Let us assume both bidders are playing according to their BNE strategy, $\beta_i(v_i) = 0.5 v_i$. Now, for the extreme case of $v_1 = 1$, $\overline{\varepsilon}_\lambda$ tends towards a linear function. At the other end of the spectrum, for $v_1 = 0.5$, $\overline{\varepsilon}_\lambda$ tends towards being constantly zero. In summary, the higher the valuation is the slower the convergence rate, with a linear rate in the worst case.
Figure~\ref{fig:smooth_error_bound} in the main paper shows the convergence rates for smaller temperatures. We depict a selection of three valuations. 

Again assuming that both bidders are playing according to their BNE strategy, the ex ante error
\begin{equation}
	\tilde{\varepsilon}_\lambda(\beta_1) \; \vcentcolon= \; \int_{v_1=0}^1 \overline{\varepsilon}_\lambda(v_1, \beta_1(v_1)) \, \mathrm{d}v_1
\end{equation}
can be approximated by taking the sample mean of Equation~\ref{eq:interim-error}. The expression goes towards zero quickly for ever smaller temperatures and is depicted in Figure~\ref{fig:smooth_error_bound} in the main paper.


\section{Impact of Batch Size}\label{sec:bacth-size}
We have run experiments for different batch sizes. The performance increase for ever larger batch sizes diminishes and optimal results are reached for temperature values just below $0.01$ as can be seen in Table~\ref{tab:batch_size}.

\begin{table}[t]
	\centering
	\begin{tabular}{lrr}
		\toprule
		Batch size               & $\min L_2$ & $\lambda$ \\
		\midrule
		$2^{10} = 1{,}024$       & 0.0177     & 0.0239    \\
		$2^{14} = 16{,}384$      & 0.0044     & 0.0119    \\
		$2^{18} = 262{,}144$     & 0.0044     & 0.0089    \\
		$2^{22} = 4{,}194{,}304$ & 0.0042     & 0.0089    \\
		\bottomrule
	\end{tabular}
	\caption{Results for a selection of different batch sizes in the FPSB base setting. We report $\lambda$ for which the $L_2$ loss is minimized. $2^{18}$ is the default value used during training and $2^{22}$ the maximal value possible on the GPU used.}
	\label{tab:batch_size}
\end{table}	


\section{Reproducibility and Hyperparameters\label{sec:reproducibility}}
We have implemented all auctions and algorithms in the PyTorch framework. The code is available at \href{https://github.com/heidekrueger/bnelearn}{Github}.

\subsection{Learning}
We use common hyperparameters across all settings except where noted otherwise. 
The feed-forward neural networks are fully connected with two hidden layers of ten nodes each with SeLU activations, as well as ReLU activations applied to the output layer. 
We model all bidders by a shared policy because the auctions considered are symmetric. Hence, learning is stabilized but limited to finding symmetric BNE. Furthermore, we perform supervised pretraining of 50 iterations towards truthful strategies to prevent degenerate initializations.
All experiments are run on a single Nvidia GeForce 2080Ti GPU with 11\,GB of memory and a batch size of $2^{18}$ for learning.
Each experiment was repeated five times with 2{,}000 iterations.
Furthermore, the following algorithm specific settings were used:
\begin{itemize}
	\item  For NPGA, we choose a population size of 64 and a variance of 1 for the normal distribution from which we draw population samples in parameter space. The variance is then scaled by the model size as is done in \citep{bichler2021learning}.
	\item In the case of REINFORCE, the output dimension is increased by a factor of two because for each bid, a normal distribution (with its two parameters) is learned instead.
	\item For the smoothed game, we choose a temperature of 0{.}01.
\end{itemize}

\subsection{Evaluating}
A batch size of $2^{22}$ was used for the calculation of the $L_2$ loss. The choice of batch sizes was mainly driven by maxing out the GPU memory. Learning requires more memory than evaluating $L_2$, so the latter was possible to conduct with larger batch sizes.
For the utility loss $\varepsilon$, we decreased the number of prior samples from the player currently under evaluation to $n_\text{own-batch} = 2^{10}$. For each of these valuations, his or her best response --- with possible actions from an equidistant grid of size $n_\text{gird} = 2^{10}$ --- is approximated over a sample of $n_\text{opponent-batch} = 2^{20}$ opponent valuations. A higher batch size for the opponents is necessary for reaching the required precision in estimating the utilities. In total, the calculation of $\varepsilon$ requires $n_\text{own-batch} \cdot n_\text{gird} \cdot n_\text{opponent-batch} = 2^{40} > 1\,$trillion game evaluations.


% \bibliography{bibliography}

% \end{document}




