\documentclass[article,nojss]{jss}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{multirow}
\DeclareMathOperator*{\argmin}{argmin}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
%\author{Samuel Watson\\University of Birmingham \And 
%        Yi Pan\\University of Birmingham}
\author{Samuel Watson\\University of Birmingham}
\title{Generalised Linear Mixed Model Specification, Analysis, Fitting, and Optimal Design in \proglang{R} with the \pkg{glmmr} Packages}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Samuel Watson} %% comma-separated
\Plaintitle{Tools for Generalised Linear Mixed Model Analysis and Fitting in R with the glmmr Packages} %% without formatting
\Shorttitle{glmmr packages for R} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
   We describe the \proglang{R} package \pkg{glmmrBase} and an extension \pkg{glmmrOptim}. \pkg{glmmrBase} provides a flexible approach to specifying and analysing generalised linear mixed models. We use an object-orientated class system within \proglang{R} to provide methods for a wide range of covariance and mean functions relevant to multiple applications including cluster randomised trials, cohort studies, spatial and spatio-temporal modelling, and split-plot designs. The class generates relevant matrices and statistics and a wide range of methods including full likelihood estimation of generalised linear mixed models using Markov Chain Monte Carlo Maximum Likelihood, Laplace approximation, power calculation, and access to relevant calculations. The class also includes Hamiltonian Monte Carlo simulation of random effects, sparse matrix methods, and other functionality to support efficient estimation. The \pkg{glmmrOptim} package implements a set of algorithms to identify c-optimal experimental designs where observations are correlated and can be specified using a generalised linear mixed model. Several examples and comparisons to existing packages are provided to illustrate use of the packages.
}
\Keywords{Markov Chain Monte Carlo Maximum Likelihood, Generalised Linear Mixed Model, \proglang{R}, \proglang{C++}, c-Optimal Experimental design}
\Plainkeywords{Markov Chain Monte Carlo Maximum Likelihood, Generalised Linear Mixed Model, R, C++, c-Optimal Experimental design} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Samuel Watson\\
  Institute for Applied Health Research\\
  University of Birmingham \\
  Birmingham, UK\\
  E-mail: \email{S.I.Watson@bham.ac.uk}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Introduction}

Generalised linear mixed models (GLMM) are a highly flexible class of statistical models that incorporate both `fixed' and `random' effects. GLMMs permit the incorporation of latent effects and parameters and allow for complex covariance structures. For example, they are widely used in the analysis of: clustered data, such as from cluster randomised trials, to capture latent cluster means; cohort studies to incorporate temporal correlation between observations on individuals; or, in geospatial statistical models as the realisation of a Gaussian process used to model a latent spatial or spatio-temporal surface. Their use in such a wide variety of statistical applications means there exist a several packages and libraries for software like \proglang{R} to provide relevant calculations for study design, model fitting, and other analyses. For many types of analysis, such as a power calculation for a cluster trial, there exist multiple different packages each implementing a set of specific models. Users may therefore be required to use multiple packages with different interfaces for a single analysis. A more general and flexible system that permits users to add and extend functionality may therefore simplify statistical workflows and facilitate more complex analyses.

In this article, we discuss the \proglang{R} package \pkg{glmmrBase} and its extensions \pkg{glmmrMCML} and \pkg{glmmrOptim} for the \proglang{R} programming language. These packages provide a general framework for GLMM specification with calculation of relevant matrices, statistics, and other functions designed to provide useful analyses for a large range of model specifications and to support implementation of other GLMM related software in \proglang{R}. We describe the base classes implemented through the \pkg{glmmrBase} package, and then extensions for full likelihood Markov Chain Monte Carlo Maximum Likelihood (MCML) model fitting with the \pkg{glmmrMCML} package, and optimal experimental design algorithms for studies with correlated observations in the \pkg{glmmrOptim} package. We summarise and compare existing software that provides functionality in these areas where relevant in each section. 

\subsection{Generalised linear mixed models}
\label{subsec: glmm_framework}
A generalised linear mixed model (GLMM) has the linear predictor for observation $i$
\begin{equation*}
    \eta_i = \mathbf{x}_i\boldsymbol{\beta} + \mathbf{z}_i \mathbf{u}
\end{equation*}
where $\mathbf{x}_i$ is the $i$th row of matrix $X$, which is a $n \times P$ matrix of covariates, $\beta$ is a vector of parameters, $\mathbf{z}_i$ is the $i$th row of matrix $Z$, which is the $n \times Q$ ``design matrix'' for the random effects, and $\mathbf{u} \sim N(0,D)$, where $D$ is the $Q \times Q$ covariance matrix of the random effects terms that depends on parameters $\boldsymbol{\theta}$. In this article we use bold lower case characters, e.g. $\mathbf{x}_i$ to represent vectors, normal script lower case, e.g. $\beta_1$, to represent scalars, and upper case letters, e.g. $X$, to represent matrices.

The model is then
\begin{equation*}
y_i \sim G(h(\eta_i);\phi)
\end{equation*}
where $\mathbf{y}$ is a $n$-length vector of outcomes with elements $y_i$, $G$ is a distribution, $h(.)$ is the link function such that $\mu_i = h(\eta_i)$ where $\mu_i$ is the mean value, and $\phi$ is an additional scale parameter to complete the specification. 

When $G$ is a distribution in the exponential family, we have:
\begin{align}
    f_{y|\mathbf{u}}(y_i|\mathbf{u}, \beta, \phi) &= \exp{(y_i\eta_i - c(\eta_i))/a(\phi) + d(y,\phi)} \\
    \mathbf{u} & \sim f_{\mathbf{u}}(\mathbf{u}|\theta)
\end{align}
The likelihood of this model is given by:
\begin{equation}
\label{eq:lik1}
    L(\beta,\phi,\theta|\mathbf{y}) = \int \prod_{i=1}^n f_{y|\mathbf{u}}(y_i|\mathbf{u}, \beta, \phi)f_{\mathbf{u}}(\mathbf{u}|\theta) d \mathbf{u}
\end{equation}
The likelihood (\ref{eq:lik1}) generally has no closed form solution, and so different algorithms and approximations have been proposed to estimate the model parameters. We discuss model fitting in Section \ref{sec:mcml}. Where relevant we represent the set of all parameters as $\boldsymbol{\Theta} = (\boldsymbol{\beta}, \phi, \boldsymbol{\theta})$.


\section{A GLMM Model in glmmrBase}
The \pkg{glmmrBase} package defines a \code{Model} class using \pkg{R6} classes \citep{r6}, which provide an encapsulated object orientated programming system. Most of the functionality of the package revolves around the \code{Model} class, which also interfaces with underlying \proglang{C++} classes. A non-exhaustive list of the functions and objects calculated by or a member of the \code{Model} class is shown in Table \ref{tab:model}. The class also contains two subclasses, the \code{covariance} and \code{mean} classes, which handle the random effects and linear predictor, respectively. \pkg{R6} classes allow encapsulating classes to `share' class objects, so that a single \code{covariance} object could be shared by multiple \code{Model} objects. Similarly, \pkg{R6} provides familiar object-orientated functionality, including class inheritance, so that new classes can be created that inherit from the \code{Model} class to make use of the range of functions. Within \proglang{R}, we use the \pkg{Matrix} package for matrix storage and linear algebra for operations, and the Eigen \proglang{C++} library for \proglang{C++} functions. Linkage between \proglang{R} and \proglang{C++} is provided through the \proglang{RcppEigen} package \citep{Eddelbuettel2011}. We describe the base functionality and model specification in this section, and then describe higher-level functionality including model fitting in subsequent sections.

\begin{table}[]
    \centering
    \begin{tabular}{p{0.15\linewidth}p{0.25\linewidth}|p{0.4\linewidth}|p{0.05\linewidth}}
    \toprule
    \multicolumn{2}{l|}{\textbf{Method}} & \textbf{ Description} & \textbf{Sec.} \\
    \midrule
       \multirow{9}{*}{\code{covariance}}  & \code{D} & The matrix $D$ & \ref{subsec:covcalc}\\
       & \code{Z} & The matrix $Z$ & \ref{sec:matZ}\\
         & \code{chol\_D()} & Generate the cholesky decomposition of $D$ & \ref{subsec:covcalc}\\
         & \code{log\_likelihood()} & For a given vector $\mathbf{u}$ calculate the multivariate Gaussian log-likelihood with zero mean and covariance $D$ & \ref{subsec:covparest}\\
         & \code{simulate\_re()} & Simulates a vector $\mathbf{u}$ & \ref{sec:datasim}\\
         & \code{sparse()} & Choose whether to use sparse matrix methods & \ref{subsec:covcalc} \\
         & \code{parameters} & The parameters $\theta$\\
         & \code{formula} & The random effects formula used to create $D$ & \ref{subsec:cov_fun} \\
         & \code{update\_parameters()} & Updates $\theta$ and related matrices \\
         \midrule
         \multirow{6}{*}{\code{mean}} & \code{X} & The matrix X &  \\
         & \code{parameters} & The parameters $\beta$ \\
         & \code{offset} & The optional model offset \\
         & \code{formula} & The fixed effects formula used to create $X$ & \ref{subsec:mean_fun} \\
         & \code{linear\_predictor()} & Generates $X\beta$ plus offset \\
        & \code{update\_parameters()} & Updates $\beta$ and related matrices \\
        \midrule
        \multicolumn{2}{l|}{\code{family}} &  A \proglang{R} family object \\
        \multicolumn{2}{l|}{\code{var\_par}} & An optional scale parameter\\
        \multicolumn{2}{l|}{\code{fitted()}} & Generates the full linear predictor \\
        \multicolumn{2}{l|}{\code{predict()}} & Generates predictions from the model at new data values & \ref{sec:predict} \\
        \multicolumn{2}{l|}{\code{sim\_data()}} & Simulates data from the model & \ref{sec:datasim} \\
        \multicolumn{2}{l|}{\code{Sigma()}} & Generates $\Sigma$ (or an approximation) & \ref{sec:approxsigma} \\
        \multicolumn{2}{l|}{\code{information\_matrix()}} & Generates $X^T\Sigma^{-1}X$ & \ref{sec:approxsigma} \\
        \multicolumn{2}{l|}{\code{use\_attenutation()}} & Option for improving approximation of $\Sigma$ & \ref{sec:approxsigma} \\
        \multicolumn{2}{l|}{\code{power()}} & Estimates the power & \ref{subsec:power} \\
        \multicolumn{2}{l|}{\code{MCML()}} & Markov Chain Monte Carlo Maximum Likelihood model fitting & \ref{sec:mcml}  \\
        \multicolumn{2}{l|}{\code{LA()}} & Maximum Likelihood model fitting with Laplace approximation & \ref{sec:LA}  \\
        \multicolumn{2}{l|}{\code{mcmc\_sample()}} & Sample $\mathbf{u}$ using MCMC & \ref{sec:mcmc}  \\
        \multicolumn{2}{l|}{\code{w\_matrix()}} & Returns $diag(W)$ & \ref{sec:approxsigma}  \\
        \multicolumn{2}{l|}{\code{dh\_deta()}} & Returns $\partial h^{-1}(\eta)/\partial \eta$ & \ref{sec:approxsigma}  \\
        \multicolumn{2}{l|}{\code{log\_gradient()}} & Returns either $\nabla_{\mathbf{u}}L(\beta,\phi,\theta\vert \mathbf{y})$ or $\nabla_{\beta}L(\beta,\phi,\theta\vert \mathbf{y})$ & \ref{sec:mcmc}  \\
         \bottomrule
    \end{tabular}
    \caption{A non-exhaustive list of the publicly available methods and objects in the \code{Model} class. \code{Model} also contains two sub-classes \code{covariance} and \code{mean} whose respective methods are also shown. The column \textbf{Sec.} shows the relevant section of the article detailing the methods.}
    \label{tab:model}
\end{table}

An example call to generate a new \code{Model} is:
\begin{CodeChunk}
\begin{CodeInput}
model <- Model$new(formula = ~ factor(t) - 1 + (1|gr(j)*ar1(t)),
                   data = data,
                   covariance = list(parameters = c(0.25,0.8),
                   mean = list(parameters = rep(0,5)),
                   family = gaussian())
\end{CodeInput}
\end{CodeChunk}
We discuss each of the elements of this call in turn. The initialisation function above is designed to be flexible. The user can optionally specify a formula to the function, or separate formulae for the fixed and random effects in the list to those arguments. We discuss specification of random and fixed effects formulae separately, but they can be combined as in the above example.

\subsection{Data Generation Tools}
\label{subsec:other_fun}
We introduce methods to generate data for hierarchical models and blocked designs. As we show in subsequent examples, `fake' data generation is typically needed to specify a particular data structure at the design stage of a study, but can be useful in other circumstances. \citet{Nelder1965} suggested a simple notation that could express a large variety of different blocked designs. The notation was proposed in the context of split-plot experiments for agricultural research, where researchers often split areas of land into blocks, sub-blocks, and other smaller divisions, and apply different combinations of treatments. However, the notation is useful for expressing a large variety of experimental designs with correlation and clustering, including cluster trials, cohort studies, and spatial and temporal prevalence surveys. We have included the function \code{nelder()} in the package that generates a data frame of a design using the notation. 

There are two operations:
\begin{enumerate}
\item \code{>} (or $\to$ in Nelder's notation) indicates ``clustered in''.
\item \code{*} (or $\times$ in Nelder's notation) indicates a crossing that generates all combinations of two factors.
\end{enumerate}

The function takes a formula input indicating the name of the variable and a number for the number of levels, such as \code{abc(12)}. So for example \code{~cl(4) > ind(5)} means in each of five levels of \code{cl} there are five levels of \code{ind}, and the individuals are different between clusters. The formula \code{~cl(4) * t(3)} indicates that each of the four levels of \code{cl} are observed for each of the three levels of \code{t}. Brackets are used to indicate the order of evaluation. Some specific examples are illustrated in Table \ref{table:notation_example}.

\begin{table}
\centering
\begin{tabular}[t]{p{5cm}|p{8cm}}
\hline
\textbf{Formula} & \textbf{Meaning} \\
\hline
\code{~person(5) * time(10)} & A cohort study with five people, all observed in each of ten periods \code{time} \\
\hline
\code{~(cl(4) * t(3)) > ind(5)} & A repeated-measures cluster study with four clusters (labelled \code{cl}), each observed in each time period \code{t} with cross-sectional sampling and five individuals (labelled \code{ind}) in each cluster-period.\\
\hline
\code{~(cl(4) > ind(5)) * t(3)} & A repeated-measures cluster cohort study with four clusters (labelled \code{cl}) with five individuals per cluster, and each cluster-individual combination is observed in each time period \code{t}.\\
\hline
\code{~((x(100) * y(100)) > hh(4)) * t(2)}& A spatial-temporal grid of 100x100 and two time points, with 4  households per spatial grid cell.\\
\hline
\end{tabular}
\caption{\label{table:notation_example}Examples of formulae for the \code{nelder()} function}
\end{table}

Use of this function produces a data frame:
\begin{CodeChunk}
\begin{CodeInput}
data <- nelder(~(j(4) * t(5)) > i(5))
head(data)
\end{CodeInput}
\begin{CodeOutput}
>   j t i
> 1 1 1 1
> 2 1 1 2
> 3 1 1 3
> 4 1 1 4
> 5 1 1 5
> 6 1 2 6
\end{CodeOutput}
\end{CodeChunk}

The data frame shown above may represent, for example, a cluster randomised study with cross-sectional sampling. Such an approach to study design assumes the same number of each factor for each other factor, which is not likely adequate for certain study designs. We may expect unequal cluster sizes, staggered inclusion/drop-out, and so forth, and so a user-generated data set would instead be required. Certain treatment conditions may be specified with this approach including parallel trial designs, stepped-wedge implementation, or factorial approaches by specifying a treatment arm as part of the block structure.

\label{subsec:cov_fun}
\subsection{Covariance specification}
We adapt and extend the random effects specification approach of the \proglang{R} package \pkg{lme4} \citep{lme4} and other packages, to allow for relatively complex structures through the specification of a covariance function. A covariance function is specified as an additive formula made up of components with structure \code{(z|f(j))}. The left side of the vertical bar specifies the covariates in the model that have a random effects structure; using \code{(1|f(j))} specifies a `random intercept' model. The right side of the vertical bar specifies the covariance function \code{f} for that term using variable \code{j} named in the data. Multiple covariance functions on the right side of the vertical bar are multiplied together, i.e., \code{(1|f(j)*g(t))}. The currently implemented functions (as of version 0.3.1) are listed in Table~\ref{table:cov_fun}. We discuss use of compactly supported covariance functions in Section \ref{subsec:compact}.

\begin{sidewaystable}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
\textbf{Function} & $Cov(x_i,x_{i'})$ & $\theta$ & \textbf{Code}\\
\hline
Identity/ Group membership & $\theta_1^2 \mathbf{1}(x_i = x_{i'})$ & $\theta_1 > 0$ & \code{gr(x)}\\
Exponential & $\theta_1 \text{exp}(- \vert x_i - x_{i'}\vert / \theta_2 )$ &$\theta_1,\theta_2 > 0$& \code{fexp(x)}\\
& $\text{exp}(- \vert x_i - x_{i'}\vert /\theta_1)$&$\theta_1 > 0$ & \code{fexp0(x)}\\
Squared Exponential & $\theta_1 \text{exp}(- (\vert x_i - x_{i'}\vert / \theta_2)^2)$ &$\theta_1,\theta_2 > 0$& \code{sqexp(x)}\\
& $\text{exp}(-( \vert x_i - x_{i'}\vert/\theta_1)^2 )$ &$\theta_1 > 0$& \code{sqexp0(x)}\\
Autoregressive order 1 & $\theta_1^{\vert x_i - x_{i'} \vert}$ &$0 < \theta_1 < 1$& \code{ar1(x)}\\
Bessel & $K_{\theta_1}(x)$ & $\theta_1$ > 0 & \code{bessel(x)}\\
Matern & $\frac{2^{1-\theta_1}}{\Gamma(\theta_1)}\left( \sqrt{2\theta_1}\frac{x}{\theta_2} \right)^{\theta_1} K_{\theta_1}\left(\sqrt{2\theta_1}\frac{x}{\theta_2})\right)$ & $\theta_1,\theta_2 > 0$ & \code{matern(x)} \\
\textit{Compactly supported*} &&& \\
Wendland 0 & $\theta_1(1-y)^{\theta_2}, 0 \leq y \leq 1; 0, y \geq 1$ & $\theta_1>0, \theta_2 \geq (d+1)/2$ & \code{wend0(x)}\\
Wendland 1 & $\theta_1(1+(\theta_2+1)y)(1-y)^{\theta_2+1}, 0 \leq y \leq 1; 0, y \geq 1$ & $\theta_1>0, \theta_2 \geq (d+3)/2$ & \code{wend1(x)}\\
Wendland 2 & $
    \theta_1(1+(\theta_2+2)y + \frac{1}{3}((\theta_2+2)^2 - 1)y^2)(1-y)^{\theta_2+2}, 0 \leq y \leq 1 
$ & $\theta_1>0,\theta_2 \geq (d+5)/2$ & \code{wend1(x)}\\
& $ 0, y \geq 1 $ &&\\
Whittle-Matern $\times$ Wendland** & $\theta_1\frac{2^{1-\theta_2}}{\Gamma(\theta_2)}y^{\theta_2}K_{\theta_2}(y)(1+\frac{11}{2}y + \frac{117}{12}y^2)(1-y), 0 \leq y \leq 1; 0, y \geq 1$ & $\theta_1,\theta_2 > 0$ & \code{prodwm(x)}  \\
Cauchy $\times$ Bohman*** & $\theta_1(1-y^{\theta_2})^{-3}\left( (1-y)\text{cos}(\pi y)+\frac{1}{\pi}\text{sin}(\pi y) \right), 0 \leq y \leq 1; 0, y \geq 1$ & $\theta_1>0, 0 \leq \theta_2 \leq 2$ & \code{prodcb(x)} \\
Exponential $\times$ Kantar**** & $\theta_1\exp{(-y^{\theta_2})}\left( (1-y)\frac{\sin{(2\pi y)}}{2\pi y} + \frac{1}{\pi}\frac{1-\cos{(2\pi y)}}{2\pi y} \right), 0 \leq y \leq 1$ & $\theta_1,\theta_2 > 0$ & \code{prodek(x)}\\
& $0, y \geq 1$ && \\
\hline
\end{tabular}
\caption{\label{table:cov_fun}Supported covariance functions. $\vert . \vert$ is the Euclidean distance. $K_a$ is the modified Bessel function of the second kind. *Variable $y$ is defined as $x/r$ where $r$ is the effective range, see Section \ref{subsec:compact}. For the compactly supported functions $d$ is the number of dimensions in \code{x}. **Permissible in one or two dimensions. ***Only permissible in one dimension. ****Permissible in up to three dimensions.}
\end{sidewaystable}

One combines smaller functions to provide the desired overall covariance function. For example, for a stepped-wedge cluster randomised trial we could consider the standard specification (see, for example, \citet{Li2021}) with an exchangeable random effect for the cluster level (\code{j}), and a separate exchangeable random effect for the cluster-period (with \code{t} representing the discrete time period), which would be \code{~(1|gr(j))+(1|gr(j,t))}. Alternatively, we could consider an autoregressive cluster-level random effect that decays exponentially over time so that, for person $i$ in cluster $j$ at time $t$, $Cov(y_{ijt},y_{i'jt}) = \theta_1^2$, for $i\neq i'$, $Cov(y_{ijt},y_{i'jt'}) = \theta_1^2 \theta_2^{\vert t-t' \vert}$ for $t \neq t'$, and $Cov(y_{ijt},y_{i'j't}) = 0$ for $j \neq j'$. This function would be specified as \code{~(1|gr(j)*ar1(t))}. We could also supplement the autoregressive cluster-level effect with an individual-level random intercept, where \code{ind} specified the identifier of the individual, as \code{~(1|gr(j)*ar1(t)) + (1|gr(ind))}, and so forth. To add a further random parameter on some covariate \code{x} we must add an additional term, for example, \code{~(1|gr(j)*ar1(t)) + (1|gr(ind)) + (x|gr(ind))}.

\subsubsection{Covariance function parameters}
The \code{covariance} argument of the call to \code{Model} takes a list, which can contain covariance function parameters, a random effects specification, and a particular data frame. Where the model is being used for model fitting, specified parameter values are used as starting values, alternatively, where the model is used to calculate specific matrices or functions, it uses these parameter values. 

Parameter values are provided to the covariance function as a vector. The covariance functions described in Table 3 have different parameters $\theta$, and a value is used to generate the matrix $D$ and related objects for analyses. The elements of the vector correspond to each of the functions in the covariance formula in the order they are written. For example,
\begin{itemize}
    \item Formula: \code{~(1|gr(j))+(1|gr(j,t))}; parameters: \code{c(0.25,0.1)} describes the covariance function for $i\neq i'$
\begin{equation*}
Cov(y_{ijt},y_{i'j't'}) = 
\begin{cases}
0.25^2 + 0.10^2 & \text{if } j=j', t=t' \\
0.25^2 & \text{if } j=j', t\neq t' \\
0 & \text{otherwise}
\end{cases}
\end{equation*}

    \item Formula: \code{~(1|gr(j)*fexp0(t))}; parameters: \code{c(0.25,0.8)} describes the covariance function
$$
Cov(y_{ijt},y_{i'j't'}) = 
\begin{cases}
0.25^2* \text{exp}(-\frac{\vert t-t' \vert}{0.8}) & \text{if } j=j' \\
0 & \text{otherwise}
\end{cases}
$$
\end{itemize}

For covariance functions with a single variable argument, the function is specified in terms of the absolute distance between the values of the variable. For multiple variable arguments, the Euclidean distance is used. For example \code{~(1|fexp0(x,y))} will generate a covariance matrix where the covariance between two observations with positions $s=(x,y)$ and $s' = (x',y')$ is $Cov(s,s') = \text{exp}(- \vert \vert s-s' \vert \vert/ \theta)$ where $||.||$ is the Euclidean distance. In these cases, the positions $x$ and $y$ will often be unique such that $Z$ will be an identity matrix and the random effect is equivalent to a Gaussian process with exponential covariance function. For some of the covariance functions, we provide two parameterisations (for example, \code{fexp} and \code{fexp0}). The aim of these functions is to provide a version that is compatible with group membership covariance structures, since a model specified as \code{gr(j)*fexp(t)} would not be identifiable since there would be two free parameters multiplying the exponential function.

\subsection{Computation of matrix Z}
\label{sec:matZ}
The matrix $Z$ is constructed in a similar way as other packages, such as described for \pkg{lme4} \citep{lme4}. $Z$ is comprised of the matrices corresponding to each block $Z = [Z_1, Z_2, ..., Z_B]$. For a model formula \code{(1|f(x1,x2,...))}, the dimension of the corresponding matrix $Z_b$ is $n$ rows and number of columns equal to the number of unique rows of the combination of the variables in \code{x1,x2,...}. The $i$th row and $j$th column of $Z_b$ is then equal to 1 if the $i$th individual has the value corresponding to the $j$th unique combination of the variables and zero otherwise. For formulae specifying random effects on covariates (``variable slopes models''), e.g. \code{(z|f(x1,x2,...))}, then the $i$th row and $j$th column of $Z_b$ is further multiplied by $x_i$.

\subsection{Covariance Calculation and Storage}
\label{subsec:covcalc}
The formula specifying the random effects is translated into a form that can be used to efficiently generate the value at any position in the matrix $D$ so that it can be quickly updated when the parameter values change, which facilitates functionality such as model fitting when using an arbitrary combination of covariance functions. 

A large number of covariance function specifications and their related study designs lead to a block diagonal structure for $D$:
\begin{equation}
\label{eq:blockd}
   D =  \begin{bmatrix}
    D_1 & \mathbf{0} & \hdots \\
    \mathbf{0} &  D_2 & \hdots \\
    \vdots & \ddots & \vdots \\
    \hdots & \mathbf{0} & D_B 
    \end{bmatrix}
\end{equation}
Internally, the formula into parsed into blocks, both determined by the presence of a \code{gr} function in any particular random effect specification, and for each additive component of the specification. The package does not yet support correlated random effects between blocks, although this is planned for future versions. The underlying \proglang{C++} class then stores the unique combinations of the specified variables for each block, along with parameter indexes, and an `instruction set' presented by a vector of integers to generate the values at a given position of the block's sub-matrix. We use a reverse Polish notation for to specify the instructions for a given block; this approach facilitates combining multiple functions in any given order and calculating the Euclidean distance of an arbitrary number of variables. When the value of a particular element of the matrix $D$ is required, the program can then identify the block and execute its instructions, which include, for example, push a datum to the stack, push a parameter to the stack, multiply, and so forth. We then use two different approaches for building $D$ and its Cholesky decomposition $L$ such that $D = LL^T$. 

Where there is a blocked structure to $D$ we can treat the matrix as sparse. Similarly, the matrix $L$ is also sparse as:
\begin{equation}
   L =  \begin{bmatrix}
    L_1 & \mathbf{0} & \hdots \\
    \mathbf{0} &  L_2 & \hdots \\
    \vdots & \ddots & \vdots \\
    \hdots & \mathbf{0} & L_B 
    \end{bmatrix}
\end{equation}
Matrix $Z$ is also almost always sparse. We therefore store $D$, $L$, and $Z$ using compressed row storage (CRS) format. The Cholesky decomposition can also be efficiently calculated using a sparse matrix algorithm. Many of the functions in the package, for example calculating the log-likelihood or gradient (see Section \ref{sec:mcml}), require products of these matrices; $ZL$ is frequently used. Sparse matrix multiplication algorithms are also used to improve efficiency. We provide a basic \proglang{C++} sparse matrix class, the LDL decomposition, and basic operations in the \pkg{SparseChol} package for \proglang{R}. However, it is important to note that a sparse matrix $D$ does not guarantee a sparse Cholesky factor; the ordering of observations in the data in very important in this regard. \citet{Furrer2006} examine different orderings in these cases.

As an alternative we can directly exploit the block diagonal structure of $D$. We can directly generate the decomposition of each block $L_b$ separately. We use the Choleskyâ€“Banachiewicz algorithm for this purpose. Many calculations can also be broken down by block, for example $X^T\Sigma^{-1}X = \sum_b X^T_b L_b^{-T}L_b^{-1}X_b$. This example can also be further simplified using a forward substitution algorithm (see Section \ref{subsec:covparest}). To switch between sparse and dense matrix methods, one can use \code{cov\$sparse(TRUE)} and \code{cov\$sparse(FALSE)}, respectively. The default is to use sparse methods. 

\subsection{The Covariance class}
As described above, the \code{Model} class incorporates a \code{Covariance} sub-class that we can use directly (see Table \ref{tab:model}). This example using data set \code{data} has a cluster-level exchangeable random effect with standard deviation 0.25:
\begin{CodeChunk}
\begin{CodeInput}
cov <- Covariance$new(
    formula = ~ (1|gr(j,t)),
    data= data,
    parameters = 0.05
)
\end{CodeInput}
\end{CodeChunk}
In this example, there are four unique values of variable \code{j} and five values of \code{t} from one to five, such that there are 20 unique levels of \code{j} and \code{t}:
\begin{CodeChunk}
\begin{CodeInput}
> cov$Z
100 x 20 sparse Matrix of class "dgCMatrix"
                                             
 [1,] 1 . . . . . . . . . . . . . . . . . . .
 [2,] 1 . . . . . . . . . . . . . . . . . . .
 [3,] 1 . . . . . . . . . . . . . . . . . . .
 [4,] 1 . . . . . . . . . . . . . . . . . . .
 [5,] 1 . . . . . . . . . . . . . . . . . . .
 [6,] . 1 . . . . . . . . . . . . . . . . . .
 [7,] . 1 . . . . . . . . . . . . . . . . . .
 [8,] . 1 . . . . . . . . . . . . . . . . . .
 [9,] . 1 . . . . . . . . . . . . . . . . . .
[10,] . 1 . . . . . . . . . . . . . . . . . .

> round(cov$D,4)
20 x 20 sparse Matrix of class "dsCMatrix"    
 [1,] 0.0025 0.0018 0.0013 0.0010 0.0007 .      .      .      . 
 [2,] 0.0018 0.0025 0.0018 0.0013 0.0010 .      .      .      .   
 [3,] 0.0013 0.0018 0.0025 0.0018 0.0013 .      .      .      . 
 [4,] 0.0010 0.0013 0.0018 0.0025 0.0018 .      .      .      . 
 [5,] 0.0007 0.0010 0.0013 0.0018 0.0025 .      .      .      . 

\end{CodeInput}
\end{CodeChunk}

The same objects can similarly be accessed from the containing model object, for example, \code{mod\$Covariance\$D}.

\subsection{Linear Predictor} 
\label{subsec:mean_fun}
Specification of the mean function follows standard model formulae in \proglang{R}. As with the covariance, a \code{MeanFunction} object can be created directly by specifying a formula, data, family, and optionally the parameter values, or some or all of these arguments can be passed as a list to a call to \code{Model}.  A complete example specification is:
\begin{CodeChunk}
\begin{CodeInput}
mf <- MeanFunction$new(formula = ~ factor(t)+ int - 1,
                       data = data,
                       parameters = rep(0,6),
                       family = gaussian())
\end{CodeInput}
\end{CodeChunk} 

%One can also include non-linear functions of variables in the mean function. These are handled in the analyses by first-order approximation. For example, if there are non-linear functions in the mean function then the approximation to the variance of $\hat{\beta}$ is $(F^T \Sigma^{-1}F)^{-1}$ where $F$ is a $n \times P$ matrix, where $P$ is the number of parameters. The $p$th column of $F$ is $\partial\boldsymbol{\eta}/\partial \beta_p$. The user can add additional functions by specifying a new function that takes as an input a named list with elements data and pars, and outputs a matrix with the linearised components. The function name must begin with \code{d} and in the formula the \code{d} is removed. For example, one can specify \code{~fexp(t) + ...} for the function $\beta_1 \text{exp}(-\beta_2 t)$, which would call:
%\begin{CodeChunk}
%\begin{CodeInput}
%dfexp <- function(x){
%  m <- as.matrix(x$data) %*% matrix(x$pars[2:length(x$pars)],ncol=1)
%  X <- matrix(exp(m),ncol=1)
%  for(i in 1:ncol(x$data)){
%    X <- cbind(X,x$data[,i]*x$pars[i+1]*exp(m))
%  }
%  return(X)
%}
%\end{CodeInput}
%\end{CodeChunk}
%then to use this function in a formula:
%\begin{CodeChunk}
%\begin{CodeInput}
%mf <- MeanFunction$new(formula = ~ fexp(x),
%                       data = data,
%                       parameters = c(0,1),
%                       family = gaussian())
%\end{CodeInput}
%\end{CodeChunk}

\subsection{Additional Arguments}
\label{sec:addarguments}
For Gaussian models, and other distributions requiring an additional scale parameter $\phi$, one can also specify the option \code{var_par} which is the conditional variance $\phi = \sigma$ at the individual level. The default value is 1. Currently (version 0.3.1), the package supports the following families (link functions): Gaussian (identity, log), Poisson (log, identity), Binomial (logit, log, probit, identity), Gamma (log, inverse, identity), and Beta (logit). For the beta distribution one must provide \code{Beta()} to the \code{family} argument.

\subsection{Approximation of the Covariance Matrix}
\label{sec:approxsigma}
The \code{Model} class can provide an approximation to the covariance matrix $\Sigma$ with the member function \code{\$Sigma()}. This approximation is also used to calculate the information matrix with \code{\$information\_matrix()} and estimate power in the member function \code{\$power()} (see Section \ref{subsec:power}). We use the first-order approximation based on the marginal quasi-likelihood proposed by \citet{breslow1993approximate}:
\begin{equation}
    \Sigma = W^{-1} + ZDZ^T
\end{equation}
where $W = \text{diag}\left( \left(\frac{\partial h^{-1}(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}}\right)^2 \text{Var}(\mathbf{y}| \mathbf{u})\right)$, which are recognisable as the GLM iterated weights \citep{breslow1993approximate, mccullagh2019generalized}. For Gaussian-identity mixed models this approximation is exact. The diagonal of the matrix $W$ can be obtained using the member function \code{\$w\_matrix()}. The information matrix is:
\begin{equation}
\label{eq:infomat}
    M = (X^T\Sigma^{-1}X)^{-1}
\end{equation}

\citet{Zeger1988} suggest that when using the marginal quasilikelihood, one can improve the approximation to the marginal mean by ``attenuating'' the linear predictor for non-linear models. For example, with the log link the ``attenuated'' mean is $E(y_i) \approx h^{-1}(x_i\beta + z_iDz_i^T/2)$ and for the logit link  $E(y_i) \approx h^{-1}(x_i\beta\text{det}(a Dz_i^Tz_i + I)^{-1/2})$ with $a = 16\sqrt{3}/(15\pi)$. To use ``attenuation'' one can set \code{mod\$use\_attenutation(TRUE)}, the default is not to use attenutation.

\subsection{Updating Parameters}
\label{subsec:updating}
The parameters of the covariance function and linear predictor can be modified by directly changing the relevant vectors, e.g. \code{mod\$covariance\$parameters = c(0.1,0.9)}. However, a preferred alternative is to use the functions \code{update\_parameters()}, which also then trigger re-generation of the matrices and update the underlying \proglang{C++} classes: \code{model\$update_parameters( cov.pars = c(0.1,0.9))}.

\subsection{Class Inheritance}
The \pkg{R6} class system provides many of the standard features of other object orientated systems, including class inheritance. The classes specified in the \pkg{glmmrBase} package can be inherited from to provide the full range of calculations to other applications. As an example we can define a new class that has a member function that returns the log determinant of the matrix $D$:
\begin{CodeChunk}
\begin{CodeInput}
CovDet <- R6::R6Class("CovDet",
                       inherit = Covariance,
                       public = list(
                       det = function(){
                         return(Matrix::determinant(self$D)$modulus)
                       }))
cov <- CovDet$new(formula = ~(1|gr(j)*ar1(t)),
                      parameters = c(0.05,0.8),
                      data= data)
cov$det()
[1] -340.4393
\end{CodeInput}
\end{CodeChunk}
More complex applications may include classes to implement simulation-based analyses that simulate new data and fit a GLMM using one of the package's model fitting methods.

\subsection{Power Calculation}
\label{subsec:power}
Power and sample size calculations are an important component of the design of many studies, particularly randomised trials, which we use as an example. Cluster randomised trials frequently use GLMMs for data analysis, and hence are the basis for estimating the statistical power of a trial design \citep{Hooper2016}. Different approaches are used to estimate power or calculate sample size given an assumed correlation structure, most frequently using ``design effects'' \citep{Hemming2020}. However, given the large range of possible models and covariance structures, many software packages implement a narrow range of specific models and provide wrappers to other functions. For example, we identified eight different \proglang{R} packages available on CRAN or via a R Shiny web interface for calculating cluster trial power or sample size. These packages and their features are listed in Tables \ref{table:features} and \ref{table:supported_models}. As is evident, the range of models is relatively limited. Beyond this specific study type the R package \pkg{simr} provides functions to simulate data from GLMMs and estimate statistics like power using Monte Carlo simulation for specific designs. 

\begin{table}
\centering
\small
\begin{tabular}[t]{l|l|l|l|l|l|l}
\hline
Package & \shortstack{Custom \\ Designs} & \shortstack{Data \\simulation} & \shortstack{Power by\\ simulation} & \shortstack{Power by \\approx.} & \shortstack{Non-simple\\ randomisation} &  \shortstack{Optimal\\ designs}\\
\hline
\pkg{SWSamp} & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$\\
\hline
\pkg{samplingDataCRT} & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ &  $\times$\\
\hline
\pkg{ShinycRCT} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ &  $\times$\\
\hline
\pkg{swCRTdesign} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$\\
\hline
\pkg{clusterPower} & $\sim^1$ & $\times$ & \checkmark & \checkmark & $\sim^3$ & $\times$\\
\hline
\pkg{CRTdistpower} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$  & $\times$\\
\hline
\pkg{swdpwr} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ &  $\times$\\
\hline
\pkg{SteppedPower} & \checkmark & $\times$ & $\times$ & \checkmark & $\times$ & $\times$\\
\hline
\pkg{glmmrBase} & \checkmark & \checkmark & \checkmark & \checkmark &  \checkmark & \checkmark\\
\hline
\end{tabular}
\caption{\label{table:features}Features of packages for \proglang{R} to calculate power and sample size for cluster randomised trial designs}
\end{table}

\begin{table}
\centering
\scriptsize
\begin{tabular}[t]{l|l|l|l|l|l|l|l|l}
\hline
Package & \shortstack{Non-\\canonical\\ link} & \shortstack{Binom./\\ Poisson} & \shortstack{Other\\dist.} & \shortstack{Compound\\symmetry} & \shortstack{Temporal\\ decay} & \shortstack{Random\\slopes} & Covariates & \shortstack{Other\\ functions}\\
\hline
\pkg{SWSamp} & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$\\
\hline
\pkg{samplingDataCRT} & $\times$ & $\sim$ & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$\\
\hline
\pkg{ShinycRCT} & $\times$ & \checkmark & $\times$ & \checkmark & \checkmark & $\times$ & $\times$ & $\times$\\
\hline
\pkg{swCRTdesign} & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$\\
\hline
\pkg{clusterPower} & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$\\
\hline
\pkg{CRTdistpower} & $\times$ & \checkmark & $\times$ & $\sim^2$ & $\times$ & $\times$ & $\times$ & $\times$\\
\hline
\pkg{swdpwr} & \checkmark & \checkmark & $\times$ & \checkmark & $\times$ & $\times$ & $\times$ & $\times$\\
\hline
\pkg{SteppedPower} & $\times$ & $\sim$ & $\times$ & \checkmark & $\times$ & \checkmark & $\sim^3$ & $\times$\\
\hline
\pkg{glmmrBase} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark\\
\hline
\end{tabular}
\caption{\label{table:supported_models}Supported models of packages for \proglang{R} to calculate power and sample size for cluster randomised trial designs}
\end{table}


As an alternative, the flexible model specification and range of functionality of \pkg{glmmrBase} can be used to quick estimate the power for a particular design and model. The \code{Model} class member function \code{\$power()} estimates power by calculating the information matrix $M$ described above. The square root of the diagonal of this matrix provides the (approximate) standard errors of $\beta$. The power of a two-sided test for the $i$th parameter at a type I error level of $\alpha$ is given by $\Phi(\vert\beta_i\vert/\sqrt{M_{ii}} - \Phi^{-1}(1-\alpha/2))$ where $\Phi$ is the Gaussian cumulative distribution function. As an example, to estimate power for a stepped-wedge parallel cluster randomised trial (see \citet{Hooper2016}) with 10 clusters and 11 time periods and ten individuals per cluster period, where we use a Binomial-logit model with time period fixed effects set to zero, and use a auto-regressive covariance function with parameters 0.25 and 0.7:
\begin{CodeChunk}
\begin{CodeInput}
data <- nelder(~(cl(10) * t(11)) > i(10))
data$int <- 0 # int is the intervention effect
data[data$t > data$cl,'int'] <- 1
model <- Model$new(formula = ~ factor(t) + int - 1 + (1|gr(j)*ar1(t)),
                   data = data,
                   covariance = list(parameters = c(0.25,0.7)),
                   mean = list(parameters = c(rep(0,12)),0.5),
                   family = binomial())
> model$power()
>     Parameter   Value  SE        Power
> ...
> 12         int   0.5 0.1802427 0.7921986
\end{CodeInput}
\end{CodeChunk}
One can use the functionality of the \code{Model} class to investigate power for a range of model parameters. For example, to estimate power for covariance parameters in the ranges (0.05,0.5) and (0.2,0.9) we can create a data frame holding the relevant combination of values and iteratively update the model:
\begin{CodeChunk}
\begin{CodeInput}
param_data <- expand.grid(par1 = seq(0.05,0.5,by=0.05), 
                          par2 = c(0.2,0.9,by=0.1),power=NA)
for(i in 1:nrow(param_data)){
    model$update_parameters(cov.pars = 
            c(param_data$par1[i], param_data$par2[i]))
    param_data$power[i] <- model$power()[12,4]
}
\end{CodeInput}
\end{CodeChunk}

\subsection{Data Simulation}
\label{sec:datasim}
The \code{glmmrBase} package also simplifies data simulation for GLMMs. Data simulation is a commonly used approach to evaluate the properties of estimators and statistical models. The \code{Model} class has member function \code{sim\_data()}, which will generate either vector of simulated output $y$ (argument \code{type="y"}), a data frame that combines the linked dataset with the newly simulated data (argument \code{type="data"}), or a list containing the simulated data, random effects, and matrices $Z$ and $X$ (argument \code{type="all"}). To quickly simulate data new random effects are drawn as $u \sim N(0,1)$ and then the random effects component of the model simulated as $ZLu$. Simulation only of the random effects terms can be done with the relevant function in the \code{Covariance} class, \code{\$covariance\$simulate\_re()}.

\section{Model Fitting}
\label{sec:mcml}
The parameters and standard errors of GLMMs are difficult to estimate in a maximum likelihood context given the intergral in the likelihood in Equation \ref{eq:lik1}. Several approximations and associated software are available for GLMMs, however, there are few options for fitting more complex models. The \code{Model} class includes two functions: \code{MCML}, for MCML model fitting, and \code{LA}, which uses a Laplace approximation, both described below. 

\subsection{Existing software}
\label{subsec:software}
Software for fitting generalised linear mixed models has been available in \proglang{R} environment \citep{r_lang} for many years. The widely-used package \pkg{lme4} provides maximum likelihood and restricted maximum likelihood (REML) estimators \citep{lme4}. \pkg{lme4} builds on similar methods proposed by e.g. \citet{Bates2004} and \citet{Henderson1982}. For non-linear models it uses Laplacian approximation or adaptive Gaussian quadrature. However, \pkg{lme4} only allows for compound symmetric (or exchangable) covariance structures, i.e. group-membership type random effects. Function \code{glmmPQL()} in package \pkg{MASS} \citep{mass} implements Penalized quasi-likelihood (PQL) methods for GLMMs, but is again limited to the same covariance structures. The package \pkg{nlme} \citep{nlme} also uses REML approaches for linear mixed models and approximations for models with mean specifications with non-linear functions of covariates discussed in \cite{lindstrom1990nonlinear}. \pkg{nlme} provides a wider range of covariance structures, such as autoregressive functions, which are also available nested within a broader group structure. The package providing Frequentist model estimation of GLMMs with the broadest support for different specifications, including many different covariance functions, is \pkg{glmmTMB}, which uses the \pkg{TMB} (Template Model Builder) package to support model specification and implementation of Laplacian Approximation to estimate models. 
We also note that there are also several packages for Bayesian model fitting in R, including \pkg{brms} \citep{Burkner2017} and \pkg{rstanarm}, which interface with the Stan statistical software (described below), and \pkg{MCMCglmm}. 

Evidently, there already exists a good range of support for GLMM model fitting available in \proglang{R}. \pkg{glmmrBase} may be seen to provide a complement to existing resources rather than another substitute. As we discuss below, in a maximum likelihood context, the existing software all provide approximations rather than full likelihood model fitting. These approximations are typically accurate and fast, but they can also frequently fail to converge and in more complex models the quality of the approximation may degrade, which has lead to methods for bias corrections and improvements to these approximations (e.g. \citet{Breslow1995,Lin1996,Shun1995,Capanu2013}). \pkg{glmmrBase} provides an alternative that can be used to support more complex analyses, flexible covariance specification, and provide a comparison to ensure approximations are reliable.

\subsection{Markov Chain Monte Carlo Maximum Likelihood}
\label{subsec:mcml}
Markov Chain Monte Carlo Maximum Likelihood (MCML) are a family of algorithms for estimating GLMMs using the full likelihood that treat the random effect terms as missing data, which are simulated on each iteration of the algorithm. Estimation can then be achieved to an arbitrary level of precision depending on the number of samples used. Approximate methods such as Laplace approximation or quadrature provide significant computational advantages over such approaches, however they may trade-off the quality of parameter and variance estimates versus full likelihood approaches. Indeed, the quality of the Laplace approximation method for GLMMs can deteriorate if there are smaller numbers of clustering groups, smaller numbers of observations to estimate the variance components, or if the random effect variance is relatively large.

The package provides full likelihood model fitting for the range of GLMMs that can be specified using the package via three Markov Chain Monte Carlo maximum likelihood (MCML) algorithms described by \cite{mcculloch1997maximum}. Each algorithm has three main steps per iteration: draw samples of the random effects using Markov Chain Monte Carlo (MCMC), the mean function parameters are then estimated conditional on the simulated random effects using a maximum likelihood method, and then the parameters of the multivariate Gaussian likelihood of the random effects are then estimated using maximum likelihood. The process is repeated until convergence. An optional simulated likelihood step can be added to the end to refine parameter estimates. We describe each step in turn and optimisations; the overall structure of the algorithm is shown in Algorithm \ref{alg:overall}.

\begin{algorithm}
\caption{Overview of the MCMCML algorithm. $U$ represents a $Q \times m$ matrix, with each column a realisiation of the random effects $u$.}
\label{alg:overall}
\begin{algorithmic}
\Procedure {MCMCML}{$Z$,$X$,$\mathbf{y}$,$\beta^{(0)}$,$\phi^{(0)}$,$\theta^{(0)}$,$\tau$,$m$}
\State $d \gets 1, i \gets 0$;
\While{$d > \tau$}
    \State $L\gets \text{Cholesky}(D(\theta^{(i)}))$\Comment{Generate Cholesky decomposition of $D$};
    \State $V^{(i+1)}\gets \text{MCMC}(\mathbf{y},X,\beta^{(i)},Z,L,m)$
    \State $U^{(i+1)}\gets L*V^{(i+1)}$\Comment{(1) Generate $m$ samples of $u$ using MCMC};
    \State $(\beta^{(i+1)},\phi^{(i+1)}) \gets \text{OptimB}(y,X,Z,U^{(i+1)})$\Comment{(2) Estimate $\beta$ and $\phi$};
    \State $\theta^{(i+1)} \gets \text{OptimU}(U^{(i+1)})$\Comment{(3) Estimate $\theta$};
    \State $d \gets \max(\beta^{(i+1)}-\beta^{(i)},\phi^{(i+1)}-\phi^{(i)},\theta^{(i+1)}-\theta^{(i)})$;
    \State $i \gets i+1$;
\EndWhile
\State (Optional) $(\beta^{(i+1)},\phi^{(i+1)},\theta^{(i+1)}) \gets \text{SimLik}(y,X,Z,U^{(i)})$\Comment{Simulated likelihood step}
\EndProcedure
\end{algorithmic}
\end{algorithm}

%\subsubsection{Additional Classes and Functions}
%The \pkg{glmmrMCML} packages extends the classes of \pkg{glmmrBase} in several ways.


%The \proglang{C++} class \code{DMatrix} is extended by class \code{MCMCDMatrix}, which adds functions to calculate the log determinant of matrix $D$ as described in Section \ref{subsec:cclasses} and to calculate the log-likelihood of a multivariate Guassian model with covariance matrix $D$. We also include a \proglang{C++} class called \code{SparseDMatrix} that uses sparse matrix methods to calculate the log-likelihood. The \proglang{C++} class \code{mcmlModel} holds a complete model specification and provides methods for calculating the log likelihood and log gradient. The class \code{mcmloptim} holds references to a $D$ matrix and \code{mcmlModel} objects and provides all the optimisation methods, and the \code{mcmc} class holds references to a \code{mcmlModel} object to provide the HMC algorithm.

\subsection{MCMC sampling of random effects}
\label{sec:mcmc}
Reliable convergence of the algorithm requires a set of $m$ independent samples of the random effects, i.e. realisations from the distribution of the random effect conditional on the observed data, and with fixed parameters. MCMC methods can be used to sample from the `posterior density' $f_{\mathbf{u}|\mathbf{y}}(\mathbf{u}|\mathbf{y},\beta^{(i)},\phi^{(i)},\theta^{(i)}) \propto f_{\mathbf{y}|\mathbf{u}}(\mathbf{y}|\mathbf{u},\beta^{(i)},\phi^{(i)})f_{\mathbf{u}}(\mathbf{u}|\theta^{(i)})$. We offer two MCMC methods to sample the random effects. To improve sampling for both methods, instead of generating samples of $\mathbf{u}$ directly, we instead sample $\mathbf{v}$ from the model:
\begin{align}
\begin{split}
\label{eq:mcmc}
    \mathbf{y} &\sim G(h(X\hat{\beta} + \Tilde{Z}\mathbf{v}); \hat{\phi}) \\
    \mathbf{v} &\sim N(0,I)
    \end{split}
\end{align}
where $\Tilde{Z} = ZL$, and $I$ is the identity matrix. Once $m$ samples of $\mathbf{v}$ are returned, we then transform the samples as $\mathbf{u} = L\mathbf{v}$. 

First, we have natively implemented Hamiltonian Monte Carlo (HMC), which uses Hamiltonian dynamics to propose new values within a Metropolis-Hastings algorithm. A detailed discussion of Hamiltonian dynamics and HMC is provided elsewhere (e.g. \citet{Betancourt2018}. The HMC algorithm is described by \citet{Homan2014}, which we implement in \pkg{glmmrBase} to generate $M$ samples of $\mathbf{v}$, which includes an adaptation step to dynamically set the step size. The algorithm requires the gradient of the log likelihood of $\mathbf{v}$, $\mathcal{L}(v) = \log(f_{\mathbf{y}|\mathbf{v}}(\mathbf{y}|\mathbf{v},\beta,\phi)) + \log(f_{\mathbf{v}}(\mathbf{v}))$. For the model in Equation \ref{eq:mcmc} the gradient has a relatively simple closed form, for example for the Gaussian model with identity link:
\begin{align}
\label{eq:loggradv}
    \nabla \mathcal{L}(\mathbf{v})^T &= \frac{1}{\sigma^2}\Tilde{Z}^T(\mathbf{y} - X\beta - \Tilde{Z}\mathbf{v}) - \mathbf{v} 
\end{align}

The HMC algorithm has several control parameters, which are saved in  \code{model\$mcmc\_options}. These options are: the number of warmup iterations (default 500), the number of adaptation iterations $M_{adapt}$ (50), the number of sampling iterations (250), the maximum number of leapfrog steps (100), the target acceptance proportion $\delta$ (0.95), and the `integration time' $\lambda$ (5). Lower values of $\lambda$ are likely required for reliable sampling for non-linear models \citep{Homan2014}. The limit of the number of leapfrog steps provides a reduction on the integration time if required, but at the potential cost of increasing the correlation between samples in the chain and lowering the effective sample size. The HMC algorithm can require careful tuning for more complex models to achieve reliable and stable convergence. As an alternative we have therefore also included MCMC sampling with Stan, which does not require tuning. Stan also implements HMC but uses other advances such as the No-U-Turn Sampler (NUTS) \citep{carpenter2017stan,Homan2014}, which automatically `tunes` the HMC parameters. Stan is used instead of the internal HMC sampler if the option \code{usestan=TRUE} in the call to \code{MCML}, which is the default. The NUTS algorithm is highly recommended here if the user can install and use Stan, as it does not require tuning, and can generally achieve a much better effective sample size per unit time and so lead to better convergence of the MCMCML algorithm.

%\begin{algorithm}
%\caption{Hamiltonian Monte Carlo with Adaptive Step Size}
%\label{alg:hmc}
%\begin{algorithmic}
%\Procedure {HMC}{$u^{(0)},\lambda,\delta,M,M^{adapt}$}
%
%\State $\mathcal{L}(v) = \log(f_{\mathbf{y}|\mathbf{v}}(\mathbf{y}|\mathbf{v},\beta,\phi)) + \log(f_{\mathbf{v}}(\mathbf{v}))$
%
%\State $\epsilon_0 \gets 0.01, \mu \gets \log(0.1), \bar{\epsilon}_0 \gets 1, \bar{H}_0 \gets 0$
%
%\For{m = 1 to M}
%    \State Sample $r \sim N(0,I)$\Comment{The `velocity'}
%    \State Set $v^{(m)} \gets v^{(m-1)}$, \Tilde{v} \gets v^{(m-1)}, \Tilde{r} \gets r, L_m \gets \max(1, \text{Round}(\lambda/\epsilon_{m-1}))$;
%    \For{i = 1 to $L_m$}
%        \State Set $\Tilde{v},\Tilde{r} \gets \text{Leapfrog}(\Tilde{v},\Tilde{r},\epsilon_{m-1})$
%    \EndFor
%    \State $a \gets \min\{1,\exp( \mathcal{L}(\Tilde{v}) - \mathcal{L}(v^{(m-1)}) - 0.5\Tilde{r}^T\Tilde{r} + 0.5r^Tr ) \}, b \sim \text{Uniform}(0,1) $
%    \If{b < a}
%        \State $v^{(m)} \gets \Tilde{v}$    
%    \EndIf
%    
%    \If{$m \leq M_{adapt}$} \Comment{Adapt step size}
%        \State $\bar{H}_m \gets \left( 1- \frac{1}{10 + m} \right)\bar{H}_{m-1} + \frac{1}{10+m}(\delta - a)$
%        
%        \State $\log \epsilon_m \gets \mu - \frac{\sqrt{m}}{0.05}\bar{H}_m, \log \bar{\epsilon}_m \gets m^{-0.75}\log \epsilon_m + (1 - m^{-0.75}) \log \bar{\epsilon}_{m-1}$
%    \Else
%        \State $\epsilon_m \gets \bar{\epsilon}_{M_{adapt}} $
%    \EndIf
%\EndFor
%\EndProcedure
%
%\Function {Leapfrog}{$v,r,\epsilon$} \Comment{Leapfrog integrator}
%    \State $\Tilde{r} \gets r + \frac{\epsilon}{2}\nabla_v \mathcal{L}(v)$
%    \State $\Tilde{v} \gets v + \epsilon\Tilde{r}$
%    \State $\Tilde{r} \gets r + \frac{\epsilon}{2}\nabla_v \mathcal{L}(v)$
%    \State \textbf{return} $\Tilde{v},\Tilde{r}$
%\EndFunction
%\end{algorithmic}
%\end{algorithm}

Each iteration of the HMC algorithm must evaluate the log-likelihood (and its gradient). Since the elements of both $\mathbf{y}$ and $\mathbf{v}$ are all independent in this version of the model, evaluating the log-likelihood is parallelisable within a single chain, which significantly improves running time. Parallelisation is implemented in the HMC sampler. The Stan function \code{reduce\_sum()} provides functionality to parallelise these calculations in the Stan programs. However, this function is only available in Stan version 2.23 and higher, so, at the time of writing (version 0.3.1) it is not available in the \proglang{R} package \pkg{rstan}, the most common way to use Stan in \proglang{R}. We instead require use of the external program \pkg{cmdstan} through the \proglang{R} package \pkg{cmdstanr} until \pkg{rstan} is updated \citep{stan2022}.

\subsection{Estimation of mean function parameters}
\label{sec:estbeta}
There are two methods to generate new estimates of $\beta$ and $\phi$ conditional on $\mathbf{y}$, $U^{(i)}$, and $\phi^{(i)}$ for the function \texttt{OptimB} in Algorithm \ref{alg:overall}:
\begin{enumerate}
    \item Monte Carlo Expectation Maximisation (MCEM): Choose $\beta$ and $\phi$ that maximises $E_u\left[ \log f_{\mathbf{y}|\mathbf{u}}(\mathbf{y}|\mathbf{u},\beta,\phi) \right]$;
    \item Monte Carlo Newton Raphson (MCNR): Set 
    \begin{equation*}
        \beta^{(i+1)} = \beta^{(i)} + E_u \left[ X^T W(\theta^{(i)},\mathbf{u}) X \right] ^{-1}X^T \left( E_u \left[ W(\theta^{(i)},\mathbf{u}) \frac{\partial h^{-1}(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}} (\mathbf{y - \boldsymbol{\mu}}(\beta^{(i)},\mathbf{u}))|\mathbf{y} \right] \right)
    \end{equation*}
    where, as before, we use the first-order approximation (with or without attenutation), $W(\theta,\mathbf{u}) = \text{diag}\left( \left(\frac{\partial h^{-1}(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}}\right)^2 \text{Var}(\mathbf{y}| \mathbf{u})\right)$.
\end{enumerate}
For both methods, the expectations are with respect to the random effects. We estimate the expectations using the realised samples from the MCMC step of the algorithm. For MCEM the estimate is calculated in a straightforward way by calculating the sum over $n$ and $m$ of the log-likelihood and dividing by $m$. Both the inner and outer loops of this sum can be parallelised to provide performance gains. We can similarly parallelise the sums to calculate the expectations for MCNR. For maximisation of the likelihood (or rather minimisation of the negative log likelihood), we use the BOBYQA algorithm, a numerical, derivative free optimser that allows bounds for expectation maximisation \citep{powell2009bobyqa}. We provide a \proglang{C++} interface to the BOBYQA algorithm in the \proglang{R} package \pkg{rminqa}.

\subsection{Estimation of covariance parameters}
\label{subsec:covparest}
The final step of each iteration is to generate new estimates of $\theta$ given the samples of the random effects. We aim to maximise $\hat{E}_{\mathbf{u}} \left[ \log(f_{\mathbf{u}}(U_{,i}|\theta)) \right] = \frac{1}{m}\sum_{i=1}^m \log(f_{\mathbf{u}}(U_{,i}|\theta))$, where $U_{,i}$ represents the $i$th column of $U$. The multivariate Gaussian density is:
\begin{align*}
    \log f_{\mathbf{u}}(\mathbf{u}|\theta) &= -\frac{m}{2}\log(2\pi) - \frac{1}{2}\log(|D|) - \frac{1}{2}\mathbf{u}^T D^{-1} \mathbf{u} \\
    &= -\frac{m}{2}\log(2\pi) - \frac{1}{2}\sum_{i = 1}^{n} \log(\text{diag}(L)_i) - \frac{1}{2}\mathbf{u}^T L^{-T}L^{-1} \mathbf{u}
\end{align*}

As described in Section \ref{subsec:compact}, we can take two approaches to the Cholesky factorisation of matrix $D$, either exploiting the block diagonal structure or using sparse matrix methods (see Section \ref{subsec:covcalc}). In both cases the quadratic form can be rewritten as $\mathbf{u}^T D^{-1} \mathbf{u} = \mathbf{u}^T L^{-T}L^{-1} \mathbf{u} = \mathbf{z}^T\mathbf{z}$, so we can obtain $\mathbf{z}$ by solving the linear system $L\mathbf{z} = \mathbf{u}$ using forward substitution, which, if $L$ is also sparse provides efficiency gains. The log determinant is $\log(|D|) = \sum \log(\text{diag}(L))$. In the case of using a block diagonal formulation, we calculate the form:
\begin{equation*}
    \log f_{\mathbf{u}}(\mathbf{u}|\theta) = \sum_{b=0}^{\texttt{B}} \left(-\frac{m}{2}\log(2\pi) - \frac{1}{2}\sum_{i = r_b}^{s_b} \log(\text{diag}(L)_i) - \frac{1}{2}\mathbf{z}^T_{r_b:s_b} \mathbf{z}_{r_b:s_b} \right)
\end{equation*}
where $r_b$ and $s_b$ indicates the start and end indexes of the vector, respectively, corresponding to block $b$. 

\subsection{Simulated Likelihood Step}
It is possible to estimate the value of the likelihood directly using a simulated maximum likelihood step. Starting from Equation \ref{eq:lik1}:
\begin{align*}
     L(\beta,\phi,\theta|\mathbf{y}) &= \int f_{\mathbf{y}|\mathbf{u}}(\mathbf{y}|\mathbf{u}, \beta, \phi)f_{\mathbf{u}}(\mathbf{u}|\theta) d \mathbf{u} \\
     &= \int \frac{f_{\mathbf{y}|\mathbf{u}}(\mathbf{y}|\mathbf{u}, \beta, \phi)f_{\mathbf{u}}(\mathbf{u}|\theta)}{h_{\mathbf{u}}(\mathbf{u})} h_{\mathbf{u}}(\mathbf{u}) d \mathbf{u}\\
     &\approx \frac{1}{m}\sum_{i=1}^m \frac{f_{\mathbf{y}|\mathbf{u}}(\mathbf{y}|\mathbf{u}^{(i)}, \beta, \phi)f_{\mathbf{u}}(\mathbf{u}^{(i)}|\theta)}{h_{\mathbf{u}}(\mathbf{u}^{(i)})}
\end{align*}
where $\mathbf{u}^{(i)}$ is selected from the importance sampling distribution $h_{\mathbf{u}}(\mathbf{u})$, for which we use the likelihood evaluated at the current values of the model parameters. The importance sampling approach gives an unbiased estimate of the likelihood. In some applications this likelihood may be maximised in an iterative approach, updating the sampling distribution on each step. However, here we include it as an optional step following completion of either the MCEM or MCNR algorithms as a `refinement' step, as the estimated parameters are likely to already be close to those that maximise the likelihood. 

\subsubsection{Standard errors}
The default method of calculating the standard errors is to use the information matrix in Equation \ref{eq:infomat}, where the user can specify whether to use attenuation. As an alternative one can estimate the Hessian matrix of the model parameters using numerical differentiation. The standard errors are then the square root of the diagonal of the inverse of the Hessian. 

\subsection{Model Estimation}
The member function \code{MCML} fits the model. There are multiple options to control the algorithm and set its parameters; most options are determined by the relevant parameters of the \code{Model} object. The main options for the user to choose are the algorithm (MCEM or MCNR; default is MCNR) and the tolerance of the algorithm $\tau$ to determine termination (default is $\tau = 0.01$). We provide an example in a following section.

\subsection{Other output}
The \code{MCML} function returns an object of (S3) class \code{mcml} for which print and summary methods are provided. The print function returns standard regression output and basic diagnostics. We implement the conditional Akaike Information Criterion method described by \citet{Vaida2005} for GLMMs, and approximate conditional and marginal R-squared values using the method described by \citet{Nakagawa2013} for GLMMs. The object returned by \code{MCML} also contains other potentially useful output. In particular, there is often interest in the random effects themselves. As these are simulated during the MCMCML algorithm, the final set of $m$ samples are returned as a $Q \times m$ matrix facilitating further analysis.

\subsection{Compactly Supported Covariance Functions}
\label{subsec:compact}
A block diagonal matrix $D$ can be considered relatively sparse, especially if the number of blocks is large and their dimension small. However, in other cases the number of blocks is $B=1$, such that $Z$ is the identity matrix. For example, models that use a Gaussian process specification, such as a geospatial statistical model \citep{Diggle1998}, will have a dense covariance matrix. Fully dense covariance matrices can present a computational bottleneck as the computational complexity per covariance `block' is $O(n^3)$. One approach to approximating these models is to use a compactly supported covariance function, which can be viewed as `covariance tapering' \citep{Kaufman2008}, although the approach implemented here is what they describe as the `one taper' method, which may be biased in some circumstances. As shown in Table \ref{table:cov_fun}, we include several compactly supported covariance functions developed by \citet{Gneiting2002} and \citet{Wendland1995}. A covariance function with compact support has value zero beyond some `effective range'. Using these functions can result in a sparse matrix $D$, which is then amenable to the sparse matrix methods in this package.  It is important to note that product covariance functions above are only valid for certain numbers of dimensions, which are listed in the table. To implement these functions with an effective range of $r$, beyond which the covariance is zero, one must divide each variable by $r$. For example, if there are two spatial dimensions, \code{x} and \code{y}, in the data frame \code{data} then one would create \code{data\$xr <- data\$x/r}, and equivelently for \code{y}. Then, one could use the covariance function, for example, \code{~ wend0(xr,yr)}.

\subsection{Laplace Approximation}
\label{sec:LA}
We also provide model fitting using a Laplace approximation to the likelihood with the \code{MCMLModel} member function \code{LA()}. We include this approximation as it provides a faster means of fitting the models in this package. These estimates may be of interest in their own right, but can also serve as useful starting values for the MCML algorithm. We note that the Laplace approximation methods in \pkg{glmmrBase} are not as computationally efficient as the approximations provided in \pkg{glmmTMB} and \pkg{lme4} for equivalent models. To improve computational time, we approximate the log-likelihood of the re-parameterised model given in Equation (\ref{eq:mcmc}) as:
\begin{equation}
\label{eq:la1}
    \log L(\beta,\phi,\theta,\mathbf{v}|\mathbf{y}) \approx -\frac{1}{2}\vert I + \Tilde{Z}^T W \Tilde{Z} \vert + \sum_{i=1}^n\log(f_{y\vert\mathbf{u}}(y_i\vert \mathbf{v},\beta,\phi))) -\frac{1}{2}\mathbf{v}^T\mathbf{v}
\end{equation}
where $\Tilde{Z} = ZL$ and $D=LL^T$ and we explicitly include $\mathbf{v}$. The approximation is evaluated at the values of $\mathbf{v}$ that maximise the function:
\begin{equation}
\label{eq:la2}
    \mathcal{L}(\beta,\phi,\theta,\mathbf{v}|\mathbf{y}) = \sum_{i=1}^n\log(f_{y\vert\mathbf{u}}(y_i\vert \mathbf{v},\beta,\phi))) -\frac{1}{2}\mathbf{v}^T\mathbf{v}
\end{equation}
We therefore need to iterate model fitting to obtain estimates of $\theta$, $\beta$, and $\phi$ and then to then obtain estimates of $\mathbf{v}$ until the values converge. We offer two iterative algorithms for model fitting both with the structure shown in Algorithm \ref{alg:la}, which reflects the steps used by \code{glmer} in \pkg{lme4}. For both steps 2 and 3 we minimise the approximation to the negative log-likelihood using the derivative free optimisation algorithm BOBYQA \citep{powell2009bobyqa}. For the first approach we also use this algorithm for Step 1, for the second approach we use a scoring algorithm to update the estimates of $\beta$ and $\mathbf{v}$:
\begin{align*}
    \begin{split}
        \beta^{(i+1)} &= \beta^{(i)} + (X^TWZ)^{-1}X^TW \frac{\partial \boldsymbol{\eta}}{\partial \boldsymbol{\mu}} (\mathbf{y} - \boldsymbol{\mu}) \\
        \mathbf{v}^{(i+1)} &= (I+\Tilde{Z}^TW \Tilde{Z})^{-1}\nabla \mathcal{L}(\mathbf{v})
    \end{split}
\end{align*}
where $\frac{\partial \boldsymbol{\eta}}{\partial \boldsymbol{\mu}}$ is a diagonal matrix with elements $\frac{\partial \eta_i}{\partial \mu_i}$. The log gradient $\nabla \mathcal{L}(\mathbf{v})$, for example in Equation (\ref{eq:loggradv}), is as required for the HMC algorithm; the \code{Model} class member function \code{\$log\_gradient()} will return the gradient of the log likelihood for a given $\mathbf{v}$. The scoring algorithm approach is much quicker, and is the default.

\begin{algorithm}
\caption{Laplace approximation model fitting}
\label{alg:la}
\begin{algorithmic}
\Procedure {LA}{$\beta^{(0)},v^{(0)},\theta^{(0)},\phi^{(0)}$}
 \State $d \gets 1, i \gets 0, \tau \gets 0.01$
 \While{$d > \tau$}
    \State $\beta^{(i+1)},\mathbf{v}^{(i+1)} \gets \argmin_{\beta,\mathbf{v}} -\mathcal{L}_2(\beta,\phi^{(i)},\theta^{(i)},\mathbf{v}|\mathbf{y})$ \Comment{Step 1}
    \State $\theta^{(i+1)},\phi^{(i+1)} \gets \argmin_{\theta,\phi} -\mathcal{L}(\beta^{(i+1)},\phi,\theta,\mathbf{v}^{(i+1)}|\mathbf{y})$ \Comment{Step 2}
    \State $i \gets i+1$
 \EndWhile
 \State $\beta^{(i)}\theta^{(i)},\phi^{(i)} \gets \argmin_{\beta,\theta,\phi} -\mathcal{L}(\beta,\phi,\theta,\mathbf{v}^{(i-1)}|\mathbf{y})$ \Comment{Step 3}
 \EndProcedure
\end{algorithmic}
 \end{algorithm}

 
\subsection{Prediction}
\label{sec:predict}
One can generate predictions from the model at new values of $X$, $Z$, and the variables that define the covariance function. Conditional on the estimated or simulated values of $\mathbf{u}$, the distribution of the random effects at new values is:
\begin{equation*}
    \mathbf{u}_{new} \sim N\left( \Sigma_{01}\Sigma_{00}^{-1}\mathbf{u}, \Sigma_{11}-\Sigma_{01}\Sigma_{00}^{-1}\Sigma_{01}^T \right)
\end{equation*}
where $\Sigma_{00}$ is the covariance matrix of the data at the observed values, $\Sigma_{11}$ is the covariance matrix at the new values, and $\Sigma_{01}$ is the covariance between the observed and new values. Where there are multiple samples of $\mathbf{u}$, the conditional mean in averaged across them. Following a model fit, we can generate the linear predictor and mean and covariance at new locations using the member function \code{\$predict(newdata)}. This differs from the functionality of the function \code{\$fitted()}, which generates the linear predictor (with or without random effects) for the existing model at the observed data locatons, and \code{\$sim\_data()}, which simulates outcome data at the existing data values.

\subsection{Examples and Comparison with Other Packages}
We have aimed to optimise the coding to improve computational efficiency, however, MCML estimation is inherently much slower than related approximate likelihood approaches. For comparable covariance structure models \pkg{lme4} (for exchangable covariance functions), \pkg{glmmTMB} (more generally), and the Laplacian Approximations in this package are anywhere from 10 to 100 or more times faster (milliseconds versus seconds) than MCML. One may therefore ask why to use MCML over these much faster approaches. As described above, the Laplace approximation can perform relatively poorly with certain model specification or data. There does not exist any extensive investigation of the performance of MCML yet, but it is likely to perform much better in these situations. In the examples below we provide an illustrative comparison of estimates from different packages alongside those from \pkg{glmmrMCML}. Frequently, the Laplace approximation used by \pkg{lme4}, \pkg{glmmTMB}, or this package encountered a singular matrix issue due leading to a covariance parameter being estimated as zero. In these cases MCML was able to provide reliable estimates. In some scenarios the estimates of $\beta$ also differed between model fitting approaches. 

\subsubsection{Cluster randomised trial}
\label{subsec:crct}
We simulate data from a parallel cluster randomised trial with ten clusters, five time periods, and ten individuals per cluster period cross-sectionally sampled. Clusters are assigned in a 1:1 ratio to treatment and control. Ten clusters (five per arm) is a relatively small sample size, and we choose values of the covariance parameters to be relatively large, giving a level of cluster correlation relatively high compared to values seen in many applied settings, to examine models that may be hard to fit and where there may be differences between approximate methods and MCML. 

The model is, for individual $i$ in cluster $j$ at time $t$:
\begin{align*}
    y_{ijt} & \sim \text{Binomial}(1,p_{ijt}) \\
    p_{ijt} &= \Lambda(\beta_0 d_{j} + \beta_1I(t=1) + ... + \beta_6 I(t=5) + \gamma_{jt})
\end{align*}
where $d_j$ is an indicator for treatment status, $I(.)$ is the indicator function, and $\gamma_{jt}$ is a random effect. For the data generating process we use $\beta_0 = 0.5$ and $\beta_1,...,\beta_6 \sim N(0,1)$. We specify the exchangeable correlation structure $\gamma_{jt} = \gamma_{1,j} + \gamma_{2,jt}$ where $\gamma_{1,j} \sim N(0,0.5^2)$ and  $\gamma_{2,jt} \sim N(0,0.3^2)$. See \citet{Li2021} for a discussion of such models in the context of cluster randomised trials.

We can specify the model as follows, where we have included parameter values for the data simulation step:
\begin{CodeChunk}
\begin{CodeInput}
data <- nelder(~ (cl(10)*t(5))>i(10))
data$int <- 0
data[data$cl>5,'int'] <- 1
model <- Model$new(
   formula = ~ int + factor(t)-1 + (1|gr(cl))+(1|gr(cl,t)),
   covariance = list(parameters = c(0.5,0.3)),
   mean = list(parameters = c(0.5,-2.1,-2.2,-2.0,-1.5,-1.8)),
   data = data,
   family = binomial())
y <- model$sim_data()
\end{CodeInput}
\end{CodeChunk}
We re-simulated the data if either \code{glmer} or \code{glmmTMB} did not converge. The relatively high levels of cluster correlation required higher numbers of MCMC samples to achieve convergence, which we set to $m=500$. We used Stan NUTS for MCMC sampling. Parameters are updated within the model after each fit and so would act as the starting values for the next fit. For MCML model fitting we first used the Laplace approximation to generate starting values and then used MCML, adding the times together. We fit the models in \pkg{glmmrMCML} specified above using the following code:
\begin{CodeChunk}
\begin{CodeInput}
model1$mcmc_options$samps <- 1000 # set the number of samples to 500
fit1 <- model1$LA(y) #Laplace approximation with Newton-Raphson
fit2 <- model1$LA(y,method="nloptim") #Laplace approximation with BOBYQA
fit3 <- model1$MCML(y,usestan=TRUE) #MCNR 
fit4 <- model1$MCML(y,method="mcem",usestan=TRUE) #MCEM 
\end{CodeInput}
\end{CodeChunk}

Table \ref{tab:simres1} shows the results from the four model fitting methods alongside equivalent estimates from the \code{glmer} function from the \pkg{lme4} package and the \code{glmmTMB} function in the \pkg{glmmTMB} package. Point estimates of $\beta$ are within 0.01 for all the Laplace approximation methods, but differ by up to 0.05 from the MCML methods. Standard errors are generally larger with the MCML method, resulting from the larger covariance parameter estimates with MCML.

\begin{table}[]
    \centering
    \begin{tabular}{c|cccccc}
    \toprule
         & MCNR & MCEM & LA-Optim & LA-NR & \code{glmer} & \code{glmmTMB} \\
         \midrule
    $\beta_1$ & 1.01 (0.31) & 1.02 (0.32) & 0.98 (0.26) & 0.98 (0.26) & 0.98 (0.25) & 0.98 (0.25) \\
    $\beta_2$ & -2.36 (0.37) & -2.38 (0.37) & -2.32 (0.34) & -2.32 (0.33) & -2.31 (0.36) & -2.31 (0.34) \\
    $\beta_3$ & -2.02 (0.38) & -2.04 (0.38) & -1.97 (0.34) & -1.97 (0.34) & -1.96 (0.31) & -1.96 (0.31) \\
    $\beta_4$ & -2.58 (0.36) & -2.60 (0.37) & -2.50 (0.33) & -2.50 (0.33) & -2.50 (0.36) & -2.50 (0.36) \\
    $\beta_5$ & -1.76 (0.33) & -1.78 (0.33) & -1.72 (0.29) & -1.72 (0.29) & -1.72 (0.30) & -1.72 (0.30) \\
    $\beta_6$ & -1.78 (0.34) & -1.79 (0.35) & -1.73 (0.31) & -1.73 (0.31) & -1.72 (0.30) & -1.72 (0.30) \\
    \midrule
    $\theta_1$ & 0.25 & 0.28 & 0.06 & 0.05 & 0.00 & 0.00  \\
    $\theta_2$ & 0.37 & 0.38 & 0.22 & 0.22 & 0.20 & 0.20 \\
    \midrule
    Time (s) & 13.3 & 72.0 & 4.4 & 0.8 & 0.4 & 0.7 \\
    \bottomrule    
    \end{tabular}
    \caption{Parameter estimates from a simulated cluster randomised trial with exchangable cluster covariance function. Values are point estimates (SE). LA-Optim: Laplace approximation with derivative free optimisation, LA-NR: Laplace approximation with Newton-Raphson. \code{glmer} is from the \pkg{lme4} package and \code{glmmTMB} is from the \pkg{glmmTMB} package. Timings are indicative based on a single run on a Intel Core i7 9700K, 16 GB RAM, Windows 10.}
    \label{tab:simres1}
\end{table}

For the second example, we use the same data as above, but re-specify the covariance function as an autoregressive structure within clusters. Data simulated from this model frequently resulted in convergence problems with \pkg{glmmTMB}; we used data for which the model converged. We can either generate a new model object, or update the existing model:
\begin{CodeChunk}
\begin{CodeInput}
model$covariance <- Covariance$new(formula = ~(1|gr(cl)*ar1(t)),
                                    parameters = c(0.5,0.7),
                                    data = data)
model$check()
\end{CodeInput}
\end{CodeChunk}
where the \code{\$check()} function forces an update.

Table \ref{tab:simres2} reports the results from fitting this model alongside estimates from \pkg{glmmTMB} (\pkg{lme4} is not able to fit this covariance function). There are relatively large differences in point estimates between the methods, particularly between the output of \code{glmmTMB} and the other methods. We do not claim any particular set of estimates is correct, rather we demonstrate that one can get relatively large differences between fitting methods, which may necessitate further investigation.

\begin{table}[]
    \centering
    \begin{tabular}{c|ccccc}
    \toprule
         & MCNR & MCEM & LA-Optim & LA-NR & \code{glmmTMB} \\
         \midrule
    $\beta_1$ & 0.34 (0.42) & 0.33 (0.43) & 0.35 (0.44) & 0.34 (0.44) & 0.24 (0.34) \\
    $\beta_2$ & -1.90 (0.42) & -1.90 (0.42) & -1.96 (0.44) & -1.95 (0.44) & -2.01 (0.35) \\
    $\beta_3$ & -2.22 (0.43) & -2.21 (0.43) & -2.26 (0.44) & -2.26 (0.45) & -2.27 (0.37) \\
    $\beta_4$ & -2.02 (0.41) & -2.02 (0.41) & -2.05 (0.43) & -1.27 (0.40) & -2.12 (0.35) \\
    $\beta_5$ & -1.25 (0.39) & -1.24 (0.39) & -1.28 (0.40) & -1.27 (0.40) & -1.53 (0.31) \\
    $\beta_6$ & -2.21 (0.40) & -2.21 (0.40) & -2.26 (0.42) & -2.25 (0.42) & -2.26 (0.37) \\
    \midrule
    $\theta_1$ & 0.64 & 0.65 & 0.72 & 0.73 & 0.46 \\
    $\theta_2$ & 0.80 & 0.80 & 0.72 & 0.72 & 0.83\\
    \midrule
    Time (s) & 22.1 & 72.1 & 2.7 & 1.1 & 0.7 \\
    \bottomrule    
    \end{tabular}
    \caption{Parameter estimates from a simulated cluster randomised trial with autoregressive cluster covariance function. Values are point estimates (SE). LA-Optim: Laplace approximation with derivative free optimisation, LA-NR: Laplace approximation with Newton-Raphson. \code{glmmTMB} is from the \pkg{glmmTMB} package. Timings are indicative based on a single run on a Intel Core i7 9700K, 16 GB RAM, Windows 10.}
    \label{tab:simres2}
\end{table}



% For example, we can extract the sample mean for each random effect from \code{fit3} in the example above
%\begin{CodeChunk}
%\begin{CodeInput}
%df$z <- rowMeans(fit3$re.samps)
%\end{CodeInput}
%\end{CodeChunk}

%\subsubsection{Comparison with Other Packages}
 %The other GLMM model fitting packages described in Section \ref{subsec:software} all use different approximations, particularly the Laplacian approximation. While this method has generally performed well in simulation studies for simpler models, there is a lack of comparison of performance in terms of bias or type I error for more complex structures and models. We aim to conduct a comparison in the near future.

\section{C-Optimal Experimental Designs with glmmrOptim}
The \pkg{glmmrOptim} package provides a set of algorithms and methods to solve the c-optimal experimental design problem for GLMMs. More detail on the methods in this package and a comparison of their performance can be found in \citet{Watson2022}. 

We assume there are $n$ total possible observations indexed by $i \in [1,...,n]$ whose data generating process is described by a GLMM described in Section \ref{subsec: glmm_framework}. The observations (indexes) are grouped into $J$ `units', for example clusters or individual people, which are labelled as $\mathcal{E}_j \subset [1,...,n]$. These units may contain just a single observation. The set of all units is the \textit{design space} $\mathcal{D}:={e_1,...,e_J}$ and a `design' is $d \subset D$. The optimal experimental design problem is then to find the design $d$ of size $J' < J$ that minimises the c-optimality function:
\begin{equation}
\label{eq:coptim}
    g(d) = c^T M_d^{-1} c
\end{equation}
where $M$ is the information matrix described in Equation \ref{eq:infomat} associated with design $d$. There is no exact solution to this problem, in part because of the complexity resulting from the non-zero correlation between observations and experimental units. 

There are several algorithms that can be used to generate approximate solutions. A full discussion of this problem in the context of GLMMs and evaluation of relevant algorithms is given in \citep{Watson2022} (see also \citet{Fedorov1972,Wynn1970,Nemhauser1978,Fisher1978}). Here, we briefly describe the relevant algorithms and their implementation in the \pkg{glmmrOptim} package.

\subsection{Existing software}
The package \pkg{glmmrOptim} provides algorithms for identifying approximate c-optimal experimental designs when the observations and experimental conditions may be correlated. In \proglang{R}, there are several packages that provide related functionality. The \pkg{skpr} package provides D, I, Alias, A, E, T, and G-optimal, but not c-optimal, designs for models including with correlated observations, but only where the experimental conditions are uncorrelated. The \proglang{R} package \pkg{AlgDesign} also provides algorithms for estimating D, A, and I-optimal experimental designs, including for models with correlated observations, but again without correlation between experimental conditions. \pkg{AlgDesign} implements a version of the Greedy Algorithm (described below) for D-optimality. Both packages utilise, among other approaches, the approximation described below in Section \ref{subsec:uncorexp}. Other relevant packages include \pkg{OptimalDesign}, which uses the commercial software \pkg{Gurobi}, and the packages \pkg{FrF2} and \pkg{conf.design}, which consider factorial designs specifically. Only \pkg{skpr} provides both optimal experimental design algorithms and associated methods like Monte Carlo simulation and power calculation. \pkg{glmmrBase} can provide power calculations using the functions discussed in Section \ref{subsec:power}, as well as the other suite of methods to interrogate and fit relevant models once an optimal design has been identified, which emphasizes the usefulness of the linked set of packages we present.

\subsection{Combinatorial Optimisation Algorithms}
\label{subsec:combinopt}
There are three basic combinatorial algorithms relevant to the c-optimal design problem. `Combinatorial' here means choosing $J'$ discrete elements (experimental units) from the set of $J$ to minimise Equation (\ref{eq:coptim}). 

Algorithm 3 describes the `local search algorithm'. This algorithm starts with a random design $d_0$ of size $J'$ and then makes the optimal swap between an experimental condition in the design and one not currently in the design. It proceeds until no improving swap can be made.  Algorithm 4 shows the `greedy search algorithm', which starts from an empty design and adds the best experimental condition at each step until a design of size $J'$ is reached. Algorithm 5 shows the `reverse greedy search algorithm' that instead starts from the complete design space and iteratively removes the worst experimental condition until a design of size $J'$ is achieved. 

These algorithms are not guaranteed to produce a c-optimal design. We can often place an upper bound on the value of the solution relative to the true optimal design. For the local search this is 1.5 times the best possible value \citep{Fisher1978}. The greedy and reverse greedy searches have a similar upper bound in some cases, however, for the GLMM experimental design problem we consider here, there is in fact no upper limit \citep{Ilev2001,Sviridenko2017}. However, empirical comparisons suggest they may still provide adequate solutions \citep{Watson2022}. An additional consideration for the greedy algorithm is that we cannot start with an empty design, since this would be degenerate and the information matrix not positive definite. We must therefore start with a small non-degenerate design, which is likely to also affect its performance.

We offer these three algorithms in the \pkg{glmmrOptim} package. We allow the use to use any combination of these algorithms in succession. For example, a local search for a design of size $p << J'$ followed by a greedy search, followed by a local search. While such combinations do not necessarily have any theoretical guarantees, they may be of interest to users of the package. A more complete discussion of the methods provided by this package can be found in \citet{Watson2022}, with further examples in \citet{Watson2023}.

%In the package we offer four variants of these algorithms: 
%\begin{enumerate}
%    \item  Local search algorithm starting from a randomly chosen design of size $J'$;
%    \item  Local search algorithm starting from a randomly chosen design of size $ s < J'$, followed by greedy algorithm, followed by a local search;
%    \item  Local search algorithm starting from a randomly chosen design of size $J' < n$, followed by greedy algorithm;
%    \item Greedy algorithm starting from a randomly chosen design of size $J' < n$, followed by a local search algorithm.
%\end{enumerate}
%Watson and Pan show that variants 1 and 4 generally provide the best solutions, with 4 requiring the least computational effort.

\begin{algorithm}
\caption{Local search algorithm}
\begin{algorithmic}
 \State Let $d_0$ be size $J'$ design 
 \State Set $\delta = 1$ and $d \leftarrow d_0$ 
 \While{$\delta > 0$}
 \ForAll{element $e_j \in d$ and $e_{j'}\in D / d$}
    Calculate $g(d / \{e_j\} \cup \{e_{j'}\})$ 
 \EndFor
 \State Set $d' \leftarrow \argmin_{j,j'} g(d / \{e_j\} \cup \{e_{j'}\})$ 
 \State $\delta = g(d') - g(d)$ 
 \If{$\delta > 0$ }
    $d \leftarrow d'$
 \EndIf
 \EndWhile
\end{algorithmic}
 \end{algorithm}
 
 \begin{algorithm}
\caption{Greedy search algorithm}
\begin{algorithmic}
 \State Let $d$ be a non-degenerate design of size $s < J'$
 \State Set $k = s$\;
 \While{$k < J'$}
 \ForAll{element $e_{j}\in D / d$}
    Calculate $g(d \cup \{e_{j}'\})$
 \EndFor
 \State Set $d \leftarrow d \cup \argmin_{e_j} g(d \cup \{e_j\})$ \;
 \State $k \leftarrow k + 1$
 \EndWhile
\end{algorithmic}
\end{algorithm}

 \begin{algorithm}
\caption{Reverse greedy search algorithm}
\begin{algorithmic}
 \State Set $d \gets D$
 \State Set $k = \vert D \vert$;
 \While{$k > J'$}
 \ForAll{element $e_{j}\in d$}
    Calculate $g(d / \{e_{j}'\})$
 \EndFor
 \State Set $d \leftarrow d / \argmin_{e_j} g(d / \{e_j\})$ \;
 \State $k \leftarrow k - 1$
 \EndWhile
\end{algorithmic}
\end{algorithm}

\subsubsection{Computation}
We use several methods to improve computational time to execute these algorithms. The most computationally expensive step is the calculation of $g(d)$ after adding or swapping an experimental condition, since the calculation of the information matrix requires inversion of the covariance matrix $\Sigma$ (see Equation (\ref{eq:infomat})). However, we can avoid inverting $\Sigma$ on each step by using rank-1 up- and down-dating.

For a design $d$ with $J'$ observations with inverse covariance matrix $\Sigma^{-1}_d$ we can obtain the inverse of the covariance matrix of the design with one observation removed $d' = d / \{i\}$, $\Sigma^{-1}_{d'}$ as follows. Without loss of generality we assume that the observation to be removed is the last row/column of $\Sigma^{-1}_d$. We can write $\Sigma^{-1}_d$ as 
\begin{equation*}
    \Sigma^{-1}_d = B = \begin{pmatrix}
     C & f \\
     f^T & e \\
    \end{pmatrix}
\end{equation*}
where $C$ is the $(J'-1) \times (J'-1)$ principal submatrix of $B$, $f$ is a column vector of length $(J'-1)$ and $e$ is a scalar. Then,
\begin{equation*}
     \Sigma^{-1}_{d / \{i\}} = G =  C - ff^T/e
\end{equation*}

For a design $d$ with $J'$ observations with inverse covariance matrix $\Sigma^{-1}_d$, we aim now to obtain the inverse covariance matrix of the design $d' = d \cup \{i'\}$. Recall that $Z$ is a $n \times Q$ design effect matrix with each row corresponding to an observation. We want to generate $H^{-1} = \Sigma_{d'}^{-1}$. Note that:
\begin{equation*}
    H = \Sigma_{d'} = \begin{pmatrix}
    G^{-1} & k \\
    k^T & h \\
    \end{pmatrix}
\end{equation*}
where $k = Z_{i \in d}DZ_{i'}$ is the column vector corresponding to the elements of $\Sigma = W^{-1} + ZDZ^T$ with rows in the current design and column corresponding to $i'$, and $h$ is the scalar $W^{-1}_{i',i'} + Z_{i'}DZ_{i'}^T$. Also now define:
\begin{equation*}
    H^* = \begin{pmatrix}
    \Sigma_d & 0 \\
    0 & h 
    \end{pmatrix}
\end{equation*}
so that 
\begin{equation*}
    H^{* -1} = \begin{pmatrix}
    \Sigma^{-1}_d & 0 \\
    0 & 1/h
    \end{pmatrix}
\end{equation*}
and 
\begin{equation*}
    H^{**} = \begin{pmatrix}
    \Sigma_d & k \\
    0 & h 
    \end{pmatrix}
\end{equation*}
and $u = (k^T, 0)^T$ and $v=(0,...,0,1)^T$, both of which are length $J'$ column vectors. So we can get $H^{**}$ from $H^*$ using a rank-1 update as $H^{**} = H^* + uv^T$ and similarly $H = H^{**} + vu^T$. Using the Sherman-Morison formula:
\begin{equation*}
    H^{**-1} = H^{* -1} - \frac{H^{* -1}uv^TH^{* -1}}{1+v^TH^{* -1}u}
\end{equation*}
and 
\begin{equation*}
    H^{-1} = H^{** -1} - \frac{H^{** -1}vu^TH^{** -1}}{1+u^TH^{**-1}v}
\end{equation*}
So we have calculated the updated inverse with only matrix-vector multiplication, which has complexity $O(n^2)$ rather than the $O(n^3)$ required if we just took a new inverse of matrix $\Sigma_{d'}$.

Other steps we include to improve efficiency are to check if any experimental conditions are repeated, if so then we can avoid checking a swap of an experimental condition for itself. Internally, the program only stores the unique combinations of rows of $X$ and $Z$ and tracks the counts of each in or out of the design. Finally, when the function is executed (see Section \ref{subsec:implementoptim}), a check is performed to determine whether the experimental conditions are all uncorrelated with one another. If they are uncorrelated then we can implement an alternative approach to calculating $g(d)$, since we can write the information matrix as:
\begin{equation}
\label{eq:infomatsum}
    M_d = \sum_{j = 1}^{J'} M_{e_j} = \sum_{j=1}^{J'}X_{i \in e_j}^T\Sigma^{-1}_{i \in e_j}X_{i \in e_j}
\end{equation}
where we use $X_{i \in e_j}$ to indicate the rows of $X$ in condition $e_j$, and $\Sigma^{-1}_{i \in e_j}$ the submatrix of $\Sigma^{-1}$ in $e_j$. Thus, rather than using a rank-1 up- or down-date procedure iterated over the observations in each experimental condition, we can calculate the `marginal' information matrix associated with each experimental condition and add or subtract it from $M_d$ as required. This method is generally faster as the number of observations per experimental condition is often much smaller than $J'$.

\subsection{Approximate Unit Weights}
\label{subsec:uncorexp}
If the experimental conditions are uncorrelated, we can use a different method to combinatorial optimisation. We assume that all the experimental conditions in $D$ are unique (the software automatically removes duplicates when using this method), and we place a probability measure over them $\phi = \{(\phi_j,X_{i \in e_j},\Sigma_{i \in e_j}): j=1,...,J, \phi_j \in [0,1]\}$ where $\sum_j \phi_j = 1$; $\phi$ is then an approximate design. The weights $\phi_j$ can be interpreted as the proportion of `effort' placed on each experimental condition. We can rewrite Equation (\ref{eq:infomatsum}) for the information matrix of this design as:
\begin{equation}
\label{eq:infomatsum2}
    M_\phi = \sum_{j=1}^{J}X_{i \in e_j}^T\Sigma^{-1}_{i \in e_j}X_{i \in e_j}\phi_j
\end{equation}
where the subscript $i\in e_j$ indicates the submatrix relating to the indexes in $e_j$. The problem is then to find the optimal design $\phi^* = \argmin_\phi c^T M_\phi c$.

\citet{Holland-Letz2011} and \citet{Sagnol2011} generalise Elfving's Theorem \citep{Elfving1952}, which provides a geometric characterisation of the c-optimal design problem in these terms, to the case of correlation within experimental conditions. \citet{Sagnol2011} shows that this problem can be written as a second order cone program, and as such be solved with interior point methods for conic optimisation problems. We include this program in the \pkg{glmmrOptim} package, which we implement using the \pkg{CVXR} package for conic optimisation. 

The approximate weighting approach returns the set of weights $\phi_1,...,\phi_J$ corresponding to each unique experimental condition. These weights need to be converted into an exact design with integer counts of each experimental condition $n_1,...,n_J$. The problem of allocating a fixed set of $J'$ items to $J$ categories according to a set of proportions is known as the apportionment problem. There a several rounding methods used for the apportionment problem proposed by the founding fathers of the United States for determining the number of representatives from each state. \citet{PUKELSHEIM1992} show that a modified version of Adams' method is most efficient when there needs to be at least one experimental condition of each type. In other cases the methods of Hamilton, Webster, or Jefferson may be preferred. We provide the function \code{apportion()}, which generates exact designs from all these methods for a desired sample size and set of weights. The output of this function is automatically provided when the approximate weighting method is used.

\subsection{Robust c-Optimal Designs}
The preceding discussion has assumed that the correct model specification is known. However, in many scenarios there may be multiple plausible models, and a design optimal for one model or set of parameters, may perform poorly for another. Robust optimal design methods aim to produce an (approximate) optimal design over multiple candidate models. We implement a method amendable to the combinatorial algorithms described in Section \ref{subsec:combinopt} following \citet{Dette1993}. Let $\mathcal{M}$ represent a GLMM model. We assume, following Dette REF, that the true model belongs to a class of GLMMs $\boldsymbol{\Xi} = \{\mathcal{M}_1,...,\mathcal{M}_R\}$ and we define a vector $\rho = \{\rho_1,...,\rho_R\}$, where $\rho_r \in [0,1]$ and $\sum_{r=1}^R \rho_r = 1$, which represents the prior weights or prior probabilities of each model. There are two robust c-optimality criteria we can use. The first:
\begin{equation*}
    h(d) = \sum_{r=1}^R \rho_r \log(c^T_r M^{-1}_{d,r} c_r)
\end{equation*}
was proposed by \citet{Dette1993} and \citet{Lauter1974}. The second is the weighted mean
\begin{equation*}
    h(d) = \sum_{r=1}^R \rho_r c^T_r M^{-1}_{d,r} c_r
\end{equation*}
Both criteria result in functions with the appropriate properties to ensure the local search algorithm maintains its theoretical guarantees. \citet{Dette1993} generalises Elfving's theorem to this robust criterion, however, further work is required to `doubly' generalise it to both correlated observations and robust criterion. While there may be a straightforward combination of the work of \citet{Dette1993} and that of \citet{Holland-Letz2011} and \citet{Sagnol2011}; translating this into a program for conic optimisation methods is a topic for future research.

\subsection{Implementation}
\label{subsec:implementoptim}
The \pkg{glmmrOptim} package adds an additional \code{DesignSpace} class, which references one or more \code{Model} objects. The \code{Model} objects must represent the same number of observations, with each row of $X$ and $Z$ representing an observation. The other arguments to initialise the class are (optionally) the weights on each design, units of each observation. The observations are assumed to be separate experimental conditions unless otherwise specified. 

The main member function of the \code{DesignSpace} class is \code{optimal}, which runs one of the  algorithms described above and returns the rows in the optimal design, or weights for each experimental unit, along with other relevant information. If the experimental conditions are uncorrelated with one another then the approximate weighting method is used by default; combinatorial algorithms can instead be used with the option \code{use\_combin=TRUE}. The user can run one or more of the combinatorial algorithms sequentially. The algorithms are numbered as 1 is the local search, 2 is the greedy search, and 3 is the reverse greedy search. Specifying \code{algo=1} will run the local search. Specifying, \code{algo=c(3,1)} will first run a reverse greedy search and then run a local search on the resulting design. We note that some combinations will be redundant, for example, running a greedy search after a reverse greedy search will have no effect since the resulting design will already be of size $J'$. However, some users may have interest in combining the approaches. A list (one per model in the design space) containing the vectors $c$ in Equation (\ref{eq:coptim}) must be provided. 

For certain problems, an optimal design may include all of the same value of one or more dichotomous covariates, which would result in a non-positive definite information matrix. For example, some models include adjustment for discrete time periods, but not all time periods feature in the subset of observations in the optimal design. The program checks that the information matrix is positive definite at each step, and if not, it reports which columns may be causing the failure. These columns can then be removed using the \code{rm\_cols} argument of the \code{optimal} function.

\subsection{Examples}
We present a set of examples relating to identifying an optimal cluster trial design within the design space represented by the top left panel of Figure \ref{fig:optim}. There are six clusters and five time periods, and we may observe a maximum of ten individuals per cluster-period. \citet{Girling2016} and \citet{Watson2023} consider similar design problems. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{optim_jss-page-001.jpg}
    \caption{Top left: Design space for the optimal experimental design examples. Each cell is a cluster-period in a cluster randomised trial with a maximum of ten observations. The numbers in the cells indicate intervention status. Top right: optimal design of 100 individual observations. Bottom left: optimal design of ten cluster-periods comprising ten observations. Bottom right: optimal design of two cluster sequences (rows).}
    \label{fig:optim}
\end{figure}

\subsubsection{Correlated Experimental Conditions with a Single Observation}
First, we consider the case when each observation is a separate experimental condition so we may observe any number of individuals in a time period. We will identify an approximate optimal design with 100 observations with a maximum of ten observations in a single cluster-period. As the experimental conditions are correlated we will use the local search algorithm (option \code{algo=1}). Given that the algorithm is not guaranteed to produce the optimal design, we run it ten times and select the design with lowest value of the objective function. Each run of the algorithm for this problem takes approximately 0.5s using the hardware specified in Table \ref{tab:simres1}. We will specify a Gaussian model with identity link function and exchangeable covariance function:
\begin{CodeChunk}
\begin{CodeInput}
#simulate data
df <- nelder(formula(~ (cl(6) * t(5)) > ind(10)))
df$int <- 0
df[df$t >= df$cl,'int'] <- 1
des <- Model$new(formula = ~factor(t) + int - 1+(1|gr(cl)) + (1|gr(cl,t)),
                 covariance = list(parameters = c(0.25,0.1)),
                 mean = list(parameters = rep(0,6)),
                 data = df,
                 family=gaussian())
ds <- DesignSpace$new(des) #create design space
opt <- ds$optimal(m=100,C=list(c(rep(0,5),1)),algo=1) #run the algorithm 
\end{CodeInput}
\end{CodeChunk}
The top right panel of Figure \ref{fig:optim} shows the approximate optimal design produced by the algorithm. The design is plotted with \pkg{ggplot2} using the following code; we can subset the data using the member function \code{subset\_rows()} of the \code{Model} class.
\begin{CodeChunk}
\begin{CodeInput}
des$subset_rows(opt$rows)
dfp <- aggregate(des$mean_function$data$cl,
                 list(des$mean_function$data$cl,des$mean_function$data$t),
                 length)
require(ggplot2)
ggplot(data=dfp,aes(x=Group.2,y=Group.1,fill=factor(x)))+
  geom_tile(color="white")+
  scale_fill_viridis_d(name="Cluster-\nperiod\ntotal",direction = -1)+
  theme_bw()+
  theme(panel.grid=element_blank())+
  labs(x="Time",y="Cluster")+
  scale_y_continuous(breaks=1:6)
\end{CodeInput}
\end{CodeChunk}

\subsubsection{Correlated Experimental Conditions with Multiple Observations}
We secondly consider the case where each cluster period is an experimental condition containing ten observations, and we aim to select a design of size ten cluster-periods.
\begin{CodeChunk}
\begin{CodeInput}
# update the experimental conditions
ds$experimental_condition <- rep(1:30, each = 5)
opt2 <- ds$optimal(m=10,C=list(c(rep(0,5),1)),algo=1) 
\end{CodeInput}
\end{CodeChunk}
The bottom left panel of Figure \ref{fig:optim} again shows the approximate optimal design produced by the algorithm, reflecting the `staircase' design from the previous example.

\subsubsection{Uncorrelated Experimental Conditions}
Finally, we conside the case where each whole cluster represents an experimental condition and we aim to pick two of these six clusters. In this example, the experimental conditions are uncorrelated. By default the \code{optim} function will use the second-order cone program and return optimal weights for each experimental condition. We can force the function to instead use the local or greedy search algorithms with the option \code{force\_hill=TRUE}:
\begin{CodeChunk}
\begin{CodeInput}
# update the experimental conditions
ds$experimental_condition <- df$cl
opt3 <- ds$optimal(m=2,C=list(c(rep(0,5),1)),algo=1,use_combin=TRUE) 
\end{CodeInput}
\end{CodeChunk}
Figure \ref{fig:optim} again shows the approximate optimal design produced by the algorithm. We note that a design containing rows 1 and 5 achieves the same variance. We can compare these results to the approximate optimal weights:
\begin{CodeChunk}
\begin{CodeInput}
w <- ds$optimal(m=2,C=list(c(rep(0,5),1)),algo=1)
w$weights 
0.2377032 0.1311486 0.1311482 0.1311482 0.1311486 0.2377032
\end{CodeInput}
\end{CodeChunk}

\section{Discussion}
In this article we describe the \pkg{glmmrBase} package for R that provides functionality for model specification, analysis, and fitting for GLMMs. We also describe the \pkg{glmmrOptim} package that extends its functionality. The number of packages in R alone that provide similar functionality attests to the growing popularity of GLMMs in statistical analyses. For example, we identified eight R packages for calculating power for cluster-randomised trials alone using similar models. Our intention with \pkg{glmmrBase} is to provide a broad set of tools and functions to support a wide range of GLMM related analyses, while maintaining a relatively simple interface through the \pkg{R6} class system. For an analysis like a power calculation, the package can provide a direct estimate, any of the intermediary steps for similar analyses, or related functionality like data simulation. The power analysis can also be applied to a design identified through the \pkg{glmmrOptim} package as classes in this package inherit the functions of the \pkg{glmmrBase} classes. This support is intended to reduce the requirement for multiple different packages for different model structures where comparisons may be required. We encourage users to request additional functions to add to the package.

The \pkg{glmmrBase} package provides MCML model fitting, along with a Laplace approximation. The package is not designed to provide a replacement for existing packages, like \pkg{lme4} or \pkg{glmmTMB}, which can often run in less time for the same model. Rather it is intended to provide a complementary, alternative approach to GLMM model fitting when the approximations provided by packages like \pkg{lme4} or \pkg{glmmTMB} may fail to produce reliable estimates or their algorithms may not converge. For simpler models, such as those with exchangable covariance functions or large numbers of observations, methods like PQL and Laplacian approximation can produce highly reliable estimates, and in a fraction of the time that MCML can. However, in more complex cases, as our illustrative examples show, there can be large differences. We encountered frequent failures to converge when simulating data for the given examples. More complex covariance structures are of growing interest to more accurately represent data generating processes. We have used the running example of a cluster randomised trial over multiple time periods, early modelling used a single group-membership random effect, whereas more contemporary approaches focus on temporal decay models, like autoregressive random effects \citep{Li2021}. However, \pkg{glmmTMB} is one of very few packages, prior to \pkg{glmmrMCML} to offer model fitting for this covariance structure.

We also discussed the \pkg{glmmrOptim} package, which provides algorithms for identifying approximate optimal experimental designs for studies with correlated observations. Again, this package is design to complement existing resources. The package was built to implement a range of relevant algorithms, and so provides the only software for c-optimal designs with correlated observations. However, for other types of optimality, including D-, A, or I-, other packages will be required, particularly \pkg{skpr}, although we are not aware of approaches that can handle these types of optimality yet for designs with correlated experimental conditions. One of the advantages of the integrated approach offered by our package is that one can quickly generate the design produced by the algorithm and interrogate it to calculate statistics like power, generate relevant matrices for subsequent analyses, and to simulate data.  

Planned future developments of the package include allowing for heteroskedastic models with specification of functions to describe the individual-level variance and adding more complex models such as hurdle models and multivariate models. We have also developed a further package that builds on \pkg{glmmrBase} to provide permutation-based inference building on the methods proposed in \citet{Watson2021b} and our R package \pkg{crctStepdown}. 

%\section*{Acknowledgments}
%We would like to thank ...

\bibliography{ref}

%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".



\end{document}