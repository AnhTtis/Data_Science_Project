% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
% Vd đầu phần sơ bộ đó em viết, this section presents background knowledge regrading ..... However, .... đoạn này viết là pp hiện tại ko dùng được cho cat features, muốn áp dụng được cần ... do đó phần 2.3 trình bày ...
% Tung
% Sau đó đến phần pp em vẽ một sơ đồ phương pháp của em ra, phần nào dùng cái dequantization này thì link nó vào
% Tung
% Phần related work em viết chưa đạt vì chưa so sánh với pp của em với pp mà em liệt kê ra
% Tung
% Mục đích viết phần related work là để làm rõ những hạn chế của các pp tồn tại và nó khác gì với pp của em
% Tung
% đây ko phải là bài báo review mà em list hết ra làm gì
% Tung
% Trong phần 4. Methodology ngay sau đó và trước phần 4.1 em phải trình bày về động lực chính dẫn đến idea của em, sơ đồ tổng quan phương pháp
% Tung
% Sau đó mới list ra từng phần làm gì và bắt đầu link đến các phần nhỏ tiếp theo
% Tung
% 2 cái hình 1 và 2 của em giờ nằm trơ trọi ko link được vào method ��
% Tung
% Viết phần general ideas trên thì em mới link được ideas với phương pháp rồi mô tả tổng quan phương pháp
% Tung
% sau đó mới đi vào chi tiết định nghĩa từng phần riêng biệt
% Tung
% trong định nghĩa từng phần sẽ mô tả kỹ làm những gì
% Tung
% áp dụng những pp hiện có nào và vì sao làm nó
% Tung
% Vì sao dùng cấu hình fixed cho thực nghiệm em cũng cần giải thích
% Tung
% Tung Thanh
% lấy các tham số đó từ paper nào hoặc vì sao em setting default như thế cũng phải nêu ra


\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath,amssymb}
\usepackage{caption}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{bbm}
\usepackage{amsmath} 
\usepackage{color}
\usepackage{lmodern} 
\usepackage{diagbox}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\definecolor{todocolor}{rgb}{0.9,0.1,0.1}
\definecolor{lcolor}{rgb}{0.7,0.7,0.3}
\definecolor{qcolor}{rgb}{0,0,1}

\newcommand{\nbc}[3]{
		{\colorbox{#3}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}
		{\textcolor{#3}{\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}}
}
\newcommand{\qli}[1]{\nbc{Qian}{#1}{qcolor}}
\newcommand{\duong}[1]{\nbc{Duong}{#1}{lcolor}}


% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
% \title{Contribution Title\thanks{Supported by organization x.}}
% \title{Generating counterfactual explanation with normalizing flow}

\title{CeFlow: A Robust and Efficient Counterfactual Explanation Framework for Tabular Data using Normalizing Flows}
%
\titlerunning{CeFlow: Counterfactual explanation with NF}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

\author{Tri Dung Duong\inst{1} \and
Qian Li\inst{2} \and
Guandong Xu\inst{1}\thanks{Corresponding author: Guandong.Xu@uts.edu.au}}
%
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Faculty of Engineering and Information Technology, University of Technology Sydney, NSW, Australia \and
School of Electrical Engineering, Computing and Mathematical Sciences,\\ Curtin University, WA, Australia
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% University of Technology Sydney, Sydney, Australia\\
% \email{\{abc,lncs\}@uni-heidelberg.de}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Counterfactual explanation is a form of interpretable machine learning that generates perturbations on a sample to achieve the desired outcome. The generated samples can act as instructions to guide end users on how to observe the desired results by altering samples. Although state-of-the-art counterfactual explanation methods are proposed to use variational autoencoder (VAE) to achieve promising improvements, they suffer from two major limitations: 1) the counterfactuals generation is prohibitively slow, which prevents algorithms from being deployed in interactive environments; 2) the counterfactual explanation algorithms produce unstable results due to the randomness in the sampling procedure of variational autoencoder. In this work, to address the above limitations, we design a robust and efficient counterfactual explanation framework, namely CeFlow, which utilizes normalizing flows for the mixed-type of continuous and categorical features. Numerical experiments demonstrate that our technique compares favorably to state-of-the-art methods. We release our source code\footnote{\url{https://github.com/tridungduong16/fairCE.git}} for reproducing the results.

% \qli{you didn't mention these works use variational autoencouder}. 





\keywords{Counterfactual explanation  \and Normalizing flow \and Interpretable machine learning.}
\end{abstract}
%
%
%
\section{Introduction}
Machine learning (ML) has resulted in advancements in a variety of scientific and technical fields, including computer vision, natural language processing, and conversational assistants.
% \qli{no need to discuss below content, these are common senses, directly move to interpretable advances}However, when ML models increasingly gain popularity in several critical decision-making scenarios such as criminal justice \cite{zavrvsnik2019algorithmic,kaur2020interpreting}, credit assessment \cite{galindo2000credit} or employment recruitment \cite{langenkamp2020hiring,pessach2020employees}, there is a critical need for models' interpretablity, accountability and reliability \cite{battaglia2018relational}. These properties plays a vital role in making machine learning models transparent and human understandable \cite{miller2019explanation} which is indispensable to model debugging or promote decision-making.
Interpretable machine learning is a machine learning sub-field that aims to provide a collection of tools, methodologies, and algorithms capable of producing high-quality explanations for machine learning model judgments. A great deal of methods in interpretable ML methods has been proposed in recent years. Among these approaches, counterfactual explanation (CE) is the prominent example-based method involved in how to alter features to change the model predictions and thus generates counterfactual samples for explaining and interpreting models \cite{mahajan2019preserving,artelt2020convex,grath_interpretable_2018,wachter2017counterfactual,xu2020causality}.
An example is that for a customer \texttt{A} rejected by a loan application, counterfactual explanation algorithms aim to generate counterfactual samples such as ``your loan would have been approved if your income was \$51,000 more'' which can act as a recommendation for a person to achieve the desired outcome. Providing counterfactual samples for black-box models has the capability to facilitate human-machine interaction, thus promoting the application of ML models in several fields. 

The recent studies in counterfactual explanation utilize variational autoencoder (VAE) as a generative model to generate counterfactual sample \cite{pawelczyk2020learning,mahajan2019preserving}. Specifically, the authors first build an encoder and decoder model from the training data. Thereafter, the original input would go through the encoder model to obtain the latent representation. They make the perturbation into this representation and pass the perturbed vector to the encoder until getting the desired output. However, these approaches present some limitations. First, the latent representation which is sampled from the encoder model would be changed corresponding to different sampling times, leading to unstable counterfactual samples. Thus, the counterfactual explanation algorithm is not robust when deployed in real applications. Second, the process of making perturbation into latent representation is so prohibitively slow \cite{mahajan2019preserving} since they need to add random vectors to the latent vector repeatedly; accordingly, the running time of algorithms grows significantly. Finally, the generated counterfactual samples are not closely connected to the density region, making generated explanations infeasible and non-actionable.  
To address all of these limitations, we propose a Flow-based counterfactual explanation framework (CeFlow) that integrates normalizing flow which is an invertible neural network as the generative model to generate counterfactual samples. Our contributions can be summarized as follows:
\begin{itemize}
    \item We introduce CeFlow, an efficient and robust counterfactual explanation framework that leverages the power of normalizing flows in modeling data distributions to generate counterfactual samples. The usage of flow-based models enables to produce more robust counterfactual samples and reduce the algorithm running time. 
    
    % The usage of flow-based models allow to gene
    % To the best of our knowledge, this is the first work exploring the usage of flow-based\qli{why it is helpful} models in generating counterfactual samples for tabular data.

    \item We construct a conditional normalizing flow model that can deal with tabular data consisting of continuous and categorical features by utilizing variational dequantization and Gaussian mixture models. 
    
    % This allows to compute the distance easily without the need of defining the distance function for the input space. 
    % \qli{what is dequantization? didn't mention it in the intro why it is helpful}. 
    % This allows to compute the distance easily without the need of defining the distance function for the input space.  
    \item The generated samples from CeFlow are close to and related to high-density regions of other data points with the desired class. This makes counterfactual samples likely reachable and therefore naturally follow the distribution of the dataset. 
    
    % meeting the most essential properties in the counterfactuals literature.
    
%     into the closest parts of high density regions of the other classes.

\end{itemize}

% \begin{table}[]
% \begin{tabular}{@{}cccc@{}}
% \toprule
% Method        & Fast generation & Continuous and categorical features & Stability \\ \midrule
% ECINN         & yes             & no                                  & yes       \\
% CCHVAE        & no              & yes                                 & no        \\
% CeFlow (Ours) & yes             & yes                                 & yes       \\ \bottomrule
% \end{tabular}
% \caption{Comparison of the properties of different methods.}
% \label{tab:my-table}
% \end{table}


% \begin{figure*}[!htb]
% \centering
% \includegraphics[width=0.8\textwidth]{figure/scheme.pdf}
% \caption{The scheme of counterfactual explanation algorithm with normalizing flows.}
% \label{fig:lambda}
% \end{figure*}


% Faithful counterfactuals. The generated counterfactuals
% are proximate and connected to regions of high data density
% and therefore likely attainable, addressing the most important desiderata in the literature on counterfactuals [10, 21];
% • Suitable for tabular data and classifier agnostic. The
% data distribution is modelled by an autoencoder that handles
% heterogeneous data and interval constraints by choosing
% appropriate likelihood models. It can also be combined with
% a multitude of autoencoder architectures [7, 8, 12, 13, 17, 19];
% • No ad-hoc distance measures for input data. The CCHVAE does not require ad-hoc predefined distance measures for input data to generate counterfactuals. This is can be an advantage over existing work, since it can be difficult to devise meaningful distance measures for tabular data.

\section{Related works}
An increasing number of methods have been proposed for the counterfactual explanation. The existing methods can be categorized into gradient-based methods \cite{wachter2017counterfactual,mothilal2020explaining}, auto-encoder model \cite{mahajan2019preserving}, heuristic search methods \cite{poyiadzi2020face,sharma2020certifai} and integer linear optimization \cite{kanamori2020dace}. Regarding gradient-based methods, The authors in the study construct the cross-entropy loss between the desired class and counterfactual samples' prediction with the purpose of changing the model output. The created loss would then be minimized using gradient-descent optimization methods. In terms of auto-encoder model, generative models such as variational auto-encoder (VAE) is used to generate new samples in another line of research. The authors \cite{pawelczyk2020learning} first construct an encoder-decoder architecture. They then utilize the encoder to generate the latent representation, make some changes to it, and run it through the decoder until the prediction models achieve the goal class. However, VAE models which maximize the lower bound of the log-likelihood instead of measuring exact log-likelihood can produce unstable and unreliable results. On the other hand, there is an increasing number of counterfactual explanation methods based on heuristic search to select the best counterfactual samples such as Nelder-Mead \cite{grath2018interpretable}, growing spheres \cite{laugel2018comparison}, FISTA \cite{dhurandhar2019model,van2019interpretable}, or genetic algorithms \cite{dandl2020multi,lash2017generalized}. Finally, the studies \cite{ustun2019actionable} propose to formulate the problem of finding counterfactual samples as a mixed-integer linear optimization problem and utilize some existing solvers \cite{bliek1u2014solving,artelt2020convex} to obtain the optimal solution.




%%% need to be paraphrased
% \textbf{Adversarial Attacks}
% CF examples are also related to adversarial attacks\cite{chakraborty2018adversarial}: they both represent instances obtained from minimal perturbations to the input, which induce changes in the prediction made by the learned model. One difference between the two is in the intent: adversarial examples are meant to fool the model, while CF examples are meant to explain the prediction (Freiesleben, 2021; Lucic et al., 2022).



% This approach draws much attention with a plethora of studies \cite{grath2018interpretable,dhurandhar2018explanations,mothilal2020explaining,mothilal2020explaining} that aim to customize the loss function to enhance the properties of counterfactual generation. For example, the study~\cite{grath_interpretable_2018} extends the distance functions in Eq~\eqref{eqn:original} by using a weight vector ($\Theta$) to emphasize the importance of each feature. Some algorithms such as $k$-nearest neighbors or global feature evaluation can be deployed to find this vector ($\Theta$). DiCE \cite{mothilal2020explaining} is another approach prosoing that diversity score be used to generate a greater number of generated samples, providing consumers more feasible options. They use the weighted sum to integrate several loss functions and the gradient-descent approach to approximate the ideal solution. The class prototype is used in the research \cite{van2019interpretable} to guide the search progress into the expected class distribution. The differentiable methods are the prominent approach in counterfactual explanation that allows to optimize easily and control the loss functions, but are only restricted to the differentiable models, and finds it hard to deal with the non-continuous values in tabular data. 

% the counterfactual samples is generated by obtaining the counterfactual instances' predictions, and there by minimizing the loss between the desired class and the counterfactual instances' prediction. 

% \textbf{Auto-encoder model:}
% Generative models such as variational auto-encoder (VAE) is used to generate new samples in another line of research. The authors \cite{pawelczyk2020learning} first construct an encoder-decoder architecture. They then utilise the encoder to generate the latent representation, make some changes to it, and run it through the decoder until the prediction models achieve the goal class.
% % Meanwhile, another line of recent work \cite{mahajan2019preserving} proposes the conditional auto-encoder model by combining different loss functions including prediction loss and proximity loss. They thereafter generate multiple counterfactual samples for all input data points by conditioning on the target class. 
% % These studies heavily rely on gradient-descent optimization which can face difficulties when handling categorical features. Moreover, 
% VAE models which maximize the lower bound of the log-likelihood instead of measuring exact log-likelihood can produce unstable and unreliable results. 


% Maximize the lower bound of the log-likelihood
% The study utilizes variational auto-encoder architecture to generate the counterfactual samples. Particularly, they make some perturbation until the decoder model generate the sample satisfying the expected target variable. Another approach to conditional generative adversarial nets (GAN) to generate the counterfactual samples. This method is also based on gradient descent optimization which face difficulties when handling categorical features. 

% Black-box approaches use gradient-free optimiza-
% tion algorithms such as Nelder-Mead [56], growing spheres [74],
% FISTA [30, 108], or genetic algorithms [26, 72, 98] to solve the
% optimization problem. Finally, some approaches do not cast the
% goal into an optimization problem and solve it using heuris-
% tics [57, 67, 91, 113]. Poyiadzi et al. [89] propose FACE, which
% uses Dijkstra’s algorithm [31] to find the shortest path between
% existing training datapoints to find counterfactual for a given
% input. Hence, this method does not generate new datapoints.
% \textbf{Heuristic search methods:} There is an increasing number of counterfactual explanation methods based on evolutionary algorithms to select the best counterfactual samples such as Nelder-Mead \cite{grath2018interpretable}, growing spheres \cite{laugel2018comparison}, FISTA \cite{dhurandhar2019model,van2019interpretable}, or genetic algorithms \cite{dandl2020multi,lash2017generalized,sharma2020certifai}.

% For example, CERTIFAI \cite{sharma2020certifai} customizes the genetic algorithm for the counterfactuals search progress, while FACE is another technique that uses Dijsstra's algorithm to produce counterfactual samples. 



% There is an increasing number of counterfactual explanation methods for non-differentiable models, which makes the previous gradient-based approach not applicable. The primary reasoning behind these techniques is to use evolutionary algorithms to select the best counterfactual samples based on the defined cost function. 




% by determining the shortest path between the original input and the current data points.

% \textbf{Heuristic search methods:} There is an increasing number of counterfactual explanation methods for non-differentiable models, which makes the previous gradient-based approach not applicable. The primary reasoning behind these techniques is to use evolutionary algorithms to select the best counterfactual samples based on the defined cost function. There are several heuristic search algorithm that has been proposed such as Nelder-Mead \cite{grath2018interpretable}, growing spheres \cite{laugel2018comparison}, FISTA \cite{dhurandhar2019model,van2019interpretable}, or genetic algorithms \cite{dandl2020multi,lash2017generalized,sharma2020certifai}. For example, CERTIFAI \cite{sharma2020certifai} customizes the genetic algorithm for the counterfactuals search progress. CERTIFAI adopts the indicator functions (1 for different values, else 0) for categorical features and mean squared error for continuous ones. On the top of that, the article \cite{poyiadzi2020face} proposes FACE, a technique that uses Dijsstra's algorithm to produce counterfactual samples by determining the shortest path between the original input and the current data points. The main advantage of FACE is that the path generated by Dijsstra's algorithm provides insight into the step-by-step and practical measures they might take to attain their objectives. The produced samples of this approach are confined to the input space. 


% \textbf{Integer linear optimization} The studies \cite{ustun2019actionable,cui2015optimal} propose to formulate the problem of finding counterfactual samples according to the cost function as a mixed-integer linear optimization problem and then utilize some existing solvers \cite{bliek1u2014solving,artelt2020convex} to obtain the optimal solution.

% These approaches have an advantage when dealing with non-continuous features and non-differentiable functions; they are however limited to linear models only. 


% To speed up the counterfactual samples search process, the study \cite{artelt2020convex} introduces convex constraints to bound the solutions in a region of data space locally. These approaches have an advantage when dealing with non-continuous features and non-differentiable functions; they are however limited to linear models only. 

\section{Preliminaries}

Throughout the paper, lower-cased letters $x$ and $\boldsymbol{x}$ denote the deterministic scalars and vectors, respectively. We consider a classifier $\mathcal{H}: \mathcal{X} \rightarrow \mathcal{Y}$ that has the input of feature space $\mathcal{X}$ and the output as $\mathcal{Y} = \{1 ... \mathcal{C}\}$ with $\mathcal{C}$ classes. Meanwhile, we denote a dataset $\mathcal{D} = \{\boldsymbol{x}_n, y_n\}^N_{n=1}$ consisting of $N$ instances where $\boldsymbol{x}_n \in \mathcal{X}$ is a sample, $y_n \in \mathcal{Y}$ is the predicted label of individuals $\boldsymbol{x}_n$ from the classifier $\mathcal{H}$. Moreover, $f_\theta$ is denoted for a normalizing flow model parameterized by $\theta$. Finally, we split the feature space into two disjoint feature subspaces of categorical features and continuous features represented by $\mathcal{X}^\text{cat}$ and $\mathcal{X}^\text{con}$ respectively such that $\mathcal{X} = \mathcal{X}_\text{cat} \times \mathcal{X}_\text{con}$ and $\boldsymbol{x}=(\boldsymbol{x}^\text{cat}, \boldsymbol{x}^\text{con})$, and $\boldsymbol{x}^{\text{cat}_j}$ and $\boldsymbol{x}^{\text{con}_j}$ is
the corresponding $j$-th feature of $\boldsymbol{x}^\text{cat}$ and $\boldsymbol{x}^\text{con}$.

% Next, we will provide the general objective of counterfactual explanation in section~\ref{cf:objective}. Thereafter, section~\ref{nf:objective} illustrates the fundamental knowledge about normalizing flows which would be applied in our methods. 

 

% \begin{definition}[Counterfactual Explanation]
\subsection{Counterfactual explanation}
\label{cf:objective}

With the original sample $\boldsymbol{x}_\text{org} \in \mathcal{X}$ and its predicted output $y_\text{org} \in \mathcal{Y}$, the counterfactual explanation aims to find the nearest counterfactual sample $\boldsymbol{x}_\text{cf}$ such that the outcome of classifier for $\boldsymbol{x}_\text{cf}$ is changed to desired output class $y_\text{cf}$. We aim to identify the perturbation $\boldsymbol{\delta}$ such that counterfactual instance $\boldsymbol{x}_\text{cf} = \boldsymbol{x}_\text{org} + \boldsymbol{\delta}$ is the solution of the following optimization problem:


% In general, the counterfactual explanation $\boldsymbol{x}_\text{cf}$ for the individual $\boldsymbol{x}_\text{org}$ is the solution of the following optimization problem:

\begin{equation}
\label{eqn:original}
\small 
\boldsymbol{x}_\text{cf} = \argmin_{\boldsymbol{x}_\text{cf} \in \mathcal{X}} d(\boldsymbol{x}_\text{cf}, \boldsymbol{x}_\text{org}) \quad\text{subject to}\quad \mathcal{H}(\boldsymbol{x}_\text{cf}) = y_\text{cf}
\end{equation}
where $d(\boldsymbol{x}_\text{cf}, \boldsymbol{x}_\text{org})$ is the function measuring the distance between $\boldsymbol{x}_\text{org}$ and $\boldsymbol{x}_\text{cf}$. Eq~\eqref{eqn:original} demonstrates the optimization objective that minimizes the similarity of the counterfactual and original samples, as well as ensures to change the classifier to the desirable outputs. To make the counterfactual explanations plausible, they should only suggest minimal changes in features of the original sample. \cite{mothilal2020explaining}. 




% To make it clear, we consider a simple scenario that a person with a set of features \{income: \$50k, CreditScore: ``good'', 
%\qian{``good''}
% education: ``bachelor''
%\qian{``bachelor''}
% , age: 52\} applies for a loan in a financial organization and receives the reject decision from a predictive model. In this case, the company can utilize the counterfactual explanation (CF) as an advisor that provides constructive advice for this customer. To allow this customer successfully get the loan, CF can give an advice that how to change the customer's profile such as increasing his/her income to \$51k, or enhancing the education degree to ``Master''.
% \qian{correct the usage of the ""or '' across the paper by using `` xx''.}\duong{done} 
% This toy example illustrates that CF is capable of providing interpretable advice that how to makes the least changes for the sample to achieve the desired outcome. 

% \subsection{Counterfactual explanation}
% Consider the original sample $x_0 = (x_0^1, \cdots, x_0^D) \in \mathcal{X}$, and original model prediction $y_0 \in \mathcal{Y}$, the goal of counterfactual explanation algorithms is to identify the closest sample $x_{cf}$ such that the classifier's output class changes to the intended output class $y_cf$. In general, the counterfactual explanation $x_{cf}$ for the individual $x_0$ is the solution of the following optimization problem:

% \begin{equation}
% \small 
% \label{eqn:original}
% x_{cf}^{*} = \argmin_{x_{cf} \in \mathcal{X}} f(x_{cf}) \quad\text{subject to}\quad h(x_{cf}^*) = y_{cf}
% \end{equation}
% % \end{definition}
% where $f(x_{cf})$ is the distance function that measures the closeness of $x_0$ and $x_{cf}$. Eq~\eqref{eqn:original} demonstrates the optimization objective that minimizes the similarity of the counterfactual and original samples, as well as encourage the classifier to change its decision output. It means that small changes in a few features provide more understandable and plausible explanations.



\subsection{Normalizing flow}
\label{nf:objective}
% Flow-based generative models (Rezende & Mohamed, 2015; Dinh et al., 2015;
% 2016; Kingma & Dhariwal, 2018) provide an attractive framework for transforming any probability distribution q into another distribution q¯.

Normalizing flows (NF) \cite{dinh2014nice} is the active research direction in generative models that aims at modeling the probability distribution of a given dataset. The study \cite{dinh2016density} first proposes a normalizing flow, which is an unsupervised density estimation model described as an invertible mapping $f_\theta: \mathcal{X} \rightarrow \mathcal{Z}$ from the data space $\mathcal{X}$ to the latent space $\mathcal{Z}$. Function $f_\theta$ can be designed as a neural network parametrized by $\theta$ with architecture that has to ensure invertibility and efficient computation of log-determinants. The data distribution is modeled as a transformation $f_\theta^{-1}: \mathcal{Z} \rightarrow \mathcal{X}$ applied to a random variable from the latent distribution $\boldsymbol{z} \sim p_{\mathcal{Z}}$, for which Gaussian distribution is chosen. The change of variables formula gives the density of the converted random variable $\boldsymbol{x}=f_\theta^{-1}(\boldsymbol{z})$ as follows:
\begin{equation}
\label{normalizingflowobjective0}
\small
\begin{aligned}
    p_{\mathcal{X}}(\boldsymbol{x}) & =p_{\mathcal{Z}}(f_\theta(\boldsymbol{x})) \cdot\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}}\right)\right|\\
         & \propto \log \left(p_{\mathcal{Z}}(f_\theta(\boldsymbol{x}))\right)  + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}}\right)\right|\right)
\end{aligned}
\end{equation}
With $N$ training data points $\mathcal{D} = \{\boldsymbol{x}_n\}_{n=1}^N$, the model with respects to parameters $\theta$ can be trained by maximizing the likelihood in Equation \eqref{loglikelihood}:
\begin{equation}
\small 
\label{loglikelihood}
   \theta = \argmax_{\theta} \left( \prod_{n=1}^N \left(\log (p_{\mathcal{Z}}(f_\theta(\boldsymbol{x}_n))) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta(\boldsymbol{x}_n)}{\partial \boldsymbol{x}_n}\right)\right|\right) \right) \right)
\end{equation}
% \subsection{Coupling layer}
% \label{coupling}
% A number of flow architectures \cite{dinh2014nice,dinh2016density,kingma2018glow} have been proposed recently. One of the most prominent architecture is Real NVP \cite{dinh2016density} which is based on an affine coupling layer. This layer splits an input variable $\boldsymbol{x}$ into two non-overlapping parts $\boldsymbol{x}_1$; $\boldsymbol{x}_2$ and applies affine transformation based on the first part $\boldsymbol{x}_1$ to the other $\boldsymbol{x}_2$ in the following way:

% \begin{equation}
%     \boldsymbol{z}_1 = \boldsymbol{x}_1, \; \boldsymbol{z}_2 = \boldsymbol{x}_2 \odot \text{exp}(s(\boldsymbol{x}_1)) + t(\boldsymbol{x}_1)
% \end{equation}

% where $s$ and $t$ are arbitrary neural networks. This transformation has a simple triangular Jacobian, and the use of neural networks allows to model complex non-linear dependencies.

% % Recent progress in the design of flow models has involved carefully constructing flows to increase their expressiveness while preserving tractability of the inverse and Jacobian determinant computations. One example is the invertible $1 \times 1$ convolution flow, whose inverse and Jacobian determinant can be calculated and differentiated with standard automatic differentiation libraries (Kingma \& Dhariwal, 2018). Another example, which we build upon in our work here, is the affine coupling layer (Dinh et al., 2016). It is a parameterized flow $\mathbf{y}=f_{\theta}(\mathbf{x})$ that first splits the components of $\mathbf{x}$ into two parts $\mathbf{x}_{1}, \mathbf{x}_{2}$, and then computes $\mathbf{y}=\left(\mathbf{y}_{1}, \mathbf{y}_{2}\right)$, given by

% % \begin{equation}
% %     \mathbf{y}_{1}=\mathbf{x}_{1}, \quad \mathbf{y}_{2}=\mathbf{x}_{2} \cdot \exp \left(\mathbf{a}_{\theta}\left(\mathbf{x}_{1}\right)\right)+\mathbf{b}_{\theta}\left(\mathbf{x}_{1}\right)
% % \end{equation}


% % Here, $\mathbf{a}_{\theta}$ and $\mathbf{b}_{\theta}$ are outputs of a neural network that acts on $\mathbf{x}_{1}$ in a complex, expressive manner, but the resulting behavior on $\mathrm{x}_{2}$ always remains an elementwise affine transformation - effectively, $\mathbf{a}_{\theta}$ and $\mathbf{b}_{\theta}$ together form a dataparameterized family of invertible affine transformations. This allows the affine coupling layer to express complex dependencies on the data while keeping inversion and loglikelihood computation tractable. Using - and exp to respectively denote elementwise multiplication and exponentia- tion, the affine coupling layer is defined by:

% % \begin{equation}
% % \begin{aligned}
% % \mathbf{x}_{1} &=\mathbf{y}_{1}, \\
% % \mathbf{x}_{2} &=\left(\mathbf{y}_{2}-\mathbf{b}_{\theta}\left(\mathbf{y}_{1}\right)\right) \cdot \exp \left(-\mathbf{a}_{\theta}\left(\mathbf{y}_{1}\right)\right) \\
% % \log \left|\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right| &=\mathbf{1}^{\top} \mathbf{a}_{\theta}\left(\mathbf{x}_{1}\right)
% % \end{aligned}
% % \end{equation}


% % The splitting operation $\mathbf{x} \mapsto\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)$ and merging operation $\left(\mathbf{y}_{1}, \mathbf{y}_{2}\right) \mapsto \mathbf{y}$ are usually performed over channels or over space in a checkerboard-like pattern (Dinh et al., 2016).




\section{Methodology}
In this section, we illustrate our approach (CeFlow) which leverages the power of normalizing flow in generating counterfactuals. First, we define the general architecture of our framework in section~\ref{ps}. Thereafter, section~\ref{nfcat} and \ref{nfarch} illustrate how to train and build the architecture of the invertible function $f$ for tabular data, while section~\ref{nfperturb} describes how to produce the counterfactual samples by adding the perturbed vector into the latent representation. 

\subsection{General architecture of CeFlow}
\label{ps}

\begin{figure*}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{figure/framework.pdf}
\caption{Counterfactual explanation with normalizing flows (CeFlow).}
\label{fig:generalachitecture}
\end{figure*}

Figure~\ref{fig:generalachitecture} generally illustrates our framework. Let $\boldsymbol{x}_\text{org}$ be an original instance, and ${f}_\theta$ denote a pre-trained, invertible and differentiable normalizing flow model on the training data. In general, we first construct an invertible and differentiable function $f_\theta$ that converts the original instance $\boldsymbol{x}_\text{org}$ to the latent representation $\boldsymbol{z}_\text{org} = f(\boldsymbol{x}_\text{org})$. After that, we would find the scaled vector $\boldsymbol{\delta}_{z}$ as the perturbation and add to the latent representation $\boldsymbol{z}_\text{org}$ to get the perturbed representation ${\boldsymbol{z}}_\text{cf}$ which goes through the inverse function $f_\theta^{-1}$ to produce the counterfactual instance $\boldsymbol{x}_\text{cf}$. With the counterfactual instance $\boldsymbol{x}_\text{cf} = f_\theta^{-1}(\boldsymbol{z}_\text{org} + \boldsymbol{\delta}_{z})$, we can re-write the objective function Eq.~\eqref{eqn:original} into the following form:

\begin{equation}
\label{eqn:distance}
\small 
\begin{cases}
\boldsymbol{\delta}_{z} = \argmin_{\boldsymbol{\delta}_{z} \in \mathcal{Z}} d(\boldsymbol{x}_\text{org}, \boldsymbol{\delta}_{z}) \\ 
        \mathcal{H}(\boldsymbol{x}_\text{cf}) = y_\text{cf} 
\end{cases}
\end{equation}

% \begin{equation}
% \label{eqn:original}
% \boldsymbol{x}_\text{cf} = \argmin_{\boldsymbol{x}_\text{cf} \in \mathcal{X}} d(\boldsymbol{x}_\text{cf}, \boldsymbol{x}_\text{org}) \quad\text{subject to}\quad \mathcal{H}(\boldsymbol{x}_\text{cf}) = y_\text{cf}
% \end{equation}



% with 

% \begin{equation}
% \small 
% \boldsymbol{z} = f(\boldsymbol{x}_\text{org})
% \end{equation}

% where the invertible function $f$ convert the original instance $\boldsymbol{x}_\text{org}$ to the latent representation $\boldsymbol{z}$, and the inverse function $f^{-1}$ transform the perturbed latent one $\boldsymbol{z} + \boldsymbol{\delta}$ to the input space to obtain the counterfactual sample $\boldsymbol{x}_\text{cf}$. 

One of the biggest problems of deploying normalizing flow is how to handle mixed-type data which contains both continuous and categorical features. Categorical features are in discrete forms, which is challenging to model by the continuous distribution only \cite{ho2019flow++}. Another challenge is to construct the objective function to learn the conditional distribution on the predicted labels
\cite{winkler2019learning,izmailov2020semi}. In the next section, we will discuss how to construct the conditional normalizing flow $f_\theta$ for tabular data. 

% $\boldsymbol{x} = \{\boldsymbol{x}^\text{con}, \boldsymbol{x}^\text{cat}\}$. 


% features $\boldsymbol{x} = \{\boldsymbol{x}^\text{con}, \boldsymbol{x}^\text{cat}\}$ which consist of both continuous and categorical features.




% \subsection{Normalizing flows for continuous features}
% \label{nfcon}
% For the continuous feature $\mathcal{X}^\text{con}$, the simple NF can be used. An unsupervised density estimation model illustrates an invertible mapping $f: \mathcal{X}^\text{con} \rightarrow \mathcal{Z}$ from the data space $\mathcal{X}^\text{con}$ to the latent space $\mathcal{Z}$. The data distribution may be modelled as a transformation $f^{-1}: \mathcal{Z}^\text{con} \rightarrow \mathcal{X}^_\text{con}$ applied to a random variable from the latent distribution $z \sim p_{\mathcal{Z}}$. The change of variables formula gives the density of the converted random variable $\boldsymbol{x}^\text{con}=f^{-1}(\boldsymbol{z}^\text{con})$.

% \begin{equation}
% \small 
% \label{eq:logtransformation}
% \log p_{\mathcal{X}}(\boldsymbol{\boldsymbol{x}^\text{con}})=p_{\mathcal{Z}}(f(\boldsymbol{x}^\text{con})) + \log (\left|\operatorname{det}\left(\frac{\partial f}{\partial \boldsymbol{x}^\text{con}}\right)\right|)
% \end{equation}


\subsection{Normalizing flows for categorical features}
\label{nfcat}
This section would discuss how to handle the categorical features. Let $\{\boldsymbol{z}^{\text{cat}_m}\}_{m=1}^M$ be the continuous representation of $M$ categorical features $\{\boldsymbol{x}^{\text{cat}_m}\}_{m=1}^M$ for each $\boldsymbol{x}^{\text{cat}_m} \in \{0,1,...,K-1\}$ with $K > 1$.  Follow by several studies in the literature \cite{ho2019flow++,hoogeboom2020learning}, we utilize variational dequantization to model the categorical features. The key idea of variational dequantization is to add noise $\boldsymbol{u}$ to the discrete values $\boldsymbol{x}^{\text{cat}}$ to convert the discrete distribution $p_{\mathcal{X}^{\text{cat}}}$ into a continuous distribution $p_{\phi_\text{cat}}$. With $\boldsymbol{z}^{\text{cat}} = \boldsymbol{x}^{\text{cat}}+\boldsymbol{u}_k$, $\phi_\text{cat}$ and $\theta_\text{cat}$ be models' parameters, we have following objective functions:
\begin{equation} 
\small 
\label{loglikelihoodcate1}
\begin{split}
\log p_{\mathcal{X}^{\text{cat}}}(\boldsymbol{x}^{\text{cat}}) 
&\geq \int_{\boldsymbol{u}} \log \frac{p_{\phi_\text{cat}}(\boldsymbol{z}^{\text{cat}})}{q_{\theta_\text{cat}}(\boldsymbol{u}|\boldsymbol{x}^{\text{cat}})}du \\
% & \approx \frac{1}{K} \sum_{k=1}^K \log \frac{p_{\phi_\text{cat}}(\boldsymbol{x}^{\text{cat}} + \boldsymbol{u}_k)}{q_{\theta_\text{cat}}(\boldsymbol{u}_k|\boldsymbol{x}^{\text{cat}})} \\ 
& \approx \frac{1}{K} \sum_{k=1}^K \log \prod_{m=1}^M \frac{p_{\phi_\text{cat}}(\boldsymbol{x}^{\text{cat}_m} + \boldsymbol{u}_k)}{q_{\theta_\text{cat}}(\boldsymbol{u}_k|\boldsymbol{x}^{\text{cat}})} 
\end{split}
\end{equation}

Followed the study \cite{hoogeboom2020learning}, we choose Gaussian dequantization which is more powerful than the uniform dequantization as $q_{\theta_\text{cat}}(\boldsymbol{u}_k|\boldsymbol{x}^{\text{cat}}) = \text{sig} \left( \mathcal{N}\left(\boldsymbol{\mu}_{\theta_\text{cat}}, \boldsymbol{\Sigma}_{\theta_\text{cat}}\right)\right)$ with mean $\boldsymbol{\mu}_{\theta_\text{cat}}$, covariance $\boldsymbol{\Sigma}_{\theta_\text{cat}}$ and sigmoid function sig(·).


% The model can be trained on $N$ training data points $\mathcal{D} = \{\boldsymbol{x}_n^{\text{cat}_j}\}_{n=1}^N$ as follows:

% \begin{equation}
% \label{normalizingflowobjectivecategorical}
% \small 
% \begin{aligned}
%  \phi_\text{cat}^*, \theta_\text{cat}^* &= \argmax_{\phi_\text{cat}, \theta_\text{cat}} \prod_{n=1}^{N} \left( \frac{1}{K} \sum_{k=1}^K \log \frac{p_{\phi_\text{cat}}(\boldsymbol{x}^{\text{cat}} + \boldsymbol{u}_k)}{q_{\theta_\text{cat}}(\boldsymbol{u}_k|\boldsymbol{x}^{\text{cat}}_n)} \right)
% \end{aligned}
% \end{equation}


\subsection{Conditional Flow Gaussian Mixture Model for tabular data}
\label{nfarch}



The categorical features $\boldsymbol{x}^{\text{cat}}$ going through the variational dequantization would convert into continuous representation $\boldsymbol{z}^{\text{cat}}$. We then perform merge operation on continuous representation $\boldsymbol{z}^\text{cat}$ and continuous feature $\boldsymbol{x}^\text{con}$ to obtain values $\left(\boldsymbol{z}^\text{cat}, \boldsymbol{x}^\text{con}\right) \mapsto \boldsymbol{x}^\text{full}$. Thereafter, we apply flow Gaussian mixture model \cite{izmailov2020semi} which is a probabilistic generative model for training the invertible function $f_\theta$. For each predicted class label $y \in\{1. . .\mathcal{C}\}$, the latent space distribution $p_{\mathcal{Z}}$ conditioned on a label $k$ is the Gaussian distribution $\mathcal{N}\left(\boldsymbol{z}^\text{full} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)$ with mean $\boldsymbol{\mu}_{k}$ and covariance $\boldsymbol{\Sigma}_{k}$:
\begin{equation}
\small
p_{\mathcal{Z}}(\boldsymbol{z}^\text{full} \mid y=k)=\mathcal{N}\left(\boldsymbol{z}^\text{full} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)
\end{equation}
As a result, we can have the marginal distribution of $\boldsymbol{z}^\text{full}$:
\begin{equation}
\label{marginaldistributionz}
\small 
p_{\mathcal{Z}}(\boldsymbol{z}^\text{full})=\frac{1}{\mathcal{C}} \sum_{k=1}^{\mathcal{C}} \mathcal{N}\left(\boldsymbol{z}^\text{full} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)
\end{equation}
The density of the transformed random variable $\boldsymbol{x}^\text{full} = f_\theta^{-1}(\boldsymbol{z}^\text{full})$ is given by:
\begin{equation}
\small
\label{normalizingflowobjective}
    p_{\mathcal{X}}(\boldsymbol{x}^\text{full}) = \log \left(p_{\mathcal{Z}}(f_\theta(\boldsymbol{x}^\text{full}))\right)  + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}^\text{full}}\right)\right|\right)
\end{equation}
Eq.~\eqref{marginaldistributionz} and Eq.~\eqref{normalizingflowobjective} together lead to the likelihood for data as follows:
\begin{equation}
\label{loglikelihoodcontinuous}
\small 
p_{\mathcal{X}}(\boldsymbol{x}^\text{full} \mid y=k)=\log \left(\mathcal{N}\left(f_\theta(\boldsymbol{x}^\text{full}) \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}^\text{full}}\right)\right|\right)
\end{equation}
We can train the model by maximizing the joint likelihood of the categorical and continuous features on $N$ training data points $\mathcal{D} = \{(\boldsymbol{x}_n^{\text{con}}, \boldsymbol{x}_n^{\text{cat}})\}_{n=1}^N$ by combining Eq.~\eqref{loglikelihoodcate1} and Eq.~\eqref{loglikelihoodcontinuous}:
% \begin{equation}
%  p_{\mathcal{D}}\left(\mathcal{D} \mid \theta\right)=\prod_{n=1}^{N} \left( \prod_{\boldsymbol{x}_n^{\text{con}_i} \in \mathcal{X}^{\text{con}}} \log \left(\mathcal{N}\left(f_\theta(\boldsymbol{x}) \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}}\right)\right|\right)\right)
% \end{equation}
% \begin{equation}
%  \argmax_{\theta} = \prod_{n=1}^{N} \left( \prod_{\boldsymbol{x}_n^{\text{con}_i} \in \mathcal{X}^{\text{con}}} \log \left(\mathcal{N}\left(f_\theta(\boldsymbol{x}) \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}}\right)\right|\right)\right)
% \end{equation}
\begin{equation}
\label{normalizingflowobjective0}
\small 
\begin{aligned}
 \theta^*, \phi_\text{cat}^*, \theta_\text{cat}^* &= \argmax_{\theta, \phi_\text{cat}, \theta_\text{cat}} \prod_{n=1}^{N} \left( \prod_{\boldsymbol{x}_n^{\text{con}} \in \mathcal{X}^{\text{con}}} p_{\mathcal{X}}\left(\boldsymbol{x}_n^{\text{con}}\right) \prod_{\boldsymbol{x}_n^{\text{cat}} \in \mathcal{X}^{\text{cat}}} p_{\mathcal{X}}\left(\boldsymbol{x}_n^{\text{cat}}\right) \right) \\
 &= \argmax_{\theta, \phi_\text{cat}, \theta_\text{cat}} \prod_{n=1}^{N} \left( \log \left(\mathcal{N}\left(f_\theta(\boldsymbol{x}_n^\text{full}) \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}_n^\text{full}}\right)\right|\right) \right)
\end{aligned}
\end{equation}
% \begin{equation}
% \label{loglikelihood}
%   \theta = \argmax_{\theta} \left( \prod_{i=1}^N \left(\log (p_{\mathcal{Z}}(f_\theta(\boldsymbol{x}_i))) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta(\boldsymbol{x}_i)}{\partial \boldsymbol{x}_i}\right)\right|\right) \right) \right)
% \end{equation}
% \begin{equation}
% \label{loglikelihood}
%   \theta = \argmax_{\theta} \left( \prod_{i=1}^N \left(\log (p_{\mathcal{Z}}(f_\theta(\boldsymbol{x}_i))) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta(\boldsymbol{x}_i)}{\partial \boldsymbol{x}_i}\right)\right|\right) \right) \right)
% \end{equation}
% over the parameters $\theta$ of the bijective function $f_\theta$, which learns a density model. 
\subsection{Counterfactual generation step}
\label{nfperturb}
% ================================= 
% \newline
%%%%%%%%%%%%%%%%%%%%%%%5


In order to find counterfactual samples, the recent approaches \cite{mothilal2020explaining,wachter2017counterfactual} normally define the loss function and deploy some optimization algorithm such as gradient descent or heuristic search to find the perturbation. These approaches however demonstrates the prohibitively slow running time, which prevents from deploying in interactive environment\cite{holtgen2021deduce}. Therefore, inspired by the study \cite{hvilshoj2021ecinn}, we add the scaled vector as the perturbation from the original instance $\boldsymbol{x}_\text{org}$ to counterfactual one $\boldsymbol{x}_\text{cf}$. By Bayes’ rule, we notice that under a uniform prior distribution over labels $p(y=k) = \frac{1}{\mathcal{C}}$ for $\mathcal{C}$ classes, the log posterior probability becomes:
\begin{equation}
\small 
\label{logequation}
    \log p_\mathcal{X}(y=k|\boldsymbol{x})=\log\frac{p_\mathcal{X}(\boldsymbol{x}|y=k)}{\sum_{k=1}^{\mathcal{C}}p_\mathcal{X}(\boldsymbol{x}|y=k)} \propto \left| \left| f_{\theta}(\boldsymbol{x}) - \boldsymbol{\mu}_k\right|\right|^2
\end{equation}
We observed from Eq.~\eqref{logequation} that latent vector $\boldsymbol{z} = f_{\theta}(\boldsymbol{x})$ will be predicted from the class $y$ with the closest model mean $\boldsymbol{\mu}_k$. For each predicted class $k \in \{1...\mathcal{C}\}$, we denote $\mathcal{G}_k = \{ {\boldsymbol{x}_m, y_m}\}_{m=1}^M$ as a set of $M$ instances with the same predicted class as $y_m=k$. We define the mean latent vector $\boldsymbol{\mu}_k$ corresponding to each class $k$ such that:
\begin{equation}
\small
    \boldsymbol{\mu}_k = \frac{1}{M}\sum_{\boldsymbol{x}_m \in \mathcal{G}_k}{f_{\theta}(\boldsymbol{x}_m)}
    \label{meanvector}
\end{equation}
Therefore, the scaled vector that moves the latent vector $\boldsymbol{z}_\text{org}$ to the decision boundary from the original class $y_\text{org}$ to counterfactual class $y_\text{cf}$ is defined as:
\begin{equation}
\small
    \boldsymbol{\Delta}_{y_\text{org} \rightarrow y_\text{cf}} = \left| \boldsymbol{\mu}_{y_\text{org}}- \boldsymbol{\mu}_{y_\text{cf}} \right|
\end{equation}

The scaled vector $\boldsymbol{\Delta}_{y_\text{org} \rightarrow y_\text{cf}}$ is added to the original latent representation $\boldsymbol{z}_\text{cf} = f_\theta(\boldsymbol{x}_\text{org})$ to obtained the perturbed vector. The perturbed vector then goes through inverted function $f_\theta^{-1}$ to re-produce the counterfactual sample:

% Given a target class q and an input x, a counterfactual example ˆ x(q) is produced from the
% predicted class C(x) = p by adding a scaled version of Dp;q to the latent space embedding
% z = f(x) and inverting it through the INN,

\begin{equation}
\small 
\label{counterfactual}
    \boldsymbol{x}_\text{cf} = f_{\theta}^{-1}(f_{\theta}(\boldsymbol{x}_\text{org}) + \alpha\boldsymbol{\Delta}_{y_\text{org} \rightarrow y_\text{cf}})
\end{equation}

We note that the hyperparameter $\alpha$ needs to be optimized by searching in a range of values. The full algorithm is illustrated in Algorithm~\ref{alg:mulobj}. 


    \scalebox{0.96}{
    \begin{minipage}{\linewidth}
\begin{algorithm}[H]
\small
\caption{Counterfactual explanation flow (CeFlow)}
\label{alg:mulobj}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE An original sample $\boldsymbol{x}_\text{org}$ with its prediction $y_\text{org}$, desired class $y_\text{cf}$, a provided machine learning classifier $\mathcal{H}$ and encoder model $Q_{\phi}$.
\STATE Train the invertible function $f_\theta$ by maximizing the log-likelihood: 
$$
\begin{aligned}
 \theta^*, \phi_\text{cat}^*, \theta_\text{cat}^* &= \argmax_{\theta, \phi_\text{cat}, \theta_\text{cat}} \prod_{n=1}^{N} \left( \prod_{\boldsymbol{x}_n^{\text{con}} \in \mathcal{X}^{\text{con}}} p_{\mathcal{X}}\left(\boldsymbol{x}_n^{\text{con}}\right) \prod_{\boldsymbol{x}_n^{\text{cat}} \in \mathcal{X}^{\text{cat}}} p_{\mathcal{X}}\left(\boldsymbol{x}_n^{\text{cat}}\right) \right) \\
 &= \argmax_{\theta, \phi_\text{cat}, \theta_\text{cat}} \prod_{n=1}^{N} \left( \log \left(\mathcal{N}\left(f_\theta(\boldsymbol{x}_n^\text{full}) \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right) + \log \left(\left|\operatorname{det}\left(\frac{\partial f_\theta}{\partial \boldsymbol{x}_n^\text{full}}\right)\right|\right) \right)
\end{aligned}
$$
% \end{equation}
\STATE Compute mean latent vector $\boldsymbol{\mu}_{k}$ for each class $k$ by $\boldsymbol{\mu}_k = \frac{1}{M}\sum_{\boldsymbol{x}_m \in \mathcal{G}_k}{f(\boldsymbol{x}_m)}$.
\STATE Compute the scaled vector $\boldsymbol{\Delta}_{y_\text{org} \rightarrow y_\text{cf}} = \left| \boldsymbol{\mu}_{y_\text{org}}- \boldsymbol{\mu}_{y_\text{cf}} \right|$.
\STATE Find the optimal hyperparameter $\alpha$ by searching a range of values. 
% \qli{should explain in the text as well, what kind of grid search and citations}.
\STATE Compute $    \boldsymbol{x}_\text{cf} = f_{\theta}^{-1}(f_{\theta}(\boldsymbol{x}_\text{org}) + \alpha\boldsymbol{\Delta}_{y_\text{org} \rightarrow y_\text{cf}})
$.
\ENSURE $\boldsymbol{x}_\text{cf}$.
\end{algorithmic}
\end{algorithm}
\end{minipage}%
}


\section{Experiments}
% We evaluate CeFLow on three datasets to demonstrate that our method outperforms state-of-the-art ones. We show that
% CeFLow outperforms the baselines on tabular and text
% data. FlowGMM is also state-of-the-art as an end-to-end
% generative approach to semi-supervised image classification, conditioned on architecture. However, FlowGMM is
% constrained by the RealNVP architecture, and thus does not
% outperform the most powerful approaches in this setting,
% which involve discriminative classifiers.
% In all experiments, we use the RealNVP normalizing flow
% architecture. Throughout training, Gaussian mixture parameters are fixed: the means are initialized randomly from
% the standard normal distribution and the covariances are
% set to I. See Appendix B for further discussion on GMM
% initialization and training.


We run experiments on three datasets to show that our method outperforms state-of-the-art approaches. The specification of hardware for the experiment is Python 3.8.5 with 64-bit Red Hat, Intel(R) Xeon(R) Gold 6238R CPU @ 2.20GHz. We implement our algorithm by using Pytorch library and adopt the RealNVP architecture \cite{dinh2016density}. During training progress, Gaussian mixture parameters are fixed: the means are initialized randomly from the standard normal distribution and the covariances are
set to ${I}$. More details of implementation settings can be found in our code repository\footnote{\url{https://anonymous.4open.science/r/fairCE-538B}}.  

We evaluate our approach via three datasets: \texttt{Law} \cite{wightman1998lsac}, \texttt{Compas} \cite{larson2016we} and \texttt{Adult} \cite{Dua:2019}. \texttt{Law}\footnote{\url{http://www.seaphe.org/databases.php}}\cite{wightman1998lsac} dataset provides information of students with their features: their entrance exam scores (LSAT), grade-point average (GPA) and first-year average grade (FYA). \texttt{Compas}\footnote{\url{https://www.propublica.org}}\cite{larson2016we} dataset contains information about 6,167 prisoners who have features including gender, race and other attributes related to prior conviction and age. \texttt{Adult}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/adult}}\cite{Dua:2019} dataset is a real-world dataset consisting of both continuous and categorical features of a group of consumers who apply for a loan at a financial institution. 

We compare our proposed method (CeFlow) with several state-to-the-art methods including \text{Actionable Recourse (AR)} \cite{ustun2019actionable}, \text{Growing Sphere (GS)} \cite{laugel2017inverse}, \text{FACE} \cite{poyiadzi2020face}, \text{CERTIFAI} \cite{sharma2020certifai}, DiCE \cite{mothilal2020explaining} and \text{C-CHVAE} \cite{pawelczyk2020learning}. Particularly, we implement the CERTIFAI with library PyGAD\footnote{\url{https://github.com/ahmedfgad/GeneticAlgorithmPython}} and utilize the available source code\footnote{\url{https://github.com/divyat09/cf-feasibility}} for implementation of DiCE, while other approaches are implemented with Carla library \cite{pawelczyk2021carla}. Finally, we report the results of our proposed model on a variety of metrics 
including \text{success rate} (success), \text{${l}_1$-norm} ($l_1$), \text{categorical proximity} \cite{mothilal2020explaining}, \text{continuous proximity} \cite{mothilal2020explaining} and \text{mean log-density} \cite{artelt2020convex}. Note that for $l_1$-norm, we report mean and variance of \text{$l_1$-norm} corresponding to \text{$l_1$-mean} and \text{$l_1$-variance}. Lower \text{$l_1$-variance} aims to illustrate the algorithm's robustness. 


% \begin{itemize}
    % \item \texttt{Law} \cite{mahajan2019preserving} is a 10,000-record synthetic dataset with three features ($x_1$,$x_2$,$x_3$) and a binary output ($y$).
    % \item \texttt{Law}\footnote{\url{http://www.seaphe.org/databases.php}}\cite{wightman1998lsac} dataset provides information of students with their features: sex, race and their entrance exam scores (LSAT), grade-point average (GPA) and first year average grade (FYA). 
    
    % The main task is to determine which applicants will be accepted to the law program. 
    
    % \item \texttt{Compas}\footnote{\url{https://www.propublica.org}}\cite{larson2016we} dataset released by ProPublica contains information about 6,167 prisoners, and each individual has features including gender, race and other attributes related to prior conviction and age. 
    
    % The aim is to determine whether a prisoner would re-offend within two years of being freed.
    
    % \item \texttt{Adult}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/adult}}\cite{Dua:2019} dataset is a real-world dataset consisting of both continuous and categorical features of a group of consumers who apply for a loan at a financial institution. 
    
    % The basic goal is to evaluate if a person's annual income exceeds $\$50,000$.

% \end{itemize}





% \subsection{Baselines}

% Actionable Recourse (AR): Paper
% CCHVAE: Paper
% Contrastive Explanations Method (CEM): Paper
% Counterfactual Latent Uncertainty Explanations (CLUE): Paper
% CRUDS: Paper
% Diverse Counterfactual Explanations (DiCE): Paper
% Feasible and Actionable Counterfactual Explanations (FACE): Paper
% Growing Sphere (GS): Paper
% Revise: Paper
% Wachter: Paper
% FOCUS: Paper
% FeatureTweak: Paper


% We compare our proposed method (CeFlow) with two baselines, namely CERTIFAI and DiCE. To the best of our knowledge, there is not much similar work in this area and CERTIFAI and DiCE are two state-of-the-art approaches in the counterfactual explanation. 

% \textbf{Baselines}. We compare our proposed method (CeFlow) with several state-to-the-art methods including \text{Actionable Recourse (AR)} \cite{ustun2019actionable}, \text{Growing Sphere (GS)} \cite{laugel2017inverse}, \text{FACE} \cite{poyiadzi2020face}, \text{CERTIFAI} \cite{sharma2019certifai} and DiCE \cite{mothilal2020explaining}. Particularly, we implement the CERTIFAI with library PyGAD\footnote{\url{https://github.com/ahmedfgad/GeneticAlgorithmPython}} and utilize the available source code\footnote{\url{https://github.com/divyat09/cf-feasibility}} for implementation of DiCE, while other approaches is implemented with Carla library \cite{pawelczyk2021carla}. 



% \begin{itemize}
%     \item \textbf{Actionable Recourse (AR)} \cite{ustun2019actionable} generates a minimal feature perturbation to change the decision of linear model by utilizing some solver such as CPLEX and CBC to minimize a pre-defined cost function. 
%     % \item \textbf{Growing Sphere (GS)} \cite{laugel2017inverse} is a random search algorithm, which generates samples around the factual input point until the generated samples belongs to the counterfactual class The random samples are generated around a original instance $\boldsymbol{x}_\text{org}$ using growing hyperspheres. 

%     \item \textbf{Growing Sphere (GS)} \cite{laugel2017inverse} is a random search algorithm, which generates samples around the original instance $\boldsymbol{x}_\text{org}$ until the generated samples $\boldsymbol{x}_\text{cf}$ belong to the counterfactual class $y_\text{cf}$.
    
    
%     \item \textbf{FACE} \cite{poyiadzi2020face} generates a feasible and actionable counterfactual actions by using a shortest path algorithm.

%     \item \textbf{CERTIFAI} \cite{sharma2019certifai} utilizes genetic algorithm to finds the counterfactual samples more effectively. We implement the CERTIFAI with Python library PyGAD\footnote{\url{https://github.com/ahmedfgad/GeneticAlgorithmPython}}.
%     \item \textbf{DiCE} \cite{mothilal2020explaining} uses the gradient-descent algorithm to optimize the combined loss function which consists of proximity, diversity and sparsity ones together. We utilize the source code\footnote{\url{https://github.com/divyat09/cf-feasibility}} for implementation.
%     \item \textbf{C-CHVAE} \cite{pawelczyk2020learning} builds an encoder and decoder model and use the heuristic search to find the perturbation into the latent space. 
    
    
%     % is a random search algorithm, which generates samples around the factual input point until a point with a corresponding counterfactual class label was found. The random samples are generated around x using growing hyperspheres. For binary input dimensions, the method makes use of Bernoulli sampling. Immutable features are readily specified by excluding them from the search procedure.
%     % \item \textbf{CEM} \cite{dhurandhar2018explanations}
%     % \item \textbf{REVISE} \cite{joshi2019towards}
%     % \item \textbf{ProCE} \cite{duong2021prototype}
% \end{itemize}

% For all the experiments, we build two predictions model namely \textbf{1$^{\text{st}}$} \textbf{classifier} and \textbf{2$^{\text{nd}}$} \textbf{classifier}. The first model is a neural network with three hidden layers, while the second one has five hidden layers.


% \textbf{1$^{\text{st}}$} \textbf{classifier}
% \begin{itemize}
%     \item[$\bullet$] Hidden Layer 1(Number of features, 64), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Hidden Layer 2(64, 32), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Hidden Layer 3(32, 16), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Last Layer (16, Data size), Sigmoid
% \end{itemize}

% \textbf{2$^{\text{nd}}$} \textbf{classifier}
% \begin{itemize}
%     \item[$\bullet$] Hidden Layer 1(Number of features, 256), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Hidden Layer 1(Number of features, 128), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Hidden Layer 1(Number of features, 64), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Hidden Layer 2(64, 32), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Hidden Layer 3(32, 16), BatchNorm, Dropout(0.1), ReLU
%     \item[$\bullet$] Last Layer (16, Data size), Sigmoid
% \end{itemize}


% \textbf{Evaluation Metrics}. We report the results of our proposed model on a variety of metrics 
% including: \text{success rate} (success), \text{${l}_1$-norm} ($l_1$), \text{categorical proximity} \cite{mothilal2020explaining}, \text{continuous proximity} \cite{mothilal2020explaining} and \text{mean log-density} \cite{artelt2020convex}. 

% \begin{itemize}

% This section would illustrate metrics which are used for evaluation purposes. We sample a number of ${N}_\text{samp}$ factual samples and generate the counterfactual samples for them. $N_\text{cat}$ and $N_\text{con}$ are the number of categorical and continuous features, respectively, while $\mathbbm{1}(.)$ is the indicator function that returns 1 if the conditions are satisfied, otherwise returns 0. 

% \textbf{Target-class validity} (\%Tcv) \cite{mahajan2019preserving,poyiadzi2020face} measures the percentage of the counterfactual samples belonging to the desired class, evaluating how well the algorithm can produce valid samples. Particularly, \%Tcv is calculated as the ratio of the number of samples classified as the desired target and the number of factual samples. \qian{explain a little bit why the definition is about Target-class validity} \duong{done} Higher target-class validity is favorable, demonstrating that the algorithm can generate greater numbers of desirable counterfactuals.

% \item \textbf{Success rate} (success) \cite{mahajan2019preserving,poyiadzi2020face} evaluates how well the algorithm can produce valid samples. Particularly, success rate is calculated as the ratio of the number of samples belonging to the desired class and the number of factual samples. Higher target-class validity is favorable, demonstrating that the algorithm can generate greater numbers of counterfactual samples towards the desirable target variable.

% \begin{equation}
% \small 
% \text{success} = \sum_{i=0}^{n} \frac{\mathbbm{1} (\mathcal{H}(\boldsymbol{x}_\text{cf}) = y_\text{cf})}{{N}_\text{samp}}
% \end{equation}

% \item \textbf{${l}_1$-norm} ($l_1$) \cite{mahajan2019preserving,poyiadzi2020face} measures distances between vectors which is calculated as the sum of absolute difference of the components of the vectors. \textbf{$l_1$-norm} evaluates the similarity between the original instance and counterfactual instances. We also report mean and variance of \textbf{$l_1$-norm} corresponding to \textbf{$l_1$-mean} and \textbf{$l_1$-variance}. Lower \textbf{$l_1$-mean} is preferable, which demonstrates that there is only a few changes need to perform, while lower \textbf{$l_1$-variance} illustrates the algorithm robustness. 

% \begin{equation}
% \small 
% l_1 = \sum_{i=0}^{{N}_\text{samp}} ||\boldsymbol{x}_\text{cf} - \boldsymbol{x}_\text{org}||
% \end{equation}

% \item \textbf{Categorical proximity} \cite{mothilal2020explaining} measures the proximity for categorical features representing the total number of matches on the values of each category between $\boldsymbol{x}_\text{cf}$ and $\boldsymbol{x}_\text{org}$. Higher categorical proximity is better, implying that the counterfactual sample preserves the minimal changes from the original samples.

% \begin{equation}
% \small 
%     \text{Cat\_proximity} = 1- \sum_{i=0}^{{N}_\text{samp}} \sum_{j=0}^{{N_{cat}}} \mathbbm{1} (\boldsymbol{x}^j_\text{cf} \neq \boldsymbol{x}^j_\text{org})
% \end{equation}

% with $x_{cf}$ and $x$ are the counterfactual samples and original instance, respectively, $dist(x_{cf_i}, x_{cf_j})$ illustrates the distance between two generated counterfactual instances, $d$ is the number of input features, $k$ is the number of counterfactual samples to be generated. 


% \item \textbf{Continuous proximity} \cite{mothilal2020explaining} illustrates the proximity of the continuous features, which is calculated as the negative of $L_2$-norm distance between the continuous features in $\boldsymbol{x}_
% \text{cf}$ and $\boldsymbol{x}_\text{org}$. Higher continuous proximity is preferable, implying that the distance between the continuous features of $\boldsymbol{x}_\text{org}$ and $\boldsymbol{x}_\text{cf}$ should be as small as possible.

% \begin{equation}
% \small 
%     \text{Con\_proximity} = -\sum_{i=0}^{{N}_\text{samp}} \sum_{j=0}^{{N_{con}}} \norm{\boldsymbol{x}^j_\text{cf} - \boldsymbol{x}^j_\text{org}}^2_2
% \end{equation}

% \item \textbf{Mean log-density} \cite{artelt2020convex} measures how much data support counterfactual examples have from positively classified instances. Larger density is better which means that counterfactual instances should be closer to individuals with the desired labels.

% \end{itemize}


% Followed by the study \cite{artelt2020convex}, we use a kernel density estimator (KDE) to estimate densities from training data. With $k(\cdot\,,\cdot)$ denoted for a suitable kernel function, a kernel density estimator is defined as follows:

% \begin{equation}
% \small 
%     \hat{p}_\text{KDE}(\boldsymbol{x}_\text{cf}) = \sum k(\boldsymbol{x}_\text{cf}, \boldsymbol{x}_i)
% \end{equation}


% \textbf{Result}
\begin{table*}[!htb]
\caption{Performance of all methods on the classifier.
We compute $p$-value by conducting a paired $t$-test between our approach (CeFlow) and baselines with 100 repeated experiments for each metric. }
% \label{tab:result}
\begin{adjustbox}{width=0.7\columnwidth,center}
\begin{scriptsize}

\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Performance}}                                                                           & \multicolumn{3}{c}{\textbf{p-value}}                            \\ \cmidrule(l){3-6} \cmidrule(l){7-9}
    &                           & \textbf{success  } & \textbf{$l_1$-mean  } & \multicolumn{1}{c}{\textbf{$l_1$-var  }} & \textbf{log-density  } & \textbf{success} & \textbf{$l_1$} & \textbf{log-density }  \\ 
    \midrule
\multirow{7}{*}{\texttt{Law}}        & AR                               & 98.00                                  & 3.518                                  & 2.0e-03                           & -0.730                                  & 0.041                             & 0.020                           & 0.022                                  \\
                                  & GS                               & 100.00                              & 3.600                                  & 2.6e-03                           & -0.716                                  & 0.025                             & 0.048                           & 0.016                                  \\
                                  & FACE                             & 100.00                              & 3.435                                  & 2.0e-03                           & -0.701                                  & 0.029                             & 0.010                           & 0.017                                  \\
                                  & CERTIFAI                         & 100.00                              & 3.541                                  & 2.0e-03                           & -0.689                                  & 0.029                             & 0.017                           & 0.036                                  \\
                                  & DiCE                             & 94.00                               & \textbf{3.111}        & 2.0e-03                           & -0.721                                  & 0.018                             & 0.035                           & 0.048                                  \\
                                  & C-CHVAE                          & 100.00                               & 3.461                                  & 1.0e-03                           & -0.730                                  & 0.040                             & 0.037                           & 0.016                                  \\
                                  & CeFlow                           & \textbf{100.00}    & \text{3.228}          & \textbf{1.0e-05} & -\textbf{0.679}        & -                                 & -                               & -                                      \\ \midrule
\multirow{7}{*}{\texttt{Compas}}           & AR                               & 97.50                                  & 1.799                                  & 2.4e-03                           & -14.92                                  & 0.038                             & 0.034                           & 0.046                                  \\
                                  & GS                               & 100.00                              & 1.914                                  & 3.2e-03                           & -14.87                                  & 0.019                             & 0.043                           & 0.040                                  \\
                                  & FACE                             & 98.50                                & 1.800                                  & 4.8e-03                           & -15.59                                  & 0.036                             & 0.024                           & 0.035                                  \\
                                  & CERTIFAI                         & 100.00                              & 1.811                                  & 2.4e-03                           & -15.65                                  & 0.040                             & 0.048                           & 0.038                                  \\
                                  & DiCE                             & 95.50                               & 1.853                                  & 2.9e-03                           & -14.68                                  & 0.030                             & 0.029                           & 0.018                                  \\
                                  & C-CHVAE                          & 100.00                               & 1.878                                  & 1.1e-03                           & -13.97                                 & 0.026                             & 0.015                           & 0.027                                  \\
                                  & CeFlow                           & \textbf{100.00}    & \textbf{1.787}        & \textbf{1.8e-05} & -\textbf{13.62}        & -                                 & -                               & -                                      \\ \midrule
\multirow{7}{*}{\texttt{Adult}}           & AR                               & 100.00                               & 3.101                                  & 7.8e-03                           & -25.68                                  & 0.044                             & 0.037                           & 0.018                                  \\
                                  & GS                               & 100.00                              & 3.021                                  & 2.4e-03                           & -26.55                                  & 0.026                             & 0.049                           & 0.028                                  \\
                                  & FACE                             & 100.00                              & 2.991                                  & 6.6e-03                           & -23.57                                  & 0.027                             & 0.015                           & 0.028                                  \\
                                  & CERTIFAI                         & 93.00                               & 3.001                                  & 4.1e-03                           & -25.55                                  & 0.028                             & 0.022                           & 0.016                                  \\
                                  & DiCE                             & 96.00                               & 2.999                                  & 9.1e-03                           & -24.33                                  & 0.046                             & 0.045                           & 0.045                                  \\
                                  & C-CHVAE                          & 100.00                               & 3.001                                  & 8.7e-03                           & -24.45                                  & 0.026                             & 0.043                           & 0.019                                  \\
                                  & CeFlow                           & \textbf{100.00}    & \textbf{2.964}        & \textbf{1.5e-05} & -\textbf{23.46}        & -                             & -                           & -                                  \\ \bottomrule
\end{tabular}
\end{scriptsize}
\end{adjustbox}
\label{tab:result}
\end{table*}
% \begin{table*}[!htb]
% \begin{adjustbox}{width=0.36\columnwidth,center}
% \begin{tabular}{ccc}
% \toprule
% \textbf{\textbf{Method}} & \textbf{\textbf{Dataset}} & \textbf{\textbf{Running time (s)}} \\ \midrule
% AR                                        & \texttt{Law}              & 3.030  $\pm$    0.105                               \\
% GS                                        & \texttt{Law}              & 7.126    $\pm$  0.153                               \\
% FACE                                      & \texttt{Law}              & 6.213    $\pm$  0.007                               \\
% CERTIFAI                                  & \texttt{Law}              & 6.522    $\pm$  0.088                               \\
% DiCE                                      & \texttt{Law}              & 8.022    $\pm$  0.014                               \\
% C-CHVAE                                   & \texttt{Law}              & 9.022    $\pm$  0.066                               \\
% CeFlow                                    & \texttt{Law}              & \textbf{ 0.850    $\pm$  0.055 }   \\ \midrule
% AR                                        & \texttt{Compas}           & 5.125    $\pm$  0.097                               \\
% GS                                        & \texttt{Compas}           & 8.048    $\pm$  0.176                               \\
% FACE                                      & \texttt{Compas}           & 7.688    $\pm$  0.131                               \\
% CERTIFAI                                  & \texttt{Compas}           & 13.426    $\pm$  0.158                              \\
% DiCE                                      & \texttt{Compas}           & 7.810    $\pm$  0.076                               \\
% C-CHVAE                                   & \texttt{Compas}           & 10.922    $\pm$  0.044                               \\
% CeFlow                                    & \texttt{Compas}           & \textbf{0.809    $\pm$  0.162}     \\ \midrule
% AR                                        & \texttt{Adult}            & 7.046    $\pm$  0.151                               \\
% GS                                        & \texttt{Adult}            & 6.472    $\pm$  0.021                               \\
% FACE                                      & \texttt{Adult}            & 13.851    $\pm$  0.001                              \\
% CERTIFAI                                  & \texttt{Adult}            & 7.943    $\pm$  0.046                               \\
% DiCE                                      & \texttt{Adult}            & 11.821    $\pm$  0.162                              \\
% C-CHVAE                                   & \texttt{Adult}            & 10.922    $\pm$  0.024                              \\
% CeFlow                                    & \texttt{Adult}            & \textbf{0.837    $\pm$  0.026 }    \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% %%% note 
% \caption{We report running time of different methods on three datasets.}
% \label{tab:runningtime}
% \end{table*}


\begin{table*}[!htb]
\caption{We report running time of different methods on three datasets.}
\begin{adjustbox}{width=1\columnwidth,center}
\begin{scriptsize}
\begin{tabular}{@{}rrrrrrrr@{}}
\toprule
\multicolumn{1}{l}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{AR}}                    & \multicolumn{1}{c}{\textbf{GS}}                    & \multicolumn{1}{c}{\textbf{FACE}}                   & \multicolumn{1}{c}{\textbf{CERTIFAI}}                & \multicolumn{1}{c}{\textbf{DiCE}}                    & \multicolumn{1}{c}{\textbf{C-CHVAE}}                 & \multicolumn{1}{c}{\textbf{CeFlow}}                                          \\ \midrule
\multicolumn{1}{l}{\textbf{Law}}                           & 3.030  $\pm$    0.105 & 7.126    $\pm$  0.153 & 6.213    $\pm$  0.007  & 6.522    $\pm$  0.088  & 8.022    $\pm$  0.014  & 9.022    $\pm$  0.066  & \textbf{0.850 $\pm$  0.055} \\
\multicolumn{1}{l}{\textbf{Compas}}                                 & 5.125    $\pm$  0.097 & 8.048    $\pm$  0.176 & 7.688    $\pm$  0.131  & 13.426    $\pm$  0.158 & 7.810    $\pm$  0.076  & 6.879    $\pm$  0.044 & \textbf{0.809    $\pm$  0.162}   \\
\multicolumn{1}{l}{\textbf{Adult}}                                 & 7.046    $\pm$  0.151 & 6.472    $\pm$  0.021 & 13.851    $\pm$  0.001 & 7.943    $\pm$  0.046  & 11.821    $\pm$  0.162 & 12.132    $\pm$  0.024 & \textbf{0.837    $\pm$  0.026}  \\ \bottomrule
\end{tabular}
\end{scriptsize}
\end{adjustbox}
%%% note 
\label{tab:runningtime}
\end{table*}

% \begin{figure}[!ht]
% \centerline{\includegraphics[width=0.6\textwidth]{figure/proximity.pdf}}
% \caption{Baseline results in terms of \textbf{Categorical proximity} and \textbf{Continuous proximity}. Higher continuous and categorical proximity are better.}
% \label{fig:proximity}
% \end{figure}

\begin{figure}[!ht]
\centerline{\includegraphics[width=0.8\textwidth]{figure/proximity_ho.pdf}}
\caption{Baseline results in terms of \textbf{Categorical proximity} and \textbf{Continuous proximity}. Higher continuous and categorical proximity are better.}
\label{fig:proximity}
\end{figure}

The performance of different approaches regarding three metrics: $l_1$, success metrics and log-density are illustrated in Table~\ref{tab:result}. Regarding success rate, all three methods achieve competitive results, except the AR, DiCE and CERTIFAI performance in all datasets with around 90\% of samples belonging to the target class. These results indicate that by integrating normalizing flows into counterfactuals generation, our proposed method can achieve the target of counterfactual explanation task for changing the models' decision. Apart from that, for $l_1$-mean, CeFlow is ranked second with 3.228 for \texttt{Law}, and is ranked first for \texttt{Compas} and \texttt{Adult} (1.787 and 2.964). Moreover, our proposed method generally achieves the best performance regarding $l_1$-variance on three datasets. CeFlow also demonstrates the lowest log-density metric in comparison with other approaches achieving at -0.679, -13.62 and -23.46 corresponding to \texttt{Law}, \texttt{Compas} and \texttt{Adult} dataset. This illustrates that the generated samples are more closely followed the distribution of data than other approaches. We furthermore perform a statistical significance test to gain more insights into the effectiveness of our proposed method in producing counterfactual samples compared with other approaches. Particularly, we conduct the paired $t$-test between our approach (CeFlow) and other methods on each dataset and each metric with the obtained results on 100 randomly repeated experiments and report the result of $p$-value in Table~\ref{tab:result}. We discover that our model is statistically significant with $p < 0.05$, proving CeFlow's effectiveness in counterfactual samples generation tasks. Meanwhile, Table~\ref{tab:runningtime} shows the running time of different approaches. Our approach achieves outstanding performance with the running time demonstrating around 90\% reduction compared with other approaches. Finally, as expected, by using normalizing flows, CeFlow produces more robust counterfactual samples with the lowest $l_1$-variance and demonstrates an effective running time in comparison with other approaches. 

Figure~\ref{fig:proximity} illustrates the categorical and continuous proximity. In terms of categorical proximity, our approach achieves the second-best performance with lowest variation in comparison with other approaches. The heuristic search based algorithm such as FACE and GS demonstrate the best performance in terms of this metric. Meanwhile, DiCE produces the best performance for continuous proximity, whereas CeFlow is ranked second. In general, our approach (CeFlow) achieves competitive performance in terms of proximity metric and demonstrates the least variation in comparison with others. On the other hand, Figure~\ref{fig:variation} shows the variation of our method's performance with the different values of $\alpha$. We observed that the optimal values are achieved at 0.8, 0.9 and 0.3 for \texttt{Law}, \texttt{Compas} and \texttt{Adult} dataset, respectively. 


% \qli{better to explain why, and in which scenerio CeFlow could perform better than DiCE}. 



% Figure~\ref{fig:size} and~\ref{fig:instance} show the variation of our method's performance with the different numbers of $K$-nearest neighbors for class prototype and $E$-dimensional embedding sizes of the auto-encoder model, respectively. It is clear from Figure~\ref{fig:size} that the performance of continuous proximity for \texttt{Law}, \texttt{Sangiovese} and \texttt{Adult} datasets is nearly stable with different embedding sizes, while \texttt{Law} witnesses a quite significant variation, increasing from around -0.336 to -0.224 corresponding to embedding sizes of 32 to 256, followed by a slight decrease to -0.33 (embedding size 512). A similar pattern also is recorded for the remaining metrics including categorical proximity, IM1, and IM2 with the good and stable performance at an embedding size of 256. The slight small fluctuations possibly illustrate that the impact of embedding size on the model performance is not very significant. Moreover, 256 is the preferable embedding size, while the sizes of 32 and 512 seem to be relatively small and large to sufficiently capture latent information for embedding vectors. Regarding categorical proximity, the performance declines slightly by 0.1 from 32 to 64, and thereafter varies slightly around 4.0 - 4.09 with embedding sizes of 128, 256, and 512. On the other hand, as can be seen from Figure~\ref{fig:instance}, IM1 and IM2 demonstrate a similar pattern illustrated by the worst performance when the number of instances of 15, followed by a stagnant performance from 25 to 45 instances. It is believed that the similar trend occurring in IM1 and IM2 is reasonable due to their similar properties illustrated in Section~\ref{sec:eval}. Meanwhile, there is no significant variation in the performance of continuous and categorical proximity across four datasets. These results suggest that the performance of our proposed method witnesses a small variation in all evaluation metrics regarding two hyperparameters (embedding sizes and numbers of nearest neighbors), implying our model's stability and robustness.

% The performance of different metrics on {1$^\text{st}$} and {2$^\text{nd}$ classifier} are illustrated in Table~\ref{tab:result1} and ~\ref{tab:result2}, respectively. Regarding to the 1$^\text{st}$ classifier from Table~\ref{tab:result1}, all three methods achieve the competitive target-class validity, except the Watch performance in all datasets with around 90\% of samples belonging to the target class. Regarding the percentage of samples satisfying the causal constraints, by far the greatest performance is achieved by ProCE with 85.91\%, 91.84\%, 95.64\% and 90.43\% for \texttt{Law}, \texttt{Sangiovese}, \texttt{Adult} and \texttt{Law} datasets, respectively. FACE also produces a competitive performance across four datasets in terms of this metric, standing at 81.49\%, 88.65\%, 92.49\% and 86.71\% while the majority of generated samples from Watch violate the causal constraints (63.61\%, 58.1\%, 70.40\% and 76.71\%). The performance of \%Ccv cannot be achieved to 100\% for all the methods which demonstrates that it is quite challenging to maintain the causal constraints in counterfactual samples. Moreover, these results indicate that by integrating the structural causal model, our proposed method can effectively produce the counterfactual samples preserving the features' causal relationships. Regarding interpretability scores, our proposed method achieved the best IM1 and IM2 on four datasets. DiCE is ranked second recorded with competitive result in \texttt{Adult} dataset (0.0809 for IM1 and 0.2679 for IM2) and \texttt{Law} dataset (0.0423 for IM1 and 0.0427 for IM2). The performance of all metrics on the 2$^\text{nd}$ classifier in Table~\ref{tab:result2} also demonstrates the competitive performance of our proposed method across all metrics. We also notice that although the 2$^\text{nd}$ has a more complicated architecture than the 1$^\text{st}$ classifier, there is a small variation on the performance of counterfactual explanation algorithm. Finally, as expected, by using prototype as a guideline of the counterfactual search process, ProCE produces more interpretable counterfactual instances recorded with good performance in IM1 and IM2.
%  By contrast, it is challenging for other approaches to reconstruct the counterfactual samples, leading to high interpretability scores (IM1 and IM2). 
% \begin{figure}[!htb]
% \centerline{\includegraphics[width=0.6\textwidth]{figure/proximity.pdf}}
% \caption{Baseline results in terms of \textbf{Categorical proximity} and \textbf{Continuous proximity}. Higher continuous and categorical proximity are better.}
% \label{fig:proximity}
% \end{figure}
\begin{figure}[!ht]
\centerline{\includegraphics[width=0.6\textwidth]{figure/alpha_variation.pdf}}
\caption{Our performance under different values of hyperparameter $\alpha$. Note that there are no categorical features in \texttt{Law} dataset.}
\label{fig:variation}
\end{figure}
\section{Conclusion}
In this paper, we introduced a robust and efficient counterfactual explanation framework called CeFlow that utilizes the capacity of normalizing flows in generating counterfactual samples. We observed that our approach produces more stable counterfactual samples and reduces counterfactual generation time significantly. The better performance witnessed is likely because that normalizing flows can get the exact representation of the input instance and also produce the counterfactual samples by using the inverse function. Numerous extensions to the current work can be investigated upon successful expansion of normalizing flow models in interpretable machine learning in general and counterfactual explanation in specific. One potential direction is to design a normalizing flow architecture to achieve counterfactual fairness in machine learning models.

\section*{Acknowledgement}
This work is supported by the Australian Research Council (ARC) under Grant No. DP220103717, LE220100078, LP170100891, DP200101374. 

% \qli{just give one expansion is enough}\duong{added}

\bibliography{sample}
\bibliographystyle{splncs04}

\end{document}
