
\section{Computational Details}\label{app:compdeets}
\vspace{-4pt}
\subsection{Spectra Pre-processing}\label{app:preprocess}
\vspace{-10pt}

In Fig.~\ref{fig:sibar1} we present the full TA spectra at 1~ps. Measurements at 0.10~V (shown), 0.00~V and 0.33~V (not shown) were also taken in the experiment, but these three traces were not stable across the delay window, and so they were omitted from the analysis. No smoothing is performed on the TA data.

\begin{figure}[h]
\begin{center}
    \resizebox{.45\textwidth}{!}{\includegraphics{TA_1ps_ABC.pdf}}\vspace{-2pt}
\end{center}
\vspace{-14pt}
\caption{\label{fig:sibar1} TA spectra at 1~ps seen in Fig.~\ref{fig:initial_spectra}, left, showing the full range of the measurement, for completeness. An additional trace at 0.10~V that was not used in the text is also shown. Its $\Delta A$ profile is roughly as expected, but the values are anomalously large.}
\end{figure}

Our steady state data are noisy in comparison to the TA spectra, so we smooth the experimental points using a Savitzky-Golay (SavGol) as distributed with the freely available $\mathrm{scipy.signal}$ library. The filter is chosen to have a window length of 17, and a polynomial order of 3. These are chosen conservatively to produce well-defined peak positions. We plot both the smooth and the data points on both relevant figures for transparency.

Both linear and TA spectra must be baselined for direct comparison with our model. In both cases we opt for a linear background subtraction. We do not account for the B-exciton around 2.05~eV. For these linear spectra, the background becomes linear between the A- and B-exciton features, so the overlap is small compared to the heights of the peaks. This may have some contribution to the poorer fit of the blue-side in the linear spectra. For the TA, one of the photoinduced features of the B-exciton will overlap with that of the A-exciton, but we have no good way of removing it at this time; we note the positive features (to the blue) of the B-exciton appear smaller in comparison. 

\begin{figure}[!h]
  \centering
  \begin{minipage}[c]{0.2\textwidth}
    \resizebox{1.0\textwidth}{!}{\includegraphics{ss_background.pdf}}
  \end{minipage}
  \begin{minipage}[c]{0.2\textwidth}
    \resizebox{1.0\textwidth}{!}{\includegraphics{ta_background.pdf}}
  \end{minipage}
  \vspace{-6pt}
  \caption{\label{fig:sibar2} Illustrative plots showing the background subtraction methodology. Both are for applied voltages of 0.3~V \textbf{Left}: Steady state spectrum, where the full black circles are the endpoints for the dashed black line. Subtracting this line results in the trace shown in Fig.~\ref{fig:steadystate}, right, of the main text. \textbf{Right}: TA spectrum at 1~ps, where the vertical black lines show the peak positions that delimit the linear fit. Subtracting this fit, shown as a dashed black line, results in the lighter purple curve which, after baselining, is the trace of Fig.~\ref{fig:TAfit1ps}.
  }
  \vspace{-6pt}
\end{figure}

In the case of the steady state data of Fig.~\ref{fig:steadystate}, we choose the lowest-gradient points either side of the 1.9~eV peak region and define a linear function between them that defines the background. An illustration of this is provided for the 0.3~V trace in Fig.~\ref{fig:sibar2}, left. This results in a very small negative region to one side of the peak, owing to the imprecise choice. The negative region is cropped. The position of the peak is fairly insensitive to moving the endpoints of this fit, at least for the data shown in this paper: there is enough of a linear region either side of the peak.

For the TA data the background subtraction is somewhat imperfect, for the reasons discussed in the main text. Here, the positive (photoinduced) features either side of the main bleach are identified, and a linear fit (not line) is performed between them. This is defined as the background. Again, we provide an illustration for the 0.3~V trace, in Fig.~\ref{fig:sibar2}, right. The gradient of this fit does not change hugely expanding the window to the red and blue respectively, as the traces are fairly symmetrical---in this case the curves are then cropped at the resulting maxima on each side, which may move slightly.


\vspace{-14pt}
\subsection{Optimization Procedure}
\label{app:optimization-procedure}
\vspace{-6pt}
Here we outline how the model parameters are extracted from the potential-dependent photoelectrochemical spectra at a particular time. We begin with a top-level summary of the method, and then we will provide the in-depth technical details and code.

At its most basic, the problem is one of resolution. For these data, the optical response saturates as the applied potential reaches its largest values. In the MND description, this saturation occurs because the superpeak has only exciton contribution, as the conduction band becomes fully depleted. Therefore, at this zero doping condition, we can identify the exciton peak position, height, and broadening. However, other values, such as the trion position at low doping (because there is no intensity), are challenging to extract by eye. Thus, to obtain a good choice of the model parameters that agree with the experimental data, we developed the following Monte Carlo-based fitting algorithm: 
\pagebreak
\begin{enumerate}
    \item For a given set of parameters consistent with the output of an MND model we obtain---after broadening with the (parametrically fixed) Gaussian---a particular manifold of curves, such as that displayed in Fig.~\ref{fig:steadystate}(d).
    \item Each experimental superpeak, having been isolated from the sloping background, can then be paired with its counterpart in this manifold that has the smallest point-wise deviation. 
    \item The sum of all the best-fit errors from the `global' set defines a total error. 
    \item We make a small change to the parameters, subject to the constraints established by the MND Hamiltonian, and repeat steps 1--3.
    \item We accept the change made in step~4 if it lowers the total error, else we reject it with a probability decreasing with error (Metropolis Monte Carlo).
\end{enumerate}
We iterate this procedure until the error converges.

At the more technical level, there are a few practical details that need to be addressed. Pseudoised code of the important functions is provided for clarity. First, as described in the main text, the TA spectra before 1~ps display a strong, short-lived `spike' feature. For completeness, we display time-series data showing this feature  at the A-exciton (superpeak) wavelength at low, medium, and high voltage in Fig.~\ref{fig:sibar0.5}. As this effect has been assigned to relaxing free-carriers,\cite{Ceballos2016, Cunningham2017} we focus on the post 1~ps regime where the interpretation of the fitted parameters is more straightforward, and their values at consecutive time points ought to be good guesses for one another.

\begin{figure}[h]
\begin{center}
    \resizebox{.3\textwidth}{!}{\includegraphics{A_exciton_spike.pdf}}\vspace{-2pt}
\end{center}
\vspace{-18pt}
\caption{\label{fig:sibar0.5} TA spectra at 0.55~V (yellow), 0.30~V (magenta), and 0.10~V (blue) at the A-exciton `superpeak' wavelength as a function of time. The short-lived decay component before 1~ps has been assigned as a non-equilibrium signature of free-carriers waiting to form excitons (and therefore trions). While the effect looks larger at higher potential, it should be noted that there is an accompanying shift (Fig.~\ref{fig:initial_spectra}, right) which we assign as a peak-splitting. As is discussed throughout the text superpeak parameters should not be directly interpreted.}
\end{figure}

\vspace{-14pt}
\subsubsection*{Steps 1--3}
\vspace{-10pt}

For a given pre-processed set of curves, we begin with an initial guess. When running on the full time series, this guess is the solution to the previous time step (for which a solution exists). For the steady state spectra, or for the first (1 ps) TA spectra, the guess is made by hand. For Fig.~\ref{fig:TAfit1ps}, this takes in the following parameters,

\begin{minted}[fontsize=\scriptsize]{python} 
pos_start_X = 1.899; pos_end_X = 1.909
pos_start_T = 1.876; pos_end_T = 1.865
heights_X = [1.1, 0.9, .7, 0.4, 0.15, 0.05, 0.00]
heights_T = [0.00, 0.01, 0.02, 0.06, 0.11, 0.17, 0.25]
width_start_X = 2.0E-3; width_end_X = 12.0E-3
width_start_T = 5E-3; width_end_T = width_start_T
\end{minted}

using them to produce curves like those of Figure~3 in Ref.~\onlinecite{Chang2019a}, i.e. curves that respect the constraints of the MND model. In particular, the position and width parameters define linear functions between the starting and ending doping levels. The heights are instead a quadratic fit to the list of points provided (uniform over the doping region); seven points were used to stabilise the fit against noise during the Monte Carlo loop (\textit{vide infra}), as when only three points were used the rejection rate of guesses was unacceptably high. Using a set of variable points was quicker to implement than finding the constraints required to make the quadratic fit monotonically decrease over the doping range. Fortunately, this less-robust approach allowed us to find the positive trion values discussed in the text.

Once these doping curves have been obtained, the manifold pictured in Fig.~\ref{fig:steadystate}, right, can be constructed. Specifically,

\begin{minted}[fontsize=\scriptsize]{python} 
instr = 6E-2 * gaussian(x_axis, (x_upper + x_lower) 
        / 2, 1.1 * 5.0E-2)  # measurement broadening
manifold = np.array([np.zeros_like(x_axis) for E in E_axis])
for i, E in enumerate(E_axis):
    X_peak = height_X_f(E) * 
        lorentz(x_axis, pos_X_f(E), width_X_f(E), E)
    T_peak = height_T_f(E) * 
        lorentz(x_axis, pos_T_f(E), width_T_f(E), E)
    XT_peak = X_peak + T_peak
    peak = np.convolve(instr, XT_peak, "same")
    manifold[i] = peak
\end{minted}

The doping window ($\mathrm{E}\_\mathrm{axis}$) is always chosen to be from 0--30~meV, as this ought to provide enough range in the first instance. However, as noted, changing the parameters can incorporate a stretching of the doping range which means that---particularly for the TA spectra---it should not be directly interpreted as matching that in Ref.~\onlinecite{Chang2019a}. Instead, a feature like the crossover point between trion and exciton should be matched between the original model and the parameterized curves, if such a comparison is desired. The `gaussian' function has unit height, and the `lorentz' function is defined as
\begin{minted}[fontsize=\scriptsize]{python} 
def lorentz(p, p_0, w, E=0.03):
    w_p = 2. * w / (1. + np.exp(-(100 * E / 0.03) * (p - p_0)))
    x = (p - p_0) / (w_p / 2)
    return 1. / (1. + np.power(x, 2))
\end{minted}
where the asymmetry parameters is set to $100$ to achieve agreement with the plots in Ref.~\onlinecite{Chang2019a}. This value is never changed.

Each of the experimental spectra can then be compared against the manifold to find their best fit, subject to the condition that increasing experimental voltage decreases the doping level from the previous value. Further, the fit cannot move more than one quarter of the doping range in one go, as this would not line up with the experimental observation and is just done for sanity. Roughly speaking, the algorithm cannot jump around on the doping axis and must respect the voltage ordering. Additionally, the best fit choice downweights the error away from the central region by around 50\% using a Gaussian mask: this is because the tails were never fitted particularly well, as discussed in the main text. The mask was chosen by reducing the value (Gaussian width) until the fit produced peak positions that were in agreement for the pure exciton. In code,

\begin{minted}[fontsize=\scriptsize]{python} 
def choice_E_fit(*args, **kwargs):
    ...
    minerr = len(manifold)  # we start the scan from the right
    maxstep = int(len(manifold)/4) # we won't go more in one step
    besterrs = []
    for i, voltage in enumerate(voltages):
        else:
            x_L = peak_xaxes[i][0]
            x_R = peak_xaxes[i][-1]
            cfit = np.copy(peaks[i] / sq)  #normalization factor
            gfilter = gaussian(x_axis[x_L:x_R], 
                    x_axis[int(x_L/2+x_R/2)], 
                    (x_axis[x_R]+x_axis[x_L]) / 30)
            # 30 is 50% at the wings, 50 is 10% at the wings
            errors = np.zeros_like(E_axis)
            for k, theory in enumerate(manifold):
                tfit = theory[x_L:x_R]
                diff = cfit - tfit
                diff *= gfilter
                errors[k] = (np.linalg.norm(diff))
            # we don't want to scan too far
            steplim = max(0, minerr-maxstep)  # need to clamp it at 0
            # dont overshoot the previous fit value
            steplim += np.argmin(errors[steplim:max(1, minerr)])
            # max is to stop it crashing due to empty list
            choice_E = E_axis[steplim]
            choice_Es.append(choice_E)
            besterrs.append(errors[steplim])  # we need this for MC
    choice_Es.reverse()  # it will be low to high this way
    return choice_Es, besterrs
\end{minted}

\vspace{-14pt}
\subsubsection*{Steps 4 and 5}
\vspace{-10pt}
Now a single iteration has been defined, the Monte Carlo loop may be performed. Steps in parameter space are defined in a way that respects the MND model. From the reference parameters (previous accepted value) the small changes are chosen by

\begin{minted}[fontsize=\scriptsize]{python} 
def make_delta(reference, loop_params, min_BE=0.005, min_shift=0.000):
    delta_params = copy.copy(reference)
    # keep the beginning exciton position fixed
    delta_params[0] = 0
    # its end-point can vary more, but must be bluer
    # the exciton MUST shift with potential. 
    # This kwarg is to stop is being static, which can happen.
    delta_params[1] = max((loop_params[0]+delta_params[0]-
                                    loop_params[1]+min_shift), 
                                    np.random.normal(0, 5E-3))
    # the trion is harder to fit
    # it must be more red
    # however they might go ontop of each other, 
    give a minimum binding energy: min_BE kwarg
    delta_params[2] = min((loop_params[0]+delta_params[0]-
                                    loop_params[2]-min_BE), 
                                np.random.normal(0, 1E-2))
    # the trion must always be more red than the exciton, 
    # good enough to make the start lower, since it gets more red
    delta_params[3] = min((loop_params[2]+delta_params[2]-
                                            loop_params[3]), 
                                    np.random.normal(0, 1E-2))
    # now heights.
    # exciton starts at 1 by normalization
    # quadratic fit musn't have a turning point, no minimum...
    # TODO
    hX = delta_params[4]
    hT = delta_params[5]
    hX[0] = max((0.95-loop_params[4][0]), 
            min(1.05-loop_params[4][0], 
                np.random.normal(0, 0.2)))
    # first max: don't let it drop too much from 1.0
    # second min: the data are normalized
    hT[0] = 0  # no trion at the start
    # we know its close to zero, right now first argument is!
    # the trion never gets above about 0.6 units
    for i in (1, 2, 3, 4, 5, 6):
        hX[i] = max(-loop_params[4][i],
                    min(loop_params[4][i - 1] + hX[i - 1] - 
                                        loop_params[4][i], 
                                np.random.normal(0, 0.2)))
        # first max/min to stop extreme new_params,
        # second min to stop overshoot previous value
        hT[i] = max(-loop_params[5][i], 
                min(0.6-loop_params[5][i],
                    max(loop_params[5][i - 1] + hT[i - 1] -
                                        loop_params[5][i], 
                                np.random.normal(0, 0.05))))
        # difference here is trion is growing, so max not min
    # now the widths
    delta_params[6] = max(-loop_params[6] + 1E-4,
                        np.random.normal(0, 5.0E-4))  
    # widths cannot be too small
    # has to end wider than it started
    delta_params[7] = max(-loop_params[7] + loop_params[6] + 
                                            delta_params[6], 
                                np.random.normal(0, 3.0E-3))  
    # there's a lot of possibility here
    # this needs to be close to where the exciton starts
    delta_params[8] = max(loop_params[6] + delta_params[6] -
                                            loop_params[8], 
                                np.random.normal(0, 1.0E-4))
    delta_params[9] = delta_params[8]  # fixed same by theory
    
    return delta_params
\end{minted}
which is sufficiently general that it did not need to be changed. The Monte Carlo loop itself is done in a staged way, where many small steps are compared before attempting a Metropolis test.

\begin{minted}[fontsize=\scriptsize]{python} 
def run_MC(*args, **kwargs):
    ...
    """tests on the first spectrum suggested that you can
    find a rare improvement in a sample of 3000 or 4000,
    so I would make this the inner loop"""
    choice_es, besterrs = choice_E_fit(params)  # initial guess
    err_hand = np.sum(besterrs)
    print(f"the overall error is {err_hand}")
    best_overall = [params, choice_es, err_hand]
    loop_params = copy.copy(params)
    
    # now we need the loop that will vary the parameters
    def inner_search(new_params):
        fit_funcs = make_regime(*new_params)  # turn into curves
        choice_es, besterrs = choice_E_fit(new_params)
        toterr = np.sum(besterrs)
        return new_params, choice_es, toterr
        
        
    np.random.seed(2022)  # year of our Lord.
    for j in range(outer_loop):
        # the outer loop performs the MC
        print(f"------OUTER LOOP IS NOW AT {j}-------")
        results = [0 for i in range(inner_loop)]
    
        # SERIAL version, in the real code this is parallel
        t_inner = perf_counter()
        for k in range(inner_loop):
            results = inner_search(add_params(loop_params, 
                make_delta(reference, loop_params)))
        # add_params does what it sounds like
    
        best_in_show = min(results, key=lambda t: t[2])
        if best_in_show[2] < best_overall[2]:
            print(f"OUTER LOOP lowers error to {best_in_show[2]}")
            accept = 1
        else:
            prob_metro = np.exp(
                    (best_overall[2] - best_in_show[2]) / temper)
            # choose temper to get sensible reject rate
            if metro>0: print(f"{best_overall[2]} -> 
                        {best_in_show[2]} => {100*prob_metro:.0f}%")
            if np.random.uniform() < prob_metro and metro>0:
                accept = 1
                print(f"ACCEPTS worse result in OUTER LOOP")
            else:
                if metro>0: 
                    print(f"REJECTS worse result in OUTER LOOP")
                accept = 0
    
        if accept == 1:
            loop_params = best_in_show[0]
            best_overall = best_in_show
            with open(pkl_end, "wb") as f:
                pickle.dump(best_overall, f)  # save to disk
    
    print("------ALL OUTER LOOPS COMPLETE-------")
\end{minted}
This is done because the random steps in parameter space are chosen brute force, not in some downhill-seeking way. Backpropagation would probably be a far more efficient way of choosing steps, but this code was not found to be expensive to run locally. The files saved to disk can then be used as the guess for the next time step, and for the analysis presented in the main text.

\vspace{-14pt}
\subsection{TA data results}\label{sec:TAresults}
\vspace{-10pt}
Figure~\ref{fig:si1} is provided to support the claim that the Monte Carlo method is achieving meaningful fits across time. Each fit is independent---only using the previous result as an initial guess---and can in principle give completely different results to its neighbours if the fit is poor. The stability of the doping level, and the fact that Figs.~\ref{fig:si2}~and~\ref{fig:si3} show a continuousness across time is supporting evidence, along with the visual goodness of fit, that the algorithm is performing adequately well.

Gaps in the time series around 1~ps are due to experimental anomalies where at least one of the traces displays erroneous intensity behaviour (e.g. the 0.3~V trace is suddenly twice as strong) or the absence of any well-defined peaks. Since these are confined usually to only one or two traces at a time, and not seen at the next time point, these are diagnosed as temporary interruptions to the data collection process. To be consistent, the entire set of spectra are omitted at each of these points, which were all confirmed by manual inspection.

\begin{figure}[!h]
\vspace{-12pt}
\begin{center}
    \resizebox{.4\textwidth}{!}{\includegraphics{dope_v_time.pdf}}\vspace{-2pt}
\end{center}
\vspace{-20pt}
\caption{\label{fig:si1} Doping level assigned to each experimental trace (voltage) as a function of time. Since we contend that the conduction band density should be mostly due to the background carriers from the applied potential, we expect our fitting algorithm to assign a roughly constant doping level with time. The ordering of the values is enforced by the algorithm, e.g. the 0.15~V trace must be assigned a higher value than the 0.2~V trace.}
\vspace{-6pt}
\end{figure}

\begin{figure}[!h]
\begin{center}
    \resizebox{.4\textwidth}{!}{\includegraphics{ratio.pdf}}\vspace{-2pt}
\end{center}
\vspace{-20pt}
\caption{\label{fig:si2} Fig.~\ref{fig:TAresults}, left, but taking the ratio of bottom panel to top panel. The inset shows the result of taking the logarithm of the four traces with entries greater than one (trion dominant). Approximate single-exponential behaviour is observed.}
\vspace{-6pt}
\end{figure}


\begin{figure}[!h]
\begin{center}
    \resizebox{.4\textwidth}{!}{\includegraphics{shifts.pdf}}\vspace{-2pt}
\end{center}
\vspace{-20pt}
\caption{\label{fig:si3} Shift of the peaks corresponding to extreme doping levels (applied voltage) as predicted by the model. Yellow circles are the shift of the 0 meV exciton, blue squares are the shift of the 23 meV trion, which is the (approximate) assignment of the 0.15~V experimental trace, as shown in Fig.~\ref{fig:si1}.}
\vspace{-14pt}
\end{figure}

\vspace{-14pt}
\subsection{2D spectroscopy MND code}
\vspace{-10pt}
As the code\cite{Lindoy2022} is a direct extension of Ref.~\onlinecite{Chang2019a}, it is already programmed with our default values. We ran for 40~fs, and moved the exciton peak to the appropriate energy value (1.9~eV rather than 2.0~eV). The position of the peak is entirely arbitrary (sets the energy zero) for this Hamiltonian. There is no bath, only relaxation to the ground state.

