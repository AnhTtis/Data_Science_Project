\documentclass{article}

\usepackage[english]{babel}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bbm}

\usepackage{multirow}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{xcolor}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{cleveref}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}
[section]
\newtheorem{lemma}{Lemma}
[section]

\newcommand{\definitionautorefname}{Definition}
\newcommand{\propositionautorefname}{Proposition}
\newcommand*{\Appendixautorefname}{Appendix}

\usepackage{makecell}

\newcommand{\JM}[1]{{\color{blue!80!black}#1\color{black}}}
\newcommand{\UK}[1]{{\color{red!80!black}#1\color{black}}}
\newcommand{\RS}[1]{{\color{purple!80!black}#1\color{black}}}
\newcommand{\FD}[1]{{\color{cyan!80!black}#1\color{black}}}
\newcommand{\SR}[1]{{\color{green!80!black}#1\color{black}}}


\usepackage{arydshln}

\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}




\def\appendixautorefname{Appendix}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\hyper@makecurrent}{%
    \ifx\Hy@param\Hy@chapterstring
        \let\Hy@param\Hy@chapapp
    \fi
}{%
    \iftoggle{inappendix}{%
        \@checkappendixparam{chapter}%
        \@checkappendixparam{Section}%
        \@checkappendixparam{subsection}%
        \@checkappendixparam{subsubsection}%
        \@checkappendixparam{paragraph}%
        \@checkappendixparam{subparagraph}%
    }{}%
}{}{\errmessage{failed to patch}}

\newcommand*{\@checkappendixparam}[1]{%
    \def\@checkappendixparamtmp{#1}%
    \ifx\Hy@param\@checkappendixparamtmp
        \let\Hy@param\Hy@appendixstring
    \fi
}
\makeatletter

\newtoggle{inappendix}
\togglefalse{inappendix}

\apptocmd{\appendix}{\toggletrue{inappendix}}{}{\errmessage{failed to patch}}


\addto\extrasenglish{
  \def\subsectionautorefname{Subsection}
}
\addto\extrasenglish{
  \def\subsectionautorefname{Section}
}

\addto\extrasenglish{
  \def\sectionautorefname{Section}
}

\usepackage[backend=biber,
    style=apa,
    maxcitenames=2,
    minbibnames=2,
    maxbibnames=2, 
    language=english,
    doi=false,
    isbn=false,
    url=false,
    uniquename=false
]{biblatex} 
\addbibresource{references.bib} %



\newcounter{question}
\setcounter{question}{0}

\newcommand{\question}[1]{\item[Q\refstepcounter{question}\thequestion.] \textit{#1}}
\newcommand{\answer}[1]{\item[A\thequestion.] #1}


\newcommand{\given}{\,|\,}
\newcommand{\E}{\mathbb{E}}


\title{Finding Competence Regions in Domain Generalization}
\author{Jens Müller, Stefan T. Radev, Robert Schmier, Felix Draxler, Carsten Rother, \\Ullrich Köthe}

\frenchspacing

\begin{document}
\maketitle

\begin{abstract}
\noindent We propose a ``learning to reject'' framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution.
Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright.
Trustworthiness is then predicted via a proxy \textit{incompetence score} that is tightly linked to the performance of a classifier. 
We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain.
For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting.
Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significant improvements of the average accuracy below a suitable incompetence threshold.
However, the scores are not yet good enough to allow for a favorable accuracy/rejection trade-off in all tested domains.
Surprisingly, our results also indicate that classifiers optimized for DG robustness do not outperform a naive Empirical Risk Minimization (ERM) baseline in the competence region, that is, where test samples elicit low incompetence scores.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\textwidth]{figures/pull_figure.pdf}
    \caption{The main principle behind \textit{incompetence scores} for improved domain generalization: We reject instances above the incompetence threshold, which is located at the $95\%$ quantile of the training distribution.}
    \label{fig:overview}
\end{figure}

Although modern deep learning exhibits excellent generalization, it is prone to silent failures when the actual data distribution differs from the distribution during training \autocite{sanner2021reliable, yang2021generalized}.
We address this problem in a ``learning to reject'' framework \autocite{hendrickx2021machine,zhang2023survey}: 
\textit{Given a pre-trained model and potentially problematic data instances, can we determine if the model's responses are still trustworthy?} 
In answering this question, we consider the case where the distribution shift is mild (e.g., from one hospital to the next), so that the model remains competent on many instances in spite of the shift.
Thus, we do not want to reject all out-of-distribution (OOD) instances outright, but only those for which the estimated model competence falls below some acceptance threshold.

In line with previous research on \textit{Domain Generalization} \autocite[DG;][]{gulrajani2020search}, we assume that we do not possess data beyond the training and validation sets.
Therefore, we can neither determine out-of-domain competence directly, nor define the acceptance threshold in a Bayes-optimal way \autocite{chow1970optimum}. 
Instead, we investigate proxy scores that are negatively correlated with competence:
We call them \textit{incompetence scores} and they should monotonically decrease as the model accuracy increases (see~\autoref{sec:method}).
For a simple example of such a score, we may consider the distance of a new data point to the nearest neighbor of the training data in the model's learned feature space. 
In this case, we expect the performance to drop with increasing distance.
Interestingly, our experiments demonstrate that the monotonicity property typically holds for well-known choices of these scores. 


To define a plausible trade-off between accuracy and acceptance rate without out-of-domain data, we decided to place the acceptance threshold so that $5\%$ of the training distribution are rejected (see~\autoref{fig:overview}).
In comparison to a naive method not rejecting any data, this threshold 
considerably improves the accuracy on the accepted subset from the shifted domain. 
Ideally, the threshold would define a competence region where the accuracy is indistinguishable from in-distribution (ID) data, but even the best incompetence scores do not yet achieve this goal on all data sets and domains, calling for further research.

Our notion of incompetence also differs from established OOD detection methods \autocite{yang2021generalized} in another crucial aspect: It is always relative to a specific {\em task}.
The following example illustrates why this is important: 
A robust classifier should be able to ignore noise and recognize correct labels even for noisy data.
In contrast, a system monitoring the health of the data acquisition hardware may find crucial clues in the noise and should not ignore it.
Thus, \textit{competence depends on context-specific data characteristics and cannot be defined in absolute terms}.
In this work, we focus solely on classification tasks, but our method should likewise apply to other tasks such as regression.

Furthermore, we present a comprehensive experimental evaluation of incompetence scores.
For comparability with prior work, we focus on standard data sets from the domain generalization literature \autocite{gulrajani2020search} and consider the closed vs.~open world setting (i.e., new appearances of known classes vs.~hitherto unknown classes) and the effect of measuring incompetence through different data representations.
We investigate whether state-of-the-art classifiers (SOTA) that are optimized specifically for domain shift robustness exhibit more accurate competence regions than naively trained ones. 
Moreover, we investigate whether it is possible to estimate an incompetence threshold, such that a classifier can recover its ID accuracy in the corresponding competence region under domain shift.
In summary, we make the following contributions:
\begin{enumerate}
    \item We show experimentally that accuracy decreases as incompetence scores increase and highlight the resulting trade-offs between rejection rate and accuracy gain. 
    \item We find that both feature- and logit-based scores are competitive in the closed world, whereas feature-based approaches work best in the open world setting.
    \item We propose an approach to determine an incompetence threshold from ID data and demonstrate its utility for most domain shifts considered in this work.
    \item We observe that robust classifiers do not outperform a naive baseline in terms of generalization performance in the elicited competence regions.
\end{enumerate}



















\begin{figure*}[tbh]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/ood_standalone.pdf}
    \caption{An incompetence score is able to sort out-of-distribution (OOD) images from the PACS data set, so that higher incompetence scores result in lower classification accuracy. \textit{(Left)} Example images from the training domains. \textit{(Right)} Images from the test domains resulting in lowest and highest incompetence scores (using a Deep-KNN scoring function) in the feature space of a baseline ERM classifier. 
    Green and red frames denote correctly and incorrectly classified images, respectively. Higher incompetence scores correlate with a decrease in the classifier's accuracy.}
    \label{fig:img_surprisal}
\end{figure*}

\section{Related Work}

\subsection{OOD Detection and Generalization}
Dealing with anomalous (i.e., out-of-distribution; OOD) instances that differ from the ones contained in the training set (i.e., our proxy for the in-distribution; ID) is a widely discussed and conceptually overloaded topic in the machine and statistical learning literature \autocite{yang2021generalized, shen2021towards, ood-pyod, ood-openood}.
OOD detection address the problem of flagging unusual data points which could undermine the reliability of machine learning systems \autocite{yang2021generalized}; OOD generalization addresses the need to make predictions even when the test distribution is completely unknown or known to be different than the training distribution \autocite{shen2021towards}.

In this work, we are interested in analyzing established domain-robust classifiers. Thus, we focus on OOD detection methods that do not modify the classifier architecture or training. Such methods are called \textit{post-hoc} scores \autocite{yang2021generalized}, as they do not intervene on the downstream classifier, but only ``post-process'' its feature or logit space.

Post-hoc OOD scores have been shown to perform well across a variety of OOD detection benchmarks \autocite{ood-openood}.
Previous work analyzed post-hoc OOD detection scores to predict the accuracy of a classifier on novel inputs \autocite{techapanurak2021practical} or to detect ID failure cases \autocite{xia2022augmenting}.
In addition, \textcite{techapanurak2021practical} compute an aggregated OOD-score over an entire ID data set to predict the global accuracy of a classifier on OOD data. 
Differently, we aim to predict the likelihood of error from individual incompetence score values and show that this approach provides us with a finer control over the trade-off between coverage and accuracy (see~\autoref{sec:tentative_solution}).

Despite the large volume of literature focusing on OOD detection and generalization, there are no extensive studies applying OOD scores to domain generalization benchmarks. 
Thus, one of the main goals of this work was to provide such a comprehensive analysis on the utility of OOD scores for improving domain generalization. 



\subsection{Domain Generalization}

The goal of domain generalization (DG) is to train models that generalize well under covariate shifts \autocite{zhou2022domain}, such as adversarial attacks \autocite{goodfellow2014explaining} or style changes \autocite{gatys2016image}, for which the label space remains unchanged during testing \autocite{yang2021generalized}.
In DG settings, we assume that we have access to different environments or data sets (e.g., art and sketch images) and the goal is to make good predictions in completely unknown environments (e.g., real world images).

Compared to Domain Adaptation \autocite[DA;][]{wang2018deep}, where we have unlabeled data from the test domain, the DG problems assume that we have no knowledge about the test domain(s).
Moreover, it has been shown that classifiers can assign high likelihoods under domain shift even when they are plainly wrong which makes it hard to detect failure cases \autocite{nguyen2015deep,nalisnick2019detecting}.
Thus, proxy ``incompetence'' OOD scores appear to be good candidates for spotlighting such failures.
However, to the best of our knowledge, there are no extensive studies which attempt to quantify the competence of domain-robust models.

Many benchmark data sets in DG have been established, on which researchers can study generalization performance beyond a single training environment \autocite{gulrajani2020search,koh2021wilds}.
In this work, we consider the main data sets contained in the DomainBed benchmark \autocite{gulrajani2020search}. 
We additionally distinguish between a \textit{closed world} setting, where only instances of known classes are encountered in the test domain, and an \textit{open world} setting, where instances of unknown classes are also present in the test domain. 
We believe the open world setting to be of practical interest, even though typical DG problems are formulated under a closed world assumption \autocite{zhou2022domain}.

Some DG methods explicitly exploit domain labels in order to train robust classifiers \autocite{arjovsky2019invariant, muller2021learning, shi2021gradient}.
However, it is not clear which DG methods can achieve consistently robust performance across different data sets.
On the one hand, it has been suggested that a strong standard classifier trained with empirical risk estimation (ERM) performs favorably across multiple DG data sets \autocite{gulrajani2020search}.
On the other hand, some DG methods have been shown to outperform an ERM baseline on several benchmark data sets \autocite{koh2021wilds}.
Here, we complement the existing literature by examining whether the competence regions of different DG classifiers differ in terms of the achieved improvements in accuracy.

\subsection{Selective Classification}
Inference with a reject option \autocite[aka \textit{selective classification}, ][]{geifman2017selective, el2010foundations} enables classifiers to refrain from making a prediction under ambiguous or novel conditions \autocite{hendrickx2021machine}.
Moreover, \textcite{zhang2023survey} outline the three main reasons why a reject option could be a reasonable choice: 1) failure cases; 2) unknown cases; and 3) fake inputs.
For instance, \textcite{kamath2020selective} train natural language processing (NLP) models for selective question answering under domain shift.
\textcite{varshney2022investigating} investigate the utility of MaxProb (a common OOD detection score) as a rejection criterion across several NLP data sets.
\textcite{ren2022out} use the Mahalanobis distance as OOD detection method to filter inputs to NLP models for conditional text generation and \textcite{mesquita2016classification} showcase the reject option for catching software defects.

The main challenge selective classifiers face is how to reduce the error rate by ``rejecting'' instances for which no reliable prediction can be made, while keeping coverage (i.e., the number of ``accepted'' instances) as high as possible \autocite{nadeem2009accuracy, chow1970optimum}.
And while the theoretical characteristics of the resulting trade-off have been systematically studied \autocite{el2010foundations, wiener2011agnostic}, the empirical utility of OOD ``rejection scores'' for ensuring robust performance in the DG setting remains largely unclear. 


In this work, we perform an extensive evaluation of this trade-off across a wide variety of state-of-the-art OOD scores, domain-robust classifiers, DG data sets and environments. 

\section{Method}
\label{sec:method}

\subsection{Notation}

We denote with $c_{\theta}$ an arbitrary classifier with a vector of trainable parameters $\theta$ (e.g., neural network weights) which we typically suppress for readability.
To evaluate a classifier, we consider its accuracy, which we denote as $\textrm{A}_\textrm{dist}$, based on queries from some reference distribution $x \sim p_\textrm{dist}(x)$.

\subsection{Incompetence Scores}
The goal of an incompetence score $s_c \colon \mathbb{R}^D \to \mathbb{R}$ is to indicate whether a classifier $c$ is familiar with some input $x \in \mathcal{X}$. 
We consider familiarity with the input to be equivalent to competence.
The fundamental principle in this work is that instances eliciting a high incompetence score are intrinsically hard to predict and \textit{vice versa}. 
Due to the close conceptual connection between competence and familiarity or incompetence and OOD, we employ OOD scores as proxy for incompetence. %

\subsection{Admissible Incompetence Scores}
\label{sec:incompetence_scores}

Detecting out-of-competence means checking whether some given incompetence score $s_c(x) \in \mathbb{R}$ falls below some threshold $\alpha$ (classified as in-competence) or above (classified as out-of-competence). We consider scores $s_c(x)$ that depend on the classifier $c$ and the input $x$ at hand. %

The threshold $\alpha$ trades off accuracy (how well does the classifier perform on accepted data) with coverage (how many samples does the score accept). 
In this section, we describe how a useful (ideal) incompetence score should affect downstream classification as a function of the threshold $\alpha$.
In particular, consider the subset of input space where the classifier is deemed competent given a fixed threshold $\alpha$. 
It is the region of input space with incompetence score less than $\alpha$:
\begin{align}
    \label{eq:competence-region}
    X_{c}(\alpha) := \{ x : s_c(x) \leq \alpha \}.
\end{align}
We use the ID data to determine a suitable threshold for the competence region, for instance we later pick $\alpha = \alpha_{95}$ such that 95\% of the ID data is in $X_{c}(\alpha_{95})$.
We consider the performance of the classifier $c$ restricted to the competence region $X_{c}(\alpha)$ as a function of $\alpha$. 
Therefore, we filter out unseen OOD data $p_{\operatorname{OOD}}$ and keep samples with incompetence scores lower than $\alpha$ to compute the resulting accuracy $\mathrm A_{\operatorname{OOD}}(\alpha)$ of our classifier.
For low $\alpha \approx \min_{x \sim p_{\operatorname{OOD}}} s_c(x)$, we expect the competence region to contain samples $x$ which the classifier can recognize even under a distribution shift. 
As $\alpha$ increases, the classifier faces samples beyond its competence. 
For $\alpha > \max_{x \sim p_{\operatorname{OOD}}} s_c(x)$, we evaluate the classifier on the entire OOD test data.

\begin{figure*}[bpt]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/safe_curves.pdf}
    \caption{The accuracy of the ERM classifier on OOD data $\mathrm A_{\operatorname{OOD}}(\alpha)$ as the competence region is enlarged by increasing the incompetence threshold $\alpha$. As predicted in \autoref{prop:main}, the accuracy starts off $\mathrm A_{\operatorname{OOD}}(\alpha) \geq \mathrm A_{\operatorname{ID}}$ and then falls off monotonically with $\alpha$. At same time, the fraction of data the classifier is applied to increases. The classifier accuracy and fraction of considered data can easily be traded off using this figure.}
    \label{fig:histo_g_alpha}
\end{figure*}

We summarize the above description in the fundamental principle of this work:
\textbf{An admissible incompetence score must assign low incompetence to those regions where the downstream accuracy is high.}
In contrast, OOD detection is agnostic to the downstream task and focuses merely on flagging anomalous inputs. %
To show some fundamental properties of the downstream accuracy $\mathrm A_{\operatorname{OOD}}(\alpha)$ as a function of $\alpha$, we formalize this principle as follows:
\begin{definition}
    \label{def:competence-detector}
    An incompetence score $s_c(x)$ is called \emph{admissible} if the downstream accuracy $\mathrm A_{\operatorname{OOD}}(\alpha)$ decreases monotonically as $\alpha$ is increased for any distribution of interest.
\end{definition}
\noindent This monotonic trend requires that the incompetence score $s_c(x)$ is closely related to the performance of the classifier. 
Such a connection allows us to make strong predictions on the downstream accuracy as a function of $\alpha$:



\begin{proposition}
    \label{prop:main}
    Given a classifier $c_{\theta}(x)$ and its corresponding in-distribution $p_{\operatorname{ID}}$. Then, for a test distribution of interest $p_{\operatorname{OOD}}$ and a corresponding admissible score $s_c(x)$ as in \autoref{def:competence-detector}:
    \begin{itemize}
        \item[(a)] If there is a threshold $\alpha^* \in \mathbb R$ such that for all $\alpha \leq \alpha^*$ ID and OOD have the same support and classification accuracy, that is $X_c(\alpha) \cap \operatorname{supp}(p_{\operatorname{ID}}) = X_c(\alpha) \cap \operatorname{supp}(p_{\operatorname{OOD}})$ and if $\mathrm A_{\operatorname{ID}}(\alpha) = \mathrm A_{\operatorname{OOD}}(\alpha)$,

        then, $\mathrm A_{\operatorname{OOD}}(\alpha) \geq \mathrm A_{\operatorname{ID}}$ for $\alpha < \alpha^*$.
        \item[(b)] In the limit of $\alpha \to \infty$, we find that $\mathrm A_{\operatorname{OOD}}(\alpha) \to \mathrm A_{\operatorname{OOD}}$.
    \end{itemize}
\end{proposition}
The first statement states that we expect a classifier's accuracy on OOD test data $x \sim p_{\operatorname{OOD}}(x)$ restricted to the corresponding competence region $X_c(\alpha)$ to be at least as good as its expected accuracy on ID data $x \sim p_{\operatorname{ID}}(x)$ if $\alpha$ is sufficiently small.
The additional assumption in (a) requires that the classifier has the same competence for ID and OOD data within the high competence region $X_{c}(\alpha^*)$. 
In that case, we can even surpass the ID accuracy, which we observe in practice (see~\autoref{sec:choosing-competence-threshold}). 
The second statement states that as $\alpha$ increases, we attain the expected performance of the classifier on the OOD data.
Between these two extremes, the accuracy decreases monotonically by \autoref{def:competence-detector}. 
Thus, monotonicity describes the expected behavior of the OOD accuracy $\mathrm{A}_{\operatorname{OOD}}(\alpha)$ as a function of the incompetence threshold $\alpha$. 
We give the proof in \autoref{app:proof}.









































\section{Experiments}
\label{sec:experiments}

In our experiments, we analyze the effect of an incompetence threshold $\alpha$ (see~\autoref{sec:incompetence_scores}) on DG performance.
In the following, we first describe our experimental protocol. 
Then, we analyze the competence region in dependence on the threshold $\alpha$ and show that the competence region behaves as predicted in \autoref{prop:main}. 
Finally, we carry out an extensive investigation of the competence region for closed and open world settings, where we show the utility of the concept for various incompetence scores and point out current weaknesses. 

As an introductory example to the competence region, we consider \autoref{fig:img_surprisal} which depicts the experimental procedure on the PACS data set for a standard classifier trained with Empirical Risk Minimization \autocite[ERM;][]{vapnik1999overview}.
We train the classifier on the domains Art, Photo, and Sketch, and apply the trained classifier in the unknown Cartoon domain. 
The samples in the test domain are ordered by the predicted incompetence score $s_c(x)$. 
As expected, the classifier still performs well on Cartoon samples with low incompetence scores (9 out of 9 classified correctly in the example), but the accuracy drops for high scores (only 2 out of 9 correct classification). 
Qualitatively, the score correctly notices that images with significantly different characteristics are much harder to classify. 
In the following sections, we quantify this behavior systematically for a number of different classifiers, incompetence scores, data sets, and domain. 
But first, we give details on our experimental setup.

\subsection{Experimental Setup}
We consider all combinations of nine pre-trained classifiers $c_{\theta}(x)$, varying both in architecture and training, nine OOD post-hoc scores $s_c(x)$ as incompetence scores on a total of 32 DG tasks from six different DG data sets. 
The pre-trained classifiers are obtained as follows. We train various state-of-the-art classifiers from DG literature, namely Fish \autocite{shi2021gradient}, GroupDRO \autocite{sagawa2019distributionally}, SD \autocite{pezeshki2021gradient}, SagNet \autocite{nam2021reducing}, Mixup \autocite{yan2020improve} and VREx \autocite{krueger2021out}. Furthermore, we train three different neural network architectures with empirical-risk-minimization \autocite{vapnik1999overview}: A ResNet based architecture which we denote by ERM \autocite{he2016deep}, a Vision Transformer \autocite{dosovitskiy2020image} and a Swin Transformer \autocite{liu2021swin}. 
Training details and hyperparameter settings are listed in \autoref{app:training}. 

These models are trained on six domain generalization data sets from the DomainBed repository \autocite{gulrajani2020search}: PACS \autocite{li2017deeper}, OfficeHome \autocite{venkateswara2017deep}, VLCS \autocite{fang2013unbiased}, TerraIncognita \autocite{beery2018recognition}, DomainNet \autocite{peng2019moment} and SVIRO \autocite{cruz2020sviro}. Each DG data set consists of four to ten different domains from which we construct different DG tasks: We train a classifier on all but one domain. The one left out during training is then the OOD test domain where the competence region is evaluated. As an example consider the DG task behind the earlier example in \autoref{fig:img_surprisal}: If we train a model on the domains Photos, Art images, and Sketches, the DG task asks for an accurate model on the domain Cartoons which constitute the OOD test domain (see \autoref{fig:img_surprisal}). Overall we consider 32 DG tasks which result in 288 trained networks. On each of these models we compute various incompetence scores via a number of \textit{post-hoc} methods.
The \textit{post-hoc} methods used in this paper can be grouped into the following categories:
\begin{itemize}
     \item Feature-based: Virtual-logit Matching \autocite[ViM;][]{ood-vim}, Deep-KNN \autocite[Deep-KNN;][]{ood-sun2022out};
    \item Density-based: Gaussian mixture models (GMM), minimum Mahalanobis distance between features and class-wise centroids  \autocite[Mahalanobis;][]{ood-mahalanobis};
    \item Reconstruction-based: reconstruction error of PCA in feature space \autocite{aggarwal2017introduction};
    \item Logit-based: energy score \autocite[Energy;][]{ood-energy}, maximum logit \autocite[Logit;][]{ood-maxlogitKL}, maximum softmax \autocite[Softmax;][]{ood-softmax}, and energy-react \autocite[Energy-React;][]{ood-energyReact}.
\end{itemize}
Note, that we interpret higher scores as indicative of incompetence (e.g., we consider the negative of the maximum softmax and the maximum logit).

\begin{table*}[tbh]
    \begin{center}
    \resizebox{\textwidth}{!}{\input{result-table.tex}}
    \end{center}
    \caption{Accuracy on competence region of OOD domain for different domain generalization data sets and incompetence scores. As the threshold for the competence regions, we choose the 95\% percentile of the ID validation set.  
    For all metrics, a higher value means better performance ($\uparrow$). All displayed values are medians over different domain roles and classifiers, brackets indicate 90\% confidence interval.}
    \label{tab:main}
\end{table*}





For each DG task, we distinguish four data sets. 
For the ID distribution, we consider a training set, a validation set for hyperparameter optimization, and a test set that has no influence on the optimization process for the subsequent evaluation. 
The classifiers are trained on the ID training set. We compute the scores for the ID distribution on the ID validation set and the ID accuracy on the ID test set. 
The OOD test set is given by the DG task (e.g., as in \autoref{fig:img_surprisal}). 
After training, we apply all post-hoc methods to the penultimate feature layer or the ouput (logits) layer of the classifier, as is typical in the OOD detection literature. 
If the post-hoc method needs to fit the data (as for instance with GMMs), we fit the score function on the ID training data. 

\subsection{Competence Threshold}
\label{sec:choosing-competence-threshold}


In this section, we analyze the performance of the classifiers as a function of the threshold $\alpha$ which determines their competence region (see \autoref{eq:competence-region} in \autoref{sec:method}).
To this end, we compute the incompetence scores on the ID validation data set and on all OOD data samples. 

\autoref{fig:histo_g_alpha} depicts the resulting score distributions and accuracy $\mathrm A_{\operatorname{OOD}}(\alpha)$ as a function of the threshold~$\alpha$ for a single classifier (ERM). 
Here, we consider four incompetence scores on one of the DG tasks provided by PACS and TerraIncognita, respectively. 
We find that the considered incompetence scores fulfill the requirement for a competence detector in \autoref{def:competence-detector} that the accuracy must decrease monotonically as the threshold $\alpha$ increases. %
We then find the theoretical results in \autoref{sec:method} confirmed: For low $\alpha$, the accuracy $\mathrm A_{\operatorname{OOD}}(\alpha)$ is high, and even exceeds the average accuracy on the ID data $\mathrm A_{\operatorname{ID}}$ (see \autoref{prop:main} (a)). 
It eventually decreases until $\mathrm A_{\operatorname{OOD}}(\alpha) \to \mathrm A_{\operatorname{OOD}}$ for large $\alpha$ (see~\autoref{prop:main}b).

\autoref{fig:histo_g_alpha} also depicts the fraction of ID and OOD data that is considered (i.e., not rejected) as we increase the incompetence threshold $\alpha$:
\begin{align}\label{eq:frac}
\operatorname{Coverage}_{\operatorname{dist}}(\alpha) = \frac{|\mathcal D_{\operatorname{dist}} \cap X_{c}(\alpha)|}{|\mathcal D_{\operatorname{dist}}|}.
\end{align}
For instance, we can compare the methods at the $\alpha_{95}$ which includes 95\% of the ID data (vertical gray line in \autoref{fig:histo_g_alpha}). 
Here, Logit keeps a significantly larger fraction of test data compared to the other incompetence scores. However, this results in a lower accuracy in the competence region $\mathrm A_{\operatorname{OOD}}(\alpha_{95})$.

Unfortunately, due to the nature of the DG problem, the accuracy curve $\mathrm A_{\operatorname{OOD}}(\alpha)$ in \autoref{fig:histo_g_alpha} is not accessible during inference, which makes it difficult to choose a suitable threshold $\alpha$. 
In \autoref{fig:histo_g_alpha} we can observe that ViM and KNN achieve at the 95\% percentile (w.r.t. the ID validation set) an accuracy that is comparable to the ID accuracy, rendering the predictions in this competence region very accurate and trustworthy. 
GMM and Logit obtain very high accuracies in the competence region $X_{c}(\alpha)\cap \mathcal{D}_\textrm{OOD}$ for small $\alpha$ values, but exhibit a larger drop in accuracy at the 95\% percentile (w.r.t. the ID distribution). 
We show the accuracies $\mathrm A_{\operatorname{OOD}}(\alpha)$ for different threshold values $\alpha$ for all data sets and DG tasks in \autoref{app:dependence_threshold}.


\subsection{Extensive Survey}
\label{sec:extensive_results}

In the following, we evaluate all nine incompetence scores on all six DG data sets using the nine classifiers. 
Since each data set features 32 different DG tasks, we perform a total of $32\cdot 9 \cdot 9 =2592$ experiments. 
For each experiment, we obtain accuracy curves as in \autoref{fig:img_surprisal} as a function of $\alpha$. 
To summarize and compare the performance of each score on each data set, we need to deal with the trade-off between accuracy and coverage. 
Thus, we measure accuracy $\mathrm A_{\operatorname{OOD}}(\alpha_{95})$ at the score $\alpha_{95}$, such that 95\% of ID validation data fall below this threshold, that is $\operatorname{Frac}_{\operatorname{ID}}(\alpha_{95}) = 95\%$. %
As mentioned in \autoref{sec:choosing-competence-threshold}, choosing $\alpha$ in DG is notoriously difficult, since we have no access to the test domain(s) during training. 
The following quantities provide useful summary statistics for comparing our results across all experiments:

\begin{enumerate}
    \item OOD-Gain $= \mathrm A_{\operatorname{OOD}}(\alpha_{95}) - \mathrm A_{\operatorname{OOD}}$: The performance gain in the OOD domain by considering only the data in the competence region $X_{c}(\alpha_{95})$.
    \item ID-Gain$= \mathrm A_{\operatorname{OOD}}(\alpha_{95}) - \mathrm A_{\operatorname{ID}}$: Expresses the performance gap between the accuracy on OOD data in the competence region $X_{c}(\alpha_{95})$ and the accuracy on the entire ID data $\mathrm A_{\operatorname{ID}}$.
    \item Coverage $= \operatorname{Coverage}_{\operatorname{OOD}}(\alpha_{95})$ as given by \autoref{eq:frac}: The proportion of OOD data that falls within the competence region.
\end{enumerate}

\noindent For each quantity, a higher value indicates better performance ($\uparrow$). 
Note that the coverage of the competence region alone is not informative. 
A naive approach that includes all samples in the competence region would achieve the largest competence region but would fall short in terms of OOD-Gain or ID-Gain. 

\autoref{tab:main} summarizes the results from our extensive sweep over classifiers, data sets, and incompetence scores. 
The displayed values are the medians over different domain roles and classifiers.

Overall, we observe that in the competence region, higher accuracy is achieved compared to the naive application on all OOD data instances. 
This confirms that incompetence score and accuracy are indeed tightly linked. 
However, for most DG data sets and incompetence scores, we are not able to replicate the ID accuracy. 
This indicates that we cannot naively expect the classifier to attain the same accuracy as observed in the ID distribution in the 95\% percentile $\alpha_{95}$. 
Further important findings are:
\begin{itemize}
    \item In general, feature-based (Deep-KNN, ViM), as well as logit-based incompetence scores (Softmax, Logit, Energy, Energy-React) obtain significantly higher accuracy on OOD data (higher OOD-Gain) by filtering the data to the competence region $X_c(\alpha_{95})$ than the density- and reconstruction-based approaches (Mahalanobis, GMM, PCA).
    \item The feature-based scores achieve a significant performance boost on TerraIncognita. TerraIncognita contains DG tasks that suffer from a particularly huge drop in accuracy from ID to OOD distribution (see \autoref{app:training}). %
    \item The proportion of OOD data that falls inside the competence region (i.e., coverage) is smallest for feature-based methods, but they also provide the highest accuracy across all DG data sets.
\end{itemize}


\begin{figure*}[bht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/acc_boxplots.pdf}
    \caption{Performance of Logit and Softmax scores (logit-based) against Deep-KNN and ViM (feature-based) for an increasing fraction of open world data (unknown classes) in the test domain. The performance gain on the OOD data (\textit{OOD-Gain}, higher is better) for the logit-based methods is less pronounced compared to ViM and Deep-KNN.}
    \label{fig:boxplot_openWorld}
\end{figure*}

\subsection{Open World Performance}

In this section, we study how different incompetence scores shape the competence region when instances of unknown classes are present in the ID distribution. 
Accordingly, for each domain in PACS, VLCS, Office-Home, and TerraIncognita data sets, we create a matching ``open world`` domain containing only instances of unknown classes. In total, we create 16 open world domains.
For example, if we evaluate a model on the PACS Sketch domain, we create an open world domain containing only sketches of classes that are not in the PACS data set. 
We describe the procedure for creating the open world domains in detail in \autoref{app:open_world}. 
In the following, we restrict our analysis to the 16 domains for which an open world twin exists.

We enrich the existing test domains with 0\%, 5\%, 10\%, 15\%, 20\%, and 25\% instances with unknown classes. 
A good incompetence score should mark instances of unknown classes with a high value and therefore render them outside of the competence region $X_{c}(\alpha_{95})$. 
In this case, the OOD-Gain would increase as more open world instances find their way into the test set. 
In \autoref{fig:boxplot_openWorld} we observe that this behavior is achieved particularly well for the ViM score. 
The Logit and Softmax scores are less successful in delineating unknown class instances from the competence region and therefore the OOD-Gain is less pronounced. 

\begin{figure*}[bht]
     \centering
     \begin{subfigure}[b]{1.00\textwidth}
         \centering
         \includegraphics[width=.95\textwidth]{figures/acc_boxplots_tentative.pdf}
     \end{subfigure}
     \begin{subfigure}[b]{1.00
\textwidth}
         \centering
         \includegraphics[width=.95\textwidth]{figures/frac_boxplots_tentative.pdf}
     \end{subfigure}%
     \caption{ID-Gain and Coverage for Logit and ViM if transformed as described in \autoref{sec:tentative_solution}. Threshold is set such that the ID-Gain should be at least 0. \textit{Above:} ID-Gain for Logit and ViM due to different data sets. \textit{Below:} Coverage for Logit and ViM due to different datasets. Medians and quantiles are in both figures over different domain roles and classifiers. The threshold is set as the ID accuracy.}
    \label{fig:tentative_solution}
\end{figure*}

Indeed, to test whether this observation holds statistically across all classifiers, we fit a hierarchical linear regression \autocite{stephen2002hierarchical} on OOD Gain with \texttt{Classifier}, \texttt{Percentage Open World}, and \texttt{Incompetence Score} as well as their interactions as fixed factors, together with \texttt{Data Set} and \texttt{Test Domain} as random factors (to account for the fact that the same classifier is evaluated in multiple data sets and test domains).
The statistical results confirm the general trends visible in \autoref{fig:boxplot_openWorld}. 
First, we find significant main effects of \texttt{Percentage Open World} (i.e., overall OOD Gain increases with increasing number of open world instances) and \texttt{Incompetence Score} (i.e., ViM and Deep-KNN achieve a higher overall OOD Gain).
Importantly, the only significant interaction revealed by the hierarchical regression model suggests that ViM is able to achieve the largest OOD Gain as the fraction of open world samples increases.
Note, that the same trend is present for Deep-KNN, but it fails to achieve statistical significance due to its high variability (see~\autoref{fig:boxplot_openWorld}).
Moreover, none of the effects involving the factor \texttt{Classifier} turn out to be significant predictors of OOD Gain, suggesting that the results are largely classifier-independent. 

In the closed world setting differences between logit- and feature-based scores are for most DG data sets small (see e.g. \autoref{tab:main}). However, we have shown that it is very relevant in the setting where instances of unknown classes occur. 
In \autoref{app:open_world} we show the open world behavior for all incompetence scores. 

\subsection{Estimating the Incompetence Threshold}
\label{sec:tentative_solution}
Choosing the 95\% percentile of the ID distribution as incompetence threshold can be considered as weighting the trade-off between accuracy and coverage towards coverage -- only 5\% of ID data are rejected. 
We now seek a slightly different incompetence threshold which puts more weight on the accuracy. 
The question we want to address is whether \textit{we can set a threshold such that a certain accuracy is achieved in the competence region?} 
This question is of high practical relevance, but also particularly challenging for two reasons. 
First, many scores used so far have no out-of-the-box connection to the accuracy and second, we deal with a domain shift that might result in a completely new score-accuracy relationship.

Thus, as a potential remedy, we suggest learning $\widetilde{s}_c(x) =p_{\operatorname{ID}}(c(x) \ne y \mid s_c(x))$ and using this conditional probability as a \textit{transformed} score. This score represents the probability of an incorrect prediction given the original score. 
If we define a competence region with an incompetence threshold of $1- \operatorname{A}_{\operatorname{ID}}$, we can expect an accuracy of at least $\operatorname{A}_{\operatorname{ID}}$ on ID data. 
We hope that this relation also holds under domain shift. 
To predict $\widetilde{s}_c(x) =p_{\operatorname{ID}}(c(x) \ne y \mid s_c(x))$ we rely on an architecture that is constrained to be monotonic as proposed in \autocite{muller2021learning}. Therefore, we do not change the order of the scores and equip the transformed score with an inductive bias that is consistent with \autoref{def:competence-detector}. The transformed score also has a predictable extrapolation behavior which is helpful when the distribution shifts. 
Note that since the transformation is monotonic, a threshold for the original score is also a valid threshold for the transformed score and \textit{vice versa}. Therefore, we can also interpret this approach as estimating an incompetence threshold such that a certain accuracy is achieved.

Accordingly, \autoref{fig:tentative_solution} depicts the ID-Gain and Coverage for ViM and Logit (transformed), if we select $1 - \operatorname{A}_{\operatorname{ID}}$ as the incompetence threshold. 
The transformed ViM score suggests that we achieve in most cases at least the ID accuracy, but at the cost of small coverage. The transformed Logit score has higher coverage, but it often fails to reproduce the ID accuracy (e.g., in the TerraIncognita data set). 
However, while we attain the ID accuracy for most cases, we still observe some failure cases, which makes the approach only tentative.
Note, that these results also suggest that the information contained in the logits is not sufficient to give suitable competence regions in the sense of our question.

\section{Conclusion}

Accepting only predictions from the competence region of a classifier increases its accuracy dramatically under domain shift. Determining the fraction of samples where the classifier could be considered competent is a question of how to weigh the trade-off between accuracy and coverage. 
Choosing this trade-off via the incompetence threshold is application dependent and particularly challenging in the domain generalization (DG) setting  where the test distribution is different from the training distribution. We showed that even in DG, it is possible to achieve higher than ID accuracy under domain shift -- at the  price of potentially little coverage (see \autoref{fig:img_surprisal} or \autoref{app:dependence_threshold}). 

We investigated a coverage-oriented threshold that would reject only 5\% of all instances from the training distribution. 
In this case, we achieved a considerable improvement under distribution shift compared to a naive application where no samples are rejected. 
However, at this particular threshold, we could recover the ID accuracy only in some settings. 
Thus, we also studied whether we can learn an accuracy-oriented threshold where some predefined ID accuracy is guaranteed in the competence region.
This approach was able to replicate the ID accuracy in the competence region for most investigated domain shifts. 
However, for a few domains, OOD accuracy drops significantly below the expected ID, calling for a more detailed understanding of the behavior of incompetence scores in DG. 
Nonetheless, we showed that the accuracy of the competence region behaves monotonically with the threshold $\alpha$ (see \autoref{prop:main} and \autoref{sec:choosing-competence-threshold}).
 
Furthermore, we investigated differences between the closed and open world settings. 
We found that in the open world setting, feature-based methods, such as Deep-KNN \autocite{ood-sun2022out} and ViM \autocite{ood-vim}, elicit a particularly useful competence region. 
In a closed world DG setting, a clear winner does not emerge, but ViM and Deep-KNN seem to be competitive to logit-based approaches.
We also analyzed whether we could find differences in the accuracy of the competence region with respect to different classifiers. 
We could not find statistically significant effects on the accuracy in the competence region, leaving the benefit of robust algorithms for DG and different architectures for enlarged competence regions questionable.

All methods are comparably fast to evaluate and therefore easily accessible for practitioners. 
However, the resolution of the trade-off between accuracy and coverage is not yet satisfactory in all cases, calling for more research on better competence scores.






\clearpage
\section*{References}
\printbibliography[heading=none]

\newpage
\appendix
\input{appendix.tex}

\end{document}