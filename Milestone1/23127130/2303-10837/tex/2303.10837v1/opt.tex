\section{Optimizations}
\label{sec:opt}

It is widely acknowledged that HE inevitably introduces large overheads regarding both computation and communication~\cite{gouert2022new}.

\begin{figure}
\includegraphics[width=0.47\textwidth]{figs/model_comp.pdf}
\includegraphics[width=0.47\textwidth]{figs/model_comm.pdf}
\caption{(w/o Optimization) Computational (up) and Computation (bottom) Overhead Comparison For Models of Different Sizes: FedML-HE vs. Nvidia FLARE  vs. Plaintext Aggregation. Due to TenSeal's larger file sizes, FLARE did not finish the run on BERT on our 32GB memory machine (exceeding memory).}
\label{fig:comp}
\end{figure}

%yuhang: go on kk

% \begin{figure}
% \includegraphics[width=0.47\textwidth]{figs/model_comm.pdf}
% \caption{(w/o Optimization) Communicational Overhead For Models of Different Sizes: FedML-HE vs. Nvidia FLARE vs. Plaintext Aggregation}
% \label{fig:comm}
% \end{figure}

\textit{Observation}: The computational and communicational (package size) overheads introduced by HE are $O(n)$, both growing linearly with the input size $n$, which in our case the sizes of the models for aggregation. This is also shown by the evaluation results in Figure~\ref{fig:comp} %and Figure~\ref{fig:comm} 
(also Table~\ref{tab:models} in \tsref{sec:models}). Empirically, it typically has $10\times$ more computation overhead and $15\times$ more communication overhead than the plaintext FL.
%\yuhang{The computation cost and the communication cost (file size) are $O(n)$, both grows linearly with the model size $n$.} 
%\carlee{is there existing theoretical analysis on HE overheads to support this observation? E.g., big-O runtime bounds?} 

With this observation, we focus on reducing the input model sizes for HE functions as the primary optimization direction, which reduces the overheads of computation and communication simultaneously without loss of general usability and privacy. The input model sizes are determined in two stages: the resulting model parameters from the ML pipeline and the chosen model parameters used in HE functions.

\begin{figure}
\includegraphics[width=0.45\textwidth]{figs/opt_scheme_fixed.pdf}
\caption{A Universal Overhead Optimization Scheme: Parameter Efficiency x Parameter Selection. Reducing the number of model parameters and selectively encrypting them significantly reduces FedML-HE's overhead while preserving privacy.}
\vspace{-0.4cm}
\label{fig:opt_scheme}
\end{figure}

Thus, we propose a universal overhead optimization scheme. As shown in Figure~\ref{fig:opt_scheme}, input model sizes of HE can be optimized in two steps: (1) \textbf{Parameter Efficiency} to reduce the model sizes from an ML perspective and (2) \textbf{Parameter Selection} to selectively choose a certain portion of the model as inputs for HE functions while the rest of the model undergoes a plaintext aggregation at a certain privacy level. The optimization scheme can be defined as $r_{total} = r_{PE} \times r_{PS}$, where $r_{total}$ is the total optimization rate, $r_{PE}$ is the optimization rate from \textbf{Parameter Efficiency} and $r_{PS}$ is the optimization rate from \textbf{Parameter Selection}.

In the rest of \tsref{sec:opt}, we will discuss in detail how we can optimize overheads in these two steps. 

\begin{figure*}[ht]
\includegraphics[width=1\textwidth]{figs/mask_fixed.pdf}
\caption{Parameter Selection: in the initialization stage, clients first calculate privacy sensitivities on the model using its own dataset and local sensitivities will be securely aggregated to a global model privacy map. The encryption mask will be then determined by the privacy map and a set selection value $s$ per overhead requirements and privacy guarantee. Only the masked parameters will be aggregated in the encrypted form.}
\label{fig:mask}
\end{figure*}

\subsection{Parameter Efficiency}
In general, there are two directions to downsize the communicated models from an ML perspective: \textbf{Model Compression} to minimize the parameters from a local model upon communication and \textbf{Parameter-Efficient Tuning} to find a small set of parameters to efficiently tune large models. Although we here introduce several efficiency optimization techniques, further efficiency techniques can also be easily integrated into our scheme.

\noindent\textbf{Model Compression} is the typical method to reduce the number of communication parameters when training from scratch, which compresses the model to have a smaller size for communication~\cite{tang2019doublesqueeze, alistarh2017qsgd, wang2018atomo, aji2017sparse}. It may include sparsification (only communicate parameters with relatively higher weights), quantization (reduce the number of bits required to store the weights), and low-rank decomposition. The compression technique can largely reduce the number of parameters or the number of bits for each parameter to be encrypted and homomorphically aggregated. %\carlee{how does quantization do this? does it make HE more efficient?}

\noindent\textbf{Parameter-Efficient Tuning} is widely used for fine-tuning large models. For example, LoRA~\cite{hu2021lora} adjusts lightweight trainable parameters while keeping most pre-trained parameters frozen. Under this tuning scheme, only a small amount of trainable parameters will be shared in federated learning for HE.



\subsection{Parameter Selection}

Fully encrypted models can guarantee no access to plaintext local models from the adversary. However, previous work on privacy leakage analysis shows that ``partial transparency'', e.g. hiding parts of the models~\cite{hatamizadeh2022gradvit, mo2020layer}, can grant the adversary a limited chance to successfully perform attacks like gradient inversion attacks~\cite{lu2022april}. Although FLARE~\cite{roth2022nvidia} code provides a configuration to only perform HE operations on certain layers, its interaction and tradeoffs regarding privacy leakage are unknown, leaving users a ``Pandora's privacy box''. On the contrary, to clarify, our solution can support parameter-wise selection for more fine-grained overhead control with privacy leakage analysis (Figure~\ref{fig:mask}).

%We observe that transformers with (i) a smaller patch size, (ii) more parameters, and (iii) stronger training recipe with distillation, reveal more original information ", for transformer specifically, Component-wise, ", reconstructions from MLP gradients lack important details, whereas utilizing
%the gradients of MSA layers alone can already yield high-quality reconstructions
\subsubsection{Parameter Privacy Sensitivity Map} 
\hfill\\
Privacy leakage analysis can be done by directly performing gradient inversion attack~\cite{wei2020framework} and evaluating the success rate of the attack, which can take much more time than the model training. Differently, Mutual Information (MI)~\cite{xutheory} is proposed to calculate the usable information of model parameters while is practically similar to computing the area under the curve of attack models with even higher computation cost~\cite{mo2020layer}.

Sensitivity is then adopted for measuring the general privacy risk on gradients w.r.t. input, high-level features, or output~\cite{novaksensitivity, sokolic2017robust, mo2020layer}. Theoretically, if the privacy sensitivity of the plaintext parameters obtained by $\mathcal{A}$ is lower, the success rate of the attack can be also expected to be lower. Given model $\mathbf{W}$ and $K$ data samples with input matrix $\mathbf{X}$ and ground truth label vector $\mathbf{y}$, we compute the sensitivity of each parameter $w_m$ by 
$$\frac{1}{K} \sum_{k=1}^K \left\|J_m\left(y_k\right) \right\|,$$ 
where $J_m\left(y_k\right) =\frac{\partial}{\partial y_k}\left(\frac{\partial \ell\left(\mathbf{X}, \mathbf{y}, \mathbf{W}\right)}{\partial w_m}\right) \in R$, $\ell(\cdot)$ is the loss function given $\mathbf{X}$, $\mathbf{y}$ and $\mathbf{W}$, and $\left\|\cdot \right\|$ calculates the absolute value. The intuition is to calculate how large the gradient of the parameter will change with the true output $y_k$ for each data point $k$.


% block $\mathbf{w}_m$ (a 
%  group of parameters) w.r.t the ground truth label $\hat{\mathrm{Y}}$ with 

% $$
% \frac{1}{K \sqrt{|\mathrm{g}_m (\hat{\mathrm{Y}}_k )| }} \sum_{k=1}^K\left\|\frac{\mathbf{J}_m^{(\mathrm{g})}\left(\hat{\mathrm{Y}}_k\right)}{\operatorname{range}\left(\mathrm{g}_m\left(\hat{\mathrm{Y}}_k\right)\right)}\right\|_F,
% $$

% where $\mathbf{J}_m^{(\mathrm{g})}(\hat{\mathrm{Y}})=\frac{\partial \mathrm{g}_m(\hat{\mathrm{Y}})}{\partial \hat{\mathrm{Y}}}=\frac{\partial}{\partial \hat{\mathrm{Y}}}\left(\frac{\partial \ell\left(\hat{\mathrm{Y}}, \boldsymbol{\theta}_m\right)}{\partial \boldsymbol{\theta}_m}\right)$ and $\mathrm{g}_m(.) = \frac{\partial \ell\left(\hat{\mathrm{Y}}, \boldsymbol{\theta}_m\right)}{\partial \boldsymbol{\theta}_m}$ represents the function that produces block $m$ 's gradients $\mathrm{g}_m$ with true output $\hat{\mathrm{Y}}$. $\ell(.)$ is the loss function over $\hat{\mathrm{Y}}$ and $\boldsymbol{\theta}$ (parameters of the complete model). %so $\mathrm{g}_m($.$)$ can be regarded as the partial derivative of $\ell(.)$ w.r.t. block $m$'s parameters $\boldsymbol{\theta}_m$ (i.e. backward propagation). 
% Function range(.) returns the range of values in one vector (i.e. $\max ()-\min ())$. 




\subsubsection{Encryption Mask}
\hfill\\
As the privacy sensitivity analysis above as well as our results show in \tsref{sec:opt_results}, different parts of a model contribute to attacks by revealing uneven amounts of information. Using this insight, we propose to only select and encrypt parts of the model that are more important and susceptible to attacks to reduce HE overheads while maintaining the privacy guarantee. We use encryption masks (EM) to effectively select encrypted parameters. EM works by first pre-calculating the model parameter privacy map via the privacy sensitivity and applying the mask with a set privacy threshold to pick parameters to encrypt that fit certain overhead expectations. 

\subsubsection{Mask Agreement} 
\hfill\\
The encryption mask is dependent on the data it is processed on. With potentially different data distributions at each client, it is challenging to have all clients agree on a certain global encryption mask without revealing local datasets since the local privacy map carries information about the client data. To solve this problem, we encrypted aggregate local privacy maps at the server such that no further private information about local data is revealed. During the initialization, clients first calculate the local privacy sensitivities using their own local data and then the server aggregates local sensitivity maps to a global privacy map (as shown in the first part of Figure~\ref{fig:mask}). The encryption mask is configured using a privacy-overhead threshold $s$ and the global privacy map is then shared among clients as part of the federated learning configuration. 

% In practice, privacy maps are usually sliced into around a hundred parts (i.e. a couple of hundred numbers to aggregate), the HE overhead from this mask agreement process is relatively small (Figure~\ref{fig:pie}).

% \subsubsection{Empirical Selection Recipes} 
% \hfill\\
% We then provide parameter selection strategies: \carlee{explain how these are related}
% \begin{itemize}
%     \item \noindent\textbf{Layer Importance}: Layer selection is a simpler variant of parameter selection. We rank the sensitivity of each layer, then encrypt the most important layers. Empirically, layers in the beginning and end stages tend to be more important for information leakage~\cite{hatamizadeh2022gradvit}.
%     \item \noindent\textbf{Training Stages}: \weizhao{whats the conclusion about this in the literature?}
% \end{itemize}


% \weizhao{Do we still want the parts below? Seems like a jump from selection}

% \begin{itemize}
%     \item \noindent\textbf{Multiple Local Steps}: the gradient inversion attack requires the gradient and the model, while in FL, we usually perform gradient descents with multiple local steps. The gradient of each step is unavailable by communicating the models, which increases the search space of attack. Based on ~\cite{wei2020framework}, when local step $>9$, the attack success rate is low. 
%     \item \noindent\textbf{Larger Batch Size}: Increasing the batch size of gradient descent will make the gradient inversion attack try to reconstruct all data in a batch at the same time, which will also increase the search space. Based on~\cite{huang2021evaluating}, when batch size $>32$, it makes the reconstruction almost unrecognizable.
%     %\item Batch Size~\cite{wei2020framework, huang2021evaluating}
%     %\item \noindent\textbf{Gradient Alternate Hiding}: to perform most gradient inversion attacks, the adversary has to calculate the gradients between each FL round's model updates. Alternately encrypting models in certain rounds can make it harder for the adversary to compute the fine-grained gradients at the first stage of attacks.
%     %\item \noindent\textbf{Training Round Guarding}: beginning rounds are more important to reduce attack effect since gradients change at bigger steps per training data.
%     %\item Larger Batch Size
% \end{itemize}
% We can then provide a simple empirical recipe, by \emph{encrypting 10\% model parameters and performing gradient descent with batch size $= 64$ and local step $=10$}, every single component can make attacks infeasible and all these components together can provide a good solution for preserving the privacy with low communication overhead.

%\weizhao{mathematical proofs here: 1. different layer importance 2. different training stage}

%\yuhang{1. Attack by masking a few layers or parameters is feasible? 2. Attacks on different layers 3. Attacks on different parameters 4. Attack on different training round. 5. Attack with accumulated training information? 6. Attack on multiple local training steps}




% \subsection{Robustness Mechanism}
% maybe?