\section{Evaluation}
In this section, we first evaluate the vanilla (without optimization) version of our framework's performance with an empathized focus on diagnosing the overheads (both in computation and communication) introduced by Homomorphic Encryption. Later in \tsref{sec:opt_results}, we demonstrate the evaluation results to show how our proposed universal optimization scheme largely mitigates these overheads for real-world deployment.

\subsection{Experiment Setup}
\noindent\textbf{Models:} we tested our framework on models in different ML domains with different sizes (Table~\ref{tab:models}).

\noindent\textbf{Dataset:} MNIST dataset ($70k$ images) for experiments and the CIFAR-100 dataset ($50k$ images).

\noindent\textbf{HE Libraries:} we implement our HE core using both PALISADE and TenSEAL. Unless otherwise specified, our results show the evaluation of the PALISADE version (faster and with more HE functionalities). 

\noindent\textbf{Default Crypto Parameters:} unless otherwise specified, we choose the multiplicative depth of 1, the scaling factor bit digit of 52, an HE packing batch size of 4096, and a securityLevel of 128 as our default HE cryptographic parameters during the evaluation.

\noindent\textbf{Machine:} we use an Intel 8-core 3.60GHz i7-7700 CPU with 32 GB memory and an NVIDIA Tesla T4 GPU on Ubuntu 18.04.6 LTS (Docker Env) for primarily microbenchmarking overheads of the HE operations.

% We provided detailed experimental instructions in Appendix~\ref{sec:tutorial}.


\begin{table*}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline\hline
        Model & Task & Model Size & \makecell[l]{HE\\ Time (s)} & \makecell[l]{Non-HE\\ Time (s)} & \makecell[l]{Comp\\Ratio} & Ciphertext & Plaintext& \makecell[l]{Comm\\Ratio}  \\ \hline\hline
        Linear Model & Regression & 101 & 0.216 & 0.001 & 150.85 & 266.00 KB & 1.10 KB& 240.83 \\ \hline
        \makecell[l]{TimeSeries\\Transformer} & Time Series & 5,609 & 2.792 & 0.233 & 12.00 & 532.00 KB & 52.65 KB& 10.10 \\ \hline
        MLP (2 FC) & \makecell[l]{Classification\\ Regression} & 79,510 & 0.586 & 0.010 & 60.46 & 5.20 MB& 311.98 KB & 17.05 \\ \hline
        LeNet & CV & 88,648 & 0.619 & 0.011 & 57.95 & 5.97 MB& 349.52 KB & 17.50 \\ \hline
        \makecell{RNN(2 LSTM\\ + 1 FC)} & NLP & 822,570 & 1.195 & 0.013 & 91.82 & 52.47 MB& 3.14 MB& 16.70 \\ \hline
        \makecell{CNN (2 Conv\\ + 2 FC)} & CV & 1,663,370 & 2.456 & 0.058 & 42.23 & 103.15 MB& 6.35 MB& 16.66 \\ \hline
        MobileNet & CV & 3,315,428 & 9.481 & 1.031 & 9.20 & 210.41 MB& 12.79 MB& 16.45 \\ \hline
        ResNet-18 & CV & 12,556,426 & 19.950 & 1.100 & 18.14 & 796.70 MB& 47.98 MB& 16.61 \\ \hline
        ResNet-34 & CV & 21,797,672 & 37.555 & 2.925 & 12.84 & 1.35 GB& 83.28 MB& 16.60 \\ \hline
        ResNet-50 & CV & 25,557,032 & 46.672 & 5.379 & 8.68 & 1.58 GB& 97.79 MB& 16.58 \\ \hline
        GroupViT & Multi Modal & 55,726,609 & 86.098 & 19.921 & 4.32 & 3.45 GB& 212.83 MB& 16.61 \\ \hline
        \makecell{Vision\\ Transformer} & CV & 86,389,248 & 112.504 & 17.739 & 6.34 & 5.35 GB& 329.62 MB&16.62 \\ \hline
        BERT & NLP & 109,482,240 & 136.914 & 19.674 & 6.96 & 6.78 GB& 417.72 MB& 16.62 \\ \hline
    \end{tabular}
    \caption{Vanilla Fully-Encrypted Models of Different Sizes: with $3$ clients; Comp Ratio is calculated by time costs of HE over time costs of Non-HE; Comm Ratio is calculated by file sizes of HE over file sizes of Non-HE. CKKS is configured with default crypto parameters.}
    \label{tab:models}
\end{table*}

\subsection{General HE Impacts On Overheads}
\label{sec:he-benchmark}
% \carlee{put a summary here of this section and the research questions it answers--all of this is interesting but having 5 subsubsections makes it hard to absorb. Maybe even just list the factors whose impacts you evaluate and why they are important.}\weizhao{added}
\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline\hline
        \makecell{HE\\Batch\\Size} & \makecell{Scaling\\Bits} & \makecell{Comp\\(s)} & \makecell{Comm\\(MB)} & \makecell{Model Test \\Accuracy \\$\Delta$ (\%)} \\ 
        \hline\hline
        1024 & 14 & 8.834 & 407.47 & -0.28 \\ \hline
        1024 & 20 & 7.524 & 407.47 & -0.21 \\ \hline
        1024 & 33 & 7.536 & 407.47 & 0 \\ \hline
        1024 & 40 & 7.765 & 407.47 & 0 \\ \hline
        1024 & 52 & 7.827 & 407.47 & 0 \\ \hline
        2048 & 14 & 3.449 & 204.50 & -0.06 \\ \hline
        2048 & 20 & 3.414 & 204.50 & -0.13 \\ \hline
        2048 & 33 & 3.499 & 204.50 & 0 \\ \hline
        2048 & 40 & 3.621 & 204.50 & 0 \\ \hline
        2048 & 52 & 3.676 & 204.50 & 0 \\ \hline
        4096 & 14 & 1.837 & 103.15 & -1.85 \\ \hline
        4096 & 20 & 1.819 & 103.15 & 0.32 \\ \hline
        4096 & 33 & 1.886 & 103.15 & 0 \\ \hline
        4096 & 40 & 1.998 & 103.15 & 0 \\ \hline
        4096 & 52 & 1.926 & 103.15 & 0 \\ \hline
    \end{tabular}
\caption{Computational \& Communicational Overhead of Different Crypto Parameter Setups: tested with CNN (2 Conv+ 2 FC) and on 3 clients; model test accuracy $\Delta$s is the difference between the best plaintext global model and the best global encrypted global models.}
\vspace{-0.5cm}
\label{tab:params}
\end{table}

We evaluate the HE-based training overheads (without our optimization in place) across various FL training scenarios and configurations. This analysis covers diverse model scales, HE cryptographic parameter configurations, client quantities involved in the task, and communication bandwidths. This helps us to identify bottlenecks in the HE process throughout the entire training cycle. We also benchmark our framework against other open-source HE solutions to demonstrate its advantages.


\subsubsection{Results on Different Scales of Models}
\label{sec:models}
\hfill\\
We evaluate our framework on models with different size scales and different domains, from small models like the linear model to large foundation models such as Vision Transformer~\cite{dosovitskiy2020image} and BERT~\cite{devlin2018bert}. As Table~\ref{tab:models}, Figure~\ref{fig:comp} show, both computational and communicational overheads are generally proportional to model sizes.

Table~\ref{tab:models} illustrates more clearly the overhead increase from the plaintext federated aggregation. The computation fold ratio is in general $5$x $\sim 20$x while the communication overhead can jump to a common $15$x. Small models tend to have a higher computational overhead ratio increase. This is mainly due to the standard HE initialization process, which plays a more significant role when compared to the plaintext cost. The communication cost increase is significant for models with sizes smaller than $4096$ (the packing batch size) numbers. Recall that the way our HE core packs encrypted numbers makes an array whose size is smaller than the packing batch size still requires a full ciphertext.

% \begin{figure}[ht]
% \includegraphics[width=0.4\textwidth]{figs/model_ratio.pdf}
% \caption{(Non-Optimization) Computational \& Communicational Overhead Increase: fold ratio from Plaintext Aggregation to FedML-HE.}
% \label{fig:ratio}
% \end{figure}



\subsubsection{Results on Different Cryptographic Parameters}
\hfill\\
We evaluate the impacts of variously-configured cryptographic parameters. We primarily look into the packing batch size and the scaling bits. The packing batch size determines the number of slots packed in a single ciphertext while the scaling bit number affects the ``accuracy'' (i.e., how close the decrypted ciphertext result is to the plaintext result) of approximate numbers represented from integers. 

From Table~\ref{tab:params}, the large packing batch sizes in general result in faster computation speeds and smaller overall ciphertext files attributed to the packing mechanism for more efficiency. However, the scaling factor number has an almost negligible impact on overheads.
\begin{figure*}[htp]

\centering
\includegraphics[width=0.333\textwidth]{figs/pie-plain.pdf}\hfill
\includegraphics[width=0.333\textwidth]{figs/pie-fhe.pdf}\hfill
\includegraphics[width=0.333\textwidth]{figs/pie-he-opt.pdf}

\caption{Time Distribution of A Non-HE Cycle on ResNet-50: assume an expected bandwidth of $200$ MB/s and fixed local training time for plaintext FL (left), HE w/o optimization (middle), and HE w/ optimization (middle). Optimization setup uses \textit{DoubleSqueeze} with $k=1,000,000$ and encryption mask with $s = 30\%$.}
\label{fig:pie}

\end{figure*}

Unsurprisingly, it aligns with the intuition that the higher bit scaling number results in higher ``accuracy'' of the decrypted ciphertext value, which generally means the encrypted aggregated model would have a close model test performance to the plaintext aggregated model. However, it is worth mentioning that since CKKS is an approximate scheme with noises, the decrypted aggregated model can yield either positive or negative model test accuracy $\Delta$s, but usually with a negative or nearly zero $\Delta$.

\begin{table*}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline\hline
        Frameworks &HE Core& Key Management & Deployment & Comp (s) & \makecell{Comm \\(MB)} & \makecell{HE\\Multi-Party\\ Functionalities}\\ \hline
        \hline
        Ours &PALISADE & \makecell{\cmark} & \cmark & 2.456 & 105.72  & \makecell{PRE,\\ ThHE}\\
                \hline
        Ours (w/ Opt) &PALISADE & \makecell{\cmark} & \cmark & 0.874 & 16.37  & \makecell{PRE,\\ ThHE}\\
                \hline    
        Ours &SEAL (TenSEAL) & \makecell{\cmark} & \cmark & 3.989 & 129.75  & ---\\\hline
        \makecell{Nvidia FLARE\\(9a1b226)} &SEAL (TenSEAL)& \makecell{\cmark} & \makecell{NVFLARE\\ Dashboard} & 2.826 & 129.75  &---\\ \hline
        \makecell{IBMFL\\(8c8ab11)}&SEAL (HELayers) & $\bigcirc$ & \makecell{Multi-Cloud\\ OpenShift\\ Orchestrator} & 3.955 & 86.58  & ---\\ \hline
        
        Plaintext&--- & --- & --- & 0.058 & 6.35  & ---\\ \hline
    \end{tabular}
    \caption{Different Frameworks: tested with CNN (2 Conv
+ 2 FC) and on 3 clients; Github commit IDs are specified. For key management, our work uses a key authority server; FLARE uses a security content manager; IBMFL currently provides a local simulator.}
    \label{tab:frameworks}
\end{table*}

\begin{figure}[ht]
\includegraphics[width=0.47\textwidth]{figs/client_number_200_step.pdf}
\caption{Step Breakdown of HE Computational Cost vs. Number of Clients (Up to 200): tested on fully-encrypted CNN}
\vspace{-0.5cm}
\label{fig:n_200}
\end{figure}

\subsubsection{Impact from Number of Clients}
\hfill\\
As real-world systems often experience a dynamic amount of participants within the FL system, we evaluate the overhead shift over the change in the number of clients. Figure~\ref{fig:n_200} breaks down the cost distribution as the number of clients increases. With a growing number of clients, it also means proportionally-added ciphertexts as inputs to the secure aggregation function thus the major impact is cast on the server. When the server is overloaded, our system also supports client selection to remove certain clients without largely degrading model performance.


\subsubsection{Communication Cost on Different Bandwidths}
\hfill\\
\begin{figure}
\includegraphics[width=0.47\textwidth]{figs/comm_bar.pdf}
\caption{Impact of Different Bandwidths on Communication and Training Cycles on Fully-Encrypted ResNet-50: HE means HE-enabled training and Non means plaintext. Others include all other procedures except communication during training. Percentages represent the portion of communication cost in the entire training cycle.}
\vspace{-0.5cm}
\label{fig:comm_bar}
\end{figure}
FL parties can be allocated in different geo-locations which might result in communication bottlenecks. Typically, there are two common scenarios: (inter) data centers and (intra) data centers. In this part, we evaluate the impact of the bandwidths on communication costs and how it affects the FL training cycle. We categorize communication bandwidths using $3$ cases:
\begin{itemize}
    \item Infiniband (IB): communication between intra-center parties. 5 GB/s as the test bandwidth.
    \item Single AWS Region (SAR): communication between inter-center parties but within the same geo-region (within US-WEST). 592 MB/s as the test bandwidth.
        \item Multiple AWS Region (MAR): communication between inter-center parties but across the different geo-region (between US-WEST and EU-NORTH). 15.6 MB/s as the test bandwidth.
\end{itemize}




As shown in Figure~\ref{fig:comm_bar}, we deploy FedML-HE on $3$ different geo-distributed environments, which are operated under different bandwidths. It is obvious that the secure HE functionality has an enormous impact on low-bandwidth environments while medium-to-high-bandwidth environments suffer limited impact from increased communication overhead during training cycles, compared to Non-HE settings.




% \textit{DoubleSqueeze} using Top-k Compression with $k=300,000$.

\subsubsection{Comparison with Other FL-HE Frameworks}
\label{sec:diff_frameworks}
\hfill\\
We compare our framework to the other open-sourced FL frameworks with HE capability, namely NVIDIA FLARE (NVIDIA) and IBMFL. 

Both NVIDIA and IBMFL utilize Microsoft SEAL as the underlying HE core, with NVIDIA using OpenMinded's python tensor wrapper over SEAL and TenSEAL; IBMFL using IBM'spython wrapper over SEAL and HELayers (HELayers also has an HElib version). As \tsref{sec:he_fedml} states that our HE core module can be replaced with different available HE cores, to give a more comprehensive comparison, we also implement a TenSEAL version of our framework for evaluation.

Table~\ref{tab:frameworks} demonstrates the performance summary of different FedML-HE frameworks using an example of a CNN model with $3$ clients. Our PALISADE-powered framework has the smallest computational overhead due to the performance of the PALISADE library. In terms of communication cost, FedML-HE (PALISADE) comes second after IBMFL's smallest file serialization results due to the efficient packing of HELayers' Tile tensors~\cite{aharoni2011helayers}. 

Note that NVIDIA's TenSEAL-based realization is faster than the TenSEAL variant of our system. This is because NVIDIA scales each learner's local model parameters locally rather than weighing ciphertexts on the server. This approach reduces the need for the one multiplication operation usually performed during secure aggregation (recall that HE multiplications are expensive). However, such a setup would not suit the scenario where the central server does not want to reveal its weighing mechanism per each individual local model to learners as it reveals partial (even full in some cases) information about participants in the system.



\begin{figure*}[ht]
\includegraphics[width=1.0\textwidth]{figs/heatmap_lenet.pdf}
\caption{Model Privacy Map Calculated Using Parameter Privacy Sensitivity On LeNet: darker color indicates higher sensitivity. Each subfigure shows the sensitivity of parameters of the current layer. The sensitivity of parameters is imbalanced and many parameters have very little sensitivity (its gradient is hard to be affected by tuning the data input for attack).}
\label{fig:lenet_map}
\end{figure*}

\subsection{Optimizations}
\label{sec:opt_results}



To mitigate the HE overhead surge, our universal optimization scheme works in two stages as described in \tsref{sec:opt}: (1) \textbf{Parameter Efficiency} reduces the shared model parameters and (2) \textbf{Parameter Selection} selects certain portions of parameters for encrypted computation while leaving the rest in plaintext per desired overhead expectations and privacy promise. The overhead reductions from two stages stack as the final optimization.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
        \makecell{Efficiency\\Optimization} & PT& CT & CT (Opt) \\ \hline
        \makecell{ResNet-18 \\(12 M)~\cite{tang2019doublesqueeze}} & 47.98 MB& 796.70 MB & 19.03 MB  \\\hline
        \makecell{BERT\\(110 M)~\cite{hu2021lora}} & 417.72 MB & 6.78 GB & 16.66 MB  \\\hline
    \end{tabular}
    \caption{Parameter Efficiency Overhead: PT means plaintext and CT means ciphertext. Communication reductions are 0.60 and 0.96.}
    \label{tab:eff}
    \vspace{-0.3cm}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        Selection & \makecell{Comp\\(s)} & Comm & \makecell{Comp\\Ratio} & \makecell{Comm\\Ratio} \\ \hline\hline
        
        % Enc w/ L1-4 & 42.546 & 1.97 GB & 2.40 & 6.13 \\ \hline
        % Enc w/ L9-12 & 40.390 & 1.97 GB & 2.28 & 6.13 \\ \hline
        % \makecell{Enc w/ L1-4\\\&L9-12} & 76.739 & 3.62 GB & 4.33 & 11.25 \\ \hline
        % Enc w/ L1-12 & 105.549 & 5.27 GB & 5.95 & 16.38 \\ \hline
        Enc w/ 0\% & 17.739 & 329.62 MB & 1.00 & 1.00 \\ \hline
        Enc w/ 10\% & 30.874 & 844.49 MB & 1.74 & 2.56 \\ \hline
        Enc w/ 30\%  & 50.284 & 1.83 GB & 2.83 & 5.69 \\ \hline
        Enc w/ 50\% & 70.167 & 2.83 GB & 3.96 & 8.81 \\ \hline
        Enc w/ 70\%& 88.904 & 3.84 GB & 5.01 & 11.93 \\ \hline
        Enc w/ All & 112.504 & 5.35 GB & 6.34 & 16.62 \\ \hline
    \end{tabular}
    \caption{Overheads With Different Parameter Selection Configs Tested on Vision Transformer: ``Enc w/ 10\%'' means performs encrypted computation only on 10\% of the parameters; all computation and communication results include overheads from plaintext aggregation for the rest of the parameters.}
    \vspace{-0.3cm}
    \label{tab:selection}
\end{table}


\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{figs/attack_results.pdf}\hfill
\includegraphics[width=0.4\textwidth]{figs/attack_random.pdf}
\caption{Selection Protection Against Gradient Inversion Attack On LeNet: attack results when protecting top-$s$ sensitive parameters (top) vs protecting random parameters (bottom). Each configuration is attacked 10 times and the best-recovered image is selected.}
\label{fig:attack_results}
\vspace{-0.3cm}
\end{figure}
\subsubsection{Optimized Overhead}
\hfill\\
In Table~\ref{tab:eff}, we first case study different \textbf{Parameter Efficiency} ML techniques for both training-from-scratch and fine-tuning scenarios. Efficiently reducing the sizes of shared models directly helps with HE computation and communication efficiency.



We then examine the overhead optimization effects from \textbf{Parameter Selection}. We examine the overhead change when parameters with high privacy importance are selected and encrypted. Table~\ref{tab:selection} shows the overhead reduction from only encrypting certain parts of models, where both overheads are nearly proportional to the size of encrypted model parameters, which is coherent with the general relationship between HE overheads and input sizes.   




Figure~\ref{fig:pie} dissects the training cycle composition for the HE framework (both with and without optimizations) and the plaintext framework respectively. For a medium-sized model, the overheads (both computation and communication) from HE shift some portion of the local training procedure to aggregation-related steps compared to Non-HE, but not with an infeasible margin relatively speaking. Though generally smaller models require shorter training time, the overheads of the HE-based aggregation also drop proportionally. 






\subsubsection{Effectiveness of Selection Defense}
\hfill\\
To evaluate the defense effectiveness of \textbf{Parameter Selection}, we first use privacy sensitivity to generate a privacy map (Figure~\ref{fig:lenet_map}) and then verify the effectiveness of selection by performing gradient inversion (DLG~\cite{zhu2019deep}).

DLG works by adjusting potential recovery data according to the loss feedback between the actual model gradient updates and the generated gradients. 

We use image samples from CIFAR-100 to calculate the parameter sensitivities of the model. In the DLG attack experiments, we use Multi-scale Structural Similarity Index (MSSSIM), Visual Information Fidelity (VIF), and Universal Quality Image Index (UQI) as metrics to measure the similarity between recovered images and original training images to measure the attack quality hence the privacy leakage\footnote{The image similarity metric library used is at \url{https://pypi.org/project/sewar/}.}.
In Figure~\ref{fig:attack_results}, compared to random encryption selection where encrypting $42.5\%$ of the parameters can start to protect against attacks,  our top-$10\%$ encryption selection according to the model privacy map only alone can defend against the attacks, meaning lower overall overhead with the same amount of privacy protection.


\noindent\textbf{Empirical Selection Recipe} Our selection strategy works by first encrypting more important model parameters. Empirically, from our experimental investigation, model parameters in the beginning and end stages tend to be more important for information leakage~\cite{hatamizadeh2022gradvit} and attack defense, which can be used as a general guideline on top of model privacy maps.


% \subsection{Deep Leakage from Gradients}
% \ssh{a better title for this subsection??}
% \weizhao{TODO: integrate this part in the above subsection}

% We evaluated the effectiveness of FHE in preventing leakages from models. 
% Previous studies~\cite{} have shown that recovering training data from client models and global models in each round of FL is challenging, as  the training data in different FL iterations may vary due to the selection of FL clients and the local sampling of their data. Additionally, the size of training data can be large, and the distribution of local data on the clients can vary significantly, making it even more challenging to recover local data. 
% However, recent studies have demonstrated attacking methods such as inferring real training data from training models (or gradients), are becoming stronger, making it necessary to protect models (or gradients) while transferring the local models.

% To evaluate the effectiveness of FHE, we designed a scenario where the leaked information would be at its maximum. Specifically, we chose a simple convolutional neural network, such as LeNet, and trained the model using only one data point. We used a state-of-the-art approach, DLG~\cite{}, which uses a single gradient to recover the training data to demonstrate the necessary to protect models. We then protect one layer of the gradient each time using FHE and ran DLG to evaluate whether our approach can prevent DLG from recovering training data from the gradients. 
% We use CIFAR10, and use XXX score to calculate a similarity between the recovered image with the original image. 

% \ssh{compare with other methods such as adding noise??? (maybe add this part in previous sections) the other methods are lossy; check \S5 in the deep leakage paper.}












