\subsection{Design Goals}
\noindent\textbf{Adversary Definition}
We define a semi-honest adversary $\mathcal{A}$ that can corrupt the aggregation server or any subset of local clients. $\mathcal{A}$ follows the protocol but tries to learn as much information as possible. Loosely speaking, in the presence of such an adversary, the security definition requires that only the private information in local models from the corrupted clients will be learned when $\mathcal{A}$ corrupts a subset of clients; no private information from local models nor global models will be learned by $\mathcal{A}$ when $\mathcal{A}$ corrupts the aggregation server.

When $\mathcal{A}$ corrupts both the aggregation server and a number of clients, the default setup where the private key is shared with all clients (also with corrupted clients) will enable $\mathcal{A}$ to decrypt local models from benign clients (by combining encrypted local models received by the corrupted server and the private key received by any corrupted client). This issue can be solved by adopting the threshold or multi-key variant of HE where decryption has to be collaboratively performed by a certain amount of clients~\cite{aloufi2021computing, ma2022privacy, du2023efficient}. 
%\paragraph{Assumptions}
% We consider several realistic attacks that can be potentially executed by the attacker:

% \begin{itemize}
%     \item \textbf{Package drop} The corrupted node deliberately drops the package that is forwarded to it without sending it to the next node.
%     \item \textbf{Refuse to receive the package} The corrupted node can reject the package sent from the previous node and pretend it does not go through.
%     \item \textbf{DDOS from the sender}
% \end{itemize}

\noindent\textbf{Security And Privacy Goal}
Our framework should guarantee that in the presence of $\mathcal{A}$, a set of clients wishes to collaboratively train a global model on each individual clientâ€™s local dataset in the federated setting, but only the aggregated model will be shared among clients. No 
%private 
information about any individual training data 
%\carlee{which parts of user data are private?} \weizhao{added training data} 
will be learned by $\mathcal{A}$ neither directly nor indirectly via attacks such as gradient inversion.

\noindent\textbf{Efficient Federated Training Goal}
Encrypted models by Homomorphic Encryption are around $15\times$ larger than the plaintext models and usually incur $10\times$ computation overhead, which limits HE's scalability on large models across edge devices.

With overhead optimization strategies, the encrypted FL system can efficiently train a good model with fast overall training time with low communication overhead. The system should have similar overall training time and communication cost compared to the plaintext federated training. The system also needs to support different overhead optimization strategies and support for different encryption libraries for providing different levels of overhead reduction and privacy preservation for dynamic user requirements.

%\yuhang{need more content} \carlee{explain why HE often causes more compute/communication overhead}

%\carlee{since the architecture is agnostic to which HE library is used, is flexibility to use different libraries also a design goal?}