\section{System Design}
\label{sec:system}

\begin{figure}
\includegraphics[width=0.48\textwidth]{figs/fedml-fhe.pdf}
\caption{FedML-HE System Illustration: local models are encrypted upon federated aggregation and the server acts as a computing service without access to local models.}
\vspace{-0.5cm}
\label{fig:fedml-fhe}
\end{figure}

In this section, we first define the problem and describe the algorithmic design of HE-based aggregation; then illustrate our HE implementation with reasoning about the HE libraries. Lastly, we explain the realization of the FedML-HE deployment platform.

\input{threatmodel}

\subsection{Algorithm for HE-Based Federated Learning}


Our privacy-preserving federated learning system utilizes Homomorphic Encryption to enable the aggregation server to combine local model parameters without viewing them in their unencrypted form.

As FedAvg has been proven as still one of the most robust federated aggregation strategies while maintaining computational simplicity~\cite{wang2022unreasonable}, we primarily implement FedAvg in our system. However, as shown in \tsref{sec:he_fedml} and Figure~\ref{fig:design}, our system can be easily extended to support more FL aggregation functions with HE by encrypting and computing the new parameters in these algorithms (e.g. FedProx~\cite{li2020federated}, FedOpt~\cite{reddiadaptive}, and Scaffold~\cite{karimireddy2020scaffold}). %\carlee{give examples?}


Our HE-based secure aggregation algorithm, as shown in Algorithm~\ref{alg:fedml-fhe} and illustrated in Figure~\ref{fig:fedml-fhe}, can be summarized as:
$[\![\mathbf{W}_{\text {glob}}]\!]=\sum_{i=1}^N \alpha_i [\![\mathbf{W}_i]\!]$, where $[\![\mathbf{W}_i]\!]$ is the $i$th encrypted local model, $\alpha_i$ is weighting factor for client $i$ and $[\![\mathbf{W}_{\text {glob}}]\!]$ is the encrypted global model. Note that weighting factors can be either encrypted or in plaintext depending on whether the aggregation server is trustworthy enough to obtain that information. In our system, we set weighting factors to be plaintext by default. We only need one depth of HE multiplication in our algorithm for weighting, which is preferred to reduce HE multiplication operations.



\begin{algorithm}[ht]
\SetKwFor{ForPar}{for}{do in parallel}{end forpar}
    \caption{HE-Based Federated Aggregation}
    \label{alg:fedml-fhe}
    %\weizhao{@Yuhang, can you fix the algo format here?}
    \begin{itemize}
    
        \item Aggregation server $\mathcal{S}$ and $N$ clients;
        \item Key authority server generates a key pair $(pk, sk)$ and the crypto context, then distributes it to clients and server. (except server does not get $sk$);
        \item Client $i\in [N]$ owns a local dataset $\mathcal{D}_i$ and initializes a local model $\mathbf{W}_i$ with the aggregation weighing factor $\alpha_i$;$[\![\mathbf{W}]\!]$ is the encrypted model;
        \item $T$ is the number of communication rounds;
    \end{itemize}

\raggedright %yuhang: done
\For{$t = 1, 2, \dots, T$}{
    \ForPar{each client $i \in [N]$}{
        \If{$t > 1$}{
            Receive $[\![\mathbf{W}_\text{glob}]\!]$ from $\mathcal{S}$;\\
            $\mathbf{W}_i \gets Dec(sk, [\![\mathbf{W}_\text{glob}]\!])$;\\
        }
        $\mathbf{W}_i \gets Train(\mathbf{W}_i, \mathcal{D}_i)$;\\
        $[\![\mathbf{W}_i]\!] \gets Enc(pk, \mathbf{W}_i)$;\\
        Send $[\![\mathbf{W}_i]\!]$ to server $\mathcal{S}$;\\
    }
    \tcp{Server Aggregation}
    $[\![\mathbf{W}_\text{glob}]\!] \gets \sum_{i=1}^N \alpha_i [\![\mathbf{W}_i]\!]$;\\
}
\end{algorithm}

\begin{figure}[ht]
\includegraphics[width=0.47\textwidth]{figs/dls.pdf}
\caption{Framework Overview: our framework consists of a three-layer structure including Crypto Foundation to support basic HE building blocks, ML Bridge to connect crypto tools with ML functions, and FL Orchestration to coordinate different parties during a task.}
\label{fig:design}
\end{figure}


 %\carlee{does this change for other types of aggregation?} \weizhao{yes, it depends on the multiplicative depth of the aggregation function, i.e., how many mult operations in the function}

\begin{figure*}[ht]
\frame{\includegraphics[width=1\textwidth]{figs/mlops_platform.pdf}}
\caption{Our Deployment Platform: local simulation can be easily deployed to real-world edge-cloud scenarios without code modification}
\label{fig:mlops-flow}
\end{figure*}

\begin{table*}[ht]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        API Name & Description \\ \hline
        \textit{pk, sk} =  \textbf{key\_gen}(\textit{params}) & Generate a pair of HE keys (public key and private key) \\ \hline
        \textit{1d\_local\_model} = \textbf{flatten}(\textit{local\_model}) & Flatten local trained model tensors into a 1D local model \\ \hline
        \textit{enc\_local\_model} = \textbf{enc}(\textit{pk, 1d\_model}) & Encrypt the 1D model \\ \hline
        \makecell{\textit{enc\_global\_model} = \textbf{he\_aggregate}(\\\textit{enc\_models[n], weight\_factors[n]})} & Homomorphically aggregate a list of 1D local models \\ \hline
        \textit{dec\_global\_model} = \textbf{dec}(\textit{sk, enc\_global\_model}) & Decrypt the 1D global model \\ \hline
        \makecell{\textit{global\_model} = \textbf{reshape}(\\\textit{dec\_global\_model, model\_shape})} & Reshape the 1D global model back to the original shape \\ \hline
    \end{tabular}
\caption{HE Framework APIs}
\label{tab:apis}
\end{table*}

\subsection{HE Implementation}
\label{sec:he_fedml}
In this part, we will explain in detail how we implement our HE-based aggregation. 
% \carlee{talk a bit more about why you used a layered design and how this facilitates integration.}

Figure~\ref{fig:design} provides a high-level design of our framework. Our framework consists of three major layers: 
\begin{itemize}
    \item \textbf{Crypto Foundation}: the foundation layer where python wrappers are built to realize HE functions including key generation, encryption/decryption, secure aggregation, and ciphertext serialization using open-sourced HE libraries;
    \item \textbf{ML Bridge}: the bridging layer to connect the FL system orchestration and cryptographic functions. Specifically, we have ML processing APIs to process inputs to HE functions from local training processes and outputs vice versa. Additionally, we realize the optimization module here to mitigate the HE overheads;
    \item \textbf{FL Orchestration}: the FL system layer where the key authority server manages the key distribution as well as (server/client) managers and task executors orchestrate participants.
\end{itemize}

Our layered design makes the HE crypto foundation and the optimization module \emph{semi-independent}, allowing different HE libraries to be easily switched into FedML-HE and further FL optimization techniques to be conveniently added to the system.

Detailed key APIs are listed in Table~\ref{tab:apis}, which helps illustrate the HE-based FL procedure:

\begin{enumerate}
    \item \textbf{Before local training}: if it is before the first FL round, each client receives the same pair of keys $(pk, sk)$ along with related crypto contexts from the key authority server using \textbf{key\_gen}; otherwise, each client \textbf{dec} then \textbf{reshape} the encrypted global model. 
    \item \textbf{After local training}: each client \textbf{flatten} then \textbf{enc} its local model and sends it to the aggregation server.
    \item \textbf{On aggregation}: the server \textbf{he\_aggregate} a list of encrypted local models with a weight factor list and sends back the encrypted global model.
\end{enumerate}



Note that models are regarded as generic tensors and the HE functions are universally applied to all models without the need for special modifications for each different type of model. It is also worth mentioning that instead of directly applying HE functions on tensors, we \textbf{flatten} tensors into the 1D shape. This is because directly encrypting tensors will yield large ciphertexts (e.g. on TenSEAL~\cite{tenseal2021}, a small tensor with a size of $5$k numbers results in a ciphertext of $1.7$ GB), which would easily crash memory on most machines with a reasonable number of local ML models that are also reasonably-sized. However, encrypting a one-dimensional vector largely reduces the ciphertext size. %, which is the reason why in our framework, we first flatten the models before feeding them into HE functions.

\noindent\textbf{Choices of HE libraries} Our Crypto Foundation layer can universally support different HE realizations. Currently, several open-source HE libraries are available~\cite{palisade,sealcrypto, helib}. Among these libraries, PALISADE has one of the fastest HE implementations~\cite{cryptoeprint:2022/425} (verified by our experimental results in \tsref{sec:diff_frameworks}) and also supports several decentralized multi-party functionalities, such as Proxy Re-Encryption~\cite{ateniese2006improved, jin2022secure} and Threshold Homomorphic Encryption~\cite{aloufi2021computing} for serverless or decentralized FL scenarios~\cite{he2022spreadgnn}. In our framework, we prioritize PALISADE as our HE core. We use pybind11~\cite{pybind11} to pythonize PALISADE functions (C++) in our framework.



\subsection{Deploy Anywhere: An FedML-HE Deployment Platform MLOps For Edges/Cloud}


We implement our deployment-friendly platform\footnote{Our platform is available at \url{https://doc.fedml.ai/mlops/user_guide.html}.} such that FedML-HE can be easily deployed across cloud and edge devices as shown in Figure~\ref{fig:mlops-flow}. Before the training starts, a user uploads the configured server package and the local client package to the web platform. The server package defines the operations on the FL server, such as the aggregation function and client sampling function; the local client package defines the customized model architecture to be trained (model files will be distributed to edge devices in the first round of the training). Both packages are written in Python. The platform then builds and runs the docker image with the uploaded server package to operate as the server for the training with edge devices configured using the client package.
% In the first round of the training, the server would distribute the Python-described model code to the edge devices, which would directly load the model architecture information using the TorchScipt or MNN engine with the C++ code. In the following rounds of training, the server and clients would only transmit the tensors of the model weights to save communication costs.

As shown in Figure~\ref{fig:mlops}, during the training, users can also keep tracking the learning procedure including device status, training progress/model performance, and FedML-HE system overheads (e.g., training time, communication time, CPU/GPU utilization, and memory utilization) via the web interface. Our platform keeps close track of overheads, which allows users to in real-time pinpoint HE overhead bottlenecks if any.

\begin{figure}[ht]
\includegraphics[width=0.48\textwidth]{figs/mlops.pdf}
\caption{Deployment Interface Example of FedML-HE: Overhead distribution monitoring on each edge device (e.g. Desktop (Ubuntu), Laptop (MacBook), and Raspberry Pi 4), which can be used to pinpoint HE overhead bottlenecks and guide optimization.}
\vspace{-0.4cm}
\label{fig:mlops}
\end{figure}
% Currently, we provide three experimental tracking abilities: monitoring device status and training progress of each edge device, visualizing the curves of training loss and validation performance, and visualizing system performance (e.g., training time, communication time, CPU/GPU utilization, and memory utilization). Besides these, users could also view the logs from both server and client devices on the MLOps web during the training, which provides convenience to debug the code under the real edge-server deployment.

% After it is verified that federated learning can produce modeling benefits on specific applications in the experimental simulation, users can use our platform to upgrade the simulation into production without modifying the code. The simulated source code can be deployed directly to edge devices with real data without
% further modification.  Such a deployment platform not only lowers the learning curve and product deployment difficulty of FL, but also allows a user-friendly interface for pinpointing HE overhead bottlenecks. As shown in Figure~\ref{fig:mlops}, the distributed system overheads of FedML-HE are visualized and can be monitored by users in real time.

% \begin{figure*}[ht]
% \includegraphics[width=1\textwidth]{figs/mlops_working_flow.jpg}
% \caption{The working flow of MLOps. After the user finishes the local developing of the project using FedML, MLOps could smoothly deploy it into the real-world edge-cloud system without modification of code. \yuhang{Will redraw the figure.}}
% \label{fig:mlops_workingflow}
% \end{figure*}




