\section{Introduction}
% Federated learning allows model training on decentralized edge devices
% %different data sources from different data owners 
% without aggregating data to one central location by aggregating locally-trained models, which presumably should guarantee the privacy of data. The privacy-preserving property of general federated learning (FL) systems relies on the simple aggregation of local machine learning models such as federated averaging (FedAvg)~\cite{mcmahan2017communication}. 
% Instead of uploading data to the server for centralized training, clients process their local data and share the local models with the server. However, this approach reveals individual local models to the aggregation server and potentially other parties in the system. Given that local models are usually trained on datasets of a limited size, sharing local models in a plaintext view during aggregation poses a privacy vulnerability. Several known attacks have been proposed to retrieve private training data information and infringe on individual privacy, such as membership inference attacks~\cite{nasr2019comprehensive, wang2019beyond, truex2019demystifying} and other sensitive user data recovery attacks~\cite{zhu2019deep, criswell2014kcofi,bhowmick2018protection, hitaj2017deep, hatamizadeh2022gradvit}. As shown in Figure~\ref{fig:attack_flow}, a compromised server can carry attacks to recover personal data used for local training, which could lead to the exposure of sensitive personal information, including the user identity, medical records, smart home device (e.g. security cameras) deployment, and private text messages. %\carlee{explain why individual data is private and why you might need to train a model over all individuals' data, even though the data is private. Give some examples}\yuhang{added a few example.}




Federated learning enables model training on decentralized edge devices without aggregating data to a central location by aggregating locally-trained models, which should ideally ensure data privacy. Privacy preservation in standard federated learning systems depends on straightforward aggregation functions, such as federated averaging (FedAvg)~\cite{mcmahan2017communication}. Rather than uploading data to a central server for training, clients process their local data and share the local models with the server. However, this approach exposes individual local models to the aggregation server and potentially other parties within the system.
Since local models are typically trained on limited-size datasets, sharing local models in plaintext during aggregation poses a privacy vulnerability. Various attacks have been proposed to extract private training data information and compromise individual privacy, including membership inference attacks~\cite{nasr2019comprehensive, wang2019beyond, truex2019demystifying} and sensitive user data recovery attacks~\cite{zhu2019deep, criswell2014kcofi,bhowmick2018protection, hitaj2017deep, hatamizadeh2022gradvit}. As illustrated in Figure~\ref{fig:attack_flow}, a compromised server can conduct attacks to recover personal data used for local training, which can expose sensitive personal information, e.g., if FL is used on identifying information, medical records, smart home (e.g., security camera) deployment, or private messages.

\begin{figure}
\includegraphics[width=0.48\textwidth]{figs/attack_flow.pdf}
\caption{\textbf{FL Inversion Attacks}: an adversary server can recover local training data from unprotected local model updates.}
\vspace{-0.4cm}
\label{fig:attack_flow}
\end{figure}

% Existing defense mechanisms to prevent the FL server from learning the plaintext local models include general Multi-Party Computation (MPC) secure aggregation protocols~\cite{bonawitz2017practical, so2022lightsecagg} by masking private inputs and noise-based Differential Privacy (DP) solutions~\cite{truex2019hybrid,byrd2020differentially} by adding privacy noise to original inputs. However, they either require extra interactive synchronization steps while being prone to failures like client dropout (MPC) which makes prudent engineering mandatory during deployment~\cite{zhang2020batchcrypt}
% %\carlee{isn't HE also vulnerable to collusion? If all clients but one are malicious, they can exactly infer the other client's model from the model updates}\weizhao{I used the wrong wording here. Was meant to point out that it is hard for MPC solutions to handle situations like client dropouts}
% or introduce the issue of model performance degradation due to privacy noises (DP). Homomorphic Encryption (HE)~\cite{paillier1999public,gentry2009fully, cryptoeprint:2012/144, brakerski2014leveled, cheon2017homomorphic}, on the other hand, provides a resilient post-quantum solution that preserves local models to protect against attacks and guarantee local model privacy while still providing adequate utility. HE-based federated aggregation works by encrypting local models upon aggregation and performing model aggregating in the form of ciphertexts, which has been adopted by a few FL systems~\cite{nvidiafl,ibmfl,zhang2020batchcrypt, du2023efficient}. 


Existing defense mechanisms to prevent the FL server from learning plaintext local models include Multi-Party Computation (MPC) secure aggregation protocols~\cite{bonawitz2017practical, so2022lightsecagg} and noise-based Differential Privacy (DP) solutions~\cite{truex2019hybrid,byrd2020differentially}. MPC protocols mask private inputs, while DP solutions add privacy noise to original inputs. However, these methods have limitations: MPC requires additional interactive synchronization steps and is susceptible to failures like client dropout, necessitating careful engineering during deployment, while DP can result in model performance degradation due to privacy noises.
Homomorphic Encryption (HE)~\cite{paillier1999public,gentry2009fully, cryptoeprint:2012/144, brakerski2014leveled, cheon2017homomorphic} offers a robust post-quantum solution that protects local models against attacks and ensures local model privacy while maintaining adequate utility. HE-based federated aggregation encrypts local models during aggregation and performs model aggregation over ciphertexts. This approach can enable secure federated learning deployments and has been adopted by several FL systems~\cite{nvidiafl,ibmfl,zhang2020batchcrypt, du2023efficient} and a few domain-specific applications have been implemented~\cite{stripelis2021secure, yao2022fedgcn, jin2022secure}. %, providing a resilient alternative to traditional defense mechanisms.


%add the applications after paper get accepted.
%Several HE-based solutions have been adopted for privacy-sensitive FL applications %(Figure~\ref{fig:applications}) 

%\yuhang{maybe replace the figure with the comparison table of other frameworks}.

% \begin{figure*}
% \includegraphics[width=1\textwidth]{figs/apps_fixed.pdf}
% \caption{Example Applications: secure federated neuroimaging learning~\cite{stripelis2021secure}, secure federated learning in IoT~\cite{zhang2022homomorphic, yao2022fedgcn, jin2022secure}, and secure federated NLP~\cite{lin2021fednlp}.}
% \label{fig:applications}
% \end{figure*}

\begin{table}[ht]
\begin{tabular}{|c|c|c|c|}
\hline
Features&IBMFL& \makecell{Nvidia \\ FLARE} & FedML-HE \\ \hline
\makecell{Homomorphic\\Encryption}      &      \cmark &  \cmark  &   \cmark   \\ \hline
\makecell{Key Management}  &    $\bigcirc$&   \cmark &    \cmark  \\ \hline
\makecell{Efficient \\Encrypted\\Computation}      &      \xmark&  $\bigcirc$&   \cmark   \\ \hline 
\makecell{HE\\Multi-Party\\Functionalities} &      \xmark&  \xmark  &    \cmark  \\ \hline
\end{tabular}
\caption{\textbf{Comparison with Existing HE-Based FL Systems}: $\bigcirc$ implies limited support.}
\vspace{-0.5cm}
\label{tab:compare_intro}
\end{table}
% However, even with advances in the Homomorphic Encryption research community to improve HE performance both theoretically and engineeringly, HE as a strong but complex cryptographic foundation still has impractical overheads in real-world applications. Previous HE solutions only utilize existing generic HE methods without enough optimization for large-scale secure federated learning deployment. The scalability of encrypted computation and communication during federated training becomes a bottleneck, limiting its feasibility for real-world edge computing scenarios. This limitation from HE overheads is rather noticeable (commonly $\sim$15x increase in both computation and communication), especially when training large federated models across resource-constrained mobile devices, where encrypted computing and communicating large models might take much longer time than the actual model training. 

% %To overcome the bottleneck on encryption time and communication cost, we first optimize HE specified for FL with fundamental changes to speed up the encryption. We then provide a universal optimization scheme (Parameter Efficiency x Parameter Selection) to provide a co-optimization of communication cost and privacy preservation. Finally, 



% We then propose FedML-HE, an efficient Homomorphic-Encryption-based privacy-preserving federated learning system with a universal optimization scheme that can be practically deployed across distributed edge devices\footnote{Our code is open sourced at \textit{\{anonymized-for-submission\}}.}. 
%\carlee{what does ``systematic'' mean here? does ``algorithmic'' refer to the ML training algorithm?} 
 

Despite advances in the Homomorphic Encryption research community to enhance HE performance in theory and practice, HE remains a powerful but complex cryptographic foundation with impractical overheads for most real-world applications. Prior HE solutions mainly employ existing generic HE methods without sufficient optimization for large-scale privacy-preserving federated learning deployment. The scalability of encrypted computation and communication during federated training then becomes a bottleneck, restricting its feasibility for real-world edge computing scenarios. This HE overhead limitation is rather noticeable (commonly $\sim$15x increase in both computation and communication ~\cite{gouert2022new}) when training large federated models across resource-constrained mobile devices, where encrypted computing and communication of large models might take considerably longer than the actual model training.

To address these challenges, we propose FedML-HE, an efficient Homomorphic Encryption-based privacy-preserving FL system with a universal optimization scheme, designed for practical deployment across distributed edge devices\footnote{Our code is  open-source at \url{https://github.com/FedML-AI/FedML}.}. Our system significantly reduces overheads, enabling HE-based federated learning to be more accessible and efficient in real-world scenarios. 

\noindent\textbf{Key contributions}: 
% \carlee{the title calls our work a ``library,'' but the text below never refers to a library.}
\begin{itemize}
    % \item We propose the algorithmic and programming \carlee{call this a system design} design for realizing HE functionality in the federated learning system with HE underlying foundation analysis. \carlee{does this include the HE ``optimizations'' mentioned in the paragraph above? try to be more specific about what exactly about HE is being optimized} Our HE-based framework supports deployment with the MLOps platform, which provides key management, system profiling, grouping and edge binding. \carlee{does this refer to a specific MLOps platform? If so, why did you choose it? How portable is your HE framework to other ML platforms?}

    \item We implement our HE-based FL system on a user/device-friendly deployment platform that includes key management, system profiling, grouping, and edge binding. The platform also enables easy monitoring of HE overhead distribution across edge devices.
    
    % \item We first benchmark our HE-FL framework under different aspects of FL training, which helps us pinpoint the HE bottlenecks. We also compare with other HE-based solutions in which the results indicate that our vanilla solution already requires the smallest computational overhead without any optimization with more functionalities (more details in Table~\ref{tab:compare_intro} and Table~\ref{tab:frameworks}).

    \item We benchmark our HE-FL framework under various aspects of FL training, which allows us to pinpoint the HE bottlenecks. Our comparison with other HE-based solutions reveals that our vanilla implementation already has the lowest computational overhead while offering more functionalities (see Table~\ref{tab:compare_intro} and Table~\ref{tab:frameworks}).

    % \item We propose a universal HE-FL optimization scheme \textbf{Parameter Efficiency} x \textbf{Parameter Selection} by minimizing the size of model updates for encrypted computation while preserving privacy guarantee to mitigate both computational and communicational overhead increases from HE operations. With optimization, our framework can achieve sizeable overhead reduction, especially on large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and $\sim$40x reduction for BERT respectively), which shows the feasibility of further real-world HE-based FL deployments. 


    \item We propose a universal HE-FL optimization scheme, \textbf{Parameter Efficiency} x \textbf{Parameter Selection}, that minimizes the size of model updates for encrypted computation while preserving privacy guarantees. This approach mitigates both computational and communication overhead increases from HE operations. With optimization, our framework achieves significant overhead reduction, particularly for large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and $\sim$40x reduction for BERT), demonstrating the potential for practical HE-based FL deployments.
\end{itemize}


