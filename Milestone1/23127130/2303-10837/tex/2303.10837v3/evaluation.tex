\section{Evaluation}


\noindent In this section, we focus on the evaluation results to show how our proposed universal optimization scheme largely mitigates these overheads for real-world deployment but still guarantees adequate defense against privacy attacks. Note that additional experimental results regarding other FL system aspects are included in in the appendix.


\subsection{Experiment Setup}
\noindent\textbf{Models.} We test our framework on models in different ML domains with different sizes including Llama-2 (7 billion) (more details in in the appendix).

% \noindent\textbf{Attack Dataset.} MNIST dataset ($70k$ images) and the CIFAR-100 dataset ($50k$ images) for experiments.

\noindent\textbf{HE Libraries.} We implement our HE core using both PALISADE and TenSEAL. Unless otherwise specified, our results show the evaluation of the PALISADE version. 

\noindent\textbf{Default Crypto Parameters.} Unless otherwise specified, we choose the multiplicative depth of 1, the scaling factor bit digit of 52, an HE packing batch size of 4096, and a security level of 128 as our default HE cryptographic parameters during the evaluation.

\noindent\textbf{Microbenchmark.} For microbenchmarking HE overheads, we use an Intel 8-core 3.60GHz i7-7700 CPU with 32 GB memory and an NVIDIA Tesla T4 GPU on Ubuntu 18.04.6. %\carlee{This sounds like everything was done on a single machine, which doesn't seem like a real FL system (which we claim to build). Didn't we benchmark the training itself too?}

% We provided detailed experimental instructions in Appendix~\ref{sec:tutorial}.



% \begin{figure}[ht]
% \includegraphics[width=0.4\textwidth]{figs/model_ratio.pdf}
% \caption{(Non-Optimization) Computational \& Communicational Overhead Increase: fold ratio from Plaintext Aggregation to FedML-HE.}
% \label{fig:ratio}
% \end{figure}

% \textit{DoubleSqueeze} using Top-k Compression with $k=300,000$.




\subsection{Optimizations}
\label{sec:opt_results}

To mitigate the HE overhead surge, our optimization scheme \textbf{Selective Parameter Encryption} works by selecting sensitive portions of parameters for encrypted computation while leaving the rest in plaintext per desired overhead expectations and privacy promise. In this section, we first evaluate the overhead optimization from \textbf{Selective Parameter Encryption} and then use the state-of-the-art privacy attacks to evaluate the effectiveness of our selection defense during FL training.

Note that other parameter efficiency techniques~\cite{tang2019doublesqueeze, hu2021lora} for both training-from-scratch and fine-tuning scenarios can also be applied in our system before \textbf{Selective Parameter Encryption} and efficiently reducing the sizes of shared models directly helps with HE computation and communication efficiency (we also include preliminary results on this part in the appendix.



\subsubsection{Optimized Overheads}


%\carlee{This sounds like you didn't actually measure the training times? Why 200 Mbps; does this come from a testbed measurement?}
We first examine the overhead optimization gains from \textbf{Selective Parameter Encryption}. We examine the overhead change when parameters with high privacy importance are selected and encrypted. Figure~\ref{fig:comp_opt} shows the overhead reduction from only encrypting certain parts of models, where both overheads are nearly proportional to the size of encrypted model parameters, which is coherent with the general relationship between HE overheads and input sizes. Note that after 10\% encryption per our \textbf{Selective Parameter Encryption}, the overheads are close to the ones of plaintext aggregation.  
\begin{figure}[t]
\includegraphics[width=0.45\textwidth]{figs/model_comp_opt.pdf}
\includegraphics[width=0.45\textwidth]{figs/model_comm_opt.pdf}
\caption{Computational (up) and Computation (down) Overhead Comparison For Models of Different Sizes (logarithmic scale): 10\% Encryption is based on our selection strategy and 50\% encryption is based on random selection.}
\label{fig:comp_opt}
\end{figure}

\begin{figure*}[ht]

\centering
\includegraphics[width=0.333\textwidth]{figs/pie-plain.pdf}\hfill
\includegraphics[width=0.333\textwidth]{figs/pie-fhe.pdf}\hfill
\includegraphics[width=0.333\textwidth]{figs/pie-he-opt.pdf}

\caption{Time Distribution of A Training Cycle on ResNet-50: with a single AWS region bandwidth of $200$ MB/s for plaintext FL (left), HE w/o optimization (middle), and HE w/ optimization (right). Optimization setup uses \textit{DoubleSqueeze}~\cite{tang2019doublesqueeze} with $k=1,000,000$ and encryption mask with an encrypted ratio $s = 30\%$.}
\label{fig:pie}
\end{figure*}


Figure~\ref{fig:pie} provides a perspective of overhead distribution to dissect the training cycle composition for the HE framework (both with and without optimizations) and the plaintext framework respectively with a single AWS region bandwidth. For a medium-sized model, the overheads (both computation and communication) from HE shift some portion of the local training procedure to aggregation-related steps compared to Non-HE, but not with an infeasible margin relatively speaking. Though generally smaller models require shorter training time, the overheads of the HE-based aggregation also drop proportionally. 








\subsubsection{Effectiveness of Selection Defense}

% \begin{figure*}[ht]
% \includegraphics[width=1.0\textwidth]{figs/heatmap_lenet.pdf}
% \caption{Model Privacy Map Calculated Using Parameter Privacy Sensitivity On LeNet: darker color indicates higher sensitivity. Each subfigure shows the sensitivity of parameters of the current layer. The sensitivity of parameters is imbalanced and many parameters have very little sensitivity (its gradient is hard to be affected by tuning the data input for attack).}
% \label{fig:lenet_map}
% \end{figure*}


\begin{figure*}[ht!]
\centering
\includegraphics[width=0.45\textwidth]{figs/attack_results.pdf}%\hfill
\includegraphics[width=0.465\textwidth]{figs/attack_random.pdf}
\caption{Selection Protection Against Gradient Inversion Attack~\cite{zhu2019deep} On LeNet with the CIFAR-100 Dataset: attack results when protecting top-$s$ sensitive parameters (left) vs protecting random parameters (right). Each configuration is attacked 10 times and the best-recovered image is selected.}
\label{fig:attack_results}
\end{figure*}
\noindent To evaluate the defense effectiveness of \textbf{Selective Parameter Encryption}, we first use privacy sensitivity to generate a privacy map (Figure~\ref{fig:lenet_map}) and then verify the effectiveness of selection by performing gradient inversion (DLG~\cite{zhu2019deep}). We also provide defense results with Language Model Inversion Attacks~\cite{fowl2022decepticons} on Bert.



%DLG works by adjusting potential recovery data according to the loss feedback between the actual model gradient updates and the generated gradients. 
\underline{\textit{Defense effectiveness on CV tasks}}. We use image samples from CIFAR-100 to calculate the parameter sensitivities of the model. In the DLG attack experiments, we use Multi-scale Structural Similarity Index (MSSSIM), Visual Information Fidelity (VIF), and Universal Quality Image Index (UQI) as metrics to measure the similarity between recovered images and original training images to measure the attack quality hence the privacy leakage\footnote{The image similarity metric library used is at \url{https://pypi.org/project/sewar/}.}.
In Figure~\ref{fig:attack_results}, compared to random encryption selection where encrypting $42.5\%$ of the parameters can start to protect against attacks,  our top-$10\%$ encryption selection according to the model privacy map only alone can defend against the attacks, meaning lower overall overhead with the same amount of privacy protection.

\underline{\textit{Defense effectiveness on NLP tasks}}. We use language samples from wikitext dataset in our experiment. As shown in Figure~\ref{fig:bert-attack}, with our sensitivity map indicating the top 30\% privacy-sensitive parameters, our encryption mask can prevent inversion attacks that yields better defense results than randomly encrypting 75\% of the model parameters.



\noindent\textbf{Empirical Selection Recipe.} Our selection strategy works by first encrypting more important model parameters. Empirically, from our experimental investigation, encrypting top-30\% most sensitive parameters, as well as the first and last model layers, tends to be robust to avoid information leakage~\cite{hatamizadeh2022gradvit} and attack defense (e.g. Figure~\ref{fig:lenet_map}), which can be used as a general guideline on top of model privacy maps.

\begin{figure*}[htp!]
\centering
\includegraphics[width=1\textwidth]{figs/bert_attack.pdf}
\caption{Language Model Inversion Attacks~\cite{fowl2022decepticons} on Bert with the wikitext Dataset: \colorbox{red}{Red} indicates falsely-inverted words and \colorbox{yellow}{Yellow} indicates correctly-inverted words.}
\label{fig:bert-attack}
\end{figure*}


% \subsection{Deep Leakage from Gradients}
% \ssh{a better title for this subsection??}
% \weizhao{TODO: integrate this part in the above subsection}

% We evaluated the effectiveness of FHE in preventing leakages from models. 
% Previous studies~\cite{} have shown that recovering training data from client models and global models in each round of FL is challenging, as  the training data in different FL iterations may vary due to the selection of FL clients and the local sampling of their data. Additionally, the size of training data can be large, and the distribution of local data on the clients can vary significantly, making it even more challenging to recover local data. 
% However, recent studies have demonstrated attacking methods such as inferring real training data from training models (or gradients), are becoming stronger, making it necessary to protect models (or gradients) while transferring the local models.

% To evaluate the effectiveness of FHE, we designed a scenario where the leaked information would be at its maximum. Specifically, we chose a simple convolutional neural network, such as LeNet, and trained the model using only one data point. We used a state-of-the-art approach, DLG~\cite{}, which uses a single gradient to recover the training data to demonstrate the necessary to protect models. We then protect one layer of the gradient each time using FHE and ran DLG to evaluate whether our approach can prevent DLG from recovering training data from the gradients. 
% We use CIFAR10, and use XXX score to calculate a similarity between the recovered image with the original image. 

% \ssh{compare with other methods such as adding noise??? (maybe add this part in previous sections) the other methods are lossy; check \S5 in the deep leakage paper.}
