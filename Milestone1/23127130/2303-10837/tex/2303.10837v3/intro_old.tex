\section{Introduction}
%\weizhao{Reminder: check whether all figures/algos/tables are referenced}
\shanshan{FL ensures privacy because it does not collect training data to a central server. Such feature is not mainly ensured by the aggregation function, but to the distributed training process. Here maybe we need to change the logic: 1. FL ensure privacy due to its distributed training method, clients do not need to share data; 2. however, even though clients do not upload their data, their uploaded model updates may still leak information 3. (potentially) Furthermore, some users may have higher requirements on privacy; they might think their local models are their own assets (or they are aware that some adversaries may use their model to perform some attacks such as inverting training data) and do not wish to share the whole modle to other party. That's why we want to propose a HE-based approach. }
Federated learning (FL) has been widely adopted in modern-day machine learning training in practice due to its ability to allow distributed clients to collectively train a global model without directly sharing data. Privacy preservation in standard federated learning systems depends on the distributed training process and the model aggregation function, such as FedAvg~\cite{mcmahan2017communication}. Instead of uploading data to a central server for training, clients process their local data and send the local models to the server where local models are aggregated. However, this approach exposes individual local models to the aggregation server and potentially eavesdroppers in the system, %\carlee{is this true for foundation models?}
which poses a privacy vulnerability if local models are in plaintext especially when local models are typically trained on small local datasets (e.g. smartphone text data for LLMs) and adversaries can easily invert sensitive information from such fine-grained models of limited data samples~\cite{zhu2019deep, criswell2014kcofi,bhowmick2018protection, hitaj2017deep, hatamizadeh2022gradvit} as shown in Figure~\ref{fig:attack_flow}. 
% \textcolor{red}{Adversaries that are curious about clients' training data may utilize the clients' model updates to extract local training data, \textit{i}.\textit{e}., gradient reversion attacks or data reconstruction attacks in literature. As an example, in an FL training iteration, after collecting model updates from clients, a compromised server may recover private training data, such as medical records, credit cards, smart home deployment, or private messages, , thus may violate privacy. }\shanshan{I rewrote this part and commented the original description. If it's not concise and clear, just revert to the original version.}
%FL inversion attacks~\cite{zhu2019deep, criswell2014kcofi,bhowmick2018protection, hitaj2017deep, hatamizadeh2022gradvit} have been proposed to extract private training data information and compromise individual privacy. 
% As illustrated in Figure~\ref{fig:attack_flow}, a compromised server can conduct attacks to recover personal data used for local training, which can expose sensitive personal information, e.g., medical records, credit cards, smart home deployment, or private messages.


\begin{figure}
    \centering
     \frame{\includegraphics[width=0.47\textwidth]{figs/attack_flow_crop.pdf}}
    \caption{ FL Inversion Attacks: an adversary server can recover local training data from unprotected local model updates.}
    \label{fig:attack_flow}
\end{figure}


% \begin{figure*}[htbp]
%   \centering

%   \begin{subfigure}[t]{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/attack_flow_crop.pdf}
%     \caption{ FL Inversion Attacks: an adversary server can recover local training data from unprotected local model updates.}
%     \label{fig:attack_flow}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[t]{0.5\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/fedml-fhe_crop.pdf}
%     \caption{HE-based Federated Aggregation: models are encrypted and the server acts as a computing service without access to models.}
%     \label{fig:fedml-fhe}
%     \end{subfigure}
%     \caption{FL Inversion Attacks and HE-based Federated Aggregation Workflow.}
% \end{figure*}


% Existing defense mechanisms to prevent the FL server from learning the plaintext local models include general Multi-Party Computation (MPC) secure aggregation protocols~\cite{bonawitz2017practical, so2022lightsecagg} by masking private inputs and noise-based Differential Privacy (DP) solutions~\cite{truex2019hybrid,byrd2020differentially} by adding privacy noise to original inputs. However, they either require extra interactive synchronization steps while being prone to failures like client dropout (MPC) which makes prudent engineering mandatory during deployment~\cite{zhang2020batchcrypt}
% %\carlee{isn't HE also vulnerable to collusion? If all clients but one are malicious, they can exactly infer the other client's model from the model updates}\weizhao{I used the wrong wording here. Was meant to point out that it is hard for MPC solutions to handle situations like client dropouts}
% or introduce the issue of model performance degradation due to privacy noises (DP). Homomorphic Encryption (HE)~\cite{paillier1999public,gentry2009fully, cryptoeprint:2012/144, brakerski2014leveled, cheon2017homomorphic}, on the other hand, provides a resilient post-quantum solution that preserves local models to protect against attacks and guarantee local model privacy while still providing adequate utility. HE-based federated aggregation works by encrypting local models upon aggregation and performing model aggregating in the form of ciphertexts, which has been adopted by a few FL systems~\cite{nvidiafl,ibmfl,zhang2020batchcrypt, du2023efficient}. 

\weizhao{Need to add more comparison vs DP/Secure aggregation to point out HE's advantages. Add HE can get the exact solution}
Existing defense methods are used to prevent the FL server from learning plaintext local models including secure aggregation~\cite{bonawitz2017practical, so2022lightsecagg} and noise-based differential privacy (DP)~\cite{truex2019hybrid,byrd2020differentially}. Secure aggregation privately masks local models, while DP adds noise to original models. However, secure aggregation requires additional interactive synchronization steps and is susceptible to failures like client dropout, necessitating careful engineering during deployment\shanshan{need to describe more seriously, e.g., secure aggregation is not suitable for real-world edge device applications, as edge devices often drop out due to complex real-world situations, e.g., internet connection, poor cell phone signal, application crash or no connection due to an incoming call, limited computing resources, the cell phone is forced to shut down due to low battery...}, while DP can result in model performance degradation due to privacy noises.\shanshan{maybe we can explain in this way: we add dp noise to gradients, but the output of the model might not be controlled, e.g., they might change the results by classifying a data to a different class.}
Homomorphic encryption (HE)~\cite{paillier1999public,gentry2009fully, cryptoeprint:2012/144, brakerski2014leveled, cheon2017homomorphic} offers a robust post-quantum solution that protects local models against attacks and \textit{ensures local model privacy while keeping the model aggregation with exact gradients}. \carlee{be more explicit that HE offers stronger privacy guarantees} %\carlee{``utility'' is not defined here. Do you mean that the model is aggregated with exact gradients, so that it makes sufficient progress?}
HE-based federated learning (HE-FL) encrypts local models on clients and performs model aggregation over ciphertexts on the server, as illustrated in Figure~\ref{fig:fedml-fhe}. This approach enables secure federated learning deployments without model performance drop \carlee{``performance'' is vague--say explicitly here that the convergence w.r.t. training rounds is unchanged because HE uses exact gradients for computation, compared to masking or DP} and has been adopted by several FL systems~\cite{nvidiafl,ibmfl,zhang2020batchcrypt, du2023efficient} and a few domain-specific applications~\cite{stripelis2021secure, yao2022fedgcn}. %\carlee{update the FedGCN reference here} %, providing a resilient alternative to traditional defense mechanisms.

\shanshan{may further discuss its advantage over MPC and DP: 1. robust to dropout; 2. do not modify gradients. 3. can satisfy higher requirements on privacy: users might think their local models are their own assets and do not wish to share the whole modle to other party. HE can satisfy this requirement while ensure accuracy. }


%add the applications after paper get accepted.
%Several HE-based solutions have been adopted for privacy-sensitive FL applications %(Figure~\ref{fig:applications}) 

%\yuhang{maybe replace the figure with the comparison table of other frameworks}.

% \begin{figure*}
% \includegraphics[width=1\textwidth]{figs/apps_fixed.pdf}
% \caption{Example Applications: secure federated neuroimaging learning~\cite{stripelis2021secure}, secure federated learning in IoT~\cite{zhang2022homomorphic, yao2022fedgcn, jin2022secure}, and secure federated NLP~\cite{lin2021fednlp}.}
% \label{fig:applications}
% \end{figure*}

\begin{table*}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
    Features&IBMFL& Nvidia FLARE & Ours \\ \hline
Homomorphic Encryption     &      \cmark &  \cmark  &   \cmark   \\ \hline
Threshold Key Management  &    \xmark &   $\bigcirc$ &    \cmark  \\ \hline
Efficient Encrypted Computation     &      \xmark&  $\bigcirc$&   \cmark   \\ \hline 
Practical Encrypted Foundation Model Training \ &     $\bigcirc$&  $\bigcirc$  &    \cmark  \\ \hline
\end{tabular}
\vspace{0.1cm}
\caption{\textbf{Comparison with Existing HE-Based FL Systems}: $\bigcirc$ implies limited support.}
\label{tab:compare_intro}
\end{table*}
% However, even with advances in the Homomorphic Encryption research community to improve HE performance both theoretically and engineeringly, HE as a strong but complex cryptographic foundation still has impractical overheads in real-world applications. Previous HE solutions only utilize existing generic HE methods without enough optimization for large-scale secure federated learning deployment. The scalability of encrypted computation and communication during federated training becomes a bottleneck, limiting its feasibility for real-world edge computing scenarios. This limitation from HE overheads is rather noticeable (commonly $\sim$15x increase in both computation and communication), especially when training large federated models across resource-constrained mobile devices, where encrypted computing and communicating large models might take much longer time than the actual model training. 

% %To overcome the bottleneck on encryption time and communication cost, we first optimize HE specified for FL with fundamental changes to speed up the encryption. We then provide a universal optimization scheme (Parameter Efficiency x Parameter Selection) to provide a co-optimization of communication cost and privacy preservation. Finally, 



% We then propose FedHE, an efficient Homomorphic-Encryption-based privacy-preserving federated learning system with a universal optimization scheme that can be practically deployed across distributed edge devices\footnote{Our code is open sourced at \textit{\{anonymized-for-submission\}}.}. 
%\carlee{what does ``systematic'' mean here? does ``algorithmic'' refer to the ML training algorithm?} 
 
\noindent\textbf{Overhead bottleneck of HE-FL}\shanshan{do we need this subtitle?} Despite the advantages of homomorphic encryption, HE remains a powerful but complex cryptographic foundation with impractical overheads for most real-world applications. Prior FL-HE solutions mainly employ existing generic HE methods without sufficient optimization for large-scale FL deployment~\cite{nvidiafl,ibmfl,zhang2020batchcrypt, du2023efficient}. The scalability of encrypted computation and communication during federated training then becomes a bottleneck, restricting its feasibility for real-world scenarios. This HE overhead limitation is particularly noticeable (\textit{commonly $\sim$15x increase in both computation and communication}~\cite{gouert2022new}) when training large foundation models across resource-constrained devices, where encrypted computing and communication of large models might take considerably longer than the actual model training.
\label{sec:microbenchmark}
\noindent It is widely known that HE inevitably introduces large overheads regarding both computation and communication~\cite{gouert2022new}. To verify this, we evaluate the vanilla HE implementation to pinpoint the overhead bottlenecks.\shanshan{use one sentence to explain the results and put the figures to experiment section?}\carlee{I agree that we do not need a figure here, we can just cite the figure and overall performance number and put its main explanation in a later section}

\begin{figure}[ht]
\includegraphics[width=0.49\textwidth]{figs/model_overhead.pdf}
% \includegraphics[width=0.23\textwidth]{figs/model_comm.pdf}
\caption{(Without Optimization) Computational (left) and Computation (right) Overhead Comparison for Models of Different Sizes: FedHE vs. Nvidia FLARE  vs. Plaintext Aggregation. Due to TenSeal's larger file sizes, FLARE did not finish the run on BERT on our 32GB memory machine.}
\label{fig:comp}
\end{figure}

To address these challenges, we propose FedHE, an efficient Homomorphic Encryption-based privacy-preserving FL system with Selective Parameter Encryption, designed for practical deployment across distributed edge devices\footnote{Our code is open sourced at \textit{\{anonymized-for-submission\}}.}. Our system significantly reduces overheads\shanshan{what overheads? storage/communication/computation/etc ?}, enabling HE-based federated learning to be more accessible and efficient in real-world scenarios. 

\noindent\textbf{Key contributions}: 
% \carlee{the title calls our work a ``library,'' but the text below never refers to a library.}
\begin{itemize}
    % \item We propose the algorithmic and programming \carlee{call this a system design} design for realizing HE functionality in the federated learning system with HE underlying foundation analysis. \carlee{does this include the HE ``optimizations'' mentioned in the paragraph above? try to be more specific about what exactly about HE is being optimized} Our HE-based framework supports deployment with the MLOps platform, which provides key management, system profiling, grouping and edge binding. \carlee{does this refer to a specific MLOps platform? If so, why did you choose it? How portable is your HE framework to other ML platforms?}

    
    % \item We first benchmark our HE-FL framework under different aspects of FL training, which helps us pinpoint the HE bottlenecks. We also compare with other HE-based solutions in which the results indicate that our vanilla solution already requires the smallest computational overhead without any optimization with more functionalities (more details in Table~\ref{tab:compare_intro} and Table~\ref{tab:frameworks}).

    % \item We benchmark our HE-FL framework under various aspects of FL training, which allows us to pinpoint the HE bottlenecks. Our comparison with other HE-based solutions reveals that our vanilla implementation already has the lowest computational overhead while offering more functionalities (see Table~\ref{tab:compare_intro} and Table~\ref{tab:frameworks}).

    % \item We propose a universal HE-FL optimization scheme \textbf{Parameter Efficiency} x \textbf{Parameter Selection} by minimizing the size of model updates for encrypted computation while preserving privacy guarantee to mitigate both computational and communicational overhead increases from HE operations. With optimization, our framework can achieve sizeable overhead reduction, especially on large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and $\sim$40x reduction for BERT respectively), which shows the feasibility of further real-world HE-based FL deployments. 

    \item We propose FedHE, the first practical Homomorphic Encryption-based privacy-preserving FL system that firstly supports key management, several optimization components to reduce overhead and is designed to support foundation model training 
    \item 
    selectively encrypts the most privacy-sensitive parameters to minimize the size of encrypted model updates while providing customizable privacy 
    preservation. Theoretical guarantees. %\carlee{parameter selection should be explained more here}\yuhang{added} 
    \item We integrate FedHE on a user/device-friendly deployment platform that includes key management, system profiling, grouping, and edge binding, which also enables easy monitoring of HE overhead distribution across edge devices. We also benchmark our FedHE framework under various aspects of FL training, which allows us to pinpoint the HE bottlenecks. \weizhao{explain "integrate"}
    \item  Extensive experiments show that the optimized system achieves significant overhead reduction, particularly for large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and $\sim$40x reduction for BERT), demonstrating the potential for HE-based FL deployments.
\end{itemize}


