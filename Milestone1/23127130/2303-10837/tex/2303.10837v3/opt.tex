\subsection{Efficient Optimization by Selective Parameter Encryption}
\label{sec:opt}
\begin{figure*}[ht]
\includegraphics[width=1\textwidth]{figs/mask_fixed.pdf}
\caption{\textbf{Selective Parameter Encryption}: in the initialization stage, clients first calculate privacy sensitivities on the model using its own dataset and local sensitivities will be securely aggregated to a global model privacy map. The encryption mask will be then determined by the privacy map and a set selection value $p$ per overhead requirements and privacy guarantee. Only the masked parameters will be aggregated in the encrypted form.}
\label{fig:mask}
\end{figure*}

\begin{figure*}[ht]
\includegraphics[width=1.0\textwidth]{figs/heatmap_lenet.pdf}
\caption{Model Privacy Map Calculated by Sensitivity on LeNet: darker color indicates higher sensitivity. Each subfigure shows the sensitivity of parameters of the current layer. The sensitivity of parameters is imbalanced and many parameters have very little sensitivity (its gradient is hard to be affected by tuning the data input for attack).}
\label{fig:lenet_map}
\end{figure*}

%\weizhao{@Yuhang, could you reorg this section to have subsections instead of paragrapghs with headings?}
%We then focus on reducing the input model sizes for HE functions as the primary optimization direction, which reduces the overheads of computation and communication simultaneously without loss of general usability and privacy. %The input model sizes are determined in two stages: the resulting model parameters from the ML pipeline and the chosen model parameters used in HE functions.

\noindent Fully encrypted models can guarantee no access to plaintext local models from the adversary with high overheads. However, previous work on privacy leakage analysis shows that ``partial transparency'', e.g. hiding parts of the models~\citep{hatamizadeh2022gradvit, mo2020layer}, can limit an adversary's ability to successfully perform attacks like gradient inversion attacks~\citep{lu2022april}. We therefore propose \textbf{Selective Parameter Encryption} to \textit{selectively encrypt the most privacy-sensitive parameters} in order to reduce impractical overhead while providing customizable privacy preservation; see Figure~\ref{fig:mask}.  %In the rest of \tsref{sec:opt}, we will discuss in detail how we perform selective parameter encryption.

%Although the FLARE~\citep{roth2022nvidia} code provides a configuration to only perform HE operations on certain layers, its interaction and tradeoffs regarding privacy leakage are unknown, leaving users a ``Pandora's privacy box''. On the contrary, our solution can support parameter-wise selection for more fine-grained overhead control with privacy leakage analysis (Figure~\ref{fig:mask}).


%We observe that transformers with (i) a smaller patch size, (ii) more parameters, and (iii) stronger training recipe with distillation, reveal more original information ", for transformer specifically, Component-wise, ", reconstructions from MLP gradients lack important details, whereas utilizing
%the gradients of MSA layers alone can already yield high-quality reconstructions
%\textbf{Parameter Privacy Sensitivity Map} 
% \carlee{explain what we do with this map first} 
%\noindent 

\textbf{Step 1: Privacy Leakage Analysis on Clients.} Directly performing a gradient inversion attack~\cite{wei2020framework} and evaluating the success rate of the attack can take much more time than the model training. 
% Differently, Mutual Information (MI)~\cite{xutheory} is proposed to calculate the usable information of model parameters while is practically similar to computing the area under the curve of attack models with even higher computation cost~\cite{mo2020layer}.
We then adopt sensitivity~\cite{novaksensitivity, sokolic2017robust, mo2020layer} for measuring the general privacy risk on gradients w.r.t. input. %Theoretically, if the privacy sensitivity of the plaintext parameters obtained by $\mathcal{A}$ is lower, the success rate of the attack can be also expected to be lower. 
Given model $\mathbf{W}$ and $K$ data samples with input matrix $\mathbf{X}$ and ground truth label vector $\mathbf{y}$, we compute the sensitivity for each parameter $w_m$ by 
$\frac{1}{K} \sum_{k=1}^K \left\|J_m\left(y_k\right) \right\|,$ 
where $J_m\left(y_k\right) =\frac{\partial}{\partial y_k}\left(\frac{\partial \ell\left(\mathbf{X}, \mathbf{y}, \mathbf{W}\right)}{\partial w_m}\right) \in R$, $\ell(\cdot)$ is the loss function given $\mathbf{X}$, $\mathbf{y}$ and $\mathbf{W}$, and $\left\|\cdot \right\|$ calculates the absolute value. The intuition is to calculate how large the gradient of the parameter will change with the true output $y_k$ for each data point $k$. Each client $i$ then sends the encrypted parameters sensitivity matrix $[\![\mathbf{S}_i]\!]$ to the server.


% block $\mathbf{w}_m$ (a 
%  group of parameters) w.r.t the ground truth label $\hat{\mathrm{Y}}$ with 

% $$
% \frac{1}{K \sqrt{|\mathrm{g}_m (\hat{\mathrm{Y}}_k )| }} \sum_{k=1}^K\left\|\frac{\mathbf{J}_m^{(\mathrm{g})}\left(\hat{\mathrm{Y}}_k\right)}{\operatorname{range}\left(\mathrm{g}_m\left(\hat{\mathrm{Y}}_k\right)\right)}\right\|_F,
% $$

% where $\mathbf{J}_m^{(\mathrm{g})}(\hat{\mathrm{Y}})=\frac{\partial \mathrm{g}_m(\hat{\mathrm{Y}})}{\partial \hat{\mathrm{Y}}}=\frac{\partial}{\partial \hat{\mathrm{Y}}}\left(\frac{\partial \ell\left(\hat{\mathrm{Y}}, \boldsymbol{\theta}_m\right)}{\partial \boldsymbol{\theta}_m}\right)$ and $\mathrm{g}_m(.) = \frac{\partial \ell\left(\hat{\mathrm{Y}}, \boldsymbol{\theta}_m\right)}{\partial \boldsymbol{\theta}_m}$ represents the function that produces block $m$ 's gradients $\mathrm{g}_m$ with true output $\hat{\mathrm{Y}}$. $\ell(.)$ is the loss function over $\hat{\mathrm{Y}}$ and $\boldsymbol{\theta}$ (parameters of the complete model). %so $\mathrm{g}_m($.$)$ can be regarded as the partial derivative of $\ell(.)$ w.r.t. block $m$'s parameters $\boldsymbol{\theta}_m$ (i.e. backward propagation). 
% Function range(.) returns the range of values in one vector (i.e. $\max ()-\min ())$. 




As shown in Figure~\ref{fig:lenet_map}, different parts of a model contribute to attacks by revealing uneven amounts of information. Using this insight, we propose to only select and encrypt parts of the model that are more important and susceptible to attacks to reduce HE overheads while preserving adequate privacy. 

\textbf{Step 2: Encryption Mask Agreement across Clients.} The sensitivity map is dependent on the data it is processed on. With potentially heterogeneous data distributions, the server aggregates local sensitivity maps to a global privacy map $\sum_{i=1}^N \alpha_i [\![\mathbf{S}_i]\!]$. The global encryption mask $\mathbf{M}$ is then configured using a privacy-overhead ratio $p\in[0,1]$ which is the ratio of selecting the most sensitive parameters for encryption. The global encryption mask is then shared among clients as part of the federated learning configuration. 

%We apply encryption masks with a set privacy threshold (e.g. 30\%) to pick parameters to encrypt that fit certain overhead expectations.

%\textbf{Mask Agreement} 
%The encryption mask is dependent on the data it is processed on. With potentially different data distributions at each client, it is challenging to have all clients agree on a certain global encryption mask without revealing local datasets since the local privacy map carries information about the client data. To solve this problem, we encrypted aggregate local privacy maps at the server such that no further private information about local data is revealed. During the initialization, clients first calculate the local privacy sensitivities using their own local data and then the server aggregates local sensitivity maps to a global privacy map (as shown in the first part of Figure~\ref{fig:mask}). The encryption mask is configured using a privacy-overhead threshold $s$ (which is the encrypted ratio of the entire model) and the global privacy map is then shared among clients as part of the federated learning configuration. 

%\carlee{I don't think this threshold has been defined yet?}

% In practice, privacy maps are usually sliced into around a hundred parts (i.e. a couple of hundred numbers to aggregate), the HE overhead from this mask agreement process is relatively small (Figure~\ref{fig:pie}).

% \subsubsection{Empirical Selection Recipes} 
% \hfill\\
% We then provide parameter selection strategies: \carlee{explain how these are related}
% \begin{itemize}
%     \item \noindent\textbf{Layer Importance}: Layer selection is a simpler variant of parameter selection. We rank the sensitivity of each layer, then encrypt the most important layers. Empirically, layers in the beginning and end stages tend to be more important for information leakage~\cite{hatamizadeh2022gradvit}.
%     \item \noindent\textbf{Training Stages}: \weizhao{whats the conclusion about this in the literature?}
% \end{itemize}


% \weizhao{Do we still want the parts below? Seems like a jump from selection}

% \begin{itemize}
%     \item \noindent\textbf{Multiple Local Steps}: the gradient inversion attack requires the gradient and the model, while in FL, we usually perform gradient descents with multiple local steps. The gradient of each step is unavailable by communicating the models, which increases the search space of attack. Based on ~\cite{wei2020framework}, when local step $>9$, the attack success rate is low. 
%     \item \noindent\textbf{Larger Batch Size}: Increasing the batch size of gradient descent will make the gradient inversion attack try to reconstruct all data in a batch at the same time, which will also increase the search space. Based on~\cite{huang2021evaluating}, when batch size $>32$, it makes the reconstruction almost unrecognizable.
%     %\item Batch Size~\cite{wei2020framework, huang2021evaluating}
%     %\item \noindent\textbf{Gradient Alternate Hiding}: to perform most gradient inversion attacks, the adversary has to calculate the gradients between each FL round's model updates. Alternately encrypting models in certain rounds can make it harder for the adversary to compute the fine-grained gradients at the first stage of attacks.
%     %\item \noindent\textbf{Training Round Guarding}: beginning rounds are more important to reduce attack effect since gradients change at bigger steps per training data.
%     %\item Larger Batch Size
% \end{itemize}
% We can then provide a simple empirical recipe, by \emph{encrypting 10\% model parameters and performing gradient descent with batch size $= 64$ and local step $=10$}, every single component can make attacks infeasible and all these components together can provide a good solution for preserving the privacy with low communication overhead.

%\weizhao{mathematical proofs here: 1. different layer importance 2. different training stage}

%\yuhang{1. Attack by masking a few layers or parameters is feasible? 2. Attacks on different layers 3. Attacks on different parameters 4. Attack on different training round. 5. Attack with accumulated training information? 6. Attack on multiple local training steps}



