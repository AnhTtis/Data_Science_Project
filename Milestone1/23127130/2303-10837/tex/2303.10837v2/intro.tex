\section{Introduction}
%\weizhao{Reminder: check whether all figures/algos/tables are referenced}
Federated learning (FL) is increasingly popular in contemporary machine learning practices due to its ability to allow distributed clients to collectively train a global model without directly sharing data. Privacy preservation in standard federated learning systems depends on the distributed training process and the model aggregation function, such as  FedAvg~\cite{mcmahan2017communication}, FedSGD~\cite{fedsgd}, and % FedOPT~\cite{reddi2021adaptive}, FedPROX~\cite{FedProx}, 
FedGAN~\cite{rasouli2020fedgan}. %, and FedNOVA~\cite{Wang2020TacklingTO}.
In FL, 
instead of uploading raw data to a central server for training, clients train models locally and share their models with the server, where the local models are aggregated based on the aggregation functions. 
While FL ensures that local raw data do not leave their original locations, 
it remains vulnerable to eavesdroppers and malicious FL servers that might exploit plaintext local models (or model updates) 
to reconstruct sensitive training data, i.e., data reconstruction attacks or gradient inversion attacks in literature~\cite{zhu2019deep, criswell2014kcofi,bhowmick2018protection, hitaj2017deep,han2023fedmlsecurity,hatamizadeh2022gradvit,fowl2022decepticons}, as shown in Figure~\ref{fig:attack_flow}. %  this approach exposes individual local models to the aggregation server and potentially eavesdroppers in the system, %\carlee{is this true for foundation models?}
This poses a privacy vulnerability especially when local models are trained on small local datasets, a common scenario in real-world applications such as smartphone text data for LLMs. Local models derived from these small datasets inherently contain fine-grained information, making it easier for adversaries to extract sensitive information from small model updates. 
% \textcolor{red}{Adversaries that are curious about clients' training data may utilize the clients' model updates to extract local training data, \textit{i}.\textit{e}., gradient reversion attacks or data reconstruction attacks in literature. As an example, in an FL training iteration, after collecting model updates from clients, a compromised server may recover private training data, such as medical records, credit cards, smart home deployment, or private messages, , thus may violate privacy. }\shanshan{I rewrote this part and commented the original description. If it's not concise and clear, just revert to the original version.}
%FL inversion attacks~\cite{zhu2019deep, criswell2014kcofi,bhowmick2018protection, hitaj2017deep, hatamizadeh2022gradvit} have been proposed to extract private training data information and compromise individual privacy. 
% As illustrated in Figure~\ref{fig:attack_flow}, a compromised server can conduct attacks to recover personal data used for local training, which can expose sensitive personal information, e.g., medical records, credit cards, smart home deployment, or private messages.
\begin{table*}[t]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
                        & \makecell{Model\\ Degradation} & Overheads & Client Dropout & \makecell{Interactive\\ Sync} & \makecell{Aggregated Model\\Visible To Server} \\ \hline
Differential Privacy    & With noise                   & \textbf{Light}        & \textbf{Robust}           & \textbf{No}     & Yes          \\ \hline
Secure Aggregation & \textbf{Exact}                       & Medium       & Susceptible            & Yes   & Yes           \\ \hline
Homomorphic Encryption  & \textbf{Exact}                & Large       & \textbf{Robust}          & \textbf{No}     &\textbf{No}         \\ \hline

\end{tabular}
\caption{Comparison of Differential Privacy, Secure Aggregation, and Homomorphic Encryption}
\label{tab:dp_mpc_he_compare}
\end{table*}

\begin{table*}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
    Features&IBMFL& Nvidia FLARE & Ours \\ \hline
Homomorphic Encryption     &      \cmark &  \cmark  &   \cmark   \\ \hline
Threshold Key Management  &    \xmark &   \xmark &    \cmark  \\ \hline
% Efficient Encrypted Computation
Selective Parameter Encryption&      \xmark&  $\bigcirc$&   \cmark   \\ \hline 
Encrypted Foundation Model Training \ &     $\bigcirc$&  $\bigcirc$  &    \cmark  \\ \hline
\end{tabular}
\vspace{0.1cm}
\caption{Comparison with Existing HE-Based FL Systems. $\bigcirc$ implies limited support: for Selective Parameter Encryption, FLARE offers the (random) partial encryption option which does not have clear indications on privacy impacts; for Encrypted Foundation Model Training, the other two platforms require massive resources to train foundation models in encrypted federated learning.}
\label{tab:compare_intro}
\end{table*}

\begin{figure}
    \centering
     \frame{\includegraphics[width=0.47\textwidth]{figs/attack_flow_crop.pdf}}
    \caption{Data Reconstruction Attacks: an adversarial server can recover local training data from local model updates.}
    \label{fig:attack_flow}
\end{figure}
% \begin{figure*}[htbp]
%   \centering
%   \begin{subfigure}[t]{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/attack_flow_crop.pdf}
%     \caption{ FL Inversion Attacks: an adversary server can recover local training data from unprotected local model updates.}
%     \label{fig:attack_flow}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[t]{0.5\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/fedml-fhe_crop.pdf}
%     \caption{HE-based Federated Aggregation: models are encrypted and the server acts as a computing service without access to models.}
%     \label{fig:fedml-fhe}
%     \end{subfigure}
%     \caption{FL Inversion Attacks and HE-based Federated Aggregation Workflow.}
% \end{figure*}
% Existing defense mechanisms to prevent the FL server from learning the plaintext local models include general Multi-Party Computation (MPC) secure aggregation protocols~\cite{bonawitz2017practical, so2022lightsecagg} by masking private inputs and noise-based Differential Privacy (DP) solutions~\cite{truex2019hybrid,byrd2020differentially} by adding privacy noise to original inputs. However, they either require extra interactive synchronization steps while being prone to failures like client dropout (MPC) which makes prudent engineering mandatory during deployment~\cite{zhang2020batchcrypt}
% %\carlee{isn't HE also vulnerable to collusion? If all clients but one are malicious, they can exactly infer the other client's model from the model updates}\weizhao{I used the wrong wording here. Was meant to point out that it is hard for MPC solutions to handle situations like client dropouts}
% or introduce the issue of model performance degradation due to privacy noises (DP). Homomorphic Encryption (HE)~\cite{paillier1999public,gentry2009fully, cryptoeprint:2012/144, brakerski2014leveled, cheon2017homomorphic}, on the other hand, provides a resilient post-quantum solution that preserves local models to protect against attacks and guarantee local model privacy while still providing adequate utility. HE-based federated aggregation works by encrypting local models upon aggregation and performing model aggregating in the form of ciphertexts, which has been adopted by a few FL systems~\cite{nvidiafl,ibmfl,zhang2020batchcrypt, du2023efficient}. 
Existing defense methods that prevent privacy leakage from plaintext local models include differential privacy (DP)~\cite{truex2019hybrid,byrd2020differentially} and secure aggregation~\cite{bonawitz2017practical, so2022lightsecagg}. 
DP adds noise to original models but may result in model performance degradation due to the privacy noises introduced. On the other hand, secure aggregation employs zero-sum masks to shield local model updates, ensuring that the details of each update remain private. However, secure aggregation demands additional interactive synchronization steps and is sensitive to client dropout, making it less practical in real-world FL applications, where the unstable environments of clients face challenges such as unreliable internet connections, and software crashes.



%\shanshan{need to describe more seriously, e.g., secure aggregation is not suitable for real-world edge device applications, as edge devices often drop out due to complex real-world situations, e.g., internet connection, poor cell phone signal, application crash or no connection due to an incoming call, limited computing resources, the cell phone is forced to shut down due to low battery...}, 
% while DP can result in model performance degradation due to privacy noises.\shanshan{maybe we can explain in this way: we add dp noise to gradients, but the output of the model might not be controlled, e.g., they might change the results by classifying a data to a different class.}
As shown in Table~\ref{tab:dp_mpc_he_compare}, compared to the non-HE FL solutions above, homomorphic encryption (HE)~\cite{paillier1999public,gentry2009fully, cryptoeprint:2012/144, brakerski2014leveled, cheon2017homomorphic} offers a robust post-quantum secure solution that protects local models against attacks and \textit{provides stronger privacy guarantee while keeping the model aggregation with exact gradients}. %\carlee{be more explicit that HE offers stronger privacy guarantees} %\carlee{``utility'' is not defined here. Do you mean that the model is aggregated with exact gradients, so that it makes sufficient progress?}
HE-based federated learning (HE-FL) encrypts local models on clients and performs model aggregation over ciphertexts on the server. This approach enables secure federated learning deployments with exactly the same model performance as vanilla FL %\carlee{``performance'' is vague--say explicitly here that the convergence w.r.t. training rounds is unchanged because HE uses exact gradients for computation, compared to masking or DP} 
and has been adopted by several FL systems~\cite{roth2022nvidia,ibmfl,zhang2020batchcrypt, du2023efficient} and a few domain-specific applications~\cite{stripelis2021secure, yao2022fedgcn}. %\carlee{update the FedGCN reference here} %, providing a resilient alternative to traditional defense mechanisms.

%\shanshan{may further discuss its advantage over MPC and DP: 1. robust to dropout; 2. do not modify gradients. 3. can satisfy higher requirements on privacy: users might think their local models are their own assets and do not wish to share the whole modle to other party. HE can satisfy this requirement while ensure accuracy. }


%add the applications after paper get accepted.
%Several HE-based solutions have been adopted for privacy-sensitive FL applications %(Figure~\ref{fig:applications}) 

%\yuhang{maybe replace the figure with the comparison table of other frameworks}.

% \begin{figure*}
% \includegraphics[width=1\textwidth]{figs/apps_fixed.pdf}
% \caption{Example Applications: secure federated neuroimaging learning~\cite{stripelis2021secure}, secure federated learning in IoT~\cite{zhang2022homomorphic, yao2022fedgcn, jin2022secure}, and secure federated NLP~\cite{lin2021fednlp}.}
% \label{fig:applications}
% \end{figure*}


% However, even with advances in the Homomorphic Encryption research community to improve HE performance both theoretically and engineeringly, HE as a strong but complex cryptographic foundation still has impractical overheads in real-world applications. Previous HE solutions only utilize existing generic HE methods without enough optimization for large-scale secure federated learning deployment. The scalability of encrypted computation and communication during federated training becomes a bottleneck, limiting its feasibility for real-world edge computing scenarios. This limitation from HE overheads is rather noticeable (commonly $\sim$15x increase in both computation and communication), especially when training large federated models across resource-constrained mobile devices, where encrypted computing and communicating large models might take much longer time than the actual model training. 

% %To overcome the bottleneck on encryption time and communication cost, we first optimize HE specified for FL with fundamental changes to speed up the encryption. We then provide a universal optimization scheme (Parameter Efficiency x Parameter Selection) to provide a co-optimization of communication cost and privacy preservation. Finally, 



% We then propose FedML-HE, an efficient Homomorphic-Encryption-based privacy-preserving federated learning system with a universal optimization scheme that can be practically deployed across distributed edge devices\footnote{Our code is open sourced at \textit{\{anonymized-for-submission\}}.}. 
%\carlee{what does ``systematic'' mean here? does ``algorithmic'' refer to the ML training algorithm?} 
 
Despite the advantages of homomorphic encryption, HE remains a powerful but complex cryptographic foundation with impractical overheads (as shown in Figure~\ref{fig:comp}) for most real-world applications. Prior FL-HE solutions mainly employ existing generic HE methods without sufficient optimization for large-scale FL deployment~\cite{roth2022nvidia,ibmfl,zhang2020batchcrypt, du2023efficient}. The scalability of encrypted computation and communication during federated training then becomes a bottleneck, restricting its feasibility for real-world scenarios. This HE overhead limitation is particularly noticeable (\textit{commonly $\sim$15x increase in both computation and communication}~\cite{gouert2022new}) when training large foundation models across resource-constrained devices, where encrypted computing and communication of large models might take considerably longer than the actual model training.
\label{sec:microbenchmark}
\noindent It is widely known that HE inevitably introduces large overheads regarding both computation and communication~\cite{gouert2022new}. To verify this, we evaluate the vanilla HE implementation to pinpoint the overhead bottlenecks.

\textit{Observation}: As shown by the evaluation results in Figure~\ref{fig:comp}, the computational and communication (package size) overheads introduced by HE is $O(n)$, both growing linearly with the input size $n$, which in our case the sizes of the models for aggregation. \emph{Although the unoptimized system is faster than Nvidia FLARE, the execution time and file size are still impractical, especially for large models.}

%\shanshan{use one sentence to explain the results and put the figures to experiment section?}\carlee{I agree that we do not need a figure here, we can just cite the figure and overall performance number and put its main explanation in a later section}

\begin{figure}[ht]
\includegraphics[width=0.49\textwidth]{figs/model_overhead.pdf}
% \includegraphics[width=0.23\textwidth]{figs/model_comm.pdf}
\caption{Computational (left) and Computation (right) Overhead Comparison for Models of Different Sizes: Naive FedML-HE vs. Nvidia FLARE  vs. Plaintext Aggregation. Due to TenSeal's larger file sizes, FLARE did not finish the run on BERT on our 32GB memory machine.} %\shanshan{this figure is only referenced in \S3.4, too far away from \S1}
\label{fig:comp}
\end{figure}

\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{figs/fedhe_system.pdf}
\caption{FedML-HE System Pipeline: in the \textbf{Encryption Key Agreement} stage, clients can either use distributed threshold key agreement protocol or outsource a trusted key authority. We simplify the illustration here by abstracting the key pair of the public key and secret key (partial secret keys if using threshold protocol) as one key; in the \textbf{Encryption Mask Calculation} stage, clients use local datasets to calculate local model sensitivity maps which are homomorphically aggregated at the server to generate an encryption mask; in the \textbf{Encrypted Federated Learning} stage, clients use homomorphic encryption with encryption mask to protect local model updates where the server aggregates them but does not have access to sensitive local models.}
\label{fig:mlops-flow}
\end{figure*}

To address these challenges, we propose FedML-HE, an efficient Homomorphic Encryption-based privacy-preserving FL system with \textit{Selective Parameter Encryption}, designed for practical deployment across distributed edge devices. Our system significantly reduces communication and computation overheads, enabling HE-based federated learning to be more accessible and efficient in real-world scenarios (comparison with other popular HE-based FL work can be found in Table~\ref{tab:compare_intro}). 


%\sri{Table 1 needs to be revised, not sure threshold key management is a research feature. What is practical encrypted foundation model training? Selective parameter encryption should be a row as a feature since that is the main technical contribution}

\noindent\textbf{Key contributions}: 
% \carlee{the title calls our work a ``library,'' but the text below never refers to a library.}
\begin{itemize}[noitemsep,topsep=0pt]
    % \item We propose the algorithmic and programming \carlee{call this a system design} design for realizing HE functionality in the federated learning system with HE underlying foundation analysis. \carlee{does this include the HE ``optimizations'' mentioned in the paragraph above? try to be more specific about what exactly about HE is being optimized} Our HE-based framework supports deployment with the MLOps platform, which provides key management, system profiling, grouping and edge binding. \carlee{does this refer to a specific MLOps platform? If so, why did you choose it? How portable is your HE framework to other ML platforms?}

    
    % \item We first benchmark our HE-FL framework under different aspects of FL training, which helps us pinpoint the HE bottlenecks. We also compare with other HE-based solutions in which the results indicate that our vanilla solution already requires the smallest computational overhead without any optimization with more functionalities (more details in Table~\ref{tab:compare_intro} and Table~\ref{tab:frameworks}).

    % \item We benchmark our HE-FL framework under various aspects of FL training, which allows us to pinpoint the HE bottlenecks. Our comparison with other HE-based solutions reveals that our vanilla implementation already has the lowest computational overhead while offering more functionalities (see Table~\ref{tab:compare_intro} and Table~\ref{tab:frameworks}).

    % \item We propose a universal HE-FL optimization scheme \textbf{Parameter Efficiency} x \textbf{Parameter Selection} by minimizing the size of model updates for encrypted computation while preserving privacy guarantee to mitigate both computational and communicational overhead increases from HE operations. With optimization, our framework can achieve sizeable overhead reduction, especially on large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and $\sim$40x reduction for BERT respectively), which shows the feasibility of further real-world HE-based FL deployments. 

    \item We propose FedML-HE, the first practical Homomorphic Encryption-based privacy-preserving FL system that supports encryption key management, encrypted FL platform deployment, encryption optimizations to reduce overhead, and is designed to support efficient foundation model federated training.
    \item We propose \textbf{Selective Parameter Encryption} that selectively encrypts the most privacy-sensitive parameters to minimize the size of encrypted model updates while providing customizable privacy 
    preservation. 
    \item Theoretical privacy analysis shows the HE system can ensure privacy under single-key and threshold adversaries and encrypting most sensitivity parameters provides orders-of-magnitude better privacy guarantees. %We are also the first proof HE can fit in differential privacy analysis.
    
    %\item We integrate homomorphic encryption into a federated learning deployment platform that supports modules for preferred homomorphic encryption libraries, homomorphic encryption key agreement, and extensive privacy optimizations.
    %\sri{we should not stress on this customizable, feels too cryptic to me and its not like a provide an interface for cuztomization anyways; we should simply focus on the selective parameter encryption and doin this in a rigorous way which to the best our knowledge is the first approach for federated HE}    
    \item  Extensive experiments show that the optimized system achieves significant overhead reduction while preserving privacy against state-of-the-art ML privacy attacks, particularly for large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and up to $\sim$40x reduction for BERT), demonstrating the potential for real-world HE-based FL deployments.
\end{itemize}





