\newpage
\appendix
%\section{Supplementary Material}
\section{Preliminaries}
\subsection{Federated Learning}
Federated learning is first proposed in~\cite{mcmahan2017communication}, which builds distributed machine learning models while keeping personal data on clients. Instead of uploading data to the server for centralized training, clients process their local data and share updated local models with the server. Model parameters from a large population of clients are aggregated by the server and combined to create an improved global model. 

The FedAvg~\cite{mcmahan2017communication} is commonly used on the server to combine client updates and produce a new global model. At each round, a global model $\mathbf{W}_\text {glob}$ is sent to $N$ client devices.
Each client $i$ performs gradient descent on its local data with $E$ local iterations to update the model $\mathbf{W}_i$.
%For a client learning rate $\eta$, the local client update of 1 local epoch, $w_k$, is given by
%\begin{equation}
%   w_k\leftarrow w_k - \eta g_k.
%\end{equation}
The server then does a weighted aggregation of the local models to obtain a new global model, $\mathbf{W}_{\text {glob}}=\sum_{i=1}^N \alpha_i \mathbf{W}_i,$ where $\alpha_i$ is the weighting factor for client $i$.
%the number of local data points in

Typically, the aggregation runs using plaintext model parameters through a central server  (in some cases, via a decentralized protocol), giving the server visibility of each local client's model in plaintext.

\subsection{Homomorphic Encryption}
\input{fhe.tex}
 Homomorphic Encryption is a cryptographic primitive that allows computation to be performed on encrypted data without revealing the underlying plaintext. It usually serves as a foundation for privacy-preserving outsourcing computing models. HE has generally four algorithms (\textit{KeyGen}, \textit{Enc}, \textit{Eval}, \textit{Dec}) as defined in Figure~\ref{fig:fhe}. The fundamental concept is to encrypt data prior to computation, perform the computation on the encrypted data without decryption, and then decrypt the resulting ciphertext to obtain the final plaintext.

Since FL model parameters are usually not integers, our method is built on the Cheon-Kim-Kim-Song (CKKS) scheme~\cite{cheon2017homomorphic}, a (leveled) HE variant that can work with approximate numbers.


\section{Key Management And Threshold HE}
\label{sec:key_management}
Our general system structure assumes the existence of a potentially compromised aggregation server, which performs the HE-based secure aggregation. Alongside this aggregation server, there also exists a trusted key authority server that generates and distributes HE keys and related crypto context files to authenticated parties (as described previously in Algorithm 1 in the main paper. We assume there is no collusion between these two servers.

Moreover, secure computation protocols for more decentralized settings without an aggregation server are also available using cryptographic primitives such as Threshold HE~\cite{aloufi2021computing}, Multi-Key HE~\cite{aloufi2021computing}, and Proxy Re-Encryption~\cite{ateniese2006improved, jin2022secure}. In such settings, secure computation and decryption can be collaboratively performed across multiple parties without the need for a centralized point. We plan to introduce a more decentralized version of FedML-HE in the future. Due to the collaborative nature of such secure computation, the key management will act more as a coordination point instead of a trusted source for key generation. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figs/model_threshold.pdf}
\caption{Microbenchmark of Threshold-HE-Based FedAvg Implementation: we use a two-party threshold setup. Both the single-key variant and the threshold variant are configured with an estimated precision of 36 for a fair comparison.}
\label{fig:threshold-he}
\end{figure}

The threshold variant of HE schemes is generally based on Shamir's secret sharing~\cite{shamir1979share} (which is also implemented in PALISADE). Key generation/agreement and decryption processes are in an interactive fashion where each party shares partial responsibility of the task. Threshold key generation results in each party holding a share of the secret key and threshold decryption requires each party to partially decrypt the final ciphertext result and merge to get the final plaintext result. We provide benchmarkings of the threshold-HE-based FedAvg implementation in Figure~\ref{fig:threshold-he}.

\begin{comment}
    

\section{FedML-HE Tutorial}
In this section, we provide an easy-to-follow tutorial on how to deploy FedML-HE for real applications\footnote{Detailed Tutorial for MLOps can be found at \url{https://doc.fedml.ai/mlops/user_guide.html}.}.
\label{sec:tutorial}

\begin{figure*}
\includegraphics[width=1\textwidth]{figs/mlops_config.png}
\caption{MLOps Setup Using Server/Client Packages}
\label{fig:mlops_config}
\end{figure*}

\begin{figure*}
\includegraphics[width=1\textwidth]{figs/he_mlops.png}
\caption{System Panel Results of An Example Run Using FedML-HE}
\label{fig:mlops_he}
\end{figure*}

\begin{figure*}
\includegraphics[width=1\textwidth]{figs/plain_mlops.png}
\caption{System Panel Results of An Example Run Using Plaintext FedML}
\label{fig:mlops_plain}
\end{figure*}

\end{comment}



\section{Framework APIs and Platform Deployment}
\subsection{Framework APIs}
Table~\ref{tab:apis} shows the framework APIs in our system related to HE.

\begin{table*}[ht!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        API Name & Description \\ \hline
        \textit{pk, sk} =  \textbf{key\_gen}(\textit{params}) & \makecell{Generate a pair of HE keys\\(public key and private key)} \\ \hline
        \textit{1d\_local\_model} = \textbf{flatten}(\textit{local\_model}) & \makecell{Flatten local trained model\\tensors into a 1D local model}\\ \hline
        \textit{enc\_local\_model} = \textbf{enc}(\textit{pk, 1d\_model}) & Encrypt the 1D model \\ \hline
        \makecell{\textit{enc\_global\_model} = \textbf{he\_aggregate}(\\\textit{enc\_models[n], weight\_factors[n]})} & \makecell{Homomorphically aggregate\\a list of 1D local models}\\ \hline
        \textit{dec\_global\_model} = \textbf{dec}(\textit{sk, enc\_global\_model}) & Decrypt the 1D global model \\ \hline
        \makecell{\textit{global\_model} = \textbf{reshape}(\\\textit{dec\_global\_model, model\_shape})} & \makecell{Reshape the 1D global model\\back to the original shape}\\ \hline
    \end{tabular}
\caption{HE Framework APIs}
\label{tab:apis}
\end{table*}

\subsection{Deploy Anywhere: A Deployment Platform MLOps For Edges/Cloud}

We implement our deployment-friendly platform such that FedML-HE can be easily deployed across cloud and edge devices.. Before the training starts, a user uploads the configured server package and the local client package to the web platform. The server package defines the operations on the FL server, such as the aggregation function and client sampling function; the local client package defines the customized model architecture to be trained (model files will be distributed to edge devices in the first round of the training). Both packages are written in Python. The platform then builds and runs the docker image with the uploaded server package to operate as the server for the training with edge devices configured using the client package.
% In the first round of the training, the server would distribute the Python-described model code to the edge devices, which would directly load the model architecture information using the TorchScipt or MNN engine with the C++ code. In the following rounds of training, the server and clients would only transmit the tensors of the model weights to save communication costs.



As shown in Figure~\ref{fig:mlops}, during the training, users can also keep tracking the learning procedure including device status, training progress/model performance, and FedML-HE system overheads (e.g., training time, communication time, CPU/GPU utilization, and memory utilization) via the web interface. Our platform keeps close track of overheads, which allows users to in real-time pinpoint HE overhead bottlenecks if any.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{figs/mlops.pdf}
\caption{Deployment Interface Example of FedML-HE: Overhead distribution monitoring on each edge device (e.g. Desktop (Ubuntu), Laptop (MacBook), and Raspberry Pi 4), which can be used to pinpoint HE overhead bottlenecks and guide optimization.}
\label{fig:mlops}
\end{figure*}
% Currently, we provide three experimental tracking abilities: monitoring device status and training progress of each edge device, visualizing the curves of training loss and validation performance, and visualizing system performance (e.g., training time, communication time, CPU/GPU utilization, and memory utilization). Besides these, users could also view the logs from both server and client devices on the MLOps web during the training, which provides convenience to debug the code under the real edge-server deployment.

% After it is verified that federated learning can produce modeling benefits on specific applications in the experimental simulation, users can use our platform to upgrade the simulation into production without modifying the code. The simulated source code can be deployed directly to edge devices with real data without
% further modification.  Such a deployment platform not only lowers the learning curve and product deployment difficulty of he FL, but also allows a user-friendly interface for pinpointing HE overhead bottlenecks. As shown in Figure~\ref{fig:mlops}, the distributed system overheads of FedML-HE are visualized and can be monitored by users in real time.

% \begin{figure*}[ht]
% \includegraphics[width=1\textwidth]{figs/mlops_working_flow.jpg}
% \caption{The working flow of MLOps. After the user finishes the local developing of the project using FedML, MLOps could smoothly deploy it into the real-world edge-cloud system without modification of code. \yuhang{Will redraw the figure.}}
% \label{fig:mlops_workingflow}
% \end{figure*}

\section{Additional Experiments}
\label{sec:he-benchmark}

\begin{table*}[ht]
    \small
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline\hline
        Model & Model Size & \makecell[l]{HE\\ Time (s)} & \makecell[l]{Non-HE\\ Time (s)} & \makecell[l]{Comp\\Ratio} & Ciphertext & Plaintext& \makecell[l]{Comm\\Ratio}  \\ \hline\hline
        Linear Model  & 101 & 0.216 & 0.001 & 150.85 & 266.00 KB & 1.10 KB& 240.83 \\ \hline
        \makecell[l]{TimeSeries\\Transformer} & 5,609 & 2.792 & 0.233 & 12.00 & 532.00 KB & 52.65 KB& 10.10 \\ \hline
        MLP (2 FC) & 79,510 & 0.586 & 0.010 & 60.46 & 5.20 MB& 311.98 KB & 17.05 \\ \hline
        LeNet  & 88,648 & 0.619 & 0.011 & 57.95 & 5.97 MB& 349.52 KB & 17.50 \\ \hline
        \makecell{RNN(2 LSTM\\ + 1 FC)}  & 822,570 & 1.195 & 0.013 & 91.82 & 52.47 MB& 3.14 MB& 16.70 \\ \hline
        \makecell{CNN (2 Conv\\ + 2 FC)} & 1,663,370 & 2.456 & 0.058 & 42.23 & 103.15 MB& 6.35 MB& 16.66 \\ \hline
        MobileNet  & 3,315,428 & 9.481 & 1.031 & 9.20 & 210.41 MB& 12.79 MB& 16.45 \\ \hline
        ResNet-18 & 12,556,426 & 19.950 & 1.100 & 18.14 & 796.70 MB& 47.98 MB& 16.61 \\ \hline
        ResNet-34 & 21,797,672 & 37.555 & 2.925 & 12.84 & 1.35 GB& 83.28 MB& 16.60 \\ \hline
        ResNet-50 & 25,557,032 & 46.672 & 5.379 & 8.68 & 1.58 GB& 97.79 MB& 16.58 \\ \hline
        GroupViT & 55,726,609 & 86.098 & 19.921 & 4.32 & 3.45 GB& 212.83 MB& 16.61 \\ \hline
        \makecell{Vision\\ Transformer} & 86,389,248 & 112.504 & 17.739 & 6.34 & 5.35 GB& 329.62 MB&16.62 \\ \hline
        BERT & 109,482,240 & 136.914 & 19.674 & 6.96 & 6.78 GB& 417.72 MB& 16.62 \\ \hline
        Llama 2 & 6.74 B  & 13067.154 & 2423.976 & 5.39 & 417.43 GB& 13.5 GB& 30.92 \\ \hline
    \end{tabular}
    \caption{Vanilla Fully-Encrypted Models of Different Sizes: with $3$ clients; Comp Ratio is calculated by time costs of HE over time costs of Non-HE; Comm Ratio is calculated by file sizes of HE over file sizes of Non-HE. CKKS is configured with default crypto parameters.}
    \label{tab:models}
\end{table*}


% \carlee{put a summary here of this section and the research questions it answers--all of this is interesting but having 5 subsubsections makes it hard to absorb. Maybe even just list the factors whose impacts you evaluate and why they are important.}\weizhao{added}


We evaluate the HE-based training overheads (without our optimization in place) across various FL training scenarios and configurations. This analysis covers diverse model scales, HE cryptographic parameter configurations, client quantities involved in the task, and communication bandwidths. This helps us to identify bottlenecks in the HE process throughout the entire training cycle. We also benchmark our framework against other open-source HE solutions to demonstrate its advantages.

\subsection{Parameter Efficiency Techniques in HE-Based FL}
\label{sec:eff_exps}
Table~\ref{tab:eff} shows the optimization gains by applying model parameter efficiency solutions in HE-Based FL.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
        \makecell{Models} & PT (MB)& CT & \makecell{Opt\\(MB)} \\ \hline
        \makecell{ResNet-18 \\(12 M)\\~\cite{tang2019doublesqueeze}} & 47.98 & 796.70 MB & 19.03  \\\hline
        \makecell{BERT\\ (110 M)\\~\cite{hu2021lora}} & 417.72 & 6.78 GB & 16.66  \\\hline
    \end{tabular}
    \caption{Parameter Efficiency Overhead: PT means plaintext and CT means ciphertext. Communication reductions are 0.60 and 0.96.}
    \label{tab:eff}
\end{table}


\subsection{Results on Different Scales of Models}
\label{sec:models}
\hfill\\
We evaluate our framework on models with different size scales and different domains, from small models like the linear model to large foundation models such as Vision Transformer~\cite{dosovitskiy2020image} and BERT~\cite{devlin2018bert}. As Table~\ref{tab:models} show, both computational and communicational overheads are generally proportional to model sizes.

Table~\ref{tab:models} illustrates more clearly the overhead increase from the plaintext federated aggregation. The computation fold ratio is in general $5$x $\sim 20$x while the communication overhead can jump to a common $15$x. Small models tend to have a higher computational overhead ratio increase. This is mainly due to the standard HE initialization process, which plays a more significant role when compared to the plaintext cost. The communication cost increase is significant for models with sizes smaller than $4096$ (the packing batch size) numbers. Recall that the way our HE core packs encrypted numbers makes an array whose size is smaller than the packing batch size still requires a full ciphertext.



\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline\hline
        \makecell{HE\\Batch\\Size} & \makecell{Scaling\\Bits} & \makecell{Comp\\(s)} & \makecell{Comm\\(MB)} & \makecell{Model Test \\Accuracy \\$\Delta$ (\%)} \\ 
        \hline\hline
        1024 & 14 & 8.834 & 407.47 & -0.28 \\ \hline
        1024 & 20 & 7.524 & 407.47 & -0.21 \\ \hline
        1024 & 33 & 7.536 & 407.47 & 0 \\ \hline
        1024 & 40 & 7.765 & 407.47 & 0 \\ \hline
        1024 & 52 & 7.827 & 407.47 & 0 \\ \hline
        2048 & 14 & 3.449 & 204.50 & -0.06 \\ \hline
        2048 & 20 & 3.414 & 204.50 & -0.13 \\ \hline
        2048 & 33 & 3.499 & 204.50 & 0 \\ \hline
        2048 & 40 & 3.621 & 204.50 & 0 \\ \hline
        2048 & 52 & 3.676 & 204.50 & 0 \\ \hline
        4096 & 14 & 1.837 & 103.15 & -1.85 \\ \hline
        4096 & 20 & 1.819 & 103.15 & 0.32 \\ \hline
        4096 & 33 & 1.886 & 103.15 & 0 \\ \hline
        4096 & 40 & 1.998 & 103.15 & 0 \\ \hline
        4096 & 52 & 1.926 & 103.15 & 0 \\ \hline
    \end{tabular}
\caption{Computational \& Communicational Overhead of Different Crypto Parameter Setups: tested with CNN (2 Conv+ 2 FC) and on 3 clients; model test accuracy $\Delta$s is the difference between the best plaintext global model and the best global encrypted global models.}
\vspace{-0.5cm}
\label{tab:params}
\end{table}

\subsection{Results on Different Cryptographic Parameters}
\hfill\\
We evaluate the impacts of variously-configured cryptographic parameters. We primarily look into the packing batch size and the scaling bits. The packing batch size determines the number of slots packed in a single ciphertext while the scaling bit number affects the ``accuracy'' (i.e., how close the decrypted ciphertext result is to the plaintext result) of approximate numbers represented from integers. 

From Table~\ref{tab:params}, the large packing batch sizes in general result in faster computation speeds and smaller overall ciphertext files attributed to the packing mechanism for more efficiency. However, the scaling factor number has an almost negligible impact on overheads.


Unsurprisingly, it aligns with the intuition that the higher bit scaling number results in higher ``accuracy'' of the decrypted ciphertext value, which generally means the encrypted aggregated model would have a close model test performance to the plaintext aggregated model. However, it is worth mentioning that since CKKS is an approximate scheme with noises, the decrypted aggregated model can yield either positive or negative model test accuracy $\Delta$s, but usually with a negative or nearly zero $\Delta$.




\subsection{Impact from Number of Clients}
\hfill\\
As real-world systems often experience a dynamic amount of participants within the FL system, we evaluate the overhead shift over the change in the number of clients. Figure~\ref{fig:n_200} breaks down the cost distribution as the number of clients increases. With a growing number of clients, it also means proportionally-added ciphertexts as inputs to the secure aggregation function thus the major impact is cast on the server. When the server is overloaded, our system also supports client selection to remove certain clients without largely degrading model performance.


\begin{figure}[htbp]
  \centering

  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/client_number_200_step.pdf}
    \caption{Step Breakdown of HE Computational Cost vs. Number of Clients (Up to 200): tested on fully-encrypted CNN}
    \label{fig:n_200}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
        \includegraphics[width=\textwidth]{figs/comm_bar.pdf}
        \caption{Impact of Different Bandwidths on Communication and Training Cycles on Fully-Encrypted ResNet-50: HE means HE-enabled training and Non means plaintext. Others include all other procedures except communication during training. Percentages represent the portion of communication cost in the entire training cycle.}
        \label{fig:comm_bar}
    \end{subfigure}
    \caption{Results on Different Number of Clients and Communication Setup}
\end{figure}


\subsection{Communication Cost on Different Bandwidths}
FL parties can be allocated in different geo-locations which might result in communication bottlenecks. Typically, there are two common scenarios: (inter) data centers and (intra) data centers. In this part, we evaluate the impact of the bandwidths on communication costs and how it affects the FL training cycle. We categorize communication bandwidths using $3$ cases:
\begin{itemize}
    \item Infiniband (IB): communication between intra-center parties. 5 GB/s as the test bandwidth.
    \item Single AWS Region (SAR): communication between inter-center parties but within the same geo-region (within US-WEST). 592 MB/s as the test bandwidth.
        \item Multiple AWS Region (MAR): communication between inter-center parties but across the different geo-region (between US-WEST and EU-NORTH). 15.6 MB/s as the test bandwidth.
\end{itemize}


As shown in Figure~\ref{fig:comm_bar}, we deploy FedML-HE on $3$ different geo-distributed environments, which are operated under different bandwidths. It is obvious that the secure HE functionality has an enormous impact on low-bandwidth environments while medium-to-high-bandwidth environments suffer limited impact from increased communication overhead during training cycles, compared to Non-HE settings.

\subsection{Different Encryption Selections}
Table~\ref{tab:selection} shows the overhead reductions with different selective encryption rates.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        Selection & \makecell{Comp\\(s)} & Comm & \makecell{Comp\\Ratio} & \makecell{Comm\\Ratio} \\ \hline\hline
        
        % Enc w/ L1-4 & 42.546 & 1.97 GB & 2.40 & 6.13 \\ \hline
        % Enc w/ L9-12 & 40.390 & 1.97 GB & 2.28 & 6.13 \\ \hline
        % \makecell{Enc w/ L1-4\\\&L9-12} & 76.739 & 3.62 GB & 4.33 & 11.25 \\ \hline
        % Enc w/ L1-12 & 105.549 & 5.27 GB & 5.95 & 16.38 \\ \hline
        Enc w/ 0\% & 17.739 & 329.62 MB & 1.00 & 1.00 \\ \hline
        Enc w/ 10\% & 30.874 & 844.49 MB & 1.74 & 2.56 \\ \hline
        Enc w/ 30\%  & 50.284 & 1.83 GB & 2.83 & 5.69 \\ \hline
        Enc w/ 50\% & 70.167 & 2.83 GB & 3.96 & 8.81 \\ \hline
        Enc w/ 70\%& 88.904 & 3.84 GB & 5.01 & 11.93 \\ \hline
        Enc w/ All & 112.504 & 5.35 GB & 6.34 & 16.62 \\ \hline
    \end{tabular}
    \caption{Overheads With Different Parameter Selection Configs Tested on Vision Transformer: ``Enc w/ 10\%'' means performs encrypted computation only on 10\% of the parameters; all computation and communication results include overheads from plaintext aggregation for the rest of the parameters.}
    \label{tab:selection}
\end{table}



\subsection{Comparison with Other FL-HE Frameworks}
\label{sec:diff_frameworks}

\begin{table*}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline\hline
        Frameworks &HE Core& \makecell{Key\\Management}& Comp (s) & \makecell{Comm \\(MB)} & \makecell{HE\\Multi-Party\\ Functionalities}\\ \hline
        \hline
        Ours &PALISADE & \makecell{\cmark}  & 2.456 & 105.72  & \makecell{PRE,\\ ThHE}\\
                \hline
        Ours (w/ Opt) &PALISADE & \makecell{\cmark} & 0.874 & 16.37  & \makecell{PRE,\\ ThHE}\\
                \hline    
        Ours & \makecell{SEAL\\(TenSEAL)} & \makecell{\cmark}  & 3.989 & 129.75  & ---\\\hline
        \makecell{Nvidia FLARE\\(9a1b226)} &\makecell{SEAL\\(TenSEAL)}& \makecell{\cmark} & 2.826 & 129.75  &---\\ \hline
        \makecell{IBMFL\\(8c8ab11)}&\makecell{SEAL\\(HELayers)} & $\bigcirc$ & 3.955 & 86.58  & ---\\ \hline
        
        Plaintext&--- & --- & 0.058 & 6.35  & ---\\ \hline
    \end{tabular}
    \caption{Different Frameworks: tested with CNN (2 Conv
+ 2 FC) and on 3 clients; Github commit IDs are specified. For key management, our work uses a key authority server; FLARE uses a security content manager; IBMFL currently provides a local simulator.}
    \label{tab:frameworks}
\end{table*}

We compare our framework to the other open-sourced FL frameworks with HE capability, namely NVIDIA FLARE (NVIDIA) and IBMFL. 

Both NVIDIA and IBMFL utilize Microsoft SEAL as the underlying HE core, with NVIDIA using OpenMinded's python tensor wrapper over SEAL and TenSEAL; IBMFL using IBM'spython wrapper over SEAL and HELayers (HELayers also has an HElib version). Our HE core module can be replaced with different available HE cores, to give a more comprehensive comparison, we also implement a TenSEAL version of our framework for evaluation.

Table~\ref{tab:frameworks} demonstrates the performance summary of different FedML-HE frameworks using an example of a CNN model with $3$ clients. Our PALISADE-powered framework has the smallest computational overhead due to the performance of the PALISADE library. In terms of communication cost, FedML-HE (PALISADE) comes second after IBMFL's smallest file serialization results due to the efficient packing of HELayers' Tile tensors~\cite{aharoni2011helayers}. 

Note that NVIDIA's TenSEAL-based realization is faster than the TenSEAL variant of our system. This is because NVIDIA scales each learner's local model parameters locally rather than weighing ciphertexts on the server. This approach reduces the need for the one multiplication operation usually performed during secure aggregation (recall that HE multiplications are expensive). However, such a setup would not suit the scenario where the central server does not want to reveal its weighing mechanism per each individual local model to learners as it reveals partial (even full in some cases) information about participants in the system.


