\section{Related Work}


%\subsection{Previous Work}
\noindent\textbf{Existing Privacy Attacks On FL.}
Threats and attacks on privacy in the domain of Federated Learning have been studied in recent years~\cite{mothukuri2021survey}. General FL privacy attacks can be categorized into two types: inference attacks~\cite{nasr2019comprehensive, wang2019beyond, truex2019demystifying} and data leakage/reconstruction~\cite{criswell2014kcofi,bhowmick2018protection, hitaj2017deep}. Attacks are usually carried out on the models to retrieve certain properties of data providers or even reconstruct the data in the training datasets. With direct access to more fine-grained local models trained on a smaller dataset~\cite{wang2019beyond}, the adversary can have a higher chance of a successful attack.  Moreover, further attacks can be performed using GAN-based attacks to even fully recover the original data~\cite{hitaj2017deep}. The majority of the privacy attacks can be traced back to the direct exposure of plaintext accesses to local models to other parties (usually the server).

\noindent\textbf{Existing Non-HE Defense Mechanism.}
Local differential privacy has been adopted to protect local model updates by adding differential noise on the client side before the server-side aggregation ~\cite{truex2019hybrid,byrd2020differentially} where privacy guarantee requires large-scale statistical noise on fine-grained local updates that generally degrades model performance by a large margin. On the other hand, other work proposes to apply zero-sum masks (usually pair-wise) to mask local model updates such that any individual local update is indistinguishable to the server~\cite{bonawitz2017practical, so2022lightsecagg}. However, such a strategy introduces several challenges including key/mask synchronization requirements and federated learner dropouts. Compared to these solutions providing privacy protection in FL, HE is non-interactive and dropout-resilient (vs. general secure aggregation protocols~\cite{bonawitz2017practical, so2022lightsecagg}) and it introduces negligible model performance degradation (vs. noise-based differential privacy solutions~\cite{truex2019hybrid,byrd2020differentially}).

\noindent\textbf{Existing HE-based FL Work.}
Existing HE-based FL work either apply restricted HE schemes (e.g. additive scheme Paillier)~\cite{zhang2020batchcrypt,fang2021privacy, jiang2021flashe} without extensibility to further FL aggregation functions as well as sufficient performance and security guarantee (due to Paillier) or provide a generic HE implementation on FL aggregation~\cite{roth2022nvidia,ibmfl, jiang2021flashe, du2023efficient, ma2022privacy}. However, previous work still leaves the HE overhead increase issue as an open question. In our work, we propose a universal optimization scheme to largely reduce the overhead while providing promised privacy guarantees in a both systematic and algorithmic fashion, which makes HE-based FL viable in practical deployments. 

% \noindent\textbf{Parameter Sensitivity.}

% \carlee{we should also talk about methods to compute parameter sensitivity and emphasize that nobody has applied this to privacy optimization}