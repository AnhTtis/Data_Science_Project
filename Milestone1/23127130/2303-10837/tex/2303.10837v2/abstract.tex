\begin{abstract} %\carlee{the library should have a name.}
%\weizhao{make sure all figures are referenced, including ones in appendix; appendix has to be seperate}
Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, \textit{the first practical federated learning system with efficient HE-based secure model aggregation}. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., $\sim$10x reduction for ResNet-50, and up to $\sim$40x reduction for BERT), demonstrating the potential for scalable HE-based FL deployment.

% ########### old version
%\weizhao{is efficient a better word?}
% Federated learning (FL) trains machine learning models across distributed edge devices by aggregating the local model updates instead of the local data. However, recent research indicates that the privacy guarantee from the simple federated aggregation algorithms can be easily broken when the aggregation server has direct access to the local model updates that can be used for recovering sensitive personal information in the local data. Several privacy-preserving solutions have been proposed including Homomorphic-Encryption-based approaches. However, despite Homomorphic Encryption (HE) being a post-quantum generic secure computation scheme, applications based on such a cryptographic foundation suffer from impractical overheads. 
% In this paper, we propose FedML-HE, \emph{the first practical system for efficient HE-based secure federated aggregation}, which adopts a novel universal overhead optimization scheme designed to largely reduce both computational and communicational overheads during deployment while providing a dynamic desired privacy guarantee. With optimization, our system can achieve sizeable overhead reduction, especially on large models (e.g., $\sim$10x reduction for HE-federated training ResNet-50 and $\sim$40x reduction for BERT respectively), which shows the feasibility of further scalable HE-based FL deployments. %\carlee{in general, this abstract does not make the work seem related to mobile systems. Add at least one sentence about running FL on mobile/edge devices}\yuhang{added "cross decentralized edge devices"}
\end{abstract}