\section{Privacy By Selective Parameter Encryption}
In this section, we first provide proof to analyze the privacy of fully encrypted federated learning and then analyze the privacy guarantee of Selective Parameter Encryption.
% \carlee{These section/subsection titles need to be changed to say what is being proved, not just that a proof was provided}
\subsection{Proof of Base Protocol}
In this subsection, we prove the privacy of base protocol where homomorphic-encryption-based federated learning utilizes the full model parameter encryption (i.e., the selective parameter encryption rate is set to be \textit{1}). We define the adversary in Definition~\ref{def:adv} and privacy in Definition~\ref{def:privacy}. 

\begin{definition}[Single-Key Adversary]
\label{def:adv}
\textit{A semi-honest adversary $\mathcal{A}$ can corrupt (at the same time) any subset of $n$ learners and the aggregation server, but not at the same time.}
\end{definition}

Note that the ref of the proof assumes the single-key setup and the privacy of the threshold variant of HE-FL (as shown in Definition~\ref{def:adv-threshold}) can be easily proved by extending the proofs of threshold homomorphic encryption~\cite{boneh2006chosen, laud2008threshold, asharov2012multiparty}.

\begin{definition}[Threshold Adversary]
\label{def:adv-threshold}
\textit{ A semi-honest adversary $\mathcal{A_Th}$ can corrupt (at the same time) any subset of $n-k$ learners and the aggregation server.}
\end{definition}

\begin{definition}[Privacy]
    \label{def:privacy}
	\textit{A homomorphic-encryption federated learning protocol $\pi$ is simulation secure in the presence of a semi-honest adversary $\mathcal{A}$, there exists a simulator $\mathcal{S}$  in the ideal world that also corrupts the same set of parties and produces an output identically distributed to $\mathcal{A}$'s output in the real world.}
\end{definition}

\noindent\textbf{Ideal World.} Our ideal world functionality $\mathcal{F}$  interacts with
learners and the aggregation server as follows:

\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item Each learner sends a registration message to $\mathcal{F}$ for a federated training model task $\mathbf{W}_\text{glob}$. $\mathcal{F}$  determines a subset $N' \subset N$ of learners whose data can be used to compute the global model $\mathbf{W}_\text{glob}$. 
    
    \item Both honest and corrupted learners upload their local models to $\mathcal{F}$.
    
    %The corrupted learners
    %     controlled by the adversary $\mathcal{A}$  may abort (by not sending anything),
    %     send their actual input, or send any arbitrary value that may depend
    %     upon their input, other malicious publishers' input, or auxiliary data.
    %     If a publisher value is not received before a time out period, $\mathcal{F}$  uses a
    %     nullifying value for its input, e.g., $0$ for addition and $1$ for
    %     multiplication. 
    %     %We call $i$th publisher's input $x_i$; $0 \leq i < m$, $m$ being the
    %     %total number of publishers. 

        
    \item If local models $\vec{\mathbf{W}}$ of learners in $N'$ are enough to compute
        $\mathbf{W}_\text{glob}$, $\mathcal{F}$  sends $\mathbf{W}_\text{glob} \gets \sum_{i=1}^{N'} \alpha_i \mathbf{W}_i$ to all learners in $N'$, otherwise $\mathcal{F}$  sends empty message $\bot$.
\end{itemize}

\noindent\textbf{Real World.} In real world, $\mathcal{F}$  is replaced by our
protocol described in Algorithm~\ref{alg:fedml-fhe} with full model parameter encryption.


We describe a simulator $\mathcal{S}$  that simulates the view of the $\mathcal{A}$  in the real-world execution of our protocol. Our privacy definition~\ref{def:privacy} and the simulator $\mathcal{S}$ prove both confidentiality and correctness. We omit the simulation of the view of $\mathcal{A}$ that corrupts the aggregation server here since the learners will not receive the ciphertexts of other learners' local models in the execution of $\pi$ thus such a simulation is immediate and trivial.

\input{simulator}


\subsection{Proof of Encrypted Learning by DP Theory}

\begin{definition}[Adjacent Datasets]
\label{def:adj_datasets}
Two datasets $D_1$ and $D_2$ are said to be adjacent if they differ in the data of exactly one individual. Formally, they are adjacent if:
$$
\left|D_1 \Delta D_2\right|=1
$$
\end{definition} 

\begin{definition}[$\epsilon$-Differential Privacy]
\label{def:dp}
A randomized algorithm $\mathcal{M}$ satisfies $\epsilon$-differential privacy if for any two adjacent datasets $D_1$ and $D_2$, and for any possible output $O \subseteq \operatorname{Range}(\mathcal{F})$, the following inequality holds:
$$
\frac{\operatorname{Pr}\left[\mathcal{M}\left(D_1\right) \in O\right]}{\operatorname{Pr}\left[\mathcal{M}\left(D_2\right) \in O\right]} \leq e^\epsilon
$$
Smaller values of the privacy parameter $\epsilon$ imply stronger privacy guarantees.
\end{definition} 

\begin{definition}[Laplace mechanism]
\label{def:laplace}
Given a function $f: \mathcal{D} \rightarrow \mathbb{R}$, 

where $\mathcal{D}$ is the domain of the dataset and $d$ is the dimension of the output, the Laplace mechanism adds Laplace noise to the output of $f$.

Let $b$ be the scale parameter of the Laplace distribution, which is given by:
$$
\operatorname{Lap}(x \mid b)=\frac{1}{2 b} e^{-\frac{|x|}{b}}
$$

Given a dataset $D$, the Laplace mechanism $\mathcal{F}$ is defined as:
$$
\mathcal{M}(D)=f(D)+\operatorname{Lap}(0 \mid b)^d
$$
\end{definition} 

\begin{definition}[Sensitivity]
\label{def:sensitivity}
To ensure $\epsilon$-differential privacy, we need to determine the appropriate scale parameter $b$. This is where the sensitivity of the function $f$ comes into play. The sensitivity $\Delta f$ of a function $f$ is the maximum difference in the output of $f$ when applied to any two adjacent datasets:
$$
\Delta f=\max _{D_1, D_2:\left|D_1 \Delta D_2\right|=1}\left\|f\left(D_1\right)-f\left(D_2\right)\right\|_1
$$
\end{definition} 
Based on Definition~\ref{def:adj_datasets}, \ref{def:dp}, \ref{def:laplace} and \ref{def:sensitivity} we have

\begin{lemma}[Achieving $\epsilon$-Differential Privacy by Laplace Mechanism~\cite{dwork2008differential, abadi2016deep}]
\label{lemma:dp_laplace}
To achieve $\epsilon$-differential privacy, we choose the scale parameter $b$ as:
$$
b=\frac{\Delta f}{\epsilon}
$$

With this choice of $b$, the Laplace mechanism $\mathcal{F}$ satisfies $\epsilon$-differential privacy.

\end{lemma}

By adding noise $\operatorname{Lap}(0 \mid b)^d$ on one parameter in the model gradient where $b=\frac{\Delta f}{\epsilon}$, we can achieve $\epsilon$-differential privacy. We then show homomorphic encryption provides a much stronger differential privacy guarantee.


\begin{theorem}[Achieving $0$-Differential Privacy by Homomorphic Encryption]
\label{theorem:he_dp}
For any two adjacent datasets $D_1$ and $D_2$, since $\mathcal{M}(D)$ is computationally indistinguishable, we have
$$
\frac{\operatorname{Pr}\left[\mathcal{M}\left(D_1\right) \in O\right]}{\operatorname{Pr}\left[\mathcal{M}\left(D_2\right) \in O\right]} \leq e^{\epsilon}.
$$
We then have $\epsilon = 0$ if $O$ is encrypted. 
\end{theorem}

In other words, $\mathcal{A}$ cannot retrieve sensitive information from encrypted parameters.

\subsection{Proof of Selective Parameter Selection}

\begin{lemma}[Sequential Composition~\cite{dwork2008differential},]
\label{lemma:sequential}
If $\mathcal{M}_1(x)$ satisfies $\epsilon_1$-differential privacy and $\mathcal{M}_2(x)$ satisfies $\epsilon_2$-differential privacy, then the mechanism $\mathcal{G}(x)=\left(\mathcal{M}_1(x), \mathcal{M}_2(x)\right)$ which releases both results satisfies $(\epsilon_1+\epsilon_{2})$-differential privacy
\end{lemma}

%Based on Lemma~\ref{lemma:dp_laplace} and~\ref{lemma:sequential}, we can show the sequential privacy by adding the Laplace noise on all model parameters

Based on Lemma~\ref{lemma:dp_laplace},~\ref{lemma:sequential} and Thoerem~\ref{theorem:he_dp}, we can now analyze the privacy of Selective Parameter Encryption

\begin{theorem}[Achieving $\sum_{i\in {[N] / \mathcal{S}}} \frac{\Delta f_i}{b}$-Differential Privacy by Partial Encryption]
If we apply Homomorphic Encryption on partial model parameters $\mathcal{S}$ and Laplace Mechanism on remaining model parameters $[N] / \mathcal{S}$ with fixed noise scale $b$. For each parameter $i\in [N] / \mathcal{S}$, we have $\epsilon_i = \frac{\Delta f_i}{b}$. Such partial encryption satisfies $\sum_{i\in {[N] / \mathcal{S}}} \frac{\Delta f_i}{b}$-differential privacy.

\end{theorem}
Let $J = \sum_{i=1}^N \frac{\Delta f_i}{b}$ and assume $\Delta f \sim \mathcal{U}(0,1)$ where $\mathcal{U}$ represents the uniform distribution, we can then show the privacy cost of adding Laplace noise on all parameters, random parameter encryption, and selective parameter encryption.
\begin{remark}[Achieving $J$-Differential Privacy by Laplace Mechanism on All Model Parameters]
If we add Laplace noise on all parameters with fixed noise scale $b$, it satisfies $J$-differential privacy.
\end{remark}

\begin{remark}[Achieving $(1-p) J$-Differential Privacy by Random Selection]
If we randomly select model parameters with probability $p$ and homomorphically encrypt the remaining parameters, it satisfies $(1-p) J$-differential privacy.

\end{remark}

\begin{remark}[Achieving $(1-p)^2 J$-Differential Privacy by Sensitive Parameter Selection]
If we select the most sensitive parameters with ratio $p$ and homomorphically encrypt the remaining parameters, it satisfies $(1-p)^2 J$-differential privacy.
\end{remark}

\textbf{Key Observation}: Selective Parameter Encryption requires \textit{$(1-p)^2$ times less privacy budget} than random selection and complete differential privacy with the same privacy preservation.
% start new things