\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\CE}{CE}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{9197} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{SpiderMesh: Spatial-aware Demand-guided Recursive Meshing \\for RGB-T Semantic Segmentation}

\author{Siqi Fan\textsuperscript{1}, Zhe Wang\textsuperscript{1}, Yan Wang\textsuperscript{1*}, Jingjing Liu\textsuperscript{1*}\\
\textsuperscript{1}Institute for AI Industry Research (AIR), Tsinghua University\\
% Institution1 address\\
{\tt\small \{fansiqi, wangzhe, wangyan, JJLiu\}@air.tsinghua.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{To whom the correspondence should be addressed.}
\footnotetext[2]{Code will be made publicly available at \href{https://github.com/leofansq/SpiderMesh}{GitHub}.}
%%%%%%%%% ABSTRACT
\begin{abstract}
   For semantic segmentation in urban scene understanding, RGB cameras alone often fail to capture a clear holistic topology, especially in challenging lighting conditions. Thermal signal is an informative additional channel that can bring to light the contour and fine-grained texture of blurred regions in low-quality RGB image. Aiming at RGB-T (thermal) segmentation, existing methods either use simple passive channel/spatial-wise fusion for cross-modal interaction, or rely on heavy labeling of ambiguous boundaries for fine-grained supervision. We propose a Spatial-aware Demand-guided Recursive Meshing (SpiderMesh) framework that: $1)$ proactively compensates inadequate contextual semantics in optically-impaired regions via a demand-guided target masking algorithm; $2)$ refines multimodal semantic features with recursive meshing to improve pixel-level semantic analysis performance. We further introduce an asymmetric data augmentation technique M-CutOut, and enable semi-supervised learning to fully utilize RGB-T labels only sparsely available in practical use. Extensive experiments on MFNet and PST900 datasets demonstrate that SpiderMesh achieves state-of-the-art performance on standard RGB-T segmentation benchmarks.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

To realize robust pixel-wise scene understanding in real-world urban environment, RGB-based semantic segmentation is often inadequate due to low image quality from poor lighting conditions, such as nighttime scenes and over-exposure scenarios (as illustrated in Figure~\ref{fig:rgb-t}). 
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.32]{./figure/rgb-t.jpg}
  \caption{\textit{Top}: RGB images. \textit{Bottom}: Thermal images. Some RGB regions are blacked out or blurred in nighttime or over-exposure scenes, while the corresponding thermal images are robust to 
  varying illumination conditions and can provide region-level semantic compensation (images from MFNet dataset \cite{mfnet}).}
  \label{fig:rgb-t}
\end{figure}
One effective way to overcome this is to dynamically adjust the exposure time of cameras. However, this introduces new challenges such as motion blur under long exposure time. To amend this, RGB-T brings in thermal sensors that are relatively robust to variation in illumination conditions \cite{mfnet, rtfnet, fuseseg, feanet, gmnet, mffenet, ABMDRNet}. Unlike visual cameras that react to visible light spectrum, thermal sensors capture infrared radiations emitted by objects \cite{ITI}, which can bring to light the nuanced texture information about environmental surroundings even in challenging lighting conditions. 

RGB-D (depth) \cite{fusenet, Wang_2018_ECCV, Cheng_2017_CVPR, Jiao_2019_CVPR, Xiong_2020_CVPR} on the other hand, leverages depth map to either enhance the whole RGB signal with depth value (channel-wise enhancement) or highlight RGB features of foreground regions based on depth (guided self-enhancement). However, RGB-D still falls short when additional semantic context for targeted areas is needed for strengthening poorly captured RGB regions. 

Existing methods on RGB-T segmentation mainly focus on two aspects: cross-modal feature interaction between RGB and thermal images, and pixel-wise semantic analysis. To exploit cross-modality information from RGB-T pairs, early approaches adopt simple operations such as summation and concatenation \cite{mfnet, rtfnet, fuseseg, mffenet}. Recent attention-based fusion methods \cite{feanet, gmnet, ABMDRNet} integrate RGB and thermal features by taking into consideration their relative context. However, channel-wise fusion directly overlaps RGB and thermal features, blind to their relative spatial positions (Figure~\ref{fig:spatial-channel} (a)). The `\textit{passive}' spatial-wise integration strategy directly provides features without asking for real needs (Figure~\ref{fig:spatial-channel} (b)). To achieve fine-grained fusion for semantic analysis, multi-supervision has been applied to semantically ambiguous regions~\cite{gmnet, mffenet}, but boundary supervision highly relies on accurate pixel-level labeling, which induces high-cost in practical applications. To better leverage the intrinsic contextual relativity between paired thermal signals and RGB features (e.g., to specifically and automatically target darkened areas in RGB images), a more target-guided proactive integration strategy is needed (Figure~\ref{fig:spatial-channel} (c)). 

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.45]{./figure/spatial-channel.jpg}
  \caption{Compared with channel-wise and passive spatial-wise fusion,  proactive spatial-wise fusion can fully exploit extra semantic feature for targeted region.}
  \label{fig:spatial-channel}
\end{figure}

In order to achieve this goal, we need to answer the following questions: $1)$ How to proactively use thermal signals to compensate the inadequate contextual semantics in optically-impaired regions? 
$2$) How to utilize a small set of RGB-T pairs with limited annotations to reach preferable segmentation performance for practical use?

To address the first challenge, we design a demand-guided target masking algorithm to enhance the features of poorly captured regions in RGB images in a `\textit{request}' manner, via proactive region-level masking with the learned compensation needs. To alleviate information loss incurred in encoding and maximize feature utility, we propose a spatial-aware recursive meshing method, which enhances cross-modal RGB-T features iteratively for pixel-wise semantic analysis. To fully exploit limited labeled pairwise data, we further propose an asymmetric data augmentation technique, named \textit{mono-modal CutOut} (M-CutOut), which creates artificial optically-impaired regions and encourages the network to learn more compensated features from thermal signals. An architecture design with semi-supervised learning capability is also introduced to utilize both natural and artificial regional complementarity in RGB-T. Extensive experiments on the popular MFNet and PST900 benchmarks demonstrate that our proposed framework, \textit{Spatial-aware Demand-guided Recursive Meshing} (SpiderMesh), achieves state-of-the-art performance on RGB-T semantic segmentation, outperforming existing methods.

Our contributions are summarized as follows:
\vspace{-7pt}
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parsep}{0pt}
  \setlength{\parskip}{0pt}
  \item We propose a demand-guided target masking algorithm to directly meet the real needs of RGB-T feature compensation in a proactive `request’ manner.
  \item We propose a spatial-aware recursive meshing method to iteratively refine multimodal semantic features.
  \item Our SpiderMesh framework not only achieves state-of-the-art performance, but also effectively addresses practical concerns such as computational complexity, robustness to signal loss, and manual labeling cost. 
\end{itemize}


\section{Related Work}

In this section, we briefly review two related topics, single-modal RGB semantic segmentation and multimodal RGB-T segmentation.

\subsection{RGB Semantic Segmentation}
RGB semantic segmentation is a pixel-level scene understanding task. Different from relying on hand-crafted features in traditional approaches, FCN \cite{FCN} was proposed to perform end-to-end segmentation. SegNet \cite{segnet} introduced an encoder-decoder network architecture and initiated a new trend. To preserve detailed contextual information, skip connections are introduced between encoder and decoder in UNet \cite{unet}. Deeplabv3 \cite{deeplabv3} proposed atrous spatial pyramid pooling (ASPP) to apply parallel atrous convolutions with different dilation rates. To tackle information loss during encoding, BiSeNet \cite{bisenet} was proposed with two paths (spatial and context) to preserve the spatial information and broaden the receptive field at the same time. The transformer-based methods, such as SegFormer\cite{segformer}, further boosted the performance. Although RGB semantic segmentation has achieved promising progress in recent years, most methods are still susceptible to challenging lighting conditions with poor image quality.

\begin{figure*}
  \centering
  \includegraphics[scale=0.47]{./figure/SpiderMesh.jpg}
  \caption{\textit{Top}: Overall architecture of SpiderMesh; \textit{Bottom}: RGB-T feature flow in SpiderMesh. $L_i$ denotes different layer of the backbone.}
  \label{fig:spidermesh}
\end{figure*}

\subsection{RGB-T Segmentation}
Thermal images can provide complementary information for those less informative regions in RGB images. Most of RGB-T fusion employed an explicit aggregation operation. MFNet \cite{mfnet} collected a RGB-T semantic segmentation dataset and proved that segmentation performance can be significantly improved by utilizing thermal images. Two identical encoders were employed in RTFNet \cite{rtfnet} and FuseSeg \cite{fuseseg}, and the thermal features were gradually integrated into RGB features. FEANet \cite{feanet} refined the detail features using attention mechanism to deal with small objects. MFFENet \cite{mffenet} used spatial attention to emphasize foreground objects. The multimodal features are fused coarsely with simple operations (e.g., summation, concatenation) in these approaches. GMNet \cite{gmnet} proposed different fusion strategies for shallow and deep features to integrate multi-level features. All these approaches treat RGB-T with either channel-wise fusion or passive feature enhancement, which does not fully exploit the regional complementary semantic information. Some recent works explored to utilize the power of transformer. MFTNet \cite{MFTNet} used modified transformer to learn intraspectral correlations and interspectral interaction, but introduced additional computational complexity. To further boost the performance, alignment-based fusion is utilized via domain adaptation techniques \cite{ABMDRNet, msuda}. Different from existing methods, we propose a systematic demand-guided approach addressing not only performance but also practical concerns. Focusing on less informative regions in RGB images, we enhance those targeted regions with thermal features via proactive spatial-wise interaction. Instead of applying multi-supervision for fine-grained training \cite{gmnet, mffenet}, we only utilize semantic supervision considering labeling cost, and enable the extension of our framework to semi-supervised semantic segmentation.

\section{SpiderMesh Framework }

In this section, we describe the proposed dual-branch framework for RGB-T semantic segmentation, which consists of demand-guided target masking, spatial-aware recursive meshing, a novel M-CutOut technique for data augmentation, and a mutual learning strategy for semi-supervised adaptation.

\subsection{Overall Architecture}

As illustrated in Figure~\ref{fig:spidermesh}, RGB-T semantic segmentation is technically resolved into cross-modal feature interaction and pixel-level feature refinement in SpiderMesh framework. RGB-T pairs are fed into corresponding branches for each modality. Each branch adopts an encoder-decoder structure and employs ResNet \cite{resnet} as backbone for feature extraction. The number of input channels in the first convolutional layer of the thermal branch is set to $1$. Five encoder layers are utilized consecutively to extract features. A DTM (demand-guided target masking) module is embedded after each layer. Data scale is gradually decreased from $H \times W$ to $\frac{H}{32} \times \frac{W}{32}$. Next, a SRM (spatial-aware recursive meshing) module is used as the decoder to enhance unsampled features with fine-grained multimodal semantic features. We utilize bi-linear interpolation for upsampling. Although the two branches are treated equally during encoding and decoding, we regard the RGB branch as the main branch for generating final predictions. Thus, the enhanced thermal feature $f_{the}^{e}$ is introduced to the RGB branch and added with enhanced RGB feature $f_{rgb}^{e}$, which is further fed to a classifier. Meanwhile, $f_{the}^{e}$ is also fed to a classifier to output an auxiliary prediction:

\begin{equation}
  y_{rgb} = Conv(f_{rgb}^{e} + f_{the}^{e})
\end{equation}
\begin{equation}
  y_{the} = Conv(f_{the}^{e})
\end{equation}
where a convolutional layer is used as the classifier.

\subsection{Demand-guided Target Masking}
To better leverage the regional complementary texture feature across RGB and thermal signals, we propose a DTM module, a proactive spatial-wise fusion component whose architecture is illustrated in Figure~\ref{fig:dtm}. The overall feature interaction is guided by the demand map dynamically learned via spatial-wise attention. However, in real-world scenes, both RGB and thermal input can be affected by complex environment (e.g., over-exposure lighting, temperature rise caused by abnormal heat), resulting in noise in some channels, which may lead to noisy demand map. To tackle this challenge, channel-wise attention is employed as the pre-processing of regional feature compensation for selecting more representative channels. Channel-wise and spatial-wise attention, as basic attention-based operations, have various implementation options, and we adopt a statistical approach \cite{cbam} for our task-specific purpose. The spatial-wise attention is utilized to decide which region needs more feature complementation, while the channel-wise attention is used for feature-level denoising.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.4]{./figure/dtm.png}
  \caption{Architecture of DTM module. The less informative regions are complemented via demand-guided target masking in a `request' manner.}
  \label{fig:dtm}
\end{figure}

Take the RGB modality as an example, demand-guided target masking includes the following steps:

\textbf{Channel-wise Denoising} To deal with the inevitable camera noise, $f_{rgb} \in \mathbb{R}^{H \times W \times C}$ is first denoised. The channel-wise statistics are computed with max-pooling and mean-pooling and further forwarded to shared MLPs to produce channel-wise weights. Then, the two weights are fed into a summation operation and a sigmoid layer to generate the channel-wise attention map $m_{rgb}^c \in \mathbb{R}^{1 \times 1 \times C}$, which is used to weigh $f_{rgb}$ by element-wise multiplication, resulting in $f_{rgb}^c \in \mathbb{R}^{H \times W \times C}$.

\begin{equation}
  f_{rgb}^c = m_{rgb}^c \cdot f_{rgb} = \mathit{CA}(f_{rgb}) \cdot f_{rgb}
\end{equation}
where $\mathit{CA}(\cdot) = \mathit{sigmoid}(\mathit{MLP}(AP(\cdot))+\mathit{MLP}(MP(\cdot)))$ is channel-wise attention operator. `$AP(\cdot)$' and `$MP(\cdot)$' denote mean-pooling and max-pooling operators.

\textbf{Attentive Demand Map} To generate spatial-wise demand for thermal signal complementation, the two statistical operations (max-pooling and mean-pooling) are utilized along the channel axis for spatial-wise statistics. The pooled features are concatenated and forwarded to a convolution operation with a filter in the size of $7 \times 7$ for further region-level statistics. After a sigmoid operation, the attentive demand map $m_{rgb}^s \in \mathbb{R}^{H \times W \times 1}$ is obtained, representing the demand of spatial-wise complementation for $f_{rgb}^c$:

\begin{equation}
  m_{rgb}^s = \mathit{SA}(f_{rgb}^c)
\end{equation}
where $\mathit{SA}(\cdot) = \mathit{sigmoid}(\mathit{Conv}(AP(\cdot))+\mathit{Conv}(MP(\cdot)))$ is spatial-wise attention operator.

\textbf{Demand-guided Fusion} The thermal feature $f_{the}^c$ is spatial-wise weighted according to the adaptive demand represented by the value of $m_{rgb}^s$. 
Then, $f_{rgb}^c$ is integrated with $f_{the}^c$ attentively in a `request' manner:

\begin{equation}
  f_{rgb}^{'} = f_{rgb}^c + m_{rgb}^s \cdot f_{the}^c
\end{equation}

For the thermal modality, $f_{the}^{'}$ can be obtained likewise. In addition, input features $f_{rgb}$ and $f_{the}$ are also fused using summation operation to generate multimodal feature $f_{m}$ for detailed semantic feature refinement in later stage.

\subsection{Spatial-aware Recursive Meshing}
Semantic segmentation is a pixel-level scene understanding task, which relies on fine-grained semantic features for pixel-wise classification. However, detailed information loss caused by downsampling is inevitable during encoding. To compensate information loss and refine the fine-grained features, we propose a SRM module that leverages the fused multimodal features in a recursive manner with spatial awareness. SRM module is composed of a modified ASPP block \cite{deeplabv3} and three spatial-aware feature refinement blocks, as shown in Figure~\ref{fig:srm}.

\begin{figure}[htbp]
  \centering 
  \includegraphics[scale=0.4]{./figure/srm.png}
  \caption{Architecture of SRM module. The detailed information loss caused by downsampling is compensated via spatial-aware recursive meshing with fused multimodal features. \textit{`modal'} is replaced with `rgb' or `the' according to which branch it applied in.}
  \label{fig:srm}
\end{figure}

For RGB modality, the encoded feature map $f_{rgb_4}^{'}$ is first embedded with more global features via atrous spatial pyramid pooling. We use three dilation rates ($d=2,4,8$), and the number of channels is reduced to 256 after this block. To refine the fine-grained semantic features, the features of different receptive fields are recursively introduced via skip-connection. Instead of simple concatenation of features, we perform spatial-aware feature refinement to proactively mesh upsampled features with multimodal features $f_{m_i}$ which contain rich detailed semantic information from both RGB-T signals. Considering the complexity, channel reduction is applied to $f_{m_i}$ via convolutional operation. For features with a scale index $i$, the feature refinement step can be formulated as:

\begin{equation}
  \begin{aligned}
    f_{u_{i-1}} &= E(f_{u_i}, f_{m_{i-1}}) \\
                &= Up(f_{u_i}) \oplus (\mathit{Conv}(f_{m_{i-1}}) \cdot m_{u_{i-1}}^s) \\
                &= Up(f_{u_i}) \oplus (\mathit{Conv}(f_{m_{i-1}}) \cdot \mathit{SA}(Up(f_{u_i})))
  \end{aligned}
\end{equation}
where `$\oplus$' is the concatenation operator, `$\mathit{SA}(\cdot)$' is the spatial-wise attention operator, and `$Up(\cdot)$' and `$Conv(\cdot)$' denote the upsampling and convolution operators, respectively. $m_{u_i}^s$ is the spatial-aware attentive mask generated via spatial-wise attention operation, which indicates where and how much the feature needs to be compensated. 

The input RGB feature is compensated recursively, which can be represented by:
\begin{equation}
  f_{rgb}^{e} = Up(E(E(E(ASPP(f_{rgb_4}^{'}), f_{m_3}), f_{m_2}), f_{m_1}))
\end{equation}
where `$ASPP(\cdot)$' is the atrous spatial pyramid pooling. Similarly, the encoded thermal feature is compensated recursively to generate $f_{the}^{e}$.


\subsection{M-CutOut Augmentation}
Different from the common approach that applies CutOut \cite{cutout} to all modalities correspondingly considering the consistency, in our framework, we break the stereotype and propose a task-specific asymmetric augmentation technique to artificially introduce new optically-impaired regions, which only applies random CutOut to RGB images. An example is shown in Figure~\ref{fig:m-cutout}. 

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.38]{./figure/m-cutout.jpg}
  \caption{An example of M-CutOut, designed for data augmentation specific to regional complementarity, like RGB-T.}
  \label{fig:m-cutout}
\end{figure}

A rectangle region of a fixed ratio to the input is randomly positioned in a mask $M$ initialized with value $1$, and the pixels inside the region are set to $0$. Then, we mask the input pixels of RGB with $M$. M-CutOut encourages the model to learn compensated features by introducing new complementary regions.

\subsection{Semi-supervised Learning}

Benefiting from dual-branch architecture and M-CutOut, SpiderMesh can be easily extended to semi-supervised segmentation task by leveraging both natural and artificial regional complementarity in RGB-T (Figure~\ref{fig:semi}). 

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.37]{./figure/semi.png}
  \caption{Framework for semi-supervised semantic segmentation.}
  \label{fig:semi}
\end{figure}

Both labeled and unlabeled data are provided in semi-supervised learning tasks. Without loss of generality, let $G$ denote the label for labeled data pairs $(X_{rgb}^{l}, X_{the}^{l})$ and $(X_{rgb}^{u}, X_{the}^{u})$ denote unlabeled data. 
When using labeled data, the network is trained in a supervised manner:

\begin{equation}
  \mathcal{L_S} = \CE(G, h_{rgb}(X_{rgb}^{l})) + \CE(G, h_{the}(X_{the}^{l}))
\end{equation}
where `$h_{rgb}(\cdot)$' and `$h_{the}(\cdot)$' denote the two branches in SpiderMesh, and `$\CE(\cdot)$' is the cross-entropy loss function.

Inspired by the applications of pseudo supervision in RGB segmentation task \cite{Chen_2021_CVPR, cpcl}, the network is trained via cross-modal mutual learning with unlabeled data for multimodal alignment-based fusion. 
We first generate the pseudo label $Y_{rgb}$ and $Y_{the}$ using predictions of original input data with weak augmentations only:

\begin{equation}
  Y_{rgb} = \argmax_y h_{rgb}(y|X_{rgb}^{u})
\end{equation}
$Y_{the}$ can be obtained likewise.

Then, we apply cross-modal pseudo supervision between the generated pseudo label and the predictions of augmented data using M-CutOut. The cross-modal supervision is conducive to modality alignment: 
\begin{equation}
  \mathcal{L_U} = \CE(Y_{the}, h_{rgb}(M \cdot X_{rgb}^{u})) + \CE(Y_{rgb}, h_{the}(X_{the}^{u}))
\end{equation}

The losses for supervised and unsupervised training are combined to form the final training objective:
\begin{equation}
  \mathcal{L} = \mathcal{L_S} + \mathcal{L_U}
\end{equation}

\begin{table*}
  \footnotesize
  \centering
  % \renewcommand{\arraystretch}{1.4}
  \begin{tabular}{cc|c|cccccccc} 
  \hline
        \textbf{Category}       &\textbf{Methods}                       & \textbf{mIoU}         & \textbf{Car}          & \textbf{Person}       & \textbf{Bike}         & \textbf{Curve}        & \textbf{Car-stop}     & \textbf{Guardrail}       & \textbf{Color-cone}      & \textbf{Bump}               \\ \hline
        
        \multirow{5}*{RGB}      & SegNet\cite{segnet}       & 42.3         & 65.3         & 55.7         & 51.1         & 38.4         & 10.0         & 0.0             & 12.0            & 51.5               \\
                                & UNet\cite{unet}           & 45.1         & 66.2         & 60.5         & 46.2         & 41.6         & 17.9         & 1.8             & 30.6            & 44.2               \\
                                & PSPNet\cite{psp}          & 46.1         & 74.8         & 61.3         & 50.2         & 38.4         & 15.8         & 0.0             & 33.2            & 44.4               \\
                                & SwinT\cite{swin}          & 49.0         & 85.2         & 57.6         & 61.0         & 33.2         & 28.0         & 2.4             & 42.7            & 33.5               \\
                                & BiSeNet\cite{bisenet}     & 50.0         & 84.1         & 63.2         & 60.1         & 36.7         & 25.3         & 5.0             & 42.2            & 35.9               \\ \hline
                            
        \multirow{2}*{RGB-D}    & SA-Gate\cite{Sagate}      & 45.8         & 73.8         & 59.2         & 51.3         & 38.4         & 19.3         & 0.0             & 24.5            & 48.8                \\
                                & ACNet\cite{acnet}         & 46.3         & 79.4         & 64.7         & 52.7         & 32.9         & 28.4         & 0.8             & 16.9            & 44.4                \\ \hline
                                
        \multirow{9}*{RGB-T}    & MFNet\cite{mfnet}             & 39.7             & 65.9             & 58.9             & 42.9             & 29.9             & 9.9              & 0.0             & 25.2              & 27.7               \\
                                & RTFNet\cite{rtfnet}           & 53.2             & 87.4             & 70.3             & 62.7             & 45.3             & 29.8             & 0.0             & 29.1              & 55.7   \\
                                & FuseSeg\cite{fuseseg}         & 54.5             & \underline{87.9} & 71.7             & \textbf{64.6}    & 44.8             & 22.7             & 6.4             & 46.9              & 47.9               \\
                                & ABMDRNet\cite{ABMDRNet}       & 54.8             & 84.8             & 69.6             & 60.3             & 45.1             & \underline{33.1} & 5.1             & 47.4              & 50.0               \\
                                & FEANet\cite{feanet}           & 55.3             & 87.8             & 71.1             & 61.1             & 46.5             & 22.1             & 6.6             & 55.3              & 48.9               \\
                                & MFFENet-single\cite{mffenet}  & 55.5             & 87.1             & \textbf{74.4}    & 61.3             & 45.6             & 30.6             & 5.2             & \textbf{57.0}     & 40.5               \\
                                & MFTNet\cite{MFTNet}           & \underline{57.3} & \underline{87.9} & 66.8             & \underline{64.4} & \underline{47.1} & \textbf{36.1}    & \underline{8.4} & \underline{55.5}  & 62.2               \\
                                & DooDLENet\cite{DooDLeNet}     & \underline{57.3} & 86.7             & 72.2             & 62.5             & 46.7             & 28.0             & 5.1             & 50.7              & \textbf{65.8} \\
                                
        \cline{2-11}            & SpiderMesh (Ours)                   & \textbf{57.9}    & \textbf{88.1}    & \underline{72.8} & 63.7             & \textbf{48.4}    & 28.2             & \textbf{8.8}    & 48.2              & \underline{64.2}      \\ \hline
    


  \end{tabular}
  \caption{Quantitative evaluation on MFNet Dataset. Results of RGB and RGB-D semantic segmentation methods are obtained from \cite{rtfnet,mffenet,MFTNet,ABMDRNet}. 
           The best values are marked by bold, and the second are marked by underline. All scores are in $\%$.}
  \label{tab:sota_mf}
\end{table*}

\begin{table*}
  \footnotesize
  \centering
  % \renewcommand{\arraystretch}{1.4}
  \begin{tabular}{cc|c|ccccc} 
  \hline
    \textbf{Category}       & \textbf{Methods}                       & \textbf{mIoU}         & \textbf{Survivor}     & \textbf{Hand-drill}   & \textbf{Backpack}     & \textbf{Fire-extinguisher}  & \textbf{Background}  \\ \hline

    \multirow{1}*{RGB}      & UNet\cite{unet}           & 52.8         & 31.6         & 38.3         & 52.9         & 43.0               & 98.0       \\ \hline
    \multirow{5}*{RGB-T}    & MFNet\cite{mfnet}             & 57.0         & 20.7         & 41.1         & 64.3         & 60.4               & 98.6        \\
                            & RTFNet\cite{rtfnet}           & 57.6         & 36.4         & 25.4         & 75.3         & 52.0               & 98.9        \\
                            & PSTNet\cite{pst900}           & 68.4         & 50.0         & 53.6         & 69.2         & 70.1               & 98.9        \\
                            & MFFENet-single\cite{mffenet}  & \underline{77.1} & \underline{63.0} & \underline{66.8} & \underline{76.6}  & \textbf{79.8} & \underline{99.3} \\

    \cline{2-8}             & SpiderMesh (Ours)                & \textbf{82.3} & \textbf{71.9} & \textbf{79.7} & \textbf{84.0} & \underline{76.6} & \textbf{99.4} \\ \hline
    
    
  \end{tabular}
  \caption{Quantitative evaluation on PST900 Dataset. Results of compared baselines are obtained from \cite{mffenet}. 
           The best values are marked by bold, and the second are marked by underline. All scores are in $\%$.}
  \label{tab:sota_pst}
\end{table*}

\section{Experiments}

This section describes experiments on standard RGB-T semantic segmentation datasets. We compare our model with state-of-the-art methods and provide detailed ablation studies and qualitative analysis. Further evaluation on semi-supervised setting also validates the extendability of SpiderMesh framework.


\subsection{Experimental Setting}


\textbf{Datasets}
We evaluate the proposed SpiderMesh on public datasets of both urban scenes (MFNet \cite{mfnet}) and underground scenes (PST900 \cite{pst900}). 
MFNet Dataset \cite{mfnet} is the only public dataset on RGB-T semantic segmentation for urban traffic scenes. It contains $1,569$ 
pairs of RGB and thermal images captured simultaneously, which comprises 820 daytime and 749 nighttime paired images. The pixels are annotated with one of $9$ classes of urban traffic scenes (e.g., car, person, bike, curve, guardrail). For fair comparison, we follow the same splitting scheme as in previous work (training/validation/test is set to $50$/$25$/$25$). Batch size is $6$, and the input is resized to a fixed size of $480 \times 640$.

PST900 is a challenging underground environment dataset proposed for the DARPA Subterranean Challenge \cite{pst900}. It contains $894$ aligned RGB-T 
image pairs collected from diverse environments with varying lighting conditions. The pixels are annotated into $4$ foreground classes (survivor, hand-drill, backpack, fire-extinguisher). We adopt the same splitting scheme as in \cite{pst900} for fair 
comparison. Input data is resized to $720 \times 1280$, and batch size is $2$.

\textbf{Baselines}
We compare SpiderMesh with RGB-T segmentation baselines \cite{mfnet, rtfnet, fuseseg, ABMDRNet, feanet, mffenet, pst900, MFTNet, DooDLeNet}, RGB-D methods \cite{Sagate, acnet}, and RGB methods \cite{segnet, unet, psp, swin, bisenet}. Mean intersection over union (mIoU) is used as the evaluation metric. Compared with MFNet dataset, only few methods are evaluated on PST900, the reported results of which are taken from \cite{mffenet}. 

\textbf{Implementation Details}
We employ ResNet-152 as backbone. The encoder is initialized with the pre-trained weights provided by PyTorch. The initial learning rate is set to 
$10^{-2}$, and exponential decay scheme is adopted to gradually decrease the learning rate. We use SGD optimizer with momentum for training. The momentum 
and weight decay are set as $0.9$ and $5 \times 10^{-4}$. The network is trained until convergence (200 epochs). For training, we apply several data 
augmentation methods, including random flipping, random cropping, and the proposed M-CutOut. Experiments are implemented in PyTorch on a server with NVIDIA A30.

\subsection{Main Results}

Table~\ref{tab:sota_mf} reports the comparison results between SpiderMesh and baselines on MFNet dataset. SpiderMesh achieves the best performance on mIoU ($57.9\%$), and outperforms baselines on $3$ categories (car, curve, and guardrail), with the second best IoU on $2$ categories (person and bump). Among them, cars and pedestrians are the two most common objects in urban scenes. Most of RGB-T methods outperforms both RGB and RGB-D techniques, which demonstrates the importance of tackling RGB-T segmentation in a task-specific way to leverage the regional complementarity. The reported performance of MFFENet-single \cite{mffenet} is only under semantic supervision for fair comparison. Although the full-version GMNet can achieve 57.3 \% mIoU utilizing multi-supervision, their performance drops to 53.9\% when boundary supervision is not applied \cite{gmnet}. The segmentation of \textit{guardrail} and the distinction between \textit{road} and \textit{bump} classes are challenging, which relies on fine-grained semantic features. Benefiting from the proactive feature compensation and recursive feature refinement, SpiderMesh can leverage extra texture features of optical-impaired regions with thermal images and enhance detailed semantic features for segmentation, resulting in the first or second performance on \textit{guardrail} and \textit{bump} classes.

\begin{figure*}
  \centering
  \includegraphics[scale=0.42]{./figure/demo.jpg}
  \caption{Visualization examples on MFNet dataset. From top to bottom: RGB input images, thermal input images, RGB segmentation predictions, segmentation groundtruth,  predictions obtained by SpiderMesh.}
  \label{fig:demo}
\end{figure*}

Figure~\ref{fig:demo} provides some visualization examples, which show that SpiderMesh effectively leverages multimodal data for better scene understanding. For example, in nighttime scenarios (columns 2 to 4), objects in dark areas can be successfully detected. In over-exposure scenes (columns 1 and 5), objects affected by strong exposure can be well outlined. Although the results are not perfect (e.g. missed segmentation in column 5), the influence of dazzling light is significantly weakened, compared to RGB segmentation.

To further evaluate the proposed SpiderMesh, we also compare it with baselines on PST900 dataset, as reported in Table~\ref{tab:sota_pst}. In line with expectations, SpiderMesh consistently outperforms others on mIoU under diverse underground scenes. For $4$ foreground categories, it achieves the best performance on $3$ (survivor, hand-drill, and backpack). SpiderMesh achieves a performance increase of $8.9\%$ on \textit{survivor} class over MFFENet-single, by effectively leveraging regional complementary features from thermal images.


\subsection{Ablation Study}

To better understand SpiderMesh, we further conduct several groups of experiments for ablation study on MFNet dataset (details of the compared architectures are provided in appendix). 

\textbf{Effect of Each Component} We conduct the following experiments to study the impact of the three components of SpiderMesh. The performances of the ablated networks are summarized in Table~\ref{tab:ablation}. In the baseline network, multimodal features are only fused at the classifier in the RGB branch, and normal feature upsampling operations are adopted.

\begin{table}[ht]
  \footnotesize
  \centering
  \begin{tabular}{l|c} 
  \hline
    \textbf{Ablated SpiderMesh}                    & \textbf{mIoU (\%)}      \\ \hline
    Full-version                             & 57.9           \\
    Removing M-CutOut                        & 55.9           \\
    Removing M-CutOut \& SRM                 & 55.2           \\
    Removing M-CutOut \& SRM \& DTM          & 52.4           \\ \hline

  \end{tabular}
  \caption{Ablation study on SpiderMesh components.}
  \label{tab:ablation}
\end{table}

As shown in the table, the overall gain of the three components is $5.5\%$, and removing each component results in a performance drop. Among them, performance improvement from DTM and M-CutOut are more significant, thanks to their regional complementation for RGB regions with thermal images. SRM further compensates the spatial-wise information loss with fused multimodal features and leads to an improvement of $0.7\%$.

\textbf{Architecture Design} The following ablation studies are conducted to understand the design choices for the proposed modules. First of all, we explore different RGB-T fusion methods in DTM. The baseline network is the same as that in previous experiment. We report the comparisons in Table~\ref{tab:dcf}. As one of the simple choices, the employment of feature summation during encoding can improve mIoU to $53.4\%$. However, the features are fused without distinction. The cross-modal weighted fusion (CWF) is in a passive `post’ manner, where one modality is spatial-wise weighted via self-attention and then integrated to the other modality. It performs better than summation, but is still not as good as the proactive manner in DTM. In DTM, cross-modal features are compensated according to the adaptive demand. Figure~\ref{fig:dmap} provides the visualization of demand maps, showing that the less informative an area is, the higher its demand for fusion is. For example, the dark and overexposed areas of RGB images usually have higher compensation demands, corroborating our hypothesis on the regional complementary power of thermal signals.

\begin{table}[ht]
  \footnotesize
  \centering
  \begin{tabular}{l|c} 
  \hline
    \textbf{RGB-T feature fusion method}           & \textbf{mIoU (\%) }     \\ \hline
    Summation at classifier                        & 52.4           \\
    Summation during encoding                      & 53.4           \\
    Cross-modal weighted fusion                    & 54.5           \\ \hline
    DTM (Ours)    & 55.2           \\ \hline

  \end{tabular}
  \caption{Ablation study on DTM module.}
  \label{tab:dcf}
\end{table}



\begin{figure}
  \centering
  \includegraphics[scale=0.35]{./figure/dmap.jpg}
  \caption{Visualization of the demand map for requesting cross-modal regional complementary information. \textit{Left}: demand map of RGB in dazzling-light scene. \textit{Middle}: demand map of RGB in nighttime. \textit{Right}: demand map of thermal in daytime.}
  \label{fig:dmap}
\end{figure}

Secondly, we study the design choices for SRM. The ablated network with DTM only is regarded as baseline. Experimental results in Table~\ref{tab:srm} show that all the three versions of SRM outperforms baseline benefiting from the spatial awareness, and the refinement with fused multimodal feature is a better choice since it contains rich detailed semantic information from both RGB-T signals.

\begin{table}[ht]
  \footnotesize
  \centering
  \begin{tabular}{l|c} 
  \hline
    \textbf{Methods}                                  & \textbf{mIoU (\%) }     \\ \hline
    Baseline (DTM only)                               & 55.2           \\
    SRM w/ self-modal feature only                    & 55.4           \\
    SRM w/ cross-modal feature only                   & 55.4           \\ \hline
    SRM w/ fused multimodal feature (Ours)            & 55.9           \\ \hline

  \end{tabular}
  \caption{Ablation study on SRM module. }
  \label{tab:srm}
\end{table}

\textbf{Robustness Analysis} It is important to analyze the robustness of multimodal approaches, as signal loss due to software/hardware failure is common in practice. When the input of one modality is lost, 
some existing methods may not be able to retain performance due to crippled network. To test SpiderMesh, we manually set the input of a modality to $0$ to simulate the signal loss, and evaluate the performance in daytime 
and nighttime scenarios. The results of both branches are reported for analysis.

\begin{table}[ht]
  \footnotesize
  \centering
  \begin{tabular}{cc|ccc} 
  \hline

    \textbf{Input modality}                    & \textbf{Branch}     &  \textbf{Daytime}           & \textbf{Nighttime}       & \textbf{All time}      \\ \hline
    \multirow{2}*{RGB + thermal}               & RGB                 &  \textbf{52.0}              & \textbf{56.0}            & \textbf{57.9} \\
                                               & Thermal             &  51.0                       & 55.7                     & 57.3          \\ \hline
    \multirow{2}*{RGB only}                    & RGB                 &  \textbf{40.1}              & \textbf{32.5}            & \textbf{39.6} \\
                                               & Thermal             &  39.8                       & 32.4                     & 39.2          \\ \hline
    \multirow{2}*{Thermal only}                & RGB                 &  \textbf{41.7}              & \textbf{51.1}            & \textbf{50.5} \\
                                               & Thermal             &  41.6                       & 50.8                     & 50.2          \\ \hline
    
  \end{tabular}
  \caption{Robustness analysis on modality signal loss (\%).}
  \label{tab:robust}
\end{table}

Overall, the performance with inputs from both modalities is the best, which demonstrates the benefit of leveraging multimodal information. The performance differences between RGB-only and thermal-only input indicate that thermal is the more dominant modality due to its high reliability under poor lighting conditions. The slight advantage of the RGB branch comes from further feature fusion at the classifier. It can also be observed that in each setting, both branches can still generate valid predictions when signal loss occurs. These results suggest that SpiderMesh is relatively robust when facing input signal loss.

\textbf{Complexity Analysis} We obtain several versions of SpiderMesh by replacing the backbone with different versions of ResNet. The complexity and corresponding performances are summarized in Table~\ref{tab:complexity}. Note that even the most lightweight model, SpiderMesh-50, can achieve comparable performance to FuseSeg \cite{fuseseg}. We further compare SpiderMesh-152 with two representative RGB-T methods using ResNet-152 as backbone. Taking the advantage of transformer in modeling long-range dependencies, MFTNet \cite{MFTNet} outperforms RTFNet \cite{rtfnet}, but is still not as good as ours. Meanwhile, SpiderMesh is more computationally efficient compared with them.

\begin{table}[ht]
  \footnotesize
  \centering
  \begin{tabular}{cc|cc} 
  \hline

    \textbf{Version}       & \textbf{Backbone}     &  \textbf{GFlops}      & \textbf{mIoU (\%)}   \\ \hline
    RTFNet\cite{rtfnet}     & ResNet-152   & 290.6       & 53.2        \\
    MFTNet\cite{MFTNet}     & ResNet-152   & 330.6        & 57.3        \\ \hline
    SpiderMesh-50    & ResNet-50    &  168.2       & 54.4        \\
    SpiderMesh-101   & ResNet-101   &  214.0       & 56.1        \\
    SpiderMesh-152   & ResNet-152   &  259.8       & 57.9        \\ \hline
    
  \end{tabular}
  \caption{Complexity analysis on different versions of SpiderMesh.}
  \label{tab:complexity}
\end{table}

\subsection{Evaluation on Semi-supervision}

We further evaluate SpiderMesh under semi-supervision setting. The training set is randomly split into two subsets, one of which is regarded as unlabeled subset. We make sure that each class appears in the labeled subset. Table~\ref{tab:semi} shows that SpiderMesh can make use of unlabeled data and yield a performance lift of $2.1\%$. This performance in low-data regime is comparable with that of other methods under full supervision as reported in Table~\ref{tab:sota_mf}. Considering the randomness in splitting scheme, we conduct 5 experiments with different partitions, and the gain is $2.1 \pm 0.1\%$ leveraging unlabeled pairwise RGB-T data.

\begin{table}[ht]
 
  \footnotesize
  \centering
  \begin{tabular}{c|cc|c} 
  \hline
    \textbf{Setting} & \textbf{Labeled} & \textbf{Unlabeled} & \textbf{mIoU (\%)} \\ \hline
    Supervised-only  & 392 &0     & 53.2               \\
    Semi-supervised  & 392 &392     & 55.3               \\ \hline

  \end{tabular}
  \caption{Results on semi-supervision in low-data regime.}
  \label{tab:semi}
\end{table}

\section{Conclusion}
In this paper, we point out that the key to RGB-T segmentation is to fully exploit the regional complementarity of thermal signals on optically-invisible regions. Addressing that, we propose the systematic SpiderMesh framework. DTM proactively compensates the features of less informative regions via demand-guided target masking in a `request’ manner, SRM recursively refines detailed semantic features for segmentation task. M-CutOut is proposed to create new optically-impaired regions and encourage the model to learn compensated features, and a semi-supervised setting is first explored leveraging both natural and artificial regional complementarity.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{mybib}
}

\end{document}
