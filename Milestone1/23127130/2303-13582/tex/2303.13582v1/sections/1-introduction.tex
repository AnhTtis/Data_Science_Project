\section{Introduction}
\vspace{-0.2cm}
Neural radiance fields (NeRF)~\cite{mildenhall2020nerf} have enabled high fidelity novel view synthesis from dozens of input views. Such number of views are difficult to obtain in practice, however. When given only a small number of sparse views, vanilla NeRF tends to struggle with reconstructing shape accurately, due to inadequate constraints enforced by the volume rendering loss alone.

Shape priors can help remedy this problem. Various forms of shape priors have been proposed for NeRFs, such as object category-level priors~\cite{jang2021codenerf}, and handcrafted data-independent priors~\cite{Niemeyer2021Regnerf}. The former requires knowledge of category labels and is not applicable to scenes, and the latter is agnostic to the specifics of the scene and only encodes low-level regularity (e.g., local smoothness). A form of prior that is both scene-dependent and category-agnostic is per-view monocular depth estimates, which have been explored in prior work~\cite{deng2022depth,roessle2022dense}. Unfortunately, monocular depth estimates are often inaccurate, due to estimation errors and inherent ambiguities, such as albedo vs. shading (cf. check shadow illusion), concavity vs. convexity (cf. hollow face illusion), distance vs. scale (cf. miniature cinematography), etc. As a result, the incorrect or inconsistent priors imposed by such depth estimates may mislead the NeRF into reconstructing incorrect shape and produce artifacts in the generated views. 

In this paper, we propose a method that embraces the uncertainty and ambiguities present in monocular depth estimates, by modelling a probability distribution over depth estimates. The ambiguities are retained at the stage of monocular depth estimation, and are only resolved once information from multiple views are fused together. We do so with a principled loss defined on probability distributions over depth estimates for different views. This loss selects the subset of modes of probability distributions that are consistent across all views and matches them with the modes of the depth distribution along different rays as modelled by the NeRF. It turns out that this operation can be interpreted as a probabilistic analogue of classical depth-based space carving~\cite{space_carving}. For this reason, we dub our method \emph{Space Carving with Ambiguity-aware Depth Estimates}, or SCADE for short.

Compared to prior approaches of depth supervision~\cite{deng2022depth,roessle2022dense} that only supervise the moments of NeRF's depth distribution (rather than the modes), our key innovation is that we supervise the modes of NeRF's depth distribution. The supervisory signal provided by the former is weaker than the latter, because the former only constrains the value of an integral aggregated along the ray, whereas the latter constrains the values at different individual points along the ray. Hence, the supervisory signal provided by the former is 2D (because it integrates over a ray), whereas the supervisory signal provided by the our method is 3D (because it is point-wise) and thus can be more fine-grained.
% Therefore, the supervisory signal provided by the proposed approach can be richer than prior approaches. }

An important technical challenge is modelling probability distributions over depth estimates. Classical approaches use simple distributions with closed-form probability densities such as Gaussian or Laplace distributions. Unfortunately these distributions are not very expressive, since they only have a single mode (known as ``unimodal'') and have a fixed shape for the tails. Since each interpretation of an ambiguous image should be a distinct mode, these simple unimodal distributions cannot capture the complex ambiguities in depth estimates. Na\"{i}ve extensions like a mixture of Gaussians are also not ideal because some images are more ambiguous than others, and so the number of modes needed may differ for the depth estimates of different images. Moreover, learning such a mixture requires backpropagating through E-M, which is nontrivial. Any attempt at modifying the probability density to make it capable of handling a variable number of modes can easily run into an intractable partition function~\cite{hinton2012practical,cho2013gaussian,ranzato2007unified}, which makes learning difficult because maximum likelihood requires the ability to evaluate the probability density, which is a function of the partition function. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ambiguities_illustration.png}
    \vspace{-1.3em}
    \caption{{\textbf{Ambiguities from a single image}. We show samples from our ambiguity-aware depth estimates that is able to handle different types of ambiguities. (Top) An image of a chair taken from the top-view. Without context of the scene, it is unclear that it is an image of a chair. We show different samples from our ambiguity-aware depth estimates that are able to capture different degrees of convextiy. (Bottom) An image of a cardboard under bad lighting conditions that capture the albedo vs shading ambiguity that is also represented by our different samples.}}
    \label{fig:ambiguities}
    \vspace{-1.3em}
\end{figure}

To get around this conundrum, we propose representing the probability distribution with a set of samples generated from a neural network. Such a distribution can be learned with a conditional GAN; however, because GANs suffer from mode collapse, they cannot model multiple modes~\cite{isola2017image} and are therefore unsuited to modelling ambiguity. Instead, we propose leveraging conditional Implicit Maximum Likelihood Estimation (cIMLE)~\cite{Li2020MultimodalIS,chimle} to learn the distribution, which is designed to avoid mode collapse. 

We consider the challenging setting of leveraging out-of-domain depth priors to train NeRFs on real-world indoor scenes with sparse views. Under this setting, the depth priors we use are trained on a different dataset (e.g., Taskonomy) from the scenes our NeRFs are trained on (e.g., ScanNet). This setting is more challenging than usual due to domain gap between the dataset the prior is trained on and the scenes NeRF is asked to reconstruct. We demonstrate that our method outperforms vanilla NeRF and NeRFs with supervision from depth-based priors in novel view synthesis. 

In summary, our key contributions include:
\begin{itemize}
    \item An method that allows the creation of NeRFs in unconstrained indoor settings from only a modest number of 2D views by introducing a sophisticated way to exploit ambiguity-aware monocular depth estimation. 
    \item A novel way to sample distributions over image depth estimates based on conditional Implicit Maximum Likelihood Estimation that can represent depth ambiguities and capture a variable number of discrete depth modes.
    \item A new space-carving loss that can be used in the NeRF formulation to optimize for a mode-seeking 3D density that helps select consistent depth modes across the views and thus compensate for the under-constrained photometric information in the few view regime.
\end{itemize}
