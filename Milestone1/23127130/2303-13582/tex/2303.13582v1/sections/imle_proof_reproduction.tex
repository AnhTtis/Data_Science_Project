\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\providecommand{\lemmaname}{Lemma}
\providecommand{\theoremname}{Theorem}

Here we show an adaptation of the proof provided in IMLE~\cite{li2018implicit} in the context of learning our ambiguity-aware depth estimates. Recall that we are given a set of input images $\{I_1, I_2, ..., I_n\}$ each with a corresponding ground truth depth map $D_1, D_2, ..., D_n$. As we know that monocular depth estimation is inherently ambiguous, we desire to learn a \emph{multimodal distribution of depth estimates} conditioned on an input image given only one ground truth lable (\ie depth map). 

Thus, we want to learn the network parameters $\phi$ for conditional distribution $G$ such that $G_\phi(I, z)$ models the distribution of depth estimates for a given input image $I\in \{I_1, I_2, ..., I_n\}$, where $z\sim \mathcal{N}(0, \mathbf{I})$ are latent codes sampled from a normal distribution.

Unlike GAN's~\cite{karras2019style} that optimize that each sample is similar to a ground truth data point, cIMLE~\cite{Li2020MultimodalIS} prevents mode collapse by instead enforcing that all ground truth data points are explained by at least one generated sample. Hence, in order to learn $\phi$, the objective function that we want to optimize is maximizing the sum of the likelihoods at the training examples. 

Consider our ambiguity-aware prior $G_{\phi, i}$, an implicit generative model, the likelihood induced by this model $P_{\phi, i}$ is computationally intractable to compute as it cannot be expressed in closed form. In this proof, we show that maximizing this likelihood is equivalent to optimizing a sample-based objective, making it tractable. We first i) rewrite the desired objective function (Sec~\ref{sec:objective_function}), ii) show its equivalence to the loss function used in training (Sec~\ref{sec:equivalence_loss_function}), then finally iii) show its equivalence to maximizing the sum of the likelihoods at the training examples, \ie the single ground truth depth maps associated with each image (Sec~\ref{sec:imle_convergence_proof}).

\subsubsection{Objective function}
\label{sec:objective_function}
Let's consider the following objective function:

\begin{equation}
\begin{split}
\max_{\phi}\mathcal{L}_{\{\delta_{i}\}_{i}}(\phi):=\max_{\phi}\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\Big[\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}\Big(\delta_{i}-\\
\frac{1}{M}\sum_{j=1}^{M}\Phi_{\delta_{i}}(d(y_{i, j},D_i)\Big)\Big]
\end{split}
\end{equation}

\noindent $y_{i, j}$ is a sampled depth estimate, \ie $y_{i, j}= G(I_i, z_j)$, $M$ is the number of samples drawn, $d(y_{i, j},D_i)$ denotes the distance between the sampled depth estimate and the given ground truth depth map for image $I_i$. 

$\delta_i >0$ denotes the threshold of the radius of the largest neighborhood that we are interested in, \ie the neighborhood around the ground truth data points (depth maps) where we are interested in having generated depth estimate samples at. This radius is dependent on the training example (hence the subscript $i$) as some examples may have a larger/smaller neighborhood of interest than others. $\Phi_{\delta_i}$ is a function we choose, which we will define below, and $w_i$ is a weighting factor that is also dependent on the training example.

Note that here, we reuse $\mathcal{L}$ to denote the likelihood, and it should not be confused with the notation for the loss functions in the main paper.\\

\noindent\textbf{Choosing $\Phi_\delta$.}\\

For $\delta>0$ (threshold on the radius), $\Phi_\delta$ is chosen as
\begin{equation}
\begin{split}
\Phi_{\delta}(t)=
\begin{cases}
t & 0 \leq t\leq\delta\\
\delta & t>\delta
\end{cases},
\end{split}
\end{equation}

\noindent Intuitively, this assigns the random variable t, which is our case will be the the distance $d(\cdot)$ between the ground truth depth and a sampled depth estimate, to a value depending on the radius threshold $\delta$. Any distance larger than $\delta$, \ie is the sampled estimate is far enough, is set to $\delta$.

Consequently, the chosen antiderivative is shown below
\[
\Phi_{\delta}'(t)=\begin{cases}
1 & 0 \leq t\leq\delta\\
0 & t>\delta
\end{cases}
\]

\noindent\textbf{Relating to model distribution $P_{\phi, i}$}\\
Three lemmas written below tie together the likelihood $\mathcal{L}_{\delta}$ to the objective function. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}
Let $Y$ be a non-negative random variable and $f$ be a continuous
function on $[0,\infty)$, and $f'$ to denote a function
whose antiderivative is $f$.

\[
\mathbb{E}\left[f(Y)\right]=f(0)+\int_{0}^{\infty}f'(t)\mathrm{Pr}(Y\geq t)dt
\]
\end{lem}

\begin{proof}

\begin{align*}
&f(0)+\int_{0}^{\infty}f'(t)\mathrm{Pr}(Y\geq t)dt \\ &=f(0)+\int_{0}^{\infty}\int_{t}^{\infty}f'(t)p(y)dydt\\
 & =f(0)+\int_{\{y\geq t,t\geq0\}}f'(t)p(y)d\left(\begin{array}{c}
y\\
t
\end{array}\right)\\
%  & =f(0)+\int_{\{t\leq x,t\geq0\}}f'(t)p(x)d\left(\begin{array}{c}
% x\\
% t
% \end{array}\right)\\
 & =f(0)+\int_{0}^{\infty}\int_{0}^{y}f'(t)p(y)dtdy\\
 & =f(0)+\int_{0}^{\infty}\left(\int_{0}^{y}f'(t)dt\right)p(y)dy\\
 & =f(0)+\int_{0}^{\infty}\left(f(y)-f(0)\right)p(y)dy\quad\text{(2nd FTC)}\\
 & =f(0)+\int_{0}^{\infty}f(y)p(y)dy-\int_{0}^{\infty}f(0)p(y)dy\\
%  & =f(0)+\int_{0}^{\infty}f(y)p(y)dx-f(0)\int_{0}^{\infty}p(y)dy\\
 & =f(0)+\mathbb{E}\left[f(Y)\right]-f(0)\\
 & =\mathbb{E}\left[f(Y)\right]
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}
With the chosen $\Phi_{\delta}(\cdot)$ and $\Phi_{\delta}'(\cdot)$ shown previously,
$\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\Phi_{\delta_{i}}(d(y_{i, j},D_i))\right]=\delta_{i}-\int_{0}^{\delta_{i}}\mathrm{Pr}(d(y_{i, j},D_i)<t)dt$.
\end{lem}

\begin{proof}
By definition, $\Phi_{\delta_{i}}(0)=0$.

\begin{align*}
& \mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\Phi_{\delta_{i}}(d(y_{i, j},D_i)\right] \\
&=\Phi_{\delta_{i}}(0) + \int_{0}^{\infty}\Phi_{\delta_{i}}'(t)\mathrm{Pr}(d(y_{i, j},D_i)\geq t)dt\quad\\
&\text{(From Lemma 1)}\\
& =\int_{0}^{\delta_{i}} 1 \cdot \mathrm{Pr}(d(y_{i, j},D_i)\geq t)dt \\
&+ \int_{\delta_{i}}^{\infty} 0 \cdot \mathrm{Pr}(d(y_{i, j},D_i)\geq t)dt\\
 & =\int_{0}^{\delta_{i}}\mathrm{Pr}(d(y_{i, j},D_i)\geq t)dt\\
 & =\int_{0}^{\delta_{i}}\left(1-\mathrm{Pr}(d(y_{i, j},D_i)<t)\right)dt\\
 & =\delta_{i}-\int_{0}^{\delta_{i}}\mathrm{Pr}(d(y_{i, j},D_i)<t)dt
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}
The likelihood above is equivalent to  $\mathcal{L}_{\{\delta_{i}\}_{i}}(\phi)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{Mw_{i}}\sum_{j=1}^{M}\int_{0}^{\delta_{i}}\mathrm{Pr}(d(y_{i, j},D_i)<t)dt$.
\end{lem}

\begin{proof}

\begin{align*}
&\mathcal{L}_{\{\delta_{i}\}_{i}}(\phi) \\
& =\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}\left(\delta_{i}-\frac{1}{M}\sum_{j=1}^{M}\Phi_{\delta_{i}}(d(y_{i, j},D_i))\right)\right]\\
 & =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}\left(\delta_{i}-\frac{1}{M}\sum_{j=1}^{M}\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\Phi_{\delta_{i}}(d(y_{i, j},D_i)\right]\right)\\
 & =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}\\
 &\left(\delta_{i}-\frac{1}{M}\sum_{j=1}^{M}\left(\delta_{i}-\int_{0}^{\delta_{i}}\mathrm{Pr}(d(y_{i, j},D_i)<t)dt\right)\right)\quad\\
 &\text{(From Lemma 2)}\\
 & =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{Mw_{i}}\sum_{j=1}^{M}\int_{0}^{\delta_{i}}\mathrm{Pr}(d(y_{i, j},D_i)<t)dt
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Equivalence to loss function for training}
\label{sec:equivalence_loss_function}
Here shows the equivalence of a tractable sample-based loss function used for training. \\

\noindent\textbf{Radius Threshold $\delta_i$}
Lemma 3 shows that the likelihood computes the probability the model $P_phi$ assigns to the neighborhood of the training sample, which is controlled by the radius threshold $\delta_i$. To maximize the likelihood, a small neighborhood is desired, hence a small value of $\delta_i$ is desirable. However, if $\delta_i$ is ``too small", then by the chosen $\Phi_{\delta_i}$, if $d(y_{i, j},D_i)>\delta_i$, for all $j$, then $d(y_{i, j},D_i)=\delta_i \forall j$, which leads to $\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}(\delta_{i}-\frac{1}{M}\sum_{j=1}^{M}\Phi_{\delta_{i}}(d(y_{i, j},D_i) =0$. This leads to having no gradients w.r.t. to $\phi$ since it is constant, which does not allow for network training. Thus the smallest $\delta_i$ that can have such that the expression's value is not constant and allows for gradients is $\min_{j\in[M]}d(y_{i, j},D_i)$. The likelihood objective then becomes:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\begin{align*}
& \mathcal{L}_{\{\delta_{i}\}_{i}}(\theta) =\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\\
&\left[\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}\left(\delta_{i}-\frac{M-1}{M}\delta_{i}-\frac{1}{M}\min_{j\in[M]}d(y_{i, j}, D_i)\right)\right]\\
 & =\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\frac{1}{n}\sum_{i=1}^{n}\frac{1}{w_{i}}\left(\frac{1}{M}\delta_{i}-\frac{1}{M}\min_{j\in[M]}d(y_{i, j}, D_i)\right)\right]\\
 & =\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\frac{1}{nM}\sum_{i=1}^{n}\frac{1}{w_{i}}\left(\delta_{i}-\min_{j\in[m]}d(y_{i, j}, D_i)\right)\right]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

The sample-based loss function then becomes equivalent to the objective of maximizing the likelihood as follows:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\begin{align*}
&\arg\max_{\phi}\mathcal{L}_{\{\phi_{i}\}_{i}}(\phi) \\
& =\arg\max_{\phi}\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\\
&\left[\frac{1}{nM}\sum_{i=1}^{n}\frac{1}{w_{i}}\left(\delta_{i}-\min_{j\in[M]}d(y_{i, j}, D_i)\right)\right]\\
& =\arg\max_{\phi}\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\sum_{i=1}^{n}\frac{\delta_i}{w_{i}} - \frac{1}{w_{i}}\min_{j\in[M]}d(y_{i, j}, D_i)\right]\\
%  & =\arg\max_{\theta}\mathbb{E}_{\{\mathbf{\tilde{x}}_{j}\}_{j}\sim P_{\theta}}\left[\sum_{i=1}^{n}\frac{\tau_{i}}{w_{i}}-\sum_{i=1}^{n}\frac{1}{w_{i}}\max(\min_{j\in[m]}d(\tilde{\mathbf{x}}_{j},\mathbf{x}_{i}),\delta\tau_{i})\right]\\
& =\arg\max_{\phi}\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[-\sum_{i=1}^{n} \frac{1}{w_{i}}\min_{j\in[M]}d(y_{i, j}, D_i)\right]\\
 & =\arg\min_{\phi}\mathbb{E}_{\{y_{i,j} \sim P_{\phi,i}\}_{i,j}}\left[\sum_{i=1}^{n}\frac{1}{w_{i}}\min_{j\in[M]}d(y_{i, j}, D_i)\right]
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent which is the sample-based loss function, \ie taking the minimum loss for the set of drawn samples. In our case, $w_i=0 \forall i$, and for each training data point, we sample $M=20$ estimates by drawing $z_j\sim \mathcal{N}(0, \mathbf{I})$, and taking the minimum loss w.r.t. to the corresponding single ground truth depth map $D_i$ for the training data point. This allows us to learn multimodal depth distributions to capture the inherent ambiguities in monocular depth estimation.

\subsubsection{Equivalence to maximizing the sum of the likelihoods.}
\label{sec:imle_convergence_proof}
For completeness, here shows the equivalence of the objective function to maximizing the sum of the likelihood as proven in IMLE~\cite{li2018implicit}. The learning of $\phi$ involves solving a sequence of optimization problems at current values for $\delta_i$, and as optimization progresses later into the sequence, $\delta_i$ becomes smaller and smaller and eventually converges to the maximum likelihood.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}
$\lim_{\{\delta_{i}\to0^{+}\}_{i}}\mathcal{L}_{\{\delta_{i}\}_{i}}(\phi)=\frac{1}{n}\sum_{i=1}^{n}p_{\delta}(D_{i})$.
\end{lem}

\begin{proof}

\begin{align*}
&\mathcal{L}_{\{\delta_{i}\}_{i}}(\phi) \\
& =\frac{1}{n}\sum_{i=1}^{n}\frac{1}{Mw_{i}}\sum_{j=1}^{M}\int_{0}^{\tau_{i}}\mathrm{Pr}(d(y_{i, j}, D_i)<t)dt\quad\\
&\text{(From Lemma 3)}\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\frac{1}{w_{i}}\int_{0}^{\tau_{i}}\int_{B_{t}(D_i)}p_{\phi, i}(\mathbf{y})d\mathbf{y}dt\\
%  & =\frac{1}{nm}\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{1}{w_{i}}\int_{\delta\tau_{i}}^{\tau_{i}}\int_{B_{t}(\mathbf{x}_{i})}p_{\theta}(\mathbf{x})d\mathbf{x}dt\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\frac{\int_{0}^{\delta_{i}}\int_{B_{t}(D_i)}P_{\phi, i}(\mathbf{y})d\mathbf{y}dt}{\int_{0}^{\delta_{i}}\int_{B_{t}(D_{i})}d\mathbf{y}dt}
\end{align*}

\begin{align*}
&\lim_{\{\delta_{i}\to0^{+}\}_{i}}\mathcal{L}_{\{\delta_{i}\}_{i}}(\phi) \\
& =\frac{1}{nM}\sum_{i=1}^{n}\left(\lim_{\delta_{i}\to0^{+}}\left(\sum_{j=1}^{M}\frac{\int_{0}^{\delta_{i}}\int_{B_{t}(D_{i})}p_{\phi, i}(\mathbf{y})d\mathbf{y}dt}{\int_{0}^{\delta_{i}}\int_{B_{t}(D_{i})}d\mathbf{y}dt}\right)\right)\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\left(\lim_{\delta_{i}\to0^{+}}\frac{\int_{0}^{\delta_{i}}\int_{B_{t}(D_{i})}p_{\phi, i}(\mathbf{y})d\mathbf{y}dt}{\int_{0}^{\delta_{i}}\int_{B_{t}(D_{i})}d\mathbf{y}dt}\right)\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\left(\lim_{\delta_{i}\to0^{+}}\frac{\int_{B_{\delta_{i}}(D_{i})}p_{\phi, i}(\mathbf{y})d\mathbf{y}}{\int_{B_{\delta_{i}}(D_{i})}d\mathbf{y}}\right)\quad\\
 &\text{(L'H\^{o}pital and 2nd FTC)}\\
%  & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\left(\lim_{\delta_{i}\to0^{+}}\frac{\int_{B_{\delta_{i}}(\mathbf{x}_{i})}p_{\theta}(\mathbf{x})(1-\delta\mathbf{1}_{B_{\delta\tau_{i}}(\mathbf{x}_{i})}(\mathbf{x}))d\mathbf{x}}{\int_{B_{\tau_{i}}(\mathbf{x}_{i})}1-\delta\mathbf{1}_{B_{\delta\tau_{i}}(\mathbf{x}_{i})}(\mathbf{x})d\mathbf{x}}\right)\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\left(\lim_{\delta_{i}\to0^{+}}\frac{\int_{0}^{\delta_{i}}\int_{\{\mathbf{y}\vert d(\mathbf{y},D_i)=r\}}p_{\phi, i}(\mathbf{y})d\mathbf{y}dr}{\int_{0}^{\delta_{i}}\int_{\{\mathbf{y}\vert d(\mathbf{y},D_i)=r\}}d\mathbf{y}dr}\right)\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}\left(\lim_{\delta_{i}\to0^{+}}\frac{\int_{\{\mathbf{y}\vert d(\mathbf{y},D_i)=\delta_{i}\}}p_{\phi, i}(\mathbf{y})d\mathbf{y}}{\int_{\{\mathbf{y}\vert d(\mathbf{y},D_i)=\delta_{i}\}}d\mathbf{y}}\right)\quad\\
 &\text{(L'H\^{o}pital and 2nd FTC)}\\
 & =\frac{1}{nM}\sum_{i=1}^{n}\sum_{j=1}^{M}p_{\phi, i}(D_i)\\
 & =\frac{1}{n}\sum_{i=1}^{n}p_{\phi, i}(D_i)
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%