\section{Additional Results}
\subsection{Experiments on Tanks and Temples}
We conduct further experiments to test the robustness of SCADE. We evaluate on three scenes from the Tanks and Temples~\cite{Knapitsch2017} dataset, namely three large indoor rooms - Church, Courtroom and Auditorium scenes. The training set consists of 21, 26 and 21 sparse views for the Church, Courtroom and Auditorium scenes respectively, and the test set consists of 8 sparse views, so the amount of data is similar to that used in prior work~\cite{roessle2022dense}. We also followed similar data preprocessing steps as prior work~\cite{roessle2022dense} and ran SfM~\cite{schoenberger2016sfm} on all images to obtain camera poses for training.

As shown in Table~\ref{tbl:tnt_results}, \textbf{SCADE} trained with the same out-of-domain prior that we used for the other datasets (which was trained on Taskonomy~\cite{zamir2018taskonomy}) outperforms the baselines on the Tanks and Temples dataset as well. Moreover, Figure~\ref{fig:tanks_and_temples_quali} shows qualitative results. As shown, \textbf{SCADE} is able to recover objects better than the baselines such as the table in the Church, the group of chairs in the Courtroom (second column), and the rows of seats in the Auditorium (clearer in the side-view seats on the second column). Moreover, results also show that \textbf{SCADE} avoids clouds of dust such as the lights on the wall of the Church (second column), painting on the wall of the Courtroom (last column) and details on the repetitive seats of the auditorium.

% \mika{Qualitative results explanation done in a rush. Please edit if possible :)}

\begin{table}
\centering
\setlength\tabcolsep{2pt}
\begin{tabularx}{\linewidth}{c|C|C|C}
    \toprule
    &  PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
    \midrule
    Vanilla NeRF~\cite{mildenhall2020nerf} & 17.19 & 0.559 & 0.457 \\
    DDP~\cite{roessle2022dense} & 19.18 & 0.651 & 0.361 \\
    \midrule
    SCADE & \textbf{20.13} & \textbf{0.662} & \textbf{0.358}\\
    \bottomrule
\end{tabularx}
\caption{\textbf{Quantitative results for the Tanks and Temples~\cite{Knapitsch2017} dataset}. }
\label{tbl:tnt_results}
% \vspace{-1.2\baselineskip} 
\end{table}

\subsection{Video Demo}
% We also attached a video trajectory of a scene for each of the three datasets in our experiments. Please watch the attached video file.  
Our project page \href{https://scade-spacecarving-nerfs.github.io}{scade-spacecarving-nerfs.github.io} shows a video trajectory from each of the three datasets in our experiments.
As shown on the Scannet scene, \textbf{SCADE} is able to better recover and crisp up the black chair; on the In-the-Wild data scene, the room and objects behind the glass wall are better captured, and finally, on the Tanks and Temples scene, the table on the center is more solid and also clear up dust on the wall and aisle of the church. 

% \mika{Need to make the video and explain what's happening I guess.}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/courtroom_quali.pdf}
%     \caption{Qualitative Results for Courtroom scene in Tanks and Temples.}
%     \label{fig:courtroom}
%     % \vspace{-1.3em}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/church_quali.pdf}
%     \caption{Qualitative Results for Church scene in Tanks and Temples.}
%     \label{fig:church}
%     % \vspace{-1.3em}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/auditorium_quali.pdf}
%     \caption{Qualitative Results for Auditorium scene in Tanks and Temples.}
%     \label{fig:auditorium}
%     % \vspace{-1.3em}
% \end{figure*}

% \section{Additional Ablation}
% Additional ablation on in-the-wild data.

% \begin{table}
% \centering
% \setlength\tabcolsep{2pt}
% \begin{tabularx}{\linewidth}{c|C|C|C}
%     \toprule
%     &  PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
%     \midrule
%      MonoSDF supervision & 20.95 & 0.731 & 0.371 \\
%      \midrule
%      DDP prior - single sample & 21.03 & 0.734 & 0.371 \\
%      DDP prior - multiple samples & 20.45 & 0.721 & 0.386 \\
%      \midrule
%      Our prior - single sample & 21.31 & 0.734 & 0.365 \\
%     \midrule
%     SCADE & \textbf{21.48} & \textbf{0.736} & \textbf{0.356}\\
%     \bottomrule
% \end{tabularx}
% \caption{\textbf{Ablation Study on In-the-Wild Data}. 
% % 2D refers to taking the expected depth along the ray. 3D refers to our space carving loss through sampling in 3D along the ray. 
% Mono-SDF supervision refers to supervising with MonoSDF loss on expected ray termination distance (as opposed to the full distribution) using our prior. }
% \label{tbl:ablation}
% % \vspace{-1.2\baselineskip} 
% \end{table}

\begin{table}
\centering
\setlength\tabcolsep{2pt}
\begin{tabularx}{\linewidth}{c|C|C|C}
    \toprule
    &  PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
    \midrule
    $M=1\text{  }$ & 21.22 & 0.714 & 0.318 \\
    $M=5\text{  }$ & 21.05 & 0.722 & 0.304 \\
    $M=10\text{  }$ & 21.41 & 0.729 & 0.296\\
    $M=20\text{  }$ & 21.54 & \textbf{0.732} & \textbf{0.292}\\
    $M=40\text{  }$ & \textbf{21.61} & 0.729 & 0.293\\
    $M=80\text{  }$ & 21.58 & 0.729 & 0.295\\
    \bottomrule
\end{tabularx}
\caption{\textbf{Ablation on $M$}. We ablate on the number of depth estimates used from our ambiguity-aware prior to train SCADE.}
\label{tbl:m_ablation}
% \vspace{-1.2\baselineskip} 
\end{table}

\subsection{Ablation on Number of Hypotheses $M$}
We further ablate on the number of estimates $M$ from our ambiguity-aware prior for training SCADE. Table~\ref{tbl:m_ablation} shows the quantative results for the Scannet dataset. As shown, the results in general improve as we increase the number of depth estimates as this gives us a better approximation of the depth distribution. We observe that the improvement is marginal as we increase the number of depth estimates beyond $20$. Hence, we use $M=20$ in our experiments.

\section{Implementation Details}
% We train our ambiguity-aware prior on the Taskonomy~\cite{zamir2018taskonomy} dataset. We initialize the weights with the pretrained LeReS~\cite{Wei2021CVPR} model. 
We train our ambiguity-aware prior with a batch size of 16 and use a learning rate of 0.001 for the base model, and 0.0001 for the MLP layers before AdaIn~\cite{huang2017arbitrary}. We use a latent code dimension of $32$, and we follow the standard cIMLE training strategy~\cite{Li2020MultimodalIS} and sample 20 latent codes per image and resample every 10 epochs. 

To train our NeRF model, we use a batch size of 1024 rays for 500k iterations. We use the Adam optimizer~\cite{kingma2014adam} with a learning rate of $5e^{-4}$ decaying to $5e^{-5}$ in the last 100k iterations. We use the same architecture as the original NeRF~\cite{mildenhall2020nerf}, which samples 64 points the coarse network and an additional 128 points for the fine network. Because the depth estimates are in relative scale, we directly optimize for the scale and shift for each input image. We directly optimize a 2-dim variable for \emph{each input image} that scales and shifts depth hypotheses. These variables are jointly optimized with the NeRF for the first 400k iters (learning rate of 1e-6), and are then kept frozen for the last 100k iters. 

\section{Ambiguity-Aware Depth Estimates}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/depth_hypothesis.png}
    
    \caption{{\textbf{Depth Estimate Samples}. Here we show two examples of train images from scenes used in our experiments that show the ambiguity in (top) different degrees of convexity and (b) albedo vs shading ambiguity on the door frame and possible existence of an object inside the bookshelf. Please see Fig.~\ref{fig:depth_hypothesis_many} for more examples.}}
    \label{fig:depth_hyp}
    \vspace{-1.3em}
\end{figure}

We show some samples from the depth distribution of our multimodal prior from scenes in ScanNet and our in-the-wild data in Figures~\ref{fig:teaser_supp} and \ref{fig:depth_hyp}. We see that depth from a single input image is ambiguous as captured by our multimodal prior. In Figure~\ref{fig:teaser_supp}, we are able to capture the multimodality in ray termination distance caused by non-opaque glass surfaces. In Figure~\ref{fig:depth_hyp} (top row) we are able to capture different degrees of concavity of the sofa as well as the ambiguity in the depth of the far wall and floor. In Figure~\ref{fig:depth_hyp} (bottom row), we have ambiguity on the presence of a dark colored object on the boxed shelf and the depth of the door w.r.t. the door frame due to albedo vs shading ambiguity. Figure~\ref{fig:depth_hypothesis_many} shows more samples of our multimodal depth estimates on train images for the different scenes used in our experiments. Note that the depth map visualizations are normalized per image, i.e. the colors represent per image relative depth. 

% \red{How do we tell them to view in high res color?}

% \mika{More text descriptions on the large figure?}

\subsection{Adaptation of the cIMLE~\cite{Li2020MultimodalIS} proof for our Ambiguity-Aware Prior}
\input{sections/imle_proof_reproduction}

% \section{Implementation Details}
% We further provide additional implementation detail: for each input image, we initialize the scale and shift of the depth estimates from our ambiguity-aware prior by aligning with the visible sparse SfM points. This is done through least squares fitting w.r.t. the sparse SfM points and the sampled depth estimate from the prior.

% \mika{Not sure if I am explaining this right. Or if we should explicitly say "align each hypothesis with SfM points independently".}

\section{Training Images Samples}
We also show samples of train images from the three scenes in each of the three datasets in our experiments. Samples are shown in Figure~\ref{fig:train_images}.

\begin{figure*}
  \centering
    \includegraphics[width=\textwidth]{figures/glass_hypothesis.pdf}
    \captionof{figure}{\textbf{Ambiguity-Aware Depth Estimates}. Hypothesis from two input views with non-opaque surfaces. This figure shows that in both cases, our ambiguity-aware prior is able to recover a distribution of depth estimates that is multimodal. These multimodal distributions allow to capture the room as well as the recover the objects behind the glass. Given the multiple views with multimodal distributions, NeRF is able to \textbf{find the mode} that is consistent, hence allowing for less blurry and better photometric reconstruction. Please view the attached video demo for the results on this In-the-Wild scene.
    % \mika{Please help improve this. I think we should fill the first page? Not sure if it is too big. It was too blurry in the teaser.}
    }`
    \label{fig:teaser_supp}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.91\linewidth]{figures/tnt_quali_new.pdf}
    \caption{Qualitative Results for the Tanks and Temples~\cite{Knapitsch2017} dataset.}
    \label{fig:tanks_and_temples_quali}
    % \vspace{-1.3em}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/depth_hypothesis_many_v2.pdf}
    \caption{\textbf{Samples from our Ambiguity-Aware Depth Estimates on train images of the different scenes used in our experiments}. Ambiguity is shown in [Left; right]: (a) How far the back wall is relative to the chair as well as the width of the cabinet and how far it is relative to the desk; whether the door is at a different compared to the wall and the relative depth of the the second chair w.r.t. to the nearer chair and the wall. (b) Objects on the desk have varying depths, e.g. it is unclear from a single view whether the papers have a thickness or not; relative depth of the chair w.r.t. the wall and the camera (c) Depth of the bookshelf; albedo v.s. shading of the door w.r.t to the door frame. (d) Depth of the curtain, whether it is flat on the wall or not, and without scene context, it can also be interpreted as painted texture on the wall; relative depths of the different cluttered objects. (e) Relative depths of the barrier, the seats and the far back wall with a cabinet; depth of the far back corner of the room w.r.t. the desk and chair and the camera. (f) Whether the painting is flat on the wall or the frame protrudes it out; relative depths of the chairs and the far back wall. (g) Whether the painted texture is convex or is flat (i.e. just painted) on the wall; whether there is a far back door or is just a texture on the wall. (h) Both are similar to g. left but on different viewpoints and on the opposite side of the room. (i) Non-opaque surface ambiguity due to the glass cabinet; glass door behind the sofa is also non-opaque. 
    % \mika{Please check this too and if it is too much detail. I can also add more examples if time permits.} 
    }
    \label{fig:depth_hypothesis_many}
    % \vspace{-1.3em}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/training_images.pdf}
    \caption{Samples of training images from the three scenes from the three datasets - Scannet~\cite{dai2017scannet}, In-the-Wild and Tanks and Temples~\cite{Knapitsch2017}.}
    \label{fig:train_images}
    % \vspace{-1.3em}
\end{figure*}