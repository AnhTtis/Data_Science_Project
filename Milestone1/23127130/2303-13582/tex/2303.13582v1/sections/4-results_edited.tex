\section{Results}
\vspace{-0.2cm}
In this section, we present our experimental evaluation to demonstrate the advantages of \textbf{SCADE}. 
% We first discuss our data sources, evaluation metric and implementation details, and then present our experimental results against baselines then finally provide thorough ablation studies.

\subsection{Datasets and Evaluation Metrics}
\vspace{-0.2cm}
We evaluate our method on ScanNet~\cite{dai2017scannet} and an in-the-wild dataset we collected~\footnote{Please also see supplement for results on the Tanks and Temples dataset~\cite{Knapitsch2017}.}.
% ~\footnote{Due to licensing issues, our company policy restricts us from using various other common datasets.}. 
We use the sparse-view ScanNet data used by DDP~\cite{roessle2022dense} in their evaluations, which comprises of three sample scenes each with 18 to 20 train images and 8 test images. To further test the robustness of our method, we further evaluated our method on three in-the-wild scenes collected using an iPhoneX. We captured sparse views in three different scenes -- basement, kitchen and lounge, where each scene has 18 to 23 train images and 8 test images. Following similar data preprocessing steps as DDP~\cite{roessle2022dense}, we ran SfM~\cite{schoenberger2016sfm} on all images to obtain camera poses for NeRF training. For quantitative comparison, we follow the original NeRF~\cite{mildenhall2020nerf} paper and report the PSNR, SSIM~\cite{wang2004image} and LPIPS~\cite{zhang2018unreasonable} on the novel test views. 

\subsection{Implementation Details}
\vspace{-0.2cm}
We train our ambiguity-aware prior on the Taskonomy~\cite{zamir2018taskonomy} dataset. We initialize the weights with the pretrained LeReS~\cite{Wei2021CVPR} model. We use $M=20$ depth estimates in our experiments. Please see supplement for additional implementation details. 

\begin{figure}[t]
\vspace{-2mm}
	\begin{center}
		\includegraphics[width=\linewidth]{figures/depth_fusion.pdf}
	\end{center}
    \vspace{-0.6cm}
    \caption{\textbf{Depth and Fusion Comparison.} Depth map and fusion comparison between SCADE and DDP.}
	\vspace{-0.3cm}
	\label{fig:depth_fusion}
\end{figure}

\begin{table}
\centering
\setlength\tabcolsep{2pt}
\begin{tabularx}{\linewidth}{c|C|C|C}
    \toprule
    & PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
    \midrule
    Vanilla NeRF~\cite{mildenhall2020nerf} & 19.03 & 0.670 & 0.398\\
    NerfingMVS~\cite{Wei2021CVPR} & 16.29 & 0.626 & 0.502\\
    IBRNet~\cite{wang2021ibrnet}& 13.25 & 0.529 & 0.673\\
    MVSNeRF~\cite{mvsnerf} & 15.67 & 0.533 & 0.635\\
    DS-NeRF~\cite{deng2022depth} & 20.85 & 0.713 & 0.344\\
    DDP~\cite{roessle2022dense} & 19.29 & 0.695 & 0.368\\
    \midrule
    \textbf{SCADE} (Ours) & \textbf{21.54} & \textbf{0.732} & \textbf{0.292}\\
    \bottomrule
\end{tabularx}
\vspace{-0.2cm}
\caption{\textbf{ScanNet Results}. Results for DS-NeRF and NerfingMVS follow what was reported in prior literature~\cite{roessle2022dense}. Because our setting requires out-of-domain priors, the results for DDP are with out-of-domain priors. The results of DDP with in-domain priors are (20.96, 0.737, 0.236) for PSNR, SSIM and LPIPS, respectively.}
\label{tbl:ScanNet_results}
\vspace{-1.2\baselineskip} 
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/scannet_quali_new.pdf}
    \vspace{-0.2cm}
    \caption{{\bf{Qualitative results on ScanNet}.}}
    \vspace{-0.4cm}
    \label{fig:ScanNet}
    % \vspace{-1.3em}
\end{figure*}

\subsection{Experiments on ScanNet}
\vspace{-0.1cm}
We evaluate on ScanNet following the evaluation from DDP~\cite{roessle2022dense}. We compare to the original NeRF~\cite{mildenhall2020nerf} (\textbf{Vanilla NeRF}) and the recent state-of-the-art NeRF with depth prior-based supervision, Dense Depth Prior~\cite{roessle2022dense} (\textbf{DDP}). Table~\ref{tbl:ScanNet_results} shows SCADE quantitatively outperforming the baselines. Because we are interested in simulating real-world conditions with a domain gap between the prior and the NeRF, our setting requires the use of out-of-domain priors. Since DDP uses a in-domain prior, we retrain DDP's depth completion network using their official code and hyperparameters on Taskonomy~\cite{zamir2018taskonomy}, \ie the same out-of-domain dataset that our prior is trained on. 


Figure~\ref{fig:ScanNet} shows our qualitative results. As shown, compared to the baselines, SCADE is able to avoid producing the clouds of dust that are present in the results of baselines (first, third and last column). Moreover, SCADE is also able to snap and recover objects in the scene such as the details on the blinds (second column), the back of the chair (fourth column) and the legs of the piano stool (fifth column). Moreover, Fig.~\ref{fig:depth_fusion} shows rendered depthmaps and fusion results using~\cite{zeng20163dmatch} on a Scannet scene. Notice that SCADE is able to recover better geometry compared to DDP -- see corner of the calendar, cabinets and office chair in the right image.

\subsection{Experiments on In-the-Wild Data}
\vspace{-0.1cm}
To further test the robustness of SCADE, we further evaluate on data collected in-the-wild with a standard phone camera (iPhoneX). The intrinsics for different views are different and are not known to the algorithm. As shown in Table~\ref{tbl:wild_results}, \textbf{SCADE} with the same out-of-domain prior (trained on Taskonomy) outperforms the baselines on in-the-wild captured data, which demonstrates its robustness. Figure~\ref{fig:in_the_wild} shows qualitative examples. Interestingly, we are able to recover the room behind the glass better the baselines, whose results are more blurry (first, third and fourth column). We are also able to recover objects behind the glass such as the monitor and chair (first column). As discussed in Sect. \ref{section:sup_ray_term}, non-opaque surfaces are the most challenging because both the ground truth distribution of ray termination distances induced by NeRF and the distribution of estimated depths from the prior must be multimodal. The fact that we do well validates the ability of our model to capture the multimodal distributions. 

Similar to the ScanNet results, we are able to recover crisp shapes of objects such as the thin table leg and the white chair (second and last column), and we are also able to clear up dust near the microwave and printer compared to the baselines (fifth and last column). 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/wild_quali_new.pdf}
    \vspace{-0.3cm}
    \caption{{\bf{Qualitative results on In-the-Wild data}.}}
    \label{fig:in_the_wild}
    \vspace{-0.4cm}
\end{figure*}

\begin{table}
\centering
\setlength\tabcolsep{2pt}
\begin{tabularx}{\linewidth}{c|C|C|C}
    \toprule
    &  PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
    \midrule
    Vanilla NeRF~\cite{mildenhall2020nerf} & 20.46 & 0.713 & 0.398 \\
    DDP~\cite{roessle2022dense} & 21.28 & 0.727 & 0.366 \\
    \midrule
    \textbf{SCADE} (Ours) & \textbf{22.82} & \textbf{0.743} & \textbf{0.347}\\
    \bottomrule
\end{tabularx}
\vspace{-0.2cm}
\caption{\textbf{In-the-wild Results}. }
\label{tbl:wild_results}
\vspace{-1.5\baselineskip} 
\end{table}

% \vspace{-0.2cm}
\subsection{Ablation Study}
% We further conduct ablation studies to validate the effectiveness of the components introduced in SCADE. %\mika{Please help name these ablations - it isn't clear now I think.}\\
% \vspace{-0.3cm}
\paragraph{Supervision on full distribution vs moments} We first validate the importance of using our novel space carving loss that supervises the full distribution of ray termination distance rather than just its moments. The latter integrates along the ray, and so provides one target value for the entire ray, which effectively makes it 2D supervision. The former provides different target values for different samples along the same ray, which makes it 3D supervision. We adapt our method to use 2D supervision proposed in the recent MonoSDF~\cite{yu2022monosdf}, which computes the expected ray termination distance and aligns the output depth map with a monocular depth estimation prior using the MiDaS~\cite{ranftl2020towards} loss. We use our learned prior as the monocular depth estimate. Table~\ref{tbl:ablation} validates the effectiveness of our space carving loss.
%3D coordinates through its samples, in contrast to existing approaches that penalize on expected (rendered 2D) depth. 

\vspace{-0.25cm}
\paragraph{Multimodal prior vs unimodal prior.} Our prior models does not constrain the depth distribution to a particular form and allows it to be multimodal. In contrast, DDP's depth completion prior assumes a Gaussian, which is unimodal. We ablate our prior against DDP's by sampling from their Gaussian distribution. We train with our space carving loss using both a single sample, as well as $M$ samples drawn from DDP's prior. As shown in Table~\ref{tbl:ablation}, while both perform better than the 2D supervision provided by MonoSDF, but they are subpar compared to using our multimodal prior.

\vspace{-0.25cm}
\paragraph{Multiple vs single sample.} Finally, we also ablate on using a single sample vs multiple samples for our space carving loss. For the single sample set-up, we supervise using the sample mean of the depth estimates, which is equivalent to the maximum likelihood estimate of the depth under Gaussian likelihood. Results in Table~\ref{tbl:ablation} show the importance of using multiple samples for our space carving loss.

\begin{table}
\centering
\setlength\tabcolsep{2pt}
\begin{tabularx}{\linewidth}{c|C|C|C}
    \toprule
    &  PSNR $\uparrow$& SSIM $\uparrow$& LPIPS $\downarrow$\\
    \midrule
     MonoSDF supervision & 20.13 & 0.710 & 0.332 \\
     % \midrule
     DDP prior - single sample & 20.85 & 0.712 & 0.320 \\
     DDP prior - multiple samples & 21.00 & 0.718 & 0.316 \\
     % \midrule
     Our prior - single sample & 21.22 & 0.714 & 0.318 \\
    \midrule
    \textbf{SCADE} (Ours) & \textbf{21.54} & \textbf{0.732} & \textbf{0.292}\\
    \bottomrule
\end{tabularx}
\vspace{-0.2cm}
\caption{\textbf{Ablation Study}. 
% 2D refers to taking the expected depth along the ray. 3D refers to our space carving loss through sampling in 3D along the ray. 
Results on the Scannet dataset. MonoSDF supervision refers to supervising with MonoSDF loss on expected ray termination distance using our prior. }
\vspace{-0.4cm}
\label{tbl:ablation}
% \vspace{-1.2\baselineskip} 
\end{table}

\vspace{-0.2cm}
\paragraph{Sparsity.} We further show addt'l results under varying number of views in Tab~\ref{tbl:sparsity_table}. Note that in general sparsity is not fully reflected by the absolute number of views, because all else being equal, a larger scene requires more views to attain complete coverage. 
% PixelNeRF and DS-NeRF consider more constrained camera configurations such as forward-facing and outside-in, where many views can be dropped without losing coverage. 
Following prior work~\cite{roessle2022dense}, we also report the average number of views that see the same point as another measure of sparsity. As shown, our setting is at the edge of the theoretical lower limit of 2 for general-purpose reconstruction, which shows view sparsity.



\begin{table}
{
\footnotesize
\centering
\setlength\tabcolsep{2pt}
\begin{tabularx}{\linewidth}{c|c|c|c|c}
    \toprule
    ave. \#views visible & 1.87 & 1.98 & 2.01 & 2.2 \\
    absolute \#views & 18 & 20 & 22 & 24 \\
    \midrule
    DDP / SCADE & 19.35/ \textbf{21.66} & 22.4/ \textbf{23.67}  &  23.10/ \textbf{24.00} & 23.56/ \textbf{24.84}  \\
    \bottomrule
\end{tabularx}
\vspace{-0.3cm}
\caption{PSNR for DDP~\cite{roessle2022dense}/SCADE on scene781 of Scannet.}
\label{tbl:sparsity_table}
}
\vspace{-0.1cm}
\end{table}

\begin{figure}[t]
\vspace{-2mm}
	\begin{center}
		\includegraphics[width=\linewidth]{figures/ambiguity_discussion.pdf}
	\end{center}
    \vspace{-0.6cm}
    \caption{\textbf{Depth Mode Discussion.} a) Train images from Taskonomy~\cite{zamir2018taskonomy} and their labels. Notice that non-opaque surfaces are labelled differently. b) Output of our prior on reflective surfaces.}
	\vspace{-0.4cm}
	\label{fig:ambiguity-discussion}
\end{figure}

\subsection{Discussion on our Ambiguity-Aware Prior}
\vspace{-0.2cm}
We provide addt'l details on our ambiguity-aware prior on i) how and why it works, ii) its performance on reflective surfaces, which is a common failure case for depth estimate, and iii) provide an intuition on how what our depth modes look like. i) We achieve variable depth modes by exploiting inconsistently labeled training data. As shown in Fig~\ref{fig:ambiguity-discussion}-a, in Taskonomy~\cite{zamir2018taskonomy}, different training images with non-opaque surfaces label depth differently: shooting through the glass (left), on the glass (middle), or a mixture of both (right). Despite multiple possible depth labels, each image only has one ground truth label. Training with cIMLE allows our prior to model these multiple possible (ambiguous) outputs through sampling\footnote{See supplement for more qualitative examples on our ambiguity-aware depth estimates.}, even when given only one label per image. Interestingly when testing our prior on a test image with a mirror, we find that it is able to capture variable modes on reflective surfaces, including the ``correct" flat surface as shown in Fig~\ref{fig:ambiguity-discussion}-b. An intuition of what the depth modes look like is for example if a ray intersects $n-1$ non-opaque surfaces, the $n$ modes are intersections with the non-opaque surfaces and the terminating point on the opaque surface.

