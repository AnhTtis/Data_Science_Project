\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.1cm}
% \subsection{Novel View Synthesis and 3D Reconstruction}
\paragraph{Novel View Synthesis and 3D Reconstruction.}Reconstructing the structure of a scene from a few views is a long standing problem in computer vision with a long literature. Neural Radiance Fields or NeRF~\cite{mildenhall2020nerf} revolutionized the field by showing how to use weights of an MLP to represent a scene that is rendered using volume rendering~\cite{max1995optical}. A key innovation was the use of positional encoding~\cite{tancik2020fourier, vaswani2017attention} to increase the effective capacity of the MLPs that model a emitted radiance and density as a function of position and viewing direction. Extensions of NeRF include works on unconstrained photo collections~\cite{martinbrualla2020nerfw}, dynamic scenes~\cite{li2021neural}, deformable scenes~\cite{park2021nerfies}, and reflective materials~\cite{verbin2022refnerf, bi2020neural}.

A critical shortcoming of NeRF is its reliance on having many input views of a scene. Several approaches have been proposed, including adding patch likelihood losses~\cite{Niemeyer2021Regnerf}, data-driven priors~\cite{yu2021pixelnerf, tancik2020meta}, semantic consistency prior~\cite{Jain_2021_ICCV}, image features~\cite{wang2021ibrnet}, or surface~\cite{Niemeyer2021Regnerf, verbin2022refnerf}, occupancy~\cite{Oechsle2021ICCV}, and depth~\cite{wei2021nerfingmvs, kendall2017uncertainties, roessle2022dense} priors. DS-NeRF~\cite{deng2022depth} uses the sparse point reconstructions recovered during Structure-from-Motion, to supervise the depth of sparse points in the recovered NeRF. In the same spirit, DDP~\cite{roessle2022dense} uses a depth completion network, that takes as input sparse point cloud projected to one an input view, and produces a depth estimate. Both works~\cite{kendall2017uncertainties,roessle2022dense } model depth with uncertainty but only supervise with \emph{moments} of NeRF's depth distribution. 
% DDP uses those estimates as supervision of the density, and also to reduce the need to sample across the whole scene for each ray. 
In contrast, our work is able to represent \emph{multimodal} depth estimates, which can handle the inherit ambiguities of depth estimation, and is able to seek the modes for each depth estimate to make a consistent prediction of the scene structure.

Inspired by traditional multi-view stereo, MVS-NeRF~\cite{mvsnerf} and RC-MVSNet~\cite{chang2022rcmvsnet} incorporate the use cost volumes~\cite{gu2019cas} to NeRF. To construct the cost volumes, these works look for agreement between features to look for correspondences, which is difficult under the setting of large variations in appearance or viewpoint. In contrast, our approach introduces a novel space carving loss that does not rely on feature correspondences and instead directly defines the loss in 3D.

Another line of works focus on geometry reconstruction~\cite{wang2021neus, yariv2021volume, yu2022monosdf} by using the volumetric rendering scheme to learn a neural SDF representation. These works tackle a different problem on modeling geometry reconstruction, unlike NeRFs whose focus is modeling appearance for novel view synthesis, where our work falls under.
% \subsection{Depth Estimation from Single View}
\vspace{-0.5cm}
\paragraph{Depth Estimation from Single View.}Depth estimation from a single image is a complex task due multiple ambiguities, including scene scale and shading / albedo ambiguities. Early efforts used MRFs to compute predictions on superpixels~\cite{saxena2008make3d}. The advent of consumer depth cameras like Kinect~\cite{zhang2012microsoft} enabled the acquisition of larger scale indoor 3D datasets~\cite{silberman2012indoor}, leading to methods that used deep neural networks to predict depth from a single color image~\cite{eigen2014depth}. Nonetheless, scaling datasets for depth estimation remained a challenge. Deep3D~\cite{xie2016deep3d} enabled the creation of stereo views by training on a large dataset of stereo movies, while~\cite{godard2017unsupervised} used the self-supervision of left-right consistency to learn depth from stereo views from driving cars. This approach was extended to videos where the egomotion is also estimated and self-supervision happens across time~\cite{zhou2017unsupervised}. MegaDepth~\cite{li2018megadepth} uses the depth from SfM reconstructions of internet photos, together with semantic segmentation that conveys ordinal depth supervision cues, i.e. transient objects must be in front of the static scenes. Most recent, multi-task learning has shown promise in training depth estimators that work well in out-of-domain data~\cite{ranftl2020towards}.

In some cases, surface normal estimation has fewer pitfalls than direct depth estimation, as it suffers somewhat less from scale ambiguities. GeoNet~\cite{qi2018geonet} jointly estimates depth and surface normals, then uses the geometric relation between them to refine the estimates. 
LeRes~\cite{yin2022towards} uses a second geometry reasoning module that refines a focal length estimate and the depth itself. Our depth estimation approach is derived from LeReS without the second stage, as the focal length is already estimated during SfM for NeRF scenes.

Most relevant is the work of Kendall and Gal~\cite{kendall2017uncertainties}, that learns depth estimation in a Bayesian setting, where their objective maximizes the likelihood under a predicted distribution. In our work, we use instead a multimodal depth estimation technique built on cIMLE~\cite{Li2020MultimodalIS}, which makes our prior robust to ambiguities in depth prediction.


