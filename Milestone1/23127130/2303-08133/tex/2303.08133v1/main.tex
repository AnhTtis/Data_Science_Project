
\documentclass{article} % For LaTeX2e
\usepackage[numbers,sort&compress,square]{natbib}
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{linkcolor=[rgb]{0.7,0.1,0.1}}
\hypersetup{citecolor=[rgb]{0.4,0.15,0.95}}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{epsfig}
\usepackage{graphicx}


\usepackage{caption}
\renewcommand{\captionlabelfont}{\footnotesize}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{multirow}

\usepackage{pifont}

\usepackage{tocloft}
%\usepackage{todonotes}
\usepackage[toc,page,header]{appendix}
%%%%% for adjustbox
\usepackage{adjustbox}

\usepackage{minitoc}
\renewcommand{\ptifont}{\large \bf}
% Make the "Part I" text invisible
\renewcommand \thepart{}
\renewcommand \partname{}

\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.94}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}

\newcommand{\zl}[1]{{\color{red}{\bf\sf [Zhen: #1]}}}
\newcommand{\lp}[1]{{\color{green}{\bf\sf [Liam: #1]}}}
\newcommand{\wy}[1]{{\color{cyan}{\bf\sf [Weiyang: #1]}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\expt}[0]{\mathop{\mathbb{E}}}
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}

\title{\vspace{-6mm}\underline{MeshDiffusion}:\\[.5mm]Score-based Generative 3D Mesh Modeling}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
\iclrfinalcopy
% Non-anonymous submissions will be rejected without review.

\author{\fontsize{9.5pt}{\baselineskip}\selectfont Zhen Liu\textsuperscript{1,2\thanks{Work done partially during an internship at Max Planck Institute for Intelligent Systems.}}, Yao Feng\textsuperscript{2,3}, Michael J. Black\textsuperscript{2}, Derek Nowrouzezahrai\textsuperscript{4}, Liam Paull\textsuperscript{1}, Weiyang Liu\textsuperscript{2,5}\\[0.3mm]
\textsuperscript{1}Mila, Université de Montréal~~~~~\textsuperscript{2}Max Planck Institute for Intelligent Systems - T\"ubingen\\[0.1mm]
\textsuperscript{3}ETH Zürich~~~~~\textsuperscript{4}McGill University~~~~~\textsuperscript{5}University of Cambridge
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents
\maketitle

{
\vspace{-8mm}
\begin{center}
         %\hsize\textwidth
         %\linewidth\hsize
         \fontsize{9.5pt}{\baselineskip}\selectfont
         {Project Page:}~\tt\href{https://meshdiffusion.github.io/}{meshdiffusion.github.io}
         \vskip 0.25in
\end{center}
}

\begin{figure}[H]
    \centering
    \vspace{-0.2in} 
    \includegraphics[width=1\linewidth]{newfig/mesh_teaser_v6.pdf}
    \vspace{-0.15in} 
    \caption{\footnotesize (a) Unconditionally generated 3D mesh samples randomly selected from the proposed \textit{MeshDiffusion}, a simple diffusion model trained on a direct parametrization of 3D meshes without bells and whistles. (b) 3D mesh samples generated by \textit{MeshDiffusion} with text-conditioned textures from \cite{richardson2023texture}. \textit{MeshDiffusion} produces highly realistic and fine-grained geometric details while being easy and stable to train.}
    \label{fig:teaser}
\end{figure}


\begin{abstract}
\vspace{-0.5mm}
We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation.
Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. 
To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.

\end{abstract}

\vspace{-1.2mm}
\section{Introduction}
\vspace{-1.2mm}

As one of the most challenging tasks in computer vision and graphics, generative modeling of high-quality 3D shapes is of great significance in many applications such as virtual reality and metaverse~\cite{dionisio20133d}. Traditional methods for generative 3D shape modeling are usually built upon representations of voxels~\cite{wu2016learning} or point clouds~\cite{achlioptas2018learning}, mostly because ground truth data of these representations are relatively easy to obtain and also convenient to process. Both representations, however, do not produce fine-level surface geometry and therefore cannot be used for photorealistic rendering of shapes of different materials in different lighting conditions. And despite being convenient to process for computers, both voxels and point clouds are relatively hard for artists to edit, especially when the generated 3D shapes are complex and of low quality. Moreover, modern graphics pipelines are built and optimized for explicit geometry representations like meshes, making meshes one of the most desirable final 3D shape representations.
While it is still possible to use methods like Poisson reconstruction to obtain surfaces from voxels and points clouds, the resulted surfaces are generally noisy and contain many topological artifacts, even with carefully tuned hyperparameters.

To improve the representation flexibility, sign distance fields (SDFs) have been adopted to model shape surfaces, which enables us to use marching cubes~\cite{lorensen1987marching} to extract the zero-surfaces and thus 3D meshes. 
However, SDFs are typically harder to learn as it requires a carefully designed sampling strategy and regularization. Because SDFs are usually parameterized with multi-layer perceptrons (MLPs) in which a smoothness prior is implicitly embedded, the generated shapes tend to be so smooth that sharp edges and important (and potentially semantic) details are lost. Moreover, SDFs are costly to render and therefore less suitable for downstream tasks like conditional generation with RGB images, which require an efficient differentiable renderer during inference.

We instead aim to generate 3D shapes by directly producing 3D meshes, where surfaces are represented as a graph of triangular or polygon faces. With 3D meshes, all local surface information is completely included in the mesh vertices (along with the vertex connectivity), because the surface normal of any point on the shape surface is simply a nearest neighbor or some local linear combination of vertex normals. Such a regular structure with rich geometric details enables us to better model the data distribution and learn generative models that are more geometry-aware. In light of recent advances in score-based generative modeling~\cite{song2020score,ho2020denoising} where powerful generative performance and effortless training are demonstrated, we propose to train diffusion models on these vertices to generate meshes.
However, it is by no means a trivial task and poses two critical problems: (1) the numbers of vertices and faces are indefinite for general object categories, and (2) the underlying topology varies wildly and edges have to be generated at the same time.

A natural solution is to enclose meshes with another structure such that the space of mesh topology and spatial configuration is constrained. One common approach is to discretize the 3D space and encapsulate each mesh in a tiny cell, and it is proven useful in simulation~\cite{pfaff2020learning} and human surface modeling~\cite{onizuka2020tetratsdf}. Observing that this sort of mesh modeling is viable in recent differentiable geometry modeling literature~\cite{remelli2020meshsdf, shen2021dmtet, munkberg2021nvdiffrec}, we propose to train diffusion models on a discretized and uniform tetrahedral grid structure which parameterizes a small yet representative family of meshes. 
With such a grid representation, topological change is subsumed into the SDF values and the inputs to the diffusion model now assume a fixed and identical size. More importantly, since SDF values are now independent scalars instead of the outputs of an MLP, the parameterized shapes are no longer biased towards smooth surfaces.
Indeed, by such an explicit probabilistic modeling we introduce an explicit geometric prior into shape generation, because a score matching loss of diffusion models on the grid vertices has direct and simple correspondence to the vertex positions of triangular mesh.

We demonstrate that our method, dubbed \textit{MeshDiffusion}, is able to produce high-quality meshes and enables conditional generation with a differentiable renderer. MeshDiffusion is also very stable to train without bells and whistles. We validate the superiority of the visual quality of our generated samples qualitatively with different rendered views and quantitatively by proxy metrics. We further conduct ablation studies to show that our design choices are necessary and well suited for the task of 3D mesh generation. Our contributions are summarized below:

\begin{itemize}[leftmargin=*,nosep]
\setlength\itemsep{0.28em}
    \item To our knowledge, we are the first to apply diffusion model for unconditionally generating 3D high-quality meshes and to show that diffusion models are well suited for 3D geometry.
    \item Taking advantage of the deformable tetrahedral grid parametrization of 3D mesh shapes, we propose a simple and effortless way to train a diffusion model to generate 3D meshes.
    \item We qualitatively and quantitatively demonstrate the superiority of MeshDiffusion on different tasks, including (1) unconditional generation, (2) conditional generation and (3) interpolation.
\end{itemize}

\vspace{-1.7mm}
\section{Related Work}
\vspace{-1.5mm}

\textbf{3D Shape Generation}. 3D shape generation is commonly done by using generative models on voxels \citep{wu2016learning} and point clouds \citep{yang2019pointflow}, due to their simplicity and accessibility. These resulted (often noisy) voxels or point clouds, however, do not explicitly encode surface information and therefore has to be processed with surface reconstruction methods in order to be used in applications like relighting and simulation. Recent advances in implicit representations \citep{mildenhall2020nerf} lead to a series of generative models on neural fields \citep{kosiorek2021nerf, schwarz2020graf, chan2021pi, chan2022efficient}. Based on these representations, one can learn 3D shapes directly from 2D images \citep{schwarz2020graf, chan2021pi, chan2022efficient} with differentiable rendering in the generator. Most of these implicit representations, despite representing shapes in a more photorealistic way, require additional post-processing steps to extract explicit meshes, and are often more time-consuming to render images. If the underlying topology is known, one may directly generate the vertices of a mesh \cite{ma2020learning, tan2018variational}. \cite{nash2020polygen} extends this approach to the topology-varying case with autoregressive models, sequentially generating vertices and edges, but it is hardly scalable and yields unsatisfactory results on complex geometry. A batch of concurrent work propose similar solutions to mesh generation, including: GET3D \cite{gao2022get3d} which uses StyleGAN \cite{Karras2020ada} with a differentiable renderer on tetrahedral grid representations and learns to generate 3D meshes from 2D RGB images, TetGAN \cite{gao2022tetgan} which trains generative adversarial networks~(GANs) on tetrahedral grids and LION \cite{zeng2022lion} which uses a trained Shape-As-Points \cite{Peng2021SAP} network to build meshes from diffusion-model-generated latent point clouds.

\textbf{Mesh Reconstruction}.
Reconstructing 3D meshes of generic objects is challenging and often ill-posed due to its highly complex structures~\cite{wu2020unsupervised,liu2022structural,wang2018pixel2mesh,kanazawa2018learning,kato2018neural}. One often resorts to some non-mesh intermediate representations that are easy to process, and then transforms them back to meshes with mesh reconstruction methods. One of the most popular methods is marching cubes~\citep{lorensen1987marching} which assumes that a surface is represented by the zero level set of some continuous field, and this continuous field can be well approximated by linear interpolation of discrete grid points. It is possible to avoid implicit representations and directly construct meshes. For instance, by assuming the points lie on a surface, one can build triangular meshes by connecting these points, a process known as Delaunay triangulation \citep{boissonnat1984geometric, gopi2000surface, dey2001delaunay, kolluri2004spectral}. It is relatively rare because of its strong assumption on data. With known mesh topology, one can deform a mesh template to fit a given representation. This approach can be easily used to fit multiview images of shapes with known topology (\eg, human faces) using a differentiable renderer \citep{li2021topologically}. It is recently shown that we are able to parametrize meshes of varying topology~\citep{remelli2020meshsdf, shen2021dmtet} and optimize them using differentiable renderers~\citep{munkberg2021nvdiffrec}, which our work leverages.


\textbf{Score-based Generative Models}. Recent years have witnessed a surge in modeling data distributions with score-based models~\citep{song2020score, song2020sliced, ho2020denoising}, which parameterizes the logarithm of the gradient of the probability, known as the score function, rather than the probability directly. Different from energy-based models, the often intractable normalization constant can be avoided and therefore training can be performed by simply matching the score function. It has been shown that, by using multi-scale models \citep{ho2020denoising}, U-Net architectures \cite{ronneberger2015u} and denoising score matching \cite{vincent2011connection}, score-based models can perform high-fidelity image generation and inpainting \citep{ho2020denoising, song2020score, rombach2022high}.

\vspace{-2.1mm}
\section{Preliminaries}
\vspace{-2mm}

\textbf{Deep marching tetrahedra} (DMTet) \citep{shen2021dmtet} is a method to parametrize and optimize meshes of arbitrary topology in a differentiable way. The 3D space is discretized with a deformable tetrahedral grid, in which each vertex possesses a SDF value. The SDF of each 3D position in the space is computed by marching tetrahedra \citep{doi1991efficient}, which assumes SDF values to be barycentric interpolation of the SDF values of the vertices of the enclosing tetrahedra. More specifically, for a point $x_q$ inside a tetrahedron of vertices $x_1, x_2, x_3, x_4$ with SDF values being $s_1, s_2, s_3, s_4$, we obtain its unique barycentric coordinate $(a_1, a_2, a_3, a_4)$ such that $\thickmuskip=2mu \medmuskip=2mu x_q = \sum_i a_i x_i$ ($\thickmuskip=2mu \medmuskip=2mu a_i \in [0, 1]$). The SDF value $s_q$ of $x_q$ is then computed as $\thickmuskip=2mu \medmuskip=2mu s_q = \sum_i a_i s_i$. As a result, if there exists a triangular mesh in the tetrahedron (\ie, $s_1, s_2, s_3, s_4$ are not of the same sign), then we can know that the triangular mesh is exactly the zero surface in the tetrahedron. The triangular mesh vertex $v_p$ on a tetrahedron edge $(v_a, v_b)$ is therefore computed by $\thickmuskip=2mu \medmuskip=2mu v_p = {(v_a s_b - v_b s_a)}/{(s_b - s_a)}$. The advantage of using DMTet, compared to variants of deep marching cubes \citep{liao2018deep, remelli2020meshsdf} which rely on the marching cubes algorithm \citep{lorensen1987marching}, is that the grids are deformable and thus more capable of capturing some finer geometric details like very thin handles of chairs. While the cubic grid used for deep marching cubes can be deformable as well, the deformed cubes are much worse objects to deal with, compared to tetrahedra which will remain tetrahedra after deformation.
Notably, DMTet can be fitted with a differentiable renderer by jointly optimizing for the geometry, the texture and the lighting of a 3D shape given multi-view RGB images within a reasonable amount of time and memory budget \citep{munkberg2021nvdiffrec}.

\textbf{Diffusion models} are a family of score-based generative models which learn and infer with the ground-truth time-dependent score function defined by a forward diffusion process~\citep{ho2020denoising}. Under the stochastic differential equation (SDE) framework proposed in \citep{song2020score}, diffusion models construct the probability of each point in a space by diffusing all data points $x_0 \in \mathcal{D}$ to a target distribution at time $T$ (normally $p_T(x_T) = \mathcal{N}(x_T; 0, I)$) in a time-variant manner: $\log p_t(x_t | \mathcal{D}) = \expt_{x_0 \in \mathcal{D}}[\log p_t(x_t | x_0)]$,
where $p_t(x_t | x_0)$ is modeled by the following Itô process, with $w(t)$ being a standard Wiener process: $dx = F(x,t)dt + G(t)dw$, in which $F(x, t)$ and $G(x, t)$ are some predefined functions. The training objective is to approximate the score function $\nabla_x \log p_t(x_t | x_0)$ with a neural network denoted by $s_\theta(x_t, t)$, where $\lambda(t)$ is a scalar weight subject to the model design:
\begin{equation}
\footnotesize
    L(\theta ; \mathcal{D}) = \expt_{t \in [T], x_0 \in \mathcal{D}} \lambda(t) \norm{s_\theta(x_t, t) - \nabla_x \log p_t(x_t | x_0)}_2^2.
\end{equation}
With the trained score function predictor, samples can be generated by solving the reverse SDE \citep{song2020score} with the initial distribution $p_T(x_T)$ using solvers like Euler-Maruyama method \citep{kloeden2011numerical}:
\begin{equation}
\footnotesize
    dx = \big(F(x,t) - G^2(x) \nabla_x \log p_t(x_t | x_0)\big) dt + G(x)dw.
\end{equation}
A popular diffusion model DDPM \citep{ho2020denoising}, which utilizes a discrete diffusion process $p(x_t | x_{t-1}) = \mathcal{N}(x_t; (1 - \beta_t) x_{t-1}, \beta_t I)$, can be formulated within this SDE framework as \citep{song2020score}: $dx = -\beta_t x dt + \sqrt{\beta_t} dw$. The parameters $\beta_t$ are chosen such that $p_T(x_T)$ is close to a standard Gaussian distribution.

\begin{figure}[t]
    \centering
    \vspace{-0.1in} 
    \includegraphics[width=1\linewidth]{newfig/meshdiffusion_pipeline_v3.pdf}
    \vspace{-0.15in} 
    \caption{\footnotesize Overview of the proposed MeshDiffusion model.}
    \label{fig:pipeline}
    \vspace{-2mm}
\end{figure}

\vspace{-1.2mm}
\section{{\textit{{MeshDiffusion}}}: Diffusion Model on Meshes}
\vspace{-1.2mm}

With DMTet, 3D meshes can be approximated and parameterized with tetrahedral grids. Moreover, by initializing the tetrahedral grid using body-centered cubic (BCC) tiling \citep{labelle2007isosurface} and treating the deformation of each vertex only as an attribute, we obtain a uniform and almost-regular tetrahedral grid (with fewer vertex degrees on the boundary) in which 3D translational symmetry is preserved. Such nice properties enable us to easily train a diffusion model for generating 3D meshes. From now on, we always assume that the undeformed tetrahedral grid is \textit{uniformly} initialized and predict the deformation of tetrahedral grid vertices from their initial positions. As a result, our model takes as input a uniform tetrahedral grid with 4-dimensional attributes (specifically, 3 dimensions are for deformation and 1 dimension is for SDF). These structural priors inspire us to use 3D convolutions.

\vspace{-1.2mm}
\subsection{3D Convolutional Networks for Deformable Tetrahedral Grids}
\vspace{-1.2mm}

While it is natural to use graph neural networks (GNNs) for diffusion models on tetrahedral grids, we argue that it is better to use convolutional neural networks~(CNNs) which generally have better model capacity and contextual information than GNNs due to the embedded spatial priors in the convolution operators. Specifically, the set of vertex positions of a \textit{uniform} tetrahedral grid, created in the way described in \citep{labelle2007isosurface}, is a subset of the vertex position set of a uniform cubic lattice. A standard voxel-like representation can then be easily created by infilling the ``missing'' sites of a tetrahedral grid (illustrated in Figure~\ref{fig:pipeline}). 
We follow the common practices in diffusion models \citep{song2020score} and use a 3D U-Net for the score function predictor. Detailed architectures and settings are in the appendix.

\vspace{-1.2mm}
\subsection{Training Objective}
\vspace{-1.2mm}

\label{sec:train_obj}

Since the tetrahedral grid representation of each object in the datasets is not given, our training objective can be formulated by the following constrained optimization:
\begin{equation}
\footnotesize
\begin{aligned}
    &\min_{\theta} \expt_{\substack{t \in \text{Cat}(\{1, ..., T\}), y_0 \in \mathcal{D} \\ x_t \sim p(x_t | x_0 = g_{\phi^*}(y_0))}} \lambda(t) \norm{s_\theta(x_t, t) - \nabla_x \log p_t(x_t | g_{\phi^*}(y_0))}_2^2\\
    &~~~~~~~~~~~~~~~~~~~~~~~\text{s.t.}~~~\phi^*=\arg\min_{\phi}L_{\text{Render}}\big(y_0, g_\phi(y_0)\big),
    % \nonumber
\end{aligned}
\end{equation}
in which $\mathcal{D}$ is the 3D shape dataset, $y_0 \sim \mathcal{D}$ is the set of 2D views of an object sampled from $\mathcal{D}$, $t$ is uniformly sampled from $\{1, ..., T\}$, $s_\theta(x, t)$ is the score function approximator parameterized by a neural network $\theta$, $g_\phi(y_0)$ is the mapping from 2D views of $y_0$ to its tetrahedral grid representation and $L_{\text{Render}}$ is the rendering (and relevant regularization) loss used in \citep{munkberg2021nvdiffrec}.


\algrenewcommand\algorithmicindent{0.5em}%
\begin{figure}[t]
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Training and Inference} \label{alg:training}
  \small
  \begin{algorithmic}[1]
    \Statex \textbf{Training:}
    \State Fitting $x'$ for each $y \in \mathcal{D}$, and normalize the SDF values to $\pm 1$, resulting the dataset $(x', y) \in \mathcal{D'}$.
    \State Fitting $x$ for each $(x', y) \in \mathcal{D'}$ by conditioning the SDF values on those of $x'$.
    \State Train a diffusion model $s_\theta(x, t)$ on $\mathcal{D'}$ by treating the normalized SDF values as float numbers.
    \Statex \textbf{Inference:}
    \setcounter{ALG@line}{0}
    \State Obtain $x_0$ by solving the reverse SDE with initial distribution $q(x_T)$ and normalize the SDF values of $x_0$ to $\pm 1$.
    \State (Optionally) Regenerate $x_0$ by conditioning on the previously generated (and normalized) SDF values.
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Conditional Generation} \label{alg:cond_gen}
  \small
  \begin{algorithmic}[1]
    \vspace{.13in}
    \State Randomly initialize and fit a deformable tetrahedral grid using the given RGBD view.
    \State Using rasterization, find all occluded tetrahedra of which we mask all tetrahedral vertices. 
    \State Re-initialize the masked tetrahedral vertices and run the completion algorithm using the trained diffusion model.
    \State (Optionally) Finetune the unmasked tetrahedral vertices as well near the end of the completion process.
    \vspace{.04in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure}


While in theory it is possible to train $g_\phi$ such that $s_\theta$ can be better learned, we instead take the simplest two-stage and non-amortized approach as detailed optimization and encoder architecture design to use is beyond our scope. 
Specifically, we first solve for the constraint $L_{\text{Render}}$ and create a dataset $\mathcal{D}_{x}$ of tetrahedral grids by fitting a tetrahedral grid $x_0$ for each $y_0 \sim \mathcal{D}$ in a non-amortized way. The second stage is simply to optimize for the main objective, \ie, a $L_2$ denoising loss of the diffusion model, on this generated dataset $\mathcal{D}_{x}$. Despite simplicity, we find that this simple procedure is very stable and yields reasonably good performance.

In principle, the first data fitting stage can be trained with multiview RGB images by following the procedure in \citep{munkberg2021nvdiffrec}. However, we notice that with RGB images only, it fails to learn some complex geometries, especially when surface materials are highly specular. To demonstrate our idea while not delving too deep into the rendering details, we instead assume that we have access to 3D mesh datasets (in our experiments, ShapeNet datasets \citep{chang2015shapenet}) so that RGBD images can be rendered with random lighting and simple surface materials. More specifically, we use a single default material with diffuse components only for all ground truth meshes, and render multiview RGBD images with some known but randomly rotated environment light (represented as a cubemap \citep{shirley2009fundamentals}). With the additional depth information, our reconstruction objective is $ L_{\text{Render}} = \alpha_{\text{image}}L_\text{image} + \alpha_{\text{depth}}L_\text{depth} + \alpha_{\text{chamfer}} L_{\text{chamfer}} + \alpha_{\text{penalty}} L_{\text{penalty}}$,
in which $L_\text{image}$ and $L_\text{depth}$ are the image loss (RGB and silhouette) and the depth loss between the rendered views and the ground truth views, respectively; $L_\text{chamfer}$ is the Chamfer distance \citep{achlioptas2018learning} between a sampled point cloud from the predicted mesh and one from the ground truth mesh; $L_\text{penalty}$ is the regularization terms to move reconstructed meshes out from bad local minima. Details of these losses are included in the appendix.

In order to remove absolute scales from SDFs (Section~\ref{subsec:sdf_discrete}), we perform the optimization twice, where during the second pass we fix the SDFs to be the signs (\ie, $\pm 1$) of the resulted SDF values from the first pass. This can effectively improve the results.
\vspace{-1.2mm}
\subsection{Reducing Noise-Sensitivity of Marching Tetrahedra}
\vspace{-1.2mm}
\label{subsec:sdf_discrete}

It is tempting to train diffusion models on arbitrary $\mathcal{D}_{x}$, the dataset of fitted tetrahedral grids of objects, with low $L_{\text{Render}}$. However, the naively trained models generate highly non-smooth surfaces (Figure~\ref{fig:wo_discrete}) due to a mismatch between the $L_2$ denoising objective on the 4-dimensional inputs (deformation and SDF) and the ideal $L_2$ reconstruction objective on triangular mesh vertex positions. Recall that the triangular mesh vertex position $v_p$ on a single tetrahedron edge $e = (a, b)$ is computed by linear interpolation $v_p = {(v_a s_b - v_b s_a)}/{(s_b - s_a)}$, in which $v_a, v_b$ are the positions of tetrahedron vertices $a$ and $b$, and $s_a, s_b$ are the corresponding SDF values. With perfectly fitted $v_a, v_b$ and $s_b$ but a small amount of noise $\epsilon$ on both $s_a$ and $s_b$, the triangular mesh vertex is subject to a perturbation that is inversely proportional to $s_b - s_a$, which may incur numerical instability:
\begin{equation}
\footnotesize
    v_{p, \text{noisy}} = \frac{v_a (s_b + \epsilon) - v_b (s_a + \epsilon)}{(s_b + \epsilon) - (s_a + \epsilon)} = v_p + \frac{v_a - v_b}{s_b - s_a} \epsilon.
\end{equation}
An arbitrarily fitted dataset $\mathcal{D}_{x}$ does not provide any guarantee on the scale of $|s_b - s_a|$ and clearly $\norm{v_{p, \text{noisy}} - v_p}$ can be unbounded in the worst case, even when the $L_2$ denoising loss is low on $\mathcal{D}_{x}$. 

A similar issue arises when we consider the topological changes due to a small noise in the SDF values. For illustration, we consider the case where some tetrahedral vertex $s_a$ with $-C < s_a < 0$ ($C \ll 1$ is a tiny positive scalar), and all its neighboring vertices possess SDF value of 1. With a small noise on $s_a$, there can be a high probability of local topological changes (\eg, an undesirable hole) in the resulted 3D meshes even when the $L_2$ denoising loss is reasonably small.

Due to these undesired behaviors, we propose to normalize the SDF values and adopt the two-pass optimization scheme described in Section~\ref{sec:train_obj}, such that we can ensure $s_a, s_b \in \{+1, -1\}$ and $| s_b - s_a | = 2$ for all mesh-generating tetrahedron edges.


\setlength{\columnsep}{13pt}
\begin{wrapfigure}{r}{0.295\textwidth}
  \begin{center}
  \advance\leftskip+1mm
  \renewcommand{\captionlabelfont}{\footnotesize}
    \vspace{-0.28in}  
    \includegraphics[width=0.268\textwidth]{newfig/wo_discrete_v2.pdf}
    \vspace{-0.05in} 
    \caption{\footnotesize Undesirable artifacts produced by na\"ive DDPM due to the sensitivity of marching tetrahedra to SDF noises. Better viewed by zooming in.}
    \label{fig:wo_discrete}
    \vspace{-0.2in} 
  \end{center}
\end{wrapfigure}


With normalized SDFs values and continuous deformation vectors, it is natural train a hybrid diffusion model in which the SDFs are modeled like in D3PM \citep{austin2021structured} since normalized SDFs take discrete values of $\pm 1$. However, we empirically find that it suffices to simply treat the normalized SDFs as float numbers and train a standard Gaussian diffusion model. The inference thus requires a final normalization operation to round SDFs to $\pm 1$ according to their signs.

Because the generative model is trained under the assumption that SDF values are un-normalized, a further refinement step can be applied to improve visual quality. Specifically, we simply set the generated and normalized SDFs as the conditional inputs and then run a conditional generation step on the deformation vectors only.

\begin{figure}[t]
    \centering
    % \vspace{-0.2in} 
    \includegraphics[width=1.0\linewidth]{newfig/model_comp_v3.pdf}
    \vspace{-0.15in} 
    \caption{\footnotesize Qualitative comparison among different generative models. All the samples are randomly selected.}
    \label{fig:model_comparison}
    \vspace{-2mm}
\end{figure}

\vspace{-1.2mm}
\subsection{Conditional Generation with Single-view Images}
\vspace{-1.2mm}

In many cases, we would like to generate 3D objects given a (possibly partial) single view RGBD image. 
Such a task is simple with MeshDiffusion by a two-stage process: first to fit a tetrahedral grid with the given single RGBD view and second to use our MeshDiffusion model to (stochastically) correct the wrong parts.
We follow the procedure shown in Algorithm~\ref{alg:cond_gen}. For the conditional inference process, we consider the simplest conditional generation method~\citep{ho2020denoising} (also called replacement method in \citep{ho2022video}). Specifically, we sample $\hat{x}_{t-1}$ from $x_t$ and replace $\hat{x}^b_{t-1}$ by $p_{t-1}(x^b_{t-1} | x^a_0)$ which is defined by the forward diffusion. More details are included in the appendix.



\input{tables/eval-pc-metrics}

\vspace{-1.75mm}
\section{Experiments and Results}
\vspace{-1.75mm}
\subsection{General Settings}
\vspace{-1.25mm}
\textbf{Data Fitting}. We fit tetrahedral grids on five ShapeNet subcategories: Chair, Airplane, Car, Rifle and Table, the same set of categories used in \citep{zheng2022sdfstylegan, chen2019learning}. Fitting each object takes roughly 20-30 minutes on a single Quadro RTX 6000 GPU. We use the same train/test split in \citep{zheng2022sdfstylegan}. The detailed architecture, training and hyperparameter settings are explained in the appendix. 

\textbf{Generative Model}.
To show the effectiveness and universality of our approach of training diffusion models on 3D meshes, for the U-Net architecture we take an extremely approach by simply replacing the 2D convolution and other 2D modules in the U-Net architecture used by \citep{song2020score} with their 3D counterparts. To improve model capacity, we provide to the model as a conditional input a mask indicating which lattice sites are artificially introduced. We slightly increase the depth of the network due to the larger input size. We do not tune the hyperparameters of diffusion SDEs but instead use the same set of hyperparameters as described in \citep{song2020score}, which also validates our MeshDiffusion is easy to train. We train the discrete-time category-specific diffusion models for all datasets for total 90k iterations with batch size 48 on 8 A100-80GB GPUs. The training process typically takes 2 to 3 days.

For ablation on alternative architectures, we train GANs on our datasets with an architecture similar to the one used in \citep{wu2016learning}. Our SDF-based baselines include IM-GAN \citep{chen2019learning} and SDF-StyleGAN \citep{zheng2022sdfstylegan}. We also compare MeshDiffusion against GET3D \citep{gao2022get3d} which also uses DMTet for mesh parametrization.

\vspace{-1.2mm}
\subsection{Unconditional Generation}
\vspace{-1.2mm}

For qualitative comparison, we randomly sample 3D shapes from each of the trained generative models for each dataset. We remove isolated meshes of tiny sizes and then apply both remeshing and the standard Laplace smoothing~\citep{nealen2006laplacian} on all the generated meshes (smoothed with $\lambda = 0.25$ and $5$ optimization steps). We visualize samples produced by MeshDiffusion and the existing state-of-the-art 3D mesh generative models in Figure~\ref{fig:model_comparison}.
We note that MeshDiffusion produces the sharpest samples and preserve the finest geometric details, while pure SDF-based methods tend to be too smooth.
Part of the reason is that these SDF-based methods assume a very smooth interpolation between points, while MeshDiffusion explicitly models the interpolation in a piecewise linear way. 

\subsubsection{Quantitative Evaluation}

\setlength{\columnsep}{11pt}
\begin{wraptable}{r}[0cm]{0pt}
    \scriptsize
	\centering
        \hspace{-2.5mm}
	\setlength{\tabcolsep}{2.6pt}
	\renewcommand{\arraystretch}{1.3}
	\renewcommand{\captionlabelfont}{\footnotesize}
	%\vspace{0.7mm}
\begin{tabular}{lccccc}
\specialrule{0em}{0pt}{-15pt}
  Model & Chair & Airplane & Car & Rifle & Table \\
\shline
IM-GAN \citep{chen2019learning}
    & 64.19 & 74.57 & 141.2 & 103.3 & 51.70 \\
SDF-StyleGAN \citep{zheng2022sdfstylegan}
    & \bf 36.48 & 65.77 & \bf 128.70 & 65.50 & \bf 42.29  \\\rowcolor{Gray}
MeshDiffusion
    & 39.62 & 
    \bf 64.30 & 130.20 & \bf 54.73 & 48.55
    \\
    \specialrule{0em}{0pt}{-6pt}
\end{tabular}
\caption{\footnotesize FID scores averaged across 24 views.}
\label{table:fid}
\vspace{-2mm}
\end{wraptable}

In Table~\ref{table:eval_pc}, we show various point cloud distances~\cite{achlioptas2018learning, zheng2022sdfstylegan} and light field distance (LFD) \citep{chen2003visual} between the test set and the generated samples by sampling point clouds of size $2048$ from both ground truth and generated meshes. The results consistently show that MeshDiffusion can better capture the geometric details, and therefore achieves better point cloud metrics in most of the cases. For a more detailed description on the metrics and  experimental settings, please refer to the appendix.

We also follow \citep{zheng2022sdfstylegan} and measure Frechet inception distances (FIDs) \citep{brock2018large} on rendered views as a proxy of goodness of surfaces. Same as in \citep{zheng2022sdfstylegan}, the views are rendered with $24$ camera poses uniformly distributed on a sphere; 3 fixed point light sources are used to light the meshes; a gray diffuse material is used for all meshes. The FID score of each view is computed with an ImageNet-pretrained model and averaged to obtain the FID scores for 3D shapes. With suitable hyperparameters for resolution and Laplacian smoothing, our MeshDiffusion is able to achieve competitive scores compared to SDF-StyleGAN. We note, however, the computed FID is not a genuine score for 3D shapes since the distribution of rendered views of these meshes is very different from the distribution of natural RGB images. In addition, the multiview FID score used by SDF-StyleGAN \citep{zheng2022sdfstylegan} assumes flat shading, while it is more natural and common to use other shading methods like Phone shading for smoother images. Results in Table~\ref{table:fid} show that even with such a over-simplified shading, MeshDiffusion still yields very competitive FID scores compared to recent state-of-the-art methods.

Additionally, we perform ablation study on the choices of models in Table~\ref{table:ablation}. It can be observed that the our SDF normalization strategy described in Section~\ref{subsec:sdf_discrete} is indeed beneficial for the diffusion model, and our customized diffusion model is better suited for our mesh generation setting.


\input{tables/ablation}

\begin{wrapfigure}{r}{0.55\linewidth}
\vspace{-10mm}
\centering
\includegraphics[width=0.99\linewidth]{newfig/cond_gen_v4.pdf}
  \vspace{-5mm}
\caption{\footnotesize Conditional generation on a single RGBD view.}
    \label{fig:cond_gen}
\vspace{-5mm}
\end{wrapfigure}

\vspace{-1.2mm}
\subsection{Conditional Generation}
\vspace{-1.2mm}

Our model can generate meshes by conditioning on a single-view RGBD image, as shown in Figure~\ref{fig:cond_gen}. Because the geometry estimated from the single-view fitting is not perfect even in the given single view, we allow the originally-fixed tetrahedral vertices to be slightly updated by the diffusion model near the end of the diffusion completion process (in our experiments, $T = 50$; the inference process starts at $T=1000$ and ends at $T=0$). Results demonstrate that MeshDiffusion can generate plausible and reasonable completion results conditioned on the given view.






\vspace{-1.2mm}
\subsection{Interpolation Results}
\vspace{-1.2mm}

We use DDIM \citep{song2021denoising} to convert the stochastic sampling process into a deterministic one. With DDIM, the initial random Gaussian noise $x_T$ is treated as the ``latent code'' of the generated image, and we run the following sampling process: $x_{t-1} = \frac{\alpha_{t-1}}{\alpha_t} [x_t - (1 - \alpha_{t})s_\theta(x_t, t)] + (1 - \alpha_{t-1}) s_\theta(x_t, t)$. Following the settings in DDIM paper \citep{song2021denoising}, we use spherical interpolation for the latent codes. We visualize some of the interpolation sequences in Figure~\ref{fig:interpolation_chair}. We set the number of inference steps to $100$ for faster inference and use quadratic time spacing as described in \citep{song2021denoising}.

\vspace{-1.2mm}
\subsection{Text-conditioned Texture Generation}
\vspace{-1.2mm}

We show in Figure~\ref{fig:teaser} and Figure~\ref{fig:more_textured_meshes} that the 3D meshes generated by MeshDiffusion can be easily painted with some texture generation methods. In our experiment, we use a recent work -- TEXTure~\cite{richardson2023texture} to generate text-conditioned textures on generated raw 3D meshes. As can be observed from the results, MeshDiffusion along with TEXTure can produce fairly realistic and reasonable textured 3D mesh models. With more advanced texture generation methods, we believe that the advantage of MeshDiffusion's high-quality 3D geometry can be better demonstrated.



% \begin{wrapfigure}{r}{0.7\linewidth}
\begin{figure}[t]
\vspace{-0.5mm}
\centering
\includegraphics[width=0.99\linewidth]{newfig/more_texture.pdf}
    \vspace{-0.05in}
    \caption{\footnotesize More examples of text-conditioned textures synthesized by \cite{richardson2023texture} on our generated meshes.}
    \label{fig:more_textured_meshes}
\vspace{-2mm}
% \end{wrapfigure}
\end{figure}





\vspace{-1.5mm}
\section{Discussions}
\vspace{-1.5mm}

\textbf{Optimization issues with DMTet.} While DMTet is capable of fitting geometries, it fails in cases where the underlying topology is complex. Besides, it is not encouraged to learn the true topology of shapes and may produce invisible topological holes by contracting the neighboring triangular mesh vertices close enough. Furthermore, we observe that the optimization process with differentiable rasterization of 3D meshes may produce floating and isolated meshes, especially when depth supervision is introduced. It is therefore worth designing better optimization techniques, regularization methods and possibly better parametrization of meshes for the purpose of training mesh diffusion models.

\begin{wrapfigure}{r}{0.55\linewidth}
\vspace{-3mm}
\centering
\includegraphics[width=0.99\linewidth]{newfig/interpolation_chair_v3.pdf}
    \vspace{-0.16in} 
    \caption{\footnotesize Some examples of our interpolation results.}
    \label{fig:interpolation_chair}
\vspace{-3mm}
\end{wrapfigure}

\textbf{Diffusion model design.} Our experiments demonstrate the effectiveness of the simple design with few hyperparameter and architecture changes: a 3D-CNN-based U-Net on augmented cubic grids with DDPM. We note that it is straightforward to utilize advanced diffusion models and switch to more memory-efficient architectures. Especially, since it is possible to regularize SDFs with methods such as Lipschitz constraints \citep{liu2022learning}, it is a promising approach to train diffusion models on the latent space produced by a regularized autoencoder, the same strategy adopted in \citep{rombach2022high, zeng2022lion}.



\textbf{Limitations.} 
Diffusion model typically assumes a known dataset in the input modality (augmented tetrahedral grids in our case), but to efficiently train diffusion models on 2D images, we need a better way to amortize the costs and fully leverage the power of differentiable rendering. Our paper avoids this important aspect but instead adopts the two-stage approach of "reconstruction-then-generation". 
Moreover, in our formulation, the differentiable renderer is useful only during the tetrahedral grid creation process, while in principle we believe there can be ways to incorporate the differentiable render in the training and inference process of diffusion models. 
Finally, our diffusion model is built with a very na\"ive architecture, thus limiting the resolution of input tetrahedral grids, while we notice that some of the fine details cannot be fully captured with the current resolution of 64 during the dataset creation stage. With better architecture designs or adaptive resolution techniques (as in \citep{shen2021dmtet}), we may greatly increase the resolution and generate a more diverse set of fine-level geometric details.


\vspace{-1.2mm}
\section{Concluding Remarks}
\vspace{-1.2mm}

We demonstrate that our \textit{MeshDiffusion}, a diffusion model on 3D meshes parameterized by tetrahedral grids, is able to generate fine-details of 3D shapes with arbitrary topology. Such a model is minimally designed but still outperforms the baselines. We demonstrate that, despite being trained and performing inference in the 3D space, can be easily used when only 2.5D information is available. We believe that our model can potentially shed some light on future studies of score-based models for learning and generating high-fidelity shapes. Future work may include but not limited to text-conditioned mesh generation with diffusion models, joint synthesis of texture and geometry with more realistic materials, and motion synthesis and physical simulation on generated meshes. 

\newpage


\section*{Acknowledgement}

We thank Yuliang Xiu, Jinlong Yang, Tim Xiao, Haiwen Feng, Yandong Wen for constructive suggestions. We would like to thank Samsung Electronics Co., Ldt. for funding this research.

\textbf{Disclosure.} MJB has received research gift funds from Adobe, Intel, Nvidia, Meta/Facebook, and Amazon. MJB has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH. While MJB is a part-time employee of Meshcapade, his research was performed solely at, and funded solely by, the Max Planck Society. DN is supported by NSERC Discovery Grant (RGPIN-5011360) and LP is supported by NSERC Discovery Grant (RGPIN-04653).

\vspace{3.2mm}
\textbf{Ethics Statement.} Our model is developed for general 3D mesh generation and is in a very preliminary stage in terms of automatically generating product-quality meshes. Still, our model may potentially be used to generate inappropriate contents if trained on specific datasets.
\vspace{-1.2mm}


{\small
\bibliographystyle{plain}
\bibliography{bib}
}

\newpage
\appendix
\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\renewcommand \thepart{} % make "Part" text invisible
\renewcommand \partname{}
\part{\Large\centerline{Appendix}}
 % Start the appendix part
\parttoc % Insert the appendix TOC


\newpage


\section{Details on Dataset Preparation}


\subsection{Architecture and Losses}

\textbf{RGB and silhouette loss.} By interpolating between a black background image and the binary mask (produced by rasterization to indicate the existence of rasterized meshes on each pixel), we obtain a smoothed binary mask $M$ and $M_{\text{gt}}$, for the fitted meshes and predicted meshes respectively. We then compute the silhouette loss:

\[
    L_{\text{silhouette}} = \norm{M - M_{\text{gt}}}_2^2.
\]

We compute the averaged pixel-wise log-$L_1$ loss between rendered images and ground truth images:

\[
    L_{\text{RGB}} = \mathop{\mathbb{E}}_{x \in \mathcal{D}, Q \in \mathcal{D}_{\text{pose}}} \log (\norm{I_{\text{gt}} \odot M_{\text{gt}} - \text{Render}(x, Q) \odot M})
\]

in which $\mathcal{D}_{\text{pose}}$ is the set of random camera poses which always look at the center of the object, and $x$ is the sampled tetrahedral grid from the dataset $\mathcal{D}$.



\textbf{Depth loss.} We use a mixed loss for depth: when the depth is greater than a threshold (set to $1.0$), we use a $L_2$ loss; otherwise, we switch to $L_1$ loss as we find it producing much better surfaces.

\[
    L_{\text{depth}} = \mathop{\mathbb{E}}_{x \in \mathcal{D}, Q \in \mathcal{D}_{\text{pose}}} \norm{d_{\text{gt}} - \text{Render}_{\text{depth}}(x, Q)}_{\text{mixed}}.
\]

The depth images are computed by barycentric interpolation following rasterization, in which the background is assumed to have a large default depth of $20$, which is much greater than the object depth (constrained in $[-1, 1]$).

To better remove inner artifacts and generate inner structures, we also include the depth loss on the second layer from rasterization, \textit{i.e.}, the second triangular mesh (if any) intersected by each view ray. We weight this second layer depth loss term by $0.1$ as we observe that it can interfere with the first layer depth loss.

The complete depth loss term is thus

\[
    L_{\text{depth, complete}} = \mathop{\mathbb{E}}_{x \in \mathcal{D}, Q \in \mathcal{D}_{\text{pose}}} \Big[\norm{d_{\text{gt}} - \text{Render}_{\text{depth}}(x, Q)}_{\text{mixed}} + 0.1 * \norm{d_{\text{gt, 2nd}} - \text{Render}_{\text{depth}, \text{2nd}}(x, Q)}_{\text{mixed}}\Big].
\]

\textbf{Chamfer loss.} At every iteration, we randomly sample $50,000$ points from both ground truth meshes and predicted meshes and compute the standard Chamfer distance:

\[
    L_{\text{Chamfer}} = \sum_{x \in P_{\text{fitted}}} \min_{y \in P_{\text{gt}}} \norm{x - y}_2^2 + \sum_{y \in P_{\text{gt}}} \min_{x \in P_{\text{fitted}}} \norm{x - y}_2^2.
\]

\textbf{SDF regularization loss.} Following \citep{munkberg2021nvdiffrec}, we use the same $L_2$ regularization loss and penalize the difference of SDFs of two neighboring tetrahedral vertices (\textit{i.e.}, $E_{tet}$ the set of all edges in the tetrahedral grid) as in \citep{munkberg2021nvdiffrec} so that the occluded regions do not produce complex inner geometry. It is also helpful for optimizing tetrahedral grids.

\[
    L_{\text{SDF}} = \sum_{(u, v) \in E_{tet}} \norm{\text{SDF}(u) - \text{SDF}(v)}^2.
\]

\textbf{Hyperparameter setting.} We set $\alpha_{\text{image}} = \alpha_{\text{Chamfer}} = 1.0$ and $\alpha_{\text{depth}} = 100.0$. We set $\alpha_{\text{SDF}}$ to $0.2$ and use a linear decay of the scale towards $0.01$. We use an Adam optimizer for all the parameters with a learning rate of $5e-4$ and $(\beta_1, \beta_2) = (0.9, 0.999)$. We train both reconstruction passes with $5000$ iterations.

\subsection{Implmentation Details}

The initial tetrahedral grid is initialized in a cube $[-1, 1]^3$ by a dense body-centered cubic (BCC) tiling of tetrahedra \citep{labelle2007isosurface} (see \url{https://github.com/crawforddoran/quartet} for code examples). As some tetrahedral generation packages produce additional tetrahedral vertices on the boundary of the cube which breaks translational symmetry (hence detrimental to the performance of 3D CNN), we remove such symmetry-breaking boundaries if any.

We find that using the clipped deformation vectors is better than using the \textit{tanh}-ed deformation vectors as they remove some nonlinearity. And to ensure that the deformation vectors are always differentiable, we clip the deformation vectors only after each gradient update, but not during the forward computational pass.

We set the range of deformation to be three times of that used in \citep{shen2021dmtet} so that the meshes can better capture details in the absence of SDF scales (Section~\ref{subsec:sdf_discrete}), especially when the grid resolution is low. As self-intersecting meshes may appear, it is necessary to perform standard mesh processing operations including remeshing and Laplacian smoothing. For grids of higher resolution (\textit{e.g.}, 128), we instead set the deformation to be 1.5x of hat used in \citep{shen2021dmtet}.

Because we normalize SDFs after the first reconstruction pass, it is more desirable for the underlying shape to be captured mostly by the topology implied by SDF values, not the vertex deformation. To encourage convergence to such solutions, during the first $2000$ iterations, we periodically scale the learned SDF values by $0.4$ for every $300$ iterations.

We observe that depth supervision is not enough to remove some isolated floater meshes in the space. Therefore, we cull these floaters by setting the SDF values of all tetrahedral vertices lying outside the visual hull to positive values (with the assumption that positive values represent ouside-ness). Due to constraints on computational resources, we do not perform culling in a full 3D way, but use rendered shape silhouette to determine floaters visible in the rendered views only.

We notice that some of the meshes in ShapeNet are not watertight while some others have parts which are very thin. Depth supervision on these parts can lead to more topological holes. Therefore, during rasterization we extract the depths (stored in Z-buffers) of the two closet meshes for each pixel and decide if we compute the depth loss on these pixels according to the difference in depth of these two meshes. Specially, depth supervision is not included if the difference is too small or there is only one rasterized mesh (meaning that the ray only intersects with a single mesh).

Motivated by the fact that only SDFs of the mesh-generating tetrahedra (\textit{i.e.}, tetrahedra with vertices of different SDF signs) matter, we set the SDFs of all the non-mesh-generating tetrahedral vertices to $\pm 1$ (signs depending on if vertices inside/outside the shape) to reduce the complexity of the dataset.

For the single-view reconstruction stage in the conditional generation experiments, we observe that the lack of other views leads to many floating artifacts when the target shape consists of a bulk volume. Therefore, during the first $300$ iterations, we periodically (every $10$ iterations) query the nearest neighbor ground truth mesh vertices for every tetrahedral vertex, and determine if the vertex is in front of the camera and not behind the current view (by computing the nearest surface normal, the displacement from the nearest face center and the view direction from camera). If the tetrahedral vertex satisfies the condition, we set its SDF value to $+1$.

\newpage
\section{Details on Mesh Diffusion Models}

\begin{table*}[h]
	\renewcommand{\captionlabelfont}{\footnotesize}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\centering
	\setlength{\abovecaptionskip}{4pt}
	\setlength{\belowcaptionskip}{0pt}
	\scriptsize
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Input \\ \hline
		\textbf{Encoder} \\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 64 \\ \hline
		Pooling, Stride 2\\ \hline
		3 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 64 \\ \hline
		Pooling, Stride 2\\ \hline
		3 $\times$ ResBlocks (with one attention layer in between), \\ 3 $\times$ 3 kernel, width 128 \\ \hline
		Pooling, Stride 2\\ \hline
		3 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
		Pooling, Stride 2\\ \hline
		3 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
            \textbf{FC} \\ \hline
		1 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
            Attention \\ \hline
		1 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
		\textbf{Decoder} \\ \hline
		3 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
		Upsampling, Stride 2\\ \hline
		3 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
		Upsampling, Stride 2\\ \hline
		3 $\times$ ResBlocks (with one attention layer in between), \\ 3 $\times$ 3 kernel, width 128 \\ \hline
		Upsampling, Stride 2\\ \hline
		3 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 64 \\ \hline
		Upsampling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 64 \\ \hline
	\end{tabular}
\quad
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Input \\ \hline
		\textbf{Encoder} \\ \hline
		2 $\times$ ResBlocks, 5 $\times$ 5 kernel, width 128 \\ \hline
		Pooling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 128 \\ \hline
		Pooling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
		Pooling, Stride 2\\ \hline
		2 $\times$ ResBlocks (with one attention layer in between), \\ 3 $\times$ 3 kernel, width 256 \\ \hline
		Pooling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 512 \\ \hline
		Pooling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 512 \\ \hline
            \textbf{FC} \\ \hline
		1 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 512 \\ \hline
            Attention \\ \hline
		1 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 512 \\ \hline
		\textbf{Decoder} \\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 512 \\ \hline
		Upsampling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 512 \\ \hline
		Upsampling, Stride 2\\ \hline
		2 $\times$ ResBlocks (with one attention layer in between), \\ 3 $\times$ 3 kernel, width 256 \\ \hline
		Upsampling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 256 \\ \hline
		Upsampling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 3 $\times$ 3 kernel, width 128 \\ \hline
		Upsampling, Stride 2\\ \hline
		2 $\times$ ResBlocks, 5 $\times$ 5 kernel, width 128 \\ \hline
	\end{tabular}
	\caption{\footnotesize Architecture of the 3D U-Net (Left: resolution 64, Right: resolution 128). The shortcuts from the encoder to the decoder are not shown.}\label{table:diffusion_arch}
\end{table*}


We adapt the network of DDPM in \citep{ho2020denoising} and use a base width of $64$. The encoder in the U-Net is shown in Table~\ref{table:diffusion_arch}, and the decoder follows the same but reverse pattern. For higher resolution grids, we slightly reduce the number of layers in each resolution stage but double the base width.

All the values at the artificially-introduced sites of a cubic lattice are set to zero. We append to the cubic grid in the first and last few layers a binary mask that indicates which vertices are from the tetrahedral grid and which are fake. The score matching loss is also masked accordingly so that the augmented vertices in the predicted cubic grids do not contribute.

To prevent overfitting, we augment the dataset by randomly translate all tetrahedron vertices by the same but tiny amount.

\newpage
\section{Quantitative Metrics for 3D Meshes}

We briefly explain the metrics we used for evaluating the quality of 3D Meshes.

\textbf{Minimum Matching Distance (MMD).} MMD measures the average distance to the nearest neighbor of individual points in one point cloud to another point cloud.

\textbf{Coverage.} Suppose every point cloud in a set $A$ is approximated by its nearest neighbor in another set $B$. Coverage measures the fraction of elements in $B$ which are used to cover $A$ in the nearest neighbor matching sense. Higher the coverage, Better that $A$ is a representative set of $B$.

\textbf{Leave-one-out Accuracy (1-NNA).} As its name indicates, 1-NNA is the average leave-one-out accuracy of the 1-NN classifier fitted on $A$ to classify $B$. It measures if each element in the set $A$ is important in representing another set $B$. A low 1-NNA score means that $A$ well covers $B$.

\textbf{Jensen-Shannon Divergence (JSD).} With JSD, we compute the distance between distribution of ground truth point clouds and that of the generated point clouds: $\text{JSD}(P_A, P_B) = \frac{1}{2} \text{KL}(P_A || M) + \frac{1}{2} \text{KL} (P_B || M)$, in which $M = \frac{1}{2} (P_A + P_B)$.

\textbf{Light Field Distance (LFD).} LFD leverages the so-called light field descriptor \citep{chen2003visual}, which combines local region-based and contour-based descriptors to measure the silhouettes and thus object shapes.

\newpage
\section{Generation Sequence}

For each time step $t \in \{1, ..., T\}$ during the DDPM inference process, we can compute the predicted $x_0$ by the current value of $x_t$ \citep{ho2020denoising}:
\[
    \hat{x}_0 = \frac{1}{\sqrt{\overline{\alpha_t}}} (x_t - \sqrt{1 - \overline{\alpha}_t} \epsilon_\theta(x, t)),
\]
in which $\epsilon_\theta(x,t)$ is the learned denoising network, $\alpha_t = \Pi_{s=1}^t (1 - \beta_s)$ and $\beta_t$ is the noise scale hyperparameter in DDPM.

We show example generation trajectories in Figure~\ref{fig:gen_seq} by visualizing the predicted $x_0$ through time. All $\hat{x}_0$'s are clipped to $[0, 1]$. We visualize the sequence from $T = 250$ to $T = 50$, as the predicted tetrahedral grids before $T = 250$ produce nothing other than noise.



% \include{newfig/gen_seq_figs}
\begin{figure}[H]
    \centering
    % \vspace{-0.2in} 

    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_005_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_006_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_007_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_008_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_009_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_010_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_011_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_012_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_013_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_014_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_015_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_016_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_017_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_018_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_019_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_020_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_021_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_022_000.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_023_000.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000/img_diffusion_024_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000/img_diffusion_025_000.pdf}
    
    
    %%%%%%%%%%%%%%%%%%
    
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_005_001.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_006_001.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_007_001.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_008_001.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_009_001.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_010_001.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_011_001.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_012_001.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_013_001.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_014_001.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_015_001.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_016_001.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_017_001.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_018_001.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_019_001.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_020_001.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_021_001.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_022_001.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_023_001.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_024_001.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new001/img_diffusion_025_001.pdf}

    
    %%%%%%%%%%%%%%%%%%

    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_005_008.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_006_008.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_007_008.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_008_008.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_009_008.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_010_008.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_011_008.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_012_008.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_013_008.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_014_008.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_015_008.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_016_008.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_017_008.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_018_008.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_019_008.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_020_008.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_021_008.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_022_008.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_023_008.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/008/img_diffusion_024_008.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new008/img_diffusion_025_008.pdf}

    %%%%%%%%%%%%%

    
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_005_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_006_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_007_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_008_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_009_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_010_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_011_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_012_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_013_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_014_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_015_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_016_000.pdf}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_017_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_018_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_019_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_020_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_021_000.pdf}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_022_000.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_023_000.png}
    %\includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/000_airplane/img_diffusion_024_000.png}
    \includegraphics[trim={7cm 7cm 7cm 7cm},clip,width=0.07\linewidth]{newfig/gen_seq/new000_airplane/img_diffusion_025_000.pdf}
    
    
    \caption{\footnotesize Generation sequences of predicted $x_0$.}
    \label{fig:gen_seq}
\end{figure}


\newpage
\section{Nearest Neighbor Visualization}

We show the nearest neighbors of some generated samples in the validation set in Figure~\ref{fig:nn_viz}.

\begin{figure}[H]
    \centering
    % \vspace{-0.2in} 
    %\hspace{-0.4in}
    \includegraphics[width=1\linewidth]{newfig/nn_viz_v2.pdf}
    \centering
    \caption{\footnotesize Nearest neighbor (from the ShapeNet dataset) of the generated samples.}
    \label{fig:nn_viz}
\end{figure}


\newpage
\section{Qualitative Comparison to GET3D}

We visualize some generated meshes from MeshDiffusion and GET3D in Figure~\ref{fig:get3d_qual_comp}. Both models are able to generate finer details of shapes due to the use of DMTet. On the car category of ShapeNet, generated meshes by GET3D tend to have fewer or less noticeable holes, possibly because GET3D is directly trained on adversarial losses on rendered images. In comparison, MeshDiffusion has to learn the complex and often invisible 3D structures, which is also one of our advantages for modeling complex 3D geometry with inner structures. However, potentially due to the limited capacity of the decoder (compared to a recurrent U-Net in diffusion models) and the hardness of GAN training (\textit{v.s.} supervised learning with denoising loss), we observe that the failure cases appear more often in GET3D compared to MeshDiffusion. And the novel examples in MeshDiffusion seem to make more sense compared to those from GET3D.


\begin{figure}[H]
    \centering
    % \vspace{-0.2in} 
    \includegraphics[width=0.95\linewidth]{newfig/get3d_qual_comp_v2.pdf}
    \caption{\footnotesize Qualitative Comparison between MeshDiffusion and GET3D.}
    \label{fig:get3d_qual_comp}
\end{figure}





\newpage
\section{Inner Structure Generation}

Since our MeshDiffusion is explicitly trained with 3D information, it is capable of generating inner structures invisible from the outside. We give a comparison between samples generated by MeshDiffusion (trained with a higher resolution of 128) and GET3D in Figure~\ref{fig:inner_structure}. 


\begin{figure}[H]
    \centering
    % \vspace{-0.2in} 
    \includegraphics[width=0.45\linewidth]{newfig/get3d_car_inner.pdf}
    \includegraphics[width=0.45\linewidth]{newfig/meshdiffusion_car_inner.pdf}
    \caption{\footnotesize Inner structures of generated car meshes. Left: GET3D, Right: MeshDiffusion.}
    \label{fig:inner_structure}
\end{figure}

To fully evaluate the capability of surface generation (as GET3D is only trained to optimize surface appearance), we follow the same procedure described in \url{https://github.com/nv-tlabs/GET3D/tree/master/evaluation_scripts} and compare the point cloud metrics in Table~\ref{table:eval_car_inner_pc}, with generated sample set 5x larger than the validation set.



\input{tables/eval-car-inner-pc}

\newpage


\section{More Unconditional Generation Samples}
\ 

\begin{figure}[h!]
    \centering
    \vspace{-0.2in} 
    \includegraphics[width=0.97\linewidth]{newfig/chair_appendix_v2.pdf}
    \vspace{-0.02in} 
    \caption{\footnotesize Generated 3D chair meshes.}
    \label{fig:chair_appendix}
\end{figure}


\newpage
\ 

\begin{figure}[t!]
    \centering
    \vspace{-0.2in} 
    \includegraphics[width=0.97\linewidth]{newfig/car_appendix_v2.pdf}
    \vspace{-0.02in} 
    \caption{\footnotesize Generated 3D car meshes.}
    \label{fig:airplane_appendix}
\end{figure}

\newpage
\ 

\begin{figure}[t!]
    \centering
    \vspace{-0.2in} 
    \includegraphics[width=0.97\linewidth]{newfig/airplane_appendix_v2.pdf}
    \vspace{-0.02in} 
    \caption{\footnotesize Generated 3D airplane meshes.}
    \label{fig:car_appendix}
\end{figure}

\newpage
\ 

\begin{figure}[t!]
    \centering
    \vspace{-0.2in} 
    \includegraphics[width=0.97\linewidth]{newfig/rifle_appendix_v2.pdf}
    \vspace{-0.02in} 
    \caption{\footnotesize Generated 3D rifle meshes.}
    \label{fig:rifle_appendix}
\end{figure}

\newpage
\ 

\begin{figure}[t!]
    \centering
    \vspace{-0.2in} 
    \includegraphics[width=0.97\linewidth]{newfig/table_appendix_v2.pdf}
    \vspace{-0.02in} 
    \caption{\footnotesize Generated 3D table meshes.}
    \label{fig:table_appendix}
\end{figure}


\end{document}

