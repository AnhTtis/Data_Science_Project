\section*{Discussion}

We have developed a model for walking speed which is very robust, due the large volume of data (88,000 km) used to build it, and which correlates with the data over a wider range of conditions than commonly used formulae. Data from tracks confirms that each of the walking slope, the hill slope and the terrain type or obstruction are significant factors in determining walking speeds. The model improves on existing methods to predict walking speeds (Fig \ref{fig4} \& \ref{fig5}). We have also shown the specific improvement that our new model has on predicting walking speeds in off-road conditions, compared to the simple off-road speed reductions used by existing models.

Our results confirm that Naismith's rule (Fig \ref{fig1}) is still a good rule-of-thumb to use when estimating the total walking time for a route, especially in situations where the calculation must be done by hand. However, the findings here can be used as an addition to Naismith's rule; it is likely that (under Naismith's rule) the predicted ascent time will be overestimated and the predicted descent time will be underestimated. It is not uncommon for hikers to contact one another when they reach the summit of a hill, and provide an estimated arrival time back at the campsite. Knowing that the descent will likely take longer than estimated by Naismith's rule will result in more accurate arrival estimations being given. Similarly, the knowledge of how the hill slope reduces walking speeds, or that just 10 cm of vegetation can reduce walking speeds by up to 0.6 km/h may well affect route choices made when out on a walk. 

The benefit of using crowdsourced GPS data to build our model is also a limitation of the approach, as we did not have control over data collection. This meant that models were unable to account for group size, ability and composition, or other potential variables such as weather conditions, as factors in determining walking speed (although the volume of data should cause these effects to average out). 

A new method was developed to identify and remove as many non-walking or hiking tracks as possible and to identify and remove substantial breaks. While the methods used to filter the datasets were blinded to the outcome of the model generation, the choice of filtering methods will have had an impact on the dataset and subsequent model. We did not perform a quantitative analysis on the breakfinding performance as we did not have access to any ground truth data regarding when breaks were actually taken. Any analysis would first require us to manually identify the locations of breaks in the GPS tracks, which would still be a subjective identification method.

A further limitation of our data came when we looked to classify points into paved roads, unpaved roads or off-road. We did not have accurate knowledge of whether the routes were following a road or path, and a combination of GPS drift and map error meant that we had to use a search radius around each data point to identify roads. While doing this, we were aware that we were likely overclassifying tracks on roads. While our model appears to be robust to this overclassification (due to the volumes of correctly classified data used), the overclassification left us with a reduced number of off-road datapoints from which to predict off-road travel speeds.

Furthermore, the use of crowdsourced data meant that all of our data came from `walkable' regions by definition. When including the terrain obstruction variable, we were unable to determine if there is a level of terrain obstruction which makes walking impossible. Similarly, the vast majority of the data was collected on shallow hill- and walking slopes, leading to a lack of data in steep areas. While this does mean that we can be very confident about our walking speed predictions in less steep regions (where most walking occurs), it is unclear whether the lack of data on steeper regions is a result of steep slopes being relatively rare, or that they cannot be easily navigated, so hikers chose an alternate path. 
As described above we had to make a number of assumptions regarding data filtering and processing including model selection, and other choices may give different results. To support anyone who wants to challenge or test these assumptions, or try different models, we have made the code available on Github. Further, all of the data sources used are detailed in \nameref{S1_Appendix}.