\section{Introduction}
\label{sec:intro}


Visual reasoning, the ability to composite rules on internal representations to reason and answer questions about visual scenes, has been a long-standing challenge in the field of artificial intelligence and computer vision. Several datasets \cite{Zhu2016Visual7WGQ, Goyal2017MakingTV,Johnson2017CLEVRAD} have been proposed to tackle this challenge. However, they mainly focus on visual reasoning on 2D single-view images. Since 2D single-view images only cover a limited region of the whole space, such reasoning inevitably has several weaknesses, including occlusion, and failing to answer 3D-related questions about the entire scene that we are interested in. As shown in Fig. \ref{fig:teaser}, it's difficult, even for  humans, to count the number of chairs in a scene due to the object occlusion, and it's even harder to infer 3D relations like ``closer" from a single-view 2D image.

On the other hand, there's strong psychological evidence that human beings conduct visual reasoning in the underlying 3D representations \cite{spelke1993gestalt}.
 Recently, there have been several works focusing on 3D visual question answering \cite{Ye20213DQA, Azuma2022ScanQA3Q, yan2021clevr3d, etesam20223dvqa}. They mainly use traditional 3D representations (\textit{e.g.,} point clouds) for visual reasoning. This is inconsistent with the way human beings perform 3D reasoning in real life. Instead of being given an entire 3D representation of the scene at once, humans will actively walk around and explore the whole environment, ingesting image observations from different views and converting them into a holistic 3D representation that assists them in understanding and reasoning about the environment. Such abilities are crucial for many embodied AI applications, such as building assistive robots.
 % \gc{Talk about this is useful for embodied AI and building assistive robot}  
 
% Talk about recent work on 3D reasoning, and then discuss why our setting is different and useful...




To this end, we propose the novel task of 3D visual reasoning from multi-view images taken by active exploration of an embodied agent.  Specifically, we generate a large-scale benchmark, 3DMV-VQA (3D multi-view visual question answering), that contains approximately 5k scenes and 50k question-answering pairs about these scenes. For each scene, we provide a collection of multi-view image observations. We generate this dataset by placing an embodied agent in the Habitat-Matterport environment \cite{Ramakrishnan2021HabitatMatterport3D}, which actively explores the environment and takes pictures from different views. We also obtain
% \gc{not we provide. give citation.}
scene graph annotations from the Habitat-Matterport 3D semantics dataset (HM3DSem) \cite{yadav2022habitat}, including ground-truth locations, segmentations, semantic information of the objects, as well as relationships among the objects in the environments, for model diagnosis. To evaluate the models' 3D reasoning abilities on the entire environment, we design several 3D-related question types, including concept, counting, relation and comparison. 


Given this new  task, the key challenges we would like to investigate include: 1) how to efficiently obtain the compact visual representation to encode  crucial properties (\textit{e.g.,} semantics and relations) by integrating all incomplete observations of the environment in the process of active exploration for 3D visual reasoning? 2) How to ground the semantic concepts on these 3D representations that could be leveraged for downstream tasks, such as visual reasoning? 3) How to infer the relations among the objects, and perform step-by-step reasoning?

As the first step to tackling these challenges, we propose a novel model, 3D-CLR (3D Concept Learning and Reasoning). First, to efficiently obtain a compact 3D representation from multi-view images, we use a neural-field model based on compact voxel grids \cite{Sun2022DirectVG} which is both fast to train and effective at storing scene properties in its voxel grids. 
% As for grounding the semantic concepts and relationships, we observe that 
% semantic concepts are diverse and open-vocabulary in real world scenes. %, while relational concepts can be very limited and thus can be considered a close-class vocabulary \cite{Landau1993WhatA, Hayward1995SpatialLA}. 
% \gc{Talk about previous works always learn concepts from paired 3D and language data, which can not scale up our dataset, which cover a variety of objects}
As for concept learning, we observe that previous works on 3D scene understanding \cite{Chen2020ScanRefer3O, Achlioptas2020ReferIt3DNL} lack the diversity and scale with regard to semantic concepts due to the limited amount of paired 3D-and-language data.  
Although large-scale vision-language models (VLMs) have achieved impressive performances for zero-shot semantic grounding on 2D images, leveraging these pretrained models for effective open-vocabulary 3D grounding of semantic concepts remains a challenge. To address these challenges, we propose to encode the features of a pre-trained 2D vision-language model (VLM) into the compact 3D representation defined across voxel locations.
% \gc{talk about map 2D to 3d are challenging. so we propose new method.
% }
Specifically, we use the CLIP-LSeg\cite{li2022language} model to obtain features on multi-view images, and propose an alignment loss to map the features in our 3D voxel grid to 2D pixels. By calculating the dot-product attention between the 3D per-point features and CLIP language embeddings, we can ground the semantic concepts in the 3D compact representation.
% Drawing inspiration from previous works of 3D concept grounding on neural fields \cite{hong20223d}, we propose to ground the objects in the 3D representations by calculating the similarities between feature vectors learnt from neural fields and the learnt language embeddings. However, since we are working on open-vocabulary question-answering pairs on real-world scenes, we propose to learn the features of the neural fields from the CLIP pretrained model.
Finally, to answer the questions, we introduce a set of neural reasoning operators, including \textsc{filter}, \textsc{count}, \textsc{relation} operators and so on, which take the 3D representations of different objects as input and output the predictions. 

We conduct experiments on our proposed  3DMV-VQA benchmark. Experimental results 
 show that our proposed 3D-CLR outperforms all baseline models a lot. However, failure cases and model diagnosis show that challenges still exist concerning the grounding of small objects and the separation of close object instances. We provide an in-depth analysis of the challenges and discuss potential future directions.
 % \gc{delete generalization, talk about challenges of this dataset.} 
 

%Experiments on this 3DMV-VQA benchmark show that our 3D-CLR model outperforms all baseline models. However, failure cases and model diagnosis show that challenges still exist concerning the grounding of small objects, the separation of close object instances, and ambiguities in 3D relations. To sum up, we have the following contributions. 
 


%which incorporates a neural radiance field to obtain the compact 3D representations, 2D pre-trained vision and language model (\ie CLIP\cite{Radford2021LearningTV}) to ground semantic concepts in the 3D representations from 2D images, and a neural relational module network to ground the relationships among the objects. 


% As the first step to tackle these challenges, we propose a novel model, 3D-CLR (3D Concept Learning and Reasoning). First, to efficiently obtain a compact 3D representation from multi-view images, we use a neural-field model based on compact voxel grids \cite{Sun2022DirectVG} which is both fast to train and effective at storing scene properties in its voxel grids. 
% % As for grounding the semantic concepts and relationships, we observe that 
% % semantic concepts are diverse and open-vocabulary in real world scenes. %, while relational concepts can be very limited and thus can be considered a close-class vocabulary \cite{Landau1993WhatA, Hayward1995SpatialLA}. 
% % \gc{Talk about previous works always learn concepts from paired 3D and language data, which can not scale up our dataset, which cover a variety of objects}
% As for concept learning, we observe that previous works on 3D scene understanding \cite{Chen2020ScanRefer3O, Achlioptas2020ReferIt3DNL} lack the diversity and scale with regard to semantic concepts due to the limited amount of paired 3D-and-language data.  
% On the other hand, large-scale 2D vision-language models (VLMs) have achieved impressive performances, but how to leverage these pretrained models for 3D concept learning remains a challenge.
% Inspired by this, to enable effective open-vocabulary 3D grounding of semantic concepts,  we propose to encode the features of a pretrained 2D vision-language model (VLM) into the compact 3D representation defined across voxel locations.
% % \gc{talk about map 2D to 3d are challenging. so we propose new method.
% % }
% Specifically, we use the CLIP-LSeg\cite{Li2022LanguagedrivenSS} model, based on CLIP features, to obtain features on multi-view images, and propose an alignment loss to align the features in our 3D voxel grid to 2D pixels. \gc{Yining, I don't think this part is correct.}
% % Drawing inspiration from previous works of 3D concept grounding on neural fields \cite{hong20223d}, we propose to ground the objects in the 3D representations by calculating the similarities between feature vectors learnt from neural fields and the learnt language embeddings. However, since we are working on open-vocabulary question-answering pairs on real-world scenes, we propose to learn the features of the neural fields from the CLIP pretrained model.
% Finally, to answer the questions, we introduce a set of neural operators, including filter, count, and relational, which takes the 3D representations of different objects and outputs the predictions. 

 \noindent To sum up, we have the following contributions in this paper. 
 \vspace{-2mm}
\begin{itemize}[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 
\item We propose the novel task of 3D concept learning and reasoning from multi-view images. 

\item By having robots actively explore the embodied environments, we collect a large-scale benchmark on 3D multi-view visual question answering (3DMV-VQA). 

\item We devise a model that incorporates a neural radiance field, 2D pretrained vision and language model, and neural reasoning operators to ground the concepts and perform 3D reasoning on the multi-view images. We  illustrate that our model outperforms all baseline models.

\item We perform an in-depth analysis of the challenges of this new task and highlight potential future directions. 
\end{itemize}