\section{Related Work}
\label{sec:related}


\noindent\textbf{Visual Reasoning}
There have been numerous tasks focusing on learning visual concepts from natural language, including visually-grounded question answering \cite{gan2017vqs,Ganju2017WhatsIA}, text-image retrieval \cite{Vendrov2016OrderEmbeddingsOI} and so on. Visual reasoning has drawn much attention recently as it requires human-like understanding of the visual scene. A wide variety of benchmarks have been created over the recent years~\cite{Johnson2017CLEVRAD, Goyal2017MakingTV,chencomphy,Zhu2016Visual7WGQ, Hong2021PTRAB,chen2020cops}. However, they mainly focus on visual reasoning from 2D single-view images, while there's strong psychological evidence that human beings perform visual reasoning on the underlying 3D representations. In this paper, we propose the novel task of visual reasoning from multi-view images, and collect a large-scale benchmark for this task. In recent years, numerous visual reasoning models have also been proposed, ranging from attention-based methods \cite{Hudson2018CompositionalAN,chen2019weakly}, graph-based methods \cite{Huang2020LocationAwareGC}, to models based on large pretrained vision-language model \cite{Li2022AlignAP,chen2023see}.
% Specifically, MAC \cite{Hudson2018CompositionalAN} combined multi-modal attention for compositional reasoning. LGCN \cite{Huang2020LocationAwareGC} proposes to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. ALPRO\cite{Li2022AlignAP} utilizes a pretrained video-language model for downstream video question answering. 
% \gc{need to cut down and give more citation to recent CVPR/ICCV/ECCV papers} 
These methods model the reasoning process implicitly with neural networks. Neural-symbolic methods \cite{Yi2018NeuralSymbolicVD, Mao2019TheNC,chen2021grounding} explicitly perform symbolic reasoning on the objects representations and language representations. They use perception models to extract 2D masks as a first step, and then execute operators and ground concepts on these pre-segmented masks, but are limited to a set of pre-defined concepts on simple scenes. \cite{hong20223d} proposes to use the feature vectors from occupancy networks \cite{Mescheder2019OccupancyNL} to do visual reasoning in the 3D space. However, they also use a synthetic dataset, and learn a limited set of semantic concepts from scratch. We propose to learn 3D neural field features from 2D multi-view real-world images, and incorporate a 2D VLM
for open-vocabulary reasoning.

\noindent\textbf{3D Reasoning}
Understanding and reasoning about 3D scenes has been a long-standing challenge. Recent works focus on leveraging language to explore 3D scenes, such as object captioning \cite{Chen2020ScanRefer3O, Chen2021Scan2CapCD} and object localization from language \cite{Achlioptas2020ReferIt3DNL, Feng2021FreeformDG, Huang2021TextGuidedGN}. Our work is mostly related to 3D Visual Question Answering \cite{Ye20213DQA, Azuma2022ScanQA3Q, yan2021clevr3d, etesam20223dvqa} as we both focus on answering questions and reasoning about 3D scenes. However, these works use point clouds as 3D representations, which diverts from the way human beings perform 3D reasoning. Instead of being given an entire 3D representation all at once, human beings would actively move and explore the environment, integrating multi-view information to get a compact 3D representation. Therefore, we propose 3D reasoning from multi-view images. In addition, since 3D assets paired with natural language descriptions are hard to get in real-life scenarios, previous works struggle to ground open-vocabulary concepts. In our work, we leverage 2D VLMs for zero-shot open-vocabulary concept grounding in the 3D space.

%\noindent\textbf{Embodied Reasoning} Our work is also closely related to Embodied Question Answering (EQA) and Interactive Question Answering (IQA), which also involve an embodied agent exploring the environment and answer the question. However, the reasoning mainly focuses on the outcome or the history of the navigation on 2D images, and does not require a holistic 3D understanding of the environment. \gc{give some examples} \gc{then talk about the instrurtion following.}

\noindent\textbf{Embodied Reasoning} Our work is also closely related to Embodied Question Answering (EQA)~\cite{Das2018EmbodiedQA,yu2019multi} and Interactive Question Answering (IQA)~\cite{gordon2018iqa,konstantinova2013interactive}, which also involve an embodied agent exploring the environment and answering the question. However, the reasoning mainly focuses on the outcome or the history of the navigation on 2D images and does not require a holistic 3D understanding of the environment. There are also works~\cite{suglia2021embodied,song2022one,shridhar2020alfred,zheng2022vlmbench,gao2022dialfred,dingembodied} targeting instruction following in embodied environments, in which an agent is asked to perform a series of tasks based on language instructions. Different from their settings, for our benchmark an embodied agent actively explores the environment and takes multi-view images for 3D-related reasoning.

%we leverage neural radiance fields and pre-trained vision and lanuage models to perform 3D reasoning in the form of question answering.   

\noindent\textbf{Neural Fields}
Our approach utilizes neural fields to parameterize an underlying 3D compact representations of scenes for reasoning.   Neural field models (\textit{e.g.,} \cite{mildenhall2020nerf}) have gained much popularity since they can reconstruct a volumetric 3D scene representation from a set of images. Recent works \cite{Garbin2021FastNeRFHN, Hedman2021BakingNR, Yu2021PlenOctreesFR, Sun2022DirectVG} have pushed it further by using classic voxel-grids to explicitly store the scene properties (\textit{e.g.}, density, color and feature) for rendering, which allows for real-time rendering and is utilized by this paper.
Neural fields have also been used to represent dynamic scenes \cite{niemeyer2019occupancy,du2021nerflow}, appearance \cite{sitzmann2019srns,mildenhall2020nerf,Niemeyer2020DVR,yariv2020multiview,saito2019pifu}, physics \cite{Kollmannsberger2021PhysicsInformedNN}, robotics \cite{Jiang2021SynergiesBA, simeonov2021neural}, acoustics \cite{luo2022learning} and more general multi-modal signals \cite{du2021gem}. There are also some works that integrate semantics or language in neural fields \cite{Jain2022ZeroShotTO, Wang2021CLIPNeRFTD}. However, they mainly focus on using language for manipulation, editing or generation. 
\cite{hong20223d} leverages neural descriptor field \cite{simeonov2021neural} for 3D concept grounding. However, they require ground-truth occupancy values to train the neural field, which can not be applied to real-world scenes. In this paper, we propose to leverage voxel-based neural radiance field \cite{Sun2022DirectVG} to get the compact representations for 3D visual reasoning. 
% \gc{need to differentiate your neruips paper and highlight they can not work on real world scene. }
