\section{Method}
% \label{sec:method}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/framework_v3.pdf}
\vspace{-5mm}
\caption{An overview of our 3D-CLR framework. First, we learn a 3D compact scene representation from multi-view images using neural fields (I). Second, we use CLIP-LSeg model to get per-pixel 2D features (II). We utilize a 3D-2D alignment loss to assign features to the 3D compact representation (III). By calculating the dot-product attention between the 3D per-point features and CLIP language embeddings, we could get the concept grounding in 3D (IV). Finally, the reasoning process is performed via a set of neural reasoning operators, such as \textsc{Filter}, \textsc{Get\_Instance} and \textsc{Count\_Relation} (V). Relation operators are learned via relation networks.}
\vspace{-5mm}
\label{fig:framework}
\end{figure*}

Fig.~\ref{fig:framework} illustrates an overview of our framework. Specifically, our framework consists of three steps.  First, we learn a 3D compact representation from multi-view images using neural field. And then we propose to leverage pre-trained 2D vision-and-language model to ground concepts on 3D space. This is achieved by 1) generating 2D pixel features using CLIP-LSeg; 2) aligning the features of 3D voxel grid and 2D pixel features from CLIP- LSeg~\cite{li2022language}; 3) dot-product attention between the 3D features and CLIP language features~\cite{li2022language}. Finally, to perform visual reasoning, we propose neural reasoning operators, which execute the question step by step on the 3D compact representation and outputs a final answer. For example, we use \textsc{Filter} operators to ground semantic concepts on the 3D representation, \textsc{Get\_Instance} to get all instances of a semantic class, and \textsc{Count\_Relation} to count how many pairs of the two semantic classes have the queried relation.
% \gc{metion all the neural operators.}
% Works from linguistic and cognitive science suggest that semantic concepts are diverse and open-vocabulary, while relational concepts describing 3D objects' relationships can be very limited and thus can be considered a close-class vocabulary \cite{Landau1993WhatA, Hayward1995SpatialLA}. Therefore, it's unrealistic to learn the embeddings of all the concepts in the question-answering pairs, while it's more natural to learn the relation embeddings. Inspired by this, we propose to leverage 2D pretrained vision-language model (\textit{i.e.,} CLIP) for open-vocabulary semantic concept learning, while proposing a neural relation module network for relational reasoning. 

\subsection{Learning 3D Compact Scene Representations}

% Since 3D-related reasoning works on 3D compact representations rather than 2D images, we first propose to use a neural field to extract 3D representations from multi-view images. The next step is to learn the 3D features for visual reasoning. However, 3D assets are limited in diversity and scale, posing challenges for training large-scale 3D foundation models, while there's much progress on large-scale 2D pretrained models which provide decent features\cite{Radford2021LearningTV, Ramesh2021ZeroShotTG}. Since neural field maps a 2D pixel to several 3D points along the ray, it's natural to get 3D features for 2D per-pixel features. We apply CLIP-LSeg\cite{Li2022LanguagedrivenSS} to learn per-xel 2D features, and use an alignment loss to align 3D features with 2D features.

% \paragraph{3D Compact Representation from neural field.} 
Neural radiance fields  \cite{mildenhall2020nerf} are capable of learning a 3D representation that can reconstruct a volumetric 3D scene representation from a set of images. Voxel-based methods \cite{Garbin2021FastNeRFHN, Hedman2021BakingNR, Yu2021PlenOctreesFR, Sun2022DirectVG} speed up the learning process by explicitly storing the scene properties (\textit{e.g.}, density, color and feature) in its voxel grids. We leverage Direct Voxel Grid Optimization (DVGO) \cite{Sun2022DirectVG} as our backbone for 3D compact representation for its fast speed. DVGO stores the learned density and color properties in its grid cells. The rendering of multi-view images is by interpolating through the voxel grids to get the density and color for each sampled point along each sampled ray, and integrating the colors based on the rendering alpha weights calculated from densities according to quadrature rule \cite{Max1995OpticalMF}. The model is trained by minimizing the L2 loss between the rendered multi-view images and the ground-truth multi-view images. By extracting the density voxel grid, we can get the 3D compact representation (\textit{e.g.,} By visualizing points with density greater than 0.5, we can get the 3D representation as shown in Fig. \ref{fig:framework} I. ) 

\subsection{3D Semantic Concept Grounding}
Once we extract the 3D compact representation of the scene, we need to ground the semantic concepts for reasoning from language. 
Recent work from \cite{hong20223d} has proposed to ground concepts from paired 3D assets and question-answers. Though promising results have been achieved on synthetic data, it is not feasible for open-vocabulary 3D reasoning in real-world data, since it is hard to collect large-scale 3D vision-and-language paired data.  To address this challenge, our idea is to leverage  pre-trained 2D vision and language model \cite{Radford2021LearningTV, Ramesh2021ZeroShotTG} for 3D concept grounding in real-world scenes.  But how can we map 2D concepts into 3D neural field representations? Note that 3D compact representations can be learned from 2D multi-view images and that each 2D pixel actually corresponds to several 3D points along the ray. Therefore, it's possible to get 3D features from 2D per-pixel features. Inspired by this, we first add a feature voxel grid representation to DVGO, in addition to density and color, to represent 3D features. 
% it's natural to utilize 2D VLMs to ground semantic concepts on the 3D representations. 
 We then apply CLIP-LSeg\cite{li2022language} to learn per-pixel 2D features, which can be attended to by CLIP concept embeddings. We use an alignment loss to align 3D features with 2D features so that we can perform concept grounding on the 3D representations.
% Since 3D voxel grids and 2D pixels are aligned via alpha compositing, we add one L1 loss to force the features of 3D voxel grids to align with the 2D LSeg pixels based on the alpha values. 

\noindent\textbf{2D Feature Extraction.}
To get per-pixel features that can be attended by concept embeddings, we use the features from language-driven semantic segmentation (CLIP-LSeg) \cite{li2022language}, which learns 2D per-pixel features from a pre-trained vision-language model (\textit{i.e.,} \cite{Radford2021LearningTV}). Specifically, it
uses the text encoder from CLIP, trains an image encoder to produce an embedding vector for each pixel, and calculates the scores of word-pixel correlation by dot-product. By outputting the semantic class with the maximum score of each pixel, CLIP-LSeg is able to perform zero-shot 2D semantic segmentation.

\noindent\textbf{3D-2D Alignment.}
In addition to density and color, we also store a 512-dim feature in each grid cell in the compact representation. To align the 3D per-point features with 2D per-pixel features, we calculate an L1 loss between each pixel and each 3D point sampled on the ray of the pixel. The overall L1 loss along a ray is the weighted sum of all the pixel-point alignment losses, with weights same as the rendering weights: $\mathcal{L}_{\text {feature}}=\sum_{i=1}^K w_i(\|\boldsymbol{f_i}-F(\boldsymbol{r})\|),$
where $\boldsymbol{r}$ is a ray corresponding to a 2D pixel, $F(\boldsymbol{r})$ is the 2D feature from CLIP-LSeg, $K$ is the total number of sampled points along the ray and $\boldsymbol{f_i}$ is the feature of point $i$ by interpolating through the feature voxel grid, $w_i$ is the rendering weight.
% \gc{add equations.} 

\noindent\textbf{Concept Grounding through Attention.}  Since our feature voxel grid representation is learnt from CLIP-LSeg, by calculating the dot-product attention $<\boldsymbol{f}, \boldsymbol{v}> $ between per-point 3D feature $\boldsymbol{f}$ and the CLIP concept embeddings $\boldsymbol{v}$, we can get zero-shot view-independent concept grounding and semantic segmentations in the 3D representation, as is presented in Fig. \ref{fig:framework} IV. 
% \gc{add equations.}

\subsection{Neural Reasoning Operators}
Finally, we use the grounded semantic concepts for 3D reasoning from language. We first transform questions into a sequence of operators that can be executed on the 3D representation for reasoning. We adopt a LSTM-based semantic parser   \cite{Yi2018NeuralSymbolicVD} for that. As \cite{Mao2019TheNC, hong20223d}, we further devise a set of operators which can be executed on the 3D representation.  Please refer to \textbf{Appendix} for a full list of operators.

\noindent\textbf{Filter Operators.}  We filter all the grid cells with a certain semantic concept.

\noindent\textbf{Get\_Instance Operators.} We implement this by utilizing DBSCAN \cite{Ester1996ADA}, an unsupervised algorithm which assigns clusters to a set of points. Specifically, given a set of points in the 3D space, it can group together the points that are closely packed together for instance segmentation.

\noindent\textbf{Relation Operators.} We cannot directly execute the relation on the 3D representation as we have not grounded relations. Thus, we represent each relation using a distinct neural module (which is practical as the vocabulary of relations is limited \cite{Landau1993WhatA}). We first concatenate the voxel grid representations of all the referred objects and feed them into the relation network.
% \yd{Do we do something afterwards -- we first concatenate, then what?} 
The relation network consists of three 3D convolutional layers and then three 3D deconvolutional layers. A score is output by the relation network indicating whether the objects have the relationship or not. Since vanilla 3D CNNs are very slow, we use Sparse Convolution \cite{spconv2022} instead. Based on the relations asked in the questions, different relation modules are chosen. 

% \subsection{Learning 3D Compact Representation}
% In recent years, neural field models(\textit{e.g.,} \cite{mildenhall2020nerf}) have gained much popularity since they can reconstruct a volumetric 3D scene representation from a set of images. Recent works \cite{Garbin2021FastNeRFHN, Hedman2021BakingNR, Yu2021PlenOctreesFR, Sun2022DirectVG} have pushed it further by using classic voxel-grids to explicitly store the scene properties (\textit{e.g.}, density, color and feature) for rendering, which allows for real-time rendering. Since concept grounding and relation learning are expected to work on the per-point features in the 3D space \cite{hong20223d} of thousands of scenes, it's more suitable to use voxel-grid-based methods since they store explicit properties in each point which can be directly used for reasoning, and super-fast convergence makes it feasible to train thousands of scenes. Specifically, we use the fine reconstruction process of Direct Voxel Grid Optimization \cite{Sun2022DirectVG} as our backbone for 3D compact representation for its fast speed. 

% A compact voxel-grid representation models the modalities of interest (\textit{e.g.,} density, color or feature) explicitly in its grid cells. To query the properties at any given 3D point, interpolation is used:
% \begin{equation}
% \operatorname{interp}(\boldsymbol{x}, \boldsymbol{V}):\left(\mathbb{R}^3, \mathbb{R}^{C \times N_x \times N_y \times N_z}\right) \rightarrow \mathbb{R}^C
% \end{equation}
% where $\boldsymbol{x}$ is the queried 3D point,  $\boldsymbol{V}$ is the voxel grid, and $C$ is
% the dimension of one of the modalities, and $N_x, N_Y, N_z$ is the number of voxels. We first predict the density of a specified point by interpolating the density grid. This is crucial for the geometric reconstruction of the scene.  
% \begin{equation}
% \sigma=\operatorname{interp}\left(\boldsymbol{x}, \boldsymbol{V}^{(\text {density })}\right)
% \end{equation}
% where $\sigma$ is the volume density at position $\boldsymbol{x}$. For the modeling of color emission, we use an explicit-implicit hybrid representation where  a shallow MLP is placed after the color voxel grid interpolation process:
% \begin{equation}
% \boldsymbol{c}=\operatorname{MLP}^{(\mathrm{rgb})}\left(\operatorname{interp}\left(\boldsymbol{x}, \boldsymbol{V}^{(\mathrm{color})}\right), \boldsymbol{x}, \boldsymbol{d}\right)
% \end{equation}
% where $\boldsymbol{c}$ is the view-dependent color emission at position $\boldsymbol{x}$ viewing from direction $\boldsymbol{d}$.

% To render the color $\hat{C}(\boldsymbol{r})$ of ray $r$, K points are sampled on ray $r$ with densities and colors $\left\{\left(\sigma_i, \boldsymbol{c}_i\right)\right\}_{i=1}^K$. The K results are accumulated by the quadrature rule by Max \cite{Max1995OpticalMF}:
% \begin{align}
% \hat{C}(\mathbf{r})=\sum_{i=1}^K T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right) \mathbf{c}_i, 
% &\\
% T_i=\exp \left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
% \end{align}
% where $\delta_i=t_{i+1}-t_i$ is the distance between adjacent points along a ray, and $\alpha_i=1-\exp \left(-\sigma_i \delta_i\right)$ is the alpha value for traditional alpha compositing.

% The backbone is trained by minimizing the mean
% square error between the rendered and observed color. 

% \begin{equation}
% \mathcal{L}_{\text {color }}=\|\hat{C}(\boldsymbol{r})-C(\boldsymbol{r})\|_2^2
% \end{equation}

% By extracting the density values of the voxel grid $\boldsymbol{V}^{(\text {density })} \in \mathbb{R}^{1 \times N_x \times N_y \times N_z}$, we can get the compact 3D representation of the scene, as shown in the middle of Figure 2.

% We refer the readers to \cite{Sun2022DirectVG} for more details about the Direct Voxel Grid Optimization.


% \subsection{3D Semantic Concept Grounding}
% In \cite{hong20223d}, a Neural Descriptor Field (NDF) \cite{simeonov2021neural} which gives a feature vector for each 3D coordinate a feature vector is used for concept grounding by aligning the feature vector with the learned concept embeddings. Drawing inspiration from this, we also propose to use a feature voxel-grid  (in addition to density voxel grid and color voxel grid) used for concept grounding. The compact 3D feature representation is composed of one feature voxel-grid representation plus one view-independent shallow MLP: 

% \begin{equation}
% \boldsymbol{f}=\operatorname{MLP}^{(\mathrm{feature})}\left(\operatorname{interp}\left(\boldsymbol{x}, \boldsymbol{V}^{(\mathrm{feature})}\right), \boldsymbol{x}, \boldsymbol{d}\right)
% \end{equation}

% However, the drawback of \cite{hong20223d} is that the embeddings of concepts are learnt from sratch, which is unrealistic in the open-vocabulary reasoning in real-world data. Furthermore, compared to 2D data, 3D assets are limited in diversity and scale, posing challenges for training large vision-language models (VLMs) on 3D-and-language data. Therefore, there's no large-scale 3D VLMs that can be directly used for concept grounding. On the contrary, there's much progress on large-scale 2D VLMs \cite{Radford2021LearningTV, Ramesh2021ZeroShotTG} thanks to the countless image-caption data on the internet. Since we obtain 3D compact representations from 2D multi-view images, it's natural to utilize 2D VLMs to ground semantic concepts on the 3D representations. Based on the CLIP model \cite{Radford2021LearningTV}, LSeg\cite{Li2022LanguagedrivenSS} manages to ground semantic concepts on each 2D pixel (and thus each ray $r$). We denote the feature of ray $r$ as $F(\boldsymbol{r})$.
% Since 3D voxel grids and 2D pixels are aligned via alpha compositing, we add one L1 loss to force the features of 3D voxel grids to align with the 2D LSeg pixels based on the alpha values. Specifically,

% \begin{equation}
% \mathcal{L}_{\text {feature}}=\sum_{i=1}^K T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right)(\|\boldsymbol{f}-F(\boldsymbol{r})\|)
% \end{equation}

% Assuming we have a set of concepts $P$, the similarities between a concept $\boldsymbol{p} \in P$ and a feature $\boldsymbol{f}$ is calculated as $\langle \boldsymbol{f}, \boldsymbol{p} \rangle$. We define a \textsc{Filter} operator. Specifically, the 3D compact representation for a semantic class $p$ after filtering out that class is:

% \begin{equation}
% \boldsymbol{V}_{\boldsymbol{p}} =  min(\langle\boldsymbol{V}^{(\mathrm{feature})}, \boldsymbol{p}\rangle, \boldsymbol{V}^{(\mathrm{density})}) 
% \end{equation}

% In practice, we only set the values of voxel grids with densities < 0.5 to 0, since we find that those points are irrelevant to the 3D geometry of the scene.

% To get each instance of the objects of the same category, we use DBSCAN \cite{Ester1996ADA} to implement the \textsc{Get\_Instance} operator which assigns clusters to all true values of $\boldsymbol{V}_{\boldsymbol{p}}$. The DBSCAN takes the 3D coordinates as input.




