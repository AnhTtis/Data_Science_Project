% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter t
he CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\newcommand{\model}{ICL\xspace}
\newcommand{\modelFull}{Interactive Cross-Modality Learner}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Large Pre-trained Models As \\ Interactive Few-shot Learners for Visual  Reasoning}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   %1. Existing method and challenges
   Large language models (LLMs) have demonstrated remarkable capacities for few-shot learning on natural language processing tasks. However, it remains largely unsolved for the few-shot cross-modality reasoning tasks, which require a model to perceive relative regions in images and perform multi-step reasoning to get the desired output.
   %2. Our method &  %3. core idea
   To fill this gap, we propose a novel framework named \modelFull~(\model) for few-shot cross-modality reasoning. \model contains two key components, a coarse-to-fine perception module and a self-consistent rationale module. The perception module adaptively selects the key regions in images that are corresponding to the task and transforms them into text context for the pretrained LLMs. The rationale module requires the LLMs to generate rationales for the output, verify the generated rationales with a cross-modality classifier and make sure that the rationales could further infer the predicted output consistently. 
    %4. Our benefit 1). Better performance 2); Better reasoning with rationales. 3).  IT d
   We conduct experiments on a range of reasoning tasks, including science question answering, visual abductive reasoning, and causal video question answering. We found our \model enjoys several benefits, 1). it achieves better performance than the previous baselines with LLMs for cross-modality reasoning; 2). it increases LLMs' transparency and trustworthiness with rationales for each predicted output; 3). it requires no need for computation-intensive parameter optimization compared with finetuning baselines.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Few-shot cross-modality learning~\ref{} is an important research topic in both computer vision and natural language processing.
Given only a few cross-modality demonstrations (\eg several images and their corresponding question-answering pairs), a model is required to watch the images and read the text to generate the desired output. Such capability to perform cross-modality reasoning from several examples is highly related to human cognition~\ref{}. It also has broad applications such as chatbot,  robot navigation~\ref{} and cross-modality retrieval~\ref{}.

%  Existing works for few-shot modality reasoning. 1). Fine-tuning a foundation model. Computation intensive and requires a finetuned model for each downstream tasks; 
Inspired by large language models (LLMs)'~\ref{} great success on few-shot natural language processing reasoning, researchers have been trying to adopt LLMs for cross-modality reasoning on both vision and text. Existing works can be mainly summarized into two categories. The first category of existing works~\ref{} uses the pretrained LLMs as the startpoint, adds additional visual perception modules to transform the visual inputs into latent inputs for LLMs, and finetunes the models with the given cross-modality examples. Although such a pipeline could achieve high performance on the downstream cross-modality tasks, it either requires a large cross-modality dataset to pre-train a large cross-modality model~\ref{} or requires finetuning each downstream task with an individual model~\ref{}, which are typically computational-intensive and time-consuming.     

\begin{figure}[t]
  \centering
  \fbox{\rule{0pt}{2.5in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \caption{Motivation of our model.}
   \label{fig:teaser}
\end{figure}

%2). Learning in context for LLMs and updates LLMs' prompts for visual question answering. Limitations: No interaction and black box without rationales.
To avoid computational-intensive finetuning for large language models, Yang \etal~\ref{} propose a model named PICa for few-shot visual question answering, which converted images into textual prompts for GPT3 models and asks GPT3 to answer the questions. Despite their high performance on  knowledge-based visual question answering, their model has several limitations. First, the captioning processing in PICa is independent of the question's semantic, limiting their performance on acquiring important visual context from images. Second, their pipeline has no rationale reasoning mechanism, leaving the question-answering a black-box process. 

% As shown in figure~\ref{}, we propose a powerful model named.
To this end, we propose a novel framework named \modelFull~(\model) for cross-modality reasoning. As shown in Figure~\ref{}, our model contains two key components, a coarse-to-fine perception module and a self-consistent rationale module. Given a visual-textual pair, our model first the perception module to select important visual regions corresponding to the given task and transform these regions into into textual descriptions to prompt the large large models. Moreover, to provide more transparent and trustworthy cross-modality reasoning, we introduce a self-consistent module for rationale reasoning. Specifically, we first require the large language model to generate rationales for the predicted outputs. We then estimate the matching similarity between these rationales and the given visual input with a cross-modality classifier~\ref{}. Finally, the selected rationale is fed to the prompt to make sure that the rationale could infer the same output consistently.  

% Compared with existing models, our model enjoys a lot of benefits.
To summarize, we introduce \model, a novel few-shot cross-modality visual reasoning framework that contains a perception module that captures visual context related to the given task in a coarse-to-fine manner, and a rationale module that produces rationales consistent with both the predicted output and visual context. Such a framework has two major advantages. First, extensive experiments are conducted on a range of cross-modality reasoning tasks, including science question answering,  visual abductive reasoning, and causal video question answering and found that \model achieves better performance than the baseline models. Moreover, Our model provides consistent reasoning rationales for each output prediction, enjoying more transparency and trustworthiness than previous methods. 

\section{Experiments}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c}
        \toprule
        Methods & Direct Answer & Multiple-Choice & Rationale \\
        \midrule
        CoT  &  &  & \\
        Pica &  &  & \\
        Ours & & & \\
        \bottomrule
    \end{tabular}
    \caption{AOK-VQA}
    \label{tab:my_label}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
         Methods & Direct Answer & Rationale \\
         \midrule
         CoT  & &  \\
         Pica & &  \\
         Ours & &  \\
         \bottomrule
    \end{tabular}
    \caption{OK-VQA}
    \label{tab:my_label}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
         Methods & Direct Answer & Multiple-Choice \\
         \midrule
         CoT  &  &   \\
         Pica OPT-66B &  &   \\
         Ours  & &  \\
         \bottomrule
    \end{tabular}
    \caption{CausalVQA}
    \label{tab:my_label}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
         Methods & Direct Answer & Multiple-Choice \\
         \midrule
         CoT  &  &   \\
         Pica &  &   \\
         Ours & &  \\
         \bottomrule
    \end{tabular}
    \caption{Science-QA}
    \label{tab:my_label}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
         Methods & Direct Answer & Multiple-Choice \\
         \midrule
         CoT  &  &   \\
         Pica &  &   \\
         Ours & &  \\
         \bottomrule
    \end{tabular}
    \caption{Sherlock}
    \label{tab:my_label}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        \toprule
         Methods & Direct Answer & Multiple-Choice \\
         \midrule
         CoT  &  &   \\
         Pica &  &   \\
         Ours & &  \\
         \bottomrule
    \end{tabular}
    \caption{Web-QA}
    \label{tab:my_label}
\end{table}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
