% \documentclass[10pt,twocolumn,letterpaper]{article}

% %%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version

% % \input{math_commands.tex}

% \usepackage{url}
% \usepackage{graphicx}
% \usepackage{multirow}
% \usepackage{subcaption}
% % \usepackage{dblfloatfix}
% \usepackage{booktabs}
% \usepackage{colortbl}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{cite}

% % Include other packages here, before hyperref.

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
% \usepackage{cleveref}
% \crefname{section}{§}{§§}
% \Crefname{section}{§}{§§}

% \def\cvprPaperID{6727} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}
% %\setcounter{page}{4321} % For final version only


% \begin{document}

%%%%%%%%% TITLE
\onecolumn
\appendix

\begin{center}
	{
		\Large{\textbf{Supplementary Material for \\``3D Concept Learning and Reasoning from Multi-View Images"}}
	}
\end{center}
    % \title{Supplementary Material for \\``3D Concept Learning and Reasoning from Multi-View Images"}
    
    % \author{First Author\\
    % Institution1\\
    % Institution1 address\\
    % {\tt\small firstauthor@i1.org}
    % % For a paper whose authors are all at the same institution,
    % % omit the following lines up until the closing ``}''.
    % % Additional authors and addresses can be added with ``\and'',
    % % just like the second author.
    % % To save space, use either the email address or home page, not both
    % \and
    % Second Author\\
    % Institution2\\
    % First line of institution2 address\\
    % {\tt\small secondauthor@i2.org}
    % }
    
    %  \maketitle


%%%%%%%%% ABSTRACT

%%%%%%%%% BODY TEXT

% comment it if you do not like!
{
    \hypersetup{linkcolor=black}
    \tableofcontents
}
\clearpage




% {\small
% \bibliographystyle{ieee_fullname}
% % \bibliography{egbib}
% \bibliography{cvprbib}
% }

% \noindent In Section \ref{sec:dataset}, we provide more details about the dataset, including question type distribution and question length distribution. We also provide three more examples on our dataset.\\
% \noindent In Section \ref{sec:details}, we provide more implementation details 

%\newpage
\section{Dataset}
\label{sec:dataset}
\subsection{Dataset Statistics}
Figure \ref{fig:statistics}, we show some statistics about our dataset. From Figure \ref{fig:statistics}(a), we can see that relation type takes up the most portion of the questions, which is reasonable since our dataset focuses on 3D reasoning, and spatial relation is a crucial perspective. From Figure \ref{fig:statistics} (b), we can see that our questions cover a wide range of word lengths.
\begin{figure}[htbp]
\centering
\begin{subfigure}{.35\textwidth}
\includegraphics[width=\linewidth]{figures/dist.pdf}
\caption{Distribution of Question Type}
\label{fig:type}
\end{subfigure}\hspace{0.\textwidth}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\linewidth]{figures/length.pdf}
\caption{Distribution of Question Word Length}
\label{fig:length}
\end{subfigure}
\caption{Dataset Statistics}
\label{fig:statistics}
\end{figure}

\subsection{More Dataset Examples}
In Figure \ref{fig:examples}, we show some more examples of our 3DMV-VQA dataset.
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/supp_teaser.pdf}
\vspace{-3mm}
\caption{More Examples of 3DMV-VQA Dataset.}
\label{fig:examples}
\end{figure*}

\section{Implementation details}
\label{sec:details}
\subsection{Reasoning Operators}
\begin{itemize}
\item \textsc{Filter}
The \textsc{Filter} operator takes a voxel grid and a concept as input, and outputs the filtered voxel grid where points do not belong to the input have zero density values.

\item \textsc{Get\_Instance}
The \textsc{Get\_Instance} operator takes a voxel grid as input and outputs a list of voxel grids of different instances. We use DBSCAN, an unsupervised algorithm to assign the voxel grid points with densities greater than 0.5 into different instances. If the input is of the same semantic object, we directly use DBSCAN to get the instances. Else, we first get the voxel grids of different classes from semantic concept grounding, and then get the instances of all the instances of each semantic class. After that we integrate the instances of all semantic classes.

\item \textsc{Query}
The \textsc{Query} operator takes the voxel grid of an instance and returns the semantic concept of the instance.

\item \textsc{Count}
The \textsc{Count} operator takes a list of voxel grids as input and returns the length of the list.

\item \textsc{Exist}
The \textsc{Exist} operator takes a list of voxel grids as input and examines if the list is empty or not. If the list is empty, then the targeted concept doesn't exist.

\item \textsc{Get\_Room\_Instance}
The \textsc{Get\_Room\_Instance} operator takes a voxel grid as input and returns a list of new voxel grids of different room instances.
To get the instances of the rooms, we use the results of 3D semantic grounding and extract all walls. We then 
 segment the whole scene into rooms using the wall instances. 

\item \textsc{Filter\_Room}
The \textsc{Filter\_Room} operator takes a list of voxel grids of different room instances and returns another list of voxel grids where there are non-zero density values.

\item \textsc{Count\_Room}
The \textsc{Count\_Room} operator takes a list of voxel grids of different room instances and returns the length of the list of voxel grids where there are non-zero density values. It's a combination of the \textsc{Filter\_Room} operator and the \textsc{Count} operator.

\item \textsc{Exist\_Room}
The \textsc{Exist\_Room} operator takes a list of voxel grids of different room instances and returns whether the list of voxel grids where there are non-zero density values is empty or not. It's a combination of the \textsc{Filter\_Room} operator and the \textsc{Exist} operator.

\item \textsc{Relation}
The \textsc{Relation} operator takes a relation tuple (the voxel grids of two or three instances) and a relation, concatenate them and pass them into the relation module network of the specified relation, and outputs a score (which can be turned into True/False value according to whether the score is greater than 0.5 or not ) indicating whether the two/three instances have the relation or not.

\item \textsc{Filter\_Relation}
The \textsc{Filter\_Relation} operator takes two/three lists of voxel grids of different semantic classes and a specified relation. For all possible tuples chosen from the lists, each containing the two or three instances of different semantic classes, we pass the concatenated tuple into the relation module network and collects all True/False values. We filter out all tuples with true values and returns a list of the concatednated voxel grids.

\item \textsc{Exist\_Relation}
The \textsc{Exist\_Relation} operator takes two/three lists of voxel grids of different semantic classes and a specified relation. For all possible tuples chosen from the lists, each containing the two or three instances of different semantic classes, we pass the concatenated tuple into the relation module network and collects all True/False values. We examine whether there's a tuple with true value. It's a combination of the \textsc{Filter\_Relation} operator and the \textsc{Exist} operator.

\item \textsc{Count\_Relation}
The \textsc{Count\_Relation} operator takes two/three lists of voxel grids of different semantic classes and a specified relation. For all possible tuples chosen from the lists, each containing the two or three instances of different semantic classes, we pass the concatenated tuple into the relation module network and collects all True/False values. We count how many tuples with true values we have. It's a combination of the \textsc{Filter\_Relation} operator and the \textsc{Count} operator.

\item \textsc{Relation\_More}
The \textsc{Relation\_More} operator takes a specified relation in the comparison form (\textit{e.g.,} closer, more left), a first voxel grid, together with two second voxel grids for comparison. For each of the two second voxel grids, it's concatenated with the first voxel grid and input into the relation network. Then we output the voxel grid with the higher score value. 

\item \textsc{Relation\_Most}
The \textsc{Relation\_Most} operator takes a specified relation in the comparison form (\textit{e.g.,} closest, leftmost), a first voxel grid, together with a list of second voxel grids for comparison. For each of the second voxel grids, it's concatenated with the first voxel grid and input into the relation network. Then we output the voxel grid with the higher score value. 

\item \textsc{Larger\_than}
The \textsc{Larger\_than} operator takes as input two integers and returns whether the first integer is greater than the second.

\item \textsc{Smaller\_than}
The \textsc{Smaller\_than} operator takes as input two integers and returns whether the first integer is smaller than the second.

\end{itemize}

\subsection{Baselines}
\paragraph{CNN-LSTM, MAC \& MAC(V)} We use an ImageNet-pretrained ResNet-50 to extract 14 × 14 × 1024 feature maps for MAC and MAC(V). We use the 2048-dimensional feature
from the last pooling layer. 
% 
\paragraph{3D-Feature + LSTM} We first use PCA to downgrade the feature size from 512 to 16. We then pass the the 3D-feature through three 3D-CNN (implemented by sparse convolutions) layers with intermediate size 64, to further downsample. We then concatenate this 3D feature with LSTM language feature and into them into a MLP to get the final answer. 

\paragraph{ALPRO} We end-to-end finetune ALPRO model on our dataset with the pre-trained checkpoint for 10 epochs. Our settings for finetuning is the same as the finetuning configurations for MSRVTT-QA in ALPRO paper. We take the 1500 answer candidates of MSRVTT-QA, and replace some irrelavent candidates with answers appear in our dataset. During inference, the model take as input image frames of shape $224\times224$, and output a set of classification probabilities of 1500. The predictions are obtained as the answer with the highest probability.  

\paragraph{LGCN} We take our QA-pairs as "FrameQA" task splited in LGCN paper on TGIF-QA dataset. For a QA-pair, we take image frames, each extracting 5 bounding boxes and their regional features of 1024-dim using MaskRCNN, and embed the question by word level and character level the same way as mentioned LGCN paper. Other settings for training and inference use same as FrameQA configurations.

\paragraph{NS-VQA} We first use CLIP-LSeg to get per-pixel semantic label to perform semantic concept grounding. After Filtering with certain concepts, all the pixels that do not belong to the concepts are set to white-transparent pixels. For counting problems, we also use DBSCAN which takes the pixel x-y values concatenated by their color values as input, and assign instance clusters to all the non-transparent pixels. For training of the relation network, we use pretrained ResNet-50 to get the 2D features of the images with filtered instances, concatenate them with language features, and then go through one MLP to output a score. We ``maxpool" the predictions in each image. For example, for concept questions that ask about whether there's a semantic class in the scene, we iterate through all images and get the prediction, if there's a prediction ``yes" in one of the images then the final prediction is also ``yes". For counting problems, we also iterate through and get integer predictions, and we get the largest prediction among all the images as our final prediction. For query problems, we iterate through all images and get the predictions which are concepts, we choose the concept which appears most frequently among the images.

\section{Experiments}
\label{sec:exp}
 


\subsection{Generalization Results}
\subsubsection{Generalization to Replica}
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/replica.pdf}
\caption{Qualitative examples of generalizing to Replica dataset.}
\label{fig:replica}
\end{figure*}
\paragraph{Result Analysis} To further show that 3D-CLR trained on HM3D can be generalized to new reasoning datasets, we further collect a small visual question answering dataset on Replica \cite{Straub2019TheRD} with Habitat following the same data generation as HM3D, and with the same question type distribution as in Figure \ref{fig:type}. Table \ref{tab:replica} shows the results. We can see that 3D-CLR can maintain the performance on Replica as it performs on HM3D, which shows 3D-CLR's good generalization ability. Specifically, 3D-CLR and NS-VQA can maintain the performance on the conceptual questions, suggesting that CLIP-LSeg is able to perform semantic concept grounding on new concepts in the new dataset. Moreover, we see that the performance on counting problem is even better than that of HM3D, this is probably because Replica scenes are simpler and contains fewer details about tiny objects, thus making instance segmentation easier. As for relational problems, 3D-CLR can also maintain good results, showing that the relation networks training on HM3D can also be utilized on other datasets and further suggesting that the vocabulary of relations is limited yet general across all scenes, and can be learned from scratch.
\begin{table}[htbp]
	\begin{center}
	\begin{tabular}{lccccc}
	\toprule
      Methods   & Concept & Counting & Relation& Comparison \\ 

        MAC &55.7&16.4&40.9&58.8\\    
        MAC(V) &54.1&17.4&41.2&60.8\\
        NS-VQA &57.2&18.7&30.4&62.3\\ 

        \midrule
        3D-CLR  & \textbf{65.3} & \textbf{45.1} &\textbf{53.6}& \textbf{73.5}\\ 

    \bottomrule
	\end{tabular}
	\end{center}
 \vspace{-5mm}

	\caption{Question-answering accuracy of 3D visual reasoning baselines on different question types when generalizing to Replica dataset.}

	\label{tab:replica}
\end{table}
\paragraph{Qualitative Examples} In Figure \ref{fig:replica}, we show some qualitative examples of generalizing to Replica scenes. From the examples, we can come to several conclusions. First, 3D-CLR can perform zero-short semantic concept grounding on unseen concepts in HM3D, such as ``projector screen" and capture relations such as ``between". Second, it still performs poorly in counting questions. In the example on the left, it cannot count the instances of ``chairs" and on the right, it cannot count the instances of ``pillows". This is because the objects are ``sticked" to each other and are not separated in space. Therefore, DBSCAN cannot tell the objects apart.

\subsubsection{Generalization to Unseen Concepts}
To demonstrate 3D-CLR's zero-shot concept grounding ability and generalization ability, we generate some more question-answer pairs on unseen concepts. Recall that in the dataset section in the main paper, we would merge some concepts with similar concepts (\textit{e.g.,} ``stuffed animal" with ``toy"). To generate the datasets with unseen concepts, we use these merged concepts instead of the concepts in the proposed 3DMV-VQA dataset. We also append some unseen concepts manually to the new dataset. We assure that there are no overlapping words between the seen concepts and unseen concepts. Table \ref{tab:unseen} shows the generalization result. We can see that 3D-CLR and NS-VQA can still have good results on the conceptual problems, suggesting that they could perform zero-shot concept grounding. However, MAC and MAC(V) has very poor performances, much worse than the performances than HM3D. This suggests that the modular design and incorporation of CLIP-LSeg equips 3D-CLR with zero-shot generalization ability.

\begin{table}[htbp]
	\begin{center}
	\begin{tabular}{lccccc}
	\toprule
      Methods   & Concept & Counting & Relation& Comparison \\ 

        MAC &51.3&15.6&36.2&53.5\\    
        MAC(V) &51.4&16.1&38.5&54.2\\
        NS-VQA &58.6&19.2&29.7&58.1\\ 

        \midrule
        3D-CLR  & \textbf{63.4} & \textbf{37.7} &\textbf{55.1}& \textbf{68.9}\\ 

    \bottomrule
	\end{tabular}
	\end{center}
  \vspace{-4mm}

	\caption{Question-answering accuracy of 3D visual reasoning baselines on different question types when generalizing to unseen categories.}
 \vspace{-4mm}
	\label{tab:unseen}
\end{table}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/qualitative_supp.pdf}
\caption{More Qualitative Examples on 3DMV-VQA.}
\label{fig:qual_supp}
\end{figure*}

\subsection{More Qualitative Examples on 3DMV-VQA}
In Figure \ref{fig:qual_supp}, we show more qualitative examples on our 3DMV-VQA dataset. As we can see, 3D-CLR can generalize well to unseen concepts like ``Christmas tree", and can perform well on counting problems if the instances are well apart from each other.


% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{11_references}
% }

% \end{document}
