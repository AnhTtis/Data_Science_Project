
\section{Experiments}
\label{sec:exp}

 \subsection{Experimental Setup}
% \gc{talk about training setup, evaluation metric, implementation details of your our method}
\noindent\textbf{Evaluation Metric.} We report the visual question answering accuracy on the proposed 3DMV-VQA dataset
w.r.t the four types of questions. The train/val/test split is 7:1:2. 

\noindent\textbf{Implementation Details} For 3D compact representations, we adopt the same architectures as DVGO, except skipping the coarse reconstruction phase and directly training the fine reconstruction phase. After that, we freeze the density voxel grid and color voxel grid, for the optimization of the feature voxel grid only. The feature grid has a world size of 100 and feature dim of 512. We train the compact representations for 100,000 iterations and the 3D features for another 20,000 iterations. For LSeg, we use the official demo model, which has the ViT-L/16 image encoder and CLIPâ€™s ViT-B/32 text encoder. We follow the official script for inference and use multi-scale inference. For DBSCAN, we use an epsilon value of 1.5, minimum samples of 2, and we use L1 as the clustering method. For the relation networks, each relation is encoded into a three-layer sparse 3D convolution network with hidden size 64. The output is then fed into a one-layer linear network to produce a score, which is normalized by sigmoid function. We use cross-entropy loss to train the relation networks, and we use the one-hop relational questions with ``yes/no" answers to train the relation networks.

\subsection{Baselines}
Our baselines range from vanilla neural networks, attention-based methods, fine-tuned from large-scale VLM, and graph-based methods, to neural-symbolic methods.
\begin{itemize}
[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em]
\item \textbf{LSTM}. The question is transferred to word embeddings which are input into a word-level LSTM \cite{Hochreiter1997LongSM}. The last LSTM hidden state is fed into a multi-layer perceptron (MLP) that outputs a distribution
over answers. This method is able to model question-conditional bias since it uses no image information.
\item \textbf{CNN+LSTM}. The question is encoded by the final hidden states from LSTM. We use a resnet-50 to extract frame-level features of images and average them over the time dimension. The
features are fed to an MLP to predict the final answer. This is a simple baseline that
examines how vanilla neural networks perform on 3DMV-VQA.
\item \textbf{3D-Feature+LSTM}. We use the 3D features we get from 3D-2D alignment and downsample the voxel grids using 3D-CNN as input, concatenated with language features from LSTM and fed to an MLP.
\item  \textbf{MAC} \cite{Hudson2018CompositionalAN}. MAC utilizes a Memory, Attention and Composition cell to perform iterative reasoning process. Like CNN+LSTM, we use the average pooling over multi-view images as the feature map. 

\item \textbf{MAC(V)}. We treat the multi-view images along a trajectory as a video. We modify the MAC model by applying a temporal attention unit across the video frames to generate a latent encoding for the video.
\item \textbf{NS-VQA}\cite{Yi2018NeuralSymbolicVD}. This is a 2D version of our 3D-CLR model. We use CLIP-LSeg to ground 2D semantic concepts from multi-view images, and the relation network also takes the 2D features as input. We execute the operators on each image and max pool from the answers to get our final predictions.
\item \textbf{ALPRO} \cite{Li2022AlignAP}. ALPRO is a video-and-language pre-training framework. A transformer model is pretrained  on large webly-source video-text pairs and can be used for downstream tasks like Video Question answering.
\item \textbf{LGCN} \cite{Huang2020LocationAwareGC}. LGCN represents the contents in the video as a location-aware graph by incorporating the location information of an
object into the graph construction.
% \item Human Evaluation
\end{itemize}

\subsection{Experimental Results}

\begin{table*}[t]
	\begin{center}\small
 
	\begin{tabular}{lccccc}
	\toprule
      Methods   & Concept & Counting & Relation& Comparison & Overall\\ 
     
    \midrule
        Q-type (rand.)  &49.4 &10.7 &21.6 & 49.2 &26.4\\ 
        Q-type (freq.)  &50.8 &11.3  &23.9 &50.3 &28.2\\
        LSTM           &53.4&15.3&24.0&55.2 &29.8\\
        \midrule
        CNN+LSTM  &57.8&22.1&35.2&59.7 &37.8\\ 
        MAC &62.4&19.7&47.8&62.3 &46.7\\    
        MAC(V) &60.0&24.6&51.6&65.9 &50.0\\
        NS-VQA &59.8&21.5&33.4&61.6 &38.0\\ 
        ALPRO &65.8 &12.7&42.2&68.2 &43.3\\
        LGCN &56.2&19.5&35.5&66.7 &39.1\\
        3D-Feature+LSTM  &61.2 &22.4 & 49.9 & 61.3 &48.2\\ 
        \midrule
        3D-CLR (Ours)  & \textbf{66.1} & \textbf{41.3} &\textbf{57.6}& \textbf{72.3} &\textbf{57.7}\\ 

    \bottomrule
	\end{tabular}
	\end{center}
	\vspace{-13pt}
	\caption{Question-answering accuracy of 3D visual reasoning baselines on different question types.}
	\vspace{-12pt}
	\label{tab:reasoning}
\end{table*}

\noindent\textbf{Result Analysis.} We summarize the performances for each question type of baseline models in Table \ref{tab:reasoning}. All models are trained on the training set until convergence, tuned on the validation set, and evaluated on the test set. We provide detailed analysis below.


First, for the examination of language-bias of the dataset, we find that the performance of LSTM is only slightly higher than random and frequency, and all other baselines outperform LSTM a lot. This suggests that there's little language bias in our dataset. Second, we observe that encoding temporal information in MAC (\textit{i.e.,} MAC(V)) is better than average-pooling of the features, especially in counting and relation. This suggests that average-pooling of the features may cause the model to lose information from multi-view images, while attention on multi-view images helps boost the 3D reasoning performances. Third, we also find that fine-tuning on large-scale pretrained model (\textit{i.e.,} ALPRO) has relatively high accuracies in concept-related questions, but for counting it's only slightly higher than the random baseline, suggesting that pretraining on large-scale video-language dataset may improve the model's perception ability, but does not provide the model with the ability to tackle with more difficult reasoning types such as counting. Next, we find that LGCN has poor performances on the relational questions, indicating that building a location-aware graph over 2D objects still doesn't equip the model with 3D location reasoning abilities. Last but not least, we find that 3D-based baselines are better than their 2D counterparts. 3D-Feature+LSTM performs well on the 3D-related questions, such as counting and relation, than most of the image-based baselines. Compared with 3D-CLR, NS-VQA can perform well in the conceptual questions. However, it underperforms 3D-CLR a lot in counting and relation, suggesting that these two types of questions require the holistic 3D understanding of the entire 3D scenes. Our 3D-CLR outperforms other baselines by a large margin, but is still far from satisfying. From the accuracy of the conceptual question, we can see that it can only ground approximately 66\% of the semantic concepts. This indicates that our 3DMV-VQA dataset is indeed very challenging.

\noindent\textbf{Qualitative Examples.} In Fig. \ref{fig:qualitative}, we show four qualitative examples. From the examples, we show that our 3D-CLR can infer an accurate 3D representation from multi-view images, as well as ground semantic concepts on the 3D representations to get the semantic segmentations of the entire scene.  Our 3D-CLR can also learn 3D relationships such as ``close", ``largest", ``on top of" and so on. However, 3D-CLR also fails on some questions. For the third scene in the qualitative examples, it fails to ground the concepts ``mouse" and ``printer". Also, it cannot accurately count the instances sometimes. We give detailed discussions below. 
\begin{figure*}[t]
\centering
%\includegraphics[width=\linewidth]{figures/qual_v5.pdf}
\includegraphics[width=\linewidth]{figures/qualitative_v2_1.pdf}
\vspace{-6mm}
\caption{Qualitative examples of our 3D-CLR. We can see that 3D-CLR can ground most of the concepts and answer most questions correctly. However, it still fails sometimes, mainly because it cannot separate close object instances and ground small objects. }
\vspace{-4mm}
\label{fig:qualitative}
\end{figure*}

% \begin{figure}[t]
% \centering
% \includegraphics[width=\linewidth]{figures/qual_v2.pdf}
% \caption{}
% \label{fig:qualitative}
% \end{figure}

\subsection{Discussions}
\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{figures/abs_bar.png}
\vspace{-3mm}

\caption{Model diagnosis of our 3D-CLR. }
\vspace{-5mm}

\label{fig:ablative}
\end{figure}


We perform an in-depth analysis to understand the challenge of this dataset. We leverage the modular design of our 3D-CLR, replacing individual components of the framework with ground-truth annotations for model diagnosis. The result is shown in Fig \ref{fig:ablative}. 3D-CLR w/ Semantic denotes our model with ground-truth semantic concepts from HM3DSem annotations. 3D-CLR w/ Instance denotes that 
we have ground-truth instance segmentations of semantic concepts. From Fig.~\ref{fig:qualitative} and Fig.~\ref{fig:ablative}, we summarize several key challenges of our benchmark:

\noindent \textbf{Very close object instances} From Fig.~\ref{fig:ablative}, we can see that even with ground-truth semantic labeling of the 3D points, 3D-CLR still has unsatisfying results on counting questions. This suggests that the instance segmentations provided by DBSCAN are not accurate enough. From the top two qualitative examples in Fig.~\ref{fig:qualitative}, we can also see that if two chairs contact each other, DBSCAN will not tell them apart and thus have poor performance on counting. One crucial future direction is to improve unsupervised instance segmentations on very close object instances.

\noindent \textbf{Grounding small objects}
Fig.~\ref{fig:ablative} suggests that 3D-CLR fails to ground a large portion of the semantic concepts, which hinders the performance. From the last example in Fig. ~\ref{fig:qualitative}, we can see that 3D-CLR fails to ground small objects like ``computer mouse". Further examination indicates there are two possible reasons: 1) CLIP-LSeg fails to assign the right features to objects with limited pixels; 2) The resolution of feature voxel grid is not high enough and therefore small objects cannot be represented in the compact representation. An interesting future direction would be learning exploration policies that enable the agents to get closer to uncertain objects that cannot be grounded.
% \gc{talk about learning policy to get closer look at uncertain objects.}

\noindent \textbf{Ambiguity on 3D relations} 
Even with ground-truth semantic and instance segmentations, the performance of the relation network still needs to be improved. We find that most of the failure cases are correlated to the ``inside" relation. From the segmentations in Fig.~\ref{fig:qualitative}, we can see that 3D-CLR is unable to ground the objects in the cabinets. A potential solution can be joint depth and segmentation predictions. 
% \noindent\textbf{Ground small objects}

% \noindent\textbf{Sefig:ablativete very close object instances}

% \noindent\textbf{Ambiguity on 3D relations}

% \subsection{Discussions}





