\input{_constants}
% \review % \review OR \arxiv OR \cameraready
\arxiv
\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}
\input{cvpr_header}
\begin{document}
%% TITLE
\title{3D Concept Learning and Reasoning from Multi-View Images}
%%

\author{Yining Hong\textsuperscript{1},  Chunru Lin\textsuperscript{2}, Yilun Du\textsuperscript{3}, \\ Zhenfang Chen\textsuperscript{5}, Joshua B. Tenenbaum\textsuperscript{3}, Chuang Gan\textsuperscript{4, 5}
 \\ 
\textsuperscript{1}UCLA, \textsuperscript{2,4}Shanghai Jiaotong University, \textsuperscript{3}MIT CSAIL,\\
\textsuperscript{4}UMass Amherst, \textsuperscript{5}MIT-IBM Watson AI Lab\\
\url{https://vis-www.cs.umass.edu/3d-clr/}
% \texttt{xuanli1@math.ucla.edu, yilingq@umd.edu, pyc@csail.mit.edu,}\\
% \texttt{jkrishna@mit.edu, lin@cs.unc.edu, cffjiang@math.ucla.edu,}\\ \texttt{chuangg@umass.edu}
}


\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
%\centering
    \includegraphics[width=\linewidth]{figures/teaser_v6.pdf}
    \vspace{-0.1in}
    \captionof{figure}{An exemplar scene with multi-view images and question-answer pairs of our 3DMV-VQA dataset. 3DMV-VQA contains four question types: concept, counting, relation, comparison. \textcolor{orange}{Orange} words denote semantic concepts; \textcolor{blue}{blue} words denote the relations.} \label{fig:teaser}
    \vspace{0.15in}
\end{center}
}]



\input{00_abstract}
\vspace{-3mm}
Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world.  Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA). This dataset is collected by an embodied agent actively moving and capturing RGB images in an environment using the Habitat simulator.  In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models for visual reasoning on our benchmark and find that they all perform poorly. We suggest that a principled approach for 3D reasoning from multi-view images should be to infer a compact 3D representation of the world from the multi-view images, which is further grounded on open-vocabulary semantic concepts, and then to execute reasoning on these 3D representations. As the first step towards this approach, we propose a novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly combines these components via neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Experimental results suggest that our framework outperforms baseline models by a large margin, but the challenge remains largely unsolved. We further perform an in-depth analysis of the challenges and highlight potential future directions. 


\input{01_intro}
\input{02_related}
\input{03_dataset}
\input{04_method}

\input{05_exp}

\input{10_conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{11_references}
}

\ifarxiv \clearpage \input{12_appendix} \fi

\end{document}
