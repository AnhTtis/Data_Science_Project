\section{Dataset Generation}
\label{sec:dataset}

\subsection{Multi-View Images}

Our dataset includes 5k 3D scenes from the Habitat-Matterport 3D  Dataset (HM3D) dataset \cite{Ramakrishnan2021HabitatMatterport3D}, and approximately 600k images rendered from the 3D scenes. The images are rendered via Habitat \cite{szot2021habitat, habitat19iccv}. 

\noindent\textbf{Scene Generation} 
We build our benchmark on top of the HM3DSem dataset\cite{yadav2022habitat}, which is a large-scale dataset of 3D real-world indoor scenes with densely annotated semantics. It consists of 142,646 object instance annotations across 216 3D spaces and 3,100 rooms within those spaces. HM3D dataset uses texture information to annotate pixel-accurate object boundaries, which provides large-scale object annotations and ensures the scale, quality, and diversity of 3D visual reasoning questions of our benchmark.

To construct a benchmark that covers questions of different difficulty levels, it's crucial that we include 3D scenes of different scales in our benchmark. We start with single rooms in HM3D scenes, which has an appropriate amount of semantic concepts and relationships to base some simple questions on.
To get the scale of single rooms, we calculate bounding boxes of rooms according to floor instance segmentations. We then proceed to generate bounding boxes for scenes with multiple adjacent rooms. For more complex holistic scene understanding, we also include whole-house scenes, which may contain tens of rooms. Overall, the 3DMV-VQA benchmark contains three levels of scenes (2000 single-room scenes, 2000 multi-room scenes and 100 whole-house scenes).
% The MVR universe contains three levels of scenes (2000 single room scenes, 2000 multi room scenes and 100 whole house scenes) based on HM3DSem dataset \cite{yadav2022habitat}. 

% When constructing the scene, we first load geometries and semantics of 100 annotated scenes of whole houses in HM3DSem dataset. To get the range of single rooms, we calculate bounding boxes of rooms according to floor instance segmentation. We pick adjacent 2-3 rooms to form multi rooms. 

\noindent\textbf{Image Rendering}
After we get the bounding box of each scene, we load the scene into the Habitat simulator. We also put a robot agent with an RGB sensor at a random initial point in the bounding box. The data is collected via exploration of the robot agent. Specifically, at each step of the data collection process, we sample a navigable point and make the agent move to the point along the shortest path. When the agent has arrived at a point, we rotate the agent $30^{\circ} $ along z-axis for 12 times so that the agent can observe the $360^{\circ}$ view of the scene at the position. It can also look up and down, with a random mild angle from [$-10^{\circ}$,$10^{\circ}$] along the x-axis. A picture is taken each time the agent rotates to a new orientation. In total 12 pictures are taken from each point. While traveling between points, the robot agent further takes pictures. We also exploit a policy such that when the camera is too far from or too close to an object and thus the agent cannot see anything, we discard the bad-view images.  
% With rooms grounded, we load the scene into Habitat simulator and put an agent with a RGB sensor at a random initial position in the range. To start data collection, we sample a random navigable point and let agent move to the point along the shortest path, taking pictures every agent step. Then repeat the random walk until we collect enough data. Moreover, to ensure every corner seen, we rotate the agent $30^{\circ} $ along z-axis, with a random mild angle from [$-10^{\circ}$,$10^{\circ}$] up and down, for 12 times at a position with a certain probability. We also exploit a policy to discard bad-view images, where camera is too far from or too close to an object, to get better NeRF reconstruction results. To be more concrete, we use a depth sensor to get $d_{min}, d_{max}, d_{mean}$, the minimal, maximal and average distance from the camera to all the captured objects. We just keep images that satiesfy: 
% \begin{equation*}
%   \begin{cases}
%     0.2 \le & d_{min}\ \ \ \ \ \ \ \ \ \ \ \ , \\ 
%     & d_{mean} < 1.0, \\ 
%     1.5 \le & d_{max}\ \ \le 4.0.
%   \end{cases}
% \end{equation*}

%For convenience, we over-render 1000 images from every room and provide lists  for room level data. For single room level, we randomly sample 500 images with camera poses. For multi room level, we still randomly sample roughly even images from each room, and combine to 1000 images with camera poses. For whole house level, we uniformly sample navigable points and render 1000 new images each.  

\subsection{Questions and Answers}
 We pair each scene with machine-generated questions from pre-defined templates. All questions are open-ended and can be answered with a single word (samples in Fig. \ref{fig:teaser}).
 
\noindent\textbf{Concepts and Relationships} To generate questions and answers, we utilize the semantic annotations of HM3DSem\cite{yadav2022habitat} to get the semantic concepts and their bounding boxes, as well as the bounding boxes of the rooms.  We merge semantic concepts with similar meanings (\textit{e.g.,}, L-shaped sofa to sofa, desk chair / computer chair {e.g.} to chair).  We also define 11 relationships: inside, above, below, on the top of, close, far, large, small, between, on the left, and on the right. Before generating questions, we first generate a scene graph for each scene containing all concepts and relationships.

\noindent\textbf{Question Types}
We define four types of questions: concept, counting, relation and comparison.
\vspace{-2mm}
\begin{itemize}
[align=right,itemindent=0em,labelsep=2pt,labelwidth=1em,leftmargin=*,itemsep=0em] 
\item \textbf{Concept.} Conceptual questions query whether there's an object of a certain semantic concept in the scene, or whether there's a room containing the objects of the semantic concept.
\item \textbf{Counting.} Counting-related questions ask about how many instances of a semantic concept are in the scene, or how many rooms contain objects of the semantic concept.
\item \textbf{Relation.} Relational questions ask about the 11 relationships and their compositions. Based on the number of relations in a question, we have one-hop to three-hop questions for the relation type.
\item \textbf{Comparison.} The comparison question type focuses on the comparison of two objects, two semantic concepts or two rooms. It can be combined with the relational concepts to compare two objects (\textit{e.g.,} larger, closer to, more left \textit{etc}). It also compares the number of instances of two semantic concepts, or the number of objects of certain concepts in different rooms.
\end{itemize}

\noindent\textbf{Bias Control.} Similar to previous visual reasoning benchmarks \cite{Johnson2017CLEVRAD, hong20223d}, we use machine-generated questions since the generation process is fully controllable so that we can avoid dataset bias. Questions are generated from pre-defined templates, and transformed into natural
language questions with associated semantic concepts and relationships from the scene.    We manually define 41 templates for question generation. We use depth-first search to generate questions. We perform bias control based on three perspectives: template counts, answer counts, and concept counts. For selecting templates, we sort the templates each time we generate a question to ensure a balanced question distribution. We force a flat
answer distribution for each template by rejection sampling. Specifically, once we generate a question and an answer, if the number of the questions having the same answer and template is significantly larger than other answers, we discard it and continue searching. Once we find an answer that fits in the ideal answer distribution, we stop the depth-first searching for this question. We also force a flat concept distribution for each template using the same method. In addition to controlling the number of concepts mentioned in the templates, we also control the number of relation tuples consisting of the same concept sets. 