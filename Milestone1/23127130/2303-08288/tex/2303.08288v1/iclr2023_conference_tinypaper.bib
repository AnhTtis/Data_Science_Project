@inproceedings{Jawahar2019-ed,
  address   = {Florence, Italy},
  author    = {Jawahar, Ganesh  and
               Sagot, Beno{\^\i}t  and
               Seddah, Djam{\'e}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  doi       = {10.18653/v1/P19-1356},
  pages     = {3651--3657},
  publisher = {Association for Computational Linguistics},
  title     = {What Does {BERT} Learn about the Structure of Language?},
  url       = {https://aclanthology.org/P19-1356},
  year      = {2019}
}

@article{Anil2022-wm,
  abstract      = {The ability to extrapolate from short problem instances to
                   longer ones is an important form of out-of-distribution
                   generalization in reasoning tasks, and is crucial when
                   learning from datasets where longer problem instances are
                   rare. These include theorem proving, solving quantitative
                   mathematics problems, and reading/summarizing novels. In
                   this paper, we run careful empirical studies exploring the
                   length generalization capabilities of transformer-based
                   language models. We first establish that naively finetuning
                   transformers on length generalization tasks shows
                   significant generalization deficiencies independent of model
                   scale. We then show that combining pretrained large language
                   models' in-context learning abilities with scratchpad
                   prompting (asking the model to output solution steps before
                   producing an answer) results in a dramatic improvement in
                   length generalization. We run careful failure analyses on
                   each of the learning modalities and identify common sources
                   of mistakes that highlight opportunities in equipping
                   language models with the ability to generalize to longer
                   problems.},
  archiveprefix = {arXiv},
  author        = {Anil, Cem and Wu, Yuhuai and Andreassen, Anders and
                   Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and
                   Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and
                   Neyshabur, Behnam},
  eprint        = {2207.04901},
  primaryclass  = {cs.CL},
  title         = {Exploring Length Generalization in Large Language Models},
  year          = {2022}
}

@inproceedings{Hu2020-za,
  address   = {Online},
  author    = {Hu, Jennifer  and
               Gauthier, Jon  and
               Qian, Peng  and
               Wilcox, Ethan  and
               Levy, Roger},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.acl-main.158},
  pages     = {1725--1744},
  publisher = {Association for Computational Linguistics},
  title     = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
  url       = {https://aclanthology.org/2020.acl-main.158},
  year      = {2020}
}

@article{Ma2022-sq,
  abstract      = {There is an ongoing debate on whether neural networks can
                   grasp the quasi-regularities in languages like humans. In a
                   typical quasi-regularity task, English past tense
                   inflections, the neural network model has long been
                   criticized that it learns only to generalize the most
                   frequent pattern, but not the regular pattern, thus can not
                   learn the abstract categories of regular and irregular and
                   is dissimilar to human performance. In this work, we train a
                   set of transformer models with different settings to examine
                   their behavior on this task. The models achieved high
                   accuracy on unseen regular verbs and some accuracy on unseen
                   irregular verbs. The models' performance on the regulars is
                   heavily affected by type frequency and ratio but not token
                   frequency and ratio, and vice versa for the irregulars. The
                   different behaviors on the regulars and irregulars suggest
                   that the models have some degree of symbolic learning on the
                   regularity of the verbs. In addition, the models are weakly
                   correlated with human behavior on nonce verbs. Although the
                   transformer model exhibits some level of learning on the
                   abstract category of verb regularity, its performance does
                   not fit human data well, suggesting that it might not be a
                   good cognitive model.},
  archiveprefix = {arXiv},
  author        = {Ma, Xiaomeng and Gao, Lingyu},
  eprint        = {2210.09167},
  primaryclass  = {cs.CL},
  title         = {How do we get there? Evaluating transformer neural networks
                   as cognitive models for English past tense inflection},
  year          = {2022}
}

@inproceedings{Edelman2022-jx,
  abstract  = {Self-attention, an architectural motif designed to model
               long-range interactions in sequential data, has driven numerous
               recent breakthroughs in natural language processing and beyond.
               This work provides a theoretical analysis of the inductive
               biases of self-attention modules. Our focus is to rigorously
               establish which functions and long-range dependencies
               self-attention blocks prefer to represent. Our main result shows
               that bounded-norm Transformer networks ``create sparse
               variables'': a single self-attention head can represent a sparse
               function of the input sequence, with sample complexity scaling
               only logarithmically with the context length. To support our
               analysis, we present synthetic experiments to probe the sample
               complexity of learning sparse Boolean functions with
               Transformers.},
  author    = {Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang,
               Cyril},
  booktitle = {Proceedings of the 39th International Conference on Machine
               Learning},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and
               Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  pages     = {5793--5831},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Inductive Biases and Variable Creation in {Self-Attention}
               Mechanisms},
  volume    = {162},
  year      = {2022}
}

@article{Garg2022-hl,
  abstract      = {In-context learning refers to the ability of a model to
                   condition on a prompt sequence consisting of in-context
                   examples (input-output pairs corresponding to some task)
                   along with a new query input, and generate the corresponding
                   output. Crucially, in-context learning happens only at
                   inference time without any parameter updates to the model.
                   While large language models such as GPT-3 exhibit some
                   ability to perform in-context learning, it is unclear what
                   the relationship is between tasks on which this succeeds and
                   what is present in the training data. To make progress
                   towards understanding in-context learning, we consider the
                   well-defined problem of training a model to in-context learn
                   a function class (e.g., linear functions): that is, given
                   data derived from some functions in the class, can we train
                   a model to in-context learn ``most'' functions from this
                   class? We show empirically that standard Transformers can be
                   trained from scratch to perform in-context learning of
                   linear functions -- that is, the trained model is able to
                   learn unseen linear functions from in-context examples with
                   performance comparable to the optimal least squares
                   estimator. In fact, in-context learning is possible even
                   under two forms of distribution shift: (i) between the
                   training data of the model and inference-time prompts, and
                   (ii) between the in-context examples and the query input
                   during inference. We also show that we can train
                   Transformers to in-context learn more complex function
                   classes -- namely sparse linear functions, two-layer neural
                   networks, and decision trees -- with performance that
                   matches or exceeds task-specific learning algorithms. Our
                   code and models are available at
                   https://github.com/dtsip/in-context-learning .},
  archiveprefix = {arXiv},
  author        = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and
                   Valiant, Gregory},
  eprint        = {2208.01066},
  primaryclass  = {cs.CL},
  title         = {What Can Transformers Learn {In-Context}? A Case Study of
                   Simple Function Classes},
  year          = {2022}
}

@article{Von_Oswald2022-ga,
  abstract      = {Transformers have become the state-of-the-art neural network
                   architecture across numerous domains of machine learning.
                   This is partly due to their celebrated ability to transfer
                   and to learn in-context based on few examples. Nevertheless,
                   the mechanisms by which Transformers become in-context
                   learners are not well understood and remain mostly an
                   intuition. Here, we argue that training Transformers on
                   auto-regressive tasks can be closely related to well-known
                   gradient-based meta-learning formulations. We start by
                   providing a simple weight construction that shows the
                   equivalence of data transformations induced by 1) a single
                   linear self-attention layer and by 2) gradient-descent (GD)
                   on a regression loss. Motivated by that construction, we
                   show empirically that when training self-attention-only
                   Transformers on simple regression tasks either the models
                   learned by GD and Transformers show great similarity or,
                   remarkably, the weights found by optimization match the
                   construction. Thus we show how trained Transformers
                   implement gradient descent in their forward pass. This
                   allows us, at least in the domain of regression problems, to
                   mechanistically understand the inner workings of optimized
                   Transformers that learn in-context. Furthermore, we identify
                   how Transformers surpass plain gradient descent by an
                   iterative curvature correction and learn linear models on
                   deep data representations to solve non-linear regression
                   tasks. Finally, we discuss intriguing parallels to a
                   mechanism identified to be crucial for in-context learning
                   termed induction-head (Olsson et al., 2022) and show how it
                   could be understood as a specific case of in-context
                   learning by gradient descent learning within Transformers.},
  archiveprefix = {arXiv},
  author        = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo,
                   Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander
                   and Zhmoginov, Andrey and Vladymyrov, Max},
  eprint        = {2212.07677},
  primaryclass  = {cs.LG},
  title         = {Transformers learn in-context by gradient descent},
  year          = {2022}
}

@article{Burns2022-md,
  abstract      = {Existing techniques for training language models can be
                   misaligned with the truth: if we train models with imitation
                   learning, they may reproduce errors that humans make; if we
                   train them to generate text that humans rate highly, they
                   may output errors that human evaluators can't detect. We
                   propose circumventing this issue by directly finding latent
                   knowledge inside the internal activations of a language
                   model in a purely unsupervised way. Specifically, we
                   introduce a method for accurately answering yes-no questions
                   given only unlabeled model activations. It works by finding
                   a direction in activation space that satisfies logical
                   consistency properties, such as that a statement and its
                   negation have opposite truth values. We show that despite
                   using no supervision and no model outputs, our method can
                   recover diverse knowledge represented in large language
                   models: across 6 models and 10 question-answering datasets,
                   it outperforms zero-shot accuracy by 4\% on average. We also
                   find that it cuts prompt sensitivity in half and continues
                   to maintain high accuracy even when models are prompted to
                   generate incorrect answers. Our results provide an initial
                   step toward discovering what language models know, distinct
                   from what they say, even when we don't have access to
                   explicit ground truth labels.},
  archiveprefix = {arXiv},
  author        = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt,
                   Jacob},
  eprint        = {2212.03827},
  primaryclass  = {cs.CL},
  title         = {Discovering Latent Knowledge in Language Models Without
                   Supervision},
  year          = {2022}
}

@article{Jozefowicz2016-je,
  abstract      = {In this work we explore recent advances in Recurrent Neural
                   Networks for large scale Language Modeling, a task central
                   to language understanding. We extend current models to deal
                   with two key challenges present in this task: corpora and
                   vocabulary sizes, and complex, long term structure of
                   language. We perform an exhaustive study on techniques such
                   as character Convolutional Neural Networks or Long-Short
                   Term Memory, on the One Billion Word Benchmark. Our best
                   single model significantly improves state-of-the-art
                   perplexity from 51.3 down to 30.0 (whilst reducing the
                   number of parameters by a factor of 20), while an ensemble
                   of models sets a new record by improving perplexity from
                   41.0 down to 23.7. We also release these models for the NLP
                   and ML community to study and improve upon.},
  archiveprefix = {arXiv},
  author        = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and
                   Shazeer, Noam and Wu, Yonghui},
  eprint        = {1602.02410},
  primaryclass  = {cs.CL},
  title         = {Exploring the Limits of Language Modeling},
  year          = {2016}
}

@inproceedings{Kim2017-cy,
  author    = {Yoon Kim and
               Carl Denton and
               Luong Hoang and
               Alexander M. Rush},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/KimDHR17.bib},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  title     = {Structured Attention Networks},
  url       = {https://openreview.net/forum?id=HkE0Nvqlg},
  year      = {2017}
}

@article{Kharitonov2021-bp,
  abstract      = {Training data memorization in NLP can both be beneficial
                   (e.g., closed-book QA) and undesirable (personal data
                   extraction). In any case, successful model training requires
                   a non-trivial amount of memorization to store word
                   spellings, various linguistic idiosyncrasies and common
                   knowledge. However, little is known about what affects the
                   memorization behavior of NLP models, as the field tends to
                   focus on the equally important question of generalization.
                   In this work, we demonstrate that the size of the subword
                   vocabulary learned by Byte-Pair Encoding (BPE) greatly
                   affects both ability and tendency of standard Transformer
                   models to memorize training data, even when we control for
                   the number of learned parameters. We find that with a large
                   subword vocabulary size, Transformer models fit random
                   mappings more easily and are more vulnerable to membership
                   inference attacks. Similarly, given a prompt,
                   Transformer-based language models with large subword
                   vocabularies reproduce the training data more often. We
                   conjecture this effect is caused by reduction in the
                   sequences' length that happens as the BPE vocabulary grows.
                   Our findings can allow a more informed choice of
                   hyper-parameters, that is better tailored for a particular
                   use-case.},
  archiveprefix = {arXiv},
  author        = {Kharitonov, Eugene and Baroni, Marco and Hupkes, Dieuwke},
  eprint        = {2110.02782},
  primaryclass  = {cs.CL},
  title         = {How {BPE} Affects Memorization in Transformers},
  year          = {2021}
}

@article{Belinkov2022-uy,
  abstract  = {Abstract Probing classifiers have emerged as one of the
               prominent methodologies for interpreting and analyzing deep
               neural network models of natural language processing. The basic
               idea is simple---a classifier is trained to predict some
               linguistic property from a model's representations---and has
               been used to examine a wide variety of models and properties.
               However, recent studies have demonstrated various methodological
               limitations of this approach. This squib critically reviews the
               probing classifiers framework, highlighting their promises,
               shortcomings, and advances.},
  author    = {Belinkov, Yonatan},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  journal   = {Comput. Linguist. Assoc. Comput. Linguist.},
  language  = {en},
  number    = {1},
  pages     = {207--219},
  publisher = {MIT Press - Journals},
  title     = {Probing classifiers: Promises, shortcomings, and advances},
  volume    = {48},
  year      = {2022}
}

@article{Blevins2022-fv,
  abstract      = {Although pretrained language models (PLMs) can be prompted
                   to perform a wide range of language tasks, it remains an
                   open question how much this ability comes from generalizable
                   linguistic representations versus more surface-level lexical
                   patterns. To test this, we present a structured prompting
                   approach that can be used to prompt for linguistic structure
                   prediction tasks, allowing us to perform zero- and few-shot
                   sequence tagging with autoregressive PLMs. We evaluate this
                   approach on part-of-speech tagging, named entity
                   recognition, and sentence chunking and demonstrate strong
                   few-shot performance in all cases. We also find that, though
                   the surface forms of the tags provide some signal,
                   structured prompting can retrieve linguistic structure even
                   with arbitrary labels, indicating that PLMs contain this
                   knowledge in a general manner robust to label choice.},
  archiveprefix = {arXiv},
  author        = {Blevins, Terra and Gonen, Hila and Zettlemoyer, Luke},
  eprint        = {2211.07830},
  primaryclass  = {cs.CL},
  title         = {Prompting Language Models for Linguistic Structure},
  year          = {2022}
}

@inproceedings{Vig2019-mb,
  address   = {Florence, Italy},
  author    = {Vig, Jesse  and
               Belinkov, Yonatan},
  booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  doi       = {10.18653/v1/W19-4808},
  pages     = {63--76},
  publisher = {Association for Computational Linguistics},
  title     = {Analyzing the Structure of Attention in a Transformer Language Model},
  url       = {https://aclanthology.org/W19-4808},
  year      = {2019}
}

@inproceedings{Hewitt_undated-us,
  address   = {Minneapolis, Minnesota},
  author    = {Hewitt, John  and
               Manning, Christopher D.},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1419},
  pages     = {4129--4138},
  publisher = {Association for Computational Linguistics},
  title     = {{A} Structural Probe for Finding Syntax in Word Representations},
  url       = {https://aclanthology.org/N19-1419},
  year      = {2019}
}

@article{Rogers_undated-it,
  author  = {Rogers, Anna  and
             Kovaleva, Olga  and
             Rumshisky, Anna},
  doi     = {10.1162/tacl_a_00349},
  journal = {Transactions of the Association for Computational Linguistics},
  pages   = {842--866},
  title   = {A Primer in {BERT}ology: What We Know About How {BERT} Works},
  url     = {https://aclanthology.org/2020.tacl-1.54},
  volume  = {8},
  year    = {2020}
}

@inproceedings{Liu2019-vd,
  address   = {Minneapolis, Minnesota},
  author    = {Liu, Nelson F.  and
               Gardner, Matt  and
               Belinkov, Yonatan  and
               Peters, Matthew E.  and
               Smith, Noah A.},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1112},
  pages     = {1073--1094},
  publisher = {Association for Computational Linguistics},
  title     = {Linguistic Knowledge and Transferability of Contextual Representations},
  url       = {https://aclanthology.org/N19-1112},
  year      = {2019}
}

@inproceedings{Wiegreffe2019-cc,
  address   = {Hong Kong, China},
  author    = {Wiegreffe, Sarah  and
               Pinter, Yuval},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  doi       = {10.18653/v1/D19-1002},
  pages     = {11--20},
  publisher = {Association for Computational Linguistics},
  title     = {Attention is not not Explanation},
  url       = {https://aclanthology.org/D19-1002},
  year      = {2019}
}

@inproceedings{Jain2019-ea,
  address   = {Minneapolis, Minnesota},
  author    = {Jain, Sarthak  and
               Wallace, Byron C.},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1357},
  pages     = {3543--3556},
  publisher = {Association for Computational Linguistics},
  title     = {{A}ttention is not {E}xplanation},
  url       = {https://aclanthology.org/N19-1357},
  year      = {2019}
}

@inproceedings{Kobayashi2020-ax,
  address   = {Online},
  author    = {Kobayashi, Goro  and
               Kuribayashi, Tatsuki  and
               Yokoi, Sho  and
               Inui, Kentaro},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  doi       = {10.18653/v1/2020.emnlp-main.574},
  pages     = {7057--7075},
  publisher = {Association for Computational Linguistics},
  title     = {Attention is Not Only a Weight: Analyzing Transformers with Vector Norms},
  url       = {https://aclanthology.org/2020.emnlp-main.574},
  year      = {2020}
}

@inproceedings{Vaswani_undated-hp,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  pages     = {5998--6008},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  year      = {2017}
}

@inproceedings{Kovaleva2019-rg,
  address   = {Hong Kong, China},
  author    = {Kovaleva, Olga  and
               Romanov, Alexey  and
               Rogers, Anna  and
               Rumshisky, Anna},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  doi       = {10.18653/v1/D19-1445},
  pages     = {4365--4374},
  publisher = {Association for Computational Linguistics},
  title     = {Revealing the Dark Secrets of {BERT}},
  url       = {https://aclanthology.org/D19-1445},
  year      = {2019}
}

@article{Bibal_undated-bm,
  author = {Bibal, Adrien and Cardon, R{\'e}mi and Alfter, David and Wilkens,
            Rodrigo and Wang, Xiaoou and Fran{\c c}ois, Thomas and Watrin,
            Patrick},
  title  = {Is Attention Explanation? An Introduction to the Debate}
}

@inproceedings{Clark2019-it,
  address   = {Florence, Italy},
  author    = {Clark, Kevin  and
               Khandelwal, Urvashi  and
               Levy, Omer  and
               Manning, Christopher D.},
  booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  doi       = {10.18653/v1/W19-4828},
  pages     = {276--286},
  publisher = {Association for Computational Linguistics},
  title     = {What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention},
  url       = {https://aclanthology.org/W19-4828},
  year      = {2019}
}

@inproceedings{Tenney2019-vv,
  author    = {Ian Tenney and
               Patrick Xia and
               Berlin Chen and
               Alex Wang and
               Adam Poliak and
               R. Thomas McCoy and
               Najoung Kim and
               Benjamin Van Durme and
               Samuel R. Bowman and
               Dipanjan Das and
               Ellie Pavlick},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  title     = {What do you learn from context? Probing for sentence structure in
               contextualized word representations},
  url       = {https://openreview.net/forum?id=SJzSgnRcKX},
  year      = {2019}
}

@article{Chefer_undated-iw,
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  title  = {Transformer Interpretability Beyond Attention Visualization}
}

@inproceedings{Bostrom2020-pu,
  address   = {Online},
  author    = {Bostrom, Kaj  and
               Durrett, Greg},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  doi       = {10.18653/v1/2020.findings-emnlp.414},
  pages     = {4617--4624},
  publisher = {Association for Computational Linguistics},
  title     = {Byte Pair Encoding is Suboptimal for Language Model Pretraining},
  url       = {https://aclanthology.org/2020.findings-emnlp.414},
  year      = {2020}
}

@article{Dar2022-as,
  abstract      = {Understanding Transformer-based models has attracted
                   significant attention, as they lie at the heart of recent
                   technological advances across machine learning. While most
                   interpretability methods rely on running models over inputs,
                   recent work has shown that a zero-pass approach, where
                   parameters are interpreted directly without a
                   forward/backward pass is feasible for some Transformer
                   parameters, and for two-layer attention networks. In this
                   work, we present a theoretical analysis where all parameters
                   of a trained Transformer are interpreted by projecting them
                   into the embedding space, that is, the space of vocabulary
                   items they operate on. We derive a simple theoretical
                   framework to support our arguments and provide ample
                   evidence for its validity. First, an empirical analysis
                   showing that parameters of both pretrained and fine-tuned
                   models can be interpreted in embedding space. Second, we
                   present two applications of our framework: (a) aligning the
                   parameters of different models that share a vocabulary, and
                   (b) constructing a classifier without training by
                   ``translating'' the parameters of a fine-tuned classifier to
                   parameters of a different model that was only pretrained.
                   Overall, our findings open the door to interpretation
                   methods that, at least in part, abstract away from model
                   specifics and operate in the embedding space only.},
  archiveprefix = {arXiv},
  author        = {Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  eprint        = {2209.02535},
  primaryclass  = {cs.CL},
  title         = {Analyzing Transformers in Embedding Space},
  year          = {2022}
}

@inbook{McKnight2010-wx,
  abstract  = {… test assumes a specific distribution. Thus, the Mann - Whitney
               U is conceptually similar to the t- test … When data do not meet
               the parametric assumptions of the t- test , the Mann - Whitney U
               …},
  address   = {Hoboken, NJ, USA},
  author    = {McKnight, Patrick E and Najab, Julius},
  booktitle = {The Corsini Encyclopedia of Psychology},
  publisher = {John Wiley \& Sons, Inc.},
  title     = {{Mann-Whitney} {U} Test},
  year      = {2010}
}

@inproceedings{Devlin2018-uk,
  address   = {Minneapolis, Minnesota},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url       = {https://aclanthology.org/N19-1423},
  year      = {2019}
}

@inproceedings{Schuster2012-ro,
  abstract  = {This paper describes challenges and solutions for building a
               successful voice search system as applied to Japanese and Korean
               at Google. We describe the techniques used to deal with an
               infinite vocabulary, how modeling completely in the written
               domain for language model and dictionary can avoid some system
               complexity, and how we built dictionaries, language and acoustic
               models in this framework. We show how to deal with the
               difficulty of scoring results for multiple script languages
               because of ambiguities. The development of voice search for
               these languages led to a significant simplification of the
               original process to build a system for any new language which in
               in parts became our default process for internationalization of
               voice search.},
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 {IEEE} International Conference on Acoustics, Speech and
               Signal Processing ({ICASSP})},
  keywords  = {Decision support systems;Helium;Speech recognition;voice
               search;Japanese;Korean},
  pages     = {5149--5152},
  publisher = {ieeexplore.ieee.org},
  title     = {Japanese and Korean voice search},
  year      = {2012}
}

@article{Sanh2019-lo,
  abstract      = {As Transfer Learning from large-scale pre-trained models
                   becomes more prevalent in Natural Language Processing (NLP),
                   operating these large models in on-the-edge and/or under
                   constrained computational training or inference budgets
                   remains challenging. In this work, we propose a method to
                   pre-train a smaller general-purpose language representation
                   model, called DistilBERT, which can then be fine-tuned with
                   good performances on a wide range of tasks like its larger
                   counterparts. While most prior work investigated the use of
                   distillation for building task-specific models, we leverage
                   knowledge distillation during the pre-training phase and
                   show that it is possible to reduce the size of a BERT model
                   by 40\%, while retaining 97\% of its language understanding
                   capabilities and being 60\% faster. To leverage the
                   inductive biases learned by larger models during
                   pre-training, we introduce a triple loss combining language
                   modeling, distillation and cosine-distance losses. Our
                   smaller, faster and lighter model is cheaper to pre-train
                   and we demonstrate its capabilities for on-device
                   computations in a proof-of-concept experiment and a
                   comparative on-device study.},
  archiveprefix = {arXiv},
  author        = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and
                   Wolf, Thomas},
  eprint        = {1910.01108},
  primaryclass  = {cs.CL},
  title         = {{DistilBERT}, a distilled version of {BERT}: smaller,
                   faster, cheaper and lighter},
  year          = {2019}
}

@article{Liu2019-fq,
  abstract      = {Language model pretraining has led to significant
                   performance gains but careful comparison between different
                   approaches is challenging. Training is computationally
                   expensive, often done on private datasets of different
                   sizes, and, as we will show, hyperparameter choices have
                   significant impact on the final results. We present a
                   replication study of BERT pretraining (Devlin et al., 2019)
                   that carefully measures the impact of many key
                   hyperparameters and training data size. We find that BERT
                   was significantly undertrained, and can match or exceed the
                   performance of every model published after it. Our best
                   model achieves state-of-the-art results on GLUE, RACE and
                   SQuAD. These results highlight the importance of previously
                   overlooked design choices, and raise questions about the
                   source of recently reported improvements. We release our
                   models and code.},
  archiveprefix = {arXiv},
  author        = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei
                   and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis,
                   Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  eprint        = {1907.11692},
  primaryclass  = {cs.CL},
  title         = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
  year          = {2019}
}

@inproceedings{xlmr,
  address   = {Online},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  publisher = {Association for Computational Linguistics},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  url       = {https://aclanthology.org/2020.acl-main.747},
  year      = {2020}
}

@inproceedings{Miller1995-os,
  author    = {Miller, George A.},
  booktitle = {Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992},
  title     = {{W}ord{N}et: A Lexical Database for {E}nglish},
  url       = {https://aclanthology.org/H92-1116},
  year      = {1992}
}

@article{vit,
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal = {ArXiv preprint},
  title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url     = {https://arxiv.org/abs/2010.11929},
  volume  = {abs/2010.11929},
  year    = {2020}
}

@book{nltk,
  author    = {Bird, Steven and Klein, Ewan and Loper, Edward},
  publisher = {O'Reilly Media, Inc.},
  title     = {{Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit}},
  year      = {2009}
}

@misc{dvc,
  author    = {Ruslan Kuprieiev and
               skshetry and
               Dmitry Petrov and
               Peter Rowlands and
               Paweł Redzyński and
               Casper da Costa-Luis and
               Alexander Schepanovski and
               Gao and
               David de la Iglesia Castro and
               Ivan Shcheklein and
               Batuhan Taskaya and
               Jorge Orpinel and
               Fábio Santos and
               Dave Berenbaum and
               daniele and
               Ronan Lamy and
               Aman Sharma and
               Zhanibek Kaimuldenov and
               Dani Hodovic and
               Nikita Kodenko and
               Andrew Grigorev and
               Earl and
               Nabanita Dash and
               George Vyshnya and
               maykulkarni and
               Max Hora and
               Vera and
               Sanidhya Mangal},
  doi       = {10.5281/zenodo.7646429},
  publisher = {Zenodo},
  title     = {DVC: Data Version Control - Git for Data \& Models},
  url       = {https://doi.org/10.5281/zenodo.7646429},
  version   = {2.45.1},
  year      = {2023}
}

@inproceedings{transformers,
  address   = {Online},
  author    = {Wolf, Thomas  and
               Debut, Lysandre  and
               Sanh, Victor  and
               Chaumond, Julien  and
               Delangue, Clement  and
               Moi, Anthony  and
               Cistac, Pierric  and
               Rault, Tim  and
               Louf, Remi  and
               Funtowicz, Morgan  and
               Davison, Joe  and
               Shleifer, Sam  and
               von Platen, Patrick  and
               Ma, Clara  and
               Jernite, Yacine  and
               Plu, Julien  and
               Xu, Canwen  and
               Le Scao, Teven  and
               Gugger, Sylvain  and
               Drame, Mariama  and
               Lhoest, Quentin  and
               Rush, Alexander},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  doi       = {10.18653/v1/2020.emnlp-demos.6},
  pages     = {38--45},
  publisher = {Association for Computational Linguistics},
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  url       = {https://aclanthology.org/2020.emnlp-demos.6},
  year      = {2020}
}

@inproceedings{datasets,
  abstract      = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
  address       = {Online and Punta Cana, Dominican Republic},
  archiveprefix = {arXiv},
  author        = {Lhoest, Quentin  and
                   Villanova del Moral, Albert  and
                   Jernite, Yacine  and
                   Thakur, Abhishek  and
                   von Platen, Patrick  and
                   Patil, Suraj  and
                   Chaumond, Julien  and
                   Drame, Mariama  and
                   Plu, Julien  and
                   Tunstall, Lewis  and
                   Davison, Joe  and
                   {\v{S}}a{\v{s}}ko, Mario  and
                   Chhablani, Gunjan  and
                   Malik, Bhavitvya  and
                   Brandeis, Simon  and
                   Le Scao, Teven  and
                   Sanh, Victor  and
                   Xu, Canwen  and
                   Patry, Nicolas  and
                   McMillan-Major, Angelina  and
                   Schmid, Philipp  and
                   Gugger, Sylvain  and
                   Delangue, Cl{\'e}ment  and
                   Matussi{\`e}re, Th{\'e}o  and
                   Debut, Lysandre  and
                   Bekman, Stas  and
                   Cistac, Pierric  and
                   Goehringer, Thibault  and
                   Mustar, Victor  and
                   Lagunas, Fran{\c{c}}ois  and
                   Rush, Alexander  and
                   Wolf, Thomas},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  eprint        = {2109.02846},
  pages         = {175--184},
  primaryclass  = {cs.CL},
  publisher     = {Association for Computational Linguistics},
  title         = {Datasets: A Community Library for Natural Language Processing},
  url           = {https://aclanthology.org/2021.emnlp-demo.21},
  year          = {2021}
}
