
To provide a better understanding of the cross-domain analysis we evaluated both models as a function of the number of samples used in the training stage. I.e., we kept track of the model's performance when trained with a specific percentage of MeLiSA samples. To measure the classification performance, we used the $F_1$-score, which takes into account possible class unbalances.

\subsection{Fine-tuning}

Figures \ref{fig:melisa_pretrain} and \ref{fig:melisa_pretrain_bert} show, for the biLSTM and BERT models respectively, the $F_1$-score on the MeLiSA validation split as a function of the training steps. This score is shown for trainings with different amount of MeLiSA samples. This first experiment was developed not only to pre-train a model in the source domain, but also to check if adding more samples improves performance in the same domain. It can be seen that this is true for validation and that BERT outperforms the biLSTM model in this task.

Once the model was pre-trained, we analyzed if it could be used to improve performance in other domains. That is, we compared if a model pre-trained on the MeLiSA training set and fine-tuned on the training set of the target dataset performs better than the same model trained with the target training set only. Figures \ref{fig:finetune_cine} and \ref{fig:finetune_tass} show the training curves of the biLSTM model for the MuchoCine and TASS datasets, while Fig. \ref{fig:finetune_cine_bert} and \ref{fig:finetune_tass_bert} show the same curves for BERT. All of these plots represents the $F_1$-score as a function of training steps for different amounts of MeLiSA samples. It should be noticed that the blue curve in these four plots represents the performance of the model without any addition of MeLiSA samples (0.0\%). 

In addition to these plots, we also evaluate the models on the test set of all target datasets. Table \ref{tab:melisa_finetunning} shows the $F_1$-score for the model trained only on the target training set and the relative increment (\%) of this score when more samples of MeLiSA are added to the training. Except for a few specific cases, this table shows that the addition of training samples from MeLiSA improves the $F_1$-score when the model is fine-tuned to other domains.

It is interesting to notice some common patterns between plots in Fig. \ref{fig:finetunning} and Table \ref{tab:melisa_finetunning}. In the first place, there is a clear superiority of the BERT model above the biLSTM in terms of $F_1$-score, which can be seen in all the cases where both models were trained with the same amount of MeLiSA samples. However, it should be take into account that BERT has been pre-trained with significantly more data than the biLSTM model, so a further analysis is needed to make a fair comparison between both models. 

Secondly, we observe that adding more samples to the biLSTM model not always improves its performance. This can be anticipated from Fig. \ref{fig:finetune_cine} (where the target domain are movie reviews from the MuchoCine dataset) and confirmed in Table \ref{tab:melisa_finetunning}. The plot shows that the blue line (0\% of additional samples) is above the yellow one (0.1\% of additional samples) by a significant margin and the table shows that a biLSTM model trained without any additional samples ($F_1=0.2441$ for 0\% of additional MeLiSA samples) performs better than a model trained with some additional samples from the product reviews domain ($F_1=0.1868$ for 0.1\% of additional MeLiSA samples and $F_1=0.2255$ for 10\%). Table \ref{tab:melisa_finetunning} shows a similar behaviour for the tweets domain (TASS column), where the addition of MeLiSA samples does not increase the performance in a proportional amount. 

In summary, Table \ref{tab:melisa_finetunning} suggests that the overall result of using additional training samples from the product reviews domain to pre-train a model in the tweets or movie reviews domains is better in terms of $F_1$-score performance. However, the specific number of samples needed to considerably improve performance may depend on the specific domain and the model used. 


\begin{figure*}
    \centering
    \subfloat[]{\includegraphics[height=.35\textwidth,width=.35\textwidth]{images/zero_shot.png}\label{fig:lstm_zero_shot}}
    \hfil
    \subfloat[]{\includegraphics[height=.35\textwidth,width=.35\textwidth]{images/zero_shot_bert.png}\label{fig:bert_zero_shot}}
    \caption{Zero shot results on test split. Plots show the $F_1$-score as a function of the percentage of training samples for the (a) biLSTM and (b) BERT models.}
    \label{fig:zero_shot}
\end{figure*}

\subsection{Zero-shot}

We also studied each model's performance on the zero-shot CDSC task. In this case, models pre-trained in the context of product reviews were directly tested on the test portion of the Amazon, TASS and MuchoCine datasets. Results are shown in Fig. \ref{fig:lstm_zero_shot} and \ref{fig:bert_zero_shot} for the biLSTM and BERT model, respectively.

There are two main things to point out about this experiment. The first one is that zero-shot classification seems to be significantly better (at least in terms of $F_1$-score) when knowledge is transfered to the same domain. It is interesting to notice, however, that both models perform similar when all samples are used in the pre-training step but BERT has a better performance when less data is used at that step. This could have some relation with the Amazon results shown in Table \ref{tab:melisa_finetunning} from the fine-tuning Section, where the relative improvement of the biLSTM model seems also to be higher than the one of BERT. This may suggest that adding more samples to a model could result in a better relative improvement (in terms of $F_1$-score) when the model is less pre-trained, even if the samples are taken from a different (though from the same domain) dataset.

Another thing to notice is that CDSC is very challenging when there is no fine-tuning adaptation to the target domain, which can be shown in the TASS and MuchoCine curves of both models. It seems to be, however, a slightly improvement of the performance when the percentage of data samples is increased from 0.1\% to 10.0\% for the BERT model. It is not clear from our study if this could mean something relevant but it could provide some additional evidence that a better pre-trained model could take advantage of additional data from other domain, which is also suggested by Table \ref{tab:melisa_finetunning}'s results.
