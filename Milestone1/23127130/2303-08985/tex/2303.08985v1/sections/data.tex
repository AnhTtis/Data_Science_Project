The idea of building a database from product reviews was inspired in~\cite{panglee}, which contains a very comprehensive analysis of opinion mining on this type of websites. Many of the criteria on how to filter reviews and balance the amount of review in each class were also drawn from that work. Other useful sources were~\cite{socher2013} and~\cite{maas2011}, which describe similar English databases, and~\cite{navasloro2020}, which is a review on Spanish SA databases.

\subsection{Data Preparation}

Figure \ref{fig:MLreview} shows a typical review from the MercadoLibre website. Each review consists mainly on a \textit{body comment}, a \textit{title} and a \textit{number of stars}, which is considered as the type of sentiment present in the body, i.e., from ``very negative'' (one star) through ``very positive'' (five stars). In addition to this features, each review has the number of likes and dislikes given to the comment by other users. The difference between the number of likes and dislikes is called \textit{valorization} and it could be used as a metric of how useful was the comment to other users. Comments with high valorization were prioritized at the time of selecting the reviews.

\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{images/ml_review.png}
    \caption{Sample of a product review taken from the MercadoLibre website.}
    \label{fig:MLreview}
\end{figure}

The MeLiSA database was build using the MercadoLibre API to access to the data. This API does not require any kind of authentication and it is open source, so all data in the database was public at the time it was downloaded. All reviews were downloaded between August 2020 and January 2021. Because MercadoLibre commercial presence is not the uniform through Latin America, only the countries with more reviews were considered for the their inclusion in the database. These countries were Argentina, Colombia, Peru, Uruguay, Chile, Venezuela and Mexico. In addition, in order to avoid any correlation between the country from which the review belong and the star rate we select a balanced amount of reviews per star rate inside the same country.

Each product published in MercadoLibre belongs to a specific category. Because it is likely that reviews of products from the same category have similar vocabulary, correlation between the product category and the star rate can happen if the data is not correctly balanced. As an instance, if category A has a tendency to contains low star rate reviews, models trained with that kind of data will tend to assign low star rate to reviews that present words from A's semantic field. In order to avoid that kind of bias, we manually clustered all the MercadoLibre product categories, resulting in a big five group: ``Food and drinks'', ``Arts and entertainment'', ``Home'', ``Personal Health'' and ``Electronics and Technology''. We then selected the same amount of reviews for each category and star rate.

With this approach we were able to select $487,\!368$ Spanish reviews from the MercadoLibre website. These samples were used to create a train, validation and test partition of the dataset with the following criteria: each split should be balanced by star rate and the test set should have the same amount of samples for the same category, country and star rate. The number of samples per split is shown in Table \ref{tab:data_splits}.

\begin{table}[ht!]
    \centering
    \caption{Number of data samples per split}
    \label{tab:data_splits}
    \begin{tabular}{r|ccc}
         & $|$Train$|$ & $|$Val$|$ & $|$Test$|$ \\\hline
         MeLiSA & $439,\!250$ & $23,\!118$ & $25,\!000$ \\
         Amazon & $200,\!000$ & $5,\!000$ & $5,\!000$ \\
         MuchoCine & $3,\!290$ & $193$ & $388$ \\
         TASS (2012) & $5,\!192$ & $574$ & $39,\!382$ \\
    \end{tabular}
\end{table}

\begin{figure*}[!t]
    \centering
    \subfloat[]{\includegraphics[width=.40\textwidth]{images/bilstm_v2.png}
    \label{fig:blstm_classifier}}
    \hfil
    \subfloat[]{\includegraphics[width=.58\textwidth]{images/bert.png}
    \label{fig:bert_classifier}}
    \caption{Classification models. (a) BiLSTM. (b) BERT.}
    \label{fig:classification_models}
\end{figure*}

\subsection{Additional Databases}

In addition to MeLiSA, Table \ref{tab:data_splits} shows the number of samples in every split of three others datasets. These datasets were used to evaluate the task of sentiment classification when trained with MeLiSA as the source domain. We briefly describe these databases below:

\begin{itemize}
    \item \textbf{Amazon}~\cite{amazon}. This database is a collection of reviews on products sold on Amazon\footnote{https://www.amazon.com/}, collected between 2015 and 2019. It was created with the purpose of studying multilingual language understanding and contains comments in English, Japanese, German, French, Spanish and Chinese. For this work, the Spanish portion of the database was used, which consists of a training portion (200,000 reviews), a validation portion (5,000 reviews), and a test portion (5,000 reviews).
    
    \item \textbf{MuchoCine}~\cite{muchocine}. This database consists in 3,871 movie reviews extracted from the MuchoCine\footnote{http://www.muchocine.net/} website. It contains comments with their respective titles and a star rate between 1 and 5, given by the movie reviewer. This database is open source and relatively balanced by star rate, but does not have a standard division into training, validation or testing, and simply consists on a single set of review-rate sample pairs. For this work, a random partition was used in order to reserve 85\% of the samples for training, 5\% for validation and 10\% for testing. This partition remained the same for all experiments.
    
    \item \textbf{TASS (2012)}~\cite{tass2012}. The SEPLN Semantic Analysis Workshop (\textit{Taller de An√°lsis de sentimientos de la SEPLN}, or TASS) is a workshop that has been part of the Spanish Natural Language Processing Society (SEPLN) since 2012. Since that year, a series of databases have been developed to facilitate the study of different NLP tasks. The general corpus, created for its first edition in 2012, consists of a series of semi-automatically tagged Spanish tweets with 6 possible categories that denote the degree of sentiment of the tweet: P+ (very positive), P (positive), NEU (neutral), N (negative), N+ (very negative) and NONE (no sentiment). The corpus version is available for download after registration and contains 7,219 training tweets and 60,798 test tweets. It is not balanced by star rate. In order to compare the results with the rest of the databases used in this work, both sets were modified so that only those tweets labeled with P+, P, NEU, N or N+ (that is, without NONE) are included.
\end{itemize}


