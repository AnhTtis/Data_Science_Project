
\begin{figure*}
    \centering
    \subfloat[]{\includegraphics[width=.33\textwidth]{images/melisa_pretrain.png}\label{fig:melisa_pretrain}}
    \hfil
    \subfloat[]{\includegraphics[width=.33\textwidth]{images/finetune_cine_vs_samples.png}\label{fig:finetune_cine}}
    \hfil
    \subfloat[]{\includegraphics[width=.33\textwidth]{images/finetune_tass_vs_samples.png}\label{fig:finetune_tass}}
    
    \subfloat[]{\includegraphics[width=.33\textwidth]{images/melisa_pretrain_bert.png}\label{fig:melisa_pretrain_bert}}
    \hfil
    \subfloat[]{\includegraphics[width=.33\textwidth]{images/finetune_cine_vs_samples_bert.png}\label{fig:finetune_cine_bert}}
    \hfil
    \subfloat[]{\includegraphics[width=.33\textwidth]{images/finetune_tass_vs_samples_bert.png}\label{fig:finetune_tass_bert}}
    \caption{Training curves ($F_1$-score) for different amounts of training samples. Figures (a), (b) and (c) shows the fine-tuning results on the validation split for the biLSTM model and Figures (d), (e) and (f) for BERT. (a) and (d) are the validation scores on MeLiSA, (b) and (e) are the scores on the MuchoCine validation split and (c) and (f) the scores on the TASS validation.}
    \label{fig:finetunning}
\end{figure*}

\subsection{Classification Models}

As mentioned in Section \ref{sec:intro}, we explored the cross-domain analysis using two different neural-based classification models which are shown in Fig. \ref{fig:classification_models}.

\begin{itemize}
    \item \textbf{BiLSTM classifier}. This architecture is based on a bidirectional recurrent neural network with a LSTM unit activation and it is illustrated in Figure \ref{fig:blstm_classifier}. In this model, each word $w_t$ of the input sequence $w_1,\ldots,w_T$ is represented as a continuous vector $\mathbf{x}_t$ trough an embedding layer at the beginning of the network. This vector sequence is then forwarded to two different LSTM networks~\cite{lstm} (one forward and one backward). The outputs at the last step of these layers are then concatenated and forwarded to the linear output layer, which gives the probabilities of each class through a Softmax activation function. 
    \item \textbf{BERT classifier}. Since the emergence of the Transformer architecture~\cite{transformer}, a series of models based on self-attention mechanisms have been proposed to pre-train a language model. One of this models is the Bidirectional Encoder Representations from Transformers (BERT), illustrated in Figure \ref{fig:bert_classifier}, which consists of 12 identical transformer encoder layers. These layers contains a multi-head self-attention layer~\cite{transformer} at the input followed by a linear layer (feed forward) with some residual connections and layer normalization~\cite{layernorm} in between. As in the LSTM classifier, every word at the input of the network is represented as a continuous vector, but additionally some special tokens are added to the sentence. In particular, the \texttt{[CLS]} token is included at the start of every sentence and it is used to extract features of the entire sequence. That is, at the output of the encoder a sequence of the same length as the input is obtained but only the first vector is used as an input of the output layer, which returns the class probability.
\end{itemize}

The key difference in our analysis of these two models is that the biLSTM network was trained from scratch, whereas the BERT classifier used was pre-trained on a Language Modeling task. Specifically, the pre-trained model called BETO~\cite{beto}, which is trained on a 3 billion words corpus called Spanish Unnanotated Corpora\footnote{https://github.com/josecannete/spanish-corpora} (SUC) was used. %The SUC database is a collection of unnanotated documents, most of them extracted from the Spanish portions of the Open Parallel Corpus (OPUS) subcorpora. The OPUS project is intended to provide the community with a publicly available parallel corpus of free online data. 
Documents included in the SUC contains all the data from Spanish Wikipedia available at the time the corpus was released and all of the sources of the OPUS Project~\cite{opus} that had text in Spanish. This sources
includes United Nations and Government journals, TED Talks, Subtitles, News Stories and more. However, none of these sources are review-like documents. The total size of the corpora gathered was comparable with the corpora used in the original BERT.
% @misc{cardelino,
%     title = "Spanish Billion Words Corpus and Embeddings",
%     author = "Cristian Cardellino",
%     year = "2016",
%     month = "March",
%     URL = "https: //crscardellino.github.io/SBWCE/"
% }

\begin{table*}
    \centering
    \caption{Test results ($F_1$-score) in the fine-tuning configuration.}
    \label{tab:melisa_finetunning}
    \begin{tabular}{r|cc|cc|cc}
        & \multicolumn{2}{c|}{Amazon} & \multicolumn{2}{c|}{TASS} & \multicolumn{2}{c}{MuchoCine} \\
        (\%) & biLSTM & BERT & biLSTM & BERT & biLSTM & BERT \\\hline
        0.0 & 0.5594 & 0.5860 & 0.3470 & 0.3850 & 0.2441 & 0.2758 \\
        0.1 & 0.5643(+0.88 \%) & 0.5860(+0.00 \%) & 0.3557(+2.51 \%) & 0.4104(+6.60 \%) & 0.1868(-23.47 \%) & 0.2834(+2.76 \%) \\
        10.0 & 0.5654(+1.07 \%) & 0.5865(+0.09 \%) & 0.3761(+8.39 \%) & 0.3974(+3.22 \%) & 0.2255(-7.62 \%) & 0.3210(+16.39 \%) \\
        100.0 & 0.5662(+1.22 \%) & 0.5845(-0.26 \%) & 0.3503(+0.96 \%) & 0.4045(+5.06 \%) & 0.2616(+7.17 \%) & 0.3125(+13.31 \%) \\
    \end{tabular}
\end{table*}

\subsection{Experimental Set-Up}

The above mentioned models were used to perform CDSC using the fine-tuning and the zero-shot configurations. For the fine-tuning case, the following steps were applied:
\begin{enumerate}
    \item The model was trained using the train split of the source dataset (MeLiSA, for instance) and hyperparameter search was done with its corresponding validation portion.
    \item Once trained, the model was trained again using the training portion of the target dataset (MuchoCine, for instance), and a new hyperparameter search was done with the target validation split.
    \item Once retrained, the model was evaluated on the test split of the target dataset.
\end{enumerate}

The only difference between both configurations is that step 2 is omitted for the zero-shot learning. This means that evaluation on the target dataset was done without using any training sample of that dataset. As a consequence, zero-shot learning is usually more challenging than fine-tuning and tends to show lower performance. However, it can provide a better idea of the model's generalization capability.

In order to test if CDSC can be achieved from product domains to more general domains like movie reviews or tweets, we used our MeLiSA dataset as source domain and MuchoCine and TASS as target domains. We also used the Amazon dataset as target domain to keep track of the model's learning capability, although this domain is, in principle, be very similar to the source domain. 

Experiments were carried on in \texttt{Python}, and the \texttt{Pytorch} module was used to implement the model training algorithm. We also used the \texttt{Huggingface Transformers} library to load the Spanish BERT pre-trained parameters and a NVIDA GTX 1080 GPU to reduce time computation. We followed \cite{random_grid_hyperparms} to perform random grid sample hyperparameter search on the biLSTM model. The best biLSTM model found consisted in a two-layer LSTM cell with hidden dimension of 50 and an embedding matrix of dimension $60,\!000\times 300$. Dropout was used as a regularization technique with a probability of $0.1$ and Adam Optimization with a batch size of 16 and a learning rate of 1e-3 was found to give the best validation results. For the pre-trained BERT model, layer dimensions are fixed in advance (12 layers with inner dimension of 768). We also used Adam Optimization to train this model and fixed the learning rate and the batch size to 5e-5 and 16 respectively, as suggested in \cite{beto}.



