With the growing availability and popularity of opinion-rich resources such as social networks, e-commerce platforms and blogs, there has been an interest to correctly understanding what people think. For this reason, the fields of Opinion Mining and Sentiment Analysis (SA), which deal with the computational treatment of opinion, sentiment, and subjectivity in text, have lately become hot topics in research and industrial applications~\cite{panglee}\cite{Birjali2021}.

As part of the SA field, Sentiment Classification (SC) is the task of automatically predicting the sentiment level of a text. That is, to assign a value (usually an integer from 1 to 5) to a piece of text that represents the sentiment information present in it (usually, 1 being very negative and 5 being very positive). In order to perform this prediction, a classification model can be trained using data from the Internet, like product or movie reviews and texts extracted from Social Networks like Twitter. Each of these sources are known in the literature as the text domain. 

Due to the massive access to data, opinions found in the Internet can span so many different domains  where annotated training data for all of them~\cite{peng2018} is not always available. For example, product reviews can be much more easy to collect that annotated tweets, because reviews are usually labeled with the user's star rating, while tweets need to be manually labeled. This has motivated much research on cross-domain sentiment classification (CDSC) which aims to transfer the knowledge from label rich domain (source domain) to the label few domain (target domain)~\cite{adversarial}.

\subsection{The Cross-Domain Approach}

In recent years, the raise to prominence of Deep Learning has been essential to perform knowledge transfer from one task to another. Specifically, pivot-based methods~\cite{blitzer2007}, domain-invariant features extraction~\cite{peng2018} and multitask learning~\cite{ijcai2019} are some of the techniques found in the literature to improve the performance of CDSC systems. All of these methods need to train a dedicated NLP model from scratch for every new domain with its own specialized training data. Alternatively, substantial work has shown that unsupervised pre-trained language models on large text corpus are beneficial for text classification and other NLP tasks, which can avoid training a new model from scratch~\cite{google2021}. Pre-training a deep model consists in learning some or all of its multiple layers using a large amount of easy-to-collect data on a general task like Language Modeling. Then, these layers are used to initialize the model that performs a specific task like sentiment classification.
Recent advances in developing pre-trained models like BERT~\cite{bert} and GPT~\cite{gpt2} through language modeling have proved that transfer learning can improve performance in several supervised task by fine-tuning the model with supervised data. Moreover, most recent models like GPT-3~\cite{gpt3} achieved promising results in some tasks without the need of fine-tuning, i.e., with a zero-shot configuration. The term ``zero-shot'' is used to denote that, once the model has been pre-trained, it is evaluated on additional data without any additional training effort.

In this work we address the task of Spanish CDSC by training a model on product reviews from the MercadoLibre website and evaluating it on three others datasets: the Multilingual Amazon Reviews Corpus (the Spanish portion)~\cite{amazon}, the MuchoCine Reviews Corpus~\cite{muchocine} and the TASS General Coprus (2012)~\cite{tass2012}, which are described in Section~\ref{sec:data}. We perform experiments using two different models, one trained from scratch and one pre-trained on a Language Model task. Additionally, we evaluate these models in each of the above mentioned datasets in two different configurations:
\begin{itemize}
    \item \textit{fine-tuning}, where a model is pre-trained to perform SC in the context of products reviews and then is trained again with the target dataset to perform SC in a new domain.
    \item \textit{zero-shot}, where a model is trained to perform SC in the context of product reviews and evaluated in the target domain without any additional training.
\end{itemize}

\subsection{Related Works}

Despite the interest in CDSC research, few works have been found in the literature about cross-domain in Spanish sentiment classification. A cross-domain study was performed in \cite{gonazalez2014}, where the authors uses different lexicons to identify the adaptability of some words in different domains. Other than that, some works~\cite{brooke2009}~\cite{Miranda2016} perform a cross-linguistic approach in which some of the cross-domain techniques used in English are applied to Spanish text. To the best of our knowledge, this is the first work where a cross-domain classification in Spanish is made using pre-trained transformers model. Additionally, no other work where a Spanish model with a zero-shot configuration was evaluated was found in the literature.

The rest of the paper is organized as follows. Section \ref{sec:data} describe the MeLiSA (MercadoLibre for Sentiment Analysis) dataset, which was created using the MercadoLibre API\footnote{https://developers.mercadolibre.com.ar} and consists on a large collection of product reviews from seven Latin American countries. This Section also describes the additional datasets used for the cross-domain study. Section \ref{sec:models} describe the two models used to perform sentiment classification, and how this classification was implemented and evaluated. In Section \ref{sec:results_discuss} the results of the cross-domain sentiment classification are shown and analyzed. Finally, Section \ref{sec:conclusions} contains the main conclusions from the study and possible future work.
