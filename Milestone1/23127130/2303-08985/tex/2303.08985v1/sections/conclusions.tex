
In this work we performed a preliminary study on the Spanish Cross-Domain Sentiment Classification task, which is an important topic in the field of Natural Language Processing but that has not been received enough attention for the Spanish language. Specifically, we were interested in analyzing the knowledge transfer from the product reviews domain to the tweets and movie reviews domains. To address this problem, we created a dataset called MeLiSA (MercadoLibre for Sentiment Analysis) which consists in product reviews from seven Latin American countries and used it to train two different neural-based classification models. We then tested them on three other datasets, which included the Spanish portion of the Multilingual Amazon Reviews Corpus (product reviews domain), the MuchoCine corpus (movie reviews domain) and the 2012 release of the TASS corpus (general content tweets domain).

We considered two models in this work. The first one, was a model  pre-trained on a Language Modeling task transformer encoder (BERT) and the second one a bidirectional recurrent model trained from scratch (biLSTM). In order to perform a complete cross-domain analysis, both models were tested in the fine-tuning and the zero-shot configurations. Results suggest that cross-domain classification can be improved by fine-tuning the model to the specific target domain, but the number of samples needed to achieve optimal results may depend on the target domain and the model used. On the other hand, zero-shot classification seems to perform well on the same domain but very poorly on other domains when trained with product reviews. However, from the performed experiments, we hypothesize that bigger pre-trained models could take more advantage from source domain samples than not pre-trained models.

A possible continuation of this work may include further testing of this last hypothesis by including other pre-trained and not pre-trained models. In particular, it would be useful to test the cross-domain classification on a model trained from scratch and the same model pre-trained on a Language Modeling task. %We also think that it would be a good contribution to the community to make MeLiSA public, but a series of ethical issues should be considered first. \textcolor{red}{Leo: Quizas la ultima oracion la sacaria}