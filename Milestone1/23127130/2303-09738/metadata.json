{
    "arxiv_id": "2303.09738",
    "paper_title": "Computing one-bit compressive sensing via zero-norm regularized DC loss model and its surrogate",
    "authors": [
        "Kai Chen",
        "Ling Liang",
        "Shaohua Pan"
    ],
    "submission_date": "2023-03-17",
    "revised_dates": [
        "2023-03-20"
    ],
    "latest_version": 1,
    "categories": [
        "math.OC"
    ],
    "abstract": "One-bit compressed sensing is very popular in signal processing and communications due to its low storage costs and low hardware complexity, but it is a challenging task to recover the signal by using the one-bit information. In this paper, we propose a zero-norm regularized smooth difference of convexity (DC) loss model and derive a family of equivalent nonconvex surrogates covering the MCP and SCAD surrogates as special cases. Compared to the existing models, the new model and its SCAD surrogate have better robustness. To compute their $Ï„$-stationary points, we develop a proximal gradient algorithm with extrapolation and establish the convergence of the whole iterate sequence. Also, the convergence is proved to have a linear rate under a mild condition by studying the KL property of exponent 0 of the models. Numerical comparisons with several state-of-art methods show that in terms of the quality of solution, the proposed model and its SCAD surrogate are remarkably superior to the $\\ell_p$-norm regularized models, and are comparable even superior to those sparsity constrained models with the true sparsity and the sign flip ratio as inputs.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09738v1"
    ],
    "publication_venue": null
}