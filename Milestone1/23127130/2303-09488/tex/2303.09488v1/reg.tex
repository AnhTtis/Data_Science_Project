\documentclass[12pt,leqno,oneside]{article}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{microtype} 
\usepackage[english]{babel} %

\usepackage[a4paper,margin=1in,marginpar=1in]{geometry} %
\usepackage{csquotes}       %

\usepackage{authblk}  %
\renewcommand\Authfont{\scshape}
\renewcommand\Affilfont{\normalfont\itshape\small} 
\renewcommand\Authsep{, }
\renewcommand\Authand{, }
\renewcommand\Authands{, }

\usepackage[shortlabels]{enumitem}

\usepackage{amsmath}   %
\usepackage{amsthm}    %
\usepackage{mathtools} %
%
%
\usepackage[lcgreekalpha]{stix2}          %


%
%

%
\setcounter{secnumdepth}{5}
\renewcommand{\theparagraph}{(\alph{paragraph})}

%
\usepackage[
backend=biber,        %
style=alphabetic,     %
giveninits=true,      %
maxalphanames=5,      %
minalphanames=5,      %
maxbibnames=99        %
]{biblatex}

%
\ExecuteBibliographyOptions{isbn=false}
\AtEveryBibitem{%
  \clearfield{note}%
  \clearfield{language}%
}

\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[fieldsource=doi,final]
      \step[fieldset=url,null]
    }  
  }
}

%
\renewcommand*\finalnamedelim{\addspace\&\space}

\usepackage{xcolor}                   %
\definecolor{green}{HTML}{2ECC71}
\definecolor{blue}{HTML}{3498DB}
\definecolor{red}{HTML}{E74C3C}
\definecolor{orange}{HTML}{FD6A02}
\usepackage[pdfpagelabels]{hyperref}  %
\hypersetup{%
  colorlinks,
  linktocpage,
  urlcolor  = blue,
  citecolor = green,
  linkcolor = red}

\usepackage[capitalise,sort]{cleveref} %

\AfterEndEnvironment{proof}{\noindent\ignorespaces} %
\makeatletter
\def\@endtheorem{\endtrivlist}
\makeatother

%
\Crefname{paragraph}{\S}{\SS}
\Crefname{equation}{}{}
\Crefname{enumi}{}{}
\Crefname{conditioni}{Condition}{Conditions}
\Crefname{conditionalti}{Condition}{Conditions}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\Crefname{theorem}{Theorem}{Theorems}
\newtheorem{theoremintro}{Theorem}
\Crefname{theoremintro}{Theorem}{Theorems}
\renewcommand{\thetheoremintro}{\Alph{theoremintro}}
\newtheorem{lemma}[theorem]{Lemma}
\Crefname{lemma}{Lemma}{Lemmas}
\newtheorem{proposition}[theorem]{Proposition}
\Crefname{proposition}{Proposition}{Propositions}
\newtheorem{corollary}[theorem]{Corollary}
\Crefname{corollary}{Corollary}{Corollaries}
\newtheorem{conjecture}[theorem]{Conjecture}
\Crefname{conjecture}{Conjecture}{Conjectures}
\newtheorem{assumption}{Assumption}
\theoremstyle{definition}
\newtheorem{example}{Example}
\Crefname{example}{Example}{Examples}
\newtheorem*{example*}{Example}
\Crefname{assumption}{Assumption}{Assumptions}
\newtheorem{definition}[theorem]{Definition}
\Crefname{definition}{Definition}{Definitions}
\newtheorem{question}{Question}
\Crefname{question}{Question}{Questions}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\Crefname{remark}{Remark}{Remarks}

\numberwithin{equation}{section} %

\usepackage{booktabs} %


%

\DeclarePairedDelimiter{\paren}{\lparen}{\rparen}
\DeclarePairedDelimiter{\bracket}{\lbrack}{\rbrack}
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\openclosed}{\lparen}{\rbrack}
\DeclarePairedDelimiter{\closedopen}{\lbrack}{\rparen}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiterX{\psh}[2]{\langle}{\rangle}{#1, #2}

\DeclarePairedDelimiterXPP{\Exp}[1]{\exp}{\lparen}{\rparen}{}{#1}
\DeclarePairedDelimiterXPP{\Log}[1]{\log}{\lparen}{\rparen}{}{#1}
\DeclarePairedDelimiterXPP{\Inf}[1]{\inf}{\lbrace}{\rbrace}{}{#1}
\DeclarePairedDelimiterXPP{\Sup}[1]{\sup}{\lbrace}{\rbrace}{}{#1}
\DeclarePairedDelimiterXPP{\Max}[1]{\max}{\lbrace}{\rbrace}{}{#1}
\DeclarePairedDelimiterXPP{\Min}[1]{\min}{\lbrace}{\rbrace}{}{#1}


%

\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\dom}{\mathbb{D}om}
\DeclareMathOperator{\supp}{supp}

\DeclareMathOperator{\esp}{\mathbf{E}}
\DeclareMathOperator{\prob}{\mathbf{P}}
\DeclareMathOperator{\var}{\mathbf{Var}}
\DeclareMathOperator{\law}{\mathbf{law}}

\newcommand{\given}[1][]{%
  \nonscript\:#1\vert
  \allowbreak
  \nonscript\:
\mathopen{}}

\DeclarePairedDelimiterXPP{\Prob}[1]{\prob}[]{}{#1}
\DeclarePairedDelimiterXPP{\Esp}[1]{\esp}[]{}{#1}
\DeclarePairedDelimiterXPP{\Var}[1]{\var}[]{}{#1}
\DeclarePairedDelimiterXPP{\Law}[1]{\law}[]{}{#1}

\author{Ronan HERRY\thanks{R.H gratefully acknowledges funding from the Centre Henri Lebesgue.}}
\affil{%
  IRMAR, Université de Rennes 1
  \par
  \normalfont\href{mailto:ronan.herry@univ-rennes1.fr}{\texttt{ronan.herry@univ-rennes1.fr}}%
  }
\author{Dominique MALICET}
\affil{%
  LAMA, Université Gustave Eiffel
  \par
  \normalfont\href{mailto:dominique.malicet@univ-eiffel.fr}{\texttt{dominique.malicet@univ-eiffel.fr}}%
  }
\author{Guillaume POLY}
\affil{%
  IRMAR, Université de Rennes 1
  \par
  \normalfont\href{mailto:guillaume.poly@univ-rennes1.fr}{\texttt{guillaume.poly@univ-rennes1.fr}}%
  }

\addbibresource{reg.bib}

\usepackage{soulutf8} %
\newcommand{\todo}[1]{{\textcolor{red}{\textbf{\hl{#1}}}}}

\setlength{\parindent}{0mm}

\begin{document}
\title{Regularity of laws via Dirichlet forms -- Application to quadratic forms in independent and identically distributed random variables}
 \maketitle
 \vspace{-2em}
\begin{abstract}
  We present a new tool to study the regularity of a function $F$ of a sequence $(X_{i})$ of independent and identically distributed random variables.
  Our main result states that, under mild conditions on the law of $X_{1}$, the regularity of the law of $F$ is controlled by the regularity of the law of a conditionally Gaussian object, canonically associated with $F$.
At the technical level our analysis relies on the formalism of Dirichlet forms and an explicit construction of the Malliavin derivative of $F$ in the direction of a Gaussian space. 
As an application, we derive an explicit control of the regularity of the law of a quadratic from in the $X_{i}$'s in terms of spectral quantities, when the law of $X_{1}$ belongs to a large class of distribution including, for instance, all the Gaussian, all the Beta, all the Gamma, and all the polynomials thereof.
%
\end{abstract}

\section*{Introduction}

\subsection*{Main contributions}

\subsubsection*{Regularity of the law of random variables via Dirichlet form analysis}
We develop a systematic method to study the regularity of the law of a real-valued random variable $F$, when the probability space is equipped with a Dirichlet form (see \cref{s:reminders-dirichlet} for reminders).
The theory of Dirichlet forms provides us with a unbounded non-negative bilinear form $\Gamma \colon L^{2} \times L^{2} \to L^{1}$, as well as an algebra of \emph{smooth} random variables $\mathbb{D}^{\infty}$.
We fix a sequence $(X_{i})$ of independent and identically distributed random variables such that $X_{1} \in \mathbb{D}^{\infty}$, and satisfying furthermore a Dirichlet-independence condition \cref{ass:dirichlet-independence}.
We consider 
\begin{equation*}
  \Gamma \coloneq diag(\Gamma(X_{1},X_{1}), \dots, \Gamma(X_{n}, X_{n})).
\end{equation*}
For a symmetric matrix $\mathsf{A}$ with spectrum $(\lambda_{i})$, we also write
\begin{equation*}
  \mathcal{R}_{q}(\mathsf{A}) \coloneq \sum_{i_{1} \ne \dots \ne i_{q}} \lambda^{2}_{i_{1}} \dots \lambda^{2}_{i_{q}}, \qquad q \in \mathbb{N},
\end{equation*}

Our main result reads as follows.
\begin{theoremintro}[{\cref{th:regularity-negative-moments-spectral-remainder}}]\label{th:intro-regularity-negative-moments-spectral-remainder}
  Let $F = f(X_{1}, \dots, X_{n})$ with $f$ smooth.
  Assume that for all $q \in \mathbb{N}$, $\mathcal{R}_{q}(\Gamma^{1/2} (\nabla^{2}f(X)) \Gamma^{1/2})^{-1/4} \in L^{1}$, where $\nabla^{2}f$ is the usual Hessian matrix of $f$.
  Then, $F$ has a smooth law.
\end{theoremintro}
We actually obtain a precise quantitative relationship between the $q$ for which $\mathcal{R}_{q}$ is in $L^{-1/4}$ and the Sobolev regularity of the law of $F$.

\subsubsection*{Application to quadratic forms}

In this paper, our archetypical application is concerned with regularity of the law of a quadratic form in the $X_{i}$'s.
\begin{theoremintro}[{\cref{th:regularity-quadratic-form}}]\label{th:intro-regularity-quadratic-form}
  Let the previous notation prevail, and assume further that
  \begin{equation}\label{eq:intro-small-ball-gamma}
    \Prob*{ \Gamma(X_{1}, X_{1}) \leq \varepsilon } \lesssim \varepsilon^{\theta},
  \end{equation}
  for some $\theta > 0$.
  Let $\set*{\mathsf{A}^{(n)} = (a_{ij}^{(n)}) : n \in \mathbb{N} }$ be a sequence of symmetric operators such that
  \begin{align*}
    & \liminf_{n \to \infty} \mathcal{R}_{q}(\mathsf{A}^{(n)}) > 0;
  \\&\lim_{n \to \infty} \sum_{j} \paren*{a_{ij}^{(n)}}^{2} = 0.
  \end{align*}
  Then, for all $q \in \mathbb{N}$, there exists $N_{q}$ such that
  \begin{equation*}
    \Law*{\psh{X}{\mathsf{A}^{(n)} X}} \in \mathscr{C}^{q},\qquad n \geq N_{q}.
  \end{equation*}
\end{theoremintro}
We actually obtain precise estimate regarding the Sobolev norms of the density of the quadratic form $\psh{X}{\mathsf{A}^{(n)}X}$.
The assumption \cref{eq:intro-small-ball-gamma} ensures some minimal initial Sobolev regularity at the level of of the law of $X_{1}$ that allows us to bootstrap our argument.
We stress it is a rather mild assumption: we show in \cref{s:examples} that a large class of random variables, including, for instance, the Gaussian variables, the Beta variables, the Gamma variables, and the multi-linear polynomials thereof.

\subsection*{Motivations and related works}

\subsubsection*{Smooth central limit theorem}
It is part of the probabilistic folklore, that if $(X_{i})$ is a sequence of centred, normalized, independent and identically distributed random variables whose common law has density in a Sobolev space, then the regularity of the law of the linear functionals
\begin{equation*}
  S_{n} \coloneq \frac{1}{n^{1/2}} \sum_{i=1}^{n} X_{i},
\end{equation*}
improves with $n$.
This fact can easily be seen at the level of the decay of the Fourier transform of $S_{n}$.
Our work leverages the theory of Dirichlet forms to obtain a non-linear equivalent of this regularization phenomenon.

Improving the type of convergence in central limit theorem for non-linear random fields is a longstanding problem in probability theory.
On the Wiener space, that is when the $X_{i}$'s are independent Gaussian, let us mention, among other, the works \cite{NourdinPolyTotalVariation,HuLuNualart,BallyCarmellinoWiener} that establish that central convergence in law can be improved to convergence in total variation; while \cite{NPY} derives similar result for convergence in entropy.
In our companion paper \cite{HMP}, we derive that, of Wiener chaoses, central convergence can actually always be improved to $\mathscr{C}^{\infty}$ convergence of the densities.

For non-Gaussian random variables, less results are available.
See however, \cite{BallyCarmellinoInvariance,BallyCaramellinoPoly} where convergence in total variation is considered.
As in the present, all the aforementioned result, are derived through ideas pertaining to Malliavin calculus and Dirichlet forms.

We stress out that we do not need to assume asymptotic normality, or even convergence in law, of the random variables under consideration as our conditions in \cref{th:intro-regularity-negative-moments-spectral-remainder,th:intro-regularity-quadratic-form} are purely spectral.
In some cases, it is however, possible to recover those spectral conditions from asymptotic normality.
See for instance \cref{s:normal-convergence} for examples at the level of quadratic forms over a general Dirichlet space,

\subsubsection*{Regularity of the law via positivity of the Malliavin gradient}

In his seminal work \citeauthor{Malliavin} \cite{Malliavin} lays the foundation of an infinite dimensional differential calculus at the level of the Wiener space to study the regularity of the density of a solution of a SDE, thus giving a new proof of a celebrated theorem by \citeauthor{HormanderHypoelliptic} \cite{HormanderHypoelliptic}.
\cite{Malliavin} establishes that some form of positivity of the Malliavin gradient implies some form of regularity at the level of the law.
Since then, the existence of negative moments of the Malliavin gradient plays a prominent role in many works related to Gaussian analysis (see, for instance, \cite{HNTXBreuerMAjor,NourdinNualartFisher,AruGMC} for recent works).
In the literature, the problem of existence of negative moments is, most of the time, tackled on a case by case basis exploiting the specificity of the model under consideration, or taken as an assumption.
To the best of our knowledge, \cite{BogachevKosovZelenov} is the only work developing a comprehensive theory of regularity for Gaussian polynomials.
They obtain result regarding Besov regularity, and their work is also based on the study of the Malliavin derivative.
In this work, we introduce novel ideas to systematically study the existence of negative moments of the Malliavin derivative, allowing us to derive $\mathscr{C}^{\infty}$ regularity of the density.

\subsubsection*{Regularity via positivity of the carré du champ}
Since the theory of Dirichlet forms can be seen as a non-Gaussian generalization of Malliavin calculus, it not surprising that results regarding the regularity of laws from positivity of the Malliavin derivative can be recast in the setting of Dirichlet forms. 
To that extent, let us mention the celebrated criterion of \citeauthor{BouleauHirsch} \cite[Thm.\ I.7.1]{BouleauHirsch} regarding existence of a density.
By working at the level of abstract Dirichlet space rather than the Wiener space, we can apply our theorem to a much larger class of random variables.
We, however, pay a price: the differential calculus at the level of the abstract Dirichlet space is more involved, and applying \cref{th:intro-regularity-negative-moments-spectral-remainder} to general functionals is rather intricate, this is why we only state \cref{th:intro-regularity-quadratic-form} in the setting of quadratic forms.
We reserve exploring the case of higher degree polynomials to a subsequent work.

\subsection*{Outline of the proof and of our construction}

\subsubsection*{A Gaussian representation of the carré du champ}

The quantity $\Gamma(F,F)$ plays the role, in the non-smooth setting of our probability space, of the square length of the gradient of $F$.
As anticipated, we derive explicit controls of the negative moments of $\Gamma(F,F)$.
To do so, we define the \emph{Bouleau derivative}:
\begin{equation*}
  \sharp_{G} F \coloneq \sum_{i=1}^{n} \Gamma(X_{i}, X_{i})^{1/2} \partial_{i}f(X_{1}, \dots, X_{n}) G_{i},
\end{equation*}
where $(G_{i})$ is a sequence of independent standard Gaussian variables.
By construction, $\sharp_{G}F$ has the same law as $\Gamma(F,F)^{1/2}N$ where $N$ is an independent standard Gaussian variables.
In particular, we have the following \emph{Fourier-Laplace equivalence}:
\begin{equation*}
  \Esp*{ \mathrm{e}^{\mathrm{i} t \sharp_{G}F} } = \Esp*{ \mathrm{e}^{-t^{2}/2 \Gamma(F,F)} }.
\end{equation*}
From this identity, we see that existence of negative moments for $\Gamma(F,F)$ is guaranteed by some sufficiently fast Fourier decay of $\sharp_{G}F$, which in turns is equivalent to some regularity of the law of $\sharp_{G}F$.
Combining all of this, we obtain the following intermediary result.
\begin{theoremintro}[{\cref{th:sobolev-regularity-derivative}}]\label{th:intro-sobolev-regularity-derivative}
  Assume that $\sharp_{G}F$ has a smooth law, then so does $F$.
\end{theoremintro}


\subsubsection*{Comparison of the Malliavin derivative and the Bouleau derivative}
In the theory of Dirichlet forms, and in particular in Gaussian analysis, it is rather standard to represent the carré du champ $\Gamma$ through a so-called \emph{Malliavin derivative}.
This consists in a separable Hilbert space $\mathbb{G}$ together with a map $\nabla \colon L^{2} \to L^{2}(\mathbb{G})$ such that
\begin{equation*}
  \norm{\nabla F}_{\mathbb{G}} = \Gamma(F,F).
\end{equation*}
All separable Hilbert spaces being isomorphic little care is usually given to the choice of $\mathbb{G}$.
The Bouleau derivative $\sharp_{G}F$ is a Malliavin derivative, where we take $\mathbb{G}$ to specifically be a Gauss space.
By doing so, we introduce gaussianity in a \emph{a priori} non-Gaussian world, and we can now leverage the rich structure of Gaussian analysis.
This paradigmatic shift in the representation of the derivative is reminiscent of proof of the isometric embeddality of $\ell^{2}$ into $L^{q}$, where $\ell^{2}$ is explicitly sent into the Gauss space.
The idea of taking derivatives in the direction of Gaussian variables can be traced back to a slightly different construction of \citeauthor{BouleauError} \cite[Chap.\ V \S 2]{BouleauError} to study different problems.


\subsubsection*{Introducing a non-linearity by iterating the derivative}
The object $\sharp_{G}F$ is still to close from $F$, and we were not able to derive meaningful estimates by working directly at the level of $\sharp_{G}F$.
This motivates the introduction of the second derivative $\sharp_{H} \sharp_{G}F$.
For an other sequence $H = (H_{i})$ of independent Gaussian variables, we define \emph{iterated Bouleau gradient}, $\sharp_{H} \sharp_{G} F$ whose precise definition is too involved for this introduction.
Let us simply mention that our definition guarantees that
\begin{itemize}[wide]
  \item $\sharp_{H}\sharp_{G}F$ has the same law as $\Gamma(\sharp_{G}F, \sharp_{G}F)^{1/2}N$, where in the above expression we \emph{freeze} the quantity depending on $G$ when computing the carré du champ;
  \item there exists a random matrix, the \emph{Bouleau derivative} $\mathsf{D}^{2}F$, independent of $G$ and $H$, such that
\begin{equation*}
  \sharp_{H} \sharp_{G} F = \psh{G}{(\mathsf{D}^{2}F) H};
\end{equation*}
\item $\mathsf{D}^{2}F = \Gamma^{1/2} (\nabla^{2}f(X)) \Gamma^{1/2} + \mathsf{B}$ where $\mathsf{B}$ is a random matrix that we manage to discard in our analysis.
\end{itemize}

Similarly to T.\ Royen's proof of the Gaussian correlation inequality \cite{Royen}, we exploit properties of square of Gaussian variables rather than of Gaussian variables directly.
In our case, this allows us to control the regularity of $\sharp_{H} \sharp_{G} F$ in terms of the spectral properties of $\mathsf{D}^{2}F$, and thus of $\Gamma^{1/2} (\nabla^{2}f(X)) \Gamma^{1/2}$, which leads to \cref{th:intro-regularity-negative-moments-spectral-remainder}.

\subsubsection*{Controlling the Bouleau Hessian of a quadratic form}
When considering a quadratic form $F \coloneq \psh{X}{\mathsf{A}X}$, the term to control in the Bouleau Hessian is \emph{almost deterministic}: $\Gamma^{1/2} \mathsf{A} \Gamma^{1/2}$.
Owing to this particular form, we can explicitly control the right-hand side in \cref{th:intro-regularity-negative-moments-spectral-remainder} under some spectral assumptions on $\mathsf{A}$, which gives \cref{th:intro-regularity-quadratic-form}.
As a main technical tool, we use two results of independent interest:
\begin{itemize}[wide]
  \item a result showing that existence of a negative moment is preserved by taking multi-linear polynomials (\cref{th:small-ball-l2});
  \item a splitting argument that allows us to improve the positivity of some quantities under spectral conditions (\cref{th:small-ball-improved}).
\end{itemize}
These results complement, and are actually based, on existing result regarding invariance and anti-concentration for multi-linear polynomials \cite{CarberyWright,MDOInvariance}.


\tableofcontents%

\setcounter{secnumdepth}{5}

\section{Reminders and notations}

\subsection{Linear algebra}\label{s:linear-algebra}

\subsubsection{Symmetric Hilbert--Schmidt operators}
We write $\ell^{2}(\mathbb{N})$ for the usual Hilbert space of square-integrable $\mathbb{N}$-indexed sequences $x = (x_{i} : i \in \mathbb{N})$.
A \emph{symmetric Hilbert--Schmidt operator} $\mathsf{A}$ acting on $\ell^{2}(\mathbb{N})$ can be identified with a bi-sequence $\mathsf{A} \simeq (a_{ij} : i,\, j \in \mathbb{N})$ satisfying:
\begin{description}
  \item[symmetry] $a_{ij} = a_{ji}$ for all $i$ and $j \in \mathbb{N}$;
  \item[square-integrability] $\tr \mathsf{A}^{2} \coloneq \norm{a}_{\ell^{2}}^{2} \coloneq \sum_{i,j} a_{ij}^{2} < \infty$;
\end{description}
We sometimes also assume that $\mathsf{A}$ has a \emph{vanishing diagonal}, that is $a_{ii} = 0$ for all $i \in \mathbb{N}$.

\paragraph{Eigenvalues and spectral quantities}
The operator $\mathsf{A}$ is diagonalizable in an orthonormal basis with real eigenvalues $(\lambda_{i} : i \in \mathbb{N})$.
We always assume that they are decreasingly ordered by their eigenvalues: $\abs{\lambda_{1}} \geq \abs{\lambda_{2}} \geq \dots$.
The square-integrability can be rephrased in terms of the eigenvalues
\begin{equation*}
  \tr \mathsf{A}^{2} = \sum_{i \in \mathbb{N}} \lambda_{i}^{2} < \infty.
\end{equation*}
We consider various quantities associated with $\mathsf{A}$:
\begin{itemize}
  \item the \emph{spectral radius} 
\begin{equation*}
  \rho(\mathsf{A}) \coloneq \sup_{i \in \mathbb{N}} \abs{\lambda_{i}};
\end{equation*}
\item the \emph{spectral remainders}
\begin{equation*}
  \mathcal{R}_{q}(\mathsf{A}) \coloneq \sum_{i_{1} \ne \dots \ne i_{q}} \lambda_{i_{1}}^{2} \dots \lambda_{i_{q}}^{2}, \qquad q \in \mathbb{N};
\end{equation*}
\item the \emph{partial influences}
\begin{equation*}
  \tau_{i}(\mathsf{A}) \coloneq \sum_{j \in \mathbb{N}} a_{ij}^{2}, \qquad i \in \mathbb{N};
\end{equation*}
\item the \emph{maximal influence}
\begin{equation*}
  \tau(\mathsf{A}) \coloneq \sup_{i \in \mathbb{N}} \tau_{i}(\mathsf{A}) = \sup_{i \in \mathbb{N}} \sum_{j \in \mathscr{P}_{q}(\mathbb{N})} a_{ij}^{2}.
\end{equation*}
\end{itemize}

We always have that
\begin{equation*}
  \tau_{i}(\mathsf{A}) \leq \tau(\mathsf{A}) \leq \tr \mathsf{A}^{2}.
\end{equation*}
For $I$ and $J \subset \mathbb{N}$, we also write
\begin{equation*}
  \mathsf{A}(I,J) \coloneq (a_{ij} : (i,j) \in I \times J),
\end{equation*}
for the extracted operator.
As anticipated, our \cref{th:regularity-quadratic-form} deals with operator whose spectral remainders are positive and influence is small.
The following result shows that it is actually sufficient to control the spectral radius.

\begin{lemma}\label{th:spectral-radius-implies-influence}
  Let $\mathsf{A}$ be a symmetric Hilbert--Schmidt operator with $\tr \mathsf{A}^{2} = 1$.
  Then,
  \begin{align}
    & \label{eq:bound-spectral-remainder-spectral-radius} \mathcal{R}_{q}(\mathsf{A}) \geq \prod_{k=1}^{q-1} (1- k \rho(\mathsf{A})^{2})
  \\& \label{eq:bound-influence-spectral-radius} \tau(\mathsf{A}) \leq \rho(\mathsf{A})^{2}.
  \end{align}
\end{lemma}

\begin{proof}
  By definition,
  \begin{equation*}
    \mathcal{R}_{q}(\mathsf{A}) = \sum_{i_{1} \ne \dots \ne i_{q}} \lambda_{i_{1}}^{2} \dots \lambda_{i_{q-1}}^{2} \paren*{ \sum_{i_{q} \not\in \set{i_{1},\dots, i_{q-1}}} \lambda_{i_{q}}^{2} } \geq \mathcal{R}_{q-1}(\mathsf{A}) \sum_{i \geq q} \lambda_{i}^{2} \geq \mathcal{R}_{q-1}(\mathsf{A}) (1-(q-1)\rho(\mathsf{A})^{2}).
  \end{equation*}
  \cref{eq:bound-spectral-remainder-spectral-radius} follows by an immediate induction.
For \cref{eq:bound-influence-spectral-radius} consider $a_{i}$ the $i$-th row vector of $\mathsf{A}$ and $e_{i}$ the $i$-th vector of the canonical basis.
Then, by the Cauchy--Schwarz inequality
\begin{equation*}
  \tau_{i}(\mathsf{A}) = \psh{\mathsf{A} e_{i}}{a_{i}} \leq \norm{\mathsf{A} e_{i}} \norm{a_{i}} \leq \rho(\mathsf{A}) \tau_{i}(\mathsf{A})^{1/2}.
\end{equation*}
The proof is complete.
\end{proof}


\paragraph{Properties of the spectral remainders}
Provided $\rk \mathsf{A} < q$, then $\mathcal{R}_{q}(\mathsf{A}) = 0$.
Actually, the spectral remainder $\mathcal{R}_{q}(\mathsf{A})$ measures the distance of $\mathsf{A}$ to the operators of rank $q$.
See \cite[Lem.\ 3 \& 4]{HMP} for a precise statement.

The following representation of the spectral remainder will come handy.
\begin{lemma}[{\cite[Thm.\ 6]{CauchyBinet}}]\label{th:cauchy-binet}
  For a symmetric Hilbert--Schmidt operator $\mathsf{A}$:
  \begin{equation*}
    \mathcal{R}_{q}(\mathsf{A}) = \sum_{\substack{I,\, J \subset \mathbb{N}\\\abs{I} = \abs{J} = q}} \bracket*{\det \mathsf{A}(I,J)}^{2}.
  \end{equation*}
\end{lemma}

A first consequence of this representation is the following.
\begin{lemma}\label{th:additivity-spectral-remainders}
  Let $\mathsf{A} = (a_{ij})$ and $\mathsf{A}' = (a'_{ij})$ be symmetric Hilbert--Schmidt operators on $\ell^{2}(\mathbb{N})$ such that $a_{ij} a'_{ij} = 0$ for all $i$ and $j \in \mathbb{N}$.
  Then, for all $p \in \mathbb{N}$,
  \begin{equation*}
    \mathcal{R}_{p}(\mathsf{A} + \mathsf{A'}) \geq \mathcal{R}_{p}(\mathsf{A}) + \mathcal{R}_{p}(\mathsf{A'}).
  \end{equation*}
\end{lemma}

\begin{proof}
  Let us write
  \begin{equation*}
    \supp \mathsf{A} \coloneq \set*{ (I, J) \subset \mathbb{N}^{2} :a_{ij} \ne 0, \, (i,j) \in I \times J },
  \end{equation*}
  and similarly for $\supp \mathsf{A}'$.
  BY assumption, $\supp \mathsf{A}$ and $\supp \mathsf{A}'$ are disjoint.
  Thus, by \cref{th:cauchy-binet}
  \begin{equation*}
    \mathcal{R}_{p}(\mathsf{A}' + \mathsf{A}) \geq \sum_{\substack{(I,J) \in \supp \mathsf{A}\\ \abs{I} = \abs{J} = p}} \bracket*{\det \mathsf{A}(I,J) }^{2} + \sum_{\substack{(I,J) \in \supp \mathsf{A}'\\ \abs{I} = \abs{J} = p}} \bracket*{\det \mathsf{A}'(I,J) }^{2}.
  \end{equation*}
\end{proof}

\subsubsection{Determinantal operators associated with a Hilbert--Schmidt operator}\label{s:linear-algebra-ell-1}

Let $q \in \mathbb{N}$ and $\mathscr{P}_{q}(\mathbb{N}) \coloneq \set*{ I \subset \mathbb{N} : \abs{I} = q }$.
We identify $\mathscr{P}_{1}(\mathbb{N}) = \mathbb{N}$.
In view of the Cauchy--Binet formula, it is natural to consider the operator $\mathsf{B} \coloneq (b_{IJ} : I,\, J \in \mathscr{P}_{q}(\mathbb{N}))$ with non-negative coefficients
\begin{equation*}
  b_{IJ} \coloneq \bracket*{\det \mathsf{A}(I,J)}^{2}.
\end{equation*}
We treat $\mathsf{B}$ as a genuine operator, however due to the non-negative nature of its entries, we consider $\ell^{1}$-quantities rather than $\ell^{2}$:
\begin{itemize}
  \item  the \emph{total mass}:
    \begin{equation*}
      \sigma(\mathsf{B}) \coloneq \norm{b}_{\ell^{1}} \coloneq \sum_{i,j} b_{ij} = \mathcal{R}_{q}(\mathsf{A});
    \end{equation*}
  \item the \emph{$\ell^{1}$-partial influences} of the index $i$:
    \begin{equation*}
      \upsilon_{i}(\mathsf{B}) \coloneq \sum_{I,\,J \in \mathscr{P}_{q}(\mathbb{N})} 1_{i \in I \cup J} b_{IJ};
    \end{equation*}
  \item the \emph{$\ell^{1}$-maximal influences}:
    \begin{equation*}
      \upsilon(\mathsf{B}) \coloneq \sup_{i} \upsilon_{i}(\mathsf{B}).
    \end{equation*}
\end{itemize}


\begin{lemma}\label{th:influence-det}
  With the above notations, assume that $\tr \mathsf{A}^{2} = 1$, then
  \begin{equation*}
    \upsilon(\mathsf{B}) \leq 2 q \tau(\mathsf{A}).
  \end{equation*}
\end{lemma}

\begin{proof}
  Let $i \in \mathbb{N}$.
  For $I$ and $J\in \mathscr{P}_{q}(\mathbb{N})$, write $I = \set{ i_{1}, \dots, i_{q}}$ and $J = \set{j_{1}, \dots, j_{q}}$ with the elements being increasingly ordered.
  By definition, we have
  \begin{equation*}
    \det \mathsf{A}(I, J) = \sum_{\sigma \in \Sigma_{q}} (-1)^{\abs{\sigma}} \prod_{l=1}^{q} a_{i_{l} j_{\sigma(l)}}.
  \end{equation*}
  By Jensen's inequality,
  \begin{equation*}
    b_{IJ} = \bracket*{ \det \mathsf{A}(I,J) }^{2} \leq q! \sum_{\sigma \in \Sigma_{q}} a_{i\sigma(j)}^{2}.
  \end{equation*}
  Thus, we find that
  \begin{equation*}
    \upsilon_{i}(\mathsf{B}) \leq \sum_{I \cup J \ni i} q! \sum_{\sigma \in \Sigma_{q}} \prod_{l=1}^{q} a^{2}_{i_{l}j_{\sigma(l)}}.
  \end{equation*}
  On the one hand, when $(j_{1}, \dots, j_{q})$ ranges through increasingly ordered sets $(j_{\sigma(1)}, \dots, j_{\sigma(q)})$ for $\sigma$ ranging in $\Sigma_{q}$ ranges through all the pairwise disjoint indices $(j_{1}, \dots, j_{q})$.
  On the other hand to go from increasingly ordered $(i_{1}, \dots, i_{q})$ to pairwise disjoint $(i_{1}, \dots, i_{q})$, we have to pay a factor $q!$.
  It follows that
  \begin{equation*}
    \upsilon_{i}(\mathsf{B}) \leq 2 \sum_{i_{1} \ne \dots \ne i_{q}} \sum_{j_{1} \ne \dots \ne j_{q}} 1_{\set*{\exists l : i_{l} = i}} \prod_{l=1}^{q} a^{2}_{i_{l}j_{l}}.
  \end{equation*}
  By symmetry, we finally get
  \begin{equation*}
    \upsilon_{i}(\mathsf{B}) \leq 2 q \paren*{\sum_{j} a^{2}_{ij}}\paren*{ \sum_{k,j} a_{kj}^{2}}^{q-1} = 2q \tau_{i}(\mathsf{A}) \paren*{\tr \mathsf{A}^{2}}^{q-1}.
  \end{equation*}
  This concludes the proof since $\tr \mathsf{A}^{2} = 1$.
\end{proof}

\subsection{Sobolev regularity and Sobolev regularity in Fourier modes}

\subsubsection{Regularity for functions}
Let us first define some quantities in order to measure this regularity at the level of the density function $f$.

\paragraph{Hölder regularity.}
For $k \in \mathbb{N}$, we write $\mathscr{C}^{k}$ for the space of $k$ times continuously differentiable functions on $\mathbb{R}$, equipped with the norm
\begin{equation*}
  \norm{f}_{\mathscr{C}^{k}} \coloneq \max_{l=1,\dots,k} \Sup*{ \abs{f^{(l)}(x)} : x \in \mathbb{R} }.
\end{equation*}
Additionally, for $\alpha \in [0,1]$, we write $\mathscr{C}^{k,\alpha}$ for the space of $f \in \mathscr{C}^{k}$ whose $k$-th derivative is also $\alpha$-Hölder, equipped with the norm
\begin{equation*}
  \norm{f}_{\mathscr{C}^{k,\alpha}} \coloneq \norm{f}_{\mathscr{C}^{k}} + \sup_{x\ne y} \frac{\abs{f^{(k)}(x) - f^{(k)}(y)}}{\abs{x-y}^{\alpha}}.
\end{equation*}

\paragraph{Sobolev regularity.}
Actually, it is more convenient to work with the notion of weak regularity whose basic definitions are recalled below.
For $p \in [1,\infty]$, we write $\mathscr{L}^{p}$ for the \emph{Lebesgue space of order $p$} on $\mathbb{R}$.
We also define $\mathscr{S}$ is the \emph{Schwartz space} of rapidly decreasing functions on $\mathbb{R}$, and $\mathscr{S}'$ is its dual the space of \emph{tempered distributions}.
On $\mathscr{S}'$, we can define by duality a \emph{derivative operator} $\partial$.
By spectral calculus, we can actually make sense of any power of the massive Laplace operator, that is we can define, for all $s \in \mathbb{R}$, the operator $(1-\partial^{2})^{s}$ on $\mathscr{S}'$.
Every element $f \in \mathscr{L}^{p}$ induces a tempered distribution still denoted by $f$.
Conversely, for a tempered distribution $T$ we write $T \in \mathscr{L}^{p}$ whenever it is induced by a function in $\mathscr{L}^{p}$.
Recall the definition of the \emph{fractional Sobolev spaces}:
\begin{equation*}
  \mathscr{W}^{s,p} \coloneq \set*{ f \in \mathscr{S}' : (1-\partial^{2})^{s/2} f \in \mathscr{L}^{p} },  \qquad p \in [1,\infty],\, s \in \mathbb{R},
\end{equation*}
equipped with the norm
\begin{equation*}
  \norm{f}_{\mathscr{W}^{s,p}} \coloneq \norm{(1 - \partial^{2})^{s/2} f}_{\mathscr{L}^{p}}.
\end{equation*}
For $k \in \mathbb{N}$, the Sobolev norm $\norm{\cdot}_{\mathscr{W}^{s,p}}$ is equivalent to the \emph{classical} Sobolev norm
\begin{equation*}
  \norm{f}_{\mathscr{L}^{p}} + \norm{\partial^{s}f}_{\mathscr{L}^{p}}.
\end{equation*}
Sobolev spaces are relevant for our analysis due to the celebrated \emph{Sobolev embeddings}
\begin{equation}\label{eq:embedding-sobolev-holder}
  \mathscr{W}^{s,p} \hookrightarrow \mathscr{C}^{k,\alpha}, \qquad k + \alpha = s - \frac{1}{p},\, k \in \mathbb{N}, \, \alpha \in [0,1].
\end{equation}

\paragraph{Sobolev regularity in Fourier mode.}
In this work, it is convenient to work with the \emph{Fourier transform}.
Recall that by duality, we can define, on $\mathscr{S}'$, the \emph{Fourier transform}, and its inverse $\mathscr{F}^{-1}$.
Recall that the Fourier transform sends differential operators to multiplication operators.
In particular,
\begin{equation*}
  \mathcal{F}\bracket*{(1-\partial^{2})^{s} f}(\xi) = (1+\xi^{2})^{s} \hat{f}(\xi), \qquad \xi \in \mathbb{R},\, s \in \mathbb{R}.
\end{equation*}
We also write $\hat{f} = \mathcal{F} f$.
We then define the \emph{Sobolev space in Fourier mode}
\begin{equation*}
  \mathcal{F} \mathscr{W}^{s,p} \coloneq \set*{ f \in \mathscr{S}' : ( \xi \mapsto (1+\xi^{2})^{s/2} \hat{f}(\xi) ) \in \mathscr{L}^{p} }, \qquad p \in [1,\infty], \, s \in \mathbb{R},
\end{equation*}
equipped with the norm
\begin{equation*}
  \norm{f}_{\mathcal{F}\mathscr{W}^{s,p}} \coloneq \norm{(1+\xi^{2})^{s/2} \hat{f}}_{\mathscr{L}^{p}} = \norm*{ \mathcal{F} \bracket*{ (1-\partial^{2})^{s/2} f} }_{\mathscr{L}^{p}}.
\end{equation*}
Due to the Fourier isomorphism theorem \cite[Thm.\ 7.1.11]{Hormander}, we have that $\mathcal{F} \mathscr{W}^{s,2} = \mathscr{W}^{s,2}$ for all $s \in \mathbb{R}$.
In general, by \cite[Thm.\ 7.1.13]{Hormander}, for $p \in [1,2]$ and $p' \coloneq p/(p-1) \in [2,\infty]$ the Hölder conjugate of $p$, we have that $\mathcal{F} \colon \mathscr{L}^{p} \to \mathscr{L}^{p'}$ is bounded.
While, by \cite[Thm.\ 7.9.3]{Hormander}, for $q > 2$, and $s > 1/2 - 1/q$, $\mathcal{F} \colon \mathscr{L}^{q} \to \mathscr{W}^{-s,2}$ is bounded.
Consequently, we have that
\begin{equation}\label{eq:embedding-fourier-sobolev}
  \mathscr{W}^{s,p} \hookrightarrow \mathcal{F} \mathscr{W}^{s,p'} \hookrightarrow \mathscr{W}^{s-t,2}, \qquad p \in [1,2],\, s\in \mathbb{R},\, t > \frac{1}{p} - \frac{1}{2}.
\end{equation}

\subsubsection{Regularity at the level of the random variable}


It is more convenient to manipulate quantities defined at the level of a random variables rather than referring to its density.
Whenever a random variable $Z$ has density $f$, we let
\begin{align*}
  & \mathbf{N}_{s,p}(Z) \coloneq \norm{f}_{\mathcal{F}\mathscr{W}^{s,p}}
\\& \mathbf{W}_{s,p}(Z) \coloneq \norm{f}_{\mathscr{W}^{s,p}}.
\end{align*}
We easily verify that
\begin{equation*}
  \mathbf{N}_{s,p}(Z) = \paren*{ \int \abs*{ \paren*{1+\xi^{2}}^{s/2} \Esp*{ \mathrm{e}^{\mathrm{i} \xi Z} } }^{p} \mathrm{d} \xi }^{1/p},
\end{equation*}
where, when $p =\infty$, the above integral is understood as an essential supremum.
Owing to the duality between $\mathscr{L}^{p}$ and $\mathscr{L}^{p'}$, we find that
\begin{equation*}
  \mathbf{W}_{s,p}(Z) = \Sup*{ \Esp*{ \abs{ (1-\partial^{2})^{s/2} \varphi(Z)} } : \varphi \in \mathscr{C}_{c}^{\infty},\, \norm{\varphi}_{\mathscr{L}^{p'}} \leq 1 }.
\end{equation*}


\section{Regularity of laws of a random variables arising from a Dirichlet structure}

\subsection{Generalities on Dirichlet forms}\label{s:generalities-dirichlet}

\subsubsection{Reminders and notations}\label{s:reminders-dirichlet}
Let us first recall some basic notions regarding Dirichlet forms.
We follow \cite{BouleauHirsch} (see also \cite{FOT,BGL}).
In all the paper, $(\Omega, \mathfrak{W}, \prob)$ is a sufficiently large probability space.
We assume that $\Omega$ contains a Fréchet space $E$, and we let $\mathfrak{B}(E)$ be the Borel $\sigma$-algebra of $E$.
We write $\mathbb{L}^{p} \coloneq L^{p}(E, \mathfrak{B}(E), \prob)$.

\paragraph{Dirichlet forms and their generators.}
A \emph{Dirichlet form} over $E$ is a closed symmetric non-negative bilinear form defined on a dense Hilbert space $\dom \mathcal{E} \subset \mathbb{L}^{2}$
\begin{equation*}
  \mathcal{E} \colon \dom \mathcal{E} \times \dom \mathcal{E} \to \mathbb{R},
\end{equation*}
and additionally satisfying the so-called \emph{Markov property}:
\begin{equation*}
  \mathcal{E}(F \wedge 1, F \wedge 1) \leq \mathcal{E}(F, F), \qquad F \in \dom \mathcal{E}.
\end{equation*}
We always assume that the Dirichlet form is \emph{diffusive} (also known as \emph{local}), that is
\begin{equation*}
  \mathcal{E}(\varphi(F), \psi(F)) = 0, \qquad \varphi,\, \psi \in \mathscr{C}^{\infty}_{c},\, \varphi \psi = 0.
\end{equation*}
To such a Dirichlet form corresponds a unbounded self-adjoint \emph{Markov generator}
\begin{equation*}
  \mathsf{L} \colon \mathbb{L}^{2} \to \mathbb{L}^{2},
\end{equation*}
with domain $\dom \mathsf{L} \subset \dom \mathcal{E}$, and characterized by the following integration by parts formula
\begin{equation*}
  \mathcal{E}(F, R) = - \Esp*{ F \mathsf{L} R}, \qquad F \in \dom \mathcal{E},\, R \in \dom \mathsf{L}.
\end{equation*}

\paragraph{Dirichlet-Sobolev spaces.}
The non-negative self-adjoint operator $-\mathsf{L}$ admits a spectral decomposition $\set*{ \mathsf{E}_{\lambda} : \lambda \geq 0 }$.
This allows to define the unbounded operator $\psi(-\mathsf{L})$ acting on $\mathbb{L}^{2}$ for all measurable $\psi$ on $\mathbb{R}_{+}$.
In particular, we can define the \emph{Sobolev spaces}
\begin{equation*}
  \mathbb{D}^{s,p} \coloneq \set*{ F \in \mathbb{L}^{p} : (1-\mathsf{L})^{s/2} F \in \mathbb{L}^{p} }, \qquad s \geq 0,\, p \in [1,\infty],
\end{equation*}
equipped with the norm
\begin{equation*}
  \norm{F}_{\mathbb{D}^{s,p}} \coloneq \norm{ (1-\mathsf{L})^{s/2} F}_{\mathbb{L}^{p}}.
\end{equation*}
%
%
%
%
%
We often abbreviate $\norm{\cdot}_{\mathbb{D}^{s,p}} = \norm{\cdot}_{s,p}$.
Remark that it is also possible to define Sobolev spaces of negative orders but we do not use them here.
We have that $\dom \mathcal{E} = \mathbb{D}^{1,2}$ and $\dom \mathsf{L} = \mathbb{D}^{2,2}$.
We have that
\begin{equation*}
  \mathbb{D}^{s,p} \hookrightarrow \mathbb{D}^{r,q}, \qquad s \geq r,\, 1 < q \leq p < \infty.
\end{equation*}
We also write
\begin{equation*}
  \mathbb{D}^{\infty} \coloneq \bigcap_{s \geq 0,\, p \in (1,\infty)} \mathbb{D}^{s,p}.
\end{equation*}
The space $\mathbb{D}^{\infty}$ is an algebra stable under the action of $\mathsf{L}$ and $\Gamma$.
Given a separable Hilbert space $H$, we similarly define the $H$-valued versions of the Sobolev spaces, denoted by $\mathbb{L}^{p}(H)$, $\mathbb{D}^{s,p}(H)$, and $\mathbb{D}^{\infty}(H)$.

\paragraph{Carré du champ}
We always assume that the form admits a \emph{carré du champ}: there exists a non-negative symmetric continuous bilinear operator
\begin{equation*}
  \Gamma \colon \dom \mathcal{E} \times \dom \mathcal{E} \to \mathbb{L}^{1},
\end{equation*}
such that
\begin{equation*}
  \Esp*{ S \Gamma(F,R) } = \frac{1}{2} \paren*{ \mathcal{E}(FS, R) + \mathcal{E}(RS, F) - \mathcal{E}(S, FR)} , \qquad F,\, R,\, S \in \dom \mathcal{E} \cap \mathbb{L}^{\infty}.
\end{equation*}
For smooth random variables, we have the straightforward definition
\begin{equation*}
  \Gamma(F, R) \coloneq \frac{1}{2} \paren*{ \mathsf{L}(FR) - F \mathsf{L} R - R \mathsf{L} F} , \qquad F,\,R \in \mathbb{D}^{\infty}.
\end{equation*}
More generally, if $F = (F_{1}, \dots, F_{l}) \in \paren*{\dom \mathcal{E}}^{l}$ and $R = (R_{1}, \dots, R_{m}) \in \paren*{\dom \mathcal{E}}^{p}$, we consider the matrix-valued carré du champ
\begin{equation*}
  \Gamma(F, R)_{ij} \coloneq \Gamma(F_{i}, R_{j}), \qquad 1 \leq i \leq l,\, 1 \leq j \leq m.
\end{equation*}

\paragraph{Important formulas related to the locality}
Since the Dirichlet form is \emph{diffusive}, the carré du champ satisfies a chain rule
\begin{equation}\label{eq:chain-rule-carre-du-champ}
  \Gamma(\varphi(F), R) = \varphi'(F) \Gamma(F,R), \qquad F,\, R \in \dom \mathcal{E},\, \varphi \in \mathscr{C}^{1};
\end{equation}
and a Leibniz rule
\begin{equation}\label{eq:leibniz-carre-du-champ}
  \Gamma(FR, S) = R\Gamma(F,S) + F\Gamma(R,S).
\end{equation}
In particular, we find the formula
\begin{equation}\label{eq:leibniz-formula-3}
  \Esp*{ \Gamma(F, S) R } = - \Esp*{ FR \mathsf{L} S } - \Esp*{ \Gamma(R,S) F}, \qquad F,\, R,\, S \in \mathbb{D}^{\infty}.
\end{equation}

%

%
%
%
%
%
  %
%
%


\subsubsection{Regularity of laws from positivity of the carré du champ}\label{s:regularity-positivity-gamma}

The analysis of Dirichlet forms is relevant for the study of regularity of laws.
Indeed, as it is well-known: if $F \in \mathbb{D}^{\infty}$ and $\Gamma(F,F)^{-1} \in \cap_{1<p<\infty} \mathbb{L}^{p}$, then $F$ has a law that is $\mathscr{C}^{\infty}$.
See \cite[Thm.\ 1.14]{Watanabe} for this statement on the Wiener space.
Here, we give a quantitative version of this estimate in the setting of Dirichlet forms.

Let us define recursively $R_{0} \coloneq 1$, and
\begin{equation*}
  R_{k+1} \coloneq  \Gamma(F,F) \bracket*{ - R_{k} \mathsf{L} F - \Gamma(F, R_{k}) } + R_{k} \Gamma(F, \Gamma(F,F)), \qquad k \in \mathbb{N}.
\end{equation*}
We immediately verify that $R_{k} \in \mathbb{D}^{\infty}$ for all $k \in \mathbb{N}$.
The quantities $R_{k}$'s allows us to perform integration by parts and thus to control the regularity of the density of $F$.
\begin{proposition}\label{th:integration-by-parts-gamma}
  Let $F \in \mathbb{D}^{\infty}$ such that $\Gamma(F,F)^{-1} \in \mathbb{L}^{4k}$ for some $k \in \mathbb{N}$.
  Then,
\begin{equation}\label{eq:ipp-phi-k-gamma}
  \Esp*{ \varphi^{(k)}(F) } = \Esp*{ \varphi(F) \frac{R_{k}}{\Gamma(F,F)^{2k}} }, \qquad \varphi \in \mathscr{C}^{\infty}_{c}.
\end{equation}
  In particular, we have that
  \begin{equation}\label{eq:bound-sobolev-gamma}
  \mathbf{W}_{k,1}(F) \leq \Esp*{ \frac{\abs{R_{k}}}{\Gamma(F,F)^{2k}}  }.
  \end{equation}
\end{proposition}
%

\begin{proof}
  Since $\varphi$ is bounded, by Hölder's inequality the right-hand side of \cref{eq:ipp-phi-k-gamma} is well-defined.
  For short we will write in this proof $S \coloneq \Gamma(F,F)$ and
  \begin{equation*}
    W_{k} \coloneq \frac{R_{k}}{\Gamma(F,F)^{2k}} = \frac{R_{k}}{S^{2k}}.
  \end{equation*}
  We establish \cref{eq:ipp-phi-k-gamma} by induction.
  For $k = 0$, it is trivial.
  Assume that $S^{-1} \in \mathbb{L}^{4k+4}$ and that the claim is established up to some $k \in \mathbb{N}$.
  By \cref{eq:ipp-phi-k-gamma}, we get that
  \begin{equation}\label{eq:ipp-gamma-induction-step}
    \Esp*{ \varphi^{(k+1)}(F) } = \Esp*{ \varphi'(F) W_{k} }.
  \end{equation}
  Using the diffusion property \cref{eq:chain-rule-carre-du-champ}, we get that
  \begin{equation}\label{eq:ipp-gamma-appears}
    \Esp*{ \varphi'(F) W_{k} } = \Esp*{ \Gamma(\varphi(F), F) \frac{R_{k}}{S^{2k+1}} }.
  \end{equation}
  We shall now carry out computation as if $S \in \mathbb{D}^{\infty}$.
  On the one hand, we have that $S^{-2k-1} \in \mathbb{L}^{2}$.
  On the other hand, by the chain rule \cref{eq:chain-rule-carre-du-champ}, we find that
  \begin{equation*}
    \Gamma(S^{-2k-1}, S^{-2k-1}) = (2k+1)^{2} S^{-4k-2} \Gamma(S, S).
  \end{equation*}
  Let us mention that this \emph{a priori} formal computation can be made rigorous by doing the computation at the level of the smooth random variable $\paren*{ S + \varepsilon}^{-2k-1}$, and then letting $\varepsilon \to 0$.
  Since $\Gamma(S, S) \in \mathbb{D}^{\infty}$, this shows that
  \begin{equation*}
    \Gamma(S^{-2k-1}, S^{-2k-1})^{1/2} \in \mathbb{L}^{2}.
  \end{equation*}
  Thus, by Meyer's inequalities \cite[Chap.\ II, Thm.\ 7.3.3]{BouleauHirsch}, we have that $S^{-2k-1} \in \mathbb{D}^{1,2} = \dom \mathcal{E}$.
  Since $R_{k} \in \dom \mathcal{E}$, we have shown that $R_{k}S^{-2k-1} \in \dom \mathcal{E}$.
  Since we also have that $\varphi(F)$ and $F \in \mathbb{D}^{\infty}$, all the following computations make sense.
  By integration by parts and the Leibniz rule, we get that
  \begin{equation*}
    \begin{split}
      - \Esp*{ \varphi(F) \frac{R_{k}}{S^{2k+1}} \mathsf{L} F } &= \Esp*{ \Gamma\paren*{\varphi(F) \frac{R_{k}}{S^{2k+1}}, F} }
                                                              \\&= \Esp*{ \Gamma(\varphi(F), F) \frac{R_{k}}{S^{2k+1}} } + \Esp*{ \varphi(F) \Gamma\paren*{\frac{R_{k}}{S^{2k+1}}, F} }.
    \end{split}
  \end{equation*}
  Combining with \cref{eq:ipp-gamma-induction-step,eq:ipp-gamma-appears} yields
  \begin{equation*}
    \Esp*{ \varphi^{(k+1)}(F) } = \Esp*{ \varphi(F) \bracket*{ - R_{k}S^{-2k-1} \mathsf{L}F - \Gamma(R_{k}S^{-2k-1}, F) } }.
  \end{equation*}
  By the Leibniz rule and the chain rule, we find that
  \begin{equation*}
    \Gamma(R_{k}S^{-2k-1},F) = S^{2k-1} \Gamma(R_{k}, F) - (2k+1) R_{k} \Gamma(S,F) S^{-2k-2}.
  \end{equation*}
  Recalling that $S = \Gamma(F,F)$ this concludes the proof in view of the definition of the $R_{k}$'s.
\end{proof}

By construction the $R_{k}$'s are obtained by iteratively applying the carré du champ operator and the generator to $F$.
In particular, it norms can be control by some monomial in a \emph{finite number} of the Sobolev norms $\mathbb{D}^{s,p}$ with $s \in \mathbb{R}$ and $p \in (1,\infty)$ possibly multiplied by a positive constant.
We thus introduce a new notation $\Psi(\norm{F}_{\mathbb{D}^{\infty}})$ to indicate quantities controlled by such monomial, in other words
\begin{equation*}
  \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \coloneq O\paren*{\prod_{j=1}^{l} \norm{F}_{\mathbb{D}^{s_{j},p_{j}}}^{k_{j}}},
\end{equation*}
for some possibly changing values of $l \in \mathbb{N}$, $s_{j} \in \mathbb{R}$, $p_{j} \in (1,\infty)$, and $k_{j} \in \mathbb{N}$.
We stress that $\norm{F}_{\mathbb{D}^{\infty}}$ is not even defined.
With this notation, we can rephrase \cref{th:integration-by-parts-gamma} as follows.
\begin{theorem}\label{th:sobolev-gamma-l-minus-p}
  Let $k \in \mathbb{N}$ and $F \in \mathbb{D}^{\infty}$ such that $\Gamma(F, F)^{-1} \in \mathbb{L}^{4k}$.
  Then, $F$ has a density in $\mathscr{W}^{k,1}$ with
  \begin{equation*}
    \mathbf{W}_{k,1}(F) \leq \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \norm{\Gamma(F,F)^{-2k}}_{\mathbb{L}^{2}}.
  \end{equation*}
\end{theorem}

\subsection{Regularity for smooth random variable of an independent sequence}\label{s:regularity-independent-sequence}

We now use the formalism of Dirichlet forms to derive regularity estimates for the density of a smooth random variable with respect to a sequence of independent and identically distributed random variables.
Roughly speaking, we construct conditionally \emph{Gaussian random variables} encoding the properties of the carré du champ.

\subsubsection{The Bouleau derivative \texorpdfstring{$\sharp$}{\#} associated with an independent sequence}\label{s:bouleau-derivative}
Fix a diffusive Dirichlet structure $\mathcal{E}$ with carré du champ $\Gamma$.
We now want to study regularity of the law of $F \in \mathbb{D}^{\infty}$.
As anticipated, our main contribution is to give sufficient conditions for the integrability of $\Gamma(F,F)^{-1}$ whenever $F$ is a function of independent random variables.

\paragraph{Smooth coordinate system}
Let us thus fix $X = (X_{i})$ a sequence of independent random variables such that $X_{i} \in \mathbb{D}^{\infty}$.
In general, $\Gamma(X_{i}, X_{i})$ is not measurable with respect to $X_{i}$.
In particular, $\Gamma(X_{i}, X_{i})$ and $\Gamma(X_{j}, X_{j})$ might fault to be independent.
In order to simplify the definition, we assume a form of \emph{Dirichlet-independence} on $(X_{i})$. 
For all $i \in \mathbb{N}$, we define recursively $\Gamma^{(0)}_{i} \coloneq X_{i}$ and
\begin{equation*}
  \Gamma^{(k+1)}_{i} \coloneq \Gamma(\Gamma^{(k)}_{i}, \Gamma^{(k)}_{i}).
\end{equation*}
Our assumption thus reads as follows
\begin{equation}\label{ass:dirichlet-independence}
  \Gamma_{i}^{(k)} \Vbar \Gamma_{j}^{(k')}, \qquad \text{and} \qquad \Gamma(\Gamma_{i}^{(k)}, \Gamma_{j}^{(k')}) = 0, \qquad i \ne j, k,\, k' \in \mathbb{N}.
\end{equation}

\paragraph{The Malliavin matrices of \texorpdfstring{$X$}{X}}

We often abbreviate $\Gamma_{i} \coloneq \Gamma^{(1)}_{i}$.
We define, for $n \in \mathbb{N}$
\begin{equation*}
  \begin{cases}
  & V_{i}^{(n)} \coloneq (\Gamma_{i}^{(0)}, \Gamma_{i}^{(1)}, \dots, \Gamma_{i}^{(n-1)}),
\\& M^{(n)} \coloneq \Gamma(V^{(n)}, V^{(n)}).
  \end{cases}
\end{equation*}
By definition, $M^{(n)}$ is a symmetric non-negative matrix.
In view of \cref{ass:dirichlet-independence}, $M^{(n)}$ is diagonal by blocks, each block being of size $n \times n$.
We write $B^{(n)}_{i}$ for the $n \times n$ matrix composing the $i$-th block of $M^{(n)}$.
For instance, we have that $B^{(1)}_{i} = \Gamma_{i}$ and
\begin{equation*}
  B^{(2)}_{i} \coloneq \begin{pmatrix}
    \Gamma_{i} & \Gamma(X_{i}, \Gamma_{i}) \\
    \Gamma(X_{i}, \Gamma_{i}) & \Gamma(\Gamma_{i}, \Gamma_{i})
  \end{pmatrix}
\end{equation*}
By construction, we also have that
\begin{equation*}
  \paren*{B^{(n+1)}_{i}}_{1 \leq j,l \leq n} = B^{(n)}_{i}.
\end{equation*}
We only use $B^{(1)}$, $B^{(2)}$, $M^{(1)}$, and $M^{(2)}$ in our construction.
We let $\mathfrak{B}_{0}(E) \coloneq \sigma(X)$, and we define
\begin{equation*}
\mathfrak{B}_{k+1}(E) \coloneq \sigma\paren*{X, M^{(k+1)}}.
\end{equation*}
For instance, a random variable $F$ is $\mathfrak{B}_{1}(E)$-measurable provided it is measurable provided $F = f(X_{1}, \Gamma_{1}, X_{2}, \Gamma_{2}, \dots)$ for some $f \colon (\mathbb{R}^{2})^{\infty} \to \mathbb{R}$ measurable; while $F$ is $\mathfrak{B}_{2}(E)$-measurable provided
\begin{equation*}
  F = f(X_{1}, \Gamma_{1}, \Gamma(X_{1}, \Gamma_{1}), \Gamma(\Gamma_{1}, \Gamma_{1}), \dots).
\end{equation*}
We also let
\begin{align*}
  & \mathbb{L}^{2}_{k} \coloneq L^{2}(E, \mathfrak{B}_{k}(E), \prob);
\\& \mathbb{D}_{k} \coloneq \mathbb{D} \cap \mathbb{L}^{2}_{k};
\\& \mathbb{D}^{\infty}_{k} \coloneq \mathbb{D}^{\infty} \cap \mathbb{L}^{2}_{k}.
\end{align*}
Informally, random variables in $\mathbb{D}^{\infty}_{k}$ are obtained by iteratively taking carré du champ of elements of smooth random variables measurable with respect to $X$.
Indeed, if $F$ and $R \in \mathbb{D}^{\infty}_{k}$ then $\Gamma(F, R) \in \mathbb{D}^{\infty}_{k+1}$.
We write $\mathbb{S}_{k}(\mathbb{R})$ for the set of symmetric matrices of size $k \times k$ with real coefficients.
For every $F \in \mathbb{L}^{2}_{k}$ there exists a measurable function $f \colon \paren*{\mathbb{R} \times \mathbb{S}_{k}(\mathbb{R})}^{\infty} \to \mathbb{R}$ such that
\begin{equation*}
  F = f(X_{1}, B^{(k)}_{1}, X_{2}, B^{(k)}_{2}, \dots).
\end{equation*}
Whenever it makes sense, we use the notation $\partial_{x_{i}}f$ to indicate the partial derivative with respect to the $i$-th coordinate; $\nabla_{b_{i}}f$ to indicate the partial gradient with respect to the $i$-th matrix coordinate; and, more generally,
\begin{equation*}
  \nabla_{i} f \coloneq
  \paren*{\begin{array}{c|c}
      \partial_{x_{i}}f & 0 \\
      \hline
      0 & \nabla_{b_{i}} f
  \end{array}} \in \mathbb{S}_{k+1}(\mathbb{R}).
\end{equation*}

\paragraph{The Bouleau derivatives}
Fix $G = (G_{i}$ a sequence of independent standard Gaussian variables, that are also independent of $\mathfrak{B}(E)$.
We use the notation $G^{\otimes k}$ to indicate a sequence where each of the $G_{i}$ is replaced by a $k \times k$ Gaussian matrix with independent entries.
Take $F$ a \emph{smooth cylinder function} on $\mathbb{L}^{2}_{k}$, that is $F = f(X_{1}, B^{(k)}_{1}, \dots, X_{n}, B^{(k)}_{n})$ for some $n \in \mathbb{N}$, and $f \in \mathscr{C}^{\infty}_{c}\paren*{\paren*{\mathbb{R} \times \mathbb{S}_{k}(\mathbb{R})}^{n}}$.
Thus, we define
\begin{equation*}
  \sharp^{(k+1)}_{G}F \coloneq \sum_{i=1}^{n} \tr\paren*{ \nabla_{i} f(X_{1}, B^{(k)}_{1}, \dots, X_{n}, B^{(k)}_{n}) \paren*{B_{i}^{k+1}}^{1/2} G_{i}^{\otimes (k+1)}}.
\end{equation*}
Namely, whenever $F = f(X_{1}, \dots, X_{n})$, we find
\begin{equation*}
  \sharp^{(1)}_{G} F = \sum_{i=1}^{n} \partial_{x_{i}} f(X_{1}, \dots, X_{n}) \Gamma_{i}^{1/2};
\end{equation*}
and, whenever $F = f(X_{1}, \Gamma_{1}, \dots, X_{n}, \Gamma_{n})$, we have that
\begin{equation*}
\sharp^{(2)}_{G} F = \sum_{i=1}^{n} \partial_{x_{i}} f(X_{1}, \Gamma_{1}, \dots, X_{n}, \Gamma_{n}) \Gamma_{i}^{1/2} G_{i,1} + \partial_{\gamma_{i}} f(X_{1}, \Gamma_{1}, \dots, X_{n}, \Gamma_{n}) \Gamma(\Gamma_{i}, \Gamma_{i})^{1/2} G_{i,2}.
\end{equation*}
We consider $\mathbb{G}$ the closed linear span of the $G_{i}$'s in $L^{2}(\Omega, \mathfrak{W}, \prob)$.
Elements of $\mathbb{G}$ are automatically a centred Gaussian.
We similarly define $\mathbb{G}^{\otimes k}$ (this notation is consistent with the actual tensor product of Hilbert spaces).

\begin{lemma}\label{th:sharp-properties}
  For all $k \in \mathbb{N}$ the previously defined operators
  \begin{equation*}
    \sharp_{G}^{(k+1)} \colon \mathbb{L}^{2}_{k} \to \mathbb{L}_{k+1}^{2}(\mathbb{G}^{\otimes (k+1)}),
  \end{equation*}
  are closable.
  We have $\dom \sharp^{(k+1)}_{G} = \mathbb{D}_{k}$, and
  \begin{equation}\label{eq:sharp-conditional-variance}
    \Esp*{ \paren*{ \sharp^{(k+1)}_{G}F}^{2} \given \mathfrak{B}(E) } = \Gamma(F,F), \qquad F \in \mathbb{D}_{k}.
  \end{equation}
  In particular, for $F \in \mathbb{D}_{k}$ and for $j$ and $l \in \mathbb{N}$, $\sharp_{G}^{(k+j)}F$ and $\sharp_{G}^{(k+l)}F$ have the same law, namely $\mathscr{N}(0, \Gamma(F,F))$.
\end{lemma}
\begin{proof}
  We show first \cref{eq:sharp-conditional-variance} for smooth cylinder functions of $\mathbb{L}^{2}_{k}$.
  For such a $F$, we find
  \begin{equation*}
    \Esp*{ \paren*{\sharp^{(k+1)}_{G}F}^{2} \given \mathfrak{B}(E) } = \psh*{ \nabla f}{M^{(k+1)} \nabla f} = \Gamma(F, F).
  \end{equation*}
  The last equality being true by the chain rule.
  Now take $(F_{k})$ a sequence of smooth cylinder functions converging to $0$ in $\mathbb{L}^{2}_{k}$ such that $(\sharp^{(k+1)}_{G}F_{k})$ converges to some $Y \in \mathbb{L}^{2}(\mathbb{G}^{\otimes k})$.
  By the nature of the Gauss space, this implies that
  \begin{equation*}
    \Esp*{ \paren*{ \sharp_{G}F_{k}}^{2} \given \mathfrak{B}(E) } = \Gamma(F_{k}, F_{k}) \xrightarrow[k \to \infty]{} \Esp*{ Y^{2} \given \mathfrak{B}(E) }.
  \end{equation*}
  Thus $Y = 0$ since $\Gamma$ is a closed operator.
  The other points are direct consequences of \cref{eq:sharp-conditional-variance} for smooth cylinder functions.
\end{proof}
It is possible to construct the Hilbert space-valued version of the Bouleau gradient.
With a slight abuse of notation, we write, for every separable Hilbert space $\mathbb{F}$, $\sharp_{G}^{(k+1)} \colon \mathbb{L}^{2}_{k} \to \mathbb{L}_{k+1}^{2}(\mathbb{F} \otimes \mathbb{G})$.

\paragraph{Simple and iterated Bouleau derivatives on \texorpdfstring{$\mathbb{L}^{2}_{X}$}{L2X}}

Now we are concerned with the gradient and the iterated gradient for elements of $\mathbb{L}^{2}_{X} \coloneq \mathbb{L}^{2}_{0}$, those are random variables measurable with respect to $X$.
We consider $H$ another sequence of standard Gaussian, independent of $G$ and $\mathfrak{B}(E)$.
As before we define $\mathbb{H}$, $H^{\otimes k}$, and $\mathbb{H}^{\otimes k}$.
It is implicitly assume that all the $G^{\otimes k}$ and $H^{\otimes k'}$ are independent, that is $\mathbb{G}^{\otimes k}$ and $\mathbb{H}^{\otimes k'}$ are orthogonal in $L^{2}(\Omega, \mathfrak{W}, \prob)$.
The gradients $\sharp_{H}^{(k+1)} \colon \mathbb{L}^{2}_{k} \to \mathbb{L}^{2}_{k+1}(\mathbb{H}^{\otimes (k+1)})$ are defined as before.
As for $\sharp_{G}$, for every Hilbert space $\mathbb{F}$, we overload the notation $\sharp^{(k+1)}_{H}$ to also indicate  for the $\mathbb{F}$-valued version $\mathbb{L}^{2}_{k}(\mathbb{F}) \to \mathbb{L}_{k+1}^{2}(\mathbb{H}^{\otimes (k+1)} \otimes \mathbb{F})$.
In particular, we can define the \emph{simple Bouleau derivative}
\begin{equation*}
  \sharp_{G} \coloneq \sharp_{G}^{(1)} \colon \mathbb{L}^{2}_{X} \to \mathbb{L}^{2}(\mathbb{G}),
\end{equation*}
and the \emph{iterated Bouleau derivative}
\begin{equation*}
  \sharp_{H}\sharp_{G} \coloneq \sharp_{H}^{(2)} \sharp_{G}^{(1)} \colon \mathbb{L}^{2}_{X} \to \mathbb{L}^{2}(\mathbb{G} \otimes \mathbb{H}^{\otimes 2}).
\end{equation*}

\paragraph{Explicit computations on smooth cylinder functions}\label{s:explicit-bouleau-derivative}
For the reader's convenience, let us explicit the simple and the iterated gradient for $F = f(X_{1}, \dots, X_{n})$ a smooth cylinder function of $\mathbb{L}^{2}_{X}$.
We then we recall that
\begin{equation*}
  \sharp_{G} F = \sum_{i=1}^{n} \partial_{x_{i}} f(X_{1}, \dots, X_{n}) \Gamma_{i}^{1/2} G_{i}.
\end{equation*}
The expression for the iterated derivative is more involved.
In order to state it, we use the shorthand notation, $\Gamma \coloneq M^{(1)} = diag(\Gamma_{1}, \dots, \Gamma_{n})$, $M \coloneq M^{(2)}$, and $B_{i} \coloneq B^{(2)}_{i}$.
We also define the (non-independent) Gaussian vectors $\widehat{H}$ (resp.\ $\widecheck{H}$) by taking the odd (resp.\ even) indices of $M^{1/2} H^{\otimes 2}$.
Then, we have that
\begin{equation}\label{eq:iterated-sharp-cylinder}
  \begin{split}
    \sharp_{H} \sharp_{G} F &= \psh*{\Gamma^{1/2}G}{\nabla^{2} f \widehat{H}} + \frac{1}{2} \psh*{\Gamma^{-1/2}G}{diag(\nabla f) \widecheck{H}}
                          \\&=\sum_{i,j} \partial_{x_{i} x_{j}} f(X_{1},\dots, X_{n}) \Gamma_{i}^{1/2} G_{i} \paren*{B_{j}^{1/2}(1,1) H_{j}^{(1)} + B_{j}^{1/2}(1,2) H_{j}^{(2)}}
                          \\& + \frac{1}{2} \sum_{i} \partial_{x_{i}} f(X_{1}, \dots, X_{n}) \Gamma_{i}^{-1/2} G_{i} \paren*{B_{i}^{1/2}(2,1) H_{i}^{(1)} + B_{i}^{1/2}(2,2) H_{i}^{(2)}}.
  \end{split} 
\end{equation}

\subsubsection{Regularity of the random variable through the Bouleau derivative}\label{s:regularity-via-bouleau-derivative}

\begin{assumption}\label{ass:product-dirichlet}
  We work under the following assumptions.
  \begin{enumerate}[wide]
    \item There exists a exists a diffusive Dirichlet form $\mathcal{E}$ with carré du champ $\Gamma$ over a Fréchet space $E \subset \Omega$ equipped with its Borel $\sigma$-algebra $\mathfrak{B}(E)$.
    \item $X = (X_{i})$ is a sequence of independent random variables such that $X_{i} \in \mathbb{D}^{\infty}$ and the Dirichlet-independence condition \cref{ass:dirichlet-independence} holds.
    \item The $X_{i}$'s are centred and of variance $1$.
    \item $G = (G_{i})$ and $H = (H_{i})$ are sequences of independent standard Gaussian variables, with associated Gauss spaces $\mathbb{G}$ and $\mathbb{H}$.
    \item $\mathfrak{B}(E)$, $\sigma(G)$, and $\sigma(H)$ are independent.
\end{enumerate}
\end{assumption}

In this case, we recall that the simple derivative $\sharp_{G} \colon \mathbb{L}^{2}_{X} \to \mathbb{L}^{2}(\mathbb{G})$ and the iterated Bouleau derivative $\sharp_{H} \sharp_{G} \colon \mathbb{L}^{2}_{X} \to \mathbb{L}^{2}(\mathbb{G} \otimes \mathbb{H}^{\otimes 2})$ are defined in \cref{s:bouleau-derivative}.

Our general principle states that the regularity of the law of $\sharp_{G}F$ controls that of the law $F$.
\begin{theorem}\label{th:sobolev-regularity-derivative}
  Under \cref{ass:product-dirichlet}, let $F \in \mathbb{D}^{\infty}_{X}$.
  Then,
  \begin{equation}\label{eq:sobolev-regularity-derivative}
    \begin{split}
      \mathbf{W}_{q,1}(F) &\leq \Psi(\norm{F}_{\mathbb{D}^{\infty}})  \mathbf{N}_{8q+1,\infty}(\sharp_{G}F) ^{1/2}
                        \\& \leq \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \mathbf{W}_{8q+1,1}(\sharp_{G}F) ^{1/2}.
    \end{split}
  \end{equation}
\end{theorem}

\begin{proof}
Let $\xi > 0$ and $\lambda > 0$.
By Markov's inequality, we have that:
\begin{equation*}
  \Prob*{ \Gamma(F,F) < \frac{1}{\xi} } = \Prob*{ \Exp*{ - \frac{\lambda^{2}}{2} \Gamma(F,F) } > \mathrm{e}^{-\lambda^{2}/(2\xi) } } \leq \mathrm{e}^{\lambda^{2}/(2 \xi)} \Esp*{ \Exp*{- \frac{\lambda^{2}}{2} \Gamma(F,F) } }.
\end{equation*}
By \cref{th:sharp-properties}, $\sharp_{G} F$ is a conditionally Gaussian variable with conditional variance $\Gamma(F,F)$, and we have that
\begin{equation*}
  \Esp*{\Exp*{ - \frac{\lambda^{2}}{2} \Gamma(F,F)} } = \Esp*{ \Exp*{ \mathrm{i} \lambda \sharp_{G} F} }.
\end{equation*}
Thus, taking $\lambda = \sqrt{2\xi}$ we have that
\begin{equation*}
  \Prob*{ \Gamma(F,F) < \frac{1}{\xi} } \leq \mathrm{e} \Esp*{ \Exp*{ \mathrm{i} (2\xi)^{1/2} \sharp_{G} F } }.
\end{equation*}
It follows that
\begin{equation}\label{eq:negative-moment-gamma-fourier}
  \begin{split}
    \Esp*{ \Gamma(F,F)^{-q} } &= q \int_{0}^{\infty} \xi^{q-1} \Prob*{ \Gamma(F,F) < \frac{1}{\xi} } \mathrm{d}\xi 
                            \\&\leq \frac{\mathrm{e}q}{2}  \int_{0}^{\infty} \xi^{2q-1} \Esp*{ \Exp*{ \mathrm{i} \xi \sharp_{G}F}  } \mathrm{d} \xi 
                            \\&\leq \frac{\mathrm{e}q}{2} \paren*{\int_{0}^{\infty} \frac{\xi^{2q-1}}{(1+\xi^{2})^{q+1/2}} \mathrm{d} \xi} \mathbf{N}_{2q +1, \infty}(\sharp_{G}F) .
  \end{split}
\end{equation}
By \cref{th:sobolev-gamma-l-minus-p},
\begin{equation*}
  \mathbf{W}_{q,1}(F) \leq \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \Esp*{ \Gamma(F,F)^{-4q} }^{1/2}.
\end{equation*}
From which, together with \cref{th:sobolev-gamma-l-minus-p}, we conclude the first inequality in \cref{eq:sobolev-regularity-derivative}.
The second inequality follows by \cref{eq:embedding-fourier-sobolev}.
\end{proof}

We can iterate \cref{th:sobolev-regularity-derivative}.
\begin{theorem}\label{th:sobolev-regularity-iterated-gradient}
  Under \cref{ass:product-dirichlet}, let $F \in \mathbb{D}^{\infty}_{X}$.
  We have that
  \begin{equation*}
    \mathbf{W}_{q,1}(F) \leq \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \Esp*{ \mathbf{N}_{64q+9, \infty}(\sharp_{H}\sharp_{G}F) }^{1/4}.
  \end{equation*}
\end{theorem}

\begin{proof}
  We would like to apply \cref{th:sobolev-regularity-derivative} to $\sharp_{G}F$.
  However, $\sharp_{G}F$ is \emph{not} in $\mathbb{D}^{\infty}_{X}$, since it is not measurable with respect to $X$.
  To circumvent this issue we consider the usual Dirichlet form associated to $G$, that is the so-called Wiener--Dirichlet form $\mathcal{E}_{W}$ \cite[Chap.\ III]{BouleauHirsch}.
  This form is diffusive and admits a carré du champ $\Gamma_{W}$.
  We can consider the diffusive product Dirichlet form $\widetilde{\mathcal{E}} \coloneq \mathcal{E} \otimes \mathcal{E}_{W}$.
  Then, $\sharp_{G} F \in \widetilde{\mathbb{D}}^{\infty}$, and
  \begin{equation*}
    \widetilde{\Gamma}(\sharp_{G}F, \sharp_{G}F) = \Gamma(\sharp_{G}F, \sharp_{G}F) + \Gamma_{W}(\sharp_{G}F, \sharp_{G}F) \geq \Gamma(\sharp_{G}F, \sharp_{G}F).
  \end{equation*}
  Hence, to obtain negative moments for $\widetilde{\Gamma}(\sharp_{G}F,\sharp_{G}F)$ and thus regularity for $\sharp_{G}F$, is is sufficient to obtain negative moments for $\Gamma(\sharp_{G}F,\sharp_{G}F)$.
  This is done exactly as in the proof of \cref{th:sobolev-regularity-derivative}.
  Details are left to the reader.
\end{proof}

\begin{remark}
  The choice of the Wiener structure in the above proof is irrelevant.
  \emph{Any} diffusive Dirichlet forms with carré to champ associated to $G$ could have worked, including the trivial Dirichlet form $\mathcal{E}_{0} \coloneq 0$.
\end{remark}

\subsubsection{Estimating the regularity of the iterated gradient by Gaussian analysis}\label{s:iterated-gradient-square-gaussian}

The random variable $\sharp_{G} \sharp_{H} F$ is conditionally a Gaussian quadratic form, that is a Gaussian quadratic form with random independent coefficients.
More precisely there exists a $\mathfrak{B}(E)$-measurable linear operator $\mathsf{D}^{2}F$ such that
\begin{equation*}
  \sharp_{H} \sharp_{G} F = \psh*{G}{\paren*{\mathsf{D}^{2}F} H^{\otimes 2}}.
\end{equation*}
We refer to $\mathsf{D}^{2}F$ as the \emph{Bouleau Hessian} of $F$.
We stress that the explicit formula for $\mathsf{D}^{2}F$ is involved but can be inferred from \cref{eq:iterated-sharp-cylinder}; we do not write formula as we will see that we can work with a simpler matrix.
In order to control its regularity, let us start with an estimation of the regularity of the law Gaussian quadratic forms, that is with deterministic coefficients.

\begin{lemma}\label{th:regularity-gaussian-quadratic-form}
  Let $N = (N_{i})$ be a sequence of independent standard Gaussian variables and $\mathsf{A}$ a symmetric Hilbert--Schmidt operator.
  Then,
  \begin{equation*}
    \mathbf{N}_{q/2,\infty}(\psh{N}{\mathsf{A} N}) \lesssim \mathcal{R}_{q}(\mathsf{A})^{-1/4}.
  \end{equation*}
\end{lemma}
\begin{proof}
  Write $F \coloneq \psh{N}{\mathsf{A}N}$.
  By diagonalization we have $\mathsf{A} = \mathsf{P}^{T} \Lambda \mathsf{P}$ where $\Lambda$ is the diagonal operator of real eigenvalues of $\mathsf{A}$ and $\mathsf{P}$ is an orthogonal operator.
  By the invariance of Gaussian measures under isometries, we find that
  \begin{equation*}
    F = \psh{\mathsf{P} N}{\Lambda \mathsf{P}N} \overset{law}{=} \psh{N}{\Lambda N}.
  \end{equation*}
  Recalling that
  \begin{equation*}
    \Esp*{ \mathrm{e}^{\mathrm{i} \xi N_{1}^{2} } } = \paren*{1-2\mathrm{i}\xi}^{-1/2},
  \end{equation*}
  we get that
  \begin{equation*}
    \abs*{ \Esp*{ \mathrm{e}^{\mathrm{i} \xi F} } } = \prod_{k} \abs*{ 1 -2 \mathrm{i}\xi \lambda_{k} }^{-1/2} = \prod_{k} (1 + 4 \xi^{2} \lambda_{k}^{2})^{-1/4}.
  \end{equation*}
  Developing the product yields
  \begin{equation*}
    \prod_{k} \paren*{ 1 + 4\xi^{2} \lambda_{k}^{2} } = 1 + \sum_{p} 4^{p} \xi^{2p} \mathcal{R}_{p}(\mathsf{A}).
  \end{equation*}
  Finally, we have that
  \begin{equation*}
    \abs*{ \Esp*{ \mathrm{e}^{\mathrm{i} \xi F} } } \lesssim (1+\xi)^{q/2} \mathcal{R}_{q}(\mathsf{A})^{-1/4},
  \end{equation*}
  from which the claim follows.
\end{proof}

We now derive the main abstract result of this paper.
Recall that we write $\Gamma_{i} \coloneq \Gamma(X_{i}, X_{i})$ and $\Gamma \coloneq \mathrm{diag}(\Gamma_{1}, \Gamma_{2}, \dots)$.
\begin{theorem}\label{th:regularity-negative-moments-spectral-remainder}
  Under \cref{ass:product-dirichlet}, let $F \in \mathbb{D}^{\infty}_{X}$.
  Then,
  \begin{equation}\label{eq:regularity-spectral-remainders-bouleau-hessian}
    \mathbf{W}_{q,1}(F) \leq \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \Esp*{ \mathcal{R}_{128q+18}\paren*{\mathsf{D}^{2}F}^{-1/4} }^{1/4}.
  \end{equation}
  Moreover, when $F = f(X)$, for some $f \colon \mathbb{R}^{\infty} \to \mathbb{R}$ such that the Hessian $\nabla^{2}f(X)$ exists, we have
  \begin{equation}\label{eq:regularity-spectral-remainders-simplified}
    \mathbf{W}_{q,1}(F) \leq \Psi(\norm{F}_{\mathbb{D}^{\infty}}) \Esp*{ \mathcal{R}_{128q+18}\paren*{\Gamma^{1/2} \paren*{ \nabla^{2} f(X) } \Gamma^{1/2} }^{-1/4} }^{1/4}.
  \end{equation}
\end{theorem}

\begin{proof}
  \cref{eq:regularity-spectral-remainders-bouleau-hessian} follows from \cref{th:sobolev-regularity-iterated-gradient,th:regularity-gaussian-quadratic-form}.

  To derive \cref{eq:regularity-spectral-remainders-simplified}, consider $F = f(X)$ for $f$ sufficiently smooth.
  Following \cref{eq:iterated-sharp-cylinder}, consider three independent standard Gaussian vectors $G$, $H^{(1)}$ and $H^{(2)}$, as well as the (non-independent) Gaussian vectors
\begin{align*}
  & \widehat{H}_{i} \coloneq B_{i}(1,1)^{1/2} H_{i}^{(1)} + B_{i}(1,2)^{1/2} H_{i}^{(2)} \sim \mathscr{N}(0, \Gamma_{i})
\\&\widecheck{H}_{i} \coloneq B_{i}(1,2)^{1/2} H_{i}^{(1)} + B_{i}(2,2)^{1/2} H_{i}^{(2)} \sim \mathscr{N}(0, \Gamma(\Gamma_{i}, \Gamma_{i})).
\end{align*}
We then have that
\begin{equation}\label{eq:iterated-sharp-explicit}
  \sharp_{H} \sharp_{G} F = \psh{\Gamma^{1/2} G}{\nabla^{2}_{ul}f(X) \widehat{H}} + \psh{\Gamma^{1/2} G}{\nabla^{2}_{d}f(X) \widehat{H}} + \frac{1}{2} \psh{\Gamma^{-1/2}G}{\overline{\nabla f(X)} \widecheck{H}},
\end{equation}
where we consider the two random diagonal operators
\begin{align*}
  & \nabla^{2}_{d}f(X) \coloneq diag(\partial^{2}_{x_{1}x_{1}}f(X), \partial^{2}_{x_{2}x_{2}}f(X), \dots);
\\&\overline{\nabla f(X)} \coloneq diag(\partial_{x_{1}}f(X), \partial_{x_{2}}f(X), \dots);
\end{align*}
as well as the upper-lower part of the Hessian
\begin{equation*}
  \nabla^{2}_{ul}f(X) = \nabla^{2}f(X) - \nabla^{2}_{d}f(X).
\end{equation*}

By construction $\nabla^{2}_{d}f(X) + \overline{\nabla f(X)}$ are diagonal operator, while $\nabla^{2}_{ul}f(X)$ is purely non-diagonal.
By \cref{th:additivity-spectral-remainders}, it is sufficient to estimate the spectral remainder of the operator appearing in the first term of the right-hand side of \cref{eq:iterated-sharp-explicit}.
Let us consider the operators
\begin{align*}
  & \mathsf{M} \coloneq \Gamma^{1/2} \nabla^{2}_{ul}f(X) \Gamma^{1/2},
\\& \mathsf{M}^{\oslash 2} \coloneq
\paren*{%
  \begin{array}{c|c}
    0 & \mathsf{M} \\
    \hline
    \mathsf{M} & 0
  \end{array}
}
\end{align*}


By construction, we have that
\begin{equation*}
  \psh{\Gamma^{1/2} G}{\nabla^{2}_{ul}f(X) \widehat{H}} \overset{law}{=} \psh{G}{\mathsf{M}H} \overset{law}{=} \psh*{\begin{pmatrix}G\\H\end{pmatrix}}{\mathsf{M}^{\oslash 2} \begin{pmatrix}G\\H\end{pmatrix}},
\end{equation*}
where $H$ is a sequences of independent standard Gaussian variables independent of $G$.
Thus, it is sufficient to control the spectral remainder $\mathcal{R}_{q}(\mathsf{M}^{\oslash 2}) \geq \mathcal{R}_{q}(\mathsf{M})$.
\end{proof}

\subsection{Regularity of laws of random quadratic forms}\label{s:regularity-quadratic-form}

In this section, we use our general theorem in order to derive sufficient condition for the regularity of random quadratic forms.
More precisely, we let $X$ satisfy \cref{ass:product-dirichlet} and $\mathsf{A} = (a_{ij})$ be a symmetric Hilbert--Schmidt operator on $\ell^{2}(\mathbb{N})$, and we consider
\begin{equation}\label{eq:q-quadratic}
  Q_{\mathsf{A}} \coloneq \psh{\mathsf{A}X}{X} = \sum_{i,j} a_{ij} X_{i} X_{j}.
\end{equation}

In order to obtain regularity on the law of $Q_{\mathsf{A}}$ we need to assume some regularity at the level of $X_{1}$.


\begin{assumption}\label{ass:small-ball-gamma}
There exists $\theta > 0$ such that
\begin{equation}\label{eq:small-ball-theta}
  \Prob*{\Gamma(X_{1}, X_{1}) \leq \varepsilon} \lesssim \varepsilon^{\theta}, \qquad \varepsilon > 0.
\end{equation}
\end{assumption}

\begin{remark}
  \cref{ass:small-ball-gamma} is equivalent to $\Esp*{ \Gamma(X_{1}, X_{1})^{-p_{0}} } < \infty$ for some $p_{0} >0$.
  In view of \cref{s:regularity-positivity-gamma} this implies that $X_{1}$ has a density in some Sobolev space $\mathscr{W}^{s_{0}, 1}$ for some $s_{0} > 0$.
  Note that we possibly have that $s_{0} < 1$.
\end{remark}

Our main result shows that the law of $Q_{\mathsf{A}}$ is regular provided the influence of $\mathsf{A}$ is small enough.
\begin{theorem}\label{th:regularity-quadratic-form}
  Under \cref{ass:product-dirichlet,ass:small-ball-gamma}.
  Let $q \in \mathbb{N}$.
  There exists $\tau_{q} > 0$ such that for all $\mathsf{A}$ with
  \begin{align}
    & \label{ass:normalized-A} \tr \mathsf{A}^{2} = 1,
  \\& \label{ass:positivity-spectral-remainder-A}\mathcal{R}_{q}(\mathsf{A}) > 0,
  \\& \label{ass:small-influence-A} \tau(\mathsf{A}) < \tau_{q}.
  \end{align}
the law of $Q_{\mathsf{A}}$ is in $\mathscr{W}^{q,1}$, and in particular, it in $\mathscr{C}^{\lfloor q-1 \rfloor}$.
Moreover, for some $\eta_{q} > 1/4$,
\begin{equation*}
  \mathbf{W}_{q,1}(Q_{\mathsf{A}}) \lesssim \frac{1}{\mathcal{R}_{128q+18}(\mathsf{A})^{\eta_{q}} }.
\end{equation*}
\end{theorem}

\begin{remark}\label{rk:explicit-tau-q}
  From our proof we see that we can take
  \begin{equation*}
    \tau_{q} \coloneq \paren*{ \theta \wedge (2048 q + 288) }^{-5} 2^{-1280q-191}.
  \end{equation*}
We did not try to optimize the constant $\tau_{q}$.
\end{remark}

\subsubsection{A general small ball estimate for multi-linear polynomials}

As could be anticipated from the previous sections our analysis is concerned with the positivity of some random quadratic forms.
We thus start with some auxiliary results of independent interest.
A \emph{multi-linear polynomial} is a function
\begin{equation*}
p(x) \coloneq \sum_{I \subset \mathbb{N}} a_{I} x_{I}, \qquad x \in \mathbb{R}^{\infty},
\end{equation*}
where $x_{I} \coloneq \prod_{i \in I} x_{i}$, and $(a_{I} : I \in \mathbb{N})$ are real coefficients such that $a_{I} = 0$ for all $I$ with $\abs{I} > d$ for some $d \in \mathbb{N}$.
We call the \emph{degree} of the polynomial the largest $d$ such that $a_{I} \ne 0$ for some $\abs{I} = d$.
\begin{proposition}\label{th:small-ball-l2}
  Let $(U_{i})$ be a sequence of independent and identically distributed random variables with finite third moment, and such that there exists $\theta > 0$ with
  \begin{equation}\label{eq:small-ball-U}
    \sup_{b \in \mathbb{R}} \Prob*{ \abs{U_{1} - b} \leq \varepsilon } \lesssim \paren*{\frac{\varepsilon}{\Var*{U_{1}}^{1/2}}}^{\theta}.
  \end{equation}
  For all $d \in \mathbb{N}$, let
  \begin{equation}\label{eq:def-theta}
    \theta_{d} \coloneq (\theta \wedge \frac{1}{8d}) 2^{-d}.
  \end{equation}
  Then, for all $p$ be a multi-linear polynomial of degree $d$.
  \begin{equation}\label{eq:small-ball-P-l2}
    \sup_{b \in \mathbb{R}} \Prob*{ \abs{p(U) - b} \leq \varepsilon } \lesssim \paren*{\frac{\varepsilon}{\Var*{p(U)}^{1/2}}}^{\theta_{d}}.
  \end{equation}
\end{proposition}

\begin{remark}
  We did not try to optimize the constant $\theta_{d}$.
  Finding the optimal exponent would be an interesting problem, not only in connexion with this work but also regarding generalization of the inequality of \citeauthor{CarberyWright} \cite{CarberyWright}.
\end{remark}

\begin{proof}
  By homogeneity, we assume that $U_{1}$ is centered and with variance $1$.
  By induction on $d$.
  For $d = 1$, the claim is simply \cref{eq:small-ball-U}.
  Let us thus fix $d > 1$, and assume that \cref{eq:small-ball-P-l2} holds for multi-linear polynomials of degree $d$ with some $\theta_{d}$.
  Let $p$ be a multi-linear polynomial of degree $d+1$.
  \paragraph{Large influence estimate}
  Let us write
  \begin{align*}
    & S_{i} \coloneq \sum_{I \ni i} a_{I} U_{I \setminus \{i\}},
  \\& R_{i} \coloneq \sum_{I \not\ni i} a_{I}U_{I}.
  \end{align*}
  In this way, we have that $p(U) = U_{i} S_{i} + R_{i}$, and $S_{i}$ and $R_{i}$ are independent of $U_{i}$.
  By \cref{eq:small-ball-U}, it follows that
  \begin{equation*}
    \begin{split}
      \Prob*{ \abs{p(U) - a} \leq \varepsilon } &= \Prob*{ \abs*{ U_{i} S_{i} + R_{i} - a} \leq \varepsilon,\, S_{i} > \eta } + \Prob*{ \abs*{ p(U)- a} \leq \varepsilon,\, S_{i} \leq \eta } 
                             \\&\lesssim \paren*{\frac{\varepsilon}{\eta}}^{\theta} + \Prob*{S_{i} \leq \eta}.
    \end{split}
  \end{equation*}
  Since $S_{i}$ is a multi-linear polynomial of degree $d$, by the induction hypothesis,
  \begin{equation}\label{eq:large-influence-bound}
    \Prob*{ \abs*{ p(U) -a } \leq \varepsilon } \lesssim \paren*{\frac{\varepsilon}{\eta}}^{\theta} + \paren*{\frac{\eta}{\norm{S_{i}}_{L^{2}}}}^{\theta_{d}}.
  \end{equation}
  \paragraph{Small influence estimate}
  The above bound is useless, whenever $\tau \coloneq \sup_{i} \Esp{ S_{i}^{2} }$ is small.
  In that case, we use a celebrated invariance principle for polynomials by \citeauthor{MDOInvariance}.
  Consider $G = (G_{i})$ a vector of independent standard Gaussian variables, then according to \cite[Thm.\ 2.1]{MDOInvariance},
  \begin{equation*}
    \Prob*{ \abs*{ p(U) - a } \leq \varepsilon } \lesssim d \tau^{1/8(d+1)} + \Prob*{ \abs*{p(G) - a} \leq \varepsilon }.
\end{equation*}
By a famous inequality of \citeauthor{CarberyWright} \cite[Cor.\ of Thm.\ 2]{CarberyWright}, we find that
\begin{equation*}
  \Prob*{ \abs*{ p(G) - a} \leq \varepsilon } \leq \paren*{\frac{\varepsilon}{\norm{p(G) - a}_{L^{2}}}}^{1/(d+1)}.
\end{equation*}
By assumption, we have that $\Var*{p(G)} = 1$.
Altogether this shows that
\begin{equation}\label{eq:small-influence-bound}
  \Prob*{ \abs*{p(U) - a} \leq \varepsilon } \lesssim \tau^{1/8(d+1)} + \varepsilon^{1/(d+1)}.
\end{equation}
\paragraph{Combining the estimates}
Whenever $\tau \lesssim \varepsilon$, we choose \cref{eq:small-influence-bound} yielding
\begin{equation*}
  \Prob*{ \abs*{ p(U) -a } \leq \varepsilon } \leq \varepsilon^{1/8(d+1)} + \varepsilon^{1/(d+1)}.
\end{equation*}
On the other hand, when $\tau \gtrsim \varepsilon$, we choose \cref{eq:large-influence-bound} with $\eta = \varepsilon^{1/2}$, and this gives
\begin{equation*}
  \Prob*{ \abs*{p(U) - a} \leq \varepsilon } \lesssim \varepsilon^{\theta/2} + \varepsilon^{\theta_{d}/2}
\end{equation*}
Thus we have that
\begin{equation*}
  \theta_{d+1} = \frac{1}{8(d+1)} \wedge \frac{\theta_{d}}{2}, \qquad \text{and} \qquad \theta_{1} = \theta.
\end{equation*}
This shows that $\theta_{d} = (\frac{1}{8d} \wedge \theta)/2^{d}$.
\end{proof}

By using the comparison between $L^{1}$ and $L^{2}$, we immediately get the following corollary.
\begin{corollary}\label{th:small-ball-P-l1}
  Under the same assumptions as in \cref{th:small-ball-l2}, and assume moreover that $U_{1} \geq 0$.
  Then,
  \begin{equation}\label{eq:small-ball-P-l1}
    \sup_{a \in \mathbb{R}} \Prob*{ \abs{p(U) - a} \leq \varepsilon } \lesssim \paren*{\frac{\varepsilon}{\Esp*{ p(U)}}}^{\theta_{d}}.
  \end{equation}
\end{corollary}

\subsubsection{Improving positivity by splitting independent terms}

Fix $q \in \mathbb{N}$.
We consider the set of subsets of $\mathbb{N}$ with \emph{exactly} $q$ elements, that is
\begin{equation*}
  \mathscr{P}_{q}(\mathbb{N}) \coloneq \set*{ I \subset \mathbb{N} : \abs{I} = q }.
\end{equation*}
We consider:
\begin{itemize}
  \item a symmetric trace-class operator
    \begin{equation*}
      \mathsf{B} \coloneq \paren*{ b_{IJ} : I,\, J \in \mathscr{P}_{q}(\mathbb{N})};
    \end{equation*}
  \item a sequence of non-negative random variables $(U_{i} : i \in \mathbb{N})$, and
    \begin{equation*}
      U_{I} \coloneq \prod_{i \in I} U_{i}, \qquad I \in \mathscr{P}_{q}(\mathbb{N});
    \end{equation*}
  \item and the non-negative random variable:
    \begin{equation*}
      S \coloneq \sum_{I,J \in \mathscr{P}_{q}(\mathbb{N})} b_{IJ} U_{I} U_{J}.
    \end{equation*}
\end{itemize}

Recall that we have defined the $\ell^{1}$-influence $\upsilon(\mathsf{B})$ and the total mass $\sigma(\mathsf{B})$ in \cref{s:linear-algebra-ell-1}.

\begin{remark}
  The random variable $S$ is non-negative since all the terms are non-negative.
  However, it is \emph{not} non-negative as a quadratic form in $U_{I}$ since the operator $\mathsf{B}$ may fail to be non-negative.
\end{remark}

\begin{proposition}\label{th:small-ball-improved}
  Assume that $\Esp*{ U_{1}^{2} } < \infty$, and that there exists $\theta > 0$ such that
  \begin{equation}\label{ass:mini-small-ball-U}
    \Prob*{ U_{1} \leq \varepsilon } \lesssim \varepsilon^{\theta}, \qquad \varepsilon > 0.
  \end{equation}
  Let $\kappa \in \mathbb{N}$ such that
  \begin{equation*}
    \upsilon(\mathsf{B}) \leq \frac{1}{2 \times 32^{\kappa}}.
  \end{equation*}
  Then, with $\theta_{2q}$ defined in \cref{eq:def-theta},
  \begin{equation*}
    \Prob*{ S \leq \varepsilon} \lesssim \paren*{\frac{\varepsilon 32^{\kappa}}{\sigma(\mathsf{B})}}^{\theta_{2q} 2^{\kappa}}, \qquad \varepsilon > 0.
  \end{equation*}
\end{proposition}

%
%
%
%
%
%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

Consider disjoint subsets $\mathbb{L}_{l} \subset \mathscr{P}_{q}(\mathbb{N})$.
Let
\begin{equation*}
  S_{l} \coloneq \sum_{(I,J) \in \mathbb{L}_{l} \times \mathbb{L}_{l}} b_{IJ} U_{I} U_{J}.
\end{equation*}
Then, the $S_{l}$'s are mutually independent.
Since, each of the $S_{l}$ is a multi-linear polynomial of degree $2q$, provided we can apply \cref{eq:small-ball-P-l1} on each of the $S_{l}$, this allows to improve our bound.
We now show this procedure is indeed feasible.

\paragraph{Splitting the mass via a probabilistic method}
Given $\mathbb{L} \subset \mathscr{P}_{p}(\mathbb{N})$, we write $\mathsf{B}^{\mathbb{L}} = (b_{IJ} : I,\, J \in \mathbb{L})$ for the extracted operator.
\begin{lemma}\label{th:mass-splitting}
  There exists $\mathbb{L} \subset \mathscr{P}_{p}(\mathbb{N})$ such that
  \begin{align*}\label{eq:total-mass-after-splitting}
    &\sigma\paren*{\mathsf{B}^{\mathbb{L}}} \geq \frac{1}{16} \sigma(\mathsf{B}) - \frac{1}{16} \upsilon(\mathsf{B});
  \\&\sigma\paren*{\mathsf{B}^{\mathbb{L}^{C}}} \geq \frac{1}{16} \sigma(\mathsf{B}) - \frac{1}{16} \upsilon(\mathsf{B}).
  \end{align*}
\end{lemma}

\begin{proof}
  Since $\sigma$ and $\upsilon$ are linear, by homogeneity, we assume that $\sigma(\mathsf{B}) = 1$.
  Let us consider a family of independent Bernoulli variables $(\varepsilon_{I} : I \in \mathscr{P}_{q}(\mathbb{N}))$ with mean $1/2$.
  We define the random set and random variables
  \begin{align*}
    & \mathbb{L} \coloneq \set*{ I \in \mathscr{P}_{p}(\mathbb{N}) : \varepsilon_{I} = 1 };
  \\& T \coloneq \sum_{(I,J) \in \mathbb{L} \times \mathbb{L}} b_{IJ} = \sum_{IJ} \varepsilon_{I} \varepsilon_{J} b_{IJ}.
  \\& \hat{T} \coloneq \sum_{(I,J) \in \mathbb{L}^{C} \times \mathbb{L}^{C}} b_{IJ} = \sum_{IJ} (1-\varepsilon_{I}) (1-\varepsilon_{J}) b_{IJ}.
  \end{align*}
  For $I,J,I',J' \in \mathscr{P}_{p}(\mathbb{N})$, we have that $\varepsilon_{I} \varepsilon_{J} (1-\varepsilon_{I'})(1-\varepsilon_{J'}) = 0$ as soon as $\set{I,J} \cap \set{I', J'} = \emptyset$.
  This later condition is implied by $(I \cup J) \cap (I' \cup J') = \emptyset$.
  Developing the product we thus find
  \begin{equation*}
    \Esp*{T \hat{T}} \geq \frac{1}{16} \sum_{(I \cup J) \cap (I' \cap J') = \emptyset}  b_{IJ} b_{I'J'} = \frac{1}{16} (\sigma(\mathsf{B}))^{2} - \frac{1}{16} \sum_{(I \cup J) \cap (I' \cap J') \ne \emptyset} b_{IJ} b_{I'J'}.
  \end{equation*}
  Now we compute
  \begin{equation*}
    \sum_{(I \cup J) \cap (I' \cap J') \ne \emptyset} b_{IJ} b_{I'J'} = \sum_{i \in \mathbb{N}} \paren*{\sum_{I,J} 1_{i \in I \cup J} b_{IJ}}^{2} = \sum_{i \in \mathbb{N}} \upsilon_{i}(\mathsf{B})^{2} \leq \upsilon(\mathsf{B}) \sum_{i \in \mathbb{N}} \upsilon_{i}(\mathsf{B}) = \upsilon(\mathsf{B}) \sigma(\mathsf{B}).
  \end{equation*}
  Since $\sigma(\mathsf{B}) = 1$, we have shown
  \begin{equation*}
    \Esp*{ T \hat{T}} \geq \frac{1}{16} (1 - \upsilon(\mathsf{B})).
  \end{equation*}
  Owing to the fact that $T \vee \hat{T} \leq 1$, we find
  \begin{equation*}
    \Esp*{ T \wedge \hat{T} } \geq \Esp*{T \hat{T}} \geq \frac{1}{16} (1 - \upsilon(\mathsf{B})).
  \end{equation*}
  This shows that there exists a realization of $\mathbb{L}(\omega)$ fulfilling the requirements of the lemma.
  %
  %
  %
  %
  %
  %

  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
\end{proof}

\paragraph{Extraction of independent sums}

We now have all the necessary tools to conclude.
\begin{proof}[{Proof of \cref{th:small-ball-improved}}]
Let us define for all $k \in \mathbb{N}$ the pairwise disjoint sets $\mathbb{L}^{(k)}_{1}, \dots, \mathbb{L}^{(k)}_{2^{k}} \subset \mathscr{P}_{q}(\mathbb{N})$ using an iterative procedure described below.
To each of the $\mathbb{L}_{l}^{(k)}$, we associate the operators $\mathsf{B}^{(k)}_{l} \coloneq \mathsf{B}^{\mathbb{L}^{(k)}_{l}}$, and the random sums
\begin{equation*}
  S_{l}^{(k)} \coloneq \sum_{(I,J) \in \mathbb{L}^{(k)}_{l} \times \mathbb{L}^{(k)}_{l}} b_{IJ} U_{I} U_{J}.
\end{equation*}
Since the $U_{i}$'s are independent and that, for a given $k$, the $\mathbb{L}^{(k)}_{l}$'s are pairwise disjoint, the $S_{l}^{(k)}$ are mutually independent.

We initialize $\mathbb{L}^{(0)}_{1} \coloneq \mathscr{P}_{q}(\mathbb{N})$.
Then, for all $k \in \mathbb{N}$, if there exists $l \in \set{1, \dots, 2^{k}}$ such that $\upsilon(\mathsf{B}) > \frac{1}{2} \sigma(\mathsf{B}^{(k)}_{l})$, we stop the procedure; otherwise define $\mathbb{L}^{(k+1)}_{l}$ and $\mathbb{L}^{(k+1)}_{l + 2^{k}}$ as the sets obtained by applying \cref{th:mass-splitting} to $\mathsf{B}^{(k)}_{l}$.
We write $k_{max}$ the integer at which the procedure stops.
Our procedure guarantees that
\begin{equation*}
  \sigma(\mathsf{B}^{(k)}_{l}) \geq \frac{\sigma(\mathsf{B})}{32^{k}}, \qquad k \leq k_{max}.
\end{equation*}
On the other hand at step $k$ the procedure continues provided
\begin{equation*}
  \upsilon(\mathsf{B}) \leq \frac{1}{2^{5k + 1}}.
\end{equation*}
This shows that $k_{max} \geq  \kappa$.
Using the mutual independence of the $S^{(k_{max})}_{l}$'s and the fact that the $B_{IJ}$'s and $U_{I}$'s are non-negative we get
\begin{equation*}
  \Prob*{ S \leq \varepsilon } \leq \prod_{l=1}^{2^{\kappa}} \Prob*{ S^{(\kappa)}_{l} \leq \varepsilon }.
\end{equation*}
We conclude by invoking \cref{th:small-ball-P-l1} to each of the terms of the product.
\end{proof}


\subsubsection{Control of the spectral remainders of \texorpdfstring{$\Gamma^{1/2} \mathsf{A} \Gamma^{1/2}$}{Γ1/2 A Γ1/2}}

\begin{proof}[{Proof of \cref{th:regularity-quadratic-form}}]
  In view of the Dirichlet independence, we find that
  \begin{equation*}
    \Gamma(\mathsf{L}^{n}X_{i}, \mathsf{L}^{m}X_{j}) = 0, \qquad i \ne j,\, n,m \in \mathbb{N}.
  \end{equation*}
  In particular, we get that
  \begin{equation*}
    \mathsf{L}^{k} Q = \sum_{l=1}^{k} \binom{k}{l} \psh{\mathsf{L}^{l} X}{\mathsf{A} \mathsf{L}^{k-l} X}.
  \end{equation*}
  Since $Q \in \mathbb{D}^{\infty}$ this implies that all the above random variables have all their moment finite, and
  \begin{equation*}
    \norm{\mathsf{L}^{k}Q}_{\mathbb{L}^{p}} \lesssim \norm{\mathsf{L}^{k}Q}_{\mathbb{L}^{2}} \lesssim \norm{\mathsf{A}} \Psi(\norm{X}_{\mathbb{D}^{\infty}}).
  \end{equation*}
  Finally, we have shown
  \begin{equation}\label{eq:bound-psi-Q}
    \Psi(\norm{Q}_{\mathbb{D}^{\infty}}) \lesssim \Psi(\norm{X}_{\mathbb{D}^{\infty}}).
  \end{equation}
  Let $q' = 128q+18$.
  In view of \cref{th:small-ball-improved,th:influence-det}, we find that
  \begin{equation}\label{eq:explicit-bound-spectral-remainder-hessian}
  \Prob*{ \mathcal{R}_{q'}(\Gamma^{1/2} \mathsf{A} \Gamma^{1/2}) \leq \varepsilon } \lesssim \paren*{ \frac{\varepsilon 32^{\kappa}}{\mathcal{R}_{q'}(\mathsf{A})} }^{\theta_{2q'} 2^{\kappa}}.
  \end{equation}
  Whenever $\eta_{q} \coloneq 2^{\kappa} \theta_{2q'} > 1/4$ and $\tau(\mathsf{A}) \leq 2^{-5\kappa -1}$, this shows that
  \begin{equation*}
    \Esp*{ \mathcal{R}_{q'}(\Gamma^{1/2} \mathsf{A} \Gamma^{1/2})^{-1/4}}^{1/4} \lesssim \mathcal{R}_{q'}(\mathsf{A})^{-\eta_{q}}.
  \end{equation*}
  In view of \cref{ass:positivity-spectral-remainder-A}, the above quantity is finite.
  We conclude by \cref{th:regularity-negative-moments-spectral-remainder,eq:bound-psi-Q,eq:explicit-bound-spectral-remainder-hessian}.
\end{proof}

\begin{remark}[Improved constants when $X_{1}$ is log-concave]\label{rk:improved-log-concave}
In the previous proof, we have use \cref{ass:small-ball-gamma} combined with a splitting argument in order to derive a small ball estimate for
\begin{equation*}
  S_{q} \coloneq \sum_{I,J \in \mathscr{P}_{q}(\mathbb{N})} b_{IJ} \Gamma_{I} \Gamma_{J}.
\end{equation*}
Since we rely on \cref{th:small-ball-l2}, we obtain the non-optimal exponent $\theta_{d}$.
However, in some cases we can expect a much better exponent to appear.
This is for instance the case as soon as $S_{q}$ is a multi-linear polynomial of degree $d_{q}$ in the $X_{i}$ and $X_{1}$ as a log-concave law.
In this case by an inequality of \citeauthor{CarberyWright} \cite[Cor.\ of Thm.\ 2]{CarberyWright}, we find that
\begin{equation*}
  \Prob*{  S_{q} \leq \varepsilon } \lesssim \paren*{ \frac{\varepsilon}{ \Esp{S_{q}}} }^{\frac{1}{d_{q}}}.
\end{equation*}
Redoing the same proof with $\theta_{d}$ replaced by $1/d$ sows that in this case, we could take a better $\tau_{q}$ than the one given in \cref{rk:explicit-tau-q}.
\end{remark}

\section{Examples and applications}

\subsection{Examples of laws satisfying our assumptions}\label{s:examples}
We now give concrete examples where \cref{th:regularity-negative-moments-spectral-remainder,th:regularity-quadratic-form} can be applied.

\subsubsection{Product Dirichlet structures}\label{s:product-dirichlet}
Recall from \cite[Chap.\ V, \S 2]{BouleauHirsch} that it is possible to consider infinite products of Dirichlet forms.
If the initial Dirichlet forms are local and admit a carré du champ then so does the product Dirichlet form \cite[Chap.\ V, Prop.\ 2.2.2]{BouleauHirsch}.
This ability to consider products provide us with a large class of examples.
More precisely, we consider a Dirichlet form $\mathcal{E}$ over a Fréchet space $E$ equipped with a measure $\mu$ such that there exists $X \in \dom \mathcal{E}$ and $X$ satisfies \cref{ass:small-ball-gamma}.
Then over the product space $(E, \mu)^{\mathbb{N}}$ we consider the the product Dirichlet structure, and the coordinate functions $X_{i} \colon (x_{l}) \mapsto x_{i}$.
Then $(X_{i})$ satisfies \cref{ass:product-dirichlet,ass:small-ball-gamma}.

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsubsection{Dirichlet forms associated with orthogonal polynomials}\label{s:orthogonal-polynomials}
We start with the only three examples of Dirichlet structure associated with orthogonal polynomials on $\mathbb{R}$ (see \cite[Part I,\, Chap.\ 2]{BGL} for more details).
\newcommand{\parexample}{\paragraph}


\parexample{Normal distribution and Hermite polynomials}\label[example]{ex:normal}
On $\mathbb{R}$, consider a standard Gaussian measure $\mu$ whose density with respect to the Lebesgue measure is proportional to $\mathrm{e}^{-x^{2}/2}$.
In this way, the identity map $X(x) \coloneq x$ is a random variable with Gaussian distribution.
Over $\mathbb{R}$, we consider the Dirichlet form
\begin{equation*}
  \mathcal{E}(u, u) \coloneq \int (u')^{2} \mathrm{d} \mu, \qquad u \in L^{2}(\mu),
\end{equation*}
with domain $\dom \mathcal{E} \coloneq W^{1,2}(\mu)$ the Sobolev space with respect to $\mu$.
In this case, we have that
\begin{equation*}
  \mathsf{L} u = u'' - X u',
\end{equation*}
with domain $\dom \mathsf{L} \coloneq W^{2,2}(\mu)$, and
\begin{equation*}
  \Gamma(u,v) = u' v'.
\end{equation*}
This shows that $\mathcal{E}$ is diffusive.
Since $\mathsf{L} x = - x$, we find $X \in \mathbb{D}^{\infty}$; and, since $\Gamma(X, X) = 1$, \cref{ass:small-ball-gamma} trivially holds.
On $(\mathbb{R}, \mu)^{\infty}$, we can consider the product Dirichlet structure where each of the marginal is equipped with the above Dirichlet form.
This structure is often called the Wiener space.
Whenever $X_{i} \colon \mathbb{R}^{\infty} \ni (x_{k}) \mapsto x_{i}$, the $X_{i}$'s are independent and identically distributed standard Gaussian, and the argumentation of \cref{s:product-dirichlet} shows that they satisfy \cref{ass:product-dirichlet,ass:small-ball-gamma}.

\paragraph{Beta distribution and Jacobi polynomials}\label[example]{ex:beta}
Over $[-1,+1] \subset \mathbb{R}$ consider $\mu$ the Beta distribution with parameter $\alpha > 0$ and $\beta > 0$, that is the law whose density with respect to the Lebesgue measure is proportional to $\paren*{1-x}^{\alpha-1} \paren*{1+x}^{\beta-1}$.
In this way, the identity map $X(x) \coloneq x$ is a Beta-distributed random variable.
Over $\mathbb{R}$, we consider the Dirichlet form
\begin{equation*}
  \mathcal{E}(u, u) \coloneq \int \paren{1-x^{2}} (u')^{2} \mathrm{d} \mu, \qquad u \in L^{2}(\mu),
\end{equation*}
with domain $\dom \mathcal{E}$ being a weighted Sobolev space with respect to $\mu$.
In this case, we have that
\begin{equation*}
  \mathsf{L} u = (1-X^{2}) u'' - \bracket*{ (\alpha + \beta) X + \alpha - \beta } u',
\end{equation*}
whose domain $\dom \mathsf{L}$ is a weighted Sobolev space of higher order, and
\begin{equation*}
  \Gamma(u,v) = (1-X^{2}) u' v'.
\end{equation*}
This shows that $\mathcal{E}$ is diffusive.
Since $\mathsf{L} X = - (\alpha+\beta) X + \alpha - \beta$, we find that $X \in \mathbb{D}^{\infty}$; and since, for $\theta > 0$,
\begin{equation*}
  \Esp*{ \Gamma(X,X)^{-\theta} } = \int_{-1}^{1} (1-x)^{\alpha - 1 - \theta} (1+x)^{\beta-1-\theta} \mathrm{d} x,
\end{equation*}
we see that \cref{ass:small-ball-gamma} holds for $\theta < \alpha \wedge \beta$.
On $(\mathbb{R}, \mu)^{\infty}$, we can consider the product Dirichlet structure where each of the marginal is equipped with the above Dirichlet form.
This structure is sometimes called the Laguerre space.
Whenever $X_{i} \colon \mathbb{R}^{\infty} \ni (x_{k}) \mapsto x_{i}$, the $X_{i}$'s are independent and identically distributed Beta random variables, and, following \cref{s:product-dirichlet}, they satisfy \cref{ass:product-dirichlet,ass:small-ball-gamma}.
Whenever $\alpha \geq 1$ and $\beta \geq 1$, the Beta distribution is log-concave and \cref{rk:improved-log-concave} applies.

\paragraph{Gamma distribution and Laguerre polynomials}\label[example]{ex:gamma}
On $\mathbb{R}_{+} \subset \mathbb{R}$ consider $\mu$ a Gamma distribution with parameter $\alpha > 0$, that is $\mu$ has density with respect to the Lebesgue measure proportional to $x^{\alpha-1} \mathrm{e}^{-x}$.
Thus, the identity map $X(x) \coloneq x$ is a Gamma-distributed random variable.
Over $\mathbb{R}$, we consider the Dirichlet form
\begin{equation*}
  \mathcal{E}(u,u) \coloneq \Esp*{ X (u')^{2} } = \int x (u')^{2} \mathrm{d} \mu, \qquad u \in L^{2}(\mu),
\end{equation*}
with domain $\dom \mathcal{E}$ being a weighted Sobolev space with respect to $\mu$.
In this case, we have that
\begin{equation*}
  \mathsf{L} u = X u'' - \bracket*{ \alpha - X} u',
\end{equation*}
whose domain $\dom \mathsf{L}$ is a weighted Sobolev space of higher order, and
\begin{equation*}
  \Gamma(u,v) = X u' v'.
\end{equation*}
This shows that $\mathcal{E}$ is diffusive.
Since $\mathsf{L} X = X$, we have that $X \in \mathbb{D}^{\infty}$; and since, for $\theta > 0$
\begin{equation*}
  \Esp*{ \Gamma(X,X)^{-\theta} } = \int_{\mathbb{R}_{+}} x^{\alpha-1-\theta} \mathrm{e}^{-x} \mathrm{d} x,
\end{equation*}
we see that \cref{ass:small-ball-gamma} holds for $\theta < \alpha$.
On $(\mathbb{R}, \mu)^{\infty}$, we can consider the product Dirichlet structure where each of the marginal is equipped with the above Dirichlet form.
This structure is sometimes called the Laguerre space.
Whenever $X_{i} \colon \mathbb{R}^{\infty} \ni (x_{k}) \mapsto x_{i}$, the $X_{i}$'s are independent and identically distributed Gamma random variables.
By \cref{s:product-dirichlet}, they satisfy \cref{ass:product-dirichlet,ass:small-ball-gamma}.
Whenever $\alpha \geq 1$, the Gamma distribution is log-concave and \cref{rk:improved-log-concave} applies.

\paragraph{Multi-linear chaos}

In the three above examples, the operator $\mathsf{L}$ has a purely discrete spectrum with an orthonormal basis of eigenfunctions given by the orthonormal polynomials, namely the Hermite polynomials for \cref{ex:normal}, the Jacobi polynomials for \cref{ex:beta}, and the Laguerre polynomials for \cref{ex:gamma}.
This fact allows us to consider multi-linear polynomials.
More precisely, consider a sequence $(\mu_{l})$ where each of the $\mu_{l}$ is in the class of the examples above.
Define
\begin{equation*}
  E_{i} \coloneq \bigotimes_{l \in \mathbb{N}} (\mathbb{R}, \mu_{l}),
\end{equation*}
equipped with the product Dirichlet structure.
Consider $Y_{i}^{(l)} \colon (x_{k}) \mapsto x_{l}$.
Hence, for all $i$, the sequence $(Y_{i}^{(l)})_{l}$ is composed of independent random variables whose (possibly non-identical) law are Gaussian, Beta, or Gamma.
Let $p$ be a multi-linear polynomial, and $X_{i} \coloneq p(Y_{i})$.
Since $\mathbb{L}_{i}$ has polynomial eigenfunctions, and the $Y_{i}^{(l)}$ have finite moments of every positive order, this shows that $X_{i} \in \mathbb{D}^{\infty}$.
Moreover, $\Gamma(X_{i}, X_{i})$ is also a multi-linear polynomial, thus by \cref{th:small-ball-l2}, and the previous results for Gaussian, Beta, or Gamma random variables, we find that \cref{ass:small-ball-gamma} holds.
Thus $(X_{i})$ satisfies \cref{ass:product-dirichlet,ass:small-ball-gamma}.

\subsubsection{Other examples} 

\paragraph{Dirichlet forms in canonical forms with respect to a smooth density}
Let $V \colon \mathbb{R} \to \mathbb{R}$ be smooth and such that, for all $k \in \mathbb{N}$, there exists $d > 0$ such that:
\begin{align}
  & \label{eq:super-quadratic-V}\liminf_{\abs{x} \to \infty} \frac{V(x)}{x^{2}} > 0;
\\& \label{eq:polynomial-decay-V-derivatives}\forall k \in \mathbb{N},\, \exists d > 0 : \limsup_{\abs{x} \to \infty} \frac{V^{(k)}(x)}{x^{d}} < \infty.
\end{align}
On $\mathbb{R}$, let $\mu$ be the probability measure whose density with respect to the Lebesgue is proportional to $\mathrm{e}^{-V}$, and consider
\begin{equation}\label{eq:canonical-dirichlet-form}
  \mathcal{E}(u, u) \coloneq \int (u')^{2} \mathrm{d} \mu.
\end{equation}
This Dirichlet form is sometimes referred to as the \emph{canonical Dirichlet form} associated to $\mu$.
By an easy instance of Hamzaa's theorem, this indeed defines a Dirichlet form, whose generator is given by
\begin{equation*}
  \mathsf{L} u \coloneq u'' - u' V',
\end{equation*}
and carré du champ is given by
\begin{equation*}
  \Gamma(u,u) \coloneq (u')^{2}.
\end{equation*}
Consider the identity map $X(x) \coloneq x$ which is a random variable distributed according to $\mu$.
We have that $\mathsf{L} X = - V'$, and more generally $\mathsf{L}^{k}X$ is a polynomial in $V$ and its derivatives up to order $k$.
In view, of our assumptions on $V$, we have that such a polynomial has finite moments of every positive order.
Thus, $X \in \mathbb{D}^{\infty}$.
Since $\Gamma(X,X) = 1$, \cref{ass:small-ball-gamma} trivially holds.
As before, considering on the product Dirichlet structure $X_{i} \colon (\mathbb{R}, \mu)^{\infty} \ni (x_{l}) \mapsto x_{i}$, we have that $(X_{i})$ satisfies \cref{ass:product-dirichlet,ass:small-ball-gamma}.
\begin{remark}
  The canonical Dirichlet form \cref{eq:canonical-dirichlet-form} can be defined for a very large class of measures $\mu$, namely the measures satisfying \emph{Hamzaa's condition}.
  In particular, they could be defined for Beta and Gamma distributions.
  This would yield different Dirichlet structures than the ones of \cref{s:orthogonal-polynomials} \cref{ex:beta,ex:gamma}.
  We stress out that in this case the identity map $X(x) \coloneq x$ is \emph{not} in $\mathbb{D}^{\infty}$ of the canonical Dirichlet form.
\end{remark}


\paragraph{Functions of a Gaussian}
In the setting of a normal distribution, consider a smooth function $\varphi$ such that:
\begin{align}
  &\label{eq:polynomial-decay-phi} \forall k \in \mathbb{N},\, \exists d > 0 : \sup_{x} \frac{\varphi^{(k)}(x)}{x^{d}} < \infty;
\\&\label{eq:negative-moment-phi} \exists \theta > 0 : \int_{\mathbb{R}} \varphi'(t)^{-2\theta} \mathrm{e}^{-t^{2}/2} \mathrm{d} s < \infty.
\end{align}
We consider $X(x) \coloneq \varphi(x)$.
Thus, $X$ is a random variable with the same distribution as $\varphi(N)$ where $N$ is a standard Gaussian.
In view, of the explicit expression for $\mathsf{L}$ and using the fact that the Gaussian has finite moments of any positive order, by \cref{eq:polynomial-decay-phi}, we find that $\mathsf{L} X \in \mathbb{D}^{\infty}$.
Moreover, since $\Gamma(X,X) = \varphi'(X)^{2}$, \cref{eq:negative-moment-phi} ensures that \cref{ass:small-ball-gamma} holds.
Thus we can use the product Dirichlet structure construction of \cref{s:product-dirichlet} in order to construct a sequence satisfying \cref{ass:product-dirichlet,ass:small-ball-gamma}.

\begin{example}
  Consider the cumulative distribution function of the standard Gaussian,
  \begin{equation*}
    \Phi(t) \coloneq \int_{-\infty}^{t} \mathrm{e}^{-s^{2}/2} \mathrm{d} s.
  \end{equation*}
  Then it is clear that \cref{eq:polynomial-decay-phi,eq:negative-moment-phi} hold.
  Since $\Phi(N)$ follows a uniform distribution when $N$ is a standard Gaussian, this argument allows us to obtain uniform distributions in a different way than the way presented for Beta distributions.
\end{example}

\subsubsection{Stability of our assumptions under probabilistic operations}
Actually our assumptions are stable under various natural probabilistic operations.
To formulate them let us say that $X \in \mathscr{A}$ whenever $X \in \mathbb{D}^{\infty}$ for some diffusive Dirichlet admitting a carré du champ, and \cref{eq:small-ball-theta} holds for some $\theta > 0$.
By \cref{s:product-dirichlet}, if $X \in \mathscr{A}$, there exists a sequence of independent and identically distributed random variables with same law as $X$ satisfying \cref{ass:product-dirichlet,ass:small-ball-gamma}.
According to our previous examples, $\mathscr{A}$ contains for instance all the Gaussian, Beta, and Gamma random variable; as well as all the multi-linear polynomials thereof.

\paragraph{Stability under convolution}
\begin{lemma}
  Let $X \in \mathscr{A}$ and $Y$ a random variable independent of $X$.
  Then $X + Y \in \mathscr{A}$.
\end{lemma}

\begin{proof}
  Let $\mathcal{E}$ be a Dirichlet form over $(E,\mu)$ with carré du champ $\Gamma$ such that $X \in \mathscr{A}$, and assume $Y$ is defined on the probability space $(F, \mathfrak{F})$, with $\mathfrak{F}$ independent of $\mathfrak{B}(E)$.
  On $F$ we consider the trivial Dirichlet form $\mathcal{E}_{F} \coloneq 0$, that is clearly diffusive and admits for carré du champ $\Gamma_{F} \coloneq 0$.
  It is also immediate that $\mathbb{D}^{\infty}_{F} = L^{2}(F,\mathfrak{F})$.
  Consider $Z \colon E \times F \ni (x,y) \mapsto X(x) + Y(y)$, where $E \times F$ is equipped with the product Dirichlet structure.
  Then $Z$ has the same law as $X + Y$.
  By definition of product Dirichlet structures, it is immediate that $Z \in \mathbb{D}^{\infty}$.
  Since
  \begin{equation*}
    \Gamma_{E \times F}(Z,Z) = \Gamma_{E}(X,X) + \Gamma_{F}(Y,Y) = \Gamma_{E}(X,X),
  \end{equation*}
  we find that $Z \in \mathscr{A}$.
\end{proof}

\paragraph{Stability under products}
\begin{lemma}
  Let $X \in \mathscr{A}$ and $Y$ a random variable independent of $X$, such that $Y$ has a finite moment for some negative order.
  Then $XY \in \mathscr{A}$.
\end{lemma}

\begin{proof}
  As in the previous proof, we consider $\mathcal{E}$ a Dirichlet form over $(E,\mu)$ such that $X \in \mathscr{A}$, and the trivial Dirichlet form $\mathcal{E}_{F} \coloneq 0$.
  Consider $Z \colon E \times F \ni (x,y) \mapsto X(x) \cdot Y(y)$, where $E \times F$ is equipped with the product Dirichlet structure.
  Then $Z$ has the same law as $XY$.
  The fact that $Z \in \mathbb{D}^{\infty}$ follows immediately from the our construction.
  Moreover,
  \begin{equation*}
    \Gamma_{E \times F}(Z,Z) = Y^{2} \Gamma_{E}(X,X) + X^{2} \Gamma_{F}(Y,Y) = Y^{2} \Gamma_{E}(X,X).
  \end{equation*}
  Thus, we find that
  \begin{equation*}
    \Prob*{ \Gamma_{E \times F}(Z,Z) \leq \varepsilon } \lesssim \varepsilon^{\theta} \Esp*{ Y^{-2\theta} }.
  \end{equation*}
  Thus, $Z \in \mathscr{A}$.
\end{proof}

\subsection{Applications}

\subsubsection{Improving normal convergence for quadratic forms}\label{s:normal-convergence}

In this section, we consider a sequence of independent and identically distributed random variables $(X_{i})$ such that \cref{ass:product-dirichlet,ass:small-ball-gamma} hold.
We also consider a sequence of quadratic forms
\begin{equation*}
  Q_{n} \coloneq \sum_{i,j} a_{ij}^{(n)} X_{i} X_{j},
\end{equation*}
for some sequence $\set*{ \mathsf{A}^{(n)} \coloneq (a_{ij}^{(n)}) : n \in \mathbb{N} }$ of symmetric Hilbert--Schmidt operators with vanishing diagonal.
In this section, we assume that $(\mathsf{A}^{(n)})$ satisfies some conditions ensuring that $(Q_{n})$ converges in law to a Gaussian distribution, and we show that the convergence is automatically improved in $\mathscr{C}^{\infty}$-convergence.
By this, we mean that for all $q \in \mathbb{N}$, there exists $N_{q} \in \mathbb{N}$ such that for $n \geq N_{q}$, the law of $Q_{n}$ has a $\mathscr{C}^{q}$-density and that this density converges in $\mathscr{C}^{q}$ to that of a standard Gaussian.

\paragraph{Leptokurtic random variables}

We say that a random variable $Z$ is \emph{leptokurtic} whenever $\Esp*{Z} = 0$, $\Esp*{Z^{2}} = 1$, and $\Esp*{Z^{4}} \geq \Esp*{N^{4}} = 3$ where $N$ is a standard Gaussian variable. 

\begin{proposition}
  With the above notation, assume moreover that $X_{1}$ is leptokurtic and that
\begin{equation*}
  Q_{n} \xrightarrow[n \to \infty]{law} \mathscr{N}(0,1).
\end{equation*}
Then,
\begin{equation*}
  Q_{n} \xrightarrow[n \to \infty]{\mathscr{C}^{\infty}} \mathscr{N}(0,1).
\end{equation*}
\end{proposition}
\begin{proof} 
By \cite{NPPS}, we find that the spectral radius $\rho(\mathsf{A}^{(n)}) \to 0$ as $n \to \infty$.
Let $q \in \mathbb{N}$.
By \cref{th:spectral-radius-implies-influence} this implies that $\limsup \mathcal{R}_{q}(\mathsf{A}^{(n)}) > 0$ and $\tau(\mathsf{A}^{(n)}) \to 0$.
We apply \cref{th:regularity-quadratic-form} to conclude.
\end{proof}

\paragraph{Uniform order statistics}

\begin{proposition}
  Let the previous notation prevail, and assume that $(X_{i})$ is the sequence of centered ordered statistics of a uniform distribution on $[0,1]$.
  Assume that
  \begin{equation*}
    Q_{n} \xrightarrow[n \to \infty]{law} \mathscr{N}(0,1).
  \end{equation*}
  Then,
  \begin{equation*}
    Q_{n} \xrightarrow[n \to \infty]{\mathscr{C}^{\infty}} \mathscr{N}(0,1).
  \end{equation*}
\end{proposition}

\begin{remark}
  Sufficient conditions for the asymptotic normality of such order statistics have appeared frequently in the statistic literature.
  See, for instance \cite{GuttorpLockhart}, and the references therein.
  In \cite{GuttorpLockhart}, the authors also derive conditions for convergence of a quadratic form in order statistics to the law of an element of the second Wiener chaos.
  The spectrum associated to the limit Gaussian quadratic form is explicit.
  Thus, it can be shown that, the law of the target distribution is smooth.
  It would be of interest to verify that our techniques also provide convergence $\mathscr{C}^{\infty}$ in this case.
  However, for brevity, we do not carry precise computations in this paper.
\end{remark}

\begin{proof}
Due to the lack of independence of in $(U_{(i)})$ our result do not apply directly, however we can use the following representation
\begin{equation*}
  Q_{n} \overset{law}{=} T_{n} \coloneq \psh{X}{\mathsf{B}^{(n)}X} \frac{1}{(1 + \bar{X})^{2}},
\end{equation*}
where $(X_{i})$ is a sequence of independent random variables distributed as centered exponentials of intensity $1$, $\bar{X}$ is the empirical mean, and $\mathsf{B}^{(n)} = \mathsf{M}^{T} \mathsf{A}^{(n)} \mathsf{M}$, for some explicit deterministic matrix $\mathsf{M}$.
By the law of large number $\bar{X} \to 0$ almost surely, and by Basu's theorem $\bar{X}$ is independent of $T_{n}$.
Thus, since the exponential distribution of intensity $1$ being leptokurtic, we find that $\rho(\mathsf{B}^{(n)}) \to 0$.
We conclude as before.
\end{proof}

\subsubsection{Improving the Carbery--Wright inequality}
Assume that the $X_{i}$'s are centered, normalized, and with log-concave, and let $Q \coloneq \psh{X}{\mathsf{A} X}$ be a random quadratic form in the $X_{i}$'s.
Then, a celebrated result of \citeauthor{CarberyWright} \cite{CarberyWright} states that
\begin{equation}\label{eq:carbery-wright}
  \Prob*{ Q \leq \varepsilon } \lesssim \varepsilon^{1/2}.
\end{equation}
However, whenever $\tau(\mathsf{A})$ is small enough \cref{th:regularity-quadratic-form} ensures that the law of $Q$ has a continuous density.
Thus \cref{eq:carbery-wright} is improved to
\begin{equation}\label{eq:carbery-wright-improved}
  \Prob*{ Q \leq \varepsilon } \lesssim \varepsilon.
\end{equation}
Note that \cref{eq:carbery-wright} holds for any $(X_{i})$ whose joint law is a log-concave distribution (not necessarily independent); while \cref{eq:carbery-wright-improved} holds for sequence of independent random variables such that \cref{ass:product-dirichlet,ass:small-ball-gamma} hold and the influence is small enough.
The use of the Carbery--Wright inequality is ubiquitous in probability theory, and an improvement of their inequality could improve several fundamental results in probability theory.
We are planning to explore in details such consequences in a future work.


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\printbibliography%
\end{document}
