\section{Introduction}
The dynamic range of current consumer-level light sensors is still quite limited. As a result, a single shot of the natural scene always fails to capture image details in the strongly or weakly lit scene areas. A promising approach is to synthesize a high dynamic range image by a collection of images captured at multiple exposures, with the over-exposed or under-exposed of one image being well captured in other images \cite{SIGGRAPH97,ward2003fast}. This approach is challenged, if the camera and scene objects move independently. In such situations, to infer the missing contents of one exposure from other exposures, perfect motion estimation and sequence registration are required. Unfortunately, image motion estimation is an ongoing problem far from solved, being especially sensitive to brightness variation and lack of details. The erroneous estimation of motion would further cause the notorious ``ghosting'' or ``motion blur'' artifacts. Therefore, the central problem of multi-exposure HDR fusion is to fuse images containing large motion.

Many previous works detect misalignment by a post-processing step, then remove the detected areas from HDR image computation. The detectors are manually designed according to domain knowledge based on local  low-level features, such as appearance \cite{grosch2006fast}, gradient \cite{zhang2011gradient}, matrix rank \cite{lee2014ghost,oh2014robust}, low-order statistics \cite{pece2010bitmap}. The representation power of such features is limited, especially in under-exposed or over-exposed regions. Consequently, they may fail to locate well-exposed correspondences in side images to ill-posed regions in the reference image.    

The application of Convolutional Neural Networks (CNN) to HDR imaging enables extracting high-level features to guide multi-exposure fusion \cite{Kalantari2017DeepHD}. Joint alignment and HDR fusion models have been shown effective \cite{wu2018end,niu2021hdrgan}. In general, CNN-based multi-exposure HDR models focus on handling two issues: to establish long-range correlation and to suppress outlier features. To allow long-range image content retrieval, a large receptive field is necessary, hence 
dilated convolution \cite{yan2019attention}, deformable convolution \cite{liu2021adnet}, non-local module \cite{yan2020nonlocal} are explored. To distinguish inlier and outlier features, feature selection by spatial attention \cite{yan2019attention} and mask \cite{xiong2021hierarchical} are devised. Although the theoretical receptive field of a convolutional network may be expanded to the full image scale, the effective receptive field may still be local \cite{luo2016understanding}. 

Compared to CNN models, Transformer is more capable at global matching \cite{vaswani2017attention}. However, by the time of writing, its application to multi-exposure HDR fusion has not been discussed, to the best of our knowledge. The difficulty of applying Transformer is its high computational load, which causes imaging latency. A newly proposed technique Swin Transformer, whose shift-window self-attention progressively expands local attention to global attention, while gaining computational efficiency \cite{liu2021swin}. Nonetheless, directly stacking Swin Transformers for multi-exposure fusion consumes huge graphics memory and obstructs the network from extracting deep features. 

In this work, we investigate applying Swin Transformer to deep fusion of multi-exposure images. Inspired by Restormer \cite{zamir2021restormer}, which modifies Transformer to gated transformer and embeds it to a U-net for single image restoration, we alter the Multi-Layer Perceptron (MLP) of Swin Transformer to a gating network. In the framework of hierarchical encoder-decoder, Gated Swin Transformer can be stacked to form deep network, which progressively establishes global correlation and dynamically suppresses outliers.   
To summarize, the presented work
\begin{itemize}
\item constructs a new model based on Swin Transformer for multi-exposure HDR imaging. 
\item proposes integrating Gated Swin Transformer with  pyramidal encoder-decoder residual network. It takes advantage of the computational efficiency of Swin Transformer, while alleviating the heavy graphics memory consumption of Swin Transformer by hierarchical computation. 
\item achieves accuracy on par with state-of-the-art methods, while gaining higher efficiency. 
\end{itemize}