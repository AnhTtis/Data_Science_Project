\section{Approach}
\subsection{Overall Workflow}
\begin{figure*}[!tbh]
\centering
\includegraphics[width=0.95\textwidth]{arch.pdf}
\caption{Overview of our multi-frame HDR model. It is mainly composed of our proposed gated swin transformer layer. The enlarged part shows the internal structure of our proposed GSTL.}
\label{arch}
\end{figure*}
The multi-exposure fusion for HDR imaging can be described as, given an odd number of $K$ Limited Dynamic Range (LDR) images $\{\mathbf{I}_{k}\}_{k\in[1,K]}$, which are shot at different shutter length $t_{k\in[1,K]}$, where $t_1 < t_2 < \cdots < t_K$, transforming the middle exposure $\mathbf{I}_{\frac{k+1}{2}}$ to a HDR image $\hat{\mathbf{I}}$.  

Following the literature convention, this work assumes $K=3$ and normalizes $\mathbf{I}_{k}$ by $t_{k}$ according to
\begin{equation}
\mathbf{H}_{k}=\frac{\mathbf{I}_{k}^{\gamma}}{t_{k}},
\end{equation}
where by default $\gamma$ is 2.2. We denote the concatenation of $\mathbf{H}_{k}$ and $\mathbf{I}_{k}$ as $\mathbf{X}_{k}$, the concatenation of all $\mathbf{X}_{1\leq k \leq 3}$ as the input tensor $\mathbf{X}$, which is the input our neural network HDR model $f(\mathbf{X};\mathbf{\Theta})$, where $\mathbf{\Theta}$ stands for the set of trainable parameters. 

The architecture of $f(\mathbf{X};\mathbf{\Theta})$ is designed as a residual network. That is, a long shortcut connects the bottom level features with the top level features, to stabilize training. Mathematically, $f(\mathbf{X};\mathbf{\Theta})$ is decomposed to 
\begin{align}
    f(\mathbf{X};\mathbf{\Theta}) = 
    p\left(
    g\left(\mathbf{X}_{2};\mathbf{\Theta}_{g}\right)+
    r\left(\mathbf{X};\mathbf{\Theta}_{r}\right);
    \mathbf{\Theta}_{p}
    \right),
\end{align}
where $g\left(\mathbf{X}_{2};\mathbf{\Theta}_{g}\right)$ is a convolution layer that operates on $\mathbf{X}_{2}$; $p\left(~;\mathbf{\Theta}_{p}\right)$ consists of the final prediction convolution layer and a Sigmoid activation layer, to output $\hat{\mathbf{I}}$; $r\left(\mathbf{X};\mathbf{\Theta}_{r}\right)$ is the main body of the neural model that maps $\mathbf{X}$ to the residual. 

The residual function $r$ is designed as a U-shape Encoder-Decoder network. The encoder contains 4 encoding blocks $e_{1\leq i\leq 4}$, with a down-scaling layer between each pair of adjacent blocks, so as to extract hierarchical features. Different from Restormer \cite{zamir2021restormer}, each of these blocks is a small ResNet. Formally, the sequential encoding process can be descried as,
\begin{align}
    \mathbf{E}_{1} & =  e_{1}\left(\mathbf{X};\mathbf{\Theta}_{e_1}\right)
    +\mathbf{X} \\\nonumber
    \mathbf{E}_{i} & =  e_{i}\left(s\left(\mathbf{E}_{i-1}\right);
    \mathbf{\Theta}_{e_i}\right)+\mathbf{E}_{i-1}, \quad i>1,
    \label{eq:encode}
\end{align}
where $\mathbf{E}_{i}$ denote feature tensors; Function $s$ represents the down-scaling layer, implemented by Pixel-Unshuffle \cite{shi2016real} in our work. 

The decoder reverses the encoding hierarchy by 3 sequential decoding ResNet blocks $d_{i=3,2,1}$, with up-scaling layers inserted in between. The feature tensors are computed by
\begin{align}
    \mathbf{D}_{3} & =  d_{3}\left([u(\mathbf{E}_{3}),\mathbf{E}_{2}];\mathbf{\Theta}_{d_3}\right)+\mathbf{E}_{3}
    \\\nonumber
    \mathbf{D}_{i} & =  d_{i}\left([u(\mathbf{D}_{i+1}),\mathbf{E}_{i}];\mathbf{\Theta}_{d_i}\right)+\mathbf{D}_{i+1}, \quad i<3,
\end{align}
where function $u$ performs up-sampling by Pixel-Shuffle, Notation $[\cdot,\cdot]$ means channel-wise concatenation. 

The decoded feature tensor $\mathbf{D}_{1}$ is further processed by a refinement block $t$ (see Sec. \ref{sec:block}). The overall architecture of the proposed model is depicted in Fig. \ref{arch}.

\subsection{Internal Architecture of Building Blocks}
\label{sec:block}
The encoding blocks $e_{1\leq i\leq 4}$, decoding blocks $d_{1\leq i\leq 3}$, and refinement block $t$ have similar internal architecture. For instance, encoder $e_{i}$ processes the input tensor $\mathbf{E}_{i-1}$ (Eq. \ref{eq:encode}) by $n_{i}$ sequential Residual Gated Swin Transformers (RGST, detailed in Sec. \ref{sec:Attention}), followed by a convolution layer. Then a skip connection sums up the convolution output and $\mathbf{E}_{i-1}$. The number of RGST units used in each block varies with block feature spatial resolution. Here we set the numbers for the encoding and decoding blocks to $[2,3,3,4]$, from the fine to coarse scale in the feature pyramids. The number of the RGST units in the refinement block is set to $2$. 

\subsection{Residual Gated Swin Transformer Unit}
\label{sec:Attention}
Our basic building unit RGST alters Swin Transformer to address the multi-exposure HDR fusion problem. We take advantage of the multi-head self-attention modules in Swin Transformer, to aggregate features according to pair-wise feature affinity, which implicitly performs cross-exposure alignment and feature fusion. Furthermore, we modify the Multi-Perception Layer (MPL) in Swin Transformer to a gated feed-forward network, where the gates are defined according to \cite{zamir2021restormer}.

\subsubsection{Self-Attention Computation}
We take the $i$-th encoding block $e_i$ as an example to analyze the process of feature aggregation weighted by windowed affinity in the RGST unit. The input feature tensor $\mathbf{E}_{i}$ is layer-wisely normalized to $\mathbf{E}_{i}$, and then partitioned to windows of size $M\times M$. In each window, $\mathbf{E}_{i}$ is linearly projected to generate the Query, Key and Value tensors by trainable matrices $\mathbf{W}_q^\mathrm{j}$, $\mathbf{W}_k^\mathrm{j}$, $\mathbf{W}_v^\mathrm{j}$, where $j$ indexes the sets of projections used in the attention module, each set counted as one ``head'' in the literature. Local pair-wise affinity is computed within each window, by multiplying the Query and Key matrices. Subsequently, each feature in the Vector matrix is replaced by an aggregation of local features weighted by the computed affinity. The output of each head are concatenated and the linearly projected to a tensor ${\mathbf{E}^{\prime}_{i}}$. Unlike global self-attention, this windowed self-attention compares each feature against locally neighbouring features. Hence it is prone to misalignment caused by large motion, which is the so-called ``aperture problem'' in the field of optical flow computation. As a result, the aggregation may be impacted heavily by misaligned  features (i.e., outliers). We refer readers to \cite{liu2021swin} for details of Swin Transformer.

\subsubsection{Gated Feed-Forward Network}
To restrain the contaminated features from further computation, we employ the gating mechanism proposed by \cite{zamir2021restormer}. In particular, ${\mathbf{E}^{\prime}_{i}}$ is first layer-wisely normalized to ${\mathbf{E}^{\prime}}_{i}$. It then enters two parallel convolution branches, each of which is a pair of pixel-wise and channel-wise convolutions, yielding tensors $\dot{\mathbf{E}}_{i}$ and $\ddot{\mathbf{E}}_{i}$. $\dot{\mathbf{E}}_{i}$ is further activated by the Gaussian Error Linearity Unit (GELU) function \cite{hendrycks2016gaussian}, obtaining the gate tensor $\mathbf{G}_{i}$, which is point-wisely multiplied to $\ddot{\mathbf{E}}_{i}$, yielding $\bar{\mathbf{E}}_{i}$. 

The rest procedure inside the encoding block $e_{i}$ involves the self-attention in shifted windows and another gated feed-forward network. Their computation is as same as the description above. Although the window shifting strategy enlarges the receptive field, it may not be fully global at the finest scale of the U-net hierarchy. As the computation proceeds to coarser scales, the receptive field is significantly enlarged to the full image. Hence using the Gated Swin Transformer as the building unit in the U-shape encoder-decoder network achieves both global receptive field and low memory consumption. 

In our work, the window width $M$ is set to 8. The number of heads for self-attention varies with the feature spatial resolution, taking the value of \([1,2,4,4]\) from fine to coarse.

\subsection{Loss Function}
The loss function to train our model consists of two loss terms. Following the literature, the first loss term $\mathcal{L}_1$ measures the closeness of the output $\hat{\mathbf{I}}$ and the ground truth $\mathbf{I}$ in the tone-mapped domain by $l_{1}$ norm distance
\begin{equation}
\mathcal{L}_1=\|\mathcal{T}(\mathbf{I})-\mathcal{T}(\hat{\mathbf{I}})\|_{1} 
\end{equation}
where the tone mapping function $\mathcal{T}$ is the differentiable $\mu$-law range compressor, commonly used in previous works \cite{Kalantari2017DeepHD}.

Additionally, to train the model to faithfully reconstruct the missing details in over-exposed and under-exposed regions, the second loss term $\mathcal{L}_2$ measures the Structural Similarity (SSIM) \cite{wang2004image}, between the tone-mapped output and ground truth,
\begin{equation}
\mathcal{L}_{2}=1-\text{SSIM}(\mathcal{T}(\mathbf{I}), \mathcal{T}(\hat{\mathbf{I}})) 
\end{equation}

The total loss function $\mathcal{L}$ is then defined by 
\begin{equation}
\mathcal{L} = \mathcal{L}_{1}+\mathcal{L}_{2}.
\end{equation}


 