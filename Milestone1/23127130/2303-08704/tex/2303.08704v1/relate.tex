\section{Related Work}
The key to multi-exposure HDR imaging is to exploit cross-exposure correlation. Early works assume that the scene is static, and synthesize the HDR image by inferring the light energy map and camera response function from exposure bracketing \cite{SIGGRAPH97,mitsunaga1999radiometric,pal2004probability} or by a weighted fusion of different exposures \cite{ward2003fast,gelfand2010multi}. The fusion approach is now more widely investigated. The advent of CNN recasts the fusion in image domain to fusion in feature space. Recently, research effort on multi-exposure fusion has been mainly devoted to addressing cross-exposure registration and misalignment, to overcome the difficulty caused by scene motion \cite{ma2017robust}. 

\textbf{CNN-Based Multi-Exposure Fusion for HDR.} Applying deep convolution to multi-exposure fusion allows establishing cross-exposure correlation by extracting high-level contextual features, yielding robust matching. The first CNN-based model estimates image motion by optical flow to align the image sequence before convolutional feature extraction \cite{Kalantari2017DeepHD}. This external registration step prevents the model from being end-to-end trainable. Moreover, accurate optical flow algorithms are generally too slow for real-time HDR imaging. \cite{wu2018end} propose an end-to-end image translation network to formulate multi-image HDR fusion. Generative Adversarial Network is investigated, implicitly aligns the images in the generator \cite{niu2021hdrgan}. Several works perform soft feature selection. For example, \cite{yan2019attention} designs a spatial attention module by filtering the concatenation of low-level features extracted from the reference and side images, whereas \cite{xiong2021hierarchical} trains a masking tensor to blend the exposures. These models show that feature selection is crucial for HDR imaging. Recently, \cite{liu2021adnet} and \cite{xiong2021hierarchical} demonstrate the benefits of pyramidal alignment and fusion.

Different from the CNN-based models, our network explores the potential of Swin Transformer for HDR imaging. The self-attention mechanism in Swin Transformer implicitly searches for global cross-exposure alignment. We modify Swin Transformer with the gating mechanism similar to \cite{yu2019free} to mask out outliers.

\textbf{Transformer Network for Single Image Restoration}
A related line of work is single image restoration, e.g., denoising, deblurring, inpainting, super-resolution, based on modified Transformer (e.g., \cite{zamir2021restormer}) or Swin Transformer (e.g., \cite{liang2021swinir}).  

Our work is closely related to Restormer. Briefly, Restormer attaches a gating unit to the feed-forward network in Transformer. To lower the computational cost inherent to Transformer, Restormer replaces the pairwise affinity computation in Transformer by cross-channel covariance. That is, it uses channel attention instead of spatial attention, therefore makes it unsuitable for inferring cross-exposure spatial correlation. Instead, we employ Swin Transformer, which replaces global self-attention by shifted window self-attention. Swin Transformer trades memory storage for computational efficiency, maintaining the capability of connecting spatially distant yet contextually close features. Inspired by the hierarchical architectures in \cite{zamir2021restormer,xiong2021hierarchical,liu2021adnet}, our Gated Swin Transformer units serve in a U-shape pyramidal encoder-decoder, which significantly saves the memory storage. 