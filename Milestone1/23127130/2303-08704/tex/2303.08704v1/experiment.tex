\section{Experimental Results}
\label{sec:experiments}
\subsection{Training Details}
\cite{Kalantari2017DeepHD} provides the first dataset specifically designed for multi-exposure HDR fusion under large motion. It consists of 74 training sets, which we use to supervise the training of our model. We crop the input images to patches of size \(256 \times 256\) at a step size of 64. This totally generates 20128 training samples. To augment training samples, we randomly rotate and flip the training images. The training adopts Adam optimizer. The learning rate is initialized to \(10^{-4}\) and is reduced to \(10^{-5}\) after 20 epochs. It is observed that 40 epochs are sufficient for the training to converge.    

\subsection{Numerical Evaluation}
We numerically measure the performance of our method on the 15 test sets of \cite{Kalantari2017DeepHD}, by Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity, computed in both tonemapping domain (-\(\mu\)) and HDR linear domain (-L). Visual difference metric HDR-VDP-2 is also adopted, where the parameters are set as same as in previous works \cite{wu2018end} and \cite{niu2021hdrgan}. 

Table \ref{table_metrics} compares our model with state-of-the-art models. For \cite{yan2020nonlocal} and \cite{xiong2021hierarchical}, we use the results reported in the publications. Note that \cite{sen2012robust} and \cite{hu2013hdr} are not machine learning based methods. Moreover,  \cite{Kalantari2017DeepHD} and \cite{wu2018end} apply optical flow and homography transformation to preprocess the input images respectively, and hence entail extra computation. 

Table \ref{table_metrics} shows that our method outperforms competing method in terms of PSNR-L, SSIM-$\mu$, SSIM-L and HDR-VDP-2. It ranks the second best in PSNR-$\mu$, being slightly (0.1dB) inferior to \cite{xiong2021hierarchical}. Note that \cite{xiong2021hierarchical} utilizes a pretrained model to detect ghosting regions for training, whereas our method does not require any pretrained model. The high PSNR and SSIM scores varify that our model has strong HDR reconstruction ability and can accurately restore the radiance and structure of the scene in both tonemapping domain and HDR linear domain. Furthermore, its high performance in term of HDR-VDP-2\cite{mantiuk2011hdr} performance indicates that our method can generate HDR image visually close to the target image.

\begin{table*}[ht]
\centering
\begin{tabular}{l|c|c|c|c|c}
\hline
& PSNR-$\mu$ & PSNR-L & SSIM-$\mu$ & SSIM-L & HDR-VDP-2 \\
\hline
\bfseries Sen & 40.97 & 38.36 & 0.9830 & 0.9746 & 60.60\\
\hline
\bfseries Hu  & 35.65 & 30.80 & 0.9725 & 0.9491 & 58.34\\
\hline
\bfseries Kalantari & 42.69 & 41.22 & 0.9888 & 0.9845 & 65.05\\
\hline
\bfseries DeepHDR& 41.99 & 41.22 & 0.9878 & 0.9859 & \underline{65.91}\\
\hline
\bfseries AHDR & 43.62 & 41.03 & 0.9900  &\underline{0.9883} & 63.85 \\
\hline 
\bfseries NHDRRNet& 42.414 & - & 0.9887 & - & 61.21 \\
\hline 
\bfseries HDR-GAN &43.92 & \underline{41.57} &\underline{0.9905} &0.9865 & 65.45\\
\hline 
\bfseries HFNet & \textbf{44.28} & 41.47 & - & - & - \\
\hline 
\bfseries Ours & \underline{44.18} & \textbf{42.19}&\textbf{0.9912} & \textbf{0.9883}& \textbf{67.07} \\
\hline
\end{tabular}
\caption{Numerical performance of the proposed model, evaluated on the dataset by Kalantari-Ramamoorthi. The best and second best results for each metric are marked in \textbf{bold} and \underline{underlined}, respectively}
\label{table_metrics}
\end{table*}

\subsection{Visual Performance Evaluation}

\begin{figure*}[!htb]
\centering
\includegraphics[width=\textwidth]{experiments/kalantari_test.png}
\caption{Visual comparison on the test set of Kalantari-Ramamoorthi dataset. Zoom-in views of reconstruction by each method are presented on the saturated regions that contain moving objects. Our network built with gated Swin Transformer yields noticeably better visual results than other methods.}
\label{fig_kalantari_test}
\end{figure*}
Fig. \ref{fig_kalantari_test} present the visual performance of our method and comparable methods on two examples from \cite{Kalantari2017DeepHD}. We present the zoom-in views of two challenging cases, where large saturated regions contain substantial non-rigid motion in the reference image. The two patch-based methods do not reconstruct the missing details in the saturated regions, as they heavily rely on the details provided by the reference image for registration. Image reconstructed by the optical flow based method \cite{Kalantari2017DeepHD} suffers motion blur artifacts. This is because the convolutions of DeepHDR and HDR-GAN have limited receptive fields, and hence are hampered to repair missing content in misaligned regions by aligned regions. The gating mechanism of AHDR is only applied to low-level features, so the high-level outliers may deteriorate the HDR fusion. In contrast to comparable methods, our model remarkably overcomes the ghosting artifacts.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{experiments/sen_test.pdf}
\caption{Visual performance comparison on example images from the dataset by Sen et al. Zoom in views on challenging areas are presented. Although the ground truth is unavailable, it can be clearly observed that our method visually performs better than comparable methods.}
\label{sen_test}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{experiments/tursun_test.pdf}
\caption{Visual performance comparison on example images from the dataset by Tursun et al. Compared to state of the art methods, our method suffers less ghosting artifact.}
\label{tursun_test}
\end{figure}

Fig.\ref{sen_test} and Fig.\ref{tursun_test} present visual performance of our method on two examples from benchmark datasets \cite{sen2012robust} and \cite{tursun2016objective}. As these test datasets   do not provide ground truth image. we mark the visual difference on the results generated by different methods. It can be seen that our method suffers less artifacts than other methods in various scenes with various motion patterns, achieving better visual results. Our method creates high-quality HDR more robustly and generalizes well. 

\subsection{Ablation Study}

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
\hline
                         & PSNR-$\mu$ & PSNR-l & SSIM-$\mu$ & SSIM-l & HDR-VDP-2 \\ \hline
restormer(w/o ssim loss) & 44.00  & 41.5   & 0.9906 & 0.9873 & 64.72  \\ \hline
Ours(w/o ssim loss)      & 44.07  & 41.83  & 0.9909 & 0.9879 &  64.78  \\ \hline
Ours                     & 44.18  & 42.19  & 0.9912 & 0.9883 & 67.07      \\ \hline
\end{tabular}
}
\caption{Experimental results of ablation study. We compare using Gated Swin Transformer v.s. Gated Transformer, and the combined loss function v.s. the traditional $l_{1}$ norm loss function.}
\label{table_ablation_block_loss}
\end{table}

We verify various components of our method, including Swin Transformer, loss function, and gating mechanism by ablation study.

\subsubsection{Ablation Study on Block Design}
Our model has similar architecture to Restormer, which uses modified Transformer, whereas we use modified Swin Transformer as the building unit. For comparison, we replace the residual modules in each block in our model with multiple transformer layers as in Restormer, with same number of transformer layers. Table \ref{table_ablation_block_loss} presents the results, which show that using Swin Transformer achieves superior performance in all measures. The reason is that the attention module of Restormer is computed channel-wise, but forgoes the cross-exposure spatial dependency to repair the non-aligned area. 

\subsubsection{Ablation Study on Loss Function}
We trained our model under different loss function configurations, as shown in \ref{table_ablation_block_loss}. The results validate that the SSIM loss benefits detail reconstruction.

\subsubsection{Ablation Study on Gating Mechanism}
\begin{table}[h]
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
\hline
           & PSNR-$\mu$ & PSNR-l & SSIM-$\mu$ & SSIM-l & HDR-VDP-2 \\ \hline
w/o gating & 43.14  & 41.03  & 0.9904 & 0.9868 &     64.88      \\ \hline
one gating & 43.44  & 41.42  & 0.9909 & 0.9882 &    67.13   \\ \hline
Ours       & 43.61  & 41.74  & 0.9909 & 0.9881 & 66.96     \\ \hline
\end{tabular}
}
\caption{Ablation experimental results to verify the effectiveness of the gating mechanism}
\label{table_ablation_gating}
\end{table}

The gating mechanism is an important component in our model. Ablation study is conducted in the gating mechanism as follows.

\textbf{w/o gating}: The gating mechanism is not used in the feed forward network of all transformer layers in the model, that it, our GST unit degenerate to the vanilla Swin Transformer.

\textbf{one gating}: The gating mechanism is only used in the first Swin Transformer layers subsequent to the embedding layer, but not used for other layers. 

 Table \ref{table_ablation_gating} shows the results of the ablation experiments, where the model is trained for 20 epochs. By removing the gating mechanism, the network relies on self-attention for image alignment, resulting in the lowest performance. On top of it, adding gates to low level layers notably improves the HDR reconstruction. Furthermore, by integrating the gating mechanism with all Swin Transformer layers, the model effectively inpaints information in non-aligned regions and obtains the highest HDR reconstruction results, thus validates the effectiveness of the gating mechanism in our model.
