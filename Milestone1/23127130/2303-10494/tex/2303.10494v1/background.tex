\section{Background}
\subsection{\plmfull}

\begin{figure}
    \includegraphics[width=0.85\linewidth]{figures/background_diagram.pdf}
    \caption{\plm overview}
    \label{fig:plm_overview}
\end{figure}

\plmfull{s} (\plm{s}) trained on large amounts of text combined with code snippets from a broad range of open-source repositories have shown impressive progress in various code-related tasks~\cite{chen2021codex, fried2022incoder, xu2022systematic}. To apply them to code, researchers may further train the models on open-source code repositories. On top of that, \plm{s} can be \emph{fine-tuned} (updating pre-trained model parameters by further training on the downstream tasks~\cite{devlin2018bert}) to target a specific task such as defect prediction~\cite{omri2020defectpred} or code clone detection~\cite{wang2022bridge}. Different from fine-tuning, \emph{prompting} is a way to directly use \llm{s} on downstream tasks without further training. Prompting involves providing a natural language description of the task and optionally including a few demonstrations of the task to the \llm{s} before the actual input. Researchers have successfully applied prompting to tasks like code completion~\cite{Nijkamp2022CG} and code summarization~\cite{kuznia2022summ}.

%

\plm{s} are based on the popular Transformer architecture~\cite{vaswani2017attention}, which combines an \textbf{encoder} with a \textbf{decoder} to perform text generation. The encoder first takes in the input to the model and then produces an encoded representation. The decoder uses this encoded vector to autoregressively generate the next token based on all previously generated tokens. Using this paradigm, researchers build larger and larger models (as large as 540B in the number of model parameters~\cite{chowdhery2022palm}) and demonstrated impressive results on code-related tasks such as code completion~\cite{chen2021codex} and code synthesis~\cite{austin2021sythesis}. 

\plm{s} can be classified into three groups based on their model architecture and pre-training objective: \textbf{Decoder-only} (Left-to-Right Language Models), \textbf{Encoder-only} (Masked Language Models), and \textbf{Encoder-decoder models}. Figure~\ref{fig:plm_overview} shows an overview of the three different \plm architectures. Decoder-only models perform left-to-right generation by producing the probability of a token given all previous tokens. One of the most well-known \plm{s}, GPT~\cite{radford2019gpt2, brown2020gpt3}, is based on this architecture. During training, decoder-only models aim to predict the next token given all previous context. Examples of decoder-only models for code are CodeGPT~\cite{lu2021codexglue}, CodeParrot~\cite{tunstall2022natural}, and Codex~\cite{chen2021codex}. These models can be directly used for program generation given previous code contexts. Encoder-only models, on the other hand, only use the encoder component to provide an encoded representation of the input. Models such as BERT~\cite{devlin2018bert} are trained using the \mlmfull (\mlm) objective, where a small percentage (e.g., 15\%) of the training tokens are masked out and the model aims to recover these masked tokens using the bi-directional context. CodeBERT~\cite{feng2020codebert}, GraphCodeBERT~\cite{guo2021graphcodebert}, and CuBERT~\cite{kanade2020cubert} are examples of encoder-only models where it can provide a representation of the input code to be used for downstream tasks such as code clone detection~\cite{wang2022bridge}. Encoder-decoder models (T5~\cite{raffel2020t5}, BART~\cite{lewis2019bart}) use both components of the transformer and are typically trained using \mspfull (\msp) objective. Different from \mlm, instead of masking out individual tokens, \msp replaces a sequence of tokens with a single span mask. The goal of the training is to recover the original sequence using both the context before and after the span mask token. For code tasks, \ctfive~\cite{wang2021codet5} and PLBART~\cite{ahmad2021PLBART} are examples of encoder-decoder models and due to the \msp pre-training objective, they can be directly used to fill in arbitrary code snippets given the bi-directional code context. 

%

\subsection{Automated Program Repair}

\aprfull (\apr) works by automatically generating patches when given the buggy project and potential fault locations. Traditional \apr tools can be categorized into constraint-based~\cite{mechtaev2016angelix, le2017s3, demacro2014nopol, long2015spr}, heuristic-based~\cite{legoues2012genprog, le2016hdrepair, wen2018capgen}, and \template~\cite{ghanbari2019prapr, hua2018sketchfix, martinez2016astor, liu2019tbar, liu2019avatar} tools. Among those, \template \apr has been regarded as the state-of-the-art in achieving the best repair performance~\cite{ghanbari2019prapr, liu2019tbar, benton2020effectiveness}. \Template \apr works by using pre-defined templates (handcrafted by human experts) which target specific patterns in source code. Each template will have an associated fix that modifies the found patterns in the source code to fix specific types of bugs. However, \template \apr tools cannot fix bugs that do not fall under the pre-defined templates. As a result, \template tools lack the ability to generalize to unseen bug types. 


In recent years, researchers have begun to focus on \learning \apr approaches such as \selfapr~\cite{ye2022selfapr}, \rewardrepair~\cite{ye2022rewardrepair}, \recoder~\cite{zhu2021recoder}, \cure~\cite{jiang2021cure}, and \coconut~\cite{lutellier2020coconut} based on the \nmtfull (\nmt)~\cite{sutskever2014mt} architecture. The goal of these tools is to learn a transformation using DL models that turns buggy code snippets into patched ones. To facilitate this, these tools require further training on specific bug-fixing datasets containing pairs of buggy and fixed code snippets. However, these bug-fixing datasets are usually scraped from open-source bug-fixing commits using handwritten heuristics such as keyword searching~\cite{zhu2021recoder, lutellier2020coconut, dallmeier2007benchmark, jiang2019infer}, which can include irrelevant code commits; even the correctly identified bug-fixing commits may contain various irrelevant code changes (such as refactoring or new feature implementation), introducing various noises in the datasets. Also, to avoid including bug-fixing commits with irrelevant code changes,
existing \learning \apr techniques will limit the commits
to ones with few lines of changes~\cite{jiang2021cure, zhu2021recoder, lutellier2020coconut}, further limiting
the amount of training data. Moreover, \nmt-based \apr tools may still not generalize to specific code or bug types that are not seen inside of the (limited) bug-fixing datasets. 

Recognizing these limitations in \nmt-based \apr, researchers have proposed \llm-based \apr tools~\cite{xia2022alpharepair, xia2023repairstudy, prenner2021codexws, kolak2022patch} which do not require bug-fixing datasets by directly using \llm{s} for \apr. \alpharepair~\cite{xia2022alpharepair} first introduced the \csapr (or infilling-style \apr) to directly leverage \plm{s} in a zero-shot manner to fill in the code given the context before and after the buggy line. \alpharepair first generates repair inputs that replace the original buggy line with masked tokens and uses the \codebert model~\cite{feng2020codebert} to directly recover the correct code to fill in each masked token. Later studies~\cite{xia2023repairstudy, kolak2022patch, prenner2021codexws} also used different \llm{s} (including Decoder-only and Encoder-decoder models) to not only perform \csapr but also repair scenarios where a complete fixed function is generated. Contrasting with \nmt-based \apr tools, \llm-based \apr leverages the pre-training objectives of \plm{s} which can directly learn the relationship between correct code and its context without relying on historical bug-fixing commits. As a result, \llm-based \apr tools have shown to achieve state-of-the-art performance on repair tasks across multiple programming languages~\cite{xia2023repairstudy}. In this work, we present the first work to further advance state-of-the-art \llm-based \apr with the insight of the plastic surgery hypothesis. 

