\section{Result Analysis}

\subsection{RQ1: Comparison with State-of-the-art}

\begin{table*}[!htbp]
\centering
\caption{Evaluation results of correct fixes on \dfj 1.2}
\label{tab:dfj_result}
\scalebox{0.8}{
\begin{tabular}{lcc|ccccccccc}
    \hline
    \textbf{Project} & \textbf{\ourtech} & \textbf{\ctfiveseed} & \textbf{\alpharepair} &  \textbf{\selfapr} &  \textbf{\rewardrepair} & \textbf{\recoder} & \textbf{\tbar} & \textbf{\cure} & \textbf{\coconut} & \textbf{\prapr} & \textbf{\dlfix} \\
    \hline
    Chart & 8 & 8 & 9 & 7 & 5 & 10 & 11 & 10 & 7 & 7 & 5 \\
    Closure & 29 & 23 & 23 & 19 &  15 & 21 & 16 & 14 & 9 & 12 & 11\\
    Lang & 19 & 18 & 13 & 10 & 7 & 11 & 13 & 9 & 7 & 6 & 8\\
    Math & 24 & 23 & 21 & 22 & 19 & 18 & 22 & 19 & 16 & 10 & 13\\
    Mockito & 6 & 5 & 5 & 3 & 3 & 2 & 3 & 4 & 4 & 3 & 1 \\
    Time & 3 & 3 & 3 & 3 & 1 & 3 & 3 & 1 & 1 & 3 & 2\\
    \hline
    \textbf{Total} & \textbf{89} & 80 & 74 & 64 & 50 & 65 & 68 & 57 & 44 & 41 & 40\\
    \hline
\end{tabular}}
\end{table*}

\subsubsection{Bugs fixed}
\label{sec:bugs_fixed}
We first compare \ourtech against both traditional and \learning \apr tools on \dfj 1.2. Table~\ref{tab:dfj_result} shows the number of bugs that can be fixed with correct patches by \ourtech and the top baseline tools. In addition to \ourtech, we also include the result of running the base \ctfive separately four times using four different seeds (Column \ctfiveseed). Compared with \ctfiveseed, we observe that our fine-tuned models and prompting strategy is able to provide additional fixes, boosting the number of correct bug fixes from 80 to 89. In total, \textit{\ourtech is able to achieve 89 correct bug fixes on \dfj 1.2 with 15 more fixes than the current state-of-the-art \apr tool.} Figure~\ref{fig:venn}a shows the number of unique bug fixes (that only one technique can exclusively fix while others cannot) generated by \ourtech compared with the top performing \apr baselines and all other tools (Other). We observe that even compared with all previous \apr approaches, \ourtech is able to \emph{provide 16 additional unique fixes that no other \apr tools have been able to fix so far on \dfj 1.2}. 


\begin{figure}
    \includegraphics[width=0.8\linewidth]{figures/venn.pdf}
    \caption{Correct fix Venn diagram on \dfj 1.2}
    \label{fig:venn}
\end{figure}



\begin{figure}
    \includegraphics[width=0.75\linewidth]{figures/unique_fix_examples_prompting.pdf}
    \caption{\idprompting unique patches}
    \label{fig:example_unique_fix_idprompting}
\end{figure}


To illustrate the ability of \ourtech, we show an example fix on a bug (\CodeIn{Closure-71}) in Figure~\ref{fig:example_unique_fix_idprompting}a which cannot be fixed by any previous tools. The fix is to invoke the method of \CodeIn{getJSDocInfo()} which is not only an uncommon method name but also is not seen/used in the surrounding context of the bug. However, this method has been used within the same file as the buggy code where another function initializes a variable called \CodeIn{overridingInfo} also using \CodeIn{getJSDocInfo()}. \idprompting is able to recognize the similarity between the buggy variable name (\CodeIn{isOverride}) and this line to extract the relevant identifier of \CodeIn{getJSDocInfo()} and provide the prompt to tell the model to directly use this identifier to generate the correct patch. This example showcases the plastic surgery hypothesis where patches can often be constructed via reusing code snippets/ingredients from other parts of the project. \ourtech directly leverages this hypothesis by extracting the relevant identifiers from similar code lines within the current file and providing it via natural language prompting to generate the correct fix. 

Another very interesting example bug (\CodeIn{Mockito-27}) fixed by \ourtech that cannot be fixed by previous tools is in Figure~\ref{fig:example_unique_fix_idprompting}b. This bug is fixed by calling the \CodeIn{MethodInterceptorFilter} constructor with a previous setting obtained using \CodeIn{getMockSetting()}. We observe that while the \idprompting did not get the exact identifier of \CodeIn{getMockSetting}, it was able to gather a closely related identifier of \CodeIn{getMockHandler}. Due to their power in code understanding/vectorization, \llm{s} do not have to always generate a patch containing the exact identifier in the prompt and can often just use it as hints or partial code for generation. This example further highlights the unique effect the plastic surgery hypothesis have on \llm-based \apr where the extracted code ingredients do not have to be exactly correct and can serve as guidance for the model to generate the correct patch. 


\begin{table}[!htbp]
\caption{Average correct patch rank on \dfj 1.2}
\label{tab:patch_ranking}
\scalebox{0.75}{
\begin{tabular}{lcccccc|c}
    \hline
    \textbf{Project} & \textbf{Chart} & \textbf{Closure} & \textbf{Lang} & \textbf{Math} & \textbf{Mockito} & \textbf{Time} & \textbf{Average}\\
    \hline
    \ctfiveseed & 221 & 256 & 100 & 142 & 56 & 342 & 186\\
    \ourtech & 26 & 94 & 76 & 145 & 89 & 260 & 115\\
    \hline
    Improvement & 88\% & 63\% & 24\% & -2\% & -59\% & 24\% & 38\%\\
    \hline
\end{tabular}}
\end{table}


\subsubsection{Individual strategy effect} 


\begin{figure}
    \includegraphics[width=0.75\linewidth]{figures/unique_fix_examples.pdf}
    \caption{\epfinetune unique patch}
    \label{fig:example_unique_fix}
\end{figure}

Figure~\ref{fig:venn}b shows the unique bugs fixed by each of the individual strategies in \ourtech. We observe that our three strategies are all able to contribute to providing unique fixes compared with the base \ctfive model (4 from \epfinetune, 2 from \rofinetune and 6 from \idprompting) and boost the overall \ourtech to achieve 89 correct patches. Interestingly, each single strategy is already a strong \apr tool, e.g., the \idprompting strategy can fix 79 bugs by itself, already  outperforming all existing tools. This demonstrates the ability of our three strategies to provide additional fixes that directly applying the base \ctfive cannot provide. Moreover, the base \ctfive model without any changes also produced 2 unique fixes, demonstrating the usefulness/necessity of applying the base model to cover the bugs that may only require general-purpose correct code knowledge. 

Since Section~\ref{sec:bugs_fixed} has shown unique fixes obtained by \idprompting, we now present an example bug fixed by \rofinetune. 
Figure~\ref{fig:example_unique_fix} shows a correct fix example that base \ctfive model cannot fix on \CodeIn{Closure-123}. For this bug, the correct fix is to initialize the variable \CodeIn{rhsContext} by calling a function \CodeIn{getContextForNoInOperator()}. What makes this bug difficult to fix for previous \apr tools is that \CodeIn{getContextForNoInOperator} is a very hard sequence to generate. First, it is not a commonly used function name such as \CodeIn{getContext}. Second, there are no code snippets using this function in the immediate context. As such, previous techniques may fail to generate this patch as it requires specific knowledge about the buggy project in order to come up with this function name. However, this function is used multiple times within the buggy project (in other functions and files). \ourtech leverages this by using \epfinetune strategy to fine-tune a model to predict masked-out tokens within the buggy project. During \epfinetune, the model can learn the usage of this specific function within the buggy project and apply it in this case to produce the correct patch. This domain-specific knowledge cannot be learned just from pre-training on a large amount of open-source code (previous \csapr tools).% 


\subsubsection{Patch ranking} We examine the ability of \ourtech to perform patch ranking in order to prioritize faster validation for correct patches. Similarly, we compare \ourtech against the baseline of running base \ctfive four times with different seeds.% 
Table~\ref{tab:patch_ranking} shows the average rank of the correct patch for the \dfj 1.2 projects on the same set of bugs both \ourtech and \ctfiveseed can fix. We observe that in four out of the six projects, \ourtech provides a better rank on average for the correct patches. On average, using \ourtech, we can achieve a 38\% reduction in the ranking of correct patches. 
\ourtech can learn/use project-specific information to rank correct patches higher since the correct patches often use project-specific identifiers which are less prioritized by the base \ctfive model. In this way, \ourtech not only fixes more bugs, but can also find the correct fixes faster and reduce the computation cost needed for patch validation.  



%

\subsection{RQ2: Detailed Ablation Study}
\label{sec:ablation}

\subsubsection{Impacts of \epfinetune}
\label{sec:ablation_epfinetune}

\begin{table}[htb]
\centering
\caption{Repetition for \epfinetune}
\label{tab:repeat_result}
\scalebox{0.7}{
\begin{tabular}{lcccc}
\hline
\textbf{Strategy} & \makecell{\textbf{\#Corr. / \#Plaus.}\\\textbf{(All)}} & \makecell{\textbf{\#Corr./\#Plaus.}\\\textbf{(New)}} & \makecell{\textbf{Comp.}\\\textbf{Error pct}} & \makecell{\textbf{\#Unique comp.}\\\textbf{per bug}} \\  
\hline
Repetitive (default)       & \textbf{15} / \textbf{30}                                                                                      & \textbf{2} / \textbf{3}                                                                                        & 82\%      & \textbf{138}           \\ 
Non-Repetitive    & 14 / 25                                                                                      & 1 / 2                                                                                        & \textbf{79\%}      & 104            \\ \hline
\end{tabular}}
\end{table}

\begin{table}[htb]
\centering
\caption{Masking rates of \epfinetune}
\label{tab:mask_rate}
\scalebox{0.75}{
\begin{tabular}{lcccc}
\hline
\textbf{Mask Rate} & \makecell{\textbf{\#Corr. / \#Plaus.}\\\textbf{(All)}} & \makecell{\textbf{\#Corr./\#Plaus.}\\\textbf{(New)}} & \makecell{\textbf{Comp.}\\\textbf{Error pct}} & \makecell{\textbf{\#Unique comp.}\\\textbf{per bug}} \\ 
\hline
10\%               & \textbf{16} / 23                                                                      & \textbf{2} / 2                                                                        & 83\%                         & 79            \\ 
20\%               & 14 / 27                                                                      & \textbf{2} / 4                                                                        & 88\%                         & 75            \\ 
30\%               & 14 / 30                                                                      & \textbf{2} / 4                                                                        & 87\%                         & 92            \\ 
40\%               & \textbf{15} / 29                                                                      & \textbf{2} / 4                                                                        & 85\%                         & 102            \\ 
50\% (default)              & \textbf{15} / 30                                                                      & \textbf{2} / 3                                                                        & \textbf{82\%}                & \textbf{138}           \\ 
60\%               & 13 / 26                                                                      & 1 / 4                                                                        & 87\%                         & 106  \\ 
70\%               & 12 / 21                                                                      & 1 / 2                                                                        & 88\%                         & 81            \\ 
80\%               & 9 / 17                                                                       & \textbf{2} / 2                                                                        & 86\%                         & 88            \\ 
90\%               & 7 / 13                                                                       & \textbf{2} / 3                                                                        & 93\%                         & 47            \\ 
\hline
\end{tabular}}
\end{table}

The goal of training using \epfinetune is to incorporate more project-specific knowledge to \ctfive. There are two hyper-parameters of \epfinetune, including \textbf{mask rate} and \textbf{repetition iterations}. Our default setting is to use a 50\% mask rate and 10 repetition times. We conduct an ablation experiment to study the impacts of these two hyper-parameters on the number of correct/plausible patches generated, the number of unique bugs fixed (via correct/plausible patches) when compared to the base \ctfive model, the compilation error rate, and the number of unique compilable patches generated per bug.
% 

We first examine the impact of repeating the masking multiple times during training to generate additional training samples. Table~\ref{tab:repeat_result} shows the results on the Closure bugs with 10 repetition iterations -- generating 10 training sets (Row Repetitive) and no repetition iterations -- generating only 1 training set (Row Non-Repetitive). We observe that the number of total correct and plausible patches produced by the repetitive training approach is higher. Additionally, when we generate new masked training samples during each iteration, the model produced is able to generate two unique bug fixes compared with the base \ctfive model. Similar results can be found when we look at the compilation error rate together with the number of unique compilable patches generated. We see that while the non-repetitive approach has a lower compilation error rate, the number of compilable patches generated is much less. By repeating the masking multiple times during training, we are able to fine-tune the model to learn more project-specific information to produce compilable patches and to fix more unique bugs. %

Next, we study the impacts of different mask rates. In this experiment, we use the default of 10 repetition iterations by generating 10 unique training samples during fine-tuning and examine how different mask rates can have on performance. Due to the extremely large search space (considering unlimited choices of mask rates), we choose mask rates from 10\% to 90\% with an interval of 10\% (9 different mask rates in total). Table~\ref{tab:mask_rate} shows our experimental results on the bugs in the \CodeIn{Closure} project. First, we observe that an extremely high mask rate (70, 80, 90\%) performs poorly in terms of the number of bugs fixed and compilation rate. While a high mask rate may force the model to learn more project-specific tokens during training, each training sample will have a majority of its tokens masked out. Compared with the final repair task of generating a single or partial line, the extremely high mask rate makes the resulting model ill-suited for repair. We observe that the default setting of 50\% mask rate strikes a good balance between achieving the high total number of bugs fixed, more unique bugs fixed compared to base \ctfive, and a relatively low compilation error rate. By using a balanced mask rate of 50\%, the \epfinetune model is able to best complement the base \ctfive in generating more unique bug fixes. 


\subsubsection{Impacts of \rofinetune}
\label{sec:ablation_rofinetune}
\begin{table}[htp]
\centering
\caption{Masking strategies of \rofinetune}
\label{tab:strategy}
\scalebox{0.7}{
\begin{tabular}{lccc}
\hline
\textbf{Strategy} & \textbf{AST masking} & \textbf{Single-line masking} & \textbf{Template masking} \\ 
\hline
\#Corr. / \#Plaus. (All) & \textbf{13} / 25      & 10 / 21              & \textbf{12} / 23         \\ 
\#Corr. / \#Plaus. (New) & 0 / 1        & \textbf{1} / 1                & \textbf{1} / 2             \\ 
Comp. Error pct & 88\% & 86\% & \textbf{76\%}\\
\#Unique comp. per bug & 101 & 129 & \textbf{139} \\
\hline
\end{tabular}}
\end{table}

In addition to looking at the impact of different configurations when using \epfinetune, we study the different ways we can apply \rofinetune. Specifically, we design two additional strategies that can be used during training to produce masked training samples. Table~\ref{tab:strategy} shows the result of our default ``Template masking'', ``AST masking'', and ``Single-line masking'' on bugs in the \CodeIn{Closure} project. Recall that our default setting applies repair templates that we use for \csapr directly on the training data to produce masked lines. AST masking will parse the selected line into an AST and randomly choose a subtree to mask out. On the other hand, single-line masking will simply mask out one entire line in a training sample. We observe that single-line masking performed the worst in terms of the number of correct and plausible patches. This is due to the fact that during repair, we use repair templates that not just regenerate complete lines but also mask out part of the lines. The model just has to regenerate the partial code within the line. Single-line masking is only trained on generating the complete line and thus does not perform well when used with repair templates. Additionally, when compared with AST masking, template masking is able to fix more unique bugs compared with the base \ctfive since it directly leverages the inference repair templates to create training samples. While AST masking makes use of the structure information, it does not fully emulate the inference setting of \csapr. Furthermore, the two baselines both resulted in a lot of patches with high compilation error rates compared with our proposed template masking strategy. Template masking is able to directly learn the types of repair templates that the final repair task will use as input for the model, resulting in fewer compilation failures. By using template masking for \rofinetune, we can train a model that is optimized for the repair task in order to generate more bug fixes to compliment the base \ctfive model, which shows that fine-tuning model with training strategies that assemble the underlying repair techniques is able to further boost its bug-fixing performance.%

\subsubsection{Impacts of \idprompting} 

\begin{table}[htp]
\centering
\caption{Configurations of \idprompting}
\label{tab:idprompting_confg}
    \scalebox{0.75}{
    \begin{tabular}{lcc}
    \hline
    \textbf{Configuration} & \makecell{\textbf{\#Corr. / \#Plaus.}\\\textbf{(All)}} & \makecell{\textbf{\#Corr./\#Plaus.}\\\textbf{(New)}} \\ 
    \hline
    \textbf{Default(top-5 current-file type separate)} & \textbf{25 / 39} & \textbf{3 / 3}\\
    \hline 
    Top-1 & 24 / 37 & 1 / 1\\
    Top-10 & 23 / 38 & 1 / 1\\
    Top-20 & 23 / 37 & 1 / 1\\
    \hline 
    Full project & 23 / 36 & 1 / 1\\
    No type & 24 / 37 & 2 / 2\\
    Together & 24 / 40 & 1 / 1\\
    \hline
    \end{tabular}}
\end{table}


We examine the different parameters of our \idprompting strategy. Table~\ref{tab:idprompting_confg} shows the results of our default (Row Default) approach and other configurations on \CodeIn{Closure} project. We first look at the effect of varying the top \(N\) identifiers and observe that when only considering the top-1 identifier for each bug we do not generate more correct fixes since it is unlikely that the relevant identifier to fix the bug always has the highest ranking. On the flip side, considering a larger amount of identifiers per bug (top-10, top-20) is also not desirable since we limit the model to sample only 5000 times per bug, and generating more prompts will decrease the number of samples per each prompt. Next, we look at the scope of the project where we find relevant tokens. Our default setting considers only the current file of the bug and we compare this to when we consider the full project (i.e., changing Line~\ref{algo:extractcodelines} of Algorithm~\ref{algo:ri_strategy} to consider all files within the project). We observe that the number of correct and plausible fixes decreases which reflects a similar finding from prior work~\cite{barr2014plastic} where a significant amount of correct fixing ingredients (relevant identifiers) can already be found within the same file. Furthermore, by considering the entire project, we could introduce more noise where potentially irrelevant identifiers could be highly ranked. Following, we compare the effect of having type information of the identifier in the prompt. We observe that our default setting (with type) is able to generate more correct fixes compared to without types, indicating the usefulness of such information in helping the model generate the correct usage of the identifier in the patch. Finally, we examine our default prompting method of only providing one relevant identifier at a time. We compare this against another approach to include all the top 5 relevant tokens in the same prompt. We see that separating each relevant identifier to its own prompt provides us with more fixes as including all identifiers together can potentially confuse the model. 

\subsubsection{Overhead of \ourtech}

As \ourtech proposes to fine-tune two separate models along with prompting via information retrieval and static analysis, we investigate the extra overhead of using \ourtech compared to just using the base \ctfive. Recall that \ourtech only fine-tunes the model on the oldest version of the project for \dfj 1.2 (one-time cost) and uses the trained models to generate patches for all bugs within that project. We find that on average, for each bug in \CodeIn{Closure}, \ourtech adds 14.3 minutes (6.6 for each fine-tuning strategy and 1.0 for prompting strategy compared with directly using the base \ctfive model. This shows that overall, \ourtech adds a minimal amount of overhead to the repair process (still within the 5-hour limit including overhead). For practical use, the fine-tuning steps can be done ahead of the actual repair task (e.g., periodically during nights or weekends), incurring no additional time cost compared to previous \llm-based \apr tools. Developers can then apply the fine-tuned models together with the base model whenever a bug is detected.












\subsection{RQ3: Generalizability of \ourtech}


\begin{table}[!htbp]
\centering
\caption{Evaluation results of correct fixes on \dfj 2.0}
\label{tab:dfj2_result}
\scalebox{0.8}{
\begin{tabular}{ccccccc}
    \hline
    \makecell{\textbf{\ourtech}} & \textbf{\ctfiveseed} & \makecell{\textbf{Alpha}\\\textbf{Repair}} & \textbf{\selfapr} & \makecell{\textbf{Reward}\\\textbf{Repair}} & \textbf{\recoder} & \textbf{\tbar}\\
    \hline
     44 & 42 & 36 & 31 & 25 & 11 & 8\\
    \hline
\end{tabular}}
\end{table}

\begin{figure}
    \includegraphics[width=0.75\linewidth]{figures/unique_fix_examples_dfj2.pdf}
    \caption{\rofinetune unique patch}
    \label{fig:example_unique_fix_dfj}
\end{figure}

We further evaluate the generalizability of \ourtech on an additional repair dataset of \dfj 2.0 containing new bugs and projects. Table~\ref{tab:dfj2_result} shows the number of correct bug fixes on single-line bugs in \dfj 2.0. We observe that \ourtech is able to achieve the state-of-the-art with the highest number of correctly fixed bugs of 44 (8 more than the best-performing baseline). Unlike other \nmt-based or traditional \template \apr tools, \ourtech does not suffer from the dataset overfitting issue of only performing well on the base \dfj 1.2 dataset. In fact, the relative improvement in the total number of bugs fixed is higher on \dfj 2.0 (22.2\% increase) compared to 1.2 (20.3\% increase). Furthermore, comparing against the baseline (Column \ctfiveseed), \ourtech is able to improve the number of total bug fixes from 42 to 44 and produce 3 unique bug fixes. 

Figure~\ref{fig:example_unique_fix_dfj} shows a bug (\CodeIn{Codec-3}) fixed by \ourtech but cannot be fixed by any other studied \apr tool. The root cause of this bug is an off-by-one error on the second argument in the function call. While this bug looks very simple to fix, one reason previous \learning \apr was not able to provide a correct fix could be the unconventional values of "4" and "3". During the training, \nmt-based \apr can learn from bug-fixing datasets where it is common to use swap a "0" to a "1" and vice-versa to fix a bug. However, changing a "4" to a "3" can be uncommon in the bug-fixing dataset. \Csapr tool that directly leverages \plm{s} can also have a hard time on this bug since the change is very small even if a direct repair template can be applied. Since this change is very small, the \llm should not add any additional code other than changing "4" to "3". However, during training one single mask span usually represents multiple different tokens, which may cause the base \ctfive model to generate more tokens than needed. Using \ourtech and specifically the \rofinetune strategy, the fine-tuned \ctfive can learn such short code generation that usually stems from repair templates such as argument replacement. As such, \ourtech is able to generate this simple patch to fix the underlying bug.%


