\section{Threats To Validity}

\mypara{Internal.} Our manual examination in determining the correct patches, semantically equivalent to reference developer patches, from plausible patches is one internal threat to validity. Following common \apr practice, the first two authors perform a careful analysis of each plausible patch along with multiple discussions to determine the correctness of a patch.%

Another internal threat to validity comes from using the \ctfive model which is trained on open-source GitHub code snippets~\cite{husain2020codesearchnet}. This means the training data could overlap with our evaluation repair dataset of \dfj. To address this, we follow prior work~\cite{xia2022alpharepair} and compute the number of patched functions by \ourtech that also exist in the pre-training dataset. In total, out of the 89 bugs fixed on \dfj 1.2, 13 of these fixes are part of the original pre-training dataset of \ctfive. This shows that the majority of the correct fixes (76/89 = 85\%) do not contain any reference developer patch in the training data. Furthermore, for a fair comparison, if we exclude the 13 bugs whose patched functions overlap with the \ctfive pre-training dataset following prior work~\cite{xia2022alpharepair}, we are still able to achieve state-of-the-art performance on \dfj 1.2 with 76 total fixes compared to 67 from the best-performing baseline on the remaining bugs. This shows that \ourtech is not simply performing well on the datasets due to the developer reference patches that the model saw during pre-training. Similarly, on \dfj 2.0, we found that 6 fixed bugs have their reference patch function within the training dataset. Applying the same removal comparison, we still achieve the state-of-the-art result of 38 compared to the best-performing baseline of 30 on \dfj 2.0. Additionally, we also demonstrate that regardless of the overlap between the training and evaluation datasets, by combining project-specific fine-tuning and prompting strategies, we can further improve the performance of the base \llm. Future work to completely address this threat would need to retrain the \ctfive model from scratch after removing the overlapping functions. 





\mypara{External.} The major external threat to validity comes from our evaluation dataset. The performance achieved by \ourtech may not generalize well to other datasets. To address this, we use two different versions of \dfj, namely 1.2 and 2.0, and demonstrated that \ourtech is able to achieve state-of-the-art results on both datasets. In the future, we plan on continuing to address this threat by performing more evaluations on other repair datasets across multiple programming languages. 