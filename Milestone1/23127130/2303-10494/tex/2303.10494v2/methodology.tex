\section{Experimental Design}
\subsection{Research Questions}
In this paper, we study the following research questions:
\begin{itemize}[noitemsep, leftmargin=*, topsep=0pt]
	\item \textbf{RQ1}: How does \ourtech compare against the state-of-the-art \apr tools?
	\item \textbf{RQ2}: What is the impact of different configurations of \ourtech{}?
	\item \textbf{RQ3}: How does \ourtech generalize in fixing additional bugs from different projects? 
\end{itemize}

We first demonstrate the repair effectiveness of \ourtech against state-of-the-art \apr tools on the popular \dfj 1.2~\cite{just2014dfj} dataset. We study not only the number of bugs fixed in total but also the number of unique bugs fixed compared with previous techniques. Furthermore, we analyze the improvement in terms of patch ranking -- to validate correct patches faster when using \ourtech. Next, we conduct an extensive ablation study on the different configurations of both our two fine-tuning strategies and one prompting strategy. Due to the time cost to train multiple models and generate patches, for the ablation study, we focus on the \CodeIn{Closure} project in \dfj 1.2 which is the largest in terms of both the number of bugs and the size of source code base. Finally, following prior work~\cite{xia2022alpharepair, xia2023repairstudy}, we evaluate against the state-of-the-art \apr tools on the \dfj 2.0~\cite{just2014dfj} dataset to illustrate that \ourtech is not simply overfitting to the 1.2 version.


\subsection{Implementation}
\ourtech is implemented in Python using the PyTorch~\cite{PyTorchWebPage} implementation of the \ctfive model from Hugging Face~\cite{HuggingFaceWebPage}. Our fine-tuning method is based on the pre-trained CodeT5-large (770M) checkpoint. We use JavaParser~\cite{javaparser} to perform static analysis of filtering inaccessible identifiers in scope.  For both \epfinetune and \rofinetune, we repeat the fine-tuning process for 10 iterations by default to augment our fine-tuning dataset. We fine-tune the \ctfive model on an NVIDIA RTX A6000 with 48GB memory using FP16. We use the following set of hyper-parameters to train models: 32 batch size and 1e-4 learning rate with 15K training steps. We use Adam optimizer~\cite{ba2014adam} to update the parameters and use a linear learning rate scheduler with 10\% warmup proportion. For both fine-tuning strategies, we extract the oldest version of the buggy project for training and use the fine-tuned models to generate patches for all bugs in that project. For \idprompting, we use the top 5 most relevant identifiers. For repair, we sample each model 5000 times and validate the top 1000 unique patches produced by each model (at most 4000 patches in total per bug) which is comparable to other baselines. To generate more unique patches for each sample, we use nucleus sampling with top \(p\) of 1 and temperature of 1. We validate the patches on a workstation using AMD Ryzen Threadripper PRO 3975WX CPU with 32-Cores and 256 GB RAM, running Ubuntu 20.04.5 LTS. Similar to prior work~\cite{xia2022alpharepair, jiang2021cure, li2020dlfix}, we use an end-to-end time limit of 5 hours to fix one bug. Note that we sum up the time spent on each of our four processes as \ourtech time cost for fair comparison.%



\subsection{Subject Systems}
We use the widely studied benchmark of \dfj~\cite{just2014dfj} -- a collection of open-source bugs found across 15 different projects to evaluate \ourtech. We follow prior work and separate the dataset into \dfj 1.2 and \dfj 2.0. \dfj 1.2 contains 391 bugs (removing 4 depreciated bugs) across 6 different projects and \dfj 2.0 contains 438 bugs across 9 additional projects. While evaluating \ourtech on all 391 bugs in \dfj 1.2, we follow prior work~\cite{xia2022alpharepair} and choose only the 82 single-line bugs in \dfj 2.0 for evaluation (since existing \learning \apr mainly target single-line fixes). 

%




\subsection{Compared Techniques}
We compare \ourtech against 20 different \apr tools including both state-of-the-art \learning and traditional \apr tools. We choose 8 recent \learning \apr tools for comparison: \alpharepair~\cite{xia2022alpharepair}, \selfapr~\cite{ye2022selfapr}, \rewardrepair~\cite{ye2022rewardrepair}, \recoder~\cite{zhu2021recoder}, \cure~\cite{jiang2021cure}, \coconut~\cite{lutellier2020coconut}, \dlfix~\cite{li2020dlfix} and \sequencer~\cite{chen2018sequencer}. \alpharepair is a recently proposed and state-of-the-art \csapr tool that directly uses a pre-trained model with accessible training set (\codebert). Additionally, we also compare against 12 representative traditional \apr tools: \tbar~\cite{liu2019tbar}, \prapr~\cite{ghanbari2019prapr}, \avatar~\cite{liu2019avatar}, \simfix~\cite{jiang2018simfix}, \fixminer~\cite{koyuncu2020fixminder}, \capgen~\cite{wen2018capgen}, \jaid~\cite{chen2017jaid}, \sketchfix~\cite{hua2018sketchfix}, \nopol~\cite{demacro2014nopol}, \jgenprog~\cite{martinez2015automatic}, \jmutrepair~\cite{martinez2016astor}, and \jkali~\cite{martinez2016astor}. Finally, since \ourtech proposes to combine the base \ctfive (with and without prompting) with the two fine-tuned models, we also compare against a baseline where we run the base \ctfive four times with four random different seeds. This is a fair and necessary baseline to compare against as \ctfive can produce different sampling outputs depending on the random seed and a developer who wishes to use our approach of combining the four models together may also allocate the same GPU resource to run \ctfive four times as well. We refer to this baseline in our evaluation as \ctfiveseed. 


We evaluate against these baselines on \textit{perfect fault localization} setting, where the ground-truth location of each bug is provided to the repair tool by comparing the reference developer patch with the buggy code. This is the preferred evaluation setting~\cite{lutellier2020coconut, jiang2021cure, zhu2021recoder, tufano2018empstudy} as it eliminates any result differences caused by using different fault localization techniques~\cite{wong2016fl}. We use the standard metrics for \apr comparison of \textit{plausible patches} -- pass the entire test suite and \textit{correct patches} -- semantically equivalent to the reference developer fix. Following all prior \apr work, correct patches are determined by manually inspecting each plausible patch. Also, following common practice in \apr, we directly report the number of correct and plausible bug fix results from previous studies~\cite{xia2022alpharepair, ghanbari2019prapr, liu2019tbar, zhu2021recoder, ye2022rewardrepair}.