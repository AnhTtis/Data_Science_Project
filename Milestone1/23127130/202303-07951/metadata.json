{
    "arxiv_id": "2303.07951",
    "paper_title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
    "authors": [
        "Maorong Wang",
        "Ling Xiao",
        "Toshihiko Yamasaki"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Online knowledge distillation (KD) has received increasing attention in\nrecent years. However, while most existing online KD methods focus on\ndeveloping complicated model structures and training strategies to improve the\ndistillation of high-level knowledge like probability distribution, the effects\nof the multi-level knowledge in the online KD are greatly overlooked,\nespecially the low-level knowledge. Thus, to provide a novel viewpoint to\nonline KD, we propose MetaMixer, a regularization strategy that can strengthen\nthe distillation by combining the low-level knowledge that impacts the\nlocalization capability of the networks, and high-level knowledge that focuses\non the whole image. Experiments under different conditions show that MetaMixer\ncan achieve significant performance gains over state-of-the-art methods.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07951v1"
    ],
    "publication_venue": "10 pages, 4 figures"
}