@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Crime and punishment in scientific research}, 
      author={Mathieu Bouville},
      year={2008},
      eprint={0803.4058},
      archivePrefix={arXiv},
      primaryClass={physics.soc-ph}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}

% MY BIB
@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1008--1014},
  year={2000},
  organization={Citeseer}
}

@article{flet2021adversarially,
  title={Adversarially Guided Actor-Critic},
  author={Flet-Berliac, Yannis and Ferret, Johan and Pietquin, Olivier and Preux, Philippe and Geist, Matthieu},
  journal={ICLR 2021-Ninth International Conference on Learning Representations},
  year={2021}
}

@article{liu2021deceptive,
  title={Deceptive Reinforcement Learning for Privacy-Preserving Planning},
  author={Liu, Zhengshang and Yang, Yue and Miller, Tim and Masters, Peta},
  journal={Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  year={2021}
}

@inproceedings{masters2017deceptive,
  title={Deceptive Path-Planning},
  author={Masters, Peta and Sardina, Sebastian},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={4368--4375},
  year={2017}
}

@inproceedings{kulkarni2018resource,
  title={Resource Bounded Secure Goal Obfuscation},
  author={Kulkarni, Anagha and Klenk, Matthew and Rane, Shantanu and Soroush, Hamed},
  booktitle={AAAI Fall Symposium on Integrating Planning, Diagnosis and Causal Reasoning},
  year={2018}
}

@article{bell2003toward,
  title={Toward a Theory of Deception},
  author={Bell, J Bowyer},
  journal={International Journal of Intelligence and Counterintelligence},
  volume={16},
  number={2},
  pages={244--279},
  year={2003},
  publisher={Taylor \& Francis}
}

@inproceedings{masters2019goal,
  title={Goal recognition for rational and irrational agents},
  author={Masters, Peta and Sardina, Sebastian},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={440--448},
  year={2019}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for Inverse Reinforcement Learning},
  author={Ng, Andrew Y and Russell, Stuart},
  booktitle={Proceedings 17th International Conference on Machine Learning},
  year={2000},
  organization={Citeseer}
}

@inproceedings{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation.},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={99},
  pages={1057--1063},
  year={1999},
  organization={Citeseer}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@incollection{littman1994markov,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine learning proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@inproceedings{perolat2017multi,
  title={A multi-agent reinforcement learning model of common-pool resource appropriation},
  author={Perolat, Julien and Leibo, Joel Z and Zambaldi, Vinicius and Beattie, Charles and Tuyls, Karl and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3643--3652},
  year={2017}
}

@article{leibo2017multi,
  title={Multi-agent reinforcement learning in sequential social dilemmas},
  author={Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  journal={arXiv preprint arXiv:1702.03037},
  year={2017}
}

@inproceedings{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, OpenAI Pieter and Mordatch, Igor},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6379--6390},
  year={2017}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{nguyen2020deep,
  title={Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications},
  author={Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
  journal={IEEE Transactions on Cybernetics},
  year={2020},
  publisher={IEEE}
}

@phdthesis{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={King's College, Cambridge}
}

@inproceedings{foerster2017stabilising,
  title={Stabilising experience replay for deep multi-agent reinforcement learning},
  author={Foerster, Jakob and others},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1146--1155},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{tesauro2004extending,
  title={Extending Q-learning to general adaptive multi-agent systems},
  author={Tesauro, Gerald},
  booktitle={Advances in Neural Information Processing Systems},
  pages={871--878},
  year={2004}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  year={2014}
}

@article{li2019transforming,
  title={Transforming cooling optimization for green data center via deep reinforcement learning},
  author={Li, Yuanlong and Wen, Yonggang and Tao, Dacheng and Guan, Kyle},
  journal={IEEE transactions on cybernetics},
  year={2019},
  publisher={IEEE}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and others},
  journal={Human Interaction, Emerging Technologies and Future Applications II},
  year={2015}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and others},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@inproceedings{hu1998online,
  title={Online learning about other agents in a dynamic multiagent system},
  author={Hu, Junling and Wellman, Michael P},
  booktitle={Proceedings of the second international conference on Autonomous agents},
  pages={239--246},
  year={1998}
}
@article{rashid2020monotonic,
  title={Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  author={Rashid, Tabish and others},
  journal={arXiv preprint arXiv:2003.08839},
  year={2020}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in {StarCraft II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{ornik2018deception,
  title={Deception in optimal control},
  author={Ornik, Melkior and Topcu, Ufuk},
  booktitle={2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={821--828},
  year={2018},
  organization={IEEE}
}

@article{whaley1982toward,
  title={Toward a general theory of deception},
  author={Whaley, Barton},
  journal={The Journal of Strategic Studies},
  volume={5},
  number={1},
  pages={178--192},
  year={1982},
  publisher={Taylor \& Francis}
}

@article{turing1950mind,
  title={Mind},
  author={Turing, AM},
  journal={Mind},
  volume={59},
  number={236},
  pages={433--460},
  year={1950}
}

@inproceedings{rowe2003counterplanning,
  title={Counterplanning deceptions to foil cyber-attack plans},
  author={Rowe, Neil C},
  booktitle={IEEE Systems, Man and Cybernetics Society Information Assurance Workshop, 2003.},
  pages={203--210},
  year={2003},
  organization={IEEE}
}

@inproceedings{shim2012biologically,
  title={Biologically-inspired deceptive behavior for a robot},
  author={Shim, Jaeeun and Arkin, Ronald C},
  booktitle={International Conference on Simulation of Adaptive Behavior},
  pages={401--411},
  year={2012},
  organization={Springer}
}

@inproceedings{kulkarni2019unified,
  title={A unified framework for planning in adversarial and cooperative environments},
  author={Kulkarni, Anagha and Srivastava, Siddharth and Kambhampati, Subbarao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={2479--2487},
  year={2019}
}

@inproceedings{ramirez2010probabilistic,
  title={Probabilistic plan recognition using off-the-shelf classical planners},
  author={Ram{\'\i}rez, Miguel and Geffner, Hector},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={24},
  number={1},
  year={2010}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{haarnoja2018soft2,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{arulkumaran2017brief,
  title={A brief survey of deep reinforcement learning},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={arXiv preprint arXiv:1708.05866},
  year={2017}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{jordan2020evaluating,
  title={Evaluating the Performance of Reinforcement Learning Algorithms},
  author={Jordan, Scott and Chandak, Yash and Cohen, Daniel and Zhang, Mengxue and Thomas, Philip},
  booktitle={International Conference on Machine Learning},
  pages={4962--4973},
  year={2020},
  organization={PMLR}
}

@article{whiteson2011introduction,
  title={Introduction to the special issue on empirical evaluations in reinforcement learning},
  author={Whiteson, Shimon and Littman, Michael L},
  journal={Machine Learning},
  volume={84},
  number={1-2},
  pages={1},
  year={2011},
  publisher={Springer Nature BV}
}

@article{vamplew2011empirical,
  title={Empirical evaluation methods for multiobjective reinforcement learning algorithms},
  author={Vamplew, Peter and Dazeley, Richard and Berry, Adam and Issabekov, Rustam and Dekker, Evan},
  journal={Machine learning},
  volume={84},
  number={1},
  pages={51--80},
  year={2011},
  publisher={Springer}
}

@inproceedings{van2007reinforcement,
  title={Reinforcement learning in continuous action spaces},
  author={Van Hasselt, Hado and Wiering, Marco A},
  booktitle={2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
  pages={272--279},
  year={2007},
  organization={IEEE}
}

@inproceedings{theodorou2010reinforcement,
  title={Reinforcement learning of motor skills in high dimensions: A path integral approach},
  author={Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  booktitle={2010 IEEE International Conference on Robotics and Automation},
  pages={2397--2403},
  year={2010},
  organization={IEEE}
}

@article{masip2004defining,
  title={Defining deception},
  author={Masip, Jaume and Garrido, Eugenio and Herrero, Carmen},
  journal={Anales de psicologia},
  year={2004},
  publisher={Murcia: Universidad de Murcia, Servicio de Publicaciones}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell System Technical Journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{ramirez2009plan,
  title={Plan recognition as planning},
  author={Ram{\'\i}rez, Miquel and Geffner, Hector},
  booktitle={25th International Joint Conference on Artificial Intelligence},
  year={2009}
}

@article{sarkadi2020deceptive,
  title={Deceptive autonomous agents},
  author={Sarkadi, Stefan},
  year={2020},
  publisher={Cranfield Online Research Data (CORD)}
}

@article{arora2021survey,
  title={A survey of inverse reinforcement learning: Challenges, methods and progress},
  author={Arora, Saurabh and Doshi, Prashant},
  journal={Artificial Intelligence},
  pages={103500},
  year={2021},
  publisher={Elsevier}
}


@INPROCEEDINGS{Fujimoto2018-dj,
  title     = "Addressing Function Approximation Error in {Actor-Critic}
               Methods",
  booktitle = "Proceedings of the 35th International Conference on Machine
               Learning",
  author    = "Fujimoto, Scott and van Hoof, Herke and Meger, David",
  abstract  = "In value-based reinforcement learning methods such as deep
               Q-learning, function approximation errors are known to lead to
               overestimated value estimates and suboptimal policies. We show
               that this problem persists in an actor-critic setting and
               propose novel mechanisms to minimize its effects on both the
               actor and the critic. Our algorithm builds on Double Q-learning,
               by taking the minimum value between a pair of critics to limit
               overestimation. We draw the connection between target networks
               and overestimation bias, and suggest delaying policy updates to
               reduce per-update error and further improve performance. We
               evaluate our method on the suite of OpenAI gym tasks,
               outperforming the state of the art in every environment tested.",
  publisher = "PMLR",
  volume    =  80,
  pages     = "1587--1596",
  series    = "Proceedings of Machine Learning Research",
  year      =  2018
}

@INPROCEEDINGS{Thrun1993-ik,
  title       = "Issues in using function approximation for reinforcement
                 learning",
  booktitle   = "Proceedings of the Fourth Connectionist Models Summer School",
  author      = "Thrun, Sebastian and Schwartz, Anton",
  abstract    = "Reinforcement learning techniques address the problem of
                 learning to select actions in unknown, dynamic environments.
                 It is widely acknowledged that to be of use in complex
                 domains, reinforcement learning techniques must be combined
                 with generalizing function approximation methods such as
                 artificial neural networks. Little, however, is understood
                 about the theoretical properties of such combinations, and
                 many researchers have encountered failures in practice. In
                 this paper we identify a prime source of such failures …",
  publisher   = "books.google.com",
  pages       = "255--263",
  institution = "Hillsdale, NJ",
  year        =  1993
}


@ARTICLE{Van_Hasselt2016-zz,
  title     = "Deep reinforcement learning with Double Q-learning",
  author    = "van Hasselt, Hado and Guez, Arthur and Silver, David",
  abstract  = "The popular Q-learning algorithm is known to overestimate action
               values under certain conditions. It was not previously known
               whether, in practice, such overestimations are common, whether
               they harm performance, and whether they can generally be
               prevented. In this paper, we answer all these questions
               affirmatively. In particular, we first show that the recent DQN
               algorithm, which combines Q-learning with a deep neural network,
               suffers from substantial overestimations in some games in the
               Atari 2600 domain. We then show that the idea behind the Double
               Q-learning algorithm, which was introduced in a tabular setting,
               can be generalized to work with large-scale function
               approximation. We propose a specific adaptation to the DQN
               algorithm and show that the resulting algorithm not only reduces
               the observed overestimations, as hypothesized, but that this
               also leads to much better performance on several games.",
  journal   = "Proc. Conf. AAAI Artif. Intell.",
  publisher = "ojs.aaai.org",
  volume    =  30,
  number    =  1,
  month     =  mar,
  year      =  2016,
  language  = "en"
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{lee2019target,
  title={Target-based temporal-difference learning},
  author={Lee, Donghwan and He, Niao},
  booktitle={International Conference on Machine Learning},
  pages={3713--3722},
  year={2019},
  organization={PMLR}
}

@article{zhang2017deeper,
  title={A deeper look at experience replay},
  author={Zhang, Shangtong and Sutton, Richard S},
  journal={arXiv preprint arXiv:1712.01275},
  year={2017}
}


@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@article{savas2021deceptive,
  title={Deceptive Decision-Making Under Uncertainty},
  author={Savas, Yagiz and Verginis, Christos K and Topcu, Ufuk},
  journal={arXiv preprint arXiv:2109.06740},
  year={2021}
}

@article{liu2014multiobjective,
  title={Multiobjective reinforcement learning: A comprehensive overview},
  author={Liu, Chunming and Xu, Xin and Hu, Dewen},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={45},
  number={3},
  pages={385--398},
  year={2014},
  publisher={IEEE}
}

@article{nguyen2020multi,
  title={A multi-objective deep reinforcement learning framework},
  author={Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Vamplew, Peter and Nahavandi, Saeid and Dazeley, Richard and Lim, Chee Peng},
  journal={Engineering Applications of Artificial Intelligence},
  volume={96},
  pages={103915},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{masters2017cost,
  title={Cost-based goal recognition for path-planning},
  author={Masters, Peta and Sardina, Sebastian},
  booktitle={Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
  pages={750--758},
  year={2017}
}

@ARTICLE{Vered2017-oa,
  title    = "Heuristic Online Goal Recognition in Continuous Domains",
  author   = "Vered, Mor and Kaminka, Gal A",
  abstract = "Goal recognition is the problem of inferring the goal of an
              agent, based on its observed actions. An inspiring
              approach---plan recognition by planning (PRP)---uses
              off-the-shelf planners to dynamically generate plans for given
              goals, eliminating the need for the …",
  journal  = "ijcai.org",
  year     =  2017
}

@article{huang2021reinforcement,
  title={Reinforcement learning for feedback-enabled cyber resilience},
  author={Huang, Yunhan and Huang, Linan and Zhu, Quanyan},
  journal={arXiv preprint arXiv:2107.00783},
  year={2021}
}

@inproceedings{gall2021active, 
  title     = {Active Goal Recognition Design}, 
  author    = {Gall, Kevin C. and Ruml, Wheeler and Keren, Sarah}, 
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}}, 
  publisher = {International Joint Conferences on Artificial Intelligence Organization}, 
  editor    = {Zhi-Hua Zhou}, 
  pages     = {4062--4068}, 
  year      = {2021}, 
  month     = {8}, 
  doi       = {10.24963/ijcai.2021/559}, 
  url       = {https://doi.org/10.24963/ijcai.2021/559}, 
}

@article{jumper2021alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{yu2018towards,
  title={Towards Sample Efficient Reinforcement Learning.},
  author={Yu, Yang},
  booktitle={International Joint Conference on Artificial Intelligence},
  pages={5739--5743},
  year={2018}
}

@inproceedings{shim2013taxonomy,
  title={A taxonomy of robot deception and its benefits in HRI},
  author={Shim, Jaeeun and Arkin, Ronald C},
  booktitle={2013 IEEE International Conference on Systems, Man, and Cybernetics},
  pages={2328--2335},
  year={2013},
  organization={IEEE}
}

@article{henderson2017multitask,
   author = {{Henderson}, P. and {Chang}, W.-D. and {Shkurti}, F. and {Hansen}, J. and 
	{Meger}, D. and {Dudek}, G.},
    title = {Benchmark Environments for Multitask Learning in Continuous Domains},
  journal = {ICML Lifelong Learning: A Reinforcement Learning Approach Workshop},
     year={2017}
}

@article{nichols2022adversarial,
  title={Adversarial Sampling-Based Motion Planning},
  author={Nichols, Hayden and Jimenez, Mark and Goddard, Zachary and Sparapany, Michael and Boots, Byron and Mazumdar, Anirban},
  journal={IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE}
}

@article{roijers2013survey,
  title={A survey of multi-objective sequential decision-making},
  author={Roijers, Diederik M and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
  journal={Journal of Artificial Intelligence Research},
  volume={48},
  pages={67--113},
  year={2013}
}

@inproceedings{gabor1998multi,
  title={Multi-criteria reinforcement learning.},
  author={G{\'a}bor, Zolt{\'a}n and Kalm{\'a}r, Zsolt and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Machine Learning},
  volume={98},
  pages={197--205},
  year={1998},
  organization={Citeseer}
}

@article{boden2017principles,
  title={Principles of robotics: regulating robots in the real world},
  author={Boden, Margaret and Bryson, Joanna and Caldwell, Darwin and Dautenhahn, Kerstin and Edwards, Lilian and Kember, Sarah and Newman, Paul and Parry, Vivienne and Pegman, Geoff and Rodden, Tom and others},
  journal={Connection Science},
  volume={29},
  number={2},
  pages={124--129},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{danaher2020robot,
  title={Robot Betrayal: a guide to the ethics of robotic deception},
  author={Danaher, John},
  journal={Ethics and Information Technology},
  volume={22},
  number={2},
  pages={117--128},
  year={2020},
  publisher={Springer}
}

@book{dignum2019responsible,
  title={Responsible artificial intelligence: how to develop and use AI in a responsible way},
  author={Dignum, Virginia},
  year={2019},
  publisher={Springer Nature}
}


@article{sarkadi2019modelling,
  title={Modelling deception using theory of mind in multi-agent systems},
  author={Sarkadi, {\c{S}}tefan and Panisson, Alison R and Bordini, Rafael H and McBurney, Peter and Parsons, Simon and Chapman, Martin},
  journal={AI Communications},
  volume={32},
  number={4},
  pages={287--302},
  year={2019},
  publisher={IOS Press}
}

@inproceedings{adar2013benevolent,
  title={Benevolent deception in human computer interaction},
  author={Adar, Eytan and Tan, Desney S and Teevan, Jaime},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1863--1872},
  year={2013}
}