\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

% my package
\usepackage{booktabs}  % for \toprule \midrule \bottomrule
\usepackage{algorithm}
\usepackage{cite} % \cite{bibtex1，bibtex2，bibtex3} --> [1]-[3]

\usepackage{hyperref}
\hypersetup{
		hypertex=true, 
		colorlinks=true, 
		linkcolor=blue, 
		anchorcolor=blue, 
		citecolor=blue
	}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2017}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS (February 2017)}
\begin{document}
\title{Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning}
\author{Xiuding Cai, Jiao Chen, Yaoyao Zhu, Beiming Wang, Yu Yao
	\thanks{Xiuding Cai, Yaoyao Zhu, Beiming Wang and Yu Yao are with Chengdu Institute of Computer Application, Chinese Academy of Sciences, Chengdu, China, and with the School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China (e-mail: \{caixiuding20, zhuyaoyao19, wangbeiming21\}@mails.ucas.ac.cn, casitmed2022@163.com). }
	\thanks{Jiao Chen was with Department of Anesthesiology, West China Hospital, Sichuan University \& The Research Units of West China (2018RU012), and with Chinese Academy of Medical Sciences, China (e-mail: chenjiao@wcuscu.cn).}}

\maketitle

\begin{abstract}
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated by extensive experiments on a real clinical anesthesia dataset. Experimental results show that PCQL is predicted to achieve higher gains than the baseline approach while maintaining good agreement with the reference dose given by the anesthesiologist, using less total dose, and being more responsive to the patient's vital signs. In addition, the confidence intervals of the agent were investigated, which were able to cover most of the clinical decisions of the anesthesiologist. Finally, an interpretable method, SHAP, was used to analyze the contributing components of the model predictions to increase the transparency of the model.
\end{abstract}

\begin{IEEEkeywords}
Anesthesia, offline reinforcement learning (ORL), anesthetic administration, propofol.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{A}{nesthesia} is a critical part of the operating room, with millions of patients requiring general anesthesia during surgery each year~\cite{weiser2008estimation}. Anesthesiologists face tremendous work pressure every day. They must monitor the patient throughout the procedure while maintaining several aspects of the patient simultaneously, including anesthesia status, physiological stability, pain management, and oxygen delivery. Routinely, an anesthesiologist is responsible for the life support of more than one patient at a time. However, in recent years, the growing number of operations has posed a huge challenge to the slow-growing position of anesthesiologists~\cite{kempthorne2017wfsa}. This serious challenge not only puts great pressure on anesthesiologists, but also increases the risk of medical malpractice and burnout~\cite{afonso2021burnout}. Given the vital role of anesthesia, the repetitive tasks faced by anesthesiologists, and the shortage of positions, automated anesthesia infusion has become an important research direction in healthcare and promises to alleviate this challenge. Automatic anesthetic administration has also been shown to allow for more accurate and responsive anesthetic drug control with a reduced total dose than complete manual control by the anesthesiologist~\cite{puri2016multicenter, brogi2017clinical, zaouter2020autonomous, ghita2020closed}. This also helps reduce the patient's postoperative recovery time, as high doses are currently known to substantially increase the likelihood of side effects, such as propofol infusion syndrome~\cite{pasin2017closed}.

Reinforcement learning (RL), a subfield of machine learning, aims to solve sequential decision problems. RL interacts with the environment by creating an agent that obtains observational states and rewards from the environment to adjust the policy to maximize cumulative rewards and further achieve optimal control. RL has been widely applied and successful in recent years in healthcare, including breast cancer screening~\cite{yala2022optimizing}, sepsis treatment~\cite{raghu2017continuous}, glioblastoma treatment~\cite{zade2020reinforcement}, diabetes glucose control~\cite{ngo2018reinforcement}, etc. Anesthesia infusion can be viewed as a decision-making process over time, in which the anesthesiologist selects the optimal combination of drug dosages based on the patient's clinical information, as well as current physical status (\emph{e.g.}, blood pressure, heart rate, depth of anesthesia, etc.) to maintain the patient's vital signs within the target interval. Therefore, the anesthesia infusion problem is naturally amenable to modeling using RL, and there is a promise for using RL for more efficient and effective anesthesia administration. 

However, traditional RL learns optimal policies by interacting with the environment through trial and error (See Fig.~\ref{fig:rl_vs_orl}). Whereas anesthesia is a real clinical procedure, any dose misuse can harm the patient. For example, overdosing propofol can damage the patient's brain, and underdosing can cause unnecessary intraoperative awareness. It is inhumane and high-risk to train RL agents directly with the human physical body. To this end, Moore et al.~\cite{moore2011reinforcement} proposed pharmacokinetic/pharmacodynamic (PK/PD) models, which simulate the distribution and transfer of drugs in the human body through differential equations. However, such models traditionally involve many hyperparameters, which are usually derived from population statistical data, which means that such models ignore the specificity across patients and therefore may be highly inaccurate when modeling an individual's environment. By means of personalized multi-task learning, Bird et al.~\cite{bird2019multi} improved the PK/PD model. While such methods have proven successful experimentally, simulation-based learning remains distant from clinical applications. Recently, Shin et al.~\cite{shin2021joint} proposed to learn a function that simulates the human environment based on real-world clinical anesthesia surgery data collected. The agent is then trained in this environment. All these algorithms mentioned above rely on environment modeling, and the reliability of trained agents depends on the simulation credibility.

\begin{figure*}[!ht]
	\centerline{\includegraphics[width=0.75\linewidth]{rl_vs_orl_v2.pdf}}
	\caption{Comparison of traditional RL (the left) and offline RL (the right) for automatic anesthesia policies learning.}
	\label{fig:rl_vs_orl}
\end{figure*}

Recently, offline reinforcement learning (ORL) has achieved impressive success and has received increasingly widespread attention. ORL is an advanced RL paradigm in which agents can learn suboptimal or even optimal policies from a collected offline dataset, while not requiring additional environmental exploration. Given that no interaction with the environment required, this new RL paradigm has attracted considerable interest in healthcare~\cite{kondruppersonalization, emerson2022offline, shiranthika2022supervised, wang2022learning}. However, current ORL methods suffer from the problem of distributional shift~\cite{fujimoto2019off}. For example, it is easy to learn overestimated Q functions, assigning high Q values to unseen operations, thus causing the model to select uncommon actions in the dataset, leading to undesired results. Moreover, the anesthesia task places higher safety requirements on the learned RL algorithms than common game tasks such as Atari~\cite{bellemare2013arcade}. An abnormal action by the agent may endanger the patient's life. To this end, we propose Policy Constraint Q-Learning (PCQL), a data-driven RL algorithm for solving the problem of learning anesthesia infusion policies on real clinical datasets. To the best of our knowledge, we are the first to apply ORL in the context of real clinical anesthesia. We first learn a superior anesthesia infusion policy from the collected anesthesia dataset based on Conservative Q-Learning (CQL;~\cite{kumar2020conservative}), a class-leading ORL algorithm. To ensure safer decisions by the agent, we add a policy constraint regularization term during the learning of the agent policy. The regularizer is learnable and obtained by supervised training on a collected offline dataset, thus implicitly modeling the distribution of state-action pairs.

We performed extensive experimental validation on a large clinical anesthesia dataset we collected. We first evaluated PCQL using the Off-Policy Evaluation method commonly used in ORL, and PCQL was predicted to achieve better performance than all baselines, including the human anesthesiologists' policy and the other RL methods. Next, we evaluated different RL algorithms trained on retrospective data considering the clinical dose by anesthesiologists as the reference dose. Thanks to the policy constraint regularization term, PCQL agrees better with the anesthesiologist's policy than other RL baselines and has the lowest error in both MAPE and RMSE metrics. We further analyzed the dose usage of PCQL, which is expected to use lower total drug doses to maintain the patient's vital signs within the target interval compared to the anesthesiologist's clinical strategy. We also found that the dose recommended by PCQL exhibited a stronger correlation with patient vital sign information and a faster adjustment frequency compared to the anesthesiologist's policy, suggesting the future possibility of accurate anesthesia automation. In addition, we analyzed PCQL confidence intervals, and visualized experimental results showed that PCQL confidence intervals covered most anesthesiologists' policies, which increased the model's credibility and reliability. Finally, we used an interpretable method, SHAP, to analyze the decision results of the model to increase the transparency of the automated anesthesia control system, which is relevant for anesthesiologists to use the system. 

\vspace{-2mm}
\section{Related Works}
\label{sec:related_works}
\subsection{Automated anesthesia infusion}
Automated anesthesia technology was first pioneered in the 1980s and has since advanced even further, as well as becoming more widely used~\cite{Wingert2021MachineLD}. This technology frees anesthesiologists from repetitive drug control tasks, allowing them to focus on the most critical aspects of each case, resulting in high-quality care for each patient. Moreover, automated anesthesia has shown great promise in improving drug infusion regimens and allowing for better precision anesthesia. Pasin et al.~\cite{pasin2017closed} have shown that Bispectral-guided total intravenous anesthesia can reduce the need for propofol during induction, better maintain the target depth of anesthesia, and reduce recovery time compared to manual control. In a multicenter study, Puri et al.~\cite{puri2016multicenter} showed that automated anesthesia can consistently achieve better performance than manual control. In a retrospective study, Brogi et al.~\cite{brogi2017clinical} found that automated anesthesia can effectively reduce overshooting or undershooting of target physiological indicators and can effectively increase the duration of maintenance at the target interval. The current development of automated anesthesia infusion technology is usually closely related to the evolution of artificial intelligence. Moore et al.~\cite{moore2011reinforcement} used a proportional integral derivative (PID) controller for controlling propofol infusion rate, which in turn regulates Bispectral (BIS). Some other control algorithms for automatic anesthesia have also been investigated, such as model-predictive controllers~\cite{sawaguchi2008model}, and rule-based controllers~\cite{liu2006titration}. However, such traditional control methods are usually limited by linear assumptions. For this reason, Moore et al.~\cite{moore2011reinforcement} first proposed the use of discrete action of RL for anesthesia control and achieved better results than PID controllers. Subsequently, Lowery et al.~\cite{lowery2013towards} extended the work of~\cite{moore2011reinforcement} to continuous space. Schamberg et al.~\cite{schamberg2022continuous} used a more advanced actor-critic algorithm to train an agent for anesthesia control. Yun et al.~\cite{yun2022hierarchical} used a hierarchical RL algorithm to learn a high-level and a low-level policy. The high-level policy generates a target BIS trajectory, and the low-level policy uses this information to learn more stable infusion control.

\vspace{-2mm}
\subsection{Environmental simulation modeling of anesthesia}
Environment modeling is fundamental to RL because the agent interacts with the environment through a "trial-and-error" paradigm, from which it learns the target policy. Existing automated anesthesia algorithms typically rely on the development of pharmacokinetic (PK) and pharmacodynamic (PD) models that simulate the response of a patient's BIS level to a specific drug dose. The PK model describes how the drug flows through the various compartments of the body (\emph{e.g.}, brain, slow compartments, etc.) based on a system of equations for a particular drug. The PD model then maps the specific drug concentration at the effect site to the effect level, \emph{i.e.}, BIS. However, simulation-based human environments usually involve a wide range of hyperparameters, which are usually derived from the statistical values of the population. This means that such models ignore the specificity between different patients and may therefore suffer unexpected inaccuracies when simulating the human environment. Bird et al.~\cite{bird2019multi} uses multi-task learning techniques to personalize the PK/PD model to an individual level, while retaining statistical power, and the results show improved prediction accuracy. Despite the promising experimental results achieved by such methods, the simulation data are still not convincing for applications in clinical anesthesia. Several studies have attempted to learn on real-world collections of clinical anesthesia procedures, and the goal of such methods is to learn an environmental function that simulates the body's response after receiving a certain dose of medication. Shin et al.~\cite{shin2021joint} learned a transition function that simulates the human environment through supervised learning, and then used proximal policy optimization combined with behavioral cloning algorithms to learn automatic anesthesia policies based on this environment, and achieved promising experimental results. Unlike~\cite{shin2021joint}, our goal is to use real clinical data and train a scoring function that evaluates the plausibility of state-action pairs and thus guarantees safer decisions by the agent.

\subsection{Offline Reinforcement Learning}
Recently, an advanced RL paradigm, offline reinforcement learning (ORL), also known as batch reinforcement learning, has been proposed. Compared to traditional online reinforcement learning, ORL allows agents to learn superior policies from collected datasets without having to perform additional exploration in the environment. Given the property of "offline", and thus avoiding costly and dangerous, unethical exploratory actions when interacting with the environment, ORL has also gained widespread interest in the medical field. Kondrup et al.~\cite{kondruppersonalization} used deep conservative reinforcement learning to determine the best ventilator settings for ICU patients. Emerson et al.~\cite{emerson2022offline} proposed using ORL to learn a safer blood glucose control strategy for people with Type 1 diabetes. Shiranthika et al.~\cite{shiranthika2022supervised} developed the supervised optimal chemotherapy regimen, which can provide cancer patients with an optimal chemotherapy-dosing schedule, thus assisting oncologists in clinical decision-making. Wang et al.~\cite{wang2022learning} used ORL to learn the optimal treatment strategy for sepsis patients in ICU. These are good illustrations of the potential of ORL applications in the medical field. However, the current ORL is susceptible to the problem of distributional shift. For example, it is easy to learn overestimated Q functions that assign high Q values to unseen operations, thus causing the model to select uncommon operations in the dataset, putting patients at risk. Researchers have made successive efforts to alleviate the overestimation problem, such as Batch Constrained Q-Learning~\cite{fujimoto2019off}, Advantage Weighted Actor-Critic~\cite{nair2020awac}, Conservative Q-Learning~\cite{kumar2020conservative}, etc.

\subsection{Conservative Q-Learning}
Conservative Q-Learning (CQL;~\cite{kumar2020conservative}) is an ORL algorithm based on a Soft Actor-Critic improvement, which solves the problem of overestimating Q values in ORL by learning a conservative estimate of the Q function.

In a standard Q function, the loss function can be written as:
$$
\begin{aligned}
	L_{DQN}&=\mathbb{E}_{s, a \sim D}\left[\left(Q(s, a)-B^{\pi_{k}} Q^{k}(s, a)\right)^{2}\right]\\
	&=\mathbb{E}_{s, a \sim D}\left[\left(Q(s, a)-(r(s,a)+\gamma \max_{a'}Q(s', a')\right)^{2}\right],
\end{aligned}
$$
where $B^{\pi_k}$ is the Bellman operator~\cite{sutton2018reinforcement} on the currently learned policy $\pi_k$ at iteration $k$. The true Q-value estimate of the state-action pair is approximated by minimizing $Q(s, a)-(r(s,a)+\gamma \max_{a'}Q(s', a')$. However, due to the inability to explore the environment, naive Q-learning tends to get overly optimistic Q values on offline data, which leads to the problem of overestimation. To this end, CQL adds a regularization term:

\begin{align*}
	L_{CQL}
	&=L_{DQN}\\
	&+\alpha \mathbb{E}_{s \sim D}\left[\log \sum_a \exp (Q(s, a))-\mathbb{E}_{a \sim D}[Q(s, a)]\right],
\end{align*}
where $\alpha$ is the factor that relaxes the importance of the conservative term in the overall loss. The log-sum-exp term penalizes the action with the largest Q-value. The second term, on the other hand, guarantees to maximize Q-values for state-action pairs in the dataset. Thus, this conservative term allows high Q-values to be assigned only to actions within the distribution.

\section{Methodology}
\label{sec:methodology}

We first formalize the procedure of automatic anesthetic administration, including the state space, action space, and reward design. Subsequently, we introduce a new constraint term that enables to constrain agent-predicted actions in the action distribution of the dataset. Finally, we describe the proposed overall framework and the specific training process.

\subsection{The Problem Setting}

In this study, the automatic administration of anesthesia is modeled as a Markov decision process (MDP) at finite time steps. Typically, the MDP is defined as a 5-tuple $<S,A,R,P,\gamma>$. At time step $t$, the agent, in the current state $s_t\in S$, takes an action $a_t\in A$ and moves to the next state $s_{t+1}\in S$ according to the transition probability $P(s'|s,a)$. The agent expects to maximize the accumulated reward as it interacts with the environment.

In the problem setting, the agent is the controller that controls the delivery rate of the drug delivery device, while the environment is the patient in the perioperative phase of anesthesia. The agent is expected to predict the optimal dosage required by the patient at the current moment, based on the state observed from the environment, such as the patient's current vital signs information, and the history of drug administration. A suitable drug delivery controller should be able to maintain the patient's vital signs consistently during the surgery with minimal drug administration costs. Below are detailed definitions of the observed states, environment, reward functions, and agents.

\subsubsection{State Space}
The state space defines the information that the agent can observe at each moment, including the patient's clinical information, real-time vital signs, fluids, and other important information. The state space contains a total of 19 variables.
\begin{itemize}
	\item Clinical information: age, gender, height, weight, BMI, ASA grade.
	\item Vital Signs: systolic arterial pressure ($\text{AP}_\text{sys}$), diastolic arterial pressure ($\text{AP}_\text{dia}$), mean arterial pressure (MAP), MAP/$\text{AP}_\text{sys}$/$\text{AP}_\text{dia}$ for the previous two moments.
	\item Analgesics: Rifentanil.
	\item The others: MAP target, MAP target error, MAP change.
\end{itemize}

In this case, the MAP target is defined as the average of the MAP of a patient over one operation.
$$
\text{MAP}^*_t=\frac{1}{T}\sum^T_{t=1}\text{MAP}_t,
$$
where $T$ is the duration of an operation and $\text{MAP}_t$ is the MAP of a patient at time step t. The MAP target error is defined as the distance between the current MAP and the target MAP.
$$
\text{MAP}(error)_{t}=\text{MAP}_t-\text{MAP}^*_t.
$$
The MAP change is defined as the difference between the MAP at the current moment and the previous moment.
$$
\text{MAP}(change)_{t}=\text{MAP}_t-\text{MAP}_{t-1}.
$$

\subsubsection{Action Space}
During automated anaesthetic infusion, more than one drug may be involved. In this study, the agent was only required to control propofol infusion rate. We also included the use of other drugs in the observed state to consider the synergistic effect of propofol with other drugs (\emph{e.g.}, analgesics). We calculated the maximum propofol dose used in the dataset $P_{\max}$ and performed a maximum normalization. The action space is thus $a\in \mathcal{A}$, continuous from 0 to 1. The final dose recommended by the agent is $a\cdot P_{\max}$, , and this allows for safer infusion control.

\subsubsection{Reward Function}
During surgery, the main objective of the agent is to maintain the patient's vital signs in a certain interval. In this study, we focused on MAP in particular since MAP measurements are more accessible and MAP is a good indicator of the patient's vital status at the current moment. Specifically, we retrospectively averaged the MAP of a patient's procedure as the MAP target $\text{MAP}^*_t$, namely, we expected the agent to maintain the patient's MAP around the MAP target. 

To learn a more robust automated anesthesia policy, we adopt a segmented functional reward design. We defined the ideal interval of $\pm 0\sim15\%$ and the suboptimal interval of $\pm 15\sim30\%$, respectively. When the patient's MAP is within these two admissible intervals, a reward is given, while a penalty is given when the MAP deviates from the suboptimal one. The specific reward function is designed as follows.
\begin{align*}
	R&_{error}(s_t,a_t)\\
	&=\begin{cases}
		+1 \quad &\text{if}~~|\text{MAP}_t-\text{MAP}^*_t |\le15\%\text{MAP}^*_t;\\
		+0.5 \quad &\text{if}~~15\%\text{MAP}^*_t\le|\text{MAP}_t-\text{MAP}^*_t |\le30\%\text{MAP}^*_t;\\
		-1 \quad &\text{else.}
	\end{cases}
\end{align*}

In addition, we hoped that the agent would use the lowest possible dosage to maintain the patient's vital signs within the target interval. To this end, we added a dose penalty as follows.
$$
R_{dosage}(s_t,a_t)=-\frac{|\text{MAP}_t-\text{MAP}^*_t|}{\text{MAP}^*_t}a_t,
$$
where $\frac{|\text{MAP}_t-\text{MAP}^*_t|}{\text{MAP}^*_t}$ is the adaptive correction factor. We do not want the agent to be overly conservative in the use of propofol, especially when the patient's MAP deviates from the target mean arterial pressure. Instead, the agent is expected to use a lower propofol dose to maintain the patient's vital signs within the target interval only when the patient's MAP is around the target. In summary, the overall reward function is calculated as follows.
\begin{equation}
	R_{total}(s_t,a_t)=R_{error}(s_t,a_t)+R_{dosage}(s_t,a_t).\label{reward}
\end{equation}

\subsection{Policy Constraint Q-Learning}
CQL constrains the Q function from the estimation of the Q value. To secure the agent's policy, we add a constraint term to the CQL from a policy perspective. Our goal is to learn a function from a collected offline dataset to evaluate the plausibility of state-action pairs.

%\begin{figure}[!h] 	\centerline{\includegraphics[width=0.95\columnwidth]{figs/arch_v1.pdf}} 	\caption{Magnetization as a function of applied field.


\subsubsection{Policy Constraint}
The scoring function consists of two components, the environment transition model $h$ and the behavior prediction model $g$. The environmental transition model aims to predict the new state to which the current state $s$ will be transferred after the selection of action $a$, i.e., $s'=h(s,a)$. And the behavior prediction model predicts the possible actions that may be taken  given two consecutive states, i.e., $a=g(s,s')$. For the action $\hat a=f(s)$ predicted by the agent at the current state $s$ (\emph{i.e.}, the dose), we first imagine its subsequent state using the environmental transition model, and then reason about the possible action $\dot a$ using the behavioral prediction model, i.e., $\dot a=g(s,h(s,\hat a))$. We constrain the action $\hat a$ predicted by the agent not to be too far from the action $\dot a$ inferred by the behavioral prediction model as likely to occur. A simple approach is to
$$
\Phi(s,\hat a) =\|g(s,h(s,\hat a))- \hat a\|_2,
$$
where $\|\cdot\|_2$ denotes the Euclidean distance.

However, such a deterministic constraint leads to the fact that the actions predicted by the agent are supervised by the output of the behavior prediction model. This is perhaps too stringent, thus weakening the exploration of other, more optimal actions. For this reason, we propose an alignment constraint in the latent space. The network architectures of the environment transition model $h$ and the behavior prediction model $g$ are similar, and both consist of an encoder, a predictor, and a projector. The predictor aims to predict the target information (\emph{e.g.} $s'$ for $h(s,a)$), while the projector is responsible for mapping the output of the encoder to the latent space, a process we denote as $\mathbf {Prj}(\cdot)$. We use the Cross Entropy function for alignment constraints, i.e.
$$
\Phi(s,\hat a) =-\text{softmax}\left(\frac{\mathbf {Prj}(h(s,\hat a))}{\tau}\right)\cdot\log\left(\text{softmax}\left( \frac{\mathbf {Prj}(h(s,\dot a))}{\tau}\right)\right),
$$
where $\tau$ is the temperature factor for scaling the learning difficulty. 

Our final objective function is
\begin{equation}
	L_{total}=L_{CQL}+\Phi(s,\hat a).\label{total}
\end{equation}


\subsubsection{Training Strategy}
We next discuss the training of the environment transfer model $h$ and the behavior prediction model $g$. In fact, both models can be learned from an offline dataset following a supervised learning paradigm. We sample $(s,a,s')\sim\mathcal D$ and train these two models using an autocoder-like approach, i.e., 
\begin{equation}
	L_{cycle}=(g(s,h(s,a)) - a) + (h(s,g(s,s')) - s'). \label{cycle}
\end{equation}
Also, we add the entropy consistency as follow,
\begin{equation}
	L_{entropy}=\mathcal H(h(s,\dot a), h(s,a))+\mathcal H(g(s,\dot s'), g(s,s')),\label{entropy}
\end{equation}
where $\mathcal H(a,b)=-\text{softmax}(a/\tau)\cdot\log(\text{softmax}(b/\tau))$ and $ \tau$ is the temperature factor. We jointly train the scoring function $\Phi(s,a)$ and the agent of the CQL algorithm, while adding consistency constraints to the action loss of CQL. The pseudo-code of the overall training algorithm of the proposed PCQL is shown in Algorithm \ref{alg:PCQL}.

%training set $\mathcal D = \{D_i,i=1,\cdots,S\}$, where $S$ is the number of surgeries, $\mathcal D_i=\{(s_t,a_t,s_{t+1})_i,t=1,\cdots,T_i\}$ is the $i$-th surgery, and $T_i$ is the time duration of the $i$-th surgery.

\begin{algorithm}  
	\caption{Policy Constraint Q-Learning (PCQL)}  
	\label{alg:PCQL}  
	\begin{algorithmic}[1]
		\STATE {\textbf{Input:} training set $\mathcal D$, number of training epochs $E$}  
		\STATE {Initialize PCQL network $\Pi$}
		\STATE {Initialize Policy Constraint network $\Phi$}
		\STATE {Construct transition dataset $\mathcal D'$ from $\mathcal D$ using \eqref{reward}}
		\FOR {epoch=1 \TO E} 
		\REPEAT 
		\STATE {Sample mini-batch from transition dataset $\mathcal D'$} 
		\STATE {Learn $\Phi$  using \eqref{cycle} and \eqref{entropy}}
		\STATE {Learn $\Pi$  using \eqref{total}}
		\UNTIL{all mini-batch are sampled}
		\ENDFOR
		\STATE {\textbf{Output:} optimal PCQL policy $\Pi^*$}
	\end{algorithmic}  
\end{algorithm}  


\section{Experiments and Results}
\label{sec:exp}

\subsection{Data Collection and Training Details}
We collected 6,303 general anesthesia surgeries from West China Hospital. We filtered the surgeries  that met any of the following conditions: i) missing corresponding dosing information; ii) surgical time shorter than half an hour; iii) severe missing vital sign records during surgery; and iv) samples using inhaled anesthesia, such as sevoflurane or desflurane instead of propofol. We used the k-NN method to fill in the remaining missing values. Finally, a total of 1,293 surgeries with 284,281 anesthesia records were obtained. We divide the collected dataset into training set, validation set, and test set according to 7: 1: 2. All our experiments are conducted based on d3rlpy~\cite{d3rlpy}. We use Adam~\cite{kingma2014adam} as the optimizer and train for 200 epochs with batchsize 256. The learning rates of actor, critic, environment transition model, and action prediction model are set to $1\times10^{-4}$, $3\times10^{-4}$, $1\times10^{-4}$ and $3\times10^{-4}$ respectively.

\subsection{Evaluation Protocol}

\subsubsection{Baselines}
We mainly set the following baselines, including anesthesiologists' clinical policy (ANE), Soft Actor-Critic (SAC,~\cite{haarnoja2018soft}), Batch Constrained Deep Q-learning (BCQ,~\cite{le2019batch}), and Conservative Q-learning (CQL,~\cite{kumar2020conservative}). The ANE is the actual clinical practice of anesthesiologists presented in the collected dataset and is our main reference. To illustrate the advantages of offline reinforcement learning when the environment is not available, we include in our comparison a dominant off-policy reinforcement learning algorithm, SAC. Second, our algorithm improves on CQL, to which we also compare PCQL. In addition, we compare another competitive offline reinforcement learning algorithm, BCQ. BCQ is a class of offline reinforcement learning based on policy constraints that aims to minimize the distance between the selected action and the action in the dataset by establishing a conditional VAE on the state.

\subsubsection{Off Policy Evaluation}
In our experimental settings, no interactive environment is available, so trained policies cannot directly compute cumulative rewards by interacting with the environment. We used the off-policy evaluation (OPE) method commonly used in offline reinforcement learning for performance evaluation, which was done on a divided test set. In recent experiments in healthcare settings, researchers have found that the Fitted Q Evaluation (FQE;~\cite{le2019batch}) approach consistently yields accurate policy performance evaluation results~\cite{tang2021model}, and we follow this practice. FQE uses a fixed trained policy, and a re-set Q function, and then re-trains the Q function. The retrained Q function estimates how much return the trained policy can obtain from initial states. The higher the estimated returns, the better the expected performance of the policy.



\subsubsection{Retrospective Evaluation}
Since the data we collect is retrospective from the operating room, we can also evaluate the model in a "consult" mode. In this setting, the intelligence can see the historical state and action information, and then give the recommended dose for the current state. At the same time, we take the actual clinical dose given by the anesthesiologist as the optimal dose and calculate the difference between the recommended dose and the actual dose. Although we could not know how the human body responds to the recommended dose by the agent, we argue  that this evaluation can still in part reflect the performance of the agent when faced with the real world.  To measure the discrepancy between the recommended doses and the actual ones, we used two common regression metrics for evaluation: mean absolute percentage error (MAPE) and root mean square error (RMSE). In this case, MAPE is defined as
$$
\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T}\frac{|y_{i,t}-y_{i,t}^*|}{\max(\varepsilon,y_{i,t}^*)}100\%
$$
where $N$ is the number of episodes and $T$ is the length of an episode. $y_{i,t}$, $y_{i,t}^*$ denote the recommended dose of the agent and the actual dose for the $i$ episode at the $t$ moment, respectively. The $\varepsilon$ stands for a very small number to avoid division by zero error. RMSE is defined as
$$
\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T}\sqrt{(y_{i,t}-y_{i,t}^*)^2}
$$


\subsection{Quantitative Analysis of Performance}
In the following, we compare the PCQL with the baseline methods ANE, BCQ, and CQL. First, we used the initial state as the main evaluation metric, and the quantitative results are shown in Table~\ref{tab:init_state}. We can see that PCQL is expected to outperform other policies. The estimated return of PCQL is 1.24 times higher than that of the anesthesiologist's strategy and 1.11 times higher than that of the baseline CQL. It is worth noting that all the RL-based policies perform well except SAC. one reason for the poor performance of SAC may be that the off-policy RL algorithm cannot correct the estimation errors of the out-of-distribution Q values in time because of the unavailability of the environment, and the extrapolation errors continue to propagate in the training, which eventually leads to the failure of the algorithm training. In contrast, the ORL algorithms BCQ and CQL mitigate this problem by constraining the output of the policy, and the estimation of Q-values, respectively. PCQL combines their ideas and achieves better results than them.


\begin{table}[h]
	\caption{Comparison of the predicted returns of different methods}
	\label{tab:init_state}
	\setlength{\tabcolsep}{3pt}
	\centering
	\begin{tabular}{cccccc}
		\toprule
		~ & ANE & SAC & BCQ & CQL & PCQL \\
		\midrule
		\textbf{Initial state $\uparrow$} & 59.755 & 39.583 & 66.619 & 66.968 & \textbf{74.156} \\
		\bottomrule
	\end{tabular}
\end{table}

Second, we evaluated the trained models in a "consultation" mode, focusing on the difference between the recommended dose from different policies and the actual clinical dose. Two common metrics, MAPE and RMSE, were used for comparison, and the results are shown in Table~\ref{tab:error}. On the MAPE metric, BCQ achieved unsatisfactory results of 17.2297\%, while PCQL and CQL kept MAPE within 10\%, and PCQL was definitely lower than CQL by about 2\%. For the RMSE indicator, PCQL also achieves the lowest RMSE compared to the two ORL baselines, BCQ and CQL. The results of SAC on both metrics are poor. 

\begin{table}[h]
	\caption{Performance comparison in retrospective evaluation}
	\label{tab:error}
	%	\setlength{\tabcolsep}{3pt}
	\centering
	\begin{tabular}{cccccc}
		\toprule
		~ & ANE & SAC & BCQ & CQL & PCQL \\
		\midrule
		\textbf{MAPE (\%)} $\downarrow$ & - & 57.625 & 17.230 & 9.205 & \textbf{7.218} \\
		\textbf{RMSE} $\downarrow$ & - & 2.364 & 0.598 & 0.284 & \textbf{0.268} \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure*}[!ht]
	\centerline{\includegraphics[width=0.93\linewidth]{dose_pred_gt_map_v2.pdf}}
	\caption{Comparison of recommended doses by PCQL and anesthesiologist in general anesthesia cases.}
	\label{fig:cmp_pred_gt_map}
\end{figure*}

\begin{table*}[!t]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\small
		\makeatletter\def\@captype{table}\makeatother
		\caption{Comparison of mean doses used}
		\label{tab:mean_dose}
		\begin{tabular}{ccc}
			\toprule
			~ & ANE & PCQL \\
			\midrule
			\textbf{Mean dose (mg/Kg/hrs)} & 3.921 & 3.719 \\
			\bottomrule
		\end{tabular}
	\end{minipage}\quad
	\begin{minipage}{0.45\textwidth}
		\centering
		\small
		\makeatletter\def\@captype{table}\makeatother
		\caption{Comparison of the correlation}
		\label{tab:corr}
		\begin{tabular}{ccc}
			\toprule
			~ & ANE & PCQL \\
			\midrule
			\textbf{Correlation} & 0.081 & 0.245 \\
			\bottomrule
		\end{tabular}
	\end{minipage}
	\vspace{-15pt}
\end{table*}


\subsection{Model Recommended Dosage Analysis}
Given that PCQL significantly outperforms baselines in both previous performance evaluations, we next focus exclusively on analyzing PCQL's recommended dose usage and comparing its advantages and disadvantages with the anesthesiologist's policy. The statistical results of mean dose values on the test set are shown in Table \ref{tab:mean_dose}. We found that the mean dose value of 3.921 mg/Kg/hrs recommended by PCQL is slightly lower than the actual clinical dose of 3.719 mg given by the anesthesiologist by 0.202 mg/Kg/hrs. The difference of 0.202 mg/Kg/hrs reduction is clinically acceptable, because even two experienced anesthesiologists may give a difference of 1 mg/Kg/hrs in the anesthetic dose for the same case of anesthetic surgery. At the same time, it is clinically meaningful if the PCQL policy is able to maintain the patient's depth of anesthesia within the target range using a lower drug dose. First, it would mean that the problem of over-anesthesia could be alleviated, reducing the incidence of post-anesthesia syndrome and thus improving the quality of patients' surgery. Second, lower dose use often means less consumption of medical resources, which helps reduce the cost of patient care.


We randomly selected several surgical cases in the test set and visualized the doses recommended by PCQL, anesthesiologist, respectively, and we also appended the corresponding MAP change curves of the patients, as shown in Fig. \ref{fig:cmp_pred_gt_map}. In Cases 4, 7, 8, and 9, the PCQL's dose curve was slightly lower and was surrounded by the anesthesiologist's dose curve. Overall, in these nine cases, the PCQL recommended dose was generally in agreement with the anesthesiologist's.

Intriguingly, we also noted that the dose recommended by PCQL, especially in cases 7 and 9, fluctuated around the actual dose given by the anesthesiologist and that this fluctuation was positively correlated with the MAP of the patient. To verify this, we calculated Pearson correlation coefficients between recommended doses and MAPs for PCQL and anesthesiologist policies, respectively. The Pearson correlation coefficient was calculated by
$$
\rho_{X,Y}=\frac{\mathbb{E}{((X-\mu_X)(Y-\mu_Y))}}{\sigma_X\sigma_Y},
$$
where $\mu_X$ and $\sigma_X$ denote the mean and variance of the variable $X$, respectively; similarly for $Y$.


As indicated in Table \ref{tab:corr}, the dose recommended by PCQL showed a stronger correlation with MAP compared to the dose recommended by the anesthesiologist (0.245 compared to 0.081). This suggests the possibility of computer-assisted precision anesthesia. Because anesthesiologists are routinely responsible for more than one procedure at a time, it is not possible to provide much-customized care for a single patient. Anesthesiologists are also required to work intensive, long hours. Since the human attention span decreases with work time, it is extremely challenging to keep an eye on the patient's vital signs and administer medications in real-time during the late stages of anesthesia. However, as can be seen in Figure \ref{fig:cmp_pred_gt_map}, the recommended dose of PCQL is more sensitive and responsive to changes in the patient's vital signs. It promises to provide higher quality and more personalized anesthesia infusion management.

\subsection{Confidence Interval Analysis of the Model}

It is a fact that even experienced anesthesiologists cannot determine the optimal dose—they usually pick a preferred dose within a confidence interval that they believe is reasonable. To estimate the confidence interval for the PCQL, we simply modified the output mechanism of the PCQL so that it probabilistically samples an action when making inferences, rather than the best one. We choose the common Gaussian policy of $\pi_\theta(a|s)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left({-\frac{a-f_\theta(s)}{2\sigma^2}}\right)$. At state $s$, the actions taken when sampled by this policy obey a normal distribution with a mean of $f_\theta(s)$ and a variance of $\sigma^2$. Fig. \ref{fig:uncertainty} gives the results of sampling 100 times, and it can be seen that the confidence interval estimated by PCQL is on the safe side without any serious deviation from the actual clinical dose. The confidence interval recommended by PCQL covers the majority of the anesthesiologist's clinical policy, and this is encouraging. Because it means that under this situation, the anesthesiologist's policies can be derived from sampling within the confidence interval of the PCQL. Although we could not be informed in retrospective data about the confidence intervals considered by anesthesiologists, we argue that this enhances the credibility and reliability of PCQL.


\begin{figure}[t] 
	\centerline{\includegraphics[width=0.95\columnwidth]{dose_dist_v2.pdf}} 	\caption{Comparison of the distribution of doses used by PCQL and anesthesiologists.} 	
	\label{fig:mape} 
\end{figure}

\begin{figure}[!t] 
	\centerline{\includegraphics[width=\columnwidth]{uncertainty_v3.pdf}} 	
	\caption{Confidence interval estimation of PCQL in general anesthesia cases.}
	\label{fig:uncertainty} 
\end{figure}

\subsection{Interpretability of the Model}

\begin{figure}[t] 	
	\centerline{\includegraphics[width=0.95\columnwidth]{barh_v2.pdf}} 	
	\caption{Results of absolute mean SHAP values for different input components of PCQL.} 	
	\label{fig:barh} 
\end{figure}

The deep neural networks used to implement our policy are notorious for their black-box characteristics. To enhance the transparency of the model, we analyzed the prediction results of the model using SHapley Additive exPlanations (SHAP;~\cite{lundberg2017unified}). SHAP is a game-theoretic interpretation approach that predicts the output of a model by fitting a linear interpretation model, and calculates SHAP values for observed features, which indicate the importance of the features in influencing the model output. SHAP values have been widely applied to analyze the relative importance of features in various machine learning models for medical applications~\cite{schamberg2022continuous, lundberg2018explainable, tjoa2020survey}. We used the publicly available SHAP library\footnote{https://github.com/slundberg/shap} to calculate the SHAP values for each feature in the observation space, and obtained the Absolute Mean SHAP Score for each feature by taking the average of the absolute values. 

As shown in Fig. \ref{fig:barh}, we found that the output of PCQL (\emph{i.e.}, the infusion rate of propofol) was most affected by remifentanil dose. We attribute this to the fact that remifentanil is an immediate injection analgesic that rapidly depresses the patient's respiration and, in addition, acts synergistically with propofol, thus greatly influencing the output of the model. Clinical characteristics of patients such as gender, age, and weight also significantly influenced model output, followed by real-time vital sign characteristics such as MAP, $\text{AP}_\text{Dia}$, and MAP target. We believe this is similar to the clinical decision making of the anesthesiologist—the anesthesiologist usually determines the primary base dose by pre-assessing the patient's physical condition, and adjust the base dose according to changes in the patient's vital signs during surgery.


\section{Conclusion}
We presented Policy Constraint Q-Learning, an offline reinforcement learning algorithm for automated anesthesia infusion. We conducted extensive experiments on a large collected anesthesia dataset to validate the effectiveness of PCQL. Experimental results showed that our method was predicted to be better than the baseline method. The recommended dose of PCQL was consistent with the anesthesiologist's strategy, but used less total dose and was more responsive to the patient's vital signs. We also analyzed confidence intervals of recommended doses for PCQL and found that they covered most of the clinical decisions of anesthesiologists. Finally, we performed an interpretability analysis of PCQL results using SHAP.

Our work validates the clinical viability of offline reinforcement learning for automated anesthesia, but there are still some issues to be addressed. First, our data is monocentric, so we cannot validate PCQL's performance on a multicenter dataset. We plan to collect clinical anesthesia data from more centers and validate it in the next step. Second, PCQL showed good experimental results from both Off Policy Evaluation and retrospective evaluation perspectives. However, clinical trials in the real world are still needed to confirm the online feasibility of the model.

%\appendices
%
%Appendixes, if needed, appear before the acknowledgment.


\bibliographystyle{IEEEtran}
\bibliography{generic-color}{}

\end{document}
