
\section{2D periodic gated RNNs}
\label{app:GRU}
In this appendix, we describe our implementation of 2D gated RNN wave function for periodic systems, that can be used to approximate the ground states of the Hamiltonians considered in this paper. If we define
\begin{align*}
    \bm{h'}_{i,j} &= [\bm{h}_{i-1,j} ; \bm{h}_{i,j-1}; \bm{h}_{\text{mod}(i+1,L_x),j}; \bm{h}_{i,\text{mod}(j+1, L_y)}], \\
    \bm{\sigma'}_{i,j} &= [\bm{\sigma}_{i-1,j} ; \bm{\sigma}_{i,j-1}; \bm{\sigma}_{\text{mod}(i+1,L_x),j}; \bm{\sigma}_{i,\text{mod}(j+1, L_y)}],
\end{align*}
then our gated 2D RNN wave function ansatz is based on the following recursion relations:
\begin{align*}
    \tilde{\bm{h}}_{i,j} &= \tanh \! \Big(
    W[\bm{\sigma'}_{i,j}; \bm{h'}_{i,j}]
    +  \bm{b} \Big), \\
   \bm{u}_{i,j} &= \text{sigmoid}\! \Big(
    W_g[\bm{\sigma'}_{i,j}; \bm{h'}_{i,j}]
    +  \bm{b}_g \Big), \\
    \bm{h}_{i,j} &= \bm{u}_{i,j} \odot \bm{\tilde{h}}_{i,j} + (1-\bm{u}_{i,j}) \odot (U \bm{h'}_{i,j}).
\end{align*}
Here `$\odot$' is the Hadamart (element-wise) product. A hidden state $\bm{h}_{i,j}$ can be obtained by combining a candidate state $\bm{\tilde{h}}_{i,j}$ and the neighbouring hidden states $\bm{h}_{i-1,j}, \bm{h}_{i,j-1}, \bm{h}_{\text{mod}(i+1,L_x),j}, \bm{h}_{i,\text{mod}(j+1, L_y)}$. The update gate $\bm{u}_{i,j}$ determines how much of the candidate hidden state $\bm{\tilde{h}}_{i,j}$ will be taken into account and how much of the neighbouring states will be considered. With this combination, it is possible to circumvent some limitations of the vanishing gradient problems~\cite{zhou2016minimal,shen2019mutual}. The weight matrices $W, W_g, U$ and the biases $b, b_g$ are variational parameters of our RNN ansatz. The size of the hidden state $\bm{h}_{i,j}$ is a hyperparameter called the number of memory units or the hidden dimension and it is denoted as $d_h$. Finally, to motivate the use of the gating mechanism, we hightlight that a gated 2D RNN was found be better than a non-gated 2D RNN for the task of finding the ground state of a 2D Heisenberg model~\cite{RNNAnnealing}. 

Since we use an enlarged Hilbert space in our 2D RNN, we take $\bm{\sigma}_{ij}$ to be the concatenation of the one-hot encodings of the binary physical variables in each unit cell. In this case if $m$ is the number of physical variables per unit cell, then $\bm{\sigma}_{ij}$ has size $2m$. We also note that the size of the Softmax layer is taken as $2^m$ so that we can autoregressively sample each unit cell variables at once.

\section{Variational monte carlo and variance reduction}
\label{app:VMC}
To optimize the energy expectation value of our RNN wave function $\ket{\Psi_{\bm{\theta}}}$, we use the Variational Monte Carlo (VMC) scheme, which consists on using importance sample to estimate the expectation value of the energy $E_{\bm{\theta}} = \bra{\Psi_{\bm{\theta}}} \hat{H} \ket{\Psi_{\bm{\theta}}}$ as follows~\cite{becca_sorella_2017,RNNWF}:
\begin{equation*}
    E_{\bm{\theta}} = \frac{1}{M}\sum_{i=1}^{M} E_{\text{loc}}(\bm{\sigma^{(i)}}),
\end{equation*}
where the local energies $E_{\text{loc}}$ are defined as
\begin{equation*}
    E_{\text{loc}}(\bm{\sigma}) = \sum_{\bm{\sigma'}} H_{\bm{\sigma} \bm{\sigma'}} \frac{\Psi_{\bm{\theta}}(\bm{\sigma'})}{\Psi_{\bm{\theta}}(\bm{\sigma})}.
\end{equation*}
Here the configurations $\{\bm{\sigma^{(i)}}\}_{i = 1}^{M}$ are sampled from our ansatz using autoregressive sampling. The choice of $M$ is a hyperparameter that can be tuned. Furthermore, $E_{\text{loc}}(\bm{\sigma})$ can be efficiently computed for local Hamiltonians. Furthermore, the gradients can be estimated as~\cite{RNNWF}
\begin{equation*}
    \partial_{\bm{\theta}} E_{\bm{\theta}} = \frac{2}{M}\mathfrak{Re} \left (\sum_{i=1}^{M} \partial_{\bm{\theta}} \log \left( \Psi_{\bm{\theta}}^{*}(\bm{\sigma^{(i)}}) \right) \left ( E_{\text{loc}}(\bm{\sigma^{(i)}}) - E_{\bm{\theta}} \right) \right).
\end{equation*}
For a stoquastic Hamiltonian $\hat{H}$~\cite{bravyi2015monte}, we can use a positive RNN wave function where the use of the real part $\mathfrak{Re}$ is not necessary. Importantly, subtracting the variational energy $E_{\bm{\theta}}$ is helpful to achieve convergence as it reduces the variance of the gradients near convergence without biasing its expectation value as shown in Ref.~\cite{RNNWF}. The subtracted term is referred to as a baseline, which is typically used for the same purpose in the context of Reinforcement learning~\cite{mohamed2019monte}. To demonstrate the noise reduction claim more rigorously compared to the intuition provided in Ref.~\cite{RNNWF}, let us focus on the variance of the gradients after subtracting the baseline. First of all, we define:
\begin{equation*}
    O_{\bm{\theta}}(\bm{\sigma}) \equiv \partial_{\bm{\theta}} \log \left( \Psi^*_{\bm{\theta}}(\bm{\sigma}) \right).
\end{equation*}
In this case, the gradient with a baseline can be written as:
\begin{align*}
    \partial_{\bm{\theta}} E_{\bm{\theta}} &= 2\mathfrak{Re} \left (\left \langle O_{\bm{\theta}}(\bm{\sigma}) \overline{E}_{\text{loc}}(\bm{\sigma}) \right \rangle \right). 
\end{align*}
where $\overline{E}_{\text{loc}}(\bm{\sigma}) \equiv E_{\text{loc}}(\bm{\sigma}) - E_{\bm{\theta}}$ and $\langle . \rangle$ denotes an expectation value over the Born distribution $|\Psi_{\bm{\theta}}(\bm{\sigma})|^2$. To estimate the gradients' noise, we look at the variance of the gradients estimator, which can be decomposed as follows~\footnote{The variance of a complex random variable $Z$ with an expectation value $\mu$ is defined in our context as $\text{Var}(Z) = \langle (Z - \mu)^* (Z - \mu) \rangle$. Furthermore, the covariance of two complex random variables $X$ and $Y$ with expectation values $\mu_X$, $\mu_Y$ respectively is defined as $\text{Cov}(X,Y) = \langle (X-\mu_X)^* (Y-\mu_Y) \rangle $.}:
\begin{align*}
    \text{Var} (O_{\bm{\theta}} \overline{E}_{\text{loc}}) &= \text{Var}( O_{\bm{\theta}} E_{\text{loc}} ) \\
    &- 2 \mathfrak{Re}(\text{Cov}(O_{\bm{\theta}} E_{\text{loc}} , O_{\bm{\theta}} E_{\bm{\theta}})) + E_{\bm{\theta}}^2 \text{Var}(O_{\bm{\theta}}).
\end{align*}
Thus the variance reduction $R$, after subtracting the baseline, is given as:
\begin{align*}
    R &\equiv \text{Var} (O_{\bm{\theta}} \overline{E}_{\text{loc}}) - \text{Var}( O_{\bm{\theta}} E_{\text{loc}} ), \\
    &= - 2 E_{\bm{\theta}} \mathfrak{Re}(\text{Cov}(O_{\bm{\theta}} E_{\text{loc}} , O_{\bm{\theta}})) + E_{\bm{\theta}}^2 \text{Var}(O_{\bm{\theta}}).
\end{align*}
Since the gradients' magnitude tends to near-zero values close to convergence, statistical errors are more likely to make the VMC optimization more challenging. We focus on this regime for this derivation to show the importance of the baseline in reducing the noise. Thus, we assume that $E_{\text{loc}} (\bm{\sigma}) = E_{\bm{\theta}} + \xi(\bm{\sigma})$, where the supremum of the local energies fluctuations is much smaller compared to the variational energy, i.e., $(\sup_{\bm{\sigma}} |\xi(\bm{\sigma})|) \ll E_{\bm{\theta}}$. From this assumption, we can deduce that:
\begin{align*}
    R &= - 2 E_{\bm{\theta}}^2 \mathfrak{Re}(\text{Cov}(O_{\bm{\theta}} , O_{\bm{\theta}})) \\
    &- 2 E_{\bm{\theta}} \mathfrak{Re}(\text{Cov}(O_{\bm{\theta}} \xi , O_{\bm{\theta}})) 
     + E_{\bm{\theta}}^2 \text{Var}(O_{\bm{\theta}}), \\
    &= - E_{\bm{\theta}}^2 \text{Var}(O_{\bm{\theta}}) - 2 E_{\bm{\theta}} \mathfrak{Re}(\text{Cov}(O_{\bm{\theta}} \xi , O_{\bm{\theta}})).
\end{align*}
By using the following inequality
\begin{align*}
    \mathfrak{Re}(\text{Cov}(O_{\bm{\theta}} \xi , O_{\bm{\theta}})) &\leq \left(\sup_{\bm{\sigma}} |\xi(\bm{\sigma})| \right) \text{Var}(O_{\bm{\theta}}), \\
    &\ll E_{\bm{\theta}} \text{Var}(O_{\bm{\theta}}),
\end{align*}
we can conclude that the variance reduction $R$ is negative. This observation highlights the importance of the baseline in reducing the statistical noise of the energy gradients near convergence.

Similarly to the stochastic estimation of the variational energy using our ansatz, we can do the same for the estimation of the variational pseudo free energy $F_{\bm \theta}$ in Eq.~\eqref{eq:FreeEnergy}. More details can be found in the supplementary information of Ref.~\cite{VNA2021}. Finally, we note that the gradients steps in our numerical simulations are performed using Adam optimizer~\cite{AdamPaper}.

\section{Hyperparameters}
\label{app:hyperparams}

For all models studied in this paper, we note that for each annealing step, we perform $N_{\rm train} = 5$ gradient steps. Concerning the learning rate $\eta$, we choose $\eta = 10^{-3}$ during the warmup phase and the annealing phase and we switch to a learning rate $\eta = 10^{-4}$ in the convergence phase. We finally note that we set the number of convergence steps as $N_{\rm convergence} = 10000$. In Tab.~\ref{tab:hyperparams}, we provide further details about the hyperparameters we choose in our study for the different models. The meaning of each hyperparameter related to annealing is discussed in detail in Refs.~\cite{VNA2021, RNNAnnealing}.

Additionally, we use $M_e = 2 \times 10^6$ samples for the estimation of the entanglement entropy along with their error bars for the toric code. For the Bose-Hubbard model we use $M_e = 10^7$ samples to reduce the error bars on the TEE in Fig.~\ref{fig:BoseHubbard}. To estimate the TEE uncertainty from the Kitaev-Preskill construction, we use the standard deviation expression of the sum of independent random variables~\cite{ku1966notes}.

Finally, we note that to avoid fine-tuning the learning rate for each value of $V$ (between $4$ and $13$) in the Bose-Hubbard model, we target the normalized Hamiltonian
\begin{equation}
    \hat{H} = -\frac{1}{V} \sum_{\langle i,j \rangle} \left( b_i^{\dagger} b_j + b_i b_j^{\dagger} \right) + \sum_{\hexagon} n_{\hexagon}^2
\end{equation}
in our numerical experiments.

\begin{table*}[htp]
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|}\hline
       Figures & Parameter & Value \\\hline
      \multirow{4}{*}{2D toric code} & Number of memory units & $d_h = 60$ \\
            & Number of samples & $M = 100$ \\
            & Initial pseudo-temperature & $T_0 = 2$ \\
            & Number of annealing steps
             & $N_{\rm annealing} = 4000$ \\             
       \hline
      \multirow{4}{*}{Bose-Hubbard model (L = 6)} & Number of memory units & $d_h = 100$ \\
            & Number of samples & $M = 500$ \\
            & Initial pseudo-temperature & $T_0 = 1$ \\
            & Number of annealing steps
             & $N_{\rm annealing} = 10000$ \\            
       \hline     
         \multirow{4}{*}{Bose-Hubbard model (L = 8)} & Number of memory units & $d_h = 100$ \\
        & Number of samples & $M = 500$ \\
        &  Pseudo-temperature & $T_0 = 0$ \\
        & Number of steps
         &  $10000$ \\             
       \hline     
          
    \end{tabular}
    \caption{A summary of the hyperparameters used to obtain the results reported in this paper. Note that the number of samples $M$ corresponds to the batch size used during the training phase. Additionally, the $L = 8$ RNN model is initialized with the $L=6$ optimized RNN for the Bose-Hubbard model.} 
    \label{tab:hyperparams}
\end{table*}

\section{Kitaev-Preskill constructions}
\label{app:KP}

In this appendix, we provide details about the subregions used to calculate the TEE using the Kitaev-Preskill construction (see Sec.~\ref{sec:TEE}). For the 2D toric code, we use three spins for each subregion and for the Bose-Hubbard model in the Kagome lattice we increase the subregions sizes mitigate finite size effects~\cite{KitaevPreskill2006} as opposed to the 2D toric code that does not suffer from this limitation~\cite{Hamma2004}. The illustrations of these subregions are provided in Fig.~\ref{fig:KP_constructions}.
\begin{figure*}[htp]
    \centering
    \includegraphics[width = 0.9\linewidth]{figs/KP_constructions.pdf}   
    \caption{An illustration of the sub-regions $A, B, C$ chosen for the Kitaev-Preskill constructions. (a) For the 2D toric code, each sub-region has $3$ spins. We also do the same for the lattice sizes $L = 6,8,10$ in Fig.~\ref{fig:renyi2_tc}(b). For the hard-core Bose-Hubbard model on the Kagome lattice, we target two different system sizes. Panel (b) shows the construction for L = 6 and panel (c) provides the construction for $L = 8$. In panels (b) and (c), each site corresponds to a block of three bosons.}
    \label{fig:KP_constructions}
\end{figure*}

\section{RNNs and MES}
\label{app:RNN_MES}

The results in Sec.~\ref{sec:TC_results} indicate that the RNN wave function encodes a superposition of minimally entangled states (MES). Here we further investigate this statement by analyzing the expectation values of the average Wilson loop operators and the average 't Hooft loop operators. 

We define the average Wilson loop operators as 
\begin{equation}
\Hat{W}^{z}_d = \frac{1}{L} \left( \sum_{\mathcal{C}_d} \prod_{\sigma_j \in \mathcal{C}_d} \hat{\sigma}_j^z \right).
\label{eq:Wilson_loop}
\end{equation}
Here $d = h,v$ and $\mathcal{C}_h, \mathcal{C}_v$ are closed non-contractible loops illustrated in Fig.~\ref{fig:loop_operators}. A set of degenerate ground states of the toric code are eigenstates of the operators $\hat{W}^z_h$, $\hat{W}^z_v$ with eigenvalues $\pm 1$. Additionally, the two eigenvalues uniquely determine the topological sector of the ground state. In this case, the topological ground states can be labeled as $\ket{\xi_{ab}}$ with $a,b = 0,1$~\cite{Quasiparticle2012, fradkin_2013}. 

We can also define the average 't Hooft loop operators on non-contractible closed loops~\cite{fradkin_2013}, such that
\begin{equation}
\hat{W}^{x}_d = \frac{1}{L} \left( \sum_{\mathcal{C}_d} \prod_{\sigma_j \in \tilde{\mathcal{C}}_d} \hat{\sigma}_j^x \right),
\label{eq:tHooft_loop}
\end{equation}
where $d = h,v$ and $\tilde{\mathcal{C}}_h$, $\tilde{\mathcal{C}}_v$ correspond to horizontal and vertical loops as illustrated in Fig.~\ref{fig:loop_operators}. These operators satisfy the anti-commutation relations $\{ \hat{W}^z_h, \hat{W}^x_v \} = 0$ and $\{ \hat{W}^z_v, \hat{W}^x_h \} = 0$.

\begin{figure}[htp]
    \centering
    \includegraphics[width = 0.7\linewidth]{figs/loop_operators.pdf}
    \caption{An illustration of the vertical and the horizontal loops used to compute the Wilson loop operators (see Eq.~\ref{eq:Wilson_loop}) and the 't Hooft loop operators (see Eq.~\ref{eq:tHooft_loop}).}
    \label{fig:loop_operators}
\end{figure}

From the optimized RNN wave function ($L = 8$), we find $\langle \hat{W}^z_h \rangle =0.0009(2)$ and $\langle \hat{W}^z_v \rangle =-0.0039(2)$ which are consistent with vanishing expectation values. We also obtain $\langle \hat{W}^x_h \rangle = 0.999847(5)$ and $\langle \hat{W}^x_v \rangle = 0.999785(5)$ for the 't Hooft loop operators, which are consistent with $+1$ expectation values. These results are in part due to the use of a positive RNN wave function which force the expectation values $\langle \hat{W}^x_h \rangle$ and $\langle \hat{W}^x_v \rangle$ to strictly positive values and rules out the possibility to obtain, e.g., $\langle \hat{W}^x_h \rangle=-1$. 

By expanding the optimized RNN wave function in the $\ket{\xi_{ab}}$ basis, where $a,b$ are binary variables, we obtain
\begin{equation*}
    \ket{\Psi_{\text{RNN}}} \approx \sum_{ab} c_{ab} \ket{\xi_{ab}}.
\end{equation*}
Here $\{\ket{\xi_{ab}} \}$ correspond to the four topological sectors and they are mutually orthogonal. $c_{ab}$ are real numbers since we use a positive RNN wave function. Additionally, the basis states $\ket{\xi_{ab}}$ satisfy
\begin{align*}
     \hat{W}^z_h \ket{\xi_{ab}} =  (-1)^a \ket{\xi_{ab}}, \\
    \hat{W}^z_v \ket{\xi_{ab}} = (-1)^b \ket{\xi_{ab}}.
\end{align*}
From the anti-commutation relations, we can show that:
\begin{align*}
    \hat{W}^x_h \ket{\xi_{ab}} = \ket{\xi_{a\bar{b}}}, \\
    \hat{W}^x_v \ket{\xi_{ab}} = \ket{\xi_{\bar{a}b}}, \\
\end{align*}
where $\bar{a} = 1-a$ and $\bar{b} = 1-b$. By plugging the last two equations in the $\hat{W}^x_h$, $\hat{W}^x_v$ expectation values of our optimized RNN wave function, we obtain:
\begin{align*}
    2 c_{00}c_{01} + 2 c_{10}c_{11} \approx 1, \\
    2 c_{00}c_{10} + 2 c_{01}c_{11} \approx 1 .
\end{align*}
From the normalization constraint $1 = \sum_{ab} c_{ab}^2$, we deduce that:
\begin{align*}
    (c_{00} - c_{01})^2 + (c_{10} -c_{11})^2 \approx 0, \\
    (c_{00} - c_{10})^2 + (c_{01} - c_{11})^2 \approx 0 .
\end{align*}
As a consequence, we conclude that $c_{00} \approx c_{01} \approx c_{10} \approx c_{11}$, which means that the optimized RNN wave function is approximately a uniform superposition of the four topological ground states $\ket{\xi_{ab}}$. This observation is also consistent with vanishing expectation values of the operators $\hat{W}^z_h$, $\hat{W}^z_v$.

Furthermore, from Ref.~\cite{Quasiparticle2012} the MES of the toric code are given as follows:
\begin{align*}
    \ket{\Xi_1} = \frac{1}{\sqrt{2}} \left( \ket{\xi_{00}} + \ket{\xi_{01}} \right), \\
    \ket{\Xi_2} = \frac{1}{\sqrt{2}} \left( \ket{\xi_{00}} - \ket{\xi_{01}} \right), \\
    \ket{\Xi_3} = \frac{1}{\sqrt{2}} \left( \ket{\xi_{10}} + \ket{\xi_{11}} \right), \\
    \ket{\Xi_4} = \frac{1}{\sqrt{2}} \left( \ket{\xi_{10}} - \ket{\xi_{11}} \right).
\end{align*}
Thus, our RNN wave function can be written approximately as a uniform superposition of the MES $\ket{\Xi_1}$ and $\ket{\Xi_3}$, i.e.
\begin{equation*}
    \ket{\Psi_{\text{RNN}}} \approx \frac{1}{\sqrt{2}} \left( \ket{\Xi_1} + \ket{\Xi_3} \right).
\end{equation*}
