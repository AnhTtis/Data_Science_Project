%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}
	

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[sigconf]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[NICE '23]{}{April 11--14, 2023}{San Antonio, TX, USA}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
% \acmBooktitle{NICE 2023, April 11--14 2023, San Antonio, TX, USA} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Training a spiking neural network on an event-based label-free flow cytometry dataset}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Muhammed Gouda}
\authornote{Both authors contributed equally to this research.}
{\email{MuhammedGoudaAhmed.Gouda@ugent.be}}
\affiliation{%
  \institution{Ghent University - imec}
  % \streetaddress{P.O. Box 1212}
  \city{Gent}
  % \state{Ohio}
  \country{Belgium}
  % \postcode{43017-6221}
}

\author{Steven Abreu}
\authornotemark[1]
\email{s.abreu@rug.nl}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{University of Groningen}
  % \streetaddress{P.O. Box 1212}
  \city{Groningen}
  % \state{Ohio}
  \country{Netherlands}
  % \postcode{43017-6221}
}



\author{Alessio Lugnan}
\email{Alessio.Lugnan@UGent.be}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Ghent University - imec}
  % \streetaddress{P.O. Box 1212}
  \city{Gent}
  % \state{Ohio}
  \country{Belgium}
  % \postcode{43017-6221}
}


\author{Peter Bienstman}
\email{Peter.Bienstman@UGent.be}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Ghent University - imec}
  % \streetaddress{P.O. Box 1212}
  \city{Gent}
  % \state{Ohio}
  \country{Belgium}
  % \postcode{43017-6221}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Gouda and Abreu, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Imaging flow cytometry systems aim to analyze a huge number of cells or micro-particles based on their physical characteristics. The vast majority of current systems acquire a large amount of images which are used to train deep artificial neural networks.  However, this approach increases both the latency and power consumption of the final apparatus. In this work-in-progress, we combine an event-based camera with a free-space optical setup to obtain spikes for each particle passing in a microfluidic channel. A spiking neural network is trained on the collected dataset, resulting in 97.7\% mean training accuracy and 93.5\% mean testing accuracy for the fully event-based classification pipeline.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
\end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{datasets, neural networks, gaze detection, text tagging}

% %% A "teaser" image appears between the author and affiliation
% %% information and the body of the document, and typically spans the
% %% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

% motivation from flow cytometry / imaging side, then motivate event-based vision, then motivate SNN, then introduce training approach for SNN

Flow cytometry is a technique used to classify different types of cells based on characteristics such as size, shape, or fluorescence. Often, the cells used in flow cytometry are labeled with different biomarkers. Such biomarkers can lead to chemical interactions with the cells that impair the experimental data \cite{doan2018diagnostic}. Label-free, or stain-free, imaging flow cytometry collect images from flowing cells which have not been labelled. These images are used to train a machine learning model for classification. 

The goal of a flow cytometry setup is to classify with high accuracy and low latency (or, equivalently, high throughput). To achieve this, it is necessary to use a sophisticated high-speed camera and a lot of computing power to process large amounts of data. 

Recent work has shown that a neuromorphic event-based camera can achieve low latency with >1,000 frames per second \cite{HeEtAl2022Neuromorphic}. Importantly, the event-based camera does not record full frames at each time step, but rather records only changes in the scene at each pixel. This operating principle inherently filters out temporally redundant information. 

However, in order to use conventional computer vision algorithms on event-based data, it is usually necessary to convert the data into frames. A more natural approach is to process the event-based imaging data using event-based image processing algorithms. We propose the use of spiking neural networks (SNNs) to process the imaging data in a fully event-based and asynchronous way. In this work-in-progress, we show first results that we obtained by training a spiking neural network on event-based flow cytometry data on a GPU. To the best of our knowledge, this are the first results of event-based flow cytometry using spiking neural networks in the literature (Ref. \cite{HeEtAl2022Neuromorphic} uses an artificial neural network, and Ref. \cite{ZhangEtAl2022Work} does not report any performance results). 

\section{Methods}

We proceed by describing the experimental setup in Section \ref{ss:exp-setup} and the spiking neural network we trained in Section \ref{ss:snn}.

\subsection{Imaging flow cytometry}
\label{ss:exp-setup}

\begin{figure}[b]
\centering
\fbox{\includegraphics[width=0.44\textwidth]{opticalSetup.png}}
\caption{The free-space optical setup built in this work. Light emitted by the 1550 nm He-Ne laser is focused on a PMMA microfluidic channel. Diffraction, scattering and interference of light due to moving particles cause changes in the intensity of light incident on the event sensor.}
\label{fig: sketch3d}
\end{figure}

The experimental setup is shown in Figure \ref{fig: sketch3d}. Here, we send artificial PMMA microbeads of different sizes to a narrow microfluidic channel (of width 200 $\mu m$). The intensity of the light detected by the event-camera stays constant as long as no particle is moving across the field of view. This does not trigger any pixel to fire events. However, once particles enter the field of view,diffraction, scattering and interference of the light trigger firing events which we use later to train our spiking neural network. The fact that we obtain events only in the presence of particles significantly improves the memory efficiency of the overall system. The size of the dataset we get with our system is on the order of tens of gigabytes, whereas in a previous work using a traditional frame-based camera, the dataset was on the order of hundreds of gigabytes \cite{lugnan2020machine}. 

% here go details about the lab setup, including a figure

We used two different classes of spherical microparticles (class A of diameter 16 µm and class B with a diameter of 20 µm). We ran four separate experiments for each class of microparticles, where each experiment ran for $T_{exp} \approx 60 s$ for a total of 480 seconds of data. The accumulation time for a single particle is $T_{acc} \approx 10 ms$. Therefore, we have around 6,000 samples per experiment, or 24,000 samples in total per class. 
% We purchased the particles from PolyAn GmbH company based in Germany. 
We train the network for four different train-test splits, each split using a different experiment as testing data, and the remaining experiments as training data.

\subsection{Spiking neural network}
\label{ss:snn}

% here go details about the SNN, surrogate gradients, etc.
We pre-process our event-based imaging data using the Tonic library \cite{tonic2021}. The pre-processing involved a spatial downsampling from $640 \times 480 \times 2$ to $32 \times 24 \times 2$ (event polarity is left unchanged), and a temporal downsampling by passing the events for each neuron corresponding to a downsampled pixel of fixed polarity through a discretized leaky-integrate-and-fire (LIF) neuron:
% U[t+1] = βU[t] + I_{\\rm in}[t+1] - RU_{\\rm thr}
% U[t+1] = βU[t] + I_{\\rm syn}[t+1] - R(βU[t] + I_{\\rm in}[t+1])
\begin{align*}
    % s^{out}_i(n+1) &= \begin{cases}
    %     1 & \text{ if } \left( \beta u_i(n) + w s^{in}_i(n+1) \right) \geq u_{thr} \\
    %     0 & \text{ else}
    % \end{cases}\\
    s^{out}_i(n+1) &= 1 \text{ if } \left( \beta u_i(n) + w s^{in}_i(n+1) \right) \geq u_{thr} \text{ else } 0 \\
    r_i(n+1) &= \max_{t \in \{ 0, \ldots, t_{rf} \} } s^{out}_i(n+1-t) \\
    u_i(n+1) &= 0 \text{ if } \left( r_i(n+1) \right) \text{ else } \left( \beta u_i(n) + w s^{in}_i(n+1) \right)
\end{align*}
where $u_i(n)$, $s^{out}_i(n)$, $r_i(n)$ denote the membrane potential, binary spike output, and binary refractory period of neuron $i$ at timestep $n$, respectively. 
Further, $\beta = 0.9$ is the membrane decay rate, $w = 1.0$ is the synaptic weight, $u_{thr}=3.0$ is the threshold voltage, $t_{rf} = 2$ is the refractory period, $s^{in}_i(n)$ equals unity if there is an input event from the event-based camera in the $i$th $20 \times 20 \times 1$ patch of pixels at timestep $n$ (and zero otherwise).

For classification, we use a feedforward network of LIF neurons with an input layer of size $32 \times 24 \times 2=1536$, a hidden layer of size $100$, and an output layer of size $20$ (using population coding with $10$ neurons per output class). The neuron parameters are constant for all neurons in the network, with the membrane decay rate $\beta=0.9$ and membrane threshold $U_{thr}=0.5$. 
We use the Adam optimizer to optimize the mean squared error on the output spike rate, with a desired population spike rate of $20\%$ (correct) and $80\%$ (incorrect). Backpropagation is applied through a shifted Heaviside function in the forward pass and a fast sigmoid 
\begin{align*}
    S = \frac{U}{1+k\|U\|}
\end{align*}
% $S = U / (1+k\|U\|)$ 
with slope $k=75$ in the backward pass \cite{ZenkeGanguli2018}. We use snnTorch \cite{EshraghianEtAl2021Training} for our implementation. 
% \begin{align*}
%     S = \begin{cases}
%         1 & \text{ if } U \geq U_{thr} \\
%         0 & \text{ if } U < U_{thr}
%     \end{cases}
% \end{align*}
% and a fast sigmoid with slope $k=75$ as surrogate gradient for the backward pass \cite{ZenkeGanguli2018}:
% \begin{align*}
%     S &= \frac{U}{1 + k\|U\|} \\
%     \frac{\partial S}{\partial U} &= \frac{1}{(1+k\|U\|)^2}
% \end{align*}

\section{Results}

% performance plot, comparison to frame-based algorithm
We trained the spiking neural network described in Section \ref{ss:snn} for 10 epochs on a NVIDIA GeForce RTX 2080 Ti GPU with 11 GB of memory. Figure \ref{fig:snn-performance} show the training and testing performance for this experiment. The figure shows that the accuracy exceeds 98.5\% for all experiments, with the exception of the testing accuracy for the network trained on experiments 1, 3, and 4. More work is needed to analyze the anomalous performance on this data split.

\begin{figure}[H]
\centering
\includegraphics[width=0.47\textwidth]{plot.png}
\caption{Performance of the spiking neural network trained with backpropagation for ten epochs. Dashed lines show testing, solid lines show training. Different lines indicate which experiments were used for training, the remaining experiment was used for testing only.}
\label{fig:snn-performance}
\end{figure}

\section{Outlook}
\label{s:outlook}

% conclusion and outline of future work (probably will be able to present the future work already at the conference, since we want to finish a lot of it before April already)
As next steps, we are investigating the use of exact timing information for a time-to-first-spike classification that should yield even faster classification. In this work, we fixed the hyperparameters use to the values mentioned in Section \ref{ss:snn}. For further performance gains, we will optimize the hyperparameters and network architecture through a cross-validation procedure. Finally, we will transfer the SNN to a neuromorphic chip for a fully neuromorphic processing pipeline.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work was performed in the context of the European projects Neoteric (grant agreement 871330), Prometheus (grant agreement 101070195) and Post-Digital project (grant agreement 860360). 
% This project has received funding from the European Union's Horizon 2020 Research and Innovation Programme under the Marie Skłodowska-Curie grant agreement No. 860360 (POST DIGITAL).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
