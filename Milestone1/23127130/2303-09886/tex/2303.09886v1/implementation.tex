\section{Implementation}
\label{sect:implementation}
Unlike other correlators of which we are aware, the CPU parts of our
correlator are implemented entirely in Python, while the GPU kernels are written in
CUDA C++. While not generally known for its performance, Python's high-level
nature has lead to high developer productivity. All the compute-intensive work
is either performed on the GPU or handled by off-the-shelf libraries such as
spead2 or numpy that use C/C++ internally. Some code is carefully
written to ensure that Python does not get used for performance-critical inner
loops.

\subsection{Batching}
The programming model in CUDA (and other GPU programming interfaces) is
batch-oriented rather than based on a continuous stream of data. Large
batches of work (millions of threads) allow for sufficient parallelism to keep
the GPU fully utilized. We thus break the input stream into \emph{chunks} of a
few million samples. The output stream is similarly decomposed into chunks,
but for reasons we will explain later, they are not in one-to-one
correspondence. Because the Python code is involved at the batch level (rather
than on individual samples or packets), large batches also help amortize the
overheads of the relatively slow interpreter.

To ensure that all parts of the system are kept busy, we use a pipelined
design with components connected by queues of chunks, as shown in
Fig.~\ref{fig:queues}. In
particular, we need host-to-GPU transfers, GPU-to-host transfers, and GPU
computations to happen concurrently to maximize the overall throughput.
We use Python's asyncio library to manage these concurrent
operations.

\begin{figure}
  \centering
  \includegraphics{figures/queues.pdf}%
  \caption[Processing linked by queues]{Processing linked by queues. Where the
  diagram shows a circular buffer, the implementation uses one queue carrying
  full buffers forward and a second queue carrying empty buffers backwards.}%
  \label{fig:queues}%
\end{figure}

The PFB uses overlapping windows, which means that some
computations require data spanning an input chunk boundary. To support this,
each chunk is allocated on the GPU with some extra space at the end. The
prefix of the following chunk is then copied to this space, and computations
are performed on this expanded chunk. Provided that this extra space is at
least as large as the PFB window size, every PFB window can now be located
inside a single chunk, as shown in Fig.~\ref{fig:overlap}.

\begin{figure}%
  \centering
  \includegraphics{figures/overlap.pdf}%
  \caption[Overlapping chunks]{Overlapping chunks. The cross-hatched area of
  each chunk is copied from the data in the following chunk. The braces show
  the overlapping PFB windows.}%
  \label{fig:overlap}%
\end{figure}

\subsection{Networking}
We use the spead2 library (a high-performance implementation of the SPEAD
protocol) both to receive input heaps from the digitizers and to transmit output
heaps to the X-engines. On the receive side, it supports collecting multiple
heaps into a chunk,
reordering them as necessary based on timestamps\cite{spead2-chunking}, before
passing the chunk to the Python code for processing. It also allows the Python
code to control the allocation of the memory: we allocate it in CUDA pinned
memory, which allows it to be efficiently copied to the GPU.

It would have been simplest to treat the two polarizations jointly in the
receive code, placing them into a single chunk. Unfortunately, design
decisions in spead2 mean that would only allow a single thread to be used, and
we were not able to achieve the required performance in this manner. Thus,
each input chunk contains only a single polarization, and Python code is used
to pair up chunks with the same timestamp. This adds complexity because this
code needs to handle corner cases where one polarization is lost.

On the transmit side, spead2 allows heaps to be defined in advance with
pointers to the data, and then transmitted many times with the values pointed
to changing each time. It also allows a list of heaps to be submitted for
transmission in one step. When allocating the output chunks, we also
create the corresponding heap structures, thus minimizing the overhead
incurred at transmission time.

\subsubsection{Data transfer}
In the default implementation, each input sample is involved in four host
memory accesses, and each output sample in two, as shown in
Fig.~\ref{fig:transfer}(a). The NIC writes packets directly to RAM. The
spead2 library then assembles the packet payloads into chunks, again in RAM.
The final input step is that the GPU pulls the chunks from RAM. On the output
side, the GPU copies chunks to RAM, and the NIC pulls data from RAM. There is
no need for the CPU to copy data into individual packets, because the NIC is
able to gather the headers and payload for each packet from different
addresses.
\begin{figure}
  \centering
  \includegraphics{figures/transfer.pdf}%
  \caption{System data flow}%
  \label{fig:transfer}%
\end{figure}

NVIDIA's GPUs are able to map GPU memory into the system's address
space. This allows for a data flow that places less load on the system's RAM,
shown in Fig.~\ref{fig:transfer}(b).
Firstly, when spead2 assembles chunks, it writes directly to the GPU memory,
rather than to a staging area in host memory. Secondly, the NIC is given
pointers to GPU memory, rather than to a copy in host memory.

For the latter optimization the results are disappointing.
We found that having the NIC read directly from the GPU performs well only
when the GPU is idle; when its memory system is heavily used, the
achieved bandwidth drops to below \qty{120}{\giga\bit\per\second},
significantly less than the \qty{160}{\giga\bit\per\second} we are able to
achieve by staging through host memory.
NVIDIA recommends\cite{gpudirect} using a motherboard where the GPU and NIC
sit behind a PCIe switch, which is not the case for the systems we tested, and
so better results may be possible. It should also be noted that we were only
able to get this feature working at all on a data center GPU (A10) and not on
gaming GPUs.

Having the CPU write directly to GPU memory is more promising. We were able to
get the feature working on gaming GPUs, but with the limitation that only
\qty{256}{\mebi\byte} can be mapped. This significantly limits the maximum
chunk size, particularly when running multiple engines per GPU, and caused
performance to be lower overall.

\subsection{Coarse Delay}
The MeerKAT channelizer implements coarse delay by duplicating or removing
samples from the stream. This is easy to do with an FPGA, but less so with a
GPU since the samples are not streamed one at a time. Additionally, any PFB
windows overlapping the insertion or removal have a mix of different
coarse delays, potentially leading to artefacts. In practice the derivative of
delay is small (less than $3\times10^{-9}$ for sidereal targets) and so these
artefacts are rare.

Instead of inserting or removing samples, we handle coarse delay by
adjusting indices used to fetch samples. For each output spectrum (with a
given timestamp), we identify the appropriate position in the input stream at
which to load the data to achieve the necessary delay.
This approach allows for absolute delays that are essentially unlimited (even
negative), and every PFB window uses a consistent delay.
However, large step changes in delay (such as when
switching targets) can be problematic if they require access to older data that
has already been discarded. We can protect against this by increasing the size
of the overlap zone shown in Fig.~\ref{fig:overlap} by a number of samples
equal to the largest desired instantaneous delay change. This is not a major issue
for a dish array as a big change in delay center usually requires the dishes
to be slewed, during which time the data will be discarded anyway.

A similar problem is that the two polarizations may have different delays,
although the difference is expected to be very small since the delays are
dominated by the geometric component, which is common. Provided the overlap
zone is large enough, we can always find a pair of chunks with the same
timestamp that holds the data for both polarizations.

\subsection{Polyphase Filter Bank}\label{sec:implementation-pfb}%
In this subsection we describe only the filtering step (Eq.~\ref{eq:fir}). The
FFT step is described in the next subsection.

It is easier to implement the filter efficiently if the coarse delay can be treated
as a constant. As noted previously, coarse delay changes are rare, so we
handle this by splitting each chunk into regions with fixed coarse delay and
using a separate kernel invocation for each region. We
will thus ignore it in the following exposition, as it is simply an index
offset in the chunk. However, it should be noted that this offset cannot be
handled with pointer arithmetic, as pointers are byte-aligned while our 10-bit
samples are not.

The input samples can be viewed as a 2D array with width $2n$, in which the
$i^{\text{th}}$ column undergoes a FIR filter with weights $w_i$. This maps
easily to CUDA, with one thread for each column. The thread holds $w_i$ in
its registers, as well as a sliding window of input samples, thus
minimizing the number of memory accesses required. However, this does not
provide sufficient parallelism to fully occupy the GPU: at least hundreds of
thousands of threads are needed. We thus split each column into smaller
pieces, with a thread per piece. While the output space is completely
partitioned between threads, some inputs are loaded by multiple threads, as
shown in Fig.~\ref{fig:pfb-threads}. There is thus a trade-off between having
too few threads (and not fully utilizing the GPU) and too many (and performing
many redundant loads). A heuristic we found worked reasonably well (but which
may be need to be tuned to the GPU model) is to ensure that each thread
computes at least $8T$ outputs, where $T$ is the number of taps, unless this
would lead to fewer than \num{131072} ($2^{17}$) threads.

\begin{figure}
  \centering
  \includegraphics{figures/pfb-threads.pdf}%
  \caption[Relationship of threads to input samples in the PFB]{Relationship
  of threads to input samples in the PFB. Each black arrow shows the input
  samples loaded by a single thread, for a 4-tap PFB.}%
  \label{fig:pfb-threads}%
\end{figure}

\subsection{Fast Fourier Transform}\label{sec:implementation-fft}%
Due to coarse delays, each invocation of the PFB FIR kernel produces a
variable amount of data. We keep invoking it until we have enough data to fill
an output chunk. The last invocation may need to be truncated to avoid
overrunning the output buffer. Once this is done, we use a library to
perform a batched 1D Fast Fourier Transform.

We have considered two libraries for the FFT: cuFFT
(provided as part of CUDA) and vkFFT\cite{vkfft}. The latter is highly
configurable; we have used the defaults, except that the transformation is
out-of-place. Figure~\ref{fig:fft} shows the performance of these two
libraries on real-to-complex and complex-to-complex transforms. It is
clear that for batched 1D transforms with the sizes of interest, cuFFT has
superior performance, and so we do not consider vkFFT further.

\begin{figure}
  \centering
  \includegraphics{figures/fft.pdf}
  \caption[FFT library performance]{FFT library performance, with batched
  single-precision 1D transformations. All results use $2^{25}$ output
  values. Time is given per complex output value, and the horizontal line
  indicates the theoretical bound given by the memory bandwidth of the GeForce
  RTX~3070~Ti. Note that for real-to-complex transforms the input size is
  twice the output size.}
  \label{fig:fft}
\end{figure}

\subsubsection{FFT precision}
For MeerKAT the digitizer samples are 10-bit signed integers and the F-engine
outputs are 8-bit signed integers. Since half-precision floating point (FP16)
has a 10-bit mantissa, one might expect that a half-precision
FFT would be sufficient, as rounding errors would be smaller than the
quantization errors associated with the input and output.

While this may be true for a single instance of the FFT, it ignores the
statistical properties of the errors. Provided the signal is suitably
dithered\cite{dither}, quantization errors will have zero mean and will
not affect the expected value of the Fourier Transform. In contrast, the
rounding errors in the FFT are data-dependent, have spectral features, and
have non-zero mean.
It is known that fixed-point PFB implementations for radio
astronomy benefit from extra internal precision for the FFT\cite{pfb-precision}
(MeerKAT uses 22-bit registers\cite{meerkat-cbf}), but
we are not aware of any studies for low-precision floating point.
To test the effect of using an FP16 FFT, we synthesized some data as follows:
\begin{enumerate}
  \item Generate a tone at a fixed frequency.
  \item Quantize it to 10-bit signed integers, using a uniform dither.
  \item Perform a real-to-complex transform using cuFFT, in either FP16 or FP32.
  \item Repeat the above many times and average the results (in double precision).
  \item Square the absolute values of the averages to convert voltage to power.
\end{enumerate}
An example of the results are shown in Fig.~\ref{fig:fft16}. While the noise
floor is similar, there are harmonics at around \qty{-75}{\dB}.
The period of the features varies depending on the binary representation of
the channel number used for the tone, reflecting the structure of the FFT.
This is significantly higher than the noise floor of the polyphase filter bank
(Fig.~\ref{fig:channel-shape}), and in the MeerKAT environment could
potentially cause strong sources of narrow-band RFI\cite{rfi-environment} to
contaminate useful parts of the band. We thus chose to stick with FP32 for the
FFT.

\begin{figure}
  \centering
  \includegraphics{figures/fft16.pdf}
  \caption[FFT simulation using FP32 and FP16]{FFT simulation using FP32
  (left) and FP16 (right), averaged over $2^{24}$ iterations. Only power is
  shown (not phase). The tone is in channel 192 of 1024.}%
  \label{fig:fft16}
\end{figure}

% FFT optimization here, or elsewhere?
\subsection{Post-processing}
The remaining steps are applying fine delay, bandpass corrections, and
quantizing to 8-bit signed Gaussian integers. These are all quite
straight-forward to implement, as they can be computed independently for each
sample. We use inline PTX (CUDA's intermediate representation) to perform the
quantization with rounding and saturation.

Additionally, the data is transposed: the input is time-major,
channel-minor, but the layout expected by the X-engines is channel-major
within each heap. The transposition is done in shared memory to improve the
memory access pattern\cite{transpose}.

\subsection{Lost Data}
So far we have assumed a lossless network in which all expected packets actually
arrive. While we aim to have enough headroom that data loss does not routinely
happen, we still need to handle it gracefully. Within each chunk, spead2 sets
flags indicating which heaps were actually received. This information is
carried through the pipeline: if the window of input samples for a PFB has any
missing data, the output spectrum is flagged as unusable. Any output heap that
contains unusable spectra is simply not transmitted. This may also cause
usable spectra to be discarded, but since this is not expected to occur during
normal operation we have not attempted to optimize it.
