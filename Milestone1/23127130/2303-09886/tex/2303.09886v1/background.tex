\section{Background}
\label{sect:background}

\subsection{Channelization}
The exact steps performed by a channelizer are likely to vary from one
instrument to the next. The MeerKAT wide-band channelizers (both the original
FPGA design and our new GPU design) perform the following steps. These are
essentially the same as those performed by Cobalt \cite{cobalt}:

\begin{enumerate}
  \item
    Digitized samples are received. The MeerKAT digitizers produce 10-bit
    signed integer voltage samples. The data rate varies
    depending on the observing band, but is up to
    \qty{1750}{\mega\sample\per\second} (\qty{17.5}{\giga\bit\per\second})
    for each of the two polarizations of each antenna.
  \item
    A coarse delay is applied. Signals of interest will arrive at
    different antennas at different times due to the finite speed of light.
    Before correlating them, this delay must be corrected. Coarse delay is
    applied in the time domain and operates only on a whole number of samples.
  \item
    A polyphase filter bank (PFB)\cite{spectrometers} is applied to convert
    time-domain data to the frequency domain. A PFB is essentially a set of
    strided finite impulse response (FIR) filters followed by a Fourier
    transform. Suppose we wish to compute a spectrum with start time $t_0$
    with $n$ channels. The PFB has a set of weights $w_{i,j}$ ($0 \le i < 2n$,
    $0 \le j < T$ where $T$ is the number of ``taps'').  The filtered sample
    $g_t$ at time $t_0 + t$ ($0 \le t < 2n$, where time is measured in
    samples) is
    \begin{equation}
      g_t = \sum_{j=0}^{T-1} s_{t_0 + t - c + 2nj} \cdot w_{t, j},
      \label{eq:fir}
    \end{equation}
    where $s$ contains the original samples and $c$ is the coarse delay.
    The filtered samples $g_t$ are then put
    through a $2n$-element real-to-complex Fourier transform, from which only
    the $n$ non-negative frequencies are retained. The Nyquist frequency
    (which lacks phase information), is dropped so that the result is a
    convenient power-of-two size.

    MeerKAT supports \num{1024}, \num{4096} or \num{32768} frequency channels,
    and the MeerKAT Extension will additionally
    support \num{8192} channels. The MeerKAT PFBs use up to 16 taps in the FIR
    filters to improve the channel isolation.
  \item
    A fine delay is applied. This is the residual delay not corrected by the
    coarse delay step, and is corrected by phase rotation in the frequency
    domain.
  \item
    Bandpass correction is applied: each value is multiplied by a complex,
    channel-dependent correction factor.
  \item
    The internal representation is quantized to 8-bit signed Gaussian
    integers, arranged into packets and transmitted to the network.
\end{enumerate}
In the signal processing steps described above, the two polarizations remain
independent of each other. However, to maintain compatibility with the MeerKAT
packet formats, each output packet contains data from both polarizations, and
hence the channelizer needs to operate on both polarizations together. Doing
so also allows for new features in the future, such as correcting for
polarization leakage. We refer to a
pipeline performing all the steps above for two polarizations of a single
antenna as an F-engine. A single server may run multiple F-engines as
independent processes.

\subsection{Network Protocol}
MeerKAT uses the Streaming Protocol for Exchange of Astronomical
Data (SPEAD)\cite{spead}, deployed over multicast UDP. This is a
protocol for transmitting multi-dimensional arrays of data with
associated metadata (such as timestamps). The basic protocol data unit is the
\emph{heap}, which may be fragmented into multiple UDP packets and reassembled
by the receiver.

Digitizers send voltage samples in \num{4096}-sample heaps, each comprising a
single packet. The 10-bit samples are packed, so the heaps contain \num{5120}
bytes of payload. The two polarizations are sent independently.

To reduce the number of F-engine output heaps, each heap contains data for 256 spectra.
The data in a heap is a $c \times 256 \times 2$ array of Gaussian integers,
where $c$ is the number of channels sent to each consumer, and for
MeerKAT can be anywhere from 4 to \num{2048}. Output heaps may comprise
multiple UDP packets, as they are typically larger than the largest possible
UDP packet.

\subsection{Graphics Processing Units}
Our target GPUs are those from NVIDIA, and so we will use the terminology used
by CUDA (NVIDIA's programming toolkit). GPUs from other vendors are similar
but use different terminology. CUDA-capable GPUs have multiple levels of
parallelism:

\begin{description}
  \item[Threads] are the finest level, and are conceptually similar to CPU
    threads. Each thread has its own registers. Threads are programmed as if
    they have independent control flow, but in practice there are limitations
    to this, and dynamic control flow at the thread level can reduce
    performance.
  \item[Warps] are groups of 32 threads, and are the unit of scheduling. For
    best performance, all the threads in a warp execute the same instruction
    at the same time, and access adjacent memory locations.
  \item[Thread blocks] are sets of threads that execute concurrently on a
    single Streaming Multiprocessor (SM)---one of the hardware units of the
    GPU. Threads in the same block can communicate through a high-speed shared
    memory that is local to the SM. Each SM also has an L1 cache.
  \item[Grids] are the coarsest level. The CPU dispatches work to the GPU as a
    grid of thread blocks. Every thread executes the same program, but is
    assigned a unique index that allows it to be differentiated from other
    threads. A grid may contain more thread blocks than the GPU has the
    resources to handle concurrently, in which case some thread blocks may
    only start after others have completed.
\end{description}

The GPUs we have tested all connect to a host system via 16 lanes of a PCIe
4.0 bus. The CUDA busGrind tool typically shows that NVIDIA GPUs can
sustain \qty{26}{\giga\byte\per\second} for unidirectional traffic, and
\qty{21}{\giga\byte\per\second} each way for bidirectional traffic. This is
1--2 orders of magnitude less than the bandwidth of the RAM on the GPU and can
easily become a bottleneck for data streaming applications.

% Cite LOFAR paper
% Cite Wei's Galaxies paper?
