\section{Results}%
\label{sect:results}%
\subsection{Hardware}%
Unless otherwise noted, all results are for a GeForce RTX 3070 Ti GPU. To improve
reproducibility, we have locked the graphics and memory clocks to
\qty{1575}{\mega\hertz} and \qty{9251}{\mega\hertz} respectively, which gives
theoretical performance of \qty{19.35}{\tera\flop\per\second} (single
precision) and \qty{592}{\giga\byte\per\second}. Despite this, we have found
that the performance of the post-processing kernel drops by 20--25\% if it is
repeated thousands of times in a tight loop, so the results for that
kernel are measured on \qty{1000} iterations at a time. This does not seem to
occur when mixed with the other kernels in real-world usage.

The CPU is an AMD EPYC 7313P (Milan) with 16 cores, \qty{3}{\giga\hertz} base
clock and \qty{3.7}{\giga\hertz} boost clock, equipped with
\qty{64}{\gibi\byte} of DDR4-3200 on a Supermicro H12SSL-i motherboard.
We considered disabling the boost clock to
give more consistent results (similar to locking the GPU clocks), but found
that doing so made a huge reduction in performance and did not substantially
improve consistency. We thus chose to keep the boost clocks enabled so that
results correspond more closely to real-world usage. Our tests are
relatively short-running, and it is possible that performance will decline
in a system that runs continuously due to thermal limitations.

The network card is an NVIDIA ConnectX-6 Dx with dual
\qty{100}{\giga\bit\per\second} ports.

\subsection{Channel Response}
To measure the channel response, we use a simulated digitizer that generates a
common full-scale tone in both polarizations, but with independent dithering.
We then cross-correlate the F-engine outputs and integrate over
\qty{8}{\second} (we use a cross-correlation rather than an auto-correlation
so that the dithering noise is uncorrelated). By varying the frequency of the
tone by small (sub-channel) amounts, we can determine the channel response of
the engine. The 8-bit F-engine output does not have enough dynamic range to
give a full picture, so we use different gain settings for different tones.
Figure~\ref{fig:channel-shape} shows the result for \num{1024} and \num{32768}
channels and 16 taps. It also shows the theoretical ideal computed by taking
the Fourier transform of the PFB weights (a Hann-windowed sinc filter). There
is extremely good agreement down to a noise floor around \qty{-120}{\dB}. We believe the
noise floor is higher with fewer channels because the ratio of coherent
gain (gain for narrow-band signals) to incoherent gain (gain for white noise)
depends on the channel count.

\begin{figure}
  \centering
  \includegraphics{figures/channel-shape.pdf}%
  \caption{Channel response for \qty{1024} and \qty{32768} channels with 16 taps}%
  \label{fig:channel-shape}%
\end{figure}

\subsection{GPU Throughput}

\subsubsection{Polyphase filter-bank}
\begin{figure}
  \centering
  \includegraphics{figures/roofline-pfb.pdf}
  \caption[Roofline plot for PFB FIR filter]{Roofline plot for PFB FIR filter.
  Within each cluster, the points are for 16, 12, 10, 8 and 4 bits per sample
  from left to right. All results are for \num{32768} channels. The line
  indicates theoretical maximum performance.}
  \label{fig:roofline-pfb}
\end{figure}

Figure~\ref{fig:roofline-pfb} shows a ``roofline'' plot of the performance of
the pre-processing filter kernel, for \num{32768} channels (the results for
other channel counts are qualitatively similar). All the results are in the
left-hand side of the graph, indicating that memory accesses dominate the
performance. The configurations with up to 16 taps all use 75\% or more of the
available memory bandwidth. However, as the number of taps goes up, the
number of registers needed increases, the number of threads that can be run
concurrently decreases and the GPU's ability to hide memory latency is
reduced. With 32 taps, the theoretical occupancy (fraction of the theoretical
maximum number of concurrent threads) is 41.67\%.

\begin{figure}
  \centering
  \includegraphics{figures/efficiency-pfb.pdf}
  \caption[Efficiency of the PFB FIR filter]{Efficiency of the PFB FIR
  filter. All results are for \num{32768} channels.}
  \label{fig:efficiency-pfb}
\end{figure}
While the roofline plot shows that the memory accesses that do occur are
performed efficiently, it does not consider that some memory accesses are
redundant. The kernel loads many bytes more than once, and if this is not
absorbed by the caches, it will harm the throughput. The maximum potential
throughput of the kernel can be computed from the total size of the input
and output buffers and the theoretical bandwidth of the device.
Figure~\ref{fig:efficiency-pfb} shows the achieved efficiency relative to this
ideal, again for \num{32768} channels.

The results above all use factor-4 unzipping. This results in uncoalesced
memory writes, which reduces the performance by 4\% on average over the test
scenarios, and 19\% in the worst case.

\subsubsection{Fourier transform}
For power-of-two sizes from \num{64} up to \num{16384} (the largest size for
which cuFFT uses a single pass), the complex-to-complex FFT is memory bound:
arithmetic intensity is at most 4.5 flops per byte, and at least 87\% of the
memory bandwidth is used (both of these occur at the largest size).

\subsubsection{Post-processing}
As with the other kernels, the post-processing is memory-bound, with an
arithmetic intensity of 6--7 flops per byte. Figure~\ref{fig:efficiency-postproc} shows the
efficiency relative to the ideal of accessing every input and output value
once at the theoretical maximum bandwidth. The efficiency declines with
increasing channel counts because the memory access pattern causes some cache
lines to be loaded multiple times. For \num{32768} channels, an extra 7\%
memory traffic is generated.

\begin{figure}
  \centering
  \includegraphics{figures/efficiency-postproc.pdf}
  \caption{Efficiency of the post-processing kernel}
  \label{fig:efficiency-postproc}
\end{figure}

\subsubsection{Overall GPU throughput}
For even a mid-range GPU, the maximum sampling rate that can be handled is
limited by PCIe bandwidth rather than the computations on the GPU. For each
combination of PFB taps and input bit depth, we have estimated the GPU memory
bandwidth required to ensure that it does not become the bottleneck. To make
this estimate, we used the following process:
\begin{enumerate}
  \item Assume a PCIe bandwidth of \qty{160}{\giga\bit\per\second} in each
    direction (achievable on NVIDIA Ampere GPUs with a little headroom), and
    from this determine a sampling rate.
  \item Measure the time required to run all the kernels on our test system,
    and use linear scaling to determine a memory bandwidth of a hypothetical
    GPU that would run the kernels just fast enough.
  \item Since the measurement above does not include any PCIe transfers, add
    the GPU memory bandwidth required for transferring the inputs and outputs
    over PCIe. Similarly, add time to copy the prefix of each chunk to the
    suffix of the previous chunk. In both cases we assume 100\% efficiency
    in the memory accesses.
\end{enumerate}

Figure~\ref{fig:compute-bw} shows the results for \num{1024} and \num{32768}
channels. It may seem counter-intuitive that going from 8 to 16 bits
per sample reduces the required bandwidth. This occurs because the PCIe
bandwidth is kept constant and hence the sampling rate decreases. The bulk of
the on-GPU memory traffic is performed in single-precision floating point
rather than scaling with the input bit depth, and so decreasing the sample
rate decreases the memory bandwidth needed for that traffic.

\begin{figure}
  \centering
  \includegraphics{figures/compute-bw.pdf}
  \caption[Estimated GPU memory bandwidth]{%
    Estimated GPU memory bandwidth. This is the minimum bandwidth the GPU will
    require for the computations to become bottlenecked by a PCIe 4 bus.
    The top of each bar represents \num{32768} channels while the horizontal
    line near the top represents \num{1024} channels.
  }
  \label{fig:compute-bw}
\end{figure}

To validate this model, we can artificially limit the memory clock on our test
hardware to simulate a lower-end GPU. This is not a perfect test, since a
lower-end GPU would generally have fewer streaming multiprocessors and a
narrower memory bus, but should give some indication of the accuracy. Our GPU
only supports a few fixed memory frequencies, so we fix it to
\qty{810}{\mega\hertz}, which gives a theoretical bandwidth of
\qty{51.84}{\giga\byte\per\second}. At \num{32768} channels, 10-bit samples,
and 16 taps, the model indicates a maximum sampling rate of
\qty{1077}{\mega\sample\per\second}. In practice, we found
\qty{930}{\mega\sample\per\second} was the highest rate we could run the
full engine without falling behind the incoming data (to the nearest
\qty{10}{\mega\sample\per\second}). This shows that there are additional
overheads not accounted for by the model, but (at least for this case) they
are less than 15\%.

\subsection{System Tuning}\label{sec:results-system}%
We found that we needed to do a substantial amount of system-level tuning to
obtain good performance, using a combination of hardware placement, BIOS
settings and kernel settings.

To test the throughput of the whole system, we run either one or four
instances of the F-engine on the system under test. Input data is
provided by digitizer simulators (one per F-engine) running on another
machine. The output data is sent into the network (as multicast streams).
Using four engines is representative of how the code is expected to be
deployed for the MeerKAT Extension, where the highest sampling rate will be
\qty{1750}{\mega\sample\per\second}. In this case, each engine is assigned to
a single quadrant of the CPU, and hence to a single Core Complex Die (CCD).
Tests with a single engine are aimed at measuring the maximum bandwidth
achievable with the current implementation, and use one thread per CCD (one
network receive thread per polarization, one network transmit thread and the
main Python thread).

Since the UDP protocol in use is lossy, it is not possible to measure maximum
throughput directly. Rather, we repeat a number of experiments in which a
fixed sampling rate is chosen, and we observe the F-engine input over
\qty{20}{\second} to check if there are any gaps in the received timestamps
(indicating packet loss). The engine is allowed to run for a few seconds
before this observation period begins, as it is quite common for some packets
to be lost while the process ``warms up''. We then use a binary search to
determine the highest sampling rate for which no packets are lost, to the
nearest \qty{10}{\mega\sample\per\second}. As the sampling rate approaches the
critical rate at which the implementation can keep up, packet loss during the
\qty{20}{\second} window becomes a random event, and so we see variation of a
few percent even when the configuration is not changed.

These results should be seen as upper bounds, as running for \qty{20}{\second}
under ideal conditions (for example, with no changes to delay) does not
guarantee stable operation in real-world use.

\subsubsection{BIOS settings}
Table~\ref{tbl:downgrade-bios} shows achieved sampling rates in each case,
starting with our optimized system as a baseline and then showing the impact of
changing one setting at a time (except for the row marked ``BIOS defaults'').
These results all use \num{32768} channels,
16-tap PFBs and 10-bit digitizer samples (a representative configuration for
the MeerKAT Extension). The chunk size is $2^{26}$ samples for one
engine or $2^{24}$ samples per engine when using four engines.

\begin{table}
  \centering%
  \caption[Effect of BIOS settings on sampling rate]{%
    Effect of BIOS settings on sampling rate. Values marked with a *
    are the effective BIOS defaults (the nominal default in most cases is
    ``Auto''). Percentages are relative to the baseline configuration.}%
  \label{tbl:downgrade-bios}%
  \pgfplotstabletypeset[
    every head row/.style={
      before row={%
        \toprule
        &&&\multicolumn{2}{c}{1 engine} & \multicolumn{2}{c}{4 engines}\\
        \cmidrule(lr){4-5}%
        \cmidrule(lr){6-7}%
      },
      after row={\midrule},
    },
    every last row/.style={
      after row={\bottomrule},
    },
    col sep=comma,
    columns={setting,baseline,test,n1,n1p,n4,n4p},
    columns/setting/.style={
      column name=Setting,
      string type,
      column type=l,
    },
    columns/baseline/.style={
      column name=Baseline,
      string type,
      column type=c,
    },
    columns/test/.style={
      column name=Tested,
      string type,
      column type=c,
    },
    columns/n1/.style={
      column name={\si{MS/s}},
    },
    columns/n4/.style={
      column name={\si{MS/s}},
    },
    columns/n1p/.style={
      column name=\%,
      fixed zerofill,
      precision=1,
      dcolumn={D{.}{.}{-1}}{c},
    },
    columns/n4p/.style={
      column name=\%,
      fixed zerofill,
      precision=1,
      dcolumn={D{.}{.}{-1}}{c},
    },
    create on use/n1p/.style={
      create col/expr={\thisrow{n1} / 62.60},
    },
    create on use/n4p/.style={
      create col/expr={\thisrow{n4} / 20.80},
    },
    section start/.style={before row={\midrule}},
    every row no 2/.style={section start},
    every row no 5/.style={section start},
    every row no 10/.style={section start},
  ]{data/downgrade/bios.csv}%
\end{table}

The BIOS settings chosen are a combination of those recommended by
AMD\cite{epyc-workload-tuning,epyc-hpc-tuning} and our own experience and
experimentation. Not all of the settings recommended by AMD are available on
our motherboard.

The first group of settings (starting with APBDIS) relate to power management.
Because the F-engine operates on a batch of data then becomes idle until the next
batch is ready, it may cause some part of the system to drop into a
lower-power, less-performant state. There is usually a latency to return to
full performance, and if that is too high it can lead to data loss. In this
case it appears that only DF Cstates are beneficial. In smaller
microbenchmarks we have seen APBDIS cause poor performance at certain data
rates---usually not the highest data rates, but rather ones that are low
enough to allow the low-power state to be engaged.

CPU power management (P-states and C-states) is also important, but we chose
to control that through the operating system rather than the BIOS.

The next group relate to the way memory accesses are performed. By default,
memory addresses are interleaved across all the memory channels (1 NUMA node
per socket, or NPS1), but the memory channels can also be partitioned into two
or four sets (NPS2/NPS4) where the OS can allocate memory from specific sets.
This can reduce memory latency if the users of the memory (CPU cores or
PCIe devices) are located close to the memory controllers. A single PCIe bus
can also be designated as the ``preferred IO bus'', and will get priority when
there is contention for memory access. Finally, ``PCIe relaxed ordering''
allows PCIe transactions to proceed out-of-order under some circumstances,
which can improve utilization by preventing head-of-line blocking.

We expected NPS1 to produce sub-optimal results for 1 engine, because the load
is not evenly balanced across the system. However, we expected NPS4 to work
well for 4 engines, because each engine runs on one CCD and uses the nearest
memory channels, which should give ideal load balancing and minimal latency.
Our hypothesis is that the copy engines on the GPU perform coarse-grained
time-sharing between the processes, and hence only utilise half or a quarter
of the memory channels at a time rather than having in-flight transactions on
them all concurrently. This does match AMD's recommendation that workloads
requiring accelerator throughput should use NPS1.

We were surprised that setting the GPU as the preferred I/O device was
optimal. It is commonly recommended that the NIC is the preferred I/O
device, because it has real-time requirements and will drop packets if it is
not able to transfer them quickly. However, because the whole system is
real-time, low GPU throughput can also lead to packet loss, and early
experiments suggest that the GPU does not cope well with contention for memory
access.

The final set of options concern features that can be enabled. Using x2APIC
may reduce interrupt latency, but this does not appear to be important, and
differences may be just noise. Using the IOMMU seems to reduce performance.

\subsubsection{Kernel settings}
Table~\ref{tbl:downgrade-kernel} shows the effect of kernel settings,
similarly to Table~\ref{tbl:downgrade-bios}. The first two settings control
CPU frequency scaling and low-power CPU states, and can also be controlled via
the BIOS. The latter appears not to affect performance, presumably indicating
that the full workload is sufficiently intense to prevent the CPU from
entering these deep C-states. As with APBDIS and DF Cstates, it is possible
that lower-bandwidth workloads will actually perform worse if they are light
enough to allow these low-power states to be used.

By default, the NVIDIA NIC loops outgoing IP multicast traffic into the
receive path so that processes running on the same machine can receive the
traffic. While convenient, this creates a significant overhead in transmitting
multicast data. Disabling this behavior (by writing to
\verb"/sys/class/net/*/settings/force_local_lb_disable") improves performance
with four engines. The loopback behavior is automatically disabled if there is
only one process using ibverbs, which is why the single-engine case is
unaffected. With other ibverbs processes present (but stopped, and hence using
no CPU time) the rate is reduced to \qty{5580}{\mega\sample\per\second}.

The last four options aim to reduce CPU overhead and allow the code to run
more efficiently. In the 4-engine case we are not CPU-bound, which is why they
make no difference. The \texttt{vm.nr_overcommit_hugepages} setting allows
spead2 to allocate its buffers in huge pages, which can reduce the number of
translation look-aside buffer (TLB) misses.

NUMA balancing is a kernel
mechanism which monitors which cores are using which memory
pages\cite{epyc-hpc-tuning}; it is
implemented by periodically unmapping some pages, causing the next access to
page fault. While the results show no effect, we have found in
longer tests that these page faults can cause occasional high latency leading
to data loss.

We enable real-time scheduling for the processes to ensure that they get CPU
time whenever they need it. By default, Linux does not allow real-time
processes to use more than \qty{0.95}{\second} of every second, to prevent a
malfunctioning real-time process from locking up a system. We increase this to
\qty{0.999}{\second} to allow the processes more CPU time without completely
removing the protection. Surprisingly, this makes much more than a 4.9\%
difference. We hypothesize that this is because a process that uses less than
95\% CPU on average may still exceed it during some second, and when it does
so, the \qty{50}{\milli\second} it is stalled is long enough for the network
buffer to be overrun. On the other hand, a \qty{1}{\milli\second} stall is
short enough that a process with less than 99.9\% average usage can recover
from it.

% TODO: talk about mitigations?

\begin{table}
  \centering%
  \caption[Effect of kernel settings on sampling rate]{%
    Effect of kernels settings on sampling rate. The tested
    values are the kernel defaults. Percentages are relative to
    the baseline configuration.}%
  \label{tbl:downgrade-kernel}%
  \pgfplotstabletypeset[
    every head row/.style={
      before row={%
        \toprule
        &&&\multicolumn{2}{c}{1 engine} & \multicolumn{2}{c}{4 engines}\\
        \cmidrule(lr){4-5}%
        \cmidrule(lr){6-7}%
      },
      after row={\midrule},
    },
    every last row/.style={
      after row={\bottomrule},
    },
    col sep=comma,
    columns={setting,baseline,test,n1,n1p,n4,n4p},
    columns/setting/.style={
      column name=Setting,
      string type,
      column type=l,
    },
    columns/baseline/.style={
      column name=Baseline,
      string type,
      column type=c,
    },
    columns/test/.style={
      column name=Tested,
      string type,
      column type=c,
    },
    columns/n1/.style={
      column name={\si{MS/s}},
    },
    columns/n4/.style={
      column name={\si{MS/s}},
    },
    columns/n1p/.style={
      column name=\%,
      fixed zerofill,
      precision=1,
      dcolumn={D{.}{.}{-1}}{c},
    },
    columns/n4p/.style={
      column name=\%,
      fixed zerofill,
      precision=1,
      dcolumn={D{.}{.}{-1}}{c},
    },
    create on use/n1p/.style={
      create col/expr={\thisrow{n1} / 62.60},
    },
    create on use/n4p/.style={
      create col/expr={\thisrow{n4} / 20.80},
    },
  ]{data/downgrade/kernel.csv}%
\end{table}

\subsubsection{Hardware placement}
The motherboard has five x16 PCIe 4.0 slots, but performance-wise they are not all
the same. The I/O die of the CPU is split into four quadrants. Each quadrant
supports 32 PCIe lanes, but we found that a quadrant is not able to sustain
full-bandwidth transfers between these lanes and system memory: the maximum is
around \qty{36}{\giga\byte\per\second} in each direction. We thus found
extremely poor performance when placing the GPU and the network interface card
(NIC) in a pair of slots connected to the same quadrant.

Even when using slots attached to different quadrants, not all combinations
are equal. Table~\ref{tbl:slots} shows the results with various combinations
of slots, and Fig.~\ref{fig:slots} shows the association of the slots to the
quadrants of the CPU\cite{H12SSL}.
\begin{table}
  \centering
  \caption[Effect of PCIe slots on performance]{%
    Effect of PCIe slots on performance. Percentages are relative to
    the top row, which is the baseline used in other results.}%
  \label{tbl:slots}%
  \pgfplotstabletypeset[
    every head row/.style={
      before row={%
        \toprule
        &&\multicolumn{2}{c}{1 engine} & \multicolumn{2}{c}{4 engines}\\
        \cmidrule(lr){3-4}%
        \cmidrule(lr){5-6}%
      },
      after row={\midrule},
    },
    every last row/.style={
      after row={\bottomrule},
    },
    col sep=comma,
    columns={NIC slot,GPU slot,n1,n1p,n4,n4p},
    columns/n1/.style={
      column name={\si{MS/s}},
    },
    columns/n4/.style={
      column name={\si{MS/s}},
    },
    columns/n1p/.style={
      column name=\%,
      fixed zerofill,
      precision=1,
      dcolumn={D{.}{.}{-1}}{c},
    },
    columns/n4p/.style={
      column name=\%,
      fixed zerofill,
      precision=1,
      dcolumn={D{.}{.}{-1}}{c},
    },
    create on use/n1p/.style={
      create col/expr={\thisrow{n1} / 62.60},
    },
    create on use/n4p/.style={
      create col/expr={\thisrow{n4} / 20.80},
    },
  ]{data/downgrade/slots.csv}%
\end{table}
\begin{figure}
  \centering
  \includegraphics{figures/slots.pdf}%
  \caption{Mapping of x16 PCIe slots to CPU quadrants on H12SSL-i motherboard}%
  \label{fig:slots}%
\end{figure}

\subsection{Power Consumption}
We used the 4-engine test case to measure power consumption, as it places
greater demand on the system (by virtue of having greater total bandwidth). We
used a sampling rate of \qty{2000}{\mega\sample\per\second}, and other
parameters are the same as for the system tuning results.

With the GPU clocks locked to the base values, the power consumption for the
GPU, as reported by \texttt{nvidia-smi}, is \qty{156}{\watt} and the power for
the whole system, as reported by the baseboard management controller (BMC) is
\qty{407}{\watt}. Unlocking the clocks causes power usage to increase by
\qty{73}{\watt}. On the other hand, the graphics clock can be reduced as low as
\qty{660}{\mega\hertz} without causing any loss of data, but doing so saves only
\qty{8}{\watt} compared to using the base clocks.

When using a sampling rate of \qty{1712}{\mega\sample\per\second}, \num{4096}
channels (a common configuration for MeerKAT), and base GPU clocks, the system
power usage is \qty{388}{\watt}, or \qty{97}{\watt} per antenna. Within the
margins of error, this is the same per-antenna power consumption as the
current SKARAB (FPGA) platform used in MeerKAT. It should be noted that the
SKARAB platform is almost a decade old and hence is not representative of the
power consumption of more modern FPGAs.
