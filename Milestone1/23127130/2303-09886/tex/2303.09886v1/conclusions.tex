\section{Conclusions and future work}%
\label{sect:conclusions}%
We have built a wide-band channelizer that is able to process the data for
four MeerKAT antennas on a single commodity GPU, and which implements all the
features of the existing FPGA-based wide-band channelizer. The throughput is
limited by the PCIe bandwidth of the GPU. The main outstanding work to make it
ready for the MeerKAT Extension correlator is the addition of a narrow-band
mode. We have done some prototyping of a low-pass filter kernel, and are
confident that it will be possible to implement concurrent wide-band and
narrow-band modes within the same pipeline.

The computations on the GPU are not a bottleneck for our chosen GPU (RTX 3070
Ti). Furthermore, there are GPUs available with significantly higher memory
bandwidth, so given sufficient budget, we do not expect them to become a
bottleneck for any use cases. We have thus not tried to squeeze out all the
possible performance. Nevertheless, there may be value in further
optimizations to allow cheaper and less power-hungry GPUs to be used.
Here are a number of high-level optimizations we have considered:
\begin{itemize}
  \item We have split off a small part of the FFT into the other kernels, but
    perhaps it can be completely fused, with step 2 of the six-step FFT merged into
    the PFB FIR kernel. The challenge here
    is that performing an FFT pass requires a fairly specific mapping of data
    to threads and thread blocks, which might not be compatible with the
    mapping currently used by those kernels. For channel counts that are low
    enough to support a single-pass FFT, it may be possible to fuse all three
    kernels together.
  \item While we have noted issues with computing the FFT in FP16, it may be
    feasible to use FP16 for the inputs and/or outputs of the FFT, with the
    internal computations done in FP32. The PFB FIR could then also
    potentially be performed in FP16, which would reduce register pressure and
    allow more taps to be used at the same throughput.
\end{itemize}

A common theme in these optimizations, as well as the optimizations we have
already implemented, is that modular design does not work well for
memory-bound applications. For example, we started by treating the PFB FIR,
the FFT and the post-processing as three independent modules, but to improve
performance we had to redistribute functionality between them, causing tight
coupling. Similarly, the PFB FIR kernel and the post-processing kernel are
tightly coupled to the input and output data formats, and cannot be used as-is
for a system that expects
different formats. This makes it difficult to create optimal yet
reusable code that can be mixed and matched in stream processing frameworks
such as Bifrost\cite{bifrost}, which use GPU memory as an interface boundary.

Because the implementation was originally designed for MeerKAT, it did not
target higher data rates per antenna. While ingress rates exceeding
\qty{120}{\giga\bit\per\second} are possible, they are limited by the
single-core performance of spead2. This is not a fundamental limitation, as
the network receive could be distributed across several threads, or spead2
could be replaced by a bespoke library tailored to the exact packet layout. We
expect that input rates of \qty{160}{\giga\bit\per\second} could be
achieved, as they are for the multi-engine case.

The results presented all use a single GPU. We have also experimented
with using two GPUs and two NICs per server (on a different server).
Unfortunately, performance does not scale linearly, because the system memory
bandwidth becomes a bottleneck, and we are forced to use sub-optimal PCIe
slots. Recently released CPUs may help with bandwidth: EPYC 9004-series processors double
the PCIe bandwidth (with PCIe 5.0) but more than double the memory bandwidth
(from \qty{205}{\giga\byte\per\second} to
\qty{461}{\giga\byte\per\second})\cite{epyc-genoa-datasheet},
while Xeon Max CPUs have on-board high bandwidth memory (HBM)\cite{xeon-max}.
