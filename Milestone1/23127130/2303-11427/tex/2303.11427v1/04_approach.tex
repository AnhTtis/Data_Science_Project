
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REMEMBER STORYTELLING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Learning to Precode using Soft Actor-Critic}
\label{sec:sac}

In deep \gls{rl}, an agent learns to approximately maximize a performance metric by interacting with a system and adjusting its behavior according to the system feedback.
Here, the metric is the sum rate \( \reward \)~\eqref{eq:sumRate} and the behavior corresponds to the precoding algorithm.
In this section, we introduce the \acrfull{sac}~\cite{haarnoja_soft_2019} \gls{rl} method,
which is characterized by rewarding exploration where uncertainty is high, thereby generating high quality data samples.
%Actor-Critic methods employ two types of \glspl{nn}, \glspl{ann} and \glspl{cnn}, to be explained in the following.
%The \gls{sac} method is characterized by using the \gls{ann} to produce stochastic behavior with high entropy, thereby stimulating explorative interactions that generate high quality data samples.
%This section introduces the components and learning loop of the \gls{sac} method.

Our \gls{sac} implementation uses five components, shown in \reffig{fig:sacprocess}: 1)~a Pre-Processing step, 2)~two \glspl{cnn} \( \criticnetworksca_{1} \) and \( \criticnetworksca_{2} \), 3)~an \gls{ann} \( \actornetworkvec \), 4)~an Experience Buffer, and 5)~a Learning Module.
We denote the \gls{nn} weights as \( {\paramscritic{1}, \paramscritic{2}, \paramsactor} \) for \( \criticnetworksca_{1}, \criticnetworksca_{2}, \actornetworkvec \), respectively.
%Two \glspl{cnn} with independent initialization are used by \gls{sac} as selecting conservatively from multiple mappings has been shown to stabilize the learning process.
The \gls{ann} will form the precoding matrix \( \precodingmatrix \). In \gls{sac}, it produces stochastic output using the reparametrization-trick:
For the \(\actionindex\)\nobreakdash-th entry~\( \actionsca_{\actionindex} \) of an output vector \( \actionvec \), the \gls{ann} produces two outputs, one of which is considered a mean \( \actornetworksca_{\actionindex, \actornetworkmean} \) and the other a scale \( \actornetworksca_{\actionindex, \actornetworkscale} \). The outputs \( {\actionsca_{\actionindex} \sim \mathcal{N}(\actornetworksca_{\actionindex, \actornetworkmean}, \actornetworksca_{\actionindex, \actornetworkscale})}\) are then sampled from a Normal distribution.
%For each scalar output~\( \actionsca_{\actionindex} \) with index \( \actionindex \) in the output vector \( { \actionvec} \) of the agent, the \gls{ann}~\( \actornetworkvec \) has two outputs, one of which is considered as the mean \( \actornetworksca_{\actionindex, \actornetworkmean} \) and the other as the scale \( \actornetworksca_{\actionindex, \actornetworkscale} \) of a Normal distribution.
%The \gls{ann}, which will ultimately output the values that constitute a precoding matrix \( \precodingmatrix \) in inference, produces stochastic output using the reparametrization-trick: For each desired scalar output~\( \actionsca_{\actionindex} \) of the agent, the \gls{ann}~\( \actornetworkvec \) has two outputs, one of which is considered as the mean \( \actornetworksca_{\actionindex, \actornetworkmean} \) and the other as the scale \( \actornetworksca_{\actionindex, \actornetworkscale} \) of a Normal distribution.
%In the following, when talking about the \gls{ann} output \( {\actornetworkvec(\cdot) = \actionvec} \), we consider the outputs~\( {\actionsca_{\actionindex} \sim \mathcal{N}(\actornetworksca_{\actionindex, \actornetworkmean}, \actornetworksca_{\actionindex, \actornetworkscale})} \) after sampling from the Normal distribution.

\begin{figure}[!t]
    \centering
    \input{fig/flowchart}
    \caption{\gls{sac} Precoder process flow. Top row describes inference, bottom row describes learning. Blue arrows relate to learning, red arrows show \gls{nn} parameter updates.}
    \label{fig:sacprocess}
\end{figure}

A discrete inference and learning step \( \timeindex \) looks as follows: First, the Pre-Processing step prepares the amplitude and phase components of the complex valued erroneous \gls{csit}~\( {\tilde{\csimatrix}_{\timeindex} \in \numberscomplex^{\numusers \times  \numsatellites\numantennasper}} \) in a flat state vector~\( {\statevec_{\timeindex} \in \numbersreal^{\num{1}\times \num{2}\numsatellites\numantennasper\numusers}} \) as input for the \gls{nn}.
%decomposes the complex valued erroneous \gls{csit}~\( {\tilde{\csimatrix}_{\timeindex} \in \numberscomplex^{\numusers \times  \numsatellites\numantennasper}} \) into its real amplitude and phase components and prepares everything in a flat state vector~\( {\statevec_{\timeindex} \in \numbersreal^{\num{1}\times \num{2}\numsatellites\numantennasper\numusers}} \) as input for the \gls{nn}.
%For example, given a complex-valued \gls{csit} matrix \( {\csimatrix_{\timeindex} \in \numberscomplex^{\numsatellites\numantennasper \times \numusers}} \), it converts to a state vector \( {\statevec_{\timeindex} \in \numbersreal^{\num{1}\times \num{2}\numsatellites\numantennasper\numusers}} \).
Based on this input vector~\( \statevec_{\timeindex} \) and its current parameters~\( \paramsactortime \), the \gls{ann}~\( {\actornetworkvec} \) will output an action vector \( {\actornetworkvec_{\paramsactortime}(\statevec_{\timeindex}) = \actionvec_{\timeindex} \in \numbersreal^{\num{1} \times \num{2}\numsatellites\numantennasper\numusers}} \). 
%Next, the \gls{ann}~\( {\actornetworkvec} \), based on this input vector~\( \statevec_{\timeindex} \) and its current parameters~\( \paramsactortime \), will output an action vector \( {\actornetworkvec_{\paramsactortime}(\statevec_{\timeindex}) = \actionvec_{\timeindex} \in \numbersreal^{\num{1} \times \num{2}\numsatellites\numantennasper\numusers}} \). 
%This vector \( \actionvec \) is then reshaped into a precoding matrix \( {\precodingmatrix_{\timeindex} \in \numberscomplex^{\numsatellites\numantennasper \times \numusers}} \) as follows. First, a half-length complex vector is constructed by multiplying the latter half of \( \actionvec \) by the imaginary unit \( j \) and adding it to the first half, discarding the latter half. Then the half-length complex vector is reshaped to the correct precoding matrix dimensions and the precoding matrix is normalized to the available transmit power per satellite, same as the \gls{mmse} precoder from \refsec{sec:MMSE}.
This real-valued vector \( \actionvec \) is then reshaped into a complex-valued precoding matrix \( {\precodingmatrix_{\timeindex} \in \numberscomplex^{\numsatellites\numantennasper \times \numusers}} \) with entries
\begin{align*}
    \precodingentry^\antidx_{\timeindex, \useridx, \satidx} = \actionvec_{t, \useridx+ \satidx+ \antidx} + j \actionvec_{\timeindex, \numantennasper+\useridx+ \satidx+ \antidx },
\end{align*}
and is normalized to the available transmit power per satellite~$\transmitpower/\numsatellites$, same as the \gls{mmse} precoder.
Subsequently, precoding is performed, the resulting sum rate~\( \reward_{\timeindex} \) \refeq{eq:sumRate} is calculated. The data sample of state~\( \statevec_{\timeindex} \), action~\( \actionvec_{\timeindex} \) and result~\( \reward_{\timeindex} \) is saved in the Experience Buffer. This concludes the inference part.

Next, to improve the quality of the \gls{ann}'s outputs, its weights~\( \paramsactortime \) must be tuned. The Actor-Critic method operates as follows: We wish to update the weights to maximize the mapping of \( (\statevec, \actionvec) \rightarrow \reward \), outputting precoding that maximizes the sum rate. This mapping is, however, unknown to the learning method.
Given the collected data samples from the Experience Buffer, the \glspl{cnn} can approximate a mapping~\( {\criticnetworksca(\statevec, \actionvec) = \rewardapprox } \). If the true mapping is approximated sufficiently well, the \gls{ann}'s weights~\( \paramsactor \) can be updated to maximize the known mappings~\( \criticnetworksca \).
Weight updates are performed once per training iteration~\( \timeindex \) on both \glspl{cnn}~\( {\criticnetworksca_{1}, \criticnetworksca_{2}} \) and \gls{ann}~\( \actornetworkvec \).
%so that the \glspl{cnn} keep step with the \gls{ann}'s behavior.
The \gls{cnn}~\( {\criticnetworksca_{1}, \criticnetworksca_{2}} \) are updated in a supervised learning manner, minimizing the squared distance
\begin{align*}
    \losscritic =
        (\criticnetworksca_{\paramscritictimegeneric}(\statevec_{\timeindex}, \actionvec_{\timeindex}) - \reward_{\timeindex})^2
    = (\rewardapprox_{\timeindex} - \reward_{\timeindex})^2
\end{align*}
between the approximation \( \hat{\reward}_{\timeindex} \) given current weights~\( \paramscritictimegeneric \) and the data sample target~\( \reward_{\timeindex} \).
We then construct an \gls{ann} loss
\begin{align*}
    \lossactorvalue = 
        -\min(
            \criticnetworksca_{1, \paramscritictime{1}}(\statevec_{\timeindex},
            \actornetworkvec_{\paramsactortime}(\statevec_{\timeindex})),
            \criticnetworksca_{2, \paramscritictime{2}}(\statevec_{\timeindex}, \actornetworkvec_{\paramsactortime}(\statevec_{\timeindex}))
        )
\end{align*}
that minimizes the negative approximate expected results~\( \hat{\reward}_{\timeindex} \), thereby maximizing the approximate expected results. Selecting conservatively from multiple, independently initialized mappings \( \criticnetworksca \) has been shown to stabilize the learning process.

This loss, however, gives no incentive for the \gls{ann} to maintain its stochastic output's scales~\( \actornetworksca_{\actionindex, \actornetworkscale} \), \ie incentive to explore new data samples where uncertain. During the learning process, insufficient exploration might lead to an incomplete data set that lacks information required to discover solutions near the global optimum.
To prevent the \gls{ann} from contracting its scales~\( \actornetworksca_{\actionindex, \actornetworkscale} \) too rapidly, the \gls{sac} method additionally adds an entropy loss
\begin{align*}
    \lossactorentropy = 
        \frac{1}{\num{2}\numsatellites\numantennasper\numusers}
        \sum_{\actionindex=1}^{\num{2}\numsatellites\numantennasper\numusers}
            \exp(\logentropyscale) \log(\pi(\actornetworksca_{\actionindex, \paramsactortime}(\statevec_{\timeindex}), \paramsactortime)),
\end{align*}
where \( \pi(\actornetworksca_{\actionindex,\paramsactortime}(\statevec_{\timeindex}), \paramsactortime) \) is the probability of \( \actornetworksca_{\actionindex, \paramsactortime}(\statevec_{\timeindex}) \) given the current weights~\( \paramsactortime \) of the \gls{ann}~\( \actornetworkvec \) and \( \logentropyscale \) is a weighting factor.
In the mean sense, this loss is directly minimized by increasing the \gls{ann} output variance and therefore represents the necessary counterweight to the first loss~\( \lossactorvalue \).
The scaling factor \( \logentropyscale \) is updated iteratively to keep the entropy roughly constant over the duration of training, \ie it is adjusted whenever the entropy falls above or below a heuristic target value. Summarily, the \gls{ann}~\( \actornetworkvec \) updates its weights~\( \paramsactor \) to minimize a composite loss \( {\lossactor = \lossactorvalue + \lossactorentropy} \).
Both \gls{ann} and \gls{cnn} parameter updates are performed using \gls{sgd}-like optimization on batches of \( \learningbatchsize \)~data samples drawn from the Experience Buffer with uniform probability.

In the following section, we evaluate specific implementation details and the performance of the learned precoder for given scenarios.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE TABLE
%\begin{table}[!t]
%	\renewcommand{\arraystretch}{1.3}
%	\caption{A Simple Example Table}
%	\label{tab:table_example}
%	\centering
%	\rowcolors{2}{white}{gray!10} 
%	\begin{tabular}{cc}
%		\hline
%		\bfseries First & \bfseries Next\\
%		\hline\hline
%		1.0 & 2.0\\
%		1.0 & 2.0\\
%		\hline
%	\end{tabular}
%\end{table}

%% EXAMPLE FIGURE
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=2.5in]{myfigure}
%	\input{figures/myfigure.pgf}
%	\caption{text}
%	\label{fig:figure_example}
%\end{figure}
