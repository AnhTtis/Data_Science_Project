
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REMEMBER STORYTELLING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Integrating \gls{ntn}, e.g. satellites and \glspl{uav}, into current terrestrial infrastructure is one of the important pillars in the development of the sixth-generation standard of mobile communication networks \cite{3GPP.TR.38.863}. So-called holistic 3D networks will enable ubiquitous global coverage, provide capacity for temporally and locally varying traffic demands and enhance the robustness of terrestrial network infrastructure \cite{Leyva-Mayorga2020, Qu}. However, a host of challenges is introduced due to the high dynamism of \gls{ntn} devices.
\gls{leo} satellites have especially fast-changing \gls{los} channels, which are mainly characterized by the relative positions between satellites and users.
To increase spectral efficiency through frequency reuse, precoding based \gls{sdma} is used in satellite communications~\cite{vazquez2018precoding}.
%satellite communications commonly combine \gls{sdma} and precoding \cite{vazquez2018precoding}. 
However, the performance suffers due to imperfect positional information~\cite{MaikBeamspace,liu2022robust}.
%imperfect positional information degrades the precoding performance~\cite{MaikBeamspace,liu2022robust}.

Finding a precoding algorithm that maximizes performance metrics such as the sum rate for multiple geometric constellations while also showing robustness against imperfect positioning and channel estimates can prove challenging. In light of this, \gls{ml} methods present themselves as an attractive choice. \gls{ml} can be used to approximate a viable algorithm where the optimum is either infeasible to determine or wholly unavailable. \gls{dl} in particular has demonstrated tremendous potential on such problems in the past decade, \cite{mnih2013playing, dahrouj_overview_2021}.
Applying \gls{dl} to precoding has recently started to gather more attention, primarily in terrestrial communications, \eg \cite{zhang_data_2022} use supervised learning to approximate a lower complexity \gls{mmse} precoder, \cite{lee_deep_2020} show the ability of \gls{rl} precoders to optimally learn on toy scenarios without interference and\cite{sohrabi_robust_2020} use an autoencoder structure to learn robust precoding and decoding under imperfect channel knowledge.
In the \gls{leo} satellite context, \cite{liu2022robust} have extended their work on robust precoding pertaining imperfect positional knowledge for a single satellite scenario by a supervised low-complexity approximation.
In this paper, we will %opt to 
use model-free deep \gls{rl}. In \gls{rl}, an agent probes the environment (\ie selecting a precoding matrix and observing the result), thereby generating data to learn from, to gain understanding of the system dynamics and adjust their behavior to maximize an objective.
%While deep \gls{rl} is typically applied to sequential decision problems such as {\color{red}example}, it is also suitable for time invariant decision processes, in fact, many {\color{red} value based} \gls{rl} algorithms gain convergence stability in the absence of time dependence.
By using this approach, no assumptions about the error modeling need to be made; it is instead discovered and inferred from the data.
However, data-driven learning requires data samples containing high information content to learn efficiently. For this reason, we use the \gls{sac} learning algorithm~\cite{haarnoja_soft_2019}. \gls{sac} encourages exploring new data samples where the algorithm's understanding is low, generating the necessary high quality data faster than random exploration.

In the next section, we introduce the system model of cooperative multibeam satellite communication, the applied \gls{csit} error models, typical precoding approaches, and the sum rate maximization problem. Following that, we explain the \gls{sac} method as it is used in this paper. We then apply \gls{sac} to maximize the sum rate in the presence of \gls{csit} error, and discuss \& contrast the performance against common \gls{mmse} and \gls{oma} precoding. Finally, we briefly discuss the scalability of the model case presented in this paper. For brevity, we assume prior knowledge of deep \glspl{nn}~\cite{goodfellow_deep_2020}.
 
\textit{Notations}: Lower and upper boldface letters denote vectors $\mathbf{x}$ and matrices $\mathbf{X}$ with $\mathbf{I}_N$ being an identity matrix of size $N \times N$. Transpose and Hermitian operators are indicated by $\{\cdot\}^\text{T}$ and $\{\cdot\}^\text{H}$, whereas $\circ$ is the Hadamard product. $|\cdot |$ and $\| \cdot \|$ signify the absolute value and Euclidean norm, respectively.
 
 
 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE TABLE
%\begin{table}[!t]
%	\renewcommand{\arraystretch}{1.3}
%	\caption{A Simple Example Table}
%	\label{tab:table_example}
%	\centering
%	\rowcolors{2}{white}{gray!10} 
%	\begin{tabular}{cc}
%		\hline
%		\bfseries First & \bfseries Next\\
%		\hline\hline
%		1.0 & 2.0\\
%		1.0 & 2.0\\
%		\hline
%	\end{tabular}
%\end{table}

%% EXAMPLE FIGURE
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=2.5in]{myfigure}
%	\input{figures/myfigure.pgf}
%	\caption{text}
%	\label{fig:figure_example}
%\end{figure}
