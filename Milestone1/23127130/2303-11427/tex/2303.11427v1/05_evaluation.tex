
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REMEMBER STORYTELLING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Evaluation}
\label{sec:experiments}
In this section, we cover specific implementation details and compare the performance of \gls{sac}-learned precoders to the \gls{mmse} and \gls{oma} baselines.

\subsection{Implementation Details}
\label{sec:implementationdetails}
The code is implemented in Python using the TensorFlow library and is available online at \cite{gracla_code_2023}. \reftab{tab:parameters} lists the key communication and learning parameters.
For each iteration \( \timeindex \), user positions are updated around a mean inter-user-distance~${\meanuserdist = \SI{1}{\km}}$ following a uniform distribution of \( \pm \)\,\SI{30}{\meter}.
%For all learned algorithms in this paper, user positions during training are redistributed at random from a uniform distribution of \( \pm \)\,\SI{30}{\meter} around a mean inter-user-distance $\meanuserdist$ for each training step.
This is done to prevent the learning process from honing in on a fixed user position regardless of the information it is fed, but it also highlights the method's ability to learn a single precoding algorithm that serves different satellite and user constellations.

\begin{table}[!t]
	\renewcommand{\arraystretch}{1.3}
	\caption{Selected Parameters}
	\label{tab:parameters}
    \medskip
	\centering
    \addtolength{\tabcolsep}{-.06cm}
    %\footnotesize
	\rowcolors{2}{white}{uniblue1!5} 
	\begin{tabular}{llll}
		\hline
        Noise Power $\noisepower$ & $6\text{e-}13\,\text{W}$
        &
        Transmit Power $\transmitpower$ & \SI{100}{W}
        \\
        Sat. Altitude & \SI{600}{\km} 
            &
        Ant. per Sat. $\numantennasper$ & \num{2}
        \\
        Sat. Nr. $\numsatellites$ & \num{2}
			&
		Gain per Sat. Ant $\satgain$ & \SI{14}{dBi}
        \\
        Sat. Distance & \SI{10}{km}
			&
		Gain per User $\usergain$ & \SI{0}{dBi}
		\\
        User Nr. $\numusers$ & \num{3}
                &
        Inter-Ant.-Distance $\antdist $ & $ 3 \wavelength/2$
        \\
        Wavelength $\wavelength$ & \SI{15}{cm}
            &
            Hidden Layers \( \times \) Nodes & \( 4 \times 512 \) \\
        Learning Steps \( \timeindex \) & \( 3\text{e}4 \)
            &
            Critic Learning Rate & \( 1\text{e-}5 \)
        \\
        Exp Buffer Size & \( 1\text{e}4 \)
            & Actor Learning Rate & \( 1\text{e-}6 \)
        \\
        Entropy Target~\( \logentropyscale \) & \num{1.0}
            & Data Batch Size~\( \learningbatchsize \) & \( \num{512} \)
        \\
		\hline
	\end{tabular}
\end{table}

We train three \gls{sac} precoders named \textrm{SAC1}, \textrm{SAC2}, \textrm{SAC3}. \textrm{SAC1} is trained with perfect \gls{csit}, \textrm{SAC2} is trained on error model 1 \refeq{eq:error} with a bound of \( {\errorbound = \num{0.1}} \), and \textrm{SAC3} is trained on error model 2 \refeq{eq:error2} with a bound of \( {\errorbound = \num{0.1}} \) and a scale of \( {\errorphasescale = \num{0.01}} \). 
All precoders are evaluated in terms of the achieved sum rate $\sumrate$. In cases with imperfect \gls{csit}, the sum rate $\bar{\sumrate}$ is averaged over \( \num{10000} \) Monte Carlo iterations.
%We evaluate all precoders in terms of achieved sum rate $\sumrate$, averaged over \( \num{10000} \) Monte Carlo iterations.

\subsection{Results}
\label{sec:results}

We first investigate the details of the precoding algorithm SAC1 for perfect \gls{csit} in \reffig{fig:distance_sweep}.
As the user distances $\userdist$ change, the user correlation in user channels alternates periodically, significantly impacting the achievable performance. The learned precoder shows gains over the \gls{mmse} precoder especially in the extreme areas, with either highly correlated or nearly orthogonal channels. When the channels are highly correlated, the learned precoder achieves its gain by focusing as much power as possible on a single user, thereby decreasing interference, but sacrificing fairness.
Crucially, the learned precoder outperforms \gls{oma} almost everywhere, demonstrating the increased spectral efficiency by frequency reuse.
In further investigations, the learned algorithms generalize well to user distances up to at least \( \pm\,\SI{300}{\meter} \) for the given scenario.
We attribute this to the learning algorithm capturing the locally periodic nature of the optimization objective during training.

\reffig{fig:error_sweep_1} compares SAC1 and SAC2 to \gls{mmse} and \gls{oma} precoding with unreliable user position measurements \eqref{eq:error} for increasing error bound $\errorbound$.
As expected, since \gls{mmse} is not sum rate optimal, SAC1 achieves the best sum rate performance with perfect \gls{csit}, with SAC2 slightly behind and both outperforming the baselines \gls{mmse} and \gls{oma}. With increasingly unreliable user position estimates, the performance of all four precoders degrades. However, SAC2, which already encountered unreliable information during training, shows resilience even at very high values of \( \errorbound \).

%We investigate the details of the precoding algorithm SAC1 in \reffig{fig:distance_sweep} for perfect \gls{csit}.
%As the user distances $\userdist$ change, the user correlation in user channels alternates periodically, significantly impacting the achievable performance. The learned precoder shows gains over the \gls{mmse} precoder especially in the extreme areas, with either highly correlated or orthogonal channels. When the channels are highly correlated, the learned precoder achieves its gain by focusing as much power as possible on a single user, thereby decreasing interference, but sacrificing fairness.
%Crucially, the learned precoder outperforms \gls{oma} almost everywhere, demonstrating the increased spectral efficiency by frequency reuse.
%In further investigations, the learned algorithms generalize well to user distances up to at least \( \pm\,\SI{300}{\meter} \) for the given scenario.
%We attribute this to the learning algorithm capturing the locally periodic nature of the optimization objective during training.
We then confirm in \reffig{fig:error_sweep_2} that the same learning approach also works for imperfections of satellite positions~\refeq{eq:error2}, demonstrating the adaptability of model-free data-driven optimization. 

\begin{figure}[!t]
	\centering
    \input{fig/distance_sweep_small_paper.pgf}
	\caption{%
        The sum rate~\eqref{eq:sumRate} for different user distances $\userdist$ with perfect \gls{csit} clearly highlights the performance gains of the \gls{sac} precoder over \gls{mmse}, particularly when channels are highly correlated (valleys) or nearly orthogonal (peaks). The \gls{sac} precoder shows glitches, \eg around \SI{980}{\meter}; it achieves its performance despite not having reached full convergence during training.
        %full convergence not having been reached during training.
    }
	\label{fig:distance_sweep}
\end{figure}

\begin{figure}[!t]
	\centering
    \input{fig/error_sweep_model_1_paper.pgf}
	\caption{%
        Mean sum rate performance of different precoding approaches in the presence of an error on \gls{csit} according to the first error model \refeq{eq:error}. Error bars represent the standard deviation. Note that user distances are also varied within each evaluation, hence the variance even at zero error.
    }
	\label{fig:error_sweep_1}
\end{figure}


\begin{figure}[!t]
	\centering
    \input{fig/error_sweep_model_2_paper.pgf}
	\caption{%
        Mean sum rate performance on the second error model, \refeq{eq:error2}. We fixate the user position error bound to \( \errorbound = 0.1 \) and sweep over the satellite position error scale~\( \errorphasescale \). Again, the learned algorithm achieves higher sum rates than the \gls{mmse} precoder, as well as slightly increased robustness at higher errors.
    }
	\label{fig:error_sweep_2}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% EXAMPLE TABLE
%\begin{table}[!t]
%	\renewcommand{\arraystretch}{1.3}
%	\caption{A Simple Example Table}
%	\label{tab:table_example}
%	\centering
%	\rowcolors{2}{white}{gray!10} 
%	\begin{tabular}{cc}
%		\hline
%		\bfseries First & \bfseries Next\\
%		\hline\hline
%		1.0 & 2.0\\
%		1.0 & 2.0\\
%		\hline
%	\end{tabular}
%\end{table}

%% EXAMPLE FIGURE
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=2.5in]{myfigure}
%	\input{figures/myfigure.pgf}
%	\caption{text}
%	\label{fig:figure_example}
%\end{figure}

