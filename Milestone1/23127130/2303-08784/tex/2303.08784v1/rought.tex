

%Compared with images, sketches are easier to collect and generate, even for objects from underrepresented classes. On the other hand, sketch-based object localization comes with several additional challenges for the localization task because the sketches are abstract, vary in quality and style, and there is a wide domain gap between the query sketch and the target image. 

%On the other hand, SGOL comes with several additional challenges for the localization task: 1) the sketches are abstract and contain limited information on object's attributes and shapes, ii) significant variability in the quality and style of sketches as these are drawn by a diverse set of untrained humans and iii) the wide domain gap between the query sketch and the target image. 

%Along with the sketch-guided object localization task, authors in~\cite{Tripathi2020SketchGuidedOL} proposed a simple localization framework based on FasterRCNN~\cite{Ren2015FasterRT} architecture. They proposed cross-modal attention-based modifications to the Region Proposal Network to address some challenges. However, being a proposal-based method, their framework is limited by the quality of the generated proposals, which is generally poor for the cases when the object is occluded or underrepresented in the dataset. Moreover, their performance is also restricted by the quality of the attention module used in their framework, as well as the performance of the underlying object detector. In order to address these challenges, Sketch-DETR is proposed by~\cite{Riba2021LocalizingIF} which extends DETR object detector~\cite{Carion2020EndtoEndOD} to enable sketch-guided object localization. The Sketch-DETR is a transformer-based encoder-decoder model that takes concatenation of the target image representation and the query sketch representation at the input of the encoder. The encoder refines the features of the target image by using the sketch query as the context. However, the refinement happens after the image features have already been learned and the encoder introduces unnecessary parameters and computation into the framework.

% We further introduced proposal alignment at the decoder that utilizes the attention module to bring closer the proposal and sketch representation.
% \section{Conclusion}
% \begin{table}[]
% \small
% \begin{tabular}{lcccc}
% \toprule
% Models                & $mAP$ & $AP@50$ & $AP@75$ & $AP^L$ \\
% \midrule
% Modified FasterRCNN & 3.3 & 7.4   & 2.8   & 6.2   \\
% CoAT~\cite{Hsieh2019OneShotOD}                  & 5.9 & 12.4  & 5.1   & 10.6  \\
% CMA~\cite{Tripathi2020SketchGuidedOL} & 7.5 & 15.0  & 6.6   & 12.4  \\
% \bottomrule
% Ours                  & \textbf{9.1} & \textbf{15.3}  & \textbf{8.9}   & \textbf{19.6} \\
% \bottomrule
% \end{tabular}
% \caption{\label{tab:disjoint_one}Comparison of our model in case of `unseen' object categories on QuickDraw! dataset. \textit{COCO val2017} dataset is used in this evaluation.}
% \end{table}
% \subsubsection{Disjoint train-test setting:} In the disjoint train-test categories setting, 14 and 7 categories out of 56 and 28 common categories between QuickDraw! and MS-COCO and Sketchy and QuickDraw! are selected as `unseen' categories, and the rest are selected as `seen' categories. In the disjoin train-test categories setting, images corresponding to only `seen' categories are used during training. In order to ensure the true disjoint train-test categories setting,  the image corresponding to the `unseen' categories is also removed from the Imagenet dataset during pretraining. Similarly, the sketch encoder is pretrained on QuickDraw! dataset after removing all the categories corresponding to the `unseen' classes.
% Let $I=I_{train} \cup I_{test}$ be the set of all natural images containing instances of different objects in the dataset.  Let $S = S_{train} \cup S_{test}$ be the set of all query sketches in the dataset. Given a sketch $s \in S$ and an image $i\in I$, the problem of sketch-guided object localization involves localizing all the instance of the objects in the image that correspond to the sketch $s$. Let, $C = C_{train}\cup C_{test}$ be the set of all categories in the dataset. Similar to~\cite{Tripathi2020SketchGuidedOL} we performed experiments in two experimental setting: \textit{common train-test} setting in which $C_{train}=C_{test}$ and \textit{disjoint train-test} setting in which $C_{train}\cap C_{test}=\phi$.

% We present a novel sketch-guided object vision transformer. Our model is inspired from ViDT~\cite{Song2021ViDTAE} which is a recent object detection model based on the transformers framework introduced in DETR~\cite{Carion2020EndtoEndOD}. Unlike DETR, it is a decoder-only model that utilizes Swin transformers~\cite{DBLP:conf/iccv/LiuL00W0LG21} as the image backbone. ViDT directly extracts the fine-grained $[\mathtt{DET}]$ features without an encoder and then utilizes a transformer-based decoder to learn the bounding boxes around the objects and the corresponding classes. It uses a fixed number of $[\mathtt{DET}]$ tokens and during the training phase, it learns to look for objects at different locations in the image. A straightforward extension to the ViDT model to enable sketch-guided object localization is to compare the representation of objects at the output of the transformer decoder, given by $[\mathtt{DET}]$ tokens, to the representation of the query sketch to localize the object. Since the object and the query representations are learnt independently of each other, it leads to sub-optimal localization performance. In this work, we propose sketch-guided vision transformer that learns the representation of the target image conditioned on the query sketch. The target image representation learned in this way are better aligned with the query sketch and therefore it leads to better localization performance. Moreover, the object and the query features are further refined at the output of the decoder to bring the corresponding object features closer to the sketch query for better scoring. The proposed model is end-to-end trainable and it is described in the following three sections: (i) Sketch-guided Vision Transformer (ii) Object refinement, and (iii) Scoring.

% We present a novel sketch-guided vision transformer. 
%Our model is inspired from ViDT~\cite{Song2021ViDTAE} which is a recent object detection model based on the transformers. It is a decoder-only model that utilizes Swin transformers~\cite{DBLP:conf/iccv/LiuL00W0LG21} as the image backbone. ViDT uses a fixed number of $[\mathtt{DET}]$ tokens at the input of a transformer-based decoder and, during training, learns to look for objects at different locations in the image. A straightforward extension to the ViDT model to enable sketch-guided object localization is to compare the representation of objects at the output of the decoder, given by $[\mathtt{DET}]$ tokens, to the representation of the query sketch to localize the object. Since the object and the query representations are learned independently, it leads to sub-optimal localization performance. 
%In these works, sketch representations are often learned using the traditional convolutional neural networks~\cite{DBLP:journals/ijcv/YuYLSXH17}, RNNs~\cite{Ha2018ANR}, and modern transformer~\cite{Ribeiro2020SketchformerTR} networks. In this work, we used CNN as the sketch feature extractor, but our model is flexible to work with any advanced sketch encoders. 

%Particularly in the sketch-based image retrieval (SBIR) community, there has been a lot of research. Research in SBIR has also extended to fine-grained SBIR~\cite{Song2017DeepSA,Bhunia2020SketchLF, DBLP:journals/corr/abs-2207-01723}, zero-shot SBIR~\cite{Dey2019DoodleTS,Dutta2019SemanticallyTP}, large-scale SBIR~\cite{Bozas2012LargeSS,Liu2017DeepSH}, etc. Among all these sketch based applications, SBIR is most similar to sketch-guided object localization. However, unlike SBIR, sketch-guided localization involves localizing all the instances of the object in the image that correspond to the query sketch in the presence of many other objects.  

% Addressing the shortcomings in the existing approaches, we proposed an object localization framework based on the ViDT~\cite{Song2021ViDTAE} model. ViDT is a decoder-only, entirely transformer-based object detector that directly learns refined $[\mathtt{DET}]$ tokens in the image encoder. ViDT model can be directly modified to perform sketch-guided localization by scoring the object representation at the output of the decoder with the query sketch. Since the ViDT and the sketch encoders are trained independently, it leads to subpar performance. In this work, we propose sketch guided vision transformer that learns the representation of the target image conditioned on the query sketch.

% Moreover, at the decoder the representation of the objects and the query sketch are further refined by the proposed attention-based refinement scheme. The sketch-guided vision transformer utilizes multi-headed cross attention where the representation of the image patches, after each block of the image encoder, are used as queries to attend to the relevant portions of the query sketch. Therefore the representation of the target image at the output of the sketch-guided vision transformer is better aligned with the sketch query. 

% In order to compare the performance of the proposed fusion methods, we incorporated the the fusion strategies proposed in Sketch-DETR and CMA into ViDT model and reported the results in Table~\ref{tab:atten_comparison}. Modified-ViDT involves scoring the representation of the $[\mathtt{DET}]$ tokens generated from a vanilla ViDT with the sketch query. Although fusion strategies in CMA and Sketch-DETR improves performance over the modified-ViDT, the proposed early fusion strategy gives the best localization performance.  



% \begin{table*}[]
% \centering
% \begin{tabular}{llccccc}
% \toprule
% \multicolumn{2}{l}{Models}                    & $mAP$           & $AP@50$         & $AP@75$         & $AP^M$         & $AP^L$  \\
% \midrule
% \multicolumn{2}{l}{Modified FasterRCNN}       & 18.2          & 31.5          & 18.7          & 18.2          & 28.3          \\
% \multicolumn{2}{l}{CoAT}                      & 27.9          & 48.6          & 28.5          & 29.2          & 42.2          \\
% \multicolumn{2}{l}{Cross-modal attention}     & 30.0          & 50.0          & 31.1          & 31.6          & 45.1          \\
% \multirow{3}{*}{Tough-To-Beat} & Faster R-CNN & 35.5          & 58.1          & -             & -             & -             \\
%                               & RetinaNet    & 37.9          & 60.1          & -             &-              & -             \\
%                               & DETR         & 41.1          & 62.7          & -             &-              & -             \\
% \multicolumn{2}{l}{Sketch-DETR}               & 41.4          & 62.1          & -             &-              & -             \\
% \multicolumn{2}{l}{Ours}                      & \textbf{48.4} & \textbf{71.8} & \textbf{51.8} & \textbf{40.6} & \textbf{70.3}\\
% \bottomrule
% \end{tabular}
% \end{table*}
% \section{Limitations}


% \subsection{Sketch Representation}
% Sketch representations are often learned using the traditional convolutional neural networks~\cite{DBLP:journals/ijcv/YuYLSXH17}, RNNs~\cite{Ha2018ANR}, and modern transformer~\cite{Ribeiro2020SketchformerTR} networks. In this work, we used CNN as the sketch feature extractor, but our model is flexible to work with any advanced sketch encoders.  

% \section{Differences from Related work(just for reference)}
% \subsection{ViDT}
% ViDT is a fully transformer based object detector, that can be directly extended to enable sketch guided localization. The learned representation of the $[\mathtt{DET}]$ tokens can be scored with the query sketch representation to localize the corresponding object. However, the representation of the sketch query and the target image are learned independently, it leads to the representations that are not aligned which in turn leads to poorer localization performance.

% \subsection{Sketch-DETR} 
% In order to align the target image and query sketch features, Sketch-DETR concatenates the sketch features with the image features at the input of the DETR encoder. Since the encoder takes the output of image backbone as input, the fusion of the sketch and image features happen late, which leads to inadequate fusion.

% \subsection{Cross-modal attention}
% CMA utilizes a FasterRCNN framework and it proposed a cross-modal attention to generate region proposals relevant to the query sketch. However, the quality of the proposal based methods are limited by the quality of the generated proposals. Though CMA tries to generate relevant proposals, it is limited by the insufficient attention mechanism applied after the representation of the target image is generated by the image encoder. 

% \section{Proposed Methodology}
% In this section, we will formally set up the problem of Sketch-guided object localization in natural images and then explain the proposed solution. Finally, we shall extend our model to incorporate multiple sketch queries.

% \begin{figure*}
%     \centering
%     % \hfill
%     \begin{subfigure}[b]{0.39\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_a_v6.pdf}
%     \caption{Localization Framework}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.29\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_b_v4.pdf}
%     \caption{Sketch-guided Image transformer}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_c_v6 (1).pdf}
%     \caption{Decoder}
%     \end{subfigure}
%     % \hfill
%     \caption{The proposed Sketch guided object localization model consists of two parts: a) Backbone and b) Decoder. The backbone takes an image, sketch and $[\mathtt{DET}]$ tokens as input. Inside the backbone module we proposed sketch2image fusion in order to learn query conditioned image representation. The S2I fusion module takes flattened image and sketch representation and uses cross multi-headed attention to learn the conditioned image representation. It uses the image tokens as queries and sketch tokens as key and value such that the representation of each image token is updated based on its attention score with the sketch tokens. Similarly, decoder updates the representation of sketch and $[\mathtt{DET}]$ tokens using cross multi-headed attention. }
%     \label{fig:my_label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     % \hfill
%     \begin{subfigure}[b]{0.608\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_a_v8.pdf}
%     % \caption{Localization Framework}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.315\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_bc_v7.pdf}
%     % \caption{Sketch-guided Image transformer}
%     \end{subfigure}
%     % \hfill
%     \caption{The proposed Sketch guided object localization model consists of two parts: a) Backbone and b) Decoder. The backbone takes an image, sketch and $[\mathtt{DET}]$ tokens as input. Inside the backbone module we proposed sketch2image fusion in order to learn query conditioned image representation. The S2I fusion module takes flattened image and sketch representation and uses cross multi-headed attention to learn the conditioned image representation. It uses the image tokens as queries and sketch tokens as key and value such that the representation of each image token is updated based on its attention score with the sketch tokens. Similarly, decoder updates the representation of sketch and $[\mathtt{DET}]$ tokens using cross multi-headed attention. }
%     \label{fig:my_label}
% \end{figure*}
% \begin{figure*}
%     \centering
%     % \hfill
%     \begin{subfigure}[b]{0.48ÃŸ\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_a_v5.pdf}
%     \caption{Localization Framework}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.48\textwidth}
%     \centering
%     \begin{subfigure}[b][0.24\textwidth]
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_b_v4.pdf}
%     \caption{Sketch-guided Image transformer}
%     \end{subfigure}
%     \begin{subfigure}[b][0.24\textwidth]
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/main_fig_c_v6.pdf}
%     \caption{Decoder}
%     \end{subfigure}
%     \end{subfigure}
%     % \begin{subfigure}[b]{0.31\textwidth}
    
%     % \end{subfigure}
%     % \hfill
%     \caption{The proposed Sketch guided object localization model consists of two parts: a) Backbone and b) Decoder. The backbone takes an image, sketch and $[\mathtt{DET}]$ tokens as input. Inside the backbone module we proposed sketch2image fusion in order to learn query conditioned image representation. The S2I fusion module takes flattened image and sketch representation and uses cross multi-headed attention to learn the conditioned image representation. It uses the image tokens as queries and sketch tokens as key and value such that the representation of each image token is updated based on its attention score with the sketch tokens. Similarly, decoder updates the representation of sketch and $[\mathtt{DET}]$ tokens using cross multi-headed attention. }
%     \label{fig:my_label}
% \end{figure*}
% \begin{table}[!t]
% \centering
% \begin{tabular}{lrr}
% \toprule
% Model                 & mAP  & AP@50 \\
% \midrule
% Ours                  & 48.0 & 71.7  \\
% ~~~~ - Object Refinement & 46.9 & 68.7  \\
% ~~~~ - Sketch-guided Vis. Trans.      & 39.4 & 56.6 \\
% \bottomrule
% \end{tabular}
% \caption{\label{tab:ablation}Effect of various components on the performance of the proposed model.}
% \end{table}

% \begin{table}[t]
% \centering
% \begin{tabular}{l|cc|cc}
% \toprule
% \multirow{2}{*}{\#Sketches} & \multicolumn{2}{|c}{Sketchy} & \multicolumn{2}{|c}{QuickDraw} \\
%                             & mAP          & AP@50        & mAP          & AP@50          \\
% \midrule
% 2                           & 50.5         & 74.6         &      47.8    &   70.6      \\
% 3                           & 50.6         & 74.6         &     49.0   &   72.5         \\
% 4                           & 50.8         & 74.8         &     49.1     &   72.5       \\
% 5                           & 50.7         & 74.7         &     49.2     &  72.6         \\
% 6                           & 50.8         & 74.7         &     49.2     &  72.7        \\
% 7                           & 50.8         & 74.7         &     49.3     &  72.8        \\
% 8                           & 50.8         & 74.8         &     49.3     &  72.8        \\

% \bottomrule
% \end{tabular}
% \caption{\label{tab:num_sk}The effect of number of sketch queries on the performance of the model at the evaluation time. Please note during training 5 sketch queries are used to train the model. The model's performance is robust to change in number of sketch queries during evaluation.}
% \end{table}
% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\textwidth]{LaTeX/figures/classwise.png}
%     \caption{\label{fig:class-wise}Class-wise mAP results on MS-COCO val2017 dataset.} %https://docs.google.com/spreadsheets/d/1CGZ9WTC6lhU73z-Xt4kPgyY-yN5VyEBLpjLRsffVZfA/edit?usp=sharing
% \end{figure*}