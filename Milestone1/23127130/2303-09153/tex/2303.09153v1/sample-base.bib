
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

 
 



 
 
%%%%%%%%%%%%%%%%%%%%%% Start of Aptara sample bib entries

% acmsmall-sam.bib
 
 
 

%这里开始是正常应用的
@article{1,
  title={Single Image Haze Removal Using Dark Channel Prior},
  author={ He, K. },
  year={2009},
}
@InProceedings{2,
author = {Tang, Ketan and Yang, Jianchao and Wang, Jue},
title = {Investigating Haze-relevant Features in A Learning Framework for Image Dehazing},
booktitle = {Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}
@article{3,
  title={DehazeNet: An End-to-End System for Single Image Haze Removal},
  author={ Cai, B.  and  Xu, X.  and  Jia, K.  and  Qing, C.  and  Tao, D. },
  journal={IEEE Transactions on Image Processing},
  volume={25},
  number={11},
  pages={5187-5198},
  year={2016},
}

@article{4,
  title={Single Image Dehazing via Multi-scale Convolutional Neural Networks with Holistic Edges},
  author={ Ren, W.  and  Pan, J.  and  Zhang, H.  and  Cao, X.  and  Yang, M. H. },
  journal={International Journal of Computer Vision},
  volume={128},
  number={1},
  pages={240-259},
  year={2020},
}

@inproceedings{5,
  title={Gated Context Aggregation Network for Image Dehazing and Deraining},
  author={ Chen, D.  and  He, M.  and  Fan, Q.  and  Liao, J.  and  Hua, G. },
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2019},
}

@inproceedings{UE5,
  title={https://docs.unrealengine.com/5.0/en-US},
  author={EPIC},
  year={2022},
 
}

@inproceedings{UE5fog,
  title={https://docs.unrealengine.com/4.27/en-US/Building-Worlds/FogEffects/},
  author={EPIC},
  year={2022},
 
}



@inproceedings{NDIM,
  title={Nighttime haze removal based on a new imaging model},
  author={ Jing, Z.  and  Yang, C.  and  Wang, Z. },
  booktitle={2014 IEEE International Conference on Image Processing (ICIP)},
  year={2014},
}

@inproceedings{NHRG,
  title={Nighttime Haze Removal with Glow and Multiple Light Colors},
  author={ Yu, L.  and  Tan, R. T.  and  Brown, M. S. },
  booktitle={IEEE International Conference on Computer Vision},
  year={2015},
}

@inproceedings{nighttime2016,
  title={Night-time dehazing by fusion},
  author={ Ancuti, C.  and  Ancuti, C. O.  and  Vleeschouwer, C. D. and AC Bovik},
  booktitle={IEEE International Conference on Image Processing},
  year={2016},
}



@inproceedings{MRP,
  title={Fast Haze Removal for Nighttime Image Using Maximum Reflectance Prior},
  author={ Jing, Z.  and  Yang, C.  and  Shuai, F.  and  Yu, K.  and  Chang, W. C. },
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017},
}
@inproceedings{OSFD,
author = {Zhang, Jing and Cao, Yang and Zha, Zheng-Jun and Tao, Dacheng},
title = {Nighttime Dehazing with a Synthetic Benchmark},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413763},
doi = {10.1145/3394171.3413763},
abstract = {Increasing the visibility of nighttime hazy images is challenging because of uneven
illumination from active artificial light sources and haze absorbing/scattering. The
absence of large-scale benchmark datasets hampers progress in this area. To address
this issue, we propose a novel synthetic method called 3R to simulate nighttime hazy
images from daytime clear images, which first reconstructs the scene geometry, then
simulates the light rays and object reflectance, and finally renders the haze effects.
Based on it, we generate realistic nighttime hazy images by sampling real-world light
colors from a prior empirical distribution. Experiments on the synthetic benchmark
show that the degrading factors jointly reduce the image quality. To address this
issue, we propose an optimal-scale maximum reflectance prior to disentangle the color
correction from haze removal and address them sequentially. Besides, we also devise
a simple but effective learning-based baseline which has an encoder-decoder structure
based on the MobileNet-v2 backbone. Experiment results demonstrate their superiority
over state-of-the-art methods in terms of both image quality and runtime. Both the
dataset and source code will be available at https://github.com/chaimi2013/3R.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2355–2363},
numpages = {9},
keywords = {deep convolutional neural networks, synthetic dataset, dehazing},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{2018RESIDE,
  title={RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information},
  author={ Vashishth, S.  and  Joshi, R.  and  Prayaga, S. S.  and  Bhattacharyya, C.  and  Talukdar, P. },
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year={2018},
}
@inproceedings{NYU,
  author    = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
  title     = {Indoor Segmentation and Support Inference from RGBD Images},
  booktitle = {ECCV},
  year      = {2012}
}
@inproceedings{2018O,
  title={O-HAZE: A Dehazing Benchmark with Real Hazy and Haze-Free Outdoor Images},
  author={ Ancuti, C. O.  and  Ancuti, C.  and  Timofte, R.  and  Vleeschouwer, C. D. },
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2018},
}
@article{2018I,
  title={I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images},
  author={ Ancuti, Codruta O  and  Ancuti, C.  and  Timofte, R.  and  Vleeschouwer, C De },
  year={2018},
}
@inproceedings{2020NH-HAZE,
  title={NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images},
  author={ Ancuti, C. O.  and  Ancuti, C.  and  Timofte, R. },
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2020},
}
@article{2019Dense,
  title={Dense Haze: A benchmark for image dehazing with dense-haze and haze-free images},
  author={ Ancuti, C. O.  and  Ancuti, C.  and  Sbert, M.  and  Timofte, R. },
  journal={arXiv},
  year={2019},
}
@article{2021RICE,
  title={RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks},
  author={ Xie, C.  and  Mousavian, A.  and  Xiang, Y.  and  Fox, D. },
  year={2021},
}

@article{2020SAD,
  title={Spatial-Adaptive Network for Single Image Denoising},
  author={ Chang, M.  and  Li, Q.  and  Feng, H.  and  Xu, Z. },
  journal={Computer Vision – ECCV 2020},
  year={2020},
}


 
 

@misc{afifi2020XYZ,
      title={CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks}, 
      author={Mahmoud Afifi and Abdelrahman Abdelhamed and Abdullah Abuolaim and Abhijith Punnappurath and Michael S. Brown},
      year={2020},
      eprint={2006.12709},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{EVDnet,
  title={End-to-End United Video Dehazing and Detection},
  author={ Li, B.  and  Peng, X.  and  Wang, Z.  and  Xu, J.  and  Dan, F. },
  year={2017},
 abstract={The recent development of CNN-based image dehazing has revealed the effectiveness of end-to-end modeling. However, extending the idea to end-to-end video dehazing has not been explored yet. In this paper, we propose an End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal consistency between consecutive video frames. A thorough study has been conducted over a number of structure options, to identify the best temporal fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and Detection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with a video object detection model. The resulting augmented end-to-end pipeline has demonstrated much more stable and accurate detection results in hazy video.},
}

@article{hazedata,
  title={Learning to Dehaze From Realistic Scene with A Fast Physics Based Dehazing Network},
  author={ Li, R.  and  Zhang, X.  and  You, S.  and  Li, Y. },
  year={2020},
 abstract={Dehaze is one of the popular computer vision research topics for long. A realtime method with reliable performance is highly desired for a lot of applications such as autonomous driving. In recent years, while learning based methods require datasets containing pairs of hazy images and clean ground truth references, it is generally impossible to capture this kind of data in real. Many existing researches compromise this difficulty to generate hazy images by rendering the haze from depth on common RGBD datasets using the haze imaging model. However, there is still a gap between the synthetic datasets and real hazy images as large datasets with high quality depth are mostly indoor and depth maps for outdoor are imprecise. In this paper, we complement the exiting datasets with a new, large, and diverse dehazing dataset containing real outdoor scenes from HD 3D videos. We select large number of high quality frames of real outdoor scenes and render haze on them using depth from stereo. Our dataset is more realistic than existing ones and we demonstrate that using this dataset greatly improves the dehazing performance on real scenes. In addition to the dataset, inspired by the physics model, we also propose a light and reliable dehaze network. Our approach outperforms other methods by a large margin and becomes the new state-of-the-art method. Moreover, the light design of the network enables our methods to run at realtime speed that is much faster than other methods.},
}

@article{BlockNeRF,
  title={Block-NeRF: Scalable Large Scene Neural View Synthesis},
  author={ Tancik, M.  and  Casser, V.  and  Yan, X.  and  Pradhan, S.  and  Mildenhall, B.  and  Srinivasan, P. P.  and  Barron, J. T.  and  Kretzschmar, H. },
  journal={arXiv e-prints},
  year={2022},
 abstract={We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.},
}

@article{darkNeRF,
  title={NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images},
  author={ Mildenhall, B.  and  Hedman, P.  and  Martin-Brualla, R.  and  Srinivasan, P.  and  Barron, J. T. },
  year={2021},
 abstract={Neural Radiance Fields (NeRF) is a technique for high quality novel view synthesis from a collection of posed input images. Like most view synthesis methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images have been processed by a lossy camera pipeline that smooths detail, clips highlights, and distorts the simple noise distribution of raw sensor data. We modify NeRF to instead train directly on linear raw images, preserving the scene's full dynamic range. By rendering raw output images from the resulting NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In addition to changing the camera viewpoint, we can manipulate focus, exposure, and tonemapping after the fact. Although a single raw image appears significantly more noisy than a postprocessed one, we show that NeRF is highly robust to the zero-mean distribution of raw noise. When optimized over many noisy raw inputs (25-200), NeRF produces a scene representation so accurate that its rendered novel views outperform dedicated single and multi-image deep raw denoisers run on the same wide baseline input images. As a result, our method, which we call RawNeRF, can reconstruct scenes from extremely noisy images captured in near-darkness.},
}

@article{wildNeRF,
  title={NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections},
  author={ Martin-Brualla, R.  and  Radwan, N.  and  Sajjadi, Msm  and  Barron, J. T.  and  Dosovitskiy, A.  and  Duckworth, D. },
  year={2020},
 abstract={We present a learning-based method for synthesizing novel views of complex outdoor scenes using only unstructured collections of in-the-wild photographs. We build on neural radiance fields (NeRF), which uses the weights of a multilayer perceptron to implicitly model the volumetric density and color of a scene. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. In this work, we introduce a series of extensions to NeRF to address these issues, thereby allowing for accurate reconstructions from unstructured image collections taken from the internet. We apply our system, which we dub NeRF-W, to internet photo collections of famous landmarks, thereby producing photorealistic, spatially consistent scene representations despite unknown and confounding factors, resulting in significant improvement over the state of the art.},
}

@inproceedings{2016Robust,
  title={Robust Image and Video Dehazing with Visual Artifact Suppression via Gradient Residual Minimization},
  author={ Chen, Chen  and  Do, Minh N.  and  Wang, Jue },
  booktitle={Springer International Publishing},
  year={2016},
 abstract={Most existing image dehazing methods tend to boost local image contrast for regions with heavy haze. Without special treatment, these methods may significantly amplify existing image artifacts such as},
}

@inproceedings{Learninghaze,
  title={Learning to Restore Hazy Video: A New Real-World Dataset and A New Method},
  author={ Zhang, X.  and  Dong, H.  and  Pan, J.  and  Zhu, C.  and  Tai, Y.  and  Wang, C.  and  Li, J.  and  Huang, F.  and  Wang, F. },
  booktitle={Computer Vision and Pattern Recognition},
  year={2021},
}

@article{MegaNeRF,
  title={Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs},
  author={ Turki, H.  and  Ramanan, D.  and  Satyanarayanan, M. },
  year={2021},
 abstract={We use neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs. To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel. We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.},
}
}

@article{NGP,
  title={Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author={T Müller and  Evans, A.  and  Schied, C.  and  Keller, A. },
  journal={arXiv e-prints},
  year={2022},
 abstract={Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\!imes\!1080}$.},
}

@article{NeRF,
  title={NeRF: representing scenes as neural radiance fields for view synthesis},
  author={ Mildenhall, B.  and  Srinivasan, P. P.  and  Tancik, M.  and  Barron, J. T.  and  Ren, N. },
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99-106},
  year={2022},
}

@misc{cuda                                    ,
    Author = {Thomas M\"uller},
    Year = {2021},
    Note = {https://github.com/nvlabs/tiny-cuda-nn},
    Title = {Tiny {CUDA} Neural Network Framework}
}

@inproceedings{pixelNeRF,
  title={pixelNeRF: Neural Radiance Fields from One or Few Images},
  author={ Yu, A.  and  Ye, V.  and  Tancik, M.  and  Kanazawa, A. },
  booktitle={Computer Vision and Pattern Recognition},
  year={2021},
}

@article{dehazeformer,
  title={Vision Transformers for Single Image Dehazing},
  author={ Song, Y.  and  He, Z.  and  Qian, H.  and  Du, X. },
  year={2022},
}

@article{2020NeRF,
  title={NeRF++: Analyzing and Improving Neural Radiance Fields},
  author={ Zhang, K.  and  Riegler, G.  and  Snavely, N.  and  Koltun, V. },
  year={2020},
}

@article{2021hNeRF,
  title={NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images},
  author={ Mildenhall, B.  and  Hedman, P.  and  Martin-Brualla, R.  and  Srinivasan, P.  and  Barron, J. T. },
  year={2021},
}

 
