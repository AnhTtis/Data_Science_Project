
\subsection{Min-Max}

The Min-Max algorithm, also known as Bounding Box algorithm, is a simple and straightforward method. It contains only very few arithmetic operations, the run-time complexity is in $\Theta(N_{\mathit{anc}})$. Min-Max builds a square (bounding box) given by $[a_{xi}-r_i,a_{yi}-r_i] \times [a_{xi}+r_i,a_{yi}+r_i]$ around each anchor node~$i$ using its location $a_i=(a_{xi},a_{yi})$ and distance estimate $r_i$, instead of using circles with radius $r_i$. The position of target satisfies every box, thus the position is in the \ac{IR} with vertices $\mathbf{V}=\{(l,b),(r,b),(l,t),(r,t)\}$, as Eq.~\eqref{eq:box} and Figure~\ref{fig:box}. Then, estimation of position $(\hat{x},\hat{y})=(\frac{l+r}{2},\frac{t+b}{2})$ is the centre of \ac{IR}.

\begin{equation}
  \label{eq:box}
 \mathit{IR}= \bigcap\limits_{i=1}^{N_{anc}} {\{a_{xi}-r_i , a_{xi}+r_i, a_{yi}-r_i , a_{yi}+r_i\} }~,
\vspace{12pt}
\end{equation}
where $N_{anc}$ denotes the number of anchors and with

\begin{equation}
\label{eq:vertex}
\renewcommand{\arraystretch}{1.3}
\left\{ \begin{array}{l}
\vspace{ 0.17cm}
 l = \max_{i = 1}^{N_{anc}} \{a_{xi}-r_i \} \\
\vspace{ 0.17cm}
 r =\min_{i = 1}^{N_{anc}} \{a_{xi}+r_i \} \\
\vspace{ 0.17cm}
 t =\min_{i = 1}^{N_{anc}} \{a_{yi}+r_i \} \\
\vspace{ 0.0cm}
 b =\max_{i = 1}^{N_{anc}} \{a_{yi}-r_i \}.\\
\end{array} \right.
\vspace{12pt}
\end{equation}

However, Min-Max can produce high position error even when having small distance measurement error, particularly when the target is located outside the perimeter of the anchor nodes. Due to the multi-path effect, most of the measured distances are larger than the actual distance, which is especially common in indoor scenarios. Furthermore, the box has larger area than the corresponding range circle. Even though the range is imprecise, the target is more likely to resist in \ac{IR} or be close to \ac{IR}. Therefore, to find a more reasonable estimation in \ac{IR} can be a potential method to increase the location accuracy.

\subsection{Extended Min-Max}

\ac{E-Min-Max} determines the \ac{IR} the same way Min-Max does but the position of the unlocalised node can be located at any point inside the \ac{IR} and not only at the centre of it. Therefore, \ac{E-Min-Max} assigns a weight \begin{math}W_{a}\end{math} to each vertex of the \ac{IR}. In the original paper \ac{E-Min-Max} is evaluated with four different weights (\begin{math}W_1, W_2, W_3, W_4\end{math})~\citep{extended2012}. We limit our evaluation to the two weights which showed the best performance, \begin{math}W_2\end{math} and \begin{math}W_4\end{math}:
\vspace{12pt}
\begin{equation}
W_{2}(j)= \frac{1}{\sum_{i=1}^n(D_{i,j} - r_i)^2}
\label{e_min_max_w2}
\end{equation}

\begin{equation}
W_{4}(j)= \frac{1}{\sum_{i=1}^n |D_{i,j}^2 - r_i^2|}
\label{e_min_max_w4}
\vspace{12pt}
\end{equation}
where \begin{math}D_{i,j}\end{math} is the Euclidean distance between anchor \begin{math}i\end{math} and vertex \begin{math}j\end{math} of the \ac{IR}. In general, \begin{math}W_4\end{math} gives better results inside the perimeter of the anchors and \begin{math}W_2\end{math} shows the best overall performance, even outside the perimeter of the anchors. The final position is estimated by calculating the weighted centroid with the weights and the coordinates of the vertices as in Eq.~\eqref{e_min_max_pos}.
\vspace{12pt}
\begin{equation}\label{e_min_max_pos}
  (\hat{x},\hat{y})= \left(\frac{\sum_{j=1}^4 W_a(j) \cdot x_j}{\sum_{j=1}^4 W_a(j)}, \frac{\sum_{j=1}^4 W_a(j) \cdot y_j}{\sum_{j=1}^4 W_a(j)}\right)
\end{equation}

Compared to the original Min-Max, \ac{E-Min-Max} requires extra operations to estimate the weights for the vertices. Especially, \ac{E-Min-Max}~(W2) includes square roots which is more expensive in terms of computation but the run-time complexity of \ac{E-Min-Max} is also in $\Theta(N_{\mathit{anc}})$.

Weighting with the absolute residues is based on the assumption that $|D_{i,j} - r_i|$ can approximate $|D_{i,j} -\bar{r}_i|$, where $\bar{r}_i$ is the $i$th actual distance. However, some distance estimation errors are extremely large due to \ac{NLOS} propagation, which results in large residues even if close to the actual target position. Thus, \ac{E-Min-Max} cannot improve the accuracy in some cases and still the error distribution for real environment is not considered.

\subsection{Maximum Likelihood Estimation}

\ac{ML} estimation can be employed if the \ac{pdf} of the distance error contained in the noisy distance measurements is known. Let $p(r\mid u)$ denote the \ac{pdf} that specifies the probability of observing the distance measurements $r$ at the position $u$ of the unlocalised node. \ac{ML} estimation calculates the source location as the value $u$ that maximizes the likelihood function, i.e.,
\vspace{12pt}
\begin{equation}
\hat{u}_{\textit{ML}} = \underset{u}{\operatorname{argmax}} \ p(r\mid u)
\label{likelihood_function_estimate}
\end{equation}

We approximate the error distribution for real environment using normal distribution $\mathcal{N}(\mu,\sigma^2)$ and  gamma distribution $\Gamma(\alpha,\beta)$. Figure~\ref{fig:histogram_ranging_and_fitting} depicts the frequency histogram of the distance measurement error collected during an experiment with over 22000 \ac{TOF} measurements, having a positive biased and right-side tailed error. Figure~\ref{fig:histogram_ranging_and_fitting} also shows the distribution fitting of the error using the normal distribution $\mathcal{N}(2.43, 3.57^2)$ (red curve) and the gamma distribution $\Gamma(3.3, 0.58)$ (blue curve).

\subsubsection{Normal distribution}

Assuming a normal distribution with mean $\mu$ and variance $\sigma^2$, the likelihood function is given by \citep{Gezici:2008:SWP:1341571.1341575}:
\vspace{12pt}
\begin{equation}
p(r\mid u) = \frac{1}{(2\pi)^{N_{anc}/2}\left| C \right|^{1/2}}\exp\Bigl(-\frac{1}{2}(r - f(u)  - \mu)^T C^{-1}(r - f(u) - \mu)\Bigr)
\label{likelihood_function_gaussian}\vspace{12pt}
\end{equation}
where $C=\operatorname{diag}(\sigma^2_1, \sigma^2_2,\dots,\sigma^2_{N_{anc}})$ is the covariance matrix for $r$ in case of uncorrelated noise components and $f(u)=\|u - a\|$ is the noise-free distance vector. The \ac{ML} estimate is obtained by maximizing Eq.~\eqref{likelihood_function_gaussian}. For computational convenience, however, the \ac{ML} estimate is obtained by maximizing the log-likelihood function
\vspace{12pt}
\begin{equation}
\ln p(r\mid u) = \ln\Biggl(\frac{1}{(2\pi)^{N_{anc}/2}\left| C \right|^{1/2}}\Biggr) - \frac{1}{2}(r - f(u) - \mu)^T C^{-1}(r - f(u) - \mu)\text{.}
\label{likelihood_function_gaussian_ln}
\end{equation}

As the first term is independent of $u$, maximizing Eq.~\eqref{likelihood_function_gaussian_ln} is equivalent to minimizing the second term. The \ac{ML} estimate therefore is:
\vspace{12pt}
\begin{equation}
\begin{split}
\hat{u}_{\textit{ML}} &= \underset{u}{\operatorname{argmin}} \text{ } (r - f(u) - \mu)^T C^{-1}(r - f(u) - \mu)\\
&=\underset{u}{\operatorname{argmin}} \text{ }\sum_{i=1}^{N_{anc}} \frac{\Bigl(r_i - \|u - a_i\| - \mu_i\Bigr)^2}{\sigma_i^2}\text{.}
\end{split}
\label{likelihood_function_gaussian_ln_estimate}
\end{equation}

Common techniques for solving Eq.~\eqref{likelihood_function_gaussian_ln_estimate} include numerical methods like Newton--Raphson procedure, Gauss--Newton method or steepest descent algorithm \citep{zekavat2011handbook}. For a noise distribution with zero mean the minimization of Eq.~\eqref{likelihood_function_gaussian_ln_estimate} corresponds to a weighted version of the NLLS algorithm where the weights are inversely proportional to the noise variances thus larger variances result in smaller weights. In general, the \ac{ML} estimator generalizes the NLLS method and is reduced to it when assuming zero mean and when all $\sigma_i^2$ are identical. For the rest of this paper the algorithm is referred to as MLE-$\mathcal{N}$.

\subsubsection{Gamma distribution}

Looking at the histogram in Figure~\ref{fig:histogram_ranging_and_fitting}, a gamma distribution looks like a better choice for calculating the likelihood. Like the histogram of distance measurements, the gamma distribution is asymmetric, has only positive support, and still allows arbitrary large distance measurement errors with decreasing probability.

The gamma distribution is parametrised by a shape parameter $\alpha\geq 0$ and a rate parameter $\beta\geq 0$. The \ac{pdf} is given by:
\vspace{12pt}
\begin{equation}
  \label{eq:crlb:pdf:gamma}
  \Gamma(\alpha,\beta)(x)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}
  \vspace{12pt}
\end{equation}
where $\Gamma(\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt$.

Since the gamma distribution is not defined for non-positive values and the distance measurements are sometimes too short, we add an offset $\eta\geq 0$ to the measurements, which is chosen to be the smallest observed measurement error. The likelihood to be at position $u$ while measuring $r_i$ from anchor $a_i$ is:
\vspace{12pt}
\begin{equation}
  p_i(r_i\mid u)=
  \begin{cases}
    \Gamma(\alpha,\beta)(r_i+\eta-\|u-a_i\|)&\text{if $r_i\geq\|u-a_i\|-\eta$}\\
    0 & \text{otherwise}
  \end{cases}\ .
  \vspace{12pt}
\end{equation}
Approximating the maximum joint likelihood $p(r\mid u)=\prod_ip_i(r_i\mid u)$ for position $u$ with respect to measurements turns out to be tricky. If the position $u$ is too close to an anchor, the joint likelihood of $u$ and some environment of $u$ is $0$, as is the gradient $\nabla \prod_ip_i(r_i\mid u)$. Thus, using log-likelihood and the method of steepest descent will fail. Instead, an iterative method that does not rely on the gradient of $p(r\mid u)$ is more successful. In degenerate cases, the result will be the initial guess, while a maximum likelihood can usually be approximated. For the rest of this paper the algorithm will be denoted as MLE-$\Gamma$.
