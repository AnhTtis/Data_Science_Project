\section{The Membership Degree Min-Max Algorithm}
\label{section:md-min-max}
 
Based on the previous work and our experiment results, Min-Max is a potential method for position estimation with the inexact range measurements.
Different from most algorithms estimating the position based on an exact mathematical derivation or probability, we propose \ac{MD-Min-Max} algorithm. \ac{MD-Min-Max} employs an empirical \acl{MF} (\acs{MF}) to convert range measurements into degrees of support on the four vertices ($\mathbf{V}=\{v_j\mid j\in{\downarrow 4}\}$) obtained by Min-Max.

For any partially ordered set $(P,\leq)$ and any $p\in P$ we define the downset ${\downarrow p}=\{q\in P\mid q\leq p\}$. From now on, we use the partially ordered set $(\mathbb{N}^+,\leq)$ of positive integers. For example, ${\downarrow 4}=\{1,2,3,4\}$.


\subsection{Concepts of Fuzzy Set}
 
Since range measurements $r$ of indoor scenarios are uncertain and imprecise, $r$ cannot estimate the target position determinately. Probability theory is the most common way to deal with uncertainty, however, it requires the \ac{pdf} of measurements and incurs high computation. More important, it cannot present the relationship between the estimated position $(\hat{x},\hat{y})$ and the defined set $\mathbf{V}$ of Min-Max. For example, if given an exact range $\bar{r}$ as Figure~\ref{fig:prob}, the weighting function $W_a(j)$ of \ac{E-Min-Max} is able to present the difference between $(\hat{x},\hat{y})$ and $\mathbf{V}$ in ideal case, however, it treats all residues equally under uncertain range errors. Probability method fails in ideal case, because $\bar{r}$ is a determinate event rather than a random variable. Fuzzy set \citep{zadeh1965fuzzy} (like 'nearby' or 'distant' of positioning) and evidences (like all range measurements) are able to describe the nearness between $(\hat{x},\hat{y})$ and $\mathbf{V}$. In Figure~\ref{fig:prob}, the result of using fuzzy set is that $(\hat{x},\hat{y})$ is nearby $v_1$ and $v_4$ but far from $v_2$ and $v_3$. Overall, fuzzy concept is more suitable to describe the relationship of $(\hat{x},\hat{y})$ to $\mathbf{V}$ obtained by Min-Max.

\subsection{Membership degree}

Of fuzzy set, we use the concept of \emph{membership degree} \citep{zadeh1965fuzzy} ($\mu(\bar{d})$) only. Here, membership degree means that one fuzzy variable partially belongs to a fuzzy set, then the estimated position is close to the $\mathbf{V}$. In \ac{MD-Min-Max} algorithm, the normal localisation formulas are replaced by rules. To make the algorithm simple and fast, we only employ one $\mathbf{rule}$ in the convert step in Algorithm~\ref{alg:md-min-max}, which presents the agreement of $(\hat{x},\hat{y})$ belonging to $\mathbf{V}$:

\begin{equation}
\label{eq:fuzzy-rule}
\begin{array}{l}
\mathbf{If}\ \left\| {v_j-\mathrm{a}} \right\| \text{ approximates } r,\\
\mathbf{then}\ (\hat{x},\hat{y})\ \text{ is nearby }v_j.\\
\end{array}
\end{equation}

A numerical value in the interval $[0,1]$ stands for the degree of agreement in Eq.\ \eqref{eq:fuzzy-rule}, and is calculated by \ac{MF}. The higher the degree is, the higher is the agreement in Eq.\ \eqref{eq:fuzzy-rule}. To show the intuition, consider Figure~\ref{fig:prob}. Eq.~\eqref{eq:fuzzy-rule} should result in a higher membership degree of $v_1$ and $v_4$ than for $v_2$ and $v_3$.
\vspace{12pt}
\begin{example}
  \begin{gather*}
    \mathbf{If}\ r-\left\| {v-\mathrm{a}} \right\| \text{ is } 1.0 \ \mathbf{then}\ r \text{ supports } v \text{ by a degree of } 0.6 \\
    \mathbf{If}\ r-\left\| {v-\mathrm{a}} \right\| \text{ is } 0.0 \ \mathbf{then}\ r \text{ supports } v \text{ by a degree of } 1.0 \\
    \mathbf{If}\ r-\left\| {v-\mathrm{a}} \right\| \text{ is } -0.3 \ \mathbf{then}\ r \text{ supports } v \text{ by a degree of } 0.9
  \end{gather*}
\end{example}
 
The framework to involve the membership degree on Min-Max is shown in Figure~\ref{fig:fuzzprocedure}, and the procedure of \ac{MD-Min-Max} is in Algorithm~\ref{alg:md-min-max}.

\begin{algorithm}
  \caption{\ac{MD-Min-Max}}\label{alg:md-min-max}
  \begin{algorithmic}[1]
    \Require{ranges $r_i$ and anchor positions $a_i$, $i \in {\downarrow N_{\mathrm{anc}}}$, and vertices $\{v_j\mid j\in{\downarrow 4}\}$ computed by Min-Max;}
    \Ensure{the estimated position $(\hat{x},\hat{y})$}
    \For{$i\in{\downarrow N_{\mathrm{anc}}}$}\Comment{Compute membership degrees}
    \For{$j\in{\downarrow 4}$}
    \State Compute $d_{ij}=|| {v_j-a_i}||$
    \State Compute $\bar{d}_{ij}$ by Eq.\ \eqref{eq:absoluted};
    \State Calculate membership degree $\mu(\bar{d}_{ij})$ by Eq. \eqref{eq:mu};
    \EndFor
    \EndFor
    \For{$j\in{\downarrow 4}$}    
    \State Calculate degree weight $\mathit{dw}_j$ by Eq.\ \eqref{eq:rulemath};
    \EndFor
    \State Estimate position $(\hat{x},\hat{y})$ as weighted average by Eq.\ \eqref{eq:aggregated1};
  \end{algorithmic}
\end{algorithm}


\subsection{Membership function}
\label{membership_function}

Typically, membership functions are defined by experts or generated from statistics. We suppose that the error distribution of distance measurements in the same scenario are similar, thus the \ac{MF} can be configured by empirical values obtained from previous experiments in the same scenario. The empirical knowledge involved in \ac{MF} helps in making the algorithm adaptive to conditions of imprecise distance measurements. The triangular \ac{MF} is determined by three parameters ($\mathit{MF}_{\mathrm{low}}, \mathit{MF}_{\mathrm{median}}, \mathit{MF}_{\mathrm{up}}$), where (($\mathit{MF}_{\mathrm{low}},0$), ($\mathit{MF}_{\mathrm{median}},1$), ($\mathit{MF}_{\mathrm{up}},0)$) are the three vertices of the triangular \ac{MF}. We calculate the three parameters of \ac{MF} as follows:
\begin{enumerate}
\item Obtain a large number of samples of range measurements $r$ and the corresponding reference ranges $\overline{r}$;
%\item Select a percentile $p$ that is used to distinguish outliers from regularly erroneous measurements;
\item Compute the median value of all $r-\bar{r}$, named as $\mathit{MF}_{\mathrm{median}}$;
\item Compute $\mathit{MF}_{\mathrm{up}}$ as the $0.995$ quantile and $\mathit{MF}_{\mathrm{low}}$ as the $0.005$ quantile;
\item Configure the triangular \ac{MF} with three parameters $((\mathit{MF}_{\mathrm{low}},0)$, $(\mathit{MF}_{\mathrm{median}},1)$, $(\mathit{MF}_{\mathrm{up}},0))$.
\end{enumerate}

\subsubsection{Analysing range measurements}
\label{range_measurements}

The first step of computing a \ac{MF} is to obtain a large number of range measurements using a reference system. For this example, we conducted an experiment where we used a robot to provide us with reference locations and collected range measurements. The experiment involved $17$ anchors placed into an office building. Each anchor was ranged $3043$ times. Since some measurements failed, we collected $22901$ distance measurements at varying distances from the anchor nodes. Figure~\ref{fig:hist-distances} displays the relative number of successful measurements at those distances. Figure~\ref{fig:range-abs-error} displays the distribution of absolute measurement errors. As one can see, the distance error is independent of the distance we measured. At short distances, measurements occur to be more noisy, with outliers at medium distances. Outlier values have been measured at a distance of about $27$ meters and $37$ meters. The lack of many large range errors at more than about $37$ meters is explained by the high probability of a failed measurement. Distance measurements of $30$ meters and longer succeeded in only $20\%$ to $30\%$ of the attempts. Figure~\ref{fig:range-abs-error} shows the distribution of the absolute errors for the measured distances. As we see, the absolute error is uncorrelated to the actual distance. Indeed, the correlation is $0.1373$. This is a strong evidence that both quantities are independent and our \ac{MF} can be formulated independent of the range, i.e.\ in terms of absolute errors.

\subsubsection{Configure \ac{MF}}

We performed two experiments, named as Mobile 1 and Mobile 2, with a mobile node moving along two different routes in the same office building. Then, a absolute range error histogram of the measurements is used to configure the \ac{MF}. The histogram presents range measurements in absolute form ($r-\bar{r}$), where $\bar{r}$ is the distance obtained by our reference system and $r$ is the measured distance. Figure~\ref{fig:Statistics} (a-b) shows the histograms for our experiments. Here, we use a triangular \ac{MF}. Its empirical parameters are shown in Figure~\ref{fig:Statistics} (c-d). 

The \ac{MF} parameters of Mobile 1 and Mobile 2 are $[-1.7, 2.38, 13.31]$ and $[-2.161, 1.636, 16.043]$ separately, which also indicates that distance in the same scenario maintains familiar behaviour. Thus, configuring the \ac{MF} based on empirical values is reasonable, making this algorithm easy to implement in other indoor scenarios.

The triangular \ac{MF} is not the only type of \ac{MF} fitting our \ac{MD-Min-Max} algorithm. Also other \ac{MF}, such as trapezoidal \ac{MF}, quadratic function \ac{MF}, rectangular \ac{MF}, or any other theoretical distribution of the statistical result can be used. \ac{MD-Min-Max} employs triangular \ac{MF} because of its conceptual simplicity and computational efficiency. For different scenarios, the \ac{MF} should be chosen according to its range condition, as approximate to the frequency histogram as possible.

\subsection{Convert Range into Membership Degree by \ac{MF}}

\ac{MD-Min-Max} first computes the four vertex coordinates $\mathbf{V}$ of \ac{IR} by Min-Max. Given the coordinates of the anchors, it is simple to compute the distance between the $i$th anchor and $j$th vertex: $d_{ij}=\left\| {v_j-a_i} \right\|$. Since the \ac{MF} is expressed in absolute measurement errors, the distance between a vertex and an anchor is also described as an absolute difference between measurement $r_i$ and $d_{ij}$, as shown in Eq.~\eqref{eq:absoluted}.
\vspace{12pt}
\begin{equation}\label{eq:absoluted}
  \bar{d}_{ij}=r_i-d_{ij}=r_i-||{v_j-a_i}||
        \text{ for $i \in {\downarrow N_{anc}}$, $j \in {\downarrow 4}$.}
\end{equation}

Then, the \ac{MF} $\mu(\bar{d})$ can be used to calculate an agreement degree $\mu(\bar{d}_{ij})$, as shown in Eq.~\eqref{eq:mu}. The range of membership degree is a real number between zero and one. It is characterized by three parameters $[\mathit{MF}_{\mathrm{low}}, \mathit{MF}_{\mathrm{median}}, \mathit{MF}_{\mathrm{up}}]$ which are obtained from the previous empirical data. For example, for positioning in Mobile~1 case, we should use the parameters obtained by the samples of Mobile~2.

\begin{equation}\label{eq:mu}
  \mu(\bar{d})=
  \begin{cases}
    \frac{\bar{d}-\mathit{MF}_{\mathrm{up}}}{\mathit{MF}_{\mathrm{median}}-\mathit{MF}_{\mathrm{up}}}&
    \text{if $\mathit{MF}_{\mathrm{median}} \leq \bar{d} < \mathit{MF}_{\mathrm{up}}$} \\
    \frac{\bar{d}-\mathit{MF}_{\mathrm{low}}}{\mathit{MF}_{\mathrm{median}}-\mathit{MF}_{\mathrm{low}}}&
    \text{if $\mathit{MF}_{\mathrm{low}} < \bar{d} < \mathit{MF}_{\mathrm{median}}$} \\
    0 & \textrm{otherwise} \\
  \end{cases}
\end{equation}

Eq.~\eqref{eq:mu} describes that the membership degree $\mu_{ij}=\mu(\bar{d}_{ij})$ decreases from $1$ to $0$ as $\bar{d}_{ij}$ moves away from $\mathit{MF}_{\mathrm{median}}$; to be more specific, if $\bar{d}_{ij}$ is outside of the interval $[\mathit{MF}_{\mathrm{low}},\mathit{MF}_{\mathrm{up}}]$, then $\mu_{ij}$ is $0$. If a range measurement $r_i$ is severely corrupted, then $\bar{d}_{ij}$ is very far from $\mathit{MF}_{\mathrm{median}}$ and $\mu(\bar{d}_{ij})$ is $0$. This is the case when the range measurement is considered to be an outlier.

A huge error between multiple ranges is uncommon as illustrated by the statistics in Figure~\ref{fig:Statistics} (a-b) and shown in several publications \citep{guvenc2007analysis,venkatraman2004novel,Venkatesh2006}. Overall, the greater the deviation from $\mathit{MF}_{\mathrm{median}}$, the higher the possibility that the range measurement has a large error. Therefore, the membership degree can averagely weaken these ranges as the long tail component in Figure~\ref{fig:Statistics} (a-b).

\subsection{Combine membership degree}

Since multiple ranges determine one estimation jointly, a conjunctive rule is made to combine multiple membership degrees into the weight on each vertex: $\mathit{dw}_j$, $j\in{\downarrow 4}$. The linguistic rule for the $j$th consequent is expressed as:
\vspace{12pt}
\begin{equation}
  \label{eq:rulelinguistic}
  \begin{array}{l}
   \textbf{If}\ \mu(\bar{d}_{ij}) \text{ fully agree to } v_j, i \in {\downarrow N_{\mathit{anc}}} \\
   \textbf{then}\ \mathit{dw}_j \text{ totally supports } v_j=(\hat{x},\hat{y}).\\
  \end{array}
\end{equation}

The signal-to-noise ratio in Eq.~\eqref{eq:rulemath}, defined as the reciprocal of the coefficient of variation of multiple degrees, is used as the weight of each vertex. The signal-to-noise ratio can be interpreted as a measure of the homogeneity of the range measurements and as the degree of agreement.
\vspace{12pt}
\begin{equation}
  \label{eq:rulemath}
  \mathit{dw}_j=
    \begin{cases}
      \frac{{\operatorname{mean} (\bigcup_{i = 1}^{N_{\mathit{anc}}} {\mu_{ij}})}}{\sqrt{\operatorname{var} (\bigcup_{i = 1}^{N_{\mathit{anc}}} {\mu_{ij}})}} & \text{if $\operatorname{var} (\bigcup_{i = 1}^{N_{\mathit{anc}}} {\mu_{ij}})> 0$} \\
      \infty & \text{if $\operatorname{var} (\bigcup_{i = 1}^{N_{\mathit{anc}}} {\mu_{ij}})= 0$}
    \end{cases}
\end{equation}

Thus, the larger $\mathit{dw}_j$ is (the higher the mean agreement and the smaller the agreement variance are), the more likely of target should be the vertex $v_j$. Combining multiple degrees in this way is not only simple, but also associates the conjunctive opinion of all ranges.


\subsection{Weighted average of~$\mathbf{V}$}

The final estimated position is the average of the four vertex coordinates weighted by their associated degree, as expressed in Eq.~\eqref{eq:aggregated1}.
\vspace{12pt}
\begin{equation}
  \label{eq:aggregated1}
  (\hat{x},\hat{y}) =\sum_{i=1}^{4} \frac{\mathit{dw}_i}{\sum_{j=1}^{4} \mathit{dw}_j} \cdot (v_{xi},v_{yi})
  \vspace{12pt}
\end{equation}
Vertices with higher accumulated degree and smaller degree variance are weighted higher. Therefore, the final estimation is considered to be a likely position within the four vertices of Min-Max, because of the good understanding of range errors derived from empirical knowledge.


\subsection{Complexity}

The run-time and memory requirements of the \ac{MD-Min-Max} algorithm are modest.
\vspace{12pt}
\begin{proposition}
  The run-time complexity of \ac{MD-Min-Max} is in $\Theta(N_{\mathit{anc}})$.
\end{proposition}

\begin{proof}
  The run-time of \ac{MD-Min-Max} is clearly dominated by the loop in Step 1. Calculating the distance and the membership degree can be performed in constant time. Weighting the degrees by the mean and the standard deviation can be performed in constant time, if a method like Welford's~\citep{welford62} is used during step 1. The loop body is executed four times for each anchor. Step 4 and 5 are again constant time.\vspace{12pt}
\end{proof}

\begin{proposition}
  The space complexity of \ac{MD-Min-Max} is in $\Theta(N_{\mathit{anc}})$.
\end{proposition}

\begin{proof}
  Most memory is required to store the two coordinates of the anchor nodes and range measurements, namely $3N_{\mathit{anc}}$ registers. Additional space is needed to store the indexing variables. The three parameters of the membership degree function, the corners of the Min-Max calculation and the weights of the four corners. 
\end{proof}

The asymptotic time and space complexity of \ac{MD-Min-Max} is equal to the one of the traditional Min-Max. Our benchmarks show that the \ac{MD-Min-Max} algorithm is about 50\% slower than the \ac{E-Min-Max} algorithms and about 9 times slower than the original Min-Max algorithm. As Min-Max is such an inexpensive algorithm, and the number of anchors $N_{\mathit{anc}}$ is low for most scenarios, limited by technical limitations of radio communication and the distance intervals, \ac{MD-Min-Max} is a viable algorithm for sensor networks, especially if we compare it to more complex algorithms like the NLLS method.
