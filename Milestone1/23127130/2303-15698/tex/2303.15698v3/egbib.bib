@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}



#### 
@article{albuquerque2019generalizing,
  title={Generalizing to unseen domains via distribution matching},
  author={Albuquerque, Isabela and Monteiro, Jo{\~a}o and Darvishi, Mohammad and Falk, Tiago H and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1911.00804},
  year={2019}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network (2015)},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  year={2015}
}
@inproceedings{zhou2021domain,
  title={Domain generalization with mixstyle},
  author={Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{zhang2021delving,
  title={Delving deep into the generalization of vision transformers under distribution shifts},
  author={Zhang, Chongzhi and Zhang, Mingyuan and Zhang, Shanghang and Jin, Daisheng and Zhou, Qiang and Cai, Zhongang and Zhao, Haiyu and Yi, Shuai and Liu, Xianglong and Liu, Ziwei},
  journal={arXiv preprint arXiv:2106.07617},
  year={2021}
}
@inproceedings{lu2021simpler,
  title={Simpler is better: Few-shot semantic segmentation with classifier weight transformer},
  author={Lu, Zhihe and He, Sen and Zhu, Xiatian and Zhang, Li and Song, Yi-Zhe and Xiang, Tao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8741--8750},
  year={2021}
}
@inproceedings{strudel2021segmenter,
  title={Segmenter: Transformer for semantic segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7262--7272},
  year={2021}
}
@InProceedings{Dai_2021_ICCV,
    author    = {Dai, Xiyang and Chen, Yinpeng and Yang, Jianwei and Zhang, Pengchuan and Yuan, Lu and Zhang, Lei},
    title     = {Dynamic DETR: End-to-End Object Detection With Dynamic Attention},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2988-2997}
}
@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}
@article{yang2013multi,
  title={Multi-view discriminant transfer learning},
  author={Yang, Pei Yang and Gao, Wei},
  year={2013},
  publisher={AAAI Press}
}
@inproceedings{wang2021embracing,
  title={Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation},
  author={Wang, Yufei and Li, Haoliang and Chau, Lap-pui and Kot, Alex C},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={2595--2604},
  year={2021}
}
@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}
@inproceedings{yun2020regularizing,
  title={Regularizing class-wise predictions via self-knowledge distillation},
  author={Yun, Sukmin and Park, Jongjin and Lee, Kimin and Shin, Jinwoo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13876--13885},
  year={2020}
}
@inproceedings{chen2021distilling,
  title={Distilling knowledge via knowledge review},
  author={Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5008--5017},
  year={2021}
}
@article{stanton2021does,
  title={Does knowledge distillation really work?},
  author={Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}
@inproceedings{zhang2019your,
  title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
  author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3713--3722},
  year={2019}
}
@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}
@article{naseer2021improving,
  title={On improving adversarial transferability of vision transformers},
  author={Naseer, Muzammal and Ranasinghe, Kanchana and Khan, Salman and Khan, Fahad Shahbaz and Porikli, Fatih},
  journal={arXiv preprint arXiv:2106.04169},
  year={2021}
}
@article{iwasawa2021test,
  title={Test-time classifier adjustment module for model-agnostic domain generalization},
  author={Iwasawa, Yusuke and Matsuo, Yutaka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{zhang2021adaptive,
  title={Adaptive risk minimization: Learning to adapt to domain shift},
  author={Zhang, Marvin and Marklund, Henrik and Dhawan, Nikita and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{krueger2021out,
  title={Out-of-distribution generalization via risk extrapolation (rex)},
  author={Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Le Priol, Remi and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5815--5826},
  year={2021},
  organization={PMLR}
}

@inproceedings{sun2016deep,
  title={Deep coral: Correlation alignment for deep domain adaptation},
  author={Sun, Baochen and Saenko, Kate},
  booktitle={European conference on computer vision},
  pages={443--450},
  year={2016},
  organization={Springer}
}
@inproceedings{li2018learning,
  title={Learning to generalize: Meta-learning for domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}
@article{yan2020improve,
  title={Improve unsupervised domain adaptation with mixup training},
  author={Yan, Shen and Song, Huan and Li, Nanxiang and Zou, Lincan and Ren, Liu},
  journal={arXiv preprint arXiv:2001.00677},
  year={2020}
}
@article{sagawa2019distributionally,
  title={Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization},
  author={Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B and Liang, Percy},
  journal={arXiv preprint arXiv:1911.08731},
  year={2019}
}
@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}
@inproceedings{beery2018recognition,
  title={Recognition in terra incognita},
  author={Beery, Sara and Van Horn, Grant and Perona, Pietro},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={456--473},
  year={2018}
}

@inproceedings{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on Imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={558--567},
  year={2021}
}
@inproceedings{wu2021cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22--31},
  year={2021}
}


@article{cha2021swad,
  title={Swad: Domain generalization by seeking flat minima},
  author={Cha, Junbum and Chun, Sanghyuk and Lee, Kyungjae and Cho, Han-Cheol and Park, Seunghyun and Lee, Yunsung and Park, Sungrae},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@InProceedings{Nam_2021_CVPR,
    author    = {Nam, Hyeonseob and Lee, HyunJae and Park, Jongchan and Yoon, Wonjun and Yoo, Donggeun},
    title     = {Reducing Domain Gap by Reducing Style Bias},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8690-8699}
}
@article{bui2021exploiting,
  title={Exploiting Domain-Specific Features to Enhance Domain Generalization},
  author={Bui, Manh-Ha and Tran, Toan and Tran, Anh and Phung, Dinh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@InProceedings{Kim_2021_ICCV,
    author    = {Kim, Daehee and Yoo, Youngjun and Park, Seunghyun and Kim, Jinkyu and Lee, Jaekoo},
    title     = {SelfReg: Self-Supervised Contrastive Regularization for Domain Generalization},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9619-9628}
    }

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}



@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2010.04159},
  year={2020}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6881--6890},
  year={2021}
}
@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={568--578},
  year={2021}
}
}
@article{tuli2021convolutional,
  title={Are Convolutional Neural Networks or Transformers more like human vision?},
  author={Tuli, Shikhar and Dasgupta, Ishita and Grant, Erin and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2105.07197},
  year={2021}
}
@article{naseer2021intriguing,
  title={Intriguing properties of vision transformers},
  author={Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{li2019episodic,
  title={Episodic training for domain generalization},
  author={Li, Da and Zhang, Jianshu and Yang, Yongxin and Liu, Cong and Song, Yi-Zhe and Hospedales, Timothy M},
  journal={In ICCV},
  year={2019}
}



@inproceedings{carlucci2019domain,
  title={Domain Generalization by Solving Jigsaw Puzzles},
  author={Carlucci, Fabio M and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2229--2238},
  year={2019}
}

@inproceedings{d2018domain,
  title={Domain generalization with domain-specific aggregation modules},
  author={Dâ€™Innocente, Antonio and Caputo, Barbara},
  booktitle={German Conference on Pattern Recognition},
  pages={187--198},
  year={2018},
  organization={Springer}
}

@book{csurka2017domain,
  title={Domain adaptation in computer vision applications},
  author={Csurka, Gabriela},
  publisher={Springer}
}

@inproceedings{ben2007analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  booktitle={Advances in neural information processing systems},
  pages={137--144},
  year={2007}
}

@article{tzeng2014deep,
  title={Deep domain confusion: Maximizing for domain invariance},
  author={Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
  journal={arXiv preprint arXiv:1412.3474},
  year={2014}
}

@article{long2015learning,
  title={Learning transferable features with deep adaptation networks},
  author={Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael I},
  journal={arXiv preprint arXiv:1502.02791},
  year={2015}
}


@inproceedings{long2016unsupervised,
  title={Unsupervised domain adaptation with residual transfer networks},
  author={Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={136--144},
  year={2016}
}

@inproceedings{bousmalis2016domain,
  title={Domain separation networks},
  author={Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  booktitle={Advances in neural information processing systems},
  pages={343--351},
  year={2016}
}
@inproceedings{kang2019contrastive,
  title={Contrastive Adaptation Network for Unsupervised Domain Adaptation},
  author={Kang, Guoliang and Jiang, Lu and Yang, Yi and Hauptmann, Alexander G},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4893--4902},
  year={2019}
}

@inproceedings{volpi2018generalizing,
  title={Generalizing to unseen domains via adversarial data augmentation},
  author={Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John C and Murino, Vittorio and Savarese, Silvio},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5334--5344},
  year={2018}
}

@inproceedings{muandet2013domain,
  title={Domain generalization via invariant feature representation},
  author={Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={10--18},
  year={2013}
}

@inproceedings{ghifary2015domain,
  title={Domain generalization for object recognition with multi-task autoencoders},
  author={Ghifary, Muhammad and Bastiaan Kleijn, W and Zhang, Mengjie and Balduzzi, David},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2551--2559},
  year={2015}
}

@inproceedings{fang2013unbiased,
  title={Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias},
  author={Fang, Chen and Xu, Ye and Rockmore, Daniel N},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1657--1664},
  year={2013}
}

@inproceedings{venkateswara2017deep,
  title={Deep hashing network for unsupervised domain adaptation},
  author={Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5018--5027},
  year={2017}
}

@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7794--7803},
  year={2018}
}

@article{huang2018ccnet,
  title={Ccnet: Criss-cross attention for semantic segmentation},
  author={Huang, Zilong and Wang, Xinggang and Huang, Lichao and Huang, Chang and Wei, Yunchao and Liu, Wenyu},
  journal={arXiv preprint arXiv:1811.11721},
  year={2018}
}

@inproceedings{hu2018squeeze,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}

@inproceedings{hu2018gather,
  title={Gather-excite: Exploiting feature context in convolutional neural networks},
  author={Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Vedaldi, Andrea},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9401--9411},
  year={2018}
}

@inproceedings{woo2018cbam,
  title={Cbam: Convolutional block attention module},
  author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and So Kweon, In},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={3--19},
  year={2018}
}

@article{geirhos2018imagenet,
  title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
  author={Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
  journal={arXiv preprint arXiv:1811.12231},
  year={2018}
}

@article{jo2017measuring,
  title={Measuring the tendency of CNNs to learn surface statistical regularities},
  author={Jo, Jason and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1711.11561},
  year={2017}
}

@inproceedings{xu2014exploiting,
  title={Exploiting low-rank structure from latent domains for domain generalization},
  author={Xu, Zheng and Li, Wen and Niu, Li and Xu, Dong},
  booktitle={European Conference on Computer Vision},
  pages={628--643},
  year={2014},
  organization={Springer}
}

@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

 @inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{mancini2018best,
  title={Best sources forward: domain generalization through source-specific nets},
  author={Mancini, Massimiliano and Bul{\`o}, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
  pages={1353--1357},
  year={2018},
  organization={IEEE}
}


@article{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  journal={arXiv preprint arXiv:1905.02175},
  year={2019}
}

@article{wang2019high,
  title={High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks},
  author={Wang, Haohan and Wu, Xindi and Yin, Pengcheng and Xing, Eric P},
  journal={arXiv preprint arXiv:1905.13545},
  year={2019}
}

@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.12152},
  year={2018}
}

@article{huang2018ccnet,
  title={Ccnet: Criss-cross attention for semantic segmentation},
  author={Huang, Zilong and Wang, Xinggang and Huang, Lichao and Huang, Chang and Wei, Yunchao and Liu, Wenyu},
  journal={arXiv preprint arXiv:1811.11721},
  year={2018}
}

@inproceedings{rodriguez2018attend,
  title={Attend and rectify: a gated attention mechanism for fine-grained recovery},
  author={Rodr{\'\i}guez, Pau and Gonfaus, Josep M and Cucurull, Guillem and XavierRoca, F and Gonzalez, Jordi},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={349--364},
  year={2018}
}

@article{shen2017learning,
  title={Learning object detectors from scratch with gated recurrent feature pyramids},
  author={Shen, Zhiqiang and Shi, Honghui and Feris, Rogerio and Cao, Liangliang and Yan, Shuicheng and Liu, Ding and Wang, Xinchao and Xue, Xiangyang and Huang, Thomas S},
  journal={arXiv preprint arXiv:1712.00886},
  year={2017}
}

@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={23--30},
  year={2017},
  organization={IEEE}
}

@inproceedings{khosla2012undoing,
  title={Undoing the damage of dataset bias},
  author={Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A and Torralba, Antonio},
  booktitle={European Conference on Computer Vision},
  pages={158--171},
  year={2012},
  organization={Springer}
}

@inproceedings{li2018deep,
  title={Deep domain generalization via conditional invariant adversarial networks},
  author={Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={624--639},
  year={2018}
}

@article{brendel2019approximating,
  title={Approximating cnns with bag-of-local-features models works surprisingly well on imagenet},
  author={Brendel, Wieland and Bethge, Matthias},
  journal={arXiv preprint arXiv:1904.00760},
  year={2019}
}

@inproceedings{hu2018squeeze,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{hu2018gather,
  title={Gather-excite: Exploiting feature context in convolutional neural networks},
  author={Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Vedaldi, Andrea},
  booktitle={NIPS},
  year={2018}
}
@inproceedings{zhao2018psanet,
  title={Psanet: Point-wise spatial attention network for scene parsing},
  author={Zhao, Hengshuang and Zhang, Yi and Liu, Shu and Shi, Jianping and Change Loy, Chen and Lin, Dahua and Jia, Jiaya},
  booktitle={ECCV},
  year={2018}
}

@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1-2},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@inproceedings{chen2019synergistic,
  title={Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation},
  author={Chen, Cheng and Dou, Qi and Chen, Hao and Qin, Jing and Heng, Pheng-Ann},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={865--872},
  year={2019}
}
@inproceedings{hong2018conditional,
  title={Conditional generative adversarial network for structured domain adaptation},
  author={Hong, Weixiang and Wang, Zhenzhen and Yang, Ming and Yuan, Junsong},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1335--1344},
  year={2018}
}

@inproceedings{yue2019domain,
  title={Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data},
  author={Yue, Xiangyu and Zhang, Yang and Zhao, Sicheng and Sangiovanni-Vincentelli, Alberto and Keutzer, Kurt and Gong, Boqing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2100--2110},
  year={2019}
}

@article{dou2018unsupervised,
  title={Unsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adversarial loss},
  author={Dou, Qi and Ouyang, Cheng and Chen, Cheng and Chen, Hao and Heng, Pheng-Ann},
  journal={arXiv preprint arXiv:1804.10916},
  year={2018}
}

@inproceedings{ren2018adversarial,
  title={Adversarial domain adaptation for classification of prostate histopathology whole-slide images},
  author={Ren, Jian and Hacihaliloglu, Ilker and Singer, Eric A and Foran, David J and Qi, Xin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={201--209},
  year={2018},
  organization={Springer}
}
@inproceedings{chen2018road,
  title={Road: Reality oriented adaptation for semantic segmentation of urban scenes},
  author={Chen, Yuhua and Li, Wen and Van Gool, Luc},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7892--7901},
  year={2018}
}
@inproceedings{tsai2018learning,
  title={Learning to adapt structured output space for semantic segmentation},
  author={Tsai, Yi-Hsuan and Hung, Wei-Chih and Schulter, Samuel and Sohn, Kihyuk and Yang, Ming-Hsuan and Chandraker, Manmohan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7472--7481},
  year={2018}
}
@inproceedings{tsai2019domain,
  title={Domain adaptation for structured output via discriminative patch representations},
  author={Tsai, Yi-Hsuan and Sohn, Kihyuk and Schulter, Samuel and Chandraker, Manmohan},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1456--1465},
  year={2019}
}
@inproceedings{vu2019advent,
  title={Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation},
  author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and P{\'e}rez, Patrick},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2517--2526},
  year={2019}
}
@article{wang2019patch,
  title={Patch-based output space adversarial learning for joint optic disc and cup segmentation},
  author={Wang, Shujun and Yu, Lequan and Yang, Xin and Fu, Chi-Wing and Heng, Pheng-Ann},
  journal={IEEE transactions on medical imaging},
  volume={38},
  number={11},
  pages={2485--2495},
  publisher={IEEE}
}


@article{wang2020learning,
  title={Learning from Extrinsic and Intrinsic Supervisions for Domain Generalization},
  author={Wang, Shujun and Yu, Lequan and Li, Caizi and Fu, Chi-Wing and Heng, Pheng-Ann},
   booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2020}
}

@article{chattopadhyay2020learning,
  title={Learning to balance specificity and invariance for in and out of domain generalization},
  author={Chattopadhyay, Prithvijit and Balaji, Yogesh and Hoffman, Judy},
   booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
   year={2020}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={6023--6032},
  year={2019}
}

@article{hendrycks2019augmix,
  title={Augmix: A simple data processing method to improve robustness and uncertainty},
  author={Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02781},
  year={2019}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi@article{zhang2023deep,
  title={Deep Representation Learning for Domain Generalization with Information Bottleneck Principle},
  author={Zhang, Jiao and Zhang, Xu-Yao and Wang, Chuang and Liu, Cheng-Lin},
  journal={Pattern Recognition},
  pages={109737},
  year={2023},
  publisher={Elsevier}
} and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@article{jackson2018style,
  title={Style Augmentation: Data Augmentation via Style Randomization},
  author={Jackson, Philip T and Atapour-Abarghouei, Amir and Bonner, Stephen and Breckon, Toby and Obara, Boguslaw},
  journal={arXiv},
  pages={arXiv--1809},
  year={2018}
}

@article{seo2019learning,
  title={Learning to Optimize Domain Specific Normalization for Domain Generalization},
  author={Seo, Seonguk and Suh, Yumin and Kim, Dongwan and Kim, Geeho and Han, Jongwoo and Han, Bohyung},
   booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
 
  year={2020}
 }
 
@inproceedings{upchurch2017deep,
  title={Deep feature interpolation for image content changes},
  author={Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7064--7073},
  year={2017}
}

@inproceedings{bengio2013better,
  title={Better mixing via deep representations},
  author={Bengio, Yoshua and Mesnil, Gr{\'e}goire and Dauphin, Yann and Rifai, Salah},
  booktitle={International conference on machine learning},
  pages={552--560},
  year={2013}
}
@inproceedings{wang2019implicit,
  title={Implicit Semantic Data Augmentation for Deep Networks},
  author={Wang, Yulin and Pan, Xuran and Song, Shiji and Zhang, Hong and Huang, Gao and Wu, Cheng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12635--12644},
  year={2019}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}


@article{netzerreading2011,
  title={Reading Digits in Natural Images with Unsupervised Feature Learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011},
  publisher={NeurIPS-W}
}

@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1406--1415},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{krizhevsky2017imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{grosso2015analysis,
  title={On the analysis and comparison of conformer-specific essential dynamics upon ligand binding to a protein},
  author={Grosso, Marcos and Kalstein, Adrian and Parisi, Gustavo and Roitberg, Adrian E and Fernandez-Alberti, Sebastian},
  journal={The Journal of Chemical Physics},
  volume={142},
  number={24},
  pages={06B619\_1},
  year={2015},
  publisher={AIP Publishing LLC}
}
@inproceedings{shi2020informative,
  title={Informative Dropout for Robust Representation Learning: A Shape-bias Perspective},
  author={Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
  booktitle={Proceedings of the 37th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020},
  year={2020}
}

@inproceedings{fernando2013unsupervised,
  title={Unsupervised visual domain adaptation using subspace alignment},
  author={Fernando, Basura and Habrard, Amaury and Sebban, Marc and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2960--2967},
  year={2013}
}

@inproceedings{gong2012geodesic,
  title={Geodesic flow kernel for unsupervised domain adaptation},
  author={Gong, Boqing and Shi, Yuan and Sha, Fei and Grauman, Kristen},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2066--2073},
  year={2012},
  organization={IEEE}
}



@inproceedings{shrivastava2014unsupervised,
  title={Unsupervised domain adaptation using parallel transport on grassmann manifold},
  author={Shrivastava, Ashish and Shekhar, Sumit and Patel, Vishal M},
  booktitle={IEEE winter conference on applications of computer vision},
  pages={277--284},
  year={2014},
  organization={IEEE}
}
@inproceedings{thopalli2019multiple,
  title={Multiple subspace alignment improves domain adaptation},
  author={Thopalli, Kowshik and Anirudh, Rushil and Thiagarajan, Jayaraman J and Turaga, Pavan},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3552--3556},
  year={2019},
  organization={IEEE}
}
@InProceedings{Prashnani_2018_CVPR,
author = {Prashnani, Ekta and Cai, Hong and Mostofi, Yasamin and Sen, Pradeep},
title = {PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@article{krzanowski1979between,
  title={Between-groups comparison of principal components},
  author={Krzanowski, WJ},
  journal={Journal of the American Statistical Association},
  volume={74},
  number={367},
  pages={703--707},
  year={1979},
  publisher={Taylor \& Francis}
}

@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}
@article{brock2018large,
  title={Large scale GAN training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}
@inproceedings{mahendran2015understanding,
  title={Understanding deep image representations by inverting them},
  author={Mahendran, Aravindh and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5188--5196},
  year={2015}
}
@inproceedings{upchurch2017deep,
  title={Deep feature interpolation for image content changes},
  author={Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7064--7073},
  year={2017}
}

@article{blanchard2011generalizing,
  title={Generalizing from several related classification tasks to a new unlabeled sample},
  author={Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={2178--2186},
  year={2011}
}

@article{blanchard2017domain,
  title={Domain generalization by marginal transfer learning},
  author={Blanchard, Gilles and Deshmukh, Aniket Anand and Dogan, Urun and Lee, Gyemin and Scott, Clayton},
  journal={arXiv preprint arXiv:1711.07910},
  year={2017}
}

@article{zhang2018mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={In International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{mancini2020towards,
  title={Towards recognizing unseen categories in unseen domains},
  author={Mancini, Massimiliano and Akata, Zeynep and Ricci, Elisa and Caputo, Barbara},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXIII 16},
  pages={466--483},
  year={2020},
  organization={Springer}
}
@article{khan2021mode,
  title={Mode-Guided Feature Augmentation for Domain Generalization},
  author={Khan, Muhammad Haris and Zaidi, Talha and Khan, Salman and Khan, Fahad Shehbaz},
  year={2021}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM Computing Surveys (CSUR)},
  publisher={ACM New York, NY}
}


@inproceedings{li2018domain,
  title={Domain generalization with adversarial feature learning},
  author={Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5400--5409},
  year={2018}
}


@inproceedings{ganin2015unsupervised,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


@article{gidaris2018unsupervised,
  title={Unsupervised representation learning by predicting image rotations},
  author={Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1803.07728},
  year={2018}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}


@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@incollection{sudre2017generalised,
  title={Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations},
  author={Sudre, Carole H and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M},
  booktitle={Deep learning in medical image analysis and multimodal learning for clinical decision support},
  pages={240--248},
  year={2017},
  publisher={Springer}
}


@inproceedings{li2017deeper,
  title={Deeper, broader and artier domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5542--5550},
  year={2017}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}




@article{albuquerqueImprovingOutofdistributionGeneralization2020,
  title = {Improving Out-of-Distribution Generalization via Multi-Task Self-Supervised Pretraining},
  author = {Albuquerque, Isabela and Naik, Nikhil and Li, Junnan and Keskar, Nitish and Socher, Richard},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.13525 [cs]},
  eprint = {2003.13525},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Self-supervised feature representations have been shown to be useful for supervised classification, few-shot learning, and adversarial robustness. We show that features obtained using self-supervised learning are comparable to, or better than, supervised learning for domain generalization in computer vision. We introduce a new self-supervised pretext task of predicting responses to Gabor filter banks and demonstrate that multi-task learning of compatible pretext tasks improves domain generalization performance as compared to training individual tasks alone. Features learnt through self-supervision obtain better generalization to unseen domains when compared to their supervised counterpart when there is a larger domain shift between training and test distributions and even show better localization ability for objects of interest. Self-supervised feature representations can also be combined with other domain generalization methods to further boost performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\3HG7X45V\\Albuquerque et al. - 2020 - Improving out-of-distribution generalization via m.pdf;C\:\\Users\\milad\\Zotero\\storage\\MYN874P6\\2003.html}
}

@inproceedings{blanchardGeneralizingSeveralRelated2011a,
  title = {Generalizing from {{Several Related Classification Tasks}} to a {{New Unlabeled Sample}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\milad\\Zotero\\storage\\6QJL3EVX\\Blanchard et al. - 2011 - Generalizing from Several Related Classification T.pdf}
}

@article{bucciSelfSupervisedLearningDomains2021,
  title = {Self-{{Supervised Learning Across Domains}}},
  author = {Bucci, Silvia and D'Innocente, Antonio and Liao, Yujun and Carlucci, Fabio Maria and Caputo, Barbara and Tommasi, Tatiana},
  year = {2021},
  month = mar,
  journal = {arXiv:2007.12368 [cs]},
  eprint = {2007.12368},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Human adaptability relies crucially on learning and merging knowledge from both supervised and unsupervised tasks: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the problem of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals on the same images. This secondary task helps the network to focus on object shapes, learning concepts like spatial orientation and part correlation, while acting as a regularizer for the classification task over multiple visual domains. Extensive experiments confirm our intuition and show that our multi-task method combining supervised and self-supervised knowledge shows competitive results with respect to more complex domain generalization and adaptation solutions. It also proves its potential in the novel and challenging predictive and partial domain adaptation scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\milad\\Zotero\\storage\\JY7NDK83\\Bucci et al. - 2021 - Self-Supervised Learning Across Domains.pdf;C\:\\Users\\milad\\Zotero\\storage\\WNL97AW4\\2007.html}
}

@article{carlucciDomainGeneralizationSolving2019,
  title = {Domain {{Generalization}} by {{Solving Jigsaw Puzzles}}},
  author = {Carlucci, Fabio Maria and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
  year = {2019},
  month = apr,
  journal = {arXiv:1903.06864 [cs]},
  eprint = {1903.06864},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Human adaptability relies crucially on the ability to learn and merge knowledge both from supervised and unsupervised learning: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the task of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals how to solve a jigsaw puzzle on the same images. This secondary task helps the network to learn the concepts of spatial correlation while acting as a regularizer for the classification task. Multiple experiments on the PACS, VLCS, Office-Home and digits datasets confirm our intuition and show that this simple method outperforms previous domain generalization and adaptation solutions. An ablation study further illustrates the inner workings of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\6UXT8FL4\\Carlucci et al. - 2019 - Domain Generalization by Solving Jigsaw Puzzles.pdf;C\:\\Users\\milad\\Zotero\\storage\\JAKEM35K\\1903.html}
}

@inproceedings{carlucciHallucinatingAgnosticImages2019,
  title = {Hallucinating {{Agnostic Images}} to {{Generalize Across Domains}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {Carlucci, Fabio Maria and Russo, Paolo and Tommasi, Tatiana and Caputo, Barbara},
  year = {2019},
  month = oct,
  pages = {3227--3234},
  issn = {2473-9944},
  doi = {10.1109/ICCVW.2019.00403},
  abstract = {The ability to generalize across visual domains is crucial for the robustness of artificial recognition systems. Although many training sources may be available in real contexts, the access to even unlabeled target samples cannot be taken for granted, which makes standard unsupervised domain adaptation methods inapplicable in the wild. In this work we investigate how to exploit multiple sources by hallucinating a deep visual domain composed of images, possibly unrealistic, able to maintain categorical knowledge while discarding specific source styles. The produced agnostic images are the result of a deep architecture that applies pixel adaptation on the original source data guided by two adversarial domain classifier branches at image and feature level. Our approach is conceived to learn only from source data, but it seamlessly extends to the use of unlabeled target samples. Remarkable results for both multi-source domain adaptation and domain generalization support the power of hallucinating agnostic images in this framework.},
  keywords = {deep learning,domain adapatation,domain generalization,Feature extraction,Image recognition,Machine learning,multisource domain adaptation,object recognition,Standards,Target recognition,Training,Visualization},
  file = {C\:\\Users\\milad\\Zotero\\storage\\JUJF47BX\\Carlucci et al. - 2019 - Hallucinating Agnostic Images to Generalize Across.pdf}
}

@article{choiStarGANUnifiedGenerative2018,
  title = {{{StarGAN}}: {{Unified Generative Adversarial Networks}} for {{Multi-Domain Image-to-Image Translation}}},
  shorttitle = {{{StarGAN}}},
  author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  year = {2018},
  month = sep,
  journal = {arXiv:1711.09020 [cs]},
  eprint = {1711.09020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\milad\\Zotero\\storage\\CII6XTWN\\Choi et al. - 2018 - StarGAN Unified Generative Adversarial Networks f.pdf;C\:\\Users\\milad\\Zotero\\storage\\V76HGTS7\\1711.html}
}

@article{everinghamPascalVisualObject2010,
  title = {The {{Pascal Visual Object Classes}} ({{VOC}}) {{Challenge}}},
  author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
  year = {2010},
  month = jun,
  journal = {International Journal of Computer Vision},
  volume = {88},
  number = {2},
  pages = {303--338},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-009-0275-4},
  langid = {english},
  file = {C\:\\Users\\milad\\Zotero\\storage\\IIUVQHEG\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf}
}

@inproceedings{fangUnbiasedMetricLearning2013a,
  title = {Unbiased {{Metric Learning}}: {{On}} the {{Utilization}} of {{Multiple Datasets}} and {{Web Images}} for {{Softening Bias}}},
  shorttitle = {Unbiased {{Metric Learning}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Fang, Chen and Xu, Ye and Rockmore, Daniel N.},
  year = {2013},
  pages = {1657--1664},
  file = {C\:\\Users\\milad\\Zotero\\storage\\M4V7799L\\Fang et al. - 2013 - Unbiased Metric Learning On the Utilization of Mu.pdf}
}

@inproceedings{fei-feiLearningGenerativeVisual2004,
  title = {Learning {{Generative Visual Models}} from {{Few Training Examples}}: {{An Incremental Bayesian Approach Tested}} on 101 {{Object Categories}}},
  shorttitle = {Learning {{Generative Visual Models}} from {{Few Training Examples}}},
  booktitle = {2004 {{Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshop}}},
  author = {{Fei-Fei}, Li and Fergus, R. and Perona, P.},
  year = {2004},
  month = jun,
  pages = {178--178},
  doi = {10.1109/CVPR.2004.383},
  abstract = {Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.},
  keywords = {Assembly,Bayesian methods,Humans,Image databases,Image recognition,Machine vision,Maximum likelihood estimation,Parameter estimation,Shape,Testing},
  file = {C\:\\Users\\milad\\Zotero\\storage\\MZEYGT4N\\1384978.html}
}

@article{ganinUnsupervisedDomainAdaptation2015,
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  month = feb,
  journal = {arXiv:1409.7495 [cs, stat]},
  eprint = {1409.7495},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\6IUL6D7B\\Ganin and Lempitsky - 2015 - Unsupervised Domain Adaptation by Backpropagation.pdf;C\:\\Users\\milad\\Zotero\\storage\\KQC8F9HU\\1409.html}
}

@article{hendrycksBenchmarkingNeuralNetwork2019,
  title = {Benchmarking {{Neural Network Robustness}} to {{Common Corruptions}} and {{Perturbations}}},
  author = {Hendrycks, Dan and Dietterich, Thomas},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.12261 [cs, stat]},
  eprint = {1903.12261},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\XXDBSQ4D\\Hendrycks and Dietterich - 2019 - Benchmarking Neural Network Robustness to Common C.pdf}
}

@article{huangArbitraryStyleTransfer2017,
  title = {Arbitrary {{Style Transfer}} in {{Real-time}} with {{Adaptive Instance Normalization}}},
  author = {Huang, Xun and Belongie, Serge},
  year = {2017},
  month = jul,
  journal = {arXiv:1703.06868 [cs]},
  eprint = {1703.06868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\milad\\Zotero\\storage\\FR8JV6WS\\Huang and Belongie - 2017 - Arbitrary Style Transfer in Real-time with Adaptiv.pdf;C\:\\Users\\milad\\Zotero\\storage\\55IPE577\\1703.html}
}

@article{ilseDIVADomainInvariant2019,
  title = {{{DIVA}}: {{Domain Invariant Variational Autoencoders}}},
  shorttitle = {{{DIVA}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Louizos, Christos and Welling, Max},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.10427 [cs, stat]},
  eprint = {1905.10427},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\L37RGE2N\\Ilse et al. - 2019 - DIVA Domain Invariant Variational Autoencoders.pdf;C\:\\Users\\milad\\Zotero\\storage\\S2ET3M75\\1905.html}
}

@article{jeonFeatureStylizationDomainaware2021a,
  title = {Feature {{Stylization}} and {{Domain-aware Contrastive Learning}} for {{Domain Generalization}}},
  author = {Jeon, Seogkyu and Hong, Kibeom and Lee, Pilhyeon and Lee, Jewook and Byun, Hyeran},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.08596 [cs]},
  eprint = {2108.08596},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Domain generalization aims to enhance the model robustness against domain shift without accessing the target domain. Since the available source domains for training are limited, recent approaches focus on generating samples of novel domains. Nevertheless, they either struggle with the optimization problem when synthesizing abundant domains or cause the distortion of class semantics. To these ends, we propose a novel domain generalization framework where feature statistics are utilized for stylizing original features to ones with novel domain properties. To preserve class information during stylization, we first decompose features into high and low frequency components. Afterward, we stylize the low frequency components with the novel domain styles sampled from the manipulated statistics, while preserving the shape cues in high frequency ones. As the final step, we re-merge both components to synthesize novel domain features. To enhance domain robustness, we utilize the stylized features to maintain the model consistency in terms of features as well as outputs. We achieve the feature consistency with the proposed domain-aware supervised contrastive loss, which ensures domain invariance while increasing class discriminability. Experimental results demonstrate the effectiveness of the proposed feature stylization and the domain-aware contrastive loss. Through quantitative comparisons, we verify the lead of our method upon existing state-of-the-art methods on two benchmarks, PACS and Office-Home.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\milad\\Zotero\\storage\\VJTS9TTI\\Jeon et al. - 2021 - Feature Stylization and Domain-aware Contrastive L.pdf;C\:\\Users\\milad\\Zotero\\storage\\2JNHM4MP\\2108.html}
}

@article{lecunGradientBasedLearningApplied1998,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  author = {Lecun, Yann and Bottou, Leon and Bengio, Y. and Haffner, Patrick},
  year = {1998},
  month = dec,
  journal = {Proceedings of the IEEE},
  volume = {86},
  pages = {2278--2324},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  file = {C\:\\Users\\milad\\Zotero\\storage\\MJBVY9IR\\Lecun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf}
}

@inproceedings{liDeeperBroaderArtier2017,
  title = {Deeper, {{Broader}} and {{Artier Domain Generalization}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  pages = {5542--5550},
  file = {C\:\\Users\\milad\\Zotero\\storage\\8WH6RPDZ\\Li et al. - 2017 - Deeper, Broader and Artier Domain Generalization.pdf}
}

@inproceedings{liDomainGeneralizationAdversarial2018,
  title = {Domain {{Generalization}} with {{Adversarial Feature Learning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C.},
  year = {2018},
  month = jun,
  pages = {5400--5409},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00566},
  abstract = {In this paper, we tackle the problem of domain generalization: how to learn a generalized feature representation for an "unseen" target domain by taking the advantage of multiple seen source-domain data. We present a novel framework based on adversarial autoencoders to learn a generalized latent feature representation across domains for domain generalization. To be specific, we extend adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to an arbitrary prior distribution via adversarial feature learning. In this way, the learned feature representation is supposed to be universal to the seen source domains because of the MMD regularization, and is expected to generalize well on the target domain because of the introduction of the prior distribution. We proposed an algorithm to jointly train different components of our proposed framework. Extensive experiments on various vision tasks demonstrate that our proposed framework can learn better generalized features for the unseen target domain compared with state-of-the-art domain generalization methods.},
  keywords = {Adaptation models,Data models,Decoding,Gallium nitride,Predictive models,Training,Training data},
  file = {C\:\\Users\\milad\\Zotero\\storage\\8VFLW33X\\8578664.html}
}

@article{liDomainGeneralizationConditional2018,
  title = {Domain {{Generalization}} via {{Conditional Invariant Representation}}},
  author = {Li, Ya and Gong, Mingming and Tian, Xinmei and Liu, Tongliang and Tao, Dacheng},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.08479 [cs, stat]},
  eprint = {1807.08479},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Domain generalization aims to apply knowledge gained from multiple labeled source domains to unseen target domains. The main difficulty comes from the dataset bias: training data and test data have different distributions, and the training set contains heterogeneous samples from different distributions. Let \$X\$ denote the features, and \$Y\$ be the class labels. Existing domain generalization methods address the dataset bias problem by learning a domain-invariant representation \$h(X)\$ that has the same marginal distribution \$\textbackslash mathbb\{P\}(h(X))\$ across multiple source domains. The functional relationship encoded in \$\textbackslash mathbb\{P\}(Y|X)\$ is usually assumed to be stable across domains such that \$\textbackslash mathbb\{P\}(Y|h(X))\$ is also invariant. However, it is unclear whether this assumption holds in practical problems. In this paper, we consider the general situation where both \$\textbackslash mathbb\{P\}(X)\$ and \$\textbackslash mathbb\{P\}(Y|X)\$ can change across all domains. We propose to learn a feature representation which has domain-invariant class conditional distributions \$\textbackslash mathbb\{P\}(h(X)|Y)\$. With the conditional invariant representation, the invariance of the joint distribution \$\textbackslash mathbb\{P\}(h(X),Y)\$ can be guaranteed if the class prior \$\textbackslash mathbb\{P\}(Y)\$ does not change across training and test domains. Extensive experiments on both synthetic and real data demonstrate the effectiveness of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\3495IZ3F\\Li et al. - 2018 - Domain Generalization via Conditional Invariant Re.pdf;C\:\\Users\\milad\\Zotero\\storage\\PZYRJEWE\\1807.html}
}

@article{manciniRecognizingUnseenCategories2020,
  title = {Towards {{Recognizing Unseen Categories}} in {{Unseen Domains}}},
  author = {Mancini, Massimiliano and Akata, Zeynep and Ricci, Elisa and Caputo, Barbara},
  year = {2020},
  month = aug,
  journal = {arXiv:2007.12256 [cs]},
  eprint = {2007.12256},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Current deep visual recognition systems suffer from severe performance degradation when they encounter new images from classes and scenarios unseen during training. Hence, the core challenge of Zero-Shot Learning (ZSL) is to cope with the semantic-shift whereas the main challenge of Domain Adaptation and Domain Generalization (DG) is the domain-shift. While historically ZSL and DG tasks are tackled in isolation, this work develops with the ambitious goal of solving them jointly,i.e. by recognizing unseen visual concepts in unseen domains. We presentCuMix (CurriculumMixup for recognizing unseen categories in unseen domains), a holistic algorithm to tackle ZSL, DG and ZSL+DG. The key idea of CuMix is to simulate the test-time domain and semantic shift using images and features from unseen domains and categories generated by mixing up the multiple source domains and categories available during training. Moreover, a curriculum-based mixing policy is devised to generate increasingly complex training samples. Results on standard SL and DG datasets and on ZSL+DG using the DomainNet benchmark demonstrate the effectiveness of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\milad\\Zotero\\storage\\JYSYFB6H\\Mancini et al. - 2020 - Towards Recognizing Unseen Categories in Unseen Do.pdf;C\:\\Users\\milad\\Zotero\\storage\\TA4UWCRK\\2007.html}
}

@article{muandetDomainGeneralizationInvariant2013,
  title = {Domain {{Generalization}} via {{Invariant Feature Representation}}},
  author = {Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  year = {2013},
  month = jan,
  journal = {arXiv:1301.2115 [cs, stat]},
  eprint = {1301.2115},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\55WT6IX9\\Muandet et al. - 2013 - Domain Generalization via Invariant Feature Repres.pdf;C\:\\Users\\milad\\Zotero\\storage\\E77Q8CY6\\1301.html}
}

@inproceedings{netzerReadingDigitsNatural2011,
  title = {Reading {{Digits}} in {{Natural Images}} with {{Unsupervised Feature Learning}}},
  booktitle = {{{NIPS Workshop}} on {{Deep Learning}} and {{Unsupervised Feature Learning}} 2011},
  author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
  year = {2011},
  file = {C\:\\Users\\milad\\Zotero\\storage\\5H39GE2A\\Netzer et al. - 2011 - Reading Digits in Natural Images with Unsupervised.pdf}
}

@article{russellLabelMeDatabaseWebBased2008,
  title = {{{LabelMe}}: {{A Database}} and {{Web-Based Tool}} for {{Image Annotation}}},
  shorttitle = {{{LabelMe}}},
  author = {Russell, Bryan C. and Torralba, Antonio and Murphy, Kevin P. and Freeman, William T.},
  year = {2008},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {77},
  number = {1-3},
  pages = {157--173},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-007-0090-8},
  langid = {english}
}

@article{somavarapuFrustratinglySimpleDomain2020,
  title = {Frustratingly {{Simple Domain Generalization}} via {{Image Stylization}}},
  author = {Somavarapu, Nathan and Ma, Chih-Yao and Kira, Zsolt},
  year = {2020},
  month = jul,
  journal = {arXiv:2006.11207 [cs]},
  eprint = {2006.11207},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional Neural Networks (CNNs) show impressive performance in the standard classification setting where training and testing data are drawn i.i.d. from a given domain. However, CNNs do not readily generalize to new domains with different statistics, a setting that is simple for humans. In this work, we address the Domain Generalization problem, where the classifier must generalize to an unknown target domain. Inspired by recent works that have shown a difference in biases between CNNs and humans, we demonstrate an extremely simple yet effective method, namely correcting this bias by augmenting the dataset with stylized images. In contrast with existing stylization works, which use external data sources such as art, we further introduce a method that is entirely in-domain using no such extra sources of data. We provide a detailed analysis as to the mechanism by which the method works, verifying our claim that it changes the shape/texture bias, and demonstrate results surpassing or comparable to the state of the arts that utilize much more complex methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\4A9DV56U\\Somavarapu et al. - 2020 - Frustratingly Simple Domain Generalization via Ima.pdf;C\:\\Users\\milad\\Zotero\\storage\\WTVL87IG\\2006.html}
}

@inproceedings{xiaoSUNDatabaseLargescale2010,
  title = {{{SUN}} Database: {{Large-scale}} Scene Recognition from Abbey to Zoo},
  shorttitle = {{{SUN}} Database},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},
  year = {2010},
  month = jun,
  pages = {3485--3492},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2010.5539970},
  abstract = {Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.},
  keywords = {Anthropometry,Bridges,Computer vision,Humans,Image databases,Large-scale systems,Layout,Legged locomotion,Spatial databases,Sun},
  file = {C\:\\Users\\milad\\Zotero\\storage\\BY7B3G5S\\Xiao et al. - 2010 - SUN database Large-scale scene recognition from a.pdf;C\:\\Users\\milad\\Zotero\\storage\\RABITME5\\5539970.html}
}

@inproceedings{yueDomainRandomizationPyramid2019,
  title = {Domain {{Randomization}} and {{Pyramid Consistency}}: {{Simulation-to-Real Generalization Without Accessing Target Domain Data}}},
  shorttitle = {Domain {{Randomization}} and {{Pyramid Consistency}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yue, Xiangyu and Zhang, Yang and Zhao, Sicheng and {Sangiovanni-Vincentelli}, Alberto and Keutzer, Kurt and Gong, Boqing},
  year = {2019},
  pages = {2100--2110}
}

@article{zhouDeepDomainAdversarialImage2020,
  title = {Deep {{Domain-Adversarial Image Generation}} for {{Domain Generalisation}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {07},
  pages = {13025--13032},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i07.7003},
  abstract = {Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on Deep Domain-Adversarial Image Generation (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {C\:\\Users\\milad\\Zotero\\storage\\YSBM52UX\\Zhou et al. - 2020 - Deep Domain-Adversarial Image Generation for Domai.pdf}
}

@article{zhouDomainGeneralizationMixStyle2021,
  title = {Domain {{Generalization}} with {{MixStyle}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.02008 [cs]},
  eprint = {2104.02008},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.\textasciitilde sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\PD7J2GBM\\Zhou et al. - 2021 - Domain Generalization with MixStyle.pdf;C\:\\Users\\milad\\Zotero\\storage\\4W4FRYYR\\2104.html}
}

@article{zhouDomainGeneralizationVision2021,
  title = {Domain {{Generalization}} in {{Vision}}: {{A Survey}}},
  shorttitle = {Domain {{Generalization}} in {{Vision}}},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  year = {2021},
  month = jul,
  journal = {arXiv:2103.02503 [cs]},
  eprint = {2103.02503},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Since first introduced in 2011, research in DG has made great progresses. In particular, intensive research in this topic has led to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, just to name a few; and has covered various vision applications such as object recognition, segmentation, action recognition, and person re-identification. In this paper, for the first time a comprehensive literature review is provided to summarize the developments in DG for computer vision over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other research fields like domain adaptation and transfer learning. Second, we conduct a thorough review into existing methods and present a categorization based on their methodologies and motivations. Finally, we conclude this survey with insights and discussions on future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\RX63BBN7\\Zhou et al. - 2021 - Domain Generalization in Vision A Survey.pdf}
}

@inproceedings{zhouLearningGenerateNovel2020a,
  title = {Learning to {{Generate Novel Domains}} for {{Domain Generalization}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {561--578},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58517-4_33},
  abstract = {This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model's ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.},
  isbn = {978-3-030-58517-4},
  langid = {english},
  file = {C\:\\Users\\milad\\Zotero\\storage\\7WRA86ZV\\Zhou et al. - 2020 - Learning to Generate Novel Domains for Domain Gene.pdf}
}

@article{zhouMixStyleNeuralNetworks2021,
  title = {{{MixStyle Neural Networks}} for {{Domain Generalization}} and {{Adaptation}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02053 [cs]},
  eprint = {2107.02053},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional neural networks (CNNs) often have poor generalization performance under domain shift. One way to improve domain generalization is to collect diverse source data from multiple relevant domains so that a CNN model is allowed to learn more domain-invariant, and hence generalizable representations. In this work, we address domain generalization with MixStyle, a plug-and-play, parameter-free module that is simply inserted to shallow CNN layers and requires no modification to training objectives. Specifically, MixStyle probabilistically mixes feature statistics between instances. This idea is inspired by the observation that visual domains can often be characterized by image styles which are in turn encapsulated within instance-level feature statistics in shallow CNN layers. Therefore, inserting MixStyle modules in effect synthesizes novel domains albeit in an implicit way. MixStyle is not only simple and flexible, but also versatile -- it can be used for problems whereby unlabeled images are available, such as semi-supervised domain generalization and unsupervised domain adaptation, with a simple extension to mix feature statistics between labeled and pseudo-labeled instances. We demonstrate through extensive experiments that MixStyle can significantly boost the out-of-distribution generalization performance across a wide range of tasks including object recognition, instance retrieval, and reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\milad\\Zotero\\storage\\8Q3J6PCD\\Zhou et al. - 2021 - MixStyle Neural Networks for Domain Generalization.pdf;C\:\\Users\\milad\\Zotero\\storage\\CTXEZ33Z\\2107.html}
}

@inproceedings{zhuUnpairedImageToImageTranslation2017,
  title = {Unpaired {{Image-To-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  pages = {2223--2232},
  file = {C\:\\Users\\milad\\Zotero\\storage\\SPSI3NIZ\\Zhu et al. - 2017 - Unpaired Image-To-Image Translation Using Cycle-Co.pdf;C\:\\Users\\milad\\Zotero\\storage\\KDCT9ERW\\Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html}
}

@inproceedings{motiian2017unified,
  title={Unified deep supervised domain adaptation and generalization},
  author={Motiian, Saeid and Piccirilli, Marco and Adjeroh, Donald A and Doretto, Gianfranco},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5715--5725},
  year={2017}
}



@inproceedings{li2019episodic,
  title={Episodic training for domain generalization},
  author={Li, Da and Zhang, Jianshu and Yang, Yongxin and Liu, Cong and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1446--1455},
  year={2019}
}


@article{canny1986computational,
  title={A computational approach to edge detection},
  author={Canny, John},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={679--698},
  year={1986},
  publisher={IEEE}
}

@inproceedings{d2018domain,
  title={Domain generalization with domain-specific aggregation modules},
  author={Dâ€™Innocente, Antonio and Caputo, Barbara},
  booktitle={German Conference on Pattern Recognition},
  pages={187--198},
  year={2018},
  organization={Springer}
}

@article{dou2019domain,
  title={Domain generalization via model-agnostic learning of semantic features},
  author={Dou, Qi and Coelho de Castro, Daniel and Kamnitsas, Konstantinos and Glocker, Ben},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{shi2020informative,
  title={Informative dropout for robust representation learning: A shape-bias perspective},
  author={Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
  booktitle={International Conference on Machine Learning},
  pages={8828--8839},
  year={2020},
  organization={PMLR}
}


@inproceedings{seo2020learning,
  title={Learning to optimize domain specific normalization for domain generalization},
  author={Seo, Seonguk and Suh, Yumin and Kim, Dongwan and Kim, Geeho and Han, Jongwoo and Han, Bohyung},
  booktitle={European Conference on Computer Vision},
  pages={68--83},
  year={2020},
  organization={Springer}
}

@inproceedings{huang2020self,
  title={Self-challenging improves cross-domain generalization},
  author={Huang, Zeyi and Wang, Haohan and Xing, Eric P and Huang, Dong},
  booktitle={European Conference on Computer Vision},
  pages={124--140},
  year={2020},
  organization={Springer}
}

@inproceedings{nuriel2021permuted,
  title={Permuted {AdaIN}: reducing the bias towards global statistics in image classification},
  author={Nuriel, Oren and Benaim, Sagie and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9482--9491},
  year={2021}
}

@inproceedings{chu2022dna,
  title={{DNA}: Domain Generalization with Diversified Neural Averaging},
  author={Chu, Xu and Jin, Yujie and Zhu, Wenwu and Wang, Yasha and Wang, Xin and Zhang, Shanghang and Mei, Hong},
  booktitle={International Conference on Machine Learning},
  pages={4010--4034},
  year={2022},
  organization={PMLR}
}


@inproceedings{xu2021fourier,
  title={A {F}ourier-based framework for domain generalization},
  author={Xu, Qinwei and Zhang, Ruipeng and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14383--14392},
  year={2021}
}

@article{yang2021adversarial,
  title={Adversarial teacher-student representation learning for domain generalization},
  author={Yang, Fu-En and Cheng, Yuan-Chia and Shiau, Zu-Yun and Wang, Yu-Chiang Frank},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19448--19460},
  year={2021}
}
@inproceedings{shi2020towards,
  title={Towards universal representation learning for deep face recognition},
  author={Shi, Yichun and Yu, Xiang and Sohn, Kihyuk and Chandraker, Manmohan and Jain, Anil K},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6817--6826},
  year={2020}
}

@inproceedings{shankar2018generalizing,
  title={Generalizing Across Domains via Cross-Gradient Training},
  author={Shankar, Shiv and Piratla, Vihari and Chakrabarti, Soumen and Chaudhuri, Siddhartha and Jyothi, Preethi and Sarawagi, Sunita},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{albuquerque2020improving,
  title={Improving out-of-distribution generalization via multi-task self-supervised pretraining},
  author={Albuquerque, Isabela and Naik, Nikhil and Li, Junnan and Keskar, Nitish and Socher, Richard},
  journal={arXiv preprint arXiv:2003.13525},
  year={2020}
}





















#####



@inproceedings{huang2017arbitrary,
  title={Arbitrary style transfer in real-time with adaptive instance normalization},
  author={Huang, Xun and Belongie, Serge},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1501--1510},
  year={2017}
}

@inproceedings{zhou2020learning,
  title={Learning to generate novel domains for domain generalization},
  author={Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  booktitle={European conference on computer vision},
  pages={561--578},
  year={2020},
  organization={Springer}
}

@inproceedings{zhou2020deep,
  title={Deep domain-adversarial image generation for domain generalisation},
  author={Zhou, Kaiyang and Yang, Yongxin and Hospedales, Timothy and Xiang, Tao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={13025--13032},
  year={2020}
}

@article{zhou2021semi,
  title={Semi-supervised domain generalization with stochastic stylematch},
  author={Zhou, Kaiyang and Loy, Chen Change and Liu, Ziwei},
  journal={arXiv preprint arXiv:2106.00592},
  year={2021}
}

@article{somavarapu2020frustratingly,
  title={Frustratingly simple domain generalization via image stylization},
  author={Somavarapu, Nathan and Ma, Chih-Yao and Kira, Zsolt},
  journal={arXiv preprint arXiv:2006.11207},
  year={2020}
}

@inproceedings{jeon2021feature,
  title={Feature stylization and domain-aware contrastive learning for domain generalization},
  author={Jeon, Seogkyu and Hong, Kibeom and Lee, Pilhyeon and Lee, Jewook and Byun, Hyeran},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={22--31},
  year={2021}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@article{Gulrajani2021InSO,
  title={In Search of Lost Domain Generalization},
  author={Ishaan Gulrajani and David Lopez-Paz},
  journal={ArXiv},
  year={2021},
  volume={abs/2007.01434}
}





@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International Conference on Machine Learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}



@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@inproceedings{lu2020stochastic,
  title={Stochastic classifiers for unsupervised domain adaptation},
  author={Lu, Zhihe and Yang, Yongxin and Zhu, Xiatian and Liu, Cong and Song, Yi-Zhe and Xiang, Tao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9111--9120},
  year={2020}
}

@inproceedings{saito2018maximum,
  title={Maximum classifier discrepancy for unsupervised domain adaptation},
  author={Saito, Kuniaki and Watanabe, Kohei and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3723--3732},
  year={2018}
}


@article{zhou2022domain,
  title={Domain generalization: A survey},
  author={Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@article{wang2022generalizing,
  title={Generalizing to unseen domains: A survey on domain generalization},
  author={Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@inproceedings{hu2020domain,
  title={Domain generalization via multidomain discriminant analysis},
  author={Hu, Shoubo and Zhang, Kun and Chen, Zhitang and Chan, Laiwan},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={292--302},
  year={2020},
  organization={PMLR}
}

@inproceedings{mahajan2021domain,
  title={Domain generalization using causal matching},
  author={Mahajan, Divyat and Tople, Shruti and Sharma, Amit},
  booktitle={International Conference on Machine Learning},
  pages={7313--7324},
  year={2021},
  organization={PMLR}
}

@article{li2020domain,
  title={Domain generalization for medical imaging classification with linear-dependency regularization},
  author={Li, Haoliang and Wang, YuFei and Wan, Renjie and Wang, Shiqi and Li, Tie-Qiang and Kot, Alex},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3118--3129},
  year={2020}
}

@article{balaji2018metareg,
  title={Metareg: Towards domain generalization using meta-regularization},
  author={Balaji, Yogesh and Sankaranarayanan, Swami and Chellappa, Rama},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}




@article{sultana2022self,
  title={Self-Distilled Vision Transformer for Domain Generalization},
  author={Sultana, Maryam and Naseer, Muzammal and Khan, Muhammad Haris and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2207.12392},
  year={2022}
}

@article{choi2022tokenmixup,
  title={TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers},
  author={Choi, Hyeong Kyu and Choi, Joonmyung and Kim, Hyunwoo J},
  journal={arXiv preprint arXiv:2210.07562},
  year={2022}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}





@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}



@inproceedings{wu2018group,
  title={Group normalization},
  author={Wu, Yuxin and He, Kaiming},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}



%%% revise
@article{zhang2023deep,
  title={Deep Representation Learning for Domain Generalization with Information Bottleneck Principle},
  author={Zhang, Jiao and Zhang, Xu-Yao and Wang, Chuang and Liu, Cheng-Lin},
  journal={Pattern Recognition},
  pages={109737},
  year={2023},
  publisher={Elsevier}
}


@article{xu2023fourier,
  title={Fourier-based augmentation with applications to domain generalization},
  author={Xu, Qinwei and Zhang, Ruipeng and Fan, Ziqing and Wang, Yanfeng and Wu, Yi-Yan and Zhang, Ya},
  journal={Pattern Recognition},
  volume={139},
  pages={109474},
  year={2023},
  publisher={Elsevier}
}







