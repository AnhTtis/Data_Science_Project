\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}

\usepackage{amsmath,amssymb}
\usepackage{xpatch} 
\usepackage[utf8]{inputenc} % allow utf-8 
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{booktabs}       
\usepackage{amsfonts}       % blackboard 
\usepackage{nicefrac}       % compact 
\usepackage{microtype}      % 
\usepackage{xcolor}         % colors
\usepackage{adjustbox}
\usepackage{amsmath}
% \usepackage{subcaption}
\usepackage{sidecap}
\usepackage{multirow}
\usepackage{verbatim}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

%%%
\newcommand{\tr}[1]{{#1}^\top}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mypar}[1]{\noindent\textbf{#1}~}

\newcommand{\mr}[1]{#1}

%\newcommand{\ppm}{$\pm \,$}
\newcommand{\ppm}{\,\scriptsize$\pm$}

\newcommand{\XX}{\mathcal{X}}
\newcommand{\YY}{\mathcal{Y}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\AAA}{\mathcal{A}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\loss}{\ell}
\newcommand{\Expect}{\mathbb{E}}


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}


\begin{document}
\title{TFS-ViT: Token-Level Feature Stylization for Domain Generalization Supplementary Material}

\author{\IEEEauthorblockN{\textbf{Mehrdad Noori}, %\IEEEauthorrefmark{1},
\textbf{Milad Cheraghalikhani},
\textbf{Ali Bahri}, 
\textbf{Gustavo A. Vargas Hakim}, \\
\textbf{David Osowiechi}, 
\textbf{Ismail Ben Ayed}, 
\textbf{Christian Desrosiers}} 
\\ \vspace{0.3cm}\IEEEauthorblockA{LIVIA, Ã‰TS Montreal, Quebec, Canada}
\\ \IEEEauthorblockA{International Laboratory on Learning Systems (ILLS)}
}
\markboth{}{}


\maketitle





\section{Result Reproduction}
Our proposed TFS-ViT method is implemented in Python and PyTorch framework, and we use an NVIDIA Tesla V100 GPU for all of our experiments. The original implementation and the instructions for reproducing the results can be found in \href{https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization}{https://github.com/Mehrdad-Noori/TFS-ViT\_Token-level\_Feature\_Stylization}.




\section{Stylization Visualization}
In order to get a better understanding of our proposed token-level feature stylization method, we train a simple ViT-based encoder-decoder network\footnote{We use ViT-base~\cite{dosovitskiy2020image} as encoder, and an architecture similar to~\cite{he2022masked} as our decoder. The "Photo", "Art", "Cartoon" domains of PACS dataset is used to train the model. During the training, no feature stylization is used. We train the network for 800 epochs using the default hyperprameters in~\cite{he2022masked}.} without performing any stylization. When the training is finished, we perform token-level stylization in the encoder using a batch of input images - precisely like what we do in TFS-ViT - and try to reconstruct images to see the effect of stylization at the pixel level. For this section, we conduct two experiments: the effects of single layer selection as well as random selections of multiple layers on the stylization using two values, $\{ 0.5, 1.0\}$, for the parameter $d$.

\textbf{Single Layer} Figure~\ref{fig:per_layer} demonstrates the reconstructed images when token-level stylization is performed on different layers of the Vision Transformer model (encoder). As can be observed from the figure, stylizing tokens at different layers of the model preserves the original structure of the input images. This observation is in contrast with CNNs in which feature-level stylization can only be adopted on the very first convolutional layers where style-related features are extracted~\cite{jeon2021feature, zhou2021domain}. Additionally, from the figure, we observe that choosing a random number of tokens (in this case $d=0.5$) and replacing them with stylized ones leads to creating more diverse samples. 

\textbf{Multiple Random Layers} Figure~\ref{fig:random_layer} illustrates the effect of choosing up to four random layers on the reconstructed images. As can be seen from the figure, stylizing more layers in addition to stylizing random tokens (in this case $d=0.5$) results in even more diverse images compared to single-layer selection. These diverse synthetic features generated from different domains are able to simulate different kinds of domain shifts during training and accordingly force the network to learn domain-invariant features.




\section{Detailed Results}

The detailed breakdown of our method's performance across all domains for VLCS~\cite{fang2013unbiased}, OfficeHome~\cite{venkateswara2017deep}, TerraIncognita~\cite{beery2018recognition}, and DomainNet~\cite{peng2019moment} can be seen in Tables. \ref{tab:ablation_vlcs}, \ref{tab:ablation_oh}, \ref{tab:ablation_terra}, and \ref{tab:ablation_dn}, respectively. 
As can be seen, our method has improves the average accuracy of baseline (ERM-ViT) and SD-ViT, which is the only ViT-based method for Domain generalisation that is currently available for all these four datasets. This is also true for most of the target domains on these datasets, not just on average, which can demonstrate our method's robustness in dealing with different kinds of domain shift.




\section{Additional Attention Map Visualizations}

In Figs. \ref{fig:vis_vlcs}, \ref{fig:vis_oh}, and \ref{fig:vis_terra}, we compared attention maps of CLS Token of our method with baseline for several images from all possible target domains on the VLCS~\cite{fang2013unbiased}, OfficeHome~\cite{venkateswara2017deep}, and TerraIncognita~\cite{beery2018recognition} datasets. In almost all of these scenarios, the cls token of our method mostly uses tokens that represent the foreground object instead of background's token which mostly contains style-related information.But the baseline approach (ERM-ViT) focuses more on the features of the background and less on the features of the object in the foreground.









\bibliographystyle{IEEEtran}
\bibliography{egbib}




\begin{figure*}[!b]
  \centering
   \includegraphics[width=1.0\linewidth]{Figs/sup_per_layer.jpg}
   %\includegraphics[width=0.9\linewidth]{Figs/diagram-crop.pdf}
  \caption{The effect of the token-level stylization on different layers of the Vision Transformer model (encoder). The left and right figures show the results of the reconstructed images when all and half of the tokens are stylized, respectively.}
  \label{fig:per_layer}
\end{figure*}



\begin{figure*}[!b]
  \centering
   \includegraphics[width=1.0\linewidth]{Figs/sup_random_layer.jpg}
   %\includegraphics[width=0.9\linewidth]{Figs/diagram-crop.pdf}
  \caption{The effect of stylizing multiple randomly-selected layers on the reconstructed images. The left and right figures show the results of the reconstructed images when all and half of the tokens are stylized, respectively.}
  \label{fig:random_layer}
\end{figure*}




\begin{table*}[t]
\begin{center}
\caption{ Our proposed method performance on different domains of the VLCS~\cite{fang2013unbiased} dataset. Mean and Standard Deviation are reported across three runs. The best and second best average is in \textbf{bold} and \underline{underlined} fonts, respectively.}
\label{tab:ablation_vlcs}
\adjustbox{max width=\textwidth}{
\begin{tabular}{lccccccc}
\toprule \noalign{\smallskip}
\textbf{Method} & \textbf{Backbone} & \textbf{\#\,of Params} &  \textbf{Caltech101} & \textbf{LableMe} & \textbf{SUN09} & \textbf{VOC2007} & \textbf{Average} \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
% ERM  & ResNet-50 & 23.5M & 98.1\ppm0.4   &       64.1\ppm0.5     &     70.7\ppm0.9       &   74.8\ppm2.4 & 76.9\ppm0.6   \\
% \midrule
ERM-ViT  & DeiT-Small & 22M & 96.7\ppm0.8     &     65.2\ppm1.0      &    73.9\ppm0.3     &     77.4\ppm0.3 & 78.3\ppm0.5  \\
SDViT~\cite{sultana2022self} & DeiT-Small & 22M & 96.8\ppm0.5      &    64.2\ppm0.8   &       76.2\ppm0.4      &    78.5\ppm0.4 & 78.9\ppm0.4     \\
TFS-ViT (ours) & DeiT-Small & 22M &  98.70\ppm0.35   &   \underline{66.57\ppm0.51}     &  76.5\ppm1.38   &  79.00\ppm1.00 & 80.19\ppm0.45     \\
ATFS-ViT (ours) & DeiT-Small & 22M &  \textbf{99.12\ppm0.25}   &  \textbf{66.71\ppm0.77}     &    75.96\ppm0.93	 &  \underline{80.82\ppm0.74} 	 & \underline{80.65\ppm0.36}    \\
\midrule
ERM-ViT  & T2T-ViT-14 &  21.5M & 96.5\ppm0.5 & 64.5\ppm0.1     &     76.4\ppm0.4 &         78.2\ppm1.0 & 78.9\ppm0.3   \\
SDViT\cite{sultana2022self} & T2T-ViT-14 & 21.5M & 96.9\ppm0.4    &  64.0\ppm0.5  &        76.7\ppm1.4   &  80.4\ppm1.3    & 79.5\ppm0.8   \\
TFS-ViT (ours) & T2T-ViT-14 & 21.5M &  98.32\ppm0.29    &    63.86\ppm0.83   & \underline{77.57\ppm0.43}     &  80.37\ppm0.15 &  80.03\ppm0.25  \\
ATFS-ViT (ours) & T2T-ViT-14 & 21.5M &  \underline{98.82\ppm0.13}  &  65.84\ppm1.30 &   \textbf{78.32\ppm0.64}	 &  \textbf{80.92\ppm0.65}  	 &  \textbf{80.98\ppm0.40}   \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}



\begin{table*}[t]
\begin{center}
\caption{Our proposed method performance on different domains of the OfficeHome~\cite{venkateswara2017deep} dataset. Mean and Standard Deviation are reported across three runs. The best and second best average is in \textbf{bold} and \underline{underlined} fonts, respectively.}
\label{tab:ablation_oh}
\adjustbox{max width=\textwidth}{
\begin{tabular}{lccccccc}
\toprule \noalign{\smallskip}
\textbf{Method} & \textbf{Backbone} & \textbf{\#\,of Params} &  \textbf{Art} & \textbf{Clipart} & \textbf{Product} & \textbf{Real World} & \textbf{Average} \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
% ERM  & ResNet-50 & 23.5M & 58.8\ppm1.0  &        51.3\ppm0.4    &      73.7\ppm0.4    &      74.7\ppm0.6 & 64.6\ppm0.2   \\
% \midrule
ERM-ViT  & DeiT-Small & 22M &  67.6\ppm0.3    &57.0\ppm0.6 &     79.4\ppm0.1    & 81.6\ppm0.4 & 71.4\ppm0.1    \\
SDViT~\cite{sultana2022self} & DeiT-Small & 22M & 68.3\ppm0.8   &  56.3\ppm0.2   & 79.5\ppm0.3  &    81.8\ppm0.1   &  71.5\ppm0.2    \\
TFS-ViT (ours) & DeiT-Small & 22M &  68.52\ppm0.22   &   57.73\ppm0.08    &  80.15\ppm0.30   &  81.90\ppm0.37 & 72.08\ppm0.13     \\
ATFS-ViT (ours) & DeiT-Small & 22M &  67.39\ppm0.20   &  56.98\ppm0.31     &   79.40\ppm0.25	 &   81.97\ppm0.45	 & 71.44\ppm0.16    \\
\midrule
ERM-ViT  & T2T-ViT-14 &  21.5M &  70.2\ppm0.5  &        59.0\ppm0.6      &    81.9\ppm0.3   &       \underline{83.6\ppm0.6} &  73.7\ppm0.2  \\
SDViT\cite{sultana2022self} & T2T-ViT-14 & 21.5M &  71.1\ppm0.5      &    59.2\ppm0.3      &    \textbf{82.8\ppm0.4}      &    83.5\ppm0.3      & 74.2\ppm0.3  \\
TFS-ViT (ours) & T2T-ViT-14 & 21.5M &  \underline{71.37\ppm0.60}    &    \underline{60.60\ppm0.53}   & \underline{82.40\ppm0.17}     &  \textbf{83.97\ppm0.18} &   \underline{74.59\ppm0.21}  \\
ATFS-ViT (ours) & T2T-ViT-14 & 21.5M &  \textbf{71.99\ppm0.10}  &  \textbf{60.67\ppm0.10} &   82.35\ppm0.70	 &  \underline{83.59\ppm0.65}  	 &  \textbf{74.65\ppm0.24}   \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}



\begin{table*}[t]
\begin{center}
\caption{ Our proposed method performance on different domains of the TerraIncognita~\cite{beery2018recognition} dataset. Mean and Standard Deviation are reported across three runs. The best and second best average is in \textbf{bold} and \underline{underlined} fonts, respectively.}
\label{tab:ablation_terra}
\adjustbox{max width=\textwidth}{
\begin{tabular}{lccccccc}
\toprule \noalign{\smallskip}
\textbf{Method} & \textbf{Backbone} & \textbf{\#\,of Params} &  \textbf{location\_100} & \textbf{location\_38} & \textbf{location\_43} & \textbf{location\_46} & \textbf{Average} \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
% ERM  & ResNet-50 & 23.5M & 56.3\ppm1.1    &  36.8\ppm4.6   &   52.6\ppm0.4   &   35.2\ppm1.7 & 45.2\ppm1.2  \\
% \midrule
ERM-ViT  & DeiT-Small & 22M & 50.2\ppm1.4 & 30.6\ppm0.9& 53.2\ppm0.2 & 39.6\ppm1.0 &43.4\ppm0.5  \\
SDViT~\cite{sultana2022self} & DeiT-Small & 22M & 55.9\ppm1.7   &       31.7\ppm2.6    &      52.2\ppm0.3  &        37.4\ppm0.6  &   44.3\ppm1.0   \\
TFS-ViT (ours) & DeiT-Small & 22M &  58.70\ppm1.43   &   40.85\ppm1.92    &  53.72\ppm0.47   &  41.14\ppm0.22 & 48.60\ppm0.61     \\
ATFS-ViT (ours) & DeiT-Small & 22M &  57.47\ppm2.61   &  36.45\ppm0.47     &   52.19\ppm0.56 	 &   38.11\ppm0.62 	 & 46.06\ppm0.70    \\
\midrule
ERM-ViT  & T2T-ViT-14 &  21.5M &52.5\ppm1.7      &    43.0\ppm1.3   &       53.7\ppm1.1  &        \underline{43.0\ppm1.6}         &  48.1\ppm0.2    \\
SDViT\cite{sultana2022self} & T2T-ViT-14 & 21.5M &  57.2\ppm2.9       &   \underline{45.4\ppm2.4}     &     \underline{57.7\ppm0.8}  &        41.9\ppm0.4    & 50.6\ppm0.8   \\
TFS-ViT (ours) & T2T-ViT-14 & 21.5M &  \underline{59.62\ppm1.44}    &    \textbf{47.40\ppm1.28}   & \textbf{58.19\ppm0.34}     &  41.84\ppm0.89 &   \textbf{51.76\ppm0.54} \\
ATFS-ViT (ours) & T2T-ViT-14 & 21.5M &  \textbf{60.32\ppm0.51}  &  43.72\ppm1.34 &    56.39\ppm0.23	 &  \textbf{44.38\ppm0.93} 	 &  \underline{51.20\ppm0.43}   \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}



\begin{table*}[t]
\begin{center}
\caption{Our proposed method performance on different domains of the DomainNet~\cite{peng2019moment} dataset. Mean and Standard Deviation are reported across three runs. The best and second best average is in \textbf{bold} and \underline{underlined} fonts, respectively.}
\label{tab:ablation_dn}
\adjustbox{max width=\textwidth}{
\begin{tabular}{lcccccccccc}
\toprule \noalign{\smallskip}
\textbf{Method} & \textbf{Backbone} & \textbf{\#\,of Params} &  \textbf{Clipart} & \textbf{Infograph} & \textbf{Painting} & \textbf{Quickdraw} & \textbf{Real}  & \textbf{Sketch} & \textbf{Average} \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
% ERM  & ResNet-50 & 23.5M & 57.6\ppm0.6   &  18.5\ppm0.3 &  45.9\ppm0.7 & 11.6\ppm0.1 & 59.5\ppm0.3    &      48.6\ppm0.3 & 40.3\ppm0.1  \\
% \midrule
ERM-ViT  & DeiT-Small & 22M &62.9\ppm0.2   &       23.3\ppm0.1    &      53.1\ppm0.2   &       15.7\ppm0.1    &      65.7\ppm0.1    &      52.4\ppm0.2 & 45.5\ppm0.0  \\
SDViT~\cite{sultana2022self} & DeiT-Small & 22M & 63.4\ppm0.1   &       22.9\ppm0.0     &     53.7\ppm0.1    &      15.0\ppm0.4      &    67.4\ppm0.1       &   52.6\ppm0.2  &  45.8\ppm0.0 \\
TFS-ViT (ours) & DeiT-Small & 22M & 64.85\ppm0.14    &       23.51\ppm0.11     &     53.60\ppm0.15    &      16.60\ppm0.00      &    67.61\ppm0.20       &   53.44\ppm0.23  &  46.60\ppm0.06 \\
ATFS-ViT (ours) & DeiT-Small & 22M & 64.36\ppm0.30   &       23.37\ppm0.19     &     53.35\ppm0.21    &       15.80\ppm0.00        &     67.39\ppm0.04       &    52.79\ppm0.16   &  46.18\ppm0.07 \\
\midrule
ERM-ViT  & T2T-ViT-14 &  21.5M & \underline{67.0\ppm0.3}       &   \underline{25.2\ppm0.2}   &       55.3\ppm0.3  &        15.3\ppm0.2   &       \textbf{70.3\ppm0.1}     &     \underline{55.9\ppm0.2}  & 48.1\ppm0.1  \\
SDViT\cite{sultana2022self} & T2T-ViT-14 & 21.5M & \textbf{67.6\ppm0.2} &     25.0\ppm0.2 &    \underline{55.8\ppm0.4}   &       15.2\ppm0.3 &     \underline{70.0\ppm0.1}   &       \textbf{55.9\ppm0.1}  & \underline{48.2\ppm0.2} \\
TFS-ViT (ours) & T2T-ViT-14 & 21.5M & 66.15\ppm0.31 &    \textbf{25.42\ppm0.04} &    \textbf{56.11\ppm0.49}   &       \textbf{17.04\ppm0.21} &     69.54\ppm0.18   &        55.75\ppm0.42  & \textbf{48.34\ppm0.13} \\
ATFS-ViT (ours) & T2T-ViT-14 & 21.5M & 66.31\ppm0.30 &     24.74\ppm0.51 &    55.44\ppm0.61   &       \underline{16.82\ppm0.81} &     69.08\ppm0.40   &       55.27\ppm0.24  & 47.94\ppm0.21 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}





\begin{figure*}[t]
  \centering
   \includegraphics[width=.95\linewidth]{Figs/vlcs_vis.pdf}
   \caption{Comparison of attention maps for the CLS token of the last layer generated by two models, ERM-ViT (baseline) and TFS-ViT (with DeiT-Small backbone), on various domains of the VLCS dataset as the unseen/target domain.}
   \label{fig:vis_vlcs}
\end{figure*}



\begin{figure*}[t]
  \centering
   \includegraphics[width=.95\linewidth]{Figs/oh_vis.pdf}
   \caption{Comparison of attention maps for the CLS token of the last layer generated by two models, ERM-ViT (baseline) and TFS-ViT (with DeiT-Small backbone), on various domains of the OfficeHome dataset as the unseen/target domain.}
   \label{fig:vis_oh}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=.95\linewidth]{Figs/terra_vis.pdf}
   \caption{Comparison of attention maps for the CLS token of the last layer generated by two models, ERM-ViT (baseline) and TFS-ViT (with DeiT-Small backbone), on various domains of the TerraIncognita dataset as the unseen/target domain.}

   \label{fig:vis_terra}
\end{figure*}




\end{document}


