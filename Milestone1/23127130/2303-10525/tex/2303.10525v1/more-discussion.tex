%auto-ignore
\subsection{Contrast between OWL and DRO}
One may contrast this optimistic  re-weighting with adversarial data re-weighting that has been used in the literature on distributionally robust risk minimization \cite{duchi2021learning, duchi2016statistics}. 
\jk{So there is a connection to distributional robustness (namely, that we look for the best interpretation rather than the worst interpretation for the data in an $\varepsilon$-ball in the space of prob measures). Distributional robustness is itself a sub-category of adversarial robustness, where the uncertainty set is taken over the space of probability measures (rather than the data space itself). So we do the exact \textbf{opposite} of adversarial robustness. We look for the most optimistic/charitable interpretation of the data rather than the worst interpretation. The form of robustness that this gives us is quite different from the robustness you get from DRO: DRO makes sense if you learn a model/parameter on some distribution $P$, and then use that model on some other distribution $P'$ which may be slightly different from $P$. We tackle outlier robustness, which means that we want to ignore those parts of $P$ that increase the loss (specifically $-\log p(x|\theta)$) the most. 
%
All that being said, there is a kind of spiritual relationship between OWl and DRO: we treat the data $P$ as already having been polluted (by some adversary), and we expect that any future data will be unpolluted (by this adversary); so we seek to undo the perturbation of the adversary to get us closer to the parametric model that we posed (and which we think would be correct in the absence of said adversarial contamination). DRO is different because rather than asking 'how can we undo the contamination?' it asks 'how can we guard against contamination so that our inferences remain useful in a future where data is contaminated by an adversary?'}