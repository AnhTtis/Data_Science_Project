%auto-ignore

As \Cref{fig:simple-failure} shows, maximum likelihood estimation can be brittle when the data generating distribution $P_0$ is allowed to have a small degree of misspecification with respect to the model family $\{P_\theta\}_{\theta \in \Theta}$.  
%
To assuage the problem of brittleness under misspecification, we propose an Optimistically Weighted Likelihood (OWL) approach that iterates between (1) optimistically re-weighting the observed data points %to match the current model estimate 
and (2) updating the parameter estimate by maximizing a weighted likelihood based on the current data weights. 

%First, in \Cref{sec:okl}, we study this parameter inference methodology at the population level using the optimistic Kullback Leibler (OKL), where we formally allow a small misspecification in $P_0$. Motivated by the population analysis, in \Cref{sec:owl}, we derive the OWL based parameter estimation methodology when only samples $x_1, \ldots, x_n$ from $P_0$ are available.

In \Cref{sec:okl}, we study this parameter inference methodology at the population level where we formally allow $P_0$ to be misspecified. Here we introduce the population level optimistic Kullback Leibler (OKL) function with parameter $\epsilon \in [0,1]$ (\Cref{def:okl}), and show that its minimizer will be a parameter $\theta$ for which $P_\theta$ is $\epsilon$-close to $P_0$ in TV distance. Under suitable conditions, the minimization of the OKL can be performed by iterating the two steps of a) projection of the current model estimate onto the $\epsilon$ TV neighborhood around $P_0$ in a Kullback Leibler sense (information projection), and b) finding the parameter that maximizes a suitably weighted integral of the log likelihood.

Motivated by this population analysis, in \Cref{sec:owl}, we derive the OWL-based parameter estimation methodology when only samples $x_1, \ldots, x_n$ from $P_0$ are available.
%In \Cref{sec:owl}, we derive a corresponding finite sample estimation method called \emph{Optimistically Weighted Likelihood} (OWL) that alternates between the following two steps of (1) finding a parameter that maximizes a weighted likelihood given a weighting for the data points, and (2) updating the weighting based on the current parameter estimate.

\subsection{Population level Optimistic Kullback Leibler minimization}
\label{sec:okl}


\begin{figure}
	\includegraphics{figures/owl.pdf}
	\caption{Population level description of OWL (left) and the OKL function (right). The point $P_\theta$ is labeled using $\theta \in \Theta$. The inference  problem is to find a point in $\Theta_I \subseteq \Theta$ where the model family intersects the  $\epsilon$-neighborhood $\ball_\epsilon$ of the data distribution $P_0$. The set $\Theta_I$ is the set of minimizers of the OKL function $\theta \mapsto I_\epsilon(\theta)$ (right). Starting from initial point $\theta_1 \in \Theta$, the OWL procedure finds a saddle point of the OKL function by iterating $\theta_{t+1} = \argmin_{\theta \in \Theta} \KL(Q^{\theta_t}|P_\theta)$ for $t=1,2, \ldots,$ until convergence, where $Q^\theta = \argmin_{Q \in \ball_\epsilon}  \KL(Q|P_\theta)$ denotes the information projection (\cite{amari2016information}) of the model $P_\theta$ on the total variation (TV) neighborhood $\ball_\epsilon$ of $P_0$. Iterations alternate between I-projection and weighted likelihood estimation steps, illustrated via solid and dashed lines.} 
    %
    %\jk{minor gripe: using red and green in the same plot isn't ideal for color blind readers; you could change red to dark orange.}
 %Note that the  inference target $\theta^*$ is contained in the region $\Theta_I$ where the model intersects the neighborhood $\ball_\epsilon$. The set $\Theta_I \subseteq \Theta$ can alternatively be described as the set of minimizers of the OKL function $\theta \mapsto I_\epsilon(\theta) = \min_{Q \in \ball_\epsilon}\KL(Q|P_\theta)$. We can find  a  saddle point of OKL by repeating the iterations $\theta_{t+1} = \argmin_{\theta \in \Theta} \KL(Q^{\theta_t}|P_\theta)$ for $t=1,2, \ldots,$ until convergence.}
	%
	%\jk{really liking this plot; points for improvement on the Left panel: $P_0$ should have a black x next to it to clarify that it is a point in the ellipsoid. Similarly for $Q^{\theta_1}, Q^{\theta_2}$. I would also leave out $\theta_3$; and I don't understand what the dashed line represents as it's not clear from either plot or description. Do we need it? Also unclear: Why are we stating that $P_0 = (1-\varepsilon)P_{\theta^*} + \varepsilon C$? This made sense in the original plot that I proposed, but this one (which I prefer) doesn't actually need to state this because $C$ isn't assigned any geometric interpretation. Also confusing: we say that the inference target $\theta^*$ 'is contained in $\Theta_I$'. Is this right? I'd argue that we set things up so that any element  $\theta^* \in \Theta_I$ is an (equally good) inference target.}
	\label{fig:okl}
\end{figure}

%We develop methodology for likelihood-based parameter estimation while formally allowing for a small amount of model misspecification, quantified in terms of a suitable metric on the space of
 Let $\cP(\cX)$ denote the space of probability distributions on the data space $\cX$. Our methodology can accommodate misspecification in terms of a variety of probability metrics (e.g.~MMD or Wasserstein) on $\cP(\cX)$, but here we mainly focus on the total variation (TV) distance for concreteness and interpretability. Let $\tv(P,Q) = \sup_{A \subseteq \cX} |P(A)-Q(A)|$ denote the TV metric between two probability distributions $P, Q \in \cP(\cX)$. Given a model family $\{P_\theta\}_{\theta \in \Theta} \subseteq \cP(\cX)$, we assume that the data generating distribution $P_0 \in \cP(\cX)$ for the data population in question satisfies $\tv(P_0, P_{\theta^*}) \leq \epsilon$ for a known value $\epsilon \geq 0$ and some unknown $\theta^* \in \Theta$. In other words, we make the following assumption:

\begin{assume}
	\label{ass:misspecification}
Given $\epsilon \geq 0$ and the true data distribution $P_0$, the set of parameters
$\Theta_I = \{ \theta \in \Theta | \tv(P_0, P_\theta) \leq \epsilon\}$ is non-empty.
\end{assume}

%When $\epsilon = 0$, this simply states that $P_0 = P_{\theta^*}$ (almost surely) for some $\theta^* \in \Theta$.
 \Cref{ass:misspecification} encompasses
Huber's $\epsilon$-contamination model~\citep{huber1964robust} since the condition $\tv(P_0,P_{\theta^*}) \leq \epsilon$ follows whenever $P_0 = (1-\epsilon) P_{\theta^*} + \epsilon C$, for an arbitrary contaminating distribution $C \in \cP(\cX)$. However, \Cref{ass:misspecification} is strictly more general, since $\tv(P_0, P_{\theta^*}) \leq \epsilon$ does not imply that $P_0$ is an $\epsilon$-contamination of $P_{\theta^*}$. 
%
Indeed, \Cref{lem:contamination-condition} in the appendix shows that for fixed $P_0$, $P_{\theta^*}$, and $\epsilon$, $P_0 = (1-\epsilon) P_{\theta^*} + \epsilon C$  for  some contaminating distribution $C$ if and only if the Radon Nikodym derivative $\frac{dP_{\theta^*}}{dP_0}$ exists and is bounded from above by $\frac{1}{1-\epsilon}$, $P_0$ almost surely.
%
%\jk{I'm wondering if I'm missing something here---does this work for mixed distributions? Say I'm given a part-continuous, part-discrete measure. I.e., $P_0$ is so that $P_0 = \varepsilon D + (1-\varepsilon)G$ for $D$ a discrete measure (say on $\mathbb{N}$) and $G$ a measure (say on $\mathbb{R}$) with a density with respect to the Lebesgue measure. Then I can pick $P_{\theta^*} = G$, but---if I remember correctly---the Radon-Nikodym derivative between $P_0$ and $G$ then doesn't exist. (After a quick search, a generalised version exists: \url{https://math.stackexchange.com/questions/3694054/radon-nikodym-derivative-of-a-mixed-distribution}; but I don't think we use this one.) I don't think this is a problem for the result in spirit; but we may have to add in a condition that prevents $C$ from being discrete? The current statement of the Lemma in the appendix doesn't do this I think.}

In general, under \Cref{ass:misspecification}, it may only be possible to identify the set $\Theta_I$, rather than any particular $\theta^*$. 
%
Although such indeterminacy may be inherent, it is practically insignificant whenever $\epsilon$ is sufficiently small so that the distinction between two elements from $\Theta_I$ is practically irrelevant~\citep{huber1964robust}. In line with this insight, the goal throughout the rest of the paper will be to identify \emph{some} parameter in $\Theta_I$.

At the population level, usual maximum likelihood parameter estimation amounts to minimizing the Kullback Leibler (KL) function $\theta \mapsto \KL(P_0|P_{\theta})$ on the parameter space $\Theta$. Even under small amounts of misspecification, KL minimizers are very brittle.
%
The origin of this phenomenon is that any minimizers of the KL function must place sufficient probability mass wherever $P_0$ does, including on outliers. 
%
In contrast, TV distance is far less sensitive to the geometry of misspecifaction. 
%
Hence one may minimize $\theta \mapsto \tv(P_0, P_\theta)$ as a robust alternative, particularly under  \Cref{ass:misspecification}. However, direct minimization of TV distance over the parameter space $\Theta$ is difficult to implement in practice due to the lack of suitable optimization primitives (e.g. maximum likelihood estimators) and the  non-convex and non-smooth nature of the optimization problem \citep[see e.g.][]{yatracos1985rates}. 


%Instead, we use TV to robustify the KL divergence. 
An approach minimizing the  KL divergence with its second argument constrained within an $\epsilon$-neighborhood under the L\'evy-Prokhorov  metric---as opposed to the TV distance---was proposed in \cite{yang2018robust} to provide Neyman-Pearson optimal tests for a robust version of the universal hypothesis testing problem for univariate distributions. 
%
Our motivation is estimation and inference rather than testing.
Indeed, asymptotic analysis of the coarsened likelihood \citep{miller2018robust} using Sanov's theorem (see \Cref{sec:coarsened-inference}) gives rise to a similar KL objective constrained by an $\varepsilon$-neighbourhood. 
%
%optimizing the KL term
%and one of our motivations for optimizing the KL term in its first argument arises from asymptotic analysis of the coarsened likelihood \cite{miller2018robust} using Sanov's theorem (see \Cref{sec:coarsened-inference}). 
The resulting function, which we term the Optimistic Kullback Leibler (OKL), is defined as follows.

\begin{definition}(Optimistic Kullback Leibler) Given $P_0$ and $\epsilon > 0$, the OKL function $I_\epsilon: \Theta \to [0,\infty]$  is defined as:
\begin{equation}
	\label{eq:okl}
    I_\epsilon(\theta) = \inf_{Q \in \ball_\epsilon(P_0)} \KL(Q|P_\theta),
\end{equation}
where $\ball_\epsilon(P_0) = \{Q \in \cP(\cX) | \tv(P_0,Q) \leq \epsilon\}$ is the TV ball of radius $\epsilon$ around $P_0$.
%
If $I_\epsilon(\theta) < \infty$, the underlying optimization over $\ball_\epsilon(P_0)$ has a unique minimizer $Q^\theta$ called the I-projection \citep{csiszar1975divergence}.
%\jk{We seem to be using $Q^{\theta}$ as the minimiser of the above objective consistently  throughout the rest of this section---I suggest we define it formally here together with the OKL?}
\label{def:okl}
\end{definition}


%\new{Next, we discuss the interpretation of the OKL function from the perspective of information geometry \cite{amari2016information}.} Since the $\ball_\epsilon(P_0)$ is a convex set that is closed with respect to the TV distance~\cite{csiszar1975divergence}, whenever $I_\epsilon(\theta) < \infty$, there is a unique $Q^\theta \in \ball_\epsilon(P_0)$, known as the information- or I-projection of $P_\theta$, such that $I_\epsilon(\theta) = \KL(Q^\theta|P_\theta)$. Thus $I_\epsilon(\theta)$ is the KL divergence between $P_\theta$ and its I-projection $Q^\theta$ onto the set $\ball_\epsilon(P_0)$.
%A geometric property for the I-projection similar to the Pythagoras theorem can also established under suitable conditions \cite{csiszar1975divergence}. 

The function $\theta \mapsto I_\epsilon(\theta)$ measures the fit of a model $P_\theta$ to the data $P_0$  allowing for a degree $\varepsilon$ of  data re-interpretation in TV distance before assessing model fit. 
%
Our terminology \emph{Optimistic Kullback Leibler} emphasizes that $I_\epsilon(\theta)$ is the KL divergence between the most optimistic re-interpretation $Q^\theta$ of the data  
within the data neighborhood $\ball_\epsilon(P_0)$ and the model distribution $P_\theta$. Here we use the term optimistic re-interpretation in the sense that, if $\theta$ is our current parameter  estimate, our methodology calculates the KL divergence optimistically, by supposing that the true data are generated from the  model-friendly distribution $Q^\theta$ rather than $P_0$.
%
Here,  $\epsilon \geq 0$ regulates the permitted degree of re-interpreting the data by controlling the neighborhood size. 

The OKL function enables us to perform robust parameter inference by finding a parameter from the set $\Theta_I$: Since $I_\epsilon(\theta) = \KL(Q^\theta|P_\theta)$, under \Cref{ass:misspecification}, the minimum OKL value of zero will be attained exactly on $\Theta_I$ (since $Q^\theta = P_\theta$ if and only if $\theta \in \Theta_I$). 
%
This implies that finding a minimizer of $\theta \mapsto I_\epsilon(\theta)$ amounts to finding a robust parameter estimate. 
%
However, the OKL may be non-convex, %even when the model takes the simple form of an exponential family, 
so that calculating the global minimizer of OKL may not be straightforward. 
%
Fortunately, the OKL lends itself to a feasible alternating optimization scheme that will reach a saddle point under suitable conditions.

Global minimization of the OKL function is equivalent to the joint global minimization 
%
%\jk{It is not obvious to me why that is the case: joint optimisation is generally not the same as sequential optimisation. It would be good to explain why this equivalence holds/point to the appendix if it's formally established.}
%
of the function $F: \Theta \times \ball_\epsilon(P_0) \to [0,\infty]$ given by $F(\theta, Q) = \KL(Q|P_\theta)$, since $I_\epsilon(\theta) = \inf_{Q \in \ball_\epsilon(P_0)} F(\theta,Q)$. Thus we will use alternating minimization to jointly minimize the function $F$, i.e.~for $t \in \nat$ we perform the $Q$-step: $Q^{\theta_{t}} = \argmin_{Q \in \ball_\epsilon(P_0)} \KL(Q|P_{\theta_t})$ and the $\theta$-step: $\theta_{t+1} = \argmin_{\theta \in \Theta} \KL(Q^{\theta_t}|P_\theta)$.  
%
For simplicity, suppose that the model family $\{P_\theta\}_{\theta \in \Theta}$ and $P_0$ have densities $\{p_\theta\}_{\theta \in \Theta}$ and $p_0$ with respect to a common measure $\lambda$, then the I-projection $Q^{\theta_t}$ will also have a density $q_t$ with respect to $\lambda$, and the iterations will take the following form.
Starting from $\theta_1 \in \Theta$ such that $I_\epsilon(\theta_1)  < \infty$, compute the following steps for $t \in \nat$:
\begin{enumerate}
	\item \underline{Q-step}: Compute the I-projection of $P_{\theta_t}$ on the ball $\ball_\epsilon(P_0)$. This corresponds to solving a convex optimization problem over the space of probability densities $\Den = \{q : \cX \to [0,\infty) \mid \int q(x) d\lambda(x) = 1\}$ with respect to $\lambda$.
	$$
	q_t = \argmin_{\substack{q \in \Den \\ \frac{1}{2} \int |q-p_0| d\lambda  \leq \epsilon}} \int q(x) \log \frac{q(x)}{p_{\theta_t}(x)} d\lambda(x).
	$$ 

	\item \underline{$\theta$-step}: Maximize the average log-likelihood $\theta \mapsto \int q_t(x)  \log p_\theta(x)  d\lambda(x)$. Note that $\KL(Q^{\theta_t}|P_\theta) = \int q_t(x) \log q_t(x)  d\lambda(x) - \int q_t(x) \log p_{\theta}(x) d\lambda(x)$ with the convention that $0 \log 0 = 0$. Hence the optimization step $\theta_{t+1} = \argmin_{\theta \in \Theta} \KL(Q^{\theta_t}|P_\theta)$ can be re-written as:
	$$
	\theta_{t+1} = \argmax_{\theta \in \Theta} \int q_t(x) \log p_\theta(x) d\lambda(x).
	$$
\end{enumerate}

The above iterations provide a scheme to minimize the OKL function in which the $Q$-step can  approximately be performed using tools from convex optimization, while the $\theta$-step can be approximated by maximization of a suitably weighted log-likelihood, which can be computed for many standard models. 
%We expand on this further as we explain how to perform this minimization in the presence of finite samples.  
Our resulting population level methodology is illustrated in \Cref{fig:okl}. 

Assuming that there is always a unique minimizer in the $\theta$-step, it is straightforward to show that the objective value $F(\theta_t, Q^{\theta_t})$ is a  strictly decreasing function of $t$ as long as $\theta_t \neq \theta_{t+1}$. Thus if $\{\theta_t\}_{t \in \nat}$ lie in a compact set and suitable continuity assumptions hold, any limit point $\tilde{\theta}$ of the sequence $\{\theta_t\}_{t \in \nat}$ will satisfy the saddle point condition $\tilde{\theta} = \argmin_{\theta \in \Theta} \KL(Q^{\tilde{\theta}}|P_\theta)$. This saddle point condition is satisfied by all the parameters in the identifiable set $\Theta_I$.


We remark that one can use optimization of the OKL function $I_\epsilon(\theta)$ as a subroutine to minimize the function $\theta \mapsto \tv(P_0, P_\theta)$. Namely, we can perform binary search over $\epsilon \in [0,1]$, increasing $\epsilon$ whenever we have $I_\epsilon(\theta) > 0$ and decreasing $\epsilon$ whenever we have $I_\epsilon(\theta) = 0$. Used this way, OKL optimization can be seen as a computationally-palatable approach to minimizing the TV distance over a model class. 
%In general, since \Cref{ass:misspecification} only guarantees us the ability to find some element of $\Theta_I$, it may not be worth the extra computational effort to run binary search or the statistical noise induced by using samples to estimate the KL terms on which we make the binary search decisions. 

%
% additional advantage to related to the c-likelihood..

\subsection{Optimistically Weighted Likelihood (OWL) estimation}
\label{sec:owl}

We now extend the population level methodology from \Cref{sec:okl} to handle the practical case when  samples $x_1, \ldots, x_n \iid P_0$ are available, and provide a computable approximation for the $Q$-step and $\theta$-step from \Cref{sec:okl}. Namely, the $Q$-step (now called $w$-step) will be approximated by a suitable convex optimization problem over weights that lie within the intersection of an  $n$-dimensional probability simplex and the $\ell_1$ ball of radius $2\epsilon$ around the vector with uniform weights; these optimal weights can be interpreted as an optimistic re-weighting of the original data points $x_{1}, \ldots, x_{n}$ to match the current model estimate. Further, the $\theta$-step will then be approximated by maximizing a weighted likelihood with the weights found in the  previous step. Since this methodology involves the repeated steps of parameter estimation using a 
 weighted likelihood (i.e.~the $\theta$-step) and re-estimating the weights on the data points to optimistically match the estimated model (i.e.~the $w$-step), we call this the Optimistically Weighted Likelihood (OWL) method.

%
\subsubsection{Approximating OKL by a finite dimensional optimization problem}

We derive the OWL methodology by approximating the OKL function $I_\epsilon(\theta)$ in terms of a finite dimensional optimization problem defined in terms of observed data $x_1, \ldots, x_n \iid P_0$. Henceforth, let us assume that the model family $\{P_\theta\}_{\theta \in \Theta}$ and measure $P_0$ have densities $\{p_\theta\}_{\theta \in \Theta}$ and $p_0$ with respect to a common measure $\lambda$. We will focus on two cases of interest: when $\cX$ is a finite space and $\lambda$ is the counting measure, and when $\cX=\R^d$ and $\lambda$ is the Lebesgue measure.

When $\cX$ is finite, we look to solve the optimization problem in \cref{eq:okl} over data re-weighting $Q = \sum_{i=1}^n w_i \delta_{x_i}$ as the weight vector $w=(w_1, \ldots, w_n)$ varies over the \ndsimplex\ $\Delta_n$ and satisfies the TV constraint $\frac{1}{2}\|w-o\|_1 \leq \epsilon$ where $o=(1/n, \ldots, 1/n) \in \Delta_n$ is the vector with uniform weights. Formally, our finite space OKL approximation is given by
%

\begin{equation}
	\label{eq:finiteokle}
	\finitehatI(\theta) = \inf_{\substack{w \in {\Delta}_n \\ \frac{1}{2} \|w-o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{nw_i \pfinite(x_i)}{p_\theta(x_i)},
\end{equation}
% For our finite-dimensional approximation, we would like to solve the optimization problem in \cref{eq:okl} over data re-weightings $Q = \sum_{i=1}^n w_i \delta_{x_i}$  as the weight vector $w=(w_1, \ldots, w_n)$ varies over  $\hat{\Delta}_n$, the subset of vectors $w \in \Delta_n$ of the $n$-dimensional probability simplex $\Delta_n$ that satisfies $w_i = w_j$ whenever $x_i=x_j$, and satisfies the total-variation constraint $\frac{1}{2}\|w-e\|_1 \leq \epsilon$ where $e=(1/n, \ldots, 1/n) \in \Delta_n$ is the vector with uniform weights. Such an approximation is effective when $\cX$ is a finite space provides the OKL approximation: 
where $\pfinite(y) = \frac{|\{i \in [n] | x_i = y\}|}{n}$ is the histogram estimator for the data generating distribution $p_0$ when $\cX$ is a finite space. An application of the log sum inequality~\cite[Theorem~2.7.1]{cover2006elements} shows that the weights that solve \cref{eq:finiteokle} have the appealing and natural property that $w_i = w_j$ whenever $x_i=x_j$.
Moreover, when the support of $p_0$ contains the support of $p_\theta$, $\finitehatI(\theta)$ converges to $I_\epsilon(\theta)$ at rate $n^{-1/2}$, as demonstrated by the following result.
\begin{theorem}
\label{thm:finite-okl-convergence-tv}
Suppose that $I_{\epsilon_0}(\theta) < \infty$ for some $\epsilon_0 > 0$ and pick $\delta > 0$ and $\epsilon > \epsilon_0$. If $\supp(p_\theta) \subseteq \supp(p_0)$ and $x_1,\ldots,x_n \iid p_0$, then with probability at least $1-\delta$, 
\[ |I_\epsilon(\theta) - \finitehatI(\theta)| \leq  O\left(\frac{ |\supp(p_0)|}{\epsilon - \epsilon_0} \sqrt{\frac{1}{n} \log \frac{|\supp(p_0)|}{\delta}} \right), \]
where $\supp(p) = \{ x \in \Xcal : p(x) > 0 \}$.
\end{theorem}
See \Cref{sec:okl-estimation-finite} for the proof and a more general theorem statement with explicit constants.

When $\cX=\R^d$, the above approximation strategy needs to be modified, since $\KL(Q|P_\theta)$ is unbounded whenever $P_\theta$ is supported on all of $\cX$ and $Q = \sum_{i=1}^n w_i \delta_{x_i}$ is a discrete distribution. 
%
In this case, \cref{eq:okl} should be formulated in terms of measures $Q \in \cP(\cX)$ that have density $q$ with respect to $\lambda$. Namely, we start with the formulation: 
\begin{equation}
		\label{eq:okl-density}
	I_\epsilon(\theta) = \inf_{\substack{q \in \Den\\ \frac{1}{2}\int |q-p_0| d\lambda \leq \epsilon}} \int q(x) \log \frac{q(x)}{p_\theta(x)} d\lambda(x)
\end{equation}
where $\Den = \{q : \cX \to [0,\infty) \mid \int q(x) d\lambda(x) = 1\}$ is the space of probability 
densities with respect to $\lambda$. Next, using a suitable probability kernel $\K: \cX \times \cX \to [0,\infty)$, we restrict the domain of the optimization problem in \cref{eq:okl-density} to the finite dimensional subspace of densities $\{q_v | v \in \Delta_n\}$, where $q_{v}(\cdot) \doteq \sum_{i=1}^n v_i \K(x_i, \cdot)$
denotes the probability density indexed by weight vector $v \in \Delta_n$. As an example, we may use the Gaussian kernel $\K(x,y) =\frac{1}{(2\pi \sigma^2)^{d/2}} \exp(-\frac{\|x-y\|^2}{2h^2})$ for a bandwidth parameter $h > 0$.
%\jk{Are we addressing how this is tuned/chosen in our experiments somewhere? If so, we should point to the relevant section in the main paper/appendix here.}

For a probability kernel $\K$, using the finite dimensional approximation $\{ q_v : v \in  \Delta_n\}$ for the space of densities, and a suitable Monte Carlo approximation to the integral objective in \cref{eq:okl-density}, we obtain the approximation:
\begin{equation}
	\label{eq:okle}
	\hatI(\theta) = \inf_{\substack{w \in \hat{\Delta}_n \\ \frac{1}{2}\|w-o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{n w_i \hat{p}(x_i)}{p_\theta(x_i)}
\end{equation}
where $\hat{p}$ is a suitable density estimator for $p_0$ based on $x_1,\ldots, x_n$, $A$ is an $n \times n$ matrix with entries $A_{ij} = \frac{\K(x_i,x_j)}{n \hat{p}(x_i)}$, and $\hat{\Delta}_n = A \Delta_n$ is the image of the \ndsimplex\ under linear operator $A$. We will typically take $\hat{p}(\cdot) = \frac{1}{n} \sum_{j=1}^n \K(\cdot, x_j)$ to be the kernel-density estimate based on the same kernel $\K$, in which case $A$ is the stochastic matrix obtained by normalizing the rows of the kernel matrix $K = (\K(x_i,x_j))_{i,j \in [n]}$ to sum to one. 

The continuous space approximation in \cref{eq:okle} yields the finite space approximation in  \cref{eq:finiteokle} as a special case when $\K(x, y) = \I{x = y}$ is taken to be the indicator kernel. The weights vectors in $\hat{\Delta}_n$ are always non-negative and approximately sum to one for large values of $n$, since $\sum_{i=1}^n A_{ij} = \frac{1}{n}\sum_{i=1}^n \frac{\kappa(x_i, x_j)}{\hat{p}(x_i)} \approx \int \frac{\kappa(x, x_j)}{\hat{p}(x)} p_0(x) dx \approx 1$.
	The derivation of \cref{eq:okle} along with  large sample consistency can be found in \Cref{sec:okl-estimation-cont}, but we briefly describe the main result here.
	
	Proving the convergence of $\hatI(\theta)$ is technically challenging, as it requires proving uniform-convergence of the objective in \cref{eq:okle} when the weights are allowed to vary over the entire range $\hat{\Delta}_n$. % and the estimator $\hat{p}$ may take values that are unbounded or arbitrarily close to zero.% 
	To get around this, we restrict the optimization domain to $\hat{\Delta}_n^\beta = A \Delta_n^\beta$ where $\Delta_n^\beta = \{ (v_1, \ldots, v_n) \in \Delta_n \, : \,  v_i \in [\frac{\beta}{n}, \frac{1}{n\beta}] \}$ % and (ii) we replace $\hat{p}$ with the truncated estimator $\hat{p}_\rho(x) = \min \{ \max\{ \hat{p}(x), \rho \}, 1/\rho \}$ 
    %
    for a suitably small constant $\beta$. Thus, the estimator that we theoretically study is given by
\begin{equation}
	\label{eq:okle-theoretical}
	\hat{I}_{\epsilon,\beta}(\theta) = \inf_{\substack{w \in \hat{\Delta}_n^\beta \\ \frac{1}{2}\|w-o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{n w_i \hat{p}(x_i)}{p_\theta(x_i)}.
\end{equation}
Given this change, we can show the following result.
\begin{theorem}
\label{thm:continuous-okle-convergence}
Suppose $\supp(p_0) = \supp(p_\theta) = \Xcal$ is a compact subset of $\R^d$ and there exists a constant $\gamma > 0$ such that $p_0(x), p_\theta(x) \in [\gamma, 1/\gamma]$ for all $x \in \Xcal$. Suppose that we use the probability kernel $\kappa(x,y) = \frac{1}{h^d} \phi(\|x - y \|/h)$ having bandwidth $h > 0$, with $\kappa$ positive semi-definite and $\phi$ bounded above by a constant and having exponentially-decaying tails. Assume that $p_0$ and $\log p_\theta$ are $\alpha$-H\"{o}lder smooth over $\Xcal$, and suppose that we use the clipped density estimator $\hat{p}(x) = \min(\max(\frac{1}{n}\sum_{i=1}^n \kappa(x_i, x), \gamma), 1/\gamma)$. Then for any constant $0 < \beta \leq \gamma^2/4$
\[ 
\left|\hat{I}_{\epsilon, \beta}(\theta) - I_\epsilon(\theta) \right| \leq \tilde{O}\left(n^{-1/2} h^{-d} + h^{\alpha/2} + \psi(\sqrt{h}) \right) ,  \]
with probability at least $1 - 1/n$, where $\psi(r) = \frac{\lambda (\Xcal \setminus \Xcal_{-r})}{\lambda(\Xcal)}$, $\lambda(\cdot)$ is the $d$-dimensional Lebesgue measure, $\Xcal_{-r} = \{ x \in \Xcal : B(x, r) \subseteq \cX \}$, and $\tilde{O}(\cdot)$ hides constants and logarithmic factors.
\end{theorem}
Observe that $\psi(r)$ measures the fraction of the volume of $\Xcal$ that is contained in the envelope of width $r$ closest to the boundary. For well-behaved sets, we expect $\psi(r)$ to decrease to 0 as $r \rightarrow 0$. For example, if $\Xcal$ is a $d$-dimensional ball of radius $r_0$, then $\psi(r) = 1 - (1 - \frac{r}{r_0})^d$.

We prove our theory with the truncated estimator $\hat{I}_{\epsilon, \beta}(\theta)$ instead of $\hat{I}_{\epsilon}(\theta)$ for techincal reasons. By a suitable version of the sandwiching lemma (\Cref{sec:sandwiching}), under the assumptions of \Cref{thm:continuous-okle-convergence}, we conjecture that the optimal weights in   \eqref{eq:okle} lie  in the set $\hat{\Delta}^\beta_n$ for a small enough constant $\beta > 0$, in which case we will have $\hat{I}_{\epsilon, \beta}(\theta) = \hat{I}_{\epsilon}(\theta)$.
%\jk{If I understand correctly, we introduce this truncated estimator only because it allows us to prove this convergence result: In the actual methodology, we use the approximation $\hat{I}_{\varepsilon}(\theta)$ which relies on the original estimator (rather than $\hat{I}_{\varepsilon, \beta, \rho}(\theta)$, which relies on the truncated version)---at least our pseudo code in 'Algorithm 1' says so. Currently, it's not clear from the paper why we choose the non-truncated version in practice. If there is a good reason, we should emphasise this, given that me made an effort to arrive at a theoretical guarantee through the truncated version. If we do it because the non-truncated performs better in practice and doesn't need as many hyperparameters, we should say so. Basically, at the moment this theoretical result feels a little disconnected from the actual methodology. (This is compounded by the fact that we don't even really use the kernelised density estimator [which we explain in the next section], so that there are currently two degrees of distance between the theory and the methodology. An easy solution: if we are going to approximate the kernelised version anyways, we can just change Algorithm 1/the presentation of our methodology to say 'our methodology relies on the truncated estimator, but in practice we often use a naive non-kernelised versioon because it works better'. This cuts out one layer of disconnect.)}
 
\subsubsection{OWL Methodology}

With a computable approximation $\hatI$ (or $\finitehatI$) to the OKL function in hand, we follow the alternating minimization strategy described in \Cref{sec:okl} to minimize the function $\theta \mapsto \hatI(\theta)$. In more detail, we replace the  density $q_t$ (or more precisely the relative density $w = q_t/p_0$) in the $Q$-step with the weight vector $w_t=(w_{t,1}, \ldots, w_{t,n}) \in \Rnn^n$ that minimizes \cref{eq:okle} for $\hatI(\theta_t)$. We rename this the $w$-step to emphasize the new setup. Next, the $\theta$-step corresponds to minimizing the function $\theta \mapsto \sum_{i=1}^n w_{t,i} \log \frac{n w_{t,i} \hat{p}(x_i)}{p_\theta(x_i)}$, which is equivalent to maximizing the weighted likelihood $\theta \mapsto \sum_{i=1}^n w_{t,i} \log p_\theta(x_i)$. 

The resulting procedure is summarized in \Cref{alg:owl}.  In \Cref{sec:comp}, we expand on the computational details for the $\theta$-step and $w$-steps, but note for now that the $w$-step involves solving a convex optimization problem for which standard tools are available, while the $\theta$-step corresponds to maximizing a weighted likelihood, which can be performed for many models through simple modifications of procedures for the corresponding maximum likelihood estimation.

Finally, it is straightforward to see that the iterates $\theta_t$ of \Cref{alg:owl} must decrease the objective function $\theta \mapsto \hatI(\theta)$, as we have
\begin{align*}
    \hatI(\theta_{t+1}) 
    &= \sum_{i=1}^n w_{t+1,i} \log \frac{n w_{t+1,i} \hat{p}(x_i)}{ p_{\theta_{t+1}}({x}_i)} 
	\leq \sum_{i=1}^n w_{t,i} \log \frac{n w_{t,i} \hat{p}(x_i)}{ p_{\theta_{t+1}}({x}_i)} \\
	&
	\leq \sum_{i=1}^n w_{t,i} \log \frac{n w_{t,i} \hat{p}(x_i)}{ p_{\theta_{t}}({x}_i)} = \hatI(\theta_t).
\end{align*}

Since \cref{eq:okle} is a convex optimization problem with a strictly convex objective, the first inequality is strict unless $w_{t+1} = w_{t}$. Hence the objective $\hat{I}_\epsilon$  must decrease strictly at each step.% until convergence $(\theta_t, w_t) \to (\theta_*, w_*)$ to a limit point is achieved. %Any such limit point will satisfy the fixed point condition $\hatI(\theta_*) = \sum_{i=1}^n w_{*,i} \log \frac{n w_{*,i} \hat{p}(x_i)}{p_{\theta_*}(x_i)}$.

\begin{algorithm}
	\caption{OWL Methodology}
	\label{alg:owl}
	\begin{algorithmic}
		\STATE \textbf{Input:} Model $\{p_\theta\}_{\theta \in \Theta}$, coarsening parameter $\epsilon \geq 0$, probability kernel $\K$, initial point $\theta_1 \in \Theta$, and iteration limit $T$.
		\FOR{$t=1,\ldots,T$}
		\STATE \underline{$w$-step:} Find $w_t=(w_{t,1}, \ldots, w_{t,n}) \in \Rnn^n$ that minimizes \cref{eq:okle} for  $\theta=\theta_t$. 
		\STATE \underline{$\theta$-step:} Find $\theta_{t+1}$ that maximizes the weighted likelihood $\theta \mapsto \sum_{i=1}^n w_{t,i} \log p_\theta(x_i)$.
		\ENDFOR
		\STATE \textbf{Output:} The robust parameter estimate $\theta_T$ and the data weights $w_T$.
	\end{algorithmic}
\end{algorithm}
%

In practice, when the data lie in a  continuous space, we often avoid using the kernel-based estimator \cref{eq:okle} to determine the weights in the $w$-step of \Cref{alg:owl} because it greatly slows down the computation (see \Cref{sec:evaluating}), and the resulting weights are sensitive to the choice of kernel $\K$. Instead, setting $\kappa(x, y)=\I{x=y}$, we perform the $w$-step by  solving the \emph{unkernelized optimization} problem:
$$
\min_{\substack{w \in \Delta_n \\ \frac{1}{2}\|w-o\|_1 \leq \epsilon }} \Big\{- \sum_{i=1}^n w_i \log p_\theta(x_i)  + \sum_{i=1}^n w_i \log w_i\Big\},
$$
obtained from \cref{eq:okle} when all the data points $x_1, \ldots, x_n$ are distinct. 
We demonstrate in \Cref{sec:kern-vs-unkern-simulations} that the unkernelized version of the OWL procedure has equally good performance compared to the kernelized version with a suitably tuned bandwidth. A potential explanation for this is that the primary role of the $w$-step is to down-weight outliers under the model density $p_\theta$, which is controlled by the first term in the optimization objective above; in contrast, the second term in the optimization objective controls the regularity of the non-outlying weights, and plays a secondary role in the $w$-step. 

\subsubsection{Setting the corruption fraction $\epsilon$}
\label{sec:tune-epsilon}

So far we have assumed that the parameter $\epsilon \in (0,1)$, which can be interpreted as the fraction of corrupted samples in the population distribution, is fixed at a known value that satisfies \Cref{ass:misspecification}. Now let us see how the population level analysis (Section \ref{sec:okl}) can inform our choice of $\epsilon$.  \Cref{ass:misspecification} is satisfied as long as $\epsilon \geq \varepsilon_0$, where
$$
\varepsilon_0 = \min_{\theta \in \Theta} \tv(P_0, P_\theta) = \left\{\epsilon \in [0,1] : \min_{\theta \in \Theta} I_\epsilon(\theta) = 0 \right\}.
$$

Hence, in principle, we could set $\epsilon = \varepsilon_0$ to use OWL to perform minimum-TV estimation \citep{yatracos1985rates}, which has the following advantages: (1) while directly minimizing TV distance is computationally intractable, the OWL methodology decomposes this problem into alternating convex optimization and weighted MLE steps, both of which are standard problems that often tend to be well-behaved, and (2) the OWL methodology provides us with weight vectors that can indicate outlying observations and relates minimum TV-estimation to  likelihood based inference. 

In order to choose $\epsilon \approx \varepsilon_0$ in practice, we define the function $\hat{g}(\epsilon) = \hatI(\hat{\theta}_\epsilon)$, where $\hat{\theta}_\epsilon$ is the parameter estimate computed by the OWL procedure for a given $\epsilon$. At the population level, the corresponding function $g(\epsilon) = \min_{\theta \in \Theta} \okl$ is monotonically decreasing in $\epsilon$ until $\epsilon = \varepsilon_0$, at which point it remains at 0. This introduces a kink, or elbow, at $\epsilon_0$ that we hope to identify in the sample estimate $\hat{g}$. Thus, our $\epsilon$-search procedure is to compute $\hat{g}$ over a fixed grid of $\epsilon$-values, smooth the resulting grid, and then select amongst the points of largest curvature (computed numerically), where the curvature of a twice-differentiable function $f$ at a point $x$ is given by $f''(x)/(1 + f'(x)^2)^{1.5}$~\citep{satopaa2011finding}. Despite the various approximations involved, our simulation results (\Cref{sec:simul}) show that the OWL procedure with such a tuned value of $\epsilon$ provides almost identical performance when compared with the OWL procedure with the true value of $\epsilon$.

%Unlike the population level function $g(\epsilon) = \min_{\theta \in \Theta} \okl$, the estimated function $\epsilon \mapsto \hat{g}(\epsilon)$ is not guaranteed to be non-negative and monotonically decreasing in $\epsilon$. Instead we search for $\epsilon$-values from a grid to find points where the curvature of the function $\hat{h}(\epsilon) = \inf_{r \in [0, \epsilon]} \hat{g}(r)$ is maximized \cite{satopaa2011finding}. Despite the various approximations involved, our simulation results (\Cref{sec:simul}) show that the OWL procedure with such a tuned value of $\epsilon$ provides almost identical performance when compared with the OWL procedure with the true value of $\epsilon$.

\subsection{OWL extension to non-identically-distributed data}
\label{sec:niid}

While the population level analysis and theoretical results for the OKL estimator were derived under the assumption that data are generated i.i.d.~from a distribution $P_0$, the OWL procedure can be adapted to robustify likelihood based inference in the setting where the data are conditionally independent, but not necessarily identically distributed.

Suppose data $z_1,\ldots, z_n \in \cZ$ are conditionally independent, with the likelihood having the product form $p_\theta(z_{1:n}) = \prod_{i=1}^n p_{\theta,i}(z_i)$, for known functions $\{p_{\theta,i}\}_{i=1}^n$. For example, if $z_i = (y_i, x_i) \in \R \times \cX$ for $i=1,\ldots,n$, this includes the case of regression models $\{q_\theta(y|x)\}_{\theta \in \Theta}$ under the setup $p_{\theta, i}(z_i) = q_\theta(y_i|x_i)$. Another example of this setup includes mixture models if we expand the parameter space to also include cluster assignments (see \Cref{sec:mixture-models}).

To robustify inference based on the product likelihood $p_\theta(z_{1:n}) = \prod_{i=1}^n p_{\theta, i}(z_i)$, we can replace the $w$- and $\theta$- steps in \Cref{alg:owl} by analogous steps in the product likelihood case. In particular, the modified $w$-step is given by
$$
w_t = \argmin_{\substack{w \in \Delta_n \\ \frac{1}{2}\|w-o\|_1 \leq \epsilon }} \left\{- \sum_{i=1}^n w_i \log p_{\theta_t, i}(x_i)  + \sum_{i=1}^n w_i \log w_i\right\}
$$
and the modified $\theta$-step is given by
$$
\theta_{t+1}= \argmin_{\theta \in \Theta} \sum_{i=1}^n w_{t,i} \log p_{\theta,i}(x_i).
$$
Despite our lack of theory in the non-identically-distributed case, we continue to see good empirical performance of the OWL estimator in this setup when evaluated on synthetic data (see \Cref{sec:simul}).
 