%auto-ignore

In this section, we expand on the computational details of the OWL methodology. Specifically, in \Cref{sec:evaluating} we discuss our approach to solving the constrained convex optimization problems in \cref{eq:finiteokle,eq:okle}, and in \Cref{sec:weighted-likelihood} we discuss the details of optimizing a weighted likelihood for exponential families and mixture models.



\subsection{Computing the \texorpdfstring{$w$}{w}-step (I-projection)}
\label{sec:evaluating}

\subsubsection{Unkernelized I-projection}
Suppose that $\Xcal$ is finite. %and let $x_1, \ldots, x_n$ denote an i.i.d sample from $P_0$. 
Then the finite approximation to the OKL of \cref{eq:finiteokle} is given by the solution to the convex optimization problem
%
% \jk{
% The below is double-counting: we sum over all individual data point $x_i$, and if they occured multiple times, we attach a weight of size $n_i$ to them. Unless I'm missing something here, the issue is that this problem could be formulated by summing over all elements of $\Xcal$ (in which case we should re-weight by the number of observations), or it could be formulated by summing over the data points. At the moment, we are somehow mixing these approaches and writing something that's inaccurate.
% %
% Note also: If we do go with summing over $\Xcal$, then the constraint $1/2\|w-e\|_1\leq \varepsilon$ doesn't make sense anymore either (because the space of $w$ then shifts from $\Delta_n$ to $\Delta_k$ for $k = |\Xcal|$). These issues carry over to the presentation below; i.e. $w \in \mathbb{R}$ vs $w \in \mathbb{R}^k$ etc.
% }
% \chris{Interestingly, it turns out that this is the correct form. The proof is in \Cref{lem:alternate-finite-okl-form}.}
%
\[ \min_{\substack{w \in {\Delta}_n \\ \frac{1}{2} \|w-o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{w_i n_i}{p_\theta(x_i)}, \]
where $n_i = |\{j : x_j = x_i \}|$ is the number of times that $x_i$ occurs in our sample.

This is a convex optimization problem for which there are a number of candidate solutions. Our approach is based on (consensus) Alternating Direction Method of Multipliers (ADMM)~\citep{boyd2011distributed, parikh2014proximal}. To frame the OKL optimization problem in the language of ADMM, we rewrite it as
%
\begin{equation}
\label{eqn:admm-form}
\min_{w \in \R^n} \overbrace{\sum_{i=1}^n w_i \log \frac{w_i n_i}{ p_\theta({x}_i)}}^{f_1(w)} + \overbrace{\ci{ w \in \Delta_n}}^{f_2(w)} + \overbrace{\ci{\frac{1}{2} \|w-o\|_1 \leq \epsilon }}^{f_3(w)},
\end{equation}
where $\ci{C}$ is 0 whenever the condition $C$ is true and $\infty$ otherwise. Written in this form, ADMM boils down to implementing the individual \emph{proximal operators} for the $f_i$'s, where the proximal operator of a closed proper convex function $f: \R^n \rightarrow \R$ and parameter $\lambda > 0$ is defined as the function
\[ \prox_{\lambda f}(x) = \argmin_{z \in \R^n} \left\{ f(z) + \frac{1}{2\lambda} \| z - x \|_2^2\right\}. \]
In our setting, the proximal operator for the KL term $f_1$ can be computed element-wise using the Lambert-W function~\citep{barratt2021optimal}, or in log-scale via the Wright omega function, for which fast algorithms exist~\citep{lawrence2012algorithm}, leading to $O(n)$ computational complexity. The proximal operator for $f_2$ corresponds to projection onto the $n$-dimensional scaled probability simplex, which can be computed in $O(n)$ time~\citep{condat2016fast}. The proximal operator for $f_3$ corresponds to projection onto an $\ell_1$-ball, and in fact it can be reduced to projection onto the simplex~\citep{condat2016fast}. 
%The proximal operator for $f_3$ is also a projection, but it depends on the specific distance. For MMD distances, it is a projection onto a fixed ellipsoid, which can be found by solving for the root of a univariate function~\citep{kiseliov1994algorithms}. For $\ell_1$ distance, it is projection onto an $\ell_1$-ball, which can be reduced to projection onto the simplex~\citep{condat2016fast}.

To simplify our discussion on the implementation of ADMM, we rewrite \cref{eqn:admm-form} for an arbitrary number of convex functions $f_1, \ldots, f_k: \R^n \rightarrow \R$ as
%\jk{I think this is confusing and can be improved: in \cref{eqn:admm-form}, the subscripts indicate functions all of which are applied to the vector $w$. Here, we are now using the exact same subscript notation to denote functions that are applied to individual components $w_i$ of the vector $w$. It's also not clear what $k$ is.} 
\[ \min_{\substack{w^{(1)}, \ldots, w^{(k)}, z \in \R^n \\ w^{(i)} = z \text{ for all }i}} \sum_{i=1}^k f_i\left(w^{(i)} \right) . \]
For penalty parameters $\lambda_1, \ldots, \lambda_k > 0$, the augmented Langrangian associated with this problem is given by
\[ L\left(w^{(1:k)}, y^{(1:k)}, z \right) = \sum_{i=1}^k f_i\left(w^{(i)}\right) + \langle y^{(i)} , w^{(i)} - z \rangle + \frac{1}{2\lambda_i} \| w^{(i)} - z \|^2, \]
where $w^{(1)}, \ldots, w^{(k)} \in \R^n$ are the primal variables, $y^{(1)}, \ldots, y^{(k)} \in \R^n$ are the dual variables, and $z \in \R^n$ is the consensus variable.
Then the consensus ADMM algorithm is derived by iteratively optimizing the augmented Langrangian coordinate-wise. That is, starting from some initialization $w^{(i,0)}, y^{(i,0)}, z^{(0)} \in \R^n$, we perform the following updates
\begin{align*}
w^{(i,t+1)} &= \argmin_{w^{(i)} \in \R^n} L\left(w^{(-i,t+1)}, w^{(i)}, y^{(1:k,t+1)}, z^{(t)} \right) = \prox_{\lambda_i f_i}\left(z^{(t)}  + \lambda_i y^{(i,t)} \right)\\
z^{(t+1)} &= \argmin_{z  \in \R^n} L\left(w^{(1:k,t+1)}, y^{(1:k,t+1)}, z \right) = \frac{1}{\sum_{i=1}^k 1/\lambda_i}  \sum_{i=1}^k \left( \frac{1}{\lambda_i}w^{(i,t+1)} - y^{(i,t)} \right) \\
y^{(i,t+1)} &= y^{(i,t)} + \frac{1}{\lambda_i} (z^{(t+1)} - w^{(i,t+1)}).
\end{align*}

\Cref{alg:consensus-admm} displays the full consensus ADMM algorithm. In our implementation, we use the self-adaptive rule suggested by \cite{he2000alternating} to independently update each of the penalty parameters.

\begin{algorithm}[t]
\caption{Consensus ADMM to compute the I-projection}
\label{alg:consensus-admm}
\begin{algorithmic}
\STATE \textbf{Input:} Proximal operators $\prox_{\lambda f_i}$ for functions in \cref{eqn:admm-form}, dimension $n$, number of iterations $T$, and proximal penalty parameters $\lambda_i > 0$.
\STATE Initialize $w_i^0, y_i^0, z^0 = \vec{0} \in \R^n$ for $i=1, 2, 3$.
\FOR{$t=0,\ldots,T-1$}
	\STATE Set $w^{(i,t+1)} = \prox_{\lambda_i f_i}( z^{(t)} + \lambda_i y^{(i,t)})$ for $i=1, 2, 3$.
	\STATE Set $z^{(t+1)} = \frac{1}{\sum_{i=1}^k 1/\lambda_i}  \sum_{i=1}^k \left( w^{(i,t+1)}/\lambda_i - y^{(i,t)} \right)$.
	\STATE Set $y^{(i,t+1)} = y^{(i,t)} + (z^{(t+1)} - w^{(i,t+1)})/\lambda_i$ for $i=1, 2, 3$.
	\STATE \emph{Optionally, adjust $\lambda_i$ for $i=1, 2, 3$.}
\ENDFOR
\STATE \textbf{Output:} $w_1^T$. 
\end{algorithmic}
\end{algorithm}

\subsubsection{Kernelized I-projection}

In the Euclidean case with $\Xcal = \R^d$, beyond the sample $x_{1}, \ldots, x_n$ and parameter $\theta \in \Theta$, we additionally have a kernel matrix $K \in \R^{n \times n}$ with induced row sums $s_i = \sum_j K_{ij}$ and row-normalized matrix $A_{ij} = K_{ij}/s_i$. The optimization problem of \cref{eq:okle} translates to
\begin{equation}
\label{eqn:general-admm-form}
\min_{v \in \R^n} \sum_{i=1}^n (Av)_i \log \frac{(Av)_i s_i}{ p_\theta({x}_i)} + \ci{v \in \Delta^n } + \ci{  \frac{1}{2}\| Av-o\|_1 \leq \epsilon}.
\end{equation}
Although all the terms in \cref{eqn:general-admm-form} remain convex in $v$, the proximal operators for the KL objective and the $\ell_1$ constraint are no longer available in closed form. To circumvent this issue, we can rewrite problems of this form as
\[ \min_{\substack{w^{(1)}, \ldots, w^{(k)}, z \in \R^n \\ w^{(i)} = M_i z \text{ for all }i}} \sum_{i=1}^k f_i\left(w^{(i)} \right), \]
% \[ \min_{\substack{w_1, \ldots, w_k, z \in \R^n \\ w_i = A_i z \text{ for all }i}} \sum_{i=1}^k f_i\left(w_i \right), \]
where each $M_i \in \R^{n \times n}$. The augmented Langrangian in this case becomes
%\[ L\left(w_{1:k}, y_{1:k}, z \right) = \sum_{i=1}^k f_i(w_i) + \langle y_i , w_i - M_i z \rangle + \frac{1}{2\lambda_i} \| w_i - $M_i z \|^2,\]
\[ L\left(w^{(1:k)}, y^{(1:k)}, z \right) = \sum_{i=1}^k f_i\left(w^{(i)}\right) + \langle y^{(i)} , w^{(i)} - M_i z \rangle + \frac{1}{2\lambda_i} \| w^{(i)} - M_i z \|^2, \]
which leads to the ADMM updates
%
% \begin{align*}
% w_i^{t+1} & = \prox_{\lambda_i f_i}\left(A_i z^t  + \lambda_i y_i^t \right)\\
% z^{t+1} & =  \left( \sum_{i=1}^k A_i^T A_i/\lambda_i \right)^{-1} \sum_{i=1}^k  A_i^T \left( \frac{w_i^{t+1}}{\lambda_i} - y_i^{t} \right) \\
% y^{t+1}_i &= y^{t}_i + \frac{1}{\lambda_i} (A_i z^{t+1} - w_i^{t+1}).
% \end{align*}
\begin{align*}
w^{(i,t+1)} &= \prox_{\lambda_i f_i}\left(M_i z^{(t)}  + \lambda_i y^{(i,t)} \right)\\
z^{(t+1)} &= \left( \sum_{i=1}^k M_i^T M_i/\lambda_i \right)^{-1}  \sum_{i=1}^k M_i^T \left( w^{(i,t+1)}/\lambda_i - y^{(i,t)} \right) \\
y^{(i,t+1)} &= y^{(i,t)} + \frac{1}{\lambda_i} ( M_i z^{(t+1)} - w^{(i,t+1)}).
\end{align*}
In our setting, each $M_i$ is either equal to $A$ or to the identity matrix $I$. Thus, the matrix inverse in the update of $z$ can be computed efficiently using a single singular value decomposition of $A$, even if the penalty parameters change between iterations. To see this, suppose that $A = U \diag(\sigma_1, \ldots, \sigma_n) V^T$ is the SVD of $A$ and let $J = \{i : M_i = A \}$. Then the $z$ update can be written as
\[ z^{(t+1)} =  V \diag\left( (\beta_{J^c} + \sigma_1^2 \beta_{J})^{-1}, \ldots, (\beta_{J^c} + \sigma_n^2 \beta_{J})^{-1} \right) V^T \sum_{i=1}^k  M_i^T \left( w^{(i,t+1)}/\lambda_i - y^{(i,t)} \right), \]
where $\beta_{J} = \sum_{i \in J} \lambda_i^{-1}$ and  $\beta_{J^c} = \sum_{i \not \in J} \lambda_i^{-1}$.

\subsubsection{Computational complexity and generalization to other distances}


As mentioned above, each of the three proximal operators in \cref{eqn:admm-form} and \cref{eqn:general-admm-form} can be implemented in $O(n)$ time. For the non-kernelized version of the ADMM algorithm in \Cref{alg:consensus-admm}, the remaining linear algebraic steps can also be implemented in $O(n)$ time, bringing the total computational complexity of the procedure to $O(Tn)$, where $T$ is the number of ADMM steps taken. For the kernelized version of the ADMM algorithm, we require an SVD computation as a preprocessing step, which takes $O(n^3)$ time, and each step additionally involves matrix-vector products, taking time $O(n^2)$. Thus, the total time complexity of the kernelized ADMM procedure amounts to $O(n^3 + Tn^2)$, which is polynomial in $n$ but not feasible for large values of $n$.
%\jk{I think what would really help at the end of this section is a short paragraph on the computational complexity of this procedure overall (per iteration). The way it reads right now, it looks like a very complicated algorithm, but from what I remember it was quite fast and scaled 'okay' (i.e. at least polynomially) in $n$}

We remark here that from an optimization perspective, for both \cref{eqn:admm-form,eqn:general-admm-form}, the $\ell_1$-distance is not special. We may substitute in any distance that is convex in its arguments and for which there are computationally efficient methods for projection onto the corresponding balls with a specified center and radius. This includes $\ell_2$-distance, whose projection can be computed by a simple shift and rescaling, and maximum mean discrepancy \citep{gretton2006kernel}
%, \jk{the first time the name 'mmd' appears in the literature is Arthur's 2006 paper 'A Kernel Method for the Two-Sample-Problem', \url{https://papers.nips.cc/paper/2006/hash/e9fb2eda3d9c55a0d89c98d6c54b5b3e-Abstract.html}.} 
whose projection operator is onto an ellipsoid and can be reduced to a one-dimensional root finding problem~\citep{kiseliov1994algorithms}.

\subsection{Computing the \texorpdfstring{$\theta$}{theta}-step: maximizing weighted likelihoods}
\label{sec:weighted-likelihood}

We now shift focus to the second step in the OWL procedure, maximizing a weighted likelihood:
\[  \max_{\theta \in \Theta} \sum_{i=1}^n w_i \log p_\theta({x}_i).  \]
We will expand on this problem for two settings: exponential families and mixture models.

\subsubsection{Weighted maximum likelihood for exponential families}

Consider the setting where $p_\theta$ is an exponential family and $\Theta \subset \R^d$ is the natural parameter space so that
\[ p_\theta(x) = h(x) \exp \left( \theta^T T(x) - A(\theta) \right), \]
where $T: \Xcal \rightarrow \R^d$ is a sufficient statistic, $h$ is a base measure, and $A:\theta \rightarrow \R$ is the log-normalizing factor. Then for $w \in \Delta_n$, the weighted maximization step solves %is the solution to the problem
\begin{align}
\label{eqn:weighted-ml-expfam}
\max_{\theta \in \Theta} \sum_{i=1}^n w_i \log p_\theta({x}_i) = \min_{\theta \in \Theta} \left\{ A(\theta)  - \theta^T \sum_{i=1}^n w_i T(x_i) \right\}.
\end{align}
This solution satisfies the gradient condition $\nabla A(\theta) = \sum_{i=1}^n w_i T(x_i).$


For exponential families, whenever  $\cov(T(X))$ is positive definite for $X \sim p_\theta$, the function $\nabla A(\theta)$ is invertible. Thus, this step can be solved quickly whenever we can compute the inverse of $\nabla A(\cdot)$. When $\cov(T(X))$ is not strictly positive definite, or more generally when the inverse of $\nabla A(\cdot)$ is not available in closed form, the objective in \cref{eqn:weighted-ml-expfam} is still convex in $\theta$ and can be solved using tools from convex optimization.

\subsubsection{Weighted maximum likelihood for mixture models}
\label{sec:mixture-models}

In the mixture model setting, the parameters $\theta$ encode mixing weights $\pi \in \Delta_K$ and component parameters $\phi_1, \ldots, \phi_K \in \Phi$ such that $p_{\phi}$ denotes a probability density over $\Xcal$. Then the likelihood under $\theta$ can be written as
\[ p_\theta(x_{1:n}) =  \prod_{i=1}^n \left( \sum_{k=1}^k \pi_k p_{\phi_k}(x_i) \right). \]
To compute the maximum likelihood estimate, it is standard to introduce latent categorical random variables $z_i \stackrel{iid}{\sim} \pi$, $z_i \in \{1,\ldots,K\}, i=1,\ldots,n,$ and rewrite the likelihood as
\begin{align*}
p_\theta(x_{1:n}) =  \E_{z \sim \pi} \left[ \prod_{i=1}^n p_{\phi_{z_i}}(x_i) \right].
\end{align*}
The above can then be maximized using the EM algorithm~\citep{dempster1977maximum}. Unfortunately, the introduction of weights into the likelihood no longer allows for an easy decomposition via these latent variables. 

However, consider the likelihood with respect to the augmented latent variables $\tilde{\theta} = (\theta, z_{1:n}) \in \tilde{\Theta}$:
\[ p_{\tilde{\theta}}(x_{1:n}) = \prod_{i=1}^n p_{\phi_{z_i}}(x_i) . \]
Then, following the setup from \Cref{sec:niid}, the weighted log-likelihood can be written as
\[ \sum_{i=1}^n w_i \log p_{\phi_{z_i}}(x_i) = \sum_{k=1}^K \sum_{i : z_i = k} w_i \log p_{\phi_{k}}(x_i).  \]
Thus, to maximize the above for fixed latent variables $z_1, \ldots, z_n$, we can maximize the weighted log-likelihood of the individual component parameters over its assigned (weighted) data. For many component distributions (e.g., Gaussians, Poissons), this can be computed in closed-form. On the other hand, for fixed component variables $\phi_1,\ldots, \phi_k$, we can maximize this weighted log-likelihood over the $z_1, \ldots, z_n$ by assigning each data point to the component that maximizes its individual likelihood. This suggests the following scheme, reminiscent of the `hard EM' algorithm~\citep{samdani2012unified}:
\begin{align*}
\phi_k^{t+1} &= \argmax_{\phi \in \Phi} \sum_{i: z^t_i = k} w_i \log p_{\phi}(x_i) \text{ for each } k=1,\ldots,K \\
z_i^{t+1} &= \argmax_{k \in \{1, \ldots, K \}} \log p_{\phi_k}(x_i) \text{ for each } i=1,\ldots,n .
\end{align*}
In general, optimizing over the augmented parameter space $\tilde{\Theta}$ implicitly assumes a generative model different from the one assumed by optimizing over $\Theta$. In particular, when one optimizes over $\Theta$, the assumed generative model is that the data come from some distribution $P_0$ that is $\epsilon$-close to $P_{\theta}$ for some $\theta \in \Theta$. The implied generative model when optimizing over $\tilde{\Theta}$ is that the data come from some mixture distribution $\sum_k \pi_k p_k$ such that each component $p_k$ is $\epsilon$-close to some $\phi_k \in \Phi$. For most settings, the first generative model is a strict generalization of the second. However, in the $\epsilon$-contamination model, these are equivalent, as implied by the following result.
\begin{proposition}
\label{prop:eps-contam-mixture-model}
Let $P_0, P_1, \ldots, P_K$ denote probability measures, $\pi \in \Delta_K$, and $\epsilon > 0$. Then the following are equivalent.
\begin{itemize}
	\item There exists a probability measure $Q$ such that $P_0 = (1- \epsilon) \sum_{k=1}^K \pi_k P_k + \epsilon Q$.
	\item There exist probability measures $Q_1, \ldots, Q_k$ such that $P_0 = \sum_{k=1}^K \pi_k ( (1- \epsilon) P_k + \epsilon Q_k )$.
\end{itemize}
\end{proposition}
The proof of \Cref{prop:eps-contam-mixture-model} follows from simple algebraic substitution.



% \subsection{Other inference approaches}

% \begin{enumerate}
%     \item MCMC using the coarsened likelihood
%     \item Connection to profile likelihood \cite{murphy2000profile} and profiles sampler \cite{lee2005profile}
% \end{enumerate}
