%auto-ignore

\subsection{Consistency of the C-MLE} 


\jk{Q: Can we prove that our weights go to $1/n$ if there is \textbf{no} misspecification? (So that all the normal regularity properties of MLE follow for the case without misspec?) Intuitively, this should be possible if $n$ becomes sufficiently large so that $KL(P_n\|P_{\theta_0}) \to 0$? I.e., we could fix $\theta =\theta_0$ and then hopefully show that $w_i^{(n)} = \frac{1}{n}$ become (a.s.) $\varepsilon_n$-minimisers for some sequence $\varepsilon_n\to 0$?}
\md{If $\epsilon_n \to 0$ sufficiently fast, then OWL Likelihood is proportional to the usual likelihood (Lemma \ref{lem:usual-likelihood}). So the next question would be how to tune the parameter $\epsilon_n$.}

\subsection{Model selection criterion}
\md{Can we obtain a version of BIC with the coarsened likelihood at least for the exponential family? Can we use this to select $\epsilon$?} 
\jk{Intuitively, I doubt that we can select $\epsilon$ this way: the larger you choose $\epsilon$, the larger the space around the empirical measure you are allowed to search over for a higher likelihood value. I.e., the larger $\epsilon$, the larger your optimistic likelihood. Therefore, larger eps means better 'evidence' and therefore this should always prefer more optimistic likelihoods. (E.g., extreme case: consider  a Bernoulli model. Then for $\epsilon$ sufficiently large, you can always shift all of the observations into one of the two outcome classes and take $p=1$ to get an estimate on the boundary and with maximum likelihood value.) But maybe we could do model selection for other aspects based on something like this.} \md{I was thinking of using the smallest epsilon such that the model evildence is ``close'' to one. That is, for the correct value of $\epsilon$, we know that $I_\epsilon(\theta_I)=0$. We would like to choose the smallest epsilon such that $\hat{I}_\epsilon(\hat{\theta}_\epsilon) \approx 0$ where $\hat{\theta}_\epsilon$ is the CMLE at value $\epsilon$. I think  Chris is already minimizing $\hat{I}_\epsilon(\hat{\theta})$ to tune free parameters like the kernel-bandwidth.}
%




\begin{enumerate}
    \item  $L_\epsilon(\theta|x_{1:n}) = \exp(-n \hat{I}_\epsilon(\theta))$
    \item  $\hat{I}_\epsilon(\theta) = \inf_{q \in B_\epsilon(\hat{p}_0)} \KL(q|p_\theta)$
\end{enumerate}

\begin{lemma}(Crude Laplace Approximation)
Let $\hat{\theta} = \argmin_{\theta} \hat{I}_\epsilon(\theta)$. We see:
\begin{equation}
\label{eq:lap-approx}
0 \geq n\hat{I}_\epsilon(\hat{\theta}) + \log \int L_\epsilon(\theta|x_{1:n}) \pi(d\theta)  \geq \log \left(\int \exp(-n F_\epsilon(\theta)) \pi(d\theta)\right)
\end{equation}
where the function $F_\epsilon(\theta) \doteq \sup_{q \in  B_\epsilon(\hat{p}_0)} \int q \log \frac{p_{\hat{\theta}}}{p_\theta}$ attains its minimum value of zero $\theta = \hat{\theta}$. 
\end{lemma}
\begin{proof} Follows from the bounds  $0 \leq \hat{I}(\theta) - \hat{I}(\hat{\theta}) \leq F(\theta)$.
\end{proof}

\begin{lemma}
Suppose that $\Theta \subseteq \R^k$ is an open set, $\pi$ has a non-degenerate density on $\Theta$ with respect to the Lebesgue measure. Further suppose that $F_\epsilon$ has a positive definite Hessian $\hat{H}_\epsilon \in \R^{k \times k}$ at $\hat{\theta}$, then by Taylor Approximation
\begin{align*}
\log\left(\int_{\Theta} \exp(-n F_\epsilon(\theta) ) \pi(\theta) d\theta \right) &\approx  \log\left(\int_{\Theta} e^{-\frac{n}{2} \left(\theta - \hat{\theta} \right)^t \hat{H} (\theta - \hat{\theta})} \pi(\theta) d\theta \right) \\
&\approx \log\left(n^{-k/2}\pi(\hat{\theta}) \int_{\R^k} e^{- h^t \hat{H} h} dh \right)\\
&= - \frac{k}{2}\log n + \frac{k}{2}\log \left(2 \pi |\hat{H}|\right) + \log(\pi(\hat{\theta}))
\end{align*}
\end{lemma}

\begin{corollary} If the assumptions of the above two lemmas are satisfied, we have the following bound on the model evidence as $n \to \infty$
$$
-n \hat{I}_\epsilon(\hat{\theta}) - \frac{k}{2} \log n + O(1) 
\leq 
\log \int_{\theta} L_\epsilon(\theta|x_{1:n}) \pi(\theta) d\theta  \leq -n \hat{I}_\epsilon(\hat{\theta})
$$
\end{corollary}

When $p_\theta(x) = \exp(\theta^t T(x) - a(\theta))$ is an exponential family. Then  $F_\epsilon(\theta) = a(\theta) - a(\hat{\theta}) + \sup_{t \in A_\epsilon} \langle\theta_0 - \theta, t\rangle$, where $A_\epsilon = \{\int T(x) dq(x) | q \in B_\epsilon(\hat{p}_0)\} \subseteq \R^k$.
Note that $F$ is a strictly convex function that attains its minimum value of $0$ at $\hat{\theta}$, we could do a second order Taylor expansion of $F$ at $\hat{\theta}$ and follow the above recipe. 
