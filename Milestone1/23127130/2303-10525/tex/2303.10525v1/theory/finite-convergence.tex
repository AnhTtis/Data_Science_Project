%auto-ignore
In this section, we will work with a finite data space but allow for a more general distance function. To this end, let $\D(p, q)$ denote the distance between probability vectors $p, q \in \Delta_\Xcal$. Then the OKL function for a general distance in \cref{eqn:okl-general-distance} translates to this setting as
\begin{equation}
	\label{eq:okle-general-distance-finite}
	 I_\epsilon(\theta) = \inf_{\substack{q \in {\Delta}_{\Xcal} \\ \D(p, p_0) \leq \epsilon}} \KL(q | p_\theta). 
\end{equation}

Given a dataset $x_1, \ldots, x_n \in \Xcal$, the finite approximation to the OKL for general distance $\D$ is given by
\begin{align}
\label{eqn:okl-estimator-general-distance}
\finitehatI(\theta) = \inf_{\substack{q \in {\Delta}_{\Xcal} \\ \supp(q) = \hX_n \\ \D(q, \pfinite) \leq \epsilon}} \sum_{x \in \hX_n} q(x) \log \frac{q(x)}{p_\theta(x)},
\end{align}
where $\hX_n = \{x_1, \ldots, x_n \}$ is the observed support and $\pfinite(y) = \frac{|\{i \in [n] | x_i = y\}|}{n}$. When $\D(p, q) = \frac{1}{2}\|p - q\|_1$ is the total variation distance, the above is equivalent to the form given in \cref{eq:finiteokle} as shown in the following lemma.
\begin{lemma}
\label{lem:alternate-finite-okl-form}
For any $\epsilon > 0$ and $x_1, \ldots, x_n \in \Xcal$,
\begin{align*}
	\inf_{\substack{q \in {\Delta}_{\Xcal} \\ \supp(q) = \hX_n \\ \frac{1}{2}\|q - \pfinite\|_1 \leq \epsilon}} \sum_{x \in \hX_n} q(x) \log \frac{q(x)}{p_\theta(x)}  
	\ = \ 
	\inf_{\substack{w \in {\Delta}_{n} \\ \frac{1}{2}\|w - o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{w_i n \pfinite(x_i)}{p_\theta(x_i)}. 
\end{align*}
where $o=(1/n, \ldots, 1/n) \in \Delta_n$.
\end{lemma}
\begin{proof}
For convenience, let $n_i = n \pfinite(x_i)$ and let $n(x) = n \pfinite(x)$. For any $q \in {\Delta}_{\Xcal}$ satisfying $\supp(q) = \hX_n$, let $w_{q} \in \Delta_n$ satisfy $w_{q,i} = q(x_i)/n_i$. Then we have
\begin{align*}
&\| w_q - o \|_1 = \sum_{i=1}^n \left|\frac{q(x_i)}{n_i} - \frac{1}{n} \right| = \sum_{x \in \hX_n} n(x) \left|\frac{q(x)}{n(x)} - \frac{1}{n} \right| = \| q - \pfinite \|_1 \text{ and } \\
&\sum_{i=1}^n w_{q,i} \log \frac{w_{q,i} n_i}{p_\theta(x_i)} 
= \sum_{i=1}^n \frac{q(x_i)}{n_i} \log \frac{q(x_i)}{p_\theta(x_i)} 
= \sum_{x \in \hX_n} q(x) \log \frac{q(x)}{p_\theta(x)}.
\end{align*}
The above two equalities imply that
\begin{align*}
	\inf_{\substack{q \in {\Delta}_{\Xcal} \\ \supp(q) = \hX_n \\ \frac{1}{2}\|q - \pfinite\|_1 \leq \epsilon}} \sum_{x \in \hX_n} q(x) \log \frac{q(x)}{p_\theta(x)}  
	\ \geq \ 
	\inf_{\substack{w \in {\Delta}_{n} \\ \frac{1}{2}\|w - o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{w_i n \pfinite(x_i)}{p_\theta(x_i)}. 
\end{align*}

Now for any $w \in \Delta_n$, let $q_w \in {\Delta}_{\Xcal}$ satisfy $q_w(x) = \sum_{i: x_i = x} w_i$. Note that we trivially must have $\supp(q_w) = \hX_n$. Moreover, we have
\begin{align*}
&\|q_w - \pfinite \|_1 = \sum_{x \in \hX_n} \left| \left( \sum_{i: x_i = x} w_i \right) - \frac{n(x)}{n} \right| 
\leq \sum_{x \in \hX_n} \sum_{i: x_i = x}  \left| w_i - \frac{1}{n} \right|
= \| w - o \|_1 \text{ and } \\
&\sum_{x \in \hX_n} q_w(x) \log \frac{q_w(x)}{p_\theta(x)}
= \sum_{x \in \hX_n} \left( \sum_{i: x_i = x} w_i \right) \log \frac{ \left( \sum_{i: x_i = x} w_i \right)}{p_\theta(x)} 
\leq  \sum_{i=1}^n w_i \log \frac{w_i n_i}{p_\theta(x_i)},
\end{align*}
where the first inequality is the triangle inequality and the second inequality is the log sum inequality~\cite[Theorem~2.7.1]{cover2006elements}.
Together, the above implies that
\begin{align*}
	\inf_{\substack{q \in {\Delta}_{\Xcal} \\ \supp(q) = \hX_n \\ \frac{1}{2}\|q - \pfinite\|_1 \leq \epsilon}} \sum_{x \in \hX_n} q(x) \log \frac{q(x)}{p_\theta(x)}  
	\ \leq \ 
	\inf_{\substack{w \in {\Delta}_{n} \\ \frac{1}{2}\|w - o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{w_i n \pfinite(x_i)}{p_\theta(x_i)}. \ \ \ \qedhere
\end{align*}
\end{proof}

To prove convergence of the estimator in \cref{eqn:okl-estimator-general-distance}, we will make the following assumptions on the space $\Xcal$ and distance $\D$.
\begin{assume}
\label{assum:finite-continuous-distance}
$\Xcal$ is a finite set, $\D$ is a metric, $\D$ is jointly convex in its arguments, and there exists a constant $C \geq 1$ for which $\D(p, q) \leq C \|p - q\|_1$ for all $p,q \in \Delta_\Xcal$.
%$\frac{1}{C} \|p\|_1 \leq \| p\| \leq C \|p\|_1$.
\end{assume}
Observe that \Cref{assum:finite-continuous-distance} fulfills the conditions of \Cref{lem:okl-continuity}, implying that $I_{\epsilon}(\theta)$ is continuous in $\epsilon$. To get quantitative bounds, we make the following assumption.
\begin{assume}
\label{assum:finite-okl}
There exist constants $V, \epsilon_0 > 0$ such that $I_{\epsilon_0}(\theta) \leq V$.
\end{assume}
Finally, we will also require some conditions on the support of $p_\theta$ and $p_0$.
\begin{assume}
\label{assum:lower-bounded-on-support}
There exists a set $S \subseteq \Xcal$ and constant $\gamma_0 > 0$ such that $\supp(p_\theta) = S \subseteq \supp(p_0)$ and $p_0(x) \geq \gamma_0$ for all $x \in S$.
\end{assume}
Given the above assumptions, we have the following convergence result for $\finitehatI(\theta)$. 

%\jk{Perhaps worth remarking somewhere that the below result implies exponential convergence to zero for $\mathbb{P}( \| \pfinite - p_0 \|_1>\varepsilon)$, for all $\varepsilon>0$? Perhaps also not worth.} 

\begin{theorem}
\label{thm:finite-okl-convergence}
Pick $\epsilon > \epsilon_0$ and let $n \geq \max \left\{ \frac{1}{\gamma_0} \log \frac{2|S|}{\delta}, \frac{1}{2} \left( \frac{C |S|}{\epsilon - \epsilon_0} \right)^2 \log \frac{4|S|}{\delta} \right\}$, and suppose \Cref{assum:finite-continuous-distance,assum:lower-bounded-on-support,assum:finite-okl} hold. If $x_1,\ldots,x_n \iid p_0$, then with probability at least $1-\delta$, 
\[ |I_\epsilon(\theta) - \finitehatI(\theta)| \leq  \frac{CV |S|}{\epsilon - \epsilon_0} \sqrt{\frac{1}{2n} \log \frac{2|S|}{\delta}}. \]
\end{theorem}
\begin{proof}
If $n \geq \frac{1}{\gamma_0} \log \frac{2|S|}{\delta}$, then with probability $1-\delta/2$, we have $S \subseteq \hX_n$. Moreover, an application of Hoeffding's inequality tells us that with probability at least $1-\delta/2$, we have
\[ \| \pfinite - p_0 \|_1 \leq |S| \sqrt{\frac{1}{2n} \log \frac{4|S|}{\delta}}. \]
By a union bound, both of these events occur with probability at least $1-\delta$. Condition on these two events occurring.

Now observe that any $q \in \Delta_{\Xcal}$ that achieves $\KL_{\Xcal}(q | p^\theta) < \infty$ must satisfy $q(x) = 0$ for all $x \not \in S$. Thus, we may rewrite the OKL and our finite estimator as
\begin{align*}
I_\epsilon(\theta) &= \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq S \\ \D(q, p_0) \leq \epsilon}} \KL(q|p_\theta) \\
\finitehatI(\theta) &= \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq \hX_n \cap S \\ \D(q, \pfinite) \leq \epsilon}} \KL(q|p_\theta) = \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq S \\ \D(q, \pfinite) \leq \epsilon}} \KL(q|p_\theta),
\end{align*}
where we have used the fact that we are conditioning on $S \subseteq \hX_n$.

Moreover, by \Cref{assum:finite-continuous-distance} and our bound on $\| \pfinite - p_0 \|_1$, we have:
\[ \D(q, \pfinite) \leq \D(q, p_0) + \D(\pfinite, p_0) \leq \D(q, p_0) + C \|\pfinite - p_0\|_1 \leq \D(q, p_0) + \alpha_n,  \]
where $\alpha_n = C |S| \sqrt{\frac{1}{2n} \log \frac{2|S|}{\delta}}$. Similarly, we also can conclude $\D(q, \pfinite) \geq \D(q, p_0) - \alpha_n.$

Applying \Cref{lem:okl-continuity}, we have
\begin{align*}
\finitehatI(\theta) &= \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq S \\ \D(q, \pfinite) \leq \epsilon}} \KL(q|p_\theta) 
\geq \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq S \\ \D(q, p_0) \leq \epsilon + \alpha}} \KL(q|p_\theta) \\
&= I_{\epsilon + \alpha_n}(\theta) \geq I_{\epsilon}(\theta) - \frac{\alpha_n}{\epsilon - \epsilon_0 + \alpha_n} V.
\end{align*}
On the other hand, if $n \geq \frac{1}{2} \left( \frac{C |S|}{\epsilon - \epsilon_0} \right)^2 \log \frac{4|S|}{\delta}$, then $\alpha_n \leq \epsilon - \epsilon_0$, and we can again apply \Cref{lem:okl-continuity} to see that
\begin{align*}
\finitehatI(\theta) &= \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq S \\ \D(q, \pfinite) \leq \epsilon}} \KL(q|p_\theta) 
\leq \inf_{\substack{q \in \Delta_{\Xcal}\\ \supp(q) \subseteq S \\ \D(q, p_0) \leq \epsilon - \alpha}} \KL(q|p_\theta) \\
&= I_{\epsilon - \alpha_n}(\theta) \leq I_{\epsilon}(\theta) + \frac{\alpha_n}{\epsilon - \epsilon_0} V.
\end{align*}
Rearranging the above inequalities gives us the lemma statement.
\end{proof}


\iffalse
\md{Next lemma needs changing notation to $F(r) = I_r$. Also the condition $\KL(p^0|p^\theta) < \infty$ and Assumption \ref{ass:dist-finite} might be too strong as the above lemmas point out.}
\begin{lemma} Suppose $\D$ satisfies assumption \ref{ass:dist-finite} and $\theta \in \Theta$ is such that $\KL(p^0|p^\theta) < \infty$. Then the function $F: [0,\infty) \to [0,\infty)$ given by $F(r) = -I_r(\theta)$ is continuous. (Note that $F(r) \leq \KL(p^0|p^\theta) < \infty$ for each $r \geq 0$.)
\label{lem:continuity}
\end{lemma}
\begin{proof} 
 Using our continuity assumption on $\D$, let us first show that the optimization problem in $I_r$ attains its minimum value at some $q_r \in A_r \doteq \{ q \in \Delta_{\cX} \mid \D(q, p^0) \leq r \}$. 
Indeed, this follows since $A_r$ is a compact subset and the function $q \mapsto \KL_{\cX}(q|p^\theta)$ is lower semi-continuous.

Let us now show that $\liminf_{h \to 0} F(r_0+h) \geq F(r_0)$ for any $r_0 \in [0,\infty)$ (with the convention that $F(r)=\infty$ when $r < 0$). Indeed, for any sequence $\{h_n\}_{n \in \nat}$ that is converging to zero, the sequence $\{q_{r_0 + h_{h_n}}\}_{n \in \nat} \subseteq \Delta_{\cX}$ is pre-compact. Hence, there is an increasing subsequence $\{n_k\}_{k \in \nat} \subseteq \nat$ and $q_* \in \Delta_n$ such that $\lim_{k \to \infty} q_{r_0 + h_{n_k}} = q_*$. Note then by the continuity of $\D$, that $\D(q_*, p_0) = \lim_{k \to \infty} \D(q_{r_0 + h_{n_k}}, p_0) \leq \limsup_{k \to \infty} r_0 + h_{n_k} = r_0$. Hence, the lower semi-continuity of the $\KL$-divergence shows that $$
\liminf_{k \to \infty} F(r_0 + h_{n_k}) = \liminf_{k \to \infty} \KL_{\cX}(q^{r_0 + h_{n_k}}|p^\theta) \geq \KL_{\cX}(q_*|p^\theta) \geq F(r_0).
$$

Note that $F$ is a non-decreasing function, i.e. $F(r) \leq F(s)$ whenever $s \geq r$. Hence, the result $\liminf_{h \downarrow 0} F(r_0 + h) \geq F(r_0) \geq \limsup_{h \downarrow 0} F(r_0 + h)$ immediately shows the right continuity of $F$, i.e. $F(r_0) = \lim_{h \downarrow 0} F(r_0 + h)$.
%

Now we shall establish the left-continuity of $F$ at some point $r=r_0 > 0$. If $\D(q_{r_0},p^0) < r_0$, then $F(r) = F(r_0)$ for each $r \in [\D(q_{r_0}, p^0), r_0)$ and the left continuity is easily satisfied. Hence, suppose from now on that $\D(q_{r_0},p^0) = r_0$. Next, for any $h \in [0,1]$, denote by $q'_h \doteq (1-h) q_{r_0} + h p^0$ the convex combination between $q_{r_0}$ and $p^0$. By the convexity of KL-divergence
\begin{equation*}
    F(\D(q'_h, p^0)) \leq \KL_{\cX}(q'_h|p^\theta) \leq (1-h)\KL_{\cX}(q_{r_0}|p^\theta) + h \KL_{\cX}(p^0|p^\theta) = (1-h) F(r_0) + h F(0).
\end{equation*} 
 
Take $h \downarrow 0$ to obtain $\limsup_{h \to 0} F(T(h)) \leq F(r_0)$, where $T(h) \doteq \D(\tilde{q}^h, p^0)$. By our assumption, $T: [0,1] \to [0,r_0]$ is a continuous and strictly decreasing function. Hence $T(h)$ is stricly increasing to $T(0)=r_0$  as $h \downarrow 0$. Thus, we have in-fact shown that $\limsup_{h \downarrow 0} F(r_0-h) = F(r_0)$. Finally, monotonicity of $F$ shows $\liminf_{h \downarrow 0} F(r_0 - h) \geq F(r_0)$, and hence we recover the left continuity of $F$.
\end{proof}
\fi
