%auto-ignore

In this section, we show consistency (and convergence rates) for the OKL estimator when $\cX = \R^d$ is the Euclidean space. To avoid technical complications with tail estimation, we will restrict our analysis to the case when both the data and the model family are supported on a compact set $S \subseteq \cX$. The results and notation in this section are  self-contained, and can be read independently of other sections. 

\subsection{Introduction}
Consider the data space $\cX = \R^d$ equipped with the Euclidean norm $\|\cdot\|$ and its Borel sigma algebra $\cB$. %\jk{Given that none of the results in this section work on unbounded spaces, why do we introduce the space as all of $\R^d$ (rather than a bounded subset of it)?}  \md{Change $S$ to $\cX$ and $\cX$ to $\R^d$ throughout.}  
%Let $B(x,r) = \{y \in \R^d | \|x-y\| < r\}$ denote the open ball of radius $r > 0$ around $x \in \R^d$, and for any set $A \subseteq \R^d$, let $A_{-r}=\{y \in A \mid B(y,r) \subseteq A\}$ denote the collection of points in $A$ that lie at a distance of at least $r$ from the boundary. We will use $A^c$ to denote the complement of $A$ in $\R^d$. 
We will let $\Den = \{f: \cX \to [0,\infty) \mid \int f(x) dx = 1, f \text{ is $\cB$-measurable}  \}$ denote the set of densities on $\cX$ with respect to the Lebesgue measure $\lambda$. Given the data density $p_0 \in \Den$, model family $\{p_\theta\}_{\theta \in \Theta} \subseteq \Den$ and coarsening radius $\epsilon > 0$, the central object of interest in this section is the OKL function defined as
\begin{equation}
	\label{eqn:okl}
	I_\epsilon(\theta) = \inf_{\substack{q \in \Den \\
			\tv(q,p_0) \leq \epsilon 
	}} \KL(q|p_\theta),
\end{equation}
where, given two densities $p,q \in \Den$, the total variation distance between them is $\tv(q, p) =\frac{1}{2}\int |q(x)-p(x)|dx$, and the KL-divergence is $\KL(p|q) = \int  p(x) \log \frac{p(x)}{q(x)}  dx$ if the absolute continuity condition $\int p(x) \I{q(x) = 0} dx = 0$ is satisfied, otherwise $\KL(p|q) = \infty$.

Given samples $x_1,\ldots, x_n \in \cX$ drawn i.i.d.~from the distribution with density $p_0$, and a suitable kernel $K_h: \cX \times \cX \to \R$, 
%and a suitably small constant $\gamma \in [0,1]$,
we will approximate $I_\epsilon(\theta)$ with the value of a finite-dimensional optimization problem given by
\begin{equation}
	\label{eqn:okle1}
	\hatI(\theta) = \inf_{\substack{w \in \hat{\Delta}_n \\ \frac{1}{2}\|w-o\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{n w_i \hat{p}(x_i)}{p_\theta(x_i)},
	%\hatI(\theta) = \inf_{\substack{w \in \hat{\Delta}^\gamma_n \\ \frac{1}{2}\|w-e\|_1 \leq \epsilon}} \sum_{i=1}^n w_i \log \frac{n w_i \hat{p}(x_i)}{p_\theta(x_i)},
\end{equation}
where $\hat{p}$ is a suitable density estimator for $p_0$,
%$\Delta_n = \{(v_1,\ldots, v_n)| \sum_{i=1}^n v_i = 1, \gamma \leq nv_i \leq \gamma^{-1} \}$ is subset of
$\Delta_n = \{(v_1,\ldots, v_n)| \sum_{i=1}^n v_i = 1, v_i \geq 0 \}$ is the \ndsimplex , $o=(1/n, \ldots, 1/n) \in \Delta_n$ is the constant probability vector, $A$ is an $n \times n$ matrix with entries $A_{ij} = \frac{K_h(x_i,x_j)}{n \hat{p}(x_i)}$, $\hat{\Delta}_n = A \Delta_n$ is the image of the set $\Delta_n \subseteq \R^n_+$ under the linear operator $A$, and $\|(x_1, \ldots, x_d)\|_1 = \sum_{i=1}^d |x_i|$ is the $\ell_1$ norm of the vector.


\subsubsection{Assumptions and statement of the result}

To describe the formal statement quantifying the approximation between \cref{eqn:okl} and \cref{eqn:okle1}, we will introduce a series of assumptions. Our result will handle the case when densities $p_0$ and $p_\theta$ are smooth densities supported on a bounded set $S \subseteq \cX$ whose boundary has measure zero, i.e.~$\lambda(S \setminus S^\circ) = 0$, where $S^\circ$ denotes the interior of $S$. We fix a value $\theta \in \Theta$ throughout this section.


\begin{assume}
	\label{assump:bounded-support}
	Suppose $S \subseteq \cX$ is a subset of the closed unit ball $\bar{B}(0,R)$ of radius $R$, with finite Lebesgue measure $V_S = \lambda(S) \geq 1$, and has a boundary measure functional $\phi(r) = \frac{\lambda (S \setminus S_{-r})}{\lambda(S)}$ that satisfies $\lim_{r \rightarrow 0} \phi(r) = 0$. 
\end{assume}


Here $S_{-r} = \{x \in S : B(x, r) \subseteq S\}$ denotes the set of points for which the unit ball of radius $B(x,r) = \{y \in \cX | \|x-y\| \leq r\}$ is also contained inside $S$. For example, when $S=B(x_0,r_0)$ is the unit ball of radius $r_0$ centered at the point $x_0$, the boundary measure functional is given by
$$
\phi(r) = 1 - \frac{\lambda(B(x, r_0-r))}{\lambda(B(x, r_0))} = 1 - \left(1 - \frac{r}{r_0} \right)^d
$$
since $S_{-r} = B(x_0, r_0 - r)$. 

Next we have the following smoothness and support condition on densities $p_0, p_\theta \in \Den$. Here the support of a density $p \in \Den$ is defined as $\supp(p) = \{x \in \cX : p(x) > 0\}$.

\begin{assume}
	\label{assump:bounded-densities}
	The supports satisfy $\supp(p_0) = \supp(p_\theta) = S$, and there exists $\gamma \in (0,1]$ such that $p_0(x), p_\theta(x) \in [\gamma, 1/\gamma]$ for all $x \in S$.
\end{assume}

\begin{assume}
	\label{assump:smooth-densities}
	$p_0$ and $\log p_\theta$ are $\alpha$-H\"{o}lder smooth on $S$. More precisely, there are constants $\alpha, C_\alpha > 0$ such that $|p_0(x) - p_0(y)|, |\log p_\theta(x) - \log p_\theta(y)| \leq C_\alpha \|x - y \|^\alpha $ for all $x, y \in S$.
\end{assume}

Finally, we will require the following assumption on the kernel $K_h: \cX \times \cX \to \R$.
% Under these assumptions, given samples $x_1, \ldots, x_n \iid p_0$, we will propose a discretization to \cref{eqn:okl} by restricting the optimization problem over continuous densities of the form $\q{w}(x) = \sum_{i=1}^n w_i K_h(x, x_i)$ for weights $w \in \Delta_n$ chosen from a  probability simplex, and for a suitable probability density kernel $K_h: \cX \times \cX \to \R$. We make the following assumption about the choice of this kernel $K_h$. 

\begin{assume}
\label{assump:kernel-properties}
The kernel satisfies $K_h(x, y) = \frac{1}{h^d}\kappa\left( \frac{\| x - y \|_2}{h} \right)$ for a non-increasing continuous function $\kappa: \R_+ \rightarrow \R_+$ satisfying (i) $\int_{\R^d} \kappa(\|x \|_2)\, dx = 1$, (ii) $\kappa(0) \leq c_0$ for some fixed constant $c_0 >0$, and (iii) there exist constants $t_0, C_\rho, \rho$ such that $\rho \leq 1$ and $t_0 \geq 1$ so that for all $t > t_0$, $k(t) \leq C_\rho \exp(-t^\rho)$. Further, suppose that (iv) for every $h > 0$, the kernel $K_h$ is a positive semi-definite kernel, i.e. the $m \times m$ matrix $(K_h(y_i, y_j))_{i,j \in [m]}$ for any $y_1, \ldots, y_m \in \R^d$ and $m \geq 1$ is positive semi-definite. 
\end{assume}

Parts (i)-(iii) in \Cref{assump:kernel-properties} are standard assumptions in the kernel density estimation literature (see, e.g.~\cite{jiang2017uniform}), while part (iv) allows us to use techniques from reproducing kernel Hilbert spaces. By Shoenberg's theorem (see e.g.~\cite{ressel1976short}) part (iv) of \Cref{assump:kernel-properties} is satisfied whenever $\kappa$ is a completely monotone function (see e.g.~\cite{merkle2014completely}). In particular, note that \Cref{assump:kernel-properties} is satisfied by the Gaussian kernel given as $\kappa(t) = \frac{1}{(2\pi)^{d/2}}\exp(-t^2)$.  

\begin{definition} Suppose \Cref{assump:kernel-properties} holds and observations $x_1, \ldots, x_n \iid p_0$ are given. Define a mapping $L: \Delta_n \mapsto \Den$ from the \ndsimplex\ $\Delta_n$ to the space of densities $\Den$ given by $L(w) = \q{w}$, where
	$$
	\q{w}(\cdot) = \sum_{i=1}^n w_i K_h(\cdot, x_i).
	$$
	\label{defn:qhatw}
\end{definition}

For the probability density estimator $\hat{p}$ in the Monte Carlo estimate of the objective in \cref{eqn:okl}, we use the kernel density estimate $\hat{p}=\q{o}$, where $o=(1/n, \ldots, 1/n) \in \Delta_n$.
%to account for the fact that the points $x_1, \ldots, x_n$ are drawn from $p_0$. We let $\hat{p}$ be the kernel density estimate based on the samples $x_1, \ldots, x_n$ using the kernel $K_h$, noting that $\hat{p}=\q{o}$, where $o=(1/n, \ldots, 1/n) \in \Delta_n$. %To avoid handling tricky biases that are introduced by reusing samples, we will use sample splitting to separate computing our density estimate $\hat{p}$ for the optimization problem \cref{eqn:okl}. To this end, we will assume that we have an i.i.d.~sample $x_1,\ldots, x_{2n} \sim p_0$, and we compute the estimate $\hat{p}(\cdot) = \frac{1}{n}\sum_{i=n+1}^{2n} K_h(x_i, \cdot)$. 

Using the parameterization $w = A v$ for $v \in \Delta_n$, one can rewrite \cref{eqn:okle1} using the above notation as:
$$
\hatI(\theta) = \inf_{\substack{v \in \Delta_n \\ \frac{1}{2n} \sum_{i=1}^n \left|\frac{\q{v}(x_i)}{\hat{p}(x_i)} - 1\right| \leq \epsilon}} \frac{1}{n}\sum_{i=1}^n \frac{\q{v}(x_i)}{\hat{p}(x_i)} \log \frac{\q{v}(x_i)}{p_\theta(x_i)}.
$$



%The estimate $\hat{p}$ will be used in the denominator to compute importance weights of the samples $x_1,\ldots,x_n$, and can thus introduce large deviations unless it is bounded from both above and below. 
In order to ensure stability of the optimization objective in \cref{eqn:okle1} and guarantee regularity in the optimal weights, we will restrict the optimization over weights that do not deviate too far from uniformity and replace $\hat{p}$ by the `clipped' version of our estimator: $\hat{p}_\gamma(x) = \min(\max( \hat{p}(x), \gamma), 1/\gamma)$. To that end, define $\Delta_n^r = \{ v \in \Delta_n : n \cdot v_i \in [r,1/r] \text{ for all } i \}$ for $r \leq 1$. Then for our theoretical result, we will consider a version of the estimator 
\begin{align}
	\label{eqn:I-hat-defn}
	\hatI(\theta) = \inf_{\substack{v \in \Delta_n^{\gamma^2/4} \\ \frac{1}{2n}\sum_{i=1}^n \left|  \frac{\q{v}(x_i)}{\hat{p}_\gamma(x_i)} - 1\right| \leq \epsilon}} \frac{1}{n} \sum_{i=1}^n \frac{\q{v}(x_i)}{\hat{p}_\gamma(x_i)} \log \frac{\q{v}(x_i)}{p_\theta(x_i)},
\end{align}
where $\gamma$ is the value from \Cref{assump:bounded-densities}.

Under the above assumptions, we have the following result:

\begin{theorem}
	\label{thm:okl-convergence-formal}
	Suppose \Cref{assump:bounded-support,assump:bounded-densities,assump:smooth-densities,assump:kernel-properties} hold and $n \geq n_0, h \leq \bar{h}, and \frac{\log n}{\sqrt{n} h^d} \leq \bar{\eta}$ for suitable constants $\bar{h}, n_0, \bar{\eta} > 0$ that depend on the quantities in the assumptions. Fix a $\beta > 0$, and suppose we obtain an i.i.d.~sample $x_1, \ldots, x_n$ of size $n$ from density $p_0$. Then with probability at least $1-e^{-(\log n)^{2\beta}}$,
	\[ \left| \hat{I}_\epsilon(\theta) - I_\epsilon(\theta) \right| \leq O\left( \frac{\log n}{\sqrt{n} h^d} + h^{\min(\alpha/2,1)} \log \frac{1}{h} + \phi(\sqrt{h}) \log \frac{1}{\phi(\sqrt{h})} \right),   \]
	where the $O(\cdot)$ hides constant factors depending on the quantities in Assumptions~\ref{assump:bounded-support}-\ref{assump:kernel-properties}, and on the choice of $\epsilon, \beta > 0$.
\end{theorem}

When $S=B(0,R)$ is a Euclidean ball of radius $R$, Taylor expansion shows that $\phi(\sqrt{h}) = \frac{\sqrt{h}d}{R} + O(h)$.  Suppose, for simplicity that $\alpha \geq 1$, then taking $h=n^{-\frac{1}{2d+1}}$ and $\beta=1$, we see that $|\hatI(\theta) - \okl| = \tilde{O}(n^{-\frac{1}{4d+2}})$ upto a logarithmic factor in $n$, with probability at least $1-1/n$. 

Note that the choice of bandwidth parameter  $h=o(n^{-\frac{1}{2d}})$ required to shrink our error terms to zero decreases more slowly with $n$ than the optimal rate $h=o(n^{-1/d})$ for density estimation \citep{rigollet2009optimal}. This suggests that our error bounds can potentially be improved.
 Note also that the   above result holds for any fixed value of  $\theta \in \Theta$; extensions to uniform convergence over all $\theta \in \Theta$ may be possible with further assumptions on the complexity of the class $\Theta$ and continuity of the map $\theta \mapsto \log p_\theta(\cdot)$, but we do not pursue this direction here.


\begin{proof}[Proof outline of \Cref{thm:okl-convergence-formal}]
We sketch the proof here. 
\begin{description}
    \item[\Cref{sec:uniform-convergence}] First, \Cref{lem:uniform-convergence} shows that with high probability for large $n$, the estimators of KL divergence and total variation in \cref{eqn:I-hat-defn} are close to their population counterparts $\KL(\q{v}|p_\theta)$ and $\tv(\q{v}, p_0)$ for any $v \in \Delta_n$ such that $\q{v}$ is suitably bounded. See  \Cref{cor:all-the-bounds} for the most useful version of this statement. 
\item[\Cref{sec:kernele-density-estimation-bounds}] Next, \Cref{lem:existence-of-good-estimator} shows that for any density $q$ over $S$ satisfying Assumption~\ref{assump:bounded-densities}, with high probability for large $n$, there exists a $v \in \Delta_n^\gamma$ such that $\q{v}$ is a pointwise accurate estimator of $K_h \star q$, where $K_h \star q$ denotes the convolution of the density $q$ with the probability kernel $K_h$. Combined with \Cref{lem:info-proj-sandwich}, which establishes the boundedness of the minimizer $\iproj$ in \cref{eqn:okl}, we have that there exists a pointwise accurate estimator $\q{v^*}$ of $K_h \star \iproj$ for some $v^* \in \Delta_n^{\gamma^2/4}$. %A similar argument shows that $\q{e}$ is a pointwise accurate estimator of $K_h \star p_0$.
    \item[\Cref{sec:okl-smoothed-approx}] \Cref{lem:smoothed-densities-kl-tv-approx} shows that under \Cref{assump:smooth-densities}, $\KL(K_h \star q | p_\theta)$ and $\tv(K_h \star q, p_0)$ must not be much greater than $\KL(q | p_\theta)$ and $\tv(q, p_0)$, respectively, when $h$ is sufficiently small. %Along with the continuity of the map $\epsilon \mapsto I_\epsilon(\theta)$ (\Cref{lem:OKL-is-continuous}), this essentially shows that one can approximate $I_\epsilon(\theta)$ by restricting the optimization problem in \cref{eqn:okl} to densities of the form $K_h \star q$ when $h$ is suitably small. 
\end{description}
We combine the above arguments (see \Cref{sec:proof-of-theorem}), to show that the optimal objective value in \cref{eqn:okl} corresponding to the minimizer $\iproj$, must be close to the optimal objective value in \cref{eqn:I-hat-defn}, thus showing that $\hat{I}_\epsilon(\theta)$ can well approximate $I_\epsilon(\theta)$ when $h$ is small, and $n$ is large.
\end{proof}

While \Cref{sec:proof-of-approx-theorem} covers the main steps of proof, there are useful results in \Cref{sec:useful-lemmmas} that provide upper bounds on the tail probability and second moment of the kernel $K_h$ based on \Cref{assump:kernel-properties}. These results are used in \Cref{sec:okl-smoothed-approx} and \Cref{sec:proof-of-theorem}.


% As discussed earlier, \Cref{sec:proof-of-approx-theorem} covers the main steps of proof. \Cref{sec:useful-lemmmas} contains some key results that support the main proof. Namely
% \begin{description}
% 	\item[\Cref{sec:sandwiching}] shows that the minimizing density in \cref{eqn:okl} also satisfies the upper and lower bounds from \Cref{assump:bounded-densities}; in fact we prove a stronger property that the minimizing density is \emph{sandwiched} between $p_\theta$ and $p_0$. Here we prove an intuitive sandwiching lemma (\Cref{lem:sandwich-phi-div}) that says that a $\phi$-divergence $D_\phi(p|q)$ with $\phi(1)=0$  (e.g.~KL-divergence or total-variation distance) always decreases when its first argument is replaced by a density that is sandwiched between $p$ and $q$. 
% 	\item[\Cref{sec:kernel-tail-bounds}] calculates upper bounds on the tail probability and second moment of the kernel $K_h$ based on  \Cref{assump:kernel-properties}. These results are used in \Cref{sec:okl-smoothed-approx} and \Cref{sec:proof-of-theorem}.
% 	% 	\item[\Cref{sec:uniform-convergence}] shows that objective function and constraint in \cref{eqn:I-hat-defn} approximate suitable KL-divergence and TV terms in \cref{eqn:okl}. 
% 	% 	\item[\Cref{sec:okl-smoothed-approx}] shows that the optimization problem in \cref{eqn:okl} can be well-approximated by replacing $q \in \Den$ with the kernel-smoothed version of the density $\smooth{q}$.
% \end{description}



\paragraph{Additional notation}
For $p \in \Den$ and $A \subseteq \cX$, we will use the shorthand $p(A)$ to denote the quantity $\int_A p(x) dx$.
% The support of $p$, denoted by $\supp(p) \subseteq \cX$, is formally defined as the intersection all of closed sets $C \subseteq \cX$ such that $p(C) = 1$.
For a  probability density kernel $K_h$, let \[\smooth{p}(\cdot) = (K_h \star p)(\cdot) = \int p(y) K_h(\cdot, y) dy \in \Den \] denote the kernel-smoothed version of a density $p \in \Den$. Similarly, for a measure $\mu$ on $\cX$, we can define $(K_h \star \mu) (x) = \int K_h(x, y) \mu(dy)$. For a set $S \subseteq \R^d$ and $p, q \in \Den$, let $\KL_S(q| p) = \int_S q(x) \log \frac{p(x)}{q(x)} \, dx$. The constant $v_d$ will denote the volume of the Euclidean ball in $\R^d$. The notation $\|f\|_1 = \int |f(x)| dx$ and $\|f\|_{\infty} = \sup_{x \in \cX} |f(x)|$ will denote the $L_1$ and $L_\infty$ norm of the function $f$. For $p, q \in \Den$, recall that $\tv(p,q) = \frac{1}{2}\|p-q\|_1.$

%) = 1$.
%
%For a $A \subseteq \cX$, let $\Den[A] = \{p \in \Den \mid \supp(p) \subseteq A\}$ denote the set of densities that are supported on the set $A$, where $\supp(p) = \{x: p(x) > 0\} \subseteq \cX$.  
%
%We sometimes also  consider the  finite dimensional probability simplex $\Delta_n = \{ p \in [0,1]^n | \sum_{i=1}^n p_i = 1 \}$. %For $p, q \in \Delta_n$, let $\supp(p) = \{i \in [n] | p_i > 0\}$, and  $\KL(p|q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i}$ if $\supp(p) \subseteq \supp(q)$ otherwise $\KL(p|q) = \infty$.


\iffalse
\subsection{Useful lemmas}
\label{sec:useful-lemmmas}


\subsubsection{Sandwiching property of the OKL optimizer}
\label{sec:sandwiching}

A simple consequence of \Cref{assump:bounded-densities} is that $\KL(p_0|p_\theta) \leq \frac{2V_S}{\gamma}\log(1/\gamma)$ is finite. In particular this shows that $I_\epsilon(\theta) < \infty$, and hence by \cite{csiszar1975divergence} there is a ($\lambda$-almost everywhere) unique density  $\iproj \in \Den$ that we will call the information ($I$-)projection such that $\tv(\iproj, p_0) \leq \epsilon$ and $\KL(\iproj|p_\theta) = I_\epsilon(\theta)$. Based on the following notion of \emph{sandwiching}, in this sub-section we will show in that the I-projection $\iproj$ is sandwiched between the two densities $p_0$ and $p_\theta$ for any value of $\epsilon > 0$. 

\begin{definition}
	For probability vectors $p, q, r \in \Delta_n$, we say that $q$ is \emph{sandwiched} between $p$ and $r$ if $\min(r_i, p_i) \leq q_i \leq \max(r_i, p_i)$ for all $i=1,\ldots, n$. Similarly, if $p, q, r \in \Den$ are probability densities, then we say that $q$ is sandwiched between $p$ and $r$ if the condition  $\min(p(x),r(x)) \leq q(x) \leq \max(p(x),r(x))$ holds for $\lambda$-almost every $x$.
\end{definition}

The following proposition will be important in proving the sandwiching property for the I-projection.
\begin{proposition}
	\label{prop:sandwich-kl-tv}
	For probability vectors (or densities), if $r$ is sandwiched between $p$ and $q$, then $\tv(r,p) \leq \tv(q,p)$ and $\KL(r|p) \leq \KL(q|p)$.
\end{proposition}

In fact, will prove the above result for any $\phi$-divergence $D_\phi(p,q) = \int \phi(p/q) q d\lambda$ when $\phi$ is a convex function $\phi(1)=0$. The total variation distance ($\phi(x) = |x - 1|$) and KL-divergence ($\phi(x) = x \log x$) will emerge as special cases.

\begin{lemma}
	\label{lem:sandwich-phi-div}
	Let $\phi: \R \to (-\infty, \infty]$ be a proper convex function with $\phi(1) = 0$. If a density $r$ is sandwiched between densities $p$ and $q$, then $D_\phi(r,q) \leq D_\phi(p,q)$.
\end{lemma}
\begin{proof}
	The sandwiching property implies that there is a function $t: \cX \to [0,1]$ such that $r = (1-t) p + t q$.  Hence	
	\begin{align*}
		D_\phi(r,q) &= \int \phi((1-t) p/q + t q/q ) q d\lambda \leq \int (1-t) \phi(p/q) q d\lambda + \int t \phi(1) q d\lambda \\
		&= D_\phi(p,q) - \int t \phi(p/q) q d\lambda \leq D_\phi(p,q) - \xi\int t q(p/q - 1) d\lambda = D_\phi(p,q). 
	\end{align*}
	where the two inequalities follow from the convexity of $\phi$ noting that there is $\xi \in \mathbb{R}$ (called the sub-gradient) such that 
	$\phi(x) \geq \phi(1) + \xi(x-1) = \xi(x-1)$ for all $x \in \R$, and the last equality holds since $\int t(p-q) d\lambda = 0$, since $p$ and $r$ are assumed to integrate to one.
\end{proof}
% We also have the following, stronger result for total variation distance.

% \begin{lemma}
	%     Let $p, q, r$ denote three probability densities (or vectors). If $\int$
	% \end{lemma}

%\subsection{Information projections are bounded}

Now, we will need a simple lemma about total variation distance before we can prove the sandwiching property of the I-projection.

\begin{lemma}
	\label{lem:tv-transform}
	Let $p,q,r \in \Den$. If $p(x)\leq q(x)$ for all $x$ satisfying $q(x) < r(x)$, then $\tv(q,p) \leq \tv(r,p)$. Similarly, if $p(x) \geq q(x)$ for all $x$ satisfying $q(x) > r(x)$, then $\tv(q,p) \leq \tv(r,p)$.
\end{lemma}
\begin{proof}
	We will prove the first statement. The second follows symmetrically. Let $S^+ = \{x : q(x) < r(x) \}$, $S^- = \{x : q(x) > r(x) \}$, and $S^= = \{x : q(x) = r(x) \}$. Then we have
	\begin{align*}
		\int |p(x) - r(x)| \, dx &= \int_{S^+} |p(x) - r(x)| \, dx + \int_{S^-} |p(x) - r(x)| \, dx + \int_{S^=} |p(x) - r(x)| \, dx \\
		&= \int_{S^+} (r(x) - q(x) + q(x) - p(x)) \, dx + \int_{S^-} |p(x) - q(x) + q(x) - r(x)| \, dx \\
		&+ \int_{S^=} |p(x) - q(x)| \, dx \\
		&\geq \int_{S^+} r(x) - q(x) + q(x) - p(x) \, dx + \int_{S^-} (|p(x) - q(x)| - |q(x) - r(x)|) \, dx \\
		&+ \int_{S^=} |p(x) - q(x)| \, dx \\
		&= \int |p(x) - q(x)| \, dx,
	\end{align*}
	where the inequality follows from the reverse triangle-inequality, and the last line follows from the fact that 
	\[ \int_{S^+} (r(x) - q(x)) \, dx = \tv(q, r) = \int_{S^-} (q(x) - r(x)) dx .\]
\end{proof}


\begin{lemma}
	\label{lem:info-proj-sandwich}
	Let $p_0, p_\theta$ be probability densities, and let $\iproj$ denote the unique minimizer in \cref{eqn:okl}. Then $\iproj$ is sandwiched between $p_0$ and $p_\theta$.
\end{lemma}
\begin{proof}
	For an arbitrary $q \in \Den$, we will show that there is a $\bar{q} \in \Den$ that is sandwiched between $p_0$ and $p_\theta$ such that $\tv(\bar{q},p_0) \leq \tv(q, p_0)$ and $\KL(\bar{q}|p_\theta) \leq \KL(q|p_\theta)$. Then since $\iproj$ is the unique minimizer of \Cref{eqn:okl} upto null sets, we must have that $\iproj$ is sandwiched between $p_0$ and $p_\theta$. 
	
	Let $q \in \Den$ be a probability density, and suppose the set $S^+ = \{x : q(x) > \max(p_0(x), p_\theta(x)) \}$ has non-empty (in fact, that it has non-zero Lebesgue measure). Let 
	\[ v = \int_{S^+} (q(x) - \max(p_0(x), p_\theta(x)) \, dx. \]
	Observe that
	\[  \tv(q, p_\theta)  =\int (p_\theta(x) - q(x)) \I{p_\theta(x) > q(x)} \, dx =  \int (q(x) - p_\theta(x)) \I{q(x) > p_\theta(x)} \, dx
	\geq v. \]
	Define the density
	\[ \bar{q}(x) = 
	\begin{cases} 
		\max(p_0(x), p_\theta(x)) & \text{ if } x \in S^+ \\   
		q(x) + \frac{v}{\tv(q, p_\theta)}(p_\theta(x) - q(x)) & \text{ if } p_\theta(x) > q(x) \\
		q(x) & \text{ otherwise }
	\end{cases}. \]   
	Then it is not hard to verify that $\bar{q}$ integrates to one and satisfies $q(x) \leq \max(p_0(x), p_\theta(x))$ everywhere. Moreover,
	applying \Cref{lem:tv-transform}
	with $p=p_0$, $q=\bar{q}$ and $r=q$, we obtain $\tv(\bar{q}, p_0) \leq \tv(q, p_0)$ since the condition $\bar{q} < q$ only occurs on the set $S_+$.
	Additionally, $\bar{q}$ is sandwiched between $p_\theta$ and $q$. Thus, \Cref{prop:sandwich-kl-tv} implies that $\KL(\bar{q} | p_\theta) \leq \KL(q | p_\theta)$.
	
	
	
	Now let $q \in \Den$ such that $S^+$ is empty but the set $S^- = \{x : q(x) < \min(p_0(x), p_\theta(x)) \}$ is non-empty. Letting 
	\[ v = \int_{S^-} (\min(p_0(x), p_\theta(x)) - q(x)) \, dx, \]
	note that $0 < v \leq \tv(q,p_\theta)$, and define the density
	\[ \bar{q}(x) = 
	\begin{cases} 
		\min(p_0(x), p_\theta(x)) & \text{ if } x \in S^- \\   
		q(x) + \frac{v}{\tv(q, p_\theta)}(p_\theta(x) - q(x)) & \text{ if } p_\theta(x) < q(x) \\
		q(x) & \text{ otherwise }
	\end{cases}. \]  
	Then observe that $\bar{q}$ is a density and it is sandwiched between $q$ and $p_\theta$. Similar arguments as above using \Cref{lem:tv-transform} and \Cref{prop:sandwich-kl-tv} show that $\tv(\bar{q}, p_0) \leq \tv(q, p_0)$ and $\KL(\bar{q} | p_\theta) \leq \KL(q | p_\theta)$.
\end{proof}
\fi

\subsection{Kernel tail bounds}
\label{sec:useful-lemmmas}

In this sub-section, we derive tail bounds for a class of probability kernels $K_h(x,y) = \frac{1}{h^d}\kappa(\|x-y\|/h)$ on $\R^d$ indexed by parameter $h > 0$ used for density estimation, when the function $\kappa: [0, \infty) \to [0,\infty)$ has exponentially decaying  tails (see parts (i)-(iii) of \Cref{assump:kernel-properties}). The following two lemmas bound the tail distribution and the variance of the random variable $Z$ having density $x \mapsto \kappa(\|x\|)$. These results will then be used to obtain tails bounds for the kernel $K_h$.

\begin{lemma} 
	\label{lem:exp-tail-bound}
	Suppose \Cref{assump:kernel-properties} holds. If $Z$ is an $\R^d$ valued random vector with probability density $x \mapsto \kappa(\|x\|)$, then 
	\begin{equation}
		\prob(\|Z\| \geq t) = \int_{\|x\| \geq t} \kappa(\|x\|) dx \leq C_1 t^{d+1-\rho} e^{-t^\rho}
	\end{equation}
	for each $t \geq  t_1 = \max(\Gamma(a+1)^{1/{\rho(a-1)}}, t_0) \geq 1$ where $\Gamma$ is the Gamma function, $a = \frac{d+1}{\rho}$, and $C_1 > 0$ is a constant depending on constants $t_0, C_\rho, \rho > 0$ in  \Cref{assump:kernel-properties} and dimension $d$. Explicitly, $C_1=\rho^{-1}C_\rho v_d 2^{a-1}$, where $v_d = \frac{\pi^{d/2}}{\Gamma(d/2+1)}$ is the volume of the unit ball in $\R^d$.    
	%\chris{We can replace $d S_d$ with $v_d$, the volume of the unit ball in $\R^d$, as this is used almost everywhere else.} \md{The previous proof had erros. Here is a new version using bounds on the incomplete gamma function.} 
\end{lemma}
\begin{proof}
	Using \cite[Lemma~4]{jiang2017uniform} and the upper bound on tails of $\kappa$ since $s \geq t \geq t_0$, 
	\begin{align*}
		\int_{\|x\| \geq t} \kappa(\|x\|) dx = v_d \int_{t}^\infty \kappa(r) r^d dr \leq C_\rho v_d \int_t^\infty e^{-r^\rho} r^{d} dr.
	\end{align*}
	Using the substitution $s = r^\rho$, we obtain
	$$
	\int_t^\infty e^{-r^\rho} r^{d} dr = \frac{1}{\rho}\int_{t^\rho}^\infty e^{-s} s^{\frac{d+1}{\rho} - 1} ds = \frac{1}{\rho}\Gamma\left(a, t^\rho\right)
	$$
	where $\Gamma(a,y) = \int_y^\infty e^{-x} x^{a-1} dx$ is the incomplete Gamma function and $a = \frac{d+1}{\rho} \geq 2$. Taking $b_a = \Gamma(a+1)^{1/(a-1)}$, we have the following bound on the incomplete Gamma function \cite[Theorem~1.1]{pinelis2020exact} for $a \geq 2$:
	$$
	\Gamma(a,y) \leq \frac{((b_a + y)^a - y^a)e^{-y}}{a b_a} = \frac{e^{-y}\int_{0}^{b_a} a (x + y)^{a-1} dx}{a b_a} \leq (b_a + y)^{a-1} e^{-y}.
	$$   
	Since $t^\rho \geq b_a$, the proof can be completed by combining the above displays along with the bound
	$$
	\Gamma(a, t^\rho) \leq 2^{a-1} t^{\rho(a-1)} e^{-t^\rho}  = 2^{a-1} t^{d+1-\rho} e^{-t^\rho}.
	$$
	%Now let us bound the integral in the rightmost term starting with the substitution $s=r^d$ followed the substitution by $h = s-t^d$
	% \begin{align*}
		%     \int_t^\infty e^{-r^\rho} r^{d-1} dr  = \frac{1}{d}\int_{t^d}^\infty e^{-s^{\rho/d}} ds &= \frac{e^{-t^\rho}}{d}  \int_0^\infty e^{-[(t^d + h)^{\rho/d} - t^\rho] } dh\\
		%     &\leq \frac{e^{-t^\rho}}{d} \int_0^\infty e^{- \frac{\rho t^{\rho-1}}{d} h} dh = \frac{d e^{-t^\rho}}{\rho t^{\rho-1}}
		% \end{align*}
	% where the last inequality follows from the bound $(a+h)^\beta - a^\beta \geq \beta a^{\beta-1}h$ for any $a, h \geq 0$ and $\beta \geq 1$. The proof is established by combining the two displays. \md{The last inequality is incorrect. Rewrite the proof in terms of bounds on the incomplete gamma function.}
\end{proof}

\begin{lemma}
	\label{lem:bounded-second-moment}
	Suppose \Cref{assump:kernel-properties} holds. If $Z$ is an $\R^d$ valued random vector with probability density $x \mapsto \kappa(\|x\|)$, then
	\[\E_{Z}[\|Z\|_2^2] \leq v_d \left(c_0 t_0^{d+3} + \frac{C_\rho}{\rho} \Gamma\left( \frac{d+3}{\rho}  \right) \right) ,\] 
	where $v_d$ is the volume of the unit ball in $d$-dimensions and $\Gamma(\cdot)$ is the Gamma function.
\end{lemma}
\begin{proof}
	We can write
	\begin{align*}
		\E_{Z}[\|Z \|_2^2] = \int_{\R^d} \|z \|_2^2 \kappa(\|z \|_2) \, dz
		= v_d \int_0^\infty \kappa(t) t^{d+2} \, du,
	\end{align*}
	where the second equality follows from \cite[Lemma~4]{jiang2017uniform}. Taking $c_0, t_0, \rho, C_\rho$ as the constants from Assumption~\ref{assump:kernel-properties}, we have
	\begin{align*}
		\int_0^\infty \kappa(t) t^{d+2} \, du &= \int_{0}^{t_0} \kappa(t) t^{d+2} \, du + \int_{t_0}^{\infty} \kappa(t) t^{d+2} \, du \\
		&\leq c_0 t_0^{d+3} + C_\rho \int_{t_0}^\infty \exp(- t^\rho) t^{d+2} \, du \\
		&\leq c_0 t_0^{d+3} + \frac{C_\rho}{\rho} \int_0^\infty \exp(-u) u^{\frac{d+3}{\rho} - 1} \, du \\
		&= c_0 t_0^{d+3} + \frac{C_\rho}{\rho} \Gamma\left( \frac{d+3}{\rho}  \right),
	\end{align*}
	where we have used the substitution $u = t^\rho$ in the second inequality as well as the fact that the integrands are non-negative from 0 to $\infty$, and the last line follows from the definition of the Gamma function.
\end{proof}

We now use the above lemmas to prove properties about the kernel $K_h$.

\begin{lemma}
	\label{lem:kernel-support-concentration}
	Suppose that \Cref{assump:kernel-properties} holds. For any $x \in \R^d$, 
	\[ 
	\int_{B(x,r)} K_h(x,y) \, dy \geq 1 -  \frac{C_1 k!}{(r/h)^4} 
	\]
	whenever $r \geq t_1 h$, where $B(x,r)$ denotes the open ball of radius $r$ around $x$, constants $C_1, t_1 > 0$ are as defined in \Cref{lem:exp-tail-bound}, and $k = \lceil \frac{5+d}{\rho}\rceil - 1$. 
	%
	In particular, for any bounded set $S \subset \R^d$ and any $x \in S$, we have
	\[ \int_S K_h(x,y) \, dy \geq 1 - \frac{C_1 k!}{(r_x/h)^4}  \]
	whenever $r_x \geq t_1 h$, where $r_x = \inf_{y \in S^c} \| x -y \|_2$. 
	%
	If $0 < h \leq 
	h_0 = \min(\frac{1}{C_1 k!}, \frac{1}{t_1^2})$, we can always take $r = \sqrt{h}$, and the bound on the right hand side simplifies to $1-\frac{C_1 k!}{(r/h)^4} \geq 1- h$. 
\end{lemma}
\begin{proof}
	Let $Z$ be an $\R^d$-valued random vector with probability density $z \mapsto \kappa(\|z\|_2)$, then 
	\begin{align*}
		1 - \int_{B(x,r)} K_h(x,y) \, dy &= \Pr( x + hZ \notin B(x,r)) \\
		&= \Pr\left(\|Z\|_2 \geq \frac{r}{h}\right) \leq C_1 (r/h)^{(d+1-\rho)}e^{-(r/h)^\rho} \\
		&\leq \frac{C_1 k!}{(r/h)^{k \rho - (d+1-\rho)}} \leq \frac{C_1 k!}{(r/h)^4}.
	\end{align*}
	where the inequality in the second line follows from \Cref{lem:exp-tail-bound} whenever $r \geq t_1 h$, and the inequality on the last line follows by using  $e^y \geq \frac{y^k}{k!}$ and $k \rho - (d+1-\rho) \geq 4$. To get the statement for set $S$, we observe that $1- \int_S K_h(x,y)  \, dy \geq 1 - \int_{B(x,r_x)} K_h(x,y) \, dy$. 
	
	%To finish the proof, we use the fact that $h^{-(\rho -1)/2} \leq h^{-\rho/2}$ and the inequalities $z \geq \log(1+ z)$ and for fixed $a \geq 1$, $z$ satisfies $z \geq a \log z$ whenever $z \geq 2 a \log a$.
\end{proof}


For any probability measure $\mu$ on $\cX$, we define the density $f_{h,\mu}(y) = \int K_h(y, x) \mu(dx) \in \Den$ obtained by convolving $\mu$ with the kernel $K_h$.
% The next result controls the tails of the density $f_{h,\mu}$ assuming that $\mu$ is supported on $S$. 
The next result bounds the TV-distance $\tv(f_{h,\mu}, f_{h,\nu})$ in terms of the supremum norm $\|f_{h,\mu}-f_{h,\nu}\|_{\infty}$ when measures $\mu$ and $\nu$ are supported on $S$. 
%This result will be useful in \Cref{lem:existence-of-good-estimator}, where uniform estimates on the supremum norm $\|f_{h,\mu}-f_{h,\nu}\|_{\infty}$ are obtained when $\mu = \sum_{i=1}^n w_i \delta_{x_i}$ and $\nu$ is the corresponding target distribution.

% \begin{corollary} Suppose \Cref{assump:bounded-support} and \Cref{assump:kernel-properties} hold, and $\mu$ is a probability measure supported on $S$. Then for any $r \geq t_1 h$,
% 	$$
% 	\int_{\|y\| \geq R+r} f_{h,\mu}(y) dy \leq C_1 (r/h)^{d+1-\rho} e^{-(r/h)^\rho}
% 	$$
% 	where constants $C_1, t_1 > 0$ as defined in \Cref{lem:exp-tail-bound}.
% 	\label{cor:kernel-smoothed-density-tails}
% \end{corollary}
% \begin{proof}
% 	Since $\mu$ is supported on $S$ and $K_h$ is symmetric, note that $f_{h,\mu}(y) = \int_S K_h(x, y) \mu(dx)$. Thus by Fubini's theorem
% 	$$
% 	\int_{\|y\| \geq R+r} f_{h,\mu}(y) dy = \int_S \left(\int_{\|y\| \geq R+ r} K_h(x, y) dy \right) \mu(dx).
% 	$$
% 	Next, note that the proof of \Cref{lem:kernel-support-concentration} shows that $\int_{\|y\| \geq R+ r} K_h(x, y) dy \leq 1 - \int_{B(x,r)} K_h(x, y) dy \leq C_1 (r/h)^{d+1-\rho} e^{-(r/h)^\rho}$ for any $x \in S$ since $B(x,r) \subseteq B(0,R + r)$ whenever $x \in S \subseteq B(0,R)$. The proof is then completed by using this inequality to upper bound the term in the display above.
% \end{proof}

\begin{corollary} 
	\label{cor:tv-uniform-estimates}
	Suppose \Cref{assump:bounded-densities,assump:kernel-properties} hold and $\mu$ and $\nu$ are two probability measures supported on $S$. Then whenever $h \leq 1/t_1$,
	$$
	\tv(f_{h,\mu}, f_{h, \nu}) \leq \frac{v_d  (R + 1)^d}{2} \|f_{h,\mu} - f_{h,\nu}\|_\infty  + C_1 h^{d+1-\rho}{e^{-h^{-1/\rho}}}
	$$ where $v_d$ is the volume of the unit ball in $\R^d$, and $t_1$ is as defined in \Cref{lem:exp-tail-bound}.
\end{corollary}
\begin{proof}
We first claim that
\begin{align}
\label{eqn:kernel-smoothed-density-tails}
\int_{\|y\| \geq R+1} f_{h,\mu}(y) dy \leq C_1 (1/h)^{d+1-\rho} e^{-(1/h)^\rho} \text{ for any $h \leq 1/t_1$} .
\end{align}
To see this, we note that $f_{h,\mu}(y) = \int_S K_h(x, y) \mu(dx)$, since $\mu$ is supported on $S$ and $K_h$ is symmetric. Thus by Fubini's theorem
$$
\int_{\|y\| \geq R+r} f_{h,\mu}(y) dy = \int_S \left(\int_{\|y\| \geq R+ r} K_h(x, y) dy \right) \mu(dx).
$$
Next, note that the proof of \Cref{lem:kernel-support-concentration} shows that 
\[ \int_{\|y\| \geq R+ 1} K_h(x, y) dy \leq 1 - \int_{B(x,1)} K_h(x, y) dy \leq C_1 (1/h)^{d+1-\rho} e^{-(1/h)^\rho}\] for any $x \in S$ since $B(x,1) \subseteq B(0,R + 1)$ whenever $x \in S \subseteq B(0,R)$, proving \cref{eqn:kernel-smoothed-density-tails}.


To finish the proof of the lemma, observe that
\begin{align*}
    \tv(f_{h,\mu}, f_{h, \nu}) &=\frac{1}{2} \int_{B(0,R+1)} |f_{h,\mu}(x) - f_{h, \nu}(x)| dx + \frac{1}{2} \int_{\|x\| \geq R+1} |f_{h,\mu}(x) - f_{h, \nu}(x)| dx \\
    &\leq  \frac{v_d}{2}(R+1)^d \|f_{h,\mu} - f_{h, \nu}\|_{\infty} + C_1 h^{d+1-\rho}{e^{-h^{-1/\rho}}}
\end{align*}
where the bound on the second term follows from \cref{eqn:kernel-smoothed-density-tails}.
\end{proof}


Next, we can see how well the convolved density $\smooth{q} = (K_h \star q) \in \Den$ approximates the density $q \in \Den$. For instance, if the density $q$ is supported on a bounded set $S \subseteq \R^d$ with non-zero Lebesgue measure $V_S$, with boundary functional $\phi(r) = \frac{\lambda(S \setminus S_{-r})}{\lambda(S)}$ (see \Cref{assump:bounded-support}), then the following Lemma provides an upper bound on the mass placed by the convolved density  $\smooth{q}$ outside the set $S$.

\begin{lemma}
	\label{lem:smooth-support-concentration}
	Suppose that \Cref{assump:bounded-support} and \Cref{assump:kernel-properties} hold. For any probability density $q \in \Den$ and density bounded above by $1/\gamma$ and supported on the set $S$,
	\[ \smooth{q}(S^c) \doteq \int_{S^c} \smooth{q}(x) dx \leq  h + \frac{V_S}{\gamma} \phi(\sqrt{h}) \]
	whenever $h \leq h_0$, where $h_0$ is as given in \Cref{lem:kernel-support-concentration}.
\end{lemma}
\begin{proof}
	We use the following chain of arguments:
	\begin{align*}
		\smooth{q}(S^c)  &= \int_{S^c} \smooth{q}(x) dx = \int_{S^c} \int_S q(y) K_h(x, y) dy dx  \\
		&= \int_S q(y) \int_{S^c} K_h(x,y) \, dx \, dy \\
		&= \int_{S_{-\sqrt{h}}} q(y) \int_{S^c} K_h(x,y) \, dx \, dy + \int_{S\setminus S_{-\sqrt{h}}} q(y) \int_{S^c} K_h(x,y) \, dx \, dy \\
		&\leq h + \int_{S\setminus S_{-\sqrt{h}}} q(y) \, dy
		\leq h + \frac{V_S}{\gamma} \phi(\sqrt{h}),
	\end{align*}
	where the first inequality uses $\int_{S^c} K_h(x,y) dx = 1 - \int_{S} K_h(y,x) dx$ is bounded above by $h$ if $y \in S_{-\sqrt{h}}$  (\Cref{lem:kernel-support-concentration}) or by one if $y \in S \setminus S_{-\sqrt{h}}$, and the second inequality holds from the definition of boundary functional $\phi$ and the upper bound on the density $q$.
\end{proof}


For a set $S \subseteq \R^d$, let $\KL_S(q| p) = \int_S q(x) \log \frac{p(x)}{q(x)} \, dx$. The following lemma provides a useful trick to translate between $\KL$ and $\tv$ expressions for densities that are not supported on $S$ to densities that are supported on $S$.

\begin{lemma}
	\label{lem:restrict-to-S}
	Suppose $q \in \Den$ is a density that may not be supported on $S$. Then there is a density $\bar{q}$ supported on $S$ such that
	$$
	\KL_S(q|p_\theta) = (1-q(S^c))(\KL(\bar{q}|p_{\theta}) + \log (1-q(S^c)))
	$$
	and $\tv(\bar{q}, p_0) \leq \tv(q, p_0) + q(S^c).$
\end{lemma}
\begin{proof} We will take $\bar{q}(x) = \frac{q(x)}{q(S)} \I{x \in S}$ to be the restriction of the density $q$ to $S$. It is straightforward to see that the  equality for the KL terms hold. The TV bound follows from the triangle inequality, noting that $\tv(\bar{q}, q) = \frac{1}{2} \int_{\R^d} \abs{\bar{q}(x) - q(x)} dx = q(S^c)$.
\end{proof}


\subsection{Proof of Theorem~1}
\label{sec:proof-of-approx-theorem}

\subsubsection{Uniform convergence of KL and TV estimators}
\label{sec:uniform-convergence}

The main result of this subsection is the following.

% \begin{lemma}
	% \label{lem:uniform-convergence}
	% Suppose that \Cref{assump:bounded-densities} and \Cref{assump:kernel-properties} hold and let $u\geq 1\geq \ell$,. Let $\hat{p}:\R^d \rightarrow \R$ be a fixed probability distribution and let $\hat{p}_\gamma(x) = \min(\max( \hat{p}, \gamma), 1/\gamma)$. If $x_1, \ldots, x_n$ are drawn i.i.d. from $p_0$, then with probability at least $1-4\delta$, we have
	% \begin{align*}
		% \left| \left(\| q_w - p_0 \|_1 - q_w(B^c)\right) - \frac{1}{n} \sum_{i=1}^n \left| \frac{q_w(x_i)}{\hat{p}(x_i)} - 1 \right| \right| 
		% &\leq \frac{2}{\gamma^2} \left(2\sqrt{\frac{c}{n h^{d}}} + \sqrt{ \frac{1}{n} \log \frac{2}{\delta}} + \frac{\nu}{\gamma^2}\right) \hspace{1em} \text{ and }  \\
		% \left| \KL_B(q_w | p_\theta) - \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{\hat{p}(x_i)} \log \frac{q_w(x_i)}{p_\theta(x_i)} \right| 
		%  &\leq  \left(\frac{4}{\gamma^2} \log \frac{1}{\gamma}\right) \left(\sqrt{\frac{c}{n h^d}} + \sqrt{ \frac{1}{n} \log \frac{2}{\delta}} + \frac{\nu}{\gamma^2}\right)
		% \end{align*}
	% for all $w \in \Wcal_{\ell, u} = \{ w \in \Delta_n \, : \,  q_w(x_i) \in [\ell, u] \text{ for all } i \in [n] \}$ simultaneously, where $\nu = 2\tv(\hat{p}, p_0) + 2 \sqrt{\frac{1}{2n} \log \frac{1}{\delta}}$.
	% \end{lemma}
\begin{lemma}
	\label{lem:uniform-convergence}
	There exists an absolute constant $c_1 > 0$ such that the following holds. Suppose observations $x_1, \ldots, x_n$ are drawn i.i.d. from $p_0$, and that \Cref{assump:bounded-densities} and \Cref{assump:kernel-properties} hold. Let $\q{w}$ be as defined in \Cref{defn:qhatw}. Let $\hat{p} = \q{o}$, where $o=(1/n, \ldots, 1/n) \in \Delta_n$, be the kernel-density estimator for $p_0$  based on observations $x_1, \ldots x_n$, and let $\hat{p}_\gamma(\cdot) = \min(\max( \hat{p}(\cdot), \gamma), 1/\gamma)$ be its truncated version. With probability at least $1-\delta$, we have
	\begin{align*}
		\left| \left(\| \q{w} - p_0 \|_1 - \q{w}(S^c)\right) - \frac{1}{n} \sum_{i=1}^n \left| \frac{\q{w}(x_i)}{\hat{p}_\gamma(x_i)} - 1 \right| \right| 
		&\leq \frac{c_1 u}{\gamma^3} \left(\frac{c_0}{\sqrt{n} h^{d}} + \sqrt{\frac{2}{n}\log \frac{6}{\delta}} + \tv(\hat{p}, p_0) \right) \hspace{1em} \text{ and }  \\
		\left| \KL_S(\q{w} | p_\theta) - \frac{1}{n} \sum_{i=1}^n \frac{\q{w}(x_i)}{\hat{p}_\gamma(x_i)} \log \frac{\q{w}(x_i)}{p_\theta(x_i)} \right| 
		&\leq  \left(\frac{c_1 u}{\gamma^3} \log \frac{u}{\ell \gamma}\right) \left(\frac{c_0}{\sqrt{n} h^d} + \sqrt{ \frac{2}{n}\log \frac{6}{\delta}} + \tv(\hat{p}, p_0) \right).
	\end{align*}
	uniformly over all $w \in \Wcal_{\ell, u} = \{ w \in \Delta_n \, : \,  \q{w}(x_i) \in [\ell, u] \text{ for all } i \in [n] \}$, where thresholds $u, l$ are chosen so that $u\geq \max(1, \gamma e) \geq 1 \geq \ell$ and $\|f\|_1 = \int |f(x)| dx$.
\end{lemma}

%\jk{It's called 'Rademacher' (not 'Rademacher') complexity/random variables.}

To establish this result, we will use techniques from uniform law of large numbers over a class of functions $\Fc \subseteq \{ f: S \rightarrow \R \}$. For a fixed class $\Fc$, given i.i.d. samples $x_1, \ldots, x_n$ from density $p_0$ supported on $S \subseteq \cX$, we consider the empirical Rademacher complexity (e.g.~\cite{boucheron2005theory}) of the function class $\Fc$ defined as:
\begin{equation}
	\Rad(\Fc) = \E_{\epsilon} \left[ \sup_{f \in \Fc} \left| \frac{1}{n} \sum_{i=1}^n \epsilon_i f(x_i)\right| \right]
\end{equation}
where the expectation is over Rademacher random variables $\epsilon_1, \ldots \epsilon_n \iid \operatorname{Uniform}(\{-1,1\})$. The Rademacher complexity is a measure of richness of a function class that can be used to obtain a uniform law of large numbers result like the following standard result.

\begin{lemma}[c.f. Theorem~3.2 of \citep{boucheron2005theory}]
	\label{lem:bbl-rademacher-bounds}
	Let $\Fc \subset \{ f: S \rightarrow [-a,a] \}$ be a fixed class of bounded functions and suppose $\delta > 0$. If $x_1, \ldots, x_n \sim p_0$, then with probability at least $1-\delta$
	\[ 
	\sup_{f \in \Fc}\left| \frac{1}{n} \sum_{i=1}^n f(x_i) - \int f(x) p_0(x) dx \right| \leq 2 \Rad(\Fc) + a\sqrt{ \frac{2}{n} \log \frac{2}{\delta}}. 
	\]
\end{lemma}

Thus a typical task to obtain uniform laws of large numbers is to obtain upper bounds on $\Rad(\Fc)$ for various function classes $\Fc$. To this end, the following results will be useful: 

\begin{lemma}[c.f. Theorem~3.3 of \citep{boucheron2005theory}]
\label{lem:bbl-rademacher-lipschitz}
Suppose $g: S \rightarrow \R$ is a bounded function, $\Fc \subset \{ f: S \rightarrow \R \}$, and $\psi: \R \rightarrow \R$ is a function with Lipschitz constant $L$. Then
\begin{align*}
    \Rad( \{ x \mapsto g(x) f(x) : f \in \Fc \}) &\leq \Rad(\Fc) \sup_{x \in S} |g(x)| \\
    \Rad( \{ x \mapsto  \psi(f(x) ): f \in \Fc \}) &\leq L  \Rad(\Fc) + \frac{|\psi(0)|}{\sqrt{n}}.
\end{align*}
\end{lemma}
\begin{proof}
	The above statements with $\psi(0)=0$ follow from Theorem~3.3 of \citep{boucheron2005theory}. To handle the case $\psi(0) \neq 0$, define $\tilde{\psi} = \psi - \psi(0)\boldsymbol{1}$ where $\boldsymbol{1}$ denotes the function taking the constant value one, and note that 
	\begin{equation*}
		\begin{aligned}
			\Rad( \{ \psi \circ f : f \in \Fc \}) &= \Rad( \{ \tilde{\psi} \circ f + \psi(0) \boldsymbol{1} : f \in \Fc \}) \\
			&= \Rad( \{ \tilde{\psi} \circ f : f \in \Fc \}) + \Rad(\{\psi(0)\boldsymbol{1}\}) \\
			&\leq L \Rad(\Fc) + |\psi(0)| \Rad(\boldsymbol{1})
		\end{aligned}
	\end{equation*}
	where we have used that $\Rad(\{\boldsymbol{1}\}) \leq \E_\epsilon \abs{\frac{1}{n}\sum_{i=1}^n \epsilon_i} \leq \sqrt{\E_\epsilon \left(\frac{1}{n}\sum_{i=1}^n \epsilon_i \right)^2} = n^{-1/2}.$
\end{proof}

\begin{lemma}[Lemma~22 of \citep{bartlett2002rademacher}]
	\label{lem:kernel-rademacher-bounds}
	Let $k : \cX \times \cX \rightarrow \R$ be a positive definite kernel satisfying $k(x,x)\leq c$ for all $x \in \cX$. Then, for the function class 
	% \[ \Fc_a = \{ \cdot \mapsto \sum_{i=1}^m w_i k(\cdot, y_i) \, : \, m \geq 1, y_i \in \cX, w_i \in \R, \text{ for } i = 1, \ldots, m, \text{ and } \sum_{i,j} w_i w_j k(y_i, y_j) \leq a^2 \},  \]
 \[ \Fc_a = \bigg\{ \cdot \mapsto \sum_{i=1}^m w_i k(\cdot, y_i) \, : \, m \geq 1, y_i \in \cX, w_i \in \R, \text{ and } \sum_{i,j} w_i w_j k(y_i, y_j) \leq a^2 \bigg\},  \]
	we have $\Rad(\Fc_a) \leq 2a \sqrt{\frac{c}{n}}$. %\md{Definition of $\Fcal$ is confusing. Are $x_1, \ldots, x_n$ or $w_1, \ldots, w_n$ varying?}
\end{lemma}

We will now use the above results to bound the Monte Carlo estimation error uniformly over functions generated from a data-dependent function family $\{\q{w} :  w \in \Delta_n \}$.

\begin{lemma}
	\label{lem:uniform-bounds-over-qclass}
	Suppose bounded functions $\alpha, \beta, \eta : S \to \R$ and a Lipschitz function $\Phi: \R \to [-M, M]$ with Lipschitz constant $L$ are given, and suppose that \Cref{assump:kernel-properties} holds. Given a function $q: S \to \R$, denote $\psi(q, x) = \eta(x) \Phi(\alpha(x)q(x) + \beta(x))$. Then for any $\delta \in (0, 2/e)$, with probability $1-\delta$ it holds that:
	$$
	\sup_{w \in \Delta_n} \abs*{\frac{1}{n}\sum_{i=1}^n \psi(\q{w}, x_i) - \int \psi(\q{w}, x) p_0(x) dx } \leq \frac{C_1}{h^d\sqrt{n}} + C_2 \sqrt{\frac{2}{n}\log \frac{2}{\delta}}
	$$ 
	where $\q{w}$ is as in \Cref{defn:qhatw}, $C_1 = 4\|\eta\|_\infty \|\alpha\|_\infty L c_0$, and  $C_2 = \sqrt{2}\|\eta\|_\infty(L \|\beta\|_\infty + |\Phi(0)|) + \|\eta\|_\infty M$.
\end{lemma}
\begin{proof}
	Using $\hat{\Qcal}$ to denote the data dependent class of functions $\{\q{w} \mid w \in \Delta_n \}$, and taking $\kappa = K_h$, $c=c_0/h^d$, and $a=\sqrt{c}$ in \Cref{lem:kernel-rademacher-bounds}, let us first note that $\hat{\Qcal} \subseteq \Fc_a$. Indeed, this holds since for any $w = (w_1, \ldots, w_n) \in \Delta_n$ we have $\q{w}(\cdot) = \sum_{i=1}^n K_h(\cdot, x_i) w_i$ and 
	$$
	\sum_{i,j=1}^n w_i w_j K_h(x_i,x_j) \leq c \sum_{i=1}^n \sum_{j=1}^n w_i w_j = c, 
	$$
	where the inequality follows from the Cauchy Schwarz bound $K_h(x, y) \leq \sqrt{K_h(x, x) K_h(y, y)}$ using the positive definiteness of the kernel $K_h$, and the bound $K_h(x,x) \leq c_0 /h^d = c$ from \Cref{assump:kernel-properties}. Further, note by \Cref{lem:kernel-rademacher-bounds} that $\Rad(\Fc_a) \leq \frac{2c_0}{h^d\sqrt{n}}$.
	
	Next, let $\Gc$ denote the fixed function class $\{x \mapsto \psi(f, x) | f \in \Fc_a\}$. Repeated application of \Cref{lem:bbl-rademacher-lipschitz} shows:
	\begin{align*}
		\Rad(\Gc) &\leq  \|\eta\|_\infty \|\alpha\|_\infty L \Rad(\Fc_a) + \frac{ \|\eta\|_\infty(L \|\beta\|_\infty + |\Phi(0)|)}{\sqrt{n}}\\
		&\leq \frac{2\|\eta\|_\infty \|\alpha\|_\infty L c_0}{h^d\sqrt{n}} + \frac{ \|\eta\|_\infty(L \|\beta\|_\infty + |\Phi(0)|)}{\sqrt{n}}.
	\end{align*}
	
	Thus we can complete the proof by an application of \Cref{lem:bbl-rademacher-bounds} for the bounded class of functions $\Fc = \Gc$, and noting the inclusion $\hat{\Qcal} \subseteq \Fc_a$.
\end{proof}


With these uniform convergence results in place, now we can prove the main result of this section:

\begin{proof}[Proof of Lemma~\ref{lem:uniform-convergence}]
	By assumption
	\[ \frac{1}{n} \sum_{i=1}^n \left| \frac{1}{p_0(x_i)} - \frac{1}{\hat{p}_\gamma(x_i)} \right| \leq \frac{1}{\gamma^2 n} \sum_{i=1}^n \left|p_0(x_i) - \hat{p}_\gamma(x_i)\right| \leq \frac{1}{\gamma^2 n} \sum_{i=1}^n (\left|p_0(x_i) - \hat{p}(x_i)\right| \wedge \gamma^{-1}), 
	\]
	where the last inequality uses the fact that $\abs{p_0(\cdot) - \hat{p}_\gamma(\cdot)} \leq \min(1/\gamma, \abs{p_0(\cdot) - \hat{p}(\cdot)})$ since $p_0(\cdot), \hat{p}_\gamma(\cdot) \in [1/\gamma, \gamma]$. Recall $\hat{p} = \q{o}$, and hence we may apply \Cref{lem:uniform-bounds-over-qclass} with $\Phi(x)=|x|\wedge \gamma^{-1}$, $\beta = p_0$, $\alpha(\cdot)= \eta(\cdot) = 1$, and $w=o$, to show with probability $1-\delta/3$:
	\begin{align*}
		\frac{1}{n}\sum_{i=1}^n \left|p_0(x_i) - \hat{p}(x_i)\right|\wedge \gamma^{-1}
		&\leq \int (\gamma^{-1} \wedge |p_0(x) - \hat{p}(x)|) p_0(x) dx + \left(\frac{4 c_0 }{ h^d \sqrt{n}} +   \frac{3}{\gamma}\sqrt{\frac{2}{n} \log \frac{6}{\delta}}\right) \\
		&\leq \frac{2}{\gamma}\tv(\hat{p}, p_0)  + \left(\frac{4 c_0 }{ h^d \sqrt{n}} +  \frac{3}{\gamma}\sqrt{\frac{2}{n} \log \frac{6}{\delta}}\right) = \frac{\nu}{\gamma},
	\end{align*}
	where $\nu = 2\tv(\hat{p}, p_0) + \frac{4 \gamma c_0 }{h^d \sqrt{n}} +  3\sqrt{\frac{2}{n}  \log \frac{6}{\delta}}$. 
	
	
Turning to the $\ell_1$-bound, for any $w \in \Wcal_\gamma$, we have
	\begin{align*}
		\left| \frac{1}{n} \sum_{i=1}^n \left| \frac{\q{w}(x_i)}{\hat{p}_\gamma(x_i)} - 1 \right| - \frac{1}{n} \sum_{i=1}^n \left| \frac{\q{w}(x_i)}{{p}_0(x_i)} - 1 \right| \right|
		&\leq \frac{1}{n} \sum_{i=1}^n \left| \left| \frac{\q{w}(x_i)}{\hat{p}_\gamma(x_i)} - 1 \right| - \left| \frac{\q{w}(x_i)}{{p}_0(x_i)} - 1 \right| \right| \\
		&\leq \frac{1}{n} \sum_{i=1}^n \left| \frac{\q{w}(x_i)}{\hat{p}_\gamma(x_i)} -  \frac{\q{w}(x_i)}{{p}_0(x_i)} \right| \leq  \frac{\nu u}{\gamma^3}.
	\end{align*}
	Here the first inequality follows from the triangle inequality, the second follows from the reverse triangle inequality, and the last follows from the upper bound on $\q{w}$ and the bounds at the beginning of the proof. 
	% Thus, we have
	% \begin{align*}
		% \left| \| q_w - p_0 \|_1 - q_w(B^c) - \frac{1}{n} \sum_{i=1}^n \left| \frac{q_w(x_i)}{\hat{p}_\gamma(x_i)} - 1 \right| \right| 
		% \leq \left| \| q_w - p_0 \|_1 - q_w(B^c)  - \frac{1}{n} \sum_{i=1}^n \left| \frac{q_w(x_i)}{{p_0}(x_i)} - 1 \right| \right|  + \frac{2\nu}{\gamma^3}.
		% \end{align*}
	Observe that $\int \abs{\frac{q(x)}{{p_0}(x)} - 1} p_0(x) dx = \| q - p_0 \|_1 - q(S^c)$ for any density $q \in \Den$. 
	%\new{We may take $\kappa = K_h$, $a=\sqrt{u}$, and $c=c_0/h^d$ in \Cref{lem:kernel-rademacher-bounds} to conclude} $ \Rad(\Qcal_{\ell, u}) \leq 2 \sqrt{\frac{c_0 u}{n h^{d}}}$ where $\Qcal_{\ell, u} = \{q_w : w \in \Wcal_{\ell, u} \}$. 
	Next, we apply \Cref{lem:uniform-bounds-over-qclass} with $\Phi(x) = |x| \wedge M$, $M = u/\gamma$, $\alpha = 1/p_0$, $\beta = -1$, $\eta = 1$, noting that:
	$$
	\psi(\q{w}, \cdot) = \abs*{\frac{\q{w}(\cdot)}{p_0(\cdot)} - 1} \wedge M  = \abs*{\frac{\q{w}(\cdot)}{p_0(\cdot)} - 1}
	$$ 
	for each $w \in \Wcal_{\ell, u}$. Thus we have
	\begin{align*}
		\left|\frac{1}{n} \sum_{i=1}^n \left| \frac{\q{w}(x_i)}{{p_0}(x_i)} - 1 \right| - \| \q{w} - p_0 \|_1  - \q{w}(S^c)  \right| \leq  \frac{4c_0}{\gamma \sqrt{n} h^{d}} + \left(3 + \frac{u}{\gamma}\right)\sqrt{ \frac{2}{n} \log \frac{6}{\delta}},
	\end{align*}
	with probability at least $1-\delta/3$ for all $w \in \Wcal_{\ell, u}$ simultaneously. 
	
	
	Turning to the KL-bound, a similar chain of reasoning gives us
	\begin{align*}
		\left| \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{\hat{p}(x_i)} \log \frac{q_w(x_i)}{p_\theta(x_i)} -  \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{p_0(x_i)} \log \frac{q_w(x_i)}{p_\theta(x_i)}   \right|
		\leq \frac{\nu u}{\gamma^3} \log \frac{u}{\gamma},
	\end{align*}
	where we have additionally used the lower bound $p_\theta(\cdot) \in [\gamma, 1/\gamma]$. Next, we will invoke \Cref{lem:uniform-bounds-over-qclass} with $\alpha=1/p_\theta, \beta = 0, \eta = p_\theta/p_0$ and $\Phi (x) = k(x) \log k(x)$ where $k(x) = \min(\max(x, u/\gamma), l \gamma)$, noting that the function $\Phi$ is bounded within radius $M=\frac{u}{\gamma} \log \frac{u}{\gamma}$ and is a Lipschitz function with Lipschitz constant bounded by $L = \sup_{x \in [l\gamma, u/\gamma]} |1 + \log x| \leq 2\log \frac{u}{l \gamma}$. Then, using the bounds $C_1 \leq \tilde{C}_1 = \frac{8c_0}{\gamma^3} \log \frac{u}{l\gamma}$ and $C_2 \leq \tilde{C}_2 = \frac{3u}{\gamma^3} \log \frac{u}{\gamma}$, \Cref{lem:uniform-bounds-over-qclass} shows that with probability $1-\delta/3$,  
	$$
	\abs*{\frac{1}{n}\sum_{i=1}^n \frac{\q{w}(x_i)}{p_0(x_i)} \log \frac{\q{w}(x_i)}{p_\theta(x_i)} - \KL_S(\q{w} | p_\theta)} \leq \frac{\tilde{C_1}}{\sqrt{n} h^d} + \tilde{C_2} \sqrt{\frac{2}{n}\log \frac{6}{\delta}}
	$$
	for each $w \in \Wcal_{\ell, u}$ simultaneously, since $\psi(\q{w}, \cdot) = \frac{\q{w}(\cdot)}{p_0(\cdot)} \log \frac{\q{w}(\cdot)}{p_\theta(\cdot)}$ and $\KL_S(\q{w}|p_\theta) =$ \\ $\int \psi(\q{w}, x) p_0(x) dx$ whenever $w \in \Wcal_{\ell, u}$.
	\iffalse
	Observe that $\E_{x \sim p_0}\left[\frac{q_w(x)}{p_0(x)} \log \frac{q_w(x)}{p_\theta(x)} \right] = \KL_B(q_w, p_\theta)$. 
	By \Cref{lem:bbl-rademacher-lipschitz} we have
	\begin{align*}
		\Rad \left( \left\{ x \mapsto \frac{q_w(x)}{p_0(x)} \log \frac{p_0(x)}{p_\theta(x)} : q_w \in \Qcal_{\ell,u}\right\} \right) &\leq \left(\frac{2}{\gamma} \log \frac{1}{\gamma} \right) \Rad(\Qcal_{\ell,u}).
	\end{align*}
	Moreover, observe that for $x \geq \alpha$, the function $x \mapsto ax \log(bx)$ has Lipschitz constant $b\left(1 + \log \frac{a}{\alpha}\right)$. Thus, we additionally have
	\[ \Rad \left( \left\{ x \mapsto \frac{q_w(x)}{p_0(x)} \log \frac{q_w(x)}{p_0(x)} : q_w \in \Qcal_{\ell,u}\right\} \right) \leq \frac{1}{\gamma} \left(1 + \log \frac{1}{ \ell \gamma} \right) \Rad(\Qcal_{\ell,u}). \]
	Finally, we have the decomposition 
	\[\frac{q_w(x)}{p_0(x)} \log \frac{q_w(x)}{p_\theta(x)} = \frac{q_w(x)}{p_0(x)} \log \frac{q_w(x)}{p_0(x)} + \frac{q_w(x)}{p_0(x)} \log \frac{p_0(x)}{p_\theta(x)}.\] 
	Thus, we may apply Lemma~\ref{lem:bbl-rademacher-bounds} and conclude that with probability at least $1-3\delta$, for all $q_w \in \Qcal_{\ell,u}$ simultaneously,
	\begin{align*}
		&\hspace{-3em}\left| \KL_B(q_w | p_\theta) - \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{{p}_0(x_i)} \log \frac{q_w(x_i)}{p_\theta(x_i)} \right| \\
		&=
		\left| \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{p_0(x_i)} \log \frac{q_w(x_i)}{p_\theta(x_i)} - \E_{x \sim p_0}\left[ \frac{q_w(x)}{p_0(x)} \log \frac{q_w(x)}{p_\theta(x)} \right] \right| \\
		&\leq \left| \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{p_0(x_i)} \log \frac{q_w(x_i)}{p_0(x_i)} - \E_{x \sim p_0}\left[ \frac{q_w(x)}{p_0(x)} \log \frac{q_w(x)}{p_\theta(x)} \right] \right| \\
		&\hspace{1em} + \left| \frac{1}{n} \sum_{i=1}^n \frac{q_w(x_i)}{p_0(x_i)} \log \frac{p_0(x_i)}{p_\theta(x_i)} - \E_{x \sim p_0}\left[ \frac{q_w(x)}{p_0(x)} \log \frac{p_0(x)}{p_\theta(x)} \right] \right| \\
		&\leq \frac{1}{\gamma} \left(1 + \log \frac{1}{\ell} + 3\log \frac{1}{\gamma} \right) \Rad(\Qcal_{\ell,u}) + \frac{u}{\gamma}\left(\log \frac{u}{\ell} + 2\log \frac{1}{\gamma} \right)\sqrt{ \frac{2}{n} \log \frac{2}{\delta}} \\
		&\leq \frac{2}{\gamma} \left(1 + \log \frac{1}{\ell} + 3\log \frac{1}{\gamma} \right)\sqrt{\frac{c_0 u}{n h^{d}}} + \frac{u}{\gamma}\left(\log \frac{u}{\ell} + 2\log \frac{1}{\gamma} \right)\sqrt{ \frac{2}{n} \log \frac{2}{\delta}}.
	\end{align*}
	\fi
	Finally, we obtain the lemma statement by using the union bound and putting all the display equations together, noting that $u/\gamma \geq e$ and $\gamma \leq 1$.
\end{proof}


\subsubsection{Sup-norm approximation of kernel density estimates}
\label{sec:kernele-density-estimation-bounds}

%\jk{I may have just overlooked it, but I don't think we formally introduce what we mean by $\Delta_n^{\gamma^2/4}$, do we? It was definitely sufficiently difficult to locate in-text that it might make sense to re-state what it is here.}

Recall our notation for the truncated probability simplex $\Delta_n^\beta = \{(w_1, \ldots, w_n) \in \Delta_n : \frac{\beta}{n} \leq w_i \leq \frac{1}{n \beta}\}$. The next lemma provides approximation results for the subset of densities $\{\q{w} : w \in \Delta_n^{\gamma^2/4}\} \subseteq \Den$ that hold with arbitrary high probability when $n$ is large enough. In particular, Item 1 shows that $\hat{p}$ approximates well the convolved density $K_h \star p_0$, Item 2 provides upper and lower bounds on the density $\q{w}$, Item 3 bounds the mass of $\q{w}$ outside the set $S$, and Item 4 shows that any convolved density of the form $K_h \star q$ can be approximated well by a density of the form $\q{w}$, whenever $q(\cdot) \in [\gamma, 1/\gamma]$ is a density supported on $S$.

\begin{lemma}
	\label{lem:existence-of-good-estimator}
	Suppose that \Cref{assump:bounded-densities} and \Cref{assump:kernel-properties} hold and $\delta \in (0,1)$ is such that $n \geq \frac{2}{\gamma^4} \log \frac{12}{\delta}$. Further suppose that $q \in \Den$ is a density supported on $S$ satisfying $\gamma \leq q(x)  \leq 1/\gamma$ for each $x \in S$. If $x_1, \ldots, x_n \sim p_0$, with probability $1-\delta$, the following items hold:
	\begin{enumerate}
		\item  $\|\hat{p} - K_h \star p_0\|_{\infty} \leq \frac{6c_0}{\sqrt{n} h^d} \sqrt{\log \frac{6}{\delta}}$. %Further, $\tv(\hat{p}, K_h \star p_0) \leq \frac{6c_0 v_d  (R + 1)^d}{\sqrt{n} h^d} \sqrt{\log \frac{2}{\delta}}  + C_1 h^{d+1-\rho}{e^{-h^{-1/\rho}}}$ where $v_d$ is the volume of the unit ball in $\R^d$.
		\item  $\frac{\gamma^2}{4} \left( \gamma - \frac{6c_0}{\sqrt{n} h^d} \sqrt{\log \frac{2}{\delta}} \right)\leq \q{w}(x) \leq \frac{4}{\gamma^2} \left( \gamma^{-1} + \frac{6c_0}{\sqrt{n} h^d} \sqrt{\log \frac{2}{\delta}} \right)$ uniformly over $w \in \Delta_n^{\gamma^2/2}$ and $x \in \R^d$.
		\item  $\q{w}(S^c) \leq  \frac{4}{\gamma^2} \left( h + \frac{V_S}{\gamma}\phi(\sqrt{h}) +  \sqrt{\frac{1}{2n} \log \frac{3}{\delta}} \right)$
		uniformly over $w \in \Delta_n^{\gamma^2/2}$, whenever $h \leq h_0$, where $h_0$ is as defined in \Cref{lem:kernel-support-concentration}.
		
		\item  Denote $\smooth{q} = K_h \star q$. There is a $w \in \Delta_n^{\gamma^2/2}$ such that
		$\| \q{w} - \smooth{q} \|_{\infty} \leq
		\frac{8 c_0}{\gamma^4 h^d} \sqrt{\frac{1}{n} \log \frac{12}{\delta}}$. In particular, 
		$$
		\abs{\KL_S(\q{w}|p_\theta) - \KL_S(\smooth{q}|p_\theta)} \leq \frac{C}{ h^d \sqrt{n}} \sqrt{\log \frac{12}{\delta}
		}$$ 
		where $C = \frac{8 c_0}{\gamma^4} V_S(1 + \log(\frac{u}{\gamma l}))$ and $u$ and $l$ are the upper and lower bounds on $\q{w}$ in Item 2. We take $C = \infty$ if $l  \leq 0.$
	\end{enumerate}
\end{lemma}


\begin{proof}[Proof of \Cref{lem:existence-of-good-estimator}]
	
	Combining \Cref{lem:bbl-rademacher-bounds} with \Cref{lem:kernel-rademacher-bounds} shows that with probability $1-\delta/3$:
	\begin{align*}
		\left|\frac{1}{n} \sum_{i=1}^n K_h(x_i, x)  - \int K_h(y, x) p_0(y) dy\right|
		&\leq  \frac{4c_0}{\sqrt{n} h^d} + \frac{c_0}{\sqrt{n} h^d}\sqrt{2\log \frac{6}{\delta}} \leq \frac{6c_0}{\sqrt{n} h^d} \sqrt{\log \frac{6}{\delta}}  .
	\end{align*}
	for all $x \in \R^d$ simultaneously.
	Noting that $\hat{p}(x) = \frac{1}{n} \sum_i K_h(x_i, x)$ and $(K_h \star p_0)(x) = \int K_h(y,x) p_0(y) dy$, the uniform bound in Item 1 follows. 
	
	To show Item 2, note for each $w \in \Delta_n^{\gamma^2/4}$ that
	\[ 
	\frac{\gamma^2 \hat{p}(x)}{4} \leq \q{w}(x) = \sum_{i=1}^n w_i K_h(x_i, x) \leq \frac{4  \hat{p}(x)}{\gamma^2} \quad \text{for each } x \in \R^d. 
	\]
	Item 2 now follows from the bound in Item 1 and as $(K_h \star p_0)(\cdot) \in [\gamma, 1/\gamma]$, since $\int K_h(y,x) dy = 1$ and $p_0(\cdot) \in [\gamma, 1/\gamma]$.
	%Combined with our bound on $\frac{1}{n} \sum_{i=1}^n K_h(x_i, x)$, we obtain the upper bound in item 2 by using the fact that $\int K_h(y,x) p_0(y) dy \leq \frac{1}{\gamma} \int K_h(y,x) dy = 1/\gamma$.
	
	Now we will show Item 3. For arbitrary $w \in \Delta_n^{\gamma^2/4}$ observe that 
	$$
	\q{w}(S^c) \leq \frac{4  \hat{p}(S^c)}{\gamma^2}. 
	$$
	An application of Hoeffding's inequality to the function $f(\cdot) = \int_{S^c} K_h(\cdot, x) dx \in [0,1]$ implies that with probability $1-\delta/3$,
	\begin{align*}
		\hat{p}(S^c) = \frac{1}{n}\sum_{i=1}^n f(x_i) \, dx 
		&\leq  \int f(y) p_0(y) dy dx  +  \sqrt{\frac{1}{2n} \log \frac{3}{\delta}} \\
		&= \int_{S^c} \int K_h(y,x) p_0(y) dy dx +  \sqrt{\frac{1}{2n} \log \frac{3}{\delta}}\\  
		&\leq h + \frac{V_S}{\gamma}\phi(\sqrt{h}) + \sqrt{\frac{1}{2n} \log \frac{3}{\delta}},
	\end{align*}
	where,  the second inequality follows by invoking \Cref{lem:smooth-support-concentration} with the choice $q=p_0$ whenever $h \leq h_0$.
	
	Finally to show Item 4, we choose the weights $w_i = \frac{q(x_i)}{Zp_0(x_i)}$ for $i = 1, \ldots, n$, where $Z = \sum_{i=1}^n \frac{q(x_i)}{p_0(x_i)}$. By Hoeffding's inequality, we have that with probability at least $1-\delta/6$, 
	\[|Z - n| = |Z - \E[Z]| \leq \frac{1}{\gamma^2} \sqrt{\frac{n}{2} \log \frac{12}{\delta}} . \]
	In the event that this holds, if $n \geq \frac{2}{\gamma^4} \log \frac{12}{\delta}$, then $Z \in [n/2, 3n/2]$ and we have that $w \in \Delta_n^{\gamma^2/4}$. Moreover, it implies that for any $x \in \R^d$,
	\begin{align*}
		\left|\q{w}(x) - \frac{1}{n} \sum_{i=1}^n \frac{q(x_i)}{p_0(x_i)} K_h(x_i, x)\right| &=
		\left| \sum_{i=1}^n w_i K_h(x_i, x) - \frac{1}{n} \sum_{i=1}^n \frac{q(x_i)}{p_0(x_i)} K_h(x_i, x) \right| \\
		&\leq \left| \frac{n - Z}{Z} \right| \max_{i \in [n]} \frac{q(x_i)}{p_0(x_i)} K_h(x_i, x) \\
		&\leq \frac{2}{n}|n - Z| \frac{c_0}{\gamma^2 h^d} \leq \frac{2c_0}{\gamma^4 h^d} \sqrt{\frac{1}{n} \log \frac{12}{\delta}}.
	\end{align*}
	Next, observe that \Cref{lem:bbl-rademacher-lipschitz} and \Cref{lem:kernel-rademacher-bounds} imply that the Rademacher complexity $\Rad$ of the function class $\{ \cdot \mapsto \frac{q(\cdot)}{p_0(\cdot)} K_h(\cdot, x) \, : \, x \in \R^d \}$ on domain $S$ is bounded above by $\frac{2c_0}{\gamma^2 h^d \sqrt{n}}$. Therefore by \Cref{lem:bbl-rademacher-bounds}, with probability $1-\delta/6$,  we have
	\begin{align*}
		%&\sup_{x \in \R^d} \left|  \frac{1}{n} \sum_{i=1}^n \frac{q(x_i)}{p_0(x_i)} K_h(x_i, x) - (K_h \star q)(x) \right|= \\
		\sup_{x \in \R^d} \left|  \frac{1}{n} \sum_{i=1}^n \frac{q(x_i)}{p_0(x_i)} K_h(x_i, x) - \int_S \frac{q(y)}{p_0(y)} K_h(y,x) \, p_0(y) dy \right| \leq \frac{c_0}{\gamma^2 h^d \sqrt{n}} \left(4 +  \sqrt{2\log \frac{12}{\delta}}\right). 
	\end{align*}
	Since $p_0, q$ are both supported on $S$ with $\inf_{x \in S} p_0(x) > 0$, we have $ (K_h \star q)(x) = \int_S q(y) K_h(x,y) \, dy = \int_S \frac{q(y)}{p_0(y)} K_h(x,y) p_0(y) dy$. Hence combining the previous two display equations and using the union bound, the uniform bound in Item 4 between $\q{w}$ and $\smooth{q}$ is seen to hold with probability $1-\delta/3$. Finally to bound the differences between the KL terms, note that
	\begin{align*}
		\abs{\KL_S(\q{w}|p_\theta) - \KL_S(\smooth{q}|p_\theta)} &\leq \int_S |\q{w}(x) \log \q{w}(x) - \smooth{q}(x) \log \smooth{q}(x)| dx \\
		&\quad + \int_S |\log p_\theta(x)| |\q{w}(x) - \smooth{q}(x)| dx \\
		&\leq \|\q{w} - \smooth{q}\|_{\infty} V_S (1 + \log \frac{u}{\gamma l}),
	\end{align*}
	where the last inequality follows by using the Lipschitz continuity of the map $\Phi(x) = x \log x$ on the interval $[l, u]$, with Lipschitz constants $1 + \log(u/l)$, and using the upper bound $|\log p_\theta(\cdot)| \leq \log(1/\gamma)$.
	
	A final application of the union bound shows that Items 1 through 4 can be simultaneously satisfied with probability $1-\delta$.
\end{proof}

\subsubsection{KL and TV approximation by kernel smoothed densities}
\label{sec:okl-smoothed-approx}

 In this section we show that the terms $\KL(q|p_\theta)$ and $\tv(q,p_0)$ do not increase by much when a density $q \in \Den$ is replaced by its kernel-smoothed version $\smooth{q} = K_h \star q$ for a suitably small bandwidth parameter $h > 0$. %Such a result can be obtained by combining  \Cref{lem:info-proj-sandwich} along with   \Cref{lem:OKL-is-continuous} and \Cref{lem:smoothed-densities-kl-tv-approx} shown below. 
%We will assume that the constant $\tilde{h}$ is smaller than the constant $h_0 > 0$ which is defined as in \Cref{lem:kernel-support-concentration}.

%we bound the total-variation distance $\tv(\cdot, p_0)$ and KL-divergence $\KL(\cdot|p_\theta)$ of the smoothed density $\smooth{q}$ in terms of that of $\q$. The following is the main result of this section.

\begin{lemma}
	\label{lem:smoothed-densities-kl-tv-approx}
	Suppose \Cref{assump:bounded-support,assump:bounded-densities,assump:smooth-densities,assump:kernel-properties} hold. There exists a constant $\tilde{c} >0$ and $\tilde{h} > 0$ depending only on the constants in the above assumption, such that the following statement holds. Suppose that $q \in \Den$ is supported on $S$ and bounded above by $1/\gamma$ and $h \leq \tilde{h}$ then
	\begin{itemize}
		\item $\tv(\smooth{q}, p_0) \leq \tv(q, p_0) + \tilde{c} \left( h^{\alpha/2} + h + \phi(\sqrt{h})  \right)$
		\item $\KL_S(\smooth{q} | p_\theta) \leq \KL(q | p_\theta) +  \tilde{c} \left( h^{\alpha/2} + h \log \frac{1}{h} + \phi(\sqrt{h}) \log \frac{1}{\phi(\sqrt{h})} \right)$.
	\end{itemize}
\end{lemma}


We will prove the TV and KL statements separately. We start with the TV statement.
\begin{lemma}
	\label{lem:bounded-tv}
	Suppose that $q \in \Den$ is supported on $S$ and suppose that \Cref{assump:bounded-support,assump:bounded-densities,assump:smooth-densities,assump:kernel-properties} hold. 
	Then 
	\[\tv(\smooth{q}, p_0) \leq \tv(q, p_0) 
	+ V_S \left( C_\alpha h^{\alpha/2} + h + \frac{\phi(\sqrt{h})}{\gamma}  \right)\]
	whenever $h \in (0,h_0)$.
\end{lemma}
\begin{proof}
	We first observe that for any densities $p, q \in \Den$, we have
	\begin{align*}
		\tv(\smooth{p},\smooth{q}) 
		&= \frac{1}{2}\int_{\R^d} |\Ez [q(x-hZ) - p(x-hZ)]| \, dx\\
		&\leq \frac{1}{2} \Ez \int_{\R^d}  |q(x-hZ) - p(x-hZ)| \, dx \\
		&= \frac{1}{2} \int_{\R^d} |q(x)-p(x)| \, dx = \tv(p, q),
	\end{align*}
	where $Z$ is an $\R^d$-valued random vector with probability density $x \mapsto \kappa(\|x\|_2)$, and the inequality follows from Jensen's inequality and Fubini's theorem. By the triangle inequality, we then have
	\begin{align*}
		\tv(\smooth{q}, p_0) \leq \tv(\smooth{q}, K_h \star p_0) + \tv(K_h \star p_0, p_0) \leq \tv(q, p_0) + \tv(K_h \star p_0, p_0).
	\end{align*}
	Next, note that for any $x \in S_{-\sqrt{h}}$ we have
\begin{align*}
    &\hspace{-3em}\left| p_0(x) - K_h \star p_0(x) \right| \\
    &= %\left| p_0(x) - \int_{\R^d} p_0(y) K_h(x,y)\, dy \right| =
    \left| \int_{\R^d} p_0(x) K_h(x, y)\, dy - \int_{\R^d} p_0(y) K_h(x,y)\, dy \right| \\
    &\leq \int_{\R^d} \left| p_0(x) -  p_0(y) \right| K_h(x,y) \, dy \\
    &= \int_{B(x,\sqrt{h})} \left| p_0(x) -  p_0(y) \right| K_h(x,y) \, dy 
    + \int_{B(x,\sqrt{h})^c} \left| p_0(x) -  p_0(y) \right| K_h(x,y) \, dy \\
    &\leq C_\alpha h^{\alpha/2} + h, 
\end{align*}
	where the first inequality follows from Jensen's inequality, and the second follows from \Cref{assump:smooth-densities} and \Cref{lem:kernel-support-concentration}, using the upper bound on the density $p_0(\cdot) \leq 1/\gamma$.
	
	Thus we have
	\begin{align*}
		&\hspace{-2em} \tv(K_h \star p_0, p_0) \\
		&= \frac{1}{2}\int_{\R^d} \left| p_0(x) - K_h \star p_0(x) \right| \, dx \\
		&= \frac{1}{2}\int_{S_{-\sqrt{h}}} \left| p_0(x) - K_h \star p_0(x) \right| \, dx + \frac{1}{2}\int_{\R^d \setminus S_{-\sqrt{h}}} \left| p_0(x) - K_h \star p_0(x) \right| \, dx \\ 
		&\leq \frac{\lambda(S_{-\sqrt{h}})}{2} \left( C_\alpha h^{\alpha/2} +  h \right) + \frac{1}{2}\int_{S \setminus S_{-\sqrt{h}}} \left| p_0(x) - (K_h \star p_0)(x) \right| \, dx
		+ \frac{1}{2} (K_h \star p_0)(S^c) \\
		&\leq %\frac{V_S}{2} \left( C_\alpha h^{\alpha/2} + h \right) + \frac{V_S}{2\gamma} \phi(\sqrt{h}) + \frac{1}{2} (h + \frac{V_S}{\gamma} \phi(\sqrt{h})) \leq
		V_S (C_\alpha h^{\alpha/2} + h + \gamma^{-1}\phi(\sqrt{h}) ).
	\end{align*}
	where the first inequality follows from the last display equation and using the fact that $p_0(x) = 0$ whenever $x \in S^c$, and the second inequality follows by using that  $|p_0(\cdot) - (K_h \star p_0)(\cdot)| \leq 1/\gamma$ and $\lambda(S\setminus S_{-r}) = V_S \phi(r)$, along with the bound $(K_h \star p_0)(S^c) \leq h + \frac{V_S}{\gamma} \phi(\sqrt{h})$ from \Cref{lem:smooth-support-concentration}.
\end{proof}

We now turn to proving the KL bound. To do so, we will use the decomposition of the KL into negative entropy and cross-entropy and bound each term separately.

\begin{lemma}
	\label{lem:bounded-neg-entropy}
	Suppose Assumptions~\ref{assump:bounded-support} and~\ref{assump:kernel-properties} hold, $q \in \Den$ has support $S$ and is bounded above by $1/\gamma$. Then 
	\begin{equation*}
		\int_S \smooth{q}(x) \log \smooth{q}(x) dx \leq \int_S q(x) \log q(x) dx 
		+ D(h)\left[ \frac{d}{2} \log(2\pi e M) + \left(\frac{d}{2} + 1 \right) \log \frac{1}{D(h)} \right],
	\end{equation*}
	where $M = 2 R^2 + 2 v_d \left(c_0 t_0^{d+3} + \frac{C_\rho}{\rho} \Gamma \left( \frac{d+3}{\rho} \right)\right)$ and $D(h) = h + \frac{V_S}{\gamma} \phi(\sqrt{h})$, whenever $h \leq \min(h_0, h_1)$ where $h_1 = \inf\{h > 0 : D(h) > 1/e\}.$
\end{lemma}
\begin{proof}
	First let us note that the integrals $\int_{\R^d} q(x) \log q(x) dx$ and $\int_{\R^d} \smooth{q}(x) \log \smooth{q}(x) dx$ are well-defined. Indeed, the first integral is well defined since $q$ is bounded above by $1/\gamma$ on a set $S$ with finite Lebesgue measure. Although, $\smooth{q}$ can also be shown to be bounded above by $1/\gamma$, its support is unbounded if the kernel $K_h$ also has unbounded support. Instead, for the existence of the second integral, it suffices to show that the second moment of $\smooth{q}$ is finite (e.g.~see \cite{ghourchian2017existence}). To show the latter, we will use the property a random variable $Y$ with density $\smooth{q}$, has the same distribution as $X + hZ$, where $X$ is random variable with density $q$, and let $Z$ be a random variable with density $x \mapsto \K(\|x\|_2)$ independent of $X$. Hence
	\begin{equation}
		\E_{Y \sim \smooth{q}}[\| Y \|_2^2] 
		= \E_{Z, X}[\|X+ h Z \|^2] 
		\leq 2 R^2 + 2 h^2 v_d \left(c_0 t_0^{d+3} + \frac{C_\rho}{\rho} \Gamma \left( \frac{d+3}{\rho} \right) \right) = M 
		\label{eq:l2bound}
	\end{equation}  
	where we have used the inequality $\|x + z \|_2^2 \leq 2\|x\|_2^2 + 2\|z \|_2^2$ along with the bound on $\E \|Z\|^2$ from \Cref{lem:bounded-second-moment} and the bound $\E\|X\|^2 \leq R^2$.
	
	
	Next, let us use the convexity of the function $\Phi:\Rnn \to \R$ given by $\Phi(x)=x \log x$ to conclude that
	\begin{equation*}
		\begin{aligned}
			\int_{\R^d} \smooth{q}(y) \log \smooth{q}(y) dy 
			&= \int_{\R^d} \Phi(\Ez[q(y-hZ)])  dy
			\overset{(i)}{\leq} \int \Ez [\Phi(q(y-hZ))] dy\\
			&\overset{(ii)}{=} \Ez \int_{\R^d} \Phi(q(y-hZ)) dy \overset{(iii)}{=} \int_S \Phi(q(u)) du = \int_S q(u) \log q(u) du
		\end{aligned}
	\end{equation*}
	where we have used Jensen's inequality in Step (i) and Fubini's theorem in Step (ii). In Step (iii) we used the change of variables $u=y-hZ$ as well as the convention that $\Phi(0) = 0$.
	
	Now observe that we can decompose the negative entropy of $\smooth{q}$ as
	\begin{align*}
		\int_{S} \smooth{q}(y) \log \smooth{q}(y) dy  
		&= \int_{\R^d} \smooth{q}(y) \log \smooth{q}(y) dy + \int_{S^c} \smooth{q}(y) \log \frac{1}{\smooth{q}(y)} dy .
	\end{align*}
	Now define the conditional probability density $\bar{q}(x) = \frac{\smooth{q}(x) \I{x \in S^c}}{\smooth{q}(S^c)}$. Then we have the identity
	\begin{align*}
		\int_{S^c} \smooth{q}(y) \log \frac{1}{\smooth{q}(y)} dy = \smooth{q}(S^c) \log \frac{1}{\smooth{q}(S^c)} + \smooth{q}(S^c) \int_{\R^d} \bar{q}(y) \log \frac{1}{\bar{q}(y)} dy .
	\end{align*}
	We now claim that $\bar{q}$ has bounded second moment. To see this, observe that we can write
	\[  \E_{y \sim \smooth{q}}[\|y \|^2] = \smooth{q}(S) \E_{y \sim \smooth{q}}[\|y \|^2 | y \in S] +   \smooth{q}(S^c) \E_{y \sim \smooth{q}}[\|y \|^2 | y \in S^c].\]
	%Letting $Z$ denote the random vector with density $z \mapsto \kappa(\|z\|_2)$ and $Y$ the random vector with density $q$, \Cref{lem:bounded-second-moment} implies
	Thus, $\E_{y \sim \bar{q}}[\|y\|^2] = \E_{y \sim \smooth{q}}[\|y \|^2 | y \in S^c] \leq M/\smooth{q}(S^c)$ by \cref{eq:l2bound}. 
	
	Now in order to bound the negative entropy of $\bar{q}$, let $\mu = \E_{y \sim \bar{q}} [y]$ and $\sigma^2 = \E_{y \sim \bar{y}}[\|y-\mu\|^2] \leq M/\smooth{q}(S^c)$ denote the mean and mean squared error of $\bar{q}$, and let $g(x) = \frac{1}{(2\pi\sigma^2)^{d/2}} e^{-\frac{\|x-\mu\|^2}{2\sigma^2}}$ be the probability density function of the the multivariate normal distribution with the same first two moments. Examining the property $\KL(\bar{q}|g) \geq 0$, we obtain
	$$ 
	\int_{\R^d} \bar{q}(y) \log \frac{1}{\bar{q}(y)} dy \leq \frac{1}{2} + \frac{d}{2} \log(2 \pi \sigma^2) \leq \frac{d}{2} \log (2 \pi e M\smooth{q}(S^c)^{-1}). 
	$$
	Finally, by \Cref{lem:smooth-support-concentration}, $\smooth{q}(S^c) \leq h + \frac{V_S}{\gamma} \phi(\sqrt{h})$. Putting it all together and noting that the function $x \mapsto x \log \frac{1}{x}$ is monotonically increasing on $x \in (0,1/e]$, gives the lemma statement.
\end{proof}

\begin{lemma} Suppose that $q \in \Den$ is supported on $S$ and bounded above by $1/\gamma$. Further suppose that \Cref{assump:bounded-support,assump:bounded-densities,assump:smooth-densities,assump:kernel-properties} hold. Then
	\label{lem:bounded-cross-entropy}
	% $$
	% \left| \int_B \smooth{q}(x) \log p_\theta(x) -  \int_B q(x) \log p_\theta(x) \right| \leq CM_\alpha h^\alpha + |\log \gamma| C_d h^{(\rho-1)/2} e^{-h^{-\rho/2}} + \frac{|\log \gamma|\lambda(B)}{\gamma} (\phi(\sqrt{h}))
	% $$ 
	% where $M_\alpha = \int_{\R^d} \kappa(\|z\|) \|z\|^{\alpha} dx < \infty$ and $C_d$ is the constant expression from \Cref{lem:exp-tail-bound}.
	\[ \left| \int_S \smooth{q}(x) \log p_\theta(x) dx -  \int_S q(x) \log p_\theta(x) dx \right| \leq  C_\alpha h^{\alpha/2} +  2h  \log \frac{1}{\gamma} 
	+\frac{V_S \phi(\sqrt{h})}{\gamma} \log \frac{1}{\gamma}
	\]
	whenever $h \leq h_0$.
\end{lemma}
\begin{proof}
	Using Fubini's theorem, we obtain 
	$$
	\int_S \smooth{q}(x) \log p_\theta(x) dx = \int_{\R^d} q(y) l(y) dy = \int_S q(y) l(y) dy
	$$
	where $l(y) = \int_S K_h(x,y) \log p_\theta(x) dx$ and last equality follows since $q$ is supported on $S$.
	For any $y \in S_{-\sqrt{h}}$ (recall this means that $B(y,\sqrt{h}) \subseteq S$) and hence we have:
	\begin{align*}
		\abs{\log p_\theta(y) - l(y)} &= \abs*{\int_{\R^d} \log p_\theta(y) K_h(x,y) dx - \int_S \log p_\theta(x)  K_h(x,y) dx } \\
		&\leq \int_{B(y,\sqrt{h})}  K_h(x,y) |\log p_\theta(y)- \log p_\theta(x)| \, dx \\
		&+ \int_{B(y,\sqrt{h})^c} K_h(x,y) \left(|\log p_\theta(y)| + |\log p_\theta(x)|\right) \, dx \\
		&\leq   C_\alpha h^{\alpha/2} +  2h  \log \frac{1}{\gamma},
	\end{align*}
	where we have used \Cref{assump:bounded-densities,assump:smooth-densities} and \Cref{lem:kernel-support-concentration} in the last step.
	
	\iffalse
	Next using the random variable notation in \Cref{lem:exp-tail-bound}, note that $A_h(y) = \Pr( y + hZ \in B)$. In particular  \Cref{lem:exp-tail-bound} shows that there is a constant $C_d$
	$$
	1-A_h(y) = \Pr( y + hZ \notin B) \leq \Pr\left(\|Z\| \geq \frac{r_y}{h}\right) \leq C_d (h/r_y)^{(\rho-1)}e^{-(r_y/h)^\rho}
	$$
	whenever $r_y \geq t_0 h$, 
	where $r_y = \inf_{x \in B^c} \|x-y\|$. In particular, whenever $r \geq t_0 h$, the bound
	$$
	1-A_h(y) \leq C_d (h/r)^{(\rho-1)}e^{-(r/h)^\rho}
	$$
	holds for each $y \in B_r \doteq \{y \in B| r_y \geq r\}$. 
	This shows
	\begin{align*}
		\left| \int_B q(y) \psi(y) -  \int_B q(y) \log p_\theta(y) \right| dy \leq \int_B q(y) \left|\psi(y) -  \log p_\theta(y) \right| dy \\
		\leq Ch^\alpha M_\alpha + |\log \gamma|C_d (h/r)^{(\rho-1)} e^{-(r/h)^\rho}  + |\log \gamma| \int_{B \setminus B_r} q(y) dy.
	\end{align*}
	Finally, using the notation in \Cref{assump:bounded-support}, we have $\phi(r) = 1 - \frac{\lambda(B_r)}{\lambda(B)}$, which provides the bound $\int_{B \setminus B_r} q(y) dy \leq \frac{\lambda(B)}{\gamma} \phi(r)$. Now take $r=\sqrt{h}$ to complete the proof.
	\fi
	
	Thus we have
	\begin{align*}
		\left| \int_S q(y) l(y) \, dy -  \int_S q(y) \log p_\theta(y) \, dy \right| 
		&\leq \int_S q(y) \left|  l(y)  - \log p_\theta(y) \right| \, dy \\
		&=  \int_{S_{-\sqrt{h}}} q(y) \left|  l(y)  - \log p_\theta(y) \right| \, dy \\
		&+ \int_{S\setminus S_{-\sqrt{h}}} q(y) \left|  l(y)  - \log p_\theta(y) \right| \, dy \\
		&\leq  C_\alpha h^{\alpha/2} +  2h  \log \frac{1}{\gamma} 
		+\frac{V_S \phi(\sqrt{h})}{\gamma} \log \frac{1}{\gamma} . \qedhere
	\end{align*}
\end{proof}

\begin{proof}[Proof of \Cref{lem:smoothed-densities-kl-tv-approx}]
	The TV statement follows immediately from \Cref{lem:bounded-tv}. To see the KL statement, we first observe that
	\[ \KL_S(\smooth{q}, p_\theta) = \int_S \smooth{q}(x) \log \frac{\smooth{q}(x)}{p_\theta(x)} \, dx = \int_S \smooth{q}(x) \log \smooth{q}(x) \, dx - \int_S \smooth{q}(x) \log p_\theta(x)\, dx.  \]
	Plugging in \Cref{lem:bounded-neg-entropy} for the first term and \Cref{lem:bounded-cross-entropy} for the second term completes the proof.
\end{proof}

%\subsection{Continuity of the optimistic KL}


\subsection{Proof of \Cref{thm:okl-convergence-formal}}
\label{sec:proof-of-theorem}

We now put all of the above together to prove \Cref{thm:okl-convergence-formal}. We suppose \Cref{assump:bounded-densities,assump:bounded-support,assump:kernel-properties,assump:smooth-densities} hold and thus obtain the following corollaries of   \Cref{lem:uniform-convergence,lem:existence-of-good-estimator,lem:smoothed-densities-kl-tv-approx}  when we take 
$\delta_{n} = e^{- (\log n)^{2\beta}}$ 
% $\delta_{n} = n^{-2\beta}$
and $\eta_{n,h} = \frac{(\log n)^\beta}{\sqrt{n} h^d}$ for a fixed constant $\beta > 0$ (e.g.~take $\beta=1$). In the following, $n_0, \bar{h}, \bar{\eta} > 0$ are constant quantities that can depend on any of the terms used in the assumptions and on $\beta > 0$, but are independent of $n$ and $h$. Given two expressions $f$ and $g$, the notation $f = O(g)$ will be used to denote that  $|f| \leq c |g|$ holds for a similar constant $c$. We will delay instantiating a concrete value of $\epsilon'$ in \Cref{cor:existence-of-good-weights} until we start the final proof, but note for now that the constants $c, n_0, \bar{h}, \bar{\eta}$ do not depend on the choice of $\epsilon'$ in any way.

%\jk{Can we say something about what the below convergence results imply about how the kernel bandwidth should be chosen as a function of $n$? Ideally, we could just say that the normal optimal rates in the literature would be fine to plug in here.}


%We will use the adjective ``suitably small'' or ``suitably large'' to refer to constant values  that depend only on the assumption of \Cref{thm:okl-convergence-formal}. 
%We will use $\bar{\delta}, \bar{h}$ to denote suitably small positive constants and  $c, n_0$ to denote suitably large finite constants, which will depend only on the assumption of \Cref{thm:okl-convergence-formal}. Explicit values of these constants can be inferred from the statement of the lemmas used below, but we will avoid stating explicit values for the ease of exposition. 

\begin{corollary}
\label{cor:existence-of-good-weights}
Suppose a fixed value $\epsilon' > 0$ is given. Then whenever $n \geq n_0, h \leq \bar{h}$ and $\eta_{n,h} \leq \bar{\eta}$, the following holds with probability at least $1-\delta_{n}$:
\begin{enumerate}
    \item $\tv(\hat{p}, p_0) =  O\left(\eta_{n,h} + h^{\min(\frac{\alpha}{2},1)} + \phi(\sqrt{h})\right)$.
    \item $\Delta_n^{\gamma^2/4} \subseteq \Wcal_{\frac{\gamma^3}{8}, \frac{8}{\gamma^3}}$, where $\Wcal_{\ell, u}$ is as defined in \Cref{lem:uniform-convergence}.
    \item $\sup_{w \in \Delta_n^{\gamma^2/2}} \q{w}(S^c) = O(h + \phi(\sqrt{h}))$.
    \item There is $w^{\epsilon'} \in \Delta_n^{\gamma^2/4}$ such that
    $$
    \KL_S(\q{w^{\epsilon'}}|p_\theta) \leq \okl[\epsilon'] + O \left( \eta_{n,h}  + h^{\frac{\alpha}{2}} + h \log \frac{1}{h} + \phi(\sqrt{h}) \log \frac{1}{\phi(\sqrt{h})}\right)
    $$
    and
    $$
    \tv(\q{w^{\epsilon'}}, p_0) \leq {\epsilon'} + O \left( \eta_{n,h}  +  h^{\min(\frac{\alpha}{2},1)} + \phi(\sqrt{h}) \right).
    $$
\end{enumerate}	
\end{corollary}

\begin{proof}
	Observe by \Cref{lem:info-proj-sandwich} that the information projection $\iproj[{\epsilon'}]$ is sandwiched between $p_0$ and $p_\theta$; in particular, $\iproj[{\epsilon'}]$ has support $S$ and is bounded between values $\gamma$ and $1/\gamma$ on $S$. Thus we will invoke \Cref{lem:existence-of-good-estimator} with $q = \iproj[\epsilon']$ and $\delta = \delta_n$, noting that the condition $n \geq \frac{2}{\gamma^2} \log \frac{12}{\delta}$ is satisfied when $n_0$ is suitably large. %Henceforth, by the union bound, suppose that conclusions of \Cref{lem:existence-of-good-estimator} are satisfied for $q = \iproj[{\epsilon'}]$ and $q = \iproj[\epsilon_2]$ with probability $1-\delta_n$. 
	Hence we may now prove the respective items.
	
	
	We show Item 1 as follows:
	$$
	\tv(\hat{p}, p_0) \leq \tv(\hat{p}, K_h \star p_0) + \tv(K_h \star p_0, p_0) = O(\eta_{n,h} + h) + O(h^{\min(\frac{\alpha}{2},1)} + \phi(\sqrt{h})),
	$$
	where the term $\tv(\hat{p}, K_h \star p_0)$ was bounded by combining Item 1 of \Cref{lem:existence-of-good-estimator} with \Cref{cor:tv-uniform-estimates}, and the term $\tv(K_h \star p_0, p_0)$ was bounded by using  \Cref{lem:smoothed-densities-kl-tv-approx} with $q = p_0$ (assuming suitable choice of constants $\bar{h}$ and $c$).
	
	Next, Item 2 follows from Item 2 of \Cref{lem:existence-of-good-estimator} as long as $\frac{6c_0}{\sqrt{n}h^d} \sqrt{\log \frac{4}{\delta_n}} = O(\eta_{n,h}) \leq \gamma/2$ is satisfied, which holds when $\bar{\eta}$ is a suitably small constant.
	
	Next, Item 3 follows from Item 3 of \Cref{lem:existence-of-good-estimator} noting that $\sqrt{\frac{1}{2n} \log \frac{6}{\delta_n}} = O( \eta_{n,h} h^d ) = O( \bar{\eta} h)$.
	
	Finally to show Item 4, we will invoke Item 4 from \Cref{lem:existence-of-good-estimator} for the choice $q=\iproj[{\epsilon'}]$. Hence we obtain a $w^{\epsilon'} \in \Delta_{n}^{\gamma^2/4}$ such that $\|\q{w^{\epsilon'}}-\iproj[{\epsilon'}]\|_\infty = O(\eta_{n,h})$ and 
	$$
	|\KL_S(\q{w^{\epsilon'}}|p_\theta) - \KL_S(K_h \star \iproj[{\epsilon'}]|p_\theta)|  =  O(\eta_{n,h}).
	$$
	Further, \Cref{cor:tv-uniform-estimates} shows
	$$
	\tv(\q{w^{\epsilon'}}, K_h \star \iproj[{\epsilon'}]) \leq O(\eta_{n,h} + h)
	$$
	since $\mu = \sum_{i=1}^n w^{\epsilon'}_i \delta_{x_i}$ and the measure $\nu$ given by density $\iproj[{\epsilon'}]$, are both supported on $S$. The bounds in Item 4 now follow using \Cref{lem:smoothed-densities-kl-tv-approx} with $q=\iproj[{\epsilon'}]$ and the triangle inequality, noting that $\KL(\iproj[{\epsilon'}]|p_\theta) = \okl[{\epsilon'}]$ and $\tv(\iproj[{\epsilon'}], p_0) \leq \epsilon'$.
\end{proof}


Since our goal is to approximate $\okl$ by $\hatI(\theta)$, let us rewrite \cref{eqn:I-hat-defn} as 
$$
\hatI(\theta) = \inf_{\substack{w \in \Delta_n^{\gamma^2/4} \\ \hatTV(\q{w}, p_0) \leq \epsilon}} \hatKL(\q{w}|p_\theta)
$$
where, given $q \in \Den$,
$$
\hatKL(q|p_\theta) = \frac{1}{n} \sum_{i=1}^n \frac{q(x_i)}{\hat{p}_\gamma(x_i)} \log \frac{q(x_i)}{p_\theta(x_i)}
$$
and
$$
\hatTV(q,p_0) = \frac{1}{2n} \sum_{i=1}^n \abs*{\frac{q(x_i)}{\hat{p}_\gamma(x_i)} - 1}.
$$

Next, the following corollary of \Cref{lem:uniform-convergence}  shows that, with high probability, the estimators $\hatKL(\q{w}|p_\theta)$ and $\hatTV(\q{w}, p_0)$ are close to their population level targets $\KL_S(\q{w}|p_\theta)$ and $\tv(\q{w}, p_0)$, uniformly over all $w \in \Delta_n^{\gamma^2/4}$.   

\begin{corollary}
	\label{cor:all-the-bounds}
	Suppose $n \geq n_0$, $h \leq \bar{h}$ and $\eta_{n,h} \leq \bar{\eta}$, then with probability at least $1-2\delta_n$ the event in \Cref{cor:existence-of-good-weights} holds, and further uniformly over all $w \in \Delta_{n}^{\gamma^2/4}$ it holds that
	$$
	\abs*{\hatKL(\q{w}|p_\theta) - \KL_S(\q{w}|p_\theta)} \leq \psi_{n,h} \quad \text{and } \abs*{\hatTV(\q{w}, p_0) - \tv(\q{w}, p_0)} \leq \psi_{n,h}
	$$
	for a deterministic quantity $\psi_{n,h}$ that satisfies $\psi_{n,h} = O \left(\eta_{n,h} + h^{\min(\frac{\alpha}{2},1)} + \phi(\sqrt{h})\right)$.
\end{corollary}
\begin{proof} Let $E_1$ denote the event in \Cref{cor:existence-of-good-weights}, and let $E_2$ denote the event in \Cref{lem:uniform-convergence} with the choices  $\delta = \delta_n$, $l = \frac{\gamma^3}{8}$, $u = \frac{8}{\gamma^2}$. We will assume that both events $E_1$ and $E_2$ hold simultaneously, which happens with probability at least $1-2\delta_n$.
	
	Recall that on the event $E_1$, we have $\Delta_{n}^{\gamma^2/4} \subseteq \Wcal_{\ell, u}$, $\sup_{w \in \Delta_n^{\gamma^2/4}} \q{w}(S^c) = O(h + \phi(\sqrt{h}))$, are  $\tv(\hat{p}, p_0) = O\left(\eta_{n,h} + h^{\min(\frac{\alpha}{2}, 1)} + \phi(\sqrt{h})\right)$. The proof can be completed by combining the above with the bounds from \Cref{lem:uniform-convergence} under the event $E_2$. 
\end{proof}


For a suitable constant $c$, the term $\xi_{n,h} = c\left(\eta_{n,h} + h^{\frac{\alpha}{2}} + h \log \frac{1}{h} + \phi(\sqrt{h}) \log \frac{1}{\phi(\sqrt{h})}\right)$ dominates all the error terms (i.e.~the terms of the form $O(\cdot)$) in the statement of \Cref{cor:existence-of-good-weights} and \Cref{cor:all-the-bounds}. Using this term, we are ready to finish the proof of \Cref{thm:okl-convergence-formal}.

\begin{proof}[Proof of \Cref{thm:okl-convergence-formal}]
We will invoke \Cref{cor:all-the-bounds} with the choice $\epsilon' = \epsilon - 2 \xi_{n,h}$ (see \Cref{cor:existence-of-good-weights}). We first claim that the following holds:
\begin{align}
\label{eqn:okl-upper-and-lower-bound}
\okl[\epsilon + 2\xi_{n,h}] - O(\xi_{n,h}) \leq \hatI(\theta) \leq \okl[\epsilon - 2\xi_{n,h}] + O(\xi_{n,h})
\end{align}
with probability at least $1-2\delta_n$.
	
For the upper bound, let $w^{\epsilon'} \in \Delta_n^{\gamma^2/4}$ be the vector from \Cref{cor:existence-of-good-weights,cor:all-the-bounds} and note that
$$
\hatTV(\q{w}, p_0) \leq \tv(\q{w}, p_0) + \xi_{n,h} \leq \epsilon ' + 2\xi_{n,h} \leq \epsilon,
$$
and hence 
$$
\hatI(\theta) \leq \hatKL(\q{w}|p_\theta) \leq \KL_S(\q{w}|p_\theta) + \xi_{n,h} \leq \okl[\epsilon'] + 2\xi_{n,h}.
$$

To lower bound $\hatI(\theta)$, let $w \in \Delta_{n}^{\gamma^2/4}$ be such that $\hatTV(\q{w}, p_0) \leq \epsilon$. Then by  \Cref{cor:existence-of-good-weights}, \Cref{cor:all-the-bounds}, and \Cref{lem:restrict-to-S}, there is a $\bar{q}_w$ that is supported on $S$ such that
$$
\tv(\bar{q}_w, p_0) \leq \tv(\q{w}, p_0) + \q{w}(S^c) \leq \hatTV(\q{w}, p_0) + 2\xi_{n,h} \leq \epsilon + 2\xi_{n,h}
$$
and 
$$
\begin{aligned}
    \hatKL(\q{w}|p_\theta) &\geq \KL_S(\q{w}|p_\theta) - \xi_{n,h}\\
     &= (1-\q{w}(S^c))  \KL(\bar{q}_w|p_\theta) + (1-\q{w}(S^c))\log (1-\q{w}(S^c)) - \xi_{n,h} \\
    &\geq (1-\xi_{n,h}) \okl[\epsilon + 2\xi_{n,h}] - 2\xi_{n,h}
\end{aligned}
$$
where we have used that $\q{w}(S^c) \leq \xi_{n,h}$, $h(x) = (1-x) \log (1-x) \geq -x$ by the convexity of $h$, and $\KL(\bar{q}_w|p_\theta) \geq \okl[\epsilon + 2\xi_{n,h}]$ since $\tv(\bar{q}_w, p_0) \leq \epsilon + 2\xi_{n,h}$. Since the right hand side of the previous display doesn't depend on the choice of $w$, by considering the infimum over all $w \in \Delta_n^{\gamma^2/4}$ such that $\hatTV(\q{w}, p_0) \leq \epsilon$ we see
$$
\begin{aligned}
    \hatI(\theta) &\geq \okl[\epsilon + 2\xi_{n,h}] - \xi_{n,h} (1+\okl[\epsilon + 2\xi_{n,h}]) \geq \okl[\epsilon + 2\xi_{n,h}] - \xi_{n,h} (1+ \KL(p_0|p_\theta)).
\end{aligned}
$$
Note that $\KL(p_0|p_\theta) \leq   2\log(1/\gamma)$ by H\"older's inequality, and we have thus shown the lower bound and established \cref{eqn:okl-upper-and-lower-bound}.

Finally, we appeal to the continuity of the OKL function with respect to the coarsening radius (i.e., \Cref{lem:okl-continuity}) to see that
\[ I_{\epsilon - 2\xi_{n,h}}(\theta) \leq I_{\epsilon}(\theta) + \frac{2\xi_{n,h}}{\epsilon} \KL(p_0 | p_\theta) \leq I_{\epsilon}(\theta) + \frac{4 \xi_{n,h}}{\epsilon} \log(1/\gamma) = I_{\epsilon}(\theta) + O(\xi_{n,h}) \]
and
\[ I_{\epsilon + 2\xi_{n,h}}(\theta) \geq I_{\epsilon}(\theta) - \frac{2\xi_{n,h}}{\epsilon + 2\xi_{n,h}} \KL(p_0 | p_\theta) \leq I_{\epsilon}(\theta) - \frac{4 \xi_{n,h}}{\epsilon} \log(1/\gamma) = I_{\epsilon}(\theta) - O(\xi_{n,h}), \]
where we have used \Cref{assump:bounded-densities} and H\"{o}lder's inequality to establish $\KL(p_0|p_\theta) \leq 2 \log(1/\gamma)$. Combining the above two inequalities with \Cref{eqn:okl-upper-and-lower-bound} completes the proof.
\end{proof}

 \iffalse
We obtain the following corollary relating $\hatI(\theta)$ to $\okl[\tilde{\epsilon}]$ for  values of $\tilde{\epsilon}$ close to $\epsilon$. 

\begin{lemma} Suppose $n \geq n_0$, $h \leq \bar{h}$ and $\eta_{n,h} \leq \bar{\eta}$, then with probability $1-2\delta_n$
	$$
	\okl[\epsilon + 2\xi_{n,h}] - O(\xi_{n,h}) \leq \hatI(\theta) \leq \okl[\epsilon - 2\xi_{n,h}] + O(\xi_{n,h})
	$$
	\label{lem:okl-bounds}
\end{lemma}
\begin{proof} 
We will invoke \Cref{cor:all-the-bounds} with the choice $\epsilon' = \epsilon - 2 \xi_{n,h}$ (see \Cref{cor:existence-of-good-weights}). 
	
	Let us first show the upper bound. For the vector $w^{\epsilon'} \in \Delta_n^{\gamma^2/4}$ from \Cref{cor:existence-of-good-weights} and \Cref{cor:all-the-bounds}, note that
	$$
	\hatTV(\q{w}, p_0) \leq \tv(\q{w}, p_0) + \xi_{n,h} \leq \epsilon ' + 2\xi_{n,h} \leq \epsilon
	$$
	and hence 
	$$
	\hatI(\theta) \leq \hatKL(\q{w}|p_\theta) \leq \KL_S(\q{w}|p_\theta) + \xi_{n,h} \leq \okl[\epsilon'] + 2\xi_{n,h}.
	$$
	
	To lower bound $\hatI(\theta)$, let $w \in \Delta_{n}^{\gamma^2/4}$ be such that $\hatTV(\q{w}, p_0) \leq \epsilon$. Then by  \Cref{cor:existence-of-good-weights}, \Cref{cor:all-the-bounds}, and \Cref{lem:restrict-to-S}, there is a $\bar{q}_w$ that is supported on $S$ such that
	$$
	\tv(\bar{q}_w, p_0) \leq \tv(\q{w}, p_0) + \q{w}(S^c) \leq \hatTV(\q{w}, p_0) + 2\xi_{n,h} \leq \epsilon + 2\xi_{n,h}
	$$
	and 
	$$
	\begin{aligned}
		\hatKL(\q{w}|p_\theta) &\geq \KL_S(\q{w}|p_\theta) - \xi_{n,h}\\
		 &= (1-\q{w}(S^c))  \KL(\bar{q}_w|p_\theta) + (1-\q{w}(S^c))\log (1-\q{w}(S^c)) - \xi_{n,h} \\
		&\geq (1-\xi_{n,h}) \okl[\epsilon + 2\xi_{n,h}] - 2\xi_{n,h}
	\end{aligned}
	$$
	where we have used that $\q{w}(S^c) \leq \xi_{n,h}$, $h(x) = (1-x) \log (1-x) \geq -x$ by the convexity of $h$, and $\KL(\bar{q}_w|p_\theta) \geq \okl[\epsilon + 2\xi_{n,h}]$ since $\tv(\bar{q}_w, p_0) \leq \epsilon + 2\xi_{n,h}$. Since the right hand side of the previous display doesn't depend on the choice of $w$, by considering the infimum over all $w \in \Delta_n^{\gamma^2/4}$ such that $\hatTV(\q{w}, p_0) \leq \epsilon$ we see
	$$
	\begin{aligned}
		\hatI(\theta) &\geq \okl[\epsilon + 2\xi_{n,h}] - \xi_{n,h} (1+\okl[\epsilon + 2\xi_{n,h}]) \geq \okl[\epsilon + 2\xi_{n,h}] - \xi_{n,h} (1+ \KL(p_0|p_\theta)).
	\end{aligned}
	$$
	Note that $\KL(p_0|p_\theta) \leq  \frac{2V_S}{\gamma} \log(1/\gamma)$, and we have thus shown the lower bound.
\end{proof}


Combining \Cref{lem:okl-bounds} with the following lemma showing continuity of the $t \mapsto \okl[t]$ at $t=\epsilon > 0$, we finally see that $\abs*{\okl - \hatI(\theta)} = O(\xi_{n,h})$ holds under the conditions of \Cref{lem:okl-bounds}.

\begin{lemma}
	\label{lem:OKL-is-continuous}
	Under \Cref{assump:bounded-support} and \Cref{assump:bounded-densities}, for any $\epsilon' \geq 0$ and $t > 0$
	$$
	0 \leq I_{\epsilon'}(\theta) - I_{\epsilon'+t}(\theta) \leq \frac{2tV_S}{\gamma(\epsilon' + t)}\log(1/\gamma).
	$$
\end{lemma}
\begin{proof} Fix $\epsilon' \geq 0$ and $t > 0$. Suppose $\iproj[\epsilon'+t]$ is the I-projection of $p_\theta$ onto the ball $B_{\epsilon' + t}(p_0) \doteq \{q \in \Den \, | \, \tv(q, p_0) \leq \epsilon' +t\}$. 
	%
	Since $\iproj[\epsilon+t] \in B_{\epsilon'+t}(p_0)$, we have $\lambda_t \iproj[\epsilon+t] + (1-\lambda_t) p_0 \in B_\epsilon(p_0)$ for $\lambda_t = \epsilon'/(\epsilon'+t)$.
	Hence using convexity of the KL divergence:
	\begin{align*}
		I_{\epsilon}(\theta) \leq \KL(\lambda_t \iproj[\epsilon'+t] + (1-\lambda_t) p_0|p_\theta) &\leq \lambda_t\KL(\iproj[\epsilon'+t]|p_\theta) + (1-\lambda_t) \KL(p_0|p_\theta)  \\
		&= \lambda_t I_{\epsilon'+t}(\theta) + \frac{t}{\epsilon' + t}  \KL(p_0 | p_\theta).
	\end{align*}
	The proof is completed by rearranging the terms and noting that $I_{\epsilon'}(\theta) \geq I_{\epsilon'+t}(\theta) \geq 0$, $\lambda_t \leq 1$, and $\KL(p_0|p_\theta) \leq  \frac{2V_S}{\gamma} \log(1/\gamma)$.
\end{proof}

\fi