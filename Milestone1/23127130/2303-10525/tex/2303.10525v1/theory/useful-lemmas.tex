%auto-ignore

This section covers some technical lemmas about the OKL function that will be used in the theoretical results in the remainder of the appendix. When showing the asymptotic connection between coarsened likelihood and OKL (\Cref{sec:coarsened-likelihood-asymptotics}), we will allow for more general distances than the total variation (TV) distance. %\jk{which parts? Maybe we could be  explicit here about which arguments rely on the TV and which ones don't and give a brief summary?}, 
%
To this end, let $\D(P, Q)$ denote a general distance between probability measures $P, Q \in \cP(\cX)$. We define the OKL function for a general distance $\D$ by
\begin{align}
	\label{eqn:okl-general-distance}
	I_\epsilon(\theta) = \inf_{\substack{Q \in  \cP(\cX) \\ \D(Q, P_0) \leq \epsilon}} \KL(Q | P_\theta). 
\end{align}

Although such extended analysis with  general distances might be possible, for simplicity, we will restrict to the case of $\D=\tv$ while proving the sandwiching property of the I-projection (\Cref{sec:sandwiching}) and the consistency of the OKL estimator in continuous spaces (\Cref{sec:okl-estimation-cont}).

\subsection{Continuity of OKL in the coarsening radius}

The following lemma shows when we can expect the OKL function to be continuous in $\epsilon$.
\begin{lemma}
\label{lem:okl-continuity}
Let $0 \leq \epsilon_0 < \epsilon$ and $\alpha > 0$. Suppose the function $Q \mapsto \D(Q, P_0)$ is convex, then
\[ 0 \leq I_{\epsilon}(\theta) - I_{\epsilon+\alpha}(\theta) \leq \frac{\alpha}{\epsilon - \epsilon_0 + \alpha} I_{\epsilon_0}(\theta). \]
If we additionally have $\alpha \leq \epsilon - \epsilon_0$, then 
\[ 0 \leq I_{\epsilon-\alpha}(\theta) - I_{\epsilon}(\theta) \leq \frac{\alpha}{\epsilon - \epsilon_0} I_{\epsilon_0}(\theta). \]
\end{lemma}
\begin{proof}
We will prove the first statement, as the second follows identically.
First observe that we always have $I_{t}(\theta) - I_{t'}(\theta) \geq 0$ for all $t' \geq t \geq 0$. Moreover, if $I_{ \epsilon_0}(\theta)$ is infinite, then the above holds trivially. Thus, we may assume that $I_{\epsilon_0}(\theta) < \infty$. 

Pick $\delta > 0$. By the definition of OKL, for any $r>0$ there exists $Q_{r} \in \cP(\cX)$ such that $\D(Q_{r}, P_0) \leq r$ and 
\[ \KL(Q_{r} | P_\theta) \leq I_{r}(\theta) + \delta. \]
Take $Q = (1-\lambda)Q_{\epsilon + \alpha} + \lambda Q_{\epsilon_0} \in \cP(\cX)$ for $\lambda = \frac{\alpha}{\epsilon - \epsilon_0 + \alpha}$. By the convexity of $\D$, we have
\[ \D(Q, P_0) \leq (1-\lambda) \D(Q_{\epsilon + \alpha}, P_0) + \lambda \D (Q_{\epsilon_0}, P_0) \leq (1-\lambda)(\epsilon + \alpha) + \lambda \epsilon_0 \leq \epsilon.  \]
Moreover, by the convexity of KL divergence (see e.g.~\cite[Lemma~2.4]{budhiraja2019analysis})
\begin{align*}
I_{\epsilon}(\theta) 
\leq \KL(Q | P_\theta)
&\leq (1-\lambda)\KL(Q_{\epsilon + \alpha} | P_\theta) + \lambda \KL( Q_{\epsilon_0} | P_\theta) \\
&\leq (1-\lambda) (I_{\epsilon + \alpha}(\theta) + \delta) + \lambda (I_{\epsilon_0}(\theta) + \delta) \\
&\leq I_{\epsilon + \alpha}(\theta) + \frac{\alpha}{\epsilon - \epsilon_0 + \alpha} I_{\epsilon_0}(\theta) + \delta.
\end{align*}
Rearranging and noting that $\delta > 0$ was chosen arbitrarily, gives us the result in the lemma. The second statement is derived in an identical manner by substituting $\epsilon - \alpha$ in place of $\epsilon$.
\end{proof}


\subsection{Sandwiching property of I-projections}
\label{sec:sandwiching}

In parts of this appendix, we will be dealing with cases where $\Xcal \subseteq \R^d$ and $P_0$ and $P_\theta$ have corresponding densities $p_0, p_\theta \in \Den$ with respect to the Lebesgue measure $\lambda$. 
%
%\jk{Does this continue to hold if $\Xcal$ is bounded? just checking because our truncated estimator [that we use for theory] only works on bounded sets; so if we need the projection to be unique in that setting we may have to assume it separately.}
%
In this setting, if $I_\epsilon(\theta) < \infty$, then by \cite{csiszar1975divergence} there is a ($\lambda$-almost everywhere) unique density  $\iproj \in \Den$ that we will call the information ($I$-)projection such that $\tv(\iproj, p_0) \leq \epsilon$ and $\KL(\iproj|p_\theta) = I_\epsilon(\theta)$. We will show that $\iproj$ satisfies the following \emph{sandwiching} property relative to $p_0$ and $p_\theta$ for any value of $\epsilon > 0$. 

\begin{definition}
For probability vectors $p, q, r \in \Delta_n$, we say that $r$ is \emph{sandwiched} between $p$ and $q$ if $\min(p_i, q_i) \leq r_i \leq \max(p_i, q_i)$ for all $i=1,\ldots, n$. Similarly, if $p, q, r \in \Den$ are probability densities, then we say that $r$ is sandwiched between $p$ and $q$ if the condition  $\min(p(x),q(x)) \leq r(x) \leq \max(p(x),q(x))$ holds for $\lambda$-almost every $x$.
\end{definition}

The following proposition will be important in proving the sandwiching property for the I-projection.
\begin{proposition}
\label{prop:sandwich-kl-tv}
For probability vectors (or densities), if $r$ is sandwiched between $p$ and $q$, then $\tv(r,p) \leq \tv(q,p)$ and $\KL(r|p) \leq \KL(q|p)$.
\end{proposition}

In fact, will prove the above result for any $\phi$-divergence $D_\phi(p,q) = \int \phi(p/q) q d\lambda$ when $\phi$ is a convex function $\phi(1)=0$. The total variation distance ($\phi(x) = |x - 1|$) and KL-divergence ($\phi(x) = x \log x$) will emerge as special cases.

\begin{lemma}
\label{lem:sandwich-phi-div}
Let $\phi: \R \to (-\infty, \infty]$ be a proper convex function with $\phi(1) = 0$. If a density $r$ is sandwiched between densities $p$ and $q$, then $D_\phi(r,q) \leq D_\phi(p,q)$.
\end{lemma}
\begin{proof}
The sandwiching property implies that there is a function $t: \cX \to [0,1]$ such that $r = (1-t) p + t q$.  Hence	
\begin{align*}
    D_\phi(r,q) &= \int \phi((1-t) p/q + t q/q ) q d\lambda \leq \int (1-t) \phi(p/q) q d\lambda + \int t \phi(1) q d\lambda \\
    &= D_\phi(p,q) - \int t \phi(p/q) q d\lambda \leq D_\phi(p,q) - \xi\int t q(p/q - 1) d\lambda = D_\phi(p,q). 
\end{align*}
where the two inequalities follow from the convexity of $\phi$ noting that there is $\xi \in \mathbb{R}$ (called the sub-gradient) such that 
$\phi(x) \geq \phi(1) + \xi(x-1) = \xi(x-1)$ for all $x \in \R$, and the last equality holds since $\int t(p-q) d\lambda = 0$, since $p$ and $r$ are assumed to integrate to one.
\end{proof}
% We also have the following, stronger result for total variation distance.

% \begin{lemma}
%     Let $p, q, r$ denote three probability densities (or vectors). If $\int$
% \end{lemma}

%\subsection{Information projections are bounded}

Now, we will need a simple lemma about total variation distance before we can prove the sandwiching property of the I-projection. For brevity, we will use notations like $\{p \leq q\}$ and $\{q > \max(p_0, p_\theta)\}$ to denote the sets $\{x \in \cX : p(x) \leq q(x)\}$ and $\{x \in \cX: q(x) > \max(p_0(x), p_\theta(x))\}$ respectively. Note that for two densities $p, q \in \Den$, the total variation distance can be expressed as $\tv(p,q) = \int_{p > q} (p-q) d\lambda = \int_{q > p} (q-p) d\lambda = \frac{1}{2} \int |p-q| d\lambda$.


%\jk{In the below theorem, why is $q(x)<r(x)$ strict? It seems like the proof would still work with an inequality ($q(x) \leq r(x)$). Similarly for the other direction.}
\begin{lemma}
\label{lem:tv-transform}
Let $p,q,r \in \Den$. %If either $\{r \leq q\} \subseteq \{p \leq r\}$ or $\{r \geq q\} \subseteq \{p \geq r\}$ then $\tv(p,r) \leq \tv(p, q)$.
If $\{r < q\} \subseteq \{p \leq r\}$ or $\{r > q\} \subseteq \{p \geq r\}$ then $\tv(p,r) \leq \tv(p,q)$.  %Similarly, if $p(x) \geq q(x)$ for all $x$ satisfying $q(x) > r(x)$, then $\tv(q,p) \leq \tv(r,p)$.
\end{lemma}
\begin{proof} 
	Suppose $\{r < q\} \subseteq \{p \leq r\}$, then
	$$
	\tv(p,r) = \int_{\{p > r\}} (p-r) d\lambda \leq \int_{\{p > r\}} (p-q) d\lambda \leq \int_{\{p > q\}} (p-q) d\lambda = \tv(p,q)
	$$
	where the two inequalities follow from the inclusion  $\{p > r\} \subseteq \{r \geq q\} \cap \{p > q\}$.
	Similarly if $\{r > q\} \subseteq \{p \geq r\}$ then, 
	$$
	\tv(p,r) = \int_{\{ p < r\}} (r-p) d\lambda \leq \int_{\{p < r\}} (q-p) d\lambda \leq \int_{\{p < q\}} (q-p) d\lambda = \tv(p,q)
	$$
	since $\{p < r\} \subseteq \{r \leq q\} \cap \{p < q\}$.	
\iffalse
Suppose first that $\{q < r\} \leq \{p \leq q\}$. The case $\{q < r\} \subseteq \{p \geq q\}$ follows symmetrically. Let $S^+ = \{q < r \}$, $S^- = \{ q > r \}$, and $S^= = \{ q = r \}$. Then we have
\begin{align*}
    \int |p(x) - r(x)| \, dx &= \int_{S^+} |p(x) - r(x)| \, dx + \int_{S^-} |p(x) - r(x)| \, dx + \int_{S^=} |p(x) - r(x)| \, dx \\
    &= \int_{S^+} (r(x) - q(x) + q(x) - p(x)) \, dx + \int_{S^-} |p(x)         - r(x)| \, dx \\
    &+ \int_{S^=} |p(x) - q(x)| \, dx \\
    &\geq \int_{S^+} r(x) - q(x) + q(x) - p(x) \, dx + \int_{S^-} (|p(x) - q(x)| - |q(x) - r(x)|) \, dx \\
    &+ \int_{S^=} |p(x) - q(x)| \, dx \\
    &= \int |p(x) - q(x)| \, dx,
\end{align*}
where the inequality follows from the reverse triangle-inequality, and the last line follows from the fact that 
\[ \int_{S^+} (r(x) - q(x)) \, dx = \tv(q, r) = \int_{S^-} (q(x) - r(x)) dx . \qedhere \]
\fi
\end{proof}


\begin{lemma}
\label{lem:info-proj-sandwich}
Let $p_0, p_\theta$ be probability densities satisfying $I_\epsilon(\theta) < \infty$, and let $\iproj$ denote the I-projection of $p_\theta$ onto the set $\{ q \in \Den : \tv(q, p_0) \leq \epsilon \}$.
Then $\iproj$ is sandwiched between $p_0$ and $p_\theta$.
\end{lemma}

%\jk{In the below proof, I don't really understand why we emphasise that $\tv(\bar{q},p_0) \leq \tv(q, p_0)$ and $\KL(\bar{q}|p_\theta) \leq \KL(q|p_\theta)$. If I follow things correctly, this doesn't help us to prove the sandwiching (instead, it's a result of the sandwiching). Is there a reason we emphasise it? Do we use this somewhere else? If so, maybe we should make it part of the actual statement of the lemma.}
\begin{proof}
	
It suffices to show that for any density $q \in \Den$, there is a density $\bar{q} \in \Den$ sandwiched between $p_0$ and $p_\theta$ such that $\tv(\bar{q},p_0) \leq \tv(q, p_0)$ and $\KL(\bar{q}|p_\theta) \leq \KL(q|p_\theta)$. We can then complete the proof by  noting that $\bar{q} = \iproj$ if we set $q = \iproj$. 

Let $q \in \Den$ be given, and suppose that the set $S^+ = \{ q  > \max(p_0,p_\theta) \}$ has non-zero Lebesgue measure. Letting  
\[ v = \int_{S^+} (q - \max(p_0, p_\theta)) \, d\lambda, \]
note that
\[  \tv(q, p_\theta)  = \int_{\{q > p_\theta \}} (q - p_\theta)  \, d\lambda 
\geq v . \]
Define the density
\[ \bar{q}(x) = 
\begin{cases} 
    \max(p_0(x), p_\theta(x)) & \text{ if } x \in S^+ \\   
    q(x) + \frac{v}{\tv(q, p_\theta)}(p_\theta(x) - q(x)) & \text{ if } p_\theta(x) > q(x) \\
    q(x) & \text{ otherwise }
\end{cases}. \]   
Then it is not hard to verify that $\bar{q}$ integrates to one and satisfies $\bar{q}(x) \leq \max(p_0(x), p_\theta(x))$ everywhere. Next,
applying \Cref{lem:tv-transform}
with $p=p_0$, $q=q$ and $r=\bar{q}$, we obtain $\tv(\bar{q}, p_0) \leq \tv(q, p_0)$ since $\{\bar{q} < q\} = S^+ \subseteq \{p_0 \leq \bar{q} \}$ holds.
Additionally, $\bar{q}$ is sandwiched between $p_\theta$ and $q$. Next, \Cref{prop:sandwich-kl-tv} implies that $\KL(\bar{q} | p_\theta) \leq \KL(q | p_\theta)$.



Now let $q \in \Den$ such that $S^+$ is empty but the set $S^- = \{ q < \min(p_0, p_\theta) \}$ is non-empty. Letting 
\[ v = \int_{S^-} (\min(p_0, p_\theta) - q) \, d\lambda, \]
note that $0 < v \leq \tv(q,p_\theta)$, and define the density
\[ \bar{q}(x) = 
\begin{cases} 
    \min(p_0(x), p_\theta(x)) & \text{ if } x \in S^- \\   
    q(x) + \frac{v}{\tv(q, p_\theta)}(p_\theta(x) - q(x)) & \text{ if } p_\theta(x) < q(x) \\
    q(x) & \text{ otherwise }
\end{cases}. \]  
Then observe that $\bar{q}$ is a density and it is sandwiched between $q$ and $p_\theta$. Similar arguments as above using \Cref{lem:tv-transform} and \Cref{prop:sandwich-kl-tv} show that $\tv(\bar{q}, p_0) \leq \tv(q, p_0)$ and $\KL(\bar{q} | p_\theta) \leq \KL(q | p_\theta)$.
\end{proof}