%auto-ignore
Suppose $\cX$ is a finite space. We will use the notation from \Cref{sec:asymp-finite}; in particular, recall the probability simplex $\Delta_{\cX} = \{q \in [0,1]^{\cX} | \sum_{x \in X} q(x) = 1\}$ and the OKL function $I_\epsilon(\theta)$ in terms of a general distance $\D$ on $\Delta_{\cX}$ as in \cref{eqn:okl-general-distance}. We begin by showing an elementary rounding lemma, which will be useful when applying Sanov's theorem in \Cref{lem:finite-sanov}.

%\jk{Given that $q_n$ below need not denote an estimator for the data-generating process, it may be confusing to write it as $q_n$ (Maybe also not; I guess it's clear enough from the lemma description.)}

\begin{lemma}
\label{lem:rounded-probability-vector}
Let $p, p_\theta \in \Delta_\Xcal$ such that $\KL(p|p_\theta) < \infty$. For any integer $n > |\Xcal|$, there exists a $q_n \in \Delta_\Xcal$ such that $n q_n(x)$ is integral for all $x \in \Xcal$, $\|q_n - p \|_1 \leq \frac{2|\Xcal|}{n}$, and 
\[ \KL(q_n | p_\theta) \leq \left(1+ \frac{|\Xcal|^2}{n}\right)\KL(p|p_\theta) + \frac{|\Xcal|}{n} \left( 2 \log \frac{n}{2} + \log |\Xcal| + \frac{|\Xcal|}{e} \right).\]
\end{lemma}
\begin{proof}
Choose an arbitrary $x^\star = \argmax_{x \in \cX} p(x)$, choosing arbitrarily if there are ties. Then we define $q_n \in \Delta_\Xcal$ as follows. For all $x \neq x^\star$, let $q_n(x) = \frac{1}{n} \lfloor n q_n(x) \rfloor$, and let $q_n(x^\star) = 1 - \sum_{x\neq x^\star}q_n(x)$. As $p \in \Delta_\Xcal$, we have $q_n \in \Delta_\Xcal$. Moreover, by construction we also have that $n q_n(x)$ is integral for all $x \in \Xcal$. 

Observe that for all $x \neq x^\star$, we have $0 \leq p(x) - q_n(x) \leq 1/n$. This implies that \[ 0 \leq q_n(x^\star) - p(x^\star) = \left(1 - \sum_{x \neq x^\star}q_n(x)\right) - \left(1-\sum_{x \neq x^\star}p(x)\right) \leq \frac{|\Xcal|}{n}.\]
Together, these statements give us that $\| p - q_n \|_1 \leq \frac{2|\Xcal|}{n}$. Further since $\KL(p|p_\theta) < \infty$, we must also have $\KL(q_n|p_\theta) < \infty$ since $\supp(q_n) \subseteq \supp(p) \subseteq \supp(p_\theta)$.  

To prove the more detailed KL bound, we first observe that by known bounds on the entropy function \citep[c.f. Theorem 17.3.3]{cover2006elements}, we have
\begin{align*}
    \sum_{x} q_n(x) \log q_n(x) 
    &\leq \sum_{x} p(x) \log p(x) + \| p - q_n \|_1 \log \frac{|\Xcal|}{\| p - q_n \|_1} \\
    &\leq \sum_{x} p(x) \log p(x) + \frac{2|\Xcal|}{n} \log \frac{n}{2}.
\end{align*} 
Next observe that we can bound the cross-entropy between $q_n$ and $p_\theta$ as follows.
\begin{align*}
\sum_x q_n(x) \log \frac{1}{p_\theta(x)} 
&= \sum_{x \neq x^\star} q_n(x) \log \frac{1}{p_\theta(x)} + q_n(x^\star) \log \frac{1}{p_\theta(x^\star)} \\
&\leq \sum_{x \neq x^\star} p(x) \log \frac{1}{p_\theta(x)} + (p(x^\star) + q_n(x^\star) - p(x^\star)) \log \frac{1}{p_\theta(x^\star)}\\
&\leq \sum_x p(x) \log \frac{1}{p_\theta(x)} + \frac{|\Xcal|}{n} \log \frac{1}{p_\theta(x^\star)}.
\end{align*}
Now observe that $p(x^\star)\geq 1/|\Xcal|$. This implies that $\log \frac{1}{p_\theta(x^\star)} \leq |\Xcal| (\KL(p | p_\theta) + 1/e) + \log |\Xcal|$. To see this, first note that if $p_\theta(x^\star) \geq 1/|\Xcal|$, then the claim is trivial. Thus, we may assume $p_\theta(x^\star) < 1/|\Xcal| \leq p(x^\star)$. The log-sum inequality implies that
\begin{align*}
\KL(p | p_\theta) 
&\geq p(x^\star) \log \frac{p(x^\star)}{p_\theta(x^\star)} + (1-p(x^\star)) \log \frac{1 - p(x^\star)}{1 - p_\theta(x^\star)} \\
&\geq p(x^\star) \log \frac{p(x^\star)}{p_\theta(x^\star)} - (1-p(x^\star)) \log \frac{1}{1 - p(x^\star)} \\
&\geq p(x^\star) \log \frac{p(x^\star)}{p_\theta(x^\star)} - \frac{1}{e} \geq \frac{1}{|\Xcal|} \log \frac{1}{|\Xcal| p_\theta(x^\star)} - \frac{1}{e}.
\end{align*}
Here, the last inequality follows from our bound on $p(x^\star)$ combined with the fact that $a\log \frac{a}{b}$ is an increasing function in $a$ for $a \geq b$. Rearranging the above gives us the claim.
\end{proof}

Our analysis of the coarsened likelihood will rely heavily on the following result, which is essentially a consequence of Sanov's theorem.

\begin{lemma}
\label{lem:finite-sanov} 
Suppose \Cref{assum:finite-continuous-distance,assum:finite-okl} hold, and let $r > \epsilon_0$ and $n \geq \frac{4C |\Xcal|}{r - \epsilon_0}$. Then 
\[  \left| \frac{1}{n} \log \prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq r\right) + I_r(\theta) \right| \leq 
\frac{|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{2C}{r - \epsilon_0} \right) \]
 where the probability operation $\prob_\theta$ is taken over random points $Z_1, \ldots, Z_n \in \cX$ drawn from the distribution $p_\theta \in \Delta_{\cX}$, and $\EmpDist{Z_{1:n}} \in \Delta_{\cX}$ is the empirical distribution of the data points $Z_1, \ldots, Z_n$.
\end{lemma}
\begin{proof}
Observe that $I_r(\theta) \leq V < \infty$ for all $r > \epsilon_0$. Sanov's theorem~\cite[Theorem~11.4.1]{cover2006elements} implies that
\[ \frac{1}{n} \log \prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq r\right) \leq - I_r(\theta) + \frac{|\Xcal|}{n}\log(n+1). \]
Thus, we only need to show the lower bound for $\frac{1}{n} \log \prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq r\right) + I_r(\theta)$. 
Pick $\delta > 0$ and for any $t > 0$, let $q_t \in \Delta_\Xcal$ satisfy $\D(q_t, p_0) \leq t$ and
\[ \KL(q_t | p_\theta) \leq I_t(\theta) + \delta. \]
Now let $\alpha_n = \frac{2C |\Xcal|}{n}$ and observe that for $n > \frac{2 C |\Xcal|}{r-\epsilon_0}$, such a $q_{r - \alpha_n} \in \Delta_{\cX}$ exists. Letting $q^{(n)}_{r - \alpha_n} \in \Delta_\Xcal$ be the discretization promised by \Cref{lem:rounded-probability-vector} and utilizing \Cref{assum:finite-continuous-distance}, we have
\begin{align*}
\D(q^{(n)}_{r - \alpha_n}, p_0) 
\leq \D(q^{(n)}_{r - \alpha_n}, q_{r - \alpha_n}) + \D(q_{r - \alpha_n}, p_0) 
\leq C \|q^{(n)}_{r - \alpha_n} - q_{r - \alpha_n}\|_1 + r - \alpha_n \leq r.
\end{align*}
Thus we have
\begin{align*}
&\hspace{-3em}\frac{1}{n} \log \prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq r \right) \geq \frac{1}{n} \log \prob_\theta\left( \EmpDist{Z_{1:n}} = q_{r-\alpha_n}^{(n)}  \right)\\
&\geq -\frac{|\Xcal|}{n} \log (n+1) - \KL( q^{(n)}_{r - \alpha_n} | p_\theta) \\
&\geq - \left(1+ \frac{|\Xcal|^2}{n}\right)(I_{r - \alpha_n}(\theta)+\delta) - \frac{|\Xcal|}{n} \left( 3 \log(n+1) + \log |\Xcal| + \frac{|\Xcal|}{e} \right) \\
&\geq - I_{r - \alpha_n}(\theta) - \delta - \frac{|\Xcal|}{n}\left( 3 \log(n+1) + \log |\Xcal| + |\Xcal| \left( V + \frac{1}{e} + \delta \right) \right) \\
&\geq -I_r(\theta) - \delta - \frac{\alpha_n V}{r - \epsilon_0} - \frac{|\Xcal|}{n}\left( 3 \log(n+1) + \log |\Xcal| + |\Xcal| \left( V + \frac{1}{e} + \delta \right) \right),
\end{align*}
where the second line follows from~\cite[Theorem~11.1.4]{cover2006elements}, the third line follows from \Cref{lem:rounded-probability-vector}, and the last line follows from \Cref{lem:okl-continuity}. Rearranging the above and utilizing the fact that $\delta > 0$ was arbitrary gives us the lemma statement.
\end{proof}

With \Cref{lem:finite-sanov} in hand, we can prove the following convergence result for the coarsened likelihood.
\begin{theorem}
    \label{lem:finite-coarsened-likelihood-convergence}
    Suppose \Cref{assum:finite-continuous-distance,assum:finite-okl} hold, and let $\epsilon > \epsilon_0$. If $x_1,\ldots,x_n \sim p_0$, then with probability at least $1-\delta$, 
    \begin{align*}
    \left| \frac{1}{n} \log L_\epsilon(\theta|x_{1:n} ) + I_{\epsilon}(\theta)  \right| &\leq  \frac{C V |\Xcal|}{\epsilon - \epsilon_0} \sqrt{\frac{2}{n} \log \frac{2|\Xcal|}{\delta}} \\
    &\hspace{3em} + \frac{3|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{4C}{\epsilon - \epsilon_0} \right).   
    \end{align*}
    whenever $n > \max \left\{2 \left( \frac{C |\Xcal|}{\epsilon - \epsilon_0} \right)^2 \log \frac{2|\Xcal|}{\delta}, \frac{8C |\Xcal|}{\epsilon - \epsilon_0} \right\}$.
\end{theorem}
\begin{proof}
For $r > 0$, define $M_{n,r}(\theta)= \prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p^0) \leq r\right)$. By \Cref{lem:finite-sanov}, 
\begin{equation}
\label{eqn:intermediate-coarsened-okl-approx}
    \left| \frac{1}{n}\log M_{n,r}(\theta) + I_r(\theta) \right| \leq  \frac{|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{2C}{r - \epsilon_0} \right)
\end{equation}
for all $r > \epsilon_0$ satisfying that $n \geq \frac{4C |\Xcal|}{r - \epsilon_0}$.

Now suppose that $x_1, \ldots, x_n \sim p_0$. Then Hoeffding's inequality combined with \Cref{assum:finite-continuous-distance} implies that with probability at least $1-\delta$,
\[ \D(\EmpDist{x_{1:n}}, p_0) \leq C \|\EmpDist{x_{1:n}} - p_0 \|_1 \leq  C|\Xcal| \sqrt{\frac{1}{2n} \log \frac{2|\Xcal|}{\delta}} =: \alpha_n.  \]
Let us condition on this event occurring. For $n > 2\left( \frac{C |\Xcal|}{\epsilon - \epsilon_0}\right)^2 \log \frac{2|\Xcal|}{\delta}$, we have $\alpha_n < (\epsilon - \epsilon_0)/2$. Thus, we may write
\begin{align*}
\left| \log M_{n,\epsilon}(\theta) -  \log L_\epsilon(\theta | x_{1:n} )\right| 
&= \left| \log \prob_\theta\left( \D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon \right) - \log \prob_\theta\left( \D(\EmpDist{Z_{1:n}}, \EmpDist{x_{1:n}}) \leq \epsilon \right) \right| \\
&= \log \max \left\{ \frac{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon\right)}{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, \EmpDist{x_{1:n}}) \leq \epsilon\right) },  \frac{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, \EmpDist{x_{1:n}}) \leq \epsilon\right)}{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon\right)} \right\} \\
&\leq \log \max \left\{  \frac{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon\right)}{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon - \alpha_n\right)},  \frac{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon + \alpha_n\right) }{\prob_\theta\left(\D(\EmpDist{Z_{1:n}}, p_0) \leq \epsilon\right)} \right\} \\
&= \log \max \left\{ \frac{M_{n,\epsilon}(\theta)}{M_{n,\epsilon-\alpha_n}(\theta)},  \frac{M_{n,\epsilon + \alpha_n}(\theta)}{M_{n,\epsilon}(\theta)} \right\}
\end{align*}
By \cref{eqn:intermediate-coarsened-okl-approx} and \Cref{lem:okl-continuity}, we have
\begin{align*}
&\frac{1}{n}\log\frac{ M_{n,\epsilon}(\theta)}{M_{n,\epsilon - \alpha_n}(\theta)} \\
&\leq I_{\epsilon}(\theta) - I_{\epsilon - \alpha_n}(\theta) + \frac{2|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{2C}{\epsilon - \alpha_n - \epsilon_0} \right) \\
&\leq \frac{2\alpha_n}{\epsilon - \epsilon_0}V + \frac{2|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{4C}{\epsilon - \epsilon_0} \right).
\end{align*}
Similarly, we also have
\begin{align*}
&\frac{1}{n}\log \frac{M_{n,\epsilon+\alpha_n}(\theta)}{M_{n,\epsilon}(\theta)} \\
&\leq I_{\epsilon + \alpha_n}(\theta) - I_{\epsilon}(\theta) + \frac{2|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{2C}{\epsilon + \alpha_n - \epsilon_0} \right) \\
&\leq \frac{\alpha_n}{\epsilon - \epsilon_0 + \alpha_n}V + \frac{2|\Xcal|}{n} \left( 3 \log(n+1) + \log|\Xcal| + |\Xcal|\left(V + \frac{1}{e} \right) + \frac{4C}{\epsilon - \epsilon_0} \right).
\end{align*}
Putting it all together gives us the theorem statement.
\end{proof}
