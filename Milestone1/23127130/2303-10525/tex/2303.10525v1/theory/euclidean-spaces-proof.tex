%auto-ignore

In this section, we establish the connection between the \emph{OWL Likelihood} and the \emph{coarsened likelihood} when $\cX$ is an infinite space. Although some of the results in this section can be applied more generally when $\cX$ is a complete separable metric space (see the proofs in Appendix \ref{sec:ldapprox-proofs}), here we will assume that $\cX \subseteq \R^d$ and that the distributions $P_0$ and $P_\theta$ have sufficiently smooth (assumption \ref{ass:ptheta_continuity}) densities $p_0, p_\theta \in \Den$ with respect to some measure $\lam$ (e.g. Lebesgue measure) on $\cX$, where $\Den \doteq \{q: \cX \to \Rnn| \int_{\cX} q d\lam = 1\}$. We also assume that $\D$ is a metric on $\cP(\cX)$ that satisfies certain conditions (assumptions \ref{def:estimating-error}, \ref{ass:dist-and-p0} ), and verify that these conditions are satisfied for kernel-based total variation distances (Definition \ref{def:smoothedtvd}). 

\paragraph{Notation}
Greek letters $\mu, \nu \in \cP(\cX)$ will denote measures on $\cX$. An integral of the form $\int f d\mu$ will be assumed to be over the space $\cX$; For example, if $\lam$ is the Lebesgue measure, then $\int f d\lam = \int_{\cX} f(x) dx$. If measures $\mu, \nu \in \cP(\cX)$ have densities $p, q \in \Den$, then we will slightly abuse notation and use $\D(p,q)$ instead of  $\D(\mu, \nu)$. 

As a starting point, similar to \eqref{eq:popvaroptfinite} \jk{We currently do not differentiate finite data space vs continuous data space versions of $I_\epsilon$; but perhaps we should? E.g., for the finite data space case, we do not need any of the kernel construction.}, we define the population version of the limiting variational optimization problem. 

\begin{definition} Define the \emph{population variational optimization problem}:
\begin{equation}
\label{eq:popvaropt}
I_\epsilon(\theta) \doteq \inf_{\substack{q \in \Den \\ \D(q, p_0) \leq \epsilon}} \KL(q|p_\theta)
\end{equation}
where $\KL(q|p_\theta) = \int q(x) \log \frac{q(x)}{p_\theta(x)} d\lam(x)$. 
\end{definition}

As in the finite case, the population variational optimization problem gives the first order behavior of the scaled coarsened log-likelihood. This result is proved in Appendix \ref{sec:ldapprox-proofs}.

\begin{theorem} If $x_{1:n} \iid P_0$ and some additional assumptions are satisfied, then as $n \to \infty$ 
\begin{equation*}
\frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) = I_\epsilon(\theta) + O_p\left(\frac{1}{\sqrt{n}}\right).
\end{equation*}
\end{theorem}

Our aim in the remainder of this section will be to approximate \eqref{eq:popvaropt} based on the observed data $x_{1:n}$. For this, we first show (Appendix \ref{sec:smoothapprox}) that under suitable smoothness assumptions on $p_\theta$ and $p_0$, we can approximate the optimization problem in \eqref{eq:popvaropt} by restricting our search to densities $q$ that are smoothed (convolved) with a suitable kernel $\K$. We then approximate this variational problem over smoothed densities by a finite dimensional variational optimization problem on the $n$-dimensional simplex, representing the convolution of the kernel $\K$ and reweightings of the observed data points $x_{1:n}$. This finite dimensional optimization problem has the form   of the LW log-likelihood that appears in Definition \ref{def:lwlikelihood}.
%


%\subsubsection{Discretization based on the observed data}

%In this section, as $n \to \infty$, we would like to show that the LW-likelihood \eqref{eq:lwlike} will approximate the variational optimization problem in Equation \ref{eq:popvaropt} for a suitable choice of kernel $\K$. The intuition behind the asymptotic  equivalence is that, under suitable smoothness assumptions on $p_\theta$ and $p_0$, we can restrict the optimization problem in \eqref{eq:popvaropt} to kernel density estimates  $q_w(\cdot) \doteq \sum_{i=1}^n w_i \K(x_i, \cdot)$ of the data re-weighted using the weight vector $w=(w_1, \ldots, w_n)$ from a (data dependent) subset $\hat{\Delta}_n$ of the $n$-dimensional probability simplex. To better interpret these weights, we will rewrite \eqref{eq:popvaropt} in terms of re-weightings of the data density $p_0$.  

%Let $\RDen{P_0} = \{ w : \cX \to [0,\infty)| \int w p_0 d\lam = 1 \}$ denote the collection of relative densities with respect to $P_0$.
%
%\jk{This notation isn't very clear. Which space is $w$ restricted to? It seems that is should be absolutely continuous wrt $\lambda$ for this object to represent what we want it to represent?}

%Now the intuition behind the discretization step is the following. Although $p_0$ is unknown,  we observe data $x_1, \ldots, x_n \iid p_0$. Hence if $\hat{p}_0$ is a suitable estimator for $p_0$, for large $n$, we may approximate $I$ by using Monte Carlo averages:
%\begin{equation*}
%   \hat{I}'_\epsilon(\theta)  = \inf_{\substack{w \in \RDen[h]{P_0} \\ \D(w \hat{p}_0, \hat{p}) \leq \epsilon}} \frac{1}{n}\sum_{i=1}^n w(x_i) \log \frac{w(x_i) \hat{p}(x_i)}{p_\theta(x_i)}.
%\end{equation*}
%based on a subclass of ``nice''  densities $\RDen[h]{P_0} \subseteq \RDen{P_0}$ that become dense in $\RDen{P_0}$ as $h \to 0$.

%Although the objective is essentially determined by the  the collection of values $v_i = \frac{w(x_i)}{n}$, the optimization problem still needs to be computed over a potentially infinite dimensional space $\RDen[h]{P_0}$. However, if the distance $\D(w \hat{p}, \hat{p})$ can be approximated by $\Dn(v, e) \approx \D(wp_0,p_0)$ based on the collection $v=(v_1, \ldots, v_n)$ then, for large values of $n$, we expect the following finite dimensional optimization to approximate $I_\epsilon(\theta)$:
%\begin{equation*}
%   \hat{I}_\epsilon(\theta)  = \inf_{\substack{v \in \hat{\Delta}_n \\ \Dn(v, e) \leq \epsilon}} \sum_{i=1}^n v_i \log \frac{v_i n\hat{p}(x_i)}{p_\theta(x_i)},
%\end{equation*}
%where $\hat{\Delta}_n \doteq \{(w(x_1), \ldots, w(x_n))/n| w \in \RDen[h]{P_0}\}$. Note that the inclusion $\hat{\Delta}_n \subseteq \Delta_n$ approximately holds due to the constraint $\frac{1}{n}\sum_{i=1}^n w(x_i) \approx \int w p_0 d\lam = 1$ and $w \geq 0$ for any $w \in \RDen{P_0}$.
 

\subsection{Proof details for TVD}

Let us now formalize the above arguments in special case when $\D$ is the total variation distance.

\begin{assume} Suppose for every $h > 0$ the kernel $\K_h: \cX \times \cX \to [0,\infty)$ is a positive definite kernel. Further suppose that $\K_h$ represents a stochastic matrix, i.e. $\int \K_h(x, y) d\lam(y) = 1$ for each $x \in \cX$. 
\label{ass:kernel}
\end{assume}

\begin{example} When $\cX = \R^d$ and $\lam$ is the Lebesgue measure, the Gaussian kernel $\K_h(x,y) = \frac{1}{h^d} \phi_d((x-y)/h)$ satisfies Assumption \ref{ass:kernel} for each bandwidth parameter $h > 0$; where $\phi_d(x) = \frac{1}{(2\pi)^{d/2}} \exp(-\|x\|^2/2)$.
\end{example}

%For any bounded function $f: \cX \to \R$ consider the  smoothening $\smooth{f} = \K_h \star f$, where recall the convolution operator $(\K \star f)(x) \doteq \int \K(x, y) f(x) d\lam(y)$

We consider the ``nice'' class of densities to be 
\begin{equation*}
\RDen[h]{P_0} = \left\{ \frac{w}{\int w p_0 d\lam} \bigg| w : \cX \to [h,1/h], \|w\|_{\K_h} \leq 1 \right\}.
\end{equation*}

Next, we use uniform convergence results with respect to the data distribution $P_0$ for suitably smooth functions.
Recall (see e.g. \cite[Definition 4.5]{wainwright2019high})
that a collection $\Fc$ of integrable real-valued functions on $\cX$ is called a Glivenco-Cantelli class for a probability distribution $\bP$ if, as $n \to \infty$,  $\|\bP_n - \bP\|_{\Fc} \pconv 0$ where 
\begin{equation*}
\|\bP_n - \bP\|_{\Fc} \doteq 
\sup_{f \in \Fc}\left|\int f(x) d\bP_n(x) - \int f(x) d\bP(x)\right| 
\end{equation*}
where $X_{1:n} \iid \bP$ and $\bP_n \doteq n^{-1}\sum_{i=1}^n \delta_{X_i}$. In Appendix \ref{sec:ulln} we show that the   function class $\Fc_h$ is a  Glivenco-Cantelli by controlling its  Radamacher complexity \cite{bartlett2002rademacher}. 

\begin{lemma} For each $h > 0$, the collection of functions 
$$
\Fc_{h} \doteq \left\{ x \mapsto w(x) \log \frac{w(x)p_0(x)}{p_\theta(x)} < \infty \bigg|  w \in \RDen[h]{P_0} \right\} \cup  \left\{ x \mapsto |w(x) - 1| \mid w \in \RDen[h]{P_0} \right\}
$$
is a Glivenko-Cantelli class for $\bP = P_0$. %\jk{I think it makes sense to give at least an intuitive recap of GC-classes for the reader before making this assumption/to discuss what this means after the assumption.}
\end{lemma}

We further assume that as $h \to 0$ we are be able to approximate the population variational optimization problem $I_\epsilon(\theta)$.  

\begin{assume} 
\label{ass:ratefunc-approximation}
Suppose that
\begin{equation*}
   I_\epsilon(\theta, h) \doteq \inf_{\substack{w \in \RDen[h]{P_0} \\ \D(wp_0, p_0) \leq \epsilon - h}}  \KL(w p_0|p_\theta)
\end{equation*}
converges to $I_\epsilon(\theta)$ as $h \to 0$. %Further suppose that the map $r \mapsto I_r(\theta)$ is  continuous at $r=\epsilon$.
\end{assume}

%\begin{proposition} Under Assumption \ref{ass:ratefunctionproperites}
%\end{proposition}

\begin{assume}(Existence of uniform log-density estimator) %Suppose $ m \leq p_0 \leq 1/m$ for some $m > 0$. 
Suppose $\hat{p}_0$ is an estimator based on $x_1, \ldots, x_n$ such that $\|\log \hat{p}_0 - \log p_0\|_\infty \pconv 0$ as $n \to \infty$.
\label{ass:density-estimator}
\end{assume}
\begin{assume} Suppose $\D(f,g) = \frac{1}{2} \int |f(x)-g(x)| d\lam(x)$ is the total variation distance. 
\label{ass:use-tvd}
\end{assume}

\begin{lemma} Suppose that the assumptions in this section are satisfied. Then for each $\eta > 0$, there  $N_0$ such that for each $n \geq N_0$ there is a $h > 0$ such that
\begin{equation}
   |I_\epsilon(\theta) - \hat{I}_\epsilon(\theta, h)| \leq \eta 
\end{equation}
with probability at least $1-\exp(-n f(\eta))$. Where

\begin{equation*}
   \hat{I}_\epsilon(\theta, h) \doteq \inf_{\substack{v \in \hat{\Delta}_{h,n}\\ \|v - e\|_1 \leq 2\epsilon}} \sum_{i=1}^n v_i \log \frac{v_i n \hat{p}_0(x_i)}{p_\theta(x_i)}.
\end{equation*}
with $\hat{\Delta}_{n,h} \doteq \{(w(x_1), \ldots, w(x_n))/(\sum_{i=1}^n w(x_i))| w \in \RDen[h]{P_0}\} \subseteq \Delta_n$.
\end{lemma}
\begin{proof}[Proof Sketch] For any $h > 0$, we can  approximate $I_\epsilon(\theta,h)$ with $\hat{I}_\epsilon(\theta,h)$ using uniform Glivenco-Cantelli results. Namely with high probability  it holds that
$$
\int w \log \frac{w p_0}{p_\theta} p_0 d\lam \approx \frac{1}{n} \sum_{i=1}^n w(x_i) \log \frac{w(x_i) p_0(x_i)}{p_\theta(x_i)} \approx \frac{1}{2n} \sum_{i=1}^n w(x_i) \log \frac{w(x_i) \hat{p}_0(x_i)}{p_\theta(x_i)} 
$$
$$
\D(wp_0, p_0) = (1/2) \int |wp_0 - p_0| d\lam \approx \frac{1}{n}\sum_{i=1}^n |w(x_i) - 1|
$$
$$
\frac{1}{n}\sum_{i=1}^n w(x_i) \approx \int wp_0 d\lam = 1
$$
for every $w \in \RDen[h]{P_0}$. In addition, Assumption \ref{ass:ratefunc-approximation} says that we may take $h$ suitable 
\end{proof}
