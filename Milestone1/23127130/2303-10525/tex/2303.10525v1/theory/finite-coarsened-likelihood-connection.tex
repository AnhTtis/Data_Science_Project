%auto-ignore
\begin{proof}[Proof of Lemma \ref{lem:lwrewritefinite}]
Given observations $x_{1:n} \in \cX^n$, we can partition the index set $[n]$ into a disjoint collection of subsets $\{J_x\}_{x \in X}$, where $J_{x} \doteq \{i \in [n]| x_i = x\}$ denotes the set of indices that realize the value $x \in X$;  further define $\hn_x \doteq |J_x|$ for each $x \in X$. In these terms, we have $\hat{s}_i = \hn_{x_i}$, and the matrix $A$ is equal to  
\md{fix notation; A is no longer defined}
\begin{equation*}
    A_{ij} = \begin{cases}
        1/\hn_x & \text{ if } i, j \in J_x \text{ for some } x \in X\\
            0 & \text { otherwise.}
    \end{cases}
\end{equation*}
Note that $A$ is a block diagonal matrix in terms of index sets $\{J_x\}_{x \in X}$, with constant value in each block. Recall that the subset $\hat{\Delta}_n$ consist of weight vectors $w \in \Delta_n$ that, for every pair $i,j \in [n]$, satisfy the constraint $w_i = w_j$ if $x_i = x_j$. This shows that $\hat{\Delta}_n$ can be identified with the set $\Delta_{\hX_n} \doteq \{q \in [0,1]^{\hX} | \sum_{x \in \hX} q_x = 1\}$, where $\hX_n = \{x \in \hX_n | \hn_x > 0\}$, via the bijective relation $w \in \hat{\Delta}_n$ if and only if $w_i = q_{x_i}/\hn_{x_i}$ for some $q \in \Delta_{\hX_n}$. The proof is then completed by rewriting \eqref{eq:lwlike} in terms of $q \in \Delta_{\hX_n}$.
\end{proof}

We now recall the following bound on KL-divergences.
\begin{lemma}
\label{lem:KL-inequality}
Let $p, q, p^\theta$ be probability vectors with support contained in $S \subseteq \Xcal$ such that $\| p -q \|_1 \leq 1/2$. Then
\[ | \KL_S(p | p_\theta) -  \KL_S(q | p_\theta)| 
\leq \| p - q \|_1 \log \frac{|S|}{\| p -q \|_1} + \sqrt{|S|} \| p - q \|_2  \max_{ x \in S} \left| \log p^\theta(x)  \right|.  \]
\end{lemma}
\begin{proof}
By the triangle inequality, we have
\[ | \KL_S(p | p_\theta) -  \KL_S(q | p_\theta)| \leq  \left| \sum_{x \in S} p_x \log \frac{1}{p_x} - \sum_{x \in S} q_x \log \frac{1}{q_x}  \right| + \left| \sum_{x \in S} (p_x - q_x) \log p_\theta(x) \right|. \]
By known bounds on the entropy function \citep[c.f. Theorem 17.3.3]{cover2006elements}, the first term on the right is bounded by $\| p - q \|_1 \log \frac{|S|}{\| p -q \|_1}$. By Cauchy-Schwarz, we can bound the second term via 
\[  \left| \sum_{x \in S} (p_x - q_x) \log p_\theta(x) \right| 
\leq \sqrt{ \left( \sum_{x \in S} (p_x - q_x)^2 \right) \left( \sum_{x \in S} \log(p^\theta(x))^2 \right) } \leq \| p - q \|_2   \max_{ x \in S} \left| \log p^\theta(x)  \right|. \]
\end{proof}

With Lemma~\ref{lem:KL-inequality} in hand, we can now prove Lemma~\ref{lem:converges-to-rate-function}.
\begin{proof}[Proof of Lemma~\ref{lem:converges-to-rate-function}]
By assumption on the existence of $q^\star$, we have $I_\epsilon(\theta) < \infty$. Let $S = \supp(p^\theta) \subseteq \Xcal$, and let $\alpha = \min_{x \in S} p^\theta(x) > 0$. Observe that any $q \in \Delta_{\Xcal}$ that achieves $\KL_{\Xcal}(q | p^\theta) < \infty$ must satisfy $q_x = 0$ for all $x \not \in S$. Thus, we may rewrite our objectives as
\begin{align*}
I_\epsilon(\theta) &=  - \inf_{\substack{q \in \Delta_{S}\\ \D(q, p^0) \leq \epsilon}} \KL_{S}(q|p^\theta) \\
\epsll &= - n \inf_{\substack{q \in \Delta_{\hX_n \cap S} \\ \D(q, \hat{p}^0) \leq \epsilon}} \KL_{S}(q|p^\theta) .
\end{align*}
Now pick any $\delta > 0$ that is small enough, and let $q_I \in \Delta_{\Xcal}$ satisfy $\D(q_I, p^0) \leq \epsilon$, $\supp(q_I) \subseteq S$, and
\[ \KL_{S}(q_I|p^\theta) \leq \inf_{\substack{q \in \Delta_{S}\\ \D(q, p^0) \leq \epsilon}} \KL_{S}(q|p^\theta) + \delta.\] 
Let $E_n$ denote the event that $S \subseteq \hX_n$ and $\| \hat{p}^0 - p^0 \|_2 \leq \delta$. Observe that $\lim_{n \rightarrow \infty} \prob(E_n) = 1.$ 

Pick any $n > 0$ and condition on $E_n$ occurring. We will first show the upper bound
\[  \inf_{\substack{q \in \Delta_{S} \\ \D(q, \hat{p}^0) \leq \epsilon}} \KL_{S}(q|p^\theta) \leq \inf_{\substack{q \in \Delta_{S}\\ \D(q, p^0) \leq \epsilon}} \KL_{S}(q|p^\theta) + 2 \delta. \]
To do so, let $q = h q^\star + (1-h)q_I$ for some $h \in [0,1]$ that is small enough. By assumption, $\D(q, p^0) < \epsilon$ and $\supp(q) \subseteq S$. Thus, for $\delta$ small enough, we will have 
\[ \D(q_I, \hat{p}^0) \leq \D(q_I, p^0) + \D(\hat{p}^0,  p^0) \leq \epsilon. \]
Moreover, observe that $\| q - q_I \|_1 \leq 2h$. By Lemma~\ref{lem:KL-inequality}, we have
\begin{align*}
\inf_{\substack{q \in \Delta_{\hX_n \cap S} \\ \D(q, \hat{p}^0) \leq \epsilon}} \KL_{S}(q|p^\theta)
&\leq \KL_S(q | p_\theta) \\
&\leq \KL_S(q_I | p_\theta) + h \log \frac{|\Xcal|}{h} + \sqrt{|\Xcal|} h \log \frac{1}{\alpha} \\
&\leq  \inf_{\substack{q \in \Delta_{S}\\ \D(q, p^0) \leq \epsilon}} \KL_{S}(q|p^\theta) + 2 \delta,
\end{align*}
where the last two inequalities follow by taking $h$ sufficiently small.

Now we will show the lower bound
\[  \inf_{\substack{q \in \Delta_{S} \\ \D(q, \hat{p}^0) \leq \epsilon}} \KL_{S}(q|p^\theta) \geq \inf_{\substack{q \in \Delta_{S}\\ \D(q, p^0) \leq \epsilon}} \KL_{S}(q|p^\theta) -  2\delta. \]
To do so, take any $\tilde{q} \in \Delta_{\Xcal}$ satisfying $\D(\tilde{q}, \hat{p}^0) \leq \epsilon$ and
\[ KL(\tilde{q} | p_\theta) \leq  \inf_{\substack{q \in \Delta_{S} \\ \D(q, \hat{p}^0) \leq \epsilon}} \KL_{S}(q|p^\theta) + \delta. \]
Let $q' = h q^\star + (1-h)q_I$ for some $h \in [0,1]$ that is small enough. By assumption, $\supp(q') \subseteq S$ and, if $\delta$ is small enough, $\D(q', p^0) \leq \epsilon$. As above, we have $\| q' - \tilde{q} \|_1 \leq 2h$. Thus, using similar arguments as above, we have for small enough $h$,
\begin{align}
\inf_{\substack{q \in \Delta_{S} \\ \D(q, \hat{p}^0) \leq \epsilon}} \KL_{S}(q|p^\theta) 
&\geq \KL(\tilde{q} | p_\theta) - \delta \\
&\geq \KL(q' | p_\theta) - 2 \delta \\
&\geq \inf_{\substack{q \in \Delta_{S}\\ \D(q, p^0) \leq \epsilon}} \KL_{S}(q|p^\theta) -  2\delta.
\end{align}
Putting it all together, we have that for any $\delta > 0$,
\[ \lim_{n \rightarrow \infty} \prob\left( \left|\frac{1}{n} \epsll -  I_\epsilon(\theta) \right| > \delta \right)  = 0. \qedhere \]
\end{proof}


\begin{proof}[Proof of Corollary~\ref{cor:cposterior-asymptotics}]
For now, let $x_1, x_2, \ldots \in \Xcal$ be an arbitrary sequence. We define the intermediate object
\[ M_{n,\epsilon}(\theta) := \prob_\theta\left(\D(\EmpDist{X_{1:n}}, p^0) \leq \epsilon\right). \]
\md{I changed notation since $M_{n}(\theta|x_{1:n})$ didn't depend on $x_{1:n}$.}
Let $\Gamma = \{ q \in \Delta_{\Xcal} \, | \, \D(q, p^0) \leq \epsilon  \}$. Our continuity conditions on $\D$ \md{I think you need Lemma \ref{lem:continuity} or such..} ensure that $\Gamma$ meets the conditions of Sanov's Theorem. Thus, we have
\[ \lim_{n\rightarrow \infty} \frac{1}{n} \log M_{n,\epsilon}(\theta) 
= \lim_{n\rightarrow \infty} \frac{1}{n} \log  \prob_\theta\left( \EmpDist{X_{1:n}} \in \Gamma \right) 
= - \inf_{q \in \Gamma} \KL_\Xcal(q | p^\theta) = I_\epsilon(\theta).  \]
Now let $x_1, x_2, \ldots \sim p^0$. Pick any $\delta >0$ sufficiently small, and let $E_n$ denote the event that $\D(p^0, \hat{p}^0) < \delta$. In the event of $E_n$, we have
\begin{align*}
\left| \log M_\epsilon(\theta | x_{1:n}) - \log L_{\epsilon}(\theta | x_{1:n})  \right| 
&= \max \left\{  \log \frac{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, p^0) \leq \epsilon\right)}{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, \hat{p}^0) \leq \epsilon\right) },  \log \frac{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, \hat{p}^0) \leq \epsilon\right) }{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, p^0) \leq \epsilon\right)} \right\} \\
&\leq \max \left\{ \log \frac{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, p^0) \leq \epsilon\right)}{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, p^0) \leq \epsilon - \delta \right) },  \log \frac{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon +\delta \right) }{\prob_\theta\left(\D(\EmpDist{X_{1:n}}, p^0) \leq \epsilon\right)} \right\}.
\end{align*}
We will focus on bounding the second term of the last line. The argument for the first is identical. Observe by Sanov's Theorem, we have
\begin{align*}
\lim_{n\rightarrow \infty} \frac{1}{n} \log  \prob_\theta\left( \D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon +\delta \right) &= \inf_{\substack{q \in \Delta_{\Xcal} \\ \D(q, {p}^0) = \epsilon + \delta}} \KL_{\Xcal}( q | p^\theta)  \\
\lim_{n\rightarrow \infty} \frac{1}{n} \log  \prob_\theta\left( \D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon \right) &= \inf_{\substack{q \in \Delta_{\Xcal} \\ \D(q, {p}^0) \leq \epsilon }} \KL_{\Xcal}( q | p^\theta) .
\end{align*}

\md{Awesome, I like the above step. It seems below that you are using a result in this and the previous lemma that shows that the rate function is continuous in some sense. Can you package these arguments into a lemma and reuse them?}

Let $N \geq 0$ be such that for all $n \geq N$, we have that the above two sequences are within $\delta$ of their limits. Now take $n \geq N$ and suppose that $E_n$ holds. Now we trivially have
\[ \inf_{\substack{q \in \Delta_{S} \\ \D(q, {p}^0) \leq \epsilon + \delta}} \KL_\Xcal( q | p^\theta) \leq \inf_{\substack{q \in \Delta_{\Xcal} \\ \D(q, {p}^0) \leq \epsilon }} \KL_\Xcal( q | p^\theta). \]
Let $q_I, q'_I \in \Delta_{\Xcal}$ satisfy $\D(q_I, p^0) \leq \epsilon + \delta$, $\D(q_I, p^0) < \epsilon$ and
\begin{align*}
\KL_{\Xcal}( q_I | p^\theta)  & \leq  \inf_{\substack{q \in \Delta_{\Xcal} \\ \D(q, {p}^0) \leq \epsilon + \delta}} \KL_{\Xcal}( q | p^\theta) + \delta  \\
\KL_{\Xcal}( q'_I | p^\theta)  & < \infty.
\end{align*}
Let $q = h q'_I + (1-h) q_I$ for some small $h \in (0,1)$ (chosen relative to $\delta$). For appropriately chosen $h$, we have $d(q, p^0) \leq \epsilon$ and $\| q - q_I \|_1 \leq 2h$. Applying Lemma~\ref{lem:KL-inequality}, we then have
\begin{align*}
\inf_{\substack{q \in \Delta_{\Xcal} \\ \D(q, {p}^0) \leq \epsilon }} \KL_{\Xcal}( q | p^\theta) &\leq \KL_{\Xcal}( q | p^\theta) \\
&\leq \KL(q_I | p^\theta) + \delta_h \\
&\leq \inf_{\substack{q \in \Delta_{\Xcal} \\ \D(q, {p}^0) \leq \epsilon + \delta}} \KL_{\Xcal}( q | p^\theta) + \delta + \delta_h.
\end{align*}
Where $\delta_h$ is some function of $h$ that goes to 0 as $\delta$ (and hence $h)$ goes to 0. Putting it all together, 
\begin{align*}
\frac{1}{n} \log  \prob_\theta\left( \D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon +\delta \right) -  \frac{1}{n} \log  \prob_\theta\left( \D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon \right) 
&\leq 3 \delta + \delta_h.
\end{align*}
Applying the same arguments shows us
\begin{align*}
\frac{1}{n} \log  \prob_\theta\left( \D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon  \right) -  \frac{1}{n} \log  \prob_\theta\left( \D(\EmpDist{X_{1:n}}, {p}^0) \leq \epsilon - \delta \right) 
&\leq 3 \delta + \delta_h.
\end{align*}
Thus,
\begin{align*}
 \left| \frac{1}{n} \log L_{\epsilon}(\theta | x_{1:n}) - I_\epsilon(\theta) \right| 
 &\leq \left| \frac{1}{n} \log L_{\epsilon}(\theta | x_{1:n}) - \frac{1}{n} \log M_\epsilon(\theta | x_{1:n})  \right| +  \left| \frac{1}{n} \log M_{\epsilon}(\theta | x_{1:n}) - I_\epsilon(\theta) \right| \\
 &\leq 4 \delta + \delta_h.
\end{align*}
This suffices to show 
\[ \frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) \pconv I_\epsilon(\theta) \quad \text{ as } n \to \infty.\]
Now we apply Lemma~\ref{lem:converges-to-rate-function}, which showed
\[ \frac{1}{n}\epsll \pconv I_\epsilon(\theta) \quad \text{ as } n \to \infty,  \]
to conclude that 
\[ \frac{1}{n} \left[\log L_\epsilon(\theta|x_{1:n}) -  \epsll\right] \pconv 0 \quad \text{ as } n \to \infty. \qedhere \]
\end{proof}

\md{Next lemma needs changing notation to $F(r) = I_r$. Also the condition $\KL(p^0|p^\theta) < \infty$ and Assumption \ref{ass:dist-finite} might be too strong as the above lemmas point out.}
\begin{lemma} Suppose $\D$ satisfies assumption \ref{ass:dist-finite} and $\theta \in \Theta$ is such that $\KL(p^0|p^\theta) < \infty$. Then the function $F: [0,\infty) \to [0,\infty)$ given by $F(r) = -I_r(\theta)$ is continuous. (Note that $F(r) \leq \KL(p^0|p^\theta) < \infty$ for each $r \geq 0$.)
\label{lem:continuity}
\end{lemma}
\begin{proof} 
 Using our continuity assumption on $\D$, let us first show that the optimization problem in $I_r$ attains its minimum value at some $q_r \in A_r \doteq \{ q \in \Delta_{\cX} \mid \D(q, p^0) \leq r \}$. 
Indeed, this follows since $A_r$ is a compact subset and the function $q \mapsto \KL_{\cX}(q|p^\theta)$ is lower semi-continuous.

Let us now show that $\liminf_{h \to 0} F(r_0+h) \geq F(r_0)$ for any $r_0 \in [0,\infty)$ (with the convention that $F(r)=\infty$ when $r < 0$). Indeed, for any sequence $\{h_n\}_{n \in \nat}$ that is converging to zero, the sequence $\{q_{r_0 + h_{h_n}}\}_{n \in \nat} \subseteq \Delta_{\cX}$ is pre-compact. Hence, there is an increasing subsequence $\{n_k\}_{k \in \nat} \subseteq \nat$ and $q_* \in \Delta_n$ such that $\lim_{k \to \infty} q_{r_0 + h_{n_k}} = q_*$. Note then by the continuity of $\D$, that $\D(q_*, p_0) = \lim_{k \to \infty} \D(q_{r_0 + h_{n_k}}, p_0) \leq \limsup_{k \to \infty} r_0 + h_{n_k} = r_0$. Hence, the lower semi-continuity of the $\KL$-divergence shows that $$
\liminf_{k \to \infty} F(r_0 + h_{n_k}) = \liminf_{k \to \infty} \KL_{\cX}(q^{r_0 + h_{n_k}}|p^\theta) \geq \KL_{\cX}(q_*|p^\theta) \geq F(r_0).
$$

Note that $F$ is a non-decreasing function, i.e. $F(r) \leq F(s)$ whenever $s \geq r$. Hence, the result $\liminf_{h \downarrow 0} F(r_0 + h) \geq F(r_0) \geq \limsup_{h \downarrow 0} F(r_0 + h)$ immediately shows the right continuity of $F$, i.e. $F(r_0) = \lim_{h \downarrow 0} F(r_0 + h)$.
%

Now we shall establish the left-continuity of $F$ at some point $r=r_0 > 0$. If $\D(q_{r_0},p^0) < r_0$, then $F(r) = F(r_0)$ for each $r \in [\D(q_{r_0}, p^0), r_0)$ and the left continuity is easily satisfied. Hence, suppose from now on that $\D(q_{r_0},p^0) = r_0$. Next, for any $h \in [0,1]$, denote by $q'_h \doteq (1-h) q_{r_0} + h p^0$ the convex combination between $q_{r_0}$ and $p^0$. By the convexity of KL-divergence
\begin{equation*}
    F(\D(q'_h, p^0)) \leq \KL_{\cX}(q'_h|p^\theta) \leq (1-h)\KL_{\cX}(q_{r_0}|p^\theta) + h \KL_{\cX}(p^0|p^\theta) = (1-h) F(r_0) + h F(0).
\end{equation*} 
 
Take $h \downarrow 0$ to obtain $\limsup_{h \to 0} F(T(h)) \leq F(r_0)$, where $T(h) \doteq \D(\tilde{q}^h, p^0)$. By our assumption, $T: [0,1] \to [0,r_0]$ is a continuous and strictly decreasing function. Hence $T(h)$ is stricly increasing to $T(0)=r_0$  as $h \downarrow 0$. Thus, we have in-fact shown that $\limsup_{h \downarrow 0} F(r_0-h) = F(r_0)$. Finally, monotonicity of $F$ shows $\liminf_{h \downarrow 0} F(r_0 - h) \geq F(r_0)$, and hence we recover the left continuity of $F$.
\end{proof}
