%auto-ignore

\iffalse
\subsection{Introduction}
Let us assume that $\cX$ is a Polish space (i.e. a complete separable metric space) equipped with its Borel sigma algebra $\cB(\cX)$.  Then $\cP(\cX)$, the set of (Borel)  probability measures on $\cX$, is also a Polish space when equipped with the topology of weak convergence \cite{billingsley1971weak}.
%
%For a measurable function $f: \cX \to \R$ and measure $\lam$ on $\cX$, let $\|f\|_{p,\lam} = \left(\int |f|^p d\lam\right)^{1/p}$ denote the $L_p$ norm of $f$ with respect to $\lam$. Given a kernel $\K: \cX \times \cX \to \R$ and a measure $\mu$ on $\cX$, the convolved density of $\mu$, denoted by the map $\kappa \star \mu : \cX \to [0,\infty)$, is the function  $x \mapsto \int \K(x, y) \mu(dy)$.
Let $\D$ be an integral probability semi-metric on $\cP(\cX)$, that is continuous with respect to the weak convergence topology.

Suppose we are given a model family $\{P_\theta\}_{\theta \in \Theta} \subseteq \cP(\cX)$ and observed data $x_1, \ldots, x_n \iid P_0 \in \cP(\cX)$. In this note, we will study the asymptotics as $n \to \infty$ of the  \emph{coarsened likelihood} defined as
$$
L_\epsilon(\theta|x_{1:n}) = \prob_\theta(\D(\EmpDist{Z_{1:n}}, \EmpDist{x_{1:n}}) \leq \epsilon ) 
$$
where, given $\theta \in \Theta$, the probability operation $\prob_\theta$ is with respect to $Z_1, \ldots, Z_n \iid P_\theta$, and $\EmpDist{z_{1:n}} = n^{-1}\sum_{i=1}^n \delta_{z_i} \in \cP(\cX)$ is the empirical distribution.

To describe the limiting expression, recall the definition of the    
Kullbackâ€“Leibler divergence (also called as relative entropy)
\begin{equation*}
	\KL(P|Q)  = \begin{cases}
		\int \log \frac{dP}{dQ} dP & \text{ if } P \ll Q \\
		\infty & \text{ otherwise }
	\end{cases}
\end{equation*}
where $P \ll Q$ denotes the absolute continuity condition that $P(A) = 0$ whenever $Q(A) = 0$, and $\frac{dP}{dQ}$ is the Random Nikodym derivative of $P$ with respect to $Q$. With this, we can define the \emph{generalized optimistic Kullback Leibler}  function 

\begin{equation*}
	\okl = \inf_{\substack{Q \in \cP(\cX)\\
			\D(Q, P_0) \leq \epsilon}} \KL(Q|P_\theta)
\end{equation*}
\fi


In this section, we will show that the coarsened likelihood continues to converge in probability to the OKL when $\cX$ is a metric space. More precisely, let $\cX$ be a Polish space (i.e.~a complete and separable metric space) equipped with its Borel sigma algebra $\cB(\cX)$. Then $\cP(\cX)$, the set of (Borel) probability measures on $\cX$, can be equipped with the topology of weak-convergence \citep{billingsley2013convergence}. In more detail, we say that a sequence of measures $\{\cP_n\}_{n \in \nat}  \subseteq \cP(\cX)$ weakly converges to a measure $P$, denoted as $P_n \dconv P$, if $\lim_{n \to \infty} \int f dP_n = \int f dP$ for each continuous and bounded function $f: \cX \to \R$. The space $\cP(\cX)$ is also a Polish space under this topology \citep{billingsley2013convergence}. 

 Recall the definition of the OKL for a general distance $\D$ over $\cP(\cX)$ from \cref{eqn:okl-general-distance}:
\[ \okl[\epsilon] = \inf_{\substack{Q \in  \cP(\cX) \\ \D(Q, P_0) \leq \epsilon}} \KL(Q | P_\theta). \]
To establish an asymptotic connection between the coarsened likelihood and the OKL, we will make the following assumption on $\D$.
\begin{assume}
\label{assump:general-distance}
    For any $P,Q, R \in \cP(\cX)$ and $\lambda \in (0,1)$, the following holds:
    \begin{enumerate}[(a)]
		\item $\D(P, P) = 0$.
		\item $\D(P, Q) = \D(Q, P)$.
		\item $\D(P, Q) \leq \D(P, R) + \D(R, Q)$.
		\item $\D$ is convex in its arguments: $\D(P, (1-\lambda)Q + \lambda R) \leq \lambda \D(P, Q) + (1-\lambda) \D(P, R)$.
		\item For any sequence of probability measures $P_n \dconv P$, $\D(P_n, Q) \to \D(P, Q)$ as $n \to \infty$.
	\end{enumerate}
\end{assume}
Conditions~(a)-(c) simply require that $\D$ is a pseudometric, i.e. it satisfies all the requirements of a metric except for the requirement that $\D(P,Q) > 0$ whenever $P \neq Q$. Condition~(d) (combined with condition~(a)) allows us to apply \Cref{lem:okl-continuity}, and condition~(e) ensures that $\D$ is continuous with respect to the topology of weak convergence.


Given this assumption on $\D$, we can demonstrate the following asymptotic convergence.

\begin{theorem}
\label{thm:clikelihood-asymptotics}
Suppose that \Cref{assump:general-distance} holds and that $\okl[\epsilon_0] < \infty$ for some $\epsilon_0 \geq 0$. For any $\epsilon > \epsilon_0$, if $x_1, \ldots, x_n \iid P_0$, then 
\begin{equation*}
	\frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) \pconv -\okl[\epsilon]
\end{equation*}
as $n \to \infty$.
\end{theorem}

\subsubsection{Proof of \texorpdfstring{\Cref{thm:clikelihood-asymptotics}}{thmclike}}

\iffalse
\begin{assume} Suppose $\D$ is an IPS that is continuous with respect to weak convergence. In other words, we assume that 
	\begin{enumerate}[(a)]
		\item  There is a class of measurable functions $\cF \subseteq \{f : \cX \to \R \}$ such that 
		$$
		\D(\mu, \nu) = \sup_{f \in \cF} |\int f d\mu - \int f d\nu|.
		$$
		\item For any sequence of probability measures $\mu_n \dconv \mu$ in $\cP(\cX)$, $\D(\mu_n, \nu) \to \D(\mu, \nu)$ as $n \to \infty$.
	\end{enumerate}
	\label{assume:dist-assumptions}
\end{assume}


\begin{lemma} 
	\label{lem:okl-continuity}
	If $\D$ satisfies part (a) of \Cref{assume:dist-assumptions}, then for any $\epsilon, t \geq 0$
	$$
	0 \leq \okl - \okl[\epsilon+t] \leq \frac{t}{\epsilon+t} \KL(P_0|P_\theta).
	$$
\end{lemma}
\begin{proof} For any $\delta > 0$, let us choose $Q \in \cP(\cX)$ such that $\KL(Q|P_\theta) \leq \okl[\epsilon+t] + \delta$. Since $\D(Q, P_0) \leq \epsilon + t$, using the IPS property  of $\D$ and $\lambda_t = t/(\epsilon+t)$, note that  
	$\D((1-\lambda_t) Q + \lambda_t P_0, P_0) = (1-\lambda_t)\D(Q, P_0) \leq \epsilon$. Thus, we can use  convexity of the KL divergence to obtain
	\begin{align*}
		\okl[\epsilon] \leq \KL((1-\lambda_t) Q^t + \lambda_t P_0|P_\theta) &\leq (1-\lambda_t)\KL(Q^t|P_\theta) + \lambda_t \KL(P_0|P_\theta)  \\
		&\leq \okl[\epsilon + t] + \delta + \frac{t}{\epsilon + t}  \KL(P_0 | P_\theta).
	\end{align*}
	Since $\okl[\epsilon] \geq \okl[\epsilon + t] \geq 0$ and $\delta > 0$ is arbitrary, \Cref{lem:okl-continuity} follows.
\end{proof}
\fi

Similar to the setting in \Cref{sec:coarsened-likelihood-asymptotic-finite}, we will study asymptotics of the coarsened likelihood $\owl$ by first studying the asymptotics of its population level analog  
\begin{equation}
	\label{eq:pop-clike}
	\owlM[\epsilon] = \prob_\theta(\D(\EmpDist{Z_{1:n}}, P_0)\leq \epsilon)
\end{equation}
obtained by replacing the  empirical distribution of the data  $\EmpDist{x_{1:n}}$ by the  population level quantity $P_0$.


Next, to study the asymptotics of $\owlM$, we will invoke Sanov's theorem from Large Deviation theory which says that the law of the empirical distribution $\EmpDist{Z_{1:n}}$  satisfies a Large Deviation principle in the space $\cP(\cX)$ with rate function $\mu \mapsto \KL(\mu|P_\theta)$, when $Z_1, \ldots, Z_n \iid P_\theta$. More precisely, we will show that the error term 
\begin{equation}
	\label{eq:sanov-error}
	\lderr = \left|\frac{1}{n}\log \owlM + \okl\right|
\end{equation}
converges to zero as $n \to \infty$.

\begin{lemma}
If \Cref{assump:general-distance} holds and $\okl[\epsilon_0] < \infty$ for some $\epsilon_0 \geq 0$, then for any $\epsilon > \epsilon_0$, $\lim\limits_{n \to \infty} \lderr = 0$.
\label{lem:sanov}
\end{lemma}
\begin{proof} 
	
	Assume $Z_1, \ldots, Z_n \iid P_\theta$. Then Sanov's  theorem \cite[Theorem 6.2.10]{demboLargeDeviationsTechniques2010} shows that for any Borel measurable subset $\Gamma \subseteq \cP(\cX)$,
	\begin{equation}
		\label{eq:sanov}
		\begin{aligned}
			-\inf_{Q \in \Gamma^\circ} \KL(Q|P_\theta) &\leq \liminf_{n \to \infty} \frac{1}{n} \log \prob_\theta(\EmpDist{Z_{1:n}} \in \Gamma)\\ 
			&\leq \limsup_{n \to \infty} \frac{1}{n} \log \prob_\theta(\EmpDist{Z_{1:n}} \in \Gamma) \leq -\inf_{Q \in \bar{\Gamma}} \KL(Q|P_\theta),
		\end{aligned}
	\end{equation}
	where $\Gamma^\circ$ and $\bar{\Gamma}$ refer to the interior and closure of $\Gamma$ under the weak-topology on $\cP(\cX)$. 
	
	
	Now consider the set $\Gamma_t = \{Q \in \cP(\cX) | \D(Q, P_0) \leq t \}$ indexed by the parameter $t \geq 0$. By the assumed continuity of $\D$ under the topology of weak-convergence (\Cref{assump:general-distance}(e)), the set  $\Gamma_\epsilon$ is closed while the set $\Gamma_{\epsilon^-} = \{Q \in \cP(\cX) \mid \D(Q,P_0) < \epsilon\}$ is an open set. Thus using $\bar{\Gamma}_{\epsilon} = \Gamma_\epsilon$ and $\cup_{r < \epsilon} \Gamma_r = \Gamma_{\epsilon^-}\subseteq \Gamma_\epsilon^\circ$, we see 
	\begin{equation*}
		\begin{aligned}
			\okl[\epsilon] = 
			\inf_{Q \in \bar{\Gamma}_\epsilon} \KL(Q|P_\theta) \leq \inf_{Q \in \Gamma^\circ_\epsilon}  \KL(Q|P_\theta)
			\leq \inf_{Q \in \Gamma_{\epsilon^-}} \KL(Q|P_\theta)  \leq \okl[r]
		\end{aligned}
	\end{equation*}
	for each $0 < r < \epsilon$. Next by the condition $\okl[\epsilon_0] < \infty$, we may apply \Cref{lem:okl-continuity} to conclude that the map $t \mapsto \okl[t]$ is continuous at $t = \epsilon$, i.e. $\lim_{t \to \epsilon} \okl[t] = \okl[\epsilon]$. Hence letting $r$ increase to $\epsilon$ in the above display, we find that
	$$
	\inf_{Q \in \bar{\Gamma}_\epsilon} \KL(Q|P_\theta) = \inf_{Q \in \Gamma^\circ_\epsilon} \KL(Q|P_\theta) = \okl[\epsilon].
	$$
	Thus taking $\Gamma = \Gamma_\epsilon$ in \cref{eq:sanov} shows that the limit
	$$
	\frac{1}{n} \log \prob_\theta(\D(\EmpDist{Z_{1:n}},P_0) \leq \epsilon) = \frac{1}{n} \log \prob_\theta(\EmpDist{Z_{1:n}} \in \Gamma_\epsilon) \to - \okl
	$$
	holds as $n \to \infty$. Recalling  the definition of $\owlM$ in \cref{eq:pop-clike}, we see that the asymptotic error term $\lderr$ in \cref{eq:sanov-error} is converging to zero as $n \to \infty$.
			

\end{proof}


Now we can prove \Cref{thm:clikelihood-asymptotics} by carefully accounting for the error between the coarsened likelihood  $\owl$ and its population-level analog $\owlM$ from  \Cref{eq:pop-clike}.

\begin{proof}[Proof of \Cref{thm:clikelihood-asymptotics}] Let $x_1, \ldots, x_n \iid P_0$ and pick $0 < t < \epsilon - \epsilon_0$. Define $E_{n,t}$ as the event that $\D(\EmpDist{x_{1:n}}, P_0) \leq t$. Since $\D$ is continuous with respect to weak convergence and $\EmpDist{x_{1:n}} \dconv P_0$ as $n \to \infty$ by the weak law of large numbers, we have $\lim_{n \to \infty} \prob(E_{n,t}) = 1$ for any $t > 0$. By \Cref{assump:general-distance}(c) we have $|\D(\EmpDist{Z_{1:n}}, P_0) -  \D(\EmpDist{Z_{1:n}}, \EmpDist{x_{1:n}})| \leq \D(P_0, \EmpDist{x_{1:n}})$, implying that on the event $E_{n,t}$ we have
%
\begin{equation}
	\label{eq:owlboundsonEnt}
	\owlM[\epsilon-t] \leq \owl \leq \owlM[\epsilon+t].
\end{equation}
%
Thus on the event $E_{n,t}$, we can bound
\begin{align*}
	\abs*{\frac{1}{n} \log \owl + \okl} 
	&\leq \abs*{\frac{1}{n} \log \owlM[\epsilon+t] + \okl}+ \abs*{\frac{1}{n} \log \owlM[\epsilon-t] + \okl}\\
	&\leq \lderr[\epsilon+t] + \lderr[\epsilon-t] + \abs{\okl[\epsilon+t] - \okl}+\abs{\okl[\epsilon-t] - \okl}\\
	&\leq \lderr[\epsilon+t] +  \lderr[\epsilon-t] + \frac{2t}{\epsilon - \epsilon_0} \okl[\epsilon_0]
\end{align*}
where the first inequality uses  \cref{eq:owlboundsonEnt}, the second uses \cref{eq:sanov-error}, and the last uses \Cref{lem:okl-continuity}. Hence given any $\delta > 0$ and $t \in (0,\epsilon - \epsilon_0)$ such that $t \okl[\epsilon_0] <  (\epsilon - \epsilon_0) \delta/4$, we have
\begin{align*}
	\prob\brR*{\abs*{\frac{1}{n} \log \owl + \okl} > \delta} \leq \prob(E_{n,t}^c) + \I{\lderr[\epsilon-t] + \lderr[\epsilon+t] > \delta/2}.
\end{align*}
Hence using \Cref{lem:sanov} and the fact that $\prob(E_{n,t}) \to 1$ as $n\to \infty$, we now see that 
$$
\lim_{n \to \infty} \prob\brR*{\abs*{\frac{1}{n} \log \owl - \okl} > \delta} = 0.
$$ 
Since $\delta > 0$ was arbitrary, this completes the proof.
\end{proof}

\subsubsection{Smoothed total variation distance satisfies \texorpdfstring{\Cref{assump:general-distance}}{assgenraldist}}
\label{sec:smoothed-tvd-is-continous-ips}

Suppose $\K: \cX \times \cX \to [0,\infty)$ is a probability kernel with respect to measure $\lambda$. That is, assume $\int \K(x,y) d\lambda(x) = 1$ for each $y \in \cX$.  Given a measure $\mu \in \cP(\cX)$, this allows us to define a smoothed probability measure $\K \star \mu \in \cP(\cX)$ that has density  

\begin{equation*}
	\frac{d(\K \star \mu)}{d\lambda} (x) =  f_{\K, \mu}(x) = \int \K(x, y) d\mu(y)
	%(\K \star \mu)(A)  = \int_A f_{\K, \mu}(x)  d\lambda(x).
\end{equation*}
with respect to $\lambda$.

Recall the definition of the total variation distance on $\cP(\cX)$, 
\begin{equation}
	\label{eq:tvd}
	\tv(\mu, \nu) = \sup_{B \in \cB(\cX)}|\mu(B)-\nu(B)| = \sup_{g: \cX \to [-1,1]} \abs*{\int g d\mu - \int g d\nu}.
\end{equation}
When $\mu$ and $\nu$ have densities $f_\mu$ and $f_\nu$ with respect to a common measure $\lambda$, one can additionally show $\tv(\mu, \nu) = \frac{1}{2}\int  \abs{f_\mu-f_\nu} d\lambda$. 

Although $\tv$ is not closed with respect to weak convergence, we can show that the following kernel-smoothed version of TV distance is.
\begin{definition} Given a probability density kernel $\K$, the smoothed total-variation distance is given by  
	\begin{equation*}
		\tvk(\mu, \nu) = \tv(\K \star \mu, \K \star \nu) = \frac{1}{2} \int \abs{f_{\K,\mu}(x) - f_{\K, \nu}(x)} d\lambda(x)
	\end{equation*}
\end{definition}
Now we will show that $\tvk$ satisfies \Cref{assump:general-distance} when $\K$ is a bounded and continuous kernel. 
\begin{proposition}
If $\K$ is a bounded and continuous kernel, then $\tvk$ satisfies \Cref{assump:general-distance}.
\end{proposition}
\begin{proof}
First observe that the smoothed TV distance is just an ordinary TV distance between smoothed densities.
%
Since the ordinary TV distance is a metric, this immediately implies that the smoothed TV  satisfies the identity, symmetry, and triangle inequality properties. 

To establish convexity of $\tvk$, note that for measures $\mu, \nu \in \cP(\cX)$ and $v \in [0,1]$, we have
\[f_{\K, (1-v)\mu + v\nu}(x) = (1-v) \int \K(x, y) d\mu(y) + v  \int \K(x, y) d\nu(y) = (1-v)f_{\K, \mu}(x) + v f_{\K, \nu}(x) .  \]
Thus, for $\mu, \nu, \pi \in \cP(\cX)$, we have
\begin{align*}
\tvk(\pi, (1-v)\mu + v\nu) 
&= \frac{1}{2}\int \abs{f_{\K,\pi}(x) - f_{\K, (1-v)\mu + v\nu}(x)} d\lambda(x) \\
&= \frac{1}{2} \int \abs{f_{\K,\pi}(x) - ((1-v)f_{\K, \mu}(x) + v f_{\K, \nu}(x))} d\lambda(x) \\
&\leq (1-v) \frac{1}{2} \int \abs{f_{\K,\pi}(x) - f_{\K, \mu}(x)} d\lambda(x) + v \frac{1}{2}\int \abs{f_{\K,\pi}(x) - f_{\K, \nu}(x)} d\lambda(x) \\
&= (1-v) \tvk(\pi, \mu )  + v \tvk(\pi,\nu),
\end{align*}
where the inequality follows from the convexity of the absolute value function. Thus, $\tvk$ is convex in its arguments.


To establish the continuity of $\tvk$ under the topology of weak-convergence, suppose $\mu_n \dconv \mu$ in $\cP(\cX)$. Then since $y \mapsto \K(x,y)$ is a continuous and bounded function, the convergence 
$$
f_{\K, \mu_n}(x) \to f_{\K, \mu}(x)
$$
follows for each $x \in \cX$ as $n \to \infty$. Since $f_{\K, \nu}$ for any $\nu \in \cP(\cX)$ is a density with respect to $\lambda$, Scheff\'e's lemma shows that
$$
\tvk(\mu_n, \mu) = \frac{1}{2} \int \abs{f_{\K, \mu_n}(x) - f_{\K, \mu}(x)}d \lambda(x) \to 0  \ \ \text{ as } n \to \infty.
$$
Finally, by the triangle inequality, $\abs{\tvk(\mu_n, \nu) - \tvk(\mu, \nu)} \leq \tvk(\mu_n, \mu)$.
\end{proof}


\iffalse
Now we will show that $\tvk$ satisfies \Cref{assume:dist-assumptions} when $\K$ is a bounded
and continuous kernel.

First let us observe that $\tvk$ is an IPS. Indeed, using the last equality in  \cref{eq:tvd} one can see that
\begin{equation*}
	\tvk(\mu,\nu) = \sup_{g: \cX \to [-1,1]} \abs*{\int \smooth[\K]{g} d \nu  - \int \smooth[\K]{g} d\mu}
\end{equation*}
where $\smooth[\K]{g}(y) = \int g(x) \K(x,y) d\lambda(x)$ for any measurable function $g: \cX \to \R$.
\fi