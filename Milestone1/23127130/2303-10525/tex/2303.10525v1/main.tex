\pdfoutput=1
\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{caption}


%%%%%%%% Clever ref
\usepackage{cleveref}

\captionsetup[figure]{font={stretch=1.2}}    %% change 1.2 as you like


%\usepackage{url} % not crucial - just used below for the URL


\usepackage{macros}
%\usepackage{breqn}

\usepackage{natbib}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\usepackage{IEEEtrantools}
\usepackage{algorithm,algcompatible,amsmath}

\usepackage[utf8]{inputenc}
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Robustifying likelihoods by optimistically re-weighting data}
%\author{Miheer Dewaskar\thanks{joint first authors} \and Christopher Tosh\footnotemark[1] \and Jeremias Knoblauch \and David Dunson}
   \author{Miheer Dewaskar\thanks{
    M.D. and C.T. contributed equally. The authors acknowledge funding from grants N00014-21-1-2510-P00001 from the Office of Naval Research (ONR) and R01ES027498, U54 CA274492-01 and R37CA271186  from the National Institutes of Health, as well as helpful discussions with Sayan Mukherjee and Amarjit Budhiraja.}\hspace{.2cm}\\
    Department of Statistical Science, Duke University\\
    and \\
    Christopher Tosh\footnotemark[1]  \\
    Department of Epidemiology and Biostatistics, \\
    Memorial Sloan Kettering Cancer Center\\
    and\\
    Jeremias Knoblauch\\
    Department of Statistics, UCL\\
    and\\
    David B. Dunson\\
    Department of Statistical Science, Duke University}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Robustifying likelihoods to slight misspecification}
\end{center}
  \medskip
} \fi

%\bigskip
\begin{abstract}
Likelihood-based inferences have been remarkably successful in wide-spanning application areas. However, even after due diligence in selecting a good model for the data at hand, there is inevitably some amount of model misspecification: outliers, data contamination or inappropriate parametric assumptions such as Gaussianity mean that most models are at best rough approximations of reality.
%
A significant practical concern is that for certain  inferences, even small amounts of model misspecification may have a substantial impact; a problem we refer to as {\em brittleness}.
%
This article attempts to address the brittleness problem in likelihood-based inferences by choosing the most model friendly data generating process in a discrepancy-based neighbourhood of the empirical measure. This leads to a new Optimistically Weighted Likelihood (OWL), which robustifies the original likelihood by formally accounting for a small amount of model misspecification. Focusing on total variation (TV) neighborhoods, we
study theoretical properties, develop inference algorithms and illustrate the methodology in applications to mixture models and regression.
\end{abstract}

\noindent%
{\it Keywords:}
Coarsened Bayes; Data contamination;  Mixture models; Model misspecification; Outliers;  Robust inference; Total variation distance. 
\vfill

%\newpage
%\spacingset{1.9} % DON'T change the spacing!

%\date{June 2022}


%\maketitle


%{\bf (DD - Looking over the below, it is currently a total mess in all the sections. We need to stop fucking around and finalize the storyline and rewrite with this storyline in mind. Ideally we look at the method that is currently being implemented with good results and build up a clear justification for this method and put it clearly in the context of the literature while including competitors in simulated and real data experiments. It seems that the TVD case should be the focus as MMD complicates the story and doesn't have as good results in practice, but can we have a very clear motivation for the approach being applied starting from the coarsest likelihood philosophy?)}

\section{Introduction}

\input{intro.tex}

\section{Optimistically Weighted Likelihoods}
\label{sec:methodology}
\input{owl.tex}


\section{Performing OWL computations}
\label{sec:comp}
\input{inference.tex}


\section{Asymptotic connection to coarsened inference}
\label{sec:coarsened-inference}
\input{asymptotic-coarsened-inference}


\section{Simulation Examples}
\label{sec:simul}
\input{simulations.tex}

\section{Application to scRNA-seq Clustering}
\label{sec:application}
\input{scRNA-seq.tex}

\section{Application to micro-credit study}
\label{sec:micro-credit}
\input{microcredit.tex}

\section{Discussion}
\input{discussion}

% \bibliographystyle{plain}
\bibliographystyle{abbrvnat}
\bibliography{refs}

\appendix

% \section{More discussion}
% \input{more-discussion}

\section{Properties of the OKL}
\label{sec:useful-lemmas}
\input{theory/useful-lemmas}


\section{Convergence of OKL estimator in finite spaces}
\label{sec:okl-estimation-finite}
\input{theory/finite-convergence}

\section{Convergence of OKL estimator in Euclidean spaces}
\label{sec:okl-estimation-cont}
\input{theory/infinite-convergence}

\section{Asymptotics of the coarsened likelihood}
\label{sec:coarsened-likelihood-asymptotics}

In this section, we will show the asymptotic convergence of the coarsened likelihood to the OKL function. 


\subsection{Asymptotics for finite spaces}
\label{sec:coarsened-likelihood-asymptotic-finite}
\input{theory/coarsened-likelihood-asymptotics-finite}

\subsection{Asymptotics for  continuous spaces}
\label{sec:coarsened-likelihood-asymptotic-cont}
\input{theory/coarsened-likelihood-asymptotics}


\section{Auxiliary lemmas}
\input{theory/aux-lemmas}

\section{Simulations with random corruptions}
\label{app:more-simulations}
\input{more-simulations}


\section{More details of the micro-credit study}
\label{app:micro-credit}
\input{more-micro-credit}

%\subsection{Proofs from Section \ref{sec:okl}}

%\input{theory/clikelihood-asymptotics}

%\subsection{Proof for finite spaces (Section \ref{sec:finitespaces})}
%\label{sec:proofs-for-finite-spaces}
%\input{theory/finite-coarsened-likelihood-connection}

\iffalse
\begin{proof}[Proof of Lemma \ref{lem:lwrewritefinite}]
Given observations $x_{1:n} \in \cX^n$, we can partition the index set $[n]$ into a disjoint collection of subsets $\{J_x\}_{x \in X}$, where $J_{x} \doteq \{i \in [n]| x_i = x\}$ denotes the set of indices that realize the value $x \in X$;  further define $\hn_x \doteq |J_x|$ for each $x \in X$. In these terms, we have $\hat{s}_i = \hn_{x_i}$, and the matrix $A$ is equal to
\begin{equation*}
    A_{ij} = \begin{cases}
        1/\hn_x & \text{ if } i, j \in J_x \text{ for some } x \in X\\
            0 & \text { otherwise.}
    \end{cases}
\end{equation*}
Note that $A$ is a block diagonal matrix in terms of index sets $\{J_x\}_{x \in X}$, with constant value in each block. Recall that the subset $\hat{\Delta}_n$ consist of weight vectors $w \in \Delta_n$ that, for every pair $i,j \in [n]$, satisfy the constraint $w_i = w_j$ if $x_i = x_j$. This shows that $\hat{\Delta}_n$ can be identified with the set $\Delta_{\hX_n} \doteq \{q \in [0,1]^{\hX} | \sum_{x \in \hX} q_x = 1\}$, where $\hX_n = \{x \in \hX_n | \hn_x > 0\}$, via the bijective relation $w \in \hat{\Delta}_n$ if and only if $w_i = q_{x_i}/\hn_{x_i}$ for some $q \in \Delta_{\hX_n}$. The proof is then completed by rewriting \eqref{eq:lwlike} in terms of $q \in \Delta_{\hX_n}$.
\end{proof}

\fi


%\section{Proofs from Section~\ref{sec:comp}}
%\label{app:comp-proofs}

\iffalse
\subsection{Proof of Proposition~\ref{prop:tv-mixture-model}}

We first prove the following simple lemma.

\begin{lemma}
\label{lem:tv-decomposition}
Let $\epsilon > 0$, and let $P, Q$ be two measures such that for all measurable sets $A$, we have $Q(A) \geq (1-\epsilon)P(A)$. Then $\tv(P, Q) \leq \epsilon$ if and only if there exists a probability measure $R$ such that $Q = (1-\epsilon)P + \epsilon R$.
\end{lemma}
\begin{proof}
First assume there exists a probability measure $R$ such that $Q = (1-\epsilon)P + \epsilon R$. For any measurable set $A$, we have
\[ |Q(A) - P(A)| = |(1- \epsilon)P(A) + \epsilon R(A) - P(A)| = \epsilon |R(A) - P(A)| \leq \epsilon, \]
where the last line follows from the fact that $P(A), R(A) \in [0,1]$. By the definition of total variation distance, we have $\tv(P, Q) \leq \epsilon$.

Now assume that $\tv(P, Q) \leq \epsilon$. By assumption, we have
$Q(A) \geq (1-\epsilon) P(A)$
for any measurable subset $A$. Thus, we can define the measure $R = \frac{1}{\epsilon}\left( Q - (1- \epsilon)P \right)$, and observe that $R$ is a probability measure. Thus,
\[  (1- \epsilon)P + \epsilon R = (1-\epsilon)P + Q - (1-\epsilon)P = Q.  \]
\end{proof}

We now turn to the proof of Proposition~\ref{prop:tv-mixture-model}. First, assume that there exist measures $Q_k$ such that $Q = \sum_k \pi_k P_k$ and $\tv(P_k, Q_k) \leq \epsilon$ for all $k=1,\ldots, K$. Then for any measurable set $A$, we have
\[ |Q(A) - P(A)| = |\sum_{k} \pi_k(Q_k(A) - P_k(A))| \leq \sum_k \pi_k |Q_k(A) - P_k(A)| \leq \epsilon. \]
By the definition of total variation distance, this implies $\tv(P, Q) \leq \epsilon$.

Now assume that $\tv(P, Q) \leq \epsilon$. By Lemma~\ref{lem:tv-decomposition}, this implies the existence of a probability measure $R$ such that $Q = (1-\epsilon)P + \epsilon R$. Now define the mixture probability measures $Q_k = (1-\epsilon)P_k + \epsilon R$. We first observe that
\[ \sum_k \pi_k Q_k =  \sum_k \pi_k ((1-\epsilon)P_k + \epsilon R ) = (1-\epsilon) P_k + \epsilon R = Q. \]
Finally, by Lemma~\ref{lem:tv-decomposition}, we have $\tv(P_k, Q_k) \leq \epsilon$. \qed
\fi

\end{document}
