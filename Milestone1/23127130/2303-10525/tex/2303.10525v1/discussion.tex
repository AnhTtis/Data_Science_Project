%auto-ignore
In this paper, we introduced the optimistically weighted likelihood (OWL) methodology, motivated by brittleness issues arising from misspecification in statistical methodology based on standard likelihoods. On the theoretical side, we established the consistency of our approach and showed its asymptotic connection to the coarsened inference methodology. We also proposed a feasible alternating optimization scheme to implement the methodology and demonstrated its empirical utility on both simulated and real data.


The OWL methodology opens up several interesting future directions. One practical open problem is how to scale to larger datasets. As a weighted likelihood method, OWL requires solving for a weight vector whose dimension is the size of the dataset. While we can solve the resulting convex optimization problem for thousands of data points, the procedure becomes significantly more complicated when the size of the dataset exceeds computer memory. How do we maintain a feasible solution, i.e. one that lies in the intersection of the simplex and some probability ball, when the entire vector cannot fit in memory?

Another practical question is how to apply the OWL approach in more complex models; for example, involving dependent data. This may be relatively straightforward for models in which the likelihood can still be written in product form due to conditional independence given random effects. This would open up its  application to nested, longitudinal, spatial and temporal data, as random effects models are routinely used in such settings.

On the theoretical side, there remain important gaps to fill between our theory and practice. In particular, our theory for continuous spaces required the use of kernels to smooth our approximations to the OKL. However, in practice, we observed that generally no such smoothing is necessary, and we may simply use the finite space approximation to the OKL to achieve excellent results. Resolving this gap would have important implications for the general applicability of the OWL methodology.

Finally, a very interesting question is how to choose the corruption parameter $\epsilon \in (0,1)$ for our procedure. In our simulation study (\Cref{sec:simul}) we chose $\epsilon$ in two ways: using the data based tuning procedure in \Cref{sec:tune-epsilon}, and using knowledge of the true corruption fraction at the population level, both of which lead to an equally robust performance. In contrast, for our application (\Cref{sec:application}) we explored model fit on a range of $\epsilon$ values. The population setup based on a general distance $\D$ (recall \Cref{sec:okl} focused on $\D=\tv$) can provide more insights on how the choice of $\epsilon$ affects parameter inference. The OWL procedure interpolates between the maximum-likelihood estimate $\argmin_{\theta \in \Theta} \KL(P_0|P_\theta)$ at $\epsilon=0$ and minimum-distance estimate $\argmin_{\theta \in \Theta} \D(P_0, P_\theta)$ at $\epsilon = \varepsilon_0$, where $\varepsilon_0 = \min_{\theta \in \Theta} \D(P_0, P_\theta)$, while choosing much larger values of $\epsilon$ will eventually degrade performance.  Is it possible that OWL with well-chosen $\epsilon \in [0, \varepsilon_0]$ can get us the best of both worlds---efficiency from the likelihood and robustness from the distance? Some of these properties can be understood by studying asymptotic properties of the parameter estimates from OWL, which is an open direction for future work.
%In principle, if we have access to the OKL $I_\epsilon(\theta)$ based on distance $\D$, the value $\epsilon = \varepsilon_0$ can be identified as the smallest value of $\epsilon$ such that $\min_{\theta \in \Theta} I_\epsilon(\theta) = 0$. However, determining $\varepsilon_0$ this way in practice based on our estimator $\hat{I}_\epsilon$ can be unstable.