%auto-ignore
Likelihood-based inference remains a broad and principled workhorse for conducting modern statistical analyses. %\jk{Refs?}
%
Indeed, when the likelihood is correctly specified, there is arguably no substitute for likelihood-based inferences \cite[see e.g.][]{zellner1988optimal}.
%
%In the context of Bayesian methods, this has been formally shown
%
Crucially, this need no longer be true if the statistical model is misspecified: in this setting, likelihood-based inferences may be misleading and lead to undesirable outcomes \citep{huber1964robust, tsou1995robust, huber2011robust, rousseeuw2011robust}.
%However, a key concern is the possibility of misspecification.
This has motivated a broad battery of methods for model comparison and goodness-of-fit assessment \cite[see e.g][]{huber2012goodness, claeskens2015model}.
%
%\jk{refs?}
%
A key intent of any such analysis is to verify that the assumed likelihood is indeed consistent with the data at hand---so as to pre-emptively avoid model misspecification and the unreliable inferences this can lead to.


%
%\jk{Ref to Bernardo?}
%
Yet, even with substantial care in model assessment, some amount of model misspecification is inevitable.
%
Unfortunately, even small degrees of model misspecification can have dire consequences in certain settings; a problem we refer to as {\em brittleness}.
%
Brittleness can occur in various applications, including in high dimensional problems \cite[e.g.][]{bradic2011penalized, bradic2016robustness,zhou2020detangling}, in the presence of outliers and contaminating distributions \cite[e.g.][]{huber1964robust,huber2011robust}, or for mixture models \cite[e.g.][]{markatou2000mixture, miller2018robust, diakonikolas2020robustly}.
%\jk{Should try to be a bit more specific about the problems here}
%
This article is focused on robustifying likelihoods to avert such brittleness.
%
%Formally, this is achieved by  allow small model misspecification to protect against brittleness.

\Cref{fig:simple-failure} illustrates the problem of brittleness and our proposed solution for the setting of model-based clustering with kernel mixture models.
%
Here, the vast majority of the data is perfectly modeled by a mixture of two well-separated Gaussians.
%
However, a small fraction of the data have been corrupted, and are instead drawn uniformly from the space between the two modes.
%
As the left hand panel demonstrates, the maximum likelihood estimate (MLE) accommodates the relatively small corruption by sacrificing a good fit on the much larger uncorrupted fraction of the data.
%
Our proposal is to instead maximise an optimistically weighted likelihood (OWL) 
\footnote{The code for the OWL methodology and all of the analysis in this paper can be found online at \url{https://github.com/cjtosh/owl}}, which re-weights the likelihood terms according to how reasonable the data are under the posed model.
%
The left hand panel shows that maximising OWL rectifies the issue of MLE, and the right hand panel shows the corresponding weights that achieve this outcome.
%


\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{figures/simple_mog_combo.pdf}
	\caption{An example illustrating failures of  maximum likelihood estimation (MLE) for misspecified models.
		The data were generated from an equally weighted mixture of two Gaussians with means $-2.5$ and $2.5$, and with standard deviation $1/4$.
		%
		5\% of the data were corrupted by replacement with i.i.d uniform$(-1,1)$ noise.
		%
		\textbf{Left}: The red solid line denotes the MLE found using expectation maximization. The green dashed line denotes the solution found by maximizing our optimistic weighted likelihood.
		The orange solid line denotes the uncorrupted density.
		%
		\textbf{Right}: Depicted are the weights on the individual likelihood terms learned by the OWL methodology.}
	\label{fig:simple-failure}
\end{figure}

%{\bf INSERT a concrete example with a corresponding figure showing problems with assuming the likelihood is correct}.

%{\bf Review other literature on robustified likelihoods that could be used to potentially address the concrete problem just introduced}

%{\bf Briefly review the C-likelihood introduced by Miller and Dunson as the perspective taken in this paper, but note the issues with the KL neighborhoods. Keep things non-technical.}

%{\bf Provide an overview of the key contributions of the current paper to make very transparent what we add.}

%{\bf Outline what is covered in the different sections}

%{\bf (DD - Looking over the below, it is currently a total mess in all the sections. We need to stop fucking around and finalize the storyline and rewrite with this storyline in mind. Ideally we look at the method that is currently being implemented with good results and build up a clear justification for this method and put it clearly in the context of the literature while including competitors in simulated and real data experiments. It seems that the TVD case should be the focus as MMD complicates the story and doesn't have as good results in practice, but can we have a very clear motivation for the approach being applied starting from the coarsest likelihood philosophy?)}


%\jk{To achieve: Motivate the problem of parameter inference under small misspecification.  We don't want to throw away the likelihood entirely for Gibbs posterior/loss-based approaches \cite{bissiri2016general}; further we don't want to focus on inference about the $\KL$-minimum point either (sandwich estimator/power posteriors); Minimum divergence estimation is attractive, but computationally difficult.

%Review existing literature on robust inference: robust sampler \cite{bhatia2019bayesian}, computationally efficient parameter inference in specialized models \cite{diakonikolas2017being, diakonikolas2019outlier,diakonikolas2019outlier,diakonikolas2020robustly}, Divergence based approaches \cite{jewson2018principles}.  C-Bayes/ABC paradigm is an appealing Bayesian framework for robustifying inference   \cite{miller2018robust, bernton2019approximate} that does not require calibrating a loss. We develop an asymptotic approach to computing the C-Bayes likelihood for different choice of metrics $D$.}




The origin of brittleness is intimately tied to the foundations of statistical science.
%
Specifically, much of statistical theory and methodology assumes a domain expert capable of precisely modelling real-world phenomena.
%
This idealised setting is sometimes referred to as the \textit{M-closed} world \citep[see][]{Bernardo}; and assumes that for a fixed but unknown parameterisation of the likelihood, we can recover the data-generating mechanism accurately.
%
Conveniently for the statistician, this leaves computation and estimation as the only problems to be resolved.
%
%This simplifying assumption is sometimes referred to as the \textit{M-closed} world \citep[see][]{Bernardo}.
%
Unfortunately, the real world is not M-closed: even the most seasoned scientists often struggle with developing models that are flawless descriptions of all aspects of the data they seek to model.
%
As a result, inferences built on this assumption are often brittle \cite[see e.g.][]{lindsay1994efficiency,Muller2013, SafeLearning},
%
%While this insight is scarcely new,
and the advent of large scale and high-dimensional data has only exacerbated this problem.



%There is a substantial existing literature developing methods that seek to account for likelihood misspecification in various ways.
%
%\jk{Refs?}
%
Perhaps the oldest strategy for addressing this is to try to explicitly model the complexities of real data settings---such as outliers and data contamination---by using a more flexible model.
%
Often, the corresponding likelihood functions correspond to mixture models, models with heavier tails, or semiparametric and nonparametric extensions.
%
%\jk{Ref to some of these works}
%
However, as one increases the complexity of the likelihood to combat model misspecification, one creates a host of new challenges.
%
These include
decreased interpretability, problems with parameter identifiability, and increased computational complexity.
%
In addition, even with a more complex likelihood, some degree of model misspecification remains inevitable.
%
As a result, even highly flexible likelihoods do not provide guaranteed protection against brittleness.


%\jk{M-estimation vs likelihood-based approaches; why we want a likelihood-based procedure \& some prior work that has been done on deriving other robust estimators that are not directly related to likelihoods}

%
%\jk{[REFS to work on robustness/misspecification here]}
%
This has led to a renewed interest in robust statistics: rather than trying to adjust the likelihood function, many modern approaches instead derive guarantees that continue to hold under various forms of misspecification.
%
This includes a large potpourri of contributions, ranging from model- and algorithm-specific methods \cite[e.g.][]{bhatia2019bayesian, diakonikolas2017being,diakonikolas2019outlier,diakonikolas2020robustly}
%
to general frameworks of obtaining robustified inferences  \cite[e.g.][]{bissiri2016general, jewson2018principles, lyddon2018Randomised, Knoblauch2019, lecue2020robust}.
%
Much of the methodological work in this area has taken one of three approaches: distance-based estimation \cite{barp2019minimum, RBOCPD, Cherief-Abdellatif2019, briol2019statistical, Matsubara2021,Dellaporta2022, alquier2022estimation, cherief2022finite}, novel means of uncertainty quantification with a Bayesian flavour \cite{lyddon2018Randomised, Huggins2020,  pompe2021asymptotics,fong2021martingale}, or modified   likelihood functions \cite{hooker2014bayesian, Ghosh2016, field1994,windham1995,markatou1997, hadi1997, markatou1998, dupuis2002}.
%
%\jk{what about median-of-means?}
%
We will propose a novel methodology that roughly falls into this last category and focuses on finding robust parameter estimates based on a modification of  the likelihood function.
%
This means that our methodology is universally applicable to both frequentist and Bayesian methods, and is easier to interpret than distance-based methods.
%, and because we are interested in robustness to contamination and outliers (not to overconfidence, which is in some sense an orthogonal concern).
%

A particular subclass of modified likelihoods that have been of great practical interest are (locally) weighted likelihoods.
%
For a likelihood function $f$, the weighted likelihood replaces $L(x_{1:n}|\theta) =
\prod_{i=1}^n f(x_i|\theta)$ by the weighted counterpart
%
\begin{IEEEeqnarray}{rCl}
	L(x_{1:n}|\theta) & = &
	\prod_{i=1}^n f(x_i|\theta)^{w_i}
	\nonumber
\end{IEEEeqnarray}
%
for a collection of weights $\{w_i\}_{i=1}^n$ that typically are allowed to depend on the data as well as $\theta$.
%
Weighting schemes like this have been proposed for local likelihood methods that interpolate between parameter and density estimation  \citep{hunsberger1994, copas1995, hjort1996, loader1996, eguchi1998}, for improving predictive performance \citep{shimodaira2000}, as well as in the context of Bayesian methods \citep{newton1994, greco2008,agostinelli2013, Lyddon2018}.
%
Motivated by robustness, re-weighting schemes first arose in the context of linear regression \cite[see e.g.][]{green1984, gervini2002, marazzi2004}.
%
The first mentions of a general construction to enhance the robustness of arbitrary likelihoods are due to \cite{beran1981} and \cite{pregibon1982}.
%
While the weighting schemes proposed in \cite{beran1981} are computationally unattractive, they inspired a range of alternative schemes that also aim at robustness and are more computationally tractable \citep{field1994,windham1995,markatou1997, hadi1997, markatou1998, dupuis2002}.
%
The most important of these are density-weighted approaches \citep{windham1995} which coincide with robust distance-based estimation methods for a particular sub-class of statistical models \citep{basu1998}, weights based on the cumulative density function and quantiles \citep{field1994, hadi1997}, and weights based on Pearson residuals \citep{markatou1997, markatou1998}.
%

While there are some asymptotic results for certain weighted likelihood constructions  \citep{lenth1987, hu1997, wang2004asymptotic, wang2005selecting, majumder2021}, most of these  only hold if models are well-specified and under conditions that are generally hard to verify.
%
Furthermore, while various weighted likelihood schemes have been proposed for robustness, the nature of this robustness often lacks interpretability.
%
Beyond that, most weighting schemes are not universally applicable and rely on additional knowledge that may not be generally available.
%
For example, the approaches of \cite{field1994, hadi1997} rely on the availability of cumulative density functions for the statistical model underlying the likelihood function.
%
Similarly, \cite{markatou1997, markatou1998} require Pearson residuals, which are generally difficult to compute on continuous spaces.
%---whose magnitude in the continuous setting crucially depends on a bandwidth hyperparameter. \jk{If we actually end up using kernel smoothing for our methodology (rather than just for the theory), may want to list another example here.} \md{We are using kernels to a) compute the density estimate, and b) to search for a smooth set of weight vectors.}


In this paper, we introduce the optimistically weighted likelihood (OWL) methodology.
%
This approach does not suffer the drawbacks of these previous proposals:
%
Specifically, OWL is provably well-behaved in the asymptotic regime, guarantees robustness by means of an interpretable construction, and is universally applicable.
%
The key intuition behind our approach is that parameter inference based on the weighted likelihood is equivalent to inference based on the standard-likelihood for a re-weighted empirical distribution. 
%
%The OWL scheme hinges on the idea of constructing weights that (a) are representative of the underlying distribution up to some distance $\epsilon$ but (b) are closely aligned with an element of the model class. 
The  weights are then chosen so that the re-weighted empirical measure (a) is within an $\epsilon$-neighborhood of the observed data distribution with respect to a suitable distance all while (b) being close to an element of the model class. 
%
The robustness of OWL  follows from the choice of distance in (a). In particular, OWL inherits the particular robustness properties of the distance it is based on.
%
Meanwhile, the optimism of (b) allows OWL to recover parameters that are $\epsilon$-close to the data generating distribution, if such parameters exist. 
%
In this paper, we choose the total variation (TV) distance for step (a):  It is easy to interpret, is a proper metric, and is robust to outliers and contamination \cite[see e.g.][]{yatracos1985rates}.
%
While it is conceptually easy to modify our methodology for alternative distances, there may be substantial computational hurdles.

Although directly inferring optimistic weights that satisfy (a) and (b) in the TV setting
%and the corresponding maximum weighted likelihood parameters 
is intractable, we develop an alternating optimization scheme that can be easily implemented in a variety of settings.
%
Empirically, the weights recovered by this procedure behave as theory would predict: they assign near-zero values to observations that are in strong disagreement with the likelihood model.
%
The right panel of \Cref{fig:simple-failure} illustrates this: as theory predicts, the weights are very small on the region containing the corrupted data, and much higher on regions that produce a good fit for the posed likelihood model.


We also show that OWL is intimately connected with the principle of \textit{coarsened inference} \citep{miller2018robust}.
%
%The coarsened likelihood is a natural modification of the standard likelihood that posits a generative model in which idealized data is first generated by the model but then corrupted in such a way as to leave the empirical distribution of the observed data within distance $\epsilon$ of the idealized data. 
%
The coarsened likelihood is a natural generalization of standard likelihood; and posits that the empirical distribution of idealized data generated from the assumed statistical model is within a discrepancy $\epsilon$ of the empirical distribution of the observed data---rather than the two being equal, which is the assumption implicitly underlying inference for well-specified models. 
%
%As a result, coarsened likelihoods allow for small but arbitrary levels of misspecification as part of the data generating process.
Despite its natural interpretation, coarsened likelihoods are difficult to evaluate or approximate directly, except in the special case when the discrepancy is chosen to be the Kullback Leibler (KL) divergence \citep{miller2018robust}. 
While directly computing coarsened likelihoods is intractable in finite samples, we use techniques from large deviation theory \citep{demboLargeDeviationsTechniques2010} to show that coarsened likelihoods converge to their corresponding optimistically weighted likelihood for a broad class of discrepancies.
%
This result provides an alternate motivation of OWL as a tool for approximating coarsened likelihoods.

%
%This re-weighting corresponds to the most optimistic interpretation of the statistical model in a neighbourhood around the empirical measure constructed from observed data.
%
%By optimistically re-weighting in this way, OWL guards against brittleness.
%
%While the coarsened inference of  \cite{miller2018robust} is similarly motivated, its application is limited to the Bayesian setting and relies on neighbourhoods  constructed via Kullback-Leibler (KL) divergence balls around the empirical measure.
%
%\md{\cite{miller2018robust} does not talk about ``perturbations''. Perhaps mention that Miller-Dunson used power-likelihoods?}
%
%Though this choice of neighbourhood is computationally attractive and leads to a novel interpretation of so-called power posteriors \cite[see e.g.][]{holmes2017assigning}, it has a key drawback: the KL divergence is not a \textit{robust} measure of discrepancy.
%
%This means that it does not define a proper distance metric, which makes the resulting neighbourhoods harder to interpret.
%
%Even more worryingly, the KL divergence is not robust to outliers and contamination.
%
%This means that coarsening and optimistically re-weighting based on the KL divergence cannot guard against  brittleness.


%In the current paper, we therefore focus on optimistically weighted likelihoods (OWLs) using neighbourhoods based on the total variation distance (TVD).
%
%While doubtless conceptually more elegant than the KL divergence, constructing OWLs with the TVD brings a series of computational challenges.
%
%We overcome these issues by showing that intriguingly, searching for the most optimistic interpretation of the data relative to the posed model amounts to locally weighting the likelihood function.
%
%Though the corresponding weights are generally intractable, we derive an optimisation scheme that can determine them quickly and reliably.
%
%Empirically, the weights behave as theory would predict: they assign near-zero values to observations that are in strong disagreement with the likelihood model.
%
%The right panel of \Cref{fig:simple-failure} illustrates this: as theory predicts, the weights are very small on the region containing the corrupted data, and much higher on regions that produce a good fit for the posed likelihood model.
%

% {\bf Point to the picture/illustration from earlier to demonstrate this!}

% Our approach is unique in that it provides a principled and interpretable way of determining weighted likelihoods.
% %
% Unlike previous approaches, our method is applicable regardless of the precise likelihood in question.
% %
% In contrast, more traditional weighting schemes typically rely on additional information that is only available in a narrow range of settings (such as the model's cumulative distribution function \cite{field1994, hadi1997}), or are sensitive to uninterpretable hyperparameters \cite{windham1995, markatou1998, basu1998}.
% %
% Most importantly, they are also derived in various ways, and in particular without the interpretation of OWL.
% %
% As a result, they can only guard against very particular forms of model misspecification; remaining brittle towards others.
%
% In contrast, OWL is derived  from a clear and interpretable principle that guards against brittleness regardless of the particular form of misspecification.
%
%\jk{Some more reasons for why what we are doing is cool/useful.}
%

In summary, our contribution is a robust OWL methodology based on neighborhoods of the data measured in TV distance.
%
While computing the optimal weights is generally intractable, we propose an easily implementable alternating optimization scheme to approximately solve this problem.
%
Beyond that, we also demonstrate an asymptotic equivalence between  OWL  and  coarsened likelihoods.
%
%To facilitate computation, we provide a suite of fast and reliable optimisation schemes for the weights.
%
The resulting inferences exhibit desirable properties, both in theory and practice.

The remainder of the paper discusses these findings as follows:
\Cref{sec:methodology} presents the OWL methodology and the associated alternating optimization scheme.
\Cref{sec:comp} discusses the computational details of each of the steps in the alternating optimization procedure.
\Cref{sec:coarsened-inference} demonstrates the asymptotic connection between OWL and coarsened inference.
\Cref{sec:simul} presents a suite of simulation experiments for the OWL methodology in both regression and clustering tasks.
\Cref{sec:application} applies the OWL methodology to a clustering application for single-cell RNAseq data.
 \Cref{sec:micro-credit} uses the OWL methodology to estimate the average intent-to-treat effect in the micro-credit study by  \cite{angelucci2015microcredit}, whose inference  was shown to be brittle to the removal of an handful of observations \citep{broderick2020automatic}.

