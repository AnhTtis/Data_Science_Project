%auto-ignore

We now turn to applications of the optimistic weighted likelihood methodology in simulated examples with artificially-injected corruptions. In each simulation, we considered two methods for choosing the points to corrupt: (i) {\em max-likelihood corruption} where we fit a maximum likelihood estimate to the uncorrupted data and select the points with the highest likelihood; and (ii) {\em random corruption} where we choose the points to corrupt uniformly at random. For clarity and space, we only present the results for max-likelihood corruptions in this section, and we defer the results for randomly-selected corruptions to \Cref{app:more-simulations}. Unless otherwise stated, the $w$-step for the OWL solutions was computed using the finite approximation to the OKL in \cref{eq:finiteokle}. 

In all comparisons, OWL refers to our methodology with the data based choice of the corruption fraction $\epsilon$ as described in \Cref{sec:tune-epsilon}, while OWL ($\epsilon$ known) refers to our methdology with $\epsilon$ equal to the true level of corruption in the data. For the other robust estimation methods requiring hyperparameters, we either used commonly-accepted values (as in Huber regression), or we set them based on knowledge of the true corruption fraction (as in RANSAC regression). In all settings, we measured the performance of the maximum-likelihood estimator (MLE) that was fit using the entire uncorrupted dataset as a gold-standard benchmark.

\subsection{Gaussian simulations}
\label{sec:kern-vs-unkern-simulations}

To investigate the performance of the OWL methodology with and without kernelization, we fit a multivariate normal distribution with mean $\mu \in \R^p$ and covariance matrix $\Sigma \in \R^{p \times p}$. We make two observations about this setting. First, maximizers of the weighted log-likehood can be computed in closed form, removing optimization difficulties. Second, this simple problem setting falls squarely within the i.i.d. framework of \Cref{sec:methodology}.

We generated synthetic datasets for dimensions $p=2$, 25, and 50, by drawing $\mu^\star_1,\ldots, \mu^\star_p$ independently from the uniform distribution over $[-10,10]$ and drawing the uncorrupted data points independently from the spherical Gaussian $\Ncal(\mu^\star, I_p)$, where $\mu^*=(\mu^*_1,\ldots, \mu^*_p)$. The corrupted data points had coordinates drawn independently from the uniform distribution over $[-10,10]$. The total size of the training set was set to 200. We measured the mean-squared error reconstruction of $\mu^\star$, i.e. for parameter estimates $\hat{\mu} \in \R^p$ and $\hat{\Sigma}\in \R^{p \times p}$:
\[ \text{mean-parameter-mse}(\hat{\mu}, \hat{\Sigma}; \mu^\star) = \frac{1}{p} \|\hat{\mu} - \mu^\star \|_2^2 = \frac{1}{p}  \sum_{i=1}^p (\hat{\mu}_i - \mu^\star_i)^2.  \]
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/gaussian_wmle_max_corruption.pdf}
    \caption{Gaussian results for max-likelihood corruptions. Dashed black line denotes average performance of MLE on full uncorrupted dataset. Shaded regions denote 95\% confidence intervals over 50 random seeds.}
    \label{fig:gaussian-max}
\end{figure}
For the kernelized OWL procedure, we used the Gaussian/RBF kernel: $K_h(x,y) = (2\pi h )^{-p/2} \exp(- \frac{1}{2h}\|x - y \|^2)$. We adaptively set the bandwidth by searching over a fixed grid and using the final parameter's OKL estimator in \cref{eq:okle} as the criterion to be minimized. To focus only on the role of kernelization, we set the corruption fraction $\epsilon$ equal to the true level of corruption in the dataset.

\Cref{fig:gaussian-max} shows the results for the Gaussian simulations. Across the range of dimensions, both kernelized and un-kernelized OWL perform much better than MLE. Moreover, the performance of OWL without kernelization is comparable to that of OWL with kernelization.
%and sometimes is even better (this is more clearly seen with the random corruptions in \Cref{app:more-simulations}). 
This is somewhat surprising, as our theory currently cannot explain why one should be able to run OWL in continuous spaces without some form of density estimation. 
%
For the rest of this section, we will only consider OWL without kernelization.

\subsection{Regression simulations}

We applied OWL to two regression settings: linear regression and logistic regression. 

\paragraph{Linear regression.} We considered a homoscedastic model with parameters $b, \sigma^2 \in \R$, $w \in \R^p$ and observations $(x_i, y_i) \in \R^{p + 1}$ assumed to follow the distribution
\[ y_i \sim \Ncal( \langle x_i, w \rangle + b , \sigma^2) . \]
The maximizers of the weighted likelihood can be computed in closed form, as in the unweighted setting. In addition to the standard least-squares maximum likelihood estimate (MLE), we compared with (1) Ridge regression, with L2 penalty chosen via cross validation; (2) Huber regression, using the Huber penalty of 1.345~\citep{huber1981robust};
%\jk{Can we point to a paper/reference justifying this number/heuristic?}
and (3) 
Random Sample Consenus (RANSAC) linear regression.
%As a gold-standard baseline, we also computed the MLE over the entire uncorrupted training set.


We compared these methods on two datasets. The first is a simulated dataset with 10-dimensional i.i.d. standard normal covariates. The ground-truth regression coefficients $w^\star_1,\ldots, w^\star_{10}$ were drawn independently from 
$N(0,4)$, and the residual standard deviation was 
$\sigma=1/4$. The training set consisted of 1,000 data points. For the test set, we drew 1,000 new data points and computed the MSE on the underlying response value, i.e.
\[ \text{mse}(x_1, \ldots, x_n ; \hat{w}, \hat{b}) \ = \  \frac{1}{n} \sum_{i=1}^n (\langle w^\star, x_i \rangle - \langle \hat{w}, x_i \rangle + \hat{b} )^2. \]
The second dataset was taken from a quantitative structure activity relationship (QSAR) dataset compiled by \cite{olier2018meta} from the ChEMBL database. It consists of 5012 chemical compounds whose activities were measured on the epidermal growth factor receptor protein erbB1. The activities were recorded as the negative log of the chemical concentration that inhibited 50\% of the protein target, i.e. the pIC$_{50}$ value. Each compound had 1024 associated binary covariates, corresponding to the 1024-dimensional FCFP4 fingerprint representation of the molecule~\citep{rogers2010extended}. We used PCA to reduce the dimension to 50. For every random seed, we computed a random 80/20 train/test split. The test MSE on this dataset is the standard MSE over the test responses.  In both datasets, for each data point selected to be corrupted, we corrupted the responses by fitting a least squares solution and observing the residuals: if the residual is positive, we set the response to be $3v$ where $v$ is the largest absolute value observed value in the training set responses, otherwise setting it to $-3v$.
%Starting with the data described above, we considered two types of methods for choosing the points to corrupt: (i) {\em Random corruption} where we choose the points to corrupt uniformly at random; (ii) {\em Max-likelihood corruption} where we fit a least squares solution to the (uncorrupted) training data and select the points with the highest likelihood. We corrupt the responses by fitting a least squares solution and observing the residuals: if the residual is positive, we set the response to be the largest observed value in the training set, otherwise setting it to the smallest value.

\Cref{fig:linear-regression} shows the results of the linear regression simulations for the max-likelihood corruptions. Across both datasets, we see that OWL is competitive with the best of the robust regression methods, whether that method is RANSAC or Huber regression. 
%It is possible that the performance of RANSAC on the QSAR data (or of Huber regression on the simulated data) could be improved by a more careful choice of hyperparameters, but it is worth noting that OWL's performance remains competitive with no change to its (interpretable) hyperparameter.
\begin{figure}
    \centering
    % \includegraphics[width=0.9\textwidth]{figures/lin_corruption.pdf}
    \includegraphics[width=0.9\textwidth]{figures/lin_max_corruption.pdf}
    \caption{Linear regression results for max-likelihood corruptions. Dashed black line denotes average performance of MLE on full uncorrupted training set. Shaded regions denote 95\% confidence intervals over 50 random seeds.}
    %\jk{Adjust fontsizes in this and all other plots  (looking really good otherwise)}
    %\jk{Bottom right plot: I don't see the RANSAC MLE on there? Is it missing, or do we need to adjust the legend?}
    \label{fig:linear-regression}
\end{figure}

\paragraph{Logistic regression.} For the logistic regression setting, we have parameters $b, \sigma^2 \in \R$, $w \in \R^p$ and observations $(x_i, y_i) \in \R^{p + 1}$ assumed to follow the distribution
\[ y_i \sim \text{Bernoulli} \left( \frac{1}{1 + \exp\left( -\langle x_i, w \rangle - b \right)} \right) . \]
In addition to the standard maximum likelihood estimate (MLE), we compared against two other baselines:
(i) {\em L2-regularized MLE}, with L2 penalty chosen via cross validation.
(ii) {\em Random Sample Consenus (RANSAC)} logistic regression.
%We again used the MLE on the entire uncorrupted training set as a gold-standard baseline.  

We compared these methods on three datasets. The first is a simulated dataset using the same parameters as the linear regression setting. The training labels are created according to the generative model. For test accuracy, we computed the zero-one loss over the true sign-values, i.e.
\[ \text{accuracy}(x_1, \ldots, x_n ; \hat{w}, \hat{b}) \ = \  \frac{1}{n} \#  \left\{ i \, : \, \sgn(\langle w^\star, x \rangle) = \sgn\left( \langle \hat{w}, x \rangle + \hat{b} \right) \right\}. \]

The second dataset is taken from the MNIST handwritten digit classification dataset~\citep{lecun1998gradient}. We considered the problem of classifying the digit `1' v.s. the digit `8,' resulting in a dataset with 14702 data points and 784 covariates,  representing pixel intensities. The third dataset is a collection of 5172 documents from the Enron spam classification dataset, preprocessed to contain 5116 covariates, representing word counts~\citep{metsis2006spam}. For both the MNIST and the Enron spam datasets, we reduced the dimensionality to 10 via PCA and used a random 80/20 train/test split.

\Cref{fig:logistic-regression} shows the results of the logistic regression simulations. Across all datasets, we again see that OWL outperforms the other approaches in the presence of both corruption and misspecification.

\begin{figure}
    \centering
    % \includegraphics[width=0.9\textwidth]{figures/log_corruption.pdf}
    \includegraphics[width=1.0\textwidth]{figures/log_max_corruption.pdf}
    \caption{Logistic regression results for max-likelihood corruption. Dashed black line denotes average performance of MLE on full uncorrupted training set. Shaded regions denote 95\% confidence intervals over 50 random seeds.}
    \label{fig:logistic-regression}
\end{figure}

\subsection{Mixture model simulations}

We applied OWL to two mixture modeling settings: mixtures of spherical Gaussians and mixtures of Bernoulli products. 


\paragraph{Gaussian mixture models.} Recall the standard Gaussian mixture modeling setup: there are a collection of means $\mu_1, \ldots, \mu_K \in \R^p$, standard deviations $\sigma_1, \ldots, \sigma_K > 0$, and mixing weights $\pi \in \Delta^{K}$. Data points $x_i \in \R^p$ are drawn i.i.d. according to
\[ x_i \sim \sum_{k=1}^K \pi_k \Ncal(\mu_k, \sigma_k^2 I_p). \] 
For our simulations, we generated a synthetic dataset of 1000 points in $\R^{10}$ by first drawing $K=3$ means $\mu^\star_1, \ldots, \mu^\star_K$ whose coordinates are i.i.d. Gaussian with standard deviation 2. The standard deviations of the component Gaussians were set to $1/2$, and the mixing weights were uniform. We compared against MLE on the corrupted data. 
%maximum likelihood estimation on both the uncorrupted data (as a gold standard baseline) and on the corrupted data. 
As a metric, we measured the average mean squared Euclidean distance between the means of the fitted model and the ground truth model. To corrupt a data point, we randomly selected half of its coordinates and set them randomly to either a large positive value or a large negative value (here, 10 and -10).

Because the likelihood function of a Gaussian mixture model is non-concave, the EM algorithm is only guaranteed to converge to a local optimum. Thus, for maximum likelihood estimation we used EM with random restarts, choosing the final model to be the one with largest likelihood. For the OWL procedure, we also used random restarts with the alternating optimization algorithm of \Cref{sec:comp}. To choose the final model, we selected the one whose weights $w$ and parameters $\theta$ minimized the OKL estimator of~\cref{eq:finiteokle}. 

The left panel of \Cref{fig:both-mixture} shows the results of the Gaussian mixture model simulations. We see that OWL remains robust against varying levels of corruptions.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/gmm_corruption.pdf}
%     \caption{Gaussian mixture model results. Dashed black line denotes average performance of MLE on full uncorrupted training set. Shaded regions denote 95\% confidence intervals over 50 random seeds.}
%     \label{fig:gaussian-mixture}
% \end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/clustering_max_corruption.pdf}
    \caption{Mixture model results for max-likelihood corruptions. Dashed black line denotes average performance of MLE on full uncorrupted training set. Shaded regions denote 95\% confidence intervals over 50 random seeds.}
    \label{fig:both-mixture}
\end{figure}

\paragraph{Bernoulli product mixture models.} Consider the following model for $p$-dimensional binary data: there are a collection of probability vectors $\lambda_1, \ldots, \lambda_K \in [0,1]^p$ and mixing weights $\pi \in \Delta^{K}$. Each data point $x_i$ is drawn i.i.d. according to the process
\begin{align*}
z_i &\sim \text{Categorical}(\pi) \\
x_{ij} &\sim \text{Bernoulli}( \lambda_{z_i j} ) \text{ for } j=1,\ldots, p.
\end{align*}
For our simulations, we generated a synthetic dataset of 1000 points in $\{0,1\}^{100}$ by first drawing $K=3$ means $\lambda^\star_1, \ldots, \lambda^\star_K$ whose coordinates are i.i.d. from a Beta$(1/10, 1/10)$ distribution. The mixing weights were chosen to be uniform over the components. 
%As in the Gaussian mixture modeling setup, we compared against the maximum likelihood estimator on both the uncorrupted and the corrupted data. 
As a metric, we measured the average mean $\ell_1$-distance between the $\lambda$ parameters of the fitted model and the ground truth model.

% Given parameters $\lambda_1, \ldots, \lambda_K \in [0,1]^K$ and mixing weights $\pi \in \Delta^K$, the \emph{co-occurrence matrix} $C \in [0,1]^{p \times p}$ is given by
% \[ C_{ij} = P(x_i = x_k | \lambda, \pi) = \sum_{k=1}^K \pi_k \lambda_{ki} \lam_{kj}. \]
% As a metric, we computed the co-occurrence matrix $C$ of the ground-truth parameters and the co-occurrence matrix $\hat{C}$ of the estimated parameters and calculated the average $L1$-distance: $\frac{1}{p^2} \sum_{ij} |C_{ij} - \hat{C}_{ij}|$.


To corrupt a data point, we flipped each zero coordinate with probability 1/2. The right panel of \Cref{fig:both-mixture} shows the results of the Bernoulli mixture model simulations. We see that OWL remains robust against varying levels of corruptions.


% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/bmm_corruption.pdf}
%     \caption{Bernoulli mixture model results. Dashed black line denotes average performance of MLE on full uncorrupted training set. Shaded regions denote 95\% confidence intervals over 50 random seeds.}
%     \label{fig:bernoulli-mixture}
% \end{figure}