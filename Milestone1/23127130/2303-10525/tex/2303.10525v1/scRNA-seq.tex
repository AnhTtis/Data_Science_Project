%auto-ignore

%\jk{This is a really cool application. Probably because I am out of my depth, I feel like we could simplify the presentation a little bit by adding sub-sections; as it seems to me that we have two themes: (i) OWL performance against other methods under pollution [as measured by ARI], (ii) OWL as a tool for descriptive statistics [used for determining inliers/outliers]. Maybe we could make this a bit clearer in how we present things?}

In this section, we apply our OWL methodology to a single-cell RNA sequencing (scRNA-seq) clustering problem. The GSE81861 cell line dataset \citep{li2017reference} contains single-cell RNA expression data for 630 cells from 7 cell lines across 57,241 genes. We followed the preprocessing steps of \cite{chandra2020escaping}: we dropped cells with low reads, normalized according to \cite{lun2016pooling}, and dropped uninformative genes with M3Drop~\citep{andrews2019m3drop}. After preprocessing, the dataset contains 531 cells and 7666 genes.  \Cref{tab:cline-breakdown} shows the breakdown of the remaining cells across cell lines. Finally, we used PCA to project down to 10 dimensions. We implemented OWL using a mixture of general Gaussians, $ \sum_{k=1}^K \pi_k \Ncal(\mu_k, \Sigma_k)$, using the same optimization procedure as in the clustering simulations of \Cref{sec:simul}.
\begin{table}
\centering
 \begin{tabular}{||c | c c c c c c c||} 
 \hline
 Cell line & A549 & GM12878 & H1 & H1437 & HCT116 & IMR90 & K562 \\
 \hline
 Counts & 74 & 126 & 164 & 47 & 51 & 23 & 46 \\ 
 \hline
 \end{tabular}
 \caption{Breakdown of samples in GSE81861 dataset by cell line.}
 \label{tab:cline-breakdown}
\end{table}

\subsection{Cluster recovery with OWL}
We measured the ability of OWL to recover the ground-truth clustering of samples. For baseline methods, we compared against maximum likelihood estimation with the same model class and K-means. %As all methods make use of randomization, we ran each method using 30 different random seeds and tracked the performance of the various runs. 
As a metric of cluster recovery, we measured the adjusted Rand index (ARI)~\citep{rand1971objective,hubert1985comparing}. In all our comparisons, we fixed the number of clusters for all methods to be 7, the number of ground truth cell lines.

The left panel of \Cref{fig:ari-comparison} shows the ARI for OWL over a range of values for the $\ell_1$ radius parameter $\epsilon$, where we also display the performance of MLE and K-means for comparison. We see that OWL performs best when $\epsilon$ takes on values between $0.25$ and $0.45$, but generally has reasonable performance when $\epsilon$ is not too large. Moreover, we see that performance of OWL varies smoothly as a function of $\epsilon$, which may reflect the continuity of the OKL function with respect to $\epsilon$ predicted by our theory.

\Cref{fig:umap-baseline-comparison} shows Uniform Manifold Approximation and Projection (UMAP) visualizations of the dataset clustered under the various methods (for one arbitrary run). We see that of all the methods, K-means performs worst by a significant margin. The improved performance of OWL (with $\epsilon=0.25$) over MLE can be mostly attributed to the better resolution the boundary between the K562 and GM12878. However, all methods struggle to identify the IMR90 cell lines as a cluster distinct from K562.


%\Cref{fig:umap-baseline-comparison} shows UMAP visualizations of the dataset clustered under the various methods (for one arbitrary run). We see that of the baselines, K-Means performs worst by a significant margin. MLE performs better than K-Means, but OWL outperforms MLE for the setting of $\epsilon = 0.2$. \jk{Maybe worth 1-2 sentences why it is legitimate to claim this? It seems that none of the methods are really doing all that well (the brown cluster isn't detected correctly by any method, is it?)} These observations also hold in \Cref{fig:ari-comparison} where we have computed the adjusted Rand index (ARI) for each of the methods across the 30 random seeds, tracking both mean performance and 95\% confidence intervals. \jk{Again, might simply be my ignorance of the subject: is ARI a standard thing? Can we give a reference or short description?} In all these comparisons, we fixed the number of clusters for all methods to be 7, the number of ground truth cell lines. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/ari_comp.pdf}
    \caption{Comparison of clustering methods. \emph{Left}: Adjusted Rand index (ARI) over the entire dataset for each of the methods. \emph{Middle}: ARI of inliers for the OWL methods. \emph{Right}: Fraction of data points classified as inliers for the OWL methods.}
    \label{fig:ari-comparison}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/rnaseq_clustering.png}
    \caption{UMAP plots of the GSE81861 dataset under the considered clustering algorithms. Top left displays the ground truth cell lines. For the other panels, colors were selected by maximizing agreement with the ground truth clustering.}
    \label{fig:umap-baseline-comparison}
\end{figure}



       
%\begin{table}
%\label{tab:ari-breakdown}
%\centering
% \begin{tabular}{||c | c c c c c c c||} 
% \hline
%  & K-Means & MLE & OWL ($\epsilon = 0.05$) &  OWL ($\epsilon = 0.1$) & OWL ($\epsilon = 0.2$) & OWL ($\epsilon = 0.3$) & OWL ($\epsilon = 0.4$) \\
% \hline
% ARI & $0.70 \pm 0.01$ & $0.89 \pm 0.00$  & $0.9 \pm 0.01$ & $0.9 \pm 0.02$ & $0.93 \pm 0.01$ & $0.88 \pm 0.03$ & $0.86 \pm 0.02$ \\
% ARI (inliers) &  &  & $0.91 \pm 0.01$ & $0.92 \pm 0.02$ & $0.98 \pm 0.01$ & $0.95 \pm 0.03$ & $0.96 \pm 0.02$ \\
% Fraction of inliers & & & $0.95 \pm 0.00$ & $0.93 \pm 0.00$ & $0.87 \pm 0.00$ & $0.8 \pm 0.00$ & $0.73 \pm 0.00$ \\
% \hline
% \end{tabular}
%  \caption{Adjusted rand index (ARI) for each of the clustering methods.}
%\end{table}

\subsection{Exploratory analysis with OWL}

In some settings, it is desirable to segment a dataset into those data points that are well-described by a model in the class (so-called \emph{inliers}) and those that do not conform well to the model class (\emph{outliers}). One interpretation of the weights that are learned by the OWL procedure is that, subject to the constraint that they are close in TV distance to the empirical distribution, they represent the most optimistic reweighting of the data relative to the model class. Thus, one might suspect that data points with higher weights are inliers and those with lower weights are outliers. Here, we explore inlier/outlier detection with OWL weights by classifying all data points with weights less than $1/n$ (the average value) as outliers, and the remainder as inliers. 


The middle panel of \Cref{fig:ari-comparison} shows the ARI of the OWL procedure when we restrict to the detected inliers. We observe that for all values of $\epsilon$, the ARI is no lower on the selected inliers than on the whole dataset, and in some cases is significantly higher. This suggests that the OWL procedure identifies a `core' set of points that are both well-described by a mixture of Gaussians as well as aligned with the ground truth clustering. The right panel of \Cref{fig:ari-comparison} shows the fraction of data points that are classified as inliers. Although it is theoretically possible for the OWL weights to classify anywhere from 1 to $n-1$ points as outliers for any value of $\epsilon$, we see that the fraction of outliers is relatively small for low values of $\epsilon$ and only increases gradually as $\epsilon$ increases. 

%The weights that are learned by the OWL procedure can be viewed as indicators of how much the model views a data point as an inlier versus an outlier, where a high weight indicates inlier. As the weights in the OWL procedure are restricted to sum to the number of data points, a natural threshold is the value 1. \Cref{fig:ari-comparison} shows the ARI of the OWL procedure when we restrict to the detected inliers \jk{We haven't explained how we formally determine what an inlier is here (i.e., what's our cutoff?); if we explain it in the appendix we should point to it and if we don't do it there, then I think we should explain it here}. It also displays the fraction of data points labeled as inliers. We can see that for all values of $\epsilon$, the ARI is higher on the selected inliers than on the whole dataset. This suggests that the OWL procedure identifies a `core' set of points that are both well-described by a mixture of Gaussians as well as aligned with the ground truth clustering.


\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/rna_elbow.pdf}
    \caption{\emph{Left}: Weighted log-likehood of the data for various settings of $\epsilon$ and the number of clusters. \emph{Right}: Normalized difference graph of the weighted log-likehood function for select values of $\epsilon$. The kneedle algorithm chooses the value with the largest corresponding normalized difference.}
    \label{fig:k-selection}
\end{figure}



\begin{table}

\centering
 \begin{tabular}{||c | c c c c c c c c c c ||} 
 \hline
  OWL $\ell_1$ radius ($\epsilon$) & 0.05 & 0.15 &  0.25 & 0.35 & 0.45 & 0.55 & 0.65 & 0.75 & 0.85 & 0.95 \\
 \hline
 Selected $K$ & 6 & 6 & 8 & 7 & 7 & 7 & 5 & 4 & 4 & 4 \\
 \hline
 \end{tabular}
  \caption{Number of clusters chosen by the kneedle method as a function of the $\ell_1$ radius $\epsilon$.}
  \label{tab:k-selection}
\end{table}

In many settings, the number of ground truth clusters are not known a priori. A common way to deal with this problem is to plot a metric such as sum-of-squares errors or log-likelihood and look for `elbows' or `knees' in the graph where there are diminishing returns for increasing model capacity. Here, we apply the `kneedle' algorithm~\citep{satopaa2011finding} to the weighted log-likehood produced by the OWL procedure. The kneedle algorithm computes the normalized differences of a given function and selects the value that maximizes the corresponding normalized differences. \Cref{fig:k-selection} shows both the weighted log-likelihoods as well as a subset of the normalized difference graphs. \Cref{tab:k-selection} shows the selected numbers of clusters for various values of $\epsilon$. We see that for relatively small values of $\epsilon$, this results in number of clusters that is close to the ground truth. While for larger values of $\epsilon$, this procedure underestimates the number of clusters in the data. This agrees with the observation in the right panel of  \Cref{fig:ari-comparison} that larger values of $\epsilon$ result in fewer points being identified as inliers, and thus fewer clusters are needed to describe those points.
