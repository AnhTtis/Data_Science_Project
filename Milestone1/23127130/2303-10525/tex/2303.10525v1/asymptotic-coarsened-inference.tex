%auto-ignore

%\jk{I don't know if it's worth pointing this out/elaborating on this further, but another another connection of OWL (at least for sufficiently small $\varepsilon$) is with distance-based estimation. In particular, OWL is as an interpolation between MLE/likelihood-based estimation (for $\varepsilon=0$) and distance-based estimation based on the chosen distance $d$ (for $\varepsilon = \min_{\theta \in \Theta}d(p_{\theta}, p_0)$, in which case the $\epsilon$-ball around $p_0$ touches the set $\{p_{\theta}: \theta \in \Theta\}$ at exactly one point---the point $\theta_d^* = \argmin_{\theta \in \Theta}d(p_{\theta}, p_0)$). So if $\varepsilon \in [0, \min_{\theta \in \Theta}d(p_{\theta}, p_0)]$, OWL with a well-chosen $\varepsilon$ can get us the best of both worlds---efficiency from the log likelihood and robustness from the distance. Maybe this might also be better mentioned in the discussion than here (if at all).}

The development of the OWL methodology in \Cref{sec:methodology} followed from a presumed form of misspecification given by \Cref{ass:misspecification}. %However, this is not the only way to frame misspecification. 
An alternative way to frame and address such misspecifications in a probabilistic framework was proposed by \cite{miller2018robust} who introduced Bayesian methodology centered around the concept of a \emph{coarsened likelihood} defined as
%
\begin{equation}
    \label{eq:clike}
    L_\epsilon(\theta|x_{1:n}) \doteq   \prob_\theta\left(\D(\EmpDist{Z_{1:n}}, \EmpDist{x_{1:n}}) \leq \epsilon\right), 
\end{equation}
where $\D$ is a suitably chosen discrepancy between empirical probability measures.
%
Here, 
$\EmpDist{x_{1:n}} = n^{-1}\sum_{i=1}^n \delta_{x_i}$ denotes the empirical distribution of data $x_{1:n}$, and the probability is computed under $\prob_\theta$---the distribution underlying the artificial data $Z_1, \ldots Z_n \iid P_\theta$ from which the random measure $\EmpDist{Z_{1:n}} = n^{-1}\sum_{i=1}^n \delta_{Z_i}$ is constructed. 
%
The coarsened likelihood implicitly captures the likelihood of a probabilistic procedure in which idealized data are first generated by some model $\prob_\theta$ in the model class under consideration, but are then corrupted in such a way that the discrepancy between empirical measures of the idealized data and the observed data is bounded by $\epsilon$. %\new{Under suitable continuity assumptions on $d$, one can show (Lemma \ref{lem:usual-likelihood} in Appendix) that as $\epsilon \to 0$  coarsened likelihood when multiplied by a proportionality factor depending on $x_{1:n}$ and $\epsilon$, converges to the standard likelihood $L(\theta|x_{1:n}) =\prod_{i=1}^n p_\theta(x_i)$.}

When $\D$ is an estimator for the KL-divergence and an exponential prior is placed on $\epsilon$, \cite{miller2018robust} showed that the Bayes posterior based on $L_\epsilon(\theta|x_{1:n})$ could be approximated  by raising the likelihood to a power less than one in the formula for the standard posterior. 
%
However, to obtain a robustified alternative to maximum likelihood estimation, one may wish to maximize $\theta \mapsto L_\epsilon(\theta|x_{1:n})$ directly for a choice of $\D$ that guarantees robustness (e.g. Maximum Mean Discrepancy or the TV distance).
Such an approach would in general be quite challenging since evaluating \cref{eq:clike} corresponds to computing a high-dimensional integral. 

In this section, we show that for large $n$, the coarsened likelihood  
	can be approximately maximized by using the OWL methodology when $\D$ is an estimator for the TV distance. %More precisely, we show that the  log of coarsened likelihood scaled by $n^{-1}$ converges to a suitably variant of the OKL function for  }
%However, supposing that one had the computational resources to do this, could we say what the inferences would look like in the limit for $n\to\infty$?
%In this section, we answer this question; and show that the coarsened likelihood is intimately connected with our OWL methodology. 
Specifically, if the observed data $x_1, \ldots, x_n$ are generated i.i.d.~from some distribution $P_0$ and $\D$ satisfies appropriate regularity conditions, then the negative rescaled coarsened likelihood $-\frac{1}{n} \log L_\epsilon(\theta|x_{1:n})$ asymptotically converges  as $n \to \infty$ to a variant of $I_\epsilon(\theta)$ based on $\D$. Hence, the OWL methodology asymptotically maximizes the coarsened likelihood $\theta \mapsto L_\epsilon(\theta|x_{1:n})$. In \Cref{sec:asymp-finite,sec:asymp-continuous}, we develop this asymptotic connection for finite and continuous spaces, respectively. All proofs for this section can be found in \Cref{sec:coarsened-likelihood-asymptotics}.


% %
% If $\epsilon$ is an exponential random variable with rate parameter $\alpha$ %exponential prior distribution $\operatorname{Exp}(\alpha)$
% and $\D$ is a suitable finite sample estimator of the KL divergence, \cite{miller2018robust} showed the posterior built on this coarsened likelihood can be approximated by uniformly raising the likelihood to the power $\xi_n = \frac{\alpha}{\alpha + n}$.
% %
% In the Bayesian setting, this results in a posterior that places more weight on its prior, which is intuitively appealing when the likelihood is suspected to be misspecified but the prior is not. 
% %
% The choice of power $\xi_n$ also stops the posterior from contracting as the sample size $n$ increases, which prevents over-confidence about the parameters being inferred. 
% %
% While this is reweighting scheme is consequential for the Bayesian setting, it is not useful for frequentist methods: maximizing a uniformly re-weighted likelihood simply replicates standard maximum likelihood estimation.


% \jk{Unclear from this: is the kernel-smoothed TVD an IPM/Integral Probability semimetric (IPS)? I think so; but it's not actually formally stated/proved in the paper. I think it might be nice to do this so that we can phrase the entire theory in terms of IPMs/IPSs.}
% %
% In this work, we build upon the coarsened inference approach of \cite{miller2018robust} by studying  Eqn.~\eqref{eq:clike} for metrics $\D$ on $\cP(\cX)$ that lead to robustness to misspecification for both frequentist and Bayesian methods.
% %
% Throughout, we focus on taking $\D$ as a kernel-smoothed variant of the Total Variation Distance (TVD). 
% %
% Our motivation for doing so is that the TVD is both robust as well as intuitively interpretable as a distance on $\cP(\cX)$. 
% %
% Despite focusing on the TVD however, our theory and methodology is applicable to any Integral Probability Metric (IPM) that is continuous with respect to the weak convergence topology (see Assumption \ref{ass:metric}).
% This includes various popular choices, including the 2-Wasserstein distance and the Maximum Mean Discrepancy with characteristic kernels \cite[Lemma~3]{simon2020metrizing}.
% %
% Note that the vanilla TVD does not satisfy the continuity condition of Assumption \ref{ass:metric}, which is why we use a  kernel-smoothed variant that does (see Definition \ref{def:smoothedtvd}).
% For metrics $\D$ on $\cP(\cX)$ that satisfy our regularity conditions, we can also show  that the coarsened likelihood behaves as we would hope.
% In particular,
% with suitable scaling that depends on both $\epsilon$ and $x_{1:n}$ but not on $\theta$,  $L_\epsilon(\theta|x_{1:n})$ converges to $L(\theta|x_{1:n})=\prod_{i=1}^n p_\theta(x_i)$ as $\epsilon \to 0$ (see Lemma \ref{lem:usual-likelihood}). 


% Coarsened likelihoods are closely related to  recent advances in approximate Bayesian computation (ABC) with summary-free statistics. 
% %
% This literature (e.g.~see \cite{frazier2020robust,bernton2019approximate, legramanti2022concentration} and references therein) has studied certain robustness properties of the coarsened likelihood in \eqref{eq:clike} along with ways to compute a posterior distribution  based on this likelihood via Monte Carlo estimation  by sampling the idealized data $X_{1:n} \iid P_\theta$. In contrast to ABC which is typically used for models in which the likelihood $p_\theta(\cdot)$ is not available or is difficult to evaluate, here we focus specifically on the case when an analytical form for the likelihood $p_\theta(\cdot)$ is available.

% Although the coarsened likelihood is a conceptually appealing way to address the brittleness problem in inference, once $\varepsilon>0$ there is no easy way to evaluate $L_\epsilon(\theta|x_{1:n})$ beyond numerical approximation of a corresponding integral. 
% %
% In this work, we will use large sample asymptotics to simplify this problem.
% %
% Specifically, we show that the coarsened likelihood can ultimately be approximated by a reweighting of individual likelihood terms.


\subsection{Asymptotic connection in finite spaces}
\label{sec:asymp-finite}

%\jk{'density with respect to the counting measure' is a bit non-standard. }

Let $\cX$ be a finite set and denote the space of probability distributions on $\cX$ by the simplex $\Delta_{\cX} \doteq \{q \in [0,1]^{\cX} | \sum_{x \in \cX} q(x) = 1\}$. Let $\{p_\theta\}_{\theta \in \Theta} \subseteq \Delta_{\cX}$ denote the collection of model distributions, and $p_0 \in \Delta_{\cX}$ denote the true data generating distribution. To establish connection of OKL with the coarsened likelihood \cref{eq:clike}, we will take $\D(p, q) = \frac{1}{2}\|p - q\|_1$ to be the TV distance. 

Given this setting, we can show that $-\frac{1}{n} \log L_\epsilon(\theta | x_{1:n})$ converges in probability to the OKL function $I_\epsilon(\theta)$ at rate $n^{-1/2}$, as demonstrated by the following theorem.
\begin{theorem}
\label{thm:finite-cposterior-tv}
Suppose that $I_{\epsilon_0}(\theta) < \infty$ for some $\epsilon_0 > 0$ and let $\delta > 0$. If $\epsilon > \epsilon_0$ and $x_1, x_2, \ldots, x_n \iid p_0$, then with probability at least $1-\delta$,
\[ \left|I_\epsilon(\theta) + \frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) \right| \leq O\left( \frac{|\Xcal|}{\epsilon - \epsilon_0} \sqrt{\frac{1}{n} \log \frac{1}{\delta}}  + \frac{|\Xcal|}{n} \left( |\Xcal| + \log(n) + \frac{1}{\epsilon - \epsilon_0} \right)  \right).\]
\end{theorem}

Our proof hinges on analyzing a quantity that is closely related to $L_\epsilon(\theta | x_{1:n})$:
\[ M_{n, \epsilon}(\theta) = \prob_{Z_{1}, \ldots Z_n \iid p_\theta}\left( \frac{1}{2} \|\EmpDist{Z_{1:n}} - p_0 \|_1 \leq  \epsilon \right). \]
Instead of looking at the distance to the empirical estimator $\EmpDist{x_{1:n}}$ as in $L_\epsilon(\theta | x_{1: n})$, the quantity $M_{n, \epsilon}(\theta)$ considers the distance to the distribution $p_0$ itself. This simplifies matters greatly, and allows us to establish the following result, which is essentially a consequence of Sanov's theorem from large deviation theory~\citep{demboLargeDeviationsTechniques2010}.

\begin{lemma}
\label{lem:finite-sanov-tv}
If $I_{\epsilon_0}(\theta) < \infty$ for some $\epsilon_0 > 0$, then
\[  \left| I_\epsilon(\theta) + \frac{1}{n} \log M_{n, \epsilon}(\theta) \right| \leq {O}\left( \frac{|\Xcal|}{n} \left( |\Xcal| + \log(n) + \frac{1}{\epsilon - \epsilon_0} \right) \right) \]
for all $\epsilon > \epsilon_0$.
\end{lemma}

The rest of the proof of \Cref{thm:finite-cposterior-tv} amounts to establishing that $L_\epsilon(\theta | x_{1:n})$ is close to $M_{n,\epsilon}(\theta)$, which follows from continuity arguments and the fact that $\EmpDist{x_{1:n}}$ converges to $p_0$ in $\ell_1$ distance.


\Cref{thm:finite-okl-convergence-tv} and \Cref{thm:finite-cposterior-tv} together show that, in the large sample limit, the OWL methodology and coarsened likelihood philosophy are two sides of the same coin: they both provide approximations of the OKL and, in turn, must approximate each other.

\subsection{Asymptotic connection in continuous spaces}
\label{sec:asymp-continuous}

Suppose $\Xcal=\R^d$ and $\Den$ denotes the set of densities on $\cX$ with respect to the Lebesgue measure. Let $\{ p_\theta \}_{\theta \in \Theta} \subseteq \Den$ denote the set of model densities and let $p_0$ denote the density of the data generating measure $P_0$.
%, i.e. sample $x_1, \ldots, x_n \in \cX$ is generated i.i.d.~from a distribution $P_0 \in \cP(\cX)$ with density $p_0$. 

Similar to the finite case, we can use Sanov's theorem from Large Deviation theory %(see \Cref{sec:coarsened-likelihood-asymptotic-cont}) 
to establish the following asymptotics for the coarsened likelihood for a suitable class of discrepancies $\D$, which includes the Wasserstein distance, Maximum Mean Discrepancy with  suitable choice of kernels \citep{simon2018kernel},  %\jk{MMD only a pseudometric if the kernel is characteristic (otherwise it doesn't satisfy the identity of discernables); kernels that are charateristic include Gaussian and Matern kernels. We probably don't have to say all that, but we could say 'Maximum Mean Discrepancy based on characteristic kernels', and reference \url{https://www.jmlr.org/papers/volume19/16-291/16-291.pdf}},
and the smoothed TV distance (\Cref{def:smoothed-tvd}).

% \begin{theorem}
% 	\label{thm:clikelihood-asymptotics-density}
% 	Suppose $x_1, \ldots, x_n \iid P_0$, $\epsilon > 0$, $\KL(p_0|p_\theta) < \infty$, and $\D: \cP(\cX) \times \cP(\cX) \to [0,\infty)$ is an integral probability semi-metric that is continuous with respect to the weak convergence topology on $\cP(\cX)$. Then
% 	\begin{equation*}
% 		-\frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) \pconv \inf_{\substack{q \in \Den\\ \D(q, p_0) \leq \epsilon}} \KL(q|p_\theta) \quad 
% 		\text{ as $n \to \infty$.}
% 	\end{equation*}
% \end{theorem}
\begin{theorem}
\label{thm:clikelihood-asymptotics-density}
Suppose $I_{\epsilon_0}(\theta) < \infty$ for some $\epsilon_0 > 0$ and $\D: \cP(\cX) \times \cP(\cX) \to [0,\infty)$ is a pseudometric that is convex in its arguments and continuous with respect to the weak convergence topology on $\cP(\cX)$. If $\epsilon > \epsilon_0$ and $x_1, \ldots, x_n \iid P_0$, then
	\begin{equation*}
		-\frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) \pconv \inf_{\substack{Q \in \cP(\cX) \\ \D(Q, P_0) \leq \epsilon}} \KL(Q|P_\theta) \quad 
		\text{ as $n \to \infty$.}
	\end{equation*}
\end{theorem}

Recall that the limiting expression in the above theorem has the same form as that of the OKL function given in \cref{eq:okl-density}. However, in order to establish connection between the OKL function and the coarsened likelihood, unlike in the finite case, we cannot merely take the discrepancy $\D$ in the coarsened likelihood to be the TV distance, since the TV distance between the two empirical distributions in \cref{eq:clike} will almost surely be equal to one. Instead, we will take $\D$ to be a smoothed version of TV distance calculated by first convolving the empirical measures with a smooth kernel function $K_h:\cX \times \cX \to [0,\infty)$ indexed by a bandwidth parameter $h > 0$. 

To formally define the smoothed TV distance, let $\phi \in \Den$ be a continuous and bounded probability density function (e.g.~standard Gaussian density), let $h > 0$ be a bandwidth parameter. Then the kernel $K_h: \cX \times \cX \to [0,\infty)$ is defined as  $K_h(x,y) = \frac{1}{h^d} \phi((x-y)/h)$, and for any measure $\mu \in \cP(\cX)$, the convolved density $K_h \star \mu \in \Den$ is defined as $(K_h \star \mu)(x) = \int K_h(x, y) \mu(dy)$.  

\begin{definition} Given two measures $\mu, \nu \in \cP(\cX)$ and bandwidth $h > 0$, the \emph{smoothed total variation (TV) distance} is defined as:
	$$
	\tvk[h](\mu,\nu) = \frac{1}{2} \int |(K_h \star \mu)(x) - (K_h \star \nu)(x)| dx.
	$$
 
	We extend the notion of smoothed TV distance $\tvk[h](p,q) = \tvk[h](\mu, \nu)$ to densities $p, q \in \Den$\ based on their  induced measures $\mu, \nu \in \cP(\cX)$.
 \label{def:smoothed-tvd}
\end{definition}


We show in \Cref{sec:smoothed-tvd-is-continous-ips} that $\D=\tvk[h]$  satisfies conditions of \Cref{thm:clikelihood-asymptotics-density}.
Further, when $\phi$ has fast tail-decay and densities $p, q \in \Den$ satisfy appropriate regularity conditions, standard results on kernel density estimation (e.g.~\cite{rinaldo2010generalized, jiang2017uniform}) show the pointwise convergence of densities  $K_h \star q \to q$ as $h \to 0$. This, when combined with Scheffe's lemma and the triangle inequality, shows that $\lim_{h \to 0} \tvk[h](p, q) = \tv(p,q)$. In other words, for suitably small bandwidth parameter  $h > 0$, the neighborhoods based on the smoothed total variation distance    approximate those based on the total variation distance.  
   
%is a spherically symmetric probability distribution with exponential tails, tools from \cite{jiang2017uniform} show that the smoothed TV distance $\tvk[h](p,q)$ approximates the TV distance $\tv(p,q)$ whenever the bandwidth parameter $h$ is sufficiently small. 

Thus, by invoking \Cref{thm:clikelihood-asymptotics-density} with the choice $\D=\tvk[h]$, one expects $- \frac{1}{n} \log L_\epsilon(\theta|x_{1:n}) \approx I_\epsilon(\theta)$ when $n$ is large and $h$ is small. As in the finite setting, we again see that maximizing the coarsened likelihood is closely related to minimizing the OKL function in the large sample regime. Hence the OWL methodology can be used to approximately maximize the coarsened likelihood when $\D=\tvk[h]$  for large sample size $n$ and a suitably small bandwidth $h$. In fact for many other metrics $\D$ satisfying the conditions of \Cref{thm:clikelihood-asymptotics-density}, one can adapt the OWL methodology to maximize the function $\theta \mapsto L_\epsilon(\theta|x_{1:n})$ as $n \to \infty$.


