\section{Other evaluation approaches}
\label{sec:other-approaches}

Experiments in this paper focus on the (FNM-)EDC for multiple reasons:
\begin{enumerate}
\item As noted in the introduction, the EDC is being standardised in the next edition of ISO/IEC 29794-1.
\item Evaluating the QA algorithm performance via FNM-EDC plots with one recognition system and the minimum as the pairwise QS function reflects the common approach employed in the contemporary literature.
\item EDC plots in general show directly how a biometric recognition error value changes as samples/comparisons are discarded based on sample QSs.
In contrast, other non-EDC approaches presented in the remainder of this section consider only parts of these operationally relevant biometric concepts.
\end{enumerate}

\begin{figure}
\centering
\begin{tabular}{c}
TinyFace \\
\includegraphics[width=0.97\linewidth]{img/other/CS-DC--zoom--mated--TinyFace} \\
\includegraphics[width=0.97\linewidth]{img/real-data/Real-FIQA-legend} \\
\end{tabular}
\caption{\label{fig:cs-dc-example} CS-DC example plot showing mean mated similarity scores.}
\vspace{-1em}
\end{figure}

One non-EDC approach is to plot the mean comparison scores instead of error values.
In this section this approach is called ``CS-DC'' for ``Comparison Score versus Discard Characteristic''.
For the sake of simplicity all CSs are assumed to be similarity scores in this section.
\autoref{fig:cs-dc-example} shows an example.
For mated comparisons higher/increasing similarity scores are better,
and for non-mated comparisons lower/decreasing similarity scores are better.
Although it is technically possible to use both mated and non-mated comparisons within the same curve by using similarity scores for one and dissimilarity scores for the other (e.g. by negating the scores for one type),
this can distort the interpretability of the results,
since the mated and non-mated comparisons should inherently be represented by different CS distributions.
One CS-DC curve should therefore use either only mated or non-mated comparisons.
This is analogous to the difference between FNM-EDC curves (False Non-Match error using mated comparisons) and FM-EDC curves (False Match error using non-mated comparisons).
The difference to FNM-EDC and FM-EDC curves is that the CS-DC curves do not consider a CS threshold that divides the comparisons into error and non-error cases.

\begin{figure}
\centering
\begin{tabular}{c}
TinyFace \\
\includegraphics[width=0.97\linewidth]{img/other/dprime-DC--zoom--TinyFace} \\
\includegraphics[width=0.97\linewidth]{img/real-data/Real-FIQA-legend} \\
\end{tabular}
\caption{\label{fig:dprime-dc-example} $d'$-DC example plot.}
\vspace{-1em}
\end{figure}

Another non-EDC approach is the ``$d'$ versus discard characteristic'' proposed by \markAuthor{Henniger \etal{}} \cite{Henniger-QA-UtilityBasedEvaluation-BIOSIG-2022},
abbreviated as ``$d'$-DC'' in this section.
Similar to the EDC and the CS-DC, samples/comparisons are discarded based on QSs, which is plotted from left to right on the X-axis.
But as per \autoref{eq:dprime},
instead of an error value this approach plots $d'$,
which is the difference between the mean of the remaining non-mated dissimilarity scores ($\mu_n$) and the mean of the remaining mated dissimilarity scores ($\mu_m$),
normalised by the corresponding standard deviation values ($\sigma$).
\begin{equation}\label{eq:dprime}
d' = \frac{\mu_n - \mu_m}{\sqrt{\sigma^2_n+\sigma^2_m}}
\end{equation}
\Ie{} for $d'$ higher/increasing values are better.
See \autoref{fig:dprime-dc-example} for an example.
The advantage of the $d'$-DC is that both mated and non-mated CSs are considered in the same curve,
but similar to the CS-DC this does not consider any CS threshold to divide comparisons into error and non-error cases.

\begin{figure}
\centering
\begin{tabular}{c}
TinyFace \\
\includegraphics[width=0.97\linewidth]{img/other/FC-EDC--zoom--TinyFace} \\
\includegraphics[width=0.97\linewidth]{img/real-data/Real-FIQA-legend} \\
\end{tabular}
\caption{\label{fig:fc-edc-example} FC-EDC example plot.}
\vspace{-1em}
\end{figure}

Note that it is however also possible to consider both mated and non-mated CSs in a single EDC curve.
This can be done for instance by plotting the changing FNMR against an approximately fixed FMR (or vice versa) for a given set of mated and non-mated comparisons,
which is called ``FC-EDC'' for ``Fixed Counterpart EDC'' in this section.
\autoref{fig:fc-edc-example} shows an example.
In this paper the FNM-EDC was prioritized for experiments due to the aforementioned prevalence in the literature and due to current standardisation efforts,
but it can be argued that an FC-EDC is a more straightforward EDC type for operationally relevant evaluations,
since it considers both the FNMR and the FMR in a single plot.

Many of the general conclusions in this paper also apply to the just described CS-DC, $d'$-DC, and FC-EDC:
\begin{itemize}
\item pAUC (\autoref{sec:pauc}): pAUC values can be computed to consider a range of discard fractions.
\item Curve interpolation (\autoref{sec:interpolation}): Since only discrete numbers of samples/comparisons are discarded along the X-axis, it is advisable to use stepwise interpolation for all of these evaluation plot types.
\item Quality score normalisation (\autoref{sec:normalisation}): Deviations introduced by normalisation can naturally affect any evaluation. For plots this can have similar effects as in the shown FNM-EDC examples.
\item Ranking stability (\autoref{sec:stability-real} and \autoref{sec:stability-synthetic}): As especially the experiments with fully synthetic scores in \autoref{sec:stability-synthetic} should demonstrate, QA algorithm rankings based on pAUC values can in general differ substantially for different pAUC discard limits, and will likely stabilize relative to each other as higher pAUC discard limits are used.
This consideration may however be obviated if operational requirements clearly specify the evaluation parameters,
and the FC-EDC in particular allows for the concrete specification of a fixed FMR or FNMR.
\end{itemize}

Besides the various described curve-based evaluations,
it is possible to directly compute scalar values to rank QA algorithms by assessing the correlation between QSs and CSs in some way.
One way to do this is by computing the Pearson correlation coefficient between the CSs and the corresponding pairwise QSs (those \eg{} being the sample QS minimum, as previously described for typical EDC curves).
Another way proposed by \markAuthor{Henniger \etal{}} \cite{Henniger-QA-UtilityBasedEvaluation-BIOSIG-2022}
is to first compute sample ``utility'' scores similar to $d'$ (see the paper for details),
and then compute Root Mean Square Error values between these scores and the QSs.
This may not work well if the scores are not using an identical or at least comparable value range,
so we propose to instead compute the Pearson correlation coefficient between the sample utility scores and the QSs
(cf.\@ \cite{Olsen-FingerImageQuality-IETBiometrics-2016} with Spearman correlation).
All three of these correlation-based QA algorithm rankings can consider both mated and non-mated comparisons at once,
but they have the disadvantage that they do not consider the concept of discarding samples based on the QSs (although there may be use cases for which this is not relevant).
They also do not divide comparisons into error and non-error cases, similar to the CS-DC and the $d'$-DC,
which could however technically be done by \eg{} assessing the correlation between the pairwise QSs and error/non-error proxy values such as 0/1.

Yet another evaluation approach is the inspection of Detection Error Trade-off (DET) curves across different QS thresholds, which shows the FNMR and the corresponding FMR on the two axes.
This can either be done similar to EDC plots by discarding samples/comparisons for each QS threshold,
or the QS thresholds can be used to separate the samples/comparisons into multiple disjunct bins from which the curves are computed.
The former approach is also being standardised in the next edition of ISO/IEC 29794-1 as the ``DET versus discard method''.
While the EDC and the other previously described curve-based evaluation approaches can inherently consider all possible QS thresholds in one curve,
a disadvantage of this DET-based approach is that only a smaller selection of QS thresholds can be sensibly examined/visualized,
since each added QS threshold for each QA algorithm results in a new DET curve.

As shown in this section there are various possible QA algorithm evaluation approaches.
Naturally not every feasible variation is included here,
but, as discussed, the EDC variations already cover common operationally relevant considerations.
