\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}}
\else
\section{Introduction}
\fi

\IEEEPARstart{B}{iometric} %
recognition \cite{Vocabulary} performance depends on the quality of the used biometric samples \cite{Vocabulary} such as face images.
In this context ``quality'' refers specifically to biometric utility \cite{Vocabulary},
as opposed to other definitions such as factor-specific quality (\eg{} ``How blurry is one image?'') or subjective image quality (\eg{} ``How noticeable are lossy compression artefacts to a human?'').

There are quality assessment (QA) algorithms that assign one scalar quality score (QS) \cite{Vocabulary} to one given biometric sample,
with higher scores implying higher biometric utility.
Biometric comparisons \cite{Vocabulary} involve information stemming from two samples,
so a QA algorithm attempts to assess a single sample's quality in terms of its utility for an unknown number of comparisons against unknown other samples.
Thus a QS can be considered an estimate of the usefulness of the information that is extractable from the sample, irrespective of other samples.

This paper is about the quantitative ranking of QA algorithms with respect to concrete biometric recognition systems via ``Error versus Discard Characteristic'' (EDC) plots,
specifically using the ``partial Area Under Curve'' (pAUC) values thereof.
Conceptually, an EDC curve for one QA algorithm shows how some concrete biometric recognition ``error'' changes as samples are progressively ``discarded'' via an increasing sample QS threshold.
Experiments with real QA algorithms in this paper were conducted in the context of face recognition, \ie{} for face image quality assessment (FIQA) \cite{Schlett-FIQA-LiteratureSurvey-CSUR-2021},
but insights from the analyses should generalise to other biometric modalities as well.

The paper is structured as follows:

\begin{itemize}
\item In the remainder of this introduction the EDC and pAUC value rankings are described further in \autoref{sec:edc} and \autoref{sec:pauc}, respectively.
\item \SectionRef{sec:real-setup} details the setup used for the experiments with real QA algorithm data.
\item \SectionRef{sec:interpolation} discusses why ``stepwise'' EDC curve interpolation should be preferred.
\item \SectionRef{sec:normalisation} examines the effect of quality score normalisation, in particular to a $[0, 100]$ integer range.
\item \SectionRef{sec:stability-real} analyses the stability of QA algorithm rankings based on FNM-EDC pAUC values across different pAUC discard limit and starting error configurations, using real data, and \autoref{sec:stability-synthetic} continues the analysis with synthetic data that enables the comparisons of actual rankings against expected rankings.
\item \SectionRef{sec:conclusions} briefly summarises the primary conclusions from the paper.
\end{itemize}

\subsection{Error versus Discard Characteristic}
\label{sec:edc}

The concept of the ``Error versus Discard Characteristic'' (EDC) was introduced under the name ``Error versus Reject Characteristic'' (ERC) by \markAuthor{Grother and Tabassi} \cite{Grother-SampleQualityMetricERC-PAMI-2007},
and is also known as ``Error versus Reject Curve'' in the literature.
At the time of writing the next (third) edition of ISO/IEC 29794-1\footnote{\url{https://www.iso.org/standard/79519.html}} is standardising this concept under the name EDC instead of ERC to avoid confusion with other meanings of the word ``reject'' \cite{Vocabulary} in the biometric context.

A concrete EDC instance with real data consists of the following parts:
\begin{itemize}
\item Specific ``error'':
Often the ``False Non Match Rate'' (FNMR) \cite{Vocabulary} is selected as the EDC's error \cite{Schlett-FIQA-LiteratureSurvey-CSUR-2021}\cite{Grother-SampleQualityMetricERC-PAMI-2007},
a configuration abbreviated as FNM-EDC,
which also is what we focus on in this paper.
\item Set of comparisons:
Each comparison corresponds to one pair of samples as input and one comparison score (CS) \cite{Vocabulary} as output.
The selected error determines which kind of comparisons are required to compute the EDC.
For the FNM-EDC only mated \cite{Vocabulary} comparisons are used, meaning that the corresponding samples in each pair must stem from the same biometric capture subject \cite{Vocabulary} and the same biometric instance \cite{Vocabulary} (e.g. from the same finger).
\item One biometric recognition system:
The recognition system computes the CSs.
One can technically involve multiple recognition systems in a single EDC plot,
but for clarity we will use exactly one recognition system in each EDC instance within this paper,
which also is common practise in the scientific literature.
Additionally be aware that all CSs used in this paper are similarity scores \cite{Vocabulary}, meaning that higher CS values imply higher similarity.

Besides the recognition system,
a CS threshold needs to be defined to reach one comparison decision (match or non-match) \cite{Vocabulary} for each CS.
For the FNM-EDC each non-match counts as one comparison error, the FNMR being the non-match count divided by the number of comparisons.
\item Set of QA algorithms:
Every QA algorithm computes one QS per sample.
\item One pairwise QS function:
This function computes a single pairwise QS from two sample QSs that correspond to one comparison.
The data points of an EDC curve for one QA algorithm are computed by progressively discarding comparisons based on the associated pairwise QS.

It is technically possible to use a different function for each QA algorithm,
or to use multiple different functions per QA algorithm to effectively create different QA algorithm variants.
But in this paper we use exactly one function,
namely the minimum of each pairs' sample QSs as the pairwise QS,
which is the de facto standard in the literature
since lower QSs are supposed to indicate lower biometric utility.

The selected function should typically reflect the discarding of samples, not comparisons,
so that the EDC simulates the operational discarding of samples before any comparisons could be conducted.
Taking the minimum of the sample QSs does reflect this use case, since a pair will be counted as discarded when one of the samples would be discarded in isolation.
As an alternative to using a pairwise QS function for the EDC computation,
one can discard samples (and the associated comparisons) by their QS,
which is functionally equivalent to this pairwise minimum QS formulation.

Choosing a function that mixes the sample QSs to get the pairwise QS would instead represent an operational scenario in which the pairwise QSs are used to discard already computable comparisons.
This would effectively constitute an augmentation of the biometric comparison by adding a certainty value.
In this scenario the usage of ``pairwise QA algorithms'' that utilise information from both samples as input could also be considered,
so this scenario would not necessarily have to be restricted to (sample) QA algorithms.
\end{itemize}

Evaluating the QA algorithm performance via FNM-EDC plots with one recognition system and the minimum as the pairwise QS function reflects the common approach employed in the contemporary literature,
and thus the focus of this paper's examples.
Other evaluation approaches are for example proposed by \markAuthor{Henniger \etal{}} \cite{Henniger-QA-UtilityBasedEvaluation-BIOSIG-2022}, namely the ``$d'$ versus discard characteristic'' that is similar to the EDC, but uses the normalised difference between the mean of all non-mated CSs and mated CSs (\ie{} $d'$, higher is better) instead of an error value (lower is better), as well as an approach less related to the EDC that computes the root mean square error (RMSE) between the sample QSs and sample ``utility scores'' derived from mated and non-mated CSs.
Another simple approach to immediately yield a single number instead of a curve per QA algorithm for evaluation purposes is the computation of the correlation between a set of CSs and a corresponding set of pairwise QSs.

\begin{figure}

\centering
\begin{tabular}{c}
LFW \\
\includegraphics[width=0.97\linewidth]{img/real-data/LFW---EDC-Example--MultiAlgorithms--XaxisPairs-YaxisFnmr-pAUC-autbl} \\
TinyFace \\
\includegraphics[width=0.97\linewidth]{img/real-data/TinyFace---EDC-Example--MultiAlgorithms--XaxisPairs-YaxisFnmr-pAUC-autbl} \\
\includegraphics[width=0.97\linewidth]{img/real-data/Real-FIQA-legend} \\
\end{tabular}
\caption{\label{fig:edc-plot-example} Two FNM-EDC plot examples on different datasets with the same five QA algorithms, 0.05 starting error, and a shaded $[0,0.2]$ pAUC for the corresponding best curves (MagFace on LFW and SER-FIQ on TinyFace), minus the ``area under theoretical best'' (below the lower grey dashed line). The used datasets and QA algorithms are described further in \autoref{sec:real-setup}.}
\vspace{-2em}
\end{figure}

Each EDC curve corresponds to one QA algorithm.
Data points in the curve consist of the error and the discard fraction.
Conventionally the error is plotted on the Y-axis, increasing upwards, lower values being better,
while the discard fraction (of comparisons) is plotted increasing left to right on the X-axis.
Both the discard fraction and (usually) the error are inherently constrained to the range $[0,1]$.
See \autoref{fig:edc-plot-example} for an example.
To compute a curve the comparisons are progressively discarded in increasing order of their pairwise QSs.
Each discard step results in a curve data point with an implicit X-axis discard fraction,
for which the corresponding Y-axis error value is computed from the remaining comparisons.
To ensure that the curve is accurate, each discard step should only discard as many comparisons as strictly necessary to reach the next data point (\ie{} the set of comparisons with the next lowest pairwise QS),
as opposed to computing data points for \eg{} fixed QS threshold increments.
Note that the curve data point for discard fraction 0 is independent of the QA algorithms,
since it only depends on the comparison set and CS threshold,
meaning that there is one ``starting error'' per EDC plot in this paper.
See \autoref{sec:stability-real} for an analysis that involves varying starting errors.

\subsection{Partial Area Under Curve}
\label{sec:pauc}

To quantitatively rank QA algorithms based on the EDC curves,
``partial Area Under Curve'' (pAUC) \cite{Olsen-FingerImageQuality-IETBiometrics-2016} values can be computed.
A pAUC value is the area under one curve for a chosen discard fraction range, \eg{} the $[0,0.2]$ range in \autoref{fig:edc-plot-example}.
Note that choosing higher discard fractions, \eg{} beyond 0.3, would not represent an operational scenario and should therefore be avoided.

Although pAUC values suffice to rank the QA algorithms relative to each other,
the magnitude of the differences might not necessarily be clearly interpretable,
since the raw pAUC values depend on the EDC starting error and the chosen discard fraction range.

\markAuthor{Olsen \etal{}} \cite{Olsen-FingerImageQuality-IETBiometrics-2016} proposed to subtract the ``area under theoretical best'' from the (p)AUC.
This refers to the area under the EDC curve for the theoretical best case where the decrease in the error equals the discard fraction\footnote{\markAuthor{Olsen \etal{}} \cite{Olsen-FingerImageQuality-IETBiometrics-2016} more specifically defined the area under theoretical best for FNM-EDC curves with the X-axis plotting the fraction of discarded samples instead of comparisons.},
\ie{} the area under the line defined by $max(0, Error-DiscardFraction)$,
which is also visible in \autoref{fig:edc-plot-example}.
Note that this is an approximation or lower limit of the theoretical best case, not the actual best case for the given comparison pairs.
The actual best case curve cannot be strictly monotonically decreasing,
since a real EDC curve can only change by discarding a non-fractional number of comparisons per data point,
which is further discussed and illustrated in \autoref{sec:interpolation}.

Subtracting the ``area under theoretical best'' from the pAUC values does not change the ranking of QA algorithms, since the same value is subtracted for each pAUC discard range configuration in which QA algorithms are ranked.
It can however serve as a straightforward\footnote{The ``area under theoretical best'' only depends on the EDC starting error.}
adjustment to make the pAUC values more easily interpretable,
since it removes the effect of the area that cannot possibly be improved.
The remaining pAUC, which is of interest, is the grey-coloured area in \autoref{fig:edc-plot-example}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/edc-properties/EDC-Example--Random}\\
	\includegraphics[width=0.37\linewidth]{img/edc-properties/EDC-Example--Random--Legend}
	\caption{\label{fig:edc-random} The mean curve of many (here 100) EDC curves based on randomly generated QSs approximates the constant starting error (here 0.05), implying that QA algorithms resulting in curves distinctly above this constant are ineffective since they behave worse than (averaged) random QS curves.} %
	\vspace{-1em}
\end{figure}

In addition to this theoretical (hard) best case lower error bound line,
a theoretical (soft) worst case upper error bound line can be defined as well:
The constant EDC starting error line approximates the mean of infinite curves for random QSs,
as demonstrated in \autoref{fig:edc-random},
and a QA algorithm should of course preferably never increase the error above the starting error regardless of the discard fraction.
Therefore, the pAUC values for QA algorithms can optionally be made relative to the pAUC for this upper error bound line for the purposes of interpretability (\ie{} the ranking remains unaffected).

\begin{table}
\caption{\label{tab:qa-algorithm-ranking-example} Two QA algorithm ranking examples corresponding to the two EDC plots in \autoref{fig:edc-plot-example} for the $[0,0.2]$ pAUC range. The constant ``area under theoretical best'' value has been subtracted from the shown pAUC values, \ie{} here $(0.05^2)/2$ since the starting error is 0.05.}
\centering
\begin{tabular}{c}
LFW \\
\begin{tabular}{r|ccc}
\theader{QA algorithm} & \theader{pAUC value} & \theader{Discrete ranking} & \theader{Relative ranking} \\
\hline
               MagFace &               0.0036 &                          1 &                       0.00 \\
            CR-FIQA(L) &               0.0041 &                          2 &                       0.16 \\
                 PCNet &               0.0056 &                          3 &                       0.65 \\
            CR-FIQA(S) &               0.0061 &                          4 &                       0.80 \\
               SER-FIQ &               0.0067 &                          5 &                       1.00 \\
\end{tabular} \\
TinyFace \\
\begin{tabular}{r|ccc}
\theader{QA algorithm} & \theader{pAUC value} & \theader{Discrete ranking} & \theader{Relative ranking} \\
\hline
               SER-FIQ &               0.0059 &                          1 &                       0.00 \\
            CR-FIQA(L) &               0.0066 &                          2 &                       0.36 \\
               MagFace &               0.0067 &                          3 &                       0.38 \\
            CR-FIQA(S) &               0.0078 &                          4 &                       0.93 \\
                 PCNet &               0.0079 &                          5 &                       1.00 \\
\end{tabular}
\end{tabular}
\end{table}

These pAUC value interpretability adjustments are not necessary as long as the main concern of the evaluation is the performance of each QA algorithm relative to the other QA algorithms for a given EDC pAUC configuration.
In that case it can suffice to simply compute the QA algorithm ranking using the raw pAUC values,
see for example \autoref{tab:qa-algorithm-ranking-example}.
The ``relative ranking'' values are the min-max normalised pAUC values.
It shows how far an algorithm is considered from being the best (0) or worst (1) relative to the others,
in contrast to the ``discrete ranking''.
This ``relative ranking'' approach is used in the experiments of this paper.

Note that other evaluation types besides comparisons among multiple QA algorithms are possible, \eg{} a certification scheme that defines a certain pAUC value limit for a tested QA algorithm's EDC curve.
