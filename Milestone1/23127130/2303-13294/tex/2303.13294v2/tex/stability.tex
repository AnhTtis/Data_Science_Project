\section{Ranking stability with real data}
\label{sec:stability-real}

In this section we examine the ``stability'' of QA algorithm rankings resulting from the FNM-EDC pAUC values across different configurations, in a FIQA scenario with real data as described in \autoref{sec:real-setup}.
Stability here refers to the similarity of the relative rankings described in \autoref{sec:pauc} for different FNM-EDC starting errors and pAUC ranges,
which can show us to which extent the rankings of QA algorithms can change with respect to these settings alone.

\subsection{EDC configuration ranges}
\label{sec:stability-edc-config}

The stability of the FNM-EDC-pAUC-based rankings is examined across a grid of two parameter configurations:
\begin{itemize}
\item Starting error (\ie{} FNMR at 0 discard fraction): Range $[0.01,0.10]$ with a $0.01$ step (10 steps).
\item pAUC discard limit (\ie{} the maximum of the pAUC range): Range $[0.01,0.20]$ with a $0.01$ step (20 steps).
\\The pAUC range minimum is always 0.
\end{itemize}
This means that rankings are computed for $10\cdot 20 = 200$ EDC configurations.
Although greater ranges can be examined, \eg{} pAUC discard limit range $[0.01,0.99]$,
the ones used here were selected to approximate values one might find in the FIQA literature \cite{Schlett-FIQA-LiteratureSurvey-CSUR-2021},
and which are operationally relevant.
Note that if a biometric system in operation would discard 20\% of captured samples due to poor quality, the operation would not be considered successful.

\subsection{Stability analysis}

\begin{figure}
\centering
\begin{tabular}{c}
LFW \\
\includegraphics[width=\linewidth]{img/real-data/pAUC20-SE10---Ranking-Error-StatDiff---LFW---Distance---pAUC-lim.} \\
TinyFace \\
\includegraphics[width=\linewidth]{img/real-data/pAUC20-SE10---Ranking-Error-StatDiff---TinyFace---Distance---pAUC-lim.}
\end{tabular}
\caption{\label{fig:stability-plots-real-data} EDC rankings \vs{} mean EDC ranking for real data. Curves correspond to the starting error 0.01 (dark purple) to 0.10 (bright yellow), with the pAUC limit varying across the X-axis. The Y-axis shows the difference of the ranking to the mean ranking (lower is better), times the QA algorithm count (five).}
\end{figure}

\begin{table}
\caption{\label{tab:stability-statistics-real-data}EDC ranking placement statistics for real data.}
\centering
\setlength{\tabcolsep}{3pt}
\textbf{LFW}\\
\begin{tabular}{r|rrr|rrr}
\theader{QA algorithm} & \theader{Span} & \theader{Best (Min)} & \theader{Worst (Max)} & \theader{Median} & \theader{Mean} & \theader{Std.dev.} \\
\hline
CR-FIQA(L) & 0.82 & 1.11 & 1.94 & 1.61 & 1.60 & 0.17 \\
CR-FIQA(S) & 1.57 & 3.43 & 5.00 & 4.40 & 4.44 & 0.34 \\
MagFace    & 0.00 & 1.00 & 1.00 & 1.00 & 1.00 & 0.00 \\
PCNet      & 1.43 & 3.57 & 5.00 & 4.33 & 4.37 & 0.64 \\
SER-FIQ    & 2.45 & 2.55 & 5.00 & 5.00 & 4.72 & 0.69 \\
\end{tabular}
\\\textbf{TinyFace}\\
\begin{tabular}{r|rrr|rrr}
\theader{QA algorithm} & \theader{Span} & \theader{Best (Min)} & \theader{Worst (Max)} & \theader{Median} & \theader{Mean} & \theader{Std.dev.} \\
\hline
CR-FIQA(L) & 2.88 & 2.00 & 4.88 & 3.56 & 3.54 & 0.95 \\
CR-FIQA(S) & 2.41 & 2.59 & 5.00 & 4.93 & 4.82 & 0.34 \\
MagFace    & 4.00 & 1.00 & 5.00 & 2.55 & 2.44 & 0.60 \\
PCNet      & 3.44 & 1.56 & 5.00 & 5.00 & 4.82 & 0.54 \\
SER-FIQ    & 0.81 & 1.00 & 1.81 & 1.00 & 1.01 & 0.08 \\
\end{tabular}
\end{table}

\autoref{fig:stability-plots-real-data} depicts the ``stability'' of the FNM-EDC-pAUC-based QA rankings for the 200 FNM-EDC-pAUC configurations.
The X-axis shows the pAUC limit configurations (20 steps), while the different curves correspond to the 10 starting error configurations.
The Y-axis shows the instability in terms of the rankings' divergence from the mean ranking, times the number of QA algorithms (here five), lower being better (more stable).
These instability values (one data point) are computed via \autoref{eq:stability-real},

\begin{equation}\label{eq:stability-real}
RankingDivergence = \Sigma^n_i |p_i-\bar{p_i}|
\end{equation}

$n$ being the QA algorithm count (five), $p_i$ being one QA algorithm's placement in the ranking, and $\bar{p_i}$ being the mean placement of the QA algorithm among all 200 rankings.
The rankings are the ``relative rankings'' previously described at the end of \autoref{sec:pauc},
meaning that a QA algorithm placement value in a ranking is the min-max normalised pAUC value for the QA algorithm's FNM-EDC curve.

Be aware that this comparison against the mean ranking is biased insofar that rankings with configurations closer to the mean configuration\footnote{Here pAUC discard limit 0.10 and starting error 0.05.} will inherently be more similar to this mean ranking, thus yielding lower instability values.
It is however possible to examine whether certain configurations exhibit instability values that cannot be explained by this bias alone.
The results illustrated in \autoref{fig:stability-plots-real-data} thus primarily indicate that the ranking divergence can increase substantially at lower pAUC limits,
but not at higher pAUC limits, irrespective of the starting error choice (\ie{} higher pAUC limits diminish the effect of the starting error choice on the ranking divergence).
Based on these results it appears that higher pAUC limits should be preferred.
Please note that we refrain from recommending more specific value bounds for these parameters,
since the stability is ultimately also dependent on the dataset and the used recognition/QA algorithms, so that concrete stable configuration values seen here may not translate precisely to other evaluation setups.

\autoref{tab:stability-statistics-real-data} lists ranking statistics per QA algorithm across the 200 rankings.
The ``Median''/``Mean'' columns show the median/mean ranking placements in the range $[1,5]$, lower being better and five being the QA algorithm count,
with ``Std.dev.'' showing the placement standard deviation in the range $[0,5/2]$\footnote{As described previously, the originally computed rankings are in the range $[0,1]$. The values in the table are adjusted by the QA algorithm count, five, to improve the interpretability.}.
FIQA algorithm evaluation is not the focus of this paper, but we can observe that the mean and median results here show CR-FIQA(L) and MagFace as more effective for both datasets. Both use a similar network architecture (ResNet100) and, perhaps more importantly, training data (MS1MV2 \cite{Deng-ArcFace-IEEE-CVPR-2019}), but their training procedures differ.
SER-FIQ was the best algorithm for TinyFace but the worst for LFW.
MagFace also appears to be especially effective for LFW relative to the other algorithms.

The ``Best''/``Worst'' placement columns in \autoref{tab:stability-statistics-real-data} with range $[1,5]$ provide additional insight regarding ranking stability,
with ``Span'' being the worst placement minus the best placement value (range $[0,5-1]$):
MagFace on TinyFace is an extreme example with the maximum span value four,
meaning that there is one EDC configuration for which MagFace appears as the best algorithm in the ranking,
and one EDC configuration where it appears as the worst algorithm.
More generally, the placement span for most of the QA algorithms in both datasets is above one.
This means that it is possible to optimise the placement for one QA algorithm by selecting a specific EDC configuration for one dataset.
Restricting the EDC parameters to more stable configurations means that such placement optimisation becomes more restricted,
even if only one dataset is considered in isolation.

\section{Ranking stability with synthetic data}
\label{sec:stability-synthetic}

A noteworthy limitation of the stability analysis in the previous section is that there was no expected (\ie{} already known) ranking of the real QA algorithms,
which is why we compared rankings against the mean ranking.
This limitation can be circumvented through fully synthetic EDC input data with a predefined ranking.

\subsection{Synthetic data setup}

\begin{figure}
\centering
\begin{tabular}{c}
Variant 1 \\
\includegraphics[width=0.99\linewidth]{img/synth-data/Synthetic-DiffFiqaTypeB---EDC-Example--MultiAlgorithms--XaxisPairs-YaxisFnmr-autbl-zoom} \\
Variant 2 \\
\includegraphics[width=0.99\linewidth]{img/synth-data/Synthetic-DiffFiqaTypeC---EDC-Example--MultiAlgorithms--XaxisPairs-YaxisFnmr-autbl-zoom}
\end{tabular}
\caption{\label{fig:stability-analysis-dataset-edc-examples-synthetic-data} EDC plot examples for the synthetic data.}
\end{figure}

Here ``fully synthetic'' means that no biometric samples are involved, only synthetic scores.
First, synthetic sample ``utility scores'' are randomly generated\footnote{$[-1,+1]$ range, but this range is not functionally important.},
each utility score representing one imaginary sample.
The set of utility scores is hidden to the QA algorithm ranking computation, insofar that it is only used to derive synthetic floating point QSs and mated CSs,
but it is not directly used as an input for the FNM-EDC evaluation.
For these experiments $50,000$ synthetic subjects with five samples each are considered, which means there are $50,000\cdot 5=250,000$ generated sample utility scores and $50,000\cdot {{5}\choose{2}}=500,000$ mated pairs.

Synthetic mated CSs are the pairwise minima of the utility scores, effectively assuming that the CS for a mated pair exclusively depends on the sample with lower utility.
Synthetic sample QSs are the utility scores plus/minus some random offset.
Different synthetic QA algorithms are defined simply by scaling this random offset for sample QS generation.
The random distributions are uniform for clarity.
As described in \autoref{sec:edc}, a pairwise QS is the minimum of the pair's sample QSs,
so without the random offset the pairwise QSs would be identical to the CSs in this synthetic setup.

\begin{figure}
	\centering
	\begin{tabular}{c}
		Variant 1 \\
		\includegraphics[width=\linewidth]{img/synth-data/pAUC20-SE10---Ranking-Error-Expected---Synthetic-DiffFiqaTypeB---Distance---pAUC-lim.} \\
		Variant 2 \\
		\includegraphics[width=\linewidth]{img/synth-data/pAUC20-SE10---Ranking-Error-Expected---Synthetic-DiffFiqaTypeC---Distance---pAUC-lim.}
	\end{tabular}
	\caption{\label{fig:stability-plots-synthetic-data} EDC rankings \vs{} expected ranking for synthetic data. Curves correspond to the starting error 0.01 (dark purple) to 0.10 (bright yellow), with the pAUC limit varying across the X-axis. The Y-axis shows the difference of the actual ranking to expected ranking (lower is better), times the QA algorithm count (five).}
\end{figure}

\begin{table}
	\caption{\label{tab:stability-statistics-adjusted-synthetic-data}Ranking stability statistics for synthetic data.}
	\centering
	\setlength{\tabcolsep}{1.8pt}
	\textbf{Variant 1}\\
	\begin{tabular}{r|rrr|rrr}
		\theader{QA algorithm} & \theader{Span} & \theader{Best (Min)} & \theader{Worst (Max)} & \theader{Median} & \theader{Mean} & \theader{Std.dev.} \\
		\hline
		SQA1 (0.05) & 1.57 & 1.00 & 2.57 & 1.00 & 1.01 & 0.14 \\
		SQA2 (0.10) & 4.00 & 1.00 & 5.00 & 2.30 & 2.31 & 0.61 \\
		SQA3 (0.15) & 3.99 & 1.00 & 4.99 & 3.55 & 3.47 & 0.71 \\
		SQA4 (0.20) & 3.84 & 1.00 & 4.84 & 4.38 & 4.29 & 0.64 \\
		SQA5 (0.25) & 2.40 & 2.60 & 5.00 & 5.00 & 4.99 & 0.21 \\
	\end{tabular}
	\\\textbf{Variant 2}\\
	\begin{tabular}{r|rrr|rrr}
		\theader{QA algorithm} & \theader{Span} & \theader{Best (Min)} & \theader{Worst (Max)} & \theader{Median} & \theader{Mean} & \theader{Std.dev.} \\
		\hline
		SQA1 (0.01) & 4.00 & 1.00 & 5.00 & 1.00 & 1.24 & 1.11 \\
		SQA2 (0.02) & 4.00 & 1.00 & 5.00 & 1.54 & 1.63 & 0.67 \\
		SQA3 (0.03) & 4.00 & 1.00 & 5.00 & 2.49 & 2.57 & 1.01 \\
		SQA4 (0.04) & 3.69 & 1.00 & 4.69 & 3.58 & 3.41 & 1.02 \\
		SQA5 (0.05) & 3.59 & 1.41 & 5.00 & 5.00 & 4.85 & 0.71 \\
	\end{tabular}
\end{table}

Two variants with different QS generation offset scaling values for the five synthetic QA algorithms are used in the following:
\begin{itemize}
\item Variant 1: $0.05,0.10,0.15,0.20,0.25$
\item Variant 2: $0.01,0.02,0.03,0.04,0.05$
\end{itemize}

The expected ranking for these synthetic QA algorithms corresponds to the order of the scaling values.
\Eg{} the lowest scaling value results in sample QSs closest to the utility scores, which in turn should result in the best placement within the computed ranking if the evaluation works as intended.

\autoref{fig:stability-analysis-dataset-edc-examples-synthetic-data} shows EDC plot examples for both synthetic variants,
whereby the labels of the synthetic QA (``SQA'') algorithms denote the expected ranking and the random offset scaling.
For example, ``SQA3 (0.15)'' has expected ranking 3 with random offset scaling 0.15.
The EDC curves differ substantially from real data curves (\eg{} \autoref{fig:edc-plot-example}),
the most relevant difference being that the expected ranking of the five synthetic QA algorithms is readily apparent by looking at the curves.
This indicates that the quantified pAUC rankings should also be able to reflect the expected ranking.

\subsection{Stability analysis}

The same 200 EDC pAUC discard limit and starting error configurations
used for the stability analysis on real data, defined in \autoref{sec:stability-edc-config},
are also used here.
In contrast to the real data analysis,
this analysis with synthetic data compares the 200 rankings against the expected ranking implied by the order of the synthetic QS generation offset scaling values,
instead of the mean ranking.
The instability is plotted in \autoref{fig:stability-plots-synthetic-data}, similar to the plot for real data in \autoref{fig:stability-plots-real-data}, but comparing against expected rankings in \autoref{eq:stability-synth},

\begin{equation}\label{eq:stability-synth}
RankingDivergence = \Sigma^n_i |p_i-e_i|
\end{equation}

$n$ being the QA algorithm count (here five), $p_i$ being one QA algorithm's placement in the ranking, and $e_i$ being the expected placement of the QA algorithm.
The expected placement value is the min-max normalised QS generation offset scaling value, so the values are $0.00, 0.25, 0.50, 0.75, 1.00$ for both variant 1 and 2.

Analogous to the observations in the real data analysis,
low pAUC limits can lead to rankings that deviate strongly from the expected rankings,
in contrast to higher pAUC limits for which the ranking divergence is comparatively small irrespective of the starting error choice.
At low pAUC limits, note that there is no specific starting error that leads to the lowest ranking divergence across all cases (\ie{} dataset and QA algorithm setups), which can be observed both for the synthetic data (\autoref{fig:stability-plots-synthetic-data}) and the real data (\autoref{fig:stability-plots-real-data}).

\autoref{tab:stability-statistics-adjusted-synthetic-data} lists statistics
similar to \autoref{tab:stability-statistics-real-data} from the real data analysis.
The ``Median'' and ``Mean'' rankings approximate the expected rankings. 1 to 5 from top to bottom,
and the ``Span'' column values highlight that all of the synthetic FIQA algorithms' individual rankings could be optimised by selecting specific pAUC limit/starting error configurations,
as long as those configurations are not constrained to higher pAUC limits.

In summary,
the main advantage of this synthetic analysis over the real data analysis is that the stability is tested relative to expected rankings, which are unknown for real algorithms.
The synthetic analysis supports the conclusion that higher (but still operationally relevant) pAUC limits should be preferred,
while the choice of the starting error becomes less important with higher pAUC limits.
