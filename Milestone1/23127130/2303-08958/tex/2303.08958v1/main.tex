%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{array,multirow}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor


\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath, bm}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{tabularx}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\def\tce{\textcolor{red}}
\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{NESS: Learning Node Embeddings from Static SubGraphs}

\begin{document}

\twocolumn[
\icmltitle{NESS: Learning Node Embeddings from Static SubGraphs}

% Alternative title
% \icmltitle{Divide and Conquer: 
% Enhancing the Expressive Power of Encoders by \\ Learning From Random Subspaces}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Talip Uçar}{to}

\end{icmlauthorlist}

\icmlaffiliation{to}{Data Science \& AI, R\&D, AstraZeneca}


\icmlcorrespondingauthor{Talip Uçar}{talip.ucar@astrazeneca.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{AstraZeneca} % otherwise use the standard text.


\begin{abstract}
We present a framework for learning \textbf{N}ode \textbf{E}mbeddings from \textbf{S}tatic \textbf{S}ubgraphs (NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we propose a novel approach for contrastive learning in the same setting. We demonstrate that using \textit{static} subgraphs during training with a GAE improves node representation for link prediction tasks compared to current autoencoding methods using the entire graph or stochastic subgraphs. NESS consists of two steps: 1) Partitioning the training graph into subgraphs using random edge split (RES) during data pre-processing, and 2) Aggregating the node representations learned from each subgraph to obtain a joint representation of the graph \textit{at test time}. Our experiments show that NESS improves the performance of a wide range of graph encoders and achieves state-of-the-art (SOTA) results for link prediction on multiple benchmark datasets.

\end{abstract}

\section{Introduction}

Link prediction in graphs is a firmly established problem in the literature, especially in the area of network analysis, and has a wide range of applications in many domains such as social, biological and transportation networks \citep{zhang2020autosf, qi2006evaluation, chami2019hyperbolic}, recommender systems \citep{zhang2018link} and cybersecurity \citep{liben2003link}. The problem is usually tackled by first learning the node embeddings that can capture both topology and node features of the graph to represent graph data in a low dimensional latent space. The various methods have been proposed to extract node embeddings such as matrix factorization \citep{ahmed2013distributed, cao2015grarep, katz1953new}, random walk based methods \citep{perozzi2014deepwalk, grover2016node2vec} and graph convolutional networks \citep{kipf2016semi, kipf2016variational,  hamilton2017inductive, chiang2019cluster}. Among them, one popular method to learn node embeddings is graph autoencoders (GAE) \citep{kipf2016variational}, which is proposed as an extension of autoencoder (AE) \citep{rumelhart1985learning} to graph domain and is one of the most popular unsupervised methods to learn node embeddings. Similar to AE, it consists of: i) An encoder, which is usually based on a graph neural network (GNN) to encode local graph structure and features of nodes in the graph, $G$, into a latent representation $\pmb{Z} \in \mathcal{R}^{N \times d}$, where $N$ is the total number of nodes in the graph and $d$ is the feature dimension. ii) A decoder, which is used to reconstruct the original graph from the latent representation, $\pmb{Z} $. Once the model is trained, we can use $\pmb{Z} $ for downstream tasks such as link prediction \citep{kipf2016variational, berg2017graph}, node and graph classification \citep{rong2019dropedge, kipf2016semi}, graph generation \citep{jin2018junction, simonovsky2018graphvae, samanta2020nevae}, and node clustering \citep{hasanzadeh2019semi, pan2018adversarially}.


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{images/SubGraph.png}}
\caption{\textbf{NESS framework:} Learning node embeddings from static subgraphs}\label{fig:subgraph_framework}
\end{center}
\vskip -0.2in
\end{figure*}


Although GNN-based GAE \citep{kipf2016variational} is effective in learning low dimensional node embeddings, it has a few shortcomings. To begin with, it suffers from scalability issues due to the following reasons \citep{chiang2019cluster}: i) Standard GAE framework proposed in \citep{kipf2016variational} uses a transductive setting and trains the model using the full-batch gradient descent, leading to a large memory requirement. ii) The time complexity and computational overhead grow with the number of layers in the encoder due to neighborhood expansion problem \citep{chiang2019cluster}. To compute the loss on a single node at layer $L$, we need the embeddings of its neighbors at the layer $L-1$ and the neightborhood grows as the number of layers increases. iii) Reconstructing a large graph by using the inner-product of node embeddings is computationally costly, O($N^2$) \citep{salha2021fastgae}. Moreover, the standard GNN-based GAEs are not expressive enough to extract useful node embeddings for link prediction task \citep{pan2021neural, xu2018powerful, morris2019weisfeiler}, in which learning the topology of a graph is important. Although GNN-based encoders can capture some knowledge of topology, they are rather limited: when the encoders are shallow (i.e. having few layers), they can only capture the immediate neighborhood around the nodes. When they are too deep, new problems such as over-smoothing \cite{li2018deeper, chen2020measuring} and over-squashing \citep{alon2020bottleneck} arise. Over-smoothing is a result of node representations converging to a constant while over-squashing emerges when too many messages are compressed to a fixed length vector \citep{alon2020bottleneck}. Moreover, GAEs are prone to overfitting since their optimisation objective is based on the reconstruction of a large sparse graph, resulting in sub-par performance for link prediction \citep{Goyal_2018}. 

Despite its shortcomings, using GAEs is a natural way for addressing link prediction problem since we use a relevant task, i.e. reconstruction of known links, to learn the node embeddings. In recent years, a number of proposals have focused on leveraging subgraphs and various sampling approaches to address the scalibility issue in GAEs \citep{hamilton2017inductive, chiang2019cluster, zeng2019graphsaint, salha2021fastgae, chen2018fastgcn, chen2017stochastic}. In contrast, we focus on using subgraphs to improve the predictive performance of node embeddings for link prediction under the transductive setting. Our approach differs from those in the literature in the following way: We first partition the training graph into multiple \textit{static} subgraphs during data preprocessing step rather than using stochastic subgraphs adapted in other works during training. In NESS, subgraphs are partitioned to be same or similar in size, and do not share any edges. We then train a single GAE using the subgraphs in a transductive setting. Thus, NESS can be considered as turning trasductive learning on a large graph into multiple instances of transductive learning, one for each of the subgraph. One of the benefits of NESS is that working with pre-defined subgraphs allows us to match the setup during training to the one at test time, especially if the size of the subgraphs is similar to that at test time. The main contributions of this paper can be summarized as:


\begin{itemize}
    \item We propose a new framework, NESS, for learning node embeddings from \textit{the static subgraphs} in GAE setting and empirically demonstrate its effectiveness on multiple benchmark datasets with various characteristics.
    \item We incorporate the advantages of contrastive learning to improve the performance of our proposed method for various encoder types such as GCN \citep{kipf2016semi}, GAT \citep{velivckovic2017graph}, ARGA \citep{pan2018adversarially} etc.
    \item We show that NESS improves the performance of existing GNN-based GAEs for link prediction task, achieving SOTA performance on multiple datasets.
    \item We conduct extensive experiments to compare various sampling approaches under GAE setting, giving insights into how they perform across many datasets and encoder types.
\end{itemize}



\section{Method}

\textbf{Definitions} Without the loss of generality, we assume a transductive learning setting throughout this work. We define an undirected, unweighted large graph as $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with vertices $\mathcal{V}$ and edges $\mathcal{E}$. We refer to its adjacency matrix as $\pmb{A} \in \mathcal{R}^{N \times N}$ and collection of its nodes as $\pmb{X} \in \mathcal{R}^{N \times f}$. We assume that diagonal elements of $\pmb{A}$ is set to 1, i.e. including self-loops. For the autoencoder setting, we define the latent variable as $\pmb{Z} \in \mathcal{R}^{N \times d}$. $N=|\mathcal{V}|$ is the total number of nodes in the graph while $f$ and $d$ are the feature dimension of nodes in $\pmb{X}$ and $\pmb{Z}$ respectively. In this work, we use lowercase letters to refer to subgraphs, while , depending on the context, the capital letters are associated with either the original large training graph, or the entire graph that include training, validation and test. Thus, we define $k$ subgraphs, their node representations, adjacency matrices, edges and latent representations as $\{\pmb{g_1}, \pmb{g_2}, ...,\pmb{g_k}\}$, $\{\pmb{x_1}, \pmb{x_2}, ...,\pmb{x_k}\}$, $\{\pmb{a_1}, \pmb{a_2}, ...,\pmb{a_k}\}$, $\{\pmb{e_1}, \pmb{e_2}, ...,\pmb{e_k}\}$ and  $\{\pmb{z_1}, \pmb{z_2}, ...,\pmb{z_k}\}$ respectively. We formally define $k^{th}$ subgraph as $\pmb{g_k} = (\pmb{v_k}, \pmb{e_k})$ with vertices $\pmb{v_k}$ and edges $\pmb{e_k}$. The adjancency matrix $\pmb{a}_k$ can be considered as the masked version of $\pmb{A}$ while still preserving the self-loops for all nodes: $\pmb{a}_k = \pmb{M}_k \odot \pmb{A}$.




\subsection{Model} 
Figure~\ref{fig:subgraph_framework} presents our framework, the NESS. We have an encoder (E), a decoder (D), and an optional projection network (P). For the purpose of this paper, we will use $\pmb{z_k}$ ($\pmb{Z}$) for node embeddings and $\pmb{\Hat{a}_k}$ ($\pmb{\Hat{A}}$) for the reconstruction of $k^{th}$ subgraph (the large graph) while using $\pmb{h_k}$ for the projection. Moreover, throughout this work, when we say that a representation is "good", we refer to its performance in the link prediction task.

In NESS framework, after splitting the large graph into train, validation and test sets, we further divide the training set into subgraphs during data preparation step. Each subgraph is fed to the same encoder (i.e. parameter sharing) to get their corresponding latent representation. Then, we reconstruct adjacency matrix of the same subgraph by using the inner product decoder. Thus, for $k^{th}$ subgraph, we have:

\textbf{Encoder:} $\pmb{z}_{k}=E(\pmb{X}, \pmb{a}_k)$, and  
\textbf{Decoder:} $\Hat{\pmb{a}}_{k}=\sigma(\pmb{z}_{k}^T\pmb{z}_{k})$.

It is important to note that since we operate in transductive setting, each $\pmb{z}_k$ corresponding to subgraph $\pmb{g}_k$ actually contains the embeddings of all $N$ nodes of graph $G$ (i.e. $\pmb{z}_k \in \mathcal{R}^{N \times d}$). The embeddings of nodes that are in subgraph $\pmb{g}_k$ are computed by using their neighborhood information (shown as a colored region in each of $\pmb{z}_1$, $\pmb{z}_2$, and $\pmb{z}_3$ in Figure~\ref{fig:subgraph_framework}) while the embeddings for the rest of the nodes are computed using only self-loops (shown as grey regions in the same figure). When reconstructing the adjancency matrix $\Hat{\pmb{a}}_{k}$, we only need the nodes that are in the subgraph $\pmb{g}_k$ to compute scores for links in $\pmb{g}_k$. From Figure~\ref{fig:subgraph_framework}, we see that training the model with a static subgraph emulates the same transductive learning setting as the one proposed in \citep{kipf2016variational} using the larger graph. Hence, NESS turns the standard transductive setting of GAE to multiple transductive settings, one for each subgraph.


\textbf{Projector:} Since we incorporate an optional constrastive loss during training, we have an additional network to project the latent variables. Thus, we compute contrastive loss using the projections rather than the latent variables since it is shown to be more effective \citep{chen2020simple}.




\begin{algorithm}[tb]
   \caption{NESS}
   \label{alg:ness}
\begin{algorithmic}
   \STATE {\bfseries Input:} Graph $G=(V, E)$, adj. $\bm A$, node features $\bm X$
   \STATE {\bfseries Output:} Node representation $\bm Z$
   \STATE {\bfseries Initialize:} Encoder $Enc$
   \STATE Partition $G$ into $k$ static $subgraphs=[\bm a_1,\bm a_2, ...,\bm a_k]$ by using random edge split.
   \FOR{$epoch=1$ {\bfseries to} $max\_epocs$}
    \STATE $loss=0$, $z\_list=[]$
        \FOR{$a_k$ {\bfseries in} subgraphs}
            \STATE $\bm z_{k} = Enc(\bm X, \bm a_k)$  
            \STATE $\Hat{\bm a}_{k} = \sigma(\bm z_{k}\bm z_{k}^T)$  
            \STATE $loss = loss + recon\_loss(\bm a_{k},\Hat{\bm a}_{k})$  
            \STATE $z\_list.append(\bm z_{k})$  
        \ENDFOR
        \IF{contrastive loss == True}
            \STATE Get combinations of $\bm z_{k}$'s: $[(\bm z_{1}, \bm z_{2}), ...]$
            \STATE Compute contrastive loss $\mathcal{L}_c$, based on Eq.~\ref{eq4}
            \STATE $loss = loss + \mathcal{L}_c$
        \ENDIF
    \STATE Update encoder $Enc$
   \ENDFOR
    \STATE \bfseries  Return $\bm Z=aggregate(\bm z_{1},\bm  z_{2}, ...,\bm  z_{k})$

\end{algorithmic}
\end{algorithm}



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
     \begin{subfigure}[c]{0.19\textwidth}
         \includegraphics[width=\textwidth]{images/_cora_rankings_clFalse_dlFalse_anFalse.png}
         \caption{Cora}
     \end{subfigure}
     \begin{subfigure}[c]{0.19\textwidth}
         \includegraphics[width=\textwidth]{images/_citeseer_rankings_clFalse_dlFalse_anFalse.png}
         \caption{Citeseer}
     \end{subfigure}
     \begin{subfigure}[c]{0.19\textwidth}
         \includegraphics[width=\textwidth]{images/_pubmed_rankings_clFalse_dlFalse_anFalse.png}
         \caption{Pubmed}
     \end{subfigure}
     \begin{subfigure}[c]{0.08\textwidth}
         \includegraphics[width=\textwidth]{images/legends1a.png}
     \end{subfigure}
     \begin{subfigure}[c]{0.19\textwidth}
         \includegraphics[width=\textwidth]{images/citationnet_GNAE.png}
         \caption{GNAE}
     \end{subfigure}
     \begin{subfigure}[c]{0.12\textwidth}
         \includegraphics[width=\textwidth]{images/legends1b.png}
     \end{subfigure}     
\caption{Comparing NESS to standard GAE (SGAE) and some of the other settings described in Section~\ref{exp_setup}.  Compared to baseline methods, NESS gives a significant performance boost across all encoder types and datasets for link prediction task. \textbf{a-c)} Listing results for multiple encoder types for Cora, Citeseer and Pubmed respectively. \textbf{d)} The performance for GNAE encoder across all settings for all three datasets in detail.}\label{fig:comparing_dynamic_to_static}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Training}
Our objective function is:
\begin{equation} \label{eq2}
\mathcal{L}_t =\mathcal{L}_r+\mathcal{L}_c,
\end{equation}
where $\mathcal{L}_t$, $\mathcal{L}_r$, and $\mathcal{L}_c$ are the total, reconstruction, and contrastive losses respectively.

\textbf{i) Reconstruction loss:} Given a subgraph $\pmb{g_k}$, we can reconstruct either the same subgraph, $\pmb{\Hat{g}_k}$, or the larger training graph $\pmb{\Hat{G}}$. 
Then, we can compute the reconstruction loss for $k^{th}$ subgraph by computing binary cross-entropy loss using either the subgraph and its reconstructed counterpart $(\pmb{a_k}, \pmb{\Hat{a}_k})$, or the large graph and the corresponding reconstructed graph $(\pmb{A},\pmb{\Hat{A}_k})$ pair as shown in Figure \ref{fig:subgraph_framework}. We choose the former, i.e. $(\pmb{a_k}, \pmb{\Hat{a}_k})$, since it is either on par or more effective in our experiments as shown in Figure~\ref{fig:ness_ablation1}b. Reconstruction loss is computed for both positive and negative edges, where we obtain negative edges by randomly sampling same number of negative edges as positive ones: 

\begin{equation}\label{eq3}
\begin{aligned}
&\mathcal{L}_r = \frac{1}{K}\sum_{k=1}^{K} l_k \mbox{, \, and }\\
l_k = -\frac{1}{2*n_{k_p}} &\sum_{i=1}^{n_k} [ \log \pmb{\Hat{e}}_{ki} + \log(1-\lnot\pmb{\Hat{e}}_{ki})]
 \end{aligned}
\end{equation}

where $\mathcal{L}_r$ is the average of reconstruction losses over all subgraphs, $K$ is the total number of subgraphs, $l_k$ is the reconstruction loss for $k^{th}$ subgraph, $n_{k_p}$ is the number of positive edges in the edge set $\pmb{e}_k$ while $\pmb{\Hat{e}}_{ki}$ and $\lnot\pmb{\Hat{e}}_{ki}$ refer to the scores for $i^{th}$ positive edge  and sampled negative edge for $k^{th}$ subgraph respectively.


\textbf{ii) Contrastive loss:}
Referring to \{$\bm{z}_1$, $\bm{z}_2$, $\bm{z}_3$\} in the example in Figure~\ref{fig:subgraph_framework}, since we are in transductive setting, we obtain two different latent representations for each node. If the node is in a subgraph, we obtain its embedding using its neighborhood information. Otherwise, we get its embedding using only the self-loop. Therefore, we can add a constrastive loss to our objective function by computing the constrastive loss between any two pairs of latent variables from the set of all latent variables \{$\bm{z}_1$, $\bm{z}_2$, $\bm{z}_3$\}. For example, given three subgraphs as in Figure \ref{fig:subgraph_framework}, there are three combinations of two: $\binom{n}{k} = \binom{3}{2} = \frac{3!}{2!(1)!} = 3$ . For four subgraphs, it would be 6 pairs of combination, and so on. Moreover, instead of using \{$\bm{z}_1$, $\bm{z}_2$, $\bm{z}_3$\} directly, we first project them using a projection network (P) to obtain \{$\bm{h}_1$, $\bm{h}_2$, $\bm{h}_3$\} \citep{chen2020simple}. Nodes at the same rows of two projections, e.g., $\bm{h_1}$ and $\bm{h_2}$, can be considered as positive pairs while remaining rows are considered as negatives to those nodes.This allows us to compute the contrastive loss for each pair of projections using a loss function such as the normalized temperature-scaled cross entropy loss (NT-Xent) \citep{chen2020simple}. For three subgraphs, \{$\bm{g}_1$, $\bm{g}_2$, $\bm{g}_3$\}, we can compute such a loss for every pair $\{\bm{h_a}, \bm{h_b}\}$ of total three pairs from the set $S=\{\{\bm{h_1, h_2}\},\{\bm{h_1, h_3}\},\{\bm{h_2, h_3}\}\}$. Overall contrastive loss is: 

\begin{eqnarray}\label{eq4}
\mathcal{L}_c = \frac{1}{J}\sum_{\{\bm{h_a},\bm{h_b}\}\in S}p(\bm{h_a},\bm{h_b})
\end{eqnarray} 
\begin{eqnarray}\label{eq5}
\resizebox{0.44\textwidth}{!}{$p(\bm{h_a},\bm{h_b})=\frac{1}{2N}\sum_{i=1}^{N}\left[l(\bm{{h_a}^{(i)}}, \bm{{h_b}^{(i)}}) + l(\bm{{h_b}^{(i)}}, \bm{{h_a}^{(i)}})\right]$}
\end{eqnarray} 
\begin{eqnarray}\label{eq6}
\resizebox{0.44\textwidth}{!}{$l(\bm{{h_a}^{(i)}}, \bm{{h_b}^{(i)}})=-\log\frac{\exp(sim(\bm{{h_a}^{(i)}},\bm{{h_b}^{(i)}})/\tau)}{\sum_{k=1}^{2N} \mathds{1}_{k \neq i} \exp(sim(\bm{{h_a}^{(i)}}, \bm{{h_b}^{(k)}})/\tau)}$}
\end{eqnarray} 
where $J$ is the total number of pairs in set $S$, $\tau$ is the temperature coefficient, $p(\bm{h_a},\bm{h_b})$ is total contrastive loss for a pair of projection $\{\bm{h_a}, \bm{h_b}\}$, $l(\bm{{h_a}^{(i)}}, \bm{{h_b}^{(i)}})$ is the loss function for a corresponding positive pairs of nodes \{$\bm{{h_a}^{(i)}}, \bm{{h_b}^{(i)}}$\} in subgraphs $\{\bm{h_a}, \bm{h_b}\}$, and $\mathcal{L}_c$ is the average of contrastive losses over all pairs. 

\subsection{Test time}
At test time, we use an aggregation function to aggregate the latent variables of all subgraphs to obtain the joint embedding of the graph $\mathcal{G}$. 

\begin{equation}\label{eq9}
\bm{Z}=\frac{1}{K}\sum_{k=1}^{K} \bm{z_k}
\end{equation}

where $K$ is the number of subgraphs. Please note that we can use any aggregation method including mean, sum, min, max, or any other permutation invariant method. We used mean aggregation in all our experiments. The NESS framework is summarized as a pseudocode in Algorithm~\ref{alg:ness}.



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
     \begin{subfigure}[c]{0.21\textwidth}
         \includegraphics[width=\textwidth]{images/g_vs_agg.png}
         \caption{Aggregation}
     \end{subfigure}
     \begin{subfigure}[c]{0.21\textwidth}
         \includegraphics[width=\textwidth]{images/full2sub_allSG2SG4SG8.png}
         \caption{Reconstruction}
     \end{subfigure}
     \begin{subfigure}[c]{0.21\textwidth}
         \includegraphics[width=\textwidth]{images/ness_recon2cl.png}
         \caption{Contrastive learning}
     \end{subfigure}
     \begin{subfigure}[c]{0.20\textwidth}
         \includegraphics[width=\textwidth]{images/ness_samplingGNAE2.png}
         \caption{Sampling methods}
     \end{subfigure}
     \begin{subfigure}[c]{0.12\textwidth}
         \includegraphics[width=\textwidth]{images/full2sub_legends.png}
     \end{subfigure}

\caption{Experiments with NESS: \textbf{a)} The difference in AUC scores (\%) obtained in two ways for NESS2: i) Aggregating the latent variables of static subgraphs i.e. $\bm Z=Agg(\bm z_1, \bm z_2)$, (our default case), ii) Getting the latent variable of the entire graph directly i.e. $\bm Z=E(\bm X, \bm A)$ at test time. The aggregation performs better across all models and datasets. Performance change (\%): \textbf{b)} when we reconstruct subgraphs rather than full graph. \textbf{c)} when we use contrastive and reconstruction loss, compared to only reconstruction loss. \textbf{d)} when comparing NESS2 to other sampling strategies.}\label{fig:ness_ablation1}
\end{center}
\vskip -0.2in
\end{figure*}


\section{Experiments}\label{exp_setup}


\textbf{Datasets} We utilize three standard benchmark datasets from citation networks (Cora, Citeseer and Pubmed) \citep{sen2008collective}, where nodes and edges correspond to documents and undirected citations respectively. Node features are represented as bag-of-words representation of a document. These datasets are considered the homophilic graphs, where nodes tend to connect with similar other nodes. \citep{mcpherson2001birds}. Moreover, we consider three datasets (Cornell, Texas, Wisconsin) from WebKB \citep{pei2020geom}, which include web pages from computer science departments of multiple universities. In this case, node features are the bag-of-words representation of web pages. Finally, we consider Chameleon dataset \citep{rozemberczki2021multi}, which is a wikipedia page-page graph under the topic chameleon. Chameleon is considered a heterophilic graph, in which connected nodes are prone to having different properties or labels \citep{zhu2020beyond, wang2022augmentation}. Statistics of all datasets are summarised in Table~\ref{data_stats} in the Appendix.

\textbf{Data pre-processing} Following the experimental protocols in \citep{kipf2016variational}, we split the graph into three parts: 10\% testing, 5\% validation, 85\% training sets, using the random edge-level split (RES). We then perform RES to generate $k$ subgraphs from training set. The splits are performed in a way that subgraphs do not share any edge. We use $k \in [2, 4, 8]$ in our experiments. Please note that since we are in the transductive learning setting, the training algorithm has access to feature vectors of all nodes.


\textbf{Models} Since NESS is a general framework for any GNN-based autoencoding setting, we can use any GNN-based encoder as our encoder-backbone to obtain node representations. Different encoders adopt different mechanisms to learn from both graph topology and node features, resulting in different node representations. In our experiments, we use GCN \citep{kipf2016variational}, GAT \citep{velivckovic2017graph}, GNAE \citep{ahn2021variational}, as well as models such as ARGA \citep{pan2019learning} that use adversarial settings. We also use a linear version of GCN-based encoder, referred as Lin, in which we don't use any non-linear activation function. All encoders are shallow (one or two layers) with the final layer dimension of 32. We implement all models similar to their original implementation and list the details of models in Section~\ref{details_of_models} of the Appendix. We also refer the reader to cited works for the further details of each encoder.

%Finally, we also include results for variational counterparts of GCN, GNAE, Lin and ARGA in the Appendix. 


\textbf{Baselines} The NESS method is designed for GAEs in transductive learning setting, so we use two other frameworks as our main baselines: i) The standard GAE framework from \citep{kipf2016variational}, where the input and output of the model is the entire training graph. We refer to this setup as SGAE when reporting results. ii) The FastGAE framework from \citep{salha2021fastgae}, where the input to the model is the entire training graph, and the model reconstructs dynamically sampled subgraphs at the output. We use our own random edge splitting (RES) method to generate subgraphs instead of the node sampling method used by FastGAE, to make it easier to compare different GAE setups. For FastGAE, we sample 50\% of the training graph per epoch when generating subgraphs during training. We also report the original results for the link prediction task from FastGAE, in addition to the results from our own implementation.

As a variation of the original FastGAE setting, we experiment with a setup where the encoder encodes a dynamically sampled subgraph, while the decoder tries to reconstruct the same subgraph. We use the RES method to sample a subgraph per epoch, with three sampling rates: 50\%, 75\%, and 88\% of the training graph. We refer to these as DS50, DS75, and DS88 respectively (DS stands for Dynamic Sampling).

To test whether doing our RES partitioning during data preprocessing matters, we also experiment with its stochastic counterpart: We partition the training graph into $k$ subgraphs dynamically by using RES during training. The main difference is that we sample new $k$ subgraphs in every iteration (i.e. not static). In particular, we generate two random subgraphs per epoch, each of which is 50\% of the training graph. We refer to this setting as dynamic random edge splitting with 2 subgraphs (DRES2). Please note that DRES2 partitions graph into two subgraphs each iteration without missing any training edges while DS50 generates one subgraph using 50\% of edges per iteration.

For NESS framework, we experiment with 2, 4, and 8 static subgraphs and refer them as NESS2, NESS4, and NESS8 in our experiments respectively. For example, for NESS8, we randomly split the training set of the original graph into 8 subgraphs with no shared edges and use the same 8 subgraphs throughout training. Please note that we use only the reconstruction loss for NESS in these experiments. So, for all baselines and NESS, we use the same setup and pipeline. Only thing that differs between them is whether we use the whole graph, stochastic subgraphs, or static subgraphs. We compare aforementioned settings by using multiple encoders and benchmark datasets. The summary of all settings is in Table~\ref{summary_settings}, showing how they differ.



\begin{table}[ht]
\caption{Summary of settings for the baselines and NESS. ($\pmb{\Bar{a}_k}, \pmb{\Bar{z}_k}$) in NESS are deterministic while ($\pmb{\Tilde{a}_k}, \pmb{\Tilde{z}_k}$) indicate stochastic process in other settings that use sampling during training. Subscript $k$ indicates $k^{th}$ subgraph in settings where we use multiple subgraphs per epoch during training.}
\label{summary_settings}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1.0\columnwidth}{!}{
{\begin{tabular}{lcccr}
\toprule
Method & Encoding & Decoding & $\pmb{Z}$ at test time \\
\midrule
SGAE    & $\pmb{Z}=E(\pmb{X}, \pmb{A})$ & $\pmb{\Hat{A}}=\sigma(\pmb{Z}\pmb{Z}^T)$&   $\pmb{Z}=E(\pmb{X}, \pmb{A})$ \\
FGAE    & $\pmb{Z}=E(\pmb{X}, \pmb{A})$ & $\pmb{\Hat{a}}=\sigma(\pmb{\Tilde{z}}\pmb{\Tilde{z}}^T)$&   $\pmb{Z}=E(\pmb{X}, \pmb{A})$  \\
DS50    & $\pmb{\Tilde{z}}=E(\pmb{X}, \pmb{\Tilde{a}})$ & $\pmb{\Hat{a}}=\sigma(\pmb{\Tilde{z}}\pmb{\Tilde{z}}^T)$& $\pmb{Z}=E(\pmb{X}, \pmb{A})$  \\
DRES2     &$\pmb{\Tilde{z}_k}=E(\pmb{X}, \pmb{\Tilde{a}_k})$ & $\pmb{\Hat{a}_k}=\sigma(\pmb{\Tilde{z}_k}\pmb{\Tilde{z}_k}^T)$ & $\pmb{Z}=E(\pmb{X}, \pmb{A})$ \\
NESS      & $\pmb{\Bar{z}_k}=E(\pmb{X}, \pmb{\Bar{a}_k})$ & $\pmb{\Hat{a}_k}=\sigma(\pmb{\Bar{z}_k}\pmb{\Bar{z}_k}^T)$&   $\pmb{Z}=Agg(\pmb{\Bar{z}_1}, ...,\pmb{\Bar{z}_k})$      \\
% \rowcolor{lightgray} Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
\bottomrule
\end{tabular}}{}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Training and optimisation} We use AdamW optimizer \citep{loshchilov2017decoupled} with a learning rate of 0.01, $betas=(0.9, 0.999)$ and $eps=1e-07$ for all of our experiments. We set the maximum number of epochs as 400, and use early stopping by using validation set loss with patience of 20 epochs i.e. we keep training the model until the validation loss starts increasing. We then take the optimal model based on the smallest validation loss.


\textbf{Evaluation} Following previous works \citep{kipf2016variational}, we evaluate link prediction by measuring  the area under the ROC curve (AUC) and average precision (AP) scores on the test set. For all experiments, we repeat each experiment ten times with different random seeds (i.e. ten different train-val-test splits of the graph and different model initialisation).



\section{Results}\label{results}
\textbf{Comparing NESS to other settings:} We train GAEs with five encoder types (GCN, GAT, GNAE, Lin, ARGA) on seven datasets across all settings. The results, presented in Figure~\ref{fig:comparing_dynamic_to_static}(a-c), show performance for citation networks using three settings: i) NESS, ii) Standard GAE (SGAE) and iii) Dynamically sampled subgraphs (DS50, DS75, DS88). We can make the following observation:
\begin{itemize}
    \item As seen in Figure~\ref{fig:comparing_dynamic_to_static}, when we dynamically sample larger subgraphs (i.e. going from DS50 to DS88) for both encoding and decoding, performance improves and reaches a level similar to SGAE training. This is expected as the use of stochastic subgraphs for both encoder and decoder approximates the SGAE setting.
    \item NESS outperforms other settings, and its performance improves as we partition the graph into more subgraphs (i.e. moving from NESS2 to NESS8). This trend holds true across different types of encoders. As shown in Figure~\ref{fig:comparing_dynamic_to_static}d, NESS even improves the performance of GNAE, which is a SOTA model and already performs well on multiple datasets under the SGAE setting.
\end{itemize}

From our results, it is clear that NESS consistently outperforms SGAE, which in turn performs better than GAEs trained with stochastic subgraphs. We also compare NESS with other settings such as FastGAE and DRES2. A comprehensive comparison is presented in Table~\ref{table_link_prediction} using GNAE as the encoder for all settings, with the exception of FGAE1 which uses GCN for direct comparison with the original implementation (FGAE*). Results from recent state-of-the-art method WalkPooling (WP) are also included, even though it is not based on GAE. As far as we know, NESS in combination with GNAE achieves new state-of-the-art performance on link prediction task for five datasets: Cora, Citeseer, and all three WebKB datasets. Other observations from Table~\ref{table_link_prediction} include: i) DRES2 performs slightly better than FGAE2 in general. ii) Similarly, FastGAE (FGAE2) performs either on par or better than SGAE. iii) These observations suggest that reconstructing subgraphs gives better performance than reconstructing the entire graph, which may be due to the model being less prone to overfitting. Additionally, encoding multiple subgraphs (DRES2) seems to be more effective than encoding one large graph (FGAE2).



\begin{table*}[tb]
\caption{Link prediction results across benchmark datasets for NESS and other settings for GNAE as encoder. NESS outperforms other settings in our experiments on all datasets and the most recent SOTA model WalkPooling (WP) on five out of seven datasets, achieving new SOTA peformance \cite{pan2021neural}. FGAE* and WP* refer to the best reported numbers in the original works. The results from our own implementation of FastGAE with GCN encoder (FGAE1) and with GNAE (FGAE2) are also listed for comparison.}
\label{table_link_prediction}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1.0\textwidth}{!}{
{\begin{tabular}{lllllllllllllll}
\toprule
               & \multicolumn{2}{c}{\textbf{chameleon}}                             & \multicolumn{2}{c}{\textbf{citeseer}}                              & \multicolumn{2}{c}{\textbf{cora}}                                  & \multicolumn{2}{c}{\textbf{cornell}}                               & \multicolumn{2}{c}{\textbf{pubmed}}                                & \multicolumn{2}{c}{\textbf{texas}}                                 & \multicolumn{2}{c}{\textbf{wisconsin}}                             \\
\midrule
& \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{AP}} & \multicolumn{1}{c}{\textbf{AUC}} \\
\midrule
\textbf{WP*}    & \multicolumn{1}{c}{-}  & \textbf{99.52±0.08}   & \multicolumn{1}{c}{-}    & 95.94±0.53  & \multicolumn{1}{c}{-}   & 95.90±0.50 & \multicolumn{1}{c}{-}  & 82.39±8.92  & \multicolumn{1}{c}{-}     & \textbf{98.72±0.1}  & \multicolumn{1}{c}{-}   & 76.02±7.05   & \multicolumn{1}{c}{-}  & 82.27±6.27     
\multirow{3}{*}{\STAB{\rotatebox[origin=c]{90}{}}} \\
\textbf{FGAE*}    & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{90.16±1.20}    & 90.22±1.14  & \multicolumn{1}{c}{92.36±1.11}   & 91.72±0.98 & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{-}  & \multicolumn{1}{c}{96.35±0.19}     & 96.12±0.2  & \multicolumn{1}{c}{-}   &  \multicolumn{1}{c}{-}   & \multicolumn{1}{c}{-}  &  \multicolumn{1}{c}{-}     \\
\midrule
\textbf{FGAE1}  & 92.8±0.73                       & 93.07±0.72                       & 85.82±1.93                      & 84.47±1.64                       & 82.13±1.3                       & 82.28±1.3                        & 68.17±5.04                      & 69.48±4.69                       & 92.83±0.4                       & 92.61±0.56                       & 61.21±4.34                      & 64.75±1.28                       & 73.75±8.01                      & 74.51±6.57                      \\
\textbf{FGAE2} & 96.92±0.28                      & 96.87±0.2                        & 98.94±0.11                      & 98.76±0.18                       & 96.81±0.55                      & 96.3±0.44                        & 82.05±0.61                      & 86.32±1.42                       & 96.33±0.13                      & 96.32±0.08                       & 83.94±1.86                      & 87.0±2.43                        & 93.68±3.85                      & 93.64±3.4                        \\
\textbf{DS50}    & 96.62±0.27                      & 96.54±0.16                       & 98.4±0.04                       & 98.19±0.04                       & 95.93±0.43                      & 95.28±0.54                       & 81.05±5.21                      & 82.76±4.3                        & 96.28±0.09                      & 96.28±0.05                       & 82.92±1.46                      & 87.28±1.63                       & 94.9±1.54                       & 94.61±0.96                       \\
\textbf{DS75}    & 96.85±0.29                      & 96.81±0.15                       & 98.46±0.14                      & 98.25±0.01                       & 96.2±0.49                       & 95.69±0.54                       & 80.91±1.53                      & 84.88±1.59                       & 96.31±0.14                      & 96.32±0.11                       & 82.59±1.58                      & 86.21±2.62                       & 92.68±2.08                      & 92.93±1.83                       \\
\textbf{DS88}    & 96.9±0.28                       & 96.89±0.16                       & 98.39±0.13                      & 98.14±0.11                       & 96.26±0.56                      & 95.83±0.52                       & 81.64±0.61                      & 85.64±1.29                       & 96.33±0.12                      & 96.34±0.11                       & 81.48±2.24                      & 84.81±2.74                       & 92.33±2.28                      & 93.15±1.63    \\                   
\textbf{SGAE}     & 96.93±0.24                      & 96.98±0.11                       & 98.55±0.16                      & 98.27±0.2                        & 96.35±0.64                      & 95.91±0.54                       & 81.13±0.8                       & 85.68±1.22                       & 96.36±0.09                      & 96.37±0.06                       & 81.51±1.98                      & 84.16±2.2                        & 91.78±3.6                       & 92.11±3.26                       \\
\midrule
\textbf{DRES2}    & 97.2±0.26     & 97.27±0.15                       & 98.53±0.32                      & 98.4±0.32                       & 96.86±0.55                      & 96.60±0.38                       & 82.04±4.94                      & 86.2±4.54                       & 96.50±0.04                      & 96.60±0.03                       & 80.58±4.68                      & 85.60±2.93                       & 94.48±1.91                      & 95.05±1.96    \\                   
\textbf{NESS2}    & 97.48±0.28                      & 97.44±0.22                       & 99.32±0.11                      & 99.27±0.1                        & 98.15±0.3                       & 97.85±0.23                       & 91.24±3.14                      & 90.72±4.1                        & \textbf{96.52±0.16}             & \textbf{96.67±0.13}              & 91.54±1.0                       & 90.74±1.85                       & \textbf{97.06±1.53}             & \textbf{97.24±1.36}  \\            
\textbf{NESS4}    & 97.85±0.21                      & 97.76±0.19                       & \textbf{99.5±0.12}              & \textbf{99.43±0.12}              & \textbf{98.71±0.3}              & \textbf{98.46±0.45}              & 92.37±3.66                      & 92.52±3.34                       & 96.43±0.19                      & 96.6±0.12                        & 95.47±2.42                      & 94.41±3.36                       & 97.46±2.18                      & 97.13±2.54                       \\
\textbf{NESS8}    & \textbf{97.93±0.16}             & \textbf{97.78±0.08}              & 99.21±0.13                      & 99.11±0.14                       & 98.29±0.27                      & 97.89±0.43                       & \textbf{94.16±1.68}             & \textbf{93.0±2.05}               & 96.05±0.17                      & 96.28±0.13                       & \textbf{96.87±3.92}             & \textbf{96.06±5.18}              & 97.55±0.61                      & 96.55±1.06     \\
\bottomrule

\end{tabular}}{}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\textbf{The effect of aggregating latent variables in NESS}
In NESS, the joint node embeddings of the graph are obtained by aggregating latent variables from static subgraphs that are consistent throughout the training process. This approach is distinct from frameworks that utilize stochastic subgraphs, as their subgraphs are subject to variation in each iteration. Thus, in order to assess the efficacy of aggregating latent variables during test time, we measure the difference in performance between the use of aggregated node embeddings and direct encoding of the entire training graph (i.e. $AUC_{agg}-AUC_{direct}$). The results, displayed in Figure~\ref{fig:ness_ablation1}a, demonstrate a notable improvement in performance from the aggregation of subgraphs across various encoder types and benchmark datasets.


\textbf{Reconstructing subgraph versus large graph in NESS}
We compare performance of two training approaches: reconstructing subgraphs vs entire training graph. The results, displayed in Figure~\ref{fig:ness_ablation1}b, demonstrate the difference in performance ($AUC_{subgraph}-AUC_{full-graph}$) between the two approaches. We observe that reconstructing subgraphs often yields performance that is on par or superior to the alternative approach, potentially due to a reduction in overfitting.



\textbf{Applying contrastive learning in NESS}
We investigate the potential of contrastive learning to enhance performance in our setting. We present the results of this experiment in Figure~\ref{fig:ness_ablation1}c, which shows the difference in performance between using both reconstruction and contrastive loss and using only reconstruction (i.e. $AUC_{r+c}-AUC_{r}$). The results indicate that contrastive learning leads to a significant improvement in performance for GCN, GAT and ARGA models across all datasets, while GNAE and Lin models experience a slight to severe degradation in performance. We posit that the linearity of GNAE and Lin models and GNAE's use of $\mathcal{L}_2$-normalization prior to message passing, may be contributing factors to this degradation. Further research on these models under the contrastive learning setting is left as a future work.



\textbf{Comparing sampling strategies for NESS}
We use random edge splitting when partitioning the training graph into equally sized subgraphs, in a way that subgraphs do not share any edge and the sum of all edges across subgraphs is equal to the total number of edges in the entire training graph. However, we want to compare this strategy with other sampling approaches. Thus, we experiment with three other settings, in which we sample static subgraphs during data preprocessing by using: i) \textit{Random edge (RE):} Edges are sampled with the same uniform probability randomly \citep{krishnamurthy2005reducing};  ii) \textit{Random walk with jump (RWJ):} It is a random walk sampling, in which, with probability p=0.1, we randomly jump to any node in the graph \citep{leskovec2006sampling}; iii) \textit{Random node sampler (RN):} Nodes are sampled with uniform probability \citep{stumpf2005subnets}. In all three settings, we sample two subgraphs during data preprocessing. Sampled subgraphs might share some edges and are not guaranteed to cover the edges in the entire training graph i.e. we might be missing some edges from the training graph. Since we sample the subgraphs once during data preprocessing in all settings, we still use the aggregation of the latent variables of subgraphs at test time to get the joint embedding of the entire graph. Thus, they all have the same setting as the NESS, and differ only in how we generate static subgraphs. Figure~\ref{fig:ness_ablation1}d shows the difference in performance between NESS with $k=2$ (NESS2) and each of three setting (e.g., $AUC_{NESS}-AUC_{RE}$) for GNAE as the encoder. Although we see that our method of partitioning the graph gives a significant performance boost across all datasets in this case, the relative performance depends on the type of encoder and dataset used. We list the results for other encoder types in Figure~\ref{fig:appx_comparing_sampling_approaches} in the Appendix. 

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
     \begin{subfigure}[c]{0.4\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_ind8_gcn_gat_arga.png}
         \caption{}
     \end{subfigure}
     \begin{subfigure}[c]{0.55\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_agg8_gcn_gat_arga.png}
         \caption{}
     \end{subfigure}
\caption{\textbf{Evaluating the subgraphs of NESS8 for Cora dataset:} \textbf{a)} The test AUC using the node embeddings of each subgraph. \textbf{b)} The test AUC using the mean aggregation of the latent representations of subgraphs, starting with the first subgraph (i.e. $\bm Z=\bm z_1$ at $x=1$), and keep adding new subgraphs to get a new joint node embedding sequentially (e.g., $\pmb{Z}=agg(\pmb{z_1}, ..., \pmb{z_4})$ at $x=4$).}\label{fig:gradual_aggregation_performance}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Measuring the predictive quality of node embeddings from each subgraph} In the NESS8 setting, we examine the effectiveness of the node embeddings obtained from each of the eight subgraphs by utilizing them for link prediction in Figure~\ref{fig:gradual_aggregation_performance}a. Our analysis reveals that certain subgraphs possess more informative properties than others, yet the differences are not substantial. Additionally, we evaluate the quality of the joint embedding in a gradual manner in Figure~\ref{fig:gradual_aggregation_performance}b. As we progressively aggregate more latent variables of subgraphs to compute the joint embeddings, $\bm Z$, of the graph, we expect that $\bm Z$ become increasingly informative. To test this, we begin by including only the first latent variable, \{$\pmb{z}_1$\}, from the first subgraph and assign it as the embedding of the entire graph, $\pmb{Z}=\pmb{z}_1$. We then compute the AUC score for link prediction on the test set of Cora. Subsequently, we add $\pmb{z}_2$ to the set and aggregate \{$\pmb{z}_1$, $\pmb{z}_2$\} to compute $\pmb{Z}$ for evaluation. This process is repeated until we obtain $\bm Z$ as the aggregate of all eight latent variables \{$\pmb{z}_1$, $\pmb{z}_2$, ..., $\pmb{z}_8$\}. Our experiments demonstrate that as we aggregate more subgraphs, $\pmb{Z}$ becomes more expressive for the link prediction task, as depicted in Figure~\ref{fig:gradual_aggregation_performance}b, resulting in a significant performance boost. Furthermore, we observe that $\pmb{Z}$ obtained by aggregating only the first few subgraphs already attains a good performance at test time, suggesting that our approach would be effective in large graphs with many missing links. Please note that the results for GNAE and Lin are presented separately in Figure~\ref{fig:gradual_aggregation_performance2_appx} of the Appendix for improved visibility.



\section{Related Work}

The literature on learning node embeddings for link prediction is extensive. Therefore, we limit our review to the most relevant works that are based on GNN-based GAEs and that leverage subgraphs, various sampling strategies and constrastive learning during training. However, a brief review of other works based on matrix factorization and random walks is also included for completeness in the Appendix.


\textbf{Subgraphs and stochastic methods} Using subgraphs has been proposed to learn more expressive representations or to address scalibility issues with large graphs. For example, Sub2Vec \citep{adhikari2018sub2vec} proposes an unsupervised algorithm to learn feature representations of arbitrary subgraphs. There are also new proposals to improve the expressivity of  graph neural networks by introducing subgraph counting \citep{ bouritsas2021improving} and graph reconstruction theory \citep{cotta2021reconstruction}. And there are a number of proposals to address the scalibility issues associated with GNN-based encoders; \textbf{Cluster-GCN} \citep{chiang2019cluster} introduces a scalable GCN algorithm by designing batches based on a graph clustering algorithm such as METIS \citep{karypis1998fast} and extends GCN \citep{kipf2016semi} to inductive setting. The graph partitioning is done over the vertices in the graph in a way that the links within the clusters are much more than the ones between clusters since the error is proportional to the number of between-cluster links. During the training, the subgraph is sampled \textit{dynamically} from subset of clusters in each iteration. Cluster-GCN is an approximation to original GCN-algorithm, and can be used as an encoder in our framework. \textbf{GraphSaint} \citep{zeng2019graphsaint}, similarly to Cluster-GCN, starts each iteration with an independently sampled subgraph induced from  the nodes sampled by a pre-defined sampler. It then generates embeddings and compute loss for nodes in the subgraph. The authors use samplers such as random node sampler (RN), random edge sampler (RE), and random walk sampler (RWS). \textbf{GraphSage} \citep{hamilton2017inductive} extends GCNs into inductive setting while also proposing trainable aggregation functions. For a given node, it samples multiple neighborhoods with varying number of hops and search depth, and aggregates information from these neighborhoods using a set of aggregator functions. \textbf{FastGCN} \citep{chen2018fastgcn} addresses the scalibility issue of GCN by introducing importance sampling to sample the neighborhood of each node when aggregating their vector representations in forward pass. The most relevant work to ours is \textbf{FastGAE} \citep{salha2021fastgae} that proposes to use stochastic subgraphs for training GAEs in transductive learning setting. However, FastGAE encodes \textit{entire graph} as in standard GAE while using stochastic subgraphs when decoding. Subgraphs are sampled via node sampling with graph mining methods. The solution obtained at the end of FastGAE's optimization is only an approximation to that of standard GAE setting. Its main advantage is scalibility during training since it does not need to reconstruct the entire graph. 


\textbf{Contrastive learning} The most works applying constrastive learning assumes an inductive learning setting.  Common approaches to get the augmented view of a graph include adding or dropping nodes and edges randomly \citep{xu2021infogcl, you2020graph, zhu2020deep, papp2021dropgnn, rong2020dropedge}, shuffling nodes \citep{velickovic2019deep} and graph diffusion \cite{hassani2020contrastive}. 


\textbf{How NESS is different:} 
Our proposal stands out from other works in the literature by utilizing both \textit{static} subgraphs and contrastive learning in the context of transductive learning. Our main focus is on enhancing the predictive power of node embeddings for link prediction, instead of scalability which is a common focus among other works such as FastGAE, FastGCN, GraphSAINT, GraphSAGE, and Cluster-GCN \citep{salha2021fastgae, chen2018fastgcn, zeng2019graphsaint, hamilton2017inductive, chiang2019cluster}. We use \textit{static} subgraphs for both encoding and decoding, unlike FastGAE which uses the entire training graph for encoding and stochastic subgraphs for decoding. Additionally, we compute the joint embedding of the entire graph by applying a final aggregation function over the latent variables of subgraphs \textit{at test time}, which is different from methods that rely on stochastic subgraphs. We demonstrate the advantages of our approach in Section~\ref{results}. Additionally, methods such as FastGCN, GraphSAGE, and Cluster-GCN focus on scalability of GNN-based encoders, making them suitable to be used as encoders in the NESS framework. Other works that use contrastive learning often employ random augmentation techniques which can lead to unrealistic views of the graph \citep{lee2022augmentation}. In contrast, our framework generates two different views of the same node from different subgraphs, as shown in Figure~\ref{fig:subgraph_framework}; one view includes only the self-loop, while the other includes the neighborhood. This eliminates the need for additional augmentation. Furthermore, we conduct contrastive learning in a transductive learning setting, which sets our work apart from many others in the literature.



\section{Conclusion}
In this work, we introduce NESS, a novel GAE-based framework that utilizes \textit{static} subgraphs during training and aggregates their latent variables \textit{at test time} to learn node embeddings in a transductive setting. We also propose a novel contrastive learning approach in the same setting. NESS is shown to yield significant performance gains in link prediction tasks. Moreover, although we focus on feature-rich graphs in the experiments, NESS can also be applied to graphs without node features. We leave further research on scalability and extension to inductive setting as future work. Lastly, while we use an autoencoder setting in this work, it can easily be adapted to a variational autoencoder setting.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \appendix
% \section{Do \emph{not} have an appendix here}

% \textbf{\emph{Do not put content after the references.}}
% %
% Put anything that you might normally include after the references in a separate
% supplementary file.

% We recommend that you build supplementary material in a separate document.
% If you must create one PDF and cut it up, please be careful to use a tool that
% doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
% pdftk usually works fine. 

% \textbf{Please do not use Apple's preview to cut off supplementary material.} In
% previous years it has altered margins, and created headaches at the camera-ready
% stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix




\section{Details of Models}\label{details_of_models}

\textbf{GNAE:} We use a linear layers with hidden dimensions of 32 followed by $\mathcal{L}_2$ normalisation. We then generate final output by using approximate personalized propagation of neural predictions (APPNP) \citep{gasteiger2018predict} with $K=1$ and $\alpha=0$.


\textbf{GCN:} We use two convolutional layers with hidden dimensions of 64 and 32 respectively. We apply ReLU activation \citep{nair2010rectified} to the output of first layer.

\textbf{GAT:} The first layer consists of K = 8 attention heads with 8 features each, for a total of 64 features. We then apply an exponential linear unit (ELU) activation \citep{clevert2015fast}. The second layer has 32 units with K=1 attention head for all datasets, except Pubmed, for which we use K=8 as suggested in \citep{velivckovic2017graph}.

\textbf{Linear (Lin):} We use a single convolutional layer with 32 dimensions.

\textbf{ARGA:} We use the same architecture as GCN. We also use a discriminator network that has three linear layers with (64, 64, 1) dimensions. We apply ReLU activation to the first two layers of Discriminator.


\section{Related Works (Continued)}

\textbf{Matrix factorization} There are numerous methods to extract node embeddings of a graph in a low-dimensional vector that captures the local structure and features of the nodes. The methods based on matrix factorization performs dimensionality reduction using a matrix, which can be an adjacency matrix \citep{ahmed2013distributed}, transition probability matrix \citep{cao2015grarep}, or Katz index \citep{katz1953new}. 

\textbf{Random Walks} These methods encode node embeddings by leveraging the random walk probability to explore the graph structure.  There are many ways to design such random walks. For example, we can treat the graph as a collection of short random walks \citep{perozzi2014deepwalk}, or can adjust the random walk to be a hybrid of the breadth first search (BFS) and the depth first search (DFS) \citep{grover2016node2vec}. Combining the random walk data with other models such as skip-gram \citep{mikolov2013efficient}, we can learn useful node representations.



\section{Additional Results}
\begin{itemize}
    \item Figure~\ref{fig:gradual_aggregation_performance2_appx} gives an extended view of the results reported in Figure~\ref{fig:gradual_aggregation_performance} in the main paper.
    \item Figure~\ref{fig:appx_comparing_sampling_approaches} shows the results for other models for the experiment reported in Figure~\ref{fig:ness_ablation1}d, comparing NESS to other settings with different sampling approaches either during data pre-processing or during training.
    
\end{itemize}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
     \begin{subfigure}[c]{0.42\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_ind8_gcn_gat_arga.png}
     \end{subfigure}
     \begin{subfigure}[c]{0.56\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_agg8_gcn_gat_arga.png}
     \end{subfigure}
     \begin{subfigure}[c]{0.42\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_ind8_gnae.png}
     \end{subfigure}
     \begin{subfigure}[c]{0.56\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_agg8_gnae.png}
     \end{subfigure}
     \begin{subfigure}[c]{0.42\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_ind8_lin.png}
         \caption{}
     \end{subfigure}
     \begin{subfigure}[c]{0.56\columnwidth}
        \includegraphics[width=\columnwidth]{images/cora_ablation_agg8_lin.png}
         \caption{}
     \end{subfigure}
\caption{\textbf{Evaluating the subgraphs of NESS8 for Cora dataset:}
\textbf{Top:} GCN, GAT and ARGA \textbf{Middle:} GNAE, \textbf{Bottom:} Linear model (Lin).
\textbf{a)} The test AUC using the node embeddings of each subgraph. \textbf{b)} The test AUC using the mean aggregation of the latent representations of subgraphs, starting with the first subgraph (i.e.$x=1$ on the plot), and keep adding new subgraphs to get a new joint node embedding sequentially (i.e. $\pmb{Z}=agg(\pmb{z_1}, ..., \pmb{z_4})$ for $x=4$). }
\label{fig:gradual_aggregation_performance2_appx}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
     \begin{subfigure}[c]{0.22\textwidth}
         \includegraphics[width=\textwidth]{images/ness_samplingGAT.png}
         \caption{GAT}
     \end{subfigure}
     \begin{subfigure}[c]{0.22\textwidth}
         \includegraphics[width=\textwidth]{images/ness_samplingLin.png}
         \caption{Lin}
     \end{subfigure}
     \begin{subfigure}[c]{0.22\textwidth}
         \includegraphics[width=\textwidth]{images/ness_samplingGCN.png}
         \caption{GCN}
     \end{subfigure}
     \begin{subfigure}[c]{0.22\textwidth}
         \includegraphics[width=\textwidth]{images/ness_samplingARGA.png}
         \caption{ARGA}
     \end{subfigure}
     \begin{subfigure}[c]{0.1\textwidth}
         \includegraphics[width=\textwidth]{images/full2sub_legends.png}
     \end{subfigure}
\caption{Showing difference in AUC scores (\%) that we obtain using our default method, compared to other sampling approaches applied during data preprocessing. Our default NESS setting using random edge split to partition the graph usually outperforms others.}\label{fig:appx_comparing_sampling_approaches}
\end{center}
\vskip -0.2in
\end{figure*}



\section{Details of the Benchmark Datasets}

We list the details of three standard benchmark datasets from citation networks (Cora, Citeseer and Pubmed) \citep{sen2008collective}, three datasets (Cornell, Texas, Wisconsin) from WebKB \citep{pei2020geom},and Chameleon dataset \citep{rozemberczki2021multi} in Table~\ref{data_stats}.



\begin{table*}[hbt!]
\caption{Dataset statistics. Homophily Ratios are taken from \citep{ma2021homophily}.}
\label{data_stats}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1.0\textwidth}{!}{
{\begin{tabular}{llllllll}
\toprule
\textbf{}              & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{Chameleon} & \textbf{Cornell} & \textbf{Texas} & \textbf{Wisconsin} \\
\midrule
\textbf{Nodes}         & 2708          & 3,327             & 19717           & 2277               & 183              & 183            & 251                \\
\textbf{Edges}         & 5429          & 4732              & 44338           & 36101              & 295              & 309            & 499                \\
\textbf{Classes}       & 7             & 6                 & 3               & 5                  & 5                & 5              & 5                  \\
\textbf{Node Features} & 1433          & 3703              & 500             & 2325               & 1703             & 1703           & 1703               \\
\textbf{Homophily Ratio}           & 0.81         & 0.74             & 0.80            & 0.23              & 0.3            & 0.11          & 0.21     \\
\bottomrule
\end{tabular}}{}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
