\section{Introduction}

The analysis of aerial image and video~\cite{kumar2001aerial} plays a pivotal role in different applications, such as search and rescue, aerial photography, surveillance, movie production, etc. However, the paucity of aerial data~\cite{li2021uav} and the complexities associated with data capture from aerial cameras/ UAVs makes it difficult and costly to train large neural networks~\cite{ye2022unsupervised} for these applications. Hence, the synthesis of aerial-view images~\cite{barisic2022sim2air} offers a promising alternative to address these challenges. Conditional image synthesis~\cite{mirza2014conditional,nichol2021glide} allows for control over the generation process. 
% Image synthesis can be either conditional or unconditional. The 

Ground-view annotated datasets~\cite{deng2009imagenet,cordts2016cityscapes,carreira2017quo,monfort2019moments} are readily available for many tasks. Hence, a method that transforms ground-view images to aerial views (or 
cross-view synthesis~\cite{regmi2019cross,tang2019multi}) could allow the reuse the of annotated metadata for a variety of aerial-view applications, e.g., classification~\cite{he2020deep}, segmentation~\cite{hao2020brief}, action recognition~\cite{kothandaraman2022far}, representation learning~\cite{sun2019videobert}, domain adaptation~\cite{choi2020unsupervised}, augmentation~\cite{jaipuria2020deflating}, etc. However, cross-view synthesis requires the network to learn a very large non-trivial translation. The network needs to hallucinate a new and enormously different view of all entities in the scene and the background, while being consistent with the details including the semantics, colors, relations between various parts of the scene, pose, etc. 

Prior work~\cite{regmi2019cross,tang2019multi,ding2020cross,liu2021cross} on ground-to-aerial generation use NeRFs and GANs. However, all of these methods use paired data for ground-view and the corresponding aerial views, which is seldom available. Moreover, training on a specific dataset limits the application to specific scenes similar to the training data; necessitating the collection of paired data for widely different distributions. Instead, our goal is to develop a generic method for generating aerial views from ground-views without any paired data or other auxiliary information such as multi-views, depth, 3D mapping, etc. 

While there are many diverse datasets of ground images, there are not many such good quality aerial datasets~\cite{li2021uav} - hence, unpaired image-to-image translation~\cite{zhu2017unpaired} is not a viable solution. On the contrary, text is an auxiliary modality that can be easily obtained using off-the-shelf image/video captioning tools~\cite{hossain2019comprehensive} %and classification labels~\cite{deng2009imagenet,carreira2017quo} corresponding to ground-view datasets. 
Moreover, text provides a natural representation space describing images. Consequently, our goal is to use the text description of a ground-view image to generate its corresponding aerial view. 

Recently, diffusion models have emerged as state-of-the-art architectures for text-to-image~\cite{kawar2022imagic,hertz2022prompt,zhang2022sine,nichol2021glide} high-quality realistic image synthesis. The availability of immense prior knowledge via large-scale robust pretrained text-to-image models~\cite{rombach2022high}, motivates us to pose ground-to-aerial view translation as text-guided single-image translation~\cite{kawar2022imagic,zhang2022sine}. Text-guided single-image translation methods finetune the diffusion model to the input image and then perform linear interpolation in the text embedding space to generate the desired output. However, direct application of these methods~\cite{nichol2021glide,zhang2022sine,kawar2022imagic} to ground-to-aerial translation either generates high-fidelity non-aerial images or low-fidelity aerial images. \looseness-1

%\begin{figure*}
%    \centering
%    \vspace*{-1em}
%    \includegraphics[scale=0.2]{Figures/results1.png}
%    \vspace*{-1em}
%    \caption{We apply Aerial Diffusion on diverse images such as animals/ birds, natural scenes, human actions, indoor settings, etc and show that our method is able to generate high-quality high-fidelity aerial images.}
%    \label{fig:results1}
%   \vspace{-5pt}
%\end{figure*}

\textit{\textbf{Main contributions.}}
We present two postulates for text-guided image translation, for ground-to-aerial translation: (i) domain gap between the finetuning task (w.r.t. ground view) and target task (aerial view generation) hinders the diffusion model from generating accurate target views and introduces bias towards the source view, (ii) a finetuned diffusion model cannot generalize well to the target prompt, if the text embedding and image spaces corresponding to the source and the target are far apart on the nonlinear text-image manifold.  

Based on these findings, we propose ``{\em Aerial Diffusion}'', a simple, yet effective, method for generating aerial views, given a single ground-view image and the corresponding text description as input. The novel elements of our algorithm include:
\begin{itemize}[nosep]
    \item Instead of directly finetuning the diffusion model with the ground-view image,  we apply a {\em homography based on inverse perspective mapping} on the ground-view image to obtain a homography projected image prior to the finetuning. This reduces the bias of the diffusion model towards the input image while finetuning.
    \item To finetune the diffusion model, we use the {\em source text} corresponding to the `ground-view' as the {\em guiding factor}, instead of the target text (`aerial view'). This helps the diffusion model search for an optimized text embedding in the vicinity of a text space close to the image space, enabling the learning of a `good' optimized text embedding. This also prevents the diffusion model from developing a bias towards an incorrect aerial view. \looseness-1
    \item To obtain a high-fidelity aerial image (w.r.t. ground-view), at inference time, we manipulate the text embedding layer, such that it prioritizes fidelity and the aerial viewpoint in an alternating manner. {\em Alternating between text embeddings corresponding to the viewpoint and fidelity switches the denoising direction}, such that the backward diffusion takes one step towards preserving fidelity followed by another step towards generating an aerial view. As noises are gradually removed, the process ends up with a high-fidelity aerial-view image on a manifold with a better fidelity-viewpoint trade-off than linear interpolation. %Hence, our mechanism allows the diffusion model to search for the optimal solution in a much higher dimensional space than being restricted to the linear interpolation space.
\end{itemize} 
We apply our method on numerous in-the-wild images from various domains such as nature, animals and birds, human actions, indoor objects, etc. Our method is able to generate high-quality aerial view images that preserve the details contained in the source-view image(Fig.~\ref{fig:results1} and Fig.~\ref{fig:teaser}). We conduct extensive ablation studies (Fig.~\ref{fig:results3}) highlighting the benefits of each element of our method; and demonstrate the trade-off between fidelity (w.r.t. source image) and faithfulness to target view via hyperparameter tuning(Fig.~\ref{fig:results2}). We compare with the state-of-the-art diffusion model-based text-guided editing approach~\cite{kawar2022imagic} and show far superior qualitative performance for ground-to-aerial translation (Fig.~\ref{fig:results3}). Comparison to other text-embedding manipulation approaches (Fig.~\ref{fig:results3}) also shows that our alternate prompting strategy works better. 
