\section{Conclusion, Limitations and Future Work}

In this paper, we introduce a novel method, Aerial Diffusion, for generating aerial views from a single ground-view image using text as a guiding factor. We use homography as guidance and a diffusion model on the generated image with an alternating denoising direction based on switching between viewpoint text embedding and fidelity of the generated images. Our method has some limitations. Our method has some limitations: (i) the homography transformation results in a directional (diagonal) bias in the generated aerial image in many cases; (ii) it is limited to the knowledge contained in the pretrained stable diffusion model and is unable to hallucinate scenes~\cite{somepalli2022diffusion} outside of this domain; (iii) the value of $\alpha$ in sampling needs to be manually tuned; (iv) the problem domain of unpaired ground-to-aerial does not have concrete quantitative analysis metrics. Future work can focus on the development of methods that overcome these limitations. Other directions include extending Aerial Diffusion to complex scenes with multiple objects (and an intricate background), generating higher-fidelity images, extending the method to videos, using the synthetic aerial data for aerial video analysis, detection, and recognition tasks. 

\noindent \textbf{Acknowledgements:} This research has been supported by ARO Grants W911NF2110026 and Army Cooperative Agreement W911NF2120076   
