\begin{figure}
    \centering
    \includegraphics[scale=0.25]{Figures/applications1.png}
    \vspace*{-2em}
    \caption{Based on the text description, Aerial Diffusion can generate aerial views with scene entities slightly different from the ground-view. The hallucination of background and unseen parts of the scene can also be controlled by text.}
    \label{fig:applications1}
    \vspace{-15pt}
\end{figure}

\section{Experiments and Results}

\subsection{Implementation details}

We use the stable diffusion~\cite{rombach2022high} text-to-image model as the backbone architecture. It has been pretrained on the massive text-image LAION-5B dataset (laion2B-en, laion-high-resolution, laion-improved-aesthetics). 

Prior to the homography transformation, we resize all images to a resolution of $256 \times 256$. We use OpenCV to apply the homography transformation on the image to generate a homography transformed image of resolution $512 \times 512$, which is used to optimize the text embedding and the diffusion model in the next step. We finetune the text embedding for $500$ iterations with a learning rate of $1e-3$ using the Adam optimizer, and the diffusion model for $1000$ iterations at a learning rate of $2e-6$. For each image, this entire optimization takes between $8$ and $9$ minutes on one NVIDIA RTX A5000 GPU with 24GB memory.

While sampling/inferencing using the finetuned diffusion model, we iteratively refine the image, starting with random noise, for $T=50$ iterations. At every iteration, the diffusion model is applied on the refined image from the previous iteration, as per the standard procedure~\cite{ho2020denoising} in sampling from diffusion models. The text embedding condition, while sampling at each iteration, alternates between $e_{1}$ and $e_{2}$, starting from $e_{1}$. The guidance scale is set to $7.5$.

\begin{figure*}
    \vspace*{-1em}
    \centering
    \includegraphics[scale=0.4]{Figures/results3.png}
    \vspace*{-1.5em}
    \caption{\textbf{Ablations and Comparisons.} Ablations: we prove the effectiveness of the homography, our alternating strategy over linear interpolation and finetuning with $e_{src}$ instead of $e_{tgt}$. SOTA Comparisons: IMAGIC ~\cite{kawar2022imagic} (arXiv Nov 2022) is unable to generate aerial views due to high bias towards the input image, domain gap and restricting the solution search to the linear interpolation manifold. Comparisons with other embedding space manipulation strategies that utilize both $e_{1}$ and $e_{2}$ reveal that our Alternating strategy is better.}
    \vspace{-10pt}
    \label{fig:results3}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{Figures/fail1.png}
    \vspace*{-1.5em}
    \caption{\textbf{Failure cases.} A. The identity of the dog and cat is interchanged. B. The fidelity of the background (bed) is lost.}
    \label{fig:fail1}
    \vspace{-15pt}
\end{figure}

\subsection{Qualitative evaluation}

We apply our method on a number of real images from various domains including nature, human actions, buildings, etc. We collect most of these images from Google Images/ Flickr. We show the results in Figure~\ref{fig:teaser} and Figure~\ref{fig:results1}. Aerial images can be captured from varying heights and angles (including side/oblique views). We do not constrain the network on the height/ angle, hence, the diffusion model generates an aerial image of arbitrary height/ angle dictated by the random noise. For each image, we choose the best result corresponding to $0 < \alpha < 1$. Aerial Diffusion is successful in generating the aerial view, given its ground-view. It is able to hallucinate the aerial view of the entities in the scene (encompassing unseen aspects) as well as the background; while being faithful to the details in the ground-view. Since the underlying diffusion model is probabilistic, we get different results for different random seeds, all the generated images are faithful to the details in the ground-view as well as conform to the viewpoint being aerial. This diversity allows users to choose among a variety of options and is also a useful property in curating synthetic datasets.  

\noindent \textbf{More applications.} We show in Figure~\ref{fig:applications1} that our method is capable of generating aerial views consistent with the text description, even when the text dictates an aerial view with scene entities slightly different from the ground-view. Moreover, background hallucination can be controlled by the text description as well. 

\noindent \textbf{Effect of $\alpha$.} We show the effect of $alpha$ in Figure~\ref{fig:results2}. The finetuned diffusion model has high bias towards the ground-view image. Hence, the value of $\alpha$ needs to be carefully tuned in order to generate a high fidelity aerial image. A low value of $\alpha$ implies higher weight to the optimized text embedding and low weight to `aerial view'. This leads to the generated image being very similar to the homography image, the viewpoint of the generated image is less aerial. A high value of $\alpha$ implies higher weight to `aerial view'. The generated image is an aerial image, though with low fidelity. While the details in the generated aerial image are not completely different from the details in the ground-view image, due to the bias of the finetuned diffusion model, the fidelity or conformance to details contained in the ground-view image is still low. A good trade-off between fidelity w.r.t. ground-view image and `aerial view' is achieved at mid-values of $\alpha$. The change from ground-view to aerial-view as $\alpha$ varies from $0$ to $1$ is not gradual, reinforcing Postulate 2. 

\subsection{Ablations and comparisons}

\noindent \textbf{Ablating the model.} We show ablation experiments in Figure~\ref{fig:results3}. Ablation 1 uses the ground-view image instead of the homography transformed image to finetune the diffusion model. It proves that the homography transformation is successful in significantly lowering the bias of the model and helps generate aerial views, reinforcing our solution to Postulate 1. Ablation 2 uses linear interpolation for sampling instead of our alternating strategy. Results with the alternating strategy are better (high fidelity images with faithfulness to `aerial view') than the results with linear interpolation, justifying our solution to Postulate 2. Ablation 3 finds $e_{opt}$ in the vicinity of $e_{tgt}$ instead of $e_{src}$. Generated images are more aerial when $e_{opt}$ is optimized in the vicinity of $e_{src}$, proving that it helps in reducing the bias. 

\noindent \textbf{SOTA Comparisons.} We compare with IMAGIC~\cite{kawar2022imagic} (CVPR 2023), the SOTA text-based single image translation method, in Figure~\ref{fig:results3}. IMAGIC has shown promising results in executing a wide variety of changes including non-rigid changes and has shown superior results over other methods such as DreamBooth~\cite{ruiz2022dreambooth}, SINE~\cite{zhang2022sine} etc. Our method is far superior than IMAGIC for ground-to-aerial translation. IMAGIC is unable to perform ground-to-aerial translation due the high bias incurred while finetuning and searching for the solution in a limited linear interpolation space. 

\noindent \textbf{Comparisons with other text embedding manipulation methods.} We compare with other strategies to manipulate the text embedding space using $e_{1}$ and $e_{2}$ in Figure~\ref{fig:results3}. In manip1, we condition on $e_{2}$ and $e_{1}$ alternatingly, starting from $e_{1}$. Clearly, it is more beneficial to start sampling from $e_{1}$ as explained in Section~\ref{sec:method}. In manip2, we use just $e_{1}$ (text embedding corresponding to aerial view) to sample and rely on the bias of the network to generate the aerial image. Our alternating sampling method is able to generate higher fidelity aerial images. In manip3, for the first $T/2$ iterations, we sample using $e_{1}$ and for the second $T/2$ iterations, we sample using $e_{2}$. In manip4, for the first $T/2$ iterations, we sample using $e_{2}$ and for the second $T/2$ iterations, we sample using $e_{1}$. These experiments prove that our alternating sampling strategy works best. 

\noindent \textbf{Quantitative evaluation.} Text guided single image ground-to-aerial translation is a recent development, and Aerial Diffusion is the first solution towards this goal. As such, no standard benchmark (and ground-truth) or quantitative metrics exist for evaluation. We evaluate Aerial Diffusion via human perceptual evaluation, Aerial Diffusion is able to generate high fidelity aerial images. 

Our method uses prior knowledge (from robust pretraining) along with the knowledge gained while finetuning to generate aerial images; and \textit{hallucinates} large parts and views of the scene that it has not encountered before. Hence, comparisons against other ground-to-aerial methods~\cite{regmi2019cross,toker2021coming,ding2020cross,shen2021cross}, that learn a specific data distribution by training on (an entire dataset with) paired data (and auxiliary information such as depth, semantic maps) is not relevant to this paper. Moreover, computing the similarity (LPIPS) vs CLIP trade-off~\cite{nichol2021glide,zhang2022sine,hertz2022prompt,kawar2022imagic} is futile since the aerial view is much
different from the ground view and similarity scores compute patch-wise image similarity.

\noindent \textbf{More results.} Please refer to the supplementary material. 