\section{Related work}

%Cover image translation, cross view image translation, NeRFs, depth, 3D recon, diffusion models and text based editing, novel view generation

There has been immense work on \textit{image-to-image translation}~\cite{isola2017image,liu2017unsupervised,lin2018conditional,pang2021image,zhu2017unpaired,wang2018video,aldausari2022video} using GANs~\cite{shamsolmoali2021image}, transformers~\cite{esser2021taming}, diffusion models~\cite{dhariwal2021diffusion,rombach2022high,croitoru2022diffusion}, etc. for problems, such as style transfer~\cite{jing2019neural}, image restoration~\cite{su2022survey}, and multimodal style translation~\cite{huang2018multimodal}. Many of these methods are capable of performing these tasks using paired/ unpaired data~\cite{alotaibi2020deep}. Recently, diffusion models~\cite{nichol2021glide,wang2022pretraining,su2022dual,sasaki2021unit,saharia2022palette,saharia2022image,yang2022paint,preechakul2022diffusion} have been successful in performing non-trivial operations, such as posture changes and multiple objects editing. Prior work on \textit{cross-view synthesis}~\cite{regmi2019cross,tang2019multi,ren2022pi,toker2021coming,ding2020cross,ma2022vision,liu2021cross,shi2022geometry,liu2020exocentric,ren2021cascaded,liu2022parallel,wu2022cross,shen2021cross,ammar2019geometric,zhao2022scene} generally use paired data and other complex auxiliary modality such as semantic maps, depth, multi-views, etc within various generative approaches. 

A closely related problem is \textit{novel view synthesis}~\cite{levoy1996light,more2021deep} where the goal is to generate new views of the scene. However, most novel-view synthesis methods including GANs~\cite{xu2019view}, NeRFs~\cite{gao2022nerf}, diffusion models~\cite{watson2022novel} use multiple views of the scene for training, even while they may be capable of performing single-view evaluation~\cite{wiles2020synsin,tucker2020single}. Again, this is prohibitive since it requires multiple views of the scene/ depth information~\cite{hou2021novel} for training. On the other hand, 3D reconstruction methods~\cite{han2019image,yuniarti2019review,fahim2021single} rely on depth information or auxiliary data such as shape priors. Moreover, 3D reconstruction is a complex and expensive task, which is redundant when the goal is to just obtain a 2D aerial view of the scene. 

Text, which is easily available, has been widely used as a guiding factor for image translation~\cite{liu2020describe,li2020image} and image editing~\cite{kawar2022imagic,hertz2022prompt,brooks2022instructpix2pix,kim2022diffusionclip,zhang2022sine,zhuang2021enjoy,hu2021lora,gal2022image,crowson2022vqgan}, particularly in the context of diffusion models recently. The availability of large databases of image-text pairs~\cite{schuhmann2022laion}, open-source pretrained models~\cite{radford2021learning}, and the fact that text is a natural representation of the world makes it conducive to use text as an auxiliary modality to provide guidance. Many \textit{text-based image editing} approaches operate on a single real image~\cite{valevski2022unitune,kawar2022imagic} and perform inference time optimization, making them easy to generalize across diverse images. \textit{Single-image approaches}~\cite{yoo2021sinir,vinker2020deep,ruiz2022dreambooth} have been proposed for image manipulation tasks without text-guidance as well. Generally, they aim to learn useful representations by finetuning a pretrained model on a single image for reconstruction. The inference then controls the feature space~\cite{shen2020interpreting,patashnik2021styleclip} to achieve the desired changes.