\section{Aerial Diffusion}

We present Aerial Diffusion for view translation from a single real ground-view source image $I_{S}$ to its aerial-view (or target image $I_{T}$), given a text description txt of the image. The text description can be obtained using an off-the-shelf image captioning tool~\cite{hossain2019comprehensive}. We assume no access to any paired data or other modalities such as depth, semantic maps, other views of the scene, etc. Corresponding to the ground-view, we use the source text description txt$_{G}$ = `front view of' + txt with text embedding $e_{src}$. Similarly, for the aerial-view, we use the target text description txt$_{A}$ = `aerial view of' + txt with text embedding $e_{tgt}$. %We provide an overview of the preliminaries in Section~\ref{sec:prelim}. 
In Section~\ref{sec:theory}, we present two postulates that lead to the method described in Section~\ref{sec:method}. 

%\subsection{Preliminaries}
%\label{sec:prelim}

%Diffusion models~\cite{ho2020denoising} are extremely powerful generative models capable of generating high quality images. The fundamental idea behind diffusion models is that sequential addition of small amounts of random Gaussian noise will eventually transform a photorealistic image $x_{0}$ to noise $x_{T} \sim \mathcal{N}(0,I)$ in $T$ steps. This is termed the forward process. The converse theorem states that if we start from random noise $x_{T} \sim \mathcal{N}(0,I)$ and iteratively refine it in a controlled fashion for $T$ steps, we will be able to generate a photorealistic image $x_{0}$. The value of $T$ is generally large since diffusion is a slow process. At each intermediate timestep $t \in \{0....T\}$, $x_{t}$ satisfies $x_{t} = \sqrt{\alpha _{t}}x_{0} + \sqrt{1 - \alpha _{t}}\epsilon_{t}$. $0=\alpha _{T} < \alpha _{T-1}....< \alpha _{1} < \alpha _{0} = 1$ are hyperparameters of the diffusion schedule and $\epsilon_{T} \sim \mathcal{N}(0,I)$. To obtain $x_{t-1}$ at each refinement step, the neural network $f_{\theta}(x_{t},t)$ is applied along with the corresponding random Gaussian noise perturbation. The neural network is trained using a simple denoising~\cite{ho2020denoising} objective to predict the noise, given a noisy image. 

%Diffusion models can be generalized to generate images conditioned on an input $y$, enabling the network $f_{\theta}(x_{t},t,y)$ to generate images that are in accordance with the condition. The condition can be in the form of text, image, audio or multimodal. Conditioning, along with training and sampling stategies enable the successful application of diffusion models to various downstream tasks. 

%A common strategy adopted for single image text-based editing is to use a robust text-to-image pretrained model in two-stage process. In these methods, the first step, called `finetuning', is to finetune the neural network to simultaneously generate the `source' image $I_{S}$ as well as find the `optimized text embedding' $e_{opt}$ (in the vicinity of $e_{tgt}$) that best generates the `source' image $I_{S}$. In the second step, called `inferencing', a linear interpolation of $e_{tgt}$ and $e_{opt}$ are used to generate the edited image $I_{T}$ from the finetuned neural network, i.e., the backward diffusion process is
%\begin{equation}\label{equ:interpolation}
%    x_{t-1}=x_t - f(x_t, t, \alpha e_{tgt}+(1-\alpha) e_{opt}),~t=T,\cdots, 0.
%\end{equation}

\begin{figure*}
\vspace*{-1em}
    \centering
    \includegraphics[scale=0.19]{Figures/overview.png}
    \vspace*{-0.5em}
    \caption{\textbf{Aerial Diffusion.} In step 1, we apply a homography transformation to the ground-view image $I_{S}$. This creates a gap between $I_{Sh}$ and txt, which in turn reduces the bias of the model towards $I_{Sh}$ in step 2. In step 2, we use the $e_{src}$ to optimize to $e_{opt}$ and finetune the diffusion model to reconstruct $I_{Sh}$ for the $e_{opt}$. Using the $e_{src}$ to find $e_{opt}$ reduces the bias of the model towards $I_{Sh}$ due to the disparity between $I_{Sh}$ and the $e_{src}$. In stage 3, we manipulate the text embedding by using an alternating strategy to find the optimal solution in a higher-dimensional non-linear space to generate a high-fidelity aerial image $I_{T}$.
    }
    \label{fig:overview}
    \vspace{-5pt}
\end{figure*}
%\begin{figure}
%    \centering
%    \includegraphics[scale=0.3]{Figures/cuboid.png}
%    \caption{\textbf{We apply a homography transformation to the ground-view image in order to reduce the inductive bias while finetuning the diffusion model.} In Figure A, we show that the bottom-face of a 2D cube can be regarded as the projection of the front-face of the cube, when viewed using a 2D camera. Figure B describes the coordinates of the ground-view image and projected image used to compute the homography.}
%    \label{fig:cuboid}
%\end{figure}

\subsection{Postulates}
\label{sec:theory}

In this section, we analyze text-guided single image translation in the context of ground-to-aerial view synthesis and present two postulates. A common strategy adopted for text-based single image translation is to use a robust text-to-image pretrained model in a two-stage process. The first step finds the `optimized text embedding' $e_{opt}$ (in the vicinity of $e_{tgt}$) that best generates the `source' image $I_{S}$ and subsequently finetune the diffusion model to generate the `source' image $I_{S}$ using $e_{opt}$. In the second step, a linear interpolation of $e_{tgt}$ and $e_{opt}$ are used to generate the edited image $I_{T}$ from the finetuned neural network, i.e., the backward diffusion process is
\begin{equation}\label{equ:interpolation}
\resizebox{0.9\hsize}{!}{$
    x_{t-1}=x_t - f(x_t, t, \alpha e_{tgt}+(1-\alpha) e_{opt}),~t=T,\cdots, 0. $}
\end{equation}

A text-based single image translation approach~\cite{kawar2022imagic,hertz2022prompt,brooks2022instructpix2pix,kim2022diffusionclip} for ground-to-aerial generation overcomes multiple limitations in terms of data availability and generalization. However, the challenges involved in ground-to-aerial translation inhibit the direct application of existing text-based single image translation methods for ground-to-aerial generation. We present two postulates for text-based single-image translation in the context of ground-to-aerial generation.

\begin{prop}
Domain gap between the finetuning task (e.g., ground view generation) and target task (aerial view generation ) hinders the diffusion model from generating accurate target views and introduces bias towards the source view.  
\end{prop}

Diffusion models are probabilistic models. They are trained~\cite{ho2020denoising} by optimizing the negative log-likelihood of the model distribution under the expectation of the data distribution. Further simplification of the equation for formulating the training loss function involves variance reduction. In the first step of finetuning, the diffusion model is being trained to reproduce the source image given the optimized text embedding, irrespective of the input random noise. Hence, it has a natural bias towards the source image. 

When the image space corresponding to the target text embedding is in the vicinity of the image space corresponding to the optimized text embedding, consistent with the variance within which the neural network was trained to generate, the generated target image is a high fidelity image consistent with the target text. When the desired transformation is large (ground-to-aerial), outside the limits of the variance, the diffusion model is unable to generate an aerial image. 

\begin{prop}
A finetuned diffusion model cannot generalize well to the target prompt if the text embedding and image spaces corresponding to the source and the target are very different and far away from each other on the nonlinear text-image embedding manifold. 
\label{sec:prop2}
\end{prop}

The embedding space and the corresponding image representation space are locally linear. Hence, when the target text embedding dictates a relatively small change to the source image, a linear combination between the optimized text embedding and the target text embedding generates a high-fidelity target image, faithful to the target text. 
% The sentences below are redundant.  Basically, when you have target and source images that involve large perspective changes (i.e. use of large rotation of camera poses), then the image representation space is no longer "locally linear".
% Moreover, since the representation spaces are locally linear, when we start moving in the direction of the target embedding from the optimized text embedding, the generated image gradually changes from the source image to a high-fidelity target image. Naturally, after a point, the fidelity of the generated image decreases. 
In contrast, when a linear interpolation of the text prompts in Eq.~\eqref{equ:interpolation} is applied 
% between the text embeddings corresponding 
to ground-to-aerial translation, depending on $\alpha$, the images generated are either high fidelity (but low target text faithfulness) or high target text faithfulness (but low fidelity). Moreover, the ground-view image doesn't gradually change to an oblique-view image followed by aerial-view image, the manifold is not smooth. Rather, the change is quite drastic and it is difficult to find an optimal solution in the linear interpolation space. 
Essentially, when there is a large perspective changes from the source to target images (i.e. involving large rotation of camera poses), the image representation space is no longer
``locally linear'', thereby linear interpolation is no longer adequate to generate high-fidelity images.
%\begin{figure}
%    \centering
%    \includegraphics[scale=0.28]{Figures/altsampling.png}
%    \caption{Linear interpolation~\cite{kawar2022imagic,zhang2022sine} between $e_{opt}$ and $e_{tgt}$ is unable to find a good solution because the representation spaces are not globally linear, while desired transformation for ground-to-aerial is large. We propose to use alternating text embeddings while sampling to find the optimal solution in a higher dimensional non-linear space. The embeddings shown in this 2D figure are for representation purposes only, the actual embeddings are in a high dimensional space.}
%    \label{fig:altsampling}
%\end{figure}

\subsection{Method}
\label{sec:method}

Motivated by the challenges described above, we propose Aerial Diffusion for text guided single-image ground-to-aerial translation. An overview of our solution is as follows. We start with a pretrained robust stable diffusion~\cite{rombach2022high} model as the backbone. Our method has three stages. In the first step, we preprocess the ground-view image $I_{S}$ with a carefully crafted homography transformation to generate $I_{Sh}$. This reduces the bias in the finetuning step. In the second step, we finetune the diffusion model by first optimizing the text-embedding within the vicinity of $e_{src}$ to find $e_{opt}$ that best generates $I_{Sh}$. Subsequently, we finetune the diffusion model to reconstruct $I_{Sh}$, given $e_{opt}$. In the third step on inferencing/sampling, we use an alternating strategy to manipulate the text embedding layer to generate a high-fidelity aerial image $I_{T}$. We now describe each step in detail. 

\vspace{-1em}
\paragraph{Step 1: Preprocessing using a homography transformation.} The bias acquired by the diffusion model during the second step of finetuning inhibits large transformations. One way to decrease the bias is to reduce the number of iterations while finetuning. However, this leads to unsurprisingly low quality generated images. To decrease the bias while finetuning, we preprocess the ground-view image by transforming it with a 2D {\em homography transformation}~\cite{szeliski2022computer}  (inverse perspective mapping). This homography projects the ground-view image to its rough 2D projected aerial view. Note that we are unable to use a 3D homography mapping to obtain the 3D aerial view projection, a better pseudo estimate of the aerial view, due to the unavailability of camera matrix, multi-views, depth information, etc. 

Consider a 3D cube (Figure~\ref{fig:overview}). Without loss of generality, the 2D image captured by a ground-camera can be regarded as the projection of the scene in the front-face of the cube. A camera facing the top face of the cube will be able to capture the accurate 2D aerial view of the scene. Since we have no knowledge of the camera parameters corresponding to the ground-view image, we are unable to shift the camera to obtain a different view of the scene. With respect to the ground-camera, the 2D projection of the front-face of the cube on the bottom face of the cube is the best `aerial projection' that we can get (inverse perspective mapping~\cite{szeliski2022computer}). This aerial projection is nowhere close to the true aerial view and does not resemble the ground-view either. Hence, when the diffusion model is finetuned, the bias is much lower than what it would have been if the optimization/finetuning were done directly with the ground-view image. This is because of the disparities between the image space of $I_{Sh}$ and $e_{src}$/ $e_{tgt}$, ingrained in the pretrained network. 

We maintain the horizontal and vertical distance between the edges of the faces in the ground view and its projected aerial view, to better preserve the resolution and the aspect ratio. Formally, the coordinates (in order) of the corners of the ground-view image and the homography projected image are $\{(0,0), (H,0), (H,W), (0,W)\}$ and $\{(0,W), (H,0), (H,W), (0,2W)\}$ respectively (Figure~\ref{fig:overview}). The homography can then be computed and applied. 

%\ty{Is it possible to have an equation here describing how to achieve the projected image from the ground view image? Divya : OpenCV computes the projection as the solution to a few equations, using SVD etc, not a straightforward equation}

\paragraph{Step 2: Finetuning the diffusion model.} We first optimize the text-embedding~\cite{kawar2022imagic,zhang2022sine} to generate $I_{Sh}$ and subsequently finetune the diffusion model using $e_{opt}$ to generate $I_{Sh}$. In contrast to popular text-based image editing approaches that find the optimized text embedding $e_{opt}$ in the vicinity of the target text embedding $e_{tgt}$, we find $e_{opt}$ in the vicinity of the source text embedding $e_{src}$. This is due to two reasons: (i) the disparity between the homography transformed view and the target text is still large (though much smaller than the disparity between the ground-view and target text). Hence, it is unlikely that a good $e_{opt}$ will be obtained when the optimization is run (around $e_{tgt}$) for a limited number of iterations. (ii) we do not want the network to develop a bias towards the homography image as the `aerial view'. 

To find $e_{opt}$, we freeze the parameters of the generative diffusion model $f_{\theta}$ and optimize $e_{src}$ using the denoising diffusion objective~\cite{ho2020denoising}. This optimization is run for a small number of iterations, in order to remain close to $e_{src}$ for meaningful embedding space manipulation at inferencing. 
\begin{equation}
    \min_{e_{opt}} \sum_{t=T}^0 L(f(x_t, t, e_{opt};\theta), I_{Sh}),
\end{equation}
\abovedisplayskip=-2pt
where $f(x,t,y)$ is the $t$-th backward diffusion step, $\theta$ denotes the U-net parameters, and $L$ is the denoising diffusion objective. To enable $e_{opt}$ reconstruct the $I_{Sh}$ with high fidelity, we finetune the diffusion model, again using the denoising diffusion objective~\cite{ho2020denoising,saharia2022image,kawar2022imagic}:
\begin{equation}
    \min_{e_{theta}} \sum_{t=T}^0 L(f(x_t, t, e_{opt};\theta), I_{Sh}).
\end{equation}

\begin{figure*}
\vspace*{-1em}
    \centering
    \includegraphics[scale=0.48]{Figures/results2.png}
    \vspace*{-0.5em}
    \caption{\textbf{Effect of $\alpha$.} Low values of $\alpha$ generate images that are less aerial, high values of $\alpha$ generate low-fidelity images. A trade-off between the viewpoint and fidelity generates high-fidelity aerial images. The transformation, with $\alpha$, is not smooth, reinforcing Postulate 2.}
    \label{fig:results2}
    \vspace{-10pt}
\end{figure*}

\paragraph{Step 3: Inferencing/ sampling by text embedding manipulation.} Our next step is to use the finetuned diffusion model to generate a high-fidelity aerial image. Prior work~\cite{kawar2022imagic,zhang2022sine} use linear interpolation between the optimized text embedding $e_{opt}$ and the target text embedding $e_{tgt}$. As described in Section~\ref{sec:theory}, linear interpolation is not the best solution for large transformations such as ground-to-aerial generation and is unable to generate high-fidelity aerial images. 

Sampling from stable diffusion~\cite{rombach2022high} involves iteratively denoising the image for $T$ steps conditioned by text, starting with random noise. To deal with the aforementioned issues, we propose to alternate between two text embeddings $e_{1}$ and $e_{2}$, starting with $e_{1}$. We designate $e_{1}$ as the target text embedding $e_{tgt}$. This imposes a strong constraint on the diffusion model to generate an aerial view image corresponding to the text description. The bias of the diffusion neural network motivates the network to generate an image whose details are close to the ground-view image. However, merely relying on the bias of the neural network to capture all details of the scene is severely insufficient. Hence, we designate $e_{2}$ to be the linear interpolation of $e_{opt}$ and $e_{tgt}$, controlled by the hyperparameter $\alpha$. The linear interpolation can be mathematically represented as $e_{2} = \alpha * e_{tgt} + (1-\alpha) * e_{opt}$. $e_{2}$ enables the network to generate a high fidelity image while retaining the aerial viewpoint. For very low values of $\alpha$, the generated image is less aerial, despite reinforcing the viewpoint to be aerial by applying $e_{1}$ alternatingly. This is because of the bias of the neural network. Very high values of $\alpha$ result in low fidelity images, some details of the generated aerial image are not consistent with the ground-view image. An optimal solution is by tuning $\alpha$.

Linear interpolation enforces the generation of an image consistent with a text embedding in the linear space between $e_{opt}$ and $e_{tgt}$. This is a reasonable when the desired change is small: when the image spaces corresponding to $e_{opt}$ and $e_{tgt}$ are closeby, linear interpolation works due to local linearity. When the desired change is large (such as ground-to-aerial translation), the image spaces corresponding to $e_{opt}$ and $e_{tgt}$ are not nearby. Since the representation spaces are not globally linear, it becomes essential to search for the solution in a much higher dimensional non-linear space. This is achieved by our alternating strategy. The pseudo code for the alternating strategy is given below. 
\begin{algorithm}
\caption{Alternate Prompting in backward diffusion enables the diffusion model generate a high-fidelity aerial image}
\label{alg:ap}
$x_T\sim\mathcal N(0,I)$\;
\For{$t\gets T$ \KwTo $0$}{
    \eIf{$t\% 2=0$}
    {
        $x_{t-1}=x_t - f(x_t, t, \alpha e_{tgt}+(1-\alpha) e_{opt}).$\;
    }{
        $x_{t-1}=x_t - f(x_t, t, e_{tgt}).$\;
    }
}
\end{algorithm}

While the sampling repetitively alternates between $e_{1}$ and $e_{2}$, it is more beneficial to use $e_{1}$ (over $e_{2}$) at the first iteration. When the diffusion process starts with $e_{1}$, the network generates starts by generating an aerial image with details weakly dictated by its bias. Subsequent iterations that alternate between $e_{2}$ and $e_{1}$ fortify the generation of a high fidelity aerial image. On the contrary, when the diffusion process starts with $e_{2}$, the generated image in the first iteration is less aerial though with very high fidelity. The bias, along with $e_{opt}$ serve as a strong prior towards a non-aerial viewpoint. Subsequent iterations that use $e_{1}$ are unable to overcome this strong prior to alter the viewpoint to aerial view. Hence, we start inferencing with $e_{1}$. 
