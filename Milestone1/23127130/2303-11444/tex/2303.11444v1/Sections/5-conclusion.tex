\section{Conclusion, Limitations and Future Work}

In this paper, we introduce a novel method, Aerial Diffusion, for generating aerial views from a single ground-view image using 'source text' and homography as guidance and a diffusion model on the generated image with
an alternating denoising direction based on switching between viewpoint text embedding and fidelity of the generated images.

Our method has some limitations. First, it is limited to the knowledge contained in the pretrained stable diffusion model and is unable to hallucinate scenes~\cite{somepalli2022diffusion} outside of this domain. Second, it cannot reconstruct details on human faces. Third, the problem domain of unpaired ground-to-aerial does not have concrete quantitative analysis metrics. Fourth, the value of $\alpha$ in sampling needs to be manually tuned. Future work can focus on the development of methods that overcome these limitations. Other directions include extending Aerial Diffusion to complex scenes with multiple objects (and an intricate background), generating higher fidelity images, extending the method to videos, using the synthetic aerial data for aerial video analysis tasks, etc. 

\noindent \textbf{Acknowledgements.} We would like to thank Abhilasha Sancheti and Kuldeep Kulkarni for their feedback on the paper. 