\section*{A.1. Results}

We extensively analyze our method on in-the-wild images across various domains such as nature, indoor settings, hypothetical scenarios, human actions, etc. All our ground-view input images are from Google Images, Flickr, and the TEDBench dataset~\cite{kawar2022imagic}. We present the results in Figures$2 - 10$. 

\section*{A.2. User study}

We quantitatively evaluate Imagicâ€™s performance via an
extensive human perceptual evaluation study. We conduct the following types of evaluation:
\begin{enumerate}
    \item Given a ground-view image and an aerial-view image generated using Aerial Diffusion, we ask participants to determine if the generated aerial-view image is a high fidelity (w.r.t. ground-view image) aerial-view image; and rate the image on the 5-point Likert scale. The average rating over 10 images is 3.289.   
    \item Given a ground-view image and two aerial-view images generated using Aerial Diffusion and Ablation 2 (i.e. Aerial Diffusion without the Alternating Sampling method), we ask participants to choose the better high fidelity aerial-view image. $83.05\%$ of the participants rate the image generated using Aerial Diffusion as the one with a higher quality. We also ask participants to rate (on the 5-point Likert scale) both images in terms of its `aerial viewpoint' and `fidelity w.r.t. ground-view image' individually. The average rating w.r.t. aerial viewpoint is $3.76$ and $2.98$ respectively for Aerial Diffusion and Ablation 2 (i.e. Aerial Diffusion without the Alternating Sampling method). The average rating w.r.t. fidelity is $3.275$ and $3.36$ respectively for Aerial Diffusion and Ablation 2 (i.e. Aerial Diffusion without the Alternating Sampling method). Clearly, our Alternating Sampling method is able to generate better high fidelity aerial images than linear interpolation. 
    \item Given a ground-view image and two aerial-view images generated using Aerial Diffusion and Ablation 3 (Aerial Diffusion with $e_{tgt}$ instead of $e_{src}$ while training), we ask participants to choose the better high fidelity aerial-view image. $78.125\%$ of the participants rate the image generated using Aerial Diffusion as the one with a higher quality. The average rating w.r.t. aerial viewpoint is $4.06$ and $3.325$ respectively for Aerial Diffusion and Ablation 3 (Aerial Diffusion with $e_{tgt}$ instead of $e_{src}$ while training). The average rating w.r.t. fidelity is $3.725$ and $3.375$ respectively for Aerial Diffusion and Ablation 3 (Aerial Diffusion with $e_{tgt}$ instead of $e_{src}$ while training). Clearly, our method using $e_{tgt}$ for training instead of $e_{src}$ is able to generate better high fidelity aerial images than linear interpolation. 
\end{enumerate}


\begin{figure*}
    \centering
    \includegraphics[scale=0.19]{Figures/overview.png}
    \caption{\textbf{Typo correction.} In the overview figure (Figure 3) of the main paper, corresponding to step 3's legend for $e_{1}$ and $e_{2}$, we interchanged the red and blue colors. We apologize for the typo. We will correct it in the final version of the paper. 
    }
    \label{fig:overview_corrected}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results1.png}
    \caption{\textbf{Results - Part 1.} We apply Aerial Diffusion on diverse images such as animals/ birds, natural scenes, human actions, indoor settings, etc and show that our method is able to generate high-quality high-fidelity aerial images.}
    \label{fig:supp_results1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results2.png}
    \caption{\textbf{Results - Part 2.} We apply Aerial Diffusion on diverse images such as animals/ birds, natural scenes, human actions, indoor settings, etc and show that our method is able to generate high-quality high-fidelity aerial images.}
    \label{fig:supp_results2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results3.png}
    \caption{\textbf{Results - Part 3.} We apply Aerial Diffusion on diverse images such as animals/ birds, natural scenes, human actions, indoor settings, etc and show that our method is able to generate high-quality high-fidelity aerial images.}
    \label{fig:supp_results3}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results4.png}
    \caption{\textbf{Results - Part 4.} Based on the text description, Aerial Diffusion can generate aerial views with scene entities slightly different from the ground-view. The hallucination of background and unseen parts of the scene can also be controlled by text.}
    \label{fig:supp_results4}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results8.png}
    \caption{\textbf{Results - Part 5.} Based on the text description, Aerial Diffusion can generate aerial views with scene entities slightly different from the ground-view. The hallucination of background and unseen parts of the scene can also be controlled by text.}
    \label{fig:supp_results4}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results9.png}
    \caption{\textbf{Results - Part 6.} Based on the text description, Aerial Diffusion can generate aerial views with scene entities slightly different from the ground-view. The hallucination of background and unseen parts of the scene can also be controlled by text.}
    \label{fig:supp_results4}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.48]{Figures/supp_results5.png}
    \caption{\textbf{Effect of $\alpha$.} Low values of $\alpha$ generate images that are less aerial, high values of $\alpha$ generate low-fidelity images. A trade-off between the viewpoint and fidelity generates high-fidelity aerial images. The transformation, with $\alpha$, is not smooth, reinforcing Postulate 2.}
    \label{fig:supp_results5}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.5]{Figures/supp_results6.png}
    \caption{\textbf{State-of-the-art comparisons.} We compare with the state-of-the-art text-based image translation method, IMAGIC~\cite{kawar2022imagic} (CVPR 2023). IMAGIC is unable to generate a high-fidelity aerial view image. It merely reproduces the ground-view image, despite tuning all hyperparameters exhaustively. The hyperparameter $\eta$ (in IMAGIC~\cite{kawar2022imagic}) takes values between $0$ and $1$, and is unable to generate a high fidelity aerial image for any value of $\eta$. When $\eta$ is increased to a value higher than $1$, the model generates an aerial image but the fidelity with respect to the ground-view is completely lost. Other methods (contemporary/prior to IMAGIC~\cite{kawar2022imagic} (CVPR 2023)) such as DDIB~\cite{su2022dual} (ICLR 2023), DreamBooth~\cite{ruiz2022dreambooth} (CVPR 2023), SINE~\cite{zhang2022sine} (CVPR 2023), SDEdit~\cite{meng2021sdedit}(ICLR 2022) and Text2LIVE~\cite{bar2022text2live}(ECCV 2022) face the same issues. These are due to the challenges involved in ground-to-aerial translation described in Section 1 and Section 3.1.}
    \label{fig:supp_results6}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[scale=0.45]{Figures/supp_results7.png}
    \caption{\textbf{Failure cases.} Our method is unable to generate high fidelity aerial views of scenes that have high complexity with multiple objects or scene entities, especially when the text description of the scene is brief and inadequate. This is a direction for future work.}
    \label{fig:supp_results7}
\end{figure*}

