%%%% ijcai22.tex

\typeout{IJCAI--22 Instructions for Authors}

% These are the instructions for authors for IJCAI-22.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai22.sty is NOT the same as in previous years
\usepackage{ijcai22}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\urlstyle{same}
\usepackage{subfigure}
\usepackage{lipsum}


% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newcommand{\bm}[1]{\mbox{\boldmath{$#1$}}}
% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as a credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
/TemplateVersion (IJCAI.2022.0)
}

\title{Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network}

% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     pcchair@ijcai-22.org
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Zhizhong Tan$^1$
\and
Min Hu$^2$\and
Yixuan Wang$^1$\and
Lu Wei$^1$ \And
Bin Liu $^1$ \thanks{Corresponding author:liubin@swufe.edu.cn.}
\affiliations
$^1$Center of Statistic Research, School of Statistics, Southwestern University of Finance and Economics\\
$^2$School of Finance, Southwestern University of Finance and Economics
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
}

% \fi

\begin{document}

\maketitle

\begin{abstract}
% Future price forecasting plays a central role in market price discovery and market risk circumvention. It is a challenging problem to predict the price with traditional econometric models as one needs to consider not only futures' historical trading data but also correlations among futures. 
% The rapid development of artificial intelligence technology has promoted the transformation of quantitative investment from classic econometric models to deep learning. 
It is a challenging problem to predict trends of futures prices with traditional econometric models as one needs to consider not only futures' historical data but also correlations among different futures. Spatial-temporal graph neural networks (STGNNs) have great advantages in dealing with such kind of spatial-temporal data. However, we cannot directly apply STGNNs to high-frequency future data because future investors have to consider both the long-term and short-term characteristics when doing decision-making. To capture both the long-term and short-term features, we exploit more label information by designing four heterogeneous tasks: price regression, price moving average regression, price gap regression (within a short interval), and change-point detection, which involve both long-term and short-term scenes. 
To make full use of these labels, we train our model in a continual manner. Traditional continual GNNs define the gradient of prices as the parameter important to overcome catastrophic forgetting (CF). Unfortunately, the losses of the four heterogeneous tasks lie in different spaces. Hence it is improper to calculate the parameter importance with their losses. We propose to calculate parameter importance with mutual information between original observations and the extracted features. The empirical results based on 49 commodity futures demonstrate that our model has higher prediction performance on capturing long-term or short-term dynamic change. 


\end{abstract}

\section{Introduction}
Futures prices are regarded as an authoritative indicator of market conditions. % Effective futures price forecasting can help production activities, market investors, and policymakers to avoid price risks, increase earnings, and stabilize futures prices.
As the core of futures quantitative investment, high-precision trend prediction of futures prices is of great practical significance. For decades, classical trend predict methods of time series such as vector autoregression (VAR)\cite{sims1980macroeconomics}, autoregressive integrated moving average (ARIMA) model\cite{box2015time}, generalized autoregressive conditional heteroskedasticity (GARCH) \cite{bollerslev1986generalized}, which assume specific models to characterize financial time series data, were widely considered to be a general approach to capture the temporal linear dependencies. 

However, more often than not, financial systems are complex and their underlying dynamics are not known \cite{taylor2009composable}. At least two significant characteristics of the futures data have been ignored or partially ignored by the existing works. First, in quantitative investment, especially in high-frequency trading, faster and more precise predictions can provide reliable signals for trading strategies so as to timely and accurately make trading decisions. But in practice, it is extremely harder to predict the short-term trend than the long-term trend. Most of the existing methods ignore the interaction between the short-term and long-term features,
% \textcolor{blue}{ the interaction between the prediction of future moments with different temporal distances}, 
which may cause the model to fail to obtain enough information, thus limiting the accuracy of the prediction \cite{cheng2020towards}.  
% \textcolor{red}{First, there are short-term investors and long-term investors in futures markets. Most traders look at the future as a means of short-term trading in the market. For example, in the field of high-frequency trading, faster and more precise predictions can provide reliable signals for trading strategies so as to timely and accurately evaluate and make trading decisions. Alternatively, some traders prefer to hold futures in a relatively long term. The features of long-term data and short-term data are correlated but not exactly the same.} 
Second, the price of one future usually shows a strong dynamic correlation with other futures \cite{basak2016model}. That is, each variable (futures contract) depends not only on its historical values but also on the observation of other variables. For example, a shortage of petroleum may impact the price of synthetic resin at the downstream industrial chain.
% For example, the dependence of various varieties in the futures market at a certain time can be naturally represented by a graph as shown in Figure \textcolor{red}{\textbf{\ref{fig:shortLongTerm}}}. 
These two features of real-time series data cannot be reflected by analytical equations containing parameters \cite{sims1980macroeconomics,box2015time}.

% \begin{figure}[!thp]
%     \centering
%     \includegraphics[width=7cm]{Figures/volatility.png}
%     \caption{volatility clustering.}
%     \label{fig:volatilityclustering}
% \end{figure}

The deep learning models are proven to be more efficient than classical econometric models. 
Lots of deep learning methods have been extended to predict the multivariate time series \cite{shih2019temporal,wu2020connecting,sen2019think}. For example, ConvLSTM \cite{shi2015convolutional} has been proposed to model the temporal dependence with the recurrent structure of LSTM and capture the spatial correlation with a traditional convolution. However, LSTM variants do not model the pair-wise dependencies among variables explicitly, which weakens model interpretability \cite{wu2020connecting}. 
Performing node feature aggregation and updates, graph neural network (GNN) \cite{gori2005new,scarselli2008graph} extends the deep learning methodology to graphs that can capture these complex dependencies naturally. 

In this work, we focus on modeling the multiple futures series by considering the two aforementioned characteristics simultaneously. To capture the spatial correlation among futures, we propose to use STGNNs to model the multivariate future data as a base model. However, if we only focus on price forecasting or change-point detection (CPD), there will be only one kind of supervised label, which is insufficient to learn comprehensive features to balance long- and short-term considerations. To alleviate this problem, we propose four heterogeneous tasks, futures' close price forecasting, gap regression, moving average (MA) regression, and change-point detection (CPD). To learn holistic long-term and short-term features, we train the proposed model with these four tasks in a continual manner. 

The rationale is that the task of gap regression captures the law behind the spreads between the maximum and the minimum within a fixed time interval. The task of real-time futures price forecasting aims to capture the short-term feature from local fluctuation. On the contrary, the task of MA depicts the average settle prices over a window of time to show the historical volatility of futures prices, which tries to extract the long-term signal. The task of CPD helps us to find abrupt changes in data when a property of the time series changes, which takes into account both long-term and short-term features. 
In addition, we force our model to learn an overall experience from the four tasks in a continual way to avoid ``catastrophic forgetting" on the long-term and short-term features \cite{liu2021overcoming}. 

Traditionally, training artificial neural networks on new tasks typically causes them to forget previously learned tasks, which refers to the issue of catastrophic forgetting \cite{liu2021overcoming}.
% After training on the current task, the model learns a set of optimal parameters that minimize this task.
Zenke et al.\cite{zenke2017continual} propose a quadratic surrogate loss to solve this problem. The key point of the surrogate loss is a per-parameter regularization strength. It tries to "remember" the best model parameters for the last task as well as to adapt to the new task. The existing works \cite{zenke2017continual,liu2021overcoming} approximate the per-parameter regularization strength with the ratio of loss dropping (the gradient of loss). In our problem, however, the tasks are heterogeneous with each other, therefore their corresponding losses are heterogeneous as well. Hence, the loss dropping based on per-parameter strength is inappropriate for futures price modeling. In this paper, we creatively introduce mutual information between the extracted features and original features to substitute the traditional loss change ratio. The rationale is that the mutual information among different tasks is much smoother than their losses. Experimental results show that our model has higher prediction accuracy and can effectively predict the dynamic trend of multi-future time series data. 

The main achievements, including contributions to the field, can be summarized as follows:
\begin{itemize}
    \item Conceptually, we propose four heterogeneous tasks to balance both long-term and short-term quantitative trading requirements.
    \item Technically, we propose a novel Heterogeneous Continual Graph Neural Network (HCGNN) to learn temporal (long-term and short-term) and spatial dependencies. We design a mutual information-based parameter strength to adapt to heterogeneous tasks.
    \item Empirically, we evaluate the proposed model on a real-world dataset that involves 49 types of futures in the Chinese futures market.
\end{itemize}

\section{Related Works}
\subsection{Deep Learning in Financial Quantitative Investment }
A lot of literature have studied deep learning in financial quantitative investment. Solving the problem of long-term dependence of the series well, LSTM \cite{hochreiter1997long} and its variants~\cite{graves2005framewise}\cite{sesti2021integrating}\cite{ly2021forecasting} are the preferred choice of most researchers in the field of financial time series forecasting, including but not limited to stock price forecasting\cite{chen2015lstm}, index forecasting \cite{baek2018modaugnet,jeong2019improving,siami2018forecasting}, commodity price forecasting\cite{grewal2022machine}, forex price forecasting\cite{ayitey2022forex}, volatility forecasting\cite{zhou2019long}. Salinas et al. \cite{salinas2020deepar} embedded LSTM or its simplified version gated recurrent units (GRU) as a module in the main architecture. Meanwhile, hybrid models were used in some of the papers. For example, Wu et al.\cite{wu2021graph} combined CNN (Convolutional Neural Networks) and LSTM to predict the rise and fall of the stock market. They exploited the ability of convolutional layers for extracting useful financial knowledge as the input of LSTM. Combining complementary ensemble empirical mode decomposition (CEEMD), PCA, and LSTM, Zhang et al. \cite{yan2020novel} constructed a deep learning hybrid prediction model for stock markets based on the idea of "decomposition-reconstruction-synthesis". For multivariate time series, Lai et al. \cite{lai2018modeling} proposed a deep learning framework, namely a long- and short-term time-series network (LSTNet) which employed convolutional neural networks to capture local dependencies. LSTNet cannot fully exploit latent dependencies between pairs of variables and is difﬁcult to scale beyond a few thousand-time series. Similar attempts were made in \cite{shih2019temporal} and \cite{sen2019think}. In general, these methods often completely disregard available relational information or can not take full advantage of nonlinear spatial-temporal dependencies.

As a popular model emerging in recent years, GNN has been applied to different tasks and domains. Due to its good performance in graph modeling methods, GNN has been continuously promoted in the financial field by transforming financial tasks into node classifications. To represent relational data in the financial field, graphs are usually constructed, including user relations graphs~\cite{xu2021towards}, company-based relationship graphs~\cite{sawhney2020deep}, and stock relationship graphs~\cite{li2021modeling,hsu2021fingat}. Sawhney et al.~\cite{sawhney2020deep} concentrated on stock movement forecasting by blending chaotic multi-modal signals including inter-stock correlations via graph attention networks (GATs) \cite{velivckovic2017graph}. Taking the lead-lag effect in the financial market into account, Cheng et al.~\cite{cheng2022financial} constructed heterogeneous graphs to learn from multi-modal inputs and proposed a multi-modality graph neural network (MAGNN) for ﬁnancial time series prediction. Besides constructing graphs from prior knowledge, for multivariate time series forecasting, researchers have been trying to discover graphical relationships between variables from the data.

With multiple ways of graph construction, it becomes challenging to obtain and select the relation for graph construction \cite{wu2020connecting,cao2020spectral}. For example, Cao et al.~\cite{cao2020spectral} proposed a StemGNN, which constructed a graph with the attention mechanism. Then they decompose the spatial-temporal signal into a spectral domain and frequency domain with GFT (graph Fourier transform) and DFT (discrete Fourier transform) respectively for multivariate time-series forecasting. Departing from their work, we extend StemGNN to a continual learning scene.

\subsection{Multi-task Continual Learning}
In the area of multi-task continual learning (CL), a well-known problem is catastrophic forgetting (CF) \cite{liu2021overcoming}. That is when training a new task the mode will update existing network parameters, which may cause accuracy degradation of the task's former tasks that the model has learned. Using regularization to mitigate CF by penalizing changes to important parameters learned in the past is a popular approach~\cite{zhang2020class}. Rehearsal is another popular approach, which stores or generates examples of previous tasks and replays them in training a new task~\cite{wang2022memory}. Almost all online CL methods are based on rehearsal. Farajtabar et al.~\cite{farajtabar2020orthogonal} addressed the CF issue from a parameter space perspective and studied an approach to restricting the direction of the gradient updates to avoid forgetting previously-learned data. Another alternative CL solution is Elastic Weight Consolidation (EWC) ~\cite{EWC2017pnas} and its variations \cite{pmlrv70zenke17a,NIPS2017_f708f064,Aljundi2018ECCV}. The idea of these methods is trying to preserve the optimal parameters inferred from the previous tasks while optimizing the parameters for the next task. Liu et al.~\cite{liu2021overcoming} extended EWC to GNNs by proposing a topology-aware weight preserving (TWP) term. However, all these models assume that all the tasks are isomorphic. In this work, we proposed a new framework for a heterogeneous multi-task setting.

\section{Problem Formulation}
% In this paper, \textcolor{red}{we are interested in the task of multivariate futures' time series forecasting.} 
We study 49 commodity futures in China market, which are multivariate time series data. In order to effectively represent the structural relationship among future varieties, we use the graph to describe the inter-series correlations. Let  $\mathcal{G}=(\mathbf{A},\mathbf{X})$, $\mathbf{A}\in \mathbb{R}^{N\times N}$ denotes an adjacent matrix of the futures and $\mathbf{X}=\{x_{i,t}\}  \in \mathbb{R}^{N\times T}$ represents the input of multivariate time series, $N$ is the number of future varieties. 
% For convenience of statement, let $\mathbf{x}^i:=\mathbf{X}_{i,:}$ be $i$-th row that represent the one future, and $\mathbf{x}_{t,t+\Delta t}:=[\mathbf{x}_t, \mathbf{x}_{t+1},...,\mathbf{x}_{t+\Delta t}]$

\subsection{Four Tasks for Future Price Modeling}
\label{sec:4tasks}
We propose four well-designed tasks for future data modeling, including \textbf{price forecasting}, \textbf{gap regression}, \textbf{moving average (MA) regression}, and \textbf{change-point detection (CPD)}. 
% The purpose of auxiliary tasks is to help the original task allocate attention to features that are also important to other tasks and assist the model in better generalizing to new tasks. 

\textbf{Future Price Forecasting.} Given the historical observations of previous $T$ timestamps $\mathbf{X}_{t-T},\dots,\mathbf{X}_{t-2},\mathbf{X}_{t-1}$, where $\mathbf{X}_{t} \in \mathbb{R}^{N}$, the goal for a multivariate time series forecasting is to learn a mapping function from the historical values to the next $T^{'}$ timestamps $\mathbf{X}_{t},\mathbf{X}_{t+1},\dots,\mathbf{X}_{t+T^{'}-1}$ in the graph $\mathcal{G}$,
\begin{equation*}\label{eq:priceForecatingTask}
         [ \mathbf{X}_{t-T},\dots,\mathbf{X}_{t-1} ,\mathbf{A};\Theta;W ] \rightarrow[ \mathbf{X}_{t}, \dots, \mathbf{X}_{t+T^{'}-1}  ],  
\end{equation*}
where $\Theta,W$ are necessary parameters.

\textbf{Gap Regression.} We encourage our model to make a regression on the dispersion ratio within a fixed time window to capture short-term features from local fluctuation as shown in Equation \ref{eq:gapRregression}. We call it gap regression,
\begin{equation}
   \Delta \mathbf{X}^{(l)}_t = \frac{ \mathbf{X}_{max} - \mathbf{X}_{min}}{l}
\label{eq:gapRregression}
\end{equation}
% \begin{equation}
%    \Delta x^{(l)} = \frac{ \text{max}(x_{i,t})- \text{min}(x_{i,t+l})}{l}
% \label{eq:gapRregression}
% \end{equation}
where $\mathbf{X}_{max}=\max ( \mathbf{X}[:,t:t+l-1])$ ($\mathbf{X}_{min}=\min ( \mathbf{X}[:,t:t+l-1])$) is a row maximum (minimum) operation within the sliding window, which returns row maximum (minimum) values of the slice $\mathbf{X}[:,t:t+l-1]$, $l$ is the window length.

% $\mathbf{X}_{max}=\max \{\mathbf{X}_t, \cdots,\mathbf{X}_{t+l-1}\}$ ($\mathbf{X}_{min}=\min \{\mathbf{X}_t, \cdots, \mathbf{X}_{t+l-1}\}$) is a row maximum (minimum) operation within the sliding window, which returns  and 
% where $l$ is the window length.





\textbf{Moving Average Regression.} A moving average is a type of convolution. It can be viewed as an example of a low-pass filter that can smooth signals. In this work, we resort to the moving average to capture the long-term change trends of futures prices. Given a discrete time series with observations $\mathbf{X}_{t}$ , a moving average of $\mathbf{X}_{t}$ can be calculated as follows,

\begin{equation*}\label{eq:movingAverage}
   \Bar{\mathbf{X}}_t =  \left ( \mathbf{h}*\mathbf{X} \right )_t = \sum_{i=-L }^{i=L }\mathbf{h}\left ( i \right ) \mathbf{X}\left( t-i \right ) ,
\end{equation*}
where $\Bar{\mathbf{X}}_t$ is the smoothed time series and $\mathbf{h}$ denotes a discrete density function (that is $\sum_{i=-L }^{i=L } \mathbf{h}\left ( i \right )=1$). The input sequence $\mathbf{X}_{t}$ could be the futures' historical settle prices.

\textbf{Predicting change-points.}
During futures trading, it is of great importance for investors to detect trend changes in price series in a timely and accurate manner, whether they are manual traders or programmed traders. In a way, if an investor can accurately predict a trend reversal, he can not only increase excess returns for the investor but also avoid a lot of unnecessary losses. We have therefore investigated the task of change-point detection to increase the excess returns of the investments. We calculate the change-points of the real price data with ruptures \cite{truong2020selective} and then try to classify them in our model. The labels of change-points are defined as follows,
\begin{equation*}
y_{x_{i,t_k}}=\left \{
\begin{array}{rcl}
 1 && \text{change-point at}\ t_k\\
0 && \text{otherwise} \\
\end{array} \right.
\label{eq:changePointLabel} 
\end{equation*}


% Let $x_{a,b}:=[x_a, x_{a+1},...,x_{b}]$, for a given threshold $\mathbf \eta (\eta>0)$, the label of change points within the fixed window is defined as,
% \begin{equation}
% y_{x_{t_k}}=\left \{
% \begin{array}{rcl}
%  1 && {max(x_{t_k,t_{k+1}})-x_{t_k}> \eta *x_{t_k} }\\
% 0 && {max(x_{t_k,t_{k+1}})-x_{t_k} \leq \eta *x_{t_k} } \\
% \end{array} \right.
% \label{eq:changePointLabel} 
% \end{equation}
% Equation \ref{eq:changePointLabel} indicates that we prefer label 1 (going-long) for our investment strategy.

% Clearly, price forecasting and gap regression are beneficial for the analysis of short-term trends in futures prices, while moving averages are relatively beneficial for the analysis of long-term price trends for investors, whether they are present in programmed or subjective trading, and change-point detection is applicable to both short-term and long-term investments. It is important to note that the terms "long-term" and "short-term" investing in this paper are relative concepts and they will change accordingly depending on the granularity of time.

% \textcolor{red}{To learn the common knowledge of all tasks while balancing the learning performance of all individual tasks, we propose four tasks that share data but retained their respective parameters with constraints. All tasks provide each other with additional data enhancement in a regularized form.} 
The four tasks in this article are described as follows,
\begin{equation}\label{eq:fourTasks}
\left[\mathbf{X}_{t-T},\dots,\mathbf{X}_{t-1},\mathbf{A};\Theta;W\right ]\overset{F}{\rightarrow}
    \begin{cases}
    \left [ \mathbf{X}_{t}, \dots, \mathbf{X}_{t+T^{'}-1} \right ] \\
    \Delta \mathbf{X}^{(l)}_t\\
    \Bar{\mathbf{X}}_t\\
    y(x_{i,t_k}), t_k \geq t
    \end{cases}       
\end{equation}
where these values can be inferred by an overall forecasting model $F:=g(f(x;\Theta); W)$ with parameter $\Theta$ and $W$ as shown in Figure \ref{fig:modelPremeter}. $f(x;\Theta)$ is a spatial-temporal module and $g(f;W)$ is a link function to adapt the downstream tasks as we stated before. Usually, $g$ is a multi-layer perception.
To learn the common knowledge of all tasks while balancing the learning performance of all individual tasks, we conduct a continual learning the four tasks, which will be introduced in next section.   

\section{Model}
In this section, we provide a detailed formulation of a Heterogeneous Continual Graph Neural Network (HCGNN). 
% Figure 1 shows the overall model framework of the proposed approach.


\subsection{Overview}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=8.5cm]{Figures/overallArch.png}
    \caption{$f(\mathbf{X};\Theta)$ is a spatial-temporal module, $g(f;W)$ is a feedforward neural network.}
    \label{fig:modelPremeter}
\end{figure}

Figure \ref{fig:modelPremeter} illustrates the overall framework of the proposed method. With the inputs of multivariate futures data from the four tasks, we extract their features $f$ with the spatial-temporal modules in $f(\mathbf{X};\Theta)$. The spatial-temporal modules are connected in a residual mode. Then we adapt them to the four downstream tasks (as shown in Equation \ref{eq:fourTasks}) with the module $g(f; W)$, which could be a multi-layer perception. The feature extraction process can be defined by the composite function $F:=g(f(\mathbf{X};\Theta); W)$ as we stated in section \ref{sec:4tasks}. 
Finally, we train the four tasks in a continual manner.
 

Suppose there are four related tasks for futures data modeling, $\mathcal{T}:=\{\mathcal{T}^1, \mathcal{T}^2, \mathcal{T}^3, \mathcal{T}^4\}$. As shown in Equation \ref{eq:fourTasks}, all four tasks share the feature $\mathbf{X}_{t-T,t-1}$, but their target spaces are heterogeneous. That is, $\mathbf{Y}^{1}_t=\mathbf{X}_{t,t+T^{'}-1}$, $\mathbf{Y}^{2}_t=\Delta \mathbf{X}^{(l)}_t$, $\mathbf{Y}^{3}_t= \Bar{\mathbf{X}}_t$, $\mathbf{Y}^{4}_t\in \{0,1\}$. We train the proposed model in a continual learning manner. Unfortunately, the traditionally continual learning failed to consider a heterogeneous setting of tasks. In this work, we propose a mutual information based method to satisfy such a heterogeneous requirement for continual learning as follows,
% Note that the setting of the proposed model is different from traditional continual learning in which the targets of each task lie in the same space. For our problem, the four targets are heterogeneous.
% The key point is how to learn the parameters $\Theta$ and $W$ with the four tasks. 

% \begin{equation}\label{eq:continualLearningFrameworkOld}
%     \begin{split}
%           L_{k+1} (\Theta,W) & =  L_{k+1}^{\text{new}} (\Theta,W) +
%           \lambda_1\sum_{n=0}^{k-1} \sum_{i,j}[\Omega^{\Theta}]_{ij}([\Theta]_{ij}-[\Theta_n^*]_{ij})^2\\
%           &+\lambda_2\sum_{n=0}^{k-1} \sum_{i,j} [\Omega^W]_{ij}([W]_{ij}-[W^*_n]_{ij})^2,
%     \end{split}
% \end{equation}
\begin{equation*}\label{eq:continualLearningFramework}
    \begin{split}
          L_{k+1} (\Theta,W) & =  L_{k+1}^{\text{new}} (\Theta,W) +
          \lambda_1\sum_{n=1}^k \Omega^{\Theta}\odot \Vert\Theta-\Theta_n^*\Vert_2^2\\
          &+\lambda_2\sum_{n=1}^k  \Omega^W\odot \Vert W-W^*_n\Vert_2^2,
    \end{split}
\end{equation*}
where $\lambda_1$ and $\lambda_2$ are regularization penalties, $L_{k+1}^{\text{new}}(\Theta, W) $ denotes the $(k+1)$-task-related loss function, e.g., it could be a cross-entropy loss for the classification task 4 or mean square error for regression tasks. $\Theta_n^*$ and $W^*_n$ are the optimal parameters of the $n$-th task.
$\odot$ is an element-wise product. $\Omega^{\Theta}$ and $\Omega^W$  play a role of ``indicator" matrices that try to ``divide" the network parameters $\Theta$ ($W$) into groups for each task. Large values in $\Omega^{\Theta}$ ($\Omega^W$) indicate the corresponding parts in $\Theta_n^*$ ($W^*_n$) are important memories for the previous task $n$ ($n=0,...,k-1$), hence we have to keep them when training the task $k+1$. In other words, we can only allow updating the parts of $\Theta$ ($W$) where the indicator values in $\Omega^{\Theta}$ ($\Omega^W$) are small.
This learning strategy can not only ensure that the parameters of low-importance scores can be freely changed to adapt to new tasks, but also punish the parameters of high-importance scores so that the model can still perform well on previous tasks.

The key part of the continual learning framework is the calculation of the per-parameter regularization strength $\Omega:=\{\Omega^W, \Omega^{\Theta}\}$ \cite{pmlrv70zenke17a}. 
In \cite{pmlrv70zenke17a}, $\Omega$ was defined with the gradient of the loss. However, considering the heterogeneous tasks in our problem, we proposed to calculate $\Omega$ with mutual information that will be introduced in section \ref{sec:Omega}. 

\subsection{Mutual Information Based $\Omega$}
\label{sec:Omega}
% our Motivation
As we mentioned before, traditionally, people define the per-parameter regularization strength $\Omega$ as the magnitude of the gradient of loss \cite{zenke2017continual}, 
\begin{equation*}
    \Omega = \left\|  \frac{\partial L}{\partial \Theta} \right \|,
\end{equation*} 
however, for our problem, the four tasks (section \ref{sec:4tasks}) are heterogeneous because the targets (Equation (\ref{eq:fourTasks})) of them lie in different spaces. It suggests that the corresponding losses of the four tasks will present in different scales. For example, the price regression loss may be totally different from the change-point classification loss. Even for two regression tasks, the moving average regression loss and price regression loss may lie in different metric spaces because the fluctuation of real-time price is more severe than the smooth average.

The heterogeneous setting of tasks motivates us to develop new parameter strength. In this work, we replace the gradient of the training loss with \textit{the gradient of mutual information between the original feature and extracted feature described by the model parameters}.

Specifically, we calculate both the mutual information between the original feature $\mathbf{X}^k$ of task $k$ ($k\in\{1, 2, 3, 4\}$) and the feature map $f(\mathbf{X}^{k};\Theta)$ learned from the deep neural network.
% as shown in Equation \ref{eq:motivation1} and \ref{eq:motivation2}. 
For an infinitesimal parameter perturbation on $\Delta \Theta=\{\Delta \boldsymbol\theta_p\}$, the changes in mutual information can be approximated by
\begin{equation}\label{eq:motivation1}
    \begin{split}
        &\text{MI}(\mathbf{X}^{k}, f(\mathbf{X}^{k};\Theta + \Delta \Theta)) -  \text{MI}(\mathbf{X}^{k}, f(\mathbf{X}^{k};\Theta))  \\& 
        \approx \sum_m \frac{\partial \text{MI}(\mathbf{X}^{k}, f(\mathbf{X}^{k};\Theta))}{\partial \boldsymbol\theta_p} \Delta \boldsymbol\theta_p,
        % \approx \sum_{i}\frac{\partial \text{MI}(x^{k} ; f(x^{k};\Theta))}{\partial \theta_i} \Delta \theta_i
    \end{split}
\end{equation}
where MI$(x,y)$ is the mutual information between $x$ and $y$.

From Equation (\ref{eq:motivation1}), we observe that the contribution of the parameter $\boldsymbol \theta_p$ to the change of mutual information can be approximated by the gradient of the mutual information with respect to $\boldsymbol \theta_p$.
% , $\frac{\partial \text{MI}(\mathbf{X}^{k}, f(\mathbf{X}^{k};\Theta))}{\partial \boldsymbol\theta_p}$.


Similarly, we can calculate the mutual information between the predicted value $g(\mathbf{Z}^k;W)$ and ground-truth target $\mathbf{X}^k$, where $\mathbf{Z}^k=f(\mathbf{X}^k;\Theta)$ is the feature map output by a deep spatial-temporal module as shown in Fig \ref{fig:modelPremeter}. 
Consequently, the change of the mutual information with respect to $\Delta W =\{\Delta \boldsymbol w_q\}$ can be approximated as follows,
\begin{equation}\label{eq:motivation2}
\begin{split}
    &\text{MI}(g(\mathbf{Z}^k; W+\Delta W), \mathbf{X}^k) -  \text{MI}(g(\mathbf{Z}^k;W), \mathbf{X}^k)  \\
    &\approx \sum_m (\frac{\partial \text{MI}(g(\mathbf{Z}^k;W), \mathbf{X}^k)}{\partial \boldsymbol w_q}) \Delta \boldsymbol w_q.
    %& \approx \sum_{i}\frac{\partial \text{MI}(g(\mathbf{a}^k;W); y^k)}{\partial W_i} \Delta W_i
\end{split}
\end{equation}

According to Equation. (\ref{eq:motivation1}) and (\ref{eq:motivation2}), we define the per-parameter regularization strength $\Omega:=\{\Omega^W, \Omega^{\Theta}\}$ as follows,
\begin{equation}\label{eq:MI_para_significance}
    \begin{split}
        \Omega^{\Theta} &= \bigg \{ \Big[\frac{\partial [\text{MI}(\mathbf{X}^{k+1} ; \mathbf{Z}^{k+1}) + \text{MI}(g(\mathbf{Z}^{k+1};W) ; \mathbf{X}^{k+1})]}{\partial \boldsymbol\theta_{p}} \Big]_{ij} \bigg\},\\
        \Omega^W &= \bigg\{ \Big[\frac{\partial \text{MI}(g(\mathbf{Z}^{k+1};W) ; \mathbf{X}^{k+1})}{\partial \boldsymbol w_{q}} \Big]_{ij} \bigg \}.
    \end{split}
\end{equation}

\subsection{The Calculation of $\Omega$}
% \textcolor{red}{In order to realize continuous learning and solve the possible deviation caused by cross entropy (CE) in the multi-task learning process, we not only reduce the feature deviation caused by cross entropy (CE) by mutual information maximization, because CE learns only discriminative features for each task, but these features may not be discriminative for other tasks. In addition, when training a new batch of incrementally arrived data, we also saved the previously learned knowledge. Unfortunately, it is notoriously difficult to calculate mutual information in practice.}

In practice, the mutual information in Equation \ref{eq:MI_para_significance} is notoriously difficult to calculate. To overcome this problem, inferring the lower bound and then maximizing the tractable objective becomes a popular choice. Oord et al.\shortcite{Oord2018RepresentationLW} proposed the InfoNCE loss as a proxy objective to maximize the Mutual Information.
According to their research framework, we assume that $\mathbf{X}^{'} $ is a different view of the input variable $\mathbf{X}$ created through data transformation. We have
\begin{equation*}
    \begin{split}
      \max_{\Theta}\text{MI}(\mathbf{X};f(\mathbf{X};\Theta))\ge \max_{\Theta}\text{MI}(f(\mathbf{X};\Theta);f(\mathbf{X}^{'};\Theta))
    \end{split}
\end{equation*}
\begin{equation*}
    \begin{split}
      \max_{W}\text{MI}(g(\mathbf{Z};W) ; \mathbf{X})\ge \max_{W}\text{MI}(g(\mathbf{Z};W) ; \mathbf{X}^{'})
    \end{split}
\end{equation*}

 According to \cite{Oord2018RepresentationLW}, 
\begin{equation}\label{eq:MIbound1}
    \begin{split}
      \max_{\Theta}\text{MI}(f(\mathbf{X};\Theta);f(\mathbf{X}^{'};\Theta))\ge \log B+\text{InfoNCE}(\left \{\mathbf{X}_{i}\right\}^{B}_{i=1})
    \end{split}
\end{equation}


\begin{equation}\label{eq:MIbound10}
    \begin{split}
      \text{InfoNCE}(\left \{\mathbf{X}_{i}\right\}^{B}_{i=1})=\frac{1}{B} \sum_{B}^{i=1}log\frac{s(\mathbf{X}_{i},\mathbf{X}^{'}_{i})}{\sum_{j=1}^{B} s(\mathbf{X}_{i},\mathbf{X}^{'}_{j})} 
    \end{split}
\end{equation}
where $s(\mathbf{X}_{i},\mathbf{X}^{'}_{j})=e^{\frac{f(\mathbf{X};\Theta)^T f(\mathbf{X}^{'};\Theta)}{r} } $ can be regarded as calculating the similarity of $\mathbf{X}_{i}$ and $\mathbf{X}^{'}_{j}$
and $r$ is the temperature. $\{\mathbf{X}_{i}\}^{B}_{i=1}$ are samples from variable $\mathbf{X}$ and $B$ is the batch size. 
we can calculate $\text{MI}(\mathbf{X};f(\mathbf{X};\Theta))$ with Equation \ref{eq:MIbound1} and \ref{eq:MIbound10}\cite{Oord2018RepresentationLW}. 
$\text{MI}(g(\mathbf{Z};W) ; \mathbf{X})$ can be calculated in the same way. 

\section{Experiments}
\begin{figure*}[!t]
    \centering    
    \includegraphics[width=\textwidth]{Figures/forecast_new.png}
    \caption{Close price forecasting results. The ground truth and predictions are colored pink and blue respectively. }
    \label{fig:visualizationPrice1}
\end{figure*}

In this section, we evaluate our model on the tick-level high-frequency futures data on 4 related and heterogeneous tasks.
% against other benchmarks, including AR, VAR-MLP, GP, RNN-GRU, LSTNet, TPA-LSTM for single-step forecasting, StemGNN, StemGNN+multitask, transformer-based model for multi-step forecasting (see Section 5.2 for more details).
To explore the best structure of the HCGNN model, we perform exhaustive ablation studies on the proposed model to validate and analyze the contribution of various components.

% During the experiment, the list of parameters set by the model and their values are shown in Table xxx.

% \begin{table}[!htp] 
% \begin{center} 
% \caption{parameters list.}
% \setlength{\tabcolsep}{11mm}{
%     \begin{tabular}{cc}
%       \toprule[1pt]
%         Parameter        & Value \\
%         \midrule[0.7pt]
%         Activation function & LeakyReLU  \\
%         Epoch & 50   \\
%         Batch size & 32    \\
%         Decay rate & 0.5   \\
%         Dropout rate & 0.5   \\
%         Leaky rate & 0.2   \\
%         .... & ....  \\
%       \bottomrule[1pt]
%     \end{tabular}   }
% \end{center}
% \end{table}


\subsection{Datasets}
The data used in our experiment is the tick-level high-frequency future market series from 14 February 2022 to 28 February 2022. Including the close price (buy price), the highest price within 1 minute, and the lowest price within 1 minute. There are 388,800 sample points. 
Considering that there are missing data for some futures varieties and there are also some newly listed futures varieties that are not available for previous trading records, we 1) delete commodity futures that more than 50\% of their log-return is zero; 2) exclude commodity futures with no trading records for the first five days or the last five days; 3) delete commodity futures with missing slot length over 15 minutes within the study period; 4) for a small number slot of missing values, we fill them with the mean. All the future series data are normalized by the Z-Score method. We finally keep 49 active commodity futures for the following analysis. We split the data into training set, validation set, and test set with a ratio of 7:2:1 in Table \ref{tab:data_set}.

% The data were collected manually by the authors. The original data is the tick-level high-frequency data, and sampling is performed every 500 ms, In order to effectively validate the reliability of the model in the face of external event shocks, we selected trading data for half a month before and after the Russia-Ukraine conflict, from 15 February 2022 to 28 February 2022, including opening, closing, high and low prices, for a total of 388,800 samples to analyze. However, considering that there are missing data for some futures varieties and that some newly listed futures varieties were not listed during most of the interval of the time period and therefore no trading data were available. In order to be more accurate and effective analysis, the data obtained were first processed as follows.
% \begin{enumerate}
%     \item Excluding commodity futures that more than 50\% of their log-return is zero.
%     \item Excluding commodity futures with no trading records for the first five days or the last five days.
%     \item Excluding commodity futures with missing slot length over 15 minutes within the study period. 
%     \item Meanwhile, for a small number slot of missing values, we fill them with the mean. All the future series data are normalized by the Z-Score method. 
% \end{enumerate}


\begin{table}[!htp] 
\begin{center} 
\setlength{\tabcolsep}{5mm}{
    \begin{tabular}{cl}
      \toprule[1pt]
        Datasets        & Period (\# samples) \\ \midrule
		% \# futures       &49  \\ 
		Training set    & 02/14-2/25 (278479)  \\ 
		Validation set  & 02/25-2/27 (79565)  \\ 
		Test set      & 02/27-2/28 (39782)    \\ 
      \bottomrule[1pt]
    \end{tabular}   }
\end{center}
\caption{Data splitting on 49 futures.}
\label{tab:data_set}
\end{table}

\begin{table}[!t] 
  \centering
  \fontsize{6}{12}\selectfont      
	  \scalebox{1.0}{          
        \begin{tabular}{ccccccc}
        \toprule
        \multirow{2}{*}{Models}&
        \multicolumn{3}{c}{1min}&\multicolumn{3}{c}{15min}\cr
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
        &MAE&RMSE&MAPE(\%)  &MAE&RMSE&MAPE(\%)\cr
        \midrule
        % VAR-MLP	 &     &     &     &     &     &          \cr
        % TPA-LSTM &     &     &     &     &     &          \cr
        % TConvLSTM  &     &     &     &     &     &          \cr
        StemGNN  &17.04     &52.35     &10.67     &10.64     &32.87     &11.04
        \cr
        % StemGNN+ multitask	 &3.59     &9.53     &0.03     &          &     &     \cr
		MTGNN	 & 84.29    & 171.09    & 1.19    &64.58     &126.08     & 0.77         \cr
        STGCN	 & 16.79     & 36.88  & 0.1925   & 16.940     & 37.230  &0.1938 \cr
        SSTGNN	 &223.80       &389.30    &4.34     & 68.68    & 135.75  &0.9976 \cr
        % Informer & 12035.99    & 49970.73    &9.9  & 12035.85    & 49970.63    &9.8    \cr
		% DCRNN	 &     &     &     &     &     &        \cr
		% STSGCN	 &     &     &     &     &     &          \cr
		% TAMP-S2GCNets	 & 12872.86     & 51330.32    &13    & 12873.11    & 51331.80       &13     \cr
        {\bf HCGNN}	&{\bf 6.48}&{\bf 12.40}&{\bf0.04}&{\bf 13.70}&{\bf 42.15}&{\bf0.1}\cr
        \bottomrule
        \end{tabular}}       
        \caption{Comparisons of the performance of close price regression between the state-of-the-art approaches and our method. We report 1-minute and 15-minute results.
        }
        \label{tab:performance_comparison}
\end{table}



We make a similar experimental setting as in Cao et al. \cite{cao2020spectral}.
Specifically, the channel size of each graph convolution layer is set as 64 and the kernel size of 1D convolution is 3. The number of training epochs is set as 50 and the learning rate is initialized by 0.001 and decayed with a rate of 0.7 after every 5 epochs.
We select mean Absolute Errors (MAE), Root Mean Square Errors (RMSE), and Mean Absolute Percentage Errors (MAPE) to evaluate the three regression tasks and report precision, recall, accuracy, and F1 to measure the ability of change-point prediction.

%\textbf{ConvLSTM}\cite{shi2015convolutional}, 
\subsection{Baselines Methods}
The baseline methods include \textbf{StemGNN} \cite{cao2020spectral}, \textbf{MTGNN} \cite{wu2020connecting}, \textbf{STGCN} \cite{yu2017spatio}, \textbf{SSTGNN} \cite{sstgnn2021}.
% \textbf{Informer} \cite{2020Informer}.
% \textbf{TAMP-S2GCNets}\cite{chen2022tampsgcnets}. 


% \begin{itemize}
% \item \textbf{ConvLSTM}\cite{shi2015convolutional}.
%   \item \textbf{StemGNN\cite{cao2020spectral}.} Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting\cite{cao2020spectral}.
%   \item \textbf{StemGNN + multitask + MI.} expand StemGNN to multitask scene but without using Mutual information (MI) maximization.
%   \item \textbf{MTGNN\cite{wu2020Connecting}:} The backbone architecture for the temporal modeling of MTGNN is Wavenet. the model constructs relationships between variables by introducing a graph learning module. Specifically, the graph learning module connects each hub node to its first k nearest neighbors in a deﬁned metric space.  Code: https://github.com/nnzhan/MTGNN
%   \item \textbf{STGCN\cite{yu2017spatio}.} STGCN models dynamic skeletons based on a time-series representation of human joint positions, and extends graph convolution to capture this spatial-temporal variation in relationships as a spatial-temporal graph convolution network. It can automatically learn spatial and temporal patterns from the data, which makes the model highly expressive and generalizable.
%   % \item \textbf{DCRNN\cite{2018Diffusion}:} It is a deep learning framework for traffic prediction that incorporates spatial-temporal dependencies in traffic flows. DCRNN captures spatial correlations using bi-directional random wandering on graphs and temporal correlations using an encoder-decoder architecture with scheduled sampling.
%   \item \textbf{STSGCN\cite{2020Spatial}.} The model is able to effectively capture complex local spatial-temporal correlations through a well-designed spatial-temporal synchronous modeling mechanism. Also, several modules with different time periods are designed in the model to capture the heterogeneity in the local spatial-temporal maps effectively. STSGCN is used for spatial-temporal network data prediction.
%   \item \textbf{TAMP-S2GCNets\cite{chen2022tampsgcnets}.} TAMP-S2GCNets integrates two emerging research directions, time-aware deep learning, and multipersistence, by summarising the temporally conditional topological properties inherent to the data as time-aware multipersistence Eulerian-poincar surfaces. This is achieved by constructing a superconvolutional module that takes into account both internal and intra-dependencies extracted from the data. Multivariate time series prediction is achieved. \url{https://github.com/HyunWookL/PM-MemNet}
%  \item \textbf{SST-GNN:\cite{sstgnn2021}}
% \end{itemize}

\subsection{Results}

% \begin{table}[!t] 
% \begin{tabular}{@{}lllll@{}}
% \toprule
% & precision    & recall         & accuracy              & f1         \\ \midrule
% % AR           &  &  &  &  \\ 
% % LSTNet        &  &  &  &  \\ 
% {StemGNN}       &0.59  &0.76  &0.64  &0.67  \\ 
% {STGCN}         &  &  &  &  \\ 
% {SSTGNN}        &  &  &  &  \\ 
% {Informer}        &  &  &  &  \\ 
% {TAMP-S2GCNets} &  &  &  &  \\ 
% % {STSGCN}        &  &  &  &  \\ 
% HCGNN          &0.82  &0.94  &0.81  &0.79  \\ 
% \bottomrule
% \end{tabular}
% \caption{Change point prediction}
% \end{table}

\subsubsection{Performance}
Table \ref{tab:performance_comparison} illustrates the comparisons of our method with the baseline methods. We see that HCGNN has an obvious advantage on modeling the high-frequency time series data. It achieve the best MAE, RMSE, and MAPE on 1 minute and 15 minutes predictions.

\subsubsection{Visualizations}
Figure \ref{fig:visualizationPrice1} visualizes predicted prices of twelve representative future varieties. The real values and predictions are plotted in pink and blue respectively. We observe that the proposed method has strong ability of predicting future price, especially on capturing the significant events.
% \textcolor{red}{Based on the forecasting results of our randomly selected futures varieties, it is easy to see the strong ability of the model proposed in the paper to forecast, while Figure \ref{fig:visualizationPrice1} also shows a high consistency between the trend of the ground truth and forecast prices}.

We also conduct change-point detection to estimate the locations of the events to provide early warning. 
Part of the results of the change-point detection are shown in Figure \ref{fig:visulization_change_points}. The red vertical lines are real change-points and the green ones are the predictions. The line was colored in both red and blue suggesting a correct prediction. We see that the proposed approach can capture most of the important change-points.
\begin{figure*}[!t]
    \centering      \includegraphics[width=0.95\textwidth]{Figures/cpd8.png}
    \caption{
    % Visualizations of change-points plotted by time slot index on the horizontal axis and close prices on the vertical. The pink curve denotes the price fluctuation, and the red and green dotted lines represent the real change-points and predicted change-points respectively.
    Change-point classification results. The pink curves are the price fluctuation.
    The change-points of ground truth and predictions are plotted in red and green dotted lines respectively.}
    \label{fig:visulization_change_points}
\end{figure*}

% \subsubsection{visualization of gap chart}
The bar charts shown in Figure \ref{fig:visualization_gap} visualize the dispersion degree (gap) of the price within twenty ticks (20 seconds). We compare the predicted gaps with their corresponding ground truth in 15 minutes. We see that our method can predict the dispersion degree (gap) well, even when there are some big fluctuations at these time slots.
% \textcolor{red}{According to the results shown in the histogram above, it is easy to see that the fluctuation of the dispersion degree (gap) of the ground truth and the predicted prices keep highly consistent. We also combine the prediction results shown in Figure \ref{fig:visualizationPrice1} in this paper, which shows that the proposed model has high accuracy in price prediction.} 

\begin{figure}[!t]
    \centering
    \includegraphics[width=9cm]{Figures/gap.png}
    \caption{Visualizations of gap predictions. Predicted gap and the corresponding ground truth are plotted in blue and brown bars respectively.}
    \label{fig:visualization_gap}
\end{figure}

\begin{table*}[!htp] 
  \centering
  \fontsize{8}{10}\selectfont      
	  \scalebox{1.0}{       
        \begin{tabular}{cccccccccccccccc}
        \toprule
        \multirow{2}{*}{Tasks}&
        \multicolumn{3}{c}{1min}&\multicolumn{3}{c}{15min} &\multicolumn{4}{c}{CPD}\cr
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-11} \cmidrule(lr){12-15}
        &MAE&RMSE&MAPE(\%) &MAE&RMSE& MAPE(\%) & precision &recall&accuracy & F1 \cr
        \midrule
         w/o GAP+CPD+MA  & 13.28  &55.57   & 3.2    &12.97  &42.15  &4.9  &0.74 &0.51 &0.54 &0.66 \cr
         w/o CPD+MA+Forecast  & 13.79  &{\bf 29.2}   & 14    &16.70  &47.39  &0.14 &0.76 &0.90 &0.73 &0.65 \cr         
         w/o GAP+CPD+Forecast  & 16.28  &40.88   & 14    &16.72  &43.66  &0.14 &0.68 &0.90 &0.69 &0.71 \cr
         w/o GAP+MA+Forecast  & 14.84  &43.79   & 0.08    &20.46 &63.95  &0.12  &0.76 &0.90 &0.73 &0.81 \cr
        w/o GAP+CPD  &12.45   &43.07   &0.08    &9.13  &28.40  &0.07  &0.74 &0.66 &0.59 &0.69 \cr
        w/o MA+CPD 	 &10.61   &39.78   &{\bf}0.07    &{\bf 7.09}  &24.11  &0.08  &0.74 &0.90 &0.74 &0.83 \cr
        w/o GAP+MA 	 &22.83  &76.95  &0.22   &18.79   &67.18  &0.11 &{\bf0.82} &{\bf0.93}  &{\bf0.81} &{\bf0.87}\cr
        w/o MA 	     &14.34   &39.45   &0.09    &11.63  &33.59  &{\bf0.011}  &0.72 &0.89 &0.73 &0.80 \cr
		w/o GAP 	 &18.89   &50.84   &0.10    &14.53  &39.47  &0.012  &0.69  &0.87    &0.68 &0.5  \cr
		w/o CPD 	 &{\bf 10.53}   &39.72   &{\bf0.07}    &7.91  &{\bf18.40}  &0.08  &0.78 &0.92 &0.75 &0.83\cr
        w/o MI	&22.87& 87.66 & 13.35 & 26.56&98.42&17.8 &0.58 &0.74 &0.62  &0.64 \cr
        % HCGNN 	 &{\bf 6.48}&{\bf 12.40}&{\bf0.04}&{\bf 13.70}&{\bf 42.15}&{\bf0.1} & & & &\cr
        \bottomrule
        \end{tabular}}
        \caption{Ablation study on price regression and change-point classification.}
        \label{tab:ablation_study}
\end{table*}





\subsection{Ablation Studies}
To better validate the effectiveness of the proposed model, we conduct an ablation study as follows,
\begin{itemize}
\item\textbf{w/o GAP+CPD+MA}: HCGNN without GAP, CPD, and MA task. We train the model only on the single Forecast task.
\item\textbf{w/o CPD+MA+Forecast}: HCGNN without CPD, MA, and Forecast task. We train the model only on the single GAP task.
\item\textbf{w/o GAP+MA+Forecast}: HCGNN without GAP, MA, and Forecast task. We train the model only on the single CPD task.
\item\textbf{w/o GAP+CPD+Forecast}: HCGNN without GAP, CPD, and Forecast task. We train the model only on the single MA task.
\item\textbf{w/o GAP+CPD}: HCGNN without GAP and CPD task. We train the model on forecasting and MA tasks in continuous learning with mutual information.  
\item\textbf{w/o MA+CPD}: HCGNN without MA and CPD. We train the model on forecasting and gap tasks in continuous learning with mutual information.
\item\textbf{w/o GAP+MA}: HCGNN without gap and MA task. We train the model on forecasting and CPD tasks in continuous learning with mutual information.
\item\textbf{w/o MA}: HCGNN without MA task. We train the model on forecasting, gap, and CPD tasks in continuous learning with mutual information.
\item\textbf{w/o GAP}: HCGNN without gap task. We train the model on forecasting, MA, and CPD tasks in continuous learning with mutual information. 
\item\textbf{w/o CPD}: HCGNN without CPD task. We train the model on forecasting, MA, and gap task in continuous learning with mutual information.
\item\textbf{w/o MI}: HCGNN without mutual information. We train the model on all four tasks continuously but replace mutual information with the traditional gradient of loss. 
\end{itemize}

For all the ablation experiments, we only back-propagate gradient of the activated task. Take the setting \textbf{w/o GAP+CPD+MA} as an example, we set the \texttt{requires\_gradient} as \texttt{True} for the price \textbf{Forecast} task while restraining the gradients of GAP, CPD, and MA.


% The results of the ablation study are as shown in Table \ref{tab:ablation_study}.
Based on the ablation experimental design approach described above, we evaluated the various variants described above on the high-frequency trading dataset and present all the experimental results in Table \ref{tab:ablation_study}. Values in bold are the best results of these settings.
We have several observations based on the results of 1-minute and 15-minute: 1) The best performance values appear in almost all the settings, it suggests that the promotion relationships among the four tasks are complex. 
% It is difficult to identify which task could 
2) Most of the best values appear in the setting of more tasks, it suggests that the overall performance get improved when adding more tasks. Together with the results of Table \ref{tab:performance_comparison} (last row), the best performance is achieved when four tasks are engaged simultaneously. 3) Mutual information is more appropriate to define parameter strength than the traditional loss-based methods with the heterogeneous multi-task setting.
 
% the model is used for long-term investment trends (e.g. 15-minute scenarios) or short-term investment trends (e.g. 1-minute or 15-minute scenarios), the predictive power of the model can be greatly improved and the best performance can be achieved for a combination of tasks. In particular, the best performance is achieved when four tasks are engaged simultaneously. 3) Mutual information is more appropriate to define parameter strength than the traditional loss-based methods with the heterogeneous multi-task setting.







% \begin{table}[!htp] 
%   \centering
%   \fontsize{7}{10}\selectfont
%       \caption{Ablation study.}
% 	  \scalebox{1.0}{
%       \label{tab:ablation_study}
%         \begin{tabular}{cccccccccc}
%         \toprule
%         \multirow{2}{*}{Tasks}&
%         \multicolumn{3}{c}{1min}&\multicolumn{3}{c}{15min}\cr
%         \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%         &MAE&RMSE&MAPE&MAE&RMSE&MAPE \cr
%         \midrule
%         w/o GAP+CPD  & 13.79    &  29.20   & 0.14    &16.72     &47.66     &0.001     \cr
%         w/o MA+CPD 	 & 13.79    &  29.20   & 0.14     &16.72     &47.66     &0.001     \cr
%         w/o GAP+MA 	 & 10.34    & 34.16    & 0.0007    &15.66     &40.84     &0.0014     \cr
%         w/o MA 	     & 27.02     & 87.84    & 0.002     &33.79     &96.95     & 0.002    \cr
% 		w/o GAP 	 & 27.02     & 87.84    & 0.002    &33.79     & 96.95    & 0.002    \cr
% 		w/o CPD 	 & 27.02    & 87.84    & 0.002    &33.79     &   96.95  &0.002     \cr
%         w/o MI	&{31.15\bf }&{100.29\bf }&{0.03\bf }&{33.79\bf }&{96.95\bf }&{0.002\bf }\cr
%         \bottomrule
%         \end{tabular}}
% \end{table}





\section{Conclusion and Future Work} 
In this work, we propose a novel deep learning model based graph called Heterogeneous Continual Graph Neural Network (HCGNN). It models spatial-temporal futures data and captures the correlations between different varieties. Meanwhile, it can catch long-term and short-term trends through multi-task continuous learning. Furthermore, we creatively take use of the mutual information maximization mechanism to address the problem of CF in continuous learning. The experiment shows HCGNN outperforms existing approaches consistently in futures time-series forecasting. In future works, we will continue exploring the applicability of HCGNN in stock, bond, forex, option, and other financial markets.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ref}

\onecolumn

\section*{Appendix}
% \lipsum[1-2]
More visualizations of gap predictions are as shown in Figure \ref{fig:gapRegression}.
\begin{figure*}[htbp]
\centering    
\includegraphics[width=\textwidth]{Figures/gap1.png}
\caption{Visualizations of gap predictions on more futures. Predicted gap and the corresponding ground truth are plotted in blue and brown bars respectively.}
\label{fig:gapRegression}
\end{figure*}


% \clearpage
% \begin{@twocolumnfalse}
% \appendix
% \section{More results on gap regression}
% More visualizations of gap predictions are as shown in Figure \ref{fig:gapRegression}.
% \begin{figure*}[htbp]
% \centering    
% \includegraphics[width=\textwidth]{Figures/gap1.png}
% \caption{Visualizations of gap predictions on more futures. Predicted gap and the corresponding ground truth are plotted in blue and brown bars respectively.}
% \label{fig:gapRegression}
% \end{figure*}
% \end{@twocolumnfalse}


\end{document}

