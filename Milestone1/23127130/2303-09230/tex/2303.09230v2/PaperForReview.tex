% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
%\documentclass[10pt,twocolumn,letterpaper]{IEEEtran}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}

\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{color}
\usepackage[accsupp]{axessibility}



\makeatletter
\def\hlinewd#1{%
  \noalign{\ifnum0=`}\fi\hrule \@height #1 \futurelet
   \reserved@a\@xhline}
\makeatother

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\def\tb{\textbf}
\newcommand{\revise}[1]{{\color{blue}{#1}}}
%\newcommand{\revise}[1]{{{#1}}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
%\def\cvprPaperID{4105} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}



\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Towards a Smaller Student: Capacity Dynamic  Distillation \\for Efficient Image Retrieval}

% \author{\IEEEauthorblockN{Yi Xie \textsuperscript{1} \quad 
% 		Huaidong Zhang \textsuperscript{1}\thanks{Corresponding author} 
%          \quad Xuemiao Xu \textsuperscript{1} 
%          \quad Jianqing Zhu  \textsuperscript{2} 
%          \quad Shengfeng He \textsuperscript{3}} \\
% 	\IEEEauthorblockA{
% 		\textsuperscript{1} South China University of Technology \quad
% 		\textsuperscript{2} Huaqiao University \quad
%   \textsuperscript{3} Singapore Management University}\\
% 	\IEEEauthorblockA{\tt\small ftyixie@mail.scut.edu.cn, \{huaidongz, xuemx\}@scut.edu.cn \\
% \tt\small jqzhu@hqu.edu.cn, shengfenghe@smu.edu.sg.}}


\author{%
Yi Xie$^{1}$ \quad Huaidong Zhang$^{1}$\thanks{Corresponding authors} \quad Xuemiao Xu$^{1,4,5,6*}$ \quad Jianqing Zhu$^{2}$  \quad Shengfeng He$^{3}$\\
$^{1}$South China University of Technology \; $^{2}$Huaqiao University \; $^{3}$Singapore Management University\\
$^{4}$State Key Laboratory of Subtropical Building Science \\ $^{5}$Ministry of Education Key Laboratory of Big Data and Intelligent Robot\\ $^{6}$Guangdong Provincial Key Lab of Computational Intelligence and Cyberspace Information\\
\tt\small ftyixie@mail.scut.edu.cn, \{huaidongz, xuemx\}@scut.edu.cn \\
 \tt\small jqzhu@hqu.edu.cn, shengfenghe@smu.edu.sg
}


\maketitle
%%%%%%%%% ABSTRACT
% \begin{abstract}
% Previous Knowledge Distillation based efficient image retrieval methods employs a lightweight network as the student model for fast inference. However, the lightweight student model lacks adequate representation capacity for effective knowledge imitation during the most critical early training period, causing final performance degeneration. To tackle this issue, we propose a Capacity Dynamic Distillation framework, which constructs a student model with editable representation capacity. Specifically, the employed student model is initially a heavy model to fruitfully learn distilled knowledge in the early training epochs, and the student model is gradually compressed during the training.
% To dynamically adjust the model capacity, our dynamic framework inserts a learnable convolutional layer within each residual block in the student model as the channel importance indicator. The indicator is optimized simultaneously by the image retrieval loss and the compression loss, and a retrieval-guided gradient resetting mechanism is proposed to release the gradient conflict. Extensive experiments show that our method has superior inference speed and accuracy, e.g., on the VeRi-776 dataset, given the ResNet101 as a teacher, our method saves 67.13\% model parameters and 65.67\% FLOPs (around 24.13\% and 21.94\% higher than state-of-the-arts) without sacrificing accuracy (around 2.11\% mAP higher than state-of-the-arts).
% \end{abstract}

\begin{abstract}
Previous Knowledge Distillation based efficient image retrieval methods employs a lightweight network as the student model for fast inference. However, the lightweight student model lacks adequate representation capacity for effective knowledge imitation during the most critical early training period, causing final performance degeneration. To tackle this issue, we propose a Capacity Dynamic Distillation framework, which constructs a student model with editable representation capacity. Specifically, the employed student model is initially a heavy model to fruitfully learn distilled knowledge in the early training epochs, and the student model is gradually compressed during the training.
To dynamically adjust the model capacity, our dynamic framework inserts a learnable convolutional layer within each residual block in the student model as the channel importance indicator. The indicator is optimized simultaneously by the image retrieval loss and the compression loss, and a retrieval-guided gradient resetting mechanism is proposed to release the gradient conflict. Extensive experiments show that our method has superior inference speed and accuracy, e.g., on the VeRi-776 dataset, given the ResNet101 as a teacher, our method saves 67.13\% model parameters and 65.67\% FLOPs without sacrificing accuracy.
\end{abstract}
\vspace{-4mm}


%%%%%%%%% BODY TEXT

\input{s1_intro.tex}
\input{s2_related.tex}
\input{s3_method.tex}
\input{s4_experiment.tex}

\section{Conclusion}\label{sec:con}
This paper proposes a Capacity Dynamic Distillation framework (CDD) for efficient image retrieval. Specifically, CDD uses a heavy model as students to fully understand teachers' knowledge in early training.
Simultaneously, the student model is gradually compressed during the training by the DGC module. Furthermore, we propose the RGGR method to release the conflict between the learning gradient and the forgetting gradient. As a result, the heavy student model can be converted into a lightweight model without critical performance degeneration. Extensive experiments demonstrate that our method is superior to many state-of-the-art approaches.

\noindent\tb{Limitation.} Although our method achieves promising results on efficient CNNs, the performance on the transformer network is yet to be validated. In the future, we will extend our method to transformer-based knowledge distillation.

\noindent\tb{Broader Impact.} Our method demonstrates that the end-to-end aggregation of KD and NP helps construct large-capacity student models, which can inspire the community to continue to explore compression methods for large-capacity student models. In addition, our approach can be applied to learn the efficient model on other matching-dependent tasks (e.g., Object re-identification).

\noindent\tb{Acknowledgement.} The work is supported by Guangdong International Technology Coopertation Project (No.2022A0505050009); Key-Area Research and Development Program of Guangdong Province, China ( 2020B010165004,2020B010166003); National Natural Science Foundation of China (No. 61972162); Guangdong Natural Science Foundation (No. 2021A1515012625); Guangzhou Basic and Applied Research Project (No. 202102021074); and Guangdong Natural Science Funds for Distinguished Young Scholar (No. 2023B1515020097).
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
