


	\begin{table*}[tp]
		\caption{Ablation results on In-Shop \cite{inshop} and VeRi776 \cite{veri}.\vspace{-3mm}}\label{tab:as}
		\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
		\renewcommand\arraystretch{1}
		\centering
		\setlength{\tabcolsep}{3.5pt}
		\begin{threeparttable}
			\resizebox{1.0\textwidth}{!}{
				\begin{tabular}{cccccccccc  cccc cccc }
					\hlinewd{1.5pt}
					\multirow{2}{*}{METHOD}
					&\multirow{2}{*}{TEACHER}
					&\multirow{2}{*}{STUDENT}
					&\multicolumn{4}{c}{In-Shop}
					& \multicolumn{4}{c}{VeRi776}\\
					\cline{4-11}
					%&KD
					&&&MP (M)& FLOPs (G)  &mAP(\%)     &R1(\%) &MP (M)& FLOPs (G)  &mAP(\%)     &R1(\%)\\
					\hlinewd{0.8pt}
					Teacher & - &ResNet101    &43.50 &12.99 &\textbf{81.72} &\textbf{95.23} &43.50 &12.99 &80.50 &96.42\\	
					CDD w/o DGC &ResNet101 &ResNet101   &35.89 &10.94  &71.87 &90.45 &39.80 &11.96  &62.14 &93.27\\
					CDD &ResNet101 &ResNet101  &20.14	&6.31 &81.38	&95.01  &18.59	&5.77 &80.36	&96.48\\
					CDD+RGGR  &ResNet101 &ResNet101 &\textbf{14.97} 	&\textbf{4.68}  &81.28	&95.14 &\textbf{14.30}	&\textbf{4.46}  &\textbf{80.67}	&\textbf{96.66}\\
					\hlinewd{1.5pt}
			\end{tabular}}
		\end{threeparttable}
	\end{table*}
	

\section{Experiments}\label{sec:exp}
    To demonstrate the superiority of our CDD+RGGR method, we conduct evaluations on three widely used public image retrieval datasets: In-Shop \cite{inshop}, VeRi776 \cite{veri}, and MSMT17 \cite{msmt17}. In what follows, we briefly introduce datasets and performance metrics. Then, we conduct ablation experiments and compare CDD+RGGR with state-of-the-art methods. we conduct analysis experiments to examine the role of CDD+RGGR comprehensively.
    
    % hyper-parameter analysis experiments to comprehensively examine the role of RGGR and present visual results that highlight the influence of RGGR on DGC's output channel number.



\subsection{Datasets and Performance Metric}
   \textbf{In-Shop Clothes Retrieval (In-Shop)} \cite{inshop} is a commonly used clothes retrieval database, which contains 72,712 images of clothing items belonging to 7,986 categories. The training set includes 3,997 classes with 25,882 images. The query set with 14,218 images of 3,985 classes. The gallery set has 3,985 classes with 12,612 images.

   \textbf{VeRi776} \cite{veri} is a vehicle retrieval dataset. The training set contains 37,746 images of 576 subjects. The query set of 1,678 images of 200 subjects. The gallery set of 11,579 images of the same 200 subjects.
%constructed by 20 cameras in unconstrained traffic scenarios

   \textbf{MSMT17} \cite{msmt17} is the largest pedestrian retrieval database, which contains 126,441 images of 4,101 pedestrian identities. The training set includes 32,621 training images of 1,041 identities. The test set includes 11,659 query images and 82,161 gallery images of 3,060 identities.
%captured by 3 indoor cameras and 12 outdoor cameras

   The \textbf{Cosine} distance between features as the retrieval algorithm, i.e., the more similar the gallery image, the higher the ranking. The mean average precision (mAP) \cite{duke} and rank-1 identification rate (R1) \cite{veri} both are applied to evaluating the retrieval accuracy performance. The number of model parameters (MP) and floating-point of operations (FLOPs) are used to measure the model size, the computational complexity, respectively.

%and the feature extraction time (FET) per image \cite{hybrid}

\subsection{Implementation Details}
   The software tools are Pytorch 1.12 \cite{pytorch}, CUDA 11.6, and python 3.9. The hardware device is one GeForce RTX 3090Ti GPU. The network training configurations are as follows.
   (1) We use ImageNet pre-trained ResNet101 \cite{resnet} as backbones and set the last stride of ResNet101 to 1, as done in \cite{vrkd, mbdl}.
   (2) The teacher network is frozen during students' training.
   (3) The data augmentation includes z-score normalization, random cropping, random erasing \cite{earse}, and random horizontal flip operations, as done in \cite{vrkd}.
   (4) The mini-batch stochastic gradient descent method \cite{alexnet} is used as the optimizer. The mini-batch size is set to 96, including 16 identities, and each identity holds 6 images.
   (5) Setting weight decays as $5\times10^{-4}$ and momentums as 0.9.
   (6) The cosine annealing strategy \cite{cosine_anneal} and linearly warmed strategy \cite{warmup} are applied to adjust learning rates.
   (7) The learning rates are initialized to $1 \times 10^{-3}$, then linearly warmed up to $1 \times 10^{-2}$ in the first 10 epochs, the drop point for learning rates is the 40-th epoch. For MSMT17 \cite{msmt17}, the total training epoch is 120. For In-Shop \cite{inshop} and VeRi776 \cite{veri}, the total training epoch is 100.
   (8) Considering that pedestrians datasets and other datasets have different different aspect ratios, for MSMT17 \cite{msmt17}, the image resolution is set as $320\times 160$. For In-Shop \cite{inshop} and VeRi776 \cite{veri}, the image resolution is set as $256\times256$.

%-------------------------------------------------------------------------

\subsection{Ablation Experiments}
 As shown in Table \ref{tab:as}, we conduct ablation experiments on In-Shop \cite{inshop} and VeRi776 \cite{veri}. Teacher means that we directly use the ResNet101 teacher to evaluate performance. CDD w/o DGC means that the student discards DGC module, causing KD of Eq. \ref{eq:dl} and NP of Eq. \ref{eq:np} to act on the same $3 \times 3$ convolutional layers.

First, compared to Teacher, we can find that CDD w/o DGC slightly wins MP and FLOPs but severely degrades R1 and mAP. Specifically, CDD w/o DGC is inferior to Teacher by 18.36\% mAP and 3.15\% R1 on VeRi776 \cite{veri}. These results show that playing KD and NP on the same convolutional layers can compress networks but is injured for retrieval performance. The fault lies in the potential conflict between KD and NP because KD guides students to inherit more teachers' knowledge, but NP encourages students to forget knowledge. Furthermore, with the usage of DGC, CDD outperforms CDD w/o DGC in terms of all metrics. For example, on VeRi776 \cite{veri}, CDD outperforms CDD w/o DGC by 18.22\% mAP, 3.21\% R1, 21.21M MP, and 6.19G FLOPs. These comparisons demonstrate that DGC can alleviate the conflict between KD and NP.


Second, with using RGGR, the student further improved inference performance without loss of accuracy performance. Compared with CDD, CDD+RGGR saves 25.67\% MP and 25.80\% FLOPs on In-Shop \cite{inshop}, and 23.08\% MP and 22.70\% FLOPs on VeRi776 \cite{veri}. The comparison demonstrates that RGGR can further boost the inference performance of students without sacrificing accuracy.





\begin{table}[tp]
	\caption{Performance comparison on In-Shop.\vspace{-2mm} }.\label{tab:sota_inhop}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\renewcommand\arraystretch{1.0}
	\centering
	\setlength{\tabcolsep}{1pt}
	%	\footnotesize
	\small
	\begin{threeparttable}
		\scalebox{0.96}{
			\setlength{\tabcolsep}{1pt}
			\begin{tabular}{c  c c cccc }
				\hlinewd{1.5pt}
				METHOD \tnote{1}
				&TEACHER
				&STUDENT
				& \begin{tabular}{c}
					MP\\
				\end{tabular}
				& \begin{tabular}{c}
					FLOPs\\
				\end{tabular}
				&\begin{tabular}{c}
					mAP\\
				\end{tabular}
				& \begin{tabular}{c}
					R1\\
				\end{tabular} \\
				\hlinewd{0.8pt}
				Fastretri \cite{fastreid}&\multirow{1}{*}{{-}}&ResNet50 &25.54 &8.13  &-  &91.97\\
				\hlinewd{0.8pt}		
				%KD \cite{kd} $\ast$ &ResNet101& ResNet34 &21.54&7.31 &80.56	&94.66   \\
				CCKD \cite{cckd} $\ast$  &ResNet101& ResNet34 &21.54 &7.31 &80.65 &94.78\\
				  VID \cite{vid} $\ast$  &ResNet101& ResNet34 &21.54 &7.31 &80.61 &94.57 \\
				FT \cite{ft} $\ast$ &ResNet101& ResNet34 &21.54&7.31  &80.51 &94.55 \\

				SP \cite{spkd} $\ast$ &ResNet101& ResNet34 &21.54&7.31  &80.81 &94.89 \\
				AT \cite{at} $\ast$ &ResNet101& ResNet34 &21.54&7.31  &80.85	&94.91 \\
				RKD \cite{rkd} $\ast$ &ResNet101& ResNet34 &21.54 &7.31 &81.03 &94.81 \\
				PKT \cite{pkt} $\ast$  &ResNet101& ResNet34 &21.54 &7.31 &80.59 &94.68 \\
				MBDL \cite{mbdl} $\ast$  &ResNet101& ResNet34 &21.54&7.31  &80.76 &94.71\\
				DKD \cite{dkd} $\ast$  &ResNet101& ResNet34 &21.54 &7.31 &80.93 &94.86\\
                CSKD \cite{csd} $\ast$  &ResNet101& ResNet34 &21.54 &7.31 &80.65 &94.77\\
                KDPE \cite{kdpe} $\ast$  &ResNet101& ResNet34 &21.54 &7.31 &78.99 &94.08\\
				ResRep \cite{resrep}$\ast$  &ResNet101& ResNet101 &16.27  &5.11  & 79.86  &94.68\\
				CDD+RGGR   &ResNet101& ResNet101 &\textbf{14.97} & \textbf{4.68}   & \textbf{81.28} & \textbf{95.14} \\
				\hlinewd{1.5pt}
			\end{tabular}
		}
		\begin{tablenotes}
			\footnotesize
			\item[1]{The $\ast$ represents the result is re-implemented.}
		\end{tablenotes}
	\end{threeparttable}
	\vspace{-2mm}
\end{table}



\begin{table}[tp]
\caption{Performance comparison on VeRi776.\vspace{-2mm}}\label{tab:sota_veri776}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\renewcommand\arraystretch{1.0}
\centering
\setlength{\tabcolsep}{3.5pt}
%	\footnotesize
\small
\begin{threeparttable}
	\scalebox{0.96}{
		\setlength{\tabcolsep}{1pt}
		\begin{tabular}{c cc cccc }
			\hlinewd{1.5pt}
			METHOD \tnote{1}
			&TEACHER
			&STUDENT
			&MP& FLOPs  &mAP     &R1\\
			\hlinewd{0.8pt}
			PVEN \cite{pven} &\multirow{2}{*}{{-}} &ResNet50  &25.54&8.13 &79.50 &95.60 \\
			ViT-Base \cite{vit} $\ast$  & &ViT \cite{vit} &86.7& 55.6 &78.92 &95.84\\
			%GiT-Base \cite{git} $\ast$ &\XSolidBrush&\XSolidBrush&  &ViT \cite{vit}  &92.1 &55.7 &80.34 &96.86\\
			\hlinewd{0.8pt}	
			%KD \cite{kd} $\ast$  &ResNet101 & ResNet34  &21.54&7.31  &76.42	 &95.47\\
			%KD-FitNet \cite{fitnets} $\ast$  &R101& R34    &21.54&7.31  &76.87&95.41\\
			CCKD \cite{cckd}$\ast$   &ResNet101 & ResNet34  &21.54 &7.31 &76.34 &95.23\\
			VID \cite{vid} $\ast$  &ResNet101 & ResNet34 &21.54 &7.31 & 76.92    &95.35\\
			FT \cite{ft} $\ast$   &ResNet101 & ResNet34 &21.54&7.31  &77.18 &95.77\\
			AT \cite{at} $\ast$   &ResNet101 & ResNet34 &21.54&7.31  &77.98  &95.41\\
			SP \cite{spkd} $\ast$  &ResNet101 & ResNet34 &21.54&7.31  &77.50 &95.77  \\
			RKD \cite{rkd} $\ast$   &ResNet101 & ResNet34 &21.54&7.31 &78.56 &95.65\\
			PKT \cite{pkt} $\ast$   &ResNet101 & ResNet34 &21.54 &7.31 &78.45  & 95.47\\
			MBDL \cite{mbdl} $\ast$   &ResNet101 & ResNet34   &21.54&7.31  & 77.91&95.53\\
			DKD  \cite{dkd} $\ast$   &ResNet101 & ResNet34   &21.54&7.31 &76.40 &95.59\\
            CSKD \cite{csd} $\ast$   &ResNet101 & ResNet34 &21.54 &7.31 &73.60 &94.34\\
            KDPE \cite{kdpe} $\ast$   &ResNet101 & ResNet34 &21.54 &7.31 &74.27 &94.93\\
			ResRep \cite{resrep} $\ast$   &ResNet101 & ResNet101 & 15.87 &4.87   &77.60  &95.11 \\
			CDD+RGGR& ResNet101 & ResNet101 &\textbf{14.30}	&\textbf{4.46}  &\textbf{80.67} &\textbf{96.66}\\
			
			\hlinewd{0.8pt}
			UMTS \cite{umts} &ResNet50  & ResNet50   &25.54&8.13  &75.9& 95.8\\
			VKD \cite{mvkd}  &ResNet50   & ResNet50  &25.54&8.13  &\textbf{79.17}&95.23\\
			ResRep \cite{resrep} $\ast$ &ResNet50 & ResNet50 &11.24 &3.83 &75.74 &94.66 \\
			CDD+RGGR &ResNet50  &ResNet50 &\textbf{9.33}&\textbf{3.20} &78.75 &\textbf{95.95}\\

			\hlinewd{1.5pt}
		\end{tabular}
		  }
	\begin{tablenotes}
		\footnotesize
		\item[1]{The $\ast$ represents the result is re-implemented.}
	\end{tablenotes}
\end{threeparttable}
\end{table}




\subsection{Comparison with State-of-the-art Methods}
Table \ref{tab:sota_inhop}, \ref{tab:sota_veri776} and \ref{tab:sota_msmt17} summarize comparisons on In-Shop \cite{inshop},  VeRi-776 \cite{veri}, and  MSMT17 \cite{msmt17} datasets, respectively. Here, both MP (M), FLOPs (G), mAP (\%), R1 (\%) are obtained by only using the student during inference. For a fair comparison, Hintonâ€™s original Kullback-Leibler divergence strategy \cite{kd} is applied to all compared re-implemented KD methods, as done in previous works \cite{cckd, mybmvc, mvkd}. 
Moreover, an advanced NP method (i.e., ResRep \cite{resrep}) was also re-implemented to compare with our method. The comparison analyses are discussed as follows.
% Besides, the backbone network is evaluated to estimate the inference performance of non-compressed methods.



\begin{table}[tp]
	\caption{Performance comparison on MSMT17.\vspace{-2mm}}\label{tab:sota_msmt17}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\renewcommand\arraystretch{1}
	\centering
	\setlength{\tabcolsep}{3.5pt}
	%	\footnotesize
	\small
	\begin{threeparttable}
	\scalebox{1}{
		\setlength{\tabcolsep}{1pt}
		\begin{tabular}{c  c c cccc c}
			\hlinewd{1.5pt}
			METHOD \tnote{1}
			&TEACHER
			&STUDENT
			&MP& FLOPs  &mAP    & R1\\
			\hlinewd{0.8pt}
			IANet \cite{ianet} &\multirow{2}{*}{{-}}  &ResNet50 &25.54 &4.07  &46.8  &75.5\\
			BINet \cite{binet}   &  &ResNet50 & 25.54 &4.07 & 52.8 & 76.1 \\
			
			\hlinewd{0.8pt}
            CCKD \cite{cckd}$\ast$  &ResNet101 & ResNet34  &21.54 &5.71 &56.65 &80.58\\
            VID \cite{vid} $\ast$  &ResNet101 & ResNet34 &21.54 &5.71 &56.59 &80.06 \\
            FT \cite{ft} $\ast$  &ResNet101 & ResNet34 &21.54&5.71  &56.67 &80.38 \\
            AT \cite{at} $\ast$  &ResNet101 & ResNet34 &21.54&5.71  &58.70 &81.62 \\
            SP \cite{spkd} $\ast$  &ResNet101 & ResNet34 &21.54&5.71  &57.32 &80.37 \\
            RKD \cite{rkd} $\ast$  &ResNet101 & ResNet34 &21.54&5.71 &57.97 &81.02\\
            PKT \cite{pkt} $\ast$  &ResNet101 & ResNet34 &21.54 &5.71 &57.02 &80.31\\
			MBDL \cite{mbdl} $\ast$ &ResNet101 & ResNet34   &21.54&5.71 &57.18 &80.50\\
            DKD \cite{dkd} $\ast$ &ResNet101 & ResNet34  &21.54&5.71 &56.95 &80.63\\
            CSKD \cite{csd} $\ast$  &ResNet101 & ResNet34 &21.54 &7.31 &54.89 &79.17\\
            KDPE \cite{kdpe} $\ast$  &ResNet101 & ResNet34 &21.54 &7.31 &52.45 &78.18\\
            ResRep \cite{resrep} $\ast$ &ResNet101 &ResNet101 &16.49 &3.94 &52.69 &75.63\\
            CDD+RGGR  &ResNet101  &ResNet101 &\textbf{15.00} &\textbf{3.66} &\textbf{60.98} &\textbf{81.68}   \\
			\hlinewd{1.5pt}
		\end{tabular}
	}
	\begin{tablenotes}
		\footnotesize
		\item[1]{The $\ast$ represents the result is re-implemented.}
	\end{tablenotes}
	\end{threeparttable}
%	\vspace{-.5cm}
\end{table}


\tb{In-Shop dataset.}
Table \ref{tab:sota_inhop} shows that CDD+RGGR has several significant advantages over other compression methods. Firstly, when using the same ResNet101 teacher, CDD+RGGR achieves the highest performance across all metrics compared to other KD methods. Specifically, regarding inference performance, CDD+RGGR outperforms these methods by 5.27M MP and 2.63G FLOPs. Secondly, compared to ResRep \cite{resrep}, CDD+RGGR achieves a higher accuracy performance of 1.42\% mAP and 0.46\% R1 while maintaining similar inference performance. 
%Overall, CDD+RGGR is state-of-the-art on the In-Shop dataset \cite{inshop}.


  \begin{figure*}[tp]
	\centering
	\includegraphics[width=1\linewidth]{p.pdf}\vspace{-1mm}
	\caption{The influences of $p$ values. (a) on mAP. (b) on MP. (c) on FLOPs. As $p$ increases, the knowledge accumulation gradient of more channels is zeroed, resulting in a slight decrease in mAP and a noticeable improvement in inference performance.}\label{fig:p}
	\vspace{-2mm}
\end{figure*}

\begin{figure}[tp]
	\centering
	\includegraphics[width=0.8\linewidth]{kmap.pdf} \vspace{-1mm}
	\caption{The $K$ influence on mAP.}\label{fig:K}
	\vspace{-2mm}
\end{figure}





\tb{VeRi776 dataset.}
   From Table  \ref{tab:sota_veri776}, the compressed method CDD+RGGR outperforms non-compressed methods like PVEN \cite{pven} and ViT \cite{vit}, which use complex backbone networks. Compared with other compressed methods that use the same ResNet101 teacher, CDD+RGGR can achieve higher accuracy with a lighter and more efficient student model with 14.30M MP and 4.46G FLOPs, along with 80.67\% mAP and 96.66\% R1. Specifically, regarding mAP accuracy, CDD+RGGR outperforms RKD \cite{rkd} by 2.11\%, PKT \cite{pkt} by 2.22\%, and all other competitors by a significant margin while using only 61.01\% FLOPs of other KD methods.
   Besides, using ResNet50 as the teacher, CDD+RGGR loses first place in mAP to VKD \cite{mvkd}, a self-distillation method that focuses on improving the accuracy of the original model (i.e., teacher models) itself rather than educating a lightweight student. Nevertheless, our approach is still valuable because CDD+RGGR can save 63.47\% MP and 59.33\% FLOPs with a slight loss in accuracy.



    %These mAP and R1 comparisons may seem unfair due to student networks are not consistent, but please note that our ERHC are compressed to be the smallest one. Our ERHC aims to learn a lightweight student and to maintain accuracy performance. We think it's acceptable to have a significant reduction in computations with a tiny loss of mAP.

	\begin{figure}[tp]
		\centering
		\includegraphics[width=0.8 \linewidth]{channel_number.pdf}\vspace{-1mm}
		\caption{The output channel number of DGC in each residual block on In-shop \cite{inshop}. With RGGR, the output channel number of DGC is significantly reduced, while mAP only drops 0.1\%.}\label{fig:channel_number}
		\vspace{-2mm}
	\end{figure}
	

\tb{MSMT17 dataset.}
  From Table \ref{tab:sota_msmt17}, it is evident that the CDD+RGGR method using the ResNet101 model as the teacher network exhibits significant advantages in terms of accuracy performance metrics when compared to the non-compressed approaches, namely, IANet and BINet. Specifically, CDD+RGGR achieves the highest mAP (i.e., 60.98\%) and the highest R1 (i.e., 81.68\%) among all the methods. Additionally, it is worth noting that CDD+RGGR also outperforms the KD methods by 6.54M MP and 2.05G FLOPs in terms of inference performance, which further underscores its superiority as a compression method. 


\begin{table}[tp]
	\caption{Evaluation of two stage cascaded designs on MSMT17.\vspace{-2mm}}\label{tab:two}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\renewcommand\arraystretch{1}
	\centering
	\setlength{\tabcolsep}{3.5pt}
	%	\footnotesize
	\small
	\begin{threeparttable}
	\scalebox{1}{
		\setlength{\tabcolsep}{1pt}
		\begin{tabular}{c  cccc}
			\hlinewd{1.5pt}
			Methods &MP (M) & FLOPs (G)  &mAP (\%)     & R1 (\%)\\
			\hlinewd{0.8pt}
            ResRep \cite{resrep}  &14.85 &3.59  &51.17 &74.61 \\
            Self-KD + ResRep \cite{resrep} &14.82 &3.59 &56.17 &78.30\\
            % \hline
            % SSL \cite{lasso_prune} &15.11 &3.57  &46.23 &70.89\\
            % Self-KD + SSL \cite{lasso_prune} &14.84 &3.56 &51.02 &74.73\\
            % \hline
            CDD + RGGR  &15.00 &3.66 &60.98 &81.68  \\
			\hlinewd{1.5pt}
		\end{tabular}
	}
	
	\end{threeparttable}
\vspace{-.5cm}
\end{table}





\subsection{Analysis Experiments}
\subsubsection{The two stage method \textit{vs} CDD+RGGR} 
To evaluate the effectiveness of our integrated end-to-end KD+NP method, we conducted a comparative study by decomposing our approach into a cascaded setting. Specifically, we employ self-KD to distill a heavy student model, which is pruned to a light student by ResRep \cite{resrep}. Then, the light student is fine-tuned to restore some performance. Notably, in the two-stage cascaded method, our proposed RGGR retrieval-based metric is disabled, and we use the convolutional layer weight metric of ResRep \cite{resrep} instead for stable training. From Table \ref{tab:two}, CDD + RGGR significantly outperforms self-KD + ResRep by 4.18\% mAP and 3.38\% R1, which demonstrates that the integrated end-to-end KD+NP method is superior to the two-stage cascaded method.

 

\subsubsection{Hyper-parameter Analysis}
\noindent \textbf{The top-K retrieval result (i.e. $K$, in Eq. \eqref{eq:rk})} As $K$ value increases, RGGR has more reference information to zeroing the knowledge accumulation gradient of unimportant channels. From Fig. \ref{fig:K}, we can find that
the mAP performance of CDD+RGGR at $K > 1$ outperforms at $K=1$ on Veri776 \cite{veri}. Moreover, at $K > 1$, CDD+RGGR has good robustness on the mAP performance.  

% Besides, please refer to the supplementary material for the influence of $K$ on inference performance.

% The increase in K value in RGGR results in the availability of more reference information that helps zero the learning gradient of unimportant channels. As depicted in Figure 1, the impact of K on CDD+RGGR mAP is quite evident. It is observed that CDD+RGGR exhibits good robustness on mAP, and the mAP value at K > 1 surpasses that at K=1 on Veri776. Additionally, for further details on the impact of K on inference performance, please refer to the supplementary material.




\noindent \textbf{The channel selection mask ratio (i.e., $p (\%)$ in Eq. \eqref{eq:index})} Fig. \ref{fig:p} show that the impact of $p$ values on performance. Specifically, as the value of $p$ increases, the knowledge accumulation gradient of more channels becomes zero, leading to a slight decrease in mAP performance but an increase in inference performance for CDD+RGGR. For instance, increasing the $p$ value from $40\%$ to $70\%$ on In-shop \cite{inshop}, the mAP performance is reduced by 0.27\%. In contrast, the FLOPs performance improved from 4.9 G to 4.3 G.


\subsection{Qualitative Results}
Fig. \ref{fig:channel_number} shows the output channel number of slim DGC in each residual block on In-shop \cite{inshop}. We can find that RGGR can effectively sparse DGC and thus significantly reduce the output channel number. For example, in the $32$-th residual block, RGGR can reduce the output channel number of DGC from 241 to 1.



%Extensive experiments show that our method has superior inference speed and accuracy, e.g., on the VeRi-776 dataset, given the ResNet101 as a teacher, our method saves 67.13\% model parameters and 65.67\% FLOPs (around 24.13\% and 21.94\% higher than state-of-the-arts) without sacrificing accuracy(around 2.11\% mAP higher than state-of-the-arts).
