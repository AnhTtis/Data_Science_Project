\begin{figure}[tp]
    \centering
    \includegraphics[width=1.0\linewidth]{framework.pdf}\vspace{-2mm}
    \caption{The capacity dynamic distillation framework. The high-capacity student can comprehensively understand teachers' knowledge in the early KD iterations. After training, the student can be converted into a light network to acquire a fast inference speed.}\label{fig:rc}
    \vspace{-4mm}
\end{figure}


\section{Method}\label{sec:app}
This section introduces the proposed CDD framework for efficient image retrieval. Unlike existing KD methods \cite{dkr, dkd} training a student model with fixed representation capacity, we present a new distillation pipeline for a dynamic representational capacity student model. The proposed distillation strategy employs a heavy student network to learn transferred knowledge effectively from teachers while gradually compressing the studentâ€™s capacity for obtaining a final model with fast inference speed. In our CCD framework, the DGC module is integrated to achieve dynamic representation compression during KD learning, as described in Section ~\ref{sec:31}.

Furthermore, we propose the RGGR mechanism to compress the student model by releasing the conflict between the retrieval-related losses and the compression loss. RGGR first evaluates the channel importance of students with respect to image retrieval. Then, the gradients derived from image retrieval-related losses are zeroed to eliminate the conflict in the final objective. This approach ensures that the weighting values of selected channels are solely optimized by the compression loss, enabling them to converge to zero more efficiently. The RGGR method will be discussed in detail in Section~\ref{sec:32}.



\subsection{Dynamic Representation Distillation}\label{sec:31}
The proposed CDD is illustrated in Fig. \ref{fig:rc}.
Similar to the typical KD methods, CCD is designed to transfer knowledge from a well-trained teacher model $P_{t}(y|x,\theta_{t})$ to an untrained student model $P_{s}(y|x,\theta_{s})$, where $x$ is the model input and $\theta$ denotes the model parameters.
Different from the previous methods employing models following the rule $|\theta_{s}|<|\theta_{t}|$ for model compression, CDD employs the student and the teacher satisfying $|\theta_{s}|\ge |\theta_{t}|$ for effective early learning, as discussed in Fig. ~\ref{fig:map}.

\subsubsection{Dynamic Representation Learning}


\tb{Distillation Learning.} From Fig. \ref{fig:rc}, we employ ResNet~ \cite{resnet} with cascaded residual blocks as an example.
During training, we consider globally and locally consistent distillation to transfer knowledge from teachers to students.

For global-consistent distillation, we use the Kullback-Leibler divergence (KL) loss $L_{kl}$ \cite{kd} to transfer logit knowledge, as done in
previous KD-based image retrieval works \cite{cckd, vrkd, mvkd}. For local-consistent distillation, different from layer-wise alignment adopted in previous methods, thanks to the large capacity student has the same network depth as teachers, we perform a block-wise alignment by focusing on the $3\times 3$ convolutional layers of each residual block to more completely transfers knowledge as follows:
\begin{equation}\label{eq:dl}
    L_{dl}(f^{t}, f^{s}) = \frac{1}{M}\sum \nolimits_{i=1}^{M}{\rm{DL}}\big({\rm{GAP}}(f^{t}), {\rm{GAP}}(f^{s})\big),
\end{equation}
where $f^{t}$ is the feature map output by the $3 \times 3$ convolutional layer of teachers and $f^{s}$ is the corresponding feature map from students; $M$ is the number of residual blocks, such as ResNet101 contains 33 residual blocks; $\rm{DL(\cdot,\cdot)}$ denotes Euclidean distance loss; The $\rm{GAP(\cdot)}$ denotes the global average pool layer to compress feature maps spatially.
%Notably, unlike previous KD works \cite{at,mvkd, mgd} that transfer feature knowledge in the residual layer, we can transfer feature knowledge at more subtle network structures (i.e., residual block). Because the residual block features channel of DRS is consistent with the teacher network, avoiding the harmful problem introduced by feature adaptive layers \cite{mgd}.}





\tb{Distillation Guided Compactor (DGC).}
To compress the student model dynamically during the training period, we add a proposed DGC module after each $3 \times 3$ convolutional layer of students.
Specifically, the DGC module here is a $1 \times 1$ convolutional layer without bias being initialized as an identity matrix, which is used to evaluate the importance of each channel of the $3 \times 3$ convolutional layer.
In this way, we can sparse the $1 \times 1$ convolutional layer during the training period. After training, the $3 \times 3$ convolutional layer can be pruned by merging these two layers. The merging operation is introduced in Section~\ref{sec:inference}.

To sparse the matrix, we implement G-LASSO loss $L_{np}$ \cite{group_lasso, resrep} on DGC to gradually decrease the student network's representational capacity as follows:
\begin{equation}\label{eq:np}
    L_{np}(\mathcal{W}^{c}) = \sum \nolimits_{i=1}^{D} \sqrt{\sum \nolimits_{j=1}^{D} (\mathcal{W}_{\scriptscriptstyle i,\scriptscriptstyle j, \scriptscriptstyle 1, \scriptscriptstyle 1}^{c})^2},
    \end{equation}
where $\mathcal{W}^{c}\in \mathbb{R}^{D \times D \times 1 \times 1}$ represents kernel parameters of DGC modules. The first and second dimensions of $\mathcal{W}^{c}$ represent the output and input channels, respectively.
%Also, the distillation constraint $L_{dl}$ is mainly learned by the $3 \times 3$ convolution with large representational capacity, and the sparsity constraint $L_{np}$  is only employed over the $1 \times 1$ convolution.

Finally, the student's overall loss $L_{all}$ includes $L_{dl}$ presented in Eq.~\ref{eq:dl}, KL loss $L_{kl}$, two widely used image retrieval losses and $L_{np}$ presented in Eq.~\ref{eq:np} as follows:
\begin{equation}\label{eq:total}
L_{all} = \frac{1}{2}L_{dl} + L_{id} + L_{tri} + L_{kl}  + \alpha L_{np},
\end{equation}
where $L_{id}$ is the label smooth regularization cross-entropy \cite{lsr}, as done in \cite{vrkd}; $L_{tri}$ is the triplet loss using hard sample mining strategy~\cite{triplet}; $\alpha$ is a hyper-parameter to control the sparsity level, with a default value of 0.004. 
% Please refer to the supplementary material for the influence of $\alpha$.

\noindent \tb{Discussion.} Intuitively, we argue that the DGC design is better than the previous two-stage ``self-KD+NP'' methods because the sparse process can't severely deviate under the supervision of the teacher network. Furthermore, the DGC design outperforms the previous end-to-end ``KD with NP'' methods because DGC can alleviate the conflict between KD and NP by decoupling their action positions.




\subsubsection{Efficient Inference with Pruning\label{sec:inference}}
After training, the output channel of DGC modules is treated as an unimportant channel and discarded if the output channel weight is less than the pruning threshold $\lambda$. The default value of $\lambda$ is $1 \times 10^{-5}$, as done in \cite{global, resrep}. Thus, the DGC module is trimmed to a slim one $\hat{\mathcal{W}}^{c}\in \mathbb{R}^{E \times D \times 1 \times 1}, E \le D$. Then, the $3 \times 3$ convolutional layer and the slim DGC module in one residual block can be simplified to being a slim $3 \times 3$ convolutional layer as follows:
\begin{equation}\label{eq:rc}
\mathcal{W} = T\big(T(\mathcal{W}') \circledast \mathcal{\hat{W}}^{c}\big), \quad B = B' \circledast \mathcal{\hat{W}}^{c},
\end{equation}
where $\mathcal{W} \in \mathbb{R}^{E \times C \times 3 \times 3} $ and $B \in \mathbb{R}^{E}$ represent the kernel parameters of the slim $3 \times 3$ convolutional layer and its bias, respectively.
$E$ and $C$ represent the output and input channel sizes of the slim $3 \times 3$ convolutional layer, respectively.
$T (\cdot)$ and $\circledast$ denote the transpose function and the convolution operator, respectively.
$\mathcal{W}' \in \mathbb{R}^{D \times C \times 3 \times 3} $ and $B' \in \mathbb{R}^{D}$ represent the kernel parameters of the $3 \times 3$ convolutional layers and its bias.

Finally, the $3 \times 3$ convolutional layers and the $\mathcal{\hat{W}}^{c}$ in each residual block are merged into a slim $3 \times 3$ convolutional layer. Besides, the last bottleneck layers (i.e., $1 \times 1$ convolutional layer) in each residual block also are thinned. For example, for the $1\times1$ convolutional layer after DGC, we can discard those unimportant channels according to the channel value in $\mathcal{\hat{W}}^{c}$. As a result, the heavy student model can be transformed into a lightweight network to acquire a fast inference speed for image retrieval.








%\noindent\textbf{Discussion}.
%
%
%
%Therefore, in later epochs, although the representational ability of DRS is already weaker than the constant weak representational student network in previous work \cite{mgd, dkd}, DRS still acquires a high accuracy performance because DRS obtain a well optimization in early epochs. Besides, DRKD can directly transfer intermediate features knowledge from teachers to DRS without any feature adaptive layer \cite{fitnets, ft, vid, crkd}


\begin{figure*}[tp]
	\centering
	\includegraphics[width=1\linewidth]{final_CRGR.pdf}\vspace{-1mm}
	\caption{The retrieval-guided gradient resetting mechanism (RGGR). For ease of visualization, we assume that the DGC's output channel is 3. RGGR believes that the DGC's first channel has the most negligible influence on retrieval results by the absolute value of the channel product between $f_{s}^{i}$ and $r_{j}^{i}$. Then, RGGR resets the accumulation gradient of the first channel to 0 to push its weight value toward 0.}\label{fig:crgr}
    \vspace{-5mm}
\end{figure*}



\subsection{Retrieval-Guided Gradient Resetting}\label{sec:32}
Although CCD has achieved early learning with large representation capacity and efficient inference by dynamic compression, the optimization conflict remains unsolved. Specifically,  the gradient conflict between the knowledge accumulation gradient from image retrieval loss $L_{acc}=\frac{1}{2}L_{dl} + L_{id} + L_{tri} + L_{kl}$ and the knowledge forgetting gradient from pruning loss $L_{np}$ may cause a low compression rate.
More specifically, most of the output channels' weight of DGC may be updated to be approximated to 0 via $L_{np}$, but never close enough for perfect pruning due to $L_{acc}$~\cite{resrep}.
%Although DRD has been a good balance between inference speed and accuracy performance, it still has room for improvement in inference speed. Specifically, the gradient of DGC supervised by the G-LASSO loss function can be decoupled into the following two gradients \cite{resrep}: (1) The knowledge learning gradient generated by the network's accuracy performance-related objective function (e.g., triplet loss \cite{triplet} for image retrieval), which can improve the network's accuracy performance. (2) The knowledge forgetting gradient generated by the G-LASSO loss function to sparse the network without care for the accuracy degeneration. Thus, there are a gradient conflict between the knowledge learning gradient and the knowledge forgetting gradient, causing cannot achieve high prunability because most of the output channels' weight of DGC merely become closer to 0 than they used to be, but not close enough for perfect pruning \cite{resrep}.

To this end,  as shown in Fig. \ref{fig:crgr}, we propose the Retrieval-Guided Gradient Resetting mechanism (RGGR) to release this gradient conflict. Specifically,
RGGR employs a $D$-dimensional binary mask $M \in \{0, 1\}$ to zero the knowledge accumulation gradient of some DGC's output channels to acquire a resetting gradient $\hat{G}$ as follows:
\begin{equation}\label{eq:m_gradient}
\hat{G}(\mathcal{W}^{c}_i) = \frac{\partial L_{acc}(X, Y, \theta_s)}{\partial \mathcal{W}^{c}_i} M_i + \alpha \frac{\partial L_{np}(X, Y, \theta_s)}{\partial \mathcal{W}^{c}_i},
\end{equation}
where $\mathcal{W}^{c}_i= W^{c}_{i,:,:,:}$ denotes the $\mathcal{W}^c$ matrix's $i$-th output channel; $X$ and $Y$ are data examples and labels respectively.

From Eq. \eqref{eq:m_gradient},  with $M_{i} = 0$, the $i$-th output channel's knowledge accumulation gradient of DGC is reset to zero, causing the first term to be ignored. Thus, $M_i = 0$ can make DGC's $i$-th output channel weight value steadily move towards 0 to achieve high compression.
Intuitively, once the knowledge accumulation gradient of important output channels is zeroed, it would causes a decrease in the retrieval performance of students. Therefore, to maintain the student's retrieval performance, RGGR will zero out the knowledge learning gradient of the output channel that has the most negligible impact on the retrieval results as follows.

\tb{Retrieval Rank Matrix $R$ Formulation.} To evaluate the channel importance of each residual block with respect to image retrieval, we build a query set $\mathit{Q}$ and a gallery set $\mathit{G}$ on each block during training to simulate the retrieval results. Different from using only one batch of teacher features as the gallery set, we construct a large gallery set  $\mathit{G}$ to reveal the overall distribution of the data $X$ adequately.
Specifically, given a batch size of $N$ samples, we extract the teacher feature $\mathcal{F}_t = [f_{\scriptscriptstyle 1}^{t}, f_{\scriptscriptstyle 2}^{t},..., f_{\scriptscriptstyle N}^{t}] \in \mathbb{R}^{N \times D}$ from the $3 \times 3$ convolutional layer and GAP layers to update the gallery $\mathit{G}$.
The gallery set is implemented in a queue with a fixed size $L$, which means the first-in-first-out policy is followed to maintain the queue length, as done in \cite{queue, crkd}.
%Fortunately, our framework does not need to optimize the gallery $\mathit{G}$. Hence, the use of the gallery adds a small amount of video memory overhead.
% It is done by maintaining an instance queue of size $L$ in each residual block for storing output features from the teacher model, as done in \cite{queue, crkd}.

For building the query set $\mathit{Q}$, we should extract the student feature $\mathcal{F}_s = [f_{\scriptscriptstyle 1}^{s}, f_{\scriptscriptstyle 2}^{s},..., f_{\scriptscriptstyle N}^{s}] \in \mathbb{R}^{N \times D}$ from DGC and GAP layers as the query set $\mathit{Q}$. However, $\mathcal{F}_s$ is not invariant and semantic enough for presenting the image information during early training, causing the matching results between $\mathcal{F}_s$ and $\mathcal{F}_t$ to be unreliable. Therefore, we use $\mathcal{F}_t$ instead of $\mathcal{F}_s$ as the query set to retrieve the gallery set $\mathit{G}$.
Formally, we obtain the retrieval rank matrix $R \in \mathbb{R}^{N \times L}$ as follows:
\begin{equation}\label{eq:r}
R = \{r_{\scriptscriptstyle j}^{i} \in \mathbb{R}^{1 \times D} \big| 1\le i \le N, 1\le j \le L \},
\end{equation}
where $r_j^{i}$ represents the $i$-th query feature retrieved from the gallery set $G$ and then returning the $j$-th gallery feature.

\tb{Binary Mask $M$ Formulation based on $R$.}
Since the retrieval rank matrix $R$ is obtained by sorting the retrieval distance matrix, we conclude that $R$ can indicate the output channels with the most negligible impact on the retrieval result.
In addition, to reduce the negative impact of gallery features with low relevance, we select top-K retrieval gallery feature from $R$ as the retrieval result $R_{\scriptscriptstyle K} \in \mathbb{R}^{N \times K}$ to find unimportant output channel as follows:
\begin{equation}\label{eq:rk}
R_{\scriptscriptstyle K} = \{r_{\scriptscriptstyle j}^{i} \in \mathbb{R}^{1 \times D} \big| 1\le i \le N, 1\le j \le K\},
\end{equation}
where $K=2$ empirically.

Thus, the unimportant output channel index $I$ between the query set $Q$ and the retrieval result $R_K$ is as follows:
\begin{equation}\label{eq:total_index}
I = \mathop{\cap}_{i=1}^{N} \mathop{\cap}_{j=1}^{K} I_{ij},
\end{equation}
where $I_{ij}$ denotes the unimportant output channel index between the $i$-th query feature $f_{\scriptscriptstyle i}^{s}$ and $r_{\scriptscriptstyle j}^{i}$ as follows:
\begin{equation}\label{eq:index}
I_{ij} = \omega\big(A (f_{\scriptscriptstyle i}^{s},r_{\scriptscriptstyle j}^{i})\big), 1\le i \le N, 1\le j \le K,
\end{equation}
where $ \omega(\cdot)$ represents a series of operations, including sorting the importance of the output channels in ascending order, picking the smallest one from the ordered index queue, storing its index, and stopping picking when $p=50\%$ ratios of the index have been selected.
The value of $p$ is chosen empirically. The $A (\cdot)$ is a function that evaluates the importance of DGC modules' each output channel based on the absolute value of the product of $f_{s}^{i}$ and $r_{j}^{i}$ as follows:


% aking the typical cosine metric \cite{hybrid, bcn} as an example, $\mathcal{D}_{imp}(\cdot)$ should reveal the degree of relevance between $f_{s}^{i}$ and $r_{j}^{i}$ in the space of cosine metric $A$:
% \begin{equation}\label{eq:cosine_dist}
% A = \frac{\sum\nolimits_{d=1}^{D} f_{i, d}^{s} r_{j, d}^{i}}{\Vert f_{s}^{i} \Vert \Vert r_{j}^{i} \Vert}, 1 \le  i \le N, 1 \le j \le K,
% \end{equation}
% where $\Vert \cdot \Vert$ represents the L2-norm.

% From Eq. \eqref{eq:cosine_dist}, we can find that the absolute value of the product of the corresponding channel of the two features can measure the importance of the channel. Thus, the $\mathcal{D}_{imp}$ is formulated as follows:
\begin{equation}\label{eq:imp}
A (f_{\scriptscriptstyle i}^{s},r_{\scriptscriptstyle j}^{i})= \big[A_d \big],  A_d= \big|f_{i, d}^{s}\cdot r_{j, d}^{i} \big|,  1 \le d \le D,
\end{equation}
where $f_{i, d}^{s}$  and $r_{j, d}^{i}$ represent the $d$-th channel value of $f_{s}^{i}$ and the $d$-th channel value of $r_{j, d}^{i}$, respectively.

Overall, based on the unimportant output channel index $I$, we update the binary mask $M$ to zero the knowledge accumulation gradient of the unimportant output channel of DGC as follows:
\begin{equation}\label{eq:reset_mask}
M = [M_{i}=0 \big|i \in I,1 \le i \le D].
\end{equation}
% The more algorithm details of Binary Mask $M$ generation please refer to the supplementary material.








