

\section{Related Works}\label{sec:rw}
%There are three key aspects highly related to this paper, namely, knowledge distillation (KD), network pruning (NP), and re-parameterization (Rep). In the following, we will comprehensively review those three aspects.

\tb{Network Pruning.}
Network Pruning (NP) \cite{lasso_prune, bn_prune,l2} aims to obtain a light network by removing unimportant parts from a well-trained yet large-scale network. Recent NP works primarily focus on structured pruning \cite{lasso_prune, bn_prune}, which applies sparsity functions to the convolutional layers of well-trained large models to filter out unimportant channels. However, NP often leads to a reduction in accuracy due to its irreversible sparsification process, which can cause significant damage to the network \cite{l2}. 


\tb{Knowledge Distillation.}
Most KD-based image retrieval works \cite{cckd, mbdl, csd} employ a lightweight model as the student and transfers the knowledge among samples from teachers to guide the student optimization.
Although these works has helped light students improve accuracy, the weak representational capacity hinders further accuracy improvement of students. Therefore, recent KD works \cite{takd, dkr} have paid attention to this situation. However, they only focused on how to transfer knowledge well to facilitate studentsâ€™ understanding of teachers' knowledge without considering enhancing students' representational capacity. 
Although self-distillation methods \cite{pskd, umts, mvkd}  assign a large student the same size as the teacher to better understand the teacher's knowledge, they suffer from large students' high computational cost in the inference phase. To address this issue, a natural solution is to aggregate NP with KD in a two-stage design. Specifically, the two-stage method \cite{cpk,kn,sar} first assign a large student to effectively distilled knowledge from teachers and then the well-trained large student network is pruned to a slim network. However, the two-stage approach may suffer a significant performance loss during the pruning stage since it still faces the challenge of sparsification process deviation. In this paper, we explore knowledge distillation with dynamic capacity compression, performed end-to-end, and the conflict between KD and NP is alleviated by a designed distillation guided compactor (DGC) module.


% For example, NPPM \cite{max} train an extra performance prediction network to predict the accuracy of sub-networks, which demonstrates the use of additional networks can improve the performance of NP.

%For the end-to-end way, NP and KD are simultaneously applied to a student has two advantages. (1) For KD, a heavy student network can be assigned to comprehensively understand teachers' knowledge in early training. Then, the heavy student can be pruned in the inference phase to acquire a high inference speed. (2) For NP, the sparse process of students can't severely deviate under the supervision of well-trained teachers. However, this end-to-end way exists a severe conflict between NP and KD because NP likes to forget knowledge but KD tends to learn more.


\tb{Re-parameterization.}
The re-parameterization (Rep) methods \cite{acnet, repvgg, DBB} construct a sequence of multiple convolutional layers to replace a single convolutional layer of an original network to enhance the feature learning ability of the original network in the training phase. Then, those sequences are simplified to a single convolutional layer to avoid extra computation consumption in the inference phase. Recent, Ding et al. \cite{resrep} proposed a gradient resetting and compactor re-parameterization (ResRep) method, which is the first attempt to apply Rep to NP. Motivated by this, we explore the gradient reseeting technique in knowledge distillation and propose zeroing out the selected channel's gradients according to feature retrieval results.
 %applies Rep to KD to address the problem that the light student has difficulty understanding teachers' knowledge. Besides, different from ResRep \cite{resrep} resetting gradient of the convolutional layer's channel with small weight value, our RGGR to zero out the convolutional layer's channel of according to feature retrieval results.


%  Technically speaking, Rep is not a model compressing method because it complicates a network during training and converts the complicated network into the original version during inference. But, the operation of converting sequences of multiple convolutional layers into a single convolutional layer has excellent potential for model compression. Most recently, Ding et al. \cite{resrep} proposed a gradient resetting and compactor re-parameterization (ResRep) method, which is the first attempt to applies Rep to NP. Furthermore, we attempt applies Rep to KD to address the problem that the light student has difficulty understanding teachers' knowledge.
