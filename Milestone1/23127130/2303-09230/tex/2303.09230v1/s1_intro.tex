

\section{Introduction}
\label{sec:intro}

Image retrieval \cite{smoothap} aims to rank all the instances in a retrieval set based on their relevance to the query image. However, many image retrieval methods \cite{bcn,tits} use heavy networks to acquire a high accuracy, causing a low inference speed and hindering practical applications. As an efficient network compression technology \cite{kd, lasso_prune, dynamic}, knowledge distillation (KD) \cite{kd, dkr, dkd} has been widely validated to be useful for boosting the performance of the lightweight student model by transferring knowledge from a heavy teacher model, which is also applied to accelerate image retrieval, as done in \cite{cckd, vrkd, mbdl}.
% Image retrieval \cite{rdir, quad, csd}, i.e., given a query image, ranks all the instances in a retrieval set according to their relevance to the query. However, many image retrieval methods \cite{bcn,tits,quad} use heavy networks to acquire a high accuracy, causing a low inference speed and hindering practical applications of image retrieval. As an efficient network compression technology, knowledge distillation (KD) \cite{kd, dkr, dkd} has been widely validated to be useful for boosting the performance of the light weight model (student) by transferring knowledge from a heavy teacher network, which is also applied to accelerate image retrieval, as done in \cite{cckd, vrkd, mbdl}.

% , which widely used in various fields. For instance, person retrieval \cite{hybrid,bcn} and vehicle retrieval \cite{jqd, quad}, focuses on searching targets captured by different cameras installed at different locations of a city, which is great potential for smart cities.

%The mAP gain on ResNet50 significantly outperforms that of ResNet18 by 5.5\% mAP.

\begin{figure}[t]
	\centering
	\includegraphics[width=.90\linewidth]{mAP_epoch.pdf}\vspace{-2mm}
	\caption{The mAP (\%) of KD methods in per epoch on VeRi776 \cite{veri}. R101 and R34 denote the ResNet101 and ResNet34 students, respectively. Our CDD+RGGR outperforms other KD methods because the heavy student has more talented in effectively learning distilled knowledge than a light student in the early epochs.}\label{fig:map}
	\vspace{-5mm}
\end{figure}

Previous KD-based image retrieval methods \cite{cckd, vrkd, rpkd} assign a lightweight network as the student model to acquire a fast inference speed.
However, we have an experimental observation that a lightweight student model is less talented in effectively learning distilled knowledge than a heavy student model in the early epochs, leading to final performance degeneration. As shown in Fig. \ref{fig:map}, a heavy ResNet101 (i.e., our) improves its performance faster than the light ones (ResNet34, other methods) in the early epochs (first 20 epochs) of KD optimization.
This finding can be compared to the human learning curve \cite{learning}, where a young kid can only comprehend a small portion of knowledge that is taught \cite{dkr}.
Many studies \cite{early, critical, earlycrop} have shown that critical network learning occurs during early training, determining the final solution of the associated optimization.
Furthermore, Cheng et al. \cite{discard} show that the network usually tries to model various visual concept knowledge in early epochs and then discard unimportant ones in later training.
%Speaking of KD in image retrieval, previous works \cite{cckd, vrkd, rpkd} usually assign a light network as the student network to acquire a friendly inference speed. However,  this introduces a challenge of light students only understanding a small portion of knowledge from the large teachers in early training because the representational capacity of students is much smaller than that of teachers as shown in Fig. \ref{fig:sketch}(a). {\color{red} This process is analogous to human learning curve \cite{learning} where a young kid can only comprehend a small portion of knowledge that is taught \cite{dkr}.} Furthermore, many studies \cite{early, critical, earlycrop, discard} have shown that critical aspects of artificial neural network learning occur during early training, determining the final solution of the associated optimization. For example, Cheng et al. \cite{discard} show that the neural network usually tries to model various visual concept knowledge in early training and then discard unimportant ones in later training. For that, the student's accuracy performance is limited if students only understand a small portion of teachers' knowledge in early training.

% \begin{figure}[tp]
% 	\centering
% 	\includegraphics[width=1.0\linewidth]{sketch.pdf}
% 	\caption{. }\label{fig:sketch}
% \end{figure}

Motivated by the above finding, we propose a new KD framework, \textbf{C}apacity \textbf{D}ynamic \textbf{D}istillation (CDD), to allow dynamic model compression during KD learning. Different from existing KD-based image retrieval methods that configure a lightweight student model, CDD employs a heavy initial network as the student model and thus the student has a high representation capacity for comprehensively understanding teachers' knowledge in the early KD iterations. To acquire a fast inference speed, we design a \textbf{D}istillation \textbf{G}uided \textbf{C}ompactor (DGC) module inserted after the heavy convolutional layer of the student as the channel importance indicator of each convolutional layer. Then, we implement a parametric sparse loss on DGC during KD learning to find the unimportant channel of the heavy convolutional layer, thus gradually reducing the capacity of the student network. The overall training process can be done end-to-end in one KD training period. After training, the sparse DGC will be pruned to a slim DGC, and the slim DGC and previous heavy convolutional layer can convert to a slim convolutional layer. Therefore, the heavy student model will be converted to a lightweight model.

To dynamically edit the student model capacity, DGC is optimized simultaneously by the image retrieval loss and the parametric sparse loss, resulting in a gradient conflict between the knowledge accumulation gradient generated from the image retrieval loss and the knowledge forgetting gradient generated from the parametric sparse loss. To release the gradient conflict, we propose a retrieval-guided gradient resetting mechanism (RGGR), which introduces a binary mask to zero out the knowledge accumulation gradient. Specifically, we first use the train data to simulate the retrieval result. Then, RGGR selects channels with little influence on simulation retrieval results and zeros the knowledge accumulation gradient, achieving a high prunability. As demonstrated in Fig.~\ref{fig:map}, when we activate RGGR (at 21-th epoch), the heavy student model focuses more on compression and suffers transient performance degradation. But, thanks to the well-trained KD optimization of students in the early epochs, the student model finally achieves a good balance between accuracy and inference speed.

%Although some KD works \cite{takd, dkr} have aware that the weak representational capacity student network has difficulty understanding teachers' knowledge, they only focus on how to transfer knowledge well to facilitate students' network understanding of teachers' knowledge without considering enhancing the student's representational capacity. A simple method is KD combined with network pruning (NP) to construct a high representational capacity student to fully understand teachersâ€™ knowledge in early training, and the student is pruned to acquire a fast inference speed in the inference. Unfortunately, KD and NP act on the same convolutional layer of students, causing a severe conflict because the sparsity imposed by NP on the untrained network will destroy the student network's learning process.

%Motivated by the aforementioned questions, in this paper, we first propose a dynamically representation knowledge distillation (DRKD) framework for image retrieval. Distinguished from existing KD methods for image retrieval configure a weak representational capacity student network, our DRKD constructs a dynamically representational capacity student network (DRS), which DRS has a high representational capacity in early training to comprehensively understand teachers' knowledge as shown in Fig .\ref{fig:sketch}(b) and DRS' representational capacity gradually declines in the training process to acquire a fast inference speed. Specifically, given a well-trained large network as the teacher and a untrained large network as the student, we insert a representational capacity compactor (RC) after the convolutional layer of students to construct DRS.

%For that, these lightweight image retrieval methods \cite{cckd, rdir, mbdl, vrkd} still have much room for improvement through enhancing the representational capacity of students to understanding knowledge from teachers.

% {\color{red} There is a natural idea to directly combine KD and NP, which has two advantages. (1) For KD, the representational capacity of students is same as a teacher, which can understanding knowledge from teacher networks. Besides, NP can prune the student network to improve inference speed. (2) For NP, sparse regularization of a student network could be more stable due to a good guidance from the teacher's output in a KD framework. However, there is a risk of the conflict between NP and KD, which is not conducive this natural idea. Although NP aims to encourage the student to forget unimportant knowledge, NP can't distinguish unimportant knowledge in the student's early stages of training. Besides, KD aims to guide the student to inherit more knowledge from a teacher, which is the opposite of NP.}


%
%Fig. \ref{fig:expand} exhibits the KD mean average precision (mAP) gain on ResNet50 \cite{resnet} and ResNet18 \cite{resnet} per epoch of the early training stage, in which the KD mAP (\%) gain on ResNet50 significantly outperforms that of ResNet18 because ResNet50 with strong representational capacity can comprehensively understand teachers' knowledge than ResNet18 with weak representational capacity.
%For example, the KD mAP gain on ResNet18 and ResNet50 are 5.2\% mAP (56.8\% $\rightarrow$ 62.0\%) and 10.7\% mAP (59.9\% $\rightarrow$ 70.6\%) in 15-th epoch, respectively.






%Specifically, in the training phase,  to select some output channels zero out, causing the representational capacity of the dynamically representational capacity student network gradually decrease from the same as that of the teacher and the dynamically representational capacity student network can understand knowledge from teachers in the early training phase. Furthermore, we design a retrieval gradient resetting (RGR) method to clear knowledge learning gradients on unimportant channels to alleviate the competition between knowledge learning gradient and knowledge forgetting gradient to achieve high prunability, in which we regard RC's output channels that have little influence on the retrieval results as unimportant channels. In the inference phase, the pruned RC can be equivalently merged with the previous convolutional layer into a light convolutional layer because the output channel number of the light convolutional layer further boosts inference speed.


The main contributions of this paper are listed as follows:

\begin{itemize}
       \vspace{-2mm}
		\item[(1)] We propose a capacity dynamic distillation framework (CDD) to effectively learn distilled knowledge in the early training epochs.
  \vspace{-1mm}
		\item[(2)] We propose retrieval-guided gradient resetting mechanism (RGGR) to release the gradient conflict between the image retrieval loss and the parametric sparse loss.
		\item[(3)] Extensive experiments demonstrate that our method is superior to state-of-the-art approaches in terms of inference and computations, by a large margin of 24.13\%MP and 21.94\% FLOPs.
\end{itemize}

