\vspace*{-4mm}
\section{Introduction}
\label{sec:intro}


Objects are central in human and computer vision. In the former, they are a fundamental primitive used to decompose the complexity of the visual world into an actionable representation. This abstraction in turn enables higher-level cognitive abilities, such as casual reasoning and planning~\cite{kahneman1992reviewing,spelke2007core}. In computer vision, object detection has achieved remarkable progress~\cite{ren2015faster,carion2020end} and is now an essential component in many applications (\eg, driving, robotics). However, these models require a large amount of manual labels from a fixed vocabulary of categories. Consequently, learning unsupervised, object-centric representations is an important step towards scaling up computer vision to the real world. 

This topic has received renewed attention recently thanks to structured generative networks with iterative inference over a fixed set of variables~\cite{burgess2019monet,greff2019multi,engelcke2019genesis,lin2020space,locatello2020object}. These methods cluster pixels in the feature space of an auto-encoder, exhibiting behavior similar to grouping based on low-level cues, such as color or texture. Hence, they are restricted to toy images with colored geometric shapes on a plain background, and fail on more complex realistic scenes~\cite{bao2022discovering}. 
\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-teaser.pdf}
    \vspace{-20 pt}
    \caption{TRI-PD dataset results: (left) top-10 foreground slot segments produced by our approach; (right) corresponding token representations. Compared to raw images, tokens in our framework present a more structured and compact space for reconstruction.
}
    \label{fig:teaser}
    \vspace{-10pt}
\end{figure}

% should be short: more details in the related work section
Two main types of works attempt to address this shortcoming.
The first family of methods sets out to simplify the grouping problem by introducing more structure into the output space, \eg, reconstructing optical flow~\cite{kipf2021conditional} or depth~\cite{elsayed2022savi++}.
They, however, require supervision, either in the form of known poses~\cite{kipf2021conditional} or ground truth bounding boxes~\cite{elsayed2022savi++}, veering away from the unsupervised goal of object discovery.
In contrast, Bao \etal~\cite{bao2022discovering} resolve the object-background ambiguity by explicitly integrating an unsupervised motion segmentation algorithm~\cite{dave2019towards} into the pipeline, showing substantial progress on realistic scenes.
%
The second main direction to improve object discovery focuses on improving the decoder part of auto-encoding architectures~\cite{singh2022simple,singh2021illiterate}, replacing convolutional decoders with transformers~\cite{vaswani2017attention,ramesh2021zero} combined with discrete variational auto-encoders (DVAE)~\cite{rolfe2016discrete} to reduce memory footprint. These more sophisticated architectures improve performance without additional supervision, including on real-world sequences. However, these methods are evaluated with different protocols (metrics, datasets) and therefore no clear architectural principles have emerged yet for unsupervised object discovery. 

In this work, we introduce a novel architecture, \emph{Motion-guided Tokens (\methodname)}, based on the combination of two fundamental structural principles: motion and discretization. We define \emph{objects as discrete entities that might have an independent motion}. As prior works have shown encouraging results thanks to unsupervised motion guidance and better transformer-based decoders, we propose to \emph{leverage motion to guide tokenization}, the vector quantization process at the heart of attention mechanisms in transformer architectures (See Figure~\ref{fig:teaser}). In addition, to comprehensively evaluate the contributions of prior works, we ablate key design choices proposed in the past, such as the decoder architecture and reconstruction space, in a unified framework.

Our key contributions are as follows.
%
(1) We introduce a novel auto-encoder architecture, \emph{\methodname}, for unsupervised video object discovery with a new transformer decoder leveraging \emph{unsupervised motion-guided tokenization}.
%
(2) Our results on real and synthetic datasets show that with sufficient capacity in the decoder, motion guidance alleviates the need for labels, optical flow, or depth decoding thanks to tokenization, improving upon the state of the art.
%
(3) We show that our motion-guided tokens map to interpretable mid-level features, going beyond typical clusters of low-level features, thus explaining why our method scales to challenging realistic videos. 
Our code, models, and synthetic data are made available at \url{https://github.com/zpbao/MoTok/}.
