In this appendix, we provide additional experimental results, visualizations, and implementation details that were not included in the main paper due to space limitations. We begin by showing more visualizations of our models on TRI-PD~\cite{bao2022discovering} and KITTI~\cite{geiger2012we}, including an additional sample video in Section~\ref{suppsec:visual}. The full results of the architecture analysis (Table 1 in the main paper) are provided in Section~\ref{suppsec:architecture}. In Section~\ref{suppsec:experiments}, we show additional experimental evaluations including comparisons to a most recent approach, comparisons to the state-of-the-art on MOVi-C~\cite{greff2022kubric} dataset, per-frame evaluation on MOVi-E, and comparison to STEVE equipped with a perceiver decoder. Finally, we provide further implementation details in Section~\ref{suppsec:implementation}.




% \section{Object Discovery Videos}



\section{Additional Visualizations}
\label{suppsec:visual}

In this section, we provide more visualizations of the proposed MoToK framework on the realistic datasets -- TRI-PD~\cite{bao2022discovering} and KITTI~\cite{geiger2012we}. Full samples from the test set of TRI-PD can be found in the supplementary video, where we use the model trained with ground-truth motion cues to render the slot and token visualizations.

\subsection{TRI-PD}

We show more visualizations of the discovered object masks and corresponding tokens for our model trained with both ground-truth motion segmentation (upper) and estimated using~\cite{dave2019towards} (bottom) in Figure~\ref{suppfig:pd}. Differently from the main paper, here we show the visualizations for all the slots but discard any slots that capture more than 20\% of the pixels in a frame (usually background). When trained using ground-truth motion cues, our model captures accurate object masks and demonstrates strong temporal consistency; we still maintain good temporal consistency with estimated motion but the accuracy around the object boundaries drops. 

We also find that token representations group pixels into mid-level regions based on texture, color, and location. These structured mid-level representations simplify the object discovery problem in our framework compared to grouping in the raw RGB space.


\begin{table}[t]
 \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c}
    Motion & Space & Decoder & FG.~ARI \\ \hline
    \xmark & VQ & Linear & 14.6 \\
    \xmark & VQ & Linear-CNN & 16.3 \\
    \xmark & VQ & Transformer & 40.1 \\
    \xmark & VQ & Perceiver & {\bf 52.4} \\ \hline
    \xmark & RGB & Linear & 12.9 \\
    \xmark & RGB & Linear-CNN & 11.7 \\
    \xmark & RGB & Transformer & 38.4\\
    \xmark & RGB & Perceiver & 49.2 \\ \hline
    \xmark & Flow & Linear & 8.5 \\
    \xmark & Flow & CNN & 9.1 \\
    \xmark & Flow & Transformer & 35.7 \\
    \xmark & Flow & Perceiver & 36.3 \\\hline
    \xmark & Depth & Linear & 7.8 \\
    \xmark & Depth & Linear-CNN & 7.6 \\
    \xmark & Depth & Linear-Transformer & 13.2\\
    \xmark & Depth & Perceiver & 18.4 \\ \hline
    \xmark & FLow + Depth & Perceiver & 38.0 \\
    \xmark & VQ (flow) & Perceiver & 44.6 \\ \hline
    \checkmark & VQ & Perceiver & \bf 63.8 \\
    \end{tabular}
    }
    \caption{Full model architecture analysis with different choices of slot decoders and reconstruction space on MOVi-E. Perceiver decoder + VQ reconstruction space yields the best performance, indicating that (1) the capacity of the decoder plays a key role in the object discovery; (2) learnable, vector-quantized reconstruction space outperforms fixed alternatives like depth of flow.}
    \label{suptab:architecture}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width = \linewidth]{figures/supp-fig-pd.pdf}
    \caption{Slot and token visualizations of the MoTok model on TRI-PD dataset. Upper: model trained with ground-truth motion cues; bottom: model trained with estimated motion cues. Our model captures accurate object masks and achieves strong temporal consistency. Token representations simplify the grouping task in the reconstruction space by grouping pixels into mid-level regions based on textures, color, and location.}
    \label{suppfig:pd}
\end{figure*}

\subsection{KITTI}

We show additional full-resolution samples from the evaluation set of KITTI in Figure~\ref{suppfig:kitti}. Our MoTok model accurately captures both pedestrians and vehicles in the real-world in a self-supervised way. Notably, our model captures the pedestrian in the shadow in the top right, and segments all the cars in the second row, illustrating the robustness of our approach.

However, compared with the results on TRI-PD (Figure~\ref{suppfig:pd}), we miss some objects and the masks are not as precise. For example, a clearly visible car in the top left is missed, and the van in the top right is under-segmented.  These results demonstrate that object discovery in the real-world is still an open challenge, and further improvements are necessary both in the model architect and in learning objectives.

\begin{figure*}
    \centering
    \includegraphics[width = \linewidth]{figures/supp-fig-kitti.pdf}
    \vspace{-15 pt}
    \caption{Additional visualizations of our MoToK on the evaluation set of KITTI. We visualize top-10 slot masks and discard the masks containing more than 20\% of the whole pixels (usually background). Our model successfully discovers both pedestrians (top left) and vehicles (second row). However, the results are not as accurate as those of the synthetic TRI-PD, indicating the challenges of object discovery in the real world.}
    \label{suppfig:kitti}
\end{figure*}

\section{Full Architecture Analysis}
\label{suppsec:architecture}

In Table~\ref{tab:architecture} in the main paper, we separately study the influence of the slot decoder architecture and the reconstruction space. In Table~\ref{suppsec:architecture}, we report the full results with the cross-selection of the slot decoders and reconstruction spaces.

Firstly, by comparing the results of different slot decoders, we find that the simple linear decoder generally fails to work. Adding an additional CNN decoder only brings limited performance gains, sometimes even hurting the performance. Between the two transformer variants, the more advanced perceiver decoder achieves top results, while being more computationally efficient, demonstrating the optimal model design of our MoToK framework. 

Secondly, the learnable VQ space yields the best object discovery performance among the four reconstruction spaces. The underlying reason is that, as shown in Figure~\ref{suppfig:pd}, the VQ space provides a structured reconstruction space, which simplifies the grouping problem. Finally, we observe that, given a powerful enough decoder, RGB space is the most effective of all the fixed reconstruction spaces we studied, and using a more structured space, such as flow and depth, can instead decrease performance. 


\section{Further Experimental Evaluations}
\label{suppsec:experiments}

\subsection{Comparison to Concurrent Work}
We additionally compare our method with a concurrent approach~\cite{seitzer2022bridging}, which conducts object discovery in the ImageNet~\cite{deng2009imagenet} pre-trained feature space of DINO~\cite{caron2021emerging}. For a fair comparison, we use a ViT~\cite{dosovitskiy2020image} backbone for these experiments, but train it from scratch. In addition, following~\cite{seitzer2022bridging} we report per frame FG.~ARI. Moreover, on KITTI, we divide the whole image into 4 patches and use 9 slots for each patch. The results in Table~\ref{supptab:DINOSAUR} demonstrate that MoTok outperforms their approach on both MOVi-E and KITTI dataset even \emph{without} ImageNet pre-training, demonstrating the effectiveness of our proposed motion-guided tokenization. 

\begin{table}[t]
    \centering
    \begin{tabular}{l|cc}
        Model & MOVi-E & KITTI\\ \hline
        DINOSAUR~\cite{seitzer2022bridging} & 65.1 & 70.3 \\
        MoTok & \bf 67.8 & \bf 70.9 \\
    \end{tabular}
    \caption{Per-frame FG.~ARI comparison between DINOSAUR~\cite{seitzer2022bridging} and our MoTok on MOVi-E and KITTI datasets. We still outperform their approach on both datasets even \emph{without} ImageNet pre-training, demonstrating the effectiveness of our proposed motion-guided tokenization. }
    \label{supptab:DINOSAUR}
\end{table}

\subsection{Evaluation on MOVi-C}

We additionally evaluate our method on MOVi-C dataset~\cite{greff2022kubric}. MOVi-C features realistic foreground and background appearance. The maximum number of objects is 10, and there is no camera motion in this dataset. We compare our method with Bao \etal~\cite{bao2022discovering}, SAVI~\cite{kipf2021conditional}, SAVI++~\cite{elsayed2022savi++}, and STEVE~\cite{singh2022simple}, and \textit{MoTok (no motion)} is our model without the motion cues. 
\begin{table}[t]
 \centering
\resizebox{0.6 \linewidth}{!}{
    \begin{tabular}{l|c}
    Model & FG.~ARI \\ \hline
    Bao \etal & 59.5 \\
    SAVi &  35.1 \\
    SAVi++ & 37.4 \\
    STEVE &  49.7 \\ \hline
    MoTok (no motion) & 51.0 \\
    MoTok & \bf 69.3 \\ 
    \end{tabular}
}
\vspace{-4pt}
    \caption{Comparison to the state-of-the-art approaches to object discovery on the validation sets of MOVi-C using Fg.~ARI. Our approach outperforms all the recent methods with the help of motion cues and vector quantization.}
    \label{supptab:movic}
    \vspace{-5pt}
\end{table}

The results are shown in Table~\ref{supptab:movic}. Our model outperforms all the baselines with the help of motion cues and vector quantization. We also have the following observations. Firstly, SAVI and SAVI++, which reconstruct in the flow and depth space, achieve better performance compared with their results on MOVi-E in this scenario (all objects are moving, no camera motion).
These results indicate that, while flow and depth space can help to resolve object/background ambiguity in a simplified environment, they are not sufficient as the setting becomes more realistic. We also see that, in this dataset, motion is indeed a very strong cue as our model achieves a larger improvement from motion cues compared to MOVi-E.



\subsection{Per-Frame Evaluation on MOVi-E}

\begin{table}[t]
 \centering
\resizebox{0.7 \linewidth}{!}{
    \begin{tabular}{l|c}
    Model & per-frame FG.~ARI\\ \hline
    Bao \etal & 55.3 \\
    SAVi & 39.2 \\
    SAVi++ &  41.3 \\
    STEVE &  54.1 \\ 
    Karazija \etal~\cite{karazija2022unsupervised} & 63.1\\\hline
    MoTok (no motion) &  56.6 \\
    MoTok & \bf 66.7 \\ 
    \end{tabular}
}
\vspace{-4pt}
    \caption{Per-frame FG.~ARI evaluation on the validation sets of MOVi-E. Our approach outperforms all the recent state-of-the-art approaches with the help of motion cues and vector quantization.}
    \label{supptab:movie}
    \vspace{-10pt}
\end{table}

To compare to the reported results of~\cite{karazija2022unsupervised}, we evaluate our method on MOVi-E using per-frame FG.~ARI in Table~\ref{supptab:movie}. Our MoTok still outperforms all the recent state-of-the-art approaches with the help of motion cues and vector quantization. Comparing with the results in Table 1 in the main paper, we find that the gap between the per-frame evaluation and video-level evaluation for SAVi and SAVi++ is much larger than for the other models, showing a worse temporal consistency of these methods. These results indicate that using a more powerful decoder is also the key to achieving strong temporal consistency in object discovery.

\subsection{Equipping STEVE with the Perceiver Decoder}

Both STEVE and our model use a discrete vector space in the model design. However, we propose a more powerful perceiver decoder. To reduce the impact of the decoder, we build a baseline \textit{STEVE-p}, for which we replace the transformer decoder with the perceiver but keep the other components unchanged and report the results on MOVi-E in Table~\ref{supptab:steve}.


\begin{table}[t]
\centering
\resizebox{0.6 \linewidth}{!}{
    \begin{tabular}{l|c}
    Model & FG.~ARI\\ \hline
    STEVE  & 51.2 \\
    STEVE-p & 45.8 \\
    \hline
    MoTok (no motion) & \bf 52.4 \\ 
    \end{tabular}
    }
    \vspace{-4pt}
    \caption{Comparison to STEVE with a perceiver decoder backbone. Equipping STEVE with a perceiver decoder instead decreases the original performance. Our MoTok (no motion) still achieves better performance compared to the best variant of STEVE.}
    \label{supptab:steve}
    \vspace{-5pt}
\end{table}

Intriguingly, we find that equipping STEVE with a perceiver decoder decreases the model's performance. We analyzed their model architecture in detail and hypothesize that this is due to the fact that they use learnable DVAE token representations as the query for the transformer. Their DVAE token representations are not optimized by the DVAE reconstruction but \textit{only} optimized through the transformer decoder. Therefore, the self-attention + cross-attention mechanism in the transformer decoder is crucial to learn a rich feature representation, whereas the perceiver decoder only conducts a cross-attention operation with the query. 

In comparison, MoTok optimizes the token representation directly through VQ-VAE reconstruction, using a learned positional embedding as a lightweight query for the transformer/perceiver decoder. Therefore, the perceiver decoder leads to a better performance in the MoTok framework. Besides, our MoTok (no motion) also achieves better performance compared to the best variant of STEVE.

\section{Implementation Details}
\label{suppsec:implementation}

\subsection{MoTok Architecture Details}

We adapt the encoder and the slot attention module from~\cite{bao2022discovering}. Concretely, we first use a ResNet18 as the backbone and reduce the downsampling ratio from 16 to 4 by using stride 1 for all the convolutional blocks except for the first one (but keep the stride 2 for the first convolution \textit{layer}). We further drop the last fully-connected layers of the ResNet to obtain a feature map. We also change the hidden dimensions of the residue block to [64,64,128,128] accordingly. We use a single-layer ConvGRU with a hidden dimension of 128 and the final dimension of 64. 

For slot decoders, we use two convolutional layers for the CNN decoder with kernel sizes 5 and 3. The hidden dimensions are set to 64. For the transformer decoder and the perceiver decoder, we use both single-layer decoders, with a hidden dimension of 64 as well. We use a learnable positional embedding as the query. For the implementation of transformer and perceiver decoders, we refer to the public implementations of the transformer\footnote{\url{ https://github.com/lucidrains/vit-pytorch}} and perceiver\footnote{\url{https://github.com/esceptico/perceiver-io}}. 

For the decoder required for the flow and depth reconstruction space, we use a shallow CNN-based decoder containing 6 transposed convolutional layers. The kernel sizes are 5 except for the last layer, which is 3. The strides for the first and fourth layers are 2, otherwise are one. For the VQ space, we adopt a public VQ-VAE implementation~\footnote{\url{ https://github.com/ritheshkumar95/pytorch-vqvae}}.

We will release our code and models for reproducibility. 


\subsection{Baseline Details}

\smallsec{SAVi / SAVi++~\cite{kipf2021conditional,elsayed2022savi++}} We use the same encoder backbone and slot attention architecture as in MoTok. We also add the corrector module designed by~\cite{kipf2021conditional} to the slot attention union. For the decoder architecture, we also use the same CNN decoder as ours except for the stride design. We change the strides for the first four layers to 2 and the last two layers to 1, leading to an overall up-sample ratio of 16, which is the same as the original design of SAVi and SAVi++.
We did not provide the data augmentation and the first-frame bounding box supervision to the two models. 

\smallsec{STEVE~\cite{singh2022simple}} We implement the STEVE model by referring to their paper and the public released code of \cite{singh2021illiterate}, which is a foundation work of STEVE. We use the same encoder backbone and slot attention architecture as MoTok and use the same DVAE and transformer architecture of the released code.

\smallsec{Bao \etal~\cite{bao2022discovering}} We use the public released code\footnote{
\url{https://github.com/zpbao/Discovery_Obj_Move}} for the implementation. We use the same encoder backbone and slot attention architecture as MoTok.


\smallsec{Estimated Annotation Generation} We generated estimation annotations on TRI-PD and KITTI datasets. For flow estimation, we use self-supervised SMURF flow~\cite{stone2021smurf} for the two datasets. We use a self-supervised GUDA~\cite{tri-packnet} approach to generate the depth annotations for both TRI-PD and KITTI. For the motion segmentation, we use~\cite{dave2019towards}, following the process in~\cite{bao2022discovering}. 


\subsection{Training Details}
All the models are trained for 500 epochs using Adam~\cite{kingma2014adam}. Following~\cite{locatello2020object}, we use a learning rate warm-up for 3000 iterations to achieve the initial learning rate of 0.0005. For the exponential learning rate decay schedule, we set the decay rate as 0.5 and the decay step as 50,000. We set $\lambda_M$ to 1 and $\lambda_T$ to 0.05. The gradients are further clipped to a global norm value of 1, 0.05, and 0.05 for MOVi-E, TRI-PD, and KITTI respectively. We use batch size 64 for the training of MOVi-E, and 8 for TRI-PD and KITTI. We train our model with 4 NVIDIA-A100 GPUs in parallel and it will take one day, three days, and half a day to train the model on MOVi-E, TRI-PD, and KITTI accordingly. We use 128 tokens for TRI-PD and KITTI, and 64 for MOVi. See our code implementation for more details.
