\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Experimental Settings}
\smallsec{Benchmarks.} We evaluate our approach on three popular benchmarks with different complexity (Figure~\ref{fig:data}).

\noindent \textbf{MOVi}~\cite{greff2022kubric} is a synthetic multi-object video dataset, created by simulating rigid body dynamics. We use this benchmark to ablate and analyze our model architecture. Following previous works~\cite{elsayed2022savi++,singh2022simple}, we use the most complex subset, MOVi-E, for evaluation. MOVi-E contains both moving and static objects (maximum 20 objects) with linear random camera motion. The resolution of this dataset is $128 \times 128$ and each video contains 24 frames with a sample rate of 12 FPS. We use standard train and test split for MOVi-E.

\noindent \textbf{TRI-PD}~\cite{bao2022discovering,tri-packnet} is a synthetic dataset based on street driving scenarios collected by using a state-of-the-art synthetic data generation service~\cite{parallel_domain}. The dataset comes with a collection of accurate annotations (\eg~for flow or depth), which we use to ablate the impact of additional information and its quality on various models. Each video in TRI-PD is 10 seconds captured at 20 FPS. There are 924 videos in the training set and 51 videos in the test set. We crop and resize each frame to the resolution of $480 \times 968$. 

\noindent \textbf{KITTI}~\cite{geiger2012we} is a real-world benchmark with city driving scenes. We use the whole 151 KITTI videos for training and the instance segmentation subsets with 200 single-frame images for evaluation. The frames are resized to $368 \times 1248$. 

\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-data.pdf}
    \vspace{-20 pt}
    \caption{Frame samples from the video datasets used in our experiments. MOVi~\cite{greff2022kubric} (left) is a multi-object video dataset created by simulating rigid body dynamics. TRI-PD~\cite{bao2022discovering} (middle) is a collection of photo-realistic, synthetic driving videos. KITTI~\cite{geiger2012we} (right) is a real-world benchmark with city driving scenes.
    }
    \label{fig:data}
    \vspace{-8pt}
\end{figure}

\smallsec{Baselines.} We compare our methods against the most recent learning-based object discovery models. In particular, \textbf{SAVi}~\cite{kipf2021conditional} and \textbf{SAVi++}~\cite{elsayed2022savi++} are two direct extensions of the original slot attention~\cite{locatello2020object} using optical flow and depth to facilitate object-centric learning. Alternatively, \textbf{STEVE}~\cite{singh2022simple} uses a more powerful transformer decoder to enhance the object discovery performance. \textbf{Bao \etal}~\cite{bao2022discovering}, \textbf{Karazija \etal}~\cite{karazija2022unsupervised} utilize motion cues to guide object discovery. In addition, in the appendix, we compare our approach to~\cite{seitzer2022bridging}, which is a concurrent work that relies on ImageNet~\cite{deng2009imagenet} pre-training~\cite{caron2021emerging}. For \cite{kipf2021conditional,elsayed2022savi++,singh2022simple,bao2022discovering}, we use either the official code or public implementation (see the appendix for details). Due to the lack of implementation, we reuse reported results with the two very recent approaches~\cite{karazija2022unsupervised, seitzer2022bridging}.


\smallsec{Evaluation Metrics.} Following prior works, we use the Foreground Adjusted Rand Index (FG.~ARI) to evaluate the performance of the models, which captures how well the predicted segmentation masks match ground-truth masks in a permutation-invariant fashion. Notice that FG.~ARI is measured in terms of the whole video, so that the temporal consistency of the slots is considered with this metric.

\smallsec{Implementation Details.} We use the same ResNet-18 ConvGRU encoder backbone for all the compared methods following \cite{bao2022discovering}. We set the number of slots as 24 for MOVi-E and 45 for PD and KITTI based on the maximal number of objects in these datasets. On TRI-PD and KITTI, we further downsample the images by 4 for SAVi and SAVi++ to fit the GPU memory. Notice that, the other methods also produce downsampled slot masks, leading to the evaluation under the same resolution for all the methods.
All the models are trained for 500 epochs using Adam~\cite{kingma2014adam}. During training, we train all the models with a randomly sampled video sequence of a fixed length (6, 5, 5 for MOVi-E, TRI-PD, and KITTI respectively). During the inference time, all the models are evaluated frame by frame until the end. For the FG.~ARI measurement on TRI-PD, we discard any instance labels covering an area of 0.5\% or less of the first sampled video frame following~\cite{elsayed2022savi++}. We also evaluate FG.~ARI at 5 FPS due to memory limitation. We use batch size 64 for MOVi-E, and 8 for TRI-PD and KITTI.

Our method and Bao \etal~\cite{bao2022discovering} require motion segmentation signals. We apply \cite{dave2019towards}, a powerful method pre-trained on the toy FlyingThings3D dataset~\cite{mayer2016large}, taking the ground-truth flow (MOVi-E) or RAFT~\cite{teed2020raft} flow as the input. For SAVi and SAVi++, we do not use the first-frame bounding box supervision for a fair comparison. We generate the optical flow and depth annotation for KITTI using two state-of-the-art methods, RAFT~\cite{teed2020raft} and VIDAR~\cite{tri-packnet}. We also use them for ablations on TRI-PD. More details about the hyper-parameters, training scheme, and annotation generation are provided in the appendix.

\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-movi.pdf}
    \vspace{-20 pt}
    \caption{Object discovery in different reconstruction spaces on the evaluation set of MOVi-E. Compared with the other reconstruction spaces, the model trained using the VQ-space demonstrates a more accurate performance and better temporal consistency.}
    \label{fig:movi}
    \vspace{-8pt}
\end{figure}

\subsection{Object Discovery on MOVi}
We use MOVi-E dataset to ablate our model with different choices of slot decoder, and reconstruction spaces, aiming to elucidate their roles in object discovery. We also compare the performance with state-of-the-art models in this section.


\begin{table}[t]
 \centering
\resizebox{0.8 \linewidth}{!}{
    \begin{tabular}{c|c|c|c}
    Motion & Space & Decoder & FG.~ARI \\ \hline
    \xmark & VQ & Linear & 14.6 \\
    \xmark & VQ & Linear-CNN & 16.3 \\
    \xmark & VQ & Transformer & 40.1 \\
    \xmark & VQ & Perceiver & {\bf 52.4} \\ \hline
    \xmark & RGB & Perceiver & 49.2 \\
    \xmark & Flow & Perceiver & 36.3 \\
    \xmark & Depth & Perceiver & 18.4 \\ 
    \xmark & Flow + Depth & Perceiver & 38.0 \\
    \xmark & VQ (flow) & Perceiver & 44.6 \\
    \end{tabular}
}
\vspace{-4pt}
    \caption{Model architecture analysis with different choices of the slot decoder and reconstruction space on MOVi-E. Perceiver decoder + VQ reconstruction space yields the best performance, indicating that (1) the capacity of the decoder plays a key role in object discovery; (2) learnable, vector-quantized reconstruction space outperforms fixed alternatives like depth or flow.}
    \label{tab:architecture}
\end{table}


\smallsec{Set up.} We first fix the reconstruction space as the VQ-space and ablate the decoder architectures (first 4 lines in Table~\ref{tab:architecture}), and then we fix the decoder architecture with the best component and reconstruct in different reconstruction space (Line 5 to 9 in Table~\ref{tab:architecture}). Since SAVi++~\cite{elsayed2022savi++} argues that reconstructing in the combined space of flow and depth is the key to their success, we add a variant \textit{Flow + Depth}, which also reconstructs in this combined space. Furthermore, as VQ-VAE can be trained to reconstruct the image in the flow space as well, we also report a \textit{VQ (flow)} variant for completeness. We do not include motion cues for the architecture analysis. A more comprehensive ablation is reported in the appendix.

\smallsec{Decoder analysis.} By comparing the results on the first four lines in Table~\ref{tab:architecture}, similar to \cite{singh2022simple}, we find that the capacity of the decoder indeed plays a key role in the object discovery performance. A simple linear decoder fails, but adding an additional CNN decoder can bring limited performance gains. Introducing powerful transformer-based decoders, on the other hand, greatly improves the object discovery capabilities of the model. Between the two transformer variants, the more advanced perceiver decoder achieves better results, while being more computationally efficient. 


\smallsec{Reconstruction space analysis.} For the four reconstruction spaces, the VQ-space yields the best object discovery performance. We analyze the learned slots and token representations in Figure~\ref{fig:movi} and make the following discoveries. Firstly, the model trained using the VQ-space better separates the objects from the background and shows stronger temporal consistency. This is due to the more structured, compact, and lower-variance reconstruction space provided by the quantized features, which simplifies the task of grouping, compared to the raw RGB space. 

Secondly, by comparing \textit{Flow} and \textit{Flow+Depth} variants, we find that the latter indeed carries more information, allowing the model to better group the objects, which is consistent with~\cite{elsayed2022savi++}. However, interestingly, we also find that given a sufficiently strong decoder, reconstructing in depth or flow space does not bring further improvements compared to RGB, and can even decrease the performance. 
Finally, comparing \textit{Flow} and \textit{VQ(flow)}, we find that although adding the quantization improves performance significantly, this variant still lags behind reconstructing in the raw RGB space, reinforcing our previous conclusions.

\smallsec{Comparison to the state of the art.} In Table~\ref{tab:movi}, we compare our approach to the state-of-the-art methods. Our model achieves the best performance among them. Additionally, compared to SAVi and SAVi++, even our variants that reconstructs in the same space outperform them, indicating that the perceiver decoder and single-stage decoding strategy are the optimal choices. Our model also outperforms STEVE, due to the fact that, in contrast to DVAE, the codebook in VQ-VAE can be jointly optimized with slot learning (see Equation~\ref{eq:vqloss}). Compared with Bao \etal, our model shows better performance even without motion cues used by that method, but further incorporating them into our approach allows it to achieve top results. 

Notice that, we compare with the variants of SAVi and SAVi++ that do not rely on ground-truth bounding boxes at \textit{test time}. Their published numbers are included at the bottom of the table for reference, but they are not comparable to the other \textit{unsupervised} methods. 


\begin{table}[t]
 \centering
 \resizebox{0.55 \linewidth}{!}{
    \begin{tabular}{l|c}
    Model & FG.~ARI  \\ \hline
    Bao \etal & 51.6 \\ 
    STEVE & 51.2 \\  
    SAVi & 28.1 \\ 
    SAVi++ & 31.7 \\  \hline
    MoTok (no motion) & \bf 52.4\\
    MoTok & \bf 63.8 \\ \hline
    \color{gray}SAVi (G.T. box)  & \color{gray} 53.4\\ 
    \color{gray} SAVi++ (G.T. box) & \color{gray} 84.1\\
    \end{tabular}
}
\vspace{-4pt}
    \caption{Comparison to the state-of-the-art object discovery approaches on the validation sets of MOVi-E using FG.~ARI. Our approach outperforms all the recent methods with the help of motion cues and vector quantization.}
    \label{tab:movi}
    \vspace{-10pt}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-pd.pdf}
    \vspace{-20 pt}
    \caption{Reconstruction, slot representation, and token masks for \cite{bao2022discovering}, STEVE-m~\cite{singh2022simple}, and our method. We use ground-truth motion cues for supervision, and visualize top-10 masks excluding background slots. MoTok effectively leverages the synergy between motion and tokenization, enabling the emergence of interpretable object-specific mid-level features, which simplify the problem of object discovery.}
    \label{fig:pd}
    \vspace{-12 pt}
\end{figure*}

\subsection{Object Discovery with Realistic Driving Videos}
\label{sec:pd}
\smallsec{Set up.} Firstly, following~\cite{bao2022discovering}, we capitalize on the ground-truth annotations available in TRI-PD, and evaluate the importance of the quality of external signals for various approaches. In particular, we separately evaluate using ground-truth (GT) flow, depth, and motion segmentation, as well as estimating those using state-of-the-art algorithms~\cite{dave2019towards,teed2020raft,tri-packnet}. In addition, we report a variant \textit{STEVE-m} for which we add the same motion segmentation supervision as to~\cite{bao2022discovering} and our approach for a fair comparison. 


\begin{table}[t]
 \centering
\resizebox{0.8 \linewidth}{!}{
    \begin{tabular}{l|c|c|cc}
    \multirow{2}{*}{Model} & \multirow{2}{*}{Signal} & 
    \multirowcell{2}{GPU Mem.\\ (GB)} & \multicolumn{2}{c}{FG.~ARI} \\ \cline{4-5}
    & & & GT & EST \\ \hline
      Bao \etal  & Motion & 21.3 & 50.4 & 34.7\\
      SAVi~ & Flow & 17.3 & 17.9 & 14.2\\
      SAVi++ & Depth + Flow & 17.5 & 18.4 & 15.3 \\
      STEVE & - & 66.7 & 12.5 & - \\
      STEVE-m & Motion & 68.2 & 45.7 & 32.2 \\
      \hline
      MoTok & Motion & 10.9 & \bf 60.6 & \bf 55.0\\
    \end{tabular}
    }
    \vspace{-4pt}
    \caption{Object discovery evaluation of different models on TRI-PD dataset. GT denotes the ground-truth guidance given to each model and EST denotes the estimated cues. State-of-the-art models fail to work without motion cues; introducing token representation helps our model better utilize the motion signal.
    }
    \label{tab:pd}
\end{table}

From Table~\ref{tab:pd}, we can observe that our method strongly outperforms all the baselines. Additionally, we find that even with the ground-truth flow or depth, SAVI and SAVI++ do not work well for realistic videos with complex background and crowded scenes. Without motion cues, even equipped with a powerful transformer decoder, STEVE fails to work in this challenging setting as well. Finally, our model achieves better performance compared to Bao \etal~with both GT and estimated motion segments, indicating its better robustness.

\smallsec{Impact of motion-guided tokens:} In Figure~\ref{fig:pd}, we visualize the RGB reconstruction, slot representation, and token masks (if possible) for Bao \etal, STEVE-m, and our MoTok. There are several key observations. Firstly, compared with STEVE-m, MoTok enables the emergence of more interpretable mid-level features, even though both models are trained with motion cues and tokenization. This is due to the interplay between object and slot discovery in our end-to-end framework, whereas STEVE-m trains DVAE separately from the rest of the model.  

Secondly,  our model achieves the best RGB reconstruction result, even for the latter frames. The high quality of the reconstruction indicates that the model can better take advantage of the appearance signal to optimize the slot representation. Finally, our model demonstrates stronger temporal consistency compared with the baselines, thanks to the structured reconstruction space. To sum up, our architecture effectively leverages the synergy between motion cues and tokenization, enabling the emergence of interpretable mid-level features, which greatly simplifies the task of object discovery.

\smallsec{Interpretable tokens.} To better illustrate the interpretability of motion-guided tokens, we measured their alignment with the ground-truth semantic labels using cluster purity~\cite{schutze2008introduction}. Vanilla VQ-VAE (no motion guidance) achieves a purity of 40.1, whereas ours reaches 65.7 (chance performance: 38.5). Qualitative comparison in Figure~\ref{fig:rebuttal} clearly demonstrates our better semantic alignment.
\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-rebuttal.pdf}
    \vspace{-20 pt}
    \caption{Visualization of the learned tokens for vanilla VQ-VAE and MoTok. The proposed MoTok model shows a better alignment with semantic categories.}
    \label{fig:rebuttal}
    \vspace{-5pt}
\end{figure}

\smallsec{Scalability to realistic videos:} We additionally report the GPU memory usage per batch in Table~\ref{tab:pd}. The per-slot decoding strategy and the use of transformer decoder limit the scalability of the other baselines. In comparison, our model achieves the best performance with the lowest memory consumption, showing a great generalization capability to real-world videos, thanks to the efficient perceiver decoder and the single-shot decoding strategy.


\smallsec{Impact of objectives.}  Finally, we ablate the objective design of our model with the GT annotations in Table~\ref{tab:pd_ablation}. 
We build three variants of our model, one without the additional token contrastive constraint defined by Equation~\ref{eq:contrastive} (no contrastive), the second without the motion cues (no motion), and the last trained with ground-truth instance masks for all objects to indicate the performance upper bound (*). By comparing the performance of these variants, we make the following observations: (1) in realistic synthetic videos even the strongest architectures fail to resolve the object/background ambiguity in the absence of motion cues, demonstrating the difficulty of object discovery tasks in the real world; (2) contrastive constraint facilitates diversity during vector quantization, increasing the information content of each token, which helps reduce the object/background ambiguity; (3) the performance of our model is close to the upper bound, indicating the effectiveness of the motion-guided tokens.

\subsection{Object Discovery in the Real World }
\smallsec{Set up.} To train the object discovery model on the real-world KITTI benchmark, we first pre-train all the models on PD using ground-truth flow and depth and then fine-tune them using estimated annotations on KITTI. 

The comparisons on KITTI are shown in Table~\ref{tab:KITTI} and Figure~\ref{fig:kitti}. We find that, consistent with our observations on TRI-PD, without motion guidance, both more powerful decoders (STEVE) and the simpler reconstruction space (SAVi and SAVi++) fail to work. However, our model still captures good dynamic object segmentation results. Compared to Bao \etal, we still achieve a better FG.~ARI score and a more solid segmentation mask, indicating the benefit of the model design and the motion-guided tokens. Finally, we outperform the very recent approach of Karazija \etal~\cite{karazija2022unsupervised} even when they include an additional warping loss objective. 

\begin{table}[t]
\begin{minipage}{0.45 \linewidth}
 \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|c}
    Model & FG.~ARI\\ \hline
    MoTok &\bf 60.6 \\ 
      % MoTok$_{w/ contrastive}$ 
      MoTok (no contrastive) & 57.4 \\
      MoTok (no motion)
      & 15.6 \\ \hline
      $\text{MoTok}^*$  & \it 64.7 \\
    \end{tabular}
    }
    \vspace{-6pt}
    \caption{Ablation study on different learning signals. MoTok$^*$ is our model trained with ground-truth instance segmentation. The motion cue and the contrastive constraint help improve the capability of the proposed model, almost reaching the performance upper bound (MoTok$^*$).
}
\label{tab:pd_ablation}
\end{minipage}
\hfill
\begin{minipage}{0.51 \linewidth}
\vspace{-8pt}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c}
    Model & FG.~ARI \\ \hline
    Bao \etal~\cite{bao2022discovering} & 47.1 \\
    SAVi & 20.0 \\
    SAVi++  & 23.9 \\
    STEVE  & 11.9 \\
    Karazija \etal~\cite{karazija2022unsupervised}& 50.8 \\
    Karazija \etal~\cite{karazija2022unsupervised} (WL)& 51.9 \\
    \hline
    MoTok & \bf 64.4\\ 
    \end{tabular}
    }
    \vspace{-6pt}
    \caption{Evaluation of object discovery in the real-world KITTI dataset. Karazija \etal~\cite{karazija2022unsupervised} (WL) is a variant trained with their proposed wrapping loss. Our model achieves state-of-the-art in real-world object discovery.
    }
    \label{tab:KITTI}
    \vspace{-5pt}
\end{minipage}
\end{table}


\subsection{Limitations and Future Work}
\smallsec{Object/background ambiguity in the real world.}
Even after successfully parsing the world into object and background slots, separating those from each other in an unsupervised way is an open challenge. Promising directions include temporal contrastive learning~\cite{jabri2020space,bian2022learning}, unsupervised clustering~\cite{caron2018deep,van2021unsupervised}, and utilizing geometric cues~\cite{zhang2019learning,zhou2022cross}.

\smallsec{Slot drift.} From Figure~\ref{fig:pd}, we notice that the final RGB reconstruction quality decreases with longer videos, which indicates that the distribution of the slot representation has shifted in the latter frames. This issue leads to degraded temporal consistency and ambiguous object masks. Better training schemes and model improvements, such as adding data augmentation during training~\cite{elsayed2022savi++}, replacing GRU units with long-short memory units~\cite{hochreiter1997long}, and enforcing temporal consistency in the token space can help.

\smallsec{Better measuring object discovery in the real world.} Measuring the performance of object discovery in the real world is challenging. As noted in~\cite{elsayed2022savi++}, one slot may re-bind to another object after the tracked object moves out of the frame and a new object enters the scene. The FG.~ARI score is not suitable for such scenarios but currently, there are no better metrics for object discovery which is able to tackle the re-binding issue. Developing a novel metric for this problem could have a significant impact on the community. 

\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-kitti.pdf}
    \vspace{-10 pt}
    \caption{Top-10 slot visualizations of all the compared methods on the evaluation set of TRI-PD and KITTI datasets. The samples on KITTI are cropped for better visualization. We discard the masks taking more than 20\% of the whole pixels. Both the more powerful decoders (STEVE) and the simpler reconstruction space (SAVi and SAVi++) fail to work in realistic driving scenes. Our model produces the most accurate masks and captures more objects. }
    \label{fig:kitti}
    \vspace{-10pt}
\end{figure}


