\section{Method}
\label{sec:method}



We now explain the proposed Motion-guided Tokens (MoTok) framework in detail. Its architecture is shown in Figure~\ref{fig:model}. We first introduce a motion-guided slot learning framework~\cite{bao2022discovering}  in Section~\ref{sec:preliminary}. We then describe the slot decoder units in Section~\ref{sec:slot2token}, which decode the slot features into a reconstruction space. In Section~\ref{sec:recon}, we describe the choices of reconstruction space and explain the vector-quantized reconstruction space in detail. Finally, we demonstrate how we optimize the model in Section~\ref{sec:objective}. 

\subsection{Preliminary: Motion-Guided Slot Learning}
\label{sec:preliminary}
Our object-centric representation learning module is derived  from~\cite{bao2022discovering} (Figure~\ref{fig:model}, \emph{left}). Concretely, given a sequence of video frames $\{I^{1}, I^2, ..., I^T\}$, we first process each frame through an encoder CNN to obtain an individual frame representation $H^t = f_\mathrm{enc}(I^t)$. These individual representations are aggregated by a spatio-temporal Convolutional Gated Recurrent Unit (ConvGRU)~\cite{ballas2015delving} to obtain video encoding via $H^{'t}={\tt ConvGRU}(R^{t-1}, H^t)$, where $R^{t-1} \in \mathbb{R}^{h' \times w' \times d_\mathrm{inp}}$ is the recurrent memory state. 

Next, we perform a single attention operation with $K$ slots to directly compute the slot state $S^t = W^{t} v(H^{'t})$, where $W^t \in \mathbb{R}^{K \times N}$ is the attention matrix, $N = h'\times w'$ indicates the flapped shape dimension, and $v(\cdot)$ is the value embedding function. $W^t$ is computed using the slot state in the previous frame $S^{t-1}$ and the input feature $H^{'t}$. For the first frame, we use a learnable initial state $S^0$. At the same time, for each slot $s^t_i$, we can also obtain the attention mask $W_{:,i}^t$. 

In \cite{bao2022discovering}, a motion cue is also added to guide the slots to find moving objects. A set of sparse, instance-level motion segmentation masks $\mathcal{M} = \{M^1, M^2, ..., M^T\}$ is assumed to be provided with every video, with $M^t = \{m_1, m_2, ..., m_{C^t}\}$, where $C^t$ is the number of moving objects that were successfully segmented in frame $t$, and $m_j \in \{0,1\}^{h' \times w'}$ is a binary mask. The attention mask $W^t \in \mathbb{R}^{K \times N}$, is then supervised with the motion segments. $M^t$ is also considered as a set of length $K$ padded with $\emptyset$ (no object), and a bipartite matching is found between them with the lowest cost:
\begin{equation}
    \hat{\sigma} = \argmin_{\sigma} \sum_{i=1}^K \mathcal{L}_\mathrm{seg}(m_i, W^t_{:,\sigma(i)}),
\label{eq:match}
\end{equation}
where $\mathcal{L}_\mathrm{seg}(m_i, W^t_{:,\sigma(i)})$ is the segmentation loss between the motion mask $m_i$ and the attention map of the slot with index $\sigma(i)$. Once the assignment $\hat{\sigma}$ has been computed, the final motion supervision objective is defined as follows:
\begin{equation}
    \mathcal{L}_\mathrm{motion} = \sum_{i=1}^K  \mathbbm{1}_{\{m_i \neq \emptyset\}} \mathcal{L}_\mathrm{seg}(m_i, W^t_{:,\hat{\sigma}(i)}),
\label{eq:motion}
\end{equation}
where $\mathbbm{1}_{\{m_i \neq \emptyset\}}$ denotes that the loss is only computed for the matched slots and $\mathcal{L}_\mathrm{seg}$ is the binary cross entropy. 


\subsection{Slot Decoders}
\label{sec:slot2token}
The goal of the Slot Decoder is to map the slot representation $(S^t, W^t)$ to a 2D feature map $F^t$ for the reconstruction space. As shown in Figure~\ref{fig:model} (\emph{middle}), we propose four choices for the slot decoder. 

\noindent
\textbf{Linear Decoder} directly maps the slot features $S^t$ to their corresponding positions based on the attention mask $W^t$:
\begin{equation}
    F^t_\mathrm{linear}(\mathbf{x}) = \frac{\sum^K_{i=1} S^t_i(\mathbf{x}) W^t_{i,\mathbf{x}}}{\sum^K_{i=1} W^t_{i, \mathbf{x}}},
    \label{eq:linear}
\end{equation}
where $\mathbf{x}$ is an arbitrary 2D position.

\noindent
\textbf{CNN Decoder} further adds two convolutional layers to the 2D feature map formed by Equation~\ref{eq:linear}:
\begin{equation}
    F^t_\text{CNN} = \text{CNN}\left(\frac{\sum^K_{i=1} S^t_i W^t_{i,:}}{\sum^K_{i=1} W^t_{i,:}}\right).
\end{equation}
\noindent
\textbf{Transformer Decoder} decodes the feature by querying the slot representation with a 2D positional embedding through a transformer decoder:
\begin{equation}
    F^t_\mathrm{transformer} = \text{Transformer}(P, S^t, S^t),
\end{equation}
where the query $P \in \mathbb{R}^{N \times d_p}$ is a learnable positional embedding.

Compared with the previous two linear decoders, the transformer decoder further considers the global connections between the slot features and the input query, so that it will form a more powerful feature map. However, an obvious limitation is that the transformer decoder applies self-attention to the input positional query, which is 1) redundant since the positional embedding itself is learnable, and 2) not computationally efficient therefore limits the scalability of the whole model. To solve this limitation, we further propose the {\em perceiver decoder}.

\noindent
\textbf{Perceiver Decoder} is inspired by \cite{jaegle2021perceiverio}, which designs a computationally efficient architecture to handle arbitrary outputs in addition to arbitrary inputs. Different from the original architecture, we only add a self-attention layer to the slot representations, followed by a cross-attention layer for the output and positional embedding queries. The whole procedure is illustrated in Algorithm~\ref{alg1}.
\begin{algorithm}[tb]
%   \caption{Perceiver decoder with $S^t$ and $P$}
\caption{Perceiver decoder}
   \label{alg1}
  Perceiver($S^t$, $P$): \\
   $S^t$ $\rightarrow$ Norm($S^t$) \\
   $\hat{S}^t$ = SelfAttention($S^t$) + $S^t$ \\
   $\hat{S}^t$ $\rightarrow$ Norm($\hat{S}^t$) \\
   $\Tilde{S}^t$ = MLP($\hat{S}^t$) + $\hat{S}^t$ \\
   $\Tilde{S}^t$ $\rightarrow$ Norm($\Tilde{S}^t$) \quad $P$ $\rightarrow$ Norm($P$) \\
   $F^t_\mathrm{Perceiver}$ = CrossAttention($P$, $\Tilde{S}^t$, $\Tilde{S}^t$) 
\end{algorithm}

After forming the 2D feature map $F^t$, we decode it to a reconstruction space with a CNN-based decoder, except for when vector-quantized space is used.  As we will show next, an additional decoder is redundant in this case. 

\subsection{Reconstruction Space}
\label{sec:recon}

There are four choices of reconstruction space in our framework. The plain RGB space contains the most information but also is the most complex to resolve the object/background ambiguity. The flow and depth spaces, as shown in Figure~\ref{fig:model} (\emph{right}), are more structured. Reconstructing in these spaces makes the grouping problem easier. However, there are still shortcomings, \eg~non-moving objects are not captured in the flow space and depth cannot easily distinguish between-objects that are near each other. Moreover, these two spaces are not as informative as the RGB space. In addition, we introduce the VQ-space, which is end-to-end trainable and is both structured and informative. We describe the VQ-space below.


\smallsec{Vector-quantized reconstruction space.}
Different from the other three reconstruction spaces, here we do not predict the reconstruction directly, but rather supervise the feature map $F^t$ to match the latent embedding space of VQ-VAE.

In particular, following~\cite{van2017neural}, we define a latent embedding space $E$ as the set of $M$ vectors $e_i$ of dimension $d_{vq}$, $E = \left\{ e_i \in \mathbb{R}^{d_{vq}} | i = 1, 2, \cdots, M \right\}$.
Given an input image $I^t$, VQ-VAE first processes it with an encoder to get the output $z_e^t = \text{Encoder}_\mathrm{VQ}(I^t)$, and then the discrete latent variables $z$ are calculated by a nearest neighborhood search among the discrete feature set $E$:
\begin{equation}
    z_q^t(\mathbf{x}) = e_k, \quad  \text{where}~k = \text{argmin}_j ||z_e^t(\mathbf{x}) - e_j ||_2,
\end{equation}
and $\mathbf{x}$ is an arbitrary 2D position.
The reconstructed image $\hat{I}^t$ is then decoded by $\hat{I}^t = \text{Decoder}_\mathrm{VQ}(z_q^t)$. The objective of VQ-VAE is:
\begin{equation}
\small
    \mathcal{L}_\text{VQVAE} = \log P(I^t | z_q^t) + ||sg[z_e^t] - z_q^t||_2 + || sg[z_q^t] - z_e^t ||_2,
\end{equation}
where $sg[\cdot]$ is the stop-gradient operation.

Then we use the quantized feature map $z_q^t$ as the target signal for the slot feature map $F^t$. The final objective of VQ-VAE and the VQ reconstruction is:
\begin{equation}
    \mathcal{L}_{\text{VQ}} = \mathcal{L}_{\text{VQVAE}} + ||sg[F^t] - z_q^t||_2 + || sg[z_q^t] - F^t ||_2.
    \label{eq:vqloss}
\end{equation}

\smallsec{Motion-guided token representation.} The last term in Equation~\ref{eq:vqloss}, $||sg[F^t] - z_q^t||_2$, enables the motion signal from slot learning to jointly optimize the token space through the output of the slot decoder. Furthermore, the token representation and the motion cues build a connection linked by the slot learning, thus enabling the {\em emergence of interpretable object-specific mid-level features of tokens}. In addition, reconstructing in a more compact token space also benefits the model by better utilizing the motion signal to achieve an improved slot representation and temporal consistency. 



\subsection{Optimization}
\label{sec:objective}

\smallsec{Token contrastive constraint.} The goal of reconstructing in the VQ-space is that it is more compact and of lower variation compared with the RGB space. To make the VQ-space more structured, we add an \emph{optional} contrastive constraint below to the vector space, which increases the independence between latent vectors:
\begin{equation}
\mathcal{L}_\mathrm{contrastive} = ||\mathbb{I} - \text{softmax}(E \cdot E^T)||,
\end{equation}
where $\mathbb{I}$ is the identity matrix and $E \in \mathbb{R}^{N \times d_{vq}}$ is the matrix of the feature embedding space $S$. 

The final loss function is a combination of the reconstruction objective, the motion objective, and the optional contrastive constraint:
\begin{equation}
    \mathcal{L} = \lambda \mathcal{L}_\mathrm{motion} + \mathcal{L}_\mathrm{recon}+ \lambda_c \mathbbm{1}_\mathrm{VQ} \mathcal{L}_\mathrm{contrastive},
    \label{eq:contrastive}
\end{equation}
where $\lambda$ and $\lambda_c$ are weighting factors and $\mathbbm{1}_\mathrm{VQ}$ is an indicator function. For the reconstruction loss, we set $\mathcal{L}_\mathrm{recon} = \mathcal{L}_\mathrm{VQ}$ when performing reconstruction in the VQ-space. Otherwise, we use an $L_2$ loss for reconstruction in the other three spaces. 




