\section{Related Work}
\label{sec:related}

In this work, we tackle the \textit{object discovery} problem in realistic videos with \textit{object-centric representation} by capitalizing on \textit{motion-guided tokens}. We review the most relevant works in these areas below.

\smallsec{Object Discovery} tackles the problem of separating objects from background without manual labels~\cite{bao2022discovering}. Classic computer vision methods use appearance-based perceptual grouping to parse a scene into object-like regions~\cite{koffka2013principles,felzenszwalb2004efficient,arbelaez2014multiscale}. Among them, \cite{arbelaez2014multiscale} first propose a multiscale fast normalized cuts algorithm and achieved robust object segmentation performance for static images. 

Recently, the topic of object discovery has experienced renewed attention under the name of unsupervised object-centric representation learning. A wide variety of learning-based methods has been introduced~\cite{greff2016tagger,burgess2019monet,greff2019multi,lin2020space,locatello2020object,engelcke2019genesis,veerapaneni2020entity,jiang2019scalor,yu2021unsupervised,singh2021illiterate}, usually with an encoder-decoder architecture~\cite{kingma2013auto,rezende2014stochastic}. These methods aim to learn compositional feature representations, \eg, a set of variables that can bind to objects in an image~\cite{greff2016tagger,burgess2019monet,greff2019multi,locatello2020object,engelcke2019genesis,veerapaneni2020entity,yu2021unsupervised,singh2021illiterate}, or a video~\cite{lin2020space,jiang2019scalor,elsayed2022savi++,kipf2021conditional,singh2022simple,karazija2022unsupervised,sajjadi2022object,bao2022discovering,seitzer2022bridging,wu2022slotformer}. Among them, \cite{locatello2020object} first formulate the SlotAttention framework, which is used to bind a set of variables, called slots, to image locations. The slots are then decoded individually and combined to reconstruct the image. 

However, without additional constraints, such methods tend to converge to pixel grouping based on low-level cues, such as color, and do not generalize to realistic images or videos with complex backgrounds. To address this limitation,~\cite{kipf2021conditional} and~\cite{elsayed2022savi++} extend the slot concept from static images to videos via reconstructing in the optical flow or depth space respectively. The intuition behind these methods is that this space provides stronger cues to separate the objects from the background.  Separately, Bao \etal~\cite{bao2022discovering} use motion cues to guide the slots to find moving objects and then generalize to all the objects that can move. 

In another line of work, Singh at al.~\cite{singh2022simple} show that a combination of a more powerful transformer decoder~\cite{dosovitskiy2020image} and discrete variational auto-encoder~\cite{rolfe2016discrete} can enhance the object discovery performance. Different from these works, we propose a unified architecture for object discovery that is flexible to different choices of decoders and reconstruction space. We also introduce the vector quantized features as an additional reconstruction space with motion guidance. 


Finally, several recent works also leverage 3D geometry as inductive biases to enforce the learning-based models to focus on object-like regions~\cite{stelzner2021decomposing,chen2020object,du2020unsupervised,henderson2020unsupervised}. Though these methods remain limited to the toy, synthetic environments, the underlying geometric priors are orthogonal to our approach and have a great potential to be combined with our proposed method as a future direction.

\smallsec{Vector quantization} is originally developed for data compression in signal processing~\cite{gray1984vector}. More recently, \cite{van2017neural} propose the Vector-Quantized Variational Autoencoder (VQ-VAE), which learns a discrete representation of images, and models their distribution aggressively. This technique has motivated a series of investigations in solving different computer vision tasks including image synthesis~\cite{razavi2019generating,esser2021taming,gu2022vector}, video generation~\cite{sun2019videobert,yan2021videogpt}, and language modeling~\cite{ramesh2021zero,ramesh2022hierarchical,gu2022vector}, to name a few. In this work, we adopt the vector quantization technique as a mid-level representation in our object discovery framework. The intuition is that reconstructing in a structured, low-dimensional feature space rather than the high-variance color space should simplify the task of resolving the object and background ambiguity. In comparison,~\cite{singh2022simple} also introduce a mid-level feature space for object discovery in videos. However, they  use DVAE, not VQ-VAE, and are motivated primarily by improving training efficiency. Moreover, they did not explore combing vector quantization with motion cues in an end-to-end trainable framework.

\smallsec{Transformers} are originally proposed for sequence-to-sequence modeling in natural language processing~\cite{vaswani2017attention}. They use a multi-head attention mechanism, instead of the recurrent memory units, to aggregate information from the input. Recently, vision transformer (ViT)~\cite{dosovitskiy2020image} and its derivations have achieved state-of-the-art in several visual tasks~\cite{carion2020end,sajjadi2022scene,zhu2020deformable,jaegle2021perceiver,jaegle2021perceiverio,sajjadi2022object,meinhardt2022trackformer}. Among them, Perceiver IO~\cite{jaegle2021perceiverio} design a computationally efficient architecture to handle arbitrary outputs in addition to arbitrary inputs, greatly enhancing the generalizability of the transformer architecture. We employ a variant of a Perceiver module as a decoder in our object discovery framework to combine high representational power with computational efficiency.

\begin{figure*}[t]
    \centering
    \includegraphics[width = \linewidth]{figures/fig-model.pdf}
    \vspace{-15 pt}
    \caption{Model architecture of the proposed Motion-guided Tokens (MoTok) framework. MoTok is a unified framework for video object discovery that is flexible with different choices of decoders and reconstruction spaces. Our framework effectively leverages the synergy between motion and tokenization, and enables the emergence of interpretable object-specific mid-level features.}
    \label{fig:model}
    \vspace{-8pt}
\end{figure*}