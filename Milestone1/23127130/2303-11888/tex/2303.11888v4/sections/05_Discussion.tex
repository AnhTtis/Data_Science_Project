\section{Discussion}
In this work, we introduce novel approaches for both Multi-sensor Fusion and Imitation Learning objective function design. The Cross Semantic Generation approach aims to extract and enhance the shared semantic information from LiDAR and RGB inputs. We used some auxiliary losses to regularize the feature space, ensuring the information flow of the features which are important for driving decisions according to human experience. Penalty-based Imitation Learning further increases the level of compliance of the agent with traffic rules. Some other approaches use an extra module to ensure the agent obeys traffic rules.  NEAT \cite{Chitta2021ICCV(NEAT)}, LAV \cite{chen2022lav} introduce some low-level control strategies in the PID controller to force braking at red lights. InterFuser uses a safety module to avoid dangerous actions such as collisions with other vehicles. These strategies largely increase the performance of the agent. However, these extra modules also make the network no longer end-to-end which may potentially cause more tuning effort and suboptimal solutions \cite{DBLP:journals/corr/abs-2003-06404}. With penalty-based Imitation Learning, we aim to avoid those decisions detached from the network. We use the penalty to make the agent more sensitive to traffic rules. The end-to-end nature of the network is guaranteed while constraining the agent to comply with traffic regulations. 

% To the best of our knowledge, it is the first time to introduce the penalty concept in imitation learning. Given that our proposed method is highly scalable, it would be interesting to extend the penalties based on new traffic rules such as collisions with pedestrians and vehicles. 

% It is also interesting to leverage multi-frame inputs to identify the violations of traffic regulations in the time dimension and introduce the corresponding penalty. For instance, the agent may drive on the line for long periods. 

Our study also has several limitations. First of all, we only use front-view images and 180-degree LiDAR data as input. The information from the rear of the vehicle is missing which may cause collisions when changing the lane. Besides, we only tried to integrate the RGB and LiDAR input, more sensors like radar input and depth-map can be taken into consideration. Additionally, we only test the performance of our model in the simulation environment. Real-world data can be more complex and contain more noise. Finally, we believe that further research should be dedicated to introducing additional penalties into the objective functions, as this approach holds promise in the development of human-level end-to-end autonomous driving agents.