\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}

\DeclareRobustCommand*{\IEEEauthorrefmarkn}[1]{%
  \raisebox{0pt}[0pt][0pt]{\textsuperscript{\footnotesize #1}}%
}

\hypersetup{backref=true,       
    pagebackref=true,               
    hyperindex=true,                
    colorlinks=true,                
    breaklinks=true,                
    urlcolor= black,                
    linkcolor= red,                
    bookmarksopen=false,
    filecolor=black,
    citecolor=green,
    linkbordercolor=blue
}
\begin{document}

\title{Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
\thanks{
\IEEEauthorrefmarkn{1}A. Sui, H. Zhou, and L. Shi are at the Trustworthy Technology and Engineering Laboratory, Huawei Munich Research Center.
}
\thanks{
\IEEEauthorrefmarkn{2}H. Zhou and L. Shi are also at the department of Computer Science, Technical University of Munich. 
}
\thanks{
\IEEEauthorrefmark{2}Corresponding author: A. Sui aifen.sui@huawei.com
}
}

\author{    
    \IEEEauthorblockN{
        Hongkuan Zhou\IEEEauthorrefmarkn{1}\,\IEEEauthorrefmarkn{2}, Aifen Sui\IEEEauthorrefmarkn{1}\,\IEEEauthorrefmark{2}, and Letian Shi\IEEEauthorrefmarkn{1}\,\IEEEauthorrefmarkn{2}
    }


% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}
With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving’s security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model’s compliance with traffic rules and unify the objective of imitation learning and the metric of autonomous driving. 
\end{abstract}

\begin{IEEEkeywords}
Autonomous Driving, Imitation Learning, Multi-sensor Fusion
\end{IEEEkeywords}

\section{Introduction}
End-to-end autonomous driving integrates the perception and decision layers into one deep neural network. The main policy learning approach for end-to-end autonomous driving is imitation learning. In this paper, we focus on multi-sensor fusion technologies and objective function redesign strategy to improve the performance of end-to-end autonomous driving. 

The fusion technologies of LiDAR and RGB sensors play an important role in autonomous driving. LiDAR sensors provide accurate 3D information for autonomous vehicles while they lack color information compared to RGB sensors; RGB sensors are more suitable to recognize traffic lights and traffic sign patterns while they are sensitive to bright light and weather conditions compared to LiDAR sensors. Many kinds of research \cite{TransFuser}, \cite{latefusion}, \cite{Filos2020CanAV} have shown that LiDAR and RGB fusion can achieve better results for end-to-end driving. In this work, we mainly focus on feature-level fusion technologies. 

In order to achieve better performance of end-to-end autonomous driving, a critical point for feature-level multi-sensor fusion is to enhance the shared features while not losing the unique features of each kind of sensor. We intend to maintain information flows during the training for both shared features and unique features, respectively. The shared features mainly refer to shared semantic information of both LiDAR and RGB input. By aligning the shared feature from multi-modality input, the shared semantic information will be enhanced. Those unique features are also important. Information such as traffic lights, and traffic sign patterns is critical for policy generation.

we also notice the current metric of autonomous driving and the object function of imitation learning is not unified which means a low loss of imitation learning does not guarantee the good performance of the agent in the test environment. The traffic rules e.g. forbidding running a red light and stop sign are not reflected in the objective function. We would like to design a new objective function that can make the model more sensitive to traffic rule violations in order to achieve better performance. 

Our main contributions to this paper can be summarized as follows:
\begin{itemize}
    \item We proposed a novel multi-sensor fusion model to extract the shared features and unique features between different modalities, making it easier for the decision network to select the critical features for policy generation.
    \item We proposed a penalty-based imitation learning approach that leverages constraint optimizations to make the end-to-end autonomous driving model more sensitive to traffic rule violations. This objective function design also unifies the metric of autonomous driving and the objective of imitation learning. We call it penalty-based imitation learning. 
\end{itemize}

\section{Related Works}
\subsection{End-to-End Autonomous Driving}
Today's autonomous driving technologies have two main branches, modular and end-to-end approaches. Modular approaches apply a fine-grained pipeline of software modules working together to control the vehicle. In contrast, the entire pipeline of end-to-end driving is treated as one single learning task. End-to-end approaches have shown great success in computer vision tasks, such as object detection, object tracking, and semantic segmentation. The success of these tasks builds a solid foundation for end-to-end autonomous driving. It is reasonable to believe end-to-end approaches are capable of solving autonomous driving problems in the near future. A remarkable advantage of end-to-end autonomous driving is its sustainable learning capability. The well-designed model can achieve better performance by feeding more data in different scenarios. Imagine millions of vehicles on the road every day, tons of data can be collected every second. After processing, these data can be used as training material for the model to improve its performance. The most common learning methods for end-to-end autonomous driving are imitation learning and reinforcement learning.\cite{TransFuser}, \cite{latefusion}, \cite{Filos2020CanAV}, \cite{Chitta2021ICCV(NEAT)}, \cite{chen2022lav}, \cite{8813900} implemented imitation learning method while  \cite{DBLP:journals/corr/abs-2001-08726} \cite{CPRL} seeks a reinforcement learning solution.

\subsection{Multi-sensor Fusion Technologies}
Sensor Fusion technologies are widely used in 3D object detection and motion forecasting. The LiDAR data, as a supplement to image data, provide more information about the surrounding environment and also enhances data reliability because of its stability in different environments. However, Process LiDAR data is time-consuming because of its large amount. The most common approach for LiDAR and RGB fusion is either projecting the LiDAR features into the image space or projecting the image features into the bird's eye view or range view space.

Multi-sensor fusion has received much research attention in the field of end-to-end autonomous driving. Prior works such as LateFusion\cite{latefusion} used a large Multi-Layer Perception network to process the features extracted by the perception networks of LiDAR and RGB inputs. This MLP layer takes the tasks of features weighting, selection, and fusion which makes it hard to generate a policy. As a result, LateFusion requires a long time for training and its performance is not good. TransFuser\cite{TransFuser} provides an approach that leverages the attention mechanism to fuse the LiDAR and RGB information. They used the transformer architecture to achieve the multi-modality global context. The Transformer-based fusion model is applied to different resolutions between the LiDAR and RGB perception networks. TransFuser+ \cite{TransFuser+}, as an extension of TransFuser, introduced more headers in the neural networks which incorporate four auxiliary tasks: depth prediction and semantic segmentation from the image branch; HD map prediction, and vehicle object detection from the BEV branch. These auxiliary tasks help to visualize the black box of the whole network. In addition, this approach also guarantees important information flow in the latent space because the information contained in the latent space should not only be able to complete the navigation task but also manually pre-defined auxiliary tasks.


\section{Methodologies}
\begin{figure}[h]
	\centering
	\includegraphics[width=1.05\linewidth]{figures/shared_semantic_features_v1.2.png}
	\caption{shared semantic features}
	\label{fig:shared semantic features}
\end{figure}
In this work, we propose a novel multi-sensor fusion approach and a penalty-based Imitation Learning method for end-to-end autonomous driving.
\subsection{Problem Setting}
The task we concentrate on is point-to-point navigation in an urban setting where the goal is to complete a route with safe reactions to dynamic agents such as moving vehicles and pedestrians. The traffic rules should also be followed. \\
\textbf{Imitation Learning (IL):}Imitation Learning can learn a policy $\pi$ that clone the behavior of an expert policy $\pi^*$. In our setup, the policy is conditioned on the multi-modalities inputs of current observations. We used the Behavior Clone (BC) approach of IL. An expert policy is applied in the environment to collect a large dataset $\mathcal{D}=\{(\textbf{x}^i, \textbf{w}^i)\}_{i=1}^{Z}$ with the size of $Z$, which contains the observation of the environment $\textbf{x}^i$ and a set of waypoints $\textbf{w}^i$ in the future timesteps. The objective function is defined as:
\begin{equation}
    \label{eq:objective function}
    \mathcal{F} = \mathbb{E}_{(\mathcal{X},\mathcal{W})\sim \mathcal{D}}[\mathcal{L}(\mathcal{W}, \pi(\mathcal{X}))]
\end{equation}
where $\mathcal{L}$ is the loss function. 

In our setting, the observation $\mathcal{X}$ consists of one RGB image and one LiDAR point cloud from the current time step. We used only one single frame since other works \cite{DBLP:journals/corr/abs-1905-06937}, \cite{DBLP:journals/corr/abs-1812-03079} have shown that using multiple frames does not improve the information gain much. A PID controller $\mathcal{I}$ is applied to perform low-level control, i.e steer, throttle, and brake based on these predicted future waypoints. \\
\textbf{Global Planner:} According to CARLA \cite{Dosovitskiy17} 0.9.10's protocol, the high-level goal locations $G$ is provided as GPS coordinates. This goal location $G$ is sparse (hundreds of meters apart) which can only be used as guidance. In contrast, those to be predicted waypoints are dense, only a few meters way away from each other. 

\subsection{Cross Semantics Generation}

\begin{figure*}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/Graph_v1.2.png}
	\caption{The architecture of our cross semantics generation. The top-down LiDAR and front RGB inputs go through two residual networks to extract 512 dimension feature vectors. We use four different MLPs to extract the shared features (orange) and the unique features (green). The unique features of RGB input are used to generate stop signs and traffic light indicators. The shared features of LiDAR are used to reconstruct the segmentation of RGB input while the shared features of RGB are used to reconstruct the segmentation of top-down LiDAR input. These shared features and unique features are concatenated and go through one MLP to reduce the size. Finally, they will be fed into one GRU decoder to predict waypoints.}
	\label{fig:Architecture}
\end{figure*}




The motivation of our approach is based on the fact that multi-modal inputs have shared semantic information and also unique information. For instance, the vehicle's and pedestrian's shape and location are the shared information of LiDAR and RGB input. Figure \ref{fig:shared semantic features} demonstrates the shared information of LiDAR and RGB input. The unique information refers to the complementary information that other inputs do not have. For the RGB input, the unique information usually means the traffic light color, traffic sign patterns, and so on. For the LiDAR input, the unique information refers to the spatial information. Our multi-sensor fusion approach aims to extract and align the shared features from LiDAR and RGB input sources so that the later decision network can leverage the organized features to achieve better performance.

In order to extract the shared information from LiDAR and RGB inputs, we propose cross semantics generation sensor fusion. As Figure \ref{fig:Architecture} demonstrates, the RGB and LiDAR input will first be fed into two residual networks \cite{DBLP:journals/corr/HeZRS15} to extract the corresponding RGB and LiDAR features. We use four different linear layers to extract the shared features and unique features of LiDAR and RGB.  The shared features of  RGB are used to generate the top-down semantic segmentation align with LiDAR input; The shared features of LiDAR are used to generate the semantic segmentation of corresponding RGB input. Given that RGB and LiDAR inputs do not contain all the information about each other but contain most of the semantic information of each other, we use semantic segmentation of the decoders. Semantic segmentation also helps to understand the composition of the surrounding environments. In our setup, the semantic segmentation contains 4 channels, the drivable area, the non-drivable area, the object in the drivable areas like vehicles and pedestrians, and others. We use L2-loss to align the shared features of RGB and LiDAR. In terms of the unique features from RGB input, we mainly concentrate on the traffic lights and stop signs. As we can see from the figure, the unique features from RGB input are used to train the traffic light and stop sign indicator which ensures the important information flows of traffic lights and stop signs in the neural network. These headers are also critical for later penalty-based Imitation.

\subsection{Waypoint Prediction Network}
As shown in Figure \ref{fig:Architecture}, all the unique and shared features are concatenated into a 512-dimensional feature vector. This vector is fed into an MLP to reduce the dimension to 64 for computational efficiency reasons. The hidden layer of GRU is initialized with a 64-dimensional feature vector. GRU’s update gate controls the information flow from the hidden layer to the output. In each timestep, it also takes the current location and goal location as input. We found that taking the GPS coordinates of the goal location as input can increase the accuracy of waypoint prediction. We follow the approach of \cite{Filos2020CanAV} that a single GRU layer is followed by a linear layer which takes the state of the hidden layer and predicts the relative position of the waypoint compared to the previous waypoint for $T=4$ time-steps. Hence, the predicted future waypoints are formed as $\{w_t = w_{t-1} + \delta w_t\}_{t=1}^T$. The start symbol for GRU is given by (0,0). \\
\textbf{Controller}: Based on the predicted waypoints, we use two PID controllers for lateral and longitudinal directions respectively. We use the same code base of \cite{chen2019lbc}.

\subsection{Loss Functions}
Similar to previous works \cite{TransFuser}, \cite{chen2019lbc}, we also use $L_1$ norm in our loss function. For each input, the loss function can be formalized as:
\begin{equation}
    \mathcal{L} = \sum_{t=1}^{T} ||w_t - w_t^{gt}||_1
\end{equation}
where $w_t$ is the t-th predicted waypoints and $w_t^{gt}$ is the t-th ground truth waypoint produced by the expert policy. \\
\textbf{Auxiliary Tasks}: In our cross semantics generation approach, we have four extra auxiliary tasks along with the main imitation learning task. As we explained in the above section, two of the auxiliary tasks are semantic segmentation. In order to ensure some important information flow in the network, we introduce two extra classification headers, namely traffic light classification and stop sign classification. These two headers help the neural network to capture traffic light and stop sign information which is significant for later penalty-based Imitation learning.\\
\textbf{Front view Semantics}. Front-view semantic segmentation has four different channels. We define $y_f$ as the ground truth 3D tensor with the dimension $H_f \times W_f \times 4$ and $\hat{y}_f$ as the output of the front view decoder with the same shape.  \\
\textbf{Top-down View Semantics}. Like front-view semantic segmentation, top-down-view semantic segmentation also has four channels. We define $y_{td}$ as the ground truth 3D tensor with the dimension $H_{td} \times H_{td} \times 4$ and $\hat{y}_{td}$ as the output of the top-down view decoder with the same shape. \\
\textbf{Image-LiDAR alignment loss}. This loss aims to align the shared semantic features of Image and LiDAR. We use a common L2-loss to align these features. \\
\textbf{Traffic Light Classification}. The output of the traffic light decoder should be a vector of 4 which indicates these four states red light, yellow light, green light, and none in the current frame. We then define $y_l$ as the ground truth traffic light vector of length 4 and $\hat{y}_l$ as the output of the traffic light decoder with the same shape. \\
\textbf{Stop Sign Classification}. The output of the stop sign decoder should have a vector of 2 which indicates if a stop sign exists in the current frame. The ground truth stop sign vector of length 2 and the output of the stop sign decoder with the same shape are defined as $y_s$ and $\hat{y}_s$, respectively. 
Based on what we defined above, the new loss function is given by:
\begin{equation}
\begin{aligned}
        \mathcal{L} = & \sum_{t=1}^{T} ||w_t - w_t^{gt}||_1 + \omega_f \mathcal{L}_{\rm CE}(y_{f},\hat{y}_{f}) + \\
        & \omega_{td} \mathcal{L}_{\rm CE}(y_{td},\hat{y}_{td}) + \omega_{l} \mathcal{L}_{\rm CE}(y_{l}, \hat{y}_{l}) +  \\ 
        & \omega_{s} \mathcal{L}_{\rm CE}(y_{s}, \hat{y}_{s}) + \omega_{a} \mathcal{L}_{2}(y_{s}, \hat{y}_{s}) 
\end{aligned}
\end{equation}
 where $\mathcal{L}_{CE}$ and $\mathcal{L}_2$ are the cross entropy loss and L2 loss, respectively. $\omega_f$, $\omega_{td}$, $\omega_{l}$,  $\omega_s$, $\omega_a$ are the weights for these auxiliary losses.


\subsection{Penalty-based Imitation Learning}
We found that the objective function design for imitation learning and the autonomous driving metric are not unified which means a low loss of the objective function does not guarantee a high driving score and high route completion. After careful study, we figure out there exist two potential reasons.

\begin{itemize}
    \item The expert agent still makes mistakes when generating the dataset. Sometimes, the expert agent runs a red light and violates the stop sign rule.
    \item The objective function is not sensitive to serious violations of the traffic rules, i.e. the violation of red lights and stop signs. The average objective function loss may not increase too much when violating the traffic rules despite that this violation may cause serious consequences which result in a huge drop in driving score and route completion.
\end{itemize}

Behavior Cloning (BC), as an imitation learning method, aims to clone the behavior of the expert agent. In such a way, the performance of the trained agent can no longer be better than the expert agent. If the expert agent makes a mistake, the trained agent will learn how to make that mistake instead of getting rid of that mistake.

We intend to re-design the objective function of imitation learning based on the traffic rules, giving extra punishments (higher loss) when the agent outputs short-term future waypoints against the traffic rules during the training process.

The traffic rules can be modeled as constrained functions which refer to conditions of the optimization problem that the solution must satisfy. In our setting, we concentrate on two kinds of traffic rule violations and one common driving experience, namely red light violations, stop sign violations, and slowing down when turning, because these are the main problems we found in our vanilla imitation learning approach. We first define three corresponding penalties to quantify these violations.

\subsubsection{Red Light Penalty}
For the red light violation, we design a red light penalty as follows:
\begin{equation}
    {\mathcal{P}}_{\rm tl} = \mathbb{E}_{\mathcal{X}\sim \mathcal{D}}[\mathbbm{1}_{\rm red}\cdot\sum_{i=1}^{t}c_i\cdot {\rm max}\{0, w_{i} - \overline{p}\} ]
\end{equation}
where $w_{i}$ is the $i$-th predicted waypoints of the trained agent; $\overline{p}$ is the position of the stop line at the intersection. Both $w_{i}, \overline{p}$ are in the coordinate system of the ego car. $c_i$ is the weight parameter and $\sum_{i} c_i =1$.$\mathbbm{1}_{\rm red}$ indicates if a red light that influences the agent exists in the current frame. $\mathcal{X}$ is the input of the current frame and $\mathcal{D}$ is the whole data set. 

In the scenarios of red lights, an extra red light penalty is defined by the distances of the predicted waypoints beyond the stop line at the intersection. If the predicted waypoints are within the stop line, then the penalty remains zero. On the other hand, if the predicted waypoints are beyond the stop line, the sum of distances between those waypoints and the stop line will be calculated as the red light penalty. The additional information for the red light penalty calculation like traffic light information and stop line location is pre-processed and saved in each frame of our dataset.

\subsubsection{Stop Sign Penalty}
Similar to the red light penalty, a stop sign penalty is given when the predicted waypoints violate the stop sign rule. The penalty is formalized as follows:
% \begin{equation}
%     {\mathcal{P}}_{\rm ss}=\mathbb{E}_{X \sim D}[\mathbbm{1}_{stopsign \land v > \epsilon}\cdot \sum_{i=1}^{n} c_i \cdot min\{0, p_{i, pred}-p_{stopsign}\}]
% \end{equation}

\begin{equation}
    {\mathcal{P}}_{\rm ss}=\mathbb{E}_{\mathcal{X} \sim \mathcal{D}}[\mathbbm{1}_{\rm stopsign}\cdot  {\rm max}\{v - \epsilon, 0\}]
\end{equation}
where $v$ is the desired speed calculated by 
\begin{equation}
    v = \frac{w_{0} - w_{1}}{\Delta t}
    \label{eq: desired speed}
\end{equation}
$w_{0}$ and $w_{1}$ is the first and second predicted waypoint, and $\Delta t$ is the time interval between each frame when collecting the data. $\mathbbm{1}_{\rm stopsign}$ is an indicator for stop sign checking. If the vehicle drives into the area that a stop influences, this indicator turns to 1 otherwise it remains zero. $\epsilon$ is the maximum speed required to pass stop sign tests. 

\subsubsection{Speed Penalty}
A speed penalty will be applied if the agent attempts to turn at excessive speed. The motivation to introduce this speed is based on the common driving experience of human beings. Also, we observe the agent sometimes can not avoid hitting pedestrians when turning at high speed since it has less time to react. The penalty is formalized as:
\begin{equation}
    {\mathcal{P}}_{\rm sp}=\mathbb{E}_{\mathcal{X} \sim \mathcal{D}} [{\rm sin}(d\theta) \cdot {\rm max} \{v - v_{\rm lb}, 0\}]
\end{equation}
where $d\theta$ is the direction deviation between the current frame and the next frame. Like stop sign penalty, $v$ is defined in Equation  \ref{eq: desired speed}. $v_{\rm lb}$ is the speed lower bound. Speed under the lower bound will not be imposed by speed punishment. 
% where $p_{i,pred}$ is the i-th predicted waypoints; $\mathcal{S}$ is the drivable street area. $d(\cdot, \cdot)$ is the distance between the waypoint and the street. $c_i$ is the weight factor. 

% As the above formula demonstrated, an additional penalty is added if the predicted waypoints are out of the street. The penalty is calculated as the weighted sum of the distances between those out of the street waypoints and the street. 

With the help of these penalties, the constrained optimization can be formalized as:

\begin{equation}
\label{eq:constraint objective function}
\begin{aligned}
\min \quad & \mathcal{F} \\
\textrm{s.t.} \quad & \mathcal{P}_{\rm tl}, \mathcal{P}_{\rm ss},\mathcal{P}_{\rm sp} = 0\\
\end{aligned}
\end{equation}
where $\mathcal{F}$ is the objective function defined in Equation \ref{eq:objective function}.

The Lagrange multiplier strategy can be applied here. We introduce three Lagrange Multiplier $\lambda_1$, $\lambda_2$, $\lambda_3$ and the Lagrange function is defined by:
\begin{equation}
\begin{aligned}
\min \quad \mathcal{F} + \lambda_1 \mathcal{P}_{\rm tl}+ \lambda_2 \mathcal{P}_{\rm ss} +\lambda_3 \mathcal{P}_{\rm sp}
\end{aligned}
\end{equation}
This is the final objective function to optimize. For simplicity, these Lagrange multipliers $\lambda_1$, $\lambda_2$, $\lambda_3$ are considered fixed hyper-parameters. Well-chosen $\lambda_1$, $\lambda_2$, $\lambda_3$ are important for optimization. According to our experiments, too large $\lambda$ influences the behaviors in other scenarios while too smaller $\lambda$ is not powerful enough for the agent to obey the corresponding traffic rules. 

The red light indicator and stop sign indicator headers are important for the agent to learn from the stop sign and red light penalty because the information flow of the stop sign and red light helps the agent to build the logistic connection between behavior, environment, and punishment. 



\section{experiments}
\begin{table*}
    \begin{center}
        \begin{threeparttable}
        \caption{}
        \label{table: Experiment_Long}
        \begin{tabular}{ c | c  c  c | c  c  c  c  c  } 
             \hline
                Model & \makecell[c]{Driving \\ Score \\ \%, $\uparrow$} & \makecell[c]{Route \\ Complication \\ \%, $\uparrow$} & \makecell[c]{Infraction \\ Score \\ $\uparrow$} &  \makecell[c]{Collision \\ Pedestrian \\ \#/km, $ \downarrow$}  & \makecell[c]{Collision \\ Vehicle \\ \#/km, $ \downarrow$} & \makecell[c]{Collision \\ Static \\ \#/km, $ \downarrow$} & \makecell[c]{Red light \\ Infraction \\ \#/km, $ \downarrow$} & \makecell[c]{Stop Sign Infraction \\ Infraction \\ \#/km, $ \downarrow$}\\
            \hline
            \hline
                P-CSG (ours) & $\textbf{56.38} \pm 4.18$ & $\textbf{94.00} \pm 1.75$ & $\textbf{0.61} \pm 0.05$ & $\textbf{0.00} \pm 0.00$ & $\textbf{0.08} \pm 0.02$ & $\textbf{0.00} \pm 0.00$ & $0.03 \pm 0.01$ & $0.01 \pm 0.01$ \\
                TransFuser & $34.50 \pm 2.54$ & $61.16 \pm 4.75$ & $0.56 \pm 0.06$ & $0.01 \pm 0.01$ & $0.58 \pm 0.07$ & $0.38 \pm 0.05$ & $0.12 \pm 0.03$ & $0.05 \pm 0.02$\\
                TransFuser+ & $36.19 \pm 0.90$ & $70.13 \pm 6.80$ & $0.51 \pm 0.03$ & $\textbf{0.00} \pm 0.00$ & $0.40 \pm 0.13$ & $0.04 \pm 0.03$&$0.11 \pm 0.10$ & $0.04 \pm 0.01$ \\
                InterFuser & $50.64 \pm 3.51 $ & $89.13 \pm 4.12$ & $0.57 \pm 0.05$  & $\textbf{0.00} \pm 0.00$ & $0.09 \pm 0.04$ & $0.01 \pm 0.01$ & $\textbf{0.02} \pm 0.01$ & $0.04 \pm 0.01$\\
                Geometric Fusion & $31.30 \pm 5.2$  & $57.17 \pm 11.6$ & $0.54 \pm 0.04$ & $0.01 \pm 0.01$ & $0.43 \pm 0.08$ & $0.02 \pm 0.01$ & $0.11 \pm 0.02$ & $0.08 \pm 0.04$\\
                LateFusion & $32.43 \pm 6.72$ & $60.41 \pm 4.23$ & $\textbf{0.61}\pm 0.06 $ & $0.03 \pm 0.02$ & $0.12 \pm 0.03$ & $0.02 \pm 0.01$ & $0.06 \pm 0.02$ & $0.06 \pm 0.02$ \\
                Expert & $34.47 \pm 3.25$ & $79.631 \pm 11.29$ & $0.46 \pm 0.08$ & $0.02 \pm 0.01$ & $0.40 \pm 0.22$ & $\textbf{0.00} \pm 0.00$ & $0.13 \pm 0.07$ & $\textbf{0.00} \pm 0.00$ \\

             \hline
        \end{tabular}
        \begin{tablenotes}
            \small
            \item We evaluate the Driving Score, Route Complication, Infraction Score, collisions, red light violations, and stop sign violations in the Town05 long benchmark. Note that all other baselines are retrained and tested in our local CARLA environment. 
        \end{tablenotes}
        \end{threeparttable}
    \end{center}
    
\end{table*}

\begin{table*}
    \begin{center}
        \begin{threeparttable}
        \caption{}
        \label{table: Ablation Study}
        \begin{tabular}{ c | c  c  c | c  c  c  c  c } 
             \hline
                Model & \makecell[c]{Driving \\ Score \\ \%, $\uparrow$} & \makecell[c]{Route \\ Complication \\ \%, $\uparrow$} & \makecell[c]{Infraction \\ Score \\ $\uparrow$} &  \makecell[c]{Collision \\ Pedestrian \\ \#/km, $ \downarrow$}  & \makecell[c]{Collision \\ Vehicle \\ \#/km, $ \downarrow$} & \makecell[c]{Collision \\ Static \\ \#/km, $ \downarrow$} & \makecell[c]{Red light \\ Infraction \\ \#/km, $ \downarrow$} & \makecell[c]{Stop Sign Infraction \\ Infraction \\ \#/km, $ \downarrow$}\\
            \hline
            \hline
                P-CSG (ours) & $\textbf{56.38} \pm 4.18$ & $\textbf{94.00} \pm 1.75$ & $\textbf{0.61} \pm 0.05$ & $\textbf{0.00} \pm 0.00$ & $\textbf{0.08} \pm 0.02$ & $\textbf{0.00} \pm 0.00$ & $\textbf{0.03} \pm 0.01$ & $\textbf{0.01} \pm 0.01$ \\
                No CSG & $42.767 \pm 7.78$ & $83.23 \pm 1.86$ & $0.51 \pm 0.11$ & $0.03 \pm 0.01$ & $0.15 \pm 0.04$ & $0.04 \pm 0.03$ & $0.04 \pm 0.00 $ & $0.02 \pm 0.00$\\
                No penalty & $34.98 \pm 5.64$ & $76.20 \pm 13.43$ & $0.51 \pm 0.18 $ & $0.01 \pm 0.00$ & $0.15 \pm 0.11$ & $0.03 \pm 0.01$ & $0.05 \pm 0.02$ & $0.05 \pm 0.01$ \\
                $\lambda_1 = 0.3$ & $37.19 \pm 8.87$ & $82.40 \pm 1.60$ & $0.48 \pm 0.09$ & $0.01 \pm 0.00$ & $0.11 \pm 0.03$ & $0.04 \pm 0.01$ & $0.06 \pm 0.01$ & $\textbf{0.01} \pm 0.00$\\
                $\lambda_1 = 0.1$ & $45.02 \pm 5.54$ & $69.35 \pm 1.90$ & $0.70 \pm 0.07$ & $0.01 \pm 0.01$ & $0.09 \pm 0.07$ & $\textbf{0.00} \pm 0.00$ & $0.04 \pm 0.01$ & $\textbf{0.01} \pm 0.00$\\
                $\lambda_2 = 0.005$ & $53.20 \pm 7.02$ & $87.55 \pm 5.73$ & $0.62 \pm 0.10 $ & $0.03 \pm 0.01$ & $0.07 \pm 0.01$ & $\textbf{0.00} \pm 0.00$ & $0.05 \pm 0.02$ & $0.02 \pm 0.02$\\
                $\lambda_2 = 0.05$ & $49.43 \pm 6.73$ & $93.87 \pm 5.51$ & $0.53 \pm 0.02$ & $0.01 \pm 0.01$ & $0.09 \pm 0.04$ & $\textbf{0.00} \pm 0.00$ & $0.05 \pm 0.02$ & $\textbf{0.01} \pm 0.00 $ \\
                $\lambda_3 = 0.3$  & $47.30 \pm 3.58$ & $92.21 \pm 4.52$ & $0.51 \pm 0.07$ & $\textbf{0.00} \pm 0.00$ & $0.11 \pm 0.02$ & $\textbf{0.00} \pm 0.00$ & $0.05 \pm 0.02$ & $0.03 \pm 0.01$ \\
                $\lambda_3 = 0.7$  & $47.35 \pm 4.97$ & $94.45 \pm 3.95$ & $0.50 \pm 0.06$ & $0.01 \pm 0.01$ & $0.13 \pm 0.03$ & $\textbf{0.00} \pm 0.00$ & $0.05 \pm 0.00$ & $\textbf{0.01} \pm 0.01$ \\
             \hline
        \end{tabular}
        \begin{tablenotes}
            \small
            \item The ablation study for important hyper-parameters. $\lambda_1, \lambda_2, \lambda_3$ are the penalty weight for the red light, speed, and stop sign respectively. The model with no cross-semantics generation structure (No CSG) and no penalty are also given for comparison.
        \end{tablenotes}
        \end{threeparttable}
    \end{center}
    
\end{table*}
In this section, our experiment setup will first be described. Then we compare our model against other baselines. We also provide ablation studies to show the improvements from penalty-based imitation learning and cross semantics generation. 

\subsection{Task Description}
The task we concentrate on is a navigation task along the predefined routes in different scenarios and areas. There exists GPS signals guiding the vehicle. Low signal or no signal situations are not taken into consideration. Some predefined scenarios will appear in each route to test the agent's ability to avoid the emergencies, such as obstacle avoidance, other vehicles running a red light, and the sudden appearance of pedestrians on the road. There exists a time limit for the agent to complete the route. Time exceeding is considered a failure in terms of route completion. 
\subsection{Training Dataset}
Realistic driving data is hard to achieve. Alternatively, we use the Carla simulator \cite{Dosovitskiy17} to collect the training data processed by the expert policy. We use the same training dataset as TransFuser \cite{TransFuser}. It includes 8 towns and around 2500 routes through junctions with an average length of 100m and about 1000 routes along curved highways with an average length of 400m. We used the expert agent same as TransFuser to generate these training data. 

\subsection{Metrics}
The metrics we use to evaluate the behavior of each are Route completion and Infraction Scores.
\subsubsection{Route Completion}
Route completion(RC) refers to the proportion of the completed route out of the whole route. Suppose $R_i$ means the route completion proportion in route $i$. The RC can be defined by:
\begin{equation}
    RC = \frac{1}{N} \sum_{i}^{N} R_i
\end{equation}
 
\subsection{Infraction Score}
Infraction Score (IS) is used to measure the driving behavior of the agent. We define $p_j$ as the penalty for an infraction instance, $j$ is incurred by the agent and $n_j$ is the number of occurrences of infraction instance $j$. Then the Infraction Score can be defined by:
\begin{equation}
    IS = \prod_{j}^{{\rm Ped, Veh, Stat, Red, Stop}} p_j^{n_j}
\end{equation}
The infraction instances include collision with a pedestrian, collision with a vehicle, collision with static layout, and red light violations. The penalties for them are 0.5, 0.60, 0.65, 0.7, and 0.8, respectively. 

\subsection{Driving Score}
The driving Score (DS) aims to measure the overall driving performance. It is defined by the weighted average of the route completion with an infraction multiplier:
\begin{equation}
    DS = \frac{1}{N} \sum_{i} ^{N} R_i P_i
\end{equation}
where $R_i$ and $P_i$ are the route completion and infraction score for $i$-th test instance.

\subsection{Infractions per km}
Infractions per km refer to the average infraction numbers per kilometer. The infractions include collisions with pedestrians, vehicles, and static elements, running a red light, off-road infractions, route deviations, timeouts, and vehicle blocks. 
\begin{equation}
    {\rm Infractions\ per\ km} = \frac{\sum_{i}^{N} \# \ {\rm infractions}_i}{\sum_{i}^{N} k_i}
\end{equation}
where $k_i$ is the driven distance (in km) for route $i$. Note that the Off-road infraction is handled differently. The total km driven out of the road is used instead of the number of infractions. 

\subsection{Test Result}
\subsubsection{Benchmark}
We use Town05 long benchmarks to evaluate our model. Town05 long benchmark contains 10 routes and all of these routes are over 2.5km. This benchmark is also used by InterFuser \cite{shao2022interfuser} and TransFuser.
\subsubsection{Baseline}
The other baselines we chose to compare with our model are TransFuser+, TransFuser, Geometric Fusion, and LateFusion. \textbf{TransFuser} \cite{TransFuser} introduces the Transformer into the multi-sensor fusion architecture to achieve better end-to-end autonomous driving results. \textbf{TransFuser+} \cite{TransFuser+}, as an extension of TransFuser, leverages several auxiliary losses to ensure important information flows such as traffic light and road line information in the network. \textbf{InterFuser} \cite{shao2022interfuser} developed a safety control module to regulate the behaviors of the agent, preventing the agent violate the traffic rules. \textbf{LateFusion} \cite{latefusion} uses a simple Multi-Layer Perception Network to integrate multi-modal information. \textbf{Geometric Fusion} \cite{TransFuser} implements both LiDAR-to-image and Image-to-LiDAR fusion to aggregate the information from LiDAR and image to increase the end-to-end autonomous driving ability. 

As Table \ref{table: Experiment_Long} shows, our model achieves the highest route completion and driving scores among all baselines. Thanks to our penalty-based imitation learning mechanism, our model performs even better than the Carla expert whose behaviors are used as ground truth for imitation learning. Compared to TransFuser, Transfuser+, and LateFusion, our model has a huge increase in driving scores and route complications. InterFuser, the current state-of-the-art model, performs well because its safety module avoids dangerous behavior inferred by the neural networks. However, this structure modularizes the decision-making process and these conflicted acts of the safety module and the neural network may have potential risks. Another disadvantage of modular approaches is that the predefined inputs and outputs of individual sub-systems might not be optimal for the driving task in different scenarios. \cite{DBLP:journals/corr/abs-2003-06404} analyses the end-to-end approaches and modular approaches of autonomous driving in detail. 
In contrast to InterFuser, we intend to restrict the behaviors of the agent by introducing penalties to the objective function so that the whole autonomous driving process remains end-to-end. As the results demonstrate, our penalty-based imitation learning can also avoid dangerous behaviors of the agent and make the agent more sensitive to the traffic rules. It achieves even better performance than InterFuser. 

\subsection{Ablation Study}
In this section, we will analyze the influences of different penalty weights for corresponding traffic rules. As Table \ref{table: Ablation Study} demonstrates, two extra weights for each penalty are selected for comparison. We also provide the result of models without CSG and without penalties for comprehensive analysis. 
We notice that the infractions of traffic lights and stop signs are largely reduced by adding penalties. Our proposed multi-sensor fusion technology (CSG) also decreases the possibility of hitting obstacles such as vehicles, pedestrians, and other statics. 
The results of different penalty weights are also listed in the table for comparison. Note that the default weights $\lambda_1$, $\lambda_2$, and $\lambda_3$ we choose for our best model are 0.5, 0.01, and 0.5 respectively.



\section{Discussion}
In this work, we introduce novel approaches for both Multi-sensor Fusion and Imitation Learning objective function design. The Cross Semantic Generation approach aims to extract and enhance the shared semantic information from LiDAR and RGB inputs. We used some auxiliary losses to regularize the feature space, ensuring the information flow of the features which are important for driving decisions according to human experience. Penalty-based Imitation Learning further increases the level of compliance of the agent with traffic rules. Some other approaches use an extra module to ensure the agent obeys traffic rules.  NEAT \cite{Chitta2021ICCV(NEAT)}, LAV \cite{chen2022lav} introduce some low-level control strategies in the PID controller to force braking at red lights. InterFuser uses a safety module to avoid dangerous actions such as collisions with other vehicles. These strategies largely increase the performance of the agent. However, these extra modules also make the network no longer end-to-end. With penalty-based Imitation Learning, we aim to avoid those decisions detached from the network. We use the penalty to make the agent more sensitive to traffic rules. The end-to-end nature of the network is guaranteed while constraining the agent to comply with traffic regulations. 

To the best of our knowledge, it is the first time to introduce the penalty concept in imitation learning. Given that our proposed method is highly scalable, it would be interesting to extend the penalties based on new traffic rules such as collisions with pedestrians and vehicles. 

It is also interesting to leverage multi-frame inputs to identify the violations of traffic regulations in the time dimension and introduce the corresponding penalty. For instance, the agent may drive on the line for long periods. 

Our study also has several limitations. First of all, we only use front-view images and 180-degree LiDAR data as input. The information from the rear of the vehicle is missing which may cause collisions when changing the lane. Besides, we only tried to integrate the RGB and LiDAR input, more sensors like Radar input and Depth-map can be taken into consideration. Finally, we only test the performance of our model in the simulation environment. Real-world data can be more complex and contain more noise. Also, the top-down and front-view semantic segmentation is generated by the Carla simulator. Label quality is hardly achievable in the real world. A fine-tuned semantic segmentation model is required to pre-process the raw data in the real world. 

\section{Conclusion}
The key points for the performance of end-to-end autonomous driving are improving fusion technologies and policy learning methods. These two points turn into two important questions. How to efficiently extract and integrate the features from different modalities? How to effectively use these features to learn a stable and well-performing policy approaching or even surpassing the human level. In this paper,  we contribute to the abovementioned aspects and achieve state-of-the-art performance. Compared to modular autonomous driving technologies, end-to-end autonomous driving has lower hardware costs and less expensive maintenance. It is also adaptable to different scenarios simply by feeding data. We believe end-to-end autonomous driving can be deployed in actual vehicles shortly.

\section{Acknowledgement}
This work was supported by Huawei Trustworthy Technology and Engineering Laboratory. We would like to thank Yingxian Li for making a detailed survey in the field of end-to-end autonomous driving and multi-sensor fusion. We also thank Prof. Fengxiang Ge and Wei Cao for the insightful discussion.

{
\small
\bibliographystyle{IEEEtran}
\bibliography{reference}
}

\end{document}
