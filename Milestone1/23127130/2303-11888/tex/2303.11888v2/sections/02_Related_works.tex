\section{related works}
\subsection{End-to-End Autonomous Driving}
Today's autonomous driving technologies have two main branches, modular and end-to-end approaches. Modular approaches apply a fine-grained pipeline of software modules working together to control the vehicle. In contrast, the entire pipeline of end-to-end driving is treated as one single learning task. End-to-end approaches have shown great success in computer vision tasks, such as object detection \cite{7112511} \cite{Fast_RCNN} \cite{Faster_RCNN} \cite{redmon2016you}, object tracking \cite{bras√≥2020learning}, and semantic segmentation \cite{ronneberger2015u}\cite{DBLP:journals/corr/CicekALBR16}. The success of these tasks builds a solid foundation for end-to-end autonomous driving. It is reasonable to believe end-to-end approaches are capable of solving autonomous driving problems in the near future. 
%A remarkable advantage of end-to-end autonomous driving is its sustainable learning capability. The well-designed model can achieve better performance by feeding more data in different scenarios. Imagine millions of vehicles on the road every day, tons of data can be collected every second. After processing, these data can be used as training material for the model to improve its performance. 
The most common learning methods for end-to-end autonomous driving are imitation learning \cite{TransFuser}, \cite{latefusion}, \cite{Filos2020CanAV}, \cite{Chitta2021ICCV(NEAT)}, \cite{chen2022lav}, \cite{8813900} and reinforcement learning  \cite{DBLP:journals/corr/abs-2001-08726} \cite{CPRL}.

\subsection{Safety Mechanism in End-to-End Autonomous Driving}
In the realm of autonomous driving, a key challenge is implementing safety mechanisms that can prevent accidents and protect passengers, pedestrians, and other road users. Within the framework of imitation learning, the agent learns driving skills by emulating expert demonstrations. The quality of these demonstrations has a significant impact on the agent's ability to drive safely in traffic. To improve the safety of the autonomous driving agent, researchers in \cite{TransFuser+} focus on enhancing the quality of the expert agent, while those in \cite{shao2022interfuser} introduce an additional safety module that filters out potentially dangerous driving behaviors generated by the network. Our contribution is to introduce the ``Penalty'' concept to the imitation learning framework, which incentivizes the trained agent to adopt safer driving behaviors.

\subsection{Multi-sensor Fusion Technologies}
Sensor Fusion technologies are commonly employed for 3D object detection and motion forecasting. Among the various types of sensors that can be integrated, the fusion of LiDAR and camera sensors is most frequently employed, where LiDAR data serves as a supplement to image data, providing additional information about the surrounding environment and improving data reliability due to its consistency in various environments. There are three branches of sensor fusion: early fusion, middle fusion, and late fusion. In early fusion, the data is fused before being fed into the learnable system, which is the most efficient approach. In middle fusion, the information is merged in the middle of the network, and the fused features are used to produce task-specific outputs. Late fusion is an ensemble learning method that combines the outputs generated by each modality into a final result. 

Multi-sensor fusion has received much research attention in the field of end-to-end autonomous driving. Prior works such as LateFusion\cite{latefusion} used a large Multi-Layer Perception (MLP) network to process the features extracted by the perception networks of LiDAR and RGB inputs. This MLP layer takes the tasks of features weighting, selection, and fusion which makes it hard to 
capture a global context of multi-modality inputs. TransFuser\cite{TransFuser} provides an approach that leverages the attention mechanism to fuse the LiDAR and RGB information. They used the transformer architecture to achieve the multi-modality global context. The Transformer-based fusion model is applied to different resolutions between the LiDAR and RGB perception networks. TransFuser+ \cite{TransFuser+}, as an extension of TransFuser, introduced more headers in the neural networks which incorporate four auxiliary tasks: depth prediction and semantic segmentation from the image branch; HD map prediction, and vehicle object detection from the BEV branch. These auxiliary tasks help to visualize the black box of the whole network. In addition, this approach also guarantees important information flow in the latent space because the information contained in the latent space should not only be able to complete the navigation task but also manually pre-defined auxiliary tasks.
