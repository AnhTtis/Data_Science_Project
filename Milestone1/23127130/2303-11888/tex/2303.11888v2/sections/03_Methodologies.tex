\section{Methodologies}

In this section, we propose a novel multi-sensor fusion approach and a penalty-based Imitation Learning paradigm for end-to-end autonomous driving.
\subsection{Problem Setting}
The task we concentrate on is point-to-point navigation in an urban setting where the goal is to complete a route with safe reactions to dynamic agents such as moving vehicles and pedestrians. The traffic rules should also be followed. 

\textbf{Imitation Learning (IL):} Imitation Learning can learn a policy $\pi$ that clone the behavior of an expert policy $\pi^*$. In our setup, the policy is conditioned on the multi-modalities inputs of current observations. We used the Behavior Clone (BC) approach of IL. An expert policy is applied in the environment to collect a large dataset $\mathcal{D}=\{(\textbf{x}^i, \textbf{w}^i)\}_{i=1}^{Z}$ with the size of $Z$, which contains the observation of the environment $\textbf{x}^i$ and a set of waypoints $\textbf{w}^i$ in the future timesteps. The objective function is defined as:
\begin{equation}
    \label{eq:objective function}
    \mathcal{F} = \mathbb{E}_{(\mathcal{X},\mathcal{W})\sim \mathcal{D}}[\mathcal{L}(\mathcal{W}, \pi(\mathcal{X}))]
\end{equation}
where $\mathcal{L}$ is the loss function. 

In our setting, the observation $\mathcal{X}$ consists of one RGB image and one LiDAR point cloud from the current time step. We used only one single frame since other works since \cite{DBLP:journals/corr/abs-1905-06937}, \cite{DBLP:journals/corr/abs-1812-03079} have shown that using multiple frames does not improve the information gain much. A PID controller $\mathcal{I}$ is applied to perform low-level control, i.e. steer, throttle, and brake based on these predicted future waypoints. 

\textbf{Global Planner:} According to CARLA \cite{Dosovitskiy17} 0.9.10's protocol, the high-level goal locations $G$ is provided as GPS coordinates. This goal location $G$ is sparse (hundreds of meters apart) which can only be used as guidance. In contrast, those to be predicted waypoints are dense, only a few meters way away from each other. 

\begin{figure*}[ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/Graph2.pdf}
	\caption{\textbf{Architecture.} The top-down LiDAR pseudo image and front camera image go through two residual networks to extract 512 dimension feature vectors. We use four different MLPs to extract the shared features and the unique features. The unique features of RGB input are used to generate stop signs and traffic light indicators. The shared features of LiDAR are used to reconstruct the segmentation of RGB input while the shared features of RGB are used to reconstruct the segmentation of top-down LiDAR input. An alignment loss is used to align the shared features from LiDAR and camera inputs into the same space. These shared features and unique features are concatenated along with the measurements (velocity, throttle, steer, brake from the last frame) and then go through one MLP to reduce the size. Finally, they will be fed into one GRU decoder to predict short-term waypoints.}
	\label{fig:Architecture}
\end{figure*}

\subsection{Cross Semantics Generation}

The motivation of our approach is based on the fact that multi-modal inputs have shared semantic information and also unique information. For instance, the geometric attributes and spatial coordinates of both vehicles and pedestrians are shared information that can be extracted from both LiDAR and RGB input. Figure \ref{fig:Shared semantic features} demonstrates the shared information of LiDAR and RGB input. The unique information refers to the complementary information that other inputs do not have. In the case of RGB input, unique information often pertains to features such as the color of traffic lights, patterns on traffic signs, and similar attributes. On the other hand, in the context of LiDAR input, unique information pertains to spatial relationships of objects. Our multi-sensor fusion approach aims to extract and align the shared features from LiDAR and RGB input sources so that the later decision network can leverage the organized features to achieve better performance.

To extract the shared information from LiDAR and RGB inputs, we propose cross semantics generation sensor fusion. As Figure \ref{fig:Architecture} demonstrates, the front RGB and top-down pre-processed LiDAR pseudo images will first be fed into two residual networks \cite{DBLP:journals/corr/HeZRS15} to extract the corresponding RGB and LiDAR features. Note that the LiDAR point cloud is pre-processed into the bird's eye view pseudo images which is the same setting as \cite{TransFuser}. We use four different linear layers to extract the shared features and unique features of LiDAR and RGB. The shared features of  RGB are used to generate the top-down semantic segmentation align with LiDAR input; The shared features of LiDAR are used to generate the semantic segmentation of corresponding RGB input. We refer this approach as cross semantics generation since the information from one modality is utilized to generate semantic representations of the other modality. In this way, the information flow is said to be 'crossed', as each modality contributes to the understanding of the other. The extracted shared features will maximized since the information derived from one modality should strive to generate an accurate semantic segmentation of the other modality to the best of its ability. An extra L2 loss is introduced to align the shared features of RGB and LiDAR into the same latent space. In our setup, the semantic segmentation contains 4 channels, the drivable area, the non-drivable area, the object in the drivable areas like vehicles and pedestrians, and others. In terms of the unique features from RGB input, we mainly concentrate on the traffic lights and stop signs. As we can see from the figure, the unique features from RGB input are used to train the traffic light and stop sign indicator which ensures the important information flows of traffic lights and stop signs in the neural network. These headers are also critical for later penalty-based Imitation which we will discuss in the following sections.

\subsection{Waypoint Prediction Network}
As shown in Figure \ref{fig:Architecture}, all the unique and shared features are concatenated into a 512-dimensional feature vector. This vector is fed into an MLP to reduce the dimension to 64 for computational efficiency reasons. The hidden layer of GRU is initialized with a 64-dimensional feature vector. GRUâ€™s update gate controls the information flow from the hidden layer to the output. In each timestep, it also takes the current location and goal location as input. We follow the approach of \cite{Filos2020CanAV} that a single GRU layer is followed by a linear layer which takes the state of the hidden layer and predicts the relative position of the waypoint compared to the previous waypoint for $T=4$ time-steps. Hence, the predicted future waypoints are formed as $\{w_t = w_{t-1} + \delta w_t\}_{t=1}^T$. The start symbol for GRU is given by (0,0). 

\textbf{Controller}: Based on the predicted waypoints, we use two PID controllers for lateral and longitudinal directions respectively. We follow the settings of \cite{chen2019lbc}.

\subsection{Loss Functions}
Similar to previous works \cite{TransFuser}, \cite{chen2019lbc}, we also use $L_1$ loss as our reconstruction loss. For each input, the loss function can be formalized as:
\begin{equation}
    \mathcal{L} = \sum_{t=1}^{T} ||w_t - w_t^{gt}||_1
\end{equation}
where $w_t$ is the t-th predicted waypoints and $w_t^{gt}$ is the t-th ground truth waypoint produced by the expert policy. 

\textbf{Auxiliary Tasks}: In our cross semantics generation approach, we have four extra auxiliary tasks along with the main imitation learning task. As we explained in the above section, two of the auxiliary tasks are semantic segmentation. In order to ensure some important information flow in the network, we introduce two extra classification headers, namely traffic light classification and stop sign classification. These two headers help the neural network to capture traffic light and stop sign information which is significant for later penalty-based Imitation learning.

\textbf{Front View Semantics}. Front-view semantic segmentation has four different channels. We define $y_f$ as the ground truth 3D tensor with the dimension $H_f \times W_f \times 4$ and $\hat{y}_f$ as the output of the front view decoder with the same shape.  

\textbf{Top-down View Semantics}. Like front-view semantic segmentation, top-down-view semantic segmentation also has four channels. We define $y_{td}$ as the ground truth 3D tensor with the dimension $H_{td} \times H_{td} \times 4$ and $\hat{y}_{td}$ as the output of the top-down view decoder with the same shape. 

\textbf{Image-LiDAR Alignment Loss}. This loss aims to align the shared semantic features of Image and LiDAR into the same latent space. We use an L2-loss to align these features. 

\textbf{Traffic Light Classification}. The output of the traffic light decoder should be a vector of 4 which indicates these four states red light, yellow light, green light, and none in the current frame. We then define $y_l$ as the ground truth traffic light vector of length 4 and $\hat{y}_l$ as the output of the traffic light decoder with the same shape. 

\textbf{Stop Sign Classification}. The output of the stop sign decoder should have a vector of 2 which indicates if a stop sign exists in the current frame. The ground truth stop sign vector of length 2 and the output of the stop sign decoder with the same shape are defined as $y_s$ and $\hat{y}_s$, respectively. 
Based on what we defined above, the new loss function is given by:
\begin{equation}
\begin{aligned}
        \mathcal{L} = & \sum_{t=1}^{T} ||w_t - w_t^{gt}||_1 + \omega_f \mathcal{L}_{\rm CE}(y_{f},\hat{y}_{f}) + \\
        & \omega_{td} \mathcal{L}_{\rm CE}(y_{td},\hat{y}_{td}) + \omega_{l} \mathcal{L}_{\rm CE}(y_{l}, \hat{y}_{l}) +  \\ 
        & \omega_{s} \mathcal{L}_{\rm CE}(y_{s}, \hat{y}_{s}) + \omega_{a} \mathcal{L}_{2}(y_{s}, \hat{y}_{s}) 
\end{aligned}
\end{equation}
 where $\mathcal{L}_{CE}$ and $\mathcal{L}_2$ are the cross entropy loss and L2 loss, respectively. $\omega_f$, $\omega_{td}$, $\omega_{l}$,  $\omega_s$, $\omega_a$ are the weights for these auxiliary losses.


\subsection{Penalty-based Imitation Learning}
\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/three_penalties.pdf}
	\caption{\textbf{Penalty Illustration.} To ensure compliance with red light and stop penalty rules, as well as promoting deceleration during turning maneuvers, our approach incorporates three distinct penalty types. The first column of the figures exemplifies the red light penalty, wherein waypoints situated beyond the stop line receive a penalty when the traffic light is red. In the second column, we demonstrate the stop sign penalty, wherein predicted waypoints within the vicinity of a stop sign are penalized if the agent fails to decelerate adequately. The speed penalty is enforced during turning actions as shown in last two figures. Specifically, if the predicted waypoints indicate an excessive speed, a speed penalty is imposed. }
	\label{fig:penalties}
\end{figure*}
We found that the objective function design for imitation learning and the autonomous driving metric are not unified which means a low loss of the objective function does not guarantee a high driving score and high route completion. After careful study, we figure out there exist two potential reasons.

\begin{itemize}
    \item The expert agent still makes mistakes when generating the dataset. Sometimes, the expert agent runs a red light and violates the stop sign rule.
    \item The objective function is not sensitive to serious violations of the traffic rules, i.e. the violation of red lights and stop signs. The average objective function loss may not increase too much when violating the traffic rules despite that this violation may cause serious consequences which result in a huge drop in driving score and route completion.
\end{itemize}

Behavior Cloning (BC), as an imitation learning method, aims to clone the behavior of the expert agent. In such a way, the performance of the trained agent can no longer be better than the expert agent. If the expert agent makes a mistake, the trained agent will learn how to make that mistake instead of getting rid of that mistake.

Our aim is to reformulate the objective function of imitation learning in line with traffic rules, whereby the agent is penalized (higher loss) when it generates short-term future waypoints that violate the traffic rules during the training process.

The traffic rules can be modeled as constrained functions which refer to conditions of the optimization problem that the solution must satisfy. In our setting, we concentrate on two kinds of traffic rule violations and one common driving experience, namely red light violations, stop sign violations, and slowing down when turning, because these are the main problems we found in our vanilla imitation learning approach. We first define three corresponding penalties to quantify these violations. Figure \ref{fig:penalties} illustrates these penalties. 

\subsubsection{Red Light Penalty}
For the red light violation, we design a red light penalty as follows:
\begin{equation}
    {\mathcal{P}}_{\rm tl} = \mathbb{E}_{\mathcal{X}\sim \mathcal{D}}[\mathbb{1}_{\rm red}\cdot\sum_{i=1}^{t}c_i\cdot {\rm max}\{0, w_{i} - \overline{p}\} ]
\end{equation}
where $w_{i}$ is the $i$-th predicted waypoints of the trained agent; $\overline{p}$ is the position of the stop line at the intersection. Both $w_{i}, \overline{p}$ are in the coordinate system of the ego car. $c_i$ is the weight parameter and $\sum_{i} c_i =1$.$\mathbb{1}_{\rm red}$ indicates if a red light that influences the agent exists in the current frame. $\mathcal{X}$ is the input of the current frame and $\mathcal{D}$ is the whole data set. 

In the scenarios of red lights, an extra red light penalty is defined by the distances of the predicted waypoints beyond the stop line at the intersection. If the predicted waypoints are within the stop line, then the penalty remains zero. On the other hand, if the predicted waypoints are beyond the stop line, the sum of distances between those waypoints and the stop line will be calculated as the red light penalty. The additional information for the red light penalty calculation like traffic light information and stop line location is pre-processed and saved in each frame of our dataset.

\subsubsection{Stop Sign Penalty}
Similar to the red light penalty, a stop sign penalty is given when the predicted waypoints violate the stop sign rule. The penalty is formalized as follows:
% \begin{equation}
%     {\mathcal{P}}_{\rm ss}=\mathbb{E}_{X \sim D}[\mathbb{1}_{stopsign \land v > \epsilon}\cdot \sum_{i=1}^{n} c_i \cdot min\{0, p_{i, pred}-p_{stopsign}\}]
% \end{equation}

\begin{equation}
    {\mathcal{P}}_{\rm ss}=\mathbb{E}_{\mathcal{X} \sim \mathcal{D}}[\mathbb{1}_{\rm stopsign}\cdot  {\rm max}\{v - \epsilon, 0\}]
\end{equation}
where $v$ is the desired speed calculated by 
\begin{equation}
    v = \frac{w_{0} - w_{1}}{\Delta t}
    \label{eq: desired speed}
\end{equation}
$w_{0}$ and $w_{1}$ is the first and second predicted waypoint, and $\Delta t$ is the time interval between each frame when collecting the data. $\mathbb{1}_{\rm stopsign}$ is an indicator for stop sign checking. If the vehicle drives into the area that a stop influences, this indicator turns to 1 otherwise it remains zero. $\epsilon$ is the maximum speed required to pass stop sign tests. 

\subsubsection{Speed Penalty}
A speed penalty will be applied if the agent attempts to turn at excessive speed. The motivation to introduce this speed is based on the common driving experience of human beings. Also, we observe the agent sometimes can not avoid hitting pedestrians when turning at high speed since it has less time to react. The penalty is formalized as:
\begin{equation}
    {\mathcal{P}}_{\rm sp}=\mathbb{E}_{\mathcal{X} \sim \mathcal{D}} [{\rm sin}(d\theta) \cdot {\rm max} \{v - v_{\rm lb}, 0\}]
\end{equation}
where $d\theta$ is the direction deviation between the current frame and the next frame. Like stop sign penalty, $v$ is defined in \eqref{eq: desired speed}. $v_{\rm lb}$ is the speed lower bound. Speed under the lower bound will not be imposed by speed punishment. 
% where $p_{i,pred}$ is the i-th predicted waypoints; $\mathcal{S}$ is the drivable street area. $d(\cdot, \cdot)$ is the distance between the waypoint and the street. $c_i$ is the weight factor. 

% As the above formula demonstrated, an additional penalty is added if the predicted waypoints are out of the street. The penalty is calculated as the weighted sum of the distances between those out of the street waypoints and the street. 

With the help of these penalties, the constrained optimization can be formalized as:

\begin{equation}
\label{eq:constraint objective function}
\begin{aligned}
\min \quad & \mathcal{F} \\
\textrm{s.t.} \quad & \mathcal{P}_{\rm tl}, \mathcal{P}_{\rm ss},\mathcal{P}_{\rm sp} = 0\\
\end{aligned}
\end{equation}
where $\mathcal{F}$ is the objective function defined in \eqref{eq:objective function}.

The Lagrange multiplier strategy can be applied here. We introduce three Lagrange Multiplier $\lambda_1$, $\lambda_2$, $\lambda_3$ and the Lagrange function is defined by:
\begin{equation}
\begin{aligned}
\min \quad \mathcal{F} + \lambda_1 \mathcal{P}_{\rm tl}+ \lambda_2 \mathcal{P}_{\rm ss} +\lambda_3 \mathcal{P}_{\rm sp}
\end{aligned}
\end{equation}
This is the final objective function to optimize. For simplicity, these Lagrange multipliers $\lambda_1$, $\lambda_2$, $\lambda_3$ are considered fixed hyper-parameters. Well-chosen $\lambda_1$, $\lambda_2$, $\lambda_3$ are important for optimization. According to our experiments, too large $\lambda$ influences the behaviors in other scenarios while too smaller $\lambda$ is not powerful enough for the agent to obey the corresponding traffic rules. 

The red light indicator and stop sign indicator headers are important for the agent to learn from the stop sign and red light penalty because the information flow of the stop sign and red light helps the agent to build the logistic connection between behavior, observation, and punishment. 