\subsection{Training}
\label{sec:method_training}
We supervise the training of \netName{} with five losses. The first three are \textbf{RGB Loss}, \textbf{Mask Loss}, and \textbf{Eikonal Loss}, the same as used in previous neural rendering works~\cite{wang2021neus,yariv2020idr}.
% \brandon{Add ref here}. 
They are defined as
\vspace{-0.5em}
\begin{equation}
    \mathcal{L}_{rgb} = \sum_{i, j}||\imagePred(i,j) - \imageGT(i,j)||\cdot \maskGT(i,j)
    \vspace{-3mm}
\end{equation}
\begin{equation}
    \mathcal{L}_{mask} = \sum_{i, j}BCE(\maskPred(i,j), \maskGT(i,j))
    \vspace{-2mm}
\end{equation}
\begin{equation}
    \mathcal{L}_{eikonal} = \frac{1}{N}\sum_{\textbf{p}}(|\frac{\partial f(\textbf{p})}{\partial \textbf{p}}| - 1)^{2}
    \vspace{-1.5mm}
\end{equation}
where BCE is the binary cross entropy.

% \weikai{Add more insights to the following loss functions.}

\vspace{-0.8em}
\paragraph{Rendering Probability Loss}
In the physical world, the existence of surfaces is binary (exist/not exist). As a result, the validity probability of a 3D sampling point is either 0 (with no surface) or 1 (with surface). We therefore add the binary cross entropy of $\vldty(\textbf{p})$ as an extra regularization:
\vspace{-2mm}
\begin{equation}
    \mathcal{L}_{bce} = \frac{1}{N}\sum_{\textbf{p}} BCE(\vldty(\textbf{p}), \vldty(\textbf{p})).
    \vspace{-3mm}
\end{equation}

\input{Figure/comparison_watertight/comparison_watertight}

\vspace{-1em}
\paragraph{Rendering Probability Regularization}
For real-world objects with open structures, the surfaces are sparsely distributed in the 3D space. To prevent \netName{} from predicting redundant surfaces, we introduce a sparsity loss to promote the formation of open surfaces:
\vspace{-2mm}
\begin{equation}
    \mathcal{L}_{sparse} = \frac{1}{N}\sum_{\textbf{p}}\vldty(\textbf{p}).
    \vspace{-3mm}
\end{equation}

% \weikai{We need a definition of the final loss function, with detailed weight for each loss component.}
\noindent We optimize the following loss function
\vspace{-1.5mm}
\begin{align}
\begin{split}
\mathcal{L} &= \mathcal{L}_{rgb}
            + \lambda_{mask} \cdot \mathcal{L}_{mask}
            + \lambda_{eikonal} \cdot \mathcal{L}_{eikonal}\\
            & + \lambda_{bce} \cdot \mathcal{L}_{bce} 
            + \lambda_{sparse} \cdot \mathcal{L}_{sparse}.
\end{split}
\vspace{-1.5mm}
\end{align}
% where $\lambda_{mask} = 0.3$, $\lambda_{eikonal} = 0.1$, $\lambda_{bce} = 0.1$, $\lambda_{sparse} = 0.05$. We use the same parameters in all experiments unless otherwise mentioned.

