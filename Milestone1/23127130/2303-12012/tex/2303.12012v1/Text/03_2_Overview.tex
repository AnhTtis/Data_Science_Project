\subsection{Formulation}
\label{sec:method_overview}
Given N images ${\imageGT (k)}_{k=1}^N$ with a resolution of $(W,\ H)$ together with corresponding camera intrinsics, extrinsics, and object masks ${\maskGT (k)}_{k=1}^N$, our goal is to reconstruct the surface of the object. 
%\brandon{In other papers and the dataset, do all camera parameters available or estimated? How to they deal with wild images? Can we also do it? If yes, we need to revise the input formulation.}
%\xiaoxu{Some methods are able to estimate camera parameters in the optimization process, such as NeuS and IDR. However, the visual effect is not as good as the reconstruction with cameras.As suggested by Weikai, We didn't try the reconstruction without camera parameters, since the conclusion will be similar to SOTA.}
The framework of our method is shown in Figure~\ref{fig:pipeline}. Given a sampled pixel $\mathbf{o}$ on an input image, we project it to the 3D space and get the sampled 3D points on the ray emitting from the pixel as 
$\left\{\mathbf{p}\left(t\right)=\mathbf{o}+t\mathbf{v}\ \middle|\ t\geq0\right\}$,
where $\mathbf{o}$ is the center of the camera and $\mathbf{v}$ is the unit direction vector of the ray. 
Then, we predict the signed distance value $f(\mathbf{p}(t))$, validity probability $\vldty(\mathbf{p}(t))$, 
and the RGB value $\clr(\mathbf{p}(t))$ of the points by our fully connected neural networks called \netName. Specifically, \netName~includes:
\begin{itemize}
	\item SDF-Net: a mapping function $f(\cdot):\mathbf{R}^3\rightarrow \mathbf{R}$ to represent the signed distance field. 
% 	We borrowed the implementation of the sdf prediction net of IDR~\cite{yariv2020idr}.
	\item Validity-Net: a mapping function $\vldty(\cdot):\mathbf{R}^3\rightarrow \mathbf{R}$ to represent the validity probability; 
% 	the multilayer perceptron consists of 8 layers. We used the non-linear maps of [10] to improve the learning of high-frequency details. In Validity-Net, we used the ReLU activation between hidden layers and Sigmoid activation for the output.
	\item Color-Net: a mapping function $\clr(\cdot):\mathbf{R}^3\rightarrow \mathbf{R}^3$ to predict the per-point color of the 3D space.
% 	We borrowed the implementation of the color prediction net of IDR~\cite{yariv2020idr}.
\end{itemize}

The outputs of the three networks are delivered to our novel \modelName~renderer to render images and masks from the implicit representations. Our renderer supports both open and closed surfaces, and therefore it provides the capability of reconstructing arbitrary shapes.

{
The predicted mask $\maskPred$ could be inferred from the rendering weights $\wt$ for each sampling point, and the predicted image $\imagePred$ could be calculated from the RGB $\clr(\pt(t))$ and the rendering weights $\wt$:
\vspace{-1em}
\begin{equation}
    \label{equ:pred_imgs}
    \begin{aligned}
        &\maskPred(\mathbf{o}, \mathbf{v}) = \int_{0}^{+\infty}\wt dt,
        \\
        &\imagePred(\mathbf{o}, \mathbf{v}) = \int_{0}^{+\infty}\wt \clr(\pt(t)))dt.
    \end{aligned}
    \vspace{-0.5em}
\end{equation}
}
The predicted masks and images are used for loss calculation during training, which will be illustrated in Section \ref{sec:method_training}. After training is completed, we go through the testing module as shown in Figure \ref{fig:pipeline}; we set the SDFs to NAN for 3D points with $\vldty(\textbf{p})$ less than 0.5, and feed them to Marching Cubes algorithm to produce the final mesh.