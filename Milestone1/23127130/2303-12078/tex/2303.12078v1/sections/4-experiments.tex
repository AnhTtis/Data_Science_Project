\input{tables/youtubevos}
\input{tables/davis_vos}
\section{Experiments}
\label{sec:expr}
\subsection{Experimental setup}
\noindent{\textbf{Datasets.}}
We conduct experiments on widely used VOS benchmarks including DAVIS 2016/2017~\cite{perazzi2016benchmark,pont20172017} and YouTube-VOS 2018/2019~\cite{xu2018youtube}.
DAVIS 2017 is a multi-object extension of DAVIS 2016, which consists of 60 (138 objects) and 30 (59 objects) videos for training and validation respectively.
YouTube-VOS is a larger-scale multi-object dataset with 3471 videos from 65 categories for training. These training videos are annotated every five frames. There are 474 and 507 videos in the 2018 and 2019 validation splits respectively. 
In our two-shot setting, we randomly select two labeled frames per video as labeled data while the remaining ones are served as unlabeled data.
Compared to full set, we only use $7.3\%$ and $2.9\%$ labeled data for YouTube-VOS and DAVIS, respectively.

\noindent{\textbf{Evaluation metric.}}
Following common practice
% ~\cite{}, 
\cite{oh2019video,cheng2021rethinking,cheng2022xmem},
for the DAVIS datasets, we adopt the standard metrics: region similarity $\mathcal{J}$, contour accuracy $\mathcal{F}$ and their average $\mathcal{J} \& \mathcal{F}$. For the YouTube-VOS datasets, we report $\mathcal{J}$ and $\mathcal{F}$ of the seen and unseen categories, and their averaged score $\mathcal{G}$.


\noindent{\textbf{Implementation details.}}
We implement our method with PyTorch~\cite{paszke2017automatic}.
For phase-1 training, we adopt the STCN~\cite{cheng2021rethinking} pre-trained on static image datasets~\cite{wang2017learning,shi2015hierarchical,zeng2019towards} with synthetic deformations. 
The parameter $K$ in random frame skipping is gradually increased from 5 to 25 with a curriculum learning schedule. The threshold $\tau_{1}$ is set to $0.9$.
The training paradigm of two-shot VOS can be seamlessly applied to various VOS models in phase-2 training. We explore STCN~\cite{cheng2021rethinking}, RDE-VOS~\cite{li2022recurrent} and XMem~\cite{cheng2022xmem}, respectively. The threshold $\tau_{2}$ is set to $0.99$.


\input{tables/phase_ablation}
\input{figures/figure_threshold}


\subsection{Main results}
We apply our two-shot VOS to STCN~\cite{cheng2021rethinking}, RDE-VOS~\cite{li2022recurrent}, and XMem~\cite{cheng2022xmem}, and compare the results with 1): their counterparts trained on full sets; (2) their counterparts trained on two-shot datasets without using unlabeled data; 
(3) other strong baselines trained on full sets.
When training a naive 2-shot model in a fully supervised manner, we repeatedly sample the labeled frames to meet the input requirement of that model. We report the results on YouTube-VOS and DAVIS validation sets in \cref{table:youtubevos} and \cref{table:davisvos}, respectively. From the Tables, we could draw two conclusions: (1) Two labeled frames per video are almost sufficient for training a pleasant VOS model—even the unlabeled data are unused. For example, 2-shot STCN already achieves 80.8\% score on YouTube-VOS 2018 benchmark, which is only -2.2\% lower than the full-set STCN achieving 83.0\% score. (2) By using 7.3\% and 2.9\% labeled data of YouTube-VOS and DAVIS benchmarks, our approach achieves comparable results in contrast to the counterpart trained on full set, and outperforms the native 2-shot counterpart by large margins. For instance, 2-shot STCN equipped with our approach achieves 85.1\%/82.7\% on DAVIS 2017/YouTube-VOS 2019, which is +4.1\%/+2.1\% higher than the naive 2-shot STCN while -0.1\%/-0.0\% lower than the full-set STCN.


\input{tables/phase_one_ablation}
\input{tables/alpha_ablation}

\subsection{Ablation study}
In this section, we validate the proposed two-shot VOS training strategy step-by-step.
All ablation studies are conducted on Youtube-VOS 2019 by applying our approach to STCN~\cite{cheng2021rethinking}. More analysis can be found in our supplementary material.

\noindent \textbf{Effects of each phase.} The results are shown in \cref{tab:phase_ablation}.  Starting from a naive 2-shot STCN (denoted as ``baseline'' afterward) which achieves 80.6$\%$ score, phase-1 training improves the score to 81.6$\%$. On top of this, phase-2 training further enhances performance to 82.7$\%$, leading to the same performance of STCN trained on fully labeled set. 

\noindent \textbf{Thresholds of pseudo-labeling.} There are two hyper-parameters $\tau_1$ and $\tau_2$ controlling pseudo-labeling in phase-1 and -2, respectively. \cref{fig:thres} displays two accuracy curves by varying $\tau_1$ and $\tau_2$. Using a higher threshold guarantees the quality of generated pseudo labels but yields less amount of pseudo data, and vice versa. We adopt a higher threshold in phase-2 training since the predictions in phase-2 are more accurate than that in phase-1. It can be seen that $\tau_1=0.9$ and $\tau_1=0.99$ yield the best result. 




\noindent \textbf{Different pseudo labelers.} \cref{tab:pseudo-labeler} ablates the effects of using different pseudo-labelers in phase-1. Specifically, we propose two variants: (1) STCN model itself; (2) STCN with a mean teacher~\cite{tarvainen2017mean} strategy. The underlying idea behind Mean Teacher (MT) is that using an exponential moving average (EMA) strategy to update the parameters of the model at each iteration, which can be formulated as:  
$\theta_{t}^{'} = \alpha \theta_{t-1}^{'} + (1 - \alpha)\theta_{t}$,
where $t$ denotes the current iteration, $\theta_{t}^{'}$ and $\theta_{t}$ denote the parameters of MT-STCN and STCN respectively,
and $\alpha$ is a weight. It can be seen that using the MT-STCN model surpasses
the one without MT strategy. We further ablate $\alpha$ in \cref{tab:alpha_ablation}. We find that $\alpha = 0.995$ yields the best performance. However, we do not employ MT strategy in phase-2 since no performance improvement is observed. 


\input{tables/bidirectional_ablation}
\input{tables/label_bank_ablation}

\noindent \textbf{Bidirectional inference.} We adopt an intermediate inference to construct a pseudo-label bank to enable phase-2 training. We compare the proposed bidirectional inference with the unidirectional inference, which is typically used in most VOS models. The results are shown in \cref{tab:inter_infer}. There is a +0.6$\%$ improvement when utilizing bidirectional inference versus unidirectional inference. The reasons are that: (1) some unlabeled frames are not associated with the pseudo labels in the unidirectional inference; 
(2) the bidirectional inference alleviates the error propagation issue.


\noindent \textbf{Dynamically update the pseudo-label bank.} We verify the effectiveness of dynamically updating the pseudo-label bank during phase-2 training, by comparing \yankun{it} with a variant that freezes the pseudo-label bank once constructed. As shown in \cref{tab:label_bank}, freezing the pseudo-label bank slightly hurt the performance. As training progresses, more accurate pseudo labels are generated, thus it is optimal to update the pseudo-label bank to further promote the learning. 

\noindent \textbf{Visualization of feature space.} We randomly pick two unlabeled frames from the constructed 2-shot YouTube-VOS 2019 training set for feature space visualization. Note we could access their annotations (foreground and background) from the full set. We use PCA to visualize the feature space of naive 2-shot STCN, 2-shot STCN with our training paradigm, and full-set STCN in \cref{fig:feature_vis}. Both 2-shot STCN equipped with our methodology, and full-set STCN show more compact clusters.



\subsection{Discussion}


\noindent \textbf{How about more shots?} We conduct experiments under the $4$-shot and $6$-shot settings. We apply our approach to $4$- and $6$-shot STCN and conduct one round of phase-1 training. Two models achieve the performance of 82.0$\%$ and 82.1$\%$ on YouTube-VOS 2019, respectively. We further conduct one round of phase-2 training. Both models achieve 82.7$\%$ on YouTube-VOS 2019, which is the same as that of 2-shot STCN equipped with our method—the performance is already saturated for two-shot VOS and acquiring more labeled data may not be beneficial.

\noindent \textbf{Robustness of our approach.} To verify the robustness of our approach, we independently construct five 2-shot VOS datasets from YouTube-VOS 2019 benchmark and train a 2-shot STCN with our methodology on each set. The results are $[$82.69$\%$, 82.70$\%$, 82.72$\%$, 82.72$\%$, 82.73$\%$$]$, with an average of 82.71$\%$ and a standard deviation of 0.015$\%$, showing the robustness of our approach.

\input{figures/feature_vis}
