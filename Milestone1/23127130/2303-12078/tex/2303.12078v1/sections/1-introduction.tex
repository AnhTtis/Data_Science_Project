\section{Introduction}
\label{sec:intro}


\input{figures/teaser.tex}

Video object segmentation (VOS), also known as mask tracking, aims to segment the target object in a video given the annotation of the reference (or first) frame. Existing approaches~\cite{oh2019video,yang2020collaborative,seong2021hierarchical,cheng2021rethinking,xie2021efficient,li2022recurrent,cheng2022xmem} are trained on densely annotated datasets such as DAVIS~\cite{perazzi2016benchmark,pont20172017} and YouTube-VOS~\cite{xu2018youtube}. However, acquiring dense annotations, particularly at the pixel level, is laborious and time-consuming. For instance, the DAVIS benchmark consists of 60 videos, each with an average of 70 labeled frames; the YouTube-VOS dataset has an even larger amount of videos, and every fifth frame of each video is labeled to lower the annotation cost. It is necessary to develop data-efficient VOS models to reduce the dependency on labeled data.


In this work, we investigate the feasibility of training a satisfactory VOS model on sparsely annotated videos. For the sake of convenience, we use the term $N$-shot to denote that $N$ frames are annotated per training video. Note that $1$-shot is meaningless since it degrades VOS to the task of image-level segmentation. We use STCN~\cite{cheng2021rethinking} as our baseline due to its simplicity and popularity. Since at least two labeled frames per video are required for VOS training, we follow the common practice to optimize a naive $2$-shot STCN model on the combination of YouTube-VOS and DAVIS, and evaluate on YouTube-VOS 2018/2019 and DAVIS 2016/2017, respectively. We compare the native $2$-shot STCN with its counterpart trained on full set in~\cref{fig:teaser-b}. Surprisingly, 2-shot STCN still achieves decent results, for instance, only a $-2.1\%$ performance drop is observed on YouTube-VOS 2019 benchmark, demonstrating the practicality of $2$-shot VOS.

So far, the wealth of information present in unlabeled frames is yet underexplored. In the last decades, semi-supervised learning, which combines a small amount of labeled data with a large collection of unlabeled data during training, has achieved considerable success on various tasks such as image classification~\cite{sohn2020fixmatch,berthelot2019mixmatch}, object detection~\cite{sohn2020simple,xu2021end} and semantic segmentation~\cite{hu2021semi,ke2020guided}. In this work, we also adopt this learning paradigm to promote $2$-shot VOS (see~\cref{fig:teaser-a}). The underlying idea is to generate credible pseudo labels for unlabeled frames during training and to optimize the model on the combination of labeled and pseudo-labeled data. 
% To this end, we propose a two-stage training strategy to make full use of unlabeled frames. In \textit{phase-1}, 
Here we continue to use STCN~\cite{cheng2021rethinking} as an example to illustrate our design principle, nevertheless, our approach is compatible with most VOS models. Concretely, STCN takes a randomly selected triplet of labeled frames as input but the supervisions are only applied to the last two—VOS requires the annotation of the first frame as reference to segment the object of interest that appeared in subsequent frames. This motivates us to utilize the ground-truth for the first frame to avoid error propagation during early training. Each of the last two frames, nevertheless, can be either a labeled frame or an unlabeled frame with a high-quality pseudo label. 
Although the performance is improved with this straightforward paradigm, the capability of semi-supervised learning is still underexplored due to the restriction of employing the ground truth as the starting frame. 
We term the process described above as \textit{phase-1}.

% In \textit{phase-2}, 
To take full advantage of unlabeled data, we lift the restriction placed on the starting frame, allowing it to be either a labeled or pseudo-labeled frame. To be specific, we adopt the VOS model trained in \textit{phase-1} to infer the unlabeled frames for pseudo-labeling. After that, each frame is associated with a pseudo label that approximates the ground-truth. The generated pseudo labels are stored in a pseudo-label bank for the convenience of access. The VOS model is then retrained without any restrictions—similar to
how it is trained through supervised learning, but each frame has either a ground-truth or a pseudo-label attached to it. It is worth noting that, as training progresses, the predictions become more precise, yielding more reliable pseudo labels—we update the pseudo-label bank once we identify such pseudo labels. 
The above described process is named as \textit{phase-2}. 
As shown in \cref{fig:teaser-b}, our approach assembled onto STCN, achieves comparable results (\textit{e.g.} 85.2\% v.s 85.1\%  on DAVIS 2017, and 82.7\%  v.s 82.7\%  on YouTube-VOS 2019) in contrast to its counterpart, STCN trained on full set, though our approach merely accesses 7.3\% and 2.9\% labeled data of YouTube-VOS and DAVIS benchmark, respectively.

% \input{figures/overview.tex}

Our contributions can be summarized as follows:
\begin{itemize}
    \item For the first time, we demonstrate the feasibility of two-shot video object segmentation: two labeled frames per video are almost sufficient for training a decent VOS model, even without the use of unlabeled data.
    \item We present a simple yet efficient training paradigm to exploit the wealth of information present in unlabeled frames. This novel paradigm can be seamlessly applied to various VOS models, \textit{e.g.}, STCN~\cite{cheng2021rethinking}, RDE-VOS~\cite{li2022recurrent} and XMem~\cite{cheng2022xmem} in our experiments.
    \item  Though we only access a small amount of labeled data (\textit{e.g.} $7.3\%$ for YouTube-VOS and $2.9\%$ for DAVIS), our approach still achieves competitive results in contrast to the counterparts trained on full set. For example, 2-shot STCN equipped with our approach achieves 85.1$\%$/82.7$\%$ on DAVIS 2017/YouTube-VOS 2019, which is +4.1$\%$/+2.1$\%$ higher than the naive 2-shot STCN while -0.1$\%$/-0.0$\%$ lower than the STCN trained on full set.
\end{itemize}

 
