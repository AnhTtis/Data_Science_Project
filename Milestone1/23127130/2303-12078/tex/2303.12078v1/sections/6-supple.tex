
\section{One-shot VOS}
One-shot VOS is meaningless as it degrades VOS to the task of image-level segmentation. However, we can still train a VOS model by augmenting a single labeled frame into a video clip. Experimentally, we train a naive one-shot STCN, which achieves 72.9$\%$ score on YouTube-VOS 2019 and performs -7.7$\%$ lower than its native two-shot counterpart. In addition, we randomly sample half of the training videos from YouTube-VOS 2019 dataset and train another native two-shot STCN model on this subset. This two-shot STCN model, which uses the same amount of labeled data as one-shot STCN, achieves 79.8$\%$ score, outperforming the one-shot counterpart by +6.9\%. This study verifies that it is non-trivial to directly apply our approach to the one-shot scenario because, if no adjustments are adopted to the existing VOS models, the learning of VOS necessitates at least two labeled frames. We leave the one-shot VOS for future research.


\section{Visualization}
\label{sec:supple}
\noindent \textbf{Visualization of feature space.} 
We randomly pick five unlabeled frames from the constructed 2-shot YouTube-VOS 2019~\cite{xu2018youtube} training set for feature space visualization. Note we could access their annotations (foreground and background) from the fully labeled set. We adopt PCA~\cite{jolliffe2016principal} to reduce the dimension of the pixel-wise features from 256 to 2. We visualize the feature space of naive 2-shot STCN, 2-shot STCN with our training paradigm, and full-set STCN in \cref{fig:feature_vis}. Both 2-shot STCN equipped with our methodology, and full-set STCN show more compact clusters.

\noindent \textbf{Visualization of mask prediction.} 
As shown in \cref{fig:mask_vis}, we visualize the mask predictions on three testing videos for several STCN variants: naive 2-shot STCN, 2-shot STCN with our training paradigm, and full-set STCN. The naive 2-shot STCN predicts pleasant masks, but fails to handle local details (\textit{e.g.} zebra tail in the second video). Please refer to the full video results in the \textit{``video''} folder in our supplemental material package.

\section{Limitations and future work}
\label{sec:limi}
Our 2-shot VOS methodology still shows a slight performance drop when applying to the methods that require more input frames during training~(\textit{e.g.} XMem~\cite{cheng2022xmem}). We will focus on addressing the problem of error propagation during pseudo-labeling in the future.
 
\input{figures/feature_vis_s}

\input{figures/mask_vis}
 
