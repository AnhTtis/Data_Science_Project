

% \subsection{Network Details}
\section{Implementation details} 


%\subsection{Data}

%\subsubsection{Localization: CALTECH-101 Airplanes}
%https://data.caltech.edu/records/mzrjq-6wc02                                                                                                      
%{\bf Localization data}
%\url{https://en.wikipedia.org/wiki/Caltech_101}

{\bf The data} for object localization is the airplane subset of Caltech-101
\cite{caltech_data}.
% More specifically, the airplanes subset was chosen to create a network
% for bounding box regression. In this subset,
There are in total 800 images of airplanes with the size of
300$\times$200 pixels.
%Annotation information is provided for each image, which includes the
%bounding box for the airplaine.
Each image is accompanied with four real-valued numbers that make up
the bounding box of the airplane.

%\subsubsection{Segmentation: ANDI Hippocampus Data}
% {\bf Segmentation data}

The image segmentation task in this work is to extract human brain
Hippocampi from magnetic resonance images (MRIs).  The data used were
obtained from the ADNI database (\url{http://adni.loni.usc.edu}), and
extracted in our previous work
\cite{chen2017hippocampus,chen2017accurate}. In
total, 110 base-line T1-weighted whole brain MRI images from different
subjects along with their hippocampus masks were downloaded.
% In our experiments we only included normal control subjects.
Due to the fact
that the hippocampus only occupies a very small part of the whole
brain and its position in the whole brain is relatively fixed, we
roughly cropped the right face of the hippocampus of each subject and use them as
the input for segmentation.  The size of the cropping box is $24
\times 56 \times 48$.
% The size of the
% cropping box for the left hippocampus is 24 x 48 x 40, and 24 x 56 x
% 48 for the right hippocampus.
% As shown in
%Fig. 1(b), this box is big enough to contain the whole hippocampus for
%each subject, but also not too tight to significantly reduce the
%difficulty for our segmentation task.




\subsection{Training and testing}
% \subsection{SNN Hyperparameters}

%{\bf SNN Hyperparameters}
%
%\subsubsection{Synaptic Smoothing} Another hyperparameter than
%NengoDL allows tuning for is the synapse parameter. This parameter
%aims to decrease the amount of noise of the spikes that are generated
%from the output of neurons. The noise of the spikes is caused by the
%fact that spikes are discrete events and exist for only a single time
%step. In order to compensate for the rapid fluctuation of spikes,
%NengoDL applies a smoothing to the spikes via the synapse
%parameter. This smoothing computes a running average of the neuron's
%output over a specified time window. The more smoothing that is
%applied, the longer the input images will have to be presented for in
%order to fully take the average of the spiking output over the
%desired time window.
%
% \subsubsection{Firing Rates}
In SNNs, {\it firing rates} play an important role in determining the
total energy consumption in the model as well as how information is
passed down throughout the network.
% If the firing rates are too low then it is
% possible that the layers will not be able to communicate with each
% other and cause the network to fail its overall task. On the other
% hand, if the firing rates are too high then the model loses out on the
% temporal sparsity advantage that SNNs offer which will cause the
% network to consume more energy.
%
%In NengoDL, there are two main methods that handle how the firing
%rates are determined in the network. The first method applies a linear
%scaling factor to each layer that impacts the overall firing rate of
%that layer. The second approach aims to regularize the firing rates of
%each layer directly during training.
%
%
NengoDL provides two different ways to set the firing rates at network
layers: {\it post-training scaling} and {\it regularizing during
  training}.  The former allows us to apply a linear scale to the
inputs of all the neurons and then scale down the outputs with the
same rate. The latter is used to directly optimize the firing rates towards
certain target values we specify.
%
%This is achieved by minimizing the
%MSE between the output activity of each layer and
%the pre-determined target rate.
%

% This is achieved by first
% determining a target rate that the neurons will fire at. Next, the
% mean squared error (MSE) loss function is computed between the output
% activity of each layer in the model and the pre-determined target
% rate. Finally, a loss weight is applied to each layer which determines
% how much each layer contributes to the total loss of the model.


%\begin{enumerate}
%	\item \textit{Firing Rate Scaling} \\ This first method is
%	achieved by applying a linear scaling factor to the input of
%	all neurons. The output of these neurons are also divided by
%	this same scale factor as well. This method is a simple way to
%	give a boost to the firing rates of all neurons in the
%	network. However, not all neurons fire at the same
%	rate. Applying a large scaling factor to a neuron that is not
%	spiking that often may be beneficial but applying that same
%	scale to a neuron that is spiking rapidly would not be. This
%	will cause the model to lose out on the temporal sparsity
%	advantage that SNNs offer which is why the second method for
%	manipulating firing rates during training is superior to this
%	method.  \item \textit{Firing Rate Regularization} \\ In this
%	method, the firing rates are directly regularized during
%	training of the ANN, and hybrid SNN as well. This is done by
%	computing the MSE loss function between the output activity of
%	each layer in the network and a specified target rate. This
%	effectively shifts the regularization point from 0 to the
%	target rate. This method ensures that all layers will spike at
%	around the same desired frequency, unlike in the previous
%	method. Moreover, this method works for non-linear activation
%	functions as well such as LIF neurons.
% \end{enumerate}

%\subsubsection{Presentation Time Steps} The final hyperparameter that
%NengoDL allows for tuning is the amount of time steps that each image
%is presented for. This is done by tiling each training and testing
%sample for a desired amount of times. The lower the presentation time
%steps, the faster NengoDL's SNN simulator will complete its inference
%on the test data. In the hybrid training step however, this parameter
%becomes increasingly important. For example, in the CAE Training
%section, each test sample is tiled 50 times. Running the SNN
%simulator on 500 test samples with each being tiled 50 times does not
%take too long. Although, training 4780 samples that are also tiled 50
%times for 10 epochs does take quite some time. Overall, this
%hyperparameter determines how long the hybrid and inference steps
%will take and in practice, one would want to present each image for
%as little time steps as possible while preserving the accuracy of the
%model.



%{\bf R-CNN training}
% \subsubsection{R-CNN Training}
{\bf LocNet training} In the experiments of LocNet,
% In our bounding box experiment,
we randomly chose 90\% of the data for
training while the other 10\% were used for testing. Our ANN model
was trained with NengoDL's soft LIF neurons with a smoothing factor of 0.005.
%The firing rates during training were regularized to fire at around 500 Hz.
The firing rates were {\it regularized during training} with a target
rate of 250 Hz. Different weights were assigned to the layers in the
regularization. As the output layer is the most important, we set its
weight to 1. Other layers are less important so we set each of their
loss weights to 0.01. Our ANN model was trained for 50 epochs using
the Adam optimizer with a learning rate of 0.01.
% In order to train the bounding box network, 90\% of the data was used
% for training while the other 10\% was used for testing. Our model was
% trained with NengoDL's soft LIF neurons in place of the ReLU
% activations with a smoothing factor of 0.005. The firing rates during
% training were regularized to fire at around 500 Hz using the method
% explained in the previous section. Using the mean squared error (MSE)
% loss function, this regularized ANN model was trained for 50 epochs
% using the Adam optimizer with a learning rate of 0.0001.

For hybrid training, the same firing rate regularization technique was
used to keep the neurons firing at around 250 Hz.
% like in the original ANN model.
The soft LIF neurons in ANN were replaced with our hybrid soft LIF
neurons. In addition to this change, a synapse of 0.005 seconds was
added to all connections in the network.
%Each image was also tiled 30 times in order to mimic each image being
%presented for 30 time steps when NengoDL's SNN simulator for inference
%with each time step being 0.001 seconds.
The hybrid model was trained for 7 epochs which was determined by
an early stopping process.
The learning rate was also decreased to 0.001.
% \subsubsection{CAE Training}
% {\bf CAE training}


% \subsubsection{CAE Training}
%During training of the CAE network, only the right hippocampus images
%and masks were used in this network.

{\bf CAE training} Our dataset to train our CAE network consists of
5280 2D images and masks of the right face of the hippocampus. 4780 of
these were used for training while 500 were used for testing. The ReLU
activations were swapped out for NengoDL's Rectified Linear neurons
during training. Firing rate scaling was used on these neurons with a
scaling factor of 1000. The ANN was trained for 100 epochs using the
Adam optimizer and the learning rate was set to 0.00001.

%using a combination
%of the binary cross entropy (BCE) loss as well as the Dice loss. The
%Adam optimizer was also used with a learning rate set to 0.00001.

%During training of the CAE network, only the right hippocampus images
%and masks were used in this network. The right-side dataset consists
%of 5280 images and masks of the hippocampus. 4780 of these were used
%for training while 500 were used for testing. The ReLU activations
%were swapped out for NengoDL's Rectified Linear neurons during
%training. Firing rate scaling was used on these neurons with a scaling
%factor of 1000. The ANN was trained for 100 epochs using a combination
%of the binary cross entropy (BCE) loss as well as the Dice loss. The
%Adam optimizer was also used with a learning rate set to 0.00001.


In the hybrid training step, the Rectified Linear neurons were
replaced with our hybrid Rectified Linear neurons.
% The same scaling factor of 1000 was also used on these neurons
% during training to keep things consistent.
A {\it post-training scaling} factor of 1000 was also used on both
neuron types during training to keep results consistent.  A synaptic
filter of 0.005 seconds was added to the output of all spiking
neurons. Finally, an early stopping technique was used and trained the
model for an additional 13 epochs.
%with the same optimizer.
%
%Finally, each training sample was tiled 50 times so that is was
%presented for 50 time steps. Each time step during training for this
%model is 0.01 seconds.

%\subsubsection{SNN Inference} NengoDL's SNN simulator was used on
%both models in order to run them as an SNN and gather their
%results. When using the simulator, the non-spiking neurons are
%swapped out for the spiking equivalent. In the R-CNN model, this
%means the soft LIF neurons were swapped out for the spiking LIF
%neurons while in the CAE model the Rectified Linear neurons were
%replaced by the Spiking Rectified Linear version. In order to keep
%things consistent with the hybrid training, the same hyperparameters
%that were used for each model were used when the model was ran using
%the simulator. This means that for the R-CNN, the synapse parameter
%for the simulator was set to 0.005 seconds and each test sample was
%tiled 30 times with each time step lasting for 0.001
%seconds. Furthermore, the CAE network uses the same parameters as it
%did during the hybrid training. This means that the synapse for the
%simulator was set to 0.005 seconds as well as tiling each test sample
%50 times. Additionally, the time step for the simulator was set to
%0.001 seconds.
