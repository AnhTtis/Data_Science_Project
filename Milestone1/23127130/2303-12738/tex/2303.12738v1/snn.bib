
@inproceedings{yue2023snn,
  title={Hybrid Spiking Neural Network Fine-tuning for Hippocampus
                  Segmentation}, 
  author={Yue, Ye and Baltes, Marc and Abujahar, Nidal and Sun, Tao and
                  Smith, Charles D and Trevor Bihl and Liu, Jundong},
  booktitle={ISBI},
  pages={},
  year={2023},
  organization={IEEE}
}

% colton's thesis, under major_references
@mastersthesis{smith2021evaluation,
  title={The Evaluation of Current Spiking Neural Network Conversion
                  Methods in Radar Data},
  author={Smith, Colton C},
  journal={Master thesis},
  year={2021},
  school={Ohio University}
}
@article{lee2016training,
  title={Training deep spiking neural networks using backpropagation},
  author={Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  journal={Frontiers in neuroscience},
  volume={10},
  pages={508},
  year={2016},
  publisher={Frontiers Media SA}
}
@article{li2021differentiable,
  title={Differentiable spike: Rethinking gradient-descent for
                  training spiking neural networks},
  author={Li, Yuhang and Guo, Yufei and Zhang, Shanghang and Deng, Shikuang and Hai, Yongqing and Gu, Shi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23426--23439},
  year={2021}
}
@article{wu2018spatio,
  title={Spatio-temporal backpropagation for training high-performance spiking neural networks},
  author={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Shi, Luping},
  journal={Frontiers in neuroscience},
  volume={12},
  pages={331},
  year={2018},
  publisher={Frontiers Media SA}
}
@article{shrestha2018slayer,
  title={Slayer: Spike layer error reassignment in time},
  author={Shrestha, Sumit B and Orchard, Garrick},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{diehl2016conversion,
  title={Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware},
  author={Diehl, Peter U and Zarrella, Guido and Cassidy, Andrew and Pedroni, Bruno U and Neftci, Emre},
  booktitle={2016 IEEE International Conference on Rebooting Computing (ICRC)},
  pages={1--8},
  year={2016},
  organization={IEEE}
}
@article{rueckauer2016theory,
  title={Theory and tools for the conversion of analog to spiking convolutional neural networks},
  author={Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael},
  journal={arXiv preprint arXiv:1612.04052},
  year={2016}
}
@article{rasmussen2019nengodl,
  title={NengoDL: Combining deep learning and neuromorphic modelling methods},
  author={Rasmussen, Daniel},
  journal={Neuroinformatics},
  volume={17},
  number={4},
  pages={611--628},
  year={2019},
  publisher={Springer}
}
{@article{kim2021beyond,
  title={Beyond classification: directly training spiking neural
                  networks for semantic segmentation},
  author={Kim, Youngeun and Chough, Joshua and Panda, Priyadarshini},
  journal={arXiv preprint arXiv:2110.07742},
  year={2021}
}
{@article{loihi2018,
  title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  journal={IEEE Micro}, 
  volume={38},
  number={1},
  pages={82-99},
  year={2018},
  doi={10.1109/MM.2018.112130359}}
  
@misc{caltech_data, title={Caltech 101}, DOI={10.22002/D1.20086},
                  abstractNote={Pictures of objects belonging to 101
                  categories. About 40 to 800 images per
                  category. Most categories have about 50
                  images. Collected in September 2003 by Fei-Fei Li,
                  Marco Andreetto, and Marc'Aurelio Ranzato. The size
                  of each image is roughly 300 x 200 pixels. We have
                  carefully clicked outlines of each object in these
                  pictures, these are included under the
                  'Annotations.tar'. There is also a MATLAB script to
                  view the annotations, 'show_annotations.m'.},
                  publisher={CaltechDATA}, author={Li and Andreeto and
                  Ranzato and Perona}, year={2022}, month={Apr}}

@ARTICLE{wu_tandem2021,
  author={Wu, Jibin and Chua, Yansong and Zhang, Malu and Li, Guoqi and Li, Haizhou and Tan, Kay Chen},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/TNNLS.2021.3095724}}
  
@article{rathi_fine_tuning2020,
  doi = {10.48550/ARXIV.2005.01807},
  
  url = {https://arxiv.org/abs/2005.01807},
  
  author = {Rathi, Nitin and Srinivasan, Gopalakrishnan and Panda, Priyadarshini and Roy, Kaushik},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% paper on kerasspiking "spike-aware" approach, similar to our hybrid network
@article{kerasspiking2020,
  doi = {10.48550/ARXIV.2002.03553},
  
  url = {https://arxiv.org/abs/2002.03553},
  
  author = {Voelker, Aaron R. and Rasmussen, Daniel and Eliasmith, Chris},
  
  keywords = {Machine Learning (cs.LG), Neurons and Cognition
                  (q-bio.NC), Machine Learning (stat.ML), FOS:
                  Computer and information sciences, FOS: Computer and
                  information sciences, FOS: Biological sciences, FOS:
                  Biological sciences},
  
  title = {A Spike in Performance: Training Hybrid-Spiking Neural
                  Networks with Quantized Activation Functions},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% citation for the soft/surrogate LIF neurons under nengoDL
@article{nengodl_softlif2015,
  doi = {10.48550/ARXIV.1510.08829},
  
  url = {https://arxiv.org/abs/1510.08829},
  
  author = {Hunsberger, Eric and Eliasmith, Chris},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary
                  Computing (cs.NE), FOS: Computer and information
                  sciences, FOS: Computer and information sciences},
  
  title = {Spiking Deep Networks with LIF Neurons},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% paper on spiking-yolo (first snn regression network we went over a while ago)
@article{spike_yolo2020, title={Spiking-YOLO: Spiking Neural Network
                  for Energy-Efficient Object Detection}, volume={34},
                  url={https://ojs.aaai.org/index.php/AAAI/article/view/6787},
                  DOI={10.1609/aaai.v34i07.6787}, number={07},
                  journal={Proceedings of the AAAI Conference on
                  Artificial Intelligence}, author={Kim, Seijoon and
                  Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
                  year={2020}, month={Apr.}, pages={11270-11277}}

% hybrid network, uses SNN for input, ANN for output, not sure if needed but is another variation of a hybrid SNN
@article{spike-flownet2020,
author="Lee, Chankyu
and Kosta, Adarsh Kumar
and Zhu, Alex Zihao
and Chaney, Kenneth
and Daniilidis, Kostas
and Roy, Kaushik",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Spike-FlowNet: Event-Based Optical Flow Estimation with Energy-Efficient Hybrid Neural Networks",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="366--382",
isbn="978-3-030-58526-6"
}

% paper we have referenced a lot, goes over snn encoding/decoding, architecture, and training (spike timing derivative, surrogate gradient, etc)
% also touches on rate coded/latency coded inputs
% goes over rate coded/latency coded objectives uses spike count, membrane potential, mean square spike rate, mean square membrane
@article{snn_overview2021,
  doi = {10.48550/ARXIV.2109.12894},
  
  url = {https://arxiv.org/abs/2109.12894},
  
  author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre and
                  Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish
                  and Bennamoun, Mohammed and Jeong, Doo Seok and Lu,
                  Wei D.},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Emerging
                  Technologies (cs.ET), Machine Learning (cs.LG), FOS:
                  Computer and information sciences, FOS: Computer and
                  information sciences},
  
  title = {Training Spiking Neural Networks Using Lessons From Deep Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


% 99.9th percentile approach on ann activations for ann-snn conversion
@ARTICLE{ann_percentile2017,
  
AUTHOR={Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},   
	 
TITLE={Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={11},           
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},       
	
DOI={10.3389/fnins.2017.00682},      
	
ISSN={1662-453X},   
}


% uses the spike-norm, or we called it "snn max" approach to weight conversion for ann-snn
@ARTICLE{snn-max2019,
  
AUTHOR={Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu,
                  Chiao and Roy, Kaushik},   
	 
TITLE={Going Deeper in Spiking Neural Networks: VGG and Residual
                  Architectures},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={13},           
	
YEAR={2019},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnins.2019.00095},       
	
DOI={10.3389/fnins.2019.00095},      
	
ISSN={1662-453X},   
   
ABSTRACT={Over the past few years, Spiking Neural Networks (SNNs) have
                  become popular as a possible pathway to enable
                  low-power event-driven neuromorphic
                  hardware. However, their application in machine
                  learning have largely been limited to very shallow
                  neural network architectures for simple problems. In
                  this paper, we propose a novel algorithmic technique
                  for generating an SNN with a deep architecture, and
                  demonstrate its effectiveness on complex visual
                  recognition problems such as CIFAR-10 and
                  ImageNet. Our technique applies to both VGG and
                  Residual network architectures, with significantly
                  better accuracy than the state-of-the-art. Finally,
                  we present analysis of the sparse event-driven
                  computations to demonstrate reduced hardware
                  overhead when operating in the spiking domain.}
}


% unsupervised approach using spike timing dependent plasticity
@ARTICLE{diehl2015_stdp,
  
AUTHOR={Diehl, Peter and Cook, Matthew},   
	 
TITLE={Unsupervised learning of digit recognition using spike-timing-dependent plasticity},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={9},           
	
YEAR={2015},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2015.00099},       
	
DOI={10.3389/fncom.2015.00099},      
	
ISSN={1662-5188},   
}


% weight normalization technique (model based and data based approach)
@article{diehl2015_conversion,
  author={Diehl, Peter U. and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing}, 
  year={2015},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN.2015.7280696}}


% nengodl unet paper on cell dataset, uses nengo loihi simulator
@article{patel2021spiking,
  title={A spiking neural network for image segmentation},
  author={Patel, Kinjal and Hunsberger, Eric and Batir, Sean and Eliasmith, Chris},
  journal={arXiv preprint arXiv:2106.08921},
  year={2021}
}



% loihi paper under major_references
@article{davies2021advancing,
  title={Advancing neuromorphic computing with loihi: A survey of
                  results and outlook},
  author={Davies, Mike and Wild, Andreas and Orchard, Garrick and
                  Sandamirskaya, Yulia and Guerra, Gabriel A Fonseca
                  and Joshi, Prasad and Plank, Philipp and Risbud,
                  Sumedh R},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={911--934},
  year={2021},
  publisher={IEEE}
}

% spyketorch paper udner major_references
@article{manna2022frameworks,
  title={Frameworks for SNNs: a review of data science-oriented
                  software and an expansion of SpykeTorch},
  author={Manna, Davide Liberato and Vicente Sola, Alex and Kirkland,
                  Paul and Bihl, Trevor Joseph and Di Caterina,
                  Gaetano},
  booktitle={International Conference On Neuromorphic Systems 2022},
  year={2022}
}

% Nature paper we went over before, goes over SNN approaches as well as hardward
@article{roy2019towards,
  title={Towards spike-based machine intelligence with neuromorphic
                  computing},
  author={Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
  journal={Nature},
  volume={575},
  number={7784},
  pages={607--617},
  year={2019},
  publisher={Nature Publishing Group}
}

% snn feature extraction under major_references
@article{sola2022keys,
  title={Keys to accurate feature extraction using residual spiking neural networks},
  author={Sola, Alex Vicente and Manna, Davide Liberato and Kirkland, Paul and Di Caterina, Gaetano and Bihl, Trevor J},
  journal={Neuromorphic Computing and Engineering},
  year={2022},
  publisher={IOP Publishing}
}

% snn review under major_references
@article{yamazaki2022spiking,
  title={Spiking neural networks and their applications: A Review},
  author={Yamazaki, Kashu and Vo-Ho, Viet-Khoa and Bulsara, Darshan and Le, Ngan},
  journal={Brain Sciences},
  volume={12},
  number={7},
  pages={863},
  year={2022},
  publisher={MDPI}
}

% paper on CMOS technology for energy estimation, not needed anymore
@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  pages={10--14},
  year={2014},
  organization={IEEE}
}
