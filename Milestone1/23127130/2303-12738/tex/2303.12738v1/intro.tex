\section{Introduction}



% DL (ANN) for computer vision \vspace{1in}\vspace{1in}

Over the past decade, deep artificial neural networks (ANNs) have
revolutionized many AI-related fields, including computer vision,
natural language processing, human speech processing, and
autonomy. This remarkable progress
% should in part be attributed to the
is in part due to the emerging of large amount of annotated data and
the widespread availability of high-performance computing devices and
general-purpose graphics processing units (GPUs).
% [Roy 2019] [IEEE 2021] [UK paper]
However, with this comes a huge computational and power burden, which
limits the application of ANNs in many tasks that require low size,
weight and power (SWaP) devices.  Spiking neural networks (SNNs) have
recently emerged as a low-power alternative,
% Spiking neural networks (SNNs) are % the third generation networks
whose neurons imitate the temporal and sparse spiking nature of
biological neurons \cite{roy2019towards,davies2021advancing}.
% In operation, they only “fire” when needed, which are sparser than
% traditional ANNs, leading to natural gains in SWaP.
SNN neurons consume energy only when spikes are generated,
% which are sparser than traditional ANNs,
leading to sparser activations and natural gains in SWaP.
% Current SNN approaches (ANN-SNN conversion, SNN training from
% scratch) and their drawbacks [SNN 2021 paper]

% Algorithms of training spiking neural networks broadly fall into two
% main categories: online approaches and offline approaches.
%Algorithms to train SNNs can be grouped into two main categories:
%online approaches and office approaches.  Online solutions train SNNs
%directly on neuromorphic hardware, where network parameters are
%updated in real-time and in place as data arrives. Offline approaches
%train models on CPUs or GPUs and later deploy the trained models to
%neuromorphic hardware.
%
% An SNN can be directly trained like RNNs \cite{24, 26},
% where a surrogate gradient needs to be used to conduct
% backpropagation.  Direct methods commonly suffer from expensive
% computation for the training process \cite{45}. ANN-to-SNN conversion
% methods, thus, have become the most efficient way to obtain a well-
% performed SNN. Methods like data-based normalization [32], [52] or
% threshold balancing [29], [32] study how to determine the spiking
% neurons configurations, e.g., number of time steps, threshold, and
% leaky factor. The major bottleneck of these prior arts is the costly
% inference latency required to preserve high accuracy.
%
 An SNN can be obtained by either converting from a fully trained ANN,
 or through a direct training procedure where a surrogate gradient
 needs to be used for the network to conduct backpropagation
 \cite{lee2016training}. ANN-to-SNN conversion methods, while
 effective, often require sophisticated thresholding or normalization
 procedures to determine the spiking neuron configurations
 \cite{smith2021evaluation,diehl2015_conversion,rueckauer2016theory,
   snn-max2019,
   %diehl2016conversion,
   yue2023snn}. Direct training solutions commonly suffer from
 expensive computation burdens on complex network architectures
 \cite{shrestha2018slayer, wu2018spatio,
   rathi_fine_tuning2020,li2021differentiable}.
% an ANN model is designed to mimic SNN structures and trained through
% weight backpropogation.
% 
% This can be done by either converting a fully
% trained ANN into an approximately equivalent SNN, or through direct
% training that uses ANN models to simulate SNN structures and learn SNN
% parameters directly through weights backpropagation.
 
Recently, hybrid training methods have been proposed, aiming to
combine the power of ANN-SNN conversion and direct SNN training.
% which are ommonly built on a certain conversion method, aiming to
% reduce congestion and latency.  which are commonly built on
% conversion approaches, promise to reduce congestion and latency.
Wu et al. \cite{wu_tandem2021} proposed a TANDEM framework for
training of an ANN and SNN jointly.  Rathi et
al. \cite{rathi_fine_tuning2020} developed a hybrid SNN training
approach, in which backpropagation is used to fine-tune the network
after conversion. Significant performance improvements have been
reported. This solution, however, requires rather sophisticated
normalization setups. % , as many ANN-to-SNN conversion methods do.
 
% ANN-to-SNN conversion methods have recently become the  most efficient
% way to obtain a well-performed SNN. Methods like data-based
% normalization [32], [52] or threshold balancing [29], [32] study how
% to determine the spiking neurons configurations, e.g., number of time
% steps, threshold, and leaky factor. The major bottleneck of these
% prior arts is the costly inference latency required to preserve high
% accuracy.

In addition, most of the current SNN models focus on  
% While remarkable advancements have been made in SNN training, mostly
% computer vision models focus on
image recognition related tasks, mostly through convolutional neural
network (CNN) models.  Other important tasks, such as object detection
\cite{spike_yolo2020} and image segmentation
\cite{kim2021beyond,patel2021spiking} have not been widely studied. It
should be noted that the former is a regression task, which requires a
network to work on real numbers. Image segmentation, on the other
hand, requires the network to produce dense classifications at the
pixel level. Both requirements pose challenges for spiking networks.


% Our approach and contributions
% \vspace{1in}
% Multiple-stage: ANN, ANN-SNN, Hybrid ANN-SNN training
% \vspace{1in}
% Take image detection and segmentation as the test applications 
% \vspace{1in}

In this work, we propose a new hybrid ANN-SNN training scheme to
address the aforementioned issues. Our approach is a fine-tuning
scheme: an ANN will be trained first, before being converted to an
SNN; then the weights of the SNN will be updated through an
alternating, forward-backward training procedure. The forward
propagation carries signals in spike-train format, essentially
conducting an SNN inference. The backward passes use ANN
backpropagation to update the network weights.

We build our networks based on soft {\it leaky integrat-and-fire} 
neurons,
% proposed by Hungerberg \cite{....},
which makes ANN-to-SNN switching rather straightforward.
%, avoiding
%complicated weight normalization processes in most ANN-to-SNN
%solutions.
%
% Our models are
% implemented with NengoDL \cite{rasmussen2019nengodl}, which can be
% recompiled and deployed under Intel Loihi \cite{loihi2018}
% neuromoprhic hardware devices.
%
We apply the proposed hybrid ANN-SNN fine-tuning scheme to object
detection and image segmentation tasks. To the best of our knowledge,
this is the first hybrid SNN training work proposed on these two
tasks.

%We apply our approach to object
%detection and brain image segmentation tasks, where R-CNN and
%convolutional autoencoder (CAE) will be used as the baseline ANN
%models. To the best our knowledge, this is the first hybrid ANN-SNN
%training work on object detention and image segmentation.  }











