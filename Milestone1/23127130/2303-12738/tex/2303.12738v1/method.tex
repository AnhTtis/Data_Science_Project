\section{Method}

In this section, we will introduce the proposed hybrid ANN-SNN
training scheme, as well as its applications in object localization
and image segmentation.  We start with the ANN models for the two
tasks.


 \subsection{Baseline ANN Models}
% \subsubsection{R-CNN Localization}


% https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/OA

% https://medium.com/@selfouly/r-cnn-3a9beddfd55a
% https://medium.com/@harman4422/object-localization-bd314d7e648f
% https://medium.com/nerd-for-tech/building-an-object-detector-in-tensorflow-using-bounding-box-regression-2bc13992973f

% Object detection is a combination of image classification and object localization.
% The latter is a common step
% in many object detection algorithms, inluding the R-CNN sequence \cite{r-cnn}. 
% R-CNN \cite{r-cnn} is the first model in a sequence of popular network
% solutions for object detection.
%R-CNN starts with extracting a number
% of region proposals to identify potential objects from an input image.
% rlies on certain region proposal algorithm (e.g., Selective Search)
% to identify potential objects from an input image.


{\bf Object localization} is a common step in object detection
algorithms to obtain accurate bounding boxes for the objects of
interest. In the R-CNN model \cite{girshick2014rich}, multiple region
proposals are first extracted from an input image and then sent to a
pre-trained CNN to extract features.
A support vector machine (SVM) model takes the extracted features to
classify the objects inside, followed by a bounding box regression step
to localize the objects. 
% are fed into a support vector machine (SVM) to
%final classification, followed by a bounding box regression to
% localize objects.

%These region proposals are then taken to extract features from a
%pre-trained CNN, followed by feeding into an SVM for final
%classification. The last step is bounding box regression, which is to
%update the bounding box to localize the object accurately.

In this work, we focus on the localization step of the R-CNN model,
with the goal to identify an accurate bounding box to capture the
object in the input image. Our solution is built as a CNN, as shown in
Fig.~\ref{fig:cae}.  It starts with a convolutional layer, followed by
a pooling layer. The pooling is implemented using a convolution
operation with stride 2. We choose not to use max-pooling as it is
difficult to implement in spiking networks. This convolution-pooling
pair is repeated three times, followed by four fully connected
layers. The network produces the predictions for the corner
coordinates (totally 4 real-valued numbers) of the bounding box.
%followed by a pooling layer. This pair is repeated
%3 times, where the pooling layer is implemented using a convolution
%operation with stride 2. We chose not to use max pooling as it is
%difficult to implement in SNNs. Then four dense fully connected layers
%compose the bounding box regressor.
We name this baseline model {\it LocNet}. %, which has 559,748 parameters in total. 

%In this work, our focus is the localization step of R-CNN, with the
% goal to identify a bounidng box in an input imge.  Our solution is
% constructed as a convolutional neural network, which starts with a
% convolutional layer, followed by a pooling layer.  This pair is
% repeated three times, where the pooling layer is implemented through
% an convolution operation with a stride of 2.  We choose not to use
% max pooling as it would be difficult to implement in SNNs.  Four
% dense fully-connected layers then follow to make up the bounding box
% regressor.  This network contains a total of 12 layers in order to
% predict the bounding box coordinates of a given image.  The total
% number of parameters present in our model is 559,748.

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]

%  \centering
%  \centerline{\includegraphics[width=8.5cm]{images/ANN_bbr.png}}
%\caption{The struture of our bounding box regression network ANN$_{bbr}$}
%\label{fig:bbr}
%\end{figure}

%{\bf R-CNN for localization} For the localization task, a
%Region-based Convolutional Neural Network (R-CNN) was used.  In this
%network, a convolutional layer followed by a pooling layer was
%repeated three times to make up the encoder portion of the
%network. The pooling layer was performed by using a convolutional
%layer with a stride of 2. This is because using a max pooling layer
%for SNNs is non-trivial. % reference here on this Four dense
%fully-connected layers then follow to make up the bounding box
%regressor.  This network contains a total of 12 layers in order to
%predict the bounding box coordinates of a given image. The total
%number of parameters present in the model is 559,748.


%https://www.nature.com/articles/s41598-020-80610-9

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
\begin{figure}[htb]
  \centering \centerline{\includegraphics[width=8.5cm]{localization_2.png}}
\caption{An illustration of the network structure of {\it LocNet}. {\it BB}
  stands for bounding box. Refer to text for details. }
\label{fig:cae}
\end{figure}


% \subsubsection{CAE Segmentation}
{\bf Segmentation network}
% The original
% segmentation task was completed using a scaled-down version of
% UNet. In this version, only 10 convolutional layers are used as
% opposed to full 23 in the original UNet. In our implementation, we
% converted this scaled-down model into a Convolutional Autoencoder
% (CAE) to perform the same segmentation task on the data. This was done
% by removing the concatentation step of corresponding layers as well as
% all of the dropout layers except in the final average pooling
% step. The reason for this change was because we found that our hybrid
% training approach did not work properly under NengoDL when the
% concatentation layers were used. Overall, this new CAE network
% consists of 14 layers and 210,864 total parameters
%
% possibly explain why, Ye's reasoning as to it being an instant layer
% instead of having a timestep associated with it show image
%
We use a convolutional autoencoder (CAE) as the baseline segmentation
network, which follows an encoding and decoding architecture.
% , as shown in Fig. 1(a).
Taking 2D images as inputs, the encoding part
repeats the conventional {\it convolution + pooling} layers to extract
high-level latent features. The decoding part reconstructs the
segmentation ground truth mask by using {\it transpose / deconvolution}
layers.


Our CAE model can be regarded as a simplified version of U-Net
\cite{ronneberger2015u}, which is a popular
%  state-of-the-art
solution for medical image segmentation. Modifications have been made
to suit our data and task. First, we reduce the number of
convolutional layers to 10 to fit the image size of our data.
%, which is much smaller than the cell images in U-Net.
Second, we replace max pooling layers with average-pooling, as there
is no effective implementation of max pooling in SNNs.  Moreover, we
remove the concatenation operations (skip connections) that connect the
encoding and decoding stacks.
% corresponding layers ``skip connections'' 
% as well as all the dropout layers except in the final avering
% pooling step.
This modification is
%with the consideration 
aimed to avoid complicated data synchronization between encoder and
decoder layers in the converted SNN model.
%simplification is partially because the focus of this work is to stuy
%hybrid ANN-SNN training, where the skip connections significantly
%complicates feature synchronization along the SNN model.





%The reason for this change was because we found that our hybrid
% training approach did not work properly under NengoDL when the
% concatentation layers were used. Overall, this new CAE network
% consists of 14 layers and 210,864 total parameters





\subsection{Proposed hybrid SNN-ANN co-training}

Our proposed ANN and SNN networks are developed under
NengoDL \cite{rasmussen2019nengodl}, which provides a number of
spiking and non-spiking neurons.
%In this work, {\it Rectified Linear}
% and {\it soft Leaky Integrate-and-Fire} (LIF) neurons are utilized.
% need reference https://arxiv.org/abs/1510.08829.
NengoDL also provides a variety of controls for SNNs,
%which include
including neuron activations, synaptic smoothing applied to each
connection, firing rate of each layer, and time steps that images are
presented in the model.
 

% {\textcolor{red} {Jundong is here}}

% Our approach: 
% Hybrid ANN/SNN co-training / refining 

Our hybrid ANN-SNN co-training approach is illustrated in
Fig.~\ref{fig:hybrid}.  An ANN of the task of interest will be trained
first, before being converted to an SNN. Then, the weights of the
converted SNNs are updated through an alternating forward-backward
fine-tuning procedure. The forward pass is essentially an SNN
inference procedure, which uses quantized activation functions
(green-color neurons in Fig.~\ref{fig:hybrid}) to generate spike
trains, flowing through the network. The backward pass computes the
error between SNN output and the ground-truth, and uses the gradient
of the loss w.r.t. the weights to update the network parameters. Spike
activation functions are switched to corresponding non-spike functions
along the backward pass.
% \url{https://arxiv.org/pdf/2002.03553.pdf} 
% The forward pass ues the quantized activation function to generate
% spike flows along the network layers, based on which network loss is
% computed.  The backward pass uses the gradient of the loss w.r.t. the
% network weights to update the parameters. % Surragate gradient 

%3.1.3. BACKPROPAGATION TRAINING To train the network via
%backpropagation, we make the simplifying assumption that (vtâˆ’1, vt) âˆ¼
%U[0, 1) are independent random variables, which implies that aËœt =
%at+Î· where Î· âˆ¼ T (âˆ’Ï‰ âˆ’1 , Ï‰âˆ’1 ) is zero-mean noise with a symmetric
%triangular distribution (see supplementary). This justifies assigning
%a gradient of zero to Î·. The forward pass uses the quantized
%activation function to compute the true error for the current Ï‰, while
%the backward pass uses the gradient of f (independently of Ï‰). In
%summary, our scheme accounts for the temporal mechanisms of spike
%generation, but allows the gradient to skip over the sequence of
%operations that keep the total spike noise bounded by Ï‰ âˆ’1 .


%We apply our approach to object detection and brain image
%segmentation tasks, where R-CNN and convolutional autoencoder (CAE)
%will be used as the baseline ANN models. To the best our knowledge,
%this is the first hybrid ANN-SNN training work on object detention
%and image segmentation.

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
\begin{figure}[htb]
  \centering \centerline{\includegraphics[width=8.5cm]{hybrid_snn.png}}
  \caption{An illustration of the proposed hybrid ANN-SNN
    co-training solution. Refer to text for details. }
\label{fig:hybrid}
\end{figure}


%In this work, {\it Rectified Linear}
% and {\it soft Leaky Integrate-and-Fire} (LIF) neurons are utilized.
% need reference https://arxiv.org/abs/1510.08829.

In our LocNet model, we use {\it soft leaky integrate-and-fire} (LIF)
neurons proposed by Hunsberger and Eliasmith
\cite{nengodl_softlif2015, rasmussen2019nengodl}.  Extended from LIF,
soft LIF neurons have smoothing operations applied around the firing
threshold. As explained in \cite{nengodl_softlif2015}, the LIF neuron
dynamics are defined by the following equation:

\begin{equation}
\tau_{RC} \frac{dv(t)}{dt} = -v(t) + J(t)
\end{equation}

where the membrane voltage is $v(t)$, the input current is $J(t)$, and
$\tau_{RC}$ represents the membrane time constant. The neuron fires a
spike when the voltage $V_{th} = 1$. The voltage is then set and
remains at zero until a refractory period of $\tau_{ref}$ has
passed. To solve for the steady-state firing rate the authors set a
constant input current given by $J(t) = j$ which produces this
equation:

\begin{equation}
r(j) = 
 \begin{bmatrix}
\tau_{ref} - \tau_{RC} \log(1 + \frac{V_{th}}{\rho(j - V_{th})})
\end{bmatrix}^{-1}
\end{equation}

where $\rho(x) = \max(x, 0)$. However, as $j \rightarrow 0_+$, the
steady-state equation's derivative approaches infinity. To counteract
this, the authors set $\rho(x) = \gamma \log(\begin{bmatrix} 1
+ e^{x/\gamma}
\end{bmatrix})$. This allows for control over the smoothing that is applied where
$\rho(x) \rightarrow \max(x, 0)$ as $\gamma \rightarrow 0$. As a
result from the smoothing, gradient-based backpropagation can now be
carried out to train the network. This design makes the conversion
from ANN to SNN rather straightforward, removing the need of
complicated thresholding or weight normalization steps in many
conversion algorithms. In this work, we take advantage of the
convenience brought by this soft LIF model to switch between ANN and
SNN to fine-tune both. For our LocNet, we use the mean squared error
(MSE) function as the network loss.

%The model was trained with NengoDLâ€™s soft LIF neurons in place of the
%ReLU activations with a smoothing factor of 0.005. The firing rates
%during training were regularized to fire at around 500 Hz using the
%method explained in the pre- vious section. Using the mean squared
%error (MSE) loss func- tion, this regularized ANN model was trained
%for 50 epochs using the Adam optimizer with a learning rate of 0.0001.

%For hybrid training, the same firing rate regularization technique was
%used to keep the neurons firing at around 500 Hz like in the original
%ANN model. The soft LIF neurons were replaced with the hybrid soft LIF
%neurons. In addition to this change, a synapse of 0.005 seconds was
%added to all connections in the network. Each image was also tiled 30
%times in order to mimic each image being presented for 30 time steps
%when NengoDLâ€™s SNN simulator for inference with each time step being
%0.001 seconds. The hybrid model was trained for 10 epochs using the
%same optimizer.


In our CAE model, we use the {\it rectified linear} neurons in
NengoDL, whose activity scales linearly with the current, unless
it passes below zero, at which point the neural activity will stay at
zero.  It should be noted SNNs' currents are either positive or zero.
The gradient of rectified linear neurons is 1 for positive currents
and 0 for when the current equals zero. The loss function for our CAE
model is set to a weighted combination of binary cross entropy (BCE)
and the Dice loss. The contribution is set to 0.5 for each loss
component.


%The Rectified Linear
%neuron models each neuron as a rectified line. This meaning that the
%neuron's activity will scale linearly with the current. The second
%neuron, the soft LIF neuron, is an LIF neuron with smoothing applied
%around the firing threshold. The larger the smoothing value, the more
%smoothing will be added to the threshold. The smoothing allows the
%activation function to be continuous which means that it can now be
%derived. Additionally, the derivation of the function allows for the
%process of backpropagation to be used when training the network. The
%Rectified Linear and soft LIF neurons are used in the CAE and R-CNN
%models respectively.


NengoDLâ€™s SNN simulator was used to run models as SNNs and collect
execution results. When using the simulator, we switch the non-spiking
neurons to their spiking counterparts. In LocNet, the soft LIF neurons
are switched to spiking LIF neurons while in the CAE model the
Rectified Linear neurons are replaced by the Spiking Rectified Linear
version. In order to make fair and consistent comparisons,
the hyperparameters were maintained the same for the ANN, SNN and hybrid
ANN-SNN of the same network.

% Hyperparameters were kept same for all ANN and hybrid ANN-SNN pairs.
%For LocNet, the synapse parameter for the simulator was set to 0.005
%seconds and each test sample was tiled 30 times with each time step
%lasting for 0.001 seconds. For the CAE model, synapse for the
%simulator was set to 0.005 seconds as well as tiling each test sample
%50 times. Additionally, the time step for the simulator was set to
%0.001 seconds.


%{\bf NengoDLâ€™s SNN simulator} was used on both models in order to run
%them as an SNN and gather their results. When using the simulator, the
%non-spiking neurons are swapped out for the spiking equivalent. In the
%R-CNN model, this means the soft LIF neurons were swapped out for the
%spiking LIF neurons while in the CAE model the Rectified Linear neu-
%rons were replaced by the Spiking Rectified Linear version.

%In order to keep things consistent with the hybrid training, the same
%hyperparameters that were used for each model were used when the model
%was ran using the simulator. This means that for the R-CNN, the
%synapse parameter for the simulator was set to 0.005 seconds and each
%test sample was tiled 30 times with each time step lasting for 0.001
%seconds. Furthermore, the CAE network uses the same parameters as it
%did during the hybrid training. This means that the synapse for the
%simulator was set to 0.005 seconds as well as tiling each test sample
%50 times. Additionally, the time step for the sim- ulator was set to
%0.001 seconds.
