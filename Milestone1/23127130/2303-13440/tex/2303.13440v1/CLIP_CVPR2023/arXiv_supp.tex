\onecolumn{%
\begin{center}
\title{\Large \textbf{Supplementary material for \\CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, \\ Fine-Grained or Not}}
\vspace{0.2cm}
%
\author{
\\Aneeshan Sain\textsuperscript{1,2}  \hspace{.2cm} 
Ayan Kumar Bhunia\textsuperscript{1} \hspace{.2cm}  
Pinaki Nath Chowdhury\textsuperscript{1,2} \hspace{.2cm}
Subhadeep Koley\textsuperscript{1,2} \hspace{.2cm}\\
Tao Xiang\textsuperscript{1,2}\hspace{.4cm}  
Yi-Zhe Song\textsuperscript{1,2} \\
\textsuperscript{1}SketchX, CVSSP, University of Surrey, United Kingdom.  \\
\textsuperscript{2}iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.\\
\tt\small{\{a.sain, a.bhunia, p.chowdhury, s.koley, t.xiang, y.song\}@surrey.ac.uk\}}
}    
\end{center}
}


\renewcommand\thesection{\Alph{section}}
\setcounter{section}{0}

\section{Prompt Design for ZS-SBIR and ZS-FG-SBIR}Here we show in detail the way visual prompts are incorporated into the Image Encoder of CLIP \cite{clip}.

For \textbf{ZS-SBIR}, we have two separate CLIP-image-encoders for photo and sketch branch with sketch and photo prompts incorporated into the respective encoders. During training the entire CLIP model is kept frozen except the LayerNorm of transformer layers and the prompts themselves, as shown in \cref{fig:ZSSBIR-prompt}.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\linewidth]{figs/ZS-SBIR-sup.pdf}
    \caption{Prompt Design for ZS-SBIR along with training objectives.}
    \label{fig:ZSSBIR-prompt}
\end{figure} 


For \textbf{FG-ZS-SBIR}, we use one CLIP-image-encoder with \textit{one common prompt} shared for both photo and sketch branches incorporated into the CLIP-image encoder. Similar to ZS-SBIR design, during training the entire CLIP model is kept frozen except the LayerNorm of transformer layers and the prompt itself, as shown in \cref{fig:FGZSSBIR-prompt}. Apart from shared image-encoders the main difference from ZS-SBIR is in considering hard-triplets within each category instead of category-level triplets as in ZS-SBIR. Furthermore, we have two more additional losses apart from the ones used for ZS-SBIR, aimed at (i) making the relative sketch-photo distances across categories uniform via f-divergence, and (ii) learning the structural correspondences between a sketch-photo pair via Patch-shuffling loss.


\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\linewidth]{figs/FG-ZS-SBIR-sup2.pdf}
    \vspace{-0.5cm}
    \caption{Prompt Design for FG-ZS-SBIR along with training objectives.}
    \label{fig:FGZSSBIR-prompt}
\end{figure} 


\section{Datasets}For evaluation on \textbf{ZS-SBIR} we have used three datasets:

(i) \textbf{Sketchy (extended)} \cite{liu2017deep} --  Sketchy \cite{sangkloy2016sketchy} contains 75,471 sketches over 125 categories having 100 images per category, with atleast 5 associated hand-drawn sketches per photo \cite{yelamarthi2018zero}. It was extended \cite{liu2017deep} further with extra 60,502 images from ImageNet \cite{russakovsky2015imagenet} (Sketchy-ext), which we use here. Following \cite{yelamarthi2018zero} for zero-shot setup we split it as 104 classes for training and 21 for testing, ensuring that \textit{test}-set images do not overlap with 1000 classes of ImageNet~\cite{russakovsky2015imagenet}.

(ii) \textbf{TUBerlin \cite{eitz2012humans}} -- contains 250 categories, with 80 free-hand sketches in each, which was  extended with a total of 204,489 images by \cite{zhang2016sketchnet}. We split it following \cite{dey2019doodle} as 30 classes for testing and 220 for training. 

(iii) \textbf{QuickDraw Extended} -- The full-version contains over 50 million sketches across 345 categories, drawn by users across the internet under 20 seconds per sketch. Augmenting the sketches with images from \textit{Flickr}, a subset of QuickDraw with 110 categories having 330,000 sketches and 204,000 photos was introduced for ZS-SBIR in \cite{dey2019doodle}. We follow their split of 80 classes for training and 30 for testing to ensure no overlap of test-set photos from ImageNet \cite{russakovsky2015imagenet}.

\vspace{+0.2cm}
For evaluation on \textbf{FG-ZS-SBIR} we require fine-grained (one-to-one matching) sketch-photo association \cite{yu2016sketch} across categories for evaluation. Accordingly we resort to Sketchy \cite{sangkloy2016sketchy} which has atleast atleast 5 associated hand-drawn sketches associated to every photo \cite{yelamarthi2018zero}. We use the same zero-shot categorical split of 104 training and 21 testing classes \cite{yelamarthi2018zero}. A few examples of sketch-photo association with multiple sketches per photo is illustrated in \cref{fig:datasetFG}.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\linewidth]{figs/FG-ZS-SBIR-dataset.pdf}
    \caption{Some examples of Fine-grained associations across categories from Sketchy \cite{sangkloy2016sketchy}.}
    \label{fig:datasetFG}
\end{figure} 

\section{More on f-Divergence} To use a single (global) margin parameter $\mu$ that works for all categories, we impose a regulariser that aims to make the sketch-photo relative distance, defined as $\delta(s, p^{+}, p^{-})$, uniform across categories. We achieve this by computing the distribution of relative distances for all triplets $(s, p^{+}, p^{-})$ in category $c$ as $\mathcal{D}_{c} = \{ \delta(s_i, p^{+}_i, p^{-}_i) \}_{i=1}^{N_{s}} $, where the $c^{th}$ category has $N_{s}$ sketch-photo pairs. Next, towards making the relative distance uniform across all categories, we minimise the KL-divergence \cite{kl-divergence} between a distribution of relative distances.

However, KL-divergence \cite{kl-divergence} only computes the information distance between two distributions -- the length of the shortest program to describe a second distribution given the first. Comparing multiple $(\geq 2)$ distributions however is comparatively less studied. The multi-distribution generalisation of information distance, aka., the $f$-\textit{divergence} is defined by a convex function $f: [0, \infty) \rightarrow \mathbb{R}$. Despite its generalisation capability, $f$-divergence for multiple distribution setup is under-explored in computer vision applications. In this paper, we thus adopt a rather simplistic definition of $f$-divergence by Sgarro \cite{sgarro1981} -- the average divergence, which is defined as,

\begin{equation}
    \frac{1}{N_s(N_s-1)} \sum_{i=1}^{N_s} \sum_{j=1}^{N_s} \texttt{KL} (\mathcal{D}_{i}, \mathcal{D}_{j})
\end{equation}

\section{Some Qualitative Results on Sketchy} Figures show qualitative results on Sketchy (ext) \cite{liu2017deep} for ZS-SBIR (\cref{fig:qual-SBIR}) and on Sketchy \cite{sangkloy2016sketchy} for FG-ZS-SBIR (\cref{fig:qual-FGSBIR}), of baseline methods vs. ours. Baselines are constructed following \cite{dey2019doodle} and \cite{yu2016sketch} for ZS-SBIR and FG-ZS-SBIR respectively.


\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\linewidth]{figs/Qualitative-zssbir.pdf}
    \caption{Qualitative results of ZS-SBIR on Sketchy \cite{liu2017deep} by a {baseline} (blue) method vs {Ours} (green). }
    \label{fig:qual-SBIR}
    \vspace{-0.4cm}
\end{figure} 

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\linewidth]{figs/Qualitative-fgzssbir.pdf}    
    \hspace*{-0.22cm}\includegraphics[width=0.7\linewidth]{figs/rebut_sup.pdf}
    \caption{Qualitative results of FG-ZS-SBIR on Sketchy \cite{sangkloy2016sketchy} by a {baseline} (blue) method vs {Ours} (green). The images are arranged in increasing order of the ranks beside their corresponding sketch-query, i.e the left-most image was retrieved at rank-1 for every category. The true-match for every query, if appearing in top-5 is marked in a green frame. Numbers denote the rank at which that true-match is retrieved for every corresponding sketch-query.}
    \label{fig:qual-FGSBIR}
    \vspace{-0.4cm}
\end{figure} 


% \section{5. Cross-dataset evaluation:} Good point! We now train our model and a competitor CC-DG [\citegreen{37}] on Sketchy [\citegreen{47}] and evaluate on ShoeV2 (ChairV2) [\citegreen{63}], and obtain 20.37\% (40.64\%) and 10.32\% (30.61\%) respectively.

\section{Limitations} We observed two plausible limitations of our method which we keep for addressal in a future work. \textit{(i)} The assumption that CLIP covers almost all classes during training, might fail in certain niche cases. \textit{(ii)} Being trained on internet-scale data (400M image-text pairs), thorough zero-shot evaluation on an unseen class is challenging. However both limitations are universal to all CLIP-based applications.