\section{Lessons learnt}

\subsection{Space and place}
This theme included the following four topics: i) intrusiveness of sensors; ii) technology readiness; iii) unexpected issues during the deployment; and iv) the trade-off among data quality, portability of sensors and affordability. 

\subsubsection{\textbf{Intrusiveness of sensors}}
%teachers 
This topic focuses on teachers' and students' perceptions of the intrusiveness of the MMLA deployment as a whole, and the wearables sensors in particular. 
All teachers agreed that the whole MMLA deployment was less intrusive than what they had initially expected. As expressed by T3: \textit{"when we look back at the previous year, we thought it was going to be a lot more intrusive than it turned out to be"}. Yet, two teachers also mentioned that the deployment may have caused \textbf{distraction to some students}, as expressed by T1: \textit{"students may be distracted by the research happening instead of being aware of the actual learning"}. For example, one observation made by the same teacher was that \textit{"sometimes researchers were entering and leaving the debriefing room to solve some technical issues"}.% and T2 suggested that in the future, \textit{"only one researcher should be needed to troubleshoot any issues, just in case things go wrong"}.

Teachers had mixed reactions about the intrusiveness of the wearable sensors. Two teachers (T3 and T4) explained that the sensors were not intrusive to students while two other teachers (T1 and T2) believed that \textbf{sensors may have been uncomfortable for some students to wear}. T2 mentioned that \textit{"the belly bag was too tight for some students"} and that \textit{"the microphone fell off for some students"}. These issues may cause students to be \textit{"self-aware about the research"} (T2). 
%students
In contrast, most of the 261 students who participated in the first iteration reported that they felt comfortable wearing the sensors during the simulation (Q: \textit{"I felt comfortable wearing the sensors during the simulation"}, Median = 6, mean = 6.166, std = 1.07, min = 1, max = 7). In the more in-depth interviews with students, most of them (N=18 out of 20) also expressed that it was fine to wear the sensors during the simulation (e.g., \textit{"It was fine wearing the sensors. I didn't mind wearing them"} - S9). Some students explained that, at the beginning, they felt \textit{conscious} (S20), \textit{nervous} (S9) or \textit{stressed} (S12) about being monitored. However, once they started the simulation, they focused more on their learning task than on the sensors. One student stated: \textit{"I was focused on my simulation. So, after I had them in and went into the simulation, it was fine. I didn't notice them."} (S17). Students also pointed out that it is normal for nurses to wear various devices or instruments. In this sense, wearing sensors was not different from their usual practice. One student explained this as follows: \textit{"In placements, we’re so used to wearing a pickpocket on the side. So it didn't matter to me because I was already used to that"} (S19). 
% COVID - remove if we need space.

When students were asked if they had any \textbf{concerns related to health and safety} while wearing the sensors (since the first iteration of the study was conducted in between lockdowns, during the COVID pandemic), all the interviewed students (except one) expressed that they did not feel any concern about this. Only one student pointed out the following: \textit{"when I was putting the equipment on, I was wondering whether it was clean"} (S20). Students mentioned they knew that the University had strict safety protocols for running these simulations and that, in clinical environments, it is a regular practice to have all equipment clean after every use. 

In sum, although some teachers were somewhat concerned about the intrusiveness of adding wearables to the learning environment in terms of students' comfort and potential distraction, most students reported that they were comfortable with wearing the sensors, citing that they commonly wear various devices in simulation-based learning sessions and at their placements. Yet, it is critical to follow strict hygiene protocols, which was the case in both iterations. 


\subsubsection{\textbf{Technology readiness}} 
This topic focuses on the potential impact of the lack of MMLA technology readiness. From a research perspective, two members of our research team (R1 and R2) explained that whilst the off-the-shelf sensors utilised in this setting were easy to install, there still is complexity in maintaining the ecology of devices during the deployment. Although wearable microphones are common in healthcare simulation, smart watches are already being worn by students to monitor themselves and indoor positioning technologies based on smart-phones already exist (typically less precise than the one used) \citep{gualda2021locate}, the \textbf{technology was not yet ready to enable bring-your-own-device (BYOD) strategies} to work smoothly in our MMLA deployments. Hence, we still needed to provide specific devices to students and they needed to be worn correctly to maximise the quality of the data collected. Moreover, all sensors were connected to a data infrastructure running on more than one computer, since different sensor vendors required particular specifications in terms of hardware and operating system configurations. This made it hard, if not impossible, to easily connect all sensors to a single machine. Consequently, a team with technical expertise was still required to closely monitor the data infrastructure and to help students to put the sensors on.

Teachers noted the \textbf{potential negative impact on the lesson plan} as a result of asking students to wear the sensors correctly during a regular class. T1 noted: \textit{"If we factor in more time for that [sensor equipping], we still only have a three-hour session, which means that we cut into time for other things. So we need to think about ways that we can reduce the amount of time spent equipping students."} However, T4 had a contrasting point of view, expressing that \textit{"there was no great time-saving difference between teams wearing the devices compared to those who did not"}, suggesting that this is not a time-consuming task. Nevertheless, from the experience in the first iteration, the senior teaching team devised ways to equip students with the sensors more efficiently. Thus, the lead course coordinator placed coloured trays (blue, yellow, green, and red) per each student's role to put the devices inside the tray (e.g., see Figure \ref{fig:equipment}, right) so students could easily equip themselves. This new configuration was well received by T1, T3 and T4, stating that it made it easier to identify the equipment and the roles (e.g. \textit{The coloured [trays] worked really well, I think that was great.} - T1; \textit{It was easier to equip students this year [second iteration] than last year [first iteration]}. - T3). 

During the second iteration, we also observed teachers instructing students on how to wear the devices properly. This illustrates how \textbf{teachers became involved in developing their own strategies to appropriate the MMLA sensing technology} and configure the space without the intervention of the researcher team. Teachers also proposed ideas to overcome the issues about technology readiness by suggesting that, in the future, clear instructions could be provided to students to wear the equipment correctly without any assistance, for example, using \textit{"big posters showing pictures of how the devices should be used properly"} (T1). In terms of readiness of the infrastructure, teachers were eager to use \textit{"on-the-go"} packed and ready-to-use sensors (T1 and T2) to adopt the tool in their learning environment without the researchers' intervention: \textit{"We can use the system without the need of the research team, we want to appropriate the use of it without needing assistance" (T2)}. This is linked to the fifth theme on the sustainability of the deployment further explained later.


\subsubsection{\textbf{Unexpected technical issues during the deployment}}
This topic focuses on unenvisaged problems with the sensing technologies deployed in the learning space that impacted data collection and the analytics. 
Our research team reported various \textbf{technological issues when configuring the devices}. For example, while setting up the space before the second iteration of the study, the research team found that a third-party driver for the positioning sensors had to be updated thus making the installation time longer than expected for the MMLA system to work exactly as in the first iteration. Other issues emerged during the study itself. R2, R3 and R5 reported that, although the physiological wristbands used in the study are marketed as high-end sensors and are relatively new, some stopped working unexpectedly (e.g., \textit{"Two [physiological wristbands] were not working or they turned off in the middle of some classes"}- R5). The research team had to investigate the cause of this malfunction and re-install the firmware in between class sessions with mixed results. Also during the study, the University-owned computers restarted unexpectedly due to software updates controlled centrally by the university's technical support team. R3 mentioned that: \textit{"the [positioning sensors] laptop restarted due to University device update"} and R5 complemented that due to this issue, the visualisations could not be generated: \textit{"one of these devices restarted on its own even in the middle of data collection, and all visualisations are depending on this device"}. The computers that were used during data collection are operated by the University, leaving the research team with limited freedom to configure them as the need arises.   

Another unexpected issue was that some \textbf{students wore the sensors incorrectly} causing data loss. As noticed by R5, some students did not wear the microphone properly (e.g., "placing the mic far from the mouth"), even though the instructions given to them were clear. This could have affected the audio quality for a few students: \textit{"The talking volume of students actually have great influence on audio data collection, if the volume is high, then the audio signals might be harmed, if the volume is low, then some audio might not be clearly recorded"} (R3). These unexpected issues hindered the correct generation of visualisations or yielded to an incomplete dataset collection for some teams.

Most of the issues described above are expected given the particular requirements imposed by multiple third-party technologies used in our MMLA deployment. While some of these technical challenges go beyond the control of the research team, researchers reflected and provided suggestions to minimise these in the future, such as replacing the current physiological wristbands with alternative wearables even though they may collect less granular data (i.e., a Fitbit sense) (R5) and reduce the number of computers used in the deployment to minimise connectivity or operating system related issues (R2 and R5):\textit{ "In total, we have 3 computers for data collection. It would be better to merge to two"} (R2). 


\subsubsection{\textbf{Trade-off among data quality, portability of sensors and affordability}}
This topic focuses on how the sensing technology used can impact data quality, and the portability and affordability of the MMLA system.
A trade-off among these three aspects of the deployment can be illustrated in terms of audio data collection. Six pairs of inexpensive, minimalist wireless microphones (Xiaokoa 2.4G -- around USD\$17 each) were used in the first iteration with the aim of minimising intrusiveness caused by the wearables. These were consisted of one slim headset without any further equipment or antenna, thus, making them highly affordable and portable. However, the lack of a less portable signal transmitter in these devices made the signal inconsistent and subject to interference. This resulted in only 23 out of 57 simulation sessions being fully recorded for all team members. A second quality issue was the voice overlap, since other students' voices were easily captured when in close proximity. Without special audio post-processing, this led to inaccurate data analysis results. To address these issues, the microphones were upgraded in the second iteration. The new microphones (Shure BLX14P31 Wireless Headsets) resolved the first quality issue about signal interference, as they provided multiple frequency bands for signal transmission. The second issue about voice overlap was also resolved, as these microphones were unidirectional, minimising the chance of getting voice overlap. Although the new microphones resolved the quality issues, it made the MMLA system less affordable as the price of each pair is around USD\$300, costing around 20 times more than the previous microphone, and they included a bodypack transmitter that students had to wear on their hip additionally to the headpiece connected through a cable, making them slightly more complex to wear. Each microphone also required a receiver in the form of a black box connected to the audio infrastructure, making the setup more complex too.  

In sum, obtaining high-quality data in a MMLA deployment was prioritised, but required more expensive and bulkier sensing devices, a tradeoff that must be weighed against the expected instructional benefit. For example, better audio data quality can lead to the careful study of the quality of students' conversations with the aim of supporting learning effectively (e.g., \cite{zhao2023mets})



\subsection{Data and analytics} 
This theme included the following three topics: i) the purpose of capturing multimodal data; ii) data incompleteness and trustworthiness; and iii) emerging issues related to the MMLA Dashboard. 

\subsubsection{\textbf{Purpose of capturing multimodal data}}
%Study from 2021 helped understand the purpose of the data collection from students perspectives, whereas study from 2022 highlighted the purpose of the analytics (i.e., visualisations) using during the debrief. 
%students
%Q6. What data do you recall was collected during the simulation?
%Students were asked to recall \emph{Q: what data was collected during the simulation?} 
This topic focuses on students' and teachers' perceptions of the educational purpose of the MMLA deployment. 
Regarding whether students understood  \textbf{what data was being collected}, all interviewed students (N=20) remembered they wore a bracelet (i.e., \emph{``a watch'', ``a little hand band'', ``a wristwatch''}), a microphone (i.e., ``a headphones'', ``a headset''), and an indoor positioning device  (i.e., \emph{``a fanny pack'', ``belly bags'', ``a tag at the front''}). Yet, only two students realised that video data were also recorded (\emph{i.e., ``you recorded us'' and ``there was video''}) possibly because video recording has been a staple in this educational context  \cite{megel2013high}.
%Q7. Why do you think the data was collected during the simulation? 
%To the question \emph{Q: why do you think the data was collected during the simulation?} students have different responses.
In the first iteration, when students were asked about their perceptions regarding the purposes behind \textbf{why their data were collected}, a half of them (N=10) indicated that data was collected for others (researchers or teachers) to understand how students perform during the simulation scenario and assist future students in their learning (e.g., \emph{``[researchers] can suggest ways that the [teachers] can change to help or assist students in the simulation, the common things that students are struggling with''} (S17). Likewise, S4 noted that the data was collected to \emph{``help [future students] have better learning resources and using us as guinea pigs to see what needed to be improved for the learning outcomes for future nurses''}. Various students (N=11) concretely explained that the new technology was to support them directly. For example, students claimed that it was \emph{``to see how everyone reacts when there's a situation going on and how people prioritise and react''} (S7) and \emph{``to provide some feedback about the whole scenario to students and educators''} (S1). Other purposes that students mentioned were related to the assessment of the simulation effectiveness (N=4) (e.g., \emph{``to see the effectiveness of the simulation, or whether it's working and if it is beneficial to our learning''} S1), the support for future nursing professionals (N=4) (e.g., \emph{``to see what we experience and we make, for improvement of future nurses''} -- S10), and only two of them were not sure about the purpose of the MMLA deployment as a whole.


%teachers
%Q5 - How did you use the “tool” (slides) during the debrief? For what purposes?
%Q6- To what extent do you think that using the tool may have been helpful for you or the students during the debrief?
%Q1-3 How was analytics used during the debrief
In relation to the second iteration, \textbf{teachers felt satisfied with the use of the debrief MMLA dashboard}. All senior teachers highlighted that the debriefing tool helped to \textit{reinforce and back up the discussion during the debrief}, stating that the tool \textit{"ignited some light bulb moments around this discussion"} (T4). As stated by T4, the tool can draw attention to key points and spark discussions towards improvement in their practice: \textit{"I used the data to draw attention to improving communication. I also used it to discuss the most useful aspects of teamwork and communication students needed to learn for a deteriorating patient"}. Specifically, the tool was used to validate and emphasise some teamwork aspects previously discussed with students (e.g., \textit{"[the data] validated some of the points that were made in the debrief"} - T5; \textit{"[the data] highlighted the team communication that we had discussed, and only discussions that we previously had with the group"}, - T4. The debrief tool was also used to \textbf{reflect on a team's reaction in critical moments}, as mentioned by T2: \textit{"for example, from students' positioning, I could see how they reacted, after the MET [medical emergency team] call, if they took a 2-2 approach or a 3-1 approach"}. Teachers also stated that the visualisations were helpful during the debrief because they showed visual evidence of the team's dynamics and performance (e.g., \textit{"it provided visual evidence of team dynamics"} - T7; \textit{"it helped to give visual confirmation of excellent collaboration the team showed"}- T1). All interviewed senior teachers (T1-T4) highlighted how the tool was used to provide what they referred to as "objective feedback", explaining that teachers often focus on one student when observing the simulation, which may later introduce bias towards that student during the debrief (we discuss perceived "objectivity" in the next section on data trustworthiness). For two teachers \textit{"the data [visualisation] was way less subjective"} (T3 and T4). %Teachers used the tool to reflect on communication (e.g., "It was very helpful to show how much communication was going on" - Tx), the team’s reaction in critical moments (e.g., "for example, in their positioning, I could see how they [students] reacted, after the MET call, if they [students] took a 2-2 approach, 3-1 approach." - T2), to discuss team's performance supported by the different visualisations (e.g., "I was able to demonstrate that although communication was effective between team members and the patient, however the delegation of the second priority (bed 2) was missed" - Tx). 
The same teachers (T3 and T4) also mentioned how the tool could benefit students' learning by providing alternative sources for reflection (e.g., \textit{"another way of telling students what they did"} - T3; \textit{"another way of seeing their performance"} - T4) and by highlighting positive aspects of students' performance, especially for cases when students assess or judge their own performance too harshly (e.g., \textit{"I highlighted to students they did a good job although they thought had done poorly"} - T4). 

In sum, from the first iteration (that involved data collection only) we learnt that when we clearly communicate the educational purpose of the study to students, they may see the value of participating in a complex MMLA intervention even if they do not receive direct benefits from it (e.g., to help improve current teaching practices or improve the technology that will be used by other students in the future). Moreover, from the second iteration, we learnt that teachers perceived the potential benefits of the richness of multimodal data, rendered into data visualisations, to support students' reflection but they need to ultimately develop the strategies to optimise the effective use of data for educational purposes.  

\subsubsection{\textbf{Data incompleteness and trustworthiness}}
%Q38. Would you trust the information presented in the layers (blue buttons)?
%Q39. Would you trust the information presented in the student’s positioning graphs (yellow buttons)?
%if they \emph{Q: would trust the information presented in the visualisations?}
This topic focuses on the potential impact of incomplete multimodal data on the perceived trustworthiness of the MMLA system gathered from students and teachers. When students from the first iteration were asked about trust of their own data presented to them during the interview, 15 out of 20 students indicated that they would \textbf{trust the multimodal information}. Their responses can be organised into two groups. Students in the first group (9 students out of 15) would trust the data because they were able to identify themselves or their perceived team outcomes from the visualisations. For example, S2 explained the following: \emph{``I trust the data because it presents what we did in the simulation, by looking at the data I can see who I was in that scenario''}. Likewise, student S3O indicated the following: \emph{"when I looked at the [visualisations], I could tell that this is what I did, at this time [pointing the visuals], and the actions we performed were very familiar"}. The second group of students (6 students out of 15) responses indicated that they trusted in the multimodal data based on the belief that sensors provide, what they referred to as: ''objective and accurate data''. For instance, one of the students described this as follows: \emph{``I wore the sensor so I know that the information came from pretty reliable sources''} (S3). However, 4 of the 5 students, who did not trust the data, %some students (4 of the 5 untrusted N=4) 
were \textbf{skeptical about the trustworthiness of the multimodal information} because at the beginning they found it difficult to read. For instance,  S10 indicated that he \emph{``do not trust the information much because [the visualisations] are not as simple and straightforward''} (S10). In the same way, S22 suggested the following: \emph{``I think I would trust the data for the most part but I think [other people] would require knowledge about the context as well''}. However, when students got familiarised with the visualisations two of them changed their opinions (N=2). For example, S20 suggested that the visualisation was \emph{``a bit hard to interpret. But, once you sort of look at it, and when you read the data as well, when you combine it all together it is trusted and the data presented definitely makes sense''}. In the end, only 3 students would not trust the multimodal data at all as they considered it did not represent what they did during the simulation (e.g. \emph{``I don't think it was accurate enough considering how my team performed''} - S7).

%When students (in the second iteration) were asked about the extent they would trust the information, 65\% of students reported that they would completely trust the information, and 26\% replied that they would trust the information to some extent (Median: 1, std dev: 0.742). Students indicated that \textbf{their sense of trust is linked to the accuracy of the information} presented in the visualisations. Twelve students (N=47) said they would trust the information because it accurately represented their behaviours and vivid experiences during the simulation. For example, S59 explained the following: \textit{"[this visualisation] accurately shows that I was working in isolation and mostly interacted with the other helper nurse whom I was familiar with. This meant that I did not contribute much to the conversation overall with the doctor or voice the fact that I was available and there to help the primary nurses in the scenario."} Similarly, S65 mentioned the following: \textit{"I do believe this is an accurate depiction of where our time was spent during the simulation. Therefore, I would trust the information and believe it would be a valuable tool to assess and adjust communication and behaviours in teamwork and working environment."} 
%Ten students also indicated that the \textbf{clarity and consistency of information supported their trust}. For example, these students expressed that the visualisations were \textit{"very accessible"}, \textit{"easy to follow"}, \textit{"easy to understand"} and \textit{"quite clear"}. S56 explicitly mentioned the following: \textit{"The visualisation validates our team's discussions about how well we communicated. We were a well-acquainted group of nursing students who communicated effectively and regularly, which is apparent in the visual. I feel our reflections and the visuals are cohesive and, therefore, trustworthy." } Likewise, S27 also reflected on the clarity and trust of the information: \textit{"I'd trust a graph like this because it's very clear to read and understand the graph, it's easy to reflect on it and see what we can improve on in the future."}

Similarly to the students in the first iteration, most students who looked at their data in the MMLA dashboard in the second iteration (43 out of the 47 survey respondents) indicated that they would generally trust the mulimodal visual representations. However, some students were also aware that sometimes the visualisations in the MMLA dashboard were generated with incomplete data. For example, four students (N=47) recognised that the \textbf{incompleteness of data affected their sense of trust}. One student explained this, as follows: \textit{"This visualisation allows the students to reflect and be accountable in communication when working in collaborative practice. However, I can only trust to some extent as I know that a line is missing between the student wearing blue colour and the doctor for communication"} (S51). Another student expressed a similar idea: \textit{"I cannot judge my communication from these data as the majority of my communication was between myself and the other primary nurse, and her data was not collected"} (S29). As noted by S46, having incomplete data could hinder the validity of results: \textit{"My only comment would be to ensure that the equipment is working and recording as required, as this, unfortunately, affects the validity of results"}.   
  
The four senior teachers also reflected on the trustworthiness of the information presented in the debrief tool. T2 and T3 pointed out that trust is built over time. T2 expressed this idea as follows: \textit{``For the first week at least, I really didn’t understand [the visualisations] to a degree. Once I knew what [the visualisation] was about, I did trust in it in the later weeks for sure"}. T1 and T3 indicated that trust is not coming from the data but from their confidence about making a good interpretation of the information presented in the tool: \textit{``It’s about how I learned to use the system that changed my trust. It is not that I’m not trusting the data. I am not trusting what I have to say about it; I don’t know what it means"} (T1). Moreover, T3 and T4 mentioned that they \textbf{over-trusted} the information. Even though their understanding did not seem aligned with the data, they wanted to make an explanation (i.e., \textit{``Sometimes I was trying to force [an explanation] to the visualisation but I shouldn’t be because it didn’t make sense to me. I was seeing things that I wasn’t expecting or that I couldn’t explain"} -- T4). 

The teachers indicated their \textbf{strong preference for using the debrief tool when the data was complete } (i.e., data from \textit{all four} team members). If the information was about less than four team members, as it was the case for teams in which not all members consented to participate in the study, using the tool can still be relevant. However, teachers had concerns about data incompleteness and a deeper explanation in the debrief was needed to complete the whole picture of what was expected from an entire team according to the expected learning outcomes. T1 explained this concern as follows: \textit{``If [teachers] don’t go through the [incomplete information], [students] don’t understand what they’re looking at, and the wider context. So I walked through how the visualisation would have looked like if we had a full set of students"}. T2 also stressed that visualisations generated from the incomplete data of a team might cause confusion to students: \textit{``giving [incomplete information] to the students without explaining it probably would have just made them confused. I think with three or four [team members] the debrief tool was generally well used"}. But also using incomplete data could be harmful in general as teachers could also make wrong assumptions as T2 explained: \textit{``Sometimes the data would confusing to me. A student reminded me that there were only three [team members] then I was 'yes, that does explain it exactly' "}. 

In sum, we learnt that there may be several risks associated with both students' and teachers' overconfidence in trusting the data merely because it is being automatically captured via sensors. Moreover, teachers and students may also believe that sensor data is objective and free from potential bias. Yet, learning data is intrinsically incomplete \cite{Kitto18Imperfection}, especially when captured via sensors, and data representations are not necessarily objective as they are unavoidably imbued with subjective design decisions \cite{Benford}. It is therefore critical to develop technical and social mechanisms to ensure MMLA systems are trustworthy, which requires teachers and students to acquire a suitable level of understanding that multimodal data is incomplete and subject to different kinds of bias. That incompleteness and bias is both \textit{intrinsic} to computational modelling of any social activity (i.e., all data and algorithms provide partial lenses onto human activity), but \textit{extrinsic} local contextual factors may also be in play (e.g., technical malfunctions; students not consenting to being tracked).


\subsubsection{\textbf{Emerging issues related to visualising multimodal data}}
This topic focuses on issues experienced by students and teachers while interacting with the MMLA dashboard. A recurrent challenge highlighted by all the senior teachers and some of the other teachers (T5, T7 and T9) was that \textbf{they did not feel in the position to make a rapid interpretation of the visualisations} and lead a reflection immediately after. Teachers expressed that this was often due to the challenge of aligning what they observed in the simulation and the information presented in the dashboard. As described by T2: \textit{``sometimes I tried to make sense and align the observed behaviours with the data, but then, during the debrief, I was thinking, Oh no, I don’t think this might be correct"}. Teachers also indicated that they needed more time to familiarise themselves with the MMLA dashboard in order to devise ways to use it in the moderation of the class reflection (e.g., \textit{``it was challenging because I have not done it before”} -- T7, and \textit{``it was challenging due to unfamiliarity with the technology”} -- T9). However, teachers recognised that using the MMLA dashboard involves a learning curve process: \textit{``once we got more familiarised with the tool, it got easier”} (T3). One potential solution is to factor in a preparation or formal training period with the whole teaching team for them to create strategies that they can follow to interpret the data visualisations generated immediately after each team session, as suggested by T4: \textit{``having some standard [moderation] with the teachers would be really useful"}. Another solution suggested by T3 was to enrich the MMLA dashboard with teachers' observations and learning expectations to create a stronger sense of trustworthiness: \textit{``I think we can articulate the data better […] having some kind of moderation where we [the teachers] manually check if students achieved a learning outcome and transfer this into the learning tool"}. 

Some teachers also mentioned that their lack of understanding was due to not knowing how the data was captured and translated into the visualisations (e.g., \emph{``how are you coming up with the yellow bar? What is being used to create that?”} - T4). The four senior teachers suggested that helping teachers to develop their \textbf{data literacy in relation to the particular MMLA dashboard} would contribute to interpreting the visualisations better and also to understanding their limitations, especially when using it for the first time. T1 explained this as follows: \textit{``knowing where the data is coming from and how the information is being calculated would help us to understand the specific parts of it”}. 

Similarly, some students in the second iteration also pointed at the challenge of connecting the complex multimodal data with specific learning constructs or performance metrics. Five out of the 32 students (N=47) who indicated that the information in the MMLA dashboard was relatively easy to read, also explained that it was hard for them to make a clear connection of its meaning with their learning experience  (e.g., \textit{"I understand what each part of the data is showing. But I think I need more explanation to understand the meaning behind each one and the reasoning on why this occurs"} - S54). One student highlighted \textbf{the need for contextual information for them to be in the position of interpreting the multimodal data}: \textit{“I think more of a prompt before the visualisation is shown, such as knowing how many patients we have and the critical information of the patient, then I could have more understanding and confidence of my performance in the simulation”} (S47). Another student expressed her desire for\textbf{ detailed explanations about the team’s performance}: \textit{“I didn't really understand what I was looking at, at first, and there is still some confusion in the last section about task transition. I don't know if being higher than the average is considered a good or bad thing”} (S64). Another student recognised the need for clear instructions to adjust their practice: \textit{“it would be good to see what you would expect to see from a highly effective team”} (S58).

%While most students in the second iteration found that the information was useful for reflecting on their learning, twenty students (N=47) indicated two main issues when interpreting the information. The first one is related to the \textbf{visual design aspects} of the dashboard. 
%Ten students felt confused by the information presented in the dashboard, at some point, mainly due to some visual cluttering displayed in one of the graphs (e.g., \textit{"you can gain an understanding of the visualisation represented by tracing the arrows, but at the same time there are many arrows which can cause confusion"} - S48;  \textit{"The lines can be confusing and require a little bit of time and effort to read. The names of the colours are covered, making it hard to see and read."} - S52). They also found some labels in the graphs disconnected from the simulation context (e.g., \textit{"The labelling of each task could be clearer. Maybe labelling it instead to be "working on bed 4" instead of "primary task" and "secondary task" as that can be a little confusing as to what is meant by this”} - S24). 
% The second issue concerns their\textbf{ ability to make sense of the data}. Five students agreed that while the information can be easily read, it was hard for them to make a clear connection of its meaning with their learning (e.g., \textit{"I understand what each part of the data is showing. I think I need more explanation to understand the meaning behind each one and the reasoning on why this occurs"} - S54). This hardship in making this connection could be due to the need for 1) contextual information, 2) explanations of their performance or 3) clear instructions scaffolding their learning. For example, one student highlighted the need for contextual information: \textit{“I think more of a prompt before the visualisation such as knowing how many patients we have and the critical information of the patient, then I could have more understanding and confidence of my performance in the simulation”} (S47). Another student expressed her desire for detailed explanations about the team’s performance: \textit{“I didn't really understand what I was looking at, at first, and there is still some confusion in the last section about task transition. I don't know if being higher than the average is considered a good or bad thing.”} (S64). And another student recognised the need for clear instructions to adjust their practice: \textit{“it would be good to see what you would expect to see from a highly effective team”} (S58)

In sum, teachers may find it hard to interpret MMLA visual interfaces given the complexity of the various sources of the intertwined data underpinning them. For effective in-the-wild deployments, teachers need to be supported to develop relevant data literacy skills to understand the basic inner-workings of the analytics and for them to develop pedagogical strategies around the use of the MMLA systems. Students also emphasised the need for design elements in the MMLA visual interface to \textit{explain} the meaning of the data and for them to understand what were the performance expectations.

\subsection{Design and human-centredness}
This theme included the following two topics: i) human-centred design, teaching and learning; and ii) human-centred design and research innovation.

%Co-design the interfaces with Teachers (debrief) Zoom meetings
%Co-design with students (report) To be conducted
\subsubsection{\textbf{Human-centred design, teaching and learning}}
This topic focuses on teachers' perceived benefits of being part of a human-centred process in the development of the MMLA system in terms of their teaching practices and students' learning.
The senior teachers felt generally \textbf{satisfied with a design process in which their voices could be reflected in the resulting end-user interface}. %This satisfaction is often guided by the importance, value and impact of the research for students, teachers and the learning experience, as indicated by T1: 
For example, T1 appreciated the partnership with the researchers throughout the design process, as follows: \textit{``We’re all involved in this because we can see that there are benefits to the students and to the teachers for doing this, that it alleviates how we do our debrief and can potentially change the way we do our sims”}. T3 also recognised the value of bringing different expertise to innovate in terms of technology innovation and pedagogical practice: \textit{``I think by combining our teams we can make something that’s new, and we’ll help students learn”}. T1 and T2 expressed that they felt involved in the design process and acknowledged that their lived experiences and suggestions were considered in the final prototype of the MMLA dashboard: \textit{``[the research team] was very flexible in working with us. They have taken a lot of our suggestions on board”} (T2); and \textit{``We felt we had contributed to it. So I thought the two teams were able to add value, and I found that really satisfying”} (T4). Teachers also felt that both researchers and teachers were working towards the common goal of supporting students' learning. T1 and T2 elaborated on this idea, as follows: \textit{``It’s good that we are going towards the same goal, but with different perspectives"} (T2); \textit{``I think it’s important that we’ve been able to have those priorities to see that there’s a common goal. We just want to work at it from different points of view"} (T1). In short, teachers appreciated partnering with researchers in the MMLA design process. This can lead to creating MMLA visual interfaces aligned with existing teaching practices and learning goals. 

\subsubsection{\textbf{Human-centred design and research innovation}}
This topic focuses on the views of both teachers and researchers on partnering to foster MMLA research innovation.
The lead senior teacher (T1) expressed that \textbf{at the beginning of the design process the teaching team was a bit uncertain about the outcome} of the MMLA deployment given its novelty, but the experience after the two MMLA iterations made these feelings fade: \textit{``I really had no idea exactly what we were doing, and I had to see it and be part of it before I could really understand it”}. Linked to this idea, T4 explained that, because the research is considerably innovative, teachers' involvement in the design process enabled them to consider students’ feelings and reactions and be careful about how students were invited to be part of the research study: \textit{``[the deployment] was very innovative. Because of this, I think I was a bit more worried about the [students], thinking that we have to do it correctly. We were given the opportunity to be diligent about it.”}

%Building a positive relationship between researchers and teachers will impact the research outcome and the whole experience. 
Also, \textbf{teachers value research collaboration if the common goal benefits students' learning}. T2 expressed this positive relationship by highlighting the openness, easiness and willingness of the research team to work with the nursing teaching team: \textit{``It’s the fact that there are some researchers who are just as passionate about this as the educators, it’s actually easy for us to keep wanting to do this because they want to work with us as much as we want to work with them. And although they aren’t nurses, actually we can work together and share points of view, so that we can try and understand what the research side is. They are willing to work, and be quite flexible with what’s going on, so that we can make it work for nursing"}. T3 also stressed the strengths of combining both, the LA research and the nursing teaching teams towards building innovative learning tools: \textit{``I think by combining our teams, we can make something that’s new, and you know we’ll help students learn"}.

Researchers' perspectives resonated with teachers' perspectives. The research team indicated that it was critical to consider \textbf{teachers' voice since they have the lived experience in deploying pedagogical interventions}. For example, T4 described how teachers would indicate whether certain logistic decisions were \textit{``unfeasible"} or not. R4 explained this as follows: \textit{``When collaborating or interacting with teachers, I think the most important thing is to put teachers' and students' educational needs before our research desires”}. R3 also explained that a benefit of collaborating with teachers in the design of the MMLA deployment was that \textit{``they could be invited to an interview, to get what they valued or missed in the visualisation which can show what can be improved or developed in the future to help them better"}. R4 also explained how important was to collaborate with teachers to \textbf{understand the meaning of the data }in light of the characteristics of the learning design: \textit{``This collaboration has led to changes in my algorithm for detecting task prioritisation. There are some minor changes in the learning design that we were unaware of but significantly impacted the analytics"}. Finally, R1 also mentioned the importance of \textbf{partnering with students} to validate the MMLA dashboard: \textit{``Doing a cognitive walkthrough while teachers and students think aloud has benefited my research in validating how useful or misleading the [multimodal] visualisation may be"}.

In sum, we learnt about the importance of involving teachers and students in the design process to not only validate the highly innovative MMLA end-user interfaces but also to expand understanding of the learning design, values that must be endorsed, and the lived experiences that can affect the logistics of the deployment. 

\subsection{Social Factors}
This theme included the following two topics: i) consenting and participation strategies; and ii) data privacy and sharing.

\subsubsection{Consenting and participation strategies}
This topic focuses on lessons learnt from the consenting strategies applied in both iterations of the study. Teachers recognised that, proportionally, more students consented to participate in the first iteration (2021) than in the second (2022). As mentioned by T4: \textit{``it didn’t seem to work as well [the second] time. We were trying very much to keep on time, and I think [the consenting process] actually became a little bit more complicated”}. While some teachers tried to explore potential explanations for this difference, citing potential seasonal differences (e.g., \textit{``students may just be so much more burnt out than they were last year. They’re disengaged on everything, not just the simulation”} -- T1); teachers tried to optimise the consenting process. In preparation for the second iteration, consenting information was sent to students beforehand as pre-class material through an online consent form and an explanatory video. Teachers recognised this strategy may have caused confusion since students often do not access pre-class materials, as explained by T4: \textit{``we also know that they don’t access the pre-class stuff, so that will limit their exposure to it. But I did feel it was just a little bit complicated for them to take on, and then agreed to consent to”}. T3 also explained that \textbf{the MMLA deployment may be too novel in the eyes of the students} and it involves several layers in relation to data (i.e., capture, analysis and visualisation) that students cannot fully comprehend:\textit{``I think the video was really helpful. We played it in the class, and it was very sound, very personable and short. Perhaps the way it communicated made it sound a bit complicated”}.

Due to the challenges in getting participants for the second data collection, the senior teaching team shifted to the same strategy as in the first iteration (2021). This consisted in inviting a researcher to give a short 1-2 minute speech just before the class to communicate the research and ask them to participate. Students could ask any clarification questions. As mentioned by T1: \textit{``I think it worked really well when one of the researchers gave the speech from the research point of view. And then one of the academics back that up”}. This space also served to explain to students that the data was de-identified and that participating in the research would not affect their grades. As mentioned by T2, this strategy was crucial to highlight to students that the data was being de-identified: \textit{``We noticed that they were worried like if their faces or names will be shown in the screen”}.  However, packing all ideas in two minutes is challenging. It may cause some pressure on the research team. T4 indicated that: \textit{``you’re trying to [inform students] in a very short space coming in and trying to convey this information. So I think [the researcher] was under a lot of pressure to do it”} (T4). 

%Strategy 3 
Moreover, all the senior teaching team agreed that the consenting strategy should be improved. After teachers experienced two in-the-wild MMLA deployments, they reflected on the value of the research, the usefulness of the MMLA debrief tool and its potential impact on students’ learning. They thus indicated they may want to move the deployment forward to happen as a part of the regular tasks in the classroom. They proposed a \textbf{'business-as-usual' use} of the MMLA dashboard if the maturity of the system allows it as in video-based debriefs in healthcare simulation \citep{megel2013high}, so all students would have the same learning opportunities. This way, the consenting would be just about optionally \textit{recording} the data for research purposes, otherwise, deleting it immediately after the class. This was explained by T1 as follows: \textit{``I still think our best bet is that the [MMLA system] is inbuilt in all simulations. Perhaps this is what we will do next year. Maybe we have only one or two sessions where the technology is not used at all. This is definitely the way to go}".


%Researchers perspectives: lack of understanding
R3 and R5 agreed that the lower participation in the study was due to the intrinsic complexity of the multimodality aspects of the data collection. R3 expressed that \textit{``some students didn't seem to actually understand what would happen in our data collection and didn't want to participate. Students might see the digital version of the consenting form as more work and then ignore that"}.  The lack of understanding could come from technical words embedded during the explanation of the study. R5 indicated that \textit{``[students] didn't actually understand what’s 'multimodal data' and didn't ask about it either”.} As reflected by R5, there should be a balance between details (e.g., over-explaining details of the data and technology that students without formal analysis training and AI literacy would not easily understand ) and simplicity (e.g. omitting key information about the potential implications related to the data collection that may be relevant to make an informed decision) to minimise students misunderstandings: \textit{``over-explaining technologies, explaining from what kind of data and showing what kind of visualisations they’ll see in debriefing, resulted in a lower number of students participating in the study. However, explaining it too simply and straightforward resulted in several students withdrawing from the study”}. For future studies, researchers suggested \textbf{simplifying the explanations and words} in the consent form and during the description of the research. R5 explained this as follows: \textit{``I reckon we need to improve some wording in the consent form by simplifying some words, for example, instead of using the word 'multimodal' we should use words they can understand such as 'audio, position, and health data')}.

%Researchers perspectives: burnt out and disengagement
%R3 and R5 points of view are aligned with the teaching team’s perspectives about students' burnt out and disengagement. As mentioned by R3, \textit{“undergraduate students are under the pressure of doing four academic units each semester. They have assignments to do and classes to participate in. Their time might be limited, so maybe they would ignore the things that don’t influence their academic results”}. R5 argued that external factors are beyond our knowledge and control, such as low rapport and lack of motivation: \textit{"There are many other external factors that are just random and beyond our knowledge & control. […] It could be caused because they don’t know each other or feeling overly stressed to participate in the simulation, or there is a lack of motivation to do so.”}

%Researchers perspectives:limited time
%R3 also acknowledged the limited time to convince students to participate in the study: “It takes time to do the introduction. It worked because we can see the numbers of participants reflected in the data collection.”

 
%Researchers perspectives: 2021 vs 2022 consenting strategies
When reflecting on consenting strategies from both iterations, R5 indicated that consenting was better in the first iteration because it was \textbf{focused only on data collection}. Therefore, teachers were not much involved as in the second iteration: \textit{``The consenting [from the previous year] was smoother than this year because we only focused on data collection. Teachers didn’t provide much help besides helping students wear sensors (i.e., pozyx, mic, and empatica)”.} R4 also reflected that \textit{``although the consenting strategy in the first year was non-environmentally friendly, as printed consent forms were handed to each student, it was very successful in getting several students volunteering to the study”}. It seems that an in-person strategy to explain to students about the MMLA deployment is better than an online strategy because clarifications can be made in person, and there is a direct engagement with students: "an online strategy can be confusing because students do not only consent to the study but also to the online confidentiality agreement they need to sign as a part of their course" (R3).

In sum, it is challenging to explain to students what a complex MMLA study entails as it involves various types of heterogeneous data sources each pointing at multiple analysis and visualisation approaches. Yet, providing too many technical details about the sensors and the analytics in advance may not necessarily contribute to clarity. Explaining the complexity of the MMLA deployment \textit{in person} can enable students to ask clarification questions and then provide informed consent. 

%Agreement for next iteration depending on maturity and technology %acceptance from the teachers


%The effect of COVID lockdowns and government mandates
\subsubsection{Data privacy and sharing}
This topic focuses on students' perspectives on multimodal data privacy and sharing. 
All interviewed students (N=20) agreed to \textbf{share their data for educational purposes in ways they could not be identified }by others outside their class. Yet, they had various different perspectives about who should benefit from looking at and using the visualisations. For instance, 10 students indicated that it would be only beneficial for those students who own the multimodal data to use it. As one of the students argued: \emph{``you just take more knowledge from your own experience than with someone else's experience''} (S5). This was supported by another student who considered that showing their own data \emph{``will make more sense to the team, who was there performing the simulation''} (S18). The rest of the students (N=10) reflected on the opportunities of sharing the data with others and its benefits for learning. For instance, S20 envisaged that \emph{``if [he] could see their teams' visualisations, compared to another team that did the exact same scenario, it would be quite interesting to see maybe why did this team recognise aspects earlier than them''}. Likewise, other students (N=5) indicated that teachers can benefit in the way they \emph{``can work on improving critical aspects of students performance and the scenario''} (S10), \emph{``can learn a lot about student thinking and performance''} (S18), and \emph{``can help them understand how well a student did''} (S3).

In sum, students did not show signs of being concerned about sharing their data with others, which may be explained by a low level of understanding of the multimodal data itself as mentioned in the previous subsection. Yet, half of the interviewed students thought that the data would be meaningful and relevant only to the students who were in the same class where the data was collected. 

\subsection{Sustainability} 
% [Question from Jimmie]: What's the definition of sustainability? On-going operation? Equipment?
%Technological sustainability (the platform control transformation from study 1 to 2)
%Microservices - sensors cameras 
This theme included the following two topics: i) technological sustainability; and ii) MMLA appropriation in the classroom.

\subsubsection{Technological sustainability}
%When researchers and teachers were asked to reflect on the sustainability of this project, two topics emerged. 
The first topic relates to the sustainability of the technological infrastructure. 
R1 and R5 suggested a flexible `detachable' architecture capable of \textbf{running through microservices as a centralised process}. The idea of this architecture is to minimise data loss in case one of the data sources is not correctly running (e.g. “\textit{the architecture should provide an easy way to de-attached pieces of the application presenting issues. That way, it would be possible to reduce any misleading data visualisation [caused by issues during data collection]”}, R1). Microservices should also allow running the MMLA solution with minimum requirements: \textit{“At a minimum, these systems should be able to run with minimum hardware and software installations. For example, we have one document that provides a list of recommended hardware that we use (i.e., audio interfaces, pozyx, etc..) and a list of software that is needed to be installed (such as ASIO4All for audio and Pozyx interface)” }(R5). 

Special care should also be given to automatically \textbf{communicate to teachers about any issues that may have been detected during the data collection} (e.g., sensors that may have been accidentally disconnected) so they can decide if they will use the tool in the debrief or not: \textit{“teachers should be informed that issues can happen and it can help them to decide whether they want to use the technology or not during their classes”} (R1). R4 also explained that technological sustainability depends on the readiness of sensing technologies, switching towards user-friendly versions of the hardware: \textit{“The sustainability of MMLA research studies largely depends on the readiness of the sensing technologies. For example, the positioning tracking system and analytics will become sustainable for teachers to use on their own when fully enterprise solutions are available, as the level of automation from data collection to analytics will be higher”}.
%Hardware integration with existing systems (audio (embedded mics in healthcare or AI diaration example)- indoor positioning (becoming embedded) - empatica-fitbit)
In short, a potential strategy to maximise long-term technical sustainability is a light-weight microservices-based architecture that can enable attaching and detaching heterogeneous sensors as required. Such technical properties of the MMLA infrastructure are tied to \textbf{building and sustaining stakeholders' trust} in it. 

\subsubsection{MMLA appropriation in the classroom}
The second topic concerns the requirements to sustainably integrate the MMLA system into regular teachers' practices. %R1 suggested reducing the equipment and developing a technological ecosystem orchestrated by one service: \textit{“Currently, we have multiple laptops to control the sensors, which leads to complexity. However, it seems not a good idea to collect data in a single laptop, so at least we should have an interface to set up all the software”} (R1) 
R1 and R2 expressed that for teachers to appropriate the MMLA system, this should move towards a toolkit (of hardware and software) that can be easily deployed and \textbf{used by non-experts users}, such as the teaching team or nursing students. R2 suggested that \textit{"all the software should be reconstructed to generate a toolkit which can be easily deployed and used”}. R1 further expanded on strategies required to embed the multimodal sensors into the existing classroom ecology, as follows: \textit{“a classroom can be equipped with charging stations where teachers and students can collect/leave the devices they are wearing, and teachers have a computer in the classroom where they can easily start/stop the data collection and generate data visualisations}". 

The senior teachers' perspectives were also aligned with the idea of running the MMLA system without requiring too much technical support: \textit{“in terms of using the system, running the tool by yourself without any help of the team of researchers. Right? I think that would be the ideal end goal that we can just run it.”} (T2); and \textit{“there should be some way of being able for it to collect it [data] automatically”} (T4). However, both T2 and T4 also suggested that\textbf{ minimal technical support is still required}: \textit{“it will always require a sort of a technical person”} (T4) and \textit{“if we get to the point where we only need one researcher there on the day to troubleshoot, just in case things go wrong to get that [the system] up”} (T2). R1 mentioned that \textit{“having a dedicated space (e.g., a specific classroom) to run this multimodal data collection on a regular basis would help other universities if they want to implement these solutions)”}. T4 supported this idea: \textit{“concerning the technology, I think that the infrastructure in the room could be improved, the sensors need to be embedded in the room”}. R3 mentioned that it would be ideal to use the equipment that is already part of the simulation room, such as ceiling microphones and 360 video cameras: \textit{“If we can use the devices they already have, like microphones, the price of the whole system would be cheaper”}. 


Finally, researchers and teachers suggested \textbf{a training period for teachers}. For example, T4 explained the following: \textit{“teachers could definitely be better educated about it and be more autonomous in that regard. It is about training the teaching staff to be able to attach the wearables and all the rest or running this the MMLA system”}. 
In sum, a potential strategy to maximise adoption and technology appropriation by non-technical end-users includes i) embedding sensing capabilities into the classroom; ii) providing teachers with a high degree of user control; iii) providing basic training for teachers to not only use the system but also to learn how to interpret and act upon the multimodal data representations effectively; and iv) providing readily available technical support in case something goes wrong. 

%  teachers' vs researchers'  perspectives (Vanessa is working on it.)


%Debrief tool - Interpretation and learning support [QUAL]
%Awareness [quantitative - likert scale] [Q21-Q24] Q40. How do you interpret the values presented in the student’s positioning graphs? Q42. Do you think the use of this timeline, layers and graphs for reflection should be guided by the teacher? Q25-Q26. How likely is it that you would use this timeline, layers and the positioning graphs regularly as a resource for reflection after each simulation? Why? Q27. Do you think these timeline/layers and positioning graphs can be used as an additional assignment/task for students to reflect on the simulation? What task/assignment can you envisage? 

%Sensors and quality of data 
%Automated transcription / sound wave (Linxuan)


%Empatica vs fitbit (Rio) 
%In contrary to the quality of data from the collection of audio information, the collection of physiological information favour portability and affordability. During data collection, (TODO: compare window gap; a cheaper wearable devices are sufficient to collect the information and gives information feedback; recent fitness devices enable developers to develop flexible custom applications; the reliability and battery usages where it is turned off in the middle of data collection; discussion about its usage to the cloud system).not all feature.   

%Learning design alignment and Orchestrability (Gloria’s interface)
%Research desription
%Selected quoted from teachers. 

%\subsubsection{Lessons Learnt}

%What do you think you or other educators can do if presented with this (layers) while leading the debrief with students immediately after the simulation?
%❏What do you think you or other educators can do if presented with this narrative visualisation (layer) while leading the debrief with students immediately after the simulation?
%Is there any potential risk that you perceive in using this report to inform your debrief session?








%Empatica vs Fitbit (Rio)

%In contrary to the quality of data from the collection of audio information, the collection of physiological information favour portability and affordability. During two data collection studies, physiological sensors (i.e., Empatica E4) could not collect data consistently. After each simulation, some of these smartwatches were turned off automatically. Batteries were depleted despite these sensors have been charged through strict protocol (e.g., between each simulation session). Their firmware have been properly updated and it seems. Furthermore, connecting more than three Empatica devices to the E4 server simultaneously will.
%- Comparing windows gap.
%- From developers point of view, a custom software application in much commercial smartwatches 
%- These research-grade physiological sensors are mainly used in the laboratory environment that doesn't require many movements \citep{morales2022Occupational}.
%The main idea is this a cheaper wearable devices are sufficient to collect the information. In the future development, commercial smartwatches enable MMLA development to give haptic feedback to the wearers.

%Broader list of research-grade sensors are available in \citep{morales2022Occupational} study.


