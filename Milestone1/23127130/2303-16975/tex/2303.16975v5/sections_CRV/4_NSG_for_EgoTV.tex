\input{tables/query_definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neuro-Symbolic Grounding (NSG)}
\label{section:proposed_framework}

\etv requires visual grounding of task-relevant entities such as actions, state changes, etc. extracted from NL task descriptions for verifying tasks in videos. To enable grounding that generalizes to novel compositions of tasks and actions, we propose the Neuro-symbolic Grounding (NSG) approach. NSG consists of three modules:~a)~semantic parser, which converts task-relevant states from NL task descriptions into symbolic graphs,~b)~query encoders, which generate the probability of a node in the symbolic graph being grounded in a video segment, and~c)~video aligner, which uses the query encoders to align these symbolic graphs with videos. NSG thus uses intermediate symbolic representations between NL task descriptions and corresponding videos to achieve compositional generalization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Queries for Symbolic Operations}
\label{subsection:queries}

To encode tasks, NSG captures task-relevant visual and relational information in a structured manner via symbolic operators called \emph{queries}.
For instance, the task description \emph{heat an apple} can be symbolically captured by the query: \code{StateQuery(apple, hot)}.
Similarly, the task description \emph{place steak on grill} can be captured by \code{RelationQuery(steak, grill, on)}, which represents the relation (\code{on}) between objects \code{steak} and \code{grill}.
Queries are characterized by \textit{types} and \textit{arguments} and are stored in a text format. Table~\ref{table:QTypes} shows the various query types and their arguments. Different query types capture different aspects, e.g., attributes, relations, etc., thereby enabling a rich symbolic representation of everyday tasks.

\subsection{Semantic Parser for Task Descriptions}
\label{subsection:plan_parsing_from_instructions}

The symbolic operators, i.e., queries, allow the semantic parser to represent a task's partial-ordered steps using a symbolic graph. Specifically, the parser translates a NL task description into a graph $G(V, E)$, where a vertex $n_i \in V$ represents a query and an edge $e_{ij}: n_i \rightarrow n_j \in E$ is an ordering constraint indicating that $n_i$ must precede $n_j$~(Figure~\ref{figure:model-layout}a). We experiment with two different methods to parse language descriptions of tasks to graphs -- (i) finetuning language models and (ii) few-shot prompting of language models. For details, refer to Appendix~\ref{section:appendix_semantic_parsing}. We perform a topological sort with the graph $G$ and generate all the possible sequences of queries consistent with the sort. For example, the topological sorting of the graph in Figure~\ref{figure:model-layout}(a) yields two ordered sequences: $(n_0, n_1, n_2, n_3)$, $(n_0, n_2, n_1, n_3)$. Note that this does not include all physically possible ways to complete a task, but a super-set of all possible sequences of task-relevant queries, including some infeasible sequences\footnote{For instance, in Figure~\ref{figure:model-layout}a, $n_1$ and $n_2$ are at the same topological level, but the sub-task in query $n_1$ could invalidate pre-conditions for $n_2$. Hence, a physically plausible task requires $n_2$ followed by $n_1$ and not vice versa. Note that \etv does not have physically implausible tasks.}. However, this super-set is useful because a task can be verified as accomplished if any sequence in this set can be ascertained to occur in the video. 

Notably, all \etv tasks map to acyclic graphs through temporal disambiguation. While this can support tasks with repeated actions, such as: (Task) \textit{pick two apples}; (Graph) pick(apple) $\rightarrow$ pick(apple); tasks that require (recursively) repeating action sequences until a desired state is reached, might result in cyclic graphs.  Examples include unstacking an arbitrary number of dishes or searching for an ingredient. While currently absent in \etv, extending to such tasks would be a valuable future direction.

\subsection{Query Encoders for Grounding}
\label{subsection:query_encoders}

Query Encoders are neural network modules that evaluate whether a query is satisfied in an input video. Specifically, a query encoder $f^{\theta_{\tau}}$ for a query $n$ of type $\tau$ (e.g., \code{StateQuery}, \code{RelationQuery} etc.), accepts NL arguments ($a$) corresponding to objects and relations in $n$ and a video ($v$) to generate the probability $\mathds{P}=f^{\theta_{\tau}}(a, v)$ of the desired query being true in the video. Learnable parameters corresponding to different query type encoders in an NSG model are jointly represented as $\theta = \bigcup_{\tau}\theta_{\tau}$.

Both the text arguments $a$ of the query and the frames of the input video $v$ are encoded using a pre-trained CLIP encoder~\cite{clip}. The token-level and frame-level representations from CLIP are separately aggregated using two LSTMs~\cite{lstm} to obtain aggregated features for $a$ and $v$, respectively. These features are then fused and passed through the neural network $f^{\theta_{\tau}}$ to obtain the probability $\mathds{P}$ of the query being true in the video (see Figure~\ref{figure:model-layout}a).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{figures/model-layout}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Video Aligner for Task Verification}
\label{subsection:plan_verification}
% say we formulate as a query alignment problem
This module of NSG must align the graph representation $G$ of the task (generated by the semantic parser) with the video. To that end, it first segments the video, then jointly learns~a)~the query encoders, which detect the queries in the video segments and~b)~the alignment between video segments and the query sequences obtained from the topological sort on $G$. Such joint learning is required since the temporal locations of the queries in the video are unknown a priori requiring simultaneous detection and alignment. If the video is a positive match for the task encoded in $G$, at least one of the query sequences from $G$ must temporally align perfectly with the video segments for successful task verification. Conversely, for negative matches, no query sequence from $G$ would \emph{completely} align with the video segments. Going forward, we use $\langle\rangle$ and $()$ to denote ordered pairs and sequences, respectively.

\noindent \textbf{Video Segmentation:} The video is segmented into non-overlapping segments\footnote{Since pretrained, off-the-shelf video segmentation models are limited to predefined action classes~\cite{escorcia2016daps} or reliant on background frame change detection~\cite{yang2022temporal} and require downstream finetuning~\cite{gao2020accurate}, we leave their integration in NSG as future work.} with a moving window of arbitrary, but fixed size $k$\footnote{If required, the last segment is zero-padded to $k$ frames.} 

\noindent \textbf{Joint Optimization:} The objective of the optimization is to jointly learn the \emph{alignment} $\mathrm{Z}$ between queries and video segments along with the \emph{query encoders} $f^{\theta}$.
% that detect queries in the segments and output a task verification probability for \etv tasks.
% , using only the ground truth label $y$ as supervision.
Given:~a)~the temporal sequence of $S$ segments $(s_t)_{t=0}^{S-1}$ with each $s_t$ spanning $k$ image frames; and~b)~a sequence of $N$ queries $(n_j)_{j=0}^{N-1}$  from the topological sort on $G$, the alignment $\mathrm{Z}$ is defined as a matrix $\mathrm{Z} \in \{0, 1\}^{N \times S}$, where $Z_{jt} = 1$ implies that the $j^{th}$ query $n_j$ is aligned the video segment $s_t$. An example alignment with $N=2$ and $S=3$ is given by the matrix $\mathrm{Z} = \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 0 & 1 \end{array} \right]$, where the rows are ordered queries $(n_0, n_1)$, the columns are temporal segments $(s_0, s_1, s_2)$, and $\langle n_0, s_0 \rangle$, $\langle n_1, s_2 \rangle$ are the aligned pairs. Assuming segmentation guarantees sufficient segments for query alignment: $S \geq N$. Using $\mathrm{Z}$ and $f^{\theta}$, the task verification probability $p^{\theta}$ can be defined as:
% \vspace{-0.3cm}
\begin{align}
     p^{\theta} = \sigma \bigg(\max_{\mathrm{Z} \in {\{0,1\}}^{N \times S}} \frac{1}{N}\sum_{j,t} \log f^{\theta}(a_j, s_t)Z_{jt}\bigg) \label{eq:p_theta} 
     %\vspace{-0.2cm}
\end{align}

Here $\sigma$ is the sigmoid function, $f^{\theta}(a_j, s_t)$ denotes the probability of querying segment $s_t$ using query $n_j$ with arguments $a_j$ (\S~\ref{subsection:query_encoders}), and $\max$ operator is over the best alignment $\mathrm{Z}$ between $N$ queries and $S$ segments. We use the ground-truth task verification label $y$ to compute $\mathrm{Z}$ and $f^{\theta}$ by minimizing the following loss:
\vspace{-0.2cm}
\begin{align}
     \min_{\theta} & \frac{1}{|\mathcal{D}|} \sum \mathcal{L}_{\text{BCE}}(p^{\theta}, y), \label{eq:bce}
    %\vspace{-0.2cm}
\end{align}

here $|\mathcal{D}|$ is the \etv dataset size and $\mathcal{L}_{\text{BCE}} (\cdot)$ is the binary cross entropy loss computed over $|\mathcal{D}|$ input,~output pairs.~Given the minimax nature of Eq.~\ref{eq:bce}, we use a 2-step iterative optimization process:~(i)~find the best alignment $\mathrm{Z}$ between queries and segments with fixed query encoder parameters $\theta$ (optimize Eq.~\ref{eq:p_theta} with fixed $f^{\theta}$);~(ii)~optimize $\theta$ using Eq.~\ref{eq:bce}, given $\mathrm{Z}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Dynamic Programming (DP)-based Alignment:} Finding the best $\mathrm{Z}$ in Eq.~\ref{eq:p_theta} given $\theta$ requires iterating over combinations of $N$ queries and $S$ segments while respecting certain constraints. The constraints, visualized in Fig.~\ref{figure:model-layout}b, ensure that~a)~no two queries are aligned to the same segment\footnote{This ensures that the order of queries can be verified, which cannot be done when queries belong to the same segment.} (Eq.~\ref{Z-a}),~b)~all queries are accounted for in $S$ (Eq.~\ref{Z-b}), and~c)~the temporal orderings between queries in the query sequences are respected (Eq.~\ref{Z-c}). Specifically, if query $n_u$ precedes $n_v$ ($n_u \rightarrow n_v$), and query $n_v$ is paired with segment $s_{\bar{t}}$ (i.e. $Z_{v\bar{t}}=1$), then query $n_u$ cannot be paired with any segment that lies after $s_{\bar{t}}$ (i.e. $Z_{ut} \neq 1 \; \forall \; t \geq \bar{t}$). The resulting optimization problem for $\mathrm{Z}$, given $\theta$ is:
% \vspace{-0.2cm}
\begin{subequations}\label{Z-main}
\begin{align}
& \max_{\mathrm{Z} \in {\{0,1\}}^{N \times S}} \sum_{j,t} \log f^{\theta}(a_j, s_t)Z_{jt} \tag{\ref{Z-main}}, \quad \text{s.t.}\\
& \sum_{j=0}^{N-1} Z_{jt} \in \{0,1\}, \quad \forall \; 0 \leq t \leq S-1 \label{Z-a}\\
&\sum_{t=0}^{S-1} Z_{jt} = 1, \quad \forall \; 0 \leq j \leq N-1 \label{Z-b}\\
% & n_u \rightarrow n_v, \; Z_{v\bar{t}}=1 \Longrightarrow Z_{ut} \neq 1, \quad \forall \; t \geq \text{argmax}_{\bar{t}} Z_{v\bar{t}} \label{Z-c}
& n_u \rightarrow n_v, \; Z_{v\bar{t}}=1 \Longrightarrow Z_{ut} \neq 1, \quad \forall \; t \geq \bar{t} \label{Z-c}
\end{align}
\end{subequations}

 
Intuitively, the solution to Eq.~\ref{Z-main} gives us the best alignment score (note, the overlap with Eq.~\ref{eq:p_theta}). The iterations over $N$ queries and $S$ segments for solving Eq.~\ref{Z-main} are underpinned by an overlapping and optimal substructure. For instance, to optimally align queries $(n_j)_{j=0}^{N-1}$ and segments $(s_t)_{t=0}^{S-1}$, one could:~a)~pair $\langle n_0, s_0 \rangle$ and optimally align the remaining queries and segments $(n_j)_{j=1}^{N-1}, (s_t)_{t=1}^{S-1}$; or (2) skip $s_0$ and still optimally align \emph{all} queries, now with the remaining segments $(n_j)_{j=0}^{N-1}, (s_t)_{t=1}^{S-1}$ (see Fig.~\ref{figure:model-layout}b(iv)). This recursive substructure leads to a DP solution for Eq.~\ref{Z-main}. 

Let, $F^{\ast}((n_j)_{\bar{j}}^{N-1}, (s_t)_{\bar{t}}^{S-1})$ denote the best alignment score for queries $(n_j)_{\bar{j}}^{N-1}$ and segments $(s_t)_{\bar{t}}^{S-1}$ from Eq.~\ref{Z-main}. Based on the aforementioned reasoning, $F^{\ast}((n_j)_{\bar{j}}^{N-1}, (s_t)_{\bar{t}}^{S-1})$ can be recursively written as: 
% \vspace{-0.2cm}
\begin{multline}
    F^{\ast}((n_j)_{\bar{j}}^{N-1}, (s_t)_{\bar{t}}^{S-1}) = \text{max} \big( \log f^{\theta}(a_{\bar{j}}, s_{\bar{t}}) \\ + F^{\ast}((n_j)_{\bar{j}+1}^{N-1}, (s_t)_{\bar{t}+1}^{S-1}), F^{\ast}((n_j)_{\bar{j}}^{N-1}, (s_t)_{\bar{t}+1}^{S-1}) \big) \label{eq:dp}
\end{multline}

The base cases for the DP are: (i) $\mathrm{Z}=\mathds{I} \; \text{if} \; N=S$; (ii) $Z_{jt} = 1 \; \forall \; t \; \text{if} \; j=N-1$. It is worth noting that the DP subproblems, together with the base cases, satisfy the constraints in Eq.~\ref{Z-a}~\ref{Z-b}~\ref{Z-c}. Since the video may match any of the sequence in the super-set of query sequences (from the topological sort on $G$), we repeat this process of computing $F^{\ast}$ for each sequence and select the maximum value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\noindent \textbf{Optimizing Query Encoder Parameters $\theta$:} After obtaining the best alignment $\mathrm{Z}$ using DP, we substitute the corresponding value of $F^{\ast}((n_j)_{j=0}^{N-1}, (s_t)_{t=0}^{S-1})$ in Eq.~\ref{eq:p_theta} and subsequently Eq.~\ref{eq:bce}. In Eq.~\ref{eq:bce}, we use single mini-batch of training examples and take one gradient-update step of the Adam optimizer for the query encoder parameters $\theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{tables/baseline_results.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%