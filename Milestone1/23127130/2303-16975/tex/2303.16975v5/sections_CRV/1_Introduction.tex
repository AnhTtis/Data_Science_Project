% \vspace{-10pt}
%%%%%%%%% BODY TEXT

\section{Introduction}
\label{section:introduction}

Inspired by recent progress in visual systems~\cite{MagicLeap, ungureanu2020hololens}, we consider an assistive egocentric agent capable of reasoning about daily activities. When invoked via natural language commands, for e.g., while baking a cake, the agent understands the steps involved in baking, tracks progress through the various stages of the task, detects and proactively prevents mistakes by making suggestions. Such a virtual agent~\cite{virtual-agent} would empower users to learn new skills and accomplish tasks efficiently.

Developing this egocentric agent capable of tracking and verifying everyday tasks based on their natural language specification is challenging for multiple reasons. First, such an agent must reason about various ways of doing a \emph{multi-step} task specified in natural language. This entails decomposing the task into relevant actions, state changes, object interactions as well as any necessary causal and temporal relationships between these entities. Secondly, the agent must ground these entities in egocentric observations to track progress and detect mistakes. Lastly, to truly be useful, such an agent must support tracking and verification for a combination of tasks and, ideally, even unseen tasks. These three challenges -- causal and temporal reasoning about task structure from natural language, visual grounding of sub-tasks, and compositional generalization -- form the core goals of our work.

As our first contribution, we propose a benchmark -- \emph{\textbf{Ego}centric \textbf{T}ask \textbf{V}erification} (\etv \inlineimg{figures/TV}) -- and a corresponding dataset in the AI2-THOR~\cite{ai2thor} simulator. % \emoji{tv}
Given a natural language (NL) task description and a corresponding egocentric video of an agent, the goal of \etv is to verify whether the task was successfully completed in the video or not.
\etv contains multi-step tasks with \emph{ordering} constraints on the steps and \emph{abstracted} NL task descriptions with omitted low-level task details inspired by the needs of real-world assistants. We also provide splits of the dataset focused on different generalization aspects, e.g., unseen visual contexts, compositions of steps, and tasks (see Figure~\ref{figure:dataset}). Consequently, \etv dataset provides the fine-grained control necessary for rigorous testing and refinement of task reasoning models, which is often missing in real-world datasets~\cite{ego_4d, epic_kitchens}. Yet, \etv mirrors the real world by leveraging visual photo-realism and task diversity.

Our second contribution is a novel approach for order-aware visual grounding~--~\emph{\textbf{N}euro-\textbf{S}ymbolic \textbf{G}rounding} (NSG), capable of compositional reasoning and generalizing to unseen tasks owing to its ability to leverage abstract NL descriptions along with compositional and temporal structure of tasks (task decomposition, ordering).~In contrast, state-of-the-art vision-language models~\cite{coca,clip,videoclip,clip_hitchiker} struggle to ground NL descriptions in egocentric videos, and do not generalize to unseen tasks.~NSG outperforms these models by~$\mathbf{33.8}\%$~on compositional generalization and~$\mathbf{32.8}\%$~on abstractly described task verification. Finally, to evaluate \nsg on real-world data, we instantiate \etv on the CrossTask~\cite{cross_task} instructional video dataset. We find that it also outperforms state-of-the-art models at task verification on CrossTask. We hope that the \etv~benchmark and dataset will enable future research on egocentric agents capable of aiding in everyday tasks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%