\section{\etv Benchmark and Dataset}
\label{section:dataset}

We present the \emph{\textbf{Ego}centric \textbf{T}ask \textbf{V}erification} (\etv) benchmark and dataset. To enable task tracking and verification for egocentric agents, \etv contains:~1)~\emph{multi-step} tasks with \emph{ordering constraints} to capture the causal and temporal nature of everyday tasks,~2)~\emph{multimodality} -- language in addition to the egocentric video to allow language-based human-agent interaction. 

\etv also aims to enable the systematic study of generalization in task verification (see Table~\ref{table:list_of_datasets_abridged}). To this end, we create the \etv dataset using a photo-realistic simulator AI2-THOR~\cite{ai2thor} -- as a rich testbed for future research on generalizable agents for task tracking and verification. Our synthetic dataset serves as a valuable proxy of real-world performance of various task verification models while providing control over various factors affecting task reasoning. Lastly, we also create a real-world task verification dataset (\S~\ref{section:cross_task_construct}) using the CrossTask dataset~\cite{cross_task}. While this dataset is not egocentric and is limited in its ability to systematically evaluate the generalization of task reasoning models, it enables the testing of task verification models in real world.

% we provide an abridged study of \etv on real-world data  .

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definitions}
\noindent \textbf{Benchmark.} The objective is to determine if a task described in natural language has been correctly executed by the agent in a given egocentric video.

\noindent \textbf{Tasks.} 
% In real-world tasks, ordering between steps might arise because of physics of the world e.g., need to pick up a knife before slicing, or semantics of the task e.g., need to slice vegetables before frying. At the same time, certain steps may be executed in any order. Consequently, 
Each task in \etv consists of multiple \emph{partially-ordered sub-tasks} or steps. A sub-task corresponds to a single object interaction via one of the six actions:~\emph{heat, clean, slice, cool, place, pick}, and is parameterized by a~\emph{target} object of interaction\footnote{Except the \emph{place} sub-task, which is additionally parameterized by a \textit{receptacle} object, we currently limit our \etv dataset to sub-tasks involving only a single target object.}. By using the ``actionable" properties of objects in AI2-THOR~\cite{ai2thor}, we ensure that the sub-tasks are parameterized with appropriate target objects in \etv, e.g., \emph{heat(book)} will never occur.

Real-world tasks consist of sub-tasks with ordering constraints, either due to physical restrictions (e.g., picking up a knife before slicing) or task semantics (e.g., slicing vegetables before frying).
We allow \etv tasks to be partially ordered, with some steps following strict ordering, e.g.~\emph{pick} sub-task happens before \emph{place} sub-task, while others are order-independent.

The ordering constraints between sub-tasks are captured in the task description using specifiers such as \emph{and}, \emph{then}, and \emph{before/after}. For simplicity, we will refer to a task using $\left< \text{\emph{sub-task}} \right> \_ \left< \text{\emph{ordering-specifier}} \right>$ notation, irrespective of the actual task description. Such tasks can then be instantiated by specifying an $(object)$ of interaction. An example task instance from \etv: ~\emph{heat\_then\_clean(apple)} is shown in Fig.~\ref{figure:dataset} with its NL description: ``apple is heated, then cleaned in a sinkbasin". The task consists of two ordered sub-tasks: heat $\rightarrow$ clean on \emph{target} object: apple. We adopt this terminology from ALFRED~\cite{ALFRED20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset} As shown in Fig.~\ref{figure:dataset}, \etv dataset consists of (task description, video) pairs with positive or negative task verification labels. By combining the six sub-tasks~\emph{heat, clean, slice, cool, put, pick} with different ordering constraints, we create 82 tasks for \etv (see Appendix~\ref{appendix:dataset_analysis_and_statistics} for an exhaustive list). Tasks are instantiated with 130 target objects (excluding visual variations in shape, texture, and color) and 24 receptacle objects, totaling 1038 task-object combinations. These are performed in 30 different kitchen scenes. We also provide comprehensive annotations for each video, including frame-by-frame breakdowns for sub-tasks, object bounding boxes, and object state information (e.g.,~\textit{hot, cold, etc.}) to facilitate future research.

%================================================================
\subsubsection{Generation}

\noindent \textbf{Task-video Generation.} We generate the videos in our dataset by leveraging the ALFRED setup~\cite{ALFRED20}. ALFRED allows us to specify the \etv tasks using Planning Domain Definition Language (PDDL) and then to generate plans for achieving these tasks using the Metric-FF planner~\cite{metric_ff}. We execute these plans using the AI2-THOR simulator and obtain their corresponding videos. Further details on encoding tasks using PDDL and planning are in Appendix~\ref{appendix:etv_taskvideogen}.

\noindent \textbf{Task-description Generation.} We convert the plans generated for each task into positive and negative task descriptions using templates. Appendix~\ref{appendix:task_templates} provides details on the process and example templates. 


\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{plots/data_stats.pdf}
    \caption{\textbf{\etv dataset.} Sub-tasks and tasks, including their difficulty measures (\S~\ref{section:evaluation}) are shown per split. Novel Scenes have more tasks since all the train tasks are repeated in unseen scenes. Likewise, complexity and ordering are higher in Novel Tasks due to the addition of unseen sub-tasks.}
    \label{figure:stats}
    % \vspace{-0.3cm}
\end{figure}

%=====================================================================

\subsubsection{Evaluation} 
\label{section:evaluation}

\noindent \textbf{Metrics.}
We use accuracy and F1 to measure the efficacy of models on \etv task verification benchmark. To capture the difficulty of tracking and verifying tasks, we introduce two measures:~(1)~\emph{Complexity}: measuring the number of sub-tasks in a task, which impacts the video length and requires higher action and object grounding, and~(2)~\emph{Ordering}: measuring the number of ordering constraints in a task and measures the difficulty of temporal reasoning required to track and verify tasks. We evaluate model scalability by testing on tasks with varying complexity and ordering.


\noindent \textbf{Generalization.}
\etv dataset enables systematic exploration of generalization in task tracking and verification via four test splits that focus on generalization to novel steps, tasks, visual contexts/scenes, and abstract task descriptions.


\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{\nt}: Unseen compositions of seen sub-tasks. For e.g., if train set is \{\textit{clean(apple)},~\textit{cool(apple)}\}, then this test split would contain tasks like: \{\textit{clean\_and\_cool(apple)},~\textit{clean\_then\_cool(apple)}, \textit{ cool\_then\_clean(apple)}\}.

    
    \item \textbf{\nst}: Unseen compositions of sub-task actions and target objects. For e.g., if the train set is \{\textit{clean(apple)},~\textit{cool(egg)},~\textit{clean\_and\_cool(tomato)}\}, then this test split would contain tasks like: \{\textit{clean(egg)},~\textit{cool(apple)},~\textit{clean\_and\_cool(apple)}\}.

    \item \textbf{\nc}: This test split contains the same tasks as in the train set. However, the tasks are executed in unseen kitchen scenes.

    \item \textbf{Abstraction}: Abstract task descriptions, which lack the low-level details of the task. For instance, for a \textit{heat\_and\_clean(apple)} task, the full task description in the train set could be ``apple is heated in a microwave and cleaned in sink basin", while the abstract task description in this split could be ``apple is heated and cleaned".
\end{itemize}

Note that all the test splits and the train set are disjoint from each other. \nst split tests an \etv model's ability to understand generalizable object affordances and tool usage. For instance, once a model learns the \emph{slice} action on an apple, this split tests if the model can apply it to an orange. On the other hand, the \nt split tests the generalization of a model's temporal and causal reasoning capabilities on unseen compositions and orderings of known sub-tasks. Existing real-world datasets like Ego4D~\cite{ego_4d} and EPIC-KITCHENS~\cite{epic_kitchens} fail to provide such systematic control and precise diagnostics across various relevant yet independent factors affecting task reasoning.

\subsubsection{Statistics}
\etv dataset consists of 7,673 samples (train set: 5,363 and test set: 2,310). The split-wise division is \nt: $540$, \nst: $350$, \nc: $1082$, Abstraction: $338$. The total duration of the egocentric videos in the \etv dataset is 168 hours, with an average video length of 84 seconds. To ensure diversity, each task in \etv is associated with $\approx$10 different task description templates (inclusive of positive and negative scenarios). We also keep an additional template set for the abstraction split. The task descriptions consist of 9 words on average, with a total vocabulary size of 72. On average, there are 4.6 sub-tasks per task in the \etv dataset, and each sub-task spans approximately 14 frames. Additionally, there are 2.4 ways to verify a task. This requires the virtual agent to understand all possible temporal orderings between sub-tasks from the task description for successful task verification. Real-world datasets mainly focus on recognizing actions, objects, and state changes~\cite{ego_4d, epic_kitchens} without this ambiguity. Figure~\ref{figure:stats} shows a comparison of train and test splits (more analysis in Appendix~\ref{appendix:dataset_analysis_and_statistics}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%