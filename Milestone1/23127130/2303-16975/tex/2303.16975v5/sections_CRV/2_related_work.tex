
\section{Related Work}
\label{section:related_works}

\noindent \textbf{Video-based Task Understanding.} 
Understanding tasks from videos has been a long-standing theme in vision research with focus on recognizing activities~\cite{charades_dataset, epic_kitchens}, human-object interactions~\cite{action_genome, ego_4d}, and object state changes~\cite{change_it, fathi2013modeling} using egocentric or exocentric videos. But apart from recognizing actions, objects, and state changes, task verification also requires understanding temporal orderings between them. Our work is, therefore, closer to research on understanding instructional tasks~\cite{cross_task, tang2019coin}, which require reasoning about multiple, ordered steps. Prior works focus on either learning the order of steps~\cite{bansal2022my, lin2022learning, huang2019neural, mao2023action} or use step-ordering as a supervisory signal for learning step-representations or step-segmentation~\cite{cross_task, shen2021learning}. Instead, we are focused on video-based order verification of steps described in NL, akin to~\cite{qian2022svip}.

\noindent \textbf{Temporal Video Grounding.} Our \etv benchmark is also closely related to the problem of Temporal Video Grounding (TVG)~\cite{mexaction2,human_activity_understanding,regneri2013grounding,change_it}.
However, prior work on TVG predominantly focuses on localizing a single action in the video~\cite{MAD_dataset,jiang2014thumos}.
In contrast, \etv requires localizing multiple actions, wherein actions could have partial ordering, i.e., actions could have more than one valid ordering amongst them. 

\input{tables/Tab_1_abridged}

\noindent \textbf{Vision-Language Benchmarks.} 
Various benchmark tasks have been proposed for enabling models that can reason across video and language modalities (see Table~\ref{table:list_of_datasets_abridged}). Examples include video question answering~\cite{clevrer,next_qa_dataset,agqa_dataset,activity_net_dataset,star_situated_reasoning,tvqa_dataset,movie_qa,cater_dataset}, video-based entailment~\cite{violin_dataset}, and embodied task completion~\cite{ALFRED20,teach_alexa,behavior_benchmark}. However, these benchmarks focus on individual specific aspects of multimodal reasoning, e.g., compositional reasoning (AGQA~\cite{agqa_dataset}, ActivityNet-QA~\cite{activity_net_dataset}, TVQA~\cite{tvqa_dataset}, and CATER~\cite{cater_dataset}) or causal reasoning (NExT-QA~\cite{next_qa_dataset}, CoPhy~\cite{cophy_dataset}, Causal-VidQA~\cite{causal_vid_qa}, EgoTaskQA\cite{jia2022egotaskqa}, and VIOLIN~\cite{violin_dataset}). In comparison, \etv focuses on both causal and compositional reasoning and further requires visual grounding of both objects and actions from text, similar to STAR~\cite{star_situated_reasoning} and CLEVRER~\cite{clevrer}, albeit in egocentric settings. Unlike embodied task completion benchmarks whose objective is to develop robotic agents that can \emph{perform everyday tasks} through task-planning (ALFRED~\cite{ALFRED20}, TEACh~\cite{teach_alexa}) and control (Behavior~\cite{behavior_benchmark}), EgoTV benchmark's objective is to develop virtual agents that can \emph{track and verify everyday tasks} performed by humans. Akin to NLP \emph{Entailment} problem~\cite{pascal_text_entailment,snli_ve_dataset}, it can also be viewed as a video-based entailment problem -- where a given ``premise" (video) is validated by a ``hypothesis" (task description).
% Unlike embodied instruction following (ALFRED~\cite{ALFRED20}, TEACh~\cite{teach_alexa}), which requires interpreting grounded task instructions for task execution, \etv focuses on task verification in egocentric videos from instruction-like descriptions. Behavior benchmark's~\cite{behavior_benchmark} objective is to develop robotic agents that can \emph{perform everyday tasks}, evaluated using standard reinforcement learning baselines. Instead, EgoTV benchmark's objective is to develop virtual agents that can \emph{track and verify everyday tasks} performed by humans. 

\noindent \textbf{Vision-Language Models.}
Vision-Language Models (VLMs)~\cite{clip,videoclip,luo2022clip4clip,blip,coca} pre-trained on large-scale image-text or video-language narration pairs have demonstrated enhanced performance on certain compositional~\cite{li2020hero} and causal~\cite{change_it} tasks. However, they generally struggle to handle compositionality and order sensitivity~\cite{VLMbag-of-words,winoground}. Instead, NSG explicitly targets order awareness and compositionality for generalization in task verification using neuro-symbolic reasoning.

\noindent \textbf{Neuro-symbolic Models.} Neuro-symbolic models combine feature extraction through deep learning with symbolic reasoning~\cite{nesy-survey, star_situated_reasoning} to capture compositional substructures. These models either reason on static images to recognize object attributes and relations (NS-CL~\cite{nscl}, NS-VQA~\cite{nsvqa}, CLOSURE~\cite{closure}, and $\nabla-$FOL~\cite{del-fol}), or on videos to recognize spatio-temporal and causal relations (NS-DR~\cite{clevrer} and DCL~\cite{dcl}). We extend this to tracking multi-step actions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%