
% \input{tables/task_topics}

\section{CrossTask Verification (CTV) Dataset}
\label{section:cross_task_construct}

Drawing from the \etv dataset, we introduce CrossTask Verification (CTV) dataset, using videos from the CrossTask dataset~\cite{cross_task}, to evaluate task verification models on real-world videos. In CTV, we prioritize assessing real-world performance of task verification models over systematic study of their generalization capabilities, unlike \etv. Thus, CTV complements \etv dataset -- CTV and \etv together provide a solid test-bed for future research on task verification.


% Example tasks are shown in Table~\ref{table:task_topics}. Notably, some actions, like \textit{add sugar}, are common across tasks such as \textit{Make lemonade} and \textit{Make coffee}. The temporal annotation of each action is available. 
% In CrossTask Verification evaluation, given a task description and the video, the model needs to predict if the task is accomplished (entailed) in the video. 

% \input{tables/query_definition}
% Like \etv, CTV consists of paired task descriptions and videos for task verification. We construct two settings: (1) \textbf{Action sequence verification.} NL descriptions are obtained by leveraging action step annotations in CrossTask. The model's objective is to determine whether the action steps are carried out correctly and sequentially. (2) \textbf{Task~verification.} Task class labels are used as descriptions. Here, the model requires a broader understanding of the task as a whole. The dataset construction details are provided in Appendix~\ref{appendix:cross_task_construct}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset Generation}
\label{subsection:cross_task_construct_problem}
\noindent Like \etv, CTV consists of paired task descriptions and videos for task verification. CrossTask has 18 task classes, each with roughly 150 videos, from which we create $\approx$ 2.7K samples. We generate task descriptions by concatenating action step annotations in CrossTask. The model's objective is to determine whether the action steps (sub-tasks) and their sequence in the video align with the description. 
%(2) \textbf{Task~verification.} Task class labels are used as descriptions. Here, the model requires a broader understanding of the task as a whole. 
See Appendix~\ref{appendix:cross_task_construct} for dataset construction details.
% Additional dataset construction details are provided in Appendix~\ref{appendix:cross_task_construct}.
\input{figures/cross_task_verification}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Dataset construction}
% We create the following splits: \noindent (1) \textbf{Action sequence verification} involves forming a task description for each video by concatenating a sequence of annotated \textit{action steps} from CrossTask.
% \begin{itemize}[leftmargin=*,noitemsep]
%     \item To ensure repetitive subtasks, as in \etv, we start by selecting the action steps within each task based on frequency. In the end, we select the 4 most frequent action steps with respect to each task. We then filter out video segments in CrossTask that don't contain the top 4 frequent action steps.
%     \item We also followed a similar process of EgoTV for generating negative descriptions. We include three ways of \textbf{negative description} including: (i) \textit{replacing action steps from other tasks}: where we substitute one of the action steps in the sequence to the action step in another task. (ii) \textit{replacing action steps from the same task}: instead of replacing step from another task, we reuse the unused action steps which were not the top 4 frequent steps in the same task. These action steps are closer in semantics and serve as hard negative. (iii) \textit{replace action step sequence order to impossible sequence}: we first list out all possible action step orders in CrossTask following \cite{mao2023action}. Then, we found an action sequence that doesn't exist in CrossTask, e.g., \textit{lower jack, upper jack, break on}. We called these sequence orders impossible since we can't lower the jack when the car is already on the ground. 
%     To make the task more challenging, we also generate \textbf{negative videos} by (i) \textit{shuffling the video order corresponding to each action step}: where the action steps weren't executed in the correct order. (ii) \textit{dropping a video segment corresponding to its action step}: where one action step is missing in the video. Therefore, the task wasn't executed successfully.
% \end{itemize}
% \noindent (2) \textbf{Task verification} is a more challenging setting where we directly use the \textbf{task class} of the video as the task description. Instead of generating negative samples as in \textit{action sequence verification}, the negative samples are from different task classes. We select a negative task class with shared action steps to ensure the model understands the procedure of a certain task instead of simply object-level recognition. We divide the task classes into 4 major topics as shown in Table \ref{table:task_topics}, where each topic contains shared action steps as described in Section \ref{appendix:cross_task_construct_problem}. The video follows the same process as \textit{Action sequence verification} where we select videos with the top 4 action steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation}
\noindent \textbf{Metrics.}
Following \etv, we use accuracy and F1 to measure the efficacy of the models on the CTV dataset. %We also evaluate the model's scalability on tasks with varying complexity and ordering.

\noindent \textbf{Generalization.}
%For \textit{action sequence verification}, 
We construct a test set using videos with seen action steps but in previously unseen compositions. To ensure novel compositions, we train on videos with up to 3 action steps and test on those with 4, as illustrated in Figure \ref{figure:cross_task_verification}. While this mirrors the Novel Task split in \etv, the CTV test set also contains unseen visual contexts (videos) -- a result of limited control during dataset creation. %Note that every sub-task in the test set is also present in the training set to ensure that all steps are observed (unseen compositions of seen sub-tasks). 
%For \textit{task verification}, we follow the same training and testing videos in the action sequence verification. The difference is that the task description was substituted from the action sequence to the task class name.
%For \textit{task verification}, similar to the previous setting, we also ensure the task description (task class) where seen and the sub-task composition in the video is unseen. 