\section{Experiments} 
\label{subsection:baseline_evaluation}
We compare various state-of-the-art (SOTA) VLMs with NSG on the \etv benchmark (see Appendix~\ref{appendix:nsg_training} for NSG's experimental training details).

\subsection{SOTA VLM Baselines}
We investigate 6 VLMs developed for video-language tasks requiring similar reasoning as \etv. Summarized in Table~\ref{table:baseline_results}, \textbf{CLIP4Clip}~\cite{luo2022clip4clip}, \textbf{CLIP Hitchhiker}~\cite{clip_hitchiker}, \textbf{CoCa}~\cite{coca} use image backbones followed by temporal aggregation, while \textbf{VideoCLIP}~\cite{videoclip}, \textbf{MIL-NCE}~\cite{miech2020end}, and \textbf{VIOLIN}~\cite{violin_dataset} use video backbones. With the exception of CoCa, which is trained with contrastive and captioning loss, all other models are trained using contrastive loss~\cite{miech2020end}. Lastly, VideoCLIP and VIOLIN use an explicit fusion of text-vision features. For each model, we freeze all pretrained feature extractors and finetune a fully-connected probe layer, along with the temporal aggregation layers where appropriate (CLIP4Clip-LSTM, VIOLIN), using \etv's train split. 

Finally, to establish upper bounds on \etv, we instantiate: (1) a \textbf{Text2text model}, which constructs video captions using ground-truth labels for objects and actions, encodes the captions and task descriptions using (pretrained) RoBERTa model~\cite{roberta} and measures alignment using the cosine similarity score (see Appendix~\ref{appendix:upper_bound_models}), and (2) an \textbf{Oracle model}, which is trained with full supervision on sub-tasks labels and locations in addition to task verification labels.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}
 In Table~\ref{table:baseline_results}, we show the performance of NSG vs. SOTA VLMs per split of \etv.~(1)~\textbf{Novel Tasks}: NSG significantly outperforms other baselines due to its ability to decompose and detect sub-tasks while using DP alignment to handle temporal constraints among them. In contrast, other baselines rely on detecting the entire task under temporal constraints, which is more challenging. Further, image-based baselines outperform video-based baselines due to their ability to capture a greater degree of compositional detail through frame-level representations.~(2) \textbf{Novel Steps}: NSG's poor performance in this split could be attributed to its low precision in the \emph{slice} sub-task (which is dominant in this split), as shown in Figure~\ref{figure:complexity-confusion_mat} [Right]. We hypothesize that since NSG only uses the aligned segments while discarding the rest, learning to utilize context from neighboring segments to capture \emph{slice} (like picking up a knife) could be a promising future direction. (3) \textbf{Novel Scenes}: Here, NSG is comparable to the best baseline VIOLIN-ResNet. Since the tasks are identical to the train split, the success of a model is contingent on the vision encoder's ability to accurately detect the same sub-tasks in unseen scenes. Consequently, models with an additional temporal aggregation layer (VIOLIN) finetuned on \etv, tend to outperform image-based models that do not have temporal aggregation (CLIP Hitchhiker) and models with frozen video features (MIL-NCE, VideoCLIP). (4) \textbf{Abstraction}: NSG significantly outperforms the baselines, primarily due to its semantic parser, which captures the underlying structure of the description and encodes the relevant concepts, such as objects and sub-tasks, to generate an (abstract) symbolic output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of NSG}
\label{subsec:analysis}

\noindent \textbf{NSG learns to localize task-relevant entities without explicit supervision.} Figure~\ref{figure:complexity-confusion_mat} shows the confusion matrix of \code{StateQuery} \& \code{RelationQuery} outputs, which capture sub-tasks, with their ground truths. The high recall demonstrates NSG's ability to localize task-relevant entities, despite being trained using only task verification labels.

\noindent \textbf{Effect of query types on NSG.} While query types with multiple entity arguments might appear capable of modeling complex dependencies amongst entities and having more expressive power, encoding multiple entities jointly using a single encoder makes the grounding problem more challenging. Hence, in practice, we found that using a combination of \code{StateQuery} \& \code{RelationQuery} types as opposed to \code{ActionQuery} (which encodes multiple entities using a single encoder) enabled better grounding and led to better performance in terms of F1-score (Table~\ref{table:query_comparison}).

\input{figures/complexity-confusion_mat}

\noindent \textbf{NSG shows consistent performance with increasing task difficulty.}
In Figure~\ref{figure:complexity-confusion_mat}, NSG's performance is minimally affected by increase in task difficulty characterized by number of sub-tasks (complexity) and ordering constraints (\S~\ref{section:evaluation}) unlike the best-performing baseline (VIOLIN-ResNet). 

\noindent \textbf{NSG is robust to segmentation window size} The effect of $k$ on NSG is minimal (Appendix~\ref{appendix:analysis}).


\noindent \textbf{NSG also enables task verification on real-world data.} NSG outperforms all competitive baselines on CTV significantly with F1-score (NSG: $\mathbf{76.3}$, CoCa: 70.9, VideoCLIP: 49.7, VIOLIN 34.7), demonstrating its causal and compositional reasoning capabilities in real-world applications (see Appendix~\ref{appendix:NSG_crosstask} for details).

\begin{table}[t]
\small
\centering
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{NSG} & \begin{tabular}[c]{@{}c@{}} Novel  \end{tabular} & \begin{tabular}[c]{@{}c@{}}Novel \end{tabular} & \begin{tabular}[c]{@{}c@{}}Novel\end{tabular} & \multirow{2}{*}{Abstract.} \\ & Tasks & Steps & Scenes & \\
\hline
\code{Action} & 78.2 & 45.6 & 70.6 & 75.5\\
\code{State+Relation} & 90.0 & 64.7 & 84.9 & 80.4\\
\hline
\end{tabular}
\vspace{2pt}
\caption{\code{(State}~+~\code{Relation)Query}~vs.~\code{ActionQuery}}
\label{table:query_comparison}
% \vspace{-5pt}
\end{table}  

\noindent \textbf{Limitations of NSG.} (1) It does not consider multiple simultaneous actions like ``picking an apple while closing the refrigerator door", (2) The assumption of equal-length video segments may be unsuitable for sub-tasks with a highly variable duration. We defer exploration of these limitations to future work, (3) Since NSG aligns the video with the entire task graph, it requires the full task execution video. Without this, alignment is partial, rendering NSG ineffective for online task verification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%t