%1) (ego/exo) task understanding 2) vision language tasks and models 3) research on grounding  4) vision-language models for compositional reasoning 5) neurosymbolic models.

\section{Related Work}
\label{section:related_works}
% We develop a neuro-symbolic reasoning approach to endow assistive agents that use vision and language modalities with task tracking and verification capabilites. 
% We take inspiration from the large body of work on video-based task understanding, vision-language models, and neuro-symbolic models.  %We also compare our \etv dataset to existing vision-language datasets.

\noindent \textbf{Video-based Task Understanding.} 
Understanding tasks from videos has been a long-standing theme in vision research with focus on recognizing activities~\cite{charades_dataset, epic_kitchens}, human-object interactions~\cite{action_genome, ego_4d}, and object state changes~\cite{change_it, fathi2013modeling} using egocentric or exocentric videos. But apart from recognizing actions, objects, and state changes, task verification also requires understanding temporal orderings between them. Our work is, therefore, closer to research on understanding instructional tasks~\cite{cross_task, tang2019coin}, which require reasoning about multiple, ordered steps. Prior works focus on either learning the order of steps~\cite{bansal2022my, lin2022learning, huang2019neural, mao2023action} or use step-ordering as a supervisory signal for learning step-representations or step-segmentation~\cite{cross_task, shen2021learning}. Instead, we are focused on video-based order verification of steps described in NL, akin to~\cite{qian2022svip}.

\noindent \textbf{Temporal Video Grounding.} Our \etv benchmark is also closely related to the problem of Temporal Video Grounding (TVG)~\cite{mexaction2,human_activity_understanding,regneri2013grounding,change_it}.
% TVG aims to determine the set of consecutive frames corresponding to a text query in an untrimmed video
% With the exception of CrossTask~\cite{cross_task} which uses a fixed ordering for grounding multiple actions,
However, prior work on TVG predominantly focuses on localizing a single action in the video~\cite{MAD_dataset,jiang2014thumos}.
In contrast, \etv requires localizing multiple actions, wherein actions could have partial ordering, i.e., actions could have more than one valid ordering amongst them. 
% Notably, our approach does not explicitly perform temporal grounding, instead relying on implicit localization of actions described in the text for task verification \nk{The last line is confusing for me; isn't calculating the Z matrix equivalent to doing explicit temporal grounding?}.

%\rd{these seem different than the ones Brian added, need to confirm if they are the correct/same ones. If they are the same, could we clean up citations? Also @Brian, hard to keep track of what t1, t2, t3.. etc are in citations. Please either use the default bib or the way rishi is doing with a shorthand}
% Hence, it is closely related to \etv's problem of  In the context of actions, temporal boundaries of action instances are predicted. 
% Previous work can be categorized into proposal-based and proposal-free approaches. Proposal-based approaches employ a propose-and-rank pipeline framework to localize the temporal boundaries of the target segment~\cite{t1,t2,t3,t4,t5,t6,shou2016temporal,escorcia2016daps}. These methods are computationally expensive and rely on proposal quality. Among proposal-free methods, \cite{t7,t8} uses attention-based grounding and~\cite{t9,t10} proposed reinforcement learning for regressing the start and end times of target video segments. 
%~\cite{thumos2017,activity_net_dataset,change_it}.%\rd{todo for Brian, also are these the right citations?}.

\input{tables/Tab_1_abridged}

\noindent \textbf{Vision-Language Benchmarks.} 
Various benchmark tasks have been proposed for enabling models that can reason across video and language modalities (see Table~\ref{table:list_of_datasets_abridged}). Examples include video question answering~\cite{clevrer,next_qa_dataset,agqa_dataset,activity_net_dataset,star_situated_reasoning,tvqa_dataset,movie_qa,cater_dataset},
% video captioning~\cite{vatex,activity_net_captions,youCook},
% cross-modal retrieval~\cite{msvd,msr-vtt,vatex},
video-based entailment~\cite{violin_dataset}, and embodied task completion~\cite{ALFRED20,teach_alexa,behavior_benchmark}. However, these benchmarks focus on individual specific aspects of multimodal reasoning, e.g., compositional reasoning (AGQA~\cite{agqa_dataset}, ActivityNet-QA~\cite{activity_net_dataset}, TVQA~\cite{tvqa_dataset}, and CATER~\cite{cater_dataset}) or causal reasoning (NExT-QA~\cite{next_qa_dataset}, CoPhy~\cite{cophy_dataset}, Causal-VidQA~\cite{causal_vid_qa}, EgoTaskQA\cite{jia2022egotaskqa}, and VIOLIN~\cite{violin_dataset}). In comparison, \etv focuses on both causal and compositional reasoning and further requires visual grounding of both objects and actions from text, similar to STAR~\cite{star_situated_reasoning} and CLEVRER~\cite{clevrer}, albeit in egocentric settings.
Unlike embodied instruction following (ALFRED~\cite{ALFRED20}, TEACh~\cite{teach_alexa}), which requires interpreting grounded task instructions for task execution, \etv focuses on task verification in egocentric videos from instruction-like descriptions. Akin to NLP \emph{Entailment} problem~\cite{pascal_text_entailment,snli_ve_dataset}, it can also be viewed as a video-based entailment problem -- where a given ``premise" (video) is validated by a ``hypothesis" (task description).

% Hence, \etv is closest in spirit to the entailment dataset: VIOLIN~\cite{violin_dataset}. However, unlike \etv, VIOLIN limits itself to causal reasoning.

% In comparison to all of the aforementioned benchmarks, \etv focuses on both causal and compositional reasoning of spatiotemporal (video) and multimodal (video, language) information. Furthermore, it require grounding of both objects and actions for egocentric task verification. These characteristics make \etv unique amongst existing vision-language and video benchmarks and datasets. 


% \etv tasks require grounding of both objects and actions. Furthermore, both causal and compositional reasoning of spatiotemporal (video) and multimodal (video, language) information is needed. These characteristics make \etv unique amongst existing vision-language and video benchmarks and datasets (see Table~\ref{table:list_of_datasets} for a detailed comparison). 

% Instead of executing instructions for a task, \etv requires causal and compositional reasoning of instructional tasks for their verification using egocentric video and is closest in spirit to video-based entailment e.g., VIOLIN~\cite{violin_dataset}. However, unlike VIOLIN, \etv focus

% In contrast to previous tasks that evaluate such abilities through implicit metrics like question answering, we employ an explicit evaluation approach by creating novel composition splits and tasking the model with learning causal order from text in a verification setting akin to visual entailment. Moreover, while previous video entailment work has focused on reasoning high-level concepts in movie understanding, such as VIOLIN\cite{violin_dataset}, our study narrows the scope to instructional tasks, thereby providing novel insights into egocentric video analysis.


%Unlike previous tasks implicitly task the model's ability to reason compositionality and causal order by other metrics such as question answering, we explicitly evaluate these caracteristics by creating unseen composition splits and task the model to learn the causal order from text in a task verification setting similar to visual entailment. In addition, instead of reasoning high-level concepts in movie understaiding such as VIOLIN\cite{violin_dataset}, we focus on reasoning of instructional tasks order using egocentric video.



%and is closest in spirit to video-based entailment~\cite{violin_dataset}. \red{RH: CREPE, EgoTaskQA} \rd{we could limit this section to VL benchmarks and discuss models in the next para if we want..}
%\bc{We should talk about the difference to video entailment task, e.g. cares about partial order in the sub-tasks}

% such as (i) visual question answering that covers datasets and tasks for spatial reasoning datasets on images: CLEVR~\cite{clevr_dataset}, GQA~\cite{gqa_dataset} and Visual Genome~\cite{visual_genome}; spatio-temporal and compositional reasoning: CLEVRER~\cite{clevrer}, NExT-QA~\cite{next_qa_dataset}, AGQA~\cite{agqa_dataset}, ActivityNet-QA~\cite{activity_net_dataset}, STAR~\cite{star_situated_reasoning} and TVQA~\cite{tvqa_dataset}; causal reasoning: CLEVRER~\cite{clevrer}, NExT-QA~\cite{next_qa_dataset}, Social-IQ~\cite{social_iq} and Causal-VidQA~\cite{causal_vid_qa}; (ii) visual captioning on images: MSCOCO~\cite{mscoco_dataset}, Flickr30k~\cite{flickr30k} and videos: MovieQA~\cite{movie_qa}; (iii) visual entailment on images SNLI-VE~\cite{snli_ve_dataset} and videos: VIOLIN~\cite{violin_dataset}. Solving these tasks requires perceptual abilities such as recognizing objects, and relations~\cite{} as well as higher-level skills such as performing logical inference, or leveraging commonsense world knowledge~\cite{}. However, these models cannot handle partially observable states or egocentric views. Additionally, the tasks in our dataset require models to perform action recognition. % reasoning on (static) images (clevr) or fully observable settings (clevrer)
\noindent \textbf{Vision-Language Models.}
Vision-Language Models (VLMs)~\cite{clip,videoclip,luo2022clip4clip,blip,coca} pre-trained on large-scale image-text or video-language narration pairs have demonstrated enhanced performance on certain compositional~\cite{li2020hero} and causal~\cite{change_it} tasks. However, they generally struggle to handle compositionality and order sensitivity~\cite{VLMbag-of-words,winoground}.
Instead, NSG explicitly targets order awareness and compositionality for generalization in task verification using neuro-symbolic reasoning.
%, attributed to the lack of hard negatives in the training setup. 

%\red{RH: work in progress}

% , and they come in two primary designs -- (i) dual encoder model for parallel encoding of text and images in the same latent space~\cite{} using contrastive loss~\cite{}; (ii) encoder-decoder models for generative modeling of text from images using language modeling loss~\cite{}. More recently, a more unified framework~\cite{} that is pretrained with both contrastive and language modeling losses has demonstrated superior performance. 

% \noindent \textbf{Compositional Reasoning.}
% % for Compositional Generalization}
% % “BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning
% Although vision-language models have achieved impressive results on various benchmarks, they often fall short at compositional generalization~\cite{winoground,VLMbag-of-words}. \rd{we cited so many papers in the previous para on compositional reasoning e.g., NExt-QA, AGQA etc. So as a reader I won't buy this last line or at least will wonder what does the line really mean.} 
% Prior works have proposed to disentangle the visual and language modules while leveraging the \textit{compositional} aspect of NL to retrieve structured representations, like executable programs via program synthesis~\cite{nscl,nsvqa}, first-order logic (FOL)~\cite{del-fol}, and parse-trees~\cite{kuo2020compositional,closure}. 
% Despite generalizability, their applications are, in general, limited to relatively simpler benchmarks like gSCAN~\cite{gScan} and CLEVR~\cite{clevr_dataset} and cannot easily be scaled up to complex datasets like \etv. 
% Thus, we use it as a (controlled) test-bed to perform quantitative evaluation on a range of end-to-end state-of-the-art vision-language frameworks and show that they fail to learn compositional generalization, and how it can be improved using our proposed framework (NSG). \rd{potentially get rid of this para or merge it with the para below on neurosymbolic models. Instead, describe vision language models in this para}
\noindent \textbf{Neuro-symbolic Models.} Neuro-symbolic models combine feature extraction through deep learning with symbolic reasoning~\cite{nesy-survey, star_situated_reasoning} to capture compositional substructures.
% Prior works~\cite{nsvqa,nscl,nsvr,closure,clevrer} have demonstrated their effectiveness at capturing compositional substructures, resulting in improved interpretability and generalization.
These models either reason on static images to recognize object attributes and relations (NS-CL~\cite{nscl}, NS-VQA~\cite{nsvqa}, CLOSURE~\cite{closure}, and $\nabla-$FOL~\cite{del-fol}), or on videos to recognize spatio-temporal and causal relations (NS-DR~\cite{clevrer} and DCL~\cite{dcl}). We extend this to tracking multi-step actions.
% While, NS-SR~\cite{star_situated_reasoning} can track actions in (real-world) videos, unlike NSG, it does not undergo end-to-end training.

% Notably, our NSG framework can handle egocentric videos in a partially observable setting. Another similar model in this domain is the NS-SR~\cite{star_situated_reasoning} which uses a modular framework to perform situated reasoning on real-world videos \rd{what type of reasoning? could we relate this reasoning to our setup?}, however, the performance drastically drops when all the modules are trained end-to-end.
% nscl, nsvqa, del-fol, NS-DR (clevrer), jarvis, star

% Systematic Generalization refers to a learning algorithm’s ability to extrapolate learned behavior to unseen situations that are distinct but semantically similar to its training data.
% . These representations are jointly processed with relational representations obtained from a visual module (like scene-graphs~\cite{}) via attention~\cite{}, graph-neural-networks~\cite{}, or symbolic reasoning~\cite{}. 