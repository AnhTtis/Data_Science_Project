\section{NSG on Real-world Data}
\input{tables/crosstask_results.tex}

To evaluate how NSG enables task verification in real world, we leverage an existing instructional video dataset called CrossTask~\cite{cross_task} to create CrossTask Verification dataset (CTV). CrosstTask contains 18 tasks with around 150 videos per task. For each task, such as ``Fixing tire", there are corresponding action steps, such as ``jack up" and ``change tire," required to complete the task and their temporal annotations are provided. The action steps for different tasks may overlap, for example, ``add sugar" is a shared step between ``Make lemonade" and ``Make coffee". 

% In this section, we describe the experimental setup for evaluating our proposed model on our newly proposed CrossTask Verification dataset (CTV). The CrossTask Verification dataset is constructed from the CrossTask dataset \cite{cross_task} which contains 18 task classes with around 150 videos per class. For each task, such as ``Fixing tire", there are corresponding action steps, such as ``jack up" and ``change tire," required to complete the task. The action steps for different tasks may overlap, for example, ``add sugar" is a shared step between ``Make lemonade" and ``Make coffee". The temporal annotation of each action step is also available.

Akin to \etv, CTV consists of paired task descriptions and videos for task verification. We construct CTV in two settings:~(1)~\textbf{Action sequence verification}: The sequence of annotated action steps for each video is concatenated to generate task description for a positive sample, while descriptions for negative samples are generated using a similar process as EgoTV (Sec.~\ref{section:dataset}). We also shuffle the video frames order corresponding to each action step to create challenging negative samples (Fig.~\ref{figure:cross_task_verification}).~(2)~\textbf{Task verification}: We directly use the task class of the video as the task description for positive samples. The task classes that have similar action steps, but different compositions are used to generate descriptions for negative samples. Appendix~\ref{appendix:cross_task_construct} provides more details and examples. We construct a test set using videos with unseen action compositions. While this is equivalent to Novel Task split  (Sec.~\ref{section:evaluation}) from \etv in spirit, CTV test set also contains some amount of novel visual context and steps due to limited control during generation.

% Since the real-world data CrossTask is not controllable as our EgoTV dataset, we only test on the Novel Task split described in Section \ref{section:evaluation}, where we withhold certain videos with unseen action compositions for testing. 

We compare the performance of NSG against selected baselines described in Sec.~\ref{subsection:baseline_evaluation}, which were either applied to the previous CrossTask evaluation (MIL-NCE, VideoCLIP) or had competitive performance (CoCa, VIOLIN) on \etv. Note that the methods for CrossTask are not directly applicable to CTV since CTV focuses on task verification instead of temporal localization. Table \ref{table:crosstask_results} shows that the baseline models MIL-NCE, VideoCLIP, CoCa perform better than VIOLIN on CTV as compared to \etv. This indicates that CTV is potentially able to better harness the gains from the large-scale VL pretraining in these models compared to \etv. Inspite of such gains,
NSG outperforms the baselines by a significant margin in both action sequence and task verification settings. The results demonstrate the applicability of NSG's causal and compositional reasoning capabilities in the real world. 

% Our proposed CrossTask Verification dataset and evaluation demonstrate the usefulness of our model for real-world video understanding tasks. The dataset construction and evaluation methods can be used for future research in video understanding tasks. Detailed experiment results of the CrossTask Verification dataset are included in the supplementary material.

% Taking inspiration from our proposed \etv dataset, we construct a new dataset CrossTask Verification from CrossTask dataset\cite{cross_task} to test the model's performances on real-world videos. 
% \bc{talk about dataset creation in more detail}
% CrossTask \cite{cross_task}---consisting of 18 task classes with around 150 videos per class. Given each task, e.g., \textit{fixing tire}, there are corresponding action steps, \textit{jack up}, \textit{change tire} to accomplish such task. Also, the action steps across different tasks are possibly shared, e.g., \textit{add sugar} in \textit{Make lemonade} or \textit{Make coffee}. The temporal annotation of each action step is available. 
% In CrossTask Verification evaluation, given a task description and the video, the model needs to predict if the task is accomplished (entailed with video). %We followed the same process 
% We constructed two settings for evaluation. (1) \textbf{Action sequence verification} is done by  concatenating a sequence of \textbf{action steps} annotated in CrossTask for each video as our task description. We also followed a similar process of EgoTV for generating negative descriptions. To make the task more challenging, we also shuffled the video order correspond to each action step to create negative samples.
% (2) \textbf{Task verification} is a more challenging setting where we directly use the \textbf{task class} of the video as the task description. The negative descriptions are sampled from the other task classes with similar action steps, e.g., both have \textit{add sugar} step to task the model's understanding of the procedure of certain task instead of simply object level recognition. 
% %\bc{Talk about shared action steps for similar task, its a hard task }
% Since the real-world data is not controllable as our \etv dataset, we only test on the Novel Task split described in Section \ref{section:evaluation}, where we withhold certain videos with unseen action compositions for testing.
% We compare the selected baselines described in Section \ref{subsection:baseline_evaluation} which were applied to the previous CrossTask evaluation or with competitive performance. Note that the methods for CrossTask are not directly applicable to this task since the objective is different, temporal localization v.s. task verification.
% As shown in Table \ref{table:crosstask_results}, our method outperforms the baselines by a significant margin, demonstrating our model's generalizability to the real-world dataset. Also, the model captures the description order and even unseen combination of action.
% More details and experiment results of the CrossTask Entailment dataset are included in the supplementary. 
%\bc{updated by 2/27, Todo: add how do we use graph} \\
%\bc{draw figure for crosstask}
%\bc{we can include classfication model if we have time}

%\red{RH: To Add? How is our method different from the CrossTask method and why we do not compare with it? (1. temporal localization assuming only positive hypotheses in Cross Task, as opposed to positive and negative hypotheses, hence weakly supervised vs. supervised)}