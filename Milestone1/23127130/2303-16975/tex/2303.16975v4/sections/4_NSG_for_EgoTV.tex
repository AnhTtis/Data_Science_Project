
\input{figures/model-layout}

\section{Neuro-Symbolic Grounding (NSG)}
\label{section:proposed_framework}

\etv requires visual grounding of task-relevant entities such as actions, state changes, etc. extracted from NL task descriptions for verifying tasks in videos. To enable grounding that generalizes to novel compositions of tasks and actions, we propose the Neuro-symbolic Grounding (NSG) approach. NSG consists of three modules:~a)~semantic parser, which converts task-relevant states from NL task descriptions into symbolic graphs,~b)~query encoders, which generate the probability of a node in the symbolic graph being grounded in a video segment, and~c)~video aligner, which uses the query encoders to align these symbolic graphs with videos. NSG thus uses intermediate symbolic representations between NL task descriptions and corresponding videos to achieve compositional generalization.

% Given a NL task description and the corresponding video, the \etv benchmark requires a
% % a binary prediction of whether the task is accomplished in the video. To that end, a 
% model to a) understand the relevant objects, actions, and their orderings from the task description and b) ground these objects and actions in the video while respecting the ordering constraints, for verifying appropriate task execution in the video. 
% To facilitate extraction of task-relevant concepts from NL descriptions in a manner conducive to visual grounding and generalization to novel tasks, we propose Neuro-Symbolic Grounding (NSG) approach. 
% Furthermore, these reasoning capabilities must generalize to novel tasks with unseen compositions of actions and objects. 

% The semantic parser and the aligner are enabled by two ingredients: DSL to define symbolic operators, Neural encoder
% leverages Domain-specific language (DSL) to capture task-relevant information in a structured, symbolic manner. 
% Specifically, using DSL, NSG parses the task instructions into a partially-ordered symbolic graph?

% True to its name, NSG leverages a Domain-specific language (DSL) to define \emph{symbolic} operators that capture task-relevant information in a structured manner. The corresponding \emph{neural} counterpart in NSG are encoders, which evaluate the symbolic operators on continuous representations e.g., video embeddings. We first describe these ingredients before diving into the aforementioned modules of our NSG approach.
% oops sorry let me take care of this
% \vspace{-15pt}
\subsection{Queries for Symbolic Operations}
\label{subsection:queries}

To encode tasks, NSG captures task-relevant visual and relational information in a structured manner via symbolic operators called \emph{queries}.
For instance, the task description \emph{heat an apple} can be symbolically captured by the query: \code{StateQuery(apple, hot)}.
Similarly, the task description \emph{place steak on grill} can be captured by \code{RelationQuery(steak, grill, on)}, which represents the relation (\code{on}) between objects \code{steak} and \code{grill}.
Queries are characterized by \textit{types} and \textit{arguments} and are stored in a text format. Table~\ref{table:QTypes} in the appendix shows the various query types and their arguments. Different query types capture different aspects, e.g., attributes, relations, etc., thereby enabling a rich symbolic representation of everyday tasks.

\subsection{Semantic Parser for Task Descriptions}
\label{subsection:plan_parsing_from_instructions}

The symbolic operators, i.e., queries, allow the semantic parser to represent a task's partial-ordered steps using a symbolic graph. Specifically, the parser translates an NL task description into a graph $G(V, E)$, where a vertex $n_i \in V$ represents a query and an edge $e_{ij}: n_i \rightarrow n_j \in E$ is an ordering constraint indicating that $n_i$ must precede $n_j$~(Figure~\ref{figure:model-layout}a). We experiment with two different methods to parse language descriptions of tasks to graphs -- (i) finetuning language models and (ii) few-shot prompting of language models. Appendix~\ref{section:appendix_semantic_parsing} describes these approaches in detail. We perform a topological sort with the graph $G$ and generate all the possible sequences of queries consistent with the sort. For example, the topological sorting of the graph in Figure~\ref{figure:model-layout}(a) yields two ordered sequences: $(n_0, n_1, n_2, n_3)$, $(n_0, n_2, n_1, n_3)$. Note that this does not include all physically possible ways to complete a task, but a super-set of all possible sequences of task-relevant queries, including some infeasible sequences\footnote{For instance, in Figure~\ref{figure:model-layout}a, $n_1$ and $n_2$ are at the same topological level, but the sub-task in query $n_1$ could invalidate pre-conditions for $n_2$. Hence, a physically plausible task requires $n_2$ followed by $n_1$ and not vice versa. Note that \etv does not have physically implausible tasks.}. However, having this super-set is useful because a task can be verified as accomplished if any sequence in this set can be ascertained to occur in the video.

\subsection{Query Encoders for Grounding}
\label{subsection:query_encoders}

Query Encoders are neural network modules that evaluate whether a query is satisfied in an input video. Specifically, a query encoder $f^{\theta_{\tau}}$ for a query $n$ of type $\tau$ (e.g., \code{StateQuery}, \code{RelationQuery} etc.), accepts NL arguments ($a$) corresponding to objects and relations in $n$ and a video ($v$) to generate the probability $\mathds{P}=f^{\theta_{\tau}}(a, v)$ of the desired query being true in the video. Learnable parameters corresponding to different query type encoders in an NSG model are jointly represented as $\theta = \bigcup_{\tau}\theta_{\tau}$.

Both the text arguments $a$ of the query and the frames of the input video $v$ are encoded using a pre-trained CLIP encoder~\cite{clip}. The token-level and frame-level representations from CLIP are separately aggregated using two LSTMs~\cite{lstm} to obtain aggregated features for $a$ and $v$, respectively. These features are then fused and passed through the neural network $f^{\theta_{\tau}}$ to obtain the probability $\mathds{P}$ of the query being true in the video (see Figure~\ref{figure:model-layout}a).

%=====================================================================
% moved plan generation to the appendix (label{appendix_semantic_parsing})
%=====================================================================

\subsection{Video Aligner for Task Verification}
\label{subsection:plan_verification}
% say we formulate as a query alignment problem
This module of NSG must align the graph representation $G$ of the task (generated by the semantic parser) with the video. To that end, it first segments the video, then jointly learns~a)~the query encoders, which detect the queries in the video segments and~b)~the alignment between video segments and the query sequences obtained from the topological sort on $G$. Such joint learning is required since the temporal locations of the queries in the video are unknown a priori requiring simultaneous detection and alignment. If the video is a positive match for the task encoded in $G$, at least one of the query sequences from $G$ must temporally align perfectly with the video segments for successful task verification. Conversely, for negative matches, no query sequence from $G$ would \emph{completely} align with the video segments. Going forward, we use $\langle\rangle$ and $()$ to denote ordered pairs and sequences, respectively.

% We next explain the video segmentation and DP-based approach in detail.

% Such alignment between queries and segments is challenging since the temporal locations of the queries in the video are unknown apriori. Hence, queries must be localized in the video, even if in an implicit manner, for successful alignment. We next explain the video segmentation and DP-based alignment in detail.

% Given the super-set of query sequences from the sort, the NSG model must implicitly localize these queries in the video segments since the temporal locations of the queries in the video are not known for the task but are nevertheless essential for task verification. If the video is indeed a positive sample for the graph $G$ derived from the task specification, then at least one of the query sequences from $G$ must temporally align perfectly with the video segments for successful task verification. Vice versa, for negative samples, no query sequence from the graph $G$ would \emph{completely} align with the video segments.

\noindent \textbf{Video Segmentation:} The video is segmented into non-overlapping segments\footnote{Since pretrained, off-the-shelf video segmentation models are limited to predefined action classes~\cite{escorcia2016daps} or reliant on background frame change detection~\cite{yang2022temporal} and require downstream finetuning~\cite{gao2020accurate}, we leave their integration in NSG as future work.} with a moving window of arbitrary but fixed size $k$\footnote{If required, the last segment is zero-padded to $k$ frames.} 
% and obtain a temporal sequence of $S$ segments $\langle s_t | 0 \leq t \leq S-1 \rangle$.
% Once we have the graph representation $G$ for the task comprising of queries on its nodes, 


% \noindent \textbf{Query Encoder:} Next we must learn to identify if a query node $n$ is indeed being performed in a video segment $s$. We do this by training neural network modules. Specifically, a query $n$ with type $\T(n)=\tau$ (e.g., \code{StateQuery}, \code{RelationQuery} etc.) is executed by a neural network $f^{\theta_{\tau}}$. It accepts NL arguments ($\textit{args}$) e.g., objects and relations in the query $n$, and a video segment ($s$), to generate the probability $\mathds{P}=f^{\theta_{\tau}}(\textit{args}, s)$ of the desired query being true in the video segment. Both the text arguments of the query and the $k$-frames of the video $s$ are encoded using a pre-trained CLIP encoder~\cite{clip}. The resultant token-level and frame-level features are separately aggregated using two LSTMs~\cite{lstm} to obtain segment and sentence features, respectively, which are then fused and passed through the neural network $f^{\theta_{\tau}}$ to obtain the probability of the query being true in the video segment (see Figure~\ref{figure:model-layout}).

% Learnable parameters corresponding to different query types are jointly represented as $\theta = \bigcup_{\tau}\theta_{\tau}$. Going forward, for a query $n_j$ and segment $s_t$, the corresponding notation is simplified to $\mathds{P} = f^{\theta}(a_j, s_t)$, where $a_j$ is short for $args_j$, and the subscript $j$ uniquely determines the query encoder (i.e. \code{StateQuery, RelationQuery}). 

\noindent \textbf{Joint Optimization:} The objective of the optimization is to jointly learn the \emph{alignment} $\mathrm{Z}$ between queries and video segments along with the \emph{query encoders} $f^{\theta}$.
% that detect queries in the segments and output a task verification probability for \etv tasks.
% , using only the ground truth label $y$ as supervision.
Given:~a)~the temporal sequence of $S$ segments $(s_t)_{t=0}^{S-1}$ with each $s_t$ spanning $k$ image frames; and~b)~a sequence of $N$ queries $(n_j)_{j=0}^{N-1}$  from the topological sort on $G$, the alignment $\mathrm{Z}$ is defined as a matrix $\mathrm{Z} \in \{0, 1\}^{N \times S}$, where $Z_{jt} = 1$ implies that the $j^{th}$ query $n_j$ is aligned the video segment $s_t$. An example alignment with $N=2$ and $S=3$ is given by the matrix $\mathrm{Z} = \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 0 & 1 \end{array} \right]$, where the rows are ordered queries $(n_0, n_1)$, the columns are temporal segments $(s_0, s_1, s_2)$, and $\langle n_0, s_0 \rangle$, $\langle n_1, s_2 \rangle$ are the aligned pairs. Assuming segmentation guarantees sufficient segments for query alignment: $S \geq N$. Using $\mathrm{Z}$ and $f^{\theta}$, the task verification probability $p^{\theta}$ can be defined as:
\begin{align}
     p^{\theta} = \sigma \bigg(\max_{\mathrm{Z} \in {\{0,1\}}^{N \times S}} \frac{1}{N}\sum_{j,t} \log f^{\theta}(a_j, s_t)Z_{jt}\bigg) \label{eq:p_theta} 
     %\vspace{-0.2cm}
\end{align}
Here $\sigma$ is the sigmoid function, $f^{\theta}(a_j, s_t)$ denotes the probability of querying segment $s_t$ using query $n_j$ with arguments $a_j$ (Sec.~\ref{subsection:query_encoders}), and $\max$ operator is over the best alignment $\mathrm{Z}$ between $N$ queries and $S$ segments. We use the ground-truth task verification label $y$ to compute $\mathrm{Z}$ and $f^{\theta}$ by minimizing the following loss:
\begin{align}
     \min_{\theta} & \frac{1}{|\mathcal{D}|} \sum \mathcal{L}_{\text{BCE}}(p^{\theta}, y), \label{eq:bce}
    %\vspace{-0.2cm}
\end{align}
here $|\mathcal{D}|$ is the \etv dataset size and $\mathcal{L}_{\text{BCE}} (\cdot)$ is the binary cross entropy loss computed over $|\mathcal{D}|$ input,~output pairs.~Given the minimax nature of Eq.~\ref{eq:bce}, we use a 2-step iterative optimization process:~(i)~find the best alignment $\mathrm{Z}$ between queries and segments with fixed query encoder parameters $\theta$ (optimize Eq.~\ref{eq:p_theta} with fixed $f^{\theta}$);~(ii)~optimize $\theta$ using Eq.~\ref{eq:bce}, given $\mathrm{Z}$.

% Let $S =\left[s_1,\dots,s_p\right]$ denote the temporal sequence of segments, with each $s_i$ spanning $k$ image frames and $N= \left[n_0, \dots, n_q\right]$ be the sequence of node queries from the topological sort on $G$. The alignment between $N$ and $S$ is encoded using an alignment matrix $\mathrm{Z} \in \{0, 1\}^{p \times q}$, where $z_{jt} = 1$ implies that the $j^{th}$ query $n_j$ is aligned with the video segment $s_t$. An example alignment with $p=2$ and $q=3$ is given by the matrix $\mathrm{Z} = \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 0 & 1 \end{array} \right]$, where the rows are ordered queries $\left[n_0, n_1\right)$, the columns are temporal segments $\left[s_0, s_1, s_2\right]$ and $(n_0, s_0)$, $(n_1, s_2)$ are the aligned pairs. We assume that segmentation ensures $p \geq q$. \red{RH: add why}

% For simplicity, we explain the alignment of a single sequence of queries with video segments and generalize later to the super-set of all query sequences from the topological sort on $G$. 

% Given: (i) the temporal sequence of $S$ segments $\langle s_t | 0 \leq t \leq S-1 \rangle$ with each $s_t$ spanning $k$ image frames; and (ii) a sequence of $N$ queries $\{n_0, \dots, n_N\}$  from the topological sort on $G$, we define the alignment matrix $\mathrm{Z} \in \{0, 1\}^{N \times S}$, where $z_{jt} = 1$ implies that the $j^{th}$ query $n_j$ is aligned the video segment $s_t$. An example alignment with $N=2$ and $S=3$ is given by the matrix $\mathrm{Z} = \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 0 & 1 \end{array} \right]$, where the rows are ordered queries $\{n_0, n_1\}$, the columns are temporal segments $\{s_0, s_1, s_2\}$ and $(n_0, s_0)$, $(n_1, s_2)$ are the aligned pairs. We assume that segmentation is done in a way that $S \geq N$. \red{RH: add why}

% The alignment is done by minimizing the binary cross-entropy loss with respect to the query model parameters $\theta$, given input $x_i$ (video, task description) and output $y_i$ (verification label): 
% \begin{align}
%      \min_{\theta} & \frac{1}{M} \sum_i \mathcal{L}_{\text{BCE}}(p^{\theta}(x_i), y_i) \label{eq:bce}\\
%      \text{where, }& p^{\theta}(x_i) = \sigma \bigg(\frac{1}{N}\sum_{j,t} \log n_j^{\theta}(s_t)Z_{jt}^{\ast}\bigg) \label{eq:p_theta}  
% \end{align}
% \begin{align}
%      \min_{\theta} & \frac{1}{M} \sum_i \mathcal{L}_{\text{BCE}}(p^{\theta}(x_i), y_i) \label{eq:bce}\\
%      \text{here, }& p^{\theta}(x_i) = \sigma \bigg(\max_{\mathrm{Z} \in {\{0,1\}}^{N \times S}} \frac{1}{N}\sum_{j,t} \log f^{\theta}(a_j, s_t)Z_{jt}\bigg) \label{eq:p_theta}  
% \end{align}

% \nk{Between eqs 2 and 3, we are introducing Z* for no reason. Can we simply get rid of the argmax in eq 3 and Z* in eq 2, and instead have a Z in eq 2 and a max inside the sigmoid instead? That should also clearly illustrate the minimax nature of the problem, which you can later exploit to justify the alternating optimization procedure over Z and $\theta$.} 
% Here, $p^{\theta}(x_i)$ is the probability of plan verification, obtained by adding the log probabilities of the query-segment pairs computed over the best alignment matrix $\mathrm{Z}$ (as denoted by the $\max$ operator). 

% Intuitively, for positive samples ($y_i=1$), this probability would be high, while for negative samples ($y_i=0$), it would be low (ideally $=0$) for all possible alignments, including the best one.

% and $\mathrm{Z}^{\ast}$ denotes the best localization of the queries in the video 
% \nk{What do we mean by best localization here? How is it defined? If I understand it correct, a video positively matched to a task description will still only align with exactly one of the query sequences generated from the task description graph $G$.}
% and is given as:
% \begin{equation}
%     \mathrm{Z}^{\ast} = \argmax_{\mathrm{Z} \in \{0,1\}^{N \times S}} \frac{1}{N} \sum_{j,t} \log n_j^{\theta}(s_t)Z_{jt}
% \label{eq:Z}
% \end{equation}

% \nk{Here j in $n_j$ is the query sequence index but it is conflicting with the query types in $n_i$ from the previous section while they don't have anything in common. We need to change the notation to correct this.}


% \nk{This intuitive picture seems slightly incorrect to me. $Z$ is only the best alignment if the label $y_i$ is 1 and the query sequence is indeed the one being executed in the video. If the label $y_i$ is 0, $p^\theta(x_i)$ would also have to be 0. For that wouldn't the elements of Z have to be maximally misaligned? But it seems like maximizing eq 3 wrt Z will force them to always align the video segments and the query sequence even when the label $y_i$ is 0. Am I misinterpreting something here?}.

% \nk{Instead of just declaring the alternating optimization routine, we should justify it properly based on the minimax nature of the problem}: 

% (i) find the best localization with fixed model parameters (optimize $\mathrm{Z}$ with $\theta$ fixed); (ii) optimize the model parameters given the best localization (optimize $\theta$ with $\mathrm{Z}$ fixed).

% \nk{The meaning of best localization is unclear to me. It seems like we are always assuming $y_i=1$ here. Wouldn't we need some kind of complete misalignment in case of $y_i=0$?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \nk{Do you mean integer programming? Because elements of Z must be binary, right?}

\noindent \textbf{Dynamic Programming (DP)-based Alignment:} Finding the best $\mathrm{Z}$ in Eq.~\ref{eq:p_theta} given $\theta$ requires iterating over combinations of $N$ queries and $S$ segments while respecting certain constraints. The constraints, visualized in Fig.~\ref{figure:model-layout}b, ensure that~a)~no two queries are aligned to the same segment\footnote{This ensures that the order of queries can be verified, which cannot be done when queries belong to the same segment.} (Eq.~\ref{Z-a}),~b)~all queries are accounted for in $S$ (Eq.~\ref{Z-b}), and~c)~the temporal orderings between queries in the query sequences are respected (Eq.~\ref{Z-c}). Specifically, if query $n_u$ precedes $n_v$ ($n_u \rightarrow n_v$), and query $n_v$ is paired with segment $s_{\bar{t}}$ (i.e. $Z_{v\bar{t}}=1$), then query $n_u$ cannot be paired with any segment that lies after $s_{\bar{t}}$ (i.e. $Z_{ut} \neq 1 \; \forall \; t \geq \bar{t}$). The resulting optimization problem for $\mathrm{Z}$, given $\theta$ is:

\begin{subequations}\label{Z-main}
\begin{align}
& \max_{\mathrm{Z} \in {\{0,1\}}^{N \times S}} \sum_{j,t} \log f^{\theta}(a_j, s_t)Z_{jt} \tag{\ref{Z-main}}, \quad \text{s.t.}\\
& \sum_{j=0}^{N-1} Z_{jt} \in \{0,1\}, \quad \forall \; 0 \leq t \leq S-1 \label{Z-a}\\
&\sum_{t=0}^{S-1} Z_{jt} = 1, \quad \forall \; 0 \leq j \leq N-1 \label{Z-b}\\
% & n_u \rightarrow n_v, \; Z_{v\bar{t}}=1 \Longrightarrow Z_{ut} \neq 1, \quad \forall \; t \geq \text{argmax}_{\bar{t}} Z_{v\bar{t}} \label{Z-c}
& n_u \rightarrow n_v, \; Z_{v\bar{t}}=1 \Longrightarrow Z_{ut} \neq 1, \quad \forall \; t \geq \bar{t} \label{Z-c}
\end{align}
\end{subequations}

% \begin{subequations}
% \begin{align*}
%     & \sum_{j=0}^{N-1} Z_{jk} \in \{0,1\} \; \forall \; 0 \leq k \leq S-1 \\
%     & \sum_{j,k} Z_{jk} = N \\
%     & n_u < n_v \Longrightarrow Z_{uk} \neq 1 \; \forall \; k \geq \text{argmax}_{\bar{k}} Z_{v\bar{k}}
% \end{align*}
% \end{subequations}

% \nk{I might be misinterpreting it but Eq 3b doesn't seem to be suggesting that all queries must be localized. It still permits a query to not be localized at all, provided that another query is localized into two video segments. Only the total number of localizations must be N. Is this correct?}
% As shown in Figure~\ref{figure:model-layout}(b), Eq.~\ref{Z-a} implies that at most one query can be paired with each segment; Eq.~\ref{Z-b} implies that all queries must be paired; Eq.~\ref{Z-c} handles the ordering of queries -- if $n_u$ precedes $n_v$ ($n_u \rightarrow n_v$), and query $n_v$ localizes in the segment $s_{\bar{t}}$, then query $n_u$ cannot localize in any segment ($Z_{ut} \neq 1$) that lies after $s_{\bar{t}}$ \nk{Eq 4c is currently not stating the fact that $n_v$ localizes in segment $s_{\bar{t}}$. We need to add $Z_{v\bar{t}}=1$}. 
Intuitively, the solution to Eq.~\ref{Z-main} gives us the best alignment score (note, the overlap with Eq.~\ref{eq:p_theta}). The iterations over $N$ queries and $S$ segments for solving Eq.~\ref{Z-main} are underpinned by an overlapping and optimal substructure. For instance, to optimally align queries $(n_j)_{j=0}^{N-1}$ and segments $(s_t)_{t=0}^{S-1}$, one could:~a)~pair $\langle n_0, s_0 \rangle$ and optimally align the remaining queries and segments $(n_j)_{j=1}^{N-1}, (s_t)_{t=1}^{S-1}$; or (2) skip $s_0$ and still optimally align \emph{all} queries, now with the remaining segments $(n_j)_{j=0}^{N-1}, (s_t)_{t=1}^{S-1}$ (see Fig.~\ref{figure:model-layout}b(iv)). This recursive substructure leads to a DP solution for Eq.~\ref{Z-main}. 

Let, $F^{\ast}((n_j)_j^{N-1}, (s_t)_t^{S-1})$ denote the best alignment score for queries $(n_j)_j^{N-1}$ and segments $(s_t)_t^{S-1}$ from Eq.~\ref{Z-main}. Based on the aforementioned reasoning, $F^{\ast}((n_j)_j^{N-1}, (s_t)_t^{S-1})$ can be recursively written as: 
\begin{multline}
    F^{\ast}((n_j)_j^{N-1}, (s_t)_t^{S-1}) = \text{max} \big( \log f^{\theta}(a_j, s_t) \\ + F^{\ast}((n_j)_{j+1}^{N-1}, (s_t)_{t+1}^{S-1}), F^{\ast}((n_j)_j^{N-1}, (s_t)_{t+1}^{S-1}) \big) \label{eq:dp}
\end{multline}
% Let us denote $F^{\ast}(n_{0:N-1}, s_{0:S-1})$ as the solution of Eq.~\ref{Z-main} which gives the best alignment score of queries $\{n_0, \dots, n_{N-1}\}$ and segments $\{s_0, \dots, s_{S-1}\}$. To solve for $F^{\ast}$, we consider two cases: (1) pair $(n_0, s_0)$ and optimally align the remaining queries and segments $F^{\ast}(n_{1:N-1}, s_{1:S-1})$; or (2) skip $s_0$ and optimally align \emph{all} queries, now with the remaining segments $F^{\ast}(n_{j:N-1}, s_{t+1:S-1})$. Given the optimal overlapping substructure, we can use dynamic programming (dp) to solve for $F^{\ast}$, given as:
% Here, we are computing the best alignment score for queries $\{n_j, \dots, n_{N-1}\}$ and segments $\{s_t, \dots, s_{S-1}\}$.
The base cases for the DP are: (i) $\mathrm{Z}=\mathds{I} \; \text{if} \; N=S$; (ii) $Z_{jt} = 1 \; \forall \; t \; \text{if} \; j=N-1$. It is worth noting that the DP subproblems, together with the base cases, satisfy the constraints in Eq.~\ref{Z-a}~\ref{Z-b}~\ref{Z-c}. Since the video may match any of the sequence in the super-set of query sequences (from the topological sort on $G$), we repeat this process of computing $F^{\ast}$ for each sequence and select the maximum value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\noindent \textbf{Optimizing Query Encoder Parameters $\theta$:} After obtaining the best alignment $\mathrm{Z}$ using DP, we substitute the corresponding value of $F^{\ast}((n_j)_{j=0}^{N-1}, (s_t)_{t=0}^{S-1})$ in Eq.~\ref{eq:p_theta} and subsequently Eq.~\ref{eq:bce}. In Eq.~\ref{eq:bce}, we use single mini-batch of training examples and take one gradient-update step of the Adam optimizer for the query encoder parameters $\theta$.

% Previous research has proposed a similar optimization solution that utilizes weak supervision to align a sequence of action steps with video frames~\cite{cross_task,change_it}, however, it assumes an ordered list of steps. In contrast, our approach not only generates the action steps but also incorporates partial ordering into the solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{tables/baseline_results.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[t]
% \small
% \begin{tabular}{llcccccc}
% \hline
% \multicolumn{1}{l|}{\textbf{Model}} & \textbf{Split} & \textbf{validation}  & \textbf{\begin{tabular}[c]{@{}c@{}}sub-goal \\ composition\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}verb-noun \\ composition\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}context-verb-noun \\ composition\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}context-goal \\ composition\end{tabular}} & \textbf{abstraction} \\ 
% \hline
% \multicolumn{2}{l}{ResNet-GloVe} & 96.8 / 96.9 & 60.3 / 56.2 & 84.9 / 84.7 & 92.5 / 92.6 & 83.2 / 82.3 & 63.1 / 54.4\\ 
% \hline
% \multicolumn{2}{l}{ResNet-GloVe-attention} & 92.8 / 92.9 & 58.8 / 48.7 & 85.4 / 85.2 & 92.3 / 92.5 & \textbf{90.5 / 90.7} & 63.9 / 56.6\\ 
% \hline
% \multicolumn{2}{l}{ResNet-BERT} & 96.9 / 96.9 & 58.8 / 46.4 & 82.9 / 82.3 & 91.7 / 91.6 & 82.2 / 81.3 & 61.4 / 46.5\\ 
% \hline
% \multicolumn{2}{l}{ResNet-BERT-attention} & 86.6 / 87.1 & 55.3 / 47.4 & 81.0 / 80.4 & 86.4 / 86.9 & 85.3 / 85.6 & 58.0 / 42.5\\
% \hline
% \multicolumn{2}{l}{I3D-GloVe} & 99.3 / 99.3 & 60.5 / 55.1 & 82.6 / 82.4 & 88.8 / 89.1 & 83.2 / 83.1 & 63.1 / 55.3\\ 
% \hline
% \multicolumn{2}{l}{I3D-GloVe-attention} & 92.0 / 92.1 & 55.5 / 44.6 & 78.4 / 78.0 & 89.4 / 89.6 & 85.4 / 85.6 & 61.4 / 51.5\\ 
% \hline
% \multicolumn{2}{l}{I3D-BERT} & 98.9 / 98.9 & 56.7 / 44.5 & 80.8 / 80.3 & 89.6 / 89.8 & 84.7 / 84.4 & 62.4 / 47.1\\ 
% \hline
% \multicolumn{2}{l}{I3D-BERT-attention} & 90.2 / 90.4 & 56.7 / 45.6 & 80.8 / 79.7 & 87.6 / 87.9 & 84.0 / 83.9 & 62.4 / 47.6\\
% \hline
% \multicolumn{2}{l}{S3D-BERT} & 99.8 / 99.6 & 58.2 / 45.3 & 78.6 / 76.5 & 88.4 / 88.2 & 85.5 / 85.4 & 64.6 / 55.0\\ 
% \hline
% \multicolumn{2}{l}{MViT-GloVe} & \textbf{99.9 / 99.9} & 64.8 / 61.4 & 88.2 / 88.0 & 92.7 / 92.7 & 88.1 / 87.8 & 65.6 / 57.2 \\ 
% \hline
% \multicolumn{2}{l}{MViT-GloVe-attention} & 99.6 / 99.6 & 57.7 / 43.4 & \textbf{91.0 / 91.1} & \textbf{93.8 / 93.8} & 90.3 / 90.3 & 65.4 / 59.8\\ 
% \hline
% \multicolumn{2}{l}{MViT-BERT} & \textbf{99.9 / 99.9} & 58.6 / 47.7 & 83.2 / 82.2 & 92.4 / 92.4 & 88.6 / 88.6 & 62.4 / 49.6 \\ 
% \hline
% \multicolumn{2}{l}{MViT-BERT-attention} & 97.7 / 97.7 & 58.6 / 49.4 & 87.1 / 87.0 & 92.2 / 92.2 & 88.9 / 88.8 & 62.7 / 48.5\\ 
% \hline
% \multicolumn{2}{l}{CLIP-CLIP} & 99.0 / 98.0 & 55.4 / 38.7 & 85.9 / 85.5 & 90.9 / 90.9 & 84.9 / 83.9 & 59.9 / 44.8\\ 
% \hline
% \multicolumn{2}{l}{\textbf{NSG (Ours)}} & 97.1 / 97.5 & \textbf{90.2 / 90.0} & 66.1 / 52.7 & 93.0 / 92.9 & 84.5 / 84.9 & \textbf{86.5 / 87.3}\\ 
% \hline
% \multicolumn{2}{l}{Oracle Model} & 99.6 / 99.7 & 96.4 / 95.0 & 97.4 / 96.7 & 97.0 / 97.4 & 97.4 / 97.6 & 97.0 / 97.2\\ 
% \hline
% \end{tabular}
% \caption{Comparison of models (baselines and the proposed model) on different data splits. The 2 values separated by a "/" denote accuracy and F1-score, respectively.}
% \label{table:baseline_results}
% \end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%=====================================================================
% \subsection{Model}
% \label{subsection:model}
% \red{this subsection should be referenced in the previous subsections}
% \nk{This subsection should come before Plan Verification}

% \textbf{Vision Module:} The video is split into segments with $k$\footnote{$k$ can be arbitrary, but, must uphold the constraint from \S \ref{subsection:plan_verification} -- $S \geq N$} frames per segment using a sliding window. If required, the last segment is zero-padded to $k$ frames. Each sequence of $k$-frames is encoded using CLIP model to $\mathds{R}^{k \times d}$. For CLIP, $d=512$. We use an RNN-based aggregation to encode each segment to $s_j \in \mathds{R}^{d_v}$. 

% \textbf{Text Module:} The pretrained T5 from \S \ref{subsection:plan_generation} is used to map each hypothesis to a partial-order graph $G(V,E)$. Each node $n_i \in V$ is in the form of a query that is executed on the video segments $s_j$. During training, the weights of the T5 transformer are kept frozen.

% \textbf{Query Module:} Recall, that each query node is of the form $StateQuery(arg_1, arg2)$ or $RelationQuery(arg_1, arg_2, arg3)$. To query a segment $s_k$ with node $n_j$, we use the Query Module to calculate the probability $f_{\theta}(n_j, s_k)$, which is then used to solve Eq.~\ref{Z-main}. Each argument $arg_i$ is passed encoded using the same CLIP model to $n_j \in \mathds{R}^{d_n}$. Based on the query type ($StateQuery$ or $RelationQuery$), we pass the node and segment encodings through a $StateQuery$ or a $RelationQuery$ neural network to obtain $f_{\theta}(n_j, s_k)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%