\section{Experiments} 
\label{subsection:baseline_evaluation}
We compare various state-of-the-art (SOTA) VLMs with NSG on the \etv benchmark (see Appendix~\ref{appendix:nsg_training} for NSG's experimental training details).

\subsection{SOTA VLM Baselines}
We investigate 6 VLMs developed for video-language tasks requiring similar reasoning as \etv. %All of the VLMs that we investigate directly extract features from raw video and text inputs and do not require additional input such as bounding boxes. 
 % We briefly explain these models after providing a broad characterization of VLMs.
Summarized in Table~\ref{table:baseline_results}, \textbf{CLIP4Clip}~\cite{luo2022clip4clip}, \textbf{CLIP Hitchhiker}~\cite{clip_hitchiker}, \textbf{CoCa}~\cite{coca} use image backbones followed by temporal aggregation, while \textbf{VideoCLIP}~\cite{videoclip}, \textbf{MIL-NCE}~\cite{miech2020end}, and \textbf{VIOLIN}~\cite{violin_dataset} use video backbones. With the exception of CoCa, which is trained with contrastive and captioning loss, all other models are trained using contrastive loss~\cite{miech2020end}. Lastly, VideoCLIP and VIOLIN use an explicit fusion of text-vision features. For each model, we freeze all pretrained feature extractors and finetune a fully-connected probe layer, along with the temporal aggregation layers where appropriate (CLIP4Clip-LSTM, VIOLIN), using \etv's train split. 

%VLMs have greatly enhanced the performance for many vision-language tasks like image-text retrieval~\cite{}, visual captioning~\cite{}, and VQA~\cite{} amongst others. These models are pretrained on large-scale image-text pairs, and they come in two primary designs -- 
%(i) dual encoder model for parallel encoding of text and images in the same latent space~\cite{} using contrastive loss~\cite{}; (ii) encoder-decoder models for generative modeling of text from images using language modeling loss~\cite{}. More recently, a more unified framework~\cite{} that is pretrained with both contrastive and language modeling losses has demonstrated superior performance. While VLMs are usually trained on 
% Given a video and a task description, the model needs to predict whether the task is accomplished (binary prediction).

% Majority of VLMs can be characterized based on their approach of extracting and fusing features from vision and text modalities. VLMs for video tasks use either a video encoder or an image encoder with temporal aggregation using sequence model, e.g., transformers~\cite{tf_transformer} or LSTM~\cite{lstm} to obtain the video features. %VLMs using image encoders operate at frame-level and thus require temporal aggregation to embed the video. 
% The vision and text encoders can be jointly trained using either~a)~contrastive loss %These image/video encoders can be trained jointly with text encoders using contrastive loss 
% that aligns both modalities in a shared latent space e.g., CLIP~\cite{clip}, MIL-NCE~\cite{miech2020end},~b)~masked token prediction losses on the generated text~\cite{blip} aka captioning loss, or~c)~combination of captioning and contrastive losses~\cite{coca}. The vision-text features from encoders can either be fused (multimodal fusion) using attention-based mechanisms or by computing cross-modal similarity scores. 
% % Alternatively, generative vision-text models use encoder-decoder architecture and are trained using masked token prediction losses on the generated text~\cite{blip}. Recent work has also proposed training VLMs using both of these losses~\cite{coca}. 
% %can then be fused (multimodal fusion) together to obtain a joint representation for a downstream task using attention-based mechanisms. Instead of such explicit fusion, some VLMs also use cross-modal similarity scores for implicit fusion. 
% % Some methods \cite{luo2022clip4clip,coca,violin_dataset} use a sequence model, e.g., transformers \cite{tf_transformer} or LSTM\cite{lstm}, to aggregate video features to adapt to longer videos.
% We investigate 6 VLMs that span the space of these characteristics for \etv. 

% %\begin{itemize}
%     \noindent \textbf{CLIP4Clip~\cite{luo2022clip4clip}} uses CLIP-based \cite{clip} text and image encoders. %, followed by ViViT-like~\cite{} temporal transformer for video embedding. 
%      Parameterized (e.g., LSTM-based) or non-parameterized (e.g., mean-pooling) aggregation of the resultant image features allows representing the video using a single feature vector, without any explicit fusion.
%     %image-text similarity computations are used for implicity fusing the modalities. 
%     %The image features over time are then processed using a Transformer. They further propose parameterized e.g., LSTM-based and non-parameterized e.g., mean-pooling mechanisms for fusing \rd{unclear whether the similarity calculation is per frame or between video and text..}
%     \noindent \textbf{CLIP Hitchhiker~\cite{clip_hitchiker}} uses a similar encoder structure as CLIP4Clip \cite{luo2022clip4clip} and performs weighted-mean pooling of frame embeddings using text-visual similarity scores.  %Instead of doing temporal aggregation of image representations to encode the video, it uses per-frame scoring mechanism based on image-text similarity. This is followed by a simple 
%     % A weighted mean of the per-frame embeddings based on the image-text similarity scores is used to embed the video and text into a single feature vector, without any explicit fusion. %Given that the image-text similarity is implicitly used to compute this feature vector, explicit fusion of video and text input is not needed. 
%     \noindent \textbf{CoCa~\cite{coca}} uses image-text (dual) encoder-decoder architecture trained using contrastive and captioning loss. The frame-level features are pooled via attentional pooling~\cite{attention_pooling} to model the temporal sequence of the video and then fused with the text features for downstream tasks.
%     \noindent \textbf{MIL-NCE~\cite{miech2020end}} learns to encode video and text into a single vector using separate encoders (S3D\cite{s3d}, word2vec\cite{word2vec}) learnt from scratch using the proposed multi-instance contrastive loss. 
%     %\rd{Rishi/Brian, fill in using above structure. Ensure difference from VideoCLIP is clear.}
%     \noindent \textbf{VideoCLIP~\cite{videoclip}} is built on top of MIL-NCE \cite{miech2020end}. It adds additional transformer layers for video and text encoders, trained using contrastive loss. The resultant embeddings can be fused together for downstream tasks.
%     \noindent \textbf{VIOLIN~\cite{violin_dataset}} uses pre-trained image/video (e.g., ResNet~\cite{resnet}, I3D~\cite{i3d}) and text encoders (e.g., GloVe~\cite{glove}, BERT~\cite{bert_model}) separately and fuses the resultant representations from each modality using bi-directional attention~\cite{bidaf}. For each of the above VLMs, we freeze the pretrained feature extractors for each modality and finetune a fully-connected probe layer, along with the temporal aggregation layers where appropriate (CLIP4Clip-LSTM, VIOLIN), using \etv's train split. 
%     % evaluate its performance on our generalization splits.
% %\end{itemize}

Finally, to establish upper-bounds on \etv, we also instantiate: (1) a \textbf{Text2text model}, %follows the Socratic approach~\cite{socratic} of composing LLMs with (open-vocabulary) VLMs to construct a descriptive caption of the video, with scene description and action annotation for different temporal segments. 
which constructs video captions using ground-truth labels for objects and actions, encodes the captions and task descriptions using (pretrained) RoBERTa model~\cite{roberta} and measures alignment using the the cosine similarity score (see Appendix~\ref{appendix:upper_bound_models}), and %While in our setup, we directly use the ground-truth labels for objects and actions, \red{we provide an end-to-end example in Section~\ref{} of the Appendix.}
% follows the Socratic approach~\cite{socratic} of 
% using a VLM (open-vocabulary object detection) to describe the objects in the scene and feeding that description as context to a language model for multi-step planning. Here, we directly use the ground-truth object/action/scene labels to construct the text-based description of the video input. 
(2) an \textbf{Oracle model}, which is trained with full supervision on sub-tasks labels and locations in addition to task verification labels.

% NSG model trained with supervision of sub-tasks labels and locations, in addition to task verification labels. 


% a trained NSG model but without the DP-based alignment prediction. Instead, we utilize the ground-truth alignment between the video segments and sub-tasks as additional supervision. Oracle model demonstrates the model's performance with full supervision on subtasks, whereas the other baselines learn the segment-to-sub-task relation inherently.

%CLIP\cite{clip} encoder for visual and text using the ground-truth alignment between the video segment and sub-task decomposition as additional supervision. 

%\rd{is this equivalent to CLIP finetuned for video-text pairs from our dataset?} \bc{slightly different but very close}
%\end{itemize}
% \bc{Rish please check if anything is not correct. Thanks!}


% Following standard procedure, we apply the fixed pretrained Vision-Language Models to extract video and text features. We then train a fully connected layer using the concatenated features to learn the final prediction.
% We characterize the way of modeling video and text representations in the following three variations as shown in Table \ref{table:baseline_results_new}: (1) Given that video representations from previous work are usually designed to encode short videos, e.g., 10 seconds video. To encode videos with a longer length, works such as MIL-NCE \cite{miech2020end}, CLIP4CLIP-mean \cite{luo2022clip4clip}, CLIP-hitchhiker \cite{clip_hitchiker}, and VideoCLIP \cite{videoclip} averaged or weighted averaged the video features to acquire a single vector representation. Their text feature is also represented by a single vector, and it is either done by max-pooling from word2vec features \cite{word2vec} or using the [CLS] token, which represents the sentence-level feature from a transformer \cite{clip}. (2) Instead of directly pooling the video features over time, some works, such as CoCa \cite{coca}, and CLIP4CLIP-seqLSTM \cite{luo2022clip4clip}, extracted the video features in temporal order and model video sequence using an additional network (self-attention or LSTM). (3) The last field of work, VIOLIN \cite{violin_dataset}, models both the sequence of word features and video features by two separate networks. This setting captures the subtle interaction between the sub-step and video frames. 

% Performance of these models on \etv is summarized in Table~\ref{table:baseline_results}. VLM baselines show limited generalization on \etv test splits. More descriptions of the baselines can be found in Table~\ref{table:appendix_baseline_results}. \\


% \subsection{Training Setup}
% % For both visual and text feature extractors, we use the pretrained models from CLIP \cite{clip}.
% When training NSG, we do not update the weights of the CLIP feature extractors (Sec.~\ref{subsection:nsg_bg}) due to GPU memory limitations. We use a batch of N = 64 samples, where we sample the video at 2.5 FPS. We set a window size $k=20$ frames for segmentation in NSG (Sec.~\ref{subsection:plan_verification}), each window representing an 8-second video segment. We use a train-validation split of 80-20 and use the validation performance as an indicator of convergence. We minimize the binary cross-entropy loss in Eq.~\ref{eq:bce}  with Adam \cite{kingma2014adam} and a learning rate of 1e-3. Each model is trained on 8 V100 GPUs for 50 epochs for two days. See the supplementary for more details.

%For fine-tuning on the variable length video clips in the YouCook2, CrossTask, and MSR-VTT datasets, we crop or pad the audio up to 50s in YouCook2 and
%CrossTask, and 30s for audio in MSR-VTT.

\subsection{Results}
 % Performance of these SOTA VLM models and NSG on \etv's generalization splits is summarized in Table~\ref{table:baseline_results}. 
 In Table~\ref{table:baseline_results}, we show the performance of NSG vs. SOTA VLMs per split of \etv.
% \noindent \textbf{NSG Performance:}
% NSG 
% \noindent \textbf{NSG vs. SOTA:} 
% We compare NSG and SOTA model performance on different splits. 
(1) \textbf{Novel Tasks}: NSG significantly outperforms other baselines due to its ability to decompose and detect sub-tasks while using DP alignment to handle temporal constraints among them. In contrast, other baselines rely on detecting the entire task under temporal constraints, which is more challenging. Further, image-based baselines outperform video-based baselines due to their ability to capture a greater degree of compositional detail through frame-level representations.
% % leverage the compositional structure of tasks by decomposing into sub-tasks and detecting each sub-task in the video, while leveraging optimal alignment from DP to handle temporal constraints amongst them.
% We hypothesize that since NSG is based on a fixed segment length assumption, learning to ground queries into multiple segments in the video is a promising direction to explore next. 
(2) \textbf{Novel Steps}: NSG's poor performance in this split could be attributed to its low precision in the \emph{slice} sub-task (which is dominant in this split), as shown in Figure~\ref{figure:complexity-confusion_mat} [Right]. We hypothesize that since NSG only uses the aligned segments while discarding the rest, learning to utilize context from neighboring segments to capture \emph{slice} (like picking up a knife) could be a promising future direction. (3) \textbf{Novel Scenes}: Here, NSG performs as well as the best baseline VIOLIN-ResNet. Since the tasks are identical to the train split, the success of a model is contingent on the vision encoder's ability to accurately detect the same sub-tasks in unseen scenes. Consequently, models with an additional temporal aggregation layer (VIOLIN) finetuned on \etv, tend to outperform image-based models that do not have temporal aggregation (CLIP Hitchhiker) and models with frozen video features (MIL-NCE, VideoCLIP). (4)~\textbf{Abstraction}: NSG significantly outperforms the baselines, primarily due to its semantic parser, which captures the underlying structure of the description and encodes the relevant concepts, such as objects and sub-tasks, to generate an (abstract) symbolic output.

% We hypothesize that fine-tuning  more sophisticated word-order-based reasoning can help improve their performance in these cases.

% To this end, prior works have demonstrated the use of language models to translate (abstract) language instruction to structured representations (PDDL generation)~\cite{llms_for_translating_to_goal}. Moreover, our query grammar can be utilized to validate and correct the generated outputs. The low performance of the baselines in this split could also be attributed to \red{something clever}. \rd{Not sure what you were trying to say in last two lines, Rishi. Please elaborate?}

% In general, we observe that models with multimodal fusion (CoCa, VIOLIN, NSG) show superior performance since the fusion enables joint processing of language instructions and videos leading to better alignment amongst the modalities. \rd{reasoning behind MM, i.e., joint processing is unclear to me. Arent all model doing that because of CLIP?} Moreover, despite having access to the ground-truth objects and sub-tasks in the video, the Text2text baseline model was unable to generalize. This emphasizes the importance of temporal alignment of the task instruction and subtasks in video and the challenges pertaining to \etv generalization splits. \rd{Don't understand the last line, @Rishi.}

% (1) VLM models that use pooling on image features struggle at abstraction split (e.g.,~CLIP HitchHiker, CLIP4Clip mean \& seqLSTM). This split requires a model to re-align abstract concepts with individual video segments, which are discarded during the pooling strategy. \red{RH: almost every model struggles at abstraction?}
% %We find that (1) simply applying CLIP\cite{clip} features with video pooling results in low performance in the abstraction split. This split requires the model to re-align abstract concepts with respect to each video segment, which is discarded during the pooling strategy. 
% (2) Methods that utilize sequence model (CLIP4Clip seqLSTM and CoCa) \red{RH: coca uses attention pooling} to aggregate visual features tend to perform better in Novel Tasks split, where the model needs to align unseen combinations of tasks in videos longer than the training videos.
% (3) Models with multimodal fusion (CoCa, VIOLIN, NSG) show better performance as the fusion enables joint processing of interaction between sub-tasks and video segments along time for \etv.
% (4) VIOLIN~\cite{violin_dataset} model, which is specifically designed for the video entailment \red{RH: we should edit this part} also captures the sub-task to video segment relations and excels especially in Novel Steps and Novel Scenes split. But it struggles on Novel Tasks where a new sub-task combination is presented. We hypothesize that VIOLIN overfits and cannot generalize to novel combinations.
% Moreover, we analyze the performance of NSG and find that it struggles on Novel Steps. During training, NSG discovers better alignment between the text query and video segments. However, when the query is unseen, NSG cannot benefit from the alignment model. We leave this to future work.
% Finally, we evaluate the Text2text baseline, which demonstrates the importance of using relevant video information in \etv. A model cannot simply process text information from the video e.g., captions for excelling at \etv \red{RH: could be made more concrete?}. Overall, our results highlight the importance of multimodal fusion and the challenges associated with abstraction and compositional generalization for video-based entailment through \etv. \red{RH: it's NSG vs SOTA but we only talk about sota -- to quote ``all math only"}


% We compare our model with various baselines described in Section \ref{subsection:baseline_evaluation} as shown in table \ref{table:baseline_results}. We found that simply apply CLIP\cite{clip} features with video pooling results in low performance in our task. Abstraction is particularly challenging for these model since the task requires the model to align abstract concepts resepect to each video segments while these methods discard such information during the pooling strategy. We also found models with mulitmodal fusion leads to better performance since our task requires model to understand the interaction between subtasks and video states along time jointly. The VIOLIN baseline is specifically designed for video entailment task, which is the closest baseline in our task. The results shows the model captured the subtask to video segment relation in certain tasks but failed when encountering Novel Tasks where new subtask combination is presented. We hypothesis the model overfits into certain sentence/video sequence pattern but can't generalize to novel combination.
% We found our model decreases in the performance of Novel steps. Our model discovers better alignment between the text query and video segments during training. However, when the query is unseen, we can not benefit from the alignment model. We will leave this to future work. The Text2text baseline demosntrate the importance of using video information in our task, the model cannot simply process text information from video. 

% Findings:
% 1. CLIP fails in abstraction which needs the model to align the concept with sentence sturcture.
% 2. Fusion benefits performance
% 3. Our closest baseline VIOLIN video entailment capture both the visual and text sequence by LSTM and fusion layer failed to align Novel tasks and abstraction, which requires the ability to align detailed subtask in the concecpt level
% Why Novel step failed? Our model encourage better alignment, but is difficult to align unseen step, leads to lower performance.

\subsection{Analysis of NSG}
\label{subsec:analysis}

\noindent \textbf{NSG learns to localize task-relevant entities without explicit supervision.} Figure~\ref{figure:complexity-confusion_mat} shows the confusion matrix of \code{StateQuery} \& \code{RelationQuery} outputs, which capture sub-tasks in \etv, with their ground truths. The high recall of NSG demonstrates its ability to successfully localize task-relevant entities, despite being trained using only task verification labels.

\noindent \textbf{Effect of query types on NSG.} While query types with multiple entity arguments might appear capable of modeling complex dependencies amongst entities and having more expressive power, encoding multiple entities jointly using a single encoder makes the grounding problem more challenging. Hence, in practice, we found that using a combination of \code{StateQuery} \& \code{RelationQuery} types as opposed to \code{ActionQuery} (which encodes multiple entities using a single encoder) enabled better grounding and ultimately led to better performance in terms of F1-score~(Table~\ref{table:query_comparison}).
% are domain-dependent and may be harder to define, they enable more fine-grained grounding by
% and hence better performance as compared to state queries. For instance, a relation query grounds \emph{clean(obj) in sink}, while a state query grounds \emph{clean(obj)} in the context of \etv. Also, query types that encode multiple task-relevant entities simultaneously make the grounding problem more challenging, reducing performance. For instance, \code{ActionQuery} jointly grounds state and relations using a single encoder.


% encoding multiple task-relevant entities in a single query type makes the grounding problem more challenging. 

% using single vs. multiple query encoders also effects NSG's performance. For instance, \code{ActionQuery} jointly grounds state and relations for \etv. Instead, \code{State + RelationQuery} uses separate encoders for the same. We find that \code{State + RelationQuery} enables better grounding ultimately leading to better task verification accuracy than \code{ActionQuery}.

% \code{RelationQuery} e.g., \emph{clean(obj) in sink} enables more fine-grained grounding as compared to \code{StateQuery} e.g., \emph{clean(obj)}. 

% On \etv, we find that \code{State + RelationQuery} enable better grounding ultimately leading to better task verification accuracy than \code{ActionQuery}.  
% The action query encoder jointly grounds single (e.g., cool) and multi-object events (e.g.,slice) in \etv. Instead, the state and relation query encoders separately ground single and multi-object events. 

% However, depending upon the domain, the \code{State/RelationQuery} may require expert knowledge for their definition. For instance, it is non-trivial to define relation queries in real-world tasks such as furniture assembly or appliance repair. Furthermore, \code{State/RelationQuery} requires \rd{data/train time tradeoff?}..However, when available, finer-grained queries such as \code{State/RelationQuery} can improve the performance of NSG.

\input{figures/complexity-confusion_mat}

\noindent \textbf{NSG shows consistent performance with increasing task difficulty.}
As seen in Figure~\ref{figure:complexity-confusion_mat}, NSG's performance is minimally affected by increase in task difficulty characterized by number of sub-tasks (complexity) and ordering constraints (Sec.~\ref{section:evaluation}) unlike the best-performing baseline (VIOLIN-ResNet). 

\noindent \textbf{NSG is robust to segmentation window size} The effect of $k$ on NSG is minimal~(Appendix~\ref{appendix:analysis}).


\noindent \textbf{NSG also enables task verification on real-world data.} We leverage an existing instructional video dataset CrossTask~\cite{cross_task} to create the CrossTask Verification (CTV) dataset~(see Appendix~\ref{appendix:cross_task_construct}). 
% We construct positive samples by concatenating annotated action steps, and negative samples following similar approach as \etv.
% We include baselines that were either applied to previous CrossTask evaluation or had competitive performance on \etv. 
NSG still outperforms all competitive baselines on CTV significantly with F1-score (NSG: $\mathbf{76.3}$, CoCa: 70.9, VideoCLIP: 49.7, VIOLIN 34.7), demonstrating its causal and compositional reasoning capabilities in real-world applications (see Appendix~\ref{appendix:NSG_crosstask} for details).
%We show that NSG also enables task verification on real-world data (see Appendix~\ref{appendix:NSG_crosstask}).

%\rd{make this more quantitative if we can @Brian}
% NSG model in Table~\ref{table:baseline_results} uses window size $k=20$ frames for generating segments, where $k$ is found using coarse grid search. However, we find that the effect of $k$ on NSG performance is minimal (see Appendix~\ref{appendix:analysis}). 

\begin{table}[t]
\small
\centering
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{NSG} & \begin{tabular}[c]{@{}c@{}} Novel  \end{tabular} & \begin{tabular}[c]{@{}c@{}}Novel \end{tabular} & \begin{tabular}[c]{@{}c@{}}Novel\end{tabular} & \multirow{2}{*}{Abstract.} \\ & Tasks & Steps & Scenes & \\
\hline
\code{Action} & 78.2 & 45.6 & 70.6 & 75.5\\
\code{State+Relation} & 90.0 & 64.7 & 84.9 & 80.4\\
\hline
\end{tabular}
\vspace{2pt}
\caption{\code{(State}~+~\code{Relation)Query}~vs.~\code{ActionQuery}}
\label{table:query_comparison}
% \vspace{-10pt}
\end{table}  

\noindent \textbf{Limitations of NSG.} NSG also has certain limitations:~(1) It does not consider multiple simultaneous actions like ``picking an apple while closing the refrigerator door", and~(2)~The assumption of equal-length video segments may be unsuitable for sub-tasks with a highly variable duration. We defer exploration of these limitations to future work.

% shows NSG performance with different window sizes. 

% We perform two kinds of ablations on the Query Module for \etv --  Firstly, we utilized the best localization prediction $\mathrm{Z^{\ast}}$ and gold-standard labels for all segments (for all positively labeled samples) to evaluate the performance of \code{StateQuery} and \code{RelationQuery} modules. The confusion matrix in Figure~\ref{figure:confusion-mat} demonstrates that both \code{StateQuery} and \code{RelationQuery} modules can accurately ground their arguments with high recall. However, the precision of the RelationQuery module is low, and it often predicts ('Other') segments that do not contain the relation as true. Secondly, we train our NSG model with \code{ActionQuery} instead of \code{State/RelationQuery} to investigate how disentangling state and relation grounding could be useful. 
% \red{RH:From Table~\ref{table:table_ablations} we observe an overall fall in performance}

% \noindent \textbf{Complexity-Ordering Analysis}: We compared the performances of NSG and the best baseline (VIOLIN-ResNet) to see how they vary with an increase in complexity and ordering of tasks. Figure~\ref{figure:complexity-ordering} demonstrates that the F1-score for NSG remains stable, while it steadily decreases for the baseline model.

% \noindent \textbf{Ablations on window size ($k$)}: As shown in Table~\ref{table:table_ablations}, we evaluate the NSG model with different moving window sizes ($\{12, 20, 32\}$) while ensuring the $S \geq N$ constraint.  Our findings reveal that the variant with $k=20$ performs the best in general while the performance of other settings was about the same, showing our model is robust to different window length selection. For the Novel Step split, we hypothesize it is because with $k=12$, some sub-tasks may span across multiple segments, whereas, for $k=32$, multiple sub-tasks can be grouped into the same segment (violating the ordering constraint Eq.~\ref{Z-main}), leading to incorrect localization.
    
    
    % We evaluate the performance of the neural operators (\textit{StateQuery} \& \textit{RelationQuery}) in detecting actions or relations within each segment $s_j$. We use the high-level action labels for each frame in the video segment to determine the primary action and use it as a reference for evaluation, with $\mathrm{Z^\ast}$ as the prediction. We only apply this method to positively labeled hypotheses, as the operators are not intended to align with negative hypotheses. \red{add results} 

%\end{itemize}
% \input{figures/complexity-ordering}
% \input{figures/confusion-matrix}

% \input{tables/table_ablations}

%Since the video and text representation tend to be close when they share similar semantic
% need to establish some terminology first
% token level features: (text, image), global features: (text, video) 

% ==== Start of older version baseline section ====

% \bc{maybe we can include this in intro or evaluation}
% Prior works have shown that VLMs demonstrate a severe lack of compositionality and order sensitivity~\cite{VLMbag-of-words,winoground}. For instance, the BLIP~\cite{blip} model fails to understand the difference between S1:\textit{horse is eating the grass} \& S2:\textit{grass is eating the horse} would often map both sentences to the same image. As stated in \cite{VLMbag-of-words}, it is attributed to the training setup which leads to the discarding of valuable order information. While the authors only superficially discuss the reasons, we hypothesize that this could be attributed to the pooling methods used in the text and visual encoders of the VLMs. Non-parameterized methods like \emph{mean-pooling} fail to account for the order information. 
% On the other hand, action localization and segmentation are considered token-level tasks~\cite{}.

% Based on our hypothesis, we group our baselines into the following categories (here, \emph{global}: global features $\in \mathds{R}^d$ for text/video input from pre-trained feature extractors; \emph{local}: token (text token or image-frame token) features for text/video from pre-trained feature extractors): 
% \begin{itemize}
%     \item \emph{global} / \emph{global}: 

%     CLIP4Clip~\cite{luo2022clip4clip}
    
%     CLIP Hitchhiker~\cite{clip_hitchiker} which uses a weighted-mean pooling of token features to global features given as.

    
%     \item \emph{global} / \emph{local}: 
%     \item \emph{local} / \emph{local}: 
% \end{itemize}

% The order is text/video.
% \bc{proposal free models. Why these baselines? Large-scale VL pretraining model. Generalizable to multiple downstream tasks}. 
% ==== End of older version baseline section ====

% BASELINE SELECTION


% BASELINE TRAINING DETAILS

% ADDITIONAL BASELINES IN APPENDIX  



% Broadly, all models use a visual backbone to encode the premise, a text backbone to encode the hypothesis, and a fusion network to merge the text and video representations to output an entailment label.~\rd{Brian and Rishi, can we be consistent with the language in the intro and rest of the paper when we write this section? Will make it easier to re-write.} For video encoding, we used 2D convolution networks (ResNet-18~\cite{resnet})\footnote{We use a BiLSTM to aggregate the features into a single video representation}, 3D convolution networks (I3D~\cite{i3d}, S3D~\cite{s3d}), and transformer models (CLIP~\cite{clip}, MViT~\cite{mvit}). Similarly, for text encoding, we used word embeddings (GloVe~\cite{glove}) and transformer models (DistilBERT~\cite{distil_bert}, CLIP~\cite{clip}). Fusion models were either attention-based (BiDAF~\cite{bidaf}) or simple concatenation of the two encodings. We report results for each model \red{with and without finetuning} of the pre-trained feature extractors on a train/validation split of $80/20$. Following are the observations from our evaluation:
% \begin{itemize}
%     \item In general, all models were found to perform poorly on the sub-goal composition and abstraction splits. This suggests that the models may suffer from overfitting, subsequently failing to generalize to novel compositions in both visual and language inputs. These findings emphasize the need for developing models that can effectively handle composition and abstraction in a wide range of inputs.
%     \item When encoding videos, transformer models demonstrated superior performance compared to 2D and 3D convolution networks. This suggests that pre-trained transformer models are better equipped to capture the rich visual features present in videos.
%     \item For text encoding, GloVe was found to perform slightly better than BERT and CLIP. This is likely due to BERT's and CLIP's higher dimensionality ($768 d$, $512 d$ respectively) leading to overfitting (particularly on the low vocabulary space of hypotheses), compared to the lower dimensionality of GloVe ($300 d$).
%     \item In general, attention-based models display superior performance across splits. This suggests that attention models are more effective at grounding the text to videos.
%     \item As shown in Figure~\ref{figure:complexity-ordering},  there is a decline in performance as the complexity of the task increases. Additionally, along the ordering axis, performance initially improves as the number of ordering constraints increases (from ordering = 0 to ordering = 1) but then deteriorates at ordering = 2. We attribute this to the fact that the training data contains a significant number of tasks with ordering = 1.
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%