% \input{tables/Tab 1 (abridged)}

\section{\etv Benchmark and Dataset}
\label{section:dataset}
%We posit that the future egocentric assistants would empower users to learn new tasks and to perform known tasks better via task tracking, verification, and mistake detection. To make progress towards such agents, 
We present the \emph{\textbf{Ego}centric \textbf{T}ask \textbf{V}erification} (\etv) benchmark and dataset.
%
% \subsection{Criteria for \etv Benchmark}
To enable task tracking and verification for real-world egocentric agents, \etv contains:~1)~\emph{multi-step} tasks with \emph{ordering constraints} to capture the causal and temporal nature of everyday tasks,~2)~\emph{multimodality} -- language in addition to the egocentric video to allow language-based human-agent interaction. 
% ~3)~\emph{abstracted} task descriptions, since humans may invoke the agents with partial details about the task. \rd{should we remove abstraction from here since it is a split of the dataset rather than the main benchmark task?}
\etv also provides data for a systematic study of generalization in task verification (see Table~\ref{table:list_of_datasets_abridged}). To this end, we create the \etv dataset using a photo-realistic simulator AI2-THOR~\cite{ai2thor} -- as a rich testbed for future research on generalizable agents for task tracking and verification. We also provide a reduced study of \etv on real-world data using the CrossTask dataset~\cite{cross_task}.

% \etv tasks require grounding of both objects and actions. Furthermore, both causal and compositional reasoning of spatiotemporal (video) and multimodal (video, language) information is needed. These characteristics make \etv unique amongst existing vision-language and video benchmarks and datasets (see Table~\ref{table:list_of_datasets_abridge} for a detailed comparison). 
% \aks{I think that this paragraph should be in the related work, pointing the reader to the difference between prior benchmarks and EgoTV.}

\subsection{Definitions}
\noindent \textbf{Benchmark.} The objective is to determine if a task described in natural language has been correctly executed by the agent in a given egocentric video.
% It can be viewed as a video-based entailment problem where the task description is the hypothesis and the agent's video is the premise, and the premise must entail the hypothesis.% One can view the task description as a hypothesis and the agent's video as a premise. \etv benchmark can thus be considered a video-based entailment problem, where the premise must entail the hypothesis. 
% \aks{I think at this point we have mentioned this many times, and don't need to repeat here.}

\noindent \textbf{Tasks.} 
% In real-world tasks, ordering between steps might arise because of physics of the world e.g., need to pick up a knife before slicing, or semantics of the task e.g., need to slice vegetables before frying. At the same time, certain steps may be executed in any order. Consequently, 
Each task in \etv consists of multiple \emph{partially-ordered sub-tasks} or steps. A sub-task corresponds to a single object interaction via one of the six actions:~\emph{heat, clean, slice, cool, place, pick}, and is parameterized by a~\emph{target} object of interaction\footnote{Except the \emph{place} sub-task, which is additionally parameterized by a \textit{receptacle} object, we currently limit our \etv dataset to sub-tasks involving only a single target object.}. By using the ``actionable" properties of objects in AI2-THOR~\cite{ai2thor}, we ensure that the sub-tasks are parameterized with appropriate target objects in \etv, e.g., \emph{heat(book)} will never occur.

Real-world tasks consist of sub-tasks with ordering constraints, either due to physical restrictions (e.g., picking up a knife before slicing) or task semantics (e.g., slicing vegetables before frying).
We allow \etv tasks to be partially ordered, with some steps following strict ordering, e.g.~\emph{pick} sub-task happens before \emph{place} sub-task, while others are order-independent.
%While \etv sub-tasks respect physics-based ordering , we do not attempt to capture semantic ordering constraints in \etv for practical ease of synthetically creating tasks in AI2-THOR. Consequently, a task in \etv could contain a \emph{cool} sub-task followed by a \emph{heat} sub-task, which is less likely to occur in the real world \red{RH: I made sure to not use cool and heat together}. \nk{As in, did you remove them from the dataset or you just avoided using videos with both heat and cool together?} \red{RH: that's allowed in the simulator, I just avoided generating such cases in the dataset} 
The ordering constraints between sub-tasks are captured in the task description using specifiers such as \emph{and}, \emph{then}, and \emph{before/after}. For ease of description, we will refer to a task in the paper using $\left< \text{\emph{sub-task}} \right> \_ \left< \text{\emph{ordering-specifier}} \right> (object)$ notation, irrespective of the actual task description. An example task from \etv: ~\emph{heat\_then\_clean(apple)} is shown in Fig.~\ref{figure:dataset} with its NL description: ``apple is heated, then cleaned in a sinkbasin". The task consists of two ordered sub-tasks: heat $\rightarrow$ clean on \emph{target}: apple.
% \aks{Give actual NL description, then say how that is encoded in the dataset. There is no NL description in the paper so far, so it appears like the tasks are specified in this way: heat\_then\_clean\_then\_slice(apple)}
% \vspace{-5pt}
\subsection{Dataset} As shown in Fig.~\ref{figure:dataset}, \etv dataset consists of (task description, video) pairs with positive or negative task verification labels. By combining the six sub-tasks~\emph{heat, clean, slice, cool, put, pick} with different ordering constraints, we create 82 tasks for \etv (see Appendix~\ref{appendix:dataset_analysis_and_statistics} for an exhaustive list). Tasks are instantiated with 130 target objects (excluding visual variations in shape, texture, and color) and 24 receptacle objects, totaling 1038 task-object combinations. These are performed in 30 different kitchen scenes. We also provide comprehensive annotations for each video, including frame-by-frame breakdowns for sub-tasks, object bounding boxes, and object state information (e.g.,~\textit{hot, cold, etc.}) to facilitate future research.

\subsubsection{Generation}

%%% #TODO: move the action details to appendix
% Our dataset is generated using the AI2-THOR simulator~\cite{ai2thor}, which is built on Unity 3D engine. It consists of 3D indoor scenes, where AI agents can navigate and interact with objects using 7 high-level actions and 12 low-level actions\footnote{\textbf{high-level actions}: GotoLocation, PickupObject, PutObject, SliceObject, CleanObject, HeatObject, CoolObject; \textbf{low-level actions}: LookUp, LookDown, MoveAhead, RotateRight, RotateLeft, OpenObject, CloseObject, ToggleObjectOn, ToggleObjectOff, SliceObject, PickupObject, PutObject.}. 

%It not only serves as a meaningful proxy for real-world events, providing realistic physics simulations and mirroring the diversity and complexity of real-world events but also provides a fully-controlled setting to host complex reasoning tasks and effective diagnostics. 

% \rd{revisit/explain the goal for our entailment task before diving into the dataset. Also explain what is our criteria for the tasks in the dataset e.g., multisteps, ordering important etc. Also we use the word ``task" to refer to entailment problem as well as tasks in dataset, which is confusing right now. May be we can refer to the later as ``goals"?}

% \rd{I recommend re-organizing this section.\\
% 1. Start with the goal of the entailment task. \\ 
% 2. Desiderata of various ``goals" that need to be entailed.
% 2a. ingredients of a ``goal": goal type, parameterization i.e., objects, ordering type, and actions: verb-noun.\\
% 2b. Sample definition: video + hypothesis \\
% 3. Generation process with Ai2thor using planner \\
% 4. Dataset stats and the concept of splits. This will then be a nice segway to Sec. 3.1.\\
% Also don't want to create confusion between low-level actions vs. high-level actions. So recommend describing them only as an implementation detail for dataset generation. We could perhaps refer to high-level actions as ``steps".}

% \textbf{Sample Definition:} Our objective is to determine if a task described in natural language (hypothesis) has been correctly executed in a given egocentric video of an agent performing the task (premise) -- i.e. whether the premise entails the hypothesis. As shown in Figure~\ref{figure:dataset}, we use a dataset consisting of samples of (premise, hypothesis) pairs, where each premise is annotated with both a positive and negative hypothesis. Thus, for each video, we get two samples. Moreover, each sample is associated with a \textit{task type} (here, \textit{heat\_then\_clean\_then\_slice}) and a \textit{target object} (here, \textit{apple}) which enumerates the main actions in the task (heat, clean, slice) and their order of execution (heat $\prec$ clean $\prec$ slice).

\noindent \textbf{Task-video Generation.} We generate the videos in our dataset by leveraging the ALFRED setup~\cite{ALFRED20}. ALFRED allows us to specify the \etv tasks using Planning Domain Definition Language (PDDL) and then to generate plans for achieving these tasks using the Metric-FF planner~\cite{metric_ff}. We execute these plans using the AI2-THOR simulator and obtain their corresponding videos. Further details on encoding tasks using PDDL and planning are in Appendix~\ref{appendix:etv_taskvideogen}.

% To enhance the realism of the videos, a set of plans are generated by the planner rather than just the best plan. This allows us to leverage the partial-ordered nature of tasks in our dataset to generate multiple, valid task execution trajectories for a given task. For instance, 

% introduction of redundant actions and results in stochasticity in the plans.

% We generate data by mapping multiple tasks to the same goal definition. For example tasks \textit{heat\_then\_clean(apple)} and \textit{heat\_and\_clean(apple)} have the same goal definition (a \textit{hot, clean} apple), still, the actions must be executed in a specific order for the first task (\textit{heat(apple)} $\prec$ \textit{clean} apple) and in any order for the second task. The videos are generated by enacting plans generated by a planner based on the goal definition, and by modifying the preconditions of the actions to enforce ordering -- 

\noindent \textbf{Task-description Generation.} We convert the plans generated for each task into positive and negative task descriptions using templates. Appendix~\ref{appendix:task_templates} provides details on the process and example templates. 
% \nk{I suggest moving the dataset generation subsection to the appendix to save space. Retain only stats and evaluation subsections.}

% We define a set of templates for each task. The positive (hypothesis) templates take the form of a task (or goal) definition, while the negative (hypothesis) templates are created by either altering the sequence of high-level actions in the positive template or replacing some of them with alternative actions. 

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{plots/data_stats.pdf}
    \caption{\textbf{\etv dataset.} Sub-tasks and tasks, including their difficulty measures (Sec:~\ref{section:evaluation}) are shown per split. Novel Scenes have more tasks since all the train tasks are repeated in unseen scenes. Likewise, complexity and ordering are higher in Novel Tasks due to the addition of unseen sub-tasks.}
    % \caption{\etv dataset statistics. (a) Distribution of tasks [Inner Level] and sub-tasks [Outer Level] in each split. Among test splits, Novel Scenes have more tasks since all the train tasks are repeated in unseen scenes. (b) Distribution of task complexity and ordering in each split. Complexity and ordering is higher in Novel Tasks split.}
    \label{figure:stats}
%\vspace{-10pt}
\end{figure}



% The dataset consists of 82 tasks (see Table~\ref{tab:list_of_tasks} in Appendix for an exhaustive list of tasks) that are performed in 30 different kitchen scenes, featuring a diverse range of objects (over 130 objects, not including visual variations). 

%We provide comprehensive annotations for each video, including frame-by-frame breakdowns of high-level actions, low-level actions, objects bounding boxes, and relevant state information (\textit{hot, cold, etc.}). Note, that this information is not available to the models during training. Refer \S~\ref{appendix:dataset_analysis_and_statistics} in Appendix for a detailed analysis of the train and test splits.

% This allows us to create a wide number of tasks in our dataset. 

%  While different task types may have the same goal definition, they may require the execution of high-level actions in a specific order. 

% for dataset generation: shall we just refer to the ALFRED paper?

%=====================================================================

\subsubsection{Evaluation} 
\label{section:evaluation}

\noindent \textbf{Metrics.}
We use accuracy and F1 to measure the efficacy of models on \etv task verification benchmark. To capture the difficulty of tracking and verifying tasks, we introduce two measures:~(1)~\emph{Complexity}: measuring the number of sub-tasks in a task, which impacts the video length and requires higher action and object grounding, and~(2)~\emph{Ordering}: measuring the number of ordering constraints in a task and measures the difficulty of temporal reasoning required to track and verify tasks. We evaluate model scalability by testing on tasks with varying complexity and ordering.

% We evaluate model performance for \etv tasks with varying degrees of complexity and ordering so as to tease apart the scalability of various models with increase in difficulty of these tasks.

% The ordering axis provides a measure of the number of ordering constraints in the task ($ord = \#$ ordering constraints in the task). All models are evaluated along these axes. For example, $(com, ord)$, of some of the tasks are given as (i) heat\_simple: (1, 0), (ii) heat\_and\_clean: (2, 0), (iii) heat\_then\_clean: (2, 1), (iv) heat\_then\_clean\_then\_slice: (3, 2). \rd{previous sentence is confusing. Readers would not get that you are referring to co-ordinates on a x-y graph.} A list of tasks grouped along complexity and ordering axes can be found in Table~\ref{tab:list_of_tasks} in Appendix.

\noindent \textbf{Generalization.}
Our synthetic dataset facilitates a systematic exploration of generalization in task tracking and verification via four test splits that focus on generalization to novel steps, tasks, visual contexts/scenes, and abstract task descriptions.

% An ideal model must be able to seamlessly generalize to novel compositions of actions and objects unseen during training. To this end, we introduce 5 generalization splits (\textit{test set}), as shown in Figure~\ref{figure:dataset}:

\begin{itemize}[leftmargin=*,noitemsep]
    \item \textbf{\nt}: Unseen compositions of seen sub-tasks. For e.g., if train set is \{\textit{clean(apple)},~\textit{cool(apple)}\}, then this test split would contain tasks like: \{\textit{clean\_and\_cool(apple)},~\textit{clean\_then\_cool(apple)}, \textit{ cool\_then\_clean(apple)}\}.
    
    % \textit{heat\_then\_place(egg)}\}. \rd{is this correct, Rishi? \red{RH: the last one is incorrect. although that is possible, it would be easier to detect heat\_then\_place when you have already seen them together in some other context. Thus, in our case-- given a set of sub-tasks to execute a task, at least two of them should never have appeared in the same context regardless of the ordering between them, for the task to be in Novel Tasks split} Also, what about heat then place (apple) task? \red{RH: if train doesn't have heat(apple) or place(apple), then this would be novel steps, yes} Is that considered under the novel step split instead?}

    \item \textbf{\nst}: Unseen compositions of sub-task actions and target objects. For e.g., if the train set is \{\textit{clean(apple)},~\textit{cool(egg)},~\textit{clean\_and\_cool(tomato)}\}, then this test split would contain tasks like: \{\textit{clean(egg)},~\textit{cool(apple)},~\textit{clean\_and\_cool(apple)}\}.
    
    % ,~\textit{heat\_and\_place(apple)}\}. \red{RH: feels a bit confusing -- novel steps is a novel subTask\_tarObj composition, so heat\_and\_place(apple) would be novel steps if either heat(apple) or place(apple) or both has never appeared in the context of any train task. Here it sounds like novel task\_tarObj composition instead. We could make this more crisp?}

    % \item \textbf{context-verb-noun composition} split: Compositions of verb (high-level action) and noun (target object) in novel contexts (scenes). For instance, train set contains tasks \textit{clean(knife)} in kitchen scenes 1-20, whereas the test set contains tasks \textit{clean(knife)} in kitchen scenes 21-25.

    \item \textbf{\nc}: This test split contains the same tasks as in the train set. However, the tasks are executed in unseen kitchen scenes.

    \item \textbf{Abstraction}: Abstract task descriptions, which lack the low-level details of the task in the descriptions. For instance, for a \textit{heat\_and\_clean(apple)} task, the full task description in the train set could be ``apple is heated in a microwave and cleaned in sink basin", while the abstract task description in this split could be ``apple is heated and cleaned".
\end{itemize}

Note that all the test splits and the train set are disjoint from each other. \nst split tests an \etv model's ability to understand generalizable object affordances and tool usage. For instance, once a model learns the \emph{slice} action on an apple, this split tests if the model can apply it to an orange. On the other hand, the \nt split tests the generalization of a model's causal reasoning capabilities on unseen compositions and orderings of known sub-tasks.

\subsubsection{Statistics}
\etv dataset consists of 7,673 samples (train set: 5,363 and test set: 2,310). The split-wise division is \nt: $540$, \nst: $350$, \nc: $1082$, Abstraction: $338$. The total duration of the egocentric videos in the \etv dataset is 168 hours, with an average video length of 84 seconds. The task descriptions consist of 9 words on average, with a total vocabulary size of 72. On average, there are 4.6 sub-tasks per task in the \etv dataset, and each sub-task spans approximately 14 frames. Additionally, there are 2.4 ways to verify a task. Figure~\ref{figure:stats} shows a comparison of train and test splits (more analysis in Appendix~\ref{appendix:dataset_analysis_and_statistics}).

% %====================================================================
% \subsection{Dataset Analysis and Statistics}
% \label{subsection:dataset_analysis_and_statistics}

%====================================================================
% \begin{figure*}[t]
% \centering
%     \includegraphics[width=\linewidth]{plots/model.pdf}
%     \caption{\red{need caption; dotted lines to denote frozen model parameters. [just a placeholder]}.}
%     \label{figure:model-layout}
% % \vspace{-0.3cm}
% \end{figure*}
%=====================================================================
% \input{figures/complexity-ordering}