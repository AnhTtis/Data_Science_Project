\vspace{-10pt}
%%%%%%%%% BODY TEXT

\section{Introduction}
\label{section:introduction}
%% 1. why should someone care?

%The advent of advanced interactive computer vision systems~\cite{hololens} and recent progress in vision-language and multi-modal models~\cite{} opens doors for such next generation of assistive agents. 
% We envision that the future assistive agents would build up on these visual and language reasoning capabilities of today and empower users to achieve goals in their everyday lives. In particular, such agents would be able to reason about \emph{unseen} human goals... 
% We posit that such agents would require the ability to understand user goals described in natural language at high-level i.e., without complete details about as well as unseen user goals. 

%Recent progress in augmented reality systems~\cite{hololens, magicleap}, as well as vision-language and multi-modal models~\cite{}, opens doors for the next generation of assistive agents. 
Inspired by recent progress in visual systems~\cite{MagicLeap, ungureanu2020hololens}, we consider an assistive egocentric agent capable of reasoning about daily activities. When invoked via natural language commands, for e.g., while baking a cake, the agent understands the steps involved in baking, tracks progress through the various stages of the task, detects and proactively prevents mistakes by making suggestions. Such an agent would empower users to learn new skills and accomplish tasks efficiently.
% One could envision invoking such an agent merely through natural language descriptions of tasks similar to how present day assistants such as Alexa, Siri etc.~\cite{voice_assistants} are invoked. 
%We envision such agents to empower users in daily life by  invoking them naturally through 

%% 2. Why is it challenging? 
%While recent progress in vision-language and multi-modal models~\cite{} opens doors for such next generation of assistive agents, various challenges remain in making such agents a reality. 
%To make such agents a reality, 

Developing such an egocentric agent capable of tracking and verifying everyday tasks based on their natural language specification is challenging for multiple reasons. First, such an agent must reason about various ways of doing a \emph{multi-step} task specified in natural language. This entails decomposing the task into relevant actions, state changes, object interactions as well as any necessary causal and temporal relationships between these entities. Secondly, the agent must ground these entities in egocentric observations to track progress and detect mistakes. Lastly, to truly be useful, such an agent must support tracking and verification for a combination of tasks and, ideally, even unseen tasks. These three challenges -- causal and temporal reasoning about task structure from natural language, visual grounding of sub-tasks, and compositional generalization -- form the core goals of our work.

% %% 3. What are we doing? What is our approach?
% \aks{I think this is a matter of preference, but I personally don't like related work in intro. I would make this paragraph be about EgoTV and NSG. Starting with something like - "To this end, we propose...", ie, your next paragraph.}
% \nk{+1, we should move parts of this para to lit review and delete the rest.}
% Recent research on language modeling enables decomposing tasks into multiple steps from natural language descriptions~\cite{llm_zero_shot_planning,proscript}. However, such \emph{task decompositions} cannot directly be leveraged for task tracking in egocentric agents because of lack of grounding into the visual observations or context. In parallel, the computer vision community has advanced action recognition~\cite{}, object detection and tracking~\cite{}, hand object interaction and object state change detection~\cite{ego_4d,change_it,}, step classification in procedural tasks~\cite{}, and even vision language reasoning~\cite{nsvqa,nscl,star_situated_reasoning,clevrer}, which may help with the grounding challenge. However, majority of current research on identifying actions, objects, steps, or state changes does not account for the overall task structure. Likewise, predominant research on vision language understanding~\cite{} and multi-modal grounding~\cite{} does not consider the temporal and causal constraints that emerge in task tracking and verification. We therefore focus on the order-aware visual grounding problem in our work, with an eye towards compositional generalization to scale usability of these agents. In particular, we aim to achieve visual grounding of the actions and objects corresponding to each step or sub-task obtained from the task description decomposition in an order-aware manner.

%% 4. What are our results/contributions?
As our first contribution, we propose a benchmark -- \emph{\textbf{Ego}centric \textbf{T}ask \textbf{V}erification} (\etv \inlineimg{figures/TV}) -- and a corresponding dataset in the AI2-THOR~\cite{ai2thor} simulator. % \emoji{tv}
Given a natural language (NL) task description and a corresponding egocentric video of an agent, the goal of \etv is to verify whether the task was successfully completed in the video or not.
\etv contains multi-step tasks with \emph{ordering} constraints on the steps and \emph{abstracted} NL task descriptions with omitted low-level task details inspired by the needs of real-world assistants. We also provide splits of the dataset focused on different generalization aspects, e.g., unseen visual contexts, compositions of steps, and tasks (see Figure~\ref{figure:dataset}).
% Next, we create splits of the dataset focused on different aspects of generalization, ranging from generalization to unseen visual context to unseen compositions of steps and tasks. Figure~\ref{figure:dataset} shows an example task and overview of generalization splits from \etv. Succeeding at \etv tasks requires decomposing tasks into partially-ordered steps from the NL description and order-aware visual grounding of these steps into the video. 

Our second contribution is a novel approach for order-aware visual grounding~--~\emph{\textbf{N}euro-\textbf{S}ymbolic \textbf{G}rounding} (NSG), capable of compositional reasoning and generalizing to unseen tasks owing to its ability to leverage abstract NL descriptions and compositional structure of tasks (task decomposition, ordering).~In contrast, state-of-the-art vision-language models~\cite{coca,clip,videoclip,clip_hitchiker} struggle to ground NL descriptions in egocentric videos, and do not generalize to unseen tasks.~NSG outperforms these models by~$\mathbf{33.8}\%$~on compositional generalization and~$\mathbf{32.8}\%$~on abstractly described task verification. Finally, to evaluate \nsg on real-world data, we instantiate \etv on the CrossTask~\cite{cross_task} instructional video dataset. %Specifically, we synthetically create videos with mistakes in CrossTask. 
We find that it also outperforms state-of-the-art models at task verification on CrossTask. We hope that the \etv~benchmark and dataset will enable future research on egocentric agents capable of aiding in everyday tasks.

% We experiment with many for the \etv tasks. We find that while these models generalize well to unseen visual context, they struggle to perform grounding from abstracted task descriptions and to generalize to new compositions of tasks. To deal with these challenges, we take inspiration from recent research on and develop . ~\rd{unclear why neurosymbolic models would do well on abstraction.} 

% To summarize, our main contributions are:~1)~\etv: a benchmark and synthetic dataset to systematically study egocentric task verification.
% 2)~\nsg: a novel neuro-symbolic approach to enable the core reasoning capability for \etv -- order-aware visual grounding. We demonstrate \nsg's capability on our synthetic \etv dataset as well as a real-world dataset derived from CrossTask. We will release both of these datasets and our models for future research on egocentric task tracking and verification. 


% Assistive agents require the ability to track actions and state changes from an egocentric perspective for effective assistance in day-to-day tasks. For example, an agent helping a user prepare a recipe would need to both generate the steps of the recipe (\textit{plan generation}) and track the user's actions to ensure the plan is executed correctly (\textit{plan verification}). We formulate this as a Video Entailment task~\cite{violin_dataset,9710490} \rd{should we call our task video-based goal entailment?}, wherein, given an egocentric video of an agent (or human) performing a task (\textit{premise}) and a NL task description (\textit{hypothesis}), the objective is to learn a model to track whether the given task was successfully executed in the video. 
% An ideal model should also be able to seamlessly generalize to novel compositions (of actions and objects) unseen during training. \rd{add a line about what we mean by abstraction and why is it important.} To this end, we generate a novel Vision-Language dataset on the AI2-THOR simulator~\cite{ai2thor} to study compositional and abstraction-based generalization. Our dataset provides effective evaluation measures in a controlled setting, while closely reflecting the diversity of real-world events. We implement and train a variety of end-to-end models based on existing state-of-the-art approaches. We empirically demonstrate that neural models suffer from overfitting and cannot effectively generalize to novel compositions of actions, objects, and scenes. 
% To address this problem, we propose an end-to-end Neuro-Symbolic (NeSy) framework that performs plan generation and verification. At the heart of our approach is the hypothesis that symbolic reasoning models are good at generalization and capturing compositional substructure, while neural models are good at learning representations from sensory data~\cite{10.5555/3326943.3327039,nscl,clevrer}. \rd{summarize contributions in a bulleted list.} \rd{also add a line about the main result e.g., x\% improvement as compared to end-to-end models}. 

% \rd{we also evaluate NeSy with real-world data: add briefly about CrossTask experiments.}

% % \fbox{\begin{minipage}{\linewidth}
% % \textbf{Problem Statement}

% % Given:
% % (i) Premise: Egocentric video of an agent performing a task.
% % (ii) Hypothesis: NL description of the task.

% % Learn: A model to track whether the premise entails the hypothesis. The output of the model is True if the given task is executed successfully in the video.
% % \end{minipage}}

% \textbf{Contributions:} 
% \begin{itemize}
%     \item We generate a benchmark video-language dataset to study compositional and abstraction-based generalization.
%     \item We evaluate the performance of a variety of state-of-the-art models and show that these (baseline) models cannot effectively generalize to novel compositions of actions.
%     \item We propose a novel end-to-end NeSy approach that significantly outperforms the baselines on some compositional generalization splits while performing on par with them on the rest.
%     \item We also evaluate our NeSy approach with real-world data showing similar performance improvements.
% \end{itemize}
