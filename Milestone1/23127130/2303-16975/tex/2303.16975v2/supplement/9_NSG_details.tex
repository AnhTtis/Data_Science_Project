

\twocolumn[{\begin{tcolorbox}[fontupper=\small, fontlower=\Large, label=prompting_box, title=Table~\ref{prompting_box}: Semantic Parsing by Prompting ChatGPT]
% \begin{multicols}{2}
\code{Q. slice of apple is heated in microwave, then placed in a plate}\\
\code{A. First the apple is sliced, hence Step 1: slice apple. Then the apple is heated, hence Step 2: heat apple. Then the apple is placed in a plate, hence Step 3: place apple in plate. Hence, the order of the steps are: slice apple < heat apple < place apple in plate}\\

\code{Q. potato is cleaned in sinkbasin and sliced, then cooled in a fridge}\\
\code{A. First the potato is cleaned and sliced in any order, hence Step 1: clean potato; Step 2: slice potato. Then the potato is cooled in a Fridge, hence Step 3: cool potato. Hence, the order of the steps are: clean potato and slice potato < cool potato}\\

\code{Q. hot, sliced, clean tomato}\\
\code{A. First the tomato is heated and sliced and cleaned in any order, hence Step 1: heat tomato; Step 2: slice tomato; Step 3: clean tomato. Hence, the order of the steps are: heat tomato and slice tomato and clean tomato}\\

\query{Q. apple is heated and cleaned in sinkbasin, then cooled and sliced}\\
\answer{A. First the apple is heated and cleaned in a sink basin in any order, hence Step 1: heat apple; Step 2: clean apple. Then the apple is cooled and sliced in any order, hence Step 3: cool apple; Step 4: slice apple. Hence, the order of the steps are: heat apple and clean apple < cool apple and slice apple.}\\

\query{Q. sliced apple is heated in microwave, then cleaned in a sinkbasin}\\
\answer{A. First the apple is sliced, hence Step 1: slice apple. Then the apple is heated in a microwave, hence Step 2: heat apple. Then the apple is cleaned in a sink basin, hence Step 3: clean apple. Hence, the order of the steps are: slice apple < heat apple < clean apple}\\

\query{Q. apple is heated in microwave after cooling and cleaning}\\
\answer{First the apple is cooled and cleaned in any order, hence Step 1: cool apple; Step 2: clean apple. Then the apple is heated in a microwave, hence Step 3: heat apple. Hence, the order of the steps are: cool apple and clean apple < heat apple.}
% \end{multicols}
\end{tcolorbox}}]

\section{NSG details}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The query types used by NSG for both the \etv dataset and the CTV dataset are shown in Table~\ref{table:QTypes}.



% \subsection{NSG code}
% We include our NSG code in the folder {\color{blue}\textbf{Code/nsg}} comprising the training code for \code{(State+Relation)Query} model in \textit{nesy\_v1.py} and \code{ActionQuery} model in \textit{nesy\_v2.py}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NSG Training details}
\label{appendix:nsg_training}
When training NSG, we do not update the weights of the CLIP feature extractors (Sec.~\ref{subsection:query_encoders}) due to GPU memory limitations. We use a batch of N = 64 samples, where we sample the video at 2.5 FPS. We set a window size $k=20$ frames for segmentation in NSG (Sec.~\ref{subsection:plan_verification}), each window representing an 8-second video segment. We use a train-validation split of 80-20 and use the validation performance as an indicator of convergence. We minimize the binary cross-entropy loss in Eq.~\ref{eq:bce} with Adam \cite{kingma2014adam} and a learning rate of 1e-3. Each model is trained on 8 V100 GPUs for 50 epochs for two days.
% \subsection{Graph generation for NSG}
% We follow AAAI workshop to create graph by frequency

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NSG Semantic Parsing}
\label{section:appendix_semantic_parsing}
% Given, two NL task descriptions: (A) \textit{apple is heated, then cleaned in a sink basin}; (B) \textit{hot, clean apple}, one can infer that description (A) requires the apple to be heated first and then cleaned, whereas, in description (B) the apple must be heated and cleaned, but the order of heating and cleaning is not specified.
We test two semantic parsing methods: (i) Finetuning language models to generate graphs from NL descriptions; (ii) Few-shot prompting of large language models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Finetuning Language Models}
Recently, it has been shown that pre-trained language models can be leveraged for graph~\cite{proscript} and plan generation~\cite{llms_for_translating_to_goal} from NL. We use a similar framework to train a T5-small transformer~\cite{tf_transformer} to generate partial-ordered plans. For this, we use a subset of the training data (in particular the positive task descriptions for which we have gold-label graphs annotations) and annotate them with their partial-ordered plans in the form of directed acyclic graphs (DAG) -- $G(V,E)$, where vertices $n_i \in V$ represent sub-tasks, and edges $e_{ij} \in E$ are ordering constraints that indicate $n_i$ must precede $n_j$ (i.e. $n_i \rightarrow n_j$). To train the text generation transformer, we represent the output graph in DOT language. The corresponding DOT representation for the graph in Figure~\ref{figure:model-layout} is given as: \code{Step~0:~StateQuery(apple,hot), Step~1:~StateQuery(apple,clean), Step~2:~StateQuery(apple,sliced), Step~3:~RelationQuery(apple,plate,in), Step~0~$\rightarrow$~Step~1, Step~0~$\rightarrow$~Step~2, Step~1~$\rightarrow$~Step~3, Step~2~$\rightarrow$~Step~3} \\



\textbf{Ablations on Plan Generation framework}: We assess the correctness of the graphs generated by the trained T5-transformer model on the positive task descriptions of our test splits. The evaluation metric is Graph Edit Distance (GED)~\cite{ged} which computes the distance between two graphs ($G_1$ and $G_2$) given as:
    \begin{equation*}
        GED(G_1, G_2) = \min_{G_1 \xrightarrow{d_1, \dots, d_k} G_2} \sum_{i=1}^k cost(d_i)
    \end{equation*}
where, $d_1, \dots d_k$ are graph edit operations (insertion, deletion, replacement of a vertex or an edge) from $G_1$ to $G_2$. With the exception of the abstraction split\footnote{For abstraction, we observed syntax errors, e.g., missing/incorrect arguments for queries in the generated graphs like \code{RelationQuery(apple, slice)},~instead of \code{RelationQuery(apple, \underline{knife}, slice)}. However, the selection of the correct query module and the partial grounding of the correct arguments lead to a significant improvement over baselines.}, the GED for all test splits was observed to be $\approx 0.03$ (GED $\downarrow$). Here, $\downarrow$ signifies that a lower GED score is better, with the lowest value being $=0$. Note, that although it is possible for the response to contain syntax errors, e.g., invalid names or queries with invalid arguments, the output can be improved by leveraging the DSL grammar to avoid invalid arguments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsubsection{Prompting Language Models} 
We also experimented with prompting. As a proof-of-concept, we show our example prompts in Table~\ref{prompting_box}. We use ChatGPT~\cite{chatGPT} with few-shot prompting. The prompt is displayed in gray, the queries in blue, and the generated output in green. We observed that the use of Chain-of-Thought~\cite{chain-of-thought} improved the output.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Integer Programming for Alignment in NSG}
\label{appendix:integer_programming}
The constrained optimization problem defined in Eqs.~\ref{Z-main}, without the ordering constraint, can also be formulated as an Integer Programming problem~\cite{integer_programming} where variables $Z_{jt}$ can only take integer values in $\{0,1\}$ (i.e. with the additional constraint $\mathrm{Z} \in \{0,1\}^{N \times S}$). 

It is important to note that the proposed DP solution adheres to the constraints Eqs.~\ref{Z-a}~\ref{Z-b}\ref{Z-c}. The first part of the DP (highlighted in green in Figure~\ref{figure:model-layout}) pairs the current query $s_j$ with the current segment $s_t$, then tries to align the rest of the queries with the remaining segments to meet the requirement of one query per segment, as specified in Eq.~\ref{Z-a}. The second part of the solution (denoted by the red box) skips over the current segment $s_t$ and tries to align the same queries with the remaining segments until a pairing is found (i.e. $Z_{jt}=1$). Thus, together with the base case of $\mathrm{Z}=\mathds{I} \; \text{if} \; N=S$, it satisfies Eq.~\ref{Z-b}. Furthermore, since the DP processes the queries and segments in a specific order (topological sorting for queries and temporal order of video segments), it also meets the ordering constraint requirement specified in Eq.~\ref{Z-c}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NSG Analysis}
\label{appendix:analysis}

\input{tables/table_ablations}
\input{figures/complexity-ordering}
\input{figures/sup/all_cf}

\begin{itemize}[leftmargin=*,noitemsep]
    \item Table~\ref{table:window_size} demonstrates the robustness of NSG to changes in the window size $k$. Specifically, we observe that NSG's performance is minimally affected by varying the window size parameter.
    
    \item We trained two different NSG models to compare the impact of query types: one with \code{StateQuery} and \code{RelationQuery}, and another with \code{ActionQuery}. Figure~\ref{figure:all_cf} reveals that a combination of \code{(State+Relation)Query} is effective at detecting sub-tasks in segments with high recall, particularly for sub-tasks like \emph{slice} and \emph{put}. We also note that the NSG model struggles to detect the \emph{slice} sub-task in the Novel Steps split.
    
    \item We evaluated the NSG model against the best-performing baseline~(VIOLIN-ResNet)~across two axes:~task complexity and ordering.~Figure~\ref{figure:complexity-ordering} shows that NSG's performance is robust to complexity and ordering variations.

    \item Since NSG only utilizes the features from aligned video segments and ignores the remaining segments, we conducted experiments to investigate the role of context (from the discarded segments) in \etv task verification. Specifically, we trained an extra BiLSTM layer to encode bidirectional context from adjacent segments on top of the CLIP-based segment features. As depicted in Table~\ref{table:window_size}, we observed improved performance across all splits, except for Novel Tasks. We attribute this reduction in performance to a potential loss of compositional and temporal comprehension of the segment features caused by the inclusion of additional context information.
\end{itemize}

\label{appendix:NSG_crosstask}

\input{figures/task_graph}
\input{tables/crosstask_results}


\twocolumn[{\begin{tcolorbox}[fontupper=\small, fontlower=\Large, label=text2text_examples, title=Table~\ref{text2text_examples}: Text2Text Baseline Examples]
% \begin{multicols}{2}
Text Description\\
\code{apple is cooled in a Fridge and cleaned in a SinkBasin}

Video Caption\\
\code{Segment:~1.~Location:~kitchen.~Objects:~countertop, apple.~Activity:~go to countertop}\\
\code{Segment:~2.~Location:~kitchen.~Objects:~fridge, apple.~Activity:~go to fridge}\\
\code{Segment:~3.~Location:~kitchen.~Objects:~apple, fridge.~Activity:~cool apple}\\
\code{Segment:~4.~Location:~kitchen.~Objects:~apple, fridge.~Activity:~cool apple}\\
\code{Segment:~5.~Location:~kitchen.~Objects:~sink, apple.~Activity:~go to sink}\\
\code{Segment:~6.~Location:~kitchen.~Objects:~apple, sink.~Activity:~clean apple} 

Task Verified\\
\answer{True}\\

% \hline

Text Description\\
\code{lettuce is picked, cooled in a Fridge, and sliced in a SinkBasin}

Video Caption\\
\code{Segment:~1.~Location:~kitchen.~Objects:~sink,lettuce.~Activity:~go to sink}\\
\code{Segment:~2.~Location:~kitchen.~Objects:~sink,lettuce.~Activity:~go to sink}\\
\code{Segment:~3.~Location:~kitchen.~Objects:~lettuce, sink.~Activity:~clean lettuce}\\
\code{Segment:~4.~Location:~kitchen.~Objects:~fridge, lettuce.~Activity:~go to fridge}\\
\code{Segment:~5.~Location:~kitchen.~Objects:~lettuce, fridge.~Activity:~cool lettuce} 

Task Verified\\
\code{\red{False}}
\end{tcolorbox}}]


\subsection{NSG on Real-world Data}

\paragraph{NSG for CTV.}

In Section \ref{subsection:queries}, we described two symbolic operations to process task description. In CTV, all of the action steps refer to a certain action. Hence, we apply the \code{ActionQuery} as in Table \ref{table:QTypes} to encode all action steps. In Section \ref{subsection:plan_parsing_from_instructions}, we described (1) how we processed task description into a graph and (2) how we generated all possible sequences from the graph. In our \textit{action sequence verification} setting, the ground truth action sequence was given as the task description. Hence, we directly feed the sequence in our later model Query Encoders and Video Aligner. In our \textit{task verification} setting, only the task class was given. 
Here, instead of generating a graph from the task description, we follow \cite{mao2023action} to construct a graph given each task by all possible sequence order as shown in Figure \ref{figure:cross_task_verification_graph}. Then, we feed the graph to the rest of our model. 




\paragraph{Performance Evaluation.}
To evaluate how NSG enables task verification in the real world, we compare the performance of NSG against selected baselines described in Sec.~\ref{subsection:baseline_evaluation}, which were either applied to the previous CrossTask evaluation (MIL-NCE\cite{miech2020end}, VideoCLIP\cite{videoclip}) or had competitive performance (CoCa\cite{coca}, VIOLIN\cite{violin_dataset}) on \etv. Note that the methods for CrossTask are not directly applicable to CTV since CTV focuses on task verification (predicting if the task is accomplished) instead of temporal localization(localizing action temporally). Table~\ref{table:crosstask_results} shows that the baseline models VideoCLIP, CoCa perform better than VIOLIN on CTV as compared to \etv. This indicates that CTV is potentially able to better harness the gains from the large-scale VL pretraining in these models compared to \etv. Despite these gains, NSG outperforms the baselines by a significant margin in both action sequence and task verification settings. We also found the \textit{Task verification} setting is more challenging than \textit{Action sequence verification} setting in general since the former offers much less information for the model to predict. Also, the same task may have different ways to accomplish it. The model needs to learn such diverse steps inherently. The results demonstrate the applicability of NSG's causal and compositional reasoning capabilities in the real world.