
\input{tables/task_topics}

\section{CrossTask Verification (CTV) Dataset}
\label{appendix:cross_task_construct}
\subsection{Problem setting}
\label{appendix:cross_task_construct_problem}
% Two settings in CTV are:~(1)~\textbf{Action sequence verification}: The sequence of annotated action steps for each
% video is concatenated to generate task description for
% a positive sample, while descriptions for negative samples are generated using a similar process as EgoTV
% (Sec.~\ref{section:dataset}). We also shuffle the video frames order corresponding to each action step to create challenging negative samples (Fig.~\ref{figure:cross_task_verification}).~(2)~\textbf{Task verification}: We directly use the task class of the video as the task description for positive samples. The task classes that have similar action steps, but different compositions are used to generate descriptions for negative samples. 

%, we leverage an existing instructional video dataset called CrossTask~\cite{cross_task} to create CrossTask Verification dataset (CTV) (Figure~\ref{figure:cross_task_verification}). CrosstTask contains 18 tasks with around 150 videos per task. For each task, such as ``Fixing tire", corresponding action steps, such as ``jack up" and ``change tire," required to complete the task and their temporal annotations are provided. 
% The action steps for different tasks may overlap, for example, ``add sugar" is a shared step between ``Make lemonade" and ``Make coffee". 
% In this section, we describe the experimental setup for evaluating our proposed model on our newly proposed CrossTask Verification dataset (CTV). The CrossTask Verification dataset is constructed from the CrossTask dataset \cite{cross_task} which contains 18 task classes with around 150 videos per class. For each task, such as ``Fixing tire", there are corresponding action steps, such as ``jack up" and ``change tire," required to complete the task. The action steps for different tasks may overlap, for example, ``add sugar" is a shared step between ``Make lemonade" and ``Make coffee". The temporal annotation of each action step is also available.

% The sequence of annotated action steps for each
% video is concatenated to generate task description for
% a positive sample, while descriptions for negative samples are generated using a similar process as EgoTV
% (Sec.~\ref{section:dataset}). We also shuffle the video frames order corresponding to each action step to create challenging negative samples (Fig.~\ref{figure:cross_task_verification}).~(2)~\textbf{Task verification}: We directly use the task class of the video as the task description for positive samples. The task classes that have similar action steps, but different compositions are used to generate descriptions for negative samples. 
% Since the real-world data CrossTask is not controllable as our EgoTV dataset, we only test on the Novel Task split described in Section \ref{section:evaluation}, where we withhold certain videos with unseen action compositions for testing. 

Taking inspiration from our proposed \etv dataset, we construct a new dataset CrossTask Verification from CrossTask dataset~\cite{cross_task} to test the model's performance on real-world videos. 
% \bc{talk about dataset creation in more detail}
CrossTask \cite{cross_task} has 18 task classes with around 150 videos per class. Given each task, e.g., \textit{fixing tire}, there are corresponding action steps, \textit{jack up}, \textit{change tire} to accomplish such task. Also, the action steps across different tasks are possibly shared, e.g., \textit{add sugar} in \textit{Make lemonade} or \textit{Make coffee}. The temporal annotation of each action step is available. 
In CrossTask Verification evaluation, given a task description and the video, the model needs to predict if the task is accomplished (entailed) in the video. %We followed the same process 
%We constructed two settings for evaluation. 
% 
\input{tables/query_definition}
Akin to \etv, CTV consists of paired task descriptions and videos for task verification. We construct two settings:~

\noindent 1.~\textbf{Action sequence verification}~NL descriptions are obtained by leveraging action step annotations in CrossTask.

\noindent 2.~\textbf{Task~verification}~task~class-labels~are~used~as~descriptions. 

In the first setting, the model needs to understand if the action steps were executed successfully in order. Since the action steps are usually short-term actions with only a few seconds, the model needs to localize the correct action step temporal location with finer granularity. In the second setting, the task class was given instead of the action step level. The model needs to understand the task as a whole (consisting of several action steps to accomplish), and a more global understanding is required.




\input{figures/cross_task_verification}

\subsection{Dataset construction}
We leverage the CrossTask dataset and its action step annotation to construct our CrossTask Verification dataset with the following two splits: 

\noindent (1) \textbf{Action sequence verification} is done by concatenating a sequence of \textit{action steps} annotated in CrossTask for each video as our task description.
\begin{itemize}[leftmargin=*,noitemsep]
    \item To ensure repetitive subtasks, as in \etv, we start by selecting the action steps within each task based on frequency. In the end, we select the 4 most frequent action steps with respect to each task. We then filter out video segments in CrossTask that don't contain the top 4 frequent action steps.
    \item We also followed a similar process of EgoTV for generating negative descriptions. We include three ways of \textbf{negative description} including: (i) \textit{replacing action steps from other tasks}: where we substitute one of the action steps in the sequence to the action step in another task. (ii) \textit{replacing action steps from the same task}: instead of replacing step from another task, we reuse the unused action steps which were not the top 4 frequent steps in the same task. These action steps are closer in semantics and serve as hard negative. (iii) \textit{replace action step sequence order to impossible sequence}: we first list out all possible action step orders in CrossTask following \cite{mao2023action}. Then, we found an action sequence that doesn't exist in CrossTask, e.g., \textit{lower jack, upper jack, break on}. We called these sequence orders impossible since we can't lower the jack when the car is already on the ground. 
    To make the task more challenging, we also generate \textbf{negative videos} by (i) \textit{shuffling the video order corresponding to each action step}: where the action steps weren't executed in the correct order. (ii) \textit{dropping a video segment corresponding to its action step}: where one action step is missing in the video. Therefore, the task wasn't executed successfully.
\end{itemize}
\noindent (2) \textbf{Task verification} is a more challenging setting where we directly use the \textbf{task class} of the video as the task description. Instead of generating negative samples as in \textit{action sequence verification}, the negative samples are from different task classes. We select a negative task class with shared action steps to ensure the model understands the procedure of a certain task instead of simply object-level recognition. We divide the task classes into 4 major topics as shown in Table \ref{table:task_topics}, where each topic contains shared action steps as described in Section \ref{appendix:cross_task_construct_problem}. The video follows the same process as \textit{Action sequence verification} where we select videos with the top 4 action steps.
%We adopt the same videos processed in the \textit{action sequence verification} step. The key difference is we do not generate negative samples as in \textit{action sequence verification}. 
% %\bc{Talk about shared action steps for similar task, its a hard task }


% \paragraph{Task Class}
% directly use the \textbf{task class} of the video as the task description. 
% The negative descriptions are sampled from the other task classes with similar action steps, e.g., both have \textit{add sugar} step to task the model's understanding of the procedure of certain task instead of simply object level recognition. 



% Since the real-world data is not controllable as our \etv dataset, we only test on the Novel Task split described in Section \ref{section:evaluation}, where we withhold certain videos with unseen action compositions for testing.






\subsection{Evaluation}
\noindent \textbf{Metrics.}
We follow \etv to use accuracy and F1 to measure the efficacy of models on CrossTask Verification benchmark. We also evaluate model scalability by testing on tasks with varying complexity and ordering.

\noindent \textbf{Generalization.}
In \textit{Action sequence verification}, we construct a test set using videos with unseen action compositions. While this is equivalent to Novel Task split (Sec.~\ref{section:evaluation}) from \etv in spirit, CTV test set also contains some amount of novel visual context and steps due to limited control during generation.
To ensure that our split has novel sub-tasks, we train with videos with maximum 3 sub-tasks/steps and test with videos containing 4 sub-tasks as shown in Figure \ref{figure:cross_task_verification}. Note that all action steps which exist in testing also exist in training to ensure all steps were observed. 







% \subsection{Training and testing}
% \subsubsection{Action Label}

% \subsubsection{Task Class}

% \subsection{Experiment results}
% Our proposed CrossTask Verification dataset and evaluation demonstrate the usefulness of our model for real-world video understanding tasks. The dataset construction and evaluation methods can be used for future research in video understanding tasks.
% We compare the selected baselines described in Section \ref{subsection:baseline_evaluation} which were applied to the previous CrossTask evaluation or with competitive performance. Note that the methods for CrossTask are not directly applicable to this task since the objective is different, temporal localization v.s. task verification.
% As shown in Table \ref{table:crosstask_results}, our method outperforms the baselines by a significant margin, demonstrating our model's generalizability to the real-world dataset. Also, the model captures the description order and even unseen combination of action.

% Our proposed CrossTask Verification dataset and evaluation demonstrate the usefulness of our model for real-world video understanding tasks. The dataset construction and evaluation methods can be used for future research in video understanding tasks. Detailed experiment results of the CrossTask Verification dataset are included in the supplementary material.

% Taking inspiration from our proposed \etv dataset, we construct a new dataset CrossTask Verification from CrossTask dataset\cite{cross_task} to test the model's performances on real-world videos. 
% \bc{talk about dataset creation in more detail}
% CrossTask \cite{cross_task}---consisting of 18 task classes with around 150 videos per class. Given each task, e.g., \textit{fixing tire}, there are corresponding action steps, \textit{jack up}, \textit{change tire} to accomplish such task. Also, the action steps across different tasks are possibly shared, e.g., \textit{add sugar} in \textit{Make lemonade} or \textit{Make coffee}. The temporal annotation of each action step is available. 
% In CrossTask Verification evaluation, given a task description and the video, the model needs to predict if the task is accomplished (entailed with video). %We followed the same process 
% We constructed two settings for evaluation. (1) \textbf{Action sequence verification} is done by  concatenating a sequence of \textbf{action steps} annotated in CrossTask for each video as our task description. We also followed a similar process of EgoTV for generating negative descriptions. To make the task more challenging, we also shuffled the video order correspond to each action step to create negative samples.
% (2) \textbf{Task verification} is a more challenging setting where we directly use the \textbf{task class} of the video as the task description. The negative descriptions are sampled from the other task classes with similar action steps, e.g., both have \textit{add sugar} step to task the model's understanding of the procedure of certain task instead of simply object level recognition. 
% %\bc{Talk about shared action steps for similar task, its a hard task }
% Since the real-world data is not controllable as our \etv dataset, we only test on the Novel Task split described in Section \ref{section:evaluation}, where we withhold certain videos with unseen action compositions for testing.
% We compare the selected baselines described in Section \ref{subsection:baseline_evaluation} which were applied to the previous CrossTask evaluation or with competitive performance. Note that the methods for CrossTask are not directly applicable to this task since the objective is different, temporal localization v.s. task verification.
% As shown in Table \ref{table:crosstask_results}, our method outperforms the baselines by a significant margin, demonstrating our model's generalizability to the real-world dataset. Also, the model captures the description order and even unseen combination of action.
% More details and experiment results of the CrossTask Entailment dataset are included in the supplementary. 
%\bc{updated by 2/27, Todo: add how do we use graph} \\
%\bc{draw figure for crosstask}
%\bc{we can include classfication model if we have time}

%\red{RH: To Add? How is our method different from the CrossTask method and why we do not compare with it? (1. temporal localization assuming only positive hypotheses in Cross Task, as opposed to positive and negative hypotheses, hence weakly supervised vs. supervised)}

