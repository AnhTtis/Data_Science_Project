
\section{Baseline descriptions and details}
\label{appendix:baselines_details}

% \input{tables/sup_baseline}

\subsection{VLM Baselines}
\label{appendix:VLM_baselines}
Majority of VLMs can be characterized based on their approach of extracting and fusing features from vision and text modalities. VLMs for video tasks use either a video encoder or an image encoder with temporal aggregation using sequence model, e.g., transformers~\cite{tf_transformer} or LSTM~\cite{lstm} to obtain the video features. %VLMs using image encoders operate at frame-level and thus require temporal aggregation to embed the video. 
The vision and text encoders can be jointly trained using either~a)~contrastive loss %These image/video encoders can be trained jointly with text encoders using contrastive loss 
that aligns both modalities in a shared latent space e.g., CLIP~\cite{clip}, MIL-NCE~\cite{miech2020end},~b)~masked token prediction losses on the generated text~\cite{blip} aka captioning loss, or~c)~combination of captioning and contrastive losses~\cite{coca}. The vision-text features from encoders can either be fused (multimodal fusion) using attention-based mechanisms or by computing cross-modal similarity scores. 
% Alternatively, generative vision-text models use encoder-decoder architecture and are trained using masked token prediction losses on the generated text~\cite{blip}. Recent work has also proposed training VLMs using both of these losses~\cite{coca}. 
%can then be fused (multimodal fusion) together to obtain a joint representation for a downstream task using attention-based mechanisms. Instead of such explicit fusion, some VLMs also use cross-modal similarity scores for implicit fusion. 
% Some methods \cite{luo2022clip4clip,coca,violin_dataset} use a sequence model, e.g., transformers \cite{tf_transformer} or LSTM\cite{lstm}, to aggregate video features to adapt to longer videos.
We investigate 6 VLMs that span the space of these characteristics for \etv. 

%\begin{itemize}
    \noindent \textbf{CLIP4Clip~\cite{luo2022clip4clip}} uses CLIP-based \cite{clip} text and image encoders. %, followed by ViViT-like~\cite{} temporal transformer for video embedding. 
     Parameterized (e.g., LSTM-based) or non-parameterized (e.g., mean-pooling) aggregation of the resultant image features allows video representation using a single feature vector, without any explicit fusion.
    %image-text similarity computations are used for implicity fusing the modalities. 
    %The image features over time are then processed using a Transformer. They further propose parameterized e.g., LSTM-based and non-parameterized e.g., mean-pooling mechanisms for fusing \rd{unclear whether the similarity calculation is per frame or between video and text..}
    \noindent \textbf{CLIP Hitchhiker~\cite{clip_hitchiker}} uses a similar encoder structure as CLIP4Clip \cite{luo2022clip4clip} and performs weighted-mean pooling of frame embeddings using text-visual similarity scores.  %Instead of doing temporal aggregation of image representations to encode the video, it uses per-frame scoring mechanism based on image-text similarity. This is followed by a simple 
    % A weighted mean of the per-frame embeddings based on the image-text similarity scores is used to embed the video and text into a single feature vector, without any explicit fusion. %Given that the image-text similarity is implicitly used to compute this feature vector, explicit fusion of video and text input is not needed. 
    \noindent \textbf{CoCa~\cite{coca}} uses image-text (dual) encoder-decoder architecture trained using contrastive and captioning loss. The frame-level features are pooled via attentional pooling~\cite{attention_pooling} to model the temporal sequence of the video and then fused with the text features for downstream tasks.
    \noindent \textbf{MIL-NCE~\cite{miech2020end}} learns to encode video and text into a single vector using separate encoders (S3D\cite{s3d}, word2vec\cite{word2vec}) learned from scratch using the proposed multi-instance contrastive loss. 
    \noindent \textbf{VideoCLIP~\cite{videoclip}}~is built on top of MIL-NCE \cite{miech2020end}.~It adds additional transformer layers for video and text encoders, trained using contrastive loss. The resultant embeddings can be fused together for downstream tasks.
    \noindent \textbf{VIOLIN~\cite{violin_dataset}} uses pre-trained image/video (e.g., ResNet~\cite{resnet}, I3D~\cite{i3d}) and text encoders (e.g., GloVe~\cite{glove}, BERT~\cite{bert_model}) separately and fuses the resultant representations from each modality using bi-directional attention~\cite{bidaf}.~For each of the above VLMs, we freeze the pretrained feature extractors for each modality and finetune a fully-connected probe layer, along with the temporal aggregation layers where appropriate (CLIP4Clip-LSTM,~VIOLIN), using \etv's train split. 

    % As shown in Table~\ref{table:appendix_baseline_results}, we also experiment with different combinations of visual (ResNet~\cite{resnet}, I3D~\cite{i3d}, S3D~\cite{s3d}, MViT~\cite{mvit}, CLIP~\cite{clip}) and text encoders (GloVe~\cite{glove}, BERT~\cite{bert_model}, CLIP~\cite{clip}) for the VIOLIN baseline with and without bidirectional attentional flow~\cite{bidaf}.
    % evaluate its performance on our generalization splits.
%\end{itemize}



\subsection{Text2Text Baseline Model}
\label{appendix:upper_bound_models}

In this baseline, we generate video captions from ground-truth objects and sub-task labels and calculate a text similarity score (cosine similarity) between the video caption and the task description using a pretrained RoBERTa model~\cite{roberta} by using a manually set threshold. We start by splitting the video into segments and generate captions for each segment comprising the segment index (for temporal grounding), scene location (kitchen), (\emph{top-k}) objects in the scene, and the activity (sub-task). Example tasks are shown in Table~\ref{text2text_examples}. This baseline is analogous to Socratic Models~\cite{socratic} which generalizes to new applications in zero-shot by leveraging the multimodal capabilities from several pretrained models. For instance, the objects from each segment can be captured using open-vocabulary VLMs~\cite{glip,regionclip,coca}, while the activities for each segment can be detected using \cite{miech2020end,videoclip} by zero-shot classification with text-to-video feature similarity. Similarly, a final summarized video caption for the whole video can be generated using an LLM. %\red{RH: missing references}

We note that despite having ground-truth textual representations of objects and sub-tasks on a scene-by-scene basis, the Text2text baseline model fails to generalize. We attribute this to two reasons:~(1) The pretrained RoBERTa model has limited capacity to capture (out-of-domain) word-level sub-task orderings to determine entailment in \etv, and~(2) Text2Text lacks visual inputs and might suffer from lack of inferring relationships between objects like those captured by \code{RelationQuery} types. 

\input{tables/sup_list_of_tasks}
\input{figures/sup/extra-analysis}
