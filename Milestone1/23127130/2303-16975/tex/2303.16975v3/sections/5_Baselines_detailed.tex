\section{Comparison with the State-of-the-art} 
\label{subsection:baseline_evaluation}
We evaluate various state-of-the-art (SOTA) VLMs and compare them with NSG on \etv benchmark. Experimental details on training NSG are provided in Appendix~\ref{appendix:nsg_training}.

\subsection{SOTA VLM Baselines}
We consider VLMs developed for downstream video tasks such as video-based entailment~\cite{violin_dataset} and text-to-video retrieval~\cite{clip_hitchiker, videoclip, coca, luo2022clip4clip}, as they require similar reasoning skills as \etv. \red{RH: shall we say vision-based tasks instead? then we can add VQA too. }%All of the VLMs that we investigate directly extract features from raw video and text inputs and do not require additional input, such as bounding boxes. 
We finetune a fully-connected probe layer for each frozen, pretrained VLM on \etv's train split and evaluate its performance on our generalization splits. Next, we provide a broad characterization of VLMs and explain the VLMs we consider for \etv.


%VLMs have greatly enhanced the performance for many vision-language tasks like image-text retrieval~\cite{}, visual captioning~\cite{}, and VQA~\cite{} amongst others. These models are pretrained on large-scale image-text pairs, and they come in two primary designs -- 
%(i) dual encoder model for parallel encoding of text and images in the same latent space~\cite{} using contrastive loss~\cite{}; (ii) encoder-decoder models for generative modeling of text from images using language modeling loss~\cite{}. More recently, a more unified framework~\cite{} that is pretrained with both contrastive and language modeling losses has demonstrated superior performance. While VLMs are usually trained on 
% Given a video and a task description, the model needs to predict whether the task is accomplished (binary prediction).

The majority of VLMs can be characterized based on their approach by how they extract and fuse features from vision and text modalities. VLMs for video tasks use either a video encoder or an image encoder with temporal aggregation to obtain the video features. %VLMs using image encoders operate at frame level and thus require temporal aggregation to embed the video. 
The vision and text encoders can be jointly trained using contrastive loss %These image/video encoders can be trained jointly with text encoders using contrastive loss 
that aligns both modalities in a shared latent space, e.g., CLIP~\cite{clip}, MIL-NCE~\cite{miech2020end}. Alternatively, generative vision-text models use encoder-decoder architecture and are trained using masked token prediction losses on the generated text~\cite{blip}. Recent work has also proposed training VLMs using both of these losses~\cite{coca}. The vision-text features from encoders can either be fused (multimodal fusion) using attention-based mechanisms or by computing cross-modal similarity scores. %can then be fused (multimodal fusion) together to obtain a joint representation for a downstream task using attention-based mechanisms. Instead of such explicit fusion, some VLMs also use cross-modal similarity scores for implicit fusion. 
Some methods \cite{luo2022clip4clip,coca,violin_dataset} use a sequence model, e.g., transformers \cite{tf_transformer} or LSTM\cite{lstm}, to aggregate video features to adapt to longer videos.
We investigate 6 VLMs that span the space of these characteristics for \etv.

%\begin{itemize}
    \noindent \textbf{CLIP4Clip~\cite{luo2022clip4clip}} uses CLIP-based \cite{clip} text and image encoders. %, followed by ViViT-like~\cite{} temporal transformer for video embedding. 
     Parameterized (e.g., LSTM-based) or non-parameterized (e.g., mean-pooling) aggregation of the resultant image features allows representing the video using a single feature vector without any explicit fusion.
    %image-text similarity computations are used for implicity fusing the modalities. 
    %The image features over time are then processed using a Transformer. They further propose parameterized, e.g., LSTM-based and non-parameterized, e.g., mean-pooling mechanisms for fusing \rd{unclear whether the similarity calculation is per frame or between video and text..}
    \noindent \textbf{CLIP Hitchhiker~\cite{clip_hitchiker}} uses a similar encoder structure as CLIP4Clip \cite{luo2022clip4clip} and performs weighted-mean pooling of frame embeddings using text-visual similarity scores.  %Instead of doing temporal aggregation of image representations to encode the video, it uses per-frame scoring mechanism based on image-text similarity. This is followed by a simple 
    % A weighted mean of the per-frame embeddings based on the image-text similarity scores is used to embed the video and text into a single feature vector without any explicit fusion. %Given that the image-text similarity is implicitly used to compute this feature vector, explicit fusion of video and text input is not needed. 
    \noindent \textbf{CoCa~\cite{coca}} uses image-text (dual) encoder-decoder architecture trained using contrastive and captioning loss. The frame-level features are pooled via attentional pooling~\cite{attention_pooling} to model the temporal sequence of the video and then fused with the text features for downstream tasks.
    \noindent \textbf{MIL-NCE~\cite{miech2020end}} learns to encode video and text into a single vector using separate encoders (S3D\cite{s3d}, word2vec\cite{word2vec}) learnt from scratch using the proposed multi-instance contrastive loss. 
    %\rd{Rishi/Brian, fill in using above structure. Ensure difference from VideoCLIP is clear.}
    \noindent \textbf{VideoCLIP~\cite{videoclip}} is built on top of MIL-NCE \cite{miech2020end}. It adds additional transformer layers for video and text encoders, trained using contrastive loss. The resultant embeddings can be fused together for downstream tasks.
    \noindent \textbf{VIOLIN~\cite{violin_dataset}} uses pre-trained image/video (e.g., ResNet~\cite{resnet}, I3D~\cite{i3d}) and text encoders (e.g., GloVe~\cite{glove}, BERT~\cite{bert_model}) separately and fuses the resultant representations from each modality using bi-directional attention~\cite{bidaf}. 
%\end{itemize}

Finally, to establish an upper-bound on \etv, we also instantiate an Oracle model and Text2text alignment baseline. 
%\begin{itemize}
\noindent \textbf{Text2text model} %follows the Socratic approach~\cite{socratic} of composing LLMs with (open-vocabulary) VLMs to construct a descriptive caption of the video, with scene description and action annotation for different temporal segments. 
The video caption is constructed using ground-truth labels for objects and actions, and the task description is encoded using the RoBERTa model~\cite{roberta}. Then cosine similarity score is computed to measure alignment (see Sec.~\ref{appendix:exp} in the Appendix). %While in our setup, we directly use the ground-truth labels for objects and actions, \red{we provide an end-to-end example in Section~\ref{} of the Appendix.}
% follows the Socratic approach~\cite{socratic} of 
% using a VLM (open-vocabulary object detection) to describe the objects in the scene and feeding that description as context to a language model for multi-step planning. Here, we directly use the ground-truth object/action/scene labels to construct the text-based description of the video input. 
\noindent \textbf{Oracle model} trained with the CLIP\cite{clip} encoder for visual and text using the ground-truth alignment between the video segment and sub-task decomposition as additional supervision. \rd{is this equivalent to CLIP finetuned for video-text pairs from our dataset?} \bc{slightly different but very close}
%\end{itemize}
% \bc{Rish please check if anything is not correct. Thanks!}


% Following standard procedure, we apply the fixed pretrained Vision-Language Models to extract video and text features. We then train a fully connected layer using the concatenated features to learn the final prediction.
% We characterize the way of modeling video and text representations in the following three variations as shown in Table \ref{table:baseline_results_new}: (1) Given that video representations from previous work are usually designed to encode short videos, e.g., 10 seconds video. To encode videos with a longer length, works such as MIL-NCE \cite{miech2020end}, CLIP4CLIP-mean \cite{luo2022clip4clip}, CLIP-hitchhiker \cite{clip_hitchiker}, and VideoCLIP \cite{videoclip} averaged or weighted averaged the video features to acquire a single vector representation. Their text feature is also represented by a single vector, and it is either done by max-pooling from word2vec features \cite{word2vec} or using the [CLS] token, which represents the sentence-level feature from a transformer \cite{clip}. (2) Instead of directly pooling the video features over time, some works, such as CoCa \cite{coca}, and CLIP4CLIP-seqLSTM \cite{luo2022clip4clip}, extracted the video features in temporal order and model video sequence using an additional network (self-attention or LSTM). (3) The last field of work, VIOLIN \cite{violin_dataset}, models both the sequence of word features and video features by two separate networks. This setting captures the subtle interaction between the sub-step and video frames. 

% Performance of these models on \etv is summarized in Table~\ref{table:baseline_results}. VLM baselines show limited generalization on \etv test splits. More descriptions of the baselines can be found in Table~\ref{table:appendix_baseline_results}. \\


% \subsection{Training Setup}
% % For both visual and text feature extractors, we use the pretrained models from CLIP \cite{clip}.
% When training NSG, we do not update the weights of the CLIP feature extractors (Sec.~\ref{subsection:nsg_bg}) due to GPU memory limitations. We use a batch of N = 64 samples, where we sample the video at 2.5 FPS. We set a window size $k=20$ frames for segmentation in NSG (Sec.~\ref{subsection:plan_verification}), each window representing an 8-second video segment. We use a train-validation split of 80-20 and use the validation performance as an indicator of convergence. We minimize the binary cross-entropy loss in Eq.~\ref{eq:bce}  with Adam \cite{kingma2014adam} and a learning rate of 1e-3. Each model is trained on 8 V100 GPUs for 50 epochs for two days. See the supplementary for more details.

%For fine-tuning on the variable length video clips in the YouCook2, CrossTask, and MSR-VTT datasets, we crop or pad the audio up to 50s in YouCook2 and
%CrossTask, and 30s for audio in MSR-VTT.

\subsection{Results}
 Performance of these VLM models and NSG on \etv is summarized in Table~\ref{table:baseline_results}.

\noindent \textbf{NSG Performance:}
NSG 

\noindent \textbf{NSG vs. SOTA:} 
(1) VLM models that use pooling on image features struggle at abstraction split (e.g.,~CLIP HitchHiker, CLIP4Clip mean \& seqLSTM). This split requires a model to re-align abstract concepts with individual video segments, which are discarded during the pooling strategy \red{RH: ???}.
%We find that (1) simply applying CLIP\cite{clip} features with video pooling results in low performance in the abstraction split. This split requires the model to re-align abstract concepts with respect to each video segment, which is discarded during the pooling strategy. 
(2) Methods that utilize sequence model (CLIP4Clip seqLSTM and CoCa) to aggregate visual features tend to perform better in Novel Tasks split, where the model needs to align unseen combinations of tasks in videos longer than the training videos.
(3) Models with multimodal fusion (CoCa, VIOLIN, NSG) show better performance as the fusion enables joint processing of interaction between sub-tasks and video segments along time for \etv.
(4) VIOLIN~\cite{violin_dataset} model, which is specifically designed for the video entailment also captures the sub-task to video segment relations and excels especially in Novel Steps and Novel Scenes split. But it struggles on Novel Tasks where a new sub-task combination is presented. We hypothesize that VIOLIN overfits and cannot generalize to novel combinations.
Moreover, we analyze the performance of NSG and find that it struggles on Novel Steps. During training, NSG discovers better alignment between the text query and video segments. However, when the query is unseen, NSG cannot benefit from the alignment model. We leave this to future work.
Finally, we evaluate the Text2text baseline, which demonstrates the importance of using relevant video information in \etv. A model cannot simply process text information from the video e.g., captions for excelling at \etv. Overall, our results highlight the importance of multimodal fusion and the challenges associated with abstraction and compositional generalization for video-based entailment through \etv.

% Key Takeaways:
% (1) Much like a complex expression whose meaning is determined from the meanings of its component parts and the way they are combined (i.e. the principle of compositionality), the meaning of a task is determined from its sub-tasks and their orderings. Thus, an ideal model for solving the task verification problem would be a model that can leverage sub-task decomposition while grounding them -- Oracle Model.
% (2) NSG exploits the compositional substructure of tasks by grounding component sub-tasks and reasoning on how they can be combined via ordering. 
% (3) Rest of the baselines as Brian described?


% We compare our model with various baselines described in Section \ref{subsection:baseline_evaluation} as shown in table \ref{table:baseline_results}. We found that simply apply CLIP\cite{clip} features with video pooling results in low performance in our task. Abstraction is particularly challenging for these model since the task requires the model to align abstract concepts resepect to each video segments while these methods discard such information during the pooling strategy. We also found models with mulitmodal fusion leads to better performance since our task requires model to understand the interaction between subtasks and video states along time jointly. The VIOLIN baseline is specifically designed for video entailment task, which is the closest baseline in our task. The results shows the model captured the subtask to video segment relation in certain tasks but failed when encountering Novel Tasks where new subtask combination is presented. We hypothesis the model overfits into certain sentence/video sequence pattern but can't generalize to novel combination.
% We found our model decreases in the performance of Novel steps. Our model discovers better alignment between the text query and video segments during training. However, when the query is unseen, we can not benefit from the alignment model. We will leave this to future work. The Text2text baseline demosntrate the importance of using video information in our task, the model cannot simply process text information from video. 

% Findings:
% 1. CLIP fails in abstraction which needs the model to align the concept with sentence sturcture.
% 2. Fusion benefits performance
% 3. Our closest baseline VIOLIN video entailment capture both the visual and text sequence by LSTM and fusion layer failed to align Novel tasks and abstraction, which requires the ability to align detailed subtask in the concecpt level
% Why Novel step failed? Our model encourage better alignment, but is difficult to align unseen step, leads to lower performance.


%Since the video and text representation tend to be close when they share similar semantic
% need to establish some terminology first
% token level features: (text, image), global features: (text, video) 

% ==== Start of older version baseline section ====

% \bc{maybe we can include this in intro or evaluation}
% Prior works have shown that VLMs demonstrate a severe lack of compositionality and order sensitivity~\cite{VLMbag-of-words,winoground}. For instance, the BLIP~\cite{blip} model fails to understand the difference between S1:\textit{horse is eating the grass} \& S2:\textit{grass is eating the horse} would often map both sentences to the same image. As stated in \cite{VLMbag-of-words}, it is attributed to the training setup which leads to the discarding of valuable order information. While the authors only superficially discuss the reasons, we hypothesize that this could be attributed to the pooling methods used in the text and visual encoders of the VLMs. Non-parameterized methods like \emph{mean-pooling} fail to account for the order information. 
% On the other hand, action localization and segmentation are considered token-level tasks~\cite{}.

% Based on our hypothesis, we group our baselines into the following categories (here, \emph{global}: global features $\in \mathds{R}^d$ for text/video input from pre-trained feature extractors; \emph{local}: token (text token or image-frame token) features for text/video from pre-trained feature extractors): 
% \begin{itemize}
%     \item \emph{global} / \emph{global}: 

%     CLIP4Clip~\cite{luo2022clip4clip}
    
%     CLIP Hitchhiker~\cite{clip_hitchiker} which uses a weighted-mean pooling of token features to global features given as.

    
%     \item \emph{global} / \emph{local}: 
%     \item \emph{local} / \emph{local}: 
% \end{itemize}

% The order is text/video.
% \bc{proposal free models. Why these baselines? Large-scale VL pretraining model. Generalizable to multiple downstream tasks}. 
% ==== End of older version baseline section ====

% BASELINE SELECTION


% BASELINE TRAINING DETAILS

% ADDITIONAL BASELINES IN APPENDIX  



% Broadly, all models use a visual backbone to encode the premise, a text backbone to encode the hypothesis, and a fusion network to merge the text and video representations to output an entailment label.~\rd{Brian and Rishi, can we be consistent with the language in the intro and rest of the paper when we write this section? Will make it easier to re-write.} For video encoding, we used 2D convolution networks (ResNet-18~\cite{resnet})\footnote{We use a BiLSTM to aggregate the features into a single video representation}, 3D convolution networks (I3D~\cite{i3d}, S3D~\cite{s3d}), and transformer models (CLIP~\cite{clip}, MViT~\cite{mvit}). Similarly, for text encoding, we used word embeddings (GloVe~\cite{glove}) and transformer models (DistilBERT~\cite{distil_bert}, CLIP~\cite{clip}). Fusion models were either attention-based (BiDAF~\cite{bidaf}) or simple concatenation of the two encodings. We report results for each model \red{with and without finetuning} of the pre-trained feature extractors on a train/validation split of $80/20$. Following are the observations from our evaluation:
% \begin{itemize}
%     \item In general, all models were found to perform poorly on the sub-goal composition and abstraction splits. This suggests that the models may suffer from overfitting, subsequently failing to generalize to novel compositions in both visual and language inputs. These findings emphasize the need for developing models that can effectively handle composition and abstraction in a wide range of inputs.
%     \item When encoding videos, transformer models demonstrated superior performance compared to 2D and 3D convolution networks. This suggests that pre-trained transformer models are better equipped to capture the rich visual features present in videos.
%     \item For text encoding, GloVe was found to perform slightly better than BERT and CLIP. This is likely due to BERT's and CLIP's higher dimensionality ($768 d$, $512 d$ respectively) leading to overfitting (particularly on the low vocabulary space of hypotheses), compared to the lower dimensionality of GloVe ($300 d$).
%     \item In general, attention-based models display superior performance across splits. This suggests that attention models are more effective at grounding the text to videos.
%     \item As shown in Figure~\ref{figure:complexity-ordering},  there is a decline in performance as the complexity of the task increases. Additionally, along the ordering axis, performance initially improves as the number of ordering constraints increases (from ordering = 0 to ordering = 1) but then deteriorates at ordering = 2. We attribute this to the fact that the training data contains a significant number of tasks with ordering = 1.
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%