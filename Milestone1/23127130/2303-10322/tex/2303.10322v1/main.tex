\documentclass[journal,onecolumn]{IEEEtran}

\let\proof\relax 
\let\endproof\relax
\usepackage{amssymb,bm}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{comment}

% *** CITATION PACKAGES ***
\usepackage{cite}
\renewcommand*{\citepunct}{, } % Was {], [}
\renewcommand*{\citedash}{--}  % Was {]--[}
\usepackage{balance}

%\usepackage{dblfloatfix}
\usepackage{url}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{accents}

\newlength{\dhatheight}
\newcommand{\doublehat}[1]{%
    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
    \addtolength{\dhatheight}{-0.25ex}%
    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
    \smash{\hat{#1}}}}

\DeclareMathOperator*{\argmin}{arg\,min}

% P option in tables
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage{xcolor}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{claim}{Claim}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newtheorem{remark}{Remark}
\newenvironment{claimproof}{\vspace{-1pc}\proof}{\endproof}

\begin{document}
% Reduce spacing above and below equations
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}

\title{Inverse Cubature and Quadrature Kalman filters}

\author{Himali Singh, Kumar Vijay Mishra and Arpan Chattopadhyay
\thanks{$^\ast$K. V. M. and A. C. have made equal contributions.}
\thanks{H. S. and A. C. are with the Electrical Engineering Department, Indian Institute of Technology Delhi, India. Email: \{eez208426, arpanc\}@ee.iitd.ac.in.} 
\thanks{K. V. M. is with the United States DEVCOM Army Research Laboratory, Adelphi, MD 20783 USA. E-mail: kvm@ieee.org.}
\thanks{A. C. acknowledges support via the faculty seed grant, professional development fund and professional development  allowance from IIT Delhi, and the  seed grant and grant no. RP04215G from I-Hub Foundation for Cobotics. H. S. acknowledges support via Prime Minister Research Fellowship. K. V. M. acknowledges support from the National Academies of Sciences, Engineering, and Medicine via Army Research Laboratory Harry Diamond Distinguished Fellowship.}
}
\maketitle

\begin{abstract}
    Recent developments in counter-adversarial system research have led to the development of inverse stochastic filters that are employed by a defender to infer the information its adversary may have learned. Prior works addressed this inverse cognition problem by proposing inverse Kalman filter (I-KF) and inverse extended KF (I-EKF), respectively, for linear and non-linear Gaussian state-space models. However, in practice, many counter-adversarial settings involve highly non-linear system models, wherein EKF's linearization often fails. In this paper, we consider the efficient numerical integration techniques to address such nonlinearities and, to this end, develop inverse cubature KF (I-CKF) and inverse quadrature KF (I-QKF). We derive the stochastic stability conditions for the proposed filters in the exponential-mean-squared-boundedness sense. Numerical experiments demonstrate the estimation accuracy of our I-CKF and I-QKF with the recursive Cram\'{e}r-Rao lower bound as a benchmark.
\end{abstract}

\begin{keywords}
Bayesian filtering, counter-adversarial systems, cubature Kalman filter, inverse cognition, quadrature Kalman filter.
\end{keywords}

\section{Introduction}
\label{sec:introduction}
Autonomous cognitive agents continually sense their surroundings and optimally adapt themselves in response to changes in the environment. For instance, a cognitive radar\cite{mishra2020toward} may tune parameters of its transmit waveform and switch the receive processing for enhanced target detection \cite{mishra2017performance} and tracking\cite{bell2015cognitive,sharaga2015optimal}. Recently, motivated by the design of counter-adversarial systems, \textit{inverse cognition} has been proposed to detect, estimate, and predict the behavior of cognitive agents \cite{krishnamurthy2019how,krishnamurthy2020identifying}. For instance, an intelligent target may observe the actions of a cognitive radar that is trying to detect the former. The target then attempts to predict the radar's future actions in a Bayesian sense. To this end, the inverse cognitive agent requires an estimate of the adversarial system's (or \textit{forward} cognitive agent's) inference. This is precisely the objective of an inverse Bayesian filter, whose goal is to estimate the posterior distribution computed by a forward Bayesian filter given noisy measurements of the posterior\cite{krishnamurthy2019how}.

The inverse cognition problem involves two agents: a `defender' (e.g., an intelligent target) and an `attacker' (e.g., a radar) that is equipped with a Bayesian filter. The attacker estimates the defender's state and then takes an action based on this estimate. The defender observes these actions of the attacker and then computes \textit{an estimate of the attacker's estimate} via an inverse filter. There are also parallels to these inverse problems in the machine learning literature. For instance, inverse reinforcement learning (IRL) problems require the adversary's reward function to be learned \textit{passively} by observing its optimal behaviour\cite{ng2000algorithms}. The inverse cognition differs from IRL in the sense that the defender \textit{actively} probes its adversarial agent.

Prior research on inverse Bayesian filtering includes inverse hidden Markov model\cite{mattila2020hmm} for finite state-space and inverse Kalman filter (I-KF)\cite{krishnamurthy2019how} for linear Gaussian state-space models. % assuming a forward Kalman filter (KF). 
These works do not address the highly non-linear counter-adversarial systems encountered in practice. In this regard, our recent work proposed inverse extended KF (I-EKF) for non-linear system settings in \cite{singh2022inverse,singh2022inverse_part1}. However, even EKF performs poorly in case of severe non-linearities and modeling errors\cite{tenney1977tracking}, for which our follow-up work \cite{singh2022inverse_part2} introduced inverses of several EKF variants such as high-order and dithered EKFs. 

A more accurate approximation of non-linear functions than the advanced EKF variants is possible through derivative-free Gaussian sigma-point KFs (SPKFs). % have been proposed to deal with EKF's limitations. SPKFs 
These filters generate a set of deterministic points and propagate them through the non-linear functions to approximate the true mean and covariance of the posterior density. While EKF is applicable to differentiable functions, SPKFs can handle discontinuities. A popular SPKF is the unscented KF (UKF)\cite{julier2004unscented}, which utilizes unscented transform to generate sigma points and %computes the posterior means and covariances as weighted sums. The unscented transform 
approximate a probability density instead of non-linear functions. The corresponding inverse UKF (I-UKF) was proposed in our recent work \cite{singh2022iukf}. 

The SPKF performance is further improved by employing better numerical integration techniques to calculate the recursive integrals in Bayesian estimation. For example, cubature KF (CKF) \cite{arasaratnam2009cubature} and quadrature KF (QKF) \cite{ito2000gaussian,arasaratnam2007qkf} numerically approximate the multidimensional integral based on, respectively, cubature and Gauss-Hermite quadrature rules. Other examples include central difference KF\cite{norgaard2000new} and cubature-quadrature KF\cite{bhaumik2013cubature}. Analogous to EKFs, higher-degree CKF\cite{jia2013high}, square-root CKF\cite{arasaratnam2009cubature}, and Gaussian-sum QKF\cite{kottakki2014state,arasaratnam2007qkf} are also possible. %The SPKFs, like EKF, assume a prior posterior density form (mostly Gaussian) and capture the system's non-linearity locally at the state estimates. However, EKF works with differentiable functions, while SPKFs can handle discontinuities. On the contrary, global non-linear filters like particle filters\cite{ristic2003beyond} directly approximate the posterior density but are computationally expensive.

In this paper, we develop inverse filters based on the afore-referenced efficient numerical integration techniques. Similar to the inverse cognition framework in \cite{krishnamurthy2019how,mattila2020hmm}, we assume perfect system information and propose inverse CKF (I-CKF) and inverse QKF (I-QKF). Our prior work \cite{singh2022inverse_part2} also addressed the case when the system model is not known. We derive the stability conditions for the proposed I-CKF and I-QKF, Our theoretical analyses show that the forward filter's stability is sufficient to guarantee the same for the inverse filter under mild conditions imposed on the system. In the process, we also obtain improved stability results, hitherto unreported in the literature, for the forward CKF. Our numerical experiments demonstrate the performance of the proposed methods compared to the recursive Cram\'{e}r-Rao lower bound (RCRLB) \cite{tichavsky1998posterior}.

%The rest of the paper is organized as follows. The next section describes the system model for the inverse cognition problem. In Section~\ref{sec:ICKF and IQKF}, we derive I-CKF and I-QKF. The stability conditions are provided in Section~\ref{sec:stability} while numerical exmaples are discussed in Section~\ref{sec:numericals}. We conclude in Section~\ref{sec:summary}.

Throughout this paper, we reserve boldface lowercase and uppercase letters for vectors (column vectors) and matrices, respectively, and $\lbrace a_{i}\rbrace_{i_{1}\leq i\leq i_{2}}$ denotes a set of elements indexed by an integer $i$. The notation $[\mathbf{a}]_{i}$ is used to denote the $i$-th component of vector $\mathbf{a}$ and $[\mathbf{A}]_{i,j}$ denotes the $(i,j)$-th component of matrix $\mathbf{A}$. The transpose operation is $(\cdot)^{T}$; the $l_{2}$ norm of a vector is $\|\cdot\|_{2}$; and the spectral norm of a matrix is $\|\cdot\|$. For matrices $\mathbf{A}$ and $\mathbf{B}$, the inequality $\mathbf{A}\preceq\mathbf{B}$ means that $\mathbf{B}-\mathbf{A}$ is a positive semidefinite (p.s.d.) matrix. For a function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$, $\frac{\partial f}{\partial\mathbf{x}}$ denotes the $\mathbb{R}^{m\times n}$ Jacobian matrix. Also, $\mathbf{I}_{n}$ and $\mathbf{0}_{n\times m}$ denote a `$n\times n$' identity matrix and a `$n\times m$' all zero matrix, respectively. The Gaussian random variable is represented as $\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu},\mathbf{Q})$ with mean $\boldsymbol{\mu}$ and covariance matrix $\mathbf{Q}$. We denote the Cholesky decomposition of matrix $\mathbf{A}$ as $\mathbf{A}=\sqrt{\mathbf{A}}\sqrt{\mathbf{A}}^{T}$. 

\section{System model}
\label{sec:system model}
Denote the defender's state at $k$-th time as $\mathbf{x}_{k}\in\mathbb{R}^{n_{x}\times 1}$. Consider the defender's discrete-time state evolution process $\{\mathbf{x}_{k}\}_{k\geq 0}$ as
\par\noindent\small
\begin{align}
    \mathbf{x}_{k+1}=f(\mathbf{x}_{k})+\mathbf{w}_{k},\label{eqn:state transition x}
\end{align}
\normalsize
where $\mathbf{w}_{k}\sim\mathcal{N}(\mathbf{0}_{n_{x}\times 1},\mathbf{Q})$ is the process noise with covariance matrix $\mathbf{Q}\in\mathbb{R}^{n_{x}\times n_{x}}$. The defender knows its own state $\mathbf{x}_{k}$ perfectly. The attacker observes the defender's state as $\mathbf{y}_{k}\in\mathbb{R}^{n_{y}\times 1}$ at $k$-th time as
\par\noindent\small
\begin{align}
    \mathbf{y}_{k}=h(\mathbf{x}_{k})+\mathbf{v}_{k},\label{eqn:observation y}
\end{align}
\normalsize
where $\mathbf{v}_{k}\sim\mathcal{N}(\mathbf{0}_{n_{y}\times 1},\mathbf{R})$ is the attacker's measurement noise with covariance matrix $\mathbf{R}\in\mathbb{R}^{n_{y}\times n_{y}}$. Using the forward filter, the attacker then computes an estimate $\hat{\mathbf{x}}_{k}$ of the defender's state $\mathbf{x}_{k}$ from observations $\{\mathbf{y}_{j}\}_{1\leq j\leq k}$. The attacker further takes an action $g(\hat{\mathbf{x}}_{k})\in\mathbb{R}^{n_{a}\times 1}$, which the defender observes as
\par\noindent\small
\begin{align}
    \mathbf{a}_{k}=g(\hat{\mathbf{x}}_{k})+\bm{\epsilon}_{k},\label{eqn:observation a}
\end{align}
\normalsize
where $\bm{\epsilon}_{k}\sim\mathcal{N}(\mathbf{0}_{n_{a}\times 1},\bm{\Sigma}_{\epsilon})$ is the defender's measurement noise with covariance matrix $\bm{\Sigma}_{\epsilon}\in\mathbb{R}^{n_{a}\times n_{a}}$. 

The defender's goal is to compute an estimate $\doublehat{\mathbf{x}}_{k}$ of the attacker's estimate $\hat{\mathbf{x}}_{k}$ given $\{\mathbf{a}_{j},\mathbf{x}_{j}\}_{1\leq j\leq k}$ in the inverse filter. The functions $f(\cdot)$, $h(\cdot)$ and $g(\cdot)$ are non-linear functions, while the noise processes $\{\mathbf{w}_{k}\}_{k\geq 0}$, $\{\mathbf{v}_{k}\}_{k\geq 1}$ and $\{\bm{\epsilon}_{k}\}_{k\geq 1}$ are mutually independent and identically distributed across time. Throughout the paper, we assume these functions and noise distributions are known to both agents. The defender also knows the attacker's forward filter. The case of the unknown system model may be handled using techniques proposed in our previous work \cite{singh2022inverse_part2}. We omit details of these methods in this paper because of the paucity of space.

\section{Inverse Filter Models}
\label{sec:ICKF and IQKF}
The (forward) CKF generates a set of `$2n_{x}$' cubature points deterministically about the state estimate based on the third-degree spherical-radial cubature rule to numerically compute a standard Gaussian weighted non-linear integral \cite{arasaratnam2009cubature}. Similarly, the ($m$-point) QKF employs a $m$-point Gauss-Hermite quadrature rule to generate $m^{n_{x}}$ quadrature points\cite{ito2000gaussian}. In \cite{arasaratnam2007qkf}, QKF was reformulated using statistical linear regression, wherein the linearized function is more accurate in a statistical sense than the first-order Taylor series approximation used in EKF. %In the following, we first develop the I-CKF assuming the attacker employs a forward CKF to compute its estimate $\hat{\mathbf{x}}_{k}$. The I-QKF is then formulated in Section~\ref{subsec:IQKF}, assuming the attacker's forward filter to be QKF.

\subsection{Inverse CKF}
\label{subsec:ICKF}
Denote the $i$-th standard basis vector in $\mathbb{R}^{n_{x}\times 1}$ by $\mathbf{e}_{i}$. Define $\bm{\xi}_{i}$ as the $i$-th element of the $2n_{x}$-points set 
\small
$\{\sqrt{n_{x}}\mathbf{e}_{1},\sqrt{n_{x}}\mathbf{e}_{2},\hdots,\sqrt{n_{x}}\mathbf{e}_{n_{x}},-\sqrt{n_{x}}\mathbf{e}_{1},\hdots,-\sqrt{n_{x}}\mathbf{e}_{n_{x}}\}$. \normalsize
The cubature points generated from a state estimate $\hat{\mathbf{x}}$ and its error covariance matrix $\bm{\Sigma}$ are $\widetilde{\mathbf{x}}_{i}=\hat{\mathbf{x}}+\sqrt{\bm{\Sigma}}\bm{\xi}_{i}$ for all $i=1,2,\hdots,2n_{x}$ with each weight $\omega_{i}=1/2n_{x}$. The third-degree cubature rule is exact for polynomials up to the third degree and computes the posterior mean accurately but the error covariance approximately. It has been reported in \cite{arasaratnam2009cubature} that higher-degree rules may not necessarily yield any improvement in the CKF performance.

\textbf{Forward CKF:} Denote the forward CKF's cubature points generated for the time and measurement update by, respectively, $\{\mathbf{s}_{i,k}\}_{1\leq i\leq 2n_{x}}$ and $\{\mathbf{q}_{i,k+1|k}\}_{1\leq i\leq 2n_{x}}$. For time update, the cubature points $\{\mathbf{s}_{i,k}\}$ are propagated to $\{\mathbf{s}^{*}_{k+1|k}\}$ through state transition in \eqref{eqn:state transition x}. The predicted state $\hat{\mathbf{x}}_{k+1|k}$ and prediction error covariance matrix $\bm{\Sigma}_{k+1|k}$ are obtained as the weighted average of these propagated points. Similarly, the measurement update involves propagating cubature points $\{\mathbf{q}_{i,k+1|k}\}$ to $\{\mathbf{q}^{*}_{i,k+1|k}\}$ through observation \eqref{eqn:observation y} for the predicted observation $\hat{\mathbf{y}}_{k+1|k}$. The state estimate $\hat{\mathbf{x}}_{k+1}$ and the associated error covariance matrix $\bm{\Sigma}_{k+1}$ computed recursively by the forward CKF are \cite{arasaratnam2009cubature}
\par\noindent\small
\begin{align}
    &\textrm{Time update:}\;\;\;\mathbf{s}_{i,k}=\hat{\mathbf{x}}_{k}+\sqrt{\bm{\Sigma}_{k}}\bm{\xi}_{i}\;\;\;\forall\;\; i=1,2,\hdots,2n_{x},\label{eqn:forward ckf prediction sigma points}\\
    &\mathbf{s}^{*}_{i,k+1|k}=f(\mathbf{s}_{i,k})\;\;\;\forall i=1,2,\hdots,2n_{x},\nonumber\\
    &\hat{\mathbf{x}}_{k+1|k}=\sum_{i=1}^{2n_{x}}\omega_{i}\mathbf{s}^{*}_{i,k+1|k},\label{eqn:forward ckf x predict}\\
    &\bm{\Sigma}_{k+1|k}=\sum_{i=1}^{2n_{x}}\omega_{i}\mathbf{s}^{*}_{i,k+1|k}(\mathbf{s}^{*}_{i,k+1|k})^{T}-\hat{\mathbf{x}}_{k+1|k}\hat{\mathbf{x}}_{k+1|k}^{T}+\mathbf{Q},\nonumber\\
    &\textrm{Measurement update:}\nonumber\\
    &\mathbf{q}_{i,k+1|k}=\hat{\mathbf{x}}_{k+1|k}+\sqrt{\bm{\Sigma}_{k+1|k}}\bm{\xi}_{i}\;\;\;\forall\;\; i=1,2,\hdots,2n_{x},\label{eqn:forward CKF update sigma points}\\
    &\mathbf{q}^{*}_{i,k+1|k}=h(\mathbf{q}_{i,k+1|k})\;\;\;\forall i=1,2,\hdots,2n_{x},\nonumber\\
    &\hat{\mathbf{y}}_{k+1|k}=\sum_{i=1}^{2n_{x}}\omega_{i}\mathbf{q}^{*}_{i,k+1|k},\label{eqn:forward ckf y predict}\\
    &\bm{\Sigma}^{y}_{k+1}=\sum_{i=1}^{2n_{x}}\omega_{i}\mathbf{q}^{*}_{i,k+1|k}(\mathbf{q}^{*}_{i,k+1|k})^{T}-\hat{\mathbf{y}}_{k+1|k}\hat{\mathbf{y}}_{k+1|k}^{T}+\mathbf{R},\nonumber\\
    &\bm{\Sigma}^{xy}_{k+1}=\sum_{i=1}^{2n_{x}}\omega_{i}\mathbf{q}_{i,k+1|k}(\mathbf{q}^{*}_{i,k+1|k})^{T}-\hat{\mathbf{x}}_{k+1|k}\hat{\mathbf{y}}_{k+1|k}^{T}\nonumber,\\
    &\mathbf{K}_{k+1}=\bm{\Sigma}^{xy}_{k+1}\left(\bm{\Sigma}^{y}_{k+1}\right)^{-1}\nonumber,\\
    &\hat{\mathbf{x}}_{k+1}=\hat{\mathbf{x}}_{k+1|k}+\mathbf{K}_{k+1}(\mathbf{y}_{k+1}-\hat{\mathbf{y}}_{k+1|k}),\label{eqn:forward ckf x update}\\
    &\bm{\Sigma}_{k+1}=\bm{\Sigma}_{k+1|k}-\mathbf{K}_{k+1}\bm{\Sigma}^{y}_{k+1}\mathbf{K}_{k+1}^{T}.\label{eqn:forward CKF sigma update}   
\end{align}
\normalsize

\textbf{I-CKF:} The inverse filter treats $\hat{\mathbf{x}}_{k}$ as the state to be estimated using observations \eqref{eqn:observation a}. Substituting in \eqref{eqn:forward ckf x update} for observation $\mathbf{y}_{k+1}$, predicted state $\hat{\mathbf{x}}_{k+1|k}$ and predicted observation $\hat{\mathbf{y}}_{k+1|k}$ from \eqref{eqn:observation y}, \eqref{eqn:forward ckf x predict} and \eqref{eqn:forward ckf y predict}, respectively, we obtain the I-CKF's state transition as
\par\noindent\small
\begin{align}
    \hat{\mathbf{x}}_{k+1}&=\frac{1}{2n_{x}}\sum_{i=1}^{2n_{x}}\left(\mathbf{s}^{*}_{i,k+1|k}-\mathbf{K}_{k+1}\mathbf{q}^{*}_{i,k+1|k}\right)+\mathbf{K}_{k+1}h(\mathbf{x}_{k+1})\nonumber\\
    &\;\;+\mathbf{K}_{k+1}\mathbf{v}_{k+1}.\label{eqn:ICKF state transition detail}
\end{align}
\normalsize
%Now, under the perfect system information assumption, 
The propagated points $\{\mathbf{s}^{*}_{i,k+1|k}\}$ and $\{\mathbf{q}^{*}_{i,k+1|k}\}$, and gain matrix $\mathbf{K}_{k+1}$ are deterministic functions of the first set of cubature points $\{\mathbf{s}_{i,k}\}$. Further, the cubature points $\{\mathbf{s}_{i,k}\}$ depend only on the previous estimate $\hat{\mathbf{x}}_{k}$ and its error covariance matrix $\bm{\Sigma}_{k}$ through \eqref{eqn:forward ckf prediction sigma points}. Hence, \eqref{eqn:ICKF state transition detail} becomes
\par\noindent\small
\begin{align}
     \hat{\mathbf{x}}_{k+1}=\widetilde{f}(\hat{\mathbf{x}}_{k},\bm{\Sigma}_{k},\mathbf{x}_{k+1},\mathbf{v}_{k+1}).\label{eqn:ICKF state transition}
\end{align}
\normalsize

The actual state $\mathbf{x}_{k+1}$ known perfectly to the defender acts as a known exogenous input. The process noise $\mathbf{v}_{k+1}$ is \textit{non-additive} because of the dependence of $\mathbf{K}_{k+1}$ on the previous state estimates. Note that the current observation $\mathbf{y}_{k}$ does not affect the current covariance matrix update $\bm{\Sigma}_{k}$, which is computed recursively from the previous state estimate $\hat{\mathbf{x}}_{k-1}$ through the weighted average covariances $\bm{\Sigma}_{k|k-1}$, $\bm{\Sigma}^{y}_{k}$ and $\bm{\Sigma}^{xy}_{k}$. In our inverse filter formulation, we treat $\bm{\Sigma}_{k}$ as another exogenous input of \eqref{eqn:ICKF state transition} and approximate it as $\bm{\Sigma}^{*}_{k}$ computed recursively using I-CKF's previous state estimate $\doublehat{\mathbf{x}}_{k-1}$ in the same manner as the forward CKF computes $\bm{\Sigma}_{k}$ using its estimate $\hat{\mathbf{x}}_{k-1}$.

We augment the state estimate $\hat{\mathbf{x}}_{k}$ with the non-additive noise term $\mathbf{v}_{k+1}$ and consider state $\mathbf{z}_{k}=[\hat{\mathbf{x}}_{k}^{T},\mathbf{v}_{k+1}^{T}]^{T}$ of dimension $n_{z}=n_{x}+n_{y}$. The state transition \eqref{eqn:ICKF state transition} in terms of $\mathbf{z}_{k}$ becomes $\hat{\mathbf{x}}_{k+1}=\widetilde{f}(\mathbf{z}_{k},\bm{\Sigma}_{k},\mathbf{x}_{k+1})$. Denote $\overline{\bm{\xi}}_{j}$ as the $j$-th element of $2n_{z}$-points set
\small
$\{\sqrt{n_{z}}\overline{\mathbf{e}}_{1},\sqrt{n_{z}}\overline{\mathbf{e}}_{2},\hdots,\sqrt{n_{z}}\overline{\mathbf{e}}_{n_{z}},-\sqrt{n_{z}}\overline{\mathbf{e}}_{1},-\sqrt{n_{z}}\overline{\mathbf{e}}_{2},\hdots,-\sqrt{n_{z}}\overline{\mathbf{e}}_{n_{z}}\}$
\normalsize
where $\overline{\mathbf{e}}_{j}$ is the $j$-th standard basis vector in $\mathbb{R}^{n_{z}\times 1}$. Define
\par\noindent\small
\begin{align}
    \hat{\mathbf{z}}_{k}=[\doublehat{\mathbf{x}}_{k}^{T},\mathbf{0}_{1\times n_{y}}]^{T},\;\;\overline{\bm{\Sigma}}^{z}_{k}=\begin{bsmallmatrix}\overline{\bm{\Sigma}}_{k}&\mathbf{0}_{n_{x}\times n_{y}}\\\mathbf{0}_{n_{y}\times n_{x}}&\mathbf{R}\end{bsmallmatrix}.\label{eqn:ICKF z hat and sigma z}
\end{align}
\normalsize
The I-CKF recursions for state estimate $\doublehat{\mathbf{x}}_{k}$ and associated error covariance matrix $\overline{\bm{\Sigma}}_{k}$ using observations \eqref{eqn:observation a} are
\par\noindent\small
\begin{align}
    &\textit{Time update:}\;\;\;\overline{\mathbf{s}}_{j,k}=\hat{\mathbf{z}}_{k}+\sqrt{\overline{\bm{\Sigma}}^{z}_{k}}\overline{\bm{\xi}}_{j}\;\;\;\forall\;\; i=1,2,\hdots,2n_{z}\nonumber\\
    &\overline{s}^{*}_{j,k+1|k}=\widetilde{f}(\overline{\mathbf{s}}_{j,k},\bm{\Sigma}_{k}^{*},\mathbf{x}_{k+1})\;\;\;\forall\;\; j=1,2,\hdots,2n_{z},\label{eqn:ICKF f propagate}\\
    &\doublehat{\mathbf{x}}_{k+1|k}=\sum_{j=1}^{2n_{z}}\overline{\omega}_{j}\overline{\mathbf{s}}^{*}_{j,k+1|k},\label{eqn:ICKF x predict}\\
    &\overline{\bm{\Sigma}}_{k+1|k}=\sum_{j=1}^{2n_{z}}\overline{\omega}_{j}\overline{\mathbf{s}}^{*}_{j,k+1|k}(\overline{\mathbf{s}}^{*}_{j,k+1|k})^{T}-\doublehat{\mathbf{x}}_{k+1|k}\doublehat{\mathbf{x}}_{k+1|k}^{T},\label{eqn:ICKF sig predict}\\
    &\textit{Measurement update:}\;\mathbf{a}^{*}_{j,k+1|k}=g(\overline{\mathbf{s}}^{*}_{j,k+1|k})\;\;\;\forall\;\; j=1,\hdots,2n_{z},\label{eqn:ICKF g propagate}\\
    &\hat{\mathbf{a}}_{k+1|k}=\sum_{j=1}^{2n_{z}}\overline{\omega}_{j}\mathbf{a}^{*}_{j,k+1|k},\label{eqn:ICKF a predict}\\
    &\overline{\bm{\Sigma}}^{a}_{k+1}=\sum_{j=1}^{2n_{z}}\overline{\omega}_{j}\mathbf{a}^{*}_{j,k+1|k}(\mathbf{a}^{*}_{j,k+1|k})^{T}-\hat{\mathbf{a}}_{k+1|k}\hat{\mathbf{a}}_{k+1|k}^{T}+\bm{\Sigma}_{\epsilon},\label{eqn:ICKF sig a}\\
    &\overline{\bm{\Sigma}}^{xa}_{k+1}=\sum_{j=1}^{2n_{z}}\overline{\omega}_{j}\overline{\mathbf{s}}^{*}_{j,k+1|k}(\mathbf{a}^{*}_{j,k+1|k})^{T}-\doublehat{\mathbf{x}}_{k+1|k}\hat{\mathbf{a}}_{k+1|k}^{T},\label{eqn:ICKF cross cov}\\
    &\doublehat{x}_{k+1}=\doublehat{x}_{k+1|k}+\overline{\mathbf{K}}_{k+1}(\mathbf{a}_{k+1}-\hat{\mathbf{a}}_{k+1|k}),\label{eqn:ICKF state update}\\
    &\overline{\bm{\Sigma}}_{k+1}=\overline{\bm{\Sigma}}_{k+1|k}-\overline{\mathbf{K}}_{k+1}\overline{\bm{\Sigma}}^{a}_{k+1}\overline{\mathbf{K}}_{k}^{T}.\label{eqn:ICKF covariance update}
\end{align}
\normalsize
with each weight $\overline{\omega}_{j}=1/2n_{z}$ and gain matrix $\overline{\mathbf{K}}_{k+1}=\overline{\bm{\Sigma}}^{xa}_{k+1}\left(\overline{\bm{\Sigma}}^{a}_{k+1}\right)^{-1}$. The I-CKF recursions follow from the non-additive noise formulation of CKF \cite{wang2017augmentedckf} with a higher $(n_{x}+n_{y})$-dimensional cubature points. However, unlike forward CKF, these cubature points are generated only once for the time update taking into account the process noise statistics (covariance $\mathbf{R}$).
\begin{remark}
The forward gain matrix $\mathbf{K}_{k+1}$ in case of I-KF is deterministic \cite{krishnamurthy2019how}. For I-EKF \cite{singh2022inverse}, this matrix depends on only the linearized system model at the state estimates. Hence, I-KF and I-EKF treat $\mathbf{K}_{k+1}$ as a time-varying parameter of the inverse filter's state transition. But the gain matrix in CKF explicitly depends on the state estimates through the covariances computed from the generated cubature points. Thus, it is not treated as a parameter of \eqref{eqn:ICKF state transition detail}. 
\end{remark}
\begin{remark}
Contrary to CKF, UKF generates a set of `$2n_{x}+1$' sigma points around the previous state estimate, including one at the previous estimate itself, with their spread and weights controlled by parameter $\kappa$. The I-UKF\cite{singh2022iukf} also assumes a known forward filter's $\kappa$. %, which leads to errors in case of incorrect assumptions. 
The I-CKF, however, does not require any such parameter information. 
\end{remark}
\begin{remark}
Note that forward UKF with $\kappa$ set to $0$ results in CKF as the attacker's forward filter. In that case, I-UKF with I-UKF's control parameter $\overline{\kappa}=0$ reduces to I-CKF.
\end{remark}
\subsection{Inverse QKF}
\label{subsec:IQKF}
Define $\mathbf{M}$ as the `$m\times m$' symmetric tridiagonal matrix with zero diagonal elements such that $[\mathbf{M}]_{(i,i+1)}=\sqrt{i/2}$ for all $1\leq i\leq m$. For the one-dimensional case, the $i$-th quadrature point of the $m$-point quadrature rule is $\zeta_{i}=\sqrt{2}\beta_{i}$ where $\beta_{i}$ is the $i$-th eigenvalue of $\mathbf{M}$. The corresponding weight $\omega_{i}=[\bm{\nu_{i}}]_{1}^{2}$, where $[\bm{\nu}_{i}]_{1}$ is the first element of the $i$-th normalized eigenvector of $\mathbf{M}$. A $m$-point quadrature rule computes the mean exactly for polynomials of order less than or equal to $(2m-1)$ and the covariance is exact for polynomials of degree less than $m$ \cite{arasaratnam2007qkf}. In a $n_{x}$-dimensional state space, the $m$-point (per-axis) QKF considers $m^{n_{x}}$ quadrature points obtained by extending the scalar quadrature points as $\bm{\zeta}_{i}=[\zeta_{i_{1}}, \zeta_{i_{2}},\hdots,\zeta_{i_{n_{x}}}]^{T}$ and $\omega_{i}=\prod_{j=1}^{n_{x}}\omega_{i_{j}}$ where $\lbrace\zeta_{i_{j}},\omega_{i_{j}}\rbrace_{1\leq i_{j}\leq m}$ are the $m$ scalar quadrature points corresponding to the $j$-th dimension. In I-QKF, we assume a forward QKF to estimate the defender's state. 

Consider the state transition \eqref{eqn:state transition x} and observations \eqref{eqn:observation y} of the forward filter. Then, \textit{ceteris paribus}, using these quadrature points in place of cubature points in the forward CKF, we obtain the $m$-point forward QKF recursions. %The quadrature points generated, respectively, for 
The corresponding time and measurement updates become
\par\noindent\small
\begin{align*}  \mathbf{s}_{i,k}&=\hat{\mathbf{x}}_{k}+\sqrt{\bm{\Sigma}_{k}}\bm{\zeta}_{i}\;\;\;\forall\;\; i=1,2,\hdots,m^{n_{x}},\\
    \mathbf{q}_{i,k+1|k}&=\hat{\mathbf{x}}_{k+1|k}+\sqrt{\bm{\Sigma}_{k+1|k}}\bm{\zeta}_{i}\;\;\;\forall\;\; i=1,2,\hdots,m^{n_{x}}.
\end{align*}
\normalsize

Assuming the parameter $m$ of the forward QKF to be known, the I-QKF's state transition is
\par\noindent\small
\begin{align*}
    \hat{\mathbf{x}}_{k+1}&=\sum_{i=1}^{m^{n_{x}}}\omega_{i}\left(\mathbf{s}^{*}_{i,k+1|k}-\mathbf{K}_{k+1}\mathbf{q}^{*}_{i,k+1|k}\right)+\mathbf{K}_{k+1}h(\mathbf{x}_{k+1})\\
    &\;\;+\mathbf{K}_{k+1}\mathbf{v}_{k+1}.
\end{align*}
\normalsize
In terms of the augmented state $\mathbf{z}_{k}=[\hat{\mathbf{x}}_{k}^{T},\mathbf{v}_{k+1}^{T}]^{T}$, state transition is $\hat{\mathbf{x}}_{k+1}=\widetilde{f}(\mathbf{z}_{k},\bm{\Sigma}_{k},\mathbf{x}_{k+1})$. Approximate $\bm{\Sigma}_{k}$ by $\bm{\Sigma}^{*}_{k}$ (evaluated similarly as in I-CKF). Denote $\hat{\mathbf{z}}_{k}$ and $\overline{\bm{\Sigma}}^{z}_{k}$ as in \eqref{eqn:ICKF z hat and sigma z}. For the $\overline{m}$-point I-QKF, we denote the quadrature points in the $n_{z}$-dimensional state space by $\lbrace\overline{\bm{\zeta}}_{j},\overline{\omega}_{j}\rbrace_{1\leq j\leq \overline{m}^{n_{z}}}$ such that I-QKF generates a set of $\overline{m}^{n_{z}}$ quadrature points as $\overline{\mathbf{s}}_{j,k}=\hat{\mathbf{z}}_{k}+\sqrt{\overline{\bm{\Sigma}}^{z}_{k}}\overline{\bm{\zeta}}_{j}$ for all $j=1,2,\hdots,\overline{m}^{n_{z}}$. The I-QKF's recursions then follow the time and measurement update procedure in \eqref{eqn:ICKF f propagate}-\eqref{eqn:ICKF covariance update}. Note that the choice of I-QKF's parameter $\overline{m}$ is independent of any assumption about the forward QKF's parameter $m$.

I-QKF recursions also similarly follow from the non-additive noise formulation of QKF\cite{arasaratnam2007qkf}. Analogous to I-CKF, I-QKF also generates only one set of quadrature points per recursion but the relative increase in the state dimension from $n_{x}$ to $n_{z}$ is more significant in the latter. The QKF and, hence, I-QKF suffer from the curse of dimensionality because the number of quadrature points increases geometrically with the state-space dimension. On the contrary, the cubature/sigma points in CKF/UKF scale up linearly. However, the expensive computations required to estimate $\{\bm{\zeta}_{i},\omega_{i}\}_{1\leq i\leq m^{n_{x}}}$ ($\{\overline{\bm{\zeta}}_{j},\overline{\omega}_{j}\}_{1\leq j\leq\overline{m}^{n_{z}}}$) in forward QKF (I-QKF) are performed offline \cite{arasaratnam2007qkf}. 
\begin{remark}\label{rmk:qkf}
For the one-dimensional state space ($n_{x}=1$), the forward $3$-point QKF coincides with the forward UKF with $\kappa=2$ \cite{ito2000gaussian}. In this case, I-QKF with $\overline{m}=3$ also coincides with I-UKF with $\overline{\kappa}=2$.
\end{remark}
\section{Stability analysis}
\label{sec:stability}
For the stability analysis, we adopt the unknown matrix approach introduced in \cite{xiong2006performance_ukf} for UKF with linear observations, wherein the linearization errors are modeled with unknown instrumental matrices. The unknown matrix approach has also been considered for CKF's stability in \cite{zarei2014convergence,wanasinghe2015stability}. However, \cite{zarei2014convergence} considered CKF with only linear observations. % and suggested modifications in CKF to enhance stability in the local asymptotic convergence sense. 
The stability conditions for CKF with non-linear measurements were derived in \cite{wanasinghe2015stability} using the exponential-mean-squared-boundedness sense. In the sequel, we provide improved stability results for the general forward CKF in the exponential-boundedness sense and then obtain the same for the I-CKF. % in Proposition~\ref{prop:forward CKF}. The I-CKF's stability conditions are then developed from the forward CKF results in Theorem~\ref{thm:I-CKF}. 

Following Remark~\ref{rmk:qkf}, the $3$-point forward QKF and $3$-point I-QKF for one-dimensional state-space coincide, respectively, with forward UKF with $\kappa=2$ and I-UKF with $\overline{\kappa}=2$. In this case, the sufficient conditions of \cite[Theorems~2 and 3]{singh2022iukf} also guarantee the stability of forward QKF and I-QKF, respectively. The general $\overline{m}$-point I-QKF case is omitted here because of the lack of space. %more challenging, and not considered here. 
In the following, we consider general time-varying process and measurement noise covariances $\mathbf{Q}_{k}$, $\mathbf{R}_{k}$ and $\overline{\mathbf{R}}_{k}$ instead of $\mathbf{Q}$, $\mathbf{R}$ and $\bm{\Sigma}_{\epsilon}$, respectively. Recall the definition of the exponential-mean-squared-boundedness.
\begin{definition}[Exponential mean-squared boundedness \cite{reif1999stochastic}] A stochastic process $\{\mathbf{b}_{k} \}_{k \geq 0}$ is defined to be exponentially bounded in the mean-squared sense if there are real numbers $\eta,\nu>0$ and $0<\lambda<1$ such that $\mathbb{E}\left[\|\mathbf{b}_{k}\|_{2}^{2}\right]\leq \eta\mathbb{E}\left[\|\mathbf{b}_{0}\|_{2}^{2}\right]\lambda^{k}+\nu$ holds for every $k\geq 0$.
\end{definition}

\textbf{Forward CKF:} Consider the forward CKF of Section~\ref{subsec:ICKF}. Define state prediction, state estimation and measurement prediction errors by $\widetilde{\mathbf{x}}_{k+1|k}\doteq\mathbf{x}_{k+1}-\hat{\mathbf{x}}_{k+1|k}$, $\widetilde{\mathbf{x}}_{k}\doteq\mathbf{x}_{k}-\hat{\mathbf{x}}_{k}$ and $\widetilde{\mathbf{y}}_{k+1}\doteq\mathbf{y}_{k+1}-\hat{\mathbf{y}}_{k+1|k}$, respectively. Following \cite{xiong2006performance_ukf,wanasinghe2015stability}, we represent the state and measurement prediction errors, respectively, as
\par\noindent\small
\begin{align}
    &\widetilde{\mathbf{x}}_{k+1|k}=\mathbf{U}^{x}_{k}\mathbf{F}_{k}\widetilde{\mathbf{x}}_{k}+\mathbf{w}_{k},\label{eqn:linearized x}\\
    &\widetilde{\mathbf{y}}_{k+1}=\mathbf{U}^{y}_{k+1}\mathbf{H}_{k+1}\widetilde{\mathbf{x}}_{k+1|k}+\mathbf{v}_{k+1},\label{eqn:linearized y}
\end{align}
\normalsize
where the unknown instrumental diagonal matrices $\mathbf{U}^{x}_{k}\in\mathbb{R}^{n_{x}\times n_{x}}$ and $\mathbf{U}^{y}_{k}\in\mathbb{R}^{n_{y}\times n_{y}}$ account for the linearization errors in $f(\cdot)$ and $h(\cdot)$, respectively. Here, $\mathbf{F}_{k}\doteq\frac{\partial f(\mathbf{x})}{\partial\mathbf{x}}\vert_{\mathbf{x}=\hat{\mathbf{x}}_{k}}$ and $\mathbf{H}_{k+1}\doteq\frac{\partial h(\mathbf{x})}{\partial\mathbf{x}}\vert_{\mathbf{x}=\hat{\mathbf{x}}_{k+1|k}}$. Finally, using \eqref{eqn:forward ckf x update}, we get $\widetilde{\mathbf{x}}_{k}=\widetilde{\mathbf{x}}_{k|k-1}-\mathbf{K}_{k}\widetilde{\mathbf{y}}_{k}$. Here, substituting \eqref{eqn:linearized x} and \eqref{eqn:linearized y} yields the forward CKF's prediction error dynamics as
\par\noindent\small
\begin{align}
    \widetilde{\mathbf{x}}_{k+1|k}=\mathbf{U}^{x}_{k}\mathbf{F}_{k}(\mathbf{I}-\mathbf{K}_{k}\mathbf{U}^{y}_{k}\mathbf{H}_{k})\widetilde{\mathbf{x}}_{k|k-1}-\mathbf{U}^{x}_{k}\mathbf{F}_{k}\mathbf{K}_{k}\mathbf{v}_{k}+\mathbf{w}_{k}.\label{eqn:forward ckf error dynamics}
\end{align}
\normalsize

The true state and measurement prediction error covariances are  $\mathbf{P}_{k+1|k}=\mathbb{E}\left[\widetilde{\mathbf{x}}_{k+1|k}\widetilde{\mathbf{x}}_{k+1|k}^{T}\right]$ and $\mathbf{P}^{y}_{k+1}=\mathbb{E}\left[\widetilde{\mathbf{y}}_{k+1}\widetilde{\mathbf{y}}_{k+1}^{T}\right]$, respectively. Define $\delta\mathbf{P}_{k+1|k}\doteq\bm{\Sigma}_{k+1|k}-\mathbf{P}_{k+1|k}$ and $\delta\mathbf{P}^{y}_{k+1}\doteq\bm{\Sigma}^{y}_{k+1}-\mathbf{P}^{y}_{k+1}$. Following \cite{xiong2006performance_ukf,wanasinghe2015stability}, we get
\par\noindent\small
\begin{align*}
    &\bm{\Sigma}_{k+1|k}\\
    &=\mathbf{U}^{x}_{k}\mathbf{F}_{k}(\mathbf{I}-\mathbf{K}_{k}\mathbf{U}^{y}_{k}\mathbf{H}_{k})\bm{\Sigma}_{k|k-1}(\mathbf{I}-\mathbf{K}_{k}\mathbf{U}^{y}_{k}\mathbf{H}_{k})^{T}\mathbf{F}_{k}^{T}\mathbf{U}^{x}_{k}+\hat{\mathbf{Q}}_{k},\\
    &\bm{\Sigma}^{y}_{k+1}=\mathbf{U}^{y}_{k+1}\mathbf{H}_{k+1}\bm{\Sigma}_{k+1|k}\mathbf{H}_{k+1}^{T}\mathbf{U}^{y}_{k+1}+\hat{\mathbf{R}}_{k+1},\\
    &\bm{\Sigma}^{xy}_{k+1}=\begin{cases}\bm{\Sigma}_{k+1|k}\mathbf{U}^{xy}_{k+1}\mathbf{H}_{k+1}^{T}\mathbf{U}^{y}_{k+1}, & n_{x}\geq n_{y}\\
    \bm{\Sigma}_{k+1|k}\mathbf{H}_{k+1}^{T}\mathbf{U}^{y}_{k+1}\mathbf{U}^{xy}_{k+1}, & n_{x}<n_{y}\end{cases}.
\end{align*}
\normalsize
Here, $\hat{\mathbf{Q}}_{k}=\mathbf{Q}_{k}+\mathbf{U}^{x}_{k}\mathbf{F}_{k}\mathbf{K}_{k}\mathbf{R}_{k}\mathbf{K}_{k}^{T}\mathbf{F}_{k}^{T}\mathbf{U}^{x}_{k}+\delta\mathbf{P}_{k+1|k}+\Delta\mathbf{P}_{k+1|k}$ and $\hat{\mathbf{R}}_{k+1}=\mathbf{R}_{k+1}+\Delta\mathbf{P}^{y}_{k+1}+\delta\mathbf{P}^{y}_{k+1}$ with $\Delta\mathbf{P}_{k+1|k}$ and $\Delta\mathbf{P}^{y}_{k+1}$ acccounting for the errors in expectation approximations. The unknown matrix $\mathbf{U}^{xy}_{k+1}$ represents errors in the estimated cross-covariance $\bm{\Sigma}^{xy}_{k+1}$.

\begin{proposition}
\label{prop:forward CKF}
Consider the system \eqref{eqn:state transition x} and \eqref{eqn:observation y} with the forward CKF. The forward CKF's estimation error $\widetilde{\mathbf{x}}_{k}$ is exponentially bounded in the mean-squared sense and bounded with probability one if the following conditions hold true.\\
\textbf{C1.} There exist positive real numbers $\bar{f}$, $\bar{h}$, $\bar{\alpha}$, $\bar{\beta}$, $\bar{\gamma}$, $\underline{\sigma}$, $\bar{\sigma}$, $\bar{q}$, $\bar{r}$, $\hat{q}$ and $\hat{r}$ such that for all $k\geq 0$,
\par\noindent\small
\begin{align*}    &\|\mathbf{F}_{k}\|\leq\bar{f},\;\;\;\|\mathbf{H}_{k}\|\leq\bar{h},\;\;\;\|\mathbf{U}^{x}_{k}\|\leq\bar{\alpha},\;\;\;\|\mathbf{U}^{y}_{k}\|\leq\bar{\beta},\;\;\;\|\mathbf{U}^{xy}_{k}\|\leq\bar{\gamma},\\        &\mathbf{Q}_{k}\preceq\bar{q}\mathbf{I},\;\;\;\mathbf{R}_{k}\preceq\bar{r}\mathbf{I},\;\;\;\hat{q}\mathbf{I}\preceq\hat{\mathbf{Q}}_{k},\;\;\;\hat{r}\mathbf{I}\preceq\hat{\mathbf{R}}_{k},\;\;\;\underline{\sigma}\mathbf{I}\preceq\bm{\Sigma}_{k|k-1}\preceq\bar{\sigma}\mathbf{I}.
\end{align*}
\normalsize
\textbf{C2.} $\mathbf{U}^{x}_{k}$ and $\mathbf{F}_{k}$ are non-singular for every $k\geq 0$.\\
\textbf{C3.} The constants satisfy the inequality $\bar{\sigma}\bar{\gamma}\bar{h}^{2}\bar{\beta}^{2}<\hat{r}$.
\end{proposition}
\begin{proof}
The proof follows the stability conditions provided in \cite{wanasinghe2015stability}, which are the same as the bounds in \textbf{C1}. %of Proposition~\ref{prop:forward CKF} 
%are the same as the stability conditions provided in \cite{wanasinghe2015stability} for CKF stability. 
However, the proof in \cite{wanasinghe2015stability} uses invertibility of $\mathbf{U}^{x}_{k}\mathbf{F}_{k}(\mathbf{I}-\mathbf{K}_{k}\mathbf{H}_{k})$ for all $k\geq 1$, which may not be true in general. In Proposition~\ref{prop:forward CKF}, similar to \cite[Theorem 2]{singh2022inverse_part1}, the inequality in \textbf{C3} guarantees $(\mathbf{I}-\mathbf{K}_{k}\mathbf{H}_{k})$ to be invertible for all $k\geq 1$. This, in turn, ensures invertibility of $\mathbf{U}^{x}_{k}\mathbf{F}_{k}(\mathbf{I}-\mathbf{K}_{k}\mathbf{H}_{k})$ under \textbf{C2}. %Note that the filter's stability is enhanced by enlarging noise covariances $\mathbf{Q}_{k}$ and $\mathbf{R}_{k}$ such that $\hat{\mathbf{Q}}_{k}$ and $\hat{\mathbf{R}}_{k}$ become positive definite\cite{xiong2006performance_ukf}.
\end{proof}

\textbf{I-CKF:} We show that I-CKF is stable if the forward CKF is stable as per Proposition~\ref{prop:forward CKF} under mild system conditions. Define $\widetilde{\mathbf{F}}_{k}\doteq\left.\frac{\partial\widetilde{f}(\mathbf{x},\bm{\Sigma}_{k},\mathbf{x}_{k+1},\mathbf{0})}{\partial\mathbf{x}}\right\vert_{\mathbf{x}=\doublehat{\mathbf{x}}_{k}}$ and $\mathbf{G}_{k}\doteq\left.\frac{\partial g(\mathbf{x})}{\partial\mathbf{x}}\right\vert_{\mathbf{x}=\doublehat{\mathbf{x}}_{k|k-1}}$. Denote $\overline{\mathbf{U}}^{x}_{k}$, $\overline{\mathbf{U}}^{a}_{k}$ and $\overline{\mathbf{U}}^{xa}_{k}$ as the unknown matrices introduced to account for errors in linearizing the functions $\widetilde{f}(\cdot)$, $g(\cdot)$ and the I-CKF's cross-covariance matrix estimation, respectively. Further, $\hat{\overline{\mathbf{Q}}}_{k}$ and $\hat{\overline{\mathbf{R}}}_{k}$ are counterparts of $\hat{\mathbf{Q}}_{k}$ and $\hat{\mathbf{R}}_{k}$, respectively, in the I-CKF's error dynamics.

\begin{theorem}
\label{thm:I-CKF}
Assume a stable forward CKF as per Proposition~\ref{prop:forward CKF}. The I-CKF's state estimation error is exponentially bounded in mean-squared sense and bounded with probability one if the following conditions hold true.\\
 \textbf{C4.} There exist positive real numbers $\bar{g},\bar{c},\bar{d},\bar{\epsilon},\hat{c},\hat{d},\underline{p}$ and $\bar{p}$ such that for all $k\geq 0$,
 \par\noindent\small
\begin{align*}      &\|\mathbf{G}_{k}\|\leq\bar{g},\;\;\|\overline{\mathbf{U}}^{a}_{k}\|\leq\bar{c},\;\;\|\overline{\mathbf{U}}^{xa}_{k}\|\leq\bar{d},\;\;\overline{\mathbf{R}}_{k}\preceq\bar{\epsilon}\mathbf{I},\;\;\hat{c}\mathbf{I}\preceq\hat{\overline{\mathbf{Q}}}_{k},\\
&\hat{d}\mathbf{I}\preceq\hat{\overline{R}}_{k},\;\;\underline{p}\mathbf{I}\preceq\overline{\bm{\Sigma}}_{k|k-1}\preceq\bar{p}\mathbf{I}.
\end{align*}
\normalsize
\textbf{C5.} There exist a real constant $\underline{y}$ (not necessarily positive) such that $\bm{\Sigma}^{y}_{k}\succeq\underline{y}\mathbf{I}$ for all $k\geq 0$.\\
\textbf{C6.} The functions $f(\cdot)$ and $h(\cdot)$ are bounded as $\|f(\cdot)\|_{2}\leq\delta_{f}$ and $\|h(\cdot)\|_{2}\leq\delta_{h}$ for some real positive numbers $\delta_{f}$ and $\delta_{h}$.\\
\textbf{C7.} For all $k\geq 0$, $\widetilde{\mathbf{F}}_{k}$ is non-singular and satisfies $\|\widetilde{\mathbf{F}}^{-1}_{k}\|\leq\bar{a}$ for some positive real constant $\bar{a}$.\\
\textbf{C8.} The constants satisfy the inequality $\bar{p}\bar{d}\bar{g}^{2}\bar{c}^{2}<\hat{d}$.
\end{theorem}
\begin{proof}
    The proof follows the steps detailed in our previous work on I-UKF in \cite[Appendix~2]{singh2022iukf}. In particular, under the assumptions of Theorem~\ref{thm:I-CKF}, the I-CKF's error dynamics can be shown to satisfy the stability conditions for a general CKF provided in Proposition~\ref{prop:forward CKF}. The additional lower bound $\bm{\Sigma}^{y}_{k}\succeq\underline{y}\mathbf{I}$ and bounded functions $f(\cdot)$ and $h(\cdot)$ ensure that the Jacobian $\widetilde{\mathbf{F}}_{k}$ is upper-bounded by a constant $c_{f}>0$ as $\|\widetilde{\mathbf{F}}_{k}\|\leq c_{f}$ for all $k\geq 0$. Further, using the unknown matrices from the forward CKF's error dynamics, we have $\overline{\mathbf{U}}^{x}_{k}=(\mathbf{I}-\mathbf{K}_{k+1}\mathbf{U}^{y}_{k+1}\mathbf{H}_{k+1})\mathbf{U}^{x}_{k}\mathbf{F}_{k}\widetilde{\mathbf{F}}_{k}^{-1}$. Hence, the non-singularity of $\widetilde{\mathbf{F}}_{k}$ leads to $\overline{\mathbf{U}}^{x}_{k}$ being invertible because $\mathbf{F}_{k}$, $\mathbf{U}^{x}_{k}$ and $(\mathbf{I}-\mathbf{K}_{k+1}\mathbf{U}^{y}_{k+1}\mathbf{H}_{k+1})$ are invertible under \textbf{C1}-\textbf{C3} of Proposition~\ref{prop:forward CKF}. The bound $\|\widetilde{\mathbf{F}}_{k}^{-1}\|\leq\overline{a}$ provides an upper-bound $\|\overline{\mathbf{U}}^{x}_{k}\|\leq\bar{\alpha}\bar{f}\bar{a}(1+\bar{k}\bar{\beta}\bar{h})$ with $\bar{k}=\bar{\sigma}\bar{\gamma}\bar{h}\bar{\beta}/\hat{r}$. All other conditions for CKF stability trivially hold true for the I-CKF's error dynamics under the \textbf{C4}-\textbf{C8}.% assumptions of Theorem~\ref{thm:I-CKF}.
\end{proof}

\section{Numerical experiments}
\label{sec:numericals}
We consider two different example systems widely used to analyze CKF and QKF performances and compare the I-CKF's and I-QKF's accuracy with I-UKF\cite{singh2022iukf}. We further consider the RCRLB\cite{tichavsky1998posterior} for the state estimation error as the theoretical benchmark. Denote the state vector series as $X^{k}=\lbrace\mathbf{x}_{0},\mathbf{x}_{1},\hdots,\mathbf{x}_{k}\rbrace$ and the observations as $Y^{k}=\lbrace\mathbf{y}_{0},\mathbf{y}_{1},\hdots,\mathbf{y}_{k}\rbrace$ with $p(Y^{k},X^{k})$ as the joint probability density of pair $(Y^{k},X^{k})$. The RCRLB provides a lower bound on mean-squared error (MSE) for the discrete-time non-linear filtering and is defined as $\mathbb{E}\left[(\mathbf{x}_{k}-\hat{\mathbf{x}}_{k})(\mathbf{x}_{k}-\hat{\mathbf{x}}_{k})^{T}\right]\succeq\mathbf{J}_{k}^{-1}$. Here, $\mathbf{J}_{k}=\mathbb{E}\left[-\frac{\partial^{2}\ln{p(Y^{k},X^{k})}}{\partial\mathbf{x}_{k}^{2}}\right]$ is the Fisher information matrix with $\frac{\partial^{2}(\cdot)}{\partial\mathbf{x}^{2}}$ as the Hessian with second order partial derivatives and $\hat{\mathbf{x}}_{k}$ is an estimate of $\mathbf{x}_{k}$. For the non-linear system given by \eqref{eqn:state transition x} and \eqref{eqn:observation y}, the forward information matrices $\lbrace\mathbf{J}_{k}\rbrace$ recursions can be computed recursively as $\mathbf{J}_{k+1}=\mathbf{H}_{k+1}^{T}\mathbf{R}_{k+1}^{-1}\mathbf{H}_{k+1}-\mathbf{Q}_{k}^{-1}\mathbf{F}_{k}(\mathbf{J}_{k}+\mathbf{F}_{k}^{T}\mathbf{Q}_{k}^{-1}\mathbf{F}_{k})^{-1}\mathbf{F}_{k}^{T}\mathbf{Q}_{k}^{-1}+\mathbf{Q}_{k}^{-1}$ \cite{xiong2006performance_ukf}, where $\mathbf{F}_{k}=\frac{\partial f(\mathbf{x})}{\partial\mathbf{x}}\vert_{\mathbf{x}=\mathbf{x}_{k}}$ and $\mathbf{H}_{k}=\frac{\partial h(\mathbf{x})}{\partial\mathbf{x}}\vert_{\mathbf{x}=\mathbf{x}_{k}}$. The information matrix recursions can be trivially modified to obtain the inverse filter's information matrix $\overline{\mathbf{J}}_{k}$. %Throughout all experiments, we compute the Jacobians of the state transition with respect to the state in the RCRLB computations were obtained using central difference numerical approximation with step size $1$.
%Posterior CRLB\cite{bell2015cognitive} has also been considered as a metric to tune tracking filters for cognitive radar target problems.
\\
\noindent\textbf{I-CKF for target tracking:} 
%\label{subsec: target tracking}
Consider a target maneuvering at an unknown constant turn rate $\Omega$ in a horizontal plane with a fixed radar tracking its trajectory with range and bearing measurements using CKF\cite{arasaratnam2009cubature}. Denote the target's state at $k$-th time instant as $\mathbf{x}_{k}=[p^{x}_{k},v^{x}_{k},p^{y}_{k},v^{y}_{k},\Omega]^{T}$ where $p^{x}_{k}$ and $p^{y}_{k}$ are the positions, and $v^{x}_{k}$ and $v^{y}_{k}$ are the velocities in $x$ and $y$ directions, respectively. The non-linear system model is $\mathbf{x}_{k+1}=\mathbf{F}\mathbf{x}_{k}+\mathbf{w}_{k}$ with observation $\mathbf{y}_{k}=\begin{bsmallmatrix}
        \sqrt{(p^{x}_{k})^{2}+(p^{y}_{k})^{2}}\\
        \tan^{-1}(p^{y}_{k}/p^{x}_{k})
    \end{bsmallmatrix}+\mathbf{v}_{k}$\cite{bar2004estimation}. Here, \small$\mathbf{F}=\begin{bsmallmatrix}
        1&\sin{(\Omega T)}/\Omega& 0 &-(1-\cos{(\Omega T)})/\Omega & 0\\
        0&\cos{(\Omega T)}& 0 &-\sin{(\Omega T)}& 0\\
        0&(1-\cos{(\Omega T)})/\Omega& 1 &\sin{(\Omega T)}/\Omega& 0\\
        0&\sin{(\Omega T)}& 0 &\cos{(\Omega T)}& 0\\
        0& 0& 0& 0& 1
    \end{bsmallmatrix}.$ \normalsize
Similar to forward filter's observations, we consider $\mathbf{a}_{k}=\begin{bmatrix}
        \sqrt{(\hat{p}^{x}_{k})^{2}+(\hat{p}^{y}_{k})^{2}}\\
        \tan^{-1}(\hat{p}^{y}_{k}/\hat{p}^{x}_{k})
    \end{bmatrix}+\bm{\epsilon}_{k}$, where $\hat{p}^{x}_{k}$ and $\hat{p}^{y}_{k}$ are the forward filter's estimates of $p^{x}_{k}$ and $p^{y}_{k}$, respectively. The noise terms $\mathbf{w}_{k}\sim\mathcal{N}(\mathbf{0},\mathbf{Q})$, $\mathbf{v}_{k}\sim\mathcal{N}(\mathbf{0},\mathbf{R})$ and $\bm{\epsilon}_{k}\sim\mathcal{N}(\mathbf{0},\bm{\Sigma}_{\epsilon})$, with $\bm{\Sigma}_{\epsilon}=\mathbf{R}$. %All other parameters, including the initial estimates and noise covariance matrices, were identical to \cite{arasaratnam2009cubature}. 
    We compared the I-CKF's performance with I-UKF and set both forward UKF's $\kappa$ and I-UKF's $\overline{\kappa}$ to $1$. %For RCRLB computations, the initial information matrix was chosen close to the inverse of the corresponding filter's steady state covariance matrix to avoid the transient behaviour\cite{singh2022inverse_part1}.

Fig.~\ref{fig:tracking lorenz}a shows the (root) time-averaged error in velocity estimation and its RCRLB (also, time-averaged) for a system that employs forward and inverse CKF, hereafter labeled \textit{ICKF-C system}. %, including forward UKF. 
We define the \textit{ICKF-U system} as the one %represents the I-CKF's estimation error when 
wherein the defender employs I-CKF assuming a forward CKF when the attacker's true forward filter is UKF. The RCRLB is computed as $\sqrt{[\mathbf{J}^{-1}]_{2,2}+[\mathbf{J}^{-1}]_{4,4}}$, where $\mathbf{J}$ is the corresponding information matrix. We observe that forward CKF and UKF have similar estimation accuracy. Hence, both ICKF-C and ICKF-U yield similar estimation errors regardless of the actual forward filter. Although the forward and inverse filters have similar RCRLBs, the difference between the estimation error and RCRLB for I-CKF is less than that for the forward CKF. Hence, I-CKF outperforms forward CKF in terms of achieving the lower bound. For the considered system, the I-UKF's error was similar to I-CKF and hence, omitted in Fig.~\ref{fig:tracking lorenz}a.
\\
\noindent\textbf{I-QKF for Lorenz system:} 
\label{subsec: lorenz system}
Consider the following $3$-dimensional Lorenz system that is derived from the Lorenz stochastic differential system \cite{ito2000gaussian}: \par\noindent\small
\begin{align*}
&\mathbf{x}_{k+1}=\begin{bsmallmatrix}
    [\mathbf{x}_{k}]_{1}+\Delta tr_{1}(-[\mathbf{x}_{k}]_{1}+[\mathbf{x}_{k}]_{2})\\
    [\mathbf{x}_{k}]_{2}+\Delta t(r_{2}[\mathbf{x}_{k}]_{1}-[\mathbf{x}_{k}]_{2}-[\mathbf{x}_{k}]_{1}[\mathbf{x}_{k}]_{3})\\
    [\mathbf{x}_{k}]_{3}+\Delta t(-r_{3}[\mathbf{x}_{k}]_{3}+[\mathbf{x}_{k}]_{1}[\mathbf{x}_{k}]_{2})
\end{bsmallmatrix}+\begin{bsmallmatrix}
    0\\0\\0.5
\end{bsmallmatrix}w_{k},\\
& y_{k}=\Delta t\sqrt{([\mathbf{x}_{k}]_{1}-0.5)^{2}+[\mathbf{x}_{k}]_{2}^{2}+[\mathbf{x}_{k}]_{3}^{2}}+0.065v_{k},\\
& a_{k}=\Delta t\sqrt{[\mathbf{x}_{k}]_{1}^{2}+([\mathbf{x}_{k}]_{2}-0.5)^{2}+[\mathbf{x}_{k}]_{3}^{2}}+0.1\epsilon_{k},
\end{align*}
\normalsize
where $w_{k}, v_{k}, \epsilon_{k}\sim\mathcal{N}(0,\Delta t)$ with parameters $\Delta t=0.01$, $r_{1}=10$, $r_{2}=28$ and $r_{3}=8/3$ such that the system has three unstable equilibria. %Similar to Section~\ref{subsec: target tracking}, we consider 
The observations $\mathbf{a}_{k}$ are of the same form as $\mathbf{y}_{k}$. The attacker employs a $5$-point forward QKF while the defender considers a $3$-point I-QKF assuming the forward QKF's $m=3$. The I-QKF's accuracy is compared with I-UKF with $\overline{\kappa}=2$ and a forward UKF with $\kappa=1.5$. The initial state $\mathbf{x}_{0}$ and the inverse filters' estimate $\doublehat{\mathbf{x}}_{0}$ were all set to $[-0.2,-0.3,-0.5]^{T}$. The forward filters' estimate $\hat{\mathbf{x}}_{0}$ were chosen as $[1.35,-3,6]^{T}$. All the initial covariance estimate were set to $0.35\mathbf{I}$ with $\mathbf{J}_{0}=\bm{\Sigma}_{0}^{-1}$ and $\overline{\mathbf{J}}_{0}=\overline{\bm{\Sigma}}_{0}^{-1}$.

Fig.~\ref{fig:tracking lorenz}b shows the time-averaged root MSE (RMSE) and RCRLB for state estimation for forward and inverse QKF and UKF. The RCRLB for state estimation is $\sqrt{\textrm{Tr}(\mathbf{J}^{-1})}$, where $\mathbf{J}$ is the corresponding information matrix. Here, the systems with incorrect forward filters are \textit{IUKF-Q} and \textit{IQKF-U} that are defined as using, respectively, QKF and UKF forward filters while the inverse is the vice versa. % case in Section~\ref{subsec: target tracking}. 
For the Lorenz system, forward QKF estimates the state more accurately than forward UKF. Hence, we observe that with the correct forward filter assumption, I-QKF (IQKF-Q case) has a lower error than forward QKF as well as I-UKF (IUKF-U case). On the other hand, IUKF-U has a similar performance as forward UKF. Even with an incorrect forward filter assumption, I-QKF (IQKF-U case) has a lower error than IUKF-U. Contrarily, IUKF-Q closely follows the corresponding forward QKF's performance. Interestingly, for the considered system, the I-UKF's RCRLB is the same as that for the forward filters, which is slightly less than that for I-QKF. In spite of this, I-QKF has a higher estimation accuracy than I-UKF as well as the forward filters.

%---------------------------------------------------------------------
\begin{figure}
  \centering
  \includegraphics[width = 1.0\columnwidth]{tracking_lorenz.png}
  \caption{Time-averaged estimation error %and RCRLB 
  of forward and inverse (a) CKF for target tracking system (averaged over $250$ runs) and (b) QKF for Lorenz system, compared with forward and inverse UKF (averaged over $50$ runs). \vspace{-10pt} }
 \label{fig:tracking lorenz}
\end{figure}
%-----------------------------------------------------

\section{Summary}
\label{sec:summary}
We developed I-CKF and I-QKF to estimate the defender's state given noisy measurements of the attacker's actions in highly non-linear systems. The proposed I-CKF and I-QKF, respectively, exploit the cubature and quadrature rules for numerical integration to approximate the recursive Bayesian integrals. The stability conditions for I-CKF are easily achieved for a stable forward CKF. %estimation error remains bounded if the forward CKF is stable with certain additional assumptions on the system. Our 
Numerical experiments show that I-CKF and I-QKF outperform I-UKF even when they incorrectly assume the true form of the forward filter. The non-trivial upshot of this result is that forward filter need not known exactly to the defender.

%\balance
\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}