
\section{Proofs}
\label{proof:section}
\subsection{Proof of \cref{lemma:characterization}}

\subsubsection{VICReg Loss and Spectral Contrastive}
\label{proof:vic-2}

This subsection will both identify the VICReg with the spectral contrastive one through their matrix formulation.

Let us begin by reformulating the invariance term in VICReg.
For $\mZ$ defined in \eqref{eq:Z}, it is generalized to multiple pairs through
\begin{align*}
    \gL_{\rm VIC-INV} 
    &= \sum_{i,j\in[N]} \mG_{ij} \norm{\vz_i - \vz_j}^2
    = \sum_{i,j\in[N]} 2 \mG_{ij} \norm{\vz_i}^2 - 2\mG_{ij} \scap{\vz_i}{\vz_j}   
    \\& = \sum_{i,j\in[N]} 2 \mG_{ij} [\mZ \mZ^\top]_{ii} - 2\mG_{ij} [\mZ\mZ^\top]_{ji}   
    = \sum_{i\in[N]} 2[\mD \mZ \mZ^\top]_{ii} - 2[\mG \mZ\mZ^\top]_{ii}   
    \\&= 2\Tr\paren{(\mD-\mG) \mZ\mZ^\top} = 2\Tr\paren{\mZ^\top (\mD-\mG) \mZ}   
\end{align*}
where $\mD$ is the degree matrix defined as a diagonal matrix, with $A$ the number of augmented samples per original input
\[
    \mD_{ij} = \ind{i=j}\cdot \sum_{k\in[N]} \mG_{ik} = A\ind{i=j}.
\]

The variance-covariance term can be simplified by replacing the the Hinge loss for the variance by a squared norm \cite{kiani2022joint}, by setting $\beta = \alpha = 1$ and replacing $A$ by $N$ in $\mD$ to regularize diagonal terms a bit more.
Those simplifications lead to a more principled regularization term that enforces orthogonality over the dataset of the different features learned by the network $f_\theta$ \cite{cabannes2023minimal}.
The consequent regularization reads $\norm{\mZ^\top \mZ/N - \mI_K}^2$.
As a consequence, VICReg can be understood as solving for
\[
    N\gL_{\rm VIC} \approx \norm{\mZ^\top \mZ - N \mI_N}^2 + 2\trace(\mZ^\top (N\mI_N -\mG)\mZ).
\]

For the spectral contrastive Loss, it is useful to incorporate negative pairs that are sampled for the same augmentations for two different samples $\scap{\mZ_i^{(1)}}{\mZ_j^{(1)}}$ in the repulsive term.
Moreover adding $\norm{\mZ_i^{(v)}}$ on both the positive part and the negative part will not change much since $-2x + x^2$ is minimized for $x=1$.
Those modifications lead to 
\begin{align*}
    \gL_{\rm VIC^2} &= - 2\sum_{i,j\in[N]} \mG_{ij} \vz_i^\top \vz_j + \sum_{i,j\in[N]} (\vz_i^\top \vz_j)^2
    = - 2\sum_{i,j\in[N]} \mG_{ij} [\mZ\mZ^\top]_{ij} + \sum_{i,j\in[N]} [\mZ \mZ^\top]_{ij}^2
    \\&= - 2\Tr(\mG \mZ\mZ^\top) + \Tr(\mZ \mZ^\top \mZ \mZ^\top)
     = \Tr(\mZ \mZ^\top \mZ \mZ^\top - 2\mG \mZ\mZ^\top + \mG^2) - \Tr(\mG^2)
     \\&= \Tr((\mZ \mZ^\top - \mG)^2) - \Tr(\mG^2)
     = \norm{\mZ \mZ^\top - \mG}^2_F + \text{cst}.
\end{align*}
The last term being a finite constant, it can be removed from the loss.

Indeed, one can relate both the spectral contrastive loss and VICReg, by remarking that
\begin{align*}
    &\norm{\mZ^\top \mZ - N \mI_N}^2 + 2\trace(\mZ^\top (N\mI_N -\mG)\mZ)
    \\&\qquad= \trace(\mZ\mZ^\top \mZ\mZ^\top - 2N\mI_N \mZ^\top\mZ +  N^2 \mI_N)  + 2\trace(\mZ^\top (N\mI_N -\mG)\mZ)
    \\&\qquad= \trace(\mZ\mZ^\top \mZ\mZ^\top - 2\mG \mZ \mZ^\top) + N^3
    = \trace((\mZ\mZ^\top - \mG)^2) - \mG^2) + N^3
    \\&\qquad= \norm{\mZ\mZ^\top - \mG}_F^2 + \text{cst}.
\end{align*}

Finally, the variance covariance term can be written as
\begin{align*}
    \norm{\mZ\mZ^\top/ N - I}^2 
    &= \norm{\sum_{i\in[N]}\vz_i \vz_i^\top - I}^2
    = \trace\paren{\sum_{i,j\in[N]} \vz_j\vz_j^\top\vz_i \vz_i^\top - 2\sum_{i\in[N]} \vz_i\vz_i^\top + I}
    \\&= \sum_{i,j\in[N]} (\vz_j^\top\vz_i)^2 - \sum_{i,j\in[N]} (\vz_i^\top\vz_i  + \vz_j^\top\vz_j) + \text{cst}
    = \sum_{i,j\in[N]} R(\vz_i, \vz_j) + \text{cst},
\end{align*}
where $R(a, b) = (a^\top b)^\top - \norm{a}^2 - \norm{b}^2$.

\subsubsection{The SimCLR Loss}
SimCLR can be seen as a generalized linear model, where two variables $A$, $B$ are observed and the probability observing $B$ knowing $A$ is given by
\[
    p_{ij} = \Pbb(B=j\,\vert\, A=i) \propto \exp\paren{\frac{\vz_i^\top \vz_j}{\norm{\vz_i}\norm{\vz_j}}}.
\]
For simplicity, let us define $\tilde\vz = \vz / \norm{\vz}$.
SimCLR tries to maximize the likelihood of $(A, B)$ denoting random pairs coming from the same augmentations based on the observation of the graph
\[
    \prod_{ij\in[N]} p_{ij}^{\mG_{ij}} = \exp\paren{\sum_{ij\in[N]} \mG_{ij} \log\paren{\frac{\exp\paren{\tilde\vz_i^\top \tilde\vz_j}}{\sum_{k\in[N]} \exp(\tilde\vz_i^\top \tilde\vz_k)}}}.
\]
The SimCLR loss is nothing but the inverse of the log likelihood.
\[
    \gL_{\rm SimCLR} = -\sum_{ij\in[N]} \mG_{ij} \log\paren{\frac{\exp\paren{\tilde\vz_i^\top \tilde\vz_j}}{\sum_{k\in[N]} \exp(\tilde\vz_i^\top \tilde\vz_k)}}.
\]

\subsubsection{Barlow Twins}
When $\lambda = 1$, which we will consider for simplicity, the BarlowTwins loss simplifies as
\[
    \gL_{\rm BT} = \sum_{i} (1-\mC_{ii})^2 + \sum_{i\neq j} \mC_{ij}^2 = \norm{\mC - \mI_k}_F^2.
\]
Because cross-correlations are normalized cross-covariances, it is useful to introduce $\tilde\mZ$ the column normalized version of $\mZ$. Formally written in normalized matrix with the Hadamard product notation as
\[
    \tilde\mZ_{ij} = \frac{z_{ij}}{\sqrt{\sum_{k} z_{kj}^2} }
    = \frac{z_{ij}}{[(\mZ\otimes\mZ)\1]_j^{1/2}}
    \qquad\text{i.e.}\qquad
    \tilde\mZ = \mZ \diag(\mZ^{\otimes 2} \1)^{-1/2}
\]
The way the cross-correlation is built can be generalized to multiple positive pairs as
\[
    \mC_{ij} = \frac{\sum_{kl} \mG_{kl} z_{ki} z_{lj}}{\sqrt{\sum_{k} z_{ki}^2} \sqrt{\sum_{l} z_{lj}^2}}
    =\sum_{kl} \mG_{kl} \tilde{z}_{ki} \tilde{z}_{lj}
    = [\tilde{\mZ}^\top \mG \tilde\mZ]_{ij}.
\]
As a consequence, the BarlowTwins loss can be rewritten with the sole use of $\mG$ as
\[
    \gL_{\rm BT} = \norm{\tilde{\mZ}^\top\mG \mZ - \mI_K}^2_F.
\]

\subsection{The SSL Losses for Supervised Learning}

This subsection is devoted to the proof of \cref{thm:recovery}.

\subsubsection{Recovery Lemma}
\label{app:recovery}

The backbone of \cref{thm:recovery} is following Lemma.

\begin{lemma}[Equivalence between $\mY$ and $\mG$]
\label{lemma:recovery}
Given any supervised classification similarity matrix $\mG = \mY\mY^\top$ \eqref{eq:G_sup_easy}, one can recover the corresponding one-hot label encoding $\mY$, up to an orthogonal transformation $\mR$, as
\[
    \exists\mathop \mR \in O(C), \quad \text{ s.t.}\quad \mY = \mP\sqrt{\mD}\mR,
\]
where $\mP\mD\mP^T$ is the eigenvalue decomposition of the adjacency matrix $\mG(\mY)$.
Moreover the rotation $\mR$ is easily recovered by specifying the labels of $C$ samples associated with each of the $C$ different classes.
\end{lemma}
\begin{proof}
    \Cref{lemma:recovery} follows from the fact that $\mG = \mY \mY^\top$ so that $\mY$ is a square root of $\mG$, and that any two square roots of a matrix are isometric.
    In particular, if the SVD of $\mY$ is written as
    \[
        \mY = \mP \sqrt{\mD} \mR,
        \qquad \mP\in O(N), \mR \in O(C), \mD = \begin{bmatrix} \mD_1 & 0 \end{bmatrix} \in \R^{N\times C}, \mD_1 = \diag(\sigma_1^2, \cdots \sigma_C^2),
    \]
    the decomposition $\mG = PDP^\top$ is an eigenvalue decomposition of $\mG$. 
    The part $P\sqrt{D}$ is unique up to the application of a rotation on the right, which could be absorbed in $R$.

    In order to recover $\mY$ from $\mG$, notice that up to a permutation of lines and columns, $\mG$ has a block diagonal structure where each block corresponds to one label.
    If each one label is given to each block, this allows to retrieve exactly $\mY$ hence to identify $\mR$ afterwards by solving for $\mR = (\mP \sqrt{mD})^{-1} \mY$.
\end{proof}

While \cref{lemma:recovery} describes the classification case, in the generic case, if the $y$ are categorical, yet the loss $\ell(y, z)$ is not the zero-one loss, it is natural to define the similarity matrix as 
\begin{equation}
    \label{eq:gen_G_sup}
    \mG \triangleq (-\ell(y_i, y_j))_{i,j\in[N]} \in \R^{C\times C}.
\end{equation}
For example, $y_i$ could be rankings modeled with $y_i \in \Sfrak_m$ where $m! = C$, i.e. $m = \Gamma^{-1}(C) - 1$, and $\ell$ could be the Kendall loss.
In this setting,
\[
    \mG = \mY \mL\mY,
    \qquad\text{where}\qquad
    \mL \triangleq (-\ell(y, z))_{y,z\in[C]} \in \R^{C\times C},
\]
and $\mY$ is retrieved through $\mY = \mP\sqrt{\mD} \mR \mL^{1/2}$, where $\mP \mD \mP^\top$ is the eigenvalue decomposition of $\mG$, and $\mR \in \R^{C\times C}$ is an unknown rotation matrix, that might be identified by specifying at most $C$ labels associated with each of the $C$ different classes, but might be identified with a smaller number of samples if $\ell$ has a strong structure implying that $\mL$ is low-rank (see Eq. (11) in  \cite{Nowak2019}).
Indeed, the fact that compared to \eqref{eq:G_sup_easy}, the graph \eqref{eq:gen_G_sup} could be much lower rank, could lead to more efficient algorithm to image it in the active learning framework.
In essence, it would better leverage the structure encoded by the loss $\ell$.

Finally, in the regression setting, one can choose $\mG_{ij} = -y_i^\top y_j$.

\subsubsection{The VICReg Loss}
The VICReg loss is characterized as
\begin{align*}
    \gL_{\rm VIC-2} = \norm{\mZ\mZ^\top - \mG}^2_F + \text{cst}.
\end{align*}
So, it is minimized for $\mZ$ being a square root of the matrix $\mG$.
This is possible when the rank of $\mG$ which is at most $C$ since $\mG = \mY\mY^\top$ is less than the rank of $\mZ$ which is $K$.
In this setting, since $\mY$ and $\mZ$ are two square roots of $\mG$, we get
\begin{align*}
    \exists\mathop \mR \in O(C, K),\qquad \mZ = \mY \mR,
\end{align*}
where we define the rotation $O(C, K)$ as
\begin{equation}
    O(C, K) = \brace{\mR\in \R^{C\times K}\midvert \mR\mR^\top = \mI_C}.
\end{equation}

\subsubsection{The SimCLR Loss}
The probabilistic interpretation of SimCLR states that the SimCLR losses tries to maximize the likelihood of the events
\[
    \cup_{ij\in[N]} \brace{\mG_{ij}=1} \cap \brace{Y=i \,\,\&\,\, X=j},
\]
which translate as a loss in the minimization of 
\[
    \gL_{\rm SimCLR} = -\sum_{ij\in[N]} \mG_{ij} \log\paren{\frac{\exp\paren{\tilde\vz_i^\top \tilde\vz_j}}{\sum_{k\in[N]} \exp(\tilde\vz_i^\top \tilde\vz_k)}}.
\]
This is the cross entropy between $\mG_{ij}$ and $p_{ij}$ defined in the proof of the characterization of SimCLR.
If the minimization with respect to $p_{ij}$ was unconstrained, then one should match $p_{ij} \propto \mG_{ij}$.
Yet, the form of $p_{ij} \in [\exp(-1), \exp(1)]$ constraints it to go for a slightly different solution.

Remark that for two $\tilde\vz_i$, $\tilde\vz_j$ whose index $i$ and $j$ belongs to different clusters defined by the graph $\mG$, the loss is a increasing function of the quantity $\exp(\tilde\vz_i \tilde\vz_j)$.
By symmetry, we deduce that all the $\tilde\vz_i\tilde\vz_j = \cos(\vz_i, \vz_j)$ should be one for all $(i, j)$ such that $\mG_{ij}=1$.
On the other hand, the loss is a decreasing function of the $\exp(\tilde\vz_i^\top \tilde\vz_j)$ when $\mG_{ij} = 1$.
When the number of sample per class is constant, we deduce by symmetry that the different anchors for the different classes should be put at the extremity of the simplex with $C$ vertices centered at the origin and rotate with an arbitrary matrix $\mR \in O(C-1)$, which allow to recover the different classes (without their explicit labels if not provided).
When the different class have different number of samples $N_i$ with $\sum_{i\in[C]} N_i = N$, and their anchor in the output space is $\vc\in\R^K$, we are trying to minimize 
\begin{equation}
    \label{eq:anchor_simclr}
    \sum_{j\in[C]} N_j \log\paren{\sum_{i\in[C]} N_i \exp(\vc_i^\top\vc_j)},
\end{equation}
which will deform the simplex to have bigger angles between classes that are highly represented.
For example, when $N_1 = N_2 \approx N / 2$, we will have $\vc_1 \approx -\vc_2$ while the other anchors are orthogonal to one another and to $\vc_1$.
Denoting $\mM\in\R^{C}$ the matrix that maps the anchor of the class $i$ for one solution of \eqref{eq:anchor_simclr} to the $i$-th element of the canonical basis $\ve_i$ as $\vv_i \mM = \ve_i$, we get that the solution 
\[
    \mZ = \mD \mY \mR \mM^{-1}, \qquad\text{with}\qquad \mD \in \diag(\R_+^N); \mR \in O(K, C).
\]
The fact that $\mZ$ is invariant by scaling each vector $\vz$ reminds us of implementation of the cross-entropy, where to avoid divergence to infinity (since the sigmoid is optimized at infinity) one has to normalize the solution.
The SimCLR loss is actually built on the same generalized linear model as the cross-entropy, and one can roughly think of SimCLR as the SSL version of the cross-entropy.

\subsubsection{BarlowTwins}
To minimize the BarlowTwins loss
\[
    \gL_{\rm BT} = \norm{\tilde\mZ^\top \mG \tilde\mZ - I_K}_F^2,
\]
we want $\tilde\mZ$ to be a square root of the inverse of $\mG$.
To be more precise, introduce the eigenvalue decomposition of $\mG$ as $\mG = \mP \mS \mP^\top$ where $\mP \in \R^{N\times C}$ and $\mS \in \R^{C\times C}$ since $\mG$ is at most of rank $C$.
The minimizer of BarlowTwins is $\tilde\mZ = [\mP \mS^{-1/2}, 0_{N\times (K-C)}]$.
Since $\tilde\mZ$ is the column normalized version of $\mZ$, $\mZ$ can be reconstructed for any diagonal matrix $\mD \in \diag(\R_+^{K})$ as $\mZ = \tilde\mZ \mD$.
Incorporating $[\mS^{-1}, 0]$ in $\mD$, we get that the minimizer of the BarlowTwins loss are exactly the matrices $\mZ = \mP \mS^{1/2}[\mD, 0_{K-C}]$ for $\mD \in \diag{\R_+^C}$.
Moreover, since both $\mP \mS^{1/2}$ and $\mY$ are square root of $\mG$, we know that the exists a rotation matrix $\mR \in O(K)$ such that $\mP \mS^{1/2} = \mY \mR$. 
All together, we get that the minimizer of the BarlowTwins loss are exactly the
\[
    \mZ = \mY\mR\mD, \qquad\text{for}\quad \mR \in O^{C, K},\, \mD \in \diag(\R_+^C)
\]
The fact that BarlowTwins do not care about the amplitude of the solution $\mZ$ reminds us of discriminant analysis that learns classifiers by optimizing ratio and angles.

\subsection{Bayes optimum}
\label{app:bayes}

For completeness, we now state a Bayes optimum proposition regarding the VICReg loss of the paper.

\begin{proposition}[Bayes optimum]
    \label{prop:bayes}
    When $K \geq C$ and there is no context, i.e. $\vx_i = \vx_0$ for all $i\in [N]$, and $y_i$ are sampled according to a noisy distribution $\paren{y\midvert \vx=\vx_0}$, the naive study of the VICReg Bayes optimum is meaningless, since
    \begin{align*}
        \argmin_{\vz \in \R^K} \gL_{\rm VIC-2}(\begin{bmatrix} \vz\\ \vdots \\ \vz \end{bmatrix};\mG) =  \brace{\vz\in\R^K\midvert \norm{\vz} = 1}.
    \end{align*}
    Yet, if one free the variable $\mZ \in \R^{N\times K}$, we have
    \begin{align*}
        \argmin_{\mZ \in \R^{N\times K}} \gL_{\rm VIC-2}(\mZ;\mG) =  N^{1/2}\cdot\brace{(\Pbb(Y=i)^{1/2} e_i)_{i\in[C]}\cdot \mR \midvert \mR \in O(C, K)},
    \end{align*}
    where $(e_i)_{i\in[K]}$ is the canonical basis of $\R^K$.
\end{proposition}

\begin{proof}
    For the first part of the proof, remark that the invariance term in VICReg will be zero for any $\vz$, so VICReg loss is minimized for any vector that minimized the variance-covariance term $\norm{\vz\vz^\top - I}^2$, which is done for any unit vector.
    
    For the second part, remark that $\mG = \mY\mY^\top$ has $C$ connected components, that are all full cliques, i.e. the adjacency is filled with one.
    As a consequence, the eigenvectors of $\mG$ associated with non-zeros elements are exactly the $(\ind_{y_i = y})_{i\in[N]}$ for $y\in [C]$, and the corresponding eigenvalues are $N_y$ where $N_y = \sum_{i\in[N]} \ind{y_i = y} = N\Pbb(Y=i)$ are the number of element in the class $i\in[C]$.
    As a consequence, a square root of $\mG$ is $N^{1/2} (\Pbb(Y=i)^{1/2} \delta_{ij})_{i\in[C], j\in[K]}$, hence the proposition following the fact that all the square root of $\mG$ are isomorphic.
\end{proof}
