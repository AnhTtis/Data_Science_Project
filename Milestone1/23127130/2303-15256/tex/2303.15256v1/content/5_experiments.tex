\section{Experiments}
\label{sec:experiments}

This section provides experimental details to validate the various theoretical results derived in previous sections. In order to remove confounding effects linked with architecture, optimization, data curation and other design choices that might impact the different empirical validation we focus here on closed-form solution based on kernel methods with synthetic dataset.
Further real-world empirical validations are provided in \cref{app:expe}.
In particular, \cref{tab:nnclr} reminds the reader how NNCLR \cite{dwibedi2021little} succeed to beat state-of-the-art SSL methods on ImageNet thanks to an active labeler oracle, which defines positive pairs as nearest neighbors in the embedding space $f_{\theta_t}(\X)$.

\begin{table}[t]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         Modality & Oracle & \@1 accuracy & \@5 accuracy  \\
    \hline
         Passive & SimCLR \cite{simclrv2} & 71.7 & - \\
         Passive & VICReg \cite{bardes2021vicreg} & 73.2 & 91.1 \\
         {\bf Active} & NNCLR \cite{dwibedi2021little} & {\bf 75.6} & {\bf 92.4}\\ 
    \hline
    \end{tabular}
    \vspace{1em}
    \caption{Best known performance on ImageNet for state-of-the-art SSL methods.
    Notice how NNCLR \cite{dwibedi2021little} derives states of the art performance on ImageNet thanks to an active rule for labelers in \cref{alg:pal}, which consists in defining positive pairs as nearest neighbors in the embedding space as detailed in \cref{alg:nnclr}.
    This rule allows to beat the passive strategy that are provided by SimCLR and VICReg.
    }
    \label{tab:nnclr}
\end{table}

Kernel methods are rich ``linear'' parametric models defined as $f_\theta = \theta^\top \phi(\vx)$, for $\phi(\vx)$ and $\theta$ belonging to a separable Hilbert space ${\cal H}$.
Because those model can approximate any function \cite{Micchelli2006}, it is important to regularize $\theta$ in practice, either with early stopping in SGD, or with Tikhonov regularization, which can be written as $\lambda\trace(\mZ^\top \mK^{-1}\mZ)$ where $\lambda > 0$ is a regularization parameter and $\mK \in \R^{N\times N}$ is the kernel matrix defined as $\mK_{ij} = k(\vx_i, \vx_j) = \phi(\vx_i)^\top\phi(\vx_j)$.
In this setting, rather than matching the top of the spectral decomposition of $\mG$, the solution recovered by VICReg amounts to the top spectral decomposition of $\mG - \lambda \mK^{-1}$ \cite{cabannes2023ssl}.
This allows to compute the ideal representation of $f_\theta$ in closed-form given any graph $\mG$ based on the regularized kernel model $f_\theta = \theta^\top \phi(\vx)$, hence ablating the effects that are unrelated to the theory described in this study.
In this controlled setting, the superiority of active algorithms is undeniable, and illustrated on \cref{fig:active}, where we illustrate the optimal downstream error one can achieve with linear probing of the minimizer $f_\theta$ of the VICReg loss.
Experimental details and more extensive validations are provided in \cref{app:expe}: in particular, the use of non-contrastive versus contrastive graphs, i.e. that set $\mG_{ij} = -1$ when $y_i \neq y_j$, is studied on \cref{fig:contrastive}; the ability to incorporate label knowledge in SSL methods is the object of \cref{fig:interpolation}; robustness to noise is shown on \cref{fig:noise}; and relations between test error and the number of connected components of the reconstructed $\mG$ is analyzed on \cref{fig:comp}.

\begin{figure}[h]
    \centering
    \includegraphics{figures/interpolation.pdf}
    \caption{
    A major motivation of this paper is to be able to add prior information on sample relationships in SSL methods, and more in particular, to have a simple way to leverage known labels.
    We do by considering $\mY$ containing one-hot encoding of known labels, and rows being zero otherwise and the mixed graph
    \(
        \mG = (1-\alpha) \cdot \mG^{(\ssl)} + \alpha \cdot \hat\mY\hat\mY^\top.
    \)
    The setting is the same as \cref{fig:setup_toy} with $N=200$ and two augmentations per sample.
    When zero labels are known (left of the plot), we are in the full SSL regime, while when all the 200 labels are known (right of the plot), we recover supervised learning performance.
    When few labels are given the effect of the supervised graph can be counterproductive if the mixing coefficient $\alpha$ is too big.
    However, when mixed properly, adding prior label information in SSL methods allows to improve performance.
    }
    \label{fig:interpolation}
\end{figure}
