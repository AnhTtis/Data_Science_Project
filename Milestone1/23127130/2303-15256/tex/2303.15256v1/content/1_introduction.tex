
\begin{figure*}[t]
    \centering
\begin{adjustbox}{width=.95\textwidth}
\begin{tikzpicture}
\node[inner sep=0pt]  (title) at (0,-.25) {\textbf{Active Learning}};

\node[inner sep=0pt,below=0.5cm of title] (desc)  {
\parbox{6.2cm}{\small\em Given an input, return its label e.g. for Imagenet: 
\texttt{\scriptsize\\- Tinca tinca\\- Carassius auratus\\- Carcharodon carcharias\\- \dots}}};

\node[inner sep=0pt, below left=2.5cm and -0.7cm of title] (q1title) {$\texttt{query}_{\texttt{1}}$};
\node[inner sep=0pt,below=0.1cm of q1title] (q1){\includegraphics[width=2cm,height=2cm]{figures/teaser/ILSVRC2012_val_00038928.jpg}};

\node[inner sep=0pt,below right=2.5cm and -0.7cm of title] (q2title)  {$\texttt{query}_{\texttt{2}}$};
\node[inner sep=0pt,below=.1cm of q2title] (q2) {\includegraphics[width=2cm,height=2cm]{figures/teaser/ILSVRC2012_val_00008373.jpg}};


\node[inner sep=0pt,below=0.3cm of q1] (option1)  {\parbox{2cm}{\centering$\texttt{response}_{\texttt{1}}$\\ aligator}};

\node[inner sep=0pt,below=0.3cm of q2] (option2)  {\parbox{2cm}{\centering$\texttt{response}_{\texttt{2}}$\\aligator}};

\node[inner sep=0pt,below=6cm of title,color=red] (comment)  {\parbox{7cm}{\centering
Expansive oracle (expert knowledge)}};
    
\draw[black,thick,rounded corners] ($(desc.north west)+(-0.2,1)$)  rectangle ($(comment.south east)+(-0.1,-0.1)$);

%%%%%%%%%%%%%%%%%%%%%%%%
\node[inner sep=0pt]  (title_PAL) at (9,0) {\textbf{Positive Active Learning (PAL)}};

\node[inner sep=0pt,below=0.1cm of title_PAL] (desc_PAL)  {
\parbox{7.8cm}{\small\em Given inputs, choose if they are semantically related: \texttt{yes}/\texttt{no}}};

\node[inner sep=0pt, below left=0.6cm and 0.4cm of title_PAL] (q1title_PAL) {$\texttt{query}_{\texttt{1}}$};
\node[inner sep=0pt,below=0.1cm of q1title_PAL] (q1_PAL){\includegraphics[width=2cm,height=2cm]{figures/teaser/ILSVRC2012_val_00038928.jpg}};
\node[inner sep=0pt, below=0.1cm of q1_PAL] (and1) {$and$};
\node[inner sep=0pt,below=0.1cm of and1] (q1_PAL2){\includegraphics[width=2cm,height=2cm]{figures/teaser/ILSVRC2012_val_00008373.jpg}};
\node[inner sep=0pt,below=0.3cm of q1_PAL2] (option1_PAL)  {\parbox{2cm}{\centering$\texttt{response}_{\texttt{1}}$\\ yes}};



\node[inner sep=0pt,below right=0.6cm and -4cm of title_PAL] (q2title_PAL)  {$\texttt{query}_{\texttt{2}}$};
\node[inner sep=0pt,below=.1cm of q2title_PAL] (q2_PAL) {\includegraphics[width=2cm,height=2cm]{figures/teaser/dog.png}};
\node[inner sep=0pt, below=0.1cm of q2_PAL] (and2) {$and$};
\node[inner sep=0pt,below=0.1cm of and2] (q2_PAL2){\includegraphics[width=2cm,height=2cm]{figures/teaser/cat.png}};

\node[inner sep=0pt,below=0.3cm of q2_PAL2] (option2_PAL)  {\parbox{2cm}{\centering$\texttt{response}_{\texttt{2}}$\\no}};

\node[inner sep=0pt,below right=-2.8cm and 1.4cm of and2] (recap){\includegraphics[width=4cm,height=5.75cm]{figures/recaptcha.png}};

    
\node[inner sep=0pt,below=6.5cm of title_PAL,color=blue] (comment_PAL)  {\centering
Low-cost relationships information (reduced expertise)
};
\draw[black,thick,rounded corners] ($(desc_PAL.north west)+(-.8,0.6)$)  rectangle ($(comment_PAL.south east)+(1.6,-0.2)$);

\node[right=1cm of and2] (a) {\rotatebox{90}{\textit{ or (recaptcha)}}};
\node[below=1cm of a] (b) {};
\node[above=1cm of a] (c) {};

\draw (b) -- (a) -- (c);

\end{tikzpicture}
\end{adjustbox}
\vspace{.25em}
    \caption{Active Self-Supervised Learning  introduces PAL ({\bf right box}), an alternative to active learning ({\bf left box}) where the oracle is asked if a collection of inputs are semantically related or not. As opposed to active learning, expert knowledge is reduced as one need not to know all the possible classes but only how to distinguish inputs from the same class. As such, PAL proves to be a low-cost alternative acting upon the similarity graph $\mG$ (recall \cref{eq:G_ssl}); by querying enough samples, the learned representations of SSL or supervised learning are identical (recall \cref{thm:recovery}). PAL querying is flexible, as an illustrative example we exhibit an \`{a} la captcha, version where a given input is presented along with a collection of other inputs, and the oracle can select among those inputs the positive ones.}
    \label{fig:pal}
\end{figure*}

\section{Introduction}

Learning representations of data that can be used to solve multiple tasks, out-of-the-box, and with minimal post-processing is one of the main goals of current AI research \cite{pan2010survey,lecun2015deep,foundation}. Such representations are generally found by processing given inputs through Deep Networks (DNs). The main question of interest around which contemporary research focuses on deals with the choice of the training setting that is employed to tune the DN's parameters. A few different strategies have emerged such as layerwise \cite{bengio2006greedy}, reconstruction based \cite{vincent2008extracting}, and more recently, based on Self-Supervised Learning (SSL) \cite{chen2020simple,misra2020self}. In fact, due to the cost of labeling and the size of datasets constantly growing, recent methods have tried to drift away from traditional supervised learning \cite{settles2011theories}. From existing training solutions, joint-embedding SSL has emerged as one of the most promising ones \cite{jepa}. It consists in learning representations that are invariant along some known transformations while preventing dimensional collapse of the representation. Such invariance is enforced by applying some known Data-Augmentation (DA), e.g. translations for images, to the same input and making sure that their corresponding representations are the same.

Despite tremendous progress, two main limitations remain in the way of a widespread deployment of SSL. First, it separates itself entirely from supervised learning i.e. progresses made in each of those fields do not transfer to one another. Second, it is not clear how to incorporate a priori knowledge into SSL frameworks beyond the usual tweaking of the loss and DAs being employed. For example, if one has access to (some) label information, \cite{chen2020big,zheltonozhskii2022contrast} propose to use SSL for pretraining, and to then fine-tune with supervised learning; alternatively, \cite{zhai2019s4l} proposes to use the label information to sample the positive pairs during SSL training although without guarantee that this variant provides any benefit into the trained DN. 

In this study, we propose to alleviate those two pitfalls by redefining existing SSL and supervised losses in terms of a similarity graph --where nodes represent data samples and edges reflect known inter-sample relationships.
Our first contribution stemming from this formulation provides a {\em generic framework to think about learning in terms of similarity graph}: it yields a spectrum on which SSL and supervised learning can be seen as two extremes. Within this realm, those two extremes are connected through the similarity matrix, and in fact can be made equivalent by varying the similarity graph.
In particular, we will obtain that when that similarity graph aligns with the underlying labels, SSL variants such as VICReg \cite{bardes2021vicreg}, BarlowTwins \cite{zbontar2021barlow} and SimCLR \cite{chen2020simple} learn representation akin to mean-square error, discriminant analysis \cite{klecka1980discriminant}, and cross-entropy supervised learning respectively.
Our second contribution naturally emerges from using such a similarity graph to define the SSL and supervised training losses, unveiling an elegant framework to reduce the cost and expert requirement of active learning summarized by:
\begin{center}
\begin{minipage}{0.6\linewidth}
\centering
\em
Tell me who your friends are,\\ and I will tell you who you are.
\end{minipage}
\end{center}
Active learning, which aims to reduce supervised learning cost by only asking an oracle for sample labels when needed \cite{Settles2010,Dasgupta2011,Hanneke2014,Karzand2020}, can now be formulated in term of relative sample comparison, rather than absolute sample labeling.
This {\em much more efficient and low-cost approach is exactly the active learning strategy stemming from our framework}: rather than asking for labels, one rather asks if two (or more) inputs belong to the same classes or not, as depicted in \cref{fig:pal}. We coin such a strategy as {\em Positive Active Learning (PAL)}, and we will present some key analysis on the benefits of PAL over traditional active learning. We summarize our contributions below:
\begin{itemize}
    \item We provide a {\em unified learning framework} based on the concept of similarity graph, which encompasses both self-supervised learning, supervised learning, as well as semi-supervised learning and many variants.
    \item We derive a {\em generic PAL algorithm} based on an oracle to query the underlying similarity graph \cref{alg:pal}.
    The different learning frameworks (SSL, supervised, and so forth) are recovered by different oracles, who can be combined to benefit from each framework distinction.
    \item We show how PAL extends into an {\em active learning framework} based on similarity queries that provides low-cost efficient strategies to annotate a dataset (\cref{fig:pal}).
\end{itemize}
All statements of this study are proven in \cref{proof:section}, code to reproduce experiments is provided at \url{https://github.com/VivienCabannes/rates}.
