\begin{figure*}[t!]
    \centering
    \begin{minipage}{.45\linewidth}
    \includegraphics[width=\linewidth]{figures/teaser/ssl3_screen-fs8.png}
    \end{minipage}
    \hspace{.05\linewidth}
    \begin{minipage}{.45\linewidth}
    \includegraphics[width=\linewidth]{figures/G_matrix.jpeg}
    \hspace{.5\linewidth}
    \begin{minipage}{0.32\linewidth}\small
    \centering
    supervised
    \end{minipage}
    \begin{minipage}{0.32\linewidth}\small
    \centering
    semi-sup.
    \end{minipage}
    \begin{minipage}{0.32\linewidth}\small
    \centering
    PAL
    \end{minipage}
    \end{minipage}
    \vspace{.5em}
    \caption{\small {\bf Left:} Depiction of the ``knowledge graph'' arising from binary classification, notice the two connected components, each corresponding to a single class. Each sample is associated with a node of the graph ({\bf blue circle}) and the known positive relation between samples is represented by an edge. This knowledge is summarized into the $\mG$ matrix depicted on the bottom row.
    {\bf Right:} Examples of the $N \times N$ symmetric adjacency matrices $\mG$ for the case of binary classification (same as at the top row). Each nonzero entry $(\mG)_{i,j}$ represents the known positive relation between sample $i$ and $j$. 
    The insight that will play a key role in our analysis is that knowing $\mG$ is equivalent to knowing the underlying per-sample labels. As such, when PAL is employed ({\bf right column}) and all the entries are recovered, SSL training will lead to the same representation as supervised training. Any a priori knowledge e.g. as in semi-supervised learning ({\bf middle column}) can be incorporated into $\mG$ prior PAL.
    }
    \label{fig:G}
\end{figure*}

\section{Background on Self-Supervised Learning}
\label{sec:background}

This section provides a brief reminder of the main self-supervised learning (SSL) methods, their associated losses, and common notations for the remainder of the study.

A common strategy to learn a model in machine learning is to curate labeled examples $(\vx_n,y_n)_n$, and to learn a model that given $\vx_n \in \X \triangleq \R^D$ as input, outputs $y_n \in [C]$, hoping that this model will learn to recognize patterns and relations that generalizes to new, unseen input data. 
Yet, as the dataset grew larger, and annotating data has become a major bottleneck, machine learning has shifted its attention to learning methods that do not require knowledge of $y_n$.
SSL has emerged as a powerful solution to circumvent the need for expensive and time-consuming labeling.
It learns a embedding $f:\X\to\R^K$ for a small $K$ by enforcing either reconstruction properties, or some invariance and symmetry onto a learned representation. 
SSL also relies on a set of observations $\mX = \{\vx_n\}_{n=1}^{N} \in \R^{N\times D}$, yet instead of labels $y_n$, it requires known {\em pairwise positive relation} that indicates whether two samples are semantically similar or not. 
For simplicity, we shall focus on the joint-embedding framework, where those positive pairs are artificially generated on the fly by applying Data Augmentations (DA), e.g. adding white noise, masking, on the same input.
Let denote $\gT_1, \gT_2:\X\to\X$ the generators of two (random) DAs $\gT_1(\vx)$ and $\gT_2(\vx)$ from an input $\vx$, $f_{\theta}:\mathbb{R}^{D} \to \mathbb{R}^{K}$ the  parametric model to be learned, and
\begin{align}
\mZ^{(1)} \triangleq \begin{bmatrix}
f_{\theta}(\gT_1(\vx_1))\\
\vdots\\
f_{\theta}(\gT_1(\vx_N))\\
\end{bmatrix},
\mZ^{(2)} \triangleq \begin{bmatrix}
(f_{\theta}(\gT_2(\vx_1))\\
\vdots\\
(f_{\theta}(\gT_2(\vx_N))\\
\end{bmatrix},\label{eq:Z}
\end{align}
where $(\vz^{(1)}_n,\vz^{(2)}_n)$, the $n^{\rm th}$ row of $\mZ^{(1)}$ and $\mZ^{(2)}$ respectively, form the $n^{\rm th}$ positive pair associated to sample $\vx_n$. Using \eqref{eq:Z}, different SSL losses will employ different measures of invariance and dimensional collapse. 
Typically, the losses are minimized with gradient descent and backpropagation to learn $\theta$.

{\bf VICReg.}~With the above notations, the VICReg loss \cite{bardes2021vicreg} reads, with hyper-parameter $\alpha, \beta > 0$,
\begin{align}
\nonumber
&\gL_{\rm VIC} = \alpha \sum_{k=1}^{K}\relu\left(1-\sqrt{\mC_{k,k}}\right)+\beta \sum_{k\neq l}\mC^2_{k,l} 
\\&\quad+ \frac{1}{N}\|\mZ^{(1)}-\mZ^{(2)}\|_2^2,
\qquad\mC \triangleq \Cov(\begin{bmatrix} \mZ^{(1)}\\\mZ^{(2)} \end{bmatrix}).
\label{eq:VICReg}
\end{align}

{\bf SimCLR.}~The SimCLR loss \cite{chen2020simple} with temperature hyper-parameter $\tau > 0$ reads
\begin{align}
    \nonumber
    &\gL_{\rm Sim}=-\sum_{i=1}^{N}\frac{\mC_{i,i}}{\tau}+\log\left(\sum_{i\neq j}^{N}\exp\paren{\frac{\mC_{i,j}}{\tau}}\right),
    \\& \mC_{i,j} \triangleq \CosSim(\mZ^{(1)}, \mZ^{(2)})_{ij} \triangleq \frac{\scap{\vz^{(1)}_i}{\vz^{(2)}_j}}{\|\vz^{(1)}_i\| \,\|\vz^{(2)}_j\|},\label{eq:simCLR}
\end{align}

{\bf BarlowTwins.}~BarlowTwins \cite{zbontar2021barlow} is built on the cross-correlation matrix $\mC_{ij} = \CosSim(\mZ^{(1)}{}^\top, \mZ^{(2)}{}^\top)$, with the hyper-parameter $\lambda$ it reads
\begin{align}
    \label{eq:BT}
    \gL_{\rm BT} = \sum_{k=1}^K (1 - \mC_{ii})^2 + \lambda \sum_{i\neq j} \mC_{ij}^2.
\end{align}
% It requires $\gO(Nk^2)$ flops to be computed.

{\bf Spectral Contrastive Loss.}~Finally, the spectral contrastive loss \cite{haochen2021provable} is theory-friendly proxy for SSL reading 
\begin{align}
    \label{eq:scl}
    \gL_{\rm VIC^2} = -2\scap{\mZ^{(1)}}{\mZ^{(2)}} + \frac{1}{N}\sum_{i\neq j} \scap{\vz^{(1)}_i}{\vz^{(2)}_j}^2.
\end{align}
In particular, as proven in \cref{proof:vic-2}, \eqref{eq:scl} recovers VICReg \eqref{eq:VICReg} when the ReLU-hinge loss is replaced by the mean-square error, hence the denomination VIC$^2$.

{\bf The Commonality between SSL Losses.}~%
All the above \cref{eq:VICReg,eq:simCLR,eq:scl,eq:BT} losses combine two terms: (i) a matching term between positive pairs, and (ii) a term to avoid collapse towards predicting a constant solution for all inputs.
(i) can take different forms such as the squared norm between $\mZ^{(1)}$ and $\mZ^{(2)}$ \eqref{eq:VICReg}, the opposite of their scalar product \eqref{eq:scl}, or of their cosine \eqref{eq:simCLR}, or the square norm between the centered-cosine and one \eqref{eq:BT}.
(ii) can also take different forms such as the infoNCE softmax \eqref{eq:scl}, or an energy that enforces richness of the learn feature, such as the variance-covariance regularization in \eqref{eq:VICReg} forcing the different coordinates of $f_\theta$ to be orthogonal \cite{cabannes2023ssl}.

While at face-value those losses seem distinct, they actually all simply consist and combine some variants of (i) and (ii), and even more importantly, they all rely on the exact same information of positive inter-sample relation for (i). This is exactly what the next \cref{sec:unify} will dive into, as a means to unify SSL losses, along with supervised losses.
