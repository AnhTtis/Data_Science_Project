

\section{The Ubiquity of Similarity Graphs}\label{sec:unify}

The goal of this section is to unify SSL and supervised learning through the introduction of a special object: a {\em similarity graph}.

\subsection{The Graphs for (Self-)Supervised Learning}

Throughout this study, a similarity graph denotes a graph for which nodes represent data samples, and edges reflect similarity relationships. 
Formally, such a graph is expressed through its symmetric adjacency matrix $\mG \in \R^{N \times N}$, the semantic relation between inputs $i$ and $j$ being encoded in the real entry $\mG_{i,j}$. The remainder of this section will demonstrate how (i) SSL losses are implicitly based on a similarity graph (ii) how those losses tackle the supervised learning problem when provided a richer graph $\mG$.

{\bf Supervised learning.}~%
In addition to the $N$ input samples $\mX \in \R^{N\times D}$, supervised learning has access to paired labels $\vy \triangleq [y_1,\dots,y_N]$.
For clarity, we focus here on categorical labels  i.e. $y_n$ belongs to $\{1,\dots,C\}$ for $C$ the number of classes.\footnote{%
    While we focus here on classification for simplicity, our approach is easily extendable for generic problems involving a loss $\ell$ by defining the graph as $\mG_{ij} = -\ell(y_i, y_j)$. 
    In the classification, $\ell$ could be the zero-one loss $\ell(y_i, y_j) = \ind{y_i\neq y_j}$, and $\mG_{ij} \simeq 1 - \ell(y_i, y_j)$.
    See \cref{app:recovery} for details.
}
The one-hot encoding of $\vy$ will be denoted by the matrix $\mY \in \R^{N\times C}$.
In terms of the similarity graph $\mG$, the label-based relation becomes naturally encoded as $\mG_{i,j} = \ind{y_i\neq y_j}$, or equivalently
\begin{align}
    \label{eq:G_sup_easy}
    \mG(\mY) = \mY \mY^\top
\end{align}
A key observation that we must emphasize is that the graph $\mG$ almost explicitly encodes for the labels $\mY$, which will be made explicit with Theorem \ref{thm:recovery}.


{\bf Multiple Epochs with Data Augmentation.}~%
When DA is employed, and training is carried for $E$ epochs, the original $N$ input samples are transformed into $N\times E$ ``augmented'' samples. For more generality, since DA will also be used in SSL, let's denote by $A \in \mathbb{N}^*$ the number of augmentations --where here $A=E$. We now have the augmented dataset
\begin{align*}
    \mX^{(A)} &\triangleq [\underbrace{\gT(\vx_1),\dots,\gT(\vx_1)}_{\text{repeated $A$ times}} ,\dots, \gT(\vx_N),\dots,\gT(\vx_N)]^\top,
\end{align*}
where each $\gT$ has its own randomness. When available, i.e. for supervised learning, the corresponding ``augmented'' labels $\mY^{(A)}$ are given by repeating $A$ times each row of $\mY$, formally written with the Kronecker product $\mY^{(\sup)}\triangleq \mY \otimes \1_A$, and from that, we can now define the supervised dataset and associated graph extending \eqref{eq:G_sup_easy} to the case of multiple epochs and DA training
\begin{align}
    \label{eq:G_sup}
    \mX^{(\sup)}\triangleq \mX^{(E)},\qquad
    \mG^{(\sup)} \triangleq {\mY^{(\sup)}}^\top \mY^{(\sup)}.
\end{align}
The resulting graph \eqref{eq:G_sup} is depicted on the left part of \cref{fig:G}.

{\bf Self-Supervised Learning.}~%
SSL does not rely on labels, but on positive pairs/tuples/views generated at each epoch. 
Let us denote by $V$ the number of positive views generated, commonly $V=2$ for positive pairs as modeled in \eqref{eq:Z}.
With $E$ the total number of epochs, SSL produces $V \times E$ samples semantically related to each original sample $\vx_n$ through the course of training {\em i.e.} in SSL $A= V\times E$ while in supervised learning $A=E$. 
The total number of samples is thus $N\times E\times V$, defining the dataset and associated graph
\begin{align}
\mX^{(\ssl)} \triangleq\mX^{(V\times E)},\,
     \mG^{(\ssl)}_{i,j} = \Indic_{\{\floor{i/VE}=\floor{j/VE}\}},\label{eq:G_ssl}
\end{align}
where the associated similarity graph $\mG^{(\ssl)}$ --now of size $NEV\times NEV$-- captures if two samples were generated as DA of the same original input.

\subsection{Self-Supervised Learning on a Graph}

This section reformulates the different SSL losses through the sole usage of the similarity graph $\mG^{(\ssl)}$. 
To lighten notations, and without loss of generality, we {\em redefine} $\mX \in \R^{N\times D}$ to denote the full dataset, i.e. $N \leftarrow NEV$ with $\mX = \mX^{(\sup)}$ for supervised learning with $V \times E$ epochs, or with $\mX = \mX^{(\ssl)}$ in SSL with $E$ epochs with $V$ views for the SSL case.
The model embedding is shortened to $\mZ \triangleq f_\theta(\mX) \in \R^{N\times K}$ as per \cref{eq:Z}.

\begin{theorem}
\label{lemma:characterization}
VICReg \eqref{eq:VICReg}, SimCLR \eqref{eq:simCLR}, and BarlowTwins \eqref{eq:BT} losses can be expressed in term of the graph $\mG$ \eqref{eq:G_ssl}
\begin{align*}
    \\ \gL_{\rm VIC^2}(\mZ;\mG)=&\| \mZ\mZ^T  - \mG \|_F^2,
    \\ \gL_{\rm Sim}(\mZ;\mG)=&-\hspace{-0.2cm}\sum_{i,j\in[N]}\mG_{i,j}\log\paren{\frac{\exp(\tilde\vz_i^\top \tilde\vz_j)}{\sum_{k\in[N]} \exp(\tilde\vz_i^\top \tilde\vz_k)}},
    \\ \gL_{\rm BT}(\mZ;\mG)=& \norm{\tilde\mZ^\top \mG \tilde\mZ - I}^2,
\end{align*}
where $\mD = \diag(\mG \1)$ is the degree matrix of $\mG$; with $\tilde\vz \triangleq \vz / \norm{\vz}$ and $\tilde\mZ$ the column normalized $\mZ$ so that each column has unit norm.
\end{theorem}

From \cref{lemma:characterization}, we obtain the direct observation that VICReg is akin to Laplacian Eigenmaps or multidimensional scaling, SimCLR is akin to Cross-entropy and BarlowTwins is akin to Canonical Correlation Analysis; observations already discovered in the literature \cite{balestriero2022contrastive} and reinforced above. 

Beyond recovering such representation learning losses, our goal is to go one step further and to tie SSL and supervised learning through the lens of $\mG$, which follows in the next section.

\subsection{Self-Supervised is a G Away from Supervised}

What happens if one takes the different SSL losses, but replaces the usual SSL graph $\mG^{({\rm ssl})}$ with the supervised one $\mG^{({\rm sup})}$?

It turns out that the learned representations emerging from such losses are identical --up to some negligible symmetries that can be corrected for when learning a linear probe-- to the one hot-encoding of $\mY$.
To make our formal statement (\cref{thm:recovery}) clearer, we introduce a the set of optimal representations that minimize a given loss:
\begin{align*}
        \gS_{\rm method}(\mG) \triangleq \argmin_{\mZ \in \R^{N \times K}} \gL_{\rm method}(\mZ;\mG),
    \end{align*}
where ``method'' refers to the different losses. 

\begin{theorem}[Interpolation optimum]
    \label{thm:recovery}
    When $K \geq C$, and $\mZ = f_\theta(\mX)$ is unconstrained (e.g. interpolation regime with a rich functions class), the SSL losses as per \cref{lemma:characterization} with the supervised graph \eqref{eq:G_sup} solve the supervised learning problem with:
    \begin{align*}
        \gS_{\rm VIC^2}(\mG^{(\sup)}) &= \brace{\mY \mR \midvert \mR \in \R^{C\times K}; \mR \mR^\top = \mI_C},\\
        \gS_{\rm Sim}(\mG^{(\sup)}) &= \brace{\mD\mY\mR\mM^{-1}\midvert \mD \in \diag_+, \mR \in O},\\
        \gS_{\rm BT}(\mG^{(\sup)}) &= \brace{\mY\mR \mD \midvert \mD \in \diag_+, \mR \in O},
    \end{align*}
    where $\mR \in O$ means that $\mR$ is a rotation matrix as defined for the VICReg loss, $\diag_+ = \diag(\R^N_+)$ are the set of diagonal matrix with positive entries, i.e. renormalization matrices, and $\mM$ is a matrix that maps a deformation of the simplex into the canonical basis.
    Moreover, provided class templates, i.e. $C$ data points associated with each of the $C$ classes, $\mY$ is easily retrieved from any methods and $\mZ \in \gS_{\rm method}$.
\end{theorem}

In essence, \cref{thm:recovery} states that SSL losses solve the supervised learning problem when the employed graph $\mG$ is $\mG^{(\sup)}$.
Moreover, the matrix $\mD$ appearing in \cref{thm:recovery} captures the fact that SimCLR solutions are invariant to rescaling logit and is akin to the cross-entropy loss, while BarlowTwin is invariant to column renormalization of $\mZ$ and is akin to discriminant analysis.
Lastly, VICReg might be thought of as a proxy for the least-squares loss.
At a high-level, \cref{thm:recovery} suggests fruitful links between spectral embedding techniques captured in \cref{lemma:characterization} and supervised learning.
We let for future work the investigation of this link and translation of spectral embedding results in the realm of supervised learning.
While \cref{thm:recovery} describes what we have coined as the ``interpolation optimum'', i.e. solution in the interpolation regime with rich models, we ought to highlight that classical statistical learning literature analyzes losses under the light of ``Bayes optimum'', i.e. solutions in noisy context-free setting \cite{Bartlett2006}.
Those Bayes optima do not make as much sense for losses that intrinsically relate different inputs, yet for completeness we provide such a proposition on Bayes optimum in \cref{app:bayes}.
