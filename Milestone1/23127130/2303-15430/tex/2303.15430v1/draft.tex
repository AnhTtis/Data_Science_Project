Multimodal models for understanding human communication need a proper representation of text, audio, and facial expressions. Many tasks often have small datasets and cannot be well trained with raw audio-visual features. We introduce Multimodal-CLIP that jointly learns to represent acoustic and visual features from a human speech video in a self-supervised manner. Contrastive loss allows aligning video segments with corresponding audio while ensuring unrelated video and audios are well separated. We experiment on several multimodal benchmark datasets to show diverse applications of the pre-trained model. Multimodal-CLIP embeddings help zero-shot similar video retrieval and speaker identification in CMU-MOSI. Combined with a pre-trained language model, Multimodal-CLIP can be fine-tuned for a diverse range of communication tasks that require multimodal understanding. Experiments on humor and sarcasm detection show performance improvement when the acoustic-visual network is initialized with pre-trained weights.