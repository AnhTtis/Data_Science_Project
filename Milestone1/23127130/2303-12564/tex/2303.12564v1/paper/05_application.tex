\section{Single-View Reconstruction}
\label{sec:experiment-svr}
\input{figure/fig_pipeline_svr}

Single-view reconstruction (SVR) is one of the most popular tasks of efficient 3D content generation, and recent work has made noticeable progress on human reconstruction based on parametric model of human characters (e.g., SMPL). %However, SVR for 3D biped cartoon characters still needs to be solved and is worth probing.
To verify the practicality of our proposed \emph{3DBiCar} and \emph{RaBit}, we conduct SVR for bipled cartoon characters. A baseline learning-based method is presented, which is termed as \textit{BiCarNet}. 

%We thus present a baseline method named \textit{BiCarNet} for efficient biped cartoon characters reconstruction from a single image.

%Single-view reconstruction (SVR) is a classic task in computer vision and computer graphics, which aims to assist people in creating 3D content efficiently. In this section, we present a baseline method for reconstructing 3D biped cartoon characters from a single image, with the help of \textit{RaBit}.
% Single view reconstruction is a popular and basic application for parametric model and human dataset (e.g. SMPL). With our \textit{RaBit} and \textit{3DBiCar}, single view reconstruction on cartoon can be handled.

\subsection{\textit{\textbf{BiCarNet}}}

%With   As shown in Fig.~\ref{fig_pipeline_svr}, given the input image, we first adopt an \textit{Encoder}, for instance, HMR~\cite{kanazawa2018end}, to map the input to the three parametric vectors of \textit{RaBit} on shape, pose, and texture, respectively. Next the vectors are fed to our \textit{RaBit} model to generate a posed body mesh, two eyeballs, and a coarse texture map. 

%\textbf{\textit{BiCarNet}.} 
Given a single masked image of cartoon characters, our \textit{BiCarNet} aims to reconstruct the corresponding 3D shape, pose, and texture. %As \textit{Rabit} spans a large space of textured models. 
The key problem is to build an encoder to map the input image to the parametric space of \textit{Rabit}. As shown in the upper part of Fig.~\ref{fig_pipeline_svr}, 
%With the fixed parametric model \textit{Rabit}, an Encoder is utilized to map the image to parametric space of shape, pose, and texture, as the upper part of Fig~\ref{fig_pipeline_svr} shows.
We adopt the learning block in HMR~\cite{kanazawa2018end} as our Encoder. Once these parametric vectors are learned, we can feed them to our \textit{RaBit} model to generate a posed body mesh, two eyeballs, and a UV texture (we name it global for convenience to introduce the following method description). 

During our preliminary experiments, we find that the shape reconstruction of characters, \ie the eyes and body, is satisfactory, while the inferred UV tends to lose detailed appearances of some small yet significant areas, such as the nose and ears. We thus adopt a part-sensitive texture reasoner (PSR) to address the above issue, as the lower part of Fig.~\ref{fig_pipeline_svr} shows. Specifically, we design five individual UV-mappings for these significant parts of the nose, ears, horns, eyes, and mouth. Five lightweight encoder-decoder branches are next introduced to learn the appearances of these local regions from the input image, respectively. The learned part UVs could be remapped to the corresponding area on the global UV map to produce a blended texture. However, a direct blending tends to cause seam artifacts. Thus we further adopt a Fuser to address the artifacts as Fig.~\ref{fig_pipeline_svr} illustrates. Please refer to the Supplementary for thorough implementations of \textit{BiCarNet}.


% Move this part to Supplementary
% !!!!!!!!!!!!!!
\iffalse 
\textbf{Implementations.} In our implementation, for shape and pose regression modules, inspired by HMR~\cite{kanazawa2018end}, we utilize two ResNet-50 architectures to embed the $512 \times 512$ input image to a 100-dimensional shape vector, and a 69-dimensional pose vector, respectively. For \textit{texture-module}, we adopt the pSp-encoder~\cite{richardson2021encoding} to learn a 512-dimensional texture vector from the image. As for the part-sensitive texture reasoner, we utilizes the pSp~\cite{richardson2021encoding} as the basic building block to learn multiple $256 \times 256$ local UV textures from the input. pix2pixHD~\cite{wang2018pix2pixHD} is employed as the fusion module, which takes $1024 \times 1024$ coarse texture maps as input and output $1024 \times 1024$ fine texture maps.
\fi
% !!!!!!!!!!!!!!

\subsection{Experiments}

\textbf{Data preparation.} We first split \textit{3DBiCar} into a training set (1,050 image-model pairs) and a testing set (450 pairs). To support a stable training of \textit{BiCarNet}, we next generate a large number of synthetic paired data with the help of \textit{RaBit}, which are highly diversified in shape, posture, and texture. To be specific, a lot of 3D textured models are first sampled from the \textit{RaBit} space, which are then rendered into images from different camera views. 
This finally produce 13,650 pairs for training. Note that, \emph{BiCarNet} takes an image with foreground masked as input. All synthetic images naturally have no background. For real images, all the foreground masks are manually annotated. 
%the synthetic images have no background and we also manually mask the foreground for all real images. 

%These augmentation data are rendered to multiple images with different camera angles. With \textit{3DBiCar} and the augmentation data, we finally obtain 13,650 pairs for training. Notice that we also separate the foreground character from the image background.
 
 % mention this on the Supplementary

% a topology-preserving inference method for shape and pose.

\textbf{Result gallery.} % results of \textit{BiCarNet} are available in Fig.~\ref{fig_wildresult}. 
Fig.~\ref{fig_wildresult} shows representative results generated by \textit{BiCarNet}. As illustrated, our \textit{BiCarNet} can generate vivid 3D cartoon characters loyal to individual cartoon images in shape, pose, and texture. We believe that our work opens the door to producing 3D biped cartoon characters from easy-to-obtain inputs.
\input{figure/wild_result}


\textbf{Results on Shape Reconstruction.} 
%Our target is reconstructing topological-consistent 3D biped cartoon characters from single images to facilitate future animation. 
As mentioned above, \textit{BiCarNet} utilizes HMR-like blocks and \textit{RaBit} for shape and pose learning. Currently, other reconstruction methods could also be used for topology-consistent geometry inference, such as GCNN-based methods~\cite{lin2021-mesh-graphormer} and UV-based methods~\cite{zeng20203d}. We choose two representative methods for comparison, i.e., Mesh-Graphormer~\cite{lin2021-mesh-graphormer,lin2021end-to-end} and DecoMR~\cite{zeng20203d}. Mesh-Graphormer combines graph convolutions and self-attentions in a transformer for 3D human reconstruction from a single image. DecoMR reconstructs 3D human mesh from single images by regressing a UV-based location map. Tab.~\ref{tab_mesh_result} shows the quantitative comparisons of the above three methods on MPVE, MPJPE, and PA-MPJPE. We also provide qualitative comparisons in Fig.~\ref{fig_mesh_result}. Both quantitative and qualitative results demonstrate that the HMR-like method achieves the highest performance on geometry inference and provides more accurate reconstructions closer to ground truths. As noted, both Mesh-Graphormer and DecoMR outperform HMR for SMPL-based human reconstruction. It is interestingly found that they perform worse in our settings. One possible reason is that our biped cartoon meshes own an extremely larger amount of vertices than SMPL to model more complex geometry. This greatly increases the challenge of vertices regression in Mesh-Graphormer and DecoMR. Thus, in our setting, directly regressing the low-dimension parameters performs better. 

%Due to the high diversity of cartoon characters, it is challenging for neural networks to directly regress a large number of points (38,726 vertices) or high-resolution position maps from a single image. In our HMR-based modules, \textit{RaBbit} models the shape and pose into low-dimensional parametric spaces, significantly reducing the learning difficulty of neural networks.

% To conduct comprehensive experiments of SVR, we compare our \textit{BiCarNet} with current advanced mesh reconstruction methods, including GCNN-based method Mesh Graphormer~\cite{lin2021-mesh-graphormer} and UV-based method DecoMR~\cite{zeng20203d}. Mesh Graphormer combines graph convolutions and self-attentions in a transformer for 3D human reconstruction from a single image. DecoMR reconstructs 3D human mesh from single images by regressing a UV-based location map. 
% We adopt HMR block~\cite{kanazawa2018end} as \textit{Encoder} for mesh geometry learning based on \textit{RaBit} and compare our method with aformentioned two reconstruction methods. 
% We use MPVE, MPJPE and PA-MPJPE as our evaluation metrics for mesh reconstruction as Table~\ref{tab_mesh_result} shows. It can be seen that our method as one setting of \textit{BiCarNet} outperforms other methods in all metrics. We also illustrate the reconstruction results of these methods as Fig.~\ref{fig_mesh_result} shows and our results are visually relatively close to the ground truths. This demonstrates the quantitative and qualitative superiority of \textit{Rabit} and \textit{BiCarNet} for accurate cartoon character reconstruction. % Both quantitative and qualitative results demonstrate that our method achieves the highest performance on geometry inference and provides more accurate reconstructions that are closer to ground truths. The main reason are: 1) It is challenging for neural networks to directly regress large number of points (38,726 vertices) or high-resolution position-maps from a single-image; 2) In our HMR-based modules, \textit{RaBbit} model the shape and pose into low-dimensional parametric spaces, which greatly reduce the learning difficulty of neural networks.
%Our target is reconstructing topological-consistent 3D biped cartoon characters from single images to facilitate future animation. 

%As mentioned above, \textit{BiCarNet} utilizes HMR-like blocks and \textit{RaBit} for shape and pose learning. 

%There are some optional methods that could also be used for topology-consistent geometry inference, such as GCNN-based methods~\cite{lin2021-mesh-graphormer,lin2021end-to-end,kolotouros2019cmr} and UV-based methods~\cite{zeng20203d,alldieck2019tex2shape,feng2018prn}. We choose two representative methods of them for comparison, i.e., Mesh-Graphormer~\cite{lin2021-mesh-graphormer} and DecoMR~\cite{zeng20203d}. 


\input{table/table_mesh_result}
\input{figure/fig_mesh_result}

\textbf{Results on Texture Inference.} 
%Our \textit{BiCarNet} contains a novel part-sensitive texture reasoner to perceive the detailed appearances of local areas, which consists of several individual . We also design a Fuser to avoid artifacts on boundaries. 
To demonstrate the capability of our proposed GAN-based texture generator, we first compare our \emph{RaBit}-based texture inference (i.e., \emph{BiCarNet}) with PCA-based inference. Specifically, for PCA-based method, we utilize the same learning architecture to map the input image into the PCA-based texture space. Furthermore, %with the traditional PCA-based method. Specifically, 
to evaluate the effectiveness of the proposed texture inference modules, we also conduct ablative analysis on \textit{BiCarNet} without Fuser and \textit{BiCarNet} without Part-sensitive Reasoner (PSR). Table~\ref{tab_texutre_result} shows the quantitative results of different texture inference methods on MSE, PSNR and FID and our proposed method achieves the highest scores compared with all other methods. Moreover, Fig.~\ref{fig_texture_result} illustrates the qualitative results of these methods for texture inference. Fig.~\ref{fig_texture_fusion} shows the results without and with Fuser, which demonstrates our fusion module can deal with unnature seam-like artifacts. We can observe that the part-sensitive texture reasoner and the Fuser help to capture the local regions of characters and recover their detailed appearances. %Our \textit{BiCarNet} is capable of recovering reasonable 3D cartoon texture from the input image. 

% \textit{BiCarNet} designs a novel part-sensitive texture reasoner to perceive the appearances of important local areas. To evaluate its effectiveness, we conduct comparison analysis on the PCA-based method, \textit{BiCarNet} without TR (Texture Reasoner), \textit{BiCarNet} without Fuser and \textit{BiCarNet}. Tab.~\ref{tab_texutre_result} shows the quantitative results of different methods on our testing dataset. Our proposed approach achieves the highest accuracy on texture inference compared with the other methods. Fig.~\ref{fig_texture_result} shows the qualitative results of different methods. As illustrated, the results generated by our approach are closer to ground truths. With the help of the part-sensitive texture reasoner, \textit{BiCarNet} is able to capture the local regions of characters and recover their detailed appearances, such as the nose and ears.

% We choose the principal component analysis (PCA) method as the baseline method. In addition, we evaluate and compare the whole-based style-GAN method and the part-based style-GAN method.
% In the baseline method, PCA is first applied to reduce the dimension of UV, and then the neural network is used to fit the reduced vector. In both PCA and style-GAN methods, the dimension of $T$ is fixed to be 512.
% We evaluate four proposed methods on our dataset. Each is defined as:
% MSE, PSNR and FID are used for evaluation. As shown in Tab. \ref{tab_texutre_result}), the styleGAN method outperformed the PCA method in all three metrics. 
% The qualitative result in Fig. \ref{fig_texture_result} illustrates that the UV from the styleGAN method is sharper, less artifact, and closer to ground truth. 

\input{table/table_texture_result}
% \vspace{-0.3cm}
\input{figure/fig_texture_result}
\input{figure/fig_texture_fusion.tex}
% \vspace{-0.6cm}

\section{More Applications}
\label{sec:application}
\subsection{Sketch-based Modeling}
Customizing 3D biped cartoon characters usually requires a heavy workload with commercial tools, even for experienced artists. Sketch-based modeling enables amateur users to get involved in 3D shape customization in a simple and intuitive fashion. We build a sketch-based modeling application with the help of \textit{3DBiCar} and \textit{RaBit}. 

We first sample a series of shape vectors randomly and feed them to \textit{RaBit} to generate 3D cartoon characters with diversified shapes, resulting in 12,000 T-pose models. Then we apply suggestive contour~\cite{han2017deepsketch2face} to render the front-view sketches with different abstraction levels and obtain 108,000 sketch-model pairs. Given a sketch as input, we employ ResNet-50 and three MLPs as the encoder and decoder to map the input sketch to 100-dimensional shape parameters. The generated shape parameters are next fed to \textit{RaBit} to reconstruct the corresponding 3D model. Please refer to the Supplemental materials for more details. Note that users only need to depict a 3D character with T-pose on a 2D canvas while the output characters of our method are animation-ready and can be directly applied to other commercial tools. Fig.~\ref{fig_sketch} displays the sketches created by users with little knowledge of modeling as well as the corresponding models generated by our system. It can be seen that our sketch-based modeling system offers a smart approach for amateur users to create 3D biped cartoon characters with diversified shapes. We will further explore the support of shape reconstruction from  sketches with arbitrary poses, and texture painting in the future.
 
% \textbf{Implementations.} For character shape, as Fig~\ref{fig_sketch} shows, given a sketch as input, we first employ ResNet-50 and three MLPs as encoder and decoder to map the input sketch to 100-dimensional shape parameters in \textit{BiCar}. Finally, we use the 3D character data we constructed as ground truth to train our neural network. For texture generation, taken as input a painting sketch, we first map it into the unfolding UV map. Then, we follow pSp-Encoder~\cite{richardson2021encoding} and encode the input incomplete texture map. Next, apply the \textit{BiCar} to decode and finally output the completed texture map. 


% \textbf{Results and Analysis.} TBD.
\input{figure/sketch_result}

\subsection{3D Character Animation}
%In previous sections, we have demonstrated the effectiveness of \textit{3DBiCar} and \textit{RaBit} in supporting 3D biped cartoon characters generation from easy-to-obtain inputs, i.e., single-view images and sketches. This section further demonstrates the usability of our generated models for character animation. 

Following the recent advance of human recovering method and parametric model~\cite{wang2022live,SMPL:2015,SMPL-X:2019}, we extract the human from input video frames and adopt a temporal-aware encoder to recover the sequence of human poses~\cite{wang2022live}. Then, a motion retargeting method~\cite{hsieh2005motion} is used to convert the poses on the human skeleton to the motion of our cartoon characters. As shown in Fig.~\ref{fig_anim}, animation-ready characters generated by our \textit{RaBit} can be directly applied to 3D animation. Please refer to the supplementary for animation video.
% qldd nb!
% jin zong nb!
%Animating 3D characters is critical in filming and gaming, yet labor-intensive and time-consuming for animators. With recent advances in deep learning, it is already possible to efficiently animate 3D humans from 2D motion videos with the help of parametric models. Similar to SMPL~\cite{SMPL:2015}, our parametric model \textit{RaBit} can also be animated by controlling the pose parameters. We build a simple application for automatically transferring motion from 2D human videos to \textit{RaBit}. Inspired by~\cite{wang2022live}, we use Yolov3\cite{redmon2018yolov3} to detect the bounding box of a human in every frame, then use the Temporal Encoder provided by TePose~\cite{wang2022live} to extract the sequence of human pose parameters. These pose parameters could be directly applied to animate \textit{RaBit} due to its similar skeleton to SMPL. Thus any cartoon character generated by \textit{RaBit} could be animated from an input video in this way. Fig.~\ref{fig_anim} shows the representative results of transferring motion from an input video to three characters generated by \textit{RaBit}. As seen, \textit{RaBit} could generate high-fidelity posed mesh with the input pose parameters.

\input{figure/fig_anim}
% video-based approach to 3D motion capture
% We propose a simple algorithm for automatic transfer of facial expressions, from videos to a 3D character, as well as between distinct 3D characters through their rendered animations. 
% Animating 3D character is a great demand in filming and gaming. However, traditional commercial 

% The mature video-based approach to 3D motion capture has recently led to the boom of vtubers, but model-making is still costly. However, after the appearance of \textit{RaBit}, the history of time-and-money-consuming drivable 3D models has passed. We set up a character animation application with video input in this section. 

% This application is the merge of video-based human body reconstruction and motion retargeting. Following TePose \cite{wang2022live}, we train a temporal 3D human pose estimation to provide a real-time pose parameter sequence. Due to the skeleton structure similar to SMPL, we can reproduce the motion in the video with our models with painless artifacts, by directly mapping and transferring pose parameters from correspondance joints in SMPL. We displace several frames of video and output models in Fig. \ref{fig_anim}.

% In the film industry, the motion fidelity of virtual characters is in high demand. Similar to SMPL\cite{SMPL:2015}, our parametric model \textit{RaBit} can also animate the model by controlling the pose parameters. Based on actual human pose datasets, placing our cartoon characters in various poses is painless, even creating animations. Fig. \ref{fig_anim} displays several frames of character poses extracting from human video and migrating to random models in \textit{3DBiCar}.

% Following TePose\cite{wang2022live}, we use Yolov3\cite{redmon2018yolov3} to detect the bounding box of a human in one frame, then use the Temporal Encoder provided by TePose to extract the human pose parameters. Applying these pose parameters to our parametric model \textit{RaBit},  we can place any cartoon character in the same pose as the human in the frame. In this way, any cartoon character created by \textit{RaBit} can be directly applied to video-based motion capture.

% % \subsection{Cartoon Model Editing}
% \iffalse
% \subsection{Part Based SMCL}
% \label{part_SMCL}
% We provide another impletement on mesh of SMCL. For specific part $p$, which contains $N_p$ points, the linear model can be expressed as:
% \begin{equation}
%     M_{Tbody}^p = F_B^p(B^p, S^p) = \bar{M}_{Tbody}^p + \sum_i^{|B^p|} \beta^p_i s^p_i
% \end{equation}
% where $\bar{M}_{Tbody}^p$ is the average mesh of part $p$, $B^p=[\beta^p_0,\beta^p_1,...,\beta^p_{|B^p|}] \in \mathbb R^{|B|}$ is shape coefficients for part $p$, $s^p_i \in \mathbb R^{3N_p}$ is one of the orthonormal principal components of shape offset, $S^p\in \mathbb R^{3N_p \times |B^p|}$ is the shape offset matrix getting from PCA. As a result, we can parameterize the final model as:
% \begin{equation}
%     M_{Tbody} = \bigoplus_p M_{Tbody}^p
% \end{equation}
% where $\oplus$ represents merging operation.
% In detail, we provide a simple way to implement the merging operation. When splitting the model, we preserve common segments for different parts on junctions. Different parts can align together by calculating translation $t$ with common segments:

% \begin{equation}
%     t = argmin_t \sum_{k\in \mathcal I_{p} \cap \mathcal I_{p'} }\|v_{p, k} - v_{p', k} \|_2^2 
% \end{equation}
% where $\mathcal I_{p}$ and $\mathcal I_{p'}$ are index sets of points from different parts. $v_{p, k}$ is the position of the point with k-th index in part $p$. 

% Although aligning the parts is able to reach a proper position, common segments of the different parts are not likely to match perfectly. We use a weighting function to merge the common segments:
% \begin{equation}
% \begin{split}
%     v_k =& u_{p, k}*v_{p, k} + u_{p', k}*v_{p', k} \\
%     =& \frac{d(u_{k}, p')^{\alpha}*v_{p, k}}{d(u_{k}, p')^{\alpha}+d(u_{k}, p)^{\alpha}} + \frac{d(u_{k}, p)^{\alpha}*v_{p', k}}{d(u_{k}, p')^{\alpha}+d(u_{k}, p)^{\alpha}}
% \end{split}
% \end{equation}
% where vertex with index $k$ is the comment segment of $p$ and $p'$. $v_{p, k}$ and $v_{p', k}$ are the vertices with index $k$ in part $p$ and $p'$ respectively. $d(u_k, p)$ is the shortest distance from $v_k$ to part $p$(exclude the common segment) on mesh. $\alpha$ is a hyperparameter to control smoothness, which is set as 1.3 in our impletement.
% \fi