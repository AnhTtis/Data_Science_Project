\section{Parametric Modeling}
% \section{\textit{BiCar}}
\label{sec:algorithm}
% \input{figure/fig_SMCL_process}
%It is a challenging task for a neural network to fit our cartoon models with texture due to numerous diverse data (up to $10^7$ parameters), even though the models are topologically consistent.

We propose the first pa\textbf{ra}metric model of 3D \textbf{bi}ped car\textbf{t}oon characters (\textit{RaBit}), which contains a linear blend model for shapes and a neural generator for textures. \textit{RaBit} simultaneously parameterizes the shape, pose, and texture of 3D biped characters. Specifically, we decompose the parametric space into identity-related body parameter $B$ (Sec.~\ref{shape}), non-rigid pose-related parameter $\Theta$ (Sec.~\ref{pose}) and texture-related parameter $T$ (Sec.~\ref{texture}). Overall, a 3D biped character is parameterized as follows,
\begin{equation}
\begin{split}
    M =& F(B, \Theta, T) \\
      =& F_T(F_P(F_S(B),\Theta), T),
\label{eq:1}
\end{split}
\end{equation}
where $F_S$, $F_P$, and $F_T$ are the parametric functions to generate shape, pose, and texture respectively. The following sections will elaborate on the details of \textit{RaBit}.

% where $F_S$, $F_P$, and $F_T$ are the parametric functions to generate shape, pose, and texture respectively, as illustrated in Fig.~\ref{fig_SMBL_process}. The following sections will elaborate the details in our parameterization.

\subsection{Shape Modeling}
\label{shape}
Recently, linear shape models dominate the representation of statistical 3D model. Numerous methods~\cite{SMPL-X:2019, blanz1999morphable, cao2013facewarehouse, FLAME:SiggraphAsia2017} have shown PCA's ability in modeling the human body and face. Inspired by~\cite{SMPL:2015}, we parameterize our character shape linearly with the following equation, 
\begin{equation}
    M_{S} = F_S(B) = \bar{M}_{S}+\sum_i^{|B|} \beta_i s_i,
\end{equation}
where $\bar{M}_{S}$ denotes the mean shape and ${M}_{S}$ is the reconstructed shape. The coefficients of linear shape are $\beta_i \in B$. $|B|$ is the number of shape parameters and is set to 100 in our implementation. $s_i \in \mathbb R^{3 \times N}$ denotes the orthogonal principal components of vertex displacements that capture shape variations in different character identities. The shape model of \textit{RaBit} is learned from 1,050 characters of \textit{3DBiCar} using PCA~\cite{SMPL:2015}. RaBit's eyeballs can be computed based on the predefined landmarks shown in Fig.~\ref{fig_template_design}. Please refer to the Supplementary for more details.

\subsection{Pose Modeling}
\label{pose}
\textit{RaBit} employs a standard vertex-based linear blend skinning technique, which uses the predefined skeleton and skinning weight matrix provided by \textit{3DBiCar}. The pose parameter $\Theta$ defines a set of angles as $\Theta=[\theta_1,\theta_2,...,\theta_K]\in \mathbb{R}^{69}$, where $\theta_k \in \mathbb{R}^{3}$ denotes the axis-angle representation of the relative rotation of joint $k$ with respect to its parent and $K=23$ represents the number of joints. $\theta_k$ can be converted to the rotation matrix format $R(\theta_k)$ using Rodrigues' formula~\cite{SMPL:2015}.
%The pose parameter $\Theta$ defines a set of angles as $\Theta=[\theta_1,\theta_2,...,\theta_K]\in \mathbb{R}^{69}$, where $K = 23$ represents the number of joints. The rotation of node $k$ can be expressed as $\theta_k \in \mathbb{R}^{3}$ where $\theta_k$ can be converted to rotation matrix format $R(\theta_k)$ using Rodrigues' formula. 
The following equations demonstrate how the pose function $F_P$ changes vertex $v_i \in M_{S}$ to its corresponding position ${v_i}^\prime \in M_{P}$,
\begin{equation}
    {v_i}^\prime = \sum^K_{k=1} w_{k, i} G_k'(\Theta, J) v_i,
\end{equation}
\begin{equation}
    G_k'(\Theta, J) = G_k(\Theta, J) G_k(\Theta', J)^{-1},
\end{equation}
\begin{equation}
    G_k(\Theta, J) = \prod_{j\in A(k)} 
    \left[ \begin{array}{cc}
        R(\theta_j)  & J_j \\
        0 & 1
    \end{array}\right],
\end{equation}
where $w_{k,i}$ is the $k$-th element of the linear blend matrix $W$ for the $i$-th vertex. $G_k(\Theta, J)$ is the global transformation of joint $k$, while $G_k'(\Theta, J)$ is the global transformation of joint $k$ after removing the transformation of rest pose $\Theta'$. $A(k)$ denotes a set including all the ancestors of joint $k$, and $J_j$ represents the location of the $j$-th joint, which is located at the bounding box center of a specific body landmark. Thus given the T-pose mesh $M_S$ and specific pose $\Theta$, we can generate the corresponding posed mesh $M_P$ by $F_P$,
\begin{equation}
    M_P = F_P(M_S,\Theta).
\end{equation}

% \input{figure/fig_sample_rabit}
\subsection{Texture Modeling}
Although traditional linear PCA is capable of building a decent statistical shape model, it fails to represent high-frequency details in textures and can produce blurry results due to its weak Gaussian assumption. Recently, GAN-based architectures~\cite{wang2018pix2pixHD,karras2020analyzing,karras2019style,fu2022stylegan,grigorev2021stylepeople,gecer2019ganfit} have shown the notable capability of generating high-fidelity images. Thus, we resort to StyleGAN2-based techniques for UV texture maps generation, but with a coherent UV unfolding (as shown in Fig.~\ref{fig_pipeline_svr}) to facilitate the learning of texture compared with~\cite{grigorev2021stylepeople}. Specifically, the neural texture generator in \textit{RaBit} translates a latent code to a texture map, which could be formulated as follows,
\begin{equation}
    G\left(T\right): \mathbb{R}^{Z} \rightarrow \mathbb{R}^{H \times W \times C},
\end{equation}
where $G\left(T\right)$ takes a Z-dimensional parameter vector as input and synthesizes a $H \times W\times C$ texture map. Thus given a posed mesh $M_P$ and a specific texture code $T$, we can generate a textured mesh $M$ with the following equation,
\begin{equation}
    M = F_T(M_P,T) = H(M_P, G\left(T\right)),
\end{equation}
where $H$ means the process of applying the texture map to the mesh model. In our implementation, the generator follows the architecture of StyleGAN2~\cite{karras2020analyzing},  while taking a 512-dimensional parameter vector as input and generating a $1024\times1024\times3$ texture map.

% Thus the texture parameterization consists of five convolutional generators $\mathcal{G}$ and one fusion module, namely $\math{G}_{global}$, $\math{G}_{eye}$, $\math{G}_{ear}$, $\math{G}_{nose}$, $\math{G}_{mouth}$ and $F$, respectively. Each generator translates a latent code to a texture map, which could be formulated as follows:
% \begin{equation}
%     \math{G}\left(t\right): \mathbb{R}^{Z} \rightarrow \mathbb{R}^{H \times W \times C},
% \end{equation}

% Where $\math{G}\left(t\right) \in \mathcal{G}$ takes a Z-dimensional parameter vector as input and synthesizes a $H \times W\times C$ texture map. The five generators are adopted to obtain one global texture map and four local texture maps. Then the four local texture maps are remapped $\mathcal{R}$ to the corresponding region at the global texture map to produce an integrated texture. However, such direct merged texture suffers from artifacts at the local boundaries, as shown in our ablation study. Thus we adopt the image-to-image translation technique as our fusion module $\mathcal{F}$ to prevent artifacts on the integrated texture. Overall, the texture parameterization $F_T$ in \textit{BiCar} is computed as follows:
% \begin{equation}
% \begin{array}{c}
% F_T(T)=\mathcal{F}\left[\mathcal{R}\left(\bigcup_{\math{G} \in \mathcal{G}} \math{G}(T)\right)\right]
% \end{array}
% \end{equation}

% In our implementation, the generators follow StyleGAN2~\cite{karras2020analyzing} and the fusion module pix2pixHD~\cite{wang2018pix2pixHD}. $\mathcal{G}_{global}$ takes a 512-dimensional parameter vector as input and synthesizes a $1024\times1024\times3$ global texture map, while the others take a 256-dimensional vector as input and generate a $256\times256\times3$ local texture map respectively. $F$ takes a $1024\times1024\times3$ image as input and generate a refined $1024\times1024\times3$ image.

% \input{figure/fig_texture_decoder}
    % Texture is another significant part of model parameterizing. Generative adversarial network (GAN) has already shown its strong capacity to generate realistic images such as StyleGAN\cite{karras2019style}. Adopting its strength, we cope the texture parameterization with a generative model, which aims to mapping latent code $T$ to a UV map for texture. Given a generative network $G$, the learned distribution of the latent code $T$ is the parameterization of the model texture.

% %With a generator, the model contains mapping and synthesis networks.

% Specifically, we consider a mapping network and a synthesis network for UV generation. The former mapping network strengthens the latent code $T$ with MLP to reduce correlation among weights. The latter synthesis network generates the texture map by applying adaptive instance normalization(AdaIN) \cite{huang2017arbitrary} with the output of the mapping network.  
% \begin{equation}
% \operatorname{AdaIN}\left(\mathbf{x}_i, \mathbf{y}\right)=\mathbf{y}_{s, i} \frac{\mathbf{x}_i-\mu\left(\mathbf{x}_i\right)}{\sigma\left(\mathbf{x}_i\right)}+\mathbf{y}_{b, i}
% \end{equation}

% We can forward a latent code with a trained generator and get a specific UV map. Meanwhile, given a UV map, we can fix the networks and optimize the distance between the current and target UV map to extract the corresponding latent code $T$. In this case, texture parameterization $T$ in \textit{BiCar} is obtained.

\label{texture}