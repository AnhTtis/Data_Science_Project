\input{figure/fig_pipeline}

\section{Single View Reconstruction}
\label{sec:svr}
%In this section, we do several experiments to show the capacity of our dataset. 
%\subsection{Shape Morphing}

% This application provides a baseline for single view reconstruction by a single general character image.
In this section, we propose a baseline for single view reconstruction based on \textit{SMCL} and \textit{3DBiCar}. We can generate realistic cartoon models from a single image with a 2D cartoon character, even for a simple watercolor painting from children. Our baseline of single review reconstruction is shown in Fig. \ref{reconstruction:pipeline}. We first extract the cartoon character from the paired image. Given this extracted character, the encoder will map this input to low-dimensional latent parameters with two CNNs and three MLPs. We use \textit{3DBiCar} as supervision to train these networks. Next, the latent codes containing shape, pose, and texture parameters will be utilized for \textit{SMCL} to build a vivid 3D cartoon character. Notice that our \textit{SMCL} also address the eyes modeling besides the shape, pose, and texture.

\subsection{Mesh Reconstruction}
% ? Many methods for single-view human reconstruction %have a strong understanding of rely on key points, which derives from mature upstream algorithms (e.g., dense human pose estimation) or massive datasets (e.g., COCO including more than two hundred thousand people).

%However, the extreme scarcity of datasets and diversification of styles make it difficult for the neural network to %understand fit key points. 


%Input images are first encode by Resnet50 and the output feature of Resnet is processed by two decoupled MLPs to produce parameters of shape and pose respectively. Posed textured models are generate with \textit{SMCL} and then supervised by ground truth model.

% The loss functions are as follows:
For mesh reconstruction, we first design following loss functions to optimize shape parameter $B$ and pose parameter $\Theta$:

\begin{equation}
    L_{para} = \|B_{pred}-B_{gt} \|_1 + \|\Theta_{pred} - \Theta_{gt}\|_1
\end{equation}
where L1 distance serves relatively robust supervision in our dataset with diverse samples.
%In our dataset, the number of instance is relatively small, and the diversity is large, so we use L1 loss for a more robust result.

Next, to ensure generated shape close to the ground truth model, $L_{shape}$ for shape supervision is introduced: 
\begin{equation}
    L_{shape} = \| F_P(\Theta_{gt}, B_{pred}) - F_P(\Theta_{gt}, B_{gt}) \|_2
\end{equation}
where the L2 distance between each vertex is counted in two models. $\Theta_{gt}$ is adopted to consentrate on the shape parameter $B$ and avoid mutual influence of $\Theta$. 

Following the similar idea, $L_pose$ is proposed to supervise pose parameter $B$ by minimizing the distance between joints: %Using $\Theta_{gt}$ and $B_{pred}$ to predict a model for loss calculation aims to prevent mutual influence and urge for a milder and more accurate backward propagation.
\begin{equation}
    L_{pose} = \| F_J(F_P(\Theta_{pred}, B_{gt})) - F_J(F_P(\Theta_{gt}, B_{gt})) \|_2
\end{equation}
where $F_J:\mathcal{R}^{N \times 3} \rightarrow \mathcal{R}^{J \times 3}$ is the function to extract the position of joints from the model. %(please refer to Sec. \ref{template}. Similar as $L_{pose}$, $\Theta_{pred}$ and $B_{gt}$ to predict a model for loss calculation.
Finally, the loss function for mesh reconstruction is the combination of aformentioned terms:

\begin{equation}
    L = L_{para} + \lambda_s L_{shape} + \lambda_p L_{pose}
\end{equation}
where $\lambda_s$ and $\lambda_p$ are 1 during experiments.

We find it hard to fit the parameters of eye with only a few features from limited images. Thus $ratio_{ed}$ and $ratio_{eR}$ are set to be the average value. This setting generates reasonable eye reconstruction with few artifacts.
%For eye reconstruction, the neural network is hard to extract such small features from images with limited data. Thus we set $ratio_{ed}$ and $ratio_{eR}$ as the average of those in the training set. With such operations, reasonable eyeballs can be generated without apparent artifacts.




\subsection{Texture Reconstruction}
%In Fig. \ref{reconstruction:pipeline}, shows our
% The input images is first encoded by Resnet50 and then processed by MLP (the structure is shown in Fig. \ref{reconstruction:pipeline}). 
To reconstruct the texture from a given image $x$, we need to fit an Encoder $E$ to get texture parameter with the generator $G$ in \textit{SMCL}. 

The loss function consists of two parts. The first part $L_{pw}$ denotes the pixel-wise L2 loss between generated UV with input image $x$ and ground truth $UV_{gt}$.
\begin{equation}
    L_{pw}= \|UV_{gt} - G(E(x)) \|_2
\end{equation}

% where $E(\cdot)$ denotes encoder, $G(\cdot)$ denotes texture generator of \textit{SMCL}, of which parameters are fixed during training encoder.
The second part follows LPIPS\cite{zhang2018unreasonable} to learn perceptual similarities.
\begin{equation}
    L_{\mathrm{LPIPS}} = \|F(x) - F(G(E(x))) \|_2
\end{equation}
where $F(\cdot)$ denotes the perceptual feature extractor. In the end, the loss function for texture reconstruction is as follows:

\begin{equation}
    L_{UV} = L_{pw} + \lambda L_{\mathrm{LPIPS}}
\end{equation}
where $\lambda$ is constant 1 in our experiments.

\iffalse
To support texture generation from a single image, we need an encoder.
Given an image as input, we need an encoder to find the corresponding latent code so that we can generate 
Although the model of styleGAN is powerful in generating texture maps, the input of styleGAN is a random latent code, which means it can only generate random texture maps. So, it is challenging to generate the specific texture map given the carton image in the wild. 


We use Resnet as the encoder. The loss function is as follows:

\begin{equation}
    L = L_{2}(x) + L_{LPIPS}(x)
\end{equation}
Similar to pixel2style2pixel, we use pixel-wise L2 loss for an overall robust result.
\begin{equation}
    L_{2}(x) = \|x - G(x) \|_2
\end{equation}
Where $G(\cdot)$ denotes the generated texture map by our decoder.

Then, we add the LPIPS loss to improve the result.
\begin{equation}
    L_{LPIPS}(x) = \|F(x) - F(G(x)) \|_2
\end{equation}
Where $F(\cdot)$ denotes the perceptual feature extractor.

We do not use the identity loss since there is a difference between our texture maps and faces.
\fi


