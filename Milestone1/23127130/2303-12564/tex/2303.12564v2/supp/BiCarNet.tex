\section{Details of \textit{BiCarNet}}

\noindent\textbf{Data Preparation.} 
We split \textit{3DBiCar} into a training set (1,050 image-model pairs) and a testing set (450 pairs). To support a stable training of \textit{BiCarNet}, we augment a large number of synthetic paired data with the help of \textit{RaBit}. Specifically, we generate a series of shape vectors by interpolating between the 1,050 models' shape parameters. Fig.~\ref{fig_interpolation} shows the representative results of interpolated shapes. For pose augmentation,  a variety of poses from other datasets (e.g., Human3.6M\cite{Ionescu2014Human36M}) are retargeted to \textit{RaBit}'s pose space, as shown in Fig.~\ref{fig_poseaug}. Furthermore, 1,050 raw textures are also utilized to generate synthetic texture maps by interpolating with \textit{RaBit}, as shown in Fig.~\ref{fig_uv_interpolation}. The above augmentations finally produce 13,650 models with texture and pose. These models are then rendered into images from different camera views for training. 

\input{supp_figure/fig_interpolation}
\input{supp_figure/fig_poseaug}
\input{supp_figure/fig_uv_interpolation}



\noindent\textbf{Implementations.} In our implementation, for the shape and pose regression modules, we utilize two ResNet-50 blocks to embed the input image ($512 \times 512 \times 3$) to a 100-dimensional shape vector and a 69-dimensional pose vector, respectively. For the texture module, we adopt pSp-encoder~\cite{richardson2021encoding} to learn a 512-dimensional texture vector from the image. As for the part-sensitive texture reasoner, we use pSp~\cite{richardson2021encoding} as the basic building block and learn multiple local UV textures ($256 \times 256 \times 3$) from the input. pix2pixHD~\cite{wang2018pix2pixHD} is employed as the fusion module (Fuser), which takes the $1024 \times 1024 \times 3$ coarsely-blended texture map as input and outputs fine texture maps with the same resolution.

\noindent\textbf{Part-Sensitive UVs.} As shown in Fig.~\ref{uv_style}, we design five individual UV-mappings for significant parts, i.e., nose, ears, horns, eyes, and mouth. These part UVs enlarge five constant regions of the global UV mapping. Five lightweight encoder-decoder branches are adopted to learn the appearances of these local regions from the input image, respectively. The learned part UVs could then be remapped to their corresponding areas on the global UV map, resulting in a blended texture.

% More detailed descriptions will be included in the revision. Here, we illustrate the part UV layouts and textures in the left part of Fig.~\ref{uv_style} to give a brief explanation. These part UVs enlarge five constant regions of the global UV mapping. 

\input{rebuttal_figure/fig_uv}