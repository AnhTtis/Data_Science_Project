\section{Single-View Reconstruction}
\label{sec:experiment-svr}
\input{figure/fig_pipeline_svr}

Single-view reconstruction (SVR) is one of the most popular tasks of efficient 3D content generation, and recent work has made noticeable progress on human reconstruction based on parametric model of human characters (e.g., SMPL). To verify the practicality of our proposed \emph{3DBiCar} and \emph{RaBit}, we conduct SVR for bipled cartoon characters. A baseline learning-based method is presented, which is termed as \textit{BiCarNet}. 

\subsection{\textit{\textbf{BiCarNet}}}

Given a single masked image of cartoon characters, our \textit{BiCarNet} aims to reconstruct the corresponding 3D shape, pose, and texture. %As \textit{Rabit} spans a large space of textured models. 
The key problem is to build an encoder to map the input image to the parametric space of \textit{Rabit}. As shown in the upper part of Fig.~\ref{fig_pipeline_svr}, we adopt the learning block in HMR~\cite{kanazawa2018end} as our Encoder. Once these parametric vectors are learned, we can feed them to our \textit{RaBit} model to generate a posed body mesh, two eyeballs, and a UV texture (we name it global for convenience to introduce the following method description). 

During our preliminary experiments, we find that the shape reconstruction of characters, \ie the eyes and body, is satisfactory, while the inferred UV tends to lose detailed appearances of some small yet significant areas, such as the nose and ears. We thus adopt a part-sensitive texture reasoner (PSR) to address the above issue, as the lower part of Fig.~\ref{fig_pipeline_svr} shows. Specifically, we design five individual UV-mappings for these significant parts of the nose, ears, horns, eyes, and mouth. Five lightweight encoder-decoder branches are next introduced to learn the appearances of these local regions from the input image, respectively. The learned part UVs could be remapped to the corresponding area on the global UV map to produce a blended texture. However, a direct blending tends to cause seam artifacts. Thus we further adopt a Fuser to address the artifacts as Fig.~\ref{fig_pipeline_svr} illustrates. Please refer to the Supplementary for thorough implementations of \textit{BiCarNet}.

\subsection{Experiments}

\textbf{Data preparation.} We first split \textit{3DBiCar} into a training set (1,050 image-model pairs) and a testing set (450 pairs). To support a stable training of \textit{BiCarNet}, we next generate a large number of synthetic paired data with the help of \textit{RaBit}, which are highly diversified in shape, posture, and texture. To be specific, a lot of 3D textured models are first sampled from the \textit{RaBit} space, which are then rendered into images from different camera views. 
This finally produce 13,650 pairs for training. Note that, \emph{BiCarNet} takes an image with foreground masked as input. All synthetic images naturally have no background. For real images, all the foreground masks are manually annotated. 

\textbf{Result gallery.} 
Fig.~\ref{fig_wildresult} shows representative results generated by \textit{BiCarNet}. As illustrated, our \textit{BiCarNet} can generate vivid 3D cartoon characters loyal to individual cartoon images in shape, pose, and texture. We believe that our work opens the door to producing 3D biped cartoon characters from easy-to-obtain inputs.
\input{figure/wild_result}


\textbf{Results on Shape Reconstruction.} 
As mentioned above, \textit{BiCarNet} utilizes HMR-like blocks and \textit{RaBit} for shape and pose learning. Currently, other reconstruction methods could also be used for topology-consistent geometry inference, such as GCNN-based methods~\cite{lin2021-mesh-graphormer} and UV-based methods~\cite{zeng20203d}. We choose two representative methods for comparison, i.e., Mesh-Graphormer~\cite{lin2021-mesh-graphormer,lin2021end-to-end} and DecoMR~\cite{zeng20203d}. Mesh-Graphormer combines graph convolutions and self-attentions in a transformer for 3D human reconstruction from a single image. DecoMR reconstructs 3D human mesh from single images by regressing a UV-based location map. Tab.~\ref{tab_mesh_result} shows the quantitative comparisons of the above three methods on MPVE, MPJPE, and PA-MPJPE. We also provide qualitative comparisons in Fig.~\ref{fig_mesh_result}. Both quantitative and qualitative results demonstrate that the HMR-like method achieves the highest performance on geometry inference and provides more accurate reconstructions closer to ground truths. As noted, both Mesh-Graphormer and DecoMR outperform HMR for SMPL-based human reconstruction. It is interestingly found that they perform worse in our settings. One possible reason is that our biped cartoon meshes own an extremely larger amount of vertices than SMPL to model more complex geometry. This greatly increases the challenge of vertices regression in Mesh-Graphormer and DecoMR. Thus, in our setting, directly regressing the low-dimension parameters performs better. 


\input{table/table_mesh_result}
\input{figure/fig_mesh_result}

\textbf{Results on Texture Inference.} 
To demonstrate the capability of our proposed GAN-based texture generator, we first compare our \emph{RaBit}-based texture inference (i.e., \emph{BiCarNet}) with PCA-based inference. Specifically, for PCA-based method, we utilize the same learning architecture to map the input image into the PCA-based texture space. Furthermore, %with the traditional PCA-based method. Specifically, 
to evaluate the effectiveness of the proposed texture inference modules, we also conduct ablative analysis on \textit{BiCarNet} without Fuser and \textit{BiCarNet} without Part-sensitive Reasoner (PSR). Table~\ref{tab_texutre_result} shows the quantitative results of different texture inference methods on MSE, PSNR and FID and our proposed method achieves the highest scores compared with all other methods. Moreover, Fig.~\ref{fig_texture_result} illustrates the qualitative results of these methods for texture inference. Fig.~\ref{fig_texture_fusion} shows the results without and with Fuser, which demonstrates our fusion module can deal with unnature seam-like artifacts. We can observe that the part-sensitive texture reasoner and the Fuser help to capture the local regions of characters and recover their detailed appearances. 

\input{table/table_texture_result}
\input{figure/fig_texture_result}
\input{figure/fig_texture_fusion.tex}

\section{More Applications}
\label{sec:application}
\subsection{Sketch-based Modeling}
Customizing 3D biped cartoon characters usually requires a heavy workload with commercial tools, even for experienced artists. Sketch-based modeling enables amateur users to get involved in 3D shape customization in a simple and intuitive fashion. We build a sketch-based modeling application with the help of \textit{3DBiCar} and \textit{RaBit}. 

We first sample a series of shape vectors randomly and feed them to \textit{RaBit} to generate 3D cartoon characters with diversified shapes, resulting in 12,000 T-pose models. Then we apply suggestive contour~\cite{han2017deepsketch2face} to render the front-view sketches with different abstraction levels and obtain 108,000 sketch-model pairs. Given a sketch as input, we employ ResNet-50 and three MLPs as the encoder and decoder to map the input sketch to 100-dimensional shape parameters. The generated shape parameters are next fed to \textit{RaBit} to reconstruct the corresponding 3D model. Please refer to the Supplemental materials for more details. Note that users only need to depict a 3D character with T-pose on a 2D canvas while the output characters of our method are animation-ready and can be directly applied to other commercial tools. Fig.~\ref{fig_sketch} displays the sketches created by users with little knowledge of modeling as well as the corresponding models generated by our system. It can be seen that our sketch-based modeling system offers a smart approach for amateur users to create 3D biped cartoon characters with diversified shapes. We will further explore the support of shape reconstruction from  sketches with arbitrary poses, and texture painting in the future.
 


\input{figure/sketch_result}

\subsection{3D Character Animation}
Following the recent advance of human recovering method and parametric model~\cite{wang2022live,SMPL:2015,SMPL-X:2019}, we extract the human from input video frames and adopt a temporal-aware encoder to recover the sequence of human poses~\cite{wang2022live}. Then, a motion retargeting method~\cite{hsieh2005motion} is used to convert the poses on the human skeleton to the motion of our cartoon characters. As shown in Fig.~\ref{fig_anim}, animation-ready characters generated by our \textit{RaBit} can be directly applied to 3D animation. Please refer to the supplementary for animation video.

\input{figure/fig_anim}
