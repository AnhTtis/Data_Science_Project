\section{Related Work}
\label{sec:related_works}
\noindent
\textbf{3D Character Datasets.} In general, 3D character datasets could be categorized as real-captured and computer-designed datasets. For capturing character data from the real world, the availability of 3D scanning devices has enabled researchers to collect many scanned 3D human-related datasets, mainly focusing on human faces~\cite{brunton2014review} and bodies~\cite{cheng2018parametric}. For human faces,  FaceWarehouse~\cite{cao2013facewarehouse} collects large-scale 3d faces with high diversity in age, ethnicity, and expression. FaceScape\cite{yang2020faceScape} further builds a large-scale detailed 3D face dataset with high resolution in texture and mesh. For human bodies, CAESAR dataset~\cite{robinette2002civilian} opens up the learning of the human body and is widely used for body shape modeling for its shape diversity and satisfying resolution of meshes. Many following works~\cite{anguelov2005scape,faust:CVPR:2014,yang2014spring,saint2018-3dBodyTex,Zheng2019DeepHuman} extend ~\cite{robinette2002civilian} in shape, pose, and texture, on quantity and quality. Although these real-captured datasets are widely used in realistic human digitalization, they are unsuitable for imaginary character generation.

For designing character data with computers, researchers try to perform deformation on real 3D human faces or bodies to construct exaggerated shapes programmatically~\cite{sela2015computational,han2017deepsketch2face,cai2021landmark,wu2018alive}. Their results lack diversity and are far from satisfactory. To address this, 3DCaricShop~\cite{qiu20213dcaricshop} proposes a large-scale 3D exaggerated faces dataset. SimpModeling~\cite{luo2021simpmodeling} constructs a large man-made animalmorphic head dataset. Although using 3DCaricShop and SimpModeling could facilitate the generation of unreal character heads, it still remains a problem to synthesize full-body cartoon characters due to the lack of corresponding body data. In our work, we tackle this problem by introducing a large 3D biped cartoon character dataset, \textit{3DBiCar}. %It contains 1,500 high-quality 3D full-body textured models and spans a wide range of biped cartoon characters.

\noindent
\textbf{Parametric Shape Modeling.} Parametric models of shapes are widely used in 3D digitizations. Blanz \textit{et~al.}~\cite{blanz1999morphable} pioneer parametric modeling using PCA and release a 3D statistical morphable face model (3DMM). Their parameterization models textured faces and provides a set of controls for intuitively manipulating shapes, expressions, and textures. Since then, PCA-based parameterizing has gradually dominated the area of statistical shape modeling over the past decades. Following 3DMM, researchers model the whole head to represent the neck region and 3D head rotations~\cite{cao2013facewarehouse, FLAME:SiggraphAsia2017}. Allen \textit{et al.}~\cite{allen2003space} open up the study of full body parameterization. However, they focus only on body shape and omit the body pose. SCAPE~\cite{anguelov2005scape} represents body shape and pose in terms of triangle deformations, while SMPL~\cite{SMPL:2015} models a whole range of natural shapes and poses based on vertex displacements. SMPL-X~\cite{SMPL-X:2019} integrates SMPL~\cite{SMPL:2015} with  FLAME~\cite{FLAME:SiggraphAsia2017} head model and the MANO~\cite{MANO:SIGGRAPHASIA:2017} hand model for expressive capturing of bodies, hands and faces together. With recent advances in deep learning, researchers turn to explore nonlinear shape models using neural networks~\cite{abrevaya2018multilinear,bagautdinov2018modeling,ranjan2018generating,bouritsas2019neural,xu2020ghum,zhou2020fully}. However, since these non-linear modeling methods are inferior in simplicity, robustness and availability, PCA-based methods remain prevalent in the research community. In this paper, we also adopt PCA into \textit{RaBit} to model the geometry of 3D biped cartoon characters.

Based on the above parametric models, researchers have made remarkable progress in human digitization, such as reconstruction from simple inputs (e.g., a single image or sparse sketches)~\cite{bogo2016keep,kanazawa2018end,pavlakos2018learning,choutas2022accurate,han2017deepsketch2face} and real-time pose retargeting~\cite{choi2021beyond,wang2022live,kocabas2020vibe}. For instance, SMPLify~\cite{bogo2016keep} estimates 3D body shape and pose parameters automatically from 2D joints with multiple ellipsoids. HMR~\cite{kanazawa2018end} proposes an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. DeepSketch2Face~\cite{han2017deepsketch2face} proposes a sketch-based system for inferring 3D face models from 2D sketches with the help of parametric models and CNN-based deep regression networks. TCMR~\cite{choi2021beyond} presents a temporally consistent mesh recovery system for recovering smooth 3D human motion from monocular videos. To probe the capability of \textit{RaBit} to downstream applications, we conduct various  utilization, such as single-view cartoon character reconstruction, sketch-based character modeling, and 3D cartoon animation. Experimental results demonstrate the practicality of \textit{3DBiCar} and \textit{RaBit}. 

\noindent
\textbf{Parametric Texture Modeling.}
Traditionally, textures are modeled as a linear subspace using the similar idea of body blendshape models. Blanz \etal~\cite{blanz1999morphable} represent the face appearance in per-vertex colors and parameterize texture as a linear model based on PCA. Dai \textit{et~al.}~\cite{dai20173d} store texture information in a UV space where the texture resolution is unconstrained by mesh resolution. Moschoglou \textit{et~al.}~\cite{moschoglou2018multi} formulate a robust matrix factorization problem to learn the parametric representation of facial UV maps from a collection of training textures. However, these linear texture models may lead to a sub-optimal appearance output~\cite{egger2016copula,han2012semiparametric} due to the weak assumption of Gaussian and tend to produce blurry results.

With recent advances in deep learning, researchers turn to utilize deep neural networks to model texture. A number of deep generative models~\cite{li2020learning,slossberg2018high,deng2018uv,gecer2019ganfit,grigorev2021stylepeople,fu2022stylegan,oechsle2019texture,gao2022get3d} have been proposed to parameterize texture into a latent space. For example, GANFIT~\cite{gecer2019ganfit} utilizes GAN-based neural networks to train a generator of facial texture in UV space for 3D face reconstruction. StylePeople~\cite{grigorev2021stylepeople} incorporates neural texture synthesis, mesh rendering, and neural rendering into the joint generation process to train a neural texture generator for the task of single-view human reconstruction. GET3D~\cite{gao2022get3d} introduces a texture-field generative model that directly generates explicit textured 3D meshes, ranging from cars, chairs, animals, motorbikes, and human characters to buildings. These methods have shown the promising capacity of neural generators to represent texture. In our work, we adopt a GAN-based neural texture generator into \textit{RaBit} to provide high-quality texture modeling. %Furthermore, in the task of single-view reconstruction, we design a part-sensitive texture reasoner to make all important local appearances perceived.

%Furthermore, we propose integrating cumulative local UV mappings to enhance the local details in texture inference from a single-view image.