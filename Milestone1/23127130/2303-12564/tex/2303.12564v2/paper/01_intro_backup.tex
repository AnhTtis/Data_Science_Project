\section{Introduction}
\label{sec:intro}

With the rapid development of digitization, creating high-quality 3D articulated characters is highly demanded in game platforms, film industries, and metaverse applications. However, even for expert artists, digitizing a character in 3D is still labor-intensive and time-consuming. How to assist people in producing visually plausible 3D content efficiently has always been a fundamental research topic in computer vision and computer graphics. It is thus essential for building a generalized character parameterization for 3D content creation.

A biped character parameterization is of crucial generalization direction. First, biped character control is one of the most basic research tasks in 3D animation~\cite{coros2010generalized, sok2007simulating, lee2010data}. Secondly, the biped model for posing and animation is indispensable for the most popular 3D studio tools, such as Blender, Maya, 3DS Max, \etc. Last but not least, current character parameterization has made a great effort on human digitalization~\cite{cheng2018parametric, robinette2002civilian, sela2015computational,han2017deepsketch2face,cai2021landmark,wu2018alive} from the real world but none of these works focuses on the fictitious biped cartoon character digitization, which is 
currently in great demand in the area of gaming, filming and virtualizing. Therefore, it is imperative to build a high-quality 3D biped cartoon character parametric model for generalized 3D character digitalization. 

To implement prevailing learning-based parameterization for fictitious biped characters, a comprehensive cartoon dataset with both topological-consistent geometry and corresponding texture is necessary. Advanced 3D sensing devices make capturing 3D data from the real world convenient and further prompt a growing number of 3D people repositories~\cite{anguelov2005scape, ionescu2013human3, SMPL:2015, pishchulin2017building, STAR:2020}. Thanks to those human datasets, it is already possible to infer nearly accurate 3D digital avatars from single-view images with the help of parametric models~\cite{SMPL:2015, SMPL-X:2019, STAR:2020} and data-driven techniques~\cite{bogo2016keep,kanazawa2018end,pavlakos2018learning,choutas2022accurate}. Recent deep neural networks~\cite{kanazawa2018end, kolotouros2019spin, SMPL-X:2019} have achieved excellence in the task of 3D human reconstruction, significantly increasing efficiency in modeling 3D human-related content. 

Still, constructing a 3D fictitious cartoon dataset simultaneously considering shape, pose, and texture is quite difficult. There are two challenges in collecting and building such a dataset. First, 3D cartoon characters available online are always topologically inconsistent and lack corresponding textures, thus hard to be utilized for many fundamental tasks, such as parametric modeling, pose retargeting, and texture synthesis. Meanwhile, collecting corresponding paired images or skinned skeletons of random shared 3D models is nearly impossible, hindering them from directly applying in single-view reconstruction or animation generation tasks. Second, diversified species, complex geometric shapes, and variegated texture styles of cartoon characters have brought additional challenges in building a 3D textured and skinned cartoon character dataset.

In the beginning, we address the scarcity of data by introducing \textit{3DBiCar}, the first large-scale publicly available 3D biped cartoon character dataset. Firstly, \textit{3DBiCar} spans a wide range of 3D biped cartoon digitalization sources with topological-consistent geometry. It contains 1646 high-diversified 2D biped cartoon character images, and corresponding high-quality 3D models with textures containing 17 species and 4 image styles. Secondly, all the 3D models in \textit{3DBiCar} are animation-ready and unified in topology. The artists were asked to craft 3D models using a pre-defined template with several landmarks and rig them with a consistent skeleton. It paves the way for learning a skinned parametric geometric model to serve several vital tasks in visual computing such as shape reconstruction, pose tracking and character animation. Thirdly, since all 3D meshes in \textit{3DBiCar} are topologically uniform, each model can be unwrapped into a unified UV map and manually painted by artists according to the paired image. 

Next, with \textit{3DBiCar}, a 3D cartoon character parametric model \textit{BiCar} is further proposed to model the 3D fictitious biped character encoding shape, pose, and texture at the same time for generalized character digitization. \textit{BiCar} is a statistical model for 3D cartoon character generation, which embeds shape, pose, and texture to a low-dimensional parametric space simultaneously. For shape and pose modeling, with pioneering human parameterization~\cite{SMPL:2015}, numerous methods~\cite{MANO:SIGGRAPHASIA:2017, SMPL-X:2019, STAR:2020} have been proposed to learn 3D human model representation from scan data, and these parametric models are widely used for digitizing humans from input images~\cite{kanazawa2018end, kolotouros2019spin, SMPL-X:2019}. However, straightly adapting these blendershape approaches to \textit{3DBiCar} fails to reconstruct high-fidelity models texture from low-dimension parameters due to the diversity and complexity texture of cartoon characters, especially their heads. We tackled this problem by introducing a non-linear generative model parameterization. Current texture modeling benefiting from deep representation learning advances, several works~\cite{bengio2013representation, kingma2013auto, karras2019style, karras2020analyzing} served well in embedding images to a low-dimension latent space. Inspired by image domain transfer methods~\cite{karras2020analyzing, wang2018pix2pixHD, karras2019style}, we encode texture into a latent parametric space and adopt a UV generator to parameterize texture.

Furthermore, we conduct comprehensive experiments to evaluate the capability of \textit{BiCar}. Specifically, to probe the potential of \textit{BiCar},  we implement exhaustive applications, including single-view cartoon character reconstruction, 3D cartoon model editing, sketch-based character modeling, and 3D cartoon animation. Qualitative results demonstrate the superb usability of our proposed parametric model \textit{BiCar}.

To summarize, our contribution can be listed as follows:
\begin{itemize}
    \item We propose the first 3D cartoon parametric model \textit{BiCar} for biped character digitizations. Our parameterization models the shape, pose, and texture of the 3D fictitious biped character simultaneously. Moreover, for texture parameterization, we adopt a novel UV generator to encode texture into the parametric space. 
    \item We introduce \textit{3DBiCar}, the first large-scale publicly available 3D biped cartoon character dataset. It spans a great range of 3D biped cartoon digitalization sources. Our data contains 1,646 high-diversified 2D biped cartoon character images and corresponding high-quality 3D models with topological-consistent geometry as well as detailed textures.
    \item We conduct various applications on \textit{BiCar} to demonstrate the promising potential of our biped cartoon parametric model in single-view cartoon character reconstruction, 3D cartoon model editing, sketch-based character modeling, and 3D cartoon animation.
\end{itemize}

%In the rest of the paper, we first review some related works in the domain of shape reconstruction and image synthesis in Sec.~\ref{sec:related_works}. Next, Sec.~\ref{sec:dataset} presents the procedure of constructing \textit{3DBiCar}, while the following Sec.~\ref{sec:algorithm} elaborates the backend algorithm for building \textit{SMCL}. In Sec.~\ref{sec:application}, we demonstrate several applications showcasing the potential of \textit{3DBiCar} and \textit{SMCL}. Finally, we make a conclusion and discussion on this work in Sec.~\ref{sec:conclusion}.

