\vspace{-0.5cm}
\section{Introduction}
\label{sec:intro}

With the rapid development of digitization, creating high-quality 3D articulated characters is highly demanded in game platforms, film industries, and metaverse scenarios. However, even for expert artists, creating a 3D character is labor-intensive and time-consuming. Therefore, reducing the cost of producing visually plausible 3D characters is essential in the field of computer vision and graphics.

Recently, researchers have made great progress in digitizing realistic human characters. The emergence and popularity of various 3D sensing devices make capturing 3D data from the real world convenient, prompting a growing number of 3D real-people scanned datasets~\cite{cao2013facewarehouse,yang2020faceScape,robinette2002civilian,anguelov2005scape,faust:CVPR:2014,yang2014spring,saint2018-3dBodyTex,Zheng2019DeepHuman}. Based on these large-scale datasets, several powerful parametric models~\cite{SMPL:2015,SMPL-X:2019,blanz1999morphable,cao2013facewarehouse} have been developed to facilitate the reconstruction and analysis of human shapes, actions, and interactions. With the help of parametric models, deep learning techniques have shown the potential to efficiently infer accurate 3D digital humans from single-view images~\cite{SMPL-X:2019,kanazawa2018end} or even sparse sketches~\cite{han2017deepsketch2face,brodt2022sketch2pose,unlu2022interactive}. Most recently, there are some works~\cite{qiu20213dcaricshop,luo2021simpmodeling} that devote to exploring the intelligent generation of cartoon-like character heads. However,
none of the prior works focuses on the modeling of 3D full-body biped cartoon characters, which are also in great demand in the area of gaming (e.g., Animal Crossing), filming (e.g., Zootopia), and virtualizing (e.g., Metaverse). In this work, we raise a new problem to the community: \emph{How to quickly produce 3D biped cartoon characters from easy-to-obtain inputs (e.g., a single image)?}

%How to quickly produce 3D biped cartoon characters from simple input, e.g., a single image or sparse strokes, still be an open problem that needs to be solved urgently.

%Thus, creating 3D biped cartoon characters from simple input, e.g., a single image or sparse strokes, is of crucial topic for 3D character digitization.

Revisiting the road map of realistic human digitization, the first step to tackling the above problem is building a high-quality 3D biped cartoon characters dataset. We thus introduce \textit{3DBiCar}, the first large-scale publicly available 3D biped cartoon character dataset following three criteria: \textit{1) \textbf{Diversity.}} \textit{3DBiCar} spans a wide range of 3D biped cartoon characters, containing 1,500 high-quality 3D models covering 15 species, as shown in the Fig.~\ref{fig_datainfo}. \textit{2) \textbf{Richness.}} Each model in \textit{3DBiCar} owns not only a detailed shape but also a texture UV-map, which are matched with a reference image. Additionally, each character is attached with two models, one with T-pose and another with the reference pose. \textit{3) \textbf{Topological-consistency.}} Each 3D model is created by carefully deforming a pre-defined template mesh. All 3D characters in \textit{3DBiCar} are unified in topology, paving the way to learn a skinned parametric model. Fig.~\ref{fig_teaser} shows some representative models of the proposed dataset.
%Therefore, \textit{3DBiCar} provides the shape, pose, texture map, and the corresponding 2D reference image for each model simultaneously, which could be directly applied to several vital tasks in visual computing such as single-view reconstruction, pose tracking, and texture synthesis.

Based on \textit{3DBiCar}, we further propose a generative model, dubbed \textit{RaBit}, for 3D biped cartoon character generation. It combines a linear blend shape model with a neural texture generator and simultaneously parameterizes the shape, pose, and texture to a low-dimensional parametric space. For shape and pose modeling, numerous methods have shown principal component analysis's (PCA's) advantage %super ability 
in building decent statistical shape models~\cite{SMPL-X:2019, blanz1999morphable, cao2013facewarehouse, FLAME:SiggraphAsia2017}. Inspired by SMPL~\cite{SMPL:2015}, we utilize the traditional PCA technique to parameterize shape. Due to the variety and complexity of cartoon texture, directly adopting PCA for texture modeling fails to reconstruct details and falls into blurry results. We tackle this problem by introducing a StyleGAN-based generator. 

% Next, with \textit{3DBiCar}, a 3D cartoon character parametric model \textit{BiCar} is further proposed to model the 3D fictitious biped character encoding shape, pose, and texture at the same time for generalized character digitization. \textit{BiCar} is a statistical model for 3D cartoon character generation, which embeds shape, pose, and texture to a low-dimensional parametric space simultaneously. For shape and pose modeling, with pioneering human parameterization~\cite{SMPL:2015}, numerous methods~\cite{MANO:SIGGRAPHASIA:2017, SMPL-X:2019, STAR:2020} have been proposed to learn 3D human model representation from scan data, and these parametric models are widely used for digitizing humans from input images~\cite{kanazawa2018end, kolotouros2019spin, SMPL-X:2019}. However, straightly adapting these blendershape approaches to \textit{3DBiCar} fails to reconstruct high-fidelity models texture from low-dimension parameters due to the diversity and complexity texture of cartoon characters, especially their heads. We tackled this problem by introducing a non-linear generative model parameterization. Current texture modeling benefiting from deep representation learning advances, several works~\cite{bengio2013representation, kingma2013auto, karras2019style, karras2020analyzing} served well in embedding images to a low-dimension latent space. Inspired by image domain transfer methods~\cite{karras2020analyzing, wang2018pix2pixHD, karras2019style}, we encode texture into a latent parametric space and adopt a UV generator to parameterize texture.

To explore the practical usage of \textit{3DBiCar} and \textit{RaBit}, we first conduct the application of single-view reconstruction. Considering prior works for SMPL-based human geometry generation from single-view images\cite{kanazawa2018end,kolotouros2019spin,bogo2016keep}, we build a baseline method with our dataset and the parametric model. We select one regression-based method for pose and shape inference. For texture inference, we find directly applying a global texture-generator tends to make the results lose detailed appearances, especially for some local but important regions (e.g., nose and ears). Thus a part-sensitive reasoner is utilized to deal with different local regions. We term our baseline method for single-view reconstruction as \emph{BiCarNet}. Moreover, two further applications, i.e., sketch-based modeling and 3D character animation, are also explored. Experimental results on these applications demonstrate that it is already able to generate reasonable outputs. %We hope that our work opens a door for researchers to explore bipedal character digitization with the proposed dataset.


%Furthermore, we conduct comprehensive experiments to explore the potential of \textit{3DBiCar} and evaluate the capability of \textit{RaBit}. Specifically,  we implement exhaustive applications, including single-view cartoon character reconstruction, sketch-based character modeling, and 3D cartoon animation. In the single-view reconstruction, we further propose to integrate cumulative local UV mappings to further enhance the representation ability of local details (e.g., nose, ears) of texture. Experimental results on these applications not only demonstrate the usability of \textit{3DBiCar} but also the outstanding capability of \textit{RaBit}. We hope that our work \textit{3DBiCar} and \textit{RaBit} will boost further research on efficient bipedal character digitization. 

% \zj{To summarize, we propose \textit{3DBiCar}, the first large-scale 3D biped cartoon character dataset, which contains 1,500 meticulously crafted, textured 3D models with a consistent mesh topology. Based on this dataset, we build the first 3D full-body cartoon parametric model \textit{RaBit} for biped character modeling. Both \textit{3DBiCar} and \textit{RaBit} will be released to facilitate future research. To demonstrate the promising potential of \textit{3DBiCar} and \textit{RaBit}, we conduct various downstream applications, including single-view reconstruction, sketch-based modeling, and 3D cartoon animation. We hope that our work will contribute to the development of 3D biped cartoon character modeling and inspire future works in this area.}
To summarize, our contributions include:
\begin{itemize}
    \item We introduce \textit{3DBiCar}, the first large-scale 3D biped cartoon character dataset. It contains 1,500 high-quality textured 3D models with a consistent mesh topology.
    
    \item We propose \textit{RaBit}, the first 3D full-body cartoon parametric model for biped character modeling. We will release both \textit{3DBiCar} and \textit{RaBit} for future research.
    
    % \item We carefully designed \emph{BiCarNet}, the first method to reconstruct 3D textured biped cartoon characters from a single-view image. A novel part-sensitive reasoner is invented for detailed texture generation.  
    \item We build \emph{BiCarNet}, the baseline method to reconstruct 3D biped cartoon characters from a single-view image. A part-sensitive reasoner is adopted for detailed texture generation.
    
    %We conduct various applications to demonstrate the promising potential of \textit{3DBiCar} and \textit{RaBit} in single-view cartoon character reconstruction, sketch-based character modeling, and 3D cartoon animation. 

    
    % \item Two other applications, i.e., sketch-based modeling and 3D character animation, are also successfully conducted. We strongly believe our work opens a door for future research. 
    \item Two other applications, i.e., sketch-based modeling and 3D character animation, are also conducted to demonstrate the promising potential of \textit{3DBiCar} and \textit{RaBit}.
    
    %We propose to adopt additional local UV mappings to further enhance the representation ability of local details in the single-view reconstruction. Quantitative experiments demonstrate the superior of our method.
\end{itemize}

% Firstly, \textit{3DBiCar} spans a wide range of 3D biped cartoon digitalization sources with topological-consistent geometry. It contains 1646 high-diversified 2D biped cartoon character images, and corresponding high-quality 3D models with textures containing 17 species and 4 image styles. Secondly, all the 3D models in \textit{3DBiCar} are animation-ready and unified in topology. The artists were asked to craft 3D models using a pre-defined template with several landmarks and rig them with a consistent skeleton. It paves the way for learning a skinned parametric geometric model to serve several vital tasks in visual computing such as shape reconstruction, pose tracking and character animation. Thirdly, since all 3D meshes in \textit{3DBiCar} are topologically uniform, each model can be unwrapped into a unified UV map and manually painted by artists according to the paired image. 

% To implement prevailing learning-based parameterization for fictitious biped characters, a comprehensive cartoon dataset with both topological-consistent geometry and corresponding texture is necessary. Advanced 3D sensing devices make capturing 3D data from the real world convenient and further prompt a growing number of 3D people repositories~\cite{anguelov2005scape, ionescu2013human3, SMPL:2015, pishchulin2017building, STAR:2020}. Thanks to those human datasets, it is already possible to infer nearly accurate 3D digital avatars from single-view images with the help of parametric models~\cite{SMPL:2015, SMPL-X:2019, STAR:2020} and data-driven techniques~\cite{bogo2016keep,kanazawa2018end,pavlakos2018learning,choutas2022accurate}. Recent deep neural networks~\cite{kanazawa2018end, kolotouros2019spin, SMPL-X:2019} have achieved excellence in the task of 3D human reconstruction, significantly increasing efficiency in modeling 3D human-related content. 

% Still, constructing a 3D fictitious cartoon dataset simultaneously considering shape, pose, and texture is quite difficult. There are two challenges in collecting and building such a dataset. First, 3D cartoon characters available online are always topologically inconsistent and lack corresponding textures, thus hard to be utilized for many fundamental tasks, such as parametric modeling, pose retargeting, and texture synthesis. Meanwhile, collecting corresponding paired images or skinned skeletons of random shared 3D models is nearly impossible, hindering them from directly applying in single-view reconstruction or animation generation tasks. Second, diversified species, complex geometric shapes, and variegated texture styles of cartoon characters have brought additional challenges in building a 3D textured and skinned cartoon character dataset.

%In the rest of the paper, we first review some related works in the domain of shape reconstruction and image synthesis in Sec.~\ref{sec:related_works}. Next, Sec.~\ref{sec:dataset} presents the procedure of constructing \textit{3DBiCar}, while the following Sec.~\ref{sec:algorithm} elaborates the backend algorithm for building \textit{SMCL}. In Sec.~\ref{sec:application}, we demonstrate several applications showcasing the potential of \textit{3DBiCar} and \textit{SMCL}. Finally, we make a conclusion and discussion on this work in Sec.~\ref{sec:conclusion}.

