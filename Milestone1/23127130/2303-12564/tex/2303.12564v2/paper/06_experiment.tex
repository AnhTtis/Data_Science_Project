% \input{figure/fig_interpolation}
% \input{figure/fig_assemble}
\input{figure/fig_mesh_result}
\input{figure/fig_texture_result}

\section{Experiments}
\label{sec:experiment}
In this section, we conduct comprehensive experiments to demonstrate the capacity of \textit{SMCL} and \textit{3DBiCar}. In \ref{sec:experiment-sampleing}, we present the parameterization of \textit{SMCL} with different sampling methods. In \ref{sec:experiment-svr}, we elabrate the implementation detail and further demonstrate the qualitative and quantitative result of our single view reconstruction. 
%\subsection{Shape Morphing}




\input{figure/wild_result}

\subsection{Single View Reconstruction}
\label{sec:experiment-svr}


%In this section, we will show training details and quantitative and qualitative results of our method.

To obtain reconstruction networks for single view reconstruction. We split \textit{3DBiCar} into a training set (including 1532 models) and a testing set (including 114 models). In order to reduce domain shift during the evaluation, we keep the distribution of test samples as consistent with the distribution of 3DBiCar. % as possible.

%Even though we pay much effort into \textit{3DBiCar}, compared with other datasets, the train set is so limited that the neural network may not converge well. 

We next render synthetic images for each model and apply image augmentation such as color jitter for data augmentation. Moreover, we also make use of these sampling methods as Sec.~\ref{sec:experiment-sampleing} to generate synthetic model-image pairs. To minimize the impact of data imbalances, we resample the training set to maintain the balance between raw and synthetic data.

\textbf{Mesh Reconstruction.} In part-based \textit{SMCL}, we perform PCA on symmetric structures simultaneously (e.g., left and right hands) to reduce the size of parts from 6 to 4. To make fair comparision between whole-based and part-based linear models, we fix the dimension of $\Theta$ to 72 and $B$ to 200 (74 dimensions for head part and 42 for others).
%50 dimensions for every part of the part-based model). 
\input{table/table_mesh_result}

MAE errors of $\Theta$ and P-MPJPE are used in the pose parameter for evaluation. We also report the Projection error (P-Proj) between manual 2D annotation and 2D projections of models after alignment to verify the pose consistency between our results and given pictures. To evaluate shape reconstruction, we use reconstruction loss under T-pose (T-rec) as a model shape metric instead of using reconstruction directly because of the strong coupling between the pose-related metric and the reconstruction metric. Our quantitative result in Tab. \ref{tab_mesh_result} demonstrate that our part-based model outperformed the whole-based model in all metrics. We also visualize the results as shown in Fig.~\ref{fig_mesh_result}. The results of the whole-based model are acceptable, while the part-based models are closer to ground truth models.

\input{table/table_texture_result}

\textbf{Texture Reconstruction.} We choose the principal component analysis (PCA) method for comparison. In this method, PCA is first applied to reduce the dimension of UV, and then the neural network is used to fit the reduced vector. In PCA method and styleGAN method, the dimension of $T$ is fixed to be 512.

MSE, PSNR and FID are used for evaluation. As shown in Tab. \ref{tab_texutre_result}), the styleGAN method outperformed the PCA method in all three metrics. The qualitative result in Fig.~\ref{fig_texture_result} illustrates that the UV from the styleGAN method is sharper, less artifact, and closer to ground truth. 

\textbf{Result of wild image} To show both mesh reconstruction and texture reconstruction, we collect some images and generate textured posed models after extracting main character from images(Fig.~\ref{fig_wildresult}).
