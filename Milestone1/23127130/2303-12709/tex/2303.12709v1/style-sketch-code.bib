@book{shneiderman2022human,
  title={Human-Centered AI},
  author={Shneiderman, B.},
  isbn={9780192845290},
  year={2022},
  publisher={Oxford University Press}


}

@inproceedings{contgrad,
author = {Ueno, Michihiko and Satoh, Shin’ichi},
title = {Continuous and Gradual Style Changes of Graphic Designs with Generative Model},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450666},
doi = {10.1145/3397481.3450666},

pages = {280–289},
numpages = {10},
keywords = {layout, deep generative networks, graphic design, latent space},
location = {College Station, TX, USA},
doi = {10.1111/spc3.12127}}

@unknown{ltx,
author = {Ko, Hyung-Kwon and Park, Gwanmo and Jeon, Hyeon and Jo, Jaemin and Kim, Juho and Seo, Jinwook},
year = {2022},
month = {10},
pages = {},
title = {Large-scale Text-to-Image Generation Models for Visual Artists' Creative Works},
doi = {10.48550/arXiv.2210.08477}
}

@inproceedings{wetoon,
author = {Ko, Hyung-Kwon and An, Subin and Park, Gwanmo and Kim, Seung Kwon and Kim, Daesik and Kim, Bohyoung and Jo, Jaemin and Seo, Jinwook},
title = {We-Toon: A Communication Support System between Writers and Artists in Collaborative Webtoon Sketch Revision},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545612},
doi = {10.1145/3526113.3545612},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {76},
numpages = {14},
keywords = {Interactive System, Interview, Human-centered AI, GAN, Communication, Webtoon, Creativity Support, User Interface, Usability Study, Collaboration},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{inproceedings,
author = {Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
year = {2021},
month = {10},
pages = {498-510},
title = {Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning},
doi = {10.1145/3472749.3474765}
}


@inproceedings{drawing8,
author = {Yurman, Paulina and Reddy, Anuradha Venugopal},
title = {Drawing Conversations Mediated by AI},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527927.3531448},
doi = {10.1145/3527927.3531448},

booktitle = {Creativity and Cognition},
pages = {56–70},
numpages = {15},
location = {Venice, Italy},
series = {C\&C '22}
}
@inproceedings{initim,
author = {Qiao, Han and Liu, Vivian and Chilton, Lydia},
title = {Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal AI Generated Art},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527927.3532792},
doi = {10.1145/3527927.3532792},
abstract = {Advances in text-to-image generative models have made it easier for people to create art by just prompting models with text. However, creating through text leaves users with limited control over the final composition or the way the subject is represented. A potential solution is to use image prompts alongside text prompts to condition the model. To better understand how and when image prompts can improve subject representation in generations, we conduct an annotation experiment to quantify their effect on generations of abstract, concrete plural, and concrete singular subjects. We find that initial images improved subject representation across all subject types, with the most noticeable improvement in concrete singular subjects. In an analysis of different types of initial images, we find that icons and photos produced high quality generations of different aesthetics. We conclude with design guidelines for how initial images can improve subject representation in AI art.},
booktitle = {Creativity and Cognition},
pages = {15–28},
numpages = {14},
keywords = {design guidelines, text-to-image, computational creativity, prompt engineering, multimodal generative models, AI co-creation},
location = {Venice, Italy},
}








@article{hcai,
author = {Ben Shneiderman},
title = {Human-Centered Artificial Intelligence: Reliable, Safe \& Trustworthy},
journal = {International Journal of Human–Computer Interaction},
volume = {36},
number = {6},
pages = {495-504},
year  = {2020},
publisher = {Taylor \& Francis},
doi = {10.1080/10447318.2020.1741118},

URL = { 
    
        https://doi.org/10.1080/10447318.2020.1741118
    
    

},
eprint = { 
    
        https://doi.org/10.1080/10447318.2020.1741118
    
    

}

}

@inproceedings{coai,
author = {Urban Davis, Josh and Anderson, Fraser and Stroetzel, Merten and Grossman, Tovi and Fitzmaurice, George},
title = {Designing Co-Creative AI for Virtual Environments},
year = {2021},
isbn = {9781450383769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450741.3465260},
doi = {10.1145/3450741.3465260},
abstract = {Co-creative AI tools provide a method of creative collaboration between a user and machine. One form of co-creative AI called generative design requires the user to input design parameters and wait substantial periods of time while the system computes design solutions. We explore this interaction dynamic by providing an embodied experience in VR. Calliope is a virtual reality (VR) system that enables users to explore and manipulate generative design solutions in real time. Calliope accounts for the typical idle times in the generative design process by using a virtual environment to encourage parallelized and embodied data-exploration and synthesis, while maintaining a tight human-in-the-loop collaboration with the underlying algorithms. In this paper we discuss design considerations informed by formative studies with generative designers and artists and provide design guidelines to aid others in the development of co-creative AI systems in virtual environments.},
booktitle = {Creativity and Cognition},
articleno = {26},
numpages = {11},
keywords = {human-ai collaboration, creativity support tools, VR/AR/XR},
location = {Virtual Event, Italy},
}

@inproceedings{exp,
author = {Zhou, Yijun and Koyama, Yuki and Goto, Masataka and Igarashi, Takeo},
title = {Interactive Exploration-Exploitation Balancing for Generative Melody Composition},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450663},
doi = {10.1145/3397481.3450663},
abstract = {Recent content creation systems allow users to generate various high-quality content (e.g., images, 3D models, and melodies) by just specifying a parameter set (e.g., a latent vector of a deep generative model). The task here is to search for an appropriate parameter set that produces the desired content. To facilitate this task execution, researchers have investigated user-in-the-loop optimization, where the system samples candidate solutions, asks the user to provide preferential feedback on them, and iterates this procedure until finding the desired solution. In this work, we investigate a novel approach to enhance this interactive process: allowing users to control the sampling behavior. More specifically, we allow users to adjust the balance between exploration (i.e., favoring diverse samples) and exploitation (i.e., favoring focused samples) in each iteration. To evaluate how this approach affects the user experience and optimization behavior, we implement it into a melody composition system that combines a deep generative model with Bayesian optimization. Our experiments suggest that this approach could improve the user’s engagement and optimization performance.},
booktitle = {26th International Conference on Intelligent User Interfaces},
pages = {43–47},
numpages = {5},
keywords = {Bayesian optimization, Generative design, creativity support tools, human-in-the-loop machine learning},
location = {College Station, TX, USA},
}

@inproceedings{insp,
author = {Herring, Scarlett R. and Chang, Chia-Chen and Krantzler, Jesse and Bailey, Brian P.},
title = {Getting Inspired! Understanding How and Why Examples Are Used in Creative Design Practice},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518717},
doi = {10.1145/1518701.1518717},
abstract = {The use of examples serves a critical role in creative design practice, but details of this process remain an enigma. This is problematic for both the understanding of design activity as well as for developing more effective design tools. In this paper, we report results of a study that understands and compares how designers (N=11) utilize, manage, and share examples to support the creative design process. The domains studied were Web, graphic, and product design. Our study shows that examples are a cornerstone of creative practice and are utilized for many reasons throughout the design process. Since examples are pivotal to the success of a project, more effective tools that support retrieval, storage, and dissemination of examples are needed. This paper contributes understanding of the benefits and roles of examples in the design process and implications for the design of more effective tools that support example usage.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {87–96},
numpages = {10},
keywords = {web design, graphic design, product design, industrial design, design, case-based design, examples},
location = {Boston, MA, USA},
series = {CHI '09}
}
@article{arr,
author = {Gonçalves, Milene and Cardoso, Carlos and Badke-Schaub, Petra},
year = {2013},
month = {01},
pages = {},
title = {What inspires designers? Preferences on inspirational approaches during idea generation},
volume = {35},
journal = {Design Studies},
doi = {10.1016/j.destud.2013.09.001}
}
@misc{dribble,
  title = {Dribbble - Discover the World’s Top Designers and Creative},
  url = {https://dribbble.com/},
  year = {2022},
  note = {\url{https://dribbble.com/}}
}

@misc{Behance,
  title = {Behance - Search Projects: Photos, videos, logos, illustrations and branding.},
  url = {https://www.behance.net},
  year = {2022},
  note = {\url{https://www.behance.net}}
}

@inproceedings{guifetch,
author = {Behrang, Farnaz and Reiss, Steven P. and Orso, Alessandro},
title = {GUIfetch: Supporting App Design and Development through GUI Search},
year = {2018},
isbn = {9781450357128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197231.3197244},
doi = {10.1145/3197231.3197244},
abstract = {A typical way to design and develop a mobile app is to sketch the graphical user interfaces (GUIs) for the different screens in the app and then create actual GUIs from these sketches. Doing so involves identifying which layouts to use, which widgets to add, and how to configure and connect the different pieces of the GUI. To help with this difficult and time-consuming task, we propose GUIFetch, a technique that takes as input the sketch for an app and leverages the growing number of open source apps in public repositories to identify apps with GUIs and transitions that are similar to those in the provided sketch. GUIFetch first searches public repositories to find potential apps using keyword matching. It then builds models of the identified apps' screens and screen transitions using a combination of static and dynamic analyses and computes a similarity metric between the models and the provided sketch. Finally, GUIFetch ranks the identified apps (or parts thereof) based on their computed similarity value and produces a visual ranking of the results together with the code of the corresponding apps. We implemented GUIFetch for Android apps and evaluated it through user studies involving different types of apps.},
booktitle = {Proceedings of the 5th International Conference on Mobile Software Engineering and Systems},
pages = {236–246},
numpages = {11},
keywords = {user interface programming, code search, user interface design},
location = {Gothenburg, Sweden},
series = {MOBILESoft '18}
}

@inproceedings{exa7,
author = {Lee, Brian and Srivastava, Savil and Kumar, Ranjitha and Brafman, Ronen and Klemmer, Scott R.},
title = {Designing with Interactive Example Galleries},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753667},
doi = {10.1145/1753326.1753667},
abstract = {Designers often use examples for inspiration; examples offer contextualized instances of how form and content integrate. Can interactive example galleries bring this practice to everyday users doing design work, and does working with examples help the designs they create? This paper explores whether people can realize significant value from explicit mechanisms for designing by example modification. We present the results of three studies, finding that independent raters prefer designs created with the aid of examples, that examples may benefit novices more than experienced designers, that users prefer adaptively selected examples to random ones, and that users make use of multiple examples when creating new designs. To enable these studies and demonstrate how software tools can facilitate designing with examples, we introduce interface techniques for browsing and borrowing from a corpus of examples, manifest in the Adaptive Ideas Web design tool. Adaptive Ideas leverages a faceted metadata interface for viewing and navigating example galleries.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2257–2266},
numpages = {10},
keywords = {examples, design thinking},
location = {Atlanta, Georgia, USA},

}
@inproceedings{rewire,
author = {Swearngin, Amanda and Dontcheva, Mira and Li, Wilmot and Brandt, Joel and Dixon, Morgan and Ko, Amy J.},
title = {Rewire: Interface Design Assistance from Examples},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174078},
doi = {10.1145/3173574.3174078},
abstract = {Interface designers often use screenshot images of example designs as building blocks for new designs. Since images are unstructured and hard to edit, designers typically reconstruct screenshots with vector graphics tools in order to reuse or edit parts of the design. Unfortunately, this reconstruction process is tedious and slow. We present Rewire, an interactive system that helps designers leverage example screenshots. Rewire automatically infers a vector representation of screenshots where each UI component is a separate object with editable shape and style properties. Based on this representation, the system provides three design assistance modes that help designers reuse or redraw components of the example design. The results from our quantitative and user evaluations demonstrate that Rewire can generate accurate vector representations of interface screenshots found in the wild and that design assistance enables users to reconstruct and edit example designs more efficiently compared to a baseline design tool.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {user interface design, pixel-based reverse engineering, wireframing},
location = {Montreal QC, Canada},
}
@inproceedings{dtour,
author = {Ritchie, Daniel and Kejriwal, Ankita Arvind and Klemmer, Scott R.},
title = {D.Tour: Style-Based Exploration of Design Example Galleries},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047216},
doi = {10.1145/2047196.2047216},
abstract = {In design, people often seek examples for inspiration. However, current example-finding practices suffer many drawbacks: templates present designs without a usage context; search engines can only examine the text on a page. This paper introduces exploratory techniques for finding relevant and inspiring design examples. These novel techniques include searching by stylistic similarity to a known example design and searching by stylistic keyword. These interactions are manifest in d.tour, a style-based design exploration tool. d.tour presents a curated database of Web pages as an explorable design gallery. It extracts and analyzes design features of these pages, allowing it to process style-based queries and recommend designs to the user. d.tour's gallery interface decreases the gulfs of execution and evaluation for design example-finding.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {165–174},
numpages = {10},
keywords = {search, design, examples},
location = {Santa Barbara, California, USA},
}

@article{ari,
author = {Marsh, Richard and Landau, Joshua and Hicks, Jason},
year = {1996},
month = {09},
pages = {669-680},
title = {How Examples May (and May Not) Constrain Creativity},
volume = {24},
journal = {Memory and Cognition},
doi = {10.3758/BF03201091}
}

@article{fix,
author = {Jansson, David and Smith, Steven},
year = {1991},
month = {01},
pages = {},
title = {Design fixation},
volume = {12},
journal = {Design Studies},
doi = {10.1016/0142-694X(91)90003-F}
}

@inproceedings{gansp,
author = {Mozaffari, Mohammad Amin and Zhang, Xinyuan and Cheng, Jinghui and Guo, Jin L.C.},
title = {GANSpiration: Balancing Targeted and Serendipitous Inspiration in User Interface Design with Style-Based Generative Adversarial Network},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517511},
doi = {10.1145/3491102.3517511},
abstract = {Inspiration from design examples plays a crucial role in the creative process of user interface design. However, current tools and techniques that support inspiration usually only focus on example browsing with limited user control or similarity-based example retrieval, leading to undesirable design outcomes such as focus drift and design fixation. To address these issues, we propose the GANSpiration approach that suggests design examples for both targeted and serendipitous inspiration, leveraging a style-based Generative Adversarial Network. A quantitative evaluation revealed that the outputs of GANSpiration-based example suggestion approaches are relevant to the input design, and at the same time include diverse instances. A user study with professional UI/UX practitioners showed that the examples suggested by our approach serve as viable sources of inspiration for overall design concepts and specific design elements. Overall, our work paves the road of using advanced generative machine learning techniques in supporting the creative design practice.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {537},
numpages = {15},
keywords = {StyleGAN, creativity support, inspiration, User interface design},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{trash,
author = {Thrash, Todd and Elliot, Andrew},
year = {2003},
month = {05},
pages = {871-89},
title = {Inspiration as a Psychological Construct},
volume = {84},
journal = {Journal of personality and social psychology},
doi = {10.1037/0022-3514.84.4.871}
}


@article{thrash2014psychology,
  title={The psychology of inspiration},
  author={Thrash, Todd M and Moldovan, Emil G and Oleynick, Victoria C and Maruskin, Laura A},
  journal={Social and personality psychology compass},
  volume={8},
  number={9},
  pages={495--510},
  year={2014},
  doi = {10.1111/spc3.12127},
  publisher={Wiley Online Library}
}



@article{eckert2000sources,
title = {Sources of inspiration: a language of design},
journal = {Design Studies},
volume = {21},
number = {5},
pages = {523-538},
year = {2000},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00022-3},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X00000223},
author = {Claudia Eckert and Martin Stacey},
keywords = {sources of inspiration, design precedents, communication, collaborative design, knitwear},
abstract = {Sources of inspiration play an important role in the design process, both in defining the context for new designs and in informing the creation of individual designs. Previous designs and other sources of ideas furnish a vocabulary both for thinking about new designs and for describing designs to others. In a study of knitwear design, a process in which the use of sources of inspiration is explicitly acknowledged, we have observed that designers communicate with each other about new designs, styles and moods, largely by reference to the sources of their ideas. In this paper we discuss why this style of communication is so important, and what information it is used to convey. We view it as the use of a language to describe regions in the space of possible designs.}
}

@inproceedings{bonnardel1999creativity,
  title={Creativity in design activities: The role of analogies in a constrained cognitive environment},
  author={Bonnardel, Nathalie},
  booktitle={Proceedings of the 3rd conference on Creativity \& cognition},
  pages={158--165},
  year={1999}
}


@inproceedings{herring2009getting,
  title={Getting inspired! Understanding how and why examples are used in creative design practice},
  author={Herring, Scarlett R and Chang, Chia-Chen and Krantzler, Jesse and Bailey, Brian P},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={87--96},
  year={2009}
}

@misc{gonccalves2014inspires,
  title={What inspires designers? Preferences on inspirational approaches during idea generation. Des Stud 35 (1): 29--53},
  author={Gon{\c{c}}alves, M and Cardoso, C and Badke-Schaub, P},
  year={2014}
}

@inproceedings{Deka:2017:Rico,
 author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
 title = {Rico: A Mobile App Dataset for Building Data-Driven Design Applications},
 booktitle = {Proceedings of the 30th Annual Symposium on User Interface Software and Technology},
 series = {UIST '17},
 year = {2017},
 keywords = {Mobile app design; design mining; design search; app
datasets},
}


@article{pix2pix,
  title={High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
  author={Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Andrew Tao and Jan Kautz and Bryan Catanzaro},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={8798-8807}
}

@article{spade,
  title={Semantic Image Synthesis With Spatially-Adaptive Normalization},
  author={Taesung Park and Ming-Yu Liu and Ting-Chun Wang and Jun-Yan Zhu},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={2332-2341}
}


@Article{color,
AUTHOR = {Tian, Nannan and Liu, Yuan and Wu, Bo and Li, Xiaofeng},
TITLE = {Colorization of Logo Sketch Based on Conditional Generative Adversarial Networks},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {4},
ARTICLE-NUMBER = {497},
URL = {https://www.mdpi.com/2079-9292/10/4/497},
ISSN = {2079-9292},
ABSTRACT = {Logo design is a complex process for designers and color plays a very important role in logo design. The automatic colorization of logo sketch is of great value and full of challenges. In this paper, we propose a new logo design method based on Conditional Generative Adversarial Networks, which can output multiple colorful logos only by providing one logo sketch. We improve the traditional U-Net structure, adding channel attention and spatial attention in the process of skip-connection. In addition, the generator consists of parallel attention-based U-Net blocks, which can output multiple logo images. During the model optimization process, a style loss function is proposed to improve the color diversity of the logos. We evaluate our method on the self-built edges2logos dataset and the public edges2shoes dataset. Experimental results show that our method can generate more colorful and realistic logo images based on simple sketches. Compared to the classic networks, the logos generated by our network are also superior in visual effects.},
DOI = {10.3390/electronics10040497}
}

@inproceedings{sket,
author = {Landay, James A. and Myers, Brad A.},
title = {Interactive Sketching for the Early Stages of User Interface Design},
year = {1995},
isbn = {0201847051},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/223904.223910},
doi = {10.1145/223904.223910},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {43–50},
numpages = {8},
location = {Denver, Colorado, USA},
series = {CHI '95}
}


@ARTICLE{910894,
  author={Landay, J.A. and Myers, B.A.},
  journal={Computer}, 
  title={Sketching interfaces: toward more human interface design}, 
  year={2001},
  volume={34},
  number={3},
  pages={56-64},
  doi={10.1109/2.910894}}

  @inproceedings{sem0,
  title={Inferring semantic layout for hierarchical text-to-image synthesis},
  author={Hong, Seunghoon and Yang, Dingdong and Choi, Jongwook and Lee, Honglak},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7986--7994},
  year={2018}
}

@inproceedings{zhu2020sean,
  title={Sean: Image synthesis with semantic region-adaptive normalization},
  author={Zhu, Peihao and Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5104--5113},
  year={2020}
}

@article{li2023gligen,
  title={GLIGEN: Open-Set Grounded Text-to-Image Generation},
  author={Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
  journal={CVPR},
  year={2023}
}

@inproceedings{park2019SPADE,
  title={Semantic Image Synthesis with Spatially-Adaptive Normalization},
  author={Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@inproceedings{calo,
author = {Cal\`{o}, Tommaso and De Russis, Luigi},
title = {Style-Aware Sketch-to-Code Conversion for the Web},
year = {2022},
isbn = {9781450390316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531706.3536462},
doi = {10.1145/3531706.3536462},
abstract = {While sketching a graphical user interface (GUI) is a necessary step towards the creation of a Web application, its transformation into a coded GUI, with the proper styles, is still a tedious and time-consuming task that a designer should perform. Recently, a set of Machine Learning techniques has been applied to automatically generate code from sketches to ease this part of the design process. These techniques effectively convert the sketches into a skeleton structure of the GUI but are not designed to consider the styles to be applied to the generated HTML page. Moreover, having the possibility to explore different styles, starting from a sketch, might further support the designer in their work. In this paper, we move the first steps to enable this opportunity by proposing a method that allows the designer to input the sketch of the Web-based GUI and select a reference style to be applied. Our method automatically injects the reference styles into the sketch components and then uses an automatic code generation technique to obtain the final code. Preliminary experiments carried out with the navigation bar component show the effectiveness of the proposed method.},
booktitle = {Companion of the 2022 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {44–47},
numpages = {4},
keywords = {convolutional neural network, web elements, user interface, machine learning},
location = {Sophia Antipolis, France},
series = {EICS '22 Companion}
}