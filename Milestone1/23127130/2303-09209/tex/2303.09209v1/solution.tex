%!TEX root = main.tex

\section{Learning to act from temporal  data}
\label{sec:learning}

%In this section we describe the implementation of the proposed solution, which is reported in \figurename~\ref{fig:pipeline}.
In this section we describe the proposed RL solution, whose pipeline is reported in \figurename~\ref{fig:pipeline}. The pipeline takes as input an event log related to a multi-actor process, the definition of a relevant KPI to optimise, as well as the ownership of the activities in the log and builds
%Starting from an event log of a multi-actor process, the definition of a relevant KPI to maximize, and the annotation on the ownership of activities in the log we construct 
via RL a (recommender) system that recommends the next best activity(ies) in order to optimise the desired KPI. It is composed of three phases: in the \emph{Preprocessing phase} an MDP is built starting from the three inputs of the pipeline (Sec.~\ref{sec:constructionMDP}); in the \emph{Reinforcement Learning phase} the MDP is  trained by the RL algorithm to learn the best policy, and, finally, in the \emph{Runtime phase}, the policy is used to recommend the best next activity(ies) for an ongoing execution (Sec.~\ref{sec:RLtraining}). %maximize the selected KPI.
\begin{figure}[t]
	\centering
	\includegraphics[width=.45\textwidth]{images/pipeline.png} 
	\caption{Pipeline of the proposed RL solution.}  \label{fig:pipeline}
\end{figure}

\subsection{MDP construction from log data}
\label{sec:constructionMDP}
The aim of the \emph{Preprocessing phase} is building an MDP starting from: (i) the event log; (ii) the list of activities carried out by the \emph{agent}, and the ones by the \emph{environment}; and (iii) the KPI to optimise.
%The first part of our solution is displayed in the \emph{Preprocessing} box of \figurename~\ref{fig:pipeline}.
%The input components are: an event log; an annotation of activities ownership: the owner can be the \emph{agent} or the \emph{environment},
%the former can be recommended by the recommender system, the latter cannot; finally a definition of the process KPI we want to maximize with the recommendations.
%From these components we construct the MDP that will be used later to train the RL agent which will constitute our recommender system.

At a high level, the following mapping between the MDP components (actions and states) and the information extracted from the event log can be defined as follows:
%First of all we need to define the MDP objects starting from the objects in the event log:
\begin{itemize}
	\item actions: the activities that the agent can carry out; %that can be selected by the agent
	\item states: a comprehensive description of the system. In a fashion similar to \cite{us@BPM22} it contains three components:
	\begin{itemize}
		\item last activity selected by the agent or the environment;
		\item the trace history, that is, the condensed information about the entire trace execution up to that point;
		\item information about the reward obtained up to that point.
	\end{itemize}
\end{itemize}

%The steps to generate the MDP starting from the input components listed above are:
Considering the definition of MDP action and state reported above, the steps to follow for the construction of the MDP are the following:
\begin{enumerate}
	\item \emph{Enriching the log.} In this step, based on the list of the agent's activities, each activity in the log is marked as an agent or as an environment (all the other activities) activity. Moreover, the KPI of interest is computed for each trace $\sigma$. Its value can depend on the executed activities or on other attributes of the trace, and represents the reward $r(\sigma)$ of the whole trace $\sigma$. 
	Note that we define $r(\sigma_k)=0$ for incomplete prefixes $\sigma_{k}$, $k < \text{len}(\sigma)$. %$\sigma_{k < \text{len}(\sigma)}$.}
%	Every activities in the log is annotated with its owner (agent's activity that can be recommended versus environment activities that cannot be recommended). Moreover for every trace $\sigma$ in the log the reward $R(\sigma)$ is computed as relevant KPI definition, this can depends on the events and on other attributes of the trace. 
\item \emph{Encoding and clustering using k-means.}
	In this step, each prefix in the log
	%\item \emph{Encoding and clustering all the prefixes in the log using kmeans algorithm.}
	%($\sigma_k \, \forall 0<k\le\text{len}(\sigma)\, \forall \sigma\in\mathcal{L}$)
	is encoded using three types of information: the frequency of the activities, the last position in which an activity has occurred, %seen position of each activity,
	and the reward obtained up to that point. %\footref{fn:reward}. 
	Namely, if the alphabet of all the activities is $\{\act{a}_1,\dots,\act{a}_n\}$
	then the encoding of a prefix $\sigma_k$ is: % of length $k$ $\sigma_k$ is:
	{\small
	\begin{equation}
		% \resizebox{.8\hsize}{!}{$
		\bm{v}_{\sigma_k} =
		\left( \frac{f_{\act{a}_1}(\sigma_k)}{f_\text{max}}, \dots ,\frac{f_{\act{a}_n}(\sigma_k)}{f_\text{max}},
				\frac{p_{\act{a}_1}(\sigma_k)}{p_\text{max}}, \dots, \frac{p_{\act{a}_n}(\sigma_k)}{p_\text{max}}, \tilde r(\sigma_k) \right)
		% $}
	\end{equation}
	}
	where $f_{\act{a}_i}$ and $p_{\act{a}_i}$ are defined in Sect.~\ref{sec:background}.
	$f_\text{max}$ and $p_\text{max}$ are respectively the highest %max 
	frequency and position
	for all activities and all prefixes in the log. Note that the latter can also be seen as the max length of all  traces in the log
	{\small
	\begin{equation}
		% \resizebox{.8\hsize}{!}{$
		f_\text{max} = \max_{\substack{\act{a}\in A, \sigma \in \mathcal{L}\\ 0<k\le\text{len}(\sigma)}} \big(f_{\act{a}}(\sigma_k)\big),
		\qquad
		p_\text{max} = \max_{\sigma \in \mathcal{L}} \big(\text{len}(\sigma)\big)
		% $}
	\end{equation}
	}
	Finally, $\tilde r(\sigma_k)$ is the \emph{prefix normalized reward}: it is equal to $0$ for proper trace prefixes, %i.e., $r(\sigma_k)=0$, 
	while it is
	%Finally, $r(\sigma_k)=0$ 	except when the prefix is the complete trace, in which case it is 
	the reward of the trace normalized with respect to the rewards of all the traces for complete traces. In other terms:
	%\todomro{se volete il minmax scaler si può esplicitare nella formula} \todocdf{Sì, forse è meglio: ho cambiato ... vedi se è ok}
	\begin{equation}
	% \resizebox{.8\hsize}{!}{$
	\tilde r(\sigma_k) =
	\begin{cases}
	0 & k < \text{len}(\sigma) \\
	\frac{r(\sigma) - \min_{\sigma' \in \mathcal{L}} r(\sigma')}{\max_{\sigma' \in \mathcal{L}} r(\sigma') - \min_{\sigma' \in \mathcal{L}} r(\sigma')} & k = \text{len}(\sigma)
	\end{cases}
	% $}
	\end{equation}
	% {\small
	% \begin{equation}
	% \tilde r(\sigma_k) = \left\{
	% \begin{aligned}
		% &0 &\;& k < \text{len}(\sigma)
		% \\
		% &\frac{r(\sigma) - \min_{\sigma' \in \mathcal{L}} r(\sigma')}{\max_{\sigma' \in \mathcal{L}} r(\sigma') - \min_{\sigma' \in \mathcal{L}} r(\sigma')}
		% &\;& k = \text{len}(\sigma)
	% \end{aligned} \right.
	% \end{equation}
	% }
	The encoded prefixes are then used to train a k-means model \cite{kmeans} that will be able to assign a new prefix %(prefix) trace
	to the cluster containing the most similar prefixes.
	%The encoded prefixes are then used to train a kmeans model which assigns the same cluster to similar prefixes.\footnote{
	%In all our examples the number of clusters has been chooses to be 100,
	%this number has been selected performing a silhouette analysis on every dataset.}
	%The reward has been included in the encoding of the prefix in order to help obtaining a clusterization suitable for the RL training.
	%\item \emph{Constructing the MDP given the enriched log and the cluster model.}
	\item \emph{Constructing the MDP.}
	Once the k-means model has been trained, the MDP can be built.
	For each prefix $\sigma_k$, a state $s_{\sigma_k}$ is defined %obtained
	as the pair of the last performed activity $\act{a}_{i_k}=\text{Act}(e_k)$ and the cluster $c_{\sigma_{k-1}}$ assigned to $\sigma_{k-1}$ by the k-means model:
	%States $s$ are defined for each partial execution of a trace $\sigma_k$ as the concatenation of the last activity performed $e_k$
	%and the cluster number of the prefix preceding the last activity computed with the kmeans model $c: \sigma_k \mapsto c(\sigma_k) \in \mathbb{N}$
	\begin{equation}
	\label{eq:state_def}
		%s(\sigma_k) = \big(e_k,\, c(\sigma_{k-1})\big)
		s_{\sigma_k} = \big(\act{a}_{i_k},\, c_{\sigma_{k-1}}\big).
	\end{equation}
	%as mentioned above in this way we include into the state definition
	This definition of state would allow us to include, in the state,
	information	about the history of the execution and its reward.
	%Actions $a$ are defined as agent's activities.
	
	Once defined the states, the MDP is built %We construct the MDP 
	by replaying the traces in the event log: we build a directed graph, where states correspond to nodes
	and edges correspond to actions moving from one node state to another.
	Moreover, for each edge, the probability of reaching the target node (computed based on the
	number of traces in the event log that reach the corresponding state) and the value
	of the reward function are computed.
	Each edge is  mapped to the tuple $(s, a, s',\mathcal{P}, \mathcal{R})$ % $(s, a, s',P(s'|s, a),r)$
	where $s$ is the state corresponding to the source node of the
	edge, $a$ is the action used for labelling the edge, $s'$ is the state corresponding to the
	target node of the edge, $\mathcal{P}(s, a, s')$ %$P(s'|s, a)$
	is computed as the percentage of the traces that
	reach the state $s'$ among all the traces that reach state $s$ and execute action $a$.
	%{\color{red}The reward function $\mathcal{R}(s, a, s')$ is computed as the average of all the edges $(s, a, s')$.}
	The reward function $\mathcal{R}(s, a, s')$ is computed as the average of the rewards $r(\sigma_k)$
	of those prefixes $\sigma_k$ corresponding to the edge $(s, a, s')$
	\footnote{A prefix $\sigma_k$ corresponds to an edge $(s, a, s')$ if $s'=s_{\sigma_k}$, $a=\text{Act}(e_l)$ and $s=s_{\sigma_{l-1}}$,
	%where $l = \max_j(\{j<k \, | \, \text{Act}(e_j)\in A_\text{agent}\})$ and $A_\text{agent} \subset A$ is the subset of agent's activities.
	where $e_l$ is the last event with $l<k$ associated to an agent's activity in $\sigma_k$.}.
	

\end{enumerate}

\subsection{RL and runtime phases}
\label{sec:RLtraining}

In the \emph{Reinforcement Learning} phase, the MDP obtained in the previous phase %subsection 
is trained by the RL algorithm 
to learn the optimal policy, as shown in  %displayed in the box \emph{reinforcement learning} of Figure 
\figurename~\ref{fig:pipeline}.
We use Monte Carlo simulation: each simulation uses the MDP to generate an episode, or trace,
whose final reward is eventually used to properly update the state-action value \eqref{eq:q_value} and then the policy via policy iteration.

In~\cite{us@BPM22} the authors noted that the MDPs mined from event data logs can have some issues.
Namely some action choices in the MDP guarantee to gain a high final reward almost certainly during the training,
however these choices are not reliable, because they appear in too few traces,
and so the very likely high reward is accidental and does not rely on a robust statistical correlation within the training data. 
This can be seen as an overfitting on the training data used to construct the MDP. % from which the MDP is constructed.

To tackle this issue in \cite{us@BPM22} a recalibration of the reward was performed for every transition
with a multiplicative factor depending on the number of occurrences of the given transition in the log.
In this way, very convenient but unreliable actions were discouraged during the training with respect to convenient actions more popular in the log.
%Inspired by that 
In this work, %we adopt a similar solution, which 
instead of scaling the reward of each transition, we scale the $q$-value function \eqref{eq:q_value} %scales the updating of the $q$-value function \eqref{eq:q_value}
as follows
\begin{equation}
\label{eq:scaled_q_value}
\tilde q(s,a) =  q(s,a) \, h(s,a)
\end{equation}
where $h: \mathcal{S}\times\mathcal{A} \to \mathbb{R}_{[0,1]}$ % $h(s,a)\in\mathbb{R}_{[0,1]}$
is a monotonically non-decreasing function of the number $n(s,a)$ of occurrences of the state-action pair %$(s,a)$
in the log. %of the state-action pair $(s,a)$.
%\todomro{Questi dettagli possono essere spostati pi\`u avanti}
The trivial choice $h^0(s,a)=1$ defines the standard $q$-value function.
In this work we consider three non-trivial types for $h$ and we test their effect on the policy learned:
\begin{itemize}
	\item a linear function of the number of occurrences: 
	{\small
	\begin{equation*}
	% \resizebox{.8\hsize}{!}{$
	h^\text{lin}(s,a) = \frac{n(s,a) - \min_{s'\in\mathcal{S}, a'\in\mathcal{A}}{n(s',a')}}
	{\max_{s'\in\mathcal{S}, a'\in\mathcal{A}}{n(s',a')} - \min_{s'\in\mathcal{S}, a'\in\mathcal{A}}n(s',a')}
	% $}
	\end{equation*}
	}
	%\text{minmaxscaler}_{a\in\mathcal{A}, s\in\mathcal{S}}\, n(s,a)
	\item a step function $h^\text{step}_{n_t}$ which is equal to zero for $n(s,a)$ smaller or equal than a certain threshold $n_t>0$ and is equal to one for higher values of $n (s,a)$, that is:
	\begin{equation*}
	% \resizebox{.4\hsize}{!}{$
	h^\text{step}_{n_t}(s,a) = 
	\begin{cases}
	0 & n(s,a) \leq n_t\\
	1 & n(s,a) > n_t
	\end{cases}
	% $}
	\end{equation*}
	\item a smooth
	%non linear concave
	function 
	\begin{equation*}
	% \resizebox{.5\hsize}{!}{$
	h^\text{smooth}_\lambda(s,a)=1-2\frac{e^{-n(s,a)/\lambda}}{1+e^{-n(s,a)/\lambda}}
	% $}
	\end{equation*}
	%parameterized with respect to a real value $\lambda>0$.
	parametrized by a real value $\lambda>0$.
	This choice for $h$ actually interpolates between the previous two proposals: indeed
	$h^\text{smooth}_{\lambda\to 0} \to h^\text{step}_{n_0=0}$
	and $h^\text{smooth}_{\lambda\to\infty} \to h^\text{lin}$.
\end{itemize}
For every choice of $h$ we obtain a different policy $\pi_h$ from the RL training.

Finally, in the \emph{Runtime} phase, as shown in %displayed in the box \emph{runtime} of 
\figurename~\ref{fig:pipeline}, the policy is used to recommend the best next activity(ies) for a ongoing process execution\footnote{
The recommender system takes the ongoing prefix, maps it to a state following the same steps as in Sect.~\ref{sec:constructionMDP}
and uses the policy to recommend the best next activity to maximize the reward (KPI).}.

%In the following section we design an evaluation setting to measure the performances of the policies $\pi^h$ obtained with our solution, and to compare them with a state of the art baseline.