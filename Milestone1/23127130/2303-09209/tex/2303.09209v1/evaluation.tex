%!TEX root = main.tex

\section{Evaluation setting}
\label{sec:evaluation}
In the following we report the evaluation setting we designed in order to measure the performance of the policies $\pi_h$ obtained by applying the proposed pipeline, and to compare them with a state-of-the-art baseline.

\subsection{Research questions}
\label{sec:RQ}
We aim at evaluating %In this section we evaluate
the capability of the proposed solution to conveniently
recommend the next activities so as to optimise the relevant KPI targeted. 
To this aim, we assess the capability of the proposed solution: (i) when different scaling functions $h$ are used; (ii) with respect to state-of-the-art approaches; (iii) when applied to datasets with different characteristics (e.g., size and success rate). More in detail, we are interested to answer %In particular, we want to answer to 
the following research questions:
%\todomro{non sono domande al momento}
\begin{enumerate}[label=\textbf{RQ\arabic*},leftmargin=2\parindent,topsep=1.5pt]
	\item\label{RQ1} How do the different instantiations of the scaling function $h$ perform in terms of policy capability to optimise the KPI of interest?
	%\item\label{RQ1} In Section \ref{sec:RLtraining} we commented on a reliability issue in the MDP used to train the RL agent.
	%To solve that issue we proposed a possible solution parametrized by a set of functions $h$.
	%We would like to compare the different types of $h$ functions presented and to see if some of them effectively 
	%produce more reliable results than the standard approach, which corresponds to the trivial choice $h=1$.
	\item\label{RQ2} How does the proposed approach perform in terms of policy capability to optimise the KPI of interest with respect to state-of-the-art techniques directly learning from temporal data (Deep RL approaches)?
	%\item\label{RQ2} The method we propose is based on classical RL algorithm and Monte Carlo simulations based on temporal data.
	%A natural research question is to compare the performance of our technique with state of the art deep RL technique, trained directly on the temporal data.
	\item\label{RQ3} How does the proposed approach perform in terms of robustness of the results with datasets of different sizes and success rates with respect to %state-of-the-art techniques directly learning from temporal data, i.e., 
	Deep RL approaches?
	%\item\label{RQ3} The third research question is about comparing the performances of our method and the deep Q-Learning baseline
	%with respect to different dimensions of the dataset: as the size of the log and on the KPI distribution in the log, that is
	%how easy is encounter a trace in the log with high KPI.
\end{enumerate}

\ref{RQ1} aims at comparing the different instantiations of the $h$ function presented and to evaluate whether they are able to recommend more effective policies than the ones generated using the standard function $h^0=1$.
%focuses on the different instantiations of the scaling function $h$ introduced in Section~\ref{sec:RLtraining}.
%, where we proposed as a possible solution to the reliability issue in the MDP a parametrized scaling function $h$ for the q-value~\eqref{eq:scaled_q_value}.
%\ref{RQ1} aims at comparing the different instantiations of the $h$ function presented and to evaluate whether they are able to recommend more effective policies than the ones generated using the standard function $h^0=1$.
The aim of \ref{RQ2} is to compare the proposed pipeline with state-of-the-art RL approaches that directly learn from temporal data without requiring further domain knowledge (e.g., the MDP) as input and, in particular, with black-box methods as Deep RL techniques.
Finally, \ref{RQ3} aims at evaluating the impact of different characteristics of the training dataset (dataset size and percentage of successful traces, i.e., traces scoring well on the KPI of interest) on our solution and Deep RL approaches to return a policy that optimises the KPI of interest.
%In detail \ref{RQ3} focuses on how much robust the two solutions are with respect to the dataset size and the percentage of successful traces, i.e., traces scoring well on the KPI of interest.}%The research question focuses on the robustness of the two families of techniques to different dataset sizes.}

\subsection{Datasets description}
\label{sec:dataset}

To evaluate the performance of the proposed solution we use %have chosen the process \btext{reported} %described in 
the real-world publicly-available BPI Challenge 2012 event log 
%of the BPI Challenge 2012 
(BPIC2012) \cite{vandongen_2012} and a set of four synthetic event logs inspired by the BPIC2012 event log.

The BPIC2012 event log describes the execution of a loan application process in a Dutch Financial Institute.
Every process execution %, or trace, 
represents a single application to the bank by a customer for a personal loan.
The application can be accepted or rejected by the bank, likewise the customer can withdraw the application at any time.
If the bank decides to grant a loan to a customer it generates an offer and sends it to the customer, that in turn can decide to accept the offer or to refuse it.
% This process is a good example of a \emph{multi-actor process}. Specifically in this process there are two main actors:
% the \emph{bank}, which grants the loans, and the \emph{customer}, who submits the loan application.
% The two actors have different goals, the customer desires a convenient offer for a loan,
% whereas %the bank seeks trustworthy customers to whom sent loan offer,
% the goal of the bank is the acceptance of the offer by the customer.
% Indeed, if the customer declines the offer, the time and the resources spent by the bank in generating the offer are wasted.
% %otherwise, if the customer decline the offer, the time and the resources spent in generating the offer are wasted.
This log is a good example of a multi-actor process scenario, where we chose to take the perspective of the bank actor, so that the agent is the \emph{bank} and the environment is the \emph{customer}. 
The two actors have different goals, the customer aims for a convenient loan offer, whereas the bank seeks reliable customers to whom to send loan offers. The overall goal of the bank is the acceptance of the offer by the customer.
Indeed, if the customer declines the offer, the time and the resources spent by the bank in generating the offer are wasted.
In this scenario we define the KPI that the bank aims at optimising as the profit deriving from the loan, excluding the costs spent in the process execution. The KPI of the bank is hence composed of two parts:
%With this process in mind we use our RL method to train an agent which can recommend the bank on the best activity to execute 
%at every time step in order to maximize a relevant KPI for the bank.
%Following the discussion above a good KPI has two main contributions:
(i) a positive part that is the profit made by the bank if the loan offer is accepted by the customer, namely the interest rate of the loan, which we arbitrarily set as the $15\%$ of the amount requested in the loan application; and
(ii) a negative part proportional to the working time spent in the process, which we set as a cost of 36 euros/h.


%\footnote{ ? one could also do the opposite point of view}
%Following the log enrichment step in Section \ref{sec:learning}, we label those activities which are owned by the bank (our agent) and by the customer (the environment).
%since the latter cannot be recommended to the bank.
%We then go through the pipeline of Figure \ref{fig:pipeline} and obtain our recommender systems to evaluate.

%We have used one real-world publicly-available datasets that, describing the behaviour of more than one actor, allow us to take the perspective of one of them (target), the BPI Challenge 2012 event log \cite{vandongen_2012} and four synthetic dataset generated from the real one.  The BPI Challenge 2012 dataset relates to a Dutch Financial Institute. The process executions reported in the event log refer to an application process for personal loan ....

%In addition to the real-world BPIC2012 event log, for our evaluation, we also consider four synthetic event logs
%which have been generated starting from the real one as we are going to describe.
%For the evaluation of the proposed method, along with the real-world BPIC2012 event log, we also generated four synthetic
%event logs that are based on the BPIC2012 event log.

%\btext{The four synthetic logs are also inspired by the BPIC2012 event log. For their generation }
For the generation of the four synthetic logs
%In order to generate the synthetic logs, 
we define a simulation model $\mathcal{M}$ %which is 
inspired by the BPIC2012 process. %described in the BPIC2012.
The simulation model consists of the BPMN model in \figurename~\ref{fig:simulation_model}, extended with additional information for the simulation aspects\footnote{
The complete BPMN model and all the simulation parameters used to define $\mathcal{M}$ are available in the evaluation repository.}
(case inter-arrival time, activity durations, routing probabilities, resource allocation and utilization, etc.).
The main differences between the four synthetic logs concern their sizes, that is the number of traces they contain,
and the probability that a loan application is (pre)accepted by the bank, which, in turn, affects the probability that a loan offer is accepted by a customer and hence the possibility for the bank to obtain a high KPI.
%that corresponds to the bank obtaining a high KPI.
% Actually is the probability of an application being accepted, that indirectly changes also the probability the loan is accepted
Table~\ref{tab:dataset} shows the details of the different event logs (including the real one). 
$\mathcal{L}_{rare}^s$ and $\mathcal{L}_{rare}^b$ are the logs with a low success rate, that is with a low percentage of traces achieving the acceptance of the offer by the customer (activity \texttt{accept} in \figurename~\ref{fig:simulation_model}), and hence with a low average KPI value. This is achieved by imposing a low probability (less than 50\%) in the model to have a pre-accepted loan (gateway 1 in the model).
%\todocdf{@Massimiliano e Francesca: l'ho riscritto così: provate a vedere se vi torna.}
%$\mathcal{L}_{rare}$ is a log that has a low probability, less than half, to proceed with operations connected with acceptance of the loan application.
%!! to make clear the difference between loan application and loan offer
%to proceed with the \emph{application accepted operations}.
After this first gateway, the probability of obtaining the loan depends on different factors: the amount required in the loan application, the number of offers created and calls made by the bank during the process.
%Therefore a $\mathcal{L}_{rare}^s$ is a log with 2000 traces, where few of these end with a positive outcome, i.e.~the customer accepting the loan offer by the bank. The same case holds for $\mathcal{L}_{rare}^b$, a log with 10000 traces where few cases exhibit a positive outcome.
%\btext{These four synthetic logs will be used to answer \ref{RQ3}.}
The differences between these four synthetic logs will be central for answering~\ref{RQ3}.
%The four synthetic logs created in this way are convenient to test the third research question \ref{RQ3}.


\subsection{Baseline definition}
%In order to answer to the second research question \ref{RQ2} we trained an offline Deep-Q Network (DQN) method to learn the best policy the bank should follows to maximize the KPI define in the previous subsection.
With the aim of comparing the proposed pipeline with state-of-the-art approaches directly learning from temporal data, that is Deep RL approaches (\ref{RQ2} and \ref{RQ3}), we trained an offline Deep-Q Network (DQN).
%In order to have a baseline for comparing the performance of the proposed method, an offline Deep-Q Network (DQN) method was utilised to learn the next best actions to take given a current state.
%In offline reinforcement learning, the agent cannot interact with the environment and collect additional transitions from the behavioural policy to learn $Q^*(s,a)$, but instead relies on a static set of transitions that were previously collected. This is different from the off-policy paradigm, where even though the agent is learning from a buffer of experiences, it still has the chance to interact with the environment during the learning process.
Within a complex environment, where there can be a large number of states, it is difficult to compute the action-value function $Q(s,a)$ for every possible state-action pair and thus model approximation methods have to be used to approximate $q^*$.
Following the architecture presented in~\cite{10.1007/978-3-030-72693-5_10}, we employed a Long-Short Term Memory (LSTM) to approximate $q^*$.
Since LSTM-based neural networks preserve the sequentiality of the input, when encoding the traces for the DQN method, we encoded the frequency at each timestep, foregoing the last seen position encoding used to construct the MDP in Section~\ref{sec:learning}.

%The LSTM model was trained over batches of size 128 for 1\,500 epochs.
Q-learning updates at iteration $i$ are done through the use of the mean squared error:
\begin{equation}
\label{eq:DQNloss}
% \resizebox{0.9\hsize}{!}{$
	L_i(\theta_i)= \mathbb{E}_{s,a,r,s'} \left[ r + \gamma \max_{a'} Q(s',a',\theta^{-}_{i}) - Q(s,a,\theta_i))\right]^2
	% $}
\end{equation}
where $\gamma$ represents the discount factor applying penalties to future rewards, $\theta_i$ represents the parameters of the LSTM model at iteration $i$ and $\theta^{-}_{i}$ represents the parameters of the target network at iteration $i$ used to approximate $\max_{a'} q^*(s,a)$.
%During training, we fix a random integer between $10\%$ and $50\%$ of the length of the training set to perform Q-learning updates, while the weights of the model remain fixed between individual updates.
%\todocdf{Do we have a reference or a motivation for the choices made (batches, epochs, Q-learning updates?}
%\todoandi{The choice was based on the different experiments we tried, changing batch sizes, epochs and learning updates to balance time/performance}
In the literature, DQNs are trained by randomly sampling batches of experiences from the replay buffer during training to avoid overfitting.
For trace executions, by sampling a single transition from one timestep to the other, the sequentiality of individual traces would not be picked up by the LSTM model.
To overcome this and avoid overfitting, we sampled traces to be trained sequentially but randomize the order of the traces in the log to avoid spurious correlations that might be learned by the order in which the traces are fed into the model.
Since some traces in the event log have long executions in terms of the activities performed, a proper window size was used when building the state input for the LSTM model to preserve long-term dependencies.
Since some actions are not available at specific timestamps, action masking is utilised to constrain the possible action space based on the last activity in the trace at the current timestep.
For our baseline we implemented a masking function that, during the $Q(s,a)$ value update,
%we encourage the model to take actions in the current state that were also seen previously in the event log.
restrict the $\max_{a'}$ function in~\eqref{eq:DQNloss} to those actions that have been seen in the event log.
Since we cannot interact with the environment and perform exploration, this allows us to restrict the space of possible choices to the ones observed in the data.
%and thus are restricted to making choices that were previously observed in the data.


\subsection{Evaluation methods}
\label{sec:ev_methods}

%For the evaluation of the optimal policy obtained by RL and for answering our three research questions, two different evaluations have been carried out: a simulation evaluation and a test log evaluation.

%For the evaluation of the optimal policies obtained with our RL pipeline and for answering the research questions of Sect.~\ref{sec:RQ}
%we have designed two different kind of evaluations: 
We design two different evaluation approaches to answer the research questions in Section~\ref{sec:RQ}: a simulation evaluation %available for the four synthetic log 
and a test log analysis. %available also for the real-world BPIC2012 log.

\subsubsection{Simulation evaluation}

A well-known problem in evaluating recommender systems for a real-world process scenario
is the difficulty of
%testing it on the ground
testing the recommendations provided by the recommender system~\cite{DBLP:conf/bpm/Dumas21}.
To circumvent this issue we leverage synthetic logs. Indeed, this allows us to
%The inclusion in our evaluation of the synthetic logs described in Section~\ref{sec:dataset} helps to circumvent this issue.
%Indeed we can 
test the policies trained on the synthetic logs, by simulating a testing situation via the model used to generate the log.
%\todocdf{Potete controllare che sia corretto?}
%by applying them directly to the simulation model which was used to generate the log itself.
%Although this offers a way to perform an online evaluation of the recommender system, the downside of using these synthetic logs is that the behaviour present within them
%is usually less complex when compared to the behaviour of real-life logs.
%The downside of synthetic logs to be too ''well-behaved'' is therefore compensated by the possibility to use their generating model to perform
%online evaluation of the recommender systems.


%The simulation evaluation using a modified version of the previous simulation model $\mathcal{M}$ used to generate the synthetic logs. In particular, $\mathcal{M}'$ used the optimal policy obtained by RL and the DQN to define the next activity for the bank.
%In fact, due to the knowledge of the process model and the environment's behaviour, we can evaluate the optimal policy in action, so whether following it the bank succeeds in achieving to a positive outcome.

The simulation evaluation uses a modified version of the simulation model $\mathcal{M}$ employed to generate the synthetic dataset in Section~\ref{sec:dataset}, 
This modified model $\mathcal{M}'$ uses the policies obtained by our method, or by the DQN baseline, to recommend the next activity for the bank.
Each step of the simulation is classified as a multiple decision point, i.e.~either the agent (bank) or the environment (customer) can act,
or as a single decision point, where only the agent can perform an activity.
%For the multiple decision points, as the \emph{gateway}$3$ and \emph{gateway}$5$ in Figure \ref{fig:simulation_model},
%the customer (environment) has the priority to perform or not an activity based on the probabilities computed over the prefix $\sigma_k$
%and the other factors like in $\mathcal{M}$. %!! questa frase dobbiamo riscriverla

For the multiple decision points, as \emph{gateways} 3, 5 and 6 in Figure \ref{fig:simulation_model},
the environment has priority over the agent,
so it can act (or not act) according to the probability distribution of the original model $\mathcal{M}$
\footnote{As described in Section~\ref{sec:dataset} the probability distribution of the environment response depends on the ongoing prefix $\sigma_k$.}.
% 
% l'agente decide dove andare, se passa per un gateway dell'environment (che sarebbe un gw che precede un'attività dell'env)
% allora agisce prima l'environment con una probabilità di agire pari a quella del modello originale M (cioè può anche non agire)
%
%Whether the environment does nothing, the agent can perform his choice based on the policy that is being evaluated. \todocdf{Non ho capito questa frase ... forse si pu\'o togliere dato che abbiamo gi\'a detto che l'ambiente ha priorit\'a in caso entrambi possano agire?}% in order to obtain the best result.
%In those cases in which the environment does not act, the actor can still make its choice based on the policy under evaluation.
The policy requires as input the prefix $\sigma_k$ of the trace that is being simulated, and returns the recommended next activity.
The model $\mathcal{M}'$ checks if the recommended next activity is allowed, i.e.~the activity satisfies the BPMN model,
otherwise the trace simulation ends with an exception and the reward is the one accumulated until then.
%!! qui ho semplificato, ma in pratica \`e giusto perch\'e anche il metodo RL da sempre solo una attivit\`a credo.
In addition, during the simulation, it may happen that a trace never reaches the end of the process
because the policy keeps recommending activities that have negative effects on the environment response,
that is, it reduces to zero the probability of the environment to respond.
In this case, we stop the trace simulation as before.

%Whether the customer does nothing, the bank can perform his choice based on policy that is being evaluated in order to obtain the best result.
%The RL and the DQN models require as input the prefix of the trace $\sigma_k$ and they return a list of recommended next activities (RL) or a single activity (DQN). In the case of a list of activities, $\mathcal{M}'$ chooses randomly an activity from the possible ones, i.e. the activities that respect the BPMN model. Instead of the single activity, $\mathcal{M}'$ only checks if it is permitted, otherwise the trace's simulation ends with an exception and the reward is the one accumulated until then. In addition, during the simulation it may happen that a trace never reaches the end of the process because the optimal policy continues to suggest an activity that, for the environment, has a negative effect and reduces up to zero probability of responding, also in this case we stop the trace's simulation as before.

%To compare our RL model with the basic model, we built a $\mathcal{M}'$ for each distinct recommended model computed over the synthetic logs: the baseline model (DQN) and the four RL models created for every policy \todo{da rivedere i nome delle policy} (\textit{none, linear, step, exponential}).
%!! \`e stato costruito un modello di simulazione M' per ogni policie (RL + DQN = 5) e per ogni log (4) = 20 modelli totali

%{\color{red} (mro: l'ho riscritta cos\`i ma andrebbe spiegata meglio forse)}
For every synthetic log ($\mathcal{L}^s, \mathcal{L}^b, \mathcal{L}_{rare}^s, \mathcal{L}_{rare}^b$)
we generate a new model $\mathcal{M}'$  for each policy trained on it ($\pi_0, \pi_\text{lin}, \pi_\text{step}, \pi_\text{smooth}, \pi_\text{DQN}$).
Then we use these models to simulate the behaviour of the environment in response to the agent's action recommended by the policy for
5\,000 traces and compute their average reward, which is the metrics we use to evaluate and compare the techniques.
%\todocdf{Potrebbe andare bene riscritta in termini di simulazione invece che di generazione di dataset? Prima era così
%Then we used these models to generate datasets of 5\,000 traces each and compute the average rewards within each datasets.} %si, ottimo
%Finally we compare the average rewards with each other and also with the average reward obtained with the original model $\mathcal{M}$ used to generate the log.
%todocdf{Ho tolto la parte in cui diciamo cosa ci facciamo}
Although this type of setting offers a way to perform an online evaluation of the recommender system, it can be used only for synthetic scenarios in which the model is available. However, the behaviour of synthetic logs can be less complex when compared to the behaviour of real-life logs. We hence also have a test log analysis evaluation.

\begin{figure*}[t]
	\centering
	\includegraphics[width=.9\textwidth]{images/simulation_model.png} 
	\caption{The BPMN model used by $\mathcal{M}$ to generate the synthetic logs and by $\mathcal{M}'$ to perform the simulation evaluation.}  \label{fig:simulation_model}
\end{figure*}


\begin{table}
	\centering
	\resizebox{.9\columnwidth}{!}{
	\begin{tabular}{lcccccc}
			\toprule
			\multirow{2}*{Dataset}  & \multirow{2}*{Trace \#} & \multirow{2}*{Variant \#}  &  \multirow{2}*{Event \#}  & Avg.trace  &
			Application & Offers  \\
			&  &  &  & length & preaccepted & accepted  \\
			\midrule
			BPI2012  &  13087 & 4366 & 262200 & 20 & 56\%  &  17\%  \\
			$\mathcal{L}^s$  &  2000 & 704 & 29077 & 29 & 60\%  &  23\%  \\
			$\mathcal{L}_{rare}^s$ &  2000  & 496 & 22165 & 22 & 40\% &  16\%  \\
			$\mathcal{L}^b$ &  10000  & 3083 & 149997 & 29 & 60\%  & 24\%   \\
			$\mathcal{L}_{rare}^b$  &  10000 & 2070 & 109652 & 22 & 40\%  &  16\% \\
			\bottomrule
	\end{tabular}}
	\caption{Datasets Description.}
	\label{tab:dataset}
\end{table}


\subsubsection{Test log analysis}

We perform an analysis of the test event log, which is the same analysis considered in the work \cite{us@BPM22}.
This analysis is important as it allows us to evaluate the performance on the real-world log, for which we do not have the exact process model.
However, to test the trustworthiness of this type of evaluation, we also apply it to the synthetic logs, so that for these datasets the two
type of evaluations (simulation and test log analysis) can be compared.


We perform two types of log analysis.
In the first one we compare the average KPI value of the traces in the test log that follow the optimal policy
with the average KPI value of all the traces in the log.
%This evaluation is applied to all the logs (real and synthetic).
In the second log analysis we focus on evaluating recommendations for ongoing executions,
i.e., we measure how much following the policy can improve the outcome of cases where some activity has already been executed.
To do this we consider, for each trace in the test event log, all its prefixes and separately analyse each of them, as a potential ongoing execution.
For each prefix $\sigma_k$ of a trace $\sigma$ in the test event log we compare the ground truth value of the KPI of interest of the trace $\sigma$ once completed,
against an estimation of the value of the KPI obtained following the optimal policy from that execution point forward.
The estimation is obtained by averaging the KPI values of all the traces $\tau$ in the log that have the
same prefix $\tau_k=\sigma_k$ and follow the optimal policy from there on.

\subsection{Training procedure}
\label{sec:RL_details}

Each dataset described in Table~\ref{tab:dataset} was split into two parts: $80\%$ was used to train the agent of the RL model and 20\% %policies and a $20\%$ part used 
for testing the retrieved policy %the test log analysis (Section~\ref{sec:ev_methods}).
\footnote{
For the test log analysis (Section~\ref{sec:ev_methods}) we excluded from the test event log %for the analysis
all those traces that end without the presence of an actual decision point for the agent, since the outcome of these traces
%(and hence their overall KPI value) %these traces outcome 
cannot be modified by the decisions of the agent.}.
%We briefly report here on the details of the RL trainings.
%We trained five policies for every datasets: four are computed with our proposed solution ($\pi_0, \pi_\text{lin}, \pi_\text{step}, \pi_\text{smooth}$)
%and a baseline $\pi_\text{DQN}$.

%{\color{red} (mro: questi dettagli potrebbero anche essere spostati nelle relative sezioni 3 e 4.3)}
%Here the details on our solution. 
Each log was preprocessed as described in Section~\ref{sec:constructionMDP}.
In the kmeans clusterization step we selected 100 clusters for every log, this choice was made after performing a silhouette analysis on every dataset.
We trained the RL agent in order to obtain 4 different policies, each corresponding to one of the instantiations of the scale function  %Each of the four policies corresponds to the selection of a type of scale function $h$ 
described in Section~\ref{sec:RLtraining}:
$\pi_0$ to the trivial case $h^0=1$, $\pi_\text{lin}$ to the linear function $h^\text{lin}$, $\pi_\text{step}$ to the step function $h^\text{step}_{n_t=50}$
and $\pi_\text{smooth}$ to the smooth function $h^\text{smooth}_{\lambda=50}$.\footnote{$n_t=50$ and $\lambda=50$ were selected after an explorative analysis available at the evaluation repository.} %The choices $n_t=50$ and $\lambda=50$ have been selected among other experiments we performed, the results about these other experiments are reported in the additional material \dots}

%Each Monte Carlo training has been performed generating 5 millions episodes, with discount parameter $\gamma=1$ (namely no discount),
%exploration rate $\epsilon=0.1$ and learning rate $\alpha$ decreasing linearly for each episode from $0.1$ to $0.001$.
%For training the baseline DQN, a starting learning rate $\alpha$ of $0.001$ that decays
%after every epoch by a factor of $0.9$ has been used for each event log. The discount parameter $\gamma$ was set to $0.95$. For both $\gamma$ and $\alpha$ we experimented with different values and chose the ones that provided the best results. As our baseline is not capable of
%performing exploration, the exploration rate $\epsilon$ was not used when training.\todoandi{Not sure if this needs to be mentioned here}

 
