%!TEX root = main.tex


\section{Evaluation results}
\label{sec:result}

%In this section we report the results of the evaluation tests described in the previous section.



%\subsection{Policies comparison}
%\label{sec:policies_comparison}

In this section we show the results of the evaluation and answer the research questions of Section \ref{sec:RQ}. All the code and material are available at \url{https://tinyurl.com/repOptimalPolicy}.  

To answer \textbf{RQ1} we compared the four policies obtained with our method for each dataset ($\pi_0, \pi_\text{lin}, \pi_\text{step}, \pi_\text{smooth}$).
We compared them using the two methods described in Sect.~\ref{sec:ev_methods}.
%In 
Figure~\ref{fig:plot_rq1l} reports the average rewards obtained for the synthetic logs in the simulation evaluation. %are shown for the synthetic logs.
It can be noticed that all the policies provide a %give 
much higher average reward than the standard average reward of the log $\mathcal{L}$ %as 
generated by the model $\mathcal{M}$.
However, among the four policies there is no definite winner, and their performances are actually close. %one another.
To refine this analysis we report in Table~\ref{tab:PvP_ptest} the difference in terms of average reward between pairs of policies, by highlighting in bold the statistically relevant differences (p-value $\le 0.05$). %average reward differences between these policies, 
%together with an indicator of statistical relevance.
%For example in the log $\mathcal{L}^b_{rare}$ there is only one statistically relevant difference between two policies ($\pi_\text{step}$ vs $\pi_0$).
For example, only one statistically relevant difference was found for the log $\mathcal{L}^b_{rare}$ between $\pi_\text{step}$ and $\pi_0$.

Figure~\ref{fig:plot_start_end} shows the average rewards of the optimal traces obtained for the five logs (synthetic and real) in the test log event analysis.
%In Figure \ref{fig:plot_start_end} the average rewards of optimal traces in the test log event analysis are shown for all five logs (synthetic and real).
In this case the results are more varied and differences can be observed when compared to %they are not completely aligned with 
the results in Figure \ref{fig:plot_rq1l}.
%This is probably due to the low statistic since there are only few traces in every log following the optimal policies.
%One can notice that 
Indeed, there are few cases in which the optimal traces with respect to a policy actually have an average reward lower than the average reward of the test log. %of all the traces in the log.
%\todocdf{Ma solo nel caso di $\pi_{lin}$ e un caso di $\$pi_{step}$. Forse possiamo dire few cases?}
However, these latter results have to be considered cautiously considering the low statistic of the samples used to compute the average reward.
%{\color{red} We stress that the result of this analysis has to be consider cautiously because of the low statistic of the samples used to computes the means
%in Figure \ref{fig:plot_start_end}}.
%This can be due to two factors: 1) actual poor performance of the policy, 2) absence in the log of enough traces following the optimal policy. {\color{red} (questa cosa posso spiegarla meglio)}
%Notice that the policy $\pi_\text{smooth}$ seems the only one to have steady performances among the logs in this evaluation.
Although the analyses carried out do not allow us to identify the best scaling function instantiation, we can state that, by applying a scaling function, we usually obtain better results and that $\pi_\text{smooth}$ seems to have steady performance across all the logs (\textbf{RQ1}).
%\todocdf{Ho spostato la frase su pi\_smooth come risposta alla RQ ... @Massimiliano non so se la vuoi rimettere prima e toglierla da qui} %mro: qui mi pare ok


\begin{figure}[t]
	\centering
	\includegraphics[width=.43\textwidth]{images/bar_plot.png} 
	\caption{Simulation evaluation results for the synthetic logs.
	The column $\mathcal{L}$ corresponds to the average reward of the synthetic log generated by the model $\mathcal{M}$,
	without the application of any policy.}  \label{fig:plot_rq1l}
\end{figure} %\todocdf{Forse in questa caption va introdotto cosa e\'L}

\begin{table}
	\centering
	\resizebox{.7\columnwidth}{!}{
	\begin{tabular}{lcccc}
			\toprule
			$\pi_1$ vs $\pi_2$ & $\mathcal{L}^s$ & $\mathcal{L}_{rare}^s$ & $\mathcal{L}^b$ & $\mathcal{L}_{rare}^b$  \\
			\midrule
			$\pi_\text{lin}$ vs $\pi_\text{smooth}$  & 29 & \bf{-152} & -45 &   7 \\
			$\pi_0$ vs $\pi_\text{smooth}$& -35  &  -23 & -28 &  -19  \\
			$\pi_0$ vs $\pi_\text{lin}$ &  \bf{-64}  & \bf{129}  & 17 &  -26    \\
			$\pi_\text{step}$ vs $\pi_\text{smooth}$ &  10  & -85 & \bf{-120} &  33 \\
			$\pi_\text{step}$ vs $\pi_\text{lin}$ &  -19 & \bf{67} & \bf{75} & 26    \\
			$\pi_\text{step}$ vs $\pi_0$  & \bf{45}  & -62 & \bf{92} &  \bf{52}  \\
			\bottomrule
	\end{tabular}
	}
	\caption{Comparison between policies for different scaling functions $h$.
	Each value is the difference of the average reward computed with the simulation evaluation.
	The boldface values are statistically relevant differences, i.e.~with p-value $\le 0.05$.}
	\label{tab:PvP_ptest}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=.43\textwidth]{images/prova_bar_plot.png} 
	\caption{Average reward of optimal traces in the logs. The $\mathcal{L}$ column represents %is to 
	the average reward of the entire test log.
	The numbers above each column correspond to the number of traces used to compute the mean.}  \label{fig:plot_start_end}
\end{figure}

%Let us consider \textbf{RQ2}.
Focusing on \textbf{RQ2}, in Figure~\ref{fig:plot_rq1l} we can notice that DQN
%In Figure \ref{fig:plot_rq1l} we can notice the DQN baseline 
usually performs worse than our method in the simulation evaluation.
More precisely, looking at Table \ref{tab:PvDQN_pbest} we can see that only for $\pi_0$ in $\mathcal{L}^b_{rare}$ and 
$\pi_\text{step}$ in $\mathcal{L}^b$ the difference with %the 
DQN %baseline 
is not statistically relevant.
Instead, $\pi_\text{lin}$ and $\pi_\text{smooth}$ perform better than the baseline in every log.

The result of the test log analysis is more difficult to interpret.
In fact, in Figure \ref{fig:plot_start_end}, there are only two test logs ($\mathcal{L}^s$ and $\mathcal{L}^b$) with
traces following the DQN policy, and these traces have a very low average reward.
However this does not directly imply that the baseline performs poorly, as can be seen from the simulation evaluation.
%and is indeed proven wrong for the synthetic logs by the simulation evaluation commented above.

%We give a confident answer to \textbf{RQ2} at least for the synthetic logs.
To understand better the case of the real-world log BPIC2012 we performed the second type of event log analysis,
%analysis of the prefixes of its test log,
which evaluates recommendations on ongoing executions.
%as described at the end of Sect.~\ref{sec:ev_methods}.
The average gained reward for every prefix length is shown %displayed 
in Figure~\ref{fig:plot_prefix}.
Notice that only at prefix length $11$ we get an estimated gain for DQN, and this is a negative gain reward.
%estimated average gain reward than the real execution.
%\todomro{mro: per alleviare il punto seguente forse si potrebbe mettere il grafico dei totali invece che delle medie per essere pi√π indulgenti verso il nostri metodi in questa valutazione}
It can also be noticed that for longer prefixes, DQN results start improving %bigger prefix length the baseline evaluation starts to improve
and, for prefixes longer than $20$, even outperform the proposed method.
%{\color{red} Indeed optimal traces recommended by the baseline agent may be missing from the log
%and so cannot contribute to the evaluation.}\todocdf{Forse questo rosso si pu\'o togliere}
%\textcolor{red}{As mentioned previously, it is likely that the optimal policy learned by the DQN
%network is not present in the log and for short prefixes the execution of the traces always diverges from the recommendations, while for longer prefixes,
%the recommendations provided by the baseline are in line with the trace executions for those large prefix lengths.}
%\todoandi{Rewrote the interpretation of the results of DQN for both analyses, to be checked}
The negative results observed in the two analyses could be attributed to the absence of optimal traces suggested by the DQN agent in the test log.
%As a result, these traces are unable to contribute to either of the evaluations.
This issue is particularly evident in the second analysis, where for shorter prefixes, the trace execution consistently deviated from the recommended actions.
%,while for longer ones, DQN outperforms the other policies in identifying the optimal recommendations.
%However, for longer prefixes, the results were better than those of the other proposed policies.}
%{\color{red} Indeed optimal traces recommended by the baseline agent may be missing from the log
%and so cannot contribute to the evaluation.}\todocdf{Forse questo rosso si pu\'o togliere}
We stress that this kind of evaluation is limited, as it can not evaluate optimal traces which are not present in the test log.
%Since the real-world scenario is the most interesting one it would be nice to improve this evaluation method so to obtain
%a more reliable evaluation starting from the analysis of the test log. 	
%\todocdf{Ho tolto la frase sul desiderata di altri log}
The analyses carried out show that the proposed pipeline allows us to significantly outperform (on synthetic logs) or in the worst case compare existing approaches (\textbf{RQ2}).

Finally, concerning \textbf{RQ3}, by looking at Figure \ref{fig:plot_rq1l} it is clear that, with respect to the size of the log used in training, our method %depends very little on
is more robust than the DQN. Indeed, the performance for %the $2\,000$ traces log
$\mathcal{L}^b$ and $\mathcal{L}^b_{rare}$
are close to the one for %correspondent $10\,000$ traces log 
$\mathcal{L}^s$ and $\mathcal{L}^s_{rare}$ respectively.
On the contrary, the performance of DQN greatly decreases for logs of small size.
%This is different from the behaviour of the DQN baseline whose performance decrease greatly for the small size logs.
This effect is even more evident in the case of the \emph{rare} logs, i.e.~those with low success rate.
Indeed in $\mathcal{L}^s$, DQN has poor but positive performance,
whereas in $\mathcal{L}_{rare}^s$ its performance is entirely negative.
%Indeed even if in $\mathcal{L}^s$ DQN has poor performance with respect to our method, this is still positive,
%whereas in $\mathcal{L}_{rare}^s$ the performance of the DQN is entirely negative.

This suggests that our method is more robust than the offline DQN baseline with respect to changes in the log size and success rate (\textbf{RQ3}).
The reason is probably two-fold:
%1) deep learning algorithms need many data to provide an efficient results, especially many example of successful traces;
(i) deep learning algorithms require more data to provide good results --- especially many examples of successful traces;
(ii) the offline trait of our DQN baseline training does not allow the agent to explore traces not present in the log, limiting its capability to
%impoverishing the capability of 
act successfully in unseen states.


\begin{figure}[t]
	\centering
	\includegraphics[width=.43\textwidth]{images/prefix_plot.png} 
	\caption{Estimated average gained reward following the policy from a given prefix length for the BPIC2012 test event log.}  \label{fig:plot_prefix}
\end{figure}




\begin{table}
	\centering
	\resizebox{.7\columnwidth}{!}{
	\begin{tabular}{lcccc}
		\toprule
		$\pi_1$ vs $\pi_2$ & $\mathcal{L}^s$ & $\mathcal{L}_{rare}^s$ & $\mathcal{L}^b$ & $\mathcal{L}_{rare}^b$  \\
		\midrule
		$\pi_\text{0}$ vs $\pi_\text{DQN}$ & \bf{336} & \bf{633} & \bf{71} & 35    \\
		$\pi_\text{lin}$ vs $\pi_\text{DQN}$ & \bf{400}  &  \bf{504} &\bf{54} & \bf{61}   \\
		$\pi_\text{step}$ vs $\pi_\text{DQN}$&  \bf{381}  & \bf{571}  & -21 & \bf{87}     \\
		$\pi_\text{smooth}$ vs $\pi_\text{DQN}$ &  \bf{371}  & \bf{656} & \bf{99} &  \bf{54} \\
		\bottomrule
	\end{tabular}
	}
	\caption{Comparison between our policies and the DQN baseline.
	Each value is the difference of the average reward computed with the simulation evaluation.
	The boldface values are statistically relevant differences, i.e.~with p-value $\le 0.05$.}
	\label{tab:PvDQN_pbest}
\end{table}