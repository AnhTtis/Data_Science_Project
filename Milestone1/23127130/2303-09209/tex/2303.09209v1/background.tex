%!TEX root = main.tex

\section{Background}
\label{sec:background}

In this section we provide the background knowledge necessary to understand the rest of the paper.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Event log}

An \emph{event log} $\mathcal{L}$ consists of traces representing executions of a process (a.k.a.~cases).
A \emph{trace} $\sigma = \langle e_1, e_2, \dots, e_n \rangle$ is an ordered finite sequence of events $e_i$,
each referring to the execution of an activity label $\act{a} \in A$.
The \emph{event} $e_i = (\act{a},t,p_1,\ldots,p_n)$ is, in turn, a complex structure comprising the activity label $\act{a}=\text{Act}(e_i)$, its timestamp $t$, indicating the time in which the event has occurred, and possibly \emph{data payloads} $p_1,\ldots,p_n$, consisting of attributes, such as, the resource(s) involved in the execution of an activity, or other data recorded during the event.
%Some of these attributes do not change throughout the different events in the trace, i.e., they refer to the whole case (\emph{trace attributes}); for instance, the personal data (Birth date) of a customer in a loan request process. Other attributes are specific of an event (\emph{event attributes}), for instance, the employee who creates an offer, which is specific of the activity \act{Create offer}.
%A partial trace execution, or \emph{prefix} of a trace, is a subset $\sigma_k$ of trace $\sigma = \langle e_1, e_2, \dots, e_n \rangle$, containing the first $k$ events of $\sigma$, with $k<n$.
A partial trace execution, or \emph{prefix}, of a trace $\sigma = \langle e_1, e_2, \dots,
e_n \rangle$ is a sequence $\sigma_k$ containing the first $k$ events of $\sigma$, with $k \leq n$.
For instance, given a trace $\sigma = \langle e_1, e_2, e_3, e_4 \rangle$, the prefix $\sigma_2 = \langle e_1, e_2 \rangle$.

We define the \emph{frequency} $f_{\act{a}}(\sigma)$ of a trace $\sigma$, 
the number of times the activity label $\act{a}$ occurs in the trace $\sigma$.
We define the \emph{position} $p_{\act{a}}(\sigma)$ of a trace $\sigma$, 
the last position in which the activity label $\act{a}$ occurs in the trace $\sigma$,
with the first event having position 1.
Both frequency and position are equal to zero if the corresponding activity is missing in the trace.
For instance, given a trace $\sigma = \langle (\act{a}_1, t_1), (\act{a}_2, t_2), (\act{a}_1, t_3)\rangle$, $f_{\act{a}_1}(\sigma)=2$ and $p_{\act{a}_1}(\sigma)=3$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reinforcement Learning}

Reinforcement Learning~\cite{Sutton1998}
is a form of unsupervised learning where an agent learns to act in an environment by
maximizing the total amount of reward received by its actions.
At each time step $t$, the agent chooses and performs an \emph{action} $a$ in response to the observation of the \emph{state} of the environment $s$.
Performing action $a$ causes, at the next time step $t+1$, the environment to stochastically move to a new state $s'$,
and gives the agent a \emph{reward} $r_{t+1}=\mathcal{R}(s,a,s')$ that indicates how well the agent has performed.
The probability that, given the current state $s$ and the action $a$, the environment moves into the new state $s'$
is given by the state transition function $\mathcal{P}(s,a,s')$.

The learning problem is therefore described as a discrete-time MDP $M$, %Markov Decision Process (MDP), 
which is defined by a tuple $M=(\mathcal{S}, \mathcal{A}, \mathcal{P},\mathcal{R}, \gamma)$, where:
\begin{itemize}
    \item $\mathcal{S}$ is the set of states.
    \item $\mathcal{A}$ is the set of agent's actions.
    \item $\mathcal{P}:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}_{[0,1]}$ is the transition probability function.
		$\mathcal{P}(s,a,s') = Pr(s_{t+1} = s' | s_t = s, a_t = a)$ is the probability of transition (at the time step $t$) from state $s$ to state $s'$
		under action $a \in \mathcal{A}$.
    \item $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ is the reward function.
		$\mathcal{R}(s,a,s')$ is the reward obtained by going from $s$ to $s'$ with action $a$.
    \item $\gamma \in \mathbb{R}_{[0,1]}$ is the discount factor for the rewards.
		Value of $\gamma < 1$  models an agent that discounts future rewards.
\end{itemize}

An MDP satisfies the \emph{Markov Property}, that is, given $s_t$ and $a_t$, the next state $s_{t+1}$ is conditionally independent
of all prior states and actions, and it only depends on the current state, i.e., $Pr(s_{t+1}| s_t, a_t)=Pr(s_{t+1}|s_0, \cdots, s_t, a_0, \cdots, a_t)$.

%The goal of RL is to learn a \emph{policy} that allows the agent to maximize the cumulative reward. %This is repeated above
A policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ is a mapping from each state $s \in \mathcal{S}$ to an action $a \in \mathcal{A}$,
and the \emph{cumulative reward} is the (discounted) sum of the rewards obtained by the agent while acting at the various time points. % $t$. \todocdf{Qui serve $t$?}

%The agent learns the policy $\pi$, which maximizes
The \emph{state-action value function} $q^\pi$ is the expected discounted future reward obtained by taking action $a$ in state $s$ and then continuing to use the policy $\pi$ thereafter
\begin{equation} 
\label{eq:q_value}
% \resizebox{0.7\hsize}{!}{$ % comment this to get standard size equations
	q^{\pi}(s,a) = \mathbb{E}_\pi
	\bigg(\sum_{k=0}^\infty\gamma^k r_{k+t+1} \bigg|s=s_t,a=a_t \bigg)
% $}
\end{equation}
where $r_{t+1} = \mathcal{R}(s_t,a_t,s_{t+1})$ is the reward obtained at time $t+1$ and $\mathbb{E}$ means expectation value.

A policy is \emph{optimal}, usually denoted as $\pi^*$, if it maximizes the state-action value~\eqref{eq:q_value},
that is if $q^*(s,a) \coloneqq q^{\pi^*}(s,a)\ge q^{\pi}(s,a)$ for all $\pi$, $s\in \mathcal{S}$ and $a\in \mathcal{A}$.
Learning the optimal policy is the goal of RL.

In RL, \emph{on-policy} methods improve the current policy, %for selecting actions,
while \emph{off-policy} methods improve a target policy different from the policy used for generating experiences.%\cite{Sutton1998}.
%In the case of off-policy methods, by separating the two policies, the target policy could be greedy, i.e.~maximizing the expected cumulative reward, while the policy that generates experiences continuously samples possible actions.

%Different algorithms can be used for either on-policy or off-policy method.
%For on-policy methods we can find the \emph{policy iteration} approach.
%In the former the optimal action-value function $q^{*}(s,a)$  is obtained by iteratively updating the estimate $q^{\pi}(s,  a)$.
%In the latter,
%The starting point is an arbitrary policy $\pi$ that is iteratively evaluated (\emph{evaluation phase})
%and improved (\emph{optimization phase}) until convergence.
%\emph{On-policy} methods include policy iteration, where an arbitrary policy $\pi$ is iteratively evaluated (\emph{evaluation phase}) and improved (\emph{optimization phase}) until convergence.
%\emph{Monte Carlo methods} are used in the policy evaluation phase for computing
%, given a policy $\pi$, for each state-action pair ${(s,a)}$,
%the action-value function $q^{\pi}(s,a)$.
%Action-value functions are estimated by averaging actual returns for each state-action pair using Monte Carlo methods.
%Episodes are generated via simulation, with actions alternating between exploiting the policy $\pi$ and exploring the action space.
%After each episode, the $q$ value is updated for every visited state-action pair $(s,a)$ by estimating its value as the mean of the sampled returns that originated from $(s,a)$ over time.
%With enough time, an accurate estimate of the action-value function $q^{\pi}$ can be obtained.
%Policy improvement involves computing a greedy policy based on $q$, where given a state $s$, the new policy returns an action that maximizes $q^{\pi}(s,\cdot)$.


Policy iteration is an on-policy method that iteratively evaluates (\emph{evaluation phase}) and improves (\emph{optimization phase}) an arbitrary policy $\pi$ until convergence.
In the evaluation phase, \emph{Monte Carlo methods} estimate the action-value function $q^{\pi}(s,a)$ using simulations
of episodes generated by alternating between exploiting the policy and exploring the action space.
%One issue that may arise is that many state-action pairs may never be visited during the optimization phase. If policy $/pi$ is deterministic, following it will only let one observe a single possible action at each state. This means that during the policy %improvement step only the same action will be taken for each state and there will be no real improvement of policy $/pi$.
%To guarantee the exploration of all state-action pairs, the \emph{exploring starts} assumption is used. This assumption guarantees the full exploration of the state-action space, as every pair is given a non-zero probability of being selected. \btext{When} %As 
%the \emph{exploring starts} assumption is unfeasible, as it might happen that we have non-zero probabilities for some state-action pairs, two different types of methods can be used, %to avoid it, namely \emph{on-policy} and \emph{off-policy} methods.
After each episode, the $q^\pi$ value is updated for every visited state-action pair $(s,a)$ by estimating its value as the mean of the sampled returns that originated from $(s,a)$ over time.
With enough time, an accurate estimate of $q^{\pi}$ can be obtained.
Policy improvement computes a greedy policy based on $q^\pi$, where given a state $s$, the new policy returns an action that maximizes $q^{\pi}(s,\cdot)$.


%In the \emph{off-policy} setting, the agent interacts with the environment and stores within a buffer the generated experiences.
%%in the form of $(s_t,a_t,s_{t+1},r_t)$ tuples.
%The actions are chosen from the current policy~$\pi_{i}$
%For the policy improvement step, experiences are samples from the buffer to learn a new policy $\pi_{i+1}$~\cite{Sutton1998}.
%Q-Learning is an off-policy method that evaluates and improves a separate greedy policy while following the behavioural policy.
%Q-learning is a model-free value iteration algorithm that iteratively updates the action-value function %through temporal difference (TD) learning~\cite{Sutton1998}.
%A Q-learning agent approximates the optimal action-value function $q^*(s,a)$ by learning from which action $a_t$ to take from $s_t$ to $s_{t+1}$
%and observe the returned reward~$r_t$.
%The $Q$ function is updated through the Bellman equation:
%\begin{equation}
%\label{eq:qupdate}
%Q(s,a) \leftarrow Q(s,a) + \alpha[\gamma \max_{a'} Q(s',a') - Q(s,a)]
%\end{equation}
%which is the update of the action-value function from state $s$, choosing action $a$ independently from the policy $\pi$ being followed. $\alpha$ is the learning rate parameter. Thus, the learned
%%action-value function
%$Q$ directly approximates $q^*$, no matter the behavioural policy being followed. As opposed to on-policy methods,
%Q-Learning does not wait until the end of the episode to get the returns and update $q^{\pi}$,
%but can do so in an incremental fashion.
%This is especially helpful for applications with very long episodes
%where we might not reach the end of the episode and observe the returns.

In an \emph{off-policy} setting, the agent collects experiences by interacting with the environment and stores them in a buffer.
The actions are chosen from the behavioural policy %current policy $\pi_{i}$,
and for the policy improvement step, experiences are sampled from the buffer to learn the target policy.%a new policy $\pi_{i+1}$. %\cite{Sutton1998}.

\emph{Q-Learning} is an off-policy method that evaluates and improves a separate greedy policy while following the behavioural policy.
It is a model-free value iteration algorithm that defines an action-value function $Q$
by the following update formula:
\begin{equation}
\label{eq:qupdate}
% \resizebox{0.7\hsize}{!}{$
Q(s,a) \leftarrow Q(s,a) + \alpha\left[\gamma \max_{a'} Q(s',a') - Q(s,a)\right]
% $}
\end{equation}
where $\alpha$ is the learning rate parameter.
The action-value function $Q$, defined by~\eqref{eq:qupdate}, directly approximates the optimal action-value function $q^*$,
independently of the policy being followed in the updates.
%\footnote{Under the assumption of coverage}
As opposed to on-policy methods, Q-Learning does not wait until the end of the episode to get the returns and update $q^{\pi}$, but can do so in an incremental fashion at each timestep.
%This is especially helpful for applications with long episodes where we might not reach the end of the episode and observe the returns.

