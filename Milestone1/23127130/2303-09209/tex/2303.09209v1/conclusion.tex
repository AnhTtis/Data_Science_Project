%!TEX root = main.tex


\section{Conclusion}
\label{sec:conclusion}

In this paper we provide an AI based approach that learns, by means of Reinforcement Learning, an optimal policy only from the observation of temporally annotated sequences of event data and recommends the best activities to carry on for optimizing a Key Performance Indicator of interest in a multi-actor setting. This is achieved first by learning an explicit Markov Decision Process (MDP) for the specific KPIs from the event log using clusterization techniques, and then by using RL training to learn the optimal policy. 

The evaluation shows that the usage of scaling for the state-action value functions $q$ is useful to solve the problem of the over-fitting on the training data from which the MDP is constructed (\textbf{RQ1}); it also shows that our approach compares with, and often overcomes, off-policy deep RL approaches especially when taking into account the log dimension and the popularity of successful traces in the event log (\textbf{RQ2} and \textbf{RQ3}).
This can open the doors to the exploitation of white box RL techniques in datasets where only temporal execution data are available, which can be applied to temporally annotated data outside the Process Mining area.