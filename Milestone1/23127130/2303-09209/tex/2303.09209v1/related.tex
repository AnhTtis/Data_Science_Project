%!TEX root = main.tex

\section{Related work}
\label{sec:related}

The state-of-the-art works related to this paper pertain to two fields: Prescriptive Process Monitoring and Reinforcement Learning. The section is hence structured by first presenting the PPM related work and then the RL state-of-the-art works applied to process mining problems.   

Several PPM techniques have been recently proposed in the literature~\cite{DBLP:journals/corr/abs-2112-01769}. %Kubrak et al. in~\cite{DBLP:journals/corr/abs-2112-01769} propose a framework for categorizing these methods based on their performance objectives, type of intervention and technique used. 
Focusing on the type of recommended interventions, we can classify existing work in three groups: (i) the ones that recommend interventions to prevent or mitigate an undesired outcome~\cite{Teinemaaetal2018,fahrenkrog2019fire,metzger2019proactive,metzger2020triggering,DBLP:journals/corr/abs-2109-02894}; (ii) the ones that recommend a resource allocation~\cite{10.1007/978-3-319-39696-5_35,DBLP:conf/icpm/ParkS19}; (iii) the ones that  recommend next activities to optimize a given KPI~\cite{weinzierl2020predictive,groger2014prescriptive,de2020design}.

%Concerning the first family of works, Teinemaa et al. in~\cite{Teinemaaetal2018,fahrenkrog2019fire} propose a method in which a cost-model is used to control the creation of alerts in order to reduce the projected cost for a particular event log and a set of parameters. Metzger et al. in~\cite{metzger2019proactive} discuss the trade off between the earliness and accuracy of the predictions for proactive process adaptation. In~\cite{metzger2020triggering}, Metzger et al. use online reinforcement learning to learn when to initiate proactive process adjustments based on forecasts and their run-time dependability. Shoush et al. in~\cite{DBLP:journals/corr/abs-2109-02894} tackle the problem of recommending interventions for avoiding an undesired outcome when a limited amount of resources is available, similarly to the resource limit imposed in the Simulator, described in Section~\ref{sec:problem}.

%A number of works also focus on recommending the best resource allocation~\cite{10.1007/978-3-319-39696-5_35,DBLP:conf/icpm/ParkS19}. In~\cite{10.1007/978-3-319-39696-5_35}, Sindhgatta et al. support decisions on resource allocation for an ongoing execution by leveraging information about the history of past process instances and related contexts. Park et al. in ~\cite{DBLP:conf/icpm/ParkS19} leverage predictions in order to optimize resource allocation when limited information required for scheduling is available.

The approach presented in this paper falls under this small third family of prescriptive process monitoring approaches. 
%the third family of prescriptive process monitoring approaches. 
The work in~\cite{weinzierl2020predictive} discusses how the most likely behavior does not guarantee to achieve the desired business goal. As a solution to this problem, they propose and evaluate a prescriptive business process monitoring technique that recommends next best actions to optimize a specific KPI, i.e., the time. The work in~\cite{groger2014prescriptive} presents a data-mining driven concept of recommendation-based business process optimization supporting adaptive and continuously optimized business processes. %This mechanism exploits prescriptive analytics and proactively generates action recommendations during process execution in order to avoid the deviation of a  predicted metric deviation.
The work in~\cite{de2020design} discusses Process-aware Recommender (PAR) systems, in which a prescriptive-analytics component, in case of executions with a negative outcome prediction, recommends the next activities that minimize the risk to complete the process execution with a negative outcome.
Similarly to this approach, the work in~\cite{us@BPM22} takes the perspective of one of the actors of the process and  aim at optimizing a domain-specific KPI of interest for this actor by leveraging an RL approach. However, in that work, the authors heavily rely on a manual pre-processing of the event log in order to extract the background knowledge needed to manually build the MDP.
Differently from the works above, we aim at taing a multi-actor perspective by building the RL pipeline automatically from the event log using only the KPI of interest and the activity ownership.  

%minimizing the cost of the execution from the perspective of this actor.

In the literature, only few RL approaches have been proposed for facing problems in Process Mining. Silvander proposes using Q-Learning with function approximation via a deep Q network (DQN) for the optimization of business processes~\cite{silvander2019business}. He suggests defining a so called decay rate to reduce the amount of exploration over time.
The work in~\cite{10.1007/978-3-030-72693-5_10} instead apply a DQN for Predictive Process Monitoring tasks, by learning to predict next activity and execution times through the means of RL rather than classical supervised learning solutions.
Huang et al. employ RL for the dynamic optimization of resource allocation in business process executions~\cite{huang2011reinforcement}. 
%However, they do not consider the proactive adaptation of processes with respect to resources at run time. Also, they use Q-Learning as a classical RL algorithm, and thus assume the environment can be represented by a finite, discrete set of states.
Metzger et al. propose an alarm-based approach to prevent and mitigate an undesired outcome~\cite{metzger2020triggering}. They use online RL to learn when to trigger proactive process adaptations based on the reliability of predictions. %Their solution can accept a continuous states's space.
Although all these works use RL in the process mining field, none of them use it for recommending the next actions to perform in order to optimize a certain KPI of interest, as in this work.
%\todocdf{@Masssimiliano: mi fai un check sui paper di De Giacomo?}
Finally, some works have applied RL and Inverse RL approaches to recommend the next actions on temporal data~\cite{DBLP:conf/recsys/Massimo018} or on data constrained by temporal constraints~\cite{DBLP:conf/aips/GiacomoIFP19}.%,DBLP:conf/aips/GiacomoFIP20}.}