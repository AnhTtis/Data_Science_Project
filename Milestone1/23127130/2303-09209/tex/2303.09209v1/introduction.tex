%!TEX root = main.tex

\section{Introduction}
\label{sec:intro}

Prescriptive Process Monitoring (PPM) is a prominent problem in Process Mining, which consists in identifying a set of actions or interventions to be recommended with the goal of optimising a target measure of interest or Key Performance Indicator (KPI). In its simplest formulations it contains methods for raising alarms or triggering interventions, to
prevent or mitigate undesired outcomes, as well as for recommending the best resource allocation.
Only recently, works have targeted the generation of recommendations of the next activity(ies) to optimize a certain KPI of interest~\cite{weinzierl2020predictive,groger2014prescriptive,de2020design}, such as, the cycle time of the process execution in a single-actor setting (or ignoring the actor dimension). Only one recent work~\cite{us@BPM22} explicitly considers the process execution in the context of a multi-actor complex environment that depends upon exogenous factors, including how the other process actors behave. In this setting, identifying the best strategy to follow for a target actor is not straightforward and Artificial Intelligence (AI) techniques based on Reinforcement Learning (RL) have shown promising results. 

Indeed, Reinforcement Learning is increasingly used as one of the state-of-the art solutions in several areas where strategic reasoning is needed in a multi-actor setting: from gaming~\cite{Go216} to dialogue generation~\cite{li-etal-2016-deep}, from smart city policy making~\cite{DBLP:journals/access/KhaidemLYALD20}, to planning~\cite{DBLP:conf/aaai/Micheli021}, just to name a few. Two RL approaches are used in the literature: 
%The two approaches Reinforcement Learning is used in literature: 
one relies entirely on data and exploits techniques of Deep RL; the other exploits explicit Markov Decision Processes (MDP), but assumes the existence of a model (the MDP) for ``playing the game''. 
%the ``game''. 
The first approach has the advantage of avoiding the construction of, often complex, models, but has the drawback of producing black box decisions; while the second is more transparent but requires a model (the MDP) of the scenario at hand for the KPIs of interest.
Unfortunately, the explicit knowledge required to build an MDP is often not available in the Process Mining domain.
Moreover, it is a knowledge difficult to obtain by manually inspecting the process executions stored in the, so-called, \emph{event logs} (as done in~\cite{us@BPM22}), or by using Process Discovery techniques~\cite{DBLP:books/sp/22/Aalst22a}, which are not tailored to discover meaningful MDP states for the considered KPIs. %\todo{Provate a vedere se questa frase va bene.} 
Thus, an important challenge to address in PPM is the ability to provide methods able to recommend next activity(ies) in multi-actor processes only exploiting the information contained in the temporally annotated (process) execution data, ideally learning an MDP in a white box manner, thus avoiding black box Deep RL approaches.  

In this paper we aim at tackling this challenge by proposing an AI based approach that learns, by means of Reinforcement Learning, an optimal policy only from the observation of temporally annotated sequences of event data i.e., the 
%, that constitute 
past process executions, and recommends the best activities to carry on for optimizing a KPI of interest in a multi-actor setting.
This is achieved first by learning an explicit MDP for the specific KPIs from the event log (Sec.~\ref{sec:constructionMDP}), and then by using RL training to learn the optimal policy (Sec.~\ref{sec:RLtraining}).
%
One of the challenges we have to tackle is the construction of a MDP which must contain a fulfilling description of the system. 
This can be technically difficult because a \emph{state space} which describes all possible configurations of the system can become unbearably big,
so some sort of effective technique to avoid that must be put in place. 
We address this challenge by using clusterization to group together similar process executions, which --- intuitively --- represent similar configurations of the system which can be grouped into a single state, thus greatly reducing the state space dimension without affecting the quality of the result.
To show the validity of the approach we evaluate it on real and synthetic datasets,
%(i) by exploiting different state-action value functions $q$ to avoid problems related to overfitting
(i) by adapting the estimation of the \emph{state-action value function} $q$ --- via scaling --- to avoid problems related to overfitting, and
(ii) by comparing it with 
%state of the art approaches able to apply Reinforcement Learning techniques directly on data, that is 
off-policy deep Reinforcement Learning approaches also taking into account the log dimension and the popularity of successful traces in the event log (Sec.~\ref{sec:evaluation} and~\ref{sec:result}).


The ability of our approach to compare with, and often overcome, Deep RL approaches shows 
how our technique contributes in a significant manner to learn explicit Markov Decision Processes directly from temporal execution data in an effective manner. Therefore, it opens the doors to the exploitation of white box RL techniques in scenarios where only temporally annotated data are available, which has the potential to be applied outside the Process Mining area. 

% \todo[inline]{Questo paragtafo lo possiamo togliere forse, e' piu' per BPM che per KR}Moreover, it constitutes a ``natural'' exploitation of event logs to provide process services firmly grounded on data, which are seen in the Process Mining community as the footprint of reality. Event logs have been used in the past to discover \emph{descriptive process models}, trough families of techniques of process discovery~\cite{DBLP:books/sp/22/Aalst22a,DBLP:books/sp/22/Augusto0022}, and more recently to build \emph{predictive}~\cite{DBLP:books/sp/22/FrancescomarinoG22} \btext{and \emph{prescriptive}} \emph{process models} \btext{not addressing the multi-actor scenario}~\cite{DBLP:journals/corr/abs-2112-01769}, mainly based on tailored Machine Learning techniques. The next natural step is therefore the one of building \emph{prescriptive process models} \btext{\emph{able to deal with multi-actor processes}} only based on the temporally annotated sequences of event data that constitute an event log.


    
