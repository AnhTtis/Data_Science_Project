% JOURNALS

% signal processing in general + speech and audio processing
@string{CSL = "Computer Speech Lang."}
@string{JAES = "J. Audio Eng. Soc."}
@string{JASA = "J. Acoust. Soc. Am."}
@string{JASMP = "EURASIP J. Audio, Speech, Music Process."}
@string{JASP = "EURASIP J. Advances Signal Process."}
@string{JSTSP = "IEEE J. Sel. Topics Signal Process."}
@string{SPL = "IEEE Signal Process. Lett."}
@string{TAP = "IEEE Trans. Antennas Propag."}
@string{TASLP = "IEEE Trans. Audio, Speech, Lang. Process."}
@string{TASLP_new = "IEEE/ACM Trans. Audio, Speech, Lang. Process."}
@string{TASSP = "IEEE Trans. Acoust., Speech, Signal Process."}
@string{TIT = "IEEE Trans. Inform. Theory"}
@string{TSAP = "IEEE Trans. Speech Audio Process."}
@string{TSP = "IEEE Trans. Signal Process."}

% robotics and systems
@string{TSMC = "IEEE Trans. Systems, Man, Cybern."}

% Machine learning
@string{FTML = "Found. Trends Mach. Learn."}
@string{JMLR = "J. Mach. Learn. Res."}
@string{NECO = "Neural Comp."}
@string{TPAMI = "IEEE Trans. Pattern Anal. Mach. Intell."}
@string{IJCV = "Int. J. Comput. Vis."}
@string{AI = "Artif. Intell."}
@string{JIVP = "EURASIP J. Image Video Process."}
@string{ML = "Mach. Learn."}

% Others
@string{JRSS = "J. Royal Stat. Soc.: Series B (Method.)"}
@string{LNCS = "Lect. Notes Comp. Sc."}
@string{JRS = "J. R. Stat. Soc"}
@string{PT = "Phys. Today"}
@string{NRLQ = "Nav. Res. Logist. Q."}


% CONFERENCES

% signal processing in general + speech and audio processing
@string{AESC = "Proc. Audio Eng. Soc. (AES) Conf."}
@string{AESCv = "Proc. Audio Eng. Soc. (AES) Conv."}
@string{AVSP = "Proc. Int. Conf. Audio-Visual Speech Process. (AVSP)"}
@string{DAFx = "Proc. Int. Conf. Digital Audio Effects (DAFx)"}
@string{DSP = "Proc. IEEE Int. Conf. Digital Signal Process. (DSP)"}
@string{EUROSPEECH = "Proc. Europ. Conf. Speech Comm. Tech. (EUROSPEECH)"}
@string{EUSIPCO = "Proc. Europ. Signal Process. Conf. (EUSIPCO)"}
@string{ICA = "Proc. Int. Congr. Acoust. (ICA)"}
@string{ICASSP = "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)"}
@string{ICIP = "Proc. IEEE Int. Conf. Image Process. (ICIP)"}
@string{ICSLP = "Proc. Int. Conf. Spoken Lang. Process. (ICSLP)"}
@string{INTERSPEECH = "Proc. Interspeech Conf."}
@string{ISMIR = "Proc. Int. Soc. for Music Inform. Retrieval Conf. (ISMIR)"}
@string{ISSPA = "Proc. IEEE Int. Symp. Signal Process. Applic. (ISSPA)"}
@string{IWAENC = "Proc. IEEE Int. Workshop Acoustic Signal Enhanc. (IWAENC)"}
@string{LVA = "Proc. Int. Conf. Latent Variable Anal. Signal Separation (LVA/ICA)"}
@string{MMSP = "Proc. IEEE Workshop Multimedia Signal Process. (MMSP)"}
@string{WASPAA = "Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA)"}

% robotics and systems
@string{HUMA  = "Proc. IEEE/RAS Int. Conf. Humanoid Robots (Humanoids)"}
@string{ICRA = "Proc. IEEE Int. Conf. Robotics Autom. (ICRA)"}
@string{IROS = "Proc. IEEE/RSJ Int. Conf. Intell. Robots Systems (IROS)"}

% Machine learning and computer vision
@string{CVPR = "Proc. IEEE Int. Conf. Computer Vision Pattern Recogn. (CVPR)"}
@string{CVPRW = "Proc. IEEE Int. Conf. Computer Vision Pattern Recogn. Workshops (CVPRW)"}
@string{ECCV = "Proc. Europ. Conf. Computer Vision (ECCV)"}
@string{ICCV = "Proc. IEEE Int. Conf. Computer Vision (ICCV)"}
@string{ICCV_new = "Proc. IEEE/CVF Int. Conf. Computer Vision (ICCV)"}
@string{ICLR = "Proc. Int. Conf. Learn. Repres. (ICLR)"}
@string{ICML = "Proc. Int. Conf. Mach. Learn. (ICML)"}
@string{IJCAI = "Proc. AAAI Int. Joint Conf. Artif. Intell. (IJCAI)"}
@string{IJCNN = "Proc. IEEE Int. Joint Conf. Neural Networks (IJCNN)"}
@string{MLSP = "Proc. IEEE Int. Workshop Mach. Learn. Signal Process. (MLSP)"}
@string{NIPS = "Adv. in Neural Inform. Process. Syst. (NeurIPS)"}
@string{AAAI = "Proc. AAAI Conf. Artif. Intell."}
@string{WACV = "IEEE Winter Conf. Appl. Comput. Vis. (WACV)"}
@string{SIGKDD = "Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min."}



@article{MAL-089,
url = {http://dx.doi.org/10.1561/2200000089},
year = {2021},
volume = {15},
journal = FTML,
title = {Dynamical Variational Autoencoders: A Comprehensive Review},
doi = {10.1561/2200000089},
issn = {1935-8237},
number = {1-2},
pages = {1-175},
author = {L. Girin and S. Leglaive and X. Bie and J. Diard and T. Hueber and X. Alameda-Pineda}
}

@inproceedings{Kingma2014,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  added-at = {2020-10-15T14:36:56.000+0200},
  author = {Kingma, D. P. and Welling, M.},
  booktitle = ICLR,
  title = {Auto-encoding variational {B}ayes},
  year = 2014,
    address={Banff, Canada}
}

@InProceedings{pmlr-v32-rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Rezende, D. J. and Mohamed, S. and Wierstra, D.},
  booktitle = ICML,
  year = 	 {2014},
 address={Beijing, China},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}
% pages = {1278--1286},

@article{https://doi.org/10.48550/arxiv.1511.05121,
  doi = {10.48550/ARXIV.1511.05121},
  
  url = {https://arxiv.org/abs/1511.05121},
  
  author = {Krishnan, R. G. and Shalit, U. and Sontag, D.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title={Deep {K}alman filters},
  journal={arXiv preprint arXiv:1511.05121},
  year={2015}
}


@inproceedings{NIPS2015_b618c321,
  title={A recurrent latent variable model for sequential data},
  author={Chung, J. and Kastner, K. and Dinh, L. and Goel, K. and Courville, A. and Bengio, Y.},
  booktitle=NIPS,
  year={2015},
  address={Montreal, Canada}
}

@inproceedings{10.5555/3157096.3157343,
  title={Sequential neural models with stochastic layers},
  author={Fraccaro, M. and S{\o}nderby, S. and Paquet, U. and Winther, O.},
  booktitle=NIPS,
  year={2016},
  address={Barcelona, Spain}
}


@article{Krishnan_Shalit_Sontag_2017, title={Structured Inference Networks for Nonlinear State Space Models}, volume={31}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10779}, DOI={10.1609/aaai.v31i1.10779}, abstractNote={ &lt;p&gt; Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood. &lt;/p&gt; }, number={1}, journal=AAAI, author={Krishnan, Rahul and Shalit, Uri and Sontag, David}, year={2017}, month={Feb.} }

@inproceedings{10.5555/3294996.3295118,
author = {Fraccaro, M. and Kamronn, S. and Paquet, U. and Winther, O.},
title = {A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning},
year = {2017},
abstract = {This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.},
booktitle = NIPS,
address = {Long Beach, CA}
}

@inproceedings{10.5555/3294771.3294950,
author = {Hsu, W.-N. and Zhang, Y. and Glass, J.},
title = {Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data},
year = {2017},
address = {Long Beach, CA},
abstract = {We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.},
booktitle = NIPS
}

@inproceedings{Bie2021ABO,
  title={A Benchmark of Dynamical Variational Autoencoders applied to Speech Spectrogram Modeling},
  author={X. Bie and L. Girin and S. Leglaive and T. Hueber and X. Alameda-Pineda},
  booktitle=INTERSPEECH,
  address = {Brno, Czech Republic},
  year={2021}
}

@ARTICLE{9894060,
  author={Bie, X. and Leglaive, S. and Alameda-Pineda, X. and Girin, L.},
  journal=TASLP_new, 
  title={Unsupervised Speech Enhancement Using Dynamical Variational Autoencoders}, 
  year={2022},
  volume={30},
  number={},
  pages={2993-3007},
  doi={10.1109/TASLP.2022.3207349}}
  
@INPROCEEDINGS{9053164,
  author={Leglaive, S. and Alameda-Pineda, X. and Girin, L. and Horaud, R.},
  booktitle=ICASSP, 
  title={A Recurrent Variational Autoencoder for Speech Enhancement}, 
  year={2020},
  address = {Barcelona, Spain (virtual conference)}
}

@ARTICLE{6795228,
  author={Williams, R. J. and Zipser, D.},
  journal={Neural Comp.}, 
  title={A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}, 
  year={1989},
  volume={1},
  number={2},
  pages={270-280},
  doi={10.1162/neco.1989.1.2.270}}

@inproceedings{10.5555/2969239.2969370,
author = {Bengio, S. and Vinyals, O. and Jaitly, N. and Shazeer, N.},
title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
year = {2015},
abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
booktitle = NIPS,
address = {Montreal, Canada}
}

@inproceedings{10.5555/3295222.3295349,
author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. and Kaiser, L. and Polosukhin, I.},
title = {Attention is All You Need},
year = {2017},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = NIPS,
address = {Long Beach, CA}
}

@inproceedings{Devlin2019BERTPO,
  title={{BERT}: Pre-training of Deep Bidirectional {T}ransformers for Language Understanding},
  author={J. Devlin and M.-W. Chang and K. Lee and K. Toutanova},
  booktitle={Proc. NAACL-HLT Conf.},
  address={Minneapolis, MI},
  year={2019}
}

@article{radford2018improving,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

@inbook{10.5555/3454287.3454804,
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
booktitle = NIPS,
articleno = {517},
numpages = {11}
}

@article{https://doi.org/10.48550/arxiv.2010.11929,
  author = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and colleagues},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  journal = {arXiv preprint arXiv:2010.11929},
  year = {2020}
  }
  %author = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Dehghani, M. and Minderer, M. and Heigold, G. and Gelly, S. and Uszkoreit, J. and Houlsby, N.},


@article{https://doi.org/10.48550/arxiv.2204.01565,
  doi = {10.48550/ARXIV.2204.01565},
  
  url = {https://arxiv.org/abs/2204.01565},
  
  author = {Bie, X. and Guo, W. and Leglaive, S. and Girin, L. and Moreno-Noguer, F. and Alameda-Pineda, X.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{HiT-DVAE}: Human Motion Generation via {H}ierarchical {T}ransformer {D}ynamical {VAE}},
  
  journal = {arXiv preprint arxiv:2204.01565},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{10.5555/1162264,
    author = {Bishop, C. M.},
    title = {Pattern Recognition and Machine Learning},
    year = {2006},
    isbn = {0387310738},
    publisher = {Springer-Verlag},
    address = {Berlin}
}

@ARTICLE{1164453,  author={Ephraim, Y. and Malah, D.},  
journal= {IEEE Trans. Acoust., Speech, Signal Process.},   
title={Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator},   year={1984},  volume={32},  number={6},  pages={1109-1121},  doi={10.1109/TASSP.1984.1164453}}

@ARTICLE{6797100,  author={Févotte, C. and Bertin, N. and Durrieu, J.-L.},  journal={Neural Comp.},   title={Nonnegative Matrix Factorization with the {I}takura-{S}aito Divergence: With Application to Music Analysis},   year={2009},  volume={21},  number={3},  pages={793-830},  doi={10.1162/neco.2008.04-08-771}}

@ARTICLE{5720325,  author={Liutkus, A. and Badeau, R. and Richard, G.},  journal=TSP,   title={Gaussian Processes for Underdetermined Source Separation},   year={2011},  volume={59},  number={7},  pages={3155-3167},  doi={10.1109/TSP.2011.2119315}}

@article{WSJ0,
author = {Garofolo, J. S. and Graff, D. and Paul, D. and Pallett, D.},
title = {{CSR-I (WSJ0) Sennheiser LDC93S6B}},
journal = {Philadelphia: Linguistic Data Consortium},
year = {1993},
doi = {10.35111/ap42-7n83},
url = {https://catalog.ldc.upenn.edu/LDC93S6B}
}

@inproceedings{valentini2016speech,
  title={Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Networks},
  author={Valentini-Botinhao, C. and Wang, X. and Takaki, S. and Yamagishi, J.},
  booktitle=INTERSPEECH,
  year={2016},
  address={San Francisco, CA}
}



@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle= CVPR, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}

@misc{https://doi.org/10.48550/arxiv.1607.06450,
  doi = {10.48550/ARXIV.1607.06450},
  
  url = {https://arxiv.org/abs/1607.06450},
  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Layer Normalization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={I. Loshchilov and F. Hutter},
booktitle=ICLR,
year={2019},
address={New Orleans, LA},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author={D. P. Kingma and J. Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  address={San Diego, CA},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle=ICLR,
}

@inproceedings{
loshchilov2017sgdr,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={I. Loshchilov and F. Hutter},
booktitle=ICLR,
year={2017},
address={Toulon, France},
url={https://openreview.net/forum?id=Skq89Scxx}
}

@INPROCEEDINGS{8683855,
  author={Roux, J. Le and Wisdom, S. and Erdogan, H. and Hershey, J. R.},
  booktitle=ICASSP, 
  title={{SDR} – {H}alf-baked or well done?}, 
  year={2019},
  address={Brighton, UK},
  doi={10.1109/ICASSP.2019.8683855}}
  
@INPROCEEDINGS{941023,
  author={Rix, A. and Beerends, J. and Hollier, M. and Hekstra, A.},
  booktitle=ICASSP, 
  title={Perceptual evaluation of speech quality ({PESQ}) - {A} new method for speech quality assessment of telephone networks and codecs}, 
  year={2001},
  address={Salt Lake City, UT},
  doi={10.1109/ICASSP.2001.941023}}
  
@ARTICLE{5713237,
  author={Taal, C. and Hendriks, R. and Heusdens, R. and Jensen, J.},
  journal=TASLP, 
  title={An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech}, 
  year={2011},
  volume={19},
  number={7},
  pages={2125-2136},
  doi={10.1109/TASL.2011.2114881}}

@inproceedings{
Binkowski2020High,
title={High Fidelity Speech Synthesis with Adversarial Networks},
author={M. Bińkowski and J. Donahue and S. Dieleman and A. Clark and E. Elsen and N. Casagrande and colleagues},
booktitle=ICLR,
address={virtual conference},
year={2020},
url={https://openreview.net/forum?id=r1gfQgSFDr}
}
%author={M. Bińkowski and J. Donahue and S. Dieleman and A. Clark and E. Elsen and N. Casagrande and L. Cobo and K. Simonyan},



@inproceedings{10.5555/3045390.3045410,
author = {Amodei, D. and Ananthanarayanan, S. and Anubhai, R. and Bai, J. and Battenberg, E. and colleagues},
title = {Deep {S}peech 2: End-to-End Speech Recognition in {E}nglish and {M}andarin},
year = {2016},
abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
booktitle = ICML,
address = {New York, NY}
}
% author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi Vaino and Jiang, Bing and Ju, Cai and Jun, Billy and LeGresley, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},




@ARTICLE{1164317,  author={Griffin, D. and Lim, J.},  journal={IEEE Trans. Acoust., Speech, Signal Process.},   title={Signal estimation from modified short-time {F}ourier transform},   year={1984},  volume={32},  number={2},  pages={236-243},  doi={10.1109/TASSP.1984.1164317}}

@INPROCEEDINGS{6701851,  author={Perraudin, N. and Balazs, P. and Søndergaard, P.},  booktitle=WASPAA,   
title={A fast {G}riffin-{L}im algorithm},   
year={2013},  
address={New Paltz, NY},   doi={10.1109/WASPAA.2013.6701851}
}


@inproceedings{fu2021metricganU,
  title={{MetricGAN-U}: Unsupervised speech enhancement-dereverberation based only on noisy/reverberated speech},
  author={Fu, S.-W. and Yu, C. and Hung, K.-H. and Ravanelli, M. and Tsao, Y.},
  address={Singapore},
  booktitle=ICASSP,
  year={2022}
}

@INPROCEEDINGS{8461530,
  author={Bando, Yoshiaki and Mimura, Masato and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
  booktitle=ICASSP, 
  title={Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization}, 
  year={2018},
  volume={},
  number={},
  pages={716-720},
  doi={10.1109/ICASSP.2018.8461530}}

@inproceedings{inproceedings,
author = {Pariente, Manuel and Deleforge, Antoine and Vincent, Emmanuel},
year = {2019},
month = {09},
pages = {3158-3162},
title = {A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders},
booktitle=INTERSPEECH,
address = {Graz, Austria}
}