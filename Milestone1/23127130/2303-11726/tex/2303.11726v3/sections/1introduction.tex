\IEEEraisesectionheading{\section{Introduction}\label{sec:intro}}

\IEEEPARstart{M}{onocular} 3D human mesh estimation aims to estimate 3D positions of the mesh vertices that are on the body surface from an RGB image. The task has attracted a lot of attention from the computer vision and computer graphics communities \cite{choi2020pose2mesh, boulic1997integration, presti20163d, li2020pastanet,loper2014mosh, kanazawa2018end, ma2021context, zhu2023motionbert} because it can benefit many applications such as virtual reality \cite{huang2017towards}. Recently, deep learning-based methods \cite{kanazawa2018end, choi2020pose2mesh, li2021hybrik} have significantly advanced the accuracy on benchmark datasets.

Pioneer methods \cite{NIPS2017_ab452534, kanazawa2018end} propose to regress the pose and shape parameters of the mesh models such as SMPL \cite{loper2015smpl} directly from images, learning a one-to-one mapping from images to the parameter space. While straightforward, their performance usually suffers from several limitations. Firstly, the mapping from the image to the $SO(3)$ parameter space is highly non-linear and suffers from image-model misalignment \cite{li2021hybrik}. Secondly, existing mesh datasets \cite{h36m_pami,vonMarcard2018,lassner2017unite} are small and limited to simple laboratory environments due to the complex capturing process. The lack of sufficient training data severely limits its performance. Lastly, due to the inherent depth ambiguity in monocular estimation, multiple 3D meshes may correspond to the same 2D projection. As a result, these deterministic methods may produce incorrect estimates in the presence of occlusions.


Recently, some works \cite{kolotouros2019convolutional, moon2020i2l} begin to formulate mesh estimation as a dense 3D keypoint detection task inspired by the success of volumetric pose estimation \cite{sun2018integral,tu2020voxelpose,zhang2022voxeltrack,qiu2019cross,ye2022faster,su2022virtualpose}. For example, in \cite{kolotouros2019convolutional, moon2020i2l}, the authors propose to regress the 3D positions of all vertices. However, it is computationally expensive because it has more than several thousand vertices. Moon \etal \cite{moon2020i2l} improves the efficiency by decomposing the 3D heatmaps into multiple 1D heatmaps at the cost of mediocre accuracy. Choi \etal \cite{choi2020pose2mesh} propose to first detect a sparser set of skeleton joints in the images, from which dense 3D meshes are regressed by exploiting the mesh topology. The methods along this direction have attracted increasing attention \cite{choi2020pose2mesh,li2021hybrik, wan2021encoder} due to two reasons. First, the proxy task of 3D skeleton estimation can leverage the abundant 2D pose datasets which notably improves the accuracy. Second, mesh regression from the skeletons is efficient. However, important information about the body shapes is lost in extracting the 3D skeletons, which is largely overlooked.  As a result, different types of body shapes, whether lean or obese, cannot be accurately estimated, \eg the results of Pose2Mesh \cite{choi2020pose2mesh} in Figure \ref{fig:teaser}.

The professional marker-based motion capture method MoSh \cite{loper2014mosh} places physical markers on the body surface to extract meshes with accurate shapes under non-rigid motions. However, the physical markers limit the approach in laboratory environments. We are inspired to think whether we can identify a set of landmarks on the mesh as virtual markers, \eg, elbow and wrist, that can be detected from wild images, allowing us to recover accurate body shapes. The desired virtual markers should satisfy several requirements. First, the number of markers should be much smaller than that of the mesh vertices so that we can use volumetric representations to estimate their 3D positions efficiently. Second, the markers should capture the mesh topology so that the intact mesh can be accurately regressed from them. Third, the virtual markers have distinguishable visual patterns and can be detected from images. 


In this work, we present a learning algorithm based on archetypal analysis \cite{cutler1994archetypal} to identify a subset of mesh vertices as the virtual markers that try to satisfy the above requirements to the best extent. Figure \ref{fig:body_arche} shows that the learned virtual markers coarsely outline the body shape and pose which paves the way for estimating meshes with accurate shapes. Then we present a simple framework \textbf{\vmname} for 3D mesh estimation on top of the representation as shown in Figure \ref{fig:pipeline} (top). It first detects the 3D positions of the virtual markers \cite{sun2018integral}. Then we recover the intact mesh simply by interpolating them. The interpolation weights are pre-trained in the representation learning step and will be adjusted by a light network based on the prediction confidences of the virtual markers for each image. 

Although the virtual marker representation eases the mesh estimation task, the vulnerability of deterministic methods to depth ambiguity and occlusions underscores the need for probabilistic modeling. For example, in Figure \ref{fig:teaser} (bottom), \vmname\ wrongly estimates the occluded left arm. Expanding upon \vmname, we further develop \textbf{\vmproname}, a probabilistic framework capable of estimating multiple plausible 3D meshes that are consistent with the given 2D observation, as depicted in Figure \ref{fig:pipeline} (bottom). Motivated by recent breakthroughs in generative models, we utilize diffusion models \cite{ho2020denoising} to enhance the diversity and robustness of our mesh predictions. Concretely, we formulate the estimation of 3D virtual markers as a conditional reverse denoising process. Based on the input 2D image and the predicted virtual markers from \vmname, we probabilistically model the 3D marker positions and finally obtain multiple reasonable 3D mesh estimates. For instance, \vmproname\ could estimate a more naturally swinging left arm during walking than \vmname\ as shown in the bottom of Figure \ref{fig:teaser}.

We extensively evaluate our approaches on three benchmark datasets. Both \vmname\ and \vmproname\ consistently outperforms the state-of-the-art methods across the benchmarks. Notably, \vmname\ demonstrates remarkable gains on the SURREAL dataset \cite{varol2017learning}, which has a variety of body shapes. Ablation studies further affirm the merits of virtual marker representation, particularly in estimating accurate shapes. Furthermore, \vmproname\ stands out in its precise modeling of data distributions, markedly enhancing performance in occlusion-heavy scenarios. Finally, experiments show decent generalization ability of our methods which generate visually appealing results for wild images. 

A preliminary version of this work was presented in CVPR 2023 \cite{ma20233d}. Our new contributions are three-fold. First, we propose \vmproname, which incorporates probabilistic modeling to handle the inherent ambiguity in monocular 3D mesh estimation. This yields a more flexible and robust human mesh distribution estimation. Second, \vmproname\ outperforms state-of-the-arts (SOTA) on multiple benchmark datasets in terms of the accuracy of distribution. Finally, \vmproname\ significantly improves the performance in scenarios with occlusions than \vmname\ and other SOTA methods, demonstrating its ability to resolve ambiguity.




