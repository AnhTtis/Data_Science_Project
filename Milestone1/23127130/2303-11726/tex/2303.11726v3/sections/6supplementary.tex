



 
We elaborate on the post-processing implementation of the virtual markers and provide additional experimental details and results. At last, we discuss data from human subjects and the potential societal impact.

\appendices

\section{Post-processing on Virtual Markers}
\label{sec:post}

As described in Section \ref{subsec:body_arche}, considering the left-right symmetric human body structure, we slightly adjust the learned virtual markers $\mathbf{Z}$ to be symmetric. In fact, after the first step that updates each $\mathbf{z}_i$ by its nearest vertex to get $\widetilde{\mathbf{Z}} \in \mathbb{R}^{3 \times K}$. $\widetilde{\mathbf{Z}}$ are almost symmetric with few exceptions. To get the final symmetric virtual markers $\widetilde{\mathbf{Z}}^{sym} \in \mathbb{R}^{3 \times K}$, for each virtual marker located in the left body part, we take its symmetric vertex in the right body to be its symmetric counterpart. 

Since the human mesh (\ie SMPL \cite{loper2015smpl}) itself is not strictly symmetric, we clarify the \emph{symmetric vertex pair} (\eg left elbow and right elbow) on a human mesh template $\mathbf{X}^t \in \mathbb{R}^{3 \times M}$ in Figure \ref{fig:supp_mesh}. We place $\mathbf{X}^{t}$ at the origin of the 3D coordinate system. Formally, we define the cost of matching $i^{th}$ vertex to $j^{th}$ vertex to be $\bm{C}_{i, j} = \left|x_i + x_j\right| + \left|y_i - y_j\right| + \left|z_i - z_j\right|$. A symmetric vertex pair $(\mathbf{X}^t_{i}, \mathbf{X}^t_{j})$ is defined to have the minimal cost $\bm{C}_{i, j}$. In this way, for each virtual marker in the left body, we take its symmetric vertex counterpart to be its symmetric virtual marker and finally get $\widetilde{\mathbf{Z}}^{sym}$.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.8in]{imgs/supp/mesh_template.pdf}
    \caption{Illustration of the human mesh template $\mathbf{X}^{t}$ at the 3D coordinate system and a symmetric vertex pair $(\mathbf{X}^t_{i}, \mathbf{X}^t_{j})$.}
    \label{fig:supp_mesh}
\end{figure}


\section{Experiments}
\label{sec:supp_experiments}

In this section, we first add detailed descriptions for datasets and then provide more experimental results of our approach.

\subsection{Datasets}

\noindent\textbf{H3.6M \cite{h36m_pami}.} Following previous works \cite{kanazawa2018end, kolotouros2019learning, kolotouros2019convolutional, pavlakos2018learning}, we use the SMPL parameters generated from MoSh \cite{loper2014mosh}, which are fitted to the 3D physical marker locations, to get the GT 3D mesh supervision. Following standard practice \cite{kanazawa2018end}, we evaluate the quality of 3D pose of $14$ joints derived from the estimated mesh, \ie $\hat{\mathbf{M}}\mathcal{J}$. We report Mean Per Joint Position Error (MPJPE) and PA-MPJPE in millimeters (mm). The latter uses Procrustes algorithm \cite{gower1975generalized} to align the estimates to GT poses before computing MPJPE. To evaluate mesh estimation results, we also report Mean Per Vertex Error (MPVE) which can be interpreted as MPJPE computed over the whole mesh. \\

\noindent\textbf{3DPW \cite{vonMarcard2018}.} The 3D GT SMPL parameters are obtained by using the data from IMUs when collected. Following the previous works \cite{lin2021end, Lin_2021_ICCV, Kocabas_2021_ICCV, zanfir2021thundr}, we use the train set of 3DPW to learn the model and evaluate on the test set. The same evaluation metrics as H3.6M are used. We further evaluate the performance of our probabilistic framework \vmproname\ on two subsets of 3DPW, \ie 3DPW-OC and 3DPW-PC \cite{vonMarcard2018, Zhang_2020_CVPR} which have both object and person occlusion. In this setting, we do not use the 3DPW training set for a fair comparison following \cite{Li_2023_ICCV}. \\ \\

\noindent\textbf{MPI-INF-3DHP \cite{mehta2017monocular}} is a 3D pose dataset with 3D GT pose annotations. Since this dataset does not provide 3D mesh annotations, following \cite{kanazawa2018end, kolotouros2019learning}, we only enforce supervision on the 3D skeletons (Eq. \ref{eq:loss_reg}) in mesh losses. \\

\noindent\textbf{UP-3D \cite{lassner2017unite}} is a wild 2D pose dataset with natural images. The 3D poses and meshes are obtained by SMPLify \cite{bogo2016keep}. Due to the lack of GT 3D poses, the fitted meshes are not accurate. Therefore we only use the 2D annotations to train the 3D virtual marker estimation network as in \cite{sun2018integral}.   \\

\noindent\textbf{COCO \cite{lin2014microsoft}} is a large wild 2D pose dataset with natural images. Previous work \cite{moon2020i2l} used SMPLify-X \cite{pavlakos2019expressive} to obtain pseudo SMPL mesh annotations but they are not accurate. However, we find that if we project the 3D mesh to 2D image, the resulting 2D mesh vertices align well with the image. So we leverage the 2D annotations to train the virtual marker estimation network as in \cite{sun2018integral}.  \\


\noindent\textbf{SURREAL \cite{varol2017learning}} is a large-scale synthetic dataset containing 6 million frames of synthetic humans. The images are photo-realistic renderings of people under large variations in shape, texture, viewpoint, and body pose. To ensure realism, the synthetic bodies are created using the SMPL body model, whose parameters are fit by the MoSh \cite{loper2014mosh} given raw 3D physical marker data. All the images have a resolution of $320 \times 240$. We use the same training split to train the model and evaluate the test split following \cite{choi2020pose2mesh}.  \\

\subsection{Implementation Details}
Following common practice \cite{kanazawa2018end, choi2020pose2mesh, moon2020i2l, zanfir2021thundr, kolotouros2019convolutional, kocabas2020vibe, Lin_2021_ICCV, lin2021end}, we conduct mix-training by using the above 2D and 3D datasets for experiments on the H3.6M and 3DPW datasets. 
To leverage the 3D pose estimation dataset, \ie MPI-INF-3DHP \cite{mehta2017monocular}, we extend the $64$ virtual markers with the $17$ landmark joints (\ie skeleton) from the MPI-INF-3DHP dataset. 
For experiments on the SURREAL dataset, we use its training set alone as in \cite{choi2020pose2mesh, luan2021pc}. We implement the proposed method with PyTorch. The experiments for \vmname\ and \vmproname\ are conducted on a Linux machine with 4 NVIDIA 16GB V100 GPUs and 2 NVIDIA A100-PCIE-80G GPUs, respectively.

To train \vmname, the whole network is trained for 40 epochs with batch size 32 using Adam \cite{kingma2015adam} optimizer. We set $\lambda_{3d}, \lambda_{c}, \lambda_{m}, \lambda_{e} = 100, 0.1, 0.1, 0.1$ for each loss term. 
To train \vmproname, the whole network is trained for 50 epochs with batch size 80 using Adam \cite{kingma2015adam} optimizer. We set $\lambda_{diffusion} = 1$ and keep the rest loss weights the same as training \vmname. For implementing LCN unit, we modify the original LCN network \cite{ci2019lcn} by injecting a Cross-attention \cite{NIPS2017_transformer} module in each LCN layer. For more implementation details, please check \projpage.



\subsection{Additional Quantitative Results}


\noindent\textbf{Different loss terms.}
Table \ref{tab:ablation_loss} reports the MPVE error on H3.6M \cite{h36m_pami} test set when ablating different loss terms. The confidence loss \cite{iskakov2019learnable} is used to encourage the interpretability of the heatmaps to have a maxima response at the GT position. Without the confidence loss, the error increases slightly. If ablating the surface losses, MPVE increases a lot, which demonstrates the smoothing effect of these two terms.   \\


\noindent\textbf{Comparison to fitting.}
In order to disentangle the ability of mesh regression from markers using $\hat{\mathbf{A}}$ with the ability to detect the virtual markers accurately from images, we first compute the estimation errors of the virtual markers. The MPJPE over all the virtual markers is $35.5$mm, which demonstrates that these virtual markers can be accurately detected from the images. 
We then fit the mesh model parameters to these virtual markers.
Table \ref{tab:fitting} shows the metrics of the fitted mesh on the SURREAL \cite{varol2017learning} test set. As we can see, the fitted mesh has a similar error as our regression ones which uses the interpolation matrix $\hat{\mathbf{A}}$, which validates the accuracy of the estimated virtual markers.   \\











\subsection{Additional Qualitative Results} 
Figure \ref{fig:supp_surreal} shows more qualitative comparisons with Pose2Mesh \cite{choi2020pose2mesh} on the SURREAL test set in which has diverse body shapes. The skeleton representation used in Pose2Mesh \cite{choi2020pose2mesh} loses the body shape information so the method can only recover mean shapes, regardless of whether the person is slim or stout. This is caused by the limited skeleton representation bottleneck so that the model learns a mean shape for the whole training dataset implicitly. In contrast, both of our approaches \vmname\ and \vmproname\ generate more accurate mesh estimation results with the help of virtual marker representation.  

\begin{table}[t]
\center
\caption{MPVE errors on H3.6M \cite{h36m_pami} test set when ablating different loss terms.}
\label{tab:ablation_loss}
\setlength{\tabcolsep}{6pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l | c c c c c}
    \hline
     & \multirow{2}{*}{\textbf{\vmname}} & \textit{w/o}  & \textit{w/o} & \textit{w/o} & \textit{w/o}  \\
     &  &  $\mathcal{L}_{conf}$ &  $\mathcal{L}_{pose}$ &  $\mathcal{L}_{normal}$ &  $\mathcal{L}_{edge}$ \\
    \hline
    MPVE$\downarrow$ & \textbf{58.0} & 59.2 & 58.3 & 60.6 & 60.4 \\
    
    \hline 
\end{tabular}}
\end{table}

\begin{table}
    \centering
    \caption{Results on SURREAL \cite{varol2017learning} test set when the mesh is obtained by fitting to the estimated virtual markers.}
    \label{tab:fitting}
    \setlength{\tabcolsep}{13pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l | c c c}
    \hline
    \multirow{2}{*}{Method} & \multirow{2}{*}{MPVE$\downarrow$} & \multirow{2}{*}{MPJPE$\downarrow$} & \multirow{2}{*}{PA-MPJPE$\downarrow$} \\
    &  &  &  \\
    \hline
    Fitting & 44.6 & 34.8 & 29.5  \\
    \rowcolor{mygray}
    \vmname & 44.7 & 36.9 & 28.9 \\
    
    \hline 
\end{tabular}}
\end{table}



\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{imgs/supp/supp_wild_vm.pdf}
    \caption{Meshes estimated by \vmname\ on Internet images with challenging cases (complex poses or extreme body shapes).}
    \label{fig:supp_wild_vm}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{imgs/supp/supp_wild_vmpro.pdf}
    \caption{Meshes estimated by \vmproname\ on Internet images with challenging cases (complex poses or extreme body shapes). For each case in a row, we present three hypotheses, showing both a projected view and a side view in each column. The rightmost column shows the overlapped side views to unveil the subtle differences.}
    \label{fig:supp_wild_vmpro}
\end{figure*}


Figure \ref{fig:supp_pw3d} shows more qualitative comparisons with Pose2Mesh \cite{choi2020pose2mesh} and METRO \cite{lin2021end} on the 3DPW test set. Pose2Mesh and METRO use the skeleton or all 3D vertices as intermediate representations, respectively. The estimated meshes are overlaid on the images according to the camera parameters. Pose2Mesh \cite{choi2020pose2mesh} has difficulty in estimating correct body pose and shapes when truncation occurs (a) or in complex postures (c). The results of METRO \cite{lin2021end} have many artifacts where the estimated mesh is not smooth, and they also fail to align the image well.  In contrast, our method estimates more accurate human poses and shapes and has smooth human mesh results. In addition, it is more robust to truncation and occlusion and aligns the image better.


Figure \ref{fig:supp_wild_vm} and \ref{fig:supp_wild_vmpro} show more qualitative results of \vmname\ and \vmproname\ on Internet images with challenging cases, respectively. These images include humans with diverse body shapes or complex poses. Our methods generalize well on the natural scenes. When occlusion happens, the probabilistic method \vmproname\ could infer multiple reasonable poses, \eg the different angles of the left arm of the running woman in the first case in Figure \ref{fig:supp_wild_vmpro}.




\begin{figure*}[t]
    \centering
      \includegraphics[width=0.85\linewidth]{imgs/supp/supp_surreal.pdf}
    \caption{Qualitative comparison between our methods and Pose2Mesh \cite{choi2020pose2mesh} on SURREAL test set \cite{varol2017learning}. Our approaches generate more accurate mesh estimation results on images of diverse body shapes. We show 2 hypotheses generated by \vmproname.}
    \label{fig:supp_surreal}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/supp/supp_pw3d.pdf}
    \caption{Qualitative comparison between our method \vmname\ and Pose2Mesh \cite{choi2020pose2mesh}, METRO \cite{lin2021end} on 3DPW test set \cite{vonMarcard2018}. Our approach is more robust to occlusion and truncation and generates more accurate mesh estimation results that align images well.}
    \label{fig:supp_pw3d}
\end{figure*}




\section{Human Subject Data} 

 We use existing public datasets of human subjects in our experiments following their official licensing requirements. With proper usage, the proposed method could be beneficial to society (\eg elderly fall detection). 




