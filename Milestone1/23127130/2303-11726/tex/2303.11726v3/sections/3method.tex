\section{Method}
\label{sec:method}

In this section, we describe the details of our approach. First, Section \ref{subsec:body_arche} introduces how we learn the virtual marker representation from mocap data. Based on the virtual marker, we present the deterministic framework \textbf{\vmname} for mesh estimation from an image and its training details in Section \ref{subsec:approach_det}. Then we extend VMarker to a probabilistic mesh estimation framework, \textbf{\vmproname} in Section \ref{subsec:approach_pro} and introduce its training details as well as the inference strategy.

\subsection{The virtual marker representation} 
\label{subsec:body_arche}

We represent a 3D mesh by a vector of vertex positions $\mathbf{x} \in \mathbb{R}^{3M}$ where $M$ is the number of mesh vertices. Denote a mocap dataset such as \cite{h36m_pami} with $N$ meshes as $\overset{\frown}{\mathbf{X}} = [\mathbf{x}_1 ,\, ... ,\, \mathbf{x}_N] \in \mathbb{R}^{3M \times N}$. To unveil the latent structure among vertices, we reshape it to $\mathbf{X} \in \mathbb{R}^{3N \times M}$ with each column $\mathbf{x}_i \in \mathbb{R}^{3N}$ representing all possible positions of the $i^{\text{th}}$ vertex in the dataset \cite{h36m_pami}.

The mesh $\mathbf{X}$ can be approximated by a low-rank matrix because the mesh representation is smooth and redundant where some vertices can be accurately reconstructed by others. Inspired by this, we aim to learn $K$ virtual markers from $M$ mesh vertices $\mathbf{Z} = [\mathbf{z}_1, ..., \mathbf{z}_K] \in \mathbb{R}^{3N \times K}$, as shown in Figure \ref{fig:body_arche}, that try to satisfy the following two requirements to the greatest extent. First, they can accurately reconstruct the intact mesh $\mathbf{X}$ by their linear combinations: $\mathbf{X}=\mathbf{Z} \mathbf{A}$, where $\mathbf{A} \in \mathbb{R}^{K \times M}$ is a coefficient matrix that encodes the spatial relationship between the virtual markers and the mesh vertices. Second, they should have distinguishable visual patterns in images so that they can be easily detected from images. Ideally, they can be on the body surface as the meshes. While it seems natural to apply singular value decomposition (SVD) \cite{golub1971singular} to $\mathbf{X}$ to compute the eigenvectors as virtual markers for reconstructing others, there is no guarantee that the virtual markers correspond to the mesh vertices, making them difficult to be detected from images. 

To that end, we apply archetypal analysis \cite{cutler1994archetypal,chen2014fast} to learn $\mathbf{Z}$ by minimizing a reconstruction error with two additional constraints: (1) each vertex $\mathbf{x}_i$ can be reconstructed by convex combinations of $\mathbf{Z}$, and (2) each marker $\mathbf{z}_i$ should be convex combinations of the mesh vertices $\mathbf{X}$: 
\begin{equation}
\label{eq:aa}
\min_{\substack{\bm{\alpha}_i \in {\Delta}_K \, for \, 1\leq i\leq M, \\ 
                \bm{\beta}_j \in {\Delta}_M \, for \, 1\leq j\leq K}} ||\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{A}||^2_F, \\
\end{equation}
where $\mathbf{A} = [\bm{\alpha}_1, ..., \bm{\alpha}_M] \in \mathbb{R}^{K \times M}$, each $\bm{\alpha}$ resides in the simplex ${\Delta}_K \triangleq \{\bm{\alpha} \in \mathbb{R}^{K} \, \mathrm{s.t.} \, \bm{\alpha} \succeq 0 \, \text{and} \, {||\bm{\alpha}||}_1 = 1\}$, and $\mathbf{B} = [\bm{\beta}_1, ..., \bm{\beta}_K] \in \mathbb{R}^{M \times K}$, $\bm{\beta}_j \in {\Delta}_M$. We adopt Active-set algorithm \cite{chen2014fast} to solve objective (Eq. \ref{eq:aa}) and obtain the learned virtual markers $\mathbf{Z} = \mathbf{X}\mathbf{B} \in \mathbb{R}^{3N \times K}$.  As shown in \cite{cutler1994archetypal,chen2014fast}, the two constraints encourage the virtual markers $\mathbf{Z}$ to unveil the latent structure among vertices, therefore they learn to be close to the extreme points of the mesh and located on the body surface as much as possible. \\



\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/pipeline.pdf}
	\caption{Overview of our deterministic (top) and probabilistic (bottom) framework. \textbf{Top:} The deterministic framework \textbf{\vmname}. Given an input image, it first estimates the 3D virtual marker (VM) positions $\vmest$. Then we update the coefficient matrix $\Aest$ based on the estimation confidence scores $\conf$ of the VM. Finally, the complete human mesh can be simply recovered by linear multiplication $\meshest = \vmest\Aest$. \textbf{Bottom:} The probabilistic framework \textbf{\vmproname}. Building upon \vmname\, we formulate the 3D VM estimation as a reverse denoising process using diffusion models. We introduce a denoiser that learns to remove the added noise from a noisy 3D VM $\vmgt(t)$ during training, conditioned on the 2D image feature $\feat$ and the estimated 2D VMs $\vmestuv$, \ie the first two dimensions of $\vmest$. At inference, we iteratively removes noise from random noise $\vmest(T) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ according to Eq. \ref{eq:infer_t_1}, yielding a denoised 3D VMs $\vmest(0)$. Finally, we obtain the estimated mesh $\meshest = \vmest(0)\Aest$. \vmproname\ can generate multiple feasible solutions aligned with 2D image cues during inference.}
    \label{fig:pipeline}
\end{figure*}


\noindent\textbf{Post-processing.}
Since human body is left-right symmetric, we adjust $\mathbf{Z}$ to reflect the property. We first replace each $\mathbf{z}_i \in \mathbf{Z}$ by its nearest vertex on the mesh and obtain $\widetilde{\mathbf{Z}} \in \mathbb{R}^{3 \times K}$. This is done on a template mesh which is in a canonical pose. This step allows us to compute the left or right counterpart of each marker. Then we replace the markers in the right body with the symmetric vertices in the left body and obtain the symmetric markers $\widetilde{\mathbf{Z}}^{sym} \in \mathbb{R}^{3 \times K}$. Finally, we update $\mathbf{B}$ and $\mathbf{A}$ by minimizing $||\mathbf{X} - \mathbf{X}\widetilde{\mathbf{B}}^{sym}\widetilde{\mathbf{A}}^{sym}||^2_F$ subject to $\widetilde{\mathbf{Z}}^{sym} = \mathbf{X} \widetilde{\mathbf{B}}^{sym}$ over all $N$ mesh data. 
More details are elaborated in the supplementary. 


Figure \ref{fig:body_arche} shows the virtual markers learned on the mocap dataset \cite{h36m_pami} after post-processing. They are similar to the physical markers and approximately outline the body shape which agrees with our expectations. They are roughly evenly distributed on the surface of the body, and some of them are located close to the body keypoints, which have distinguishable visual patterns to be accurately detected. Table \ref{tab:comparison_sym_arche} shows the reconstruction errors of using original markers $\mathbf{X}\mathbf{B}$ and the symmetric markers $\mathbf{X}\widetilde{\mathbf{B}}^{sym}$. Both can reconstruct meshes accurately. 


\begin{table}[t]
    \centering
    \caption{The reconstruction errors using the original and the symmetric sets of markers on the H3.6M dataset \cite{h36m_pami}, respectively. The errors are small indicating that they are sufficiently expressive and can reconstruct all vertices accurately. }
    \label{tab:comparison_sym_arche}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|l|c} 
    \hline 
    Type & Formula & Reconst. Error (mm) $\downarrow$  \\
    \hline  
    Original & $||\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{A}||^2_F$ & 11.67 \\
    Symmetric & $||\mathbf{X} - \mathbf{X}\widetilde{\mathbf{B}}^{sym}\widetilde{\mathbf{A}}^{sym}||^2_F$ & 10.98 \\
    \hline
    \end{tabular}}

\end{table}

\subsection{Deterministic mesh estimation framework}
\label{subsec:approach_det}
\subsubsection{\vmname}
On top of the virtual markers, we present a simple yet effective framework \vmname\ for end-to-end 3D human mesh estimation from a single image. As shown in Figure \ref{fig:pipeline} (top), it consists of two branches (yellow and orange blocks). The first branch (yellow block) uses a volumetric CNN \cite{sun2018integral} to estimate the 3D virtual marker (VM) positions $\vmest$, and the second branch (orange block) reconstructs the full mesh $\meshest$ by predicting a coefficient matrix $\Aest$:
\begin{equation}
\meshest = \vmest\Aest.
\end{equation}
We will describe the two branches in more detail. \\

\noindent\textbf{3D VM estimation.}
We train a neural network to estimate a 3D heatmap $\hmapest = [\hmapest_1 ,\, ... ,\, \hmapest_{K}] \in \mathbb{R}^{K \times D \times H \times W}$ from an image. The heatmap encodes per-voxel likelihood of each marker. There are $D \times H \times W$ voxels in total which are used to discretize the 3D space. The 3D position $\vmest_z \in \mathbb{R}^{3}$ of each marker is computed as the center of mass of the corresponding heatmap $\hmapest_z$ \cite{sun2018integral} as follows:
\begin{equation}
    \vmest_z = \sum_{d=1}^{D}\sum_{h=1}^{H}\sum_{w=1}^{W}(d, h, w) \cdot \hmapest_z(d, h, w).
\end{equation}
The positions of all markers are represented as $\vmest = [\vmest_1, \vmest_2, \cdots, \vmest_K]$. \\


\noindent\textbf{3D mesh estimation.} 
Ideally, if we have accurate estimates for all virtual markers $\vmest$, then we can recover the complete mesh by simply multiplying $\vmest$ with a fixed coefficient matrix $\widetilde{\mathbf{A}}^{sym}$ with sufficient accuracy as validated in Table \ref{tab:comparison_sym_arche}. However, in practice, some markers may have large estimation errors because they may be occluded in the monocular setting. Note that this happens frequently. For example, the markers in the back will be occluded when a person is facing the camera. As a result, inaccurate markers positions may bring large errors to the final mesh if we directly multiply them with the fixed matrix $\widetilde{\mathbf{A}}^{sym}$. 


Our solution is to rely more on those accurately detected markers. To that end, we propose to update the coefficient matrix based on the estimation confidence scores $\conf$ of the markers. In practice, we simply take the heatmap score at the estimated positions of each marker obtained by interpolation. For convenience, we denote this process as $\conf_z = \hmapest_z(\vmest_z)$. We feed the confidence scores to an updating network which is a single fully-connected (FC) layer to obtain the coefficient matrix $\Aest$. Then the mesh is reconstructed by linear multiplication $\meshest = \vmest\Aest$.




\subsubsection{Training}
\label{subsec:training_det}
We train the whole network end-to-end in a supervised way. 
The overall loss function is defined as:
\begin{equation}
\label{eq:total_loss}
\begin{aligned}
    \mathcal{L}_{vm} = \lambda_{3d}\mathcal{L}_{3d} + \lambda_{c}\mathcal{L}_{conf} + \lambda_{m}\mathcal{L}_{mesh}.
\end{aligned}
\end{equation}

\noindent\textbf{Virtual marker loss.}
We define $\mathcal{L}_{3d}$ as the $L_1$ distance between the predicted 3D virtual markers $\vmest$ and the GT $\vmgt$ as follows:
\begin{equation}
\label{eq:loss_pose}
\begin{aligned}
    \mathcal{L}_{3d} = \| \vmest-\vmgt \|_1.
\end{aligned}
\end{equation}
Note that it is easy to get GT markers $\vmgt$ from GT meshes as stated in Section \ref{subsec:body_arche} without additional manual annotations. \\

\noindent\textbf{Confidence loss.}
We also require that the 3D heatmaps have reasonable shapes, therefore, the heatmap score at the voxel containing the GT marker position $\vmgt_z$ should have the maximum value as in the previous work \cite{iskakov2019learnable}:
\begin{equation}
\label{eq:loss_conf}
\begin{aligned}
    \mathcal{L}_{conf} = - \sum_{z=1}^{K} log(\hmapest_z(\vmgt_z)).
\end{aligned}
\end{equation}

\noindent\textbf{Mesh loss.}
Following \cite{moon2020i2l}, we define $\mathcal{L}_{mesh}$ as a weighted sum of four losses:
\begin{equation}
\label{eq:mesh_loss}
\begin{aligned}
    \mathcal{L}_{mesh} = \mathcal{L}_{vertex} + \mathcal{L}_{pose} + \mathcal{L}_{normal} + \lambda_{e}\mathcal{L}_{edge}.
\end{aligned}
\end{equation}
\begin{itemize}
    \item[--] \textbf{Vertex coordinate loss.} We adopt $L_1$ loss between predicted 3D mesh coordinates $\meshest$ with GT mesh $\meshgt$ as:
        \begin{equation}
        \label{eq:loss_vertex}
        \begin{aligned}
            \mathcal{L}_{vertex} = \| \meshest-\meshgt \|_1.
        \end{aligned}
        \end{equation}
    \item[--] \textbf{Pose loss.} We use $L_1$ loss between the 3D landmark joints regressed from mesh $\meshest\mathcal{J}$ and the GT joints $\meshgt\mathcal{J}$ as:
        \begin{equation}
        \label{eq:loss_reg}
        \begin{aligned}
            \mathcal{L}_{pose} = \| \meshest\mathcal{J}-\meshgt\mathcal{J} \|_1,
        \end{aligned}
        \end{equation}
    where $\mathcal{J} \in \mathbb{R}^{M \times J}$ is a pre-defined joint regression matrix in SMPL model \cite{bogo2016keep} and $J$ is the number of joints.

    \item[--] \textbf{Surface losses.} To improve surface smoothness \cite{wang2018pixel2mesh}, we supervise the normal vector of a triangle face with GT normal vectors by $\mathcal{L}_{normal}$ and the edge length of the predicted mesh with GT length by $\mathcal{L}_{edge}$:
        \begin{equation}
        \label{eq:loss_surface}
        \begin{aligned}
            & \mathcal{L}_{normal} = \sum_{f} \sum_{\{i, j\} \subset f}  \left| \left< \frac{\meshest_i - \meshest_j}{\| \meshest_i - \meshest_j \|_2}, \normalgt_f\right> \right|, \\
            & \mathcal{L}_{edge} = \sum_{f} \sum_{\{i, j\} \subset f}  \left|\| \meshest_i - \meshest_j \|_2 - \| \meshgt_i - \meshgt_j \|_2 \right|.
        \end{aligned}
        \end{equation}
    where $f$ and $\normalgt_f$ denote a triangle face in the mesh and its GT unit normal vector, respectively. $\meshest_i$ denote the $i^{th}$ vertex of $\meshest$. $\hat{}$ denotes GT.
\end{itemize}



\subsection{Probabilistic mesh estimation framework}
\label{subsec:approach_pro}
\subsubsection{\vmproname}
As shown in Figure \ref{fig:body_arche}, virtual markers are often self-occluded, thus there may exist multiple viable solutions. An incorrect estimation may introduce errors into the final mesh estimate. Therefore, we propose a probabilistic framework for modeling the estimation of 3D VMs, as illustrated in Figure \ref{fig:pipeline} (bottom). By leveraging the powerful modeling capabilities of diffusion models \cite{ho2020denoising,Song2020ScoreBasedGM} on data distributions, we generate multiple feasible 3D VM estimates given an image condition. Specifically, we model the estimation of 3D VMs as a conditional reverse denoising process that starts with Gaussian noise and progressively denoises it to obtain the 3D VM using a denoiser network. In the following, we detail the problem formulation of the 3D VM estimation using diffusion models, including the definition of the \textit{forward diffusion process} and the \textit{reverse denoising process}. Then, we introduce the architecture design of the denoiser network. \\


\noindent\textbf{Problem formulation.}
We formulate the 3D VM estimation as a reverse denoising process \cite{ho2020denoising}, as shown in the inference workflow at the bottom of Figure \ref{fig:pipeline}. Starting with a Gaussian noise $\mathcal{N}(\mathbf{0}, \mathbf{I})$, we design a denoiser network to iteratively estimate the clean 3D VM. To achieve this, the framework includes two core processes: a forward diffusion process and a reverse denoising process. (1) The forward diffusion process disturbs a clean 3D VM, \ie the ground-truth (GT) 3D VM $\vmgt$ from the data distribution, by incrementally adding noise to generate a series of noisy 3D VMs and ultimately reaches a Gaussian prior. (2) Then the reverse denoising process iteratively denoises the Gaussian noise into clean data in a step-by-step manner. To guide the denoising process, we condition it on the 2D image cues $\feat$ and the estimated 2D VMs $\vmestuv \in \mathbb{R}^{2 \times K}$, which is the first two dimensions of the estimated 3D VMs $\vmest$ by \vmname. By formulating the estimation as a reverse denoising process, we leverage the generative capabilities of diffusion models, framing the task as a probabilistic estimation naturally. \\

\noindent\textbf{Forward diffusion process.}
Starting from a clean 3D VM $\vmgt$ drawn from the GT data distribution, also denoted as $\vmgt(0)$, we construct a time-dependent diffusion process by incrementally adding noise to $\vmgt(0)$ to generate a series of noisy 3D VMs $\{\vmgt(t)\}_{t=0}^T$, indexed by a discrete timestep $t \in \{0, 1, ..., T\}$, where $T$ denotes the total number of timesteps. Formally, we define a noisy 3D VM $\vmgt(t)$ at timestep $t$ as:
\begin{align}
    q(\vmgt(1:T)|\vmgt(0)) &:= \prod_{t=1}^{T}q(\vmgt(t)|\vmgt(t-1)), \\
    q(\vmgt(t)|\vmgt(t-1)) &:= \mathcal{N}(\vmgt(t);\sqrt{1-\beta_{t}}\vmgt(t-1),\beta_{t}\mathbf{I}),
\end{align}
where $\{\beta_t\}_{t=1}^{T}$ is a variance schedule \cite{ho2020denoising}. Leveraging the additive property of independent Gaussian distributions and reparameterization techniques \cite{kingma2013auto}, we can obtain the perturbation $\vmgt(t)$ from a clean 3D VM $\vmgt(0)$ directly:
\begin{equation} 
\label{eq:x_t}
q(\vmgt(t)|\vmgt(0)):= \sqrt{\overline{\alpha}_{t}}\vmgt(0)+\sqrt{1-\overline{\alpha}_{t}}\noisegt,
\end{equation}
where $\alpha_{t}:=1-\beta_t$, $\overline{\alpha_{t}}:=\prod_{u=1}^t \alpha_u$, and $\noisegt \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ as a Gaussian noise. Following previous work \cite{ho2020denoising}, we disturb any clean 3D VM $\vmgt(0)$ to a Gaussian distribution $\vmgt(T) \sim \mathcal{N}(\mathbf{0},\mathbf{I})$. \\


\noindent\textbf{Reverse denoising process.}
Starting from a Gaussian distribution, denoted as $\vmest(T) \sim \mathcal{N}(\mathbf{0},\mathbf{I})$, we reverse the diffusion process by incrementally denoising it to get a clean 3D VM $\vmest(0)$. We adopt Denoising Diffusion Implicit Models (DDIM) \cite{song2021denoising} to accelerate the reverse denoising process with fewer timesteps while keeping the denoising quality. Formally, following DDIM \cite{song2021denoising}, we define a timestep subset $\tau \subset \{0, 1, ..., T\}$, and the reverse process is defined as:
\begin{align}
\begin{split}
\label{eq:x_0}
\vmest({\tau_{i-1}}) &= \sqrt{\overline{\alpha}_{\tau_{i-1}}}(\frac{\vmest({\tau_i})-\sqrt{1-\overline{\alpha}_{\tau_{i}}}\noiseest_{\tau_{i}}}  {\sqrt{\overline{\alpha}_{\tau_{i}}}}) \\
&+ \sqrt{1-\overline{\alpha}_{\tau_{i-1}}-\sigma_{\tau_i}^2}\noiseest_{\tau_{i}}+\sigma_{\tau_i}\noisegt_{\tau_{i}},
\end{split}
\end{align}
where $\tau_i, \tau_{i-1}$ are the adjacent timesteps in the subset $\tau$. $\sigma_{\tau_i}(\eta) = \eta \sqrt{(1-\overline{\alpha}_{\tau_{i-1}})/({1-\overline{\alpha}_{\tau_{i}}})}\sqrt{1-\overline{\alpha}_{\tau_{i}}/\overline{\alpha}_{\tau_{i-1}}}$, where $\eta \in \mathbb{R}_{\geq 0}$ is a hyperparameter to control the generation process. $\noisegt_{\tau_{i}} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ is standard Gaussian noise. 

In order to simulate Eq. \ref{eq:x_0} to perform the denoising process, we need to know $\noiseest_{\tau_{i}}$ for all timestep $\tau_{i}$. Therefore, we train a time-dependent denoiser network to estimate it as shown at the bottom block of Figure \ref{fig:pipeline}. Instead of directly predicting $\noiseest_{\tau_{i}}$ as the original formulation by Ho \etal \cite{ho2020denoising}, following previous work \cite{ramesh2022hierarchical,tevet2023human}, we predict the clean signal itself, \ie $\vmest(0)$, and obtain the noise $\noiseest_{\tau_{i}}$ through:
\begin{equation}
\label{eq:infer_eps}
\begin{aligned}
    \noiseest_{\tau_i} = \frac{\vmest({\tau_i}) - \sqrt{\overline{\alpha}_{{\tau_i}}}\vmest(0)}{\sqrt{1-\overline{\alpha}_{\tau_i}}}.
\end{aligned}
\end{equation}
By combining Eq. \ref{eq:infer_eps} and Eq. \ref{eq:x_0}, we can get the denoised $\vmest({\tau_{i-1}})$ as follows:
\begin{equation}
\label{eq:infer_t_1}
\begin{aligned}
    \vmest({\tau_{i-1}}) &= \sqrt{\overline{\alpha}_{\tau_{i-1}}}\vmest(0) \\
    &+ \sqrt{1-\overline{\alpha}_{\tau_{i-1}}-\sigma_{\tau_i}^2}\frac{\vmest({\tau_i}) - \sqrt{\overline{\alpha}_{{\tau_i}}}\vmest(0)}{\sqrt{1-\overline{\alpha}_{\tau_i}}} \\
    &+\sigma_{\tau_i}\noisegt_{\tau_{i}}.
\end{aligned}
\end{equation} 



\noindent\textbf{Denoiser architecture design.}
Figure \ref{fig:diff_model} presents an overview of the time-dependent denoiser network architecture. The denoiser aims to estimate the clean 3D VM $\vmest(0)$ given a noisy 3D VM $\vmgt(t)$ at timestep $t$ as input. To guide the prediction, we concatenate $\vmgt(t)$ with the estimated 2D VM $\vmestuv$, and then transform them to feature $\featvm \in \mathbb{R}^{K \times C}$ by a multilayer perceptron (MLP), where $C$ denotes the number of feature channel. We then deploy a network with $B$ blocks to process the VM features $\featvm$. Each block is composed of two Locally Connected Network (LCN) units \cite{ci2019lcn, ci2020locally} sequentially. Within each unit, an LCN layer \cite{ci2019lcn, ci2020locally} is followed by Group Normalization (GN) \cite{wu2018group}, a SiLU activation function \cite{ramachandran2017searching}, and Dropout \cite{srivastava2014dropout} for regularization. The LCN layer, as proposed in \cite{ci2019lcn, ci2020locally}, is akin to an enhanced version of MLP, offering more effective feature processing across virtual markers.

To improve the alignment of the denoised 3D VM with 2D image cues, we extract 2D image features $\feat \in \mathbb{R}^{H_b \times W_b \times C_b}$ using the CNN backbone \cite{sun2018integral}. These features are sampled according to the estimated 2D VMs $\vmestuv$ and subsequently transformed into a new feature space $\featimg \in \mathbb{R}^{K \times C}$ through a linear layer, as illustrated in Figure \ref{fig:diff_model}. 
We then fuse $\featimg$ into the network by Cross-Attention \cite{NIPS2017_transformer}. To achieve this, we slightly modify the LCN layer by incorporating an attention module \cite{NIPS2017_transformer}.

In addition, we process the current timestep $t$ using an MLP to be $\feattime \in \mathbb{R}^{1 \times C}$ and add it to the processed VM features $\featvm$ to make the denoiser time-dependent following common practice \cite{Song2020ScoreBasedGM, ci2023gfpose}. Finally, an FC layer is deployed to estimate the clean signal $\vmest(0)$. Please refer to the supplementary for more details.



\subsubsection{Training}
\label{subsec:training_pro}

The whole framework is trained in an end-to-end manner. The overall loss function is defined as:
\begin{equation}
\label{eq:loss_vmpro}
\begin{aligned} 
    \mathcal{L}_{vmpro} = \mathcal{L}_{vm} + \lambda_{diffusion}\mathcal{L}_{diffusion},
\end{aligned}
\end{equation}
where $\mathcal{L}_{vm}$ is the loss function for \vmname, as defined in Eq. \ref{eq:total_loss}. Following common practice in training diffusion models \cite{ho2020denoising, tevet2023human}, we randomly sample a timestep $t \in \{0, 1, ..., T\}$ to get the noisy 3D VM $\vmgt(t)$ according to Eq. \ref{eq:x_t} and deploy $\mathcal{L}_{diffusion}$ as the $L_1$ distance between the predicted clean 3D VM $\vmest(0)$ and the GT $\vmgt(0)$:
\begin{equation}
\label{eq:loss_simple}
\begin{aligned}
    \mathcal{L}_{diffusion} = \mathbb{E}_{t,\vmgt(0),\noisegt}[\| \vmest(0)-\vmgt(0) \|_1].
\end{aligned}
\end{equation}


\subsubsection{Inference}
As shown in Figure \ref{fig:pipeline}, at inference, the denoiser iteratively removes noise from a Gaussian noise $\vmest(T) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ using DDIM \cite{song2021denoising} according to Eq. \ref{eq:infer_t_1}, yielding a denoised 3D VM $\vmest(0)$. Finally, we obtain a mesh estimate $\meshest = \vmest(0) \Aest$. By sampling $\hyponum$ Gaussian noises, we can obtain $\hyponum$ feasible mesh estimates. \\


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/diff_model_v1.pdf}
	\caption{Architecture of the denoiser network, which predicts the clean 3D VM $\vmest(0)$ from the noisy 3D VM $\vmgt(t)$. To guide the direction of denoising, we additionally input 2D features $\feat$, the current timestep $t$, and the 2D VM $\vmestuv$ predicted by \vmname. We concatenate $\vmestuv$ with the noisy 3D VM $\vmgt(t)$ to facilitate training and fuse the sampled 2D features $\feat$ through a Cross-Attention mechanism \cite{NIPS2017_transformer}. We utilize a network comprising $B$ blocks to process the features, where each block contains two Locally Connected Network (LCN) units \cite{ci2019lcn, ci2020locally}. Each LCN units consists of one LCN layer \cite{ci2019lcn, ci2020locally}, followed by Group Normalization (GN) \cite{wu2018group}, SiLU activation \cite{ramachandran2017searching}, and Dropout \cite{srivastava2014dropout}. Finally, an FC layer is used to predict the clean 3D VM $\vmest(0)$.}
	\label{fig:diff_model}
\end{figure}