\section{Experiments}
\label{sec:experiments}

\begin{table*}[ht]
\center
\caption{Comparison to the deterministic state-of-the-arts on H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} datasets. $^{\dagger}$ means using temporal cues. The methods are not strictly comparable because they may have different backbones and training datasets. We provide the numbers only to show proof-of-concept results.}
\label{tab:state_of_the_art_det}
\setlength{\tabcolsep}{6pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l l | c c c | c c c}
    \hline 
    \multirow{2}{*}{Method} & \multirow{2}{*}{Venue} & Intermediate & \multicolumn{3}{c}{H3.6M \cite{h36m_pami}} & \multicolumn{3}{|c}{3DPW \cite{vonMarcard2018}} \\

    \cline{4-6} \cline{7-9}
    & & Representation & MPVE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ & MPVE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ \\
    \hline 
    % $^{\dagger}$ Arnab \etal \cite{arnab2019exploiting} & CVPR'19  & 2D skeleton & - & 77.8 & 54.3 & - & - & 72.2 \\
    $^{\dagger}$ HMMR \cite{kanazawa2019learning} & CVPR'19 & - & - & - & 56.9 & 139.3 & 116.5 & 72.6 \\
    $^{\dagger}$ DSD-SATN \cite{sun2019human} & ICCV'19 & 3D skeleton & - & 59.1 & 42.4 & - & - & 69.5 \\
    $^{\dagger}$ VIBE \cite{kocabas2020vibe} & CVPR'20 & - & - & 65.9 & 41.5 & 99.1 & 82.9 & 51.9 \\
    $^{\dagger}$ TCMR \cite{choi2021beyond} & CVPR'21 & - & - & 62.3 & 41.1 & 102.9 & 86.5 & 52.7 \\
    $^{\dagger}$ MAED \cite{wan2021encoder} & ICCV'21 & 3D skeleton & - & 56.3 & 38.7 & 92.6 & 79.1 & 45.7 \\
    \hline
    SMPLify \cite{bogo2016keep} & ECCV'16 & 2D skeleton & - & - & 82.3 & - & - & - \\
    % \cline{2-8}
    HMR \cite{kanazawa2018end} & CVPR'18 & - & 96.1 & 88.0 & 56.8 & 152.7 & 130.0 & 81.3 \\
    GraphCMR \cite{kolotouros2019convolutional} & CVPR'19 & 3D vertices & - & - & 50.1 & - & - & 70.2 \\
    SPIN \cite{kolotouros2019learning} & ICCV'19 & - & - & - & 41.1 & 116.4 & 96.9 & 59.2 \\
    DenseRac \cite{xu2019denserac} & ICCV'19 & IUV image & - & 76.8 & 48.0 & - & - & - \\
    DecoMR \cite{zeng20203d} & CVPR'20 & IUV image & - & 60.6 & 39.3 & - & - & - \\
    ExPose \cite{choutas2020monocular} & ECCV'20 & - & - & - & - & - & 93.4 & 60.7 \\
    Pose2Mesh \cite{choi2020pose2mesh} & ECCV'20 & 3D skeleton & 85.3 & 64.9 & 46.3 & 106.3 & 88.9 & 58.3 \\
    I2L-MeshNet \cite{moon2020i2l} & ECCV'20 & 3D vertices & 65.1 & 55.7 & 41.1 & 110.1 & 93.2 & 57.7 \\
    PC-HMR \cite{luan2021pc} & AAAI'21 & 3D skeleton & - & - & - & 108.6 & 87.8 & 66.9  \\
    HybrIK \cite{li2021hybrik} & CVPR'21 & 3D skeleton & 65.7 & 54.4 & 34.5 & 86.5 & 74.1 & 45.0  \\
    METRO \cite{lin2021end} & CVPR'21 & 3D vertices & - & 54.0 & 36.7 & 88.2 & 77.1 & 47.9 \\
    ROMP \cite{sun2021monocular} & ICCV'21 & - & - & - & - & 108.3 & 91.3 & 54.9 \\
    Mesh Graphormer\cite{Lin_2021_ICCV} & ICCV'21 & 3D vertices & - & 51.2 & 34.5 & 87.7 & 74.7 & 45.6 \\
    PARE \cite{Kocabas_2021_ICCV} & ICCV'21 & Segmentation & - & - & - & 88.6 & 74.5 & 46.5 \\
    THUNDR \cite{zanfir2021thundr} & ICCV'21 & 3D markers & - & 55.0 & 39.8 & 88.0 & 74.8 & 51.5 \\
    PyMaf \cite{zhang2021pymaf} & ICCV'21 & IUV image & - & 57.7 & 40.5 & 110.1 & 92.8 & 58.9 \\
    OCHMR \cite{Khirodkar_2022_CVPR} & CVPR'22 & 2D heatmap & - & - & - & 107.1 & 89.7 & 58.3 \\
    3DCrowdNet \cite{Choi_2022_CVPR} & CVPR'22 & 3D skeleton & - & - & - & 98.3 & 81.7 & 51.5 \\
    CLIFF \cite{li2022cliff} & ECCV'22 & - & - & \textbf{47.1} & 32.7 & 81.2 & 69.0 & 43.0 \\
    FastMETRO \cite{cho2022FastMETRO} & ECCV'22 & 3D vertices & - & 52.2 & 33.7 & 84.1 & 73.5 & 44.6 \\
    VisDB \cite{yao2022learning} & ECCV'22 & 3D vertices & - & 51.0 & 34.5 & 85.5 & 73.5 & 44.9 \\
    \rowcolor{mygray}
    \rowcolor{mygray}
    \textbf{\vmname\ (Ours)} \cite{ma20233d} & CVPR'23 & Virtual marker & \textbf{58.0} & {47.3} & \textbf{32.0} & \textbf{77.9} & \textbf{67.5} & \textbf{41.3} \\
    \hline 
\end{tabular}}
\end{table*}


\begin{table*}[t]
\center
\caption{Comparison to the probabilistic state-of-the-arts on H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} datasets. The methods are not strictly comparable because they may have different backbones and training datasets. We provide the numbers only to show proof-of-concept results.}
\label{tab:state_of_the_art_pro}
\setlength{\tabcolsep}{8pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l | c c c | c c c}
    \hline 
    \multirow{2}{*}{Method} & Hypothesis & \multicolumn{3}{c}{H3.6M \cite{h36m_pami}} & \multicolumn{3}{|c}{3DPW \cite{vonMarcard2018}} \\

    \cline{3-5} \cline{6-8}
    & Number $\hyponum$ & MPVE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ & MPVE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ \\
    \hline 
    
    \multirow{2}{*}{Biggs \etal \cite{biggs2020multibodies} NeurIPS'20} & 10 & - & 59.2 & 42.2 & - & 79.4 & 56.6\\
     & 25 & - & 58.2 & 42.2 & - & 75.8 & 55.6\\
    \hline 
    Sengupta \etal \cite{sengupta2021hierarchical} ICCV'21 & 25   & - & - & - & - & 75.1 & 47.0\\
    \hline 
    \multirow{4}{*}{ProHMR \cite{Kolotouros_2021_ICCV} ICCV'21} & 10 & - & - & - & - & 88.9 & 55.0\\
     & 25 & - & - & - & - & 85.1 & 52.1\\
     & 100 & - & - & - & - & 80.1 & 48.1\\
     & 200 & - & - & - & - & 77.9 & 46.5\\
    \hline 
    \multirow{4}{*}{HuManiFlow \cite{Sengupta_2023_CVPR} CVPR'23} & 10 & - & - & - & - & 75.6 & 47.9\\
     & 25 & - & - & - & - & 71.9 & 44.5\\
     & 100 & - & - & - & - & 65.1 & 39.9\\
     & 200 & - & - & - & - & 64.5 & 38.8\\
    \hline 
    HMDiff \cite{Foo_2023_ICCV} ICCV'23 & 25 & - & 49.3 & 32.4 & 82.4 & 72.7 & 44.5\\
    \hline 
    \rowcolor{mygray}
     & 10  & 55.8 & 45.4 & 31.3 & 77.3 & 66.6 & 42.5   \\
    \rowcolor{mygray}
     & 25 & 52.9 & 42.9 & 29.8 & 72.9 & 63.2 & 40.0   \\
    \rowcolor{mygray}
     & 100 & 49.2 &  39.9 & 27.6 & 67.4 & 58.2 & 36.4   \\
    \rowcolor{mygray}
    \multirow{-4}{*}{\textbf{\vmproname\ (Ours)}} & 200 & \textbf{47.7} & \textbf{38.6} & \textbf{26.6} & \textbf{64.7} & \textbf{56.2} & \textbf{35.0}  \\

    \hline 
\end{tabular}}
\end{table*}


\subsection{Datasets and metrics}
\label{subsec:dataset}
\noindent\textbf{H3.6M \cite{h36m_pami}.} We use (S1, S5, S6, S7, S8) for training and (S9, S11) for testing. 
For the deterministic estimation task, as in \cite{kanazawa2018end, choi2020pose2mesh, lin2021end, Lin_2021_ICCV}, we report Mean Per Joint Position Error (MPJPE) and PA-MPJPE for poses that are derived from the estimated meshes. We also report Mean Per Vertex Error (MPVE) for the whole mesh. For the probabilistic estimation task, we report the minimum errors over $\hyponum$ hypotheses to measure the estimated distribution accuracy following a standard practice in recent works \cite{Sengupta_2023_CVPR, Foo_2023_ICCV}. \\


\noindent\textbf{3DPW \cite{vonMarcard2018}} is collected in natural scenes. 
Following previous works \cite{lin2021end, Lin_2021_ICCV, Kocabas_2021_ICCV, zanfir2021thundr, Sengupta_2023_CVPR, Foo_2023_ICCV}, we use the train set of 3DPW to learn the model and evaluate on the test set. The same evaluation metrics as H3.6M are used. We further evaluate the performance of our probabilistic framework \vmproname\ on two subsets of 3DPW, \ie 3DPW-OC \cite{vonMarcard2018, Zhang_2020_CVPR} and 3DPW-PC \cite{vonMarcard2018, sun2021monocular} which are composed by object- and person-specific occlusion, respectively. To ensure a fair comparison, as suggested by \cite{Li_2023_ICCV}, we do not use the 3DPW training set in these particular evaluations. \\

\noindent\textbf{SURREAL \cite{varol2017learning}} is a large-scale synthetic dataset with GT SMPL annotations and has diverse samples in terms of body shapes, backgrounds, \etc We use its training set to train a model and evaluate the test split following \cite{choi2020pose2mesh}. The same evaluation metrics as H3.6M \cite{h36m_pami} are reported.


\subsection{Implementation Details}
\label{subsec:implementation}
We learn $64$ virtual markers on the H3.6M \cite{h36m_pami} training set. We use the same set of markers for all datasets instead of learning a separate set for each one. Following \cite{kanazawa2018end, choi2020pose2mesh, moon2020i2l, zanfir2021thundr, kolotouros2019convolutional, kocabas2020vibe, Lin_2021_ICCV, lin2021end}, we conduct mix-training by using MPI-INF-3DHP \cite{mehta2017monocular}, UP-3D \cite{lassner2017unite}, and COCO \cite{lin2014microsoft} training set for experiments on the H3.6M and 3DPW datasets. 

We adapt a 3D pose estimator \cite{sun2018integral} with HRNet-W48 \cite{sun2019deep} as the image feature backbone for estimating the 3D virtual markers. We set the number of voxels in each dimension to be $64$, \ie $D = H = W = 64$ for 3D heatmaps. Following \cite{kanazawa2018end, kolotouros2019convolutional, moon2020i2l}, we crop every single human region from the input image and resize it to $256 \times 256$. The dimensions of the 2D feature map are $64$, \ie $H_b = W_b = 64$. The 2D feature map has $C_b = 48$ feature channels. The processed $\featimg$ feature channels are set to be $C=64$. The denoiser network has $B=3$ blocks. 
To train \vmname, we use Adam \cite{kingma2015adam} optimizer to train the whole framework for $40$ epochs with a batch size of $32$. The learning rates for the 3D VM estimation branch and the updating branch are set to $5 \times 10^{-4}$ and $1 \times 10^{-3}$, respectively, which are decreased by half after the $30^{th}$ epoch. To train \vmproname, we use Adam \cite{kingma2015adam} optimizer to train the whole framework for $50$ epochs with a batch size of $80$. The learning rates for the 3D VM estimation branch, the updating branch, and the denoiser are set to $4 \times 10^{-5}$, $2 \times 10^{-4}$, and $2 \times 10^{-3}$, respectively. They are decayed by $0.5$, $0.5$, and $0.1$ after $30^{th}$ and $40^{th}$ epochs, respectively. For inference, we employ DDIM \cite{song2021denoising} strategy and generate each hypothesis in $T=10$ steps with $\eta = 0$.
Please check the supplementary for more details.


\begin{table}[t]
\center
 \caption{Comparison to the state-of-the-arts on SURREAL \cite{varol2017learning} dataset. $^{*}$ means training on the test split with 2D supervisions. ``Skel. + Seg.'' means using skeleton and segmentation together. $\hyponum$ is the number of hypotheses.}
\label{tab:state_of_the_art_surreal}
\setlength{\tabcolsep}{4pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l l | c c c}
    \hline
    \multirow{2}{*}{Method} & Intermediate & \multirow{2}{*}{$\hyponum$} & \multirow{2}{*}{MPVE$\downarrow$} & \multirow{2}{*}{MPJPE$\downarrow$} & \multirow{2}{*}{PA-MPJPE$\downarrow$} \\
    & Representation &  & &  &  \\
    \hline
    HMR \cite{kanazawa2018end} & -  & - & 85.1 & 73.6 & 55.4 \\
    BodyNet \cite{varol2018bodynet} & Skel. + Seg. & - & 65.8 & - & - \\
    GraphCMR \cite{kolotouros2019convolutional} & 3D vertices & - & 103.2 & 87.4 & 63.2  \\
    SPIN \cite{kolotouros2019learning} & - & - & 82.3 & 66.7 & 43.7 \\
    DecoMR \cite{zeng20203d} & IUV image & - & 68.9 & 52.0 & 43.0 \\
    Pose2Mesh \cite{choi2020pose2mesh} & 3D skeleton & - & 68.8 & 56.6 & 39.6 \\
    PC-HMR \cite{luan2021pc} & 3D skeleton & - & 59.8 & 51.7 & 37.9  \\
    $^{*}$ DynaBOA \cite{guan2022out} & - & - & 70.7 & 55.2 & 34.0 \\
    \rowcolor{mygray}
    \textbf{\vmname\ (Ours)}          & Virtual marker & - & 44.7 & 36.9 & 28.9 \\
    \hline
    \rowcolor{mygray}
     &  & 10 & 44.3 & 36.6 & 28.0 \\
    \rowcolor{mygray}
    \textbf{\vmproname} &  & 25 & 42.0 & 34.8 & 26.2 \\
    \rowcolor{mygray}
    \textbf{(Ours)} &  & 100 & 38.1 & 31.0 & 23.5 \\
    \rowcolor{mygray}
    & \multirow{-4}{*}{Virtual marker}  & 200 & \textbf{36.5} & \textbf{29.8} & \textbf{22.4} \\
    \hline 
\end{tabular}}

\end{table}


\begin{table*}[t]
\center
\caption{Comparison to the state-of-the-art methods on the challenging occlusion-specific 3DPW-OC \cite{vonMarcard2018, Zhang_2020_CVPR} and 3DPW-PC \cite{vonMarcard2018, sun2021monocular} datasets. The top and bottom blocks are deterministic and probabilistic methods, respectively.}
\label{tab:sota_occlusion}
\setlength{\tabcolsep}{8pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l l |c c c| c c c}
    \hline
    \multirow{2}{*}{Method} & \multirow{2}{*}{Venue} & Hypothesis & \multicolumn{3}{c|}{3DPW-OC \cite{vonMarcard2018, Zhang_2020_CVPR}} & \multicolumn{3}{c}{3DPW-PC \cite{vonMarcard2018, sun2021monocular}} \\ 
    \cline{4-9}
    & & Number $\hyponum$  & MPVE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$  
    & MPVE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ \\ 
    \hline
    SPIN \cite{kolotouros2019learning} & ICCV'19 & -     & 121.4  & 95.5  & 60.7 & 159.8  & 122.1  & 77.4  \\
    I2L-MeshNet \cite{moon2020i2l} &  ECCV'20 & -         & 129.5  & 92.0  & 61.4 & 160.2  & 117.3  & 80.0  \\
    PyMAF \cite{zhang2021pymaf} & ICCV'21 & -            & 113.7  & 89.6  & 59.1 & 154.6  & 117.5  & 74.5  \\
    ROMP \cite{sun2021monocular} & ICCV'21 & -           & -      & 91.0  & 62.0 & 152.8  & 117.9  & 79.7  \\
    PARE \cite{Kocabas_2021_ICCV} & ICCV'21 & -          & 101.5  & 83.5  & 57.0 & 122.4  & 96.8   & 64.5  \\
    OCHMR \cite{Khirodkar_2022_CVPR} & CVPR'22 & -       & 145.9  & 112.2 & 75.2 & 149.6  & 117.5  & 77.1     \\
    3DCrowdNet \cite{Choi_2022_CVPR} & CVPR'22 & -       & 101.5  & 83.5  & 57.1 & 114.8  & 90.9   & 64.4  \\
    JOTR \cite{Li_2023_ICCV} & ICCV'23 & -               & 92.6   & 75.7  & 52.2 & 109.7  & 86.5   & 58.3  \\ 
    \rowcolor{mygray}
    \textbf{\vmname\ (Ours)} \cite{ma20233d} & CVPR'23 & -         & 92.8   & 78.4  & 49.0 & 110.8  & 93.9   & 66.0  \\ 
    \hline
    HMDiff \cite{Foo_2023_ICCV} & ICCV'23 & 25           & -      & -     & -    & 143.1  & 114.2  & 73.5  \\
    \rowcolor{mygray}
     & & 10 & 90.4   & 77.3  & 48.6 & 109.4  & 89.8   & 60.2  \\
    \rowcolor{mygray}
     & & 25 & 85.4   & 73.3  & 46.0 &  102.9  &  84.7   &  56.6  \\ 
    \rowcolor{mygray}
     & & 100 & 77.7   & 67.1  &  42.5 & 94.0   & 78.0   & 52.0  \\
    \rowcolor{mygray}
    \multirow{-4}{*}{\textbf{\vmproname\ (Ours)}} & \multirow{-4}{*}{-} & 200 & \textbf{75.2}  & \textbf{64.9} & \textbf{40.9} & \textbf{90.3}  & \textbf{74.6}   & \textbf{49.7}  \\
    \hline
\end{tabular}}
\end{table*}


\subsection{Comparison to the State-of-the-arts}
\subsubsection{Deterministic Track}
\label{subsec:sota_det}

\noindent\textbf{Results on H3.6M \cite{h36m_pami}.}
Table \ref{tab:state_of_the_art_det} compares our approach to the deterministic state-of-the-art (SOTA) methods on the H3.6M dataset. Our method achieves superior performance. In particular, it outperforms the methods that use skeletons (Pose2Mesh \cite{choi2020pose2mesh}, DSD-SATN \cite{sun2019human}), body markers (THUNDR) \cite{zanfir2021thundr}, or IUV image \cite{zeng20203d, zhang2021pymaf} as proxy representations, demonstrating the effectiveness of the virtual marker representation. \\

\noindent\textbf{Results on 3DPW \cite{vonMarcard2018}.}
We compare our method to the deterministic SOTA methods on the 3DPW dataset in Table \ref{tab:state_of_the_art_det}. Our approach achieves SOTA results among all the methods, validating the advantages of the virtual marker representation over the skeleton representation used in Pose2Mesh \cite{choi2020pose2mesh}, DSD-SATN \cite{sun2019human}, and other representations like IUV image used in PyMAF \cite{zhang2021pymaf}. In particular, our approach outperforms I2L-MeshNet \cite{moon2020i2l}, METRO \cite{lin2021end}, and Mesh Graphormer \cite{Lin_2021_ICCV} by a notable margin, suggesting that virtual markers are more suitable and effective representations than using all vertices as most of them are not discriminative enough to be accurately detected. \\



\noindent\textbf{Results on SURREAL \cite{varol2017learning}.}
This dataset has more diverse samples in terms of body shapes. The results are shown in Table \ref{tab:state_of_the_art_surreal}. Our approach \vmname, outperforms the SOTA methods by a notable margin, especially in terms of MPVE. Figure \ref{fig:teaser} (top) shows some challenging cases without cherry-picking. The skeleton representation loses the body shape information so the method \cite{choi2020pose2mesh} can only recover mean shapes. In contrast, our approach generates much more accurate mesh estimation results. 




\subsubsection{Probabilistic Track}
\label{subsec:sota_pro}

\noindent\textbf{Results on H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018}.}
Table \ref{tab:state_of_the_art_pro} presents a comparison between \vmproname\ and other SOTA probabilistic methods on the H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} datasets. Our method significantly surpasses the existing SOTA approaches in terms of accuracy when estimating data distributions. This serves as a validation of the effectiveness of our probabilistic framework, which benefits from the generation capability of diffusion models. Furthermore, as the number of hypotheses ($\hyponum$) increases, our method consistently improves, highlighting the diverse and accurate nature of our approach when modeling data distributions. Notably, compared to HMDiff \cite{Foo_2023_ICCV}, which is also a diffusion model-based method, our incorporation of 2D image cues and the virtual marker representations contributes to the observed notable improvement. \\

\noindent\textbf{Results on 3DPW-OC \cite{vonMarcard2018, Zhang_2020_CVPR} and 3DPW-PC \cite{vonMarcard2018, sun2021monocular}.}
Table \ref{tab:sota_occlusion} shows the comparison results of our method against existing methods on the 3DPW-OC and 3DPW-PC datasets. Both 3DPW-OC \cite{vonMarcard2018, Zhang_2020_CVPR} and 3DPW-PC \cite{vonMarcard2018, sun2021monocular} are subsets of the 3DPW \cite{vonMarcard2018} dataset, containing scenarios with occlusions by objects and people, respectively. It can be seen that our deterministic estimation results are comparable to the SOTA performance. When adopting the proposed probabilistic modeling approach, the accuracy of our method in estimating the data distribution is significantly improved, thanks to the enhanced accuracy in 3D virtual marker estimation. Figure \ref{fig:3dpw_oc} presents a comparative visualization between \vmname\ and \vmproname. It can be seen that the deterministic approach, \vmname, may encounter difficulties under occlusions, \eg the wrongly estimated right arm and the incorrect arm length. Conversely, the probabilistic method \vmproname, demonstrates a capability to accurately model realistic human poses, offering feasible estimations for the occluded parts. \\

\begin{figure}[ht]
    \centering
    \vspace{-2em}
    \includegraphics[width=0.95\linewidth]{imgs/experiments/3dpw_oc.pdf}
    \caption{Qualitative comparison of \vmname\ and \vmproname\ on 3DPW-OC \cite{vonMarcard2018,sun2021monocular} testset. For each mesh, we show a projecting view and a side view (with shadow). \vmname\ wrongly estimates the right arm when heavy occlusion happens while \vmproname\ provides two reasonable estimates. }
    \label{fig:3dpw_oc}
\end{figure}




\noindent\textbf{Results on SURREAL \cite{varol2017learning}.}
Table \ref{tab:state_of_the_art_surreal} displays the results of our probabilistic approach on the SURREAL \cite{varol2017learning} dataset. It can be seen that when $S=10$, our probabilistic framework already surpasses existing approaches. This indicates that our method can accurately model the more diverse data distribution. As the number of hypotheses ($S$) increases, the performance of our model continues to improve.


\subsection{Ablation Study on \vmname}
\label{subsec:ablation}
\noindent\textbf{Virtual marker representation.}
\label{subsubsec:ablation_effect}
We compare our method to two baselines in Table \ref{tab:ba_effect}. First, in baseline (a), we replace the virtual markers of our method with the skeleton representation. The rest are kept the same as ours (c). Our method achieves a much lower MPVE than the baseline (a), demonstrating that the virtual markers help to estimate body shapes more accurately than the skeletons. In baseline (b), we randomly sample $64$ from the $6890$ mesh vertices as virtual markers. We repeat the experiment five times and report the average number. We can see that the result is worse than ours, which is because the randomly selected vertices may not be expressive enough to reconstruct the other vertices or can not be accurately detected from images as they lack distinguishable visual patterns. The results validate the effectiveness of our learning strategy. 


\begin{table}[t]
\center
\caption{Ablation study of the virtual marker representation for our approach \vmname\ on H3.6M \cite{h36m_pami} and SURREAL \cite{varol2017learning} datasets. 
``Skeleton'' means the sparse landmark joint representation is used. 
``Rand virtual marker'' means the virtual markers are randomly selected from all the vertices without learning. (c) is our method, where the learned virtual markers are used. MPVE error is reported.}
\label{tab:ba_effect}
\setlength{\tabcolsep}{10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c | l | c | c }
    \hline 
    \multirow{2}{*}{No.} & Intermediate & \multicolumn{2}{c}{MPVE$\downarrow$} \\

    \cline{3-4}
    & Representation & H3.6M & SURREAL \\
    \hline
    (a) & Skeleton & 64.4 & 53.6 \\ 
    (b) & Rand virtual marker & 63.0 & 50.1 \\
    \rowcolor{mygray}
    (c) & Virtual marker & \textbf{58.0} & \textbf{44.7}\\
    \hline 
\end{tabular}}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/experiments/joint_diff.pdf}
	\caption{Mesh estimation results of different methods on H3.6M \cite{h36m_pami} test set. Our method \vmname\ with virtual marker representation gets better shape estimation results than Pose2Mesh \cite{choi2020pose2mesh} which uses skeleton representation. Note the waistline of the body and the thickness of the arm. }
	\label{fig:joint_diff}
\end{figure}



Figure \ref{fig:teaser} (top) shows some qualitative results on the SURREAL test set. The meshes estimated by the baseline which uses skeleton representation, \ie Pose2Mesh \cite{choi2020pose2mesh}, have inaccurate body shapes. This is reasonable because the skeleton is oversimplified and has very limited capability to recover shapes. Instead, it implicitly learns a mean shape for the whole training dataset. In contrast, the mesh estimated by using virtual markers has much better quality due to its strong representation power and therefore can handle different body shapes elegantly. Figure \ref{fig:joint_diff} also shows some qualitative results on the H3.6M test set. For clarity, we also draw the intermediate representation (blue balls) in it. \\


\noindent\textbf{Number of virtual markers.} 
\label{subsubsec:ablation_K}
We evaluate how the number of virtual markers affects estimation quality on H3.6M \cite{h36m_pami} dataset. Figure \ref{fig:body_arche} visualizes the learned 64 markers. Figure \ref{fig:different_K_vis} visualizes the learned virtual markers when $K=16,32,96 $, which are all located on the body surface and close to the extreme points of the mesh. This is expected as mentioned in Section \ref{subsec:body_arche}.  Table \ref{tab:different_K} (GT) shows the mesh reconstruction results when we have GT 3D positions of the virtual markers in objective (Eq. \ref{eq:aa}). When we increase the number of virtual markers, both mesh reconstruction error, \ie MPVE, and the regressed landmark joint error, \ie MPJPE, steadily decrease. This is expected because using more virtual markers improves the representation power. However, using more virtual markers cannot guarantee smaller estimation errors when we need to estimate the virtual marker positions from images as in our method. This is because the additional virtual markers may have large estimation errors which affect the mesh estimation result. The results are shown in Table \ref{tab:different_K} (Est). Increasing the number of virtual markers $K$ steadily reduces the MPVE errors when $K$ is smaller than $96$. However, if we keep increasing $K$, the error begins to increase. This is mainly because some of the newly introduced virtual markers are difficult to detect from images and therefore bring errors to mesh estimation. \\



\begin{figure}[t]
    \noindent\begin{minipage}{\linewidth}
        \captionof{table}{Ablation study of the different number of virtual markers ($K$) on H3.6M \cite{h36m_pami} dataset. (GT) Mesh reconstruction results when GT 3D positions of the virtual markers are used in objective (\ref{eq:aa}). (Est) Mesh estimation results obtained by our method \vmname\ when we use different numbers of virtual markers ($K$).}
        \label{tab:different_K}
        \setlength{\tabcolsep}{13pt}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{c | c  c | c  c }
            \hline 
            \multirow{2}{*}{$K$} & \multicolumn{2}{c|}{GT}  & \multicolumn{2}{c}{Est} \\
        
            \cline{2-3} \cline{4-5}
            & MPVE$\downarrow$ & MPJPE$\downarrow$ & MPVE$\downarrow$ & MPJPE$\downarrow$\\
            \hline
            16 & 46.8 & 39.8 & 58.7 & 47.8 \\ 
            32 & 20.1 & 14.2 & 58.2 & 48.3 \\
            64 & 11.0 & 7.5 & \textbf{58.0} & \textbf{47.3} \\
            96 & \textbf{9.9} & \textbf{5.6} & 59.6 & 48.2\\
            \hline 
        \end{tabular}}
        \vspace{0.8em}
    
        \includegraphics[width=\linewidth]{imgs/experiments/different_K_vis.pdf}
        \captionof{figure}{Visualization of the learned virtual markers when $K = 16, 32, 96$, from left to right, respectively.}
        \label{fig:different_K_vis}
    \end{minipage}
\end{figure}



\noindent\textbf{Updating coefficient matrix.}
\label{subsubsec:blending_effect}
We compare our method to a baseline which uses the fixed coefficient matrix $\widetilde{\mathbf{A}}^{sym}$. 
We show the quality comparison in Figure \ref{fig:blending_nr_diff}. We can see that the estimated mesh by (a) a fixed coefficient matrix has mostly correct pose and shape but there are some artifacts on the mesh while using the (b) updated coefficient matrix can get better mesh estimation results. 
As shown in Table \ref{tab:quan_bm_nr_effect}, using a fixed coefficient matrix gets larger MPVE and MPJPE errors than using the updated one. This is caused by the estimation errors of virtual markers when occlusion happens, which is inevitable since the virtual markers on the back will be self-occluded by the front body. As a result, inaccurate marker positions would bring large errors to the final mesh estimates if we directly use the fixed matrix.


\begin{figure}[t]
    \noindent\begin{minipage}{\linewidth}
        \captionof{table}{Ablation study of the coefficient matrix for \vmname\ on H3.6M \cite{h36m_pami} dataset. ``fixed'' means using the fixed coefficient matrix $\widetilde{\mathbf{A}}^{sym}$ to reconstruct the mesh. }
        \label{tab:quan_bm_nr_effect}
        \setlength{\tabcolsep}{5pt}
        \renewcommand\arraystretch{1.35}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{l | c c |c c}
            \hline 
            Method & Fixed $\widetilde{\mathbf{A}}^{sym}$ & Updated $\Aest$ & MPVE$\downarrow$ & MPJPE$\downarrow$ \\
            \hline
            (a) \vmname\ (fixed) & \cmark &  & 64.7 & 51.6 \\
            \rowcolor{mygray}
            (b) \vmname  & & \cmark  & \textbf{58.0} & \textbf{47.3} \\
            \hline 
        \end{tabular}}
        \vspace{0.8em}
    
        \includegraphics[width=\linewidth]{imgs/experiments/blending_diff.pdf}
        \captionof{figure}{Mesh estimation comparison results when using (a) fixed coefficient matrix $\widetilde{\mathbf{A}}^{sym}$, and (b) updated $\Aest$ in our method \vmname. Please zoom in to better see the details.}
        \label{fig:blending_nr_diff}
    \end{minipage}
\end{figure}


\begin{table}[t]
\center
\caption{Ablation study of the architecture design for the probabilistic method \vmproname\ on 3DPW \cite{vonMarcard2018} dataset. (a-b) ablates the use of 2D feature $\feat$ and 2D VMs $\vmestuv$ in denoiser network, respectively. Ablation (c) removes the 2D VM $\vmestuv$ sampling in 2D feature $\feat$ and directly transforms the global 2D feature $\feat$ to $\featimg$ for the denoiser. In ablation (d), the denoiser directly estimates the noise $\noiseest$ following the original formula \cite{ho2020denoising}, while keeping the rest design the same. (e) is the proposed \vmproname\ framework.}
\label{tab:vmpro_ablate}
\setlength{\tabcolsep}{3pt}
\renewcommand\arraystretch{1.35}
\resizebox{\linewidth}{!}{
\begin{tabular}{l | c c c | c c}
    \hline 
    \multirow{2}{*}{Method} & 2D feature  & 2D VM & Regress & \multirow{2}{*}{MPVE$\downarrow$} & \multirow{2}{*}{MPJPE$\downarrow$} \\
      &  $\feat$ &  $\vmestuv$ &  $\hmapestuv$ &  &  \\
    \hline
    (a) \textit{w/o} $\feat$ & & \cmark & \cmark & 80.1 & 68.2 \\
    (b) \textit{w/o} $\vmestuv$ & \cmark & & \cmark & 80.6 & 68.8 \\
    (c) \textit{w/o} sampling in $\feat$ & \cmark & \cmark & \cmark & 80.4 & 69.2 \\
    (d) $\rightarrow \noiseest$ & \cmark & \cmark & \cmark & 80.6 & 68.9 \\
    \rowcolor{mygray}
    (e) \vmproname  & \cmark & \cmark & \cmark & \textbf{76.9} & \textbf{66.1} \\
    \hline 
\end{tabular}}
\end{table}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/experiments/design.pdf}
	\caption{Mesh estimation comparison results of different ablated methods of \vmproname. (a) We ablate the usage of 2D image features $\feat$. (b) We ablate the usage of 2D estimated VM $\vmestuv$. (c) We remove the 2D VM $\vmestuv$ sampling in 2D feature $\feat$ and directly transform the global 2D feature $\feat$ to $\featimg$ for the denoiser. (d) We change the estimation target of the denoiser to the noise $\noiseest$ instead of the signal itself. (e) is our proposed \vmproname\ method. We show the GT mesh and its side view on the far right. For baselines (a-e), we show each hypothesis that is denoised from a zero noise for a fair comparison. Each hypothesis presents a projecting view and a side view.}
	\label{fig:vmpro_ablate}
\end{figure*}

\subsection{Ablation Study on \vmproname}
\noindent\textbf{Architecture design of the denoiser.}
Table \ref{tab:vmpro_ablate} compares \vmproname\ to four baselines. We report the metric when predicting $S=10$ hypotheses. In baseline (a) and (b), we remove the 2D feature $\feat$ and 2D estimated VMs $\vmestuv$ input for the denoiser, respectively. It can be seen that the errors increase a lot compared to the full model, which validates the effectiveness of the two guidance. In baseline (c), we remove the local sampling by 2D VMs $\vmestuv$ on 2D feature $\feat$, \ie the purple arrow in Figure \ref{fig:diff_model}. Instead, we directly transform the global 2D image feature $\feat$ to $\featimg$ when feeding into the denoiser and the performance degrades notably. In ablation study (d), we alter the denoiser to estimate the noise $\noiseest$ rather than the clean 3D VMs $\vmest(0)$, adjusting the inference process to align with the original denoising formulation by Ho \etal \cite{ho2020denoising}. This modification results in a decrease in performance. A similar observation was made in previous research by Tevet \etal \cite{tevet2023human}, where it is suggested that noise estimation might require substantially larger batch sizes to be effective. Unfortunately, we lack the resources to confirm this hypothesis in our settings. 

Figure \ref{fig:vmpro_ablate} presents the qualitative comparison of different designs. As observed in the image, the left arm of the subject is nearly fully extended. However, due to depth ambiguity, the estimations from baselines (a-d) align with the 2D perspective, yet inaccuracies arise in the 3D estimation of the left forearm when viewed from the side view. Our method (e), which integrates 2D local features, more accurately captures precise 3D poses, achieving estimations that most closely match the GT. This highlights the significance of our design, which not only utilizes 2D local features $\feat$ but also leverages 2D VMs $\vmestuv$ to locally sample these features effectively.\\




\begin{figure}[t]
    \noindent\begin{minipage}{\linewidth}
        \captionof{table}{Results of using different denoising steps $T$ for \vmproname\ on H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} datasets.}
        \label{tab:denoising_step}
        \resizebox{\linewidth}{!}{
        \centering
        \setlength{\tabcolsep}{8pt}
        \begin{tabular}{c | c  c | c  c  | c}
            \hline 
            \multirow{2}{*}{Steps $T$} & \multicolumn{2}{c|}{H3.6M}  & \multicolumn{2}{c|}{3DPW} & \multirow{2}{*}{FPS$\uparrow$} \\
        
            \cline{2-3} \cline{4-5}
            & MPVE$\downarrow$ & MPJPE$\downarrow$ & MPVE$\downarrow$ & MPJPE$\downarrow$ &\\
            \hline
            1 & 386.5 & 370.4  & 372.2 & 358.9 & 339.6 \\
            2 & 77.0 & 66.2 & 104.7 & 92.6 & 326.6 \\
            4 & 58.8 & 48.3 & 80.0 & 69.2 & 301.0 \\
            5 & 57.1 & 46.9 & 78.3 & 67.6 & 292.6 \\
            10 & 55.8 & 45.4 & 77.3 & 66.6  & 247.4 \\
            15 & 55.5 & 45.0 & 76.4 & 65.5  & 208.0 \\
            20 & 55.8 & 45.3 & 76.8 & 65.9 & 189.0 \\
            100 & 55.6 & 45.2 & 76.9 & 65.9 & 66.4 \\
            \hline 
        \end{tabular}}
        \vspace{0.8em}
    
        \includegraphics[width=\linewidth]{imgs/experiments/stepsT.pdf}
        \captionof{figure}{Mesh estimation results of using different denoising steps $T$ for \vmproname\ on a sample data from H3.6M \cite{h36m_pami} dataset. Each hypothesis is denoised from a zero initial noise for a fair comparison.}
        \label{fig:denoising_step}
    \end{minipage}
\end{figure}

\noindent\textbf{Denoising steps.}
Table \ref{tab:denoising_step} shows the metrics on the H3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} test sets when using different numbers of denoising steps $T$. We report the metrics of predicting $S=10$ hypotheses. We also report the speed of the whole model inferring a hypothesis on an NVIDIA A100-PCIE-80GB GPU. It can be seen that using only $T=1$ denoising step results in significant errors, and the estimated mesh is not human-shaped, as shown in Figure \ref{fig:denoising_step}. As the number of denoising steps $T$ increases, the accuracy of the estimated mesh gradually improves, and the error decreases. When $T>=10$, the accuracy of the mesh estimation stabilizes, and further steps yield only marginal improvements, as validated by the marginal reduction in error and the indistinguishable differences in visualization. Therefore, based on the acceleration of DDIM \cite{song2021denoising}, we choose $T=10$ steps for a good balance between estimation performance and inference speed. \\


\begin{table}[t]
\center
\caption{Robustness to occlusion for \vmname\ and \vmproname\ on 3DPW \cite{vonMarcard2018} test set when different body parts are occluded. We report MPVE error.
}
\label{tab:ablate_occ}
\setlength{\tabcolsep}{15pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c | c | c  c }
    \hline 
    \multirow{2}{*}{Occ. Parts} & \multirow{2}{*}{\vmname}  & \multicolumn{2}{c}{\vmproname} \\

    \cline{3-4}
    & & $\hyponum=10$ & $\hyponum=100$ \\
    \hline
    \rowcolor{mygray}
    None & 77.9 & 77.3 & 67.4 \\
    2 Arms & 79.2 & 78.7 & 68.8 \\
    2 Legs & 78.3 & 77.6  & 67.8 \\
    Body & 78.6 & 78.1 & 68.2 \\
    Random & 78.7 & 77.9 & 68.4 \\
    \hline 
\end{tabular}}
\end{table}
\vspace{-2em}





\begin{figure*}[ht]
	\centering
	\includegraphics[width=\linewidth]{imgs/experiments/quality_result.pdf}
	\caption{ \textbf{Top:} Meshes estimated by our deterministic approach \vmname\ on images from 3DPW test set. The rightmost case in the dashed box shows a typical failure. \textbf{Bottom:} Meshes estimated by \vmname\ on Internet images with challenging cases (extreme shapes or in a long dress).}
	\label{fig:quality_result_w_failure}
\end{figure*}




\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{imgs/experiments/locality.pdf}
	\caption{(a) For each virtual marker (represented by a star), we highlight the top 30 most affected vertices (represented by a colored dot) based on average coefficient matrix $\Aest$. (b) For each vertex (dot), we highlight the top 3 virtual markers (star) that contribute the most. The dependency has a strong locality which improves the robustness when some virtual markers cannot be accurately detected. }
	\label{fig:locality}
\end{figure}



\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{imgs/experiments/3dpw_det_pro.pdf}
	\caption{Qualitative comparison of \vmname\ and \vmproname\ on 3DPW \cite{vonMarcard2018} testset. \vmname\ may fail when facing ambiguity, while \vmproname\ could generate multiple reasonable solutions. For each mesh estimate, we present both a projected view and a side view in each column. We show three hypotheses generated by \vmproname.}
	\label{fig:3dpw_det_pro}
\end{figure*}


\begin{figure*}[ht!]
	\centering
	\includegraphics[width=0.95\linewidth]{imgs/experiments/quality_vmpro.pdf}
	\caption{Meshes estimated by \vmproname\ on Internet images. For each case in a row, we present three hypotheses, showing both a projected view and a side view in each column. The rightmost column shows the overlapped side views to unveil the subtle differences.}
	\label{fig:quality_result_w_failure_pro}
\end{figure*}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{imgs/experiments/vmpro_failure_latin.pdf}
	\caption{A typical failure case of \vmproname. The heavy occlusion and ambiguity cause the model to incorrectly estimate the right arm and right leg of the male dancer.}
	\label{fig:vmpro_failure}
\end{figure}
\vspace{-0.8em}

\noindent\textbf{Robustness to occlusion.}
\label{subsec:occlusion}
We report results of \vmname\ and \vmproname\ when different image regions of corresponding virtual markers are occluded by a synthetic random mask in Table \ref{tab:ablate_occ}. The errors of \vmname\ are slightly larger than the original image (None). We further analyze how inaccurate virtual markers would affect the mesh estimation, \ie when part of human body is occluded or truncated. According to the finally learned coefficient matrix $\Aest$ of our model \vmname, we highlight the relationship weights among virtual markers and all vertices in Figure \ref{fig:locality}. We can see that our model actually learns \emph{local and sparse} dependency between each vertex and the virtual markers, \eg for each vertex, the virtual markers that contribute the most are in a near range as shown in Figure \ref{fig:locality} (b). Therefore, in inference, if a virtual marker has inaccurate position estimation due to occlusion or truncation, the dependent vertices may have inaccurate estimates, while the rest will be barely affected. 

This locality enhances the robustness of the \vmproname\ method against occlusion as well. When sampling $S=10$ hypotheses, \vmproname\ exhibits a minor increase in error under various circumstances of VM occlusion. 



\subsection{Qualitative Results}
\label{subsec:quality}
\noindent\textbf{\vmname.}
Figure \ref{fig:quality_result_w_failure} (top) presents some meshes estimated by our approach \vmname\ on natural images from the 3DPW test set. The rightmost case shows a typical failure where our method has a wrong pose estimate of the left arm due to heavy occlusion. We can see that the failure is constrained to the local region and the rest of the body still gets accurate estimates. As discussed in Sec. \ref{subsec:occlusion}, we suppose that this is due to the advantages brought by the locality of the virtual marker representation, which can confine erroneous mesh estimates to a local range and thereby enhance robustness.
Figure \ref{fig:body_arche} (right) shows more examples where occlusion or truncation occurs, and our method can still get accurate or reasonable estimates robustly. Note that when truncation occurs, our method still guesses the positions of the truncated virtual markers. 

Figure \ref{fig:quality_result_w_failure} (bottom) shows our estimated meshes on challenging cases, which indicates the strong generalization ability of our model on diverse postures and actions in natural scenes. Please refer to the supplementary for more quality results. Note that since the datasets do not provide supervision of head orientation, face expression, hands, or feet, the estimates of these parts are just in canonical poses inevitably. \\




\noindent\textbf{\vmproname.} 
Figure \ref{fig:3dpw_det_pro} compares the visualization of \vmname\ and \vmproname\ on the 3DPW \cite{vonMarcard2018} test set. We draw three hypotheses generated by \vmproname, depicted in blue. Additionally, we include the results from the deterministic model \vmname\ (green). It is noticeable that, due to self-occlusion or object occlusion, \vmname\ might yield results that seem accurate in 2D projections yet are erroneous in 3D space, \eg the incorrect estimation of the right leg in the second case. This issue stems from the inherent ambiguity associated with occlusions. By leveraging the advantages of the data distribution learned by the denoising model, \vmproname\ can generate more reasonable solutions, such as the naturally standing legs. 

In Figure \ref{fig:quality_result_w_failure_pro}, we show more results of \vmproname\ on unseen Internet images. It can be observed that \vmproname\ possesses strong generalization capabilities, obtaining reasonable 3D estimations when faced with uncertain ambiguities, such as the various plausible poses of the runner's right arm in the second case (as can be seen from the side view). Furthermore, even under complex actions and backgrounds, such as the person climbing in the third image, \vmproname\ also demonstrates robust estimation results.

Figure \ref{fig:vmpro_failure} shows a typical failure case of \vmproname\ on Internet image, where our method fails when heavy occlusion and ambiguity happen. Our method incorrectly estimates the right arm and right leg of the male dancer, which may be addressed using more powerful estimators or more diverse training datasets in the future. 