




\label{sec:method}

In this section, we describe the details of our approach. First, Section \ref{subsec:body_arche} introduces how we learn the virtual marker representation from mocap data. Then we present the overall framework for mesh estimation from an image in Section \ref{subsec:approach}. At last, Section \ref{subsec:training} discusses the loss functions and training details.

\subsection{The virtual marker representation} 
\label{subsec:body_arche}





We represent a mesh by a vector of vertex positions $\mathbf{x} \in \mathbb{R}^{3M}$ where $M$ is the number of mesh vertices. Denote a mocap dataset such as \cite{h36m_pami} with $N$ meshes as $\overset{\frown}{\mathbf{X}} = [\mathbf{x}_1 ,\, ... ,\, \mathbf{x}_N] \in \mathbb{R}^{3M \times N}$. To unveil the latent structure among vertices, we reshape it to $\mathbf{X} \in \mathbb{R}^{3N \times M}$ with each column $\mathbf{x}_i \in \mathbb{R}^{3N}$ representing all possible positions of the $i^{\text{th}}$ vertex in the dataset \cite{h36m_pami}.

The rank of $\mathbf{X}$ is smaller than $M$ because the mesh representation is smooth and redundant where some vertices can be accurately reconstructed by the others. While it seems natural to apply PCA \cite{jolliffe1986principal} to $\mathbf{X}$ to compute the eigenvectors as virtual markers for reconstructing others, there is no guarantee that the virtual markers correspond to the mesh vertices, making them difficult to be detected from images. Instead, we aim to learn $K$ virtual markers $\mathbf{Z} = [\mathbf{z}_1, ..., \mathbf{z}_K] \in \mathbb{R}^{3N \times K}$ that try to satisfy the following two requirements to the greatest extent. First, they can accurately reconstruct the intact mesh $\mathbf{X}$ by their linear combinations: $\mathbf{X}=\mathbf{Z} \mathbf{A}$, where $\mathbf{A} \in \mathbb{R}^{K \times M}$ is a coefficient matrix that encodes the spatial relationship between the virtual markers and the mesh vertices. Second, they should have distinguishable visual patterns in images so that they can be easily detected from images. Ideally, they can be on the body surface as the meshes. 

We apply archetypal analysis \cite{cutler1994archetypal,chen2014fast} to learn $\mathbf{Z}$ by minimizing a reconstruction error with two additional constraints: (1) each vertex $\mathbf{x}_i$ can be reconstructed by convex combinations of $\mathbf{Z}$, and (2) each marker $\mathbf{z}_i$ should be convex combinations of the mesh vertices $\mathbf{X}$: 
\begin{equation}
\label{eq:aa}
\min_{\substack{\bm{\alpha}_i \in {\Delta}_K \, for \, 1\leq i\leq M, \\ 
                \bm{\beta}_j \in {\Delta}_M \, for \, 1\leq j\leq K}} ||\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{A}||^2_F, \\
\end{equation}
where $\mathbf{A} = [\bm{\alpha}_1, ..., \bm{\alpha}_M] \in \mathbb{R}^{K \times M}$, each $\bm{\alpha}$ resides in the simplex ${\Delta}_K \triangleq \{\bm{\alpha} \in \mathbb{R}^{K} \, \mathrm{s.t.} \, \bm{\alpha} \succeq 0 \, \text{and} \, {||\bm{\alpha}||}_1 = 1\}$, and $\mathbf{B} = [\bm{\beta}_1, ..., \bm{\beta}_K] \in \mathbb{R}^{M \times K}$, $\bm{\beta}_j \in {\Delta}_M$. We adopt Active-set algorithm \cite{chen2014fast} to solve objective (\ref{eq:aa}) and obtain the learned virtual markers $\mathbf{Z} = \mathbf{X}\mathbf{B} \in \mathbb{R}^{3N \times K}$.  As shown in \cite{cutler1994archetypal,chen2014fast}, the two constraints encourage the virtual markers $\mathbf{Z}$ to unveil the latent structure among vertices, therefore they learn to be close to the extreme points of the mesh and located on the body surface as much as possible. \\
% Figure \ref{fig:body_arche} shows the learned virtual markers.




\begin{figure*}[t]
	\centering
	\includegraphics[width=6.3in]{imgs/pipeline.pdf}
	\caption{Overview of our framework. Given an input image $\mathbf{I}$, it first estimates the 3D positions $\hat{\mathbf{P}}$ of the virtual markers. Then we update the coefficient matrix $\hat{\mathbf{A}}$ based on the estimation confidence scores $\mathbf{C}$ of the virtual markers. Finally, the complete human mesh can be simply recovered by linear multiplication $\hat{\mathbf{M}} = \hat{\mathbf{P}}\hat{\mathbf{A}}$. }
	\label{fig:pipeline}
    \vspace{-0.4cm}
\end{figure*}

\begin{table}[t]
    \centering
    \resizebox{3.0in}{!}{
    \begin{tabular}{l|l|c} 
    \hline 
    Type & Formula & Reconst. Error (mm) $\downarrow$  \\
    \hline  
    Original & $||\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{A}||^2_F$ & 11.67 \\
    Symmetric & $||\mathbf{X} - \mathbf{X}\widetilde{\mathbf{B}}^{sym}\widetilde{\mathbf{A}}^{sym}||^2_F$ & 10.98 \\
    \hline
    \end{tabular}}
    \caption{The reconstruction errors using the original and the symmetric sets of markers on the H3.6M dataset \cite{h36m_pami}, respectively. The errors are small indicating that they are sufficiently expressive and can reconstruct all vertices accurately. }
    \label{tab:comparison_sym_arche}
    \vspace{-0.4cm}
\end{table}

\noindent\textbf{Post-processing.}
Since human body is left-right symmetric, we adjust $\mathbf{Z}$ to reflect the property. We first replace each $\mathbf{z}_i \in \mathbf{Z}$ by its nearest vertex on the mesh and obtain $\widetilde{\mathbf{Z}} \in \mathbb{R}^{3 \times K}$. This step allows us to compute the left or right counterpart of each marker. Then we replace the markers in the right body with the symmetric vertices in the left body and obtain the symmetric markers $\widetilde{\mathbf{Z}}^{sym} \in \mathbb{R}^{3 \times K}$. Finally we update $\mathbf{B}$ and $\mathbf{A}$ by minimizing $||\mathbf{X} - \mathbf{X}\widetilde{\mathbf{B}}^{sym}\widetilde{\mathbf{A}}^{sym}||^2_F$ subject to $\widetilde{\mathbf{Z}}^{sym} = \mathbf{X} \widetilde{\mathbf{B}}^{sym}$. 
More details are elaborated in the supplementary. 


Figure \ref{fig:body_arche} shows the virtual markers learned on the mocap dataset \cite{h36m_pami} after post-processing. They are similar to the physical markers and approximately outline the body shape which agrees with our expectations. They are roughly evenly distributed on the surface of the body, and some of them are located close to the body keypoints, which have distinguishable visual patterns to be accurately detected. Table \ref{tab:comparison_sym_arche} shows the reconstruction errors of using original markers $\mathbf{X}\mathbf{B}$ and the symmetric markers $\mathbf{X}\widetilde{\mathbf{B}}^{sym}$. Both can reconstruct meshes accurately. 

\subsection{Mesh estimation framework}
\label{subsec:approach}

On top of the virtual markers, we present a simple yet effective framework for end-to-end 3D human mesh estimation from a single image. As shown in Figure \ref{fig:pipeline}, it consists of two branches. The first branch uses a volumetric CNN \cite{sun2018integral} to estimate the 3D positions $\hat{\mathbf{P}}$ of the markers, and the second branch reconstructs the full mesh $\hat{\mathbf{M}}$ by predicting a coefficient matrix $\hat{\mathbf{A}}$:
\vspace{-0.2cm}
\begin{equation}
\small
\hat{\mathbf{M}} = \hat{\mathbf{P}}\hat{\mathbf{A}}.
\end{equation}
We will describe the two branches in more detail. \\

\noindent\textbf{3D marker estimation.}
We train a neural network to estimate a 3D heatmap $\hat{\mathbf{H}} = [\hat{\mathbf{H}}_1 ,\, ... ,\, \hat{\mathbf{H}}_{K}] \in \mathbb{R}^{K \times D \times H \times W}$ from an image. The heatmap encodes per-voxel likelihood of each marker. There are $D \times H \times W$ voxels in total which are used to discretize the 3D space. The 3D position $\hat{\mathbf{P}}_z \in \mathbb{R}^{3}$ of each marker is computed as the center of mass of the corresponding heatmap $\hat{\mathbf{H}}_z$ \cite{sun2018integral} as follows:
\begin{equation}
\small
    \hat{\mathbf{P}}_z = \sum_{d=1}^{D}\sum_{h=1}^{H}\sum_{w=1}^{W}(d, h, w) \cdot \hat{\mathbf{H}}_z(d, h, w).
\end{equation}
The positions of all markers are represented as $\hat{\mathbf{P}} = [\hat{\mathbf{P}}_1, \hat{\mathbf{P}}_2, \cdots, \hat{\mathbf{P}}_K]$. \\


\noindent\textbf{Interpolation.} 
Ideally, if we have accurate estimates for all virtual markers $\hat{\mathbf{P}}$, then we can recover the complete mesh by simply multiplying $\hat{\mathbf{P}}$ with a fixed coefficient matrix $\widetilde{\mathbf{A}}^{sym}$ with sufficient accuracy as validated in Table \ref{tab:comparison_sym_arche}. However, in practice, some markers may have large estimation errors because they may be occluded in the monocular setting. Note that this happens frequently. For example, the markers in the back will be occluded when a person is facing the camera. As a result, inaccurate markers positions may bring large errors to the final mesh if we directly multiply them with the fixed matrix $\widetilde{\mathbf{A}}^{sym}$. 


Our solution is to rely more on those accurately detected markers. To that end, we propose to update the coefficient matrix based on the estimation confidence scores of the markers. In practice, we simply take the heatmap score at the estimated positions of each marker, \ie $\hat{\mathbf{H}}_z(\hat{\mathbf{P}}_z)$, and feed them to a single fully-connected layer to obtain the coefficient matrix $\hat{\mathbf{A}}$. Then the mesh is reconstructed by $\hat{\mathbf{M}} = \hat{\mathbf{P}}\hat{\mathbf{A}}$.




\subsection{Training}
\label{subsec:training}
We train the whole network end-to-end in a supervised way. 
% The 3D GT meshes are obtained by applying SMPLify-X \cite{pavlakos2019expressive} to images. 
The overall loss function is defined as:
\begin{equation}
\small
\label{eq:total_loss}
\begin{aligned}
    \mathcal{L} = \lambda_{vm}\mathcal{L}_{vm} + \lambda_{c}\mathcal{L}_{conf} + \lambda_{m}\mathcal{L}_{mesh}.
\end{aligned}
\end{equation}

\noindent\textbf{Virtual marker loss.}
We define $\mathcal{L}_{vm}$ as the $L_1$ distance between the predicted 3D virtual markers $\hat{\mathbf{P}}$ and the GT $\hat{\mathbf{P}}^{*}$ as follows:
\begin{equation}
\small
\label{eq:loss_pose}
\begin{aligned}
    \mathcal{L}_{vm} = \| \hat{\mathbf{P}}-\hat{\mathbf{P}}^{*} \|_1.
\end{aligned}
\end{equation}
Note that it is easy to get GT markers $\hat{\mathbf{P}}^{*}$ from GT meshes as stated in Section \ref{subsec:body_arche} without additional manual annotations. \\

\noindent\textbf{Confidence loss.}
We also require that the 3D heatmaps have reasonable shapes, therefore, the heatmap score at the voxel containing the GT marker position $\hat{\mathbf{P}}_z^{*}$ should have the maximum value as in the previous work \cite{iskakov2019learnable}:
\begin{equation}
\small
\label{eq:loss_conf}
\begin{aligned}
    \mathcal{L}_{conf} = - \sum_{z=1}^{K} log(\hat{\mathbf{H}}_z(\hat{\mathbf{P}}_z^{*})).
\end{aligned}
\end{equation}

\noindent\textbf{Mesh loss.}
Following \cite{moon2020i2l}, we define $\mathcal{L}_{mesh}$ as a weighted sum of four losses:
\begin{equation}
\label{eq:mesh_loss}
\begin{aligned}
\small
    \mathcal{L}_{mesh} = \mathcal{L}_{vertex} + \mathcal{L}_{pose} + \mathcal{L}_{normal} + \lambda_{e}\mathcal{L}_{edge}.
\end{aligned}
\end{equation}
\begin{itemize}
    \item[--] \textbf{Vertex coordinate loss.} We adopt $L_1$ loss between predicted 3D mesh coordinates $\hat{\mathbf{M}}$ with GT mesh $\hat{\mathbf{M}}^{*}$ as:
        \begin{equation}
        \small
        \label{eq:loss_vertex}
        \begin{aligned}
            \mathcal{L}_{vertex} = \| \hat{\mathbf{M}}-\hat{\mathbf{M}}^{*} \|_1.
        \end{aligned}
        \end{equation}
    \item[--] \textbf{Pose loss.} We use $L_1$ loss between the 3D landmark joints regressed from mesh $\hat{\mathbf{M}}\mathcal{J}$ and the GT joints $\hat{\mathbf{J}}^{*}$ as:
        \begin{equation}
        \small
        \label{eq:loss_reg}
        \begin{aligned}
            \mathcal{L}_{pose} = \| \hat{\mathbf{M}}\mathcal{J}-\hat{\mathbf{J}}^{*} \|_1,
        \end{aligned}
        \end{equation}
    where $\mathcal{J} \in \mathbb{R}^{M \times J}$ is a pre-defined joint regression matrix in SMPL model \cite{bogo2016keep}.

    \item[--] \textbf{Surface losses.} To improve surface smoothness \cite{wang2018pixel2mesh}, we supervise the normal vector of a triangle face with GT normal vectors by $\mathcal{L}_{normal}$ and the edge length of the predicted mesh with GT length by $\mathcal{L}_{edge}$:
        \begin{equation}
        \small
        \label{eq:loss_surface}
        \begin{aligned}
            \small
            & \mathcal{L}_{normal} = \sum_{f} \sum_{\{i, j\} \subset f}  \left| \left< \frac{\hat{\mathbf{M}}_i - \hat{\mathbf{M}}_j}{\| \hat{\mathbf{M}}_i - \hat{\mathbf{M}}_j \|_2}, \hat{\mathbf{n}}_f^{*}\right> \right|, \\
            \small
            & \mathcal{L}_{edge} = \sum_{f} \sum_{\{i, j\} \subset f}  \left|\| \hat{\mathbf{M}}_i - \hat{\mathbf{M}}_j \|_2 - \| \hat{\mathbf{M}}_i^{*} - \hat{\mathbf{M}}_j^{*} \|_2 \right|.
        \end{aligned}
        \end{equation}
    where $f$ and $\hat{\mathbf{n}}_f^{*}$ denote a triangle face in the mesh and its GT unit normal vector, respectively. $\hat{\mathbf{M}}_i$ denote the $i^{th}$ vertex of $\hat{\mathbf{M}}$. $^{*}$ denotes GT.
\end{itemize}



