

We elaborate on the post-processing implementation of the virtual markers and provide additional experimental details and results. At last, we discuss data from human subjects and the potential societal impact.

\subsection*{A. \quad Post-processing on Virtual Markers}
\label{sec:post}


As described in Section \ref{subsec:body_arche}, considering the left-right symmetric human body structure, we slightly adjust the learned virtual markers $\mathbf{Z}$ to be symmetric. In fact, after the first step that updates each $\mathbf{z}_i$ by its nearest vertex to get $\widetilde{\mathbf{Z}} \in \mathbb{R}^{3 \times K}$. $\widetilde{\mathbf{Z}}$ are almost symmetric with few exceptions. To get the final symmetric virtual markers $\widetilde{\mathbf{Z}}^{sym} \in \mathbb{R}^{3 \times K}$, for each virtual marker located in the left body part, we take its symmetric vertex in the right body to be its symmetric counterpart. 

Since the human mesh (\ie SMPL \cite{loper2015smpl}) itself is not strictly symmetric, we clarify the \emph{symmetric vertex pair} (\eg left elbow and right elbow) on a human mesh template $\mathbf{X}^t \in \mathbb{R}^{3 \times M}$ in Figure \ref{fig:supp_mesh}. We place $\mathbf{X}^{t}$ at the origin of the 3D coordinate system. Formally, we define the cost of matching $i^{th}$ vertex to $j^{th}$ vertex to be $\bm{C}_{i, j} = \left|x_i + x_j\right| + \left|y_i - y_j\right| + \left|z_i - z_j\right|$. A symmetric vertex pair $(\mathbf{X}^t_{i}, \mathbf{X}^t_{j})$ is defined to have the minimal cost $\bm{C}_{i, j}$. In this way, for each virtual marker in the left body, we take its symmetric vertex counterpart to be its symmetric virtual marker and finally get $\widetilde{\mathbf{Z}}^{sym}$.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.8in]{imgs/supp/mesh_template.pdf}
    \caption{Illustration of the human mesh template $\mathbf{X}^{t}$ at the 3D coordinate system and a symmetric vertex pair $(\mathbf{X}^t_{i}, \mathbf{X}^t_{j})$.}
    \label{fig:supp_mesh}
\end{figure}




\subsection*{B. \quad Experiments}
\label{sec:supp_experiments}

In this section, we first add detailed descriptions for datasets and then provide more experimental results of our approach.

\subsubsection*{B.1 \quad Datasets}





\paragraph{H3.6M \cite{h36m_pami}.} Following previous works \cite{kanazawa2018end, kolotouros2019learning, kolotouros2019convolutional, pavlakos2018learning}, we use the SMPL parameters generated from MoSh \cite{loper2014mosh}, which are fitted to the 3D physical marker locations, to get the GT 3D mesh supervision. Following standard practice \cite{kanazawa2018end}, we evaluate the quality of 3D pose of $14$ joints derived from the estimated mesh, \ie $\hat{\mathbf{M}}\mathcal{J}$. We report Mean Per Joint Position Error (MPJPE) and PA-MPJPE in millimeters (mm). The latter uses Procrustes algorithm \cite{gower1975generalized} to align the estimates to GT poses before computing MPJPE. To evaluate mesh estimation results, we also report Mean Per Vertex Error (MPVE) which can be interpreted as MPJPE computed over the whole mesh.

\paragraph{3DPW \cite{vonMarcard2018}.} The 3D GT SMPL parameters are obtained by using the data from IMUs when collected. Following the previous works \cite{lin2021end, Lin_2021_ICCV, Kocabas_2021_ICCV, zanfir2021thundr}, we use the train set of 3DPW to learn the model and evaluate on the test set.

\paragraph{MPI-INF-3DHP \cite{mehta2017monocular}} is a 3D pose dataset with 3D GT pose annotations. Since this dataset does not provide 3D mesh annotations, following \cite{kanazawa2018end, kolotouros2019learning}, we only enforce supervision on the 3D skeletons (Eq. (\ref{eq:loss_reg})) in mesh losses.

\paragraph{UP-3D \cite{lassner2017unite}} is a wild 2D pose dataset with natural images. The 3D poses and meshes are obtained by SMPLify \cite{bogo2016keep}. Due to the lack of GT 3D poses, the fitted meshes are not accurate. Therefore we only use the 2D annotations to train the 3D virtual marker estimation network as in \cite{sun2018integral}.  

\paragraph{COCO \cite{lin2014microsoft}} is a large wild 2D pose dataset with natural images. Previous work \cite{moon2020i2l} used SMPLify-X \cite{pavlakos2019expressive} to obtain pseudo SMPL mesh annotations but they are not accurate. However, we find that if we project the 3D mesh to 2D image, the resulting 2D mesh vertices align well with the image. So we leverage the 2D annotations to train the virtual marker estimation network as in \cite{sun2018integral}. 


\paragraph{SURREAL \cite{varol2017learning}} is a large-scale synthetic dataset containing 6 million frames of synthetic humans. The images are photo-realistic renderings of people under large variations in shape, texture, viewpoint, and body pose. To ensure realism, the synthetic bodies are created using the SMPL body model, whose parameters are fit by the MoSh \cite{loper2014mosh} given raw 3D physical marker data. All the images have a resolution of $320 \times 240$. We use the same training split to train the model and evaluate the test split following \cite{choi2020pose2mesh}.

\subsubsection*{B.2 \quad Implementation Details and Computation Resource}
Following common practice \cite{kanazawa2018end, choi2020pose2mesh, moon2020i2l, zanfir2021thundr, kolotouros2019convolutional, kocabas2020vibe, Lin_2021_ICCV, lin2021end}, we conduct mix-training by using the above 2D and 3D datasets for experiments on the H3.6M and 3DPW datasets. 
To leverage the 3D pose estimation dataset, \ie MPI-INF-3DHP \cite{mehta2017monocular}, we extend the $64$ virtual markers with the $17$ landmark joints (\ie skeleton) from the MPI-INF-3DHP dataset. 
For experiments on the SURREAL dataset, we use its training set alone as in \cite{choi2020pose2mesh, luan2021pc}. We implement the proposed method with PyTorch. All the experiments are conducted on a Linux machine with 4 NVIDIA 16GB V100 GPUs. The whole network is trained for 40 epochs with batch size 32 using Adam \cite{kingma2015adam} optimizer.

We evaluate the model complexity in terms of FLOPs (G) and the number of model parameters in Table \ref{tab:computation}. Compared to the most recent state-of-the-art methods that directly regress \textit{all mesh vertices}, such as I2L-MeshNet \cite{moon2020i2l}, METRO \cite{lin2021end}, and Mesh Graphormer \cite{Lin_2021_ICCV}, our approach with virtual marker representation reduces the computation overhead by a large margin while getting better estimation quality. The last column shows the MPVE errors on 3DPW test set for performance reference.



\begin{table}[t]
\center
\small
\setlength{\tabcolsep}{4pt}
\resizebox{3.2in}{!}{
\begin{tabular}{l | c  c | c}
    \hline 
    Methods & FLOPs (G) $\downarrow$ & Params (M) & MPVE$\downarrow$\\

    \hline
    I2L-MeshNet \cite{moon2020i2l} ECCV'20 & 28.7 & 141.2 & 110.1\\ 
    METRO \cite{lin2021end} CVPR'21 & 153.0 & 397.5 & 88.2\\ 
    Mesh Graphormer \cite{Lin_2021_ICCV} ICCV'21 & 48.8 & 180.6 & 87.7 \\ 
    \rowcolor{mygray}
    \textbf{Ours} & \textbf{22.1} & \textbf{109.6} & \textbf{77.9} \\ 
    \hline 
\end{tabular}}
\caption{Computation overhead comparison with the recent state-of-the-art methods that directly regress \textit{all 3D vertices}. The rightmost column shows the MPVE errors on the 3DPW test set for performance reference.}
\label{tab:computation}
\end{table}


\begin{table}[t]
\center
\small
\setlength{\tabcolsep}{4pt}
\resizebox{3.2in}{!}{
\begin{tabular}{l | c c c c c}
    \hline
     & \textbf{Ours} & \textit{w/o} $\mathcal{L}_{conf}$ & \textit{w/o} $\mathcal{L}_{pose}$ & \textit{w/o} $\mathcal{L}_{normal}$ & \textit{w/o} $\mathcal{L}_{edge}$ \\
    \hline
    MPVE$\downarrow$ & \textbf{58.0} & 59.2 & 58.3 & 60.6 & 60.4 \\
    
    \hline 
\end{tabular}}
\caption{MPVE errors on H3.6M \cite{h36m_pami} test set when ablating different loss terms.}
\label{tab:ablation_loss}
\end{table}


\begin{table}
    \centering
    \setlength{\tabcolsep}{12pt}
    \resizebox{3.2in}{!}{
    \begin{tabular}{l | c c c}
    \hline
    \multirow{2}{*}{Occ. VM Parts} & \multirow{2}{*}{MPVE$\downarrow$} & \multirow{2}{*}{MPJPE$\downarrow$} & \multirow{2}{*}{PA-MPJPE$\downarrow$} \\
    &  &  &  \\
    \hline
    \rowcolor{mygray}
    None (Ours) & \textbf{77.9} & \textbf{67.5} & \textbf{41.3} \\
    2 Arms & 79.2 \diff{$\uparrow$ 1.3} & 68.2 \diff{$\uparrow$ 0.7} & 42.2 \diff{$\uparrow$ 0.9} \\
    2 Legs & 78.3 \diff{$\uparrow$ 0.4} & 67.9 \diff{$\uparrow$ 0.4} & 41.7 \diff{$\uparrow$ 0.4} \\
    Body & 78.6 \diff{$\uparrow$ 0.7} & 68.0 \diff{$\uparrow$ 0.5} & 41.8 \diff{$\uparrow$ 0.5} \\
    Random & 78.7 \diff{$\uparrow$ 0.8} & 68.1 \diff{$\uparrow$ 0.6} & 41.9 \diff{$\uparrow$ 0.6} \\
    
    \hline 
\end{tabular}}
\caption{Results on 3DPW \cite{vonMarcard2018} test set when different parts of virtual markers (VM) are occluded.}
\label{tab:occl}
\end{table}

\subsubsection*{B.3 \quad Additional Quantitative Results}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{imgs/supp/supp_wild.pdf}
    \caption{Meshes estimated by our approach on Internet images with challenging cases (complex poses or extreme body shapes).}
    \label{fig:supp_wild}
\end{figure*}


\paragraph{Different loss terms.}
Table \ref{tab:ablation_loss} reports the MPVE error on H3.6M \cite{h36m_pami} test set when ablating different loss terms. The confidence loss \cite{iskakov2019learnable} is used to encourage the interpretability of the heatmaps to have a maxima response at the GT position. Without the confidence loss, the error increases slightly. If ablating the surface losses, MPVE increases a lot, which demonstrates the smoothing effect of these two terms. 

\paragraph{Robustness to occlusion.}
We report results when different virtual markers are occluded by a synthetic mask in Table \ref{tab:occl}. The errors are slightly larger than the original image (None), which validates the effectiveness of the \textit{locality} of the virtual marker representation. Occluding arm regions results in a larger error increase. This may be because the arm has larger variations in the dataset. 

\paragraph{Comparison to fitting.}
In order to disentangle the ability of mesh regression from markers using $\hat{\mathbf{A}}$ with the ability to detect the virtual markers accurately from images, we first compute the estimation errors of the virtual markers. The MPJPE over all the virtual markers is $35.5$mm, which demonstrates that these virtual markers can be accurately detected from the images. 
We then fit the mesh model parameters to these virtual markers.
% by using Adam \cite{kingma2015adam} optimizer with an initial learning rate $0.1$, and decay by half with a multi-step scheduler. 
% We optimize each test sample for $25,000$ steps and an early stop is used. 
Table \ref{tab:fitting} shows the metrics of the fitted mesh on the SURREAL \cite{varol2017learning} test set. As we can see, the fitted mesh has a similar error as our regression ones which uses the interpolation matrix $\hat{\mathbf{A}}$, which validates the accuracy of the estimated virtual markers.











\subsubsection*{B.4 \quad Additional Qualitative Results}
Figure \ref{fig:supp_surreal} shows more qualitative comparisons with Pose2Mesh \cite{choi2020pose2mesh} on the SURREAL test set in which has diverse body shapes. The skeleton representation used in Pose2Mesh loses the body shape information so the method \cite{choi2020pose2mesh} can only recover mean shapes. For example, in Figure \ref{fig:supp_surreal} (d) (e), the estimated meshes of Pose2Mesh tend to have the average body shape and fail to estimate the real body shape, regardless of whether the person is slim or stout. This is caused by the limited skeleton representation bottleneck so that the model learns a mean shape for the whole training dataset implicitly. In contrast, our approach with virtual marker representation generates more accurate mesh estimation results.  

\begin{table}
    \centering
    \setlength{\tabcolsep}{16pt}
    \resizebox{3.2in}{!}{
    \begin{tabular}{l | c c c}
    \hline
    \multirow{2}{*}{Method} & \multirow{2}{*}{MPVE$\downarrow$} & \multirow{2}{*}{MPJPE$\downarrow$} & \multirow{2}{*}{PA-MPJPE$\downarrow$} \\
    &  &  &  \\
    \hline
    Fitting & 44.6 & 34.8 & 29.5  \\
    \rowcolor{mygray}
    Ours & 44.7 & 36.9 & 28.9 \\
    
    \hline 
\end{tabular}}
\caption{Results on SURREAL \cite{varol2017learning} test set when the mesh is obtained by fitting to the estimated virtual markers.}
\label{tab:fitting}
% \vspace{-0.4cm}
\end{table}




Figure \ref{fig:supp_pw3d} shows more qualitative comparisons with Pose2Mesh \cite{choi2020pose2mesh} and METRO \cite{lin2021end} on the 3DPW test set. Pose2Mesh and METRO use the skeleton or all 3D vertices as intermediate representations, respectively. The estimated meshes are overlaid on the images according to the camera parameters. Pose2Mesh \cite{choi2020pose2mesh} has difficulty in estimating correct body pose and shapes when truncation occurs (a) or in complex postures (c). The results of METRO \cite{lin2021end} have many artifacts where the estimated mesh is not smooth, and they also fail to align the image well.  In contrast, our method estimates more accurate human poses and shapes and has smooth human mesh results. In addition, it is more robust to truncation and occlusion and aligns the image better.

\begin{figure}[t]
    \centering
    \includegraphics[width=3.3in]{imgs/supp/supp_failure.pdf}
    \caption{Typical failure cases. \textbf{(a)} The right arm has inaccurate shape estimation due to the inaccurate virtual marker estimation around the arm when occluded. \textbf{(b)} Our method treats the lower arm of another person as its own due to occlusion. \textbf{(c)} Interpenetration around the right hand.}
    \label{fig:supp_failure}
% \vspace{-0.4cm}
\end{figure}

Figure \ref{fig:supp_quality_results} shows more quality results of our approach on the 3DPW \cite{vonMarcard2018}, H3.6M \cite{h36m_pami}, MPI-INF-3DHP \cite{mehta2017monocular}, and COCO \cite{lin2014microsoft} datasets. Figure \ref{fig:supp_wild} shows more qualitative results on Internet images with challenging cases, such as extreme body shapes or complex poses. Our method generalizes well on the natural scenes. Figure \ref{fig:supp_failure} shows typical failure cases, including inaccurate shape estimation and interpenetration, which are mainly caused by inaccurate 3D virtual marker estimation when occlusion occurs. But as expected, the rest body parts are barely affected due to the \emph{local and sparse} property of the virtual marker. 



\begin{figure*}[t]
    \centering
    \includegraphics[width=5.7in]{imgs/supp/supp_surreal.pdf}
    \caption{Qualitative comparison between our method and Pose2Mesh \cite{choi2020pose2mesh} on SURREAL test set \cite{varol2017learning}. Our approach generates more accurate mesh estimation results on images of diverse body shapes.}
    \label{fig:supp_surreal}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=6.6in]{imgs/supp/supp_pw3d.pdf}
    \caption{Qualitative comparison between our method and Pose2Mesh \cite{choi2020pose2mesh}, METRO \cite{lin2021end} on 3DPW test set \cite{vonMarcard2018}. Our approach is more robust to occlusion and truncation and generates more accurate mesh estimation results that align images well.}
    \label{fig:supp_pw3d}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=6.5in]{imgs/supp/supp_quality_results.pdf}
    \caption{Meshes estimated by our approach on images from the 3DPW \cite{vonMarcard2018} dataset (row 1-4), H3.6M \cite{h36m_pami} dataset (row 5), MPI-INF-3DHP \cite{mehta2017monocular} dataset (row 6), and COCO dataset (last 2 rows) \cite{lin2014microsoft}. }
    \label{fig:supp_quality_results}
\end{figure*}



\subsection*{C. \quad Human Subject Data} 

We use existing public datasets of human subjects in our experiments following their official licensing requirements. With proper usage, the proposed method could be beneficial to society (\eg elderly fall detection). 




