{
    "arxiv_id": "2303.08423",
    "paper_title": "Communication-Efficient Design for Quantized Decentralized Federated Learning",
    "authors": [
        "Li Chen",
        "Wei Liu",
        "Yunfei Chen",
        "Weidong Wang"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-10-10"
    ],
    "latest_version": 3,
    "categories": [
        "cs.DC",
        "eess.SP"
    ],
    "abstract": "Decentralized federated learning (DFL) is a variant of federated learning, where edge nodes only communicate with their one-hop neighbors to learn the optimal model. However, as information exchange is restricted in a range of one-hop in DFL, inefficient information exchange leads to more communication rounds to reach the targeted training loss. This greatly reduces the communication efficiency. In this paper, we propose a new non-uniform quantization of model parameters to improve DFL convergence. Specifically, we apply the Lloyd-Max algorithm to DFL (LM-DFL) first to minimize the quantization distortion by adjusting the quantization levels adaptively. Convergence guarantee of LM-DFL is established without convex loss assumption. Based on LM-DFL, we then propose a new doubly-adaptive DFL, which jointly considers the ascending number of quantization levels to reduce the amount of communicated information in the training and adapts the quantization levels for non-uniform gradient distributions. Experiment results based on MNIST and CIFAR-10 datasets illustrate the superiority of LM-DFL with the optimal quantized distortion and show that doubly-adaptive DFL can greatly improve communication efficiency.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08423v1",
        "http://arxiv.org/pdf/2303.08423v2",
        "http://arxiv.org/pdf/2303.08423v3"
    ],
    "publication_venue": null
}