\section{Experiments}
\label{sec:experiments}
In this section, we conduct extensive experiments to demonstrate the effectiveness of our proposed HAST scheme. We organize this section as follows. We first provide the experiment setting in Sec.~\ref{Experimental Setup}. Second, we compare our proposed method with existing ZSQ methods in Sec.~\ref{sec:cifar} and Sec.~\ref{sec:imagenet}. Last, we investigate the effect of the hyperparameters and the proposed component in our HAST scheme in Sec.~\ref{sec:ablation}. The code to reproduce our results is available \footnote{\url{https://github.com/lihuantong/HAST}}.

\begin{table}[t]   
\begin{center}   
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c|c}   
\hline 
Bit-width & Method & Generator & Acc.(\%) \\ \hline \hline
 & full-precision & - & 94.03 \\ \hline
\multirow{8}{*}{W4A4}& Real Data & - & 91.52 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{SQuant} & - & 92.24 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 90.25 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & 91.23 \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 89.66 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 88.93 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 91.49 \\
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{92.36}$\pm0.09$ \\ \hline

\multirow{8}{*}{W3A3}& Real Data & - & 87.94 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{SQuant} & - & 79.19 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 71.10 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & 80.49 \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 69.53 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 48.99 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 77.07 \\ 
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{88.34}$\pm0.06$ \\ \hline
\multicolumn{4}{c}{(a) Results on CIFAR-10.} \\

\multicolumn{4}{c}{  } \\ \hline

Bit-width & Method & Generator & Acc.(\%) \\ \hline \hline
 & full-precision & - & 70.33 \\ \hline
\multirow{8}{*}{W4A4}& Real Data & - & 66.80 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{SQuant} & - & 63.96 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 63.58 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & 65.80 \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 63.97 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 62.62 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 64.98 \\
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{66.68}$\pm0.12$ \\ \hline

\multirow{8}{*}{W3A3}& Real Data & - & 56.26 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{SQuant} & - & 40.36 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 43.87 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & 48.64 \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 26.35 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 43.42 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 48.25 \\ 
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{55.67}$\pm0.26$ \\ \hline
\multicolumn{4}{c}{(b) Results on CIFAR-100.} \\ 
\end{tabular} 
}
\vspace{-3mm}
\caption{Results of ResNet-20 on CIFAR-10/100. WBAB indicates the weights and activations are quantized to B-bit.}
\vspace{-10mm}
\label{table1} 
\end{center}   
\end{table}


\subsection{Experimental Setup}
\label{Experimental Setup}
\par \textbf{Datasets.} We evaluate our method on three datasets, including CIFAR-10~\cite{cifar}, CIFAR-100~\cite{cifar} and ImageNet (ILSVRC2012)~\cite{imagenet}, which are commonly used in most ZSQ methods. We report top-1 accuracy of all methods. 
% The quantized models include ResNet-20~\cite{resnet} for CIFAR-10/100, ResNet-18~\cite{resnet}, MobileNetV1~\cite{mobilenetv1} and MobileNetV2~\cite{mobilenetv2} for ImageNet. 
\par \textbf{Network Architectures.} We choose ResNet-20~\cite{resnet} for Cifar-10/100 and select ResNet-18~\cite{resnet}, MobileNetV1~\cite{mobilenetv1} and MobileNetV2~\cite{mobilenetv2} for ImageNet to evaluate our method.
ResNet-20 and ResNet-18 are popular medium-sized models. MobileNetV1 and MobileNetV2 are widely-used lightweight models. All pretrained models are from pytorchcv library and all experiments are implemented with Pytorch~\cite{pytorch}.
\par \textbf{Baselines.} The compared methods based on noise optimization include ZeroQ~\cite{ZeroQ}, DSG~\cite{DSG}, and IntraQ~\cite{IntraQ}.
The generator-based methods are also compared, including GDFQ~\cite{GDFQ}, and AIT~\cite{AIT}.
In addition, SQuant~\cite{SQuant}, an recent on-the-fly ZSQ method without synthesizing data, is also involved. For ZeroQ and DSG, we report the results with the inception loss (IL)~\cite{inception}. Since AIT applies the method on three different generator-based methods, we report the best result. Note all the methods quantize all layers of the models to the ultra-low precisions, except SQuant which sets the last layer to 8-bit.
\par \textbf{Implementation details.} For data generation, the synthetic images are optimized by the loss function Eq.~(\ref{HFNL}) using Adam optimizer with a momentum of 0.9 and the initial learning rate of 0.5. We update the synthetic images for 1,000 iterations and decay the learning rate by 0.1 each time the loss of Eq.~(\ref{HFNL}) stops decreasing for 50 iterations. For all datasets, the batch size is set to 256. We synthesize 5120 images following the settings of IntraQ~\cite{IntraQ}.
For network fine-tuning, the quantized models are fine-tuned by the loss function Eq.~(\ref{eq:FAR}) using SGD with Nesterov with a momentum of 0.9 and the weight decay of $10^{-4}$. The batch size is 256 for CIFAR-10/100 and 16 for ImageNet. The initial learning rate is set to $10^{-5}$ and $10^{-6}$ for CIFAR-10/100 and ImageNet respectively. Both learning rates are decayed by 0.1 every 100 fine-tuning epochs and a total of 150 epochs are given.
For data augmentation and hyper-parameters $\beta$ in Eq.~(\ref{HFNL}) and $\alpha$ in Eq.~(\ref{eq:FAR}), we keep the same settings as~\cite{IntraQ} for fair comparison. In addition, there are three hyper-parameters in our method, including $\gamma$ in Eq.~(\ref{hil}), $\epsilon$ in Eq.~(\ref{SDP}) and $\lambda$ in Eq.~(\ref{eq:FAR}). They are respectively set to 2, 0.01, $1\times10^3$ for CIFAR-10; 2, 0.02, $2\times10^{3}$ for CIFAR-100 and 0.5, 0.01, $4\times10^{3}$ for ImageNet.


\begin{table}[t]   
\begin{center}   
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c|c}   
\hline 
Bit-width & Method & Generator & Acc.(\%) \\ \hline \hline
 & full-precision & - & 71.47 \\ \hline
\multirow{8}{*}{W4A4}& Real Data & - & 67.89 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{ZeroQ} & - & 66.14 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 60.60 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & 66.83 \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 63.38 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 63.11 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 66.47 \\
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{66.91}$\pm0.16$ \\ \hline

\multirow{7}{*}{W3A3}& Real Data & - & 51.95 \\ \cline{2-4} 
\multirow{7}{*}{}& SQuant~\cite{ZeroQ} & - & 25.74 \\
\multirow{7}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 20.69 \\
\multirow{7}{*}{}& AIT~\cite{AIT} & \ding{52} & 36.70 \\
\multirow{7}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 44.68 \\
\multirow{7}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 45.51 \\ 
\multirow{7}{*}{}& HAST(Ours) & \ding{56} & \textbf{51.15}$\pm0.27$ \\ \hline
\end{tabular} 
}
\vspace{-1mm}
\caption{Results of ResNet-18 on ImageNet. WBAB indicates the weights and activations are quantized to B-bit.}
\vspace{-10mm}
\label{table2} 
\end{center}   
\end{table}


% \subsection{Performance Comparison}
\subsection{Comparison Results on CIFAR-10/100}
\label{sec:cifar}
% \par In this section, We compare the performance on CIFAR-10/100 against existing ZSQ methods. 
% They include methods based on noise optimization: ZeroQ~\cite{ZeroQ}, DSG~\cite{DSG}, and IntraQ~\cite{IntraQ}. We also include generator-based methods for comparison: GDFQ~\cite{GDFQ}, and AIT~\cite{AIT}. 
% The compared methods based on noise optimization include ZeroQ~\cite{ZeroQ}, DSG~\cite{DSG}, and IntraQ~\cite{IntraQ}.
% The generator-based methods are also compared, including GDFQ~\cite{GDFQ}, and AIT~\cite{AIT}.
% In addition, SQuant~\cite{SQuant}, an recent on-the-fly ZSQ method without synthesizing data, is also involved. For ZeroQ and DSG, we report the results with the inception loss. Since AIT applies the method on three different generator-based methods, we report the best result. Note all the methods quantize all layers of the models to the ultra-low precisions, except SQuant which sets the last layer to 8-bit. The bit-width of the quantized model is marked as W$w$A$a$ standing for $w$-bit weight and $a$-bit activation.

\par In this section, We compare the performance on CIFAR-10/100 against existing ZSQ methods. From Table~\ref{table1}, our method achieves significant performance improvements on both CIFAR-10 and CIFAR-100. Specifically, compared to the advanced generator-based AIT, our method increases the top-1 accuracy of 3-bit quantized models by 7.85\% on CIFAR-10 and 7.03\% on CIFAR-100. When compared with IntraQ which obtains 3-bit accuracy of 77.07\% and 48.25\% on CIFAR-10 and CIFAR-100 respectively by exploiting the intra-class heterogeneity in the synthetic images, the proposed method reaches the higher performance of 88.34\% and 55.67\% using the same number of synthetic images. Notably, 
since the CIFAR-10 dataset is relatively easy when compared with CIFAR-100 and ImageNet, our method outperforms fine-tuning with real data. The gap is also only 0.59\% when it comes to CIFAR-100. 
% Similar results can be observed in 4-bit quantization as well, well demonstrating the effectiveness of hard samples synthesizing and learning.
We obtain the similar results in 4-bit quantization, demonstrating the effectiveness of our proposed HAST.



\begin{table}[t]
\begin{center}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c|c}   
\hline 
Bit-width & Method & Generator & Acc.(\%) \\ \hline \hline
 & full-precision & - & 73.39 \\ \hline
\multirow{8}{*}{W5A5}& Real Data & - & 69.87 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{ZeroQ} & - & 64.20\\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 59.76 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & - \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 67.11 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 66.61 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 68.17 \\
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{68.52}$\pm0.17$ \\ \hline


\multirow{8}{*}{W4A4}& Real Data & - & 59.66 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{ZeroQ} & - & 10.32 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 28.64 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & - \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 25.43 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 42.19 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 51.36 \\ 
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & \textbf{57.70}$\pm0.31$ \\ \hline
\multicolumn{4}{c}{(a) Results of MobileNetV1.} \\
\multicolumn{4}{c}{  } \\ \hline
Bit-width & Method & Generator & Acc.(\%) \\ \hline \hline
 & full-precision & & 73.03 \\ \hline
\multirow{8}{*}{W5A5}& Real Data & - & 72.01 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{ZeroQ} & - & 66.83 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 68.14 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & \textbf{71.96} \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 70.95 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 70.87 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 71.28 \\
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & 71.72$\pm0.19$ \\ \hline

\multirow{8}{*}{W4A4}& Real Data & - & 67.90 \\ \cline{2-4} 
\multirow{8}{*}{}& SQuant~\cite{ZeroQ} & - & 22.07 \\
\multirow{8}{*}{}& GDFQ~\cite{GDFQ} & \ding{52} & 51.30 \\
\multirow{8}{*}{}& AIT~\cite{AIT} & \ding{52} & \textbf{66.47} \\
\multirow{8}{*}{}& ZeroQ~\cite{ZeroQ}+IL~\cite{inception} & \ding{56} & 60.15 \\ 
\multirow{8}{*}{}& DSG~\cite{DSG}+IL~\cite{inception} & \ding{56} & 59.04 \\
\multirow{8}{*}{}& IntraQ~\cite{IntraQ} & \ding{56} & 65.10 \\ 
\multirow{8}{*}{}& HAST(Ours) & \ding{56} & 65.60$\pm0.27$ \\ \hline
\multicolumn{4}{c}{(b) Results of MobileNetV2.} \\ 
\end{tabular} 
}
\vspace{-3mm}
\caption{Results of MobileNetV1/V2 on ImageNet. WBAB indicates the weights and activations are quantized to B-bit}
\vspace{-10mm}
\label{table3} 
\end{center}   
\end{table}



\subsection{Comparison Results on ImageNet}
\label{sec:imagenet}
\par We further compare with the competitors on the large-scale ImageNet. The quantized networks include ResNet-18 and MobileNetV1/V2. Similar to CIFAR-10/100, we quantize all layers of the networks. Differently, since MobileNetV1/V2 are lightweight models which suffer great performance degradation when quantized to ultra-low precision, we quantize MobileNetV1/V2 to 5-bit and 4-bit following the settings of most existing methods.
\par \textbf{ResNet-18.} Table~\ref{table2} shows the experimental results of ResNet-18. In the case of 4-bit, our HSAT (66.91\%) slightly outperforms the IntraQ (66.47\%) and AIT (66.83\%). When it comes to 3-bit, our HSAT significantly surpasses AIT and IntraQ by 14.45\% and 5.64\% on ImageNet. Moreover, the performance of our method is very close to fine-tuning using real data regardless of 4-bit or 3-bit, with a gap of less than 1\%. This indicates that our HSAT 
is able to mimic real data and generalize quantized model well even on large-scale and hard datasets.



% \begin{figure*}[ht]
% \centering
% \begin{minipage}[b]{\linewidth}
%         \centering
%         \includegraphics[width=2.1in]{figs/parameter_gamma.pdf}
%         \includegraphics[width=2.1in]{figs/parameter_epsilon.pdf}
%         \includegraphics[width=2.1in]{figs/parameter_lambda.pdf}
% \end{minipage}
% \vspace{-8mm}
% \caption{Sensitivity analysis on hyper-parameters. We report the top-1 accuracy of 3-bit ResNet-18 on ImageNet.}
% \vspace{-5mm}
% \label{Fig5}
% \end{figure*}

\par \textbf{MobileNetV1/V2.} In Table~\ref{table3}, our method still outperforms almost all baselines except AIT on quantizing lightweight models. For example, while other methods achieve unexpectedly low performance on quantizing MobileNetV1 to 4-bit, our HSAT obtains 6.34\% accuracy improvement when compared with IntraQ. However, Unlike other networks, we find that the fine-tuning process of MobileNetV2 is more unstable and the performance improvement during fine-tuning is smaller. This shows that the training process of MobileNetV2 has a greater impact on the performance recovery than the sample difficulty. AIT uses a dynamic learning rate for each convolution kernel to solve this problem, while we use a uniform fixed learning rate. Thus AIT performs better than HAST on MobileNetV2. It is worth mentioning that AIT can be used in combination with HAST as long as memory and time overheads allow.

\par In short, sufficient experiments over various network architectures demonstrate that the synthetic data generated by the proposed HSAT scheme is able to significantly improve the performance of the quantized model. The results also suggest that synthesizing and learning hard samples is important for improving the quantized model. Especially when model is quantized to lower bit-width, the effect of hard samples is more obvious on final performance.



% \subsection{Further experiments}
\subsection{Ablation Study}
\label{sec:ablation}
\par In this section, we conduct ablation studies of the hyper-parameters and investigate the effect of the three components of our method. We put the hyper-parameters ablation studies, further discussions and related experimental results in the appendix.


% \par \textbf{Sensitivity analysis on hyper-parameters.} The hyper-parameters in our method including $\gamma$ in Eq.~(\ref{hil}), $\epsilon$ in Eq.~(\ref{SDP}) and $\lambda$ in Eq.~(\ref{eq:FAR}). In Figure~\ref{Fig5}, a sensitivity study of them on 3-bit ResNet-18 is performed. For $\gamma$, we choose several small values from 0 to 2 since the optimization in data generation process of ImageNet is not as easy as other small-scale datasets. We keep $\epsilon$ on a small magnitude so that the perturbation is not too large, and keep $\lambda$ on a large magnitude so that the magnitude of two loss terms are consistent. 
% % The results show that HSAT is not very sensitive to these hyper-parameters, although there is some effect. 
% The results show that the performance of HAST is somewhat sensitive to these hyper-parameters, but most of these results ($50.12\% \sim 51.15\%$) are comparable with that of the model fine-tuned on real data ($51.95\%$). Note that the worse results in Figure~\ref{Fig5} outperforms the quantized model obtained by the state-of-the-art ZSQ method ($45.51\%$) in a large margin.
% % Similar experiments can be conducted to find out the optimal value of these hyper-parameters on other datasets, as listed in Sec.~\ref{Implementation Details}.
% We conduct similar experiments to find out the optimal value of these hyper-parameters on other datasets. %, as listed in Sec.~\ref{Experimental Setup}

\par \textbf{Effects of the proposed components.} We further study the effectiveness of our proposed hard sample synthesis in Sec.~\ref{HSS}, sample difficulty promotion in Sec.~\ref{sec:SDP}, and feature alignment in Sec.~\ref{FA}. Table~\ref{table4} shows the experimental results. Note that when we use ZeroQ+IL as the baseline described in Sec.~\ref{Preliminaries}, we only obtain an accuracy of 44.68\%. From Table~\ref{table4}, when the three components are individually added to synthesize and learn hard samples, the accuracy increases compared with the baseline. Among them, adversarial augmentation obtains a high accuracy improvement of 3.79\%. This inspires us the importance of hard samples in training quantized models. Since attention transfer can help the quantized model learn more informative knowledge from the intermediate feature of the full-precision model, it still increases the performance by 1.39\% without hard samples. Furthermore, the performance continues to increase when any two of them are used. When all of them are applied, we obtain the best performance of 51.15\%.

\begin{table}[h]   
\begin{center}
\begin{tabular}{c c c|c}   
\hline 
HSS & SDP & FA & Acc.(\%) \\ \hline \hline
 & & & 44.68\\ \hline
\ding{52} & & & 45.56\\
 & \ding{52} & & 48.47\\
 & & \ding{52} & 46.07\\
 & \ding{52} & \ding{52} & 50.22\\
\ding{52} & \ding{52} & & 49.17\\
\ding{52} & & \ding{52} & 47.94\\
\ding{52} & \ding{52} & \ding{52} & \textbf{51.15}\\ \hline
\end{tabular} 
\vspace{-1mm}
\caption{Ablations on different components of our method. “HSS” indicates the hard sample synthesis, “SDP” indicates the sample difficulty promotion, and “FA” indicates the feature alignment. We report the top-1 accuracy of 3-bit ResNet-18 on ImageNet.}
\vspace{-7mm}
\label{table4} 
\end{center}   
\end{table}

\par \textbf{Limitation.} In this work, we merely considered a limited number of scenarios. Thus, we will explore the potential power for more practical scenarios, such as federated learning~\cite{Dong_2022_CVPR,tang2022virtual} (with limited communication bandwidth), vision transformer~\cite{dosovitskiy2020image} (with large-scale model parameters), and the performance of quantized models over out-of-distribution scenarios~\cite{fangout}.


% \begin{figure*}[ht]
% \centering
%     \subfloat[Gradient cosine similarity.]{
%         \label{Fig5.a}
%         \includegraphics[width=1.6in]{figs/gradient cosine similiarity.pdf}
%     }
%     \quad
%     \subfloat[Distribution of eigenvalues.]{
%         \label{Fig5.b}
%         \includegraphics[width=1.6in]{figs/density_eigenvalue of CE.pdf}
%         \includegraphics[width=1.6in]{figs/density_eigenvalue of KL.pdf}
%         \includegraphics[width=1.6in]{figs/density_eigenvalue of AT.pdf}
%     }
% \caption{Further experiments on attention transfer. (a)Gradient cosine similarity of two terms in loss function. (b)Distribution of the eigenvalues for different loss.}
% \label{fig5}
% \end{figure*}


% \subsection{Further Experiments on Feature Alignment}
% \label{Further Experiments on Attention Transfer}
% \par In this section, to further explain the role of feature alignment loss instead of cross entropy loss, we follow AIT to conduct further experiments to verify the gradient consistency and generalization of the feature alignment loss. We measured the cosine similarity of two terms in Eq.~(\ref{eq7}) and Eq.~(\ref{eq14}) respectively by training a quantized ResNet-20 model. The results are presented in Figure~\ref{Fig5.a}. The cosine similarity of KL and CE take negative values and persists small throughout the training, demonstrating that the two losses does not cooperate well with each other. However, the cosine similarity of KL and AT take positive values and keep on increasing until the end of training. This implies that the combination of them works well. Furthermore, we use PyHessian to plot the distribution of eigenvalues to analyze the local curvature of loss terms. Figure~\ref{Fig5.b} shows a huge difference among CE, KL and AT loss. While CE loss has longer tail for high eigenvalues, KL and AT have more concentration on lower eigenvalues. In line with the finding that lower eigenvalues represents smaller local curvature and improves generalization, AT is likely to generalize much better than CE.