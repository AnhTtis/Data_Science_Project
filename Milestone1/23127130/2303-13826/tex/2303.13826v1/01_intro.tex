\section{Introduction}
\label{sec:intro}
\par Deep neural networks (DNNs) achieve great success in many domains, such as image classification~\cite{DBLP:journals/cacm/KrizhevskySH17,DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/SzegedyLJSRAEVR15}, object detection~\cite{DBLP:conf/cvpr/GirshickDDM14,DBLP:conf/iccv/Girshick15,DBLP:conf/cvpr/PangCSFOL19}, semantic segmentation~\cite{DBLP:journals/ijcv/EveringhamEGWWZ15,DBLP:conf/cvpr/ZhuangSTL019}, and embodied AI~\cite{chen2022active,chen2022weakly,DBLP:journals/tetci/DuanYTZT22}. 
These achievements are typically paired with the rapid growth of parameters and computational complexity, making it challenging to deploy DNNs on resource-constrained edge devices. In response to the challenge, \emph{network quantization} proposes to represent the full-precision models, i.e., floating-point parameters and activations, using low-bit integers, resulting in a high compression rate and an inference-acceleration rate~\cite{DBLP:conf/cvpr/JacobKCZTHAK18}. These methods implicitly assume that the training data of full-precision models are available for the process of network quantization. However, the training data, e.g., medical records, can be inaccessible due to privacy and security issues. Such a practical and challenging scenario has led to the development of \emph{zero-shot quantization} (ZSQ)~\cite{ZeroQ}, quantizing networks without accessing the training data.


%To alleviate the accuracy degradation caused by quantization, Quantization Aware Training (QAT)~\cite{LSQ,QIL,APoT,PACT,DBLP:conf/cvpr/JacobKCZTHAK18} methods recover accuracy by fine-tuning the quantized model for a long time on the full original dataset. Post Training Quantization (PTQ)~\cite{DBLP:conf/icml/ZhaoHDSZ19,BRECQ,DBLP:conf/icml/NagelABLB20,DBLP:conf/iccvw/ChoukrounKYK19,DBLP:conf/nips/BannerNS19} approaches correct parameters on a small calibration set to achieve a trade-off between time-consuming and accuracy. However, in practical scenarios, original training data, such as medical records and face information, is sometimes unavailable due to the privacy and security issues. In this case, data-dependent QAT and PTQ cannot work properly.

\begin{figure}[t]
\centering
\subfloat[3-bit quantization.]{\vspace{-2mm}\includegraphics[width=1.65in]{3bit.pdf}}
\subfloat[4-bit quantization.]{\vspace{-2mm}\includegraphics[width=1.65in]{4bit.pdf}}
\vspace{-1mm}
\caption{Performance of the proposed HAST on three datasets compared with the state-of-the-art method IntraQ~\cite{IntraQ} and the method fine-tuning with real data~\cite{IntraQ}. HAST quantizes ResNet-20 on CIFAR-10/CIFAR-100 and ResNet-18 on ImageNet to 3-bit (left) and 4-bit (right), achieving performance comparable to the method fine-tuning with real data.}
\vspace{-5mm}
\label{Fig1}
\end{figure}

\par 
Many efforts have been devoted to ZSQ~\cite{DFQ,SQuant,ZeroQ,DSG,IntraQ,GDFQ}. In ZSQ, some works perform network quantization by weight equalization~\cite{DFQ}, bias correction~\cite{DBLP:conf/nips/BannerNS19}, or weight rounding strategy~\cite{SQuant}, at the cost of some performance degradation. To promote the performance of quantized models, advanced works propose to leverage synthetic data for network quantization~\cite{ZeroQ,DSG,IntraQ,GDFQ,AIT}. Specifically, they fine-tune quantized models with data synthesized using full-precision models, achieving promising improvement in performance. 


Much attention has been paid to the generation of synthetic sample, since high-quality synthetic samples lead to high-performance quantized models~\cite{ZeroQ,GDFQ}. Recent works employ generative models to synthesize data with fruitful approaches, considering generator design~\cite{AutoReCon}, boundary sample generation~\cite{Qimera}, adversarial training scheme~\cite{ZAQ}, and effective training strategy~\cite{AIT}. Since the quality of synthetic samples is typically limited by the generator~\cite{Survey}, advanced works treat synthesizing samples as a problem of noise optimization~\cite{ZeroQ}. Namely, the noise distribution is optimized to approximate some specified properties of real data distributions, such as batch normalization statistics (BNS) and inception loss (IL)~\cite{inception}. To promote model performance, IntraQ~\cite{IntraQ} focuses on the property of synthetic samples and endows samples with heterogeneity, achieving state-of-the-art performance as depicted in Figure~\ref{Fig1}.

\begin{figure*}[t]
\centering
\subfloat[Performance of converged 3-bit ResNet-20.]{\vspace{-2mm}
\label{Fig2.a}
\includegraphics[width=2in]{accuracy_on_trainset_and_testset.pdf}
}
\subfloat[Variation of error rate with sample difficulty.]{\vspace{-2mm}
\label{Fig2.b}
\includegraphics[width=1.96in]{gradient_norm_distribution_of_misclassified_samples.pdf}
}
\subfloat[Variation of fraction with sample difficulty.]{\vspace{-2mm}
\label{Fig2.c}
\includegraphics[width=2in]{gradient_norm_distribution.pdf}
}
\vspace{-1mm}
\caption{Analysis on synthetic data. (a) Performance of converged 3-bit ResNet-20. We quantize ResNet-20 to 3-bit using IntraQ~\cite{IntraQ}, Real Data~\cite{IntraQ}, and Our HAST, respectively, The top-1 accuracy on both training data (synthetic data for ZSQ methods) and test data is reported. (b) The error rate of test samples with different difficulties. (c) Distribution visualization of sample difficulty using GHM~\cite{GHM}. For each converged quantized model, we randomly sample 10,000 synthetic/real samples and count the fraction of samples based on difficulty. Note that the y-axis uses a log scale since the number of samples with different difficulties can differ by order of magnitude.}
\vspace{-3mm}
\label{Fig2}
\end{figure*}


Although existing ZSQ methods achieve considerable performance gains by leveraging synthetic samples, there is still a significant performance gap between models trained with synthetic data and those trained with real data~\cite{IntraQ}. To reduce the performance gap, we investigate the difference between real and synthetic data. Specifically, we study the difference in generalization error between models trained with real data and those trained with synthetic data. Our experimental results show that synthetic data lead to larger generalization errors than real data, as illustrated in Figure~\ref{Fig2.a}. Namely, synthetic sample lead to a more significant gap between training and test accuracy than real data. 

We conjecture that the performance gap stems from the misclassification of hard test samples. To verify the conjecture, we conduct experiments to study how model performance varies with sample difficulty, where GHM~\cite{GHM} is employed to measure the difficulty of samples quantitatively. The results shown in Figure~\ref{Fig2.b} demonstrate that, on difficult samples, models trained with synthetic data perform worse than those trained with real data.
This may result from that synthetic samples are easy to fit, which is consistent with the observation on inception loss of synthetic data~\cite{MixMix}. We verify the assumption through a series of experiments, where we count the fraction of samples of different difficulties using GHM~\cite{GHM}. The results are reported in Figure~\ref{Fig2.c}, where we observe a severe missing of hard samples in synthetic samples compared to real data. Consequently, quantized models fine-tuned with these synthetic data may fail to generalize well on hard samples in the test set. 

In light of conclusions drawn from Figure~\ref{Fig2}, the samples synthesized for fine-tuning quantized models in ZSQ should be hard to fit. To this end, we propose a novel \textbf{HA}rd sample \textbf{S}ynthesizing and \textbf{T}raining (HAST) scheme. The insight of HAST has two folds: a) The samples constructed for fine-tuning models should not be easy for models to fit; b) The features extracted by full-precision and the quantized model should be similar. To this end, in the process of synthesizing samples, HAST pays more attention to hard samples in a re-weighting manner, where the weights are equal to the sample difficulty introduced in GHM~\cite{GHM}. Meanwhile, in the fine-tuning process, HAST further promotes the sample difficulty on the fly and aligns the features between the full-precision and quantized models. 

To verify the effectiveness of HAST, we conduct comprehensive experiments on three datasets under two quantization precisions, following settings used in~\cite{IntraQ}. Our experimental results show that HAST using only 5,120 synthetic samples outperforms previous state-of-the-art method~\cite{IntraQ} and even achieves performance comparable with quantization with real data. 

Our main contributions can be summarized as follows:
\vspace{-1mm}
\begin{itemize}
    \item[$\bullet$] 
    We observe that the performance degradation in zero-shot quantization is attributed to the lack of hard samples. Namely, the synthetic samples used in existing ZSQ methods are easily fitted by quantized models, distinguishing models trained on synthetic samples from those trained on real data, as depicted in Figure~\ref{Fig2}.
    \vspace{-1mm}
    \item[$\bullet$] 
    Built upon our empirical observation, we propose a novel \textbf{HA}rd sample \textbf{S}ynthesizing and \textbf{T}raining (HAST) scheme to promote the performance of ZSQ. Specifically, HAST generates hard samples and further promotes the sample difficulty on the fly when training models, paired with a feature alignment constraint to ensure the similarity of features extracted by these two models, as summerized in Algorithm~\ref{alg:algorithm1}.
    \vspace{-1mm}
    \item[$\bullet$] 
    Extensive experiments demonstrate the superiority of HAST over existing ZSQ methods. More specifically, HAST using merely 5,120 synthetic samples outperforms the previous state-of-the-art method and achieves performance comparable to models fine-tuned using real training data, as shown in Figure~\ref{Fig1}.
\end{itemize}
