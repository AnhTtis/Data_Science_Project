\section{Related Work}
In this section, we briefly review the most relevant works in network quantization.

\label{sec:related}
\subsection{Data-Driven Quantization}
Network quantization is proposed to promote the compression rates and accelerate the inference by representing the full-precision models using low-bit integers. The straightforward and effective approach is to fine-tune models using the training data of full-precision models. In this regard, quantization aware training (QAT) methods focus on designing quantizers~\cite{PACT, QIL, LSQ, APoT, DSQ}, training strategies~\cite{DBLP:conf/cvpr/ZhuangLTSR20,DBLP:conf/cvpr/Lee0H21}, and binary networks~\cite{DBLP:conf/nips/LinJX00WHL20,DBLP:conf/iclr/MartinezYBT20,DBLP:conf/cvpr/QinGLSWYS20}.

In many practical scenarios, the training data of full-precision models can be inaccessible, limiting the effectiveness of QAT methods. To address the challenge, post-training quantization (PTQ) methods perform network quantization with limited training data~\cite{DBLP:conf/nips/BannerNS19,BRECQ}. Specifically, these methods approximate an optimal clipping value in the feature space using limited training samples~\cite{DBLP:conf/nips/BannerNS19} and introduce an allocation policy to quantize both activations and weights to 4-bit. However, both QAT and PTQ methods require training data to perform network quantization.

% \par With original training data to fine-tune the quantized model and quantizer, QAT methods focus on designing quantizers~\cite{PACT, QIL, LSQ, APoT, DSQ}, training strategies~\cite{DBLP:conf/cvpr/ZhuangLTSR20,DBLP:conf/cvpr/Lee0H21}, binary networks~\cite{DBLP:conf/nips/LinJX00WHL20,DBLP:conf/iclr/MartinezYBT20,DBLP:conf/cvpr/QinGLSWYS20}, etc. PTQ methods are limited to access a small part of training data without fine-tuning process. ~\cite{DBLP:conf/nips/BannerNS19} approximates the optimal clipping value analytically and introduces bit allocation policy and bias-correction to quantize both activations and weights to 4-bit. ~\cite{DBLP:conf/icml/NagelABLB20} shows that rounding-to-nearest is not the optimal rounding function and propose a better weight-rounding mechanism  by formulating the rounding as a layer-wise quadratic unconstrained binary problem. Based on a theoretical study of the second-order loss and empirical evidence, ~\cite{BRECQ} propose a block reconstruction to regain the accuracy. However, both QAT and PTQ require data to perform quantization.

\subsection{Zero-Shot Quantization}
\par To further relax the privacy or security issue, ZSQ methods propose to perform network quantization without accessing the training data of full-precision models. A simple yet effective approach is calibrating model parameters without training data. For example, DFQ~\cite{DFQ} equalizes the weight ranges in the network and utilizes the scale and shift parameters stored in the batch normalization layers to correct biased quantization errors. SQuant~\cite{SQuant} performs data-free quantification using the diagonal Hessian approximation layer by layer. However, these methods may lead to significant performance degradation when quantizing models with ultra-low precision~\cite{IntraQ}.

Advanced works propose to generate synthetic samples for fine-tuning quantized models, resulting in promoted model performance. GDFQ~\cite{GDFQ} first adopts generative models guided by both the batch normalization statistics and extra category label information to synthesize samples. Since the performance depends heavily on synthetic samples, many variants of GDFQ improve the performance by adopting better generator~\cite{AutoReCon}, adversarial training~\cite{ZAQ}, boundary-supporting samples generation~\cite{Qimera}, and  using an ensemble of compressed models~\cite{GZNQ}. Recently, it has been shown that data synthesis can be realized by optimizing random noise sampled from a pre-defined distribution~\cite{ZeroQ}. The noise optimization scheme is further improved by carefully designing an appropriate mechanism for synthesizing data, such as enhancing the diversity of samples~\cite{DSG} and increasing the intra-class heterogeneity of synthetic samples~\cite{IntraQ}. Although these methods achieve considerable performance gain compared to data-driven quantization, there is still a performance gap between fine-tuning with synthetic and real data.

%Another group focus on synthesizing fake images to fine-tune quantized model for better performance. GDFQ~\cite{GDFQ} first adopts generative models guided by BNS and extra category label information to generate label-oriented images. Variants of GDFQ improve the performance by adopting better generator~\cite{AutoReCon}, adversarial training~\cite{ZAQ}, boundary-supporting samples generation~\cite{Qimera} or using  an ensemble of compressed models~\cite{GZNQ}. In addition to the generator, the data synthesis can also be realized by optimizing random noise sampled from Gaussian distribution to fit real data distribution, first proposed by ZeroQ~\cite{ZeroQ}. With this scheme, choosing the adequate mixed-precision quantization for each layer is proposed together. On top of ZeroQ, DSG~\cite{DSG} relaxes the the alignment of BNS and randomly enhances the loss term to generate diverse samples. IntraQ~\cite{IntraQ} increases the intra-class heterogeneity of synthetic images considering object location, feature distance and labels.