\appendix
\label{sec:appendix}

\section{Sensitivity Analysis on Hyper-parameters}
\par
 The hyper-parameters in our method including $\gamma$ in Eq.~8, $\epsilon$ in Eq.~10 and $\lambda$ in Eq.~12. In Figure~\ref{supp_fig1}, a sensitivity study of them on 3-bit ResNet-18 is performed. For $\gamma$, we choose several small values from 0 to 2 since the optimization in data generation process of ImageNet is not as easy as other small-scale datasets. We keep $\epsilon$ on a small magnitude so that the perturbation is not too large, and keep $\lambda$ on a large magnitude so that the magnitude of two loss terms are consistent. 
% The results show that HSAT is not very sensitive to these hyper-parameters, although there is some effect. 
The results show that the performance of HAST is somewhat sensitive to these hyper-parameters, but most of these results ($50.12\% \sim 51.15\%$) are comparable with that of the model fine-tuned on real data ($51.95\%$). Note that the worse results in Figure~\ref{supp_fig1} outperforms the quantized model obtained by the state-of-the-art ZSQ method ($45.51\%$) in a large margin.
% Similar experiments can be conducted to find out the optimal value of these hyper-parameters on other datasets, as listed in Sec.~\ref{Implementation Details}.
We conduct similar experiments to find out the optimal value of these hyper-parameters on other datasets.



\section{Sample Difficulty Promotion Details}
\label{SDP Details}
\par
\textbf{Perturbation Direction Calculation.} In the main paper, we calculate the perturbation $\delta$ by maximizing the sample difficulty, which is closely related to the loss. However, there are two loss terms, i.e., the Kullback-Leibler (KL) loss and the feature alignment (FA) loss in the fine-tuning process. Thus we conduct a further experiment to select the optimal loss for perturbation direction calculation. The experimental results are shown in Table~\ref{supp_table1}. We observe that the choice of loss for calculating the perturbation direction has a certain impact on the performance. Though not optimal for all settings, we choose KL+FA to calculate perturbation direction since it shows the best in most settings.


\begin{table}[h]   
\begin{center}   
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c c}   
\hline 
Dataset & Model & Bit-width & KL & FA & KL+FA \\ \hline
\multirow{2}*{Cifar-10}& \multirow{2}*{ResNet-20} & W4A4 & 92.43 & 92.29 & 92.36 \\ 
\multirow{2}*{}& \multirow{2}*{} & W3A3 & 88.29 & 87.68 & 88.34 \\ \hline
\multirow{2}*{Cifar-100}& \multirow{2}*{ResNet-20} & W4A4 & 66.69 & 66.50 & 66.68 \\ 
\multirow{2}*{}& \multirow{2}*{} & W3A3 & 55.61 & 55.13 & 55.67 \\ \hline
\multirow{2}*{ImageNet}& \multirow{2}*{ResNet-18} & W4A4 & 66.90 & 66.69 & 66.91 \\ 
\multirow{2}*{}& \multirow{2}*{} & W3A3 & 51.06 & 50.87 & 51.15  \\ \hline
\end{tabular} 
}
\caption{Performance of our HAST when calculating perturbation direction with diferent losses. We maximize the gradient of KL, FA and KL+FA respectively to calculate perturbation direction.}
\label{supp_table1} 
\end{center}   
\end{table}


\par
\textbf{loss weights.} We apply sample difficulty promotion to the synthetic samples obtained by hard sample synthesis for more difficult samples. Then both of them are used to fine-tune the quantized model with the same loss weights. Further experiments on the loss weights of the original synthetic samples and the promotional samples are conducted. Experimental results are shown in table~\ref{supp_table2}. The loss weight of the original synthetic samples is denoted as $a$, and that of the promotional samples is denoted as $b$. We perform 3-bit quantization on CIFAR-10 and ImageNet. For CIFAR-10, we achieve the best accuracy of 88.34\% by setting both the weights to 1. When it comes to ImageNet, better performance than that reported in the main paper is obtained by increasing the weight of promotional samples.

\begin{table}[ht]   
\begin{center}   
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c c}   
\hline 
\multirow{2}{*}{$a,b$} & ResNet-20 & ResNet-18 & \multirow{2}{*}{$a,b$} & ResNet-20 & ResNet-18  \\ \cline{2-3} \cline{5-6}
 & Cifar-10 & ImageNet &  & Cifar-10 & ImageNet  \\ \hline
 1,0 & 86.17 & 47.94 & 0,1 & 88.19 & 48.55 \\
 3,1 & 85.92 & 50.52 & 1,4 & 86.69 & 52.14 \\
 2,1 & 87.53 & 50.97 & 1,3 & 86.94 & 53.12 \\
 1,1 & 88.34 & 51.15 & 1,2 & 87.73 & 52.69 \\ \hline
\end{tabular} 
}
\caption{Ablation results of loss weights in W3A3 setting. The loss weights of original synthetic samples and promotional samples are denoted as $a,b$ respectively.}
\label{supp_table2} 
\end{center}   
\end{table}

\section{Feature Alignment Analysis}
\par 
\textbf{Direct feature alignment vs. relaxed feature alignment.} Direct feature alignment~\cite{featurealignment} is an easy and effective way to transfer feature representations by directly using mean square error to align the feature. However, we use attention vector~\cite{AttentionTransfer} to relax the feature alignment constraint due to the limited capacity of quantized model. In this section, we provide the performance comparison of our HAST between using direct feature alignment (DFA) and using relaxed feature alignment (RFA). Table~\ref{supp_table3} shows the experimental results. The relaxed feature alignment obtains better performance in any settings over direct feature alignment. Significant improvements can be observed from 3-bit quantization. This shows that it is harmful for low-precision quantized model to learn the feature representations of full-precision model directly.

\begin{table}[ht]   
\begin{center}   
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c}   
\hline 
Dataset & Model & Bit-width & HAST(DFA) & HAST(RFA) \\ \hline
\multirow{2}*{Cifar-10}& \multirow{2}*{ResNet-20} & W4A4 & 91.99 & 92.36 \\ 
\multirow{2}*{}& \multirow{2}*{} & W3A3 & 83.92 & 88.34 \\ \hline
\multirow{2}*{Cifar-100}& \multirow{2}*{ResNet-20} & W4A4 & 66.53 & 66.68 \\ 
\multirow{2}*{}& \multirow{2}*{} & W3A3 & 51.50 & 55.67 \\ \hline
\multirow{2}*{ImageNet}& \multirow{2}*{ResNet-18} & W4A4 & 66.49 & 66.91 \\ 
\multirow{2}*{}& \multirow{2}*{} & W3A3 & 45.52 & 51.15 \\ \hline
\end{tabular} 
}
\caption{Performance of our HAST with direct feature alignment and relaxed feature alignment.}
\label{supp_table3} 
\end{center}   
\end{table}



\begin{figure*}[ht]
\centering
\begin{minipage}[b]{\linewidth}
        \centering
        \includegraphics[width=2.1in]{parameter_gamma.pdf}
        \includegraphics[width=2.1in]{parameter_epsilon.pdf}
        \includegraphics[width=2.1in]{parameter_lambda.pdf}
\end{minipage}
\vspace{-8mm}
\caption{Sensitivity analysis on hyper-parameters. We report the top-1 accuracy of 3-bit ResNet-18 on ImageNet.}
\label{supp_fig1}
\end{figure*}


\begin{figure*}[ht]
\centering
    \subfloat[Gradient cosine similarity.]{
        \label{supp_fig2.a}
        \includegraphics[width=1.6in]{gradient_cosine_similiarity.pdf}
    }
    \quad
    \subfloat[Distribution of eigenvalues.]{
        \label{supp_fig2.b}
        \includegraphics[width=1.6in]{density_eigenvalue_of_CE.pdf}
        \includegraphics[width=1.6in]{density_eigenvalue_of_KL.pdf}
        \includegraphics[width=1.6in]{density_eigenvalue_of_FA.pdf}
    }
\caption{Further experiments on feature alignment. (a)Gradient cosine similarity of two terms in loss function. (b)Distribution of the eigenvalues for different loss.}
\label{supp_fig2}
\end{figure*}

\par 
\textbf{Cooperation with KL.} Gradient cosine similarity was used in~\cite{AIT} to measure the cooperation ability of multiple loss terms. The authors found that the cross-entropy (CE) loss does not work well with the Kullback-Leibler (KL) loss in network fine-tuning process. We apply this metric in our work. Specifically, we fine-tune the 3-bit ResNet-20 using baseline (CE+KL)~\cite{IntraQ} and our HAST (FA+KL) respectively and measure the cosine similarity of the gradient of two distinct loss terms. As shown in Figure~\ref{supp_fig2.a}, the cosine distance between CE and KL takes negative values throughout the fine-tuning, while that of FA+KL is positive. This implies that the combinations of FA and KL cooperate well, and using them together could enhance each other, which is opposite to the combinations of CE and KL.

\par \textbf{Generalizability.}  Hessian matrix was used in~\cite{AIT} to measure the local curvature of the loss surface and compare the generalizability of the two distinct loss terms. Since Hessian matrix itself is enormous in size and computations involving its entirety is considered almost infeasible, analyzing the eigenvalues of the matrix is often the most preferred way to study its characteristics. Figure~\ref{supp_fig2.b} plots the distribution of the eigenvalues of the Hessian matrix, approximated by PyHessian~\cite{PyHessian}. We separate Hessian calculation for each loss of CE, KL and FA. A huge difference in the local curvature of the loss terms can be observed. While CE has longer tail for high eigenvalues, KL and FA has more concentration to lower eigenvalues, which means the local curvature of loss surface of KL and FA is smaller than that of CE, leading to better genrealizability according to the finding that  smaller local curvature improves generalization~\cite{AIT}.

\section{Results with smaller number of samples}
Table~\ref{supp_table4} shows the ablation on amount of the synthetic samples. The performance drops as the number of samples decreases. However, HAST with only 256 samples still performs better than previous methods, such as IntraQ with 45.51\% using 5120 samples.


\begin{table}[ht]
\centering
    \resizebox{0.45\textwidth}{!}{
    \large
    \begin{tabular}{*{10}{c}}
        \toprule
        Amount & IntraQ(5120) & 256 &  1280 & 2560 & 5120 \\
        \midrule
        ACC(\%) & 45.51 & 49.17 & 49.95 & 50.23 & 51.15 \\
        \bottomrule
    \end{tabular}
    }
\caption{Results with smaller number of samples.}
\label{supp_table4}
\end{table}