\section{Methodology}
\label{sec:method}
\par In this section, we first introduce the process of zero-shot quantization (ZSQ). Then, we detail the proposed \textbf{HA}rd sample \textbf{S}ynthesizing and \textbf{T}raining (HAST) scheme containing three parts: \emph{hard sample synthesis}, \emph{sample difficulty promotion}, and \emph{feature alignment} ensuring the similarity of representations from quantized models and full precision models. The overview of HAST is illustrated in Figure~\ref{Fig3}.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{overview.png}
\vspace{-3mm}
\caption{An overview of the proposed hard sample synthesizing and training (HAST) scheme. The hard sample synthesis focuses on generating hard samples in the data generation process. The sample difficulty promotion makes synthetic samples hard to fit. The feature alignment guides the quantized model to handle hard samples.}
\vspace{-4.9mm}
\label{Fig3}
\end{figure}


\subsection{Preliminaries}
\label{Preliminaries}
\par \textbf{Quantizer.} We focus on the asymmetric uniform quantizer to implement network quantization in this work, following~\cite{GDFQ, IntraQ}. Denoting $\theta$ as weights of a full-precision model, $l$ and $u$ as the lower and upper bound of $\theta$, and $n$ as the bit-width, the quantizer produces the quantized integer $\theta^q$ as follows:
\begin{equation}
\begin{aligned}
\label{eq1} \theta^q=\lfloor\theta\times S -z\rceil, 
\ 
S=\frac{2^n-1}{u-l}, \ 
z=S \times l+ 2^{n-1}
\end{aligned}
\end{equation}
where $S$ is the scaling factor to convert the range of $\theta$ to $n$-bit, $z$ decides which quantized value zero is mapped to, and ``$\lfloor\rceil$'' is an operation used to round its input to the nearest integer. Accordingly, the corresponding dequantized value can be calculated as:
\begin{equation}
\begin{aligned}
\label{eq2} \theta^{\prime}=\frac{\theta^q+z}{S}.
\end{aligned}
\end{equation}
Built upon the quantizer, the weights of full-precision models can be represented using low-bit integers. Consequently, the quantized model performs inference using the dequantized parameter $\theta^{\prime}$, which is typically paired with considerable performance degradation.



\textbf{Zero-Shot Quantization.} To mitigate the performance degradation, a widely adopted approach is to optimize $\theta$ such that its dequantized parameter $\theta^{\prime}$ can perform well on test sets~\cite{PACT}. This is typically achieved by optimizing $\theta$ using the data for training full-precision models~\cite{LSQ}, since $\theta^{\prime}$ is inherently the function of $\theta$. Zero-Shot Quantization (ZSQ) takes a step further to quantize models when data for training full-precision models are inaccessible.

In ZSQ, synthetic samples are usually employed for optimizing quantized models~\cite{GDFQ,ZeroQ}. The synthetic samples can be derived by noise optimization~\cite{ZeroQ,DSG,IntraQ}, which is typically instantiated by distribution approximation~\cite{ZeroQ,GDFQ}. Given a set of noise $\{\mathbf{x}_{i} \}^{N}_{i=1}$, synthetic samples are obtained by optimizing these noise to match the batch normalization statistics (BNS):
\begin{equation}
\begin{aligned}
\label{eq3} 
\min_{\{\mathbf{x}_{i} \}^{N}_{i=1}} \mathcal{L}_{BNS}
\triangleq
\frac{1}{L}
\sum_{l=1}^{L}
(
&||\mu^{l}(\theta) - 
\mu^{l}(\theta, \{\mathbf{x}_{i} \}^{N}_{i=1})|| \\
+
&||\sigma^{l}(\theta) - 
\sigma^{l}(\theta, \{\mathbf{x}_{i} \}^{N}_{i=1})||
),
\end{aligned}
\end{equation}
where $\mu^{l}(\theta)/\sigma^{l}(\theta)$ are mean/variance parameters stored in the $l$-th BN layer of full-precision model parameterized with $\theta$ and $\mu^{l}(\theta, \{\mathbf{x}_{i} \}^{N}_{i=1})/\sigma^{l}(\theta, \{\mathbf{x}_{i} \}^{N}_{i=1})$ are mean/variance parameters calculated on the sampled noise using $\theta$. Besides the BNS alignment objective function, an inception loss is also employed for optimizing sampled noise:
\begin{equation}
\begin{aligned}
\label{eq4} 
\min_{\{\mathbf{x}_{i} \}^{N}_{i=1}} \mathcal{L}_{IL}
\triangleq
\frac{1}{N}
\sum_{i=1}^{N}
CE(p(\mathbf{x}_i;\theta), \mathbf{y}_i)
,
\end{aligned}
\end{equation}
where $p(\cdot;\theta)$ stands for the probability predicted by full-precision model parameterized with $\theta$, $CE(\cdot,\cdot)$ represents the cross-entropy loss, and $\mathbf{y}_i$ is the label assigned to $\mathbf{x}_i$ as a prior classification knowledge. Consequently, synthetic data are obtained by optimizing the final objective function composed of these two terms:
\begin{equation}
\begin{aligned}
\label{eq5} 
\min_{\{\mathbf{x}_{i} \}^{N}_{i=1}} \mathcal{L}_{FNL}
\triangleq
\mathcal{L}_{BNS} + \beta\mathcal{L}_{IL}
,
\end{aligned}
\end{equation}
where $\beta$ is a hyper-parameter balancing the importance of two terms. Built upon the objective function $\mathcal{L}_{FNL}$, ZSQ can fine-tune the model parameter $\theta$ such that its dequantized parameter $\theta^{\prime}$ can perform well on synthetic samples. More specifically, the quantized network is usually fine-tuned by a teacher-student framework with the cross-entropy loss $CE$ and the Kullback-Leibler loss $KL$:
\begin{equation}
\begin{aligned}
\label{eq6} 
\min_{\theta^{\prime}}
\frac{1}{N}
\sum_{i=1}^{N}
CE(p(\mathbf{x}_i;\theta^{\prime}),
\mathbf{y}_i)
+
\alpha
KL(
p(\mathbf{x}_i;\theta)||
p(\mathbf{x}_i;\theta^{\prime})
),
\end{aligned}
\end{equation}
where $\alpha$ is a hyper-parameter balancing the importance of two terms. We use $\theta^{\prime}$ to represent the parameter of quantized models and optimize $\theta^{\prime}$. This is equal to optimizing $\theta$, as $\theta^{\prime}$ is a function of $\theta$ according to Eq.~(\ref{eq2}).


\subsection{General Scheme of Proposed Methods}

The detailed procedure of the proposed HAST scheme is summarized in Algorithm~\ref{alg:algorithm1}. HAST consists of three parts: hard sample synthesis, sample difficulty promotion, and feature alignment that is illustrated in Sec.~\ref{sec:method}.

In the process of sample synthesis, we start with a batch of random input $\mathbf{x}_i$ sampled from a standard Gaussian distribution, following~\cite{IntraQ}. The proposed hard sample synthesis, i.e., Eq. (\ref{HFNL}), optimizes $\mathbf{x}_i$, aiming to increase the sample difficulty measured with full-precision models. As shown in the top of Figure \ref{Fig3}, we feed $\mathbf{x}_i$ into the full-precision model and optimize it using the proposed hard-sample-enhanced final loss for synthesizing hard samples, i.e., Eq.~(\ref{HFNL}) by computing the BNS alignment loss (Eq.~(\ref{eq3})) and the hard-sample-enhanced inception loss (Eq.~(\ref{hil})). 

In the process of network fine-tuning, we increase the sample difficulty on the fly using Eq.~(\ref{SDP}), as shown in the bottom left of Figure~\ref{Fig3}. Then, we perform feature alignment to ensure the similarity between the full-precision and quantized models, as shown in the bottom right of Figure~\ref{Fig3}.

\begin{algorithm}[t]
	\caption{Hard Sample Synthesizing and Training for Zero-shot Quantization.}
	\label{alg:algorithm1}
	\textbf{Input:}{Pretrained full-precision model with parameter $\theta$; Initial synthetic dataset $D_s=\varnothing$; Number of synthetic images $N$; Generation iterations $T_{generate}$; Fine-tuning iterations $T_{fine-tune}$.}\\
	\textbf{Output:}{Low precision quantized model with parameter $\theta^{\prime}$.}
	\begin{algorithmic}
	\WHILE{$|D_s|\leq{N}$}
	\STATE{Sample batch of Gaussian noise $\mathbf{x}_i$ and label $y$;}
	\FOR{$t=1,...,T_{generate}$}
	\STATE{Optimize $\mathbf{x}_i$ by minimizing Eq.~(\ref{HFNL});}
	\ENDFOR
	\STATE $D_s \gets D_s\cup{\mathbf{x}_i}$
	\ENDWHILE
	\STATE{Get Synthetic dataset $D_s$.}\\
	\FOR{$t=1,...,T_{fine-tune}$}
	\STATE{Sample batch $\mathbf{x}_i$ from synthetic dataset $X$;}
	\STATE{Compute the perturbation $\delta$ by Eq.~(\ref{SDP});}
	\STATE{Update quantized model by minimizing Eq.~(\ref{eq:FAR});}
	\ENDFOR
	\STATE{Get Converged quantized model.}
	\end{algorithmic}
\end{algorithm}

\subsection{Hard Sample Synthesis}
\label{HSS}
\par Inspired by our experimental results in Figure~\ref{Fig2}, we propose to reconsider the difficulty of synthetic samples. Specifically, we increase the importance of hard samples while suppressing the importance of easy-to-fit samples, employing the GHM introduced in~\cite{GHM} to measure the sample difficulty $d$ of $\mathbf{x}$:
\begin{equation} \label{eq111}
d(\mathbf{x}, \theta) = 1 - p_{\mathbf{y}}(\mathbf{x}, \theta)
\end{equation}
where $p_{\mathbf{y}}(\mathbf{x})$ is the probability on label $\mathbf{y}$ predicted by the model parameterized with $\theta$. Through Eq.~\eqref{eq111}, we hope models focus more on transferable components~\cite{What_Transferred_Dong_CVPR2020}.

Built upon the sample difficulty $d(\mathbf{x}, \theta)$, we propose a hard-sample-enhanced inception loss $\mathcal{L}_{HIL}$ to synthesize samples:
\begin{equation}
\begin{aligned}
\label{hil} 
\min_{\{\mathbf{x}_{i} \}^{N}_{i=1}} \mathcal{L}_{HIL}
\triangleq
\frac{1}{N}
\sum_{i=1}^{N}
d(\mathbf{x}_i, \theta)^\gamma
CE(p(\mathbf{x}_i;\theta), \mathbf{y}_i)
\end{aligned}
\end{equation}
where the hyper-parameter $\gamma$ controls how strongly the importance of hard samples is enhanced. The hard-sample-enhanced inception loss $\mathcal{L}_{HIL}$, i.e., Eq.~(\ref{hil}), pays more attention to hard samples in the synthesis process than the original inception loss $\mathcal{L}_{IL}$. Figure~\ref{Fig4} shows the curves of $\mathcal{L}_{IL}$ and $\mathcal{L}_{HIL}$. As sample difficulty decreases, loss drops and finally converges to zero. However, the sample optimized using $\mathcal{L}_{HIL}$ (0.7) is more difficult than that using $\mathcal{L}_{IL}$ (0.2). Consequently, the fraction of hard samples will increase, as depicted in Figure~\ref{Fig2.c}, where samples are synthesized by optimizing the hard-sample-enhanced loss:
\begin{equation}
\begin{aligned}
\label{HFNL} 
\min_{\{\mathbf{x}_{i} \}^{N}_{i=1}} \mathcal{L}_{HFNL}
\triangleq
\mathcal{L}_{BNS} + \beta\mathcal{L}_{HIL}
.
\end{aligned}
\end{equation}

\subsection{Sample Difficulty Promotion}
\label{sec:SDP}
Samples synthesized using Eq.~(\ref{HFNL}) are hard to fit for the full-precision model parameterized with $\theta$, but their difficulties for the quantized model $\theta^{\prime}$ are lacking. This stems from the fact that the difficulty used in Eq.~(\ref{HFNL}) is measured with $\theta$ using $d(\mathbf{x}_i, \theta)$. Therefore, we propose to promote the sample difficulty using the quantized model parameterized with $\theta^{\prime}$.
%, i.e., $d(\mathbf{x}_i, \theta^{\prime})$.

The sample difficulty promotion is different from the process of hard sample synthesis. To be specific, the latter utilizes full-precision models to synthesize samples for training quantized models, while the former leverages quantized models to enhance the sample difficulty for training themself. If the synthetic samples are changed in the process of sample difficulty promotion, the synthesized samples would be both easy and difficult for quantized models. Thus, we draw inspiration from adversarial training~\cite{FGSM,zhang2021causaladv}, where samples are modified to be hard to fit on the fly.

More specifically, we increase the sample difficulty $d(\mathbf{x}_i, \theta^{\prime})$ on the fly to make synthetic samples hard to fit by introducing a perturbation $\mathbf{\delta}_i$ for $\mathbf{x}_i$ in each training iteration:
\begin{equation}
\begin{aligned}
\label{SDP}
\mathbf{\delta}_i
=
\arg \max_{||\delta^{\prime}||_{\infty} \leq \epsilon}
d(\mathbf{x}_i + \delta_i^{\prime}, \theta^{\prime}),
\end{aligned}
\end{equation}
where $||\cdot||_{\infty}$ represents the $\ell_{\infty}$-norm and $\epsilon$ controls the strength of perturbation. We attach the perturbation $\delta_i$ for $\mathbf{x}_i$ to find nearby difficult samples with a larger sample difficulty in each iteration. 

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{Hard_Sample_Synthesis.pdf}
\vspace{-4mm}
\caption{Curves of $\mathcal{L}_{IL}$ and $\mathcal{L}_{HIL}$.}
\vspace{-7mm}
\label{Fig4}
\end{figure}

\subsection{Minimizing Quantization Gap by Feature Alignment}
\label{FA}
Built upon the proposed hard sample synthesis and sample difficulty promotion, we are ready to train quantized models using hard samples. To ensure the quantized model performs the same as its full-precision model, an ideal result may be that the outputs of these two models (including intermediate features) are kept exactly the same, which shares the same spirit with~\cite{zhang2021causaladv} built upon a causal perspective. Along with this insight, we can align the features of the full-precision and quantized models:
\begin{equation}
\begin{aligned}
\label{eq:FA}
\min_{\theta^{\prime}}
\mathcal{L}_{FA}
\triangleq
\frac{\lambda}{NL}
\sum^{N}_{i=1} \sum^{L}_{l=1}
\phi^l(
f^l(\mathbf{x}_i + \delta_i;\theta), 
f^l(\mathbf{x}_i + \delta_i;\theta^{\prime})
),
\end{aligned}
\end{equation}
where $\phi^l(\cdot, \cdot)$ is a metric for the $l$-th layer measuring the difference of its inputs, e.g., mean square error, $f^l(\cdot;\theta)$ represents the feature at the $l$-th layer of a model parameterized with $\theta$, and the hyper-parameter $\lambda$ adjusts the order of magnitude of the loss.

The quantized models may not be able to match the outputs of full-precision models due to the limited model capacity. Thus, we relax the feature alignment constraint:
\begin{equation}
\begin{aligned}
\label{eq:FAR}
\min_{\theta^{\prime}}
\hat{\mathcal{L}}_{FA}
=
\frac{\lambda}{z}
\sum_{i , l \in S
}
& (
\phi(
f^l(\mathbf{x}_i + \delta_i;\theta), 
f^l(\mathbf{x}_i + \delta_i;\theta^{\prime})
) \\
+ & \alpha KL(p(\mathbf{x}_i; \theta) || p(\mathbf{x}_i; \theta^{\prime}))
)
,
\end{aligned}
\end{equation}
where $z=N |S|$, $S$ is the set of selected intermediate layers, $\phi(\cdot,\cdot)$ is the metric used for all intermediate layers, and $KL$ divergence is employed as the metric for the final output layer, i.e., the predicted probability. For the metric used in intermediate layers, we instantiate them with an attention vector~\cite{AttentionTransfer} rather than a mean square error ($|| f^l(\mathbf{x}_i + \delta_i;\theta) - f^l(\mathbf{x}_i + \delta_i;\theta^{\prime})||^2$). The metric of intermediate feature $f^l(\cdot;\theta) \in \mathbb{R}^{C \times W \times H}$ ($C, W$, and $H$ is the number of channels, width, and height) is calculated using an attention vector $Att(\cdot)$ as follows:
\begin{equation}
\begin{aligned}
\label{eq:att1} 
\phi(
f^l(\cdot;\theta), 
f^l(\cdot;\theta^{\prime})
) 
=
||Att(f^l(\cdot;\theta)) - Att(f^l(\cdot;\theta^{\prime}))||^2_2,
\end{aligned}
\end{equation}
where $Att(f^l(\cdot;\theta)) \in \mathbb{R}^{C}$ is a vector with the $c$-th element calculated as:
\begin{equation}
\label{eq:att2} 
Att(f^l)(c) = \sum_{w=1}^{W} \sum_{h=1}^{H} f^l(c,w,h)^2,
\end{equation}
where we denote $f^l(\cdot;\theta)$ by $f^l$ for simplicity.





