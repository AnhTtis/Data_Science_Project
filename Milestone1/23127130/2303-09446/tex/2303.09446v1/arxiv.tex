

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{enumitem}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}
 
\usepackage[accepted]{arxiv}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\fzero}{F\textsubscript{0}\xspace}
\newcommand{\com}[1]{\textcolor{red}{#1}}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Controlling High-Dimensional Data With Sparse Inputs}

\begin{document}

\twocolumn[
% \icmltitle{Controlling Prosody Through Multivariate Time-Series Imputation}
% \icmltitle{Multivariate Time-Series Imputation for Sparse Human-in-the-Loop Control}
\icmltitle{Controlling High-Dimensional Data With Sparse Input}

\begin{icmlauthorlist}
\icmlauthor{Dan Andrei Iliescu}{cam}
\icmlauthor{Devang Savita Ram Mohan}{papercup}
\icmlauthor{Tian Huey Teh}{papercup}
\icmlauthor{Zack Hodari}{papercup}
\end{icmlauthorlist}

\icmlaffiliation{cam}{Department of Computer Science, University of Cambridge, UK. Work done during and internship at Papercup.}
\icmlaffiliation{papercup}{Papercup Technologies Ltd, London, UK.}

\icmlcorrespondingauthor{Dan Andrei Iliescu}{dai24@cam.ac.uk}
\icmlcorrespondingauthor{Zack Hodari}{zack@papercup.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We address the problem of human-in-the-loop control for generating highly-structured data.
This task is challenging because existing generative models lack an efficient interface through which users can modify the output. 
Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels.
To solve this, we introduce a novel framework whereby an encoder maps a sparse, human-interpretable control space onto the latent space of a generative model. 
We apply this framework to the task of controlling prosody in text-to-speech synthesis. 
We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. 
We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. 
With even a very small number of input values (\raisebox{0.5ex}{\texttildelow}4), MICVAE enables users to improve the quality of the output significantly, in terms of listener preference (4:1).
\end{abstract}

\section{Introduction}
\label{intro}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{intro.pdf}}
    \caption{We propose a framework for controlling prosody in text-to-speech systems based on missing value imputation on sparse inputs from the user. Control with sparse inputs is efficient, robust and faithful.}
    \label{fig:scope}
    \end{center}
    \vskip -0.2in
\end{figure}

Many applications of deep learning are either supported by human effort or designed as tools for humans \citep{wu2022survey}. However, for models that generate highly-structured data---such as text, speech, or images---humans cannot easily fix or change the model's output. This is because the features in high-dimensional data are strongly correlated, so changing one requires changing many others. To allow human intervention, models must combine human intuition with statistical patterns learned by deep models.

Controlling generative models is an active area of research \citep{conditional-VAE:2015, mirza2014conditional, winkler2020learning}. Existing control methods fall on a spectrum from fine-grained to coarse-grained. For example, in-painting for image generation \citep{rombach2022high} represents fine-grained control: users provide information relating to the precise location in an image. In contrast, prompting of generative language models \citep{ouyang2022training} is more coarse-grained as it impacts more global changes in the model's output.

We propose that sparse inputs are an effective solution for fine-grained control of highly-structured data. When forced to provide a full set of conditioning inputs, the user may choose values that are imprecise or self-contradicting. Instead, we propose a framework whereby the user provides a few input values and the model fills in the rest.

Our approach to this formulation of the problem is the Multiple Instance Conditional Variational Autoencoder (MICVAE). This model is uniquely suited to conditioning on the sparse values provided by a human user. We will refer to the values provided by the user as ``driving'' values, since they are the interface through which the user controls the model.

Since we are the first to propose a solution to HitL control by conditioning on sparse inputs, we formulate some requirements for what constitutes a successful model. These traits are used to inform both the design of our proposed model and the evaluation methods.
\begin{enumerate}[itemsep=2pt,parsep=2pt,partopsep=0pt]
    \item \textbf{Efficiency.} The user should be able to achieve the desired output with only a few input values. We measure efficiency as the improvement brought to the output by each additional driving value.
    \item \textbf{Robustness.} The model should allow users to control the output in a predictible way regardless of the pattern in which users choose to drive values. Users may want to provide any values in any pattern. The model should generate realistic outputs under any conditions.
    \item \textbf{Faithfulness.} The output should agree with the driving values; i.e. the model should generate a valid datapoint that is closest to the driving values provided by the user.
\end{enumerate}

In particular, we explore sparse HitL control for the task of prosody generation. Prosody is a key component in human speech as it can convey emotion, attitude, a speaker's intentions, and other communicative functions \citep[Chapter~2]{zack-PhD-thesis:2022}. Prosody is communicated through perceived changes in intonation, loudness, phrasing, timing, and voice quality (e.g. creaky voice) \citep{prosody-review:2010}. These perceptual factors are hard to measure, and so are typically approximated using acoustic correlates, such as: fundamental frequency (\textbf{\fzero}; correlate of intonation), \textbf{energy} (correlate of loudness), and phoneme and silence \textbf{durations} (correlate of phrasing and timing) \citep{PaperCup-Ctrl-P:2021}.

These three acoustic correlates can be extracted automatically from the speech signal and they form the basis for the control space of our model. We refer to them as Prosodic Acoustic Features (PAFs).

Sparse human-in-the-loop control adds a priori human domain expertise on top of statistical patterns learned directly from the data. Text-to-speech models can benefit greatly from human intuition and knowledge when it comes to choosing the right rendition for a particular sentence. Different possible renditions of a single utterance can be seen in Figure~\ref{fig:demo}. However, existing conditioning paradigms are either imprecise or require manual completion of inputs that can be more easily done by a statistical model.

We design a range of quantitative evaluation procedures to validate that MICVAE is an effective method for HitL control. Firstly, we introduce a procedure called iterative refinement as a proxy for human control in order to evaluate efficiency. Next, we demonstrate the robustness of MICVAE to different patterns of sparsity by comparing with an equivalent CVAE model that does not use the Multiple-Instance encoder. Finally, we measure faithfulness through subjective listening tests to assess the perceived change in prosody resulting from a small number of driving PAFs. 

We present the following contributions:
\begin{itemize}[itemsep=2pt,parsep=2pt,partopsep=0pt]
    \item We propose MICVAE, a novel probabilistic generative model for sparse of human-in-the-loop control that is robust to differing patterns of sparsity.
    \item We demonstrate that using sparsity as an interface for control leads to efficient and faithful HitL control.
    \item We introduce iterative refinement, a novel automated evaluation procedure designed to simulate HitL control of generative models.
\end{itemize}

\section{Related Work}

\subsection{Human-in-the-loop Control}

Research combining human effort with machine learning is increasing rapidly \citep{wu2022survey}. This is in part because the development of deep learning models is an iterative process \citep{xin2018accelerating}.
Iterative labelling is one way to speed up this iteration using human effort, where humans label only the most informative data before re-training the model and repeating \citep{chai2020human}.
The human-in-the-loop can be responsible for quality control; i.e.\ correcting mistakes made by the model. Alternatively, they may provide their preferred solution---important for generation tasks where there is no single right answer, such as text, speech, and image generation.

Unfortunately, there is limited literature on the use of humans for supporting models at inference time. This is likely because doing so is expensive and reserved for revenue-generating products, something about which companies publish fewer details. Despite this, there are many examples of companies using HitL to support inference (and likely closing the loop using iterative labeling), such as Lilt, Rev, Verbit, Unbabel, and Speechmatics.

However, in the case of highly structured data, it is prohibitively costly for humans to correct or create outputs manually.
For example, a human could not easily correct the reflection on a surface in a generated image.
Currently there are a few approaches to resolve this: one is to simply collect more data at additional expense, or to have humans undertake the slow and imperfect task of fixing mistakes in predictions of highly structured data. In computer vision, inpainting provides a solution \citep{Elharrouss2019ImageIA}.

\subsection{Prosody}
Prosody serves many communicative purposes: augmenting the lexical content \citep{calhoun:information-structure:2010,trang-thesis:2020,grounding:1991} and conveying additional meaning \citep{turn-taking:2009,prosody-irony:2011,prosody-humour:2017,joint-prosody-semantics:emotion:2016}.
Prosody is communicated through suprasegmental changes in speech. This perceived variation includes intonation, loudness, timing, and voice quality. These perceptual correlates are often modelled with acoustic correlates such as \fzero, energy, durations, and spectral tilt \citep{CHiVE:2019,aggregated-prosody:2019,fastspeech-2:2020,raitio20-interspeech,PaperCup-Ctrl-P:2021}.

Prosody prediction is a challenging task as we lack the relevant context information that humans use to plan prosodic choices \citep{context-definition:1992,prosody-review:2010}. Furthermore, even if we were to provide additional contextual information, there may be no single best prosodic rendition for a given situation \citep{zack-SSW19:2019}.

\subsection{Interpretable representation learning}
Learnt representations can be useful for a variety of down-steam tasks \citep{bommasani2021opportunities}. Many representation learning methods have been proposed specifically for speech \cite{VQ-VAE:2017,oord2018representation,wav2vec-2:2020,hsu2021hubert} and prosody \citep{tacotron-GST:2018,fast-DCTTS:2021,zack-ICASSP21:2021,Karlapati2022,babianski2022granularity}.

However, not all representations are amenable for human interaction. Various methods can be applied to encourage interpretability \citep{bengio2013representation,Tschannen2018RecentAI}. Structure can be imposed on the latent variables through independence assumptions \citep{prosody-control:VAE-MI:2021} and hierarchical assumptions \citep{Hsu2017UnsupervisedLO,hierarchical-VAE:2019}. Self-supervised losses can also be used to disentangle concepts \citep{prosody-control:VAE-3dim:2020,choi2022nansy}. Discrete latent variables are another useful method to make representations more usable for humans \citep{VQ-VAE:2017,zack-SP20:2020,proso-speech:2022}.

Instead of striving for interpretable representations, it is also possible to focus on the requirements of specific down-stream tasks. For prosody control, categorical control of representations has been successful in providing an interface for human interaction, such as for emotions \citep{schroder:2001,yamagishi-emotion:2004,gustav-GMMQ-VAE:2018} and styles \citep{amazon-newscaster:2019,kim21n_interspeech,shin22b_interspeech}.

\subsection{Conditioning Generative Models}
Generative models are a broad class of probabilistic models known for their ability to produce realistic unconditional samples for text \citep{brown2020language}, images \citep{rombach2022high,ramesh2022hierarchical}, and speech \citep{VQ-VAE:2017}.

For many applications, conditional sampling---i.e.\ control---is necessary. Fortunately, variational inference provides the tools to derive conditional generative models, for example: conditional variational autoencoders \citep{conditional-VAE:2015}, conditional generative adversarial networks \citep{mirza2014conditional}, and conditional normalising flows \citep{winkler2020learning}.

Conditional generation has made great strides using text prompts for conditional sampling \citep{ouyang2022training,ramesh2022hierarchical,kim21n_interspeech}. Image in-painting is another form of conditional sampling, where part of an image is occluded and the available portions act as the conditioning information \citep{yu2018generative,ramesh2022hierarchical}.

\section{Multiple-Instance CVAE}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{model.pdf}}
    \caption{Our model, MICVAE, takes in sparse information, encodes it in a latent space, and decodes to a fully-specified output. The input and output belong to the space of prosodic acoustic features (PAFs), with 3 values (\fzero, energy and duration) for each phoneme in the sentence. The autoencoder is generates prosody conditionally on a few auxiliary labels, namely phoneme, style, and speaker embeddings.}
    % \com{This diagram is top to bottom, Figure~\ref{fig:encoder} is bottom to top. Fix one to be consistent}
    % \com{Doesn't include speaker embedding or style embedding}
    \label{fig:model}
    \end{center}
    \vskip -0.2in
\end{figure}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{encoder.pdf}}
    \caption{Our encoder uses self-attention to aggregate the information from the $K$ driving PAFs in order to produce the latent vector $\mathbf{z}$ without the need for masking. Each driving PAF $\mathbf{x}_k$ is concatenated with two positional encodings: $\mathbf{p}_k$ encodes the phone position and $\mathbf{f}_k$ encodes the feature stream.}
    \label{fig:encoder}
    \end{center}
    \vskip -0.2in
\end{figure}

In order to enable users to perform sparse control for highly-structured data, we propose a novel generative model. Our model, called Multiple Instance Conditional Variational Autoencoder (MICVAE), sits within the paradigm of the Conditional VAE \citep{conditional-VAE:2015}. The high-level architecture is illustrated in Figure~\ref{fig:model}, where user driving values are encoded into a latent space and then decoded conditional on phone embeddings into a full PAF sequence.

MICVAE's encoder is designed specifically for sparse control. To support HitL control of highly structured data, MICVAE must be capable of dealing with sparse information to allow for efficient control. This is achieved by embedding the sparse driving PAFs independently and combining them into a sentence-level latent space using self-attention. In other words, we represent the $K$ driving values as a bag-of-features.

MICVAE should be faithful to the intention of the user where possible. However, manually specifying values for highly-structured data in a consistent fashion is difficult, meaning user driven values may conflict with what is possible in reality. By using a CVAE architecture, MICVAE's probabilistic encoder will map user driven values to an approximate posterior which maximises the likelihood of the inputs. Thus, MICVAE is not blindly faithful to user driven values, but finds the point on the data manifold that is closest to the inputs. This point on the manifold can then be decoded to produce consistent PAFs.

\subsection{Multiple-Instance Encoder}

The unique component of our model is its encoder, which comprises a self-attention mechanism adapted from \citet{Ilse2018AttentionbasedDM}. The encoder architecture is illustrated in Figure~\ref{fig:encoder}.

The encoder works by separately computing a \textit{value} vector $\mathbf{v}_k \in \mathbb{R}^D$ for each driving PAF $\mathbf{x}_k \in \mathbb{R}$, where $k \in \{1:K\}$ counts the driving PAFs. The results are then aggregated into a single embedding for the entire sentence $\mathbf{z}^\prime \in \mathbb{R}^D$ using a weighted average. $\mathbf{z}^\prime$ is then projected to parameterise a latent variable. $K$ is the number of driving PAFs in the sentence, so $K \leq T \times 3$. $T$ is the number of phones in the sentence and 3 is the number of PAF dimensions (\fzero, energy and duration). The attention weights $\mathbf{a}_k \in [0, 1]$ are computed as a softmax of the \textit{key-query} products $\mathbf{b}_k$. The softmax operation ensures that the result is invariant to the number of inputs provided, and thus to the pattern of sparsity.

\begin{equation}
    \mathbf{z}^\prime = \sum_{k=1}^K \mathbf{c}_k = \sum_{k=1}^K \mathbf{a}_k \mathbf{v}_k, ~  \mathbf{a}_k = \frac{\exp(\mathbf{b}_k)}{\sum_{l=1}^K \exp(\mathbf{b}_k)}
\end{equation}
\begin{equation}
    \mathbf{z} = \mathcal{N}(\mathbf{z}_\mu, \mathbf{z}_\sigma) , \mathbf{z}_\mu = \mathbf{U}\mathbf{z}^{\prime T}, \mathbf{z}_\sigma = \mathbf{S}\mathbf{z}^{\prime T}
\end{equation}

The \textit{key-query} products $\mathbf{b}_k \in \mathbb{R}^D$ are computed in the specific way introduced by \citet{Ilse2018AttentionbasedDM}, so that we obtain an attention vector instead of an attention matrix. This is because we want our results to aggregate into a single latent vector rather than a set. To achieve this, we perform a weighted dot-product between \textit{keys} and \textit{queries} instead of a cross-product. We use a different non-linearity for each one in order to break the symmetry.

\begin{equation}
    \mathbf{b}_k = \mathbf{w}^T (\tanh(\mathbf{Qh}_k^T) \odot \mathrm{sigm} (\mathbf{Kh}_k^T))
\end{equation}

The \textit{value} vector $\mathbf{v}_k$ is also computed by passing the intermediate representation $\mathbf{h}_k \in \mathbb{R}^H$ through a perceptron:

\begin{equation}
    \mathbf{v}_k = \tanh (\mathbf{Vh}_k^T)
\end{equation}

The final piece of the puzzle is generating the intermediate representations $\mathbf{h}_k$ in a way that captures the location in the sentence and feature type of $\mathbf{x}_k$. For this, we concatenate each driving PAF $\mathbf{x}_k$ with two positional encodings, one for the location $\mathbf{p}_k \in \mathbb{R}^P$ and the other for the type of feature $\mathbf{f}_k \in \mathbb{R}^F$ (\fzero, energy or duration). The location encoding $\mathbf{p}_k$ is a sinusoidal series that follows the standard approach of \citet{Vaswani2017Attention}, while the feature encoding $\mathbf{f}_k$ consists of one of three learned vectors.

\begin{equation}
    \mathbf{h}_k = \mathrm{ReLU} (\mathbf{E} ~ [\mathbf{x}_k, \mathbf{p}_k, \mathbf{f}_k]^T)
\end{equation}

The trainable parameters of our model are the linear mappings $(\mathbf{K}, \mathbf{Q}) \in \mathbb{R}^{L \times H}, ~ \mathbf{w} \in \mathbb{R}^{D \times L}, ~ \mathbf{V} \in \mathbb{R}^{D \times H}, \mathbf{E} \in \mathbb{R}^{H \times (1+P+F)}$, and the feature encodings $\mathbf{f} \in \mathbb{R}^{F \times 3}$. We choose the following values for the dimensional hyperparameters: $H = 64, ~ D = 32, ~ L = 64, ~ F = 8, ~ P = 8$.


\subsection{Content Encoders}
In addition to the Multiple-Instance encoder used to enable sparse control, our model uses three other encoders to capture the content that is being generated. The largest of these is the phone encoder which captures the pronunciation of the words to be synthesised. Our phone encoder projects one-hot phone identity features in 384 dimensions, this is followed by 3 1-dimensional convolution banks with a kernel size of 5 and batch norm after each convolution \citep{batch-normalisation:2015}, the embeddings are then passed through a bi-directional LSTM. The final phone embeddings are 384 dimensions. Our speaker encoder simply projects the one-hot speaker identity to 32 dimensions. Our style encoder projects the one-hot style labels to 16 dimensions.

Finally, the speaker embedding and style embedding are upsampled to phone level (by repeating it) and summed with the phone embeddings to create the content embeddings.

\subsection{Prosodic Acoustic Feature Decoder}
The decoder of MICVAE generates the prosodic acoustic features (PAFs) using a sample from the approximate posterior, i.e.\ $\tilde{z} \sim \mathcal{N}(\mathbf{z}_\mu, \mathbf{z}_\sigma)$, and the content embeddings. The sample $\tilde{z}$ is upsampled to phone level and concatenated with the content embeddings, this is the input for the decoder.

Our PAF decoder has the same architecture as the acoustic feature predictor from \citet{PaperCup-Ctrl-P:2021}. Using 4 bi-directional GRU layers with dimensionality: 64, 64, 32, 32. This is followed by a 16-dimensional perceptron with a tanh non-linearity, and finally projected to 3 dimensions, which represents the number of prosodic features: \fzero, energy, and duration.


\section{Baseline: Masked CVAE}
We designed MICVAE to be robust to patterns of sparsity. However, it is hard to benchmark whether it is more or less robust to human behaviour without a baseline. We introduce Masked CVAE as a straightforward baseline for sparse control.

Masked CVAE has the same architecture as MICVAE, except with a different encoder, and thus different method of supporting sparse control.
Masked CVAE's encoder is a multi-layer Recurrent Neural Network.
In Masked CVAE, each of the three feature streams has an additional binary feature concatenated, which represents whether that value is driven or missing. Any driven value is assigned a boolean value of 1, and all other positions are set to 0. Masked CVAE's encoder takes a sequence of 6-dimensional vectors as input (as opposed to 3 for MICVAE). The additional 3 dimensions are used to indicate whether each corresponding PAF is driven or missing.

Masked CVAE is not invariant to the number of driven values like MICVAE is, thus during training it must be exposed to examples with sparse control. To train Masked CVAE we select a fixed percentage $P$ of sparsity and for each sentence during training we randomly mask out $P$\% of PAF values. In this way the model is able to support sparse control.

\section{Evaluation}

\begin{figure*}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\textwidth]{demo.png}}
    \caption{Our model produces plausible prosody while simultaneously respecting the inputs it has been given. This is in contrast to Crude AFP, which produces faithful but inconsistent prosody. This plot re-iterates the necessity for sparse control, because it shows how difficult it would be for a person to manually select the driving values in order to generate a plausible rendition. The driving PAFs are shown in green.} % \com{Show visually the missing PAFs.}
    \label{fig:demo}
    \end{center}
    \vskip -0.2in
\end{figure*}

For sparse control to be useful it must have several qualities: efficiency, robustness, and faithfulness. We design different objective and subjective evaluations to assess how MICVAE performs on these 3 criteria.

We simulate human-in-the-loop control using prosody extracted from a dataset of prosodic renditions. The ultimate use-case for our system is human-driven control, we use this automated evaluation as a proxy to 1) ensure that the experiments are reproducible and 2) enable larger scale experiments which would be costly to run using with humans.

By simulating HitL control, we are hoping to achieve a proof of concept of sparse control that can be easily reproduced. We do not claiming to perfectly reproduce human control, and we leave UX experiments with human users for future work.

\subsection{Experimental Setup}

In order to evaluate control in a reproducible and scalable way, we simulate it using prosodic features extracted from human speech. Some of those features are fed as input to the models in a sparse fashion. We call those features the ``driving'' features. 

By simulating human control, we are able to run our qualitative assessments for hundreds of utterances and many different conditions; it is also possible to easily reproduce experiments. Our quantitative results and the stimuli used for our subjective listening tests that follow are all generated using this method.

We created a dataset of prosodic renditions specifically for simulating prosody control. 30 voice actors performed different utterances in the test set, such that we gathered 10 renditions of each test utterance. We extracted the \fzero, energy, and duration for all renditions. The average length of a sentence is \raisebox{0.5ex}{\texttildelow}80 phonemes.

To simulate control for a given sentence in the test set, a rendition is chosen and a certain subset of values are selected to be driven. The driven values are actually extracted from a rendition of the same sentence by a different actor. We use this prosodic feature transplant during evaluation as a proxy for the way a human might have controlled the speech generation. To our knowledge, we are the first to propose such an automatic evaluation procedure to simulate human input.

The presence of multiple renditions per sentence allows us to test the flexibility of the model under different user behaviours. At test time, the model generates outputs conditional on driving PAFs of one actor but the speaker conditioning of another actor.

We measure the quality of control by computing the similarity between the output of the controlled model and the ground-truth PAFs. The process comprises 3 steps: First, extract the prosodic features from the ground-truth audio. Then, condition the model a subset of the PAFs selected according to a predefined pattern. Finally, compute the RMSE between the output PAFs and the ground-truth PAFs.

To ensure that this metric accurately measures control, we use mismatched driving and target speakers at test-time; i.e., The driving PAFs are extracted from speaker A's audio (driving speaker), but the speaker label comes from speaker B (target speaker). The speaker mismatch is necessary to ascertain if the prosody control is due to the driving PAFs rather than the speaker label. Otherwise, if we tested with the same target-driving speakers, a model with worse control abilities might produce better output prosody simply because it better uses the information from the speaker label.\footnote{Note that we train our models with the same driving and target speaker.} This is possible because the speaker label contains a significant amount of information about the prosody, since speaker identity and prosody are closely correlated \citep{Sini2020IntroducingPS}.

\subsubsection{Data}
We use a proprietary Latin American Spanish speech dataset with 32 speakers (17 female, 15 male). The data consists of \raisebox{0.5ex}{\texttildelow}38 hours across \raisebox{0.5ex}{\texttildelow}26,000 utterances, 800 of which are held back for validation and 182 for testing. The data is expressive and contains various speaking styles, including emotions such as happy, sad, angry, disgust, and joy, as well as styles like storytelling and conversational.

\subsubsection{Text-to-speech pipeline}
Given prosodic features from MICVAE, we synthesise speech that follows this prosodic specification using a TTS system conditioned on PAFs. Text-to-speech (TTS) synthesis involves a text processing front-end and an acoustic back-end. During text processing we perform text normalisation and grapheme-to-phoneme conversion to create a phonetic representation of a given sentence \citep{taylor:2009}. Our back-end uses an acoustic model to predict mel-spectrograms and a neural vocoder to synthesise the final waveform. Our acoustic model is based on Ctrl-P \cite{PaperCup-Ctrl-P:2021}, a Tacotron-2 like attention-based model conditioned on: \fzero, energy, duration, speaker identity, and speaking style. Our neural vocoder is a WaveRNN \cite{waveRNN:2018} that synthesises 16-bit 24kHz waveforms.

We prepare the data as follows. Phone identity, punctuation tokens, silence tokens, word boundary tokens, start and end of sentence tokens are represented with a one-hot vector. We apply pre-emphasis to the waveforms. We extract mel-spectrograms using 128 bins, a frame length of 50ms, and a frame shift of 10ms. \fzero is extracted using RAPT \citep{RAPT:1995}. Energy is computed using the frame-wise root-mean-square of the waveform. Durations are extracted using a Kaldi forced aligner \citep{PoveyASRU2011}. Mel-spectrograms, \fzero, energy, and durations are mean-variance normalised on a per-speaker basis.

\subsubsection{Methods}

We give a brief summary of each method considered in our evaluation.\footnote{Samples demonstrating TTS for these systems can be found here, \href{https://anonymous-submission-563098.github.io/sparse-control/}{anonymous-submission-563098.github.io/sparse-control}}

\paragraph{MICVAE} Our proposed model using self-attention with positional embeddings and ``feature-stream'' embeddings.

\paragraph{Masked CVAE} A conditional variational autoencoder using a mask feature to represent missing values. Unless otherwise specified, it is trained with 50\% of values randomly masked. Masked CVAE is intended to act as a baseline approach to this problem.

\paragraph{\textsc{NoControl}} The default prosody model: predicts PAFs from phones, speaker identity, and style code. Uses the same decoder as the MICVAE.

\paragraph{\textsc{CrudeControl}} A na\"ive system that forcefully modifies predicted values individually, this straw-man represents a controllable model that is entirely inconsistent. Its default prosody is generated with \textsc{NoControl}, and then modified manually with the driving PAFs.

\subsection{Evaluation Results}

Using the simulated control approach described above we design 4 evaluations to assess the efficiency, robustness, and faithfulness of MICVAE.

\subsubsection{Efficiency (Objective Evaluation)}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{efficiency.pdf}}
    \caption{Our model, MICVAE, controls prosody efficiently. Its generated PAFs are closer to the ground-truth rendition than  is the manually modified output of the AFP, called \sc{CrudeControl}.}
    \label{fig:iterative}
    \end{center}
    \vskip -0.2in
\end{figure}

We show that MICVAE provides an efficient method for HitL control. Intuitively, efficiency means that the user only needs to drive a few PAFs before the output reflects their intention. In the context of our simulation experiment, efficiency requires that the root mean squared error (RMSE) between the generated PAFs and the ground-truth PAFs decreases faster than the equivalent error of a naive solution such as \textsc{Crude Control}.

To create stimuli for this evaluation, we run a simulated control schedule which we call iterative refinement. We repeatedly predict prosodic features for a single sentence, each time providing an additional conditioning value, specifically, the value with the greatest RMSE in the previous generation step.

This allows us to measure the objective performance as we provide successively more conditioning information, starting---as a human with---with the most salient.

The results shown in Figure \ref{fig:iterative} demonstrate that MICVAE is able to recreate the prosodic features of the ground-truth more accurately than \textsc{CrudeControl} for the amounts of sparsity that are realistic for control (between 4 and 70 PAFs). It is practically infeasible to require a human to specify over 70 values per utterance for conditioning. For fewer than 4 PAFs, it becomes more likely there are multiple feasible prosodic renditions thus inferring the intended prosody becomes a noisier task. In our final evaluation, we show that even with 4 input PAFs, our model can control prosody in a meaningful way.

\subsubsection{Robustness (Objective Evaluation)}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{robustness.pdf}}
    \caption{Our model, MICVAE, is robust to changes in the sparsity pattern, whereas Masked CVAE only performs well when tested on the same sparsity pattern as the one on which it was trained.}\label{fig:robustness}
    \end{center}
    \vskip -0.2in
\end{figure}

In this section, we investigate how robust MICVAE is with respect to different patterns of sparsity as compared to the baseline, Masked CVAE. For robust HitL control, users should be free to choose which PAFs to drive and which to leave missing. The model should impute realistic values regardless of the pattern of sparsity in the input. To evaluate which method is more robust, we compare the error rates as we vary the number of driven PAFs.

We perform simulated control several times for MICVAE with different numbers of randomly chosen driving PAFs (0, 6, 12, 36, 72, 256). This process is then repeated for three different versions of Masked CVAE: one trained with no driving PAFs, one with 50\% driving PAFs provided, and one with all driving PAFs.

In Figure~\ref{fig:robustness}, we see that MICVAE outperforms all the Masked methods, even though they have the same number of parameters and a similar complexity. We also observe that the Masked methods behave differently because they are not robust to the different sparsity patterns seen during training. The performance of the Masked CVAE trained with 0\% driving PAFs barely changes with the number of testing driving PAFs, because it hasn't learned how to make use of the driving PAFs. The performance of the Masked CVAE trained with 50\% driving PAFs starts out worse than the 0\% model but improves with the number of training PAFs. Finally, the performance of the Masked CVAE trained with 100\% driving PAFs follows the same pattern as the 50\% model, only worse in terms of RMSE, demonstrating that masking is necessary for this model.

\subsubsection{Faithfulness (Subjective Evaluation)}

\begin{figure}[t]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{faithfulness.png}}
    \caption{Our model, MICVAE, is faithful to the user provided values. These are ratings in an A/B/R test where the question is ``Which of these is closer to the reference ground-truth driving audio?''. The number at the end of the model corresponds to the number of driving PAFs.}
    \label{fig:faithfulness}
    \end{center}
    \vskip -0.2in
\end{figure}

Any controllable model must reflect the user intention faithfully in the output. We explore how faithful MICVAE is with respect to the driving PAFs provided by the user.

Since we are interested in faithfulness with respect to the user's intention (rather than the precise driving values), we perform an A/B/R listening test, asking human evaluators to select which audio sounds closest to a reference audio. We compare MICVAE with 4 input PAFs (\raisebox{0.5ex}{\texttildelow}1\%) provided against MICVAE with no input PAFs provided. The reference is the corresponding driving ground truth audio. 

20 native Latin American Spanish speakers took part in the test and were paid on average \pounds9 per hour. The test included 30 screens and the stimuli were generated with simulated control.

In Figure~\ref{fig:faithfulness}, we see that MICVAE with 4 driving PAFs is voted as being significantly closer to the reference than MICVAE with no driving PAFs. In addition to faithfulness, this result highlights the efficiency of our model as it is able to generate a perceptible change in prosody with only 4 values controlled. This subjective evaluation demonstrates that using sparse control is an efficient and effective approach to producing a desired outcome for highly-structured data.

\section{Conclusion}

In this work, we tackle the limitations of control in generative models and propose a new control framework by sparse inputs. We define a set of desirable attributes for the generative model: efficiency, robustness and faithfulness. We propose a generative model adapted from the Masked CVAE better suited to sparse control, and demonstrate empirically that it possesses those qualities. To create a large number of testing stimuli in a repeatable fashion, we simulate HitL control using natural prosodic features from a testing dataset containing repeated utterances. We believe our evaluations and results lay the groundwork for broader studies of human-in-the-loop control of generative models using humans instead of simulating control.

\bibliography{main,zack-thesis}
\bibliographystyle{icml2022}


\end{document}

