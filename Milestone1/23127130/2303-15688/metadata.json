{
    "arxiv_id": "2303.15688",
    "paper_title": "Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation",
    "authors": [
        "Tong Zhao",
        "Andrea Tagliabue",
        "Jonathan P. How"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.RO",
        "cs.AI"
    ],
    "abstract": "The deployment of agile autonomous systems in challenging, unstructured environments requires adaptation capabilities and robustness to uncertainties. Existing robust and adaptive controllers, such as the ones based on MPC, can achieve impressive performance at the cost of heavy online onboard computations. Strategies that efficiently learn robust and onboard-deployable policies from MPC have emerged, but they still lack fundamental adaptation capabilities. In this work, we extend an existing efficient IL algorithm for robust policy learning from MPC with the ability to learn policies that adapt to challenging model/environment uncertainties. The key idea of our approach consists in modifying the IL procedure by conditioning the policy on a learned lower-dimensional model/environment representation that can be efficiently estimated online. We tailor our approach to the task of learning an adaptive position and attitude control policy to track trajectories under challenging disturbances on a multirotor. Our evaluation is performed in a high-fidelity simulation environment and shows that a high-quality adaptive policy can be obtained in about $1.3$ hours. We additionally empirically demonstrate rapid adaptation to in- and out-of-training-distribution uncertainties, achieving a $6.1$ cm average position error under a wind disturbance that corresponds to about $50\\%$ of the weight of the robot and that is $36\\%$ larger than the maximum wind seen during training.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15688v1"
    ],
    "publication_venue": "8 pages, 6 figures"
}