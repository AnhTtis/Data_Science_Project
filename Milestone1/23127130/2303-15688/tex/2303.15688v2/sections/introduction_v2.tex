The deployment of agile robots in uncertain environments demands strong robustness and rapid onboard adaptation capabilities. Approaches based on \ac{MPC}, such as robust/adaptive \ac{MPC} \cite{lopez2019adaptive, how2021performance, bujarbaruah2018adaptive, hanover2021performance, saviolo2022active}, achieve impressive robustness and adaptation performance under real-world uncertainties, but their computational cost, associated with solving a large optimization problem online, hinders deployment on computationally constrained platforms.

Recent strategies \cite{carius2020mpc, reske2021imitation, kaufmann2020deep, tagliabue2022demonstration, tagliabue2022robust, tagliabue2022output} avoid solving onboard the large optimization problem associated with MPC by instead deploying a fast \ac{DNN} policy, obtained from \ac{MPC} via \ac{IL}. These procedures consider the \ac{MPC} as an ``expert'' which provides task-relevant demonstrations used to train a ``student'' policy, using \ac{IL} algorithms such as \ac{BC} \cite{argall2009survey} or \ac{DAgger} \cite{ross2011reduction}.
Among these methods, our previous work, called \ac{SA} \cite{tagliabue2022demonstration}, enables the training of \ac{DNN} policies that achieve high robustness to uncertainties in a demonstration-efficient manner. \ac{SA} leverages a specific type of robust \ac{MPC}, called \ac{RTMPC}, to i) collect demonstrations that account for the effects of uncertainties, and ii) efficiently generate extra data that robustifies and makes more demonstration-efficient the learning procedure. However, while \ac{SA} can efficiently train \textit{robust} \ac{DNN} policies capable of controlling a variety of robots \cite{tagliabue2022output, tagliabue2022demonstration}, the obtained policies cannot adapt to the effects of model and environment uncertainties, resulting in large errors when subject to large disturbances.

\begin{figure}
    \centering
    \includegraphics[trim= 12 12 7 0,clip,width=0.45\textwidth]{figs/cover_img/adaptive-rtmpc-cover-horizontal.drawio_diagram_v9.pdf}\linebreak[0]%
    \includegraphics[trim=0 0 0 0,clip,width=0.48\textwidth]{figs/cover_img/cover_graph_v5.pdf}\linebreak[0]%

    \vspace{-1.5ex}
    \caption{Diagram of our combined \textit{position and attitude} controller, efficiently learned from Robust Tube MPC using Imitation Learning. The \textit{adaptation module} $\phi$ uses a history of $k$ states $x_{t-k:t-1}$ and actions $a_{t-k:t-1}$ to estimate the \textit{extrinsics} $\hat{z}_t$, a low dimensional representation of the environment parameters. This estimate allows the \textit{base policy} (controller) $\pi$ to adapt to various changes in the system, reducing error in the face of model/environment uncertainties. Selection matrix $S$ selects the position axis along the direction of the wind.}
    \label{fig:cover-img}
    \vspace{-2em}
\end{figure}

In this work we present \ac{SAMA}, a strategy to efficiently generate a \textit{robust and adaptive} \ac{DNN} policy using \ac{RTMPC}, enabling the policy to compensate for large uncertainties and disturbances. The key idea of our approach, summarized in \cref{fig:cover-img} and \cref{fig:approach_diagram}, is to augment the efficient \ac{IL} strategy \ac{SA}\cite{tagliabue2022demonstration} with an adaptation scheme inspired by the recently proposed adaptive policy learning method \ac{RMA} \cite{kumar2021rma}.  \ac{RMA} uses \ac{RL} to train in simulation a fast \ac{DNN} policy whose inputs include a learned low-dimensional representation of the model/environment parameters experienced during training. At deployment, this low-dimensional representation is estimated online, triggering adaptation. \ac{RMA} policies have demonstrated impressive adaptation and generalization performance on a variety of robots/conditions \cite{kumar2021rma, kumar2022adapting, zhang2022zero, qi2022hand}. 
Similar to \ac{RMA}, in our work we include to the inputs of the learned policy a low-dimensional representation of model/environment parameters that could be encountered at deployment. These parameters are experienced during demonstration collection from \ac{MPC} and can be efficiently estimated online, enabling adaptation. Unlike \ac{RMA}, however, we bypass the challenges associated with \ac{RL}, such as reward selection and tuning, via an efficient \ac{IL} procedure using our \ac{MPC}-guided data augmentation strategy \cite{tagliabue2022demonstration}. 
We tailor our approach to the challenging task of \textit{trajectory tracking} for a multirotor, designing a policy that controls both \textit{position and attitude} of the robot and that is capable of adapting to uncertainties in the translational and rotational dynamics. 
Our evaluation, performed under challenging model errors and disturbances in a simulation environment, demonstrates rapid adaptation to in- and out-of-distribution uncertainties while tracking agile trajectories with top speeds of $3.2$ m/s, using an adaptive policy that is learned in $1.3$ hours. This differs from prior \ac{RMA} work for quadrotors \cite{zhang2022zero}, where the focus of adaptation is only on \textit{attitude} control during quasi-static trajectories.
Additionally, \ac{SAMA} shows comparable performance to \ac{RTMPC} combined with a high-performance, state-of-the-art but significantly more computationally expensive \ac{DO}.

\noindent
\textbf{Contributions.}
\begin{inparaenum}
\item We augment our previous efficient and robust \ac{IL} strategy from \ac{MPC} \cite{tagliabue2022demonstration} with the ability to learn robust \textit{and adaptive} policies, therefore reducing tracking errors under uncertainties while maintaining high learning efficiency. Key to our work is to leverage the performance of an RMA-like \cite{kumar2021rma} adaptation scheme, but without relying on \ac{RL}, therefore avoiding reward selection and tuning.
\item We apply our proposed methodology to the task of adaptive position and attitude control for a multirotor, demonstrating for the first time \ac{RMA}-like adaptation to uncertainties that cause position and orientation errors, unlike previous work \cite{zhang2022zero} that only focuses on adaptive attitude control. 
\end{inparaenum}
