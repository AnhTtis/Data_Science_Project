\noindent
\textbf{Adaptive Control.}
Adaptation strategies can be classified into two categories, direct and indirect methods. Indirect methods aim at explicitly estimating models or parameters, and these estimates are leveraged in model-based controllers, such as \ac{MPC} \cite{borrelli2017predictive}. Model/parameter identification include filtering techniques \cite{svacha2020imu, wuest2019online}, disturbance observers \cite{tagliabue2020touch, tagliabue2019robust, mckinnon2016unscented}, set-membership identification methods \cite{lopez2019adaptive, how2021performance} or active, learning-based methods \cite{saviolo2022active}. While these approaches achieve impressive performance, they often suffer from high computational cost due to the need of identifying model parameters online and, when \ac{MPC}-based strategies are considered, solving large optimization problems online.
Direct methods, instead, develop policy updates that improve a certain performance metric. This metric is often based on a reference model, while the updates involve the shallow layers of the \ac{DNN} policy \cite{joshi2019deep, joshi2020design, zhou2021bridging}. Additionally, policy update strategies can be learned offline using meta-learning \cite{richards2021adaptive, oconnel2022neural}. While these methods employ computationally-efficient \ac{DNN} policies, they require extra onboard computation to update the policy, require costly offline training procedures, and/or do not account for actuation constraints. 
Parametric adaptation laws, such as $\mathcal{L}_1$ adaptive control \cite{hovakimyan20111}, have been applied to the control inputs generated by \ac{MPC} \cite{pravitra2020} \cite{hanover2021performance}, significantly improving \ac{MPC} performance; however, these approaches still require solving onboard the large optimization problem associated with \ac{MPC}, and do not account for control limits.
Our work leverages the inference speed of a \ac{DNN} for computationally-efficient onboard deployment, training the policy using an efficient \ac{IL} procedure (our previous work \cite{tagliabue2022demonstration}) that uses a robust \ac{MPC} capable of accounting for state and actuation constraints. 

\noindent
\textbf{\acf{RMA}.}
\ac{RMA} \cite{kumar2021rma} has recently emerged as a high-performance, hybrid adaptive strategy. Its key idea is to learn a \ac{DNN} policy conditioned on a low-dimensional (encoded) model/environment representation that can be efficiently inferred online using another \ac{DNN}. The policy is trained using \ac{RL}, in a simulation where it experiences different instances of the model uncertainties/disturbances. \ac{RMA} policies have controlled a wide range of robots, including quadruped \cite{kumar2021rma}, biped \cite{kumar2022adapting}, hand-like manipulators \cite{qi2022hand} and multirotors \cite{zhang2022zero}, demonstrating rapid adaptation and generalization to previously unseen disturbances. Our work takes inspiration from the adaptation strategy introduced by \ac{RMA}, as we learn policies conditioned on a low-dimensional environment representation that is estimated online. However, unlike \ac{RMA}, our learning procedure does not require the reward selection and tuning typically encountered in \ac{RL}, as it leverages an efficient \ac{IL} strategy from robust \ac{MPC}. An additional difference to \cite{zhang2022zero}, where \ac{RMA} is used to generate a policy for attitude control of multirotors of different sizes, is that our work focuses on the challenging task of learning an adaptive trajectory tracking controller, which compensates for the effects of uncertainties on its attitude \textit{and} position.

\noindent
\textbf{Efficient Imitation Learning from MPC.}
Our previous work \cite{tagliabue2022demonstration} has focused on designing efficient policy learning procedures by leveraging \ac{IL} and a \ac{RTMPC} to guide demonstration collection and co-design a demonstration-efficient data augmentation procedure. This approach has enabled efficient learning of policies capable of performing trajectory tracking on multirotors \cite{tagliabue2022demonstration} and sub-gram, soft-actuated flapping-wings aerial vehicles \cite{tagliabue2022robust}, also enabling efficient sensorimotor policy learning \cite{tagliabue2022output}. In this work, we extend this procedure by enabling online \textit{adaptation}, while maintaining high efficiency (in terms of demonstrations and training time) of the policy learning procedure.
