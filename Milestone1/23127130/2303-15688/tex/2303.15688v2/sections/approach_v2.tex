\begin{figure*}
    % [trim = {left, bottom, right, top}, clip]
    \centering\includegraphics[width=0.9\textwidth,trim = {0, 20, 45, 0}, clip]{figs/mpc_rma_diagram/mpc_rma_diagram_v4.pdf}
    \vspace{-1.5ex}
    \caption{Schematic representation of \acf{SAMA}, our proposed approach for efficient learning of adaptive polices from \ac{MPC}. The key idea of \ac{SAMA} consists in leveraging an efficient Imitation Learning strategy, \acf{SA} \cite{tagliabue2022demonstration}, to collect demonstrations and perform data augmentation using a Robust Tube \ac{MPC}. This efficiently generated data is used to train a student policy conditioned on a latent representation $z_t$ of environment and robot parameters $e_t$. Following the \acf{RMA} \cite{kumar2021rma} procedure, we then train an adaptation module that can produce an estimate $\hat{z}_t$ of these environment parameters from a sequence of past states and actions. This approach enables efficient learning of a robust, adaptive policy from \ac{MPC} without leveraging \ac{RL}, avoiding any reward tuning and making use of available priors on the model of the robot.}
    \label{fig:approach_diagram}
    \vskip-3ex
\end{figure*}

The proposed approach, summarized in \cref{fig:approach_diagram}, consists in a \textit{three phase} policy learning procedure: 

\subsection{Phase 0: Robust Tube MPC Design}
\label{eqn:adaptive-rtmp}
As in \ac{RMA}, we train our policies in a simulation environment implementing the full nonlinear dynamic model of the robot/environment, with parameters (model/environment uncertainties, disturbances...) captured by the environment parameter vector $e$. At each timestep $t$, each entry in $e$ may change with some probability $p$, with entries changing independently of each other (see Table \ref{table:env-params} for more details on the distributions of $e$ in the train and test environments). Whenever $e$ changes, we update the \ac{RTMPC}, described in \cref{sec:rtmpc_design}, as follows. 
First, since the controller uses linear system dynamics, for a given environment parameter vector $e_t$ at time $t$ we compute a discrete-time linear system by discretizing and linearizing the full nonlinear system dynamics, obtaining:  
\begin{equation}
\label{eqn:rtmpc-time-varying-model}
    x_{t+1} = A(e_t) x_t + B(e_t) u_t.
\end{equation}
The linearization is performed by assuming a given desired operating point; for our multirotor-based evaluation, this point corresponds to the hover condition.

Second, the feedback gain $K_t$ for the ancillary controller in \cref{eqn:rtmpc-ancillary} is updated by solving the infinite horizon, discrete-time LQR problem using $(A(e_t), B(e_t), Q, R)$, leaving the tuning weights $Q$, $R$ fixed. Last, we compute the robust control invariant set $\mathbb{Z}_t$ employed by \ac{RTMPC} from the resulting $K_t$, $A(e_t)$, $B(e_t)$, and a given $\mathbb W$. Due to the computational cost of \textit{precisely} computing $\mathbb{Z}_t$ (from $K_t$, $A(e_t)$, $B(e_t)$, and $\mathbb{W}$), we generate an outer-approximation of $\mathbb{Z}_t$ via Monte Carlo simulation. This is done by computing the axis-aligned bounding box of the state deviations obtained by perturbing the closed loop system $A_{K_t}$ with randomly sampled instances of $w \in \mathbb{W}$. The set $\mathbb W$ is designed to capture the effects of linearization and discretization errors, as well as errors that are introduced by the learning/parameter estimation procedure. 

\subsection{Phase 1: Base Policy and Environment Factor Encoder Learning via Efficient Imitation}
We now describe the procedure to efficiently learn a base policy $\pi$ and an environment factor encoder $\mu$ in simulation. Similar to \ac{RMA}, our base policy takes as input the current state $x_t$, an extrinsics vector $z_t$ and, different from RMA, a reference trajectory $\mathbf{x}_t^{\text{ref}}$. It outputs a vector of actuator commands $a_t$. 
As in \ac{RMA}, the extrinsics vector $z_t$ represents a low dimensional encoding of the environment parameters $e_t$, and it is generated in this phase by the environment factor encoder $\mu$:
\begin{equation}
    \label{eqn:adaptive_policy}
    \begin{split}
        z_t &= \mu(e_t) \\ 
        a_t &= \pi(x_t, z_t, \mathbf{x}_t^{\text{ref}}).
    \end{split}
\end{equation}
We jointly train the base policy $\pi$ and environment encoder $\mu$ end-to-end. However, unlike \ac{RMA}, we do not use \ac{RL}, but demonstrations collected from \ac{RTMPC} in combination with \ac{DAgger} \cite{ross2011reduction}, treating the \ac{RTMPC} as a \textit{expert}, and the policy in \cref{eqn:adaptive_policy} as a \textit{student}. More specifically, at every timestep, given the environment parameters vector $e_t$, the current state of the robot $x_t$, and the reference trajectory $\mathbf{x}_t^{\text{ref}}$, the expert generates a control action $u_t$ by first computing a \textit{safe} reference plan $\check{\mathbf{x}}_t^*, \check{\mathbf{u}}_t^*$, and then by using the ancillary controller in \cref{eqn:rtmpc-ancillary}. The obtained control action is applied to the simulation with a probability $\beta$, otherwise the applied control action is queried from the student (\cref{eqn:adaptive_policy}). At every timestep, we store in a dataset $\mathcal{D}$ the (input, output) pairs $(\{\mathbf{x}_t^{\text{ref}}, x_t, e_t\}, u_t)$.

\noindent
\textbf{Tube-guided Data Augmentation.} \label{sec:approach:tube_augmentation} We leverage our previous work \cite{tagliabue2022demonstration} to augment the collected demonstrations with extra data that accounts for the effects of the uncertainties in $\mathbb W$. This procedure leverages the idea that the tube $\mathbb{Z}_t$ centered around $\check{x}_{0,t}^*$, as computed by \ac{RTMPC}, represents a model of the states that the system may visit when subject to the uncertainties captured by the additive disturbances $w \in \mathbb{W}$, while the ancillary controller \cref{eqn:rtmpc-ancillary} represents an efficient way to compute control actions that ensure the system remains inside the tube. Therefore, at each timestep $t$, given the ``safe'' plan computed by the expert $\check{\mathbf{x}}_t^*, \check{\mathbf{u}}_t^*$, we compute extra state-action pairs $(x_t^+, u_t^+)$ by sampling states from inside the tube $x_t^+ \in \check{x}_{0,t}^* \oplus \mathbb{Z}_t$, and computing the corresponding robust control action $u_t^+$ using the ancillary controller:
\begin{equation}
    u_t^+ = \check{u}_{0,t}^* + K(x_t^+ - \check{x}_{0,t}^*).
\end{equation}
In this way, we obtain extra (input, output) samples $(\{\mathbf{x}_t^{\text{ref}}, x_t^+, e_t\}, u_t^+)$ that are added to  the training dataset $\mathcal{D}$. 
Last, the policy in \cref{eqn:adaptive_policy} is trained end-to-end using the dataset $\mathcal{D}$, by finding the parameters of $\pi$ and $\mu$ that minimize the following \ac{MSE} loss: $\| u_i - \pi(x_i, \mu(e_i), \mathbf{x}_i^{\text{ref}})\|_2^2$, where $i$ denotes the $i$-th datapoint in $\mathcal{D}$.  

\subsection{Phase 2: Learning the Adaptation Module}
This step is performed as in \ac{RMA} \cite{kumar2021rma}, and is described in \cref{eqn:rma_adaptation_module} of this work. 
