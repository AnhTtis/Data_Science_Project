\subsection{Evaluation Approach}
We evaluate the proposed approach in the context of trajectory tracking for a multirotor, by learning to track an $8$ s long, heart-shaped trajectory ($\heartsuit$) and an $8$ s long, eight-shaped trajectory ($\infty$) with a maximum velocity of $3.2$ m/s. All evaluation is performed on a desktop machine with Intel i9-10920X CPUs and Nvidia RTX $3090$ GPUs.

\input{tables/envs_parameters.tex}

\noindent
\textbf{Simulation Details.} \label{sec:evaluation:sim} Learning and evaluation are performed in simulation, implemented by integrating a realistic nonlinear model of the dynamics of a multirotor (with $6$ motors):
\begin{align}
\label{eqn:sim_nonlinear_model}
\dot{p}=v,
\;\;\;&\; 
m\dot{v}\!=\!f_{\text{cmd}} R_B(q)z\!-\!mg_Wz\!+\!f_{\text{drag}}\!+\!f_{\text{ext}}, & \nonumber \\ 
\dot{q}=\frac{1}{2}\Omega(\omega)q,
\;\;\;&\;
J \dot{\omega}\!=\!-\omega \times J \omega\!+\!\tau_{\text{cmd}}\!+\!\tau_{\text{drag}}\!+\!\tau_{\text{ext}}. & 
\end{align}
Position and velocity $p, v \in \mathbb{R}^3$ are expressed in the world frame $W$, $q \in \text{SO}(3)$ is the attitude quaternion, $\omega$ is the angular velocity, $m$ is the mass and $J$ is the inertia matrix assumed diagonal. The total torques $\tau_\text{cmd}$ and forces $f_\text{cmd}$ produced by the propellers, as expressed in body frame $B$, are linearly mapped to the propellers' thrust via a mixer/allocation matrix (e.g., \cite{tagliabue2020touch}).  
We assume the presence of isotropic drag forces and torques $f_\text{drag} = -c_{dv}v$ and $\tau_{\text{drag}} = -c_{d\omega}\omega$, with $c_{dv} >0$, $c_{d\omega} > 0$, and the presence of external forces $f_{\text{ext}}$ and torques $\tau_{\text{ext}}$. The environment parameter vector $e_t$ has size $13$, and contains the robot/environment parameters in \cref{table:env-params}. We use the \texttt{acados} integrator \cite{Verschueren2021} to simulate these dynamics with a discretization interval of $0.002$ s.

\noindent
\textbf{\ac{RTMPC} for Trajectory Tracking on a Multirotor.} The controller has state of size $n_x = 12$, consisting of position, velocity, Euler angles, and angular velocity. It generates thrust/torque commands ($n_u = 4$) mapped to the $6$ motor forces via allocation/mixer matrix ($n_a = 6$). We use an adversarial heuristic to find a value for $\mathbb{W}$, and specifically we assume that it matches the external forces and torques used in training distribution (Table \ref{table:env-params}), as they are close to the physical actuation limits of the platform. The reference trajectory is a sequence of desired positions and velocities for the next $1$\,s, discretized with a sampling time of $0.04$ s (corresponding to a planning horizon of $N=25$, and $300$-dim vector). The controller takes into account state constraints (i.e., available 3D flight space, velocity limits, etc) and actuation limits, and is simulated to run at $500$ Hz.

\noindent
\textbf{Student Policy Architecture.} The base policy $\pi$ is a $3$-layer \ac{MLP} with $256$-dim hidden layers, which takes as input the current state $x_t \in \mathbb{R}^{12}$ and extrinsics vector $z_t \in \mathbb{R}^{8}$ and outputs motor forces $a_t \in \mathbb{R}^6$. The environment encoder $\mu$ is a $2$-layer \ac{MLP} with $128$-dim hidden layers, taking as input environment parameters $e_t \in \mathbb{R}^{13}$. The adaptation module $\phi$ projects the latest $400$ state-action pairs into $32$-dim representations using a $2$-layer \ac{MLP}. Then, a $3$-layer 1-D CNN convolves the representation across time to capture temporal correlations in the input. The input channel number, output channel number, kernel size, and stride for each layer is $[32, 32, 8, 4]$. The flattened CNN output is linearly projected to obtain $\hat{z}_t$. Like the \ac{RTMPC} expert, the student policy is simulated to run at $500$ Hz.

\subsection{Training Details and Hyperparameters} \label{training-details}
All policies are implemented in PyTorch and trained with the Adam optimizer, with learning rate $0.001$ and default parameters.

\noindent
\textbf{Phase 1.} We train $\mu$ and $\pi$ by collecting $8$ s long trajectories, with $1$ s of hovering before and after the trajectories. The expert actions are sampled at $20$ Hz, resulting in $200$ expert actions per demonstration (when no additional samples are drawn from the tube). When drawing additional samples from the tube, we do so in two different ways. The first is to uniformly sample the tube for every demonstration we collect from the expert, extracting $N_\text{samples} = \{25, 50, 100\}$ samples per timestep; these methods are denoted as SAMA-$N_\text{samples}$. In the second way, we apply data augmentation (using $N_\text{samples} = 100$ samples per timestep) only to the first collected demonstration, while we use  DAgger only (no data augmentation) for the subsequent demonstrations. This method is denoted as SAMA-100-FT (Fine-Tuning, as DAgger is used to fine-tune a good initial guess generated via data augmentation). These different procedures enable us to study trade-offs between improving robustness/performance (more samples) or the training time (fewer samples). Across our evaluations, we always set the DAgger hyperparameter $\beta$ to $1$ for the first demonstration and $0$ otherwise.

\noindent
\textbf{Phase 2.} Similar to previous RMA-like approaches \cite{kumar2021rma,kumar2022adapting,zhang2022zero,qi2022hand}, we train $\phi$ via supervised regression.


\begin{figure}[ht!]
    \centering\includegraphics[width=0.5\textwidth,trim = {0.2cm, 0, 0.05cm, 0}, clip]{figs/eval/combined_train_v11.pdf}
    \captionof{figure}{Performance and robustness in the \textbf{training environment} after \textit{Phase 1}, as a function of number of demonstrations and training time. \ac{IL} allows for the learning of effective \textit{Phase 1} policies in one hour on a single core, as opposed to \ac{RL} which has been reported to take two hours on an entire desktop machine \cite{zhang2022zero}. This training time can be significantly shortened by using tube-guided data augmentation during training.}
    \label{fig:perf_robust_train}
    \vskip-2ex
\end{figure}


\subsection{Efficiency, Robustness, and Performance} 

In this part, we analyze our approach on the task of performing \textit{Phase 0 and 1}, as these are the parts where our method introduces key changes compared to prior \ac{RMA} work, on the task of learning the heart-shaped trajectory (\cref{fig:traj_combined}). We study the performance (average position error from the reference trajectory) and robustness (avoiding violation of state and actuation constraints) of our policy as a function of the total training time and the number of demonstrations used for training. Our comparison includes the \ac{RTMPC} expert that our policy tries to imitate, as well as a policy trained only with DAgger, without any data augmentation. Each policy is evaluated on $10$ separate realizations of the training/test environment. We repeat the procedure over $6$ random seeds. All training in this part is done on a single CPU and GPU.

\begin{figure}[ht!]
    \centering\includegraphics[width=0.5\textwidth,trim = {0.2cm, 0, 0.05cm, 0}, clip]{figs/eval/combined_test_v11.pdf}
    \captionof{figure}{Performance and robustness in the \textbf{testing environment} after \textit{Phase 1}, as a function of number of demonstrations and training time. The test environment presents a set of disturbances that the robot has never seen during training, as highlighted in \cref{table:env-params}. Methods that rely on our tube-guided data augmentation strategy (SAMA) generalize better than DAgger, achieving higher robustness and performance in lower time.}
    \label{fig:perf_robust_test}
    \vskip-2ex
\end{figure}

Figure \ref{fig:perf_robust_train} shows the evaluation of the policy under a new set of disturbances sampled from the same training distribution, defined in Table \ref{table:env-params}, highlighting that our tube-guided data augmentation strategy efficiently learns robust \textit{Phase 1} policies. Compared to DAgger-only, our methods achieve full robustness in less than \textit{half} the time, and using only 20\% of the required expert demonstrations. Additionally, tube-guided methods achieve about half the position error of DAgger for the same training time, reaching an average of $5$ cm  in less than $10$ minutes.  Among tube-guided data augmentation methods, we observe that fine-tuning (SAMA-100-FT) achieves the lowest tracking error in the shortest time.
Figure \ref{fig:perf_robust_test} repeats the evaluation under a challenging set of disturbances that are outside the training distribution (see Table \ref{table:env-params}). The analysis, as before, highlights the benefits of the data augmentation strategy, as SAMA methods achieve higher robustness and performance. The performance in this test environment confirms the trend that fine-tuning (SAMA-100-FT) achieves good trade-offs in terms of training time and robustness. Overall, these results highlight that our method can successfully and efficiently learn \textit{Phase 1} policies capable of handling out-of-distribution disturbances.


\begin{figure*}[ht!]
    \centering\includegraphics[trim=0 7 0 5, clip, width=0.825\textwidth]{figs/eval/traj_and_extrinsics_v16.pdf}
    \caption{Performance while tracking the heart-shaped trajectory in \cref{fig:traj_combined}. The robot is subject to an out-of-training-distribution wind-like force of $6$ N (along positive $y$-axis, shown in shaded grey area) that is $36\%$ larger than any external forces seen during training. Our method (SAMA-100-FT) is computationally efficient, robust, and adaptive, as shown by the changes in extrinsics ($\hat{z}_2$) when the robot is subject to wind. Our method achieves $10\%$ lower tracking error than RTMPC+DO, a model-based controller which is both robust and adaptive at the cost of being computationally expensive to run online (see Table \ref{table:runtime}), and which has been designed using a nominal model. We additionally maintain similar performance to the Expert, an RTMPC that has access to the ground truth model and disturbances and represents the best case performance of a model-based controller. This highlights improvements over our previous work SA \cite{tagliabue2022demonstration}, a learning-based controller which is robust and computationally efficient, but non-adaptive.}
    \label{fig:traj_and_extrinsics}
    \vspace{-3ex}
\end{figure*}

\begin{figure}
    % trim={<left> <lower> <right> <upper>}
    \centering\includegraphics[trim=35 65 400 148,clip,width=\columnwidth]{figs/eval/traj_combined_v9.pdf}
    \caption{Tracking of a heart-shaped trajectory under strong, out-of-distribution wind-like external force disturbances. The blue plane $p_x\!=\!0$ divides the environment into a part with wind ($p_x\!\leq\!0$) and one without ($p_x\!>\!0$). Our adaptive approach (SAMA-100-FT) demonstrates an improvement on our previous work (SA), which is robust but not adaptive, and it is able to match the performance of robust MPC combined with a disturbance observer (RTMPC+DO), but at a fraction of its computational cost.}
    \label{fig:traj_combined}
    \vspace{-4ex}
\end{figure}


\subsection{Adaptation Performance Evaluation}
In this part, we analyze the adaptation performance of our approach after \textit{Phase 2}. We consider the heart-shaped and the eight-shaped trajectory. For each trajectory, we train a $\mu$ and $\pi$ in \textit{Phase 1} using SAMA-100-FT, fine-tuning for $5$ DAgger iterations, collecting $10$ demonstrations per iteration during fine-tuning. Given a trained $\mu$ and $\pi$, we train $\phi$ via supervised regression in \textit{Phase 2} (Section \ref{training-details}), conducting $20$ iterations with $10$ policy rollouts collected in parallel per iteration. On $10$ CPUs and $1$ GPU, \textit{Phase 1} takes about $20$ minutes and \textit{Phase 2} takes about an hour. We note that the training efficiency of our proposed \textit{Phase 1} compares favorably to the \ac{RL}-based results in \cite{zhang2022zero}, where the authors report $2$ hours of training time for \textit{Phase 1}. 

First, we evaluate the tracking performance of our adaptive controller in an environment subject to position-dependent winds, as shown in \cref{fig:traj_combined}. The wind applies $6$ N of force, a force $36\%$ \textit{larger} than any external force encountered during training. We compare our approach with \ac{SA}, our previous non-adaptive robust policy learning method \cite{tagliabue2022demonstration}, and with the \ac{RTMPC} expert that has access to $e_t$ (the true value of the wind). We also consider an \ac{RTMPC} whose state has been augmented with external force/torques estimated via a state-of-the-art nonlinear disturbance observer (\ac{RTMPC}+\ac{DO}) based on an \ac{UKF} \cite{tagliabue2019robust}, a method that has access to the nominal model of the robot (matching the one used in this experiment) and ad-hoc external force/torque disturbance estimation. The results are presented in \cref{fig:traj_and_extrinsics} and \cref{fig:traj_combined}. The shaded section of \cref{fig:traj_and_extrinsics}, corresponding to the windy regions,  highlights that \ac{SAMA}-100-FT is able to adapt to a large, previously unseen force-like disturbance, obtaining a tracking error of less than $10$ cm at convergence, unlike the corresponding non-adaptive variant (\ac{SA}), which instead suffers from a $50$ cm tracking error. \cref{fig:traj_and_extrinsics} additionally highlights changes in the extrinsics, which do not depend on changes in reference trajectory but rather on the presence of the wind, confirming the successful adaptation of the policy. \cref{tab:tracking_performance} reports a $10 \%$ reduction in tracking error compared to \ac{RTMPC}+\ac{DO}. Second, we repeat the evaluation on the challenging eight-shaped trajectory, with the robot achieving speeds of up to $3.2$ m/s, where the robot is subject to a large set of out-of-distribution model errors: twice the nominal mass and arm length, ten times the nominal drag coefficients, and an external torque of $2.0$\,Nm. 
\cref{tab:tracking_performance} and \cref{fig:traj_combined_lem} highlight the adaptation capabilities of our approach, which performs comparably to the more computationally expensive (\cref{table:runtime}) \ac{RTMPC}+\ac{DO}.

\begin{figure}
    % trim={<left> <lower> <right> <upper>}
    \centering\includegraphics[trim=380 95 100 105,clip,width=0.867\columnwidth]{figs/eval/traj_3d_lemn_v7.pdf}
    \caption{Tracking of an eight-shaped trajectory under out-of-distribution disturbances and model errors, where mass and arm length are twice the nominal values, the drag is $10$ times the nominal, and there is a $2$\,Nm external torque disturbance. The blue plane $p_x\!=\!0$ divides the environment into a part with model errors ($p_x\!\leq\!0$) and without ($p_x\!>\!0$). Our adaptive approach (SAMA-100-FT) can adapt during agile flight, reaching top speeds of $3.2$\,m/s, while maintaining performance comparable to RTMPC+DO.}
    \label{fig:traj_combined_lem}
    \vspace{-3ex}
\end{figure}


\noindent
\textbf{Efficiency at Deployment.} Table \ref{table:runtime} reports the time to compute a new action for each method. On average, our method (\ac{SAMA}-100-FT) is $12$ times faster than the expert, and $24$ times faster than \ac{RTMPC}+\ac{DO}.

\input{tables/position_errors.tex}

\input{tables/computation_time.tex}

