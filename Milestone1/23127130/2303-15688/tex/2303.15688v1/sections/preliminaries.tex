Our approach leverages and modifies two key algorithms, \ac{RTMPC} \cite{mayne2005rtmpc} and \ac{RMA} \cite{kumar2021rma}. These methods are summarized in the following parts for completeness. 

\subsection{Notation Definition}

Consider two sets $\mathbb{X} \subseteq \mathbb{R}^{n}$ and $\mathbb{Y} \subseteq \mathbb{R}^{n}$, and a linear map $M \in \mathbb{R}^{m \times n}$. We define the set $M\mathbb{X}$ as $M\mathbb{X} :=\{Mx \mid x \in \mathbb{X}\}$. The Minkowski sum $\mathbb X \oplus \mathbb Y$ is defined as $\mathbb{X} \oplus \mathbb{Y} := \{x+y \mid x \in \mathbb{X}, y \in \mathbb{Y}\}$. The Pontryagin difference $\mathbb X \ominus \mathbb{Y}$ is defined as $\mathbb{X} \ominus \mathbb{Y} := \{z \mid z + y \in \mathbb X, \forall y \in \mathbb Y\}$.

\subsection{Robust Tube MPC (RTMPC)}
\label{sec:rtmpc_design}
 \ac{RTMPC} \cite{mayne2005rtmpc} is a type of model predictive controller capable to control a linear system assumed subject to bounded uncertainty, while guaranteeing state and actuation constraint satisfaction. 
The linearized, discrete-time model of the system is given by
\begin{equation}
\label{eqn:rtmpc-model}
    x_{t+1} = Ax_t + Bu_t + w_t,
\end{equation}
where $x \in \mathbb{R}^{n_x}$ represents the state, $u \in \mathbb{R}^{n_u}$ represents the commanded actions, and $w \in \mathbb{W} \subseteq \mathbb{R}^{n_x}$ represents additive disturbance, with the dimensions of the state and action being $n_x$ and $n_u$ respectively. Matrices $A \in \mathbb{R}^{n_x \times n_x}$ and $B \in \mathbb{R}^{n_x \times n_u}$ represent the nominal robot dynamics. The system is additionally subject to state and actuation constraints
$    x \in \mathbb{X}$, $u \in \mathbb{U}$,
and the disturbance/uncertainty is assumed to be bounded, i.e.,
$    w \in \mathbb{W}$.
At every discrete timestep $t$, an estimate of the state of the actual system $x_t$ and a reference trajectory $\mathbf{x}^{\text{ref}}_t := [x^{\text{ref}}_{0,t}, \hdots, x^{\text{ref}}_{N,t}]$ are provided. The controller then plans a sequence of states $\mathbf{\check{x}}_t := [\check{x}_{0,t}, \hdots, \check{x}_{N,t}]$ and actions $\mathbf{\check{u}}_t := [\check{u}_{0,t}, \hdots, \check{u}_{N-1,t}]$ that solve the  optimization problem: 
\begin{equation}
\label{eqn:rtmpc-optimization-constrained}
\begin{gathered}[b]
    (\mathbf{\check{x}^*_t}, \mathbf{\check{u}}^*_t) = \argmin_{\mathbf{\check{x}_t}, \mathbf{\check{u}_t}}
    V(\mathbf{\check{x}_t}, \mathbf{\check{u}}_t, \mathbf{x}_t^{\text{ref}}, x_t)
    \\
    \text{subject to } 
    \begin{lgathered}[t]
        \check{x}_{n+1,t} = A \check{x}_{n,t} + B \check{u}_{n,t},
        \\
        \check{x}_{n,t} \in \mathbb{X}_c, \check{u}_{n,t} \in \mathbb{U}_c,
        \\
        x_t \in  \check{x}_{0,t} + \mathbb{Z}.
        \\
    \end{lgathered}
    \\
\end{gathered}
\end{equation}
with $n = 0, \dots, N-1$. The objective function is:
\begin{equation}
\label{eqn:rtmpc-objective}
\begin{gathered}[b]
    V(\mathbf{\check{x}}_t, \mathbf{\check{u}}_t, \mathbf{x}^{\text{ref}}_t, x_t) := 
    \norm{e_{N,t}}_P^2 \!+\!\! \sum_{n=0}^{N-1} \norm{e_{n,t}}_Q^2 \!+\! \norm{\check{u}_{n,t}}_R^2,
\end{gathered}
\end{equation}
where $e_{n,t} := \check{x}_{n, t} - x^{\text{ref}}_{n, t}$, and $\check{\cdot}$ denotes internal variables of the optimization. Positive definite matrices $Q \in \mathbb{R}^{n_x \times n_x}$ and $R \in \mathbb{R}^{n_u \times n_u}$ are user-selected weights that define the stage cost, while $P \in \mathbb{R}^{n_x \times n_x}$ is a positive definite matrix that represents the terminal cost. Additionally, we enforce dynamic feasibility constraints on the optimization variables, so $\check{x}_{n+1,t} = A \check{x}_{n,t} + B \check{u}_{n,t}$. The $\mathbf{\check{x}}_t$ and $\mathbf{\check{u}}_t$ obtained by minimizing the objective in \cref{eqn:rtmpc-objective} are denoted $\mathbf{\check{x}}_t^* := [\check{x}_{0,t}^*, \hdots, \check{x}_{N,t}^*]$ and $\mathbf{\check{u}}_t^* := [\check{u}_{0,t}^*, \hdots, \check{u}_{N-1,t}^*]$.

Given $\mathbf{\check{x}}_t^*$ and $\mathbf{\check{u}}_t^*$, the control output is then produced by a feedback policy, called an \textit{ancillary controller}
\begin{equation}
\label{eqn:rtmpc-ancillary}
    u_t = \check{u}_{0,t}^* + K (x_t - \check{x}_{0,t}^*),
\end{equation}
where $K$ is a feedback gain matrix, chosen such that the matrix $A_K := A+BK$ is Schur stable, for example, by solving the infinite horizon, discrete-time LQR problem using $(A, B, Q, R)$. Given $K$, we compute a \textit{disturbance invariant set} $\mathbb{Z} \subseteq \mathbb{R}^{n_x}$, called a \textit{tube}, which satisfies $A_K \mathbb{Z} \oplus \mathbb{W} \subseteq \mathbb{Z}$.
%\begin{equation}
%    A_K \mathbb{Z} \oplus \mathbb{W} \subseteq \mathbb{Z},
%\end{equation}
The ancillary controller with gain $K$ guarantees that, in the controlled system, if $x_t \in \check{x}_{0,t}^* \oplus \mathbb{Z}$, then $x_{t+1} \in \check{x}_{0, t+1}^* \oplus \mathbb{Z}$; i.e., $x_{t+1}$ remains in the tube centered around $\check{x}_{0,t+1}$ for all realizations of  $w_t \in \mathbb{W}$ \cite{mayne2001robust}.  
%\todo{Add figure that visually explains tube.}

To account for the output of the ancillary controller, \ac{RTMPC} tightens state and actuation constraints in the optimization (\ref{eqn:rtmpc-objective}) according to $\mathbb{X}_c := \mathbb{X} \ominus \mathbb{Z}$, $\mathbb{U}_c := \mathbb{U} \ominus K\mathbb{Z}$.
Additionally, the controller constrains $\check{x}_{0,t}$ so that, at time $t$, the current state $x_t$ is within a tube centered around $\check{x}_{0,t}$. These restrictions combined ensure that the ancillary controller keeps the actual state $x$ in a tube centered around optimal planned state $\check{x}_0^*$, while simultaneously ensuring that the combined controller remains within state and actuation constraints for any realization of $w \in \mathbb{W}$. 

\subsection{Rapid Motor Adaptation (RMA)}


\ac{RMA} \cite{kumar2021rma} enables learning of adaptive control policies in simulation using model-free \ac{RL}.  The key idea of RMA is to learn a policy that is composed of a base policy $\pi$ and an adaptation module $\phi$. The base policy is denoted as:
\begin{equation}
a_t = \pi(x_t, z_t),
\end{equation}
and takes as input the current state $x_t \in \mathbb{R}^{n_x}$ and an \textit{extrinsics} vector $z_t \in \mathbb{R}^{n_z}$, and outputs commanded actions $a_t \in \mathbb{R}^{n_a}$. Key to this method is the extrinsics vector $z_t$, a low-dimensional representation of an environment vector $e_t \in \mathbb{R}^{n_e}$, which captures all the possible parameters/disturbances that may vary at deployment time  (i.e., mass, drag, external disturbances, \dots), and towards which the policy should be able to adapt. However, because $e_t$ is not directly accessible in the real world, it is not possible to directly compute $z_t$ at deployment time. Instead, \ac{RMA} produces an estimate $\hat{z}_t$ of $z_t$ via an \textit{adaptation module} $\phi$: 
\begin{equation}
    \hat{z}_t = \phi(x_{t-k : t}, a_{t-k; t-1}), 
\end{equation}
whose input is a history of the $k$ past states $x_{t-k : t}$ and actions $a_{t-k:t-1}$ available at  deployment, enabling rapid adaptation.  

Learning $\phi$ and $\pi$ is divided in two \textit{phases}.
\subsubsection{Phase 1: Base Policy and Environment Factor Encoder}
In \textit{Phase 1}, \ac{RMA} simultaneously trains the base policy $\pi$ and an intermediate policy, called the environment factor encoder $\mu$:
\begin{equation}
\label{eq:env_factor_encoder}
z_t = \mu(e_t)
\end{equation}
whose task is to produce the \textit{extrinsics} vector $z_t$, taking as input the vector $e_t$. 
The two modules are trained using model-free \ac{RL} (e.g., PPO \cite{schulman2017proximal}) in a simulation environment implementing the dynamics of the robot subject to instances $e_t$ of the possible model/environment uncertainties, and leveraging a reward function that captures the desired control objective. Using this procedure, \ac{RL} discovers policies that can perform well under disturbances/uncertainties. 
\subsubsection{Phase 2: Adaptation Module}
\label{eqn:rma_adaptation_module}
The adaptation module $\phi$ is obtained by generating a dataset of state-action histories in simulation via
\begin{align}
    \hat{z}_t &= \phi(x_{t-k : t}, a_{t-k; t-1}), \\
          a_t &= \pi(x_t, \hat{z}_t).
\end{align}
Because we have access to the ground truth environment parameters $e_t$ in simulation, \ac{RMA} can  compute $z_t$ at every timestep using \cref{eq:env_factor_encoder}, allowing us to train $\phi$ via supervised regression, minimizing the \ac{MSE} loss $\norm{z_t - \hat{z}_t}^2$. This is done iteratively, by alternating the collection of on-policy rollouts with updates of $\phi$ via supervised regression. 