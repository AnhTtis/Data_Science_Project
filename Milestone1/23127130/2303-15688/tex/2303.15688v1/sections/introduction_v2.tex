The deployment of complex, agile robots under uncertain models and environments demands strong robustness and rapid onboard adaptation capabilities. 
Approaches based on \ac{MPC}, such as robust/adaptive \ac{MPC} \cite{lopez2019adaptive, how2021performance, bujarbaruah2018adaptive, hanover2021performance, saviolo2022active}, have demonstrated impressive robustness and adaptation performance under real-world uncertainties, but their computational cost, associated with solving a large optimization problem online, often limits opportunities for onboard real-time deployment on computationally constrained platforms.

Recent strategies \cite{carius2020mpc, reske2021imitation, kaufmann2020deep, tagliabue2022demonstration, tagliabue2022robust, tagliabue2022output} have trained fast \ac{DNN} policies from \ac{MPC} by leveraging \ac{IL}. These approaches consider the \ac{MPC} as an ``expert'' that provides task-relevant demonstrations used to train a ``student'' policy, using \ac{IL} methods such as \ac{BC} \cite{argall2009survey} or \ac{DAgger} \cite{ross2011reduction}.
Among these methods, our previous work \cite{tagliabue2022demonstration, tagliabue2022robust}, has demonstrated that it is possible to train these policies in a way that requires very few demonstrations, and that enables the policy to achieve high robustness to uncertainties. The key idea of these methods is to use a specific type of robust \ac{MPC}, called \ac{RTMPC}, to collect demonstrations that account for the effects of uncertainties. Additionally, properties of the controller are used to derive a computationally efficient data augmentation strategy that robustifies the learning procedure, reducing the number of demonstrations needed to train a policy.
While these \ac{IL} procedures can efficiently train \textit{robust} \ac{DNN} policies capable of controlling a variety of real robots \cite{tagliabue2022output, tagliabue2022demonstration}, the obtained policies cannot adapt to the effects of model and environment uncertainties, resulting in large errors when subject to large disturbances.

In this work, we address the problem of efficiently generating a \textit{robust and adaptive} \ac{DNN} policy from a robust \ac{MPC}, endowing the policy with the ability of compensating for the errors  introduced by large uncertainties and disturbances. 
The key idea of our approach, summarized in \cref{fig:approach_diagram}, is to augment the efficient \ac{IL} strategy developed in our previous work \cite{tagliabue2022demonstration} with an adaptation scheme inspired by the recently proposed adaptive policy learning method \ac{RMA} \cite{kumar2021rma}. \ac{RMA} uses \ac{RL} to train in simulation a fast \ac{DNN} policy whose inputs include a learned low-dimensional representation of the model/environment parameters experienced during training. This low-dimensional representation can be efficiently estimated online, triggering online adaptation. \ac{RMA} policies have demonstrated impressive adaptation and generalization performance on a variety of robots/conditions \cite{kumar2021rma, kumar2022adapting, zhang2022zero, qi2022hand}.
Similar to \ac{RMA}, in our work we include to the inputs of the learned policy a low-dimensional representation of the model/environment parameters experienced during demonstration collection from \ac{MPC}; these parameters can be efficiently estimated online, enabling adaptation. Unlike \ac{RMA}, however, we bypass the challenges associated with \ac{RL}, such as reward selection and tuning, via an \ac{IL} procedure and the data efficiently generated by our \ac{MPC}-guided data augmentation strategy \cite{tagliabue2022demonstration}.   
We tailor our approach to the challenging task of \textit{trajectory tracking} for a multirotor, designing a policy that controls both \textit{position and attitude} of the robot and that is capable of adapting to uncertainties in the translational and rotational dynamics. This additionally differs from prior \ac{RMA} work for quadrotors \cite{zhang2022zero}, where the focus is only on \textit{attitude} control.
Our evaluation, performed in a high-fidelity simulation of the robot dynamics while tracking a trajectory under challenging 
disturbances, demonstrates rapid adaptation to in- and out-of-distribution uncertainties, using an adaptive policy that is learned in $1.3$ hours. Additionally our method shows comparable performance  to \ac{RTMPC} combined with a high-performance, state-of-the-art but significantly more computationally expensive disturbance observer.

In summary, our work presents the following \textbf{contributions:}
\begin{inparaenum}
\item We augment our previous efficient and robust \ac{IL} strategy from \ac{MPC} \cite{tagliabue2022demonstration} with the ability to learn robust \textit{and adaptive} policies, therefore reducing tracking errors under uncertainties while maintaining high learning efficiency. Key to our work is to leverage the performance of an RMA-like \cite{kumar2021rma} adaptation scheme, but without relying on \ac{RL}, therefore avoiding reward selection and tuning.
\item We apply our proposed methodology to the task of adaptive position and attitude control for a multirotor, demonstrating for the first time \ac{RMA}-like adaptation to uncertainties that cause position and orientation errors, unlike previous work \cite{zhang2022zero} that only focuses on adaptive attitude control. 
\end{inparaenum}

