\begin{table}[t] 
\caption{Robot/environment parameter ranges during training and testing. For most parameters, the testing range is twice as wide as the training range, allowing us to examine the performance of our methods under out-of-distribution model errors and disturbances. The nominal values are the average of the training ranges. At each timestep, each entry in $e_t$ changes with $p=0.001$ in the training environment, and $p=0.002$ in the test environment.}
\label{table:env-params}
\begin{tabular}{|p{3cm}|l|l|}
\hline
\textbf{Parameters}                                                    & \textbf{Training Range} & \textbf{Testing Range}   \\ \hline \hline
Mass [kg]                                                              & [1.04, 1.56]            & [0.78, 1.82]             \\
Mass moment of  inertia \newline \hphantom{-}about $x, y$ [kg m$^2$]   & [6.56e-3, 9.84e-3]      & [4.92e-3, 1.15e-2]       \\
Mass moment of inertia \newline \hphantom{-}about $z$ [kg m$^2$]       & [1.01e-2, 1.51e-2]      & [7.56e-3, 1.76e-2]       \\
Translational drag \newline \hphantom{-}[N/(m/s)]                      & [0.08, 0.12]            & [0.06, 0.14]             \\
Rotational drag \newline \hphantom{-}[Nm/(rad/s)]                      & [8.0e-5, 1.2e-4]        & [6.0e-5, 1.4e-4]         \\
Arm length [m]                                                         & [0.132, 0.198]          & [0.099, 0.231]           \\
Ext.~force along \newline \hphantom{-}$x, y, z$ [N]                    & [-2.55, 2.55]           & [-3.19, 3.19]            \\
Ext.~torque about \newline \hphantom{-}$x, y$ [Nm]                     & [-0.421, 0.421]         & [-0.526, 0.526]          \\
Ext.~torque about $z$ [Nm]                                             & [-4.21e-2, 4.21e-2]     & [-4.21e-2, 4.21e-2]      \\ \hline
\end{tabular}

\end{table}

\begin{table}[t] 
\caption{Time required to generate a new action. Our approach (\texttt{DAgger+Tube-100-FT}) is on average $12$ times faster than the optimization-based Expert, and $24$ times faster than an optimization-based approach with disturbance observer (RTMPC+DO). All times are reported in milliseconds (ms). While our previous work (SA) \cite{tagliabue2022demonstration} achieves a faster inference time than our method, it lacks adaptation, which our method adds with minimal computational cost.} \label{table:runtime}
\begin{tabular}{|p{1in}|l|l|l|l|l|}
\hline
Method & Setup & Mean & St. Dev. & Min & Max \\ \hline \hline
Expert                                       & CVXPY     & 9.51 & 6.16 & 5.04 & 62.2 \\ 
RTMPC+DO                                     & CVXPY     & 19.0 & 14.0 & 13.5 & 937 \\ 
SA \cite{tagliabue2022demonstration}         & PyTorch   & 0.491 & 7.55e-2 & 0.458 & 1.54 \\ 
\texttt{D+T-100-FT} (Ours)                   & PyTorch   & 0.772 & 3.68e-2 & 0.669 & 1.58 \\  \hline
\end{tabular}
\vskip-4ex
\end{table}

\begin{table}[t] 
\caption{Average position error  while tracking  $8$\,s-long trajectories in the test environment. Each policy was evaluated over $30$ realizations of the test environment Our approach (\texttt{DAgger+Tube-100-FT}) achieves successful adaptation, obtaining lower error than our previous, non-adaptive strategy  (SA), and lower or comparable errors  to RTMPC+DO.}
\begin{center}
\begin{tabular}{|l|p{.225\columnwidth}|p{.225\columnwidth}|}
\hline
Method                                                & Pos. Error [m] \newline (Heart-Shape)       & Pos. Error [m] \newline (Eight-Shape)        \\ \hline \hline
Expert                                                & \hfil0.058                             & \hfil0.104                             \\ 
RTMPC+DO                                              & \hfil0.125                             & \hfil0.163                             \\ 
SA (not adaptive) \cite{tagliabue2022demonstration}   & \hfil0.278                             & \hfil0.322                             \\ 
\texttt{D+T-100-FT} (Ours)                            & \hfil0.110                             & \hfil0.175                             \\  \hline
\end{tabular}
\end{center}
\label{tab:tracking_performance}
\vskip-5ex
\end{table}

\subsection{Evaluation Approach}

We evaluate the proposed approach in the context of trajectory tracking for a multirotor, by learning to track an $8$ second long, heart-shaped trajectory and an $8$ second long, eight-shaped trajectory with a maximum velocity of $3.2$ m/s.

\noindent
\textbf{Simulation details.} \label{sec:evaluation:sim} Learning and evaluation is performed in a high fidelity simulation environment, employing a realistic nonlinear model of the dynamics of a multirotor (with $6$ motors):
\begin{align}
\label{eqn:sim_nonlinear_model}
\dot{p} &= v \nonumber \\
m\dot{v} &= f_{\text{cmd}} R_B(q) z - mg_Wz + f_{\text{drag}} + f_{\text{ext}} \nonumber \\
\dot{q} &= \frac{1}{2}\Omega(\omega)q \nonumber \\
J \dot{\omega} &= - \omega \times J \omega + \tau_{\text{cmd}} + \tau_{\text{drag}} + \tau_{\text{ext}}
\end{align}
where position and velocity $p, v \in \mathbb{R}^3$ are expressed in the world frame $W$ and $q \in \text{SO}_3$ is the attitude quaternion. The total torques $\tau_\text{cmd}$ and forces $f_\text{cmd}$ produced by the propellers, as expressed in body frame $B$, can be linearly mapped to the propellers' thrust via a mixer/allocation matrix (e.g., \cite{tagliabue2020touch}).  
We assume that the robot is subject to isotropic drag forces and torques, with $f_\text{drag} = -c_{dv}v$ and $\tau_{\text{ext}} = -c_{d\omega}\omega$, with $c_{dv} >0$, $c_{d\omega} > 0$, and to external forces $f_{\text{ext}}$ and torques $\tau_{\text{ext}}$. The environment parameter vector $e_t$ has size $13$, and contains the robot/environment parameters in \cref{table:env-params}. We use the \texttt{acados} integrator \cite{Verschueren2021} to simulate these dynamics with a discretization interval of $0.002$ s.

\noindent
\textbf{\ac{RTMPC} for trajectory tracking on a multirotor.} The controller has state of size $n_x = 12$, consisting of position, velocity, Euler angles, and angular velocity. It generates thrust/torque commands ($n_u = 4$) mapped to the $6$ motor forces via allocation/mixer matrix ($n_a = 6$). We use an adversarial heuristic to find a value for $\mathbb{W}$, and specifically we assume that it matches the external forces and torques used in training distribution (Table \ref{table:env-params}), as they are close to the physical actuation limits of the platform. The reference trajectory is a sequence of desired positions and velocities for the next $1$ s, discretized with sampling time of $0.04$ s (corresponding to a planning horizon of $N=25$, and $300$-dim vector). The controller takes into account state constraints (i.e., available 3D flight space, velocity limits, etc) and actuation limits, and is simulated to run at $500$ Hz.

\noindent
\textbf{Student policy architecture.} The base policy $\pi$ is a $3$-layer \ac{MLP} with $256$-dim hidden layers, which takes as input the current state and extrinsics vector $z_t \in \mathbb{R}^{8}$ and outputs motor forces $a_t \in \mathbb{R}^6$. The environment encoder $\mu$ is a $2$-layer \ac{MLP} with $128$-dim hidden layers, taking as input environment parameters $e_t \in \mathbb{R}^{13}$. The adaptation module $\phi$ projects the latest $400$ state-action pairs into $32$-dim representations using a $2$-layer \ac{MLP}. Then, a $3$-layer 1-D CNN convolves the representation across time to capture temporal correlations in the input. The input channel number, output channel number, kernel size, and stride for each layer is $[32, 32, 8, 4]$. The flattened CNN output is linearly projected to obtain $\hat{z}_t$. Like the \ac{RTMPC} expert, the student policy is simulated to run at $500$ Hz.

\noindent
\subsection{Training details and hyperparameters} \label{training-details}
All policies are implemented in PyTorch and trained with the Adam optimizer, with learning rate $0.001$ and default parameters.
\textbf{Phase 1.} We train $\mu$ and $\pi$ by collecting $10$ s long trajectories starting from slightly different initial states. The expert actions are sampled at $20$ Hz, resulting in $200$ expert actions per demonstration (when no additional samples are drawn from the tube). When drawing additional samples from the tube, we do so in two different ways. The first is to uniformly sample the tube, extracting $N_\text{samples} = \{25, 50, 100\}$ samples per timestep for every demonstration that we collect from the expert; these methods are denoted as \texttt{DAgger+Tube-$N_\text{samples}$}. In the second way, we apply data augmentation (using $N_\text{samples} = 100$ samples per timestep) only to the first collected demonstration, while we use  DAgger only (no data augmentation) for the subsequent demonstrations. This method is denoted as \texttt{DAgger+Tube-100-FT} (Fine-Tuning, as DAgger is used to fine-tune a good initial guess generated via data augmentation). These different procedures enable us to study trade offs between achieving higher robustness/performance (when adding more samples) and training time (by adding fewer). Across our evaluations, we always set the DAgger hyperparameter $\beta$ to $1$ for the first demonstration and $0$ otherwise.  \textbf{Phase 2.} Similar to previous RMA-like approaches \cite{kumar2021rma,kumar2022adapting,zhang2022zero,qi2022hand}, we train $\phi$ via supervised regression, conducting $20$ iterations with $10$ policy rollouts collected in parallel per iteration. 

\subsection{Efficiency, robustness, and performance} 
\begin{figure}[t!]
    \centering\includegraphics[width=0.5\textwidth,trim = {0.2cm, 0, 0.05cm, 0,}, clip ]{figs/eval/combined_train_v7.pdf}
    \caption{Performance and robustness in the \textbf{training environment} after \textit{Phase 1}, as a function of number of demonstrations and training time. \ac{IL} allows for the learning of effective \textit{Phase 1} policies in one hour on a single core, as opposed to \ac{RL} which has been reported to take two hours on an entire desktop machine \cite{zhang2022zero}. This training time can be significantly further shortened using tube-guided data augmentation during training.} \label{fig:perf_robust_train}
    \vskip-4ex
\end{figure}

\begin{figure}[t!]
    \centering\includegraphics[width=0.5\textwidth,trim = {0.2cm, 0, 0.05cm, 0}, clip]{figs/eval/combined_test_v7.pdf}
    \caption{Performance and robustness in the \textbf{testing environment} after \textit{Phase 1}, as a function of number of demonstrations and training time. The test environment presents a set of disturbances that the robot has never seen during training, as highlighted in \cref{table:env-params}. Methods that rely on our tube-guided data augmentation strategy (\texttt{DAgger+Tube}) generalize better than \texttt{DAgger}, achieving higher robustness and performance in lower time.}
    \label{fig:perf_robust_test}
    \vskip-4ex
\end{figure}

\begin{figure*}[ht!]
    \centering\includegraphics[width=0.95\textwidth]{figs/eval/traj_and_extrinsics_v10.pdf}
    \caption{Performance while tracking the heart-shaped trajectory in \cref{fig:traj_combined}. The robot is subject to an out-of-training-distribution wind-like force of $6$ N (along positive $y$-axis, shown in shaded grey area) that is $36\%$ larger than any external forces seen during training. Our method (\texttt{DAgger+Tube-100-FT}) is computationally efficient, robust and adaptive, as shown by the changes in extrinsics ($\hat{z}_2$) when the robot is subject to wind. Our method achieves $10\%$ lower tracking errors than  RTMPC+DO,  a model-based controller which is both robust and adaptive at the cost of being computationally expensive to run online (see Table \ref{table:runtime}). We addionally maintain similar performance to the Expert, an RTMPC that has access to the ground truth model and disturbances and represents best case performance of a model-based controller. This additionally highlights improvements over our previous work SA \cite{tagliabue2022demonstration}, a learning-based controller which is robust and computationally efficient, but non-adaptive.}
    \label{fig:traj_and_extrinsics}
    \vskip-2ex
\end{figure*}
In this part, we analyze our approach on the task of performing \textit{Phase 0 and 1}, as these are the parts where our method introduces key changes compared to prior \ac{RMA} work, on the task of learning the heart-shaped trajectory (\cref{fig:traj_combined}). We study the performance (average position error from the reference trajectory) and robustness (avoiding violation of state and actuation constraints) of our policy as a function of the total training time and the number of demonstrations used for training. Our comparison includes the \ac{RTMPC} expert (\texttt{Expert}) that our policy tries to imitate, and a policy trained only with DAgger (\texttt{DAgger}), without any data augmentation. Each policy is evaluated on $10$ separate realizations of the training/test environment. We repeat the procedure over $6$ random seeds. All training is done on a single core of a desktop computer.  

Figure \ref{fig:perf_robust_train} shows the evaluation of the policy under a new set of disturbances sampled from the same training distribution, defined in Table \ref{table:env-params}, . highlighting that our tube-guided data augmentation strategy efficiently learns robust \textit{Phase 1} policies. Compared to DAgger-only, our methods achieves full robustness in less than \textit{half} the time, and using only 20\% of the required expert demonstrations. Additionally, tube-guided methods achieve about half the position error of DAgger for the same training time, reaching an average of $5$ cm  in less than $10$ minutes.  Among tube-guided data augmentation methods, we observe that fine-tuning (\texttt{DAgger+Tube-100-FT}) achieves the lowest tracking error in the shortest time.
Figure \ref{fig:perf_robust_test} repeats the evaluation under a challenging set of disturbances that are outside the training distribution (see Table \ref{table:env-params}). The analysis, as before, highlights the benefits of the data augmentation strategy, as \texttt{DAgger+Tube} methods achieves higher robustness and performance. The performance on this test environment confirms the trend that fine-tuning (\texttt{DAgger+Tube-100-FT}) achieves good trade-offs in terms of training time and robustness. Overall, these results highlight that our method can successfully and efficiently learn \textit{Phase 1} policies capable to handle out-of-distribution disturbances. Additionally, the training efficiency of our proposed \textit{Phase 1} compares favorably to the \ac{RL}-based results in \cite{zhang2022zero}, where the authors report $2$ hours of training time for \textit{Phase 1}. 

\begin{figure}
    \centering\includegraphics[trim=35 65 400 148,clip,width=\columnwidth]{figs/eval/traj_combined_v7.pdf}
    \caption{Tracking of a heart-shaped trajectory under strong, out-of-distribution wind-like external force disturbances. The blue plane $x=0$ divides the environment into a part with wind ($x<0$) and one without ($x>0$). Our adaptive approach (\texttt{DAgger+Tube-100-FT}) demonstrates an improvement on our previous work (SA), which is robust but not adaptive, and it is able to match the performance of robust MPC combined with a disturbance observer (RTMPC + DO), but at a fraction of its computational cost.}
    \label{fig:traj_combined}
    \vskip-3ex
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{figs/eval/traj_combined_lem_v4.pdf}
    \caption{Tracking of a eight-shaped trajectory under strong, out-of-distribution disturbances and model errors, where mass and arm length are twice the nominal values, the drag is $10$ times the nominal, and there is a $2$\,Nm external torque disturbance. The blue plane $x=0$ divides the environment into a part with model errors ($x<0$) and without ($x>0$). Our adaptive approach (\texttt{DAgger+Tube-100-FT}) can adapt during agile flight, reaching top speeds of $3.2$\,m/s, while maintaining performance comparable to RTMPC+DO.}
    \label{fig:traj_combined_lem}
    \vskip-3ex
\end{figure}


\subsection{Adaptation Performance Evaluation}

In this part we analyze the adaptation performance of our approach after \textit{Phase 2}. We consider the heart-shaped and the eight-shaped trajectory. For each trajectory, we train a $\mu$ and $\pi$ in \textit{Phase 1} with \texttt{DAgger+Tube-100-FT}, fine-tuning for $5$ DAgger iterations, collecting $10$ demonstrations per iteration during fine-tuning. Given a trained $\mu$ and $\pi$, we train $\phi$ via supervised regression in \textit{Phase 2} (Section \ref{training-details}). \textit{Phase 1} takes about 20 minutes on a desktop machine with $2$ GPUs; phase 2 takes about an hour.

First, we evaluate the tracking performance of our adaptive controller in an environment subject to position-dependent winds, as shown in \cref{fig:traj_combined}. The wind applies $6$ N of force, a force $36\%$ \textit{larger} than any external force encountered during training. We compare our approach with Sampling-Augmentation (SA), our previous non-adaptive robust policy learning method \cite{tagliabue2022demonstration}, and with the \ac{RTMPC} expert that has access to $e_t$ (the true value of the wind). We also consider an \ac{RTMPC} whose state has been augmented with external force/torques estimated via a state-of-the-art nonlinear disturbance observer (RTMPC+DO) \cite{tagliabue2019robust}, a method that has access to the nominal model of the robot (matching the one used in this experiment) and ad-hoc external force/torque disturbance estimation. The results are presented in \cref{fig:traj_and_extrinsics} and \cref{fig:traj_combined}. The shaded section of \cref{fig:traj_and_extrinsics}, corresponding to the windy regions,  highlights that our approach (\texttt{DAgger+Tube-100-FT}) is able to adapt to a large, previously unseen force-like disturbance, obtaining a tracking error, at convergence, lower than $10$ cm, unlike the corresponding non-adaptive variant (SA), that instead suffers from a $50$ cm tracking error. \cref{fig:traj_and_extrinsics} additionally highlights changes in the extrinsics, that do not depend on changes in reference trajectory but rather on the presence of the wind, confirming successful adaptation of the policy. \cref{tab:tracking_performance} reports a $10 \%$ reduction in tracking error compared to RTMPC+DO. Second, we repeat the evaluation on the challenging eight-shaped trajectory, with speeds of up to $3.2$ m/s, where the robot is subject to a large set of out-of-distribution model errors: twice the nominal mass and arm length, and ten-times the nominal drag coefficients, and an external torque of $2.0$ Nm. 
\cref{tab:tracking_performance} and \cref{fig:traj_combined_lem} highlight the adaptation capabilities of our approach, that performs comparably to the more computationally expensive (\cref{table:runtime}) RTMPC+DO.
\textbf{Efficiency at deployment.} Table \ref{table:runtime} reports the time to compute a new action for each policy. On average, our method (\texttt{DAgger+Tube-100-FT}) is $12$ times faster than the expert, and $24$ times faster than RTMPC combined with a disturbance observer. Evaluation is performed on an Intel i9-10920X with two Nvidia RTX $3090$ GPUs.

