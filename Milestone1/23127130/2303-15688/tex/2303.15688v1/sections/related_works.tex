\textbf{Adaptive Control.}
Adaptation strategies can be classified into two categories, direct and indirect methods. Indirect methods aim at explicitly estimating models or parameters, and these estimates are leveraged in model-based controllers, such as \ac{MPC} \cite{borrelli2017predictive}. Model/parameters identification include filtering techniques \cite{svacha2020imu, wuest2019online}, disturbance observers \cite{tagliabue2020touch, tagliabue2019robust, mckinnon2016unscented}, set-membership identification methods \cite{lopez2019adaptive, how2021performance} or active, learning-based methods \cite{saviolo2022active}. While these approaches achieve impressive performance, they often suffer from high computational cost due to the need of identifying model parameters online and, when \ac{MPC}-based strategies are considered, solving online large optimization problems.
Direct methods, instead, develop policy updates that improve a certain performance metric. This metric is often based on a reference model, while the updates involve the shallow layers of the \ac{DNN} policy \cite{joshi2019deep, joshi2020design, zhou2021bridging}. Additionally, policy update strategies can be learned offline using meta-learning \cite{richards2021adaptive, oconnel2022neural}. While these methods employ computationally-efficient \ac{DNN} policies, they require extra onboard computation to update the policy, and/or require costly offline training procedures, or do not account for actuation constrains. 
Our work leverages the inference speed of a \ac{DNN} for computationally-efficient onboard deployment, training the policy using an efficient \ac{IL} procedure (our previous work \cite{tagliabue2022demonstration}) that uses a robust \ac{MPC}  capable to account for state and actuation constraints. 

\iffalse
\begin{itemize}
\item Non-parametric: 
    \begin{itemize}
        \item model-reference: MRAC: \cite{joshi2019deep} (Deep MRAC) \cite{joshi2020design} (Deep MRAC on multirotor), + Angela's online tuning \cite{zhou2021bridging}, MRAC + GP \cite{chowdhary2014bayesian}
        \item MPC + L1, scaramuzza \cite{hanover2021performance}
        \item adaptive MPC, Pavone \cite{sinha2022adaptive}
    \end{itemize}
\item Parametric / System identification
    \begin{itemize}
        \item Brett ADTMPC: \cite{lopez2019adaptive, how2021performance}, requires running MPC onboard, compute expensive
        \item Neural lander: \cite{shi2019neural}: is there any online adaptation here? Don't think so, maybe remove.
        \item Online/active learning learning: \cite{saviolo2022active} (saviolo)
        \item (model learning with GP: \cite{zhang2020learning} -> not really adaptive, only model learning offline)
    \end{itemize}
\item Hybrid: 
    \begin{itemize}
        \item Meta Learning: \cite{richards2021adaptive} (pavone), \cite{oconnel2022neural} (chung)
        \item RMA
    \end{itemize}
\end{itemize}
\fi

\textbf{\acf{RMA}.}
\ac{RMA} \cite{kumar2021rma} has recently emerged as a high-performance, hybrid adaptive strategy. Its key idea is to learn a \ac{DNN} policy conditioned on a low-dimensional (encoded) model/environment representation that can be efficiently inferred online using another \ac{DNN}. The policy is trained using \ac{RL}, in a simulation where it experiences different instances of the model uncertainties/disturbances. \ac{RMA} policies have controlled a wide range of robots, including quadruped \cite{kumar2021rma}, biped \cite{kumar2022adapting}, hand-like manipulators \cite{qi2022hand} and multirotors \cite{zhang2022zero}, demonstrating rapid adaptation and generalization to previously unseen disturbances. Our work takes inspiration from the adaptation strategy introduced by \ac{RMA}, as we learn policies conditioned on a low-dimensional environment representation that is estimated online. However, unlike \ac{RMA}, our learning procedure does not require the  reward selection and tuning typically encountered in \ac{RL}, as it leverages an efficient \ac{IL} strategy and \ac{MPC}. An additional difference to \cite{zhang2022zero}, where \ac{RMA} is used to generate a policy for attitude control of multirotors of different sizes, is that our work focuses on the challenging task of learning an adaptive trajectory tracking controller, that compensates for the effects of uncertainties on its attitude \textit{and} position.

\textbf{Efficient Imitation Learning from MPC}
%\andrea{TODO: describe here the data augmentation strategy, because it is not described anywhere else... And need to introduce DAgger.}
%Recent work \cite{kaufmann2020deep} has  demonstrated the ability to learn agile control policies from \ac{MPC} leveraging on-policy \ac{IL} (DAgger \cite{ross2011reduction}) in combination with robustification procedures (Domain Randomization \cite{tobin2017domain}). These methods, however, require long training times on expensive training equipment.
Our previous work \cite{tagliabue2022demonstration} has focused on designing efficient policy learning procedures by leveraging \ac{IL} and a \ac{RTMPC} to guide demonstration collection and co-design a demonstration-efficient data augmentation procedure. This approach has enabled efficient learning of policies capable to perform trajectory tracking on multirotors \cite{tagliabue2022demonstration} and sub-gram, soft-actuated flapping-wings aerial vehicles  \cite{tagliabue2022robust}, also enabling efficient sensorimotor policy learning \cite{tagliabue2022output}. In this work, we extend this procedure by enabling online \textit{adaptation}, while maintaining high efficiency (in terms of demonstrations and training time) of the policy learning procedure. 