\documentclass[letterpaper, 10 pt, conference]{ieeeconf} 

\IEEEoverridecommandlockouts         
\overrideIEEEmargins                

\input{utils/usepackage.tex}
\input{utils/macros.tex}

\title{\LARGE \bf
Efficient Deep Learning of Robust, Adaptive Policies \\ using  Tube MPC-Guided Data Augmentation}


\author{Tong Zhao$^{1,*}$, Andrea Tagliabue$^{2,*}$, Jonathan P. How$^{2}$% <-this % stops a space
\thanks{*Equal contribution}% <-this % stops a space
\thanks{$^{1}$ Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology. \texttt{tzhao@mit.edu}}%
\thanks{$^{2}$ Department of Aeronautics and Astronautics, Massachusetts Institute of Technology. \tt\{atagliab, jhow\}@mit.edu}%
\thanks{This work was funded by the Air Force Office of Scientific Research MURI FA9550-19-1-0386.}
}


\input{utils/acronyms.tex}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The deployment of agile autonomous systems in challenging, unstructured environments requires adaptation capabilities and robustness to uncertainties. Existing robust and adaptive controllers, such as the ones based on \ac{MPC}, can achieve impressive performance at the cost of heavy online onboard computations. Strategies that efficiently learn robust and onboard-deployable policies from \ac{MPC} have emerged, but they still lack fundamental adaptation capabilities. In this work, we extend an existing efficient \ac{IL} algorithm for robust policy learning from \ac{MPC} with the ability to learn policies that adapt to challenging model/environment uncertainties. The key idea of our approach consists in modifying the \ac{IL} procedure by conditioning the policy on a learned lower-dimensional model/environment representation that can be efficiently estimated online. We tailor our approach to the task of learning an adaptive position and attitude control policy to track trajectories under challenging disturbances on a multirotor. Our evaluation is performed in a high-fidelity simulation environment and shows that a high-quality adaptive policy can be obtained in about $1.3$ hours. We additionally empirically demonstrate rapid adaptation to in- and out-of-training-distribution uncertainties, achieving a $6.1$ cm average position error under a wind disturbance that corresponds to about $50\%$ of the weight of the robot and that is $36\%$ larger than the maximum wind seen during training.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:introduction}
\input{sections/introduction_v2.tex}

\section{Related Works} \label{sec:relaed_works}
\input{sections/related_works.tex}

\section{Preliminaries} \label{sec:preliminaries}
\input{sections/preliminaries.tex}

\section{Approach} \label{sec:approach}
\input{sections/approach_v2.tex}

\section{Evaluation} \label{sec:evaluation}
\input{sections/evaluation.tex}

\section{Conclusions} \label{sec:conclusions}
\input{sections/conclusion.tex}

%\section*{ACKNOWLEDGMENT}
%This work was funded by the Air Force Office of Scientific Research MURI FA9550-19-1-0386. We thank Lauren Li, Xiaoyi (Jeremy) Cai, Kota Kondo and Yulun Tian  for reviewing the manuscript.  

\balance
\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}


