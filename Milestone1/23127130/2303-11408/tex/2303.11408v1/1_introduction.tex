
\section{Introduction}
\label{sec:introduction}

Diffusion-based text-to-image (TTI) systems are one of the most recent machine learning approaches in prompted image generation, with models such as Stable Diffusion~\cite{rombach2022high}, Make-a-Scene~\cite{gafni2022make}, Imagen~\cite{saharia2022photorealistic} and DALL{\textperiodcentered} E~\cite{ramesh2022hierarchical} gaining considerable popularity in a matter of months.  Many of these models are finding their way into applications ranging from generating stock imagery~\cite{shutterstockdalle2022}  (see Figure~\ref{fig:tti_applications}) to graphic design~\cite{adobedalle} as they generate increasingly realistic and diverse images based on user prompts.
% 
As sociotechnical systems that are widely deployed in different sectors and tools, ML-enabled TTI systems are also particularly likely to amplify existing societal biases and inequities --- indeed, to the extent that machine learning (ML) artifacts are constructed by people, biases are present in all ML models (and, indeed, technology in general).
However, despite recent scholarship calling for stronger bias analysis grounded in the models' application contexts to start addressing real-world harms ~\cite{Blodgett2020LanguageI,hundt2022robots,srinivasan2021biases}, these risks and biases remain sparsely documented, often described in very broad terms in model cards~\cite{sdv14modelcard, mishkin2022dall} and in papers describing new models~\cite{saharia2022photorealistic}.
The anecdotal evidence of racist, homophobic and misogynistic images shared both on social media\footnote{For instance, see the following Twitter threads: \href{https://twitter.com/Demhamasta/status/1615520883010830338}{[1]}, \href{https://twitter.com/SashaMTL/status/1582729717232173057}{[2]}, \href{https://twitter.com/WriteArthur/status/1512429306349248512}{[3]}.} and in mainstream journalism~\cite{algoportrait,lensa} in recent months outlines the need to go much further in both the scope and the specificity of the bias documentation accompanying deployed models.


\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{subfigure}[b]{0.3\textwidth}
            \includegraphics[width=\textwidth]{socialworker_SD14.jpg}
            \caption{SD 1.4}
        \end{subfigure}
        \hspace{0.1cm}
        \begin{subfigure}[b]{0.3\textwidth}
            \includegraphics[width=\textwidth]{socialworker_SD2.jpg}
            \caption{SD 2}
        \end{subfigure}
        \hspace{0.1cm}
        \begin{subfigure}[b]{0.3\textwidth}
            \includegraphics[width=\textwidth]{socialworker_Dalle.jpg}
            \caption{Dall{\textperiodcentered}E 2}
        \end{subfigure}   
        \caption*{Average face for ``Photo portrait of a social worker''}
    \end{subfigure}
    \hspace{1.5cm}  
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{subfigure}[b]{0.3\textwidth}
            \includegraphics[width=\textwidth]{CEO_SD14.jpg}
            \caption{SD 1.4}
        \end{subfigure}
        \hspace{0.1cm}
        \begin{subfigure}[b]{0.3\textwidth}
            \includegraphics[width=\textwidth]{CEO_SD2.jpg}
            \caption{SD 2}
        \end{subfigure}
        \hspace{0.1cm}
        \begin{subfigure}[b]{0.3\textwidth}
            \includegraphics[width=\textwidth]{CEO_dalle2.jpg}
            \caption{Dall{\textperiodcentered}E 2}
        \end{subfigure}   
        \caption*{Average face for ``Photo portrait of a CEO''}
    \end{subfigure}
  \caption{Images for prompts containing the profession 'social worker' (left) and 'CEO' (right) generated by Stable Diffusion v.1.4 (SD 1.4) and  v.2 (SD 2) and \DallE, as presented in our \href{https://huggingface.co/spaces/tti-bias/diffusion-faces}{Average Faces Comparison} tool.}
  \label{fig:teaser}
\end{figure}


In the present article, we introduce a set of approaches to support the analysis of the social biases embedded in TTI systems and enable their documentation, which we illustrate by comparing images generated by Stable Diffusion v.1.4, Stable Diffusion v.2, and \DallE.
%Working with generated images allows us to work both with models that are open-access as well as those that only provide API access, and focusing our analysis on feature correlations between sets of generated images allows us to study trends correlated with social variation without assigning pre-defined social categories to synthetic images that lack a social context. 
Specifically, we focus on comparing how the model outputs vary when input prompt texts mention different professions and gender-coded adjectives to how they represent variation when the prompts explicitly mention words related to gender and ethnicity.
We also present a series of interactive visualization tools that enable the exploration of model generations, contributing towards lowering the barrier to entry for exploring these models, particularly for non-technical users.
We conclude with a discussion of the implications of our work and propose promising future research directions for continuing work in this space.

%\subsection{How Diffusion Models Work} \label{sec:diffusion-works-how}
% 

% \paragraph{Use case 1: using a TTI model to generate profile pictures for social media}
%AI-powered image generation apps have gathered popularity in recent years, with apps like Lensa and ProfilePicture AI being used to generate millions of images. Many of these apps have been covered in the media due to problematic transformations such as lightening the skin of people of color~\cite{algoportrait} or objectifying women by undressing them depending on their perceived ethnicity~\cite{lensa}. Also, the increased use of TTI models in artist-oriented tools such as the "AI Magic" tools proposed by RunwayML~\cite{runwayml2023}, which provide users with moods such as "beautiful" and "aggressive", can also have unintended consequences when these words are differently coded for different social groups, which carries the risk of amplifying stereotypes. In general, disparate representation of identity groups in the media reinforces unconscious biases and diminishes minorities or women's access to specific professions~\cite{metaxa2021image}, as well as having a documented negative effect on the self-esteem of individuals from the groups concerned~\cite{perloff2014social}, so usage of TTI systems in such applications should be done with a clear documentation of and communication around its biases and limitations.

% \paragraph{Use case 2: integrating a TTI model into a stock picture generation service}

% \paragraph{Use case 3: integrating a TTI model into a “virtual sketch artist” software}

%accompanied by calls for more extensive auditing and analysis of model biases.

%Given that TTI systems are sociotechnical systems that are widely deployed in different sectors and tools, they are particularly likely to amplify existing societal biases and inequities.
% Recent scholarship has called for a stronger grounding of bias analysis in real-world application and harms~\cite{Blodgett2020LanguageI} --- harms that depend significantly on the ML systems' deployment choices and contexts~\cite{hundt2022robots,srinivasan2021biases}. %Following these recommendations, we detail three applications of TTI systems and discuss how their design choices may transform model biases into downstream negative impacts.% for individuals and communities.


% \YJcomment{Just moved the text on Kow Diffusion Models Work to Section 2, will replace with a summary here}
