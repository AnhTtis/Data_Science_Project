
\section{Discussion} \label{sec:discussion}

In the present article, we described our analysis of social dynamics reflected in the outputs of three text-to-image systems: \DallE, Stable Diffusion v 1.4 and Stable Diffusion v 2. Our goal was to explore different avenues of analysis that remained model-agnostic and applicable in a variety of different contexts.
We also strove to avoid common issues associated with handling identity characteristics that stem from relying on models that predict race or gender as fixed categories inferrable from a person's appearance, and to support intersectional analyses by jointly modeling social variables.
As such, our approach remains modular and adaptable to evolving definitions of gender and ethnicity.
Finally, the tools that we have created in the process of our research support the interactive and empirical examination of biases, aiming to empower members of our communities to use them for the use cases and avenues of research that are relevant to them. We now discuss some of the consequences of the findings presented so far, as well as the limitations of our proposed approach.

\subsection{Consequences of Observed Biases in Text-to-Image Systems}


In January 2023, Shutterstock announced that they were launching a generative AI tool fully integrated into its existing stock image platform, proposing custom-generated imagery based on user prompts (see Figure~\ref{fig:tti_applications}(a))~\cite{shutterstock2023}.
Stock images are ubiquitous across the internet, making them apt to easily propagate stereotypes and biases when they are then used in websites, advertisements, and media. In this work, we have shown that using TTI systems in a common use case of stock imagery, pictures of people in professional occupations,~\footnote{At the time of writing, 2 of the 9 featured examples on the \href{https://www.shutterstock.com/photos}{ShutterStock landing page} nominally depict a ``Doctor'' and people in a ``Business'' setting.} is likely to exacerbate those biases. These include lower gender diversity across the board (Section~\ref{sec:results:gender-biases-discrete}), depicting certain care professions (dental assistant, event planner) as nearly exclusively feminine, or positions of authority (director, CEO) as exclusively masculine for Stable Diffusion v.1.4 and \DallE respectively (Section~\ref{sec:results:cluster-biases:targeted}). These biases can then participate in the devaluation of certain kinds of work~\cite{occupational_feminization} or putting up additional barriers to access to careers for already under-represented groups~\cite{metaxa2021image}.

An even more concerning application of TTI systems would be one that uses its ability to generate human figures from descriptions to build a “virtual sketch artist” software (as in Figure~\ref{fig:tti_applications}(b)) that could be used by police departments to generate photo-realistic images of suspects based on the verbal testimony of witnesses~\footnote{The prototype code for such a software already exists, using the OpenAI API to fetch images of suspects based on text-based prompts~\cite{eagleAI2022}, although we have not found evidence of it being used in a product at the current time.}.
In our work, we have shown that some adjectives that are likely to be used in such depictions are not only gender-coded but also carry ethnicity-related biases --- for example, the ``ambitious'' adjective over-represents the cluster associated with the terms ``Black'' and ``man'' more than those associated with either of those terms separately (Table~\ref{tab:appendix:clusters-adjective-gender-all}).
%On top of known biases of the CLIP model in terms of disparate associations with `thief' and `criminal', as well as disproportionately misclassifying black people~\cite{agarwal2021evaluating}, it is conceivable to imagine that these biases will directly result in discrimination by systematically directing police departments towards people of color, 
Using a system with these biases in such a setting would put already over-targeted populations at an even increased risk of harm ranging from physical injury to unlawful imprisonment.

Finally, the ordering of the models in terms of diversity provided in Table~\ref{tab:diversity-main} calls further attention to the trade-off between diversity and ``safety filtering'' as it is currently commonly applied, adding further anecdotal evidence to a phenomenon already described by model developers.~\footnote{Compared to Stable Diffusion v.1.4, v.2 is trained on a larger dataset and also fine-tuned on an ``aesthetic'' subset with NSFW-filtering. The ``safety'' filtering approach of the \DallE dataset is describe in a \href{}{blog post} where the authors note its adverse impact on diversity.} While there is still much work to be done to better evaluate the unintended impact of filtering out ``undesirable'' content, growing evidence shows that marginalized populations can bear the brunt of the negative consequences when developers' values are prioritized~\cite{Birhane2021TheVE,dodge2021documenting} (especially values of scale~\cite{Bender2021OnTD}).

% \YJcomment{We showed significant correlation between adjectives and social attributes - ambitious for example over-represents not just men but Black men! See Table~\ref{tab:appendix:clusters-adjective-gender-sd-14-dalle}. What does it mean for a digital sketch artist that uses adjectives in the input?}

%\YJcomment{TODO: Our results provide \textbf{additional} anectodal evidence of a negative correlation between NSFW filtering in pre-training \textbf{as it is currently applied} and diversity of the generations, in line with some of the information in the DallE safety post. This will be a good occasion to call (again!) for more intentional curation instead of post-hoc "hotfixes" for social issues.}

\subsection{Limitations and Future Work} \label{sec:limitations}

Despite our best efforts, our research presents several limitations that we are cognizant of.
First, the models that we used for generating captions and VQA answers both have their own biases (many of which we describe in Sections~\ref{sec:background:diffusion} and~\ref{subsec:bias-multimodal}), which we are unable to control for in our analyses. We aimed to compensate for these by leveraging multiple models and comparing their outputs, as well as by using less symbolic models such as BoVW.
%But in any case, any machine analysis of machine generated content will lack a social grounding through which to analyze such trends due to the fictive nature of the individuals depicted.
Second, while the open nature of the Stable Diffusion models makes us reasonably certain that we did not miss any major confounding factors in our comparative analysis, the same is not true of \DallE~. Given that it is only available via an API, we were unable to control for any kind of prompt injection or filtering~\footnote{A blog post from the creators of \DallE in July 2022 stated that they added a technique to improve the diversity of its representations~\cite{dallesafety2022}, with many speculating that it is based on prompt injection~\cite{siddiqui2022metadata,sign-that-spells-offer-22}.}, or indeed whether we were at all prompting the same model on different days. We were therefore only able to compare the output of all three models on the assumption that the images they generate correspond to the model outputs based on the input prompts. %In general, efforts aiming to carry out systematic audits of any ML systems will be hindered both by a lack of access to the internal workings of the systems, making in-depth oversight of proprietary models difficult despite the importance of model auditing for promoting accountability and integrity~\cite{raji2020closing}. 
  %Automatic evaluation of social biases must look beyond commonly used diversity metrics that require assignment to a socially constructed group~\cite{lorber1991social,Jenkins1994RethinkingEI}. 
Third, our analyses are limited to a given set of social attributes in terms of gender and ethnicity, which, as we describe in Section~\ref{sec:background:gender-ethnicity}, are attributes that are inherently fluid, multidimensional and non-discretizable. Finally, we recognize that none of the authors of this paper have primary academic backgrounds in scientific disciplines relevant to the social science dimensions of gender and ethnicity, and we do not have first-hand experiences for many of the identity characteristics that we refer to.

We consider our work to be a first step in exploring societal biases in text-to-image models, with much follow up work needed to make this work more complete and nuanced. An important part of this work would be to keep exploring different dimensions and aspects of social bias, including age and visual markers of religion, as well as other target attributes that are tied to stereotypes and power dynamics. We hope that future work will continue to carefully consider the complex, interconnected nature of many types of biases and the fact that many attributes cannot be inferred visually from generated images. Finally, we believe that there is much potential work to be done in further developing  interactive tools such as those we created to support qualitative analysis and storytelling around the model biases, as well as empowering stakeholders and communities with less technical expertise to engage with and probe TTI systems and other ML artefacts.
% Another aspect that we started exploring in our work but have yet to make meaningful progress upon is the role of random seed selection on the behavior of model behavior and biases. For instance, in the case of open-access models such as Stable Diffusion, by keeping a random seed fixed and changing prompt attributes (such as adjectives), it is possible to carry out model `perturbations' and hone in on the aspects of the image that change and which ones stay fixed. Given the increased usage of these models in many contexts and use cases, we plan on continuing this work in order to better understand the limitations and capabilities of TTI models and the biases that they encode and transmit. 
\clearpage
