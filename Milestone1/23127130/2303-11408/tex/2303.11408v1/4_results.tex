\section{Results}
\label{sec:results}

The goal of our analysis is to develop ways of analyzing and comparing the biases of text-to-image systems that would allow users of these systems to shed light on these otherwise impenetrable systems.
%While the results that we present below are specific to the set of prompts that we describe in Section~\ref{methodology:prompting}, our intention is to make our approach as generic as possible.
Section~\ref{sec:results:gender-biases-discrete} compares the {\sc profession} target attribute in the ``Professions'' dataset to the {\sc gender} social attribute through the use of discrete textual representations of the images obtained with a captioning and a VQA system;
%We contrast the proportion of textual markers of gender for men and women to the gender statistics from the U.S. Labor Bureau to quantify the models' tendency to exacerbate biases. 
Section~\ref{sec:results:cluster-biases} then explores the system outputs' joint variation across dimensions of gender and ethnicity following the methodology introduced in Section~\ref{sec:methodology:bias-analysis}. Finally, Section~\ref{sec:results:tools} describes new interactive tools that we developed based on the artifacts created in our approach and showcases how the tools can support a qualitative analysis and storytelling around the model biases.

\subsection{Gender Bias Analysis through Text Markers}
\label{sec:results:gender-biases-discrete}

As described in Section~\ref{sec:methodology:prompting}, we used captions and open-ended VQA to obtain textual representations of the generated images, whose discrete nature makes identifying trends related to social attribute variation more tractable. % We examine these through the lens of ethnicity and gender below.
More specifically, we base our evaluation on the likelihood that a caption or VQA answer for a given profession contains gender-marked words such as `man' and `woman' or gender-unspecified descriptors such as `person' or the profession name --- we present these results in Table~\ref{table:vqa-captions}.
% with the percentages of men and women in these professions provided by the Labor Bureau~\footnote{The US Labor Bureau only provides statistics for two gender categories on their \href{https://www.bls.gov/cps/earnings.htm\#demographics}{website}.}
% , mentioned in both the captions and the words returned by the VQA model when prompted with \textit{"What word best describes this person's appearance?"}: we present these results in Table~\ref{table:vqa-captions}.
%, also comparing the percentage of captions that contain gender-specific terms with the most recent statistics provided by the US Labor bureau regarding the proportion of employed persons per profession~\cite{laborbureau2022}. 
\begin{table}[h!]
\begin{tabular}{l|c|c|c|c|c|c|c|c}
& \multicolumn{3}{c|}{captions} & \multicolumn{3}{c}{VQA} & \multicolumn{2}{|c}{Labor Bureau} \\ \hline
&   \% woman & \% man & \begin{tabular}[c]{@{}l@{}} \% gender\\markers\end{tabular} &  \% woman & \% man & \begin{tabular}[c]{@{}l@{}} \% gender\\markers\end{tabular} &  \% woman & \% man \\ \hline
SD v.1.4 & 38.04\% & 61.96\% & 97.24 \% & 37.77 \% & 62.23\%~ & 47.92\% & &\\ \cline{1-7}
SD v.2  & 33.45\% & 66.55 \% & 96.66\% & 31.10 \% & 68.90\% & 44.50\% & 47.03\% & 52.97\%\\ \cline{1-7}
Dall-E 2 & 19.96\% & 80.04\% & 99.09\% & 21.95 \% & 78.05\% & 44.25\% & & \\ \cline{1-7}
Average & 30.48\% & 69.52\% & 97.66\% & 30.06\% & 69.67\% &  45.56\% & &  \\ \hline
\end{tabular}
\caption{The average percentage of mentions of `woman', `man', `person' in the captions generated by a Vision Transformer Model, the BLIP VQA model and the difference between these percentages and those provided by the U.S. Bureau of Labor Statistics. N.B. these percentages are based on the number of captions/VQA appearance words containing gender markers, not the total number of data points.}
\label{table:vqa-captions}
\vspace{-0.75cm}
\end{table}
In total, 97.66\% of the captions generated contained gender-marked terms, versus 45.56\% of VQA appearance predictions: this is consistent given the fact that VQA mostly consists of single word predictions, whereas captions are full sentences. To put the percentage of predictions that contain gender-marked terms such as 'man' and 'woman' into perspective, we compare them with the percentages of men and women in these professions provided by the BLS~\footnote{The BLS only provides statistics for two gender categories on their \href{https://www.bls.gov/cps/earnings.htm\#demographics}{website}.}. We find that \DallE~has the largest discrepancy compared to the BLS-provided numbers, with its captions mentioning women on average 27\% less, and its VQA mentioning them 25\% less, and Stable Diffusion v.1.4 having the least (approximately 9\% for both captions and VQA). The professions with the biggest discrepancy between the BLS and both captions and VQA across all models are: \emph{clerk} (57 and 55\% less), \emph{data entry keyer} (55/53\% less) and \emph{real estate broker} (52/54\% less), whereas those that have more captions that mention women are: \emph{singer} (29/36\% more), \emph{cleaner} (20/16\% more) and \emph{dispatcher} (19/16\% more). Very few of the generated image captions mention gender-neutral terms such as `person' (an average of less than 1\% of captions for any of the models, distributed equally across professions), and none use the `non-binary' gender marker.
%; the professions with the highest proportion of unspecified gender in their captions were manicurist (3.9\%), carpet installer (3.8\%) and cleaner (3.3\%) -- closer inspection of the images generated for these professions using the Bias Explorer tool described in Section~\ref{sec:results:tools} showed that many of the images for these professions had human hands or bodies without visible faces, which could have contribute to the captioning model using gender-neutral terms to caption those images. 
Also, less than 0.5\% of captions and 2\% of VQA generations explicitly mention the profession in the prompt, but it is interesting to note that a single profession, \emph{police officer}, had explicit mentions of the profession name in 80.95\% of captions. In the case of VQA, several others also had a significant number of explicit mentions, including \emph{doctor} (70.48\% of VQA), \emph{firefighter} (45.71\%) and \emph{pilot} (29.84\% of VQA). We believe this to be due to the markedness~\cite{andrews1990markedness} of these professions and of the high proportion of gendered references to individuals (as opposed to using gender-neutral terms such as `person`) in the data used for training both models. Furthermore, the images generated for each profession and model combination can explored using the \href{https://huggingface.co/spaces/tti-bias/diffusion-bias-explorer}{Bias Explorer} and \href{https://huggingface.co/spaces/tti-bias/diffusion-faces}{'Average Face'} interactive demos.

\subsection{From Multidimensional Identity Clusters to Quantifying Biases}
\label{sec:results:cluster-biases}

Section~\ref{sec:results:gender-biases-discrete} presents a first view of the gender biases in the models, surfaced by the presence of gender markers in an image's generated textual description.
In order to further quantify the models' diversity in terms of both gender and ethnicity without having to solve a poorly-specified and poorly-motivated identity label assignment problem, we now turn to our proposed non-parametric method to identify patterns of variation in the images' embedding spaces that correlate with joint variation of the social attributes under consideration.

\subsubsection{Characterizing the ``Identities'' clusters.}
\begin{figure}[h!]
\centering
\begin{minipage}[b]{0.6\textwidth}
\begin{tabular}{l@{\hspace{.25em}}l@{\hspace{.25em}}l@{\hspace{.25em}}l}
            \includegraphics[width=.24\textwidth]{cluster_16_img_1_Photo_portrait_of_a_Hispanic_woman_at_work_3.jpg} &             \includegraphics[width=.24\textwidth]{cluster_16_img_2_Photo_portrait_of_a_Multiracial_woman_at_work_3.jpg} & 
              \includegraphics[width=.24\textwidth]{cluster_16_img_3_Photo_portrait_of_a_Latino_woman_at_work_6.jpg} &
                \includegraphics[width=.24\textwidth]{cluster_16_img_4_Photo_portrait_of_a_Hispanic_woman_at_work_9.jpg} \\
            \includegraphics[width=.24\textwidth]{cluster_13_img_1_Photo_portrait_of_a_Latinx_person_at_work_4.jpg} &             \includegraphics[width=.24\textwidth]{cluster_13_img_2_Photo_portrait_of_a_Latino_person_at_work_1.jpg} & 
              \includegraphics[width=.24\textwidth]{cluster_13_img_3_Photo_portrait_of_a_Latinx_woman_at_work_3.jpg} &
                \includegraphics[width=.24\textwidth]{cluster_13_img_4_Photo_portrait_of_a_Hispanic_woman_at_work_8.jpg} \\
\end{tabular}
\vspace{-1.3em}
\caption{Clusters 16 (top) and 13 (bottom) in the 48-clusters setting both feature \textit{Latinx} and \textit{woman} prompt terms, with different hair types and skin tones.}\label{tab:entropies}
\end{minipage}
\hfill
\begin{minipage}[b]{0.35\textwidth}
    \small
    \centering
    \begin{tabular}{p{.5cm}|ccc|}
    & \multicolumn{3}{c|}{Number of Clusters} \\
                & 12          & 24            & 48          \\
    \bottomrule
    BLIP    & \multirow{2}{*}{\textbf{1.10}}  & \multirow{2}{*}{\textbf{1.49}}  & \multirow{2}{*}{\textbf{1.98}}  \\
    VQA & & & \\
    &&&\\
    CLIP        & 1.3          & 1.6          & 2.15         
    \end{tabular}
    \vspace{.5em}
    \captionof{table}{The VQA question embedding best separates the social attributes according to the entropy of the clusters. Using a 99\% confidence interval bootstrap estimator, all values are $\pm$0.02. Statistically significant results are bolded.\vspace{-1.7em}}\label{fig:latinx-clusters}
\end{minipage}
\end{figure}

\iffalse
\begin{figure}[h!]
\centering
\begin{subfigure}[t]{0.4\textwidth}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_16_img_1_Photo_portrait_of_a_Hispanic_woman_at_work_3.jpg}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_16_img_2_Photo_portrait_of_a_Multiracial_woman_at_work_3.jpg}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_16_img_3_Photo_portrait_of_a_Latino_woman_at_work_6.jpg}
        \end{subfigure}   
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_16_img_4_Photo_portrait_of_a_Hispanic_woman_at_work_9.jpg}
        \end{subfigure}   
    \end{subfigure}
    \vspace{0.2 cm}
    \begin{subfigure}[t]{\textwidth}
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_13_img_1_Photo_portrait_of_a_Latinx_person_at_work_4.jpg}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_13_img_2_Photo_portrait_of_a_Latino_person_at_work_1.jpg}
        \end{subfigure}
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_13_img_3_Photo_portrait_of_a_Latinx_woman_at_work_3.jpg}
        \end{subfigure}   
        \begin{subfigure}[t]{0.23\textwidth}
            \includegraphics[width=\textwidth]{cluster_13_img_4_Photo_portrait_of_a_Hispanic_woman_at_work_8.jpg}
        \end{subfigure}   
    \end{subfigure}
\end{subfigure}
\hspace{0.3cm}
\begin{subfigure}[t]{0.5\textwidth}
    \small
    \begin{tabular}{l|lll}
                & 12 Clusters             & 24 Clusters             & 48 Clusters             \\
    \bottomrule
    BLIP VQA    & \textbf{1.103(+0.021)}  & \textbf{1.485(+0.020)}  & \textbf{1.982(+0.024)}  \\
    CLIP        & 1.277(+0.019)           & 1.577(+0.023)           & 2.148(+0.020)         
    \end{tabular}
\end{subfigure}
\captionlistentry[table]{A table beside a figure}
\captionsetup{labelformat=andtable}
\label{tab:entropies}
\caption{
\textbf{Figure, left:} Cluster 16 (top) and Cluster 13 (bottom) in the 48-clusters setting both feature Latinx and woman in most of the clusters' images' prompts, but the clusters represent different hair types and skin tones.
\textbf{Table, right:} The VQA question embedding best separates the social attributes according to the entropy of the clusters. Statistically significant results per a 99\% confidence interval bootstrap estimator are bolded.
}
\label{fig:latinx-clusters}
\end{figure}
\fi

We embed and cluster the pictures for each $<$gender, ethnicity$>$ prompt, using the embedding and clustering methods discussed in Section \ref{sec:methodology}. We then calculate the average entropy across these clusters, shown in Table~\ref{tab:entropies} -- a lower entropy score means that the prompted social attributes are more clustered together. As can be seen, the BLIP VQA question embeddings have the lowest entropy, suggesting this embedding method is the most useful for representing visual depictions of social attributes in this setting (see Figure ~\ref{fig:appendix:clustering} in the Appendix for more details on the embeddings).
We also find that while most clusters correspond to specific  intersections of gender and ethnicity, some focus on one or the other: see the Appendix (Tables~\ref{tab:cluster-composition-12}, \ref{tab:cluster-composition-48-a}, and ~\ref{tab:cluster-composition-48-b}) for the full distribution disaggregated by social identity terms.

Figure~\ref{fig:latinx-clusters} illustrates how the method of using visual embeddings for gender and ethnicity terms, rather than single labels, allows for ethnicity to be represented with varied visual features.
The example images shown are from the two clusters that prominently feature the ethnicity term ``Latinx''\footnote{The term ``Hispanic'' is used in the U.S.~Census to correspond to ``Hispanic, Latino, or Spanish origin'', and this has recently been revised to capture the geographic diversity of this broad category. \cite{marks2021improvements}.}. 
In both clusters, ``Latinx'' is the most frequently appearing ethnicity term and ``woman'' is the most frequent gender word. The two clusters depict people with different skin tones and hair types, showcasing the visual variation of the ethnicity group and further reinforcing the difficulty of identifying ethnicities solely based on visual characteristics. We provide an \href{https://huggingface.co/spaces/tti-bias/cluster-explorer}{interactive tool} to further explore the individual clusters.


% \MMdelete{In order to understand what the different regions of the embedding space identified by our clustering method correspond to, we can look at the distribution of social attributes across the prompts that led to generating a cluster's images; we provide the full distribution disaggregated by social attribute in the BLIP 12-cluster and 48-cluster settings in the Appendix (Tables~\ref{tab:cluster-composition-12}, \ref{tab:cluster-composition-48-a}, and ~\ref{tab:cluster-composition-48-b}). The distribution showcases some of the qualities of social attributes that are inherently hard for a classification-based approach to capture; for example, while most clusters correspond to a specific  intersections of the gender and ethnicity dimensions, some also mostly focus on the one or the other as more salient to handle stereotypical representations.}

%\MMdelete{Figure~\ref{fig:latinx-clusters} further showcases our method's ability to handle representation of ethnicity at a greater granularity than classification-based methods. For example, the "Hispanic" category is of particular interest in the US context, since following a recent change Hispanic origin and ethnicity constitute two different questions in the Census form~\cite{marks2021improvements}. 
%In order to investigate how our model might represent this phenomenon, we show the most representative examples of all the clusters in the 48-cluster setting that have ``Hispanic'', ``Latino'', or ``Latinx'' as the most represented ethnicity in the Appendix (Figure~\ref{fig:clustering-latinx}), and examples from the ones that contain the most prompts featuring the gender marker ``woman'' in Figure~\ref{fig:latinx-clusters}. Looking at the most representative examples of all the clusters in the 48-cluster setting that have ``Hispanic'', ``Latino'', or ``Latinx'' as the most represented ethnicity, we find that both clusters 13 and 16 in the 48-cluster setting have Latinx as the most featured ethnicity word in the prompts, and woman as the most represented gender word. However, as can be seen in Figure~\ref{fig:latinx-clusters}, the two clusters depict the human figures with different skin tones and hair types, showcasing the variability of this social group and further reinforcing the difficulty of identifying ethnicities based on visual characteristics.}

 %provides a further visual representation of these three embedding methods through a 2D projection, showing that both ethnicity and gender in the prompts are separated by these models.
% \MMdelete{Table~\ref{tab:entropies} compares the three embedding methods, in terms of how well their clusters capture variation of social attributes. We measure this by considering the average entropy of the distribution of the pictures sampled for a prompt with each (gender, ethnicity) combination across clusters; a low entropy means that social attributes in prompts tend to be mapped to specific clusters rather than spread across all of them. By this measure, the BLIP VQA question embeddings best represent social attributes. Figure ~\ref{fig:compare-models-2d} in the Appendix provides a further visual representation of these three embedding methods through a 2D projection, showing that both ethnicity and gender in the prompts are separated by these models.}
% \YJcomment{TODO: add to appendix - BLIP and CLIP separate SD 14 similarly, CLIP is a little better at separating \DallE generations and BLIP is much better at separating SD v2 generations}

\subsubsection{Measuring the aggregated social diversity of the model outputs.}

Having identified regions in the space of the image generations' embeddings that reflect variations in visual features associated with social attributes,
%reproduce dynamics of the social attributes at play,
we can now use them to quantify the diversity and representativity of the models' outputs for the target attributes.
We measure diversity as the entropy of the distribution of the examples across these regions. Entropy increases from the least diverse setting (all examples in the same region) to the most diverse (all regions equally likely). 
\begin{table}[h!]
\centering
\small
\begin{tabular}{l||cc|cc||cc|cc}
           & \multicolumn{4}{c||}{BLIP VQA embedding}                                  & \multicolumn{4}{c}{CLIP embedding}                                        \\
           & \multicolumn{2}{c|}{12 clusters}    & \multicolumn{2}{c||}{48 clusters}    & \multicolumn{2}{c|}{12 clusters}         & \multicolumn{2}{c}{48 clusters} \\
Model & Ids         & Profs                 & Ids         & Profs                 & Ids               & Profs                & Ids               & Profs        \\
\bottomrule
SD v1.4    & 3.40 $(\pm$0.04) & \textbf{2.30 $(\pm$0.01)} & 4.95 $(\pm$0.07) & \textbf{4.09 $(\pm$0.01)} & {\ul 3.25 $(\pm$0.02)} & \textbf{2.44 $(\pm$0.01)} & {\ul 4.72 $(\pm$0.05)} & \textbf{4.05 $(\pm$0.01)} \\
SD v2      & 3.43 $(\pm$0.03) & 1.86 $(\pm$0.01)          & 4.90 $(\pm$0.06) & 3.64 $(\pm$0.01)          & 3.08 $(\pm$0.04)       & 2.13 $(\pm$0.01)         & {\ul 4.72 $(\pm$0.06)} & 3.59 $(\pm$0.01) \\
\DallE & 3.35 $(\pm$0.04) & 1.36 $(\pm$0.01)          & 4.85 $(\pm$0.04) & 3.07 $(\pm$0.01)          & {\ul 3.24 $(\pm$0.04)} & 1.95 $(\pm$0.01)         & 4.55 $(\pm$0.05)       & 3.46 $(\pm$0.01)
\end{tabular}
\caption{Comparing the diversity of images generated by all three models, measured as the entropy of their image assignments across clusters (99\% bootstrap confidence interval). While the ``identities'' images  generated by all models are spread roughly evenly across clusters (``Ids'' columns), the ``professions'' images generated by \DallE are significantly less diverse than those generated by Stable Diffusion v1.4, with v2 standing in the middle between the two (``Profs'' columns).}
\label{tab:diversity-main}
\vspace{-0.7cm}
\end{table}

Table~\ref{tab:diversity-main} summarizes the outcome of this measurement across datasets, embedding methods, and number of clusters. Notably, entropy is very similar across models for their respective ``identities'' datasets; the values are mostly within each other's confidence intervals in the corresponding columns.
This means that all models showcase a similar range of visual features when explicitly prompted to depict various combinations of the gender and ethnicity social attributes.
% This means that all models are indeed able to explore a diverse range of depictions of gender and ethnicity when expressly prompted to do so.
However, in a generation setting that is closer to a standard use case where these social attributes are left unspecified, the generations of Stable Diffusion v2 and \DallE are much less visually diverse, with \DallE ranking last in all settings. This pattern is also apparent when projecting all of the image embeddings into a common 2D space --  we can observe that while the images span the whole space for all models for the ``identities'' dataset, the prompts focused on the target attributes cover a smaller part of the space for Stable Diffusion v2 than for v1, and smallest of all for \DallE (see Figure~\ref{fig:appendix:clustering} in the Appendix for a visual representation).

We motivated our analysis of the social dynamics of TTI systems in Section~\ref{sec:background} and ~\ref{sec:methodology} by pointing out their potential for exacerbating existing social biases and stereotypes. 
While aggregated measures of diversity are an important first step in understanding those dynamics, the impact of both of these phenomena depend on which social groups are stereotyped or misrepresented.
Thus, we need to also be able to characterize which specific social attributes are under-represented to give rise to lower diversity values across the identified regions.
%bias analyses need to be grounded in a social context that depends on how specific groups are represented. In the case of our analysis, this grounding starts with understanding which social groups are under-represented in a way that leads to the observed lower diversity for some of the models.
Table~\ref{tab:cluster-assignments-12} starts providing such a characterization by linking the frequency of specific clusters to the most featured social attributes in their ``identities'' images' generation prompts. Of particular note are clusters 5, 6, and 8 -- while \DallE has slightly more examples assigned to cluster 5, which corresponds to features associated with ``non-binary'' prompts, it also under-represents both clusters associated with ``Black'' and ``African American'' ethnicity prompts, reflecting well-known social biases against Black people in the US~\cite{Benjamin2019RaceAT}.

\begin{table}[h!]
\footnotesize
\begin{tabular}{l|lc|lc|lc|lc|lc|lc|lc|lc}
Cluster & \multicolumn{2}{c}{4} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{5} & \multicolumn{2}{c}{6} & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{7} & \multicolumn{2}{c}{11} & \multicolumn{2}{c}{8} \\
\midrule
\bottomrule
SD 1.4 & \multicolumn{2}{c|}{47.1} & \multicolumn{2}{c|}{27.3} & \multicolumn{2}{c|}{4.0} & \multicolumn{2}{c|}{3.9} & \multicolumn{2}{c|}{4.9} & \multicolumn{2}{c|}{3.2} & \multicolumn{2}{c|}{3.1} & \multicolumn{2}{c}{3.0} \\
SD 2 & \multicolumn{2}{c|}{58.2} & \multicolumn{2}{c|}{24.1} & \multicolumn{2}{c|}{4.8} & \multicolumn{2}{c|}{4.7} & \multicolumn{2}{c|}{1.9} & \multicolumn{2}{c|}{1.3} & \multicolumn{2}{c|}{1.8} & \multicolumn{2}{c}{2.5} \\
Dall-E  & \multicolumn{2}{c|}{74.0} & \multicolumn{2}{c|}{13.1} & \multicolumn{2}{c|}{5.8} & \multicolumn{2}{c|}{0.1} & \multicolumn{2}{c|}{1.5} & \multicolumn{2}{c|}{3.3} & \multicolumn{2}{c|}{1.6} & \multicolumn{2}{c}{0.4} \\
\midrule
\multirow{3}{*}{Ethnic.} & White & 29.2 & Lat-x & 19.1 & White & 15.8 & AfrAm & 32.9 & S-Asi & 30.2 & PacIs & 16.0 & 1stNa & 20.6 & AfrAm & 35.7 \\
 & Unspe & 28.7 & Cauca & 14.7 & Cauca & 14.7 & Black & 31.1 & Hispa & 20.3 & SEAsi & 13.5 & Lat-x & 14.7 & Black & 34.4 \\
 & Cauca & 27.5 & Hispa & 13.7 & Multi & 8.5 & Multi & 24.8 & Lat-o & 16.3 & Lat-o & 10.9 & Lat-o & 13.7 & Multi & 22.1 \\
\cmidrule{2-17}
\multirow{3}{*}{Gender} & Man   & 55.6 & Wom & 81.4 & NB    & 90.4 & Wom & 52.8 & Man   & 51.0 & Man   & 42.3 & NB    & 67.6 & Man   & 53.9 \\
 & Unspe & 42.1 & Unspe & 11.8 & Wom & 9.6 & NB    & 28.6 & Unspe & 45.5 & Unspe & 32.7 & Wom & 26.5 & Unspe & 42.9 \\
 & NB    & 2.2 & NB    & 6.9 &       &     & Unspe & 18.6 & NB    & 3.5 & NB    & 25.0 & Unspe & 5.9 & NB    & 2.6 
\end{tabular}
\caption{The 8 most represented ``identities'' clusters in the 12-cluster setting. The top 3 lines correspond to the proportion of images in the ``professions'' dataset generated by each model that are assigned to the cluster. The bottom 6 lines show the top three ethnicity and top three gender words for the prompts that generated the cluster's images.
\textbf{Cluster 4} is the most represented cluster across models and is made up of ``identities'' images generated for prompts mostly mentioning the words White/Caucasian/Man. 
\textbf{Cluster 6}, which is made up of ``identities'' examples mostly mentioning Black/African American/Woman, represents only 3.9\% and 4.7\% of Stable Diffusion ``professions'' generations for v1.4 and v2 respectively, and only 0.1\% for \DallE}
\label{tab:cluster-assignments-12}
\vspace{-0.7cm}
\end{table}

\subsubsection{Disaggregated analysis by adjective and profession}
\label{sec:results:cluster-biases:targeted}

Our approach also allows for a more granular analysis of the relation between the target (profession+adjective) and social (ethnicity/gender) attributes, allowing us to connect our findings to prior work on bias and stereotypes.
% and to hone in on specific dynamics that may be more relevant or likely to lead to real-world harm in a given social context.
To that end, we compare the diversity of cluster assignments for each of the target attributes by looking at the measures presented in Tables~\ref{tab:diversity-main} and~\ref{tab:cluster-assignments-12} for the subsets of the ``Professions'' dataset corresponding to each adjective, profession, and generation model. % (the Appendix presents the measures for each adjective in Tables~\ref{tab:appendix:clusters-adjective-gender-all} and ~\ref{tab:appendix:clusters-adjective-gender-sd-14-dalle} and for each profession in Tables~\ref{tab:appendix:clusters-professions-all}, ~\ref{tab:appendix:clusters-professions-sd-14}, and~\ref{tab:appendix:clusters-professions-dalle}).

Our first finding is that the adjectives do have a significant impact on the social attributes depicted in the generated images. By ordering the adjectives by the proportion of their images assigned to a cluster that primarily focus the gender marker ``man'', we can see an overall stable ordering across generation models. The adjectives ``compassionate'', ``emotional'', and ``sensitive'' lead to the least assignments to the ``man''-associated clusters (54.2\%, 60.5\%, and 61.4\% respectively), and the adjectives ``stubborn'', ``intellectual'', and ``unreasonable'' to the most (77\%, 78.2\%, and 78.4\%), which shows that the TTI systems under consideration reproduce known stereotypical associations with gender-coded adjectives~\cite{gaucher2011evidence}. 

Next, we sort the sets of images generated for each profession from most to least diverse according to our cluster entropy-based measure, and compare it to the BLS gender statistics. Across models, we find that more observed gender balance in the US workforce corresponds to more diverse generations, albeit along different axes --- primarily gender for ``singer'' (clusters 2 and 6 are more over-represented with respect to the average than 4 and 8), primarily ethnicity for ``taxi driver'' (2 and 6 under-represented, 8 over-represented) or ``maid'', and the intersection of gender and ethnicity for ``social worker'' specifically featuring the cluster most associated with ``Black'' and ``woman'' (cluster 6) (see Table~\ref{tab:appendix:clusters-professions-all} in the Appendix). We also find consistent differences between Stable Diffusion v1.4  and \DallE. Low-diversity professions for Stable Diffusion v 1.4 include ones where BLS reports more than 80\% women, with the model exacerbating gender stereotypes for ``dental assistant'', ``event planner'', ``nutritionist'', and ``receptionist'', whereas these professions tend to have higher diversity in the \DallE generation datasets that over-represent the ``man'' clusters more strongly across the board. On the other hand, we see that some of the lowest-diversity professions in the \DallE images include positions of authority such as ``CEO'' or ``director'', assigning over 97\% of generations in both cases to cluster 4 (\href{https://huggingface.co/spaces/tti-bias/cluster-explorer}{mostly ``white'' and ``man''}) even though the BLS reports these professions as 29.1\% and 39.6\% women respectively. This analysis barely scratches the surface of the different bias dynamics unearthed, and we encourage readers to look through the full data for further patterns they may find of particular interest (Table~\ref{tab:appendix:clusters-professions-sd-14} and \ref{tab:appendix:clusters-professions-dalle} in the Appendix).
% \YJcomment{TODO: spin up a Space with minimally the full Pandas tables for the cluster descriptions and cluster assignments.}

\subsection{Interactive Tools for Exploring Results}
\label{sec:results:tools}

% We also provide several no-code tools to accompany our analysis, which members of community can utilize to further continue their exploration of the images that we have generated. We provide more details regarding our methodology in the sections below.

In parallel to the approaches described in previous sub-sections, we also introduce a series of interactive tools to support story-based examination of biases in images generated by the TTI systems. The primary goal of these tools is not to produce quantitative insights into the images, but to allow more in-depth exploration of them. In fact, we created these tools as part of our own analyses, to help ourselves delve into the generated images in more detail and identify relevant trends to guide our analyses. We present the three tools that we have created below:
\begin{figure}[h!]
\centering
\hfill
\begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{diffusionexplorer.jpg}
    \caption{Diffusion Bias Explorer}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{average-faces.jpg}
    \caption{Average Face Comparison Tool}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{knn-space.jpg}
    \caption{k-NN Explorer}
\end{subfigure}
\hfill
\caption{The 3 interactive tools created as part of our analysis: \href{https://hf.co/spaces/tti-bias/diffusion-bias-explorer}{Diffusion Bias Explorer}, \href{https://hf.co/spaces/tti-bias/diffusion-faces}{Average Faces Comparison}, and the \href{https://hf.co/spaces/tti-bias/identities-knn}{k-NN Explorer}.}
\label{fig:tools}
\vspace{-0.5cm}
\end{figure}
\paragraph{Diffusion Bias Explorer} One of the first tools that we created in the scope of this project was the \href{https://hf.co/spaces/tti-bias/diffusion-bias-explorer}{Diffusion Bias Explorer} (See Fig.~\ref{fig:tools}~(a)), which enabled users to compare what the same set of prompts -- based on the list of professions and adjectives that we describe in Section~\ref{sec:methodology:prompting} -- resulted in when fed through the 3 TTI systems. It allowed us to uncover initial patterns -- including the homogeneity of certain professions (such as CEO) or the differences between versions of Stable Diffusion as well as \DallE. In fact, we used the Diffusion Bias Explorer all throughout the project in order to visually verify tendencies discovered using our analyses, since it allowed us to quickly view images for any given adjective+profession+model combination.
 
\paragraph{Average Face Comparison Tool} Another tool that we created was the \href{https://hf.co/spaces/tti-bias/diffusion-faces}{Average Face Comparison Tool} (see Fig.~\ref{fig:tools}~(b)), which leverages the \texttt{\href{https://github.com/johnwmillr/Facer}{Facer}} Python package to carry out face detection, alignment based on facial features and averaging across professions and adjectives. This helped us further see more high-level patterns in terms of the visual aspects of the images generated by the different diffusion models while avoiding facial recognition and classification techniques that would prescribe gender or ethnicity. Also, the blurriness of the average images gave us signal about how homogeneous and heterogeneous certain combinations of professions and adjectives were, and to get a better sense of how adding an adjectives changes the average faces representing professions. 

\paragraph{Nearest Neighbors Explorers: BoVW and Colorfulness} - To enable a more structured exploration of the generated images we also developed two nearest-neighbor lookup tools. Users can choose a specific image as a starting point---for example, a  photo of a Black woman generated by a specific model---and explore that photo's neighborhood either by \href{https://hf.co/spaces/tti-bias/identities-colorfulness-knn}{colorfulness} (as defined by \citet{colorfulness}), or by a \href{https://hf.co/spaces/tti-bias/identities-bovw-knn}{bag-of-visual-words~\citep{bag-of-visual-words} TF-IDF index}. To build this index, we used a bag-of-visual-words model to obtain image embeddings that do not depend on an external pre-training dataset. We then extracted each image's SIFT features~\citep{sift} and used k-means~\cite{lloyd1982least} to quantize them into a codebook representing a visual vocabulary and compute TF-IDF sparse representations and indexed the images using a graph-based approach~\citep{nndescent}. These search tools enable a structured traversal of the dataset and thereby facilitate a qualitative exploration of the images it contains, either in terms of color or structural similarity. This is especially useful in detecting stereotypical content, such as the abundance of Native American headdresses or professions that have a predominance of given colors (like firefighters in red suits and nurses in blue scrubs).% but also specific failure modes, such as the misinterpretation of the `stocker' profession as a dog-breed. % One interesting insight is that images generated by \DallE are on average the most colorful. Images of men are on average less colorful than all other gender labels, consistently across all three models.
