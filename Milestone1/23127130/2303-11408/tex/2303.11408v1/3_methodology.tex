\section{Methodology: Social Bias Analysis in TTI Systems}
\label{sec:methodology}

In this work, motivated by the necessity to better understand and contrast social dynamics across different models, we propose a new approach for quantifying the biases of  TTI systems. We have several requirements for such an evaluation. First, our method needs to \textbf{work with black-box systems}, as many TTI systems provide little to no access to their internals. Second, we want to develop an approach that is \textbf{modular enough to apply to different dimensions of bias}; i.e. to adapt to the combinations of the demographic and operational variables that are most relevant to different application contexts. Third, given the characteristics of the social variables at play as well as the issues inherent in classifying social attributes from external characteristics outlined in Section~\ref{sec:background:gender-ethnicity}, we want our method to quantify diversity \textbf{without requiring identity labels}.

In order to meet these requirements, we propose a novel \textbf{non-parametric method for evaluating bias phenomena} based on prompt variation and leveraging several  feature extraction systems. Our approach stems from the following intuition: images generated by TTI systems may lack inherent social attributes, but they do showcase features in their depiction of human figures that viewers interpret as social markers. We cannot define these markers \textit{a priori} due to the complexity and interdependence of the visual features that lead viewers to map visual representations to their social experience, nor could we reliably predict them even if we had a good enough definition~\cite{buolamwini2018gender}.
However, by using more tractable proxy representations of the images and characterizing their dynamics while controlling the social variation of a TTI system's inputs, we can start identifying regions of the representation space that correlate with this social variation, and use those to evaluate the diversity and representativeness of the system in an application setting.

Previous work has also called for explicitly stating the normative assumptions inherent in any bias evaluation framework~\cite{wang2022towards}.
Based on the previous work reviewed in Section~\ref{sec:background:tti-biases}, we are particularly concerned with evaluating the stereotypical or disparate representations of social and demographic characteristics in specific application settings.
To this end, we operationalize bias in terms of a set of \textbf{social attributes} that describe the social groups we want to ensure are fairly treated, and a set of \textbf{target attributes} that represent the variation in use-time inputs to a deployed TTI system.
Our assumptions are then: (i)~the overall distribution of model outputs across values of the target attributes should be diverse with regard to the social attributes, and (ii)~different values of the target attribute should not lead to different distributions over the social attributes. Our approach aims at measuring to what extent the TTI systems depart from these norms --- we show its components in Figure~\ref{fig:methodology} and describe them in more detail in the rest of this Section.

\begin{figure}[h!]
\center
% \includegraphics[width=0.8\linewidth]{methodology_v3.jpg}
% \includegraphics[width=0.8\linewidth]{SD_bias_methodology_v1-1.jpg}
\includegraphics[width=0.9\linewidth]{methodology.jpg}
\caption{Our approach to evaluating bias in TTI systems: We first define the \emph{target} and \emph{social attributes} of interest and programmatically generate two sets of prompts spanning the values of these attributes. We then create two corresponding datasets by randomly  sampling several image generations for each of the prompts. The target attribute dataset is used to support \emph{interactive visualization tools} and a \emph{text-based bias evaluation}. Both datasets are also embedded into a common vector space used to \emph{characterize correlations} between the social and target attribute variations. These three views of the systems' output space provide complementary insights into the bias mechanisms, which we jointly leverage to compare various systems' biases.}
\label{fig:methodology}

\end{figure}

\subsection{Exploring the Model's Generation Space through Prompt Variation}
\label{sec:methodology:prompting}

As mentioned above, our approach works by triggering \textbf{controlled variation of the TTI system} outputs across social and target attributes.
In the rest of this paper, motivated by the commercial application of TTI systems to stock imagery~\cite{shutterstockdalle2022}, we consider the scenario where a TTI system is used to generate depictions of different professional situations --- as previous work has shown stereotypical representations and lack of diversity in the representation of occupations in image search systems contribute to exacerbating disparities~\cite{metaxa2021image,kay2015unequal}.
To that end, we first design a \textbf{prompt pattern} corresponding to this scenario, use it to sample images generated by 3 TTI systems, and then leverage a suite of \textbf{feature extraction systems} to help us better model the generative dynamics of these systems.

\paragraph{Prompted Datasets for Social and Target Attributes}
% \label{methodology:prompting:generation}

In order to easily generate a diverse set of prompts to explore the systems' output space, we use the pattern \textit{``Photo portrait of a $[X]$ $[Y]$''},~\footnote{Before converging on this prefix, we experimented with several others, including \textit{"Photo of"}, \textit{"Photograph of"}, \textit{"Portrait of"}, \textit{"Close up of"}, but found that \textit{"Photo portrait of"} gave the most realistic results.} where $X$ and $Y$ can span the values of the social attributes ---  \textbf{ethnicity} and \textbf{gender} respectively--- and of the target attributes --- a descriptive \textbf{adjective} and the \textbf{profession} name.
For the \emph{target attributes}, we rely on a list of 150 occupations taken from the \href{https://www.bls.gov/cps/cpsaat11.htm}{U.S. Bureau of Labor Statistics (BLS)} for the \textit{profession} and on a list of 20 male- and female-coded \textit{adjectives} taken from a study on gender representation in job advertisements~\cite{gaucher2011evidence}, leading to a set of 3,150 prompts of all possible combinations of adjectives and professions (each combination of adjective+profession, plus only the professions without adjectives).
We then use these prompts to generate 10 images per prompt for each TTI system, for a total of 96,540 images.

For the \emph{social attributes}, we add a suffix to make the pattern \textit{``Photo portrait of a $[ethnicity]$ $[gender]$ at work''}-- adding the "at work" suffix makes the images more directly comparable to those generated for professions, given that these are often set in workplace settings. For gender, we use three values in this study: ``man'', ``woman'', and ``non-binary person''; and as an additional option we also use the unspecified ``person'' to use the pattern without specifying gender. This is still far from a complete exploration of gender variation, and in particular misses gender terms relevant to trans* experiences and gender experiences outside of the US context. For ethnicity, we are similarly grounded in the North American context, as we started with a list of ethnicities in the US census which we then expanded with several synonyms per group (the full list is available in Appendix~\ref{sec:appendix:prompt-target-attributes}). Enumerating all values of the gender and ethnicity markers we defined led to a total of 68 prompts. Examples of generations for both the social attributes and target attributes sets are shown in Figure~\ref{fig:prompts}.

\begin{figure}
%\captionsetup{justification=centering}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \begin{subfigure}[t]{0.31\textwidth}
            \includegraphics[width=\textwidth]{latinx_man_sd_14.jpg}
            \caption{Latinx man,\\SD 1.4}
        \end{subfigure}
        \hspace{0.05cm}
        \begin{subfigure}[t]{0.31\textwidth}
            \includegraphics[width=\textwidth]{woman_sd_2.jpg}
            \caption{woman,\\SD 2}
        \end{subfigure}
        \hspace{0.05cm}
        \begin{subfigure}[t]{0.31\textwidth}
            \includegraphics[width=\textwidth]{multiracial_person_dalle.jpg}
            \caption{multiracial person, Dall{\textperiodcentered}E 2}
        \end{subfigure}   
        \caption*{The ``Identities'' dataset is generated by providing the input \emph{``Photo portrait of a[n] $X$ at work.''}}
    \end{subfigure}
    \hspace{0.1cm}  
    \begin{subfigure}[t]{0.47\textwidth}
        \begin{subfigure}[t]{0.31\textwidth}
            \includegraphics[width=\textwidth]{ambitious_plumber_sd_14.jpg}
            \caption{ambitious plumber, SD 1.4}
        \end{subfigure}
        \hspace{0.05cm}
        \begin{subfigure}[t]{0.31\textwidth}
            \includegraphics[width=\textwidth]{compassionate_CEO_SD_2.jpg}
            \caption{compassionate CEO, SD 2}
        \end{subfigure}
        \hspace{0.05cm}
        \begin{subfigure}[t]{0.31\textwidth}
            \includegraphics[width=\textwidth]{nurse_sd_2.jpg}
            \caption{nurse, \DallE}
        \end{subfigure}   
        \caption*{The ``Professions'' dataset is generated by providing the input \emph{``Photo portrait of a[n] $X$.''}}
    \end{subfigure}
    %\captionsetup{justification=raggedright}
  \caption{We generate two image datasets to evaluate the models' biases, one where we vary mentions of gender and ethnicity across prompts (``identities'', examples \textbf{left}) and one where we vary mentions of profession and adjectives that have been found to be gender-coded (``professions'', examples \textbf{right}).}
  \label{fig:prompts}
  \vspace{-0.5cm}
\end{figure}

\paragraph{Image-to-Text Systems} 
%\label{sec:methodology:prompting:representation-discrete}

One category of image representations that we can more easily leverage to find bias patterns is that of text-based representations; specifically, textual representations of the figures depicted in an image. We use two ML-based systems to automatically obtain such descriptions --- one designed for image captioning and one designed for Visual Question Answering (VQA). For the former, we leverage the \href{https://huggingface.co/nlpconnect/vit-gpt2-image-captioning}{ViT GPT-2 Model}~\cite{nlp_connect_2022} trained on the MS-COCO dataset~\cite{lin2014microsoft} to generate captions for each of the images.
The VQA system, conversely, outputs a single word or a short phrase that answers the question \textit{``What word best describes this person's appearance?''} for each image --- 
we used the \href{https://huggingface.co/Salesforce/blip-vqa-base}{BLIP VQA base model}~\cite{BLIP2022} which was pre-trained on a set of 124M image-text pairs including MS-COCO~\cite{lin2014microsoft} and a subset of 115M image-text pairs from LAION-400M~\cite{schuhmann2021laion} and fine-tuned on VQA 2.0~\cite{goyal2017making}.
Both models are open-ended, which means that we do not know a priori whether the outputs will feature words directly related to the social attributes we are studying--- our goal in using them is to analyze aggregate statistics of these words across the generated images.

\paragraph{Image Embedding Systems} 
% \label{sec:methodology:prompting:representation-dense}

While textual descriptions of the images are much more tractable than their raw pixel representations, they also carry more limited information --- especially in the short VQA answer. A middle ground between the two levels of faithfulness to the information contained in an image can be achieved by leveraging dense embedding techniques that project the images into a multidimensional vector space, which we explore using two such systems. First, we look at the broadly used \href{https://huggingface.co/openai/clip-vit-base-patch32}{CLIP image encoder}~\cite{radford2021learning}, pre-trained on an undisclosed set of 400M web-scraped image-text pairs, which is used to guide the training of both the \DallE model as well as Stable Diffusion  v1.4, as well as to filter the pre-training datasets of \DallE and both versions of Stable Diffusion. Second, we also leverage the same BLIP VQA system as above to obtain image embeddings --- after early experiments using the last layer of the image encoder module of the system, we found that the normalized average of the token embeddings produced by the VQA encoder conditioned on the image seemed to produce a space with more meaningful nearest neighbors.

\subsection{Analysing Biases Using our Generated Datasets}
\label{sec:methodology:bias-analysis}

We now have two datasets of images with rich feature representations: the\textbf{``Identities'' dataset}, which contains 680 images for each TTI system generated based on the 68 prompts that explore the space of the model's output across various combinations of explicit gender and ethnicity mentions in the prompt, along with the CLIP, and BLIP VQA embeddings of these images. The \textbf{``Professions'' dataset} contains 31,500 images per TTI system, corresponding to variations of adjectives and professions along with embeddings from the same systems, along with textual captions and VQA answers describing the images. In the rest of this Section, we describe a novel non-parametric method that leverages these datasets for diversity estimation in TTI system outputs without gender or ethnicity label assignment.
% 
Our goal with this method is to create a meaningful model of the visual aspect of TTI systems that are correlated with perceptions of social attributes; with the ``Identities'' prompt dataset, we probe the model's ability to vary its outputs in ways that correspond to social variation. After projecting those outputs into an embedding space through the information bottleneck of a BLIP or CLIP model and delineating regions of the projected space that meaningfully distribute the social attributes in the prompts, the diversity of the ``Professions'' images across these regions should reflect diversity across social attributes.

In order to identify such regions, we use a two-step hierarchical clustering of the ``Identities'' image vectors. In order to obtain balanced clusters, we first cluster the vectors obtained for the datasets generated by Stable Diffusion v1.4 and 2 and \DallE together (680 per model, adding up to 2040 in total) with a dot product similarity and Ward linkage~\cite{ward1963hierarchical}, and then merge the smallest clusters to the closest centroid until we are left with 12, 24, or 48 clusters.
In contrast to methods that identify independent directions of variation in the space to denote social dimensions, we rely on clustering to address the interdependence of the social attributes; our method also models social variation without \textit{a priori} specifying how many clusters should correspond to gender or to ethnicity modes, or whether an ethnicity from our prompt set should correspond to a single cluster or be split between several. This yields a flexible but still interpretable delineation of the model's output space that we can subsequently used to characterize the ``Professions'' examples by assigning them to the closest cluster by centroid - we show how to interpret these regions as well as use them to quantify the model diversity and describe specific bias dynamics in the following Section. %, alongside our analysis of gender-marked words in the ``Professions'' text descriptions and the interactive visual tools that we created to explore the raw images.
 
% We test this approach with three embedding systems, at three different levels of granularity. We use the image embedding component of CLIP after its projection layer, as well as a BLIP model fine-tuned on a VQA task. The BLIP model works by first embedding an image using a Vision Transformer system, then embedding the tokens of the question conditioned on the image representation to then finally generate an answer. We found that taking the normalized average of the question token embeddings for targeted questions provided an apt representation of the image, so we used this embedding method with the questions \textit{``what word best describes this person's ethnicity?''}, and \textit{``what word best describes this person's appearance?''}.

% \paragraph{Clustering embeddings} \textbf{TODO: Yacine} Talk about how the clusters correspond to modes in the joint ethnicity+gender space, their social meaning is then visualized by looking at the top examples in the cluster --- similar to topic model evaluation.

% We leverage these different, albeit complementary approaches to hone in on different aspects of the generated images as well as to minimize the bias that using any single model would entail. Also, we consciously do not leverage any classification models, given that both VQA and image captioning models produce more general output and allow for constrained decoding which can be adapted to a specific set of terms (as opposed to limited to the small set of ethnic/gender categories that were defined in training dataset of classification models).

%\subsubsection{Caption vocabulary} In order to capture the most relevant features in the captions that we generated, we compared the most salient terms for each of the professions and adjectives, as well as number of times people-related terms such as 'man', 'woman', 'person' and [the name of the profession] were used for each profession and adjective, using sci-kit learn~\cite{scikit-learn}). This enabled us to hone in on specific phenomena such as \emph{markedness} -- i.e., whether specific gender- or ethnicity-marked terms were mentioned in the captions or not, as well as allowing us to identify professions that are more \emph{visually-salient} (e.g. police officer) compared to others that were less so (e.g. office worker).

%\subsubsection{Cluster assignment}
