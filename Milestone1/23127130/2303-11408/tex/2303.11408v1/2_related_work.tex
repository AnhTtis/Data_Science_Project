\section{Background}
\label{sec:background}


Bias in Machine Learning is a complex concept, encompassing disparate model performance across different categories, social stereotypes propagated by models, and biases inherent in datasets that are then learned by models and misunderstood by users. There has been extensive and insightful work analyzing the biases of ML models, both from the ML community and from many fields at the intersection of technology and society, such as social science, cognitive science, law, and policy. Our project builds upon this work in ML -- we endeavor to briefly describe the most relevant findings in the current section, and refer readers to further readings for more details about these topics. 

\subsection{Gender and Ethnicity as Variables}
\label{sec:background:gender-ethnicity}

Any project analyzing biases relevant to personal identity characteristics, such as gender or ethnicity, requires grappling with the nuances of using personal identity labels. Both gender and ethnicity are widely described as socially-constructed variables~\cite{Jenkins1994RethinkingEI} that are not founded in biology but constructed over time and based on social interactions and dynamics~\cite{lorber1991social, smedley2005race,Penner2013EngenderingRP}. Studies focusing on key sources of demographic data such as the U.S. census have found that individuals' self-identification varies over time ~\cite{Liebler2017AmericasCR} and that the boundaries between categories are often multi-dimensional and fluid ~\cite{Bowker1999SortingTO}. Both race and gender are also inherently non-discrete but rather multidimensional, spanning a spectrum along which people choose to associate themselves~\cite{Carothers2013MenAW}. In fact, research has insisted on the inherent intersectionality of gender and ethnicity, arguing that treating these categories independently obscures important phenomena \cite{Crenshaw1991MappingTM,Bowker1999SortingTO,Thomas2011GenderedRI}. 

Despite these decades of foundational work, many technical and socio-technical tools continue to operationalize both ethnicity and gender in fixed, categorical ways, often assuming that both variables can be determined externally by a person's appearance. Calls for more inclusive and nuanced approaches have stemmed from fields such as HCI~\cite{keyes2018misgendering,Gyamerah2021ExperiencesAF, OgbonnayaOgburu2020CriticalRT} and are starting to be adopted by these communities. However, archaic and damaging stereotypes of binary gender and a fixed set of ethnicity categories continue to persist in fields such as ML, which is particularly problematic given that, for instance, defining a set of categories in a (labeled) dataset will be transferred to models trained on that dataset, which can further contribute to perpetuating algorithmic harms and unfairness~\cite{benthall2019racial, barocas2017fairness,keyes2018misgendering,buolamwini2018gender}. Self-identification has been motivated as a more acceptable approach to annotation, given that often neither race nor gender are determined by apparent characteristics alone, and is increasingly becoming the standard for datasets involving humans (e.g. ~\cite{hazirbas2021towards}). Nonetheless, careful thought must be put into the way in which social attributes (such as gender and race) are used as variables in ML models~\cite{wang2022towards}. Additionally, self-identification data is often unavailable in those settings where ML is used, for example when the people represented in a dataset were not directly involved in its creation process or when a system deals with representations of people that are either fully synthetic or aggregated from real individuals, as is the case for TTI systems in general and diffusion models in particular.

\label{sec:diffusion-applications}
\begin{figure}[!t]
\centering
\begin{subfigure}[t]{0.58\linewidth}
    \includegraphics[width=\linewidth]{shutterstock.jpg}
    \caption{ShutterStock uses a TTI system to generate stock pictures for its customers}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.35\linewidth}
    \includegraphics[width=\linewidth]{forensic_ai.jpg}
    \caption{A system that leverages \DallE~to generate forensic sketches of potential suspects.}
\end{subfigure}
\caption{Some real-life applications of TTI systems that have the potential to transmit harmful societal biases.}
\label{fig:tti_applications}

\end{figure}


\subsection{Diffusion Models}
\label{sec:background:diffusion}

Diffusion models are a class of generative models inspired by non-equilibrium thermodynamics~\citep{weng2021diffusion} and trained to reverse the gradual addition of noise that is layered onto an image.
Similar to other generative models, diffusion models learn to convert noise samples---typically Gaussian---to an image. To achieve this, an image from the training dataset is gradually combined with noise at discrete time steps, until eventually, the noise overwhelms the image itself.
The model is then tasked to learn how to reverse the process and achieves this by learning how to reverse one step of noise diffusion, and does so at every training time step.
One key difference that has led to the widespread adoption of diffusion models is that they are simpler to train than previous generations of generative models, such as GANs, given sufficient data owing to their likelihood-based loss function, whose mode-covering characteristic~\citep{diffusion-efficiency} is also one of the main reasons why model outputs are so diverse compared to other classes of generative models. 

The creation of large, diverse image-text pair datasets such as LAION~\cite{schuhmann2021laion,schuhmann2022laion} has significantly contributed to the widespread adoption of diffusion models.
Both the text and images in the datasets are leveraged during diffusion model training: the text is used to guide the diffusion process described above to provide information about what the final outcome should resemble.
This is done by augmenting the denoising steps with the token embeddings of the corresponding text as generated by a pre-trained model such as CLIP~\citep{radford2021learning,open-clip}, whose image and text encoders have previously been jointly trained to create embeddings that are similar in latent space.
This can contribute to improving the quality of the generated images and make training more stable.
It also allows the model to be used for text-guided image generation from random noise at inference time.
%
However, this iterative approach where each step jointly depends on the text representation and the specific point in the denoising trajectory makes it particularly difficult to directly access the latent space of the model.
There can also be additional phases of text and image processing both pre- and post-training, for instance to filter out problematic content or detect images that are violent or sexual in nature.
This is why we refer to \emph{Text-to-Image (TTI) Systems} when referring to Stable Diffusion and \DallE as opposed to models: they are an assemblage of components and modules that all contribute to generating the final image.
Each of them can thus contribute to introducing and amplifying social biases at different stages of the model training and deployment pipeline in a myriad different ways, many of which are poorly understood in isolation, let alone in their interactions.

\subsection{Bias Evaluation in Multimodal ML} \label{subsec:bias-multimodal}
 While it has been well-established that text models and image models absorb problematic biases independently (see ~\cite{bolukbasi2016man,blodgett2021stereotyping,buolamwini2018gender,aka2021measuring}, among many others), the rapid increase in multimodal models (e.g., text-to-image, image-to-text, text-to-audio, etc.) has outpaced the development of approaches for understanding their biases. The recent wave of modality-shared latent space multimodal models, such as diffusion-based models, brings with it open questions about the interaction of biases from different modalities in a shared latent space. It has not yet been discovered whether biases in each modality amplify or compound one another, a gap that this work seeks to address. For example, if a multimodal model represents the visual concepts of ``people'' and ``White people'' in a very different space than ``Black people'',  yet represents the linguistic concept of ``people'' with higher positive sentiment terms than ``Black people'', then a compounded outcome could be model output where Black people are depicted %or communicated about 
 more negatively than White people. 

Indeed, there is support for this concern: earlier work on multimodal vision and language models found that societal biases regarding race, gender and appearance propagate to downstream outputs in tasks such as image captioning (e.g.,~\cite{hendricks2018women,bhargava2019exposing}) and image search (e.g.,~\cite{zhao2017men, mitchell2020diversity}), and has found that such biases are learned by state-of-the-art multimodal models~\cite{wolfe2022american, wolfe2022markedness}. For example, in image captioning, a task where descriptions are generated for images, models are much more likely to generate content that stereotypes the people depicted, such as by associating women with shopping and men with snowboarding \cite{zhao2017men,hendricks2018women}, or producing lower-quality captions for images of people with darker skin \cite{zhao2021understanding}. Similarly, research on biases in image search has found that algorithmic curation of images in search and digital media are likely to reinforce biases, which can have a negative impact on the sense of belonging of individuals from these groups~\cite{kay2015unequal,singh2020female,metaxa2021image,wang2021assessing}. Given the exponential popularity of image generation models, this can have negative impacts on the minority groups who feel under- or mis-represented by these technologies~\cite{sign-that-spells-offer-22}. 

\subsection{Text-to-Image Systems and their Biases}
\label{sec:background:tti-biases}

The recent increased popularity of TTI systems has also been accompanied by work that aims to better understand their inner workings, such as the functioning of their safety filters~\cite{rando2022}, the structure of their semantic space~\cite{brack2022stable}, the extent to which they generate stereotypical representations of given demographics~\cite{bianchi2022easily,cho2022dall} and memorize and regenerate specific training examples~\cite{carlini2022,somepalli2022diffusion}. Work in the bias and fairness community has also examined how models such as CLIP~\cite{radford2021learning} encode societal biases between race and cultural identity~\cite{agarwal2021evaluating,wolfe2022american,wolfe2022markedness} as well recent approaches for debiasing text and image models~\cite{berg2022prompt,bansal2022well,wang2021gender}. Some initial research has attempted to reduce the bias of images generated by Stable Diffusion models by exploring the latent space of the model and guiding image generation along different axes to make generations more representative~\cite{brack2022stable,schramowski2022safe}, although the generalizability of these approaches is still unclear. 

There are many sources from which biases can originate in TTI models, none of which have been fully explored as of yet; for instance, the training data used for training diffusion such as Stable Diffusion is scraped from the Web, which has shown to contain harmful and pornographic content~\cite{birhane2021multimodal, luccioni2021s} as well as mislabeled and corrupted examples~\cite{siddiqui2022metadata}. . These datasets then undergo filters to isolate subsets that are considered, for example, `aesthetic` or `safe for work`, which are created based on the output of classification models trained or fine-tuned on other datasets~\cite{schuhmann2022laion-aesthetics}, which can come with unintended consequences. For instance, the creators of \DallE~observed that attempting to filter out ``explicit'' content from their training data actually contributed to bias amplification,
necessitating additional post-hoc bias mitigation measures~\cite{nichol2022dallemitigations}. Further biases can be introduced during the training process, given that many TTI systems use the CLIP model~\cite{radford2021learning} to guide the training and generative process despite its biases (see above), the impact of which on image generation is still unclear. Finally, the biases of safety filters and prompt enhancement techniques used in TTI systems such Stable Diffusion v.1.4 and \DallE~are poorly understood and largely undocumented~\cite{rando2022,sign-that-spells-offer-22}. For instance, keyword-based approaches have been shown to have a disparate impact on already marginalized groups~\cite{dodge2021documenting}.
