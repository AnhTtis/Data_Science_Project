\documentclass[manuscript,screen,review]{acmart}

\newcommand{\DallE}{Dall{\textperiodcentered}E 2~}
\begin{document}

\author{Alexandra Sasha Luccioni}
%\authornote{Both authors contributed equally to this research.}
% TODO: bring back authornote for the non-anonymous version (seems bugged)

\title{Sources of Bias in ML-enabled Text-to-Image Models}

\email{sasha.luccioni@hf.co}
\affiliation{%
  \institution{Hugging Face}
  \country{Canada}
}
\maketitle

\label{sec:appendix:diffusion-bias-sources}

To the extent that ML models are constructed by people, biases are present in all ML models (and, indeed, technology in general). Given that TTI systems are sociotechnical systems that are widely deployed in different sectors and tools, they are particularly likely to amplify existing societal biases and inequities. We aim to provide a non-exhaustive list of bias sources below:

\paragraph{Biases in training data} The LAION datasets~\cite{schuhmann2021laion,schuhmann2022laion}, are built on top of the Common Crawl~\footnote{\url{https://commoncrawl.org}}, which significantly over-represents violent and pornographic content~\cite{luccioni2021s}; this has contributed to the LAION datasets themselves containing numerous documented biases and harmful associations~\cite{birhane2021multimodal}. Different subsets of LAION are used for training Stable Diffusion models, and while the exact data sources used for training \DallE~are not disclosed, pairs of images and captions from publicly available sources were used during training and ~\cite{mishkin2022dall}. These kinds of datasets gathered via Web scraping contain content that is problematic on many levels, ranging from sexual content (which may, in part, be filtered out using the pre-filtering approaches described in the following paragraph), to more pernicious behaviors such as socially-constructed discrimination against minority groups~\cite{steed2020biases}, as well as legally-questionable content such as copyrighted materials~\cite{Paullada_2021}. 

\paragraph{Biases in pre-training filtering} The datasets used for training Stable Diffusion and \DallE~were not used in their original forms -- both went through different types of filtering, which inexorably introduce different biases into the initial datasets. For instance, both Stable Diffusion models were fine-tuned on subsets of the LAION-5B dataset that have ``high visual quality", evaluated using classifiers trained on existing generated images that are considered to be aesthetically-pleasing~\cite{schuhmann2022laion-aesthetics}, despite the vagueness and subjectivity of this concept. For instance,  Stable Diffusion v2 was further trained on a subset of LAION-5B that was from which ``explicit pornographic material was filtered out using the \href{https://github.com/LAION-AI/CLIP-based-NSFW-Detector}{CLIP-based NSFW detector}, which was trained on manually-annotated training data. The definition of NSFW for this detector is similarly unclear, given that it encompasses a series of concepts ranging from nudity to violence and gore that are ill-defined and subjective, and can depend based on factors such as culture and religion.  The creators of \DallE~also made an effort to filter ``explicit'' content from their training data, which included both sexual and violent content as well as image of hate symbols, since they observed that this filtering actually contributes to bias amplification
and report to take measures to mitigate this bias~\cite{nichol2022dallemitigations}.

\paragraph{Biases in prompt interpretation} the CLIP (Contrastive Language-Image Pre-training) model~\cite{radford2021learning} is a multimodal model trained on Web-scraped content gathered from Wikipedia. It plays a key role in text-to-image systems, used to both guide the training process and to generate embeddings for the text prompts during inference~\footnote{Different versions of CLIP are used by different text-to-image models; namely, while \DallE~and Stable Diffusion v1.4 use the original CLIP model~\cite{radford2021learning}, which is trained on Wikipedia, Stable Diffusion v2 uses OpenCLIP~\citep{open-clip}, which is trained on LAION-2B.}. However, research has found that CLIP has a number of problematic biases surrounding age, gender, and race or ethnicity, for instance treating images that had been labeled as White, middle-aged, and Male as unmarked, without descriptors for age, gender, or race/ethnicities, compared to other images~\cite{wolfe2022markedness}. This suggests that when encoding an incoming prompt to the models, the text encoder implicitly interprets unspecified or underspecified gender and identity groups to signify White and male. This behavior has also been observed with regards to \DallE~,~\cite{sign-that-spells-offer-22}, but remains under-explored for other text-to-image systems. 

\paragraph{Biases in the models' latent space} Given the complex structure and various components of diffusion models, it is impossible to access models' latent spaces (i.e., the data distributions the model learns during training) directly, even for open-access models such as Stable Diffusion. One way to approximate the latent space is by using techniques such as generation sampling using fixed seeds, which have been used by some recent research analyzing model behavior~\cite{carlini2022}. This can provide a limited view of the latent space of models, especially when coupled with techniques that explore the embedding spaces for models such as CLIP, which are leveraged in TTI systems. As such, recent work analyzing CLIP model predictions has found high racial misclassification rates for images labeled as `Black`, as well as prediction of the words `thief` and `criminal` for images labeled as `male`~\cite{agarwal2021evaluating}. Initial work has been done in terms of reducing the bias of images generated by Stable Diffusion models by exploring the latent space of the model and guiding image generation along different axes such as gender to make generations more safe or representative~\cite{brack2022stable,schramowski2022safe}. However, more work is necessary to better understand the structure of the latent space of different types of diffusion models and the factors that can influence the bias reflected in generated images. 

\paragraph{Biases in post-hoc filtering} Contrary to Stable Diffusion v.2., the training data used to train Stable Diffusion v.1.4 was not filtered according to NSFW characteristics; however, it was released with a safety classifier included by default whose goal was to ``remove outputs that may not be desired by the model user"~\cite{stablev1release}. The exact workings of the classifier were not shared publicly, but an attempt to reverse-engineer the filter concluded that it was based on a predefined list of keywords and the similarity of the CLIP embeddings of these words and images generated by the model; the same analysis found that the filter missed many images containing violence, gore, flagging predominantly sexual content~\cite{rando2022}. \DallE~also carries out different kinds of post-hoc filtering on user prompts, both from a safety perspective to identify prompts violating its content policy (which covers ``shocking content; depictions of illegal activity; and content regarding public and personal health"~\cite{mishkin2022dall}), as well as from a representativity perspective, applying a ``technique so that \DallE~generates images of people that more accurately reflect the diversity of the worldâ€™s population"~\cite{dallesafety2022}. While the exact approaches used in both of these cases are not explicitly described, informal empirical investigations have hypothesized that they are based on adding gendered or racialized keyword such as `Black' to human-oriented prompts in order to get the model produce more diverse outputs~\cite{sign-that-spells-offer-22}. If this is the case, this can lead to unintended impacts with regards to already marginalized demographics, since recent research on keyword-based filtering of large text corpora has shown that mentions of sexual orientations (e.g., lesbian, gay, bisexual) have the highest likelihood of being filtered out by these approaches~\cite{dodge2021documenting}.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\end{document}