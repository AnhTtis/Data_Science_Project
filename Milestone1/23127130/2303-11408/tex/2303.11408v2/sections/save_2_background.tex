
\section{Background}
\label{sec:background}

\section{Background}
\label{sec:background}

There has been extensive, insightful work both within the ML community and in other ones in terms of analyzing and documenting the biases and limitations of ML models.
Biases of both text and image models have been predominantly studied separately in both computer vision and natural language processing, and many approaches have been proposed for both detecting and mitigating their various biases .
\begin{itemize}
    \item \citet{bolukbasi2016man} comparing word embeddings learned with word2vec to find biases corresponding to gendered stereotypes
    \item \citet{buolamwini2018gender} bring evidence of disparate performance on automatic gender recognition systems by Fitzpatrick color scale
    \item \citet{aka2021measuring} propose a PMI-based method to surface model associations between predicted labels that may be indicative of bias, and showcase it on gendered biases in image classification
\end{itemize}

\subsection{Bias Evaluation in Multimodal ML}

However, the intersection of modalities in multimodal ML models (e.g. text-to-image, image-to-text, text-to-audio, etc.) makes detecting and quantifying bias more complex, since there are multiple distinct sources of biases that must be considered (e.g. in the case of image captioning models, both the image data and the text data may amplify different inequities).
While studies of biases in multimodal models are less numerous, there is a body of work in both the image search (e.g.~\cite{zhao2017men,metaxa2021image,feng2022has, mitchell2020diversity} and image captioning (e.g.~\cite{hendricks2018women,bhargava2019exposing,zhao2021understanding}) domains. %TODO: TLDR sentence about findings? something from that CEO paper, about the link between datasets and models maybe? 

Another relevant topic of research focuses on representativity in terms of gender and race in image search results and the impact that this has~\cite{metaxa2021image,kay2015unequal,singh2020female}.
There has also been important work about mitigating biases in image search~\cite{wang2021gender}
\begin{itemize}
    \item \cite{kay2015unequal} evaluates the gender biases in an image search product (2015) as well as their impact on users, including reinforcing stereotypes but also overestimating result quality when the results agree with prior stereotypical beliefs. They find a slight exaggeration for gender and no significant deviation for US census racial categories
    \item \cite{singh2020female} study gender bias in representations of occupations on digital media platform, and find that stereotypes are most likely broken through human curation whereas algorithmic curation is more likely to reinforce it.
    \item \cite{metaxa2021image} follows up on \cite{kay2015unequal}, finds no improvement on gender and under-representation of people of color, and impact on peoples' sense of belonging based on their own identity (in-group increases for gender but \textbf{not race})
    \item \cite{wang2021gender} gender bias in image search with CLIP, proposes interventions (fair sampling of negative samples, clipping dense features correlated with gender)
\end{itemize}


Work on text-to-image models, while less prolific, has also started to gain momentum, with papers from recent years examining how models such as CLIP~\cite{radford2021learning} encode societal biases surrounding the link between race and cultural identity~\cite{wolfe2022american} as well recent approaches for debiasing text and image models~\cite{berg2022prompt,bansal2022well}. 
Their initial findings suggest that it is possible to improve the bias metrics for gender and race attributes using techniques such as adversarial learning and natural language interventions, although the extent to which is generalizable to different vision-language models is still unclear. 
Important work has been done in terms of analyzing the contents of large multimodal datasets that are used to train these models, finding evidence of worrying trends ranging from pornography to misogyny~\cite{birhane2021multimodal} as well as mislabeled examples and corrupted examples~\cite{siddiqui2022metadata}.
Finally, recent research has aimed to better understand the inner workings of diffusion models, carrying out empirical research to understand the functioning of their safety filters~\cite{rando2022}, the structure of their semantic space~\cite{brack2022stable} and the extent to which they generate stereotypical representations of given demographics~\cite{bianchi2022easily,cho2022dall} and memorize and regenerate specific training examples~\cite{carlini2022,somepalli2022diffusion}. All of these research directions provide us with complementary perspectives on the different sources of biases in text-to-image models, ranging from the data they are trained on to the internal representations that they encode, and contribute to a better understanding of these models in general.

One particular difficulty in devising a fully automated quantitative evaluation of social biases in generative ML models comes from the impossibility to assign social variables to artefact generated by an ML model.
Social categories such as gender and race are socially constructed and only meaningful within a social context~\cite{smedley2005race,lorber1991social}.
\citet{bianchi2022easily} address this issue by considering the similarity between generated outputs and real images of people who self-identified gender and ethnicity characteristics; however, this still assumes a fixed set of gender and ethnicity categories that are common to both real and generated images.
Our approach is similar in that it relies on image embeddings, but focuses on the variation in the space corresponding to the variation in the latent representation of the variables' textual markers rather than on self-identification by real people.

\subsection{Gender and Ethnicity as Variables}

\paragraph{Literature review on using gender and ethnicity as social variables in research.}
Starting with some notes:
\begin{itemize}
    \item \citet{Jenkins1994RethinkingEI}'s book reviews ethnicity as a constructed variable, the role of interactions, the relationship between categories and groups, and power in external categorization
    \item \citet{Liebler2017AmericasCR} find that individual people's racial self-identification varies over time in the US census
    \item \citet{smedley2005race} examine the construction of race from a historical and social perspective. Social race not founded in biology but \textit{``essentializes and stereotypes people, their social statuses, their social behaviors, and their social ranking [\ldots]  Physical traits have been transformed into markers or signifiers of social race identity. But the flexibility of racial ideology is such that distinctive physical traits need no longer be present for humans to racialize others''} \YJcomment{HT Sasha :)}
    \item \citet{lorber1991social}' Social Construction of Gender outlines historical variation of how gender roles are constructed trhough interaction with society and parents \YJcomment{HT Sasha :)}
    \item \citet{Crenshaw1991MappingTM}'s foundational work on intersectionality outlines how treating gender and ethnicity as independent variables obscures phenomena
    \item \citet{Thomas2011GenderedRI} find that ``gendered racial identity had greater salience for the participants as compared to the separate constructs of racial or gender identity'' for young Black women in the US
    \item \citet{Penner2013EngenderingRP} study the differential effect of gender and socioeconomic status across racial categories. \citet{OBrien2015EthnicVI} find ethnically mediated differences in implicit gender-STEM bias and participation.
    \item \citet{Gyamerah2021ExperiencesAF} study differences in gender-based violence against trans women across ethnic groups
    \item \citet{OgbonnayaOgburu2020CriticalRT}'s work on critical race theory for HCI calls for re-prioritization of storytelling
    \item \citet{Bowker1999SortingTO}'s historical analysis of classification includes a focus on the fluid boundaries and multidimensionality of categories. Includes a reference to the US OMB allowing people to select multiple categories in the US census in 1997 in chapter 6
    \item \citet{Stumpf2020GenderInclusiveHR}'s work on gender-inclusive HCI --- TODO, follow pointers in ``This does not reflect the argument that gender is a complex performance that takes in identity and expression \cite{Butler2005GenderTF}, in which gender is conceptualized along a spectrum to which people choose to associate themselves. In addition, differences among genders are not simply categorical and instead are multi-dimensional \cite{Carothers2013MenAW}'' \YJcomment{\citet{Carothers2013MenAW}'s work especially aligns with our conceptualization of gender in a latent space}
    \item \citet{keyes2018misgendering} reviews operationalization of gender in Automatic Gender Recognition and underlines common issues including flawed assumptions about the concept of gender being binary/immutable/physiological and that gender can be determined externally by a person's appearance. 
\end{itemize}
Relevant notions to outline: contextual definitions have no inherent existence outside of interactions (human-human or human-computer) --- difference between talking about groups and categories --- hiding variation by independent evaluation of gender and ethnicity --- flexible and fluctuating definitions

\subsection{Social Variables in (Generative) ML}

While the goal of our work is to quantify the societal biases ingrained in diffusion models, finding the appropriate approach for this can be difficult given the complexities of annotating protected categories in generated images~\cite{barocas2017fairness,keyes2018misgendering,buolamwini2018gender}.
Self-identification is seen as more acceptable approach to annotation (given that often neither race nor gender can be identified based on apparent characteristics alone), and is increasingly becoming the standard for datasets involving humans (e.g. ~\cite{hazirbas2021towards}).
This is obviously impossible for generated images, and indeed, given that the people on the images are not real, it is impossible to practice self-identification.
Indeed, the definition of racial categories and the use of these categories as if they are objective and distinct is socially constructed and does not reflect its true complexity and multi-dimensionality~\cite{hanna2020towards}.
The operationalization of race in algorithmic tools such as image classification models, is even more problematic, given that it treats race as a fixed, objective and measurable attribute, which can further contribute to perpetuating algorithmic harms and unfairness~\cite{benthall2019racial, buolamwini2018gender}


\subsection{Bias Evaluation in Multimodal ML}

There has been extensive, insightful work both within the ML community and in other ones in terms of analyzing and documenting the biases and limitations of ML models.
Biases of both text and image models have been predominantly studied separately in both computer vision and natural language processing, and many approaches have been proposed for both detecting and mitigating their various biases (e.g.~\cite{bolukbasi2016man,buolamwini2018gender,aka2021measuring}).
However, the intersection of modalities in multimodal ML models (e.g. text-to-image, image-to-text, text-to-audio, etc.) makes detecting and quantifying bias more complex, since there are multiple distinct sources of biases that must be considered (e.g. in the case of image captioning models, both the image data and the text data may amplify different inequities).
While studies of biases in multimodal models are less numerous, there is a body of work in both the image search (e.g.~\cite{zhao2017men,metaxa2021image,feng2022has, mitchell2020diversity} and image captioning (e.g.~\cite{hendricks2018women,bhargava2019exposing,zhao2021understanding}) domains. %TODO: TLDR sentence about findings? something from that CEO paper, about the link between datasets and models maybe? 

Another relevant topic of research focuses on representativity in terms of gender and race in image search results and the impact that this has~\cite{metaxa2021image,kay2015unequal,singh2020female}.
There has also been important work about mitigating biases in image search~\cite{wang2021gender}

Work on text-to-image models, while less prolific, has also started to gain momentum, with papers from recent years examining how models such as CLIP~\cite{radford2021learning} encode societal biases surrounding the link between race and cultural identity~\cite{wolfe2022american} as well recent approaches for debiasing text and image models~\cite{berg2022prompt,bansal2022well}. 
Their initial findings suggest that it is possible to improve the bias metrics for gender and race attributes using techniques such as adversarial learning and natural language interventions, although the extent to which is generalizable to different vision-language models is still unclear. 
Important work has been done in terms of analyzing the contents of large multimodal datasets that are used to train these models, finding evidence of worrying trends ranging from pornography to misogyny~\cite{birhane2021multimodal} as well as mislabeled examples and corrupted examples~\cite{siddiqui2022metadata}.
Finally, recent research has aimed to better understand the inner workings of diffusion models, carrying out empirical research to understand the functioning of their safety filters~\cite{rando2022}, the structure of their semantic space~\cite{brack2022stable} and the extent to which they generate stereotypical representations of given demographics~\cite{bianchi2022easily,cho2022dall} and memorize and regenerate specific training examples~\cite{carlini2022,somepalli2022diffusion}. All of these research directions provide us with complementary perspectives on the different sources of biases in text-to-image models, ranging from the data they are trained on to the internal representations that they encode, and contribute to a better understanding of these models in general.

One particular difficulty in devising a fully automated quantitative evaluation of social biases in generative ML models comes from the impossibility to assign social variables to artefact generated by an ML model.
Social categories such as gender and race are socially constructed and only meaningful within a social context~\cite{smedley2005race,lorber1991social}.
\citet{bianchi2022easily} address this issue by considering the similarity between generated outputs and real images of people who self-identified gender and ethnicity characteristics; however, this still assumes a fixed set of gender and ethnicity categories that are common to both real and generated images.
Our approach is similar in that it relies on image embeddings, but focuses on the variation in the space corresponding to the variation in the latent representation of the variables' textual markers rather than on self-identification by real people.

\subsection{Gender and Ethnicity as Variables}

\paragraph{Literature review on using gender and ethnicity as social variables in research.}
Starting with some notes:
\begin{itemize}
    \item \citet{Jenkins1994RethinkingEI}'s book reviews ethnicity as a constructed variable, the role of interactions, the relationship between categories and groups, and power in external categorization
    \item \citet{Liebler2017AmericasCR} find that individual people's racial self-identification varies over time in the US census
    \item \citet{smedley2005race} examine the construction of race from a historical and social perspective. Social race not founded in biology but \textit{``essentializes and stereotypes people, their social statuses, their social behaviors, and their social ranking [\ldots]  Physical traits have been transformed into markers or signifiers of social race identity. But the flexibility of racial ideology is such that distinctive physical traits need no longer be present for humans to racialize others''} \YJcomment{HT Sasha :)}
    \item \citet{lorber1991social}' Social Construction of Gender outlines historical variation of how gender roles are constructed trhough interaction with society and parents \YJcomment{HT Sasha :)}
    \item \citet{Crenshaw1991MappingTM}'s foundational work on intersectionality outlines how treating gender and ethnicity as independent variables obscures phenomena
    \item \citet{Thomas2011GenderedRI} find that ``gendered racial identity had greater salience for the participants as compared to the separate constructs of racial or gender identity'' for young Black women in the US
    \item \citet{Penner2013EngenderingRP} study the differential effect of gender and socioeconomic status across racial categories. \citet{OBrien2015EthnicVI} find ethnically mediated differences in implicit gender-STEM bias and participation.
    \item \citet{Gyamerah2021ExperiencesAF} study differences in gender-based violence against trans women across ethnic groups
    \item \citet{OgbonnayaOgburu2020CriticalRT}'s work on critical race theory for HCI calls for re-prioritization of storytelling
    \item \citet{Bowker1999SortingTO}'s historical analysis of classification includes a focus on the fluid boundaries and multidimensionality of categories. Includes a reference to the US OMB allowing people to select multiple categories in the US census in 1997 in chapter 6
    \item \citet{Stumpf2020GenderInclusiveHR}'s work on gender-inclusive HCI --- TODO, follow pointers in ``This does not reflect the argument that gender is a complex performance that takes in identity and expression \cite{Butler2005GenderTF}, in which gender is conceptualized along a spectrum to which people choose to associate themselves. In addition, differences among genders are not simply categorical and instead are multi-dimensional \cite{Carothers2013MenAW}'' \YJcomment{\citet{Carothers2013MenAW}'s work especially aligns with our conceptualization of gender in a latent space}
    \item \citet{keyes2018misgendering} reviews operationalization of gender in Automatic Gender Recognition and underlines common issues including flawed assumptions about the concept of gender being binary/immutable/physiological and that gender can be determined externally by a person's appearance. 
\end{itemize}
Relevant notions to outline: contextual definitions have no inherent existence outside of interactions (human-human or human-computer) --- difference between talking about groups and categories --- hiding variation by independent evaluation of gender and ethnicity --- flexible and fluctuating definitions

\subsection{Social Variables in (Generative) ML}

While the goal of our work is to quantify the societal biases ingrained in diffusion models, finding the appropriate approach for this can be difficult given the complexities of annotating protected categories in generated images~\cite{barocas2017fairness,keyes2018misgendering,buolamwini2018gender}.
Self-identification is seen as more acceptable approach to annotation (given that often neither race nor gender can be identified based on apparent characteristics alone), and is increasingly becoming the standard for datasets involving humans (e.g. ~\cite{hazirbas2021towards}).
This is obviously impossible for generated images, and indeed, given that the people on the images are not real, it is impossible to practice self-identification.
Indeed, the definition of racial categories and the use of these categories as if they are objective and distinct is socially constructed and does not reflect its true complexity and multi-dimensionality~\cite{hanna2020towards}.
The operationalization of race in algorithmic tools such as image classification models, is even more problematic, given that it treats race as a fixed, objective and measurable attribute, which can further contribute to perpetuating algorithmic harms and unfairness~\cite{benthall2019racial, buolamwini2018gender}

%TODO: human evaluation of models: https://arxiv.org/abs/2204.00447
% https://finale.seas.harvard.edu/files/finale/files/human_evaluation_of_models_built_for_interpretability.pdf 

%TODO Chris: 
% - epistemic opacity
% hermeneutic injustice 
% decontextualization of images 
% Latour Blackboxing
% feng2022has and UW


