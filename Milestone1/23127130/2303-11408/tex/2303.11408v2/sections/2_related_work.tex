\section{Background}
\label{sec:background}

Bias in ML is a complex concept, encompassing disparate model performance across different categories, social stereotypes propagated by models, and (mis-)representations present in datasets that can then be encoded by models and misunderstood by users. There has been extensive and insightful work analyzing the biases of ML models, both from the ML community and from many fields at the intersection of technology and society, such as social science, cognitive science, law, and policy (e.g.~\cite{scheuerman2021auto,hanna2020towards,de2019bias, chao2018courts,selbst2019fairness}, among many others). Our project builds upon this work in ML -- we endeavor to briefly describe the most relevant findings in the current section, and refer readers to further readings for more details about these topics. 

\label{sec:diffusion-applications}
%\begin{figure}[!t]
%\centering
%\begin{subfigure}[t]{0.58\linewidth}
%    \includegraphics[width=\linewidth]{images/shutterstock.png}
%    \caption{ShutterStock uses a TTI system to generate stock pictures for its customers}
%\end{subfigure}
%\hfill
%\begin{subfigure}[t]{0.35\linewidth}
%    \includegraphics[width=\linewidth]{images/forensic_ai.png}
%    \caption{A system that leverages \DallE~to generate forensic sketches of potential suspects.}
%\end{subfigure}
%\caption{Some real-life applications of TTI systems that have the potential to transmit harmful societal biases.}
%\label{fig:tti_applications}
%\end{figure}

\subsection{Bias Evaluation in Multimodal ML} \label{subsec:bias-multimodal}
 While it has been well-established that text models and image models encode problematic biases independently (see ~\cite{bolukbasi2016man,blodgett2021stereotyping,buolamwini2018gender,aka2021measuring}, among many others), the rapid increase in multimodal models (e.g., text-to-image, image-to-text, text-to-audio, etc.) has outpaced the development of approaches for understanding their specific biases. 
 %The recent wave of modality-shared latent space multimodal models, such as diffusion-based models, brings with it open questions about the interaction of biases from different modalities in a shared latent space. 
It has not yet been established whether biases in each modality amplify or compound one another, a gap that our work seeks to address. For example, if a multimodal model represents the visual concepts of ``people'' and ``White people'' in a very different space than ``Black people'',  yet represents the linguistic concept of ``people'' with higher positive sentiment terms than ``Black people'', then a compounded outcome could be model output where Black people are depicted %or communicated about 
 more negatively than other identity groups. 

Indeed, there is support for this concern: past work on multimodal vision and language models found that societal biases regarding race, gender and appearance propagate to downstream outputs in tasks such as image captioning (e.g.,~\cite{hendricks2018women,bhargava2019exposing}) and image search (e.g.,~\cite{zhao2017men, mitchell2020diversity}), and has found that such biases are learned by state-of-the-art multimodal models~\cite{wolfe2022american, wolfe2022markedness}. For instance, in the case of image captioning, models are much more likely to generate content that stereotypes the people depicted, such as by associating women with shopping and men with snowboarding~\cite{zhao2017men,hendricks2018women}, or producing lower-quality captions for images of people with darker skin~\cite{zhao2021understanding}. Similarly, research on biases in image search has found that algorithmic curation of images in search and digital media are likely to reinforce biases, which can have a negative impact on the sense of belonging of individuals from these groups~\cite{kay2015unequal,singh2020female,metaxa2021image,wang2021assessing}. Given the exponential popularity and widespread deployment of image generation systems, this can translate into  negative impacts on the minority groups who feel under- or mis-represented by these technologies~\cite{sign-that-spells-offer-22}. 

\subsection{Text-to-Image Systems and their Biases}
\label{sec:background:tti-biases}

The recent increased popularity of TTI systems has also been accompanied by work that aims to better understand their inner workings, such as the functioning of their safety filters~\cite{rando2022}, the structure of their semantic space~\cite{brack2022stable}, the extent to which they generate stereotypical representations of given demographics~\cite{bianchi2022easily,cho2022dall, naik2023social} and memorize and regenerate specific training examples~\cite{carlini2022,somepalli2022diffusion}. Work in the bias and fairness community has also examined how models such as CLIP~\cite{radford2021learning}, which are used to guide the diffusion process, encode societal biases between race and cultural identity~\cite{agarwal2021evaluating,wolfe2022american,wolfe2022markedness}.
Some initial research has attempted to propose approaches for debiasing text and image models~\cite{berg2022prompt,bansal2022well,wang2021gender}, as well as reducing the bias of images generated by Stable Diffusion models by exploring the latent space of the model and guiding image generation along different axes to make generations more representative~\cite{brack2022stable,schramowski2022safe}, 
although the generalizability of these approaches is still unclear. 

One of the reasons for this is that there are many sources from which biases can originate in TTI systems; for instance, the training data used for training diffusion such as Stable Diffusion is scraped from the Web and has been shown to contain harmful and pornographic content~\cite{birhane2021multimodal, luccioni2021s} as well as mislabeled and corrupted examples~\cite{siddiqui2022metadata}. These datasets then undergo filters to isolate subsets that are considered, for example, `aesthetic' or `safe for work'~\footnote{The criteria used to establish which images are esthetically-pleasing and safe for work are also unclear and merit further investigation.}, which are created based on the output of classification models trained or fine-tuned on other datasets~\cite{schuhmann2022laion-aesthetics}, which can itself result in further unintended consequences. For instance, the creators of \DallE~observed that attempting to filter out ``explicit'' content from their training data actually contributed to bias amplification,
necessitating additional post-hoc bias mitigation measures~\cite{nichol2022dallemitigations}. Further biases can be introduced during the training process, given that many TTI systems use the CLIP model~\cite{radford2021learning} to guide the training and generative process despite its biases (see above), the impact of which on image generation is still unclear. Finally, the biases of safety filters and prompt enhancement techniques used in TTI systems are poorly understood and largely undocumented~\cite{sign-that-spells-offer-22}. For instance, keyword-based approaches have been shown to have a disparate impact on already marginalized groups~\cite{dodge2021documenting}, so using them for safety filtering at prompt level can have similar consequences.
%despite recent scholarship calling for stronger bias analysis grounded in the models' application contexts to start addressing real-world harms~\cite{Blodgett2020LanguageI,hundt2022robots,srinivasan2021biases},



%\subsection{Gender and Ethnicity as Variables}
%\label{sec:background:gender-ethnicity}

%Any project analyzing biases relevant to personal identity characteristics, such as gender or ethnicity, requires grappling with the nuances of using personal identity labels. Both gender and ethnicity are widely described as socially-constructed variables~\cite{Jenkins1994RethinkingEI} that are not founded in biology but constructed over time and based on social interactions and dynamics~\cite{lorber1991social, smedley2005race,Penner2013EngenderingRP}. Studies focusing on key sources of demographic data such as the U.S. census have found that individuals' self-identification varies over time ~\cite{Liebler2017AmericasCR} and that the boundaries between categories are often multi-dimensional and fluid ~\cite{Bowker1999SortingTO}. Both race and gender are also inherently non-discrete but rather multidimensional, spanning a spectrum along which people choose to associate themselves~\cite{Carothers2013MenAW}. In fact, research has insisted on the inherent intersectionality of gender and ethnicity, arguing that treating these categories independently obscures important phenomena \cite{Crenshaw1991MappingTM,Bowker1999SortingTO,Thomas2011GenderedRI}. 

%Despite these decades of foundational work, many technical and socio-technical tools continue to operationalize both ethnicity and gender in fixed, categorical ways, often assuming that both variables can be determined externally by a person's appearance. Calls for more inclusive and nuanced approaches have stemmed from fields such as HCI~\cite{keyes2018misgendering,Gyamerah2021ExperiencesAF, OgbonnayaOgburu2020CriticalRT} and are starting to be adopted by these communities. However, archaic and damaging stereotypes of binary gender and a fixed set of ethnicity categories continue to persist in fields such as ML, which is particularly problematic given that, for instance, defining a set of categories in a (labeled) dataset will be transferred to models trained on that dataset, which can further contribute to perpetuating algorithmic harms and unfairness~\cite{benthall2019racial, barocas2017fairness,keyes2018misgendering,buolamwini2018gender}. Self-identification has been motivated as a more acceptable approach to annotation, given that often neither race nor gender are determined by apparent characteristics alone, and is increasingly becoming the standard for datasets involving humans (e.g. ~\cite{hazirbas2021towards}). Nonetheless, careful thought must be put into the way in which social attributes (such as gender and race) are used as variables in ML models~\cite{buolamwini2018gender, wang2022towards}. Additionally, self-identification data is often unavailable in those settings where ML is used, for example when the people represented in a dataset were not directly involved in its creation process or when a system deals with representations of people that are either fully synthetic or aggregated from real individuals, as is the case for TTI systems in general and diffusion models in particular.
