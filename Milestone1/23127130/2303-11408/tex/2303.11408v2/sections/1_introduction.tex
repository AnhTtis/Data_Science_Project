
\section{Introduction}
\label{sec:introduction}

\begin{figure}[t!]
\center
% \includegraphics[width=0.8\linewidth]{images/methodology_v3.png}
% \includegraphics[width=0.8\linewidth]{images/SD_bias_methodology_v1-1.jpg}
\includegraphics[width=\linewidth]{images/methodology.png}
\caption{Our approach to evaluating bias in TTI systems}
\label{fig:methodology}
\end{figure}

Diffusion-based approaches are one of the most recent Machine Learning~(ML) techniques in prompted image generation, with models such as Stable Diffusion~\cite{rombach2022high}, Make-a-Scene~\cite{gafni2022make}, Imagen~\cite{saharia2022photorealistic} and \DallE~\cite{ramesh2022hierarchical} gaining considerable popularity in a matter of months. 
These generative approaches are inspired by the principles of non-equilibrium thermodynamics~\cite{weng2021diffusion} and trained to reverse the gradual addition of noise that is layered onto an image.
%Similar to other generative models, diffusion models learn to convert noise samples---typically Gaussian---to an image. To achieve this, an image from the training dataset is gradually combined with noise at discrete time steps, until eventually, the noise overwhelms the image itself.
%The model is then tasked to learn how to reverse the process and achieves this by learning how to reverse one step of noise diffusion, and does so at every training time step.
One key difference that has led to the widespread adoption of diffusion models is that they are simpler to train than previous generations of generative models owing to their likelihood-based loss function, whose mode-covering characteristic~\cite{diffusion-efficiency} is also one of the main reasons why model outputs are so diverse compared to other classes of generative models. However, this iterative approach also makes it particularly difficult to directly access the latent space of the model, making it difficult to directly access and analyze the latent space of diffusion models.

Both the training and inference functions of diffusion models also rely on additional phases of text and image processing (e.g. safety filters, embeddings, etc.), which is why we refer to them as \emph{Text-to-Image (TTI) Systems} as opposed to models: they are an assemblage of components and modules that all contribute to generating the final image, and it is hard to disentangle which module is responsible for downstream behavior, and what kind of biases are baked in due to models such as CLIP~\cite{open-clip} being used to guide training.
%The creation of large, diverse image-text pair datasets such as LAION~\cite{schuhmann2021laion,schuhmann2022laion} has significantly contributed to the widespread adoption of diffusion models.
%Both the text and images in the datasets are leveraged during diffusion model training: the text is used to guide the diffusion process described above to provide information about what the final outcome should resemble.
%This is done by augmenting the denoising steps with the token embeddings of the corresponding text as generated by a pre-trained model such as CLIP~\cite{radford2021learning,open-clip}, whose image and text encoders have previously been jointly trained to create embeddings that are similar in latent space.
%This can contribute to improving the quality of the generated images and make training more stable.
%It also allows the model to be used for text-guided image generation from random noise at inference time.
%
Many of these systems are also increasingly finding their way into applications ranging from generating stock imagery~\cite{shutterstockdalle2022} to aiding graphic design~\cite{adobedalle} and used to generate realistic and diverse images based on user prompts.
And as with any ML artifact---or, indeed, technology in general---TTI systems exhibit biases, and widely deploying them in different sectors makes them particularly prone to amplifying and perpetuating existing societal inequities. However, these biases remain sparsely documented and hard to audit, often only described in broad terms in model cards~\cite{sdv14modelcard, mishkin2022dall} and in papers describing new models~\cite{saharia2022photorealistic}.
%The anecdotal evidence of racist, homophobic and misogynistic images shared both on social media\footnote{For instance, see the following Twitter threads: \href{https://twitter.com/Demhamasta/status/1615520883010830338}{[1]}, \href{https://twitter.com/SashaMTL/status/1582729717232173057}{[2]}, \href{https://twitter.com/WriteArthur/status/1512429306349248512}{[3]}.} and in mainstream journalism~\cite{algoportrait,lensa} in recent months outlines the need to go much further in both the scope and the specificity of the bias documentation accompanying deployed models.



In the present article, we introduce a set of tools and approaches to support the auditing of the social biases embedded in TTI systems,
by comparing model outputs for a targeted set of prompts
%based on varying model inputs and comparing model outputs
(see Fig~\ref{fig:methodology}).
We illustrate our approach by comparing images generated by Stable Diffusion v.1.4, v.2, and \DallE
%Working with generated images allows us to work both with models that are open-access as well as those that only provide API access, and focusing our analysis on feature correlations between sets of generated images allows us to study trends correlated with social variation without assigning pre-defined social categories to synthetic images that lack a social context. 
in terms of the social biases they encode through the lens of different professions and explicit mentions of words related to gender and ethnicity.
We also share tools that enable users to explore other TTI systems and topics -- contributing towards lowering the barrier to entry for exploring these systems -- as well as the datasets of profession generations for all of the TTI systems analyzed.
We conclude with a discussion of the implications of our work and propose promising future research directions for continuing work in this space.
