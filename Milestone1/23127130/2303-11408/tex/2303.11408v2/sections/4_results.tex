\section{Results}
\label{sec:results}

The goal of our analysis is to develop ways of analyzing and comparing the biases of text-to-image systems that would allow users of these systems to shed light on these otherwise impenetrable systems.
%While the results that we present below are specific to the set of prompts that we describe in Section~\ref{methodology:prompting}, our intention is to make our approach as generic as possible.
Section~\ref{sec:results:gender-biases-discrete} compares the {\sc profession} target attribute in the \emph{``Professions'' dataset} to the {\sc gender} identity characteristic through the use of discrete textual representations of the images obtained with a captioning and a VQA system;
%We contrast the proportion of textual markers of gender for men and women to the gender statistics from the U.S. Labor Bureau to quantify the models' tendency to exacerbate biases. 
%additional 
Section~\ref{sec:results:cluster-biases} then explores the system outputs' joint variation across dimensions of {\sc gender} and {\sc ethnicity} following the methodology introduced in Section~\ref{sec:methodology:bias-analysis}, and provides a detailed analysis of our 3 systems of focus as well as an overview of results for a larger set of open TTI systems. Finally, Section~\ref{sec:results:tools} describes new interactive tools that we developed based on our approach and showcases how the tools can support a qualitative analysis and storytelling around TTI system biases.

\subsection{Gender Bias Analysis through Text Markers}
\label{sec:results:gender-biases-discrete}

As described in Section~\ref{sec:methodology}, we used captions and open-ended VQA to obtain textual representations of the generated images, whose discrete nature makes identifying trends related to social attribute variation more tractable. % We examine these through the lens of ethnicity and gender below.
More specifically, we base our evaluation on the likelihood that a caption or VQA answer for a given profession contains gender-marked words such as `man' and `woman' or gender-unspecified descriptors such as `person' or the profession name --- we present these results in Table~\ref{table:vqa-captions}.  %We used a bootstrap estimator to obtain a 99\% confidence interval for the entropy comparisons in the Appendix (Table~\ref{\label{tab:diversity-main}}), we'll add similar confidence intervals for the results in the main text.
% with the percentages of men and women in these professions provided by the Labor Bureau~\footnote{The US Labor Bureau only provides statistics for two gender categories on their \href{https://www.bls.gov/cps/earnings.htm\#demographics}{website}.}
% , mentioned in both the captions and the words returned by the VQA model when prompted with \textit{"What word best describes this person's appearance?"}: we present these results in Table~\ref{table:vqa-captions}.
%, also comparing the percentage of captions that contain gender-specific terms with the most recent statistics provided by the US Labor bureau regarding the proportion of employed persons per profession~\cite{laborbureau2022}. 

\begin{table}[h!]
%\hspace{-1cm}
\small
\begin{tabular}{l|c|c|c|c|c|c|c|c}
& \multicolumn{3}{c|}{captions} & \multicolumn{3}{c}{VQA} & \multicolumn{2}{|c}{Labor Bureau} \\ \hline
&   \% woman & \% man & \begin{tabular}[c]{@{}l@{}} \% gender\\markers\end{tabular} &  \% woman & \% man & \begin{tabular}[c]{@{}l@{}} \% gender\\markers\end{tabular} &  \% woman & \% man \\ \hline
SD v.1.4 & 38.04\% & 61.96\% & 97.24 \% & 37.77 \% & 62.23\%~ & 47.92\% & &\\ \cline{1-7}
SD v.2  & 33.45\% & 66.55 \% & 96.66\% & 31.10 \% & 68.90\% & 44.50\% & 47.03\% & 52.97\%\\ \cline{1-7}
\DallE & 19.96\% & 80.04\% & 99.09\% & 21.95 \% & 78.05\% & 44.25\% & & \\ \cline{1-7}
Average & 30.48\% & 69.52\% & 97.66\% & 30.06\% & 69.67\% &  45.56\% & &  \\ \hline
\end{tabular}
\caption{The average percentage of mentions of `woman', `man', `person' in the captions generated by a Vision Transformer Model, the BLIP VQA model and the difference between these percentages and those provided by the U.S. Bureau of Labor Statistics. N.B. these percentages are based on the number of captions/VQA appearance words containing gender markers, not the total number of data points.}
\label{table:vqa-captions}
%\vspace{-0.75cm}
\end{table}

In total, 97.66\% of the captions generated contained gender-marked terms, versus 45.56\% of VQA appearance predictions: this is consistent given the fact that VQA mostly consists of single word predictions, whereas captions are full sentences. To put the percentage of predictions that contain gender-marked terms such as 'man' and 'woman' into perspective, we compare them with the percentages of men and women in these professions provided by the BLS~\footnote{The BLS only provides statistics for two gender categories on their \href{https://www.bls.gov/cps/earnings.htm\#demographics}{website}.}. We find that \DallE~has the largest discrepancy compared to the BLS-provided numbers, with its captions mentioning women on average 27\% less, and its VQA mentioning them 25\% less, and Stable Diffusion v.1.4 having the least (approximately 9\% for both captions and VQA). The professions with the biggest discrepancy between the BLS and both captions and VQA across all systems are: \emph{clerk} (57 and 55\% less), \emph{data entry keyer} (55/53\% less) and \emph{real estate broker} (52/54\% less), whereas those that have more captions that mention women are: \emph{singer} (29/36\% more), \emph{cleaner} (20/16\% more) and \emph{dispatcher} (19/16\% more). 

Very few of the generated image captions mention gender-neutral terms such as `person' (an average of less than 1\% of captions for any of the systems, distributed equally across professions), and none use the `non-binary' gender marker.
%; the professions with the highest proportion of unspecified gender in their captions were manicurist (3.9\%), carpet installer (3.8\%) and cleaner (3.3\%) -- closer inspection of the images generated for these professions using the Bias Explorer tool described in Section~\ref{sec:results:tools} showed that many of the images for these professions had human hands or bodies without visible faces, which could have contribute to the captioning model using gender-neutral terms to caption those images. 
Also, less than 0.5\% of captions and 2\% of VQA generations explicitly mention the profession in the prompt, but it is interesting to note that a single profession, \emph{police officer}, had explicit mentions of the profession name in 80.95\% of captions. In the case of VQA, several others also had a significant number of explicit mentions, including \emph{doctor} (70.48\% of VQA), \emph{firefighter} (45.71\%) and \emph{pilot} (29.84\% of VQA). We believe this to be due to the \emph{markedness}~\footnote{Markedness is a linguistic concept that refers to the distinctiveness of a word or concept compared to others â€“ in the case of professions, the fact that firefighters and pilots have distinctive uniforms that allow them to be easily distinguished from other professions}~\cite{andrews1990markedness} of these professions and of the high proportion of gendered references to individuals (as opposed to using gender-neutral terms such as `person`) in the data used for training both TTI systems. %Furthermore, the images generated for each profession+system combination can explored using the \href{https://huggingface.co/spaces/tti-bias/diffusion-bias-explorer}{Bias Explorer} and \href{https://huggingface.co/spaces/tti-bias/diffusion-faces}{'Average Face'} interactive demos.

\subsection{Gender and Ethnicity Distribution in the Image Space}
\label{sec:results:cluster-biases}

Section~\ref{sec:results:gender-biases-discrete} leveraged image-to-text systems to help surface a first category of biases in the output of the three TTI systems of interest.
In order to further quantify the systems' diversity in terms of both {\sc gender} and {\sc ethnicity} without having to solve a poorly-specified and poorly-motivated identity label assignment problem, we now turn to our proposed cluster-based method to identify relevant trends in the visual features produced by these systems.

% \subsubsection{Interpreting Identity Clusters and Assignments}
\subsubsection{Characterizing Identity Regions in the Image Space}

We apply the method described in Section~\ref{sec:methodology:bias-analysis} to identify significant regions in the TTI systems' output space that help summarize the variation of visual features corresponding to different combinations of gender and ethnicity phrases in the generation prompts. Specifically, we cluster images corresponding to 68 combinations of 4 phrases for gender and 17 for ethnicity into 24 regions. Thus, by quantifying the distribution of a system's generations over these regions, we can identify trends in their visual features that are correlated with the identity characteristics used to identify the regions.

Table~\ref{tab:results:cluster-summaries} showcases this by providing summaries of 10 of the most represented regions.
%in the \emph{``Professions'' dataset} across al three models of interest.
While the regions themselves do not correspond to specific genders or ethnicities, we can get of sense of the visual features they represent by looking at the top gender and ethnicity phrases that were featured in prompts assigned to that region in the \emph{``Identities'' dataset}. For example, we see that region 4, which accounts for over 40\% of the \emph{``Professions''} images, tends to contain images that were generated for prompts describing White men. We can further see that regions which mostly features prompts with the word \textbf{woman} make up a significantly smaller part of the \emph{``Professions''} dataset overall (regions 15, 13, 1, and 10 together add up to 25.5\%).

\begin{figure}[t!]
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=\linewidth]{images/neurips/lab_tech_bar_mid_small.png}
\caption{Cluster assignments of "Laboratory technician" images.
\vspace{0.25cm}
}
\label{fig:results:lab-tech-bars}
\end{minipage}
\hfill
\begin{minipage}[b]{0.67\textwidth}
\centering
\includegraphics[width=\linewidth]{images/neurips/lab_tech_examplars.png}
\caption{Examples of Profession images for ``laboratory technician'' assigned to clusters 4, 15, 1, and 3. There are no \DallE images assigned to 1.
\vspace{0.25cm}
}
\label{fig:results:lab-tech-examplars}
\end{minipage}
\begin{minipage}[b]{\textwidth}
\centering
\footnotesize
\begin{tabular}{ll|ll|llll}
Region \# & Share & \multicolumn{2}{c|}{Top gender phrases} & \multicolumn{4}{c}{Top ethnicity phrases} \\
\midrule
  4 & 40.1\% & unspecified & man           & Caucasian & White & unspecified & Latinx \\
 \textbf{15} & 12.6\% & \textbf{woman} & non-binary & White & Caucasian & unspecified & First Nation \\
 21 & 9.2 \% & man & unspecified            & unspecified & White & & \\
 18 & 8.8 \% & man & unspecified            & unspecified & White & Caucasian & \\
 \textbf{13} & 7.5 \% & \textbf{woman} & unspecified & Latinx & Hispanic & Latino & unspecified  \\
 \uline{22} & 3.1 \% & unspecified & man            & SE Asian & \uline{Black} & Indigenous & Multiracial  \\
%  0 & 2.7 \% & man & unspecified            & South Asian & Hispanic & Latino & Latinx \\
  \uline{\textbf{1}} & 2.7 \% & \textbf{woman} & non-binary  & \uline{Black} & Afr.American & Multiracial & unspecified  \\
 \textbf{10} & 2.7 \% & non-binary & \textbf{woman}  & Latinx & White & Indigenous & unspecified  \\
%  \textbf{7} & 2.0 \% & \textbf{woman} & unspecified & Multiracial & Hispanic & Latinx & Latino  \\
  \uline{3} & 1.9 \% & man & unspecified            & \uline{Black} & Afr.American & Multiracial & Pacific Isl.  \\
 17 & 1.3 \% & non-binary &                 & White & unspecified & Caucasian & Hispanic 
\end{tabular}
\vspace{0.1cm}
\captionof{table}{
Identity clusters can be represented by the top gender and ethnicity phrases in their generation prompts. We show 10 of the main clusters above and outline those that feature the \textbf{woman} and \uline{Black} gender and ethnicity phrases.}
\label{tab:results:cluster-summaries}
\end{minipage}
\end{figure}

Figures~\ref{fig:results:lab-tech-bars} and ~\ref{fig:results:lab-tech-examplars} help us better understand how this cluster assignment functions in practice by focusing on the 630 images (210 for each system) generated for the \emph{``Photo portrait of a laboratory technician.''} prompt. Figure~\ref{fig:results:lab-tech-bars} shows the distribution over the regions separately for each of the systems, outlining a significant difference between the \DallE and Stable Diffusion outputs: with region 4 twice as prominent for the former. In order to verify our intuition about what cluster assignments mean, we visualize generations for the prompt assigned to selected regions in Figure~\ref{fig:results:lab-tech-examplars}, and find that the representations are coherent with the region summaries from Table~\ref{tab:results:cluster-summaries} -- \DallE is missing images for Cluster 1 because none of its generations are assigned to that cluster.

\subsubsection{Gender and Ethnicity Representation across Systems}

% TODO: justify the choice of Black? 2nd most self-identified in the US

The previous paragraph shows how we can use the identified regions to better understand the diversity of generations by a given system for any profession of interest. Given a prompt, cluster assignments tell us whether the model tends to generate images for this prompt that are more similar to the ones generated for a given {\sc gender} or {\sc ethnicity}. While this level of detail enables fine-grained analysis of a model's specific biases, more general bias trends across multiple models can be harder to apprehend at a glance.  
%While the profession-level region assignments provide a detailed view of a given systems' behavior in a particular setting, their global trends can be hard to apprehend at a glance. In order to address this limitation, we propose to aggregate the region assignments over groups of professions and regions.

In the following paragraphs, we propose an aggregation scheme across professions and clusters to better support such comparisons. We group professions based on the US Bureau of Labor Statistics. Rather than looking at 146 distributions for each of the professions in the list, we rank jobs based on the gender and ethnicity distributions reported by workers in the US and group them into 5 bins of 29 to 30 (quintiles). We rank and group professions by the proportion of workers who self-report as women when looking at regions of the image space that are correlated with {\sc gender}, and by the proportion of workers who self-report as Black when examining {\sc ethnicity}.
% We group professions based on the BLS statistics: we rank jobs by the reported proportions of women and Black workers and by their median incomes (weekly earnings), and group them into five quintiles (groups of 29 or 30 professions) based on these three statistics.

For regions, we create groups based on whether the corresponding gender and ethnicity phrases are prominently features in their \emph{``Identities''} prompts (see Table~\ref{tab:results:cluster-summaries}), with \textbf{woman} and \uline{Black} in the top-2 and top-4 gender and ethnicity phrases respectively. We then compare the proportion of images assigned to the grouped region per corresponding quintile to the average BLS value for this quintile. This allows us to assess not just whether a group is under- or over-represented by the models, but also whether a model attenuates, reproduces or exacerbates social biases. % For the income grouping, we instead look at overall the diversity of the images generated, as measured by the entropy of the distribution over the 24 regions.

Table~\ref{tab:results:quintiles-original-models} provides the results for these three analyses for both Stable Diffusion versions and for \DallE. The quintile analysis makes the systems easier to compare while still retaining an important level of specificity: for example, if a system under-represents women in its outputs, this lets us know whether it is reproducing or exacerbating societal biases. Taking the example of Stable Diffusion v1.4, we can see that while the system seems to match the US distribution for the least diverse professions (whether the workforce is mostly identified as men or women), regions that feature \textbf{woman} are under-represented across the more balanced professions. We can also identify broad trends across systems: while all under-represented regions of the Space associated with the \textbf{woman} and \uline{Black} phrases, the phenomenon is least pronounced for Stable Diffusion v1.4, and most pronounced for \DallE.
% The analysis also shows a negative correlation between income and diversity across systems, again with \DallE as the least diverse system.

\begin{table}[t!]
\centering
\small
\begin{tabular}{l||l|lll||l|lll}%||l|lll}
\begin{tabular}[c]{@{}l@{}}Quintiles\\ by rank\end{tabular} & \begin{tabular}[c]{@{}l@{}}{\tiny BLS} \\ {\tiny Woman}\end{tabular} & \multicolumn{3}{c||}{\begin{tabular}[c]{@{}l@{}}Clusters featuring\\ "woman" phrase \%\end{tabular}} & \begin{tabular}[c]{@{}l@{}} {\tiny BLS} \\ {\tiny Black}\end{tabular} & \multicolumn{3}{c}{\begin{tabular}[c]{@{}l@{}}Clusters featuring\\ "Black" phrase \%\end{tabular}} \\ % & \begin{tabular}[c]{@{}l@{}}{\tiny BLS}\\ {\tiny income}\end{tabular} & \multicolumn{3}{c}{Entropy} \\
\midrule
 &  & {\tiny SD 1.4}   & {\tiny SD 2}  & {\tiny \DallE}    &   & {\tiny SD 1.4}  & {\tiny SD 2}  & {\tiny \DallE}  \\ % &  & {\tiny SD 1.4}   & {\tiny SD 2}   & {\tiny \DallE}   \\
\bottomrule
Low 20\%    & 7.5   & 8.5     {\tiny $(\pm$0.36)}  & 5.3 {\tiny $(\pm$0.28)} & 1.4 {\tiny $(\pm$0.17)} & 4.7 & 6.6  {\tiny $(\pm$0.60)}  & 4.8  {\tiny $(\pm$0.56)}  & 3.9 {\tiny $(\pm$0.48)}  \\ % & 686     & 2.6      & 2.3  & 1.9    \\
20 to 40  & 26.5   & 15.2     {\tiny $(\pm$0.46)}  & 14.9 {\tiny $(\pm$0.46)}  & 2.8 {\tiny $(\pm$0.20)}  & 7.1   & 8.2  {\tiny $(\pm$0.55)}  & 4.9 {\tiny $(\pm$0.54)} & 4.3 {\tiny $(\pm$0.32)} \\ %  & 859   & 2.4      & 2.0    & 1.6     \\
40 to 60   & 47.1     & 32.2  {\tiny $(\pm$0.57)}   & 23.1 {\tiny $(\pm$0.53)}    & 6.0    {\tiny $(\pm$0.28)}   & 10.5    & 7.3 {\tiny $(\pm$0.61)}   & 8.1 {\tiny $(\pm$0.63)}  & 3.4{\tiny $(\pm$0.53)} \\ % & 1093  & 2.4      & 2.1    & 1.6     \\
60 to 80     & 68.4    & 54.3 {\tiny $(\pm$0.60)} & 46.3 {\tiny $(\pm$0.63)} & 21.6 {\tiny $(\pm$0.57)}    & 14.4     & 11.2 {\tiny $(\pm$0.64)} & 11.4  {\tiny $(\pm$0.64)}  & 5.3 {\tiny $(\pm$0.54)} \\ %   & 1365    & 2.3      & 2.1    & 1.7     \\
Top 20\%    & 86.8    & 83.0  {\tiny $(\pm$0.45)}  & 78.3 {\tiny $(\pm$0.49)}   & 56.3 {\tiny $(\pm$0.62)}  & 22.1 & 16.8 {\tiny $(\pm$0.66)}  & 16.1 {\tiny $(\pm$0.57)} & 4.5 {\tiny $(\pm$0.46)} \\ %  & 1891          & 2.0      & 1.6    & 1.3    
\end{tabular}
\caption{
Comparing cluster assignments to Bureau of Labor Statistics (BLS).
Each line corresponds to one fifth of the professions grouped by BLS-reported percentage of women (left), and Black workers (middle). 95\% confidence intervals per a bootstrap estimator are provided.
%weekly earnings (right).
}
\label{tab:results:quintiles-original-models}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.49\linewidth]{images/comparisons/genders_new.pdf}
    \includegraphics[width=0.49\linewidth]{images/comparisons/ethnicities_new.pdf}
    % \includegraphics[width=0.24\linewidth]{images/comparisons/gender_old.pdf}
    % \includegraphics[width=0.24\linewidth]{images/comparisons/ethnicity_old.pdf}
    \caption{Comparing 11 TTI systems' representations of the regions associated with the \textbf{woman} and \uline{Black} phrases. The format of results is the same as in Table~\ref{tab:results:quintiles-original-models} above, presented as line plots here to more easily contrast multiple models.  On the "rank" x-axis, each tick corresponds to a quintile of professions grouped by the BLS-reported proportions of women (left), and Black workers (right) in those professions. The "percentage" y-axis corresponds to the percentage of image generations for those professions assigned to the clusters selected as described in Table 2 and in the text. }
    \label{fig:results:quintiles-new-models}
\end{figure}

\paragraph{Evaluating additional systems} While we initially focused our evaluation on three TTI systems with the highest visibility at the start of this work, our method is easily applicable to any TTI system that can generate an image given a prompt -- we release the necessary code for this alongside our  paper. We further benchmarked an additional 11 systems selected from the \href{https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads}{most downloaded text-to-image models on the Hugging Face Hub} at the time of writing. We summarize results for the same gender and ethnicity quintiles as above in Figure~\ref{fig:results:quintiles-new-models}. We see that even though these systems all share Stable Diffusion models as their pre-trained initialization, the diversity of their outputs across both dimensions does seem to depend on the specific fine-tuning and adaptation process, sometimes independently from each other. These results should provide a starting point for further investigation into these systems' specific datasets and help guide specific use cases.

\subsection{Interactive Tools for Interactive Exploration}
\label{sec:results:tools}

As part of our own analysis of images generated by different TTI systems, we have created tools to help us delve into the images in more detail and identify relevant patterns to guide our analyses. We present the three of the tools that we have created, and the observations that they have allowed us to make, below:

\begin{figure}[h!]
\centering
\hfill
\begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{images/diffusionexplorer.png}
    \caption{Diffusion Bias Explorer}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{images/average-faces.png}
    \caption{Average Face Comparison Tool}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
    \includegraphics[width=\linewidth]{images/knn-space.png}
    \caption{k-NN Explorer}
\end{subfigure}
\hfill
\caption{The 3 interactive tools created as part of our analysis: \href{https://hf.co/spaces/tti-bias/diffusion-bias-explorer}{Diffusion Bias Explorer}, \href{https://hf.co/spaces/tti-bias/diffusion-faces}{Average Faces Comparison}, and the \href{https://hf.co/spaces/tti-bias/identities-knn}{k-NN Explorer}.}
\label{fig:tools}
\vspace{-0.5cm}
\end{figure}
\paragraph{Diffusion Bias Explorer} One of the first tools that we created in the scope of this project was the \href{https://hf.co/spaces/tti-bias/diffusion-bias-explorer}{Diffusion Bias Explorer} (See Fig.~\ref{fig:tools}~(a)), which enabled users to compare what the same set of prompts -- based on the list of professions described in Section~\ref{sec:methodology} -- resulted in when fed through the 3 TTI systems. It allowed us to uncover initial patterns -- including the homogeneity of certain professions (such as CEO) or the differences between versions of Stable Diffusion as well as \DallE. In fact, we used the Diffusion Bias Explorer all throughout the project in order to visually verify tendencies discovered using our analyses, since it allowed us to quickly view images for any given profession+TTI system combination.
 
\paragraph{Average Face Comparison Tool} Another tool that we created was the \href{https://hf.co/spaces/tti-bias/diffusion-faces}{Average Face Comparison Tool} (see Fig.~\ref{fig:tools}~(b)), which leverages the \texttt{\href{https://github.com/johnwmillr/Facer}{Facer}} Python package to carry out face detection and alignment based on facial features and averaging across professions. This helped us further see more high-level patterns in terms of the visual aspects of the images generated by the different TTI systems while avoiding facial recognition and classification techniques that would prescribe gender or ethnicity. Also, the blurriness of the average images gave us signal about how homogeneous and heterogeneous certain professions were. 

\paragraph{Nearest Neighbors Explorers: BoVW and Colorfulness} To enable a more structured exploration of the generated images we also developed two nearest-neighbor lookup tools. Users can choose a specific image as a starting point---for example, a photo of a Black woman generated by a specific TTI system---and explore that photo's neighborhood either by \href{https://hf.co/spaces/tti-bias/identities-colorfulness-knn}{colorfulness} (as defined by \cite{colorfulness}), or by a \href{https://hf.co/spaces/tti-bias/identities-bovw-knn}{bag-of-visual-words~\cite{bag-of-visual-words} TF-IDF index}. To build this index, we used a bag-of-visual-words model to obtain image embeddings that do not depend on an external pre-training dataset. We then extracted each image's SIFT features~\cite{sift} and used k-means~\cite{lloyd1982least} to quantize them into a codebook representing a visual vocabulary and computed TF-IDF sparse representations and indexed the images using a graph-based approach~\cite{nndescent}. These search tools enable a structured traversal of the dataset and thereby facilitate a qualitative exploration of the images it contains, either in terms of color or structural similarity. This is especially useful in detecting stereotypical content, such as the abundance of stereotypical Native American headdresses or professions that have a predominance of given colors (like firefighters in red suits and nurses in blue scrubs).% but also specific failure modes, such as the misinterpretation of the `stocker' profession as a dog-breed. % One interesting insight is that images generated by \DallE are on average the most colorful. Images of men are on average less colorful than all other gender labels, consistently across all three models.

We provide code and detailed instructions about how to duplicate and edit these tools to work with other sets of prompts and images, to facilitate their dissemination and customization in the community.