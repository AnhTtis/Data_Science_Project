\section{Image Feature Extraction Systems}
\label{sec:appendix:feature-extraction}

One category of image representations that we can more easily leverage to find bias patterns is that of text-based representations; specifically, textual representations of the figures depicted in an image. We use two ML-based systems to automatically obtain such descriptions --- one designed for image captioning and one designed for Visual Question Answering (VQA). For image captioning, we leverage the \href{https://huggingface.co/nlpconnect/vit-gpt2-image-captioning}{ViT GPT-2 Model}~\cite{nlp_connect_2022} trained on the MS-COCO dataset~\cite{lin2014microsoft} to generate captions for each of the images.
This model has a total vocabulary size of 50,257 tokens and produces multiple-word captions for each image that we then represent, allowing us to extract much richer representations for each image  (vectorizing the captions with scikit-learn~\cite{scikit-learn} allowed us to work with 982-dimensional vectors), and to compare images from different models in a common textual space which also has the advantage of being interpretable to humans.

\paragraph{VQA answers} We also utilized the \href{https://huggingface.co/Salesforce/blip-image-captioning-base}{BLIP base model}~\cite{BLIP2022} pretrained on MS-COCO~\cite{lin2014microsoft} and fine-tuned on VQA 2.0~\cite{goyal2017making}. We queried the model using a visual question answering (VQA) set up, asking the model a series questions pertaining to the appearance, gender and profession of the images generated by the TTI systems (see the Appendix~\ref{appendix:vqa-words} for the full list of VQA questions). The BLIP model has a total vocabulary size of 30,524 and produces single-word answers which we constrained the model the 'gender' and 'appearance' outputs to the gender and ethnicity categories that we used in our prompts to make the analysis more tractable (see Appendix~\ref{appendix-prompts} for the full list of words used in our prompts). 

\paragraph{Image Embedding Systems}

\paragraph{Pixel Features: Colorfulness and Bag of Visual Words}
%Seeing as humans consume images in pixel space, we find it fitting to spend some time analyzing the generated images 
The first approach we leverage involves computing each image's colorfulness through the proxy metric introduced by~\citet{colorfulness} and  extract each image's SIFT features~\citep{sift}. We use k-means~\cite{lloyd1982least} to quantize these features to a codebook representing a visual vocabulary
and use the resulting vocabulary to compute a TF-IDF sparse vector representation for each image using the bag-of-visual-words model~\citep{bag-of-visual-words} which we index using a graph-based approach~\citep{nndescent}. The goal of this is to enable a more structured and in-depth look at the raw pixel space of the images generated by the 3 TTI systems to try to identify patterns and structure without leveraging pre-trained models.


\paragraph{Image-Text alignment embeddings with CLIP} \textbf{TODO: Yacine- check this} We also used the \href{https://huggingface.co/openai/clip-vit-base-patch32}{CLIP base model} to generate a series of image and text embeddings [something else here??]. Given the utilization of CLIP models to guide the training process for Stable Diffusion, CLIP models can be an approximation of the models' latent space, as well as the similarity between text and images.

\paragraph{VQA embeddings with BLIP} \textbf{TODO: Yacine} Talk about why VQA embeddings are more general than classification + can be guided to represent specific aspects



\subsubsection{Examining Ethnicity and Gender in VQA Constrained Predictions}
As mentioned in Section~\ref{sec:methodology}, an intention in this work is to avoid using classification approaches that reduce the number ethnicities to a handful of classes present in datasets. By constraining generation of the BLIP VQA model, we were able to control the output space while leveraging model capabilities. Our initial prompts to the TTI systems contained 16 different ethnicities that were equally represented, which we used to constrain the VQA generation space (for a list of constrained generation words, see Appendix~\ref{appendix:vqa-words}). Despite this wide variety the VQA model predictions were limited to 6 categories: Asian (30\%), Black (25\%), Hispanic (15\%), Caucasian (11\%), Native American (10\%) and White (9\%). This vocabulary is similar to the U.S.~Census categories, with \textit{Native American} notably reflecting a U.S.-centric bias. It is interesting to note that while some ethnicities (such as African-American and Caucasian) were well recognized by the VQA model, others (such as Hispanic and Native American) were much less so, and common patterns of confusion emerged, such as predicting `White' or `Asian' for either group. Finally, images generated based on prompts that were not marked with an ethnicity were mainly perceived as White by the VQA model, with over 80\% of these images across all models (\DallE, Stable Diffusion v.1 and v.2) 
predicted as White or Caucasian.  These findings outline the limitations of automatic ethnicity prediction based on visual characteristics, and reflect the limited vocabulary of common fine-tuning datasets. We found that for ethnicity in particular, using the interactive tools that we created (which we present in Section~\ref{sec:results:tools}) gave us more insights into, for instance, the average representations across professions and ethnicities, which were harder to quantify using algorithmic tools given the non-visual components of identity characteristics.
%
Also, despite expanding the gender variable to include a third `non-binary' category, the VQA model outputs remained restricted to marked, binary gender categories for all 2010 identity generations: it predicted 51\% 'man' and 49\% 'woman' for input images equally balanced in terms of the 3 gender categories. For images based on prompts that used the unmarked term `person', 74\% of the VQA answers predicted `man' and 26\% predicted `woman'; this pattern was reversed for prompts with non-binary gender (76\% `woman'; 24\% `man').