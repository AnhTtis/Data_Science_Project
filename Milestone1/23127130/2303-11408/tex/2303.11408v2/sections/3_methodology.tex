\section{Methodology: Auditing Social Biases in TTI Systems}
\label{sec:methodology}
In this work, motivated by the necessity to better understand and audit social dynamics in multimodal ML, we propose a new approach for quantifying the biases of TTI systems.
Our approach stems from the following intuition: images generated by TTI systems may lack inherent social attributes, but they do showcase features in their depiction of human figures that viewers interpret as social markers.
We cannot define these markers \textit{a priori} due to the social nature of identity characteristics such as race or gender; they are multidimensional notions spanning a spectrum along which people choose to associate themselves~\cite{Carothers2013MenAW} rather than discrete external quantities, and cannot be determined by a person's appearance.
Indeed, while previous work in ML has adopted binary gender and fixed prior ethnicity categorizations for the sake of convenience, the downstream impact of these choices that are propagated from (labeled) datasets to trained models can contribute to perpetuating algorithmic harms and unfairness~\cite{benthall2019racial, barocas2017fairness,keyes2018misgendering,buolamwini2018gender}.
We therefore endeavor to use more flexible proxy representations of the visual features in TTI systems' generated images to identify regions of the representation space that viewers may associate with social variation,
% characterizing their dynamics while controlling the social variation of a TTI system's inputs in order to identify regions of the representation space that correlate with this social variation,
and use those to audit the diversity of TTI systems in an application setting.


Our approach, shown in Figure~\ref{fig:methodology}, is the following:
first, we define a set of \emph{identity characteristics} for which we want to evaluate diversity and representativity -- in the rest of this paper, we will consider both gender and ethnicity.
%that we want to audit and which TTI system(s) we want to compare.
Second, we generate a set of images with TTI systems by controlling the variation of markers of these social attributes in the systems' input prompts -- i.e. by spanning different ways of referring to a person's gender or ethnicity in the prompt.
%Secondly, we generate \textbf{XX} images per TTI system for the same set of attributes.
Third, we generate sets of images for prompts with different \textit{social attributes} that we want to audit -- here, we focus on the profession attribute and generate images for a set of 146 professions.
% Thirdly, we define a set of \emph{identity characteristics} that we want to use to orient our analysis.
Finally, given the multimodal nature of TTI systems, we carry out two sets of analyses: a text-based analysis that leverages Visual Question Answering (VQA) models in the text modality, and a clustering-based evaluation to characterize correlations between social attributes and identity characteristics directly in the image modality.
%These views of the systems' output space provide complementary insights into the bias mechanisms, which we jointly leverage to compare various systems' biases.
We explain each step in more detail in the sections below.

\subsection{Generating a Dataset of Identity Characteristics and Social Attributes} \label{subsec:methodology:prompting}

In order to generate a diverse set of prompts to evaluate the system outputs' variation across dimensions of interest, we use the pattern \textit{``Photo portrait of a $[X]$ $[Y]$''},~\footnote{Before converging on this prefix, we experimented with several others, including \textit{"Photo of"}, \textit{"Photograph of"}, \textit{"Portrait of"}, \textit{"Close up of"}, but found that \textit{"Photo portrait of"} gave the most realistic results.} where $X$ and $Y$ can span the values of the identity characteristics ---  ethnicity and gender --- and of the professional attribute that we focus our analysis on --- the name of the profession.
For the professional names, we rely on a list of 146 occupations taken from the \href{https://www.bls.gov/cps/cpsaat11.htm}{U.S. Bureau of Labor Statistics (BLS)}, which also provides us with additional information such as the demographic characteristics and salaries of each profession that we can leverage in our analysis. For the identity characteristics, we add a suffix to make the pattern \textit{``Photo portrait of a $[ethnicity]$ $[gender]$ at work''}-- since adding the "at work" suffix makes the images more directly comparable to those generated for professions, given that these are often set in workplace settings. For gender, we use three values in this study: ``man'', ``woman'', and ``non-binary person''; and as an additional option we also use the unspecified ``person'' to use the same pattern without specifying gender. This is still far from a complete exploration of gender variation, and in particular misses gender terms relevant to trans* experiences and gender experiences outside of the US context. For ethnicity, we are similarly grounded in the North American context, as we started with a list of ethnicities in the US census which we then expanded with several synonyms per group (the full list is available in Appendix~\ref{sec:appendix:prompt-target-attributes}). Enumerating all values of the gender and ethnicity markers we defined led to a total of 68 prompts. Examples of generations for both the social attributes and target attributes sets are shown in Figure~\ref{fig:methodology}.

We use these prompts to generate two supporting datasets for our evaluation of three TTI systems to compare the social biases they encode: Stable Diffusion v.1.4, v.2, and \DallE. We choose these three systems because they represent popular TTI systems that were state-of-the-art at the start of this project (early 2023) and allow us both to compare two versions of the same system (Stable Diffusion v.1.4. and v.2) as well as open versus closed-source systems (Stable Diffusion vs \DallE).
We generate an \emph{``Identities'' dataset} to ground our analysis by generating a set of images per model and combination of gender and ethnicity phrases in the prompts, for a total of 68 prompts and 2040 images.
We then generate a \emph{``Professions'' dataset} to evaluate for each model, by generating a set of images for each system for each of the 146 professions in our set.
%We then use the prompts described above to generate 100 images per prompt for each TTI system, for a total of XXXX  images.
%This provides us two datasets of images with rich feature representations: the\emph{``Identities'' dataset}, which contains 680 images for each of the 3 TTI systems (10 images per prompt, generated based on the 68 prompts) that explore the space of the system's output across various combinations of explicit gender and ethnicity mentions in the prompt,
%and the \emph{``Professions'' dataset}, which contains 15,000 images per TTI system (100 images per prompt for 150 prompts), corresponding to professions.
In the following section, we describe our two methods for evaluating the social diversity showcased in the \emph{``Professions'' datasets} without gender or ethnicity label assignment, including a novel non-parametric method that compares its images to those in the \emph{``Identities'' dataset}.

\subsection{Different Approaches for Analyzing Generated Images}
\label{sec:methodology:bias-analysis}

% Our goal with the method we describe below is to audit TTI systems from the perspective of social bias, analyzing the visual characteristics that are correlated with perceptions of social attributes.
Given the multimodal nature of TTI systems we wish to audit for social biases, we propose two methods to analyze their outputs; one method relying on text representations of the images, and one that focuses on direct comparisons in the image space; we explain these in the sections below.
%with the ``Identities'' prompt dataset, we probe the model's ability to vary its outputs in ways that correspond to social variation. After projecting those outputs into an embedding space through the information bottleneck of a BLIP or CLIP model and delineating regions of the projected space that meaningfully distribute the social attributes in the prompts, the diversity of the ``Professions'' images across these regions should reflect diversity across social attributes.

\paragraph{Text Features Analysis: Image-to-Text Systems}

One category of image representations that we can leverage more easily to find evidence of bias is that of text-based representations; specifically, textual representations of the figures depicted in an image. We use two ML-based systems to automatically obtain such descriptions --- one designed for image captioning (the \href{https://huggingface.co/nlpconnect/vit-gpt2-image-captioning}{ViT GPT-2 Model}~\cite{nlp_connect_2022} trained on the MS-COCO dataset~\cite{lin2014microsoft}) and one designed for Visual Question Answering, or VQA (the \href{https://huggingface.co/Salesforce/blip-vqa-base}{BLIP VQA base model}~\cite{BLIP2022} which was pre-trained on a set of 124M image-text pairs including MS-COCO~\cite{lin2014microsoft} and a subset of LAION-400M~\cite{schuhmann2021laion}, then fine-tuned on VQA 2.0~\cite{goyal2017making}). While the image captioning model generates multi-word captions for each of the images, the VQA system outputs a single word or a short phrase that answers the question \textit{``What word best describes this person's appearance?''}.
Both models are open-ended, which means that we do not know a priori whether the outputs will feature words directly related to the social attributes we are studying: our goal in using them is to analyze aggregate statistics of these words across the generated images, without constraining either model to labels that reflect our own biases. We also recognize that both of the model contain confounding factors and biases that will be reflected in the text that they output, which is why we do not interpret the captions as a ground truth, but a feature among others.

\paragraph{Visual Features Analysis: Clustering-Based Approach}

While textual descriptions of the images are much more tractable than their raw pixel representations, they also carry more limited information, especially in the case of the short VQA answers. A middle ground between the two levels of faithfulness to the information contained in an image can be achieved by leveraging dense embedding techniques that project the images into a multidimensional vector space.
%, which we explore using two such systems.
% First, we look at the broadly used \href{https://huggingface.co/openai/clip-vit-base-patch32}{CLIP image encoder}~\cite{radford2021learning}, pre-trained on an undisclosed dataset of web-scraped image-text pairs, which is used to guide the training of both \DallE and Stable Diffusion v1.4, as well as to filter the pre-training datasets of \DallE and both versions of Stable Diffusion.
To that end, we leverage the same BLIP VQA system as above to also obtain image embedding; namely, the normalized average of the question token embeddings produced by the VQA encoder conditioned on the image. We chose this approach because it allowed us to focus the embedding model on the person depicted in an image - which produced an embedding structure better suited to our goal than alternatives including the \href{https://huggingface.co/openai/clip-vit-base-patch32}{CLIP image encoder}~\cite{radford2021learning} (see the Appendix for comparisons).

In order to make use of those embedding systems to evaluate social biases in a model's output, we need to be able to identify regions of the embedding space that correspond to visual features associated with a viewer's perception of gender or ethnicity. We do this by leveraging the \emph{``Identities'' dataset} described above. Specifically, we cluster the embeddings of the 2040 data points into 24 sets of images with a Ward linkage criterion for the dot product~\cite{ward1963hierarchical}. We can then identify trends in a TTI systems' generated images, including those in the \emph{``Professions'' dataset}, by quantifying which of these 24 regions they tend to over- or under-represent. Assigning an example to one of these regions is different from assigning it a predefined identity characteristic. Indeed, each of the region corresponds to identity prompts that span different gender and identity phrases. Additionally, delineating 24 regions in the space would not be enough to cover the 68 combinations of identity phrases showcased in the prompts. However, since the clustering was run on a space whose main source of variation came from those identity phrases, we can expect that they do encode visual features that are meaningfully associated with those -- and jointly varying gender and ethnicity phrases in the prompts allows the regions to encode phenomena that are specific to their intersection.  We will validate this intuition and propose specific analyses based on a TTI system outputs' cluster assignments in Section~\ref{sec:results:cluster-biases}.

%\paragraph{Clustering}
%After projecting the generated images into this BLIP VQA embedding space, we use clustering to delineate regions of the projected space that meaningfully distribute the social attributes in the prompts and the diversity of the generated images.
%We use a two-step hierarchical clustering approach: we first cluster the vectors obtained for the datasets generated by Stable Diffusion v1.4 and 2 and \DallE together (680 per TTI system, adding up to 2040 in total) with a dot product similarity and Ward linkage~\cite{ward1963hierarchical}, and then merge the smallest clusters to the closest centroid until we are left with 24 clusters
%~\footnote{We also tried the same analyses with the CLIP model instead of the BLIP model, and with 12/48 clusters instead of 24, and present those results in the Supplementary Materials.}.
%We use cluster assignments as somewhat akin to `topic models' of gender and ethnicity, which also allows us to group clusters together based on specific contextual needs and characteristics of interest.
%\textbf{based on specific contextual needs} (e.g. cluster 7 in both "woman" and "Black" analysis)
%In contrast to methods that identify independent directions of variation in the space to denote social dimensions, we rely on clustering to address the interdependence of the social attributes; our method also systems' social variation without \textit{a priori} specifying how many clusters should correspond to gender or to ethnicity modes, or whether an ethnicity from our prompt set should correspond to a single cluster or be split between several. This yields a flexible but still interpretable delineation of the TTI system's output space. %that we can subsequently used to characterize the ``Professions'' examples by assigning them to the closest cluster by centroid - we show how to interpret these regions as well as use them to quantify the model diversity and describe specific bias dynamics. %, alongside our analysis of gender-marked words in the ``Professions'' text descriptions and the interactive visual tools that we created to explore the raw images.
%Our approach is also, by definition, extensible to  other TTI systems, allowing us to  analyze new images by assigning them to existing clusters. 
%TODO: In order to analyze a new image, we assign it to a cluster.
 
% We test this approach with three embedding systems, at three different levels of granularity. We use the image embedding component of CLIP after its projection layer, as well as a BLIP model fine-tuned on a VQA task. The BLIP model works by first embedding an image using a Vision Transformer system, then embedding the tokens of the question conditioned on the image representation to then finally generate an answer. We found that taking the normalized average of the question token embeddings for targeted questions provided an apt representation of the image, so we used this embedding method with the questions \textit{``what word best describes this person's ethnicity?''}, and \textit{``what word best describes this person's appearance?''}.

% \paragraph{Clustering embeddings} \textbf{TODO: Yacine} Talk about how the clusters correspond to modes in the joint ethnicity+gender space, their social meaning is then visualized by looking at the top examples in the cluster --- similar to topic model evaluation.

% We leverage these different, albeit complementary approaches to hone in on different aspects of the generated images as well as to minimize the bias that using any single model would entail. Also, we consciously do not leverage any classification models, given that both VQA and image captioning models produce more general output and allow for constrained decoding which can be adapted to a specific set of terms (as opposed to limited to the small set of ethnic/gender categories that were defined in training dataset of classification models).

%\subsubsection{Caption vocabulary} In order to capture the most relevant features in the captions that we generated, we compared the most salient terms for each of the professions and adjectives, as well as number of times people-related terms such as 'man', 'woman', 'person' and [the name of the profession] were used for each profession and adjective, using sci-kit learn~\cite{scikit-learn}). This enabled us to hone in on specific phenomena such as \emph{markedness} -- i.e., whether specific gender- or ethnicity-marked terms were mentioned in the captions or not, as well as allowing us to identify professions that are more \emph{visually-salient} (e.g. police officer) compared to others that were less so (e.g. office worker).

%\subsubsection{Cluster assignment}

\paragraph{Interactive Exploration}
In parallel to the approaches described in previous sub-sections, we also introduce a series of interactive tools to support story-based examination of biases in images generated by the TTI systems (which we will describe in more detail in Section~\ref{sec:results:tools}). The primary goal of these tools is not to produce quantitative insights into the images, but to allow more ad-hoc in-depth explorations of them, since there are many aspects of generated images that are hard to analyze automatically (such as the presence of certain specific characteristics and visual elements in images), but which can be observed during interactive explorations, based on different angles defined by the user.

