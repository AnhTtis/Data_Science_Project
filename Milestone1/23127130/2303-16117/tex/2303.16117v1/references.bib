%%%%%%%%%%%%%%%% Project 3 



@article{kelly2022virtue,
  title={The Virtue of Complexity Everywhere},
  author={Kelly, Bryan T and Malamud, Semyon and Zhou, Kangying},
  journal={Available at SSRN},
  year={2022}
}

@misc{Kelly22deep,
author = {Didisheim, Antoine and Kelly, Bryan and Malamud, Semyon},
  %doi = {10.48550/ARXIV.2203.05417},
  %url = {https://arxiv.org/abs/2203.05417},
  %keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Regression Ensembles},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Zero v1.0 Universal}
}


%%% Double deep descent 
@article{NakkiranPreetum2021Dddw,
issn = {1742-5468},
journal = {Journal of statistical mechanics},
language = {eng},
number = {12},
pages = {124003-},
title = {Deep double descent: where bigger models and more data hurt},
volume = {2021},
year = {2021},
abstract = {Abstract
We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets
worse
and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the
effective model complexity
and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually
hurts
test performance.},
author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
}


%%%% Lottery ticket hypothesis 


@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@inproceedings{NEURIPS2019_a4613e8d,
 author = {Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
 url = {https://proceedings.neurips.cc/paper/2019/file/a4613e8d72a61b3b69b32d040f89ad81-Paper.pdf},
 volume = {32},
 year = {2019}
}



%% Python packages 

@misc{Takuya19,
  doi = {10.48550/ARXIV.1907.10902},
  url = {https://arxiv.org/abs/1907.10902},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



%% Catch22 
@Article{Lubba2019,
author={Lubba, Carl H.
and Sethi, Sarab S.
and Knaute, Philip
and Schultz, Simon R.
and Fulcher, Ben D.
and Jones, Nick S.},
title={catch22: CAnonical Time-series CHaracteristics},
journal={Data Mining and Knowledge Discovery},
year={2019},
month={Nov},
day={01},
volume={33},
number={6},
pages={1821-1852},
issn={1573-756X},
doi={10.1007/s10618-019-00647-x},
url={https://doi.org/10.1007/s10618-019-00647-x}
}

%% hctsa
@article{hctsa,
publisher = {Royal Society, The},
title = {Highly comparative time-series analysis: the empirical structure of time series and their methods},
year = {2013-Apr},
author = {Fulcher, BD and Little, MA and Jones, NS},
keywords = {Science & Technology},
language = {eng},
}



%% Signature Transform Theory 
@misc{Chevyrev16,
  doi = {10.48550/ARXIV.1603.03788},
  url = {https://arxiv.org/abs/1603.03788},
  author = {Chevyrev, Ilya and Kormilitzin, Andrey},
  title = {A Primer on the Signature Method in Machine Learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Terry22,
  doi = {10.48550/ARXIV.2206.14674},
  url = {https://arxiv.org/abs/2206.14674},
  author = {Lyons, Terry and McLeod, Andrew D.},
  title = {Signature Methods in Machine Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{James20,
  doi = {10.48550/ARXIV.2006.00873},
  url = {https://arxiv.org/abs/2006.00873},
  author = {Morrill, James and Fermanian, Adeline and Kidger, Patrick and Lyons, Terry},
  title = {A Generalised Signature Method for Multivariate Time Series Feature Extraction},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
kidger2021signatory,
title={Signatory: differentiable computations of the signature and logsignature transforms, on both {\{}CPU{\}} and {\{}GPU{\}}},
author={Patrick Kidger and Terry Lyons},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=lqU2cs3Zca}
}


@inproceedings{Lyons07,
publisher = {Springer Berlin Heidelberg},
series = {École d'Été de Probabilités de Saint-Flour, 1908},
title = {Differential Equations Driven by Rough Paths : Ecole d’Eté de Probabilités de Saint-Flour XXXIV-2004 },
year = {2007},
author = {Lyons, Terry J.},
address = {Berlin, Heidelberg},
edition = {1st ed. 2007.},
isbn = {1-280-85347-6},
keywords = {Mathematical analysis},
language = {eng},
}





%% TimeSeries Deep Models 


@book{percival_walden_2020, 
place={Cambridge}, 
series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
title={Spectral Analysis for Univariate Time Series}, 
%DOI={10.1017/9781139235723}, 
publisher={Cambridge University Press},
author={Percival, Donald B. and Walden, Andrew T.}, 
year={2020}, 
collection={Cambridge Series in Statistical and Probabilistic Mathematics},
}


@misc{Bryan19,
  doi = {10.48550/ARXIV.1912.09363},
  url = {https://arxiv.org/abs/1912.09363},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  title = {Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}






%%% Previous Numerai ML studies

@INPROCEEDINGS{9591781,
  author={N, Jeipratha. P. and Vasudevan, B.},
  booktitle={2021 2nd International Conference on Smart Electronics and Communication (ICOSEC)}, 
  title={Effective Implementation of Neural Network Model with Tune Parameter for Stock Market Predictions}, 
  year={2021},
  volume={},
  number={},
  pages={1038-1042},
  doi={10.1109/ICOSEC51865.2021.9591781}
}

  
@INPROCEEDINGS{8442645,
  author={Singh, Shekar and Sharma, Seema},
  booktitle={2018 8th International Conference on Cloud Computing, Data Science & Engineering (Confluence)}, 
  title={Forecasting Stock Price Using Partial Least Squares Regression}, 
  year={2018},
  volume={},
  number={},
  pages={587-591},
  doi={10.1109/CONFLUENCE.2018.8442645}
}




%%% Stochsatic Control Problem 


@book{Kirk70,
author = {Kirk, Donald E.},
address = {Mineola, N.Y},
booktitle = {Optimal control theory : an introduction},
isbn = {0486434842},
keywords = {Control theory},
language = {eng},
lccn = {2003070111},
publisher = {Dover Publications},
title = {Optimal control theory : an introduction },
year = {2004 - 1970},
}

@book{Sutton18,
author = {Sutton, Richard S.},
address = {Cambridge, Massachusetts},
booktitle = {Reinforcement learning : an introduction},
edition = {Second edition.},
isbn = {9780262352703},
keywords = {Reinforcement learning},
language = {eng},
publisher = {The MIT Press},
series = {Adaptive computation and machine learning series},
title = {Reinforcement learning : an introduction },
year = {2018},
}


@incollection{ZhangQing2015SDP,
abstract = {This article is concerned with one of the traditional approaches for stochastic control problems: Stochastic dynamic programming. Brief descriptions of stochastic dynamic programming methods and related terminology are provided. Two asset-selling examples are presented to illustrate the basic ideas. A list of topics and references are also provided for further reading.},
author = {Zhang, Qing},
address = {London},
booktitle = {Encyclopedia of Systems and Control},
copyright = {Springer-Verlag London 2015},
isbn = {9781447150572},
keywords = {Asset-selling rule ; Bellman equation ; Hamilton-Jacobi-Bellman equation ; Markov decision problem ; Optimality principle ; Stochastic control ; Viscosity solution},
language = {eng},
pages = {1333-1337},
publisher = {Springer London},
title = {Stochastic Dynamic Programming},
year = {2015},
}


%%% Replication Studies 

@misc{https://doi.org/10.48550/arxiv.2203.06498,
  doi = {10.48550/ARXIV.2203.06498},
  url = {https://arxiv.org/abs/2203.06498},
  author = {Hullman, Jessica and Kapoor, Sayash and Nanayakkara, Priyanka and Gelman, Andrew and Narayanan, Arvind},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2106.03253,
  doi = {10.48550/ARXIV.2106.03253},
  url = {https://arxiv.org/abs/2106.03253},
  author = {Shwartz-Ziv, Ravid and Armon, Amitai},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Tabular Data: Deep Learning is Not All You Need},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2101.02118,
  doi = {10.48550/ARXIV.2101.02118},
  url = {https://arxiv.org/abs/2101.02118},
  author = {Elsayed, Shereen and Thyssens, Daniela and Rashed, Ahmed and Jomaa, Hadi Samer and Schmidt-Thieme, Lars},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Do We Really Need Deep Learning Models for Time Series Forecasting?},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{10.1145/3434185,
author = {Dacrema, Maurizio Ferrari and Boglio, Simone and Cremonesi, Paolo and Jannach, Dietmar},
title = {A Troubling Analysis of Reproducibility and Progress in Recommender Systems Research},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3434185},
doi = {10.1145/3434185},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {20},
numpages = {49},
keywords = {evaluation; reproducibility, deep learning, Recommender systems}
}


@software{the_turing_way_community_2021_5671094,
  author       = {The Turing Way Community},
  title        = {{The Turing Way: A handbook for reproducible, 
                   ethical and collaborative research}},
  month        = nov,
  year         = 2021,
  note         = {{This work was supported by The UKRI Strategic 
                   Priorities Fund under the EPSRC Grant
                   EP/T001569/1, particularly the "Tools, Practices
                   and Systems" theme within that grant, and by The
                   Alan Turing Institute under the EPSRC grant
                   EP/N510129/1.}},
  publisher    = {Zenodo},
  version      = {1.0.1},
  doi          = {10.5281/zenodo.5671094},
  url          = {https://doi.org/10.5281/zenodo.5671094}
}


@techreport{NBERw28432,
 title = "Is There A Replication Crisis In Finance?",
 author = "Jensen, Theis Ingerslev and Kelly, Bryan T and Pedersen, Lasse Heje",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "28432",
 year = "2021",
 month = "February",
 doi = {10.3386/w28432},
 URL = "http://www.nber.org/papers/w28432",
}


@article{10.1093/rfs/hhy131,
    author = {Hou, Kewei and Xue, Chen and Zhang, Lu},
    title = "{Replicating Anomalies}",
    journal = {The Review of Financial Studies},
    volume = {33},
    number = {5},
    pages = {2019-2133},
    year = {2018},
    month = {12},
    issn = {0893-9454},
    doi = {10.1093/rfs/hhy131},
    url = {https://doi.org/10.1093/rfs/hhy131},
    eprint = {https://academic.oup.com/rfs/article-pdf/33/5/2019/33710468/hhy131.pdf},
}




@ARTICLE{585893,
  author={Wolpert, D.H. and Macready, W.G.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={No free lunch theorems for optimization}, 
  year={1997},
  volume={1},
  number={1},
  pages={67-82},
  doi={10.1109/4235.585893}}


%%%% Open Source Cross Sectional Asset Pricing 
@article{ChenZimmermann2021,
  title={Open Source Cross Sectional Asset Pricing},
  author={Chen, Andrew Y. and Tom Zimmermann},
  journal={Critical Finance Review},
  year={Forthcoming}
}


%%%% Examples of ML papers in Finance 

@INPROCEEDINGS{9469554,
  author={Arosemena, Juan and Pérez, Noel and Benítez, Diego and Riofrío, Daniel and Flores-Moyano, Ricardo},
  booktitle={2021 IEEE Colombian Conference on Applications of Computational Intelligence (ColCACI)}, 
  title={Stock Price Analysis with Deep-Learning Models}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ColCACI52978.2021.9469554}}


@INPROCEEDINGS{8126078,
  author={Selvin, Sreelekshmy and Vinayakumar, R and Gopalakrishnan, E. A and Menon, Vijay Krishna and Soman, K. P.},
  booktitle={2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
  title={Stock price prediction using LSTM, RNN and CNN-sliding window model}, 
  year={2017},
  volume={},
  number={},
  pages={1643-1647},
  doi={10.1109/ICACCI.2017.8126078}}
  
  
@INPROCEEDINGS{7550882,
  author={Akita, Ryo and Yoshihara, Akira and Matsubara, Takashi and Uehara, Kuniaki},
  booktitle={2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)}, 
  title={Deep learning for stock prediction using numerical and textual information}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICIS.2016.7550882}}

@INPROCEEDINGS{8355458,
  author={Althelaya, Khaled A. and El-Alfy, El-Sayed M. and Mohammed, Salahadin},
  booktitle={2018 9th International Conference on Information and Communication Systems (ICICS)}, 
  title={Evaluation of bidirectional LSTM for short-and long-term stock market prediction}, 
  year={2018},
  volume={},
  number={},
  pages={151-156},
  doi={10.1109/IACS.2018.8355458}}


@ARTICLE{8653278,
  author={Wen, Min and Li, Ping and Zhang, Lingfei and Chen, Yan},
  journal={IEEE Access}, 
  title={Stock Market Trend Prediction Using High-Order Information of Time Series}, 
  year={2019},
  volume={7},
  number={},
  pages={28299-28308},
  doi={10.1109/ACCESS.2019.2901842}}  

@ARTICLE{8573780,
  author={Liu, Guang and Wang, Xiaojie},
  journal={IEEE Access}, 
  title={A Numerical-Based Attention Method for Stock Market Prediction With Dual Information}, 
  year={2019},
  volume={7},
  number={},
  pages={7357-7367},
  doi={10.1109/ACCESS.2018.2886367}}



%%%% Gradient Boosting Methods 

%% Gradient Boosting paper 
@article{FriedmanJeromeH.2001GfaA,
year = {2001},
author = {Friedman, Jerome H.},
copyright = {Copyright 2001 Institute of Mathematical Statistics},
issn = {0090-5364},
journal = {The Annals of statistics},
keywords = {62-02 ; 62-07 ; 62-08 ; 62G08 ; 62H30 ; 68T10 ; boosting ; decision trees ; Function estimation ; robust nonparametric regression},
language = {eng},
number = {5},
pages = {1189-1232},
publisher = {The Institute of Mathematical Statistics},
title = {Greedy function approximation: A gradient boosting machine},
volume = {29},
}

@article{B_hlmann_2007,
	doi = {10.1214/07-sts242},
	url = {https://doi.org/10.1214%2F07-sts242},
	year = 2007,
	month = {nov},
	publisher = {Institute of Mathematical Statistics},
	volume = {22},
	number = {4},
	author = {Peter Bühlmann and Torsten Hothorn},
	title = {Boosting Algorithms: Regularization, Prediction and Model Fitting},
	journal = {Statistical Science}
}

@article{FREUND1997119,
title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
journal = {Journal of Computer and System Sciences},
volume = {55},
number = {1},
pages = {119-139},
year = {1997},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.1997.1504},
url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
author = {Yoav Freund and Robert E Schapire},
}


@article{Kotsiantis2011DecisionTA,
  title={Decision trees: a recent overview},
  author={Sotiris B. Kotsiantis},
  journal={Artificial Intelligence Review},
  year={2011},
  volume={39},
  pages={261-283}
}


@inproceedings{10.1145/2939672.2939785,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@inproceedings{NIPS2017_6449f44a,
 author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
 url = {https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{NEURIPS2018_14491b75,
 author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {CatBoost: unbiased boosting with categorical features},
 url = {https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf},
 volume = {31},
 year = {2018}
}



@InProceedings{pmlr-v38-korlakaivinayak15,
  title = 	 {{DART: Dropouts meet Multiple Additive Regression Trees}},
  author = 	 {Korlakai Vinayak, Rashmi and Gilad-Bachrach, Ran},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {489--497},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/korlakaivinayak15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/korlakaivinayak15.html},
}


%%%% NN tabular models 

@misc{https://doi.org/10.48550/arxiv.2110.01889,
  doi = {10.48550/ARXIV.2110.01889},
  url = {https://arxiv.org/abs/2110.01889},
  author = {Borisov, Vadim and Leemann, Tobias and Seßler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Neural Networks and Tabular Data: A Survey},
  publisher = {arXiv},
  year = {2021},
}


@article{Arik_Pfister_2021, 
title={TabNet: Attentive Interpretable Tabular Learning}, 
volume={35}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/16826}, 
number={8}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Arik, Sercan Ö. and Pfister, Tomas}, 
year={2021}, 
month={May}, 
pages={6679-6687}
}


@inproceedings{AutoInt_Song_2019,
	doi = {10.1145/3357384.3357925},
	url = {https://doi.org/10.11452F3357384.3357925},
	year = 2019,
	month = {nov},
	publisher = {{ACM}},
	author = {Weiping Song and Chence Shi and Zhiping Xiao and Zhijian Duan and Yewen Xu and Ming Zhang and Jian Tang},
  	title = {{AutoInt}},
	booktitle = {Proceedings of the 28th {ACM} International Conference on Information and Knowledge Management}
}



@misc{TabTransformer,
  doi = {10.48550/ARXIV.2012.06678},
  url = {https://arxiv.org/abs/2012.06678},
  author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TabTransformer: Tabular Data Modeling Using Contextual Embeddings},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Zero v1.0 Universal}
}

@misc{NODE,
  doi = {10.48550/ARXIV.1909.06312},
  url = {https://arxiv.org/abs/1909.06312},
  author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}



%% Neural Networks 

@article{
doi:10.1073/pnas.1611835114,
author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
title = {Overcoming catastrophic forgetting in neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {114},
number = {13},
pages = {3521-3526},
year = {2017},
doi = {10.1073/pnas.1611835114},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1611835114},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
}

@misc{https://doi.org/10.48550/arxiv.1412.6980,
  doi = {10.48550/ARXIV.1412.6980},
  url = {https://arxiv.org/abs/1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  title = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


%% Hyperopt
@inproceedings{10.5555/3042817.3042832,
author = {Bergstra, J. and Yamins, D. and Cox, D. D.},
title = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
year = {2013},
publisher = {JMLR.org},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–115–I–123},
location = {Atlanta, GA, USA},
series = {ICML'13}
}




%%%% Kaggle Competitions Links 

@online{ubiquant-market-prediction,
  author="Kaggle",
  title="{ubiquant-market-prediction}",
  url="https://www.kaggle.com/competitions/ubiquant-market-prediction/overview",
  note="(2022, Jan 18)",
}

@online{jane-street-market-prediction,
  author="Kaggle",
  title="{jane-street-market-prediction}",
  url="https://www.kaggle.com/c/jane-street-market-prediction",
  note="(2021, Feb 15)",
}

@online{optiver-realized-volatility-prediction,
  author="Kaggle",
  title="{optiver-realized-volatility-prediction}",
  url="https://www.kaggle.com/competitions/optiver-realized-volatility-prediction",
  note="(2021, Jun 28)",
}

@online{numerai,
  author="Numerai",
  title="{Numerai Hedge Fund}",
  url="https://numerai.fund/",
  note="(2022, Apr 12)",
}

@online{numerai-tc,
  author="Numerai",
  title="{Numerai True Contribution}",
  url="https://forum.numer.ai/t/true-contribution-details/5128",
  note="(2022, Apr 12)",
}

@online{numerai-signals,
  author="Numerai",
  title="{Numerai Signals Competition}",
  url="https://signals.numer.ai/tournament",
  note="(2022, Apr 12)",
}
