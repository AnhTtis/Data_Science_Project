\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{cite}
% \usepackage{biblatex}
% \addbibresource{sbr.bib} %Import the bibliography file
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{optidef}
% \usepackage{svg}
% \usepackage{cleveref}
\usepackage{float}
\usepackage{cancel}
% \usepackage[demo]{graphicx}

\usepackage{amssymb}
\usepackage[margin=1in]{geometry}


\usepackage{graphicx}
\usepackage{pdfpages}

\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}




\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
        \setcounter{section}{0}
        \renewcommand{\thesection}{S\arabic{section}}%
     }

\title{Supplementary Information: Robust 3D fluorescent imaging through scattering with deep learning on synthetic training data}

\author[1]{Jeffrey Alido}
\author[1]{Joseph Greene}
\author[1]{Yujia Xue}
\author[1]{Guorong Hu}
\author[1]{Yunzhe Li}
\author[2]{Kevin J. Monk}
\author[2]{Brett T. DeBenedicts}
\author[2,3]{Ian G. Davison}
\author[1,3]{Lei Tian}

\affil[1]{Department of Electrical and Computer Engineering, Boston University, Boston, MA 02215, USA.}
\affil[2]{Department of Biology, Boston University, Boston, MA 02215, USA.}
\affil[3]{Department of Biomedical Engineering, Boston University, Boston, MA 02215, USA.}
\affil[*]{Correspondence: leitian@bu.edu, Tel.: 1-617-353-1334}

\begin{document}
\includepdf[pages=-]{SBRNet_main.pdf}
\clearpage
\beginsupplement

\maketitle

\clearpage
\tableofcontents

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Value noise visualization}
\begin{figure}[h!] 
    \centering
    % \hspace*{-2cm}    
  \includegraphics[width=14cm]{value.pdf}
  \caption{Raw 600 x 600 value noise and its low-pass filtered result using a Gaussian kernel with std of 13 pixels. Note the spatially varying intensities which contributes to different local SBRs.}
  \label{fig: value noise viz}
\end{figure}

\clearpage
\section{Statistical comparison of synthetic and experimental data}
\begin{figure}[h!]
    \centering
    \hspace*{-1cm}    
  \includegraphics[width=19cm]{sim181.pdf}
  \caption{Qualitative and quantitative comparison of synthetic and experimental data for $l_s = \SI{182}{\micro\meter}$. We show the spatial frequency, power spectra, and intensity histograms for different views of a CM\textsuperscript{2} measurement and observe strong similarities.}
  \label{fig: 182 stats}
\end{figure}
\begin{figure}[h!] 
    \centering
    \hspace*{-1cm}    
  \includegraphics[width=19cm]{sim72.pdf}
  \caption{Qualitative and quantitative comparison of synthetic and experimental data for $l_s = \SI{72}{\micro\meter}$. We show the spatial frequency, power spectra, and intensity histograms for different views of a CM\textsuperscript{2} measurement and observe strong similarities.}
  \label{fig: 72 stats}
\end{figure}

\clearpage
\section{Network architecture}
\begin{figure}[h!] 
    \centering
    % \hspace*{-2cm}    
  % \includesvg[inkscapelatex=false,width=12cm]{architecture.svg}
  \includegraphics[width=12cm]{architecture.pdf}
  \caption{Schematic of our CNN architecture. The superscript of the convolutional layers denotes the number of input channels to the number of output channels, and the subscript denotes the kernel size. BN denotes a batch normalization layer, and B denotes batch size number.}
  \label{fig: network architecture}
\end{figure}

\clearpage
 \section{Synthetic test data precision and recall}
\begin{figure}[h!]
    \centering
    \hspace*{-1cm}    
  \includegraphics[width=19cm]{synthetic.pdf}
  \caption{Simulation test data recall and precision metrics. The labeled SBR values are the average SBR values for the particles in the first depth layer before any optical attenuation. Similar to the optical signal, the SBR of a particle would decay exponentially over increasing depth of the particle, which is why we observe a systematic decay of performance for all scattering lengths, as seen in the normalized depths curves. Shaded regions represent standard error over 25 statistical samples. Note that the precision curve for free space peaks at a consistent depth, indicating the network's preference over objects with that depth's system response.}
  \label{fig: simulation recall and precision}
\end{figure}

\clearpage
 \section{Confocal measurement of experimental scattering phantoms}
\begin{figure}[h!]
    \centering
    \hspace*{-.5cm}    
  \includegraphics[width=16cm]{confocal.pdf}
  \caption{XZ MIP of $\SI{1600}{\micro\meter} \times \SI{50}{\micro\meter}$ ROI of the confocal measurements of the scattering phantoms. The dashed-dotted shell represents 1 scattering length from the surface of the phantom. We observe reasonable intensity attenuation that follows Beer-Lambert's law.}
  \label{fig: confocal proof}
\end{figure}

\clearpage
% \section{Experimental scattering phantom reconstruction}
% \begin{figure}[h!]
%     \centering
%   \includegraphics[width=1\linewidth]{181results.pdf}
%   \caption{Reconstruction results for the \SI{182}{\micro\meter} scattering phantom that contains a mixture of 10 and \SI{15}{\micro\meter} fluorescent beads with added background fluorescence for low SBR particle measurements. (a) The raw CM\textsuperscript{2} measurement with a zoom-in of a small region of interest (ROI) with beads we label to validate reconstruction performance. (b) Line profiles of the labeled beads in the raw CM\textsuperscript{2} measurement. We list each bead's depth relative to the surface of the phantom as well as their measurement SBRs. (c) MIPs of confocal microscopy 3D measurements, reconstructions with SBR-Net, FS-Net, and the model-based ADMM algorithm. The dashed-dotted line in the confocal XZ/YZ MIPs represent a distance of one scattering length from the surface of the phantom. For this scattering phantom, SBR-Net is able to recover all 5 beads at their correct depth location, which is as deep as over half a scattering length. SBR-Net also reconstructs and localizes a particle with a measurement SBR of 1.08. However, while SBR-Net localizes particles well, it reconstructs them with some inaccuracies in intensity and size. FS-Net and ADMM both fail to reconstruct the particle of the 5 with the lowest SBR of 1.08, and localizes the remaining 4 with poor accuracy. Additionally, the XY MIPs show that SBR-Net provides background rejection for the 2D reconstruction case while retaining low SBR particles, which FS-Net and ADMM  remove.}
%   \label{fig: 182 results}
% \end{figure}

% \clearpage

% \begin{figure}[h!]
%     \centering
%   \includegraphics[width=1\linewidth]{72results.pdf}
%   \caption{Reconstruction results for the \SI{72}{\micro\meter} scattering phantom that contains a mixture of 10 and \SI{15}{\micro\meter} fluorescent beads with added background fluorescence for low SBR particle measurements. (a) The raw CM\textsuperscript{2} measurement with a zoom-in of a small region of interest (ROI) with beads we label to validate reconstruction performance. (b) Line profiles of the labeled beads in the raw CM\textsuperscript{2} measurement. We list each bead's depth relative to the surface of the phantom as well as their measurement SBRs. (c) MIPs of confocal microscopy 3D measurements, reconstructions with SBR-Net, FS-Net, and the model-based ADMM algorithm. The dashed-dotted line in the confocal XZ/YZ MIPs represent a distance of one scattering length from the surface of the phantom. For this scattering phantom, SBR-Net is able to recover all 5 beads at their correct depth location. FS-Net and ADMM both fail to reconstruct all 5 particles. Across a large FOV, we see consistent performance of SBR-Net reconstructing low SBR particles, while FS-Net and ADMM fail to reconstruct them.}
%   \label{fig: 72 results}
% \end{figure}

% \clearpage

% \begin{figure}
%     \centering
%   \includesvg[inkscapelatex=false,width=1\linewidth]{bgr f1 quantitative.svg}
%   \caption{Quantitative evaluation of BGR-Net, SBR-Net and FS-Net on synthetic test data.}
%   \label{fig: bgr vs fs F1}
% \end{figure}
%  SBR-Net performs better than BGR-Net until a high enough SBR of around 1.55, where target particles have more optical contrast and  are less likely to be removed in the background removal step. However, for practical applications such as in vivo neural imaging, many neurons exhibit measurement SBRs lower than 1.55, rendering SBR-Net a more suitable approach for such a task.

% \clearpage
% \section{\textit{Ex-vivo} fixed rodent brain slice experiment}
% \begin{figure}[h!]
%     \centering
%   \includesvg[inkscapelatex=false, width=16cm]{thin brain slice_supp.svg}
%   \caption{Fixed brain slice reconstruction results with comparisons with FS-Net and model-based inversion.}
%   \label{fig: brainslice}
% \end{figure}

% \clearpage
% \section{SBR-Net trained on different SBR ranges applied to the brain slice measurement}
% \begin{figure}[h!]
%     \centering
%     \hspace*{-1.8cm} 
%   \includesvg[inkscapelatex=false, width=20cm]{sbr range brain slice.svg}
%   % \caption{We observe the robustness-accuracy tradeoff where SBR-Net (2.0 - 3.0) has more false negatives and less accuracy  in the XY MIP, but at the cost of robustness in 3D localization, seen in the YZ MIP of ROI 2. Similarly, we observe the converse for SBR-Net (1.01 - 2.0) where there is more accurate 2D XY localization as highlighted in the dashed yellow oval, but many more false positives in the 3D view of the YZ MIP. SBR-Net (1.1 - 3.0) is a balance between these two cases.}
%   \caption{We observe the robustness-accuracy tradeoff in the sparse labelling region of ROI 1 where SBR-Net (1.1 - 3.0) reconstructs fewer emitters compared to SBR-Net (1.01 - 2.0) (red dashed oval), but has more 3D localization accuracy, as highlighted in the yellow dashed oval. While SBR-Net (1.01 â€“ 2.0) may recover more in the 2D reconstruction of the XY MIP, the 3D localization performance is poor, demonstrating a tradeoff of accuracy for robustness. For the denser labelling region of ROI 2, SBR-Net (2.0 - 3.0) performs poorly in robustness, recovering fewer emitters, but has better localization accuracy (light blue dashed oval). As a separate note, SBR-Net (1.01 - 2.0) outperforms the other two networks in XYlocalization as seen in the XY MIP overlays on confocal measurements.}
%   \label{fig: sbrrangebrainslice}
% \end{figure}

\clearpage

\section{Effect of deep learning factors on SBR-Net generalization to experimental data}

% \begin{figure}[h!]
%     \centering
%     \hspace*{-.5cm}    
%   \includesvg[inkscapelatex=false,width=10cm]{svd cameraman decay.svg}
%   \caption{Decay of the singular values of a sample experimental measurement of a scattering phantom compared to that of the standard cameraman demo image. }
%   \label{fig:500vs1500trainingdata}
% \end{figure}

\begin{figure}[h!]
    \centering
  \includegraphics[width=16cm]{unetvsresnet.pdf}
  \caption{F1 scores for UNet-based (solid) and ResNet-based (dashed) architectures for data with different peak SBRs (1.05, 2.05, 3.0) across 3 scattering lengths (320, 160, \SI{80}{\micro\meter}).}
  \label{fig: unetresnetf1}
\end{figure}

\begin{figure}[h!]
    \centering
  \includegraphics[width=16cm]{valid.pdf}
  \caption{Validation loss for SBR-Net (500 pairs), SBR-Net (1500 pairs), and UNet-based SBR-Net.}
  \label{fig: valloss}
\end{figure}


% \subsection{Number of unique training data pairs} \label{section:unique data pairs}


% It is well-known that the number of training data has a significant impact on neural network generalization performance. \cite{}. Therefore, we experiment with the number of unique training data pairs to test the model's generalization performance. Only 500 unique training data pairs allows good generalization to experimental data because there is not a diversity of features, and we wish to avoid overfitting to synthetic training data as the neural network's behavior will become too brittle to generalize to experimental data, which has small distributional differences with synthetic data. 

% We are mainly interested in learning features similar to neurons and background fluorescence, allowing the learning effort to require fewer examples compared to tasks that use large image datasets with more diverse features like CIFAR-10, for example. Looking at the decay plot of the singular values of our experimental phantom raw measurement and a generic image such as "cameraman" in Figure \ref{fig:500vs1500trainingdata}, we see that the image can be sufficiently described by fewer singular vectors. This corresponds to requiring fewer low-level learned features for which a smaller training dataset is better suited in order to avoid overfitting to synthetic data. 

% We carry out an experiment where we change only the number of unique training data pairs; our main result is trained on 500 unique pairs (80/20, train/validate) and we train the network with 1500 unique pairs (80/20, train/validate). We specify "unique" pairs because convolutional neural networks (CNNs) are equivariant in translation.  This means that even though $224\times 224$ pixel patches from the entire $512\times 512$ data are chosen randomly online during training, allowing a larger number of different inputs for the network to learn from, smaller subpatches are seen multiple times in the same epoch, in different locations, which does not contribute to a lower validation loss due to the translation equivariance of our CNN with $3\times 3$ convolution kernels. 

% Figure \ref{fig:500vs1500trainingdata} also that the SBR-Net trained on 1500 unique pairs suffers from hallucination artifacts, that demonstrate poor generalization to experimental data compared to our main result of an SBR-Net trained on only 500 unique pairs. From analysis in the main text, we understand that a lack of robustness results in lower precision scores, which is equivalent to more hallucination artifacts. Overfitting to synthetic data, even when the validation loss (also on synthetic data) is small, is undesirable for generalization to experimental data because neural networks are notoriously brittle, and even the smallest distributional shift in input data may cause the network to fail. 


% \subsection{Network architecture: UNet vs ResNet}


% We also test how the network architecture affects the generalization performance by comparing our ResNet-based SBR-Net with one that is UNet-based. We find that while the UNet architecture performs better on synthetic test data in terms of depth penetration, as seen in Figure \ref{fig: unetresnetf1}, it generates many more hallucination artifacts for experimental data. One likely reason for the hallucinations is that this UNet-based model may also overfit to synthetic data due to having 34,558,008 parameters, which is 20x more than the ResNet-based architecture (1,732,872), leading to a similar brittle behavior when input data is even slightly different than training data, a similar rationale as that in Section \ref{section:unique data pairs}. While the UNet-based architecture may be more suitable to address the receptive field problem that we observed in Section 2.3, its poor generalization to experimental data due to the sensitivity to out-of-distribution input data would require extensive tuning of several parameters.\\

% We can measure the level of overfitting to synthetic data by comparing the validation losses between the networks. In Figure \ref{fig: valloss}, we verify that the SBR-Net trained 1500 data pairs and the UNet-based SBR-Net have lower validation losses, fitting to synthetic data more. While this is desirable in most deep learning tasks, our validation set is \textit{also} synthetic training data, when we would like the model to generalize to experimental data without ground truth paired training data. Ironically, in our analysis, a lower validation loss represents worse robustness in experimental data.


% Thus, validation loss, in our analysis, is, counterintuitively, a measure of lack of robustness.
% The receptive field problem discussed in Section \ref{section: exp phantom recon} is due to the limited receptive field of ResNet architectures where only $3\times 3$ convolutional layers expand the receptive field that scales linearly with the depth of the network. UNet architectures employ several max-pooling layers, each layer doubling the receptive field of the input tensor, so that a more global receptive field is achieved. We experiment with a UNet-based SBR-Net to evaluate if we can address the axial localization displacement problem in the ResNet-based architecture, and understand how different architectures affect generalizability.

% We find that while the UNet architecture performs better on synthetic test data in terms of depth penetration, it generates many more hallucination artifacts for experimental data, making it harder to judge how well it addresses the receptive field problem. One likely reason for the hallucinations is that this UNet-based model may overfit to synthetic data due to having 34,558,008 parameters, which is 20x more than the ResNet-based architecture (1,732,872), leading to a more brittle behavior when input data is even slightly different than training data, a similar rationale as that in Section \ref{section:unique data pairs}.



\clearpage



\section{Variance stabilization speeds up training convergence}

\begin{figure}[h!]
    \centering
  \includegraphics[width=12cm]{variance.pdf}
  \caption{Variance stabilization speeds up training convergence by a factor of 7. Shading represents standard deviation for 5 training experiments.}
  \label{fig: variance stabilization plot}
\end{figure}

When two independent random variables, $X$ and $Y$  are added, the variance of the sum, $Z$ is  
\begin{eqnarray} \label{eq:var}
    Var[Z] &=& Var[aX] + Var[bY]\\
    &=& a^2Var[X] + b^2Var[Y]. \nonumber
\end{eqnarray}

% For the purposes of this section, we'll take  $a$ and $b$ equal to 1, and we can safely assume that $X$ and $Y$ are independent during the first few epochs. We define $X$ and $Y$ to have variance $\sigma^2$. 

In our ResNet-based CNN architecture, there are many channels with random elements being added, and we can safely assume they are independent during the first few epochs. When this happens, the variance grows rapidly over the 20 ResBlocks as well as with the branch fusion. The consequence is that the forward propagation signal has a significantly large range of values which may saturate and flood the sigmoid function at the end of the network, resulting in less meaningful backpropagation signals that lead to slower convergence to local minima. 

In our case, $a$ and $b$ are equal to 1, and $X$ and $Y$ have the same variance, $\sigma^2$, so the variance of the forward propagation signal doubles after every ResBlock. We initialize the weights according to Kaiming He initialization \cite{he_delving_2015}, $W\sim \mathcal{N}(0,\frac{2}{n_l})$, where $W$ is an element of the CNN layer and $n_l$ is the number of elements in that layer. By the end, the output magnifies the input variance by $2^{21}$ (20 residual connections and one branch fusion). To ensure that the variance of the forward propagation signal does not explode and that we can take advantage of He initialization to its fullest in a ResNet-type architecture, we must ensure that the variance of the forward propagation signal is stabilized, meaning that the variance of the sum of two channels in the network architecture is the same as the variance of the channels that are being summed. From Equation \ref{eq:var}, we see the solution is to divide the sum by $\sqrt{2}$:

\begin{equation}
    Var[Z] = Var\left[\frac{X+Y}{\sqrt{2}}\right] = \frac{Var[X]+Var[Y]}{\sqrt{2}^2}=\frac{\sigma^2+\sigma^2}{2} = \sigma^2.
\end{equation}

Thus, for every part of our network where two channels are being added (i.e. the residual connections and the branch fusion), we simply divide the sum by $\sqrt{2}$. In general, channels being fused together with addition should have the sum divided by the square root of the number of channels being fused together.

We experiment with 4 different network structures: without variance stabilization, with variance stabilization for every residual connection and the branch fusion, variance stabilization at only the branch fusion point with the original ResNet residual connections, and dividing by 2 for all residual connections and the branch fusion. Figure \ref{fig: variance stabilization plot} show the convergence of the training loss over epochs, and we see that the architecture with variance stabilization is optimal. It takes the architecture with variance stabilization 7 times faster to reach the same training loss as it does for the original architecture without variance stabilization. The original architecture and the one where only the branch fusion point has been divided by $\sqrt{2}$ are prone to severely non-optimal model weights due to the large variance of the forward propagation signal combined with warm restarts of the learning rate.

\clearpage


% \subsection{ResNet vs UNet Architecture} \label{section: unet}
% \begin{figure}
%     \centering
%   \includegraphics[width=1\linewidth]{unet exp.pdf}
%   \caption{Comparing CNN architectures: UNet vs ResNet. The top row shows the performance of the UNet (solid) and ResNet (dashed) architectures for synthetic phantoms of different scattering lengths. The images are XZ MIPs of the \SI{72}{\micro\meter} reconstructions.}
%   \label{fig: unet exp}
% \end{figure}

% The receptive field problem discussed in Section \ref{section: exp phantom recon} is due to the limited receptive field of ResNet architectures where only $3\times 3$ convolutional layers expand the receptive field that scales linearly with the depth of the network. UNet architectures employ several max-pooling layers, each layer doubling the receptive field of the input tensor, so that a more global receptive field is achieved. We experiment with a UNet-based SBR-Net to evaluate if we can address the axial localization displacement problem in the ResNet-based architecture.

% We find that while the UNet architecture performs better on synthetic test data in terms of depth penetration, it generates many more hallucination artifacts for experimental data, making it harder to judge how well it addresses the receptive field problem. One likely reason for the hallucinations is that this UNet-based model may overfit to synthetic data due to having 34,558,008 parameters, which is 20x more than the ResNet-based architecture (1,732,872), leading to a more brittle behavior when input data is even slightly different than training data.



\section{Model-based reconstruction}
This section adopts the following notation: bolded uppercase letters denote operators, a superscript of $H$ denotes the Hermitian adjoint of the operator, bolded lowercase letters denote vectors, $\top$ denotes the transpose, and unbolded characters denote scalars. 

The imaging system is a slice-wise shift-invariant model, followed by a cropping operator, $\mathbf{C}$, to account for the finite area of the camera sensor:
\begin{equation} \label{eq:discrete fwdmdl}
    \mathbf{y} = \mathbf{CAx}
\end{equation}
where $\mathbf{x}=[\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_n]$ is the discretized 3D object with $n$ number of axial slices, and $\mathbf{A}=[\mathbf{A}_1,\mathbf{A}_2,...,\mathbf{A}_n]$ is the slice-wise discrete forward model such that $\mathbf{Ax}=\sum_{i=1}^n\mathbf{A}_i\mathbf{x}_i$, projecting all the depth-wise 2D measurements onto the camera sensor plane.

The inverse problem is highly ill-posed due to the 2D to 3D dimension mismatch, so we incorporate sparsity priors in the spatial and gradient domain of the object and solve the following optimization problem:
\begin{equation} \label{eq:optimization1}
    \hat{\mathbf{x}} = \argmin_{\mathbf{x}\geq 0} \frac{1}{2}\| \mathbf{y}-\mathbf{CAx} \|_2^2 + \tau_1\|\mathbf{x}\|_1+ \tau_2\|\mathbf{Dx}\|_1
\end{equation}
where $\mathbf{D}$ is the 3D finite difference operator, and $\tau_1$ and $\tau_2$ are manually tuned non-negative regularization parameters. We use a spatial sparsity prior because our objects are beads or neurons in a volume, where there will be many more zero-valued voxels than nonzero-valued voxels. The 3D total variation prior is widely used for natural objects.  

We solve this optimization problem using the alternating direction method of multipliers (ADMM) \cite{boyd_distributed_2010,xue_single-shot_2020,antipa_diffusercam_2018}. The first step is to perform variable splitting by setting $\mathbf{u} = \mathbf{Ax}$, $\mathbf{v = x}$, $\mathbf{w = Dx}$, $\mathbf{z=x}$, and adding their corresponding constraint sets to the objective. We rewrite the optimization problem as

\begin{argmini}|s|
{\mathbf{x,u,v,w,z}}{\frac{1}{2}\| \mathbf{y-Cu} \|_2^2 + \tau_1\|\mathbf{v}\|_1+ \tau_2\|\mathbf{w}\|_1}
{}{}
\addConstraint{\mathbf{u=Ax}}
\addConstraint{\mathbf{v=x}}
\addConstraint{\mathbf{w=Dx}}
\addConstraint{\mathbf{z=x}}
\addConstraint{\mathbf{z}\geq0}.
\end{argmini}
The next step is to form the augmented Lagrangian to make the optimization unconstrained:
\begin{equation}
\begin{split}
    \mathcal{L}(\{\mathbf{u,v,w,z,x}\},\{\boldsymbol{\kappa,\lambda,\mu,\nu}\}) = \frac{1}{2}\| \mathbf{y-Cu} \|_2^2 &+ \tau_1\|\mathbf{v}\|_1 + \tau_2\|\mathbf{w}\|_1 \\ 
    &+\frac{\rho_1}{2}\| \mathbf{Ax - u} \|_2^2 + \boldsymbol{\kappa}^{\top}(\mathbf{Ax-u}) \\
    &+\frac{\rho_2}{2}\| \mathbf{x - v} \|_2^2 + \boldsymbol{\lambda}^{\top}(\mathbf{x-v}) \\
    &+\frac{\rho_3}{2}\| \mathbf{Dx - w} \|_2^2 + \boldsymbol{\mu}^{\top}(\mathbf{Dx-w}) \\
    &+\frac{\rho_4}{2}\| \mathbf{x - z} \|_2^2 + \boldsymbol{\nu}^{\top}(\mathbf{x-z}) \\
    &+\mathcal{I}_+(\mathbf{z})
\end{split}
\end{equation}
where $\rho_i$ are positive penalty parameters, $(\boldsymbol{\kappa,\lambda,\mu,\nu})$ are the Lagrangian multipliers, or dual variables, and $\mathcal{I}_+(\mathbf{z})$ is the non-negativity barrier function defined as 
\begin{equation}
\mathcal{I}_+(z) = \left\{
        \begin{array}{ll}
            \infty & \quad z < 0 \\
            0 & \quad z \geq 0.
        \end{array}
    \right.
\end{equation}
The dual function is defined as
\begin{equation} \label{eq: dual function}
    g(\boldsymbol{\kappa,\lambda,\mu,\nu}) = \inf_{\mathbf{u,v,w,z,x}} \mathcal{L}(\{\mathbf{u,v,w,z,x}\},\{\boldsymbol{\kappa,\lambda,\mu,\nu}\})
\end{equation}
and solving Eq. \ref{eq:optimization1} is equivalent to solving the \textit{dual problem}:
\begin{equation} \label{eq:dual problem}
    \max_{\boldsymbol{\kappa,\lambda,\mu,\nu}} g(\boldsymbol{\kappa,\lambda,\mu,\nu}).
\end{equation}
The optimization problem is then a saddle point problem solved by iteratively alternating between a minimization step of the primal variables, followed by a maximization step of the dual variables,
\begin{equation}
    \max_{\boldsymbol{\kappa,\lambda,\mu,\nu}} \min_{\mathbf{u,v,w,z,x}} \mathcal{L}(\{\mathbf{u,v,w,z,x}\},\{\boldsymbol{\kappa,\lambda,\mu,\nu}\}).
\end{equation}

We update the dual variables using gradient ascent where the step size is the corresponding penalty parameter. Using calculus, we compute the update steps for the primal variables. The algorithm is as follows:

\begin{equation}
    \begin{aligned}
        \mathbf{u}_{k+1}& = \argmin_{\mathbf{u}}\left\{ \frac{1}{2}\|\mathbf{y-Cu}\|_2^2+\frac{\rho_1}{2}\|\mathbf{Ax}_k-\mathbf{u}\|_2^2+\boldsymbol{\kappa}_k^{\top}(\mathbf{Ax}_k-\mathbf{u}) \right\}  \\
        &=(\mathbf{C}^H\mathbf{C}+\rho_1\mathbf{I})^{-1}(\mathbf{C}^H\mathbf{y}_k+\boldsymbol{\kappa}_k + \rho_1\mathbf{A}\mathbf{x}_k) \\
        \mathbf{v}_{k+1} &= \argmin_{\mathbf{v}}\left\{ \tau_1 \|\mathbf{v}\|_1  +\frac{\rho_2}{2}\|\mathbf{x}_k-\mathbf{v}\|_2^2+\boldsymbol{\lambda}_k^{\top}(\mathbf{x}_k-\mathbf{v})\right\}\\
        &=\argmin_{\mathbf{v}}\left\{ \frac{\tau_1}{\rho_2}\|\mathbf{v}\|_1 + \frac{1}{2}\|\mathbf{x}_k-\mathbf{v}+\frac{\boldsymbol{\lambda}_k}{\rho_2}\|_2^2+ \cancel{\frac{\boldsymbol{\lambda}_k^{\top}\boldsymbol{\lambda}_k}{\rho_2^2}} \right\} \\
        &=\mbox{Prox}_{\frac{\tau_1}{\rho_2}\|\cdot\|_1}\left(\mathbf{x}_k-\frac{\boldsymbol{\lambda}_k}{\rho_2}\right)\\
        &= \mathcal{S}_{\frac{\tau_1}{\rho_2}}\left( \mathbf{x}_k-\frac{\boldsymbol{\lambda}_k}{\rho_2}\right) \\
        \mathbf{w}_{k+1} &= \argmin_{\mathbf{w}} \left\{  \tau_2\|\mathbf{w}\|_1 + \frac{\rho_3}{2}\| \mathbf{D}\mathbf{x}_k-\mathbf{w} \|_2^2+\boldsymbol{\mu}_k^{\top}(\mathbf{D}\mathbf{x}_k-\mathbf{w}) \right\} \\
        &= \argmin_{\mathbf{w}} \left\{  \frac{\tau_2}{\rho_3}\|\mathbf{w}\|_1 +\frac{1}{2}\|\mathbf{D}\mathbf{x}_k-\mathbf{w}+\frac{\boldsymbol{\mu}_k}{\rho_3}\|_2^2 + \cancel{\frac{\boldsymbol{\mu}_k^{\top}\boldsymbol{\mu}_k}{\rho_3^2}} \right\}  \\
        &= \mbox{Prox}_{\frac{\tau_2}{\rho_3}\|\cdot \|_1}\left( \mathbf{D}\mathbf{x}_k-\frac{\boldsymbol{\mu}_k}{\rho_3} \right) \\
        &= \mathcal{S}_{\frac{\tau_2}{\rho_3}}\left( \mathbf{D}\mathbf{x}_k-\frac{\boldsymbol{\mu}_k}{\rho_3} \right)\\
        \mathbf{z}_{k+1} &= \argmin_{\mathbf{z}} \left\{\frac{\rho_4}{2}\|\mathbf{x}_k-\mathbf{z}\|_2^2 + \boldsymbol{\nu}_k^{\top}(\mathbf{x}_k-\mathbf{z}) + \mathcal{I}_+(\mathbf{z})\right\}\\
        &= \argmin_{\mathbf{z}} \left\{\mathcal{I}_+(\mathbf{z}) + \|\mathbf{x}_k-\mathbf{z}+\frac{\boldsymbol{\nu}_k}{\rho_4}\|_2^2 + \cancel{\frac{\boldsymbol{\nu}_k^{\top}\boldsymbol{\nu}_k}{\rho_4}}\right\} \\
        & = \max\left(\mathbf{0},\mathbf{x}_k+\frac{\boldsymbol{\nu}_k}{\rho_4}\right)\\
        \mathbf{x}_{k+1} &= \argmin_{\mathbf{x}} \left\{  
        \frac{\rho_1}{2} \|\mathbf{Ax}-\mathbf{u}_{k+1} \|_2^2+ \boldsymbol{\kappa}_{k+1}^{\top}(\mathbf{Ax}-\mathbf{u}_{k+1}) +\frac{\rho_2}{2} \|\mathbf{x}-\mathbf{v}_{k+1} \|_2^2+ \boldsymbol{\lambda}_{k+1}^{\top}(\mathbf{x}-\mathbf{v}_{k+1}) \right.{} \\
        &\quad \left. {} +\frac{\rho_3}{2} \|\mathbf{Dx}-\mathbf{w}_{k+1} \|_2^2+ \boldsymbol{\mu}_{k+1}^{\top}(\mathbf{Dx}-\mathbf{w}_{k+1}) + \frac{\rho_4}{2} \|\mathbf{x}-\mathbf{z}_{k+1} \|_2^2+ \boldsymbol{\nu}_{k+1}^{\top}(\mathbf{x}-\mathbf{z}_{k+1}) \right\} \\
        &= \frac{\mathbf{A}^{H}(\rho_1\mathbf{u}_{k+1}-\boldsymbol{\kappa}_k) + \mathbf{D}^{H}(\rho_3\mathbf{w}_{k+1}-\boldsymbol{\mu}_k)+\rho_2\mathbf{v}_{k+1}+\rho_4\mathbf{z}_{k+1}+\boldsymbol{\nu}_k-\boldsymbol{\lambda}_k    }{ \rho_1 \mathbf{A}^{H}\mathbf{A}+\rho_2\mathbf{I}+\rho_3\mathbf{D}^{H}\mathbf{D}+\rho_4\mathbf{I}    }\\
        \boldsymbol{\kappa}_{k+1} &= \boldsymbol{\kappa}_k + \rho_1(\mathbf{A}\mathbf{x}_{k+1}-\mathbf{u}_{k+1} )\\
        \boldsymbol{\lambda}_{k+1} &= \boldsymbol{\lambda}_k + \rho_2(\mathbf{x}_{k+1}-\mathbf{v}_{k+1})  \\
        \boldsymbol{\mu}_{k+1} &= \boldsymbol{\mu}_k + \rho_3(\mathbf{D}\mathbf{x}_{k+1}-\mathbf{w}_{k+1} ) \\
        \boldsymbol{\nu}_{k+1} &=\boldsymbol{\nu}_k + \rho_4(\mathbf{x}_{k+1}-\mathbf{z}_{k+1} )\\
    \end{aligned}
\end{equation}
where $k$ is the iteration step and $\mbox{Prox}_{\alpha f}$ refers to the proximal operator of function $\alpha f$. In our case, $\alpha f$, is the $l1$ norm with regularization parameter, $\alpha$. This is equal to the soft-thresholding function, $\mathcal{S}_{\alpha}$.

The adjoint of the cropping operator is zero-padding outside the sensor region, and $\mathbf{C}^H\mathbf{C}$ may be interpreted as a binary mask with a rectangle of ones in the center corresponding to the size of the sensor, so that the matrix is diagonalized and easily invertible. The finite difference operator, $\mathbf{D}$, is implemented using the forward difference operator (i.e. $x_{i+1} - x_i$), and its adjoint is the backwards difference operator, (i.e. $x_{i-1} - x_i$). We assume a circular boundary for simplicity. $\mathbf{D}^H\mathbf{D}$ may be implemented by using a convolutional kernel to carry out the $\mathbf{D}$ and $\mathbf{D}^H$ operator. The forward model operator, $\mathbf{A}$, the 3D finite difference operator $\mathbf{D}$, and their adjoints  were carried out in the Fourier domain for computational efficiency and ease of invertibility when the matrix is diagonalized. Our implementation may be found in our open source GitHub \cite{github}.

We find the optimal regularization parameters through grid search and visually inspecting the most accurate reconstructions across all three scattering phantoms. The values are $\tau_1=0.01$, $\tau_2=0.005$, and we keep the penalty parameters $\rho_1 = \rho_2 = \rho_3 = \rho_4 = 1$. The algorithm is carried out in MATLAB R2019b using a  Intel Xeon E5-1620 v4 3.5GHz CPU and takes approximately 1.45 hours for one volume.

% \section*{Data availability}
% The neural network and the data set used in this work are available at \url{fill this in}.





% REFERENCES
\newpage
\bibliography{sbr}
\bibliographystyle{ieeetr}
\end{document}