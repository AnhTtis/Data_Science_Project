
@report{zhang_rapid_2022,
	title = {Rapid deep widefield neuron finder driven by virtual calcium imaging data},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.01.25.474600v1},
	abstract = {Widefield microscope provides optical access to multi-millimeter fields of view and thousands of neurons in mammalian brains at video rate. However, calcium imaging at cellular resolution has been mostly contaminated by tissue scattering and background signals, making neuronal activities extraction challenging and time-consuming. Here we present a deep widefield neuron finder ({DeepWonder}), which is fueled by simulated calcium recordings but effectively works on experimental data with an order of magnitude faster speed and improved inference accuracy than traditional approaches. The efficient {DeepWonder} accomplished fifty-fold signal-to-background ratio enhancement in processing terabytes-scale cortex-wide recording, with over 14000 neurons extracted in 17 hours in workstation-grade computing resources compared to nearly week-long processing time with previous methods. {DeepWonder} circumvented the numerous computational resources and could serve as a guideline to massive data processing in widefield neuronal imaging.},
	pages = {2022.01.25.474600},
	institution = {{bioRxiv}},
	author = {Zhang, Yuanlong and Zhang, Guoxun and Han, Xiaofei and Wu, Jiamin and Li, Ziwei and Li, Xinyang and Xiao, Guihua and Xie, Hao and Fang, Lu and Dai, Qionghai},
	urldate = {2022-03-07},
	date = {2022-01-28},
	langid = {english},
	doi = {10.1101/2022.01.25.474600},
	note = {Section: New Results
Type: article},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\U6QP2G9Z\\Zhang et al. - 2022 - Rapid deep widefield neuron finder driven by virtu.pdf:application/pdf;Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\6XSWXXZ4\\2022.01.25.html:text/html},
}

@article{xue_deep-learning-augmented_2022,
	title = {Deep-learning-augmented computational miniature mesoscope},
	volume = {9},
	rights = {\&\#169; 2022 Optica Publishing Group},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-9-9-1009},
	doi = {10.1364/OPTICA.464700},
	abstract = {Fluorescence microscopy is essential to study biological structures and dynamics. However, existing systems suffer from a trade-off between field of view ({FOV}), resolution, and system complexity, and thus cannot fulfill the emerging need for miniaturized platforms providing micron-scale resolution across centimeter-scale {FOVs}. To overcome this challenge, we developed a computational miniature mesoscope ({CM}2) that exploits a computational imaging strategy to enable single-shot, 3D high-resolution imaging across a wide {FOV} in a miniaturized platform. Here, we present {CM}2 V2, which significantly advances both the hardware and computation. We complement the 3×3 microlens array with a hybrid emission filter that improves the imaging contrast by 5×, and design a 3D-printed free-form collimator for the {LED} illuminator that improves the excitation efficiency by 3×. To enable high-resolution reconstruction across a large volume, we develop an accurate and efficient 3D linear shift-variant ({LSV}) model to characterize spatially varying aberrations. We then train a multimodule deep learning model called {CM}2Net, using only the 3D-{LSV} simulator. We quantify the detection performance and localization accuracy of {CM}2Net to reconstruct fluorescent emitters under different conditions in simulation. We then show that {CM}2Net generalizes well to experiments and achieves accurate 3D reconstruction across a ∼7-mm {FOV} and 800-µm depth, and provides ∼6-µm lateral and ∼25-µm axial resolution. This provides an ∼8× better axial resolution and ∼1400× faster speed compared to the previous model-based algorithm. We anticipate this simple, low-cost computational miniature imaging system will be useful for many large-scale 3D fluorescence imaging applications.},
	pages = {1009--1021},
	number = {9},
	journaltitle = {Optica},
	shortjournal = {Optica, {OPTICA}},
	author = {Xue, Yujia and Yang, Qianwan and Hu, Guorong and Guo, Kehan and Tian, Lei},
	urldate = {2022-12-20},
	date = {2022-09-20},
	note = {Publisher: Optica Publishing Group},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\QK8VLIE2\\Xue et al. - 2022 - Deep-learning-augmented computational miniature me.pdf:application/pdf},
}

@article{xue_single-shot_2020,
	title = {Single-shot 3D wide-field fluorescence imaging with a Computational Miniature Mesoscope},
	volume = {6},
	url = {https://www.science.org/doi/full/10.1126/sciadv.abb7508},
	doi = {10.1126/sciadv.abb7508},
	abstract = {Fluorescence microscopes are indispensable to biology and neuroscience. The need for recording in freely behaving animals has further driven the development in miniaturized microscopes (miniscopes). However, conventional microscopes/miniscopes are inherently constrained by their limited space-bandwidth product, shallow depth of field ({DOF}), and inability to resolve three-dimensional (3D) distributed emitters. Here, we present a Computational Miniature Mesoscope ({CM}2) that overcomes these bottlenecks and enables single-shot 3D imaging across an 8 mm by 7 mm field of view and 2.5-mm {DOF}, achieving 7-μm lateral resolution and better than 200-μm axial resolution. The {CM}2 features a compact lightweight design that integrates a microlens array for imaging and a light-emitting diode array for excitation. Its expanded imaging capability is enabled by computational imaging that augments the optics by algorithms. We experimentally validate the mesoscopic imaging capability on 3D fluorescent samples. We further quantify the effects of scattering and background fluorescence on phantom experiments.},
	pages = {eabb7508},
	number = {43},
	journaltitle = {Science Advances},
	author = {Xue, Yujia and Davison, Ian G. and Boas, David A. and Tian, Lei},
	urldate = {2022-12-20},
	date = {2020-10-21},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\DC2RB7WB\\Xue et al. - 2020 - Single-shot 3D wide-field fluorescence imaging wit.pdf:application/pdf},
}

@misc{greene_edof-miniscope_2022,
	title = {{EDoF}-Miniscope: pupil engineering for extended depth-of-field imaging in a fluorescence miniscope},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.08.05.502947v1},
	doi = {10.1101/2022.08.05.502947},
	shorttitle = {{EDoF}-Miniscope},
	abstract = {Extended depth of field ({EDoF}) microscopy has emerged as a powerful solution to greatly increase the access into neuronal populations in table-top imaging platforms. Here, we present {EDoF}-Miniscope, which integrates an optimized thin and lightweight binary diffractive optical element ({DOE}) onto the gradient refractive index ({GRIN}) lens of a head-mounted fluorescence miniature microscope, i.e. “miniscope”. We achieve an alignment accuracy of 70 μm to allow a 2.8X depth-of-field extension between the twin foci. We optimize the phase profile across the whole back aperture through a genetic algorithm that considers the primary {GRIN} lens aberrations, optical property of the submersion media, and axial intensity loss from tissue scattering in a Fourier optics forward model. Compared to other computational miniscopes, our {EDoF}-Miniscope produces high-contrast signals that can be recovered by a simple algorithm and can successfully capture volumetrically distributed neuronal signals without significantly compromising the speed, signal-to-noise, signal-to-background, and maintain a comparable 0.9-μm lateral spatial resolution and the size and weight of the miniature platform. We demonstrate the robustness of {EDoF}-Miniscope against scattering by characterizing its performance in 5-μm and 10-μm beads embedded in scattering phantoms. We demonstrate that {EDoF}-Miniscope facilitates deeper interrogations of neuronal populations in a 100-μm thick mouse brain sample, as well as vessels in a mouse brain. Built from off-the-shelf components augmented by a customizable {DOE}, we expect that this low-cost {EDoF}-Miniscope may find utility in a wide range of neural recording applications.},
	publisher = {{bioRxiv}},
	author = {Greene, Joseph and Xue, Yujia and Alido, Jeffrey and Matlock, Alex and Hu, Guorong and Kiliç, Kivilcim and Davison, Ian and Tian, Lei},
	urldate = {2022-12-20},
	date = {2022-08-05},
	langid = {english},
	note = {Pages: 2022.08.05.502947
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\54I6S4AW\\Greene et al. - 2022 - EDoF-Miniscope pupil engineering for extended dep.pdf:application/pdf},
}

@article{yanny_miniscope3d_2020,
	title = {Miniscope3D: optimized single-shot miniature 3D fluorescence microscopy},
	volume = {9},
	rights = {2020 The Author(s)},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-020-00403-7},
	doi = {10.1038/s41377-020-00403-7},
	shorttitle = {Miniscope3D},
	abstract = {Miniature fluorescence microscopes are a standard tool in systems biology. However, widefield miniature microscopes capture only 2D information, and modifications that enable 3D capabilities increase the size and weight and have poor resolution outside a narrow depth range. Here, we achieve the 3D capability by replacing the tube lens of a conventional 2D Miniscope with an optimized multifocal phase mask at the objective’s aperture stop. Placing the phase mask at the aperture stop significantly reduces the size of the device, and varying the focal lengths enables a uniform resolution across a wide depth range. The phase mask encodes the 3D fluorescence intensity into a single 2D measurement, and the 3D volume is recovered by solving a sparsity-constrained inverse problem. We provide methods for designing and fabricating the phase mask and an efficient forward model that accounts for the field-varying aberrations in miniature objectives. We demonstrate a prototype that is 17 mm tall and weighs 2.5 grams, achieving 2.76 μm lateral, and 15 μm axial resolution across most of the 900 × 700 × 390 μm3 volume at 40 volumes per second. The performance is validated experimentally on resolution targets, dynamic biological samples, and mouse brain tissue. Compared with existing miniature single-shot volume-capture implementations, our system is smaller and lighter and achieves a more than 2× better lateral and axial resolution throughout a 10× larger usable depth range. Our microscope design provides single-shot 3D imaging for applications where a compact platform matters, such as volumetric neural imaging in freely moving animals and 3D motion studies of dynamic samples in incubators and lab-on-a-chip devices.},
	pages = {171},
	number = {1},
	journaltitle = {Light: Science \& Applications},
	shortjournal = {Light Sci Appl},
	author = {Yanny, Kyrollos and Antipa, Nick and Liberti, William and Dehaeck, Sam and Monakhova, Kristina and Liu, Fanglin Linda and Shen, Konlin and Ng, Ren and Waller, Laura},
	urldate = {2022-12-20},
	date = {2020-10-02},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Microscopy, Imaging and sensing},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\WVRHLUES\\Yanny et al. - 2020 - Miniscope3D optimized single-shot miniature 3D fl.pdf:application/pdf},
}

@article{adams_vivo_2022,
	title = {In vivo lensless microscopy via a phase mask generating diffraction patterns with high-contrast contours},
	volume = {6},
	rights = {2022 The Author(s)},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00851-z},
	doi = {10.1038/s41551-022-00851-z},
	abstract = {The simple and compact optics of lensless microscopes and the associated computational algorithms allow for large fields of view and the refocusing of the captured images. However, existing lensless techniques cannot accurately reconstruct the typical low-contrast images of optically dense biological tissue. Here we show that lensless imaging of tissue in vivo can be achieved via an optical phase mask designed to create a point spread function consisting of high-contrast contours with a broad spectrum of spatial frequencies. We built a prototype lensless microscope incorporating the ‘contour’ phase mask and used it to image calcium dynamics in the cortex of live mice (over a field of view of about 16 mm2) and in freely moving Hydra vulgaris, as well as microvasculature in the oral mucosa of volunteers. The low cost, small form factor and computational refocusing capability of in vivo lensless microscopy may open it up to clinical uses, especially for imaging difficult-to-reach areas of the body.},
	pages = {617--628},
	number = {5},
	journaltitle = {Nature Biomedical Engineering},
	shortjournal = {Nat. Biomed. Eng},
	author = {Adams, Jesse K. and Yan, Dong and Wu, Jimin and Boominathan, Vivek and Gao, Sibo and Rodriguez, Alex V. and Kim, Soonyoung and Carns, Jennifer and Richards-Kortum, Rebecca and Kemere, Caleb and Veeraraghavan, Ashok and Robinson, Jacob T.},
	urldate = {2022-12-20},
	date = {2022-05},
	langid = {english},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Optical imaging, 3-D reconstruction, Ca2+ imaging, Fluorescence imaging, Applied optics},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\6PR4Z6EZ\\Adams et al. - 2022 - In vivo lensless microscopy via a phase mask gener.pdf:application/pdf},
}

@misc{wijethilake_deep2_2022,
	title = {{DEEP}\${\textasciicircum}2\$: Deep Learning Powered De-scattering with Excitation Patterning},
	url = {http://arxiv.org/abs/2210.10892},
	doi = {10.48550/arXiv.2210.10892},
	shorttitle = {{DEEP}\${\textasciicircum}2\$},
	abstract = {Limited throughput is a key challenge in in-vivo deep-tissue imaging using nonlinear optical microscopy. Point scanning multiphoton microscopy, the current gold standard, is slow especially compared to the wide-field imaging modalities used for optically cleared or thin specimens. We recently introduced 'De-scattering with Excitation Patterning or {DEEP}', as a widefield alternative to point-scanning geometries. Using patterned multiphoton excitation, {DEEP} encodes spatial information inside tissue before scattering. However, to de-scatter at typical depths, hundreds of such patterned excitations are needed. In this work, we present {DEEP}\${\textasciicircum}2\$, a deep learning based model, that can de-scatter images from just tens of patterned excitations instead of hundreds. Consequently, we improve {DEEP}'s throughput by almost an order of magnitude. We demonstrate our method in multiple numerical and physical experiments including in-vivo cortical vasculature imaging up to four scattering lengths deep, in alive mice.},
	number = {{arXiv}:2210.10892},
	publisher = {{arXiv}},
	author = {Wijethilake, Navodini and Anandakumar, Mithunjha and Zheng, Cheng and So, Peter T. C. and Yildirim, Murat and Wadduwage, Dushan N.},
	urldate = {2022-12-20},
	date = {2022-10-21},
	eprinttype = {arxiv},
	eprint = {2210.10892 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\PSKZ6MAQ\\Wijethilake et al. - 2022 - DEEP\$^2\$ Deep Learning Powered De-scattering with.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\DV3AA9UA\\2210.html:text/html},
}

@inreference{noauthor_value_2021,
	title = {Value noise},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Value_noise&oldid=1024311499},
	abstract = {Value noise is a type of noise commonly used as a procedural texture primitive in computer graphics. It is conceptually different from, and often confused with gradient noise, examples of which are Perlin noise and Simplex noise. This method consists of the creation of a lattice of points which are assigned random values. The noise function then returns the interpolated number based on the values of the surrounding lattice points.
For many applications, multiple octaves of this noise can be generated and then summed together, just as can be done with Perlin noise and Simplex noise, in order to create a form of fractal noise.},
	booktitle = {Wikipedia},
	urldate = {2022-12-20},
	date = {2021-05-21},
	langid = {english},
	note = {Page Version {ID}: 1024311499},
	file = {Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\D9VLPTHS\\Value_noise.html:text/html},
}

@article{foi_practical_2008,
	title = {Practical Poissonian-Gaussian noise modeling and fitting for single-image raw-data},
	volume = {17},
	issn = {1057-7149},
	doi = {10.1109/TIP.2008.2001399},
	abstract = {We present a simple and usable noise model for the raw-data of digital imaging sensors. This signal-dependent noise model, which gives the pointwise standard-deviation of the noise as a function of the expectation of the pixel raw-data output, is composed of a Poissonian part, modeling the photon sensing, and Gaussian part, for the remaining stationary disturbances in the output data. We further explicitly take into account the clipping of the data (over- and under-exposure), faithfully reproducing the nonlinear response of the sensor. We propose an algorithm for the fully automatic estimation of the model parameters given a single noisy image. Experiments with synthetic images and with real raw-data from various sensors prove the practical applicability of the method and the accuracy of the proposed model.},
	pages = {1737--1754},
	number = {10},
	journaltitle = {{IEEE} transactions on image processing: a publication of the {IEEE} Signal Processing Society},
	shortjournal = {{IEEE} Trans Image Process},
	author = {Foi, Alessandro and Trimeche, Mejdi and Katkovnik, Vladimir and Egiazarian, Karen},
	date = {2008-10},
	pmid = {18784024},
	keywords = {Algorithms, Image Enhancement, Reproducibility of Results, Sensitivity and Specificity, Computer Simulation, Data Interpretation, Statistical, Image Interpretation, Computer-Assisted, Models, Statistical},
	file = {Submitted Version:C\:\\Users\\jeffr\\Zotero\\storage\\77LCN7JB\\Foi et al. - 2008 - Practical Poissonian-Gaussian noise modeling and f.pdf:application/pdf},
}

@online{noauthor_mie_nodate,
	title = {Mie Scattering Calculator},
	url = {https://omlc.org/calc/mie_calc.html},
	urldate = {2023-01-07},
	file = {Mie Scattering Calculator:C\:\\Users\\jeffr\\Zotero\\storage\\DZ9P9CZC\\mie_calc.html:text/html},
}

@misc{zhang_imaging_2022,
	title = {Imaging through the Atmosphere using Turbulence Mitigation Transformer},
	url = {http://arxiv.org/abs/2207.06465},
	doi = {10.48550/arXiv.2207.06465},
	abstract = {Restoring images distorted by atmospheric turbulence is a long-standing problem due to the spatially varying nature of the distortion, nonlinearity of the image formation process, and scarcity of training and testing data. Existing methods often have strong statistical assumptions on the distortion model which in many cases will lead to a limited performance in real-world scenarios as they do not generalize. To overcome the challenge, this paper presents an end-to-end physics-driven approach that is efficient and can generalize to real-world turbulence. On the data synthesis front, we significantly increase the image resolution that can be handled by the {SOTA} turbulence simulator by approximating the random field via wide-sense stationarity. The new data synthesis process enables the generation of large-scale multi-level turbulence and ground truth pairs for training. On the network design front, we propose the turbulence mitigation transformer ({TMT}), a two stage U-Net shaped multi-frame restoration network which has a noval efficient self-attention mechanism named temporal channel joint attention ({TCJA}). We also introduce a new training scheme that is enabled by the new simulator, and we design new transformer units to reduce the memory consumption. Experimental results on both static and dynamic scenes are promising, including various real turbulence scenarios.},
	number = {{arXiv}:2207.06465},
	publisher = {{arXiv}},
	author = {Zhang, Xingguang and Mao, Zhiyuan and Chimitt, Nicholas and Chan, Stanley H.},
	urldate = {2023-01-08},
	date = {2022-07-13},
	eprinttype = {arxiv},
	eprint = {2207.06465 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\XBW48HVE\\Zhang et al. - 2022 - Imaging through the Atmosphere using Turbulence Mi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\FH5Y2V5K\\2207.html:text/html},
}

@article{kobayashi_wide_2019,
	title = {Wide and Deep Imaging of Neuronal Activities by a Wearable {NeuroImager} Reveals Premotor Activity in the Whole Motor Cortex},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-44146-x},
	doi = {10.1038/s41598-019-44146-x},
	abstract = {Wearable technologies for functional whole brain imaging in freely moving animals would advance our understanding of cognitive processing and adaptive behavior. Fluorescence imaging can visualize the activity of individual neurons in real time, but conventional microscopes have limited sample coverage in both the width and depth of view. Here we developed a novel head-mounted laser camera ({HLC}) with macro and deep-focus lenses that enable fluorescence imaging at cellular resolution for comprehensive imaging in mice expressing a layer- and cell type-specific calcium probe. We visualized orientation selectivity in individual excitatory neurons across the whole visual cortex of one hemisphere, and cell assembly expressing the premotor activity that precedes voluntary movement across the motor cortex of both hemispheres. Including options for multiplex and wireless interfaces, our wearable, wide- and deep-imaging {HLC} technology could enable simple and economical mapping of neuronal populations underlying cognition and behavior.},
	pages = {8366},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Kobayashi, Takuma and Islam, Tanvir and Sato, Masaaki and Ohkura, Masamichi and Nakai, Junichi and Hayashi, Yasunori and Okamoto, Hitoshi},
	urldate = {2023-01-08},
	date = {2019-06-10},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Fluorescence imaging, Motion detection, Motor cortex},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\B3JDC4LM\\Kobayashi et al. - 2019 - Wide and Deep Imaging of Neuronal Activities by a .pdf:application/pdf},
}

@misc{he_delving_2015,
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
	url = {http://arxiv.org/abs/1502.01852},
	shorttitle = {Delving Deep into Rectifiers},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit ({PReLU}) that generalizes the traditional rectified unit. {PReLU} improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our {PReLU} networks ({PReLU}-nets), we achieve 4.94\% top-5 test error on the {ImageNet} 2012 classification dataset. This is a 26\% relative improvement over the {ILSVRC} 2014 winner ({GoogLeNet}, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	number = {{arXiv}:1502.01852},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2023-01-07},
	date = {2015-02-06},
	eprinttype = {arxiv},
	eprint = {1502.01852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\2ZZ78NFS\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\5YWK53CS\\1502.html:text/html},
}

@article{li_deep_2018,
	title = {Deep speckle correlation: a deep learning approach toward scalable imaging through scattering media},
	volume = {5},
	rights = {© 2018 Optical Society of America},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-5-10-1181},
	doi = {10.1364/OPTICA.5.001181},
	shorttitle = {Deep speckle correlation},
	abstract = {Imaging through scattering is an important yet challenging problem. Tremendous progress has been made by exploiting the deterministic input\&\#x2013;output \&\#x201C;transmission matrix\&\#x201D; for a fixed medium. However, this \&\#x201C;one-to-one\&\#x201D; mapping is highly susceptible to speckle decorrelations \&\#x2013; small perturbations to the scattering medium lead to model errors and severe degradation of the imaging performance. Our goal here is to develop a new framework that is highly scalable to both medium perturbations and measurement requirement. To do so, we propose a statistical \&\#x201C;one-to-all\&\#x201D; deep learning ({DL}) technique that encapsulates a wide range of statistical variations for the model to be resilient to speckle decorrelations. Specifically, we develop a convolutional neural network ({CNN}) that is able to learn the statistical information contained in the speckle intensity patterns captured on a set of diffusers having the same macroscopic parameter. We then show for the first time, to the best of our knowledge, that the trained {CNN} is able to generalize and make high-quality object predictions through an entirely different set of diffusers of the same class. Our work paves the way to a highly scalable {DL} approach for imaging through scattering media.},
	pages = {1181--1190},
	number = {10},
	journaltitle = {Optica},
	shortjournal = {Optica, {OPTICA}},
	author = {Li, Yunzhe and Xue, Yujia and Tian, Lei},
	urldate = {2023-01-09},
	date = {2018-10-20},
	note = {Publisher: Optica Publishing Group},
	keywords = {Neural networks, Scattering media, Multiple scattering, Absorption coefficient, Light scattering, Speckle patterns},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\78S77N35\\Li et al. - 2018 - Deep speckle correlation a deep learning approach.pdf:application/pdf},
}

@online{262588213843476_2d_nodate,
	title = {2D and 3D Perlin Noise in {MATLAB}},
	url = {https://gist.github.com/OrganicIrradiation/2a927ab7f9cfeb1bff78},
	abstract = {2D and 3D Perlin Noise in {MATLAB}. {GitHub} Gist: instantly share code, notes, and snippets.},
	titleaddon = {Gist},
	author = {262588213843476},
	urldate = {2023-01-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\Y89IVVLZ\\2a927ab7f9cfeb1bff78.html:text/html},
}

@online{noauthor_value_nodate,
	title = {Value Noise and Procedural Patterns: Part 1},
	url = {https://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/procedural-patterns-noise-part-1/creating-simple-1D-noise.html},
	urldate = {2023-01-10},
	file = {Value Noise and Procedural Patterns\: Part 1:C\:\\Users\\jeffr\\Zotero\\storage\\2SLJK434\\creating-simple-1D-noise.html:text/html},
}

@article{antipa_diffusercam_2018,
	title = {{DiffuserCam}: lensless single-exposure 3D imaging},
	volume = {5},
	rights = {© 2017 Optical Society of America},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-5-1-1},
	doi = {10.1364/OPTICA.5.000001},
	shorttitle = {{DiffuserCam}},
	abstract = {We demonstrate a compact, easy-to-build computational camera for single-shot three-dimensional (3D) imaging. Our lensless system consists solely of a diffuser placed in front of an image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D reconstruction grid is chosen to match the experimentally measured two-point optical resolution, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide a new theory for analyzing resolution in such systems.},
	pages = {1--9},
	number = {1},
	journaltitle = {Optica},
	shortjournal = {Optica, {OPTICA}},
	author = {Antipa, Nick and Kuo, Grace and Heckel, Reinhard and Mildenhall, Ben and Bostan, Emrah and Ng, Ren and Waller, Laura},
	urldate = {2023-01-10},
	date = {2018-01-20},
	note = {Publisher: Optica Publishing Group},
	keywords = {Image quality, Image sensors, Optical systems, Plenoptic imaging, Systems design, Three dimensional imaging},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\ZPEZJNKK\\Antipa et al. - 2018 - DiffuserCam lensless single-exposure 3D imaging.pdf:application/pdf},
}

@article{kauvar_cortical_2020,
	title = {Cortical Observation by Synchronous Multifocal Optical Sampling Reveals Widespread Population Encoding of Actions},
	volume = {107},
	issn = {0896-6273},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7687350/},
	doi = {10.1016/j.neuron.2020.04.023},
	abstract = {To advance the measurement of distributed neuronal population representations of targeted motor actions on single trials, we developed an optical method ({COSMOS}) for tracking neural activity in a largely uncharacterized spatiotemporal regime. {COSMOS} allowed simultaneous recording of neural dynamics at {\textasciitilde}30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice during a three-option lick-to-target task. We identified spatially distributed neuronal population representations spanning the dorsal cortex that precisely encoded ongoing motor actions on single trials. Neuronal correlations measured at video rate using unaveraged, whole-session data had localized spatial structure, whereas trial-averaged data exhibited widespread correlations. Separable modes of neural activity encoded history-guided motor plans, with similar population dynamics in individual areas throughout cortex. These initial experiments illustrate how {COSMOS} enables investigation of large-scale cortical dynamics and that information about motor actions is widely shared between areas, potentially underlying distributed computations., Kauvar, Machado, et al. have developed a new method, {COSMOS}, to simultaneously record neural dynamics at {\textasciitilde}30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice. With {COSMOS}, they observe cortex-spanning population encoding of actions during a three-option lick-to-target task.},
	pages = {351--367.e19},
	number = {2},
	journaltitle = {Neuron},
	shortjournal = {Neuron},
	author = {Kauvar, Isaac V. and Machado, Timothy A. and Yuen, Elle and Kochalka, John and Choi, Minseung and Allen, William E. and Wetzstein, Gordon and Deisseroth, Karl},
	urldate = {2023-01-19},
	date = {2020-07-22},
	pmid = {32433908},
	pmcid = {PMC7687350},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\GKUJE84E\\Kauvar et al. - 2020 - Cortical Observation by Synchronous Multifocal Opt.pdf:application/pdf},
}

@article{pegard_compressive_2016,
	title = {Compressive light-field microscopy for 3D neural activity recording},
	volume = {3},
	rights = {© 2016 Optical Society of America},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-3-5-517},
	doi = {10.1364/OPTICA.3.000517},
	abstract = {Understanding the mechanisms of perception, cognition, and behavior requires instruments that are capable of recording and controlling the electrical activity of many neurons simultaneously and at high speeds. All-optical approaches are particularly promising since they are minimally invasive and potentially scalable to experiments interrogating thousands or millions of neurons. Conventional light-field microscopy provides a single-shot 3D fluorescence capture method with good light efficiency and fast speed, but suffers from low spatial resolution and significant image degradation due to scattering in deep layers of brain tissue. Here, we propose a new compressive light-field microscopy method to address both problems, offering a path toward measurement of individual neuron activity across large volumes of tissue. The technique relies on spatial and temporal sparsity of fluorescence signals, allowing one to identify and localize each neuron in a 3D volume, with scattering and aberration effects naturally included and without ever reconstructing a volume image. Experimental results on live zebrafish track the activity of an estimated 800+ neural structures at 100 Hz sampling rate.},
	pages = {517--524},
	number = {5},
	journaltitle = {Optica},
	shortjournal = {Optica, {OPTICA}},
	author = {Pégard, Nicolas C. and Liu, Hsiou-Yuan and Antipa, Nick and Gerlock, Maximillian and Adesnik, Hillel and Waller, Laura},
	urldate = {2023-01-19},
	date = {2016-05-20},
	note = {Publisher: Optica Publishing Group},
	keywords = {Multiphoton microscopy, Multiple scattering, Three dimensional imaging, Light sheet microscopy, Spatial resolution, Three dimensional microscopy},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\NTF24U25\\Pégard et al. - 2016 - Compressive light-field microscopy for 3D neural a.pdf:application/pdf},
}

@misc{github,
	title = {https://github.com/bu-cisl/Computational-Miniature-Mesoscope-{CM}2},
	url = {https://github.com/bu-cisl/Computational-Miniature-Mesoscope-CM2},
}

@article{mengual_turbiscan_1999,
	title = {{TURBISCAN} {MA} 2000: multiple light scattering measurement for concentrated emulsion and suspension instability analysis},
	volume = {50},
	issn = {0039-9140},
	url = {https://www.sciencedirect.com/science/article/pii/S0039914099001290},
	doi = {10.1016/S0039-9140(99)00129-0},
	shorttitle = {{TURBISCAN} {MA} 2000},
	abstract = {Emulsion or suspension destabilisation often results from coalescence or particle aggregation (flocculation) leading to particle migration (creaming or sedimentation). Creaming and sedimentation are often considered as reversible, while coalescence and flocculation spell disaster for the formulator. Thus, it is of prime importance to detect coalescence or cluster formation at an early stage to shorten the ageing tests and to improve the formulations. This work mainly concerns the independent and anisotropic scattering of light from an emulsion or suspension in a cylindrical glass measurement cell, in relation with the optical analyser {TURBISCAN} {MA} 2000. The propagation of light through a concentrated dispersion can be used to characterise the system physico-chemical stability. Indeed, photons undergo many scattering events in an optically thick dispersion before escaping the medium and entering a receiver aperture. Multiple scattering thus contributes significantly to the transmitted and backscattered flux measured by {TURBISCAN} {MA} 2000. We present statistical models and numerical simulations for the radiative transfer in a suspension (plane or cylindrical measurement cells) only involving the photon mean path length, the asymmetry factor and the geometry of the light receivers. We further have developed an imaging method with high grey level resolution for the visualisation and the analysis of the surface flux in the backscattered spot light. We compare the results from physical models and numerical simulations with the experiments performed with the imaging method and the optical analyser {TURBISCAN} {MA} 2000 for latex beads suspensions (variable size and particle volume fraction). We then present a few examples of concentrated emulsion and suspension instability analysis with {TURBISCAN} 2000. It is shown that the instrument is able to characterise particle or aggregate size variation and particle/aggregate migration and to detect these phenomena much more earlier than the operator’s naked eye, especially for concentrated and optically thick media.},
	pages = {445--456},
	number = {2},
	journaltitle = {Talanta},
	shortjournal = {Talanta},
	author = {Mengual, O. and Meunier, G. and Cayré, I. and Puech, K. and Snabre, P.},
	urldate = {2023-01-19},
	date = {1999-09-13},
	langid = {english},
	keywords = {Characterisation, Concentrated dispersion, Instability, Multiple light scattering},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\YNCWZMSC\\Mengual et al. - 1999 - TURBISCAN MA 2000 multiple light scattering measu.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\225AERWH\\S0039914099001290.html:text/html},
}

@misc{loshchilov_sgdr_2017,
	title = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
	url = {http://arxiv.org/abs/1608.03983},
	doi = {10.48550/arXiv.1608.03983},
	shorttitle = {{SGDR}},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the {CIFAR}-10 and {CIFAR}-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of {EEG} recordings and on a downsampled version of the {ImageNet} dataset. Our source code is available at https://github.com/loshchil/{SGDR}},
	number = {{arXiv}:1608.03983},
	publisher = {{arXiv}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2023-01-19},
	date = {2017-05-03},
	eprinttype = {arxiv},
	eprint = {1608.03983 [cs, math]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\KMJR98F6\\Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\EVFPXTAC\\1608.html:text/html},
}

@article{boyd_distributed_2010,
	title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	pages = {1--122},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Boyd, Stephen},
	urldate = {2023-01-19},
	date = {2010},
	langid = {english},
	file = {Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf:C\:\\Users\\jeffr\\Zotero\\storage\\DR77GM6C\\Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf:application/pdf},
}
