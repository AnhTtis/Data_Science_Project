
@article{cheng_development_2019,
	title = {Development of a beam propagation method to simulate the point spread function degradation in scattering media},
	volume = {44},
	issn = {0146-9592},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6889808/},
	doi = {10.1364/OL.44.004989},
	abstract = {Scattering is one of the main issues that limit the imaging depth in deep tissue optical imaging. To characterize the role of scattering, we have developed a forward model based on the beam propagation method and established the link between the macroscopic optical properties of the media and the statistical parameters of the phase masks applied to the wavefront. Using this model, we have analyzed the degradation of the point-spread function of the illumination beam in the transition regime from ballistic to diffusive light transport. Our method provides a wave-optic simulation toolkit to analyze the effects of scattering on image quality degradation in scanning microscopy. Our open-source implementation is available at https://github.com/BUNPC/Beam-Propagation-Method.},
	number = {20},
	urldate = {2023-11-17},
	journal = {Optics letters},
	author = {Cheng, Xiaojun and Li, Yunzhe and Mertz, Jerome and SakadŽić, Sava and Devor, Anna and Boas, David A. and Tian, Lei},
	month = oct,
	year = {2019},
	pmid = {31613246},
	pmcid = {PMC6889808},
	pages = {4989--4992},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\DHDUS6FJ\\Cheng et al. - 2019 - Development of a beam propagation method to simula.pdf:application/pdf},
}

@misc{noauthor_sbr-net_2023,
	title = {{SBR}-{Net}},
	url = {https://github.com/bu-cisl/sbrnet},
	urldate = {2023-11-17},
	publisher = {Boston University Computational Imaging Systems Lab},
	month = nov,
	year = {2023},
	annote = {original-date: 2023-10-20T18:05:41Z},
}

@article{tahir_adaptive_2022,
	title = {Adaptive {3D} descattering with a dynamic synthesis network},
	volume = {11},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-022-00730-x},
	doi = {10.1038/s41377-022-00730-x},
	abstract = {Deep learning has been broadly applied to imaging in scattering applications. A common framework is to train a descattering network for image recovery by removing scattering artifacts. To achieve the best results on a broad spectrum of scattering conditions, individual “expert” networks need to be trained for each condition. However, the expert’s performance sharply degrades when the testing condition differs from the training. An alternative brute-force approach is to train a “generalist” network using data from diverse scattering conditions. It generally requires a larger network to encapsulate the diversity in the data and a sufficiently large training set to avoid overfitting. Here, we propose an adaptive learning framework, termed dynamic synthesis network (DSN), which dynamically adjusts the model weights and adapts to different scattering conditions. The adaptability is achieved by a novel “mixture of experts” architecture that enables dynamically synthesizing a network by blending multiple experts using a gating network. We demonstrate the DSN in holographic 3D particle imaging for a variety of scattering conditions. We show in simulation that our DSN provides generalization across a continuum of scattering conditions. In addition, we show that by training the DSN entirely on simulated data, the network can generalize to experiments and achieve robust 3D descattering. We expect the same concept can find many other applications, such as denoising and imaging in scattering media. Broadly, our dynamic synthesis framework opens up a new paradigm for designing highly adaptive deep learning and computational imaging techniques.},
	number = {1},
	urldate = {2023-11-17},
	journal = {Light: Science \& Applications},
	author = {Tahir, Waleed and Wang, Hao and Tian, Lei},
	month = feb,
	year = {2022},
	keywords = {Imaging and sensing, Microscopy},
	pages = {42},
	annote = {Number: 1 Publisher: Nature Publishing Group},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\2PF5NSPU\\Tahir et al. - 2022 - Adaptive 3D descattering with a dynamic synthesis .pdf:application/pdf},
}

@article{horton_vivo_2013,
	title = {In vivo three-photon microscopy of subcortical structures within an intact mouse brain},
	volume = {7},
	issn = {1749-4893},
	url = {https://www.nature.com/articles/nphoton.2012.336},
	doi = {10.1038/nphoton.2012.336},
	abstract = {Two-photon fluorescence microscopy1 enables scientists in various fields including neuroscience2,3, embryology4 and oncology5 to visualize in vivo and ex vivo tissue morphology and physiology at a cellular level deep within scattering tissue. However, tissue scattering limits the maximum imaging depth of two-photon fluorescence microscopy to the cortical layer within mouse brain, and imaging subcortical structures currently requires the removal of overlying brain tissue3 or the insertion of optical probes6,7. Here, we demonstrate non-invasive, high-resolution, in vivo imaging of subcortical structures within an intact mouse brain using three-photon fluorescence microscopy at a spectral excitation window of 1,700 nm. Vascular structures as well as red fluorescent protein-labelled neurons within the mouse hippocampus are imaged. The combination of the long excitation wavelength and the higher-order nonlinear excitation overcomes the limitations of two-photon fluorescence microscopy, enabling biological investigations to take place at a greater depth within tissue.},
	number = {3},
	urldate = {2023-11-17},
	journal = {Nature Photonics},
	author = {Horton, Nicholas G. and Wang, Ke and Kobat, Demirhan and Clark, Catharine G. and Wise, Frank W. and Schaffer, Chris B. and Xu, Chris},
	month = mar,
	year = {2013},
	keywords = {Biophotonics, Multiphoton microscopy},
	pages = {205--209},
	annote = {Number: 3 Publisher: Nature Publishing Group},
	file = {Accepted Version:C\:\\Users\\jeffr\\Zotero\\storage\\VXTJZTLH\\Horton et al. - 2013 - In vivo three-photon microscopy of subcortical str.pdf:application/pdf},
}

@techreport{zhang_rapid_2022,
	title = {Rapid deep widefield neuron finder driven by virtual calcium imaging data},
	url = {https://www.biorxiv.org/content/10.1101/2022.01.25.474600v1},
	abstract = {Widefield microscope provides optical access to multi-millimeter fields of view and thousands of neurons in mammalian brains at video rate. However, calcium imaging at cellular resolution has been mostly contaminated by tissue scattering and background signals, making neuronal activities extraction challenging and time-consuming. Here we present a deep widefield neuron finder (DeepWonder), which is fueled by simulated calcium recordings but effectively works on experimental data with an order of magnitude faster speed and improved inference accuracy than traditional approaches. The efficient DeepWonder accomplished fifty-fold signal-to-background ratio enhancement in processing terabytes-scale cortex-wide recording, with over 14000 neurons extracted in 17 hours in workstation-grade computing resources compared to nearly week-long processing time with previous methods. DeepWonder circumvented the numerous computational resources and could serve as a guideline to massive data processing in widefield neuronal imaging.},
	urldate = {2022-03-07},
	institution = {bioRxiv},
	author = {Zhang, Yuanlong and Zhang, Guoxun and Han, Xiaofei and Wu, Jiamin and Li, Ziwei and Li, Xinyang and Xiao, Guihua and Xie, Hao and Fang, Lu and Dai, Qionghai},
	month = jan,
	year = {2022},
	doi = {10.1101/2022.01.25.474600},
	pages = {2022.01.25.474600},
	annote = {Section: New Results Type: article},
	file = {Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\T9QBE47X\\2022.01.25.html:text/html},
}

@article{foi_practical_2008,
	title = {Practical {Poissonian}-{Gaussian} noise modeling and fitting for single-image raw-data},
	volume = {17},
	issn = {1057-7149},
	doi = {10.1109/TIP.2008.2001399},
	abstract = {We present a simple and usable noise model for the raw-data of digital imaging sensors. This signal-dependent noise model, which gives the pointwise standard-deviation of the noise as a function of the expectation of the pixel raw-data output, is composed of a Poissonian part, modeling the photon sensing, and Gaussian part, for the remaining stationary disturbances in the output data. We further explicitly take into account the clipping of the data (over- and under-exposure), faithfully reproducing the nonlinear response of the sensor. We propose an algorithm for the fully automatic estimation of the model parameters given a single noisy image. Experiments with synthetic images and with real raw-data from various sensors prove the practical applicability of the method and the accuracy of the proposed model.},
	number = {10},
	journal = {IEEE transactions on image processing: a publication of the IEEE Signal Processing Society},
	author = {Foi, Alessandro and Trimeche, Mejdi and Katkovnik, Vladimir and Egiazarian, Karen},
	month = oct,
	year = {2008},
	pmid = {18784024},
	keywords = {Algorithms, Computer Simulation, Computer-Assisted, Data Interpretation, Image Enhancement, Image Interpretation, Models, Reproducibility of Results, Sensitivity and Specificity, Statistical},
	pages = {1737--1754},
}

@misc{noauthor_value_2021,
	title = {Value noise},
	url = {https://en.wikipedia.org/w/index.php?title=Value_noise&oldid=1024311499},
	abstract = {Value noise is a type of noise commonly used as a procedural texture primitive in computer graphics. It is conceptually different from, and often confused with gradient noise, examples of which are Perlin noise and Simplex noise. This method consists of the creation of a lattice of points which are assigned random values. The noise function then returns the interpolated number based on the values of the surrounding lattice points. For many applications, multiple octaves of this noise can be generated and then summed together, just as can be done with Perlin noise and Simplex noise, in order to create a form of fractal noise.},
	urldate = {2022-12-20},
	journal = {Wikipedia},
	month = may,
	year = {2021},
	annote = {Page Version ID: 1024311499},
}

@misc{wijethilake_deep2_2022,
	title = {{DEEP}{\textbackslash}ˆ2{\textbackslash}: {Deep} {Learning} {Powered} {De}-scattering with {Excitation} {Patterning}},
	shorttitle = {{DEEP}{\textbackslash}ˆ2{\textbackslash}},
	url = {http://arxiv.org/abs/2210.10892},
	abstract = {Limited throughput is a key challenge in in-vivo deep-tissue imaging using nonlinear optical microscopy. Point scanning multiphoton microscopy, the current gold standard, is slow especially compared to the wide-field imaging modalities used for optically cleared or thin specimens. We recently introduced 'De-scattering with Excitation Patterning or DEEP', as a widefield alternative to point-scanning geometries. Using patterned multiphoton excitation, DEEP encodes spatial information inside tissue before scattering. However, to de-scatter at typical depths, hundreds of such patterned excitations are needed. In this work, we present DEEP{\textbackslash}ˆ2{\textbackslash}, a deep learning based model, that can de-scatter images from just tens of patterned excitations instead of hundreds. Consequently, we improve DEEP's throughput by almost an order of magnitude. We demonstrate our method in multiple numerical and physical experiments including in-vivo cortical vasculature imaging up to four scattering lengths deep, in alive mice.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Wijethilake, Navodini and Anandakumar, Mithunjha and Zheng, Cheng and So, Peter T. C. and Yildirim, Murat and Wadduwage, Dushan N.},
	month = oct,
	year = {2022},
	doi = {10.48550/arXiv.2210.10892},
	note = {Issue: arXiv:2210.10892
arXiv: 2210.10892 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{adams_vivo_2022,
	title = {In vivo lensless microscopy via a phase mask generating diffraction patterns with high-contrast contours},
	volume = {6},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00851-z},
	doi = {10.1038/s41551-022-00851-z},
	abstract = {The simple and compact optics of lensless microscopes and the associated computational algorithms allow for large fields of view and the refocusing of the captured images. However, existing lensless techniques cannot accurately reconstruct the typical low-contrast images of optically dense biological tissue. Here we show that lensless imaging of tissue in vivo can be achieved via an optical phase mask designed to create a point spread function consisting of high-contrast contours with a broad spectrum of spatial frequencies. We built a prototype lensless microscope incorporating the ‘contour’ phase mask and used it to image calcium dynamics in the cortex of live mice (over a field of view of about 16 mm2) and in freely moving Hydra vulgaris, as well as microvasculature in the oral mucosa of volunteers. The low cost, small form factor and computational refocusing capability of in vivo lensless microscopy may open it up to clinical uses, especially for imaging difficult-to-reach areas of the body.},
	number = {5},
	urldate = {2022-12-20},
	journal = {Nature Biomedical Engineering},
	author = {Adams, Jesse K. and Yan, Dong and Wu, Jimin and Boominathan, Vivek and Gao, Sibo and Rodriguez, Alex V. and Kim, Soonyoung and Carns, Jennifer and Richards-Kortum, Rebecca and Kemere, Caleb and Veeraraghavan, Ashok and Robinson, Jacob T.},
	month = may,
	year = {2022},
	keywords = {3-D reconstruction, Applied optics, Ca2+ imaging, Fluorescence imaging, Optical imaging},
	pages = {617--628},
	annote = {Number: 5 Publisher: Nature Publishing Group},
}

@article{yanny_miniscope3d_2020,
	title = {{Miniscope3D}: optimized single-shot miniature {3D} fluorescence microscopy},
	volume = {9},
	issn = {2047-7538},
	shorttitle = {{Miniscope3D}},
	url = {https://www.nature.com/articles/s41377-020-00403-7},
	doi = {10.1038/s41377-020-00403-7},
	abstract = {Miniature fluorescence microscopes are a standard tool in systems biology. However, widefield miniature microscopes capture only 2D information, and modifications that enable 3D capabilities increase the size and weight and have poor resolution outside a narrow depth range. Here, we achieve the 3D capability by replacing the tube lens of a conventional 2D Miniscope with an optimized multifocal phase mask at the objective’s aperture stop. Placing the phase mask at the aperture stop significantly reduces the size of the device, and varying the focal lengths enables a uniform resolution across a wide depth range. The phase mask encodes the 3D fluorescence intensity into a single 2D measurement, and the 3D volume is recovered by solving a sparsity-constrained inverse problem. We provide methods for designing and fabricating the phase mask and an efficient forward model that accounts for the field-varying aberrations in miniature objectives. We demonstrate a prototype that is 17 mm tall and weighs 2.5 grams, achieving 2.76 μm lateral, and 15 μm axial resolution across most of the 900 × 700 × 390 μm3 volume at 40 volumes per second. The performance is validated experimentally on resolution targets, dynamic biological samples, and mouse brain tissue. Compared with existing miniature single-shot volume-capture implementations, our system is smaller and lighter and achieves a more than 2× better lateral and axial resolution throughout a 10× larger usable depth range. Our microscope design provides single-shot 3D imaging for applications where a compact platform matters, such as volumetric neural imaging in freely moving animals and 3D motion studies of dynamic samples in incubators and lab-on-a-chip devices.},
	number = {1},
	urldate = {2022-12-20},
	journal = {Light: Science \& Applications},
	author = {Yanny, Kyrollos and Antipa, Nick and Liberti, William and Dehaeck, Sam and Monakhova, Kristina and Liu, Fanglin Linda and Shen, Konlin and Ng, Ren and Waller, Laura},
	month = oct,
	year = {2020},
	keywords = {Imaging and sensing, Microscopy},
	pages = {171},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}


@article{greene_pupil_2023,
	title = {Pupil engineering for extended depth-of-field imaging in a fluorescence miniscope},
	volume = {10},
	issn = {2329-423X, 2329-4248},
	url = {https://www.spiedigitallibrary.org/journals/neurophotonics/volume-10/issue-4/044302/Pupil-engineering-for-extended-depth-of-field-imaging-in-a/10.1117/1.NPh.10.4.044302.full},
	doi = {10.1117/1.NPh.10.4.044302},
	abstract = {SignificanceFluorescence head-mounted microscopes, i.e., miniscopes, have emerged as powerful tools to analyze in-vivo neural populations but exhibit a limited depth-of-field (DoF) due to the use of high numerical aperture (NA) gradient refractive index (GRIN) objective lenses.AimWe present extended depth-of-field (EDoF) miniscope, which integrates an optimized thin and lightweight binary diffractive optical element (DOE) onto the GRIN lens of a miniscope to extend the DoF by 2.8 × between twin foci in fixed scattering samples.ApproachWe use a genetic algorithm that considers the GRIN lens’ aberration and intensity loss from scattering in a Fourier optics-forward model to optimize a DOE and manufacture the DOE through single-step photolithography. We integrate the DOE into EDoF-Miniscope with a lateral accuracy of 70 μm to produce high-contrast signals without compromising the speed, spatial resolution, size, or weight.ResultsWe characterize the performance of EDoF-Miniscope across 5- and 10-μm fluorescent beads embedded in scattering phantoms and demonstrate that EDoF-Miniscope facilitates deeper interrogations of neuronal populations in a 100-μm-thick mouse brain sample and vessels in a whole mouse brain sample.ConclusionsBuilt from off-the-shelf components and augmented by a customizable DOE, we expect that this low-cost EDoF-Miniscope may find utility in a wide range of neural recording applications.},
	number = {4},
	urldate = {2023-11-20},
	journal = {Neurophotonics},
	author = {Greene, Joseph and Xue, Yujia and Alido, Jeffrey and Matlock, Alex and Hu, Guorong and Kiliç, Kivilcim and Davison, Ian and Tian, Lei},
	month = may,
	year = {2023},
	note = {Publisher: SPIE},
	pages = {044302},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\X6YYAL79\\Greene et al. - 2023 - Pupil engineering for extended depth-of-field imag.pdf:application/pdf},
}


@article{xue_single-shot_2020,
	title = {Single-shot {3D} wide-field fluorescence imaging with a {Computational} {Miniature} {Mesoscope}},
	volume = {6},
	url = {https://www.science.org/doi/full/10.1126/sciadv.abb7508},
	doi = {10.1126/sciadv.abb7508},
	abstract = {Fluorescence microscopes are indispensable to biology and neuroscience. The need for recording in freely behaving animals has further driven the development in miniaturized microscopes (miniscopes). However, conventional microscopes/miniscopes are inherently constrained by their limited space-bandwidth product, shallow depth of field (DOF), and inability to resolve three-dimensional (3D) distributed emitters. Here, we present a Computational Miniature Mesoscope (CM2) that overcomes these bottlenecks and enables single-shot 3D imaging across an 8 mm by 7 mm field of view and 2.5-mm DOF, achieving 7-μm lateral resolution and better than 200-μm axial resolution. The CM2 features a compact lightweight design that integrates a microlens array for imaging and a light-emitting diode array for excitation. Its expanded imaging capability is enabled by computational imaging that augments the optics by algorithms. We experimentally validate the mesoscopic imaging capability on 3D fluorescent samples. We further quantify the effects of scattering and background fluorescence on phantom experiments.},
	number = {43},
	urldate = {2022-12-20},
	journal = {Science Advances},
	author = {Xue, Yujia and Davison, Ian G. and Boas, David A. and Tian, Lei},
	month = oct,
	year = {2020},
	pages = {eabb7508},
	annote = {Publisher: American Association for the Advancement of Science},
}

@article{xue_deep-learning-augmented_2022,
	title = {Deep-learning-augmented computational miniature mesoscope},
	volume = {9},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-9-9-1009},
	doi = {10.1364/OPTICA.464700},
	abstract = {Fluorescence microscopy is essential to study biological structures and dynamics. However, existing systems suffer from a trade-off between field of view (FOV), resolution, and system complexity, and thus cannot fulfill the emerging need for miniaturized platforms providing micron-scale resolution across centimeter-scale FOVs. To overcome this challenge, we developed a computational miniature mesoscope (CM2) that exploits a computational imaging strategy to enable single-shot, 3D high-resolution imaging across a wide FOV in a miniaturized platform. Here, we present CM2 V2, which significantly advances both the hardware and computation. We complement the 3×3 microlens array with a hybrid emission filter that improves the imaging contrast by 5×, and design a 3D-printed free-form collimator for the LED illuminator that improves the excitation efficiency by 3×. To enable high-resolution reconstruction across a large volume, we develop an accurate and efficient 3D linear shift-variant (LSV) model to characterize spatially varying aberrations. We then train a multimodule deep learning model called CM2Net, using only the 3D-LSV simulator. We quantify the detection performance and localization accuracy of CM2Net to reconstruct fluorescent emitters under different conditions in simulation. We then show that CM2Net generalizes well to experiments and achieves accurate 3D reconstruction across a ∼7-mm FOV and 800-µm depth, and provides ∼6-µm lateral and ∼25-µm axial resolution. This provides an ∼8× better axial resolution and ∼1400× faster speed compared to the previous model-based algorithm. We anticipate this simple, low-cost computational miniature imaging system will be useful for many large-scale 3D fluorescence imaging applications.},
	number = {9},
	urldate = {2022-12-20},
	journal = {Optica},
	author = {Xue, Yujia and Yang, Qianwan and Hu, Guorong and Guo, Kehan and Tian, Lei},
	month = sep,
	year = {2022},
	pages = {1009--1021},
	annote = {Publisher: Optica Publishing Group},
}

@misc{noauthor_mie_nodate,
	title = {Mie {Scattering} {Calculator}},
	url = {https://omlc.org/calc/mie_calc.html},
	urldate = {2023-01-07},
	file = {Mie Scattering Calculator:C\:\\Users\\jeffr\\Zotero\\storage\\G5XIAQ28\\mie_calc.html:text/html},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2023-01-07},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {Issue: arXiv:1502.01852
arXiv: 1502.01852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\9U7TIZVD\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\H7V5FBJJ\\1502.html:text/html},
}

@article{kobayashi_wide_2019,
	title = {Wide and {Deep} {Imaging} of {Neuronal} {Activities} by a {Wearable} {NeuroImager} {Reveals} {Premotor} {Activity} in the {Whole} {Motor} {Cortex}},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-44146-x},
	doi = {10.1038/s41598-019-44146-x},
	abstract = {Wearable technologies for functional whole brain imaging in freely moving animals would advance our understanding of cognitive processing and adaptive behavior. Fluorescence imaging can visualize the activity of individual neurons in real time, but conventional microscopes have limited sample coverage in both the width and depth of view. Here we developed a novel head-mounted laser camera (HLC) with macro and deep-focus lenses that enable fluorescence imaging at cellular resolution for comprehensive imaging in mice expressing a layer- and cell type-specific calcium probe. We visualized orientation selectivity in individual excitatory neurons across the whole visual cortex of one hemisphere, and cell assembly expressing the premotor activity that precedes voluntary movement across the motor cortex of both hemispheres. Including options for multiplex and wireless interfaces, our wearable, wide- and deep-imaging HLC technology could enable simple and economical mapping of neuronal populations underlying cognition and behavior.},
	number = {1},
	urldate = {2023-01-08},
	journal = {Scientific Reports},
	author = {Kobayashi, Takuma and Islam, Tanvir and Sato, Masaaki and Ohkura, Masamichi and Nakai, Junichi and Hayashi, Yasunori and Okamoto, Hitoshi},
	month = jun,
	year = {2019},
	keywords = {Fluorescence imaging, Motion detection, Motor cortex},
	pages = {8366},
	annote = {Number: 1 Publisher: Nature Publishing Group},
	file = {Full Text PDF:C\:\\Users\\jeffr\\Zotero\\storage\\R7QDY7IU\\Kobayashi et al. - 2019 - Wide and Deep Imaging of Neuronal Activities by a .pdf:application/pdf},
}

@misc{zhang_imaging_2022,
	title = {Imaging through the {Atmosphere} using {Turbulence} {Mitigation} {Transformer}},
	url = {http://arxiv.org/abs/2207.06465},
	abstract = {Restoring images distorted by atmospheric turbulence is a long-standing problem due to the spatially varying nature of the distortion, nonlinearity of the image formation process, and scarcity of training and testing data. Existing methods often have strong statistical assumptions on the distortion model which in many cases will lead to a limited performance in real-world scenarios as they do not generalize. To overcome the challenge, this paper presents an end-to-end physics-driven approach that is efficient and can generalize to real-world turbulence. On the data synthesis front, we significantly increase the image resolution that can be handled by the SOTA turbulence simulator by approximating the random field via wide-sense stationarity. The new data synthesis process enables the generation of large-scale multi-level turbulence and ground truth pairs for training. On the network design front, we propose the turbulence mitigation transformer (TMT), a two stage U-Net shaped multi-frame restoration network which has a noval efficient self-attention mechanism named temporal channel joint attention (TCJA). We also introduce a new training scheme that is enabled by the new simulator, and we design new transformer units to reduce the memory consumption. Experimental results on both static and dynamic scenes are promising, including various real turbulence scenarios.},
	urldate = {2023-01-08},
	publisher = {arXiv},
	author = {Zhang, Xingguang and Mao, Zhiyuan and Chimitt, Nicholas and Chan, Stanley H.},
	month = jul,
	year = {2022},
	doi = {10.48550/arXiv.2207.06465},
	note = {Issue: arXiv:2207.06465
arXiv: 2207.06465 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\685KLXZN\\Zhang et al. - 2022 - Imaging through the Atmosphere using Turbulence Mi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\EVQB72Z2\\2207.html:text/html},
}

@article{zhang_rapid_2023,
	title = {Rapid detection of neurons in widefield calcium imaging datasets after training with synthetic data},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-023-01838-7},
	doi = {10.1038/s41592-023-01838-7},
	abstract = {Widefield microscopy can provide optical access to multi-millimeter fields of view and thousands of neurons in mammalian brains at video rate. However, tissue scattering and background contamination results in signal deterioration, making the extraction of neuronal activity challenging, laborious and time consuming. Here we present our deep-learning-based widefield neuron finder (DeepWonder), which is trained by simulated functional recordings and effectively works on experimental data to achieve high-fidelity neuronal extraction. Equipped with systematic background contribution priors, DeepWonder conducts neuronal inference with an order-of-magnitude-faster speed and improved accuracy compared with alternative approaches. DeepWonder removes background contaminations and is computationally efficient. Specifically, DeepWonder accomplishes 50-fold signal-to-background ratio enhancement when processing terabytes-scale cortex-wide functional recordings, with over 14,000 neurons extracted in 17 h.},
	urldate = {2023-04-25},
	journal = {Nature Methods},
	author = {Zhang, Yuanlong and Zhang, Guoxun and Han, Xiaofei and Wu, Jiamin and Li, Ziwei and Li, Xinyang and Xiao, Guihua and Xie, Hao and Fang, Lu and Dai, Qionghai},
	month = apr,
	year = {2023},
	keywords = {Fluorescence imaging, Image processing, Machine learning, Mouse, Neuroscience},
	pages = {1--8},
	annote = {Publisher: Nature Publishing Group},
}

@article{mockl_accurate_2020,
	title = {Accurate and rapid background estimation in single-molecule localization microscopy using the deep neural network {BGnet}},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1916219117},
	doi = {10.1073/pnas.1916219117},
	abstract = {Background fluorescence, especially when it exhibits undesired spatial features, is a primary factor for reduced image quality in optical microscopy. Structured background is particularly detrimental when analyzing single-molecule images for 3-dimensional localization microscopy or single-molecule tracking. Here, we introduce BGnet, a deep neural network with a U-net-type architecture, as a general method to rapidly estimate the background underlying the image of a point source with excellent accuracy, even when point-spread function (PSF) engineering is in use to create complex PSF shapes. We trained BGnet to extract the background from images of various PSFs and show that the identification is accurate for a wide range of different interfering background structures constructed from many spatial frequencies. Furthermore, we demonstrate that the obtained background-corrected PSF images, for both simulated and experimental data, lead to a substantial improvement in localization precision. Finally, we verify that structured background estimation with BGnet results in higher quality of superresolution reconstructions of biological structures.},
	number = {1},
	urldate = {2023-03-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Möckl, Leonhard and Roy, Anish R. and Petrov, Petar N. and Moerner, W. E.},
	month = jan,
	year = {2020},
	pages = {60--67},
	annote = {Publisher: Proceedings of the National Academy of Sciences},
}

@article{hampson_adaptive_2021,
	title = {Adaptive optics for high-resolution imaging},
	volume = {1},
	issn = {2662-8449},
	url = {https://www.nature.com/articles/s43586-021-00066-7},
	doi = {10.1038/s43586-021-00066-7},
	abstract = {Adaptive optics (AO) is a technique that corrects for optical aberrations. It was originally proposed to correct for the blurring effect of atmospheric turbulence on images in ground-based telescopes and was instrumental in the work that resulted in the Nobel prize-winning discovery of a supermassive compact object at the centre of our galaxy. When AO is used to correct for the eye’s imperfect optics, retinal changes at the cellular level can be detected, allowing us to study the operation of the visual system and to assess ocular health in the microscopic domain. By correcting for sample-induced blur in microscopy, AO has pushed the boundaries of imaging in thick tissue specimens, such as when observing neuronal processes in the brain. In this primer, we focus on the application of AO for high-resolution imaging in astronomy, vision science and microscopy. We begin with an overview of the general principles of AO and its main components, which include methods to measure the aberrations, devices for aberration correction, and how these components are linked in operation. We present results and applications from each field along with reproducibility considerations and limitations. Finally, we discuss future directions.},
	number = {1},
	urldate = {2023-03-01},
	journal = {Nature Reviews Methods Primers},
	author = {Hampson, Karen M. and Turcotte, Raphaël and Miller, Donald T. and Kurokawa, Kazuhiro and Males, Jared R. and Ji, Na and Booth, Martin J.},
	month = oct,
	year = {2021},
	keywords = {Imaging techniques, Optical physics},
	pages = {1--26},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{al-juboori_light_2013,
	title = {Light {Scattering} {Properties} {Vary} across {Different} {Regions} of the {Adult} {Mouse} {Brain}},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067626},
	doi = {10.1371/journal.pone.0067626},
	abstract = {Recently developed optogenetic tools provide powerful approaches to optically excite or inhibit neural activity. In a typical in-vivo experiment, light is delivered to deep nuclei via an implanted optical fiber. Light intensity attenuates with increasing distance from the fiber tip, determining the volume of tissue in which optogenetic proteins can successfully be activated. However, whether and how this volume of effective light intensity varies as a function of brain region or wavelength has not been systematically studied. The goal of this study was to measure and compare how light scatters in different areas of the mouse brain. We delivered different wavelengths of light via optical fibers to acute slices of mouse brainstem, midbrain and forebrain tissue. We measured light intensity as a function of distance from the fiber tip, and used the data to model the spread of light in specific regions of the mouse brain. We found substantial differences in effective attenuation coefficients among different brain areas, which lead to substantial differences in light intensity demands for optogenetic experiments. The use of light of different wavelengths additionally changes how light illuminates a given brain area. We created a brain atlas of effective attenuation coefficients of the adult mouse brain, and integrated our data into an application that can be used to estimate light scattering as well as required light intensity for optogenetic manipulation within a given volume of tissue.},
	number = {7},
	urldate = {2023-03-01},
	journal = {PLOS ONE},
	author = {Al-Juboori, Saif I. and Dondzillo, Anna and Stubblefield, Elizabeth A. and Felsen, Gidon and Lei, Tim C. and Klug, Achim},
	month = jul,
	year = {2013},
	keywords = {Artificial light, Cameras, Experimental design, Fiber optics, Light, Light scattering, Nerve fibers, Optogenetics},
	pages = {e67626},
	annote = {Publisher: Public Library of Science},
}

@article{kang_dynamical_2021,
	title = {Dynamical machine learning volumetric reconstruction of objects’ interiors from limited angular views},
	volume = {10},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-021-00512-x},
	doi = {10.1038/s41377-021-00512-x},
	abstract = {Limited-angle tomography of an interior volume is a challenging, highly ill-posed problem with practical implications in medical and biological imaging, manufacturing, automation, and environmental and food security. Regularizing priors are necessary to reduce artifacts by improving the condition of such problems. Recently, it was shown that one effective way to learn the priors for strongly scattering yet highly structured 3D objects, e.g. layered and Manhattan, is by a static neural network [Goy et al. Proc. Natl. Acad. Sci. 116, 19848–19856 (2019)]. Here, we present a radically different approach where the collection of raw images from multiple angles is viewed analogously to a dynamical system driven by the object-dependent forward scattering operator. The sequence index in the angle of illumination plays the role of discrete time in the dynamical system analogy. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical learning as a better fit to regularize the reconstructions. We devised a Recurrent Neural Network (RNN) architecture with a novel Separable-Convolution Gated Recurrent Unit (SC-GRU) as the fundamental building block. Through a comprehensive comparison of several quantitative metrics, we show that the dynamic method is suitable for a generic interior-volumetric reconstruction under a limited-angle scheme. We show that this approach accurately reconstructs volume interiors under two conditions: weak scattering, when the Radon transform approximation is applicable and the forward operator well defined; and strong scattering, which is nonlinear with respect to the 3D refractive index distribution and includes uncertainty in the forward operator.},
	number = {1},
	urldate = {2023-02-27},
	journal = {Light: Science \& Applications},
	author = {Kang, Iksung and Goy, Alexandre and Barbastathis, George},
	month = apr,
	year = {2021},
	keywords = {Applied optics, Imaging and sensing},
	pages = {74},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{nobauer_video_2017,
	title = {Video rate volumetric {Ca2}+ imaging across cortex using seeded iterative demixing ({SID}) microscopy},
	volume = {14},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4341},
	doi = {10.1038/nmeth.4341},
	abstract = {The seeded iterative demixing strategy, when used in combination with light-field microscopy, enables calcium imaging at single-neuron resolution in the mouse brain at high volumetric imaging rates and depths of up to 380 μm.},
	number = {8},
	urldate = {2023-02-24},
	journal = {Nature Methods},
	author = {Nöbauer, Tobias and Skocek, Oliver and Pernía-Andrade, Alejandro J. and Weilguny, Lukas and Traub, Francisca Martínez and Molodtsov, Maxim I. and Vaziri, Alipasha},
	month = aug,
	year = {2017},
	keywords = {Fluorescence imaging, Microscopy, Mouse, Neuroscience},
	pages = {811--818},
	annote = {Number: 8 Publisher: Nature Publishing Group},
}

@inproceedings{levoy_light_2006,
	series = {{SIGGRAPH} '06},
	title = {Light field microscopy},
	isbn = {978-1-59593-364-5},
	url = {https://doi.org/10.1145/1179352.1141976},
	doi = {10.1145/1179352.1141976},
	abstract = {By inserting a microlens array into the optical train of a conventional microscope, one can capture light fields of biological specimens in a single photograph. Although diffraction places a limit on the product of spatial and angular resolution in these light fields, we can nevertheless produce useful perspective views and focal stacks from them. Since microscopes are inherently orthographic devices, perspective views represent a new way to look at microscopic specimens. The ability to create focal stacks from a single photograph allows moving or light-sensitive specimens to be recorded. Applying 3D deconvolution to these focal stacks, we can produce a set of cross sections, which can be visualized using volume rendering. In this paper, we demonstrate a prototype light field microscope (LFM), analyze its optical performance, and show perspective views, focal stacks, and reconstructed volumes for a variety of biological specimens. We also show that synthetic focusing followed by 3D deconvolution is equivalent to applying limited-angle tomography directly to the 4D light field.},
	urldate = {2023-02-24},
	booktitle = {{ACM} {SIGGRAPH} 2006 {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Levoy, Marc and Ng, Ren and Adams, Andrew and Footer, Matthew and Horowitz, Mark},
	month = jul,
	year = {2006},
	note = {Place: New York, NY, USA},
	keywords = {deconvolution, light fields, microscopy, synthetic aperture, tomography, volume rendering},
	pages = {924--934},
}

@misc{noauthor_light_nodate,
	title = {Light field microscopy {\textbackslash}textbar {ACM} {SIGGRAPH} 2006 {Papers}},
	url = {https://dl.acm.org/doi/10.1145/1179352.1141976},
	urldate = {2023-02-24},
}

@article{wang_real-time_2021,
	title = {Real-time volumetric reconstruction of biological dynamics with light-field microscopy and deep learning},
	volume = {18},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01058-x},
	doi = {10.1038/s41592-021-01058-x},
	abstract = {Light-field microscopy has emerged as a technique of choice for high-speed volumetric imaging of fast biological processes. However, artifacts, nonuniform resolution and a slow reconstruction speed have limited its full capabilities for in toto extraction of dynamic spatiotemporal patterns in samples. Here, we combined a view-channel-depth (VCD) neural network with light-field microscopy to mitigate these limitations, yielding artifact-free three-dimensional image sequences with uniform spatial resolution and high-video-rate reconstruction throughput. We imaged neuronal activities across moving Caenorhabditis elegans and blood flow in a beating zebrafish heart at single-cell resolution with volumetric imaging rates up to 200 Hz.},
	number = {5},
	urldate = {2023-02-24},
	journal = {Nature Methods},
	author = {Wang, Zhaoqiang and Zhu, Lanxin and Zhang, Hao and Li, Guo and Yi, Chengqiang and Li, Yi and Yang, Yicong and Ding, Yichen and Zhen, Mei and Gao, Shangbang and Hsiai, Tzung K. and Fei, Peng},
	month = may,
	year = {2021},
	keywords = {Microscopy, Optical imaging},
	pages = {551--556},
	annote = {Number: 5 Publisher: Nature Publishing Group},
}

@article{prevedel_simultaneous_2014,
	title = {Simultaneous whole-animal {3D} imaging of neuronal activity using light-field microscopy},
	volume = {11},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2964},
	doi = {10.1038/nmeth.2964},
	abstract = {This paper reports the use of light-field microscopy for fast, large-scale imaging of neuronal activity in vivo. It is applied to image the entire animal in the worm and the whole brain in zebrafish.},
	number = {7},
	urldate = {2023-02-24},
	journal = {Nature Methods},
	author = {Prevedel, Robert and Yoon, Young-Gyu and Hoffmann, Maximilian and Pak, Nikita and Wetzstein, Gordon and Kato, Saul and Schrödel, Tina and Raskar, Ramesh and Zimmer, Manuel and Boyden, Edward S. and Vaziri, Alipasha},
	month = jul,
	year = {2014},
	keywords = {Ca2+ imaging, Fluorescence imaging, Neurophysiology, Neuroscience},
	pages = {727--730},
	annote = {Number: 7 Publisher: Nature Publishing Group},
}

@article{guo_fourier_2019,
	title = {Fourier light-field microscopy},
	volume = {27},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-27-18-25573},
	doi = {10.1364/OE.27.025573},
	abstract = {Observing the various anatomical and functional information that spans many spatiotemporal scales with high resolution provides deep understandings of the fundamentals of biological systems. Light-field microscopy (LFM) has recently emerged as a scanning-free, scalable method that allows for high-speed, volumetric imaging ranging from single-cell specimens to the mammalian brain. However, the prohibitive reconstruction artifacts and severe computational cost have thus far limited broader applications of LFM. To address the challenge, in this work, we report Fourier LFM (FLFM), a system that processes the light-field information through the Fourier domain. We established a complete theoretical and algorithmic framework that describes light propagation, image formation and system characterization of FLFM. Compared with conventional LFM, FLFM fundamentally mitigates the artifacts, allowing high-resolution imaging across a two- to three-fold extended depth. In addition, the system substantially reduces the reconstruction time by roughly two orders of magnitude. FLFM was validated by high-resolution, artifact-free imaging of various caliber and biological samples. Furthermore, we proposed a generic design principle for FLFM, as a highly scalable method to meet broader imaging needs across various spatial levels. We anticipate FLFM to be a particularly powerful tool for imaging diverse phenotypic and functional information, spanning broad molecular, cellular and tissue systems.},
	number = {18},
	urldate = {2023-02-24},
	journal = {Optics Express},
	author = {Guo, Changliang and Liu, Wenhao and Hua, Xuanwen and Li, Haoyu and Jia, Shu},
	month = sep,
	year = {2019},
	keywords = {Image quality, Image reconstruction, Imaging techniques, Light propagation, Optical design, Plenoptic imaging},
	pages = {25573--25594},
	annote = {Publisher: Optica Publishing Group},
}

@article{zhu_large_2022,
	title = {Large field-of-view non-invasive imaging through scattering layers using fluctuating random illumination},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-29166-y},
	doi = {10.1038/s41467-022-29166-y},
	abstract = {Non-invasive optical imaging techniques are essential diagnostic tools in many fields. Although various recent methods have been proposed to utilize and control light in multiple scattering media, non-invasive optical imaging through and inside scattering layers across a large field of view remains elusive due to the physical limits set by the optical memory effect, especially without wavefront shaping techniques. Here, we demonstrate an approach that enables non-invasive fluorescence imaging behind scattering layers with field-of-views extending well beyond the optical memory effect. The method consists in demixing the speckle patterns emitted by a fluorescent object under variable unknown random illumination, using matrix factorization and a novel fingerprint-based reconstruction. Experimental validation shows the efficiency and robustness of the method with various fluorescent samples, covering a field of view up to three times the optical memory effect range. Our non-invasive imaging technique is simple, neither requires a spatial light modulator nor a guide star, and can be generalized to a wide range of incoherent contrast mechanisms and illumination schemes.},
	number = {1},
	urldate = {2023-02-24},
	journal = {Nature Communications},
	author = {Zhu, Lei and Soldevila, Fernando and Moretti, Claudio and d’Arco, Alexandra and Boniface, Antoine and Shao, Xiaopeng and de Aguiar, Hilton B. and Gigan, Sylvain},
	month = mar,
	year = {2022},
	keywords = {Fluorescence imaging, Imaging and sensing, Imaging techniques, Microscopy},
	pages = {1447},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@misc{alido_robust_2023,
	title = {Robust single-shot {3D} fluorescence imaging in scattering media with a simulator-trained neural network},
	url = {http://arxiv.org/abs/2303.12573},
	abstract = {Imaging through scattering is a pervasive and difficult problem in many biological applications. The high background and the exponentially attenuated target signals due to scattering fundamentally limits the imaging depth of fluorescence microscopy. Light-field systems are favorable for high-speed volumetric imaging, but the 2D-to-3D reconstruction is fundamentally ill-posed and scattering exacerbates the condition of the inverse problem. Here, we develop a scattering simulator that models low-contrast target signals buried in heterogeneous strong background. We then train a deep neural network solely on synthetic data to descatter and reconstruct a 3D volume from a single-shot light-field measurement with low signal-to-background ratio (SBR). We apply this network to our previously developed Computational Miniature Mesoscope and demonstrate the robustness of our deep learning algorithm on a 75 micron thick fixed mouse brain section and on bulk scattering phantoms with different scattering conditions. The network can robustly reconstruct emitters in 3D with a 2D measurement of SBR as low as 1.05 and as deep as a scattering length. We analyze fundamental tradeoffs based on network design factors and out-of-distribution data that affect the deep learning model's generalizability to real experimental data. Broadly, we believe that our simulator-based deep learning approach can be applied to a wide range of imaging through scattering techniques where experimental paired training data is lacking.},
	urldate = {2023-10-28},
	publisher = {arXiv},
	author = {Alido, Jeffrey and Greene, Joseph and Xue, Yujia and Hu, Guorong and Li, Yunzhe and Monk, Kevin J. and DeBenedicts, Brett T. and Davison, Ian G. and Tian, Lei},
	month = mar,
	year = {2023},
	doi = {10.48550/arXiv.2303.12573},
	note = {Issue: arXiv:2303.12573
arXiv: 2303.12573 [physics]},
	keywords = {Electrical Engineering and Systems Science - Signal Processing, Physics - Biological Physics, Physics - Optics},
	file = {arXiv Fulltext PDF:C\:\\Users\\jeffr\\Zotero\\storage\\ZAJPR3MH\\Alido et al. - 2023 - Robust single-shot 3D fluorescence imaging in scat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\6EWCX3CN\\2303.html:text/html},
}

@article{moretti_readout_2020,
	title = {Readout of fluorescence functional signals through highly scattering tissue},
	volume = {14},
	issn = {1749-4893},
	url = {https://www.nature.com/articles/s41566-020-0612-2},
	doi = {10.1038/s41566-020-0612-2},
	abstract = {Fluorescence is a powerful means to probe information processing in the mammalian brain1. However, neuronal tissues are highly heterogeneous and thus opaque to light. A wide set of non-invasive or invasive techniques for scattered light rejection, optical sectioning or localized excitation have been developed, but non-invasive optical recording of activity through a highly scattering layer beyond the ballistic regime is impossible as yet. Here, we show that functional signals from fluorescent time-varying sources located below a highly scattering bone tissue can be retrieved efficiently by exploiting matrix factorization algorithms to demix this information from temporal sequences of low-contrast fluorescence speckle patterns.},
	number = {6},
	urldate = {2023-02-24},
	journal = {Nature Photonics},
	author = {Moretti, Claudio and Gigan, Sylvain},
	month = jun,
	year = {2020},
	keywords = {Applied optics, Fluorescence imaging, Imaging and sensing},
	pages = {361--364},
	annote = {Number: 6 Publisher: Nature Publishing Group},
}

@article{mertz_optical_2011,
	title = {Optical sectioning microscopy with planar or structured illumination},
	volume = {8},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1709},
	doi = {10.1038/nmeth.1709},
	abstract = {Presented is a Review of the principles and practice of optical sectioning microscopy using planar illumination or structured illumination with descriptions of the advantages and disadvantages of these techniques.},
	number = {10},
	urldate = {2023-02-24},
	journal = {Nature Methods},
	author = {Mertz, Jerome},
	month = oct,
	year = {2011},
	keywords = {Microscopy, Optical imaging},
	pages = {811--819},
	annote = {Number: 10 Publisher: Nature Publishing Group},
}

@article{mertz_strategies_2019,
	title = {Strategies for volumetric imaging with a fluorescence microscope},
	volume = {6},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-6-10-1261},
	doi = {10.1364/OPTICA.6.001261},
	abstract = {Three-dimensional fluorescence imaging has been a longstanding goal for microscopists, made all the more challenging when aiming for a trifecta of resolution, speed, and field of view. The purpose of this review is to summarize some current strategies in volumetric microscopy, both camera- and scanning-based.},
	number = {10},
	urldate = {2023-02-24},
	journal = {Optica},
	author = {Mertz, Jerome},
	month = oct,
	year = {2019},
	keywords = {Diffractive optical elements, Fluorescence microscopy, Light sheet microscopy, Liquid crystal modulators, Scanning microscopy, Three dimensional imaging},
	pages = {1261--1268},
	annote = {Publisher: Optica Publishing Group},
}

@article{aharoni_circuit_2019,
	title = {Circuit {Investigations} {With} {Open}-{Source} {Miniaturized} {Microscopes}: {Past}, {Present} and {Future}},
	volume = {13},
	issn = {1662-5102},
	shorttitle = {Circuit {Investigations} {With} {Open}-{Source} {Miniaturized} {Microscopes}},
	url = {https://www.frontiersin.org/articles/10.3389/fncel.2019.00141},
	abstract = {The ability to simultaneously image the spatiotemporal activity signatures from many neurons during unrestrained vertebrate behaviors has become possible through the development of miniaturized fluorescence microscopes, or miniscopes, sufficiently light to be carried by small animals such as bats, birds and rodents. Miniscopes have permitted the study of circuits underlying song vocalization, action sequencing, head-direction tuning, spatial memory encoding and sleep to name a few. The foundation for these microscopes has been laid over the last two decades through academic research with some of this work resulting in commercialization. More recently, open-source initiatives have led to an even broader adoption of miniscopes in the neuroscience community. Open-source designs allow for rapid modification and extension of their function, which has resulted in a new generation of miniscopes that now permit wire-free or wireless recording, concurrent electrophysiology and imaging, two-color fluorescence detection, simultaneous optical actuation and read-out as well as wide-field and volumetric light-field imaging. These novel miniscopes will further expand the toolset of those seeking affordable methods to probe neural circuit function during naturalistic behaviors. Here, we will discuss the early development, present use and future potential of miniscopes.},
	urldate = {2023-02-24},
	journal = {Frontiers in Cellular Neuroscience},
	author = {Aharoni, Daniel and Hoogland, Tycho M.},
	year = {2019},
}

@article{weisenburger_guide_2018,
	title = {A {Guide} to {Emerging} {Technologies} for {Large}-{Scale} and {Whole}-{Brain} {Optical} {Imaging} of {Neuronal} {Activity}},
	volume = {41},
	issn = {1545-4126},
	doi = {10.1146/annurev-neuro-072116-031458},
	abstract = {The mammalian brain is a densely interconnected network that consists of millions to billions of neurons. Decoding how information is represented and processed by this neural circuitry requires the ability to capture and manipulate the dynamics of large populations at high speed and high resolution over a large area of the brain. Although the use of optical approaches by the neuroscience community has rapidly increased over the past two decades, most microscopy approaches are unable to record the activity of all neurons comprising a functional network across the mammalian brain at relevant temporal and spatial resolutions. In this review, we survey the recent development in optical technologies for Ca2+ imaging in this regard and provide an overview of the strengths and limitations of each modality and its potential for scalability. We provide guidance from the perspective of a biological user driven by the typical biological applications and sample conditions. We also discuss the potential for future advances and synergies that could be obtained through hybrid approaches or other modalities.},
	journal = {Annual Review of Neuroscience},
	author = {Weisenburger, Siegfried and Vaziri, Alipasha},
	month = jul,
	year = {2018},
	pmid = {29709208},
	pmcid = {PMC6037565},
	keywords = {Animals, Brain, Ca2+ imaging, functional brain network, high-speed optical neuronal recording, Humans, large-scale imaging, neural circuit dynamics, Neural Pathways, Neurons, Optical Imaging, volumetric imaging},
	pages = {431--452},
}

@article{skocek_high-speed_2018,
	title = {High-speed volumetric imaging of neuronal activity in freely moving rodents},
	volume = {15},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0008-0},
	doi = {10.1038/s41592-018-0008-0},
	abstract = {Thus far, optical recording of neuronal activity in freely behaving animals has been limited to a thin axial range. We present a head-mounted miniaturized light-field microscope (MiniLFM) capable of capturing neuronal network activity within a volume of 700 × 600 × 360 µm3 at 16 Hz in the hippocampus of freely moving mice. We demonstrate that neurons separated by as little as {\textbackslash}textasciitilde15 µm and at depths up to 360 µm can be discriminated.},
	number = {6},
	urldate = {2023-02-22},
	journal = {Nature Methods},
	author = {Skocek, Oliver and Nöbauer, Tobias and Weilguny, Lukas and Martínez Traub, Francisca and Xia, Chuying Naomi and Molodtsov, Maxim I. and Grama, Abhinav and Yamagata, Masahito and Aharoni, Daniel and Cox, David D. and Golshani, Peyman and Vaziri, Alipasha},
	month = jun,
	year = {2018},
	keywords = {Fluorescence imaging, Imaging, Mouse, Neuroscience},
	pages = {429--432},
	annote = {Number: 6 Publisher: Nature Publishing Group},
}

@article{oleary_experimental_1995,
	title = {Experimental images of heterogeneous turbid media by frequency-domain diffusing-photon tomography},
	volume = {20},
	issn = {1539-4794},
	url = {https://opg.optica.org/ol/abstract.cfm?uri=ol-20-5-426},
	doi = {10.1364/OL.20.000426},
	abstract = {We present images of heterogeneous turbid media derived from measurements of diffuse photon-density waves traveling through highly scattering tissue phantoms. To our knowledge, the images are the first experimental reconstruction based on data collected in the frequency domain. We demonstrate images of both absorbing and scattering heterogeneities and show that this method is sensitive to the optical properties of the heterogeneity. The algorithm employs a differential measurement scheme that reduces the effect of errors resulting from incorrect estimation of the background optical properties. The relative advantages of sources with low and high modulation frequency are discussed within this context.},
	number = {5},
	urldate = {2023-02-22},
	journal = {Optics Letters},
	author = {O’Leary, M. A. and Boas, D. A. and Chance, B. and Yodh, A. G.},
	month = mar,
	year = {1995},
	keywords = {Image quality, Optical properties, Photon density waves, Photon diffusion, Scattering, Tomography},
	pages = {426--428},
	annote = {Publisher: Optica Publishing Group},
}

@article{boas_scattering_1995,
	title = {Scattering and {Imaging} with {Diffusing} {Temporal} {Field} {Correlations}},
	volume = {75},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.75.1855},
	doi = {10.1103/PhysRevLett.75.1855},
	abstract = {We consider the transport of the electric field temporal autocorrelation in heterogeneous, fluctuating turbid media. Experiments are performed in strongly scattering media with spatially separated static and dynamic components, and low resolution “dynamical” images of such media are obtained using autocorrelation measurements of the emerging speckle fields taken along the sample surface. Our analysis, based on a diffusion approximation to the field correlation transport equation, reveals that the field correlation scatters from macroscopic dynamical heterogeneities within turbid media.},
	number = {9},
	urldate = {2023-02-22},
	journal = {Physical Review Letters},
	author = {Boas, D. A. and Campbell, L. E. and Yodh, A. G.},
	month = aug,
	year = {1995},
	pages = {1855--1858},
	annote = {Publisher: American Physical Society},
}

@article{zhang_computational_2021,
	title = {Computational optical sectioning with an incoherent multiscale scattering model for light-field microscopy},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26730-w},
	doi = {10.1038/s41467-021-26730-w},
	abstract = {Quantitative volumetric fluorescence imaging at high speed across a long term is vital to understand various cellular and subcellular behaviors in living organisms. Light-field microscopy provides a compact computational solution by imaging the entire volume in a tomographic way, while facing severe degradation in scattering tissue or densely-labelled samples. To address this problem, we propose an incoherent multiscale scattering model in a complete space for quantitative 3D reconstruction in complicated environments, which is called computational optical sectioning. Without the requirement of any hardware modifications, our method can be generally applied to different light-field schemes with reduction in background fluorescence, reconstruction artifacts, and computational costs, facilitating more practical applications of LFM in a broad community. We validate the superior performance by imaging various biological dynamics in Drosophila embryos, zebrafish larvae, and mice.},
	number = {1},
	urldate = {2023-02-17},
	journal = {Nature Communications},
	author = {Zhang, Yi and Lu, Zhi and Wu, Jiamin and Lin, Xing and Jiang, Dong and Cai, Yeyi and Xie, Jiachen and Wang, Yuling and Zhu, Tianyi and Ji, Xiangyang and Dai, Qionghai},
	month = nov,
	year = {2021},
	keywords = {3-D reconstruction, Ca2+ imaging, Data processing, Fluorescence imaging},
	pages = {6391},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{adams_vivo_2022-1,
	title = {In vivo lensless microscopy via a phase mask generating diffraction patterns with high-contrast contours},
	volume = {6},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00851-z},
	doi = {10.1038/s41551-022-00851-z},
	abstract = {The simple and compact optics of lensless microscopes and the associated computational algorithms allow for large fields of view and the refocusing of the captured images. However, existing lensless techniques cannot accurately reconstruct the typical low-contrast images of optically dense biological tissue. Here we show that lensless imaging of tissue in vivo can be achieved via an optical phase mask designed to create a point spread function consisting of high-contrast contours with a broad spectrum of spatial frequencies. We built a prototype lensless microscope incorporating the ‘contour’ phase mask and used it to image calcium dynamics in the cortex of live mice (over a field of view of about 16 mm2) and in freely moving Hydra vulgaris, as well as microvasculature in the oral mucosa of volunteers. The low cost, small form factor and computational refocusing capability of in vivo lensless microscopy may open it up to clinical uses, especially for imaging difficult-to-reach areas of the body.},
	number = {5},
	urldate = {2023-02-17},
	journal = {Nature Biomedical Engineering},
	author = {Adams, Jesse K. and Yan, Dong and Wu, Jimin and Boominathan, Vivek and Gao, Sibo and Rodriguez, Alex V. and Kim, Soonyoung and Carns, Jennifer and Richards-Kortum, Rebecca and Kemere, Caleb and Veeraraghavan, Ashok and Robinson, Jacob T.},
	month = may,
	year = {2022},
	keywords = {3-D reconstruction, Applied optics, Ca2+ imaging, Fluorescence imaging, Optical imaging},
	pages = {617--628},
	annote = {Number: 5 Publisher: Nature Publishing Group},
}

@article{cheng_comparing_2020,
	title = {Comparing the fundamental imaging depth limit of two-photon, three-photon, and non-degenerate two-photon microscopy},
	volume = {45},
	issn = {1539-4794},
	url = {https://opg.optica.org/ol/abstract.cfm?uri=ol-45-10-2934},
	doi = {10.1364/OL.392724},
	abstract = {We have systematically characterized the degradation of imaging quality with depth in deep brain multi-photon microscopy, utilizing our recently developed numerical model that computes wave propagation in scattering media. The signal-to-background ratio (SBR) and the resolution determined by the width of the point spread function are obtained as functions of depth. We compare the imaging quality of two-photon (2PM), three-photon (3PM), and non-degenerate two-photon microscopy (ND-2PM) for mouse brain imaging. We show that the imaging depth of 2PM and ND-2PM are fundamentally limited by the SBR, while the SBR remains approximately invariant with imaging depth for 3PM. Instead, the imaging depth of 3PM is limited by the degradation of the resolution, if there is sufficient laser power to maintain the signal level at large depth. The roles of the concentration of dye molecules, the numerical aperture of the input light, the anisotropy factor g, noise level, input laser power, and the effect of temporal broadening are also discussed.},
	number = {10},
	urldate = {2023-02-17},
	journal = {Optics Letters},
	author = {Cheng, Xiaojun and Sadegh, Sanaz and Zilpelwar, Sharvari and Devor, Anna and Tian, Lei and Boas, David A.},
	month = may,
	year = {2020},
	keywords = {Brain imaging, Image quality, Multiphoton microscopy, Multiple scattering, Scattering media, Wave propagation},
	pages = {2934--2937},
	annote = {Publisher: Optica Publishing Group},
}

@article{lee_deep_2023,
	title = {Deep learning based on parameterized physical forward model for adaptive holographic imaging with unpaired data},
	volume = {5},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00584-3},
	doi = {10.1038/s42256-022-00584-3},
	abstract = {Holographic imaging poses the ill posed inverse mapping problem of retrieving complex amplitude maps from measured diffraction intensity patterns. The existing deep learning methods for holographic imaging often depend solely on the statistical relation between the given data distributions, compromising their reliability in practical imaging configurations where physical perturbations exist in various forms, such as mechanical movement and optical fluctuation. Here, we present a deep learning method based on a parameterized physical forward model that reconstructs both the complex amplitude and the range of objects under highly perturbative configurations where the object-to-sensor distance is set beyond the range of given training data. To prove reliability in practical biomedical applications, we demonstrate holographic imaging of red blood cells flowing in a cluster and diverse types of tissue section presented without any ground truth data. Our results suggest that the proposed approach permits the adaptability of deep learning methods to deterministic perturbations, and therefore extends their applicability to a wide range of inverse problems in imaging.},
	number = {1},
	urldate = {2023-02-15},
	journal = {Nature Machine Intelligence},
	author = {Lee, Chanseok and Song, Gookho and Kim, Hyeonggeon and Ye, Jong Chul and Jang, Mooseok},
	month = jan,
	year = {2023},
	keywords = {Biomedical engineering, Computational science, Diagnosis, Imaging and sensing, Optics and photonics},
	pages = {35--45},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{liu_recovery_2022,
	title = {Recovery of continuous {3D} refractive index maps from discrete intensity-only measurements using neural fields},
	volume = {4},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00530-3},
	doi = {10.1038/s42256-022-00530-3},
	abstract = {Intensity diffraction tomography (IDT) refers to a class of optical microscopy techniques for imaging the three-dimensional refractive index (RI) distribution of a sample from a set of two-dimensional intensity-only measurements. The reconstruction of artefact-free RI maps is a fundamental challenge in IDT due to the loss of phase information and the missing-cone problem. Neural fields has recently emerged as a new deep learning approach for learning continuous representations of physical fields. The technique uses a coordinate-based neural network to represent the field by mapping the spatial coordinates to the corresponding physical quantities, in our case the complex-valued refractive index values. We present Deep Continuous Artefact-free RI Field (DeCAF) as a neural-fields-based IDT method that can learn a high-quality continuous representation of a RI volume from its intensity-only and limited-angle measurements. The representation in DeCAF is learned directly from the measurements of the test sample by using the IDT forward model without any ground-truth RI maps. We qualitatively and quantitatively evaluate DeCAF on the simulated and experimental biological samples. Our results show that DeCAF can generate high-contrast and artefact-free RI maps and lead to an up to 2.1-fold reduction in the mean squared error over existing methods.},
	number = {9},
	urldate = {2023-02-15},
	journal = {Nature Machine Intelligence},
	author = {Liu, Renhao and Sun, Yu and Zhu, Jiabei and Tian, Lei and Kamilov, Ulugbek S.},
	month = sep,
	year = {2022},
	keywords = {3-D reconstruction, Optical imaging},
	pages = {781--791},
	annote = {Number: 9 Publisher: Nature Publishing Group},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
	annote = {ISSN: 1063-6919},
}


@misc{su_is_2019,
	title = {Is {Robustness} the {Cost} of {Accuracy}? – {A} {Comprehensive} {Study} on the {Robustness} of 18 {Deep} {Image} {Classification} {Models}},
	shorttitle = {Is {Robustness} the {Cost} of {Accuracy}?},
	url = {http://arxiv.org/abs/1808.01688},
	abstract = {The prediction accuracy has been the long-lasting and sole standard for comparing the performance of different image classification models, including the ImageNet competition. However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. Visually imperceptible perturbations to natural images can easily be crafted and mislead the image classifiers towards misclassification. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models. Our extensive experimental results reveal several new insights: (1) linear scaling law - the empirical \{\vphantom{\}}{\textbackslash}textbackslashell\_2{\textbackslash} and \{\vphantom{\}}{\textbackslash}textbackslashell\_{\textbackslash}textbackslashinfty{\textbackslash} distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness Pareto frontier can be used as an evaluation criterion for ImageNet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in \{\vphantom{\}}{\textbackslash}textbackslashell\_{\textbackslash}textbackslashinfty{\textbackslash} distortion; (4) there exist models (in VGG family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family. Experiment code is publicly available at {\textbackslash}textbackslashurl\{https://github.com/huanzhang12/Adversarial\_Survey\}.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Su, Dong and Zhang, Huan and Chen, Hongge and Yi, Jinfeng and Chen, Pin-Yu and Gao, Yupeng},
	month = mar,
	year = {2019},
	doi = {10.48550/arXiv.1808.01688},
	note = {Issue: arXiv:1808.01688
arXiv: 1808.01688 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{deng_learning_2020,
	title = {Learning to synthesize: robust phase retrieval at low photon counts},
	volume = {9},
	issn = {2047-7538},
	shorttitle = {Learning to synthesize},
	url = {https://www.nature.com/articles/s41377-020-0267-2},
	doi = {10.1038/s41377-020-0267-2},
	abstract = {The quality of inverse problem solutions obtained through deep learning is limited by the nature of the priors learned from examples presented during the training phase. Particularly in the case of quantitative phase retrieval, spatial frequencies that are underrepresented in the training database, most often at the high band, tend to be suppressed in the reconstruction. Ad hoc solutions have been proposed, such as pre-amplifying the high spatial frequencies in the examples; however, while that strategy improves the resolution, it also leads to high-frequency artefacts, as well as low-frequency distortions in the reconstructions. Here, we present a new approach that learns separately how to handle the two frequency bands, low and high, and learns how to synthesize these two bands into full-band reconstructions. We show that this “learning to synthesize” (LS) method yields phase reconstructions of high spatial resolution and without artefacts and that it is resilient to high-noise conditions, e.g., in the case of very low photon flux. In addition to the problem of quantitative phase retrieval, the LS method is applicable, in principle, to any inverse problem where the forward operator treats different frequency bands unevenly, i.e., is ill-posed.},
	number = {1},
	urldate = {2023-01-26},
	journal = {Light: Science \& Applications},
	author = {Deng, Mo and Li, Shuai and Goy, Alexandre and Kang, Iksung and Barbastathis, George},
	month = mar,
	year = {2020},
	keywords = {Applied optics, Imaging and sensing},
	pages = {36},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{yao_early_2007,
	title = {On {Early} {Stopping} in {Gradient} {Descent} {Learning}},
	volume = {26},
	issn = {1432-0940},
	url = {https://doi.org/10.1007/s00365-006-0663-2},
	doi = {10.1007/s00365-006-0663-2},
	abstract = {In this paper we study a family of gradient descent algorithms to approximate the regression function from reproducing kernel Hilbert spaces (RKHSs), the family being characterized by a polynomial decreasing rate of step sizes (or learning rate). By solving a bias-variance trade-off we obtain an early stopping rule and someprobabilistic upper bounds for the convergence of the algorithms. We also discuss the implication of these results in the context of classification where some fast convergence rates can be achieved for plug-in classifiers. Some connections are addressed with Boosting, Landweber iterations, and the online learning algorithms as stochastic approximations of the gradient descent method.},
	number = {2},
	urldate = {2023-01-26},
	journal = {Constructive Approximation},
	author = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
	month = aug,
	year = {2007},
	keywords = {Convergence Rate, Gradient Descent, Gradient Descent Method, Reproduce Kernel Hilbert Space, Tikhonov Regularization},
	pages = {289--315},
}

@article{matlock_multiple-scattering_2023,
	title = {Multiple-scattering simulator-trained neural network for intensity diffraction tomography},
	volume = {31},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-31-3-4094},
	doi = {10.1364/OE.477396},
	abstract = {Recovering 3D phase features of complex biological samples traditionally sacrifices computational efficiency and processing time for physical model accuracy and reconstruction quality. Here, we overcome this challenge using an approximant-guided deep learning framework in a high-speed intensity diffraction tomography system. Applying a physics model simulator-based learning strategy trained entirely on natural image datasets, we show our network can robustly reconstruct complex 3D biological samples. To achieve highly efficient training and prediction, we implement a lightweight 2D network structure that utilizes a multi-channel input for encoding the axial information. We demonstrate this framework on experimental measurements of weakly scattering epithelial buccal cells and strongly scattering C. elegans worms. We benchmark the network\&\#x2019;s performance against a state-of-the-art multiple-scattering model-based iterative reconstruction algorithm. We highlight the network\&\#x2019;s robustness by reconstructing dynamic samples from a living worm video. We further emphasize the network\&\#x2019;s generalization capabilities by recovering algae samples imaged from different experimental setups. To assess the prediction quality, we develop a quantitative evaluation metric to show that our predictions are consistent with both multiple-scattering physics and experimental measurements.},
	number = {3},
	urldate = {2023-01-26},
	journal = {Optics Express},
	author = {Matlock, Alex and Zhu, Jiabei and Tian, Lei},
	month = jan,
	year = {2023},
	pages = {4094--4107},
	annote = {Publisher: Optica Publishing Group},
}

@misc{noauthor_recovery_nodate,
	title = {Recovery of continuous {3D} refractive index maps from discrete intensity-only measurements using neural fields {\textbackslash}textbar {Nature} {Machine} {Intelligence}},
	url = {https://www.nature.com/articles/s42256-022-00530-3},
	urldate = {2023-01-26},
}

@article{ning_deep-learning-based_2020,
	title = {Deep-learning-based whole-brain imaging at single-neuron resolution},
	volume = {11},
	issn = {2156-7085},
	url = {https://opg.optica.org/boe/abstract.cfm?uri=boe-11-7-3567},
	doi = {10.1364/BOE.393081},
	abstract = {Obtaining fine structures of neurons is necessary for understanding brain function. Simple and effective methods for large-scale 3D imaging at optical resolution are still lacking. Here, we proposed a deep-learning-based fluorescence micro-optical sectioning tomography (DL-fMOST) method for high-throughput, high-resolution whole-brain imaging. We utilized a wide-field microscope for imaging, a U-net convolutional neural network for real-time optical sectioning, and histological sectioning for exceeding the imaging depth limit. A 3D dataset of a mouse brain with a voxel size of 0.32 \&\#x00D7; 0.32 \&\#x00D7; 2 \&\#x00B5;m was acquired in 1.5 days. We demonstrated the robustness of DL-fMOST for mouse brains with labeling of different types of neurons.},
	number = {7},
	urldate = {2023-01-26},
	journal = {Biomedical Optics Express},
	author = {Ning, Kefu and Zhang, Xiaoyu and Gao, Xuefei and Jiang, Tao and Wang, He and Chen, Siqi and Li, Anan and Yuan, Jing},
	month = jul,
	year = {2020},
	keywords = {Image quality, Imaging techniques, Medical imaging, Optical imaging, Three dimensional imaging, Two photon imaging},
	pages = {3567--3584},
	annote = {Publisher: Optica Publishing Group},
}

@article{zhang_deep_2018,
	title = {Deep learning optical-sectioning method},
	volume = {26},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-26-23-30762},
	doi = {10.1364/OE.26.030762},
	abstract = {Current optical-sectioning methods require complex optical system or considerable computation time to improve imaging quality. Here we propose a deep learning-based method for optical sectioning of wide-field images. This method only needs one pair of contrast images for training to facilitate reconstruction of an optically sectioned image. The removal effect of background information and resolution that is achievable with our technique is similar to traditional optical-sectioning methods, but offers lower noise levels and a higher imaging depth. Moreover, reconstruction speed can be optimized to 14 Hz. This cost-effective and convenient method enables high-throughput optical sectioning techniques to be developed.},
	number = {23},
	urldate = {2023-01-26},
	journal = {Optics Express},
	author = {Zhang, Xiaoyu and Chen, Yifan and Ning, Kefu and Zhou, Can and Han, Yutong and Gong, Hui and Yuan, Jing},
	month = nov,
	year = {2018},
	keywords = {Image processing, Imaging techniques, Light sheet microscopy, Optical transfer functions, Phase recovery, Structured illumination microscopy},
	pages = {30762--30772},
	annote = {Publisher: Optica Publishing Group},
}

@misc{noauthor_deep_nodate,
	title = {Deep learning based on parameterized physical forward model for adaptive holographic imaging with unpaired data {\textbackslash}textbar {Nature} {Machine} {Intelligence}},
	url = {https://www.nature.com/articles/s42256-022-00584-3},
	urldate = {2023-01-26},
}

@article{guan_compensating_2021,
	title = {Compensating the cell-induced light scattering effect in light-based bioprinting using deep learning},
	volume = {14},
	issn = {1758-5090},
	url = {https://dx.doi.org/10.1088/1758-5090/ac3b92},
	doi = {10.1088/1758-5090/ac3b92},
	abstract = {Digital light processing (DLP)-based three-dimensional (3D) printing technology has the advantages of speed and precision comparing with other 3D printing technologies like extrusion-based 3D printing. Therefore, it is a promising biomaterial fabrication technique for tissue engineering and regenerative medicine. When printing cell-laden biomaterials, one challenge of DLP-based bioprinting is the light scattering effect of the cells in the bioink, and therefore induce unpredictable effects on the photopolymerization process. In consequence, the DLP-based bioprinting requires extra trial-and-error efforts for parameters optimization for each specific printable structure to compensate the scattering effects induced by cells, which is often difficult and time-consuming for a machine operator. Such trial-and-error style optimization for each different structure is also very wasteful for those expensive biomaterials and cell lines. Here, we use machine learning to learn from a few trial sample printings and automatically provide printer the optimal parameters to compensate the cell-induced scattering effects. We employ a deep learning method with a learning-based data augmentation which only requires a small amount of training data. After learning from the data, the algorithm can automatically generate the printer parameters to compensate the scattering effects. Our method shows strong improvement in the intra-layer printing resolution for bioprinting, which can be further extended to solve the light scattering problems in multilayer 3D bioprinting processes.},
	number = {1},
	urldate = {2023-01-26},
	journal = {Biofabrication},
	author = {Guan, Jiaao and You, Shangting and Xiang, Yi and Schimelman, Jacob and Alido, Jeffrey and Ma, Xinyue and Tang, Min and Chen, Shaochen},
	month = dec,
	year = {2021},
	pages = {015011},
	annote = {Publisher: IOP Publishing},
}

@misc{noauthor_adaptive_nodate,
	title = {Adaptive {3D} descattering with a dynamic synthesis network {\textbackslash}textbar {Light}: {Science} \& {Applications}},
	url = {https://www.nature.com/articles/s41377-022-00730-x},
	urldate = {2023-01-26},
}

@article{kim_long-term_2016,
	title = {Long-{Term} {Optical} {Access} to an {Estimated} {One} {Million} {Neurons} in the {Live} {Mouse} {Cortex}},
	volume = {17},
	issn = {2211-1247},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5459490/},
	doi = {10.1016/j.celrep.2016.12.004},
	abstract = {A major technological goal in neuroscience is to enable the interrogation of individual cells across the live brain. By creating a curved glass replacement to the dorsal cranium and surgical methods for its installation, we developed a chronic mouse preparation providing optical access to an estimated 800,000–1,100,000 individual neurons across the dorsal surface of neocortex. Post-surgical histological studies revealed comparable glial activation as in control mice. In behaving mice expressing a Ca2+ indicator in cortical pyramidal neurons, we performed Ca2+ imaging across neocortex using an epi-fluorescence macroscope and estimated that 25,000–50,000 individual neurons were accessible per mouse across multiple focal planes. Two-photon microscopy revealed dendritic morphologies throughout neocortex, allowed time-lapse imaging of individual cells, and yielded estimates of {\textbackslash}textgreater1 million accessible neurons per mouse by serial tiling. This approach supports a variety of optical techniques and enables studies of cells across {\textbackslash}textgreater30 neocortical areas in behaving mice., Kim et al. present a preparation for long-term imaging in which a curved glass window replaces the mouse dorsal cranium. This method enables large-scale Ca2+ imaging of neuronal dynamics across neocortex in behaving mice and yields an estimated {\textbackslash}textgreater1 million optically accessible neurons by two-photon microscopy.,},
	number = {12},
	urldate = {2023-01-26},
	journal = {Cell reports},
	author = {Kim, Tony Hyun and Zhang, Yanping and Lecoq, Jérôme and Jung, Juergen C. and Li, Jane and Zeng, Hongkui and Niell, Cristopher M. and Schnitzer, Mark J.},
	month = dec,
	year = {2016},
	pmid = {28009304},
	pmcid = {PMC5459490},
	pages = {3385--3394},
}

@article{nichani_deeper_2021,
	title = {Do {Deeper} {Convolutional} {Networks} {Perform} {Better}?},
	url = {https://openreview.net/forum?id=rYt0p0Um9r},
	abstract = {Over-parameterization is a recent topic of much interest in the machine learning community. While over-parameterized neural networks are capable of perfectly fitting (interpolating) training data, these networks often perform well on test data, thereby contradicting classical learning theory. Recent work provided an explanation for this phenomenon by introducing the double descent curve, showing that increasing model capacity past the interpolation threshold leads to a decrease in test error. In line with this, it was recently shown empirically and theoretically that increasing neural network capacity through width leads to double descent. In this work, we analyze the effect of increasing depth on test performance. In contrast to what is observed for increasing width, we demonstrate through a variety of classification experiments on CIFAR10 and ImageNet-32 using ResNets and fully-convolutional networks that test performance worsens beyond a critical depth. We posit an explanation for this phenomenon by drawing intuition from the principle of minimum norm solutions in linear networks.},
	urldate = {2023-01-25},
	author = {Nichani, Eshaan and Radhakrishnan, Adityanarayanan and Uhler, Caroline},
	month = mar,
	year = {2021},
}

@article{lei_how_2022,
	title = {How {Training} {Data} {Affect} the {Accuracy} and {Robustness} of {Neural} {Networks} for {Image} {Classification}},
	url = {https://openreview.net/forum?id=HklKWhC5F7},
	abstract = {Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples. For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications. In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural networks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease. How and when such turning points occur vary for different neural networks and different datasets.},
	urldate = {2023-01-25},
    journal = {""},
	author = {Lei, Suhua and Zhang, Huan and Wang, Ke and Su, Zhendong},
	month = feb,
	year = {2022},
}

@article{aharoni_all_2019,
	title = {All the light that we can see: a new era in miniaturized microscopy},
	volume = {16},
	issn = {1548-7105},
	shorttitle = {All the light that we can see},
	url = {https://www.nature.com/articles/s41592-018-0266-x},
	doi = {10.1038/s41592-018-0266-x},
	abstract = {One major challenge in neuroscience is to uncover how defined neural circuits in the brain encode, store, modify, and retrieve information. Meeting this challenge comprehensively requires tools capable of recording and manipulating the activity of intact neural networks in naturally behaving animals. Head-mounted miniature microscopes are emerging as a key tool to address this challenge. Here we discuss recent work leading to the miniaturization of neural imaging tools, the current state of the art in this field, and the importance and necessity of open-source options. We finish with a discussion on what the future may hold for miniature microscopy.},
	number = {1},
	urldate = {2023-01-25},
	journal = {Nature Methods},
	author = {Aharoni, Daniel and Khakh, Baljit S. and Silva, Alcino J. and Golshani, Peyman},
	month = jan,
	year = {2019},
	keywords = {Biological techniques, Neuroscience},
	pages = {11--13},
	annote = {Number: 1 Publisher: Nature Publishing Group},
}

@article{li_deep-3d_2022,
	title = {Deep-{3D} microscope: {3D} volumetric microscopy of thick scattering samples using a wide-field microscope and machine learning},
	volume = {13},
	issn = {2156-7085},
	shorttitle = {Deep-{3D} microscope},
	url = {https://opg.optica.org/boe/abstract.cfm?uri=boe-13-1-284},
	doi = {10.1364/BOE.444488},
	abstract = {Confocal microscopy is a standard approach for obtaining volumetric images of a sample with high axial and lateral resolution, especially when dealing with scattering samples. Unfortunately, a confocal microscope is quite expensive compared to traditional microscopes. In addition, the point scanning in confocal microscopy leads to slow imaging speed and photobleaching due to the high dose of laser energy. In this paper, we demonstrate how the advances in machine learning can be exploited to \&\#x0022;teach\&\#x0022; a traditional wide-field microscope, one that\&\#x2019;s available in every lab, into producing 3D volumetric images like a confocal microscope. The key idea is to obtain multiple images with different focus settings using a wide-field microscope and use a 3D generative adversarial network (GAN) based neural network to learn the mapping between the blurry low-contrast image stacks obtained using a wide-field microscope and the sharp, high-contrast image stacks obtained using a confocal microscope. After training the network with widefield-confocal stack pairs, the network can reliably and accurately reconstruct 3D volumetric images that rival confocal images in terms of its lateral resolution, z-sectioning and image contrast. Our experimental results demonstrate generalization ability to handle unseen data, stability in the reconstruction results, high spatial resolution even when imaging thick (\&\#x223C;40 microns) highly-scattering samples. We believe that such learning-based microscopes have the potential to bring confocal imaging quality to every lab that has a wide-field microscope.},
	number = {1},
	urldate = {2023-01-23},
	journal = {Biomedical Optics Express},
	author = {Li, Bowen and Tan, Shiyu and Dong, Jiuyang and Lian, Xiaocong and Zhang, Yongbing and Ji, Xiangyang and Veeraraghavan, Ashok},
	month = jan,
	year = {2022},
	keywords = {Confocal microscopy, High power lasers, Image quality, Medical imaging, Spatial resolution, Three dimensional microscopy},
	pages = {284--299},
	annote = {Publisher: Optica Publishing Group},
}

@article{song_neural_2021,
	title = {Neural anatomy and optical microscopy ({NAOMi}) simulation for evaluating calcium imaging methods},
	volume = {358},
	issn = {0165-0270},
	url = {https://www.sciencedirect.com/science/article/pii/S0165027021001084},
	doi = {10.1016/j.jneumeth.2021.109173},
	abstract = {Background The past decade has seen a multitude of new in vivo functional imaging methodologies. However, the lack of ground-truth comparisons or evaluation metrics makes the large-scale, systematic validation vital to the continued development and use of optical microscopy impossible. New-method We provide a new framework for evaluating two-photon microscopy methods via in silico Neural Anatomy and Optical Microscopy (NAOMi) simulation. Our computationally efficient model generates large anatomical volumes of mouse cortex, simulates neural activity, and incorporates optical propagation and scanning to create realistic calcium imaging datasets. Results We verify NAOMi simulations against in vivo two-photon recordings from mouse cortex. We leverage this in silico ground truth to directly compare different segmentation algorithms and optical designs. We find modern segmentation algorithms extract strong neural time-courses comparable to estimation using oracle spatial information, but with an increase in the false positive rate. Comparison between optical setups demonstrate improved resilience to motion artifacts in sparsely labeled samples using Bessel beams, increased signal-to-noise ratio and cell-count using low numerical aperture Gaussian beams and nuclear GCaMP, and more uniform spatial sampling with temporal focusing versus multi-plane imaging. Comparison with existing methods NAOMi is a first-of-its kind framework for assessing optical imaging modalities. Existing methods are either anatomical simulations or do not address functional imaging. Thus there is no competing method for simulating realistic functional optical microscopy data. Conclusions By leveraging the rich accumulated knowledge of neural anatomy and optical physics, we provide a powerful new tool to assess and develop important methods in neural imaging.},
	urldate = {2023-01-20},
	journal = {Journal of Neuroscience Methods},
	author = {Song, Alexander and Gauthier, Jeff L. and Pillow, Jonathan W. and Tank, David W. and Charles, Adam S.},
	month = jul,
	year = {2021},
	keywords = {Large-scale recordings, Neural data analysis, Neural simulation, Two-photon microscopy},
	pages = {109173},
}

@misc{262588213843476_2d_nodate,
	title = {{2D} and {3D} {Perlin} {Noise} in {MATLAB}},
	url = {https://gist.github.com/OrganicIrradiation/2a927ab7f9cfeb1bff78},
	abstract = {2D and 3D Perlin Noise in MATLAB. GitHub Gist: instantly share code, notes, and snippets.},
	urldate = {2023-11-17},
	author = {{262588213843476}},
	file = {Snapshot:C\:\\Users\\jeffr\\Zotero\\storage\\DHIMTTTS\\2a927ab7f9cfeb1bff78.html:text/html},
}

@misc{noauthor_httpsgithubcombu-cislcomputational-miniature-mesoscope-cm2_nodate,
	title = {https://github.com/bu-cisl/{Computational}-{Miniature}-{Mesoscope}-{CM2}},
	url = {https://github.com/bu-cisl/Computational-Miniature-Mesoscope-CM2},
}

@article{boyd_distributed_2010,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	number = {1},
	urldate = {2023-01-19},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Boyd, Stephen},
	year = {2010},
	pages = {1--122},
}

@misc{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = may,
	year = {2017},
	doi = {10.48550/arXiv.1608.03983},
	note = {Issue: arXiv:1608.03983
arXiv: 1608.03983 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@article{mengual_turbiscan_1999,
	title = {{TURBISCAN} {MA} 2000: multiple light scattering measurement for concentrated emulsion and suspension instability analysis},
	volume = {50},
	issn = {0039-9140},
	shorttitle = {{TURBISCAN} {MA} 2000},
	url = {https://www.sciencedirect.com/science/article/pii/S0039914099001290},
	doi = {10.1016/S0039-9140(99)00129-0},
	abstract = {Emulsion or suspension destabilisation often results from coalescence or particle aggregation (flocculation) leading to particle migration (creaming or sedimentation). Creaming and sedimentation are often considered as reversible, while coalescence and flocculation spell disaster for the formulator. Thus, it is of prime importance to detect coalescence or cluster formation at an early stage to shorten the ageing tests and to improve the formulations. This work mainly concerns the independent and anisotropic scattering of light from an emulsion or suspension in a cylindrical glass measurement cell, in relation with the optical analyser TURBISCAN MA 2000. The propagation of light through a concentrated dispersion can be used to characterise the system physico-chemical stability. Indeed, photons undergo many scattering events in an optically thick dispersion before escaping the medium and entering a receiver aperture. Multiple scattering thus contributes significantly to the transmitted and backscattered flux measured by TURBISCAN MA 2000. We present statistical models and numerical simulations for the radiative transfer in a suspension (plane or cylindrical measurement cells) only involving the photon mean path length, the asymmetry factor and the geometry of the light receivers. We further have developed an imaging method with high grey level resolution for the visualisation and the analysis of the surface flux in the backscattered spot light. We compare the results from physical models and numerical simulations with the experiments performed with the imaging method and the optical analyser TURBISCAN MA 2000 for latex beads suspensions (variable size and particle volume fraction). We then present a few examples of concentrated emulsion and suspension instability analysis with TURBISCAN 2000. It is shown that the instrument is able to characterise particle or aggregate size variation and particle/aggregate migration and to detect these phenomena much more earlier than the operator’s naked eye, especially for concentrated and optically thick media.},
	number = {2},
	urldate = {2023-01-19},
	journal = {Talanta},
	author = {Mengual, O. and Meunier, G. and Cayré, I. and Puech, K. and Snabre, P.},
	month = sep,
	year = {1999},
	keywords = {Characterisation, Concentrated dispersion, Instability, Multiple light scattering},
	pages = {445--456},
}

@article{pegard_compressive_2016,
	title = {Compressive light-field microscopy for {3D} neural activity recording},
	volume = {3},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-3-5-517},
	doi = {10.1364/OPTICA.3.000517},
	abstract = {Understanding the mechanisms of perception, cognition, and behavior requires instruments that are capable of recording and controlling the electrical activity of many neurons simultaneously and at high speeds. All-optical approaches are particularly promising since they are minimally invasive and potentially scalable to experiments interrogating thousands or millions of neurons. Conventional light-field microscopy provides a single-shot 3D fluorescence capture method with good light efficiency and fast speed, but suffers from low spatial resolution and significant image degradation due to scattering in deep layers of brain tissue. Here, we propose a new compressive light-field microscopy method to address both problems, offering a path toward measurement of individual neuron activity across large volumes of tissue. The technique relies on spatial and temporal sparsity of fluorescence signals, allowing one to identify and localize each neuron in a 3D volume, with scattering and aberration effects naturally included and without ever reconstructing a volume image. Experimental results on live zebrafish track the activity of an estimated 800+ neural structures at 100 Hz sampling rate.},
	number = {5},
	urldate = {2023-01-19},
	journal = {Optica},
	author = {Pégard, Nicolas C. and Liu, Hsiou-Yuan and Antipa, Nick and Gerlock, Maximillian and Adesnik, Hillel and Waller, Laura},
	month = may,
	year = {2016},
	keywords = {Light sheet microscopy, Multiphoton microscopy, Multiple scattering, Spatial resolution, Three dimensional imaging, Three dimensional microscopy},
	pages = {517--524},
	annote = {Publisher: Optica Publishing Group},
}

@article{kauvar_cortical_2020,
	title = {Cortical {Observation} by {Synchronous} {Multifocal} {Optical} {Sampling} {Reveals} {Widespread} {Population} {Encoding} of {Actions}},
	volume = {107},
	issn = {0896-6273},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7687350/},
	doi = {10.1016/j.neuron.2020.04.023},
	abstract = {To advance the measurement of distributed neuronal population representations of targeted motor actions on single trials, we developed an optical method (COSMOS) for tracking neural activity in a largely uncharacterized spatiotemporal regime. COSMOS allowed simultaneous recording of neural dynamics at {\textbackslash}textasciitilde30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice during a three-option lick-to-target task. We identified spatially distributed neuronal population representations spanning the dorsal cortex that precisely encoded ongoing motor actions on single trials. Neuronal correlations measured at video rate using unaveraged, whole-session data had localized spatial structure, whereas trial-averaged data exhibited widespread correlations. Separable modes of neural activity encoded history-guided motor plans, with similar population dynamics in individual areas throughout cortex. These initial experiments illustrate how COSMOS enables investigation of large-scale cortical dynamics and that information about motor actions is widely shared between areas, potentially underlying distributed computations., Kauvar, Machado, et al. have developed a new method, COSMOS, to simultaneously record neural dynamics at {\textbackslash}textasciitilde30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice. With COSMOS, they observe cortex-spanning population encoding of actions during a three-option lick-to-target task.},
	number = {2},
	urldate = {2023-01-19},
	journal = {Neuron},
	author = {Kauvar, Isaac V. and Machado, Timothy A. and Yuen, Elle and Kochalka, John and Choi, Minseung and Allen, William E. and Wetzstein, Gordon and Deisseroth, Karl},
	month = jul,
	year = {2020},
	pmid = {32433908},
	pmcid = {PMC7687350},
	pages = {351--367.e19},
}

@article{antipa_diffusercam_2018,
	title = {{DiffuserCam}: lensless single-exposure {3D} imaging},
	volume = {5},
	issn = {2334-2536},
	shorttitle = {{DiffuserCam}},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-5-1-1},
	doi = {10.1364/OPTICA.5.000001},
	abstract = {We demonstrate a compact, easy-to-build computational camera for single-shot three-dimensional (3D) imaging. Our lensless system consists solely of a diffuser placed in front of an image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D reconstruction grid is chosen to match the experimentally measured two-point optical resolution, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide a new theory for analyzing resolution in such systems.},
	number = {1},
	urldate = {2023-01-10},
	journal = {Optica},
	author = {Antipa, Nick and Kuo, Grace and Heckel, Reinhard and Mildenhall, Ben and Bostan, Emrah and Ng, Ren and Waller, Laura},
	month = jan,
	year = {2018},
	keywords = {Image quality, Image sensors, Optical systems, Plenoptic imaging, Systems design, Three dimensional imaging},
	pages = {1--9},
	annote = {Publisher: Optica Publishing Group},
}

@misc{noauthor_value_nodate,
	title = {Value {Noise} and {Procedural} {Patterns}: {Part} 1},
	url = {https://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/procedural-patterns-noise-part-1/creating-simple-1D-noise.html},
	urldate = {2023-01-10},
}

@misc{262588213843476_2d_nodate-1,
	title = {{2D} and {3D} {Perlin} {Noise} in {MATLAB}},
	url = {https://gist.github.com/OrganicIrradiation/2a927ab7f9cfeb1bff78},
	abstract = {2D and 3D Perlin Noise in MATLAB. GitHub Gist: instantly share code, notes, and snippets.},
	urldate = {2023-01-10},
	author = {{262588213843476}},
}

@article{li_deep_2018,
	title = {Deep speckle correlation: a deep learning approach toward scalable imaging through scattering media},
	volume = {5},
	issn = {2334-2536},
	shorttitle = {Deep speckle correlation},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-5-10-1181},
	doi = {10.1364/OPTICA.5.001181},
	abstract = {Imaging through scattering is an important yet challenging problem. Tremendous progress has been made by exploiting the deterministic input\&\#x2013;output \&\#x201C;transmission matrix\&\#x201D; for a fixed medium. However, this \&\#x201C;one-to-one\&\#x201D; mapping is highly susceptible to speckle decorrelations \&\#x2013; small perturbations to the scattering medium lead to model errors and severe degradation of the imaging performance. Our goal here is to develop a new framework that is highly scalable to both medium perturbations and measurement requirement. To do so, we propose a statistical \&\#x201C;one-to-all\&\#x201D; deep learning (DL) technique that encapsulates a wide range of statistical variations for the model to be resilient to speckle decorrelations. Specifically, we develop a convolutional neural network (CNN) that is able to learn the statistical information contained in the speckle intensity patterns captured on a set of diffusers having the same macroscopic parameter. We then show for the first time, to the best of our knowledge, that the trained CNN is able to generalize and make high-quality object predictions through an entirely different set of diffusers of the same class. Our work paves the way to a highly scalable DL approach for imaging through scattering media.},
	number = {10},
	urldate = {2023-01-09},
	journal = {Optica},
	author = {Li, Yunzhe and Xue, Yujia and Tian, Lei},
	month = oct,
	year = {2018},
	keywords = {Absorption coefficient, Light scattering, Multiple scattering, Neural networks, Scattering media, Speckle patterns},
	pages = {1181--1190},
	annote = {Publisher: Optica Publishing Group},
}
