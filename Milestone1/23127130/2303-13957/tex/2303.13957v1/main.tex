\documentclass[lettersize,journal]{IEEEtran}




\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\usepackage[caption=false,font=footnotesize]{subfig}

% updated with editorial comments 8/9/2021

\usepackage{svg}
\usepackage{stackengine} 
\newcommand\oast{\stackMath\mathbin{\stackinset{c}{0ex}{c}{0ex}{\ast}{\bigcirc}}}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amssymb}
\newcommand{\Reals}{\ensuremath{\mathbb{R}}} 
\renewcommand{\baselinestretch}{.972}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\cmark}{\ding{51}}%

\usepackage{graphicx}
\usepackage{comment}
\usepackage{multirow}
\usepackage{import}
\usepackage{wasysym,ifsym}
\usepackage{stmaryrd}
\usepackage{float}
\usepackage[flushleft, para]{threeparttable}
% \usepackage{gensymb}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{xspace}
\newcommand{\name}{BCF\xspace}
\newcommand{\codename}{GSBC\xspace}


\usepackage[acronym, style=super]{glossaries}
\renewcommand*{\glsgroupskip}{}
\newacronym{code}{GSBC}{generalized sparse block code}
\glsdisablehyper
% TO CHECK 
% block codes factorizer or block code factorizer?



\usepackage{fancyhdr}
\fancypagestyle{mahmood}{%
   \fancyhf{} % clear all fields
   \renewcommand{\headrulewidth}{0.4pt}
   \fancyhead[C]{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.}
}%

\makeatletter
\let\ps@IEEEtitlepagestyle\ps@mahmood
\makeatother

\begin{document}



% \title{Sparse Block Codes Factorizers Can Effectively Replace Large Fully Connected Layers}
% \title{Block Code Factorizers, and their Application to Effective Replacement of Large Fully Connected Layers}
\title{Factorizers for Distributed Sparse Block Codes}
%--> Not reflective as it may read that SBC already exits. So something like "Sparse Block Codes Factorizers, and its Application to Effectively Replace Large Fully Connected Layers"
%~\IEEEmembership{Staff,~IEEE,}
\author{Michael Hersche, Aleksandar Terzic, Geethan Karunaratne, Jovin Langenegger, Ang\'eline Pouget, Giovanni Cherubini, Luca Benini, Abu Sebastian, Abbas Rahimi
        % <-this % stops a space
\thanks{M. Hersche, G. Karunaratne, J. Langenegger, G. Cherubini, A. Sebastian, and Abbas Rahimi are with the Hybrid Cloud Research Department, IBM Research-Zurich, S\"aumerstrasse 4, 8803 R\"uschlikon, Switzerland (email: abr@zurich.ibm.com). A. Terzic, A. Pouget, and L. Benini are with the Department of Information Technology and Electrical Engineering, ETH Z\"urich, Gloriastrasse 35, 8092 Z\"urich, Switzerland. M. Hersche, G. Karunaratne, and J. Langenegger are also with the Department of Information Technology and Electrical Engineering, ETH Z\"urich.}% <-this % stops a space
}

% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
%
Distributed sparse block codes (SBCs) exhibit compact representations for encoding and manipulating symbolic data structures using fixed-with vectors.
% 
One major challenge however is to disentangle, or factorize, such data structures into their constituent elements without having to search through all possible combinations. 
%
This factorization becomes more challenging when queried by noisy SBCs wherein symbol representations are \emph{relaxed} due to perceptual uncertainty and approximations made when modern neural networks are used to generate the query vectors.
%
To address these challenges, we first propose a fast and highly accurate method for factorizing a more flexible and hence generalized form of SBCs, dubbed GSBCs.
%
Our iterative factorizer introduces a threshold-based nonlinear activation, a conditional random sampling, and an $\ell_\infty$-based similarity metric, and employs circular convolution and correlation.
%
Its random sampling mechanism in combination with the search in superposition allows to analytically determine the expected number of decoding iterations, which matches the empirical observations up to the \codename's bundling capacity. 
%% dense creates confusion in this context so we remove
%It surpasses factorizer of dense bipolar codes by solving large problem sizes at a faster convergence speed.
%
Secondly, the proposed factorizer maintains its high accuracy when queried by noisy product vectors generated using deep convolutional neural networks (CNNs).
%
This facilitates its application in replacing the large fully connected layer (FCL) in CNNs, whereby $C$ trainable class vectors, or attribute combinations, can be implicitly represented by our factorizer having $F$-factor codebooks, each with $\sqrt[\leftroot{-2}\uproot{2}F]{C}$ fixed codevectors.
%
We provide a methodology to flexibly integrate our factorizer in the classification layer of CNNs with a novel loss function.
%
With this integration, the convolutional layers can generate a noisy product vector that our factorizer can still decode, whereby the decoded factors can have different interpretations based on downstream tasks.
%: it can be disentangled to a set of underlying attribute factors, or blindly classified as a class category.
%
We demonstrate the feasibility of our method on four deep CNN architectures over CIFAR-100, ImageNet-1K, and RAVEN datasets.  
%
In all use cases, the number of parameters and operations are significantly reduced compared to the FCL.


\end{abstract}

\begin{IEEEkeywords}
vector-symbolic architectures, sparse block codes, convolutional neural networks, deep learning.
\end{IEEEkeywords}


\section{Introduction}
One problem in deep neural representation is that information becomes entangled in complex ways that may suit a specific task but make it harder to generalize to slightly different situations. 
%
One promising approach is to factorize representations in which various aspects of knowledge are represented separately and can then be flexibly recombined to represent novel experiences~\cite{betaVAE} with better downstream performance for abstract reasoning tasks~\cite{DisEngRep_NIPS19}. 
%
Solving such factorization problems is also fundamental to biological perception and cognition, e.g., factoring sensory and spatial representations~\cite{DeclarMem_Hippo2006,TEM_Cell2020}, factoring time-varying pixel data~\cite{Hinton2010,Sompolinsky2010,Olshausen2012,Olshausen2020}, factoring a sentence structure~\cite{SmolenskyTensor1990,Jackendoff2002}, and analogical reasoning~\cite{Hummel1997,KanervaAnalogy1998,KanervaPattern1998,PlateAnalogy2000,GaylerIsomorphism2009}. 
%
Although it is unclear how the biological neural circuits may solve these factorization problems, one elegant solution is to cast the entanglement and disentanglement of neurally-encoded information as multiplication and un-multiplication (factorization) of high-dimensional distributed vectors representing neural activities.
%
Such wide vectors are naturally presented in the context of brain-inspired vector-symbolic architectures (VSAs)~\cite{GaylerJackendoff2003,PlateHolographic1995,PlateHolographic2003,KanervaHyperdimensional2009}. 



%Resonator networks implement an iterative search method based on vector symbolic architectures (VSAs)~\cite{GaylerJackendoff2003,PlateHolographic1995,PlateHolographic2003,HD_09}.
%
VSA is a powerful computing model that is built on a rich algebra supporting a set of well-defined operations, including multiplicative binding, unbinding, additive bundling (superposition), permutations, and similarity search that allow performing symbolic computations on top of random distributed vectors.
%
In a VSA, all representations---from atoms to composites---are high-dimensional distributed vectors of the same, fixed dimensionality.
%
An atom in a VSA is a randomly drawn i.i.d. vector that is dissimilar (i.e., quasi-orthogonal) to other random vectors with very high probability, a phenomenon aka concentration of measure~\cite{LedouxConcentration2001}.
%
Deep convolutional neural networks (CNNs) have exploited this abundance of quasi-orthogonal vectors in representation learning to express new class categories, or new object combinations, with effective applications in few-shot~\cite{MANN_NatCom21} continual~\cite{Hersche_2022_CVPR} learning, and visual abstract reasoning~\cite{NVSA} tasks.
%
Counterintuitively, the quasi-orthogonal vectors can still encode semantic information of the objects.
%
For instance, a \texttt{black circle} with two factors of color and shape can be described by a two-factor product vector by binding ($\oast$) its atomic quasi-orthogonal vectors ($\mathbf{x}_{\mathrm{black}}\oast \mathbf{x}_{\mathrm{circle}}$).
%
The resulting product vector is quasi-orthogonal to all other possible vectors (atomic and composite). 
%
An effective VSA-based iterative search method, dubbed resonator network~\cite{Resonator1,Resonator2}, is proposed to factorize the product vector to its atomic factors: $\mathbf{x}_{\mathrm{black}}$ and $\mathbf{x}_{\mathrm{circle}}$.
%
This factorization, or disentanglement, can reveal the semantic relation between a \texttt{black circle} and a \texttt{black square} ($\mathbf{x}_{\mathrm{black}}\oast \mathbf{x}_{\mathrm{square}}$) because both include $\mathbf{x}_{\mathrm{black}}$.


Considering $F$ factors, each factor $f$ having a codebook of $M_f$ codevectors, there are $\prod_{f=1}^{F} M_f$ possible combinations to be searched in the product space for factorizing a product vector computed by multiplicative binding.
%
The resonator network as a powerful dynamical system avoids such brute-force search through the combinatorial space of possible solutions by exploiting the \emph{search in superposition} capability of VSAs.  
%
However, the resonator network suffers from limit cycles and a relatively low operational capacity (i.e., the ratio between the maximum factorizable problem size and the required vector dimensionality).
%
To overcome these limitations, a stochastic in-memory factorizer~\cite{langenegger2022imcfac} introduces new nonlinearities and leverages intrinsic noise of computational memristive devices. 
%
As a result, it increases the operational capacity by at least five orders of magnitude, while also reducing the convergence time compared to the resonator network.


% Integration with Neural networks => towards (sparse) block codes
%The ability to quickly and efficiently search over an enormous space could pave the way for utilizing this method in deep neural networks beyond visual disentanglement.  
%
So far, both resonator networks~\cite{Resonator1,Resonator2} and stochastic factorizers~\cite{langenegger2022imcfac} have been successfully used to infer the factors of dense bipolar product vectors (i.e., each vector element is a Rademacher random variable) that are synthetically generated by binding codevectors. 
%
Instead, when inexact, or noisy, product vectors are generated by a CNN, which mirrors the binding-based encoding approximately, both approaches face challenges in accurately factorizing the noisy dense queries. 
%
In fact, experimental results indicated an accuracy drop of up to $16.22\%$ when factorizing noisy bipolar product queries generated by a small-sized CNN (see Table~\ref{tab:noFC}). 
%
As an alternative representational scheme, sparse block codes (SBCs)~\cite{RachkovskijBinding2001, laiho2015sparse, FradySDR2021} could offer better synergy to integrate factorizers with neural networks.
%
SBCs are biologically plausible~\cite{MasseFruitFly2009,HTM2017,WILLSHAW1969, OlshausenSparseAct1996} and highly amenable to implementation on emerging neuromorphic hardware~\cite{Bent2022SDRTrueNorth,Renner2022SDRBindingSpike}.
%
However, the operations and dynamics currently provided by resonator networks and stochastic factorizers cannot work with SBCs.
% 
Besides, CNNs might also emit noisy SBC query product vectors, which so far cannot be processed with current SBC's operations. 
% 
To address these challenges, new methods need to be developed that can accurately factorize noisy product queries for SBCs.



% Their operations and dynamics cannot work with alternative representations such as sparse distributed vectors~\cite{RachkovskijBinding2001,laiho2015sparse,FradySDR2021}.
% %
% Besides biological plausibility of sparse distributed vectors~\cite{MasseFruitFly2009,HTM2017}, they are also highly amenable to implementation on emerging neuromorphic hardware~\cite{Bent2022SDRTrueNorth,Renner2022SDRBindingSpike}.
% %
% Such sparse representations resemble the neural activity patterns where activated vectors have a few nonzero elements~\cite{WILLSHAW1969, OlshausenSparseAct1996}, and can also have a localized structure~\cite{hinton1984distributed,heeger1992normalization}.
% %
% Accordingly, working with these representations should offer a better synergy to integrate factorizers with deep neural networks, as we will show in the experiments (in Table~\ref{tab:noFC}).
% %
% Deep neural networks have a sparse structure, but their observations are often noisy and only partially sparse. 
% %
% This calls for factorization methods that support sparse representations and their noisy observations.



%
% Such sparse representations resemble the sparse activity patterns of conventional neural networks in which activated vectors have a few nonzero elements~\cite{WILLSHAW1969, OlshausenSparseAct1996}, and can also have a localized structure~\cite{hinton1984distributed,heeger1992normalization}.
% %
% Accordingly, working with these representations should offer a better synergy to integrate factorizers with deep neural networks, as we will show in the experiments (in Table~\ref{tab:noFC}).


This paper provides the following contributions, which are divided into two main parts.
%
In Part~I, we propose the first iterative block code factorizer (\name) that can reliably factorize blockwise distributed product vectors.
% 
The codebooks are binary SBCs~\cite{laiho2015sparse}, which span the product space, while \name can factorize \gls{code} product vectors, which can be generated by CNNs. % and hence be considered to be noisy. 
%
Factorizing binary SBCs is a special case.
%
\name introduces a configurable threshold, a conditional sampling mechanism, and a new $\ell_\infty$-based similarity search operations for the first time in the VSA context.
%
\name uses circular convolution and correlation~\cite{PlateHolographic2003} as general binding and unbinding operators in \gls{code}.
%
During the iterative decoding, the novel sampling mechanism induces a random search in superposition over the product space if no confident solution is present. 
%
\name improves the convergence speed of the state-of-the-art factorizer~\cite{langenegger2022imcfac} on a large problem size of $10^6$ by up to 6$\times$. 
%
To gain a deeper understanding of \name's iterative search in superposition, we leverage its configurable threshold and sampling mechanism to configure it as an unconditional sampler that randomly searches over the product space. 
%
This allows us, to analytically determine the expected number of decoding iterations, which matches with the empirical observations when operating within the \gls{code}'s bundling capacity. 




In Part~II, motivated by the superior performance of \name, we exloit its application in reducing the number of parameters in fully connected layers (FCLs).
%
FCLs are ubiquitous in modern deep learning architectures, and play a major role by accounting for most of the parameters in various architectures such as transformers~\cite{ProdKey_NIPS19,FCLareKVmem2021}, extreme classifiers~\cite{XML-CNN2017,ganesan2021learning}, and CNNs for edge devices~\cite{qian2020doweneed}.
%
The FCL's trainable parameters $\mathbf{W}\in \Reals^{D_i \times D_o}$ can be replaced by a \name with $F$ codebooks of fixed parameters, each $\mathbf{X}^f\in \{0,1\}^{D_i \times \sqrt[\leftroot{-2}\uproot{2}F]{D_o}}$.
%
The structure of the codebooks is naturally given when the product space is defined by semantic attributes (e.g., in RAVEN~\cite{Raven_19}), or can be arbitrarily defined when no semantic attributes are provided (e.g., for natural images in ImageNet-1K~\cite{deng2009imagenet}).
%
To map sensory inputs to \gls{code} product vectors, we propose a methodology to train deep convolutional layers with a novel blockwise additive loss that can directly use \name in place of an FCL classifier.
%
Our \name reduces the total number of parameters of a wide range of deep CNNs and datasets by 0.5--44.5\% while maintaining a high accuracy within $0$--$4.46\%$ compared to the baseline CNNs using the large FCL.
%
Our \name also lowers the computational cost of the classifier layer by $55.2$--$86.7\%$ with respect to FCLs.


% It may or may not use a projection layer at the interface.
%
% We evaluate our approach on a wide range of deep CNNs ranging from ShuffleNetV2 (with 2.3\,M parameters) to ResNet-50 (with 25.5\,M parameters).
%
% In the RAVEN dataset~\cite{Raven_19}, each image panel is associated with four visual attributes spanning 4200 combinations.
%
% Our factorizer accordingly uses four semantically-meaningful codebooks to represent them.
%
% For natural images in ImageNet-1K~\cite{deng2009imagenet}, where no semantic attributes are provided, our factorizer blindly uses two codebooks, each with 32 codevectors, to cover 1000 class categories. 
%
% The accuracy gap can be further reduced to $0$--$1.2\%$ by embedding a projection between the last convolution layer and the \name, resulting in a 2.2--21.7\% reduction in the total number of parameters. 




\section{Related Work}
%
\subsection{Factorizing distributed representations}
%
A resonator network iteratively searches over the alternatives for each factor individually by leveraging a combination of nonlinear dynamics and searching in superposition~\cite{Resonator1,Resonator2}. 
%
This iterative process converges empirically by finding correct factors under operational capacity constraints~\cite{Resonator2}. 
%
The resonator network can accurately factorize dense bipolar distributed vectors generated by a two-layer perceptron network trained to approximate the multiplicative binding for colored MNIST digits~\cite{Resonator1}.
%
Its operational capacity and convergence speed have been improved by the stochastic factorizer in~\cite{langenegger2022imcfac}. 
%
However, we observed that the accuracy of both the resonator network and the stochastic factorizer significantly drops (by as much as 15\%) when it is queried with product vectors generated from deep CNNs processing natural images (see Table~\ref{tab:noFC}).  
%
This challenge motivated us to switch to alternative block code representations, whereby we can retain high accuracy by using our \name.
%
%Moreover, compared to the state-of-the-art stochastic factorizer, \name can solve large problems (e.g., $\prod_{f=1}^{F} M_f=10^6$) with fewer iterations irrespective of the number of factors $F$ (see Table~\ref{tab:dense_vs_sparse}).
Moreover, compared to the state-of-the-art stochastic factorizer, \name requires fewer iterations irrespective of the number of factors $F$ (see Table~\ref{tab:dense_vs_sparse}).
%
Interestingly, it only requires two iterations to converge for problems with a search space as large as $10^4$.


%The stochastic factorizer can factorize a limited set of 1200 synthetic objects provided by the RAVEN dataset. 
% more complex synthetic images from RAVEN (having 4200 combinations vs. 630 in colored MNIST and 1200 in limited RAVEN), or 


\subsection{Fixing the final FCL in CNNs}
%
Typically, a learned affine transformation is placed at the end of deep CNNs, yielding a per-class value used for classification. 
%
In this FCL classifier, the number of parameters is proportional to the number of class categories. 
%
Therefore FCLs constitute a large portion of the networkâ€™s total parameters: for instance, in models for edge devices, FCLs constitute 44.5\% of ShuffleNetV2~\cite{ShuffleNetV2_ECCV2018}, or 37\% of MobileNetV2~\cite{MobileNetV2_CVPR2018} for ImageNet-1K.
%
This dominant parameter count is more prominent in lifelong continual learning models, where the number of classes quickly goes over a thousand and further increases over time~\cite{Hersche_2022_CVPR}.



%For instance, in the transformers a feed-forward FCL is replaced by a more powerful but efficient key-value memory layer~\cite{two_works} that effectively reduces the overall number of parameters.
%
To reduce the training complexity associated with FCLs, various techniques have been proposed to fix their weight matrix during training.
%
In turn, an FCL is replaced by a Hadamard matrix~\cite{hoffer2018fix}, or a cheaper Identity matrix~\cite{qian2020doweneed}, or vertices of a simplex equiangular tight frame~\cite{NeuralCollaps_NIPS2021}.
%
Although partly effective, due to square-shaped structures, these methods are restricted to problems in which the number of classes is smaller than or equal to the feature dimensionality, i.e., $D_i$=$D_o$.
%, therefore they cannot be used for an arbitrary large number of classes. 
%
To address this limitation, methods that simply draw class vectors randomly distributed over a hypersphere~\cite{mettes2019hyperspherical,RandomClassVec2021} were proposed.
%
However, these methods still need to store the individual class vectors, which imposes the FCL's conventional cost of $\mathcal{O}(D_i \cdot D_o)$ for memory storage and compute complexity during training and inference.  


Our \name with two factors can reduce the memory and compute complexity to $\mathcal{O}(D_i \cdot \sqrt{D_o})$.
%
This is done by using randomly-drawn distributed binary SBCs that form an intermediate product vector space whose dimensionality ($D_p$) is significantly lower than the number of classes ($D_p\ll D_o$), but high enough such that a large number of classes can be expressed thanks to the supplied quasi-orthogonality.
%
We show that the product vector space can be built either at the output of the last convolutional layer directly (i.e., its dimensionality is set by the feature dimension $D_p$=$D_i$), or at the output of a smaller FCL as a projection layer (i.e., its dimensionality can be chosen).
%
This flexibly enables a trade-off between the number of removable parameters and obtainable accuracy.
%
%The former approach fully removes the FCL parameters while the latter approach reduces the parameters 







\section{VSA Preliminary}
% Introduction
%VSA is a brain-inspired non von Neumann computing framework based on representing information with high-dimensional but fixed vectors (thousands of dimensions). 
%
%In VSAs, vectors are holographic and (pseudo)random with independent and identically distributed (i.i.d.) components.
VSAs define operations over (pseudo)random vectors with independent and identically distributed (i.i.d.) components.
% codebook
Computing with VSAs begins by defining a basis in the form of a codebook $\mathbf{X}:=\{\mathbf{x}_1,\mathbf{x}_2, ..., \mathbf{x}_M\}:=\{\mathbf{x}_i\}_{i=1}^{M}$.
% Quasi orthogonality 
If the dimension $D_p$ of two randomly drawn vectors of the space is sufficiently large, they are highly likely to have an almost-zero similarity, i.e., they are quasi-orthogonal~\cite{KanervaHyperdimensional2009}. 
% main operations
VSAs use three primary operations---bundling, binding, and permutation---that form an algebra over the space of vectors. 
%
Combined with a similarity metric, these operations support various cognitive data structures: variable binding, sequence, and hierarchy.
%
See~\cite{HDCSurvey_PI} for a review.

% Dense representation
For example, consider a VSA model based on the bipolar vector space~\cite{GaylerJackendoff2003}, i.e., $\mathbf{x} \in \left\{-1, +1 \right\}^{D_p}$. 
% 
One can define binding and unbinding in this vector space as the Hadamard (i.e., elementwise) product.
%
The similarity between two vectors in the space is typically measured using the cosine similarity metric.
%Cosine similarity serves as similarity measure between two vectors. 
% 
A possible bundling operation is the elementwise sum followed by the sign function, setting all negative elements to $-1$ and the positive to $+1$. 
% 
To keep representations bipolar, elements with a sum equal to zero are randomly set to $-1$ or $+1$.

% Binary sparse block codes
As an alternative, binary sparse block codes (binary SBCs)~\cite{laiho2015sparse} induce a local blockwise structure that exhibits ideal variable binding properties~\cite{FradySDR2021} and high information capacity when used in associative memories~\cite{gripon2011sparse, knoblauch2020iterative, schlegel2022comparison}. 
% As an alternative, binary sparse block codes (binary SBCs)~\cite{laiho2015sparse} come with reduced storage requirements while maintaining a high bundling capacity~\cite{RachkovskijStructures2001,schlegel2022comparison}. 
% Bundling capacity corresponds to the number of random vectors which can be bundled together and later be accurately retrieved, and is an important metric for measuring the performance of a VSA model \cite{schlegel2022comparison}.
%
In binary SBCs, the $D_p$-dimensional vectors are divided into $B$ blocks of equal length, $L$=$D_p/B$, where only one element per block is set to 1. 
%
The vectors can either be described with a $D_p$-dimensional binary SBC vector (denoted as $\mathbf{x}$), or with a $B$-dimensional offset vector where each element indicates the index of the nonzero element within each block (denoted as $\mathbf{\dot{x}}$).
% 
The vectors are initialized by randomly setting one element in each block to 1.
% 
The bundling of two or more vectors is defined as their elementwise addition, followed by a selection function that retains the sparsity by setting the largest element of each block to 1 and the remaining elements to 0.
%
% In the case of several maximizing elements within a block, a random one is retained in the resulting bundling.
%
The binding of two vectors is the elementwise modulo-$L$ sum of their offset representation.
% 
Similarly, unbinding is defined using the modulo-$L$ difference. 
%
Both binding and unbinding preserve dimensionality and sparsity. 
%
A typical choice for the similarity metric is the normalized dot-product, which counts the number of overlapping elements of two vectors~\cite{BinDensity_2018}.

\section{Part~I: Factorization of Generalized Sparse Block Codes}

\begin{table}[]
\caption{Comparison of operations of binary SBCs and our \glspl{code}. All operations except for the similarity are applied blockwise.}
\label{tab:table1}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
             & \textbf{Binary SBCs}\cite{laiho2015sparse}                 & \textbf{\glspl{code}} (ours)     \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}      
Binding ($\oast$) & \begin{tabular}[c]{@{}c@{}}Modulo-$L$ sum \\ of nonzero indices\end{tabular}            &\begin{tabular}[c]{@{}c@{}} Blockwise \\ circular convolution  \end{tabular}                                              \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} 
Unbinding ($\oslash$)  & \begin{tabular}[c]{@{}c@{}}Modulo-$L$ difference\\  between nonzero indices\end{tabular} & \begin{tabular}[c]{@{}c@{}} Blockwise\\ circular correlation   \end{tabular}                                             \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} 
Bundling ($\oplus$)  & Argmax of sum                                                                          & \begin{tabular}[c]{@{}c@{}}Sum \& normalization\end{tabular} \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} 
Similarity  & Dot-product                                                                         & \begin{tabular}[c]{@{}c@{}}$\ell_\infty$-based sim. \\ or dot-product\end{tabular} \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{figure*}
    \centering
    \includegraphics{figures/SBCfactorizer_v6.pdf}
    \caption{Block code factorizer (\name) for $F$=2 factors. 
    It can factorize synthetic binary SBC product vectors and \gls{code} product vectors ($\mathbf{p}$) which might be the result of a neural network mapping.
    }
  \label{fig:SBCfactorizer}
\end{figure*}

This section presents our first contribution: we propose a novel block code factorizer (\name) that efficiently finds the factors of product vectors based on block codes. 
%
We first introduce \glsfirst{code}, a generalization of the previously presented binary SBC. 
%
 We present corresponding binding, unbinding, and bundling operations and a novel similarity metric based on the $\ell_\infty$-distance.
% 
We then continue to the exposition and experimental evaluation of our \name, which is capable of fast and accurate factorization of \gls{code} product vectors.

\subsection{Generalized sparse block codes (GSBCs)}
\label{BCAs}
%
Like binary SBCs, \glspl{code} divide the $D_p$-dimensional vectors into $B$ blocks of equal length $L=D_p/B$. 
%
However, the individual blocks are not restricted to be binary or sparse.
% 
The requirements imposed upon the vectors are that their elements are in $\Reals^{+}$, and each block has a unit $\ell_1$-norm.
%
Binary SBCs satisfy both constraints and are valid GSBCs. 
% 
Fig.~\ref{fig:SBCfactorizer} illustrates an example of a binary SBC and a \gls{code} vector. 
% 
The blockwise distribution of the \gls{code} representation can be interpreted as blockwise probabilistic mass functions, serving as a proxy for the binary SBC vector in this example. 
% 
Such \gls{code} product vectors can be produced by neural networks (see Part~II). 
%
Besides their benefits in the integration with neural networks, the \gls{code} representations inside \name enable more accurate and faster factorization. 


The individual operations for the \glspl{code} are defined as follows:
% 
\paragraph{Binding/Unbinding}
We exploit general binding and unbinding operations in blockwise circular convolution and correlation to support arbitrary block representations.
%
Specifically, if both operands have blockwise unit $\ell_1$-norm, the result does as well.

\paragraph{Bundling}
The bundling of several vectors is defined as their elementwise sum followed by a normalization operation, ensuring that each result block has unit $\ell_1$-norm.

%
\paragraph{$\ell_\infty$-based Similarity}
We propose a novel similarity measure based on the $\ell_\infty$-norm of the elementwise difference between two \gls{code} vectors $\mathbf{x}_i$ and $\mathbf{x}_j$: 
\begin{align}
    s_{\infty}(\mathbf{x}_i, \mathbf{x}_j) := 1-\ell_\infty(\mathbf{x}_i- \mathbf{x}_j),
\end{align}
with $\ell_\infty(\mathbf{a}) = \max_i|\mathbf{a}[i]|$, where $\mathbf{a}[i]$ denotes the $i$-th element of a vector $\mathbf{a}$.

%
For any \gls{code} vectors $\mathbf{a}$ and $\mathbf{b}$, it holds that $0\leq\ell_\infty(\mathbf{a}- \mathbf{b})\leq1$. 
%
Therefore, our novel similarity metric satisfies $0 \leq s_{\infty}(\mathbf{a}, \mathbf{b}) \leq 1$, whereby equality on the right-hand side holds if, and only if, $\mathbf{a}=\mathbf{b}$.
% $\mathbf{a}$ and $\mathbf{b}$ which satisfy the requirements imposed above, namely unit $\ell_1$-norm blocks and non-negative elements, 
%
Table~\ref{tab:table1} compares the operations of the \glspl{code} with respect to the binary SBCs.


% In the Appendix, we empirically investigate the bundling capacity of a BCA equipped with the $\ell_\infty$-based similarity and find that it outperforms all VSA models investigated in the survey work \cite{schlegel2022comparison}. 
%
%The probability of two random block-wise sparse vectors being orthogonal under the proposed similarity is stricly larger than that of the dot product similarity. For the dot product to be non-zero, the two vectors must overlap in at least one block, whereas the $\ell_\infty$-based similarity is non-zero only if the two vectors fully overlap.
% The $\ell_\infty$-based similarity is highly effective in retrieving individual elements from their bundled representation.
% % 
% To empirically show this, we conducted an experiment following~\cite{schlegel2022comparison}, in which we find the minimum dimension using which we can accurately retrieve all elements from a bundle consisting of 15 elements.
% %was identified for retrieving all elements from a bundle that contains 15 elements. 
% %
% Our novel similarity enables BCAs to achieve the lowest required dimension (290), hence the highest capacity per dimension, compared to all other VSAs investigated in~\cite{schlegel2022comparison}, e.g., BSC (750), HRR (510), FHRR (330), or binary BSC (320). 
% % 
% Appendix~1 provides more details on the bundling capacity experiments. 

\subsection{Factorization problem}
%
We define the factorization problem for \glspl{code} and our factorization approach for two factors. 
%
Applying our method to more than two factors is straightforward; corresponding experimental results will be presented in Section~\ref{subsec:fac_results}. 
%

Given two codebooks\footnote{In our experiments, the codebooks consist of binary SBC codewords, but they could be \gls{code} vectors too.}, $\mathbf{X}^1$:=$\lbrace\mathbf{x}_i^1 \rbrace_{i=1}^{M_1}$ and $\mathbf{X}^2$:=$\lbrace\mathbf{x}_i^2 \rbrace_{i=1}^{M_2}$, and a product vector
%\footnote{The product vector can on the other hand be non-binary and dense, e.g. a noise-corrupted binding of several SBC codevectors or a neural network output.} 
$\mathbf{p}$=$\mathbf{x}_i^1\oast \mathbf{x}_j^2$ formed by binding two factors from the codebooks, we aim to find the estimate factors $\hat{\mathbf{x}}^1 \in \mathbf{X}^1$ and $\hat{\mathbf{x}}^2 \in \mathbf{X}^2$ that satisfy
\begin{align}
    \mathbf{p} = \hat{\mathbf{x}}^1\oast \hat{\mathbf{x}}^2.
\end{align}
%
A naive brute-force approach would compare the product vector ($\mathbf{p}$) to all possible combinations spanned by the product space $\mathbf{P}=\mathbf{X}^1\oast \mathbf{X}^2 := \left\{\mathbf{x}_1^1\oast \mathbf{x}_1^2, \mathbf{x}_1^1\oast \mathbf{x}_2^2,...,  \mathbf{x}_{M_1}^1\oast \mathbf{x}_{M_2}^2\right\}$. 
%
This results in a combinatorial search problem requiring $M_1 \cdot M_2$ similarity computations. \name can significantly reduce computational complexity.
% 

\subsection{Block code factorizer (\name)}
Here, we introduce our novel \name that efficiently finds the factors of product vectors based on \glspl{code}, shown in Fig.~\ref{fig:SBCfactorizer}.  
%
%We replace the binding, unbinding, and bundling operations from the bipolar dense factorizer~\cite{Resonator1, Resonator2} with equivalent operations that we defined for block codes. %With the new architecture, we can achieve faster convergence and larger operational capacity compared to the bipolar dense resonator networks~\cite{Resonator2,langenegger2022imcfac}.
%The new architecture allows us to accurately factorize 
%
%The codebooks consist of binary SBCs, however, the input product vector can be either a binary SBC, or any block code satisfying the constraints imposed in section \ref{BCAs}. 
%with an arbitrary sparsity.
% 
%This is a distinct advantage when factorizing ``noisy'' product vectors, e.g., query vectors emitted by a CNN.
%
The product vector decoding begins with initializing the estimate factors $\hat{\mathbf{x}}^1(0)$ and $\hat{\mathbf{x}}^2(0)$ by bundling all vectors from the corresponding codebooks. 
% \begin{align}
%     \hat{\mathbf{x}}^f(0) = \bigoplus_{i=1}^{M_f} \mathbf{x}_i^f. 
% \end{align}
%
Then, the estimate factors are iteratively updated through the following steps. 

\textbf{Step 1: Unbinding.} At  the start of iteration $t\geq1$, the estimate factors from the previous iteration $t-1$ are unbound from the product vector by using blockwise circular correlation:
%Concretely, to generate a new estimate of factor $\hat{\mathbf{x}}^2$, we unbind our estimate of $\hat{\mathbf{x}}^1$ from $\mathbf{p}$, resulting in:
\begin{align}
    \tilde{\mathbf{x}}^1(t) &= \mathbf{p}\oslash\hat{\mathbf{x}}^2(t-1) \\
    \tilde{\mathbf{x}}^2(t) &= \mathbf{p}\oslash\tilde{\mathbf{x}}^1(t-1).
\end{align}
%
%There is a sequential dependence between the factor estimates, i.e. we first compute the estimate for the first factor before progressing onto the second one. It is possible to search for both factors in parallel, however we find that this negatively impacts the operational capacity and convergence speed.
%$\tilde{\mathbf{x}}^1(t)$ and $\tilde{\mathbf{x}}^2(t)$
%We first present the configurable options for each block in the auto-associative search and describe the selection of the hyperparameters at the end. 
% Similarity

\textbf{Step 2: Similarity search.} Next, we query the associative memory, which contains the codebook $\mathbf{X}^f$ for the factor $f$, with the unbound factor estimates. 
%
Here, we deploy our novel $\ell_\infty$-based similarity as an effective associative search.
%
At iteration $t$, this yields a vector of similarity scores $\mathbf{a}^f(t) \in \Reals^{M_f}$ for each factor $f$. 
% 
The $i$-th element in $\mathbf{a}^f(t)$ is computed as:
% 
\begin{align}
    \mathbf{a}^f(t)[i] = s_\infty(\tilde{\mathbf{x}}^f(t), \mathbf{x}^f_i).
\end{align}
%
We observe that using a conventional dot-product similarity causes a significant performance drop, as shown in Fig.~\ref{fig:factorizer-main-results}.


\textbf{Step 3: Sparse activation and conditional random sampling.} 
%
Recent work on stochastic factorizers~\cite{langenegger2022imcfac} demonstrated that applying a threshold function to the elements of the similarity vector can improve convergence speed and operational capacity. 
%
We deploy a similar idea in our \name. 
%
In this step, the previously computed similarities are compared against a fixed threshold $T \in \Reals^{+}$. 
Similarity values that are larger than the threshold propagate forward, whereas lower ones get zeroed out:
\begin{align}
    \mathbf{a'}^f(t) = \mathrm{thresh}(\mathbf{a}^f(t);T) \\
    \mathrm{thresh}(\mathbf{a};T)[i] = \begin{cases}
    \mathbf{a}[i],& \text{if } \mathbf{a}[i]\geq T\\
    0,              & \text{otherwise}.
\end{cases}
\end{align}

%While we indeed observe an increased factorization accuracy with the addition of the threshold nonlinearity, there is an issue with this approach. It lies in the fact that random sparse binary vectors are highly likely to be orthogonal under the $s_\infty(\cdot)$ similarity measure.
This nonlinearity allows us to focus on the most promising solutions by discarding the likely incorrect low-similarity ones. 
%which we have observed to hinder convergence speed.
%Yet there is a possibility of ending up with an all-zero similarity vector, effectively stopping the procedure.
However, thresholding entails the possibility of ending up with an all-zero similarity vector, effectively stopping the decoding procedure.
%
To alleviate this issue, upon encountering an all-zero similarity vector, we randomly generate a subset of equally weighted similarity values:
%
\begin{align}
    \mathbf{a''}^f(t) = \begin{cases}
    \mathbf{a'}^f(t),& \text{if } \mathbf{a'}^f(t)\neq \mathbf{0}\\
    \mathbf{a}_{rand},              & \text{otherwise,}
\end{cases}
\label{eqn:eq8}
\end{align}
%
where $\mathbf{a}_{rand} \in \Reals^{M_f}$ is a vector in which $A$-many randomly selected elements are set to $1/A$.
% 
In combination with step 4 (weighted bundling), the conditional random sampling given by (\ref{eqn:eq8}) yields an equally weighted bundling of $A$ randomly selected codewords. 

The novel threshold and conditional sampling mechanisms are simple and interpretable, yet leading to faster convergence. 
%
The stochastic factorizer~\cite{langenegger2022imcfac} relied on various noise instantiations at every decoding iteration. 
%
The necessary stochasticity was supplied from intrinsic noise of phase-change memory devices and analog-to-digital converters of a computational analog memory tile.
% 
Instead, \name remains deterministic in the decoding iterations unless all elements in the similarity vector are zero, in which case it activates only a single random source. 
%
This can be seen as a conditional \emph{restart} of \name using a new random initialization. 
% 
The conditional random sampling could be implemented with a single random permutation of a seed vector in which $A$-many arbitrary values are set to $1/A$.
% 
The conditional random sampling mechanism is also interpretable, in the sense that it allows to analytically determine the expected number of decoding iterations, subject to the bundling capacity (see Section~\ref{subsec:bcf-ablation}). 


%In effect, it randomly samples $A$-many codevectors, $A$ being a hyperparameter. 

%At this point, the aforementioned benefit of high bundling capacity on the factorization procedure becomes evident. The more vectors we can superimpose with low interference, the larger portion of the search space we can cover in each step of the random walk over the space of possible solutions.




\textbf{Step 4: Weighted bundling.} Finally, we generate the next factor estimate $\hat{\mathbf{x}}^f(t+1)$ as the normalized weighted bundling of the factor's codevectors:
%
\begin{align}
     \hat{\mathbf{x}}^f(t+1) = \frac{(\mathbf{X}^f)^T \mathbf{a}''^f(t)}{\sum_{i=1}^{M_f} \mathbf{a}''^f(t)[i]}.
\end{align}
%
The codevectors $\lbrace\mathbf{x}_i^1 \rbrace_{i=1}^{M_1}$ are \glspl{code} with unit $\ell_1$-norm blocks; hence, dividing the weighted bundling by the sum of the weights yields valid \glspl{code}.

%Normalization is a crucial step. Without it, the $s_\infty(\cdot, \cdot)$ similarity function is no longer constrained to the $[0,1]$ codomain. BCA vectors whose blocks have an $\ell_1$-norm larger than $1$ can result in exploding similarities after feeding the estimate back into the next iteration of the procedure, which will certainly not interact well with the fixed threshold. The same rationale holds when the BCA vectors' blocks have an $\ell_1$-norm smaller than $1$, which may induce vanishing similarities.

%, defined in Eq.~\eqref{eq:weightedbundeling}, using $\mathbf{a}''^f$ and $\mathbf{X}^f$. 
% \begin{align}
%     \hat{\mathbf{x}}^f(t+1) = \mathrm{blocknorm}\left(\mathrm{top}S\left( (\mathbf{X}^f)^T \mathbf{a}''^f(t) \right)\right).  
% \end{align}
% The blockwise top$S$ function sparsifies the weighted superposition by sorting the elements and activating the highest $S$ values per block.
% % 
% Moreover, we normalize the sparsified vector such that each block sums up to 1. 
% % 
% An example of the sparsification after the weighted superposition is illustrated in Fig.~\ref{fig:SBCfactorizer}. 
% %
% Assuming a maximally sparse product vector, the selection of $S$ is the only parameter which controls the sparsity inside the resonator. % 
% In the limit case of $S=1$, the factorizer is exlusively working with binary SBCs. 
%
% In the case of binary SBCs, the activation function sets the per-block maximizing element to '1' and all remaining to zero. 
% % 
% In our SBC case, we relax the sparsity constraint and activate multiple values per block using the blockwise top$B$ activation followed by a blockwise normalization such that each block sums up to one. 


\textbf{Step 5: Convergence detection.} The iterative decoding is repeated until \name converges or a predefined maximum number of iterations ($N$) is reached.
%
We define the maximum number of iterations such that \name does at most as many similarity searches as the brute-force approach~\cite{langenegger2022imcfac}: 
%
\begin{align}\label{eq:maxiter}
    N := \frac{\prod_{f=1}^{F} M_f}{\sum_{f=1}^F M_f}.
\end{align}
%
The convergence detection mechanism is based on an additional, fixed threshold. Decoding is stopped as soon as both similarity vectors ($\mathbf{a}^1(t)$ and $\mathbf{a}^2(t)$) contain an element that exceeds a predefined detection threshold value ($T_c$)\cite{langenegger2022imcfac}.
%
% We did not observe the detection threshold significantly influencing the factorizer's performance.
We set it to $T_c$=$0.8$ for synthetic product vectors and $T_c$=$0.5$ for noisy product vectors from deep neural networks. 

%Finally, for a given ($F$, $D_p$, $B$), we find the set of optimal SBC factorizer configuration specified by $\mathbf{c^\star}$=$[S, A, T_n, T_c]$ for all problem sizes ($\prod_{f=1}^{F} M_f$) by Bayesian optimization (see Appendix~1). 

\subsection{Hyperparameter optimization}
This section explains the methodology for finding optimal \name hyperparameters to achieve high accuracy and fast convergence.
%
The optimal configuration is denoted by $\mathbf{c^\star}=(T^\star, A^\star)$, corresponding to the optimal threshold and sampling width.
%
As an automatic hyperparameter search method, we employ Bayesian optimization.
%
%
%We define the optimal set of hyperparameters to achieve the highest accuracy with the least iterations for an intermediate problem size ($10^7$), and apply the found hyperparameters on all problem sizes. 
% for each problem of interest $\prod_{f=1}^{F} M_f\in[1e3,1e8]$.
%


%In our experiments we observe that larger problem sizes require a larger threshold value. This can be explained by the fact that associative memory search over larger codebooks is likely to activate more elements, leading to higher interference between promising high-similarity solutions and likely incorrect low-similarity ones.
%
%Because of this, we need to tune the hyperparameters at each problem size that we evaluate the factorizer at.

%
The loss function is defined as the error rate given by the percentage of incorrect factorizations out of 512 randomly selected product vectors.
%
To put a strong emphasis on fast convergence, we reduced the maximal number of the iterations to $N' = 0.05 N$ for all Bayesian optimization runs. 
%
% increasing the variation of the error rate across multiple configurations.
% %
% The higher variation yields in a wider separation between optimal and bad configuration, helping to find better configurations faster.
%
%More formally we minimize the loss function:
%\begin{align}
%    \mathbf{c^\star} = \operatornamewithlimits{arg\,min}_{\mathbf{c}} e(\mathbf{c}).
%\end{align}
%
The error rate is an unknown function of the hyperparameters, modeled as a Gaussian process with a radial basis function kernel.
%
Hyperparameter sampling is done using the expected improvement acquisition function.
%

%
For each problem ($F$, $D_p$, $\prod_{f=1}^{F} M_f$, $B$), we run five separate hyperparameter searches, each of which tests 200 different hyperparameter combinations restricted to the domains $A \in [0,M_f]$ and $T \in [0,1]$.
%
Finally, we select the hyperparamters with the lowest error rate at the default maximum number of iterations ($N$). 
%


Fig.~\ref{fig:hyperparams} shows the resulting threshold ($T$) and sampling width ($A$) over various problem sizes for $D_p$=512, $B$=4, and $F$=2. 
%
For a range of problem sizes ($10^3$--$10^5$), \name does not require the threshold and sampling dynamics: it sets the threshold and the sampling width to 0.
%
For larger problem sizes ($>$$10^5$), the threshold $T$ grows with the problem size. 
%
This can be explained by the fact that querying larger codebooks is likely to activate more codevectors, which will be bundled together. 
%
As such, we expect a higher interference between likely incorrect low-similarity solutions and promising high-similarity solutions. 
%
A higher threshold effectively reduces the number of bundled vectors, reducing interference. 
%
Similarly, the sampling width ($A$) grows until a problem size of 4,000,000, where it sharply declines.
%
The sharp decline was observed for all investigated problem settings ($F$, $D_p$, $B$) and might stem from the limited bundling capacity. 


%Hyperparameters for other configurations 

\begin{figure}
\centering
\subfloat[Optimal threshold ($T^*$).]{\includegraphics[width=0.5\linewidth]{figures/optimal_thresh.png}\label{fig:optThresh}}
\subfloat[Optimal sampling width ($A^*$).]{\includegraphics[width=0.5\linewidth]{figures/optimal_A.png}  \label{fig:optA}}  
\caption{Threshold and sampling width of Bayesian optimization for $D_p$=512, $B$=4, and $F$=2.}
\label{fig:hyperparams}
\end{figure}
%
%The final hyperparameters are those with the lowest error rate for the given problem.
%
%To define the final hyperparameters we run a grid-search optimization in close proximity to the hyperparameters obtained by the Bayesian optimization with the uncompromised number of iterations $N$ instead of $N'$.
%
%This procedure allows us to find performant configurations with a low computational effort.
%


%Table~\ref{tab:hyperparameters} lists the hyperparameters found for a various set of SBC factorizers.
%
%In the binary SBC factorizer, the bundling operation simply uses an argmax whereby the sparsity is fixed maximally, and cannot be tuned. 
%
%We therefore tune the rest of  hyperparameters for the binary SBC factorizer, i.e., $\mathbf{c^\star}=[A^\star, T^\star]$.

% For the binary SBC factorizer, we tune the hyperparameters without the bundling sparsification, i.e., $\mathbf{c^\star}=[A^\star, T_n^\star, T_c^\star]$.
%
% For the SBC, we added the bundling sparsification $S$ as an additional hyperparameter.
%

%
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/figS5.eps}
%     \fontsize{7}{9}
%     \vspace*{-8mm}
%     \selectfont
%     \caption{Impact of $A$ and $S$ on the operation capacity of the SBC factorizer with $s_{\infty}$ similarity for $D_p$=512, $B$=4, and $F$=2.
%     }
%   \label{fig:S5}
% \end{figure}

%\begin{table}[]
%\caption{The optimal hyperparameters for the factorizers.}
%\resizebox{\linewidth}{!}{
%\begin{tabular}{llllllllll}
%\toprule
%$D$ & $F$ & $B$ & Factorizer & Sim. & $T_n$ & $A$ & $S$ & $T_c$ \\
%\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-7}\cmidrule(r){8-8}\cmidrule(r){9-9}\cmidrule(r){10-10}
%512 & 2   & 32 & Binary SBC & dotp. & 0.80 & 22   & - & 22.4   \\ 
%512 & 2   & 4  & Binary SBC & dotp. & 0.88 & 30   & - & 2.8   \\ 
%512 & 2   & 4  & SBC & dotp. & 1.8e-3 & 46   & 17 & 2.0   \\ 
%512 & 2   & 4  & SBC & $s_{\infty}$ & 1e-05 & 20   & 20 & 0.5   \\
%\bottomrule
%\end{tabular}
%}
%\label{tab:hyperparameters}
%\end{table}


% Finally, for a given ($F$, $D_p$, $B$), we find the set of optimal hyperparameters $\mathbf{c^\star}$=$[S, A, T_n, T_c]$ by Bayesian optimization (see Appendix~1). 
% %
% These hyperparameters configure our SBC factorizer to work with all problem sizes ($\prod_{f=1}^{F} M_f$), and ``noisy'' product vectors as well.

\subsection{Experimental setup}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/bcf_mainresults.pdf}
    \fontsize{7}{9}
    \selectfont
    \vspace*{-2mm}
    \caption{Factorization accuracy (left) and number of iterations (right) of various \name configurations on synthetic (i.e., exact) product vectors for different problem sizes ($\prod_{f=1}^{F} M_f$). 
    We set $D_p$=$512$, $F$=$2$, and $B$=$4$. 
    The maximum operational capacity is marked with a cross. Problem sizes exceeding the operational capacity are marked with dashed lines which face an accuracy lower than 99\%. \name configured with binary SBC operations (in blue) cannot solve any of the displayed problem sizes at the required accuracy. 
    }
  \label{fig:factorizer-main-results}
\end{figure*}
%
We evaluate the performance of our novel \name on randomly selected synthetic product vectors.
%
For each problem ($F$, $D_p$, $\prod_{f=1}^{F} M_f$, $B$), we assess the factorization accuracy and the number of iterations by averaging over 5000 experiments. In each experiment, we randomly select one vector from each of the $F$-many codebooks, bind the selected vectors together to form a product vector, then use the product vector as the input into \name. In this section, the queries are always binary SBCs. In contrast, the representations inside \name (i.e., factor estimates at any step of the decoding loop) do not have this restriction imposed upon them unless specified otherwise.
%(e.g. in Figure \ref{fig:factorizer-main-results}).
%
%We define the factorization accuracy as the ratio of correctly solved factorizations out of 5000 randomly drawn problems.
%
%We compare the performance on a wide range of problem sizes.
%
The operational capacity is defined as the largest problem size for which \name achieves an accuracy higher than $99\%$~\cite{Resonator2}.
%
\subsection{Comparative results}\label{subsec:fac_results}
%
Fig.~\ref{fig:factorizer-main-results} compares the accuracy (left) and the number of iterations (right) of various \name configurations with $D_p$=$512$ and $F$=$2$. 
%
Dotted lines indicate a less than $99\%$ factorization accuracy. 
%
Starting with binary SBC vectors and the dot-product similarity metric, we can see that this \name configuration fails to solve any problem of size larger than $10^3$ accurately.
%
The operational capacity increases to $4.2\cdot10^3$ when relaxing the sparsity constraint of binary SBCs by allowing for \gls{code} representations inside \name. 
%$4200$.
%
However, the convergence speed is still low, requiring almost as many searches as the brute-force approach.
% 
The introduction of the $\ell_\infty$-based similarity increases the operational capacity by more than an order of magnitude. 
%
We can also notice a drastic reduction in the number of iterations necessary for converging to the correct solution. 
%
For problem sizes up to $10^{4}$, \name needs only $2$ iterations to converge, the minimum possible number of iterations to detect convergence reliably.
%
However, as the problem size goes beyond $1.2\cdot10^5$, \name starts encountering limit cycles and spurious fixed points, which hinder its convergence to the correct solution~ \cite{Resonator2}. 
%
To this end, we introduce the threshold nonlinearity coupled with conditional random sampling. 
%
With these new dynamics, \name further increases the operational capacity by over an order of magnitude to $5\cdot 10^6$. 

%This factorizer model is used to obtain the results in figures \ref{fig:figure3} and \ref{fig:figure4}.
%



\begin{figure*}
    \centering
    \includegraphics[width=.99\textwidth]{figures/figure3.pdf}
     \vspace*{-2mm}
    \caption{Effect of the dimension $D_p$ on number of iterations for \name with $B$=4, $F$=2 (left) and $F$=3 (right).
    }
  \label{fig:figure3}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=.99\textwidth]{figures/figure4.pdf}
    \vspace*{-2mm}
    \caption{Effect of the number of blocks $B$ on number of iterations for \name with $D$=512, $F$=2 (left) and $F$=3 (right).
    }
  \label{fig:figure4}
\end{figure*}

Next, we analyze \name's decoding performance for a varying number of blocks ($B$), vector dimensions ($D_p$), and numbers of factors ($F$).
%
Fig.~\ref{fig:figure3} shows the number of iterations of \name for $F$=2 (left) and $F$=3 (right) factors when varying the vector dimension.
%
For both two and three factors, the operational capacity and convergence speed increase with growing vector dimensionality. 
%This can be attributed to the observation that bundling capacity increases with increased $D_{p}$, which permits us to use a larger $A$ hyperparameter, covering a larger portion of the search space in each iteration. 
%
%For $D_p$=256, the operational capacity of the SBC factorizer lies at 316,227.
%
As we move from two to three factors, the operational capacity remains approximately the same while the number of decoding iterations increases. 
%
However, an increase in the number of iterations does not directly lead to higher computational cost as each iteration requires fewer search operations ($F\cdot \sum_f M_f$) for larger $F$ due to the $F$-root dependence of $M_f$. 
%  
For example, at $D_p$=$512$ and $\prod_{f=1}^{F} M_f$=$10^6$, \name at $F=2$ requires, on average, $15.73$ iterations corresponding to a total of $31,460$ searches, whereas at $F=3$ it requires, on average, $85.17$ iterations and $25,551$ searches. 
% 


% However, the computational cost of a single iteration is lower when using a larger number of factors. This is due to the fact that for large problems the computational complexity is dominated by the complexity of associative memory search over codebooks whose sizes are proportional to the $F$-th root of the total problem size.
%(see discussion in Appendix 3.4).
%For higher dimensions, the SBC factorizer increases its operational capacity, while reducing the average number of iterations for correctly factorizing a product vector for at a fixed problem size. 
%
%For $D_p$=2048 the operational capacity lies beyond 1e8, outperforming operational capacity of the baseline resonator with the same number of the dimensions by more than two orders of the magnitude, while still requiring fewer iterations.
%

%Fig.~\ref{fig:figure4} shows a similar trend when using $F$=3 factors.
% 
%Generally, the factorizer with $F$=$3$ achieves a similar operational capacity compared to $F$=$2$.
%
%The number of iterations is higher, however, the computational cost of searches per iteration is lower when using a larger number of factors (see discussion in Appendix 3.4).
%
%The operational capacity for the baseline resonator with $D_p$=2048 lies at 2,511,886, in close proximity to the operational capacity of the SBC factorizer with $D_p$=512 (5,011,872).
%
% Generally, the operational capacity increases with a growing dimension $D_p$, while the number iterations for a fixed $M$ decreases.
%
%The SBC factorizer with $D_p$=2048 achieves an operational capacity beyond $1e8$, while still requiring fewer operations as compared to the baseline resonator.
%

%
Fig.~\ref{fig:figure4} shows \name's performance for a fixed $D_p$ while varying the number of the blocks ($B$) for two and three factors. 
%
For a very small number of blocks ($B$=2), the operational capacity lies at approximately $10^3$ for both $F$=2 and $F$=3.
%
The operational capacity increases to around $5\cdot10^6$ when $B$=4. 
%
Further increasing the number of blocks ($B\geq8$) exhibits an operational capacity beyond $10^8$, the largest problem size we measured. 
%
The convergence speed peaks at $B$=4 and $B$=8 blocks, and gradually declines as the vectors get denser.
%
Overall, the experimental results shown in Fig.~\ref{fig:figure3} and Fig.~\ref{fig:figure4} demonstrate the broad applicability of our \name: it accurately solves factorization problems within the computational constraints for a wide range of problem sizes, block sizes ($B\geq4$), and number of factors ($F\in\{2,3\}$).

Finally, we compare our \name with the state-of-the-art stochastic factorizer~\cite{langenegger2022imcfac} operating with dense bipolar vectors. 
% 
We fix the problem size to $10^6$ and compare the number of iterations for three configurations according to those featured in \cite{langenegger2022imcfac}, namely $F$=2 with $D_p$=1024, $F$=3 with $D_p$=1536, and $F$=4 with $D_p$=2048. 
%
We configure our \name with $B$=4 blocks. 
%
Table \ref{tab:dense_vs_sparse} summarizes the results. 
%
Both factorizers achieve $>99\%$ accuracy across all configurations, whereby our \name requires up to 6$\times$ fewer iterations. 
%

\begin{table}[]
\caption{Comparison between stochastic factorizer~\cite{langenegger2022imcfac} and our \name at problem size $10^6$. }
\label{tab:dense_vs_sparse}
\centering
% \resizebox{0.7\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{ccrr}
\toprule
& & \multicolumn{2}{c}{Number of iterations}\\
\cmidrule(r){3-4}
$F$ & $D$ &  Dense bipolar~\cite{langenegger2022imcfac} & \name ($B$=$4$)\\
\cmidrule(r){1-2}\cmidrule(r){3-3}\cmidrule(r){4-4}
2 & 1024  & 68.47 & 11.16\\
3 & 1536  & 72.48 & 52.91\\
4 & 2048  & 157.17 & 89.05\\

\bottomrule
\end{tabular}
\end{threeparttable}
% }
\end{table}

\subsection{Ablation study}\label{subsec:bcf-ablation}
This section provides more insights into \name's two main hyperparameters: the threshold ($T$) and the sampling width ($A$). 
% 

\paragraph{Effect of sampling width in an unconditional random sampler}
 
The sampling width ($A$) determines how many codevectors will be randomly sampled and bundled in case the threshold similarity activation results in an all-zero vector. 
%
Intuitively, we expect too low sampling widths to result in a slow walk over the space of possible solutions. 
%
Alternatively, suppose the sampling width is too large (e.g., larger than the bundling capacity). In that case, we expect high interference between the randomly sampled codevectors to hinder the accuracy and convergence speed due to the limited bundling capacity.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/sampling_vs_factorizer.pdf}
    \vspace*{-6mm}
    \caption{Number of iterations when \name is configured as an unconditional random sampler with varying sampling width ($A$). We set $B$=4, $F$=2, and $M_{1}$=$M_{2}$=$M$=$1000$.
    }
  \label{fig:figureS2}
\end{figure}

To experimentally demonstrate this effect, we run \name with $F$=2 in an operational mode that corresponds to unconditional random vector sampling. 
%
Concretely, we fix the sampling width ($A$), generate the initial guess as a bundling of $A$-many randomly selected codevectors, and set both the sparsification threshold ($T$) and the convergence detection threshold ($T_c$) to the inverse sampling width ($1/A$). %
With this configuration, we expect \name to execute a random walk over the solution space with sampling width determining the number of solutions that are simultaneously evaluated. 
%
If the procedure samples the correct solution and the interference from sampled incorrect solutions is low, the correct factor triggers the threshold, and the factorization stops.
% 
As a result, at every iteration, we test $M_f \cdot A^{F-1}$ combinations per factor~$f$. 
%
The expected number of iterations for this procedure is: 
\begin{align}
    \mathbb{E}[t] = \frac{\prod_{f=1}^{F} M_f}{(\sum_{f=1}^{F} M_f)\cdot A^{F-1}}, 
\end{align}
where the numerator reflects the overall problem size, and the denominator the number of combinations the random sampler tests per iteration. 
%
With $F$=2 factors and codebooks of equal size $M_1$=$M_2$=$M$, the expected number of iterations equals $M/(2A)$. 
%with the factor $\frac{1}{2}$ emerging because we can sample two times in each iteration.
% 


Fig.~\ref{fig:figureS2} shows experimental results with this factorizer mode for $F$=2, $M$=$1000$, varying $D_p$ between 512 and 1024. 
%
The expected number of iterations corresponds to the expression $M/(2A)$. 
All presented configurations reach $>99\%$ accuracy, but in a different number of iterations.
%
For small sampling widths, the empirical results match the expectation. 
%
As the sampling width increases, the discrepancy between the empirical and theoretical results grows, more evidently at the smaller dimensions.
%
The discrepancy could stem from the bundling capacity, i.e., the number of retrievable elements, which decreases with a shrinking dimension. 

\paragraph{Effect of sampling width ($A$) in \name}

In this set of experiments, we do not restrict the threshold to be $1/A$.
%
Instead, we run a grid search for each sampling width over threshold values ($T$) in $[0,1]$ and use the optimal value in our benchmarks.

%
Table \ref{tab:varyA} shows how the accuracy and the number of iterations change as we vary the sampling width ($A$) in $\{10,50,100,200,500,1000\}$.
%when the factorizer is run with $F$=2 factors, each one coming from a codebook of size $M_{1}$=$M_{2}$=$M$=$1000$, with $D_{p}$=512. 
%
As expected, there is a sweet spot for sampling width, which in this setting lies at around 100. Smaller values of sampling widths do not negatively impact the accuracy, but convergence speed does decrease. As we increase the sampling width beyond 100, the accuracy drops.
%
Moreover, with a fine-tuned threshold value, we can factorize product vectors significantly faster (15.73 iterations) than when \name is run in the unconditional random sampling mode (38.87 iterations).


\begin{table}[]
\caption{\name performance when varying the sampling width ($A$).\\ $D_p$=512, $F$=2, $M_{1}$=$M_{2}$=$1000.$}
\label{tab:varyA}
\centering
\resizebox{0.7\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{rrrr}
\toprule
         \multicolumn{1}{c}{A}            & \multicolumn{1}{c}{$T^*$} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Num. Iters.}\\
         \hline \\[-1.8ex]
                     %\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-9}
10 & 0.00602 & 99.4\% & 39.16\\
50 & 0.00641 & 99.4\% & 17.86\\
100 & 0.00641 & 99.4\% & 15.73\\
500 & 0.00722 & 98.6\% & 24.25\\
1000 & 0.00441 & 46.9\% & 276.78\\

\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}


\paragraph{Similarity metric}
\begin{figure}
\centering
\subfloat[$\ell_{\infty}$-based similarity.]{\includegraphics[width=0.47\linewidth]{figures/sims_inf_M200.pdf} } \,
\subfloat[Dot-product similarity.]{\includegraphics[width=0.47\linewidth]{figures/sims_dotp_M200.pdf} }
\caption{Log-scale histograms of $\ell_{\infty}$-based and the dot-product similarity values. \name with $D_p$=512, $B$=4, $F$=2, $M_1$=$M_2$=200, and $T$=0.}
\label{fig:figureS1}
\end{figure}

%
Here, we compare the $l_{\infty}$-based similarity with the dot-product similarity by considering the similarity distributions of the associative memory search inside \name. 
%
We select a problem size that cannot be solved by the dot-product similarity, but can be solved by the $s_{\infty}$-based similarity. 
%
The threshold nonlinearity and conditional random sampling are disabled. 
%
For $F$=2, $B$=4, and $D_{p}$=512, one such problem size is $4\cdot 10^4$.
%
We execute the decoding for two iterations and show the resulting histogram in Fig.~\ref{fig:figureS1}.
%
The $l_{\infty}$-based similarity tends to induce sparse activations: most similarities have a value of 0 and will have no effect on the weighted bundling of codevectors. 
%
Conversely, the dot-product similarity exhibits a wider distribution with almost no zero-valued similarities.
%
Finally, the $l_{\infty}$-based similarity was able to find the correct solution (similarity value of 1), whereas the dot-product similarity was not. 

% The difference in behavior is not particularly surprising. The codebooks $\mathbf{X}^f$:=$\lbrace\mathbf{x}_i^1 \rbrace_{i=1}^{M_f}$ contain binary SBC vectors. When we measure the similarity of the unbound factor estimate with any single codevector, we see that under the $s_{\infty}$ similarity the two are orthogonal if at \emph{any} location at which the codevector contains a 1, the unbound estimate contains a 0. Under the dot product similarity, for the two to be orthogonal, it has to hold that at \emph{each} location at which the codevector contains a 1, the unbound estimate contains a 0.




\section{Part~II: Effective Replacement of Large FCLs with Block Code Factorizers}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/FCLreplacement.pdf}
    \caption{Replacement of a large FCL with our \name (b) without or (c) with a projection $\mathbf{W'}$. 
    %
    % (a) CNNs commonly use a large final FCL for doing the classification, where the number of trainable parameters of the FCL is dominant of the overall architecture.
    %
    % We replace the FCL by our SBC \name, interfacing it with the convolutional layers either with (b) a projection layer $\mathbf{W'}$ or (c) directly without any projection. 
    }
  \label{fig:FCLreplacement}
\end{figure}

%
So far, we have applied our \name on synthetic (i.e., exact) product vectors. 
% 
In this section, we present our second contribution, where we expand the application of our \name to classification tasks in deep CNNs.
%
This is done by replacing the large final FCL in CNNs with our \name, as shown in Fig.~\ref{fig:FCLreplacement}. 
%
Instead of training $C$ hyperplanes for $C$ classes, embodied in the trainable weights $\mathbf{W}\in \Reals^{D_i \times D_o}$ of the FCL, where $C$=$D_o$, we represent the classes with a fixed binary SBC product space $\mathbf{P}\in \{0,1\}^{D_p \times D_o}$.
%
The product space requires only $B \cdot \sum_{f=1}^{F} M_f$ fixed integer values to be stored with the binary SBC offset notation accounting for 256 values on ImageNet-1K with $B$=$4$ and $M_1$=$M_2$=32. 
% 
We provide two variants to interface the $D_i$-dimensional output features of the CNN's final convolutional layer with our \name, depicted in Fig.~\ref{fig:FCLreplacement}b and Fig.~\ref{fig:FCLreplacement}c. 
% 
In the first variant, \name is directly interfaced with the CNN's output features; hence, the dimensionality becomes $D_p$=$D_i$. 
%
The second variant uses an intermediate, trainable projection $\mathbf{W'}\in \Reals^{D_i \times D_p}$, where $D_p \ll D_o$, % 
The number of parameters is significantly reduced in both variants, by $D_o\cdot D_i$ without the projection, and by $(D_o-D_p)\cdot D_i$ with the projection. 

\subsection{Casting classification as a factorization problem}
%
  First, we describe how the classification problem can be transformed into a factorization problem. 
% 
The codebooks and product space are naturally provided if a class is a combination of multiple attribute values.
% 
For example, the RAVEN dataset contains different objects formed by a combination of shape, position, color, and size. 
% 
Hence, we define four codebooks ($\mathbf{X}^1$, $\mathbf{X}^2$, $\mathbf{X}^3$, and $\mathbf{X}^4$) where the size of each codebook ($M_f$) corresponds to the number of values the individual attribute can have~\cite{NVSA} (e.g., the codebook $\mathbf{X}^1$ representing five shapes has $M_1$=$5$ elements). 
% 
The resulting product space is $\mathbf{P}$=$\mathbf{X}^1\oast\mathbf{X}^2\oast\mathbf{X}^3\oast\mathbf{X}^4$. 

% arbitrary classification problem
If no such semantic information is available, the codebooks and product space are chosen arbitrarily. 
%
When targeting two factors, we first define a product space $\mathbf{P}$=$\mathbf{X}^1 \oast \mathbf{X}^2$ that contains $M_1\cdot M_2$ unique quasi-orthogonal product vectors. 
% 
The size of the product space is set to the number of classes $C$, such that each product vector in $\mathbf{P}$ can be assigned to a unique class. 
% 
For example, for representing the $C$=$100$ classes in the CIFAR-100 dataset, we define a product space with size $100$ using two codebooks of size $M_1$=$M_2$=$\sqrt{C}$=$10$. 
% 
Then, the product vector $\mathbf{p}_1:= \mathbf{x}^1_1\oast\mathbf{x}^2_1$ belongs to ``\textit{class 1}'' and  $\mathbf{p}_{100}:= \mathbf{x}^1_{10}\oast\mathbf{x}^2_{10}$ to ``\textit{class 100}''\footnote{If $\sqrt{C}$ is not an integer, we take the ceiling of $\sqrt{C}$.}.
%and ignore the products that are left over. E.g., for $C$=$1000$ we use $M_1$=$M_2$=$32$, yielding $M_1\cdot M_2$=$1024$, and ignore $24$ products.}. 


\subsection{Training CNNs with blockwise cross-entropy loss}
After defining the product space, we train a function $f_\theta$ (e.g., a CNN) with trainable parameters $\theta$ to map the input data (e.g., images) to the target product vectors of the corresponding classes. 
%
Fig.~\ref{fig:FCLtraining}a illustrates the training procedure.
%
Given a labeled training sample $(\mathbf{I},y)$ containing the input image $\mathbf{I}\in \Reals^{c \times h \times w}$ and the target label $y$, we first pass the image through the function $f_\theta$, yielding $\mathbf{q}=f_\theta(\mathbf{I})$. 
% 
Next, we generate the target product vector by mapping the target label $y$ to the factor indices ($f^1$ and $f^2$) and forming the corresponding product $\mathbf{p} = \mathbf{x}^1_{f^1} \oast \mathbf{x}^2_{f^2}$. 
%

A typical loss function for binary sparse target vectors is the binary cross-entropy loss in connection with the sigmoid nonlinearity.  
% 
However, we experienced a significant classification drop when using the binary cross-entropy loss; e.g., the accuracy of MobileNetV2 on the ImageNet-1K dataset dropped below 1\% when using this loss function. 
% 
To this end, we propose a novel blockwise loss that computes the sum of per-block categorical cross-entropy loss (CEL). 
%
For each block $b$, we extract the $L$-dimensional block $\mathbf{q}_b:=\mathbf{q}[(b-1)L+1:bL]$ from the output features $\mathbf{q}$ and the target index $\mathbf{\dot{p}}[b]$. 
%
Then, the blockwise CEL is defined as: 
%


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/FCLtraining.pdf}
    \caption{Training and inference with \name. 
    }
  \label{fig:FCLtraining}
\end{figure}

\begin{align}
    \mathcal{L}\left(\mathbf{q}, \dot{\mathbf{p}}, s\right) = \frac{1}{B }\sum_{b=1}^B \mathcal{L}_{\mathrm{CEL}}\left( s \cdot \mathbf{q}_b,\mathbf{\dot{p}}[b] \right),
\end{align}
where $\mathcal{L}_{\mathrm{CEL}}$ is the categorical CEL, which combines the softmax activation with the negative log-likelihood loss, and $s$ a trainable inverse softmax temperature for improved training~\cite{hoffer2018fix,RandomClassVec2021,scott2021mises}. 
% 
The loss function $\mathcal{L}$ is minimized by batched stochastic gradient descent (SGD). 



\subsection{\name-based inference}
The \name-based inference is illustrated in Fig.~\ref{fig:FCLtraining}b. 
%
We pass a query image ($\mathbf{I}$) through the CNN, yielding the query product ($\mathbf{q}$), which can be interpreted as a ``noisy'' version of the ground-truth product vector $\mathbf{p}$. 
%
We then search for the product vector $\hat{\mathbf{p}} \in \mathbf{P}$ with the highest similarity to $\mathbf{q}$. 
% 
One baseline is to compute the similarity between $\mathbf{q}$ and each product vector in $\mathbf{P}$ in a brute-force manner; however, this requires many similarity computations and the storage of each product vector in $\mathbf{P}$. 
% 
Instead, we search for the closest product vector by factorizing the query product vector using \name, shown in Fig.~\ref{fig:FCLtraining}b. 
% 

% nonlinearity 
% Binary SBC factorizers exclusively operate on maximally sparse binary vectors; hence, the output of CNN had to be passed through a blockwise argmax activation, resulting in accuracy degradation. 
%
% Instead, our SBC factorizer can operate on product vectors with arbitrary sparsities as long as the vectors have the blockwise structure.
% 
We pass the output of the CNN through a blockwise softmax function with an inverse softmax temperature $s_F$, which shapes and normalizes the blockwise distribution of the query vector.
% 
The optimal inverse softmax temperature was found to be $s_F$=$1.5$, based on a grid search on the ImageNet-1K training set with MobileNetV2, and applied for all architectures. 
\begin{table*}[h!]
\centering
\caption{Comparison of approaches which replace the final FCL without any projection layer ($D_i$=$D_p$). We report the average accuracy $\pm$ the standard deviation over five runs with different seeds for the baseline and our \glspl{code}.
}
\label{tab:noFC}
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{lrccccrcccrrr}
\toprule
 &       &    &          \multicolumn{10}{c}{FCL replacement approach} \\
 \cmidrule(r){4-13}
 &       &    &          &                & \multicolumn{3}{c}{Bipolar dense} & \multicolumn{5}{c}{Our \name ($B$=$4$)}        \\
\cmidrule(r){6-8}\cmidrule(r){9-13}
\begin{tabular}[c]{@{}l@{}}Dataset/\\ architecture\end{tabular}  & ($D_i$, $D_o$) &\begin{tabular}[c]{@{}c@{}}Baseline \\ acc. \end{tabular} & \begin{tabular}[c]{@{}c@{}}Had. \\ acc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Id. \\ acc.\end{tabular} &\begin{tabular}[c]{@{}c@{}}BF \\ acc.\end{tabular}      & \begin{tabular}[c]{@{}c@{}}Res. \\ acc.$^*$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. \\ iter.$^*$\end{tabular}    & \begin{tabular}[c]{@{}c@{}}BF \\ acc.\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Fac. \\ acc.\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Avg. \\ iter.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Param. \\saving$\downarrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}FCL\,comp. \\saving$\downarrow$\end{tabular} \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-8}\cmidrule(r){9-9}\cmidrule(r){10-13}\morecmidrules\cmidrule(r){10-13}
\textbf{ImageNet-1K}     \\
ShuffleNetV2          & (1024, 1k) &  $69.22^{\pm0.20}$  &  $68.02$  &     $67.62$ & $66.17$  & $54.54$  & $150$ & $65.09^{\pm0.10}$ & $64.76^{\pm0.13}$  & $7$ &  $44.5\%$ & $55.2\%$\\
MobileNetV2           & (1280, 1k)  &  $71.57^{\pm0.13}$  &  $71.30$       &     $70.72$ & $70.64$ & $60.83$  & $150$ &  $70.00^{\pm0.07}$ & $69.76^{\pm0.13}$ & $6$ &  $37.6\%$ & $61.6\%$\\
ResNet-18             & (512, 1k) &  $70.39^{\pm0.11}$  &  N/A        &  N/A        &   $68.65$ & $54.17$  & $150$ &$68.44^{\pm0.08}$ & $68.00^{\pm0.07}$ & $7$ & $4.4\%$ & $55.2\%$\\
ResNet-50             & (2048, 1k)  &  $76.21^{\pm0.28}$  &  $75.30$       &  $74.65$  & $75.80$ & $67.98$ & $150$  & $76.34^{\pm0.04}$ & $76.25^{\pm0.07}$ & $5$ & $8.0\%$ & $68.0\%$\\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-8}\cmidrule(r){9-9}\cmidrule(r){10-13}\morecmidrules\cmidrule(r){10-13}
\textbf{CIFAR-100} \\
ResNet-18             & (512, 100)  &  $78.10^{\pm0.31}$  &  $77.21$  &  $76.56$    & $76.63$ & $71.15$ & $150$ & $77.31^{\pm0.15}$ & $77.19^{\pm0.17}$ & $2$ &  $0.5\%$ & $60.0\%$ \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-8}\cmidrule(r){9-9}\cmidrule(r){10-13}\morecmidrules\cmidrule(r){10-13}
\textbf{RAVEN} \\
ResNet-18             & (512, 4.2k)  &  $99.88^{\pm0.01}$  &  N/A       & N/A       &   $99.89$  &  $94.92$ & $45$  &  $99.87^{\pm0.01}$  &  $99.82^{\pm0.02}$ & $16$  & $16.2\%$ & $86.7\%$ \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-8}\cmidrule(r){9-9}\cmidrule(r){10-13}\morecmidrules\cmidrule(r){10-13}
p-value            & \multicolumn{1}{c}{--}  &  \multicolumn{1}{c}{--}  &  $0.125$       & $0.125$       &   $0.063$  &  $0.031$ & \multicolumn{1}{c}{--}  &  \multicolumn{1}{l}{$0.094$}  & \multicolumn{1}{l}{$0.063$}   & \multicolumn{1}{r}{--} & \multicolumn{1}{c}{--} & \multicolumn{1}{c}{--} \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item[] {Acc.= Accuracy (\%); N/A= Not applicable; Had.= Reproduced Hadamard~\cite{hoffer2018fix}; Id.= Reproduced Identity~\cite{qian2020doweneed}; BF= Brute-force; Res.= Resonator nets~\cite{Resonator1}; Fac.= Block code factorizer whereby it sets $F$=$2$ for ImageNet-1K and CIFAR-100, and $F$=$4$ for RAVEN.
p-value is determined by signed Wilcoxon test with respect to baseline accuracy.}
\item[]$^*$ Maximum number of iterations was increased to $N$=$150$ for better performance in the resonator nets that leads to 10$\times$ more operations than the brute-force search. 
\end{tablenotes}
\end{threeparttable}
}
\end{table*}

\subsection{Experimental Setup}

% We evaluate our methods on ImageNet-1K~\cite{deng2009imagenet} with $C$=$1000$ classes, on CIFAR-100~\cite{krizhevsky2009learning} with $C$=$100$ classes, and on the RAVEN dataset~\cite{Raven_19} with $C$=$4200$ attribute combinations. 
\paragraph{Datasets} We evaluate our new method on three image classification benchmarks.\\ 
\textbf{ImageNet-1K.} The ImageNet-1K dataset~\cite{deng2009imagenet} is a large-scale image classification benchmark with colored images from $C$=$1000$ different classes. 
% 
The training set contains over $1.2$\,M samples, and the validation set includes $50,000$ samples ($50$ per class), all with a resolution of $224 \times 224$.\\ 
% 
\textbf{CIFAR-100.} The CIFAR-100 dataset~\cite{krizhevsky2009learning} contains colored images with a resolution of $32 \times 32$ from $C$=$100$ classes. 
% 
The dataset provides 500 samples for training and 100 for testing per class.\\
% 
\textbf{RAVEN.} The RAVEN dataset~\cite{Raven_19} contains Raven's progressive matrices tests with gray-scale images with a resolution of $160\times 160$.
% 
A test provides 8 context and 8 answer panels, each containing objects with the following attributes:  position, type, size, and color. 
%
In this work, we exclusively target the recognition of single objects inside the panels.
%
Therefore, we extract all panels with single objects from the center, 2x2 grid, and 3x3 grid constellation.
% 
The extraction gives us a dataset with $136,321$ panels for training, $45,144$ for validation, and $45,144$ for testing. 
%
We combine the positions from the different constellations, yielding 14 unique positions: 1 for the center, 4 for the 2x2 grid, and 9 for the 3x3 grid. 
% 
Overall, this dataset contains $C$=$4200$ attribute combinations (14 positions $\times$ 5 types $\times$ 6 sizes $\times$10 colors). 

\paragraph{Architectures}
% 
ShuffleNetV2~\cite{ShuffleNetV2_ECCV2018}, MobileNetV2~\cite{MobileNetV2_CVPR2018}, ResNet-18, and ResNet-50~\cite{he2016deep} serve as baseline architectures. 
%
In addition to our \name-based replacement approach, we evaluate each architecture with the bipolar dense resonator~\cite{Resonator1, Resonator2}, the Hadamard readout~\cite{hoffer2018fix}, and the Identity readout~\cite{qian2020doweneed}. 
% 
For the FCL replacement strategies without the intermediate projection layer, we removed the nonlinearity and batch norm of the last convolutional layer of all CNN architectures. 
% 
This significantly improved the accuracy of all replacement strategies; e.g., the accuracy of MobileNetV2 with the Identity replacement improved from $60.28\%$ to $70.72\%$ when removing the batch norm and the ReLU6 of the last convolutional layer. 
% 
All other architectural blocks remained the same, including shortcuts in the last block of the ResNet-18 and ResNet-50. 
%
To apply the Identity approach where $D_i \neq D_o$, we used an identity matrix that reads out the first $D_o$ elements of the output feature vector and ignores the remaining $D_i-D_o$ elements. 
% 
We could not adjust the dimension $D_i$ since it would have required additional adaptations in the CNNs, e.g., a downsampling layer in the shortcut connection of ResNet-50. 

\paragraph{Training setup}
%
The CNN models are implemented in PyTorch (version 1.11.0) and trained and validated on a Linux machine using up to 4 NVIDIA Tesla V100 GPUs with 32\,GB memory.
%
We train all CNN architectures with SGD with architecture-specific hyperparameters, described in Appendix~\ref{appx:cnn-training}.
% 
Overall, for each architecture, we use the same training configuration for the baseline and all replacement strategies (i.e., Hadamard, Identity, resonator networks, and our \name). 
% Note that, given an architecture and a dataset, we use the same training configuration for the baseline and all replacement strategies. 
% 
We repeat the main experiments five times with a different random seed and report the average results and standard deviation to account for training variability.
%
%See Appendix~2 for a more detailed description of our setup.
%




% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[h!]
\centering
\caption{Classification accuracy when interfacing the last convolution layer with \name using a projection layer with $D_p$=$512$. 
% We report the average accuracy $\pm$ the standard deviation over five runs. 
}
\label{tab:withFC}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{lrrcccrrc}
\toprule
 & \multicolumn{3}{c}{Baseline} & \multicolumn{5}{c}{Our \name ($D_p$=$512$, $B$=$4$)}                                                                      \\
\cmidrule(r){2-4}\cmidrule(r){5-9}
  \begin{tabular}[c]{@{}l@{}}Dataset/\\ architecture\end{tabular}                            & ($D_i$, $D_o$) & \# Param.           & Acc.          & \begin{tabular}[c]{@{}c@{}}BF \\ acc.\end{tabular}    & \begin{tabular}[c]{@{}c@{}}Fac. \\ acc.\end{tabular}    & \begin{tabular}[c]{@{}c@{}}Avg. \\ iter.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Param. \\ saving$\downarrow$\end{tabular}  & \begin{tabular}[c]{@{}c@{}}FCL\,comp. \\ saving$\downarrow$\end{tabular} \\
\cmidrule(r){1-1}\cmidrule(r){2-4}\cmidrule(r){5-5}\cmidrule(r){6-9}\morecmidrules\cmidrule(r){6-9}
\textbf{ImageNet-1K}     \\
ShuffleNetV2             & (1024, 1k) &  $2.3$\,M  &  $69.22^{\pm0.20}$  &  $68.67^{\pm0.11}$  &  $68.41^{\pm0.14}$ & $6$  & $21.7\%$  & $29.6\%$ \\
MobileNetV2              & (1280, 1k) & $3.4$\,M  &  $71.57^{\pm0.13}$  &  $71.69^{\pm0.11}$  &  $71.49^{\pm0.10}$ & $6$ & $18.4\%$ & $33.4\%$  \\
ResNet-18                & (512, 1k) & $11.5$\,M  &   $70.39^{\pm0.11}$   &  $69.57^{\pm0.17}$  &  $69.19^{\pm0.13}$ & $6$  & $2.2\%$ & $10.4\%$ \\
ResNet-50                & (2048, 1k) &$25.5$\,M  &  $76.21^{\pm0.28}$  &  $76.72^{\pm0.06}$  &  $76.56^{\pm0.08}$ & $5$ & $3.9\%$ & $40.8\%$  \\
\cmidrule(r){1-1}\cmidrule(r){2-4}\cmidrule(r){5-5}\cmidrule(r){6-9}\morecmidrules\cmidrule(r){6-9}
\textbf{RAVEN} \\
ResNet-18               & (512, 4.2k) & $13.3$\,M  &  $99.88^{\pm0.01}$  &  $99.88^{\pm0.00}$  &  $99.85^{\pm0.02}$ & $16$  & $14.2\%$ & $78.6\%$\\
\cmidrule(r){1-1}\cmidrule(r){2-4}\cmidrule(r){5-5}\cmidrule(r){6-9}\morecmidrules\cmidrule(r){6-9}
p-value              & \multicolumn{1}{c}{--} & \multicolumn{1}{c}{--}  &  \multicolumn{1}{c}{--}  &  \multicolumn{1}{l}{$0.465$}  &  \multicolumn{1}{l}{$0.313$} &  \multicolumn{1}{r}{--} & \multicolumn{1}{c}{--} &\multicolumn{1}{c}{--} \\
\bottomrule
\end{tabular}
}
\end{table*}


%
% Results without projection
\subsection{Comparative Results}
Table~\ref{tab:noFC} compares the classification accuracy of the baseline with various replacement approaches without projection, namely Hadamard~\cite{hoffer2018fix}, Identity~\cite{qian2020doweneed}, bipolar dense~\cite{Resonator1}, and our \name.
%
On ImageNet-1K, \name reduces the total number of parameters of deep CNNs by 4.4\%--44.5\%\footnote{The relative parameter reduction depends on the size of the overall network and the final FCL, which we replace with our method.}, while maintaining a high accuracy within $2.39\%$ across the majority of architectures, with the only exception of ShuffleNetV2 showing $4.46\%$ accuracy drop due to its very large FCL accounting for 44.5\% of total parameters.
% 
Our \name matches the brute-force accuracy within $0.44\%$ in all architectures while requiring only $5$--$7$ iterations on average, despite the query product vectors from the CNNs being ``noisy.''
%
Inspired by extremely fast convergence on the synthetic experiments (see Fig.~\ref{fig:factorizer-main-results}), we could show that \name can match the brute-force accuracy within $0.54\%$ when only allowing up to $N$=$3$ iterations (see Appendix~\ref{appx:n}).  
%
This does not hold for the bipolar dense resonators showing significant accuracy drop (up to $16.22\%$), compared to the brute-force search, despite allowing a high number of iterations ($N$=$150$) and conducting extensive hyperparameter tuning across various loss functions, including arcface~\cite{arcface_19} (see Appendix~\ref{appx:bipolar}). 

\begin{table*}[h!]
\caption{Classification accuracy (\%) on ImageNet-1K for the baseline and the block code-based replacement approaches ($F$=$2$) with different number of blocks ($B$).
%using either brute-force (BF) search or our \name (Fac.). 
Lower number of blocks ($B$) results in higher accuracy.}
\label{tab:B}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrllllllll}
\toprule
                                                                  &    & \multicolumn{2}{c}{ShuffleNetV2} & \multicolumn{2}{c}{MobileNetV2} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{ResNet-50} \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-8}\cmidrule(r){9-10}
\begin{tabular}[c]{@{}l@{}}Classification\\ approach\end{tabular} & B  & \multicolumn{1}{c}{BF}             & \multicolumn{1}{c}{Fac.} & \multicolumn{1}{c}{BF}             & \multicolumn{1}{c}{Fac.}   & \multicolumn{1}{c}{BF}             & \multicolumn{1}{c}{Fac.}& \multicolumn{1}{c}{BF}             & \multicolumn{1}{c}{Fac.}         \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-8}\cmidrule(r){9-10}
Baseline                                                     & --  & $69.22^{\pm0.20}$ & \multicolumn{1}{c}{--} & $71.57^{\pm0.13}$ &   \multicolumn{1}{c}{--} &  $70.39^{\pm0.11}$         &  \multicolumn{1}{c}{--}  &   $76.21^{\pm0.28}$     & \multicolumn{1}{c}{--}      \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-8}\cmidrule(r){9-10}
\multirow{4}{*}{\glspl{code}} & 4  & $65.09^{\pm0.10}$ &    $64.76^{\pm0.13}$ & $70.00^{\pm0.07}$ & $69.76^{\pm0.13}$ & $68.43^{\pm0.08}$    &   $68.00^{\pm0.07}$ & $76.34^{\pm0.04}$ & $76.25^{\pm0.07}$ \\
   & 8  & $64.30$ & $63.65$ & $69.53$ & $69.20$ & $67.90$ & $67.76$ & $76.69$ & $76.48$\\
   & 16 & $63.22$ & $62.04$ & $69.34$ & $68.83$ & $67.02$ & $64.98$ & $76.52$ & $76.27$ \\
   & 32 & $61.96$ & $59.74$ & $69.12$ & $68.35$ & $65.09$ & $61.39$ & $76.08$ & $75.62$ \\
\bottomrule
\end{tabular}
}
\end{table*}
On CIFAR-100, \name matches the baseline within $0.91\%$ with only 2 average iterations; while on RAVEN, it requires a slightly higher number of iterations ($16$) due to the larger number of factors ($F$=$4$) and the asymmetric codebook sizes. 
%
Across all datasets and architectures, our \name reduces the large FCL's computational cost by $55.2$--$86.7\%$.


Considering the other FCL replacement approaches, Hadamard consistently outperforms Identity. 
%
However, both the memory and computation requirements of Hadamard are $\mathcal{O}(D_i \cdot D_o)$, while our \name reduces both of them to $\mathcal{O}(D_i \cdot \sqrt{D_o})$, as $F$=$2$ and $N$=$3$ are constant. 
%
Hence, Hadamard is ineffective for a large value of $D_o$. 
% 
Moreover, Identity is only competitive when $D_i$ is within the range of $D_o$ (MobileNetV2 and ShuffleNetV2); for other combinations, either it is not applicable (ResNet-18 where $D_i <D_o$), or ineffective (ResNet-50 where $D_i > D_o$). 

% Comparison to pruning strategies 
We compare our approach to weight pruning techniques, which usually sparsify the weights in all layers, whereas we focus on the final FCL due to its dominance in compact networks. 
%
Such pruning can be similarly applied to earlier layers in addition to our method. 
%
Pruning the final FCL of a pretrained MobileNetV2 with iterative magnitude-based pruning~\cite{zhu2017prune} yields significant accuracy degradation as soon as more than 95\% of the weights are set to zero. 
%
In contrast, our method remains accurate (69.76\%) in high sparsity regimes (i.e., 99.98\% zero elements). 
% 
See Appendix~\ref{appx:pruning} for more details. 



% Further comparison on CIFAR100
Furthermore, we compare our results with~\cite{RandomClassVec2021}, which randomly initialize the final FCL and keep it fixed during training. 
%
On CIFAR-100 with ResNet-18, they could show that fixing the final FCL layer even slightly improves the accuracy compared to the trainable FCL ($75.9\%$ vs. $74.9\%$; see their Table~1). 
%
However, our \name-based approach outperforms their fixed FCL approach ($77.19\%$) while reducing the memory requirements and the FCL compute cost. 

% Results w/ projectionT
Table~\ref{tab:withFC} shows the performance of our \name when using the projection layer ($D_p$=$512$).
% 
The projection layer improves the \name-based accuracy in all benchmarks, especially in cases where the \name replacement approach faced challenges (i.e., ShuffleNetV2). 
% 
Overall, we reduce the total number of parameters by 2.2\%--21.7\% while maintaining the accuracy within $1.2\%$, compared to the baseline with trainable FCL.
%
% See Appendix~3 for more insights on the number of blocks $B$, dimension $D_p$, number of factors $F$, number of iterations $N$, and additional comparisons to related works. 
% 
% Moreover, our \name reduces the FCL's computational cost by $9.1$--$78.6\%$, compared to the large FCL baseline. 

\subsection{Ablation Study}
We give further insights into the \name-based classification by analyzing the effect of the number of blocks, the projection dimension, the number of factors, and the initialization of the CNN weights. 

%
\paragraph{Number of blocks $B$}
Table~\ref{tab:B} shows the brute-force and \name classification accuracy for block codes with different numbers of blocks. 
% 
The brute-force accuracy degrades as the number of blocks ($B$) increases, particularly in networks where the final FCL is dominant (e.g., ShuffleNetV2). 
% 
These experiments demonstrate that deep CNNs are well-matched with very sparse vectors (e.g., $B$=4), and motivated us to devise a \name that can factorize product vectors with such a low number of blocks. 
%
Our \name achieves an accuracy within $0.43\%$ of the brute-force accuracy for product vectors with $B$=4.
%
For a larger number of blocks ($B\geq8$), our \name matches the brute-force accuracy within $3.7\%$.
% 
Note that \name's hyperparameters were exclusively tuned for $B$=$4$, and then applied for other blocks. 
% 
Hence, \name for $B$=$\{8,16,32\}$ could be further improved by hyperparameter tuning.




% \paragraph{Maximum number of iterations $N$}
% %
% Inspired by extremely fast convergence on the synthetic experiments (see Fig.~), we could show that the \name can match the brute-force accuracy within 0.36\% when only allowing up to N=3 iterations. See Appendix~2.1.

\paragraph{Projection dimension $D_p$}
%
We varied the projection dimension ($D_p$) from 128 (high reduction) to 1000 (no reduction since $D_p$=$D_o$) for MobileNetV2 on ImageNet-1K. 
% 
With an extremely low dimension ($D_p$=$128$), \name shows a $2.86\%$ accuracy drop compared to the baseline with trainable FCL while saving $32.8\%$ of the parameters.
%
When going to higher dimensions (e.g., $D_p$=768), \name even surpasses the baseline accuracy while saving $8.7\%$ of the parameters. 
% 
See Appendix~\ref{appx:d}. 

\paragraph{Number of factors $F$}
%
So far, we have evaluated \name with two factors, each having codebooks of size $M_1$=$M_2$=$32$ on the ImageNet-1K dataset. 
% 
We demonstrate \name's capability with $F$=$3$ codebooks of size $M_f$=$10$, which achieves similar accuracy while requiring a higher number of iterations (11 vs. 6) than $F$=$2$. 
%
However, since each iteration requires fewer search operations for $F$=$3$ ($30$ vs. $64$ searches), the overall saving in computational complexity of the FCL remains similar. 
% 
See Appendix~\ref{appx:f}. 

\paragraph{Initialize ResNet-18 with pretrained weights}
%
Finally, we show that the training of \name-based CNNs can be improved by initializing their weights from a model that was pretrained on ImageNet-1K. 
% 
Table~\ref{tab:pretrain} shows the positive impact of pretraining of ResNet-18 (with projection) on ImageNet-1K. 
% 
The pretraining improves the accuracy of \name by $0.62\%$, compared to the random initialization, when keeping the number of epochs the same.  
% 
Moreover, when reducing the number of training epochs to $75$ and $50$, \name still yielded accurate predictions ($68.83\%$ and $68.05\%$). 
%
This experiment shows that our \name-based method can be applied with reduced training cost if a pretrained model is available. 

\begin{table}[]
\caption{Loading a pretrained ResNet-18 model improves accuracy and training time. Classification accuracy (\%) on ImageNet-1K using \name with ResNet-18 (with projection $D_p$=$512$, $B$=$4$, $F$=$2$). %At the beginning of the training, the ResNet-18's weights are loaded from pretrained ResNet-18 (ImageNet-1K).
}
\label{tab:pretrain}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcrll}
\toprule
         & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Pretrained\\ model\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Training\\ epochs\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}BF\\ acc.\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Fac. \\ acc.\end{tabular}} \\
\cmidrule(r){1-1}\cmidrule(r){2-3}\cmidrule(r){4-5}
Baseline &   \xmark          & 100 & $70.39^{\pm0.11}$  & \multicolumn{1}{c}{--} \\
\glspl{code}     &   \xmark           & 100 & $69.57^{\pm0.17}$ & $69.10^{\pm0.14}$  \\
\cmidrule(r){1-1}\cmidrule(r){2-3}\cmidrule(r){4-5}
\multirow{3}{*}{\glspl{code}}      &   \cmark  & 50 & $68.50^{\pm0.14}$  & $68.05^{\pm0.08}$  \\
      &   \cmark  & 75 & $69.21^{\pm0.09}$  &  $68.83^{\pm0.07}$ \\
      &  \cmark   & 100 & $70.08^{\pm0.16}$  & $69.72^{\pm0.15}$       \\
\bottomrule
\end{tabular}
}
\end{table}


% Ablation study. 


\section{Conclusion}
%
We proposed an iterative factorizer for generalized sparse block codes.
%
Its codebooks are randomly distributed high-dimensional binary sparse block codes, whose number of blocks can be as low as four. 
%
The multiplicative binding among the codebooks forms a quasi-orthogonal product space that represents a large number of class categories, or combinations of attributes.
%
As a use-case for our factorizer, we also proposed a novel neural network architecture that replaces trainable parameters in an FCL (aka classifier) with our factorizer, whose reliable operation is verified by accurately classifying/disentangling noisy query vectors generated from various CNN architectures.
%
This quasi-orthogonal product space not only reduces the memory footprint and computational complexity of the networks working with it, but also can reserve a huge representation space to prevent future classes/combinations from coming into conflict with already assigned ones, thereby further promoting interoperability in a continual learning setting.



% The insight of reserving 

% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper
\bibliographystyle{IEEEtran}
\bibliography{bibliography}


\clearpage
\appendix 
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
This appendix provides more details on the effective replacement of large FCLs using the proposed BCF. 
%
\subsection{Overall CNN training setup}\label{appx:cnn-training}
We train all CNN architectures with SGD with architecture-specific hyperparameters, summarized in Table~\ref{tab:SGDtraining}.
%
The training setups for the different networks mainly differ in the chosen learning rate schedule. 
%
ShuffleNetV2 was trained for $400$ epochs using a learning rate initially set to 0.5 and linearly decreased towards $0$ at every epoch.
%
MobileNetV2 was trained with a cosine learning rate schedule~\cite{loshchilov2016sgdr}, where the learning rate is decreased based on a cosine function (single cycle) from $0.04$ to $0$ within 150 epochs. 
%
An additional warmup period of 5 epochs was used. 
% 
ResNet-18 and ResNet-50 were trained with an exponential decaying learning rate schedule. 
%
For training the ResNet-18 on the RAVEN dataset, we started with a pre-trained model from the ImageNet-1k dataset~\cite{RPM_ImageNet_pretraining_2020}.

\subsection{Loss functions for the bipolar dense resonator network integration} \label{appx:bipolar}
%
We evaluated different loss functions for replacing FCL with the bipolar dense resonator networks. 
%
A standard cross entropy loss operating on the cosine similarities between the query vector and the fixed bipolar vectors, scaled with a trainable inverse softmax temperature $s$~\cite{hoffer2018fix}, yielded good brute-force accuracies but significant drops when integrating the resonator networks. 
% 
For example, when replacing the final FCL of ShuffleNetV2 on ImageNet-1K, we achieved a brute-force accuracy of $66.14\%$ but a much lower resonator network accuracy ($44.88\%$). 
% 
Alternative adaptive cosine-based loss functions, such as AdaCos~\cite{Zhang_2019_adacos}, were ineffective too. 
%

Instead, we found that the fixed but configurable arcface~\cite{arcface_19} loss function is the most suitable for the resonator network integration. 
% 
Arcface computes the angle of the target logit, adds an additive angular margin ($m$) to the target angle, and gets the target logit back again by the cosine function. 
% 
Finally, the logits are rescaled by a fixed scaling factor ($s$) before applying the cross entropy loss. 
% 
To find the optimal hyperparameters ($s,m$), we conducted a grid search across a wide range of configurations ($s\in \{1,10,30,50,70\},m\in \{0.0,0.05, 0.1, 0.15 \}$). 
%
On ImgeNet-1K, the search yielded the parameters ($70, 0.1$) for ShuffleNetV2, ($50, 0.1$) for MobileNetV2, ($70, 0.1$) for ResNet-18, and ($70, 0.1$) for ResNet-50.
%
The optimal parameters for the remaining datasets with ResNet-18 were ($30, 0.1$) on CIFAR-100 and ($10, 0.1$) on RAVEN. 
% 
The large margin separation significantly increased the resonator network-based accuracy, e.g., by $9.66\%$ for ShuffleNetV2 on ImageNet-1K. 

\begin{table}[]
\caption{Hyperparamters for CNN training.}
\label{tab:SGDtraining}
\centering
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{lcrcccccc}
\toprule
                     &                            &                               &                    &                                                        & \multicolumn{4}{c}{Learning rate schedule}                                                                                                                         \\
                     \cmidrule(r){6-9}
         \begin{tabular}[c]{@{}l@{}}Dataset/\\ architecture\end{tabular}            & \multicolumn{1}{c}{Ep.} & \multicolumn{1}{c}{Bs.} & \multicolumn{1}{c}{Mmt.} & Wd. & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Init. \\ value\end{tabular}} & Type  & \begin{tabular}[c]{@{}c@{}}Decay\\ rate\end{tabular} & \begin{tabular}[c]{@{}c@{}}Step\\ size\end{tabular} \\
                     \cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-9}
\textbf{ImageNet-1K} &                            &                               &                                                                            &                                                                           &           &                                &                                \\
ShuffleNetV2         & 400                        & 2048                          & 4e-5  & 0.9                                                                   & 0.5                                                                       & lin    & \multicolumn{1}{c}{--}         & \multicolumn{1}{c}{--}         \\
MobileNetV2          & 150                        & 256                           & 4e-5   & 0.9                                                                     & 0.04                                                                      & cos    & \multicolumn{1}{c}{--}         & \multicolumn{1}{c}{--}         \\
ResNet-18            & 100                        & 256                           & 1e-4   & 0.9                                                                     & 0.1                                                                       & exp & 0.1                            & 30              \\
ResNet-50            & 100                        & 256                           & 1e-4   & 0.9                                                                     & 0.1                                                                       & exp & 0.1                            & 30              \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-9}
\textbf{CIFAR-100}   &                            &                               &                                                                            &                                                                           &           &                                &                                \\
ResNet-18            & 200                        & 128                           & 5e-4    & 0.9                                                                    & 0.1                                                                       & exp & 0.2                            & 60             \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}\cmidrule(r){6-6}\cmidrule(r){7-9}
\textbf{RAVEN}       &                            &                               &                                                                            &                                                                           &           &                                &                                \\
ResNet-18            & 100                        & 256                           & 1e-4   & 0.9                                                                     & 0.1                                                                       & exp & 0.1                            & 50 \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item[] \normalfont{Ep.= Epochs; Bs.= Batch size; Mmt.= Momentum; Wd.= Weight decay.}
\end{tablenotes}
\end{threeparttable}
}
\end{table}

\subsection{Projection dimension $D_p$}\label{appx:d}
%
The main results of \name using the projection layer are reported for \gls{code} vectors with a dimension of $D_p=512$. 
% 
Here, we show that the dimension can be flexibly varied, providing a trade-off between parameter (and therefore computation) saving and accuracy. 
% 
We varied the dimension $D_p$ from 128 (high reduction) to 1000 (no reduction since $D_p$=$D_o$) for MobileNetV2 on ImageNet-1K. 
%
The hyperparameters of \name were kept the same for all $D_p$.
% 
Table~\ref{tab:D} shows the results. 
% 
With an extremely low $D_p$=$128$, \name shows $2.86\%$ accuracy drop compared to the baseline with trainable FCL while saving $32.8\%$ of the parameters.
%
With a larger $D_p \geq 512$, it yields iso-accuracy with the baseline while saving up to $18.4\%$ of the parameters. 
%
With $D_p$=768, \name eventually surpasses the accuracy of the baseline while saving $8.7\%$ of the parameters. 


\begin{table}[t]
\centering
\caption{Classification accuracy (\%) on ImageNet-1K when replacing the FCL in MobileNetV2 with  \name using a projection layer with variable $D_p$. The remaining configurations ($B$=$4$, $F$=$2$) are kept constant.}
\label{tab:D}
\begin{tabular}{lrllr}
\toprule
 & $D_p$ & \multicolumn{1}{c}{BF} & Fac. & \begin{tabular}[c]{@{}l@{}}Param.\\ saving$\downarrow$\end{tabular} \\
 \cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} \cmidrule(r){4-5}
Baseline & --  &      $71.57^{\pm0.13}$ &  \multicolumn{1}{c}{--}    & $0.0\%$ \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} \cmidrule(r){4-5}
\multirow{5}{*}{\glspl{code}} & $128$  &  $70.55^{\pm0.10}$  & $68.71^{\pm0.12}$      &   $32.8\%$  \\
 & $256$  &  $71.15^{\pm0.07}$  & $70.64^{\pm0.13}$     &   $28.0\%$   \\
 & $512$  &  $71.69^{\pm0.11}$  & $71.41^{\pm0.10}$     &   $18.4\%$   \\
 & $768$  &  $71.80^{\pm0.09}$  & $71.64^{\pm0.11}$     &   $8.7\%$    \\
 & $1000$ &  $71.86^{\pm0.16}$  & $71.56^{\pm0.18}$     &   $0.0\%$    \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Maximum number of iterations $N$}\label{appx:n}
%
On ImageNet-1K, the maximum number of iterations of \name with 2 codebooks is set to $N=\lfloor C/(M_1+M_2) \rfloor$=$15$. 
%
However, motivated by the extremely fast convergence on the synthetic product vectors (see Fig.~3 in main text), we show that the maximum number of iterations can be further limited to only $3$ with negligible accuracy loss while reducing the computational cost.
% 
Table~\ref{tab:N} compares the performance on ImageNet-1K between $N$=$15$ and $N$=$3$. 
% 
Across all architectures, our \name matches the brute-force accuracy within $0.44\%$ with $N$=$15$ iterations, and within $0.54\%$ with $N$=$3$ iterations at maximum.
%
In both cases, the average number of iterations is lower than the maximum $N$; thus, the factorizer converges on average before the maximum $N$ is reached. 






\begin{table*}[h!]
\centering
\caption{\name-based replacement approach without and with projection ($D_p$=$512$) when allowing \name a maximum of $N$=$15$ (standard) iterations, or $N$=$3$. 
%It is shown that the maximum number of iterations in SBC factorizer can be effectively reduced to $N$=$3$.
}
\label{tab:N}
\begin{tabular}{lllllll}
\toprule
& & \multicolumn{5}{c}{GSBCs (B=4)} \\
\cmidrule(r){3-7}
 &  &  & \multicolumn{2}{c}{\name ($N$=$15$)} &  \multicolumn{2}{c}{\name ($N$=$3$)} \\
 \cmidrule(r){4-5}\cmidrule(r){6-7}
 & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Baseline\\ acc.\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}BF\\ acc.\end{tabular}} & \multicolumn{1}{c}{Acc.}              & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Avg. \\ iter.\end{tabular}} & \multicolumn{1}{c}{Acc.}             & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Avg. \\ iter.\end{tabular}} \\
  \cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} \cmidrule(r){4-5}\cmidrule(r){6-7}
\textbf{No projection}                                                          \\
ShuffleNetV2           & $69.22^{\pm0.20}$ &  $65.09^{\pm0.10}$ & $64.76^{\pm0.13}$   &   $7.3$ & $64.68^{\pm0.15}$  & $2.4$                                                                           \\
MobileNetV2            & $71.57^{\pm0.13}$ &   $70.00^{\pm0.07}$ & $69.76^{\pm0.13}$ &  $6.2$ &  $69.72^{\pm0.13}$ & $2.3$                                                                           \\
ResNet-18              & $70.39^{\pm0.11}$ & $68.44^{\pm0.08}$ & $68.00^{\pm0.07}$ & $6.7$ & $67.90^{\pm0.07}$ &   $2.4$                                                                        \\
ResNet-50              & $76.21^{\pm0.28}$ & $76.34^{\pm0.04}$ & $76.25^{\pm0.07}$  & $4.8$ & $76.23^{\pm0.07}$  &  $2.2$                                                                          \\
 \cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3} \cmidrule(r){4-5}\cmidrule(r){6-7}
 \textbf{Projection}\\
ShuffleNetV2           &  $69.22^{\pm0.20}$ &  $68.67^{\pm0.11}$ &  $68.41^{\pm0.14}$ &  $6.0$ &  $68.33^{\pm0.13}$ & $2.4$ \\
MobileNetV2            &  $71.57^{\pm0.13}$ &  $71.69^{\pm0.11}$ &  $71.49^{\pm0.10}$ & $5.5$ & $71.44^{\pm0.11}$ & $2.3$                                                                           \\
ResNet-18              &  $70.39^{\pm0.11}$ &  $69.57^{\pm0.17}$ & $69.19^{\pm0.13}$ & $6.2$ &  $69.10^{\pm0.14}$ & $2.4$                                                                      \\
ResNet-50              & $76.21^{\pm0.28}$ &   $76.72^{\pm0.06}$ & $76.56^{\pm0.08}$ & $4.6$ & $76.54^{\pm0.06}$ & $2.2$  \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Number of factors $F$}\label{appx:f}
%
So far, we have evaluated \name with two factors, each having codebooks of size $M_1$=$M_2$=$32$ on the ImageNet-1K dataset. 
% 
We demonstrate its capability with $F$=$3$ codebooks of size $M_f$=$10$ each. 
% 
Consequently, the maximum number of iterations becomes $N$=$\lfloor 1000/30\rfloor$=$33$. 
% 
Table~\ref{tab:F} compares the classification accuracy of the factorizer for $F$=$2$ with $F$=$3$. 
% 
The factorizer with the higher number of factors achieves similar accuracy while requiring higher iterations (11 vs. 6) compared to $F$=$2$. 
% 
However, since each iteration requires fewer search operations for $F$=$3$ ($30$ vs. $64$ searches), the overall compute saving of the FCL remains similar. 
%
Our \name provides a time-space trade-off between the number of factors while keeping the accuracy and the computational cost constant: a low number of factors ($F$=$2$) requires more space to store the codebooks ($2\cdot32$=$64$ codevectors) but features lower average iterations (6), whereas a higher number of factors ($F$=$3$) requires less space ($3\cdot 10$=$30$ codevectors) but more iterations on average (11). 

\begin{table}[]
\caption{Classification accuracy (\%) on ImageNet-1K when replacing the FCL in MobileNetV2 with \name using a projection with $D_p$=$512$ and variable $F$. }
\label{tab:F}
\resizebox{\linewidth}{!}{
\begin{tabular}{llllrr}
\toprule
                     & \multicolumn{1}{c}{F} & \multicolumn{1}{c}{BF} & \multicolumn{1}{c}{Fac.} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Avg. \\ iter.\end{tabular}} & \begin{tabular}[c]{@{}c@{}}FCL\,comp.\\saving$\downarrow$\end{tabular} \\
\cmidrule(r){1-1} \cmidrule(r){2-2} \cmidrule(r){3-6}
Baseline             &  \multicolumn{1}{c}{--}         &   $71.57^{\pm0.13}$  &       \multicolumn{1}{c}{--}                   &     \multicolumn{1}{c}{--} & $0.0\%$                                                            \\
\cmidrule(r){1-1} \cmidrule(r){2-2} \cmidrule(r){3-6}
\multirow{2}{*}{\glspl{code}} &    $2$   &  $71.69^{\pm0.12}$ &   $71.49^{\pm0.10}$& $6$ & $33.4\%$ \\
                     &  $3$   &  $71.61^{\pm0.07}$ & $71.27^{\pm0.10}$ & $11$ & $35.6\%$ \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Comparison with weight pruning}\label{appx:pruning}
% Comparison to pruning strategies 
We compare our approach to weight pruning techniques, which mostly sparsify the weights in all layers, whereas we focus on the final FCL due to its dominance in compact networks. 
%
Such pruning can be similarly applied to earlier layers in addition to our method. 
%
For a one-to-one comparison, we prune the final FCL weights of a pretrained MobileNetV2 using iterative magnitude-based pruning~\cite{zhu2017prune}.
%yielding (80\% zero elements: 71.1\% accuracy), (90\%: 70.4\%), (95\%: 69.5\%), (99\%: 65.8\%), (99.98\%: 18.4\%). 
%
As shown in Table~\ref{tab:pruning}, the accuracy of magnitude-based pruning method quickly degrades as soon as more than 95\% of the weights are set to zero. 
%
In fact, this susceptibility forced related works to prune the final FCL only to 90\%~\cite{schwarz2021powerpropagation}. 
%
In contrast, our method remains accurate in high sparsity regimes: the codebooks store only $B\cdot F\cdot M_f$=$4 \cdot 2 \cdot 32$=$256$ indices instead of $ D_p \cdot C$ values (i.e., 99.98\% zero elements) with an achieved an accuracy of 69.76\%.

\begin{table}[t]
\caption{Comparison of \name with pruning of final FCL weights of a pretrained MobileNetV2 on ImageNet-1K.}
\label{tab:pruning}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{lrl}
\toprule
  Approach       & Zero elements (\%) & Accuracy (\%) \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}
Baseline &    \multicolumn{1}{r}{--} & $71.57^{\pm0.13}$   \\
\name      &  99.98 & $69.76^{\pm0.13}$ \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule(r){3-3}
\multirow{5}{*}{FCL pruning} &      80.00    & 71.13 \\
    & 90.00    & 70.44 \\
    & 95.00    & 69.48 \\
    & 99.00    & 65.82 \\
    & 99.98 & 18.38 \\
\bottomrule
\end{tabular}
% }
\end{table}





\end{document}



