%\IEEEraisesectionheading{\section{Experiments}}
\section{Experiments}
We now validate our method on several benchmark datasets against relevant continual learning baselines on Task-Incremental Learning (TIL) and Few-shot Class Incremental Learning (FSCIL).

\subsection{Task-incremental Learning (TIL)}
First, we consider task-incremental continual learning with a multi-head configuration for all experiments in the paper. We follow the experimental setups in recent works \cite{Saha2021, Yoon2020, deng2021flattening}.

\noindent 
\textbf{Datasets and architectures.} We use six different popular sequential datasets for CL problems with five different neural network architectures as follows: 
\textbf{1) Permuted MNIST (PMNIST):} A variant of MNIST~\cite{LeCun1998} where each task has a deterministic permutation to the input image pixels.
\textbf{2) 5-Datasets:} A mixture of 5 different vision datasets~\cite{Saha2021}: CIFAR-10~\cite{Krizhevsky2009}, MNIST~\cite{LeCun1998}, SVHN~\cite{Netzer2011SVHN}, FashionMNIST~\cite{Xiao2017Fashion}, and notMNIST~\cite{Yaroslav2011notMNIST}. 
\textbf{3) Omniglot Rotation:} An OCR images dataset composed of 100 tasks, each including 12 classes. We further preprocess the raw images by generating their rotated version in $90 \degree,~180\degree, \text{~and~} 270\degree$, followed by \cite{Yoon2020}. \textbf{4) CIFAR-100 Split} \cite{Krizhevsky2009}\textbf{:} A visual object dataset constructed by randomly dividing 100 classes of CIFAR-100 into ten tasks with ten classes per task. \textbf{5) CIFAR-100 Superclass:} We follow the setting from \cite{Yoon2020} that divides CIFAR-100 dataset into 20 tasks according to the 20 superclasses, and each superclass contains five different but semantically related classes. \textbf{6) TinyImageNet}~\cite{Stanford}\textbf{:} A variant of ImageNet~\cite{krizhevsky2012imagenet} containing 40 of 5-way classification tasks with the image size by $64 \times 64\times 3$.

We use two-layered MLP with $100$ neurons per layer for PMNIST, variants of LeNet~\cite{LeCun1998} for the experiments on Omniglot Rotation and CIFAR-100 Superclass experiments. Also, we use a modified version of AlexNet similar to \cite{Serra2018, Saha2021} for the CIFAR-100 Split dataset and a reduced ResNet-18 similar to \cite{chaudhry2019continual, Saha2021} for 5-Datasets. For TinyImageNet, we also use the same network architecture as \cite{gupta2020maml, deng2021flattening}, which consists of 4 Conv layers and three fully connected layers. \\

\noindent 
\textbf{Baselines.} We compare our WSN with strong CL baselines; regularization-based methods: \textbf{HAT}~\cite{Serra2018} and \textbf{EWC}~\cite{Kirkpatrick2017}, rehearsal-based methods: \textbf{GPM}~\cite{Saha2021}, and a pruning-based method: \textbf{PackNet}~\cite{mallya2018packnet} and \textbf{SupSup}~\cite{wortsman2020supermasks}. \textbf{PackNet} and \textbf{SupSup} is set to the baseline to show the effectiveness of re-used weights. We also compare with naive sequential training strategy, referred to as \textbf{FINETUNE}. \textbf{Multitask Learning (MTL)} and \textbf{Single-task Learning (STL)} are not a CL method. MTL trains on multiple tasks simultaneously, and STL trains on single tasks independently.\\

\noindent
We summarize the architecture-based baselines as follows:
\begin{enumerate}[itemsep=0em, topsep=-1ex, itemindent=0em, leftmargin=1.2em, partopsep=0em]
    \item \textbf{PackNet}~\cite{mallya2018packnet}: iterative pruning and network re-training architecture for packing multiple tasks into a single network.
    \item \textbf{SupSup}~\cite{wortsman2020supermasks}: finding supermasks (subnetworks) within a randomly initialized network for each task in continual learning.
    \item \textbf{WSN / \textcolor{magenta}{SoftNet}}~(ours): jointly training model and finding task-adaptive subnetworks of novel/prior parameters for continual learning. Note that, in the inference step, \textcolor{magenta}{SoftNet} is acquired empirically by injecting small noises $U(0,\text{1e-3})$ to the backgrounds $\bm{0}$ of acquired task WSN while holding the foregrounds $\bm{1}$ of WSN. \\ 
\end{enumerate}

%% main table 1
%\input{materials/5_main_table_new}
\input{materials/5_main_table_larger}

%% Experiments with Models
\noindent
\textbf{Experimental settings.} \label{exp_setting} 
As we directly implement our method from the official code of \cite{Saha2021}, we provide the values for HAT and GPM reported in \cite{Saha2021}. For Omniglot Rotation and Split CIFAR-100 Superclass, we deploy the proposed architecture in multi-head settings with hyperparameters as reported in \cite{Yoon2020}. All our experiments run on a single-GPU setup of NVIDIA V100. We provide more details of the datasets, architectures, and experimental settings, including the hyperparameter configurations for all methods in \Cref{app_sec:exper_detail}.   

\input{materials/5_main_plots_acc_vs_cap}

%% Performance Metrics
\noindent 
\textbf{Performance metrics.} We evaluate all methods based on the following four metrics: 
\begin{enumerate}[itemsep=0em, topsep=-1ex, itemindent=0em, leftmargin=1.2em, partopsep=0em]
\item {\textbf{Accuracy (ACC)}} measures the average of the final classification accuracy on all tasks: $\mathrm{ACC}=\frac{1}{T} \sum_{i=1}^{T} A_{T, i}$, where $A_{T,i}$ is the test accuracy for task $i$ after training on task $T$.

\item {\textbf{Capacity (CAP)}} measures the total percentage of non-zero weights plus the prime masks for all tasks as follows: $\mathrm{Capacity}= (1-\mathcal{S}) + \frac{(1-\alpha) T}{32}$, where we assume all task weights of 32-bit precision. $\mathcal{S}$ is the sparsity of $\mathbf{M}_{T}$ and the average compression rate $\alpha \approx 0.78$ that we acquired through 7bit Huffman encoding, which depends on the size of bit-binary maps; the compression rate $\alpha$ also depends on compression methods typically.   

\item {\textbf{Forward Transfer (FWT)}}  measures how much the representations that we learned so far help learn new tasks, namely: $FWT = \frac{1}{T} \sum_{i=2}^{T} A_{i-1, i} - R_i$ where $R_i$ is the accuracy of a randomly initialized network on task $i$.

\item {\textbf{Backward Transfer (BWT)}} measures the forgetting during continual learning. Negative BWT means that learning new tasks causes the forgetting on past tasks: $\mathrm{BWT}=\frac{1}{T-1}\sum_{i=1}^{T-1} A_{T, i}-A_{i, i}$.

\end{enumerate}

% ============ FSCIL experiments ========
\subsection{Few-shot Class Incremental Learning (FSCIL)}
We introduce experimental setups - Few-Shot Class Incremental Learning (FSCIL) settings to provide soft-subnetworks' effectiveness. We empirically evaluate and compare our soft-subnetworks with state-of-the-art methods and vanilla subnetworks in the following subsections. \\

\noindent
\textbf{Datasets.} To validate the effectiveness of the soft-subnetwork, we follow the standard FSCIL experimental setting. We randomly select 60 classes as the base and 40 as new classes for CIFAR-100 and miniImageNet. In each incremental learning session, we construct 5-way 5-shot tasks by randomly picking five classes and sampling five training examples for each class. \\

\noindent
\textbf{Baselines.} We mainly compare our SoftNet with architecture-based methods for FSCIL: FSLL~\cite{mazumder2021few} that selects important parameters for each session, and HardNet, representing a binary subnetwork. Furthermore, we compare other FSCIL methods such as iCaRL~\cite{rebuffi2017icarl}, Rebalance~\cite{hou2019learning}, TOPIC~\cite{tao2020few}, IDLVQ-C~\cite{chen2020incremental}, and F2M~\cite{shi2021overcoming}. We also include a joint training method~\cite{shi2021overcoming} that uses all previously seen data, including the base and the following few-shot tasks for training as a reference. Furthermore, we fix the classifier re-training method (cRT)~\cite{kang2019decoupling} for long-tailed classification trained with all encountered data as the approximated upper bound. \\

\noindent
\textbf{Experimental settings.} The experiments are conducted with NVIDIA GPU RTX8000 on CUDA 11.0. We also randomly split each dataset into multiple sessions. We run each algorithm ten times for each dataset and report their mean accuracy. We adopt ResNet18~\cite{he2016deep} as the backbone network. For data augmentation, we use standard random crop and horizontal flips. In the base session training stage, we select top-$c\%$ weights at each layer and acquire the optimal soft-subnetworks with the best validation accuracy. In each incremental few-shot learning session, the total number of training epochs is $6$, and the learning rate is $0.02$. We train new class session samples using a few minor weights of the soft-subnetwork (Conv4x layer of ResNet18) obtained by the base session learning. We specify further experiment details in Appendix. % APPENDIX


% ============ Section 5 (WSN for TIL) ================
\section{Results On Task-Incremental Learning}
\subsection{Comparisons with Baselines}
We evaluate our algorithm on three-standard benchmark datasets: Permuted MNIST, 5-Datasets, and Omniglot Rotation. We set PackNet and SupSup to baselines as non-reused weight methods and compared WSN with the baselines, including other algorithms as shown in \Cref{tab:main_table_new}. Our WSN outperformed all the baselines in three measurements, achieving the best average accuracy of 96.41\%, 93.41\%, and 87.28\% while using the least capacity compared to the other existing methods, respectively. Moreover, our WSN was proved to be a forget-free model like PackNet. Compared with PackNet, however, WSN showed the effectiveness of reused weights for continual learning and its scalability in the Omniglot Rotation experiments with the least capacities by a large margin. \Cref{fig:acc_vs_cap} provides the accuracy over total capacity usage. Our WSN's accuracy is higher than PackNet's, approximately at 80\% total capacity usage on 5-Dataset, and WSN outperformed others at 80\% total capacity usage on Omniglot Rotation. The lower performances of PackNet might attribute to Omniglot Rotation dataset statistics since, regardless of random rotations, tasks could share visual features in common such as circles, curves, and straight lines. Therefore, non-reused methods might be brutal to train a new task model unless prior weights were transferred to the current model, a.k.a. forward transfer learning. To show the WSN's power of forward transferring knowledge, SoftNet was prepared by initializing zero-part of masks to small random perturbations. Then, we observed that, in the inference step, SoftNet showed great power to transfer knowledge while maintaining WSN's performances of ACC and CAP.    

% = Comparisons with SOTA and Compression and Capacity =
\subsection{Comparisons with the SOTA in TIL}
\input{materials/5_main_table_sota_larger}

\input{materials/5_main_plots_tinyimagenet2}
\input{materials/5_main_plots_capacity_acc}

We use a multi-head setting to evaluate our WSN algorithm under the more challenging visual classification benchmarks. The WSN's performances are compared with others w.r.t three measurements on three major benchmark datasets as shown in \Cref{tab:main_sota_table}. Our WSN outperformed all state-of-the-art, achieving the best average accuracy of 76.38\%, 61.79\%, and 71.96\%. WSN is also a forget-free model (BWT = ZERO) with the least model capacity in these experiments. Note that we assume the model capacities are compared based on the model size without extra memory, such as samples. We highlight that our method achieves the highest accuracy, the lowest capacity, and backward transfer on all datasets. \Cref{fig:main_tinyimg_plots} shows the process of performance and compressed capacity changing with the number of tasks on the TinyImageNet datasets, where the “Average Progressive Capacity” metric is defined as the average capacity (the proportion of the number of network weights used for any one of the tasks) after five runs of the experiment with different seed values. Furthermore, we consistently showed progressively improved performances of WSN than others on CIFAR-100 Split datasets as shown in \Cref{fig:bar_cifar100_10}. The increasing number of reused weights (see \Cref{fig:main_confs_wsn_packnet}) could explain the progressively improved performances as shown in \Cref{fig:main_conf_hard_soft_wsn_cifar100}. Other forward-transferring results are depicted in Appendix.


% = Average performance and bit-maps compression =
\subsection{Forget-Free Performance and Model Capacity}
We prepare results on performance and bit-map compressed capacity on the TinyImageNet dataset as shown in \Cref{fig:main_tinyimg_plots} - “c=0.1” and “c=0.1+7bit-Huffman” refers respectively to using 10\% of network weights and no compression on the binary mask and the latter refers to the same with 7bit-Huffman encoding. In \Cref{fig:main_tinyimg_plots} (a), using initial capacity, $c=0.1$ shows better performances over others. With fixed $c=0.1$, the bit-wise Huffman compression rate delivers positive as the number of tasks increases, as shown in \Cref{fig:main_tinyimg_plots} (b). The most interesting part is that the average compression rate increases as the number of bits to compress increases, and the increasing ratio of reused weights (symbols with a high probability in the Huffman encoding) might affect the compression rate (symbols with a small probability might be rare in the Huffman tree, where the infrequent symbols tend to have long bit codes). We investigated how the compression rate is related to the total model capacity. The more bits the binary mask compression does, the less the model capacity to save is required. This shows that within 40 tasks, N-bit Huffman compressed capacities are sub-linearly increasing as binary map capacities increase linearly. The 7-bit Huffman encoding is enough to compress binary maps without exceeding the model capacity, even though the compression rate $\alpha$ depends on compression methods typically.

% we will re-write this section, entirely after conducting experiments.
\subsection{Catastrophic Forgetting From WSN's Viewpoint} 
We interpreted how reused weights affect the inference performances on the TinyImageNet dataset as shown in \Cref{fig:main_tinyimg_all_acc_plots}. We divide all used weights for solving sequential tasks into specific sets for more precise interpretability. All used weights (a) within a trained dense network are separated as follows:
\textbf{\textit{all used}} represents all activated sets of weights up to task $t - 1$.
\textbf{\textit{per task}} represents an activated set of weights at task $t$.
\textbf{\textit{new per task}} represents a new activated set of weights at task $t$.
\textbf{\textit{reused per task}} represents an intersection set of weights per task and all used weights.
\textbf{\textit{reused for all tasks}} represents an intersection set of weights reused from task $1$ up to task $t-1$.

% More diverse reused Weight for solving TinyImageNet tasks
First, our WSN adaptively reuses weights to solve the sequential tasks. In  \Cref{fig:main_tinyimg_all_acc_plots} (b), initial and progressive task capacity start from $c$ value; the proportion of reused weights per task decreases in the early task learning stage. However, it tends to be progressively saturated to $c=0.1$ since the number of the new activated set of weights decreases, and the proportion of reused weights for all prior tasks tends to decrease. In other words, WSN uses diverse weights to solve the sequential tasks within all used weights rather than depending on the reused weights for all prior tasks as the number of tasks increases.  

% the most important weights reused per task 
Second, our WSN provides a stepping stone for forget-free continual learning. Regarding the benefits of WSN, in \Cref{fig:main_tinyimg_all_acc_plots} (c), we interpret the importance of three types of weights through an ablation study. The additional evaluations were performed by the acquired task binary masks and trained models to investigate the importance of reused weights in each layer, where the "w/" refers to "with reused network weights" and the "w/o" refers to "without reused network weights." Model forgetting occurred from the performances without using weights reused per task severely. The most significant weights were weights reused per task, the subset of all used weights; the importance of the weights reused for all prior tasks decreases as the number of tasks increases since its capacity gets small relatively, as shown in \Cref{fig:main_tinyimg_all_acc_plots} (b). Moreover, in \Cref{fig:main_tinyimg_all_acc_plots} (d), we inspected layer-wise forgetting caused by removing weights reused per task of network layers; the performance sensitivities were quite diverse. In particular, we observed that the most performance drops at the Conv1 layer.

\input{materials/5_acc_bar_plots}
\input{materials/5_main_conf_cifar100_100}

% conclusions: selecting reused weights for other domains
Finally, our WSN reuses weights if the knowledge from previous tasks is already enough to solve the task at hand and employs a few new weights otherwise. Specifically, from task 7, all used weights seem enough to infer all tasks since the weights reused per task catch up with the task performances. For more generalized forget-free continual learning, the model should consider the layer-wise sensitivity of weights reused per task when selecting weights reused for all prior tasks. These analyses might broadly impact other machine learning fields, such as transfer, semi-supervised, and domain adaptation.

%\input{rebuttals/table2_reinit}
%\input{rebuttals/init_reinit}
\input{rebuttals/packnet_supsup}

%\vspace{-0.1in}
%(To be clarified): The effect of weight re-initialization
%\subsection{Re-initialized v.s. Initialized WSN}
%Although the re-initialized weight score explores task-relevant subnetworks, our WSN would lose all task knowledge when the weight score is re-initialized. To validate it, we have analyzed WSN with re-initialization of weight scores, as shown in \Cref{tab:table2_reinit} and \Cref{fig:reinit_tiny}. When the weight score is re-initialized for each task, WSN learns all the weights of the network only after a small number of tasks, and the newly learned weights drop quickly to $0\%$ while the reused weight shared to all tasks also drops to $0\%$ (See \Cref{fig:reinit_tiny} (a)). On the other hand, WSN adaptively selects task-relevant weights. Since WSN does not initialize weight scores for every task, the network weights learned for the prior tasks will be offered to be reused with a "premium" as the weight score is updated for the new task (See \Cref{fig:reinit_tiny} (b)). To this end, WSN learns new tasks by appropriately selecting learned reused weights (substantially includes shared network weights) and small new learned weights. The effectiveness of the reused weights is also proven by the computational efficiency as summarized in \Cref{tab:comp_efficiency}. WSN consistently converged faster than PackNet and SupSup on six different benchmark datasets. In \Cref{app_sec:add_supsup_wsn}, the long sequence of tasks reveals the comparison study between SupSup and Hard-WSN.


%\subsection{Architecture-wise Accuracy}
%Depending on architectures, the performances of subnetworks vary, and the sparsity is also one another: ResNet18 tends to use dense parameters, whereas ResNet20 tends to use sparse parameters on CIFAR-100 for 5-way 5-shot as shown in \Cref{tab:cifar100_5way_5shot_resnet18_20}. The SoftNet with ResNet20 has a more sparse solution as $c=90 \%$ than ResNet18 on this CIFAR-100 FSCIL setting. From these observations, our SoftNet could significantly impact deep neural network architecture search - it helps to search sparse and task-specific architecture.
%\input{soft-materials/6_exper/5_main_res18_20_cifar100_5way_5shot}

%\input{rebuttals/IFSL_5way_5shot}
\input{materials/5_main_confusions}
%\input{materials/5_main_confusion_tinyimagenet2}

\subsection{Sparse Binary Maps}
We prepared task-wise binary mask correlations to investigate how WSN reuses weights over sequential tasks. As shown in \Cref{fig:main_confs_wsn_packnet} (a) and (b), WSN tends to progressively transfer weights used for prior tasks to weights for new ones compared with PackNet. \Cref{fig:main_confs_wsn_packnet} (c) and (d) showed that the tendency of reused weights differs according to the $c$. This result might suggest that more sparse reused binary maps lead to generalization than others. %, as shown in \Cref{app_sec:add_results_binary}, more results are stated. 


% ================== FSCIL ==========
\section{Results On Few-shot CIL}

\input{soft-materials/6_exper/5_main_cifar100_5way_5shot}

\input{soft-materials/6_exper/5_main_miniImageNet_5way_5shot}

\subsection{Results and Comparisons}
% architecture based performances
We compared SoftNet with the architecture-based FSLL and HardNet (WSN) methods. We pick FSLL as an architecture-based baseline since it selects important parameters for acquiring old/new class knowledge. The architecture-based results on CIFAR-100 and miniImageNet are presented in \Cref{tab:main_cifar100_5way_5shot} and \Cref{tab:main_miniImageNet_5way_5shot} respectively. The performances of HardNet show the effectiveness of the subnetworks that go with less model capacity compared to dense networks. To emphasize our point, we found that ResNet18, with approximately $50\%$ parameters, achieves comparable performances with FSLL on CIFAR-100 and miniImageNet. In addition, the performances of ResNet20 with $30\%$ parameters (HardNet) are comparable with those of FSLL on CIFAR-100. 

%as denoted in Appendix of \Cref{tab:main_cifar100_5way_5shot_resnet18} and \Cref{tab:miniImageNet_5way_5shot_baseline}, including performances (\Cref{fig:main_plot_hardsoft_cifar100} and \Cref{fig:main_plot_hardsoft_miniImageNet}) and smoothness in t-SNE plots (\Cref{fig:main_plot_tsne_miniImageNet}).

% Figure 2
Experimental results are prepared to analyze the overall performances of SoftNet according to the sparsity and dataset as shown in \Cref{fig:softnet_cifar100_miniImageNet}. As we increase the number of parameters employed by SoftNet, we achieve performance gain on both benchmark datasets. The performance variance of SoftNet's sparsity seems to depend on datasets because the performance variance on CIFAR-100 is less than that on miniImageNet. In addition, SoftNet retains prior session knowledge successfully in both experiments as described in the dashed line, and the performances of SoftNet ($c=60.0 \%$) on the new class session (8, 9) of CIFAR-100 than those of SoftNet ($c=80.0 \%$) as depicted in the dashed-dot line. From these results, we could expect that the best performances depend on the number of parameters and properties of datasets. %We further result on comparisons of HardNet (WSN) and SoftNet in \Cref{app_sec:results}.

\input{soft-materials/6_exper/5_main_plots_overall_resnet18_cifar100_5way_5shot}

% Comparision with SOTA
Our SoftNet outperforms the state-of-the-art methods and cRT, which is used as the approximate upper bound of FSCIL~\cite{shi2021overcoming} as shown in \Cref{tab:main_cifar100_5way_5shot} and \Cref{tab:main_miniImageNet_5way_5shot}. Moreover, \Cref{fig:overall_classification_acc} represents the outstanding performances of SoftNet on CIFAR-100 and miniImageNet. SoftNet provides a new upper bound on each dataset, outperforming cRT, while HardNet (WSN) provides new baselines among pruning-based methods.

\input{soft-materials/6_exper/5_main_overall_classification_acc}

\subsection{Considerations From SoftNet}
%Based on our thorough empirical study, we uncover the following facts: (1) Depending on architectures, the performances of subnetworks vary, and the sparsity is also one another: ResNet18 tends to use dense parameters, while ResNet20 tends to use sparse parameters on CIFAR-100 FSCIL settings. This result provides the general pruning-based model with a hidden clue. (2) Fine-tuning strategies are essential in retaining prior knowledge and learning new knowledge. We found that performance varies depending on fine-tuning a Conv layer through the layer-wise inspection. Lastly, (3) from overall experimental results, the base session learning is significant for lifelong learners to acquire generalized performances in FSCIL.


%To expand upon the results of our paper, we conduct more experiments on various datasets mentioned in the previous section. We first display the full performance table with more capacity values $c$ employed towards our method in \Cref{tab:main_cifar100_5way_5shot_resnet18} and \Cref{tab:miniImageNet_5way_5shot_baseline}. Next, we identify how choosing a different architecture would impact the performance of our algorithm in \Cref{tab:cifar100_5way_5shot_hardnet_resnet20}. Furthermore, we analyze the performance of our method on the CUB-200-2011 dataset in \Cref{tab:main_cub200_10way_5shot}. 

Through extensive experiments, we deduce the following conclusions for incorporating our method in the few-shot class incremental learning regarding architectures. 

%\noindent 
%\textbf{Structure.} We identified a SubNetwork of ResNet18 and ResNet20 with varying capacities on CIFAR-100 for the 5-way 5-shot FSCIL setting as shown in \Cref{tab:main_cifar100_5way_5shot_resnet18} and \Cref{tab:cifar100_5way_5shot_hardnet_resnet20}. First, according to both tables, our method performs better as we use more parameters within our network. In addition, as denoted in our paper, we see how effective subnetwork is by observing how HardNet, with only 50\% of its dense capacity, achieves comparable performance to methods utilizing dense networks, while SoftNet can do the same with only 30\% of its dense capacity. Furthermore, we argue that our method is architecture-dependent. Our observation from \Cref{tab:cifar100_5way_5shot_hardnet_resnet20} shows that at ResNet18, our architecture performs the best at the maximum capacity of $c=99 \%$, while at ResNet20, we achieve the optimum performance at $c=90 \%$.

\noindent 
\textbf{Comparisions of HardNet (WSN) and SoftNet}. Furthermore, increasing the number of network parameters leads to better overall performance in both subnetworks types, as shown in \Cref{fig:main_plot_hardsoft_cifar100} and \Cref{fig:main_plot_hardsoft_miniImageNet}. Subnetworks, in the form of HardNet and SoftNet, tend to retain prior (base) session knowledge denoted in dashed (\sampleline{dashed}) line, and HardNet seems to be able to classify new session class samples without continuous updates stated in dashed-dot (\sampleline{dash pattern=on .7em off .2em on .05em off .2em}) line. From this, we could expect how much previous knowledge HardNet learned at the base session to help learn new incoming tasks (Forward Transfer). The overall performances of SoftNet are better than HardNet since SoftNet improves both base/new session knowledge by updating minor subnetworks. Subnetworks have a broader spectrum of performances on miniImageNet (\Cref{fig:main_plot_hardsoft_miniImageNet}) than on CIFAR-100 (\Cref{fig:main_plot_hardsoft_cifar100}). This could be an observation caused by the dataset complexity - i.e., if the miniImagenet dataset is more complex or harder to learn for a subnetwork or a deep model as such subnetworks need more parameters to learn miniImageNet than the CIFAR-100 dataset. 

%\input{materials/6_exper/5_main_plots_soft_hard}
\input{soft-materials/6_exper/5_main_plots_cifar100_hard_soft}
\input{soft-materials/6_exper/5_main_plots_miniImage_hard_soft}

\input{soft-materials/6_exper/5_main_plots_tsne}

\noindent 
\textbf{Smoothness of SoftNet.} SoftNet has a broader spectrum of performances than HardNet on miniImageNet. $20\%$ of minor subnet might provide a smoother representation than HardNet because the performance of SoftNet was the best approximately at $c=80\%$. We could expect that model parameter smoothness guarantees quite competitive performances from these results. To support the claim, we prepared the loss landscapes of a dense neural network, HardNet, and SoftNet on two Hessian eigenvectors~\cite{yao2020pyhessian} as shown in Fig. \ref{fig:main_plot_loss_lens}. We observed the following points through simple experiments. From these results, we can expect how much knowledge the specified subnetworks can retain and acquire on each dataset. The loss landscapes of Subnetworks (HardNet and SoftNet) were flatter than those of dense neural networks. The minor subnet of SoftNet helped find a flat global minimum despite random scaling weights in the training process. 

Moreover, we compared the embeddings using t-SNE plots as shown in \Cref{fig:main_plot_tsne_miniImageNet}. In t-SNE's 2D embedding spaces, the overall discriminative of SoftNet is better than that of HardNet in terms of base class set and novel class set. This $70\%$ of minor subnet affects SoftNet positively in base session training and offers good initialized weights in novel session training.

\noindent 
\textbf{Preciseness.} Regarding fine-grained and small-sized CUB200-2011 FSCIL settings as shown in Appendix Table, HardNet (WSN) also shows comparable results with the baselines, and SoftNet outperforms others as denoted in \Cref{tab:main_cub200_10way_5shot}. In this FSCIL setting, we acquired the best performances of SoftNet through the specific parameter selections. As of now, our SoftNet achieves state-of-the-art results on the three datasets.

% ICLR Rebuttals
\subsection{Additional Comparisons with SOTA}
\textbf{Comparisons with SOTA}. We compare SoftNet with the following state-of-art-methods on TOPIC class split~\cite{tao2020few} of three benchmark datasets - CIFAR100 (Appendix, Table. 7), miniImageNet (Appendix, Table. 8), and CUB-200-2011 (Appendix, Table. 9). We summarize the current FSCIL methods such as \textbf{CEC}~\cite{zhang2021few}, \textbf{LIMIT}~\cite{zhou2022few}, \textbf{MetaFSCIL}~\cite{chi2022metafscil}, \textbf{C-FSCIL}~\cite{hersche2022constrained}, \textbf{Subspace Reg.}~\cite{akyurek2021subspace}, \textbf{Entropy-Reg}~\cite{liu2022few}, and \textbf{ALICE}~\cite{peng2022few}. Leveraged by regularized backbone ResNet, SoftNet outperformed all existing current works on CIFAR100,  miniImageNet. On CUB-200-201, the performances of SoftNet were comparable with those of ALICE and LIMIT, considering that ALICE used class/data augmentations and LIMIT added an extra multi-head attention layer.

%\textbf{Comparisons with SOTA}. We compare SoftNet with the following state-of-art-methods on TOPIC class split~\cite{tao2020few} of three benchmark datasets - CIFAR100 (Appendix, \Cref{tab:main_cifar100_5way_5shot_resnet18_split}), miniImageNet (Appendix, \Cref{tab:miniImageNet_5way_5shot_baseline_split}), and CUB-200-2011 (Appendix, \Cref{tab:main_cub200_10way_5shot}). We summarize the current FSCIL methods such as \textbf{CEC}~\cite{zhang2021few}, \textbf{LIMIT}~\cite{zhou2022few}, \textbf{MetaFSCIL}~\cite{chi2022metafscil}, \textbf{C-FSCIL}~\cite{hersche2022constrained}, \textbf{Subspace Reg.}~\cite{akyurek2021subspace}, \textbf{Entropy-Reg}~\cite{liu2022few}, and \textbf{ALICE}~\cite{peng2022few}. Leveraged by regularized backbone ResNet, SoftNet outperformed all existing current works on CIFAR100,  miniImageNet. On CUB-200-201, the performances of SoftNet were comparable with those of ALICE and LIMIT, considering that ALICE used class/data augmentations and LIMIT added an extra multi-head attention layer.



%\input{soft-materials/6_exper/6_main_cifar100_resnet18_5way_5shot_split}

%\input{soft-materials/6_exper/6_main_miniImageNet_5way_5shot_split}

%\input{soft-materials/6_exper/5_main_cub_10way_5shot}

%\textbf{Comparisions of SoftNet and AANet}. Our SoftNet and AANet~\cite{liu2021adaptive} have proposed alleviating catastrophic forgetting in FSCIL and CIL, respectively. AANet consists of multi-ResNets: one residual block learns new knowledge while another fine-tunes to maintain the previously learned knowledge. Through the learnable scaling parameter for the linear combination of the multi-ResNet features, AANet showed outstanding performances in the CSIL setting. However, AANet tends to overfit since the ResNet block’s parameters are fully used to update a few new class data in FSCIL. This point makes it difficult to train AANet on a few samples even though the performance at session 1 is comparable with SoftNet as shown in \Cref{tab:main_cifar100_5way_5shot_resnet18_topic_aanet}.
%\input{soft-materials/6_exper/7_main_cifar100_topic_aanet}


