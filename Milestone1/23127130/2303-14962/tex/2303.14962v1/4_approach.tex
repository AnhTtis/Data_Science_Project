%\section{Forget-Free Continual Learning with Soft-Winning SubNetworks}
\section{Soft-Winning SubNetworks}
In this section, we present our pruning-based continual learning methods, \textit{Winning SubNetworks} (WSN) \cite{kang2022forget} and \textit{Soft-Winning SubNetworks} (SoftNet) \cite{kang2022soft}. In WSN, the neural network searches for the task-adaptive winning tickets and updates only the weights that have not been trained on the previous tasks. After training on each task, the subnetwork parameters of the model are frozen to ensure that the proposed method is inherently immune to catastrophic forgetting. The WSN method is designed to selectively transfer previously learned knowledge to future tasks (i.e., forward transfer), which can significantly reduce the training time needed for convergence during sequential learning. This feature is especially critical for large-scale learning problems where a continual learner trains on multiple tasks sequentially, leading to significant time and computational savings. Originally, SoftNet was proposed to address the issues of forgetting previous sessions and overfitting a few samples of new sessions. To achieve this, the method trains two types of subnetworks concurrently. \\

\input{materials/4_concept_wsn}

\noindent 
\textbf{Problem Statement.} Consider a supervised learning setup where $T$ tasks arrive to a learner sequentially. We denote that $\mathcal{D}_t=\{\textbf{x}_{i,t}, y_{i,t}\}_{i=1}^{n_t}$ is the dataset of task $t$, composed of $n_t$ pairs of raw instances and corresponding labels. We assume a neural network $f(\cdot;\bsy\theta)$, parameterized by the model weights $\bsy\theta$ and standard continual learning scenario aims to learn a sequence of tasks by solving the following optimization procedure at each step $t$: 
\begin{equation}
\bsy \theta^{\ast}=\minimize_{\bsy\theta} \frac{1}{n_t}\sum^{n_t}_{i=1}\mathcal{L}(f(\textbf{x}_{i,t};\bsy\theta), y_{i,t}),
\label{eq:task_loss}
\end{equation}
where $\mathcal{L}(\cdot, \cdot)$ is a classification objective loss such as cross-entropy loss. $\mathcal{D}_t$ for task $t$ is only accessible when learning task $t$. Note rehearsal-based continual learning methods allow memorizing a small portion of the dataset to replay. We further assume that task identity is given in the training and testing stages. 

%To allow room for learning future tasks, a continual learner often adopts over-parameterized deep neural networks. As a continual learner often adopts over-parameterized deep neural networks to allow resource freedom for future tasks, we can find the subnetworks that obtain on-par or even better performance. 
Continual learners frequently use over-parameterized deep neural networks to ensure enough capacity for learning future tasks. This approach often leads to the discovery of subnetworks that perform as well as or better than the original network. Given the neural network parameters $\bsy\theta$, the binary attention mask $\mathbf{m}^*_t$ that describes the optimal subnetwork for task $t$ such that $|\mathbf{m}^*_t|$ is less than the model capacity $c$  follows as:
\begin{equation}
\begin{split}
    \mathbf{m}^*_t &= \underset{\mathbf{m}_t\in\{0,1\}^{|\bsy\theta|}}{\minimize} \frac{1}{n_t}\sum^{n_t}_{i=1}\mathcal{L}\big(f(\textbf{x}_{i,t};\bsy\theta\odot \mathbf{m}_t), y_{i,t}\big) - \mathcal{J}  \\
    &\quad\quad\quad\quad\quad\quad \text{subject to~}|\mathbf{m}^*_t|\leq c,
\end{split}
\label{eq:subnetwork}
\end{equation}
where task loss $\mathcal{J}=\mathcal{L}\big(f(\textbf{x}_{i,t};\bsy\theta), y_{i,t}\big)$ and $c\ll|\bsy\theta|$ (used as the selected proportion $\%$ of model parameters in the following section). In the optimization section, we describe how to obtain $\mathbf{m}^*_t$ using a single learnable weight score $\mathbf{s}$ that is subject to updates while minimizing task loss jointly for each task. %faced by the corresponding continual learner. 

\subsection{Winning SubNetworks (WSN)}\label{sub_sec:wsn}
Let each weight be associated with a learnable parameter we call \textit{weight score} $\mathbf{s}$, which numerically determines the importance of the weight associated with it; that is, a weight with a higher weight score is seen as more important. We find a sparse subnetwork $\hat{\bsy\theta}_t$ of the neural network and assign it as a solver of the current task $t$. We use subnetworks instead of the dense network as solvers for two reasons: (1) Lottery Ticket Hypothesis \cite{frankle2018lottery} shows the existence of a subnetwork that performs as well as the whole network, and (2) subnetwork requires less capacity than dense networks, and therefore it inherently reduces the size of the expansion of the solver.

Motivated by such benefits, we propose a novel \textit{Winning SubNetworks} (WSN\footnote{WSN code is available at \url{https://github.com/ihaeyong/WSN.git}}), which is the joint-training method for continual learning that trains on task - while selecting an important subnetwork given the task $t$ as shown in Fig. \ref{fig:concept_figure}. The illustration of WSN explains how to acquire binary weights within a dense network step by step. We find $\hat{\bsy\theta}_t$ by selecting the $c$\% weights with the highest weight scores $\mathbf{s}$, where $c$ is the target layerwise capacity ratio in \%. A task-dependent binary weight represents the selection of weights $\mathbf{m}_t$ where a value of $1$ denotes that the weight is selected during the forward pass and $0$ otherwise. Formally, $\mathbf{m}_t$ is obtained by applying a indicator function $\mathbbm{1}_c$ on $\mathbf{s}$ where $\mathbbm{1}_c(s)=1$ if $\mathbf{s}$ belongs to top-$c\%$ scores and $0$ otherwise. Therefore, the subnetwork $\hat{\bsy\theta}_t$ for task $t$ is obtained by $\hat{\bsy\theta}_t = \bsy\theta \odot \mathbf{m}_t$.  

\subsection{Soft-Subnetworks (SoftNet)}
Several works have addressed overfitting issues in continual learning from different perspectives, including NCM~\cite{hou2019learning}, BiC~\cite{wu2019large}, OCS~\cite{yoon2022online}, and FSLL~\cite{mazumder2021few}. To mitigate the overfitting issue in subnetworks, we use a simple yet efficient method named \emph{SoftNet} proposed by \cite{kang2022soft}. The following new paradigm, referred to as \emph{Regularized Lottery Ticket Hypothesis}~\cite{kang2022soft} which is inspired by the \emph{Lottery Ticket Hypothesis}~\cite{frankle2018lottery} has become the cornerstone of SoftNet: 

\noindent
\textbf{Regularized Lottery Ticket Hypothesis (RLTH).} \textit{A randomly-initialized dense neural network contains a regularized subnetwork that can retain the prior class knowledge while providing room to learn the new class knowledge through isolated training of the subnetwork.} \\

\noindent
\textbf{SoftNet}. Based on RLTH, we propose a method, referred to as \textbf{Soft}-Sub\textbf{Net}works (\textbf{SoftNet}\footnote{SoftNet code is available at \url{https://github.com/ihaeyong/SoftNet-FSCIL.git}}). SoftNet jointly learns the randomly initialized dense model, and soft mask $\bm{m} \in [0, 1]^{|\bm \theta|}$ on Soft-subnetwork on each task training; the soft mask consists of the major part of the model parameters $m=1$ and the minor ones $m<1$ where $m=1$ is obtained by the top-$c\%$ of model parameters and $m<1$ is obtained by the remaining ones ($100 - \text{top-}c\%$) sampled from the uniform distribution $U(0, 1)$. Here, it is critical to select minor parameters $m<1$ in a given dense network. 

\input{materials/6_main_plots_hard_soft}

\subsection{Convergence of Subnetworks}

\textbf{Convergences of HardNet (WSN) / SoftNet.} To interpret the convergence of SoftNet, we follow the Lipschitz-continuous objective gradients~\cite{boyd2004convex, bottou2018optimization}: the objective function of dense networks $R: \mathbb{R}^d \rightarrow \mathbb{R}$ is continuously differentiable and the gradient function of $R$, namely, $\nabla R: \mathbb{R}^d \rightarrow \mathbb{R}^d$, \textit{Lipschitz continuous with Lipschitz constant} $L > 0$, i.e., 

\begin{equation} \label{eq:dense_lip}
\begin{split}
    || \nabla R(\bm{\theta}) - \nabla R(\bm{\theta}') ||_2 & \leq  L || \bm{\theta} - \bm{\theta}' || \;\; \\ 
    & \text{ for all } \{\bm{\theta}, \bm{\theta}'\} \subset \mathbb{R}^d.
\end{split} 
\end{equation}

% true or not
\noindent
\textbf{Proposition.} Subnetwork achieves a faster rate than dense networks. To prove this, following the same formula, we define the Lipschitz-continuous objective gradients of subnetworks as follows:

\begin{equation} \label{eq:subnet_lip}
\begin{split}
    || \nabla R(\bm{\theta} \odot \bm{m}) - \nabla R(\bm{\theta}' \odot \bm{m}) ||_2 &\leq  L || (\bm{\theta} - \bm{\theta}') \odot \bm{m} || \\ 
    & \text{ for all } \{\bm{\theta}, \bm{\theta}'\} \subset \mathbb{R}^d.
\end{split} 
\end{equation}
where $\bs{m}$ is a binary mask. In comparision of Eq. \ref{eq:dense_lip} and \ref{eq:subnet_lip}, we use the theoretical analysis~\cite{ye2020good} where subnetwork achieve a faster rate of $R(\bm{\theta} \odot \bm{m})=\mathcal{O}(1/||\bm{m}||^2_1)$ at most. The comparison is as follows:
\begin{equation}
\begin{split}
    \frac{|| \nabla R(\bm{\theta} \odot \bm{m}) - \nabla R(\bm{\theta}' \odot \bm{m}) ||_2}{|| (\bm{\theta} - \bm{\theta}') \odot \bm{m} ||}  &< \\
    \frac{|| \nabla R(\bm{\theta}) - \nabla R(\bm{\theta}') ||_2}{|| \bm{\theta} - \bm{\theta}' ||} &\leq L
\end{split}
\end{equation}
The smaller the value is, the flatter the solution (loss landscape) has. The equation is established from the relationship $R(\bm{\theta} \odot \bm{m}) \ll R^\ast(\bm{\theta})$, where $R^\ast(\bm{\theta}$ denotes the best possible loss achievable by convex combinations of all parameters despite $|| (\bm{\theta} - \bm{\theta}') \odot \bm{m} ||<|| \bm{\theta} - \bm{\theta}' ||$. \\

\noindent 
\textbf{Corollary.}
Furthermore, we have the following inequality if $|| R(\bm{\theta} \odot \bm{m}_{hard})-R(\bm{\theta} \odot \bm{m}_{soft})|| \simeq 0$ and $||\bm{m}_{hard}|| < ||\bm{m}_{soft}||$:
\begin{equation} 
\begin{split}
    & \frac{|| \nabla R(\bm{\theta} \odot \bm{m}_{hard}) - \nabla R(\bm{\theta}' \odot \bm{m}_{hard}) ||_2}{|| (\bm{\theta} - \bm{\theta}') \odot \bm{m}_{hard} ||} \geq \\ & \frac{|| \nabla R(\bm{\theta} \odot \bm{m}_{soft}) - \nabla R(\bm{\theta}' \odot \bm{m}_{soft}) ||_2}{|| (\bm{\theta} - \bm{\theta}') \odot \bm{m}_{soft}||}
\end{split}
\end{equation}
where the equality holds \textit{iff} $||\bm{m}_{hard}|| = ||\bm{m}_{soft}||$. We prepare the loss landscapes of Dense Network, HardNet (WSN), and SoftNet as shown in \Cref{fig:main_plot_loss_lens} as an example to support the above subnetwork's inequality.

%The activations are now computed as follows: 
\subsection{Optimization of Winning SubNetworks (WSN)}\label{sub_sec:opt_wsn}
To jointly learn the model weights and task-adaptive binary masks of subnetworks associated with each task, given an objective $\mathcal{L}(\cdot)$, we optimize $\bsy\theta$ and $\mathbf{s}$ with:
\begin{equation}
\label{eq:optimizer}
    \minimize_{\bsy\theta, \mathbf{s}} \mathcal{L} (\bsy\theta \odot \mathbf{m}_t; \mathcal{D}_t).
\end{equation}
However, this vanilla optimization procedure presents two problems: (1) updating all $\bsy\theta$ when training for new tasks will cause interference to the weights allocated for previous tasks, and (2) the indicator function always has a gradient value of $0$; therefore, updating the weight scores $\mathbf{s}$ with its loss gradient is not possible. To solve the first problem, we selectively update the weights by allowing updates only on the weights not selected in the previous tasks. To do that, we use an \textit{accumulate binary mask} $\mathbf{M}_{t-1}=\logicalor_{i=1}^{t-1} \mathbf{m}_i$ when learning task $t$, then for an optimizer with learning rate $\eta,$ the $\bsy\theta$ is updated as follows: 
\begin{equation}
\label{eq:param_update}
\bsy\theta \leftarrow \bsy\theta - \eta \left(\frac{\partial \mathcal{L}}{\partial \bsy\theta} \odot (\mathbf{1}-\mathbf{M}_{t-1})\right),
\end{equation} 
effectively freezing the weights of the subnetworks selected for the previous tasks. To solve the second problem, we use Straight-through Estimator \cite{Hinton2012, Bengio2013, Ramanujan2020} in the backward pass since $\mathbf{m}_t$ is obtained by top-$c\%$ scores. Specifically, we ignore the derivatives of the indicator function and update the weight score as follows: 
\begin{equation}
\label{eq:ste}
\quad \mathbf{s} \leftarrow \mathbf{s} - \eta\left(\frac{\partial \mathcal{L}}{\partial \mathbf{s}}\right).
\end{equation}

The use of separate weight scores $\mathbf{s}$ as the basis for selecting subnetwork weights makes it possible to reuse some of the weights from previously chosen weights $\bsy\theta \odot \mathbf{m}_t$ in solving the current task $t$, which can be viewed as \textit{transfer learning}. Likewise, previously chosen weights that are irrelevant to the new tasks are not selected; instead, weights from the set of not-yet-chosen weights are selected to meet the target network capacity for each task, which can be viewed as \textit{finetuning} from tasks $\{1,...,t-1\}$ to task $t$. Our WSN optimizing procedure is summarized in pseudo algorithm \ref{alg:algorithm1}.

\input{materials/4_algorithm}

% ---- binary mask ----
\subsection{Winning Ticket Encoding}
A binary mask is needed to store task-specific weights for each task in WSN. However, the main issue is that as the number of tasks increases in deep-learning models, the number of binary masks required also increases. We use a compression algorithm to compress all binary task masks to address this capacity issue and achieve forget-free continual learning. To accomplish this, we first convert a sequence of binary masks into a single accumulated decimal mask and then convert each integer into an ASCII code to represent a unique symbol. Next, we apply Huffman encoding~\cite{huffman1952method}, a lossless compression algorithm, to the symbols. We empirically observed that Huffman encoding compresses 7-bit binary maps with a compression rate of approximately 78\% and decompresses them without any bit loss. Furthermore, experimental results demonstrate that the compression rate is sub-linearly increasing with the size of binary bits.


\subsection{SoftNet via Complementary Winning Tickets}
% learnable attention mask (weight score)
Similar to WSN's optimization discussed in \Cref{sub_sec:opt_wsn}, let each weight be associated with a learnable parameter we call \textit{weight score} $\bm{s}$, which numerically determines the importance of the associated weight. In other words, we declare a weight with a higher score more important. At first, we find a subnetwork ${\bm \theta}^\ast = \bm{\theta} \odot \bm{m}^\ast_t$ of the dense neural network and then assign it as a solver of the current session $t$. The subnetworks associated with each session jointly learn the model weight $\bm{\theta}$ and binary mask $\bm{m}_t$. Given an objective $\mathcal{L}_t$, we optimize $\bm{\theta}$ as follows:
\begin{equation}
    \bm{\theta}^\ast, \bm{m}^\ast_t = \minimize_{\bm{\theta}, \bm{s}} \mathcal{L}_t (\bm{\theta} \odot \bm{m}_t; \mathcal{D}_t).
    \label{eq:softnet_loss}
\end{equation}
where $\bm{m}_t$ is obtained by applying an indicator function $\mathbbm{1}_c$ on weight scores $\bm{s}$. Note $\mathbbm{1}_c(s)=1$ if $s$ belongs to top-c$\%$ scores and $0$ otherwise.

% how to identify the soft subnetwork
In the optimization process for FSCIL, however, we consider two main problems: (1) Catastrophic forgetting: updating all $\bm{\theta} \odot \bm{m}_{t-1}$ when training for new sessions will cause interference with the weights allocated for previous tasks; thus, we need to freeze all previously learned parameters $\bm{\theta} \odot \bm{m}_{t-1}$; (2) Overfitting: the subnetwork also encounters overfitting issues when training an incremental task on a few samples, as such, we need to update a few parameters irrelevant to previous task knowledge., i.e., $\bm{\theta} \odot (\bm{1}-\bm{m}_{t-1})$.  

% realization of soft-subnetwork
To acquire the optimal subnetworks that alleviate the two issues, we define a soft-subnetwork by dividing the dense neural network into two parts-one is the major subnetwork $\bm{m}_\text{major}$, and another is the minor subnetwork $\bm{m}_\text{minor}$. The defined Soft-SubNetwork (\textcolor{cyan}{SoftNet}) follows as:
\begin{equation}
    \bm{m}_\text{soft} = \bm{m}_\text{major} \oplus \bm{m}_\text{minor},
    \label{eq:soft_mask}
\end{equation}
where $\bm{m}_\text{major}$ is a binary mask and $\bm{m}_\text{minor} \sim U(0,1)$ and $\oplus$ represents an element-wise summation. As such, a soft-mask is given as $\bs{m}_t^\ast \in [0,1]^{|\bm{\theta}|}$ in Eq.\ref{eq:softnet_loss}. In the all-experimental FSCIL setting, $\bm{m}_\text{major}$ maintains the base task knowledge $t=1$ while $\bm{m}_\text{minor}$ acquires the novel task knowledge $t \geq 2$. Then, with base session learning rate $\alpha,$ the $\bsy\theta$ is updated as follows: $\bm{\theta} \leftarrow \bm{\theta} - \alpha \left(\frac{\partial \mathcal{L}}{\partial \bm{\theta}} \odot \bm{m}_\text{soft}\right)$ effectively regularize the weights of the subnetworks for incremental learning. The subnetworks are obtained by the indicator function that always has a gradient value of $\bm{0}$; therefore, updating the weight scores $\bm{s}$ with its loss gradient is impossible. We update the weight scores by using Straight-through Estimator \cite{Hinton2012, Bengio2013, Ramanujan2020} in the backward pass. Specifically, we ignore the derivatives of the indicator function and update the weight score $\bm{s} \leftarrow \bm{s} - \alpha \left(\frac{\partial \mathcal{L}}{\partial \bm{s}} \odot \bm{m}_\text{soft} \right)$, where $\bm{m}_{\text{soft}}=\bm{1}$ for exploring the optimal subnetwork for base session training. Our Soft-subnetwork optimizing procedure is summarized in Algorithm \ref{alg:algorithm2}. Once a single soft-subnetwork $\bs{m}_{\text{soft}}$ is obtained in the base session, then we use the soft-subnetwork for the entire new sessions without updating.

\input{soft-materials/5_softnet/4_algorithm}

\subsection{Soft-SubNetwork for Incremental Learning}
We now describe the overall procedure of our soft-pruning-based incremental learning/inference method under the following FSCIL settings. This includes the training phase with a normalized informative measurement, as outlined in prior work~\cite{shi2021overcoming}, and the inference phase.

\noindent 
\textbf{Few-shot Class Incremental Learning} (FSCIL) aims to learn new sessions with only a few examples continually. A FSCIL model learns a sequence of $T$ training sessions $\{\mathcal{D}^1,\cdots, \mathcal{D}^T\}$, where $\mathcal{D}^t=\{z_i^t = (\bm{x}_i^t, y_i^t)\}^{n_t}_i$ is the training data of session $t$ and $\bm{x}_i^t$ is an example of class $y_i^t \in \mathcal{O}^t$. In FSCIL, the base session $\mathcal{D}^1$ usually contains a large number of classes with sufficient training data for each class. In contrast, the subsequent sessions ($t \geq 2$) will only contain a small number of classes with a few training samples per class, e.g., the $t^{\mathrm{th}}$ session $\mathcal{D}^t$ is often presented as a \textit{N}-way \textit{K}-shot task. In each training session $t$, the model can access only the training data $\mathcal{D}^t$ and a few examples stored in the previous session. When the training of session $t$ is completed, we evaluate the model on test samples from all classes $\mathcal{O} = \bigcup_{i=1}^t \mathcal{O}^i$, where $\mathcal{O}^i \bigcap \mathcal{O}^{j\neq i} = \emptyset$ for $\forall i, j \leq T$.

\noindent 
\textbf{Base Training} $(t = 1)$. In the base learning session, we optimize the soft-subnetwork parameter $\bm\theta$ (including a fully-connected layer as a classifier) and weight score $\bm s$ with cross-entropy loss jointly using the training examples of $\mathcal{D}^1$. 

\noindent 
\textbf{Incremental Training} $(t \geq 2)$. In the incremental few-shot learning sessions $(t \geq 2)$, leveraged by $\bm{\theta} \odot \bm{m}_{\text{soft}}$, we fine-tune few minor parameters $\bm{\theta} \odot \bm{m}_{\text{minor}}$ of the soft-subnetwork to learn new classes. Since $\bm{m}_{\text{minor}} < \bm{1}$, the soft-subnetwork alleviates the overfitting of a few samples. Furthermore, instead of Euclidean distance \cite{shi2021overcoming}, we employ a metric-based classification algorithm with cosine distance to finetune the few selected parameters. In some cases, Euclidean distance fails to give the real distances between representations, especially when two points with the same distance from prototypes do not fall in the same class. In contrast, representations with a low cosine distance are located in the same direction from the origin, providing a normalized informative measurement. We define the loss function as: 
\begin{equation}
\begin{split}
    & \mathcal{L}_m (z; \bm{\theta} \odot \bm{m}_{soft}) =\\
    & -\sum_{z \in \mathcal{D}} \sum_{o \in \mathcal{O}} \mathbbm{1}(y=o) \log \left( \frac{e^{-d(\bm{p}_o, f(\bm{x};\; \bm{\theta} \odot \bm{m}_{soft}))}}{\sum_{o_k \in \mathcal{O}} e^{-d(\bm{p}_{o_k}, f(\bm{x};\; \bm{\theta} \odot \bm{m}_{soft}))}} \right)
\end{split}
    \label{eq:proto_loss}
\end{equation}
where $d\left(\cdot, \cdot\right)$ denotes cosine distance, $\bm{p}_o$ is the prototype of class $o$, $\mathcal{O} = \bigcup_{i=1}^t \mathcal{O}^i$ refers to all encountered classes, and $\mathcal{D} = \mathcal{D}^t \bigcup \mathcal{P}$ denotes the union of the current training data $\mathcal{D}^t$ and the exemplar set $\mathcal{P} = \left\{\bm{p}_2 \cdots, \bm{p}_{t-1}\right\}$, where $\mathcal{P}_{t_e} \left(2 \leq t_e < t\right)$ is the set of saved exemplars in session $t_e$. Note that the prototypes of new classes are computed by $\bm{p}_o = \frac{1}{N_o} \sum_i \mathbbm{1}(y_i = o) f(\bm{x}_i; \bm{\theta} \odot \bm{m}_{soft})$ and those of base classes are saved in the base session, and $N_o$ denotes the number of the training images of class $o$. We also save the prototypes of all classes in $\mathcal{O}^t$ for later evaluation.  \\

\noindent 
\textbf{Inference for Incremental Soft-Subnetwork.} In each session, the inference is also conducted by a simple nearest class mean (NCM) classification algorithm \cite{mensink2013distance, shi2021overcoming} for fair comparisons. Specifically, all the training and test samples are mapped to the embedding space of the feature extractor $f$, and Euclidean distance $d_u(\cdot, \cdot)$ is used to measure the similarity between them. The classifier gives the $k$th prototype index $o_k^{\ast} = \arg\min_{o \in \mathcal{O}} d_u(f(\bm{x}; \bm{\theta} \odot \bm{m}_{soft}), \bm{p}_o)$ as output.
