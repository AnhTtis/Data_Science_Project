\section{Conclusion}
Inspired by \emph{Regularized Lottery Ticket Hypothesis (RLTH)}, which states that competitive smooth (non-binary) subnetworks exist within a dense network in continual learning tasks, we investigated the performances of the proposed two architecture-based continual learning methods referred to as Winning SubNetworks (WSN) which sequentially learns and selects an optimal binary-subnetwork (WSN) and an optimal non-binary Soft-Subnetwork (SoftNet) for each task, respectively. Specifically, WSN and SoftNet jointly learned the regularized model weights and task-adaptive non-binary masks of subnetworks associated with each task whilst attempting to select a small set of weights to be activated (winning ticket) by reusing weights of the prior subnetworks. The proposed WSN and SoftNet were inherently immune to catastrophic forgetting as each selected subnetwork model does not infringe upon other subnetworks in Task Incremental Learning (TIL). In TIL, binary masks spawned per winning ticket were encoded into one N-bit binary digit mask, then compressed using Huffman coding for a sub-linear increase in network capacity to the number of tasks. Surprisingly, we observed that in the inference step, SoftNet generated by injecting small noises to the backgrounds of acquired WSN (holding the foregrounds of WSN) provides excellent forward transfer power for future tasks in TIL. Softnet showed its effectiveness over WSN in regularizing parameters to tackle the overfitting, to a few examples in Few-shot Class Incremental Learning (FSCIL). 