%\IEEEraisesectionheading{\section{Experimental Details}\label{app_sec:exper_detail}}
\section{Experimental Details of WSN for TIL}\label{app_sec:exper_detail}
%We now validate our method on several benchmark datasets against relevant continual learning baselines. We followed similar experimental setups described in \cite{Saha2021} for baseline comparisons and explained in \cite{deng2021flattening} for SOTA comparisons, including the dataset splits, preprocessing, and training budget.

We followed similar experimental setups (architectures and hyper-parameters) described in \cite{Saha2021} for baseline comparisons and explained in \cite{deng2021flattening} for SOTA comparisons.


%\subsection{Datasets for Baseline and SOTA}
%The following datasets are used in baseline comparisons:\\

%\noindent 
%\textbf{1-1) Permuted MNIST (PMNIST)} is a variant of MNIST \cite{LeCun1998} where each task has a deterministic permutation applied to the input image pixels. The ground truth labels remain the same. \textbf{1-2) $5$-Datasets} \cite{Saha2021} is a mixture of 5 different vision datasets (CIFAR10 \cite{Krizhevsky2009}, MNIST\cite{LeCun1998}, SVHN \cite{Netzer2011SVHN}, FashionMNIST \cite{Xiao2017Fashion}, and notMNIST \cite{Yaroslav2011notMNIST}). We pad 0 values to raw images of MNIST and FashionMNIST and convert them to RGB format for a dimension of $3\times32\times32$. Afterward, we normalize the raw image data as in \cite{Serra2018}. \textbf{1-3) Omniglot Rotation} has split 1200 classes into 100 tasks with 12 classes for each task. We further preprocess the raw images by generating their rotated version in $(90 \degree, 180\degree, 270\degree)$ as in \cite{YoonJ2018iclr}. We use the Omniglot dataset to test the scalability of our method for a large number of tasks. 

%\noindent 
%In performing the SOTA comparison, we are employing the following datasets:\\

%\noindent 
%\textbf{2-1) CIFAR-100 Split} \cite{Krizhevsky2009} is constructed by randomly dividing 100 classes of CIFAR-100 into 10 tasks with 10 classes per task. \textbf{2-2) CIFAR-100 Superclass} \cite{YoonJ2018iclr} is divided into 20 tasks according to the 20 superclasses of the CIFAR-100 dataset, and each superclass contains 5 different but semantically related classes. \textbf{2-3) TinyImageNet} \cite{Stanford} is constructed by splitting 200 classes into 40 5-way classification tasks without data augmentation.

% Subsections for outlining details of our architecture
\subsection{Architecture Details}
All the networks for our experiments are implemented in a multi-head setting. 
%They also utilize ReLU in the hidden units and softmax with cross-entropy loss in the final layer.
% Insert Architecture used for PMNIST experiments here!
\noindent 
\textbf{Two-layered MLP:} In conducting the PMNIST experiments, we are following the exact setup as denoted by \cite{Saha2021} fully-connected network with two hidden layers of 100~\cite{Lopez-Paz2017}.

% Insert Architecture used for 5-Dataset experiments here!
\noindent
\textbf{Reduced ResNet18:} In conducting the 5-Dataset experiments, we use a smaller version of ResNet18 with three times fewer feature maps across all layers as denoted by \cite{Lopez-Paz2017}.

% Insert Architecture used for Omniglot Rotation experiments here!
% Architecture for Omniglot Rotation Experiments: For
\noindent
\textbf{Modified LeNet:} In conducting the Omniglot Rotation and CIFAR-100 Superclass experiments, we use a large variant of LeNet as the base network with 64-128-2500-1500 neurons based on \cite{Yoon2020}.

\noindent
\textbf{Modified AlexNet:} In conducting the split CIFAR-100 dataset, we use a modified version of AlexNet similar to \cite{Serra2018, Saha2021}.

\noindent
\textbf{4 Conv layers and 3 Fully connected layers:} For TinyImageNet, we use the same network architecture as \cite{gupta2020maml, deng2021flattening}.



% Subsections for outlining details for training our baselines and model
%\subsection{Training Details}
% To be filled in
%We use the exact setup and dataset splits described in \cite{Saha2021} to train all the models within the baseline comparison. In conducting experiments with the PMNIST dataset, we keep 10\% of the training data from each task for validation. On the other datasets, however, we keep only 5\% of training data from each task for validation. We train all models within the baseline experiments using stochastic gradient descent (SGD). For each task in PMNIST, we train the network for 5 epochs with a batch size of 10. In 5-Dataset and Omniglot Rotation experiments, we train each task for 100 epochs with the early termination strategy based on the validation loss proposed in \cite{Serra2018}. For experiments in both datasets, we fix the batch size to 64. We run experiments under five different seed values for all experiments.

% To be filled in
%For conducting experiments for GPM \cite{Saha2021} in the Omniglot Rotation dataset, we use the threshold hyperparameter $\epsilon_{th} = 0.98$ for all the layers and increasing the value of $\epsilon_{th}$ by 0.0002 for each incoming new tasks.

% To be filled in
%For PackNet \cite{mallya2018packnet} and WSN, sparsity constraint $c$ is applied across every layer within these methods except the last layer, where the multi-head classifier lies. Since PackNet \cite{mallya2018packnet} includes layer-wise capacity $c$ for parameters that will be isolated for an incoming new set of tasks, we fix $c = 1/\tau$ while performing PMNIST and 5-Dataset experiments. Here, $c$ denotes the layer-wise capacity, and $\tau$ indicates the number of incoming tasks. For Omniglot experiments, we are fixing $c = 2/\tau$ as it turns out to be the setup that produces the most optimum accuracy-capacity tradeoff. Since PackNet \cite{mallya2018packnet} involves fine-tuning toward isolated network parameters for a given task to recover the accuracy, we allow the network to fine-tune up to half of the number of epochs required to finish the preceding training process.

%As denoted in \Cref{tab:main_table_new} and \Cref{tab:main_sota_table}, we are performing experiments on WSN using different layer-wise capacity $c$ ranging from 0.03 to 0.5 to analyze the behavior of WSN. WSN decides whether to use the remaining free parameters within the architecture or reuse the weights in the past tasks upon dealing with the incoming tasks.

\subsection{List of Hyperparameters}

\input{materials/6_app_hyper_params}

\noindent 
\Cref{tab:main_hyper_table} details the hyperparameter setup for the baselines and our approach.
$n_s$ in GPM denotes the number of random training examples sampled from the replay buffer to construct the representation matrix for each architecture layer. 

%\section{Additional Results on WSN for TIL}\label{app_sec:add_results}
%\subsection{Additional Analysis on Baseline Comparisons.}
%\input{materials/6_app_plots_acc_vs_cap}
%\Cref{fig:acc_vs_cap_appendix} shows the detailed accuracy over total capacity usage for the baselines and our approach on the PMNIST and Omniglot Rotation datasets with PackNet removed due to its substantial total capacity.

%Employing WSN with the initial layer-wise capacity of $c = 0.3$ within the PMNIST dataset returns the best-performing model regarding accuracy-capacity tradeoff. 

%\subsection{Forget-Free Performances and Capacities.} \label{app_sec:add_results_perform}
%\input{materials/5_main_plots_tinyimgnet}
%We prepare performances and bit-map compressed capacities on the TinyImageNet Dataset as shown in \Cref{fig:app_main_tinyimg_plots}. The $c=0.1$ shows better performances over others. With fixed $c=0.1$, the bit-wise Huffman compression rate shows positive as the number of tasks increases. The most interesting part is that as the number of bits to compress increases, the average compression rate gets higher as the increased amount of reused weights might affect the compression rate (nodes with a small probability might be rare). We investigated how the compression rate is related to the total model capacity. The more bits the binary mask compression does, the less the model capacity to save is required. We validated that within 40 tasks, N-bit Huffman compressed capacities are sub-linearly increasing while binary map capacities increase linearly. The 7-bit Huffman encoding is enough to compress binary maps without exceeding the model capacity even though the compression rate $\alpha$ depends on compression methods typically.
 
%\subsection{Comparisons with the SOTA}
%\input{materials/5_main_table_sota_full}
%The WSN's performances are compared with others w.r.t three measurements on three major benchmark datasets as shown in \Cref{tab:main_sota_full_table}.

% 브레이킹 : 웨이트 스코어를 늘려서 해결하믄 방법 (장점)
%\subsection{Comparisions of SupSup and WSN}\label{app_sec:add_supsup_wsn}
%\textbf{Capability of WSN on PMNIST-250:} We conduct experiments of WSN $c=0.5, 0.7, 0.8$ (LeNet 300-100, s=$100$) with \textcolor{malachite}{a single weight score} on 250 tasks PMNIST~\cite{wortsman2020supermasks} as shown in \Cref{fig:supsup_wsn_pmnis250}. WSN performance with $c=0.7$, initially at $97.3\%$, gradually drops after the 90th task, and it performed better than SupSup ($96.4\%$) until then. When we divide PMNIST-250 into 90 tasks and use three weight scores, WSN performs better accuracy and memory efficiency than SupSup (requiring 250 weight scores).

%\input{materials/5_main_plot_supsup_wsn}

%\subsection{Capacities of Re-used Weights.}
%\input{materials/5_main_plots_capacity}

%There exist two kinds of reused weight in a total model capacity: per task-dependent weights and all task-dependent ones. To interpret the proportion of the two kinds of reused weights, we prepare the progressive capacities of layer-wise reused weights as shown in  \Cref{fig:main_tinyimg_all_capa_plots}. In \Cref{fig:main_tinyimg_all_capa_plots} (a), the $c$ value determines the model capacity of the initial task, entire tasks, and the proportions of reused weights per task, and for all tasks. From \Cref{fig:main_tinyimg_all_capa_plots} (b), the capacity of Conv4 is greater than that of Conv1, while the Conv4 proportion of reused weights for all tasks is smaller than Conv1. From the results, we conclude that higher progressive capacity varieties result in fewer reused weights for all tasks. In \Cref{fig:main_tinyimg_all_capa_plots} (c), the Conv4 has the properties such as the most dynamic change of progressive capacity and the least progressive capacity of reused weights for all tasks. From \Cref{fig:main_tinyimg_all_capa_plots} (d), within Linear1 and Linear2, except for the early task learning stage, the progressive capacities show the same tendency. The WSN did not show performance drops as the progressive capacity of reused weights for all tasks decreases, as shown in  \Cref{fig:app_main_tinyimg_plots} (a).

%\subsection{Sparse Binary Maps}\label{app_sec:add_results_binary}
%We prepared binary map correlation results on several benchmark datasets. \Cref{fig:main_pmnist_conf} showed the progressive reused weights tendency. However, it tends to reuse weights least since PMNIST has an independent task. Also, \Cref{fig:main_cifar100_conf} showed the same tendency as in PMNIST results. \Cref{fig:main_5_data_conf} shows the most discrete binary map since all tasks consist of 5 individual datasets. \Cref{fig:main_cifar100sc_conf}, \Cref{fig:main_tinyimg_conf}, and \Cref{fig:main_omniglot_conf} showed a similar sparse binary map correlation according to the $c$ value. From these observations, we could conclude that the sparsity of reused binary maps leads to better performances than others.

% Reused Weight Performances on CIFAR100-Split
%\subsection{Additional Analysis}
%We reported the progressive ratio of weights used so far per layer as the number of tasks increases in \Cref{fig:main_tinyimg_all_capa_plots}. Here, we interpreted how reused weights affect the performances on CIFAR-100 Split dataset as shown in \Cref{fig:reused_w_accuracy}. The $c=0.5$ represents the average accuracy of reused weights per task; the most significant weights were the subset of all prior weights since model forgetting occurs from the performances without being reused per task. The weights being reused for all tasks don't seem to always positively affect forget-free in that the importance of the weights decreases as the number of tasks increases. Our WSN reuse weights if the knowledge from previous tasks is already enough to solve the task at hand and employs a few new weights otherwise. Specifically, in task 5, all used weights seem enough to infer all CIFAR-100 Split tasks since the weights reused per task catch up with the task performance.

%\input{materials/5_acc_reused_per_tasks}

% WSN's cosine similarity
\input{materials/7_main_confusion_pmnist}
%\input{materials/5_main_confusion_cifar100_10}
\input{materials/7_main_confusion_5data}
%\input{materials/5_main_confusion_cifar100_100}
%\input{materials/5_main_confusion_tinyimagenet}
%\input{materials/7_main_confusion_omniglot}

% WSN / SoftNet's Forward Transfer
%\input{materials/5_main_conf_cifar100_sc}
%\input{materials/5_main_conf_TinyImageNet}
\input{materials/5_main_conf_pmnist}
\input{materials/5_main_conf_fiveData}
%\input{materials/5_main_conf_omniglot}

\section{Experimental Details of SoftNet for FSCIL}\label{app_sec:exp_detail}
%We validate the effectiveness of the soft-subnetwork in our method on several benchmark datasets against various architecture-based methods for Few-Shot Class Incremental Learning (FSCIL). 



%To proceed with the details of our experiments, we first explain the datasets and how we involve them in our experiments. Later, we detail experiment setups, including architecture details, preprocessing, and training budget. 

\subsection{Datasets} The following datasets are utilized for comparisons with current state-of-the-art:

\noindent 
\textbf{CIFAR-100} In CIFAR-100, each class contains $500$ images for training and $100$ images for testing. Each image has a size of $32\times 32$. Here, we follow an identical FSCIL procedure as in \cite{shi2021overcoming}, dividing the dataset into a base session with 60 base classes and eight novel sessions with a 5-way 5-shot problem on each session.

\noindent
\textbf{miniImageNet} miniImageNet consists of RGB images from 100 different classes, where each class contains $500$ training images and $100$ test images of size $84 \times 84$. Initially proposed for few-shot learning problems, miniImageNet is part of a much larger ImageNet dataset. Compared with CIFAR-100, the miniImageNet dataset is more complex and suitable for prototyping. The setup of miniImageNet is similar to that of CIFAR-100. To proceed with our evaluation, we follow the procedure described in \cite{shi2021overcoming}, incorporating 60 base classes and eight novel sessions through 5-way 5-shot problems. 

% note: the Topic split of (about) 6,000 training images and 6,000 test images
\noindent
\textbf{CUB-200-2011} CUB-200-2011 contains $200$ fine-grained bird species with $11,788$ images with varying images for each class. To proceed with experiments, we split the dataset into $6,000$ training images, and $6,000$ test images as in \cite{tao2020few}. During training, We randomly crop each image to size $224 \times 224$. We fix the first 100 classes as base classes, utilizing all samples in these respective classes to train the model. On the other hand, we treat the remaining 100 classes as novel categories split into ten novel sessions with a 10-way 5-shot problem in each session.

%\subsection{Experiment Setups} We begin this section by describing the setups used for experiments in CIFAR-100 and miniImageNet. After that, we proceed with a follow-up discussion on the configuration we employ for experiments involving the CUB-200-2011 dataset.\\

%\noindent
%\textbf{CIFAR-100 and miniImageNet.} For experiments in these two datasets, we are using NVIDIA GPU RTX8000 on CUDA 11.0. We randomly split these two datasets into multiple sessions, as described in the previous sub-section. We run each algorithm ten times for experiments on both datasets with a fixed split and report their mean accuracy. We adopt ResNet18~\cite{he2016deep} as the backbone network. For data augmentation, we use standard random crop and horizontal flips. During the training stage in the base session, we select top-$c\%$ weights at each layer and acquire the optimal soft-subnetworks with the best validation accuracy. For each incremental few-shot learning session, we train our model for six epochs with a learning rate is $0.02$. We train new class session samples using a few minor weights of the soft-subnetwork (conv4x layer of ResNet18 and conv3x layer of ResNet20) obtained by learning at the base session.

%\noindent
%\textbf{CUB-200-2011.} Besides experiments in the previous two datasets, we conducted an additional experiment on this dataset. We prepare this dataset following the split procedure described in the previous sub-section. We run each algorithm ten times and report their mean accuracy. We also adopt ResNet18~\cite{he2016deep} as the backbone network and follow the same data augmentation as in the previous two datasets. We follow the same base-session training procedure as in the other two datasets. In each incremental few-shot learning session $t>1$, the total number of training epochs is $10$, and the learning rate is $0.1$. We train new class session samples using a few minor weights of the soft-subnetwork (conv4x layer of ResNet18) obtained at the base session.


%\subsection{Layer-wise Accuracy}
%In incremental few-shot learning sessions, we train new class session samples using a few minor weights $\bm{m}_{\text{minor}}$ of the specific layer. At the same, we entirely fix the remaining weights to investigate the best performances as shown in \Cref{tab:miniImageNet_5way_5shot_layerwise}. The best performances involve fine-tuning at the Conv5x layer with $c=97 \%$. It means features computed by the lower layer are general and reusable in different classes. On the other hand, features from the higher layer are specific and highly dependent on the dataset.
%\input{soft-materials/6_exper/5_main_miniImageNet_5way_5shot_layer}

%\section{Additional Results on SoftNet for FSCIL}\label{app_sec:add_results}



\subsection{Comparisons with SOTA}
We compare SoftNet with the following state-of-art-methods on TOPIC class split~\cite{tao2020few} of three benchmark datasets - CIFAR100 (\Cref{tab:main_cifar100_5way_5shot_resnet18_split}), miniImageNet (\Cref{tab:miniImageNet_5way_5shot_baseline_split}), and CUB-200-2011 (\Cref{tab:main_cub200_10way_5shot}).

\input{soft-materials/6_exper/6_main_cifar100_resnet18_5way_5shot_split}

\input{soft-materials/6_exper/6_main_miniImageNet_5way_5shot_split}

\input{soft-materials/6_exper/5_main_cub_10way_5shot}

% Current work in Summery.
%\begin{itemize}[\leftmargin=*]
%    \item \textbf{CEC}~\cite{zhang2021few}: The authors proposed a Continually Evolved Classifier (CEC) that employs a graph model to propagate context information between classifiers for adaptation.
    
%    \item \textbf{LIMIT}~\cite{zhou2022few}: The authors proposed a new paradigm for FSCIL based on meta-learning by LearnIng Multi-phase Incremental Tasks (LIMIT), which synthesizes fake FSCIL tasks from the base dataset. Besides, LIMIT also constructs a calibration module based on a transformer, which calibrates the old class classifiers and new class prototypes into the same scale and fills in the semantic gap.
    
%    \item \textbf{MetaFSCIL}~\cite{chi2022metafscil}: The authors proposed a bilevel optimization based on meta-learning to optimize the network to learn how to learn incrementally in the setting of FSCIL. Concretely, They proposed to sample sequences of incremental tasks from base classes for training to simulate the evaluation protocol. For each task, the model is learned using a meta-objective to perform fast adaptation without forgetting. Furthermore, they proposed a bi-directional guided modulation to modulate activations and reduce catastrophic forgetting. 
    
%    \item \textbf{C-FSCIL}~\cite{hersche2022constrained}: The authors proposed C-FSCIL, which is architecturally composed of a frozen meta-learned feature extractor, a trainable fixed-size fully connected layer, and a rewritable dynamically growing memory that stores as many vectors as the number of encountered classes. 
    
%    \item \textbf{Subspace Reg.}~\cite{akyurek2021subspace}: The authors presented a straightforward approach that uses logistic regression classifiers for few-shot incremental learning. The key to this approach is a new family of subspace regularization schemes that encourage weight vectors for new classes to lie close to the subspace spanned by the weights of existing classes.
    
 %  \item \textbf{Entropy-Reg}~\cite{liu2022few}: The authors alternatively proposed using data-free replay to synthesize data by a generator without accessing real data.
    
 %   \item \textbf{ALICE}~\cite{peng2022few}: The authors proposed a method - Augmented Angular Loss Incremental Classification or ALICE - inspired by the similarity of the goals for FSCIL and modern face recognition systems. Instead of the commonly used cross-entropy loss, they proposed using the angular penalty loss to obtain well-clustered features in ALICE.

%\end{itemize}

%Leveraged by regularized backbone ResNet, SoftNet outperformed all existing current works on CIFAR100 as shown in ~\Cref{tab:main_cifar100_5way_5shot_resnet18_split}. On miniImageNet ~\Cref{tab:miniImageNet_5way_5shot_baseline_split} and CUB-200-201 ~\Cref{tab:main_cub200_10way_5shot}, the performances of SoftNet were comparable with those of ALICE and LIMIT, considering that ALICE used class/data augmentations and LIMIT added an extra multi-head attention layer. \\

%\noindent
%\textbf{Comparisions of SoftNet and AANet}. Our SoftNet and AANet~\cite{liu2021adaptive} have proposed alleviating catastrophic forgetting in FSCIL and CIL, respectively. AANet consists of multi-ResNets: one residual block learns new knowledge while another fine-tunes to maintain the previously learned knowledge. Through the learnable scaling parameter for the linear combination of the multi-ResNet features, AANet showed outstanding performances in the CSIL setting. However, AANet tends to overfit since the ResNet block’s parameters are fully used to update a few new class data in FSCIL. This point makes it difficult to train AANet on a few samples even though the performance at session 1 is comparable with SoftNet as shown in \Cref{tab:main_cifar100_5way_5shot_resnet18_topic_aanet}.

%\input{soft-materials/6_exper/7_main_cifar100_topic_aanet}



