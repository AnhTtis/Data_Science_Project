%\IEEEraisesectionheading{\section{Related Works}}
\section{Related Works}

% continual learning
% --------------------------------------
% - Regularization-based CL
% - Rehearsal-based CL
% - Architecture-based CL
%   (+) avoid forgetting
%   (-) requires task-adaptive parameters
\noindent
\textbf{Continual Learning.} Continual learning~\cite{McCloskey1989, ThrunS1995, KumarA2012icml, LiZ2016eccv}, also known as lifelong learning, is the challenge of learning a sequence of tasks continuously while utilizing and preserving previously learned knowledge to improve performance on new tasks. Several major approaches have been proposed to tackle the challenges of continual learning, such as catastrophic forgetting. One such approach is \textit{regularization-based methods}~\cite{Kirkpatrick2017,chaudhry2020continual, Jung2020, titsias2019functional, mirzadeh2020linear}, which aim to reduce catastrophic forgetting by imposing regularization constraints that inhibit changes to the weights or nodes associated with past tasks. \textit{Rehearsal-based approaches}~\cite{rebuffi2017icarl, chaudhry2018efficient, chaudhry2019continual, Saha2021, deng2021flattening} store small data summaries to the past tasks and replay them during training to retain the acquired knowledge. Some methods in this line of work~\cite {ShinH2017nips, aljundi2019online} accommodate the generative model to construct the pseudo-rehearsals for previous tasks. \textit{Architecture-based approaches}~\cite{mallya2018piggyback, Serra2018, li2019learn, wortsman2020supermasks, kang2022forget, kang2022soft} use the additional capacity to expand~\cite{xu2018reinforced, YoonJ2018iclr} or isolate~\cite{rusu2016progressive} model parameters, preserving learned knowledge and preventing forgetting. Both rehearsal and architecture-based methods have shown remarkable efficacy in suppressing catastrophic forgetting but require additional capacity for the task-adaptive parameters~\cite{wortsman2020supermasks} or the replay buffers. \\


% Pruning-based approaches of Architecture based CL
% ---------------------------------------------------------------------------
% (1) increase performances by adopting additional memory (external capacity)
% (2) build memory and computationally efficient continual learners using pruning-based constraints
% ---------------------------------------------------------------------------
%  CLNP selects important neurons
%  Piggyback trains task-specific binary masks
%  HAT proposes task-specific learnable attention vectors
%  LL-Tickets show a sparse network
% WSN jointly learns the model and task-adaptive binary masks
\noindent
\textbf{Pruning-based Continual Learning.} While most works aim to increase the performance of continual learners by adding memory, some researchers have focused on building memory and computationally efficient continual learners by using pruning-based constraints. CLNP \cite{golkar2019continual} is one example of a method that selects important neurons for a given task using $\ell_1$ regularization to induce sparsity and freezes them to maintain performance. Neurons that are not selected are reinitialized for future task training. Another method, Piggyback \cite{mallya2018piggyback}, trains task-specific binary masks on the weights given a pre-trained model. However, this method does not allow for knowledge transfer among tasks, and its performance highly depends on the quality of the backbone model. HAT~\cite{Serra2018} proposes task-specific learnable attention vectors to identify important weights per task. The masks are formulated to layerwise cumulative attention vectors during continual learning. A recent method, LL-Tickets~\cite{Chen2021lifelonglottery}, shows a sparse subnetwork called lifelong tickets that performs well on all tasks during continual learning. The method searches for more prominent tickets from current ones if the obtained tickets cannot sufficiently learn the new task while maintaining performance on past tasks. However, LL-Tickets require external data to maximize knowledge distillation with learned models for prior tasks, and the ticket expansion process involves retraining and pruning steps. WSN~\cite{kang2022forget} is another method that jointly learns the model and task-adaptive binary masks on subnetworks associated with each task. It attempts to select a small set of weights activated (winning ticket) by reusing weights of the prior subnetworks. \\



% Soft-subnetwork
% ---------------------------------------------------------------------------
% works with context-dependent gating of sub-spaces, parameters ,or layers addresses catastrophic forgetting
% (1) howto preventing significant change in model weights such as SI and EWC.
% (2) flat minima == acquring sub-spaces
%    (+) robust to random perturbations
% (3) Soft-subnetworks: Regularizing subnetworks for generalization
\noindent 
\textbf{Soft-subnetwork.} 
Recent studies have shown that context-dependent gating of sub-spaces~\cite{he2018overcoming}, parameters~\cite{mallya2018packnet, he2019task, mazumder2021few}, or layers~\cite{serra2018overcoming} of a single deep neural network is effective in addressing catastrophic forgetting during continual learning. Moreover, combining context-dependent gating with constraints that prevent significant changes in model weights, such as SI~\cite{zenke2017continual} and EWC~\cite{Kirkpatrick2017}, can lead to further performance improvements, as shown by \textit{Masse et al.}~\cite{masse2018alleviating}. Flat minima, which can be seen as acquiring sub-spaces, have also been proposed to address catastrophic forgetting. Previous studies have demonstrated that a flat minimizer is more robust to random perturbations~\cite{hinton1993keeping, hochreiter1994simplifying, jiang2019fantastic}. In a recent study by \textit{Shi et al.}~\cite{shi2021overcoming}, obtaining flat loss minima in the base session, which refers to the first task session with a sufficient number of training instances, was found to be necessary to alleviate catastrophic forgetting in Few-Shot Class Incremental Learning (FSCIL). They achieved this by shifting the model weights on the obtained flat loss contour. Our work investigates the performance of two proposed architecture-based continual learning methods: WSN / Soft-Subnetworks (SoftNet). We select sub-networks~\cite{frankle2018lottery, Liu2018Hier, you2019drawing, zhou2019deconstructing, wortsman2019discovering, Ramanujan2020, kang2022forget, chijiwa2022metaticket} and optimize the regularized sub-network parameters in a sub-space~\cite{kang2022soft} in Task Incremental Learning (TIL) and Few-Shot Class Incremental Learning (FSCIL) settings.
