\begin{figure*}[ht]
    \centering
    \setlength{\tabcolsep}{-6pt}{%
    \begin{tabular}{cc}
    
    %\includegraphics[width=0.37\textwidth]{images/loss_lens/loss_func_surface_3D_epoch30.pdf} & 
    
    \includegraphics[width=0.49\textwidth]{images/loss_lens/loss_func_surface_3D_epoch70.pdf} &

    \includegraphics[width=0.49\textwidth]{images/loss_lens/loss_func_surface_3D_epoch100.pdf} \\ 
    
    % plot captions
    \small (a) epoch=70, $c=10.0\%$ & \small (b) epoch=100, $c=10.0\%$  \\
    \end{tabular}
    }
    \caption{\small \textbf{Loss landscapes of } \textcolor{teal}{DenseNet}, \textcolor{red}{HardNet}, \textbf{and} \textcolor{cyan}{SoftNet}: Subnetworks provide a more flat global minimum than dense neural networks. To demonstrate the loss landscapes, we trained a simple three-layered, fully connected model (fc-4-25-30-3) on the Iris Flower dataset (which is three classification problem) for 100 epochs.}
    
    \label{fig:main_plot_loss_lens}
\end{figure*}