\begin{algorithm}[ht]
  \caption{Soft-Subnetworks (SoftNet) for FSCIL.}\label{alg:algorithm2}
    \small
    \begin{algorithmic}[1]
    \INPUT $\{\mathcal{D}^t\}_{t=1}^{\mathcal{T}}$, model weights $\bm\theta$, and score weights $\bm{s}$, layer-wise capacity $c$ \\
    \STATE \textcolor{blue}{// Training over base classes $t = 1$} \\ 
    \STATE Randomly initialize $\bm\theta$ and $\bm{s}$. \\
    \FOR{epoch $e = 1, 2, \cdots$}
        %\STATE Set $\gamma = \delta$ \text{if $k > T_k$ else $1$} in Eq. \ref{eq:soft_mask}\\ 
        \STATE Obtain softmask $\bm{m}_\text{soft}$ of $\bm{m}_{major}$ and $\bm{m}_{minor} \sim U(0,1)$ at each layer \\
        \FOR{batch $\bm{b}_t\sim \mathcal{D}^t$} 
            \STATE Compute $\mathcal{L}_{base}\left( \bm\theta \odot \bm{m}_\text{soft};\bm{b}_t \right)$ by Eq.~\ref{eq:softnet_loss}
            
            \STATE $\bm\theta \leftarrow \bm\theta - \alpha \left(\frac{\partial \mathcal{L}}{\partial \bm\theta} \odot \bm{m}_\text{soft}\right)$ %\COMMENT{Soft weight update by Eq.~\ref{eq:param_update}}
            \STATE $\bm{s} \leftarrow \bm{s} - \alpha \left(\frac{\partial \mathcal{L}}{\partial \bm{s}} \odot \bm{m}_\text{soft}\right)$ %\COMMENT{Soft weight score update by Eq.~\ref{eq:score_update}}
        \ENDFOR
    \ENDFOR \\
    \STATE \textcolor{blue}{// Incremental learning $t \geq 2$} \\
    \STATE \text{Combine the training data $\mathcal{D}^t$} \\
    \STATE \text{~~~and the exemplars saved in previous few-shot sessions} \\
    %\STATE Obtain softmask $\bm{m}_\text{soft}(\gamma=\delta)$ of the smallest top-$(1-c)\%$ weights at each layer; remaining weights fixed.
    \FOR{epoch $e = 1, 2, \cdots$}
        \FOR{batch $\bm{b}_t\sim \mathcal{D}^t$} 
            \STATE Compute $\mathcal{L}_{m}\left( \bm\theta \odot \bm{m}_\text{soft};\bm{b}_t \right)$ by Eq.~\ref{eq:proto_loss} 
            
            \STATE $\bm{\theta} \leftarrow \bm{\theta} - \beta \left(\frac{\partial \mathcal{L}}{\partial \bm{\theta}} \odot \bm{m}_{\textcolor{red}{minor}} \right)$ %\COMMENT{Soft weight update by Eq.~\ref{eq:param_update}}
            %\STATE $\bm{s} \leftarrow \bm{s} - \beta \left(\frac{\partial \mathcal{L}}{\partial \bm{s}} \odot \bm{m}_\text{soft}\right)$ \COMMENT{Soft weight score update by Eq. \ref{eq:score_update}}
        \ENDFOR
    \ENDFOR \\
    \OUTPUT model parameters $\bm{\theta}$, $\bm{s}$, and $\bm{m}_{\text{soft}}$.
  \end{algorithmic}
\end{algorithm}