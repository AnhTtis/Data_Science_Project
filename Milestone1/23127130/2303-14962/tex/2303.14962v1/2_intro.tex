\section{Introduction}

\IEEEPARstart{C}{ontinual} Learning (CL), also known as Lifelong Learning \cite{ThrunS1995,rusu2016progressive,zenke2017continual, hassabis2017neuroscience}, is a learning paradigm where a series of tasks are learned sequentially. The principle objective of continual learning is to replicate human cognition, characterized by the ability to learn new concepts or skills incrementally throughout one's lifespan. An optimal continual learning system could facilitate a positive forward and backward transfer, leveraging the knowledge gained from previous tasks to solve new ones while also updating its understanding of previous tasks with the new knowledge. However, building successful continual learning systems is challenging due to the occurrence of \textit{catastrophic forgetting} or \textit{catastrophic interference}~\cite{McCloskey1989}, a phenomenon where the performance of the model on previous tasks significantly deteriorates when it learns new tasks. This can make it challenging to retain the knowledge acquired from previous tasks, ultimately leading to a decline in overall performance. To tackle the catastrophic forgetting problem in continual learning, numerous approaches have been proposed, which can be broadly classified as follows: (1) \textbf{Regularization-based methods}~\cite{Kirkpatrick2017, chaudhry2020continual, Jung2020, titsias2019functional, mirzadeh2020linear} aim to keep the learned information of past tasks during continual training aided by sophisticatedly designed regularization terms, (2) \textbf{Rehearsal-based methods}~\cite{rebuffi2017icarl, riemer2018learning, chaudhry2018efficient, chaudhry2019continual, Saha2021} utilize a set of real or synthesized data from the previous tasks and revisit them, and (3) \textbf{Architecture-based methods}~\cite{mallya2018piggyback, Serra2018, li2019learn, wortsman2020supermasks, kang2022forget, kang2022soft} propose to minimize the inter-task interference via newly designed architectural components. 

\input{materials/3_concept}

% Memory-efficient learning 
Despite the remarkable success of recent works on rehearsal- and architecture-based continual learning, a majority of recent methods request external memory as new tasks arrive, making the model difficult to scale to larger and more complex tasks. Rehearsal-based CL requires additional storage to store the replay buffer or generative models, and architecture-based methods leverage additional model capacity to account for new tasks. These trends lead to an essential question: how can we build a memory-efficient CL model that does not exceed the backbone network's capacity or even requires a much smaller capacity? Several studies have shown that deep neural networks are over-parameterized~\cite{Denil2013, Han2016learning_both_weights_struct, Li2016pruning_convnets} and thus removing redundant/unnecessary weights can achieve on-par or even better performance than the original dense network. More recently, Lottery Ticket Hypothesis (LTH)~\cite{frankle2018lottery} demonstrates the existence of sparse subnetworks, named \emph{winning tickets}, that preserve the performance of a dense network. However, searching for optimal winning tickets during continual learning with iterative pruning methods requires repetitive pruning and retraining for each arriving task, which could be more practical. Furthermore, leveraged by Regularized Lottery Ticket Hypothesis (RLTH)~\cite{kang2022soft}, subnetworks exemplified that it could overfit a few task data, potentially limiting their effectiveness on new tasks or datasets.

%Furthermore, inspired by Regularized Lottery Ticket Hypothesis (RLTH)~\cite{kang2022soft}, the regularized subnetworks showed the generalized performance in Few-shot Class Incremental Learning.

To tackle the issues of external replay buffer, capacity, and over-fitting, we suggest a novel regularized CL method which finds the high-performing \textit{Regularized Winning SubNetwork} referred to as Soft-Subnetwork (\textbf{SoftNet})~\cite{kang2022soft} given tasks without the need for retraining and rewinding. As a baseline of SoftNet, non-regularized Winning Subnetworks referred to as \textbf{WSN}~\cite{kang2022forget} are restated, as shown in \Cref{fig:concept_comparison} (d). Also, we set previous pruning-based CL approaches \cite{mallya2018piggyback, wortsman2020supermasks} (see \Cref{fig:concept_comparison} (a)) to baselines of architectures, which obtain task-specific subnetworks given a pre-trained backbone network. Our WSN incrementally learns model weights and task-adaptive binary masks (the subnetworks) within the neural network. To allow the forward transfer when a model learns on a new task, we reuse the learned subnetwork weights for the previous tasks, however selectively, as opposed to using all the weights \cite{mallya2018packnet} (see \Cref{fig:concept_comparison} (b)), that may lead to biased transfer. Further, the WSN eliminates the threat of catastrophic forgetting during continual learning by freezing the subnetwork weights for the previous tasks and does not suffer from the negative transfer, unlike \cite{YoonJ2018iclr} (see \Cref{fig:concept_comparison} (c)), whose subnetwork weights for the previous tasks can be updated when training on the new tasks.

% motivation of selective regularized-reused weights 
LTH often leverages the magnitudes of the weights as a pruning criterion to find the optimal subnetworks. However, in CL, relying only on the weight magnitude may be ineffective since the weights are shared across classes, and thus training on the new tasks will change the weights trained for previous tasks (reused weights). The update of reused weights will trigger an avalanche effect where weights selected to be part of the subnetworks for later tasks will always be better in the eyes of the learner, which will result in the catastrophic forgetting of the knowledge for the prior tasks. Thus, in CL, the learner must train on the new tasks without changing the reused weights. To find the optimal subnetworks, we decouple the information of the learning parameter and the network structure into two separate learnable parameters, namely, \emph{weights} and \emph{weight scores}. The weight scores are binary masks with the same shapes as the weights. Now, subnetworks are found by selecting the weights with the top-$k$ percent weight ranking scores. More importantly, decoupling the weights and the structure allows us to find the optimal subnetwork online without iterative retraining, pruning, and rewinding. There is one more thing to consider to find the optimal subnetworks induced by binary masks in CL. According to CL tasks, the subnetworks tend to overfit the few samples, i.e., Few-Shot Class Incremental Learning (FSCIL). Therefore, we could find regularized subnetworks yielded by smooth (soft) masks. To this end, the proposed methods are designed to jointly learn the weights and the structure of the optimal regularized subnetworks, whose overall size is smaller than a dense network.

\noindent
Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=*]
    \item Inspired by Regularized Lottery Ticket Hypothesis (RLTH), we propose novel forget-free continual learning methods referred to as WSN and SoftNet, which learn a compact subnetwork for each task while keeping the weights selected by the previous tasks intact. 
    
    \item Our proposed WSN and SoftNet do not perform explicit pruning for learning the subnetwork. Our methods eliminate catastrophic forgetting and enable forward transfer from previous tasks to new ones in Task Incremental Learning (TIL).  

    \item Our WSN obtains compact subnetworks using Huffman coding with a sub-linear increase in the network capacity, outperforming existing continual learning methods regarding accuracy-capacity trade-off and forward / backward transfer in TIL. 
    
    \item Our SoftNet trains two different types of subnetworks for solving the FSCIL problem, alleviating the continual learner from forgetting previous sessions and overfitting simultaneously, outperforming strong baselines on public benchmark tasks.
\end{itemize}