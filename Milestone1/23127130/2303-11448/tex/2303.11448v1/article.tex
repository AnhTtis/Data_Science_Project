\documentclass[amsmath,amssymb,aps,floatfix,nofootinbib]{revtex4-2}

\usepackage{physics}
\usepackage{bm}
\usepackage{bbold}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{xargs}
\usepackage{url}
\usepackage{comment}

% custom commands
\newcommand{\SU}{\mathrm{SU}}
\newcommand{\su}{\mathfrak{su}}
\newcommand{\dg}{\dagger}
\newcommand{\p}{\partial}
\newcommand{\one}{\mathbb{1}}
\newcommand{\Om}[0]{{(\Omega)}}
\newcommand{\der}[2]{\frac{\mathrm{d} #1 }{\mathrm{d} #2 }} % derivative
\newcommand{\diff}{\mathrm{d}}
\newcommand{\conv}{\ast}
\newcommand{\mx}{\mathbf x}
\newcommand{\my}{\mathbf y}
\newcommand{\mz}{\mathbf z}
\newcommand{\lat}{\mathbb Z^D}
\newcommand{\TT}{\mathbb T}
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\nobrhyph}[2]{#1\mbox{-}\nobreak\hspace{0pt}#2}
\newcommand{\G}[1]{\nobrhyph{$G$}{#1}}
\newcommand{\gequivariance}{\G{equivariance}}
\newcommand{\gequivariant}{\G{equivariant}}
\newcommand{\gconv}{\G{convolution}}
\newcommand{\gconvabb}{\G{Conv}}
\newcommand{\zconv}{\nobrhyph{$\lat$}{convolution}}
\DeclareMathOperator {\argmax}{argmax}

\begin{document}
	\title{Geometrical aspects of lattice gauge equivariant convolutional neural networks}
	
	\author{Jimmy Aronsson}
	\email[]{jimmyar@chalmers.se}
	\affiliation{Chalmers University of Technology, Department of Mathematical Sciences, SE-412 96 Gothenburg, Sweden}
	
	\author{David I.~M\"uller}
	\email[Corresponding author: ]{dmueller@hep.itp.tuwien.ac.at}
	\author{Daniel Schuh}
	\email[]{schuh@hep.itp.tuwien.ac.at}
	\affiliation{TU Wien, Institute for Theoretical Physics, A-1040 Vienna, Austria}
	
	\date{\today}
	
	\begin{abstract}
		Lattice gauge equivariant convolutional neural networks (L-CNNs) are a framework for convolutional neural networks that can be applied to non-Abelian lattice gauge theories without violating gauge symmetry. We demonstrate how L-CNNs can be equipped with global group equivariance. This allows us to extend the formulation to be equivariant not just under translations but under global lattice symmetries such as rotations and reflections. Additionally, we provide a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as a special case of gauge equivariant neural networks on $\SU(N)$ principal bundles.
	\end{abstract}
	
	\maketitle
	
	\section{Introduction}
	
	In recent years, machine learning methods incorporating ideas based on symmetry and geometry, often summarized under the term geometric deep learning \cite{Bronstein:2021aaa, Gerken:2021sla}, have received much attention in both computer vision and physics. Most famously, convolutional neural networks (CNNs) \cite{Lecun:1989backprop} have proven to be an excellent machine learning architecture for computer vision tasks such as object detection and classification. The classic examples include determining whether an image contains a particular animal (e.g.~cat or dog \cite{Parkhi:2012cats}), or identifying numbers in images of hand-written digits \cite{Deng:2012mnist}. For these tasks it has been demonstrated that CNN architectures excel both in terms of accuracy and reduced model size, i.e.~the number of model parameters. A key differentiating feature of CNNs compared to generic neural networks is that they are formulated as stacks of convolutional layers, which exhibit translational symmetry or, more accurately, translational equivariance. If a translation is applied to the input of a convolutional layer, then the resulting output will be appropriately shifted as well. 
	This equivariance property is highly useful in the case of image classification, where the absolute position of a particular feature (a cat; a hand-written digit) in the image is not important. Translational equivariance further implies weight sharing, which reduces the number of required model parameters and training time. Consequently, CNNs provide not just more accurate but also more robust models compared to their translationally non-symmetric counterparts.
	
	Whereas CNNs are only guaranteed to be translation equivariant, the concept of equivariance in neural networks can be extended to symmetries beyond translations, such as rotations or reflections. Group equivariant CNNs (G-CNNs) \cite{Cohen:2016aaa,Cohen:2018eq,Aronsson:2022homogeneous} and steerable CNNs \cite{Cohen:2016ste,Weiler:2017ste,Cesa:2021program} use convolutions on groups to achieve equivariance with respect to general global symmetries. Analogous to the equivariance property of traditional CNNs, group transformations (e.g.~roto-translations) applied to the input of a group equivariant convolutional layer, lead to the same group transformation being consistently applied to the output. Group convolutional layers thus commute with group transformations. In certain applications where larger symmetries are important, these networks have been shown to further improve performance compared to networks exhibiting less symmetry \cite{Graham:2020dense,Gerken:2022spherical}. From a physical perspective, the symmetries considered in CNNs and, more generally, in G-CNNs are analogous to  global symmetries of lattice field theories, which has led to numerous applications of CNNs in high energy physics (see \cite{Boyda:2022nmh} for a review). For example,
	CNNs have been applied to detect phase transitions and learn observables \cite{Zhou:2018ill, Blucher:2020mjt, Bachtis:2020ajb, Bulusu:2021rqz, Bachtis:2021xoh} and as generative models \cite{Nicoli:2020njz, Albergo:2021vyo, deHaan:2021erb, Gerdes:2022eve, Albergo:2021bna} in both scalar and fermionic lattice field theories. 
	
	In addition to global symmetries, the laws of physics of the fundamental interactions are based on the notion of local symmetry, which is the foundation of gauge theories. Local symmetries allow for group transformations that can differ at every point in space-time. In machine learning, gauge equivariant neural networks \cite{Cohen:2019aaa, Cheng:2019covariance} (see also \cite{Gerken:2021sla} for a review) have been proposed as architectures that are well-suited for data living on curved manifolds. In high energy physics, similar methods have been applied to problems in lattice gauge theory. For example, gauge symmetric machine learning models have been used as generative models \cite{Kanwar:2020xzo, Boyda:2020hsi, Albergo:2021vyo, Abbott:2022zhs, Bacchio:2022vje, Lehner:2023bba} to avoid the problem of critical slowing down inherent to Markov Chain Monte Carlo simulations at large lattice sizes. Going beyond specific applications, some of the authors of this work have recently proposed Lattice gauge equivariant CNNs (L-CNNs) \cite{Favoni:2020reg}  as a general gauge equivariant architecture for generic machine learning problems in lattice gauge theory. L-CNNs use gauge field configurations as input and can process data in a manner compatible with gauge symmetry. They consist of a set of gauge equivariant layers to build up networks as stacks of individual layers. In particular, gauge equivariant convolutional layers (L-Convs) are convolutional layers which use parallel transport to preserve gauge symmetry while combining data at different lattice sites. Because of their expressiveness, L-CNNs can be used as universal approximators of arbitrary gauge equivariant and invariant functions. It has been demonstrated in \cite{Favoni:2020reg} that L-CNNs can accurately learn gauge invariant observables such as Wilson loops from datasets of gauge field configurations. Similar to CNNs, the convolutions used in L-CNNs are equivariant under lattice translations.
	
	In this paper we revisit L-CNNs from a geometric point of view and extend them by including a larger degree of lattice symmetry. First, we review lattice gauge theory and our original formulation of L-CNNs in Section~\ref{sec:background}. L-CNNs were originally constructed by incorporating local symmetry into ordinary CNNs, which means that L-CNNs are equivariant under lattice translations but not under other lattice symmetries such as rotations and reflections. We remedy this in Section~\ref{sec:general} by applying methods from G-CNNs to L-CNNs. Our main result is a gauge equivariant convolution that can be applied to tensor fields and that is equivariant under translations, rotations, and reflections. Finally, in Section~\ref{sec:bundles}, we put the original L-CNNs in a broader context by relating them to a mathematical theory for equivariant neural networks. In doing so, we demonstrate how convolutions in L-CNNs can be understood as discretizations of convolutions on $\SU(N)$ principal bundles.
	
	\section{Theoretical background} \label{sec:background}
	
	In this section we review Yang-Mills theory and lattice gauge theory in the way these topics are usually introduced within high-energy physics. Having defined concepts such as gauge symmetry and gauge invariance, we then review aspects of L-CNNs.
	
	\subsection{Yang-Mills theory}
	
	We consider $\SU(N)$ Yang-Mills theory on Euclidean space-time \mbox{$\mathcal{M} = \mathbb{R}^{D}$} with \mbox{$D-1 > 0$} spatial dimensions. We choose Cartesian coordinates~$x^\mu$ on $\mathcal{M}$ with \mbox{$\mu \in \{1, 2, \dots, D\}$} such that the metric on $\mathcal{M}$ is Euclidean, i.e.~\mbox{$g_{\mu\nu} = \delta_{\mu\nu}$}, where $\delta_{\mu\nu}$ is the Kronecker symbol. The degrees of freedom in this theory are gauge fields~$A_\mu(\mx)$, which are $\su(N)$-valued vector fields on $\mathcal{M}$. We further choose a matrix representation of $\su(N)$, namely the fundamental representation spanned by the generators~\mbox{$t^a \in \mathbb{C}^{N\times N}$} with \mbox{$a\in \{1, 2, \dots, N^2 - 1 \}$}, which are traceless Hermitian matrices usually normalized to satisfy
	\begin{equation}
		\Tr \left[ t^a t^b \right] = \frac{1}{2} \delta^{ab}.
	\end{equation}
	With a basis for both $\mathcal{M}$ and $\su(N)$, a gauge field can be written as the 1-form
	\begin{equation}
		\mathcal{A}(\mx) = A_\mu(\mx) \diff  x^\mu =  A^a_\mu(\mx) t^a \diff x^\mu,
	\end{equation}
	with components~\mbox{$A^a_\mu: \mathcal{M} \rightarrow \mathbb{R}$}. Two different gauge fields~$\mathcal{A}$ and~$\mathcal{A}'$ are considered to be gauge equivalent if their components can be related via a gauge transformation~$T_\Omega$,
	\begin{equation}
		A'_\mu(\mx) = T_\Omega A_\mu(\mx) = \Omega(\mx) (A_\mu(\mx) - i \p_\mu) \Omega^\dagger(\mx),
	\end{equation}
	where \mbox{$\Omega: \mathcal{M} \rightarrow \SU(N)$} is a differentiable function on space-time. Gauge fields that can be related via gauge transformations form an equivalence class. Within gauge theory, (the components of) gauge fields are not considered physical, observable fields. Rather, the physical state of a system is the same for all gauge fields in any particular equivalence class. Observables within gauge theory therefore must be gauge invariant functionals of $\mathcal{A}$. The most prominent example for such a gauge invariant functional is the Yang-Mills action
	\begin{equation} \label{eq:YM_action}
		S[\mathcal{A}] = \frac{1}{2g^2} \intop_\mathcal{M} \diff^{D} x \,  \Tr \left[ F_{\mu\nu}(\mx) F^{\mu\nu}(\mx) \right],
	\end{equation}
	which maps a gauge field~$\mathcal{A}$ to a single real number~$S[\mathcal{A}] \in \mathbb{R}$. Here, $g > 0$ is the Yang-Mills coupling constant, and the $\su(N)$-valued field strength tensor is given by
	\begin{equation}\label{eq:YM_field_strength}
		F_{\mu\nu}(\mx) = \p_\mu A_\nu(\mx) - \p_\nu A_\mu(\mx) + i \left[ A_\mu(\mx), A_\nu(\mx) \right],
	\end{equation}
	where $\left[ \ ,  \ \right]$ denotes the commutator of matrices in the fundamental representation of $\su(N)$. Under gauge transformations, the field strength tensor is transformed according to
	\begin{equation}
		T_\Omega F_{\mu\nu}(\mx) = \Omega(\mx) F_{\mu\nu}(\mx) \Omega^\dagger(\mx).
	\end{equation}
	Because of the transformation behavior of the field strength tensor and the trace in the Yang-Mills action, the value of the action is invariant under gauge transformations, i.e.
	\begin{equation}
		S[T_\Omega \mathcal{A}] = S[\mathcal{A}].
	\end{equation}
	The invariance of the Yang-Mills action under gauge transformations is called gauge symmetry.
	
	\subsection{Lattice gauge theory}
	Lattice discretizations of non-Abelian Yang-Mills theory with exact lattice gauge symmetry can be constructed with the help of the link formalism of lattice gauge theory~\cite{Wilson:1974sk}. In this formalism, the gauge fields~\mbox{$A_\mu(\mx) \in \mathfrak{su}(N)$} are replaced by gauge link variables~\mbox{$U_{\mx,\mu} \in \mathrm{SU}(N)$} defined on the edges of a finite hypercubic lattice~$\Lambda$ with periodic boundary conditions. We use the fundamental representation of $\mathfrak{su}(N)$ and $\mathrm{SU}(N)$ to represent gauge fields and gauge links as complex matrices. The links~$U_{\mx,\mu}$ connect a lattice site~$\mx$ to its neighboring sites~\mbox{$\mx+\mu = \mx + a \hat{e}_\mu$}, separated by the lattice spacing~$a$ and the Euclidean basis vector~$\hat{e}_\mu$. The inverse link, which connects $\mx+\mu$ to $\mx$, is written as \mbox{$U_{\mx+\mu, -\mu} = U^\dagger_{\mx,\mu}$}.
	
	In terms of the gauge field, a gauge link is given by the path-ordered exponential
	\begin{equation}
		U_{\mx,\mu} = \mathcal{P} \exp{ i \intop^1_0 \diff s \der{x^\nu(s)}{s} A_\nu(\mx(s))  } \, .
	\end{equation}
	In the convention that is used here, the path ordering operator~$\mathcal{P}$ shifts fields earlier in the path to the left and fields later to the right of the product. The function \mbox{$\mx(s): [0,1] \rightarrow \mathbb{R}^D$} parameterizes the straight-line path connecting $\mx$ to \mbox{$\mx+\mu$}. Geometrically, the gauge links prescribe how to parallel transport along the edges of the lattice. They transform under general lattice gauge transformations~$T_\Omega$, \mbox{$\Omega: \Lambda \rightarrow \mathrm{SU}(N)$} according to
	\begin{equation} \label{eq:U_trans}
		T_\Omega U_{\mx,\mu} = \Omega_\mx U_{\mx,\mu} \Omega^\dagger_{\mx+\mu}.
	\end{equation}
	
	Gauge links are the shortest possible Wilson lines on the lattice. Longer Wilson lines are formed by multiplying links that connect consecutive points to form an arbitrary path on the lattice. For closed paths, they are referred to as Wilson loops, and the smallest loop, which is the \mbox{$1 \times 1$}~loop, is called a plaquette and reads
	\begin{equation}
		U_ {\mx,\mu\nu} = U_{\mx,\mu} U_{\mx+\mu, \nu} U_{\mx+\mu+\nu, -\mu} U_{\mx+\nu, -\nu}.
	\end{equation}
	It transforms under gauge transformations as given by
	\begin{equation} \label{eq:plaq_trans}
		T_\Omega U_{\mx,\mu\nu} = \Omega_\mx U_{\mx,\mu\nu} \Omega_\mx^\dagger.
	\end{equation}
	The Wilson action~\cite{Wilson:1974sk}, which can be written in terms of plaquettes, reads
	\begin{equation} \label{eq:wilson_action}
		S_W[U] = \frac{2}{g^2} \sum_{\mx \in \Lambda} \sum_{\mu < \nu} \mathrm{Re} \, \Tr \left[ \one - U_{\mx,\mu\nu} \right].
	\end{equation}
	Note that \eqref{eq:wilson_action} is invariant under global symmetries of the lattice such as translations, discrete rotations and reflections. Its invariance under lattice gauge transformations follows from the local transformation property of the plaquettes and the trace.
	
	In the continuum limit, for small lattice spacings~\mbox{$a \ll 1$}, gauge links can by approximated by the matrix exponential
	\begin{equation}
		U_{\mx,\mu} \approx \exp\left(i a A_\mu\left(\mx+\frac{1}{2} \mu\right)\right)
	\end{equation}
	at the midpoint~\mbox{$\mx+\frac{1}{2} \mu$}. Furthermore, in this limit, plaquettes approximate the non-Abelian field strength tensor, given by Eq.~\eqref{eq:YM_field_strength},
	\begin{equation}
		U_{\mx,\mu\nu} \approx \exp\left( i a^2 F_{\mu\nu} \left(\mx+\frac{1}{2} \mu +\frac{1}{2} \nu \right) \right),
	\end{equation}
	and the Wilson action approximates the Yang-Mills action, introduced in Eq.~\eqref{eq:YM_action}.
	
	\subsection{Lattice gauge equivariant convolutional neural networks}\label{subsec:L-CNN}
	
	An L-CNN is built up by individual layers~$\Phi$, which take as input at least one tuple~\mbox{$(\mathcal{U}, \mathcal{W})$}. The first part of the tuple, \mbox{$\mathcal{U} = \{ U_{\mx,\mu} \}$}, is a set of gauge links in the fundamental representation that transform non-locally as in Eq.~\eqref{eq:U_trans}. The second part, \mbox{$\mathcal{W} = \{ W_{\mx,a} \}$}, is a set of complex matrices \mbox{$W_{\mx,a} \in \mathbb C^{N \times N}$} that transform locally, like plaquettes, as in Eq.~\eqref{eq:plaq_trans}:
	\begin{equation} \label{eq:W_trans}
		T_\Omega W_{\mx, a} = \Omega_\mx W_{\mx, a} \Omega^\dagger_\mx.
	\end{equation}
	Here, the index \mbox{$a \in \{ 1, \ldots N_\mathrm{ch} \}$} refers to the channel, and $N_\mathrm{ch}$ denotes the total number of channels in the layer in question. The output of the layer is, generally, again a tuple $(\mathcal{U}', \mathcal{W}')$, with possibly~a different number of channels. We require every layer to be lattice gauge equivariant in the sense of
	\begin{equation} \label{eq:gauge_equiv}
		\Phi(T_\Omega \mathcal{U}, T_\Omega \mathcal{W}) = T'_\Omega \Phi(\mathcal{U}, \mathcal{W}),
	\end{equation}
	where $T'_\Omega$ denotes the application of the gauge transformation~$\Omega$ to the output of the layer. Generally, one can consider layers where \mbox{$T'_\Omega \neq T_\Omega$}. This would be the case if the representation of the input tuple~\mbox{$(\mathcal U, \mathcal W)$} is different from the one of the output tuple~\mbox{$(\mathcal U', \mathcal W')$}. In this work we only consider layers that do not change the representation of SU($N$) or the transformation behavior of the links~$\mathcal{U}$ and the locally transforming matrices~$\mathcal W$ in any way. Additionally, we only focus on layers that do not modify the set of gauge links~$\mathcal{U}$, i.e.~we always require that \mbox{$\mathcal U ' = \mathcal{U}$}. A network built from multiple gauge equivariant layers \mbox{$\{\Phi_1, \Phi_2, \dots, \Phi_N\}$} through composition, \mbox{$\Phi_N \circ \dots \circ \Phi_2 \circ \Phi_1$}, respects lattice gauge equivariance in the sense of Eq.~\eqref{eq:gauge_equiv}. In the following, we review some of the layers introduced in~\cite{Favoni:2020reg}.
	
	
	A convolutional layer usually aims to combine data from different locations with trainable weights in a translationally equivariant manner. In an L-CNN, such a layer is required to respect lattice gauge equivariance as well. This is fulfilled by the Lattice gauge equivariant Convolutional (L-Conv) layer, which is a map \mbox{$(\mathcal{U}, \mathcal{W}) \mapsto (\mathcal{U}, \mathcal{W}')$}, defined as
	\begin{equation}
		W'_{\mx, a} = \sum\limits_{b, \mu, k} \psi_{a, b, \mu, k} U_{\mx, k \cdot \mu} W_{\mx + k \cdot \mu, b} U^\dagger_{\mx, k \cdot \mu},
		\label{eq:L-conv}
	\end{equation}
	with the trainable weights~\mbox{$\psi_{a, b, \mu, k} \in \mathbb{C}$}, output channel index \mbox{$1 \le a \le N_\mathrm{ch, out}$}, input channel index \mbox{$1 \le b \le N_\mathrm{ch, in}$}, lattice directions \mbox{$1 \le \mu \le D$} and distances \mbox{$-N_k \le k \le N_k$}, where $N_k$ determines the kernel size. Note that the output channels associated with $W'$ are a linear combination of the input channels of $W$. In general, the number of input channels~$N_\mathrm{ch, in}$ may differ from the number of output channels~$N_\mathrm{ch, out}$. The matrices $U_{\mx, k \cdot \mu}$ appearing in Eq.~\eqref{eq:L-conv}  describe parallel transport starting at the point $\mx$ to the point $\mx+k\cdot \mu$. They are given by
	\begin{equation}
		U_{\mx, k \cdot \mu} = \prod^{k - 1}_{i = 0} U_{\mx + i \cdot \mu, \mu} = U_{\mx, \mu} U_{\mx + \mu, \mu} U_{\mx + 2 \cdot \mu, \mu} \dots U_{\mx + (k - 1)\cdot \mu, \mu}
	\end{equation}
	for positive $k$ and
	\begin{equation}
		U_{\mx, k \cdot \mu} = \prod^{k - 1}_{i = 0} U_{\mx - i \cdot \mu, -\mu} = U_{\mx, -\mu} U_{\mx - \mu, -\mu} U_{\mx - 2 \cdot \mu, -\mu} \dots U_{\mx - (k - 1)\cdot \mu, -\mu}
	\end{equation}
	for negative $k$.
	Only parallel transports along straight paths are considered because the shortest path between two lattice sites is not unique otherwise. Data on lattice points that are not connected by straight paths can be combined by stacking multiple layers. A bias term can be included by adding the unit element~$\mathbb 1$ to the set~$\mathcal{W}$. A further increase in expressivity can be achieved by also adding the Hermitian conjugates of $W_{\mx, i}$ to $\mathcal{W}$. A general L-Conv layer thus may be written as
	\begin{equation}
		W'_{\mx, a} = \sum\limits_{b, \mu, k} \psi_{a, b, \mu, k} U_{\mx, k \cdot \mu} W_{\mx + k \cdot \mu, b} U^\dagger_{\mx, k \cdot \mu} + \sum\limits_{b, \mu, k} \tilde \psi_{a, b, \mu, k} U_{\mx, k \cdot \mu} W^\dagger_{x + k \cdot \mu, b} U^\dagger_{\mx, k \cdot \mu} + \psi_0 \mathbb 1,
	\end{equation}
	with weights~$\psi_{a, b, \mu, k}$, $\tilde \psi_{a, b, \mu, k}$ and a bias term~$\psi_0$. For brevity, we will use the more compact form given in Eq.~\eqref{eq:L-conv}. L-Conv layers are gauge equivariant by virtue of the transformation behavior of the parallel transporters~$U_{\mx, k \cdot \mu}$. From Eq.~\eqref{eq:U_trans} it follows that 
	\begin{equation}
		T_\Omega U_{\mx, k\cdot\mu} = \Omega_\mx U_{\mx, k\cdot\mu} \Omega^\dagger_{\mx+k\cdot\mu}.
	\end{equation}
	The matrices~$U_{\mx, k \cdot \mu}$ thus allow the L-Conv layer to combine data from various lattice sites without violating gauge symmetry.
	
	It follows that if data is combined only locally, there is no need for parallel transport to construct a lattice gauge equivariant layer. This is realized by Lattice gauge equivariant Bilinear (L-Bilin) layers, which are maps \mbox{$(\mathcal{U}, \mathcal{W}), (\mathcal{U}, \mathcal{W}') \mapsto (\mathcal{U}, \mathcal{W}'')$}, given by
	\begin{equation}
		W''_{\mx, a} = \sum\limits_{b, c} \alpha_{a, b, c} W_{\mx, b} W'_{\mx, c}. \label{eq:L-Bilin}
	\end{equation}
	The weights~$\alpha_{a, b, c} \in \mathbb{C}$ are trainable, have an output channel index \mbox{$1 \le a \le N_\mathrm{ch, out}$} and two input channel indices \mbox{$1 \le b \le N_\mathrm{ch, in}$} and \mbox{$1 \le c \le N'_\mathrm{ch, in}$}. Analogously to the L-Conv layer, the unit element and the Hermitian conjugates can be added to~$\mathcal{W}$ to increase the expressivity of the L-Bilin layer.
	
	Lattice gauge equivariant Activation functions (L-Act), which are maps \mbox{$(\mathcal{U}, \mathcal{W}) \mapsto (\mathcal{U}, \mathcal{W}')$}, are the generalization of standard activation functions to the L-CNN. They can be applied at every lattice site and are given by 
	\begin{equation} \label{eq:L-Act}
		W'_{\mx,a} = \nu_{\mx,a}(\mathcal{W}) W_{\mx,a},
	\end{equation}
	where $\nu$ is any scalar-valued and gauge invariant function. One option is \mbox{$\nu_{\mx,a}(\mathcal{W}) = \Theta (\mathrm{Re} (\Tr (W_{\mx,a})))$}, with the Heaviside function~$\Theta$. For real-valued scalars~$s$, this would lead to the well-known ReLU activation function, which can also be written as \mbox{$\mathrm{ReLU} (s) = \Theta (s) s$}.
	
	Finally, the last important layer to consider is the Trace layer. This layer maps \mbox{$(\mathcal{U}, \mathcal{W}) \mapsto \mathcal{T}_{\mx,a}$}, which converts the lattice gauge equivariant quantities~$(\mathcal{U}, \mathcal{W})$ to lattice gauge invariant quantities
	\begin{equation}
		\mathcal{T}_{\mx,a} (\mathcal{U}, \mathcal{W}) = \Tr (W_{\mx,a}).
	\end{equation}
	A gauge invariant layer such as this is necessary if the network output is supposed to approximate~a (gauge invariant) physical observable.
	
	As an example, an L-CNN can compute the plaquettes as a pre-processing step and use them as local variables~$W_{\mx,a}$ in the subsequent layers. With stacks of L-Conv and L-Bilin layers, it can build arbitrarily shaped loops, depending on the number of these stacks~\cite{Favoni:2020reg}. The network expressivity can be increased further by introducing L-Acts between said stacks, and if the output is a physical observable, there should be a Trace layer at the end. After the Trace layer, a conventional CNN or other neural network can be added without breaking lattice gauge equivariance.
	
	\section{Extending L-CNNs to general group equivariance}\label{sec:general}
	
	In the last section, we have reviewed the L-Conv operation on a hypercubic lattice \mbox{$\Lambda = \lat$}. Like a standard convolutional layer, the L-Conv layer is equivariant under (integer) translations on $\lat$, which, when interpreted as a group~$\TT$, can be identified with the lattice itself, \mbox{$\TT \sim \lat$}. However, lattice gauge theories on hypercubic lattices typically exhibit larger isometry groups~$G$ that include discrete rotations and reflections, which we denote by the subgroup $K \subset G$. In this section, we explicitly construct L-CNN layers that are compatible with general \nobrhyph{$G$}{symmetry}.
	
	The original approach to G-CNNs \cite{Cohen:2016aaa} is based on promoting feature maps from functions on the lattice~$\lat$, or, more generally, functions on some space~$\mathcal{M}$, to functions on the group~$G$. Group equivariant convolutions, or \gconv{}s,
	are analogous to traditional convolutions, except that integrals (or sums in the case of a discrete space~$\mathcal{M}$) are carried out over the whole group~$G$. In contrast, the modern approach to G-CNNs on homogeneous spaces~$\mathcal{M}$ uses a fiber bundle formalism \cite{Cohen:2018eq,Aronsson:2022homogeneous} in which feature maps are modeled via fields on $\mathcal{M}$, that is, via sections of associated vector bundles over $\mathcal{M}$. This approach is geometrically pleasing because it means that the inputs to and outputs from convolutional layers live directly on $\mathcal{M}$. It also offers computational advantages since convolutional layers become integrals over \mbox{$\mathcal{M} \simeq G/K$} rather than integrals over the larger space~$G$. Here, $K$ is the subgroup of $G$ that stabilizes an arbitrarily chosen origin in $\mathcal{M}$.
	
	However, in order to take advantage of this simplification, one needs to identify feature maps \mbox{$f : G \to \mathbb{R}^N$} with fields on $\mathcal{M}$. This imposes a constraint given by
	\begin{equation}
		f(gk) = \rho(k)^{-1} f(g),
	\end{equation}
	where \mbox{$g \in G, k \in K$}, and $\rho$ is a representation of $K$ (see \cite{Aronsson:2022homogeneous} for details). This constraint is difficult to enforce numerically and is sometimes ignored, preventing the geometric view of $f$ as a field on $\mathcal{M}$. Fortunately, ignoring the constraint effectively promotes $f$ to a field on the group~$G$ instead, similar to the approach laid out in \cite{Cohen:2016aaa}, and the bundle formalism still applies after changing the homogeneous space from $\mathcal{M}$ to $G$. Even though ignoring the constraint makes convolutional layers more expensive to compute, it drastically simplifies their implementation in machine learning frameworks such as \textit{PyTorch}~\cite{PyTorch}. In our case, this is because the group~$G$ of lattice symmetries is a semi-direct product \mbox{$G = \TT \rtimes K$} of translations~\mbox{$x \in \mathbb{T}$} and rotoreflections~\mbox{$r \in G/\mathbb{T} = K$}. Group elements can thus be split into products \mbox{$g = xr$}. Consequently, feature maps \mbox{$f: G \rightarrow \mathbb R^N$} can be viewed as ``stacks'' of feature maps $f_r(x)$ on the lattice~$\lat$. It can be shown that \gconv{}s can be expressed in terms of traditional $\lat$-convolutions \cite{Cohen:2016aaa}, for which highly efficient implementations already exist.
	
	Our strategy to develop a \gequivariant{} framework for L-CNNs is thus the following:
	We first review group equivariant networks without gauge symmetry by working out explicit \gconv{}s for scalar fields, vector fields and general tensor fields discretized on the lattice~$\lat$, in the spirit of the original G-CNN formulation \cite{Cohen:2016aaa}. We then show how \gconv{}s can be combined with our approach to lattice gauge equivariant convolutions to obtain fully \gequivariant{} L-Convs. We extend our approach to bilinear layers (L-Bilin), activation layers (L-Act), trace layers and pooling layers, which allows us to formulate fully \gequivariant{} L-CNNs.
	
	\subsection{Group equivariant convolutions for scalars on the lattice \texorpdfstring{\textbf{}}{} \label{sec:scalar_gconvs}} 
	
	Convolutional layers in traditional CNNs act on feature maps, e.g.~functions~\mbox{$f:\lat \rightarrow \mathbb R^n$}, where $n$ is the number of channels. For explicitness we consider real-valued feature maps. A real-valued convolution with $n$ input channels and $n'$ output channels is given by
	\begin{equation} \label{eq:standard_conv}
		[\psi \conv f]^a(\mx) = \sum^{n}_{b=1}\sum_{\my \in \lat} \psi^{ab}(\my - \mx) f^b(\my), \quad a \in \{1,2, \dots n' \},
	\end{equation}
	where \mbox{$\psi: \lat \rightarrow \mathbb R^{n' \times n}$} are the kernel weights. Here we explicitly use bold faced letters to denote points on the lattice~$\lat$ and the symbol~$\conv$ to denote a \zconv{}. The convolution operation is equivariant with respect to translations: applying a translation~\mbox{$z \in \TT$}, which can be identified~with the point~\mbox{$\mz \in \lat$}, to \mbox{$[\psi \conv f]$} yields
	\begin{equation}
		\begin{aligned} \label{eq:Zd_conv_equivariance}
			L_z [\psi \conv f]^a(\mx) &= [\psi \conv f]^a(\mx - \mz) \\
			&= \sum^{n}_{b=1}\sum_{\my \in \lat} \psi^{ab}(\my - \mx + \mz) f^b(\my) \\
			&= \sum^{n}_{b=1}\sum_{\my' \in \lat} \psi^{ab}(\my' - \mx) f^b(\my' - \mz) \\
			&= [\psi \conv L_z f]^a(\mx).
		\end{aligned}
	\end{equation}
	The left translation~$L_z$ commutes with the convolution operation. The main idea of \cite{Cohen:2016aaa} is to introduce convolutions that are \gequivariant{}, i.e.~that commute with $L_g$ for \mbox{$g \in G$}. More specifically, two types of \gconv{}s (\gconvabb{}s) are introduced: the first-layer \gconv{} acts on feature maps on $\lat$ and promotes them to feature maps on the group~$G$, and the full \gconv{}, which acts on feature maps on $G$ and outputs feature maps on $G$. To simplify notation and without loss of generality, we set the number of channels to one.
	
	The first-layer \gconv{} acting on a feature map~\mbox{$f:\lat \rightarrow \mathbb R$} is given by \cite{Cohen:2016aaa}
	\begin{equation} \label{eq:gconv_1}
		[\psi \star f](g) = \sum_{\my \in \lat} \psi(g^{-1} \cdot \my) f(\my), \quad g \in G,
	\end{equation}
	where \mbox{$\psi: \lat \rightarrow \mathbb R$} are the kernel weights, and \mbox{$g^{-1} \cdot \my$} denotes the action of the group element~$g^{-1}$ on the point~\mbox{$\my \in \lat$}. Note that we denote \zconv{}s, as in~\eqref{eq:standard_conv}, by~$\conv$ and \gconv{}s by~$\star$. Uniquely splitting the group element~$g$ into a translation~\mbox{$x \in \TT$} (identified with \mbox{$\mx \in \lat$}) and a rotoreflection~\mbox{$r \in K =  G/\TT$} about the origin,~$g = xr$, we have
	\begin{equation} \label{eq:G_action_lat}
		g^{-1} \cdot \my = R^{-1} (\my - \mx),
	\end{equation}
	where \mbox{$R \in \mathbb Z^{D \times D}$} is a matrix representation of the rotoreflection~$r$. This split is unique because $G$ is a semi-direct product \mbox{$G = \TT \rtimes K$}. In addition, the translations $\TT$ form a normal subgroup of $G$:
	\begin{equation}
		g^{-1} x g \in \TT, \quad \forall x \in \TT, \, g \in G.
	\end{equation}
	Note that the result of the \gconv{} in Eq.~\eqref{eq:gconv_1} is a function \mbox{$[f \star \psi]: G \rightarrow \mathbb R$} on the group~$G$. The effect of the rotoreflection~$r$ is that the feature map~$f$ is convolved with the rotated kernel
	\begin{equation}
		L_r \psi(\my - \mx) = \psi(R^{-1} (\my - \mx)).
	\end{equation}
	The first-layer \gconvabb{} can therefore be written as a convolution over~$\lat$
	\begin{equation} \label{eq:gconv_1_split}
		[\psi \star f](xr) = [L_r \psi \conv f](\mx),
	\end{equation}
	which we refer to as the split form (see also section 7 of \cite{Cohen:2016aaa}). 
	
	The full \gconv{} acts on feature maps~\mbox{$f: G\rightarrow \mathbb R$} and is used after the first-layer \gconv{}. It is given by
	\begin{equation} \label{eq:gconv_2}
		[\psi \star f](g) = \sum_{h \in G} \psi(g^{-1} h) f(h),
	\end{equation}
	where \mbox{$\psi: G \rightarrow \mathbb R$} are the kernel weights. Since we are dealing with discrete groups, we use a sum over the group elements to define the convolution. Both $g$ and $h$ are elements of the group $G$,~and $g^{-1} h$ denotes the group product. Just as before, we would like to write this operation in terms of \zconv{}s (the split form) using \mbox{$g = xr$} and \mbox{$h = y s$} with \mbox{$x,y\in \TT$} and \mbox{$r,s \in G/\TT$}. In order to perform this split, we need to be able to interpret functions on $G$ as ``stacks'' of functions on $\lat$. Given \mbox{$f: G\rightarrow \mathbb R$} and \mbox{$h = ys$} we write
	\begin{equation}
		f(h) = f(ys) = f_s(\my),
	\end{equation}
	where \mbox{$f_s:\lat \rightarrow \mathbb R$} for each element~\mbox{$s \in G / \TT$}. The function~$f$ is therefore equivalent to a stack of functions~\mbox{$\{ f_s \, | \, s \in G/\TT\}$}. A left translation acting on $f$ with \mbox{$g=xr$} and \mbox{$h=ys$} induces
	\begin{equation}
		\begin{aligned} \label{eq:scalar_left_translation_split}
			L_g f(h) &= f(g^{-1} h)  \\
			&= f((xr)^{-1} ys)  \\
			&= f(r^{-1} x^{-1} y r r^{-1} s)  \\
			&=f_{r^{-1} s}(R^{-1}(\my - \mx)),
		\end{aligned}
	\end{equation}
	where \mbox{$z = r^{-1} x^{-1} y r \in \TT$ (as $\TT$} is a normal subgroup of $G$) and \mbox{$r^{-1} s \in G / \TT$}. In the last line we have made use of the fact that pure translations~\mbox{$x \in \TT$} can be uniquely identified with points~\mbox{$\mx \in \lat$} via the action of the translation subgroup on the origin~$\mathbf 0$. The point~$\mz$ associated with $z$ is given by
	\begin{equation}
		\mz = z \cdot \mathbf 0 = r^{-1} \cdot ((x^{-1} y) \cdot (r \cdot \mathbf 0)) =   r^{-1} \cdot ((x^{-1} y) \cdot \mathbf 0) = r^{-1} \cdot (\my - \mx) = R^{-1} (\my - \mx),
	\end{equation}
	where \mbox{$r \cdot \mathbf 0 = \mathbf 0$} because rotoreflections form the stabilizer subgroup associated with the origin.
	
	The kernel in Eq.~\eqref{eq:gconv_2} can thereby be written as
	\begin{equation}
		\psi(g^{-1} h ) =\psi_{r^{-1} s}(R^{-1}(\my - \mx)),
	\end{equation}
	hence the split form of the full \gconv{} is given by
	\begin{equation}
		\begin{aligned} \label{eq:gconv_2_split}
			[\psi \star f](xr) &= \sum_{s \in G/\TT} \sum_{\my \in \lat} \psi_{r^{-1} s}(R^{-1}(\my - \mx)) f_s(\my) \\
			&= \sum_{s \in G/\TT} [(L_r \psi_{r^{-1} s}) \conv f_s](\mx),
		\end{aligned}
	\end{equation}
	which is a sum of multiple $\lat$-convolutions with rotated kernels~\mbox{$L_r \psi_{r^{-1} s}(\my - \mx)$}. The split forms Eqs.~\eqref{eq:gconv_1_split} and \eqref{eq:gconv_2_split} are particularly useful for concrete implementations in machine learning frameworks. Writing the \gconv{}s in terms of \zconv{}s allows us to make use of highly optimised implementations such as the \textit{Conv2D} and \textit{Conv3D} functions provided by \textit{PyTorch}.
	
	Both types of \gconv{}s can be compactly written as
	\begin{equation} \label{eq:gconv_scalar_compact}
		[\psi \star f](g) = \sum_{h \in H} L_g \psi(h) f(h),
	\end{equation}
	where we use \mbox{$H = \lat$} for the first-layer \gconvabb{} and \mbox{$H = G$} for the full \gconvabb{}. In this form, it is evident that the two types merely differ in the group that is being summed over (translations~\mbox{$\TT \sim \lat$} in the first-layer \gconvabb{}, the full group~$G$ in the full \gconvabb{}) and how the left translation acts on the kernel~$\psi$. Depending on the choice of $H$, the left translated kernel~\mbox{$L_g \psi$} is either a rotated $\lat$-kernel for \mbox{$H = \lat$} or a translated kernel on the group for \mbox{$H = G$}. It is now easy to check that \gconv{}s are in fact equivariant under left translations $L_g$. Let \mbox{$k\in G$}, then we have
	\begin{equation}
		\begin{aligned}
			L_k [\psi \star f](g) &= [\psi \star f](k^{-1}g) \\
			&=  \sum_{h \in H} L_{k^{-1} g} \psi(h) f(h) \\
			&= \sum_{h \in H} L_{g} \psi(k h) f(h) \\
			&= \sum_{h' \in H} L_{g} \psi(h') f(k^{-1} h') \\
			&= \sum_{h' \in H} L_{g} \psi(h') L_k f(h') \\
			&= [\psi \star L_k f](g),
		\end{aligned}
	\end{equation}
	where we have used the substitution \mbox{$h' = k h$} in the fourth line. For the first-layer \gconvabb{} \mbox{($H = \lat$)}, where \mbox{$h \in \lat$, $h' = k h$} is to be interpreted as a rotated and shifted coordinate on $\lat$ as in Eq.~\eqref{eq:G_action_lat}, whereas for the full \gconvabb{} \mbox{($H = G$)}, \mbox{$h' = k h$} is simply a translated group element in $G$. Note that \gequivariance{} can also be shown for the split forms Eqs.~\eqref{eq:gconv_1_split} and \eqref{eq:gconv_2_split}, but the proof is analogous to the one shown above.
	
	\subsection{Group equivariant convolutions for vector and tensor fields}
	
	We have explicitly shown that the \gconv{}s are equivariant under general transformations~$g$ via left translations $L_g$. The feature maps~\mbox{$f: \lat \rightarrow \mathbb R$}, on which the convolutions act, transform~like scalar fields under $L_g$ with \mbox{$g = xr$}:
	\begin{equation}
		L_g f(\my) = f(R^{-1} (\my - \mx)).
	\end{equation}
	In order to extend \gconv{}s to vectors and tensors, we would expect transformations that~act on the vector structure. For example, consider a vector field~\mbox{$v: \lat \rightarrow \mathbb R^D$} on the lattice with~components \mbox{$v^i: \lat \rightarrow \mathbb R$}. Acting on $v$ with a general transformation \mbox{$g = xr$} yields
	\begin{equation}
		(L_g v)^i(\my) = R^i{}_j v^j(R^{-1}(\my - \mx)),
	\end{equation}
	where $R^i{}_j$ are the components of the matrix representation of \mbox{$r \in G / \TT$} on $\mathbb R^D$. More generally, a type $(n,m$) tensor field~$w$, i.e.~with $n$ vector and $m$ co-vector components, transforms according to
	\begin{equation}
		(L_g w)^{i_1 \dots i_n}{}_{j_1 \dots j_m}(\my) = R^{i_1}{}_{i_1'} \dots R^{i_n}{}_{i_n'} w^{i_1' \dots i_n'}{}_{j_1' \dots j_m'}(R^{-1} (\my - \mx))  (R^{-1})^{j_1'}{}_{j_1} \dots (R^{-1})^{j_m'}{}_{j_m}.
	\end{equation}
	
	Based on the compact form of scalar \gconv{}s, Eq.~\eqref{eq:gconv_scalar_compact}, we now make the following guess at \gconv{}s which map tensors of type $(n, 0)$ to tensors of the same type:
	\begin{equation}
		[\psi \star w]^{i_1 \dots i_n}(g) = \sum_{h \in H} (L_g \psi)^{i_1 \dots i_n}{}_{j_1 \dots j_n}(h) w^{j_1 \dots j_n}(h),
	\end{equation}
	where \mbox{$H = \TT \sim \lat$} for the first-layer and \mbox{$H=G$} for the full \gconvabb{}. Here, the kernel~$\psi$ is a tensor of type $(n, n)$ and acts as general linear transformation of the tensor components of $w$. For the full \gconvabb{}, \mbox{$H = G$}, left translations acting on tensor fields~$\psi$ of type $(n,m)$ on the group are given by
	\begin{equation}
		(L_g \psi)^{i_1 \dots i_n}{}_{j_1 \dots j_m}(h) = R^{i_1}{}_{i_1'} \dots R^{i_n}{}_{i_n'} \psi^{i_1' \dots i_n'}{}_{j_1' \dots j_m'}(g^{-1} h)  (R^{-1})^{j_1'}{}_{j_1} \dots (R^{-1})^{j_m'}{}_{j_m}.
	\end{equation}
	\gconv{}s for tensors of mixed type $(n, m)$ are defined analogously. 
	
	The tensor \gconv{}s can be cast into a more compact form by introducing condensed index notation: we write the block of indices \mbox{$i_1\dots i_n$} as multi-indices~$I$ so that
	\begin{equation}
		\begin{aligned}
			w^I{}_J &:= w^{i_1 \dots i_n}{}_{j_1 \dots j_n}, \\
			\delta^I{}_J &:= \delta^{i_1}{}_{j_1} \dots \delta^{i_n}{}_{j_n}, \\
			R^I{}_J &:= R^{i_1}{}_{j_1} \dots R^{i_n}{}_{j_n}.
		\end{aligned}
	\end{equation}
	Contractions of $R$ and $R^{-1}$ can be written as
	\begin{equation}
		\begin{aligned}
			R^I{}_J (R^{-1})^J{}_K &= R^{i_1}{}_{j_1} \dots R^{i_n}{}_{j_n} (R^{-1})^{j_1}{}_{k_1} \dots (R^{-1})^{j_n}{}_{k_n}  \\
			&= R^{i_1}{}_{j_1} (R^{-1})^{j_1}{}_{k_1} \dots R^{i_n}{}_{j_n}(R^{-1})^{j_n}{}_{k_n}  \\
			&= \delta^{i_1}{}_{k_1} \dots \delta^{i_n}{}_{k_n}  \\
			&= \delta^I{}_J.
		\end{aligned}
	\end{equation}
	Using multi-indices, the tensor \gconv{}s are given by
	\begin{equation} 
		[\psi \star w]^{I}(g) = \sum_{h \in H} (L_g \psi)^{I}{}_{J}(h) w^{J}(h), \label{eq:tensor_gconv_compact}
	\end{equation}
	and left translations act on tensors according to
	\begin{equation} \label{eq:Lg_tensor}
		(L_g w)^{I}{}_{J}(h) = R^{I}{}_{I'} w^{I'}{}_{J'}(g^{-1} h)  (R^{-1})^{J'}{}_{J}.
	\end{equation}
	Equivariance of Eq.~\eqref{eq:tensor_gconv_compact} follows from
	\begin{equation}
		\begin{aligned} \label{eq:gconv_tensor_equiv}
			(L_k [\psi \star w])^I(g) &= R^I{}_{I'} [\psi \star w]^{I'}(k^{-1} g) \\
			&= \sum_{h \in H} R^I{}_{I'} (L_{k^{-1} g} \psi)^{I'}{}_J(h) w^J(h) \\
			&= \sum_{h \in H}  R^I{}_{I'} (R^{-1})^{I'}{}_{I''} (L_{g} \psi)^{I''}{}_{J'}(k h) R^{J'}{}_{J} w^J(h) \\
			&= \sum_{h' \in H}  (L_{g} \psi)^{I}{}_J(h') R^{J'}{}_{J} w^J(k^{-1} h') \\
			&= [\psi \star L_k w]^I(g),
		\end{aligned}
	\end{equation}
	where we have used the substitution \mbox{$h' = k h$}. 
	
	We can also define \gconv{}s that change the tensor type. Given multi-indices \mbox{$I_n = i_1 \dots i_n$} and \mbox{$J_m = j_1 \dots j_m$} with \mbox{$n \neq m$} in general, we define
	\begin{equation} 
		\tilde{w}^{I_n} = [\psi \star w]^{I_n}(g) = \sum_{h \in H} (L_g \psi)^{I_n}{}_{J_m}(h) w^{J_m}(h), \label{eq:gen_tensor_gconv}
	\end{equation}
	with \mbox{$H = \lat$} for the first-layer and \mbox{$H = G$} for the full \gconvabb{}. Here, the output feature map~$\tilde w$ is a tensor of type $(n, 0)$ while the input feature map~$w$ is a tensor of type $(m, 0)$. The kernel~$\psi$ is of type $(n, m)$. For example, one can use these types of \gconv{}s to reduce a rank 2 tensor to a vector field within a G-CNN while keeping the equivariance property.
	
	\subsection{Split forms of tensor \texorpdfstring{$G$}{G}-convolutions}
	
	As in the case of scalar \gconv{}s, it is beneficial to write tensor \gconv{}s Eqs.~\eqref{eq:tensor_gconv_compact} in their split forms, analogous to Eqs.~\eqref{eq:gconv_1_split} and \eqref{eq:gconv_2_split}. Using \mbox{$h = ys \in G$} with \mbox{$y \in \TT$} and \mbox{$s \in G / \TT$}, we write the tensor field of type $(n, m)$
	\begin{equation}
		w^I{}_J(h) = w^I{}_J(ys) = (w_s)^I{}_J(\my),
	\end{equation}
	where \mbox{$\my \in \lat$}. Analogous to Eq.~\eqref{eq:scalar_left_translation_split}, left translations act on $w$ via
	\begin{equation}
		(L_g w)^I{}_J(h) = R^I{}_{I'} (w_{r^{-1} s})^{I'}{}_{J'}(R^{-1} (\my - \mx)) (R^{-1})^{J'}{}_J,
	\end{equation}
	where \mbox{$g = xr$}, $R^I{}_J$ is the $(n, m)$ matrix representation of $r$ and $R$ is its representation on $\lat$. The two types of tensor \gconv{}s can then be written as
	\begin{align}
		&\begin{aligned}
			\relax [\psi \star w]^{I}(g) &= \sum_{\my \in \lat} R^I{}_{I'} \psi^{I'}{}_{J'}(R^{-1}(\my - \mx)) (R^{-1})^{J'}{}_{J} w^{J}(\my)  \\
			&= [(L_r \psi)^I{}_J \conv w^J](\mx), \\
		\end{aligned}\\
		&\begin{aligned}
			\relax [\psi \star w]^{I}(g) &= \sum_{\my \in \lat} \sum_{s \in G / \TT}  R^I{}_{I'} (\psi_{r^{-1} s})^{I'}{}_{J'}(R^{-1}(\my - \mx)) (R^{-1})^{J'}{}_{J} w_s^{J}(\my) \\
			&= \sum_{s \in G / \TT} [(L_r \psi_{r^{-1} s})^I{}_J \conv (w_s)^J](\mx),
		\end{aligned}
	\end{align}
	where $\psi^I{}_J$ denotes the tensor components of the kernels, and \mbox{$\psi^I{}_J: \lat \rightarrow \mathbb R$} for the first-layer and \mbox{$\psi^I{}_J: G \rightarrow \mathbb R$} for the full \gconvabb{}, respectively. Analogously, similar split forms can be obtained for tensors of mixed type and for \gconv{}s that change the tensor type as in Eq.~\eqref{eq:gen_tensor_gconv}.
	
	\subsection{Implementing group equivariance in L-Convs for scalars and tensors}
	
	Having worked out \gconv{}s for general tensors, we shift our focus back to gauge equivariance. The L-Conv introduced in Eq.~\eqref{eq:L-conv} can alternatively be written as 
	\begin{equation}
		[\psi \conv W](\mx) = \sum_{\my \in \lat} \psi(\my - \mx) U_{\mx \rightarrow \my} W(\my) U_{\my \rightarrow \mx},
	\end{equation}
	where we write $W(\mx)$ instead of $W_\mx$ to denote a gauge dependent feature map. We keep the number of input and output channels set to one for convenience. The paths~\mbox{$\mx \rightarrow \my$} in the subscripts of the links~$U$ are kept general to further simplify notation. Since we only want to allow for straight paths, however, we set all other elements of the kernel~$\psi$ to zero. Note that we only need to consider how the convolution acts on the local variables~$W$ because the links are unaffected by an L-Conv.
	
	The L-Conv operation is translationally equivariant, i.e.~the gauge equivariant convolution commutes with translations. When acting on \mbox{$[\psi \conv W]$} with a translation~$z$, we find
	\begin{equation}
		\begin{aligned}
			L_z [\psi \conv W](\mx) = [\psi \conv W](\mx - \mz) &= \sum_{\my \in \lat} \psi(\my - \mx + \mz) U_{\mx - \mz \rightarrow \my} W(\my) U_{\my \rightarrow \mx - \mz} \\
			&= \sum_{\my' \in \lat} \psi(\my' - \mx) U_{\mx - \mz \rightarrow \my' - \mz} W(\my' - \mz) U_{\my' - \mz \rightarrow \mx - \mz} \\
			&= [\psi \conv L_z W](\mx),
		\end{aligned}
	\end{equation}
	where the shift by $z$ induces a translation on both $W$ and $U$. We note that our notation for gauge equivariant convolutions explicitly hides the gauge links, but we want to stress that translations~must also be applied to the links~$U$ and that links should be viewed as part of the input. Keeping in~mind the transformation properties of $U$ and $W$, given by Eqs.~\eqref{eq:U_trans} and \eqref{eq:W_trans}, it is straightforward to check that the L-Conv is equivariant under gauge transformations:
	\begin{equation}
		[\psi \conv T_\Omega W](\mx) = \sum_{\my \in \lat } \psi(\my - \mx) T_\Omega U_{\mx \rightarrow \my} T_\Omega W(\my) T_\Omega U_{\my \rightarrow \mx} = \Omega(\mx) [\psi \conv W](\mx) \Omega^\dagger(\mx).
	\end{equation}
	Thus, the L-Conv commutes both with translations on $\lat$ and with lattice gauge transformations. 
	
	Previously, we have determined the split forms of \gconv{}s when the group $G$ is a semi-direct product of translations~\mbox{$\TT \sim \lat$} and proper and improper rotations~\mbox{$G/\TT$}. For scalar feature maps we found Eqs.~\eqref{eq:gconv_1_split} and \eqref{eq:gconv_2_split}, which reduce the \gconv{} to \zconv{}s with rotated kernels. Following the same ideas, we now extend our lattice gauge equivariant convolutions so that they respect not only translations but larger symmetry groups~$G$. If we consider \mbox{$W:\lat \rightarrow \mathbb C^{N\times N}$} to transform as a scalar under $G$,
	\begin{equation}
		L_g W(\mx) = W(g^{-1} \cdot \mx) = W(R^{-1}(\mx - \my)),
	\end{equation}
	then we can easily extend \gconv{}s to gauge dependent fields~$W$ by replacing the standard \zconv{} with the L-Conv operation. For the first-layer \gconvabb{} of Eq.~\eqref{eq:gconv_1_split} we then have
	\begin{equation}
		\begin{aligned}\relax 
			[\psi \star W](xr) &= [L_r \psi \conv W](\mx) \\
			&= \sum_{\my \in \lat} \psi(R^{-1}(\my - \mx)) U_{\mx \rightarrow \my} W(\my) U_{\my \rightarrow \mx}.
		\end{aligned}
	\end{equation}
	The resulting feature map~\mbox{$\tilde W(g) = [\psi \star W](g)$} is now a feature map on the group~$G$ and transforms under $G$ according to
	\begin{equation}
		L_g \tilde W(h) = \tilde W(g^{-1} h) =  [\psi \star L_g W](h).
	\end{equation}
	Additionally, under lattice gauge transformations we have
	\begin{equation}
		\begin{aligned}
			T_\Omega \tilde W(g) &= T_\Omega \tilde W(xr) \\
			&= [L_r \psi \conv T_\Omega W](xr) \\
			&= \Omega(\mx) [L_r \psi \conv W](xr) \Omega^\dagger(\mx) \\
			&= \Omega(\mx) \tilde W(xr) \Omega^\dagger(\mx),
		\end{aligned}
	\end{equation}
	i.e.~the feature map~$\tilde{W}(g) = \tilde{W}(xr)$ transforms locally at $\mathbf x$. Similarly, the full \gconvabb{} is given by
	\begin{equation}
		[\psi \star W](g) = \sum_{h \in G} L_g \psi(h) U_{q(g) \rightarrow q(h)} W(h) U_{q(h) \rightarrow q(g)},
	\end{equation}
	where $q :G \rightarrow \lat$ projects group elements $g = xr$ down to the lattice by acting on the origin:
	\begin{equation}
		q(g) = g \cdot \mathbf{0} = x \cdot r \cdot \mathbf{0} = x \cdot \mathbf{0} = \mathbf{x}.
	\end{equation}
	
	More compactly, both types of \gconvabb{} can be written as
	\begin{equation}
		[\psi \star W](g) = \sum_{h \in H} L_g \psi(h) U_{q(g) \rightarrow q(h)} W(h) U_{q(h) \rightarrow q(g)},
	\end{equation}
	with \mbox{$H = \TT \sim \lat$} or \mbox{$H = G$}. Similarly, we can generalize these scalar \gconv{}s to tensor convolutions in the same way as in the previous section. A general \gconv{} for gauge dependent tensors is given by
	\begin{equation} \label{eq:lgconv_tensor}
		\tilde{W}^I(g) = [\psi \star W]^I(g) = \sum_{h \in H} (L_g \psi(h))^I{}_J U_{q(g) \rightarrow q(h)} W^J(h) U_{q(h) \rightarrow q(g)},
	\end{equation}
	where $W$ and $\psi$ are of type $(n, 0)$ and $(n, m)$, respectively, and the output feature map transforms as a tensor of type $(m, 0)$. Note that gauge equivariant \gconv{}s merely differ in the appearance of parallel transporters compared to Eq.~\eqref{eq:tensor_gconv_compact}. Consequently, the proof of equivariance of Eq.~\eqref{eq:lgconv_tensor} under left translations in $G$ largely follows the steps performed in Eq.~\eqref{eq:gconv_tensor_equiv}.
	
	One aspect of \gequivariant{} L-Conv layers that has been missing in the discussion so far is the inclusion of bias terms. Generally, bias terms are  additional terms added to the convolution,
	\begin{equation} \label{eq:bias_gconv}
		\tilde{W}^I(g) = [\psi \star W]^I(g) + b^I(g),
	\end{equation}
	where $b^I(g)$ is a tensor that is independent of the feature map $W^I(g)$. To retain group equivariance in Eq.~\eqref{eq:bias_gconv}, we require $b^I(g)$ to be invariant under left translations:
	\begin{equation}
		(L_k b)^I(g) = R^I{}_J b^J(k^{-1}g) \overset{!}{=} b^I(g), \qquad \forall k \in G.
	\end{equation}
	Depending on the symmetry group~$G$, the number of dimensions~$D$ and the tensor type~$(n,0)$ of the feature map, such invariant tensors may or may not exist.
	In the case of scalar feature maps, namely \mbox{$W: \lat \rightarrow \mathbb C^{N \times N}$} (first layer) and \mbox{$W: G \rightarrow \mathbb C^{N \times N}$} (deeper layers), bias terms simply correspond to unit matrices added to the output of the convolution:
	\begin{equation}
		\tilde{W}(g) = [\psi \star W](g) = \sum_{h \in H} (L_g \psi(h)) U_{q(g) \rightarrow q(h)} W(h) U_{q(h) \rightarrow q(g)} + b_0 \mathbb 1,
	\end{equation}
	where $b_0$ is a trainable parameter. However, the geometric structure of these terms becomes more complicated for general tensor feature maps and depends on the symmetry group~$G$. For example, if $G$ consists of rotations, reflections and translations, then there is no vector-type (i.e. tensor of type $(1,0)$) bias term. For rank~2 tensors, i.e.~tensors of type $(2,0)$, bias terms correspond to Kronecker deltas~$\delta_{ij}$. In $D=3$ dimensions, rank~3 bias terms may be given by the Levi-Civita tensor~$\epsilon_{ijk}$ assuming that reflections are not considered as symmetries. More generally for the rotation group in $D=3$, higher ranks are given by products and contractions of $\delta_{ij}$ and $\epsilon_{ijk}$.
	
	\subsection{\texorpdfstring{$G$}{G}-equivariant bilinear layers}
	
	Bilinear layers, which map two feature maps into one, can be generalized to respect \gequivariance{}. Without including channels, a \gequivariant{} bilinear layer for tensors reads
	\begin{equation} \label{eq:bilin}
		W^I(g) = (L_g \hat{\psi})^I{}_{JK} V_1^J(g) V_2^K(g),
	\end{equation}
	where $V_1$ and $V_2$ are tensor feature maps of type $(n_1, 0)$ and $(n_2, 0)$, respectively. The product of $V_1$ and $V_2$ is understood to be a matrix product with respect to their $\mathbb{C}^{N \times N}$ matrix structure.
	The weight tensor~$\hat{\psi}$ is of type \mbox{$(m, n_1+n_2)$} and the resulting tensor feature map~$W$ is of type $(m, 0)$. Note that the weight tensor is constant, i.e.~it does not depend on the group element~$g$. However, because of its index structure it transforms in the usual way
	\begin{equation}
		(L_g \hat\psi)^I{}_{JK} = R^I{}_{I'} \hat\psi^{I'}{}_{J'K'} (R^{-1})^{J'}{}_{J} (R^{-1})^{K'}{}_{K}, 
	\end{equation}
	where $R$ is the matrix representation of the rotational part~$r$ of \mbox{$g=xr$}. It follows that the bilinear layer is equivariant under left translations:
	\begin{equation}
		(L_k W)^I(g) = (L_g \hat{\psi})^I{}_{JK} (L_k V_1)^J(g) (L_k V_2)^K(g).
	\end{equation}
	Gauge equivariance follows from the fact that when both $V_1$ and $V_2$ are locally transforming as in Eq.~\eqref{eq:plaq_trans}, their matrix product also transforms locally. Consequently, the resulting feature map~$W^I$ transforms locally as well.
	
	The bilinear layer can be expressed as a special case of a gauge equivariant tensor convolution. Consider a tensor~$V$ of type \mbox{$(n_1+n_2,0)$} that factorizes into two tensors~$V_1$ and $V_2$ with types~\mbox{$(n_1,0)$} and $(n_2,0)$:
	\begin{equation}
		V^{J K} = V_1^{J} V_2^{K}.
	\end{equation}
	A general lattice gauge equivariant convolution applied to $V$ then reads
	\begin{equation}
		W^I(g) = \sum_{h \in H} (L_g \psi)^I{}_{JK}(h) U_{q(g) \rightarrow q(h)} V_1^J(h) V_2^K(h) U_{q(h) \rightarrow q(g)}.
	\end{equation}
	The bilinear layer is local, i.e.~feature maps are all evaluated at the same element~$g$. We therefore assume that the kernel has the particular form
	\begin{equation}
		\psi^I{}_{JK}(h) = \hat{\psi}^I{}_{JK} \delta(h),
	\end{equation}
	where \mbox{$\delta(e) = 1$} and \mbox{$\delta(h) = 0$} for \mbox{$h\neq e$} with the unit element $e$, and $\hat{\psi}$ is a constant tensor. The left translated kernel~\mbox{$L_g \psi(h)$} only has a single non-zero contribution to the sum, namely when $g^{-1}h$ is the unit element $e$ of the group, i.e.~\mbox{$h = g$}. The convolution then reduces to Eq.~\eqref{eq:bilin}, because the parallel transporters reduce to unit matrices for constant paths \mbox{$q(g) \rightarrow q(g)$}:
	\begin{equation}
		U_{q(g) \rightarrow q(g)} = \one.
	\end{equation}
	Bias terms may also be included in the bilinear layer. As in the case of \gequivariant{} L-Conv layers, bias terms must be invariant tensors.
	
	\subsection{Trace layers}
	
	It is straightforward to show that trace layers are compatible with \gequivariance{}. Given a locally gauge transforming tensor field~$W^I(g)$ we define the traced tensor~$w^I(g)$ simply as
	\begin{equation}
		w^I(g) = \mathrm{Tr}[W^I(g)],
	\end{equation}
	where the trace is taken over the $\mathbb{C}^{N \times N}$ matrix structure. The trace yields a gauge invariant tensor, which can be shown via
	\begin{equation}
		\begin{aligned}
			T_\Omega w^I(g) &= \mathrm{Tr}[T_\Omega W^I(g)] \\
			&= \mathrm{Tr}[\Omega(\mx) W^I(g) \Omega^\dagger(\mx)] \\
			&= \mathrm{Tr}[ W^I(g) ] \\
			&= w^I(g),
		\end{aligned}
	\end{equation}
	where we have assumed \mbox{$q(g) = \mx$}. The traced tensor transforms under $G$ as a tensor because the left translation~$L_k$, \mbox{$\forall k \in G$} commutes with the trace operation over $\mathbb{C}^{N \times N}$. Note that the trace layer does not have any trainable parameters. It can be used at the end of an L-CNN to obtain gauge invariant scalars and tensors, which can then be further processed by standard group equivariant networks.
	
	\subsection{Activation functions}
	
	The purpose of an activation function is to introduce non-linearity into the network. In a~CNN or G-CNN with scalar input, it is usually applied point-wise, i.e.~\mbox{$f'(\mx) = \nu(f(\mx))$}, where \mbox{$f, f': G \rightarrow \mathbb{R}$} are the feature maps before and after applying the (scalar) activation function \mbox{$\nu: \mathbb{R} \rightarrow \mathbb{R}$}, respectively. When the input feature map is a general tensor field~$w$ of type $(n,0)$, we choose an ansatz similar to Eq.~\eqref{eq:L-Act}, namely
	\begin{equation} \label{eq:G-act}
		w^{\prime I} = C_\nu w^I = \nu (w) w^I,
	\end{equation}
	with the operator~$C_\nu$, which applies the activation function to the tensor. For \gequivariance{} to hold, $C_\nu$ has to commute with $L_g$
	\begin{equation} \label{eq:G-equiv_act}
		C_\nu L_g w^I = L_g C_\nu w^I.
	\end{equation}
	Keeping in mind the transformation property of $w$, which is given by Eq.~\eqref{eq:Lg_tensor}, we find
	\begin{equation} \label{eq:act_inv_G}
		\nu (\widetilde{w}(h)) = \nu (w(h)),
	\end{equation}
	where \mbox{$\widetilde{w}^I(h) = R^{I}{}_{I'} w^{I'}{}(h)$}.
	Thus, the activation function~$\nu$ has to be invariant under transformations in the group~$G$.
	
	The generalization of Eq.~\eqref{eq:G-act} to lattice gauge invariant activation functions is straightforward. We make the ansatz
	\begin{equation}
		W^{\prime I} (g) = \nu (w(g)) W^I(g),
	\end{equation}
	with the local variables~$W^I$ and $W^{\prime I}$ before and after the application of the activation function, respectively, and \mbox{$w^I(g) = \Re (\Tr (W^I(g)))$}. The form of $w(g)$ guarantees equivariance under lattice gauge transformations, and Eq.~\eqref{eq:act_inv_G} guarantees equivariance under transformations in~$G$.
	
	A possible choice for an activation function is a norm non-linearity~\cite{Gerken:2021sla}
	\begin{equation}
		\nu(w(g)) = \alpha(\lVert w(g) \rVert),
	\end{equation}
	with a norm that satisfies Eq.~\eqref{eq:act_inv_G}, such as \mbox{$\lVert w(g) \rVert = \sqrt{\sum_I (w^I(g))^2}$}. Since the output of a norm is always non-negative, choosing the Heaviside step function~$\Theta$ as the function~$\alpha$, which we have done in Section~\ref{sec:background} to mimic the well-known ReLU activation function,  would not lead to a non-linearity. Therefore, we introduce a trainable bias~\mbox{$b \in \mathbb{R}$} and set
	\begin{equation}
		\alpha(\lVert w(g) \rVert) = \Theta(\lVert w(g) \rVert - b).
	\end{equation}
	In the above example, the ReLU activation function becomes active if the norm of $w$ exceeds the bias~$b$.
	
	\subsection{Pooling}
	
	Pooling layers, which are often used to reduce the domain of a feature map, can also be generalized to G-CNNs. In analogy to~\cite{Cohen:2016aaa}, we split pooling layers into separate pooling and subsampling steps. The pooling step performs a convolution-like operation on the feature map, but does not change the domain. The domain reduction happens during the subsequent subsampling step with a particular stride. Typically, subsampling leads to a reduction in symmetry, depending on the stride. We first review this procedure for scalar feature maps on the group~$G$ before generalizing it to tensor fields on $G$ and finally to the L-CNN.
	
	As a motivating example we consider sum pooling. In a traditional CNN in two dimensions, sum pooling is performed by summing up all values of a feature map in a pooling domain~$\mathcal{D} \subset \mathbb{Z}^2$, e.g.~a \mbox{$2 \times 2$} region, which is moved across the lattice. It differs from average pooling only by a constant factor determined by the cardinality of $\mathcal{D}$. Since sum pooling in a traditional CNN can be written as a special case of convolution (every kernel coefficient in the pooling region is set to one), it is equivariant with respect to translations. Analogously, sum pooling on a feature map~\mbox{$f: G \rightarrow \mathbb{R}$} can be viewed as a special case of the full \gconv{} in Eq.~\eqref{eq:gconv_2}, when setting the kernel~$\psi(g)$ to one in the pooling domain~$\mathcal{D} \subset G$ and zero elsewhere:
	\begin{equation}
		f'(g) = \sum_{h \in G} \psi(g^{-1} h) f(h) = \sum_{h \in g\mathcal{D}} f(h). \label{eq:sum_pool_scalar}
	\end{equation}
	Here, \mbox{$g\mathcal{D} = \{gd: d \in \mathcal{D}\}$} refers to the \nobrhyph{$g$}{translated} pooling region and \mbox{$f': G \rightarrow \mathbb{R}$} is the feature map after the pooling step. Another well-known pooling operation is max pooling, given by
	\begin{equation}
		f'(g) = \max_{h \in g\mathcal{D}} f(h).
	\end{equation}
	
	Sum and max pooling can be generalized to other pooling operations by introducing an operator~$P$ that acts on a feature map~$f$ by
	\begin{equation}
		f'(g) = (Pf)(g) = \mathcal{P}(f(g d_1), f(g d_2), \ldots, f(g d_N)),
	\end{equation}
	where \mbox{$\mathcal{P}: \mathbb{R}^N \rightarrow \mathbb{R}$} is a function, and $N$ is the cardinality of \mbox{$\mathcal{D} = \{ d_1, d_2, \ldots, d_N \}$}. The operator~$P$ \nobrhyph{$g$}{translates} the set $\mathcal{D}$ over the feature map, so the same pooling operation is performed everywhere on~$G$, rendering it \gequivariant{}. That is, it commutes with the left translation operator~$L_k$. This can be explicitly shown via
	\begin{equation}
		\begin{aligned}
			L_k (Pf)(g) &= (Pf)(k^{-1} g) \\
			&= \mathcal{P}(f(k^{-1} g d_1), \dots, f(k^{-1} g d_N)) \\
			&= \mathcal{P}(L_k f(g d_1), \dots, L_k f(g d_N)) \\
			&= (P L_k f)(g).
		\end{aligned}
	\end{equation}
	
	To achieve a pooling with stride~$s$ in a traditional CNN, after the pooling step, the feature map is subsampled on the subgroup \mbox{$\TT_s \subset \TT$} consisting of all translations that are multiples of $s$ elementary translations. The resulting feature map is then equivariant under $\TT_s$. Analogously, for feature maps \mbox{$f: G \rightarrow \mathbb R$}, subsampling is performed on a subgroup~$H \subset G$. This procedure is known as subgroup pooling and the resulting feature map retains equivariance only under the subgroup~$H$.
	
	If the pooling region~$\mathcal{D}$ is itself a subgroup of $G$, then $g\mathcal{D}$ are left cosets of $\mathcal{D}$. They partition the group~$G$ into disjoint, equally sized subsets. Since the left cosets are invariant under the right-action (or right translation) of $\mathcal{D}$, i.e.~\mbox{$gd\mathcal{D} = g\mathcal{D}$}, $\forall d\in \mathcal{D}$, the corresponding parts of the feature map are invariant under said action as well, and we can pick one such part to be the resulting feature map without losing \gequivariance{}. This type of pooling is called coset pooling. For example, in a network with scalar input on~$\lat$ that is promoted to the group~\mbox{$G = \TT \rtimes K$} by a first-layer \gconv{} and further convolved with full \gconv{}s, coset pooling over $K$ would yield a \gequivariant{} feature map on \mbox{$\TT \sim \lat$}.
	
	In order to generalize sum pooling to tensor fields~$w(g)$ on $G$, we take full tensor \gconv{}s, which are given by Eq.~\eqref{eq:tensor_gconv_compact}, with \mbox{$H = G$} as a starting point and set the kernel~\mbox{$\psi^I{}_J(g) = \delta^I{}_J \psi(g)$}, where $\psi(g)$ is  one if \mbox{$g \in \mathcal{D}$} and zero everywhere else. We get
	\begin{equation}
		\begin{aligned}
			w^{\prime I}(g) = [\psi \star w]^{I}(g) &= \sum_{h \in G} (L_g \psi)^{I}{}_{J}(h) w^{J}(h) \\
			&= \sum_{h \in G} R^I{}_{I'} \psi^{I'}{}_{J'}(g^{-1} h) (R^{-1})^{J'}{}_J w^{J}(h) \\
			&= \sum_{h \in G} R^I{}_{I'} \delta^{I'}{}_{J'} \psi (R^{-1})^{J'}{}_J w^{J}(h) \\
			&= \sum_{h \in G} \psi \, w^{I}(h) = \sum_{h \in g\mathcal{D}} w^{I}(h), \label{eq:sum_pool_tensor}
		\end{aligned}
	\end{equation}
	which differs from sum pooling in the scalar case, Eq.~\eqref{eq:sum_pool_scalar}, only by the tensor index~$I$. We take this resemblance as motivation to define the general pooling operator~$P$ on tensor fields of type $(n,0)$, i.e.~\mbox{$w: G \rightarrow V$} with $V = (\mathbb R^D)^n$, as
	\begin{equation}
		w'^I(g) = (P w)^I(g) = \mathcal{P} (w(g d_1), \ldots, w(g d_N))^I,
	\end{equation}
	where the function $\mathcal P$ maps
	\mbox{$\mathcal{P}: V^N \rightarrow V$}.
	The pooling operation commutes with left translations if
	\begin{equation}
		\mathcal{P}(R w(d_1), \ldots, R w(d_N))^I = R^I{}_{I'} \mathcal{P}(w(d_1), \ldots, w(d_N))^{I'},
	\end{equation}
	which means that if the pooling operation commutes with the outer transformation~$R$, then it is \gequivariant{}. This is the case for sum pooling defined in Eq.~\eqref{eq:sum_pool_tensor}. Furthermore, if the norm~\mbox{$\lVert \cdot \rVert$} is unaffected by the outer transformation~$R$, then $G$-equivariance also holds for the max pooling operation
	\begin{equation}
		w'^I(g) = \max_{h \in g\mathcal{D}} w^I(h) = w^I \left( g' \right),
	\end{equation}
	where 
	\begin{equation}
		g' = \underset{{h \in g\mathcal{D}}}{\argmax} \, \lVert w(h) \rVert,
	\end{equation}
	corresponds to the element~$g'$ that maximizes the norm of $w$ in the translated pooling domain~$g\mathcal{D}$.
	\footnote{For simplicity, we assume that the maximum of the feature map is unique. In practical applications of G-CNNs and L-CNNs, it is highly unlikely to encounter cases where the maximum cannot be uniquely determined.} The above definitions for tensor feature maps $w:G \rightarrow V$ are consistent with the corresponding definitions for scalar feature maps  $ f: G \rightarrow \mathbb R $, where $V=\mathbb R$ and the rotation matrices $R$ reduce to the identity operation. 
	
	To generalize sum pooling of tensor feature maps to L-CNNs, we start with the L-CNN full tensor \gconv{}, which is given by Eq.~\eqref{eq:lgconv_tensor}, with \mbox{$H = G$}. We set the kernel~\mbox{$\psi^I{}_J(g) = \delta^I{}_J \psi(g)$}, where $\psi(g)$ is one inside the pooling domain and zero elsewhere. Steps analogous to Eq.~\eqref{eq:sum_pool_tensor} lead to
	\begin{equation}
		W^{\prime I}(g) = \sum_{h \in g\mathcal{D}} U_{q(g) \rightarrow q(h)} W^I(h) U_{q(h) \rightarrow q(g)} = \sum_{h \in g\mathcal{D}} W_g^I(h),
	\end{equation}
	where $W$ and $W'$ are the local variables before and after the pooling step, respectively, and
	\begin{equation}
		W_g^I(h) = U_{q(g) \rightarrow q(h)} W^I(h) U_{q(h) \rightarrow q(g)},
	\end{equation}
	denotes the variable~$W$, parallel transported from $h$ to $g$. Note that the fact that we only consider straight paths in the convolution restricts the shape of the pooling region~$\mathcal{D}$. In principle, however, it can be chosen arbitrarily as long as the paths $q(h) \rightarrow q(g)$ of the parallel transporters~$U_{q(h) \rightarrow q(g)}$ are chosen accordingly. A general pooling step, represented by the aforementioned pooling operator~$P$, on local tensor variables~$W$ can be written as
	\begin{equation}
		W'^I (g) = (PW)^I(g) = \mathcal{P} (W_g(g d_1), \ldots, W_g(g d_N))^I,
	\end{equation}
	where \mbox{$\mathcal{P}: V^N \rightarrow V$}. It is \gequivariant{} if the outer transformation commutes with the pooling operation
	\begin{equation}
		\mathcal{P} (R W_g(d_1), \ldots, R W_g(d_N))^I = R^I{}_{I'} \mathcal{P} (W_g(d_1), \ldots, W_g(d_N))^{I'}.
	\end{equation}
	Similarly, we require the pooling layer to be equivariant under lattice gauge transformations
	\begin{equation}
		\begin{aligned}
			T_\Omega (PW)^I(g) &= \Omega(\mx) (PW)^I(g) \Omega^\dagger(\mx) \overset{!}{=} (P T_\Omega W)^I(g),
		\end{aligned}
	\end{equation}
	which implies that the function~$\mathcal{P}$ must also satisfy
	\begin{equation}
		\mathcal{P} \big( \Omega W_g (d_1) \Omega^\dagger, \ldots, \Omega W_g (d_N) \Omega^\dagger \big)^I = \Omega \, \mathcal{P} \big( W_g (d_1), \ldots,  W_g (d_N) \big)^I \,  \Omega^\dagger.
	\end{equation}
	We omitted the argument~\mbox{$\mx = q(g) = q(xr)$} of \mbox{$\Omega = \Omega(\mx)$} for simplicity. For example, max pooling can be defined as
	\begin{equation}
		W^{\prime I}(g) = \max_{h \in g \mathcal{D}} W_g^{I}(h) = W_g^I(g'),
	\end{equation}
	with
	\begin{equation}
		g' = \underset{{h \in gD}}{\argmax} \, \big\lVert \Re \big( \Tr (W^I(h)) \big) \big\rVert,
	\end{equation}
	where the trace leads to the parallel transporters dropping out. Clearly, this form of max pooling satisfies both \gequivariance{} and equivariance under gauge transformations.
	
	As a concrete example of how these pooling layers can be used, consider a network that uses local tensor variables~$W^I$ on $\lat$ as input. In the first layer, the input feature maps are promoted to the group~\mbox{$G = \TT \rtimes K$} via a first-layer lattice \gconv{}. After that, the feature maps are convolved by full lattice \gconv{}s. Coset pooling over~$K$ can then be applied to reduce the domain of the feature maps from $G$ back to $\lat$ while retaining both gauge and global group symmetry. For example, if we use sum pooling, each resulting feature map reads
	\begin{equation}
		W'^I (g) =  \sum_{h \in gK} W^I(h),
	\end{equation}
	where the parallel transporters are trivial because the projections \mbox{$q(g) = q(h) = \mx$} coincide at the same point \mbox{$\mx \in \lat$}. The feature map is invariant in the sense of \mbox{$W'^I (xr) = W'^I (xr')$} for \mbox{$r, r' \in K$}. Subsampling in $K$ thus leads to a \gequivariant{} tensor $W'^I (\mx)$ on $\lat$. In this way, \gequivariant{} L-CNNs can be used to model gauge and group equivariant functions which map input feature maps on the lattice $\lat$ to new output feature maps on $\lat$.
	
	\subsection{Computational requirements of \texorpdfstring{\gequivariant{}}{G-equivariant} L-CNNs}
	
	Having defined the \gequivariant{} generalizations of relevant L-CNN layers, we need to comment on the computational resources required by these layers. There are two main differences to our original formulation of L-CNNs: the first concerns the domain on which feature maps are defined, and the second concerns the computational complexity of a \gconv{} layer.
	
	Regarding the first point, we recall that in standard L-CNNs, feature maps are functions on the lattice~$\lat$ (with periodic boundary conditions) consisting of \mbox{$N_\mathrm{ch} \cdot N_l^D$} complex matrices in each layer, where $N_\mathrm{ch}$ is the number of channels and $N_l^D$ is the total number of lattice sites on the hypercubic lattice. Depending on the dimensions of the lattice, our \gequivariant{} generalizations enlarge these domains considerably. After the first-layer \gconv{}, feature maps are promoted to functions on the global symmetry group~$G$. Since $G$ is a semi-direct product of the translation group~\mbox{$\TT \sim \lat$} and the stabilizer group~$K$, we may write the number of group elements as \mbox{$|G| = |K| N_l^D$}. Feature maps on $G$ may thus be represented by \mbox{$N_\mathrm{ch} \cdot |K| \cdot N_l^D$} complex matrices, which leads to an increased memory requirement by a factor of $|K|$ compared to the original L-CNN formulation. For example, if we consider rotoreflections in \mbox{$D=2$}, we have \mbox{$|K| = 4 \cdot 2 = 8$} group elements since there are four possible rotations and two mirror operations about the two axes. For \mbox{$D=3$}  and \mbox{$D=4$} we have \mbox{$|K| = 48$} (octahedral group) and \mbox{$|K| = 384$} (hyperoctahedral group), respectively. These factors are large, considering the fact that the models used in our original study~\cite{Favoni:2020reg} easily saturated the available memory on modern GPUs. For the physically relevant case of \mbox{$D=4$} it may be argued that the symmetry subgroup~$K$ may be smaller than the full hyperoctahedral group. In practice, one typically uses lattice sizes \mbox{$N_t \cdot N_l^3$}, where $N_t$ is the number of cells along the time directions with \mbox{$N_t \neq N_l$}, which reduces the symmetry. Thus, the group~$K$ should consist of rotations and reflections in the spatial directions (octahedral group) and reflections along the fourth axis only, which leads to \mbox{$|K| = 2 \cdot 48 = 96$}. Nevertheless, the memory footprint of \gequivariant{} L-CNNs is generically much larger than of their $\lat$-equivariant counterparts. 
	
	Additionally, one has to consider the computational complexity of a \gconv{} layer, i.e.~the number of operations required to evaluate such convolutions. Similar to traditional CNNs, the original L-Conv layer consists of a sum over the lattice~$\lat$. We typically restrict the kernel of the L-Conv to the lattice axes, such that in total \mbox{$D \cdot (N_k-1) + 1$} terms have to summed over to compute the result of the convolution. \gconv{}s extend this sum to run over the full symmetry group~$G$. Thus, the number of terms to consider is larger by a factor of $|K|$.
	
	\section{L-CNNs from a bundle theoretic viewpoint}\label{sec:bundles}
	
	Having laid out the details of how to extend L-CNNs to larger global symmetries in previous sections, we now focus on the mathematical foundations of gauge equivariant convolutional neural networks and how they relate to the translationally equivariant L-CNN. There is a mathematical theory of equivariant neural networks that uses fiber bundles to describe symmetries and to capture geometric information in data \cite{Cohen:2018eq,Aronsson:2022homogeneous}. This theoretical framework models data points as fields or, more generally, as sections of vector bundles associated to a principal bundle that specifies relevant symmetries. In Sections~\ref{sec:bundle_formalism}-\ref{sec:bundle_layers} we show that the original L-CNN \cite{Favoni:2020reg} is a discretization of a continuous model within this theory. Section~\ref{sec:bundle_generalized} looks at how the original L-CNN can be directly generalized to other representations, i.e.~beyond locally transforming matrices~$W(\mx)$ as input. Finally, in Section~\ref{sec:bundle_G-equivariance}, we discuss the possibility of placing fully group-equivariant L-CNNs into this theoretical framework.
	
	\subsection{Bundle formalism}\label{sec:bundle_formalism}
	
	Geometric deep learning, and equivariant neural networks in particular, uses fiber bundles because they allow nontrivial global geometries; fiber bundles generalize the (geometrically trivial) product \mbox{$\mathcal{M} \times F$} between two spaces $\mathcal{M}$ and $F$, respectively known as the base space and the characteristic fiber. We visualise this product as attaching a fiber~\mbox{$\{\mx\} \times F$} to each point \mbox{$\mx \in \mathcal{M}$} of the base space. A general fiber bundle is a collection of fibers
	\begin{equation}
		E = \bigcup_{\mx \in \mathcal{M}} F_\mx,
	\end{equation}
	where each fiber~\mbox{$F_\mx \simeq F$} in the total space~$E$ is equivalent to the characteristic fiber. There is also a projection~\mbox{$\pi : E \to \mathcal{M}$} that maps each element \mbox{$p \in F_\mx \subset E$} to the point~\mbox{$\pi(p) = \mx$} where its fiber is attached. Although the total space~$E$ can be a more complicated object than a trivial bundle \mbox{$\mathcal{M} \times F$}, it is required to look like a product \mbox{$\mathcal{D} \times F$} on certain local regions~\mbox{$\mathcal{D} \subset \mathcal{M}$}. A common example is the comparison between a Mbius strip and a cylinder~\mbox{$S^1 \times [0,1]$}, where $S^1$ is the circle. Locally, both of these objects look like segments~\mbox{$\mathcal{D} \times [0,1]$} for \mbox{$\mathcal{D} \subset S^1$} but they have different global geometry. The Mbius strip is thus a nontrivial fiber bundle.
	
	Principal bundles are fiber bundles such that the characteristic fiber is a group~$K$, often called the structure group. Since the total space consists of fibers~\mbox{$K_\mx \simeq K$}, the structure group acts as an internal degree of freedom at each point~\mbox{$\mx \in \mathcal{M}$} and principal bundles are therefore used to study local gauge symmetry. 
	
	Consider the trivial principal bundle on \mbox{$\mathcal{M} = \mathbb{R}^D$} with structure group~\mbox{$K = \SU(N)$},
	\begin{equation}
		\pi : \mathbb{R}^D \times \SU(N) \to \mathbb{R}^D, \qquad \pi(\mx,\Omega) = \mx,
	\end{equation}
	which we often refer to as \mbox{$P = \mathbb{R}^D \times \SU(N)$}. This bundle describes an $SU(N)$ gauge symmetry that is incorporated into fields over $\mathbb{R}^D$ in the following way: Let $V$ be a linear space whose elements transform according to a linear representation~$\rho$ of $SU(N)$,
	\begin{equation}\label{eq:gauge_transformation_rho}
		v \mapsto \rho(\Omega)v.
	\end{equation}
	Triples \mbox{$(\mx, \Omega, v)$} are interpreted as an element~\mbox{$v \in V$} located at the position \mbox{$\mx \in \mathbb{R}^D$} and expressed in the gauge~\mbox{$\Omega \in \SU(N)$}. The transformation in Eq.~\eqref{eq:gauge_transformation_rho} means that if we let the identity matrix~\mbox{$\mathbb 1 \in SU(N)$} represent an initial gauge, then a gauge transformation~\mbox{$\mathbb 1 \mapsto \Omega$} can be achieved by transforming \mbox{$v \mapsto \rho(\Omega)v$} instead, hence the triples \mbox{$(\mx,\Omega,v)$} and \mbox{$(\mx,\mathbb 1,\rho(\Omega)v)$} are gauge equivalent. We can remove the gauge degree of freedom by defining an equivalence relation
	\begin{equation}
		(\mx, \Omega, v) \sim (\mx, \mathbb 1, \rho(\Omega)v)
	\end{equation}
	and considering the set of equivalence classes \mbox{$E_\rho = P \times_\rho V = (P \times V)/\sim$}. The equivalence class
	\begin{equation}\label{eq:equiv_class}
		[\mx, \Omega, v] = [\mx, \mathbb 1, \rho(\Omega)v]
	\end{equation}
	is thus a gauge invariant expression for the element~\mbox{$v \in V$} at the position~\mbox{$\mx \in \mathbb{R}^D$}.
	
	By construction, the quotient space~$E_\rho$ is a so-called associated bundle
	\begin{equation}
		\pi_\rho : E_\rho \to \mathbb{R}^D, \qquad \pi_\rho ([\mx, \Omega, v]) = \mx,
	\end{equation}
	and such bundles are used extensively in the mathematical theory of equivariant neural networks. For example, the inputs to and outputs from an equivariant neural network are called data points and are defined as sections of associated bundles, i.e.~generalized fields~\mbox{$s : \mathbb{R}^D \to E_\rho$} of the form
	\begin{equation}\label{eq:data_point}
		s(\mx) = [\mx, \Omega, v(\mx)] = [\mx, \mathbb{1}, \rho(\Omega)v(\mx)],
	\end{equation}
	where $\Omega$ runs over all values of \mbox{$\Omega \in SU(N)$} by definition of the equivalence class. The idea behind this definition is that instead of using feature maps (which depend on the choice of gauge), we use gauge-invariant data points~$s$ as input data, which map from the base space into the class of equivalent triplets. Thus, one can consider $s(\mx)$ at some point~$\mx$ as a gauge invariant object and use data points to describe vector and tensor fields in a geometric (gauge and coordinate independent) way. In contrast, a feature map of locally transforming matrices~$W(\mx)$ is a single representative of the gauge-invariant data point~$s_W(\mx)$. Similarly, one may also consider gauge-dependent tensor fields~$W^I(\mx)$ as representatives of their respective data points.
	
	Each fiber~$E_\mx$ of the associated bundle~$E_\rho$ is a linear space with respect to linear combinations
	\begin{equation}
		\alpha [\mx, \Omega, v] + \alpha' [\mx, \Omega, v'] = [\mx, \Omega, \alpha v + \alpha' v'],
	\end{equation}
	for scalars~\mbox{$\alpha,\alpha'$} and \mbox{$v,v' \in V$}. We can therefore take pointwise linear combinations \mbox{$\alpha s(\mx) + \alpha' s'(\mx)$} of data points, making the set~$\Gamma(\rho)$ of all data points~\mbox{$s : \mathbb{R}^D \to E_\rho$} into a linear space.
	
	Layers in an equivariant neural network are maps
	\begin{equation}\label{eq:ENN_layer}
		\Phi : \Gamma(\rho_1) \to \Gamma(\rho_2),
	\end{equation}
	between the spaces of data points for two possibly different representations~\mbox{$(\rho_1,V_1)$} and \mbox{$(\rho_2,V_2)$} of the structure group~$SU(N)$. Layers can be either linear or nonlinear, and we say that $\Phi$ is gauge equivariant if it commutes with gauge transformations
	\begin{equation}\label{eq:gauge_transformation_of_s}
		\begin{aligned}
			T_{{\Omega}}s(\mx)
			&= T_{{\Omega}}[\mx,\tilde\Omega,v(\mx)]\\
			&= [\mx, \tilde\Omega {\Omega}(\mx), v(\mx)]\\
			&= [\mx, \tilde\Omega, \rho({\Omega}^\dagger(\mx))v(\mx)].
		\end{aligned}
	\end{equation}
	
	We define gauge equivariant neural networks as compositions of gauge equivariant layers such as Eq.~\eqref{eq:ENN_layer}. Note that this definition makes no mention of convolutional layers or translation equivariance, allowing it to be used even in the absence of global symmetry. Convolutional layers are one of possibly many different types of layers. Moreover, (non-linear) activation functions, or the composition of a linear transformation and an activation function, are also considered layers under this definition as there is no requirement of linearity in Eq.~\eqref{eq:ENN_layer}. 
	In the following, we will investigate how the original L-CNN relates to this theory.
	
	\subsection{Locally transforming variables}
	
	We claim that the locally transforming variables used in the original L-CNN are directly related to data points of the associated bundle~\mbox{$E_\Ad = P \times_\Ad \mathbb{C}^{N \times N}$}. Here, \mbox{$V = \mathbb{C}^{N \times N}$} is the linear space of complex \mbox{$N \times N$}-matrices and \mbox{$\rho = \Ad$} is the adjoint representation
	\begin{equation}
		\Ad(\Omega) : W \mapsto \Omega W \Omega^\dagger, \qquad W \in \mathbb{C}^{N \times N}.
	\end{equation}
	In order to investigate how the data points, given by Eq.~\eqref{eq:data_point}, are related to $W(\mx)$ for this bundle, we fix a (local) gauge
	\begin{equation}
		\omega : \mathcal{D} \to P, \qquad \mathcal{D} \subseteq \mathbb{R}^D,
	\end{equation}
	i.e.~a local section of the principal bundle~\mbox{$P = \mathbb{R}^D \times SU(N)$}. This principal bundle is trivial, and thus the gauge is given by \mbox{$\omega(\mx) = (\mx, g(\mx))$} for a unique function~\mbox{$g : \mathcal{D} \to SU(N)$}.
	If  \mbox{$s : \mathbb{R}^D \to E_\Ad$} is a data point, then the gauge selects a specific representative
	\begin{equation}\label{eq:representative_of_s_1}
		f(\mx) = (\mx, g(\mx), W(\mx)) \in P \times \mathbb{C}^{N \times N}
	\end{equation}
	of the equivalence class $s(\mx)$ for \mbox{$\mx \in \mathcal{D}$}. The triple in Eq.~\eqref{eq:representative_of_s_1} describes a matrix~\mbox{$W(\mx) \in \mathbb{C}^{N\times N}$}, placed at the position~$\mx \in \mathbb{R}^D$ and expressed in the gauge~\mbox{$g(\mx)$}. As our notation indicates, we argue that $W(\mx)$ is the matrix-valued, locally transforming variable used in the original L-CNN. 
	For the purpose of verifying the transformation behavior of $W(\mx)$ under gauge transformations, we fix a second gauge~\mbox{$\omega' : \mathcal{D}' \to P$}, given by \mbox{$\omega'(\mx) = (\mx , g'(\mx))$}, and use it to select a representative
	\begin{equation}\label{eq:representative_of_s_2}
		f'(\mx) = (\mx, g'(\mx), W'(\mx))
	\end{equation}
	of the equivalence class~\mbox{$s(\mx) \in E_\mx$} for \mbox{$\mx \in \mathcal{D}'$}. For each $\mx$ in the intersection~\mbox{$\mathcal{D} \cap \mathcal{D}'$}, Eqs. \eqref{eq:representative_of_s_1} and \eqref{eq:representative_of_s_2} select possibly different representatives~\mbox{$f(\mx) \sim f'(\mx)$} of the same equivalence class~$s(\mx)$ and must therefore be related by
	\begin{equation}
		(\mx, g'(\mx), W'(\mx)) = (\mx,g(\mx)\Omega^\dagger(x),\Omega(x) W(\mx) \Omega^\dagger(\mx)).
	\end{equation}
	Here, \mbox{$\Omega(\mx) = {g'}^\dagger(\mx) g(\mx)$} is the gauge transformation that transforms between $\omega$ and $\omega'$. This shows that the matrices~$W(\mx)$ exhibit the correct transformation behavior
	\begin{equation}
		W'(\mx) = \Omega(\mx) W(\mx) \Omega(\mx)^\dagger.
	\end{equation}
	
	Channels~\mbox{$a = 1,\ldots,m$} can be introduced by taking direct sums: The multi-channel variable
	\begin{equation}
		\mathcal{W}(\mx) = \big(W^1(\mx),\ldots,W^m(\mx)\big),
	\end{equation}
	transforms under \mbox{$\Ad \oplus \cdots \oplus \Ad$} and represents a data point~\mbox{$s_\mathcal{W}(\mx) = [\mx,\Omega,\mathcal{W}(\mx)]$} of the bundle
	\begin{equation}
		E_{\Ad \oplus \cdots \oplus \Ad} \simeq E_\Ad \oplus \cdots \oplus E_\Ad.
	\end{equation}
	Let us introduce the shorthand notations \mbox{$n \Ad = \bigoplus_{a=1}^n \Ad$} and \mbox{$\Ad^n = \bigotimes_{a=1}^n \Ad$}.
	
	\subsection{Equivariant layers}\label{sec:bundle_layers}
	
	The convolutional layer of Eq.~\eqref{eq:L-conv} can be viewed as a discretization of a continuous convolution
	\begin{equation}\label{eq:bundle_conv}
		[\psi \star \mathcal{W}]^a(\mx) = \sum_{b} \int_{\mathbb{R}^D} \, \dd \my^D \, \psi^{ab}(\my-\mx) U_{\mx\to \my} W^b(\my) U_{\mx \to \my}^\dagger,
	\end{equation}
	with kernel components~\mbox{$\psi^{ab} : \mathbb{R}^D \to \mathbb{R}$} that are non-zero only on the coordinate axes and
	\begin{equation}
		U_{\mx \to \my} = \mathcal{P} \exp{ i \intop^1_0 \diff s \der{x^\nu(s)}{s} A_\nu(x(s))  }
	\end{equation}
	is the parallel transporter along the straight line from $\mx$ to $\my$.
	
	As discussed in Section~\ref{subsec:L-CNN}, this design choice is due to parallel transport being path-dependent and the non-uniqueness of shortest paths on the lattice. This convolution is a linear transformation by virtue of being an integral operator, and it maps \mbox{$\mathcal{W} = (W^1,\ldots,W^m)$} to \mbox{$\mathcal{W}' = ({W'}^1,\ldots,{W'}^n)$} in a gauge equivariant manner:
	\begin{equation}
		[\psi \star T_\Omega \mathcal{W}]^a(\mx) = T_\Omega [\psi \star \mathcal{W}]^a(\mx).
	\end{equation}
	The action of Eq.~\eqref{eq:bundle_conv} on data points~\mbox{$s_\mathcal{W}(\mx) = [\mx, \Omega, \mathcal{W}(\mx)]$} is therefore well-defined and independent of the choice of representative for the equivalence class. Thus, the continuous convolution defines a gauge equivariant linear layer
	\begin{equation}
		\Phi_\mathrm{Conv} : \Gamma(m\Ad) \to \Gamma(n\Ad), \qquad s_\mathcal{W} \mapsto s_{\mathcal{W}'}.
	\end{equation}
	
	Analogously, the original bilinear layer in Eq.~\eqref{eq:L-Bilin} is a straightforward discretization of
	\begin{equation}\label{eq:L-Bilin_continuous}
		{W''}^a(\mx) = \sum_{b=1}^{m} \sum_{c=1}^{m'} \alpha^{abc} W^b(\mx) {W'}^c(\mx),
	\end{equation}
	and can be linearized using tensor products. If \mbox{$s_\mathcal{W} \in \Gamma(m\Ad)$} and \mbox{$s_{\mathcal{W}'} \in \Gamma(m'\Ad)$}, then
	\begin{equation}
		s_{\mathcal{W} \otimes \mathcal{W}'} \in \Gamma(m \Ad \otimes m'\Ad) = \Gamma(mm' \Ad^2).
	\end{equation}
	That is, \mbox{$\mathcal{W} \otimes \mathcal{W}' = (W^b \otimes {W'}^c)$}. If we let $n$ be the number of output channels, the transformation in Eq.~\eqref{eq:L-Bilin_continuous} defines a gauge equivariant linear layer
	\begin{equation}
		\Phi_\mathrm{Bilin} : \Gamma(mm' \Ad^2) \to \Gamma(n\Ad), \qquad
		s_{\mathcal{W} \otimes \mathcal{W}'} \mapsto s_{\mathcal{W}''} .
	\end{equation}
	
	For each channel, trace layers~\mbox{${W'}^a(\mx) = \Tr(W^a(\mx,\Omega))$} compute the trace along the \mbox{$N \times N$} matrix structure and transform under the trivial representation~\mbox{$\rho = \Id$}. Gauge equivariance with respect to the trivial representation is
	equivalent to gauge invariance, so trace layers are gauge equivariant linear layers
	\begin{equation}
		\Phi_\mathrm{Trace} : \Gamma(m \Ad) \to \Gamma(m\Id), \qquad s_\mathcal{W} \mapsto s_{\mathcal{W}'}
	\end{equation}
	
	Finally, the non-linear activation functions~\mbox{${W'}^a(\mx) = \nu^a(\mathcal{W}(\mx)) W^a(\mx)$} transform $\mathcal{W}(\mx)$ by scaling each locally transforming variable~$W^a(\mx)$ using a non-linear and gauge invariant function~$\nu^a$. Since these activation functions do not affect the transformation behavior of the input feature map~$\mathcal{W}$, they are gauge equivariant non-linear layers
	\begin{equation}
		\Phi_\mathrm{Act} : \Gamma(m\Ad) \to \Gamma(m\Ad), \qquad s_\mathcal{W} \mapsto s_{\mathcal{W}'}.
	\end{equation}
	
	\subsection{L-CNNs for data in other representations}\label{sec:bundle_generalized}
	
	The original L-CNN requires that (input) data take the form of $\mathbb{C}^{N\times N}$-valued matrix variables~$W(\mx)$. However, the connection to the bundle theory for equivariant neural networks makes this requirement relatively straightforward to generalize to functions~$f(\mx)$ taking values in a linear space~$V$, and which transform under gauge transformations as
	\begin{equation} \label{eq:gauge_trans_general_rep}
		f(\mx) \mapsto \rho(\Omega(\mx)) f(\mx).
	\end{equation}
	This more general L-CNN uses the same trivial principal bundle~\mbox{$P = \mathbb{R}^D \times \SU(N)$} with the same gauge links, only the data is more general. This allows us to also consider matter fields as input: for example, fields in the fundamental representation~$\mathbb C^N$ (e.g.~quark fields) transform according to
	\begin{equation}
		\rho(\Omega(\mx)) f(\mx) = \Omega(\mx) f(\mx),
	\end{equation}
	where $\Omega(\mx)$ is a $\mathbb C^{N \times N}$ matrix. Similarly, for fields in the adjoint representation (in the sense of adjoint fermions or bosons), we have
	\begin{equation}
		\left(\rho(\Omega(\mx)) f(\mx)\right)^a = \Omega(\mx)^{ab} f(\mx)^b,
	\end{equation}
	with color indices~\mbox{$a,b \in \{1, 2, \dots, N^2-1\}$} and the adjoint matrix
	\begin{equation}
		\Omega(\mx)^{ab} = 2 \mathrm{Tr} \left[ t^a \Omega(\mx)^\dagger t^b \Omega(\mx) \right].
	\end{equation}
	In these cases, we have an associated bundle~\mbox{$E_\rho = P \times_\rho V$} consisting of equivalence classes, Eq.~\eqref{eq:equiv_class}, and the locally transforming input data~$f(\mx)$ are gauge-dependent representatives of data points~\mbox{$s_f(\mx) = [\mx,\Omega,f(\mx)]$}. Allowing multiple channels,
	\begin{equation}
		\mathcal{F} = (f^1,\ldots,f^n),
	\end{equation}
	can be accomplished by using direct sum representations~\mbox{$n \rho = \bigoplus_{a=1}^n \rho$}.
	
	The convolution in Eq.~\eqref{eq:bundle_conv} generalizes to a gauge equivariant layer~\mbox{$\Phi : \Gamma(m\rho) \to \Gamma(n\rho)$} given~by
	\begin{equation}
		[\psi \star \mathcal{F}]^a(\mx,\Omega) = \sum_{b=1}^n \int_{\mathbb{R}^D} \dd \my^D \, \psi^{ab}(\my-\mx) \rho(U_{\mx \to \my}) f^b(\my,\Omega),
	\end{equation}
	where \mbox{$\psi^{ab} : \mathbb{R}^D \to \mathbb{R}$} are kernel components.
	
	Bilinear layers and trace layers do not generalize directly to data that, unlike $W(\mx)$, are not~matrix valued and must instead be tailored to different representations. If $f(\mx)$ is an SU($N$) vector field, for instance, trace layers could be defined as
	\begin{equation}
		{f'}^a(\mx) = \Tr( f^a(\mx){f^a}(\mx)^\dagger),
	\end{equation}
	which acts linearly on \mbox{$f^a(\mx) \otimes f^a(\mx)^\dagger$} and therefore defines a linear layer~\mbox{$\Gamma(m(\rho \otimes \rho^\dagger)) \to \Gamma(m{\Id})$}. This trace layer is gauge invariant and can be used to define activation functions as gauge equivariant non-linear layers~\mbox{$\Gamma(m\rho) \to\Gamma(m\rho)$} given by
	\begin{equation}
		{f'}^a(\mx) = \nu^a(w(\mx)) f^a(\mx),
	\end{equation}
	for each channel~\mbox{$a = 1,\ldots,m$}. Here, \mbox{$w^a(\mx) = \Re\left(\Tr(f^a(\mx)f^a(\mx)^\dagger)\right)$}.
	
	\subsection{The difficulty in achieving full group equivariance}\label{sec:bundle_G-equivariance}
	
	Now that we have connected the original L-CNN to the mathematical theory of equivariant neural networks, it is desirable to do the same for the fully \gequivariant{} L-CNN discussed in Section~\ref{sec:general}. Fully adhering to the existing theory would require us to identify a suitable principal bundle that describes both the global symmetry under $G$ as well as the local gauge symmetry~SU($N$). The complication with this is that gauge symmetry is described by the principal bundle~\mbox{$P = \mathbb{R}^D \times SU(N)$}, whereas global symmetry and group equivariance is more closely related to the principal bundle
	\begin{equation}\label{eq:G_bundle}
		q : G \to \mathbb R^D, \qquad q(g) = q(xr) = \mathbf{x},
	\end{equation}
	with structure group~\mbox{$K = G/\mathbb{T}$}. It is not obvious whether a single principal bundle can correctly describe both symmetries simultaneously. One interesting candidate is the principal bundle
	\begin{equation}\label{eq:G_SU_bundle}
		q : G \times SU(N) \to G, \qquad q(g,\Omega) = g.
	\end{equation}
	It is of the same general form \mbox{$q : \mathcal{G} \to \mathcal{G}/\mathcal{K}$} as Eq.~\eqref{eq:G_bundle} but with the total space~\mbox{$\mathcal{G} = G \times SU(N)$} and the structure group~\mbox{$\mathcal{K} = SU(N)$}. The corresponding data points are sections of associated bundles~\mbox{$E_\rho = \mathcal{G} \times_\rho V$} and are given by 
	\begin{equation}
		s(g) = [g, \Omega, f(g)] = [g, \mathbb{1}, \rho(\Omega)f(g)],
	\end{equation}
	for linear representations~$\rho$ of $SU(N)$. Gauge equivariance works similarly as for~\mbox{$P = \mathbb{R}^D \times SU(N)$}. However, this bundle describes group equivariance with respect to \mbox{$\mathcal{G} = G \times SU(N)$}, not with respect to $G$ alone. This means that $SU(N)$ would have to represent a global symmetry in addition to the local gauge symmetry. In particular, continuous convolutions would be integrals over \mbox{$G \times SU(N)$}. This is not compatible with the group equivariant L-CNN, so Eq.~\eqref{eq:G_SU_bundle} cannot be the correct principal bundle. The question, then, is whether there is a more appropriate principal bundle or if the bundle theory for equivariant neural networks can be appropriately broadened. We leave this question for future work.
	
	\section{Conclusions and outlook}
	
	In this work we have reviewed the L-CNN framework~\cite{Favoni:2020reg} from a geometrical perspective and extended the original formulation by accounting for additional global symmetries on the lattice. The L-CNN framework introduced a set of gauge equivariant layers which can be used to build machine learning models for performing computations on gauge link configurations $\{ U_{x,\mu} \}$. These layers, consisting of convolutions, bilinear operations, activation functions and trace layers, are equivariant under lattice gauge transformations. This is achieved by accounting for parallel transport in the definition of gauge equivariant convolutions. In addition, L-CNNs, which are fundamentally based on discrete convolutions on the lattice~$\lat$, are also equivariant under global translations of the input data.
	
	The global symmetry group~$G$ on a hypercubic lattice consists not only of translations, but also includes discrete rotations and reflections. One of the drawbacks of L-CNNs is that they only respect the translational part of the full global symmetry. To remedy this, we have revisited \nobrhyph{G}{CNNs}~\cite{Cohen:2016aaa}, which use convolutional layers compatible with general global symmetry transformations. We have first reviewed how to use these \gconv{}s on vector- and tensor-valued data and then combined the G-CNN approach with our L-CNN framework to obtain network architectures that are not only gauge equivariant, but also equivariant under the full global symmetry group on the lattice. There is a computational drawback associated with this extension: the domain on which feature maps are defined must be enlarged to the full symmetry group~$G$, requiring more computational memory. Similarly, it increases the computational complexity of convolutions, as they have to be carried out over feature maps on the group.
	
	Finally, we have linked L-CNNs to the fiber bundle theoretic description of equivariant neural networks~\cite{Aronsson:2022homogeneous} for the $\lat$-equivariant case.  This has allowed us to determine the associated bundles used for input data in the original L-CNN formulation and also consider more general input data for different representations of the gauge group. More generally, we have shown how L-CNNs~can be understood as a special discretized case of gauge equivariant neural networks on fiber bundles~\cite{Gerken:2021sla}. Despite this, a bundle description of \gequivariant{} L-CNNs is still lacking. Identifying the correct principal bundle to simultaneously describe both global $G$ and local SU($N$) symmetry would be a welcome extension of this work.
	
	\begin{acknowledgments}
		
		The authors thank Andreas Ipp for many helpful discussions regarding group equivariant neural networks and comments on the manuscript. DM and DS have been supported by the Austrian Science Fund FWF No.~P32446, No.~P34764 and No.~P34455. DM acknowledges additional support from FWF No.~P28352. JA has been supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.
		DM and JA thank the organizers of the Banach Center  Oberwolfach Graduate Seminar ``Mathematics of Deep Learning'', which took place in late 2019 and sparked this collaboration.
		
	\end{acknowledgments}
	
	\bibliographystyle{elsarticle-num}
	\bibliography{references}
	
\end{document}