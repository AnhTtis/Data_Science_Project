{
    "arxiv_id": "2303.14588",
    "paper_title": "Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text Diacritization",
    "authors": [
        "Bashar Al-Rfooh",
        "Gheith Abandah",
        "Rami Al-Rfou"
    ],
    "submission_date": "2023-03-25",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "Most of previous work on learning diacritization of the Arabic language relied on training models from scratch. In this paper, we investigate how to leverage pre-trained language models to learn diacritization. We finetune token-free pre-trained multilingual models (ByT5) to learn to predict and insert missing diacritics in Arabic text, a complex task that requires understanding the sentence semantics and the morphological structure of the tokens. We show that we can achieve state-of-the-art on the diacritization task with minimal amount of training and no feature engineering, reducing WER by 40%. We release our finetuned models for the greater benefit of the researchers in the community.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14588v1"
    ],
    "publication_venue": null
}