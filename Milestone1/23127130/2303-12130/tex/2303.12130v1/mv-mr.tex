%% arara directives
% arara: xelatex
% arara: bibtex
% arara: xelatex
% arara: xelatex

%\documentclass{article} % One-column default
\documentclass[twocolumn, switch]{article} % Method A for two-column formatting

\usepackage{preprint}

%% Math packages
\usepackage{amsmath, amsthm, amssymb, amsfonts}

%% Bibliography options
\usepackage[numbers,square]{natbib}
\bibliographystyle{unsrtnat}
%\usepackage{natbib}
%\bibliographystyle{Geology}

%% General packages
\usepackage[utf8]{inputenc}	% allow utf-8 input
\usepackage[T1]{fontenc}	% use 8-bit T1 fonts
\usepackage{xcolor}		% colors for hyperlinks
\usepackage[colorlinks = true,
            linkcolor = purple,
            urlcolor  = blue,
            citecolor = cyan,
            anchorcolor = black]{hyperref}	% Color links to references, figures, etc.
\usepackage{booktabs} 		% professional-quality tables
\usepackage{nicefrac}		% compact symbols for 1/2, etc.
\usepackage{microtype}		% microtypography
\usepackage{lineno}		% Line numbers
\usepackage{float}			% Allows for figures within multicol
%\usepackage{multicol}		% Multiple columns (Method B)

\usepackage{lipsum}		%  Filler text

 %% Special figure caption options
\usepackage{newfloat}
\DeclareFloatingEnvironment[name={Supplementary Figure}]{suppfigure}
\usepackage{sidecap}
\sidecaptionvpos{figure}{c}

% Section title spacing  options
\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsection{0pt}{10pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsubsection{0pt}{8pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}

% ORCiD insertion
\usepackage{tikz,xcolor,hyperref}

\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
	\begin{tikzpicture}
	\draw[lime, fill=lime] (0,0) 
	circle [radius=0.16] 
	node[white] {{\fontfamily{qag}\selectfont \tiny ID}};
	\draw[white, fill=white] (-0.0625,0.095) 
	circle [radius=0.007];
	\end{tikzpicture}
	\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}
			{\noexpand\orcidicon}}
}


\usepackage{adjustbox}
\usepackage{multirow}

% Define the ORCID iD command for each author separately. Here done for two authors.
% \newcommand{\orcidauthorA}{0000-0000-0000-0001}
% \newcommand{\orcidauthorB}{0000-0000-0000-0002}
% \newcommand{\orcidauthorC}{0000-0000-0000-0003}
% \newcommand{\orcidauthorD}{0000-0000-0000-0004}


% commands
\newcommand{\papername}{MV-MR}

%%%%%%%%%%%%%%%%   Title   %%%%%%%%%%%%%%%%
\title{\papername: multi-views and multi-representations for self-supervised learning and knowledge distillation}

% Add watermark with submission status
% \usepackage{xwatermark}
% Left watermark
% \newwatermark[firstpage,color=gray!60,angle=90,scale=0.32, xpos=-4.05in,ypos=0]{\href{https://doi.org/}{\color{gray}{Publication doi}}}
% Right watermark
% \newwatermark[firstpage,color=gray!60,angle=90,scale=0.32, xpos=3.9in,ypos=0]{\href{https://doi.org/}{\color{gray}{Preprint doi}}}
% Bottom watermark
% \newwatermark[firstpage,color=gray!90,angle=0,scale=0.28, xpos=0in,ypos=-5in]{*correspondence: \texttt{vitaliy.kinakh@unige.ch}}

%%%%%%%%%%%%%%%  Author list  %%%%%%%%%%%%%%%
\usepackage{authblk}
\renewcommand*{\Authfont}{\bfseries}
\author{Vitaliy Kinakh}
\author{Mariia Drozdova}
\author{Slava Voloshynovskiy}

\affil{Department of Computer Science, University of Geneva}
\affil{\{ \textit{vitaliy.kinakh, mariia.drozdova, svolos}\} \textit{@unige.ch}}
% \affil[2]{Department of Biology, University Y}

% Option 2 for author list
%\author{
%  David S.~Hippocampus\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
%  %% examples of more authors
%   \And
% Elias D.~Striatum \\
%  Department of Electrical Engineering\\
%  Mount-Sheikh University\\
%  Santa Narimana, Levand \\
%  \texttt{stariate@ee.mount-sheikh.edu} \\
%  \AND
%  Coauthor \\
%  Affiliation \\
%  Address \\
%  \texttt{email} \\
%  % etc.
%}

%%%%%%%%%%%%%%    Front matter    %%%%%%%%%%%%%%
\begin{document}

\twocolumn[ % Method A for two-column formatting
  \begin{@twocolumnfalse} % Method A for two-column formatting
  
\maketitle

\begin{abstract}
We present a new method of self-supervised learning and knowledge distillation based on the multi-views and multi-representations (\papername). The {\papername} is based on the maximization of dependence between learnable embeddings from augmented and non-augmented views, jointly with the maximization of dependence between learnable embeddings from augmented view and multiple non-learnable representations from non-augmented view. We show that the proposed method can be used for efficient self-supervised classification and model-agnostic knowledge distillation. Unlike other self-supervised techniques, our approach does not use any contrastive learning, clustering, or stop gradients. {\papername} is a generic framework allowing the incorporation of constraints on the learnable embeddings via the usage of image multi-representations as regularizers. Along this line, knowledge distillation is considered a particular case of such a regularization. {\papername} provides the state-of-the-art performance on the STL10 and ImageNet-1K datasets among non-contrastive and clustering-free methods. We show that a lower complexity ResNet50 model pretrained using proposed knowledge distillation based on the CLIP ViT model achieves state-of-the-art performance on STL10 linear evaluation.
The code is available at: \href{https://github.com/vkinakh/mv-mr}{github.com/vkinakh/mv-mr}
\end{abstract}
%\keywords{First keyword \and Second keyword \and More} % (optional)
\vspace{0.35cm}

  \end{@twocolumnfalse} % Method A for two-column formatting
] % Method A for two-column formatting

%\begin{multicols}{2} % Method B for two-column formatting (doesn't play well with line numbers), comment out if using method A


%%%%%%%%%%%%%%%  Main text   %%%%%%%%%%%%%%%
% \linenumbers

\section{Introduction}
\label{sec:intro}
Self-supervised learning (SSL) methods are alternatives to supervised ones. In recent years, the gap between SSL and supervised methods decreased in the downstream tasks that include image classification \cite{zhou2021ibot},  object detection \cite{huang2022survey} and semantic image segmentation \cite{zheng2021hierarchical}, \cite{punn2022bt}. A general idea behind the SSL models for image classification is to train an embedding network, often called an {\em encoder}, on the unlabeled dataset and then to use this pretrained encoder for the downstream tasks either by training a classifier on top of the frozen embeddings or by fine-tuning the whole model.  The general goal is to ensure the invariance of embeddings to different inputs known as {\em augmentations} or {\em views}. However, this approach might lead to trivial solutions when two branches of encoders produce the same output. This might result in a {\em collapse} in training. Therefore, there have been a lot of recent works tackling this issue by regularizing the networks to avoid such a {\em collapse}. To prevent these negative effects, there exist several major approaches that use different tricks: (a) direct mutual information maximization between the input image $\bf x$ and the embedding $\bf z$ like in  InfoNCE \cite{oord2018representation} and associated contrastive loss with positive-negative pairs that are very costly in practice, (b) methods that avoid the collapse by introducing different asymmetries into two branches at the training stage, like training one network with gradient descent and updating the other one with an {\em exponential moving average} of the weights from the first one \cite{chen2021exploring} or introducing regularizers on the learned representations such as {\em decorrelation regularization} on the dimensions of the embeddings \cite{zbontar2021barlow} etc.

The proposed approach avoids the collapse by introducing the dependence maximization between trainable embeddings and hand-crafted features using distance correlation\cite{szekely2007measuring}. Distance correlation, unlike other losses in latent space, allows computing dependencies between  feature vectors of different shapes. We maximize the dependence between different embeddings while preserving the variance in them. We show that variance preservation maximizes the entropy of embeddings that makes them unique and distinguishable. Our approach is different from InfoNCE\cite{oord2018representation}, which advocates a contrastive loss, that maximizes the mutual information (MI) between input image $\bf x$ and its embeddings $\bf z$. In contrast to the InfoNCE, our approach is not contrastive, does not require large batch sizes, and has multiple advantages in practice while dealing with large images. It is also different from methods such as Barlow Twins \cite{zbontar2021barlow} and VICReg \cite{bardes2021vicreg} since we do not explicitly minimize the dependencies between the components within the embedding.

We also show that the proposed approach can be used for efficient model and latent space-agnostic knowledge distillation. The approach is based on the dependence maximization between the embeddings of the target trainable encoder, represented by the ResNet50\cite{he2016deep}, and the embeddings of the pretrained encoder, represented by the CLIP\cite{radford2021learning} based on ViT-B-16\cite{dosovitskiy2020vit}. Since the distance correlation is agnostic to the latent space shape, any pretrained encoder with any latent space can be used for knowledge distillation. To our best knowledge, we propose the first method that allows for knowledge distillation for models with the latent space of different shapes without any labels.

The main goal behind MV-MR is twofold: (i) maximizing the invariance of embeddings, i.e., maximizing the proximity of embeddings for the same image observed under different views, and (ii) maximizing the amount of information in each embedding, i.e., maximizing the variability of the embedding. Furthermore, to avoid the collapse at training, we regularize the branch with the augmentations by imposing the dependence constraints on a set of representations extracted from various encodings.

% hand-crafted, i.e., non-trainable, encodings.
% {\bf We propose a novel framework that allows for the regularization of SSL methods and knowledge distillation by utilizing the distance correlation as a measure of dependence between the trainable embeddings and other features of different natures: other trainable embeddings, hand-crafted features, images, or their augmented views. By selecting the features, we are able to improve the classification accuracy, improve training stability and avoid collapse in the training.}

The novelty behind the proposed approach can be summarized as follows: (i) we introduce a novel SSL approach that avoids collapse thanks to an additional regularization term that maximizes the dependence between trainable embeddings and feature vectors using distance correlation; (ii) up to our best knowledge, the proposed method is among the first that uses the dependence maximization of the latent space based on distance correlation for SSL; (iii) the proposed method is agnostic to the latent space shape, thus can be used with any type of features; (iv) we introduce a novel knowledge distillation technique, that is agnostic to model  and shape of latent space; (v) we demonstrate the state-of-the-art classification results on STL10 (89.71\%) and ImageNet-1K (74.5\%) datasets using linear evaluation protocol for noncontrastive SSL methods; (vi) we provide the information-theoretic explanation of the proposed method that contributes to the explainable ML; (vii) we demonstrate how the complex CLIP model with 86.2M of parameters trained on 400M text-image pairs can be distilled to a ResNet50 model with just 23.5M parameters trained on 1.3M images of ImageNet-1K. 

 We have three terms in our objective function: 
(a) the first term $\mathcal{L}_1$ consists of the mean square error (MSE) loss between the embeddings from non-augmented view and augmented views of the same image, which is used for the invariance of embeddings, and additional variation terms that are used for maximization of the variability of the embeddings\footnote{We demonstrate that this term originates from an upper bound on mutual information between these embeddings under corresponding assumptions.}; (b) the second term $\mathcal{L}_{2}$ stands for the distance correlation between the embeddings from the augmented and non-augmented views that complements the first term to capture non-linear relations between the embeddings that go beyond the second order statistics and (c) the third term $\mathcal{L}_{3}$ corresponds to the distance correlation between the embeddings from augmented view and multiple non-learnable image representations. As the non-learnable or hand-crafted representations, we have studied various techniques of invariant data representation that are well-known in computer vision and image processing applications. The studied hand-crafted features include but are not limited by: ScatNet \cite{oyallon2018scattering} features, local standard deviation (LSD) based \cite{narendra1981real} filters, and histograms of oriented gradients (HOG) \cite{dalal2005histograms}. Besides, to demonstrate the flexibility of the proposed method, we have also considered random augmentations of the original images flattened into feature vectors as instances of hand-crafted features.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{images/Fig1.pdf}
    \caption{
    % \papername: SSL architecture with variance, invariance regularization, and regularization on the latent representation by dependence maximization between obtained embeddings and hand-crafted features. 
    \papername: proposed SSL approach. 
    % The system works with a single augmented view of an image ${\bf x}_i, 1 \leq i \leq B$, from a batch of $B$ images. 
    Two {\em views} of the image are produced: one original and the other augmented by $q_{\phi_{\mathrm{t}}}(\tilde{\mathbf{x}}| \mathbf{x})$, then these views are encoded via encoder $q_{\phi_{\mathbf{z}}}(\mathbf{z}|\mathbf{x})$, producing ${\bf z}_i$ that denotes an original embedding and $\tilde{\mathbf{z}}_{i}$ denoting an augmented one. The representations $\mathbf{z}_{i, k}^{*}$ are obtained via via $K$ hand-crafted feature extraction mappers $q_{\phi_{z_{k}^{*}}}\left(\mathbf{z}_{k}^{*} | \mathbf{x}\right), 1\leq k \leq K$. The same process is applied to each image ${\bf x}_i$ in the batch $1 \leq i \leq B$.
    % This image is encoded directly via an encoder $q_{\phi_{\mathbf{z}}}(\mathbf{z}|\mathbf{x})$  producing the embedding ${\bf z}_i$, via the augmentation channel $q_{\phi_{\mathrm{t}}}(\tilde{\mathbf{x}}| \mathbf{x})$ producing an augmented view $\tilde{\mathbf{x}}_{i}$ and then by the same expander resulting in an augmented embedding $\tilde{\mathbf{z}}_{i}$ and generally via $K$ hand-crafted feature extraction mappers $q_{\phi_{z_{k}^{*}}}\left(\mathbf{z}_{k}^{*} | \mathbf{x}\right), 1\leq k \leq K$ producing $\mathbf{z}_{i, k}^{*}$ representations for each image from the batch $B$.
    % We consider these mappers as deterministic ones, i.e., $q_{\phi_{z_{k}^{*}}}(\textbf{z}_{k}^{*}|\textbf{x}) = \delta \left (\mathbf{z}_{k}^{*} - f_{\phi_{z_k^{*}}}(\mathbf{x}) \right )$.
    The embedding is regularized by a loss $\mathcal{L}_1(\phi_{\mathrm{z}})$ minimizing the Euclidean distances between the embeddings ${\bf z}_i$ and $\tilde{\mathbf{z}}_{i}$ while ensuring that their variance is above a threshold. The loss $\mathcal{L}_2(\phi_{\mathrm{z}})$ ensures the dependence between the pair of augmented and non-augmented embeddings using the distance correlation. The regularization loss $\mathcal{L}_3(\phi_{\mathrm{z}})$ is imposed by maximizing the distance correlation between the augmented embedding $\tilde{\mathbf{z}}_{i}$ and a set of hand-crafted features $\mathbf{z}_{i, k}^{*}, 1 \leq k \leq K$ computed for the given batch $B$.
    %that is denoted as a loss $\mathcal{L}_3(\phi_{\mathrm{z}})$. 
    %The encoders are implemented as deterministic mappers $q_{\phi_{\textbf{z}}}({\textbf{z}|\textbf{x}}) = \delta \left ( \mathbf{z} - f_{\phi_{z}} (\mathbf{x}) \right )$ based on ResNet50 \cite{he2016deep} backbones with output dimension 2048. The projectors have 3 fully-connected layers of size 8192.
    }
    \label{fig:d_cor_ssl}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{images/Fig_MVMR_distillation.pdf}
    \caption{\papername: distillation approach. $q_{\phi_{z^{*}}}(\textbf{z}^{*}|\textbf{x})$ is the complex teacher model used as a feature extractor in order to train a light student model $q_{\phi_{z}}\left(\mathbf{z} | \mathbf{x}\right)$.}
    \label{fig:mv_mr_distill}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\papername: motivation and intuition}
{\papername} pretraining and distillation schemes are schematically shown in Figures \ref{fig:d_cor_ssl}, \ref{fig:mv_mr_distill}. The dimensions of embeddings with and without augmentations are the same, i.e., $\tilde{\bf{z}} \in \mathbb{R}^D$ and ${\bf z} \in \mathbb{R}^D$. These embeddings are extracted from the augmented $\tilde{\bf x}$ and non-augmented $\bf{x}$ via a generalized embedder  $q_{\phi_{z}}(\cdot | \cdot)$ that can be deterministic or stochastic. A $k^{th}$  hand-crafted descriptor ${\bf{z}}_k^*$, with $k\in\{1,2,\cdots, K
\}$ and $K$ stands for the total number of hand-crafted descriptors, is generally a tensor of dimensions $H_k \times W_k \times C_k$ and is flattened to $D_k = H_k  W_k C_k$. This description is generally obtained via deterministic assignment  ${\mathbf{z}_{k}^{*}} = f_{\phi_{z_{k}^{*}}}({\bf x})$ or sometimes stochastic mapping $ {\bf Z}^*_k \sim q_{\phi_{z_{k}^{*}}}\left(\mathbf{z}_{k}^{*}| \mathbf{x}\right)$.

\subsection{Motivation: regularisation in self-supervised representation learning}
The learned representation should contain the informative representation of data with lower dimensionality and should be invariant under some transformations, i.e., to ensure the same latent representation for the data from the same sample passed through certain transformations. The satisfaction of these conflicting requirements in practice is not a trivial task. Many state-of-the-art SSL techniques try to find a reasonable compromise between these requirements and practical feasibility solely in the scope of machine learning formulation by imposing certain constraints on the properties of the learned representation via the optimization of encoder parameters under augmentations. 

At the same time, there exists a rich body of achievements in the computer vision community in the domain of the hand-crafted design of robust, invariant yet discriminating data representations\cite{loew2004distinctive}\cite{rublee2011orb}\cite{pietikainen2015two}\cite{dalal2005histograms}. Generally, the computer vision descriptors are very rich in terms of targeted invariant features and quite efficient in terms of computation. However, to our best knowledge, such descriptors are not yet fully integrated into the development of SSL techniques. {\em Therefore, one of the objectives of this paper is to propose a framework where the SSL representation learning might be coupled with the constraints on the embedding space offered by the invariant computer vision representations}. Our objective is not to consider a case-by-case approach on how to couple SSL with a particular computer vision representation but instead to propose a generic approach where any form of desirable computer vision representation can be integrated into the SSL optimization problem in an easy and tractable way. This ensures that the learned representation possesses the targeted properties inherited from the used computer vision descriptors. Furthermore, features extracted by such descriptors might be considered as a form of invariant data representations which is one of the desired properties of trained encoders. Thus, maximizing the dependence between the trainable embedding and such representation might be a novel form of regularization leading to the increased invariance yet collapse-avoiding technique. Since a single computer vision descriptor might not capture all desirable properties and have different representation formats, the targeted framework should be flexible enough to deal uniformly with all these descriptors within a simple optimization problem.

In summary, our motivation is to include regularization constraints on the solution by borrowing some inherent computer vision feature invariance to certain transformations. In this way, we will target learning the low dimensional embedding which contains only essential information about the data that might be of interest for the targeted downstream task and where all information about the augmentations is excluded.
\raggedbottom

\subsection{Intuition}
The basic idea behind {\papername} is to introduce constraints on the invariance of embedding via a new loss function. Our overall objective is to maximize the mutual information  $I(\tilde{\bf Z}; {\bf Z} )$ between the augmented embedding $\tilde{\bf Z}$ and the embedding without the augmentation  $\bf Z$ and mutual information $I(\tilde{\bf Z}; {\bf Z}_k^* )$ between $\tilde{\bf Z}$  and some invariant feature ${\bf Z}_k^* $ extracted from $\bf X$ using a mapper ensuring a known invariance to the desired transformation. 

\subsubsection{Measuring dependencies between embeddings of non-augmented and augmented data}
\label{upper_bound_MI}

{\bf Upper bound on mutual information}: In the first case, one can decompose the mutual information as:
\begin{equation}
    \begin{aligned}
     I(\tilde{\bf Z} ; {\bf Z}) &= \mathbb{E}_{p(\tilde{\bf z},{\bf z})} \left[{\log \frac{p({\tilde{\bf z}}, {\bf z})}{p(\tilde{\bf z})p({\bf z})}}\right] &= \mathbb{E}_{p(\tilde{\bf z},{\bf z})} \left[{\log \frac{p({\tilde{\bf z}}| {\bf z})}{p(\tilde{\bf z})}}\right] \\
     &= h(\tilde{\bf Z}) - h(\tilde{\bf Z}|{\bf Z}).
    \label{equation:MI_defintion}
     \end{aligned}
\end{equation}

 The maximization of this mutual information is equivalent to the maximization of the differential entropy $h(\tilde{\bf Z}) = - \mathbb{E}_{p(\tilde{\bf z})} \left [ \log {p(\tilde{\bf z})}\right]$ and the minimization of conditional differential entropy $h(\tilde{\bf Z}|{\bf Z}) = -  \mathbb{E}_{p(\tilde{\bf z},{\bf z})} \left[{\log p({\tilde{\bf z}}| {\bf z})}\right]$
\footnote{We assume that the differential entropy is non-negative under the considered settings.}.
% Since the computation of the marginal distribution $p(\tilde{\bf z})$ and conditional distribution $p({\tilde{\bf z}}| {\bf z})$ is difficult in practice and to avoid the computation of contrastive loss as in InfoNCE\cite{oord2018representation}, we will proceed with the upper bounds on these terms. 
Since the computation of the marginal distribution $p(\tilde{\bf z})$ and conditional distribution $p({\tilde{\bf z}}| {\bf z})$  is difficult in practice, we will proceed with bounding these terms. We assume that the desired embeddings need to be bounded by some variance $\sigma_z^2$ to avoid a training collapse when the encoders produce constant and non-informative vectors so that the entropy maximizing distribution for the first entropy term is the Gaussian one, i.e., $p(\tilde{\bf z}) \propto 1/C_0 \text{exp}(-\beta_z{\bf C}_{\tilde{\mathrm{z}}\tilde{\mathrm{z}}})$, where $C_0$ stands for the normalization constant, $\beta_z$ denotes scaling and ${\bf C}_{\tilde{\mathrm{z}}\tilde{\mathrm{z}}} = \sigma_z^2{\bf I}_{D}$, where ${\bf I}_{D}$ denotes the identity matrix of dimension $D \times D$. The conditional entropy $h(\tilde{\bf Z}|{\bf Z})$ is minimized when the embedding $\tilde{\bf Z}$ contains as much as possible information about ${\bf Z}$, i.e., when two vectors are dependent. Assuming that $p(\tilde{{\bf z}}| {\bf z}) \propto 1/C_1 \text{exp}(-\beta_{zz} d(\tilde{{\bf z}}, {\bf z}))$, where $d(\tilde{{\bf z}}, {\bf z})$ denotes some distance between two vectors such as $\ell_2$-norm for the Gaussian distribution or $\ell_1$-norm for Laplacian one, the minimization of the conditional entropy $h(\tilde{\bf Z}|{\bf Z})$ reduces to the minimization of the distance $d(\tilde{{\bf z}}, {\bf z})$.

{\bf Distance covariance}: Another way to measure the dependency between the data is based on distance covariance proposed by \cite{szekely2007measuring}. In the general case of dependence between the data, the distance covariance is non-invariant to strictly monotonic transformations, unlike mutual information. Nevertheless, the distance covariance has several attractive properties: (i) it can be efficiently computed for two vectors that have generally different dimensions $\tilde{\bf z} \in \mathbb{R}^D$ and ${\bf z} \in \mathbb{R}^{D'}$ and (ii) it is easier to compute in practice in contrast to the mutual information. Additionally, the distance covariance captures higher-order dependencies between the data in contrast to the Pearson correlation.
The distance covariance  $ \operatorname{dCov}^2(\tilde{\mathbf{Z}}, \mathbf{Z})$
%$\mathrm{dCov}^{2}({\tilde Z},Z) = \left\|\phi_{ {\tilde Z};Z}(t, s)-\phi_{\tilde Z}(t) \phi_{Z}(s)\right\|_w^2$
measures the distance between the joint characteristic function  $\varphi_{\mathbf{Z}, \tilde{\mathbf{Z}}}(\mathbf{t}, \mathbf{u})$ and the product of the marginal characteristic functions $\varphi_{\mathbf{Z}}(\mathbf{t}) \varphi_{\tilde{\mathbf{Z}}}(\mathbf{u})$\cite{szekely2007measuring}. This definition has a lot of similarities to the mutual information (\ref{equation:MI_defintion}) which measures the ratio between the joint distribution $p({\tilde{\bf z}}, {\bf z})$ and the product of marginals $p(\tilde{\bf z})p({\bf z})$.
Since $\varphi_{\mathbf{Z}, \tilde{\mathbf{Z}}}(\mathbf{t}, \mathbf{u}) = \varphi_{\mathbf{Z}}(\mathbf{t}) \varphi_{\tilde{\mathbf{Z}}}(\mathbf{u})$ when $\tilde{\mathbf{Z}}$ and $\mathbf{Z}$ are independent random vectors, then the distance covariance is equal to zero.

In the following, we will proceed with the normalized version of distance covariance known as {\em distance correlation} defined as: 
\begin{equation}
    \label{equation:d_corr}
 \operatorname{dCor}(\tilde{\textbf{Z}}, \textbf{Z})=\frac{\mathrm{dCov}^{2}(\tilde{\textbf{Z}},\textbf{Z})}{\sqrt{\mathrm{dVar}(\tilde{\textbf{Z}}) \mathrm{dVar}(\textbf{Z})}},
\end{equation}
where $\textbf{Z} = [\textbf{z}_{1}, ..., \textbf{z}_{B}]$ denotes a batch of size $B$ of embeddings from original views,  $\tilde{\textbf{Z}} = [\tilde{\textbf{z}}_{1}, ..., \tilde{\textbf{z}}_{B}]$ stands for a batch of embeddings from augmented views and:
\begin{equation}
    \label{equation:dcov}
    \mathrm{dCov}_{B}^{2}(\tilde{\textbf{Z}}, \textbf{Z}):=\frac{1}{B^{2}} \sum_{j=1}^{B}\sum_{i=1}^{B} A_{j, i} C_{j, i}.
\end{equation}

Note that $\mathrm{dVar}(\textbf{Z}) = \mathrm{dCov}_{B}^{2}({\textbf{Z}}, \textbf{Z})$. In equation (\ref{equation:dcov}), we use the notations $A_{j, i}:=a_{j, i}-\bar{a}_{j \cdot}-\bar{a}_{\cdot i}+\bar{a}_{\cdot \cdot}, \quad$ $C_{j, i}:=c_{j,i}-\bar{c}_{j \cdot}-\bar{c}_{\cdot i}+\bar{c}_{\cdot \cdot}$,  where $\quad a_{j, i} =\left\|\tilde{\textbf{Z}}_{j}-\tilde{\textbf{Z}}_{i}\right\| , \quad$ $c_{j, i} =\left\|\textbf{Z}_{j}-\textbf{Z}_{i}\right\|$, where $j, i=1,2, \ldots, B$.

\subsubsection{Dependence between embeddings of augmented data and multiple hand-crafted representations}

The second mutual information $I(\tilde{\bf Z} ; {\bf Z}_k^* )$ between $\tilde{\bf Z}$  and some invariant feature ${\bf Z}_k^* $ deals with the vectors of different dimensions. Thus, one can either map these vectors to the same dimension and apply the above arguments, use Hilbert-Schmidt proxy \cite{NIPS2007_d5cfead9}, or proceed with the distance correlation dependence measure for the uniformity of consideration. We focus on the distance correlation case due to its property of handling vectors of different dimensions and its ability to capture higher-order data statistics. 

\section{Related work}

{\bf Pretext task methods.} The main idea behind these methods is to design a specific task {\em a.k.a. pretext task} for the dataset which will contain some "labels" of the pretext task without having any access to the labels of the target task. Such pretext tasks include but are not limited to: applying and predicting parameters of the geometric transformations \cite{gidaris2018unsupervised}, jigsaw puzzle solving \cite{noroozi2016unsupervised}, etc., inpainting \cite{pathak2016context}and colorization \cite{larsson2017colorization} of the images and reversing augmentations. Typically, the pretext task methods and coupled with other SSL techniques in recent years \cite{kinakh2021scatsimclr}, \cite{yi2022using}, \cite{zaiem2021pretext}.

{\bf Contrastive methods.} Most of the contrastive SSL methods are based on different extensions of the InfoNCE\cite{oord2018representation} formulation. 
% In turns the InfoNCE is based on the direct maximization of the mutual information between the input image $\bf{X}$ and its embedding $\bf{Z}$ with the augmented view $\tilde{\bf{X}}$ between them. 
The InfoNCE method is based on the direct maximization of the mutual information between the input image $\bf{X}$ and its embedding $\bf{Z}$, via minimization of the contrastive loss. Thus, the estimation of the distributions $p(\bf{z}|\bf{x})$ and $p(\bf{z})$ with the marginalization over $\tilde{\bf{X}}$. The examples of contrastive methods are SimCLR\cite{chen2020simple}, SwAV \cite{caron2020unsupervised}, DINO \cite{caron2021emerging}.

{\bf Clustering methods.} Clustering-based SSL methods are based on the idea of assigning cluster labels to the learned representations in an unsupervised manner with some regularization like maintaining uniformity of these cluster labels. DeepCluster  \cite{caron2018deep} method iteratively groups the features from the encoder using the standard {\it k}-means clustering and then uses them as an assignment for the supervision to update the weights of the encoder at the next iterations. SwAV \cite{caron2020unsupervised} and DINO\cite{caron2021emerging} are other notable clustering-based SSL methods that combine contrastive learning and clustering by clustering the data while requiring the same cluster assignment for different views of the same image.

{\bf Distillation methods.} Distillation-based SSL methods like BYOL \cite{grill2020bootstrap}, SimSiam  \cite{chen2021exploring} and others use the teacher-student type of training, where the student network is trained with the gradient descent, while the teacher network is not updated with gradient descent, but rather with an exponential moving average update or other methods. Such a design is used to avoid collapse. 

{\bf Collapse preventing methods.} Similar to distillation, collapse-preventing methods try to prevent the collapse by the usage of special regularization of embeddings. Barlow Twins \cite{zbontar2021barlow} method aims at making the covariance matrix of the embeddings to be an identity matrix. It means that each dimension of the embeddings should be decorrelated with all other dimensions. Additionally, the minimum variance of embedding per each dimension in the batch is constrained. VICReg \cite{bardes2021vicreg} method extends the Barlow Twins \cite{zbontar2021barlow} approach by imposing an additional constraint on the distance between the embeddings with and without augmentations. 

{\bf Knowledge distillation.} Knowledge distillation\cite{gou2021knowledge} is a type of model optimization, where a simple small model (student model) is trained to match the bigger complex model (teacher model). There are multiple types of knowledge distillation schemes: offline distillation\cite{hinton2015distilling},  online distillation\cite{mirzadeh2020improved}, and self-distillation\cite{zhang2019your}. There are multiple types of knowledge types, used for distillation: response-based knowledge\cite{hinton2015distilling},\cite{ba2014deep} feature-based knowledge\cite{romero2014fitnets}, and others. We show how our method can be used as offline feature-based knowledge distillation.

\section{\papername: detailed description}

\subsection{Method}
The training objective consists of two parts (a) ensuring the invariance of the learned representation under the applied augmentations and simultaneously (b) imposing constraints on the learned representation. The final loss integrates the loss based on the upper bound of the mutual information and distance correlation.

\subsubsection{Training objectives for the representation learning based on the mutual information}

We follow the overall idea of ensuring the invariance of learned representation under the family of applied augmentations and we proceed along the line discussed in the previous section. Since both branches have the same dimension $D$, we proceed with the maximization of the upper bound on the mutual information between these dimensions as considered in section \ref{upper_bound_MI}. 

To train the encoder, we penalize the embeddings of augmented view $\tilde{\mathbf{z}}_{i}$ to be as close as possible to the embeddings of non-augmented view $\mathbf{z}_{i}$ using MSE loss \ref{equation:MSE} between them:

\begin{equation}
    \label{equation:MSE}
    d\left(\mathbf{Z}, \tilde{\mathbf{Z}}\right)=\frac{1}{B} \sum_{i=1}^{B}\left\|\mathbf{z}_{i}-\tilde{\mathbf{z}}_{i}\right\|_{2}^{2}.
\end{equation}
Even though MSE is one of the most common losses for ensuring closeness between the embeddings, we show that this term corresponds to the conditional entropy term in the mutual information \ref{equation:MI_defintion}.

The variance regularization term corresponding to the entropy term in the mutual information \ref{equation:MI_defintion} is used to control the variance of the embeddings. We use a hinge function of the standard deviation of the embeddings along the batch dimension:

\begin{equation}
    \label{equation:Variance_reg}
   v(\mathbf{Z})=\frac{1}{D} \sum_{d=1}^{D} \max \left(0, \gamma-S\left(\textbf{z}[d], \epsilon\right)\right),
\end{equation}

where $\mathbf{z}[d]$ denotes the $d^{th}$ dimension of $\bf z$, $S$ is the standard deviation defined as:
\begin{equation}
    \label{equation:STD}
    S(a, \epsilon)=\sqrt{\operatorname{Var}(a)+\epsilon},
\end{equation}

and $\gamma$ is the margin parameter for the standard deviation, 
%that is a constant target value for the standard deviation,
$\epsilon$ is a small
scalar for numerical stability.

The corresponding loss that ensures the correspondence between the embeddings from the augmented and non-augmented views and the variance of both embeddings is defined as:
\begin{equation}
    \label{equation:loss_1}
    \mathcal{L}_{1}(\phi_{z}) = \lambda d\left(\textbf{Z}, \tilde{\textbf{Z}}\right)+\mu\left[v(\textbf{Z})+v\left(\tilde{\textbf{Z}}\right)\right],
\end{equation}
where $\lambda$ and $ \mu$ are hyper-parameters controlling the importance of each term in the loss. This loss is parametrized by the parameters $\phi_z$ of the encoder and projector. It should be pointed out that for the symmetry, we impose the constraint on the variance for both augmented and non-augmented embeddings.

It is interesting to point out that the obtained result coincides with the loss used in VICReg\cite{bardes2021vicreg} where its origin was not considered from the information-theoretic point of view as the maximization of mutual information between the embeddings. At the same time, it should be noted that there is no constraint on the covariance matrix of embeddings as in VICReg\cite{bardes2021vicreg} and Barlow Twins \cite{zbontar2021barlow} methods. 

\subsubsection{Training objectives for representation learning based on distance covariance}

The distance correlation is used for the dependence maximization between the embedding from the augmented view with the non-augmented one and the set of representations from the hand-crafted functions. 

Accordingly, the loss
$\mathcal{L}_{2}(\phi_{z})$ denotes the minimization formulation of the distance correlation maximization problem between embeddings from augmented and non-augmented views:
\begin{equation}
    \label{equation:loss_2}
    \mathcal{L}_{2}(\phi_{z}) = \alpha \left[1 - \mathrm{dCor}(\tilde{\textbf{Z}}, {\textbf{Z}}) \right],
\end{equation}
and the loss $\mathcal{L}_{3}(\phi_{z})$ denotes the same for  the embedding from the augmented view and $k^{th}$ hand-crafted representation:
\begin{equation}
    \label{equation:loss_3}
    \mathcal{L}_{3}(\phi_{z}) = \sum_{k=1}^{K} {\beta_{k} \left [ 1 - \mathrm{dCor}(\tilde{\textbf{Z}}, \textbf{Z}_k^{*}) \right ]},
\end{equation}
where $\alpha$ and $\beta_{k}$ are hyper-parameters controlling the importance of each term in the loss. In our experiments, we set $\alpha=1$ and perform a grid search on the values of $\lambda$ and $\mu$ with the base condition $\lambda=\mu>1$ and $\beta_{k}=1$. 
% The implementations details and pseudocode are provided in Appendices \ref{appendix:implementation}, \ref{appendix:preudocode}.

The final loss function is a sum of three losses:
\begin{equation}
    \label{equation:final_loss}
    \mathcal{L}\left( \phi_{z} \right)=\mathcal{L}_{1}(\phi_{z}) + \mathcal{L}_{2}(\phi_{z}) + \mathcal{L}_{3}(\phi_{z}).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications of proposed model}

In this section, we demonstrate the application of the proposed model to a) self-supervised pretraining of the model and b) self-supervised model distillation.

\subsection{Self-supervised pretraining for classification}

The proposed method can be used for efficient self-supervised model pretraining for the classification. Once pretrained, the model is finetuned for classification. We report our results on STL10 and ImageNet-1K datasets on linear evaluation and semi-supervised finetuning with 1\% and 10\% of labels pipelines in Table \ref{table:ImageNet_1k_evaluation}, and Table \ref{table:stl10_evaluation}. In a linear evaluation pipeline, the pretrained encoder is used as it, without further training, while only a one-layer linear classifier is trained with labeled data. In the semi-supervised finetuning pipeline, the classifier head is attached to the pretrained encoder and the full model is finetuned on the labeled data.

\subsection{Knowledge distillation}

The proposed method can also be used for efficient knowledge distillation. It is done by using the pretrained model (teacher) as the feature extractor and computing the distance correlation between the embeddings of the trainable encoder (student) and the pretrained teacher encoder. In practice, this can be used to match the performance of the big pretrained models with smaller models or match the performance of the models that have been trained on proprietary datasets. In contrast to the standard knowledge distillation approaches \cite{hinton2015distilling}, our approach does not use any labels, nor require the latent space of the same shape. As a practical example, we demonstrate that by using the proposed knowledge distillation approach, we are able to match the performance of the CLIP\cite{radford2021learning} based on ViT-B-16\cite{dosovitskiy2020vit} with 86.2M parameters pretrained on 400M images from LAION-400M\cite{schuhmann2021laion} dataset, by the ResNet50 with 23.5M parameters pretrained on STL10 and ImageNet-1K datasets as shown in section \ref{subsection:distillation}. Then the distilled model can be used for downstream tasks such as classification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{section:results}

In this section, we will demonstrate the performance of the proposed method for two downstream tasks: (a) SSL-based classification and (b) knowledge distillation-based classification.

\subsection{SSL based classification}

We evaluate the representations obtained after the pretraining ResNet50 backbone with {\papername} on ImageNet-1K \cite{deng2009imagenet} and STL-10 \cite{coates2011analysis} datasets for 1000 epochs using the loss function described above. The model pretrained on ImageNet-1K is evaluated with linear protocol and semi-supervised protocol with 1\% and 10\% of labeled images.


\subsubsection{Evaluation on ImageNet-1K}

{\bf Linear evaluation protocol}: A linear classifier is trained on top of the frozen representations of the ResNet50 pretrained using {\papername} for 100 epochs with the cross-entropy loss. {\bf Semi-supervised evaluation protocol}: The pretrained ResNet50 is fine-tuned with a fraction of the ImageNet-1K dataset: 1\% or 10\% of sampled labels for 100 epochs with the cross-entropy loss.

The results on the validation set of ImageNet-1K for linear and semi-supervised evaluation protocols of the model pretrained on the training set of ImageNet-1K are shown in Table \ref{table:ImageNet_1k_evaluation}. 
% The performance of the proposed method {\papername} is on par with other SSL methods, but our method does not use negative pairs as SimCLR, clustering as in SwAV, or stop-gradients as in BYOL and SimSiam.  
The main advantage of the {\papername} is that it presents a new way of regularizing latent space for self-supervised pretraining by using distance correlation between the embeddings from the model and hand-crafted image features. Due to the lack of computational resources, we did not run the parameters optimization for ImageNet-1K pretraining, so we think that the results might be further improved.

\begin{table}
% \renewcommand{\arraystretch}{0.5}
% \setlength{\tabcolsep}{3pt}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{lcccccc}
\hline
\multirow{3}{*}{Method} & \multicolumn{2}{c}{Linear} & \multicolumn{4}{c}{Semi-supervised}\\ \cline{2-7} 
& \multirow{2}{*}{Top 1} & \multirow{2}{*}{Top 5} & \multicolumn{2}{c}{Top 1} & \multicolumn{2}{c}{Top 5} \\ \cline{4-7} 
 & & & 1\% & 10\% & 1\% & 10\% \\ \hline
Supervised & 76.5 & - & 25.4 & 56.4 & 48.4 & 80.4 \\ \hline
% MoCo \cite{he2020momentum} & 60.6 & - & - & - & - & - \\
PIRL \cite{misra2020self} & 63.6 & - & - & - & - & - \\
% CPC v2 \cite{oord2018representation} & 63.8 & - & - & - & - & - \\
% CMC \cite{tian2020contrastive} & 66.2 & - & - & - & - & - \\
% SimCLR \cite{chen2020simple} & 69.3 & 89.0 & 48.3 & 65.6 & 75.5 & 87.8 \\
% MoCo v2 \cite{chen2020improved} & 71.1 & - & - & - & - & - \\
SimSiam \cite{chen2021exploring} & 71.3 & - & - & - & - & - \\
% SwAV \cite{caron2020unsupervised} & 71.8 & - & - & - & - & - \\
InfoMin Aug \cite{tian2020makes} & 73.0 & 91.1 & - & - & - & - \\
OBoW \cite{gidaris2021obow} & 73.8 & - & - & - & - & - \\
BYOL \cite{grill2020bootstrap} & 74.3 & 91.6 & 53.2 & 68.8 & 78.4 & 89.0 \\
% SwAV \\ (w/ multi-crop) \cite{caron2020unsupervised} & 75.3 & - & 53.9 & 70.2 & 78.5 & 89.9 \\
Barlow Twins \cite{zbontar2021barlow} & 73.2 & 91.0  & 55.0 & 69.7 & 79.2 & 89.3 \\
VICReg \cite{bardes2021vicreg} & 73.2 & 91.1 & 54.8 & 69.5 & 79.4 & 89.5 \\
{\bf {\papername} (ours)} & \bf{74.5} & \bf{92.1} & \bf{56.1} & \bf{69.9} & \bf{79.4} & \bf{89.5} \\ \hline
\end{tabular}
\end{adjustbox}
\caption{{\bf Evaluation on ImageNet-1K}. Evaluation of the representations from ResNet50 {\bf non-contrastive} backbones pretrained with {\papername} on: (1) linear evaluation protocol on top of frozen representations from ImageNet; (2) semi-supervised classification on top of fine-tuned representations with 1\% and 10\% of labeled ImageNet samples.}
\label{table:ImageNet_1k_evaluation}
\end{table}

\subsubsection{Evaluation STL10}

In this study, we wanted to demonstrate the method performance when directly training on a small-size dataset. Each class is represented by about 500 images per class. The results on the test set of STL10 \cite{coates2011analysis} dataset for linear evaluation protocol of the model pretrained on the training and unlabeled subsets of STL10 are shown in Table \ref{table:stl10_evaluation}. 
 
The model was trained with the following hand-crafted features: flattened original and augmented images, ScatNet features, HOG features, and LSD features. Our model achieves state-of-the-art results in the linear evaluation protocol on the STL10 dataset. In the linear evaluation protocol, the linear model is trained on the training subset of STL10. The model is not evaluated on a semi-supervised evaluation protocol. The proposed model demonstrates the best performance among the compared methods. 

\begin{table}[h]
\begin{tabular}{lc}
\hline
Method       & STL10 \\ \hline
ADC \cite{haeusser2018associative} & 53.0 \\
IIC \cite{ji2019invariant} & 61 \\
TSUK \cite{han2020mitigating} & 66.5   \\
SCAN \cite{van2020scan} & 80.9   \\
ScatSimCLR \cite{kinakh2021scatsimclr} & 85.1  \\
RUC \cite{park2021improving} & 86.7   \\
{\bf {\papername} (ours)} & {\bf 89.7}  \\ \hline
\end{tabular}
\caption{{\bf Evaluation on STL10}. Classification accuracy for the linear evaluation protocol on top of frozen representations from STL10 dataset.}
\label{table:stl10_evaluation}
\end{table}

\subsubsection{Transfer learning}

To evaluate the pretrained representation of multiclass classification on the VOC07\cite{pascalvoc2007} dataset, we train a linear classifier on top of the frozen representations from the pretrained encoder for 100 epochs. The mAP on the VOC07 dataset is reported in Table \ref{table:voc07_transfer} along with the results from other non-contrastive state-of-the-art SSL methods with a ResNet50 as a backbone.

\begin{table}
\begin{tabular}{lc}
\hline
\multirow{2}{*}{Method} & Linear Classification \\ \cline{2-2} 
 & VOC07 \\ \hline
Supervised & 87.5 \\ \hline
% MoCo \cite{he2020momentum}    & 79.8    \\
PIRL \cite{misra2020self} & 81.1 \\
% SimCLR \cite{chen2020simple} & 85.5 \\
% MoCo v2  \cite{chen2020improved} & 86.4 \\
BYOL \cite{grill2020bootstrap} & 86.6 \\
% SwAV (w/ multi-crop) \cite{caron2020unsupervised} & 88.9 \\
OBoW \cite{gidaris2021obow} & 89.3 \\
Barlow Twins \cite{zbontar2021barlow} & 86.2 \\
VICReg \cite{bardes2021vicreg} & 86.6 \\
{\bf \papername (ours)} & 87.1 \\ \hline
\end{tabular}

\caption{{\bf Transfer learning on multiclass classification on VOC07\cite{pascalvoc2007} dataset}. Evaluation of the non-contrastive representations from pretrained model on multiclass classification using the linear classifier on top of frozen representations. We report mAP.}
\label{table:voc07_transfer}
\end{table}

\subsection{Knowledge distillation based classification}
\label{subsection:distillation}

To evaluate the proposed approach to the knowledge distillation-based classification, we have used pretrained CLIP\cite{radford2021learning} model based on ViT-B-16\cite{dosovitskiy2020vit} encoder as the teacher and the ResNet50\cite{he2016deep} as the student model. The CLIP is trained based on the contrastive loss between the image and text embeddings. To proceed with the knowledge distillation in the same way as for the SSL training, we use the default projector $8192-8192-8192$ after the ResNet50 encoder. The pretrained CLIP ViT model uses images of the shape $224 \times 224$ as an input and outputs the latent vector of the shape 512, as shown in Figure \ref{fig:mv_mr_distill}. The teacher model is evaluated using zero-shot evaluation and linear evaluation pipelines. The student model is evaluated only using a linear evaluation pipeline.

The {\em goal of the experimental validation is to demonstrate whether the ResNet50 model with 23.5M  parameters trained only on 1.3M images can provide similar performance to the CLIP based on the ViT model with 86.2M parameters, trained on 400M images}. It is important to point out that the training is performed without any additional labels according to the proposed knowledge distillation framework. In Table \ref{table:distillation_imagenet1k}, we report results of knowledge distillation, where CLIP based on ViT-B-16 is used as a teacher model and ResNet50 as a student model. The model is only trained for 200 epochs using the proposed knowledge distillation approach on ImageNet-1K and then evaluated on STL10 and ImageNet-1K datasets. The obtained results confirm that the convolutional ResNet50 model with 4$\times$ fewer parameters in comparison to the transformer ViT teacher model and trained on a considerably smaller amount of unlabeled data can closely approach the performance of the teacher model without any special labeling, clustering, additional augmentations, or complex contrastive losses. It is also important to point out that the CLIP model trained on 400M text-image pairs initially outperformed all previous SSL methods for the STL-10 dataset. Remarkably, the proposed knowledge distillation largely preserved this performance and achieved 95.6\%  versus the best SSL MV-MR result 89.7\%  as indicated in Tables \ref{table:stl10_evaluation}, \ref{table:distillation_imagenet1k}. Thus, both the proposed MV-MR SSL training and knowledge distillation achieve state-of-the-art results on the STL-10 dataset and competitive results for the ImageNet-1K among all non-contrastive and clustering-free SSL methods.

\begin{table}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{lccc}
\hline
\textbf{Approach} &  \textbf{Parameters}  & \textbf{STL10} & \textbf{ImageNet-1K} \\ \hline
CLIP ViT-B-16 \\ (zero-shot) &  86.2M   & 96.8           & 67.1                 \\ \hline
CLIP ViT-B-16 \\ (linear evaluation) & 86.2M & 98.5      & 77.4                     \\ \hline
{\bf MV-MR} ResNet50 \\ (linear evaluation) & 23.5M & 95.6 & 75.3                    \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Knowledge distillation experiment with ImageNet-1K as training dataset, CLIP based on ViT-B-16 as teacher model, and ResNet50 as a student model.}
\label{table:distillation_imagenet1k}
\end{table}


\subsection{Ablation studies}

In this subsection, we describe the ablation studies on losses (Tables \ref{table:loss_ablations}).  In each of the experiments, we use the same training and evaluation setup: dataset: STL10, epochs: $100$, batch size: $64$, 16-bit precision, batch accumulation: $1$ batch. We use a linear evaluation pipeline. We demonstrate the impact of representation learning based on the maximization of the considered upper bound on the mutual information and the maximization of distance covariance in various settings. In this ablation, we show that the best results are achieved when using three loss terms: $\mathcal{L}_1$, $\mathcal{L}_2$, and $\mathcal{L}_2$. The ablation studies on augmentations and feature extractors are presented in the supplementary material.

\begin{table}
\begin{tabular}{ccccc}
\hline & & & \multicolumn{2}{c}{Accuracy} \\ \cline{4-5} 
\multirow{-2}{*}{ $\mathcal{L}_1$} & \multirow{-2}{*}{$\mathcal{L}_2$} & \multirow{-2}{*}{$\mathcal{L}_3$} & Top 1 & Top 5                        \\ \hline
\multicolumn{5}{c}{1 loss}                                                                                \\ \hline
\checkmark & & & 50.86 & 93.95  \\
 & \checkmark & & 46.71 & 92.18 \\
 & & \checkmark & 44.1  & 92.08 \\ \hline
\multicolumn{5}{c}{2 losses} \\ \hline
\checkmark & \checkmark & & 50.76 & 93.83 \\
\checkmark & & \checkmark & 47.39 & 92.54 \\
 & \checkmark & \checkmark & 40.06 & 89.31 \\ \hline
\multicolumn{5}{c}{3 losses} \\ \hline
\checkmark & \checkmark & \checkmark & {\bf 69.38} & {\bf 98.85} \\ \hline
\end{tabular}

\caption{Ablation studies on the combination of losses. We check the importance of each loss term for the training of the model. It is shown that using loss terms $\mathcal{L}_{1}$, $\mathcal{L}_{2}$ and $\mathcal{L}_{3}$ provides the best classification performance. Also, we observe a phenomenon in which loss terms $\mathcal{L}_{2}$ and $\mathcal{L}_{3}$ work the best when applied jointly with the loss $\mathcal{L}_{1}$. However, it is interesting to point out that a disjoint usage of these losses does not lead to reasonable performance enhancement. The exact nature of this phenomenon is not completely clear and additional investigation should be performed.}
\label{table:loss_ablations}
\end{table}

More ablation studies: a study on the hand-crafted features used, a study on the augmentation used during training, and a study on the shape of the projector are presented in the supplementary material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation details}
\label{appendix:implementation}

The architecture of the {\papername} is similar to ones used in other SSL methods such BarlowTwins \cite{zbontar2021barlow}, VICReg \cite{bardes2021vicreg}, and others. The model $f_{\phi_{z}}$, shown in Figure \ref{fig:d_cor_ssl}, consists of two main parts: (i) the encoder, which is used for downstream tasks, and (ii) the projector, which is used for the mapping of encoder outputs to the embeddings used for training loss functions \ref{fig:d_cor_ssl}. In our experiments, we use standard ResNet50 \cite{he2016deep} available in $torchvision$ library\cite{paszke2019pytorch} as the encoder and projector, which consists of two linear layers of the size $8192$ followed by batch normalization and ReLU and output linear layer.

In our experiments, we use computer vision feature extraction methods applied to the original data: original RGB image, that is being flattened into a feature vector, ScatNet features of the image\cite{andreux2020kymatio}, random augmented images, flatten into a feature vector, histogram of oriented gradients (HOG), and local standard deviation filter (LSD filter)\cite{narendra1981real}.

\textbf{ScatNet transform}: ScatNet \cite{oyallon2018scattering, andreux2020kymatio} is a class of Convolutional Neural Networks (CNNs) that has a set of useful properties: (i) deformation stability, (ii) fixed weights, (iii) sparse representations, (iv) interpretable representation.

\textbf{Random augmented image}: In our experiments, we have applied the following augmentations to the image: random cropping, horizontal flipping, random color augmentations, grayscale and Gaussian blur. Then the image is flattened into a 1-dimensional feature vector.

\textbf{HOG}: Histogram of oriented gradients (HOG) \cite{dalal2005histograms} is a feature description that is based on the counting of occurrences of gradient orientation in the localized portion of an image. %The HOG focuses on the structure and the shape of the object in the image since it uses magnitude and the angle of the gradient to compute the features. It generates histograms of the magnitudes and orientations of the gradients for all the regions of the image.

\textbf{LSD filter}: A local standard deviation filter\cite{narendra1981real} is a filter that computes a standard deviation in a defined image region over the image. The region is usually of a rectangular shape of size $3 \times 3$ or $5 \times 5$ pixels. %It can be considered as a function whose value is proportional to the image "activity", i.e., the flat region produces a value close to zero while the textures and edges produce high values.

We use the PyTorch framework \cite{paszke2019pytorch} for the implementation of the proposed approach. %We use the projector with the size of linear layers $8192-8192-8192$, each of the layers except the last is followed by batch normalization and ReLU activation. 
We use ScatNet with the following parameters: $J = 2$ and $L = 8$. We use the HOG feature extractor with the following parameters: number of bins $24$ and pool size  $8$. We use a kernel of the size 3x3 in the STD filter. As augmentations for both image representation and as the input to the encoder we use: random resized cropping, random horizontal flipping with probability $0.5$, random color jittering augmentation with brightness $0.8$, contrast $0.8$, saturation $0.8$, hue $0.2$ and probability $0.8$, random grayscale with probability $0.2$ and Gaussian blur of the kernel size $0.1$ of the image size, mean $0$ and sigma in range the $[0.1, 2]$.

For the losses, the margin parameter $\gamma$ is set to $1$ and $\epsilon$ is set to $1e-4$ in $v(\mathbf{Z})=\frac{1}{D} \sum_{d=1}^{D} \max \left(0, \gamma-S\left(\textbf{z}[d], \epsilon\right)\right)$.

During {\bf self-supervised pretraining} experiments, that are presented in Table \ref{table:ImageNet_1k_evaluation} and Table \ref{table:stl10_evaluation}, we train models for $1000$ epochs, batch size $256$, gradient accumulation every 4 steps, base learning rate $1e-4$, Adam\cite{kingma2014adam} optimizer, cosine learning rate schedule and $16$-bit precision. During {\bf linear evaluation} on STL10 and ImageNet-1K, we train a single-layer linear model for $100$ epochs with a batch size $256$, learning rate $1e-4$, and Adam optimizer. During {\bf semi-supervised evaluation} on ImageNet-1K, we train a model for $100$ epochs with a batch size $128$, learning rate $1e-4$, and Adam optimizer. During the knowledge distillation, we train the model for $200$ epochs, with $512$ batch size, base learning rate $1e-4$, Adam\cite{kingma2014adam} optimizer, cosine learning rate schedule, and $16$-bit precision.

When training, weight parameters $\lambda = 1$ and  $\mu = 1$ in in $\mathcal{L}_{1}$, $\alpha = 1$ in $\mathcal{L}_{2}$ and $\beta_{k} = 1, k = 1...K$ in  $\mathcal{L}_{3}$.

\section{Conclusions}

In this paper, we introduce novel self-supervised {\papername} learning and knowledge distillation approaches, which are based on the maximization of the several dependency measures between two embeddings obtained from views with and without augmentations and multiple representations extracted from non-augmented views. The proposed methods use an upper bound on mutual information and a distance correlation for the dependence estimation for the representations of different dimensions. We explain the intuition behind the proposed method of upper bound on the mutual information and the usage of distance correlation as a dependence measure. Our method achieves state-of-the-art self-supervised classification on the STL10 dataset and achieves comparable state-of-the-art results on ImageNet-1K datasets on linear evaluation and semi-supervised evaluations. We show that ResNet50 pretrained using knowledge distillation on CLIP ViT-B-16 achieves comparable performance with far fewer parameters.


%%%%%%%%%%%% Supplementary Methods %%%%%%%%%%%%
\appendix
\section{Distance correlation definition}
\label{appendix:d_corr}

The distance correlation\cite{szekely2007measuring} term between the batch of embeddings of augmented view $\tilde{\textbf{Z}}$ and the batch of embeddings of original view $\textbf{Z}$ is computed by utilizing the following formula:

\begin{equation}
    \label{equation:d_corr_appendix}
    \mathrm{d} \operatorname{Cor}(\tilde{\textbf{Z}}, \textbf{Z})=\frac{\mathrm{dCov}^{2}(\tilde{\textbf{Z}},\textbf{Z})}{\sqrt{\mathrm{dVar}(\tilde{\textbf{Z}}) \mathrm{dVar}(\textbf{Z})}},
\end{equation}

where $\textbf{Z} = [\textbf{z}_{1}, ..., \textbf{z}_{B}]$ - batch of embeddings from original views,  $\tilde{\textbf{Z}} = [\tilde{\textbf{z}}_{1}, ..., \tilde{\textbf{z}}_{B}]$ - batch of embeddings from augmented views and:

\begin{equation}
    \label{equation:app_dcov}
    \mathrm{dCov}_{B}^{2}(\tilde{\textbf{Z}}, \textbf{Z}):=\frac{1}{B^{2}} \sum_{j=1}^{B}\sum_{i=1}^{B} A_{j, i} C_{j, i}.
\end{equation}

The distance correlation term between the batch of embeddings of augmented views $\tilde{\mathbf{Z}}$ and the batch of features computed from the original views $\mathbf{Z}^{*}$ is computed by the following formula:

\begin{equation}
    \label{equation:d_corr_feat}
    \mathrm{dCor}(\tilde{\textbf{Z}}, \textbf{Z}^{*})=\frac{\mathrm{dCov}^{2}(\tilde{\textbf{Z}},\textbf{Z}^{*})}{\sqrt{\mathrm{dVar}(\tilde{\textbf{Z}}) \mathrm{dVar}(\textbf{Z}^{*})}},
\end{equation}

where $\textbf{Z}^{*} = [\textbf{z}^{*}_{1}, ..., \textbf{z}^{*}_{B}]$ - batch of features computed from original views and

\begin{equation}
    \label{equation:dcov_feat}
    \mathrm{dCov}_{B}^{2}(\tilde{\textbf{Z}}, \textbf{Z}^{*}):=\frac{1}{B^{2}} \sum_{j=1}^{B}\sum_{i=1}^{B} A_{j, i} D_{j, i}.
\end{equation}


In formulas \ref{equation:app_dcov}, \ref{equation:dcov_feat} $A_{j, i}:=a_{j, i}-\bar{a}_{j \cdot}-\bar{a}_{\cdot i}+\bar{a}_{\cdot \cdot}, \quad$ $C_{j, i}:=c_{j,i}-\bar{c}_{j \cdot}-\bar{c}_{\cdot i}+\bar{c}_{\cdot \cdot}, \quad$ $D_{j, i}:=d_{j,i}-\bar{d}_{j \cdot}-\bar{d}_{\cdot i}+\bar{d}_{\cdot \cdot}$  where  $\quad a_{j, i} =\left\|\tilde{\textbf{Z}}_{j}-\tilde{\textbf{Z}}_{i}\right\| , \quad$  $c_{j, i} =\left\|\textbf{Z}_{j}-\textbf{Z}_{i}\right\| \quad$ and $d_{j, i} =\left\|\textbf{Z}^{*}_{j}-\textbf{Z}^{*}_{i}\right\|,\quad$ where $ j, i=1,2, \ldots, B$.

\section{Ablation studies}

\begin{table}[H]
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{Original image} & \multirow{2}{*}{ScatNet} & \multirow{2}{*}{Augmented image} & \multirow{2}{*}{HOG} & \multirow{2}{*}{LSD} & \multicolumn{2}{c}{Accuracy} \\ \cline{6-7} 
& & & & & Top 1 & Top 5        \\ \hline
\multicolumn{7}{c}{1 feature}  \\ \hline
\checkmark & & & & & 58.82         & 96.81        \\
 & \checkmark & & & & 54.12        & 95.23        \\
 & & \checkmark & & & 63.51        & 97.81        \\
 & & & \checkmark & & 54.15        & 95.26        \\
 & & & & \checkmark & 53.94        & 95.44        \\ \hline
\multicolumn{7}{c}{2 features}                    \\ \hline
\checkmark & \checkmark & & & & 64.44 & 97.97     \\
\checkmark & & \checkmark & & & 66.18 & 98.39     \\
\checkmark & & & \checkmark & & 63 & 97.78        \\
\checkmark & & & & \checkmark & 63.14 & 97.8      \\
 & \checkmark & \checkmark & & & 63.3 & 97.78     \\
 & \checkmark & & \checkmark & & 62.95 & 97.6     \\
 & \checkmark & & & \checkmark & 59.41 & 96.78    \\
 & & \checkmark & \checkmark & & 63.66 & 97.69    \\
 & & \checkmark & & \checkmark & 60.21 & 96.8     \\
 & & & \checkmark & \checkmark & 62.46 & 97.71    \\ \hline
\multicolumn{7}{c}{3 features} \\ \hline
\checkmark & \checkmark & \checkmark & & & 65.82 & 98.18   \\
\checkmark & \checkmark & & \checkmark & & 65.52 & 97.97   \\
\checkmark & \checkmark & & & \checkmark & 60.96 & 97.08   \\
\checkmark & & \checkmark & \checkmark & & 65.11 & 98.12   \\
\checkmark & & \checkmark & & \checkmark & 65.19 & 98      \\
\checkmark & & & \checkmark & \checkmark & 65.37 & 98.29   \\
 & \checkmark & \checkmark & \checkmark & & 65.45 & 98.18  \\
 & \checkmark & \checkmark & & \checkmark & 64.35 & 97.93  \\
 & \checkmark & & \checkmark & \checkmark & 60.63 & 97.1   \\
 & & \checkmark & \checkmark & \checkmark & 64.9  & 98.08  \\ \hline
\multicolumn{7}{c}{4 features} \\ \hline
\checkmark & \checkmark & \checkmark & \checkmark & & 68.25 & 98.45 \\
\checkmark & \checkmark & \checkmark & & \checkmark & 68.2  & 98.53 \\
\checkmark & \checkmark & & \checkmark & \checkmark & 64.56 & 97.44 \\
\checkmark & & \checkmark & \checkmark & \checkmark & 67.21 & 98.48 \\
 & \checkmark & \checkmark & \checkmark & \checkmark & 67.05 & 98.22 \\ \hline
\multicolumn{7}{c}{5 features} \\ \hline
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark & {\bf 69.38} & {\bf 98.85} \\ \hline
\end{tabular}
\end{adjustbox}

\caption{Ablation studies of the combinations of features used for the $\mathcal{L}_3$ loss. In this setup, all three losses are used: $\mathcal{L}_1$ and $\mathcal{L}_2$ and $\mathcal{L}_3$. ScatNet transformation of the original image. Augmented image - randomly augmented original image. HOG - histogram of oriented gradients computed from original view. LSD - original view filtered with the local standard deviation filter. Since all of these features are images, they are flattened before computing distance correlation.}
\label{table:features_ablations_zz}
\end{table}

In this section, we describe the ablation studies on the combination of features for loss term $\mathcal{L}_3$ (Table \ref{table:features_ablations_zz}), a number of layers, and size of the projector in the trainable encoder (Table \ref{table:projector_ablations}), and image augmentations (Table \ref{table:augmentation_ablations}). In each of the experiments, we use the same training and evaluation setup: dataset: STL10\cite{coates2011analysis}, epochs: $100$, batch size: $64$, 16-bit precision, batch accumulation: $1$ batch. When pretraining, all 3 loss terms are used. After model pretraining, it is evaluated using linear evaluation.

We describe the ablation studies on the combinations of features used for the $\mathcal{L}_3$ loss term in combination with loss terms $\mathcal{L}_{1}$ and $\mathcal{L}_{2}$ in Tables \ref{table:features_ablations_zz}. We study the impact of features on the classification accuracy of the model. We use the following features in the study: the original image flattened into a vector, ScatNet\cite{andreux2020kymatio} features of the original image, an augmented image flattened into a vector, and a histogram of oriented gradients of the original image, and features from the local standard deviation filter (LSD). We use ScatNet with the following parameters: $J = 2$ and $L = 8$. We use the HOG\cite{dalal2005histograms} feature extractor with the following parameters: number of bins $24$ and pool size  $8$. Kernel of the size 3x3 is used in the LSD filter\cite{narendra1981real}. As augmentations for image representation we use: random resized cropping, random horizontal flipping with probability $0.5$, random color jittering augmentation with brightness $0.8$, contrast $0.8$, saturation $0.8$, hue $0.2$ and probability $0.8$, random grayscale with probability $0.2$ and Gaussian blur of the kernel size $0.1$ of the image size, mean $0$ and sigma in range the $[0.1, 2]$. We show that the best results are achieved, when we use the combination of all feature extractors, mentioned above.

\begin{table}
% \setlength{\tabcolsep}{1pt}
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{Random Crop} & \multirow{2}{*}{Horizontal Flip} & \multirow{2}{*}{Color} & \multirow{2}{*}{Grayscale} & \multirow{2}{*}{Blur} & \multicolumn{2}{c}{Accuracy} \\ \cline{6-7} & & & & & Top 1 & Top 5 \\ \hline
\checkmark & & & & & 43.02 & 90.31 \\
\checkmark & \checkmark & & & & 49.99 & 93.98 \\
\checkmark & \checkmark & \checkmark & & & 50.58 & 93.76 \\
\checkmark & \checkmark & \checkmark & \checkmark & & {\bf 70.82} & {\bf 98.96} \\
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 69.38 & 98.85 \\ \hline
\end{tabular}
\end{adjustbox}

\caption{Ablation studies on the image augmentations.}
\label{table:augmentation_ablations}
\end{table}

The ablation studies on image augmentations are presented in Table \ref{table:augmentation_ablations}. As augmentations, we compare 
random resized cropping, random horizontal flipping,  random color augmentations, random grayscale, and random Gaussian blur. We use the same parameters for each augmentation, as when augmented images are used as features. We show that the best classification results are achieved when a combination of random cropping, horizontal flipping, color jittering, and random grayscale is used.
%random resized cropping, random horizontal flipping with probability $0.5$, random color jittering augmentation with brightness $0.8$, contrast $0.8$, saturation $0.8$, hue $0.2$ and probability $0.8$, random grayscale with probability $0.2$ and Gaussian blur of the kernel size $0.1$ of the image size, mean $0$ and sigma in range the $[0.1, 2]$. We show that the best classification results are achieved when a combination of random cropping, horizontal flipping, color jittering, and random grayscale is used.

\begin{table}
\begin{tabular}{lr}
\hline
\multicolumn{1}{c}{\textbf{Projector size}} & \multicolumn{1}{c}{\textbf{Accuracy}} \\ \hline
\textbf{8192-8192-8192}                     & \textbf{69.38}                        \\
4096-4096-4096                              & 51.90                                  \\
2048-2048-2048                              & 51.35                                 \\
1024-1024-1021                              & 51.86                                 \\
512-512-512                                 & 49.40                                 \\
256-256-256                                 & 49.02                                 \\
8192-8192                                   & 49.90                                  \\
4096-4096                                   & 48.81                                 \\
2048-2048                                   & 48.66                                 \\
1024-1024                                   & 48.73                                 \\
512-512                                     & 48.06                                 \\
256-256                                     & 48.07                                 \\
8192                                        & 48.65                                 \\
4096                                        & 48.51                                 \\
2048                                        & 48.20                                  \\
1024                                        & 47.61                                 \\
512                                         & 46.11                                 \\
256                                         & 47.17                                 \\
without projector                           & 16.70                                  \\ \hline
\end{tabular}

\caption{Ablation studies on the projector. Projectors consist of blocks with linear layers, batch normalization, and ReLU activation. We always keep the last layer linear.}
\label{table:projector_ablations}
\end{table}

The ablation studies on the number of layers and their size in the encoder's projects are presented in Table \ref{table:projector_ablations}. When the number of layers is bigger than one in the projector, it consists of blocks with linear layers, batch normalization, and ReLU activation. We always keep the last layer linear. We show the best classification results are shown when the projector consists of 3 layers, each with $8192-8192-8192$ neurons.

%%%%%%%%%%%%% Acknowledgements %%%%%%%%%%%%%
%\footnotesize
%\section*{Acknowledgements}

%%%%%%%%%%%%%%   Bibliography   %%%%%%%%%%%%%%
\normalsize
\bibliography{references}

%%%%%%%%%%%%  Supplementary Figures  %%%%%%%%%%%%
%\clearpage

%%%%%%%%%%%%%%%%   End   %%%%%%%%%%%%%%%%
% \end{multicols}  % Method B for two-column formatting (doesn't play well with line numbers), comment out if using method A
\end{document}
