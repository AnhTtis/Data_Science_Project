\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[dvipsnames]{xcolor}
\usepackage{ragged2e}
%\usepackage{caption}
\usepackage{subcaption}
\captionsetup[subfigure]{aboveskip=-20pt}
\captionsetup{justification=RaggedRight,singlelinecheck=false}

\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}
\usepackage{comment}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\extr}{extr}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\cH}{\mathcal{H}}
 
\begin{document}

%\title{The Hidden-Manifold Hopfield Model shows learning phase transitions and is able to generalize}
\title{The Hidden-Manifold Hopfield Model and a learning phase transition}
%\title{The Hopfield Model trained on strongly-correlated data shows learning phase transitions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{M. Negri }
\email[Corresponding author; ]{matteo.negri@uniroma1.it}
\affiliation{ University of Rome ‘La Sapienza’, Department of Physics, Piazzale Aldo Moro 5, 00185 Roma, Italy}
\affiliation{CNR-NANOTEC, Institute of Nanotechnology, Rome Unit, Piazzale Aldo Moro, 00185 Roma, Italy}

\author{C. Lauditi }
\affiliation{ Department of Applied Science and Technology, Politecnico di Torino, 10129 Torino, Italy}
\affiliation{ Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}

\author{G. Perugini }
\affiliation{ Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}

\author{C. Lucibello }
\affiliation{ Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}

\author{E. Malatesta }
\affiliation{ Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
   \author{M. Negri $^1$ $^2$}
\email[Corresponding author; ]{matteo.negri@uniroma1.it}
\author{C. Lauditi $^3$ $^4$}
\author{G. Perugini $^4$}
\author{C. Lucibello $^4$}
\author{E. Malatesta $^4$}

\affiliation{$^1$ University of Rome ‘La Sapienza’, Department of Physics,
Piazzale Aldo Moro 5, 00185 Roma, Italy}
\affiliation{$^2$ CNR-NANOTEC, Institute of Nanotechnology, Rome Unit, Piazzale Aldo Moro, 00185 Roma, Italy}
\affiliation{$^3$ Department of Applied Science and Technology, Politecnico di Torino, 10129 Torino, Italy}
\affiliation{$^4$ Artificial Intelligence Lab, Bocconi University, 20136 Milano, Italy} 
\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\date{\today}

\begin{abstract}

The Hopfield model has a long-standing tradition in statistical physics, being one of the few neural networks for which a theory is available. 
%Generalized versions of the Hopfield model recently raised renewed interest because they became the building blocks of the state-of-the-art deep neural networks called \emph{transformers}. 
Extending the theory of Hopfield models for correlated data could help understand the success of deep neural networks, for instance describing how they extract features from data.
Motivated by this, we propose and investigate a generalized Hopfield model that we name \emph{Hidden-Manifold Hopfield Model}: we generate the couplings from $P=\alpha N$ examples with the Hebb rule using a non-linear transformation of $D=\alpha_D N$ random vectors that we call \emph{factors}, with $N$ the number of neurons. Using the replica method, we obtain a phase diagram for the model that shows a phase transition where the factors hidden in the examples become attractors of the dynamics; this phase exists above a critical value of $\alpha$ and below a critical value of $\alpha_D$.  We call this behaviour \emph{learning transition}.

\end{abstract}

\maketitle

%\section{Introduction}

The Hopfield model (HM) \cite{hopfield1982neural} is a paradigmatic connectionist model of associative memory with biological plausibility that allows the dynamical retrieval of stored patterns from corrupted observations.
In the case of uncorrelated patterns, retrieval is possible for a number of patterns that scales linearly with the system size $N$, and the critical prefactor can be computed to high precision using spin-glass theory techniques \cite{amit1987statistical}.

Following Hopfield's seminal work, several generalizations have been investigated.  A recent surge of interest involves generalizations that go beyond  pairwise interactions and yield polynomial \cite{gardner1987multiconnected, krotov2016dense} or even exponential capacity \cite{demircigil2017model, ramsauer2020hopfield}. Notably, the modern Hopfield network proposed in \cite{ramsauer2020hopfield} is closely related to the attention mechanism that has revolutionized deep learning in the last years \cite{vaswani2017attention}. 
Other research lines preserve the pairwise structure of the standard Hopfield model (SHM) while proposing different (non-Hebb) rules for the couplings in order to address the problem of correlation among patterns decreasing the capacity \cite{amit1987information, fontanari1990storage, der1992modified, van1997hebbian, lowe1998storage}. Many sensible models of correlation in and among patterns have been proposed. For example, in \cite{gutfreund1988neural} the authors study a biased distribution of binary patterns, that can even be generalized to a hierarchical structure of correlation as it was discussed in \cite{cortes1987hierarchical, krogh1988mean}. Another approach is to consider correlations in the form of Markov chains \cite{lowe1998storage}, with can be used to produce a correlation length both between different spins of a given pattern and between the same spin in different patterns.


% To this date, the Hopfield model is one of the few analytically solvable models for neural networks, and the fact that it became relevant for deep neural networks is a promising chance to understand the success of deep learning, for which a comprehensive theory is still not available.

% The Hopfield model \cite{hopfield1982neural} is a recurrent artificial neural network that works as an associative memory: given a set of patterns, the model is able to memorize and retrieve them if the variables are initialized close enough to one of the memories. The model has been solved analytically in the framework of statistical mechanics when the patterns are uncorrelated and their number is proportional to the number of neurons \cite{amit1987statistical}. Nevertheless, this model has limited practical use: its critical capacity is relatively low and it further decreases as soon as correlations are introduces in the patterns. 

% Generalizations of Hopfield model to the case of correlated patterns have already been considered immediately after the solution of the uncorrelated case \cite{amit1987statistical}. Most of these studies considered modifications of the Hebb rule that would allow the storage of the highest possible number of correlated patterns. For example, in \cite{gutfreund1988neural} the authors study a biased distribution of binary patterns, that can even be generalized to a hierarchical structure of correlation as it was discussed in \cite{cortes1987hierarchical, krogh1988mean}. Another approach is to consider correlations in the form of Markov chains \cite{lowe1998storage}, with can be used to produce a correlation length both between different spins of a given pattern and between the same spin of different patterns.

Most theoretical studies of (generalized) HMs assume simple distributions for the patterns \cite{amit1987statistical, gardner1987multiconnected}, while in practical applications the patterns are linearly or non-linearly encoded from and decoded to a different space \cite{steinberg2022associative}.

In this work, we addressed this limitation by proposing a generative model for  the patterns where each pattern is produced by the linear combinations of a fixed vocabulary of what we call \emph{factors} weighted by pattern specific \emph{coefficients}, followed by an elementwise non-linearity.
We analyze the model in the high-dimensional regime using the replica method for the statistical physics of disordered systems. 

This data-generating process generalizes the structure of linear superposition proposed in \cite{mezard2017mean}, where it was discussed in relation to the mapping between a Hopfield network and a restricted Boltzmann machine. A similar linear (but dense) mapping has been discussed in \cite{agliari2013parallel, smart2021mapping}.
Our model is also deeply related to the so-called  \emph{hidden-manifold} model  \cite{goldt2020modeling}, which has been used as an analytically solvable model of feedforward neural networks fitting datapoints that live on a low dimensional sub-manifold of their embedding space. In fact, this low-dimensional latent structure is typical of many real-world datasets, e.g. the ones made of natural images.  

Here we do not modify the Hebb rule, as we will see that it is enough to produce a new behaviour of the model, in conjunction with the structure of correlation that we choose. In fact, we observe that if the correlations in the data are strong enough the model switches from a storage phase to a learning phase, in the sense that attractors appear corresponding to the factors in the data. We argue that this behaviour opens up a new paradigm for this model and shows that it may have some phenomenology in common with neural networks.

%This paper is organized as follows: in section~\ref{sec:definition} we define the model and we give an intuitive explanation for the appearance of attractors correlated with the factors of the data. In section~\ref{sec:replica_calc} we sketch the replica calculations for the storage and learning critical lines at zero temperature. In section~\ref{sec:discussion} we comment the phase diagram and we discuss why our results are relevant for the factor extraction task and, more generally, for the matrix factorization problem.

%\section{The Hidden-Manifold Hopfield Model}
%\label{sec:definition}
%\subsection{Definition}

\paragraph*{Model definition.}

The Hopfield model \cite{hopfield1982neural} can be defined as a statistical physics model with $N$ binary spins $s_i=\pm 1$, $i=1,...,N$, and an energy function with all-to-all pairwise interactions 
\begin{equation}
\cH(\sigma) =-\frac{1}{2}\sum_{i\neq j}J_{ij}s_{i}s_{j}.
\end{equation}
The coupling matrix $J$ is defined through a set of $P$ \emph{patterns}  $\{\bxi_{\nu} \}_{\nu=1}^P$ via the Hebbian rule
\begin{equation}
J_{ij}=\frac{1}{N}\sum_{\nu=1}^{P}\xi_{\nu i}\xi_{\nu j}.
\label{eq:hebb_rule}
\end{equation}
In the standard statistical physics setting \cite{amit1987statistical},  $\xi_{\nu i}$ are independently and uniformly distributed binary spins.
We consider instead, structured patterns given by
a linear projection and a latent vector composed with a non-linearity:
\begin{equation}
\xi_{\nu i}=\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\nu k}f_{ki}\right).
\label{eq:correlated_examples}
\end{equation}
Here $\sigma(\cdot)$ is a generic non-linear function, $f_{ki}$ is
called the matrix of \emph{factors} and $c_{\nu k}$ is the matrix of \emph{coefficients}. A sparse and linear version of this structure is analyzed in Ref. \cite{mezard2017mean}.
The specific case we consider through the paper is the one of i.i.d. binary factors $f_{ki}=\pm1$, i.i.d standard Gaussian coefficients, and $\sigma$ equal to the $\sign$ function. By tuning $D$ we can switch between weakly and strongly correlated examples. In fact, in the $\alpha_D\to\infty$ we expect to recover the SHM as the examples become uncorrelated.  
In this work, the numerical results and most of the analytical ones are obtained in the limit $T\to0$. In this limit, the update rule of each spin at time $t$ reads 
\begin{equation}
    s_i^{(t+1)} = \sign \left( \sum_{j=1}^N J_{ij} s_j^{(t)}\right).
    \label{eq:update_rule}
\end{equation}
If a spin configuration $\tilde s_i$ satisfies the relation $\tilde s_i = \sign ( \sum_{j=1}^N J_{ij} \tilde s_i )$, then we say that $\tilde s_i$ is a fixed point of the dynamics. If the dynamics converges to $\tilde s_i$ even when a fraction of spins has been flipped, then $\tilde s_i$ is an attractor. The original task of the Hopfield model is to store $P$ examples as attractors. This can also be seen as a denoising operation, since the model is capable of retrieving the stored patterns starting from noisy versions of them. In \cite{amit1987statistical} the authors computed the maximum number i.i.d. patterns that can be retrieved, allowing for a small fraction of errors, in the scaling regime where $P=\alpha N$ as $N$ grows to infinity with $\alpha$ fixed. They obtain a critical value $\alpha_c\simeq 0.138$ such that the model is able to retrieve all patterns if $\alpha <\alpha_c$, while above $\alpha_c$ the model shows a first-order phase transition referred as \emph{catastrophic forgetting} and no storage is possible: the fixed point of the dynamics are completely uncorrelated with the patterns.

In our HMHM, the basic question that we are interested in is whether the factors $\mathbf f_k$ can be attractors themselves, and what happens to the attractors corresponding to the patterns.

\begin{figure*}
    \centering
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[trim={0.2cm 0.0cm 0.0cm 0.0cm},width=0.49\textwidth,clip]{fig_1A.pdf}}
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[trim={0.0cm 0.0cm 0.0cm 0.0cm},width=0.49\textwidth,clip]{fig_1B.pdf}}
    \caption{ \textbf{Storage and learning transitions}. a) The phase diagram of the HMHM shows three regions: the \textit{storage} phase (below the \textcolor{Orange}{orange line}), where patterns $\bxi_\nu$ are attractors; the \textit{learning} phase (above \textcolor{MidnightBlue}{blue line}), where factors $\mathbf{f}_k$ are attractors, and the spin glass phase (between the lines), where the attractors are uncorrelated with either $\bxi_\nu$ or $\mathbf{f}_k$. The two asymptotes are at $\alpha\simeq0.138$ and $\alpha_D\simeq0.138$. b) The plot shows the factor magnetization $\mu$ along a vertical cut of the phase diagram: increasing $\alpha$ the factor magnetization $\mu$ becomes different from zero with a first-order phase transition. The dashed line is the analytical prediction of the RS theory, while the markers are numerical experiments averaged over many samples for each value of $\alpha$. The simulations are performed initializing the model to a factor $\mathbf{f}_k$, running the update rule~\eqref{eq:update_rule}, then measuring $\mu_k$ at convergence. We used 100, 50, 20 and 10 samples for increasing values of $N$.}
    \label{fig:phase_diagram}
\end{figure*}

%\section{Learning phase transitions}
%\label{sec:replica_calc}
%\paragraph*{Learning phase transitions.}
\paragraph*{Replica analysis.}

%\subsection{Setup of the replica calculations}  

Since we are interested in the thermodynamic limit $N\to\infty$, we choose a regime where both $P$ and $D$ are proportional to $N$. At the same time, we keep the following ratios fixed
\begin{equation}
    \alpha=\frac{P}{N},\quad \alpha_D=\frac{D}{N}.
\end{equation}
These will be the control parameters for our model. They are related via the relation $\alpha=\alpha_T \alpha_D$, where $\alpha_T=P/D$.

In order to identify the phase transitions of the HMHM we want to compute the averaged free energy
\begin{equation}
    \phi = \lim_{N\to\infty} -\frac{1}{\beta N} \langle \log Z \rangle_{c,f},
    \label{eq:phi}
\end{equation}
where we specified that we have two sources of disorder that must be averaged: the coefficients $c$ and the factors $f$. $Z=\sum_{\left\{ s_{i}\right\}} e^{-\beta \cH}$ is the partition function, where the sum is taken over the possible values of the spins $s_{i}=\pm1$
for $i=1,...,N$.

In order to compute the average of $\log Z$ in eq.~\eqref{eq:phi} we use the replica method \cite{mezard1987spin}, that consists in writing the average of logarithm as  $\langle \log Z \rangle = \lim_{n\to0} (\langle Z^n \rangle-1)/n$.

%\begin{comment}
The replicated partition function averaged over the disordered reads
\begin{multline}
\langle  Z^{n}\rangle= e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\nu a}\frac{dm_{\nu}^{a}}{\sqrt{2\pi}}
e^{ \frac{\beta}{2}\sum_{\nu=1}^{P}\sum_{a=1}^{n}\left(m_{\nu}^{a}\right)^{2}} \\ 
\times \left\langle \prod_{\nu a}\delta\left(m_{\nu}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\nu k}f_{ki}\right)s_{i}^{a}\right)\right\rangle _{c,f}\label{eq:GET_Z}
\end{multline}
where we introduced the set of auxiliary variables 
%\end{comment}
%As in \cite{amit1987statistical}, in order to compute the averaged partition function we introduce a set of auxiliary variables
\begin{equation}
    m_{\nu}^a=\frac{1}{N}\sum_{i}\xi_{\nu i}s_{i}^a, \quad a \in[n],\ \nu\in[P].
    %=\frac{1}{\sqrt{N}}\sum_{i}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\nu k}f_{ki}\right)s_{i}^a;
\end{equation}
We call these \emph{pattern magnetizations} to distinguish them from another set of order parameters, whose definition we anticipate here:
\begin{equation}
    \mu_{k}^a=\frac{1}{N}\sum_{i}f_{ki}s_{i}^a, \quad a \in[n],\ k\in[D].
    \label{eq:def_muab}
\end{equation}
We call these the \emph{factor magnetizations}. We want to see if there is a region of the  $\alpha_D$ vs $\alpha$ phase diagram where $\mu_k>0$ for some $k$. We also want to see what happens to the pattern magnetizations in the same phase diagram.

Similarly to~\cite{amit1987statistical}, we make some ansatz on the structure of the solution for both these order parameters. We study two cases:
%, respectively in section~\ref{sec:factor_retrieval} and section~\ref{sec:pattern_retrieval}
the case where the model retrieves only one of the factors, and the case where the model retrieves only one of the examples.

%\subsection{Retrieval of the factors}
%\label{sec:factor_retrieval}
\paragraph*{Factor retrieval.}

%\subsubsection{Gaussian equivalence}

In order to analyze the retrieval of one factor only we impose that $\mu_1=O(1)$ and $\mu_k=O(1/\sqrt{N})$ for $k>1$. At the same time we impose $m_\nu=O(1/\sqrt{N})\,,\;\forall \nu$. In the thermodynamic limit this means that we look for a solution of the form 
\begin{equation}
    \bmu = (\mu,0,...,0) \quad  \mathbf m = (0,...,0).
    \label{eq:factor_retrieval_state}
\end{equation}

Given this ansatz, in order to compute the average over the coefficients $c$ in eq.~\eqref{eq:GET_Z} we must separate the contribution $k=1$ from the rest with $k>1$.  Then,  we expand $\sigma$ around the summation over $k>1$,  obtaining the following expression for the delta function:

%This assumption allows us to use the Gaussian Equivalence Theorem (GET) \cite{mei2022generalization, gerace2020generalisation, goldt2022gaussian, goldt2020modeling, hu2022universality} to compute the averages over the components of the matrix $c_{\nu k}$. In order to apply this theorem we separate the contribution for $k=1$ from the second line of eq.~\eqref{eq:GET_Z}, which becomes

%\sum_{k>1}^{D}c_{\nu k}f_{ki}\
\begin{multline}
    \delta\left(m_{\nu}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\sum_{k>1}^{D}c_{\nu k}f_{ki}\right)s_{i}^{a}\right.\\
    \left.+\frac{c_{\nu1}}{\sqrt{\alpha_{D}}}\frac{1}{N}\sum_{i=1}^{N}\sigma'\left(\sum_{k>1}^{D}c_{\nu k}f_{ki}\right)f_{1i}s_{i}^{a}\right).
\end{multline}
Integrating this expression results in a distribution of $m_{\nu}^{a}$ that is a Gaussian $\mathcal{N}(m_{\nu}^{a};\bar{m},\Sigma)$ with mean 
\begin{equation}
   \bar{m}_{\nu}^{a}=\frac{c_{\nu1}}{\sqrt{\alpha_{D}}}\mu_{1}\kappa_{1} 
\end{equation}
and covariance matrix
\begin{equation}
\Sigma^{ab}=\kappa_{*}^{2}q^{ab}+\kappa_{1}^{2}p^{ab},
\end{equation}
where we defined the following quantities: 
\begin{align}
q^{ab}&=\frac{1}{N}\sum_{i}s_{i}^{a}s_{i}^{b}\label{eq:def_qab} \\
p^{ab}&=\frac{1}{D}\sum_{k>1}\mu_{k}^{a}\mu_{k}^{b}\label{eq:def_pab} %\\
%\mu_{k}^{a}&=\frac{1}{\sqrt{N}}\sum_{i}f_{ki}s_{i}^{a}\label{eq:def_muab}
\end{align}
and coefficients
$\kappa_{0}  =\int Dz\,\sigma(z)$,
$\kappa_{1}  =\int Dz\,z\sigma(z)$,
$\kappa_{2} =\int Dz\,\sigma^{2}(z)$,
$\kappa_{*}^{2} =\kappa_{2}-\kappa_{1}^{2}-\kappa_{0}^{2}$.
%Note that the factor magnetizations $\mu^a_k$ appearing in eq.~\eqref{eq:def_pab} must be multiplied for a factor $\sqrt{N}$ to be finite in the thermodynamic limit.  TODO: MENTION GET
This calculation goes under the name of Gaussian Equivalence Theorem (GET) and it has been developed in \cite{mei2022generalization, gerace2020generalisation, goldt2022gaussian, goldt2020modeling, hu2022universality,Baldassi2022Learning} and applied in cases with zero mean.
The replicated partition function now reads
\begin{multline}
    \left\langle Z^{n}\right\rangle =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\nu a}\frac{dm_{\nu}^{a}}{\sqrt{2\pi}}\\
    \times
    \exp\left\{\frac{\beta}{2}\sum_{\nu=1}^{P}\sum_{a=1}^{n}\left(m_{\nu}^{a}\right)^{2}\right\} \left\langle \prod_\nu\mathcal{N}(m_{\nu};\bar{m},\Sigma)\right\rangle _{c_{1},f}
\end{multline}
where $\left\langle ...\right\rangle $ represents the average over the remaining quenched disorder $f$ and $c_{1}=\{c_{\nu1}\}^P_{\nu=1}$.  

%We observe that the replicated partition function \eqref{eq:GET_Z} is quadratic in $m_{\nu}^{a}$ and therefore we will be able to integrate the variables $m_{\nu}^{a}$ right away.

We solve this model in the replica-symmetric (RS) ansatz. For the complete derivation see the appendix (section \ref{sec:fac_ret_full_calc}). At the end of the long but straightforward calculation we end up with a free energy $f^\mathrm{RS}$ that depends on eight order parameters: the factor magnetization $\mu$, the overlap between different replicas $q$, the diagonal and off-diagonal parts of $p^{ab}$ and their four conjugate parameters $\hat{\mu}$, $\hat{q}$, $\hat{p}_\text{d}$, $\hat{p}$.  
Given the control parameters $\beta$, $\alpha$ and $\alpha_D$, we obtain the physical value of the order parameters by extremizing the free energy:
\begin{equation}
    f^\mathrm{RS}_\text{opt} = \extr_{\mu, \hat{\mu}, q, \hat{q}, {p}_\text{d}, \hat{p}_\text{d}, p,\hat{p}} f^\mathrm{RS}(\mu, \hat{\mu}, q, \hat{q}, {p}_\text{d}, \hat{p}_\text{d}, p,\hat{p}).
\end{equation}
%The RS free energy reads
%\begin{widetext}
%\begin{align}
%f^\mathrm{RS}\left(q,\hat{q},p_{\text{d}},p,\hat{p}_{\text{\text{d}}},\hat{p},\mu,\hat{\mu}\right)= & \frac{\alpha}{2}-\frac{\alpha}{2}\beta\hat{q}(q-1)+\frac{\alpha}{2}\hat{p}_{\text{d}}p_{\text{d}}-\frac{\alpha}{2}\hat{p}p-\frac{\alpha_{T}}{2}\left(\hat{p}_{\text{d}}-\hat{p}\right)\mu^{2}+\hat{\mu}\mu\nonumber \\
%+ & \frac{\alpha}{2\beta}\left[\log\left[1-\beta\left(\Sigma_{\text{d}}-\Sigma\right)\right]-\frac{\beta\Sigma}{1-\beta\left(\Sigma_{\text{d}}-\Sigma\right)}\right]\nonumber \\
%+ & \frac{\alpha}{2\alpha_{T}\beta}\left[\log\left(1-\alpha_{T}\beta\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)\right)-\frac{\alpha_{T}\beta\left(\hat{p}+\hat{p}_{\text{d}}q-2q\hat{p}\right)}{1-\alpha_{T}\beta\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)}\right]\nonumber \\
%- & \frac{1}{\beta}\left\langle \int Dz\,\log\left[2\cosh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right]\right\rangle 
%\end{align}
%\end{widetext}

%\subsubsection{Saddle-point equations}


Deriving $f^\mathrm{RS}$ with respect to the order parameters we obtain  a set of eight equations that must be solved together (the so-called \emph{saddle-point equations}). We write here only three of them, leaving the rest to the appendix (see eq.~\eqref{eq:fac_saddle_point_eq}):
\begin{align}
q &=\left\langle \int Dz\,\tanh^{2}\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right\rangle _{f}\label{eq:saddle_hatq} \\
\mu &=\left\langle \int Dz\,f\tanh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right\rangle _{f}\label{eq:saddle_hatmu} \\
\hat{q}&=\frac{\kappa_{*}^{2}(\kappa_{1}^{2}p+\kappa_{*}^{2}q+\kappa_{1}^{2}\frac{\alpha_{T}}{\alpha}\mu^{2})}{(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1))^{2}}\notag\\
&+\frac{\hat{p}+\alpha_{T}\beta q(\hat{p}-\hat{p}_{\text{d}})^{2}}{\beta(\alpha_{T}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\label{eq:saddle_{q}}
\end{align}
where $Dz=dz\,e^{-z^2/2}/\sqrt{2\pi}$ is a compact notation for  Gaussians integration measures.
We can observe that the first two of these equations resemble closely the ones for $q$ and $m$ for the SHM (see \cite{amit1987statistical}): now $f$ has the role of the retrieved pattern and $\mu$ has the role of the magnetization. The major difference is that in our case the equation for the conjugate $\hat{q}$ is more complicated and depends on the rest of the order parameters. A minor difference is that inside the integrals of the first two equation, $\hat{\mu}$ appears instead of $\mu$. 
%This does not seem too relevant since $\mu$ and $\hat{\mu}$ are linearly proportional according to the saddle-point equation for $\hat{\mu}$ (see the appendix).

%Before solving iteratively these equations we perform limit $\beta \to \infty$, as it greatly simplifies the expressions. To still have finite order parameters in this limit we scale them as it follows:
%\begin{align}
%q & =1-\frac{\delta q}{\beta}\\
%p & =p_{d}-\frac{\delta p}{\beta}\\
%\hat{p} & =\beta\,\delta\hat{p}_{d}-\frac{1}{2}\delta\hat{p}\\
%\hat{p}_{d} & =\beta\,\delta\hat{p}_{d}+\frac{1}{2}\delta\hat{p}
%\end{align}
The solution to these equations in the limit $\beta \to \infty$ is shown in figure~\ref{fig:phase_diagram}: for $\alpha>\alpha^\text{crit}(\alpha_D)$ the factor magnetization becomes finite with a discontinuous jump, showing that the model is actually capable of storing the factors $f$ as attractors.
%(we checked numerically that the size of attraction basin is finite)
This jump is a first-order phase transition similarly to the catastrophic forgetting, but with the important difference that the magnetization becomes finite when $\alpha$ is \emph{larger} rather than smaller than a critical value. The critical point $\alpha^\text{crit}(\alpha_D)$ rapidly increases when $\alpha_D$ increases, up to the point where it diverges for $\alpha_D\simeq 0.138$. This critical value is numerically identical to the critical capacity of the SHM and it is not a coincidence. In fact, in the limit $P \gg N$, we have that the Hebbian coupling matrix becomes that of a SHM of just factors (see appendix~\ref{sec:recover_SHM} for the derivation):
\begin{equation}
    J_{ij} = \frac{1}{N} \sum_{\nu=1}^P \xi_{\nu i} \xi_{\nu j} \overset{P\to \infty}{\simeq} \kappa_1^2 \frac{1}{D} \sum_{k=1}^D f_{ik} f_{jk}.
\end{equation}
Therefore, the saddle-point equations of the HMHM must become identical to those of the SHM with $\mu$ playing the role of the magnetization and $f$ that of the retrieved patterns (the correct scalings for this limit and the explicit calculation are shown in the appendix, section \ref{sec:fac_zero_T_limit}). One way to look at this behaviour is to fix a value of $\alpha$ and to increase $\alpha_D$, thus moving horizontally in the phase diagram of figure~\ref{fig:phase_diagram}a: when $\alpha_D$ is low enough the model is able to retrieve the factors, then, when they become too many, the equivalent of a catastrophic forgetting happens. This transition happens at the Hopfield critical capacity only if $\alpha=\infty$, where the matching between the two models is perfect.

The comparison between this analytical solution and numerical simulations is shown in figure~\ref{fig:phase_diagram}b, where we find a very good agreement for $\alpha_D=0.03$. We test other ranges of $\alpha$ and $\alpha_D$ in the appendix and we find again good agreement (see figure \ref{fig:num_comp2_fac}).



%\subsection{Retrieval of the patterns}
%\label{sec:pattern_retrieval}



\paragraph*{Pattern retrieval.}
%\subsection{Retrieval of one pattern}

%\subsubsection{Gaussian equivalence}

For the second case we say that $m_1=O(1)$ and $m_\nu=O(1/\sqrt{N})$ for $\nu>1$. At the same time we impose that $\mu_k=O(1/\sqrt{N})\,,\;\forall k$. In the thermodynamic limit this means that we look for a solution of the form 
\begin{equation}
    \bmu = (0,...,0),\quad \mathbf  m = (m,0,...,0).
    \label{eq:pattern_retrieval_state}
\end{equation}

In this setting we must be careful to apply the GET only to the vanishing pattern magnetizations, leaving the terms involving $m_1$ as they are. The resulting expression of the average replicated partition function reads:

\begin{multline}
    \left\langle Z^{n}\right\rangle =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\nu a}\frac{dm_{\nu}^{a}}{\sqrt{2\pi}}\left\langle \prod_\nu\mathcal{N}(m_{\nu};0,\Sigma)\right.\\
    \times \exp\left\{ \frac{\beta}{2}\sum_{a=1}^{n}\left(m_{1}^{a}\right)^{2}+\frac{\beta}{2}\sum_{\nu>1}^{P}\sum_{a=1}^{n}\left(m_{\nu}^{a}\right)^{2}\right\} \\
 \times \left. \prod_{a}\delta\left(m_{1}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{1k}f_{ki}\right)s_{i}^{a}\right)\right\rangle _{\tilde{c}_{1},f}
\end{multline}
%Note that now the average over $f_{ki}$ appears both in the Gaussian distribution resulting from the application of the GET and in a term involving $m_1$, reminiscent of the standard Hopfield model calculation in \cite{amit1987statistical}.
where $\left\langle ...\right\rangle $ represents the average over the remaining quenched disorder $f$ and $\tilde{c}_{1}=\{c_{1k}\}^D_{k=1}$. 

As we did for the factor retrieval case, we solve the model within the RS ansatz and we report the complete calculation in the  (section \ref{sec:patt_ret_full_calc}). This time set the order parameters does not include $\mu$ and $\hat{\mu}$, but it does include $m$ (there is no need for a conjugate variable $\hat{m}$). The order parameters also include the auxiliary variables $t$, $\hat{t}$ that are needed to linearize a term in an intermediate integral. The definition of $t$ is $ t= \frac{1}{N} \sum_i^N \hat{v}_i s_i$
where $\hat{v}_i$ are the conjugate variables of the auxiliary variables $v_i = \frac{1}{\sqrt{D}}\sum_k^D c_{\nu k} f_{ki}$.
The auxiliary variables $v_i$ and $\hat{v}_i$ do not appear in the free energy because they can be integrated right away.
Summarizing, the set of nine order parameters is $m, q, \hat{q}, {p}_\text{d}, \hat{p}_\text{d}, p,\hat{p}, t, \hat{t}$. 

Again the equations for $m$, $q$ and $\hat{q}$ resemble the SHM ones in \cite{amit1987statistical}. We write them, as well as those for the other order parameters, in the appendix (see eq.~\eqref{eq:patt_saddle_point_eq}).

%Again we show here only how the equation for $m$, $q$ and $\hat{q}$ change from the standard case in \cite{amit1987statistical}, and we write the rest of them in the appendix:
\begin{comment}
\begin{align}
q = \int & Dv Dx \notag\\
& \times \tanh^2 \left[\beta\left(v \hat{t}+\sigma(v)\, m + \sqrt{\alpha \, \hat{q}-\hat{t}^2}\,x \right)  \right] \label{eq:saddle_q_2}
\end{align}
\begin{align}
m = \int & Dv  Dx \,\sigma (v) \notag\\
& \times \tanh \left[\beta \left(v \hat{t}+\sigma (v)\, m +\sqrt{\alpha \hat{q}-\hat{t}^2}\,x \right) \right] \label{eq:saddle_m_2}
\end{align}
\begin{align}
\hat{q} &= \frac{\alpha_T}{\alpha_D} \, \frac{t^2 \, \hat{p}^2}{(1-\beta \,\alpha_T \, q\,\hat{p})^2}\notag\\
&+\frac{\kappa_*^2 (\kappa_*^2 \,q +\kappa_1^2 \, p)}{\left[1-\beta \left( \kappa_*^2 (1-q)+ \kappa_1^2 (p_d-p)\right)\right]^2}\notag\\
&+\frac{\hat{p}+\beta \alpha_T \, q (\hat{p}_d -\hat{p})^2}{\beta \left[1-\beta \alpha_T (1-q)(\hat{p}_d -\hat{p}) \right]^2 \label{eq:saddle_qhat_2}}
\end{align}
\end{comment}

We solve these equations in the limit $\beta\to\infty$ and we show the results in figure~\ref{fig:phase_diagram}a. A useful limit to consider is $\alpha_D\to\infty$: in this limit the equations converge to the SHM ones (see appendix, section \ref{sec:limit_aD_inf}), which was expected since the examples become uncorrelated. This produces an horizontal asymptote at $\alpha\simeq0.138$ for the spinodal line of $m$.  Decreasing $\alpha_D$ the example patterns become more correlated and the catastrophic forgetting happens at a lower value of $\alpha$, until it happens at $\alpha=0$ for $\alpha_D\to0$.

The comparison between this analytical solution and numerical simulations is shown in figure~\ref{fig:num_comp}: we find that, as we move from the $\alpha_D\gg1$ regime (where we know that the simulations must match the SHM theory), the catastrophic forgetting happens at a value of $\alpha$ lower than the predicted one; furthermore, the mismatch increases for lower values of $\alpha_D$ (see also figures~\ref{fig:num_comp3_patt} in the appendix). This last fact suggests that strong correlations might be responsible of a failure of the RS ansatz. In fact, in \cite{amit1987statistical}, the authors found that the correct ansatz at zero temperature is indeed the full-replica-symmetry-breaking one, but the corrections to the RS calculations are small in their model. To support this hypothesis, we checked the entropy of our solution and we found that it becomes more negative the smaller the value of $\alpha_D$ (see figure~\ref{fig:negative_entropy}a in the appendix). We also ruled out a possible inconsistency of the ansatz in eq.~\eqref{eq:pattern_retrieval_state}(see figure~\ref{fig:negative_entropy}b in the appendix).

\begin{figure}
    \centering
    \includegraphics[trim={0.5cm 0.5cm 0.0cm 0.0cm},width=0.49\textwidth,clip]{fig_3A.pdf}
    \caption{Comparison with numerical results for the retrieval of one pattern. Each pixel represents the mean pattern magnetization for given values of $\alpha$ and $\alpha_D$, averaged over 25 samples of size $N=2000$. The simulations are performed initializing the model to a pattern $\bxi_\nu$, running the update rule~\eqref{eq:update_rule}, then measuring $m_\nu$ at convergence.  }
    \label{fig:num_comp}
\end{figure} 

\paragraph*{Learning transition.}
%\subsection{Learning transitions}

Summing up the results, we have a phase diagram with two transition lines demarking three regions (see figure~\ref{fig:phase_diagram}a): the factor retrieval region, for which we obtain a non-zero factor magnetization  solution to saddle point  ~\eqref{eq:factor_retrieval_state}; the pattern retrieval region, for which we have non-zero pattern magnetization solutions for eq.~\eqref{eq:pattern_retrieval_state}; a spin-glass region between the two.
The behaviour that we call \emph{learning transition} can be observed following a vertical line in the phase diagram, namely fixing a value of $\alpha_D$ and increasing $\alpha$. Starting from small $\alpha$ we obtain a model of storage of correlated patterns: the capacity is smaller than the uncorrelated case, but the phenomenology is similar since there is a maximum number of patterns that can be stored, and attempting to store a larger number results in catastrophic forgetting. The surprising result is that, when we have $\alpha_D\leq0.138$ (i.e. when the correlations are strong enough), if we keep increasing the number of patterns we find another phase beyond the spin-glass one. In this new phase attractors corresponding to the factors $\mathbf{f}_k$ appear. If we interpret the patterns $\bxi_\mu$ as an unsupervised training dataset, we see that if the dataset is big enough the model is capable of inferring the factors hidden in the data. This behaviour resembles the feature extraction that deep neural networks and some shallow generative models perform
\cite{hinton2006fast, hinton2006reducing, tubiana2017emergence, krotov2016dense}. 
Our model represents an extension to the classical Hopfield settings that while
being amenable to theoretical analysis, can potentially capture the phenomenology of much more complex architectures, similarly to what the hidden-manifold model does for the 
supervised learning phenomenology \cite{goldt2020modeling}. It could be also interesting to extend the analysis proposed in this work to modern versions of the Hopfield model, such as the super-linear capacity ones introduced in Refs. \cite{krotov2016dense, ramsauer2020hopfield}.

% This similarity raises the question if the learning transition described in this work could be relevant in understanding deep architectures such as the modern Hopfield network \cite{ramsauer2020hopfield} or other attention-based models.

\paragraph*{Acknowledgements.} We thank Marc Mézard for many useful comments and discussions. MN acknowledges the support of LazioInnova - Regione Lazio under the program Gruppi di ricerca 2020 - POR FESR Lazio 2014-2020, Project NanoProbe (Application code A0375-2020-36761).

%\bibliographystyle{unsrt}
\bibliography{references}



\appendix
\onecolumngrid

\section{Recovering standard Hopfield model}
\label{sec:recover_SHM}
Consider a generic activation function $\sigma(z)$. For large number
of examples $P$ and a rotationally invariant distribution $P(\mathbf{c})$,
the HMHM couplings become

\begin{eqnarray}
J_{ij} & = & \frac{1}{P}\sum_{\mu}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k}c_{k\mu}f_{ki}\right)\sigma\left(\frac{1}{\sqrt{D}}\sum_{k}c_{k\mu}f_{kj}\right)\\
 & = & \frac{1}{P}\sum_{\mu}\sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}\mathbf{c}_{\mu}\cdot\mathbf{f}_{i}}}\right)\sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}\mathbf{c}_{\mu}\cdot\mathbf{f}_{j}}}\right)\\
 & \approx & \int dP(\mathbf{\mathbf{c}})\ \sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}}\ensuremath{\mathbf{c}\cdot\mathbf{f}_{i}}}\right)\sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}}\ensuremath{\mathbf{c}\cdot\mathbf{f}_{j}}}\right)\\
 & = & r\left(\frac{1}{D}\mathbf{f}_{i}\cdot\mathbf{f}_{j}\right).
\end{eqnarray}

Where in the last line we used rotational invariance to express the
coupling as a function of the scalar product among the two couplings.
The function $r(z)$ depends on the ensemble considered and on the
activation function. Notice that if $r(z)\approx az$ for small argument
we recover the standard Hopfield model, up to a prefactor that can
be reabsorbed in the temperature. 

We show that standard Hopfield is indeed the large $P$ limit in the
case of Gaussian $\mathbf{c}$ and antisymmetric and non-decreasing
activation functions. In fact we have

\begin{eqnarray}
J_{ij} & \approx & \int d\mathcal{N}(\mathbf{\mathbf{c}})\ \sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}}\ensuremath{\mathbf{c}\cdot\mathbf{f}_{i}}}\right)\sigma\left(\text{\ensuremath{\frac{1}{\sqrt{D}}}\ensuremath{\mathbf{c}\cdot\mathbf{f}_{j}}}\right)\\
 & = & \int\frac{dud\hat{u}}{(2\pi)^{2}}\ \sigma\left(u\right)\sigma\left(v\right)\exp\left\{-i\hat{u}u-i\hat{v}v-\frac{1}{2D}\hat{u}^{2}\lVert\mathbf{f}_{i}\rVert^{2}-\frac{1}{2D}\hat{v}^{2}\lVert\mathbf{f}_{j}\rVert^{2}-\frac{1}{D}\hat{u}\hat{v}\mathbf{f}_{i}\cdot\mathbf{f}_{j}\right\}
\end{eqnarray}
Considering independently distributed factor vectors, we assume $\lVert\mathbf{f}_{i}\rVert^{2}=D$,
$\lVert\mathbf{f}_{j}\rVert^{2}=D$, $\mathbf{f}_{i}\cdot\mathbf{f}_{j}=O(\sqrt{D})$,
therefore we can expand to the first order in the small interaction
term and obtain

\begin{equation}
J_{ij}\approx\kappa_1^{2}\frac{1}{D}\mathbf{f}_{i}\cdot\mathbf{f}_{j}=\kappa_1^{2}\frac{1}{D}\sum_{k=1}^{D}f_{ki}f_{kj}
\end{equation}
where we recognized
\begin{equation}
\int Dz\ \sigma'(z) = \int Dz\ z \sigma(z) = \kappa_1
\end{equation}

The matrix $J$ has therefore an Hopfield structure with $D$ stored
patterns. 
 
\section{Retrieval of one factor: replica-symmetric calculation}
\label{sec:fac_ret_full_calc}

We expand $\sigma(...)$
to the first order around the rest of the summation $\frac{1}{\sqrt{D}}\sum_{k>1}^{D}c_{\mu k}f_{ki}$:
\begin{align}
 & \delta\left(m_{\mu}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k=1}^{D}c_{\mu k}f_{ki}\right)s_{i}^{a}\right)\\
\simeq & \delta\left(m_{\mu}^{a}-\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\sigma\left(\frac{1}{\sqrt{D}}\sum_{k>1}^{D}c_{\mu k}f_{ki}\right)s_{i}^{a}+\sum_{i=1}^{N}\sigma'\left(\frac{1}{\sqrt{D}}\sum_{k>1}^{D}c_{\mu k}f_{ki}\right)\frac{1}{\sqrt{\alpha_{D}}}\frac{1}{N}c_{\mu1}f_{1i}s_{i}^{a}\right)
\end{align}
The last term will give a finite fist
moment to the distrubution of $m_{\mu}^{a}$. This means that we can
apply the GET to compute the covariance matrix of the distribution
$P(m_{\mu}^{a})$:
\begin{equation}
P(m_{\mu}^{a})=\mathcal{N}(m_{\mu}^{a};\bar{m},\Sigma)
\end{equation}
where the covariance is 
\begin{equation}
\Sigma^{ab}=\kappa_{*}^{2}q^{ab}+\kappa_{1}^{2}p^{ab}
\end{equation}
and the mean is
\begin{align}
\bar{m}_{\mu}^{a} & =\frac{c_{\mu1}}{\sqrt{\alpha_{D}}}\mu_{1}^{a}\kappa_{1}
\end{align}
The order parameters that are involved in the covariance are those
for $k>1$
\begin{equation}
q^{ab}=\frac{1}{N}\sum_{i}s_{i}^{a}s_{i}^{b}\label{eq:def2_qab}
\end{equation}
\begin{equation}
p^{ab}=\frac{1}{D}\sum_{k>1}\mu_{k}^{a}\mu_{k}^{b}\label{eq:def2_pab}
\end{equation}

\begin{equation}
\mu_{k}^{a}=\frac{1}{\sqrt{N}}\sum_{i}f_{ki}s_{i}^{a}\label{eq:def2_muab}
\end{equation}
while the mean involves the variable 
\begin{equation}
   \mu_{1}^{a}=\frac{1}{N}\sum_{i}f_{1i}s_{i}^{a} 
\end{equation}

After sending $m_{\mu}^{a}\to m_{\mu}^{a}+\bar{m}_{\mu}^{a}$ the
partition function reads:

\begin{align}
\left\langle \left\langle Z^{n}\right\rangle \right\rangle  & =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{\mu a}\frac{dm_{\mu}^{a}}{\sqrt{2\pi}}\prod_{ab}\frac{dq^{ab}d\hat{q}^{ab}}{2\pi}\prod_{ab}\frac{dp^{ab}d\hat{p}^{ab}}{2\pi}\prod_{ka}\frac{d\mu_{k}^{a}\hat{\mu}_{k}^{a}}{2\pi}\prod_{1a}\frac{d\mu_{1}^{a}\hat{\mu}_{1}^{a}}{2\pi}\cdot\nonumber \\
 & \cdot\left\langle \exp\left\{ +\frac{\beta}{2}\sum_{\mu a}\left(m_{\mu}^{a}+\bar{m}_{\mu}^{a}\right)^{2}\right\} \prod_{\mu}\frac{1}{\sqrt{\det\Sigma}}\exp\left\{ -\frac{1}{2}\sum_{ab}m_{\mu}^{a}(\Sigma^{-1})^{ab}m_{\mu}^{b}\right\} \right.\cdot\nonumber \\
 & \cdot\prod_{a<b}\exp\left\{ i\alpha N\hat{q}^{ab}\left(q^{ab}-\frac{1}{N}\sum_{i}s_{i}^{a}s_{i}^{b}\right)\right\} \cdot\nonumber \\
 & \cdot\prod_{a\leq b}\exp\left\{ i\alpha N\hat{p}^{ab}\left(p^{ab}-\frac{1}{D}\sum_{k>1}\mu_{k}^{a}\mu_{k}^{b}\right)\right\} \cdot\nonumber \\
 & \left.\cdot\prod_{k>1,a}\exp\left\{ i\hat{\mu}_{k}^{a}\left(\mu_{k}^{a}-\frac{1}{\sqrt{N}}\sum_{i}f_{ki}s_{i}^{a}\right)\right\} \prod_{a}\exp\left\{ i\hat{\mu}_{1}^{a}\left(\mu_{1}^{a}-\frac{1}{N}\sum_{i}f_{1i}s_{i}^{a}\right)\right\} \right\rangle _{c_{\mu1},f_{ki}}
\end{align}


\subsection{Average over $c_{\mu1}$}

Average over $c_{\mu1}$:

\begin{align}
 & \prod_{\mu}\left\langle \exp\left\{ \frac{\beta}{2}\sum_{a}\left(m_{\mu}^{a}+c_{\mu1}\frac{\mu_{1}^{a}\kappa_{1}}{\sqrt{\alpha_{D}}}\right)^{2}\right\} \right\rangle _{c_{\mu1}}\nonumber \\
= & \prod_{\mu}\exp\left\{ \frac{\beta}{2}\sum_{a}m_{a\mu}^{2}\right\} \left\langle \exp\left\{ c_{\mu1}\left(\beta\sum_{a}m_{a\mu}\left[\frac{\mu_{1}^{a}\kappa_{1}}{\sqrt{\alpha_{D}}}\right]\right)+\frac{1}{2}c_{\mu1}^{2}\left(\beta\sum_{a}\left[\frac{\mu_{1}^{a}\kappa_{1}}{\sqrt{\alpha_{D}}}\right]^{2}\right)\right\} \right\rangle _{c_{\mu1}}\nonumber \\
= & \prod_{\mu}\exp\left\{ \frac{\beta}{2}\sum_{a}m_{a\mu}^{2}\right\} \frac{1}{\sqrt{1-\beta\frac{\kappa_{1}^{2}}{\alpha_{D}}\sum_{a}\mu_{a1}^{2}}}\exp\left\{ \frac{\beta^{2}}{2}\frac{\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(\sum_{a}m_{a\mu}\mu_{a1}\right){}^{2}}{1-\beta\frac{\kappa_{1}^{2}}{\alpha_{D}}\sum_{a}\mu_{a1}^{2}}\right\} 
\end{align}

TODO: the square root term gives a relevant (namely $O(n)$) contribution
to the free energy. It depends on which kind of replica symmetry we
consider. In the RS ansatz is just
\[
\psi=\exp\left\{ n\frac{\beta}{2}\frac{\kappa_{1}^{2}}{\alpha_{D}}\mu_{1}^{2}+O(n^{2})\right\} 
\]


\subsection{Integrate the \emph{pattern} magnetizations}

We can integrate the magnetizations:
\begin{align}
 & \int\prod_{a}dm_{a\mu}\exp\left\{ \frac{\beta}{2}\sum_{a}m_{a\mu}^{2}\right\} \exp\left\{ \frac{\beta^{2}}{2}\frac{\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(\sum_{a}m_{a\mu}\mu_{a1}\right){}^{2}}{1-\beta\frac{\kappa_{1}^{2}}{\alpha_{D}}\sum_{a}\mu_{a1}^{2}}\right\} \frac{1}{\sqrt{\det\Sigma}}\exp\left\{ -\frac{1}{2}\sum_{ab}m_{a\mu}(\Sigma^{-1})_{ab}m_{b\mu}\right\} \nonumber \\
= & \frac{1}{\sqrt{\det\Sigma}}\int\prod_{a}dm_{a\mu}\exp\left\{ -\frac{1}{2}\left(\sum_{ab}m_{a\mu}\left(-\beta\delta_{ab}-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\mu_1^a \mu_1^b +(\Sigma^{-1})_{ab}\right)m_{b\mu}\right)\right\} \nonumber \\
= & \frac{1}{\sqrt{\det\left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)}}
\end{align}
where $C=1-\beta\frac{\kappa_{1}^{2}}{\alpha_{D}}\sum_{a}\mu_{a1}^{2}$.
We also dropped the term $1/\sqrt{C}$ since $C\to1$ when $n\to0$
(we could drop everywhere but for now we keep it for clarity). We
also defined a new variable 
\[
p_{1}^{ab}=\mu_{1}^{a}\mu_{1}^{b}
\]
Since this term is factorized w.r.t the $\mu$ index, the contribution
in the free energy reads

\begin{equation}
\phi_{0}=\exp\left\{ -\frac{\alpha N}{2}\log\det\left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)\right\} \label{eq:phi_0}
\end{equation}


\subsection{Split of the partition function}

We observe now that the partition function is already splitted between
the terms $k>1$ and $k=1$ (we include the definition of $p_{1}^{ab}$):
\begin{align}
\left\langle \left\langle Z^{n}\right\rangle \right\rangle  & =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{ab}\frac{dq^{ab}d\hat{q}^{ab}}{2\pi}\prod_{ab}\frac{dp^{ab}d\hat{p}^{ab}}{2\pi}\prod_{ka}\frac{d\mu_{k}^{a}\hat{\mu}_{k}^{a}}{2\pi}\prod_{1a}\frac{d\mu_{1}^{a}\hat{\mu}_{1}^{a}}{2\pi}\cdot\nonumber \\
 & \cdot\left\langle \exp\left\{ \alpha N\psi-\frac{\alpha N}{2}\log\det\left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)\right\} \right.\cdot\nonumber \\
 & \cdot\exp\left\{ -\frac{\alpha N}{2}\sum_{a\ne b}\hat{q}^{ab}q^{ab}+\frac{\alpha}{2}\sum_{a\ne b}\hat{q}^{ab}\sum_{i}s_{i}^{a}s_{i}^{b}\right\} \cdot\nonumber \\
 & \cdot\exp\left\{ -\frac{\alpha N}{2}\sum_{ab}\hat{p}^{ab}p^{ab}+\frac{\alpha_{T}}{2}\sum_{ab}\hat{p}^{ab}\sum_{k>1}\mu_{k}^{a}\mu_{k}^{b}\right\} %\exp\left\{ -\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}p_{1}^{ab}+\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}\mu_{1}^{a}\mu_{1}^{b}\right\} \cdot
 \nonumber \\
 & \left.\cdot\exp\left\{ i\sum_{k>1,a}\hat{\mu}_{k}^{a}\mu_{k}^{a}-\frac{1}{\sqrt{N}}i\sum_{k>1,a}\hat{\mu}_{k}^{a}\sum_{i}f_{ki}s_{i}^{a}\right\} \exp\left\{ i\sum_{a}\hat{\mu}_{1}^{a}\mu_{1}^{a}-\frac{1}{N}i\sum_{a}\hat{\mu}_{1}^{a}\sum_{i}f_{1i}s_{i}^{a}\right\} \right\rangle _{f_{ki}}
\end{align}


\subsection{Integrate the \emph{factor} magnetizations }

We write all the parts that depend on $\left\{ \hat{\mu}_{k}^{a}\mu_{k}^{a}\right\} _{k>1}$
in one integral:

\begin{align}
Z_{1} & =\int\prod_{ka}\frac{d\mu_{k}^{a}d\hat{\mu}_{k}^{a}}{2\pi}\exp\left\{ \frac{\alpha_{T}}{2}\sum_{ab}\hat{p}^{ab}\sum_{k}\mu_{k}^{a}\mu_{k}^{b}+i\sum_{ak}\hat{\mu}_{k}^{a}\mu_{k}^{a}\right\} \left\langle \exp\left\{ -\frac{1}{\sqrt{N}}i\sum_{ka}\hat{\mu}_{k}^{a}\sum_{i}f_{ki}s_{i}^{a}\right\} \right\rangle _{f_{ki}}\label{eq:Z1}
\end{align}
First, we compute the disorder average on the $k>1$ terms:

\begin{align}
\left\langle \prod_{k>1,a}\exp\left\{ -\frac{i}{\sqrt{N}}\hat{\mu}_{k}^{a}\sum_{i}f_{ki}s_{i}^{a}\right\} \right\rangle  & =\prod_{k>1,i}\int df_{ki}\exp\left\{ -\frac{1}{2}f_{ki}^{2}-\frac{i}{\sqrt{N}}f_{ki}\sum_{a}\hat{\mu}_{k}^{a}s_{i}^{a}\right\} \nonumber \\
 & =\exp\left\{ -\frac{1}{2}\sum_{ab} \left(\sum_{k>1}\hat{\mu}_{k}^{a}\hat{\mu}_{k}^{b} \right) \left(\frac{1}{N}\sum_{i}s_{i}^{a}s_{i}^{b} \right)\right\} 
 \label{eq:av_f_Z1}
\end{align}
Then we recognize that the integral in equation \eqref{eq:Z1} is factorized
over $k>1$, so and we can write $Z_{1}=e^{\alpha_{D}N\phi_{1}}$,
with
\[
\phi_{1}=\log\int\prod_{a}\frac{d\mu^{a}d\hat{\mu}^{a}}{2\pi}\exp\left\{ i\sum_{a}\hat{\mu}^{a}\mu^{a}-\frac{1}{2}\sum_{ab}q^{ab}\hat{\mu}^{a}\hat{\mu}^{b}+\frac{\alpha_{T}}{2}\sum_{ab}\hat{p}^{ab}\mu^{a}\mu^{b}\right\} 
\]
where we also inserted the definition of $q^{ab}$ in equation \eqref{eq:av_f_Z1}
and we have included for simplicity the diagonal term $q^{aa}=1$
in the summation $\frac{1}{2}\sum_{ab}q^{ab}\hat{\mu}^{a}\hat{\mu}^{b}$.
At this point we can integrate over the variables $\hat{\mu}^{a},\mu^{a}$:
\begin{align}
\phi_{1} & =\log\int\prod_{a}\frac{d\mu^{a}}{\sqrt{2\pi}}\exp\left\{ \frac{\alpha_{T}}{2}\sum_{ab}\hat{p}\mu^{a}\mu^{b}\right\} \int\prod_{a}\frac{d\hat{\mu}^{a}}{\sqrt{2\pi}}\exp\left\{ i\sum_{a}\hat{\mu}^{a}\mu^{a}-\frac{1}{2}\sum_{ab}\hat{\mu}^{a}\hat{\mu}^{b}q^{ab}\right\} \nonumber \\
 & =-\frac{1}{2}\log\det\left(\mathbb{I}-\frac{\alpha}{\alpha_{D}}q\hat{p}\right)
\end{align}


\subsection{Manipulation before RS ansatz}

Now the partition function reads 

\begin{align}
\left\langle \left\langle Z^{n}\right\rangle \right\rangle  & =e^{-\frac{\beta}{2}Pn}\sum_{\left\{ s_{i}^{a}\right\} }\int\prod_{ab}\frac{dq^{ab}d\hat{q}^{ab}}{2\pi}\prod_{ab}\frac{dp^{ab}d\hat{p}^{ab}}{2\pi}%\prod_{ab}\frac{dp_{1}^{ab}d\hat{p}_{1}^{ab}}{2\pi}
\prod_{a}\frac{d\mu_{1}^{a} d\hat{\mu}_{1}^{a}}{2\pi}\cdot\nonumber \\
 & \cdot\left\langle \exp\left\{ \alpha N\psi-\frac{\alpha N}{2}\log\det\left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)-\frac{\alpha_{D}N}{2}\log\det\left(\mathbb{I}-\frac{\alpha}{\alpha_{D}}q\hat{p}\right)\right\} \right.\cdot\nonumber \\
 & \cdot\exp\left\{ -\frac{\alpha N}{2}\sum_{a\ne b}\hat{q}^{ab}q^{ab}-\frac{\alpha N}{2}\sum_{ab}\hat{p}^{ab}p^{ab}+\frac{\alpha}{2}\sum_{a\ne b}\hat{q}^{ab}\sum_{i}s_{i}^{a}s_{i}^{b}\right\} \cdot\nonumber \\
 & \left.\cdot\exp\left\{ %-\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}p_{1}^{ab}+\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}\mu_{1}^{a}\mu_{1}^{b}+
 i\sum_{a}\hat{\mu}_{1}^{a}\mu_{1}^{a}-\frac{1}{N}i\sum_{a}\hat{\mu}_{1}^{a}\sum_{i}f_{1i}s_{i}^{a}\right\} \right\rangle _{f_{ki}}
\end{align}
Now we rearrange terms moving all the terms that depend on the spins
to the last line. We also do the scaling $\hat{\mu}_{1}^{a}\to iN\hat{\mu}_{1}^{a}$
:
\begin{align}
\left\langle \left\langle Z^{n}\right\rangle \right\rangle  & =e^{-\frac{\beta}{2}Pn}\int\prod_{a<b}\frac{dq^{ab}d\hat{q}^{ab}}{2\pi}\prod_{a\le b}\frac{dp^{ab}d\hat{p}^{ab}}{2\pi}%\prod_{ab}\frac{dp_{1}^{ab}d\hat{p}_{1}^{ab}}{2\pi}
\prod_{a}\frac{d\mu_{1}^{a}d\hat{\mu}_{1}^{a}}{2\pi}\cdot\nonumber \\
 & \cdot\exp\left\{ \alpha N\psi-\frac{\alpha N}{2}\sum_{a\ne b}\hat{q}^{ab}q^{ab}-\frac{\alpha N}{2}\sum_{ab}\hat{p}^{ab}p^{ab}%-\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}p_{1}^{ab}+\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}\mu_{1}^{a}\mu_{1}^{b}
 -N\sum_{a}\hat{\mu}_{1}^{a}\mu_{1}^{a}\right\} \cdot\label{eq:Z_preRS}\\
 & \cdot\exp\left\{ -\frac{\alpha N}{2}\log\det\left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)-\frac{\alpha_{D}N}{2}\log\det\left(\mathbb{I}-\frac{\alpha}{\alpha_{D}}q\hat{p}\right)\right\} \nonumber \\
 & \cdot\left\langle \sum_{\left\{ s_{i}^{a}\right\} }\exp\left\{ \frac{\alpha}{2}\sum_{a\ne b}\hat{q}^{ab}\sum_{i}s_{i}^{a}s_{i}^{b}+\sum_{a}\hat{\mu}_{1}^{a}\sum_{i}f_{1i}s_{i}^{a}\right\} \right\rangle _{f_{ki}}
\end{align}
The last line is the equivalent term present in the standard Hopfield
model and we call it

\begin{align}
Z_{2} & \equiv\left\langle \sum_{\left\{ s_{i}^{a}\right\} }\exp\left\{ \frac{\alpha}{2}\sum_{a\ne b}\hat{q}^{ab}\sum_{i}s_{i}^{a}s_{i}^{b}+\sum_{a}\hat{\mu}_{1}^{a}\sum_{i}f_{1i}s_{i}^{a}\right\} \right\rangle _{f_{ki}}\nonumber \\
 & =\exp\left\{ N\phi_{2}\right\} 
\end{align}
Where we have factorized over the site index $i$ and defined 
\begin{equation}
\phi_{2}\equiv\log\left\langle \sum_{\left\{ s^{a}\right\} }\exp\left\{ \frac{\alpha}{2}\sum_{a\ne b}\hat{q}^{ab}s^{a}s^{b}+\sum_{a}\hat{\mu}_{1}^{a}f_{1}s^{a}\right\} \right\rangle _{f_{k}}
\end{equation}
Notice the abuse of notation since $\Sigma$ here means the matrix
and later it means the non-diagonal elements of the same matrix.

\subsection{RS ansatz}

We make an RS ansatz for all the order parameters:
\begin{subequations}
    \begin{align}        \mu_{k'}^{a}&=\mu_{k'}\\
    \hat{\mu}_{k'}^{a}&=\hat{\mu}_{k'} \\
    q^{ab}&=\delta^{ab}+q(1-\delta^{ab})\\
    \hat{q}^{ab}&=\delta^{ab}+\hat{q}(1-\delta^{ab})\\
    p^{ab}&=p_{\text{d}}\delta^{ab}+p(1-\delta^{ab})\\    \hat{p}^{ab}&=\hat{p}_{\text{d}}\delta^{ab}+\hat{p}(1-\delta^{ab})    
    %(p_{1}){}_{ab}=p_{1}^{\text{d}}\delta_{ab}+p_{1}\left(1-\delta_{ab}\right)\quad(\hat{p}_{1}){}_{ab}=\hat{p}_{1}^{\text{d}}\delta_{ab}+\hat{p}_{1}\left(1-\delta_{ab}\right)
    \end{align}
\end{subequations}
For convenience we also define 
\begin{equation}
\Sigma_{ab}=\Sigma^{\text{d}}\delta_{ab}+\Sigma\left(1-\delta_{ab}\right)
\end{equation}
where
\begin{align*}
\Sigma_{\text{d}} & =\kappa_{*}^{2}+\kappa_{1}^{2}p_{\text{d}}\\
\Sigma & =\kappa_{*}^{2}q+\kappa_{1}^{2}p
\end{align*}


\subsection{Manipulation of $\phi_{2}$}

The RS ansatz allows us to linearise the term $s^{a}s^{b}$ (now it
becomes important to specify that $a\neq b$):

\begin{align}
\phi_{2}= & \log\left\langle \exp\left\{ -\frac{1}{2}\alpha\hat{q}n\right\} \sum_{\left\{ s^{a}\right\} }\exp\left\{ \frac{1}{2}\alpha\hat{q}\left(\sum_{a}s^{a}\right)^{2}+\sum_{a}\hat{\mu}_{1}f_{1}s^{a}\right\} \right\rangle \nonumber \\
= & -\frac{1}{2}\alpha\hat{q}n+\log\left\langle \sum_{\left\{ s^{a}\right\} }\int Dz\,\exp\left\{ \left(z\sqrt{\alpha\hat{q}}+\hat{\mu}_{1}f_{1}\right)\sum_{a}s^{a}\right\} \right\rangle \nonumber \\
= & -\frac{1}{2}\alpha\hat{q}n+n\left\langle \int Dz\,\log\left[2\cosh\left(z\sqrt{\alpha\hat{q}}+\hat{\mu}_{1}f_{1}\right)\right]\right\rangle 
\end{align}


\subsection{New version of the first determinant}

We can compute explicitly the determinant term in eq.$\,$\eqref{eq:phi_0}
once we make the RS ansatz:

\begin{equation}
D_{1}=\det\left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)
\end{equation}

The product results: 
\begin{equation}
\left(p_{1}\Sigma\right)_{ab}= \mu_1^2\left(\left(n-1\right)\Sigma+\Sigma^{\text{d}}\right)
\end{equation}
and the complete matrix therefore reads:
\begin{align}
M_{ab}= & \left(\mathbb{I}-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}p_{1}\Sigma\right)_{ab}=\nonumber \\
= & \left[1-\beta(\Sigma^{\text{d}} - \Sigma)\right]\delta_{ab}- \beta \Sigma + \mu_1^2\left(\left(n-1\right)\Sigma+\Sigma^{\text{d}}\right)
\end{align}
For a RS matrix the following holds:
\begin{equation}
\log\det X^{ab}=n\log\left[x_{\text{d}}-x\right]+n\frac{x}{x_{\text{d}}-x}+O(n^{2})\label{eq:det_RS}
\end{equation}
Given this formula, it is useful to first compute the term $x_{\text{d}}-x$:
\begin{align}
M_{\text{d}}-M %&=1-\beta\Sigma^{\text{d}}-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(\left(n-1\right)p_{1}\Sigma+p_{1}^{\text{d}}\Sigma^{\text{d}}\right)+\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(\left(n-2\right)p_{1}\Sigma+p_{1}^{\text{d}}\Sigma+p_{1}\Sigma^{\text{d}}\right)\nonumber \\
&=1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)%-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(p_{1}^{\text{d}}-p_{1}\right)\left(\Sigma^{\text{d}}-\Sigma\right)+O(n)
\end{align}
Putting it back we get:
\begin{align}
\log\det M_{ab}= & n\log\left[1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)%-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(p_{1}^{\text{d}}-p_{1}\right)\left(\Sigma^{\text{d}}-\Sigma\right)
\right]%+\nonumber \\& 
+n\frac{-\beta\Sigma-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}} \mu_1^2\left(\Sigma^{\text{d}} - \Sigma\right)}{1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)%-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(p_{1}^{\text{d}}-p_{1}\right)\left(\Sigma^{\text{d}}-\Sigma\right)
 }+O(n^{2})
\end{align}


\subsection{Second determinant}

\begin{align}
D_{2} & =\log\det\left(\delta^{ab}-\alpha_{T}\sum_{c}q^{ac}\hat{p}^{cb}\right)
\end{align}
Where the matrix product reads
\begin{equation}
\delta^{ab}-\alpha_{T}\sum_{c}q^{ac}\hat{p}^{cb}=\left(1-\alpha_{T}\left(\left(n-1\right)q\hat{p}+\hat{p}_{\text{d}}\right)\right)\delta^{ab}-\alpha_{T}\left(\left(n-2\right)q\hat{p}+\hat{p}_{\text{d}}q+\hat{p}\right)\left(1-\delta^{ab}\right)\label{eq:M2}
\end{equation}
By plugging eq.$\,$\eqref{eq:M2} in eq.$\,$\eqref{eq:det_RS} we get
\begin{align}
D_{2} & =n\log\left[1-\alpha_{T}\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)\right]-n\frac{\alpha_{T}\left(\hat{p}+\hat{p}_{\text{d}}q-2q\hat{p}\right)}{1-\alpha_{T}\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)}+O(n^{2})
\end{align}


\subsection{Interaction term }

Let us now write the remaing terms (namely the second line of eq.$\,$\eqref{eq:Z_preRS})
by plugging the RS order parameters:
\begin{align}
 & \alpha N\psi-\frac{\alpha N}{2}\sum_{a\ne b}\hat{q}^{ab}q^{ab}-\frac{\alpha N}{2}\sum_{ab}\hat{p}^{ab}p^{ab}%-\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}p_{1}^{ab}+\frac{\alpha N}{2}\sum_{ab}\hat{p}_{1}^{ab}\mu_{1}^{a}\mu_{1}^{b}
 -N\sum_{a}\hat{\mu}_{1}^{a}\mu_{1}^{a}\nonumber \\
= & Nn\frac{\beta}{2}\frac{\kappa_{1}^{2}}{\alpha_{D}}\mu_{1}^{2}-\frac{\alpha N}{2}n(n-1)\hat{q}q-\left[\frac{\alpha N}{2}n\hat{p}_{\text{d}}p_{\text{d}}+\frac{\alpha N}{2}n(n-1)\hat{p}p\right]%-\left[\frac{\alpha N}{2}n\hat{p}_{1\text{d}}p_{1\text{d}}+\frac{\alpha N}{2}n(n-1)\hat{p}_{1}p_{1}\right]+
%\nonumber \\
 %& +\left[\frac{\alpha N}{2}n\hat{p}_{\text{1d}}\mu_{1}\mu_{1}+\frac{\alpha N}{2}n(n-1)\hat{p}_{1}\mu_{1}\mu_{1}\right]
 -Nn\hat{\mu}_{1}\mu_{1}\nonumber \\
= &\frac{\alpha N}{2}n\hat{q}q-\frac{\alpha N}{2}n\hat{p}_{\text{d}}p_{\text{d}}+\frac{\alpha N}{2}n\hat{p}p%-\frac{\alpha N}{2}n\hat{p}_{1\text{d}}p_{1\text{d}}+\frac{\alpha N}{2}n\hat{p}_{1}p_{1}
%\nonumber \\& 
+Nn\frac{\beta}{2}\frac{\kappa_{1}^{2}}{\alpha_{D}}\mu_{1}^{2}%+\frac{\alpha N}{2}n\hat{p}_{\text{1d}}\mu_{1}\mu_{1}-\frac{\alpha N}{2}N\hat{p}_{1}\mu_{1}\mu_{1}
-Nn\hat{\mu}_{1}\mu_{1}
+O(n^{2})
\end{align}


\subsection{Free energy}

The order parameters involved in the RS free energy are

\begin{equation}
-\beta f^\mathrm{RS}\left(q,\hat{q},\left(p_{\text{d}},p\right),\left(\hat{p}_{\text{\text{d}}},\hat{p}\right),%\left(p_{1\text{d}},p_{1}\right),\left(\hat{p}_{1\text{\text{d}}},\hat{p}_{1}\right),
\left(\mu_{1},\hat{\mu}_{1}\right)\right)=\lim_{n\to0}\frac{1}{Nn}\log\left\langle \left\langle Z^{n}\right\rangle \right\rangle 
\end{equation}
In the limit $n\to0$ we have $C\to1$, so we will drop $C$ in some
steps.
\begin{align}
-\beta f^\mathrm{RS}= & -\beta\frac{\alpha}{2}+\frac{\alpha}{2}\hat{q}q-\frac{\alpha}{2}\hat{p}_{\text{d}}p_{\text{d}}+\frac{\alpha}{2}\hat{p}p+ \alpha %\left(
\beta\frac{\kappa_{1}^{2}}{2\alpha_{D}}%+\left(\hat{p}_{\text{1d}}-\hat{p}_{1}\right)\right)
\mu_{1}^2-\hat{\mu}_{1}\mu_{1}+\nonumber \\
% & -\frac{\alpha}{2}\hat{p}_{1\text{d}}p_{1\text{d}}+\frac{\alpha}{2}\hat{p}_{1}p_{1}+\nonumber \\
 & -\frac{\alpha}{2}\left[\log\left[1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)%-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(p_{1}^{\text{d}}-p_{1}\right)\left(\Sigma^{\text{d}}-\Sigma\right)
 \right]-\frac{\beta\Sigma+\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}} \mu_1^2 \left(\Sigma^{\text{d}} -\Sigma\right)}{1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)%-\frac{\beta^{2}}{C}\frac{\kappa_{1}^{2}}{\alpha_{D}}\left(p_{1}^{\text{d}}-p_{1}\right)\left(\Sigma^{\text{d}}-\Sigma\right)
 }\right]+\nonumber \\
 & -\frac{\alpha_{D}}{2}\left[\log\left(1-\alpha_{T}\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)\right)-\frac{\alpha_{T}\left(\hat{p}+\hat{p}_{\text{d}}q-2q\hat{p}\right)}{1-\alpha_{T}\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)}\right]+\nonumber \\
 & -\frac{1}{2}\alpha\hat{q}+\left\langle \int Dz\,\log\left[2\cosh\left(z\sqrt{\alpha\hat{q}}+\hat{\mu}_{1}f_{1}\right)\right]\right\rangle 
\end{align}
We can also write free energy with the following $\beta$ scalings:
\begin{align}
\hat{q} & \to\beta^{2}\hat{q}\label{eq:scaling_qhat}\\
\hat{\mu} & \to\beta\hat{\mu}\label{eq:scaling_muhat}\\
\hat{p} & \to\beta\hat{p}\label{eq:scaling_phat}\\
\hat{p}_{d} & \to\beta\hat{p}_{d}\label{eq:scaling_phatd}\\
\hat{p}_{1} & \to\beta\hat{p}_{1}\label{eq:scaling_p1hat}\\
\hat{p}_{1d} & \to\beta\hat{p}_{1d}\label{eq:scaling_p1hatd}
\end{align}
where equations \eqref{eq:scaling_qhat} and \eqref{eq:scaling_muhat}
are so that the integral term has the same scaling as the standard
Hopfield case. Equations \eqref{eq:scaling_phat} and \eqref{eq:scaling_phatd}
are so that the product $\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)$
remains finite when $\beta\to\infty$, since we now that in this limit
$1-q=O(1/\beta)$ (and also $p_{\text{d}}-p=O(1/\beta)$) . We choose
the scalings \eqref{eq:scaling_p1hat} and \eqref{eq:scaling_p1hatd}
to be consistent with those for $\hat{p}$ and $\hat{p}_{d}$.

By plugging in these changes and removing a factor $-\beta$ from
both sides we get
\begin{align}
f^\mathrm{RS}= & \frac{\alpha}{2}-\frac{\alpha}{2}\beta\hat{q}\left(q-1\right)+\frac{\alpha}{2}\hat{p}_{\text{d}}p_{\text{d}}-\frac{\alpha}{2}\hat{p}p-\frac{\alpha}{2}\frac{\kappa_{1}^{2}}{\alpha_{D}}\mu_{1}^{2}+\hat{\mu}_{1}\mu_{1}\nonumber \\
 & +\frac{\alpha}{2\beta}\left[\log\left[1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)\right]-\frac{\beta\Sigma+\beta^{2}\frac{\kappa_{1}^{2}}{\alpha_{D}}\mu_{1}^{2}\left(\Sigma^{\text{d}}-\Sigma\right)}{1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)}\right]+\nonumber \\
 & +\frac{\alpha}{2\alpha_{T}\beta}\left[\log\left(1-\alpha_{T}\beta\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)\right)-\frac{\alpha_{T}\beta\left(\hat{p}+\hat{p}_{\text{d}}q-2q\hat{p}\right)}{1-\alpha_{T}\beta\left(\hat{p}_{\text{d}}-\hat{p}\right)\left(1-q\right)}\right]+\nonumber \\
 & -\frac{1}{\beta}\left\langle \int Dz\,\log\left[2\cosh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}_{1}f\right]\right)\right]\right\rangle 
 \label{eq:new_RS_free_energy}
\end{align}
%where we also sent $C\to1$.

\subsection{Saddle-point equations}


We take the derivatives of equation \eqref{eq:new_RS_free_energy} and
we get

\begin{subequations}
\label{eq:fac_saddle_point_eq}
\begin{align}    
q &=\left\langle \int Dz\,\tanh^{2}\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right\rangle _{f}\label{eq:saddle_{h}atq}=\int Dz\,\tanh^{2}\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}\right]\right)\\
\mu &=\left\langle \int Dz\,f\tanh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}f\right]\right)\right\rangle _{f}\label{eq:saddle_{h}atmu}=\int Dz\,\tanh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}\right]\right)\\
\hat{q}&=\frac{\kappa_{*}^{2}(\kappa_{1}^{2}p+\kappa_{*}^{2}q+\kappa_{1}^{2}\frac{\alpha_{T}}{\alpha}\mu^{2})}{(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1))^{2}}+\frac{\hat{p}+\alpha_{T}\beta q(\hat{p}-\hat{p}_{\text{d}})^{2}}{\beta(\alpha_{T}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\label{eq:saddle2_{q}}\\
\hat{p}&=\frac{\beta\kappa_{1}^{2}(\kappa_{1}^{2}p+\kappa_{*}^{2}q+\kappa_{1}^{2}\frac{\alpha_{T}}{\alpha}\mu^{2})}{\left(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1)\right)^{2}}\label{eq:saddle_{p}}\\
\hat{p}_{\text{d}}&=\frac{\kappa_{1}^{2}(1+\beta\kappa_{1}^{2}(2p-p_{\text{d}})+\beta\kappa_{*}^{2}(2q-1)+\beta\kappa_{1}^{2}\frac{\alpha_{T}}{\alpha}\mu^{2})}{(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1))^{2}}\label{eq:saddle_{p}d}\\
p&=\frac{q+\alpha_{T}\beta\hat{p}(q-1)^{2}}{(\alpha_{T}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\label{eq:saddle_{h}atp}\\
p_{\text{d}} & =\frac{1+\alpha_{T}\beta(2\hat{p}-\hat{p}_{\text{d}})(q-1)^{2}}{(\alpha_{T}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\label{eq:saddle_{h}atpd}\\
\hat{\mu}&=\frac{\alpha}{\alpha_{D}}\mu_{1}\frac{\kappa_{1}^{2}}{1-\beta\left(\Sigma^{\text{d}}-\Sigma\right)}\label{eq:saddle_{m}u}\\
\end{align}
\end{subequations}

\subsection{Limit $\beta\to\infty$}
\label{sec:fac_zero_T_limit}
The saddle point (where we plugged the relation $\alpha_{T}=\frac{\alpha}{\alpha_{D}}$)
are: \begin{subequations} 
\begin{align}
q & =\int Dz\,\tanh^{2}\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}\right]\right)\\
\mu & =\int Dz\,\tanh\left(\beta\left[z\sqrt{\alpha\hat{q}}+\hat{\mu}\right]\right)\\
\hat{q} & =\frac{\kappa_{*}^{2}(\kappa_{1}^{2}p+\kappa_{*}^{2}q)}{(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1))^{2}}+\frac{\hat{p}+\frac{\alpha}{\alpha_{D}}\beta q(\hat{p}-\hat{p}_{\text{d}})^{2}}{\beta(\frac{\alpha}{\alpha_{D}}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\\
\hat{p} & =\frac{\beta\kappa_{1}^{2}(\kappa_{1}^{2}p+\kappa_{*}^{2}q)}{\left(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1)\right)^{2}}\\
\hat{p}_{\text{d}} & =\frac{\kappa_{1}^{2}(1+\beta\kappa_{1}^{2}(2p-p_{\text{d}})+\beta\kappa_{*}^{2}(2q-1))}{(1+\beta\kappa_{1}^{2}(p-p_{\text{d}})+\beta\kappa_{*}^{2}(q-1))^{2}}\\
p & =\frac{1}{\alpha_{D}}\mu^{2}+\frac{q+\frac{\alpha}{\alpha_{D}}\beta\hat{p}(q-1)^{2}}{(\frac{\alpha}{\alpha_{D}}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\\
p_{\text{d}} & =\frac{1}{\alpha_{D}}\mu^{2}+\frac{1+\frac{\alpha}{\alpha_{D}}\beta(2\hat{p}-\hat{p}_{\text{d}})(q-1)^{2}}{(\frac{\alpha}{\alpha_{D}}\beta(q-1)(\hat{p}-\hat{p}_{\text{d}})-1)^{2}}\\
\hat{\mu} & =\frac{\alpha}{\alpha_{D}}(\hat{p}_{\text{d}}-\hat{p})\mu
\end{align}
\end{subequations} In the infinite $\beta$ limit the order parameters
scale, as it can be seen by inspection, as \begin{subequations} 
\begin{align}
q & =1-\frac{\delta q}{\beta}\\
p & =p_{d}-\frac{\delta p}{\beta}\\
\hat{p} & =\beta\,\delta\hat{p}_{d}-\frac{1}{2}\delta\hat{p}\\
\hat{p}_{d} & =\beta\,\delta\hat{p}_{d}+\frac{1}{2}\delta\hat{p}
\end{align}
\end{subequations} so that the difference $\hat{p}_{d}-\hat{p}=\delta\hat{p}$
is finite. The other order parameters, i.e. $\hat{q}$, $\hat{\mu}$
and $\mu$.

The 8 saddle point equations now reduce to the following ones for
the new rescaled order parameters 
\begin{subequations} 
\begin{align}
\delta q & =\left.\frac{d}{dx}H\left(-\frac{\hat{\mu}+x}{\sqrt{\alpha\hat{q}}}\right)\right|_{x=0}=\frac{2}{\sqrt{\alpha\hat{q}}}G\left(-\frac{\hat{\mu}}{\sqrt{\alpha\hat{q}}}\right)\\
\mu & =2H\left(-\frac{\hat{\mu}}{\sqrt{\alpha\hat{q}}}\right) - 1\\
\hat{q} & =\frac{\kappa_{\star}^{2}(\kappa_{1}^{2}p_{\text{d}}+\kappa_{\star}^{2})}{(1-\kappa_{1}^{2}\delta p-\kappa_{\star}^{2}\delta q)^{2}}+\frac{\delta\hat{p}_{\text{d}}+\frac{\alpha}{\alpha_{D}}\delta\hat{p}^{2}}{(1-\frac{\alpha}{\alpha_{D}}\,\delta q\,\delta\hat{p})^{2}}\\
\delta\hat{p} & =\frac{\kappa_{1}^{2}}{1-\kappa_{1}^{2}\delta p-\kappa_{\star}^{2}\delta q}=\frac{\kappa_{1}^{2}}{1-\delta\Sigma}\\
\delta\hat{p}_{\text{d}} & =\frac{\kappa_{1}^{2}(\kappa_{1}^{2}p_{\text{d}}+\kappa_{*}^{2})}{\left(1-\kappa_{1}^{2}\delta p-\kappa_{\star}^{2}\delta q\right)^{2}}=\frac{\kappa_{1}^{2}\,\Sigma_{\text{d}}}{\left(1-\delta\Sigma\right)^{2}}\\
\delta p & =\beta(p_{d}-p)=\frac{\delta q}{1-\frac{\alpha}{\alpha_{D}}\,\delta q\,\delta\hat{p}}\\
p_{\text{d}} & =\frac{1}{\alpha_{D}}\mu^{2}+\frac{1+\frac{\alpha}{\alpha_{D}}\delta q^{2}\delta\hat{p}_{\text{d}}}{(1-\frac{\alpha}{\alpha_{D}}\,\delta q\,\delta\hat{p})^{2}}\\
\hat{\mu} & =\frac{\alpha}{\alpha_{D}}\,\delta\hat{p}\,\mu
\end{align}
\end{subequations} 
where in the first equality we have used the identity
\begin{equation}
1-\tanh^{2}(x)=\frac{d}{dx}\tanh(x)
\end{equation}
and defined the function
\begin{equation}
H(x)= \frac{1}{2} \mathrm{erfc} \left( \frac{x}{\sqrt{2}} \right)
\end{equation}
where the complementary error function $\mathrm{erfc}$ reads
\begin{equation}
\mathrm{erfc}(x)=  2 \int_x ^\infty \frac{dy}{\sqrt{\pi}} e^{-y^2} 
\end{equation}
Given the scalings for the order parameters, the free energy expression at zero temperature turns out to be
\begin{equation}
\begin{split}
f = &-\frac{\alpha}{2}\left(1+ \delta q \,\hat{q}+\delta p \,\delta\hat{p}_d + p_d \,\delta \hat{p}  \right) + \frac{\alpha}{2} \left(\frac{\kappa_*^2 + \kappa_1^2 \,p_d}{1-\kappa_1^2 \delta p - \kappa_*^2 \delta q} + \frac{\delta \hat{p} + \delta q \,\delta \hat{p}_d}{1-\alpha_T\, \delta q \,\delta \hat{p}} \right) +\frac{\alpha_T}{2}\mu^2 \, \delta \hat{p}-\mu \, \hat{\mu}\\ 
&+ \frac{1}{2} \int Dz \, \left(z\sqrt{\alpha \, \hat{q}}+\hat{\mu} \right) \left(2 \,\theta \left( z\sqrt{\alpha \, \hat{q}}+\hat{\mu}\right) -1 \right)
\end{split}
\end{equation}
\subsection{Limit $\alpha \to \infty$ (from $\beta \to \infty$)}
The scalings are as follows:
\begin{subequations}
	\begin{align}
		\delta q &\to \frac{\alpha_{D}}{\alpha} \delta q \\
		\hat q &\to \frac{\alpha}{\alpha_{D}} \hat{q} \\
		\delta p &\to \frac{\alpha_{D}}{\alpha} \delta p \\
		%\delta \hat p &\to \kappa_{1}^2\\
		%p_d &\to cost\\
		%\delta \hat p_d &\to \kappa_1^2 (\kappa_1^2 p_d + \kappa_{*}^2) \\
		\hat \mu &\to \frac{\alpha}{\alpha_{D}} \hat{\mu} \\		
	\end{align}
\end{subequations}
The value of $\mu$ and $\hat \mu$ depend if we are in the retrieval phase ($\alpha_D$ low, here $\mu \to 1$ and $\hat \mu \to \infty$ as $\alpha$) or the in the non-retrieval phase. We therefore scale also $\mu$ with $\alpha$. The equations become
\begin{subequations} 
	\begin{align}
		\delta q & =\frac{2}{\sqrt{\alpha_{D}\hat{q}}}G\left(-\frac{\hat{\mu}}{\sqrt{\alpha_{D} \hat{q}}}\right)\\
		\mu & =2H\left(-\frac{\hat{\mu}}{\sqrt{\alpha_{D}\hat{q}}}\right) - 1\\
		\hat{q} & =\frac{\delta \hat{p}^2}{(1-\delta\hat{p}\,\delta q)^2}\\
		\delta\hat{p}_{\text{d}} & =\kappa_{1}^{2}(\kappa_{1}^{2}p_{\text{d}}+\kappa_{*}^{2})\\
		\delta p &=\frac{\delta q}{1-\kappa_{1}^{2}\,\delta q}\\
		p_{\text{d}} & =\frac{1}{\alpha_{D}}\mu^{2}+\frac{1}{(1-\kappa_{1}^{2}\,\delta q)^{2}}\\
		\hat{\mu} & =\kappa_{1}^{2}\,\mu
	\end{align}
\end{subequations}
Simplifying
\begin{subequations} 
	\begin{align}
		\delta q & =\frac{2}{\sqrt{\alpha_{D}\hat{q}}}G\left(-\frac{\kappa_{1}^{2}\,\mu}{\sqrt{\alpha_{D} \hat{q}}}\right)\\
		\mu & =2H\left(-\frac{\kappa_{1}^{2}\,\mu}{\sqrt{\alpha_{D}\hat{q}}}\right) - 1\\
		\hat{q} & =\frac{\kappa_{1}^{4}}{(1-\,\delta q\,\kappa_{1}^{2})^{2}}\\
		p_{\text{d}} & =\frac{1}{\alpha_{D}}\mu^{2}+\frac{1}{(1-\delta q\,\kappa_{1}^{2})^{2}}\\
		\delta p &=\frac{\delta q}{1-\delta q\,\kappa_{1}^{2}}\\
	\end{align}
\end{subequations}
Notice that the last equation is totally decoupled, and it depends only on the value assumed by $\delta q$. Now, by rescaling the variables $\kappa_{1}^2 \delta q \to \delta q$ and $\hat{q}\to \kappa_1^4\,\hat{q}$, we obtain the standard Hopfield equations for the factors
\begin{subequations} 
	\begin{align}
		\delta q & =\frac{2 }{\sqrt{\alpha_{D}\hat{q}}}G\left(-\frac{\mu}{\sqrt{\alpha_{D} \hat{q}}}\right)\\
		\mu & =2H\left(-\frac{\mu}{\sqrt{\alpha_{D}\hat{q}}}\right) - 1\\
		\hat{q} & =\frac{1}{(1-\,\delta q)^{2}}
	\end{align}
\end{subequations}

\section{retrieval of one pattern: replica-symmetric calculation }
\label{sec:patt_ret_full_calc}


The replicated partition function is
\begin{equation}
\langle \langle Z^n\rangle \rangle =  \prod_a \sum_{\{s^a_i = \pm 1\}} \langle \langle  e^{\frac{\beta}{2N}\sum_\nu \left( \sum_i \sigma \left(\frac{1}{\sqrt{D}}\sum_k F_{ki}\, x^{\mu}_k \right)s^a_i\right)^2}\rangle \rangle
\end{equation}
where the averages are taken over the quenched variables $x^\mu_k$ and $F_{ki}$. 
\\Since we wants one magnetization of order $\mathcal{O}(1)$ and the remaining ones of order $\mathcal{O}(\frac{1}{\sqrt{N}})$ we rescale properly the finite magnetization $m^a_1 \to \sqrt{N}m^a_1$. 
\begin{equation}
\begin{split}
\langle \langle Z^n \rangle \rangle = \int \prod_{\mu a} \frac{dm^a_\nu}{\sqrt{2\pi}} \sum_{\{s^a_i\}}e^{\frac{\beta\, N}{2}\sum_a (m^a_1)^2 +\frac{\beta}{2}\sum_{\mu >1}\sum_a (m^a_\nu)^2}\langle \langle \prod_a \delta \left(\sqrt{N}m^a_1-\frac{1}{\sqrt{N}}\sum_i \sigma \left( \frac{1}{\sqrt{D}}\sum_k F_{ki}\,x^1_k\right) s^a_i\right)\\
\prod_{a,\mu>1} \delta\left(m^a_\nu -\frac{1}{\sqrt{N}}\sum_i \sigma \left(\frac{1}{\sqrt{D}}\sum_k F_{ki} \,x^{\mu}_k \right)s^a_i \right) \rangle\rangle\\
\end{split}
\end{equation}
Taking the average over the remaining $P-1$ patterns $x^{\mu}_k$, we get
\begin{equation}
\begin{split}
\langle Z^n \rangle =& \int \prod_a \frac{dm^a}{2\pi} \sum_{\{s^a_i\}} \int \prod_{ab}\frac{dq^{ab}\,d\hat{q}^{ab}}{2\pi}\, \frac{dp^{ab}\, d\hat{p}^{ab}}{2\pi}\prod_{ak} \frac{d\mu^a_k \, d\hat{\mu}^a_k}{2\pi} \prod_i \frac{dv_i \,d\hat{v}_i}{2\pi}\\
& \exp\left\{-\frac{P-1}{2}\log \det \left(\mathbf{I}- \beta \Sigma\right) - \frac{\beta N}{2}\sum_a (m^a)^2+\beta \sum_a m^a \sum_i \sigma (v_i)s^a_i\right\}\\
&\exp\left\{-N\alpha \sum_{a<b}q^{ab}\,\hat{q}^{ab}+\alpha \sum_{a<b}\hat{q}^{ab}\sum_is^a_i s^b_i-N\alpha \sum_{a\leq b}p^{ab}\hat{p}^{ab}+\alpha_T \sum_{a\leq b}\hat{p}^{ab}\sum_k\mu^a_k \mu^b_k+ i\sum_{ak}\mu^a_k \hat{\mu}^a_k \color{black}+ i \sum_i v_i \hat{v}_i\right\}\\
&\prod_{ki} \left\langle \exp\left\{-i F_{ki} \left(\sum_a \frac{\hat{\mu}^a_k \,s^a_i}{\sqrt{N}}+\frac{x^1_k \hat{v}_i}{\sqrt{D}} \right)\right\}\right\rangle_{F_{ki}}
\end{split}
\end{equation}
where $\Sigma$ is the covariance matrix 
\begin{equation}
\Sigma^{ab} = \kappa_*^2 q^{ab} + \kappa_1^2 p^{ab}
\end{equation}
while the order parameters are defined as follows
\begin{subequations}
\begin{align}
&q^{ab} = \frac{1}{N}\sum_i s^a_i s^b_i\\
&p^{ab} = \frac{1}{D}\sum_k \mu^a_k \mu^b_k\\
&\mu^a_k = \frac{1}{\sqrt{N}}\sum_i F_{ki}s^a_i.
\end{align}
\end{subequations}
The average over the factors is
\begin{equation}
\begin{split}
\prod_{ki} \langle e^{-i F_{ki} \left(\sum_a \frac{\hat{\mu}^a_k \,s^a_i}{\sqrt{N}}+\frac{x^1_k \hat{v}_i}{\sqrt{D}} \right)}\rangle_{F_{ki}} &= \prod_{ki} \int \frac{dF_{ki}}{\sqrt{2\pi}}\,\exp\left\{-\frac{1}{2}F_{ki}^2-i F_{ki} \left(\sum_a \frac{\hat{\mu}^a_k \,s^a_i}{\sqrt{N}}+\frac{x^1_k \hat{v}_i}{\sqrt{D}} \right)\right\} \\
&= \exp\left\{-\frac{1}{2}\sum_{ab}\left(\sum_k \hat{\mu}^a_k \hat{\mu}^b_k \right)\,q^{ab} \color{black}-\frac{1}{2}\sum_i \hat{v}^2_i-\frac{1}{\sqrt{ND}}\sum_i \hat{v}_i \sum_a s^a_i \sum_k \hat{\mu}^a_k x^1_k\right\}
\end{split}
\end{equation}

The replicated partition function becomes
\begin{equation}
\begin{split}
\langle Z^n \rangle =& \int \prod_a \frac{dm^a}{2\pi}\,\sum_{\{s^a_i\}} \int \prod_{ab} \frac{dq^{ab}\,d\hat{q}^{ab}}{2\pi}\,\frac{dp^{ab}\,d\hat{p}^{ab}}{2\pi}\,\prod_{ak}\frac{d\mu^a_k \,d\hat{\mu}^a_k}{2\pi}\,\prod_i \frac{dv_i\,d\hat{v}_i}{2\pi}\\
&\times \exp\left\{-\frac{\alpha N}{2}\log \det \left(\mathbb{I}-\beta \Sigma \right)-\frac{\beta N}{2}\sum_a (m^a)^2-\frac{N\alpha}{2}\sum_{a\neq b}q^{ab}\hat{q}^{ab}\right\}\\
&\times \exp\left\{-\frac{N\alpha}{2}\sum_{ab}p^{ab}\hat{p}^{ab}+i\sum_i v_i \hat{v}_i+i\sum_{ak}\mu^a_k\hat{\mu}^a_k+\beta \sum_a m^a \sum_i \sigma(v_i)s^a_i +\frac{\alpha}{2}\sum_{a\neq b}\hat{q}^{ab}\sum_i s^a_i s^b_i \right\} \\
&\times \exp\left\{-\frac{1}{2}\sum_{ab}\left(\sum_k \hat{\mu}^a_k \hat{\mu}^a_k \right)q^{ab}+\frac{\alpha_T}{2}\sum_{ab}\hat{p}^{ab}\sum_k \mu^a_k \mu^b_k-\frac{1}{2}\sum_i \hat{v}^2_i\right\}\\
&\times \exp\left\{-\frac{1}{\sqrt{ND}}\sum_i \hat{v}_i \sum_a s^a_i\sum_k \hat{\mu}^a_k x^1_k-\frac{1}{2}\sum_i \hat{v}^2_i\right\}
\end{split}
\end{equation}
Now we want to integrate over the factors so we make explicit the terms depending on $\mu$ and $\hat{\mu}$ 
\begin{equation}
\begin{split}
&\int \prod_{ak} \frac{d\mu^a_k\,d\hat{\mu}^a_k}{2\pi}\,e^{\frac{\alpha_T}{2} \sum_{ab}\hat{p}^{ab}\sum_k \mu^a_k \mu^b_k + i \sum_{ak}\hat{\mu}^a_k\left(\mu^a_k + \frac{i}{\sqrt{ND}}x^1_k \sum_i \hat{v}_i s^a_i \right)-\frac{1}{2}\sum_{ab}\left(\sum_k \hat{\mu}^a_k \hat{\mu}^b_k \right)q^{ab}}= e^{\alpha_D N \phi_1}
\end{split}
\end{equation}
by noticing that the expression is factorized over the index $k$. Then
\begin{equation}
\begin{split}
\phi_1 &= \ln \int \prod_a \frac{d\mu^a}{\sqrt{2\pi \det q}}\,e^{\frac{\alpha_T}{2}\sum_{ab}\hat{p}^{ab}\mu^a \mu^b -\frac{1}{2}\sum_{ab}\left( \mu^a +\frac{i}{\sqrt{ND}}x^1 \sum_i \hat{v}_i s^a_i\right) (q^{-1})_{ab}\left(\mu^b +\frac{i}{\sqrt{ND}}x^1\sum_j \hat{v}_j s^b_j \right)} \\
&=\ln \int \prod_a \frac{d\mu^a}{\sqrt{2\pi \det q}}\,e^{-\frac{1}{2}\sum_{ab}\mu^a \left(q^{-1}-\alpha_T \hat{p} \right)_{ab} \mu^b -\frac{i x^1}{\sqrt{ND}}\sum_{ab}\left( \sum_i \hat{v}_i s^a_i\right)(q^{-1})_{ab}\mu^b+\frac{1}{2ND}\sum_{ab}\left( \sum_i \hat{v}_i s^a_i\right) (q^{-1})_{ab}\left(\sum_j \hat{v}_j s^b_j \right)}\\
&= -\frac{1}{2}\ln \det \left(\mathbb{I}-\alpha_T\, q \hat{p} \right)-\frac{\alpha_T}{2ND}\sum_{ab}\left( \sum_i \hat{v}_i s^a_i\right) \left(\hat{p}^{-1}-\alpha_T \,q \right)^{-1}_{ab}\left(\sum_j \hat{v}_j s^b_j \right)
\end{split}
\end{equation}
Calling $t^a = \frac{1}{N}\sum_i \hat{v}_i s^a_i$ and going back to the original expression we get
\begin{equation}
\begin{split}
\langle Z^n \rangle =& \int \prod_a \frac{dm^a}{\sqrt{2\pi}}\sum_{\{s^a_i\}}\int \prod_{ab} \frac{dq^{ab}\,d\hat{q}^{ab}}{2\pi}\,\frac{dp^{ab}\,d\hat{p}^{ab}}{2\pi} \int \prod_i \frac{dv_i \, d\hat{v}_i}{2\pi} \int \prod_a \frac{dt^a\,d\hat{t}^a}{2\pi}\\
&\times e^{-\frac{\alpha N}{2}\ln \det \left( \mathbb{I}-\beta \Sigma\right)-\frac{\alpha_D N}{2}\ln \det \left(\mathbb{I}-\alpha_T q\hat{p}\right)- \frac{\beta N}{2}\sum_a (m^a)^2}\\
&\times\, e^{-\frac{N\alpha}{2}\sum_{a\neq b}q^{ab}\hat{q}^{ab}-\frac{N\alpha}{2}\sum_{ab}p^{ab}\hat{p}^{ab}+i\sum_i v_i \hat{v}_i +i N \sum_a t^a \hat{t}^a +\beta \sum_a m^a \sum_i \sigma(v_i)s^a_i+\frac{\alpha}{2}\sum_{a\neq b}\hat{q}^{ab}\sum_i s^a_i s^b_i -\frac{1}{2}\sum_i \hat{v}^2_i }\\
&\times \,e^{-\frac{N\alpha_T}{2}\sum_{ab}t^a \left(\hat{p}^{-1}-\alpha_T\,q \right)^{-1}_{ab} t^b-i\sum_a \hat{t}^a \sum_i \hat{v}_i s^a_i}.
\end{split}
\end{equation}
Now we rearrange all the terms depending on $v,\hat{v}$ and on the spins
\begin{equation}
\begin{split}
\int \prod_i \frac{dv_i\,d\hat{v}_i}{2\pi}\sum_{\{s^a_i\}} \,e^{-\frac{1}{2}\sum_i \hat{v}^2_i + i \sum_i \hat{v}_i \left(v_i - \sum_a \hat{t}^a s^a_i \right)+\frac{\alpha}{2}\sum_{a\neq b}\hat{q}^{ab}\sum_is^a_i s^b_i +\beta \sum_a m^a \sum_i \sigma(v_i) s^a_i} = e^{N\phi_2}
\end{split}
\end{equation}
where
\begin{equation}\label{phi2}
\begin{split}
\phi_2 &= \ln \int Dv\sum_{\{s^a\}}e^{-\frac{1}{2}\sum_{ab}\hat{t}^a\hat{t}^b s^a s^b+ v \sum_a \hat{t}^a s^a + \frac{\alpha}{2}\sum_{a\neq b}\hat{q}^{ab}s^a s^b +\beta \sigma(v) \sum_a m^a s^a}\\
&= \ln \int Dv\sum_{\{s^a\}}\,e^{-\frac{1}{2}\sum_{ab}\left(\alpha \,\hat{q}^{ab} -\hat{t}^a \hat{t}^b \right)s^a s^b +\sum_a s^a \left(v\,\hat{t}^a +\beta\, \sigma(v)\, m^a \right)} 
\end{split}
\end{equation}
\subsection{RS Ansatz}
By imposing the ansatz as follows $\ldots$ one finds that the quadratic terms in $\phi_2$ can be linearized as follows
\begin{equation*}
\frac{1}{2}\sum_{ab}\left(\alpha \, \hat{q}^{ab}-\hat{t}^a \hat{t}^b \right)s^a s^b = \frac{(\alpha\,\hat{q}-\hat{t}^2)}{2}\left(\sum_a s^a \right)^2-\frac{\alpha \,\hat{q}}{2}\sum_a (s^a)^2
\end{equation*}
therefore the $\eqref{phi2}$ becomes
\begin{equation}
\begin{split}
\phi_2 &= -n \frac{\alpha}{2}\hat{q}+\ln \int Dx \, \int Dv \sum_{\{s^a\}}\,e^{\sum_a s^a\left( v\,\hat{t}+\beta\,\sigma(v)\,m + \sqrt{\alpha \hat{q}-\hat{t}^2}\,x\right)}\\
&=-n \frac{\alpha}{2}\hat{q}+n \int Dx \, \int Dv \ln 2\cosh \left( v\,\hat{t}+\beta\,\sigma(v)\, m+\sqrt{\alpha \hat{q}-\hat{t}^2}\,x\right).
\end{split}
\end{equation}
Finally we must substitute the ansatz into the remaining terms that we recall for clarity 
\begin{equation}
\begin{split}
Z^n &= \int \prod_a \frac{dm^a}{2\pi}\int \prod_{ab}\frac{dq^{ab}\,d\hat{q}^{ab}}{2\pi}\,\frac{dp^{ab}\,d\hat{p}^{ab}}{2\pi}\int \prod_a \frac{dt^a\,d\hat{t}^a}{2\pi}\,e^{-\frac{\alpha N}{2}\log\det (\mathbb{I}-\beta \Sigma) -\frac{\alpha_D N}{2}\log \det (\mathbb{I}-\alpha_T \,q\hat{p})-\frac{\beta N}{2}\sum_a (m^a)^2 -\frac{\alpha N}{2}\sum_{a\neq b}q^{ab}\hat{q}^{ab}}\\
&\times \, e^{-\frac{\alpha N}{2}\sum_{ab}p^{ab}\hat{p}^{ab}-N\sum_a t^a \hat{t}^a + \frac{\alpha_T\,N}{2}\sum_{ab}t^a \left( \hat{p}^{-1}-\alpha_T \,q\right)^{-1}_{ab} t^b + N \phi_2}.
\end{split}
\end{equation}
\subsection{Interaction term}
The interaction term reads
\begin{equation}
\begin{split}
\phi_I = -\frac{\beta}{2}m^2 +\frac{\alpha}{2}q \hat{q}-\frac{\alpha}{2} p_d \hat{p}_d +\frac{\alpha}{2}p\hat{p}-t\hat{t}+\frac{\alpha_T}{2}t^2 \left(\hat{p}^{-1}_d -\alpha_T \right)^{-1}-\frac{\alpha_T}{2}t^2 \left(\hat{p}^{-1} -\alpha_T \,q\right)^{-1}
\end{split}
\end{equation}
and once the determinants have been calculated and the parameters corrected with temperature 
\begin{subequations}
\begin{align*}
&\hat{q} \to \beta^2 \,\hat{q}\\
&\hat{t} \to \beta \,\hat{t}\\
&\hat{p}_d \to \beta \,\hat{p}_d\\
&\hat{p} \to \beta \,\hat{p}
\end{align*}
\end{subequations}
the free energy can be easily expressed as
\begin{equation}
\begin{split}
f_\mathrm{RS} &= \lim_{n \to 0}-\frac{1}{\beta n N}\ln \langle\langle Z^n \rangle\rangle \\
&= \frac{1}{2}m^2 - \frac{\beta\alpha}{2}\hat{q}(q-1) + \frac{\alpha}{2}p_d\hat{p}_d -\frac{\alpha}{2}p\hat{p}+t\hat{t}-\frac{\alpha_T}{2}\frac{t^2 \,\hat{p}_d}{(1-\beta \alpha_T \,\hat{p}_d)}+\frac{\alpha_T}{2}\frac{t^2 \, \hat{p}}{(1-\beta \alpha_T \,q\,\hat{p})}+\\
&\,\,\,\,\,\,+\frac{\alpha}{2\beta}\left[\ln \left(1-\beta\left( \Sigma_d - \Sigma\right) \right)-\frac{\beta \Sigma}{1-\beta (\Sigma_d - \Sigma)} \right]+\frac{\alpha}{2\beta \alpha_T}\left[\ln \left(1-\beta \alpha_T (\hat{p}_d-\hat{p})(1-q) \right) - \frac{\beta \alpha_T \left(\hat{p}+q\hat{p}_d-2q\hat{p} \right)}{1-\beta\alpha_T(\hat{p}_d -\hat{p})(1-q)}\right]+\\
&\,\,\,\,\,\,-\frac{1}{\beta} \int Dx \int Dv \, \ln 2 \cosh \left[ \beta \left(v\,\hat{t}+\sigma(v) \,m + \sqrt{\alpha \hat{q}-\hat{t}^2}\,x \right)\right]
\end{split}
\end{equation}
where $\Sigma = \kappa_*^2 \,q+\kappa_1^2 \,p$ and $\Sigma_d = \kappa_*^2 +\kappa_1^2 \,p_d$.

\subsection{Saddle point equations}
By imposing the activation function $\sigma (v) = \text{sign} (v)$ one can write down the saddle point equations 
\begin{subequations}
\label{eq:patt_saddle_point_eq}
\begin{align}
q &= \int Dv \int Dx \,\tanh^2 \left[\beta\left(v \hat{t}+\sigma(v)\, m + \sqrt{\alpha \, \hat{q}-\hat{t}^2}\,x \right)  \right] \nonumber \\
&= 2\int Dv \,\Theta (v) \int Dx \, \tanh^2\left[\beta \, \left(m+ \hat{t}\,v+\sqrt{\alpha\, \hat{q}-\hat{t}^2}\,x \right) \right]\\
t &= \frac{2\beta \,m}{\sqrt{2\pi}}\left[1-\int Dx \,\tanh^2 \left(\beta \,x\sqrt{\alpha \,\hat{q}-\hat{t}^2} \right) \right]\\
m &= \int Dx \int Dv \, \sigma (v) \tanh \left[\beta \left(v \hat{t}+\sigma (v)\, m +\sqrt{\alpha \hat{q}-\hat{t}^2}\,x \right) \right] \nonumber \\
&= 2 \int Dv \,\Theta(v) \int Dx \,\tanh\left[\beta \, \left(m+ \hat{t}\,v+\sqrt{\alpha\, \hat{q}-\hat{t}^2}\,x \right) \right] \\
p &= \frac{1}{\alpha_D}\,\frac{t^2}{(1-\beta \, \alpha_T \, q\,\hat{p})^2}+\frac{q+\beta \alpha_T \,(1-q)^2 \,\hat{p}}{(1-\beta \alpha_T (1-q)(\hat{p}_d -\hat{p}))^2}\\
p_d &= \frac{1}{\alpha_D} \, \frac{t^2}{(1-\beta \,\alpha_T \, \hat{p}_d)^2}+\frac{1+\beta \alpha_T (1-q)^2 (2\hat{p}-\hat{p}_d)}{(1-\beta \alpha_T (1-q)(\hat{p}_d-\hat{p}))^2}\\
\hat{q} &= \frac{\alpha_T}{\alpha_D} \, \frac{t^2 \, \hat{p}^2}{(1-\beta \,\alpha_T \, q\,\hat{p})^2}+\frac{\kappa_*^2 (\kappa_*^2 \,q +\kappa_1^2 \, p)}{\left[1-\beta \left( \kappa_*^2 (1-q)+ \kappa_1^2 (p_d-p)\right)\right]^2}+\frac{\hat{p}+\beta \alpha_T \, q (\hat{p}_d -\hat{p})^2}{\beta \left[1-\beta \alpha_T (1-q)(\hat{p}_d -\hat{p}) \right]^2}\\
\hat{t} &= \alpha_T \, t \left(\frac{\hat{p}_d}{1-\beta \,\alpha_T \,\hat{p}_d} - \frac{\hat{p}}{1-\beta \,\alpha_T \,q \,\hat{p}} \right)\\
\hat{p} &= \frac{\beta \, \kappa_1^2 \left(\kappa_1^2 \,p +\kappa_*^2 \,q \right)}{\left[1-\beta \left( \kappa_1^2 (p_d -p) +\kappa_*^2 (1-q)\right)\right]^2}\\
\hat{p}_d &= \frac{\kappa_1^2 \, \left(1-\beta \left(\kappa_1^2 (p_d-2p) +\kappa_*^2 (1-2 q) \right) \right)}{\left[1-\beta \left( \kappa_1^2 (p_d -p) +\kappa_*^2 (1-q)\right) \right]^2}
\end{align}
\end{subequations}
\subsection{Limit $\beta \to \infty$}
\label{sec:patt_zero_T_limit}

The scalings for the order parameters turn out to be
\begin{subequations}
\begin{align*}
&q \to 1-\frac{\delta q}{\beta}\\
&p = p_d -\frac{\delta p}{\beta}\\
&\hat{p} = \beta \delta \hat{p}_d-\frac{1}{2}\delta\hat{p}\\
&\hat{p}_d = \beta \delta \hat{p}_d+\frac{1}{2}\delta\hat{p}\\
&\hat{t} \to \frac{\delta \hat{t}}{\beta^2}
\end{align*}
\end{subequations}
from which on can derive how the equations change in the limit
\begin{subequations}
\begin{align}
&\delta q= \frac{2}{\sqrt{\alpha \, \hat{q}}}\,G\left(-\frac{m}{\sqrt{\alpha \, \hat{q}}} \right)\\
&t = \frac{2\,m}{\pi \sqrt{\alpha \,\hat{q}-\hat{t}^2}}\\
&m = 2H\left(-\frac{m}{\sqrt{\alpha	\,\hat{q}}} \right)-1\\
&p_d = \frac{1+\frac{\alpha}{\alpha_D}\,\delta q^2\delta \hat{p}_d}{(1-\frac{\alpha}{\alpha_D}\delta q\,\delta \hat{p})^2}\\
&\delta p = \beta \left(p_d -p \right)=\frac{\delta q}{1-\frac{\alpha}{\alpha_D}\, \delta q \, \delta \hat{p}}\\
&\hat{q}= \frac{\kappa_{\star}^{2}(\kappa_{1}^{2}\,p_{\text{d}}+\kappa_{\star}^{2})}{(1-\kappa_{1}^{2}\delta p-\kappa_{\star}^{2}\delta q)^{2}}+\frac{\delta\hat{p}_{\text{d}}+\frac{\alpha}{\alpha_{D}}\delta\hat{p}^{2}}{(1-\frac{\alpha}{\alpha_{D}}\,\delta q\,\delta\hat{p})^{2}}\\
&\delta\hat{t} = \delta q \, t\\
&\delta\hat{p} = \frac{\kappa_1^2}{1-\kappa_1^2 \,\delta p - \kappa_*^2 \, \delta q}\\
&\delta \hat{p}_d = \frac{\kappa_1^2 \left(\kappa_1^2 \, p_d +\kappa_*^2 \right)}{(1-\kappa_1^2 \,\delta p - \kappa_*^2 \, \delta q)^2}
\end{align}
\end{subequations}
Given the scalings fo the order parameters, the free energy expression turns out to be
\begin{equation}
\begin{split}
f = &\frac{1}{2}\left(m^2 + \alpha \, \delta p \delta \hat{p}_d + \alpha \, \delta q \left(\hat{q}- \frac{\delta \hat{p}_d}{1-\alpha_T \, \delta q \, \delta \hat{p}} \right) \right)+ \frac{\alpha}{2}\delta \hat{p} \left(p_d - \frac{1}{1-\alpha_T \, \delta q \delta \hat{p}} \right) +\\
&-\int Dx \, \int Dv \, \left(m+ x \sqrt{\alpha \, \hat{q}}\right)\, \left(\theta \left(m+ x \sqrt{\alpha \, \hat{q}} \right) + \theta \left( v \right) -1  \right)
\end{split}
\end{equation}
\subsection{Limit $\alpha_D \to \infty$}
\label{sec:limit_aD_inf}
Taking the $\alpha_D \to \infty$ limit we should recover the standard hopfield model. Indeed, the saddle point equations become 
\begin{subequations}
\begin{align}
&q = \int Dx \, \tanh^2 \left[ \beta \left(m + \sqrt{\alpha \, \hat{q}}\,x \right)\right]\\
&m = \int Dx \, \tanh \left[\beta \left(m + \sqrt{\alpha \, \hat{q}}\,x \right) \right]\\
&p \to q \\
&p_d \to 1 \\
&\hat{q} = \frac{\hat{p}}{\beta} + \frac{q\,\kappa_*^2 (\kappa_1^2 +\kappa_*^2)}{\left[1-\beta (1-q)(\kappa_1^2 + \kappa_*^2)\right]^2}= \frac{(\kappa_1^2 + \kappa_*^2)^2\,q}{\left[1-\beta (1-q)(\kappa_1^2 + \kappa_*^2)\right]^2}\\
&\hat{p} = \frac{\beta \, q \, \kappa_1^2 (\kappa_1^2 +\kappa_*^2)}{\left[1-\beta (1-q)(\kappa_1^2 + \kappa_*^2)\right]^2}\\
&\hat{p}_d = \frac{\kappa_1^2 \left(1-\beta \left(1-2 q \right) (\kappa_1^2 + \kappa_*^2)\right)}{\left[1-\beta (1-q)(\kappa_1^2 + \kappa_*^2)\right]^2}\\
&\hat{t} \to 0
\end{align}
\end{subequations}
where $\kappa_1^2 +\kappa_*^2 =1$ for $\sigma (v) = \text{sign}(v)$.

\section{Numerical results}
\counterwithin{figure}{section}
\begin{figure}[h]
    \centering
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[width=0.49\textwidth]{fig_1C.pdf}}
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[width=0.49\textwidth]{fig_1D.pdf}}
    \caption{ Comparison with numerical results for the retrieval of one factor. Note that, as we increase $\alpha$, the finite-size effects become more pronounced. The simulations are performed initializing the model to a factor $\textbf{f}_k$, running the update rule~\eqref{eq:update_rule}, then measuring $\mu_k$ at convergence. We used 100, 50, 20 and 10 samples for increasing values of $N$.}
    \label{fig:num_comp2_fac}
\end{figure}

\begin{figure}
    \centering
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[width=0.49\textwidth]{fig_2A.pdf}}
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[width=0.49\textwidth]{fig_2B.pdf}}
    \caption{ Comparison with numerical results for the retrieval of one pattern. The discrepancy increases for smaller values of $\alpha_D$. The simulations are performed initializing the model to a pattern $\bxi_\nu$, running the update rule~\eqref{eq:update_rule}, then measuring $m_\nu$ at convergence. We used 100, 50, 20, 10 and 5 samples for increasing values of $N$. }
    \label{fig:num_comp3_patt}
\end{figure}
\begin{figure}
    \centering
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[width=0.49\textwidth]{fig_3B.pdf}}
    \subcaptionbox{\hspace*{6cm}}{\includegraphics[width=0.49\textwidth]{fig_3C.pdf}}
    \caption{ Analysis of the discrepancy between simulations and theory in the case of the retrieval of one pattern. a) If the temperature is low enough, the entropy becomes negative, signaling the incorrectness of the RS ansatz. As we lower $\alpha_D$ we see that the entropy becomes more negative, which is consistent with the RS solution progressively becoming a worse approximation of the numerical simulations. This could explain why the discrepancy increases by lowering $\alpha_D$. b) Numerical check that the residual magnetizations $m_{\nu>1}$ and $\mu_k$ correctly go to zero for $N\to\infty$ when we initialize the model to $\bxi_1$. This excludes the possibility that the ansatz \eqref{eq:pattern_retrieval_state} is inconsistent with the simulations.}
    \label{fig:negative_entropy}
\end{figure}

\end{document}
